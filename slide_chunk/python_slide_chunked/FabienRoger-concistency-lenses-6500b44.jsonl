{"filename": "tests/test_distance.py", "chunked_list": ["import torch as th\nfrom torch.distributions import Categorical, kl_divergence\n\nfrom tuned_lens.stats import js_distance, js_divergence\n\n\ndef test_js_divergence():\n    p = Categorical(logits=th.randn(10))\n    q = Categorical(logits=th.randn(10))\n    m = Categorical(probs=0.5 * (p.probs + q.probs))  # type: ignore\n\n    kl_fwd = kl_divergence(p, m)\n    kl_bwd = kl_divergence(q, m)\n    gt_js = 0.5 * (kl_fwd + kl_bwd)\n\n    our_js_fwd = js_divergence(p.logits, q.logits)  # type: ignore\n    our_js_bwd = js_divergence(q.logits, p.logits)  # type: ignore\n\n    th.testing.assert_close(gt_js, our_js_fwd)\n    th.testing.assert_close(our_js_fwd, our_js_bwd)  # Symmetry", "\n\ndef test_js_distance():\n    a = th.randn(1000, 3)\n    b = th.randn(1000, 3)\n    c = th.randn(1000, 3)\n\n    dist_ab = js_distance(a, b)\n    dist_bc = js_distance(b, c)\n    dist_ac = js_distance(a, c)\n\n    # Triangle inequality\n    assert th.all(dist_ab + dist_bc >= dist_ac)", ""]}
{"filename": "tests/test_data.py", "chunked_list": ["import math\n\nimport transformers as tr\nfrom datasets import Dataset\n\nfrom tuned_lens import data\n\n\ndef test_chunk_and_tokenize(\n    text_dataset: Dataset, small_model_tokenizer: tr.PreTrainedTokenizerBase\n):\n    max_length = 128\n    chunked, _ = data.chunk_and_tokenize(\n        text_dataset,\n        small_model_tokenizer,\n        load_from_cache_file=False,\n        max_length=max_length,\n    )\n\n    length = min(small_model_tokenizer.model_max_length, max_length)\n    for i in range(len(chunked)):\n        assert len(chunked[i][\"input_ids\"]) == length", "def test_chunk_and_tokenize(\n    text_dataset: Dataset, small_model_tokenizer: tr.PreTrainedTokenizerBase\n):\n    max_length = 128\n    chunked, _ = data.chunk_and_tokenize(\n        text_dataset,\n        small_model_tokenizer,\n        load_from_cache_file=False,\n        max_length=max_length,\n    )\n\n    length = min(small_model_tokenizer.model_max_length, max_length)\n    for i in range(len(chunked)):\n        assert len(chunked[i][\"input_ids\"]) == length", "\n\ndef test_compute_nats_to_bpb_ratio(\n    text_dataset: Dataset, gpt2_tokenizer: tr.PreTrainedTokenizerBase\n):\n    max_length = 128\n    _, ratio = data.chunk_and_tokenize(\n        text_dataset, gpt2_tokenizer, load_from_cache_file=True, max_length=max_length\n    )\n    # We expect the ratio to be around 0.29, see https://arxiv.org/pdf/2101.00027.pdf,\n    # section 3.1\n    assert 0.2 / math.log(2) < ratio < 0.4 / math.log(2)", ""]}
{"filename": "tests/test_unembed.py", "chunked_list": ["import torch as th\nimport transformers as tr\n\nfrom tuned_lens.model_surgery import get_final_norm\nfrom tuned_lens.nn import Unembed\n\n\ndef back_translate(unembed: Unembed, h: th.Tensor, tol: float = 1e-4) -> th.Tensor:\n    \"\"\"Project hidden states into logits and then back into hidden states.\"\"\"\n    scale = h.norm(dim=-1, keepdim=True) / h.shape[-1] ** 0.5\n    logits = unembed(h)\n    return unembed.invert(logits, h0=th.randn_like(h), tol=tol).preimage * scale", "\n\ndef test_correctness(random_small_model: tr.PreTrainedModel):\n    # One problem: we want to check that we handle GPT-J's unembedding bias\n    # correctly, but it's zero-initialized. Give it a random Gaussian bias.\n    U = random_small_model.get_output_embeddings()\n    if U.bias is not None:\n        U.bias.data.normal_()\n\n    unembed = Unembed(random_small_model)\n    ln_f = get_final_norm(random_small_model)\n\n    x = th.randn(1, 1, random_small_model.config.hidden_size)\n    y = U(ln_f(x)).log_softmax(-1)  # type: ignore[attr-defined]\n\n    th.testing.assert_close(y, unembed(x).log_softmax(-1))\n\n    x_hat = back_translate(unembed, x, tol=1e-5)\n    th.testing.assert_close(y.exp(), unembed(x_hat).softmax(-1), atol=5e-4, rtol=0.01)", ""]}
{"filename": "tests/test_subspaces.py", "chunked_list": ["import pytest\nimport torch as th\n\nfrom tuned_lens.causal import remove_subspace\n\n\n@pytest.mark.parametrize(\"d\", list(range(1, 1000, 100)))\ndef test_remove_subspace(d: int):\n    a = th.randn(10, d, dtype=th.float64)\n\n    for k in range(1, d, 10):\n        b = th.randn(d, k, dtype=th.float64)\n        inner = a @ b\n\n        a_ = remove_subspace(a, b, mode=\"zero\")\n        inner_ = a_ @ b\n        th.testing.assert_close(inner_, th.zeros_like(inner_))\n\n        a_ = remove_subspace(a, b, mode=\"mean\")\n        inner_ = a_ @ b\n        th.testing.assert_close(inner_, inner.mean(0, keepdim=True).expand_as(inner_))", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_load_artifact.py", "chunked_list": ["from tuned_lens.load_artifacts import load_lens_artifacts\n\n\ndef test_load_lens_artifact_smoke():\n    load_lens_artifacts(\"gpt2\", \"AlignmentResearch/tuned-lens\")\n"]}
{"filename": "tests/test_model_surgery.py", "chunked_list": ["import pytest\nimport torch as th\nfrom transformers import PreTrainedModel, models\n\nfrom tuned_lens import model_surgery\n\n\ndef test_get_final_layer_norm_raises(opt_random_model: PreTrainedModel):\n    opt_random_model.base_model.decoder.final_layer_norm = None\n    with pytest.raises(ValueError):\n        assert model_surgery.get_final_norm(opt_random_model)", "\n\ndef test_get_final_layer_norm(random_small_model: PreTrainedModel):\n    ln = model_surgery.get_final_norm(random_small_model)\n    assert isinstance(ln, (th.nn.LayerNorm, models.llama.modeling_llama.LlamaRMSNorm))\n\n\ndef test_get_layers_from_model(random_small_model: PreTrainedModel):\n    path, layers = model_surgery.get_transformer_layers(random_small_model)\n    assert isinstance(layers, th.nn.ModuleList)\n    assert isinstance(path, str)\n    assert len(layers) == random_small_model.config.num_hidden_layers", ""]}
{"filename": "tests/test_lenses.py", "chunked_list": ["from pathlib import Path\n\nimport mock\nimport pytest\nimport torch as th\nimport transformers as trf\n\nfrom tuned_lens.load_artifacts import load_lens_artifacts\nfrom tuned_lens.nn.lenses import LogitLens, TunedLens, TunedLensConfig\nfrom tuned_lens.nn.unembed import Unembed", "from tuned_lens.nn.lenses import LogitLens, TunedLens, TunedLensConfig\nfrom tuned_lens.nn.unembed import Unembed\n\n\n@pytest.fixture\ndef model_config():\n    config = mock.MagicMock(trf.PretrainedConfig)\n    config.hidden_size = 128\n    config.vocab_size = 100\n    config.num_hidden_layers = 3\n    return config", "\n\n@pytest.fixture\ndef model(model_config):\n    model = mock.MagicMock(trf.PreTrainedModel)\n    model.config = model_config\n    model.get_output_embeddings = mock.MagicMock(return_value=th.nn.Linear(128, 100))\n    return model\n\n", "\n\n@pytest.fixture\ndef unembed():\n    mock_unembed = mock.MagicMock(Unembed)\n    W = th.randn(100, 128)\n    mock_unembed.forward = lambda x: th.matmul(x, W.T)\n    mock_unembed.unembedding_hash.return_value = 42\n    return mock_unembed\n", "\n\n@pytest.fixture\ndef logit_lens(unembed):\n    logit_lens = LogitLens(unembed)\n    return logit_lens\n\n\n@pytest.fixture\ndef tuned_lens_config():\n    return TunedLensConfig(\n        base_model_name_or_path=\"test-model\",\n        d_model=128,\n        num_hidden_layers=3,\n        bias=True,\n    )", "@pytest.fixture\ndef tuned_lens_config():\n    return TunedLensConfig(\n        base_model_name_or_path=\"test-model\",\n        d_model=128,\n        num_hidden_layers=3,\n        bias=True,\n    )\n\n", "\n\n@pytest.fixture\ndef random_tuned_lens(tuned_lens_config, unembed):\n    tuned_lens = TunedLens(\n        unembed,\n        tuned_lens_config,\n    )\n    return tuned_lens\n", "\n\ndef test_logit_lens_smoke(logit_lens):\n    randn = th.randn(1, 10, 128)\n    logit_lens(randn, 0)\n\n\ndef test_tuned_lens_from_model(random_small_model: trf.PreTrainedModel):\n    tuned_lens = TunedLens.from_model(random_small_model)\n    assert tuned_lens.config.d_model == random_small_model.config.hidden_size", "\n\ndef test_tuned_lens_forward(random_tuned_lens: TunedLens):\n    randn = th.randn(1, 10, 128)\n    logits_forward = random_tuned_lens.forward(randn, 0)\n    logits = random_tuned_lens.unembed.forward(randn + random_tuned_lens[0](randn))\n    assert th.allclose(logits_forward, logits)\n\n\ndef test_tuned_lens_save_and_load(\n    unembed: Unembed, random_tuned_lens: TunedLens, tmp_path: Path\n):\n    randn = th.randn(1, 10, 128)\n\n    logits_before = random_tuned_lens(randn, 1)\n    random_tuned_lens.save(tmp_path)\n    reloaded_tuned_lens = TunedLens.from_unembed_and_pretrained(\n        lens_resource_id=tmp_path, unembed=unembed\n    )\n    logits_after = reloaded_tuned_lens(randn, 1)\n    assert th.allclose(logits_before, logits_after)", "\ndef test_tuned_lens_save_and_load(\n    unembed: Unembed, random_tuned_lens: TunedLens, tmp_path: Path\n):\n    randn = th.randn(1, 10, 128)\n\n    logits_before = random_tuned_lens(randn, 1)\n    random_tuned_lens.save(tmp_path)\n    reloaded_tuned_lens = TunedLens.from_unembed_and_pretrained(\n        lens_resource_id=tmp_path, unembed=unembed\n    )\n    logits_after = reloaded_tuned_lens(randn, 1)\n    assert th.allclose(logits_before, logits_after)", "\n\ndef test_from_model_and_pretrained_propogates_kwargs(\n    random_tuned_lens: TunedLens, unembed: Unembed, tmp_path: Path\n):\n    random_tuned_lens.save(tmp_path)\n\n    with mock.patch(\n        \"tuned_lens.load_artifacts.load_lens_artifacts\",\n        mock.MagicMock(\n            load_lens_artifacts,\n            return_value=(tmp_path / \"config.json\", tmp_path / \"params.pt\"),\n        ),\n    ) as mock_load_lens_artifacts:\n        mock_load_lens_artifacts.__code__.co_varnames = (\n            \"resource_id\",\n            \"unembed\",\n            \"revision\",\n        )\n        TunedLens.from_unembed_and_pretrained(\n            lens_resource_id=\"does not use\", unembed=unembed, revision=\"foo\"\n        )\n        assert mock_load_lens_artifacts.call_args.kwargs[\"revision\"] == \"foo\"\n\n        with pytest.raises(TypeError):\n            # Should not just be able to pass any kwarg\n            TunedLens.from_unembed_and_pretrained(\n                lens_resource_id=\"does not use\",\n                unembed=unembed,\n                revision=\"foo\",\n                bad_kwarg=\"bar\",\n            )\n\n        with pytest.raises(TypeError):\n            # Should not be able to specify both resource_id and and lens_resource_id\n            TunedLens.from_unembed_and_pretrained(\n                lens_resource_id=\"does not use\", unembed=unembed, resource_id=\"bar\"\n            )", ""]}
{"filename": "tests/test_utils.py", "chunked_list": ["import numpy as np\n\nfrom tuned_lens.utils import tensor_hash\n\n\ndef test_tensor_hash():\n    random = np.random.default_rng(42)\n    a = random.normal(size=(10, 1000)).astype(np.float32)\n    b = random.normal(size=(10, 1000)).astype(np.float32)\n    assert tensor_hash(a) != tensor_hash(b)\n    assert tensor_hash(a) == tensor_hash(a)\n    assert tensor_hash(a) == tensor_hash(a.astype(np.float16))", ""]}
{"filename": "tests/test_stats.py", "chunked_list": ["import random\n\nimport torch as th\nfrom torch.distributions import Dirichlet, kl_divergence\n\nfrom tuned_lens.stats import LogitStats\n\n\ndef test_logit_stats_correctness():\n    \"\"\"Test that `LogitStats` recovers the true Dirichlet within a small error.\"\"\"\n    th.manual_seed(42)\n\n    x = Dirichlet(th.tensor([1.0, 1.0, 1.0]))\n    logits1 = x.sample(th.Size([10000])).log() + random.uniform(-0.1, 0.1)\n    logits2 = x.sample(th.Size([10000])).log() + random.uniform(-0.1, 0.1)\n\n    stats = LogitStats()\n    stats.update(logits1)\n    stats.update(logits2)\n    x2 = stats.mle()\n\n    assert kl_divergence(x, x2) < 1e-3", "def test_logit_stats_correctness():\n    \"\"\"Test that `LogitStats` recovers the true Dirichlet within a small error.\"\"\"\n    th.manual_seed(42)\n\n    x = Dirichlet(th.tensor([1.0, 1.0, 1.0]))\n    logits1 = x.sample(th.Size([10000])).log() + random.uniform(-0.1, 0.1)\n    logits2 = x.sample(th.Size([10000])).log() + random.uniform(-0.1, 0.1)\n\n    stats = LogitStats()\n    stats.update(logits1)\n    stats.update(logits2)\n    x2 = stats.mle()\n\n    assert kl_divergence(x, x2) < 1e-3", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["from pathlib import Path\n\nimport pytest\nimport torch as th\nimport transformers as tr\nfrom datasets import Dataset\n\n\n@pytest.fixture(scope=\"module\")\ndef text_dataset_path() -> Path:\n    dir_path = Path(__file__).parent.absolute()\n    return Path(dir_path, \"test_data\", \"pile_text.jsonl\")", "@pytest.fixture(scope=\"module\")\ndef text_dataset_path() -> Path:\n    dir_path = Path(__file__).parent.absolute()\n    return Path(dir_path, \"test_data\", \"pile_text.jsonl\")\n\n\n@pytest.fixture(scope=\"module\")\ndef text_dataset(text_dataset_path: Path) -> Dataset:\n    dataset = Dataset.from_json(str(text_dataset_path))\n    assert isinstance(dataset, Dataset)\n    return dataset", "\n\n@pytest.fixture(\n    scope=\"module\",\n    params=[\n        \"EleutherAI/pythia-70m-deduped\",\n        \"bigscience/bloom-560m\",\n        \"EleutherAI/gpt-neo-125M\",\n        \"facebook/opt-125m\",\n        \"mockmodel/llama-tiny\",", "        \"facebook/opt-125m\",\n        \"mockmodel/llama-tiny\",\n        \"gpt2\",\n    ],\n)\ndef random_small_model(request: str) -> tr.PreTrainedModel:\n    small_model_name = request.param\n    th.manual_seed(42)\n\n    # We use a random model with the correct config instead of downloading the\n    # whole pretrained checkpoint.\n    if small_model_name == \"mockmodel/llama-tiny\":\n        config = tr.LlamaConfig(\n            vocab_size=32_000,\n            hidden_size=128,\n            num_hidden_layers=4,\n            num_attention_heads=4,\n        )\n    else:\n        config = tr.AutoConfig.from_pretrained(small_model_name)\n\n    model = tr.AutoModelForCausalLM.from_config(config)\n    model.eval()\n\n    return model", "\n\n@pytest.fixture(\n    scope=\"module\",\n    params=[\n        \"EleutherAI/pythia-70m-deduped\",\n        \"bigscience/bloom-560m\",\n        \"EleutherAI/gpt-neo-125M\",\n        \"facebook/opt-125m\",\n        \"gpt2\",", "        \"facebook/opt-125m\",\n        \"gpt2\",\n    ],\n)\ndef small_model_tokenizer(request: str) -> tr.PreTrainedTokenizerBase:\n    return tr.AutoTokenizer.from_pretrained(request.param, use_fast=True)\n\n\n@pytest.fixture(scope=\"module\")\ndef gpt2_tokenizer():\n    return tr.AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)", "@pytest.fixture(scope=\"module\")\ndef gpt2_tokenizer():\n    return tr.AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n\n\n@pytest.fixture(scope=\"module\")\ndef opt_random_model() -> tr.PreTrainedModel:\n    config = tr.AutoConfig.from_pretrained(\"facebook/opt-125m\")\n    model = tr.AutoModelForCausalLM.from_config(config)\n    model.eval()\n    return model", "\n\n@pytest.fixture(scope=\"module\")\ndef gpt2_random_model_local_path(\n    tmpdir_factory, gpt2_tokenizer: tr.PreTrainedTokenizerBase\n):\n    config = tr.AutoConfig.from_pretrained(\"gpt2\")\n    model = tr.AutoModelForCausalLM.from_config(config)\n    assert isinstance(model, tr.PreTrainedModel)\n    tmp_path = tmpdir_factory.mktemp(\"gpt2_random_model_local\")\n    model.save_pretrained(tmp_path)\n    gpt2_tokenizer.save_pretrained(tmp_path)\n    return tmp_path", ""]}
{"filename": "tests/plotting/test_prediction_trajectory.py", "chunked_list": ["import numpy as np\nimport pytest\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom tuned_lens.nn.lenses import LogitLens\nfrom tuned_lens.plotting import PredictionTrajectory\n\n\n@pytest.fixture\ndef prediction_trajectory_no_tokenizer():\n    layers = 3\n    num_tokens = 10\n    vocab_size = 12\n    return PredictionTrajectory(\n        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n    )", "@pytest.fixture\ndef prediction_trajectory_no_tokenizer():\n    layers = 3\n    num_tokens = 10\n    vocab_size = 12\n    return PredictionTrajectory(\n        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n    )", "\n\n@pytest.fixture\ndef prediction_other__trajectory_no_tokenizer():\n    layers = 3\n    num_tokens = 10\n    vocab_size = 12\n    return PredictionTrajectory(\n        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n    )", "\n\n@pytest.fixture\ndef prediction_trajectory_with_tokenizer():\n    layers = 3\n    num_tokens = 10\n    vocab_size = 12\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    return PredictionTrajectory(\n        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n        tokenizer=tokenizer,\n    )", "\n\n@pytest.fixture\ndef model_and_tokenizer():\n    model_name = \"gpt2\"\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    return model, tokenizer\n\n", "\n\n@pytest.fixture\ndef lens(model_and_tokenizer):\n    model, _ = model_and_tokenizer\n    return LogitLens.from_model(model)\n\n\ndef test_prediction_trajectory_from_lens_and_model_smoke(model_and_tokenizer, lens):\n    model, tokenizer = model_and_tokenizer\n    input_ids = tokenizer.encode(\"Hello world!\")\n    traj = PredictionTrajectory.from_lens_and_model(\n        lens, model, input_ids, tokenizer=tokenizer\n    )\n    assert traj.num_layers == model.config.n_layer\n    assert traj.num_tokens == len(input_ids)\n    assert traj.vocab_size == model.config.vocab_size", "def test_prediction_trajectory_from_lens_and_model_smoke(model_and_tokenizer, lens):\n    model, tokenizer = model_and_tokenizer\n    input_ids = tokenizer.encode(\"Hello world!\")\n    traj = PredictionTrajectory.from_lens_and_model(\n        lens, model, input_ids, tokenizer=tokenizer\n    )\n    assert traj.num_layers == model.config.n_layer\n    assert traj.num_tokens == len(input_ids)\n    assert traj.vocab_size == model.config.vocab_size\n", "\n\ndef test_largest_prob_labels_smoke(prediction_trajectory_with_tokenizer):\n    labels = prediction_trajectory_with_tokenizer.largest_prob_labels(\n        min_prob=0.1, topk=5\n    )\n    assert labels.label_strings.shape == (3, 10)\n    assert labels.sequence_labels.shape == (10,)\n    assert labels.hover_over_entries.shape == (3, 10, 5)\n", "\n\ndef test_largest_delta_in_prob_labels_smoke(prediction_trajectory_with_tokenizer):\n    layers = 3\n    num_tokens = 10\n    vocab_size = 12\n    other = PredictionTrajectory(\n        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n        tokenizer=prediction_trajectory_with_tokenizer.tokenizer,\n    )\n    labels = prediction_trajectory_with_tokenizer.largest_delta_in_prob_labels(\n        other, min_prob_delta=0.1, topk=5\n    )\n    assert labels.label_strings.shape == (3, 10)\n    assert labels.sequence_labels.shape == (10,)\n    assert labels.hover_over_entries.shape == (3, 10, 5)", "\n\ndef test_cross_entropy_smoke(prediction_trajectory_no_tokenizer):\n    traj = prediction_trajectory_no_tokenizer\n    ce_stat = traj.cross_entropy()\n\n    assert ce_stat.name == \"Cross Entropy\"\n    assert ce_stat.units == \"nats\"\n    assert ce_stat.labels is None\n    assert ce_stat.stats.shape == (3, 10)", "\n\ndef test_entropy_smoke(prediction_trajectory_no_tokenizer):\n    traj = prediction_trajectory_no_tokenizer\n    entropy_stat = traj.entropy()\n\n    assert entropy_stat.name == \"Entropy\"\n    assert entropy_stat.units == \"nats\"\n    assert entropy_stat.labels is None\n    assert entropy_stat.stats.shape == (3, 10)", "\n\ndef test_forward_kl_smoke(prediction_trajectory_no_tokenizer):\n    traj = prediction_trajectory_no_tokenizer\n    forward_kl_stat = traj.forward_kl()\n\n    assert forward_kl_stat.name == \"Forward KL\"\n    assert forward_kl_stat.units == \"nats\"\n    assert forward_kl_stat.labels is None\n    assert forward_kl_stat.stats.shape == (3, 10)", "\n\ndef test_max_probability_smoke(prediction_trajectory_no_tokenizer):\n    traj = prediction_trajectory_no_tokenizer\n    max_probability_stat = traj.max_probability()\n\n    assert max_probability_stat.name == \"Max Probability\"\n    assert max_probability_stat.units == \"probs\"\n    assert max_probability_stat.labels is None\n    assert max_probability_stat.stats.shape == (3, 10)", "\n\ndef test_kl_divergence_smoke(\n    prediction_trajectory_no_tokenizer, prediction_other__trajectory_no_tokenizer\n):\n    traj = prediction_trajectory_no_tokenizer\n    other = prediction_other__trajectory_no_tokenizer\n\n    kl_stat = traj.kl_divergence(other)\n    assert kl_stat.name == \"KL(Self | Other)\"\n    assert kl_stat.units == \"nats\"\n    assert kl_stat.labels is None\n    assert kl_stat.stats.shape == (3, 10)\n\n    assert np.isclose(kl_stat.stats, 0.0).all()", "\n\ndef test_js_divergence_smoke(\n    prediction_trajectory_no_tokenizer, prediction_other__trajectory_no_tokenizer\n):\n    traj = prediction_trajectory_no_tokenizer\n    other = prediction_other__trajectory_no_tokenizer\n\n    js_stat = traj.js_divergence(other)\n    assert js_stat.name == \"JS(Self | Other)\"\n    assert js_stat.units == \"nats\"\n    assert js_stat.labels is None\n    assert js_stat.stats.shape == (3, 10)\n\n    assert np.isclose(js_stat.stats, 0.0).all()", "\n\ndef test_total_variation_smoke(\n    prediction_trajectory_no_tokenizer, prediction_other__trajectory_no_tokenizer\n):\n    traj = prediction_trajectory_no_tokenizer\n    other = prediction_other__trajectory_no_tokenizer\n\n    js_stat = traj.total_variation(other)\n    assert js_stat.name == \"TV(Self | Other)\"\n    assert js_stat.units == \"probs\"\n    assert js_stat.labels is None\n    assert js_stat.stats.shape == (3, 10)\n\n    assert np.isclose(js_stat.stats, 0.0).all()", ""]}
{"filename": "tests/plotting/test_trajectory_plotting.py", "chunked_list": ["import numpy as np\nimport pytest\nfrom plotly import graph_objects as go\n\nfrom tuned_lens.plotting.trajectory_plotting import (\n    TrajectoryLabels,\n    TrajectoryStatistic,\n    _stride_keep_last,\n)\n", ")\n\n\ndef test_stride_keep_last():\n    x = np.array([1, 2, 3, 4, 5])\n\n    assert np.array_equal(_stride_keep_last(x, 1), x)\n    assert np.array_equal(_stride_keep_last(x, 2), np.array([1, 3, 5]))\n    assert np.array_equal(_stride_keep_last(x, 3), np.array([1, 4, 5]))\n    assert np.array_equal(_stride_keep_last(x, 4), np.array([1, 5]))\n    assert np.array_equal(_stride_keep_last(x, 5), np.array([1, 5]))", "\n\ndef test_trajectory_statistic_post_init():\n    stats = np.zeros((2, 2), dtype=float)\n    labels = TrajectoryLabels(\n        label_strings=np.zeros((2, 2), dtype=np.str_),\n        sequence_labels=np.zeros(2, dtype=np.str_),\n    )\n\n    with pytest.raises(AssertionError):\n        TrajectoryStatistic(\"test\", np.zeros((2, 3), dtype=float), labels)\n\n    stats = np.zeros((3, 3), dtype=float)\n    labels = TrajectoryLabels(\n        label_strings=np.zeros((3, 3), dtype=np.str_),\n        sequence_labels=np.zeros(3, dtype=np.str_),\n    )\n\n    ts = TrajectoryStatistic(\"test\", stats, labels)\n    assert ts is not None", "\n\ndef test_trajectory_statistic_num_layers():\n    stats = np.zeros((2, 2), dtype=float)\n    ts = TrajectoryStatistic(\"test\", stats)\n    assert ts.num_layers == 2\n\n    stats = np.zeros((3, 3), dtype=float)\n    ts = TrajectoryStatistic(\"test\", stats)\n    assert ts.num_layers == 3", "\n\ndef test_trajectory_statistic_heatmap():\n    stats = np.zeros((2, 2), dtype=float)\n    ts = TrajectoryStatistic(\"test\", stats)\n    heatmap = ts.heatmap()\n    assert isinstance(heatmap, go.Heatmap)\n\n\ndef test_trajectory_statistic_figure():\n    stats = np.zeros((2, 2), dtype=float)\n    ts = TrajectoryStatistic(\"test\", stats)\n    figure = ts.figure()\n    assert isinstance(figure, go.Figure)", "\ndef test_trajectory_statistic_figure():\n    stats = np.zeros((2, 2), dtype=float)\n    ts = TrajectoryStatistic(\"test\", stats)\n    figure = ts.figure()\n    assert isinstance(figure, go.Figure)\n"]}
{"filename": "tests/plotting/test_token_formatter.py", "chunked_list": ["import pytest\n\nfrom tuned_lens.plotting import TokenFormatter\n\n\n@pytest.fixture\ndef formatter():\n    return TokenFormatter()\n\n\ndef test_format_non_string(formatter):\n    assert formatter.format(None) == \"<unk>\"\n    assert formatter.format(123) == \"<unk>\"", "\n\ndef test_format_non_string(formatter):\n    assert formatter.format(None) == \"<unk>\"\n    assert formatter.format(123) == \"<unk>\"\n\n\ndef test_format_ellipsis(formatter):\n    token = \"ThisIsALongToken\"\n    expected = \"ThisIs\u2026\"\n    assert formatter.format(token) == expected", "\n\ndef test_format_no_ellipsis(formatter):\n    formatter.max_string_len = None\n    token = \"ThisIsALongToken\"\n    expected = \"ThisIsALongToken\"\n    assert formatter.format(token) == expected\n\n\ndef test_format_newline_token_replacement(formatter):\n    formatter.max_string_len = None\n    token = \"Hello\u010aWorld\"\n    expected = \"Hello\\\\nWorld\"\n    assert formatter.format(token) == expected", "\ndef test_format_newline_token_replacement(formatter):\n    formatter.max_string_len = None\n    token = \"Hello\u010aWorld\"\n    expected = \"Hello\\\\nWorld\"\n    assert formatter.format(token) == expected\n\n\ndef test_format_whitespace_token_replacement(formatter):\n    formatter.max_string_len = None\n    token = \"Hello\u0120World\"\n    expected = \"Hello_World\"\n    assert formatter.format(token) == expected", "def test_format_whitespace_token_replacement(formatter):\n    formatter.max_string_len = None\n    token = \"Hello\u0120World\"\n    expected = \"Hello_World\"\n    assert formatter.format(token) == expected\n\n\ndef test_format_multiple_replacements(formatter):\n    formatter.max_string_len = None\n    token = \"Line1\u010aLine2\u0120Line3\"\n    expected = \"Line1\\\\nLine2_Line3\"\n    assert formatter.format(token) == expected", ""]}
{"filename": "tests/scripts/test_integration.py", "chunked_list": ["from pathlib import Path\n\nfrom tuned_lens.__main__ import main\n\n\ndef test_eval_subcommand(\n    text_dataset_path: Path, gpt2_random_model_local_path: Path, tmp_path: Path\n):\n    # Note we do not specify a lens here, so we are using the logit lens\n    args = (\n        f\"eval --data.name {text_dataset_path}\"\n        f\" --model.name {gpt2_random_model_local_path}\"\n        \" --limit 20 --max_length 128\"\n        f\" --output {tmp_path}\"\n    )\n    args = args.split()\n    main(args)", "\n\ndef test_train_subcommand(\n    text_dataset_path: Path, gpt2_random_model_local_path: Path, tmp_path: Path\n):\n    args = (\n        f\"train --data.name {text_dataset_path}\"\n        f\" --model.name {gpt2_random_model_local_path}\"\n        \" --max_length 128\"\n        f\" --output {tmp_path}\"\n    )\n    args = args.split()\n    main(args)", ""]}
{"filename": "tests/scripts/__init__.py", "chunked_list": [""]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nfrom importlib import metadata\n", "from importlib import metadata\n\nproject = \"tuned-lens\"\ncopyright = \"2023, FAR AI\"\nhtml_title = \"Tuned Lens\"\nhtml_favicon = (\n    \"data:image/svg+xml,\"\n    \"<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22>\"\n    \"<text y=%22.9em%22 font-size=%2290%22>\ud83d\udd0e</text>\"\n    \"</svg>\"", "    \"<text y=%22.9em%22 font-size=%2290%22>\ud83d\udd0e</text>\"\n    \"</svg>\"\n)\nauthor = (\n    \"Nora Belrose\"\n    \" Zach Furman,\"\n    \" Logan Smith,\"\n    \" Danny Halawi,\"\n    \" Lev McKinney,\"\n    \" Igor Ostrovsky,\"", "    \" Lev McKinney,\"\n    \" Igor Ostrovsky,\"\n    \" Stella Biderman,\"\n    \" Jacob Steinhardt\"\n)\nrelease = metadata.version(\"tuned_lens\")\n\nextensions = [\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.autodoc\",", "    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.autosectionlabel\",\n    \"sphinx_autodoc_typehints\",\n    \"sphinx.ext.doctest\",\n    \"myst_parser\",\n    \"nbsphinx\",\n]\n", "]\n\nnapoleon_google_docstring = True\nnapoleon_use_param = False\nnapoleon_use_ivar = True\n\ntemplates_path = [\"_templates\"]\nexclude_patterns = [\"build\", \"Thumbs.db\", \".DS_Store\", \"**.ipynb_checkpoints\"]\n\n", "\n\nhtml_theme = \"furo\"\nhtml_static_path = [\"_static\"]\nhtml_theme_options = {\n    \"source_repository\": \"https://github.com/AlignmentResearch/tuned-lens\",\n    \"source_branch\": \"main\",\n    \"source_directory\": \"docs/source\",\n    \"light_css_variables\": {\n        \"sidebar-item-font-size\": \"85%\",", "    \"light_css_variables\": {\n        \"sidebar-item-font-size\": \"85%\",\n    },\n}\n"]}
{"filename": "tuned_lens/__main__.py", "chunked_list": ["\"\"\"Script to train or evaluate a set of tuned lenses for a language model.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\n\nfrom simple_parsing import ArgumentParser, ConflictResolution\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\nfrom .scripts.eval_loop import Eval\nfrom .scripts.train_loop import Train\n", "from .scripts.train_loop import Train\n\n\n@dataclass\nclass Main:\n    \"\"\"Routes to the subcommands.\"\"\"\n\n    command: Union[Train, Eval]\n\n    def execute(self):\n        \"\"\"Run the script.\"\"\"\n        self.command.execute()", "\n\n@record\ndef main(args: Optional[list[str]] = None):\n    \"\"\"Entry point for the CLI.\"\"\"\n    parser = ArgumentParser(conflict_resolution=ConflictResolution.EXPLICIT)\n    parser.add_arguments(Main, dest=\"prog\")\n    args = parser.parse_args(args=args)\n    prog: Main = args.prog\n    prog.execute()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "tuned_lens/load_artifacts.py", "chunked_list": ["\"\"\"Load lens artifacts from the hub or locally storage.\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom huggingface_hub import hf_hub_download\n\n\ndef load_lens_artifacts(\n    resource_id: str,\n    repo_id: Optional[str] = None,\n    repo_type: Optional[str] = None,\n    revision: Optional[str] = None,\n    config_file: str = \"config.json\",\n    ckpt_file: str = \"params.pt\",\n    subfolder: str = \"lens\",\n    cache_dir: Optional[str] = None,\n) -> tuple[Path, Path]:\n    \"\"\"First checks for lens resource locally then tries to download it from the hub.\n\n    Args:\n        resource_id: The id of the lens resource.\n        repo_id: The repository to download the lens from. Defaults to\n            'AlignmentResearch/tuned-lens'. However, this default can be overridden by\n            setting the TUNED_LENS_REPO_ID environment variable.\n        repo_type: The type of repository to download the lens from. Defaults to\n            'space'. However, this default can be overridden by setting the\n            TUNED_LENS_REPO_TYPE environment variable.\n        config_file: The name of the config file in the folder contain the lens.\n        ckpt_file: The name of the checkpoint file in the folder contain the lens.\n        revision: The revision of the lens to download.\n        subfolder: The subfolder of the repository to download the lens from.\n        cache_dir: The directory to cache the lens in.\n\n    Returns:\n        * The path to the config.json file\n        * The path to the params.pt file\n\n    Raises:\n        ValueError: if the lens resource could not be found.\n    \"\"\"\n    if repo_id is None:\n        if os.environ.get(\"TUNED_LENS_REPO_ID\"):\n            repo_id = os.environ[\"TUNED_LENS_REPO_ID\"]\n        else:\n            repo_id = \"AlignmentResearch/tuned-lens\"\n\n    if repo_type is None:\n        if os.environ.get(\"TUNED_LENS_REPO_TYPE\"):\n            repo_type = os.environ[\"TUNED_LENS_REPO_TYPE\"]\n        else:\n            repo_type = \"space\"\n\n    # Fist check if the resource id is a path to a folder that exists\n    local_path = Path(resource_id)\n    if (local_path / config_file).exists() and (local_path / ckpt_file).exists():\n        return local_path / config_file, local_path / ckpt_file\n\n    subfolder = \"/\".join((subfolder, resource_id))\n    params_path = hf_hub_download(\n        filename=ckpt_file,\n        repo_id=repo_id,\n        repo_type=repo_type,\n        revision=revision,\n        subfolder=subfolder,\n        cache_dir=cache_dir,\n    )\n\n    config_path = hf_hub_download(\n        filename=config_file,\n        repo_id=repo_id,\n        repo_type=repo_type,\n        revision=revision,\n        subfolder=subfolder,\n        cache_dir=cache_dir,\n    )\n\n    if config_path is not None and params_path is not None:\n        return Path(config_path), Path(params_path)\n\n    raise ValueError(\"Could not find lens resource locally or on the hf hub.\")", "def load_lens_artifacts(\n    resource_id: str,\n    repo_id: Optional[str] = None,\n    repo_type: Optional[str] = None,\n    revision: Optional[str] = None,\n    config_file: str = \"config.json\",\n    ckpt_file: str = \"params.pt\",\n    subfolder: str = \"lens\",\n    cache_dir: Optional[str] = None,\n) -> tuple[Path, Path]:\n    \"\"\"First checks for lens resource locally then tries to download it from the hub.\n\n    Args:\n        resource_id: The id of the lens resource.\n        repo_id: The repository to download the lens from. Defaults to\n            'AlignmentResearch/tuned-lens'. However, this default can be overridden by\n            setting the TUNED_LENS_REPO_ID environment variable.\n        repo_type: The type of repository to download the lens from. Defaults to\n            'space'. However, this default can be overridden by setting the\n            TUNED_LENS_REPO_TYPE environment variable.\n        config_file: The name of the config file in the folder contain the lens.\n        ckpt_file: The name of the checkpoint file in the folder contain the lens.\n        revision: The revision of the lens to download.\n        subfolder: The subfolder of the repository to download the lens from.\n        cache_dir: The directory to cache the lens in.\n\n    Returns:\n        * The path to the config.json file\n        * The path to the params.pt file\n\n    Raises:\n        ValueError: if the lens resource could not be found.\n    \"\"\"\n    if repo_id is None:\n        if os.environ.get(\"TUNED_LENS_REPO_ID\"):\n            repo_id = os.environ[\"TUNED_LENS_REPO_ID\"]\n        else:\n            repo_id = \"AlignmentResearch/tuned-lens\"\n\n    if repo_type is None:\n        if os.environ.get(\"TUNED_LENS_REPO_TYPE\"):\n            repo_type = os.environ[\"TUNED_LENS_REPO_TYPE\"]\n        else:\n            repo_type = \"space\"\n\n    # Fist check if the resource id is a path to a folder that exists\n    local_path = Path(resource_id)\n    if (local_path / config_file).exists() and (local_path / ckpt_file).exists():\n        return local_path / config_file, local_path / ckpt_file\n\n    subfolder = \"/\".join((subfolder, resource_id))\n    params_path = hf_hub_download(\n        filename=ckpt_file,\n        repo_id=repo_id,\n        repo_type=repo_type,\n        revision=revision,\n        subfolder=subfolder,\n        cache_dir=cache_dir,\n    )\n\n    config_path = hf_hub_download(\n        filename=config_file,\n        repo_id=repo_id,\n        repo_type=repo_type,\n        revision=revision,\n        subfolder=subfolder,\n        cache_dir=cache_dir,\n    )\n\n    if config_path is not None and params_path is not None:\n        return Path(config_path), Path(params_path)\n\n    raise ValueError(\"Could not find lens resource locally or on the hf hub.\")", ""]}
{"filename": "tuned_lens/__init__.py", "chunked_list": ["\"\"\"The tuned lens package.\"\"\"\nfrom .nn import TunedLens\n"]}
{"filename": "tuned_lens/utils.py", "chunked_list": ["\"\"\"Utilities for distributed training and handling nested collections of tensors.\"\"\"\n\nimport hashlib\nfrom itertools import islice\nfrom typing import Any, Callable, Iterable, Sequence, Type, TypeVar, Union, cast\n\nimport numpy as np\nimport torch as th\nimport torch.distributed as dist\nfrom numpy.typing import NDArray", "import torch.distributed as dist\nfrom numpy.typing import NDArray\n\nT = TypeVar(\"T\")\n\n\ndef assert_type(typ: Type[T], obj: Any) -> T:\n    \"\"\"Assert that an object is of a given type at runtime and return it.\"\"\"\n    if not isinstance(obj, typ):\n        raise TypeError(f\"Expected {typ.__name__}, got {type(obj).__name__}\")\n\n    return cast(typ, obj)", "\n\ndef maybe_all_cat(x: th.Tensor) -> th.Tensor:\n    \"\"\"Concatenate a tensor across all processes.\"\"\"\n    if not dist.is_initialized():\n        return x\n\n    buffer = x.new_empty([dist.get_world_size() * x.shape[0], *x.shape[1:]])\n    dist.all_gather_into_tensor(buffer, x)\n    return buffer", "\n\ndef maybe_all_gather_lists(lst: list) -> list:\n    \"\"\"Gather a list of objects from all processes.\"\"\"\n    if not dist.is_initialized():\n        return lst\n\n    lists = [[] for _ in range(dist.get_world_size())]\n    dist.all_gather_object(lists, lst)\n    return sum(lists, [])", "\n\ndef maybe_all_reduce(x: th.Tensor, op: str = \"mean\") -> th.Tensor:\n    \"\"\"Reduce a tensor across all processes.\"\"\"\n    if not dist.is_initialized():\n        return x\n\n    if op == \"sum\":\n        dist.all_reduce(x, op=dist.ReduceOp.SUM)\n    elif op == \"mean\":\n        dist.all_reduce(x, op=dist.ReduceOp.SUM)\n        x /= dist.get_world_size()\n    else:\n        raise ValueError(f\"Unknown reduction op '{op}'\")\n\n    return x", "\n\ndef maybe_unpack(x):\n    \"\"\"Unpack a tuple if it's a tuple, otherwise return the value.\"\"\"\n    if isinstance(x, tuple):\n        x, *_ = x\n\n    return x\n\n\ndef shift_labels(x: th.Tensor, shift: int):\n    \"\"\"Shift labels by a given amount.\n\n    Args:\n        x: (batch x seq_len) labels to shift.\n        shift: Amount to shift by. Positive values take from the start, negative values\n            negative values take from the end.\n\n    Returns:\n        (batch x (seq_len - shift)) labels shifted by the given amount.\n    \"\"\"\n    if shift > 0:\n        return x[:, shift:]\n    if shift < 0:\n        return x[:, :shift]\n\n    return x", "\n\ndef shift_labels(x: th.Tensor, shift: int):\n    \"\"\"Shift labels by a given amount.\n\n    Args:\n        x: (batch x seq_len) labels to shift.\n        shift: Amount to shift by. Positive values take from the start, negative values\n            negative values take from the end.\n\n    Returns:\n        (batch x (seq_len - shift)) labels shifted by the given amount.\n    \"\"\"\n    if shift > 0:\n        return x[:, shift:]\n    if shift < 0:\n        return x[:, :shift]\n\n    return x", "\n\ndef shift_preds(x: th.Tensor, shift: int):\n    \"\"\"Shift predictions by a given amount.\n\n    Args:\n        x: (batch x seq_len) predictions to shift.\n        shift: Amount to shift by. Positive values take from the end, negative values\n            from the start.\n\n    Returns:\n        (batch x (seq_len - shift)) predictions shifted by the given amount.\n    \"\"\"\n    if shift > 0:\n        return x[:, :-shift]\n    if shift < 0:\n        return x[:, -shift:]\n\n    return x", "\n\nT = TypeVar(\"T\")\n\n\n# Backported from Python 3.10\ndef pairwise(it: Iterable[T]) -> Iterable[tuple[T, T]]:\n    \"\"\"Iterate over pairs of elements in an iterable.\"\"\"\n    yield from zip(it, islice(it, 1, None))\n", "\n\n# Define pytree type recursively- this works for Pylance but unfortunately not MyPy\nAnyTree = Union[th.Tensor, dict[Any, \"AnyTree\"], list[\"AnyTree\"], tuple[\"AnyTree\", ...]]\nTreeType = TypeVar(\"TreeType\", bound=AnyTree)\n\n\ndef pytree_flatten(tree: AnyTree) -> Iterable[th.Tensor]:\n    \"\"\"Recursively iterate over all tensors in a pytree, in topological order.\"\"\"\n    # Stopping condition\n    if isinstance(tree, th.Tensor):\n        yield tree\n\n    # Recursive case\n    elif isinstance(tree, dict):\n        for elem in tree.values():\n            yield from pytree_flatten(elem)\n\n    elif isinstance(tree, Sequence):\n        for elem in tree:\n            yield from pytree_flatten(elem)", "\n\ndef pytree_map(\n    func: Callable[[th.Tensor], Any], tree: TreeType, strict: bool = True\n) -> TreeType:\n    \"\"\"Recursively apply a function to all tensors in a pytree.\n\n    Args:\n        func: Function to apply to each tensor.\n        tree: Pytree to apply the function to.\n        strict: If True, raise an error if a non-tensor leaf is encountered.\n\n    Returns:\n        A new pytree with the same structure. Non-tensor leaves are copied.\n    \"\"\"\n    # Stopping condition\n    if isinstance(tree, th.Tensor):\n        return func(tree)\n\n    # Recursive case\n    if isinstance(tree, dict):\n        return {k: pytree_map(func, v) for k, v in tree.items()}\n\n    if isinstance(tree, list):\n        return [pytree_map(func, v) for v in tree]\n\n    if isinstance(tree, tuple):\n        return tuple(pytree_map(func, v) for v in tree)\n\n    if strict:\n        raise TypeError(\n            f\"Found leaf '{tree}' of unsupported type '{type(tree).__name__}'- use \"\n            f\"`strict=False` to ignore\"\n        )\n    else:\n        return tree", "\n\ndef pytree_cat(trees: Sequence[AnyTree], dim: int = 0) -> AnyTree:\n    \"\"\"Concatenate pytrees along a given dimension.\n\n    All pytrees are expected to use the same collection; undefined behavior\n    will occur if this is not the case.\n\n    Args:\n        trees: Sequence of pytrees containing tensors to concatenate.\n        dim: Dimension to concatenate along.\n\n    Returns:\n        - A new pytree with the same structure.\n    \"\"\"\n    transposed_iter = zip(*(pytree_flatten(tree) for tree in trees))\n    leaf_iter = (th.cat(seq, dim) for seq in transposed_iter)\n    try:\n        return pytree_map(lambda _: next(leaf_iter), trees[0])  # type: ignore\n    except (RuntimeError, StopIteration) as e:\n        # Calling next() on an exhausted generator raises a RuntimeError, annoyingly\n        if isinstance(e, StopIteration) or \"StopIteration\" in str(e):\n            raise TypeError(\"All pytrees must have the same structure\") from e\n        else:\n            raise", "\n\ndef pytree_stack(trees: Sequence, dim: int = 0) -> AnyTree:\n    \"\"\"Stack pytrees along a given dimension.\n\n    All pytrees are expected to use the same collection; undefined behavior\n    will occur if this is not the case.\n\n    Args:\n        trees: Sequence of pytrees containing tensors to stack.\n        dim: Dimension to concatenate along.\n\n    Returns:\n        A new pytree with the same structure.\n    \"\"\"\n    if not len(trees):\n        raise ValueError(\"Cannot stack empty sequence of pytrees\")\n\n    transposed_iter = zip(*(pytree_flatten(tree) for tree in trees))\n    leaf_iter = (th.stack(seq, dim) for seq in transposed_iter)\n    try:\n        return pytree_map(lambda _: next(leaf_iter), trees[0])  # type: ignore\n    except (RuntimeError, StopIteration) as e:\n        # Calling next() on an exhausted generator raises a RuntimeError, annoyingly\n        if isinstance(e, StopIteration) or \"StopIteration\" in str(e):\n            raise TypeError(\"All pytrees must have the same structure\") from e\n        else:\n            raise", "\n\ndef revcumsum(x: Sequence[th.Tensor]) -> list[th.Tensor]:\n    \"\"\"Reverse cumulative sum of a sequence of tensors.\"\"\"\n    if not len(x):\n        return []\n\n    running_total = th.zeros_like(x[0])\n    sums = [running_total.add_(r).clone() for r in reversed(x)]\n    sums.reverse()\n    return sums", "\n\ndef send_to_device(tree: TreeType, device: th.device) -> TreeType:\n    \"\"\"Recursively send all tensors in a pytree to a device.\"\"\"\n    return pytree_map(lambda t: t.to(device), tree)\n\n\ndef tensor_hash(tensor: NDArray) -> str:\n    \"\"\"Fast hash of a matrix that is robust to dtype and small perturbations.\n\n    Note this relies on the ordering of the elements in the matrix, so it is\n    if the matrix is in any way sorted this will not work well. In addition,\n    this hash is intended for large tensors 64 + elements.\n    \"\"\"\n    return hashlib.sha256(str.encode(np.array_str(tensor, precision=1))).hexdigest()", ""]}
{"filename": "tuned_lens/data.py", "chunked_list": ["\"\"\"Tools for tokenizing and manipulating text datasets.\"\"\"\nimport math\nfrom multiprocessing import cpu_count\nfrom typing import TypeVar, Union\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import PreTrainedTokenizerBase\n\nT = TypeVar(\"T\", bound=Union[Dataset, DatasetDict])\n", "T = TypeVar(\"T\", bound=Union[Dataset, DatasetDict])\n\n\ndef chunk_and_tokenize(\n    data: T,\n    tokenizer: PreTrainedTokenizerBase,\n    *,\n    format: str = \"torch\",\n    num_proc: int = min(cpu_count() // 2, 16),\n    text_key: str = \"text\",\n    max_length: int = 2048,\n    return_final_batch: bool = False,\n    load_from_cache_file: bool = True,\n) -> tuple[T, float]:\n    \"\"\"Perform GPT-style chunking and tokenization on a dataset.\n\n    The resulting dataset will consist entirely of chunks exactly `max_length` tokens\n    long. Long sequences will be split into multiple chunks, and short sequences will\n    be merged with their neighbors, using `eos_token` as a separator. The fist token\n    will also always be an `eos_token`.\n\n    Args:\n        data: The dataset to chunk and tokenize.\n        tokenizer: The tokenizer to use.\n        format: The format to return the dataset in, passed to `Dataset.with_format`.\n        num_proc: The number of processes to use for tokenization.\n        text_key: The key in the dataset to use as the text to tokenize.\n        max_length: The maximum length of a batch of input ids.\n        return_final_batch: Whether to return the final batch, which may be smaller\n            than the others.\n        load_from_cache_file: Whether to load from the cache file.\n\n    Returns:\n        * The chunked and tokenized dataset.\n        * The ratio of nats to bits per byte see https://arxiv.org/pdf/2101.00027.pdf,\n            section 3.1.\n    \"\"\"\n\n    def _tokenize_fn(x: dict[str, list]):\n        chunk_size = min(tokenizer.model_max_length, max_length)\n        sep = tokenizer.eos_token or \"<|endoftext|>\"\n        joined_text = sep.join([\"\"] + x[text_key])\n        output = tokenizer(\n            # Concatenate all the samples together, separated by the EOS token.\n            joined_text,  # start with an eos token\n            max_length=chunk_size,\n            return_attention_mask=False,\n            return_overflowing_tokens=True,\n            truncation=True,\n        )\n\n        total_tokens = sum(len(ids) for ids in output[\"input_ids\"])\n        total_bytes = len(joined_text.encode(\"utf-8\"))\n\n        assert (\n            \"overflowing_tokens\" not in output\n        ), \"We should not have any overflowing tokens.\"\n\n        if not return_final_batch:\n            # We know that the last sample will almost always be less than the max\n            # number of tokens, and we don't want to pad, so we just drop it.\n            output = {k: v[:-1] for k, v in output.items()}\n\n        output_batch_size = len(output[\"input_ids\"])\n\n        if output_batch_size == 0:\n            raise ValueError(\n                \"Not enough data to create a single batch complete batch.\"\n                \" Either allow the final batch to be returned,\"\n                \" or supply more data.\"\n            )\n\n        # We need to output this in order to compute the number of bits per byte\n        div, rem = divmod(total_tokens, output_batch_size)\n        output[\"length\"] = [div] * output_batch_size\n        output[\"length\"][-1] += rem\n\n        div, rem = divmod(total_bytes, output_batch_size)\n        output[\"bytes\"] = [div] * output_batch_size\n        output[\"bytes\"][-1] += rem\n\n        return output\n\n    data = data.map(\n        _tokenize_fn,\n        # Batching is important for ensuring that we don't waste tokens\n        # since we always throw away the last element of the batch we\n        # want to keep the batch size as large as possible\n        batched=True,\n        batch_size=2048,\n        num_proc=num_proc,\n        remove_columns=get_columns_all_equal(data),\n        load_from_cache_file=load_from_cache_file,\n    )\n    total_bytes: float = sum(data[\"bytes\"])\n    total_tokens: float = sum(data[\"length\"])\n    return data.with_format(format, columns=[\"input_ids\"]), (\n        total_tokens / total_bytes\n    ) / math.log(2)", "\n\ndef get_columns_all_equal(dataset: Union[Dataset, DatasetDict]) -> list[str]:\n    \"\"\"Get a single list of columns in a `Dataset` or `DatasetDict`.\n\n    We assert the columms are the same across splits if it's a `DatasetDict`.\n\n    Args:\n        dataset: The dataset to get the columns from.\n\n    Returns:\n        A list of columns.\n    \"\"\"\n    if isinstance(dataset, DatasetDict):\n        cols_by_split = dataset.column_names.values()\n        columns = next(iter(cols_by_split))\n        if not all(cols == columns for cols in cols_by_split):\n            raise ValueError(\"All splits must have the same columns\")\n\n        return columns\n\n    return dataset.column_names", ""]}
{"filename": "tuned_lens/model_surgery.py", "chunked_list": ["\"\"\"Tools for finding and modifying components in a transformer model.\"\"\"\n\nfrom contextlib import contextmanager\nfrom typing import Any, Generator, TypeVar, Union\n\nimport torch as th\nimport transformers as tr\nfrom transformers import models\n\n\ndef get_value_for_key(obj: Any, key: str) -> Any:\n    \"\"\"Get a value using `__getitem__` if `key` is numeric and `getattr` otherwise.\"\"\"\n    return obj[int(key)] if key.isdigit() else getattr(obj, key)", "\n\ndef get_value_for_key(obj: Any, key: str) -> Any:\n    \"\"\"Get a value using `__getitem__` if `key` is numeric and `getattr` otherwise.\"\"\"\n    return obj[int(key)] if key.isdigit() else getattr(obj, key)\n\n\ndef set_value_for_key_(obj: Any, key: str, value: Any) -> None:\n    \"\"\"Set value in-place if `key` is numeric and `getattr` otherwise.\"\"\"\n    if key.isdigit():\n        obj[int(key)] = value\n    else:\n        setattr(obj, key, value)", "\n\ndef get_key_path(model: th.nn.Module, key_path: str) -> Any:\n    \"\"\"Get a value by key path, e.g. `layers.0.attention.query.weight`.\"\"\"\n    for key in key_path.split(\".\"):\n        model = get_value_for_key(model, key)\n\n    return model\n\n\ndef set_key_path_(\n    model: th.nn.Module, key_path: str, value: Union[th.nn.Module, th.Tensor]\n) -> None:\n    \"\"\"Set a value by key path in-place, e.g. `layers.0.attention.query.weight`.\"\"\"\n    keys = key_path.split(\".\")\n    for key in keys[:-1]:\n        model = get_value_for_key(model, key)\n\n    setattr(model, keys[-1], value)", "\n\ndef set_key_path_(\n    model: th.nn.Module, key_path: str, value: Union[th.nn.Module, th.Tensor]\n) -> None:\n    \"\"\"Set a value by key path in-place, e.g. `layers.0.attention.query.weight`.\"\"\"\n    keys = key_path.split(\".\")\n    for key in keys[:-1]:\n        model = get_value_for_key(model, key)\n\n    setattr(model, keys[-1], value)", "\n\nT = TypeVar(\"T\", bound=th.nn.Module)\n\n\n@contextmanager\ndef assign_key_path(model: T, key_path: str, value: Any) -> Generator[T, None, None]:\n    \"\"\"Temporarily set a value by key path while in the context.\"\"\"\n    old_value = get_key_path(model, key_path)\n    set_key_path_(model, key_path, value)\n    try:\n        yield model\n    finally:\n        set_key_path_(model, key_path, old_value)", "\n\nNorm = Union[th.nn.LayerNorm, models.llama.modeling_llama.LlamaRMSNorm]\n\n\ndef get_final_norm(model: tr.PreTrainedModel) -> Norm:\n    \"\"\"Get the final norm from a model.\n\n    This isn't standardized across models, so this will need to be updated as\n    we add new models.\n    \"\"\"\n    if not hasattr(model, \"base_model\"):\n        raise ValueError(\"Model does not have a `base_model` attribute.\")\n\n    base_model = model.base_model\n    if isinstance(base_model, models.opt.modeling_opt.OPTModel):\n        final_layer_norm = base_model.decoder.final_layer_norm\n    elif isinstance(base_model, models.gpt_neox.modeling_gpt_neox.GPTNeoXModel):\n        final_layer_norm = base_model.final_layer_norm\n    elif isinstance(\n        base_model,\n        (\n            models.bloom.modeling_bloom.BloomModel,\n            models.gpt2.modeling_gpt2.GPT2Model,\n            models.gpt_neo.modeling_gpt_neo.GPTNeoModel,\n            models.gptj.modeling_gptj.GPTJModel,\n        ),\n    ):\n        final_layer_norm = base_model.ln_f\n    elif isinstance(base_model, models.llama.modeling_llama.LlamaModel):\n        final_layer_norm = base_model.norm\n    else:\n        raise NotImplementedError(f\"Unknown model type {type(base_model)}\")\n\n    if final_layer_norm is None:\n        raise ValueError(\"Model does not have a final layer norm.\")\n\n    assert isinstance(final_layer_norm, Norm.__args__)  # type: ignore\n\n    return final_layer_norm", "\n\ndef get_transformer_layers(model: tr.PreTrainedModel) -> tuple[str, th.nn.ModuleList]:\n    \"\"\"Get the decoder layers from a model.\n\n    Args:\n        model: The model to search.\n\n    Returns:\n        A tuple containing the key path to the layer list and the list itself.\n\n    Raises:\n        ValueError: If no such list exists.\n    \"\"\"\n    if not hasattr(model, \"base_model\"):\n        raise ValueError(\"Model does not have a `base_model` attribute.\")\n\n    path_to_layers = [\"base_model\"]\n    base_model = model.base_model\n    if isinstance(base_model, models.opt.modeling_opt.OPTModel):\n        path_to_layers += [\"decoder\", \"layers\"]\n    elif isinstance(base_model, models.gpt_neox.modeling_gpt_neox.GPTNeoXModel):\n        path_to_layers += [\"layers\"]\n    elif isinstance(\n        base_model,\n        (\n            models.bloom.modeling_bloom.BloomModel,\n            models.gpt2.modeling_gpt2.GPT2Model,\n            models.gpt_neo.modeling_gpt_neo.GPTNeoModel,\n            models.gptj.modeling_gptj.GPTJModel,\n        ),\n    ):\n        path_to_layers += [\"h\"]\n    elif isinstance(base_model, models.llama.modeling_llama.LlamaModel):\n        path_to_layers += [\"layers\"]\n    else:\n        raise NotImplementedError(f\"Unknown model type {type(base_model)}\")\n\n    path_to_layers = \".\".join(path_to_layers)\n    return path_to_layers, get_key_path(model, path_to_layers)", "\n\n@contextmanager\ndef delete_layers(model: T, indices: list[int]) -> Generator[T, None, None]:\n    \"\"\"Temporarily delete the layers at `indices` from `model` while in the context.\"\"\"\n    list_path, layer_list = get_transformer_layers(model)\n    modified_list = th.nn.ModuleList(layer_list)\n    for i in sorted(indices, reverse=True):\n        del modified_list[i]\n\n    set_key_path_(model, list_path, modified_list)\n    try:\n        yield model\n    finally:\n        set_key_path_(model, list_path, layer_list)", "\n\n@contextmanager\ndef permute_layers(model: T, indices: list[int]) -> Generator[T, None, None]:\n    \"\"\"Temporarily permute the layers of `model` by `indices` while in the context.\n\n    The number of indices provided may be not be equal to the number of\n    layers in the model. Layers will be dropped or duplicated accordingly.\n    \"\"\"\n    list_path, layer_list = get_transformer_layers(model)\n    permuted_list = th.nn.ModuleList([layer_list[i] for i in indices])\n    set_key_path_(model, list_path, permuted_list)\n\n    try:\n        yield model\n    finally:\n        set_key_path_(model, list_path, layer_list)", "\n\ndef permute_layers_(model: th.nn.Module, indices: list[int]):\n    \"\"\"Permute the layers of `model` by `indices` in-place.\n\n    The number of indices provided may be not be equal to the number of\n    layers in the model. Layers will be dropped or duplicated accordingly.\n    \"\"\"\n    list_path, layer_list = get_transformer_layers(model)\n    permuted_list = th.nn.ModuleList([layer_list[i] for i in indices])\n    set_key_path_(model, list_path, permuted_list)", "\n\n@contextmanager\ndef replace_layers(\n    model: T, indices: list[int], replacements: list[th.nn.Module]\n) -> Generator[T, None, None]:\n    \"\"\"Replace the layers at `indices` with `replacements` while in the context.\"\"\"\n    list_path, layer_list = get_transformer_layers(model)\n    modified_list = th.nn.ModuleList(layer_list)\n    for i, replacement in zip(indices, replacements):\n        modified_list[i] = replacement\n\n    set_key_path_(model, list_path, modified_list)\n    try:\n        yield model\n    finally:\n        set_key_path_(model, list_path, layer_list)", ""]}
{"filename": "tuned_lens/plotting/trajectory_plotting.py", "chunked_list": ["\"\"\"Contains utility classes for creating heatmap visualizations.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional\n\nimport numpy as np\nfrom numpy.typing import NDArray\nfrom plotly import graph_objects as go\n\n\n@dataclass\nclass TrajectoryLabels:\n    \"\"\"Contains sets of labels for each layer and position in the residual stream.\"\"\"\n\n    # (n_layers x sequence_length) label for each layer and position in the stream.\n    label_strings: NDArray[np.str_]\n    # (sequence_length) labels for the sequence dimension typically the input tokens.\n    sequence_labels: NDArray[np.str_]\n    # (n_layers x sequence_length x k) k entries to display when hovering over a cell.\n    # For example, the top k prediction from the lens at each layer.\n    hover_over_entries: Optional[NDArray[np.str_]] = None", "\n@dataclass\nclass TrajectoryLabels:\n    \"\"\"Contains sets of labels for each layer and position in the residual stream.\"\"\"\n\n    # (n_layers x sequence_length) label for each layer and position in the stream.\n    label_strings: NDArray[np.str_]\n    # (sequence_length) labels for the sequence dimension typically the input tokens.\n    sequence_labels: NDArray[np.str_]\n    # (n_layers x sequence_length x k) k entries to display when hovering over a cell.\n    # For example, the top k prediction from the lens at each layer.\n    hover_over_entries: Optional[NDArray[np.str_]] = None", "\n\n@dataclass\nclass TrajectoryStatistic:\n    \"\"\"This class represents a trajectory statistic that can be visualized.\n\n    For example, the entropy of the lens predictions at each layer.\n    \"\"\"\n\n    # The name of the statistic.\n    name: str\n    # (n_layers x sequence_length) value of the statistic at each layer and position.\n    stats: NDArray[np.float32]\n    # labels for each layer and position in the stream. For example, the top 1\n    # prediction from the lens at each layer.\n    labels: Optional[TrajectoryLabels] = None\n    # The units of the statistic.\n    units: Optional[str] = None\n    # The maximum value of the statistic.\n    max: Optional[float] = None\n    # The minimum value of the statistic.\n    min: Optional[float] = None\n\n    def __post_init__(self) -> None:\n        \"\"\"Validate class invariants.\"\"\"\n        assert len(self.stats.shape) == 2\n        assert self.labels is None or (\n            self.labels.label_strings.shape == self.stats.shape\n            and self.labels.sequence_labels.shape[0] == self.stats.shape[1]\n        )\n\n    @property\n    def num_layers(self) -> int:\n        \"\"\"Return the number of layers in the stream.\"\"\"\n        return self.stats.shape[0]\n\n    def heatmap(\n        self,\n        layer_stride: int = 1,\n        colorscale: str = \"rdbu_r\",\n        **kwargs,\n    ) -> go.Heatmap:\n        \"\"\"Returns a Plotly Heatmap object for this statistic.\n\n        Args:\n            layer_stride : The number of layers between each layer plotted.\n            colorscale : The colorscale to use for the heatmap.\n            **kwargs : Additional keyword arguments to pass to the Heatmap constructor.\n\n        Returns:\n            A plotly Heatmap where the x-axis is the sequence dimension, the y-axis is\n            the layer dimension, and the color of each cell is the value of\n            the statistic.\n        \"\"\"\n        labels = np.array([\"input\", *map(str, range(1, self.num_layers - 1)), \"output\"])\n\n        color_matrix = self.stats\n\n        color_matrix = _stride_keep_last(color_matrix, layer_stride)\n        labels = _stride_keep_last(labels, layer_stride)\n\n        heatmap_kwargs: Dict[str, Any] = dict(\n            y=labels,\n            z=color_matrix,\n            colorbar=dict(\n                title=f\"{self.name} ({self.units})\",\n                titleside=\"right\",\n            ),\n            zmax=self.max,\n            zmin=self.min,\n        )\n\n        if self.labels is not None:\n            label_strings = self.labels.label_strings\n            label_strings = _stride_keep_last(label_strings, layer_stride)\n            # Hack to ensure that Plotly doesn't de-duplicate the x-axis labels\n            x_labels = [\n                x + \"\\u200c\" * i for i, x in enumerate(self.labels.sequence_labels)\n            ]\n\n            heatmap_kwargs.update(\n                colorscale=colorscale,\n                text=label_strings,\n                texttemplate=\"<b>%{text}</b>\",\n                x=x_labels,\n            )\n\n            if self.labels.hover_over_entries is not None:\n                hover_over_entries = _stride_keep_last(\n                    self.labels.hover_over_entries, layer_stride\n                )\n                heatmap_kwargs.update(\n                    customdata=hover_over_entries,\n                    hoverlabel=dict(bgcolor=\"rgb(42, 42, 50)\"),\n                    hovertemplate=\"<br>\".join(\n                        f\" %{{customdata[{i}]}}\"\n                        for i in range(hover_over_entries.shape[2])\n                    )\n                    + \"<extra></extra>\",\n                )\n\n        heatmap_kwargs.update(kwargs)\n        return go.Heatmap(**heatmap_kwargs)\n\n    def figure(\n        self,\n        title: str = \"\",\n        layer_stride: int = 1,\n        colorscale: str = \"rdbu_r\",\n        token_width: int = 80,\n    ) -> go.Figure:\n        \"\"\"Produce a heatmap plot of the statistic.\n\n        Args:\n            title : The title of the plot.\n            layer_stride : The number of layers between each layer we plot.\n            colorscale : The colorscale to use for the heatmap.\n            token_width : The width of each token in the plot.\n\n        Returns:\n            The plotly heatmap figure.\n        \"\"\"\n        heatmap = self.heatmap(layer_stride, colorscale)\n        figure_width = 200 + token_width * self.stats.shape[1]\n\n        fig = go.Figure(heatmap).update_layout(\n            title_text=title,\n            title_x=0.5,\n            width=figure_width,\n            xaxis_title=\"Input\",\n            yaxis_title=\"Layer\",\n        )\n        return fig", "\n\ndef _stride_keep_last(x: NDArray, stride: int):\n    return np.concatenate([x[:-1:stride], [x[-1]]])\n"]}
{"filename": "tuned_lens/plotting/__init__.py", "chunked_list": ["\"\"\"Provides tools for plotting.\"\"\"\nfrom .prediction_trajectory import PredictionTrajectory\nfrom .token_formatter import TokenFormatter\nfrom .trajectory_plotting import TrajectoryLabels, TrajectoryStatistic\n"]}
{"filename": "tuned_lens/plotting/prediction_trajectory.py", "chunked_list": ["\"\"\"Plot a lens table for some given text and model.\"\"\"\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Optional, Sequence, Union\n\nimport numpy as np\nimport torch as th\nfrom numpy.typing import NDArray\nfrom transformers import (", "from numpy.typing import NDArray\nfrom transformers import (\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    PreTrainedTokenizerFast,\n)\n\nfrom ..nn.lenses import Lens\nfrom .token_formatter import TokenFormatter\nfrom .trajectory_plotting import TrajectoryLabels, TrajectoryStatistic", "from .token_formatter import TokenFormatter\nfrom .trajectory_plotting import TrajectoryLabels, TrajectoryStatistic\n\nTokenizer = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n\n\n@dataclass\nclass PredictionTrajectory:\n    \"\"\"Contains the trajectory predictions for a sequence of tokens.\n\n    A prediction trajectory is the set of next token predictions produced by the\n    conjunction of a lens and a model when evaluated on a specific sequence of tokens.\n    This class include multiple methods for visualizing different\n    aspects of the trajectory.\n    \"\"\"\n\n    # The log probabilities of the predictions for each hidden layer + the models logits\n    # Shape: (num_layers, seq_len, vocab_size)\n    log_probs: NDArray[np.float32]\n    input_ids: NDArray[np.int64]\n    targets: Optional[NDArray[np.int64]] = None\n    tokenizer: Optional[Tokenizer] = None\n\n    def __post_init__(self) -> None:\n        \"\"\"Validate class invariants.\"\"\"\n        assert len(self.log_probs.shape) == 3, \"log_probs.shape: {}\".format(\n            self.log_probs.shape\n        )\n        assert (\n            self.log_probs.shape[1] == self.input_ids.shape[0]\n        ), \"log_probs.shape: {}, input_ids.shape: {}\".format(\n            self.log_probs.shape, self.input_ids.shape\n        )\n        assert (\n            self.targets is None or self.targets.shape[0] == self.input_ids.shape[0]\n        ), \"targets.shape: {}, input_ids.shape: {}\".format(\n            self.targets.shape, self.input_ids.shape\n        )\n\n    @property\n    def num_layers(self) -> int:\n        \"\"\"Returns the number of layers in the stream.\"\"\"\n        return self.log_probs.shape[0] - 1\n\n    @property\n    def num_tokens(self) -> int:\n        \"\"\"Returns the number of tokens in this slice of the sequence.\"\"\"\n        return self.log_probs.shape[1]\n\n    @property\n    def vocab_size(self) -> int:\n        \"\"\"Returns the size of the vocabulary.\"\"\"\n        return self.log_probs.shape[2]\n\n    @property\n    def model_log_probs(self) -> NDArray[np.float32]:\n        \"\"\"Returns the log probs of the model.\"\"\"\n        return self.log_probs[-1, ...]\n\n    @property\n    def probs(self) -> NDArray[np.float32]:\n        \"\"\"Returns the probabilities of the predictions.\"\"\"\n        return np.exp(self.log_probs)\n\n    @classmethod\n    def from_lens_and_model(\n        cls,\n        lens: Lens,\n        model: PreTrainedModel,\n        input_ids: Sequence[int],\n        tokenizer: Optional[Tokenizer] = None,\n        targets: Optional[Sequence[int]] = None,\n        start_pos: int = 0,\n        end_pos: Optional[int] = None,\n        mask_input: bool = False,\n    ) -> \"PredictionTrajectory\":\n        \"\"\"Constructs a slice of the model's prediction trajectory.\n\n        Args:\n            lens : The lens to use for constructing the latent predictions.\n            model : The model to get the predictions from.\n            tokenizer : The tokenizer to use for decoding the predictions.\n            input_ids : The input ids to pass to the model.\n            targets : The targets for the input sequence.\n            start_pos : The start position of the slice across the sequence dimension.\n            end_pos : The end position of the slice accross the sequence dimension.\n            mask_input : whether to forbid the lens from predicting the input tokens.\n\n        Returns:\n            A PredictionTrajectory object containing the requested slice.\n        \"\"\"\n        with th.no_grad():\n            input_ids_th = th.tensor(input_ids, dtype=th.int64, device=model.device)\n            outputs = model(input_ids_th.unsqueeze(0), output_hidden_states=True)\n\n        # Slice arrays the specified range\n        model_log_probs = (\n            outputs.logits[..., start_pos:end_pos, :]\n            .log_softmax(-1)\n            .squeeze()\n            .detach()\n            .cpu()\n            .numpy()\n        )\n        stream = [h[..., start_pos:end_pos, :] for h in outputs.hidden_states]\n\n        input_ids_np = np.array(input_ids[start_pos:end_pos])\n        targets_np = (\n            np.array(targets[start_pos:end_pos]) if targets is not None else None\n        )\n\n        # Create the stream of log probabilities from the lens\n        traj_log_probs = []\n        for i, h in enumerate(stream[:-1]):\n            logits = lens.forward(h, i)\n\n            if mask_input:\n                logits[..., input_ids_np] = -th.finfo(h.dtype).max\n\n            traj_log_probs.append(\n                logits.log_softmax(dim=-1).squeeze().detach().cpu().numpy()\n            )\n\n        # Add model predictions\n        if traj_log_probs[-1].shape[-1] != model_log_probs.shape[-1]:\n            logging.warning(\n                \"Lens vocab size does not match model vocab size.\"\n                \"Truncating model outputs to match lens vocab size.\"\n            )\n        # Handle the case where the model has more/less tokens than the lens\n        min_logit = -np.finfo(model_log_probs.dtype).max\n        trunc_model_log_probs = np.full_like(traj_log_probs[-1], min_logit)\n        trunc_model_log_probs[..., : model_log_probs.shape[-1]] = model_log_probs\n\n        traj_log_probs.append(trunc_model_log_probs)\n\n        return cls(\n            tokenizer=tokenizer,\n            log_probs=np.array(traj_log_probs),\n            targets=targets_np,\n            input_ids=input_ids_np,\n        )\n\n    def _get_topk_tokens_and_values(\n        self,\n        k: int,\n        sort_by: NDArray[np.float32],\n        values: NDArray[np.float32],\n    ) -> NDArray[np.str_]:\n\n        # Get the top-k tokens & probabilities for each\n        topk_inds = np.argpartition(sort_by, -k, axis=-1)[..., -k:]\n        topk_sort_by = np.take_along_axis(sort_by, topk_inds, axis=-1)\n        topk_values = np.take_along_axis(values, topk_inds, axis=-1)\n\n        # Ensure that the top-k tokens are sorted by probability\n        sorted_top_k_inds = np.argsort(-topk_sort_by, axis=-1)\n        topk_inds = np.take_along_axis(topk_inds, sorted_top_k_inds, axis=-1)\n        topk_values = np.take_along_axis(topk_values, sorted_top_k_inds, axis=-1)\n\n        # reshape topk_ind from (layers, seq, k) to (layers*seq*k),\n        # convert_ids_to_tokens, then reshape back to (layers, seq, k)\n        topk_tokens = self.tokenizer.convert_ids_to_tokens(topk_inds.flatten().tolist())\n        topk_tokens = np.array(topk_tokens).reshape(topk_inds.shape)\n\n        return topk_tokens, topk_values\n\n    def largest_prob_labels(\n        self,\n        formatter: Optional[TokenFormatter] = None,\n        min_prob: np.float_ = np.finfo(np.float32).eps,\n        topk: int = 10,\n    ) -> TrajectoryLabels:\n        \"\"\"Labels for the prediction trajectory based on the most probable tokens.\n\n        Args:\n            formatter : The formatter to use for formatting the tokens.\n            min_prob : The minimum probability for a token to used as a label.\n            topk : The number of top tokens to include in the hover over menu.\n\n        Raises:\n            ValueError: If the tokenizer is not set.\n\n        Returns:\n            a set of stream labels that can be applied to a trajectory statistic.\n        \"\"\"\n        if self.tokenizer is None:\n            raise ValueError(\"Tokenizer must be set to get labels.\")\n\n        if formatter is None:\n            formatter = TokenFormatter()\n\n        input_tokens = self.tokenizer.convert_ids_to_tokens(self.input_ids.tolist())\n\n        entry_format_fn = np.vectorize(\n            lambda token, percent: f\"{formatter.format(token)} {percent:.2f}%\"\n        )\n\n        topk_tokens, topk_probs = self._get_topk_tokens_and_values(\n            k=topk, sort_by=self.log_probs, values=self.probs\n        )\n\n        top_tokens = topk_tokens[..., 0]\n        top_probs = topk_probs[..., 0]\n\n        label_strings = np.where(\n            top_probs > min_prob, formatter.vectorized_format(top_tokens), \"\"\n        )\n\n        return TrajectoryLabels(\n            label_strings=label_strings,\n            sequence_labels=formatter.vectorized_format(input_tokens),\n            hover_over_entries=entry_format_fn(topk_tokens, topk_probs * 100),\n        )\n\n    def largest_delta_in_prob_labels(\n        self,\n        other: \"PredictionTrajectory\",\n        formatter: Optional[TokenFormatter] = None,\n        min_prob_delta: np.float_ = np.finfo(np.float32).eps,\n        topk: int = 10,\n    ) -> TrajectoryLabels:\n        \"\"\"Labels for a trajectory statistic based on the largest change in probability.\n\n        Args:\n            other : The other prediction trajectory to compare to.\n            formatter : A TokenFormatter to use for formatting the labels.\n            min_prob_delta : The minimum change in probability to include a label.\n            topk : The number of top tokens to include in the hover over menu.\n\n        Raises:\n            ValueError: If the tokenizer is not set.\n\n        Returns:\n            A set of stream labels that can be added to a trajectory statistic.\n        \"\"\"\n        if self.tokenizer is None:\n            raise ValueError(\"Tokenizer must be set to get labels.\")\n\n        if formatter is None:\n            formatter = TokenFormatter()\n\n        input_tokens = self.tokenizer.convert_ids_to_tokens(self.input_ids.tolist())\n\n        entry_format_fn = np.vectorize(\n            lambda token, percent: f\"{formatter.format(token)} \u0394{percent:.2f}%\"\n        )\n\n        deltas = other.probs - self.probs\n\n        topk_tokens, topk_deltas = self._get_topk_tokens_and_values(\n            k=topk, sort_by=np.abs(deltas), values=deltas\n        )\n\n        top_tokens = topk_tokens[..., 0]\n        top_deltas = topk_deltas[..., 0]\n\n        label_strings = np.where(\n            np.abs(top_deltas) > min_prob_delta,\n            formatter.vectorized_format(top_tokens),\n            \"\",\n        )\n\n        return TrajectoryLabels(\n            label_strings=label_strings,\n            sequence_labels=formatter.vectorized_format(input_tokens),\n            hover_over_entries=entry_format_fn(topk_tokens, 100 * topk_deltas),\n        )\n\n    def cross_entropy(self, **kwargs) -> TrajectoryStatistic:\n        \"\"\"The cross entropy of the predictions to the targets.\n\n        Args:\n            **kwargs: are passed to largest_prob_labels.\n\n        Returns:\n            A TrajectoryStatistic with the cross entropy of the predictions to the\n            targets.\n        \"\"\"\n        if self.targets is None:\n            raise ValueError(\"Cannot compute cross entropy without targets.\")\n\n        assert self.targets.shape == self.log_probs[-1].shape[:-1], (\n            \"Batch and sequence lengths of targets and log probs must match.\"\n            f\"Got {self.targets.shape} and {self.log_probs[-1].shape[:-1]}.\"\n        )\n\n        return TrajectoryStatistic(\n            name=\"Cross Entropy\",\n            units=\"nats\",\n            labels=self.largest_prob_labels(**kwargs) if self.tokenizer else None,\n            stats=-self.log_probs[:, np.arange(self.num_tokens), self.targets],\n        )\n\n    def entropy(self, **kwargs) -> TrajectoryStatistic:\n        \"\"\"The entropy of the predictions.\n\n        Args:\n            **kwargs: are passed to largest_prob_labels.\n\n        Returns:\n            A TrajectoryStatistic with the entropy of the predictions.\n        \"\"\"\n        return TrajectoryStatistic(\n            name=\"Entropy\",\n            units=\"nats\",\n            labels=self.largest_prob_labels(**kwargs) if self.tokenizer else None,\n            stats=-np.sum(np.exp(self.log_probs) * self.log_probs, axis=-1),\n        )\n\n    def forward_kl(self, **kwargs) -> TrajectoryStatistic:\n        \"\"\"KL divergence of the lens predictions to the model predictions.\n\n        Args:\n            **kwargs: are passed to largest_prob_labels.\n\n        Returns:\n            A TrajectoryStatistic with the KL divergence of the lens predictions to the\n            final output of the model.\n        \"\"\"\n        model_log_probs = self.model_log_probs.reshape(\n            1, self.num_tokens, self.vocab_size\n        )\n        return TrajectoryStatistic(\n            name=\"Forward KL\",\n            units=\"nats\",\n            labels=self.largest_prob_labels(**kwargs) if self.tokenizer else None,\n            stats=np.sum(\n                np.exp(model_log_probs) * (model_log_probs - self.log_probs), axis=-1\n            ),\n        )\n\n    def max_probability(self, **kwargs) -> TrajectoryStatistic:\n        \"\"\"Max probability of the among the predictions.\n\n        Args:\n            **kwargs: are passed to largest_prob_labels.\n\n        Returns:\n            A TrajectoryStatistic with the max probability of the among the predictions.\n        \"\"\"\n        return TrajectoryStatistic(\n            name=\"Max Probability\",\n            units=\"probs\",\n            labels=self.largest_prob_labels(**kwargs) if self.tokenizer else None,\n            stats=np.exp(self.log_probs.max(-1)),\n        )\n\n    def kl_divergence(\n        self, other: \"PredictionTrajectory\", **kwargs\n    ) -> TrajectoryStatistic:\n        \"\"\"Compute the KL divergence between self and other prediction trajectory.\n\n        Args:\n            other : The other prediction trajectory to compare to.\n            **kwargs: are passed to largest_delta_in_prob_labels.\n\n        Returns:\n            A TrajectoryStatistic with the KL divergence between self and other.\n        \"\"\"\n        kl_div = np.sum(self.probs * (self.log_probs - other.log_probs), axis=-1)\n\n        return TrajectoryStatistic(\n            name=\"KL(Self | Other)\",\n            units=\"nats\",\n            stats=kl_div,\n            labels=self.largest_delta_in_prob_labels(other, **kwargs)\n            if self.tokenizer\n            else None,\n            min=0,\n            max=None,\n        )\n\n    def js_divergence(\n        self, other: \"PredictionTrajectory\", **kwargs\n    ) -> TrajectoryStatistic:\n        \"\"\"Compute the JS divergence between self and other prediction trajectory.\n\n        Args:\n            other : The other prediction trajectory to compare to.\n            **kwargs: are passed to largest_delta_in_prob_labels.\n\n        Returns:\n            A TrajectoryStatistic with the JS divergence between self and other.\n        \"\"\"\n        js_div = 0.5 * np.sum(\n            self.probs * (self.log_probs - other.log_probs), axis=-1\n        ) + 0.5 * np.sum(other.probs * (other.log_probs - self.log_probs), axis=-1)\n\n        return TrajectoryStatistic(\n            name=\"JS(Self | Other)\",\n            units=\"nats\",\n            stats=js_div,\n            labels=self.largest_delta_in_prob_labels(other, **kwargs)\n            if self.tokenizer\n            else None,\n            min=0,\n            max=None,\n        )\n\n    def total_variation(\n        self, other: \"PredictionTrajectory\", **kwargs\n    ) -> TrajectoryStatistic:\n        \"\"\"Total variation distance between self and other prediction trajectory.\n\n        Args:\n            other : The other prediction trajectory to compare to.\n            **kwargs: are passed to largest_delta_in_prob_labels.\n\n        Returns:\n            A TrajectoryStatistic with the total variational distance between\n            self and other.\n        \"\"\"\n        t_var = np.abs(self.probs - other.probs).max(axis=-1)\n\n        return TrajectoryStatistic(\n            name=\"TV(Self | Other)\",\n            units=\"probs\",\n            stats=t_var,\n            labels=self.largest_delta_in_prob_labels(other, **kwargs)\n            if self.tokenizer\n            else None,\n            min=0,\n            max=1,\n        )", ""]}
{"filename": "tuned_lens/plotting/token_formatter.py", "chunked_list": ["\"\"\"Contains a class for formatting tokens for display in plots.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport numpy as np\n\n\n@dataclass\nclass TokenFormatter:\n    \"\"\"Format tokens for display in a plots.\"\"\"\n\n    ellipsis: str = \"\u2026\"\n    newline_replacement: str = \"\\\\n\"\n    newline_token: str = \"\u010a\"\n    whitespace_token: str = \"\u0120\"\n    whitespace_replacement: str = \"_\"\n    max_string_len: Optional[int] = 7\n\n    def __post_init__(self) -> None:\n        \"\"\"Post init hook to vectorize the format function.\"\"\"\n        self.vectorized_format = np.vectorize(self.format)\n\n    def format(self, token: str) -> str:\n        \"\"\"Format a token for display in a plot.\"\"\"\n        if not isinstance(token, str):\n            return \"<unk>\"\n\n        if self.max_string_len is not None and len(token) > self.max_string_len:\n            token = token[: self.max_string_len - len(self.ellipsis)] + self.ellipsis\n        token = token.replace(self.newline_token, self.newline_replacement)\n        token = token.replace(self.whitespace_token, self.whitespace_replacement)\n        return token", "class TokenFormatter:\n    \"\"\"Format tokens for display in a plots.\"\"\"\n\n    ellipsis: str = \"\u2026\"\n    newline_replacement: str = \"\\\\n\"\n    newline_token: str = \"\u010a\"\n    whitespace_token: str = \"\u0120\"\n    whitespace_replacement: str = \"_\"\n    max_string_len: Optional[int] = 7\n\n    def __post_init__(self) -> None:\n        \"\"\"Post init hook to vectorize the format function.\"\"\"\n        self.vectorized_format = np.vectorize(self.format)\n\n    def format(self, token: str) -> str:\n        \"\"\"Format a token for display in a plot.\"\"\"\n        if not isinstance(token, str):\n            return \"<unk>\"\n\n        if self.max_string_len is not None and len(token) > self.max_string_len:\n            token = token[: self.max_string_len - len(self.ellipsis)] + self.ellipsis\n        token = token.replace(self.newline_token, self.newline_replacement)\n        token = token.replace(self.whitespace_token, self.whitespace_replacement)\n        return token", ""]}
{"filename": "tuned_lens/causal/subspaces.py", "chunked_list": ["\"\"\"Provides tools for extracting causal bases from models and ablating subspaces.\"\"\"\nfrom contextlib import contextmanager\nfrom typing import Iterable, Literal, NamedTuple, Optional, Sequence\n\nimport torch as th\nimport torch.distributed as dist\nimport torch.nn.functional as F\nfrom tqdm.auto import trange\n\nfrom ..model_surgery import get_transformer_layers", "\nfrom ..model_surgery import get_transformer_layers\nfrom ..nn import Lens\nfrom ..utils import maybe_all_reduce\nfrom .utils import derange\n\n\n@contextmanager\ndef ablate_subspace(\n    model: th.nn.Module,\n    A: th.Tensor,\n    layer_index: int,\n    mode: Literal[\"mean\", \"resample\", \"zero\"] = \"zero\",\n    orthonormal: bool = False,\n):\n    \"\"\"Context manager that ablates a subspace of activations.\n\n    Args:\n        model: A hugging face transformer model.\n        A: Either a 2D matrix whose column space is to be removed, or a 1D vector whose\n            span is to be removed.\n        layer_index: The index of the layer to ablate.\n        mode: Which method to use for removing information along the subspace.\n            Defaults to `\"zero\"`.\n        orthonormal: if True, `A` is assumed to be orthonormal.\n    \"\"\"\n    _, layers = get_transformer_layers(model)\n\n    def wrapper(_, __, outputs):\n        h, *extras = outputs\n        h_ = remove_subspace(h, A, mode, orthonormal)\n\n        return h_, *extras\n\n    handle = layers[layer_index].register_forward_hook(wrapper)  # type: ignore\n    try:\n        yield model\n    finally:\n        handle.remove()", "def ablate_subspace(\n    model: th.nn.Module,\n    A: th.Tensor,\n    layer_index: int,\n    mode: Literal[\"mean\", \"resample\", \"zero\"] = \"zero\",\n    orthonormal: bool = False,\n):\n    \"\"\"Context manager that ablates a subspace of activations.\n\n    Args:\n        model: A hugging face transformer model.\n        A: Either a 2D matrix whose column space is to be removed, or a 1D vector whose\n            span is to be removed.\n        layer_index: The index of the layer to ablate.\n        mode: Which method to use for removing information along the subspace.\n            Defaults to `\"zero\"`.\n        orthonormal: if True, `A` is assumed to be orthonormal.\n    \"\"\"\n    _, layers = get_transformer_layers(model)\n\n    def wrapper(_, __, outputs):\n        h, *extras = outputs\n        h_ = remove_subspace(h, A, mode, orthonormal)\n\n        return h_, *extras\n\n    handle = layers[layer_index].register_forward_hook(wrapper)  # type: ignore\n    try:\n        yield model\n    finally:\n        handle.remove()", "\n\nclass CausalBasis(NamedTuple):\n    \"\"\"An ordered orthonormal basis for a subspace of activations.\n\n    Attributes:\n        energies: A vector of shape (k,) containing the energies of the\n            basis vectors. Each energy is the expected KL divergence of\n            the post-intervention logits wrt the control logits when the\n            corresponding basis vector is ablated.\n        vectors: A matrix of shape (d, k) where d is the ambient dimension\n            and k is the dimension of the subspace. The columns of this\n            matrix are basis vectors, ordered by decreasing energy.\n    \"\"\"\n\n    energies: th.Tensor\n    vectors: th.Tensor", "\n\ndef extract_causal_bases(\n    lens: Lens,\n    hiddens: Sequence[th.Tensor],\n    k: int,\n    *,\n    labels: Optional[th.Tensor] = None,\n    max_iter: int = 100,\n    mode: Literal[\"mean\", \"resample\", \"zero\"] = \"mean\",\n) -> Iterable[CausalBasis]:\n    \"\"\"Extract causal bases for probes at each layer of a model.\n\n    Args:\n        lens: A lens to compute causal bases for.\n        hiddens: A sequence of hidden states from the model.\n        k: The number of basis vectors to compute for each layer.\n        max_iter: The maximum number of iterations to run L-BFGS for each vector.\n        mode: Which method to use for removing information along the subspace.\n            Defaults to `\"zero\"`.\n    \"\"\"\n    lens.requires_grad_(False)\n\n    device = hiddens[0].device\n    dtype = hiddens[0].dtype\n    d = hiddens[0].shape[-1]\n\n    hiddens = [h.detach() for h in hiddens]\n    num_layers = len(hiddens) - 1\n\n    assert k <= d\n    if k < 1:\n        k = d\n\n    eye = th.eye(d, device=device, dtype=dtype)\n\n    show_pbar = not dist.is_initialized() or dist.get_rank() == 0\n    pbar = trange(num_layers * k) if show_pbar else None\n\n    # Outer loop iterates over layers\n    for i in range(num_layers):\n        U = lens.unembed.unembedding.weight.data.T\n\n        logits = lens(hiddens[i], i)\n        log_p = logits.log_softmax(-1)\n        U = lens.transform_hidden(U, i)  # TODO not sure if we need transposes here\n\n        # Compute the baseline loss up front so that we can subtract it\n        # from the post-ablation losses to get the loss increment\n        if labels is not None:\n            base_loss = F.cross_entropy(\n                log_p[:, :-1].flatten(0, -2), labels[:, 1:].flatten()\n            )\n        else:\n            base_loss = 0.0\n\n        # Initialize basis vectors with left singular vectors of U\n        u, *_ = th.linalg.svd(U, full_matrices=False)\n        basis = CausalBasis(th.zeros(k, device=device), u[:, :k].float())\n\n        # Inner loop iterates over directions\n        p = log_p.exp()\n        for j in range(k):\n            if pbar:\n                pbar.set_description(f\"Layer {i + 1}/{num_layers}, vector {j + 1}/{k}\")\n\n            # Construct the operator for projecting away from the previously\n            # identified basis vectors\n            if j:\n                A = basis.vectors[:, :j]\n                proj = eye - A @ A.T\n            else:\n                proj = eye\n\n            def project(x: th.Tensor) -> th.Tensor:\n                # Project away from previously identified basis vectors\n                x = proj @ x\n\n                # Project to the unit sphere\n                return x / (x.norm() + th.finfo(x.dtype).eps)\n\n            basis.vectors[:, j] = project(basis.vectors[:, j])\n            v = th.nn.Parameter(basis.vectors[:, j])\n\n            nfev = 0\n            energy_delta = th.tensor(0.0, device=device)\n            last_energy = th.tensor(0.0, device=device)\n\n            opt = th.optim.LBFGS(\n                [v],\n                line_search_fn=\"strong_wolfe\",\n                max_iter=max_iter,\n            )\n\n            def closure():\n                nonlocal energy_delta, nfev, last_energy\n                nfev += 1\n\n                opt.zero_grad(set_to_none=False)\n                v_ = project(v)\n                h_ = remove_subspace(hiddens[i], v_, mode=mode, orthonormal=True)\n\n                logits = lens(h_, i)\n\n                if labels is not None:\n                    loss = -F.cross_entropy(\n                        logits[:, :-1].flatten(0, 1), labels[:, 1:].flatten()\n                    )\n                else:\n                    log_q = logits.log_softmax(-1)\n                    loss = -th.sum(p * (log_p - log_q), dim=-1).mean()\n\n                loss.backward()\n                maybe_all_reduce(loss)\n                maybe_all_reduce(v.grad)  # type: ignore[arg-type]\n\n                assert v.grad is not None\n                new_energy = -loss.detach() - base_loss\n                energy_delta = new_energy - last_energy\n                last_energy = new_energy\n\n                if pbar:\n                    pbar.set_postfix(energy=last_energy.item())\n\n                if not loss.isfinite():\n                    print(\"Loss is not finite\")\n                    loss = th.tensor(0.0, device=device)\n                    opt.zero_grad(set_to_none=False)\n\n                return loss\n\n            while nfev < max_iter:\n                opt.step(closure)  # type: ignore\n                v.data = project(v.data)\n\n                if abs(energy_delta / last_energy) < 1e-4:\n                    break\n\n            basis.vectors[:, j] = project(v.data)\n            basis.energies[j] = last_energy\n\n            if pbar:\n                pbar.update()\n\n        indices = basis.energies.argsort(descending=True)\n        yield CausalBasis(basis.energies[indices], basis.vectors[:, indices])", "\n\ndef remove_subspace(\n    u: th.Tensor,\n    A: th.Tensor,\n    mode: Literal[\"mean\", \"resample\", \"zero\"] = \"zero\",\n    orthonormal: bool = False,\n) -> th.Tensor:\n    \"\"\"Remove all information in `u` along the column space of `A`.\n\n    This can be done by zero, mean, or resample ablation. With zero ablation,\n    `u` is projected onto the orthogonal complement of col(`A`), so the resulting\n    vectors are orthogonal to every column in `A`. With mean ablation, `u` is projected\n    onto the subspace s.t. the angles between the resulting vectors and the columns of\n    `A` are equal to their mean values. With resample ablation, the variation in `u`\n    is shuffled across vectors.\n\n    Args:\n        u: The vectors to be projected.\n        A: Either a 2D matrix whose column space is to be removed, or a 1D vector whose\n            span is to be removed.\n        mode: Which method to use for removing information along the subspace.\n            Defaults to `\"zero\"`.\n        orthonormal: Whether to assume `A` is orthonormal. Defaults to `False`.\n\n    Returns:\n        th.Tensor: The transformed vectors.\n    \"\"\"\n    if A.ndim == 1:\n        A = A[..., None]\n\n    d, _ = A.shape\n    if u.shape[-1] != d:\n        raise ValueError(f\"Last dimension of u must be {d}, but is {u.shape[-1]}\")\n\n    # https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_special_cases\n    if orthonormal:\n        proj = A @ A.mT\n    else:\n        proj = A @ th.linalg.solve(A.mT @ A, A.mT)\n\n    if mode == \"zero\":\n        dummy = -u\n    else:\n        samples = u.flatten(0, -2)\n        N = samples.shape[0]\n        if N < 2:\n            raise ValueError(\"Need at least 2 vectors for mean and resample ablation\")\n\n        if mode == \"mean\":\n            dummy = samples.mean(0) - u\n        elif mode == \"resample\":\n            # Shuffle the rows of `samples` without fixed points.\n            dummy = derange(samples).view_as(u) - u\n        else:\n            raise ValueError(f\"Unknown mode {mode}\")\n\n    return u + th.einsum(\"ij,...j->...i\", proj, dummy)", ""]}
{"filename": "tuned_lens/causal/__init__.py", "chunked_list": ["\"\"\"Tools for finding and intervening on important subspaces of the residual stream.\"\"\"\nfrom .subspaces import (\n    CausalBasis,\n    ablate_subspace,\n    extract_causal_bases,\n    remove_subspace,\n)\nfrom .utils import derange, sample_derangement\n", ""]}
{"filename": "tuned_lens/causal/utils.py", "chunked_list": ["from typing import Optional\n\nimport torch as th\n\n\ndef derange(batch: th.Tensor, generator: Optional[th.Generator] = None) -> th.Tensor:\n    \"\"\"Shuffle a tensor along axis 0, making sure there are no fixed points.\"\"\"\n    # Things get more complicated if there are multiple ranks. We perform the\n    # derangement *hierarchically*, first generating a shared permutation of the ranks\n    indices = sample_derangement(\n        batch.shape[0], device=batch.device, generator=generator\n    )\n    return batch[indices]", "\n\ndef sample_derangement(\n    n: int,\n    device: th.device = th.device(\"cpu\"),\n    generator: Optional[th.Generator] = None,\n) -> th.Tensor:\n    \"\"\"Uniformly sample a random permutation with no fixed points.\"\"\"\n    if n < 2:\n        raise ValueError(\"Derangements only exist for n > 1\")\n\n    indices = th.arange(n, device=device)\n    permutation = th.randperm(n, device=device, generator=generator)\n\n    # Reject any permutations with fixed points. This seems inefficient,\n    # but the expected number of th.randperm calls is actually O(1); it\n    # asymptotically approaches e \u2248 2.7.\n    # See https://www.cs.upc.edu/~conrado/research/talks/analco08.pdf.\n    while th.any(permutation == indices):\n        permutation = th.randperm(n, device=device, generator=generator)\n\n    return permutation", ""]}
{"filename": "tuned_lens/causal/ablation.py", "chunked_list": ["\"\"\"Provides tools for ablating layers of a transformer model.\"\"\"\nfrom contextlib import contextmanager\nfrom typing import Literal\n\nimport torch as th\n\nfrom ..model_surgery import get_transformer_layers\nfrom .utils import derange\n\n", "\n\n@contextmanager\ndef ablate_layer(\n    model: th.nn.Module,\n    layer_index: int,\n    method: Literal[\"resample\", \"mean\", \"zero\"],\n    *,\n    mode: Literal[\"batch\", \"token\"] = \"batch\",\n):\n    \"\"\"Replace residual outputs of the specified layer with dummy values.\n\n    If the method is \"resample\", the residuals are replaced with corresponding\n    residuals from a randomly sampled sequence in the batch. If the method is \"mean\",\n    the residuals are replaced with their minibatch means. If the method is \"zero\",\n    all residuals are replaced with the zero vector.\n\n    Args:\n        model: The model to modify.\n        layer_index: The index of the layer to modify.\n        method: How to ablate the layer see above.\n        mode: Whether to compute the mean only over the batch dimension or over the\n            batch and token dimensions.\n    \"\"\"\n    assert layer_index >= 0\n\n    def ablate_hook(_, inputs, outputs):\n        x, *_ = inputs\n        y, *extras = outputs\n        if method == \"zero\":\n            return x, *extras\n\n        residuals = y - x\n        original_shape = x.shape\n        if mode == \"token\":\n            x = x.flatten(0, 1)\n            residuals = residuals.flatten(0, 1)\n\n        batch_size = x.shape[0]\n        if batch_size < 2:\n            raise ValueError(\"Mean ablation requires a batch size >= 2\")\n\n        if method == \"resample\":\n            ablated = x + derange(residuals)\n        elif method == \"mean\":\n            ablated = x + residuals.mean(0, keepdim=True)\n        else:\n            raise ValueError(f\"Unknown ablation method: {method}\")\n\n        return ablated.reshape(original_shape), *extras\n\n    _, layers = get_transformer_layers(model)\n    handle = layers[layer_index].register_forward_hook(ablate_hook)  # type: ignore\n\n    try:\n        yield model\n    finally:\n        handle.remove()", ""]}
{"filename": "tuned_lens/scripts/explanation_generation.py", "chunked_list": ["from tuned_lens.scripts.generation_utils import threaded_generations\nfrom tuned_lens.scripts.ingredients import (\n    Data,\n)\nfrom transformers import AutoTokenizer\nimport os\nfrom fire import Fire\nimport json\n\nSYSTEM_PROMPT = \"\"\"As a language model explainer, your task is to analyze a given truncated text and deduce what information is present in the activations of a language model at any layer in the final position. Do not attempt to complete the text provided by the user, and directly provide information in the following format:", "\nSYSTEM_PROMPT = \"\"\"As a language model explainer, your task is to analyze a given truncated text and deduce what information is present in the activations of a language model at any layer in the final position. Do not attempt to complete the text provided by the user, and directly provide information in the following format:\nCurrent word: <last word or punctuation of the text>\nCurrent sentence: <current sentence in which the last word is present, summarized if and only if it is more than 10 words long, and otherwise repeated at-verbatim>\nProbable next words: <comma-separated list of 5 most probable next words/punctuation>\nText style: <comma-separated list of 10 words describing the style of the text, its tone, the writer position, ...>\nParagraph summary: <summary of the paragraph, of at most 30 words>\nText summary: <summary of the text, of at most 30 words>\nProbable next sentences: <list of the 3 most likely completions of this text, *each should be exactly 8 words long*: interrupt yourself in the middle of the sentence if you exceed 8 words!! Hint: completions should porbably start with one of the most probable next words>\n1. <first completion>", "Probable next sentences: <list of the 3 most likely completions of this text, *each should be exactly 8 words long*: interrupt yourself in the middle of the sentence if you exceed 8 words!! Hint: completions should porbably start with one of the most probable next words>\n1. <first completion>\n2. <second completion>\n3. <third completion>\nThe prompt of the user will always be of the form\n\"<Text that ends with word or punctuation <w>>\n->\"\nand you should start immediatly with\nCurrent word: <w>\"\"\"\n", "Current word: <w>\"\"\"\n\n\ndef run(\n    pile_path: str = \"~/datasets/pile/val.jsonl\",\n    write_path: str = \"data.jsonl\",\n    model_name: str = \"EleutherAI/pythia-70m-deduped\",\n    max_samples: int = 40000,\n    max_length: int = 256,\n    additional_tokens_kept: int = 20,\n    min_length: int = 20,\n    nb_solutions: int = 1,\n    model: str = \"gpt-3.5-turbo\",\n    temperature: float = 0.0,\n):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    expanded_path = os.path.expanduser(pile_path)\n    data = Data(name=[expanded_path], max_length=max_length + additional_tokens_kept)\n    dataset, _ = data.load(tokenizer)\n\n    eos = \"<|endoftext|>\"\n\n    def tokenize(toks):\n        return tokenizer.decode(toks).split(eos)[-1]\n\n    texts = []\n    i = 0\n    while len(texts) < max_samples and i < len(dataset):\n        pt = dataset[len(texts)]\n        if len(pt[\"input_ids\"]) > min_length + additional_tokens_kept:\n            texts.append(\n                (\n                    tokenize(pt[\"input_ids\"][:-additional_tokens_kept]),\n                    tokenize(pt[\"input_ids\"][-additional_tokens_kept:]),\n                )\n            )\n        i += 1\n\n    prompts = [\n        ((text, e), [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": text + \"\\n->\"}])\n        for text, e in texts\n    ]\n    results = threaded_generations(prompts, nb_solutions=nb_solutions, model=model, temperature=temperature, n_threads=50)\n\n    expanded_write_path = os.path.expanduser(write_path)\n    with open(expanded_write_path, \"w\") as f:\n        for r in results:\n            json.dump(r, f)\n            f.write(\"\\n\")", "\n\nif __name__ == \"__main__\":\n    Fire(run)\n"]}
{"filename": "tuned_lens/scripts/explore_explanation_generation.py", "chunked_list": ["from tuned_lens.scripts.generation_utils import threaded_generations\nfrom tuned_lens.scripts.ingredients import (\n    Data,\n)\nfrom transformers import AutoTokenizer\n\nSYSTEM_PROMPT = \"\"\"As a language model explainer, your task is to analyze a given truncated text and deduce what information is present in the activations of a language model at any layer in the final position. Do not attempt to complete the text provided by the user, and directly provide information in the following format:\nCurrent word: <last word or punctuation of the text>\nCurrent sentence: <current sentence in which the last word is present, summarized if and only if it is more than 10 words long, and otherwise repeated at-verbatim>\nProbable next words: <comma-separated list of 5 most probable next words/punctuation>", "Current sentence: <current sentence in which the last word is present, summarized if and only if it is more than 10 words long, and otherwise repeated at-verbatim>\nProbable next words: <comma-separated list of 5 most probable next words/punctuation>\nText style: <comma-separated list of 3-10 words describing the style of the text>\nParagraph summary: <summary of the paragraph, of at most 30 words>\nText summary: <summary of the text, of at most 30 words>\nProbable next sentences: <list of the 3 most likely completions of this text, *each should be exactly 8 words long*: interrupt yourself in the middle of the sentence if you exceed 8 words!! Hint: completions should porbably start with one of the most probable next words>\n1. <first completion>\n2. <second completion>\n3. <third completion>\"\"\"\n", "3. <third completion>\"\"\"\n\n\npile_path = \"/home/fabien/datasets/pile/val.jsonl\"\n\nmodel_name = \"EleutherAI/pythia-70m-deduped\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ndata = Data(name=[pile_path], max_length=256)\ndataset, _ = data.load(tokenizer)\n# %%", "dataset, _ = data.load(tokenizer)\n# %%\n\n# %%\nprint(tokenizer.decode(dataset[5][\"input_ids\"]))\n# %%\neos = \"<|endoftext|>\"\nn = 10\n\n\ndef tokenize(toks):\n    return tokenizer.decode(toks).split(eos)[-1]", "\n\ndef tokenize(toks):\n    return tokenizer.decode(toks).split(eos)[-1]\n\n\ntexts = [(tokenize(dataset[i][\"input_ids\"][:-3]), tokenize(dataset[i][\"input_ids\"][-3:])) for i in range(n)]\nfor t, e in texts:\n    if eos in t:\n        print(t, \"|\", e)", "\n# %%\nprompts = [\n    ((text, e), [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": text}]) for text, e in texts\n]\nresults = threaded_generations(prompts, nb_solutions=2, model=\"gpt-3.5-turbo\")\n# %%\nfor r in results:\n    print(\"|\".join(r[0]))\n    print(\"=================>\")\n    for s in r[1]:\n        print(s)\n        print(\"-\" * 80)\n    print(\"=\" * 80)", "# %%\n"]}
{"filename": "tuned_lens/scripts/ingredients.py", "chunked_list": ["\"\"\"Shared configuration for the scripts.\"\"\"\nimport enum\nimport os\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import Optional, Union\n\nimport torch as th\nimport torch.distributed as dist\nfrom datasets import Dataset, DatasetDict, load_dataset", "import torch.distributed as dist\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom simple_parsing import field\nfrom torch.distributed.fsdp import (\n    CPUOffload,\n    FullyShardedDataParallel,\n    MixedPrecision,\n)\nfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\nfrom torch.distributed.optim import ZeroRedundancyOptimizer", "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\nfrom torch.distributed.optim import ZeroRedundancyOptimizer\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizerBase,\n    get_linear_schedule_with_warmup,\n)", "    get_linear_schedule_with_warmup,\n)\n\nfrom tuned_lens.data import (\n    chunk_and_tokenize,\n)\nfrom tuned_lens.model_surgery import get_transformer_layers\nfrom tuned_lens.nn.lenses import Lens\nfrom tuned_lens.utils import (\n    TreeType,", "from tuned_lens.utils import (\n    TreeType,\n    send_to_device,\n)\n\n\n@dataclass\nclass Data:\n    \"\"\"Configuration for the dataset.\"\"\"\n\n    name: list[str] = field(default_factory=lambda: [\"the_pile\", \"all\"], nargs=\"*\")\n    \"\"\"Name of dataset to use. Can either be a local .jsonl file or a name\n    suitable to be passed to the HuggingFace load_dataset function.\"\"\"\n\n    split: str = \"validation\"\n    \"\"\"Split of the dataset to use.\"\"\"\n\n    text_column: str = \"text\"\n    \"\"\"Column of the dataset containing text to run the model on.\"\"\"\n\n    revision: Optional[str] = None\n    \"\"\"The revision of the dataset to use\"\"\"\n\n    max_length: int = 2048\n    \"\"\"The maximum length of the input sequences.\"\"\"\n\n    def load(self, tokenizer: PreTrainedTokenizerBase) -> tuple[Dataset, float]:\n        \"\"\"Load the dataset, tokenize it and compute nats_to_bpb.\"\"\"\n        print(f\"Loading dataset '{' '.join(self.name)}'\")\n\n        if len(self.name) == 1 and self.name[0].endswith(\".jsonl\"):\n            dataset = Dataset.from_json(self.name[0])\n            assert isinstance(dataset, Dataset)\n        else:\n            dataset = load_dataset(*self.name, split=self.split, revision=self.revision)\n            if not isinstance(dataset, (Dataset, DatasetDict)):\n                raise ValueError(\n                    \"Only Dataset and DatasetDict instances are supported.\"\n                )\n\n        processed, nats_to_bpb = chunk_and_tokenize(\n            dataset, tokenizer, text_key=self.text_column, max_length=self.max_length\n        )\n\n        print(f\"Using nats per token to bits per byte ratio: {nats_to_bpb}\")\n\n        assert isinstance(processed, Dataset)\n\n        return processed, nats_to_bpb", "\n\n@dataclass\nclass Model:\n    \"\"\"Configuration for the model and tokenizer.\"\"\"\n\n    name: str\n    \"\"\"Name of model to use in the Huggingface Hub.\"\"\"\n\n    revision: str = \"main\"\n    \"\"\"Git revision to use for pretrained models.\"\"\"\n\n    slow_tokenizer: bool = field(action=\"store_true\")\n    \"\"\"Use a slow tokenizer.\"\"\"\n\n    tokenizer: Optional[str] = None\n    \"\"\"Name of pretrained tokenizer to use from the Huggingface Hub. If None, will use\n    AutoTokenizer.from_pretrained('<model name>').\"\"\"\n\n    tokenizer_type: Optional[str] = None\n    \"\"\"Name of tokenizer class to use. If None, will use AutoTokenizer.\"\"\"\n\n    def load_tokenizer(self, must_use_cache: bool = False) -> PreTrainedTokenizerBase:\n        \"\"\"Load the tokenizer from huggingface hub.\"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(\n            self.tokenizer or self.name,\n            revision=self.revision,\n            use_fast=not self.slow_tokenizer,\n            tokenizer_type=self.tokenizer_type,\n            local_files_only=must_use_cache,\n        )\n\n        assert isinstance(tokenizer, PreTrainedTokenizerBase)\n        return tokenizer\n\n    def load(\n        self, must_use_cache: bool = False\n    ) -> tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n        \"\"\"Load the model and tokenizer.\"\"\"\n        print(f\"Loading pretrained weights for '{self.name}'...\")\n        model = AutoModelForCausalLM.from_pretrained(  # type: ignore\n            self.name,\n            low_cpu_mem_usage=True,\n            revision=self.revision,\n            torch_dtype=\"auto\",\n            local_files_only=must_use_cache,\n        )\n\n        assert isinstance(model, PreTrainedModel)\n        model.eval()\n        model.requires_grad_(False)\n\n        return model, self.load_tokenizer(must_use_cache=must_use_cache)", "\n\nclass OptimizerOption(enum.Enum):\n    \"\"\"Options for the optimizer to use when training the model.\"\"\"\n\n    ADAM = \"adam\"\n    SGD = \"sgd\"\n\n\n@dataclass\nclass Optimizer:\n    \"\"\"Configuration for the optimizer.\"\"\"\n\n    weight_decay: float = 1e-3\n    \"\"\"Weight decay coefficient.\"\"\"\n\n    lr_scale: float = 1.0\n    \"\"\"The default LR (1e-3 for Adam, 1.0 for SGD) is scaled by this factor.\"\"\"\n\n    momentum: float = 0.9\n    \"\"\"Momentum coefficient for SGD, or beta1 for Adam.\"\"\"\n\n    zero: Optional[bool] = field(action=\"store_true\")\n    \"\"\"Use ZeroRedundancyOptimizer.\"\"\"\n\n    optimizer: OptimizerOption = OptimizerOption.SGD\n    \"\"\"The type of optimizer to use.\"\"\"\n\n    warmup_steps: Optional[int] = None\n    \"\"\"Number of warmup steps. Defaults to min(0.2 * num_steps, 1000) for Adam and 0\n    for SGD.\"\"\"\n\n    def create_scheduler(\n        self, opt: th.optim.Optimizer, num_steps: int\n    ) -> th.optim.lr_scheduler.LambdaLR:\n        \"\"\"Create the LR scheduler.\"\"\"\n        if self.warmup_steps is None:\n            # Adam generally performs poorly without an LR warmup\n            if self.optimizer == \"adam\":\n                self.warmup_steps = min(1000, num_steps // 5)\n                print(f\"Using {self.warmup_steps} LR warmup steps for Adam\")\n            else:\n                self.warmup_steps = 0\n\n        scheduler = get_linear_schedule_with_warmup(\n            opt, self.warmup_steps, num_steps - self.warmup_steps\n        )\n\n        return scheduler\n\n    def create_optim(self, params: list[th.nn.Parameter]) -> th.optim.Optimizer:\n        \"\"\"Create the optimizer.\"\"\"\n        # Don't train things that don't need gradients\n        \u03b2 = self.momentum\n        if self.optimizer == OptimizerOption.SGD:\n            config = dict(\n                # PyTorch's implementation effectively scales the LR by 1 / (1 - \u03b2),\n                # so we undo that here. See https://www.youtube.com/watch?v=k8fTYJPd3_I\n                # for discussion. Once we do this, the optimal LR seems to be unity.\n                lr=self.lr_scale * (1 - \u03b2),\n                momentum=\u03b2,\n                # Empirically Nesterov momentum seems to improve convergence speed.\n                nesterov=True,\n                weight_decay=self.weight_decay,\n            )\n            opt_class = th.optim.SGD\n        elif self.optimizer == OptimizerOption.ADAM:\n            config = dict(\n                # Helps convergence slightly by ensuring that the LR actually decays\n                amsgrad=True,\n                betas=(\u03b2, 0.999),\n                lr=self.lr_scale * 1e-3,\n                weight_decay=self.weight_decay,\n            )\n            opt_class = th.optim.Adam\n        else:\n            raise ValueError(f\"Unknown optimizer '{self.optimizer}'\")\n\n        if self.zero:\n            opt = ZeroRedundancyOptimizer(params, optimizer_class=opt_class, **config)\n        else:\n            opt = opt_class(params, **config)  # type: ignore[call-arg]\n\n        return opt", "\n@dataclass\nclass Optimizer:\n    \"\"\"Configuration for the optimizer.\"\"\"\n\n    weight_decay: float = 1e-3\n    \"\"\"Weight decay coefficient.\"\"\"\n\n    lr_scale: float = 1.0\n    \"\"\"The default LR (1e-3 for Adam, 1.0 for SGD) is scaled by this factor.\"\"\"\n\n    momentum: float = 0.9\n    \"\"\"Momentum coefficient for SGD, or beta1 for Adam.\"\"\"\n\n    zero: Optional[bool] = field(action=\"store_true\")\n    \"\"\"Use ZeroRedundancyOptimizer.\"\"\"\n\n    optimizer: OptimizerOption = OptimizerOption.SGD\n    \"\"\"The type of optimizer to use.\"\"\"\n\n    warmup_steps: Optional[int] = None\n    \"\"\"Number of warmup steps. Defaults to min(0.2 * num_steps, 1000) for Adam and 0\n    for SGD.\"\"\"\n\n    def create_scheduler(\n        self, opt: th.optim.Optimizer, num_steps: int\n    ) -> th.optim.lr_scheduler.LambdaLR:\n        \"\"\"Create the LR scheduler.\"\"\"\n        if self.warmup_steps is None:\n            # Adam generally performs poorly without an LR warmup\n            if self.optimizer == \"adam\":\n                self.warmup_steps = min(1000, num_steps // 5)\n                print(f\"Using {self.warmup_steps} LR warmup steps for Adam\")\n            else:\n                self.warmup_steps = 0\n\n        scheduler = get_linear_schedule_with_warmup(\n            opt, self.warmup_steps, num_steps - self.warmup_steps\n        )\n\n        return scheduler\n\n    def create_optim(self, params: list[th.nn.Parameter]) -> th.optim.Optimizer:\n        \"\"\"Create the optimizer.\"\"\"\n        # Don't train things that don't need gradients\n        \u03b2 = self.momentum\n        if self.optimizer == OptimizerOption.SGD:\n            config = dict(\n                # PyTorch's implementation effectively scales the LR by 1 / (1 - \u03b2),\n                # so we undo that here. See https://www.youtube.com/watch?v=k8fTYJPd3_I\n                # for discussion. Once we do this, the optimal LR seems to be unity.\n                lr=self.lr_scale * (1 - \u03b2),\n                momentum=\u03b2,\n                # Empirically Nesterov momentum seems to improve convergence speed.\n                nesterov=True,\n                weight_decay=self.weight_decay,\n            )\n            opt_class = th.optim.SGD\n        elif self.optimizer == OptimizerOption.ADAM:\n            config = dict(\n                # Helps convergence slightly by ensuring that the LR actually decays\n                amsgrad=True,\n                betas=(\u03b2, 0.999),\n                lr=self.lr_scale * 1e-3,\n                weight_decay=self.weight_decay,\n            )\n            opt_class = th.optim.Adam\n        else:\n            raise ValueError(f\"Unknown optimizer '{self.optimizer}'\")\n\n        if self.zero:\n            opt = ZeroRedundancyOptimizer(params, optimizer_class=opt_class, **config)\n        else:\n            opt = opt_class(params, **config)  # type: ignore[call-arg]\n\n        return opt", "\n\n@dataclass\nclass Distributed:\n    \"\"\"Configuration and utilities for distributing the model.\"\"\"\n\n    fsdp: bool = field(action=\"store_true\")\n    \"\"\"Run the model with Fully Sharded Data Parallelism.\"\"\"\n\n    cpu_offload: bool = field(action=\"store_true\")\n    \"\"\"Use CPU offloading. Must be combined with fsdp\"\"\"\n\n    @property\n    def rank(self) -> int:\n        \"\"\"The rank of this process.\n\n        Note that in general this is not the same as the local rank.\n        However, for single-node training, the local rank is the same as the\n        global rank.\n        \"\"\"\n        return int(os.environ[\"RANK\"]) if dist.is_initialized() else 0\n\n    @property\n    def local_rank(self) -> int:\n        \"\"\"The local rank of this process.\"\"\"\n        return int(os.environ[\"LOCAL_RANK\"]) if dist.is_initialized() else 0\n\n    @property\n    def world_size(self) -> int:\n        \"\"\"Get the world size from torch.distributed.\"\"\"\n        return int(os.environ[\"WORLD_SIZE\"]) if dist.is_initialized() else 1\n\n    @property\n    def primary(self) -> bool:\n        \"\"\"Whether this is the rank 0 process.\"\"\"\n        return self.rank == 0\n\n    @property\n    def device(self) -> th.device:\n        \"\"\"The device associated with this process.\"\"\"\n        return th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n\n    def shard_model(\n        self, model: PreTrainedModel\n    ) -> Union[FullyShardedDataParallel, PreTrainedModel]:\n        \"\"\"Shard the model using Fully Sharded Data Parallelism.\"\"\"\n        if self.fsdp:\n            _, layers = get_transformer_layers(model)\n            layer_cls = type(layers[0])\n            print(f\"Using '{layer_cls.__name__}' for transformer_auto_wrap_policy.\")\n            return FullyShardedDataParallel(\n                model,\n                auto_wrap_policy=partial(\n                    transformer_auto_wrap_policy, transformer_layer_cls={layer_cls}\n                ),\n                cpu_offload=CPUOffload(offload_params=self.cpu_offload),\n                device_id=self.rank,\n                # This turns out to be important for training speed\n                forward_prefetch=True,\n                mixed_precision=MixedPrecision(\n                    param_dtype=th.float16,\n                    reduce_dtype=th.float16,\n                    buffer_dtype=th.float16,\n                ),\n            )\n        elif self.cpu_offload:\n            raise ValueError(\"CPU offload requires FSDP.\")\n        else:\n            model.to(self.device)\n            return model\n\n    def distribute_lens(self, lens: Lens) -> Union[DDP, Lens]:\n        \"\"\"Distribute the lens using DistributedDataParallel and send lens to device.\"\"\"\n        if self.world_size > 1:\n            lens.to(self.device)\n            return DDP(lens, device_ids=[self.local_rank], find_unused_parameters=True)\n        else:\n            return lens.to(self.device)\n\n    def shard_dataset(self, dataset: Dataset) -> Dataset:\n        \"\"\"Shard the dataset based on local rank.\"\"\"\n        if dist.is_initialized():\n            dataset = dataset.shard(self.world_size, self.rank)\n        return dataset\n\n    def init(self) -> None:\n        \"\"\"Initialize distributed process group if started with elastic launch.\"\"\"\n        # Support both distributed and non-distributed training\n        local_rank = os.environ.get(\"LOCAL_RANK\")\n        if local_rank is not None:\n            dist.init_process_group(\"nccl\")\n            assert (\n                th.cuda.is_available()\n            ), \"CUDA must be available for distributed training\"\n            th.cuda.set_device(self.local_rank)\n\n    def barrier(self) -> None:\n        \"\"\"Barrier for all processes.\"\"\"\n        if dist.is_initialized():\n            dist.barrier()\n\n    def send_to_device(self, pytree: TreeType) -> TreeType:\n        \"\"\"Move pytree to the current device.\"\"\"\n        return send_to_device(pytree, self.device)", ""]}
{"filename": "tuned_lens/scripts/eval_loop.py", "chunked_list": ["\"\"\"Evaluation loop for the tuned lens model.\"\"\"\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom itertools import islice\nfrom pathlib import Path\nfrom typing import Optional\n\nimport torch as th\nfrom simple_parsing import field\nfrom torch.utils.data import DataLoader", "from simple_parsing import field\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nfrom tuned_lens.nn.lenses import Lens, LogitLens, TunedLens\nfrom tuned_lens.scripts.ingredients import (\n    Data,\n    Distributed,\n    Model,\n)", "    Model,\n)\nfrom tuned_lens.stats import LogitStats\nfrom tuned_lens.utils import (\n    maybe_all_reduce,\n    pytree_map,\n    pytree_stack,\n    shift_labels,\n    shift_preds,\n)", "    shift_preds,\n)\n\n\n@dataclass\nclass Eval:\n    \"\"\"Type hinting for CLI args.\"\"\"\n\n    data: Data\n\n    model: Model\n\n    dist: Distributed\n\n    lens: Optional[str] = field(alias=[\"-l\"], default=None)\n    \"\"\"Path to the tuned lens model.\"\"\"\n\n    seed: int = 42\n    \"\"\"Random seed used for data shuffling.\"\"\"\n\n    grad_alignment: Optional[bool] = field(action=\"store_true\")\n    \"\"\"Evaluate gradient alignment.\"\"\"\n\n    limit: Optional[int] = None\n    \"\"\"Number of batches to evaluate on. If None, will use the entire dataset.\"\"\"\n\n    output: Optional[Path] = field(alias=[\"-o\"], default=None)\n    \"\"\"JSON file to save the eval results to.\"\"\"\n\n    transfer: Optional[bool] = field(action=\"store_true\")\n    \"\"\"Evaluate how well probes transfer to other layers.\"\"\"\n\n    token_shift: Optional[int] = None\n    \"\"\"How to shift the labels wrt the input tokens (1 = next token, 0 = current token,\n    -1 = previous token, etc.)\"\"\"\n\n    per_gpu_batch_size: int = 1\n    \"\"\"Number of samples to try to fit on a GPU at once.\"\"\"\n\n    residual_stats: bool = field(action=\"store_true\")\n\n    def load_lens(self, model) -> Lens:\n        \"\"\"Load the tuned lens model.\"\"\"\n        if self.lens is None:\n            return LogitLens.from_model(model)\n        else:\n            return TunedLens.from_model_and_pretrained(model, self.lens)\n\n    @th.autocast(\"cuda\", enabled=th.cuda.is_available())\n    @th.no_grad()\n    def execute(self):\n        \"\"\"Trains a TunedLens model against a transformer on a dataset.\"\"\"\n        # Load model, tokenizer, data, and lens\n        self.dist.init()\n        model = tokenizer = data = lens = nats_to_bpb = model_name = None\n        if self.dist.primary:\n            # Let the primary processes populate the cache\n            model, tokenizer = self.model.load()\n            data, nats_to_bpb = self.data.load(tokenizer)\n            lens = self.load_lens(model)\n\n        self.dist.barrier()  # Wait for primary to finish filling the cache\n\n        if not self.dist.primary:\n            # Let the non-primary processes load from the cache\n            model, tokenizer = self.model.load(must_use_cache=True)\n            data, nats_to_bpb = self.data.load(tokenizer)\n            lens = self.load_lens(model)\n\n        assert model and tokenizer and data and lens and nats_to_bpb\n\n        model = self.dist.shard_model(model)\n        # Note since we are not training we can just move the lens to the device.\n        # No need to use DDP\n        lens = lens.to(self.dist.device)\n        data = self.dist.shard_dataset(data)\n\n        dl = DataLoader(\n            data.shuffle(seed=self.seed),  # type: ignore[arg-type],\n            batch_size=self.per_gpu_batch_size,\n        )\n\n        lens.eval()\n\n        if self.limit:\n            dl = islice(dl, self.limit)\n            total = self.limit\n        else:\n            total = len(dl)\n\n        *_, model_name = model.config.name_or_path.split(\"/\")\n\n        if self.lens:\n            root_dir = Path(self.lens, \"tuned-lens-eval\")\n        else:\n            root_dir = Path(model_name, \"logit_lens_eval\")\n\n        if self.output is not None:\n            root_dir = self.output\n\n        root_dir.mkdir(exist_ok=True, parents=True)\n\n        L = model.config.num_hidden_layers\n        batches = []\n        transfer_batches = []\n\n        final_logit_stats = LogitStats()\n        lens_statistics = [LogitStats() for _ in range(L)]\n        self.dist.barrier()\n        print(f\"All processes initialized. Running evaluation on {total} batches.\")\n\n        pbar = tqdm(dl, desc=\"Evaluating\", position=self.dist.rank, total=total)\n        for batch in pbar:\n            batch = self.dist.send_to_device(batch)\n            with th.no_grad():\n                output = model(**batch, output_hidden_states=True)\n\n            hidden_states = output.hidden_states[-1:]\n\n            shift = self.token_shift if self.token_shift is not None else 1\n            final_lps = output.logits.log_softmax(dim=-1)\n            final_probs = final_lps.exp()\n            labels = shift_labels(batch[\"input_ids\"], shift)\n\n            batch_output = defaultdict(dict)\n            transfer_ces = th.zeros(L, L, device=final_lps.device)\n            transfer_kls = th.zeros(L, L, device=final_lps.device)\n\n            # Compute tuned lens eval and statistics if applicable\n            for j, h in zip(range(L), hidden_states):\n                name = f\"layer_{j}\"\n                lens_lps = lens(h, idx=j).log_softmax(dim=-1)\n                lens_probs = lens_lps.exp()\n\n                batch_output[\"lens_ce\"][name] = th.nn.functional.cross_entropy(\n                    shift_preds(lens_lps, shift).flatten(0, 1),\n                    labels.flatten(),\n                    reduction=\"none\",\n                )\n                batch_output[\"lens_entropy\"][name] = th.sum(\n                    -lens_probs * lens_lps, dim=-1\n                )\n                batch_output[\"lens_kl\"][name] = th.sum(\n                    final_probs * (final_lps - lens_lps), dim=-1\n                )\n                lens_statistics[j].update(lens_lps, assume_normalized=True)\n\n                if self.transfer:\n                    # Each iteration of the loop processes a different *probe*\n                    # layer i for the test layer j.\n                    for i in range(L):\n                        transfer_lps = lens(h, idx=i).log_softmax(dim=-1)\n                        transfer_ces[i, j] = th.nn.functional.cross_entropy(\n                            shift_preds(transfer_lps, shift).flatten(0, 1),\n                            labels.flatten(),\n                        )\n                        transfer_kls[i, j] = th.sum(\n                            lens_probs * (lens_lps - transfer_lps), dim=-1\n                        ).mean()\n\n            final_logit_stats.update(final_lps, assume_normalized=True)\n\n            batch_output[\"baseline_ce\"][\"final\"] = th.nn.functional.cross_entropy(\n                shift_preds(final_lps, shift).flatten(0, 1),\n                labels.flatten(),\n                reduction=\"none\",\n            )\n            batch_output[\"baseline_entropy\"][\"final\"] = th.sum(\n                -final_probs * final_lps, dim=-1\n            )\n            batches.append(pytree_map(th.mean, batch_output))  # type: ignore[arg-type]\n            transfer_batches.append(\n                {\n                    \"transfer_ce\": transfer_ces,\n                    \"transfer_kl\": transfer_kls,\n                }\n            )\n            # Keep the processes synced\n            self.dist.barrier()\n\n        pbar.close()\n        agg = pytree_map(lambda x: nats_to_bpb * x.mean(), pytree_stack(batches))\n        agg = pytree_map(lambda x: maybe_all_reduce(x), agg)\n        if self.dist.primary:\n            th.save(agg, root_dir / \"aggregate_metrics.pt\")\n\n        if self.transfer:\n            agg_transfer = pytree_map(\n                lambda x: nats_to_bpb * x.mean(0), pytree_stack(transfer_batches)\n            )\n            agg_transfer = pytree_map(lambda x: maybe_all_reduce(x), agg_transfer)\n            if self.dist.primary:\n                th.save(agg_transfer, root_dir / \"aggregate_transfer_metrics.pt\")\n\n        for stats in lens_statistics:\n            stats.all_reduce_()\n\n        if self.dist.primary:\n            th.save(lens_statistics, root_dir / \"lens_logit_stats.pt\")", ""]}
{"filename": "tuned_lens/scripts/train_loop.py", "chunked_list": ["\"\"\"Training loop for training a TunedLens model against a transformer on a dataset.\"\"\"\nimport dataclasses\nimport enum\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom itertools import islice\nfrom pathlib import Path\nfrom typing import Optional\nimport numpy as np\nimport scipy as sp", "import numpy as np\nimport scipy as sp\n\nimport torch as th\nfrom simple_parsing import field\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedModel\n\nfrom tuned_lens import TunedLens", "\nfrom tuned_lens import TunedLens\nfrom tuned_lens.scripts.ingredients import (\n    Data,\n    Distributed,\n    Model,\n    Optimizer,\n)\nfrom tuned_lens.utils import maybe_all_reduce, shift_labels, shift_preds\n", "from tuned_lens.utils import maybe_all_reduce, shift_labels, shift_preds\n\n\nclass LossChoice(enum.Enum):\n    \"\"\"Options of what loss to select when training the model.\"\"\"\n\n    CE = \"ce\"\n    KL = \"kl\"\n\n", "\n\n@dataclass\nclass Train:\n    \"\"\"Training loop for the tuned lens.\"\"\"\n\n    model: Model\n    \"\"\"Model configuration.\"\"\"\n\n    data: Data\n    \"\"\"Data configuration.\"\"\"\n\n    opt: Optimizer\n    \"\"\"Optimizer configuration.\"\"\"\n\n    dist: Distributed\n    \"\"\"Configuration for how to distribute the training.\"\"\"\n\n    seed: int = 42\n    \"\"\"Random seed for data shuffling.\"\"\"\n\n    lens_name_or_path: Optional[str] = field(alias=[\"-l\"], default=None)\n\n    constant: Optional[bool] = field(action=\"store_true\")\n    \"\"\"Train only the bias term.\"\"\"\n\n    num_steps: int = 250\n    \"\"\"Number of training steps.\"\"\"\n\n    output: Optional[Path] = field(alias=[\"-o\"], default=None)\n    \"\"\"File to save the lenses to. Defaults to the model name.\"\"\"\n\n    pre_ln: Optional[bool] = field(action=\"store_true\")\n    \"\"\"Apply layer norm before, and not after, each probe.\"\"\"\n\n    separate_unembeddings: Optional[bool] = field(action=\"store_true\")\n    \"\"\"Learn a separate unembedding for each layer.\"\"\"\n\n    tokens_per_step: int = 2**18\n    \"\"\"Number of tokens per step.\"\"\"\n\n    wandb: Optional[str] = None\n    \"\"\"Name of run in Weights & Biases.\"\"\"\n\n    wandb_upload_checkpoints: Optional[bool] = field(action=\"store_true\")\n    \"\"\"Upload checkpoints to Weights & Biases.\"\"\"\n\n    token_shift: Optional[int] = None\n    \"\"\"How to shift the labels wrt the input tokens (1 = next token, 0 = current token,\n    -1 = previous token, etc.)\"\"\"\n\n    per_gpu_batch_size: int = 1\n    \"\"\"Number of samples to try to fit on a GPU at once.\"\"\"\n\n    loss: LossChoice = LossChoice.KL\n    \"\"\"Loss function to use.\"\"\"\n\n    consistency_weight: float = 0.0\n\n    start_temp: float = 1.0\n    end_temp: float = 0\n    no_bias: bool = field(action=\"store_true\")\n\n    def get_lens(self, model: PreTrainedModel) -> TunedLens:\n        \"\"\"Load or create a TunedLens model.\"\"\"\n        if self.lens_name_or_path is None:\n            lens = TunedLens.from_model(model, bias=not self.no_bias)\n        else:\n            lens = TunedLens.from_model_and_pretrained(model, self.lens_name_or_path)\n\n        lens_size = sum(p.numel() * p.element_size() for p in lens.parameters())\n        print(f\"Tuned lens parameter memory usage: {lens_size / 2 ** 20:.2f} MB\")\n\n        if self.constant:\n            for probe in lens:\n                probe.weight.requires_grad_(False)\n\n        return lens\n\n    def _init_logging(self, model_name: str, lens: TunedLens):\n        \"\"\"Initialize logging to weights and biases.\"\"\"\n        if not self.dist.primary or not self.wandb:\n            return\n\n        import wandb\n\n        wandb.init(\n            config=dataclasses.asdict(self),\n            group=model_name,\n            name=self.wandb,\n        )\n        wandb.watch(lens)\n\n    def _log(\n        self,\n        opt: th.optim.Optimizer,\n        step: int,\n        losses: dict[str, list[float]],\n        tuned_lens: TunedLens,\n        temperature: float,\n    ):\n        \"\"\"Log statistics about the training process to weights and biases.\"\"\"\n        if not self.dist.primary or not self.wandb:\n            return\n\n        import wandb\n\n        log_dict = {}\n        log_dict.update({f\"loss/{k}\": th.tensor(v).mean() for k, v in losses.items()})\n        log_dict[\"temperature\"] = temperature\n\n        # Log statistics about optimizer & probes\n        for i, probe in enumerate(tuned_lens):\n            name = \"input\" if i == 0 else f\"{i - 1}.ffn\"\n            states = [opt.state[p] for p in probe.parameters()]\n\n            # Approximate the true grad norm using the optimizer's moving\n            # avg\n            corr = 1 - self.opt.momentum**step\n            if self.opt.optimizer == \"sgd\" and not self.opt.zero:\n                log_dict[\"grad_norm/\" + name] = th.cat(\n                    [\n                        # Undo PyTorch's scaling of the gradient by\n                        # 1 / (1 - \u03b2)\n                        (1 - self.opt.momentum) * s[\"momentum_buffer\"].flatten() / corr\n                        for s in states\n                    ]\n                ).norm()\n            elif self.opt.optimizer == \"adam\" and not self.opt.zero:\n                log_dict[\"grad_norm/\" + name] = th.cat(\n                    [s[\"exp_avg\"].flatten() / corr for s in states]\n                ).norm()\n\n            if isinstance(probe, th.nn.Linear):\n                if not self.no_bias:\n                    log_dict[\"bias_norm/\" + name] = probe.bias.data.norm()\n                log_dict[\"weight_norm/\" + name] = probe.weight.data.norm()\n\n        for key, weight in tuned_lens.named_parameters():\n            log_dict[f\"ln_bias_{key}\"] = weight.data.norm()\n\n        wandb.log(log_dict)\n\n    def _save_model(self, tuned_lens: TunedLens, model_name: str):\n        \"\"\"Save the model to disk and try to upload to wandb if enabled.\"\"\"\n        if not self.dist.primary:\n            return\n\n        assert model_name is not None\n        output = model_name if self.output is None else self.output\n        print(f\"Saving lens to {output}\")\n        tuned_lens.save(output)\n\n        if self.wandb and self.wandb_upload_checkpoints:\n            import wandb\n\n            artifact = wandb.Artifact(\n                name=\"final_checkpoint\",\n                type=\"checkpoint\",\n                description=\"A trained lens.\",\n            )\n            artifact.add_dir(output)\n            wandb.log_artifact(artifact)\n\n    def calculate_gradient_accumulation_steps(self, tokens_per_sample: int) -> int:\n        \"\"\"Calculate the number of batches of data to process before taking a step.\"\"\"\n        # chunk_and_tokenize ensures the samples are all the same length\n        samples_per_step, rem = divmod(self.tokens_per_step, tokens_per_sample)\n        if rem:\n            raise ValueError(\n                f\"Number of tokens per step ({self.tokens_per_step:_}) must be \"\n                f\"divisible by the number of tokens per sample ({tokens_per_sample}).\"\n            )\n\n        global_batch_size = self.per_gpu_batch_size * self.dist.world_size\n        grad_acc_steps, rem = divmod(samples_per_step, global_batch_size)\n        if rem:\n            # If the number of samples per step isn't divisible by the global batch\n            # size, use ceil division and let the user know about it.\n            grad_acc_steps += 1\n            adjusted_count = grad_acc_steps * global_batch_size * tokens_per_sample\n            print(\n                f\"Note: Increasing grad acc steps from {grad_acc_steps - 1} to \"\n                f\"{grad_acc_steps} to maintain load balance across \"\n                f\"{self.dist.world_size} GPUs.\"\n            )\n            print(\n                f\"Using {adjusted_count:_} tokens per training step \"\n                f\"({self.tokens_per_step:_} requested).\"\n            )\n        else:\n            print(f\"Gradient accumulation steps: {grad_acc_steps}\")\n            print(f\"Using {self.tokens_per_step:_} tokens per training step.\")\n        return grad_acc_steps\n\n    def execute(self):\n        \"\"\"Trains a TunedLens model against a transformer on a dataset.\"\"\"\n        # Load model, tokenizer, data, and lens\n        self.dist.init()\n        model = tokenizer = data = lens = nats_to_bpb = model_name = None\n        if self.dist.primary:\n            # Let the primary processes populate the cache\n            model, tokenizer = self.model.load()\n            data, nats_to_bpb = self.data.load(tokenizer)\n            lens = self.get_lens(model)\n\n            *_, model_name = model.config.name_or_path.split(\"/\")\n            self._init_logging(model_name, lens)\n\n        self.dist.barrier()  # Wait for primary to finish filling the cache\n\n        if not self.dist.primary:\n            # Let the non-primary processes load from the cache\n            model, tokenizer = self.model.load(must_use_cache=True)\n            data, nats_to_bpb = self.data.load(tokenizer)\n            lens = self.get_lens(model)\n\n        assert model and tokenizer and data and lens and nats_to_bpb\n\n        # Shard the model using fully shared data parallel\n        model = self.dist.shard_model(model)\n        # Distribute the lens across the GPUS using distributed data parallel\n        ddp_lens = self.dist.distribute_lens(lens)\n        # Shard the dataset for use with distributed data parallel\n        data = self.dist.shard_dataset(data)\n\n        dl = DataLoader(\n            data.shuffle(seed=self.seed),  # type: ignore[arg-type]\n            batch_size=self.per_gpu_batch_size,\n        )\n\n        # Don't train the unembedding matrix or final layer norm\n        params = [p for p in ddp_lens.parameters() if p.requires_grad]\n\n        # Create the optimizer and scheduler\n        opt = self.opt.create_optim(params)\n        scheduler = self.opt.create_scheduler(opt, self.num_steps)\n\n        tokens_per_sample = len(data[0][\"input_ids\"])\n\n        grad_acc_steps = self.calculate_gradient_accumulation_steps(tokens_per_sample)\n\n        if self.dist.cpu_offload and grad_acc_steps > 1:\n            raise ValueError(\"CPU offloading cannot be used with gradient accumulation\")\n\n        losses = defaultdict(list)\n        total_batches = self.num_steps * grad_acc_steps\n\n        # Wait for all processes to finish setup\n        self.dist.barrier()\n        print(\"All processes have completed setup. Starting training.\")\n\n        # Main training loop\n        pbar = tqdm(islice(dl, total_batches), desc=\"Training\", total=total_batches)\n        for batch_idx, batch in enumerate(pbar, start=1):\n            assert isinstance(batch, dict)\n            batch = self.dist.send_to_device(batch)\n            with th.no_grad():\n                output = model(**batch, output_hidden_states=True)\n\n            final_logits = output.logits\n            hidden_stats = output.hidden_states[:-1]\n\n            shift = self.token_shift\n            if self.loss == LossChoice.CE:\n                labels = batch[\"input_ids\"]\n\n                # Predict the *next* token by default w/ cross entropy\n                if shift is None:\n                    shift = 1\n            elif self.loss == LossChoice.KL:\n                labels = final_logits.log_softmax(dim=-1)\n\n                # Match the *current* token distribution by default\n                if shift is None:\n                    shift = 0\n            else:\n                raise NotImplementedError(f\"Unknown loss {self.loss}\")\n\n            labels = shift_labels(labels, shift)\n\n            # cosine schedule\n            u = batch_idx / (total_batches - 1)\n            c = 1 - ((np.cos(np.pi * u) + 1) / 2) ** 2\n            reconstruct_temp = self.start_temp + (self.end_temp - self.start_temp) * c\n\n            # We do this sequentially to save VRAM\n            for i, h in enumerate(hidden_stats):\n                # bfloat16 has larger dynamic range than float16 and seems to be better\n                # for computing log softmax & KL loss\n                with th.autocast(self.dist.device.type, dtype=th.bfloat16):\n                    logits_pre_shift = ddp_lens(h, idx=i)\n                    logits = shift_preds(logits_pre_shift, shift)\n\n                    if self.loss == LossChoice.CE:\n                        loss = th.nn.functional.cross_entropy(\n                            logits.flatten(0, -2), labels.flatten()\n                        )\n                    elif self.loss == LossChoice.KL:\n                        loss = th.sum(\n                            labels.exp() * (labels - logits.log_softmax(-1)), dim=-1\n                        ).mean()\n                    else:\n                        raise NotImplementedError\n\n                    loss *= nats_to_bpb\n                    logging_loss = loss.detach()\n                    logging_loss = maybe_all_reduce(logging_loss).item()\n\n                    if self.consistency_weight > 0:\n                        if reconstruct_temp < 1e-6:\n                            probs = th.nn.functional.one_hot(\n                                logits.argmax(dim=-1), logits.shape[-1]\n                            )\n                        else:\n                            probs = th.nn.functional.softmax(\n                                logits_pre_shift / reconstruct_temp, dim=-1\n                            )\n                        reconstruction = ddp_lens.reconstruct(probs, idx=i)\n\n                        # use cosine similiarity as reconstruction loss\n                        reconstruction_loss = (\n                            1\n                            - th.nn.functional.cosine_similarity(\n                                h, reconstruction, dim=-1\n                            ).mean()\n                        )\n\n                        logging_reconstruction_loss = reconstruction_loss.detach()\n                        logging_reconstruction_loss = maybe_all_reduce(\n                            logging_reconstruction_loss\n                        ).item()\n\n                        loss += self.consistency_weight * reconstruction_loss\n\n                        # sparse if p_max > 0.9\n                        p_max = probs.max(dim=-1)[0]\n                        count_sparse = (p_max > 0.9).float().mean().item()\n\n                    if self.dist.primary:\n                        losses[f\"loss_{i}\"].append(loss.detach().item())\n                        if self.consistency_weight > 0:\n                            losses[f\"translator_{i}\"].append(logging_loss)\n                            losses[f\"reconstruction_{i}\"].append(\n                                logging_reconstruction_loss\n                            )\n                            losses[f\"sparsity_{i}\"].append(count_sparse)\n\n                    scaled_loss = loss / grad_acc_steps\n\n                scaled_loss.backward()\n\n            step, rem = divmod(batch_idx, grad_acc_steps)\n            if rem == 0:\n                pbar.set_postfix(\n                    {\n                        \"reconstruction_5\": th.tensor(losses[\"reconstruction_5\"])\n                        .mean()\n                        .item()\n                    }\n                )\n                th.nn.utils.clip_grad_norm_(lens.parameters(), 1.0)\n                opt.step()\n                opt.zero_grad(set_to_none=False)\n                scheduler.step()\n                self._log(opt, step, losses, lens, reconstruct_temp)\n                losses.clear()\n\n        if self.dist.primary:\n            assert model_name is not None\n            output = model_name if self.output is None else self.output\n            print(f\"Saving lens to {output}\")\n            lens.save(output)", ""]}
{"filename": "tuned_lens/scripts/__init__.py", "chunked_list": ["\"\"\"Implementations of subcommands.\"\"\"\n"]}
{"filename": "tuned_lens/scripts/train_injected.py", "chunked_list": ["import random\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GPTNeoXForCausalLM\nimport os\nimport json\nimport torch\nfrom tqdm import trange\nimport wandb\nfrom fire import Fire\n\n\ndef run(\n    batch_size: int = 32,\n    n_epochs: int = 3,\n    completions_path: str = \"data.jsonl\",\n    model_name: str = \"EleutherAI/pythia-410m-deduped\",\n    layers: list[int] = [0, 6, 10, 16],\n    lr: float = 5e-4,\n    max_length: int = 512,\n    generate_every: int = 20,\n):\n    for k, v in locals().items():\n        print(f\"{k}: {v}\")\n\n    wandb.init(project=\"sft-lens\", config=locals())\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    ref_model = GPTNeoXForCausalLM.from_pretrained(model_name).to(device)\n    decoder_model = GPTNeoXForCausalLM.from_pretrained(model_name).to(device)\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    with open(completions_path, \"r\") as f:\n        completions = [json.loads(l) for l in f]\n\n    all_texts: list[tuple[str, str, str]] = []\n    for (text, text_end), answers in completions:\n        for answer in answers:\n            all_texts.append((text, text_end, answer))\n\n    ref_model_dim = ref_model.config.hidden_size\n    decoder_model_dim = decoder_model.config.hidden_size\n\n    adapters = [torch.nn.Linear(ref_model_dim, decoder_model_dim).to(device) for _ in layers]\n    for adapter in adapters:\n        # identity initialization\n        adapter.weight.data.copy_(torch.eye(ref_model_dim, device=device))\n\n    adapter_parameters = sum([list(adapter.parameters()) for adapter in adapters], [])\n\n    optimizer = torch.optim.Adam(list(decoder_model.parameters()) + adapter_parameters, lr=lr)\n    # linear lr warmup\n    warmup_steps = 0.05 * len(all_texts) * n_epochs / batch_size\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(1, step / warmup_steps))\n\n    eos = tokenizer.eos_token\n\n    # dry save to check things work\n    e = 0\n    os.makedirs(f\"checkpoints/{wandb.run.id}/epoch_{e}\", exist_ok=True)\n    decoder_model.save_pretrained(f\"checkpoints/{wandb.run.id}/epoch_{e}\")\n    torch.save(\n        {layers[i]: adapter for i, adapter in enumerate(adapters)},\n        f\"checkpoints/{wandb.run.id}/epoch_{e}/adapters\",\n    )\n\n    for e in range(n_epochs):\n        pbar = trange(0, len(all_texts), batch_size)\n        random.shuffle(all_texts)\n\n        for i in pbar:\n            batch = all_texts[i : i + batch_size]\n\n            with torch.no_grad():\n                inputs = tokenizer(\n                    [eos + t[0] for t in batch],\n                    return_tensors=\"pt\",\n                    padding=True,\n                    truncation=True,\n                    max_length=max_length,\n                )\n                # get activations at every layer\n                outputs = ref_model(**inputs.to(device), output_hidden_states=True)\n                # get activations at last position of text\n                last_tok_pos = inputs[\"attention_mask\"].sum(dim=1) - 1\n                last_pos_activations = [\n                    outputs[\"hidden_states\"][l][torch.arange(len(outputs[\"hidden_states\"][l])), last_tok_pos, :]\n                    for l in layers\n                ]\n\n            losses = []\n            generations = []\n\n            for layer_i, (layer, state) in enumerate(zip(layers, last_pos_activations)):\n                expected_strings = [f\"{eos}Layer {layer}\\n{t[2]}{eos}\" for t in batch]\n                expected_tokens = tokenizer(\n                    expected_strings, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n                ).to(device)\n\n                def state_insertion_hook(module, inp, out):\n                    if out.shape[1] > 1:\n                        out[:, 0, :] = adapters[layer_i](state[: len(out), :])\n                        return out\n                    else:\n                        # in generation, we don't inject\n                        return out\n\n                handle = decoder_model.gpt_neox.embed_in.register_forward_hook(state_insertion_hook)\n\n                try:\n                    if (i // batch_size) % generate_every == 0:\n                        # generate sequence with injected state of batch elt\n                        with torch.no_grad():\n                            inputs = tokenizer(f\"{eos}Layer {layer}\\n\", return_tensors=\"pt\")\n                            top_generation_tokens = decoder_model.generate(\n                                **inputs.to(device),\n                                do_sample=False,\n                                max_new_tokens=200,\n                                pad_token_id=tokenizer.eos_token_id,\n                            )\n                            top_generation_string = tokenizer.decode(top_generation_tokens[0])\n                            generations.append(top_generation_string)\n\n                    # compute logits\n                    logits: torch.Tensor\n                    logits = decoder_model(**expected_tokens, labels=expected_tokens.input_ids).logits[:, :-1, :]\n\n                    labels = expected_tokens.input_ids[:, 1:]\n                    loss_per_pos = torch.nn.functional.cross_entropy(\n                        logits.reshape(-1, logits.shape[-1]), labels.reshape(-1), reduction=\"none\"\n                    ).reshape(labels.shape)\n                    masked_loss = loss_per_pos * expected_tokens.attention_mask[:, :-1]\n                    loss = masked_loss.sum() / expected_tokens.attention_mask[:, :-1].sum()\n\n                    losses.append(loss.item())\n\n                    loss /= len(last_pos_activations)\n                    loss.backward()\n                finally:\n                    handle.remove()\n\n            # clip grad\n            torch.nn.utils.clip_grad_norm_(decoder_model.parameters(), 1.0)\n\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n\n            pbar.set_description(f\"loss: {sum(losses)/len(losses):.4f}\")\n\n            to_log = {\n                \"loss\": sum(losses) / len(losses),\n                **{f\"losses/layer_{layers[i]}\": loss for i, loss in enumerate(losses)},\n                \"lr\": scheduler.get_last_lr()[0],\n            }\n            if generations:\n                generation_table = wandb.Table(columns=[\"layer\", \"generation\", \"text\", \"text end\", \"label\"])\n                for layer_i, s in enumerate(generations):\n                    generation_table.add_data(layers[layer_i], s, *batch[0])\n                to_log[\"generations\"] = generation_table\n            wandb.log(to_log)\n\n        os.makedirs(f\"checkpoints/{wandb.run.id}/epoch_{e}\", exist_ok=True)\n        decoder_model.save_pretrained(f\"checkpoints/{wandb.run.id}/epoch_{e}\")\n        torch.save(\n            {layers[i]: adapter for i, adapter in enumerate(adapters)},\n            f\"checkpoints/{wandb.run.id}/epoch_{e}/adapters\",\n        )\n\n    wandb.finish()", "\n\ndef run(\n    batch_size: int = 32,\n    n_epochs: int = 3,\n    completions_path: str = \"data.jsonl\",\n    model_name: str = \"EleutherAI/pythia-410m-deduped\",\n    layers: list[int] = [0, 6, 10, 16],\n    lr: float = 5e-4,\n    max_length: int = 512,\n    generate_every: int = 20,\n):\n    for k, v in locals().items():\n        print(f\"{k}: {v}\")\n\n    wandb.init(project=\"sft-lens\", config=locals())\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    ref_model = GPTNeoXForCausalLM.from_pretrained(model_name).to(device)\n    decoder_model = GPTNeoXForCausalLM.from_pretrained(model_name).to(device)\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n    with open(completions_path, \"r\") as f:\n        completions = [json.loads(l) for l in f]\n\n    all_texts: list[tuple[str, str, str]] = []\n    for (text, text_end), answers in completions:\n        for answer in answers:\n            all_texts.append((text, text_end, answer))\n\n    ref_model_dim = ref_model.config.hidden_size\n    decoder_model_dim = decoder_model.config.hidden_size\n\n    adapters = [torch.nn.Linear(ref_model_dim, decoder_model_dim).to(device) for _ in layers]\n    for adapter in adapters:\n        # identity initialization\n        adapter.weight.data.copy_(torch.eye(ref_model_dim, device=device))\n\n    adapter_parameters = sum([list(adapter.parameters()) for adapter in adapters], [])\n\n    optimizer = torch.optim.Adam(list(decoder_model.parameters()) + adapter_parameters, lr=lr)\n    # linear lr warmup\n    warmup_steps = 0.05 * len(all_texts) * n_epochs / batch_size\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(1, step / warmup_steps))\n\n    eos = tokenizer.eos_token\n\n    # dry save to check things work\n    e = 0\n    os.makedirs(f\"checkpoints/{wandb.run.id}/epoch_{e}\", exist_ok=True)\n    decoder_model.save_pretrained(f\"checkpoints/{wandb.run.id}/epoch_{e}\")\n    torch.save(\n        {layers[i]: adapter for i, adapter in enumerate(adapters)},\n        f\"checkpoints/{wandb.run.id}/epoch_{e}/adapters\",\n    )\n\n    for e in range(n_epochs):\n        pbar = trange(0, len(all_texts), batch_size)\n        random.shuffle(all_texts)\n\n        for i in pbar:\n            batch = all_texts[i : i + batch_size]\n\n            with torch.no_grad():\n                inputs = tokenizer(\n                    [eos + t[0] for t in batch],\n                    return_tensors=\"pt\",\n                    padding=True,\n                    truncation=True,\n                    max_length=max_length,\n                )\n                # get activations at every layer\n                outputs = ref_model(**inputs.to(device), output_hidden_states=True)\n                # get activations at last position of text\n                last_tok_pos = inputs[\"attention_mask\"].sum(dim=1) - 1\n                last_pos_activations = [\n                    outputs[\"hidden_states\"][l][torch.arange(len(outputs[\"hidden_states\"][l])), last_tok_pos, :]\n                    for l in layers\n                ]\n\n            losses = []\n            generations = []\n\n            for layer_i, (layer, state) in enumerate(zip(layers, last_pos_activations)):\n                expected_strings = [f\"{eos}Layer {layer}\\n{t[2]}{eos}\" for t in batch]\n                expected_tokens = tokenizer(\n                    expected_strings, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n                ).to(device)\n\n                def state_insertion_hook(module, inp, out):\n                    if out.shape[1] > 1:\n                        out[:, 0, :] = adapters[layer_i](state[: len(out), :])\n                        return out\n                    else:\n                        # in generation, we don't inject\n                        return out\n\n                handle = decoder_model.gpt_neox.embed_in.register_forward_hook(state_insertion_hook)\n\n                try:\n                    if (i // batch_size) % generate_every == 0:\n                        # generate sequence with injected state of batch elt\n                        with torch.no_grad():\n                            inputs = tokenizer(f\"{eos}Layer {layer}\\n\", return_tensors=\"pt\")\n                            top_generation_tokens = decoder_model.generate(\n                                **inputs.to(device),\n                                do_sample=False,\n                                max_new_tokens=200,\n                                pad_token_id=tokenizer.eos_token_id,\n                            )\n                            top_generation_string = tokenizer.decode(top_generation_tokens[0])\n                            generations.append(top_generation_string)\n\n                    # compute logits\n                    logits: torch.Tensor\n                    logits = decoder_model(**expected_tokens, labels=expected_tokens.input_ids).logits[:, :-1, :]\n\n                    labels = expected_tokens.input_ids[:, 1:]\n                    loss_per_pos = torch.nn.functional.cross_entropy(\n                        logits.reshape(-1, logits.shape[-1]), labels.reshape(-1), reduction=\"none\"\n                    ).reshape(labels.shape)\n                    masked_loss = loss_per_pos * expected_tokens.attention_mask[:, :-1]\n                    loss = masked_loss.sum() / expected_tokens.attention_mask[:, :-1].sum()\n\n                    losses.append(loss.item())\n\n                    loss /= len(last_pos_activations)\n                    loss.backward()\n                finally:\n                    handle.remove()\n\n            # clip grad\n            torch.nn.utils.clip_grad_norm_(decoder_model.parameters(), 1.0)\n\n            optimizer.step()\n            optimizer.zero_grad()\n            scheduler.step()\n\n            pbar.set_description(f\"loss: {sum(losses)/len(losses):.4f}\")\n\n            to_log = {\n                \"loss\": sum(losses) / len(losses),\n                **{f\"losses/layer_{layers[i]}\": loss for i, loss in enumerate(losses)},\n                \"lr\": scheduler.get_last_lr()[0],\n            }\n            if generations:\n                generation_table = wandb.Table(columns=[\"layer\", \"generation\", \"text\", \"text end\", \"label\"])\n                for layer_i, s in enumerate(generations):\n                    generation_table.add_data(layers[layer_i], s, *batch[0])\n                to_log[\"generations\"] = generation_table\n            wandb.log(to_log)\n\n        os.makedirs(f\"checkpoints/{wandb.run.id}/epoch_{e}\", exist_ok=True)\n        decoder_model.save_pretrained(f\"checkpoints/{wandb.run.id}/epoch_{e}\")\n        torch.save(\n            {layers[i]: adapter for i, adapter in enumerate(adapters)},\n            f\"checkpoints/{wandb.run.id}/epoch_{e}/adapters\",\n        )\n\n    wandb.finish()", "\n\nif __name__ == \"__main__\":\n    Fire(run)\n"]}
{"filename": "tuned_lens/scripts/generation_utils.py", "chunked_list": ["import re\nimport threading\nimport time\nfrom time import sleep\nfrom typing import Optional, TypeVar\n\nimport attrs\nimport openai\nfrom tqdm import tqdm\nimport tiktoken", "from tqdm import tqdm\nimport tiktoken\n\ntokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n\n\n@attrs.frozen\nclass GenerationParams:\n    temperature: float\n    model: str", "\n\nTOKENS_PER_MODEL = {\n    \"gpt-3.5-turbo\": 4096,\n    \"gpt-4\": 8192,\n}\nMAX_RPM_PER_MODEL = {\n    \"gpt-3.5-turbo\": 3500,\n    \"gpt-4\": 200,\n}", "    \"gpt-4\": 200,\n}\nMAX_TPM_PER_MODEL = {\n    \"gpt-3.5-turbo\": 90_000,\n    \"gpt-4\": 40_000,\n}\n\nMAX_TOKENS_MARGIN = 20\n\n\ndef strip_triple_backquotes(s) -> Optional[str]:\n    \"\"\"Remove code inside fences.\n\n    Support both raw fences and python fences (with triple backquotes).\n\n    Return None if code fence is not closed.\n    \"\"\"\n\n    closed_fence_result = re.search(r\"(```(?:python\\n)?)([\\s\\S]*?)(```)\", s)\n\n    if closed_fence_result is not None:\n        return closed_fence_result.group(2)\n    else:\n        unclosed_fence_result = re.search(r\"```(.*?)\", s)\n\n        if unclosed_fence_result is not None:\n            return None\n        else:\n            return s", "\n\ndef strip_triple_backquotes(s) -> Optional[str]:\n    \"\"\"Remove code inside fences.\n\n    Support both raw fences and python fences (with triple backquotes).\n\n    Return None if code fence is not closed.\n    \"\"\"\n\n    closed_fence_result = re.search(r\"(```(?:python\\n)?)([\\s\\S]*?)(```)\", s)\n\n    if closed_fence_result is not None:\n        return closed_fence_result.group(2)\n    else:\n        unclosed_fence_result = re.search(r\"```(.*?)\", s)\n\n        if unclosed_fence_result is not None:\n            return None\n        else:\n            return s", "\n\ndef nb_tokens_in_prompt(prompt_messages: list[dict[str, str]]) -> int:\n    return sum(len(tokenizer.encode(s[\"content\"])) for s in prompt_messages) + MAX_TOKENS_MARGIN\n\n\ndef generate(\n    prompt_messages: list[dict[str, str]],\n    nb_solutions: int = 5,\n    max_new_tokens: Optional[int] = None,\n    temperature: float = 0.7,\n    model: str = \"gpt-3.5-turbo\",\n    max_attemps: int = 10,\n    only_accept_finished: bool = True,\n    sleep_time: float = 0,\n    max_sleep_time_when_fail: float = 30,\n):\n    if max_new_tokens is None:\n        assert model in TOKENS_PER_MODEL\n\n    # chat models\n    if model == \"gpt-3.5-turbo\" or model == \"gpt-4\":\n        for attempt in range(max_attemps):\n            try:\n                st = time.time()\n                max_tokens_possible = TOKENS_PER_MODEL[model] - nb_tokens_in_prompt(prompt_messages)\n                if max_new_tokens is None:\n                    max_new_tokens = max_tokens_possible\n                else:\n                    max_new_tokens = min(max_new_tokens, max_tokens_possible)\n\n                completion = openai.ChatCompletion.create(\n                    model=model,\n                    messages=prompt_messages,\n                    temperature=temperature,\n                    max_tokens=max_new_tokens,\n                    n=nb_solutions,\n                )\n                generations = [\n                    choice[\"message\"][\"content\"]\n                    for choice in completion[\"choices\"]\n                    if (choice[\"finish_reason\"] == \"stop\") or (not only_accept_finished)\n                ]\n                request_time = time.time() - st\n                if request_time < sleep_time:\n                    sleep(sleep_time - request_time)\n                break\n            except Exception as e:\n                generations = []\n                if attempt == max_attemps - 1:\n                    print(f\"Failed {max_attemps} times, last error: {e}\")\n                sleep(max_sleep_time_when_fail * 2 ** (-(max_attemps - attempt - 1)))\n    else:\n        raise ValueError(f\"Model {model} not supported\")\n\n    return generations", "\n\nT = TypeVar(\"T\")\n\n\ndef threaded_generations(\n    prompts_messages: list[tuple[T, list[dict[str, str]]]],\n    nb_solutions: int = 1,\n    max_new_tokens: Optional[int] = None,\n    temperature: float = 0.7,\n    model: str = \"gpt-3.5-turbo\",\n    max_attemps: int = 10,\n    only_accept_finished: bool = True,\n    n_threads: int = 5,\n) -> list[tuple[T, list[str]]]:\n    threads_results: list[list[tuple[T, list[str]]]] = [[] for _ in range(n_threads)]\n\n    pbar = tqdm(total=len(prompts_messages))\n    lock = threading.Lock()\n\n    active_threads = n_threads\n\n    def thread_task(thread_id: int):\n        nonlocal active_threads\n\n        start = thread_id * len(prompts_messages) // n_threads\n        end = (thread_id + 1) * len(prompts_messages) // n_threads\n        prompts_to_process = prompts_messages[start:end]\n        thread_results = []\n        for obj, prompt in prompts_to_process:\n            effective_max_rpm = min(MAX_RPM_PER_MODEL[model], MAX_TPM_PER_MODEL[model] / nb_tokens_in_prompt(prompt))\n\n            sleep_time = (60 / effective_max_rpm) * active_threads\n\n            generations = generate(\n                prompt, nb_solutions, max_new_tokens, temperature, model, max_attemps, only_accept_finished, sleep_time\n            )\n            thread_results.append((obj, generations))\n\n            with lock:\n                pbar.update(1)\n\n        threads_results[thread_id] = thread_results\n        with lock:\n            active_threads -= 1\n\n    threads = []\n    for thread_id in range(n_threads):\n        thread = threading.Thread(target=thread_task, args=(thread_id,))\n        thread.start()\n        threads.append(thread)\n\n    for thread in threads:\n        thread.join()\n\n    pbar.close()\n\n    return sum(threads_results, [])", ""]}
{"filename": "tuned_lens/nn/__init__.py", "chunked_list": ["\"\"\"A set of PyTorch modules for transforming the residual streams of models.\"\"\"\nfrom .lenses import Lens, LogitLens, TunedLens, TunedLensConfig\nfrom .unembed import (\n    InversionOutput,\n    Unembed,\n)\n"]}
{"filename": "tuned_lens/nn/lenses.py", "chunked_list": ["\"\"\"Provides lenses for decoding hidden states into logits.\"\"\"\nimport abc\nimport inspect\nimport json\nfrom copy import deepcopy\nfrom dataclasses import asdict, dataclass\nfrom logging import warning\nfrom pathlib import Path\nfrom typing import Dict, Generator, Optional, Union\n", "from typing import Dict, Generator, Optional, Union\n\nimport torch as th\nfrom transformers import PreTrainedModel\n\nfrom tuned_lens import load_artifacts\nfrom tuned_lens.nn.unembed import Unembed\n\n\nclass Lens(abc.ABC, th.nn.Module):\n    \"\"\"Abstract base class for all Lens.\"\"\"\n\n    unembed: Unembed\n\n    def __init__(self, unembed: Unembed):\n        \"\"\"Create a Lens.\n\n        Args:\n            unembed: The unembed operation to use.\n        \"\"\"\n        super().__init__()\n\n        self.unembed = unembed\n\n    @abc.abstractmethod\n    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"Convert a hidden state to the final hidden just before the unembeding.\n\n        Args:\n            h: The hidden state to convert.\n            idx: The layer of the transformer these hidden states come from.\n        \"\"\"\n        ...\n\n    @abc.abstractmethod\n    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"Decode hidden states into logits.\"\"\"\n        ...", "\nclass Lens(abc.ABC, th.nn.Module):\n    \"\"\"Abstract base class for all Lens.\"\"\"\n\n    unembed: Unembed\n\n    def __init__(self, unembed: Unembed):\n        \"\"\"Create a Lens.\n\n        Args:\n            unembed: The unembed operation to use.\n        \"\"\"\n        super().__init__()\n\n        self.unembed = unembed\n\n    @abc.abstractmethod\n    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"Convert a hidden state to the final hidden just before the unembeding.\n\n        Args:\n            h: The hidden state to convert.\n            idx: The layer of the transformer these hidden states come from.\n        \"\"\"\n        ...\n\n    @abc.abstractmethod\n    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"Decode hidden states into logits.\"\"\"\n        ...", "\n\nclass LogitLens(Lens):\n    \"\"\"Unembeds the residual stream into logits.\"\"\"\n\n    unembed: Unembed\n\n    def __init__(\n        self,\n        unembed: Unembed,\n    ):\n        \"\"\"Create a Logit Lens.\n\n        Args:\n            unembed: The unembed operation to use.\n        \"\"\"\n        super().__init__(unembed)\n\n    @classmethod\n    def from_model(\n        cls,\n        model: PreTrainedModel,\n    ) -> \"LogitLens\":\n        \"\"\"Create a LogitLens from a pretrained model.\n\n        Args:\n            model: A pretrained model from the transformers library you wish to inspect.\n        \"\"\"\n        unembed = Unembed(model)\n        return cls(unembed)\n\n    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"For the LogitLens, this is the identity function.\"\"\"\n        del idx\n        return h\n\n    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"Decode a hidden state into logits.\n\n        Args:\n            h: The hidden state to decode.\n            idx: the layer of the transformer these hidden states come from.\n        \"\"\"\n        del idx\n        return self.unembed.forward(h)", "\n\n@dataclass\nclass TunedLensConfig:\n    \"\"\"A configuration for a TunedLens.\"\"\"\n\n    # The name of the base model this lens was tuned for.\n    base_model_name_or_path: str\n    # The hidden size of the base model.\n    d_model: int\n    # The number of layers in the base model.\n    num_hidden_layers: int\n    # whether to use a bias in the linear translators.\n    bias: bool = True\n    # The revision of the base model this lens was tuned for.\n    base_model_revision: Optional[str] = None\n    # The hash of the base's unembed model this lens was tuned for.\n    unemebd_hash: Optional[str] = None\n    # The name of the lens type.\n    lens_type: str = \"linear_tuned_lens\"\n\n    def to_dict(self):\n        \"\"\"Convert this config to a dictionary.\"\"\"\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, config_dict: Dict):\n        \"\"\"Create a config from a dictionary.\"\"\"\n        config_dict = deepcopy(config_dict)\n        # Drop unrecognized config keys\n        unrecognized = set(config_dict) - set(inspect.getfullargspec(cls).args)\n        for key in unrecognized:\n            warning(f\"Ignoring config key '{key}'\")\n            del config_dict[key]\n\n        return cls(**config_dict)", "\n\nclass AddBias(th.nn.Module):\n    \"\"\"An elementwise affine transformation.\"\"\"\n\n    def __init__(self, d_model: int):\n        \"\"\"Create an elementwise affine transformation.\n\n        Args:\n            d_model: The dimensionality of the input.\n            bias: Whether to use a bias.\n        \"\"\"\n        super().__init__()\n\n        self.bias = th.nn.Parameter(th.zeros(d_model))\n\n    def forward(self, x: th.Tensor) -> th.Tensor:\n        \"\"\"Apply the affine transformation to the input.\n\n        Args:\n            x: The input to transform.\n        \"\"\"\n        return x + self.bias", "\n\nclass TunedLens(Lens):\n    \"\"\"A tuned lens for decoding hidden states into logits.\"\"\"\n\n    config: TunedLensConfig\n    unembed: Unembed\n    layer_translators: th.nn.ModuleList\n    probs_ln: th.nn.LayerNorm\n\n    def __init__(\n        self,\n        unembed: Unembed,\n        config: TunedLensConfig,\n    ):\n        \"\"\"Create a TunedLens.\n\n        Args:\n            unembed: The unembed operation to use.\n            config: The configuration for this lens.\n        \"\"\"\n        super().__init__(unembed)\n\n        self.config = config\n        unembed_hash = unembed.unembedding_hash()\n        config.unemebd_hash = unembed_hash\n\n        translator = th.nn.Linear(config.d_model, config.d_model, bias=config.bias)\n        # translator.weight.data.zero_()\n        # identity init\n        translator.weight.data.copy_(th.eye(config.d_model))\n        if config.bias:\n            translator.bias.data.zero_()\n\n        # Don't include the final layer since it does not need a translator\n        self.layer_translators = th.nn.ModuleList(\n            [\n                th.nn.utils.parametrizations.orthogonal(deepcopy(translator))\n                for _ in range(self.config.num_hidden_layers)\n            ]\n        )\n        self.vocab_size = unembed.unembedding.weight.shape[0]\n        self.probs_ln = AddBias(self.vocab_size)\n\n    def __getitem__(self, item: int) -> th.nn.Module:\n        \"\"\"Get the probe module at the given index.\"\"\"\n        return self.layer_translators[item]\n\n    def __iter__(self) -> Generator[th.nn.Module, None, None]:\n        \"\"\"Get iterator over the translators within the lens.\"\"\"\n        yield from self.layer_translators\n\n    @classmethod\n    def from_model(\n        cls,\n        model: PreTrainedModel,\n        model_revision: Optional[str] = None,\n        bias: bool = True,\n    ) -> \"TunedLens\":\n        \"\"\"Create a lens from a pretrained model.\n\n        Args:\n            model: The model to create the lens from.\n            model_revision: The git revision of the model to used.\n            bias: Whether to use a bias in the linear translators.\n\n        Returns:\n            A TunedLens instance.\n        \"\"\"\n        unembed = Unembed(model)\n        config = TunedLensConfig(\n            base_model_name_or_path=model.config.name_or_path,\n            base_model_revision=model_revision,\n            d_model=model.config.hidden_size,\n            num_hidden_layers=model.config.num_hidden_layers,\n            bias=bias,\n        )\n\n        return cls(unembed, config)\n\n    @classmethod\n    def from_model_and_pretrained(\n        cls,\n        model: PreTrainedModel,\n        lens_resource_id: Optional[str] = None,\n        **kwargs,\n    ) -> \"TunedLens\":\n        \"\"\"Load a tuned lens from a folder or hugging face hub.\n\n        Args:\n            model: The model to create the lens from.\n            lens_resource_id: The resource id of the lens to load. Defaults to the\n                model's name_or_path.\n            **kwargs: Additional arguments to pass to\n                :func:`tuned_lens.load_artifacts.load_lens_artifacts` and\n                `th.load <https://pytorch.org/docs/stable/generated/torch.load.html>`_.\n\n        Returns:\n            A TunedLens instance whose unembeding is derived from the given model\n            and whose layer translators are loaded from the given resource id.\n        \"\"\"\n        if lens_resource_id is None:\n            lens_resource_id = model.config.name_or_path\n\n        return cls.from_unembed_and_pretrained(Unembed(model), lens_resource_id, **kwargs)\n\n    @classmethod\n    def from_unembed_and_pretrained(\n        cls,\n        unembed: Unembed,\n        lens_resource_id: str,\n        **kwargs,\n    ) -> \"TunedLens\":\n        \"\"\"Load a tuned lens from a folder or hugging face hub.\n\n        Args:\n            unembed: The unembed operation to use for the lens.\n            lens_resource_id: The resource id of the lens to load.\n            **kwargs: Additional arguments to pass to\n                :func:`tuned_lens.load_artifacts.load_lens_artifacts` and\n                `th.load <https://pytorch.org/docs/stable/generated/torch.load.html>`_.\n\n        Returns:\n            A TunedLens instance.\n        \"\"\"\n        # Validate kwargs\n        load_artifact_varnames = load_artifacts.load_lens_artifacts.__code__.co_varnames\n\n        config_path, ckpt_path = load_artifacts.load_lens_artifacts(\n            resource_id=lens_resource_id,\n            **{k: v for k, v in kwargs.items() if k in load_artifact_varnames},\n        )\n\n        with open(config_path, \"r\") as f:\n            config = TunedLensConfig.from_dict(json.load(f))\n\n        # validate the unembed is the same as the one used to train the lens\n        if config.unemebd_hash and unembed.unembedding_hash() != config.unemebd_hash:\n            warning(\n                \"The unembeding matrix hash does not match the lens' hash.\"\n                \"This lens may have been trained with a different unembedding.\"\n            )\n\n        # Create the lens\n        lens = cls(unembed, config)\n\n        th_load_kwargs = {**{k: v for k, v in kwargs.items() if k not in load_artifact_varnames}}\n        # Load parameters\n        state = th.load(ckpt_path, **th_load_kwargs)\n\n        lens.layer_translators.load_state_dict(state[\"translators\"])\n        lens.probs_ln.load_state_dict(state[\"probs_ln\"])\n\n        return lens\n\n    def save(\n        self,\n        path: Union[Path, str],\n        ckpt: str = \"params.pt\",\n        config: str = \"config.json\",\n    ) -> None:\n        \"\"\"Save the lens to a directory.\n\n        Args:\n            path : The path to the directory to save the lens to.\n            ckpt : The name of the checkpoint file to save the parameters to.\n            config : The name of the config file to save the config to.\n        \"\"\"\n        path = Path(path)\n        path.mkdir(exist_ok=True, parents=True)\n        state_dict = {\n            \"translators\": self.layer_translators.state_dict(),\n            \"probs_ln\": self.probs_ln.state_dict(),\n        }\n\n        th.save(state_dict, path / ckpt)\n        with open(path / config, \"w\") as f:\n            json.dump(self.config.to_dict(), f)\n\n    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"Transform hidden state from layer `idx`.\"\"\"\n        # Note that we add the translator output residually, in contrast to the formula\n        # in the paper. By parametrizing it this way we ensure that weight decay\n        # regularizes the transform toward the identity, not the zero transformation.\n        return self[idx](h)\n        # return h + self[idx](h)\n\n    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"Transform and then decode the hidden states into logits.\"\"\"\n        h = self.transform_hidden(h, idx)\n        return self.unembed.forward(h)\n\n    def __len__(self) -> int:\n        \"\"\"Return the number of layer translators in the lens.\"\"\"\n        return len(self.layer_translators)\n\n    def reconstruct(self, probs: th.Tensor, idx: int) -> th.Tensor:\n        \"\"\"Reconstruct the hidden states from the logits.\"\"\"\n        scaled_probs = self.probs_ln(probs)\n\n        unembed_matrix = self.unembed.unembedding.weight  # (vocab_size, d_model)\n        translator_matrix = self.layer_translators[idx].weight  # (d_model, d_model)\n\n        return th.einsum(\"...v, vd, dh -> ...h\", scaled_probs, unembed_matrix, translator_matrix)", ""]}
{"filename": "tuned_lens/nn/unembed.py", "chunked_list": ["\"\"\"Provides a class for mapping transformer hidden states to logits (and vice versa).\"\"\"\nimport copy\nfrom dataclasses import dataclass\nfrom typing import Literal, Optional, cast\n\nimport torch as th\nfrom torch.distributions import Distribution\nfrom transformers import PreTrainedModel\n\nfrom tuned_lens import model_surgery", "\nfrom tuned_lens import model_surgery\nfrom tuned_lens.utils import tensor_hash\n\n\n@dataclass\nclass InversionOutput:\n    \"\"\"Output of `Unemebd.invert`.\"\"\"\n\n    preimage: th.Tensor\n    grad_norm: th.Tensor\n    kl: th.Tensor\n    loss: th.Tensor\n    nfev: int", "\n\nclass Unembed(th.nn.Module):\n    \"\"\"Module that maps transformer hidden states to logits (and vice versa).\"\"\"\n\n    final_norm: model_surgery.Norm\n    unembedding: th.nn.Linear\n\n    def __init__(\n        self,\n        model: PreTrainedModel,\n    ):\n        \"\"\"Initialize unmebed.\n\n        Args:\n            model: A HuggingFace model from which to extract the unembedding matrix.\n        \"\"\"\n        super().__init__()\n        final_norm = model_surgery.get_final_norm(model)\n\n        unembeding_matrix = model.get_output_embeddings()\n        if not isinstance(unembeding_matrix, th.nn.Linear):\n            # With nn.Linear we know that the unembedding matrix is .weight;\n            # we don't want to guess incorrectly for other module classes.\n            raise ValueError(\"Currently we only support nn.Linear unembeddings.\")\n\n        self.final_norm = copy.deepcopy(final_norm)\n        self.unembedding = copy.deepcopy(unembeding_matrix)\n\n        # In general we don't want to finetune the unembed operation.\n        self.requires_grad_(False)\n\n    def unembedding_hash(self) -> str:\n        \"\"\"Hash the unmbedding matrix to identify the model.\"\"\"\n        parameter = self.unembedding.weight.data.detach().cpu().numpy()\n        return tensor_hash(parameter)\n\n    def forward(self, h: th.Tensor) -> th.Tensor:\n        \"\"\"Convert hidden states into logits.\"\"\"\n        return self.unembedding(self.final_norm(h))\n\n    def invert(\n        self,\n        logits: th.Tensor,\n        *,\n        h0: Optional[th.Tensor] = None,\n        max_iter: int = 1000,\n        optimizer: Literal[\"lbfgs\", \"sgd\"] = \"lbfgs\",\n        prior_weight: float = 0.0,\n        prior: Optional[Distribution] = None,\n        step_size: float = 1.0,\n        tol: float = 1e-3,\n        weight: Optional[th.Tensor] = None,\n    ) -> InversionOutput:\n        \"\"\"Project logits onto the image of the unemebed operation.\n\n        When the hidden state dimension is smaller than the vocabulary size, the\n        unembed operation cannot perfectly represent arbitrary logits, since its image\n        is restricted to a subspace; this phenomenon is known as the softmax bottleneck\n        (cf. https://arxiv.org/abs/1711.03953). Because of this, the inverse can only\n        be approximate in general. Here, we use gradient-based optimization to find a\n        hidden state that minimizes the KL divergence from the target distribution p to\n        unembeded logits q(h): h* = argmin_h KL(p || q(h)).\n\n        Args:\n            logits: Tensor of shape `[..., vocab_size]` containing logits to invert.\n            h0: Initial guess for the hidden state. If `None`, the least-squares\n                solution of the linear equation xU = logits is used, where U is the\n                unembedding matrix.\n            max_iter: Maximum number of iterations for the optimizer to take.\n            optimizer: Optimization algorithm to use. Currently, only \"lbfgs\" and \"sgd\"\n                are supported.\n            prior_weight: The weight of the prior distribution is given in the loss.\n            prior: Prior distribution over hidden states used to regularize\n                the inversion.\n            step_size: The step size for the optimizer.\n            tol: Tolerance for the inversion objective.\n            weight: Optional tensor of shape `[..., vocab_size]` containing weights\n                for each vocabulary item. If `None`, all classes are weighted equally.\n        \"\"\"\n        d_model = cast(int, self.unembedding.in_features)\n        leading_dims = logits.shape[:-1]\n\n        if h0 is None:\n            # Initialize with the Moore-Penrose pseudoinverse\n            h0 = th.zeros((*leading_dims, d_model), device=logits.device)\n\n        # Sanity check the shape of the initial hidden state. Can silently lead to\n        # incorrect results due to broadcasting if we don't check this.\n        elif h0.shape != (*leading_dims, d_model):\n            raise ValueError(\n                f\"Initial hidden state has shape {h0.shape} but should have shape \"\n                f\"{(*leading_dims, d_model)} given logits shape {logits.shape}.\"\n            )\n\n        h_star = th.nn.Parameter(h0)\n        if optimizer == \"lbfgs\":\n            opt = th.optim.LBFGS(\n                [h_star],\n                line_search_fn=\"strong_wolfe\",\n                lr=step_size,\n                max_iter=max_iter,\n                tolerance_change=tol,\n            )\n        elif optimizer == \"sgd\":\n            opt = th.optim.SGD([h_star], lr=step_size)\n        else:\n            raise ValueError(f\"Unknown optimizer '{optimizer}'\")\n\n        log_p = logits.log_softmax(dim=-1)\n        p = log_p.exp()\n        if weight is not None:\n            p *= weight\n\n        def compute_loss(h: th.Tensor) -> tuple[th.Tensor, th.Tensor]:\n            log_q = self(h).log_softmax(-1)\n            kl = th.sum(p * (log_p - log_q), dim=-1).nanmean()\n            loss = kl.clone()\n\n            if prior_weight and prior is not None:\n                # We evaluate the prior density on the post-norm hidden state,\n                # to prevent the pre-norm hidden from collapsing towards zero.\n                h_ = self.final_norm(h)\n                loss += prior_weight * -prior.log_prob(h_).mean()\n\n            return loss, kl\n\n        nfev = 0  # Number of function evals, like in scipy.optimize.minimize\n        loss, kl = log_p.new_tensor(th.inf), log_p.new_tensor(th.inf)\n\n        def closure():\n            nonlocal nfev, loss, kl\n            nfev += 1\n\n            opt.zero_grad(set_to_none=False)\n            loss, kl = compute_loss(h_star)\n\n            if not loss.isfinite():\n                raise RuntimeError(\"Inversion objective is not finite.\")\n\n            loss.backward()\n            return loss\n\n        grad_norm = log_p.new_tensor(th.inf)\n        while nfev < max_iter:\n            opt.step(closure)  # type: ignore\n\n            final_grad = h_star.grad\n            assert final_grad is not None\n\n            grad_norm = final_grad.norm()\n            if grad_norm < tol or loss < tol:\n                break\n\n        with th.no_grad():\n            output = InversionOutput(\n                preimage=self.final_norm(h_star.data),\n                grad_norm=grad_norm,\n                kl=kl.detach(),\n                loss=loss.detach(),\n                nfev=nfev,\n            )\n\n        return output", ""]}
{"filename": "tuned_lens/stats/logit_stats.py", "chunked_list": ["\"\"\"Online MLE for the Dirichlet distribution from which logits are sampled.\"\"\"\nfrom typing import Optional\n\nimport torch as th\nfrom torch.distributions import Dirichlet\n\nfrom ..utils import maybe_all_reduce\n\n\nclass LogitStats:\n    \"\"\"Online MLE for the Dirichlet distribution from which logits are sampled.\n\n    Shape and device are lazily inferred from the first stream that is passed to\n    `update()`. Only a running mean of the log-likelihoods for each class is stored,\n    so memory use is negligible and constant in the number of samples. The maximum\n    likelihood distribution is computed on request using L-BFGS.\n    \"\"\"\n\n    n: int\n    marginal_probs: Optional[th.Tensor]\n    sufficient_stats: Optional[th.Tensor]\n\n    def __init__(self):\n        \"\"\"Create a LogitStats object.\"\"\"\n        self.n = 0\n        self.marginal_probs = None\n        self.sufficient_stats = None\n\n    def all_reduce_(self):\n        \"\"\"All-reduce the stats across all processes.\"\"\"\n        if self.sufficient_stats is not None:\n            maybe_all_reduce(self.sufficient_stats)\n\n    @th.no_grad()\n    def update(self, logits: th.Tensor, assume_normalized: bool = False):\n        \"\"\"Update the sufficient statistics with a new batch of logits.\"\"\"\n        K = logits.shape[-1]\n        logits = logits.reshape(-1, K).float()\n        if not assume_normalized:\n            logits = logits.log_softmax(dim=-1)\n\n        N = logits.shape[0]\n        if self.marginal_probs is None:\n            self.marginal_probs = logits.new_zeros(K)\n        if self.sufficient_stats is None:\n            self.sufficient_stats = logits.new_zeros(K)\n        elif self.sufficient_stats.shape[-1] != K:\n            raise ValueError(f\"Expected {self.sufficient_stats.shape[-1]} but got {K}\")\n\n        # Online mean update for the marginal probabilities\n        delta = logits.exp().mean(0) - self.marginal_probs\n        self.n += N\n        self.marginal_probs += delta * N / self.n\n\n        # Online mean update for the sufficient statistics\n        delta = logits.mean(0) - self.sufficient_stats\n        self.sufficient_stats += delta * N / self.n\n\n    def mle(self, max_iter: int = 100, tol: float = 1e-4) -> Dirichlet:\n        \"\"\"Compute the MLE for the Dirichlet generating the logits seen so far.\"\"\"\n        if self.sufficient_stats is None:\n            raise ValueError(\"No sufficient statistics available\")\n\n        log_alpha = th.nn.Parameter(th.zeros_like(self.sufficient_stats))\n        opt = th.optim.LBFGS(\n            [log_alpha],\n            line_search_fn=\"strong_wolfe\",\n            max_iter=max_iter,\n            tolerance_change=tol,\n        )\n\n        def closure():\n            opt.zero_grad(set_to_none=False)\n\n            # See http://jonathan-huang.org/research/dirichlet/dirichlet.pdf,\n            # page 5 for the formula\n            alpha = log_alpha.exp()\n            normalizer = alpha.sum().lgamma() - alpha.lgamma().sum()\n            loss = -(normalizer + (alpha - 1) @ self.sufficient_stats)\n            loss.backward()\n            return loss\n\n        opt.step(closure)  # type: ignore\n        return Dirichlet(log_alpha.data.exp())", "\nclass LogitStats:\n    \"\"\"Online MLE for the Dirichlet distribution from which logits are sampled.\n\n    Shape and device are lazily inferred from the first stream that is passed to\n    `update()`. Only a running mean of the log-likelihoods for each class is stored,\n    so memory use is negligible and constant in the number of samples. The maximum\n    likelihood distribution is computed on request using L-BFGS.\n    \"\"\"\n\n    n: int\n    marginal_probs: Optional[th.Tensor]\n    sufficient_stats: Optional[th.Tensor]\n\n    def __init__(self):\n        \"\"\"Create a LogitStats object.\"\"\"\n        self.n = 0\n        self.marginal_probs = None\n        self.sufficient_stats = None\n\n    def all_reduce_(self):\n        \"\"\"All-reduce the stats across all processes.\"\"\"\n        if self.sufficient_stats is not None:\n            maybe_all_reduce(self.sufficient_stats)\n\n    @th.no_grad()\n    def update(self, logits: th.Tensor, assume_normalized: bool = False):\n        \"\"\"Update the sufficient statistics with a new batch of logits.\"\"\"\n        K = logits.shape[-1]\n        logits = logits.reshape(-1, K).float()\n        if not assume_normalized:\n            logits = logits.log_softmax(dim=-1)\n\n        N = logits.shape[0]\n        if self.marginal_probs is None:\n            self.marginal_probs = logits.new_zeros(K)\n        if self.sufficient_stats is None:\n            self.sufficient_stats = logits.new_zeros(K)\n        elif self.sufficient_stats.shape[-1] != K:\n            raise ValueError(f\"Expected {self.sufficient_stats.shape[-1]} but got {K}\")\n\n        # Online mean update for the marginal probabilities\n        delta = logits.exp().mean(0) - self.marginal_probs\n        self.n += N\n        self.marginal_probs += delta * N / self.n\n\n        # Online mean update for the sufficient statistics\n        delta = logits.mean(0) - self.sufficient_stats\n        self.sufficient_stats += delta * N / self.n\n\n    def mle(self, max_iter: int = 100, tol: float = 1e-4) -> Dirichlet:\n        \"\"\"Compute the MLE for the Dirichlet generating the logits seen so far.\"\"\"\n        if self.sufficient_stats is None:\n            raise ValueError(\"No sufficient statistics available\")\n\n        log_alpha = th.nn.Parameter(th.zeros_like(self.sufficient_stats))\n        opt = th.optim.LBFGS(\n            [log_alpha],\n            line_search_fn=\"strong_wolfe\",\n            max_iter=max_iter,\n            tolerance_change=tol,\n        )\n\n        def closure():\n            opt.zero_grad(set_to_none=False)\n\n            # See http://jonathan-huang.org/research/dirichlet/dirichlet.pdf,\n            # page 5 for the formula\n            alpha = log_alpha.exp()\n            normalizer = alpha.sum().lgamma() - alpha.lgamma().sum()\n            loss = -(normalizer + (alpha - 1) @ self.sufficient_stats)\n            loss.backward()\n            return loss\n\n        opt.step(closure)  # type: ignore\n        return Dirichlet(log_alpha.data.exp())", ""]}
{"filename": "tuned_lens/stats/__init__.py", "chunked_list": ["\"\"\"Statistics for evaluating lens performance.\"\"\"\nfrom .distance import (\n    js_distance,\n    js_divergence,\n    kl_divergence,\n)\nfrom .logit_stats import LogitStats\n"]}
{"filename": "tuned_lens/stats/distance.py", "chunked_list": ["\"\"\"Various distance metrics for probability distributions.\"\"\"\nimport math\n\nimport torch as th\n\n\ndef js_divergence(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:\n    \"\"\"Compute the Jensen-Shannon divergence between two sets of logits.\n\n    Conceptually, the JSD is the info value of learning which of two distributions,\n    P or Q, that a random variable is drawn from, starting from a uniform prior over\n    P and Q. Since the entropy of a Bernoulli variable is at most ln(2), the JSD is\n    guaranteed to be in the range [0, ln(2)]. It is also symmetric and finite even\n    for distributions with disjoint supports.\n\n    Mathematically, the JSD is simply [KL(P || M) + KL(Q || M)] / 2, where M\n    is the mean of P and Q.\n    \"\"\"\n    log_p = logit_p.log_softmax(dim)\n    log_q = logit_q.log_softmax(dim)\n\n    # Mean of P and Q\n    log_m = th.stack([log_p, log_q]).sub(math.log(2)).logsumexp(0)\n\n    kl_p = th.sum(log_p.exp() * (log_p - log_m), dim)\n    kl_q = th.sum(log_q.exp() * (log_q - log_m), dim)\n    return 0.5 * (kl_p + kl_q)", "\n\ndef js_distance(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:\n    \"\"\"Compute the square root of the Jensen-Shannon divergence of two logit vectors.\"\"\"\n    return js_divergence(logit_p, logit_q, dim).sqrt()\n\n\ndef kl_divergence(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:\n    \"\"\"Compute the KL divergence between two sets of logits.\"\"\"\n    log_p = logit_p.log_softmax(dim)\n    log_q = logit_q.log_softmax(dim)\n    return th.sum(log_p.exp() * (log_p - log_q), dim)", "\n\ndef sqrtmh(x: th.Tensor) -> th.Tensor:\n    \"\"\"Unique PSD square root of a Hermitian positive semi-definite matrix.\"\"\"\n    dtype = x.dtype\n\n    # This is actually precision-sensitive\n    L, Q = th.linalg.eigh(x.double())\n    res = Q * L.clamp(0.0).sqrt() @ Q.mH\n    return res.to(dtype)", ""]}
