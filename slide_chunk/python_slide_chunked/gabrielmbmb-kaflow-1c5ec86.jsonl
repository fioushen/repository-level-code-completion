{"filename": "tests/test_serializers.py", "chunked_list": ["from __future__ import annotations\n\nfrom pydantic import BaseModel\n\nfrom kaflow.serializers import (\n    AvroSerializer,\n    JsonSerializer,\n    ProtobufSerializer,\n    Serializer,\n)", "    Serializer,\n)\nfrom tests.key_value_pb2 import KeyValue\n\n\nclass KeyValueModel(BaseModel):\n    key: str\n    value: str\n\n\ndef test_serializer_extra_annotation_keys() -> None:\n    assert Serializer.extra_annotations_keys() == []", "\n\ndef test_serializer_extra_annotation_keys() -> None:\n    assert Serializer.extra_annotations_keys() == []\n\n\ndef test_json_serializer_serialize() -> None:\n    serializer = JsonSerializer()\n    assert serializer.serialize({\"key\": \"value\"}) == b'{\"key\": \"value\"}'\n", "\n\ndef test_json_serializer_deserialize() -> None:\n    serializer = JsonSerializer()\n    assert serializer.deserialize(b'{\"key\": \"value\"}') == {\"key\": \"value\"}\n\n\ndef test_avro_serializer_serialize() -> None:\n    serializer = AvroSerializer(\n        avro_schema={\n            \"type\": \"record\",\n            \"name\": \"test\",\n            \"fields\": [\n                {\"name\": \"key\", \"type\": \"string\"},\n                {\"name\": \"value\", \"type\": \"string\"},\n            ],\n        }\n    )\n    assert (\n        serializer.serialize({\"key\": \"unit_test_key\", \"value\": \"unit_test_value\"})\n        == b\"\\x1aunit_test_key\\x1eunit_test_value\"\n    )", "\n\ndef test_avro_serializer_deserialize() -> None:\n    serializer = AvroSerializer(\n        avro_schema={\n            \"type\": \"record\",\n            \"name\": \"test\",\n            \"fields\": [\n                {\"name\": \"key\", \"type\": \"string\"},\n                {\"name\": \"value\", \"type\": \"string\"},\n            ],\n        }\n    )\n    assert serializer.deserialize(b\"\\x1aunit_test_key\\x1eunit_test_value\") == {\n        \"key\": \"unit_test_key\",\n        \"value\": \"unit_test_value\",\n    }", "\n\ndef test_avro_extra_annotation_keys() -> None:\n    assert AvroSerializer.extra_annotations_keys() == [\"avro_schema\"]\n\n\ndef test_protobuf_serializer_serialize() -> None:\n    serializer = ProtobufSerializer(protobuf_schema=KeyValue)\n    assert (\n        serializer.serialize({\"key\": \"unit_test_key\", \"value\": \"unit_test_value\"})\n        == b\"\\n\\runit_test_key\\x12\\x0funit_test_value\"\n    )", "\n\ndef test_protobuf_serializer_deserialize() -> None:\n    serializer = ProtobufSerializer(protobuf_schema=KeyValue)\n    assert serializer.deserialize(b\"\\n\\runit_test_key\\x12\\x0funit_test_value\") == {\n        \"key\": \"unit_test_key\",\n        \"value\": \"unit_test_value\",\n    }\n\n\ndef test_protobuf_extra_annotation_keys() -> None:\n    assert ProtobufSerializer.extra_annotations_keys() == [\"protobuf_schema\"]", "\n\ndef test_protobuf_extra_annotation_keys() -> None:\n    assert ProtobufSerializer.extra_annotations_keys() == [\"protobuf_schema\"]\n"]}
{"filename": "tests/test_application.py", "chunked_list": ["def test_application() -> None:\n    assert True\n"]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/key_value_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: key_value.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf.internal import builder as _builder\n\n# @@protoc_insertion_point(imports)", "\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n    b'\\n\\x0fkey_value.proto\\x12\\x06kaflow\"&\\n\\x08KeyValue\\x12\\x0b\\n\\x03key\\x18\\x01'\n    b\" \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\tb\\x06proto3\"\n)", "    b\" \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\tb\\x06proto3\"\n)\n\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, \"key_value_pb2\", globals())\nif _descriptor._USE_C_DESCRIPTORS is False:\n    DESCRIPTOR._options = None\n    _KEYVALUE._serialized_start = 27  # noqa\n    _KEYVALUE._serialized_end = 65  # noqa\n# @@protoc_insertion_point(module_scope)", "# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "tests/_utils/__init__.py", "chunked_list": [""]}
{"filename": "tests/_utils/test_asyncio.py", "chunked_list": ["import asyncio\n\nfrom kaflow._utils.asyncio import asyncify\n\n\ndef test_asyncify() -> None:\n    def func(a: int, b: int) -> int:\n        return a + b\n\n    func = asyncify(func)\n\n    assert asyncio.iscoroutinefunction(func)\n    assert asyncio.run(func(1, 2)) == 3", ""]}
{"filename": "tests/_utils/test_inspect.py", "chunked_list": ["import inspect\nfrom typing import Any, Callable\n\nimport pytest\nfrom typing_extensions import Annotated\n\nfrom kaflow._utils.inspect import (\n    annotated_param_with,\n    has_return_annotation,\n    is_annotated_param,", "    has_return_annotation,\n    is_annotated_param,\n    is_not_coroutine_function,\n)\n\n\ndef func_with_none_return_annotation() -> None:\n    pass\n\n\ndef func_with_no_return_annotation():\n    pass", "\n\ndef func_with_no_return_annotation():\n    pass\n\n\ndef func_with_return_annotation() -> int:\n    pass\n\n", "\n\nasync def coroutine() -> None:\n    pass\n\n\n@pytest.mark.parametrize(\n    \"param, expected\", [(Annotated[int, \"extra\", \"metadata\"], True), (int, False)]\n)\ndef test_is_annotated_param_true(param: Any, expected: bool) -> None:\n    assert is_annotated_param(param) == expected", ")\ndef test_is_annotated_param_true(param: Any, expected: bool) -> None:\n    assert is_annotated_param(param) == expected\n\n\nclass DummyUnitTest:\n    pass\n\n\n@pytest.mark.parametrize(", "\n@pytest.mark.parametrize(\n    \"param, annotated_with, expected\",\n    [\n        (Annotated[int, \"magic_unit_test_flag\"], \"magic_unit_test_flag\", True),\n        (Annotated[int, \"extra\", \"metadata\"], \"magic_unit_test_flag\", False),\n        (Annotated[int, DummyUnitTest()], DummyUnitTest, True),\n    ],\n)\ndef test_annotated_param_with(param: Any, annotated_with: Any, expected: bool) -> None:\n    assert annotated_param_with(annotated_with, param) == expected", ")\ndef test_annotated_param_with(param: Any, annotated_with: Any, expected: bool) -> None:\n    assert annotated_param_with(annotated_with, param) == expected\n\n\n@pytest.mark.parametrize(\n    \"func, expected\",\n    [\n        (func_with_none_return_annotation, False),\n        (func_with_no_return_annotation, False),", "        (func_with_none_return_annotation, False),\n        (func_with_no_return_annotation, False),\n        (func_with_return_annotation, True),\n    ],\n)\ndef test_has_return_annotation(func: Callable[..., Any], expected: bool) -> None:\n    signature = inspect.signature(func)\n    assert has_return_annotation(signature) == expected\n\n", "\n\n@pytest.mark.parametrize(\n    \"func, expected\", [(coroutine, False), (func_with_return_annotation, True)]\n)\ndef test_is_not_coroutine_function(func, expected: bool) -> None:\n    assert is_not_coroutine_function(func) == expected\n"]}
{"filename": "kaflow/logger.py", "chunked_list": ["import logging\n\nlogger = logging.getLogger(\"kaflow\")\n"]}
{"filename": "kaflow/dependencies.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import Any, Literal\n\nfrom di.dependent import Marker\n\nScope = Literal[\"app\", \"consumer\"]\nScopes = (\"app\", \"consumer\")\n\n\ndef Depends(\n    call: Any = None,\n    *,\n    use_cache: bool = True,\n    wire: bool = True,\n    scope: Scope | None = None,\n) -> Marker:\n    return Marker(\n        call=call,\n        use_cache=use_cache,\n        wire=wire,\n        scope=scope,\n    )", "\n\ndef Depends(\n    call: Any = None,\n    *,\n    use_cache: bool = True,\n    wire: bool = True,\n    scope: Scope | None = None,\n) -> Marker:\n    return Marker(\n        call=call,\n        use_cache=use_cache,\n        wire=wire,\n        scope=scope,\n    )", ""]}
{"filename": "kaflow/message.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import Any, NamedTuple, Union\n\nfrom kaflow.typing import TopicValueKeyHeader\n\n\nclass ReadMessage(NamedTuple):\n    value: Union[TopicValueKeyHeader, None] = None\n    key: Union[TopicValueKeyHeader, None] = None\n    headers: Union[dict[str, Any], None] = None\n    offset: Union[int, None] = None\n    partition: Union[int, None] = None\n    timestamp: Union[int, None] = None", "\n\nclass Message(NamedTuple):\n    value: Union[bytes, None] = None\n    key: Union[bytes, None] = None\n    headers: Union[dict[str, bytes], None] = None\n    offset: Union[int, None] = None\n    partition: Union[int, None] = None\n    timestamp: Union[int, None] = None\n", ""]}
{"filename": "kaflow/_consumer.py", "chunked_list": ["from __future__ import annotations\n\nimport asyncio\nfrom typing import TYPE_CHECKING, Any, Awaitable, Callable, Coroutine, Sequence\n\nfrom di.dependent import Dependent\nfrom di.executors import AsyncExecutor\nfrom pydantic import BaseModel\n\nfrom kaflow.dependencies import Scopes", "\nfrom kaflow.dependencies import Scopes\nfrom kaflow.exceptions import KaflowDeserializationException\nfrom kaflow.message import Message, ReadMessage\n\nif TYPE_CHECKING:\n    from aiokafka import ConsumerRecord\n    from di import Container, ScopeState\n\n    from kaflow.applications import ConsumerFunc, DeserializationErrorHandlerFunc\n    from kaflow.serializers import Serializer\n    from kaflow.typing import TopicValueKeyHeader", "\n\ndef _deserialize(\n    value: bytes,\n    param_type: type[TopicValueKeyHeader] | None,\n    deserializer: Serializer | None,\n) -> TopicValueKeyHeader:\n    if deserializer:\n        deserialized: dict[str, Any] = deserializer.deserialize(value)\n        if (\n            param_type is not None\n            and hasattr(param_type, \"__bases__\")\n            and BaseModel in param_type.__bases__\n        ):\n            return param_type(**deserialized)\n        return deserialized\n    return value", "\n\nclass TopicConsumerFunc:\n    __slots__ = (\n        \"name\",\n        \"container\",\n        \"publish_fn\",\n        \"exception_handlers\",\n        \"deserialization_error_handler\",\n        \"func\",\n        \"value_param_type\",\n        \"value_deserializer\",\n        \"key_param_type\",\n        \"key_deserializer\",\n        \"headers_type_deserializers\",\n        \"sink_topics\",\n        \"executor\",\n        \"container_state\",\n        \"dependent\",\n    )\n\n    def __init__(\n        self,\n        *,\n        name: str,\n        container: Container,\n        publish_fn: Callable[\n            [\n                str,\n                bytes | None,\n                bytes | None,\n                dict[str, bytes] | None,\n                int | None,\n                int | None,\n            ],\n            Coroutine[Any, Any, None],\n        ],\n        exception_handlers: dict[type[Exception], Callable[..., Awaitable[None]]],\n        deserialization_error_handler: DeserializationErrorHandlerFunc | None = None,\n        func: ConsumerFunc,\n        value_param_type: type[TopicValueKeyHeader],\n        value_deserializer: Serializer | None = None,\n        key_param_type: type[TopicValueKeyHeader] | None = None,\n        key_deserializer: Serializer | None = None,\n        headers_type_deserializers: dict[\n            str, tuple[type[TopicValueKeyHeader], Serializer | None]\n        ]\n        | None = None,\n        sink_topics: Sequence[str] | None = None,\n    ) -> None:\n        self.name = name\n        self.container = container\n        self.publish_fn = publish_fn\n        self.exception_handlers = exception_handlers\n        self.deserialization_error_handler = deserialization_error_handler\n        self.func = func\n        self.value_param_type = value_param_type\n        self.value_deserializer = value_deserializer\n        self.key_param_type = key_param_type\n        self.key_deserializer = key_deserializer\n        self.headers_type_deserializers = headers_type_deserializers\n        self.sink_topics = sink_topics\n        self.executor = AsyncExecutor()\n\n    def prepare(self, state: ScopeState) -> None:\n        self.container_state = state\n        self.dependent = self.container.solve(\n            Dependent(self.func, scope=\"consumer\"), scopes=Scopes\n        )\n\n    def _deserialize_value(self, value: bytes) -> TopicValueKeyHeader:\n        return _deserialize(value, self.value_param_type, self.value_deserializer)\n\n    def _deserialize_key(self, key: bytes) -> TopicValueKeyHeader | None:\n        if self.key_param_type:\n            return _deserialize(key, self.key_param_type, self.key_deserializer)\n        return None\n\n    def _deserialize_headers(\n        self, headers: Sequence[tuple[str, bytes]]\n    ) -> dict[str, Any] | None:\n        if self.headers_type_deserializers:\n            headers_ = {}\n            for key, value in headers:\n                header_type, deserializer = self.headers_type_deserializers.get(\n                    key, (None, None)\n                )\n                headers_[key] = _deserialize(value, header_type, deserializer)\n            return headers_\n        return None\n\n    async def _deserialize(\n        self, record: ConsumerRecord\n    ) -> tuple[\n        TopicValueKeyHeader | None,\n        TopicValueKeyHeader | None,\n        dict[str, TopicValueKeyHeader] | None,\n        bool,\n    ]:\n        async def handle_deserialization_error(\n            error_message: str, record: ConsumerRecord, exception: Exception\n        ) -> None:\n            exc = KaflowDeserializationException(\n                error_message.format(self.name), record=record\n            )\n            if not self.deserialization_error_handler:\n                raise exc from exception\n            await self.deserialization_error_handler(exc)\n\n        deserialized = True\n        try:\n            value = self._deserialize_value(record.value)\n        except Exception as e:\n            await handle_deserialization_error(\n                (\n                    \"Failed to deserialize value of message comming from topic\"\n                    f\" `{self.name}`\"\n                ),\n                record,\n                e,\n            )\n            value = None\n            deserialized = False\n\n        try:\n            key = self._deserialize_key(record.key)\n        except Exception as e:\n            await handle_deserialization_error(\n                (\n                    \"Failed to deserialize key of message comming from topic\"\n                    f\" `{self.name}`\"\n                ),\n                record,\n                e,\n            )\n            key = None\n            deserialized = False\n\n        try:\n            headers = self._deserialize_headers(record.headers)\n        except Exception as e:\n            await handle_deserialization_error(\n                (\n                    \"Failed to deserialize headers of message comming from topic\"\n                    f\" `{self.name}`\"\n                ),\n                record,\n                e,\n            )\n            headers = None\n            deserialized\n\n        return value, key, headers, deserialized\n\n    def _lookup_exception_handler(\n        self, exc: Exception\n    ) -> Callable[..., Awaitable[None]] | None:\n        for cls in type(exc).__mro__:\n            if cls in self.exception_handlers:\n                return self.exception_handlers[cls]\n        return None\n\n    async def _execute_dependent(\n        self,\n        consumer_state: ScopeState,\n        message: ReadMessage,\n    ) -> Any:\n        try:\n            return await self.dependent.execute_async(\n                executor=self.executor,\n                state=consumer_state,\n                values={ReadMessage: message},\n            )\n        except tuple(self.exception_handlers.keys()) as e:\n            handler = self._lookup_exception_handler(e)\n            if not handler:\n                raise e\n            await handler(e)\n            return None\n\n    async def _publish_messages(self, message: Message) -> None:\n        if self.sink_topics:\n            await asyncio.gather(\n                *[\n                    self.publish_fn(\n                        topic,\n                        message.value,\n                        message.key,\n                        message.headers,\n                        message.partition,\n                        message.offset,\n                    )\n                    for topic in self.sink_topics\n                ]\n            )\n\n    async def _process(self, read_message: ReadMessage) -> Message | None:\n        async with self.container.enter_scope(\n            \"consumer\", state=self.container_state\n        ) as consumer_state:\n            message = await self._execute_dependent(\n                consumer_state=consumer_state, message=read_message\n            )\n        if message and isinstance(message, Message):\n            await self._publish_messages(message)\n            return message\n        return None\n\n    async def consume(self, record: ConsumerRecord) -> Message | None:\n        value, key, headers, deserialized = await self._deserialize(record)\n        if not deserialized:\n            return None\n        message = ReadMessage(\n            value=value,\n            key=key,\n            headers=headers,\n            offset=record.offset,\n            partition=record.partition,\n            timestamp=record.timestamp,\n        )\n        return await self._process(read_message=message)", ""]}
{"filename": "kaflow/__init__.py", "chunked_list": ["from kaflow.applications import Kaflow\nfrom kaflow.dependencies import Depends\nfrom kaflow.message import Message\nfrom kaflow.parameters import (\n    FromHeader,\n    FromKey,\n    FromValue,\n    MessageOffset,\n    MessagePartition,\n    MessageTimestamp,", "    MessagePartition,\n    MessageTimestamp,\n)\nfrom kaflow.serializers import Json, String, has_fastavro, has_protobuf\n\n__all__ = [\n    \"Kaflow\",\n    \"Depends\",\n    \"Message\",\n    \"FromHeader\",", "    \"Message\",\n    \"FromHeader\",\n    \"FromKey\",\n    \"FromValue\",\n    \"MessageOffset\",\n    \"MessagePartition\",\n    \"MessageTimestamp\",\n    \"Json\",\n    \"String\",\n]", "    \"String\",\n]\n\nif has_fastavro:\n    from kaflow.serializers import Avro  # noqa: F401\n\n    __all__.append(\"Avro\")\n\nif has_protobuf:\n    from kaflow.serializers import Protobuf  # noqa: F401\n\n    __all__.append(\"Protobuf\")", "if has_protobuf:\n    from kaflow.serializers import Protobuf  # noqa: F401\n\n    __all__.append(\"Protobuf\")\n\n__version__ = \"0.2.2\"\n"]}
{"filename": "kaflow/typing.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import Union\n\nfrom pydantic import BaseModel\n\nTopicValueKeyHeader = Union[bytes, object, BaseModel]\n"]}
{"filename": "kaflow/serializers.py", "chunked_list": ["from __future__ import annotations\n\nimport io\nimport json\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Any, TypeVar, cast\n\nfrom typing_extensions import Annotated\n\nif TYPE_CHECKING:\n    pass", "\nif TYPE_CHECKING:\n    pass\n\ntry:\n    import fastavro\n\n    has_fastavro = True\nexcept ImportError:\n    has_fastavro = False", "\ntry:\n    from google.protobuf import json_format\n\n    has_protobuf = True\nexcept ImportError:\n    has_protobuf = False\n\nMESSAGE_SERIALIZER_FLAG = \"MessageSerializer\"\n", "MESSAGE_SERIALIZER_FLAG = \"MessageSerializer\"\n\nT = TypeVar(\"T\")\n\n\nclass Serializer(ABC):\n    @abstractmethod\n    def serialize(self, data: Any) -> bytes:\n        ...\n\n    @abstractmethod\n    def deserialize(self, data: bytes) -> Any:\n        ...\n\n    @staticmethod\n    def extra_annotations_keys() -> list[str]:\n        return []", "\n\nclass StringSerializer(Serializer):\n    def __init__(self, **kwargs: Any) -> None:\n        pass\n\n    def serialize(self, data: Any) -> bytes:\n        return str(data).encode()\n\n    def deserialize(self, data: bytes) -> Any:\n        return data.decode()", "\n\nString = Annotated[T, StringSerializer, MESSAGE_SERIALIZER_FLAG]\n\n\nclass JsonSerializer(Serializer):\n    def __init__(self, **kwargs: Any) -> None:\n        pass\n\n    def serialize(self, data: Any) -> bytes:\n        return json.dumps(data).encode()\n\n    def deserialize(self, data: bytes) -> Any:\n        return json.loads(data)", "\n\nJson = Annotated[T, JsonSerializer, MESSAGE_SERIALIZER_FLAG]\n\n\nif has_fastavro:\n\n    class AvroSerializer(Serializer):\n        def __init__(\n            self,\n            avro_schema: dict[str, Any] | None = None,\n            include_schema: bool = False,\n            seek_offset: int | None = None,\n            **kwargs: Any,\n        ) -> None:\n            self.avro_schema = avro_schema\n            self.include_schema = include_schema\n            self.seek_offset = seek_offset\n\n        def serialize(self, data: Any) -> bytes:\n            bytes_io = io.BytesIO()\n            if self.include_schema:\n                fastavro.writer(bytes_io, self.avro_schema, [data])\n            else:\n                fastavro.schemaless_writer(bytes_io, self.avro_schema, data)\n            return bytes_io.getvalue()\n\n        def deserialize(self, data: bytes) -> Any:\n            bytes_io = io.BytesIO(data)\n            if self.seek_offset is not None:\n                bytes_io.seek(self.seek_offset)\n            if self.avro_schema:\n                return fastavro.schemaless_reader(bytes_io, self.avro_schema)\n            return list(fastavro.reader(bytes_io))[0]\n\n        @staticmethod\n        def extra_annotations_keys() -> list[str]:\n            return [\"avro_schema\"]\n\n    Avro = Annotated[T, AvroSerializer, MESSAGE_SERIALIZER_FLAG]", "\nif has_protobuf:\n\n    class ProtobufSerializer(Serializer):\n        def __init__(self, protobuf_schema: type[Any], **kwargs: Any) -> None:\n            self.protobuf_schema = protobuf_schema\n\n        def serialize(self, data: Any) -> bytes:\n            entity = self.protobuf_schema()\n            for key, value in data.items():\n                setattr(entity, key, value)\n            return cast(bytes, entity.SerializeToString())\n\n        def deserialize(self, data: bytes) -> Any:\n            entity = self.protobuf_schema()\n            entity.ParseFromString(data)\n            return json_format.MessageToDict(entity)\n\n        @staticmethod\n        def extra_annotations_keys() -> list[str]:\n            return [\"protobuf_schema\"]\n\n    Protobuf = Annotated[T, ProtobufSerializer, MESSAGE_SERIALIZER_FLAG]", ""]}
{"filename": "kaflow/testclient.py", "chunked_list": ["from __future__ import annotations\n\nimport asyncio\nfrom functools import wraps\nfrom time import time\nfrom typing import TYPE_CHECKING, Any, Awaitable, Callable\n\nfrom aiokafka import ConsumerRecord\n\nif TYPE_CHECKING:\n    from kaflow.applications import Kaflow\n    from kaflow.message import Message", "\nif TYPE_CHECKING:\n    from kaflow.applications import Kaflow\n    from kaflow.message import Message\n\n\ndef intercept_publish(\n    func: Callable[..., Awaitable[None]]\n) -> Callable[..., Awaitable[None]]:\n    @wraps(func)\n    async def wrapper(*args: Any, **kwargs: Any) -> None:\n        pass\n\n    return wrapper", "\n\nclass TestClient:\n    \"\"\"Test client for testing a `Kaflow` application.\"\"\"\n\n    def __init__(self, app: Kaflow) -> None:\n        self.app = app\n        self.app._publish = intercept_publish(self.app._publish)  # type: ignore\n        self._loop = asyncio.get_event_loop()\n\n    def publish(\n        self,\n        topic: str,\n        value: bytes,\n        key: bytes | None = None,\n        headers: dict[str, bytes] | None = None,\n        partition: int = 0,\n        offset: int = 0,\n        timestamp: int | None = None,\n    ) -> Message | None:\n        if timestamp is None:\n            timestamp = int(time())\n        record = ConsumerRecord(\n            topic=topic,\n            partition=partition,\n            offset=offset,\n            timestamp=timestamp,\n            timestamp_type=0,\n            key=key,\n            value=value,\n            checksum=0,\n            serialized_key_size=len(key) if key else 0,\n            serialized_value_size=len(value),\n            headers=headers,\n        )\n\n        async def _publish() -> Message | None:\n            consumer = self.app._get_consumer(topic)\n            async with self.app.lifespan():\n                return await consumer.consume(record)\n\n        return self._loop.run_until_complete(_publish())", ""]}
{"filename": "kaflow/parameters.py", "chunked_list": ["from __future__ import annotations\n\nimport inspect\nfrom typing import TYPE_CHECKING, Any, TypeVar\n\nfrom di.dependent import Dependent, Marker\nfrom typing_extensions import Annotated\n\nfrom kaflow._utils.inspect import annotated_param_with\nfrom kaflow.message import ReadMessage", "from kaflow._utils.inspect import annotated_param_with\nfrom kaflow.message import ReadMessage\nfrom kaflow.serializers import Serializer\n\nif TYPE_CHECKING:\n    from kaflow.applications import ConsumerFunc\n    from kaflow.typing import TopicValueKeyHeader\n\nFROM_VALUE_FLAG = \"from_value\"\nFROM_KEY_FLAG = \"from_key\"", "FROM_VALUE_FLAG = \"from_value\"\nFROM_KEY_FLAG = \"from_key\"\nFROM_HEADER_FLAG = \"from_header\"\n\n\ndef _get_serializer_info(param: Any) -> tuple[type[Serializer] | None, dict[str, Any]]:\n    serializer: type[Serializer] | None = None\n    serializer_extra: dict[str, Any] = {}\n    for item in param.__metadata__:\n        if inspect.isclass(item) and issubclass(item, Serializer):\n            serializer = item\n        elif isinstance(item, dict):\n            serializer_extra = item\n    return serializer, serializer_extra", "\n\ndef _annotated_serializer_info(\n    param: tuple[str, Any], func_name: str\n) -> tuple[type[TopicValueKeyHeader], type[Serializer] | None, dict[str, Any]]:\n    \"\"\"Get the type and serializer of a parameter annotated (`Annotated[...]`) with\n    a Kaflow serializer. This function expect the parameter to be `Annotated` with a\n    Kaflow serializer, the `kaflow.serializers.MESSAGE_SERIALIZER_FLAG`, and optionally\n    with a `dict` containing extra information that could be used by the serializer.\n\n    Args:\n        param: the annotated parameter to get the type and serializer from.\n        func_name: the name of the function that contains the annotated parameter.\n\n    Returns:\n        A tuple with the type and serializer of the annotated parameter, and a `dict`\n        containing extra information that could be used by the serializer.\n    \"\"\"\n    param_type = param[1].__args__[0]\n    serializer, serializer_extra = _get_serializer_info(param[1])\n    if not serializer and param_type is not bytes:\n        raise ValueError(\n            f\"'{param[0]}' parameter of '{func_name}' function has not been annotated\"\n            \" with a Kaflow serializer and its type is not `bytes`. Please annotate\"\n            \" the parameter with a Kaflow serializer or use `bytes` as the type.\"\n        )\n    return param_type, serializer, serializer_extra", "\n\ndef _serializer_info(\n    param: tuple[str, Any], func_name: str\n) -> tuple[type[TopicValueKeyHeader], Serializer | None]:\n    param_type, serializer, serializer_extra = _annotated_serializer_info(\n        param, func_name\n    )\n    if serializer:\n        return param_type, serializer(**serializer_extra)\n    return param_type, None", "\n\ndef _get_params(signature: inspect.Signature) -> dict[str, list[tuple[str, Any]]]:\n    params: dict[str, list[Any]] = {\n        FROM_VALUE_FLAG: [],\n        FROM_KEY_FLAG: [],\n        FROM_HEADER_FLAG: [],\n    }\n    for param in signature.parameters.values():\n        if annotated_param_with(Value, param.annotation):\n            params[FROM_VALUE_FLAG].append((param.name, param.annotation))\n        elif annotated_param_with(Key, param.annotation):\n            params[FROM_KEY_FLAG].append((param.name, param.annotation))\n        elif annotated_param_with(Header, param.annotation):\n            params[FROM_HEADER_FLAG].append((param.name, param.annotation))\n    return params", "\n\ndef _get_from_value_param_info(\n    params: dict[str, list[tuple[str, Any]]], func_name: str\n) -> tuple[type[TopicValueKeyHeader], Serializer | None]:\n    if not params[FROM_VALUE_FLAG]:\n        raise ValueError(\n            f\"'{func_name}' function does not have a parameter annotated\"\n            \" `FromValue` to receive the value of the message from the topic.\"\n        )\n    if len(params[FROM_VALUE_FLAG]) > 1:\n        raise ValueError(\n            f\"'{func_name}' function has more than one parameter\"\n            \" annotated `FromValue`. Only one parameter can be annotated\"\n            \" `FromValue` to receive the value of the message from the topic.\"\n        )\n    return _serializer_info(params[FROM_VALUE_FLAG][0], func_name)", "\n\ndef _get_from_key_param_info(\n    params: dict[str, list[tuple[str, Any]]], func_name: str\n) -> tuple[type[TopicValueKeyHeader] | None, Serializer | None]:\n    key_param_type: type[TopicValueKeyHeader] | None = None\n    key_serializer: Serializer | None = None\n    if params[FROM_KEY_FLAG]:\n        key_param_type, key_serializer = _serializer_info(\n            param=params[FROM_KEY_FLAG][0], func_name=func_name\n        )\n    return key_param_type, key_serializer", "\n\ndef _get_from_headers_param_info(\n    params: dict[str, list[Any]], func_name: str\n) -> dict[str, tuple[type[TopicValueKeyHeader], Serializer | None]] | None:\n    headers = {}\n    for header_param in params[FROM_HEADER_FLAG]:\n        name, _ = header_param\n        (header_param_type, header_serializer) = _serializer_info(\n            header_param, func_name\n        )\n        headers[name] = (header_param_type, header_serializer)\n    if headers:\n        return headers\n    return None", "\n\ndef get_function_parameters_info(\n    func: ConsumerFunc,\n) -> tuple[\n    type[TopicValueKeyHeader],\n    Serializer | None,\n    type[TopicValueKeyHeader] | None,\n    Serializer | None,\n    dict[str, tuple[type[TopicValueKeyHeader], Serializer | None]] | None,\n]:\n    signature = inspect.signature(func)\n    params = _get_params(signature)\n    value_param_type, value_serializer = _get_from_value_param_info(\n        params=params, func_name=func.__name__\n    )\n    key_param_type, key_serializer = _get_from_key_param_info(\n        params=params, func_name=func.__name__\n    )\n    headers_type_serializers = _get_from_headers_param_info(\n        params=params, func_name=func.__name__\n    )\n    return (\n        value_param_type,\n        value_serializer,\n        key_param_type,\n        key_serializer,\n        headers_type_serializers,\n    )", "\n\nclass _MessageAttr(Marker):\n    def __init__(self, attr_name: str) -> None:\n        self.attr_name = attr_name\n\n    def register_parameter(self, param: inspect.Parameter) -> Dependent[Any]:\n        def get_value(message: Annotated[ReadMessage, Marker(scope=\"consumer\")]) -> Any:\n            return getattr(message, self.attr_name)\n\n        return Dependent(get_value, scope=\"consumer\", use_cache=False)", "\n\nclass Value(_MessageAttr):\n    def __init__(self) -> None:\n        super().__init__(\"value\")\n\n\nclass Key(_MessageAttr):\n    def __init__(self) -> None:\n        super().__init__(\"key\")", "\n\nclass Header(Marker):\n    def __init__(self, alias: str | None = None) -> None:\n        self.alias = alias\n        super().__init__(call=None, scope=\"consumer\", use_cache=False)\n\n    def register_parameter(self, param: inspect.Parameter) -> Dependent[Any]:\n        if self.alias:\n            name = self.alias\n        else:\n            name = param.name\n\n        def get_header(\n            message: Annotated[ReadMessage, Marker(scope=\"consumer\")]\n        ) -> Any:\n            if message.headers:\n                return message.headers.get(name)\n            return None\n\n        return Dependent(get_header, scope=\"consumer\")", "\n\nclass Partition(_MessageAttr):\n    def __init__(self) -> None:\n        super().__init__(\"partition\")\n\n\nclass Timestamp(_MessageAttr):\n    def __init__(self) -> None:\n        super().__init__(\"timestamp\")", "\n\nclass Offset(_MessageAttr):\n    def __init__(self) -> None:\n        super().__init__(\"offset\")\n\n\n_T = TypeVar(\"_T\")\nFromValue = Annotated[_T, Value()]\nFromKey = Annotated[_T, Key()]", "FromValue = Annotated[_T, Value()]\nFromKey = Annotated[_T, Key()]\nFromHeader = Annotated[_T, Header()]\nMessageOffset = Annotated[int, Offset()]\nMessagePartition = Annotated[int, Partition()]\nMessageTimestamp = Annotated[int, Timestamp()]\n"]}
{"filename": "kaflow/exceptions.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from aiokafka import ConsumerRecord\n\n\nclass KaflowException(Exception):\n    ...", "class KaflowException(Exception):\n    ...\n\n\nclass KaflowDeserializationException(KaflowException):\n    def __init__(self, message: str, record: ConsumerRecord) -> None:\n        super().__init__(message)\n        self.record = record\n", ""]}
{"filename": "kaflow/applications.py", "chunked_list": ["from __future__ import annotations\n\nimport asyncio\nimport inspect\nfrom collections import defaultdict\nfrom contextlib import asynccontextmanager\nfrom functools import wraps\nfrom typing import (\n    TYPE_CHECKING,\n    Any,", "    TYPE_CHECKING,\n    Any,\n    AsyncContextManager,\n    AsyncIterator,\n    Awaitable,\n    Callable,\n    Coroutine,\n    Literal,\n    Sequence,\n    Union,", "    Sequence,\n    Union,\n)\nfrom uuid import uuid4\n\nfrom aiokafka import AIOKafkaConsumer, AIOKafkaProducer\nfrom aiokafka.helpers import create_ssl_context\nfrom di import Container, ScopeState\nfrom di.dependent import Dependent\nfrom di.executors import AsyncExecutor", "from di.dependent import Dependent\nfrom di.executors import AsyncExecutor\nfrom kafka.coordinator.assignors.roundrobin import RoundRobinPartitionAssignor\nfrom kafka.partitioner.default import DefaultPartitioner\n\nfrom kaflow import parameters\nfrom kaflow._consumer import TopicConsumerFunc\nfrom kaflow._utils.asyncio import asyncify\nfrom kaflow._utils.inspect import is_not_coroutine_function\nfrom kaflow._utils.overrides import DependencyOverrideManager", "from kaflow._utils.inspect import is_not_coroutine_function\nfrom kaflow._utils.overrides import DependencyOverrideManager\nfrom kaflow.dependencies import Scopes\nfrom kaflow.exceptions import KaflowDeserializationException\nfrom kaflow.message import Message\n\nif TYPE_CHECKING:\n    from aiokafka.abc import AbstractTokenProvider\n    from kafka.coordinator.assignors.abstract import AbstractPartitionAssignor\n\n    from kaflow.asyncapi.models import AsyncAPI\n    from kaflow.serializers import Serializer\n    from kaflow.typing import TopicValueKeyHeader", "\nConsumerFunc = Callable[..., Union[Message, Awaitable[Union[Message, None]], None]]\nProducerFunc = Callable[..., Union[Message, Awaitable[Message]]]\nExceptionHandlerFunc = Callable[[Exception], Awaitable]\nDeserializationErrorHandlerFunc = Callable[[KaflowDeserializationException], Awaitable]\n\n\nclass Kaflow:\n    def __init__(\n        self,\n        brokers: str | list[str],\n        client_id: str | None = None,\n        group_id: str | None = None,\n        acks: Literal[0, 1, \"all\"] = 1,\n        compression_type: Literal[\"gzip\", \"snappy\", \"lz4\", \"zstd\", None] = None,\n        max_batch_size: int = 16384,\n        partitioner: Callable[\n            [bytes, list[int], list[int]], int\n        ] = DefaultPartitioner(),\n        max_request_size: int = 1048576,\n        linger_ms: int = 0,\n        send_backoff_ms: int = 100,\n        connections_max_idle_ms: int = 540000,\n        enable_idempotence: bool = False,\n        transactional_id: str | None = None,\n        transaction_timeout_ms: int = 60000,\n        fetch_max_wait_ms: int = 500,\n        fetch_max_bytes: int = 52428800,\n        fetch_min_bytes: int = 1,\n        max_partition_fetch_bytes: int = 1 * 1024 * 1024,\n        request_timeout_ms: int = 40 * 1000,\n        retry_backoff_ms: int = 100,\n        auto_offset_reset: Literal[\"earliest\", \"latest\", \"none\"] = \"latest\",\n        enable_auto_commit: bool = True,\n        auto_commit_interval_ms: int = 5000,\n        check_crcs: bool = True,\n        metadata_max_age_ms: int = 5 * 60 * 1000,\n        partition_assignment_strategy: list[AbstractPartitionAssignor] | None = None,\n        max_poll_interval_ms: int = 300000,\n        rebalance_timeout_ms: int | None = None,\n        session_timeout_ms: int = 10000,\n        heartbeat_interval_ms: int = 3000,\n        consumer_timeout_ms: int = 200,\n        max_poll_records: int | None = None,\n        kafka_api_version: str = \"auto\",\n        security_protocol: Literal[\n            \"PLAINTEXT\", \"SSL\", \"SASL_PLAINTEXT\", \"SASL_SSL\"\n        ] = \"PLAINTEXT\",\n        exclude_internal_topics: bool = True,\n        connection_max_idle_ms: int = 540000,\n        isolation_level: Literal[\n            \"read_committed\", \"read_uncommitted\"\n        ] = \"read_committed\",\n        cafile: str | None = None,\n        capath: str | None = None,\n        cadata: bytes | None = None,\n        certfile: str | None = None,\n        keyfile: str | None = None,\n        cert_password: str | None = None,\n        sasl_mechanism: Literal[\n            \"PLAIN\", \"GSSAPI\", \"OAUTHBEARER\", \"SCRAM-SHA-256\", \"SCRAM-SHA-512\"\n        ]\n        | None = None,\n        sasl_plain_username: str | None = None,\n        sasl_plain_password: str | None = None,\n        sasl_kerberos_service_name: str = \"kafka\",\n        sasl_kerberos_domain_name: str | None = None,\n        sasl_oauth_token_provider: AbstractTokenProvider | None = None,\n        lifespan: Callable[..., AsyncContextManager[None]] | None = None,\n        asyncapi_version: str = \"2.6.0\",\n        title: str = \"Kaflow\",\n        version: str = \"0.0.1\",\n        description: str | None = None,\n        terms_of_service: str | None = None,\n        contact: dict[str, str | Any] | None = None,\n        license_info: dict[str, str | Any] | None = None,\n    ) -> None:\n        # AIOKafka\n        self.brokers = brokers\n        self.client_id = client_id or f\"kaflow-{uuid4()}\"\n        self.group_id = group_id\n        self.acks = acks\n        self.compression_type = compression_type\n        self.max_batch_size = max_batch_size\n        self.partitioner = partitioner\n        self.max_request_size = max_request_size\n        self.linger_ms = linger_ms\n        self.send_backoff_ms = send_backoff_ms\n        self.connections_max_idle_ms = connections_max_idle_ms\n        self.enable_idempotence = enable_idempotence\n        self.transactional_id = transactional_id\n        self.transaction_timeout_ms = transaction_timeout_ms\n        self.fetch_max_wait_ms = fetch_max_wait_ms\n        self.fetch_max_bytes = fetch_max_bytes\n        self.fetch_min_bytes = fetch_min_bytes\n        self.max_partition_fetch_bytes = max_partition_fetch_bytes\n        self.request_timeout_ms = request_timeout_ms\n        self.retry_backoff_ms = retry_backoff_ms\n        self.auto_offset_reset = auto_offset_reset\n        self.enable_auto_commit = enable_auto_commit\n        self.auto_commit_interval_ms = auto_commit_interval_ms\n        self.check_crcs = check_crcs\n        self.metadata_max_age_ms = metadata_max_age_ms\n        self.partition_assignment_strategy = partition_assignment_strategy or (\n            RoundRobinPartitionAssignor,\n        )\n        self.max_poll_interval_ms = max_poll_interval_ms\n        self.rebalance_timeout_ms = rebalance_timeout_ms\n        self.session_timeout_ms = session_timeout_ms\n        self.heartbeat_interval_ms = heartbeat_interval_ms\n        self.consumer_timeout_ms = consumer_timeout_ms\n        self.max_poll_records = max_poll_records\n        self.kafka_api_version = kafka_api_version\n        self.security_protocol = security_protocol\n        self.exclude_internal_topics = exclude_internal_topics\n        self.connection_max_idle_ms = connection_max_idle_ms\n        self.isolation_level = isolation_level\n        self.cafile = cafile\n        self.capath = capath\n        self.cadata = cadata\n        self.certfile = certfile\n        self.keyfile = keyfile\n        self.cert_password = cert_password\n        self.sasl_mechanism = sasl_mechanism\n        self.sasl_plain_username = sasl_plain_username\n        self.sasl_plain_password = sasl_plain_password\n        self.sasl_kerberos_service_name = sasl_kerberos_service_name\n        self.sasl_kerberos_domain_name = sasl_kerberos_domain_name\n        self.sasl_oauth_token_provider = sasl_oauth_token_provider\n\n        if security_protocol in [\"SSL\", \"SASL_SSL\"]:\n            self.ssl_context = create_ssl_context(\n                cafile=cafile,\n                capath=capath,\n                cadata=cadata,\n                certfile=certfile,\n                keyfile=keyfile,\n                password=cert_password,\n            )\n        else:\n            self.ssl_context = None\n\n        # AsyncAPI\n        self.asyncapi_version = asyncapi_version\n        self.title = title\n        self.version = version\n        self.description = description\n        self.terms_of_service = terms_of_service\n        self.contact = contact\n        self.license_info = license_info\n        self.asyncapi_schema: AsyncAPI | None = None\n\n        # di\n        self._container = Container()\n        self._container_state = ScopeState()\n        self.dependency_overrides = DependencyOverrideManager(self._container)\n\n        self._loop = asyncio.get_event_loop()\n        self._consumer: AIOKafkaConsumer | None = None\n        self._producer: AIOKafkaProducer | None = None\n\n        self._consumers: dict[str, TopicConsumerFunc] = {}\n        self._producers: dict[str, list[ProducerFunc]] = defaultdict(list)\n        self._sink_topics: set[str] = set()\n\n        self._exception_handlers: dict[\n            type[Exception], Callable[..., Awaitable[None]]\n        ] = {}\n        self._deserialization_error_handler: DeserializationErrorHandlerFunc | None = (\n            None\n        )\n\n        @asynccontextmanager\n        async def lifespan_ctx() -> AsyncIterator[None]:\n            executor = AsyncExecutor()\n            dep: Dependent[Any]\n\n            async with self._container_state.enter_scope(\n                scope=\"app\"\n            ) as self._container_state:\n                if lifespan:\n                    dep = Dependent(\n                        _wrap_lifespan_as_async_generator(lifespan), scope=\"app\"\n                    )\n                else:\n                    dep = Dependent(lambda: None, scope=\"app\")\n                solved = self._container.solve(dep, scopes=Scopes)\n                try:\n                    await solved.execute_async(\n                        executor=executor, state=self._container_state\n                    )\n                    self._prepare()\n                    yield\n                finally:\n                    self._container_state = ScopeState()\n\n        self.lifespan = lifespan_ctx\n\n    def _prepare(self) -> None:\n        for topic_processor in self._consumers.values():\n            topic_processor.prepare(self._container_state)\n\n    def _add_topic_consumer_func(\n        self,\n        topic: str,\n        func: ConsumerFunc,\n        value_param_type: type[TopicValueKeyHeader],\n        value_deserializer: Serializer | None = None,\n        key_param_type: type[TopicValueKeyHeader] | None = None,\n        key_deserializer: Serializer | None = None,\n        headers_type_deserializers: dict[\n            str, tuple[type[TopicValueKeyHeader], Serializer | None]\n        ]\n        | None = None,\n        sink_topics: Sequence[str] | None = None,\n    ) -> None:\n        topic_processor = TopicConsumerFunc(\n            name=topic,\n            container=self._container,\n            publish_fn=lambda *args, **kwargs: self._publish(*args, **kwargs),\n            exception_handlers=self._exception_handlers,\n            deserialization_error_handler=self._deserialization_error_handler,\n            func=func,\n            value_param_type=value_param_type,\n            value_deserializer=value_deserializer,\n            key_param_type=key_param_type,\n            key_deserializer=key_deserializer,\n            headers_type_deserializers=headers_type_deserializers,\n            sink_topics=sink_topics,\n        )\n        self._consumers[topic] = topic_processor\n\n    def _create_consumer(self) -> AIOKafkaConsumer:\n        return AIOKafkaConsumer(\n            *self._consumers.keys(),\n            bootstrap_servers=self.brokers,\n            client_id=self.client_id,\n            group_id=self.group_id,\n            fetch_min_bytes=self.fetch_min_bytes,\n            fetch_max_bytes=self.fetch_max_bytes,\n            fetch_max_wait_ms=self.fetch_max_wait_ms,\n            max_partition_fetch_bytes=self.max_partition_fetch_bytes,\n            max_poll_records=self.max_poll_records,\n            request_timeout_ms=self.request_timeout_ms,\n            retry_backoff_ms=self.retry_backoff_ms,\n            auto_offset_reset=self.auto_offset_reset,\n            enable_auto_commit=self.enable_auto_commit,\n            auto_commit_interval_ms=self.auto_commit_interval_ms,\n            check_crcs=self.check_crcs,\n            metadata_max_age_ms=self.metadata_max_age_ms,\n            partition_assignment_strategy=self.partition_assignment_strategy,\n            max_poll_interval_ms=self.max_poll_interval_ms,\n            rebalance_timeout_ms=self.rebalance_timeout_ms,\n            session_timeout_ms=self.session_timeout_ms,\n            heartbeat_interval_ms=self.heartbeat_interval_ms,\n            consumer_timeout_ms=self.consumer_timeout_ms,\n            api_version=self.kafka_api_version,\n            security_protocol=self.security_protocol,\n            ssl_context=self.ssl_context,\n            exclude_internal_topics=self.exclude_internal_topics,\n            connections_max_idle_ms=self.connections_max_idle_ms,\n            isolation_level=self.isolation_level,\n            sasl_mechanism=self.sasl_mechanism,\n            sasl_plain_username=self.sasl_plain_username,\n            sasl_plain_password=self.sasl_plain_password,\n            sasl_kerberos_domain_name=self.sasl_kerberos_domain_name,\n            sasl_kerberos_service_name=self.sasl_kerberos_service_name,\n            sasl_oauth_token_provider=self.sasl_oauth_token_provider,\n        )\n\n    def _create_producer(self) -> AIOKafkaProducer:\n        return AIOKafkaProducer(\n            bootstrap_servers=self.brokers,\n            client_id=self.client_id,\n            metadata_max_age_ms=self.metadata_max_age_ms,\n            request_timeout_ms=self.request_timeout_ms,\n            api_version=self.kafka_api_version,\n            acks=self.acks,\n            compression_type=self.compression_type,\n            max_batch_size=self.max_batch_size,\n            partitioner=self.partitioner,\n            max_request_size=self.max_request_size,\n            linger_ms=self.linger_ms,\n            send_backoff_ms=self.send_backoff_ms,\n            retry_backoff_ms=self.retry_backoff_ms,\n            security_protocol=self.security_protocol,\n            ssl_context=self.ssl_context,\n            connections_max_idle_ms=self.connections_max_idle_ms,\n            enable_idempotence=self.enable_idempotence,\n            transactional_id=self.transactional_id,\n            transaction_timeout_ms=self.transaction_timeout_ms,\n            sasl_mechanism=self.sasl_mechanism,\n            sasl_plain_username=self.sasl_plain_username,\n            sasl_plain_password=self.sasl_plain_password,\n            sasl_kerberos_service_name=self.sasl_kerberos_service_name,\n            sasl_kerberos_domain_name=self.sasl_kerberos_domain_name,\n            sasl_oauth_token_provider=self.sasl_oauth_token_provider,\n        )\n\n    def consume(\n        self,\n        topic: str,\n        sink_topics: Sequence[str] | None = None,\n    ) -> Callable[[ConsumerFunc], ConsumerFunc]:\n        def register_consumer(func: ConsumerFunc) -> ConsumerFunc:\n            (\n                value_param_type,\n                value_deserializer,\n                key_param_type,\n                key_deserializer,\n                headers_type_deserializers,\n            ) = parameters.get_function_parameters_info(func)\n            self._add_topic_consumer_func(\n                topic=topic,\n                func=func,\n                value_param_type=value_param_type,\n                value_deserializer=value_deserializer,\n                key_param_type=key_param_type,\n                key_deserializer=key_deserializer,\n                headers_type_deserializers=headers_type_deserializers,\n                sink_topics=sink_topics,\n            )\n            if sink_topics:\n                self._sink_topics.update(sink_topics)\n            if is_not_coroutine_function(func):\n                func = asyncify(func)\n            return func\n\n        return register_consumer\n\n    def produce(self, sink_topic: str) -> Callable[[ProducerFunc], ProducerFunc]:\n        def register_producer(func: ProducerFunc) -> Callable[..., Any]:\n            self._sink_topics.update([sink_topic])\n\n            def _create_coro(topic: str, message: Any) -> Coroutine[Any, Any, None]:\n                if not isinstance(message, Message):\n                    raise ValueError(\n                        \"Kaflow producer function has to return an instance of\"\n                        \" `Message` containing the information of the message to be\"\n                        f\" send. Update `{func.__name__}` to return a `Message`\"\n                        \" instance.\"\n                    )\n                return self._publish(\n                    topic=topic,\n                    value=message.value,\n                    key=message.key,\n                    headers=message.headers,\n                    partition=message.partition,\n                    timestamp=message.timestamp,\n                )\n\n            if is_not_coroutine_function(func):\n\n                @wraps(func)\n                def sync_wrapper(*args: Any, **kwargs: Any) -> Any:\n                    message = func(*args, **kwargs)\n                    asyncio.run_coroutine_threadsafe(\n                        coro=_create_coro(sink_topic, message),\n                        loop=self._loop,\n                    )\n                    return message\n\n                return sync_wrapper\n\n            @wraps(func)\n            async def async_wrapper(*args: Any, **kwargs: Any) -> Any:\n                message = await func(*args, **kwargs)  # type: ignore\n                await _create_coro(sink_topic, message)\n                return message\n\n            return async_wrapper\n\n        return register_producer\n\n    def exception_handler(\n        self, exception: type[Exception]\n    ) -> Callable[[ExceptionHandlerFunc], ExceptionHandlerFunc]:\n        def register_exception_handler(\n            func: ExceptionHandlerFunc,\n        ) -> ExceptionHandlerFunc:\n            if is_not_coroutine_function(func):\n                func = asyncify(func)\n            self._exception_handlers[exception] = func\n            return func\n\n        return register_exception_handler\n\n    def deserialization_error_handler(\n        self,\n    ) -> Callable[[DeserializationErrorHandlerFunc], DeserializationErrorHandlerFunc]:\n        def register_deserialization_error_handler(\n            func: DeserializationErrorHandlerFunc,\n        ) -> DeserializationErrorHandlerFunc:\n            self._deserialization_error_handler = func\n            if is_not_coroutine_function(func):\n                self._deserialization_error_handler = asyncify(func)\n            return func\n\n        return register_deserialization_error_handler\n\n    def asyncapi(self) -> AsyncAPI:\n        # if not self.asyncapi_schema:\n        #     self.asyncapi_schema = build_asyncapi(\n        #         asyncapi_version=self.asyncapi_version,\n        #         title=self.title,\n        #         version=self.version,\n        #         description=self.description,\n        #         terms_of_service=self.terms_of_service,\n        #         contact=self.contact,\n        #         license_info=self.license_info,\n        #         consumers=self._consumers,\n        #         producers=self._producers,\n        #     )\n        # return self.asyncapi_schema\n        raise NotImplementedError(\"AsyncAPI is not implemented yet.\")\n\n    async def _publish(\n        self,\n        topic: str,\n        value: bytes | None = None,\n        key: bytes | None = None,\n        headers: dict[str, bytes] | None = None,\n        partition: int | None = None,\n        timestamp: int | None = None,\n    ) -> None:\n        if not self._producer:\n            raise RuntimeError(\n                \"The producer has not been started yet. You're probably seeing this\"\n                f\" error because `{self.__class__.__name__}.start` method has not been\"\n                \" called yet.\"\n            )\n\n        if isinstance(headers, dict):\n            headers_ = [(k, v) for k, v in headers.items()]\n        else:\n            headers_ = None\n\n        await self._producer.send_and_wait(\n            topic=topic,\n            value=value,\n            key=key,\n            partition=partition,\n            timestamp_ms=timestamp,\n            headers=headers_,\n        )\n\n    def _get_consumer(self, topic: str) -> TopicConsumerFunc:\n        return self._consumers[topic]\n\n    async def _consuming_loop(self) -> None:\n        if not self._consumer:\n            raise RuntimeError(\n                \"The consumer has not been started yet. You're probably seeing this\"\n                f\" error because `{self.__class__.__name__}.start` method has not been\"\n                \" called yet.\"\n            )\n        async for record in self._consumer:\n            consumer = self._get_consumer(record.topic)\n            await consumer.consume(record=record)\n\n    async def start(self) -> None:\n        self._consumer = self._create_consumer()\n        self._producer = self._create_producer()\n\n        async with self.lifespan():\n            await self._consumer.start()\n            await self._producer.start()\n            await self._consuming_loop()\n\n    async def stop(self) -> None:\n        if self._consumer:\n            await self._consumer.stop()\n        if self._producer:\n            await self._producer.stop()\n\n    def run(self) -> None:\n        try:\n            self._loop.run_until_complete(self.start())\n        except asyncio.CancelledError:\n            pass\n        except KeyboardInterrupt:\n            pass\n        finally:\n            self._loop.run_until_complete(self.stop())\n            self._loop.close()\n\n    @property\n    def consumed_topics(self) -> list[str]:\n        return list(self._consumers.keys())\n\n    @property\n    def sink_topics(self) -> list[str]:\n        return list(self._sink_topics)", "\n\n# Taken from adriandg/xpresso\n# https://github.com/adriangb/xpresso/blob/0a69b5131440cd114baeab7243db7bd3255e66ed/xpresso/applications.py#L392\n# Thanks :)\ndef _wrap_lifespan_as_async_generator(\n    lifespan: Callable[..., AsyncContextManager[None]]\n) -> Callable[..., AsyncIterator[None]]:\n    # wrap true context managers in an async generator\n    # so that the dependency injection system recognizes it\n    async def gen(*args: Any, **kwargs: Any) -> AsyncIterator[None]:\n        async with lifespan(*args, **kwargs):\n            yield\n\n    # this is so that the dependency injection system\n    # still picks up parameters from the function signature\n    sig = inspect.signature(gen)\n    sig = sig.replace(parameters=list(inspect.signature(lifespan).parameters.values()))\n    setattr(gen, \"__signature__\", sig)  # noqa: B010\n\n    return gen", ""]}
{"filename": "kaflow/_utils/overrides.py", "chunked_list": ["# Taken from: https://github.com/adriangb/xpresso/blob/main/xpresso/_utils/overrides.py\nfrom __future__ import annotations\n\nimport contextlib\nimport inspect\nimport typing\nfrom types import TracebackType\nfrom typing import cast\n\nfrom di import Container", "\nfrom di import Container\nfrom di.api.dependencies import DependentBase\nfrom di.api.providers import DependencyProvider\nfrom di.dependent import Dependent\nfrom typing_extensions import get_args\n\nfrom kaflow._utils.inspect import is_annotated_param\n\n\ndef get_type(param: inspect.Parameter) -> type:\n    if is_annotated_param(param):\n        type_ = next(iter(get_args(param.annotation)))\n    else:\n        type_ = param.annotation\n    return cast(type, type_)", "\n\ndef get_type(param: inspect.Parameter) -> type:\n    if is_annotated_param(param):\n        type_ = next(iter(get_args(param.annotation)))\n    else:\n        type_ = param.annotation\n    return cast(type, type_)\n\n\nclass DependencyOverrideManager:\n    _stacks: typing.List[contextlib.ExitStack]\n\n    def __init__(self, container: Container) -> None:\n        self._container = container\n        self._stacks = []\n\n    def __setitem__(\n        self, target: DependencyProvider, replacement: DependencyProvider\n    ) -> None:\n        def hook(\n            param: typing.Optional[inspect.Parameter],\n            dependent: DependentBase[typing.Any],\n        ) -> typing.Optional[DependentBase[typing.Any]]:\n            if not isinstance(dependent, Dependent):\n                return None\n            scope = dependent.scope\n            dep = Dependent(\n                replacement,\n                scope=scope,\n                use_cache=dependent.use_cache,\n                wire=dependent.wire,\n            )\n            if param is not None and param.annotation is not param.empty:\n                type_ = get_type(param)\n                if type_ is target:\n                    return dep\n            if dependent.call is not None and dependent.call is target:\n                return dep\n            return None\n\n        cm = self._container.bind(hook)\n        if self._stacks:\n            self._stacks[-1].enter_context(cm)\n\n    def __enter__(self) -> DependencyOverrideManager:\n        self._stacks.append(contextlib.ExitStack().__enter__())\n        return self\n\n    def __exit__(\n        self,\n        __exc_type: typing.Optional[typing.Type[BaseException]],\n        __exc_value: typing.Optional[BaseException],\n        __traceback: typing.Optional[TracebackType],\n    ) -> typing.Optional[bool]:\n        return self._stacks.pop().__exit__(__exc_type, __exc_value, __traceback)", "\n\nclass DependencyOverrideManager:\n    _stacks: typing.List[contextlib.ExitStack]\n\n    def __init__(self, container: Container) -> None:\n        self._container = container\n        self._stacks = []\n\n    def __setitem__(\n        self, target: DependencyProvider, replacement: DependencyProvider\n    ) -> None:\n        def hook(\n            param: typing.Optional[inspect.Parameter],\n            dependent: DependentBase[typing.Any],\n        ) -> typing.Optional[DependentBase[typing.Any]]:\n            if not isinstance(dependent, Dependent):\n                return None\n            scope = dependent.scope\n            dep = Dependent(\n                replacement,\n                scope=scope,\n                use_cache=dependent.use_cache,\n                wire=dependent.wire,\n            )\n            if param is not None and param.annotation is not param.empty:\n                type_ = get_type(param)\n                if type_ is target:\n                    return dep\n            if dependent.call is not None and dependent.call is target:\n                return dep\n            return None\n\n        cm = self._container.bind(hook)\n        if self._stacks:\n            self._stacks[-1].enter_context(cm)\n\n    def __enter__(self) -> DependencyOverrideManager:\n        self._stacks.append(contextlib.ExitStack().__enter__())\n        return self\n\n    def __exit__(\n        self,\n        __exc_type: typing.Optional[typing.Type[BaseException]],\n        __exc_value: typing.Optional[BaseException],\n        __traceback: typing.Optional[TracebackType],\n    ) -> typing.Optional[bool]:\n        return self._stacks.pop().__exit__(__exc_type, __exc_value, __traceback)", ""]}
{"filename": "kaflow/_utils/inspect.py", "chunked_list": ["from __future__ import annotations\n\nimport inspect\nfrom typing import TYPE_CHECKING, Any, Awaitable, Callable, TypeVar\n\nfrom typing_extensions import Annotated, ParamSpec, TypeGuard, get_args, get_origin\n\nif TYPE_CHECKING:\n    from inspect import Signature\n", "\n\ndef is_annotated_param(param: Any) -> bool:\n    return get_origin(param) is Annotated\n\n\ndef annotated_param_with(item: Any, param: Any) -> bool:\n    if is_annotated_param(param):\n        for arg in get_args(param):\n            if arg == item or (type(item) == type and isinstance(arg, item)):\n                return True\n    return False", "\n\ndef has_return_annotation(signature: Signature) -> bool:\n    return (\n        signature.return_annotation is not None\n        and signature.return_annotation != inspect.Signature.empty\n    )\n\n\nP = ParamSpec(\"P\")", "\nP = ParamSpec(\"P\")\nR = TypeVar(\"R\")\n\n\ndef is_not_coroutine_function(\n    func: Callable[P, R | Awaitable[R]]\n) -> TypeGuard[Callable[P, R]]:\n    \"\"\"Check if a function is not a coroutine function. This function narrows the type\n    of the function to a synchronous function.\n\n    Args:\n        func: The function to check.\n\n    Returns:\n        `True` if the function is not a coroutine function, `False` otherwise.\n    \"\"\"\n    return not inspect.iscoroutinefunction(func)", ""]}
{"filename": "kaflow/_utils/__init__.py", "chunked_list": [""]}
{"filename": "kaflow/_utils/asyncio.py", "chunked_list": ["from __future__ import annotations\n\nimport asyncio\nimport contextvars\nimport functools\nfrom typing import Awaitable, Callable, TypeVar\n\nfrom typing_extensions import ParamSpec\n\nP = ParamSpec(\"P\")", "\nP = ParamSpec(\"P\")\nR = TypeVar(\"R\")\n\n\n# Inspired by https://github.com/tiangolo/asyncer\ndef asyncify(func: Callable[P, R]) -> Callable[P, Awaitable[R]]:\n    \"\"\"A decorator to convert a synchronous function into an asynchronous one.\n\n    Args:\n        func: The synchronous function to convert.\n\n    Returns:\n        The asynchronous function.\n    \"\"\"\n\n    async def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n        # TODO: update to use `asyncio.to_thread`, once Python 3.8 is deprecated\n        loop = asyncio.get_running_loop()\n        ctx = contextvars.copy_context()\n        func_call = functools.partial(ctx.run, func, *args, **kwargs)\n        return await loop.run_in_executor(None, func_call)  # type: ignore\n\n    return wrapper", ""]}
{"filename": "kaflow/asyncapi/_builder.py", "chunked_list": ["from __future__ import annotations\n\nfrom enum import Enum\nfrom typing import TYPE_CHECKING, Any\n\nfrom pydantic import BaseModel\nfrom pydantic.schema import get_model_name_map, model_process_schema\nfrom typing_extensions import TypeGuard\n\nfrom kaflow.asyncapi import models", "\nfrom kaflow.asyncapi import models\n\nif TYPE_CHECKING:\n    from kaflow._consumer import TopicConsumerFunc\n    from kaflow.applications import ProducerFunc\n\n\ndef not_bytes_and_base_model(cls: type) -> TypeGuard[type[BaseModel]]:\n    return cls is not None and cls is not bytes and issubclass(cls, BaseModel)", "def not_bytes_and_base_model(cls: type) -> TypeGuard[type[BaseModel]]:\n    return cls is not None and cls is not bytes and issubclass(cls, BaseModel)\n\n\ndef get_flat_models(\n    consumers: dict[str, TopicConsumerFunc] | None = None\n) -> set[type[BaseModel] | type[Enum]]:\n    models: set[type[BaseModel] | type[Enum]] = set()\n    if consumers:\n        for consumer in consumers.values():\n            if not_bytes_and_base_model(consumer.value_param_type):\n                models.add(consumer.value_param_type)\n            if consumer.key_param_type:\n                if not_bytes_and_base_model(consumer.key_param_type):\n                    models.add(consumer.key_param_type)\n            if consumer.headers_type_deserializers:\n                for header in consumer.headers_type_deserializers:\n                    header_type = consumer.headers_type_deserializers[header][0]\n                    if not_bytes_and_base_model(header_type):\n                        models.add(header_type)\n    return models", "\n\ndef get_model_definitions(\n    flat_models: set[type[BaseModel] | type[Enum]],\n    model_name_map: dict[type[BaseModel] | type[Enum], str],\n) -> dict[str, Any]:\n    definitions: dict[str, dict[str, Any]] = {}\n    for model in flat_models:\n        model_schema, model_definitions, model_nested_models = model_process_schema(\n            model, model_name_map=model_name_map\n        )\n        definitions.update(model_definitions)\n        model_name = model_name_map[model]\n        definitions[model_name] = model_schema\n    return definitions", "\n\ndef build_asyncapi(\n    asyncapi_version: str,\n    title: str,\n    version: str,\n    description: str | None = None,\n    terms_of_service: str | None = None,\n    contact: dict[str, Any] | None = None,\n    license_info: dict[str, Any] | None = None,\n    consumers: dict[str, TopicConsumerFunc] | None = None,\n    producers: dict[str, list[ProducerFunc]] | None = None,\n) -> models.AsyncAPI:\n    asyncapi_info: dict[str, Any] = {\"title\": title, \"version\": version}\n    if description:\n        asyncapi_info[\"description\"] = description\n    if terms_of_service:\n        asyncapi_info[\"termsOfService\"] = terms_of_service\n    if contact:\n        asyncapi_info[\"contact\"] = contact\n    if license_info:\n        asyncapi_info[\"license\"] = license_info\n    components: dict[str, dict[str, Any]] = {}\n    flat_models = get_flat_models(consumers)\n    model_name_map = get_model_name_map(flat_models)\n    definitions = get_model_definitions(flat_models, model_name_map)\n    if definitions:\n        components[\"schemas\"] = definitions\n    output = {\n        \"asyncapi\": asyncapi_version,\n        \"info\": asyncapi_info,\n        \"components\": components,\n    }\n    return models.AsyncAPI(**output)", ""]}
{"filename": "kaflow/asyncapi/docs.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from kaflow.asyncapi.models import AsyncAPI\n\n\ndef get_asyncapi_html(\n    title: str,\n    asyncapi_schema: AsyncAPI,\n    asyncapi_react_component_js_url: str = \"https://unpkg.com/@asyncapi/web-component@1.0.0-next.47/lib/asyncapi-web-component.js\",\n    asyncapi_react_component_css_url: str = \"https://unpkg.com/@asyncapi/react-component@1.0.0-next.12/styles/default.min.css\",\n) -> str:\n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <link\n            rel=\"stylesheet\"\n            href=\"{asyncapi_react_component_css_url}\"\n        />\n        <title>{title}</title>\n    </head>\n    <body>\n        <script\n            src=\"{asyncapi_react_component_js_url}\"\n            defer\n        ></script>\n        <asyncapi-component\n            schema='{asyncapi_schema.json(by_alias=True, exclude_none=True)}'\n            cssImportPath=\"{asyncapi_react_component_css_url}\"\n        ></asyncapi-component>\n    </body>\n    </html>\n    \"\"\"\n    return html", "def get_asyncapi_html(\n    title: str,\n    asyncapi_schema: AsyncAPI,\n    asyncapi_react_component_js_url: str = \"https://unpkg.com/@asyncapi/web-component@1.0.0-next.47/lib/asyncapi-web-component.js\",\n    asyncapi_react_component_css_url: str = \"https://unpkg.com/@asyncapi/react-component@1.0.0-next.12/styles/default.min.css\",\n) -> str:\n    html = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <link\n            rel=\"stylesheet\"\n            href=\"{asyncapi_react_component_css_url}\"\n        />\n        <title>{title}</title>\n    </head>\n    <body>\n        <script\n            src=\"{asyncapi_react_component_js_url}\"\n            defer\n        ></script>\n        <asyncapi-component\n            schema='{asyncapi_schema.json(by_alias=True, exclude_none=True)}'\n            cssImportPath=\"{asyncapi_react_component_css_url}\"\n        ></asyncapi-component>\n    </body>\n    </html>\n    \"\"\"\n    return html", ""]}
{"filename": "kaflow/asyncapi/models.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Literal,\n    Mapping,", "    Literal,\n    Mapping,\n    Optional,\n    Union,\n)\n\nfrom pydantic import AnyUrl, BaseModel, Field\n\nfrom kaflow.logger import logger\n", "from kaflow.logger import logger\n\n# Taken from: https://github.com/tiangolo/fastapi/blob/master/fastapi/openapi/models.py\ntry:\n    import email_validator  # type: ignore\n\n    assert email_validator\n    from pydantic import EmailStr\nexcept ImportError:  # pragma: no cover\n\n    class EmailStr(str):  # type: ignore\n        @classmethod\n        def __get_validators__(cls) -> Iterable[Callable[..., Any]]:\n            yield cls.validate\n\n        @classmethod\n        def validate(cls, v: Any) -> str:\n            logger.warning(\n                \"`email-validator` not installed, email fields will be treated as\"\n                \" str.\\nTo install, run: `pip install email-validator`\"\n            )\n            return str(v)", "\n\nclass Contact(BaseModel):\n    name: Optional[str] = None\n    url: Optional[AnyUrl] = None\n    email: Optional[EmailStr] = None\n\n\nclass License(BaseModel):\n    name: str\n    url: Optional[AnyUrl] = None", "class License(BaseModel):\n    name: str\n    url: Optional[AnyUrl] = None\n\n\nclass Info(BaseModel):\n    title: str\n    version: str\n    description: Optional[str] = None\n    termsOfService: Optional[str] = None\n    contact: Optional[Contact] = None\n    license: Optional[License] = None", "\n\nclass ServerVariable(BaseModel):\n    enum: Optional[List[str]] = None\n    default: Optional[str] = None\n    description: Optional[str] = None\n    examples: Optional[List[str]] = None\n\n\nclass ServerBinding(BaseModel):\n    schemaRegistryUrl: Optional[AnyUrl] = None\n    schemaRegistryVendor: Optional[str] = None\n    bindingVersion: Optional[str] = None", "\nclass ServerBinding(BaseModel):\n    schemaRegistryUrl: Optional[AnyUrl] = None\n    schemaRegistryVendor: Optional[str] = None\n    bindingVersion: Optional[str] = None\n\n\nclass TopicConfiguration(BaseModel):\n    cleanup_policy: Optional[str] = Field(None, alias=\"cleanup.policy\")\n    retention_ms: Optional[int] = Field(None, alias=\"retention.ms\")\n    retention_bytes: Optional[int] = Field(None, alias=\"retention.bytes\")\n    delete_retention_ms: Optional[int] = Field(None, alias=\"delete.retention.ms\")\n    max_message_bytes: Optional[int] = Field(None, alias=\"max.message.bytes\")", "\n\nclass ChannelBinding(BaseModel):\n    topic: Optional[str] = None\n    partitions: Optional[int] = None\n    replicas: Optional[int] = None\n    topicConfiguration: Optional[TopicConfiguration] = None\n    bindingVersion: Optional[str] = None\n\n\nclass OperationBinding(BaseModel):\n    groupId: Optional[Schema] = None\n    clientId: Optional[Schema] = None\n    bindingVersion: Optional[str] = None", "\n\nclass OperationBinding(BaseModel):\n    groupId: Optional[Schema] = None\n    clientId: Optional[Schema] = None\n    bindingVersion: Optional[str] = None\n\n\nclass MessageBinding(BaseModel):\n    key: Optional[Schema] = None\n    schemaIdLocation: Optional[str] = None\n    schemaIdPayloadEncoding: Optional[str] = None\n    schemaLookupStrategy: Optional[str] = None\n    bindingVersion: Optional[str] = None", "class MessageBinding(BaseModel):\n    key: Optional[Schema] = None\n    schemaIdLocation: Optional[str] = None\n    schemaIdPayloadEncoding: Optional[str] = None\n    schemaLookupStrategy: Optional[str] = None\n    bindingVersion: Optional[str] = None\n\n\nclass Server(BaseModel):\n    url: str\n    protocol: str\n    protocolVersion: Optional[str] = None\n    description: Optional[str] = None\n    variables: Optional[Mapping[str, Union[ServerVariable, Reference]]] = None\n    security: Optional[List[Mapping[str, Any]]] = None\n    bindings: Optional[Mapping[str, Union[ServerBinding, Reference]]] = None", "class Server(BaseModel):\n    url: str\n    protocol: str\n    protocolVersion: Optional[str] = None\n    description: Optional[str] = None\n    variables: Optional[Mapping[str, Union[ServerVariable, Reference]]] = None\n    security: Optional[List[Mapping[str, Any]]] = None\n    bindings: Optional[Mapping[str, Union[ServerBinding, Reference]]] = None\n\n\nclass ExternalDocs(BaseModel):\n    description: Optional[str] = None\n    url: AnyUrl", "\n\nclass ExternalDocs(BaseModel):\n    description: Optional[str] = None\n    url: AnyUrl\n\n\nclass Tag(BaseModel):\n    name: str\n    description: Optional[str] = None\n    externalDocs: Optional[ExternalDocs] = None", "\n\nclass BaseOperation(BaseModel):\n    operationId: Optional[str] = None\n    summary: Optional[str] = None\n    description: Optional[str] = None\n    tags: Optional[List[Tag]] = None\n    externalDocs: Optional[ExternalDocs] = None\n    bindings: Optional[Mapping[str, Union[OperationBinding, Reference]]] = None\n", "\n\nclass OperationTrait(BaseOperation):\n    pass\n\n\nclass Operation(BaseOperation):\n    traits: Optional[List[OperationTrait]] = None\n\n\nclass Reference(BaseModel):\n    ref: str = Field(..., alias=\"$ref\")", "\n\nclass Reference(BaseModel):\n    ref: str = Field(..., alias=\"$ref\")\n\n\nclass Schema(BaseModel):\n    title: Optional[str] = None\n    type: Optional[str] = None\n    required: Optional[List[str]] = None\n    multipleOf: Optional[float] = None\n    maximum: Optional[float] = None\n    exclusiveMaximum: Optional[float] = None\n    minimum: Optional[float] = None\n    exclusiveMinimum: Optional[float] = None\n    maxLength: Optional[int] = None\n    minLength: Optional[int] = None\n    pattern: Optional[str] = None\n    maxItems: Optional[int] = None\n    minItems: Optional[int] = None\n    uniqueItems: Optional[bool] = None\n    maxProperties: Optional[int] = None\n    minProperties: Optional[int] = None\n    enum: Optional[List[Any]] = None\n    const: Optional[str] = None\n    examples: Optional[List[str]] = None\n    readOnly: Optional[bool] = None\n    writeOnly: Optional[bool] = None\n    properties: Optional[Dict[str, Schema]] = None\n    patternProperties: Optional[Dict[str, Schema]] = None\n    additionalProperties: Optional[Schema] = None\n    additionalItems: Optional[Schema] = None\n    items: Optional[Union[Schema, List[Schema]]] = None\n    propertyNames: Optional[Schema] = None\n    contains: Optional[Schema] = None\n    allOf: Optional[List[Schema]] = None\n    oneOf: Optional[List[Schema]] = None\n    anyOf: Optional[List[Schema]] = None\n    not_: Optional[Schema] = Field(None, alias=\"not\")\n    description: Optional[str] = None\n    format: Optional[str] = None\n    default: Optional[Any] = None\n    discriminator: Optional[str] = None\n    externalDocs: Optional[ExternalDocs] = None\n    deprecated: Optional[bool] = None", "\n\nclass Parameter(BaseModel):\n    description: Optional[str] = None\n    schema_: Optional[Schema] = Field(None, alias=\"schema\")\n    location: Optional[str] = None\n\n\nclass Channel(BaseModel):\n    ref: Optional[str] = Field(None, alias=\"$ref\")\n    description: Optional[str] = None\n    servers: Optional[List[str]] = None\n    subscribe: Optional[Operation] = None\n    publish: Optional[Operation] = None\n    parameters: Optional[Dict[str, Union[Parameter, Reference]]] = None\n    bindings: Optional[Mapping[str, Union[ChannelBinding, Reference]]] = None", "class Channel(BaseModel):\n    ref: Optional[str] = Field(None, alias=\"$ref\")\n    description: Optional[str] = None\n    servers: Optional[List[str]] = None\n    subscribe: Optional[Operation] = None\n    publish: Optional[Operation] = None\n    parameters: Optional[Dict[str, Union[Parameter, Reference]]] = None\n    bindings: Optional[Mapping[str, Union[ChannelBinding, Reference]]] = None\n\n\nclass CorrelationId(BaseModel):\n    description: Optional[str] = None\n    location: str", "\n\nclass CorrelationId(BaseModel):\n    description: Optional[str] = None\n    location: str\n\n\nclass MessageExample(BaseModel):\n    headers: Optional[Mapping[str, Any]] = None\n    payload: Any\n    name: Optional[str] = None\n    summary: Optional[str] = None", "\n\nclass BaseMessage(BaseModel):\n    messageId: Optional[str] = None\n    headers: Optional[Union[Schema, Reference]] = None\n    payload: Any\n    correlationId: Optional[Union[CorrelationId, Reference]] = None\n    schemaFormat: Optional[str] = None\n    contentType: Optional[str] = None\n    name: Optional[str] = None\n    title: Optional[str] = None\n    summary: Optional[str] = None\n    description: Optional[str] = None\n    tags: Optional[List[Tag]] = None\n    externalDocs: Optional[ExternalDocs] = None\n    bindings: Optional[Mapping[str, Union[MessageBinding, Reference]]] = None\n    examples: Optional[List[MessageExample]] = None", "\n\nclass MessageTrait(BaseMessage):\n    pass\n\n\nclass Message(BaseMessage):\n    traits: Optional[List[Union[MessageTrait, Reference]]] = None\n\n\nclass OAuthFlow(BaseModel):\n    authorizationUrl: Optional[AnyUrl] = None\n    tokenUrl: Optional[AnyUrl] = None\n    refreshUrl: Optional[AnyUrl] = None\n    scopes: Mapping[str, str]", "\n\nclass OAuthFlow(BaseModel):\n    authorizationUrl: Optional[AnyUrl] = None\n    tokenUrl: Optional[AnyUrl] = None\n    refreshUrl: Optional[AnyUrl] = None\n    scopes: Mapping[str, str]\n\n\nclass OAuthFlows(BaseModel):\n    implicit: Optional[OAuthFlow] = None\n    password: Optional[OAuthFlow] = None\n    clientCredentials: Optional[OAuthFlow] = None\n    authorizationCode: Optional[OAuthFlow] = None", "\nclass OAuthFlows(BaseModel):\n    implicit: Optional[OAuthFlow] = None\n    password: Optional[OAuthFlow] = None\n    clientCredentials: Optional[OAuthFlow] = None\n    authorizationCode: Optional[OAuthFlow] = None\n\n\nclass SecurityScheme(BaseModel):\n    type: Literal[\n        \"userPassword\",\n        \"apiKey\",\n        \"X509\",\n        \"symmetrictEncryption\",\n        \"asymmetricEncryption\",\n        \"httpApiKey\",\n        \"http\",\n        \"oauth2\",\n        \"openIdConnect\",\n        \"plain\",\n        \"scramSha256\",\n        \"scramSha512\",\n        \"gssapi\",\n    ]\n    description: Optional[str] = None\n    name: Optional[str] = None\n    in_: Optional[Literal[\"user\", \"password\", \"query\", \"header\", \"cookie\"]] = None\n    scheme: Optional[str] = None\n    bearerFormat: Optional[str] = None\n    flows: Optional[OAuthFlows] = None\n    openIdConnectUrl: Optional[AnyUrl] = None", "class SecurityScheme(BaseModel):\n    type: Literal[\n        \"userPassword\",\n        \"apiKey\",\n        \"X509\",\n        \"symmetrictEncryption\",\n        \"asymmetricEncryption\",\n        \"httpApiKey\",\n        \"http\",\n        \"oauth2\",\n        \"openIdConnect\",\n        \"plain\",\n        \"scramSha256\",\n        \"scramSha512\",\n        \"gssapi\",\n    ]\n    description: Optional[str] = None\n    name: Optional[str] = None\n    in_: Optional[Literal[\"user\", \"password\", \"query\", \"header\", \"cookie\"]] = None\n    scheme: Optional[str] = None\n    bearerFormat: Optional[str] = None\n    flows: Optional[OAuthFlows] = None\n    openIdConnectUrl: Optional[AnyUrl] = None", "\n\nclass Components(BaseModel):\n    schemas: Optional[Mapping[str, Union[Schema, Reference]]] = None\n    servers: Optional[Mapping[str, Union[Server, Reference]]] = None\n    serverVariables: Optional[Mapping[str, Union[ServerVariable, Reference]]] = None\n    channels: Optional[Mapping[str, Union[Channel, Reference]]] = None\n    messages: Optional[Mapping[str, Union[Message, Reference]]] = None\n    securitySchemes: Optional[Mapping[str, Union[SecurityScheme, Reference]]] = None\n    parameters: Optional[Mapping[str, Union[Parameter, Reference]]] = None\n    correlationIds: Optional[Mapping[str, Union[CorrelationId, Reference]]] = None\n    operationTraits: Optional[Mapping[str, Union[OperationTrait, Reference]]] = None\n    messageTraits: Optional[Mapping[str, Union[MessageTrait, Reference]]] = None\n    serverBindings: Optional[Mapping[str, Union[ServerBinding, Reference]]] = None\n    channelBindings: Optional[Mapping[str, Union[ChannelBinding, Reference]]] = None\n    operationBindings: Optional[Mapping[str, Union[OperationBinding, Reference]]] = None\n    messageBindings: Optional[Mapping[str, Union[MessageBinding, Reference]]] = None", "\n\nclass AsyncAPI(BaseModel):\n    \"\"\"A model containing all the information required to generate an AsyncAPI spec.\n\n    https://www.asyncapi.com/docs/reference/specification/v2.6.0\n    \"\"\"\n\n    asyncapi: str\n    id: Optional[str] = None\n    info: Info\n    servers: Optional[Mapping[str, Server]] = None\n    defaultContentType: Optional[str] = None\n    channels: Mapping[str, Channel] = {}\n    components: Optional[Components] = None\n    tags: Optional[List[Tag]] = None\n    externalDocs: Optional[ExternalDocs] = None", ""]}
{"filename": "kaflow/asyncapi/__init__.py", "chunked_list": [""]}
