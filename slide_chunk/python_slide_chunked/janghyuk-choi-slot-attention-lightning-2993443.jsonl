{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\nfrom setuptools import find_packages, setup\n\nsetup(\n    name=\"src\",\n    version=\"0.0.1\",\n    description=\"Slot Attention Lightning\",\n    author=\"Janghyuk Choi\",\n    author_email=\"janghyuk.ch@gmail.com\",", "    author=\"Janghyuk Choi\",\n    author_email=\"janghyuk.ch@gmail.com\",\n    url=\"https://github.com/janghyuk-choi/slot-attention-lightning\",\n    install_requires=[\"pytorch-lightning\", \"hydra-core\"],\n    packages=find_packages(),\n    # use this to customize global commands available in the terminal after installing the package\n    entry_points={\n        \"console_scripts\": [\n            \"train_command = src.train:main\",\n            \"eval_command = src.eval:main\",", "            \"train_command = src.train:main\",\n            \"eval_command = src.eval:main\",\n        ]\n    },\n)\n"]}
{"filename": "configs/__init__.py", "chunked_list": ["# this file is needed here to include configs when building project as a package\n"]}
{"filename": "tests/test_datamodules.py", "chunked_list": ["from pathlib import Path\n\nimport pytest\nimport torch\n\nfrom src.data.mnist_datamodule import MNISTDataModule\n\n\n@pytest.mark.parametrize(\"batch_size\", [32, 128])\ndef test_mnist_datamodule(batch_size):\n    data_dir = \"data/\"\n\n    dm = MNISTDataModule(data_dir=data_dir, batch_size=batch_size)\n    dm.prepare_data()\n\n    assert not dm.data_train and not dm.data_val and not dm.data_test\n    assert Path(data_dir, \"MNIST\").exists()\n    assert Path(data_dir, \"MNIST\", \"raw\").exists()\n\n    dm.setup()\n    assert dm.data_train and dm.data_val and dm.data_test\n    assert dm.train_dataloader() and dm.val_dataloader() and dm.test_dataloader()\n\n    num_datapoints = len(dm.data_train) + len(dm.data_val) + len(dm.data_test)\n    assert num_datapoints == 70_000\n\n    batch = next(iter(dm.train_dataloader()))\n    x, y = batch\n    assert len(x) == batch_size\n    assert len(y) == batch_size\n    assert x.dtype == torch.float32\n    assert y.dtype == torch.int64", "@pytest.mark.parametrize(\"batch_size\", [32, 128])\ndef test_mnist_datamodule(batch_size):\n    data_dir = \"data/\"\n\n    dm = MNISTDataModule(data_dir=data_dir, batch_size=batch_size)\n    dm.prepare_data()\n\n    assert not dm.data_train and not dm.data_val and not dm.data_test\n    assert Path(data_dir, \"MNIST\").exists()\n    assert Path(data_dir, \"MNIST\", \"raw\").exists()\n\n    dm.setup()\n    assert dm.data_train and dm.data_val and dm.data_test\n    assert dm.train_dataloader() and dm.val_dataloader() and dm.test_dataloader()\n\n    num_datapoints = len(dm.data_train) + len(dm.data_val) + len(dm.data_test)\n    assert num_datapoints == 70_000\n\n    batch = next(iter(dm.train_dataloader()))\n    x, y = batch\n    assert len(x) == batch_size\n    assert len(y) == batch_size\n    assert x.dtype == torch.float32\n    assert y.dtype == torch.int64", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_sweeps.py", "chunked_list": ["import pytest\n\nfrom tests.helpers.run_if import RunIf\nfrom tests.helpers.run_sh_command import run_sh_command\n\nstartfile = \"src/train.py\"\noverrides = [\"logger=[]\"]\n\n\n@RunIf(sh=True)", "\n@RunIf(sh=True)\n@pytest.mark.slow\ndef test_experiments(tmp_path):\n    \"\"\"Test running all available experiment configs with fast_dev_run=True.\"\"\"\n    command = [\n        startfile,\n        \"-m\",\n        \"experiment=glob(*)\",\n        \"hydra.sweep.dir=\" + str(tmp_path),\n        \"++trainer.fast_dev_run=true\",\n    ] + overrides\n    run_sh_command(command)", "\n\n@RunIf(sh=True)\n@pytest.mark.slow\ndef test_hydra_sweep(tmp_path):\n    \"\"\"Test default hydra sweep.\"\"\"\n    command = [\n        startfile,\n        \"-m\",\n        \"hydra.sweep.dir=\" + str(tmp_path),\n        \"model.optimizer.lr=0.005,0.01\",\n        \"++trainer.fast_dev_run=true\",\n    ] + overrides\n\n    run_sh_command(command)", "\n\n@RunIf(sh=True)\n@pytest.mark.slow\ndef test_hydra_sweep_ddp_sim(tmp_path):\n    \"\"\"Test default hydra sweep with ddp sim.\"\"\"\n    command = [\n        startfile,\n        \"-m\",\n        \"hydra.sweep.dir=\" + str(tmp_path),\n        \"trainer=ddp_sim\",\n        \"trainer.max_epochs=3\",\n        \"+trainer.limit_train_batches=0.01\",\n        \"+trainer.limit_val_batches=0.1\",\n        \"+trainer.limit_test_batches=0.1\",\n        \"model.optimizer.lr=0.005,0.01,0.02\",\n    ] + overrides\n    run_sh_command(command)", "\n\n@RunIf(sh=True)\n@pytest.mark.slow\ndef test_optuna_sweep(tmp_path):\n    \"\"\"Test optuna sweep.\"\"\"\n    command = [\n        startfile,\n        \"-m\",\n        \"hparams_search=mnist_optuna\",\n        \"hydra.sweep.dir=\" + str(tmp_path),\n        \"hydra.sweeper.n_trials=10\",\n        \"hydra.sweeper.sampler.n_startup_trials=5\",\n        \"++trainer.fast_dev_run=true\",\n    ] + overrides\n    run_sh_command(command)", "\n\n@RunIf(wandb=True, sh=True)\n@pytest.mark.slow\ndef test_optuna_sweep_ddp_sim_wandb(tmp_path):\n    \"\"\"Test optuna sweep with wandb and ddp sim.\"\"\"\n    command = [\n        startfile,\n        \"-m\",\n        \"hparams_search=mnist_optuna\",\n        \"hydra.sweep.dir=\" + str(tmp_path),\n        \"hydra.sweeper.n_trials=5\",\n        \"trainer=ddp_sim\",\n        \"trainer.max_epochs=3\",\n        \"+trainer.limit_train_batches=0.01\",\n        \"+trainer.limit_val_batches=0.1\",\n        \"+trainer.limit_test_batches=0.1\",\n        \"logger=wandb\",\n    ]\n    run_sh_command(command)", ""]}
{"filename": "tests/test_train.py", "chunked_list": ["import os\n\nimport pytest\nfrom hydra.core.hydra_config import HydraConfig\nfrom omegaconf import open_dict\n\nfrom src.train import train\nfrom tests.helpers.run_if import RunIf\n\n\ndef test_train_fast_dev_run(cfg_train):\n    \"\"\"Run for 1 train, val and test step.\"\"\"\n    HydraConfig().set_config(cfg_train)\n    with open_dict(cfg_train):\n        cfg_train.trainer.fast_dev_run = True\n        cfg_train.trainer.accelerator = \"cpu\"\n    train(cfg_train)", "\n\ndef test_train_fast_dev_run(cfg_train):\n    \"\"\"Run for 1 train, val and test step.\"\"\"\n    HydraConfig().set_config(cfg_train)\n    with open_dict(cfg_train):\n        cfg_train.trainer.fast_dev_run = True\n        cfg_train.trainer.accelerator = \"cpu\"\n    train(cfg_train)\n", "\n\n@RunIf(min_gpus=1)\ndef test_train_fast_dev_run_gpu(cfg_train):\n    \"\"\"Run for 1 train, val and test step on GPU.\"\"\"\n    HydraConfig().set_config(cfg_train)\n    with open_dict(cfg_train):\n        cfg_train.trainer.fast_dev_run = True\n        cfg_train.trainer.accelerator = \"gpu\"\n    train(cfg_train)", "\n\n@RunIf(min_gpus=1)\n@pytest.mark.slow\ndef test_train_epoch_gpu_amp(cfg_train):\n    \"\"\"Train 1 epoch on GPU with mixed-precision.\"\"\"\n    HydraConfig().set_config(cfg_train)\n    with open_dict(cfg_train):\n        cfg_train.trainer.max_epochs = 1\n        cfg_train.trainer.accelerator = \"cpu\"\n        cfg_train.trainer.precision = 16\n    train(cfg_train)", "\n\n@pytest.mark.slow\ndef test_train_epoch_double_val_loop(cfg_train):\n    \"\"\"Train 1 epoch with validation loop twice per epoch.\"\"\"\n    HydraConfig().set_config(cfg_train)\n    with open_dict(cfg_train):\n        cfg_train.trainer.max_epochs = 1\n        cfg_train.trainer.val_check_interval = 0.5\n    train(cfg_train)", "\n\n@pytest.mark.slow\ndef test_train_ddp_sim(cfg_train):\n    \"\"\"Simulate DDP (Distributed Data Parallel) on 2 CPU processes.\"\"\"\n    HydraConfig().set_config(cfg_train)\n    with open_dict(cfg_train):\n        cfg_train.trainer.max_epochs = 2\n        cfg_train.trainer.accelerator = \"cpu\"\n        cfg_train.trainer.devices = 2\n        cfg_train.trainer.strategy = \"ddp_spawn\"\n    train(cfg_train)", "\n\n@pytest.mark.slow\ndef test_train_resume(tmp_path, cfg_train):\n    \"\"\"Run 1 epoch, finish, and resume for another epoch.\"\"\"\n    with open_dict(cfg_train):\n        cfg_train.trainer.max_epochs = 1\n\n    HydraConfig().set_config(cfg_train)\n    metric_dict_1, _ = train(cfg_train)\n\n    files = os.listdir(tmp_path / \"checkpoints\")\n    assert \"last.ckpt\" in files\n    assert \"epoch_000.ckpt\" in files\n\n    with open_dict(cfg_train):\n        cfg_train.ckpt_path = str(tmp_path / \"checkpoints\" / \"last.ckpt\")\n        cfg_train.trainer.max_epochs = 2\n\n    metric_dict_2, _ = train(cfg_train)\n\n    files = os.listdir(tmp_path / \"checkpoints\")\n    assert \"epoch_001.ckpt\" in files\n    assert \"epoch_002.ckpt\" not in files\n\n    assert metric_dict_1[\"train/acc\"] < metric_dict_2[\"train/acc\"]\n    assert metric_dict_1[\"val/acc\"] < metric_dict_2[\"val/acc\"]", ""]}
{"filename": "tests/test_eval.py", "chunked_list": ["import os\n\nimport pytest\nfrom hydra.core.hydra_config import HydraConfig\nfrom omegaconf import open_dict\n\nfrom src.eval import evaluate\nfrom src.train import train\n\n", "\n\n@pytest.mark.slow\ndef test_train_eval(tmp_path, cfg_train, cfg_eval):\n    \"\"\"Train for 1 epoch with `train.py` and evaluate with `eval.py`\"\"\"\n    assert str(tmp_path) == cfg_train.paths.output_dir == cfg_eval.paths.output_dir\n\n    with open_dict(cfg_train):\n        cfg_train.trainer.max_epochs = 1\n        cfg_train.test = True\n\n    HydraConfig().set_config(cfg_train)\n    train_metric_dict, _ = train(cfg_train)\n\n    assert \"last.ckpt\" in os.listdir(tmp_path / \"checkpoints\")\n\n    with open_dict(cfg_eval):\n        cfg_eval.ckpt_path = str(tmp_path / \"checkpoints\" / \"last.ckpt\")\n\n    HydraConfig().set_config(cfg_eval)\n    test_metric_dict, _ = evaluate(cfg_eval)\n\n    assert test_metric_dict[\"test/acc\"] > 0.0\n    assert abs(train_metric_dict[\"test/acc\"].item() - test_metric_dict[\"test/acc\"].item()) < 0.001", ""]}
{"filename": "tests/test_configs.py", "chunked_list": ["import hydra\nfrom hydra.core.hydra_config import HydraConfig\nfrom omegaconf import DictConfig\n\n\ndef test_train_config(cfg_train: DictConfig):\n    assert cfg_train\n    assert cfg_train.data\n    assert cfg_train.model\n    assert cfg_train.trainer\n\n    HydraConfig().set_config(cfg_train)\n\n    hydra.utils.instantiate(cfg_train.data)\n    hydra.utils.instantiate(cfg_train.model)\n    hydra.utils.instantiate(cfg_train.trainer)", "\n\ndef test_eval_config(cfg_eval: DictConfig):\n    assert cfg_eval\n    assert cfg_eval.data\n    assert cfg_eval.model\n    assert cfg_eval.trainer\n\n    HydraConfig().set_config(cfg_eval)\n\n    hydra.utils.instantiate(cfg_eval.data)\n    hydra.utils.instantiate(cfg_eval.model)\n    hydra.utils.instantiate(cfg_eval.trainer)", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["\"\"\"This file prepares config fixtures for other tests.\"\"\"\n\nimport pyrootutils\nimport pytest\nfrom hydra import compose, initialize\nfrom hydra.core.global_hydra import GlobalHydra\nfrom omegaconf import DictConfig, open_dict\n\n\n@pytest.fixture(scope=\"package\")\ndef cfg_train_global() -> DictConfig:\n    with initialize(version_base=\"1.3\", config_path=\"../configs\"):\n        cfg = compose(config_name=\"train.yaml\", return_hydra_config=True, overrides=[])\n\n        # set defaults for all tests\n        with open_dict(cfg):\n            cfg.paths.root_dir = str(pyrootutils.find_root(indicator=\".project-root\"))\n            cfg.trainer.max_epochs = 1\n            cfg.trainer.limit_train_batches = 0.01\n            cfg.trainer.limit_val_batches = 0.1\n            cfg.trainer.limit_test_batches = 0.1\n            cfg.trainer.accelerator = \"cpu\"\n            cfg.trainer.devices = 1\n            cfg.data.num_workers = 0\n            cfg.data.pin_memory = False\n            cfg.extras.print_config = False\n            cfg.extras.enforce_tags = False\n            cfg.logger = None\n\n    return cfg", "\n@pytest.fixture(scope=\"package\")\ndef cfg_train_global() -> DictConfig:\n    with initialize(version_base=\"1.3\", config_path=\"../configs\"):\n        cfg = compose(config_name=\"train.yaml\", return_hydra_config=True, overrides=[])\n\n        # set defaults for all tests\n        with open_dict(cfg):\n            cfg.paths.root_dir = str(pyrootutils.find_root(indicator=\".project-root\"))\n            cfg.trainer.max_epochs = 1\n            cfg.trainer.limit_train_batches = 0.01\n            cfg.trainer.limit_val_batches = 0.1\n            cfg.trainer.limit_test_batches = 0.1\n            cfg.trainer.accelerator = \"cpu\"\n            cfg.trainer.devices = 1\n            cfg.data.num_workers = 0\n            cfg.data.pin_memory = False\n            cfg.extras.print_config = False\n            cfg.extras.enforce_tags = False\n            cfg.logger = None\n\n    return cfg", "\n\n@pytest.fixture(scope=\"package\")\ndef cfg_eval_global() -> DictConfig:\n    with initialize(version_base=\"1.3\", config_path=\"../configs\"):\n        cfg = compose(config_name=\"eval.yaml\", return_hydra_config=True, overrides=[\"ckpt_path=.\"])\n\n        # set defaults for all tests\n        with open_dict(cfg):\n            cfg.paths.root_dir = str(pyrootutils.find_root(indicator=\".project-root\"))\n            cfg.trainer.max_epochs = 1\n            cfg.trainer.limit_test_batches = 0.1\n            cfg.trainer.accelerator = \"cpu\"\n            cfg.trainer.devices = 1\n            cfg.data.num_workers = 0\n            cfg.data.pin_memory = False\n            cfg.extras.print_config = False\n            cfg.extras.enforce_tags = False\n            cfg.logger = None\n\n    return cfg", "\n\n# this is called by each test which uses `cfg_train` arg\n# each test generates its own temporary logging path\n@pytest.fixture(scope=\"function\")\ndef cfg_train(cfg_train_global, tmp_path) -> DictConfig:\n    cfg = cfg_train_global.copy()\n\n    with open_dict(cfg):\n        cfg.paths.output_dir = str(tmp_path)\n        cfg.paths.log_dir = str(tmp_path)\n\n    yield cfg\n\n    GlobalHydra.instance().clear()", "\n\n# this is called by each test which uses `cfg_eval` arg\n# each test generates its own temporary logging path\n@pytest.fixture(scope=\"function\")\ndef cfg_eval(cfg_eval_global, tmp_path) -> DictConfig:\n    cfg = cfg_eval_global.copy()\n\n    with open_dict(cfg):\n        cfg.paths.output_dir = str(tmp_path)\n        cfg.paths.log_dir = str(tmp_path)\n\n    yield cfg\n\n    GlobalHydra.instance().clear()", ""]}
{"filename": "tests/helpers/package_available.py", "chunked_list": ["import platform\n\nimport pkg_resources\nfrom pytorch_lightning.accelerators import TPUAccelerator\n\n\ndef _package_available(package_name: str) -> bool:\n    \"\"\"Check if a package is available in your environment.\"\"\"\n    try:\n        return pkg_resources.require(package_name) is not None\n    except pkg_resources.DistributionNotFound:\n        return False", "\n\n_TPU_AVAILABLE = TPUAccelerator.is_available()\n\n_IS_WINDOWS = platform.system() == \"Windows\"\n\n_SH_AVAILABLE = not _IS_WINDOWS and _package_available(\"sh\")\n\n_DEEPSPEED_AVAILABLE = not _IS_WINDOWS and _package_available(\"deepspeed\")\n_FAIRSCALE_AVAILABLE = not _IS_WINDOWS and _package_available(\"fairscale\")", "_DEEPSPEED_AVAILABLE = not _IS_WINDOWS and _package_available(\"deepspeed\")\n_FAIRSCALE_AVAILABLE = not _IS_WINDOWS and _package_available(\"fairscale\")\n\n_WANDB_AVAILABLE = _package_available(\"wandb\")\n_NEPTUNE_AVAILABLE = _package_available(\"neptune\")\n_COMET_AVAILABLE = _package_available(\"comet_ml\")\n_MLFLOW_AVAILABLE = _package_available(\"mlflow\")\n"]}
{"filename": "tests/helpers/__init__.py", "chunked_list": [""]}
{"filename": "tests/helpers/run_sh_command.py", "chunked_list": ["from typing import List\n\nimport pytest\n\nfrom tests.helpers.package_available import _SH_AVAILABLE\n\nif _SH_AVAILABLE:\n    import sh\n\n\ndef run_sh_command(command: List[str]):\n    \"\"\"Default method for executing shell commands with pytest and sh package.\"\"\"\n    msg = None\n    try:\n        sh.python(command)\n    except sh.ErrorReturnCode as e:\n        msg = e.stderr.decode()\n    if msg:\n        pytest.fail(msg=msg)", "\n\ndef run_sh_command(command: List[str]):\n    \"\"\"Default method for executing shell commands with pytest and sh package.\"\"\"\n    msg = None\n    try:\n        sh.python(command)\n    except sh.ErrorReturnCode as e:\n        msg = e.stderr.decode()\n    if msg:\n        pytest.fail(msg=msg)", ""]}
{"filename": "tests/helpers/run_if.py", "chunked_list": ["\"\"\"Adapted from:\n\nhttps://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/helpers/runif.py\n\"\"\"\n\nimport sys\nfrom typing import Optional\n\nimport pytest\nimport torch", "import pytest\nimport torch\nfrom packaging.version import Version\nfrom pkg_resources import get_distribution\n\nfrom tests.helpers.package_available import (\n    _COMET_AVAILABLE,\n    _DEEPSPEED_AVAILABLE,\n    _FAIRSCALE_AVAILABLE,\n    _IS_WINDOWS,", "    _FAIRSCALE_AVAILABLE,\n    _IS_WINDOWS,\n    _MLFLOW_AVAILABLE,\n    _NEPTUNE_AVAILABLE,\n    _SH_AVAILABLE,\n    _TPU_AVAILABLE,\n    _WANDB_AVAILABLE,\n)\n\n\nclass RunIf:\n    \"\"\"RunIf wrapper for conditional skipping of tests.\n\n    Fully compatible with `@pytest.mark`.\n\n    Example:\n\n        @RunIf(min_torch=\"1.8\")\n        @pytest.mark.parametrize(\"arg1\", [1.0, 2.0])\n        def test_wrapper(arg1):\n            assert arg1 > 0\n    \"\"\"\n\n    def __new__(\n        self,\n        min_gpus: int = 0,\n        min_torch: Optional[str] = None,\n        max_torch: Optional[str] = None,\n        min_python: Optional[str] = None,\n        skip_windows: bool = False,\n        sh: bool = False,\n        tpu: bool = False,\n        fairscale: bool = False,\n        deepspeed: bool = False,\n        wandb: bool = False,\n        neptune: bool = False,\n        comet: bool = False,\n        mlflow: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            min_gpus: min number of GPUs required to run test\n            min_torch: minimum pytorch version to run test\n            max_torch: maximum pytorch version to run test\n            min_python: minimum python version required to run test\n            skip_windows: skip test for Windows platform\n            tpu: if TPU is available\n            sh: if `sh` module is required to run the test\n            fairscale: if `fairscale` module is required to run the test\n            deepspeed: if `deepspeed` module is required to run the test\n            wandb: if `wandb` module is required to run the test\n            neptune: if `neptune` module is required to run the test\n            comet: if `comet` module is required to run the test\n            mlflow: if `mlflow` module is required to run the test\n            kwargs: native pytest.mark.skipif keyword arguments\n        \"\"\"\n        conditions = []\n        reasons = []\n\n        if min_gpus:\n            conditions.append(torch.cuda.device_count() < min_gpus)\n            reasons.append(f\"GPUs>={min_gpus}\")\n\n        if min_torch:\n            torch_version = get_distribution(\"torch\").version\n            conditions.append(Version(torch_version) < Version(min_torch))\n            reasons.append(f\"torch>={min_torch}\")\n\n        if max_torch:\n            torch_version = get_distribution(\"torch\").version\n            conditions.append(Version(torch_version) >= Version(max_torch))\n            reasons.append(f\"torch<{max_torch}\")\n\n        if min_python:\n            py_version = (\n                f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n            )\n            conditions.append(Version(py_version) < Version(min_python))\n            reasons.append(f\"python>={min_python}\")\n\n        if skip_windows:\n            conditions.append(_IS_WINDOWS)\n            reasons.append(\"does not run on Windows\")\n\n        if tpu:\n            conditions.append(not _TPU_AVAILABLE)\n            reasons.append(\"TPU\")\n\n        if sh:\n            conditions.append(not _SH_AVAILABLE)\n            reasons.append(\"sh\")\n\n        if fairscale:\n            conditions.append(not _FAIRSCALE_AVAILABLE)\n            reasons.append(\"fairscale\")\n\n        if deepspeed:\n            conditions.append(not _DEEPSPEED_AVAILABLE)\n            reasons.append(\"deepspeed\")\n\n        if wandb:\n            conditions.append(not _WANDB_AVAILABLE)\n            reasons.append(\"wandb\")\n\n        if neptune:\n            conditions.append(not _NEPTUNE_AVAILABLE)\n            reasons.append(\"neptune\")\n\n        if comet:\n            conditions.append(not _COMET_AVAILABLE)\n            reasons.append(\"comet\")\n\n        if mlflow:\n            conditions.append(not _MLFLOW_AVAILABLE)\n            reasons.append(\"mlflow\")\n\n        reasons = [rs for cond, rs in zip(conditions, reasons) if cond]\n        return pytest.mark.skipif(\n            condition=any(conditions),\n            reason=f\"Requires: [{' + '.join(reasons)}]\",\n            **kwargs,\n        )", "\n\nclass RunIf:\n    \"\"\"RunIf wrapper for conditional skipping of tests.\n\n    Fully compatible with `@pytest.mark`.\n\n    Example:\n\n        @RunIf(min_torch=\"1.8\")\n        @pytest.mark.parametrize(\"arg1\", [1.0, 2.0])\n        def test_wrapper(arg1):\n            assert arg1 > 0\n    \"\"\"\n\n    def __new__(\n        self,\n        min_gpus: int = 0,\n        min_torch: Optional[str] = None,\n        max_torch: Optional[str] = None,\n        min_python: Optional[str] = None,\n        skip_windows: bool = False,\n        sh: bool = False,\n        tpu: bool = False,\n        fairscale: bool = False,\n        deepspeed: bool = False,\n        wandb: bool = False,\n        neptune: bool = False,\n        comet: bool = False,\n        mlflow: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            min_gpus: min number of GPUs required to run test\n            min_torch: minimum pytorch version to run test\n            max_torch: maximum pytorch version to run test\n            min_python: minimum python version required to run test\n            skip_windows: skip test for Windows platform\n            tpu: if TPU is available\n            sh: if `sh` module is required to run the test\n            fairscale: if `fairscale` module is required to run the test\n            deepspeed: if `deepspeed` module is required to run the test\n            wandb: if `wandb` module is required to run the test\n            neptune: if `neptune` module is required to run the test\n            comet: if `comet` module is required to run the test\n            mlflow: if `mlflow` module is required to run the test\n            kwargs: native pytest.mark.skipif keyword arguments\n        \"\"\"\n        conditions = []\n        reasons = []\n\n        if min_gpus:\n            conditions.append(torch.cuda.device_count() < min_gpus)\n            reasons.append(f\"GPUs>={min_gpus}\")\n\n        if min_torch:\n            torch_version = get_distribution(\"torch\").version\n            conditions.append(Version(torch_version) < Version(min_torch))\n            reasons.append(f\"torch>={min_torch}\")\n\n        if max_torch:\n            torch_version = get_distribution(\"torch\").version\n            conditions.append(Version(torch_version) >= Version(max_torch))\n            reasons.append(f\"torch<{max_torch}\")\n\n        if min_python:\n            py_version = (\n                f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n            )\n            conditions.append(Version(py_version) < Version(min_python))\n            reasons.append(f\"python>={min_python}\")\n\n        if skip_windows:\n            conditions.append(_IS_WINDOWS)\n            reasons.append(\"does not run on Windows\")\n\n        if tpu:\n            conditions.append(not _TPU_AVAILABLE)\n            reasons.append(\"TPU\")\n\n        if sh:\n            conditions.append(not _SH_AVAILABLE)\n            reasons.append(\"sh\")\n\n        if fairscale:\n            conditions.append(not _FAIRSCALE_AVAILABLE)\n            reasons.append(\"fairscale\")\n\n        if deepspeed:\n            conditions.append(not _DEEPSPEED_AVAILABLE)\n            reasons.append(\"deepspeed\")\n\n        if wandb:\n            conditions.append(not _WANDB_AVAILABLE)\n            reasons.append(\"wandb\")\n\n        if neptune:\n            conditions.append(not _NEPTUNE_AVAILABLE)\n            reasons.append(\"neptune\")\n\n        if comet:\n            conditions.append(not _COMET_AVAILABLE)\n            reasons.append(\"comet\")\n\n        if mlflow:\n            conditions.append(not _MLFLOW_AVAILABLE)\n            reasons.append(\"mlflow\")\n\n        reasons = [rs for cond, rs in zip(conditions, reasons) if cond]\n        return pytest.mark.skipif(\n            condition=any(conditions),\n            reason=f\"Requires: [{' + '.join(reasons)}]\",\n            **kwargs,\n        )", ""]}
{"filename": "src/train.py", "chunked_list": ["from typing import List, Optional, Tuple\n\nimport hydra\nimport pyrootutils\nimport pytorch_lightning as pl\nfrom omegaconf import DictConfig\nfrom pytorch_lightning import Callback, LightningDataModule, LightningModule, Trainer\nfrom pytorch_lightning.loggers import Logger\n\npyrootutils.setup_root(__file__, indicator=\".project-root\", pythonpath=True)", "\npyrootutils.setup_root(__file__, indicator=\".project-root\", pythonpath=True)\n# ------------------------------------------------------------------------------------ #\n# the setup_root above is equivalent to:\n# - adding project root dir to PYTHONPATH\n#       (so you don't need to force user to install project as a package)\n#       (necessary before importing any local modules e.g. `from src import utils`)\n# - setting up PROJECT_ROOT environment variable\n#       (which is used as a base for paths in \"configs/paths/default.yaml\")\n#       (this way all filepaths are the same no matter where you run the code)", "#       (which is used as a base for paths in \"configs/paths/default.yaml\")\n#       (this way all filepaths are the same no matter where you run the code)\n# - loading environment variables from \".env\" in root dir\n#\n# you can remove it if you:\n# 1. either install project as a package or move entry files to project root dir\n# 2. set `root_dir` to \".\" in \"configs/paths/default.yaml\"\n#\n# more info: https://github.com/ashleve/pyrootutils\n# ------------------------------------------------------------------------------------ #", "# more info: https://github.com/ashleve/pyrootutils\n# ------------------------------------------------------------------------------------ #\n\nfrom src import utils\n\nlog = utils.get_pylogger(__name__)\n\n\n@utils.task_wrapper\ndef train(cfg: DictConfig) -> Tuple[dict, dict]:\n    \"\"\"Trains the model. Can additionally evaluate on a testset, using best weights obtained during\n    training.\n\n    This method is wrapped in optional @task_wrapper decorator which applies extra utilities\n    before and after the call.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n\n    Returns:\n        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n    \"\"\"\n\n    # set seed for random number generators in pytorch, numpy and python.random\n    if cfg.get(\"seed\"):\n        pl.seed_everything(cfg.seed, workers=True)\n\n    log.info(f\"Instantiating datamodule <{cfg.data._target_}>\")\n    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n\n    log.info(f\"Instantiating model <{cfg.model._target_}>\")\n    model: LightningModule = hydra.utils.instantiate(cfg.model)\n\n    log.info(\"Instantiating callbacks...\")\n    callbacks: List[Callback] = utils.instantiate_callbacks(cfg.get(\"callbacks\"))\n\n    log.info(\"Instantiating loggers...\")\n    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n\n    log.info(f\"Instantiating trainer <{cfg.trainer._target_}>\")\n    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=logger)\n\n    object_dict = {\n        \"cfg\": cfg,\n        \"datamodule\": datamodule,\n        \"model\": model,\n        \"callbacks\": callbacks,\n        \"logger\": logger,\n        \"trainer\": trainer,\n    }\n\n    if logger:\n        log.info(\"Logging hyperparameters!\")\n        utils.log_hyperparameters(object_dict)\n\n    if cfg.get(\"train\"):\n        log.info(\"Starting training!\")\n        trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get(\"ckpt_path\"))\n\n    train_metrics = trainer.callback_metrics\n\n    if cfg.get(\"test\"):\n        log.info(\"Starting testing!\")\n        ckpt_path = trainer.checkpoint_callback.best_model_path\n        if ckpt_path == \"\":\n            log.warning(\"Best ckpt not found! Using current weights for testing...\")\n            ckpt_path = None\n        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)\n        log.info(f\"Best ckpt path: {ckpt_path}\")\n\n    test_metrics = trainer.callback_metrics\n\n    # merge train and test metrics\n    metric_dict = {**train_metrics, **test_metrics}\n\n    return metric_dict, object_dict", "@utils.task_wrapper\ndef train(cfg: DictConfig) -> Tuple[dict, dict]:\n    \"\"\"Trains the model. Can additionally evaluate on a testset, using best weights obtained during\n    training.\n\n    This method is wrapped in optional @task_wrapper decorator which applies extra utilities\n    before and after the call.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n\n    Returns:\n        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n    \"\"\"\n\n    # set seed for random number generators in pytorch, numpy and python.random\n    if cfg.get(\"seed\"):\n        pl.seed_everything(cfg.seed, workers=True)\n\n    log.info(f\"Instantiating datamodule <{cfg.data._target_}>\")\n    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n\n    log.info(f\"Instantiating model <{cfg.model._target_}>\")\n    model: LightningModule = hydra.utils.instantiate(cfg.model)\n\n    log.info(\"Instantiating callbacks...\")\n    callbacks: List[Callback] = utils.instantiate_callbacks(cfg.get(\"callbacks\"))\n\n    log.info(\"Instantiating loggers...\")\n    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n\n    log.info(f\"Instantiating trainer <{cfg.trainer._target_}>\")\n    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=logger)\n\n    object_dict = {\n        \"cfg\": cfg,\n        \"datamodule\": datamodule,\n        \"model\": model,\n        \"callbacks\": callbacks,\n        \"logger\": logger,\n        \"trainer\": trainer,\n    }\n\n    if logger:\n        log.info(\"Logging hyperparameters!\")\n        utils.log_hyperparameters(object_dict)\n\n    if cfg.get(\"train\"):\n        log.info(\"Starting training!\")\n        trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get(\"ckpt_path\"))\n\n    train_metrics = trainer.callback_metrics\n\n    if cfg.get(\"test\"):\n        log.info(\"Starting testing!\")\n        ckpt_path = trainer.checkpoint_callback.best_model_path\n        if ckpt_path == \"\":\n            log.warning(\"Best ckpt not found! Using current weights for testing...\")\n            ckpt_path = None\n        trainer.test(model=model, datamodule=datamodule, ckpt_path=ckpt_path)\n        log.info(f\"Best ckpt path: {ckpt_path}\")\n\n    test_metrics = trainer.callback_metrics\n\n    # merge train and test metrics\n    metric_dict = {**train_metrics, **test_metrics}\n\n    return metric_dict, object_dict", "\n\n@hydra.main(version_base=\"1.3\", config_path=\"../configs\", config_name=\"train.yaml\")\ndef main(cfg: DictConfig) -> Optional[float]:\n\n    # train the model\n    metric_dict, _ = train(cfg)\n\n    # safely retrieve metric value for hydra-based hyperparameter optimization\n    metric_value = utils.get_metric_value(\n        metric_dict=metric_dict, metric_name=cfg.get(\"optimized_metric\")\n    )\n\n    # return optimized metric\n    return metric_value", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "src/__init__.py", "chunked_list": [""]}
{"filename": "src/eval.py", "chunked_list": ["from typing import List, Tuple\n\nimport hydra\nimport pyrootutils\nfrom omegaconf import DictConfig\nfrom pytorch_lightning import LightningDataModule, LightningModule, Trainer\nfrom pytorch_lightning.loggers import Logger\n\npyrootutils.setup_root(__file__, indicator=\".project-root\", pythonpath=True)\n# ------------------------------------------------------------------------------------ #", "pyrootutils.setup_root(__file__, indicator=\".project-root\", pythonpath=True)\n# ------------------------------------------------------------------------------------ #\n# the setup_root above is equivalent to:\n# - adding project root dir to PYTHONPATH\n#       (so you don't need to force user to install project as a package)\n#       (necessary before importing any local modules e.g. `from src import utils`)\n# - setting up PROJECT_ROOT environment variable\n#       (which is used as a base for paths in \"configs/paths/default.yaml\")\n#       (this way all filepaths are the same no matter where you run the code)\n# - loading environment variables from \".env\" in root dir", "#       (this way all filepaths are the same no matter where you run the code)\n# - loading environment variables from \".env\" in root dir\n#\n# you can remove it if you:\n# 1. either install project as a package or move entry files to project root dir\n# 2. set `root_dir` to \".\" in \"configs/paths/default.yaml\"\n#\n# more info: https://github.com/ashleve/pyrootutils\n# ------------------------------------------------------------------------------------ #\n", "# ------------------------------------------------------------------------------------ #\n\nfrom src import utils\n\nlog = utils.get_pylogger(__name__)\n\n\n@utils.task_wrapper\ndef evaluate(cfg: DictConfig) -> Tuple[dict, dict]:\n    \"\"\"Evaluates given checkpoint on a datamodule testset.\n\n    This method is wrapped in optional @task_wrapper decorator which applies extra utilities\n    before and after the call.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n\n    Returns:\n        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n    \"\"\"\n\n    assert cfg.ckpt_path\n\n    log.info(f\"Instantiating datamodule <{cfg.data._target_}>\")\n    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n\n    log.info(f\"Instantiating model <{cfg.model._target_}>\")\n    model: LightningModule = hydra.utils.instantiate(cfg.model)\n\n    log.info(\"Instantiating loggers...\")\n    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n\n    log.info(f\"Instantiating trainer <{cfg.trainer._target_}>\")\n    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)\n\n    object_dict = {\n        \"cfg\": cfg,\n        \"datamodule\": datamodule,\n        \"model\": model,\n        \"logger\": logger,\n        \"trainer\": trainer,\n    }\n\n    if logger:\n        log.info(\"Logging hyperparameters!\")\n        utils.log_hyperparameters(object_dict)\n\n    log.info(\"Starting testing!\")\n    trainer.test(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)\n\n    # for predictions use trainer.predict(...)\n    # predictions = trainer.predict(model=model, dataloaders=dataloaders, ckpt_path=cfg.ckpt_path)\n\n    metric_dict = trainer.callback_metrics\n\n    return metric_dict, object_dict", "def evaluate(cfg: DictConfig) -> Tuple[dict, dict]:\n    \"\"\"Evaluates given checkpoint on a datamodule testset.\n\n    This method is wrapped in optional @task_wrapper decorator which applies extra utilities\n    before and after the call.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n\n    Returns:\n        Tuple[dict, dict]: Dict with metrics and dict with all instantiated objects.\n    \"\"\"\n\n    assert cfg.ckpt_path\n\n    log.info(f\"Instantiating datamodule <{cfg.data._target_}>\")\n    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)\n\n    log.info(f\"Instantiating model <{cfg.model._target_}>\")\n    model: LightningModule = hydra.utils.instantiate(cfg.model)\n\n    log.info(\"Instantiating loggers...\")\n    logger: List[Logger] = utils.instantiate_loggers(cfg.get(\"logger\"))\n\n    log.info(f\"Instantiating trainer <{cfg.trainer._target_}>\")\n    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, logger=logger)\n\n    object_dict = {\n        \"cfg\": cfg,\n        \"datamodule\": datamodule,\n        \"model\": model,\n        \"logger\": logger,\n        \"trainer\": trainer,\n    }\n\n    if logger:\n        log.info(\"Logging hyperparameters!\")\n        utils.log_hyperparameters(object_dict)\n\n    log.info(\"Starting testing!\")\n    trainer.test(model=model, datamodule=datamodule, ckpt_path=cfg.ckpt_path)\n\n    # for predictions use trainer.predict(...)\n    # predictions = trainer.predict(model=model, dataloaders=dataloaders, ckpt_path=cfg.ckpt_path)\n\n    metric_dict = trainer.callback_metrics\n\n    return metric_dict, object_dict", "\n\n@hydra.main(version_base=\"1.3\", config_path=\"../configs\", config_name=\"eval.yaml\")\ndef main(cfg: DictConfig) -> None:\n    evaluate(cfg)\n\n\nif __name__ == \"__main__\":\n    main()\n", ""]}
{"filename": "src/utils/rich_utils.py", "chunked_list": ["from pathlib import Path\nfrom typing import Sequence\n\nimport rich\nimport rich.syntax\nimport rich.tree\nfrom hydra.core.hydra_config import HydraConfig\nfrom omegaconf import DictConfig, OmegaConf, open_dict\nfrom pytorch_lightning.utilities import rank_zero_only\nfrom rich.prompt import Prompt", "from pytorch_lightning.utilities import rank_zero_only\nfrom rich.prompt import Prompt\n\nfrom src.utils import pylogger\n\nlog = pylogger.get_pylogger(__name__)\n\n\n@rank_zero_only\ndef print_config_tree(\n    cfg: DictConfig,\n    print_order: Sequence[str] = (\n        \"data\",\n        \"model\",\n        \"callbacks\",\n        \"logger\",\n        \"trainer\",\n        \"paths\",\n        \"extras\",\n    ),\n    resolve: bool = False,\n    save_to_file: bool = False,\n) -> None:\n    \"\"\"Prints content of DictConfig using Rich library and its tree structure.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n        print_order (Sequence[str], optional): Determines in what order config components are printed.\n        resolve (bool, optional): Whether to resolve reference fields of DictConfig.\n        save_to_file (bool, optional): Whether to export config to the hydra output folder.\n    \"\"\"\n\n    style = \"dim\"\n    tree = rich.tree.Tree(\"CONFIG\", style=style, guide_style=style)\n\n    queue = []\n\n    # add fields from `print_order` to queue\n    for field in print_order:\n        queue.append(field) if field in cfg else log.warning(\n            f\"Field '{field}' not found in config. Skipping '{field}' config printing...\"\n        )\n\n    # add all the other fields to queue (not specified in `print_order`)\n    for field in cfg:\n        if field not in queue:\n            queue.append(field)\n\n    # generate config tree from queue\n    for field in queue:\n        branch = tree.add(field, style=style, guide_style=style)\n\n        config_group = cfg[field]\n        if isinstance(config_group, DictConfig):\n            branch_content = OmegaConf.to_yaml(config_group, resolve=resolve)\n        else:\n            branch_content = str(config_group)\n\n        branch.add(rich.syntax.Syntax(branch_content, \"yaml\"))\n\n    # print config tree\n    rich.print(tree)\n\n    # save config tree to file\n    if save_to_file:\n        with open(Path(cfg.paths.output_dir, \"config_tree.log\"), \"w\") as file:\n            rich.print(tree, file=file)", "@rank_zero_only\ndef print_config_tree(\n    cfg: DictConfig,\n    print_order: Sequence[str] = (\n        \"data\",\n        \"model\",\n        \"callbacks\",\n        \"logger\",\n        \"trainer\",\n        \"paths\",\n        \"extras\",\n    ),\n    resolve: bool = False,\n    save_to_file: bool = False,\n) -> None:\n    \"\"\"Prints content of DictConfig using Rich library and its tree structure.\n\n    Args:\n        cfg (DictConfig): Configuration composed by Hydra.\n        print_order (Sequence[str], optional): Determines in what order config components are printed.\n        resolve (bool, optional): Whether to resolve reference fields of DictConfig.\n        save_to_file (bool, optional): Whether to export config to the hydra output folder.\n    \"\"\"\n\n    style = \"dim\"\n    tree = rich.tree.Tree(\"CONFIG\", style=style, guide_style=style)\n\n    queue = []\n\n    # add fields from `print_order` to queue\n    for field in print_order:\n        queue.append(field) if field in cfg else log.warning(\n            f\"Field '{field}' not found in config. Skipping '{field}' config printing...\"\n        )\n\n    # add all the other fields to queue (not specified in `print_order`)\n    for field in cfg:\n        if field not in queue:\n            queue.append(field)\n\n    # generate config tree from queue\n    for field in queue:\n        branch = tree.add(field, style=style, guide_style=style)\n\n        config_group = cfg[field]\n        if isinstance(config_group, DictConfig):\n            branch_content = OmegaConf.to_yaml(config_group, resolve=resolve)\n        else:\n            branch_content = str(config_group)\n\n        branch.add(rich.syntax.Syntax(branch_content, \"yaml\"))\n\n    # print config tree\n    rich.print(tree)\n\n    # save config tree to file\n    if save_to_file:\n        with open(Path(cfg.paths.output_dir, \"config_tree.log\"), \"w\") as file:\n            rich.print(tree, file=file)", "\n\n@rank_zero_only\ndef enforce_tags(cfg: DictConfig, save_to_file: bool = False) -> None:\n    \"\"\"Prompts user to input tags from command line if no tags are provided in config.\"\"\"\n\n    if not cfg.get(\"tags\"):\n        if \"id\" in HydraConfig().cfg.hydra.job:\n            raise ValueError(\"Specify tags before launching a multirun!\")\n\n        log.warning(\"No tags provided in config. Prompting user to input tags...\")\n        tags = Prompt.ask(\"Enter a list of comma separated tags\", default=\"dev\")\n        tags = [t.strip() for t in tags.split(\",\") if t != \"\"]\n\n        with open_dict(cfg):\n            cfg.tags = tags\n\n        log.info(f\"Tags: {cfg.tags}\")\n\n    if save_to_file:\n        with open(Path(cfg.paths.output_dir, \"tags.log\"), \"w\") as file:\n            rich.print(cfg.tags, file=file)", ""]}
{"filename": "src/utils/__init__.py", "chunked_list": ["from src.utils.pylogger import get_pylogger\nfrom src.utils.rich_utils import enforce_tags, print_config_tree\nfrom src.utils.utils import (\n    close_loggers,\n    extras,\n    get_metric_value,\n    instantiate_callbacks,\n    instantiate_loggers,\n    log_hyperparameters,\n    save_file,", "    log_hyperparameters,\n    save_file,\n    task_wrapper,\n)\n"]}
{"filename": "src/utils/utils.py", "chunked_list": ["import time\nimport warnings\nfrom importlib.util import find_spec\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List\n\nimport hydra\nfrom omegaconf import DictConfig\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import Logger", "from pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import Logger\nfrom pytorch_lightning.utilities import rank_zero_only\n\nfrom src.utils import pylogger, rich_utils\n\nlog = pylogger.get_pylogger(__name__)\n\n\ndef task_wrapper(task_func: Callable) -> Callable:\n    \"\"\"Optional decorator that wraps the task function in extra utilities.\n\n    Makes multirun more resistant to failure.\n\n    Utilities:\n    - Calling the `utils.extras()` before the task is started\n    - Calling the `utils.close_loggers()` after the task is finished or failed\n    - Logging the exception if occurs\n    - Logging the output dir\n    \"\"\"\n\n    def wrap(cfg: DictConfig):\n\n        # execute the task\n        try:\n\n            # apply extra utilities\n            extras(cfg)\n\n            metric_dict, object_dict = task_func(cfg=cfg)\n\n        # things to do if exception occurs\n        except Exception as ex:\n\n            # save exception to `.log` file\n            log.exception(\"\")\n\n            # when using hydra plugins like Optuna, you might want to disable raising exception\n            # to avoid multirun failure\n            raise ex\n\n        # things to always do after either success or exception\n        finally:\n\n            # display output dir path in terminal\n            log.info(f\"Output dir: {cfg.paths.output_dir}\")\n\n            # close loggers (even if exception occurs so multirun won't fail)\n            close_loggers()\n\n        return metric_dict, object_dict\n\n    return wrap", "\ndef task_wrapper(task_func: Callable) -> Callable:\n    \"\"\"Optional decorator that wraps the task function in extra utilities.\n\n    Makes multirun more resistant to failure.\n\n    Utilities:\n    - Calling the `utils.extras()` before the task is started\n    - Calling the `utils.close_loggers()` after the task is finished or failed\n    - Logging the exception if occurs\n    - Logging the output dir\n    \"\"\"\n\n    def wrap(cfg: DictConfig):\n\n        # execute the task\n        try:\n\n            # apply extra utilities\n            extras(cfg)\n\n            metric_dict, object_dict = task_func(cfg=cfg)\n\n        # things to do if exception occurs\n        except Exception as ex:\n\n            # save exception to `.log` file\n            log.exception(\"\")\n\n            # when using hydra plugins like Optuna, you might want to disable raising exception\n            # to avoid multirun failure\n            raise ex\n\n        # things to always do after either success or exception\n        finally:\n\n            # display output dir path in terminal\n            log.info(f\"Output dir: {cfg.paths.output_dir}\")\n\n            # close loggers (even if exception occurs so multirun won't fail)\n            close_loggers()\n\n        return metric_dict, object_dict\n\n    return wrap", "\n\ndef extras(cfg: DictConfig) -> None:\n    \"\"\"Applies optional utilities before the task is started.\n\n    Utilities:\n    - Ignoring python warnings\n    - Setting tags from command line\n    - Rich config printing\n    \"\"\"\n\n    # return if no `extras` config\n    if not cfg.get(\"extras\"):\n        log.warning(\"Extras config not found! <cfg.extras=null>\")\n        return\n\n    # disable python warnings\n    if cfg.extras.get(\"ignore_warnings\"):\n        log.info(\"Disabling python warnings! <cfg.extras.ignore_warnings=True>\")\n        warnings.filterwarnings(\"ignore\")\n\n    # prompt user to input tags from command line if none are provided in the config\n    if cfg.extras.get(\"enforce_tags\"):\n        log.info(\"Enforcing tags! <cfg.extras.enforce_tags=True>\")\n        rich_utils.enforce_tags(cfg, save_to_file=True)\n\n    # pretty print config tree using Rich library\n    if cfg.extras.get(\"print_config\"):\n        log.info(\"Printing config tree with Rich! <cfg.extras.print_config=True>\")\n        rich_utils.print_config_tree(cfg, resolve=True, save_to_file=True)", "\n\ndef instantiate_callbacks(callbacks_cfg: DictConfig) -> List[Callback]:\n    \"\"\"Instantiates callbacks from config.\"\"\"\n    callbacks: List[Callback] = []\n\n    if not callbacks_cfg:\n        log.warning(\"No callback configs found! Skipping..\")\n        return callbacks\n\n    if not isinstance(callbacks_cfg, DictConfig):\n        raise TypeError(\"Callbacks config must be a DictConfig!\")\n\n    for _, cb_conf in callbacks_cfg.items():\n        if isinstance(cb_conf, DictConfig) and \"_target_\" in cb_conf:\n            log.info(f\"Instantiating callback <{cb_conf._target_}>\")\n            callbacks.append(hydra.utils.instantiate(cb_conf))\n\n    return callbacks", "\n\ndef instantiate_loggers(logger_cfg: DictConfig) -> List[Logger]:\n    \"\"\"Instantiates loggers from config.\"\"\"\n    logger: List[Logger] = []\n\n    if not logger_cfg:\n        log.warning(\"No logger configs found! Skipping...\")\n        return logger\n\n    if not isinstance(logger_cfg, DictConfig):\n        raise TypeError(\"Logger config must be a DictConfig!\")\n\n    for _, lg_conf in logger_cfg.items():\n        if isinstance(lg_conf, DictConfig) and \"_target_\" in lg_conf:\n            log.info(f\"Instantiating logger <{lg_conf._target_}>\")\n            logger.append(hydra.utils.instantiate(lg_conf))\n\n    return logger", "\n\n@rank_zero_only\ndef log_hyperparameters(object_dict: dict) -> None:\n    \"\"\"Controls which config parts are saved by lightning loggers.\n\n    Additionally saves:\n    - Number of model parameters\n    \"\"\"\n\n    hparams = {}\n\n    cfg = object_dict[\"cfg\"]\n    model = object_dict[\"model\"]\n    trainer = object_dict[\"trainer\"]\n\n    if not trainer.logger:\n        log.warning(\"Logger not found! Skipping hyperparameter logging...\")\n        return\n\n    hparams[\"model\"] = cfg[\"model\"]\n\n    # save number of model parameters\n    hparams[\"model/params/total\"] = sum(p.numel() for p in model.parameters())\n    hparams[\"model/params/trainable\"] = sum(\n        p.numel() for p in model.parameters() if p.requires_grad\n    )\n    hparams[\"model/params/non_trainable\"] = sum(\n        p.numel() for p in model.parameters() if not p.requires_grad\n    )\n\n    hparams[\"data\"] = cfg[\"data\"]\n    hparams[\"trainer\"] = cfg[\"trainer\"]\n\n    hparams[\"callbacks\"] = cfg.get(\"callbacks\")\n    hparams[\"extras\"] = cfg.get(\"extras\")\n\n    hparams[\"task_name\"] = cfg.get(\"task_name\")\n    hparams[\"tags\"] = cfg.get(\"tags\")\n    hparams[\"ckpt_path\"] = cfg.get(\"ckpt_path\")\n    hparams[\"seed\"] = cfg.get(\"seed\")\n\n    # send hparams to all loggers\n    for logger in trainer.loggers:\n        logger.log_hyperparams(hparams)", "\n\ndef get_metric_value(metric_dict: dict, metric_name: str) -> float:\n    \"\"\"Safely retrieves value of the metric logged in LightningModule.\"\"\"\n\n    if not metric_name:\n        log.info(\"Metric name is None! Skipping metric value retrieval...\")\n        return None\n\n    if metric_name not in metric_dict:\n        raise Exception(\n            f\"Metric value not found! <metric_name={metric_name}>\\n\"\n            \"Make sure metric name logged in LightningModule is correct!\\n\"\n            \"Make sure `optimized_metric` name in `hparams_search` config is correct!\"\n        )\n\n    metric_value = metric_dict[metric_name].item()\n    log.info(f\"Retrieved metric value! <{metric_name}={metric_value}>\")\n\n    return metric_value", "\n\ndef close_loggers() -> None:\n    \"\"\"Makes sure all loggers closed properly (prevents logging failure during multirun).\"\"\"\n\n    log.info(\"Closing loggers...\")\n\n    if find_spec(\"wandb\"):  # if wandb is installed\n        import wandb\n\n        if wandb.run:\n            log.info(\"Closing wandb!\")\n            wandb.finish()", "\n\n@rank_zero_only\ndef save_file(path: str, content: str) -> None:\n    \"\"\"Save file in rank zero mode (only on one process in multi-GPU setup).\"\"\"\n    with open(path, \"w+\") as file:\n        file.write(content)\n"]}
{"filename": "src/utils/pylogger.py", "chunked_list": ["import logging\n\nfrom pytorch_lightning.utilities import rank_zero_only\n\n\ndef get_pylogger(name=__name__) -> logging.Logger:\n    \"\"\"Initializes multi-GPU-friendly python command line logger.\"\"\"\n\n    logger = logging.getLogger(name)\n\n    # this ensures all logging levels get marked with the rank zero decorator\n    # otherwise logs would get multiplied for each GPU process in multi-GPU setup\n    logging_levels = (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\")\n    for level in logging_levels:\n        setattr(logger, level, rank_zero_only(getattr(logger, level)))\n\n    return logger", ""]}
{"filename": "src/utils/vis_utils.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef visualize(image, recon_combined, recons, pred_masks, gt_masks, attns, colored_box=True):\n    \"\"\"\n    `image`: [B, 3, H, W]\n    `recon_combined`: [B, 3, H, W]\n    `recons`: [B, K, H, W, C]\n    `pred_masks`: [B, K, H, W, 1]\n    `gt_masks`: [B, K, H, W, 1]\n    `attns`: (B, T, N_heads, N_in, K)\n    \"\"\"\n\n    img = image[:1]\n    recon_combined = recon_combined[:1]\n    recons = recons[:1]\n    pred_masks = pred_masks[:1]\n    gt_masks = gt_masks[:1]\n    attns = attns[:1]\n\n    _, K, H, W, _ = pred_masks.shape\n    _, T, _, _, _ = attns.shape\n\n    # get binarized masks\n    pred_mask_max_idxs = torch.argmax(pred_masks.squeeze(-1), dim=1)\n    # `mask_max_idxs`: (1, H, W)\n\n    pred_seg_masks = torch.zeros_like(pred_masks.squeeze(-1))\n    pred_seg_masks[\n        torch.arange(1)[:, None, None],\n        pred_mask_max_idxs,\n        torch.arange(H)[None, :, None],\n        torch.arange(W)[None, None, :],\n    ] = 1.0\n    pred_seg_masks = pred_seg_masks.unsqueeze(-1)\n    # `pred_seg_masks`: (1, K, H, W, 1)\n\n    pad = (0, 0, 2, 2, 2, 2)\n\n    # set colors\n    slot_colors = (\n        torch.tensor(\n            np.array(\n                [\n                    [255, 0, 0],\n                    [255, 127, 0],\n                    [255, 255, 0],\n                    [0, 255, 0],\n                    [0, 0, 255],\n                    [75, 0, 130],\n                    [148, 0, 211],\n                    [0, 255, 255],\n                    [153, 255, 153],\n                    [255, 153, 204],\n                    [102, 0, 51],\n                    [128, 128, 128],\n                    [255, 255, 255],\n                ]\n            ),\n            dtype=torch.float32,\n        )\n        / 255.0\n    )\n\n    # handle the multi-head attention\n    attns = torch.mean(attns[:, :], dim=2).permute(0, 1, 3, 2).view(1, -1, K, H, W).unsqueeze(-1)\n    # `attns`: (1, T, K, H, W, 1)\n\n    # reshape tensors\n    attns = attns.reshape(-1, K, H, W, 1)\n    # `attns`: (T, K, H, W, 1)\n\n    attns = torch.cat([attns, pred_masks, pred_seg_masks], dim=0)\n    N_row = attns.shape[0]\n    # `attns`: (N_row, K, H, W, 1)\n    # N_row = T + 2\n    # `attns` - attention maps over iterations\n    # `pred_masks` - alpha mask generated by decoder\n    # `pred_seg_masks` - binary mask from `pred_masks` with argmax\n\n    img = torch.einsum(\"nchw->nhwc\", img)\n    gt_col = torch.ones((N_row, H, W, 3), dtype=img.dtype, device=img.device)\n    gt_col[-2] = 0  # to draw seg mask by adding values\n    gt_col[-1] = img\n    # draw boundary box for the original image\n    gt_col = F.pad(gt_col, pad=pad, mode=\"constant\", value=1.0)\n    if colored_box:\n        gt_col[1:, :2, :] = gt_col[1:, -2:, :] = gt_col[1:, :, :2] = gt_col[\n            1:, :, -2:\n        ] = slot_colors[\n            -2\n        ]  # gray\n    gt_col = F.pad(gt_col, pad=pad, mode=\"constant\", value=1.0)\n    # `gt_col`: [T+2, H, W, C]\n\n    recon_combined = torch.einsum(\"nchw->nhwc\", recon_combined)\n    pred_col = torch.ones(\n        (N_row, H, W, 3), dtype=recon_combined.dtype, device=recon_combined.device\n    )\n    pred_col[-2] = 0  # to draw seg mask by adding values\n    pred_col[-1] = recon_combined\n    # draw boundary box for the reconstructed image\n    pred_col = F.pad(pred_col, pad=pad, mode=\"constant\", value=1.0)\n    if colored_box:\n        pred_col[1:, :2, :] = pred_col[1:, -2:, :] = pred_col[1:, :, :2] = pred_col[\n            1:, :, -2:\n        ] = slot_colors[\n            -2\n        ]  # gray\n    pred_col = F.pad(pred_col, pad=pad, mode=\"constant\", value=1.0)\n    # `pred_col`: [T+2, H, W, C]\n\n    for k in range(K):\n\n        # # get vis. of attention maps based on the original image\n        picture = torch.ones_like(img) * attns[:, k, :, :, :]\n\n        # overwrite vis. of alpha mask with the recon. image\n        picture[-2] = pred_seg_masks[:, k, :, :, :]\n        picture[-1] = recons[:, k, :, :, :] * pred_seg_masks[:, k, :, :, :] + (\n            1 - pred_seg_masks[:, k, :, :, :]\n        )\n\n        try:\n            gt_col[-2, 4:-4, 4:-4, :] += gt_masks[0, k, :, :, :] * slot_colors[k - 1].to(\n                pred_seg_masks.device\n            )  # `k-1` -> to give white color to background\n        except:\n            # when #slots > # objects. it is not the big deal\n            pass \n\n        pred_col[-2, 4:-4, 4:-4, :] += pred_seg_masks[0, k, :, :, :] * slot_colors[k - 1].to(\n            pred_seg_masks.device\n        )  # `k-1` -> to give white color to background\n\n        # draw boundary box for slots\n        picture = F.pad(picture, pad=pad, mode=\"constant\", value=1.0)\n        if colored_box:\n            picture[:, :2, :] = picture[:, -2:, :] = picture[:, :, :2] = picture[\n                :, :, -2:\n            ] = slot_colors[k]\n        picture = F.pad(picture, pad=pad, mode=\"constant\", value=1.0)\n\n        if k == 0:\n            log_img = torch.cat([picture], dim=2)\n        else:\n            log_img = torch.cat([log_img, picture], dim=2)\n\n    bg_mask = torch.where(\n        torch.sum(gt_col[-2, 4:-4, 4:-4, :], dim=-1, keepdim=True) == 0,\n        torch.ones_like(gt_col[-2, 4:-4, 4:-4, :]),\n        torch.zeros_like(gt_col[-2, 4:-4, 4:-4, :]),\n    )\n    gt_col[-2, 4:-4, 4:-4, :] += bg_mask * 0.5\n    log_img = torch.cat([gt_col, pred_col, log_img], dim=2)\n\n    log_img = log_img.permute(0, 3, 1, 2)\n    return log_img", ""]}
{"filename": "src/utils/evaluator.py", "chunked_list": ["import numpy as np\nimport torch\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.special import comb\n\n\nclass ARIEvaluator:\n    \"\"\"\"\"\"\n\n    def __init__(self):\n        self.aris = []\n\n    def evaluate(self, pred, label):\n        \"\"\"\n        :param data: (image, mask)\n            image: (B, 3, H, W)\n            pred : (B, N0, H, W)\n            label: (B, N1, H, W)\n        :return: average ari\n        \"\"\"\n\n        B, K, H, W = pred.size()\n\n        # reduced to (B, K, H, W), with 1-0 values\n\n        # max_index (B, H, W)\n        max_index = torch.argmax(pred, dim=1)\n        # get binarized masks (B, K, H, W)\n        pred = torch.zeros_like(pred)\n        pred[\n            torch.arange(B)[:, None, None],\n            max_index,\n            torch.arange(H)[None, :, None],\n            torch.arange(W)[None, None, :],\n        ] = 1.0\n\n        for b in range(B):\n            this_ari = self.compute_mask_ari(label[b].to(pred.device), pred[b].to(pred.device))\n            self.aris.append(this_ari)\n\n    def reset(self):\n        self.aris = []\n\n    def get_results(self):\n        return np.mean(self.aris) if self.aris != [] else 0\n\n    def compute_ari(self, table):\n        \"\"\"Compute ari, given the index table.\n\n        :param table: (r, s)\n        :return:\n        \"\"\"\n\n        # # (r,)\n        # a = table.sum(axis=1)\n        # # (s,)\n        # b = table.sum(axis=0)\n        # n = a.sum()\n        # (r,)\n        a = table.sum(dim=1)\n        # (s,)\n        b = table.sum(dim=0)\n        n = a.sum()\n\n        comb_a = comb(a.detach().cpu().numpy(), 2).sum()\n        comb_b = comb(b.detach().cpu().numpy(), 2).sum()\n        comb_n = comb(n.detach().cpu().numpy(), 2)\n        comb_table = comb(table.detach().cpu().numpy(), 2).sum()\n\n        if comb_b == comb_a == comb_n == comb_table:\n            # the perfect case\n            ari = 1.0\n        else:\n            ari = (comb_table - comb_a * comb_b / comb_n) / (\n                0.5 * (comb_a + comb_b) - (comb_a * comb_b) / comb_n\n            )\n\n        return ari\n\n    def compute_mask_ari(self, mask0, mask1):\n        \"\"\"Given two sets of masks, compute ari.\n\n        :param mask0: ground truth mask, (N0, H, W)\n        :param mask1: predicted mask, (N1, H, W)\n        :return:\n        \"\"\"\n\n        # will first need to compute a table of shape (N0, N1)\n        # (N0, 1, H, W)\n        mask0 = mask0[:, None].byte()\n        # (1, N1, H, W)\n        mask1 = mask1[None, :].byte()\n        # (N0, N1, H, W)\n        agree = mask0 & mask1\n        # (N0, N1)\n        table = agree.sum(dim=-1).sum(dim=-1)\n\n        return self.compute_ari(table)", "\n\nclass mIoUEvaluator:\n    def __init__(self):\n        self.mious = []\n\n    def evaluate(self, pred, label):\n        \"\"\"\n        :param data: (image, mask)\n            image: (B, 3, H, W)\n            pred : (B, N0, H, W)\n            label: (B, N1, H, W)\n        :return: average miou\n        \"\"\"\n        from torch import arange as ar\n\n        B, K, H, W = pred.size()\n\n        # reduced to (B, K, H, W), with 1-0 values\n\n        # max_index (B, H, W)\n        max_index = torch.argmax(pred, dim=1)\n        # get binarized masks (B, K, H, W)\n        pred = torch.zeros_like(pred)\n        pred[ar(B)[:, None, None], max_index, ar(H)[None, :, None], ar(W)[None, None, :]] = 1.0\n\n        for b in range(B):\n            this_miou = self.compute_miou(label[b].to(pred.device), pred[b].to(pred.device))\n            self.mious.append(this_miou)\n\n    def reset(self):\n        self.mious = []\n\n    def get_results(self):\n        return np.mean(self.mious) if self.mious != [] else 0\n\n    def compute_miou(self, mask0, mask1):\n        \"\"\"\n        mask0: [N0, H, W]\n        mask1: [N1, H, W]\n        \"\"\"\n\n        mask0 = mask0.reshape(mask0.shape[0], -1)[:, None, :]  # [N0, 1, H*W]\n        mask1 = mask1.reshape(mask1.shape[0], -1)[None, :, :]  # [1, N1, H*W]\n\n        union = torch.sum(torch.clip(mask0 + mask1, 0, 1), dim=-1)  # [1, N0, N1]\n        intersection = torch.sum(mask0 * mask1, dim=-1)  # [1, N0, N1]\n        iou = intersection / (union + 1e-8)  # [N0, N1]\n\n        row_ind, col_ind = linear_sum_assignment(iou.cpu().detach().numpy() * -1)\n        miou = iou[row_ind, col_ind].mean().cpu().detach().numpy()\n\n        return miou", ""]}
{"filename": "src/data/clevrtex_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\nimport torch\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import transforms\n\nfrom src.data.components.clevrtex import CLEVRTEX\n\n\nclass CLEVRTEXDataModule(LightningDataModule):\n    \"\"\"LightningDataModule for CLEVRTEX dataset.\n\n    A DataModule implements 5 key methods:\n\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"clvt\",\n        data_dir: str = \"data/\",\n        img_size: int = 128,\n        crop_size: int = 0,\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        transform_list = list()\n        if crop_size > 0:\n            transform_list.append(transforms.CenterCrop(crop_size))\n        transform_list.extend(\n            [\n                transforms.Resize((img_size, img_size)),\n            ]\n        )\n        self.transforms = transforms.Compose(transform_list)\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n\n        self.data_train = CLEVRTEX(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=True,\n        )\n\n        self.data_val = CLEVRTEX(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=False,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nclass CLEVRTEXDataModule(LightningDataModule):\n    \"\"\"LightningDataModule for CLEVRTEX dataset.\n\n    A DataModule implements 5 key methods:\n\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"clvt\",\n        data_dir: str = \"data/\",\n        img_size: int = 128,\n        crop_size: int = 0,\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        transform_list = list()\n        if crop_size > 0:\n            transform_list.append(transforms.CenterCrop(crop_size))\n        transform_list.extend(\n            [\n                transforms.Resize((img_size, img_size)),\n            ]\n        )\n        self.transforms = transforms.Compose(transform_list)\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n\n        self.data_train = CLEVRTEX(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=True,\n        )\n\n        self.data_val = CLEVRTEX(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=False,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nif __name__ == \"__main__\":\n    _ = CLEVRTEXDataModule()\n"]}
{"filename": "src/data/__init__.py", "chunked_list": [""]}
{"filename": "src/data/ptr_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\nimport torch\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import transforms\n\nfrom src.data.components.ptr import PTR\n\n\nclass PTRDataModule(LightningDataModule):\n    \"\"\"LightningDataModule for PTR dataset.\n\n    A DataModule implements 5 key methods:\n\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"ptr\",\n        data_dir: str = \"data/\",\n        img_size: int = 128,\n        crop_size: int = 0,\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        transform_list = list()\n        if crop_size > 0:\n            transform_list.append(transforms.CenterCrop(crop_size))\n        transform_list.extend(\n            [\n                transforms.Resize((img_size, img_size)),\n            ]\n        )\n        self.transforms = transforms.Compose(transform_list)\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n\n        self.data_train = PTR(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=True,\n        )\n\n        self.data_val = PTR(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=False,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nclass PTRDataModule(LightningDataModule):\n    \"\"\"LightningDataModule for PTR dataset.\n\n    A DataModule implements 5 key methods:\n\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"ptr\",\n        data_dir: str = \"data/\",\n        img_size: int = 128,\n        crop_size: int = 0,\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        transform_list = list()\n        if crop_size > 0:\n            transform_list.append(transforms.CenterCrop(crop_size))\n        transform_list.extend(\n            [\n                transforms.Resize((img_size, img_size)),\n            ]\n        )\n        self.transforms = transforms.Compose(transform_list)\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n\n        self.data_train = PTR(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=True,\n        )\n\n        self.data_val = PTR(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=False,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nif __name__ == \"__main__\":\n    _ = PTRDataModule()\n"]}
{"filename": "src/data/clevr6_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\nimport torch\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import transforms\n\nfrom src.data.components.clevr6 import CLEVR6\n\n\nclass CLEVR6DataModule(LightningDataModule):\n    \"\"\"LightningDataModule for CLEVR6 dataset.\n\n    A DataModule implements 5 key methods:\n\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"clv6\",\n        data_dir: str = \"data/\",\n        img_size: int = 128,\n        crop_size: int = 0,\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        transform_list = list()\n        if crop_size > 0:\n            transform_list.append(transforms.CenterCrop(crop_size))\n        transform_list.extend(\n            [\n                transforms.Resize((img_size, img_size)),\n            ]\n        )\n        self.transforms = transforms.Compose(transform_list)\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n\n        self.data_train = CLEVR6(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=True,\n        )\n\n        self.data_val = CLEVR6(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=False,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nclass CLEVR6DataModule(LightningDataModule):\n    \"\"\"LightningDataModule for CLEVR6 dataset.\n\n    A DataModule implements 5 key methods:\n\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"clv6\",\n        data_dir: str = \"data/\",\n        img_size: int = 128,\n        crop_size: int = 0,\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        transform_list = list()\n        if crop_size > 0:\n            transform_list.append(transforms.CenterCrop(crop_size))\n        transform_list.extend(\n            [\n                transforms.Resize((img_size, img_size)),\n            ]\n        )\n        self.transforms = transforms.Compose(transform_list)\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n\n        self.data_train = CLEVR6(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=True,\n        )\n\n        self.data_val = CLEVR6(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=False,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nif __name__ == \"__main__\":\n    _ = CLEVR6DataModule()"]}
{"filename": "src/data/movi_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\nimport torch\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import transforms\n\nfrom src.data.components.movi import MOVi\n\n\nclass MOViDataModule(LightningDataModule):\n    \"\"\"LightningDataModule for MOVi dataset.\n\n    A DataModule implements 5 key methods:\n\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"movi\",\n        data_dir: str = \"data/\",\n        img_size: int = 128,\n        crop_size: int = 0,\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        transform_list = list()\n        if crop_size > 0:\n            transform_list.append(transforms.CenterCrop(crop_size))\n        transform_list.extend(\n            [\n                transforms.Resize((img_size, img_size)),\n            ]\n        )\n        self.transforms = transforms.Compose(transform_list)\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n\n        self.data_train = MOVi(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=True,\n        )\n\n        self.data_val = MOVi(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=False,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nclass MOViDataModule(LightningDataModule):\n    \"\"\"LightningDataModule for MOVi dataset.\n\n    A DataModule implements 5 key methods:\n\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"movi\",\n        data_dir: str = \"data/\",\n        img_size: int = 128,\n        crop_size: int = 0,\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        transform_list = list()\n        if crop_size > 0:\n            transform_list.append(transforms.CenterCrop(crop_size))\n        transform_list.extend(\n            [\n                transforms.Resize((img_size, img_size)),\n            ]\n        )\n        self.transforms = transforms.Compose(transform_list)\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        pass\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n\n        self.data_train = MOVi(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=True,\n        )\n\n        self.data_val = MOVi(\n            data_dir=self.hparams.data_dir,\n            img_size=self.hparams.img_size,\n            transform=self.transforms,\n            train=False,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nif __name__ == \"__main__\":\n    _ = MOViDataModule()\n"]}
{"filename": "src/data/mnist_datamodule.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\nimport torch\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import transforms\n\n\nclass MNISTDataModule(LightningDataModule):\n    \"\"\"Example of LightningDataModule for MNIST dataset.\n\n    A DataModule implements 5 key methods:\n\n        def prepare_data(self):\n            # things to do on 1 GPU/TPU (not on every GPU/TPU in DDP)\n            # download data, pre-process, split, save to disk, etc...\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: str = \"data/\",\n        train_val_test_split: Tuple[int, int, int] = (55_000, 5_000, 10_000),\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        self.transforms = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        )\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    @property\n    def num_classes(self):\n        return 10\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        MNIST(self.hparams.data_dir, train=True, download=True)\n        MNIST(self.hparams.data_dir, train=False, download=True)\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n        # load and split datasets only if not loaded already\n        if not self.data_train and not self.data_val and not self.data_test:\n            trainset = MNIST(self.hparams.data_dir, train=True, transform=self.transforms)\n            testset = MNIST(self.hparams.data_dir, train=False, transform=self.transforms)\n            dataset = ConcatDataset(datasets=[trainset, testset])\n            self.data_train, self.data_val, self.data_test = random_split(\n                dataset=dataset,\n                lengths=self.hparams.train_val_test_split,\n                generator=torch.Generator().manual_seed(42),\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_test,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\nclass MNISTDataModule(LightningDataModule):\n    \"\"\"Example of LightningDataModule for MNIST dataset.\n\n    A DataModule implements 5 key methods:\n\n        def prepare_data(self):\n            # things to do on 1 GPU/TPU (not on every GPU/TPU in DDP)\n            # download data, pre-process, split, save to disk, etc...\n        def setup(self, stage):\n            # things to do on every process in DDP\n            # load data, set variables, etc...\n        def train_dataloader(self):\n            # return train dataloader\n        def val_dataloader(self):\n            # return validation dataloader\n        def test_dataloader(self):\n            # return test dataloader\n        def teardown(self):\n            # called on every process in DDP\n            # clean up after fit or test\n\n    This allows you to share a full dataset without explaining how to download,\n    split, transform and process the data.\n\n    Read the docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html\n    \"\"\"\n\n    def __init__(\n        self,\n        data_dir: str = \"data/\",\n        train_val_test_split: Tuple[int, int, int] = (55_000, 5_000, 10_000),\n        batch_size: int = 64,\n        num_workers: int = 0,\n        pin_memory: bool = False,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        # data transformations\n        self.transforms = transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n        )\n\n        self.data_train: Optional[Dataset] = None\n        self.data_val: Optional[Dataset] = None\n        self.data_test: Optional[Dataset] = None\n\n    @property\n    def num_classes(self):\n        return 10\n\n    def prepare_data(self):\n        \"\"\"Download data if needed.\n\n        Do not use it to assign state (self.x = y).\n        \"\"\"\n        MNIST(self.hparams.data_dir, train=True, download=True)\n        MNIST(self.hparams.data_dir, train=False, download=True)\n\n    def setup(self, stage: Optional[str] = None):\n        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n\n        This method is called by lightning with both `trainer.fit()` and `trainer.test()`, so be\n        careful not to execute things like random split twice!\n        \"\"\"\n        # load and split datasets only if not loaded already\n        if not self.data_train and not self.data_val and not self.data_test:\n            trainset = MNIST(self.hparams.data_dir, train=True, transform=self.transforms)\n            testset = MNIST(self.hparams.data_dir, train=False, transform=self.transforms)\n            dataset = ConcatDataset(datasets=[trainset, testset])\n            self.data_train, self.data_val, self.data_test = random_split(\n                dataset=dataset,\n                lengths=self.hparams.train_val_test_split,\n                generator=torch.Generator().manual_seed(42),\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=self.data_train,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            dataset=self.data_val,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=self.data_test,\n            batch_size=self.hparams.batch_size,\n            num_workers=self.hparams.num_workers,\n            pin_memory=self.hparams.pin_memory,\n            shuffle=False,\n        )\n\n    def teardown(self, stage: Optional[str] = None):\n        \"\"\"Clean up after fit or test.\"\"\"\n        pass\n\n    def state_dict(self):\n        \"\"\"Extra things to save to checkpoint.\"\"\"\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Things to do when loading checkpoint.\"\"\"\n        pass", "\n\nif __name__ == \"__main__\":\n    _ = MNISTDataModule()\n"]}
{"filename": "src/data/components/clevrtex.py", "chunked_list": ["import json\nimport os\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torchvision.io import ImageReadMode, read_image\nfrom torchvision.transforms import transforms\n", "from torchvision.transforms import transforms\n\n\nclass CLEVRTEX(Dataset):\n    def __init__(\n        self,\n        data_dir: str = \"data/clevrtex\",\n        img_size: int = 128,\n        transform: transforms.Compose = None,\n        train: bool = True,\n    ):\n        super().__init__()\n\n        self.img_size = img_size\n        self.train = train\n        self.stage = \"train\" if train else \"val\"\n\n        self.max_num_objs = 10\n        self.max_num_masks = self.max_num_objs + 1\n\n        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n        self.scene_dir = os.path.join(data_dir, \"scenes\")\n\n        self.files = sorted(os.listdir(self.image_dir))\n        self.num_files = len(self.files)\n\n        self.transform = transform\n\n    def __getitem__(self, index):\n        image_filename = self.files[index]\n        img = (\n            read_image(os.path.join(self.image_dir, image_filename), ImageReadMode.RGB)\n            .float()\n            .div(255.0)\n        )\n        img = self.transform(img)\n        sample = {\"image\": img}\n\n        scene_name = image_filename[:-3] + \"json\"\n        metadata = json.load(open(os.path.join(self.scene_dir, self.stage, scene_name)))\n\n        if not self.train:\n            mask_filename = image_filename[:-4] + \"_flat.png\"\n            masks = read_image(os.path.join(self.mask_dir, mask_filename)).long().squeeze(0)\n            masks = F.one_hot(masks, self.max_num_masks).permute(2, 0, 1)\n            masks = self.transform(masks).unsqueeze(-1)\n            # masks: (max_num_masks, H, W, 1)\n\n            sample[\"masks\"] = masks.float()\n            sample[\"num_objects\"] = len(metadata[\"objects\"])\n\n        return sample\n\n    def __len__(self):\n        return self.num_files", ""]}
{"filename": "src/data/components/__init__.py", "chunked_list": [""]}
{"filename": "src/data/components/clevr6.py", "chunked_list": ["import json\nimport os\nfrom collections import defaultdict\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.io import ImageReadMode, read_image\nfrom torchvision.transforms import transforms\n\n\nclass CLEVR6(Dataset):\n    def __init__(\n        self,\n        data_dir: str = \"data/CLEVR6\",\n        img_size: int = 128,\n        transform: transforms.Compose = None,\n        train: bool = True,\n    ):\n        super().__init__()\n\n        self.img_size = img_size\n        self.train = train\n        self.stage = \"train\" if train else \"val\"\n\n        self.max_num_objs = 6\n        self.max_num_masks = self.max_num_objs + 1\n\n        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n        self.scene_dir = os.path.join(data_dir, \"scenes\")\n        self.metadata = json.load(\n            open(os.path.join(self.scene_dir, f\"CLEVR_{self.stage}_scenes.json\"))\n        )\n\n        self.files = sorted(os.listdir(self.image_dir))\n        self.num_files = len(self.files)\n\n        self.transform = transform\n\n        if not train:\n            self.masks = defaultdict(list)\n            masks = sorted(os.listdir(self.mask_dir))\n            for mask in masks:\n                split = mask.split(\"_\")\n                filename = \"_\".join(split[:3]) + \".png\"\n                self.masks[filename].append(mask)\n            del masks\n\n    def __getitem__(self, index):\n        filename = self.metadata[\"scenes\"][index][\"image_filename\"]\n\n        img = (\n            read_image(os.path.join(self.image_dir, filename), ImageReadMode.RGB)\n            .float()\n            .div(255.0)\n        )\n        img = self.transform(img)\n        sample = {\"image\": img}\n\n        if not self.train:\n            masks = list()\n            for mask_filename in self.masks[filename]:\n                mask = (\n                    read_image(os.path.join(self.mask_dir, mask_filename), ImageReadMode.GRAY)\n                    .div(255)\n                    .long()\n                )\n                mask = self.transform(mask)\n                masks.append(mask)\n            masks = torch.cat(masks, dim=0).unsqueeze(-1)\n            # masks: (num_objects + 1, H, W, 1)\n\n            num_masks = masks.shape[0]\n            if num_masks < self.max_num_masks:\n                masks = torch.cat(\n                    (\n                        masks,\n                        torch.zeros(\n                            (self.max_num_masks - num_masks, self.img_size, self.img_size, 1)\n                        ),\n                    ),\n                    dim=0,\n                )\n            # masks: (max_num_masks, H, W, 1)\n\n            sample[\"masks\"] = masks.float()\n            sample[\"num_objects\"] = num_masks - 1\n\n        return sample\n\n    def __len__(self):\n        return self.num_files", "\n\nclass CLEVR6(Dataset):\n    def __init__(\n        self,\n        data_dir: str = \"data/CLEVR6\",\n        img_size: int = 128,\n        transform: transforms.Compose = None,\n        train: bool = True,\n    ):\n        super().__init__()\n\n        self.img_size = img_size\n        self.train = train\n        self.stage = \"train\" if train else \"val\"\n\n        self.max_num_objs = 6\n        self.max_num_masks = self.max_num_objs + 1\n\n        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n        self.scene_dir = os.path.join(data_dir, \"scenes\")\n        self.metadata = json.load(\n            open(os.path.join(self.scene_dir, f\"CLEVR_{self.stage}_scenes.json\"))\n        )\n\n        self.files = sorted(os.listdir(self.image_dir))\n        self.num_files = len(self.files)\n\n        self.transform = transform\n\n        if not train:\n            self.masks = defaultdict(list)\n            masks = sorted(os.listdir(self.mask_dir))\n            for mask in masks:\n                split = mask.split(\"_\")\n                filename = \"_\".join(split[:3]) + \".png\"\n                self.masks[filename].append(mask)\n            del masks\n\n    def __getitem__(self, index):\n        filename = self.metadata[\"scenes\"][index][\"image_filename\"]\n\n        img = (\n            read_image(os.path.join(self.image_dir, filename), ImageReadMode.RGB)\n            .float()\n            .div(255.0)\n        )\n        img = self.transform(img)\n        sample = {\"image\": img}\n\n        if not self.train:\n            masks = list()\n            for mask_filename in self.masks[filename]:\n                mask = (\n                    read_image(os.path.join(self.mask_dir, mask_filename), ImageReadMode.GRAY)\n                    .div(255)\n                    .long()\n                )\n                mask = self.transform(mask)\n                masks.append(mask)\n            masks = torch.cat(masks, dim=0).unsqueeze(-1)\n            # masks: (num_objects + 1, H, W, 1)\n\n            num_masks = masks.shape[0]\n            if num_masks < self.max_num_masks:\n                masks = torch.cat(\n                    (\n                        masks,\n                        torch.zeros(\n                            (self.max_num_masks - num_masks, self.img_size, self.img_size, 1)\n                        ),\n                    ),\n                    dim=0,\n                )\n            # masks: (max_num_masks, H, W, 1)\n\n            sample[\"masks\"] = masks.float()\n            sample[\"num_objects\"] = num_masks - 1\n\n        return sample\n\n    def __len__(self):\n        return self.num_files", ""]}
{"filename": "src/data/components/ptr.py", "chunked_list": ["import json\nimport os\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn.functional as F\nfrom pycocotools import mask as pycocotools_mask\nfrom torch.utils.data import Dataset\nfrom torchvision.io import ImageReadMode, read_image\nfrom torchvision.transforms import transforms", "from torchvision.io import ImageReadMode, read_image\nfrom torchvision.transforms import transforms\n\n\nclass PTR(Dataset):\n    def __init__(\n        self,\n        data_dir: str = \"data/PTR\",\n        img_size: int = 128,\n        transform: transforms.Compose = None,\n        train: bool = True,\n    ):\n        super().__init__()\n\n        self.max_num_objs = 6\n        self.max_num_masks = self.max_num_objs + 1\n\n        self.img_size = img_size\n        self.train = train\n        self.stage = \"train\" if train else \"val\"\n\n        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n        self.scene_dir = os.path.join(data_dir, \"scenes\")\n\n        self.files = sorted(os.listdir(self.image_dir))\n        self.num_files = len(self.files)\n\n        self.transform = transform\n\n    def __getitem__(self, index):\n        image_filename = self.files[index]\n        img = (\n            read_image(os.path.join(self.image_dir, image_filename), ImageReadMode.RGB)\n            .float()\n            .div(255.0)\n        )\n        img = self.transform(img)\n        sample = {\"image\": img}\n\n        scene_name = image_filename[:-3] + \"json\"\n        metadata = json.load(open(os.path.join(self.scene_dir, self.stage, scene_name)))\n\n        if not self.train:\n            masks = list()\n            for obj in metadata[\"objects\"]:\n                masks.append(obj[\"obj_mask\"])\n            masks = torch.tensor(pycocotools_mask.decode(masks), dtype=torch.long)\n            masks = torch.einsum(\"hwn -> nhw\", masks)\n            masks = self.transform(masks.unsqueeze(1)).squeeze(1)\n            masks = torch.cat(\n                [(torch.sum(masks, dim=0, keepdim=True) == 0).long(), masks], dim=0\n            ).unsqueeze(-1)\n            # masks: (num_objects + 1, H, W, 1)\n\n            num_masks = masks.shape[0]\n            if num_masks < self.max_num_masks:\n                masks = torch.cat(\n                    (\n                        masks,\n                        torch.zeros(\n                            (self.max_num_masks - num_masks, self.img_size, self.img_size, 1)\n                        ),\n                    ),\n                    dim=0,\n                )\n            # masks: (max_num_masks, H, W, 1)\n\n            sample[\"masks\"] = masks.float()\n            sample[\"num_objects\"] = num_masks - 1\n\n        return sample\n\n    def __len__(self):\n        return self.num_files", ""]}
{"filename": "src/data/components/movi.py", "chunked_list": ["import json\nimport os\nfrom collections import defaultdict\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torchvision.io import ImageReadMode, read_image\nfrom torchvision.transforms import transforms\n", "from torchvision.transforms import transforms\n\n\nclass MOVi(Dataset):\n    def __init__(\n        self,\n        data_dir: str = \"data/MOVi\",\n        img_size: int = 128,\n        transform: transforms.Compose = None,\n        train: bool = True,\n    ):\n        super().__init__()\n\n        self.img_size = img_size\n        self.train = train\n        self.stage = \"train\" if train else \"val\"\n\n        self.max_num_objs = 6\n        self.max_num_masks = self.max_num_objs + 1\n\n        self.image_dir = os.path.join(data_dir, \"images\", self.stage)\n        self.mask_dir = os.path.join(data_dir, \"masks\", self.stage)\n        self.scene_dir = os.path.join(data_dir, \"scenes\")\n\n        self.files = sorted(os.listdir(self.image_dir))\n        self.num_files = len(self.files)\n\n        self.transform = transform\n\n    def __getitem__(self, index):\n        image_filename = self.files[index]\n        img = (\n            read_image(os.path.join(self.image_dir, image_filename), ImageReadMode.RGB)\n            .float()\n            .div(255.0)\n        )\n        img = self.transform(img)\n        sample = {\"image\": img}\n\n        scene_name = image_filename[:-3] + \"json\"\n        metadata = json.load(open(os.path.join(self.scene_dir, self.stage, scene_name)))\n\n        if not self.train:\n            masks = read_image(os.path.join(self.mask_dir, image_filename)).long().squeeze(0)\n            masks = F.one_hot(masks, self.max_num_masks).permute(2, 0, 1)\n            masks = self.transform(masks).unsqueeze(-1)\n            # masks: (max_num_masks, H, W, 1)\n\n            sample[\"masks\"] = masks.float()\n            sample[\"num_objects\"] = len(metadata[\"instances\"])\n\n        return sample\n\n    def __len__(self):\n        return self.num_files", ""]}
{"filename": "src/models/iodine_module.py", "chunked_list": ["\"\"\"\nsource: https://github.com/karazijal/clevrtex/blob/master/experiments/iodine.py\n\"\"\"\n\nfrom typing import Any, List\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport wandb", "import torchvision\nimport wandb\nfrom omegaconf import DictConfig\nfrom pytorch_lightning import LightningModule\nfrom torchmetrics import MaxMetric, MeanMetric\n\nfrom utils.evaluator import ARIEvaluator, mIoUEvaluator\nfrom src.utils.vis_utils import visualize\n\n\nclass LitIODINE(LightningModule):\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: DictConfig,  # torch.optim.lr_scheduler,\n        name: str = \"iodine\",\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\n        self.net = net\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_fg_ari = ARIEvaluator()\n        self.train_ari = ARIEvaluator()\n        self.train_miou = mIoUEvaluator()\n\n        self.val_mse = MeanMetric()\n        self.val_fg_ari = ARIEvaluator()\n        self.val_ari = ARIEvaluator()\n        self.val_miou = mIoUEvaluator()\n\n        self.val_fg_ari_best = MaxMetric()\n        self.val_ari_best = MaxMetric()\n        self.val_miou_best = MaxMetric()\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.train_kl = MeanMetric()\n        self.train_recon_loss = MeanMetric()\n\n    def forward(self, x: torch.Tensor):\n        outputs = self.net(x)\n        return outputs\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n        self.val_fg_ari_best.reset()\n        self.val_ari_best.reset()\n        self.val_miou_best.reset()\n\n    def model_step(self, batch: Any):\n        img = batch[\"image\"]\n        outputs = self.net(img)\n        loss = outputs[\"loss\"]\n        return loss, outputs\n\n    def training_step(self, batch, batch_idx):\n        loss, outputs = self.model_step(batch)\n\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_kl(outputs[\"kl\"])\n        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_recon_loss(outputs[\"recon_loss\"])\n        self.log(\n            \"train/recon_loss\", self.train_recon_loss, on_step=False, on_epoch=True, prog_bar=True\n        )\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx, dataloader_idx=None):\n        _, outputs = self.model_step(batch)\n\n        # Need to edit\n        self.val_mse(\n            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n        )\n        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n\n        masks = batch[\"masks\"].squeeze(-1)\n        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\n        self.val_miou.evaluate(pred_masks, masks)\n        self.val_ari.evaluate(pred_masks, masks)\n        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\n        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"patch\"])\n        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n        B, K, H, W, _ = pred_masks.shape\n\n        if batch_idx == 0:\n            n_sampels = 4\n            wandb_img_list = list()\n            for vis_idx in range(n_sampels):\n                vis = visualize(\n                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n                    recons=recons[vis_idx].unsqueeze(0),\n                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n                    attns=torch.zeros(\n                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n                    ),  # dummy attns\n                    colored_box=True,\n                )\n                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n                wandb_img_list.append(wandb_img)\n            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\n        return None\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        val_fg_ari = self.val_fg_ari.get_results()\n        self.val_fg_ari.reset()\n\n        val_ari = self.val_ari.get_results()\n        self.val_ari.reset()\n\n        val_miou = self.val_miou.get_results()\n        self.val_miou.reset()\n\n        self.val_fg_ari_best(val_fg_ari)\n        self.val_ari_best(val_ari)\n        self.val_miou_best(val_miou)\n\n        self.log_dict(\n            {\n                \"val/fg-ari\": val_fg_ari,\n                \"val/ari\": val_ari,\n                \"val/miou\": val_miou,\n                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n                \"val/ari_best\": self.val_ari_best.compute(),\n                \"val/miou_best\": self.val_miou_best.compute(),\n            },\n            prog_bar=True,\n        )\n\n    def test_step(self, batch: Any, batch_idx: int):\n        self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, outputs: List[Any]):\n        self.validation_epoch_end(outputs)\n\n    def configure_optimizers(self):\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler.scheduler(\n                optimizer=optimizer,\n            )\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "\n\nclass LitIODINE(LightningModule):\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: DictConfig,  # torch.optim.lr_scheduler,\n        name: str = \"iodine\",\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\n        self.net = net\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_fg_ari = ARIEvaluator()\n        self.train_ari = ARIEvaluator()\n        self.train_miou = mIoUEvaluator()\n\n        self.val_mse = MeanMetric()\n        self.val_fg_ari = ARIEvaluator()\n        self.val_ari = ARIEvaluator()\n        self.val_miou = mIoUEvaluator()\n\n        self.val_fg_ari_best = MaxMetric()\n        self.val_ari_best = MaxMetric()\n        self.val_miou_best = MaxMetric()\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.train_kl = MeanMetric()\n        self.train_recon_loss = MeanMetric()\n\n    def forward(self, x: torch.Tensor):\n        outputs = self.net(x)\n        return outputs\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n        self.val_fg_ari_best.reset()\n        self.val_ari_best.reset()\n        self.val_miou_best.reset()\n\n    def model_step(self, batch: Any):\n        img = batch[\"image\"]\n        outputs = self.net(img)\n        loss = outputs[\"loss\"]\n        return loss, outputs\n\n    def training_step(self, batch, batch_idx):\n        loss, outputs = self.model_step(batch)\n\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_kl(outputs[\"kl\"])\n        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_recon_loss(outputs[\"recon_loss\"])\n        self.log(\n            \"train/recon_loss\", self.train_recon_loss, on_step=False, on_epoch=True, prog_bar=True\n        )\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx, dataloader_idx=None):\n        _, outputs = self.model_step(batch)\n\n        # Need to edit\n        self.val_mse(\n            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n        )\n        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n\n        masks = batch[\"masks\"].squeeze(-1)\n        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\n        self.val_miou.evaluate(pred_masks, masks)\n        self.val_ari.evaluate(pred_masks, masks)\n        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\n        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"patch\"])\n        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n        B, K, H, W, _ = pred_masks.shape\n\n        if batch_idx == 0:\n            n_sampels = 4\n            wandb_img_list = list()\n            for vis_idx in range(n_sampels):\n                vis = visualize(\n                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n                    recons=recons[vis_idx].unsqueeze(0),\n                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n                    attns=torch.zeros(\n                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n                    ),  # dummy attns\n                    colored_box=True,\n                )\n                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n                wandb_img_list.append(wandb_img)\n            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\n        return None\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        val_fg_ari = self.val_fg_ari.get_results()\n        self.val_fg_ari.reset()\n\n        val_ari = self.val_ari.get_results()\n        self.val_ari.reset()\n\n        val_miou = self.val_miou.get_results()\n        self.val_miou.reset()\n\n        self.val_fg_ari_best(val_fg_ari)\n        self.val_ari_best(val_ari)\n        self.val_miou_best(val_miou)\n\n        self.log_dict(\n            {\n                \"val/fg-ari\": val_fg_ari,\n                \"val/ari\": val_ari,\n                \"val/miou\": val_miou,\n                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n                \"val/ari_best\": self.val_ari_best.compute(),\n                \"val/miou_best\": self.val_miou_best.compute(),\n            },\n            prog_bar=True,\n        )\n\n    def test_step(self, batch: Any, batch_idx: int):\n        self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, outputs: List[Any]):\n        self.validation_epoch_end(outputs)\n\n    def configure_optimizers(self):\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler.scheduler(\n                optimizer=optimizer,\n            )\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "\n\nif __name__ == \"__main__\":\n    _ = LitIODINE(None, None, None)\n"]}
{"filename": "src/models/monet_module.py", "chunked_list": ["\"\"\"https://github.com/karazijal/clevrtex/blob/master/experiments/monet.py.\"\"\"\n\nfrom typing import Any, List\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport wandb\nfrom omegaconf import DictConfig\nfrom pytorch_lightning import LightningModule", "from omegaconf import DictConfig\nfrom pytorch_lightning import LightningModule\nfrom torchmetrics import MaxMetric, MeanMetric\n\nfrom utils.evaluator import ARIEvaluator, mIoUEvaluator\nfrom src.utils.vis_utils import visualize\n\n\nclass LitMONet(LightningModule):\n    \"\"\"LightningModule for SlotAttentionAutoEncoder.\n\n    A LightningModule organizes your PyTorch code into 6 sections:\n        - Computations (init)\n        - Train loop (training_step)\n        - Validation loop (validation_step)\n        - Prediction Loop (predict_step)\n        - Optimizers and LR Schedulers (configure_optimizers)\n\n    Docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n    \"\"\"\n\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: DictConfig,  # torch.optim.lr_scheduler,\n        name: str = \"monet\",\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\n        self.net = net\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_fg_ari = ARIEvaluator()\n        self.train_ari = ARIEvaluator()\n        self.train_miou = mIoUEvaluator()\n\n        self.val_mse = MeanMetric()\n        self.val_fg_ari = ARIEvaluator()\n        self.val_ari = ARIEvaluator()\n        self.val_miou = mIoUEvaluator()\n\n        self.val_fg_ari_best = MaxMetric()\n        self.val_ari_best = MaxMetric()\n        self.val_miou_best = MaxMetric()\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.train_kl = MeanMetric()\n        self.train_kl_mask = MeanMetric()\n        self.train_loss_comp_ratio = MeanMetric()\n\n        self.val_mse = MeanMetric()\n\n    def forward(self, x: torch.Tensor):\n        outputs = self.net(x, train=False)\n        return outputs\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n        self.val_fg_ari_best.reset()\n        self.val_ari_best.reset()\n        self.val_miou_best.reset()\n\n    def model_step(self, batch: Any, train: bool = False):\n        img = batch[\"image\"]\n        outputs = self.net(img, train)\n        if train:\n            loss = outputs[\"loss\"]\n        else:\n            loss = None\n        return loss, outputs\n\n    def training_step(self, batch, batch_idx):\n        loss, outputs = self.model_step(batch, train=True)\n\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_kl(outputs[\"kl\"])\n        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_kl_mask(outputs[\"kl_mask\"])\n        self.log(\"train/kl_mask\", self.train_kl_mask, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_loss_comp_ratio(outputs[\"rec_loss\"] / outputs[\"kl\"])\n        self.log(\n            \"train/loss_comp_ratio\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True\n        )\n\n        return {\"loss\": loss}\n\n    def training_epoch_end(self, outputs: List[Any]):\n        # `outputs` is a list of dicts returned from `training_step()`\n        pass\n\n    def validation_step(self, batch: Any, batch_idx: int):\n        _, outputs = self.model_step(batch, train=False)\n\n        self.val_mse(\n            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n        )\n        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n\n        masks = batch[\"masks\"].squeeze(-1)\n        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\n        self.val_miou.evaluate(pred_masks, masks)\n        self.val_ari.evaluate(pred_masks, masks)\n        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\n        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"recons\"])\n        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n        B, K, H, W, _ = pred_masks.shape\n\n        if batch_idx == 0:\n            n_sampels = 4\n            wandb_img_list = list()\n            for vis_idx in range(n_sampels):\n                vis = visualize(\n                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n                    recons=recons[vis_idx].unsqueeze(0),\n                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n                    attns=torch.zeros(\n                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n                    ),  # dummy attns\n                    colored_box=True,\n                )\n                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n                wandb_img_list.append(wandb_img)\n            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\n        return None\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        val_fg_ari = self.val_fg_ari.get_results()\n        self.val_fg_ari.reset()\n\n        val_ari = self.val_ari.get_results()\n        self.val_ari.reset()\n\n        val_miou = self.val_miou.get_results()\n        self.val_miou.reset()\n\n        self.val_fg_ari_best(val_fg_ari)\n        self.val_ari_best(val_ari)\n        self.val_miou_best(val_miou)\n\n        self.log_dict(\n            {\n                \"val/fg-ari\": val_fg_ari,\n                \"val/ari\": val_ari,\n                \"val/miou\": val_miou,\n                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n                \"val/ari_best\": self.val_ari_best.compute(),\n                \"val/miou_best\": self.val_miou_best.compute(),\n            },\n            prog_bar=True,\n        )\n    \n    def test_step(self, batch: Any, batch_idx: int):\n        self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, outputs: List[Any]):\n        self.validation_epoch_end(outputs)\n\n    def configure_optimizers(self):\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler.scheduler(\n                optimizer=optimizer,\n            )\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "class LitMONet(LightningModule):\n    \"\"\"LightningModule for SlotAttentionAutoEncoder.\n\n    A LightningModule organizes your PyTorch code into 6 sections:\n        - Computations (init)\n        - Train loop (training_step)\n        - Validation loop (validation_step)\n        - Prediction Loop (predict_step)\n        - Optimizers and LR Schedulers (configure_optimizers)\n\n    Docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n    \"\"\"\n\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: DictConfig,  # torch.optim.lr_scheduler,\n        name: str = \"monet\",\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\n        self.net = net\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_fg_ari = ARIEvaluator()\n        self.train_ari = ARIEvaluator()\n        self.train_miou = mIoUEvaluator()\n\n        self.val_mse = MeanMetric()\n        self.val_fg_ari = ARIEvaluator()\n        self.val_ari = ARIEvaluator()\n        self.val_miou = mIoUEvaluator()\n\n        self.val_fg_ari_best = MaxMetric()\n        self.val_ari_best = MaxMetric()\n        self.val_miou_best = MaxMetric()\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.train_kl = MeanMetric()\n        self.train_kl_mask = MeanMetric()\n        self.train_loss_comp_ratio = MeanMetric()\n\n        self.val_mse = MeanMetric()\n\n    def forward(self, x: torch.Tensor):\n        outputs = self.net(x, train=False)\n        return outputs\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n        self.val_fg_ari_best.reset()\n        self.val_ari_best.reset()\n        self.val_miou_best.reset()\n\n    def model_step(self, batch: Any, train: bool = False):\n        img = batch[\"image\"]\n        outputs = self.net(img, train)\n        if train:\n            loss = outputs[\"loss\"]\n        else:\n            loss = None\n        return loss, outputs\n\n    def training_step(self, batch, batch_idx):\n        loss, outputs = self.model_step(batch, train=True)\n\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_kl(outputs[\"kl\"])\n        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_kl_mask(outputs[\"kl_mask\"])\n        self.log(\"train/kl_mask\", self.train_kl_mask, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_loss_comp_ratio(outputs[\"rec_loss\"] / outputs[\"kl\"])\n        self.log(\n            \"train/loss_comp_ratio\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True\n        )\n\n        return {\"loss\": loss}\n\n    def training_epoch_end(self, outputs: List[Any]):\n        # `outputs` is a list of dicts returned from `training_step()`\n        pass\n\n    def validation_step(self, batch: Any, batch_idx: int):\n        _, outputs = self.model_step(batch, train=False)\n\n        self.val_mse(\n            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n        )\n        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n\n        masks = batch[\"masks\"].squeeze(-1)\n        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\n        self.val_miou.evaluate(pred_masks, masks)\n        self.val_ari.evaluate(pred_masks, masks)\n        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\n        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"recons\"])\n        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n        B, K, H, W, _ = pred_masks.shape\n\n        if batch_idx == 0:\n            n_sampels = 4\n            wandb_img_list = list()\n            for vis_idx in range(n_sampels):\n                vis = visualize(\n                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n                    recons=recons[vis_idx].unsqueeze(0),\n                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n                    attns=torch.zeros(\n                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n                    ),  # dummy attns\n                    colored_box=True,\n                )\n                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n                wandb_img_list.append(wandb_img)\n            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\n        return None\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        val_fg_ari = self.val_fg_ari.get_results()\n        self.val_fg_ari.reset()\n\n        val_ari = self.val_ari.get_results()\n        self.val_ari.reset()\n\n        val_miou = self.val_miou.get_results()\n        self.val_miou.reset()\n\n        self.val_fg_ari_best(val_fg_ari)\n        self.val_ari_best(val_ari)\n        self.val_miou_best(val_miou)\n\n        self.log_dict(\n            {\n                \"val/fg-ari\": val_fg_ari,\n                \"val/ari\": val_ari,\n                \"val/miou\": val_miou,\n                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n                \"val/ari_best\": self.val_ari_best.compute(),\n                \"val/miou_best\": self.val_miou_best.compute(),\n            },\n            prog_bar=True,\n        )\n    \n    def test_step(self, batch: Any, batch_idx: int):\n        self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, outputs: List[Any]):\n        self.validation_epoch_end(outputs)\n\n    def configure_optimizers(self):\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler.scheduler(\n                optimizer=optimizer,\n            )\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "\n\nif __name__ == \"__main__\":\n    _ = LitMONet(None, None, None)\n"]}
{"filename": "src/models/genesis2_module.py", "chunked_list": ["\"\"\"\nsource: https://github.com/karazijal/clevrtex/blob/master/experiments/genesisv2.py\n\"\"\"\n\nfrom typing import Any, List\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport wandb", "import torchvision\nimport wandb\nfrom omegaconf import DictConfig\nfrom pytorch_lightning import LightningModule\nfrom torchmetrics import MaxMetric, MeanMetric\n\nfrom utils.evaluator import ARIEvaluator, mIoUEvaluator\nfrom src.utils.vis_utils import visualize\n\n\nclass LitGenesis2(LightningModule):\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: torch.optim.lr_scheduler,\n        name: str = \"gen2\",\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\n        self.net = net\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_fg_ari = ARIEvaluator()\n        self.train_ari = ARIEvaluator()\n        self.train_miou = mIoUEvaluator()\n\n        self.val_mse = MeanMetric()\n        self.val_fg_ari = ARIEvaluator()\n        self.val_ari = ARIEvaluator()\n        self.val_miou = mIoUEvaluator()\n\n        self.val_fg_ari_best = MaxMetric()\n        self.val_ari_best = MaxMetric()\n        self.val_miou_best = MaxMetric()\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.train_elbo = MeanMetric()\n        self.train_kl = MeanMetric()\n        self.train_loss_comp_ratio = MeanMetric()\n        self.train_recon_loss = MeanMetric()\n\n    def forward(self, x: torch.Tensor):\n        outputs = self.net(x)\n        return outputs\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n        self.val_fg_ari_best.reset()\n        self.val_ari_best.reset()\n        self.val_miou_best.reset()\n\n    def model_step(self, batch: Any):\n        img = batch[\"image\"]\n        outputs = self.net(img)\n        loss = outputs[\"loss\"]\n        return loss, outputs\n\n    def training_step(self, batch, batch_idx):\n        loss, outputs = self.model_step(batch)\n\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_elbo(outputs[\"elbo\"])\n        self.log(\"train/elbo\", self.train_elbo, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_kl(outputs[\"kl\"])\n        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_loss_comp_ratio(outputs[\"rec_loss\"] / outputs[\"kl\"])\n        self.log(\n            \"train/loss_comp_ratio\",\n            self.train_loss_comp_ratio,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n\n        self.train_recon_loss(outputs[\"rec_loss\"])\n        self.log(\n            \"train/recon_loss\", self.train_recon_loss, on_step=False, on_epoch=True, prog_bar=True\n        )\n\n        return {\"loss\": loss}\n\n    def training_epoch_end(self, outputs: List[Any]):\n        # `outputs` is a list of dicts returned from `training_step()`\n        pass\n\n    def validation_step(self, batch, batch_idx, dataloader_idx=None):\n        _, outputs = self.model_step(batch)\n\n        self.val_mse(\n            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n        )\n        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n\n        masks = batch[\"masks\"].squeeze(-1)\n        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\n        self.val_miou.evaluate(pred_masks, masks)\n        self.val_ari.evaluate(pred_masks, masks)\n        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\n        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"patch\"])\n        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n        B, K, H, W, _ = pred_masks.shape\n\n        if batch_idx == 0:\n            n_sampels = 4\n            wandb_img_list = list()\n            for vis_idx in range(n_sampels):\n                vis = visualize(\n                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n                    recons=recons[vis_idx].unsqueeze(0),\n                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n                    attns=torch.zeros(\n                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n                    ),  # dummy attns\n                    colored_box=True,\n                )\n                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n                wandb_img_list.append(wandb_img)\n            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\n        return None\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        val_fg_ari = self.val_fg_ari.get_results()\n        self.val_fg_ari.reset()\n\n        val_ari = self.val_ari.get_results()\n        self.val_ari.reset()\n\n        val_miou = self.val_miou.get_results()\n        self.val_miou.reset()\n\n        self.val_fg_ari_best(val_fg_ari)\n        self.val_ari_best(val_ari)\n        self.val_miou_best(val_miou)\n\n        self.log_dict(\n            {\n                \"val/fg-ari\": val_fg_ari,\n                \"val/ari\": val_ari,\n                \"val/miou\": val_miou,\n                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n                \"val/ari_best\": self.val_ari_best.compute(),\n                \"val/miou_best\": self.val_miou_best.compute(),\n            },\n            prog_bar=True,\n        )\n\n    def test_step(self, batch: Any, batch_idx: int):\n        self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, outputs: List[Any]):\n        self.validation_epoch_end(outputs)\n\n    def configure_optimizers(self):\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler(\n                optimizer=optimizer,\n            )\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "\n\nclass LitGenesis2(LightningModule):\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: torch.optim.lr_scheduler,\n        name: str = \"gen2\",\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\n        self.net = net\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_fg_ari = ARIEvaluator()\n        self.train_ari = ARIEvaluator()\n        self.train_miou = mIoUEvaluator()\n\n        self.val_mse = MeanMetric()\n        self.val_fg_ari = ARIEvaluator()\n        self.val_ari = ARIEvaluator()\n        self.val_miou = mIoUEvaluator()\n\n        self.val_fg_ari_best = MaxMetric()\n        self.val_ari_best = MaxMetric()\n        self.val_miou_best = MaxMetric()\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.train_elbo = MeanMetric()\n        self.train_kl = MeanMetric()\n        self.train_loss_comp_ratio = MeanMetric()\n        self.train_recon_loss = MeanMetric()\n\n    def forward(self, x: torch.Tensor):\n        outputs = self.net(x)\n        return outputs\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n        self.val_fg_ari_best.reset()\n        self.val_ari_best.reset()\n        self.val_miou_best.reset()\n\n    def model_step(self, batch: Any):\n        img = batch[\"image\"]\n        outputs = self.net(img)\n        loss = outputs[\"loss\"]\n        return loss, outputs\n\n    def training_step(self, batch, batch_idx):\n        loss, outputs = self.model_step(batch)\n\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_elbo(outputs[\"elbo\"])\n        self.log(\"train/elbo\", self.train_elbo, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_kl(outputs[\"kl\"])\n        self.log(\"train/kl\", self.train_kl, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.train_loss_comp_ratio(outputs[\"rec_loss\"] / outputs[\"kl\"])\n        self.log(\n            \"train/loss_comp_ratio\",\n            self.train_loss_comp_ratio,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n\n        self.train_recon_loss(outputs[\"rec_loss\"])\n        self.log(\n            \"train/recon_loss\", self.train_recon_loss, on_step=False, on_epoch=True, prog_bar=True\n        )\n\n        return {\"loss\": loss}\n\n    def training_epoch_end(self, outputs: List[Any]):\n        # `outputs` is a list of dicts returned from `training_step()`\n        pass\n\n    def validation_step(self, batch, batch_idx, dataloader_idx=None):\n        _, outputs = self.model_step(batch)\n\n        self.val_mse(\n            F.mse_loss(outputs[\"canvas\"], batch[\"image\"], reduction=\"none\").sum((1, 2, 3))\n        )\n        self.log(\"val/mse\", self.val_mse, on_step=False, on_epoch=True, prog_bar=True)\n\n        masks = batch[\"masks\"].squeeze(-1)\n        pred_masks = outputs[\"layers\"][\"mask\"].squeeze(2)\n\n        self.val_miou.evaluate(pred_masks, masks)\n        self.val_ari.evaluate(pred_masks, masks)\n        self.val_fg_ari.evaluate(pred_masks, masks[:, 1:])\n\n        recons = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"patch\"])\n        pred_masks = torch.einsum(\"bkchw->bkhwc\", outputs[\"layers\"][\"mask\"])\n        B, K, H, W, _ = pred_masks.shape\n\n        if batch_idx == 0:\n            n_sampels = 4\n            wandb_img_list = list()\n            for vis_idx in range(n_sampels):\n                vis = visualize(\n                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n                    recon_combined=outputs[\"canvas\"][vis_idx].unsqueeze(0),\n                    recons=recons[vis_idx].unsqueeze(0),\n                    pred_masks=pred_masks[vis_idx].unsqueeze(0),\n                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n                    attns=torch.zeros(\n                        (1, 1, 1, H * W, K), dtype=torch.float32, device=pred_masks.device\n                    ),  # dummy attns\n                    colored_box=True,\n                )\n                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n                wandb_img_list.append(wandb_img)\n            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\n        return None\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        val_fg_ari = self.val_fg_ari.get_results()\n        self.val_fg_ari.reset()\n\n        val_ari = self.val_ari.get_results()\n        self.val_ari.reset()\n\n        val_miou = self.val_miou.get_results()\n        self.val_miou.reset()\n\n        self.val_fg_ari_best(val_fg_ari)\n        self.val_ari_best(val_ari)\n        self.val_miou_best(val_miou)\n\n        self.log_dict(\n            {\n                \"val/fg-ari\": val_fg_ari,\n                \"val/ari\": val_ari,\n                \"val/miou\": val_miou,\n                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n                \"val/ari_best\": self.val_ari_best.compute(),\n                \"val/miou_best\": self.val_miou_best.compute(),\n            },\n            prog_bar=True,\n        )\n\n    def test_step(self, batch: Any, batch_idx: int):\n        self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, outputs: List[Any]):\n        self.validation_epoch_end(outputs)\n\n    def configure_optimizers(self):\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler(\n                optimizer=optimizer,\n            )\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "\n\nif __name__ == \"__main__\":\n    _ = LitGenesis2(None, None, None)\n"]}
{"filename": "src/models/__init__.py", "chunked_list": [""]}
{"filename": "src/models/slota_ae_module.py", "chunked_list": ["from typing import Any, List\n\nimport torch\nimport torchvision\nimport wandb\nfrom omegaconf import DictConfig\nfrom pytorch_lightning import LightningModule\nfrom torchmetrics import MaxMetric, MeanMetric\n\nfrom utils.evaluator import ARIEvaluator, mIoUEvaluator", "\nfrom utils.evaluator import ARIEvaluator, mIoUEvaluator\nfrom src.utils.vis_utils import visualize\n\n# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n\n\nclass LitSlotAttentionAutoEncoder(LightningModule):\n    \"\"\"LightningModule for SlotAttentionAutoEncoder.\n\n    A LightningModule organizes your PyTorch code into 6 sections:\n        - Computations (init)\n        - Train loop (training_step)\n        - Validation loop (validation_step)\n        - Prediction Loop (predict_step)\n        - Optimizers and LR Schedulers (configure_optimizers)\n\n    Docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n    \"\"\"\n\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: DictConfig,  # torch.optim.lr_scheduler,\n        name: str = \"slota\",\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False, ignore=[\"net\"])\n\n        self.net = net\n\n        # loss function\n        self.criterion = torch.nn.MSELoss()\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_fg_ari = ARIEvaluator()\n        self.val_fg_ari = ARIEvaluator()\n\n        self.train_ari = ARIEvaluator()\n        self.val_ari = ARIEvaluator()\n\n        self.train_miou = mIoUEvaluator()\n        self.val_miou = mIoUEvaluator()\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.val_loss = MeanMetric()\n\n        # for tracking best so far validation accuracy\n        self.val_fg_ari_best = MaxMetric()\n        self.val_ari_best = MaxMetric()\n        self.val_miou_best = MaxMetric()\n\n    def forward(self, x: torch.Tensor, train: bool = False):\n        outputs = self.net(x, train=train)\n        return outputs\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so we need to make sure val_acc_best doesn't store accuracy from these checks\n        self.val_fg_ari_best.reset()\n        self.val_ari_best.reset()\n        self.val_miou_best.reset()\n\n    def model_step(self, batch: Any, train: bool = False):\n        img = batch[\"image\"]\n        outputs = self.forward(img, train=train)\n        loss = self.criterion(outputs[\"recon_combined\"], img)\n        return loss, outputs\n\n    def training_step(self, batch: Any, batch_idx: int):\n        loss, outputs = self.model_step(batch, train=True)\n\n        # update and log metrics\n        self.train_loss(loss)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        # we can return here dict with any tensors\n        # and then read it in some callback or in `training_epoch_end()` below\n        # remember to always return loss from `training_step()` or backpropagation will fail!\n        return {\"loss\": loss}\n\n    def training_epoch_end(self, outputs: List[Any]):\n        # `outputs` is a list of dicts returned from `training_step()`\n        pass\n\n    def validation_step(self, batch: Any, batch_idx: int):\n        loss, outputs = self.model_step(batch, train=False)\n\n        # update and log metrics\n        self.val_loss(loss)\n        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        self.val_fg_ari.evaluate(outputs[\"masks\"].squeeze(-1), batch[\"masks\"][:, 1:].squeeze(-1))\n        self.val_ari.evaluate(outputs[\"masks\"].squeeze(-1), batch[\"masks\"].squeeze(-1))\n        self.val_miou.evaluate(outputs[\"masks\"].squeeze(-1), batch[\"masks\"].squeeze(-1))\n\n        if batch_idx == 0:\n            n_sampels = 4\n            wandb_img_list = list()\n            for vis_idx in range(n_sampels):\n                vis = visualize(\n                    image=batch[\"image\"][vis_idx].unsqueeze(0),\n                    recon_combined=outputs[\"recon_combined\"][vis_idx].unsqueeze(0),\n                    recons=outputs[\"recons\"][vis_idx].unsqueeze(0),\n                    pred_masks=outputs[\"masks\"][vis_idx].unsqueeze(0),\n                    gt_masks=batch[\"masks\"][vis_idx].unsqueeze(0),\n                    attns=outputs[\"attns\"][vis_idx, -1:].unsqueeze(0),\n                    colored_box=True,\n                )\n                grid = torchvision.utils.make_grid(vis, nrow=1, pad_value=0)\n                wandb_img = wandb.Image(grid, caption=f\"Epoch: {self.current_epoch+1}\")\n                wandb_img_list.append(wandb_img)\n            self.logger.log_image(key=\"Visualization on Validation Set\", images=wandb_img_list)\n\n        return {\"loss\": loss}\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        val_fg_ari = self.val_fg_ari.get_results()\n        self.val_fg_ari.reset()\n\n        val_ari = self.val_ari.get_results()\n        self.val_ari.reset()\n\n        val_miou = self.val_miou.get_results()\n        self.val_miou.reset()\n\n        self.val_fg_ari_best(val_fg_ari)\n        self.val_ari_best(val_ari)\n        self.val_miou_best(val_miou)\n\n        self.log_dict(\n            {\n                \"val/fg-ari\": val_fg_ari,\n                \"val/ari\": val_ari,\n                \"val/miou\": val_miou,\n                \"val/fg-ari_best\": self.val_fg_ari_best.compute(),\n                \"val/ari_best\": self.val_ari_best.compute(),\n                \"val/miou_best\": self.val_miou_best.compute(),\n            },\n            prog_bar=True,\n        )\n\n    def test_step(self, batch: Any, batch_idx: int):\n        self.validation_step(batch, batch_idx)\n\n    def test_epoch_end(self, outputs: List[Any]):\n        self.validation_epoch_end(outputs)\n\n    def configure_optimizers(self):\n        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n\n        Examples:\n            https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n        \"\"\"\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n\n            def lr_lambda(step):\n\n                if step < self.hparams.scheduler.warmup_steps:\n                    warmup_factor = float(step) / float(\n                        max(1.0, self.hparams.scheduler.warmup_steps)\n                    )\n                else:\n                    warmup_factor = 1.0\n\n                decay_factor = self.hparams.scheduler.decay_rate ** (\n                    step / self.hparams.scheduler.decay_steps\n                )\n\n                return warmup_factor * decay_factor\n\n            scheduler = self.hparams.scheduler.scheduler(\n                optimizer=optimizer,\n                lr_lambda=lr_lambda,\n            )\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "\n\nif __name__ == \"__main__\":\n    _ = LitSlotAttentionAutoEncoder(None, None, None)\n"]}
{"filename": "src/models/mnist_module.py", "chunked_list": ["from typing import Any, List\n\nimport torch\nfrom pytorch_lightning import LightningModule\nfrom torchmetrics import MaxMetric, MeanMetric\nfrom torchmetrics.classification.accuracy import Accuracy\n\n\nclass MNISTLitModule(LightningModule):\n    \"\"\"Example of LightningModule for MNIST classification.\n\n    A LightningModule organizes your PyTorch code into 6 sections:\n        - Computations (init)\n        - Train loop (training_step)\n        - Validation loop (validation_step)\n        - Test loop (test_step)\n        - Prediction Loop (predict_step)\n        - Optimizers and LR Schedulers (configure_optimizers)\n\n    Docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n    \"\"\"\n\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: torch.optim.lr_scheduler,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        self.net = net\n\n        # loss function\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n        self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.val_loss = MeanMetric()\n        self.test_loss = MeanMetric()\n\n        # for tracking best so far validation accuracy\n        self.val_acc_best = MaxMetric()\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so it's worth to make sure validation metrics don't store results from these checks\n        self.val_loss.reset()\n        self.val_acc.reset()\n        self.val_acc_best.reset()\n\n    def model_step(self, batch: Any):\n        x, y = batch\n        logits = self.forward(x)\n        loss = self.criterion(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        return loss, preds, y\n\n    def training_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.train_loss(loss)\n        self.train_acc(preds, targets)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        # we can return here dict with any tensors\n        # and then read it in some callback or in `training_epoch_end()` below\n        # remember to always return loss from `training_step()` or backpropagation will fail!\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\n    def training_epoch_end(self, outputs: List[Any]):\n        # `outputs` is a list of dicts returned from `training_step()`\n\n        # Warning: when overriding `training_epoch_end()`, lightning accumulates outputs from all batches of the epoch\n        # this may not be an issue when training on mnist\n        # but on larger datasets/models it's easy to run into out-of-memory errors\n\n        # consider detaching tensors before returning them from `training_step()`\n        # or using `on_train_epoch_end()` instead which doesn't accumulate outputs\n\n        pass\n\n    def validation_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.val_loss(loss)\n        self.val_acc(preds, targets)\n        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        acc = self.val_acc.compute()  # get current val acc\n        self.val_acc_best(acc)  # update best so far val acc\n        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n        # otherwise metric would be reset by lightning after each epoch\n        self.log(\"val/acc_best\", self.val_acc_best.compute(), prog_bar=True)\n\n    def test_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.test_loss(loss)\n        self.test_acc(preds, targets)\n        self.log(\"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"test/acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\n    def test_epoch_end(self, outputs: List[Any]):\n        pass\n\n    def configure_optimizers(self):\n        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n\n        Examples:\n            https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n        \"\"\"\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler(optimizer=optimizer)\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "class MNISTLitModule(LightningModule):\n    \"\"\"Example of LightningModule for MNIST classification.\n\n    A LightningModule organizes your PyTorch code into 6 sections:\n        - Computations (init)\n        - Train loop (training_step)\n        - Validation loop (validation_step)\n        - Test loop (test_step)\n        - Prediction Loop (predict_step)\n        - Optimizers and LR Schedulers (configure_optimizers)\n\n    Docs:\n        https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n    \"\"\"\n\n    def __init__(\n        self,\n        net: torch.nn.Module,\n        optimizer: torch.optim.Optimizer,\n        scheduler: torch.optim.lr_scheduler,\n    ):\n        super().__init__()\n\n        # this line allows to access init params with 'self.hparams' attribute\n        # also ensures init params will be stored in ckpt\n        self.save_hyperparameters(logger=False)\n\n        self.net = net\n\n        # loss function\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        # metric objects for calculating and averaging accuracy across batches\n        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n        self.val_acc = Accuracy(task=\"multiclass\", num_classes=10)\n        self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n\n        # for averaging loss across batches\n        self.train_loss = MeanMetric()\n        self.val_loss = MeanMetric()\n        self.test_loss = MeanMetric()\n\n        # for tracking best so far validation accuracy\n        self.val_acc_best = MaxMetric()\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)\n\n    def on_train_start(self):\n        # by default lightning executes validation step sanity checks before training starts,\n        # so it's worth to make sure validation metrics don't store results from these checks\n        self.val_loss.reset()\n        self.val_acc.reset()\n        self.val_acc_best.reset()\n\n    def model_step(self, batch: Any):\n        x, y = batch\n        logits = self.forward(x)\n        loss = self.criterion(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        return loss, preds, y\n\n    def training_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.train_loss(loss)\n        self.train_acc(preds, targets)\n        self.log(\"train/loss\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train/acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        # we can return here dict with any tensors\n        # and then read it in some callback or in `training_epoch_end()` below\n        # remember to always return loss from `training_step()` or backpropagation will fail!\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\n    def training_epoch_end(self, outputs: List[Any]):\n        # `outputs` is a list of dicts returned from `training_step()`\n\n        # Warning: when overriding `training_epoch_end()`, lightning accumulates outputs from all batches of the epoch\n        # this may not be an issue when training on mnist\n        # but on larger datasets/models it's easy to run into out-of-memory errors\n\n        # consider detaching tensors before returning them from `training_step()`\n        # or using `on_train_epoch_end()` instead which doesn't accumulate outputs\n\n        pass\n\n    def validation_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.val_loss(loss)\n        self.val_acc(preds, targets)\n        self.log(\"val/loss\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val/acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\n    def validation_epoch_end(self, outputs: List[Any]):\n        acc = self.val_acc.compute()  # get current val acc\n        self.val_acc_best(acc)  # update best so far val acc\n        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n        # otherwise metric would be reset by lightning after each epoch\n        self.log(\"val/acc_best\", self.val_acc_best.compute(), prog_bar=True)\n\n    def test_step(self, batch: Any, batch_idx: int):\n        loss, preds, targets = self.model_step(batch)\n\n        # update and log metrics\n        self.test_loss(loss)\n        self.test_acc(preds, targets)\n        self.log(\"test/loss\", self.test_loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"test/acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True)\n\n        return {\"loss\": loss, \"preds\": preds, \"targets\": targets}\n\n    def test_epoch_end(self, outputs: List[Any]):\n        pass\n\n    def configure_optimizers(self):\n        \"\"\"Choose what optimizers and learning-rate schedulers to use in your optimization.\n        Normally you'd need one. But in the case of GANs or similar you might have multiple.\n\n        Examples:\n            https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n        \"\"\"\n        optimizer = self.hparams.optimizer(params=self.parameters())\n        if self.hparams.scheduler is not None:\n            scheduler = self.hparams.scheduler(optimizer=optimizer)\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"monitor\": \"val/loss\",\n                    \"interval\": \"epoch\",\n                    \"frequency\": 1,\n                },\n            }\n        return {\"optimizer\": optimizer}", "\n\nif __name__ == \"__main__\":\n    _ = MNISTLitModule(None, None, None)\n"]}
{"filename": "src/models/components/__init__.py", "chunked_list": [""]}
{"filename": "src/models/components/iodine.py", "chunked_list": ["\"\"\"\nsource: https://github.com/karazijal/clevrtex/blob/master/ool/picture/models/iodine.py\n\nImplementation of IODINE from\n\n\"Multi-Object Representation Learning with Iterative Variational Inference\"\nKlaus Greff, Rapha\u00ebl Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess,\nDaniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner\nhttps://arxiv.org/abs/1903.00450\n", "https://arxiv.org/abs/1903.00450\n\n\nThis (re)-implemetation is draws from re-implementations of\nhttps://github.com/zhixuan-lin/IODINE\nhttps://github.com/MichaelKevinKelly/IODINE/\nhttps://github.com/pemami4911/IODINE.pytorch/\nand is based on official code\nhttps://github.com/deepmind/deepmind-research/tree/master/iodine\n\"\"\"", "https://github.com/deepmind/deepmind-research/tree/master/iodine\n\"\"\"\n\nimport math\nfrom typing import Union\n\nimport torch\nimport torch.distributions as dist\nimport torch.nn as nn\nimport torch.nn.functional as F", "import torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy.stats import truncnorm\nfrom torch.nn import init\n\n\ndef truncated_normal_initializer(shape, mean, stddev):\n    # compute threshold at 2 std devs\n    values = truncnorm.rvs(mean - 2 * stddev, mean + 2 * stddev, size=shape)\n    return torch.from_numpy(values).float()", "\n\ndef init_weights(net, init_type=\"normal\", init_gain=0.02):\n    \"\"\"Initialize network weights.\n\n    Modified from: https://github.com/baudm/MONet-pytorch/blob/master/models/networks.py\n\n    Parameters:\n        net (network)   -- network to be initialized\n        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n\n    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n    work better for some applications. Feel free to try yourself.\n    \"\"\"\n\n    def init_func(m):  # define the initialization function\n        classname = m.__class__.__name__\n        if hasattr(m, \"weight\") and (\n            classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1\n        ):\n            if init_type == \"normal\":\n                init.normal_(m.weight.data, 0.0, init_gain)\n            elif init_type == \"xavier\":\n                init.xavier_normal_(m.weight.data, gain=init_gain)\n            elif init_type == \"kaiming\":\n                init.kaiming_normal_(m.weight.data, a=0, mode=\"fan_in\")\n            elif init_type == \"orthogonal\":\n                init.orthogonal_(m.weight.data, gain=init_gain)\n            elif init_type == \"truncated_normal\":\n                m.weight.data = truncated_normal_initializer(m.weight.shape, 0.0, stddev=init_gain)\n            else:\n                raise NotImplementedError(\n                    \"initialization method [%s] is not implemented\" % init_type\n                )\n            if hasattr(m, \"bias\") and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\"BatchNorm2d\") != -1:\n            init.normal_(m.weight.data, 1.0, init_gain)\n            init.constant_(m.bias.data, 0.0)\n\n    net.apply(init_func)", "\n\ndef _softplus_to_std(softplus):\n    softplus = torch.min(softplus, torch.ones_like(softplus) * 80)\n    return torch.sqrt(torch.log(1.0 + softplus.exp()) + 1e-5)\n\n\ndef normal(loc, pre_softplus_var):\n    return dist.independent.Independent(\n        dist.normal.Normal(loc, torch.sqrt(F.softplus(pre_softplus_var))), 1\n    )", "\n\ndef unit_normal(shape, device):\n    loc = torch.zeros(shape).to(device)\n    scale = torch.ones(shape).to(device)\n    return dist.independent.Independent(dist.normal.Normal(loc, scale), 1)\n\n\ndef gmm_loglikelihood(x, x_loc, log_var, mask_logprobs):\n    \"\"\"\n    mask_logprobs: [N, K, 1, H, W]\n    \"\"\"\n    # NLL [batch_size, 1, H, W]\n    sq_err = (x.unsqueeze(1) - x_loc).pow(2)\n    # log N(x; x_loc, log_var): [N, K, C, H, W]\n    normal_ll = -0.5 * log_var - 0.5 * (sq_err / torch.exp(log_var))\n    # [N, K, C, H, W]\n    log_p_k = mask_logprobs + normal_ll\n    # logsumexp over slots [N, C, H, W]\n    log_p = torch.logsumexp(log_p_k, dim=1).sum(1, keepdim=True)\n    # [batch_size]\n    nll = -torch.sum(log_p, dim=[1, 2, 3])\n\n    return nll, {\"log_p_k\": log_p_k, \"normal_ll\": normal_ll, \"log_p\": log_p}", "def gmm_loglikelihood(x, x_loc, log_var, mask_logprobs):\n    \"\"\"\n    mask_logprobs: [N, K, 1, H, W]\n    \"\"\"\n    # NLL [batch_size, 1, H, W]\n    sq_err = (x.unsqueeze(1) - x_loc).pow(2)\n    # log N(x; x_loc, log_var): [N, K, C, H, W]\n    normal_ll = -0.5 * log_var - 0.5 * (sq_err / torch.exp(log_var))\n    # [N, K, C, H, W]\n    log_p_k = mask_logprobs + normal_ll\n    # logsumexp over slots [N, C, H, W]\n    log_p = torch.logsumexp(log_p_k, dim=1).sum(1, keepdim=True)\n    # [batch_size]\n    nll = -torch.sum(log_p, dim=[1, 2, 3])\n\n    return nll, {\"log_p_k\": log_p_k, \"normal_ll\": normal_ll, \"log_p\": log_p}", "\n\ndef gaussian_loglikelihood(x_t, x_loc, log_var):\n    sq_err = (x_t - x_loc).pow(2)  # [N,C,H,W]\n    # log N(x; x_loc, log_var): [N,C, H, W]\n    normal_ll = -0.5 * log_var - 0.5 * (sq_err / torch.exp(log_var))\n    nll = -torch.sum(normal_ll, dim=[1, 2, 3])  # [N]\n    return nll\n\n\ndef rename_state_dict(state_dict, old_strings, new_strings):\n    new_state_dict = {}\n    for old_string, new_string in zip(old_strings, new_strings):\n        for k, v in state_dict.items():\n            if old_string in k:\n                new_key = k.replace(old_string, new_string)\n                new_state_dict[new_key] = v\n    for k, v in state_dict.items():\n        for old_string in old_strings:\n            if old_string in k:\n                break\n        else:\n            new_state_dict[k] = v\n    return new_state_dict", "\n\ndef rename_state_dict(state_dict, old_strings, new_strings):\n    new_state_dict = {}\n    for old_string, new_string in zip(old_strings, new_strings):\n        for k, v in state_dict.items():\n            if old_string in k:\n                new_key = k.replace(old_string, new_string)\n                new_state_dict[new_key] = v\n    for k, v in state_dict.items():\n        for old_string in old_strings:\n            if old_string in k:\n                break\n        else:\n            new_state_dict[k] = v\n    return new_state_dict", "\n\nclass RefinementNetwork(nn.Module):\n    def __init__(\n        self,\n        input_shape,\n        z_size=64,\n        refinenet_channels_in=17,  # should be 17!!!\n        conv_channels=64,\n        lstm_dim=256,\n        k=3,\n        stride=2,\n    ):\n        super().__init__()\n        self.input_shape = input_shape\n        self.z_size = z_size\n        self.conv = nn.Sequential(\n            nn.Conv2d(refinenet_channels_in, conv_channels, k, stride, k // 2),\n            nn.ELU(True),\n            nn.Conv2d(conv_channels, conv_channels, k, stride, k // 2),\n            nn.ELU(True),\n            nn.Conv2d(conv_channels, conv_channels, k, stride, k // 2),\n            nn.ELU(True),\n            nn.Conv2d(conv_channels, conv_channels, k, stride, k // 2),\n            nn.ELU(True),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(conv_channels, lstm_dim),\n            nn.ELU(True),\n            # nn.Linear(lstm_dim, lstm_dim),  # Papers says only 1; Official repo has 2\n            # nn.ELU(True)\n        )\n\n        # self.input_proj = nn.Sequential(\n        #     nn.Linear(lstm_dim + 4 * self.z_size, lstm_dim),\n        #     nn.ELU(True)\n        # )\n\n        # self.lstm = nn.LSTM(lstm_dim, lstm_dim)\n        self.lstm = nn.LSTM(lstm_dim + 4 * self.z_size, lstm_dim)\n        # self.loc = nn.Linear(lstm_dim, z_size)\n        # self.softplus = nn.Linear(lstm_dim, z_size)\n        self.ref_head = nn.Linear(lstm_dim, 2 * z_size)\n\n    def forward(self, img_inputs, vec_inputs, h, c):\n        \"\"\"\n        img_inputs: [N * K, C, H, W]\n        vec_inputs: [N * K, 4*z_size]\n        \"\"\"\n        x = self.conv(img_inputs)\n        # concat with \\lambda and \\nabla \\lambda\n        x = torch.cat([x, vec_inputs], 1)\n        # x = self.input_proj(x)\n        x = x.unsqueeze(0)  # seq dim\n        self.lstm.flatten_parameters()\n        out, (h, c) = self.lstm(x, (h, c))\n        out = out.squeeze(0)\n        # loc = self.loc(out)\n        # softplus = self.softplus(out)\n        # lamda = torch.cat([loc, softplus], 1)\n        lamda = self.ref_head(out)\n        return lamda, (h, c)", "\n\nclass SpatialBroadcastDecoder(nn.Module):\n    \"\"\"Decodes the individual Gaussian image components into RGB and mask.\n\n    This is the architecture used for the Multi-dSprites experiment but I haven't seen any issues\n    with re-using it for CLEVR. In their paper they slightly modify it (e.g., uses 3x3 conv instead\n    of 5x5).\n    \"\"\"\n\n    def __init__(self, input_shape, z_size=64, conv_channels=64, k=3):\n        super().__init__()\n        self.h, self.w = input_shape[1], input_shape[2]\n        self.decode = nn.Sequential(\n            nn.Conv2d(z_size + 2, conv_channels, k, 1, padding=k // 2 - 1),\n            nn.ELU(True),\n            nn.Conv2d(conv_channels, conv_channels, k, 1, padding=k // 2 - 1),\n            nn.ELU(True),\n            nn.Conv2d(conv_channels, conv_channels, k, 1, padding=k // 2 - 1),\n            nn.ELU(True),\n            nn.Conv2d(conv_channels, conv_channels, k, 1, padding=k // 2 - 1),\n            nn.ELU(True),\n            nn.Conv2d(conv_channels, 4, 1, 1),\n        )\n\n    @staticmethod\n    def spatial_broadcast(z, h, w):\n        \"\"\"\n        source: https://github.com/baudm/MONet-pytorch/blob/master/models/networks.py\n        \"\"\"\n        # Batch size\n        n = z.shape[0]\n        # Expand spatially: (n, z_dim) -> (n, z_dim, h, w)\n        z_b = z.view((n, -1, 1, 1)).expand(-1, -1, h, w)\n        # Coordinate axes:\n        x = torch.linspace(-1, 1, w, device=z.device)\n        y = torch.linspace(-1, 1, h, device=z.device)\n        y_b, x_b = torch.meshgrid(y, x)\n        # Expand from (h, w) -> (n, 1, h, w)\n        x_b = x_b.expand(n, 1, -1, -1)\n        y_b = y_b.expand(n, 1, -1, -1)\n        # Concatenate along the channel dimension: final shape = (n, z_dim + 2, h, w)\n        z_sb = torch.cat((z_b, x_b, y_b), dim=1)\n        return z_sb\n\n    def forward(self, z):\n        z_sb = SpatialBroadcastDecoder.spatial_broadcast(z, self.h + 8, self.w + 8)\n        out = self.decode(z_sb)  # [batch_size * K, output_size, h, w]\n        return torch.sigmoid(out[:, :3]), out[:, 3]", "\n\nclass IODINE(nn.Module):\n    def __init__(\n        self,\n        input_shape: Union[tuple, list] = (3, 128, 128),\n        z_size: int = 64,\n        K: int = 7,\n        inference_iters: int = 5,\n        std: float = 0.10,\n        kl_beta=1,\n        lstm_dim: int = 256,\n        conv_channels: int = 128,\n        refinenet_channels_in: int = 17,  # should be 17\n    ):\n        super().__init__()\n\n        self.z_size = z_size\n        self.input_shape = input_shape\n        self.K = K\n        self.inference_iters = inference_iters\n        self.log_scale = math.log(std)\n        self.kl_beta = kl_beta\n        self.lstm_dim = lstm_dim\n        self.register_buffer(\"gmm_log_scale\", (self.log_scale * torch.ones(K)).view(1, K, 1, 1, 1))\n\n        self.image_decoder = SpatialBroadcastDecoder(\n            input_shape, z_size, conv_channels, k=3\n        )  # k HERE IS filter size\n        self.refine_net = RefinementNetwork(\n            input_shape, z_size, refinenet_channels_in, conv_channels, lstm_dim\n        )\n\n        init_weights(self.image_decoder, \"xavier\")\n        init_weights(self.refine_net, \"xavier\")\n\n        # learnable initial posterior distribution\n        # loc = 0, variance = 1\n        self.lamda_0 = nn.Parameter(\n            torch.cat([torch.zeros(1, self.z_size), torch.ones(1, self.z_size)], 1)\n        )\n\n        # layernorms for iterative inference input\n        affine = True  # Paper trains these parameters\n        n = self.input_shape[1]\n        self.layer_norms = torch.nn.ModuleList(\n            [\n                nn.LayerNorm((1, n, n), elementwise_affine=affine),\n                nn.LayerNorm((1, n, n), elementwise_affine=affine),\n                nn.LayerNorm((3, n, n), elementwise_affine=affine),\n                nn.LayerNorm((1, n, n), elementwise_affine=affine),\n                nn.LayerNorm((self.z_size,), elementwise_affine=affine),  # layer_norm_mean\n                nn.LayerNorm((self.z_size,), elementwise_affine=affine),  # layer_norm_log_scale\n            ]\n        )\n\n    def refinenet_inputs(self, image, means, masks, mask_logits, log_p, normal_ll, lamda, loss):\n        N, K, C, H, W = image.shape\n        # non-gradient inputs\n        # 1. image [N, K, C, H, W]\n        # 2. means [N, K, C, H, W]\n        # 3. masks  [N, K, 1, H, W] (log probs)\n        # 4. mask logits [N, K, 1, H, W]\n        # 5. mask posterior [N, K, 1, H, W]\n\n        # print(image.shape, means.shape, masks.shape, mask_logits.shape, log_p.shape, normal_ll.shape, lamda.shape, loss.shape)\n        mask_ll = normal_ll.sum(dim=2, keepdim=True)\n        mask_posterior = mask_ll - torch.logsumexp(mask_ll, dim=1, keepdim=True)  # logscale\n        # 6. pixelwise likelihood [N, K, 1, H, W]\n        # log_p_k = torch.logsumexp(log_p_k, dim=1).sum(1)\n        log_p_k = log_p.view(-1, 1, 1, H, W).expand(-1, K, -1, -1, -1)\n        # 7. LOO likelihood\n        # loo_px_l = torch.log(1e-6 + (log_p_k.exp()+1e-6 - (masks + normal_ll.unsqueeze(2).exp())+1e-6)) # [N,K,1,H,W]\n        # since counterfactuals have stop grad:\n        with torch.no_grad():\n            counterfactuals = []\n            for i in range(K):\n                pll = torch.cat((normal_ll[:, :i], normal_ll[:, i + 1 :]), dim=1)\n                msk = torch.cat((mask_logits[:, :i], mask_logits[:, i + 1 :]), dim=1)\n                counterfactuals.append(\n                    torch.logsumexp(pll + torch.log_softmax(msk, dim=1), dim=1).sum(\n                        1, keepdim=True\n                    )\n                )\n            counterfactuals = torch.stack(counterfactuals, dim=1).view(N, K, 1, H, W)\n        # 8. Coordinate channel\n        x_mesh, y_mesh = torch.meshgrid(\n            torch.linspace(-1, 1, H, device=image.device),\n            torch.linspace(-1, 1, W, device=image.device),\n        )\n        # Expand from (h, w) -> (n, k, 1, h, w)\n        x_mesh = x_mesh.expand(N, K, 1, -1, -1)\n        y_mesh = y_mesh.expand(N, K, 1, -1, -1)\n\n        # 9. \\partial L / \\partial means\n        # [N, K, C, H, W]\n        # 10. \\partial L/ \\partial masks\n        # [N, K, 1, H, W]\n        # 11. \\partial L/ \\partial lamda\n        # [N*K, 2 * self.z_size]\n        d_means, d_masks, d_lamda = torch.autograd.grad(\n            loss, [means, masks, lamda], retain_graph=self.training, only_inputs=True\n        )\n\n        d_loc_z, d_sp_z = d_lamda.chunk(2, dim=1)\n        # d_loc_z, d_sp_z = d_loc_z.contiguous(), d_sp_z.contiguous()\n\n        # Stop gradients and apply LayerNorm\n        # dmeans LN + SG\n        d_means = self.layer_norms[2](d_means.detach())\n        # dmasks LN + SG\n        d_masks = self.layer_norms[3](d_masks.detach())\n        # log_p LN + SG\n        log_p_k = self.layer_norms[0](log_p_k.detach())\n        # counterfactual SG + LN\n        loo_px_l = self.layer_norms[1](counterfactuals.detach())\n        # dzp LN + SG\n        d_loc_z = self.layer_norms[4](d_loc_z.detach())\n        d_sp_z = self.layer_norms[5](d_sp_z.detach())\n\n        # concat image-size and vector inputs\n        image_inputs = torch.cat(\n            [\n                image,  # 3\n                means,  # 3\n                masks,  # 1\n                mask_logits,  # code seems to provide probs, paper says logits # 1\n                mask_posterior,  # in code not in logscale; is here 1\n                d_means,  # 3\n                d_masks,  # 1\n                log_p_k,  # 1\n                loo_px_l,  # 1\n                x_mesh,  # 1\n                y_mesh,  # 1\n            ],\n            2,\n        )\n        vec_inputs = torch.cat([lamda, d_loc_z, d_sp_z], 1)\n\n        return image_inputs.view(N * K, -1, H, W), vec_inputs\n\n    def forward(self, x):\n        \"\"\"Evaluates the model as a whole, encodes and decodes and runs inference for T steps.\"\"\"\n        torch.set_grad_enabled(True)\n        B = x.shape[0]\n        C, H, W = self.input_shape[0], self.input_shape[1], self.input_shape[2]\n\n        # expand lambda_0\n        lamda = self.lamda_0.repeat(B * self.K, 1)  # [N*K, 2*z_size]\n        p_z = unit_normal(shape=[B * self.K, self.z_size], device=x.device)\n\n        total_loss = 0.0\n        losses = []\n        x_means = []\n        masks = []\n        h = torch.zeros(1, B * self.K, self.lstm_dim).to(x.device)\n        c = torch.zeros(1, B * self.K, self.lstm_dim).to(x.device)\n\n        for i in range(self.inference_iters):\n            # sample initial posterior\n            loc_z, sp_z = lamda.chunk(2, dim=1)\n            # loc_z, sp_z = loc_z.contiguous(), sp_z.contiguous()\n            q_z = normal(loc_z, sp_z)\n            z = q_z.rsample()\n\n            # Get means and masks\n            x_loc, mask_logits = self.image_decoder(z)  # [N*K, C, H, W]\n            x_loc = x_loc.view(B, self.K, C, H, W)\n\n            # softmax across slots\n            mask_logits = mask_logits.view(B, self.K, 1, H, W)\n            mask_logprobs = nn.functional.log_softmax(mask_logits, dim=1)\n\n            # NLL [batch_size, 1, H, W]\n            # log_var = (2 * self.gmm_log_scale)\n            nll, ll_outs = gmm_loglikelihood(x, x_loc, 2 * self.gmm_log_scale, mask_logprobs)\n\n            # KL div\n            kl_div = dist.kl.kl_divergence(q_z, p_z)\n            kl_div = kl_div.view(B, self.K).sum(1)\n\n            loss = nll + self.kl_beta * kl_div\n            loss = torch.mean(loss)\n\n            scaled_loss = ((i + 1.0) / self.inference_iters) * loss\n            losses += [scaled_loss]\n            total_loss += scaled_loss\n\n            x_means += [x_loc]\n            masks += [mask_logprobs]\n\n            # Refinement\n            if i == self.inference_iters - 1:\n                # after T refinement steps, just output final loss\n                continue\n\n            # compute refine inputs\n            x_ = x.repeat(self.K, 1, 1, 1).view(B, self.K, C, H, W)\n\n            img_inps, vec_inps = self.refinenet_inputs(\n                x_,\n                x_loc,\n                mask_logprobs,\n                mask_logits,\n                ll_outs[\"log_p\"],\n                ll_outs[\"normal_ll\"],\n                lamda,\n                loss,\n            )\n\n            delta, (h, c) = self.refine_net(img_inps, vec_inps, h, c)\n            lamda = lamda + delta\n\n        return {\n            \"canvas\": (x_loc * mask_logprobs.exp()).sum(dim=1),\n            \"loss\": total_loss,\n            \"recon_loss\": torch.mean(nll),\n            \"kl\": torch.mean(kl_div),\n            \"layers\": {\"patch\": x_loc, \"mask\": mask_logprobs.exp()},\n            \"z\": z,\n        }", ""]}
{"filename": "src/models/components/simple_dense_net.py", "chunked_list": ["from torch import nn\n\n\nclass SimpleDenseNet(nn.Module):\n    def __init__(\n        self,\n        input_size: int = 784,\n        lin1_size: int = 256,\n        lin2_size: int = 256,\n        lin3_size: int = 256,\n        output_size: int = 10,\n    ):\n        super().__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(input_size, lin1_size),\n            nn.BatchNorm1d(lin1_size),\n            nn.ReLU(),\n            nn.Linear(lin1_size, lin2_size),\n            nn.BatchNorm1d(lin2_size),\n            nn.ReLU(),\n            nn.Linear(lin2_size, lin3_size),\n            nn.BatchNorm1d(lin3_size),\n            nn.ReLU(),\n            nn.Linear(lin3_size, output_size),\n        )\n\n    def forward(self, x):\n        batch_size, channels, width, height = x.size()\n\n        # (batch, 1, width, height) -> (batch, 1*width*height)\n        x = x.view(batch_size, -1)\n\n        return self.model(x)", "\n\nif __name__ == \"__main__\":\n    _ = SimpleDenseNet()\n"]}
{"filename": "src/models/components/genesis2.py", "chunked_list": ["\"\"\"\nsource: https://github.com/karazijal/clevrtex/blob/master/ool/picture/models/thirdparty/genesis2/model.py\n\nGenesis-V2\nhttps://arxiv.org/pdf/2104.09958\n\nBased on original implementation from\nhttps://github.com/martinengelcke/genesis/blob/genesis-v2/models/genesisv2_config.py\n\n", "\n\n# =========================== A2I Copyright Header ===========================\n#\n# Copyright (c) 2003-2021 University of Oxford. All rights reserved.\n# Authors: Applied AI Lab, Oxford Robotics Institute, University of Oxford\n#          https://ori.ox.ac.uk/labs/a2i/\n#\n# This file is the property of the University of Oxford.\n# Redistribution and use in source and binary forms, with or without", "# This file is the property of the University of Oxford.\n# Redistribution and use in source and binary forms, with or without\n# modification, is not permitted without an explicit licensing agreement\n# (research or commercial). No warranty, explicit or implicit, provided.\n#\n# =========================== A2I Copyright Header ===========================\n\"\"\"\nimport numpy as np\nimport torch\nimport torch.distributions as dist", "import torch\nimport torch.distributions as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ConvGNReLU(nn.Sequential):\n    def __init__(self, nin, nout, kernel, stride=1, padding=0, groups=8):\n        super().__init__(\n            nn.Conv2d(nin, nout, kernel, stride, padding, bias=False),\n            nn.GroupNorm(groups, nout),\n            nn.ReLU(inplace=True),\n        )", "\n\nclass BroadcastLayer(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        c = torch.linspace(-1, 1, dim)\n        self.register_buffer(\"coords\", torch.stack(torch.meshgrid(c, c))[None])\n\n    def forward(self, x):\n        b_sz = x.size(0)\n        # Broadcast\n        if x.dim() == 2:\n            x = x.view(b_sz, -1, 1, 1)\n            x = x.expand(-1, -1, self.dim, self.dim)\n        else:\n            x = F.interpolate(x, self.dim)\n        return torch.cat([x, self.coords.expand(b_sz, -1, -1, -1)], dim=1)", "\n\nclass UNet(nn.Module):\n    def __init__(self, num_blocks, img_size=64, filter_start=32, in_chnls=4, out_chnls=1):\n        super().__init__()\n        c = filter_start\n        if num_blocks == 4:\n            enc_in = [in_chnls, c, 2 * c, 2 * c]\n            enc_out = [c, 2 * c, 2 * c, 2 * c]\n            dec_in = [4 * c, 4 * c, 4 * c, 2 * c]\n            dec_out = [2 * c, 2 * c, c, c]\n        elif num_blocks == 5:\n            enc_in = [in_chnls, c, c, 2 * c, 2 * c]\n            enc_out = [c, c, 2 * c, 2 * c, 2 * c]\n            dec_in = [4 * c, 4 * c, 4 * c, 2 * c, 2 * c]\n            dec_out = [2 * c, 2 * c, c, c, c]\n        elif num_blocks == 6:\n            enc_in = [in_chnls, c, c, c, 2 * c, 2 * c]\n            enc_out = [c, c, c, 2 * c, 2 * c, 2 * c]\n            dec_in = [4 * c, 4 * c, 4 * c, 2 * c, 2 * c, 2 * c]\n            dec_out = [2 * c, 2 * c, c, c, c, c]\n        self.down = []\n        self.up = []\n        # 3x3 kernels, stride 1, padding 1\n        for i, o in zip(enc_in, enc_out):\n            self.down.append(ConvGNReLU(i, o, 3, 1, 1))\n        for i, o in zip(dec_in, dec_out):\n            self.up.append(ConvGNReLU(i, o, 3, 1, 1))\n        self.down = nn.ModuleList(self.down)\n        self.up = nn.ModuleList(self.up)\n        self.featuremap_size = img_size // 2 ** (num_blocks - 1)\n        self.mlp = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(2 * c * self.featuremap_size**2, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 2 * c * self.featuremap_size**2),\n            nn.ReLU(),\n        )\n        if out_chnls > 0:\n            self.final_conv = nn.Conv2d(c, out_chnls, 1)\n        else:\n            self.final_conv = nn.Identity()\n        self.out_chnls = out_chnls\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x_down = [x]\n        skip = []\n        # Down\n        for i, block in enumerate(self.down):\n            act = block(x_down[-1])\n            skip.append(act)\n            if i < len(self.down) - 1:\n                act = F.interpolate(act, scale_factor=0.5, mode=\"nearest\")\n            x_down.append(act)\n        # FC\n        x_up = self.mlp(x_down[-1])\n        x_up = x_up.view(batch_size, -1, self.featuremap_size, self.featuremap_size)\n        # Up\n        for i, block in enumerate(self.up):\n            features = torch.cat([x_up, skip[-1 - i]], dim=1)\n            x_up = block(features)\n            if i < len(self.up) - 1:\n                x_up = F.interpolate(x_up, scale_factor=2.0, mode=\"nearest\")\n        return self.final_conv(x_up), None", "\n\n@torch.no_grad()\ndef check_log_masks(log_m_k):\n    # Note: this seems to be checking for under(over)flow when moving log->exp space\n    # Adjusted this to run on GPU most of the time until the errors need to be printed\n    flat = torch.stack(log_m_k, dim=4).exp().sum(dim=4).view(-1)\n    diff = flat - torch.ones_like(flat)\n    max_diff, idx = diff.max(dim=0)\n    if torch.anu(max_diff > 1e-3) or torch.any(torch.isnan(flat)):\n        print(f\"Max diff: {max_diff.cpu().item()}\")\n        masks_k = log_m_k.view(log_m_k.shape[0], -1)[:, idx].exp().cpu().squeeze()\n        for i, v in enumerate(masks_k):\n            print(f\"Mask value at k={i}: {v}\")\n        # TODO: change this drop into an interactive debugger if interactive env.\n        raise ValueError(\"Masks do not sum to 1.0. Not close enough.\")", "\n\nclass SemiConv(nn.Conv2d):\n    def __init__(self, in_c, out_ch, size):\n        super().__init__(in_c, out_ch, 1)\n        self.gate = nn.Parameter(torch.zeros(1), requires_grad=True)\n        self.register_buffer(\n            \"uv\",\n            torch.cat(\n                [\n                    torch.zeros(out_ch - 2, size, size),\n                    torch.stack(torch.meshgrid(*([torch.linspace(-1, 1, size)] * 2))),\n                ]\n            )[None],\n            persistent=False,\n        )\n\n    def forward(self, input):\n        x = self.gate * super().forward(input)\n        return x + self.uv, x[:, -2:]", "\n\ndef clamp_st(x, lower, upper):\n    # From: http://docs.pyro.ai/en/0.3.3/_modules/pyro/distributions/iaf.html\n    return x + (x.clamp(lower, upper) - x).detach()\n\n\ndef euclidian_norm(x):\n    # Clamp before taking sqrt for numerical stability\n    return clamp_st((x**2).sum(1), 1e-10, 1e10).sqrt()", "\n\ndef euclidian_distance(ea, eb):\n    # Unflatten if needed if one is an image and the other a vector\n    if ea.dim() == 4 and eb.dim() == 2:\n        eb = eb.unsqueeze(-1).unsqueeze(-1)\n    if eb.dim() == 4 and ea.dim() == 2:\n        ea = ea.unsqueeze(-1).unsqueeze(-1)\n    return euclidian_norm(ea - eb)\n", "\n\ndef squared_distance(ea, eb):\n    if ea.dim() == 4 and eb.dim() == 2:\n        eb = eb.unsqueeze(-1).unsqueeze(-1)\n    if eb.dim() == 4 and ea.dim() == 2:\n        ea = ea.unsqueeze(-1).unsqueeze(-1)\n    return ((ea - eb) ** 2).sum(1)\n\n\nclass InstanceColouringSBP(nn.Module):\n    def __init__(\n        self, img_size, kernel=\"gaussian\", colour_dim=8, K_steps=None, feat_dim=None, semiconv=True\n    ):\n        super().__init__()\n        # Config\n        self.img_size = img_size\n        self.kernel = kernel\n        self.colour_dim = colour_dim\n        # Initialise kernel sigma\n        if self.kernel == \"laplacian\":\n            sigma_init = 1.0 / (np.sqrt(K_steps) * np.log(2))\n        elif self.kernel == \"gaussian\":\n            sigma_init = 1.0 / (K_steps * np.log(2))\n        elif self.kernel == \"epanechnikov\":\n            sigma_init = 2.0 / K_steps\n        else:\n            return ValueError(\"No valid kernel.\")\n        self.log_sigma = nn.Parameter(torch.tensor(sigma_init).log())\n        # Colour head\n        if semiconv:\n            self.colour_head = SemiConv(feat_dim, self.colour_dim, img_size)\n        else:\n            self.colour_head = nn.Conv2d(feat_dim, self.colour_dim, 1)\n\n    def forward(self, features, steps_to_run, debug=False, dynamic_K=False, *args, **kwargs):\n        batch_size = features.size(0)\n        if dynamic_K:\n            assert batch_size == 1\n        # Get colours\n        colour_out = self.colour_head(features)\n        if isinstance(colour_out, tuple):\n            colour, delta = colour_out\n        else:\n            colour, delta = colour_out, None\n        # Sample from uniform to select random pixels as seeds\n        # rand_pixel = torch.empty(batch_size, 1, *colour.shape[2:])\n        # rand_pixel = rand_pixel.uniform_()\n        rand_pixel = torch.rand(batch_size, 1, *colour.shape[2:], device=features.device)\n        # Run SBP\n        seed_list = []\n        log_m_k = []\n        log_s_k = [\n            torch.zeros(batch_size, 1, self.img_size, self.img_size, device=features.device)\n        ]\n        for step in range(steps_to_run):\n            # Determine seed\n            scope = F.interpolate(\n                log_s_k[step].exp(), size=colour.shape[2:], mode=\"bilinear\", align_corners=False\n            )\n            pixel_probs = rand_pixel * scope\n            rand_max = pixel_probs.flatten(2).argmax(2).flatten()\n            # --TODO(martin): parallelise this--\n            seed = features.new_empty((batch_size, self.colour_dim))\n            for bidx in range(batch_size):\n                seed[bidx, :] = colour.flatten(2)[bidx, :, rand_max[bidx]]\n            # seed = colour.flatten(2).gather(-1, rand_max.view(-1, 1, 1).expand(-1, rand_max.shape[1], -1)).squeeze(-1)\n            seed_list.append(seed)\n            # Compute masks\n            # Note the distance here is in channel-wise\n            if self.kernel == \"laplacian\":\n                distance = euclidian_distance(colour, seed)  # [B, H, W]\n                alpha = torch.exp(-distance / self.log_sigma.exp())\n            elif self.kernel == \"gaussian\":\n                distance = squared_distance(colour, seed)  # [B, H, W]\n                alpha = torch.exp(-distance / self.log_sigma.exp())\n            elif self.kernel == \"epanechnikov\":\n                distance = squared_distance(colour, seed)  # [B, H, W]\n                alpha = (1 - distance / self.log_sigma.exp()).relu()\n            else:\n                raise ValueError(\"No valid kernel.\")\n            alpha = alpha.unsqueeze(1)\n            # Sanity checks\n            if debug:\n                assert alpha.max() <= 1, alpha.max()\n                assert alpha.min() >= 0, alpha.min()\n            # Clamp mask values to [0.01, 0.99] for numerical stability\n            # TODO(martin): clamp less aggressively?\n            alpha = clamp_st(alpha, 0.01, 0.99)\n            # SBP update\n            log_a = torch.log(alpha)\n            log_neg_a = torch.log(1 - alpha)\n            log_m = log_s_k[step] + log_a\n\n            if dynamic_K and log_m.exp().sum() < 20:\n                break\n            log_m_k.append(log_m)\n            log_s_k.append(log_s_k[step] + log_neg_a)\n        # Set mask equal to scope for last step\n        log_m_k.append(log_s_k[-1])\n        # Accumulate stats\n        stats = {\"colour\": colour, \"delta\": delta, \"seeds\": seed_list}\n        return log_m_k, log_s_k, stats", "\n\nclass InstanceColouringSBP(nn.Module):\n    def __init__(\n        self, img_size, kernel=\"gaussian\", colour_dim=8, K_steps=None, feat_dim=None, semiconv=True\n    ):\n        super().__init__()\n        # Config\n        self.img_size = img_size\n        self.kernel = kernel\n        self.colour_dim = colour_dim\n        # Initialise kernel sigma\n        if self.kernel == \"laplacian\":\n            sigma_init = 1.0 / (np.sqrt(K_steps) * np.log(2))\n        elif self.kernel == \"gaussian\":\n            sigma_init = 1.0 / (K_steps * np.log(2))\n        elif self.kernel == \"epanechnikov\":\n            sigma_init = 2.0 / K_steps\n        else:\n            return ValueError(\"No valid kernel.\")\n        self.log_sigma = nn.Parameter(torch.tensor(sigma_init).log())\n        # Colour head\n        if semiconv:\n            self.colour_head = SemiConv(feat_dim, self.colour_dim, img_size)\n        else:\n            self.colour_head = nn.Conv2d(feat_dim, self.colour_dim, 1)\n\n    def forward(self, features, steps_to_run, debug=False, dynamic_K=False, *args, **kwargs):\n        batch_size = features.size(0)\n        if dynamic_K:\n            assert batch_size == 1\n        # Get colours\n        colour_out = self.colour_head(features)\n        if isinstance(colour_out, tuple):\n            colour, delta = colour_out\n        else:\n            colour, delta = colour_out, None\n        # Sample from uniform to select random pixels as seeds\n        # rand_pixel = torch.empty(batch_size, 1, *colour.shape[2:])\n        # rand_pixel = rand_pixel.uniform_()\n        rand_pixel = torch.rand(batch_size, 1, *colour.shape[2:], device=features.device)\n        # Run SBP\n        seed_list = []\n        log_m_k = []\n        log_s_k = [\n            torch.zeros(batch_size, 1, self.img_size, self.img_size, device=features.device)\n        ]\n        for step in range(steps_to_run):\n            # Determine seed\n            scope = F.interpolate(\n                log_s_k[step].exp(), size=colour.shape[2:], mode=\"bilinear\", align_corners=False\n            )\n            pixel_probs = rand_pixel * scope\n            rand_max = pixel_probs.flatten(2).argmax(2).flatten()\n            # --TODO(martin): parallelise this--\n            seed = features.new_empty((batch_size, self.colour_dim))\n            for bidx in range(batch_size):\n                seed[bidx, :] = colour.flatten(2)[bidx, :, rand_max[bidx]]\n            # seed = colour.flatten(2).gather(-1, rand_max.view(-1, 1, 1).expand(-1, rand_max.shape[1], -1)).squeeze(-1)\n            seed_list.append(seed)\n            # Compute masks\n            # Note the distance here is in channel-wise\n            if self.kernel == \"laplacian\":\n                distance = euclidian_distance(colour, seed)  # [B, H, W]\n                alpha = torch.exp(-distance / self.log_sigma.exp())\n            elif self.kernel == \"gaussian\":\n                distance = squared_distance(colour, seed)  # [B, H, W]\n                alpha = torch.exp(-distance / self.log_sigma.exp())\n            elif self.kernel == \"epanechnikov\":\n                distance = squared_distance(colour, seed)  # [B, H, W]\n                alpha = (1 - distance / self.log_sigma.exp()).relu()\n            else:\n                raise ValueError(\"No valid kernel.\")\n            alpha = alpha.unsqueeze(1)\n            # Sanity checks\n            if debug:\n                assert alpha.max() <= 1, alpha.max()\n                assert alpha.min() >= 0, alpha.min()\n            # Clamp mask values to [0.01, 0.99] for numerical stability\n            # TODO(martin): clamp less aggressively?\n            alpha = clamp_st(alpha, 0.01, 0.99)\n            # SBP update\n            log_a = torch.log(alpha)\n            log_neg_a = torch.log(1 - alpha)\n            log_m = log_s_k[step] + log_a\n\n            if dynamic_K and log_m.exp().sum() < 20:\n                break\n            log_m_k.append(log_m)\n            log_s_k.append(log_s_k[step] + log_neg_a)\n        # Set mask equal to scope for last step\n        log_m_k.append(log_s_k[-1])\n        # Accumulate stats\n        stats = {\"colour\": colour, \"delta\": delta, \"seeds\": seed_list}\n        return log_m_k, log_s_k, stats", "\n\ndef genesis_x_loss(x, log_m_k, x_r_k, std, pixel_wise=False):\n    # 1.) Sum over steps for per pixel & channel (ppc) losses\n    p_xr_stack = dist.Normal(torch.stack(x_r_k, dim=4), std)\n    log_xr_stack = p_xr_stack.log_prob(x.unsqueeze(4))\n    log_m_stack = torch.stack(log_m_k, dim=4)\n    log_mx = log_m_stack + log_xr_stack\n    err_ppc = -log_mx.logsumexp(dim=4)\n    # 2.) Sum across channels and spatial dimensions\n    if pixel_wise:\n        return err_ppc\n    else:\n        return err_ppc.sum(dim=(1, 2, 3))", "\n\ndef genesis_mask_latent_loss(\n    q_zm_0_k, zm_0_k, zm_k_k=None, ldj_k=None, prior_lstm=None, prior_linear=None, debug=False\n):\n    num_steps = len(zm_0_k)\n    batch_size = zm_0_k[0].size(0)\n    latent_dim = zm_0_k[0].size(1)\n    if zm_k_k is None:\n        zm_k_k = zm_0_k\n\n    # -- Determine prior --\n    if prior_lstm is not None and prior_linear is not None:\n        # zm_seq shape: (att_steps-2, batch_size, ldim)\n        # Do not need the last element in z_k\n        zm_seq = torch.cat([zm.view(1, batch_size, -1) for zm in zm_k_k[:-1]], dim=0)\n        # lstm_out shape: (att_steps-2, batch_size, state_size)\n        # Note: recurrent state is handled internally by LSTM\n        lstm_out, _ = prior_lstm(zm_seq)\n        # linear_out shape: (att_steps-2, batch_size, 2*ldim)\n        linear_out = prior_linear(lstm_out)\n        linear_out = torch.chunk(linear_out, 2, dim=2)\n        mu_raw = torch.tanh(linear_out[0])\n        # Note: ditton about prior_linear\n        sigma_raw = torch.sigmoid(linear_out[1] + 4.0) + 1e-4\n        # Split into K steps, shape: (att_steps-2)*[1, batch_size, ldim]\n        mu_k = torch.split(mu_raw, 1, dim=0)\n        sigma_k = torch.split(sigma_raw, 1, dim=0)\n        # Use standard Normal as prior for first step\n        p_zm_k = [dist.Normal(0, 1)]\n        # Autoregressive prior for later steps\n        for mean, std in zip(mu_k, sigma_k):\n            # Remember to remove unit dimension at dim=0\n            p_zm_k += [\n                dist.Normal(mean.view(batch_size, latent_dim), std.view(batch_size, latent_dim))\n            ]\n        # Sanity checks\n        if debug:\n            assert zm_seq.size(0) == num_steps - 1\n    else:\n        p_zm_k = num_steps * [dist.Normal(0, 1)]\n\n    # -- Compute KL using Monte Carlo samples for every step k --\n    kl_m_k = []\n    for step, p_zm in enumerate(p_zm_k):\n        log_q = q_zm_0_k[step].log_prob(zm_0_k[step]).sum(dim=1)\n        log_p = p_zm.log_prob(zm_k_k[step]).sum(dim=1)\n        kld = log_q - log_p\n        if ldj_k is not None:\n            ldj = ldj_k[step].sum(dim=1)\n            kld = kld - ldj\n        kl_m_k.append(kld)\n\n    # -- Sanity check --\n    if debug:\n        assert len(p_zm_k) == num_steps\n        assert len(kl_m_k) == num_steps\n\n    return kl_m_k, p_zm_k", "\n\ndef monet_get_mask_recon_stack(m_r_logits_k, prior_mode, log):\n    if prior_mode == \"softmax\":\n        if log:\n            return F.log_softmax(torch.stack(m_r_logits_k, dim=4), dim=4)\n        return F.softmax(torch.stack(m_r_logits_k, dim=4), dim=4)\n    elif prior_mode == \"scope\":\n        log_m_r_k = []\n        log_s = torch.zeros_like(m_r_logits_k[0])\n        for step, logits in enumerate(m_r_logits_k):\n            if step == len(m_r_logits_k) - 1:\n                log_m_r_k.append(log_s)\n            else:\n                log_a = F.logsigmoid(logits)\n                log_neg_a = F.logsigmoid(-logits)\n                log_m_r_k.append(log_s + log_a)\n                log_s = log_s + log_neg_a\n        log_m_r_stack = torch.stack(log_m_r_k, dim=4)\n        return log_m_r_stack if log else log_m_r_stack.exp()\n    else:\n        raise ValueError(\"No valid prior mode.\")", "\n\ndef monet_kl_m_loss(log_m_k, log_m_r_k, debug=False):\n    if debug:\n        assert len(log_m_k) == len(log_m_r_k)\n    batch_size = log_m_k[0].size(0)\n    m_stack = torch.stack(log_m_k, dim=4).exp()\n    m_r_stack = torch.stack(log_m_r_k, dim=4).exp()\n    # Lower bound to 1e-5 to avoid infinities\n    m_stack = torch.max(m_stack, torch.tensor(1e-5))\n    m_r_stack = torch.max(m_r_stack, torch.tensor(1e-5))\n    q_m = dist.Categorical(m_stack.view(-1, len(log_m_k)))\n    p_m = dist.Categorical(m_r_stack.view(-1, len(log_m_k)))\n    kl_m_ppc = dist.kl_divergence(q_m, p_m).view(batch_size, -1)\n    return kl_m_ppc.sum(dim=1)", "\n\nclass Genesis2(nn.Module):\n    def __init__(\n        self,\n        feat_dim: int = 64,\n        kernel: str = \"gaussian\",\n        semiconv: bool = True,\n        dynamic_K: bool = False,\n        klm_loss: bool = False,\n        detach_mr_in_klm: bool = True,\n        g_goal: float = 0.5655,\n        g_lr: float = 1e-5,\n        g_alpha: float = 0.99,\n        g_init: float = 1.0,\n        g_min: float = 1e-10,\n        g_speedup: float = 10.0,\n        K_steps: int = 11,\n        img_size: int = 128,  # Clevr\n        autoreg_prior: bool = True,\n        pixel_bound: bool = True,\n        pixel_std: float = 0.7,\n        debug: bool = False,\n    ):\n        super().__init__()\n        # Configuration\n        self.K_steps = K_steps\n        self.pixel_bound = pixel_bound\n        self.feat_dim = feat_dim\n        self.klm_loss = klm_loss\n        self.detach_mr_in_klm = detach_mr_in_klm\n        self.dynamic_K = dynamic_K\n        self.debug = debug\n        # Encoder\n        self.encoder = UNet(\n            num_blocks=int(np.log2(img_size) - 1),\n            img_size=img_size,\n            filter_start=min(feat_dim, 64),\n            in_chnls=3,\n            out_chnls=-1,\n        )\n        self.att_process = InstanceColouringSBP(\n            img_size=img_size,\n            kernel=kernel,\n            colour_dim=8,\n            K_steps=self.K_steps,\n            feat_dim=feat_dim,\n            semiconv=semiconv,\n        )\n        self.seg_head = ConvGNReLU(feat_dim, feat_dim, 3, 1, 1)\n        self.feat_head = nn.Sequential(\n            ConvGNReLU(feat_dim, feat_dim, 3, 1, 1), nn.Conv2d(feat_dim, 2 * feat_dim, 1)\n        )\n        self.z_head = nn.Sequential(\n            nn.LayerNorm(2 * feat_dim),\n            nn.Linear(2 * feat_dim, 2 * feat_dim),\n            nn.ReLU(inplace=True),\n            nn.Linear(2 * feat_dim, 2 * feat_dim),\n        )\n        # Decoder\n        c = feat_dim\n        self.decoder_module = nn.Sequential(\n            BroadcastLayer(img_size // 16),\n            nn.ConvTranspose2d(feat_dim + 2, c, 5, 2, 2, 1),\n            nn.GroupNorm(8, c),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(c, c, 5, 2, 2, 1),\n            nn.GroupNorm(8, c),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(c, min(c, 64), 5, 2, 2, 1),\n            nn.GroupNorm(8, min(c, 64)),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(min(c, 64), min(c, 64), 5, 2, 2, 1),\n            nn.GroupNorm(8, min(c, 64)),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(min(c, 64), 4, 1),\n        )\n        # --- Prior ---\n        self.autoreg_prior = autoreg_prior\n        self.prior_lstm, self.prior_linear = None, None\n        if self.autoreg_prior and self.K_steps > 1:\n            self.prior_lstm = nn.LSTM(feat_dim, 4 * feat_dim)\n            self.prior_linear = nn.Linear(4 * feat_dim, 2 * feat_dim)\n        # --- Output pixel distribution ---\n        # assert pixel_std1 == cfg.pixel_std2\n        self.std = pixel_std\n        self.geco = GECO(\n            g_goal * 3 * img_size**2,\n            g_lr * (64**2 / img_size**2),\n            g_alpha,\n            g_init,\n            g_min,\n            g_speedup,\n        )\n\n    def forward(self, x):\n        batch_size, _, H, W = x.shape\n\n        # --- Extract features ---\n        enc_feat, _ = self.encoder(x)\n        enc_feat = F.relu(enc_feat)\n\n        # --- Predict attention masks ---\n        if self.dynamic_K:\n            if batch_size > 1:\n                # Iterate over individual elements in batch\n                log_m_k = [[] for _ in range(self.K_steps)]\n                att_stats, log_s_k = None, None\n                for f in torch.split(enc_feat, 1, dim=0):\n                    log_m_k_b, _, _ = self.att_process(\n                        self.seg_head(f), self.K_steps - 1, dynamic_K=True\n                    )\n                    for step in range(self.K_steps):\n                        if step < len(log_m_k_b):\n                            log_m_k[step].append(log_m_k_b[step])\n                        else:\n                            log_m_k[step].append(-1e10 * torch.ones([1, 1, H, W]))\n                for step in range(self.K_steps):\n                    log_m_k[step] = torch.cat(log_m_k[step], dim=0)\n                if self.debug:\n                    assert len(log_m_k) == self.K_steps\n            else:\n                log_m_k, log_s_k, att_stats = self.att_process(\n                    self.seg_head(enc_feat), self.K_steps - 1, dynamic_K=True\n                )\n        else:\n            log_m_k, log_s_k, att_stats = self.att_process(\n                self.seg_head(enc_feat), self.K_steps - 1, dynamic_K=False\n            )\n            if self.debug:\n                assert len(log_m_k) == self.K_steps\n\n        # -- Object features, latents, and KL\n        comp_stats = dict(mu_k=[], sigma_k=[], z_k=[], kl_l_k=[], q_z_k=[])\n        for log_m in log_m_k:\n            mask = log_m.exp()\n            # Masked sum\n            obj_feat = mask * self.feat_head(enc_feat)\n            obj_feat = obj_feat.sum((2, 3))\n            # Normalise\n            obj_feat = obj_feat / (mask.sum((2, 3)) + 1e-5)\n            # Posterior\n            mu, sigma_ps = self.z_head(obj_feat).chunk(2, dim=1)\n            # Note: Not sure why sigma needs to biased by 0.5 here; leaving though\n            sigma = F.softplus(sigma_ps + 0.5) + 1e-8\n            q_z = dist.Normal(mu, sigma)\n            z = q_z.rsample()\n            comp_stats[\"mu_k\"].append(mu)\n            comp_stats[\"sigma_k\"].append(sigma)\n            comp_stats[\"z_k\"].append(z)\n            comp_stats[\"q_z_k\"].append(q_z)\n\n        # --- Decode latents ---\n        recon, x_r_k, log_m_r_k = self.decode_latents(comp_stats[\"z_k\"])\n\n        # --- Loss terms ---\n        losses = {}\n        # -- Reconstruction loss\n        losses[\"err\"] = genesis_x_loss(x, log_m_r_k, x_r_k, self.std)\n        mx_r_k = [x * logm.exp() for x, logm in zip(x_r_k, log_m_r_k)]\n        # -- Optional: Attention mask loss\n        if self.klm_loss:\n            if self.detach_mr_in_klm:\n                log_m_r_k = [m.detach() for m in log_m_r_k]\n            losses[\"kl_m\"] = monet_kl_m_loss(\n                log_m_k=log_m_k, log_m_r_k=log_m_r_k, debug=self.debug\n            )\n        # -- Component KL\n        losses[\"kl_l_k\"], p_z_k = genesis_mask_latent_loss(\n            comp_stats[\"q_z_k\"],\n            comp_stats[\"z_k\"],\n            prior_lstm=self.prior_lstm,\n            prior_linear=self.prior_linear,\n            debug=self.debug,\n        )\n\n        # Track quantities of interest\n        stats = dict(\n            recon=recon,\n            log_m_k=log_m_k,\n            log_s_k=log_s_k,\n            x_r_k=x_r_k,\n            log_m_r_k=log_m_r_k,\n            mx_r_k=mx_r_k,\n            instance_seg=torch.argmax(torch.cat(log_m_k, dim=1), dim=1),\n            instance_seg_r=torch.argmax(torch.cat(log_m_r_k, dim=1), dim=1),\n        )\n\n        # Sanity checks\n        if self.debug:\n            if not self.dynamic_K:\n                assert len(log_m_k) == self.K_steps\n                assert len(log_m_r_k) == self.K_steps\n            check_log_masks(log_m_k)\n            check_log_masks(log_m_r_k)\n\n        recon_loss = losses[\"err\"].mean()\n        kl_m, kl_l = torch.tensor(0), torch.tensor(0)\n        # -- KL stage 1\n        if \"kl_m\" in losses:\n            kl_m = losses[\"kl_m\"].mean(0)\n        elif \"kl_m_k\" in losses:\n            kl_m = torch.stack(losses[\"kl_m_k\"], dim=1).mean(dim=0).sum()\n        # -- KL stage 2\n        if \"kl_l\" in losses:\n            kl_l = losses[\"kl_l\"].mean(0)\n        elif \"kl_l_k\" in losses:\n            kl_l = torch.stack(losses[\"kl_l_k\"], dim=1).mean(dim=0).sum()\n        kl = (kl_l + kl_m).mean(0)\n\n        elbo = recon_loss + kl\n        loss = self.geco.loss(recon_loss, kl)\n\n        ret = {\n            \"canvas\": recon,\n            \"loss\": loss,\n            \"elbo\": elbo,\n            \"rec_loss\": recon_loss,\n            \"kl\": kl,\n            \"beta\": self.geco.beta,\n            \"layers\": {\n                \"mask\": torch.stack(log_m_r_k, dim=1).exp(),\n                \"patch\": torch.stack(x_r_k, dim=1),\n                \"other_mask\": torch.stack(log_m_k, dim=1).exp(),\n            },\n        }\n\n        return ret\n\n    def decode_latents(self, z_k):\n        # --- Reconstruct components and image ---\n        x_r_k, m_r_logits_k = [], []\n        for z in z_k:\n            dec = self.decoder_module(z)\n            x_r_k.append(dec[:, :3, :, :])\n            m_r_logits_k.append(dec[:, 3:, :, :])\n        # Optional: Apply pixelbound\n        if self.pixel_bound:\n            x_r_k = [torch.sigmoid(item) for item in x_r_k]\n        # --- Reconstruct masks ---\n        log_m_r_stack = monet_get_mask_recon_stack(m_r_logits_k, \"softmax\", log=True)\n        log_m_r_k = torch.split(log_m_r_stack, 1, dim=4)\n        log_m_r_k = [m[:, :, :, :, 0] for m in log_m_r_k]\n        # --- Reconstruct input image by marginalising (aka summing) ---\n        x_r_stack = torch.stack(x_r_k, dim=4)\n        m_r_stack = torch.stack(log_m_r_k, dim=4).exp()\n        recon = (m_r_stack * x_r_stack).sum(dim=4)\n\n        return recon, x_r_k, log_m_r_k\n\n    def sample(self, batch_size, K_steps=None):\n        K_steps = self.K_steps if K_steps is None else K_steps\n\n        # Sample latents\n        if self.autoreg_prior:\n            z_k = [dist.Normal(0, 1).sample([batch_size, self.feat_dim])]\n            state = None\n            for k in range(1, K_steps):\n                # TODO(martin): reuse code from forward method?\n                lstm_out, state = self.prior_lstm(z_k[-1].view(1, batch_size, -1), state)\n                linear_out = self.prior_linear(lstm_out)\n                linear_out = torch.chunk(linear_out, 2, dim=2)\n                linear_out = [item.squeeze(0) for item in linear_out]\n                mu = torch.tanh(linear_out[0])\n                # Note: the 4.0 bias seems to be for 0-initialised self.prior_linear.bias.\n                # This gives starting sigma as 1.0001.\n                # TODO: replace this with something else, like a direct init of bias.\n                # TODO: Sigmoid saturates outside of (-88, 16) (0. gradients, 0. or 1. output).\n                # TODO: Having a positive target +4.0 seems to somewhat reduce that.\n                sigma = torch.sigmoid(linear_out[1] + 4.0) + 1e-4\n                p_z = dist.Normal(\n                    mu.view([batch_size, self.feat_dim]), sigma.view([batch_size, self.feat_dim])\n                )\n                z_k.append(p_z.sample())\n        else:\n            p_z = dist.Normal(0, 1)\n            z_k = [p_z.sample([batch_size, self.feat_dim]) for _ in range(K_steps)]\n\n        # Decode latents\n        recon, x_r_k, log_m_r_k = self.decode_latents(z_k)\n\n        stats = dict(\n            x_k=x_r_k, log_m_k=log_m_r_k, mx_k=[x * m.exp() for x, m in zip(x_r_k, log_m_r_k)]\n        )\n        return recon, stats", "\n\nclass GECO(nn.Module):\n    def __init__(self, goal, step_size, alpha=0.99, beta_init=1.0, beta_min=1e-10, speedup=None):\n        super().__init__()\n        self.err_ema = None\n        # self.goal = goal\n        # self.step_size = step_size\n        # self.alpha = alpha\n        # self.beta = torch.tensor(beta_init)\n        # self.beta_min = torch.tensor(beta_min)\n        # self.beta_max = torch.tensor(1e10)\n        # self.speedup = speedup\n        self.register_buffer(\"goal\", torch.tensor(goal))\n        self.register_buffer(\"step_size\", torch.tensor(step_size))\n        self.register_buffer(\"alpha\", torch.tensor(alpha))\n        self.register_buffer(\"beta\", torch.tensor(beta_init))\n        self.register_buffer(\"beta_min\", torch.tensor(beta_min))\n        self.register_buffer(\"beta_max\", torch.tensor(1e10))\n        if speedup is not None:\n            self.register_buffer(\"speedup\", torch.tensor(speedup))\n\n    def to_cuda(self):\n        self.beta = self.beta.cuda()\n        if self.err_ema is not None:\n            self.err_ema = self.err_ema.cuda()\n\n    def loss(self, err, kld):\n        # Compute loss with current beta\n        loss = err + self.beta * kld\n        # Update beta without computing / backpropping gradients\n        with torch.no_grad():\n            if self.err_ema is None:\n                self.err_ema = err\n            else:\n                self.err_ema = (1.0 - self.alpha) * err + self.alpha * self.err_ema\n            constraint = self.goal - self.err_ema\n            if self.speedup is not None and constraint.item() > 0:\n                factor = torch.exp(self.speedup * self.step_size * constraint)\n            else:\n                factor = torch.exp(self.step_size * constraint)\n            self.beta = (factor * self.beta).clamp(self.beta_min, self.beta_max)\n        # Return loss\n        return loss", ""]}
{"filename": "src/models/components/slota/slota.py", "chunked_list": ["import torch\nfrom torch import nn\n\n\nclass SlotAttention(nn.Module):\n    \"\"\"Slot Attention module.\n\n    Args:\n        num_slots: int - Number of slots in Slot Attention.\n        iterations: int - Number of iterations in Slot Attention.\n        num_attn_heads: int - Number of multi-head attention in Slot Attention,\n    \"\"\"\n\n    def __init__(\n        self,\n        num_slots: int = 7,\n        num_iterations: int = 3,\n        num_attn_heads: int = 1,\n        slot_dim: int = 64,\n        hid_dim: int = 64,\n        mlp_hid_dim: int = 128,\n        eps: float = 1e-8,\n    ):\n        super().__init__()\n\n        self.num_slots = num_slots\n        self.num_iterations = num_iterations\n        self.num_attn_heads = num_attn_heads\n        self.slot_dim = slot_dim\n        self.hid_dim = hid_dim\n        self.mlp_hid_dim = mlp_hid_dim\n        self.eps = eps\n\n        self.scale = (num_slots // num_attn_heads) ** -0.5\n\n        self.slots_mu = nn.Parameter(torch.randn(1, 1, self.slot_dim))\n        self.slots_sigma = nn.Parameter(torch.randn(1, 1, self.slot_dim))\n\n        self.norm_input = nn.LayerNorm(self.hid_dim)\n        self.norm_slot = nn.LayerNorm(self.slot_dim)\n        self.norm_mlp = nn.LayerNorm(self.slot_dim)\n\n        self.to_q = nn.Linear(self.slot_dim, self.slot_dim)\n        self.to_k = nn.Linear(self.hid_dim, self.slot_dim)\n        self.to_v = nn.Linear(self.hid_dim, self.slot_dim)\n\n        self.gru = nn.GRUCell(self.slot_dim, self.slot_dim)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(self.slot_dim, self.mlp_hid_dim),\n            nn.ReLU(),\n            nn.Linear(self.mlp_hid_dim, self.slot_dim),\n        )\n\n    def forward(self, inputs, num_slots=None, train=False):\n        outputs = dict()\n\n        B, N_in, D_in = inputs.shape\n        K = num_slots if num_slots is not None else self.num_slots\n        D_slot = self.slot_dim\n        N_heads = self.num_attn_heads\n\n        mu = self.slots_mu.expand(B, K, -1)\n        sigma = self.slots_sigma.expand(B, K, -1)\n        slots = torch.normal(mu, torch.abs(sigma) + self.eps)\n\n        inputs = self.norm_input(inputs)\n\n        k = self.to_k(inputs).reshape(B, N_in, N_heads, -1).transpose(1, 2)\n        v = self.to_v(inputs).reshape(B, N_in, N_heads, -1).transpose(1, 2)\n        # k, v: (B, N_heads, N_in, D_slot // N_heads).\n\n        if not train:\n            attns = list()\n\n        for iter_idx in range(self.num_iterations):\n            slots_prev = slots\n            slots = self.norm_slot(slots)\n\n            q = self.to_q(slots).reshape(B, K, N_heads, -1).transpose(1, 2)\n            # q: (B, N_heads, K, slot_D // N_heads)\n\n            attn_logits = torch.einsum(\"bhid, bhjd->bhij\", k, q) * self.scale\n\n            attn = attn_logits.softmax(dim=-1) + self.eps  # Normalization over slots\n            # attn: (B, N_heads, N_in, K)\n\n            if not train:\n                attns.append(attn)\n\n            attn = attn / torch.sum(attn, dim=-2, keepdim=True)  # Weighted mean\n            # attn: (B, N_heads, N_in, K)\n\n            updates = torch.einsum(\"bhij,bhid->bhjd\", attn, v)\n            # updates: (B, N_heads, K, slot_D // N_heads)\n            updates = updates.transpose(1, 2).reshape(B, K, -1)\n            # updates: (B, K, slot_D)\n\n            slots = self.gru(updates.reshape(-1, D_slot), slots_prev.reshape(-1, D_slot))\n\n            slots = slots.reshape(B, -1, D_slot)\n            slots = slots + self.mlp(self.norm_mlp(slots))\n\n        outputs[\"slots\"] = slots\n        outputs[\"attn\"] = attn\n        if not train:\n            outputs[\"attns\"] = torch.stack(attns, dim=1)\n            # attns: (B, T, N_heads, N_in, K)\n\n        return outputs", "\n\nif __name__ == \"__main__\":\n    _ = SlotAttention()\n"]}
{"filename": "src/models/components/slota/slota_utils.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nclass SoftPositionEmbed(nn.Module):\n    \"\"\"Builds the soft position embedding layer with learnable projection.\n\n    Args:\n        hid_dim (int): Size of input feature dimension.\n        resolution (tuple): Tuple of integers specifying width and height of grid.\n    \"\"\"\n\n    def __init__(\n        self,\n        hid_dim: int = 64,\n        resolution: Tuple[int, int] = (128, 128),\n    ):\n        super().__init__()\n        self.embedding = nn.Linear(4, hid_dim, bias=True)\n        self.grid = self.build_grid(resolution)\n\n    def forward(self, inputs):\n        self.grid = self.grid.to(inputs.device)\n        grid = self.embedding(self.grid).to(inputs.device)\n        return inputs + grid\n\n    def build_grid(self, resolution):\n        ranges = [np.linspace(0.0, 1.0, num=res) for res in resolution]\n        grid = np.meshgrid(*ranges, sparse=False, indexing=\"ij\")\n        grid = np.stack(grid, axis=-1)\n        grid = np.reshape(grid, [resolution[0], resolution[1], -1])\n        grid = np.expand_dims(grid, axis=0)\n        grid = grid.astype(np.float32)\n        return torch.from_numpy(np.concatenate([grid, 1.0 - grid], axis=-1))", "\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 128,\n        hid_dim: int = 64,\n        enc_depth: int = 4,\n    ):\n        super().__init__()\n        assert enc_depth > 2, \"Depth must be larger than 2.\"\n\n        convs = nn.ModuleList([nn.Conv2d(3, hid_dim, 5, padding=\"same\"), nn.ReLU()])\n        for _ in range(enc_depth - 2):\n            convs.extend([nn.Conv2d(hid_dim, hid_dim, 5, padding=\"same\"), nn.ReLU()])\n        convs.append(nn.Conv2d(hid_dim, hid_dim, 5, padding=\"same\"))\n        self.convs = nn.Sequential(*convs)\n\n        self.encoder_pos = SoftPositionEmbed(hid_dim, (img_size, img_size))\n        self.layer_norm = nn.LayerNorm([img_size * img_size, hid_dim])\n        self.mlp = nn.Sequential(\n            nn.Linear(hid_dim, hid_dim), nn.ReLU(), nn.Linear(hid_dim, hid_dim)\n        )\n\n    def forward(self, x):\n        x = self.convs(x)  # [B, D, H, W]\n        x = x.permute(0, 2, 3, 1)  # [B, H, W ,D]\n        x = self.encoder_pos(x)\n        x = torch.flatten(x, 1, 2)\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x", "\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        img_size: int = 128,\n        slot_dim: int = 64,\n        dec_hid_dim: int = 64,\n        dec_init_size: int = 8,\n        dec_depth: int = 6,\n    ):\n        super().__init__()\n\n        self.img_size = img_size\n        self.dec_init_size = dec_init_size\n        self.decoder_pos = SoftPositionEmbed(slot_dim, (dec_init_size, dec_init_size))\n\n        D_slot = slot_dim\n        D_hid = dec_hid_dim\n        upsample_step = int(np.log2(img_size // dec_init_size))\n\n        deconvs = nn.ModuleList()\n        count_layer = 0\n        for _ in range(upsample_step):\n            deconvs.extend(\n                [\n                    nn.ConvTranspose2d(\n                        D_hid if count_layer > 0 else D_slot,\n                        D_hid,\n                        5,\n                        stride=(2, 2),\n                        padding=2,\n                        output_padding=1,\n                    ),\n                    nn.ReLU(),\n                ]\n            )\n            count_layer += 1\n\n        for _ in range(dec_depth - upsample_step - 1):\n            deconvs.extend(\n                [\n                    nn.ConvTranspose2d(\n                        D_hid if count_layer > 0 else D_slot, D_hid, 5, stride=(1, 1), padding=2\n                    ),\n                    nn.ReLU(),\n                ]\n            )\n            count_layer += 1\n\n        deconvs.append(nn.ConvTranspose2d(D_hid, 4, 3, stride=(1, 1), padding=1))\n        self.deconvs = nn.Sequential(*deconvs)\n\n    def forward(self, x):\n        \"\"\"Broadcast slot features to a 2D grid and collapse slot dimension.\"\"\"\n        x = x.reshape(-1, x.shape[-1]).unsqueeze(1).unsqueeze(2)\n        x = x.repeat((1, self.dec_init_size, self.dec_init_size, 1))\n        x = self.decoder_pos(x)\n        x = x.permute(0, 3, 1, 2)\n        x = self.deconvs(x)\n        x = x[:, :, : self.img_size, : self.img_size]\n        x = x.permute(0, 2, 3, 1)\n        return x"]}
{"filename": "src/models/components/slota/slota_ae.py", "chunked_list": ["from typing import Any, Dict, Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom src.models.components.slota.slota import SlotAttention\nfrom src.models.components.slota.slota_utils import Decoder, Encoder\n\n\nclass SlotAttentionAutoEncoder(nn.Module):\n    \"\"\"Builds Slot Attention-based auto-encoder for object discovery.\n\n    Args:\n        num_slots (int): Number of slots in Slot Attention.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: int = 128,\n        num_slots: int = 7,\n        num_iterations: int = 3,\n        num_attn_heads: int = 1,\n        hid_dim: int = 64,\n        slot_dim: int = 64,\n        mlp_hid_dim: int = 128,\n        eps: float = 1e-8,\n        enc_depth: int = 4,\n        dec_hid_dim: int = 64,\n        dec_init_size: int = 8,\n        dec_depth: int = 6,\n    ):\n        super().__init__()\n        self.num_slots = num_slots\n\n        self.encoder_cnn = Encoder(\n            img_size=img_size,\n            hid_dim=hid_dim,\n            enc_depth=enc_depth,\n        )\n        self.decoder_cnn = Decoder(\n            img_size=img_size,\n            slot_dim=slot_dim,\n            dec_hid_dim=dec_hid_dim,\n            dec_init_size=dec_init_size,\n            dec_depth=dec_depth,\n        )\n\n        self.slot_attention = SlotAttention(\n            num_slots=num_slots,\n            num_iterations=num_iterations,\n            num_attn_heads=num_attn_heads,\n            slot_dim=slot_dim,\n            hid_dim=hid_dim,\n            mlp_hid_dim=mlp_hid_dim,\n            eps=eps,\n        )\n\n    def forward(self, image, train=True):\n        # image: (batch_size, num_channels, height, width)\n        B, C, H, W = image.shape\n\n        # Convolutional encoder with position embedding\n        x = self.encoder_cnn(image)  # CNN Backbone\n        # x: (B, height * width, hid_dim)\n\n        # Slot Attention module.\n        slota_outputs = self.slot_attention(x, train=train)\n        slots = slota_outputs[\"slots\"]\n        # slots: (N, K, slot_dim)\n\n        x = self.decoder_cnn(slots)\n        # x: (B*K, height, width, num_channels+1)\n\n        # Undo combination of slot and batch dimension; split alpha masks\n        recons, masks = x.reshape(B, self.num_slots, H, W, C + 1).split([3, 1], dim=-1)\n        # recons: (B, K, height, width, num_channels)\n        # masks: (B, K, height, width, 1)\n\n        # Normalize alpha masks over slots.\n        masks = nn.Softmax(dim=1)(masks)\n\n        recon_combined = torch.sum(recons * masks, dim=1)  # Recombine image\n        recon_combined = recon_combined.permute(0, 3, 1, 2)\n        # recon_combined: (batch_size, num_channels, height, width)\n\n        outputs = dict()\n        outputs[\"recon_combined\"] = recon_combined\n        outputs[\"recons\"] = recons\n        outputs[\"masks\"] = masks\n        outputs[\"slots\"] = slots\n        outputs[\"attn\"] = slota_outputs[\"attn\"]\n        if not train:\n            outputs[\"attns\"] = slota_outputs[\"attns\"]\n            # attns: (B, T, N_heads, N_in, K)\n\n        return outputs", "\n\nclass SlotAttentionAutoEncoder(nn.Module):\n    \"\"\"Builds Slot Attention-based auto-encoder for object discovery.\n\n    Args:\n        num_slots (int): Number of slots in Slot Attention.\n    \"\"\"\n\n    def __init__(\n        self,\n        img_size: int = 128,\n        num_slots: int = 7,\n        num_iterations: int = 3,\n        num_attn_heads: int = 1,\n        hid_dim: int = 64,\n        slot_dim: int = 64,\n        mlp_hid_dim: int = 128,\n        eps: float = 1e-8,\n        enc_depth: int = 4,\n        dec_hid_dim: int = 64,\n        dec_init_size: int = 8,\n        dec_depth: int = 6,\n    ):\n        super().__init__()\n        self.num_slots = num_slots\n\n        self.encoder_cnn = Encoder(\n            img_size=img_size,\n            hid_dim=hid_dim,\n            enc_depth=enc_depth,\n        )\n        self.decoder_cnn = Decoder(\n            img_size=img_size,\n            slot_dim=slot_dim,\n            dec_hid_dim=dec_hid_dim,\n            dec_init_size=dec_init_size,\n            dec_depth=dec_depth,\n        )\n\n        self.slot_attention = SlotAttention(\n            num_slots=num_slots,\n            num_iterations=num_iterations,\n            num_attn_heads=num_attn_heads,\n            slot_dim=slot_dim,\n            hid_dim=hid_dim,\n            mlp_hid_dim=mlp_hid_dim,\n            eps=eps,\n        )\n\n    def forward(self, image, train=True):\n        # image: (batch_size, num_channels, height, width)\n        B, C, H, W = image.shape\n\n        # Convolutional encoder with position embedding\n        x = self.encoder_cnn(image)  # CNN Backbone\n        # x: (B, height * width, hid_dim)\n\n        # Slot Attention module.\n        slota_outputs = self.slot_attention(x, train=train)\n        slots = slota_outputs[\"slots\"]\n        # slots: (N, K, slot_dim)\n\n        x = self.decoder_cnn(slots)\n        # x: (B*K, height, width, num_channels+1)\n\n        # Undo combination of slot and batch dimension; split alpha masks\n        recons, masks = x.reshape(B, self.num_slots, H, W, C + 1).split([3, 1], dim=-1)\n        # recons: (B, K, height, width, num_channels)\n        # masks: (B, K, height, width, 1)\n\n        # Normalize alpha masks over slots.\n        masks = nn.Softmax(dim=1)(masks)\n\n        recon_combined = torch.sum(recons * masks, dim=1)  # Recombine image\n        recon_combined = recon_combined.permute(0, 3, 1, 2)\n        # recon_combined: (batch_size, num_channels, height, width)\n\n        outputs = dict()\n        outputs[\"recon_combined\"] = recon_combined\n        outputs[\"recons\"] = recons\n        outputs[\"masks\"] = masks\n        outputs[\"slots\"] = slots\n        outputs[\"attn\"] = slota_outputs[\"attn\"]\n        if not train:\n            outputs[\"attns\"] = slota_outputs[\"attns\"]\n            # attns: (B, T, N_heads, N_in, K)\n\n        return outputs", "\n\nif __name__ == \"__main__\":\n    _ = SlotAttentionAutoEncoder()\n"]}
{"filename": "src/models/components/monet/monet.py", "chunked_list": ["\"\"\"\nsource: https://github.com/karazijal/clevrtex/blob/master/ool/picture/models/monet.py\n\nReimplementation of MONet\n\"MONet: Unsupervised Scene Decomposition and Representation\"\nChristopher P. Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra,\nIrina Higgins, Matt Botvinick and Alexander Lerchner\nhttps://arxiv.org/abs/1901.11390\n\n", "\n\nSomewhat based on implementation from\nhttps://github.com/baudm/MONet-pytorch\n\"\"\"\n\nimport itertools\nfrom typing import Union\n\nimport numpy as np", "\nimport numpy as np\n\n# import ipdb as ipdb\nimport torch\nimport torch.distributions as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions.kl import kl_divergence\n", "from torch.distributions.kl import kl_divergence\n\nfrom src.models.components.monet.monet_base import MONetBase\n\n\nclass ComponentEncoder(nn.Sequential):\n    \"\"\"The paper seems to to target the cell_width of 12x12 pixels.\"\"\"\n\n    def __init__(self, in_shape, z_dim=16):\n        inc, h, w = in_shape\n        h = ((((h + 1) // 2 + 1) // 2 + 1) // 2 + 1) // 2\n        w = ((((w + 1) // 2 + 1) // 2 + 1) // 2 + 1) // 2\n        super().__init__(\n            nn.Conv2d(inc + 1, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(h * w * 64, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2 * z_dim),\n        )\n        self.z_dim = z_dim\n\n    def forward(self, img, mask):\n        x = torch.cat([img, mask], dim=1)\n        x = super().forward(x)\n        mu, logstd = x[:, : self.z_dim], x[:, self.z_dim :]\n        return mu, logstd.exp()", "\n\nclass ComponentDecoder(nn.Sequential):\n    def __init__(self, z_dim, out_shape):\n        super().__init__(\n            nn.Conv2d(z_dim + 2, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, out_shape[0] + 1, kernel_size=1, stride=1, padding=0),\n        )\n        self.out_shape = out_shape\n\n    def forward(self, x):\n        h, w = self.out_shape[-2:]\n        h += 8\n        w += 8\n        x = x.view(x.size(0), -1, 1, 1).repeat(1, 1, h, w)\n        hs = torch.linspace(-1, 1, h, device=x.device, dtype=x.dtype)\n        ws = torch.linspace(-1, 1, w, device=x.device, dtype=x.dtype)\n        c = torch.stack(torch.meshgrid(hs, ws)).view(1, 2, h, w).repeat(x.size(0), 1, 1, 1)\n        x = torch.cat([x, c], dim=1)\n        x = super().forward(x)\n        img = torch.sigmoid(x[:, : self.out_shape[0]])\n        msk = x[:, -1:]\n        return img, msk", "\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, input_nc, output_nc, resize=True):\n        super().__init__()\n        self.conv = nn.Conv2d(input_nc, output_nc, 3, padding=1, bias=False)\n        self.norm = nn.InstanceNorm2d(output_nc, affine=True)\n        self._resize = resize\n\n    def comb(self, x, y):\n        if x.shape == y.shape:\n            return torch.cat([x, y], dim=1)\n        return torch.cat(\n            [F.pad(x, (0, y.size(-1) - x.size(-1), 0, y.size(-2) - x.size(-2)), \"constant\", 0), y],\n            dim=1,\n        )\n\n    def forward(self, *inputs):\n        downsampling = len(inputs) == 1\n        x = inputs[0] if downsampling else self.comb(*inputs)\n        x = self.conv(x)\n        x = self.norm(x)\n        x = skip = F.relu(x)\n        if self._resize:\n            x = F.interpolate(skip, scale_factor=0.5 if downsampling else 2.0, mode=\"nearest\")\n        return (x, skip) if downsampling else x", "\n\nclass Attention(nn.Module):\n    def __init__(self, n_blocks, in_shape, ngf=64):\n        super().__init__()\n        c, h, w = in_shape\n        x = torch.zeros(1, c + 1, h, w)\n        self.downblocks = nn.ModuleList([AttentionBlock(c + 1, ngf, resize=True)])  # Fist\n        x = self.downblocks[-1](x)[0]\n        # print(x.shape)\n        upblocks = [AttentionBlock(2 * ngf, ngf, resize=False)]  # Last\n        for i in range(1, n_blocks - 1):\n            self.downblocks.append(\n                AttentionBlock(ngf * 2 ** (i - 1), ngf * min(2**i, 8), resize=True)\n            )\n            x = self.downblocks[-1](x)[0]\n            # print(x.shape)\n            upblocks.append(\n                AttentionBlock(2 * ngf * min(2**i, 8), ngf * min(2 ** (i - 1), 8), resize=True)\n            )\n        self.downblocks.append(\n            AttentionBlock(\n                ngf * min(2 ** (n_blocks - 1), 8), ngf * min(2 ** (n_blocks - 1), 8), resize=False\n            )\n        )\n        x = self.downblocks[-1](x)[0]\n        # print(x.shape)\n        upblocks.append(\n            AttentionBlock(\n                2 * ngf * min(2 ** (n_blocks - 1), 8),\n                ngf * min(2 ** (n_blocks - 1), 8),\n                resize=True,\n            )\n        )\n        self.upblocks = nn.ModuleList(list(reversed(upblocks)))\n        inc = np.product(x.shape)\n        self.mlp = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(inc, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, inc),\n            nn.ReLU(),\n        )\n        self.output = nn.Conv2d(ngf, 1, kernel_size=1)\n\n    def forward(self, x, log_sk):\n        x = torch.cat((x, log_sk), dim=1)\n        skips = []\n        for l in self.downblocks:\n            x, skip = l(x)\n            skips.append(skip)\n            # print('down', x.shape, skip.shape)\n\n        x = self.mlp(x).view(skips[-1].shape)\n\n        for l, skip in zip(self.upblocks, reversed(skips)):\n            # print('up', x.shape, skip.shape)\n            x = l(x, skip)\n\n        logits = self.output(x)\n        return F.logsigmoid(logits), F.logsigmoid(\n            -logits\n        )  # log(sigmoid(logits)), log(1-sigmoid(logits))", "\n\nclass MONet(MONetBase):\n    def __init__(\n        self,\n        n_slots: int = 7,\n        n_blocks: int = 6,\n        shape: Union[tuple, list] = (3, 128, 128),\n        z_dim: int = 16,\n        bg_scl: int = 0.09,\n        fg_scl: int = 0.11,\n    ):\n        super().__init__(\n            pres_dist_name=\"unused\",\n            output_dist=\"unused\",\n            output_hparam=1.0,\n            n_particles=1,\n            z_pres_prior_p=1,\n            z_where_prior_loc=[0, 0, 0, 0],\n            z_where_prior_scale=[1, 1, 1, 1],\n            z_what_prior_loc=[0.0] * z_dim,\n            z_what_prior_scale=[1.0] * z_dim,\n            z_depth_prior_loc=0.0,\n            z_depth_prior_scale=1.0,\n        )\n\n        self.n = n_slots\n        self.shape = shape\n        self.enc = ComponentEncoder(shape, z_dim)\n        self.dec = ComponentDecoder(z_dim, shape)\n        self.att = Attention(n_blocks, shape)\n\n        self.beta = 0.5\n        self.gamma = 0.5\n        self.bg_scl = bg_scl\n        self.fg_scl = fg_scl\n\n    def forward(self, x, train=False):\n        n, c, h, w = x.shape\n        m = torch.zeros(n, self.n, 1, h, w, device=x.device, dtype=x.dtype)\n        x_til = torch.zeros(n, self.n, c, h, w, device=x.device, dtype=x.dtype)\n        m_til_logits = torch.zeros(n, self.n, 1, h, w, device=x.device, dtype=x.dtype)\n        log_s_k = torch.zeros(n, 1, h, w, device=x.device, dtype=x.dtype)\n\n        if train:\n            kl = torch.zeros(n, 1, device=x.device, dtype=x.dtype)\n            rec_loss = torch.zeros(n, self.n, c, h, w, device=x.device, dtype=x.dtype)\n\n        for k in range(self.n):\n            if k == self.n - 1:\n                log_m_k = log_s_k\n            else:\n                log_alpha_k, log_one_minus_alpha_k = self.att(x, log_s_k)\n                # print('log_alpha', log_alpha_k.shape)\n                log_m_k = log_s_k + log_alpha_k\n                log_s_k = log_s_k + log_one_minus_alpha_k\n\n            loc, scale = self.enc(x, log_m_k)\n            z_k_post = dist.Normal(loc, scale)\n            z_k = z_k_post.rsample()\n\n            x_til[:, k], m_til_logits[:, k] = self.dec(z_k)\n            # print('xt mt', x_til[:, k].shape, m_til_logits[:, k].shape)\n            m[:, k] = log_m_k.exp()\n\n            if train:\n                scl = self.bg_scl if k == 0 else self.fg_scl\n                rec_loss[:, k] = log_m_k + dist.Normal(x_til[:, k], scl).log_prob(x)\n                # print('rec loss', rec_loss[:, k].shape)\n                kl += (\n                    kl_divergence(z_k_post, self.what_prior(z_k_post.batch_shape)).sum(\n                        1, keepdims=True\n                    )\n                    * self.beta\n                )\n\n        m_rec = x_til * m\n        canvas = m_rec.sum(1)\n        m_til = torch.log_softmax(m_til_logits, dim=1)\n        # print('m_til', m_til.shape)\n        # print('m_rec', m_rec.shape)\n        r = {\n            \"canvas\": canvas,\n            \"layers\": {\"patch\": x_til, \"mask\": m, \"recons\": m_rec, \"other_mask\": m_til},\n        }\n        if train:\n            kl_mask = F.kl_div(m_til, m, reduction=\"none\").sum((1, 2, 3, 4)) * self.gamma\n            # print('mask', kl_mask.shape)\n            rec_loss = torch.logsumexp(rec_loss, dim=1).sum((1, 2, 3))\n            # print('rec', rec_loss.shape)\n            loss = -rec_loss + kl.squeeze(1) + kl_mask\n            loss = loss.mean()\n            r[\"loss\"] = loss\n            r[\"rec_loss\"] = (-rec_loss).mean()\n            r[\"kl\"] = kl.mean()\n            r[\"kl_mask\"] = kl_mask.mean()\n        return r\n\n    def param_groups(self):\n        return [{\"params\": self.parameters(), \"lr\": 1}]", ""]}
{"filename": "src/models/components/monet/monet_base.py", "chunked_list": ["\"\"\"\nsource: https://github.com/karazijal/clevrtex/blob/master/ool/picture/models/ool_base.py\n\"\"\"\n\nimport warnings\n\nimport numpy as np\nimport torch\nimport torch.distributions as dist\nimport torch.nn as nn", "import torch.distributions as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions.kl import kl_divergence, register_kl\nfrom torch.distributions.utils import logits_to_probs\n\n\nclass MONetBase(nn.Module):\n    def __init__(\n        self,\n        n_particles=1,\n        output_dist=\"normal\",\n        output_hparam=0.3,\n        pres_dist_name=\"bernoulli\",\n        z_pres_temperature=1.0,\n        z_pres_prior_p=0.01,\n        z_where_prior_loc=[-2.197, -2.197, 0, 0],\n        z_where_prior_scale=[0.5, 0.5, 1, 1],\n        z_what_prior_loc=None,\n        z_what_prior_scale=None,\n        z_depth_prior_loc=None,\n        z_depth_prior_scale=None,\n        z_bg_prior_loc=None,\n        z_bg_prior_scale=None,\n        z_sh_prior_loc=None,\n        z_sh_prior_scale=None,\n    ):\n        super().__init__()\n\n        self.n_particles = n_particles\n        self.pres_dist_name = pres_dist_name\n\n        self.output_dist_name = output_dist\n        self.output_hparam = output_hparam\n\n        self.z_pres_temperature = z_pres_temperature\n        self.z_pres_prior_p = z_pres_prior_p\n\n        self.z_where_prior_loc = z_where_prior_loc\n        self.z_where_prior_scale = z_where_prior_scale\n\n        if z_what_prior_loc is not None:\n            self.z_what_prior_loc = z_what_prior_loc\n        if z_what_prior_scale is not None:\n            self.z_what_prior_scale = z_what_prior_scale\n\n        if z_depth_prior_loc is not None:\n            self.z_depth_prior_loc = z_depth_prior_loc\n        if z_depth_prior_scale is not None:\n            self.z_depth_prior_scale = z_depth_prior_scale\n\n        if z_bg_prior_loc is not None:\n            self.z_bg_prior_loc = z_bg_prior_loc\n        if z_bg_prior_scale is not None:\n            self.z_bg_prior_scale = z_bg_prior_scale\n\n        if z_sh_prior_loc is not None:\n            self.z_sh_prior_loc = z_sh_prior_loc\n        if z_sh_prior_scale is not None:\n            self.z_sh_prior_scale = z_sh_prior_scale\n\n        self._prints = set()\n\n    def ensure_correct_tensor(self, maybe_tensor, other=None):\n        if not isinstance(maybe_tensor, torch.Tensor):\n            if isinstance(maybe_tensor, (int, float)):\n                mult = 1\n                if other is not None:\n                    mult = other.shape[-1]\n                maybe_tensor = torch.tensor([maybe_tensor] * mult)\n            else:\n                maybe_tensor = torch.tensor(maybe_tensor)\n        maybe_tensor = maybe_tensor.view(1, -1)\n        if other is not None:\n            maybe_tensor = maybe_tensor.to(other)\n        return maybe_tensor\n\n    def __set_tensor_value(self, maybe_tensor, dest: torch.Tensor):\n        if not isinstance(maybe_tensor, torch.Tensor):\n            if isinstance(maybe_tensor, (int, float)):\n                dest.fill_(maybe_tensor)\n                return\n            if isinstance(maybe_tensor, (list, tuple, np.ndarray)):\n                maybe_tensor = torch.from_numpy(np.array(maybe_tensor)).view(1, -1).to(dest)\n        dest.copy_(maybe_tensor, non_blocking=True)\n\n    @property\n    def z_pres_temperature(self):\n        return self._z_pres_temperature\n\n    @property\n    def z_pres_prior_p(self):\n        return self._z_pres_prior_p\n\n    @property\n    def z_where_prior_loc(self):\n        return self._z_where_prior_loc\n\n    @property\n    def z_where_prior_scale(self):\n        return self._z_where_prior_scale\n\n    @property\n    def z_what_prior_loc(self):\n        return self._z_what_prior_loc\n\n    @property\n    def z_what_prior_scale(self):\n        return self._z_what_prior_scale\n\n    @property\n    def z_depth_prior_loc(self):\n        return self._z_depth_prior_loc\n\n    @property\n    def z_depth_prior_scale(self):\n        return self._z_depth_prior_scale\n\n    @property\n    def z_bg_prior_loc(self):\n        return self._z_bg_prior_loc\n\n    @property\n    def z_bg_prior_scale(self):\n        return self._z_bg_prior_scale\n\n    @property\n    def z_sh_prior_loc(self):\n        return self._z_sh_prior_loc\n\n    @property\n    def z_sh_prior_scale(self):\n        return self._z_sh_prior_scale\n\n    @property\n    def _tensor_spec(self):\n        try:\n            p = next(self.parameters())\n        except StopIteration:\n            p = nn.Parameter(torch.zeros(1))\n        return dict(device=p.device, dtype=p.dtype)\n\n    def pres_dist(self, p=None, batch_shape=None, name=None, logits=None):\n        # 1. / (1.0 - torch.tensor(1., dtype=torch.float32, device='cuda').clamp(min=eps, max=1. - eps))\n        # Seems to give non inf on 1e-7,\n        pres_dist_name = name or self.pres_dist_name\n        # spec = self._tensor_spec\n        # device = spec['device']\n        eps = 1e-6\n        if pres_dist_name == \"bernoulli\":\n            d = dist.Bernoulli(p.clamp(min=eps, max=1.0 - eps))\n        elif pres_dist_name == \"relaxedbernoulli-hard\" or pres_dist_name == \"gumbelsoftmax-st\":\n            # Gumber-Softmax\n            p = p.clamp(min=eps, max=1.0 - eps)\n            d = dist.RelaxedBernoulli(self.z_pres_temperature, p)\n\n            def s():\n                y = d.rsample()\n                y_hard = torch.round(y).to(torch.float)\n                return (y_hard - y).detach() + y  # Straight through\n\n            d.sample = s\n            if not hasattr(self, \"defined_kl\"):\n                self.defined_kl = True\n\n                @register_kl(dist.RelaxedBernoulli, dist.RelaxedBernoulli)\n                def kl_gumbel_softmax(p, q):\n                    n_particles = self.n_particles\n                    s = p.rsample()\n                    logps = p.log_prob(s) / n_particles\n                    logqs = q.log_prob(s) / n_particles\n                    for _ in range(1, n_particles):\n                        s = p.rsample()\n                        logps += p.log_prob(s) / n_particles\n                        logqs += q.log_prob(s) / n_particles\n                    return logps - logqs\n\n        elif (\n            pres_dist_name == \"relaxedbernoulli\"\n            or pres_dist_name == \"concrete\"\n            or pres_dist_name == \"gumbel-softmax\"\n        ):\n            # Gumber-Softmax\n\n            p = p.clamp(min=eps, max=1.0 - eps)\n            d = dist.RelaxedBernoulli(self.z_pres_temperature, p)\n\n            def s():\n                y = d.rsample()\n                if self.training:\n                    return y\n                y_hard = torch.round(y).to(torch.float)\n                return (y_hard - y).detach() + y  # Straight through\n\n            d.sample = s\n\n            if not hasattr(self, \"defined_kl\"):\n                self.defined_kl = True\n\n                @register_kl(dist.RelaxedBernoulli, dist.RelaxedBernoulli)\n                def kl_gumbel_softmax(p, q):\n                    n_particles = self.n_particles\n                    s = p.rsample()\n                    logps = p.log_prob(s) / n_particles\n                    logqs = q.log_prob(s) / n_particles\n                    for _ in range(1, n_particles):\n                        s = p.rsample()\n                        logps += p.log_prob(s) / n_particles\n                        logqs += q.log_prob(s) / n_particles\n                    return logps - logqs\n\n        elif pres_dist_name == \"relaxedbernoulli-bern_kl\":\n            p = p.clamp(min=eps, max=1.0 - eps)\n            d = dist.RelaxedBernoulli(self.z_pres_temperature, p)\n\n            def s():\n                y = d.rsample()\n                if self.training:\n                    return y\n                y_hard = torch.round(y).to(torch.float)\n                return (\n                    y_hard - y\n                ).detach() + y  # Straight through -- return y_hard with gradients of y\n\n            d.sample = s\n\n            if not hasattr(self, \"defined_kl\"):\n                self.defined_kl = True\n\n                @register_kl(dist.RelaxedBernoulli, dist.RelaxedBernoulli)\n                def kl_gumbel_softmax(p, q):\n                    pb = dist.Bernoulli(p.probs)\n                    qb = dist.Bernoulli(q.probs)\n                    return kl_divergence(pb, qb)\n\n                @register_kl(dist.Bernoulli, dist.RelaxedBernoulli)\n                def kl_relbern_bern(p, q):\n                    qb = dist.Bernoulli(q.probs)\n                    return kl_divergence(p, qb)\n\n        elif pres_dist_name == \"continuousbernoulli\":\n            #  The continuous Bernoulli: fixing a pervasive error in variational autoencoders,\n            #  Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845\n            p = p.clamp(min=eps, max=1.0 - eps)\n            d = dist.ContinuousBernoulli(p)\n            d.sample = d.rsample\n        else:\n            raise ValueError(f\"Unknown distribution {pres_dist_name}\")\n        if batch_shape:\n            d = d.expand(batch_shape)\n        return d\n\n    def where_prior(self, batch_shape=None):\n        d = dist.Normal(self.z_where_prior_loc, self.z_where_prior_scale)\n        if batch_shape:\n            d = d.expand(batch_shape)\n        return d\n\n    def what_prior(self, batch_shape=None):\n        d = dist.Normal(self.z_what_prior_loc, self.z_what_prior_scale)\n        if batch_shape:\n            d = d.expand(batch_shape)\n        return d\n\n    def bg_prior(self, batch_shape=None):\n        d = dist.Normal(self.z_bg_prior_loc, self.z_bg_prior_scale)\n        if batch_shape:\n            d = d.expand(batch_shape)\n        return d\n\n    def depth_prior(self, batch_shape=None):\n        d = dist.Normal(self.z_depth_prior_loc, self.z_depth_prior_scale)\n        if batch_shape:\n            d = d.expand(batch_shape)\n        return d\n\n    def shape_prior(self, batch_shape=None):\n        d = dist.Normal(self.z_sh_prior_loc, self.z_sh_prior_scale)\n        if batch_shape:\n            d = d.expand(batch_shape)\n        return d\n\n    def output_dist(self, canvas):\n        eps = 1e-6\n        if self.output_dist_name == \"normal\":\n            d = dist.Normal(canvas, self.output_hparam)\n            d.sample = d.rsample\n        elif self.output_dist_name == \"bernoulli\":\n            d = dist.Bernoulli(canvas.clamp(min=eps, max=1.0 - eps))\n        elif self.output_dist_name == \"relaxedbernoulli\":\n            # Gumber-Softmax\n            d = dist.RelaxedBernoulli(\n                torch.tensor([self.output_hparam], device=canvas.device),\n                canvas.clamp(min=eps, max=1.0 - eps),\n            )\n            d.sample = d.rsample\n            d.mean = canvas.clamp(min=eps, max=1.0 - eps)\n        elif self.output_dist_name == \"continuousbernoulli\":\n            #  The continuous Bernoulli: fixing a pervasive error in variational autoencoders,\n            #  Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845\n            d = dist.ContinuousBernoulli(canvas.clamp(min=eps, max=1.0 - eps))\n            d.sample = d.rsample\n\n        elif self.output_dist_name == \"discmixlogistic\":\n            # warnings.warn(\"discmixlogistic distribution requires logits, which do not support currently support summation\")\n            if isinstance(self.output_hparam, (tuple, list)):\n                nmix, nbits = self.output_hparam\n                d = DiscMixLogistic(canvas, nmix, nbits)\n            else:\n                d = DiscMixLogistic(canvas, num_mix=self.output_hparam)\n        elif self.output_dist_name == \"disclogistic\":\n            d = DiscLogistic(canvas)\n        else:\n            raise ValueError(f\"Unknown output distribution {self.output_dist_name}\")\n        return d\n\n    @z_pres_temperature.setter\n    def z_pres_temperature(self, value):\n        if hasattr(self, \"_z_pres_temperature\"):\n            # self._z_pres_temperature = self.ensure_correct_tensor(value, self._z_pres_temperature)\n            self.__set_tensor_value(value, self._z_pres_temperature)\n        else:\n            self.register_buffer(\"_z_pres_temperature\", torch.tensor(value).view(1, -1))\n\n    @z_pres_prior_p.setter\n    def z_pres_prior_p(self, value):\n        if hasattr(self, \"_z_pres_prior_p\"):\n            # self._z_pres_prior_p = self.ensure_correct_tensor(value, self._z_pres_prior_p)\n            self.__set_tensor_value(value, self._z_pres_prior_p)\n        else:\n            self.register_buffer(\"_z_pres_prior_p\", torch.tensor(value).view(1, -1))\n\n    @z_where_prior_loc.setter\n    def z_where_prior_loc(self, value):\n        if hasattr(self, \"_z_where_prior_loc\"):\n            # self._z_where_prior_loc = self.ensure_correct_tensor(value, self._z_where_prior_loc)\n            self.__set_tensor_value(value, self._z_where_prior_loc)\n        else:\n            self.register_buffer(\"_z_where_prior_loc\", torch.tensor(value).view(1, -1))\n\n    @z_where_prior_scale.setter\n    def z_where_prior_scale(self, value):\n        if hasattr(self, \"_z_where_prior_scale\"):\n            # self._z_where_prior_scale = self.ensure_correct_tensor(value, self._z_where_prior_scale)\n            self.__set_tensor_value(value, self._z_where_prior_scale)\n        else:\n            self.register_buffer(\"_z_where_prior_scale\", torch.tensor(value).view(1, -1))\n\n    @z_what_prior_loc.setter\n    def z_what_prior_loc(self, value):\n        if hasattr(self, \"_z_what_prior_loc\"):\n            # self._z_what_prior_loc = self.ensure_correct_tensor(value, self._z_what_prior_loc)\n            self.__set_tensor_value(value, self._z_what_prior_loc)\n        else:\n            self.register_buffer(\"_z_what_prior_loc\", torch.tensor(value).view(1, -1))\n\n    @z_what_prior_scale.setter\n    def z_what_prior_scale(self, value):\n        if hasattr(self, \"_z_what_prior_scale\"):\n            # self._z_what_prior_scale = self.ensure_correct_tensor(value, self._z_what_prior_scale)\n            self.__set_tensor_value(value, self._z_what_prior_scale)\n        else:\n            self.register_buffer(\"_z_what_prior_scale\", torch.tensor(value).view(1, -1))\n\n    @z_depth_prior_loc.setter\n    def z_depth_prior_loc(self, value):\n        if hasattr(self, \"_z_depth_prior_loc\"):\n            # self._z_depth_prior_loc = self.ensure_correct_tensor(value, self._z_depth_prior_loc)\n            self.__set_tensor_value(value, self._z_depth_prior_loc)\n        else:\n            self.register_buffer(\"_z_depth_prior_loc\", torch.tensor(value).view(1, -1))\n\n    @z_depth_prior_scale.setter\n    def z_depth_prior_scale(self, value):\n        if hasattr(self, \"_z_depth_prior_scale\"):\n            # self._z_depth_prior_scale = self.ensure_correct_tensor(value, self._z_depth_prior_scale)\n            self.__set_tensor_value(value, self._z_depth_prior_scale)\n        else:\n            self.register_buffer(\"_z_depth_prior_scale\", torch.tensor(value).view(1, -1))\n\n    def baseline_parameters(self):\n        if hasattr(self, \"baseline\") and self.baseline is not None:\n            return self.baseline.parameters()\n        return ()\n\n    def model_parameters(self):\n        if hasattr(self, \"baseline\") and self.baseline is not None:\n            baseline_parameters = set(self.baseline_parameters())\n            for name, param in self.named_parameters():\n                if param not in baseline_parameters:\n                    yield param\n        else:\n            return self.parameters()\n\n    @z_bg_prior_loc.setter\n    def z_bg_prior_loc(self, value):\n        if hasattr(self, \"_z_bg_prior_loc\"):\n            # self._z_bg_prior_loc = self.ensure_correct_tensor(value, self._z_bg_prior_loc)\n            self.__set_tensor_value(value, self._z_bg_prior_loc)\n        else:\n            self.register_buffer(\"_z_bg_prior_loc\", self.ensure_correct_tensor(value))\n\n    @z_bg_prior_scale.setter\n    def z_bg_prior_scale(self, value):\n        if hasattr(self, \"_z_bg_prior_scale\"):\n            # self._z_bg_prior_scale = self.ensure_correct_tensor(value, self._z_bg_prior_scale)\n            self.__set_tensor_value(value, self._z_bg_prior_scale)\n        else:\n            self.register_buffer(\"_z_bg_prior_scale\", self.ensure_correct_tensor(value))\n\n    @z_sh_prior_loc.setter\n    def z_sh_prior_loc(self, value):\n        if hasattr(self, \"_z_sh_prior_loc\"):\n            # self._z_sh_prior_loc = self.ensure_correct_tensor(value, self._z_sh_prior_loc)\n            self.__set_tensor_value(value, self._z_sh_prior_loc)\n        else:\n            self.register_buffer(\"_z_sh_prior_loc\", self.ensure_correct_tensor(value))\n\n    @z_sh_prior_scale.setter\n    def z_sh_prior_scale(self, value):\n        if hasattr(self, \"_z_sh_prior_scale\"):\n            # self._z_sh_prior_scale = self.ensure_correct_tensor(value, self._z_sh_prior_scale)\n            self.__set_tensor_value(value, self._z_sh_prior_scale)\n        else:\n            self.register_buffer(\"_z_sh_prior_scale\", self.ensure_correct_tensor(value))\n\n    def onceprint(self, *args, **kwargs):\n        \"\"\"Just a useful debug function to see shapes when first running.\"\"\"\n        k = \"_\".join(str(a) for a in args)\n        if k not in self._prints:\n            print(*args, **kwargs)\n            self._prints.add(k)", "\n\nclass DiscLogistic:\n    def __init__(self, param):\n        # B, C, H, W = param.size()\n        # self.num_c = C // 2\n        self.means, self.log_scales = param.chunk(2, 1)\n        self.log_scales = self.log_scales.clamp(min=-7.0)\n\n    @property\n    def mean(self):\n        img = self.means / 2.0 + 0.5\n        return img\n\n    def log_prob(self, samples):\n        # assert torch.max(samples) <= 1.0 and torch.min(samples) >= 0.0\n        # convert samples to be in [-1, 1]\n        samples = 2 * samples - 1.0\n\n        # B, C, H, W = samples.size()\n        # assert C == self.num_c\n\n        centered = samples - self.means  # B, 3, H, W\n        inv_stdv = torch.exp(-self.log_scales)\n        plus_in = inv_stdv * (centered + 1.0 / 255.0)\n        cdf_plus = torch.sigmoid(plus_in)\n        min_in = inv_stdv * (centered - 1.0 / 255.0)\n        cdf_min = torch.sigmoid(min_in)\n        log_cdf_plus = plus_in - F.softplus(plus_in)\n        log_one_minus_cdf_min = -F.softplus(min_in)\n        cdf_delta = cdf_plus - cdf_min\n        mid_in = inv_stdv * centered\n        log_pdf_mid = mid_in - self.log_scales - 2.0 * F.softplus(mid_in)\n\n        log_prob_mid_safe = torch.where(\n            cdf_delta > 1e-5,\n            torch.log(torch.clamp(cdf_delta, min=1e-10)),\n            log_pdf_mid - np.log(127.5),\n        )\n        # woow the original implementation uses samples > 0.999, this ignores the largest possible pixel value (255)\n        # which is mapped to 0.9922\n        log_probs = torch.where(\n            samples < -0.999,\n            log_cdf_plus,\n            torch.where(samples > 0.99, log_one_minus_cdf_min, log_prob_mid_safe),\n        )  # B, 3, H, W\n\n        return log_probs\n\n    def sample(self):\n        u = torch.empty(self.means.size(), device=self.means.device).uniform_(\n            1e-5, 1.0 - 1e-5\n        )  # B, 3, H, W\n        x = self.means + torch.exp(self.log_scales) * (\n            torch.log(u) - torch.log(1.0 - u)\n        )  # B, 3, H, W\n        x = torch.clamp(x, -1, 1.0)\n        x = x / 2.0 + 0.5\n        return x", "\n\nclass DiscMixLogistic:\n    def __init__(self, param, num_mix=10, num_bits=8):\n        B, C, H, W = param.size()\n        self.num_mix = num_mix\n        self.logit_probs = param[:, :num_mix, :, :]  # B, M, H, W\n        l = param[:, num_mix:, :, :].view(B, 3, 3 * num_mix, H, W)  # B, 3, 3 * M, H, W\n        self.means = l[:, :, :num_mix, :, :]  # B, 3, M, H, W\n        self.log_scales = torch.clamp(\n            l[:, :, num_mix : 2 * num_mix, :, :], min=-7.0\n        )  # B, 3, M, H, W\n        self.coeffs = torch.tanh(l[:, :, 2 * num_mix : 3 * num_mix, :, :])  # B, 3, M, H, W\n        self.max_val = 2.0**num_bits - 1\n\n    def one_hot(self, indices, depth, dim):\n        indices = indices.unsqueeze(dim)\n        size = list(indices.size())\n        size[dim] = depth\n        y_onehot = torch.zeros(size).to(indices.device)\n        y_onehot.scatter_(dim, indices, 1)\n\n        return y_onehot\n\n    def log_prob(self, samples):\n        assert torch.max(samples) <= 1.0 and torch.min(samples) >= 0.0\n        # convert samples to be in [-1, 1]\n        samples = 2 * samples - 1.0\n\n        B, C, H, W = samples.size()\n        assert C == 3, \"only RGB images are considered.\"\n\n        # samples = samples.unsqueeze(4)  # B, 3, H , W\n        # samples = samples.expand(-1, -1, -1, -1, self.num_mix).permute(0, 1, 4, 2, 3)  # B, 3, M, H, W\n        samples = samples.unsqueeze(2).expand(-1, -1, self.num_mix, -1, -1)  # B, 3, M, H, W\n        mean1 = self.means[:, 0, :, :, :]  # B, M, H, W\n        mean2 = (\n            self.means[:, 1, :, :, :] + self.coeffs[:, 0, :, :, :] * samples[:, 0, :, :, :]\n        )  # B, M, H, W\n        mean3 = (\n            self.means[:, 2, :, :, :]\n            + self.coeffs[:, 1, :, :, :] * samples[:, 0, :, :, :]\n            + self.coeffs[:, 2, :, :, :] * samples[:, 1, :, :, :]\n        )  # B, M, H, W\n\n        # mean1 = mean1.unsqueeze(1)  # B, 1, M, H, W\n        # mean2 = mean2.unsqueeze(1)  # B, 1, M, H, W\n        # mean3 = mean3.unsqueeze(1)  # B, 1, M, H, W\n        means = torch.stack([mean1, mean2, mean3], dim=1)  # B, 3, M, H, W\n        centered = samples - means  # B, 3, M, H, W\n\n        inv_stdv = torch.exp(-self.log_scales)\n\n        plus_in = inv_stdv * (centered + 1.0 / self.max_val)\n        cdf_plus = torch.sigmoid(plus_in)\n\n        min_in = inv_stdv * (centered - 1.0 / self.max_val)\n        cdf_min = torch.sigmoid(min_in)\n\n        log_cdf_plus = plus_in - F.softplus(plus_in)\n        log_one_minus_cdf_min = -F.softplus(min_in)\n\n        cdf_delta = cdf_plus - cdf_min\n\n        mid_in = inv_stdv * centered\n        log_pdf_mid = mid_in - self.log_scales - 2.0 * F.softplus(mid_in)\n\n        log_prob_mid_safe = torch.where(\n            cdf_delta > 1e-5,\n            torch.log(torch.clamp(cdf_delta, min=1e-10)),\n            log_pdf_mid - np.log(self.max_val / 2),\n        )\n        # the original implementation uses samples > 0.999, this ignores the largest possible pixel value (255)\n        # which is mapped to 0.9922\n        log_probs = torch.where(\n            samples < -0.999,\n            log_cdf_plus,\n            torch.where(samples > 0.99, log_one_minus_cdf_min, log_prob_mid_safe),\n        )  # B, 3, M, H, W\n\n        log_probs = torch.sum(log_probs, 1) + F.log_softmax(self.logit_probs, dim=1)  # B, M, H, W\n        return torch.logsumexp(log_probs, dim=1, keepdim=True)  # B, 1, H, W\n\n    def sample(self, t=1.0):\n        # Form a (incomplete) sample from relaxed one-hot categorical and then select the highest value.\n        # This basically skips a couple of steps of sampling for speed (and memory)\n        gumbel = -torch.log(\n            -torch.log(torch.empty_like(self.logit_probs).uniform_(1e-5, 1.0 - 1e-5))\n        )  # B, M, H, W\n        sel = self.one_hot(\n            torch.argmax(self.logit_probs + gumbel, 1), self.num_mix, dim=1\n        )  # B, M, H, W\n        sel = sel.unsqueeze(1)  # B, 1, M, H, W\n\n        # select logistic parameters\n        means = torch.sum(self.means * sel, dim=2)  # B, 3, H, W\n        log_scales = torch.sum(self.log_scales * sel, dim=2)  # B, 3, H, W\n        coeffs = torch.sum(self.coeffs * sel, dim=2)  # B, 3, H, W\n\n        # cells from logistic & clip to interval\n        # we don't actually round to the nearest 8bit value when sampling\n        u = torch.empty_like(means).uniform_(1e-5, 1.0 - 1e-5)  # B, 3, H, W\n        x = means + torch.exp(log_scales) / t * (torch.log(u) - torch.log(1.0 - u))  # B, 3, H, W\n\n        x0 = torch.clamp(x[:, 0, :, :], -1, 1.0)  # B, H, W\n        x1 = torch.clamp(x[:, 1, :, :] + coeffs[:, 0, :, :] * x0, -1, 1)  # B, H, W\n        x2 = torch.clamp(\n            x[:, 2, :, :] + coeffs[:, 1, :, :] * x0 + coeffs[:, 2, :, :] * x1, -1, 1\n        )  # B, H, W\n\n        x0 = x0.unsqueeze(1)\n        x1 = x1.unsqueeze(1)\n        x2 = x2.unsqueeze(1)\n\n        x = torch.cat([x0, x1, x2], 1)\n        x = x / 2.0 + 0.5\n        return x\n\n    def rsample(self, t=1.0):\n        # form a sample from\n        gumbel = -torch.log(\n            -torch.log(torch.empty_like(self.logit_probs).uniform_(1e-5, 1.0 - 1e-5))\n        )  # B, M, H, W\n        sel = (self.logit_probs + gumbel / t).softmax(dim=1)\n        sel = sel.unsqueeze(1).clamp(min=1e-19)  # B, 1, M, H, W\n\n        # select logistic parameters\n        means = torch.sum(self.means * sel, dim=2)  # B, 3, H, W\n        log_scales = (\n            torch.logsumexp(2 * self.log_scales + 2 * sel.log(), dim=2) * 0.5\n        )  # B, 3, H, W\n        coeffs = torch.sum(self.coeffs * sel, dim=2)  # B, 3, H, W\n\n        # cells from logistic & clip to interval\n        # we don't actually round to the nearest 8bit value when sampling\n        u = torch.empty_like(means).uniform_(1e-5, 1.0 - 1e-5)  # B, 3, H, W\n        x = means + torch.exp(log_scales) / t * (torch.log(u) - torch.log(1.0 - u))  # B, 3, H, W\n\n        x0 = torch.clamp(x[:, 0, :, :], -1, 1.0)  # B, H, W\n        x1 = torch.clamp(x[:, 1, :, :] + coeffs[:, 0, :, :] * x0, -1, 1)  # B, H, W\n        x2 = torch.clamp(\n            x[:, 2, :, :] + coeffs[:, 1, :, :] * x0 + coeffs[:, 2, :, :] * x1, -1, 1\n        )  # B, H, W\n\n        x0 = x0.unsqueeze(1)\n        x1 = x1.unsqueeze(1)\n        x2 = x2.unsqueeze(1)\n\n        x = torch.cat([x0, x1, x2], 1)\n        x = x / 2.0 + 0.5\n        return x\n\n    @property\n    def mean(self):\n        probs = self.logit_probs.softmax(dim=1).unsqueeze(1)\n        means = torch.sum(self.means * probs, dim=2)\n        coeffs = torch.sum(self.coeffs * probs, dim=2)\n        x0 = means[:, 0].clamp(-1, 1.0)\n        x1 = (means[:, 1] + coeffs[:, 0] * x0).clamp(-1, 1.0)\n        x2 = (means[:, 2] + coeffs[:, 1] + x0 + coeffs[:, 2] * x1).clamp(-1, 1.0)\n        img = torch.stack([x0, x1, x2]).transpose(0, 1) / 2.0 + 0.5\n        return img\n\n    @staticmethod\n    def apply_mask(logits, mask, num_mix=10):\n        B, *other, H, W = logits.shape\n        lps = logits[..., :num_mix, :, :] + mask.clamp(min=1e-5).log()\n        l = logits[..., num_mix:, :, :]\n        l = l.view(B, *other[:-1], 3, 3 * num_mix, H, W)\n\n        means = l[..., :, :num_mix, :, :]\n        log_scales = l[..., :, num_mix : 2 * num_mix, :, :]\n        coeffs = l[..., :, 2 * num_mix : 3 * num_mix, :, :]\n\n        means = (means + 1) * mask.unsqueeze(-4) - 1  # push towards -1 rather than 0\n        log_scales = log_scales + mask.unsqueeze(-4).clamp(min=1e-5).log()\n        coeffs = coeffs * mask.unsqueeze(-4)\n\n        l = torch.cat([means, log_scales, coeffs], -4)\n        return torch.cat([lps, l.view(B, *other[:-1], -1, H, W)], -3)\n\n    @staticmethod\n    def sum(logits, dim=1, num_mix=10):\n        raise NotImplementedError()\n        B, *other, H, W = logits.shape\n        lps = logits[..., :num_mix, :, :]\n        lps = lps.sum(dim=dim)\n\n        l = logits[..., num_mix:, :, :]\n        l = l.view(B, *other[:-1], 3, 3 * num_mix, H, W)\n\n        means = l[..., :, :num_mix, :, :]\n        log_scales = l[..., :, num_mix : 2 * num_mix, :, :]\n        coeffs = l[..., :, 2 * num_mix : 3 * num_mix, :, :]\n\n        means = means.sum(dim=dim)\n        log_scales = torch.logsumexp(2 * log_scales, dim=dim) * 0.5\n        # This is how much channels add... cannot be a sum, but a mean is only a guess\n        # This this in after tanh\n        # tanh_coeffs = coeffs.tanh().mean(dim=dim).clamp(-1.+1e-5, 1.-1e-5)\n        # coeffs = .5 * ((1+tanh_coeffs).log() - (1-tanh_coeffs).log())\n        coeffs = coeffs.sum(dim=dim)\n        other = list(other)\n        del other[dim]\n        l = torch.cat([means, log_scales, coeffs], -4)\n        return torch.cat([lps, l.view(B, *other[:-1], -1, H, W)], -3)", ""]}
