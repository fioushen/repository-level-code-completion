{"filename": "preprocessing/video_feature.py", "chunked_list": ["from utils import open_file, time_to_seconds, extract_frames\nimport os\nimport glob\nimport torch\nimport torchvision.transforms as transforms\nimport clip\nimport PIL\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader", "import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport json\n\ntorch.set_num_threads(2)\n\nlist_of_annotations = glob.glob('../../jielin/msmo/annotation/*/*/*')\n\n# Load the CLIP model", "\n# Load the CLIP model\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device=device)\n\n# Modify the model to output features of size 2048\nmodel.visual.output_dim = 2048\n\n# Define the transform to preprocess the input frames\ntransform = transforms.Compose([", "# Define the transform to preprocess the input frames\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    preprocess\n])\n\nclass VideoFramesDataset(Dataset):\n    def __init__(self, frames, transform):\n        self.frames = frames\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.frames)\n\n    def __getitem__(self, idx):\n        frame = self.frames[idx]\n        preprocessed_frame = self.transform(Image.fromarray(frame))\n        return preprocessed_frame", "\nlinear = torch.nn.Linear(512, 2048, dtype=torch.float16).to(device)\nmodel.to(device)\n\nsave_np_dic = {}\n\nbatch_size = 128\n\ncorrupted_videos = []\n", "corrupted_videos = []\n\ncount = 0\n\nfor annotation in tqdm(list_of_annotations, desc = 'Extracting features: '):\n    \n    json_file = open_file(annotation)\n    id = json_file['info']['video_id']\n    keyframes = json_file['summary']\n    \n    start_time_seconds = 0\n    end_time_seconds = time_to_seconds(json_file['info']['duration'])\n\n    path_to_video = f\"../../jielin/msmo/video/{json_file['info']['category']}/{json_file['info']['sub_category']}/{json_file['info']['video_id']}.mp4\"\n    frames = extract_frames(path_to_video, start_time_seconds, end_time_seconds, 100)\n    # print(len(frames))\n    if len(frames) == 0:\n        corrupted_videos.append(path_to_video)\n        print('Corrupted ... ')\n        pass\n    else:\n\n        dataset = VideoFramesDataset(frames, transform)\n\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n        features_list = []\n\n        with torch.no_grad():\n            for batch in dataloader:\n                batch = batch.to(device)\n                features = model.encode_image(batch)\n                features = linear(features.to(device))\n                features_list.append(features.cpu())\n\n        features = torch.cat(features_list, dim=0)\n        save_np_dic[f'{id}'] = features.numpy()\n\n    # count +=1 \n    \n    # if count == 50:\n    #     break\n    print(save_np_dic)", "# The features tensor has shape [num_frames, feature_size]\nwith open('corrupted_videos.json', 'w') as f:\n    json.dump(corrupted_videos, f)\n\nnp.save('msmo_clip_features.npy', save_np_dic)"]}
{"filename": "preprocessing/keyframe_feature.py", "chunked_list": ["from utils import open_file, time_to_seconds, extract_frames\nimport os\nimport glob\nimport torch\nimport torchvision.transforms as transforms\nimport clip\nimport PIL\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader", "import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport json\n\ntorch.set_num_threads(2)\n\nlist_of_keyframes = glob.glob('../../jielin/msmo/keyframe/*/*/*')\n\n# Load the CLIP model", "\n# Load the CLIP model\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device=device)\n\n# Modify the model to output features of size 2048\nmodel.visual.output_dim = 2048\n\n# Define the transform to preprocess the input frames\ntransform = transforms.Compose([", "# Define the transform to preprocess the input frames\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    preprocess\n])\n\nclass VideoFramesDataset(Dataset):\n    def __init__(self, frames, transform):\n        self.frames = frames\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.frames)\n\n    def __getitem__(self, idx):\n        frame = self.frames[idx]\n        preprocessed_frame = self.transform(Image.fromarray(frame))\n        return preprocessed_frame", "\nlinear = torch.nn.Linear(512, 2048, dtype=torch.float16).to(device)\nmodel.to(device)\n\nsave_np_dic = {}\n\nbatch_size = 128\n\ncorrupted_videos = []\n", "corrupted_videos = []\n\ncount = 0\n\nfor path in tqdm(list_of_keyframes, desc = 'Extracting features ...'):\n    id = path.split('/')[-1]\n    image_data = []\n\n    for image in os.listdir(path):\n        image = Image.open(os.path.join(path, image))\n        image_array = np.array(image)\n        image_data.append(image_array)\n\n    stacked_array = np.stack(image_data, axis = 0)\n\n    dataset = VideoFramesDataset(stacked_array, transform)\n    dataloader = DataLoader(dataset, batch_size=128, shuffle=False)\n\n    features_list = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = batch.to(device)\n            features = model.encode_image(batch)\n            features = linear(features.to(device))\n            features_list.append(features.cpu())\n\n    features = torch.cat(features_list, dim=0)\n    save_np_dic[f'{id}'] = features.numpy()", "\n    # count +=1 \n    \n    # if count == 50:\n    #     break\n    # print(save_np_dic)\n\n# The features tensor has shape [num_frames, feature_size]\nnp.save('msmo_clip_summ_features.npy', save_np_dic)", "np.save('msmo_clip_summ_features.npy', save_np_dic)"]}
{"filename": "preprocessing/gpu.py", "chunked_list": ["import torch\n\n#If CUDA is available, prints True. Else, False.\nprint(torch.cuda.is_available())\n\n# If CUDA is availalbe, prints integer of CUDA devices that are available.\nprint(torch.cuda.device_count())\n\n# If CUDA is available, prints the ID of the current device that is ready for usage.\nprint(torch.cuda.current_device())", "# If CUDA is available, prints the ID of the current device that is ready for usage.\nprint(torch.cuda.current_device())\nID = torch.cuda.current_device()\n\n# If CUDA is available, instantiate device you will be using.\ndevice = torch.device(f'cuda:{ID}')\nprint(device)\n\n# If CUDA is available, get the name of the device.\nprint(torch.cuda.get_device_name(ID))", "# If CUDA is available, get the name of the device.\nprint(torch.cuda.get_device_name(ID))\n\n# If CUDA is available, move or create tensor using device\n# Move\nx = torch.tensor(1).to(device)\nprint(x)\n\n# Create\ny = torch.tensor(1, device = device)", "# Create\ny = torch.tensor(1, device = device)\nprint(y)\n\n# If x or y printed \"tensor(1, device='cuda:0')\" you should be good to go!"]}
{"filename": "preprocessing/utils.py", "chunked_list": ["import os\nimport json\nimport openai\nimport tiktoken\nimport time\nimport backoff\nimport random\nimport h5py\nimport numpy as np\nimport cv2", "import numpy as np\nimport cv2\nfrom multiprocessing import Pool\nfrom concurrent.futures import ThreadPoolExecutor\n\n\ndef check_video_file(video_file):\n    # Check if file exists\n    if not os.path.isfile(video_file):\n        print(\"File does not exist\")\n        return False\n\n    # Check if file is a valid video file\n    try:\n        cap = cv2.VideoCapture(video_file)\n        if not cap.isOpened():\n            print(\"Could not open video file\")\n            return False\n        else:\n            print(\"Video file opened successfully\")\n\n        # Check the encoding format of the video file\n        fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n        encoding_format = chr(fourcc & 0xFF) + chr((fourcc & 0xFF00) >> 8) + chr((fourcc & 0xFF0000) >> 16) + chr((fourcc & 0xFF000000) >> 24)\n        print(\"Video file encoding format:\", encoding_format)\n\n        # Check for the required codecs\n        codec = cv2.VideoWriter_fourcc(*'XVID')\n        if not cv2.VideoWriter_fourcc(*encoding_format) == codec:\n            print(\"Required codec not found\")\n            return False\n\n        # Check if the file is corrupted or incomplete\n        ret, frame = cap.read()\n        if not ret:\n            print(\"Video file is corrupted or incomplete\")\n            return False\n\n        # Release the video capture object\n        cap.release()\n    except:\n        print(\"An error occurred while checking the video file\")\n        return False\n\n    return True", "\ndef extract_frame(cap, frame_num):\n    # Set the frame position to the given frame number\n    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\n    # Grab the next frame\n    success = cap.grab()\n    if not success:\n        return None\n\n    # Decode and return the grabbed frame\n    _, frame = cap.retrieve()\n    return frame", "\n\ndef extract_frames(video_file, start_time, end_time, num_frames):\n    # Open the video file\n    cap = cv2.VideoCapture(video_file)\n\n    # Check if the video file was opened successfully\n    if not cap.isOpened():\n        print(f\"Could not open video file {video_file}\")\n        return []\n\n    # Get the frame rate and total number of frames in the video\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate the start and end frame numbers based on the given start and end times\n    start_frame = int(start_time * fps)\n    end_frame = int(end_time * fps)\n\n    # Calculate the frame numbers for the key frames\n    frame_nums = np.linspace(start_frame, end_frame, num_frames, dtype=np.int32)\n    # Extract the key frames sequentially\n    frames = []\n    # frames2 = []\n    for frame_num in frame_nums:\n        # Set the frame position to the given frame number\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\n        # Read the current frame\n        ret, frame = cap.read()\n        if not ret:\n            continue\n        \n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Append the frame to the list of frames\n        frames.append(frame_rgb)\n        # frames2.append(frame_rgb.tolist())\n    # Release the video capture object\n    cap.release()\n\n    return frames#, np.array(frames2)", "\n\n\ndef time_to_seconds(time_string):\n    hours, minutes, seconds = map(int, time_string.split(\":\"))\n    total_seconds = hours * 3600 + minutes * 60 + seconds\n    return total_seconds\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.encoding_for_model(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens", "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.encoding_for_model(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\n\n\ndef open_file(path_to_file):\n    file_extension = os.path.splitext(path_to_file)[1]\n    file_reader = {\n        '.json': lambda: json.load(open(path_to_file)),\n        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n        '.h5': lambda: h5py.File(path_to_file, 'r')\n    }\n    return file_reader.get(file_extension, lambda: None)()", "def open_file(path_to_file):\n    file_extension = os.path.splitext(path_to_file)[1]\n    file_reader = {\n        '.json': lambda: json.load(open(path_to_file)),\n        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n        '.h5': lambda: h5py.File(path_to_file, 'r')\n    }\n    return file_reader.get(file_extension, lambda: None)()\n", "\n\n\n\ndef retry_with_exponential_backoff(\n    func,\n    initial_delay: float = 1,\n    exponential_base: float = 2,\n    jitter: bool = True,\n    max_retries: int = 10,\n    errors: tuple = (openai.error.RateLimitError,openai.error.APIError),\n):\n    def wrapper(*args, **kwargs):\n        num_retries = 0\n        delay = initial_delay\n\n        while True:\n            try:\n                return func(*args, **kwargs)\n\n            except errors as e:\n                num_retries += 1\n\n                if num_retries > max_retries:\n                    raise Exception(\n                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n                    )\n\n                delay =1\n                time.sleep(delay)\n\n            except Exception as e:\n                raise e\n\n    return wrapper", "\n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):\n    return openai.ChatCompletion.create(**kwargs)"]}
{"filename": "preprocessing/video_feature_multisum.py", "chunked_list": ["from utils import open_file, time_to_seconds, extract_frames\nimport os\nimport glob\nimport torch\nimport torchvision.transforms as transforms\nimport clip\nimport PIL\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader", "import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport json\n\ntorch.set_num_threads(4)\n\nlist_of_annotations = glob.glob('../../jielin/msmo/annotation/*/*/*')\n\n# Load the CLIP model", "\n# Load the CLIP model\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device=device)\n\n# Modify the model to output features of size 2048\nmodel.visual.output_dim = 2048\n\n# Define the transform to preprocess the input frames\ntransform = transforms.Compose([", "# Define the transform to preprocess the input frames\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    preprocess\n])\n\n\nclass VideoFramesDataset(Dataset):\n    def __init__(self, frames, transform):\n        self.frames = frames\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.frames)\n\n    def __getitem__(self, idx):\n        frame = self.frames[idx]\n        preprocessed_frame = self.transform(Image.fromarray(frame))\n        return preprocessed_frame", "class VideoFramesDataset(Dataset):\n    def __init__(self, frames, transform):\n        self.frames = frames\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.frames)\n\n    def __getitem__(self, idx):\n        frame = self.frames[idx]\n        preprocessed_frame = self.transform(Image.fromarray(frame))\n        return preprocessed_frame", "\nlinear = torch.nn.Linear(512, 2048, dtype=torch.float16).to(device)\nmodel.to(device)\n\nsave_np_dic = {}\n\nbatch_size = 128\n\ncorrupted_videos = []\n", "corrupted_videos = []\n\ncount_1 = 0\n\n\nfor annotation in tqdm(list_of_annotations, desc = 'Extracting features: '):\n    \n    json_file = open_file(annotation)\n    id = json_file['info']['video_id']\n    keyframes = json_file['summary']\n    \n    start_time_seconds = 0\n    end_time_seconds = time_to_seconds(json_file['info']['duration'])\n\n    path_to_video = f\"../../jielin/msmo/video/{json_file['info']['category']}/{json_file['info']['sub_category']}/{json_file['info']['video_id']}.mp4\"\n    frames = extract_frames(path_to_video, start_time_seconds, end_time_seconds, 100)\n    frames = frames[:99]\n    # print(len(frames))\n    count = 0\n    if len(frames) == 0:\n        corrupted_videos.append(path_to_video)\n        print('Corrupted ... ')\n        pass\n    else:\n\n        dataset = VideoFramesDataset(frames, transform)\n\n        dataloader = DataLoader(dataset, batch_size=99, shuffle=False)\n\n        features_list = []\n        with torch.no_grad():\n            for batch in dataloader:\n                batch = batch.to(device)\n            \n                \n                # image.save(f'image_{count}_{count_1}.png')\n                features = model.encode_image(batch)\n                features = linear(features.to(device))\n                features_list.append(features.cpu())\n                count +=1\n        features = torch.cat(features_list, dim=0)\n        assert(features.shape[0] == len(frames))\n        np.save(f'MLASK/src/data/videos_frame/{id}.npy', features.numpy())\n        # print(frames)\n        np.save(f'MLASK/src/data/frames/{id}.npy', frames)\n        del frames\n        del features", "        # save_np_dic[f'{id}'] = features.numpy()\n\n    # count +=1 \n    \n    # if count == 50:\n    #     break\n# The features tensor has shape [num_frames, feature_size]\nwith open('corrupted_videos.json', 'w') as f:\n    json.dump(corrupted_videos, f)\n", "\n# np.save('msmo_clip_features.npy', save_np_dic)"]}
{"filename": "preprocessing/text_feature.py", "chunked_list": ["from utils import open_file, time_to_seconds\nimport os\nimport glob\nimport torch\nimport torchvision.transforms as transforms\nimport PIL\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm", "from torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport json\nfrom transformers import RobertaTokenizer, RobertaModel\n\ntorch.set_num_threads(2)\n\n# Load the pre-trained RoBERTa model and tokenizer\nmodel = RobertaModel.from_pretrained('roberta-base')\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')", "model = RobertaModel.from_pretrained('roberta-base')\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\nlist_of_annotations = glob.glob('../../jielin/msmo/annotation/*/*/*')\n\n# Load the CLIP model\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ntrain_dic = {}\ntest_dic = {}", "train_dic = {}\ntest_dic = {}\n\ncount = 0\n\nfor annotation in tqdm(list_of_annotations, desc = 'Extracting text features: '):\n    \n    json_file = open_file(annotation)\n    key = json_file['info']['video_id']\n    keyframes = json_file['summary']\n    summary_sequence = [item['summary'] for item in json_file['transcript']]\n\n    # Tokenize the summary sequence and convert to IDs\n    inputs = tokenizer(summary_sequence, padding=True, truncation=True, return_tensors='pt')\n    input_ids = inputs['input_ids']\n\n    # Pass the input IDs through the model to get the output embeddings\n    with torch.no_grad():\n        outputs = model(input_ids)\n        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n        if embeddings.ndimension() ==1:\n            embeddings = embeddings.unsqueeze(0).numpy()\n        else:\n            embeddings = embeddings.numpy()\n        print(embeddings.shape)\n    \n    assert embeddings.shape[0] == len(summary_sequence)\n\n    if key[-4:] == '0021' or key[-4:] == '0022' or key[-4:] == '0023' \\\n        or key[-4:] == '0024' or key[-4:] == '0025' or key[-4:] == '0026' \\\n        or key[-4:] == '0027' or key[-4:] == '0028' or key[-4:] == '0029':\n            \n        test_dic[f'{key}'] = embeddings\n    else:\n        train_dic[f'{key}'] = embeddings", "\n    # count +=1 \n    # if count == 50:\n    #     break\n\nnp.save('summary_embeddings_roberta_base_train.npy', train_dic)\nnp.save('summary_embeddings_roberta_base_test.npy', test_dic)"]}
{"filename": "preprocessing/seg_video_feature.py", "chunked_list": ["from utils import open_file, time_to_seconds, extract_frames\nimport os\nimport glob\nimport torch\nimport torchvision.transforms as transforms\nimport clip\nimport PIL\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader", "import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport json\n\ntorch.set_num_threads(2)\n\nlist_of_annotations = glob.glob('../../jielin/msmo/annotation/*/*/*')\n\n# Load the CLIP model", "\n# Load the CLIP model\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device=device)\n\n# Modify the model to output features of size 2048\nmodel.visual.output_dim = 2048\n\n# Define the transform to preprocess the input frames\ntransform = transforms.Compose([", "# Define the transform to preprocess the input frames\ntransform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.CenterCrop(224),\n    preprocess\n])\n\nclass VideoFramesDataset(Dataset):\n    def __init__(self, frames, transform):\n        self.frames = frames\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.frames)\n\n    def __getitem__(self, idx):\n        frame = self.frames[idx]\n        preprocessed_frame = self.transform(Image.fromarray(frame))\n        return preprocessed_frame", "\nlinear = torch.nn.Linear(512, 2048, dtype=torch.float16).to(device)\nmodel.to(device)\n\nsave_np_dic = {}\n\nbatch_size = 128\n\ncorrupted_videos = []\n", "corrupted_videos = []\n\ncount = 0\n\nfor annotation in tqdm(list_of_annotations, desc = 'Extracting features: '):\n    \n    json_file = open_file(annotation)\n    id = json_file['info']['video_id']\n    keyframes = json_file['summary']\n    \n    for seg in range(len(keyframes)):\n        start_time_seconds = time_to_seconds(keyframes[seg]['start_time'])\n        end_time_seconds = time_to_seconds(keyframes[seg]['end_time'])\n        # print(start_time_seconds)\n        # print(end_time_seconds)\n\n        path_to_video = f\"../../jielin/msmo/video/{json_file['info']['category']}/{json_file['info']['sub_category']}/{json_file['info']['video_id']}.mp4\"\n        frames = extract_frames(path_to_video, start_time_seconds, end_time_seconds, 100)\n        \n    # print(len(frames))\n        if len(frames) == 0:\n            corrupted_videos.append(path_to_video)\n            print('Corrupted ... ')\n            count +=1\n            pass\n        else:\n\n            dataset = VideoFramesDataset(frames, transform)\n\n            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n            features_list = []\n\n            with torch.no_grad():\n                for batch in dataloader:\n                    batch = batch.to(device)\n                    features = model.encode_image(batch)\n                    features = linear(features.to(device))\n                    features_list.append(features.cpu())\n\n            features = torch.cat(features_list, dim=0)\n            # print(features.shape)\n            np.save(f'MLASK/src/data/videos3/{id}_{count}.npy', features.numpy())\n            count +=1", "            # break\n        \n    # break\n            # save_np_dic[f'{id}'] = features.numpy()\n\n        # count +=1 \n        \n        # if count == 50:\n        #     break\n        # print(save_np_dic)", "        #     break\n        # print(save_np_dic)\n# # The features tensor has shape [num_frames, feature_size]\nwith open('corrupted_videos_2.json', 'w') as f:\n    json.dump(corrupted_videos, f)\n\n# np.save('msmo_clip_features.npy', save_np_dic)"]}
{"filename": "MultiSum/src/__init__.py", "chunked_list": [""]}
{"filename": "MultiSum/src/data/__init__.py", "chunked_list": [""]}
{"filename": "MultiSum/src/data/utils.py", "chunked_list": ["import torch\n\nfrom typing import List, Union\nfrom torch import Tensor\nimport json\nimport os\nimport numpy as np\nimport h5py\n\ndef open_file(path_to_file):\n    file_extension = os.path.splitext(path_to_file)[1]\n    file_reader = {\n        '.json': lambda: json.load(open(path_to_file)),\n        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n        '.h5': lambda: h5py.File(path_to_file, 'r')\n    }\n    return file_reader.get(file_extension, lambda: None)()", "\ndef open_file(path_to_file):\n    file_extension = os.path.splitext(path_to_file)[1]\n    file_reader = {\n        '.json': lambda: json.load(open(path_to_file)),\n        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n        '.h5': lambda: h5py.File(path_to_file, 'r')\n    }\n    return file_reader.get(file_extension, lambda: None)()", "\ndef split_list(mylist: List, chunk_size: Union[int]):\n    \"\"\"\n    Splits list into list of lists of given size. The last chunk may be of different size.\n    \"\"\"\n    return [\n        mylist[offs : offs + chunk_size] for offs in range(0, len(mylist), chunk_size)\n    ]\n\n\ndef generate_square_subsequent_mask(sz: int) -> Tensor:\n    \"\"\"\n    Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n    Unmasked positions are filled with float(0.0).\n    \"\"\"\n    return torch.triu(torch.full((sz, sz), float(\"-inf\")), diagonal=1)", "\n\ndef generate_square_subsequent_mask(sz: int) -> Tensor:\n    \"\"\"\n    Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n    Unmasked positions are filled with float(0.0).\n    \"\"\"\n    return torch.triu(torch.full((sz, sz), float(\"-inf\")), diagonal=1)\n\n", "\n\n# Copied from transformers.models.encoder_decoder.modeling_encoder_decoder.shift_tokens_right\ndef shift_tokens_right(input_ids: torch.Tensor, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    if decoder_start_token_id is None:\n        raise ValueError(\n            \"Make sure to set the decoder_start_token_id attribute of the model's configuration.\"\n        )\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    return shifted_input_ids", ""]}
{"filename": "MultiSum/src/data/mlask_data.py", "chunked_list": ["from utils import open_file\nimport glob\nimport numpy as np\n\nlist_of_features = glob.glob('../../../A2Summ/data/MSMO/feature/*')\nlist_of_video_features = [list_of_features[1], list_of_features[3]]\nlist_of_keyframe_features = [list_of_features[2], list_of_features[5]]\nlist_of_features = list_of_video_features + list_of_keyframe_features\n\n\nfor path in list_of_features:\n    if path == '../../../A2Summ/data/MSMO/feature/msmo_clip_features_test.npy' or \\\n        path == '../../../A2Summ/data/MSMO/feature/msmo_clip_features_train.npy':\n        dic = open_file(path)\n        \n        for key in dic.keys():\n            arr = dic[f'{key}']\n            # np.save(f'./videos/{key}.npy', arr)\n\n    else:\n        dic = open_file(path)\n        for key in dic.keys():\n            arr = dic[f'{key}']", "\n\nfor path in list_of_features:\n    if path == '../../../A2Summ/data/MSMO/feature/msmo_clip_features_test.npy' or \\\n        path == '../../../A2Summ/data/MSMO/feature/msmo_clip_features_train.npy':\n        dic = open_file(path)\n        \n        for key in dic.keys():\n            arr = dic[f'{key}']\n            # np.save(f'./videos/{key}.npy', arr)\n\n    else:\n        dic = open_file(path)\n        for key in dic.keys():\n            arr = dic[f'{key}']", "            # np.save(f'./keyframes/{key}.npy', arr)\n\n    \n    \n            "]}
{"filename": "MultiSum/src/data/data_laoder.py", "chunked_list": ["import csv\nimport json\nimport os\nimport unicodedata\n\nimport pytorch_lightning as pl\nimport torch\nimport numpy as np\nimport pandas as pd\n", "import pandas as pd\n\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\n\nfrom utils import generate_square_subsequent_mask\n\n\nclass MMSDataset(Dataset):\n    \"\"\"\n    Dataloder used to process the MLASK dataset\n    \"\"\"\n\n    def __init__(self, args, mode):\n        self.args = args\n        assert mode in [\"dev\", \"test\", \"train\"]\n        self.mode = mode\n        self.ids = None\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n\n        self._read_articles()\n        self._read_videos()\n        self._read_images()\n        print(self.use_ig65m)\n        print(self.use_s3d_how100m)\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, idx):\n        _return_dict = {\n            \"src\": self.src[idx],\n            \"tgt\": self.tgt[idx],\n            \"_id\": self._ids[idx],\n        }\n\n        if self.use_ig65m:\n            _return_dict[\"video_features_ig65m\"] = np.load(self.videos[idx][\"ig65m\"])\n        if self.use_s3d_how100m:\n            _return_dict[\"video_features_s3d\"] = np.load(self.videos[idx][\"s3d\"])\n\n        if self.use_vit:\n            _return_dict[\"src_img_features_vit\"] = np.load(self.src_imgs[idx][\"vit\"])\n            _return_dict[\"tgt_img_features_vit\"] = np.load(self.tgt_imgs[idx][\"vit\"])\n        if self.use_effnet:\n            _return_dict[\"src_img_features_effnet\"] = np.load(\n                self.src_imgs[idx][\"effnet\"]\n            )\n            _return_dict[\"tgt_img_features_effnet\"] = np.load(\n                self.tgt_imgs[idx][\"effnet\"]\n            )\n\n        return _return_dict\n\n    def _read_videos(self):\n        \"\"\"\n        Reads the video features\n        \"\"\"\n        self.use_ig65m = self.args.video_ig65m_path is not None\n        self.use_s3d_how100m = self.args.video_s3d_path is not None\n\n        # At least one video feature must be used\n        assert self.use_ig65m or self.use_s3d_how100m\n        # This function should be called only after reading textual data\n        assert self.ids is not None\n\n        self.videos = []\n        self._ids = []\n        for _id in self.ids:\n            _video_paths = {}\n            _video_dir = _id\n            if self.use_ig65m:\n                _video_paths[\"ig65m\"] = os.path.join(\n                    self.args.video_ig65m_path, _video_dir + \".npy\"\n                )\n                \n            if self.use_s3d_how100m:\n                _video_paths[\"s3d\"] = os.path.join(\n                    self.args.video_s3d_path, _video_dir + \".npy\"\n                )\n            self.videos.append(_video_paths)\n            self._ids.append(str(_id))\n\n    def _read_images(self):\n        \"\"\"\n        Reads the image features\n        \"\"\"\n        self.use_vit = (\n            self.args.img_extract_vit_path is not None\n            and self.args.img_tgt_vit_path is not None\n        )\n        self.use_effnet = (\n            self.args.img_extract_eff_path is not None\n            and self.args.img_tgt_eff_path is not None\n        )\n\n        # At least one image feature must be used\n        assert self.use_vit or self.use_effnet\n        # This function should be called only after reading textual data\n        assert self.ids is not None\n\n        self.src_imgs = []\n        self.tgt_imgs = []\n        for _id in self.ids:\n            _src_img_paths = {}\n            _tgt_img_paths = {}\n            # All the data instances are stored in a simple tree-like structure\n            _img_dir = _id\n            if self.use_vit:\n                _src_img_paths[\"vit\"] = os.path.join(\n                    self.args.img_extract_vit_path, _img_dir + \".npy\"\n                )\n                _tgt_img_paths[\"vit\"] = os.path.join(\n                    self.args.img_tgt_vit_path, _img_dir + \".npy\"\n                )\n            if self.use_effnet:\n                _src_img_paths[\"effnet\"] = os.path.join(\n                    self.args.img_extract_eff_path, _img_dir + \".npy\"\n                )\n                _tgt_img_paths[\"effnet\"] = os.path.join(\n                    self.args.img_tgt_eff_path, _img_dir + \".npy\"\n                )\n            self.src_imgs.append(_src_img_paths)\n            self.tgt_imgs.append(_tgt_img_paths)\n\n    def _read_articles(self):\n        \"\"\"\n        Read textual documents\n        \"\"\"\n        path = self.args.articles_path\n        model_headline = self.args.model_headline\n\n        df = pd.read_csv(\n            os.path.join(path, f\"{self.mode}_mms_joined.tsv\"),\n            sep=\"\\t\",\n            quoting=csv.QUOTE_NONE,\n        )\n\n        df.columns = [\"id\", \"date\", \"headline\", \"article\", \"abstract\"]\n\n        self.ids = df.id.values\n        self.src = df.article.values\n        if model_headline:\n            self.tgt = df.headline.values\n        else:\n            self.tgt = df.abstract.values\n\n    def collate_fn(self, batch):\n        max_src_len = self.args.max_src_len\n        max_tgt_len = self.args.max_tgt_len\n        \n        # Source tokens\n        src_encoded = self.tokenizer(\n            [_item[\"src\"] for _item in batch],\n            padding=\"longest\",\n            truncation=True,\n            max_length=max_src_len,\n        )\n\n        src_ids = torch.tensor(src_encoded[\"input_ids\"])\n        src_mask = torch.tensor(src_encoded[\"attention_mask\"])\n\n        # Target tokens\n        tgt_encoded = self.tokenizer(\n            [_item[\"tgt\"] for _item in batch],\n            padding=\"longest\",\n            truncation=True,\n            max_length=max_tgt_len,\n        )\n\n        tgt_ids = torch.tensor(tgt_encoded[\"input_ids\"])\n        tgt_mask = torch.tensor(tgt_encoded[\"attention_mask\"])\n\n        _return_dict = {\n            \"src\": [_item[\"src\"] for _item in batch],\n            \"src_ids\": src_ids,\n            \"src_mask\": src_mask,\n            \"tgt\": [_item[\"tgt\"] for _item in batch],\n            \"tgt_ids\": tgt_ids,\n            \"tgt_mask\": tgt_mask,\n            \"_id\": [_item[\"_id\"] for _item in batch],\n        }\n\n        # Video features, maximal length is taken care of during feature extraction\n        if self.use_s3d_how100m:\n            video_features_s3d = np.zeros(\n                [\n                    len(batch),\n                    max([_item[\"video_features_s3d\"].shape[0] for _item in batch]),\n                    batch[0][\"video_features_s3d\"].shape[-1],\n                ]\n            )\n            video_mask_s3d = np.full(video_features_s3d.shape[:2], float(\"-inf\"))\n        if self.use_ig65m:\n            video_features_ig65m = np.zeros(\n                [\n                    len(batch),\n                    max([_item[\"video_features_ig65m\"].shape[0] for _item in batch]),\n                    batch[0][\"video_features_ig65m\"].shape[-1],\n                ]\n            )\n            video_mask_ig65m = np.full(video_features_ig65m.shape[:2], float(\"-inf\"))\n\n        for _iter, _item in enumerate(batch):\n            if self.use_s3d_how100m:\n                video_features_s3d[_iter][\n                    : _item[\"video_features_s3d\"].shape[0],\n                    : _item[\"video_features_s3d\"].shape[1],\n                ] = _item[\"video_features_s3d\"]\n                video_mask_s3d[_iter][: _item[\"video_features_s3d\"].shape[0]] = 0.0\n            if self.use_ig65m:\n                video_features_ig65m[_iter][\n                    : _item[\"video_features_ig65m\"].shape[0],\n                    : _item[\"video_features_ig65m\"].shape[1],\n                ] = _item[\"video_features_ig65m\"]\n                video_mask_ig65m[_iter][: _item[\"video_features_ig65m\"].shape[0]] = 0.0\n\n        if self.use_s3d_how100m and self.use_ig65m:\n            assert np.array_equal(video_mask_s3d, video_mask_ig65m)\n\n        if self.use_s3d_how100m:\n            _return_dict[\"video_features_s3d\"] = torch.tensor(\n                video_features_s3d\n            ).float()\n            _return_dict[\"video_mask\"] = torch.tensor(video_mask_s3d)\n        if self.use_ig65m:\n            _return_dict[\"video_features_ig65m\"] = torch.tensor(\n                video_features_ig65m\n            ).float()\n            _return_dict[\"video_mask\"] = torch.tensor(video_mask_ig65m)\n\n        # Image features extracted from source video - we do extract a single feature for every 1s of video, up to 300 features\n        if self.use_vit:\n            src_img_features_vit = np.zeros(\n                [\n                    len(batch),\n                    max([_item[\"src_img_features_vit\"].shape[0] for _item in batch]),\n                    batch[0][\"src_img_features_vit\"].shape[-1],\n                ]\n            )\n            src_img_mask_vit = np.full(src_img_features_vit.shape[:2], 1.0)\n            tgt_img_features_vit = np.stack(\n                [_item[\"tgt_img_features_vit\"][0] for _item in batch]\n            )\n        if self.use_effnet:\n            src_img_features_effnet = np.zeros(\n                [\n                    len(batch),\n                    max([_item[\"src_img_features_effnet\"].shape[0] for _item in batch]),\n                    batch[0][\"src_img_features_effnet\"].shape[-1],\n                ]\n            )\n            src_img_mask_effnet = np.full(src_img_features_effnet.shape[:2], 1.0)\n            tgt_img_features_effnet = np.stack(\n                [_item[\"tgt_img_features_effnet\"][0] for _item in batch]\n            )\n\n        for _iter, _item in enumerate(batch):\n            if self.use_vit:\n                src_img_features_vit[_iter][\n                    : _item[\"src_img_features_vit\"].shape[0],\n                    : _item[\"src_img_features_vit\"].shape[1],\n                ] = _item[\"src_img_features_vit\"]\n                src_img_mask_vit[_iter][: _item[\"src_img_features_vit\"].shape[0]] = 0.0\n            if self.use_effnet:\n                src_img_features_effnet[_iter][\n                    : _item[\"src_img_features_effnet\"].shape[0],\n                    : _item[\"src_img_features_effnet\"].shape[1],\n                ] = _item[\"src_img_features_effnet\"]\n                src_img_mask_effnet[_iter][\n                    : _item[\"src_img_features_effnet\"].shape[0]\n                ] = 0.0\n\n        if self.use_s3d_how100m and self.use_ig65m:\n            assert np.array_equal(src_img_mask_vit, src_img_mask_effnet)\n\n        if self.use_vit:\n            _return_dict[\"src_img_features_vit\"] = torch.tensor(\n                src_img_features_vit\n            ).float()\n            _return_dict[\"src_img_mask\"] = torch.tensor(src_img_mask_vit)\n            _return_dict[\"tgt_img_features_vit\"] = torch.tensor(\n                tgt_img_features_vit\n            ).float()\n        if self.use_effnet:\n            _return_dict[\"src_img_features_effnet\"] = torch.tensor(\n                src_img_features_effnet\n            ).float()\n            _return_dict[\"src_img_mask\"] = torch.tensor(src_img_mask_effnet)\n            _return_dict[\"tgt_img_features_effnet\"] = torch.tensor(\n                tgt_img_features_effnet\n            ).float()\n        return _return_dict", "\nclass MMSDataset(Dataset):\n    \"\"\"\n    Dataloder used to process the MLASK dataset\n    \"\"\"\n\n    def __init__(self, args, mode):\n        self.args = args\n        assert mode in [\"dev\", \"test\", \"train\"]\n        self.mode = mode\n        self.ids = None\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n\n        self._read_articles()\n        self._read_videos()\n        self._read_images()\n        print(self.use_ig65m)\n        print(self.use_s3d_how100m)\n    def __len__(self):\n        return len(self.src)\n\n    def __getitem__(self, idx):\n        _return_dict = {\n            \"src\": self.src[idx],\n            \"tgt\": self.tgt[idx],\n            \"_id\": self._ids[idx],\n        }\n\n        if self.use_ig65m:\n            _return_dict[\"video_features_ig65m\"] = np.load(self.videos[idx][\"ig65m\"])\n        if self.use_s3d_how100m:\n            _return_dict[\"video_features_s3d\"] = np.load(self.videos[idx][\"s3d\"])\n\n        if self.use_vit:\n            _return_dict[\"src_img_features_vit\"] = np.load(self.src_imgs[idx][\"vit\"])\n            _return_dict[\"tgt_img_features_vit\"] = np.load(self.tgt_imgs[idx][\"vit\"])\n        if self.use_effnet:\n            _return_dict[\"src_img_features_effnet\"] = np.load(\n                self.src_imgs[idx][\"effnet\"]\n            )\n            _return_dict[\"tgt_img_features_effnet\"] = np.load(\n                self.tgt_imgs[idx][\"effnet\"]\n            )\n\n        return _return_dict\n\n    def _read_videos(self):\n        \"\"\"\n        Reads the video features\n        \"\"\"\n        self.use_ig65m = self.args.video_ig65m_path is not None\n        self.use_s3d_how100m = self.args.video_s3d_path is not None\n\n        # At least one video feature must be used\n        assert self.use_ig65m or self.use_s3d_how100m\n        # This function should be called only after reading textual data\n        assert self.ids is not None\n\n        self.videos = []\n        self._ids = []\n        for _id in self.ids:\n            _video_paths = {}\n            _video_dir = _id\n            if self.use_ig65m:\n                _video_paths[\"ig65m\"] = os.path.join(\n                    self.args.video_ig65m_path, _video_dir + \".npy\"\n                )\n                \n            if self.use_s3d_how100m:\n                _video_paths[\"s3d\"] = os.path.join(\n                    self.args.video_s3d_path, _video_dir + \".npy\"\n                )\n            self.videos.append(_video_paths)\n            self._ids.append(str(_id))\n\n    def _read_images(self):\n        \"\"\"\n        Reads the image features\n        \"\"\"\n        self.use_vit = (\n            self.args.img_extract_vit_path is not None\n            and self.args.img_tgt_vit_path is not None\n        )\n        self.use_effnet = (\n            self.args.img_extract_eff_path is not None\n            and self.args.img_tgt_eff_path is not None\n        )\n\n        # At least one image feature must be used\n        assert self.use_vit or self.use_effnet\n        # This function should be called only after reading textual data\n        assert self.ids is not None\n\n        self.src_imgs = []\n        self.tgt_imgs = []\n        for _id in self.ids:\n            _src_img_paths = {}\n            _tgt_img_paths = {}\n            # All the data instances are stored in a simple tree-like structure\n            _img_dir = _id\n            if self.use_vit:\n                _src_img_paths[\"vit\"] = os.path.join(\n                    self.args.img_extract_vit_path, _img_dir + \".npy\"\n                )\n                _tgt_img_paths[\"vit\"] = os.path.join(\n                    self.args.img_tgt_vit_path, _img_dir + \".npy\"\n                )\n            if self.use_effnet:\n                _src_img_paths[\"effnet\"] = os.path.join(\n                    self.args.img_extract_eff_path, _img_dir + \".npy\"\n                )\n                _tgt_img_paths[\"effnet\"] = os.path.join(\n                    self.args.img_tgt_eff_path, _img_dir + \".npy\"\n                )\n            self.src_imgs.append(_src_img_paths)\n            self.tgt_imgs.append(_tgt_img_paths)\n\n    def _read_articles(self):\n        \"\"\"\n        Read textual documents\n        \"\"\"\n        path = self.args.articles_path\n        model_headline = self.args.model_headline\n\n        df = pd.read_csv(\n            os.path.join(path, f\"{self.mode}_mms_joined.tsv\"),\n            sep=\"\\t\",\n            quoting=csv.QUOTE_NONE,\n        )\n\n        df.columns = [\"id\", \"date\", \"headline\", \"article\", \"abstract\"]\n\n        self.ids = df.id.values\n        self.src = df.article.values\n        if model_headline:\n            self.tgt = df.headline.values\n        else:\n            self.tgt = df.abstract.values\n\n    def collate_fn(self, batch):\n        max_src_len = self.args.max_src_len\n        max_tgt_len = self.args.max_tgt_len\n        \n        # Source tokens\n        src_encoded = self.tokenizer(\n            [_item[\"src\"] for _item in batch],\n            padding=\"longest\",\n            truncation=True,\n            max_length=max_src_len,\n        )\n\n        src_ids = torch.tensor(src_encoded[\"input_ids\"])\n        src_mask = torch.tensor(src_encoded[\"attention_mask\"])\n\n        # Target tokens\n        tgt_encoded = self.tokenizer(\n            [_item[\"tgt\"] for _item in batch],\n            padding=\"longest\",\n            truncation=True,\n            max_length=max_tgt_len,\n        )\n\n        tgt_ids = torch.tensor(tgt_encoded[\"input_ids\"])\n        tgt_mask = torch.tensor(tgt_encoded[\"attention_mask\"])\n\n        _return_dict = {\n            \"src\": [_item[\"src\"] for _item in batch],\n            \"src_ids\": src_ids,\n            \"src_mask\": src_mask,\n            \"tgt\": [_item[\"tgt\"] for _item in batch],\n            \"tgt_ids\": tgt_ids,\n            \"tgt_mask\": tgt_mask,\n            \"_id\": [_item[\"_id\"] for _item in batch],\n        }\n\n        # Video features, maximal length is taken care of during feature extraction\n        if self.use_s3d_how100m:\n            video_features_s3d = np.zeros(\n                [\n                    len(batch),\n                    max([_item[\"video_features_s3d\"].shape[0] for _item in batch]),\n                    batch[0][\"video_features_s3d\"].shape[-1],\n                ]\n            )\n            video_mask_s3d = np.full(video_features_s3d.shape[:2], float(\"-inf\"))\n        if self.use_ig65m:\n            video_features_ig65m = np.zeros(\n                [\n                    len(batch),\n                    max([_item[\"video_features_ig65m\"].shape[0] for _item in batch]),\n                    batch[0][\"video_features_ig65m\"].shape[-1],\n                ]\n            )\n            video_mask_ig65m = np.full(video_features_ig65m.shape[:2], float(\"-inf\"))\n\n        for _iter, _item in enumerate(batch):\n            if self.use_s3d_how100m:\n                video_features_s3d[_iter][\n                    : _item[\"video_features_s3d\"].shape[0],\n                    : _item[\"video_features_s3d\"].shape[1],\n                ] = _item[\"video_features_s3d\"]\n                video_mask_s3d[_iter][: _item[\"video_features_s3d\"].shape[0]] = 0.0\n            if self.use_ig65m:\n                video_features_ig65m[_iter][\n                    : _item[\"video_features_ig65m\"].shape[0],\n                    : _item[\"video_features_ig65m\"].shape[1],\n                ] = _item[\"video_features_ig65m\"]\n                video_mask_ig65m[_iter][: _item[\"video_features_ig65m\"].shape[0]] = 0.0\n\n        if self.use_s3d_how100m and self.use_ig65m:\n            assert np.array_equal(video_mask_s3d, video_mask_ig65m)\n\n        if self.use_s3d_how100m:\n            _return_dict[\"video_features_s3d\"] = torch.tensor(\n                video_features_s3d\n            ).float()\n            _return_dict[\"video_mask\"] = torch.tensor(video_mask_s3d)\n        if self.use_ig65m:\n            _return_dict[\"video_features_ig65m\"] = torch.tensor(\n                video_features_ig65m\n            ).float()\n            _return_dict[\"video_mask\"] = torch.tensor(video_mask_ig65m)\n\n        # Image features extracted from source video - we do extract a single feature for every 1s of video, up to 300 features\n        if self.use_vit:\n            src_img_features_vit = np.zeros(\n                [\n                    len(batch),\n                    max([_item[\"src_img_features_vit\"].shape[0] for _item in batch]),\n                    batch[0][\"src_img_features_vit\"].shape[-1],\n                ]\n            )\n            src_img_mask_vit = np.full(src_img_features_vit.shape[:2], 1.0)\n            tgt_img_features_vit = np.stack(\n                [_item[\"tgt_img_features_vit\"][0] for _item in batch]\n            )\n        if self.use_effnet:\n            src_img_features_effnet = np.zeros(\n                [\n                    len(batch),\n                    max([_item[\"src_img_features_effnet\"].shape[0] for _item in batch]),\n                    batch[0][\"src_img_features_effnet\"].shape[-1],\n                ]\n            )\n            src_img_mask_effnet = np.full(src_img_features_effnet.shape[:2], 1.0)\n            tgt_img_features_effnet = np.stack(\n                [_item[\"tgt_img_features_effnet\"][0] for _item in batch]\n            )\n\n        for _iter, _item in enumerate(batch):\n            if self.use_vit:\n                src_img_features_vit[_iter][\n                    : _item[\"src_img_features_vit\"].shape[0],\n                    : _item[\"src_img_features_vit\"].shape[1],\n                ] = _item[\"src_img_features_vit\"]\n                src_img_mask_vit[_iter][: _item[\"src_img_features_vit\"].shape[0]] = 0.0\n            if self.use_effnet:\n                src_img_features_effnet[_iter][\n                    : _item[\"src_img_features_effnet\"].shape[0],\n                    : _item[\"src_img_features_effnet\"].shape[1],\n                ] = _item[\"src_img_features_effnet\"]\n                src_img_mask_effnet[_iter][\n                    : _item[\"src_img_features_effnet\"].shape[0]\n                ] = 0.0\n\n        if self.use_s3d_how100m and self.use_ig65m:\n            assert np.array_equal(src_img_mask_vit, src_img_mask_effnet)\n\n        if self.use_vit:\n            _return_dict[\"src_img_features_vit\"] = torch.tensor(\n                src_img_features_vit\n            ).float()\n            _return_dict[\"src_img_mask\"] = torch.tensor(src_img_mask_vit)\n            _return_dict[\"tgt_img_features_vit\"] = torch.tensor(\n                tgt_img_features_vit\n            ).float()\n        if self.use_effnet:\n            _return_dict[\"src_img_features_effnet\"] = torch.tensor(\n                src_img_features_effnet\n            ).float()\n            _return_dict[\"src_img_mask\"] = torch.tensor(src_img_mask_effnet)\n            _return_dict[\"tgt_img_features_effnet\"] = torch.tensor(\n                tgt_img_features_effnet\n            ).float()\n        return _return_dict", "\n\nclass MMSDataModule(pl.LightningDataModule):\n    def __init__(self, args):\n        super().__init__()\n        train_set = MMSDataset(args, \"train\")\n        val_set = MMSDataset(args, \"dev\")\n        test_set = MMSDataset(args, \"test\")\n        self.train_loader = DataLoader(\n            dataset=train_set,\n            batch_size=args.train_batch_size,\n            num_workers=args.num_workers,\n            shuffle=True,\n            collate_fn=train_set.collate_fn,\n        )\n        self.val_loader = DataLoader(\n            dataset=val_set,\n            batch_size=args.val_batch_size,\n            num_workers=args.num_workers,\n            shuffle=False,\n            collate_fn=val_set.collate_fn,\n        )\n        self.test_loader = DataLoader(\n            dataset=test_set,\n            batch_size=args.val_batch_size,\n            num_workers=args.num_workers,\n            shuffle=False,\n            collate_fn=test_set.collate_fn,\n        )\n\n    def train_dataloader(self):\n        return self.train_loader\n\n    def val_dataloader(self):\n        return self.val_loader\n\n    def test_dataloader(self):\n        return self.test_loader", ""]}
{"filename": "MultiSum/src/runtime/generate_thumbnail.py", "chunked_list": ["import numpy as np\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nimport glob\nfrom tqdm import tqdm\nimport random\n\nimport cv2", "\nimport cv2\ntorch.set_num_threads(2)\nimport os\nimport json\nimport numpy as np\nimport h5py\n\n\ndef search_path_by_id(id, path_list):\n    for path in path_list:\n        if id in path:\n            return path\n    return None ", "\ndef search_path_by_id(id, path_list):\n    for path in path_list:\n        if id in path:\n            return path\n    return None \n\n\n\ndef open_file(path_to_file):\n    file_extension = os.path.splitext(path_to_file)[1]\n    file_reader = {\n        '.json': lambda: json.load(open(path_to_file)),\n        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n        '.h5': lambda: h5py.File(path_to_file, 'r')\n    }\n    return file_reader.get(file_extension, lambda: None)()", "\ndef open_file(path_to_file):\n    file_extension = os.path.splitext(path_to_file)[1]\n    file_reader = {\n        '.json': lambda: json.load(open(path_to_file)),\n        '.txt': lambda: open(path_to_file, 'r').read().split('\\n'),\n        '.npy': lambda: np.load(path_to_file, allow_pickle=True).item(),\n        '.h5': lambda: h5py.File(path_to_file, 'r')\n    }\n    return file_reader.get(file_extension, lambda: None)()", "\ndef time_to_seconds(time_string):\n    hours, minutes, seconds = map(int, time_string.split(\":\"))\n    total_seconds = hours * 3600 + minutes * 60 + seconds\n    return total_seconds\n\n\ndef extract_frame(cap, frame_num):\n    # Set the frame position to the given frame number\n    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\n    # Grab the next frame\n    success = cap.grab()\n    if not success:\n        return None\n\n    # Decode and return the grabbed frame\n    _, frame = cap.retrieve()\n    return frame", "\n\ndef extract_frames(video_file, start_time, end_time, num_frames):\n    # Open the video file\n    cap = cv2.VideoCapture(video_file)\n\n    # Check if the video file was opened successfully\n    if not cap.isOpened():\n        print(f\"Could not open video file {video_file}\")\n        return []\n\n    # Get the frame rate and total number of frames in the video\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate the start and end frame numbers based on the given start and end times\n    start_frame = int(start_time * fps)\n    end_frame = int(end_time * fps)\n\n    # Calculate the frame numbers for the key frames\n    frame_nums = np.linspace(start_frame, end_frame, num_frames, dtype=np.int32)\n    # Extract the key frames sequentially\n    frames = []\n    # frames2 = []\n    for frame_num in frame_nums:\n        # Set the frame position to the given frame number\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n\n        # Read the current frame\n        ret, frame = cap.read()\n        if not ret:\n            continue\n        \n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Append the frame to the list of frames\n        frames.append(frame_rgb)\n        # frames2.append(frame_rgb.tolist())\n    # Release the video capture object\n    cap.release()\n\n    return frames#, np.array(frames2)", "\npath_to_videos = glob.glob('../../../../../jielin/msmo/video/*/*/*')\npath_to_annotations = glob.glob('../../../../../jielin/msmo/annotation/*/*/*')\ncount = 0\navailable_fonts =glob.glob('fonts/*')\nfor i in tqdm(range(54)):\n    \n    results_whole_decoder = np.load(f'results/results_whole_{i}.npy', allow_pickle=True).item()\n    sentences = results_whole_decoder['sentences']\n    references = results_whole_decoder['references']\n    selected_frames = results_whole_decoder['selected_frames']\n    ids = results_whole_decoder['ids']\n    count += len(ids)\n    \n    for j in range(len(sentences)):\n        \n        sentence = sentences[j]\n        reference = references[j]\n        selected_frame = selected_frames[j]\n        id = ids[j]\n        \n        video = search_path_by_id(id, path_to_videos)\n        annotation = search_path_by_id(id, path_to_annotations)\n        # print(annotation)\n        # print(sentence)\n        # print(reference)\n        # print(selected_frame)\n        # print(id)\n        # print(video)\n        \n        json_file = open_file(annotation)\n        id = json_file['info']['video_id']\n        keyframes = json_file['summary']\n        \n        start_time_seconds = 0\n        end_time_seconds = time_to_seconds(json_file['info']['duration'])\n\n\n        frames = extract_frames(video, start_time_seconds, end_time_seconds, 100)\n        # frames = frames[:99]\n        frame_indexed = frames[selected_frame]\n        \n        image = Image.fromarray(frame_indexed)\n        image.save(f'run2/outputs2/{id}.png')\n        \n        # Create a drawing object\n        draw = ImageDraw.Draw(image)\n        \n        # Select a random font and size\n        font_name = random.choice(available_fonts)\n        font_size = random.randint(25, 200)\n\n        # Specify the text content\n        text = sentence\n\n        # Get the font object with the randomly selected font and size\n        font = ImageFont.truetype(font_name, size=font_size)\n\n        # Get the size of the image\n        image_width, image_height = image.size\n\n        # Generate random position for the text\n        text_width, text_height = draw.textsize(text, font=font)\n        x = random.randint(0, abs(image_width - text_width))\n        y = random.randint(0, abs(image_height - text_height))\n\n        # Add the text to the image with random font, size, and position\n        draw.text((x, y), text, font=font, fill=(255, 255, 255))\n\n        # Save the modified image\n        image.save(f\"run2/outputs/{id}.png\")"]}
{"filename": "MultiSum/src/runtime/train_mms_model.py", "chunked_list": ["#!/usr/bin/env python\n\nimport pytorch_lightning as pl\nimport os\nimport sys\n\nsys.path.append(os.path.join(os.path.dirname(__file__), \"../data\"))\nsys.path.append(os.path.join(os.path.dirname(__file__), \"../model\"))\nimport os\n_data_base = '../'", "import os\n_data_base = '../'\nfrom model_mms import MultimodalTransformer\nfrom data_laoder import MMSDataset, MMSDataModule\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom transformers import AutoTokenizer\nimport argparse", "from transformers import AutoTokenizer\nimport argparse\nimport numpy as np\nimport torch\ntorch.set_num_threads(2)\n\n\nparser = argparse.ArgumentParser(description=\"MMS training parameters.\")\nparser.add_argument(\n    \"--start_with_text_frozen\",", "parser.add_argument(\n    \"--start_with_text_frozen\",\n    type=int,\n    default=0,\n    help=\"Number of epochs with text encoder/decoder frozen\",\n)\nparser.add_argument(\n    \"--mask_video_features\",\n    action=\"store_true\",\n    help=\"Whether to mask the video features during training/inference.\",", "    action=\"store_true\",\n    help=\"Whether to mask the video features during training/inference.\",\n)\nparser.add_argument(\n    \"--use_video_ig65m\",\n    # default = None,\n    action=\"store_true\",\n    help=\"Whether to use the video_ig65m features.\",\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--use_video_s3d\",\n    # default = None,\n    action=\"store_true\",\n    help=\"Whether to use the video_s3d features.\",\n)\nparser.add_argument(\n    \"--use_image_vit\",\n    # default = None,", "    \"--use_image_vit\",\n    # default = None,\n    action=\"store_true\",\n    help=\"Whether to use the image_vit features.\",\n)\nparser.add_argument(\n    \"--use_image_effnet\",\n    # default = None,\n    action=\"store_true\",\n    help=\"Whether to use the image_effnet features.\",", "    action=\"store_true\",\n    help=\"Whether to use the image_effnet features.\",\n)\nparser.add_argument(\n    \"--smooth_cos_labels\",\n    # default = None,\n    action=\"store_true\",\n    help=\"Whether to use the smooothed targets to train as opposed to a single closest target.\",\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--use_pretrained_summarizer\",\n    action=\"store_true\",\n    help=\"Whether to use the model pre-trained on text summarization.\",\n)\n\nparser.add_argument(\n    \"--version\",\n    type=int,", "    \"--version\",\n    type=int,\n    default=2,\n    help=\"Manual versioning, to be able to compute variance for several runs.\",\n)\n\nparser.add_argument(\n    \"--env\",\n    type=str,\n    default='whole',", "    type=str,\n    default='whole',\n    help=\"Please choose text parsing setting; either whole or segment\",\n)\n\nmms_args = parser.parse_args()\n\ntraining_name = (\n    f\"version={mms_args.version}_ep_txt_fr={mms_args.start_with_text_frozen}\"\n)", "    f\"version={mms_args.version}_ep_txt_fr={mms_args.start_with_text_frozen}\"\n)\n\nif mms_args.use_video_ig65m:\n    training_name += \"_v=ig65m\"\nif mms_args.use_video_s3d:\n    training_name += \"_v=s3d\"\nif mms_args.use_image_vit:\n    training_name += \"_i=vit\"\nif mms_args.use_image_effnet:\n    training_name += \"_i=eff\"", "if mms_args.use_image_effnet:\n    training_name += \"_i=eff\"\nif mms_args.smooth_cos_labels:\n    training_name += \"_smooth\"\nif mms_args.use_pretrained_summarizer:\n    training_name += \"_pretrain\"\nif mms_args.mask_video_features:\n    training_name += \"v_masked\"\n\n", "\n\nROUGE_RAW_L_checkpoint = ModelCheckpoint(\n    filename=\"{epoch}-{step}-{ROUGE_RAW_L_F:.2f}\",\n    monitor=\"ROUGE_RAW_L_F\",\n    # monitor = 'BLEU',\n    mode=\"max\",\n    save_top_k=1,\n)\n", ")\n\nROUGE_RAW_L_stop = EarlyStopping(monitor=\"ROUGE_RAW_L_F\", mode=\"max\", patience=5)\n\n# ROUGE_RAW_L_stop = EarlyStopping(monitor=\"BLEU\", mode=\"max\", patience=5)\n\n# Section 6.3 in MLASK paper\nsummeCzech_ckpt = \"__PATH_TO_mT5_FINE-TUNED_ON_SumeCzech_DATASET__\"\n\n", "\n\nmms_data = MMSDataModule(\n    argparse.Namespace(\n        articles_path=f\"{_data_base}/data/\",\n        video_ig65m_path=f\"{_data_base}/data/videos\",\n        video_s3d_path = None,\n        img_extract_vit_path=f\"{_data_base}/data/keyframes\",\n        img_tgt_vit_path=f\"{_data_base}/data/thumbnails\",\n        img_extract_eff_path = None,", "        img_tgt_vit_path=f\"{_data_base}/data/thumbnails\",\n        img_extract_eff_path = None,\n        img_tgt_eff_path = None,\n        model_headline=False,\n        max_src_len=1536,\n        max_tgt_len=256,\n        train_batch_size=2,\n        val_batch_size=16,\n        num_workers=16,\n    )", "        num_workers=16,\n    )\n)\n\ntrain_loader = mms_data.train_dataloader()\nval_loader = mms_data.val_dataloader()\n\ntb_logger = TensorBoardLogger(\"trainings\", name=\"mms_novinky_tb\", version=training_name)\ntrainer = pl.Trainer(\n    max_epochs=50,", "trainer = pl.Trainer(\n    max_epochs=50,\n    gpus=1,\n    logger=tb_logger,\n    log_every_n_steps=50,\n    val_check_interval=1.0,\n    gradient_clip_val=5,\n    accumulate_grad_batches=16,\n    callbacks=[ROUGE_RAW_L_checkpoint, ROUGE_RAW_L_stop],\n)", "    callbacks=[ROUGE_RAW_L_checkpoint, ROUGE_RAW_L_stop],\n)\n\nmodel = MultimodalTransformer(\n    num_video_enc_layers=4,\n    use_video_ig65m=mms_args.use_video_ig65m,\n    use_video_s3d=mms_args.use_video_s3d,\n    use_image_vit=mms_args.use_image_vit,\n    use_image_effnet=mms_args.use_image_effnet,\n    smooth_cos_labels=mms_args.smooth_cos_labels,", "    use_image_effnet=mms_args.use_image_effnet,\n    smooth_cos_labels=mms_args.smooth_cos_labels,\n    lr_max_val=0.0005,\n    lr_init_val=0,\n    lr_warmup_steps=8000,\n    pre_trained_summeczech_ckpt=summeCzech_ckpt\n    if mms_args.use_pretrained_summarizer\n    else \"\",\n    start_with_text_frozen=mms_args.start_with_text_frozen,\n    mask_video_features=mms_args.mask_video_features,", "    start_with_text_frozen=mms_args.start_with_text_frozen,\n    mask_video_features=mms_args.mask_video_features,\n    args = mms_args\n)\n\ntrainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n"]}
{"filename": "MultiSum/src/runtime/__init__.py", "chunked_list": [""]}
{"filename": "MultiSum/src/runtime/test_mms_model.py", "chunked_list": ["#!/usr/bin/env python\n\nimport pytorch_lightning as pl\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(__file__), \"../data\"))\nsys.path.append(os.path.join(os.path.dirname(__file__), \"../model\"))\nimport os\n_data_base = '../'", "import os\n_data_base = '../'\n\nfrom model_mms import MultimodalTransformer\nfrom data_laoder import MMSDataset, MMSDataModule\nfrom torch.utils.data import Dataset, DataLoader\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom transformers import AutoTokenizer", "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom transformers import AutoTokenizer\n\nimport argparse\nimport numpy as np\nimport torch\n\ntorch.set_num_threads(2)\n\n", "\n\nprint(sys.argv)\n\n# CKPT_PATH = './trainings/mms_novinky_tb/version=2_ep_txt_fr=0_v=ig65m_i=vit/checkpoints/epoch=0-step=834-ROUGE_RAW_L_F=0.08.ckpt' # seg\nCKPT_PATH = './trainings/mms_novinky_tb/version=1_ep_txt_fr=0_v=ig65m_i=vit/checkpoints/epoch=4-step=559-ROUGE_RAW_L_F=1.65.ckpt' # whole\nTEST_OR_VAL = 'val'\n\nROUGE_RAW_L_checkpoint = ModelCheckpoint(\n    filename=\"{epoch}-{step}-{ROUGE_RAW_L_F:.2f}\",", "ROUGE_RAW_L_checkpoint = ModelCheckpoint(\n    filename=\"{epoch}-{step}-{ROUGE_RAW_L_F:.2f}\",\n    monitor=\"ROUGE_RAW_L_F\",\n    mode=\"max\",\n    save_top_k=1,\n)\n\nROUGE_RAW_L_stop = EarlyStopping(monitor=\"ROUGE_RAW_L_F\", mode=\"max\", patience=5)\n\n", "\n\nmms_data = MMSDataModule(\n    argparse.Namespace(\n        articles_path=f\"{_data_base}/data/\",\n        video_ig65m_path=f\"{_data_base}/data/videos\",\n        # frames = f'{_data_base}/data/frames',\n        # video_s3d_path=f\"{_data_base}/video_mp4/s3d_how100m\",\n        video_s3d_path = None,\n        img_extract_vit_path=f\"{_data_base}/data/keyframes\",", "        video_s3d_path = None,\n        img_extract_vit_path=f\"{_data_base}/data/keyframes\",\n        img_tgt_vit_path=f\"{_data_base}/data/thumbnails\",\n        # img_extract_eff_path=f\"{_data_base}/video_mp4/efficientnet_b5\",\n        img_extract_eff_path = None,\n        # img_tgt_eff_path=f\"{_data_base}/image_jpeg/efficientnet_b5\",\n        img_tgt_eff_path = None,\n        model_headline=False,\n        max_src_len=1536,\n        max_tgt_len=256,", "        max_src_len=1536,\n        max_tgt_len=256,\n        train_batch_size=2,\n        val_batch_size=16,\n        num_workers=16,\n    )\n)\n\nif TEST_OR_VAL == \"val\":\n    test_loader = mms_data.val_dataloader()\nelif TEST_OR_VAL == \"test\":\n    test_loader = mms_data.test_dataloader()\nelse:\n    sys.exit(1)", "if TEST_OR_VAL == \"val\":\n    test_loader = mms_data.val_dataloader()\nelif TEST_OR_VAL == \"test\":\n    test_loader = mms_data.test_dataloader()\nelse:\n    sys.exit(1)\n\ntrainer = pl.Trainer(\n    max_epochs=50,\n    gpus=1,", "    max_epochs=50,\n    gpus=1,\n    log_every_n_steps=50,\n    # max_steps = 1,\n    val_check_interval=1.0,\n    gradient_clip_val=5,\n    accumulate_grad_batches=16,\n    callbacks=[ROUGE_RAW_L_checkpoint, ROUGE_RAW_L_stop],\n)\n", ")\n\nmodel = MultimodalTransformer.load_from_checkpoint(CKPT_PATH)\n\ntrainer.validate(model, dataloaders=test_loader, ckpt_path=CKPT_PATH)\n"]}
{"filename": "MultiSum/src/model/mms_modeling_t5.py", "chunked_list": ["# coding=utf-8\n\n# Based on https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py\n# that is licensed under Apache-2.0 license, i.e.\n\n# Copyright 2018 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at", "# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.", "# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch T5 model.\"\"\"\n\n\nimport copy\nimport math\nimport os\nimport warnings\n", "import warnings\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.checkpoint import checkpoint\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    DUMMY_INPUTS,", "from transformers.file_utils import (\n    DUMMY_INPUTS,\n    DUMMY_MASK,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_torch_fx_proxy,\n    replace_return_docstrings,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutput,", "from transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,", "    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import logging\nfrom transformers.utils.model_parallel_utils import assert_device_map, get_device_map\nfrom transformers.models.t5.configuration_t5 import T5Config\nfrom transformers.models.t5.modeling_t5 import *\n\nlogger = logging.get_logger(__name__)\n", "logger = logging.get_logger(__name__)\n\n\nMMS_IG65M_EM_SIZE = 2048\nMMS_S3D_EMB_SIZE = 512\nMMS_VIT_EMB_SIZE = 2048\nMMS_EFFNET_EMB_SIZE = 2048\n\n\n# Based on  https://pytorch.org/tutorials/beginner/translation_transformer.html\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n        super().__init__()\n        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"pos_embedding\", pos_embedding)\n\n    def forward(self, token_embedding):\n        return self.dropout(\n            token_embedding + self.pos_embedding[: token_embedding.size(0), :]\n        )", "\n# Based on  https://pytorch.org/tutorials/beginner/translation_transformer.html\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n        super().__init__()\n        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\"pos_embedding\", pos_embedding)\n\n    def forward(self, token_embedding):\n        return self.dropout(\n            token_embedding + self.pos_embedding[: token_embedding.size(0), :]\n        )", "\n\nclass T5PreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = T5Config\n    load_tf_weights = load_tf_weights_in_t5\n    base_model_prefix = \"transformer\"\n    is_parallelizable = True\n    supports_gradient_checkpointing = True\n\n    @property\n    def dummy_inputs(self):\n        input_ids = torch.tensor(DUMMY_INPUTS)\n        input_mask = torch.tensor(DUMMY_MASK)\n        dummy_inputs = {\n            \"decoder_input_ids\": input_ids,\n            \"input_ids\": input_ids,\n            \"decoder_attention_mask\": input_mask,\n        }\n        return dummy_inputs\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        factor = (\n            self.config.initializer_factor\n        )  # Used for testing weights initialization\n        if isinstance(module, T5LayerNorm):\n            module.weight.data.fill_(factor * 1.0)\n        elif isinstance(module, (T5Model, T5ForConditionalGeneration, T5EncoderModel)):\n            # Mesh TensorFlow embeddings initialization\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n            module.shared.weight.data.normal_(mean=0.0, std=factor * 1.0)\n        elif isinstance(module, T5DenseReluDense):\n            # Mesh TensorFlow FF initialization\n            # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n            # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n            module.wi.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n            )\n            if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n                module.wi.bias.data.zero_()\n            module.wo.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_ff) ** -0.5)\n            )\n            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n                module.wo.bias.data.zero_()\n        elif isinstance(module, T5DenseGatedGeluDense):\n            module.wi_0.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n            )\n            if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n                module.wi_0.bias.data.zero_()\n            module.wi_1.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n            )\n            if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n                module.wi_1.bias.data.zero_()\n            module.wo.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_ff) ** -0.5)\n            )\n            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n                module.wo.bias.data.zero_()\n        elif isinstance(module, T5Attention):\n            # Mesh TensorFlow attention initialization to avoid scaling before softmax\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n            d_model = self.config.d_model\n            key_value_proj_dim = self.config.d_kv\n            n_heads = self.config.num_heads\n            module.q.weight.data.normal_(\n                mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5)\n            )\n            module.k.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n            module.v.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n            module.o.weight.data.normal_(\n                mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5)\n            )\n            if module.has_relative_attention_bias:\n                module.relative_attention_bias.weight.data.normal_(\n                    mean=0.0, std=factor * ((d_model) ** -0.5)\n                )\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, (T5Attention, T5Stack)):\n            module.gradient_checkpointing = value\n\n    def _shift_right(self, input_ids):\n        decoder_start_token_id = self.config.decoder_start_token_id\n        pad_token_id = self.config.pad_token_id\n\n        assert (\n            decoder_start_token_id is not None\n        ), \"self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id. See T5 docs for more information\"\n\n        # shift inputs to the right\n        if is_torch_fx_proxy(input_ids):\n            # Item assignment is not supported natively for proxies.\n            shifted_input_ids = torch.full(\n                input_ids.shape[:-1] + (1,), decoder_start_token_id\n            )\n            shifted_input_ids = torch.cat(\n                [shifted_input_ids, input_ids[..., :-1]], dim=-1\n            )\n        else:\n            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n            shifted_input_ids[..., 0] = decoder_start_token_id\n\n        assert (\n            pad_token_id is not None\n        ), \"self.model.config.pad_token_id has to be defined.\"\n        # replace possible -100 values in labels by `pad_token_id`\n        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n        assert torch.all(\n            shifted_input_ids >= 0\n        ).item(), \"Verify that `shifted_input_ids` has only positive values\"\n\n        return shifted_input_ids", "\n\nclass MMST5Stack(T5PreTrainedModel): #encoder\n    def __init__(\n        self,\n        config,\n        embed_tokens,\n        num_video_enc_layers=None,\n        use_video_ig65m=None,\n        use_video_s3d=None,\n        use_image_vit=None,\n        use_image_effnet=None,\n        use_image_self_attention=False,\n    ):\n        super().__init__(config)\n\n        self.embed_tokens = embed_tokens\n        self.is_decoder = config.is_decoder\n        self.use_image_self_attention = use_image_self_attention\n\n        # --MMS-- Text-Video Attention\n        if not self.is_decoder:\n            # Video encoder\n            self.mms_video_positional_encoding = PositionalEncoding(\n                emb_size=config.d_model, dropout=config.dropout_rate\n            )\n            mms_encoder_layer = nn.modules.transformer.TransformerEncoderLayer(\n                d_model=config.d_model,\n                nhead=8,\n                dim_feedforward=2 * config.d_model,\n                dropout=config.dropout_rate,\n                batch_first=True,\n            )\n            self.mms_video_encoder = nn.modules.transformer.TransformerEncoder(\n                encoder_layer=mms_encoder_layer,\n                num_layers=num_video_enc_layers,\n            )\n            if self.use_image_self_attention:\n                self.mms_image_self_attention = (\n                    nn.modules.transformer.TransformerEncoder(\n                        encoder_layer=mms_encoder_layer,\n                        num_layers=num_video_enc_layers,\n                    )\n                )\n\n            # Cross-modal attention\n            self.mms_video_text_attention = nn.modules.activation.MultiheadAttention(\n                embed_dim=config.d_model,\n                num_heads=8,\n                dropout=config.dropout_rate,\n                batch_first=True,\n            )\n            self.mms_sigmoid = nn.Sigmoid()\n\n            self.mms_forget_gate = nn.modules.linear.Linear(\n                in_features=2 * config.d_model, out_features=config.d_model, bias=False\n            )\n            self.mms_ff_forget_gate = nn.modules.linear.Linear(\n                in_features=2 * config.d_model, out_features=config.d_model, bias=False\n            )\n\n            if use_video_ig65m:\n                if use_video_s3d:\n                    self.mms_video_rotate = nn.modules.linear.Linear(\n                        in_features=MMS_IG65M_EM_SIZE + MMS_S3D_EMB_SIZE,\n                        out_features=config.d_model,\n                        bias=False,\n                    )\n                else:\n                    self.mms_video_rotate = nn.modules.linear.Linear(\n                        in_features=MMS_IG65M_EM_SIZE,\n                        out_features=config.d_model,\n                        bias=False,\n                    )\n            else:\n                self.mms_video_rotate = nn.modules.linear.Linear(\n                    in_features=MMS_S3D_EMB_SIZE,\n                    out_features=config.d_model,\n                    bias=False,\n                )\n\n            self.mms_image_source_attention = nn.modules.activation.MultiheadAttention(\n                embed_dim=config.d_model,\n                num_heads=8,\n                dropout=config.dropout_rate,\n                batch_first=True,\n            )\n\n            self.mms_forget_gate_image = nn.modules.linear.Linear(\n                in_features=2 * config.d_model, out_features=config.d_model, bias=False\n            )\n            self.mms_ff_forget_gate_image = nn.modules.linear.Linear(\n                in_features=2 * config.d_model, out_features=config.d_model, bias=False\n            )\n\n            if use_image_vit:\n                if use_image_effnet:\n                    self.mms_image_rotate = nn.modules.linear.Linear(\n                        in_features=MMS_VIT_EMB_SIZE + MMS_EFFNET_EMB_SIZE,\n                        out_features=config.d_model,\n                        bias=False,\n                    )\n                else:\n                    self.mms_image_rotate = nn.modules.linear.Linear(\n                        in_features=MMS_VIT_EMB_SIZE,\n                        out_features=config.d_model,\n                        bias=False,\n                    )\n            else:\n                self.mms_image_rotate = nn.modules.linear.Linear(\n                    in_features=MMS_EFFNET_EMB_SIZE,\n                    out_features=config.d_model,\n                    bias=False,\n                )\n\n            self.mms_image_selector_projection = nn.modules.linear.Linear(\n                in_features=config.d_model,\n                out_features=1,\n                bias=False,\n            )\n        # --MMS-- Text-Video Attention\n\n        self.block = nn.ModuleList(\n            [\n                T5Block(config, has_relative_attention_bias=bool(i == 0))\n                for i in range(config.num_layers)\n            ]\n        )\n        self.final_layer_norm = T5LayerNorm(\n            config.d_model, eps=config.layer_norm_epsilon\n        )\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n        self.gradient_checkpointing = False\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        # Check validity of device_map\n        self.device_map = (\n            get_device_map(len(self.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.block))\n        self.model_parallel = True\n        self.first_device = (\n            \"cpu\"\n            if \"cpu\" in self.device_map.keys()\n            else \"cuda:\" + str(min(self.device_map.keys()))\n        )\n        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n        # Load onto devices\n        for k, v in self.device_map.items():\n            for layer in v:\n                cuda_device = \"cuda:\" + str(k)\n                self.block[layer] = self.block[layer].to(cuda_device)\n\n        # Set embed_tokens to first layer\n        self.embed_tokens = self.embed_tokens.to(self.first_device)\n        # Set final layer norm to last device\n        self.final_layer_norm = self.final_layer_norm.to(self.last_device)\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.model_parallel = False\n        self.device_map = None\n        self.first_device = \"cpu\"\n        self.last_device = \"cpu\"\n        for i in range(len(self.block)):\n            self.block[i] = self.block[i].to(\"cpu\")\n        self.embed_tokens = self.embed_tokens.to(\"cpu\")\n        self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, new_embeddings):\n        self.embed_tokens = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        inputs_embeds=None,\n        head_mask=None,\n        cross_attn_head_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        video_ig65m_emb=None,\n        video_s3d_emb=None,\n        image_vit_emb=None,\n        image_effnet_emb=None,\n        video_padding_mask=None,\n        image_padding_mask=None,\n    ):\n        # Model parallel\n        if self.model_parallel:\n            torch.cuda.set_device(self.first_device)\n            self.embed_tokens = self.embed_tokens.to(self.first_device)\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        if input_ids is not None and inputs_embeds is not None:\n            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n            raise ValueError(\n                f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n            raise ValueError(\n                f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"\n            )\n\n        if inputs_embeds is None:\n            assert (\n                self.embed_tokens is not None\n            ), \"You have to initialize the model with valid token embeddings\"\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        batch_size, seq_length = input_shape\n\n        # required mask seq length can be calculated via length of past\n        mask_seq_length = (\n            past_key_values[0][0].shape[2] + seq_length\n            if past_key_values is not None\n            else seq_length\n        )\n\n        if use_cache is True:\n            assert (\n                self.is_decoder\n            ), f\"`use_cache` can only be set to `True` if {self} is used as a decoder\"\n\n        if attention_mask is None:\n            attention_mask = torch.ones(batch_size, mask_seq_length).to(\n                inputs_embeds.device\n            )\n        if (\n            self.is_decoder\n            and encoder_attention_mask is None\n            and encoder_hidden_states is not None\n        ):\n            encoder_seq_length = encoder_hidden_states.shape[1]\n            encoder_attention_mask = torch.ones(\n                batch_size,\n                encoder_seq_length,\n                device=inputs_embeds.device,\n                dtype=torch.long,\n            )\n\n        # initialize past_key_values with `None` if past does not exist\n        if past_key_values is None:\n            past_key_values = [None] * len(self.block)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask = self.get_extended_attention_mask(\n            attention_mask, input_shape, inputs_embeds.device\n        )\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(\n                    encoder_hidden_shape, device=inputs_embeds.device\n                )\n            encoder_extended_attention_mask = self.invert_attention_mask(\n                encoder_attention_mask\n            )\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n        cross_attn_head_mask = self.get_head_mask(\n            cross_attn_head_mask, self.config.num_layers\n        )\n        present_key_value_states = () if use_cache else None\n        all_hidden_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n        position_bias = None\n        encoder_decoder_position_bias = None\n\n        hidden_states = self.dropout(inputs_embeds)\n\n        for i, (layer_module, past_key_value) in enumerate(\n            zip(self.block, past_key_values)\n        ):\n            layer_head_mask = head_mask[i]\n            cross_attn_layer_head_mask = cross_attn_head_mask[i]\n            # Model parallel\n            if self.model_parallel:\n                torch.cuda.set_device(hidden_states.device)\n                # Ensure that attention_mask is always on the same device as hidden_states\n                if attention_mask is not None:\n                    attention_mask = attention_mask.to(hidden_states.device)\n                if position_bias is not None:\n                    position_bias = position_bias.to(hidden_states.device)\n                if encoder_hidden_states is not None:\n                    encoder_hidden_states = encoder_hidden_states.to(\n                        hidden_states.device\n                    )\n                if encoder_extended_attention_mask is not None:\n                    encoder_extended_attention_mask = (\n                        encoder_extended_attention_mask.to(hidden_states.device)\n                    )\n                if encoder_decoder_position_bias is not None:\n                    encoder_decoder_position_bias = encoder_decoder_position_bias.to(\n                        hidden_states.device\n                    )\n                if layer_head_mask is not None:\n                    layer_head_mask = layer_head_mask.to(hidden_states.device)\n                if cross_attn_layer_head_mask is not None:\n                    cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(\n                        hidden_states.device\n                    )\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                if use_cache:\n                    logger.warning(\n                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return tuple(module(*inputs, use_cache, output_attentions))\n\n                    return custom_forward\n\n                layer_outputs = checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    extended_attention_mask,\n                    position_bias,\n                    encoder_hidden_states,\n                    encoder_extended_attention_mask,\n                    encoder_decoder_position_bias,\n                    layer_head_mask,\n                    cross_attn_layer_head_mask,\n                    None,  # past_key_value is always None with gradient checkpointing\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask=extended_attention_mask,\n                    position_bias=position_bias,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_extended_attention_mask,\n                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n                    layer_head_mask=layer_head_mask,\n                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n                    past_key_value=past_key_value,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                )\n\n            # layer_outputs is a tuple with:\n            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n            if use_cache is False:\n                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n\n            hidden_states, present_key_value_state = layer_outputs[:2]\n\n            # We share the position biases between the layers - the first layer store them\n            # layer_outputs=hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n            # (cross-attention position bias), (cross-attention weights)\n            position_bias = layer_outputs[2]\n            if self.is_decoder and encoder_hidden_states is not None:\n                encoder_decoder_position_bias = layer_outputs[\n                    4 if output_attentions else 3\n                ]\n            # append next layer key value states\n            if use_cache:\n                present_key_value_states = present_key_value_states + (\n                    present_key_value_state,\n                )\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[3],)\n                if self.is_decoder:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n\n            # Model Parallel: If it's the last layer for that device, put things on the next device\n            if self.model_parallel:\n                for k, v in self.device_map.items():\n                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n        # print('hidden_states')\n        # print(hidden_states.shape)\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n\n        # --MMS-- Text-Video Attention\n        if not self.is_decoder:\n            if video_ig65m_emb is not None:\n                \n                if video_s3d_emb is not None:\n                    video_features = torch.cat((video_ig65m_emb, video_s3d_emb), -1)\n                else:\n                    video_features = video_ig65m_emb\n            else:\n                video_features = video_s3d_emb\n\n            # Project to the same dimension as text\n            video_features = self.mms_video_rotate(video_features)\n\n            # Encode with transformer model - Eq (3) in MLASK paper\n            transformer_encoded_video = self.mms_video_encoder(\n                src=self.mms_video_positional_encoding(video_features),\n                src_key_padding_mask=video_padding_mask,\n            )\n            transformer_encoded_video = self.dropout(transformer_encoded_video)\n\n            # Compute the text-queried visual features - same shape as hidden_states. Eq (9) in MLASK paper\n            transformer_encoded_video, _ = self.mms_video_text_attention(\n                query=hidden_states,\n                key=transformer_encoded_video,\n                value=transformer_encoded_video,\n                key_padding_mask=video_padding_mask,\n                need_weights=False,\n            )\n\n            # Use the forget gate - https://arxiv.org/pdf/2109.02401.pdf section 3.3\n            transformer_encoded_video_sigmoid = self.mms_sigmoid(\n                self.mms_forget_gate(\n                    torch.cat((hidden_states, transformer_encoded_video), -1)\n                )\n            )\n            transformer_encoded_video = transformer_encoded_video.mul(\n                transformer_encoded_video_sigmoid\n            )\n\n            # Apply the forget gate, Eq (10) in MLASK paper\n            hidden_states = self.mms_ff_forget_gate(\n                torch.cat((hidden_states, transformer_encoded_video), -1)\n            )\n            hidden_states = self.dropout(hidden_states)\n\n            # Frame selection part\n            if image_vit_emb is not None:\n                if image_effnet_emb is not None:\n                    image_seq_emb = torch.cat((image_vit_emb, image_effnet_emb), -1)\n                else:\n                    image_seq_emb = image_vit_emb\n            else:\n                image_seq_emb = image_effnet_emb\n            # Project to the same dimension as text\n            original_image_seq_emb = self.mms_image_rotate(image_seq_emb)\n\n            if self.use_image_self_attention:\n                # Eq (5) in MLASK paper\n                original_image_seq_emb = self.mms_image_self_attention(\n                    src=self.mms_video_positional_encoding(original_image_seq_emb),\n                    src_key_padding_mask=image_padding_mask,\n                )\n\n            encoder2image_mask = torch.ones_like(attention_mask)\n            # Different masking compared to huggingface modules\n            encoder2image_mask = (\n                encoder2image_mask.bool()\n                .masked_fill(attention_mask == 0, True)\n                .masked_fill(attention_mask == 1, False)\n            )\n\n            # Compute attention to fused video+text representation\n            image_seq_emb, _ = self.mms_image_source_attention(\n                query=original_image_seq_emb,\n                key=hidden_states,\n                value=hidden_states,\n                key_padding_mask=encoder2image_mask,\n                need_weights=False,\n            )\n\n            # Use the forget gate - https://arxiv.org/pdf/2109.02401.pdf section 3.3\n            image_seq_emb_sigmoid = self.mms_sigmoid(\n                self.mms_forget_gate_image(\n                    torch.cat((original_image_seq_emb, image_seq_emb), -1)\n                )\n            )\n            image_seq_emb = image_seq_emb.mul(image_seq_emb_sigmoid)\n\n            # $\\widehat{V}_{frame}$, Section 4.3 from MLASK paper\n            image_seq_emb = self.mms_ff_forget_gate_image(\n                torch.cat((original_image_seq_emb, image_seq_emb), -1)\n            )\n\n            # B x Num_Img_Frames\n            image_seq_emb_flattened = torch.squeeze(\n                self.mms_image_selector_projection(image_seq_emb)\n            )\n\n        # --MMS-- Text-Video Attention\n\n        # Add last layer\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n        \n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    present_key_value_states,\n                    all_hidden_states,\n                    all_attentions,\n                    all_cross_attentions,\n                    image_seq_emb_flattened,\n                ]\n                if v is not None\n            )\n        else:\n            return BaseModelOutputWithPastAndCrossAttentions(\n                last_hidden_state=hidden_states,\n                past_key_values=present_key_value_states,\n                hidden_states=all_hidden_states,\n                attentions=all_attentions,\n                cross_attentions=all_cross_attentions,\n            )", "\n\n@add_start_docstrings(\n    \"\"\"T5 Model with a `language modeling` head on top.\"\"\", T5_START_DOCSTRING\n)\nclass MMST5ForConditionalGeneration(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder\\.embed_tokens\\.weight\",\n        r\"decoder\\.embed_tokens\\.weight\",\n        r\"lm_head\\.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder\\.block\\.0\\.layer\\.1\\.EncDecAttention\\.relative_attention_bias\\.weight\",\n    ]\n\n    def __init__(\n        self,\n        config,\n        num_video_enc_layers: int = 2,\n        use_video_ig65m: bool = True,\n        use_video_s3d: bool = True,\n        use_image_vit: bool = True,\n        use_image_effnet: bool = True,\n        smooth_cos_labels: bool = True,\n        use_image_self_attention: bool = False,\n    ):\n        super().__init__(config)\n        self.model_dim = config.d_model\n\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.is_decoder = False\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = MMST5Stack(\n            encoder_config,\n            self.shared,\n            num_video_enc_layers,\n            use_video_ig65m,\n            use_video_s3d,\n            use_image_vit,\n            use_image_effnet,\n            use_image_self_attention,\n        )\n        self.smooth_cos_labels = smooth_cos_labels\n        self.use_image_self_attention = use_image_self_attention\n\n        self.cosine_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n\n        decoder_config = copy.deepcopy(config)\n        decoder_config.is_decoder = True\n        decoder_config.is_encoder_decoder = False\n        decoder_config.num_layers = config.num_decoder_layers\n        self.decoder = T5Stack(decoder_config, self.shared)\n\n        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        self.device_map = (\n            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.encoder.block))\n        self.encoder.parallelize(self.device_map)\n        self.decoder.parallelize(self.device_map)\n        self.lm_head = self.lm_head.to(self.decoder.first_device)\n        self.model_parallel = True\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.encoder.deparallelize()\n        self.decoder.deparallelize()\n        self.encoder = self.encoder.to(\"cpu\")\n        self.decoder = self.decoder.to(\"cpu\")\n        self.lm_head = self.lm_head.to(\"cpu\")\n        self.model_parallel = False\n        self.device_map = None\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n        self.decoder.set_input_embeddings(new_embeddings)\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def get_encoder(self):\n        return self.encoder\n\n    def get_decoder(self):\n        return self.decoder\n\n    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        decoder_input_ids=None,\n        decoder_attention_mask=None,\n        head_mask=None,\n        decoder_head_mask=None,\n        cross_attn_head_mask=None,\n        encoder_outputs=None,\n        past_key_values=None,\n        inputs_embeds=None,\n        decoder_inputs_embeds=None,\n        labels=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        video_ig65m_emb=None,\n        video_s3d_emb=None,\n        image_vit_emb=None,\n        image_effnet_emb=None,\n        video_padding_mask=None,\n        image_padding_mask=None,\n        tgt_img_cosine_scores=None,\n        tgt_image_vit_emb=None,\n        tgt_image_effnet_emb=None,\n    ):\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n        if head_mask is not None and decoder_head_mask is None:\n            if self.config.num_layers == self.config.num_decoder_layers:\n                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n                decoder_head_mask = head_mask\n\n        image_selection_loss = None\n\n        # Encode if needed (training, first prediction pass)\n        if encoder_outputs is None:\n            # Convert encoder inputs in embeddings if needed\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                head_mask=head_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                video_ig65m_emb=video_ig65m_emb,\n                video_s3d_emb=video_s3d_emb,\n                image_vit_emb=image_vit_emb,\n                image_effnet_emb=image_effnet_emb,\n                video_padding_mask=video_padding_mask,\n                image_padding_mask=image_padding_mask,\n            )\n\n            # While training\n\n            if tgt_img_cosine_scores is not None:\n                image_seq_emb_flattened = encoder_outputs[-1]\n                image_summary_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n\n                if image_vit_emb is not None:\n                    vit_cosine_sim = self.cosine_sim(\n                        image_vit_emb, torch.unsqueeze(tgt_image_vit_emb, 1)\n                    )\n                if image_effnet_emb is not None:\n                    effnet_cosine_sim = self.cosine_sim(\n                        image_effnet_emb, torch.unsqueeze(tgt_image_effnet_emb, 1)\n                    )\n                    if image_vit_emb is not None:\n                        cosine_sim = (vit_cosine_sim + effnet_cosine_sim) / 2\n                    else:\n                        cosine_sim = effnet_cosine_sim\n                else:\n                    cosine_sim = vit_cosine_sim\n\n                cosine_sim = torch.where(\n                    cosine_sim > 0, cosine_sim, torch.zeros_like(cosine_sim)\n                )\n                tgt_img_cosine_scores = cosine_sim\n                # Section 4.4 in MLASK paper, $C_{max}$ or $C_{smooth}$\n                if not self.smooth_cos_labels:\n                    tgt_image_labels = (\n                        nn.functional.one_hot(\n                            torch.argmax(tgt_img_cosine_scores, dim=1),\n                            num_classes=tgt_img_cosine_scores.shape[1],\n                        )\n                        .float()\n                        .view(-1)\n                    )\n                else:\n                    tgt_image_labels = tgt_img_cosine_scores.view(-1)\n                image_selection_loss = image_summary_loss(\n                    input=image_seq_emb_flattened.view(-1),\n                    target=tgt_img_cosine_scores.view(-1),\n                )\n                # Mask the loss - don't do this per batch but per flattened sequence\n                image_selection_loss = torch.mean(\n                    torch.where(\n                        image_padding_mask.view(-1) == 0,\n                        image_selection_loss,\n                        torch.zeros_like(image_selection_loss),\n                    )\n                )\n\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        hidden_states = encoder_outputs[0]\n\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n\n        if (\n            labels is not None\n            and decoder_input_ids is None\n            and decoder_inputs_embeds is None\n        ):\n            # get decoder inputs from shifting lm labels to the right\n            decoder_input_ids = self._shift_right(labels)\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n            hidden_states = hidden_states.to(self.decoder.first_device)\n            if decoder_input_ids is not None:\n                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n            if attention_mask is not None:\n                attention_mask = attention_mask.to(self.decoder.first_device)\n            if decoder_attention_mask is not None:\n                decoder_attention_mask = decoder_attention_mask.to(\n                    self.decoder.first_device\n                )\n\n        # Decode\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            inputs_embeds=decoder_inputs_embeds,\n            past_key_values=past_key_values,\n            encoder_hidden_states=hidden_states,\n            encoder_attention_mask=attention_mask,\n            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = decoder_outputs[0]\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.encoder.first_device)\n            self.lm_head = self.lm_head.to(self.encoder.first_device)\n            sequence_output = sequence_output.to(self.lm_head.weight.device)\n\n        if self.config.tie_word_embeddings:\n            # Rescale output before projecting on vocab\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n            sequence_output = sequence_output * (self.model_dim**-0.5)\n\n        lm_logits = self.lm_head(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-100)\n            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n        if not return_dict:\n            output = (\n                (lm_logits,)\n                + decoder_outputs\n                + decoder_outputs[1:]\n                + encoder_outputs\n                + (image_seq_emb_flattened,)\n            )\n            if loss is not None:\n                assert image_selection_loss is not None\n                return (\n                    loss,\n                    image_selection_loss,\n                ) + output\n            else:\n                return (output,)\n        else:\n            return Seq2SeqLMOutput(\n                loss=loss,\n                logits=lm_logits,\n                past_key_values=decoder_outputs.past_key_values,\n                decoder_hidden_states=decoder_outputs.hidden_states,\n                decoder_attentions=decoder_outputs.attentions,\n                cross_attentions=decoder_outputs.cross_attentions,\n                encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n                encoder_hidden_states=encoder_outputs.hidden_states,\n                encoder_attentions=encoder_outputs.attentions,\n            )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past=None,\n        attention_mask=None,\n        head_mask=None,\n        decoder_head_mask=None,\n        cross_attn_head_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\n            \"decoder_input_ids\": input_ids,\n            \"past_key_values\": past,\n            \"encoder_outputs\": encoder_outputs,\n            \"attention_mask\": attention_mask,\n            \"head_mask\": head_mask,\n            \"decoder_head_mask\": decoder_head_mask,\n            \"cross_attn_head_mask\": cross_attn_head_mask,\n            \"use_cache\": use_cache,\n        }\n\n    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n        return self._shift_right(labels)\n\n    def _reorder_cache(self, past, beam_idx):\n        # if decoder past is not included in output\n        # speedy decoding is disabled and no need to reorder\n        if past is None:\n            logger.warning(\n                \"You might want to consider setting `use_cache=True` to speed up decoding\"\n            )\n            return past\n\n        reordered_decoder_past = ()\n        for layer_past_states in past:\n            # get the correct batch idx from layer past batch dim\n            # batch dim of `past` is at 2nd position\n            reordered_layer_past_states = ()\n            for layer_past_state in layer_past_states:\n                # need to set correct `past` for each of the four key / value states\n                reordered_layer_past_states = reordered_layer_past_states + (\n                    layer_past_state.index_select(\n                        0, beam_idx.to(layer_past_state.device)\n                    ),\n                )\n\n            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n            assert len(reordered_layer_past_states) == len(layer_past_states)\n\n            reordered_decoder_past = reordered_decoder_past + (\n                reordered_layer_past_states,\n            )\n        return reordered_decoder_past", ""]}
{"filename": "MultiSum/src/model/__init__.py", "chunked_list": [""]}
{"filename": "MultiSum/src/model/model_mms.py", "chunked_list": ["import json\nimport os\nimport unicodedata\nimport math\nfrom typing import Union\nimport sys\nfrom PIL import Image, ImageDraw, ImageFont\nfrom statistics import mean\nimport pytorch_lightning as pl\nimport torch", "import pytorch_lightning as pl\nimport torch\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom torch import Tensor\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom pycocoevalcap.spice.spice import Spice", "from transformers import AutoTokenizer\nfrom pycocoevalcap.spice.spice import Spice\nfrom pycocoevalcap.cider.cider import Cider\nfrom evaluate import load\nfrom sacrebleu.metrics import BLEU\nfrom rouge_raw import RougeRaw\nfrom torchmetrics import RetrievalMAP, RetrievalRecall, RetrievalPrecision\nfrom scipy.stats import pearsonr, kendalltau\n# from nlgeval import NLGEval\nfrom nltk.translate.meteor_score import meteor_score", "# from nlgeval import NLGEval\nfrom nltk.translate.meteor_score import meteor_score\nfrom pycocotools.coco import COCO\nfrom image_similarity_measures.quality_metrics import rmse, psnr, ssim, sre\nfrom pycocoevalcap.eval import COCOEvalCap\nsys.path.append(os.path.join(os.path.dirname(__file__), \"../data\"))\nsys.path.append(os.path.join(os.path.dirname(__file__), \"../model\"))\nspice_scorer = Spice()\ncider_scorer = Cider()\nbertscore = load('bertscore')", "cider_scorer = Cider()\nbertscore = load('bertscore')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\ntorch.set_num_threads(2)\n\nimport re\n\ndef clean_text(text):\n    # Remove non-alphanumeric characters\n    cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    \n    # Remove extra whitespaces\n    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n    \n    return cleaned_text", "def clean_text(text):\n    # Remove non-alphanumeric characters\n    cleaned_text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    \n    # Remove extra whitespaces\n    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n    \n    return cleaned_text\n\ndef parse_lists(predictions, references, parse_num):\n    parsed_predictions = []\n    parsed_references = []\n\n    for pred, refs in zip(predictions, references):\n        if len(pred) <= parse_num:\n            parsed_predictions.append(pred)\n            parsed_references.append(refs)\n        else:\n            pred_parts = [pred[i:i+parse_num] for i in range(0, len(pred), parse_num)]\n            parsed_predictions.extend(pred_parts)\n            parsed_references.extend([refs] * len(pred_parts))\n\n    return parsed_predictions, parsed_references", "\ndef parse_lists(predictions, references, parse_num):\n    parsed_predictions = []\n    parsed_references = []\n\n    for pred, refs in zip(predictions, references):\n        if len(pred) <= parse_num:\n            parsed_predictions.append(pred)\n            parsed_references.append(refs)\n        else:\n            pred_parts = [pred[i:i+parse_num] for i in range(0, len(pred), parse_num)]\n            parsed_predictions.extend(pred_parts)\n            parsed_references.extend([refs] * len(pred_parts))\n\n    return parsed_predictions, parsed_references", "\n\nfrom utils import generate_square_subsequent_mask\nfrom mms_modeling_t5 import MMST5ForConditionalGeneration\n\nIG65M_EM_SIZE = 512\nS3D_EMB_SIZE = 512\nVIT_EMB_SIZE = 768\nEFFNET_EMB_SIZE = 2048\nMAX_TGT_LEN = 250", "EFFNET_EMB_SIZE = 2048\nMAX_TGT_LEN = 250\n\n\nclass MultimodalTransformer(pl.LightningModule):\n    def __init__(\n        self,\n        num_video_enc_layers: int,\n        use_video_ig65m: bool = True,\n        use_video_s3d: bool = True,\n        use_image_vit: bool = True,\n        use_image_effnet: bool = True,\n        smooth_cos_labels: bool = False,\n        lr_max_val: float = 0.0005,\n        lr_init_val: float = 0,\n        lr_warmup_steps: int = 4000,\n        pre_trained_summeczech_ckpt: str = \"\",\n        start_with_text_frozen=0,\n        mask_video_features=False,\n        use_image_self_attention=True,\n        args = None,\n    ):\n        super().__init__()\n        # Sanity checks\n        assert use_video_ig65m or use_video_s3d\n        assert use_image_vit or use_image_effnet\n        self.save_hyperparameters()\n        self.model = None\n        self.args = args\n        self._create_model()\n\n    def _create_model(self):\n        # Used with pre-trained MT5 checkpoint\n        if self.hparams.pre_trained_summeczech_ckpt != \"\":\n            self.model = MMST5ForConditionalGeneration.from_pretrained(\n                self.hparams.pre_trained_summeczech_ckpt,\n                num_video_enc_layers=self.hparams.num_video_enc_layers,\n                use_video_ig65m=self.hparams.use_video_ig65m,\n                use_video_s3d=self.hparams.use_video_s3d,\n                use_image_vit=self.hparams.use_image_vit,\n                use_image_effnet=self.hparams.use_image_effnet,\n                smooth_cos_labels=self.hparams.smooth_cos_labels,\n                use_image_self_attention=self.hparams.use_image_self_attention,\n            )\n        else:\n            self.model = MMST5ForConditionalGeneration.from_pretrained(\n                \"google/mt5-small\",\n                num_video_enc_layers=self.hparams.num_video_enc_layers,\n                use_video_ig65m=self.hparams.use_video_ig65m,\n                use_video_s3d=self.hparams.use_video_s3d,\n                use_image_vit=self.hparams.use_image_vit,\n                use_image_effnet=self.hparams.use_image_effnet,\n                smooth_cos_labels=self.hparams.smooth_cos_labels,\n                use_image_self_attention=self.hparams.use_image_self_attention,\n            )\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n        self.cosine_sim = nn.CosineSimilarity(dim=-1, eps=1e-6)\n\n        # Evaluation metrics\n        self.sacrebleu = BLEU()\n        self.rouge = RougeRaw()\n\n        self.rMAP = RetrievalMAP()\n        self.rRec_1 = RetrievalRecall(k=1)\n        self.rRec_5 = RetrievalRecall(k=5)\n        self.rRec_10 = RetrievalRecall(k=10)\n        \n        self.rPre_1 = RetrievalPrecision(k=1)\n        self.rPre_5 = RetrievalPrecision(k=5)\n        self.rPre_10 = RetrievalPrecision(k=10)\n        \n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        decoder_input_ids=None,\n        decoder_attention_mask=None,\n        head_mask=None,\n        decoder_head_mask=None,\n        cross_attn_head_mask=None,\n        encoder_outputs=None,\n        past_key_values=None,\n        inputs_embeds=None,\n        decoder_inputs_embeds=None,\n        labels=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        video_ig65m_emb=None,\n        video_s3d_emb=None,\n        image_vit_emb=None,\n        image_effnet_emb=None,\n        video_padding_mask=None,\n        image_padding_mask=None,\n        tgt_img_cosine_scores=None,\n        tgt_image_vit_emb=None,\n        tgt_image_effnet_emb=None,\n    ):\n        return self.model.forward(\n            input_ids,\n            attention_mask,\n            decoder_input_ids,\n            decoder_attention_mask,\n            head_mask,\n            decoder_head_mask,\n            cross_attn_head_mask,\n            encoder_outputs,\n            past_key_values,\n            inputs_embeds,\n            decoder_inputs_embeds,\n            labels,\n            use_cache,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            video_ig65m_emb,\n            video_s3d_emb,\n            image_vit_emb,\n            image_effnet_emb,\n            video_padding_mask,\n            image_padding_mask,\n            tgt_img_cosine_scores,\n            tgt_image_vit_emb,\n            tgt_image_effnet_emb,\n        )\n\n    def training_step(self, batch, batch_idx):\n        src_tokens = batch[\"src_ids\"]\n        src_padding_mask = batch[\"src_mask\"]\n        tgt_tokens = batch[\"tgt_ids\"]\n        tgt_padding_mask = batch[\"tgt_mask\"]\n        batch['src_img_cosine'] = []\n        tgt_img_cosine_scores = batch[\"src_img_cosine\"]\n\n        video_padding_mask = batch[\"video_mask\"]\n        image_padding_mask = batch[\"src_img_mask\"]\n\n        # Video features\n        if self.hparams.use_video_ig65m:\n            video_ig65m_emb = batch[\"video_features_ig65m\"]\n        else:\n            video_ig65m_emb = None\n        if self.hparams.use_video_s3d:\n            video_s3d_emb = batch[\"video_features_s3d\"]\n        else:\n            video_s3d_emb = None\n\n        # Image features\n        if self.hparams.use_image_vit:\n            image_vit_emb = batch[\"src_img_features_vit\"]\n            tgt_image_vit_emb = batch[\"tgt_img_features_vit\"]\n        else:\n            image_vit_emb = None\n            tgt_image_vit_emb = None\n        if self.hparams.use_image_effnet:\n            image_effnet_emb = batch[\"src_img_features_effnet\"]\n            tgt_image_effnet_emb = batch[\"tgt_img_features_effnet\"]\n        else:\n            image_effnet_emb = None\n            tgt_image_effnet_emb = None\n\n        if self.hparams.mask_video_features:\n            video_ig65m_emb = torch.randn_like(video_ig65m_emb)\n            video_s3d_emb = torch.randn_like(video_s3d_emb)\n        # Compute text summary loss\n        text_summary_loss, image_selection_loss, *_ = self.forward(\n            input_ids=src_tokens,\n            attention_mask=src_padding_mask,\n            decoder_attention_mask=tgt_padding_mask,\n            labels=tgt_tokens,\n            return_dict=False,\n            video_ig65m_emb=video_ig65m_emb,\n            video_s3d_emb=video_s3d_emb,\n            image_vit_emb=image_vit_emb,\n            image_effnet_emb=image_effnet_emb,\n            video_padding_mask=video_padding_mask,\n            image_padding_mask=image_padding_mask,\n            tgt_img_cosine_scores=tgt_img_cosine_scores,\n            tgt_image_vit_emb=tgt_image_vit_emb,\n            tgt_image_effnet_emb=tgt_image_effnet_emb,\n        )\n        self.log(\n            \"img_loss\",\n            image_selection_loss,\n            on_step=True,\n            on_epoch=False,\n            batch_size=len(batch),\n        )\n        self.log(\n            \"summary_loss\",\n            text_summary_loss,\n            on_step=True,\n            on_epoch=False,\n            batch_size=len(batch),\n        )\n\n        return text_summary_loss + image_selection_loss\n\n    def prediction_step(self, batch, batch_idx):\n        src_tokens = batch[\"src_ids\"]\n        src_padding_mask = batch[\"src_mask\"]\n        video_padding_mask = batch[\"video_mask\"]\n        image_padding_mask = batch[\"src_img_mask\"]\n\n        # Video features\n        if self.hparams.use_video_ig65m:\n            video_ig65m_emb = batch[\"video_features_ig65m\"]\n        else:\n            video_ig65m_emb = None\n        if self.hparams.use_video_s3d:\n            video_s3d_emb = batch[\"video_features_s3d\"]\n        else:\n            video_s3d_emb = None\n\n        # Image features\n        if self.hparams.use_image_vit:\n            image_vit_emb = batch[\"src_img_features_vit\"]\n        else:\n            image_vit_emb = None\n        if self.hparams.use_image_effnet:\n            image_effnet_emb = batch[\"src_img_features_effnet\"]\n        else:\n            image_effnet_emb = None\n\n        if self.hparams.mask_video_features:\n            video_ig65m_emb = torch.randn_like(video_ig65m_emb)\n            video_s3d_emb = torch.randn_like(video_s3d_emb)\n\n        # Generate the summary using the text and video features\n        txt_summary_tokens = self.model.generate(\n            input_ids=src_tokens,\n            attention_mask=src_padding_mask,\n            video_ig65m_emb=video_ig65m_emb,\n            video_s3d_emb=video_s3d_emb,\n            video_padding_mask=video_padding_mask,\n            image_effnet_emb=image_effnet_emb,\n            image_padding_mask=image_padding_mask,\n            image_vit_emb=image_vit_emb,\n            num_beams=4,\n            max_length=256,\n            repetition_penalty=2.5,\n            length_penalty=1.0,\n        )\n        predicted_sent = self.tokenizer.batch_decode(\n            txt_summary_tokens, skip_special_tokens=True\n        )\n\n        # Use the encoder explicitly to obtain the per-frame scores\n        *out2, per_frame_logits = self.model.encoder(\n            input_ids=src_tokens,\n            attention_mask=src_padding_mask,\n            video_ig65m_emb=video_ig65m_emb,\n            video_s3d_emb=video_s3d_emb,\n            video_padding_mask=video_padding_mask,\n            image_effnet_emb=image_effnet_emb,\n            image_padding_mask=image_padding_mask,\n            image_vit_emb=image_vit_emb,\n            return_dict=False,\n        )\n\n        per_frame_logits = per_frame_logits.masked_fill(image_padding_mask != 0.0, -1e6)\n\n        return {\"hyp\": predicted_sent, \"frame_scores\": per_frame_logits, \n                'encoder_hidden_state': out2[0]}\n\n    def validation_step(self, batch, batch_idx):\n        predictions = self.prediction_step(batch, batch_idx)\n        if self.hparams.use_image_vit:\n            if self.args.env == 'whole':\n                vit_cosine_sim = self.cosine_sim(\n                    batch[\"src_img_features_vit\"],\n                    torch.unsqueeze(batch[\"tgt_img_features_vit\"], 1), # if training whole\n                )\n            else:\n                vit_cosine_sim = self.cosine_sim(\n                    batch[\"src_img_features_vit\"],\n                    batch[\"tgt_img_features_vit\"], \n                )\n        if self.hparams.use_image_effnet:\n            effnet_cosine_sim = self.cosine_sim(\n                batch[\"src_img_features_effnet\"],\n                torch.unsqueeze(batch[\"tgt_img_features_effnet\"], 1),\n            )\n            if self.hparams.use_image_vit:\n                cosine_sim = (vit_cosine_sim + effnet_cosine_sim) / 2\n            else:\n                cosine_sim = effnet_cosine_sim\n        else:\n            cosine_sim = vit_cosine_sim\n\n        cosine_sim = torch.where(\n            cosine_sim > 0, cosine_sim, torch.zeros_like(cosine_sim)\n        ).cpu()\n\n        cosine_sim_raw = cosine_sim.detach().clone()\n\n        top_1_frame = torch.argmax(cosine_sim, dim=1)\n        ids = batch['_id']\n        hyp = predictions[\"hyp\"],\n        targ = batch[\"tgt\"]\n        sent = hyp[0]\n        refs = [sent for sent in targ]\n        save_dic = {\n            'sentences' : sent,\n            'references' : refs,\n            'selected_frames' : top_1_frame.tolist(),\n            'ids'       : ids\n        }\n        \n        np.save(f'results_whole/results_whole_{batch_idx}.npy', save_dic)\n                \n\n        top_1_frame_onehot = torch.nn.functional.one_hot(\n            top_1_frame, num_classes=cosine_sim.shape[-1]\n        )\n\n        # We want to select frames based on threshold, but also make sure\n        # that at least one frame (the most similar one, whatever the value) is chosen\n        cosine_sim.index_add_(\n            0,\n            torch.arange(0, cosine_sim.shape[0]).type(torch.int64),\n            top_1_frame_onehot.type(cosine_sim.type()),\n        )\n\n        mask = torch.ones(cosine_sim.size())\n        above_threshold_9 = (cosine_sim >= 0.9) * mask\n        above_threshold_75 = (cosine_sim >= 0.75) * mask\n\n        indices = torch.repeat_interleave(\n            torch.arange(0, cosine_sim.shape[0]).view(cosine_sim.shape[0], 1),\n            cosine_sim.shape[1],\n            dim=1,\n        ).view(-1)\n        \n\n        # Average cosine similarity between top1 frame and target\n        cnn_cos_scores = []\n        for ind, top1_ind in enumerate(\n            np.array(\n                torch.topk(predictions[\"frame_scores\"].cpu(), dim=-1, k=1)[1].view(-1)\n            )\n        ):\n            if self.hparams.use_image_effnet:\n                _cos1 = self.cosine_sim(\n                    batch[\"tgt_img_features_effnet\"][ind],\n                    batch[\"src_img_features_effnet\"][ind][top1_ind],\n                )\n            if self.hparams.use_image_vit:\n                if self.args.env == 'whole':\n                    _cos2 = self.cosine_sim(\n                        batch[\"tgt_img_features_vit\"][ind],\n                        batch[\"src_img_features_vit\"][ind][top1_ind],\n                    )\n                else:\n                    _cos2 = self.cosine_sim(\n                        batch['tgt_img_features_vit'][ind],\n                        batch[\"src_img_features_vit\"][ind][0],\n                    )\n                if self.hparams.use_image_effnet:\n                    cnn_cos_scores.append((_cos1 + _cos2) / 2)\n                else:\n                    cnn_cos_scores.append(_cos2)\n            else:\n                cnn_cos_scores.append(_cos1)\n\n        # Correlation between model predictions and frame-scores\n        # We consider only the non-masked values\n        pearson_r_scores = []\n        kendall_tau_scores = []\n       \n        for ind in range(predictions['frame_scores'].shape[0]):\n            _nonmasked_vals = (\n                (predictions[\"frame_scores\"][ind] == -1e6).nonzero().squeeze().view(-1)\n            )\n            \n            if _nonmasked_vals.shape[0]:\n                _nonmasked_ind = _nonmasked_vals[0]\n                \n                try:\n                    pearsonr_score = pearsonr(\n                                        cosine_sim_raw[ind][:_nonmasked_ind].numpy(),\n                                        predictions[\"frame_scores\"][ind][:_nonmasked_ind]\n                                        .cpu()\n                                        .detach()\n                                        .numpy(),\n                                        )[0]\n                    if math.isnan(pearsonr_score):\n                        pearson_r_scores.append(0)\n                    else:    \n                        pearson_r_scores.append(pearsonr_score)\n                except:\n                    pearson_r_scores.append(0)\n                kendall_tau_scores.append(\n                    kendalltau(\n                        cosine_sim_raw[ind][:_nonmasked_ind].numpy(),\n                        predictions[\"frame_scores\"][ind][:_nonmasked_ind]\n                        .cpu()\n                        .detach()\n                        .numpy(),\n                    )[0]\n                )\n            else:\n                pearson_r_scores.append(\n                    pearsonr(\n                        cosine_sim_raw[ind].numpy(),\n                        predictions[\"frame_scores\"][ind].cpu().detach().numpy(),\n                    )[0]\n                )\n\n                kendall_tau_scores.append(\n                    kendalltau(\n                        cosine_sim_raw[ind].numpy(),\n                        predictions[\"frame_scores\"][ind].cpu().detach().numpy(),\n                    )[0]\n                )\n\n        rMAP_top = self.rMAP(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=top_1_frame_onehot.view(-1),\n            indexes=indices,\n        )\n        rMAP_threshold_9 = self.rMAP(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=above_threshold_9.view(-1),\n            indexes=indices,\n        )\n        rMAP_threshold_75 = self.rMAP(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=above_threshold_75.view(-1),\n            indexes=indices,\n        )\n        rRec_1_top = self.rRec_1(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=top_1_frame_onehot.view(-1),\n            indexes=indices,\n        )\n        rRec_5_top = self.rRec_5(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=top_1_frame_onehot.view(-1),\n            indexes=indices,\n        )\n        rRec_10_top = self.rRec_10(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=top_1_frame_onehot.view(-1),\n            indexes=indices,\n        )\n        rPre_1_top = self.rPre_1(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=top_1_frame_onehot.view(-1),\n            indexes=indices,\n        )\n        rPre_5_top = self.rPre_5(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=top_1_frame_onehot.view(-1),\n            indexes=indices,\n        )\n        rPre_10_top = self.rPre_10(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=top_1_frame_onehot.view(-1),\n            indexes=indices,\n        )\n        rRec_1_threshold_9 = self.rRec_1(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=above_threshold_9.view(-1),\n            indexes=indices,\n        )\n        rRec_1_threshold_75 = self.rRec_1(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=above_threshold_75.view(-1),\n            indexes=indices,\n        )\n        rRec_5_threshold_9 = self.rRec_5(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=above_threshold_9.view(-1),\n            indexes=indices,\n        )\n        rRec_5_threshold_75 = self.rRec_5(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=above_threshold_75.view(-1),\n            indexes=indices,\n        )\n        rRec_10_threshold_9 = self.rRec_10(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=above_threshold_9.view(-1),\n            indexes=indices,\n        )\n        rRec_10_threshold_75 = self.rRec_10(\n            preds=predictions[\"frame_scores\"].cpu().view(-1),\n            target=above_threshold_75.view(-1),\n            indexes=indices,\n        )\n        \n        rmse_scores = []\n        psnr_scores = []\n        ssim_scores = []\n        sre_scores = []\n        \n        for i in range(batch['tgt_img_features_vit'].shape[0]):\n            flattened_tensor = predictions['encoder_hidden_state'][i].flatten()\n            subsampled_tensor = flattened_tensor[torch.randperm(flattened_tensor.numel())[:2048]]\n            pred_img = subsampled_tensor.reshape(32, 64).unsqueeze(2)\n            orig_img = batch['tgt_img_features_vit'][i].reshape(32, 64).unsqueeze(2)\n            rmse_scores.append(rmse(org_img=orig_img.cpu().numpy(), pred_img=pred_img.cpu().numpy()))\n            psnr_scores.append(psnr(org_img=orig_img.cpu().numpy(), pred_img=pred_img.cpu().numpy()))\n            ssim_scores.append(ssim(org_img=orig_img.cpu().numpy(), pred_img=pred_img.cpu().numpy()))\n            sre_scores.append(sre(org_img=orig_img.cpu().numpy(), pred_img=pred_img.cpu().numpy()))\n            \n        rmse_score = sum(rmse_scores)/len(rmse_scores)\n        psnr_score = sum(psnr_scores)/len(psnr_scores)\n        ssim_score = sum(ssim_scores)/len(ssim_scores)\n        sre_score = sum(sre_scores)/len(sre_scores)\n        \n        \n        \n        \n        return {\n            \"hyp\": predictions[\"hyp\"],\n            \"ref\": batch[\"tgt\"],\n            \"cnn_cos_scores\": np.mean([_score.cpu() for _score in cnn_cos_scores]),\n            \"mAP_top\": rMAP_top.numpy(),\n            \"mAP_threshold_0.9\": rMAP_threshold_9.numpy(),\n            \"mAP_threshold_0.75\": rMAP_threshold_75.numpy(),\n            \"Rec_1_top\": rRec_1_top.numpy(),\n            \"Rec_5_top\": rRec_5_top.numpy(),\n            \"Rec_10_top\": rRec_10_top.numpy(),\n            \"Pre_1_top\": rPre_1_top.numpy(),\n            \"Pre_5_top\": rPre_5_top.numpy(),\n            \"Pre_10_top\": rPre_10_top.numpy(),\n            \"Rec_1_threshold_0.9\": rRec_1_threshold_9.numpy(),\n            \"Rec_5_threshold_0.75\": rRec_5_threshold_75.numpy(),\n            \"Rec_10_threshold_0.9\": rRec_10_threshold_9.numpy(),\n            \"Rec_1_threshold_0.75\": rRec_1_threshold_75.numpy(),\n            \"Rec_5_threshold_0.9\": rRec_5_threshold_9.numpy(),\n            \"Rec_10_threshold_0.75\": rRec_10_threshold_75.numpy(),\n            \"Pearson_r\": np.mean(pearson_r_scores),\n            \"Kendall_tau\": np.mean(kendall_tau_scores),\n            'RMSE_score' : rmse_score,\n            'PSNR_score' : psnr_score,\n            'SSIM_score' : ssim_score,\n            'SRE_score'  : sre_score,\n            'encoder_hidden_state' : predictions['encoder_hidden_state'],\n        }\n\n    def validation_epoch_end(self, outputs):\n        # Frame selection evaluation\n        cnn_cos_scores = np.mean([_r[\"cnn_cos_scores\"] for _r in outputs])\n        mAP_top = np.mean([_r[\"mAP_top\"] for _r in outputs])\n        mAP_threshold_9 = np.mean([_r[\"mAP_threshold_0.9\"] for _r in outputs])\n        mAP_threshold_75 = np.mean([_r[\"mAP_threshold_0.75\"] for _r in outputs])\n        Rec_1_top = np.mean([_r[\"Rec_1_top\"] for _r in outputs])\n        Rec_5_top = np.mean([_r[\"Rec_5_top\"] for _r in outputs])\n        Rec_10_top = np.mean([_r[\"Rec_10_top\"] for _r in outputs])\n        Pre_1_top = np.mean([_r[\"Pre_1_top\"] for _r in outputs])\n        Pre_5_top = np.mean([_r[\"Pre_5_top\"] for _r in outputs])\n        Pre_10_top = np.mean([_r[\"Pre_10_top\"] for _r in outputs])\n        Rec_1_threshold_9 = np.mean([_r[\"Rec_1_threshold_0.9\"] for _r in outputs])\n        Rec_1_threshold_75 = np.mean([_r[\"Rec_1_threshold_0.75\"] for _r in outputs])\n        Rec_5_threshold_9 = np.mean([_r[\"Rec_5_threshold_0.9\"] for _r in outputs])\n        Rec_5_threshold_75 = np.mean([_r[\"Rec_5_threshold_0.75\"] for _r in outputs])\n        Rec_10_threshold_9 = np.mean([_r[\"Rec_10_threshold_0.9\"] for _r in outputs])\n        Rec_10_threshold_75 = np.mean([_r[\"Rec_10_threshold_0.75\"] for _r in outputs])\n        Pearson_r = np.mean([_r[\"Pearson_r\"] for _r in outputs])\n        Kendall_tau = np.mean([_r[\"Kendall_tau\"] for _r in outputs])\n        PSNR = np.mean([_r[\"PSNR_score\"] for _r in outputs])\n        RMSE = np.mean([_r[\"RMSE_score\"] for _r in outputs])\n        SSIM = np.mean([_r[\"SSIM_score\"] for _r in outputs])\n        SRE = np.mean([_r[\"SRE_score\"] for _r in outputs])\n        \n        \n        self.log('PSNR', PSNR, on_step = False, on_epoch = True, sync_dist = True)\n        self.log('RMSE', RMSE, on_step = False, on_epoch = True, sync_dist = True)\n        self.log('SSIM', SSIM, on_step = False, on_epoch = True, sync_dist = True)\n        self.log('SRE', SRE, on_step = False, on_epoch = True, sync_dist = True)\n\n        self.log(\n            \"cnn_cos_scores\",\n            cnn_cos_scores,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\"Pearson_r\", Pearson_r, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\n            \"Kendall_tau\", Kendall_tau, on_step=False, on_epoch=True, sync_dist=True\n        )\n        self.log(\"mAP_top\", mAP_top, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\n            \"mAP_threshold_0.9\",\n            mAP_threshold_9,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"mAP_threshold_0.75\",\n            mAP_threshold_75,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\"Rec_1_top\", Rec_1_top, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\"Rec_5_top\", Rec_5_top, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\"Rec_10_top\", Rec_10_top, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\"Pre_1_top\", Pre_1_top, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\"Pre_5_top\", Pre_5_top, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\"Pre_10_top\", Pre_10_top, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\"F1_1_top\", (2*Pre_1_top*Rec_1_top)/(Pre_1_top+Rec_1_top), on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\"F1_5_top\", (2*Pre_5_top*Rec_5_top)/(Pre_5_top+Rec_5_top), on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\"F1_10_top\", (2*Pre_10_top*Rec_10_top)/(Pre_10_top+Rec_10_top), on_step=False, on_epoch=True, sync_dist=True)\n\n        self.log(\n            \"Rec_1_threshold_0.9\",\n            Rec_1_threshold_9,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"Rec_1_threshold_0.75\",\n            Rec_1_threshold_75,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"Rec_5_threshold_0.9\",\n            Rec_5_threshold_9,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"Rec_5_threshold_0.75\",\n            Rec_5_threshold_75,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"Rec_10_threshold_0.9\",\n            Rec_10_threshold_9,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"Rec_10_threshold_0.75\",\n            Rec_10_threshold_75,\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        # Text summarization evaluation\n        predictions = [sent for _item in outputs for sent in _item[\"hyp\"]]\n        refs = [sent for _item in outputs for sent in _item[\"ref\"]]\n\n        bleu_score = self.sacrebleu.corpus_score(predictions, refs).score\n        rouge_score = self.rouge.corpus(refs, predictions)\n        bert_score = bertscore.compute(predictions = predictions, references = refs, lang=\"en\")\n        refs = [[s] for s in refs]\n        meteor = meteor_score(refs, predictions)\n        b_p = mean(bert_score['precision'])\n        b_f1 = mean(bert_score['f1'])\n        b_r = mean(bert_score['recall'])\n        \n        self.log('BERTSCORE_F1', b_f1, on_step = False, on_epoch=True, sync_dist=True)\n        self.log('BERTSCORE_P', b_p, on_step = False, on_epoch=True, sync_dist=True)\n        self.log('BERTSCORE_R', b_r, on_step = False, on_epoch=True, sync_dist=True)\n        \n        self.log('METEOR', meteor, on_step = False, on_epoch=True, sync_dist=True)\n        \n        refs = {str(i): r for i, r in enumerate(refs)}\n        preds = {str(i): [clean_text(p)] for i, p in enumerate(predictions)}\n\n        try:\n            spice_score, _ = spice_scorer.compute_score(refs, preds)\n        except:\n            spice_score = 0\n        \n        cider_score, _ = cider_scorer.compute_score(refs, preds)\n        \n        self.log('SPICE', spice_score, on_step = False, on_epoch=True, sync_dist=True)\n        self.log('CIDER', cider_score, on_step = False, on_epoch=True, sync_dist=True)\n        \n        self.log(\"BLEU\", bleu_score, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(\n            \"ROUGE_RAW_1_P\",\n            round(100 * rouge_score[\"1\"].p, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"ROUGE_RAW_1_R\",\n            round(100 * rouge_score[\"1\"].r, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"ROUGE_RAW_1_F\",\n            round(100 * rouge_score[\"1\"].f, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        self.log(\n            \"ROUGE_RAW_2_P\",\n            round(100 * rouge_score[\"2\"].p, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"ROUGE_RAW_2_R\",\n            round(100 * rouge_score[\"2\"].r, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"ROUGE_RAW_2_F\",\n            round(100 * rouge_score[\"2\"].f, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n        self.log(\n            \"ROUGE_RAW_L_P\",\n            round(100 * rouge_score[\"L\"].p, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"ROUGE_RAW_L_R\",\n            round(100 * rouge_score[\"L\"].r, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n        self.log(\n            \"ROUGE_RAW_L_F\",\n            round(100 * rouge_score[\"L\"].f, 2),\n            on_step=False,\n            on_epoch=True,\n            sync_dist=True,\n        )\n\n    def configure_optimizers(self):\n        # Enable different learning rate for pre-trained weights\n        model_params = list(self.model.named_parameters())\n\n        def is_mms(n):\n            return \"mms_\" in n\n\n        grouped_parameters = [\n            {\"params\": [p for n, p in model_params if is_mms(n)]},\n            {\"params\": [p for n, p in model_params if not is_mms(n)]},\n        ]\n\n        return torch.optim.Adam(\n            grouped_parameters,\n            lr=self.hparams.lr_init_val,\n            betas=(0.9, 0.98),\n            eps=1e-09,\n        )\n\n    def optimizer_step(\n        self,\n        epoch,\n        batch_idx,\n        optimizer,\n        optimizer_idx,\n        optimizer_closure,\n        on_tpu=False,\n        using_native_amp=False,\n        using_lbfgs=False,\n    ):\n        optimizer.step(closure=optimizer_closure)\n        # Linear increase for the first warmup_steps, then inverse square root decrease\n        if self.trainer.global_step < self.hparams.lr_warmup_steps:\n            lr = self.hparams.lr_init_val + self.trainer.global_step * (\n                (self.hparams.lr_max_val - self.hparams.lr_init_val)\n                / self.hparams.lr_warmup_steps\n            )\n        else:\n            lr = (\n                self.hparams.lr_max_val\n                * self.hparams.lr_warmup_steps**0.5\n                * self.trainer.global_step**-0.5\n            )\n        lr_mms = lr\n        lr_text = lr\n\n        if (\n            self.hparams.start_with_text_frozen\n            and self.trainer.current_epoch < self.hparams.start_with_text_frozen\n        ):\n            lr_text = 0.0\n\n        # MMS params\n        optimizer.param_groups[0][\"lr\"] = lr_mms\n        # Pre-trained text params\n        optimizer.param_groups[1][\"lr\"] = lr_text\n\n        self.log(\"learning_rate_mms\", lr_mms, on_step=True, on_epoch=False)\n\n        self.log(\"learning_rate_text\", lr_text, on_step=True, on_epoch=False)", ""]}
{"filename": "MultiSum/src/model/rouge_raw.py", "chunked_list": ["#!/usr/bin/env python3\n#\n# This file is part of SumeCzech corpus <http://hdl.handle.net/11234/1-2615>.\n#\n# Copyright 2018 Institute of Formal and Applied Linguistics, Faculty of\n# Mathematics and Physics, Charles University in Prague, Czech Republic.\n#\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.", "# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nimport re\n\nclass RougeRaw:\n    \"\"\"Compute RougeRAW-1, RougeRAW-2, RougeRAW-L metrics.\"\"\"\n\n    class FScore:\n        \"\"\"F1 score representation.\"\"\"\n        def __init__(self, correct, gold, system):\n            self.p = correct / system if system else 0.\n            self.r = correct / gold if gold else 0.\n            self.f = 2 * correct / (system + gold) if system + gold else 0.\n\n    def _rouge_n(self, n, gold_words, system_words):\n        \"\"\"Compute Rouge-n for given words.\"\"\"\n        def n_grams(n, words):\n            ngrams = {}\n            total = 0\n            for i in range(len(words) - n + 1):\n                ngram = \"\\t\".join(words[i:i + n])\n                ngrams[ngram] = 1 + ngrams.get(ngram, 0)\n                total += 1\n            return ngrams, total\n\n        gold_ngrams, gold_total = n_grams(n, gold_words)\n        system_ngrams, system_total = n_grams(n, system_words)\n\n        intersection = 0\n        for ngram in system_ngrams:\n            intersection += min(system_ngrams[ngram], gold_ngrams.get(ngram, 0))\n\n        return self.FScore(intersection, gold_total, system_total)\n\n    def _rouge_l(self, gold_words, system_words):\n        \"\"\"Compute Rouge-L for given words.\"\"\"\n        lcs = [[0] * len(system_words) for _ in gold_words]\n        for r in range(len(gold_words)):\n            for s in range(len(system_words)):\n                if gold_words[r] == system_words[s]:\n                    lcs[r][s] = 1 + (lcs[r - 1][s - 1] if r and s else 0)\n                lcs[r][s] = max(lcs[r][s], lcs[r - 1][s] if r else 0)\n                lcs[r][s] = max(lcs[r][s], lcs[r][s - 1] if s else 0)\n\n        return self.FScore(lcs[-1][-1], len(gold_words), len(system_words))\n\n    def _tokenize(self, text):\n        \"\"\"Tokenize given text.\"\"\"\n        return re.sub(r\"\\s+\", \" \", re.sub(r\"\\b\", \" \", text, re.UNICODE), re.UNICODE).strip().split(\" \")\n\n    def document(self, gold, system):\n        \"\"\"Compute RougeRAW-1, RougeRAW-2, RougeRAW-L for given documents.\n\n        Each document should be a string.\n        \"\"\"\n\n        assert isinstance(gold, str) and isinstance(system, str), \"Expected string arguments\"\n\n        lc_gold_words = [word.lower() for word in self._tokenize(gold)]\n        lc_system_words = [word.lower() for word in self._tokenize(system)]\n\n        return {\n            \"1\": self._rouge_n(1, lc_gold_words, lc_system_words),\n            \"2\": self._rouge_n(2, lc_gold_words, lc_system_words),\n            \"L\": self._rouge_l(lc_gold_words, lc_system_words),\n        }\n\n    def corpus(self, gold, system):\n        \"\"\"Compute RougeRAW-1, RougeRAW-2, RougeRAW-L for given corpora.\n\n        Each corpus should be a collection of documents, each document a string.\n        \"\"\"\n\n        assert isinstance(gold, list) and isinstance(system, list), \"Expected list arguments\"\n        assert len(gold) == len(system), \"Given corpora should be of the same length\"\n\n        rouge = {key: self.FScore(0, 0, 0) for key in [\"1\", \"2\", \"L\"]}\n\n        if len(gold):\n            for gold_document, system_document in zip(gold, system):\n                for key, value in self.document(gold_document, system_document).items():\n                    rouge[key].p += value.p\n                    rouge[key].r += value.r\n                    rouge[key].f += value.f\n\n            for key in rouge:\n                rouge[key].p /= len(gold)\n                rouge[key].r /= len(gold)\n                rouge[key].f /= len(gold)\n\n        return rouge", "\n\nif __name__ == \"__main__\":\n    import argparse\n    import json\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"gold\", type=str, help=\"Gold jsonl file path\")\n    parser.add_argument(\"system\", type=str, help=\"System jsonl output file\")\n    parser.add_argument(\"field\", type=str, help=\"Which jsonl field to compare\")\n    args = parser.parse_args()\n\n    gold = []\n    with open(args.gold, \"r\", encoding=\"utf-8\") as gold_file:\n        for gold_line in gold_file:\n            gold.append(json.loads(gold_line)[args.field])\n\n    system = []\n    with open(args.system, \"r\", encoding=\"utf-8\") as system_file:\n        for system_line in system_file:\n            system.append(json.loads(system_line)[args.field])\n\n    rouge = RougeRaw().corpus(gold, system)\n    print(\"  RougeRAW-1      RougeRAW-2      RougeRAW-L\")\n    print(\"  P    R    F     P    R    F     P    R    F\")\n    for metric in [\"1\", \"2\", \"L\"]:\n        print(\"{:04.1f} {:04.1f} {:04.1f}{}\".format(\n            100 * rouge[metric].p,\n            100 * rouge[metric].r,\n            100 * rouge[metric].f,\n            \"\\n\" if metric == \"L\" else \"  \"), end=\"\")", ""]}
