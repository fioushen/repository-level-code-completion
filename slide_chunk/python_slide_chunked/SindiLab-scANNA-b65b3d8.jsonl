{"filename": "setup.py", "chunked_list": ["\"\"\"Setup file for package installation.\"\"\"\n\nfrom setuptools import setup, find_packages\n\nwith open(\"README.md\", mode=\"r\", encoding=\"utf-8\") as readme_file:\n    readme = readme_file.read()\n\nsetup(\n    name=\"scanna\",\n    version=\"0.0.2\",", "    name=\"scanna\",\n    version=\"0.0.2\",\n    author=\"A. Ali Heydari, Oscar Davalos\",\n    author_email=\"aliheydari@ucdavis.edu\",\n    description=\n    \"N-ACT: Automatic Cell-Type classification using Neural Attention models\",\n    long_description=readme,\n    long_description_content_type=\"text/markdown\",\n    license=\"MIT\",\n    url=\"https://github.com/SindiLab/scANNA\",", "    license=\"MIT\",\n    url=\"https://github.com/SindiLab/scANNA\",\n    download_url=\"https://github.com/SindiLab/scANNA\",\n    packages=find_packages(),\n    install_requires=[\n        \"tqdm>=4.47.0\",\n        \"adabelief-pytorch>=0.2.0\",\n        \"torch>=1.13\",\n        \"scanpy>=1.7.0\",\n        \"tensorboardX>=2.1\",", "        \"scanpy>=1.7.0\",\n        \"tensorboardX>=2.1\",\n        \"prettytable\",\n    ],\n    classifiers=[\n        \"Development Status :: 1 - Beta\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: MIT Software License\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\"", "        \"Programming Language :: Python :: 3.7\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\"\n        \":: Bioinformatics :: Deep Learning\"\n    ],\n    keywords=\n    \"Single Cell RNA-seq, Automatic Classification, Attention-Neural Networks,\"\n    \"Deep Learning, Transfer Learning\")\n"]}
{"filename": "scanna/__init__.py", "chunked_list": ["\"\"\"scANNA's first level imports from different modules.\"\"\"\nfrom .data_handling import *\nfrom .model import *\nfrom .attention_query_utilities import *\n# adding package information and version\ntry:\n    import importlib.metadata as importlib_metadata\nexcept ModuleNotFoundError:\n    import importlib_metadata\n", "\npackage_name = \"scanna\"\n__version__ = importlib_metadata.version(package_name)\n"]}
{"filename": "scanna/attention_query_utilities/__init__.py", "chunked_list": ["\"\"\"Second level import module for utilities regarding attention weights.\"\"\"\nfrom .attention_query import *\n"]}
{"filename": "scanna/attention_query_utilities/attention_query.py", "chunked_list": ["\"\"\"Implementations of AttentionQuery class for interpreting attention values.\"\"\"\n\nfrom anndata import AnnData\nfrom collections import Counter\nimport gseapy as gp\nfrom itertools import chain\nfrom ..model import ProjectionAttention, AdditiveModel\nimport numpy as np\nimport pandas as pd\nimport torch", "import pandas as pd\nimport torch\nfrom typing import List\nfrom ..utilities import sparse_to_dense\n\n\nclass AttentionQuery():\n    \"\"\" Class implementation for extracting and querying attention weights.\n\n    This class contains the core methods for extracting cluster-specific\n    attention weights, which are used for querying and interpretability.\n\n    Attributes:\n        data: The scanpy object that we want to make predictions on.\n          Note: The dataframe can be changed or retrived via the defined\n          \"setter\" and \"getter\" methods.\n        split: The split of the scanpy data we want to use, e.g. \"test\" split.\n        attention_weights: The attnetion weights extracted from the model.\n        predicted: Model predictions over the chosen split.\n        score: The gene score matrix.\n        attentive_genes: Top genes with the highest weights (attention values).\n        top_genes_df: A dataframe ranked based on the top attentive genes.\n\n    \"\"\"\n\n    def __init__(self,\n                 scanpy_object: AnnData,\n                 model: ProjectionAttention | AdditiveModel = None,\n                 split_test: bool = False,\n                 which_split: str = \"test\"):\n        \"\"\" Initializer of the AttentionQuery class.\n\n        Args:\n\n            scanpy_object: The scanpy object that contains the attention weights\n              and predicted cells.\n            model: The model we want to use to make predictions and extract\n              attention weights from.\n            split_test: A boolean indicating whether the user wants to get\n              attention weights for the entire data (when \"False\"), or just a\n              split (when set to \"True\").\n            which_split: The name of the split that we are interested in and\n              which exists in the AnnData.\n\n        \"\"\"\n        if split_test:\n            print(\"==> Splitting the data to 'test' only:\")\n            self.data = scanpy_object[scanpy_object.obs.split == which_split]\n        else:\n            self.data = scanpy_object\n\n        self.split = split_test\n        self._correct_predictions_only_flag = False\n        self.model = model\n        # Initializing the value for later use.\n        self.ranked_genes_per_clust = None\n\n    def get_gene_query(self,\n                       model: ProjectionAttention |\n                       AdditiveModel = None,\n                       number_of_query_genes: int = 100,\n                       local_scanpy_obj: AnnData = None,\n                       attention_type: str = \"additive\",\n                       inplace: bool = False,\n                       mode: str = \"tfidf\",\n                       correct_predictions_only: bool = True,\n                       use_raw_x: bool = True,\n                       verbose: bool = True):\n        \"\"\" Class method with automated worflow of getting query genes.\n\n        Args:\n            model: The model we want to use to make predictions and extract\n              attention weights from.\n            number_of_query_genes: An integer indicating the number of top\n              genes desired\n            local_scanpy_obj: An AnnData object that would locally replace the\n              scanpy object that was set in the constructor for the object.\n            attention_type: The type of attention the inputted model was\n              trained with. This will be 'additive' in most cases (even when\n              scANNA included projection blocks).\n            inplace: Wheather we want changes to be inplace, or on a copy (in\n              which case it will be returned.\n            correct_predictions_only: Whether to extract attention only from the\n              correct predictions or not.\n            use_raw_x: To use the \"adata.raw.X\" or just adata.X (depending on\n              preprocessing pipeline).\n            verbose: Whether the methods should print out their process or not.\n\n        Returns:\n            Based on the ranking method (i.e. TFIDF or averages), the method\n            will return:\n\n            (1) A dataframe with ranked gene names for each cluster\n            (2) A dictionary mapping cluster names to the attention dataframe\n                (the matrix of attention values for cells x genes)\n            (3) A dictionary mapping each cluster to a series containing a gene\n                list as index, mapped to the normalized attention values.\n\n        Raises:\n            None.\n        \"\"\"\n        # As the first step, we want to extract attention values for each gene.\n        _, _ = self.assign_attention(\n            model=model,\n            inplace=inplace,\n            local_scanpy_obj=local_scanpy_obj,\n            correct_predictions_only=correct_predictions_only,\n            attention_type=attention_type,\n            verbose=verbose,\n            use_raw_x=use_raw_x)\n\n        # Next, we calculate the top attentive genes in each cluster\n        (clust_to_att_dict, top_genes_to_df_dict,\n         top_n_names) = self.get_top_n_per_cluster(n=number_of_query_genes,\n                                                   model=model,\n                                                   mode=mode,\n                                                   verbose=verbose)\n\n        # Lastly, depending on the normalization method (e.g. TFIDF), we\n        # normalize the attention values and return the top attentive genes for\n        # querying.\n        if mode.lower() == \"tfidf\" or mode.lower() == \"tf-idf\":\n            tf_idf_df = self.calculate_tfidf(\n                pd.DataFrame.from_dict(top_n_names),\n                top_genes_to_df_dict,\n                n_genes=number_of_query_genes)\n            self.ranked_genes_per_clust_df = tf_idf_df\n\n            if not inplace:\n                return tf_idf_df, clust_to_att_dict, top_genes_to_df_dict\n        else:\n            self.ranked_genes_per_clust_df = pd.DataFrame.from_dict(top_n_names)\n            if not inplace:\n                return (pd.DataFrame.from_dict(top_n_names), clust_to_att_dict,\n                        top_genes_to_df_dict)\n\n    def make_enrichment_plots(\n        self,\n        number_of_genes_to_query: int = 50,\n        which_cluster: int | str = None,\n        clusters_to_names_dict: dict = None,\n        number_of_total_clusters: int = None,\n        species: str = \"Human\",\n        return_results: bool = False,\n        save: bool = False,\n        where_to_save_plots: str = None,\n        gene_sets: List[str] | str = \"default\",\n        verbose: bool = True,\n    ):\n        \"\"\" Class method with automated worflow of getting query genes.\n\n        Args:\n            number_of_genes_to_query: An integer indicating how many genes we\n              want to run enrichment test on (query)\n            which_cluster: An optional integer or string indicating which\n              cluster we want to annotate.\n            clusters_to_names_dict: An optional dictionary containing the\n              mapping between clusters and cell type names.\n            number_of_total_clusters: An integer indicating the total number of\n              clusters present in the data. This option should be provided when\n              \"which_cluster\" is set to None.\n            species: A string indicating the species that the sample is coming\n              from (e.g. Human or Mouse).\n            return_results: A boolean indicating whether we need the dataframe\n              generated by the enrichment test.\n            save: A boolean indicating whether we want to save results (e.g\n              plots) externally or not.\n            where_to_save_plots: An optional string consisting of the path for\n              where the results are saved. The string passed must be a path and\n              not including the name of the file (the ending string will be\n              treated as a directory).\n            gene_sets: Either a list of gene sets for the query (as strings), or\n              the value \"default\" which will use default gene sets.\n            verbose: A boolean for whether we want to print out a more complete\n              dialogue.\n\n        Returns:\n            Based on user preference, either returns \"None\" or a dataframe\n            containing the enrichmeht test results.\n\n        Raises:\n            ValueError: If neither of the arguments \"which_cluster\" nor\n              \"number_of_total_clusters\" are provided when calling the method.\n        \"\"\"\n        if gene_sets == \"default\":\n            # We use Azimuth as the base database for humans\n            if species.lower() == \"human\":\n                genesets = [\"Azimuth_Cell_Types_2021\"]\n            elif species.lower() == \"mouse\" or species.lower() == \"mice\":\n                # Making sure the species name is mouse so that it works with\n                # the EnrichR API\n                species = \"mouse\"\n                # We use Tabula Muris as the main database for mice\n                genesets = [\"Tabula_Muris\"]\n            # And, we add the following two data basis for additional\n            # verification\n            else:\n                print(\"==> Did not detect 'mice' or 'humans' as species.\"\n                      \" We highly encourage specifying the gene sets explicitly\"\n                      \" for other species (or specific use cases).\")\n                genesets = []\n\n            genesets.extend(\n                [\"CellMarker_Augmented_2021\", \"PanglaoDB_Augmented_2021\"])\n\n        if which_cluster is not None:\n            which_cluster = int(which_cluster)\n            cluster = \"Cluster_\" + f\"{which_cluster}\"\n            if clusters_to_names_dict is None:\n                clusters_to_names_dict = (f\"{cluster}:Unidentified Celltype\"\n                                          f\"{which_cluster}\")\n            try:\n                gene_list = self.ranked_genes_per_clust_df[\n                    f\"{cluster}_genes\"].tolist()[:number_of_genes_to_query]\n            except KeyError as _:\n                gene_list = self.ranked_genes_per_clust_df[f\"{cluster}\"].tolist(\n                )[:number_of_genes_to_query]\n            if verbose:\n                print(\"==> Performing enrichment test on top \"\n                      f\"{number_of_genes_to_query} genes.\")\n            if save:\n                if where_to_save_plots is None:\n                    where_to_save_plots = \"./Enrichment_Plots/\"\n                else:\n                    where_to_save_geneset_plots = (\n                        f\"{where_to_save_plots}\"\n                        f\"{clusters_to_names_dict[cluster]}\"\n                        \"_geneset_score.png\")\n\n                    where_to_save_combine_score_plots = (\n                        f\"{where_to_save_plots}\"\n                        f\"{clusters_to_names_dict[cluster]}\"\n                        \"_combined_score.png\")\n            else:\n                # Making sure even if the user has provided a path, EnrichR does\n                # not save any results.\n                where_to_save_plots = None\n                where_to_save_geneset_plots = None\n                where_to_save_combine_score_plots = None\n            # Perforning the erichment test using EnrichR\n            enrich_test = gp.enrichr(\n                gene_list=gene_list,\n                gene_sets=genesets,\n                organism=species,\n                outdir=where_to_save_plots,\n            )\n            # cleaning up the string results before plotting\n            enrich_test = self._clean_enrichment_results(enrich_test)\n            print(type(enrich_test))\n            if verbose:\n                print(\"    -> Results per gene set:\")\n            _ = gp.dotplot(\n                enrich_test.results,\n                column=\"Adjusted P-value\",\n                x=\"Gene_set\",\n                size=6,\n                top_term=5,\n                figsize=(4, 6),\n                title=f\"Enrichment for {clusters_to_names_dict[cluster]}\",\n                ofname=where_to_save_geneset_plots,\n                xticklabels_rot=45,\n                show_ring=True,\n                marker=\"o\",\n            )\n\n            if verbose:\n                print(\"    -> Results for combine score:\")\n\n            _ = gp.dotplot(\n                enrich_test.results,\n                column=\"Adjusted P-value\",\n                x=\"Combine Score\",\n                size=6,\n                top_term=5,\n                figsize=(4, 6),\n                title=f\"Enrichment for {clusters_to_names_dict[cluster]}\",\n                ofname=where_to_save_combine_score_plots,\n                xticklabels_rot=45,\n                show_ring=True,\n                marker=\"o\",\n            )\n            if return_results:\n                return enrich_test.results\n\n        else:\n            if number_of_total_clusters is None:\n                raise ValueError(\n                    \"Please provide either the cluster of interest\"\n                    \" ('which_cluster' arg) or the total number of\"\n                    \" clusters ('number_of_total_clusters' arg) for\"\n                    \" the enrichment analysis.\")\n            for cluster in range(number_of_total_clusters):\n                print(f\"==> Analysis for cluster {cluster}:\")\n                self.make_enrichment_plots(\n                    number_of_genes_to_query=number_of_genes_to_query,\n                    which_cluster=cluster,\n                    return_results=False,\n                    clusters_to_names_dict=clusters_to_names_dict,\n                    number_of_total_clusters=number_of_total_clusters,\n                    species=species,\n                    save=save,\n                    where_to_save_plots=where_to_save_plots,\n                    verbose=verbose)\n            print(\">-< Done.\")\n\n    def get_important_global_genes(self,\n                                   model: ProjectionAttention |\n                                   AdditiveModel = None,\n                                   how_many_global_genes: int = 50,\n                                   split_data: bool = True,\n                                   local_scanpy_obj: AnnData = None,\n                                   attention_type: str = \"additive\",\n                                   inplace: bool = False,\n                                   rank_mode: str = \"mean\",\n                                   correct_predictions_only: bool = True,\n                                   use_raw_x: bool = True,\n                                   verbose: bool = True):\n        \"\"\" Class method with automated worflow of getting query genes.\n\n        Args:\n            model: The model we want to use to make predictions and extract\n              attention weights from.\n            how_many_global_genes : An integer indicating the number of top\n              genes desired\n            split_data: A boolean indicating whether the annotated data should\n              be splitted into train and test.\n            local_scanpy_obj: An AnnData object that would locally replace the\n              scanpy object that was set in the constructor for the object.\n            attention_type: The type of attention the inputted model was\n              trained with. This will be 'additive' in most cases (even when\n              scANNA included projection blocks).\n            inplace: Wheather we want changes to be inplace, or on a copy (in\n              which case it will be returned.\n            correct_predictions_only: Whether to extract attention only from the\n              correct predictions or not.\n            use_raw_x: To use the \"adata.raw.X\" or just adata.X (depending on\n              preprocessing pipeline).\n            verbose: Whether the methods should print out their process or not.\n\n        Returns:\n            Based on the data splitting (\"data_split\" arg set to true or false),\n            the method will return two different set of outputs:\n\n            (1) If \"data_split\" is True, then the method will output six\n                analysis outputs based on the most important genes identified.\n                These outputs consist of values, labels and cell type names for\n                train and test split (equalling to six).\n            (2) If \"data_split\" is True then the returned outputs are the\n                values, labels and cell type names for all cells (thus three\n                outputs).\n\n        Raises:\n            None.\n        \"\"\"\n        # As the first step, we want to extract attention values for each gene.\n        att_adata, _ = self.assign_attention(\n            model=model,\n            inplace=inplace,\n            local_scanpy_obj=local_scanpy_obj,\n            correct_predictions_only=correct_predictions_only,\n            attention_type=attention_type,\n            verbose=verbose,\n            use_raw_x=use_raw_x)\n\n        # Next, we extract the top attentive gene names across all cells\n        top_global_genes = self.get_top_n(n=how_many_global_genes,\n                                          rank_mode=rank_mode,\n                                          verbose=verbose).index.tolist()\n        # Now subsetting the dataset to only include the top genes\n        top_genes_subset_adata = att_adata[:, top_global_genes]\n\n        if split_data:\n            top_genes_subset_train = top_genes_subset_adata[\n                top_genes_subset_adata.obs.split == \"train\"]\n            top_genes_subset_test = top_genes_subset_adata[\n                top_genes_subset_adata.obs.split == \"test\"]\n            # Separating diffrent components for further analysis and validation\n            subset_train = sparse_to_dense(top_genes_subset_train)\n            subset_labels_train = top_genes_subset_train.obs.cluster.to_numpy()\n            subset_names_train = list(\n                top_genes_subset_train.obs.celltypes.to_numpy())\n\n            subset_test = sparse_to_dense(top_genes_subset_test)\n            subset_labels_test = top_genes_subset_test.obs.cluster.to_numpy()\n            subset_names_test = list(\n                top_genes_subset_test.obs.celltypes.to_numpy())\n\n            return (subset_train, subset_test, subset_labels_train,\n                    subset_labels_test, subset_names_train, subset_names_test)\n\n        else:\n            # Separating diffrent components for further analysis and validation\n            subset_all = sparse_to_dense(top_genes_subset_adata)\n            subset_labels_all = top_genes_subset_adata.obs.cluster.to_numpy()\n            subset_names_all = list(\n                top_genes_subset_adata.obs.celltypes.to_numpy())\n\n            return subset_all, subset_labels_all, subset_names_all\n\n    def assign_attention(self,\n                         model: ProjectionAttention |\n                         AdditiveModel = None,\n                         local_scanpy_obj: AnnData = None,\n                         attention_type: str = \"additive\",\n                         inplace: bool = False,\n                         correct_predictions_only: bool = True,\n                         use_raw_x: bool = True,\n                         device: str = \"infer\",\n                         verbose: bool = True):\n        \"\"\" The method to assign attention score to a scanpy object.\n\n        Args:\n            model: The model we want to use to make predictions and extract\n              attention weights from.\n            local_scanpy_obj: An AnnData object that would locally replace the\n              scanpy object that was set in the constructor for the object.\n            attention_type: The type of attention the inputted model was\n              trained with. This will be 'additive' in most cases (even when\n              scANNA included projection blocks).\n            inplace: Wheather we want changes to be inplace, or on a copy (in\n              which case it will be returned.\n            correct_predictions_only: Whether to extract attention only from the\n              correct predictions or not.\n            use_raw_x: To use the \"adata.raw.X\" or just adata.X (depending on\n              preprocessing pipeline).\n            verbose: Whether the methods should print out their process or not.\n\n        Returns:\n            The method will return:\n\n            (1) A scanpy object with predictions and attention weights as\n                added keys to the AnnData object.\n\n            (2) A dataframe consisting of genes as columns and attention weights\n                as the corresponding row values.\n\n        Raises:\n            ValueError: An error occured during reading the trained model.\n\n        \"\"\"\n        if model is None and self.model is None:\n            raise ValueError(\"Please provide a model for making predictions and\"\n                             \" extracting attention weights.\")\n        elif model is None and self.model is not None:\n            model = self.model\n\n        if device == \"infer\":\n            device = torch.device(\n                \"cuda\" if torch.cuda.is_available() else \"cpu\")\n        # set the flag\n        self.attention_weights = True\n\n        if local_scanpy_obj is None:\n            test_data = self.data.copy()\n            if verbose:\n                print(\"*Caution*: The method is running on the entire data.\"\n                      \" If this is not what you want, provide scanpy\"\n                      \"object.\")\n        else:\n            test_data = local_scanpy_obj\n\n        if use_raw_x:\n            test_tensor = torch.from_numpy(sparse_to_dense(test_data.raw))\n        else:\n            test_tensor = torch.from_numpy(sparse_to_dense(test_data))\n\n        if verbose:\n            print(\"==> Calling forward:\")\n\n        model.eval()\n\n        if verbose:\n            print(\"    -> Making predictions\")\n\n        with torch.no_grad():\n            model.to(device)\n            logits, score, attentive_genes = model(\n                test_tensor.float().to(device), training=False)\n            _, predicted = torch.max(logits.squeeze(), 1)\n\n        # If this call is for the entirety of the data we have, then we should\n        # assign all cells, otherwise we would get a dimension error if we are\n        # considering a subset.\n        predicted = predicted.detach().cpu().numpy()\n        score = score.detach().cpu().numpy()\n        if attention_type == \"multi-headed\":\n            score = score.reshape(int(score.shape[0] / 8), 8)\n        attentive_genes = attentive_genes.detach().cpu().numpy()\n\n        if local_scanpy_obj is None:\n            self.predicted = predicted\n            self.score = score\n            self.attentive_genes = attentive_genes\n            if verbose:\n                print(\"    -> Assigning attention weights globally: \")\n            test_data.obsm[\"attention\"] = score  #attentive_genes\n            predicted_str = [f\"{i}\" for i in predicted]\n            test_data.obs[\"prediction\"] = predicted_str\n            test_data.obs[\"prediction\"] = test_data.obs[\"prediction\"].astype(\n                \"str\"\n            )  # changed to str from category since it was causing issues\n            # adding a check for the correct data type in the cluster column\n            if test_data.obs[\"cluster\"].dtype != str:\n                test_data.obs[\"cluster\"] = test_data.obs[\"cluster\"].astype(\n                    \"str\")\n\n        if correct_predictions_only:\n            if verbose:\n                print(\"    -> **Returning only the correct predictions**\")\n            test_data = test_data[test_data.obs[\"cluster\"] ==\n                                  test_data.obs[\"prediction\"]]\n            # We will use this adata later on instead if we only want to look\n            # at the correct predictions.\n            self.correct_pred_adata = test_data\n            self._correct_predictions_only_flag = True\n\n        if verbose:\n            print(\"    -> Creating a [Cells x Attention Per Gene] DataFrame\")\n\n        try:\n            att_df = pd.DataFrame(test_data.obsm[\"attention\"].values,\n                                  index=test_data.obs.index,\n                                  columns=test_data.var.gene_ids.index)\n        except AttributeError as _:\n            att_df = pd.DataFrame(test_data.obsm[\"attention\"],\n                                  index=test_data.obs.index,\n                                  columns=test_data.var.index)\n\n        if local_scanpy_obj is None:\n            self.att_df = att_df\n\n        if inplace:\n            if verbose:\n                print(\n                    \"    -> Making all changes inplace and returning input data\"\n                    \" with changes\")\n            self.data = test_data\n            return self.self.data, att_df\n\n        else:\n            if verbose:\n                print(\"    -> Returning the annData with the attention weights\")\n            return test_data, att_df\n\n    def get_top_n(self,\n                  n: int = 100,\n                  dataframe: pd.DataFrame = None,\n                  rank_mode: str = None,\n                  verbose: bool = True):\n        \"\"\" Class method for getting the top n genes for the entire dataset.\n\n        Args:\n            n: An integer indicating the number of top genes desired.\n            dataframe: The dataframe we want to find the top n Genes in.\n            rank_mode: The mode we want to use for ranking top \"n\" genes.\n            verbose: If we want to print out a complete dialogue.\n\n        Returns:\n            A dataframe containing the top n genes with the shape cells x genes.\n\n        Raises:\n           NotImplementedError: An error occured if \"rank_mode\" argument is not\n             one of the existing modes.\n\n        \"\"\"\n\n        if not hasattr(self, \"attention_weights\"):\n            print(\"Please first set the attention weights by calling\"\n                  \"AssignAttention()\")\n            return 0\n\n        if verbose:\n            print(f\"==> Getting Top {n} genes for all cells in the original\"\n                  \" data\")\n            print(\"    -> Be cautious as this may not be cluster specific.\"\n                  \" If you want cluster specific, pleae call\"\n                  \" 'get_top_n_per_cluster()' method.'\")\n\n        # make it to be genes x cells\n        if dataframe is None:\n            att_df_trans = self.att_df.T\n\n        else:\n            att_df_trans = dataframe.T\n\n        # Finding n largest genes (features based on the mode:\n        if rank_mode is not None:\n            if verbose:\n                print(f\"    -> Ranking mode: {rank_mode}\")\n            if rank_mode.lower() == \"mean\":\n                top_genes_transpose = att_df_trans.loc[att_df_trans.sum(\n                    axis=1).nlargest(n, keep=\"all\").index]\n\n            elif rank_mode.lower() == \"nlargest\":\n                top_genes_transpose = att_df_trans.nlargest(\n                    n, columns=att_df_trans.columns, keep=\"all\")\n\n            else:\n                raise NotImplementedError(f\"Your provided mode={rank_mode} has\"\n                                          \"not been implemented yet. Please\"\n                                          \" choose between 'mean' or 'None' for\"\n                                          \"now.\")\n\n        else:\n            top_genes_transpose = att_df_trans.nlargest(\n                n, columns=att_df_trans.columns, keep=\"all\")\n\n        # return the correct order, which is cells x genes\n        if dataframe is None:\n            self.top_genes_df = top_genes_transpose\n            return self.top_genes_df\n\n        else:\n            return top_genes_transpose.T\n\n    def get_top_n_per_cluster(self,\n                              n: int = 25,\n                              model=None,\n                              mode: str = \"tfidf\",\n                              top_n_rank_method: str = \"mean\",\n                              verbose: bool = False):\n        \"\"\" Get the top n genes for each indivual cluster.\n\n        Args:\n            n: An integer indicating the number of top genes to keep.\n            model: The model we want to use to make predictions\n            mode: The mode we want to use for identifying top genes and\n              normalizing the values\n            top_n_rank_method: The mode we want to use for ranking top n genes\n              (the mode used for get_top_n method different than \"mode\"\n              argument) for this method.\n\n        Returns:\n\n        This method's reutns are \"mode\" dependent, and will return three\n        objects:\n\n        (1) A dictionary mapping each cluster to the attention scores.\n\n        (2) A dictionary containing the sum of gene attention scores for each\n            cluster.\n\n        (3) A dictionary mapping containing the name of top n genes for each\n            cluster.\n\n        Raises:\n            ValueError: An error occured during reading the trained model.\n\n        \"\"\"\n        if model is None and self.model is None:\n            raise ValueError(\"Please provide a model for making predictions and\"\n                             \" extracting attention weights.\")\n\n        print(f\"==> Top {n} genes will be selected in {mode} mode\")\n\n        # Dictionary to map clusters to their attention weights, no ranking\n        # or filtering.\n        self.clust_to_att_dict = {}\n        # Dictionary to map clusters to series containing their summed attention\n        # weights per gene.\n        self.clust_sums_dict = {}\n        # Dictionary to mapping clusters to dataframes containing the top n\n        # genes and their attention weights.\n        self.top_n_df_dict = {}\n        # dictionary to mapping clusters to a dataframe containing the top n\n        # genes based on their summed attention weights.\n        self.top_n_names_dict = {}\n\n        if self._correct_predictions_only_flag:\n            data_to_use = self.correct_pred_adata\n        else:\n            data_to_use = self.data\n\n        print(f\"==> Getting Top {n} genes for each cluster in the data\")\n        iter_list = list(data_to_use.obs.cluster.unique())\n        iter_list.sort()\n\n        for i in iter_list:\n            if verbose:\n                print(f\"    -> Cluster {i}:\")\n            # get data for the current cluster\n            curr_clust = data_to_use[data_to_use.obs.cluster == i]\n            if verbose:\n                print(f\"    -> Cells in current cluster: {curr_clust.shape[0]}\")\n\n            # Getting the cell x attention per gene df for the current cluster.\n            curr_att_df = self.att_df.loc[curr_clust.obs.index]\n            # Mapping the clusters to the attention dataframe.\n            self.clust_to_att_dict[f\"Cluster_{i}\"] = curr_att_df\n            # Getting the top n gene dataframe based on the mode.\n            if mode.lower() == \"tfidf\":\n                self.clust_sums_dict[f\"Cluster_{i}\"] = curr_att_df.T.sum(axis=1)\n            else:\n                self.top_n_df_dict[f\"Cluster_{i}\"] = self.get_top_n(\n                    n=n,\n                    dataframe=curr_att_df,\n                    verbose=False,\n                    rank_mode=top_n_rank_method)\n\n            # Gettting the top n gene names in ranked order (frp, highest\n            # expression to lowest).\n            self.top_n_names_dict[f\"Cluster_{i}\"] = curr_att_df.T.sum(\n                axis=1).nlargest(n).index\n            if verbose:\n                print(f\"    >-< Done with Cluster {i}\")\n                print()\n        print(\">-< Done with all clusters\")\n\n        if mode.lower() == \"tfidf\":\n            return (self.clust_to_att_dict, self.clust_sums_dict,\n                    self.top_n_names_dict)\n\n        else:\n            # TODO: Check the ranking and ordering on the returns\n            return (self.clust_to_att_dict, self.top_n_df_dict,\n                    self.top_n_names_dict)\n\n    def make_values_unique(self,\n                           top_n_dictionary: dict = None,\n                           threshold: int = None,\n                           verbose: bool = False):\n        \"\"\" Class method to make all the values in a dictionary unique.\n\n        Args:\n            top_n_dictionary: The dictionary containing the top n gene names for\n              all populations (or smaller populations)\n            threshold: The threshold for removing common genes: if a gene occurs\n              in threshold many populations, it will be removed.\n\n        Returns:\n            The modified *unique* top_n_dictionary dictionary based on the\n            threshold provided.\n\n        Raises:\n            None.\n        \"\"\"\n\n        if top_n_dictionary is None:\n            print(\"==> Since no dictionary was provided, we will use gene names\"\n                  \" dictionary as default\")\n            att_dict = self.top_n_names_dict.copy()\n        else:\n            att_dict = top_n_dictionary\n\n        # concat all the gene names into a list\n        all_genes = list(chain.from_iterable(att_dict.values()))\n\n        if threshold is None:\n            # do not threshold the allowed overlaps\n            if verbose:\n                print(\"    -> No thresholding... setting overlap bound to inf\")\n            threshold = np.inf\n\n        # find duplicates that appear as many times as the threshold\n        duplicate_list = [\n            item for item, count in Counter(all_genes).items()\n            if count > threshold\n        ]\n        print(f\"==> Found {len(duplicate_list)} many duplicates that appear in\"\n              \" more than {threshold} cluster(s)\")\n\n        for key in att_dict.keys():\n            att_dict[key] = [\n                item for item in att_dict[key] if item not in duplicate_list\n            ]\n\n        if top_n_dictionary is None:\n            self.global_unique_gene_names = att_dict\n\n        return att_dict\n\n    def calculate_tfidf(self,\n                        top_n_names: dict,\n                        top_n_df_dict: dict,\n                        n_genes: int = 100,\n                        verbose: bool = False):\n        \"\"\" Implementation of term frequency\u2013inverse document frequency.\n\n        This function aims to provide a list of top genes that have been\n        normalized by term-frequency (TF) - inverse document frequency (IDF).\n        We define the corpus-wide tf-idf score for each gene as\n                            (1 - Sum(tf-idf)/log(N)),\n        where N is the total number of pseudo documents (list of top genes for\n        each cluster). The returned list of top genes are then sorted in\n        descending order according to the tf-idf scores and subsequently\n        returned.\n\n        Args:\n            top_n_names: A dictionary mapping containing the name of top n genes\n              for each cluster.\n            top_n_df_dict: A dictionary containing the sum of gene attention\n              scores for each cluster.\n            n_genes: A number indicating the length for return values.\n\n        Returns:\n            A pandas dataframe with the Corpus-Wide tf-idf scores for each gene,\n            indexed by the genes, sorted in descending order by the tf-idf\n            scores.\n\n        Raises:\n            None.\n        \"\"\"\n\n        # Here we convert the intial inputs to dataframes.\n        # Below is the dataframe containing the top n genes for each cluster.\n        top_genes_df = pd.DataFrame.from_dict(top_n_names)\n        # Below is the dataframe containing the sum of gene attention scores\n        # for each cluster\n        attention_sums = pd.DataFrame.from_dict(top_n_df_dict)\n        series = pd.Series(top_genes_df.values.tolist())\n        number_documents = series.shape[0]\n        gene_counts = series.apply(lambda x: Counter(x))\n\n        # Creating a new series to store the tf values.\n        term_freq = gene_counts.apply(\n            lambda x:\n            {gene: count / sum(x.values()) for gene, count in x.items()})\n        # Create a new series to store the idf values.\n        idf = series.apply(\n            lambda x: {\n                gene: np.log(number_documents / len(\n                    [i for i in series if gene in i])) for gene in set(x)\n            })\n        # Create a new series to store the tf-idf values\n        tf_idf = term_freq.apply(\n            lambda x: {\n                gene: count * idf[term_freq[term_freq == x].index[0]][gene]\n                for gene, count in x.items()\n            })\n\n        # Create a dataframe containing the tf-idf values.\n        tfidf_normalized_series = 1 - pd.DataFrame(\n            tf_idf.tolist()).sum() / np.log(number_documents)\n\n        # Get tf-idf genes to index the attention sum dataframe\n        tfidf_genes = tfidf_normalized_series.index.tolist()\n\n        # Subset the dataframe to only contain the top genes\n        most_attentive_subset = attention_sums.loc[tfidf_genes, :]\n\n        # Perform element-wise muliplication\n        attentive_tfidf_normalized = most_attentive_subset.mul(\n            tfidf_normalized_series, axis=0)\n\n        # Dictionary for storing each cluster tf-idf normalized values\n        tfidf_norm = {}\n        iter_list = list(attentive_tfidf_normalized.columns)\n\n        # Loop through the clusters and pull genes and values\n        for i in iter_list:\n            if verbose:\n                print(f\"    -> {i}:\")\n            # get data for the current cluster\n            cluster_df = attention_sums.loc[:, i]\n\n            # get the top n gene names in ranked order (highest to lowest)\n            tfidf_norm[f\"{i}_genes\"] = cluster_df.nlargest(n_genes).index\n            tfidf_norm[f\"{i}_tfidf_values\"] = cluster_df.nlargest(\n                n_genes).values\n\n        # Create a dataframe from the dictionary\n        tfidf_out = pd.DataFrame.from_dict(tfidf_norm)\n        return tfidf_out\n\n    # Getter and setter methods for the class.\n    def get_scanpy_object(self):\n        \"\"\"Getter method for returning Scanpy object at any given time.\n\n        Args:\n            None.\n\n        Returns:\n            The current scanpy data set as the internal attribute.\n\n        Raises:\n            None\n        \"\"\"\n        return self.data\n\n    def set_scanpy_object(self, new_data):\n        \"\"\"Setter method for setting Scanpy object at any given time\n\n        Args:\n            new_data: A new scanpy object to replace the previously passed data.\n\n        Return:\n            None. Method will set the new dataset using the passed scanpy\n            object.\n\n        Raises:\n            None.\n\n        \"\"\"\n        self.data = new_data\n\n\n# Internal utility method.\n\n    def _clean_enrichment_results(self, enrichment_test: gp.Enrichr):\n        \"\"\"Internal utility method for cleaning up results from certain sets.\n\n        Args:\n            enrichment_test: The enrichment test object for the analysis\n              performed.\n\n        Returns:\n            The same object as passed on without the additional strings in the\n            \"Terms\" attribute of the object.\n\n        Raises:\n            None\n        \"\"\"\n        enrichment_test.results.Term = enrichment_test.results.Term.str.split(\n            \" CL\").str[0]\n        return enrichment_test", ""]}
{"filename": "scanna/data_handling/scanpy_to_dataloader.py", "chunked_list": ["\"\"\"Main functions for handling h5ad objects for training and testing NNs.\"\"\"\n\nimport torch\nimport numpy as np\nimport scanpy as sc\nfrom torch.utils.data import DataLoader\nfrom ..utilities.sparsity_handling import sparse_to_dense\n\ndef scanpy_to_dataloader(file_path: str = None,\n                         scanpy_object: sc.AnnData = None,\n                         train_only: bool = False,\n                         test_only: bool = False,\n                         batch_size: int = 128,\n                         workers: int = 12,\n                         log_transform: bool = False,\n                         log_base: int = None,\n                         log_method: str = \"scanpy\",\n                         annotation_key:str = \"cluster\",\n                         # For compatibility reasons we keep the `test_no_valid`\n                         # argumnet. This will be removed in future relases.\n                         test_no_valid:bool = None,\n                         verbose:bool = True,\n                         raw_x: bool = True):\n\n    \"\"\"Function to read in (or use an existing) H5AD files to make dataloaders.\n\n    Args:\n        file_path: A string that is the path to the .h5ad file.\n        scanpy_object: An existing scanpy object that should be used for the\n          dataloaders.\n        train_only: Whether we want a dataloader consisting of only training\n          data.\n        test_only: Whether we want a dataloader consisting of only testing\n          samples.\n        batch_size: An integer indicating the batch size to be used for the\n          Pytorch dataloader.\n        workers: Number of workers to load/lazy load in data.\n        log_transform: Whether we want to take log transorm of the data or not.\n        log_base: The log base we want to use. If None, we will use natural log.\n        log_method: If we want to take the log using scanpy or PyTorch.\n        annotation_key: A string containing the annotation key which will be\n          used as the label for the dataloader.\n        verbose: Verbosity option indicated as a boolean.\n        raw_x:bool: This is a dataset- and platform-dependant variable. This\n          option enables using the \"raw\" X matrix, as defined in Seurat. Useful\n          for when preprocessing in R and running N-ACT in PyTorch.\n\n    Returns:\n        This function will return two dataloaders:\n\n        (1) A Training data loader consisting of the data (at batch[0]) and\n        labels (at batch[1]).\n\n        (2) A Testing data loader consisting of the data (at batch[0]) and\n        labels (at batch[1])\n\n    Raises:\n        ValueError: If both \"train_only\" and \"test_only\" arguments are set to\n          true.\n        ValueError: If neither a path to an h5ad file or an existing scanpy\n          object is provided.\n        ValueError: If log base falls outside of the implemented ones.\n\n    \"\"\"\n    if train_only is True and test_only is True:\n        raise ValueError(\"Both options for 'train_only' and 'test_only' are \"\n                         \"passed as True, which cannot be. Please check the \"\n                         \"args and try again.\")\n    if scanpy_object is None and file_path is not None:\n        print(\"==> Reading in Scanpy/Seurat AnnData\")\n        adata = sc.read(file_path)\n    elif scanpy_object is not None and file_path is None:\n        adata = scanpy_object\n    else:\n        raise ValueError(\"Pleaes either provide a path to a h5ad file, or\"\n                         \" provide an existing scanpy object.\")\n\n    if raw_x:\n        print(\"    -> Trying adata.raw.X instead of adata.X!\")\n        try:\n            adata.X = adata.raw.X\n        except Exception as e:\n            print(f\"    -> Failed with message: {e}\")\n            print(\"    -> Reverting to adata.X if possible\")\n\n    if log_transform and log_method == \"scanpy\":\n        print(\"    -> Doing log(x+1) transformation with Scanpy\")\n        sc.pp.log1p(adata, base=log_base)\n\n    print(\"    -> Splitting Train and Test Data\")\n\n    if train_only:\n        train_adata = adata[adata.obs[\"split\"].isin([\"train\"])]\n        test_adata = None\n    elif test_only:\n        train_adata = None\n        test_adata = adata[adata.obs[\"split\"].isin([\"test\"])]\n    else:\n        train_adata = adata[adata.obs[\"split\"].isin([\"train\"])]\n        test_adata = adata[adata.obs[\"split\"].isin([\"test\"])]\n\n    # turn the cluster numbers into labels\n    print(f\"==> Using {annotation_key} to generating train and testing labels\")\n    y_train = None\n    y_test = None\n\n    if train_only:\n        y_train = [int(x) for x in train_adata.obs[annotation_key].to_list()]\n    elif test_only:\n        y_test = [int(x) for x in test_adata.obs[annotation_key].to_list()]\n    else:\n        y_train = [int(x) for x in train_adata.obs[annotation_key].to_list()]\n        y_test = [int(x) for x in test_adata.obs[annotation_key].to_list()]\n\n    print(\"==> Checking if we have sparse matrix into dense\")\n    norm_count_train = None\n    norm_count_test = None\n    train_data = None\n    test_data = None\n\n    if train_only:\n        norm_count_train = sparse_to_dense(train_adata)\n        train_data = torch.torch.from_numpy(norm_count_train)\n    elif test_only:\n        norm_count_test = sparse_to_dense(test_adata)\n        test_data = torch.torch.from_numpy(norm_count_test)\n    else:\n        norm_count_train = sparse_to_dense(train_adata)\n        norm_count_test = sparse_to_dense(test_adata)\n        train_data = torch.torch.from_numpy(norm_count_train)\n        test_data = torch.torch.from_numpy(norm_count_test)\n\n    if log_transform and log_method == \"torch\":\n        print(\"    -> Doing log(x+1) transformation with torch\")\n        if log_base is None:\n            train_data = torch.log(1 + train_data)\n            if not train_only:\n                test_data = torch.log(1 + test_data)\n        elif log_base == 2:\n            train_data = torch.log2(1 + train_data)\n            if not train_only:\n                test_data = torch.log2(1 + test_data)\n        elif log_base == 10:\n            train_data = torch.log2(1 + train_data)\n            if not train_only:\n                test_data = torch.log2(1 + test_data)\n        else:\n            raise ValueError(\n                \"    -> We have only implemented log base e, 2 and 10 for torch\"\n            )\n\n    training_data_and_labels = []\n    testing_data_and_labels = []\n    if not test_only:\n        for i in range(len(train_data)):\n            training_data_and_labels.append([norm_count_train[i], y_train[i]])\n\n    if not train_only:\n        for i in range(len(test_data)):\n            testing_data_and_labels.append([norm_count_test[i], y_test[i]])\n\n    if verbose:\n        if not test_only:\n            print(\"==> sample of the training data:\")\n            print(f\"{train_data}\")\n\n        if not train_only:\n            print(\"==> sample of the test data:\")\n            print(f\"{test_data}\")\n\n    train_data_loader = None\n    test_data_loader = None\n    if not test_only:\n        train_data_loader = DataLoader(training_data_and_labels,\n                                       batch_size=batch_size,\n                                       shuffle=True,\n                                       sampler=None,\n                                       batch_sampler=None,\n                                       num_workers=workers,\n                                       collate_fn=None,\n                                       pin_memory=True)\n\n    if not train_only:\n        test_data_loader = DataLoader(testing_data_and_labels,\n                                       batch_size=len(test_data),\n                                       shuffle=True,\n                                       sampler=None,\n                                       batch_sampler=None,\n                                       num_workers=workers,\n                                       collate_fn=None,\n                                       pin_memory=True)\n\n\n    return train_data_loader, test_data_loader", "def scanpy_to_dataloader(file_path: str = None,\n                         scanpy_object: sc.AnnData = None,\n                         train_only: bool = False,\n                         test_only: bool = False,\n                         batch_size: int = 128,\n                         workers: int = 12,\n                         log_transform: bool = False,\n                         log_base: int = None,\n                         log_method: str = \"scanpy\",\n                         annotation_key:str = \"cluster\",\n                         # For compatibility reasons we keep the `test_no_valid`\n                         # argumnet. This will be removed in future relases.\n                         test_no_valid:bool = None,\n                         verbose:bool = True,\n                         raw_x: bool = True):\n\n    \"\"\"Function to read in (or use an existing) H5AD files to make dataloaders.\n\n    Args:\n        file_path: A string that is the path to the .h5ad file.\n        scanpy_object: An existing scanpy object that should be used for the\n          dataloaders.\n        train_only: Whether we want a dataloader consisting of only training\n          data.\n        test_only: Whether we want a dataloader consisting of only testing\n          samples.\n        batch_size: An integer indicating the batch size to be used for the\n          Pytorch dataloader.\n        workers: Number of workers to load/lazy load in data.\n        log_transform: Whether we want to take log transorm of the data or not.\n        log_base: The log base we want to use. If None, we will use natural log.\n        log_method: If we want to take the log using scanpy or PyTorch.\n        annotation_key: A string containing the annotation key which will be\n          used as the label for the dataloader.\n        verbose: Verbosity option indicated as a boolean.\n        raw_x:bool: This is a dataset- and platform-dependant variable. This\n          option enables using the \"raw\" X matrix, as defined in Seurat. Useful\n          for when preprocessing in R and running N-ACT in PyTorch.\n\n    Returns:\n        This function will return two dataloaders:\n\n        (1) A Training data loader consisting of the data (at batch[0]) and\n        labels (at batch[1]).\n\n        (2) A Testing data loader consisting of the data (at batch[0]) and\n        labels (at batch[1])\n\n    Raises:\n        ValueError: If both \"train_only\" and \"test_only\" arguments are set to\n          true.\n        ValueError: If neither a path to an h5ad file or an existing scanpy\n          object is provided.\n        ValueError: If log base falls outside of the implemented ones.\n\n    \"\"\"\n    if train_only is True and test_only is True:\n        raise ValueError(\"Both options for 'train_only' and 'test_only' are \"\n                         \"passed as True, which cannot be. Please check the \"\n                         \"args and try again.\")\n    if scanpy_object is None and file_path is not None:\n        print(\"==> Reading in Scanpy/Seurat AnnData\")\n        adata = sc.read(file_path)\n    elif scanpy_object is not None and file_path is None:\n        adata = scanpy_object\n    else:\n        raise ValueError(\"Pleaes either provide a path to a h5ad file, or\"\n                         \" provide an existing scanpy object.\")\n\n    if raw_x:\n        print(\"    -> Trying adata.raw.X instead of adata.X!\")\n        try:\n            adata.X = adata.raw.X\n        except Exception as e:\n            print(f\"    -> Failed with message: {e}\")\n            print(\"    -> Reverting to adata.X if possible\")\n\n    if log_transform and log_method == \"scanpy\":\n        print(\"    -> Doing log(x+1) transformation with Scanpy\")\n        sc.pp.log1p(adata, base=log_base)\n\n    print(\"    -> Splitting Train and Test Data\")\n\n    if train_only:\n        train_adata = adata[adata.obs[\"split\"].isin([\"train\"])]\n        test_adata = None\n    elif test_only:\n        train_adata = None\n        test_adata = adata[adata.obs[\"split\"].isin([\"test\"])]\n    else:\n        train_adata = adata[adata.obs[\"split\"].isin([\"train\"])]\n        test_adata = adata[adata.obs[\"split\"].isin([\"test\"])]\n\n    # turn the cluster numbers into labels\n    print(f\"==> Using {annotation_key} to generating train and testing labels\")\n    y_train = None\n    y_test = None\n\n    if train_only:\n        y_train = [int(x) for x in train_adata.obs[annotation_key].to_list()]\n    elif test_only:\n        y_test = [int(x) for x in test_adata.obs[annotation_key].to_list()]\n    else:\n        y_train = [int(x) for x in train_adata.obs[annotation_key].to_list()]\n        y_test = [int(x) for x in test_adata.obs[annotation_key].to_list()]\n\n    print(\"==> Checking if we have sparse matrix into dense\")\n    norm_count_train = None\n    norm_count_test = None\n    train_data = None\n    test_data = None\n\n    if train_only:\n        norm_count_train = sparse_to_dense(train_adata)\n        train_data = torch.torch.from_numpy(norm_count_train)\n    elif test_only:\n        norm_count_test = sparse_to_dense(test_adata)\n        test_data = torch.torch.from_numpy(norm_count_test)\n    else:\n        norm_count_train = sparse_to_dense(train_adata)\n        norm_count_test = sparse_to_dense(test_adata)\n        train_data = torch.torch.from_numpy(norm_count_train)\n        test_data = torch.torch.from_numpy(norm_count_test)\n\n    if log_transform and log_method == \"torch\":\n        print(\"    -> Doing log(x+1) transformation with torch\")\n        if log_base is None:\n            train_data = torch.log(1 + train_data)\n            if not train_only:\n                test_data = torch.log(1 + test_data)\n        elif log_base == 2:\n            train_data = torch.log2(1 + train_data)\n            if not train_only:\n                test_data = torch.log2(1 + test_data)\n        elif log_base == 10:\n            train_data = torch.log2(1 + train_data)\n            if not train_only:\n                test_data = torch.log2(1 + test_data)\n        else:\n            raise ValueError(\n                \"    -> We have only implemented log base e, 2 and 10 for torch\"\n            )\n\n    training_data_and_labels = []\n    testing_data_and_labels = []\n    if not test_only:\n        for i in range(len(train_data)):\n            training_data_and_labels.append([norm_count_train[i], y_train[i]])\n\n    if not train_only:\n        for i in range(len(test_data)):\n            testing_data_and_labels.append([norm_count_test[i], y_test[i]])\n\n    if verbose:\n        if not test_only:\n            print(\"==> sample of the training data:\")\n            print(f\"{train_data}\")\n\n        if not train_only:\n            print(\"==> sample of the test data:\")\n            print(f\"{test_data}\")\n\n    train_data_loader = None\n    test_data_loader = None\n    if not test_only:\n        train_data_loader = DataLoader(training_data_and_labels,\n                                       batch_size=batch_size,\n                                       shuffle=True,\n                                       sampler=None,\n                                       batch_sampler=None,\n                                       num_workers=workers,\n                                       collate_fn=None,\n                                       pin_memory=True)\n\n    if not train_only:\n        test_data_loader = DataLoader(testing_data_and_labels,\n                                       batch_size=len(test_data),\n                                       shuffle=True,\n                                       sampler=None,\n                                       batch_sampler=None,\n                                       num_workers=workers,\n                                       collate_fn=None,\n                                       pin_memory=True)\n\n\n    return train_data_loader, test_data_loader", ""]}
{"filename": "scanna/data_handling/__init__.py", "chunked_list": ["\"\"\"Second level module import for data input/output.\"\"\"\nfrom .scanpy_to_dataloader import *\n"]}
{"filename": "scanna/model/scanna_projection_attention.py", "chunked_list": ["\"\"\"Implementation of scANNA model with attention + projection blocks.\"\"\"\n\nfrom .base._base_projection_model import BaseProjectionModel\nfrom .base._constants_and_types import _DEVICE_TYPES\nimport torch\n\n\nclass ProjectionAttention(BaseProjectionModel):\n    \"\"\"scANNA model with attention and projection blocks.\n\n    Attributes:\n        device: A string (either \"cuda\" or \"cpu\") determining which device\n          computations should be performed on.\n        masking_frac: The fraction of each input cell that should be masked.\n          This value should be set to zero except for when training the\n          unsupervised contrastive learning mode.\n        num_features: The number of inputted genes (input dimension).\n        attention_module: The attention module of scANNA.\n        attention_dim: The dimension of the attention layer (the same as the\n          input feature unless an encoding is applied at the start).\n        proj_block1: The first projection block of scANNA.\n        proj_block2: The second and last projection block of scANNA.\n        pwff: The pointwise Feedforward neural network that is used as the\n          activation of of the projection blocks.\n        task_module: The task module of scANNA. In this implementation, the task\n          module is defined to be the number of cell types.\n    \"\"\"\n\n    def __init__(self,\n                 input_dimension: int = 5000,\n                 task_module_output_dimension: int = 11,\n                 dropout: float = 0.0,\n                 number_of_projections: int = 8,\n                 masking_fraction: float = None,\n                 device: _DEVICE_TYPES = 'cpu'):\n        \"\"\"Initializer of the ProjectionAttention class.\"\"\"\n\n        super().__init__()\n        self.device = device\n        self.num_features = input_dimension\n        self.out_dim = task_module_output_dimension\n        self.masking_frac = masking_fraction\n        self.attention_dim = input_dimension\n\n        # We use masking for self-supervised contrastive learning only.\n        if masking_fraction not in (0.0, None):\n            self.masking_layer = torch.nn.Dropout(p=masking_fraction)\n        else:\n            self.masking_layer = torch.nn.Identity()\n\n        # scANNA Components are as follows:\n        # Component (1): Attention Module\n        # Component (2): Projection Blocks and Poinstwise Feedforward NN\n        # Component (3): Task Module.\n        # We number these modules in the declarations below for readibility.\n\n        # Component (1)\n        self.attention_module = torch.nn.Linear(self.num_features,\n                                                self.num_features)\n\n        # Component (2)\n        self.projection_block1 = self._projection_block(\n            attention_dimension=self.attention_dim,\n            number_of_projection_branches=number_of_projections,\n            dropout_probability=dropout)\n        self.projection_block2 = self._projection_block(\n            attention_dimension=self.attention_dim,\n            number_of_projection_branches=number_of_projections,\n            dropout_probability=dropout)\n        # Component (2)\n        self.pwff = self._pointwise_activation(\n            input_dimension=self.attention_dim,\n            hidden_dimensions=[128, self.attention_dim],\n            use_1x1_conv=False)\n\n        # Component (3)\n        self.task_module = torch.nn.Sequential(\n            torch.nn.Linear(self.attention_dim, self.out_dim),\n            torch.nn.LeakyReLU())\n\n    def forward(self,\n                input_tensor: torch.Tensor,\n                training: bool = True,\n                device='cpu'):\n        \"\"\"Forward pass of the projection-based scANNA model.\n\n        Args:\n            input_tensor: A tensor containing input data for training.\n            training: The mode that we are calling the forward function. True\n              indicates that we are training the model\n\n        Returns:\n            The forward method returns three tensors:\n            (1) logits: the actual logits for predictions.\n\n            (2) alphas: The attention tensor containing the weights for all\n                genes.\n\n            (3) gamma: A tensor containing the gene scores.\n\n        Raises:\n            None.\n        \"\"\"\n        self.training = training\n\n        if not self.training:\n            self.device = device\n\n        x_masked = self._parallel_eval(self.masking_layer,input_tensor)\n        alphas = self._softmax(self.attention_module(input_tensor))\n        gamma = self._gene_scores(alphas, x_masked)\n\n        # The abbrevation \"gse\" stands for gene stacked event (gse), which is\n        # the output of the a projection block (with all branches).\n        gse = self._parallel_eval(self.projection_block1, gamma)\n        x_activated = self._parallel_eval(self.pwff, gse)\n\n        gse2 = self._parallel_eval(self.projection_block2, x_activated + gamma)\n        x_activated2 = self._parallel_eval(self.pwff, gse2)\n\n        logits = self._parallel_eval(self.task_module, x_activated2 + gamma)\n\n        return logits, alphas, gamma", ""]}
{"filename": "scanna/model/__init__.py", "chunked_list": ["\"\"\"scANNA's second level imports for model modules.\"\"\"\nfrom .finetuning_scanna_projection_attention import *\nfrom .scanna_additive_attention import *\nfrom .scanna_projection_attention import *\n"]}
{"filename": "scanna/model/scanna_additive_attention.py", "chunked_list": ["\"\"\"Implementation of scANNA with additive attention.\"\"\"\n\nfrom .base._base_model import BaseNeuralAttentionModel\nfrom .base._constants_and_types import _DEVICE_TYPES\n\nimport torch\n\n\nclass AdditiveModel(BaseNeuralAttentionModel):\n    \"\"\"Implementation of additive attention and scANNA from base class.\n\n    The additive attention + FFNN is based on Colin Raffel and Daniel P. Ellis,\n    which can be found at https://arxiv.org/abs/1512.08756.\n\n    Attributes:\n\n    \"\"\"\n\n    def __init__(self,\n                 input_dimension: int = 5000,\n                 output_dimension: int = 11,\n                 device: _DEVICE_TYPES = \"cpu\"):\n\n        super().__init__()\n        self.device = device\n        self.num_features = input_dimension\n        self.out_dim = output_dimension\n        self.attention = torch.nn.Linear(self.num_features, self.num_features)\n        self.network = torch.nn.Sequential(\n            torch.nn.Linear(self.num_features, 1024), torch.nn.Tanh(),\n            torch.nn.Linear(1024, 512), torch.nn.Tanh(),\n            torch.nn.Linear(512, 256),\n            torch.nn.Tanh(), torch.nn.Linear(256, 128), torch.nn.Tanh(),\n            torch.nn.Linear(128, 64), torch.nn.Tanh(),\n            torch.nn.Linear(64, self.out_dim), torch.nn.Tanh())\n\n    def forward(self, x: torch.Tensor, training: bool = True):\n        \"\"\" Forward pass for the Feed Forward Attention network.\n\n        Args:\n            x: Gene expression matrix from scRNAseq.\n            training: A boolean indicating weather we are in training or not.\n\n        Returns:\n            Forward call returns three tensors:\n\n            (1) outputs: the output of the task module, in this case\n            classification probabilities after a hyperbolic tangent activation.\n\n            (2) alpha: the attention weights.\n\n            (3) x_c: the gene scores.\n\n        Raises:\n            None.\n        \"\"\"\n        self.training = training\n        alphas = self._softmax(self.attention(x))\n        gene_scores = self._gene_scores(alphas, x)\n        outputs = self._parallel_eval(self.network, gene_scores)\n        return outputs, alphas, gene_scores", "class AdditiveModel(BaseNeuralAttentionModel):\n    \"\"\"Implementation of additive attention and scANNA from base class.\n\n    The additive attention + FFNN is based on Colin Raffel and Daniel P. Ellis,\n    which can be found at https://arxiv.org/abs/1512.08756.\n\n    Attributes:\n\n    \"\"\"\n\n    def __init__(self,\n                 input_dimension: int = 5000,\n                 output_dimension: int = 11,\n                 device: _DEVICE_TYPES = \"cpu\"):\n\n        super().__init__()\n        self.device = device\n        self.num_features = input_dimension\n        self.out_dim = output_dimension\n        self.attention = torch.nn.Linear(self.num_features, self.num_features)\n        self.network = torch.nn.Sequential(\n            torch.nn.Linear(self.num_features, 1024), torch.nn.Tanh(),\n            torch.nn.Linear(1024, 512), torch.nn.Tanh(),\n            torch.nn.Linear(512, 256),\n            torch.nn.Tanh(), torch.nn.Linear(256, 128), torch.nn.Tanh(),\n            torch.nn.Linear(128, 64), torch.nn.Tanh(),\n            torch.nn.Linear(64, self.out_dim), torch.nn.Tanh())\n\n    def forward(self, x: torch.Tensor, training: bool = True):\n        \"\"\" Forward pass for the Feed Forward Attention network.\n\n        Args:\n            x: Gene expression matrix from scRNAseq.\n            training: A boolean indicating weather we are in training or not.\n\n        Returns:\n            Forward call returns three tensors:\n\n            (1) outputs: the output of the task module, in this case\n            classification probabilities after a hyperbolic tangent activation.\n\n            (2) alpha: the attention weights.\n\n            (3) x_c: the gene scores.\n\n        Raises:\n            None.\n        \"\"\"\n        self.training = training\n        alphas = self._softmax(self.attention(x))\n        gene_scores = self._gene_scores(alphas, x)\n        outputs = self._parallel_eval(self.network, gene_scores)\n        return outputs, alphas, gene_scores", ""]}
{"filename": "scanna/model/finetuning_scanna_projection_attention.py", "chunked_list": ["\"\"\"Implementation of transfer learning for scANNA's projection model.\"\"\"\n\nfrom .base._constants_and_types import _DEVICE_TYPES\nfrom .scanna_projection_attention import ProjectionAttention\nimport torch\n\n\nclass FineTuningModel(ProjectionAttention):\n    \"\"\"Pretrained scANNA model to be finetuned for a new data.\n\n    Attributes:\n        *Note: The attributes are the same as the passed on pre-trained model.\n        device: A string (either \"cuda\" or \"cpu\") determining which device\n          computations should be performed on.\n        masking_frac: The fraction of each input cell that should be masked.\n          This value should be set to zero except for when training the\n          unsupervised contrastive learning mode.\n        num_features: The number of inputted genes (input dimension).\n        attention_module: The attention module of scANNA.\n        attention_dim: The dimension of the attention layer (the same as the\n          input feature unless an encoding is applied at the start).\n        proj_block1: The first projection block of scANNA.\n        proj_block2: The second and last projection block of scANNA.\n        pwff: The pointwise Feedforward neural network that is used as the\n          activation of of the projection blocks.\n        task_module: The task module of scANNA. In this implementation, the task\n          module is defined to be the number of cell types.\n    \"\"\"\n\n    def __init__(self,\n                 pretrained_scanna_model: ProjectionAttention,\n                 task_module_output_dimension: int,\n                 input_dimension: int = 5000,\n                 device: _DEVICE_TYPES = 'cpu'):\n        \"\"\"Initializer of the ProjectionAttention class with trained values.\"\"\"\n\n        super().__init__()\n        self.device = device\n        self.num_features = input_dimension\n        self.out_dim = task_module_output_dimension\n        # Here we transferring attributes from the pre-trained model.\n\n        self.attention_module = pretrained_scanna_model.attention_module\n        self.projection_block1 = pretrained_scanna_model.projection_block1\n        self.projection_block2 = pretrained_scanna_model.projection_block2\n        self.pwff = pretrained_scanna_model.pwff\n        self.attention_dim = pretrained_scanna_model.attention_dim\n        # Component (3)\n        self.task_module = torch.nn.Sequential(\n            torch.nn.Linear(self.attention_dim, task_module_output_dimension),\n            torch.nn.LeakyReLU())\n\n    def forward(self,\n                input_tensor: torch.Tensor,\n                training: bool = True,\n                device='cpu'):\n        \"\"\"Forward pass of the finetuning model of scANNA (for proj. models).\n\n        Args:\n            input_tensor: A tensor containing input data for training.\n            training: The mode that we are calling the forward function. True\n              indicates that we are training the model\n            device: A string (\"cuda\" or \"cpu\") indicating which device we want\n              to use for performing computaions.\n\n        Returns:\n            The forward method returns three tensors:\n            (1) logits: the actual logits for predictions.\n\n            (2) alphas: The attention tensor containing the weights for all\n                genes.\n\n            (3) gamma: A tensor containing the gene scores.\n\n        Raises:\n            None.\n        \"\"\"\n        self.training = training\n\n        if not self.training:\n            self.device = device\n        # This is our way of freezing these core layers\n        # TODO: Find a more efficient way of freezing these layers\n        with torch.no_grad():\n            x_masked = self._parallel_eval(self.masking_layer, input_tensor)\n            alphas = self._softmax(self.attention_module(input_tensor))\n            gamma = self._gene_scores(alphas, x_masked)\n\n            # The abbrevation \"gse\" stands for gene stacked event (gse), which\n            # is the output of the a projection block (with all branches).\n            gse = self._parallel_eval(self.projection_block1, gamma)\n            x_activated = self._parallel_eval(self.pwff, gse)\n\n            gse2 = self._parallel_eval(self.projection_block2,\n                                       x_activated + gamma)\n            x_activated2 = self._parallel_eval(self.pwff, gse2)\n\n        # This is the only layer we want to train (finetiune)!\n        logits = self._parallel_eval(self.task_module, x_activated2 + gamma)\n\n        return logits, alphas, gamma", ""]}
{"filename": "scanna/model/base/_base_projection_model.py", "chunked_list": ["\"\"\" Base class for projection-based scANNA models.\"\"\"\n\nfrom ._base_model import BaseNeuralAttentionModel\nfrom ..projection_block._projector_block import Projection\nfrom ..projection_block._pointwise_feedforward_net import PointWiseFeedForward\nfrom typing import List\n\n\nclass BaseProjectionModel(BaseNeuralAttentionModel):\n    \"\"\" Base model for the projection-based scANNA model.\n\n    Attributes:\n        None.\n    \"\"\"\n\n    def _pointwise_activation(self,\n                              input_dimension: int,\n                              hidden_dimensions: List[int],\n                              use_1x1_conv: bool = False):\n        \"\"\" Pointwise activation method for the projection class.\n\n        Args:\n            input_dimension: An integer determining the input dimension of the\n              pointwise feedforward neural network (PWFF).\n            hidden_dimensions: The list of hidden dimensions of the PWFF.\n            use_1x1_conv: A boolean to determine whether we should use 1x1\n              convolutions instead of feedforward layers. This is only for\n              computational considerations and both methods should yeild the\n              same results.\n\n        Returns:\n            An initialized instance of the PointWiseFeedForward class.\n\n        Raises:\n            None.\n        \"\"\"\n\n        return PointWiseFeedForward(inp_dim=input_dimension,\n                                    hidden_dims=hidden_dimensions,\n                                    use_1x1_conv=use_1x1_conv)\n\n    def _projection_block(self,\n                          attention_dimension: int,\n                          number_of_projection_branches: int = 8,\n                          dropout_probability: float = 0.1):\n        \"\"\" Class method for the proposed projection blocks.\n\n        Args:\n            attention_dimension: An integer indicating the input dimension of\n              the projection block, which should be the same as the attention\n              dimension.\n            number_of_projection_branches: An integer determining the number of\n              branching we want to do in the calculations (similar to attention-\n              heads in transformers).\n            dropout_probability: A float indicating the dropout probability in\n            the model.\n\n        Returns:\n            An initialized instance of the Projection class.\n\n        Raises:\n            None.\n        \"\"\"\n\n        return Projection(model_dim=attention_dimension,\n                          number_of_branches=number_of_projection_branches,\n                          dropout=dropout_probability)", "class BaseProjectionModel(BaseNeuralAttentionModel):\n    \"\"\" Base model for the projection-based scANNA model.\n\n    Attributes:\n        None.\n    \"\"\"\n\n    def _pointwise_activation(self,\n                              input_dimension: int,\n                              hidden_dimensions: List[int],\n                              use_1x1_conv: bool = False):\n        \"\"\" Pointwise activation method for the projection class.\n\n        Args:\n            input_dimension: An integer determining the input dimension of the\n              pointwise feedforward neural network (PWFF).\n            hidden_dimensions: The list of hidden dimensions of the PWFF.\n            use_1x1_conv: A boolean to determine whether we should use 1x1\n              convolutions instead of feedforward layers. This is only for\n              computational considerations and both methods should yeild the\n              same results.\n\n        Returns:\n            An initialized instance of the PointWiseFeedForward class.\n\n        Raises:\n            None.\n        \"\"\"\n\n        return PointWiseFeedForward(inp_dim=input_dimension,\n                                    hidden_dims=hidden_dimensions,\n                                    use_1x1_conv=use_1x1_conv)\n\n    def _projection_block(self,\n                          attention_dimension: int,\n                          number_of_projection_branches: int = 8,\n                          dropout_probability: float = 0.1):\n        \"\"\" Class method for the proposed projection blocks.\n\n        Args:\n            attention_dimension: An integer indicating the input dimension of\n              the projection block, which should be the same as the attention\n              dimension.\n            number_of_projection_branches: An integer determining the number of\n              branching we want to do in the calculations (similar to attention-\n              heads in transformers).\n            dropout_probability: A float indicating the dropout probability in\n            the model.\n\n        Returns:\n            An initialized instance of the Projection class.\n\n        Raises:\n            None.\n        \"\"\"\n\n        return Projection(model_dim=attention_dimension,\n                          number_of_branches=number_of_projection_branches,\n                          dropout=dropout_probability)", ""]}
{"filename": "scanna/model/base/__init__.py", "chunked_list": ["\"\"\"scANNA's third level import for base model.\"\"\"\n# left blank intentionally\n"]}
{"filename": "scanna/model/base/_base_model.py", "chunked_list": ["\"\"\"Base class for scANNA.\"\"\"\nfrom __future__ import annotations\n\nfrom ..sparsemax.sparsemax import SparseMax\nimport torch\nfrom torch.nn.parallel import data_parallel\n\nclass BaseNeuralAttentionModel(torch.nn.Module):\n    \"\"\" Base model for any scANNA (additive attention or projection).\n\n    Original additive attention is from Bahdanau et al, located at\n    https://arxiv.org/pdf/1409.0473.pdf.\n\n    Attributes:\n        device: A string (either \"cuda\" or \"cpu\")\n\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initializer of the base method.\"\"\"\n        super().__init__()\n        self.device = \"cpu\"\n\n    def _parallel_eval(\n        self,\n        method: BaseNeuralAttentionModel,\n        *args,\n    ):\n        \"\"\" Universal evaluator based on the chosen device.\n\n        Args:\n            method: Class method for which we want to do evaluations.\n            *args: Appropriate arguments relating to the class method passed on.\n\n        Returns:\n            The same return as the class method passed on.\n\n        Raises:\n            The same rais behaviors as in the class method passed on.\n\n        \"\"\"\n\n        if self.device == \"cpu\":\n            return method(*args)\n        else:\n            return data_parallel(method, *args)\n\n    def _softmax(self, e_t: torch.Tensor) -> torch.Tensor:\n        \"\"\" Softmax method for the alignment score e_t.\n\n        Args:\n            e_t: Alignment scores which are the output of the attention layer.\n\n        Returns:\n            The computed probability tensor of the attention layer.\n\n        Raises:\n            None.\n        \"\"\"\n        return torch.nn.Softmax(dim=1)(e_t)\n\n    def _sparsemax(self, e_t: torch.Tensor) -> torch.Tensor:\n        \"\"\" SparseMax method for the alignment score e_t\n\n        Args:\n            e_t: Alignment scores which are the output of the attention layer.\n\n        Returns:\n            The computed SparseMax values of the attention layer outputs as a\n            tensor.\n\n        Raises:\n            None.\n        \"\"\"\n        return SparseMax(dim=1)(e_t)\n\n    def _gene_scores(self, alpha_t: torch.Tensor,\n                     x_t: torch.Tensor) -> torch.Tensor:\n        \"\"\" Method for computing gene scores.\n\n        This method computes the gene scores (traditionally referred to as\n        \"context\" in natural language processing) using the attention values and\n        the gene expressions.\n\n        Args:\n            alpha_t: The attention values (computed probabilities over e_t).\n            x_t : Raw gene counts from scRNAseq matrix.\n\n        Returns:\n            A tensor containing the gene scores.\n\n        Raises:\n            None.\n        \"\"\"\n        return torch.mul(alpha_t, x_t)", ""]}
{"filename": "scanna/model/base/_constants_and_types.py", "chunked_list": ["\"\"\"Definition of constants and types for various modules.\"\"\"\n\nfrom typing import Literal\n\n_DEVICE_TYPES = Literal[\"cuda\", \"cpu\"]\n"]}
{"filename": "scanna/model/projection_block/_pointwise_feedforward_net.py", "chunked_list": ["\"\"\"Implementation of Pointwise Feedforward neural networks.\"\"\"\n\nimport torch\nfrom typing import List\n\n\nclass PointWiseFeedForward(torch.nn.Module):\n    \"\"\"A pointwise feedforward network.\n\n    The implementation is based on the described network in Vawani et al.\n    https://arxiv.org/pdf/1706.03762.pdf.\n\n    Attributes:\n    inp_dim: The input dimension (coming from the attention layer).\n    hidden_dims: A list of two integers that determine the hidden layers.\n    use_convolution_instead: Whether we want to use a 1x1 conv to represent\n      the pointwise-fully connected network.\n    \"\"\"\n\n    def __init__(self,\n                 inp_dim: int,\n                 hidden_dims: List[int],\n                 use_1x1_conv: bool = False):\n        \"\"\"Initializer of the PointWiseFeedForward class.\"\"\"\n\n        super().__init__()\n        self.inp_dim = inp_dim\n        self.hidden_dims = hidden_dims\n        # in our experiments, nn.Linear is faster than nn.Conv1d\n        self.use_convolution_instead = use_1x1_conv\n\n        if self.use_convolution_instead:\n            params = {\n                'in_channels': self.inp_dim,\n                'out_channels': self.hidden_dims[0],\n                'kernel_size': 1,\n                'stride': 1,\n                'bias': True\n            }\n            self.first_layer = torch.nn.Sequential(torch.nn.Conv1d(**params),\n                                                   torch.nn.ReLU())\n            params = {\n                'in_channels': self.hidden_dims[0],\n                'out_channels': self.hidden_dims[1],\n                'kernel_size': 1,\n                'stride': 1,\n                'bias': True\n            }\n            self.second_layer = torch.nn.Conv1d(**params)\n        else:\n            self.first_layer = torch.nn.Sequential(\n                torch.nn.Linear(self.inp_dim, self.hidden_dims[0]),\n                torch.nn.ReLU())\n            self.second_layer = torch.nn.Linear(self.hidden_dims[0],\n                                                self.hidden_dims[1])\n\n        self.normalization = torch.nn.LayerNorm(self.inp_dim)\n\n    def forward(self, inputs: torch.Tensor):\n        \"\"\"The forward call of the PWFF mechanism.\n\n        Args:\n            inputs: The input tensor that we need to pass through the network.\n\n        Returns:\n            The output tensor of the PWFF operations.\n\n        Raises:\n            None.\n\n        \"\"\"\n        if self.use_convolution_instead:\n            inputs = inputs.permute(0, 1)\n\n        outputs = self.second_layer(self.first_layer(inputs))\n        # Applying the residual connection.\n        outputs += inputs\n\n        if self.use_convolution_instead:\n            return self.normalization(outputs.permute(0, 2, 1))\n        else:\n            return self.normalization(outputs)", ""]}
{"filename": "scanna/model/projection_block/__init__.py", "chunked_list": ["\"\"\"scANNA's third level import for projection and pointwise activation.\"\"\"\n# left blank intentionally\n"]}
{"filename": "scanna/model/projection_block/_projector_block.py", "chunked_list": ["\"\"\"Implementation of projection modules for scANNA's.\"\"\"\n\nimport torch\n\n\nclass Projection(torch.nn.Module):\n    \"\"\"Implementation of the projection blocks.\n\n    Projection blocks are a modification of the multi-head attention modules\n    introduced by Vaswani et al. (https://arxiv.org/pdf/1706.03762.pdf).\n    scANNA's projection blocks are discussed in the following paper:\n    https://arxiv.org/pdf/2206.04047.pdf.\n\n    Attributes:\n        model_dim: The dimension of the projection module.\n        number_of_projections: The number of parallel linear projections\n          (or branches).\n        projection_dims: Dimension of each projection branch.\n        projection: The linear projection module.\n        output_dropout: The dropout module of the projections.\n        normalization: The layernorm normalization method.\n    \"\"\"\n\n    def __init__(self,\n                 model_dim: int = 5000,\n                 number_of_branches: int = 8,\n                 dropout: float = 0.0):\n        \"\"\"Initializer of the Projection class.\"\"\"\n\n        super().__init__()\n        # Since we implement all projections in one tensor (for efficency), we\n        # need to make sure that the model dimension is divisible by the number\n        # of projection branches.\n        if model_dim % number_of_branches != 0:\n            raise ValueError(\"Dimension of the attention module modulo\"\n                             \"number of projection branches should == 0. Input\"\n                             f\"module is {model_dim} % {number_of_branches} =\"\n                             f\"{model_dim % number_of_branches} != 0\")\n\n        self.model_dim = model_dim\n        self.number_of_projections = number_of_branches\n        self.projection_dims = int(self.model_dim / self.number_of_projections)\n        # Linear projections done in one tensor (similar to Vaswani et al.)\n        # for efficiency.\n        self.projection = torch.nn.Linear(\n            self.model_dim,\n            self.projection_dims * self.number_of_projections)\n\n        self.output_dropout = torch.nn.Dropout(p=dropout)\n        self.normalization = torch.nn.LayerNorm(self.model_dim)\n\n    def forward(self, x_context: torch.Tensor) -> torch.Tensor:\n        \"\"\" Forward pass for computing the multi-branching projections.\n\n        Args:\n            x_context: The input tensor (which should be the gene score after\n              the attention module).\n\n        Returns:\n            A tensor containing the gene scores (after residual + layer norm).\n\n        Raises:\n            None.\n\n        \"\"\"\n        batch_size = x_context.size(0)\n\n        # Performing linear projections.\n        x_proj = self.projection(x_context)\n        x_all_heads = x_proj.view(batch_size, -1, self.number_of_projections,\n                                  self.projection_dims)\n        x = x_all_heads.permute(2, 0, 1, 3).contiguous().view(\n            batch_size * self.number_of_projections, -1, self.projection_dims)\n        # Restore input shapes.\n        x_inp_shape = torch.cat(torch.chunk(x,\n                                            self.number_of_projections,\n                                            dim=0),\n                                dim=2)\n        x = x_inp_shape.squeeze()\n        # Applying a residual connection before normalization.\n        x += x_context\n\n        return self.normalization(x)", ""]}
{"filename": "scanna/model/sparsemax/__init__.py", "chunked_list": ["\"\"\"scANNA's third level import SparseMax.\"\"\"\n# Left blank intentionally"]}
{"filename": "scanna/model/sparsemax/sparsemax.py", "chunked_list": ["\"\"\"Sparsemax activation function.\nPytorch implementation of Sparsemax function from:\n-- \"From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\"\n-- Andr\u00e9 F. T. Martins, Ram\u00f3n Fernandez Astudillo (http://arxiv.org/abs/1602.02068)\n\"\"\"\n\nfrom __future__ import division\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass SparseMax(nn.Module):\n    \"\"\"Sparsemax function.\"\"\"\n\n    def __init__(self, dim=None):\n        \"\"\"Initialize sparsemax activation\n        \n        Args:\n            dim (int, optional): The dimension over which to apply the sparsemax function.\n        \"\"\"\n        super(Sparsemax, self).__init__()\n\n        self.dim = -1 if dim is None else dim\n\n    def forward(self, input):\n        \"\"\"Forward function.\n        Args:\n            input (torch.Tensor): Input tensor. First dimension should be the batch size\n        Returns:\n            torch.Tensor: [batch_size x number_of_logits] Output tensor\n        \"\"\"\n        # Sparsemax currently only handles 2-dim tensors,\n        # so we reshape to a convenient shape and reshape back after sparsemax\n        input = input.transpose(0, self.dim)\n        original_size = input.size()\n        input = input.reshape(input.size(0), -1)\n        input = input.transpose(0, 1)\n        dim = 1\n\n        number_of_logits = input.size(dim)\n\n        # Translate input by max for numerical stability\n        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n\n        # Sort input in descending order.\n        # (NOTE: Can be replaced with linear time selection method described here:\n        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n        range = range.expand_as(zs)\n\n        # Determine sparsity of projection\n        bound = 1 + range * zs\n        cumulative_sum_zs = torch.cumsum(zs, dim)\n        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n\n        # Compute threshold function\n        zs_sparse = is_gt * zs\n\n        # Compute taus\n        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n        taus = taus.expand_as(input)\n\n        # Sparsemax\n        self.output = torch.max(torch.zeros_like(input), input - taus)\n\n        # Reshape back to original shape\n        output = self.output\n        output = output.transpose(0, 1)\n        output = output.reshape(original_size)\n        output = output.transpose(0, self.dim)\n\n        return output\n\n    def backward(self, grad_output):\n        \"\"\"Backward function.\"\"\"\n        dim = 1\n\n        nonzeros = torch.ne(self.output, 0)\n        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n\n        return self.grad_input"]}
{"filename": "scanna/utilities/sparsity_handling.py", "chunked_list": ["\"\"\"Utility function for handling sparse count matrices.\"\"\"\n\nimport numpy as np\nimport scanpy\nfrom scipy import sparse\n\ndef sparse_to_dense(annotated_data: scanpy.AnnData):\n    \"\"\"Utility function to return a dense count matrix.\n\n    Args:\n        annotated_data: The scanpy AnnData object with count matrix at \"data.X\".\n\n    Returns:\n        A numpy array of the dense matrix, either after conversion from a sparse\n        matrix or the count itself if it was dense.\n\n    Raises:\n        None.\n\n    \"\"\"\n    if sparse.issparse(annotated_data.X):\n        return np.asarray(annotated_data.X.todense())\n    else:\n        return np.asarray(annotated_data.X)", ""]}
{"filename": "scanna/utilities/weight_initialization.py", "chunked_list": ["\"\"\"Utility functions initializing neural network weights.\"\"\"\nimport torch\n\n\ndef init_weights_xavier_uniform(model: torch.nn):\n    \"\"\"Initializing the weights of a model with Xavier uniform distribution\n\n    Args:\n        model: The pytorch model that will be initilized with Xavier weights.\n\n    Returns:\n        None. All changes will be inplace.\n\n    Raises:\n        None.\n    \"\"\"\n    # We want to initialize the linear layers.\n    if isinstance(model, torch.nn.Linear):\n        torch.nn.init.xavier_uniform_(model.weight)\n        model.bias.data.fill_(0.01)", "\n\ndef init_weights_xavier_normal(model: torch.nn):\n    \"\"\"Initializing the weights of a model with Xavier normal distribution\n\n    Args:\n        model: The pytorch model that will be initilized with Xavier weights.\n\n    Returns:\n        None. All changes will be inplace.\n\n    Raises:\n        None.\n    \"\"\"\n    # We want to initialize the linear layers.\n    if isinstance(model, torch.nn.Linear):\n        torch.nn.init.xavier_normal_(model.weight)\n        model.bias.data.fill_(0.01)", ""]}
{"filename": "scanna/utilities/model_handling.py", "chunked_list": ["\"\"\"Utility functions for loading and saving neural network models.\"\"\"\nimport os\nimport torch\n\n\ndef load_model(model: torch.nn, pretrained_path: str):\n    \"\"\"Function for loading a pre-trained model from a path.\n\n    Args:\n        model: The pytorch model which will be updated with the pre-trained\n          weights from the path.\n        pretrained_path: The path to where the pre-trained models is saved.\n\n    Returns:\n        The function will return the updated model that was initially passed on,\n        and an integer indicating the number of the epochs that the pre-trained\n        model was trained.\n\n    Raises:\n        None.\n    \"\"\"\n    weights = torch.load(pretrained_path)\n    try:\n        trained_epoch = weights[\"epoch\"]\n    except Exception as error:\n        print(\"Epoch information was not found in the pre-trained model.\"\n              f\"Error: {error}.\")\n        # 50 epochs is the recommended number of epochs for training scANNA.\n        # When we don't have the epoch information, we use 50 to set the epoch\n        # information needed for various functionalities of scANNA.\n        print(\"Setting number of trained epochs to default (50)\")\n        trained_epoch = 50\n    pretrained_dict = weights[\"Saved_Model\"].state_dict()\n    model_dict = model.state_dict()\n    pretrained_dict = {\n        k: v for k, v in pretrained_dict.items() if k in model_dict\n    }\n    model_dict.update(pretrained_dict)\n    model.load_state_dict(model_dict)\n\n    return model, trained_epoch", "\n\ndef save_checkpoint_classifier(model: torch.nn,\n                               epoch: int,\n                               iteration: int,\n                               prefix: str = \"\",\n                               dir_path: str = None):\n    \"\"\"Function for saving pre-trained model for inference.\n\n    Args:\n        model: The neural network we want to save.\n        epoch: The number of epochs that the model has been trained up to (which\n          will be used in the filename).\n        iteration: Current iteration count (will be used in the filename)\n        prefix: Any prefix that should be added to the filename.\n        dir_path: The path where the model should be saved to (optional).\n\n    Returns:\n        None.\n\n    Raises:\n        None.\n    \"\"\"\n\n    if dir_path is None:\n        dir_path = \"./scANNA-Weights/\"\n\n    model_out_path = dir_path + prefix + (f\"model_epoch_{epoch}\"\n                                          f\"_iter_{iteration}.pth\")\n    state = {\"epoch\": epoch, \"Saved_Model\": model}\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n    torch.save(state, model_out_path)\n    print(f\"==> Classifier checkpoint was saved to {model_out_path}\")", "\n\ndef save_best_classifier(model, prefix=\"\", dir_path=None):\n    \"\"\"Function for saving the best model.\n\n    The best model must be determined in the main script based on some defined\n    metric.\n\n    Args:\n        model: The best neural network iteration that should be saved.\n        prefix: Any prefix that should be added to the filename.\n        dir_path: The path where the model should be saved to (optional).\n\n    Returns:\n        None.\n\n    Raises:\n        None.\n    \"\"\"\n\n    if not dir_path:\n        dir_path = \"./BestModelWeights/\"\n\n    model_out_path = dir_path + prefix + \"model_Best.pth\"\n    state = {\"Saved_Model\": model}\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n    torch.save(state, model_out_path)\n    print(f\"==> The best model weights were saved to {model_out_path}\")", ""]}
{"filename": "scanna/utilities/__init__.py", "chunked_list": ["\"\"\"scANNA's second layer import for utilies.\"\"\"\nfrom .model_evaluation import *\nfrom .model_handling import *\nfrom .parameter_counting import *\nfrom .pickling import *\nfrom .sparsity_handling import *\nfrom .weight_initialization import *\n"]}
{"filename": "scanna/utilities/model_evaluation.py", "chunked_list": ["\"\"\"Utility function for evaluating classification performance.\"\"\"\nimport numpy as np\nimport scanpy\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.metrics import classification_report as class_rep\nfrom .sparsity_handling import sparse_to_dense\nimport torch\n\n\ndef evaluate_classifier(valid_data_loader: torch.utils.data.DataLoader,\n                        classification_model: torch.nn,\n                        classification_report: bool = False,\n                        device: str = None,\n                        use_gpu: bool = True):\n    \"\"\"Function for evaluating peformance on validation/test data split.\n\n    Args:\n        valid_data_loader: A dataloader of the validation or test dataset.\n        classification_model: The model which we want to use for validation.\n        classification_report: Boolean determining whether user requires\n          classification report or not.\n        device: The device ('cuda' or 'cpu') on which model evaluation should be\n          performed on.\n        use_gpu: A boolean indicating if we should use GPU devices when they are\n          available.\n\n    Returns\n    -------\n        None\n\n    \"\"\"\n    if device is None and use_gpu is True:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        device = \"cpu\"\n\n    classification_model = classification_model.to(device)\n    print(\"==> Evaluating on Validation Set:\")\n    total = 0\n    correct = 0\n    # for sklearn metrics\n    y_true = np.array([])\n    y_pred = np.array([])\n    with torch.no_grad():\n        for _, data in enumerate(valid_data_loader):\n            features, labels = data\n            labels = labels.to(device)\n            outputs, _, _ = classification_model(features.float().to(device),\n                                                 training=False)\n            _, predicted = torch.max(outputs.squeeze(), 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            y_true = np.append(y_true, labels.detach().cpu().numpy())\n            y_pred = np.append(y_pred, predicted.detach().cpu().numpy())\n\n    print(\"    -> Accuracy of classifier network on validation set:\"\n          f\"{(100 * correct / total):4.4f} %\")\n    # calculating the precision/recall based multi-label F1 score\n    macro_score = f1_score(y_true, y_pred, average=\"macro\")\n    w_score = f1_score(y_true, y_pred, average=\"weighted\")\n    traditional_accuracy = accuracy_score(y_true, y_pred)\n    print(f\"    -> Non-Weighted F1 Score on validation set: {macro_score:4.4f}\")\n    print(f\"    -> Weighted F1 Score on validation set: {w_score:4.4f}\")\n    if classification_report:\n        print(class_rep(y_true, y_pred))\n    return y_true, y_pred, macro_score, w_score, traditional_accuracy", "\ndef evaluate_classifier(valid_data_loader: torch.utils.data.DataLoader,\n                        classification_model: torch.nn,\n                        classification_report: bool = False,\n                        device: str = None,\n                        use_gpu: bool = True):\n    \"\"\"Function for evaluating peformance on validation/test data split.\n\n    Args:\n        valid_data_loader: A dataloader of the validation or test dataset.\n        classification_model: The model which we want to use for validation.\n        classification_report: Boolean determining whether user requires\n          classification report or not.\n        device: The device ('cuda' or 'cpu') on which model evaluation should be\n          performed on.\n        use_gpu: A boolean indicating if we should use GPU devices when they are\n          available.\n\n    Returns\n    -------\n        None\n\n    \"\"\"\n    if device is None and use_gpu is True:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        device = \"cpu\"\n\n    classification_model = classification_model.to(device)\n    print(\"==> Evaluating on Validation Set:\")\n    total = 0\n    correct = 0\n    # for sklearn metrics\n    y_true = np.array([])\n    y_pred = np.array([])\n    with torch.no_grad():\n        for _, data in enumerate(valid_data_loader):\n            features, labels = data\n            labels = labels.to(device)\n            outputs, _, _ = classification_model(features.float().to(device),\n                                                 training=False)\n            _, predicted = torch.max(outputs.squeeze(), 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            y_true = np.append(y_true, labels.detach().cpu().numpy())\n            y_pred = np.append(y_pred, predicted.detach().cpu().numpy())\n\n    print(\"    -> Accuracy of classifier network on validation set:\"\n          f\"{(100 * correct / total):4.4f} %\")\n    # calculating the precision/recall based multi-label F1 score\n    macro_score = f1_score(y_true, y_pred, average=\"macro\")\n    w_score = f1_score(y_true, y_pred, average=\"weighted\")\n    traditional_accuracy = accuracy_score(y_true, y_pred)\n    print(f\"    -> Non-Weighted F1 Score on validation set: {macro_score:4.4f}\")\n    print(f\"    -> Weighted F1 Score on validation set: {w_score:4.4f}\")\n    if classification_report:\n        print(class_rep(y_true, y_pred))\n    return y_true, y_pred, macro_score, w_score, traditional_accuracy", "\n\ndef transfer_cell_types(scanpy_data: scanpy.AnnData,\n                        classification_model: torch.nn,\n                        inplace: bool = True,\n                        device: str = None,\n                        use_gpu: bool = True):\n    \"\"\"\n    Evaluating the performance of the network on validation/test dataset\n\n    Args:\n        valid_data_loader: A dataloader of the validation or test dataset.\n        classification_model: The model which we want to use for validation.\n        inplace: If we want to make the modifications directly to the passed on\n          scanpy object.\n        device: The device ('cuda' or 'cpu') on which model evaluation should be\n          performed on.\n        use_gpu: A boolean indicating if we should use GPU devices when they are\n          available.\n\n    Returns\n    -------\n        None\n\n    \"\"\"\n    if device is None and use_gpu is True:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        device = \"cpu\"\n\n    classification_model = classification_model.to(device)\n    np_data = sparse_to_dense(scanpy_data)\n    print(\"==> Making predictions:\")\n    with torch.no_grad():\n        features = torch.from_numpy(np_data)\n        outputs, _, _ = classification_model(features.float().to(device),\n                                             training=False)\n        _, predicted = torch.max(outputs.squeeze(), 1)\n\n    scanna_labels = predicted.detach().cpu().numpy()\n    if inplace:\n        scanpy_data.obs[\"scANNA_Labels\"] = scanna_labels\n        scanpy_data.obs[\"scANNA_Labels\"] = scanpy_data.obs[\n            \"scANNA_Labels\"].astype(\"category\")\n    else:\n        scanpy_data_copy = scanpy_data.copy()\n        scanpy_data_copy.obs[\"scANA_Labels\"] = scanna_labels\n        return scanpy_data_copy\n\n    print(\">-< Done\")", ""]}
{"filename": "scanna/utilities/pickling.py", "chunked_list": ["\"\"\"Utility functions related to data IO using pickle.\"\"\"\nimport pickle\n\n\ndef Pickler(data, filename: str):\n    \"\"\" A convenient utility function for compressing (\"pickling\") data.\n\n    Args:\n        data: the data source we want to compress.\n        filename: the full filename (including path) of where we want to save\n          the pickled file to.\n\n    Returns:\n        None.\n\n    Raises:\n        None.\n    \"\"\"\n\n    with open(filename, 'wb+') as outfile:\n        pickle.dump(data, outfile)", "\n\ndef Unpickler(filename: str):\n    \"\"\" A convenient utility function for decompressing (\"unpickling\") data.\n\n    Args:\n        filename: the full filename (including path) of where we want to save\n          the pickled file to.\n\n    Returns:\n        Decompressed version of the pickle file that was passed on to the\n        function.\n\n    Raises:\n        None.\n    \"\"\"\n\n    with open(filename, 'rb+') as infile:\n        return_file = pickle.load(infile)\n    return return_file", ""]}
{"filename": "scanna/utilities/parameter_counting.py", "chunked_list": ["\"\"\"Utilty function for counting model parameters.\"\"\"\nfrom prettytable import PrettyTable\nimport torch\n\n\ndef count_parameters(model: torch.nn) -> int:\n    \"\"\"Count the total number of parameters in a torch model.\n\n    This utility function will provide a total number of parameters in a model,\n    which does not provide per module parameter information.\n\n    Args:\n        model: The torch neural network that we want the parameter counts for.\n\n    Returns:\n        An integer representing the number of *trainable* parameters in a model.\n\n    Raises:\n        None.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)", "\n\ndef detailed_count_parameters(model: torch.nn) -> int:\n    \"\"\"Count number of parameters in a model and printed in a prettytable.\n\n    This utility function gets a count of trainable parameters in a torch model,\n    including per module parameter counts.\n\n    Args:\n        model: The torch nerual network that the function will count its\n          parameters.\n\n    Returns:\n        An integer representing the number of *trainable* parameters in a model.\n        Note that this utility function will also preint a prettytable object\n        containing the detailed number of parameters in each module.\n\n    Raises:\n        None.\n    \"\"\"\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad:\n            continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params += params\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return total_params", ""]}
{"filename": "training_and_finetuning_scripts/train_or_finetune_scanna.py", "chunked_list": ["\"\"\"Main script for training scANNA.\"\"\"\n\nfrom __future__ import print_function\n\n# std libs\nfrom adabelief_pytorch import AdaBelief\nimport argparse\nimport pandas as pd\nfrom scanna.utilities import (evaluate_classifier, save_checkpoint_classifier,\n                            init_weights_xavier_uniform, count_parameters,", "from scanna.utilities import (evaluate_classifier, save_checkpoint_classifier,\n                            init_weights_xavier_uniform, count_parameters,\n                            detailed_count_parameters, load_model)\nfrom scanna import scanpy_to_dataloader\nfrom scanna import AdditiveModel, ProjectionAttention, FineTuningModel\nimport scanpy\nimport time\nimport torch\nfrom torch import nn\nimport torch.utils.data", "from torch import nn\nimport torch.utils.data\nimport torch.nn.parallel\nfrom tqdm import tqdm\n\n# It is a good idea to turn this on for autograd issues.\ntorch.autograd.set_detect_anomaly(True)\ntorch.manual_seed(12345)\n\nparser = argparse.ArgumentParser()", "\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--scanna_mode\",\n    type=str,\n    default=\"projection\",\n    help=(\"Which scANNA version we want to use,\"\n          \"default = projection\"),\n)", "          \"default = projection\"),\n)\n\nparser.add_argument(\n    \"--branches\",\n    type=int,\n    default=8,\n    help=(\"The number of branches when using projection blocks\"\n          \" and attention, default = 8\"),\n)", "          \" and attention, default = 8\"),\n)\n\nparser.add_argument(\n    \"--epochs\",\n    type=int,\n    default=50,\n    help=(\"number of epochs to train the classifier,\"\n          \" default = 50\"),\n)", "          \" default = 50\"),\n)\nparser.add_argument(\n    \"--where_to_save_model\",\n    type=str,\n    default=\"./\",\n    help=(\"Directory where the trained model(s) should be saved\"\n          \" to, default='./' \"),\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--data_type\",\n    type=str,\n    default=\"scanpy\",\n    help=\"Type of train/test data, default='scanpy'\",\n)\nparser.add_argument(\n    \"--dataset\",\n    type=str,", "    \"--dataset\",\n    type=str,\n    default=\"\",\n    help=(\"The name of dataset that scANNA is being trained with,\"\n          \"(used in saving the trained model), default=''\"),\n)\nparser.add_argument(\n    \"--annotation_key\",\n    type=str,\n    default=\"celltype\",", "    type=str,\n    default=\"celltype\",\n    help=(\"The key which has label information for training\"\n          \"scANNA (used in saving the trained model),\"\n          \"default='celltype'\"),\n)\nparser.add_argument(\"--use_raw_x\",\n                    default=False,\n                    action=\"store_true\",\n                    help=(\"Whether to use adata.X (if set to 'False'decay flag,\"", "                    action=\"store_true\",\n                    help=(\"Whether to use adata.X (if set to 'False'decay flag,\"\n                          \"or to use adata.raw.X (if set to 'True'). This \"\n                          \"option depends on the type of preprocessing done.\"\n                          \"default=False\")\n                   )\nparser.add_argument(\"--is_there_validation_split\",\n                    type=bool,\n                    default=False,\n                    help=(\"Whether our annoted data also contains a \"", "                    default=False,\n                    help=(\"Whether our annoted data also contains a \"\n                          \"'validation' split or just contains the \"\n                          \"'train'/'test' splits. default=False\")\n                   )\nparser.add_argument(\"--data_path\",\n                    type=str,\n                    default=None,\n                    help=(\"The path to where the dataset is stored,\"\n                          \"default=None\"))", "                    help=(\"The path to where the dataset is stored,\"\n                          \"default=None\"))\nparser.add_argument(\n    \"--metadata_path\",\n    type=str,\n    default=None,\n    help=(\"Path where the metadata is stored\"\n          \"(which will be used for merging with counts),\"\n          \"default=None\"),\n)", "          \"default=None\"),\n)\nparser.add_argument(\"--workers\",\n                    type=int,\n                    default=28,\n                    help=\"The Number of worker for the dataloader, default=28\")\nparser.add_argument(\n    \"--batch_size\",\n    type=int,\n    default=128,", "    type=int,\n    default=128,\n    help=(\"The desired batch size for Dataloader,\"\n          \"default = 128\"),\n)\nparser.add_argument(\n    \"--optimizer\",\n    type=str,\n    default=\"adam\",\n    help=\"The desired optimizer, default=adam\",", "    default=\"adam\",\n    help=\"The desired optimizer, default=adam\",\n)\nparser.add_argument(\"--lr\",\n                    type=float,\n                    default=1e-04,\n                    help=\"learning rate, default=0.0001\")\nparser.add_argument(\"--lr_decay\",\n                    default=False,\n                    action=\"store_true\",", "                    default=False,\n                    action=\"store_true\",\n                    help=\"Enables lr decay, default=False\")\nparser.add_argument(\"--decay_rate\",\n                    type=float,\n                    default=0.95,\n                    help=\"the decay rate for the lr, default=0.95\")\nparser.add_argument(\n    \"--decay_epoch\",\n    type=int,", "    \"--decay_epoch\",\n    type=int,\n    default=10,\n    help=(\"Number of epochs that must be passed before model\"\n          \"starts decaying the learning rate, default=10\"),\n)\nparser.add_argument(\n    \"--decay_frequency\",\n    type=int,\n    default=25,", "    type=int,\n    default=25,\n    help=(\"How often should the learning rate decay (after\"\n          \"decay_epoch` many epochs have passed\"\n          \"default=25\"),\n)\nparser.add_argument(\n    \"--decay_reset_epoch\",\n    type=int,\n    default=20,", "    type=int,\n    default=20,\n    help=(\"The epoch in which we should set the learning rate\"\n          \"back to the initial LR decayed, default=20\"),\n)\nparser.add_argument(\"--decay_flag\",\n                    type=bool,\n                    default=False,\n                    help=\"decay flag, default=False\")\nparser.add_argument(\"--momentum\",", "                    help=\"decay flag, default=False\")\nparser.add_argument(\"--momentum\",\n                    default=0.9,\n                    type=float,\n                    help=\"Desired momentum value, default=0.9\")\nparser.add_argument(\"--clip\",\n                    type=float,\n                    default=100,\n                    help=\"The threshod for gradient clipping, default= 100\")\n# The options below are useful for transfer learning, where the starting epoch", "                    help=\"The threshod for gradient clipping, default= 100\")\n# The options below are useful for transfer learning, where the starting epoch\n# may differ from the current epoch stored in the model\nparser.add_argument(\"--pretrained\",\n                    default=None,\n                    type=str,\n                    help=\"path to pretrained model, default=None\")\nparser.add_argument(\"--finetune\",\n                    default=True,\n                    type=bool,", "                    default=True,\n                    type=bool,\n                    help=\"Whether we want to finetune or not, default=True\")\n\nparser.add_argument(\n    \"--start_epoch\",\n    default=1,\n    type=int,\n    help=\"Manual epoch number (useful for TL with restarts)\",\n)", "    help=\"Manual epoch number (useful for TL with restarts)\",\n)\nparser.add_argument(\n    \"--reset_epochs\",\n    default=False,\n    action=\"store_true\",\n    help=(\"Whether to start training the pretrained model at 0 \"\n          \"or not, default=False\"),\n)\nparser.add_argument(\"--cuda\",", ")\nparser.add_argument(\"--cuda\",\n                    default=True,\n                    action=\"store_true\",\n                    help=\"Whether enable cuda or not, default = True\")\nparser.add_argument(\"--parallel\",\n                    default=True,\n                    action=\"store_true\",\n                    help=\"enables parallel GPU execution, default = True\")\nparser.add_argument(\"--manualSeed\",", "                    help=\"enables parallel GPU execution, default = True\")\nparser.add_argument(\"--manualSeed\",\n                    type=int,\n                    default=0,\n                    help=\"The value to set as the manual seed, default = 0\")\n\n# Initializing global variables outside of the module level global\n# interpretation\nopt = None\nmodel = None", "opt = None\nmodel = None\n\ndef main():\n    \"\"\" Main function for training scANNA\"\"\"\n\n    # Setting both the model and input options as global variables.\n    global opt, model\n    opt = parser.parse_args()\n    # Flag for parallel GPU usage.\n    para_flag = False\n    # Determine the device for training.\n    print(\"==> Using GPU (CUDA)\")\n    # if we are allowed to run things on CUDA\n    if opt.cuda and torch.cuda.is_available():\n        device = \"cuda\"\n        # checking for multiple\n        if torch.cuda.device_count() > 1 and opt.parallel is True:\n            print(\"==> We will try to use\", torch.cuda.device_count(),\n                  \"GPUs for training\")\n            para_flag = True\n\n    else:\n        device = \"cpu\"\n        print(\"==> Using CPU\")\n        print(\"    -> Warning: Using CPUs will yield to slower training\"\n              \"time than GPUs\")\n    # --------------------- DATA LOADING ---------------------\n    print(f\" Looking for file named: {opt.data_path}\")\n    if opt.data_type.lower() == \"scanpy\":\n        print(f\"==> Reading Scanpy object for {opt.dataset}: \")\n        adata = scanpy.read_h5ad(opt.data_path)\n        if opt.metadata_path is not None:\n            metadata = pd.read_csv(opt.metadata_path)\n            print(\"    -> Merging metadata with existing ann data\")\n            try:\n                adata.obs = adata.obs.merge(metadata,\n                                            left_on=\"barcodes\",\n                                            right_on=\"barcodes\",\n                                            copy=False,\n                                            suffixes=(\"\", \"_drop\"))\n                adata.obs = adata.obs[\n                    adata.obs.columns[~adata.obs.columns.str.endswith(\"_drop\")]]\n                adata.obs.index = adata.obs[\"barcodes\"]\n\n            except KeyError as _:\n                print(\"    -> Merging on barcode failed, trying to merge on\"\n                      \"index instead\")\n                adata.obs[\"barcodes_orig\"] = adata.obs.index.tolist()\n                adata.obs = adata.obs.merge(metadata,\n                                            left_on=\"barcodes_orig\",\n                                            right_on=\"index\",\n                                            copy=False,\n                                            suffixes=(\"\", \"_drop\"))\n                adata.obs = adata.obs[\n                    adata.obs.columns[~adata.obs.columns.str.endswith(\"_drop\")]]\n                adata.obs.index = adata.obs[\"index\"]\n        # Here we are using a homemade utility function to turn scanpy object\n        # to torch dataloader\n        train_data_loader, valid_data_loader = scanpy_to_dataloader(\n            scanpy_object=adata,\n            batch_size=opt.batch_size,\n            workers=opt.workers,\n            verbose=1,\n            annotation_key = opt.annotation_key,\n            raw_x=opt.use_raw_x)\n\n        # Getting input output information for the network\n        num_genes = [\n            batch[0].shape[1] for _, batch in enumerate(valid_data_loader, 0)\n        ][0]\n        # Getting the number of celltypes.\n        number_of_classes = len(adata.obs[opt.annotation_key].unique())\n        print(f\"==> Number of classes {number_of_classes}\")\n    else:\n        raise ValueError(\">-< The data type provided is not recognized yet\")\n\n    start_time = time.time()\n    # --------------------- BUILDING THE MODEL ---------------------\n    branching_heads = opt.branches\n    print(f\"Number of Branching Projections: {branching_heads}\")\n\n    if opt.pretrained:\n        mode = \"Pretrained\"\n        print(f\"==> Loading pre-trained model from {opt.pretrained}\")\n        # First we need to setup a placeholder models to load in the trained\n        # weights into.\n        pretrained_model = ProjectionAttention(\n                                input_dimension=num_genes,\n                                task_module_output_dimension=number_of_classes,\n                                number_of_projections=branching_heads,\n                                device=device).to(device)\n\n        _, trained_epoch = load_model(pretrained_model, opt.pretrained)\n        print(f\"    -> Loaded model was trained for {trained_epoch} epochs\")\n        if opt.finetune:\n            # Rewriting mode to reflect finetuning.\n            mode = \"Finetuning\"\n            print(f\"    -> Setting up fine-tuning model\")\n            model = FineTuningModel(\n                    pretrained_scanna_model=pretrained_model,\n                    task_module_output_dimension = number_of_classes,\n                    device=device,\n                    ).to(device)\n\n        if not opt.reset_epochs:\n            print(f\"        -> Not resetting the start epoch to 0 \")\n            opt.start_epoch = trained_epoch\n        else:\n            print(f\"        -> Strating finetuning from epoch 0 \")\n        print(\"  -><- Loaded from a pre-trained model!\")\n\n    else:\n        # for testing purposes\n        if opt.scanna_mode.lower() == \"additive-attn\":\n            mode = \"AdditiveAttn\"\n            model = AdditiveModel(input_dimension=num_genes,\n                                      output_dimension=number_of_classes,\n                                      device=device).to(device)\n\n        elif opt.scanna_mode.lower() == \"projection\":\n            mode = \"Pojections+Attention\"\n            model = ProjectionAttention(\n                input_dimension=num_genes,\n                task_module_output_dimension=number_of_classes,\n                number_of_projections=branching_heads,\n                dropout=0.0,\n                device=device).to(device)\n        else:\n            print(f\"==> selected training mode: {opt.scanna_mode}\")\n            raise ValueError(\"scANNA modes are 'additive-attn' or 'projection'.\"\n                             \"Please check your entry.\")\n\n        print(\"Initializing *untrained* weights to Xavier Uniform\")\n        # initilize the weights in our model\n        model.apply(init_weights_xavier_uniform)\n\n    # If parallel gpus is enables, we want to load the model on multiple gpus\n    # and distribute the data if we want parallel.\n    if para_flag:\n        model = nn.DataParallel(model)\n\n    # The loss for the task module of scANNA\n    criterion = torch.nn.CrossEntropyLoss()\n\n    # --------------------- Optimizer Setting ---------------------\n    if opt.optimizer.lower() == \"adam\":\n        print(\"==> Optimizer: Adam\")\n        optimizer = torch.optim.Adam(params=model.parameters(),\n                                     lr=opt.lr,\n                                     betas=(0.9, 0.999),\n                                     eps=1e-08,\n                                     weight_decay=0.000,\n                                     amsgrad=False)\n\n    elif opt.optimizer.lower() == \"adabelief\":\n        print(\"==> Optimizer: AdaBelief\")\n        # Adabelief is better to be used with Transformer/LSTM1 parameters.\n        optimizer = AdaBelief(params=model.parameters(),\n                              lr=opt.lr,\n                              eps=1e-16,\n                              betas=(0.9, 0.999),\n                              weight_decay=1.2e-6,\n                              weight_decouple=False,\n                              rectify=True,\n                              fixed_decay=True,\n                              amsgrad=False)\n\n    elif opt.optimizer.lower() == \"adamax\":\n        print(\"==> Optimizer: AdaMax\")\n        optimizer = torch.optim.Adamax(params=model.parameters(),\n                                       lr=opt.lr,\n                                       betas=(0.9, 0.999),\n                                       eps=1e-08,\n                                       weight_decay=0)\n\n    elif opt.optimizer.lower() == \"amsgrad\":\n        print(\"==> Optimizer: AmsGrad Variant\")\n        optimizer = torch.optim.Adam(params=model.parameters(),\n                                     lr=opt.lr,\n                                     betas=(0.9, 0.999),\n                                     eps=1e-08,\n                                     weight_decay=0,\n                                     amsgrad=True)\n    else:\n        raise ValueError(f\"The provided optimizer:{opt.optimizer} is not\"\n                         \"available yet. Please use from: {'adam', 'adamax'\"\n                         \"'adabelief', 'amsgrad'}.\")\n\n    # --------------------- LR Decay Setting ---------------------\n    decay_setting = opt.lr_decay\n    if decay_setting:\n        print(f\"    -> Training with lr decay {opt.decay_rate}\")\n        print(f\"    -> lr decay occurs at every {opt.decay_frequency} Epochs\")\n        cf_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n            optimizer=optimizer, gamma=opt.decay_rate)\n    else:\n        print(\"    -> Training with fixed lr \")\n\n    # --------------------- Training Loop ---------------------\n    print(f\"Model: {model}\")\n    print(detailed_count_parameters(model))\n    print(f\"Total number of *trainable parameters* : {count_parameters(model)}\")\n\n    cur_iter = 0\n    batch_idx = None\n    for ep in tqdm(range(0, opt.epochs), desc=f\"scANNA {mode} Training\"):\n        batch_losses = []\n\n        for batch_idx, data in enumerate(train_data_loader):\n            cur_iter += 1\n            features, labels = data\n            optimizer.zero_grad()\n            output, _, _ = model(features.float().to(device),\n                                            training=True)\n            loss = criterion(output.squeeze(), labels.to(device))\n            batch_losses.append(loss.data.item())\n\n            loss.backward()\n            optimizer.step()\n\n            # Logic for decaying the lr:\n            if decay_setting and ep >= opt.decay_epoch:\n                if (ep % opt.decay_frequency == 0 and cur_iter != 0 and\n                        opt.decay_flag is False):\n\n                    opt.decay_flag = True\n                    cf_lr_scheduler.step()\n                    for param_group in optimizer.param_groups:\n                        print(f\"\\n    -> Decayed lr to -> {param_group['lr']}\")\n\n                if ep % opt.decay_reset_epoch == 0 and opt.decay_flag is True:\n                    opt.decay_flag = False\n                    # for the encoder\n                    for param_group in optimizer.param_groups:\n                        if param_group[\"lr\"] < 9e-07:\n                            param_group[\"lr\"] = opt.lr\n                            print(\"    -> lr too small.\"\n                                  f\" Restting lr to -> {param_group['lr']}\")\n\n                        # Resetting the learning rate to a fraction of\n                        # the initial lr\n                        param_group[\"lr\"] = opt.lr / ((ep / 100) + 1)\n                        print(\"    -> Resetting lr to be\"\n                              f\"= {param_group['lr']}\")\n\n                # Resetting the flag to allow for future decays\n                if ep % opt.decay_frequency != 0:\n                    opt.decay_flag = False\n\n        if not ep % 5:\n            print(f\"[TRAIN] Epoch: {ep} - Iteration: {cur_iter}, \"\n                  f\"loss: {loss.data.item()}\")\n\n\n# ------------------- Evaluating on Validation Loop -------------------\n#        valid_scores = [0]\n# If validation data is available, you should uncomment this part for\n# early stopping and evaluation of the model.\n#         batch_losses = []\n\n#         for batch_idx, data in enumerate(valid_data_loader):\n#             features, labels = data\n#             features = features.float()\n#             output, alphas, context = model(features.float().to(device),\n#                                             training=False)\n\n#             loss = criterion(output.squeeze(), labels.to(device))\n#             batch_losses.append(loss.data.item())\n\n#         _, _, cur_score, _, _ = evaluate_classifier(\n#                                                   valid_data_loader,\n#                                                   model,\n#                                                   classification_report=False)\n# -----------------------------------------------------------------------------\n\n    print(f\"==> Total training time {time.time() - start_time}\")\n    save_checkpoint_classifier(\n        model,\n        opt.epochs,\n        0,\n        f\"scANNA-{mode}-{opt.dataset}-{branching_heads}Branches-\",\n        dir_path=opt.where_to_save_model)\n\n    _, _, _, _, _ = evaluate_classifier(valid_data_loader,\n                                                  model,\n                                                  classification_report=True)", "\nif __name__ == \"__main__\":\n    main()\n"]}
