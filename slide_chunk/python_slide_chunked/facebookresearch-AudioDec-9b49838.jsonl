{"filename": "demoStream.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n### useage ###", "\n### useage ###\n# (run w/ gpu): python dempStream.py --tx_cuda 1 --rx_cuda 2 --model libritts_v1 --input_device x --output_device o \n# (run w/ cpu): python dempStream.py --tx_cuda -1 --rx_cuda -1 --model libritts_sym --input_device x --output_device o \n\nimport torch\nimport argparse\nfrom utils.audiodec import AudioDec, AudioDecStreamer, assign_model\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=\"libritts_sym\")\n    parser.add_argument(\"-i\", \"--input\", type=str, default=\"input.wav\")\n    parser.add_argument(\"-o\", \"--output\", type=str, default=\"output.wav\")\n    parser.add_argument('--tx_cuda', type=int, default=-1 )\n    parser.add_argument('--rx_cuda', type=int, default=-1 )\n    parser.add_argument('--input_device', type=int, default=1)\n    parser.add_argument('--output_device', type=int, default=4)\n    parser.add_argument('--frame_size', type=int, default=1200)\n    parser.add_argument('--num_threads', type=int, default=4)\n    args = parser.parse_args()\n\n    # device assignment\n    if args.tx_cuda < 0:\n        tx_device = f'cpu'\n    else:\n        tx_device = f'cuda:{args.tx_cuda}'\n    if args.rx_cuda < 0:\n        rx_device = f'cpu'\n    else:\n        rx_device = f'cuda:{args.rx_cuda}'\n    torch.set_num_threads(args.num_threads)\n\n    # model assignment\n    sample_rate, encoder_checkpoint, decoder_checkpoint = assign_model(args.model)\n\n    # AudioDec initinalize\n    print(\"AudioDec initinalizing!\")\n    audiodec = AudioDec(tx_device=tx_device, rx_device=rx_device)\n    audiodec.load_transmitter(encoder_checkpoint)\n    audiodec.load_receiver(encoder_checkpoint, decoder_checkpoint)\n\n    # Streamer initinalize\n    print(\"Streamer initinalizing!\")\n    streamer = AudioDecStreamer(\n        input_device=args.input_device,\n        output_device=args.output_device,\n        frame_size=args.frame_size,\n        sample_rate=sample_rate,\n        tx_encoder=audiodec.tx_encoder,\n        tx_device=tx_device,\n        rx_encoder=audiodec.rx_encoder,\n        decoder=audiodec.decoder,\n        rx_device=rx_device,\n    )\n\n    streamer.enable_filedump(\n        input_stream_file=args.input,\n        output_stream_file=args.output,\n    )\n\n    # run\n    print(\"Ready to run!\")\n    latency=\"low\"\n    # TODO this is responsible for ~100ms latency, seems to be driver dependent. latency=0 works on Mac but not on Windows\n    streamer.run(latency)", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=\"libritts_sym\")\n    parser.add_argument(\"-i\", \"--input\", type=str, default=\"input.wav\")\n    parser.add_argument(\"-o\", \"--output\", type=str, default=\"output.wav\")\n    parser.add_argument('--tx_cuda', type=int, default=-1 )\n    parser.add_argument('--rx_cuda', type=int, default=-1 )\n    parser.add_argument('--input_device', type=int, default=1)\n    parser.add_argument('--output_device', type=int, default=4)\n    parser.add_argument('--frame_size', type=int, default=1200)\n    parser.add_argument('--num_threads', type=int, default=4)\n    args = parser.parse_args()\n\n    # device assignment\n    if args.tx_cuda < 0:\n        tx_device = f'cpu'\n    else:\n        tx_device = f'cuda:{args.tx_cuda}'\n    if args.rx_cuda < 0:\n        rx_device = f'cpu'\n    else:\n        rx_device = f'cuda:{args.rx_cuda}'\n    torch.set_num_threads(args.num_threads)\n\n    # model assignment\n    sample_rate, encoder_checkpoint, decoder_checkpoint = assign_model(args.model)\n\n    # AudioDec initinalize\n    print(\"AudioDec initinalizing!\")\n    audiodec = AudioDec(tx_device=tx_device, rx_device=rx_device)\n    audiodec.load_transmitter(encoder_checkpoint)\n    audiodec.load_receiver(encoder_checkpoint, decoder_checkpoint)\n\n    # Streamer initinalize\n    print(\"Streamer initinalizing!\")\n    streamer = AudioDecStreamer(\n        input_device=args.input_device,\n        output_device=args.output_device,\n        frame_size=args.frame_size,\n        sample_rate=sample_rate,\n        tx_encoder=audiodec.tx_encoder,\n        tx_device=tx_device,\n        rx_encoder=audiodec.rx_encoder,\n        decoder=audiodec.decoder,\n        rx_device=rx_device,\n    )\n\n    streamer.enable_filedump(\n        input_stream_file=args.input,\n        output_stream_file=args.output,\n    )\n\n    # run\n    print(\"Ready to run!\")\n    latency=\"low\"\n    # TODO this is responsible for ~100ms latency, seems to be driver dependent. latency=0 works on Mac but not on Windows\n    streamer.run(latency)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "codecStatistic.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\nimport os\nimport sys\nimport yaml\nimport torch\nimport logging\nimport argparse\nimport numpy as np", "import argparse\nimport numpy as np\nimport soundfile as sf\n\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom dataloader import SingleDataset\nfrom models.autoencoder.AudioDec import Generator as generator_audiodec\n\n\nclass StatisticMain(object):\n    def __init__(self, args,):\n        # set logger\n        logging.basicConfig(\n            level=logging.INFO,\n            stream=sys.stdout,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n\n        # device\n        if not torch.cuda.is_available():\n            self.device = torch.device('cpu')\n            logging.info(f\"device: cpu\")\n        else:\n            self.device = torch.device('cuda')\n            logging.info(f\"device: gpu\")\n        \n        # initialize config\n        with open(args.config, 'r') as f:\n            self.config = yaml.load(f, Loader=yaml.FullLoader)\n        \n        # initialize attribute\n        self.stats_path = self.config['stats']\n        self.analyzer_checkpoint = self.config['analyzer']\n        self.analyzer_config = self._load_config(self.analyzer_checkpoint)\n        self.model_type = self.analyzer_config.get('model_type', 'symAudioDec')\n        os.makedirs(os.path.dirname(self.stats_path), exist_ok=True) \n\n\n    def _load_config(self, checkpoint, config_name='config.yml'):\n        dirname = os.path.dirname(checkpoint)\n        config_path = os.path.join(dirname, config_name)\n        with open(config_path) as f:\n            config = yaml.load(f, Loader=yaml.Loader)\n        return config\n\n\n    def load_dataset(self, subset, subset_num):\n        audio_path = os.path.join(\n            self.config['data']['path'], \n            self.config['data']['subset'][subset],\n        )\n        assert os.path.exists(audio_path), f\"{audio_path} does not exist!\"\n        self.dataset = SingleDataset(\n            files=audio_path,\n            query=\"*.wav\",\n            load_fn=sf.read,\n            return_utt_id=False,\n            subset_num=subset_num,\n        )\n        logging.info(f\"The number of {subset} audio files = {len(self.dataset)}.\")\n    \n        \n    def load_analyzer(self):\n        if self.model_type in ['symAudioDec', 'symAudioDecUniv']:\n            analyzer = generator_audiodec\n        else:     \n            raise NotImplementedError(f\"Analyzer {self.model_type} is not supported!\")\n        self.analyzer = analyzer(**self.analyzer_config['generator_params'])\n        self.analyzer.load_state_dict(\n            torch.load(self.analyzer_checkpoint, map_location='cpu')['model']['generator'])\n        self.analyzer = self.analyzer.eval().to(self.device)\n        logging.info(f\"Loaded Analyzer from {self.analyzer_checkpoint}.\")\n\n\n    def audio_analysis(self, audio):\n        x = torch.tensor(audio, dtype=torch.float).to(self.device)\n        x = x.transpose(1, 0).unsqueeze(0) # (T, C) -> (1, C, T)\n        x = self.analyzer.encoder(x)\n        z = self.analyzer.projector(x)\n        zq, _, _ = self.analyzer.quantizer(z)\n        return zq.squeeze(0).transpose(1, 0).cpu().numpy() # (T', C)\n\n    \n    def run(self):\n        with torch.no_grad(), tqdm(self.dataset, desc=\"[statistic]\") as pbar:\n            scaler = StandardScaler()\n            for idx, x in enumerate(pbar, 1):\n                zq = self.audio_analysis(x)\n                scaler.partial_fit(zq)\n            stats = np.stack([scaler.mean_, scaler.scale_], axis=0)\n            np.save(\n                    self.stats_path,\n                    stats.astype(np.float32),\n                    allow_pickle=False,\n            )   \n        logging.info(f\"Finished statistical calculation of {idx} utterances.\")", "\n\nclass StatisticMain(object):\n    def __init__(self, args,):\n        # set logger\n        logging.basicConfig(\n            level=logging.INFO,\n            stream=sys.stdout,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n\n        # device\n        if not torch.cuda.is_available():\n            self.device = torch.device('cpu')\n            logging.info(f\"device: cpu\")\n        else:\n            self.device = torch.device('cuda')\n            logging.info(f\"device: gpu\")\n        \n        # initialize config\n        with open(args.config, 'r') as f:\n            self.config = yaml.load(f, Loader=yaml.FullLoader)\n        \n        # initialize attribute\n        self.stats_path = self.config['stats']\n        self.analyzer_checkpoint = self.config['analyzer']\n        self.analyzer_config = self._load_config(self.analyzer_checkpoint)\n        self.model_type = self.analyzer_config.get('model_type', 'symAudioDec')\n        os.makedirs(os.path.dirname(self.stats_path), exist_ok=True) \n\n\n    def _load_config(self, checkpoint, config_name='config.yml'):\n        dirname = os.path.dirname(checkpoint)\n        config_path = os.path.join(dirname, config_name)\n        with open(config_path) as f:\n            config = yaml.load(f, Loader=yaml.Loader)\n        return config\n\n\n    def load_dataset(self, subset, subset_num):\n        audio_path = os.path.join(\n            self.config['data']['path'], \n            self.config['data']['subset'][subset],\n        )\n        assert os.path.exists(audio_path), f\"{audio_path} does not exist!\"\n        self.dataset = SingleDataset(\n            files=audio_path,\n            query=\"*.wav\",\n            load_fn=sf.read,\n            return_utt_id=False,\n            subset_num=subset_num,\n        )\n        logging.info(f\"The number of {subset} audio files = {len(self.dataset)}.\")\n    \n        \n    def load_analyzer(self):\n        if self.model_type in ['symAudioDec', 'symAudioDecUniv']:\n            analyzer = generator_audiodec\n        else:     \n            raise NotImplementedError(f\"Analyzer {self.model_type} is not supported!\")\n        self.analyzer = analyzer(**self.analyzer_config['generator_params'])\n        self.analyzer.load_state_dict(\n            torch.load(self.analyzer_checkpoint, map_location='cpu')['model']['generator'])\n        self.analyzer = self.analyzer.eval().to(self.device)\n        logging.info(f\"Loaded Analyzer from {self.analyzer_checkpoint}.\")\n\n\n    def audio_analysis(self, audio):\n        x = torch.tensor(audio, dtype=torch.float).to(self.device)\n        x = x.transpose(1, 0).unsqueeze(0) # (T, C) -> (1, C, T)\n        x = self.analyzer.encoder(x)\n        z = self.analyzer.projector(x)\n        zq, _, _ = self.analyzer.quantizer(z)\n        return zq.squeeze(0).transpose(1, 0).cpu().numpy() # (T', C)\n\n    \n    def run(self):\n        with torch.no_grad(), tqdm(self.dataset, desc=\"[statistic]\") as pbar:\n            scaler = StandardScaler()\n            for idx, x in enumerate(pbar, 1):\n                zq = self.audio_analysis(x)\n                scaler.partial_fit(zq)\n            stats = np.stack([scaler.mean_, scaler.scale_], axis=0)\n            np.save(\n                    self.stats_path,\n                    stats.astype(np.float32),\n                    allow_pickle=False,\n            )   \n        logging.info(f\"Finished statistical calculation of {idx} utterances.\")", "\n\ndef main():\n    \"\"\"Run feature extraction process.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-c', '--config', type=str, required=True)\n    parser.add_argument(\"--subset\", type=str, default=\"train\")\n    parser.add_argument(\"--subset_num\", type=int, default=-1)\n    args = parser.parse_args()\n\n    # initial statistic_main\n    statistic_main = StatisticMain(args=args) \n\n    # load dataset\n    statistic_main.load_dataset(args.subset, args.subset_num)\n\n    # load analyzer\n    statistic_main.load_analyzer()\n    \n    # run testing\n    statistic_main.run()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "demoFile.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n### useage ###", "\n### useage ###\n# (run w/ gpu): python demoFile.py --model libritts_v1 -i xxx.wav -o ooo.wav\n# (run w/ cpu): python demoFile.py --cuda -1 --model libritts_sym -i xxx.wav -o ooo.wav\n\nimport os\nimport torch\nimport argparse\nimport numpy as np\nimport soundfile as sf", "import numpy as np\nimport soundfile as sf\nfrom utils.audiodec import AudioDec, assign_model\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=\"libritts_v1\")\n    parser.add_argument(\"-i\", \"--input\", type=str, required=True)\n    parser.add_argument(\"-o\", \"--output\", type=str, required=True)\n    parser.add_argument('--cuda', type=int, default=0 )\n    parser.add_argument('--num_threads', type=int, default=4)\n    args = parser.parse_args()\n\n    # device assignment\n    if args.cuda < 0:\n        tx_device = f'cpu'\n        rx_device = f'cpu'\n    else:\n        tx_device = f'cuda:{args.cuda}'\n        rx_device = f'cuda:{args.cuda}'\n    torch.set_num_threads(args.num_threads)\n\n    # model assignment\n    sample_rate, encoder_checkpoint, decoder_checkpoint = assign_model(args.model)\n\n    # AudioDec initinalize\n    print(\"AudioDec initinalizing!\")\n    audiodec = AudioDec(tx_device=tx_device, rx_device=rx_device)\n    audiodec.load_transmitter(encoder_checkpoint)\n    audiodec.load_receiver(encoder_checkpoint, decoder_checkpoint)\n\n    with torch.no_grad():\n        if os.path.exists(args.input):\n            data, fs = sf.read(args.input, always_2d=True)\n        else:\n            raise ValueError(f'Input file {args.input} does not exist!')\n        assert fs == sample_rate, f\"data ({fs}Hz) is not matched to model ({sample_rate}Hz)!\"\n        x = np.expand_dims(data.transpose(1, 0), axis=1) # (T, C) -> (C, 1, T)\n        x = torch.tensor(x, dtype=torch.float).to(tx_device)\n        print(\"Encode/Decode...\")\n        z = audiodec.tx_encoder.encode(x)\n        idx = audiodec.tx_encoder.quantize(z)\n        zq = audiodec.rx_encoder.lookup(idx)\n        y = audiodec.decoder.decode(zq)[:, :, :x.size(-1)]\n        y = y.squeeze(1).transpose(1, 0).cpu().numpy() # T x C\n        sf.write(\n            args.output,\n            y,\n            fs,\n            \"PCM_16\",\n        )\n        print(f\"Output {args.output}!\")", "\n    \n\n\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "codecTrain.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\nimport os\nimport logging\nimport argparse\nimport torch\nimport soundfile as sf\n\nfrom torch.utils.data import DataLoader", "\nfrom torch.utils.data import DataLoader\nfrom dataloader import CollaterAudio, CollaterAudioPair\nfrom dataloader import SingleDataset, MultiDataset\n\nfrom models.autoencoder.AudioDec import Generator as generator_audiodec\nfrom models.vocoder.HiFiGAN import Generator as generator_hifigan\nfrom models.vocoder.HiFiGAN import Discriminator as discriminator_hifigan\nfrom models.vocoder.UnivNet import Discriminator as discriminator_univnet\n", "from models.vocoder.UnivNet import Discriminator as discriminator_univnet\n\nfrom trainer.autoencoder import Trainer as TrainerAutoEncoder\nfrom trainer.vocoder import Trainer as TrainerVocoder\nfrom trainer.denoise import Trainer as TrainerDenoise\nfrom bin.train import TrainGAN\n\nfrom losses import DiscriminatorAdversarialLoss\nfrom losses import FeatureMatchLoss\nfrom losses import GeneratorAdversarialLoss", "from losses import FeatureMatchLoss\nfrom losses import GeneratorAdversarialLoss\nfrom losses import MultiResolutionSTFTLoss\nfrom losses import MultiMelSpectrogramLoss\nfrom losses import MultiWindowShapeLoss\n\n\nclass TrainMain(TrainGAN):\n    def __init__(self, args,):\n        super(TrainMain, self).__init__(args=args,)\n        self.train_mode = self.config.get('train_mode', 'autoencoder')\n        self.model_type = self.config.get('model_type', 'symAudioDec')\n        self.data_path = self.config['data']['path']\n    \n    # DATA LOADER\n    def initialize_data_loader(self):\n        logging.info(f\"Loading datasets... (batch_lenght: {self.batch_length})\")\n\n        if self.train_mode in ['autoencoder', 'vocoder']:\n            train_set = self._audio('train')\n            valid_set = self._audio('valid')\n            collater = CollaterAudio(batch_length=self.batch_length)\n        elif self.train_mode in ['denoise']:\n            train_set = self._audio_pair('noisy_train', 'clean_train')\n            valid_set = self._audio_pair('noisy_valid', 'clean_valid')\n            collater = CollaterAudioPair(batch_length=self.batch_length)\n        else:\n            raise NotImplementedError(f\"Train mode: {self.train_mode} is not supported!\")\n\n        logging.info(f\"The number of training files = {len(train_set)}.\")\n        logging.info(f\"The number of validation files = {len(valid_set)}.\")\n        dataset = {'train': train_set, 'dev': valid_set}\n        self._data_loader(dataset, collater)\n    \n\n    def _data_loader(self, dataset, collater):\n        self.data_loader = {\n            'train': DataLoader(\n                dataset=dataset['train'],\n                shuffle=True,\n                collate_fn=collater,\n                batch_size=self.config['batch_size'],\n                num_workers=self.config['num_workers'],\n                pin_memory=self.config['pin_memory'],\n            ),\n            'dev': DataLoader(\n                dataset=dataset['dev'],\n                shuffle=False,\n                collate_fn=collater,\n                batch_size=self.config['batch_size'],\n                num_workers=self.config['num_workers'],\n                pin_memory=self.config['pin_memory'],\n            ),\n        }\n    \n    \n    def _audio(self, subset, subset_num=-1, return_utt_id=False):\n        audio_dir = os.path.join(\n            self.data_path, self.config['data']['subset'][subset])\n        params = {\n            'files': audio_dir,\n            'query': \"*.wav\",\n            'load_fn': sf.read,\n            'return_utt_id': return_utt_id,\n            'subset_num': subset_num,\n        }\n        return SingleDataset(**params)\n    \n    \n    def _audio_pair(self, subset_n, subset_c, subset_num=-1, return_utt_id=False):\n        audio_n_dir = os.path.join(\n            self.data_path, self.config['data']['subset'][subset_n])\n        audio_c_dir = os.path.join(\n            self.data_path, self.config['data']['subset'][subset_c])\n        params = {\n            'multi_files': [audio_c_dir, audio_n_dir], # (main, sub)\n            'queries': [\"*.wav\"]*2,\n            'load_fns': [sf.read]*2,\n            'return_utt_id': return_utt_id,\n            'subset_num': subset_num,\n        }\n        return MultiDataset(**params)\n    \n    \n    # MODEL ARCHITECTURE\n    def define_model(self):\n        # generator\n        generator = self._define_generator(self.model_type)\n        self.model['generator'] = generator.to(self.device)\n        # discriminator\n        discriminator = self._define_discriminator(self.model_type)\n        self.model['discriminator'] = discriminator.to(self.device)\n        # optimizer\n        self._define_optimizer_scheduler()\n        #self._show_setting()\n    \n\n    def _define_generator(self, model_type):\n        if model_type in ['symAudioDec', 'symAudioDecUniv']:\n            generator = generator_audiodec\n        elif model_type in ['HiFiGAN', 'UnivNet']:\n            generator = generator_hifigan\n        else:\n            raise NotImplementedError(f\"Model type: {model_type} is not supported for the generator!\")\n        return generator(**self.config['generator_params'])\n    \n\n    def _define_discriminator(self, model_type):\n        if model_type in ['symAudioDec', 'HiFiGAN']:\n            discriminator = discriminator_hifigan\n        elif model_type in ['symAudioDecUniv', 'UnivNet']:\n            discriminator = discriminator_univnet\n        else:\n            raise NotImplementedError(f\"Model type: {model_type} is not supported for the discriminator!\")\n        return discriminator(**self.config['discriminator_params'])\n    \n    \n    def _define_optimizer_scheduler(self):\n        generator_optimizer_class = getattr(\n            torch.optim, \n            self.config['generator_optimizer_type']\n        )\n        discriminator_optimizer_class = getattr(\n            torch.optim, \n            self.config['discriminator_optimizer_type']\n        )\n        self.optimizer = {\n            'generator': generator_optimizer_class(\n                self.model['generator'].parameters(),\n                **self.config['generator_optimizer_params'],\n            ),\n            'discriminator': discriminator_optimizer_class(\n                self.model['discriminator'].parameters(),\n                **self.config['discriminator_optimizer_params'],\n            ),\n        }\n\n        generator_scheduler_class = getattr(\n            torch.optim.lr_scheduler,\n            self.config.get('generator_scheduler_type', \"StepLR\"),\n        )\n        discriminator_scheduler_class = getattr(\n            torch.optim.lr_scheduler,\n            self.config.get('discriminator_scheduler_type', \"StepLR\"),\n        )\n        self.scheduler = {\n            'generator': generator_scheduler_class(\n                optimizer=self.optimizer['generator'],\n                **self.config['generator_scheduler_params'],\n            ),\n            'discriminator': discriminator_scheduler_class(\n                optimizer=self.optimizer['discriminator'],\n                **self.config['discriminator_scheduler_params'],\n            ),\n        }\n\n\n    # CRITERIA\n    def define_criterion(self):\n        self.criterion = {\n        'gen_adv': GeneratorAdversarialLoss(\n            **self.config['generator_adv_loss_params']).to(self.device),\n        'dis_adv': DiscriminatorAdversarialLoss(\n            **self.config['discriminator_adv_loss_params']).to(self.device),\n        }\n        if self.config.get('use_feat_match_loss', False):\n            self.criterion['feat_match'] = FeatureMatchLoss(\n                **self.config.get('feat_match_loss_params', {}),\n            ).to(self.device)\n        if self.config.get('use_mel_loss', False):\n            self.criterion['mel'] = MultiMelSpectrogramLoss(\n                **self.config['mel_loss_params'],\n            ).to(self.device)\n        if self.config.get('use_stft_loss', False):\n            self.criterion['stft'] = MultiResolutionSTFTLoss(\n                **self.config['stft_loss_params'],\n            ).to(self.device)\n        if self.config.get('use_shape_loss', False):\n            self.criterion['shape'] = MultiWindowShapeLoss(\n                **self.config['shape_loss_params'],\n            ).to(self.device)\n\n\n    # TRAINER\n    def define_trainer(self):\n        if self.train_mode in ['autoencoder']:\n            trainer = TrainerAutoEncoder\n        elif self.train_mode in ['vocoder']:\n            trainer = TrainerVocoder\n        elif self.train_mode in ['denoise']:\n            trainer = TrainerDenoise\n        else:\n            raise NotImplementedError(f\"Train mode: {self.train_mode} is not supported for Trainer!\")\n        trainer_parameters = {}\n        trainer_parameters['steps'] = 0\n        trainer_parameters['epochs'] = 0\n        trainer_parameters['data_loader'] = self.data_loader\n        trainer_parameters['model'] = self.model\n        trainer_parameters['criterion'] = self.criterion\n        trainer_parameters['optimizer'] = self.optimizer\n        trainer_parameters['scheduler'] = self.scheduler\n        trainer_parameters['config'] = self.config\n        trainer_parameters['device'] = self.device\n        self.trainer = trainer(**trainer_parameters)\n    \n\n    # MODEL INITIALIZATION\n    def initialize_model(self):\n        initial = self.config.get(\"initial\", \"\") \n        if os.path.exists(self.resume): # resume from trained model\n            self.trainer.load_checkpoint(self.resume)\n            logging.info(f\"Successfully resumed from {self.resume}.\")\n        elif os.path.exists(initial): # initial new model with the pre-trained model\n            self.trainer.load_checkpoint(initial, load_only_params=True)\n            logging.info(f\"Successfully initialize parameters from {initial}.\")\n        else:\n            logging.info(\"Train from scrach\")\n        # load the pre-trained encoder for vocoder training\n        if self.train_mode in ['vocoder']:\n            analyzer_checkpoint = self.config.get(\"analyzer\", \"\")\n            assert os.path.exists(analyzer_checkpoint), f\"Analyzer {analyzer_checkpoint} does not exist!\"\n            analyzer_config = self._load_config(analyzer_checkpoint)\n            self._initialize_analyzer(analyzer_config, analyzer_checkpoint)\n    \n\n    def _initialize_analyzer(self, config, checkpoint):\n        model_type = config.get('model_type', 'symAudioDec')\n        if model_type in ['symAudioDec', 'symAudioDecUniv']:\n            analyzer = generator_audiodec\n        else:\n            raise NotImplementedError(f\"Model type: {model_type} is not supported for the analyzer!\")\n        self.model['analyzer'] = analyzer(**config['generator_params']).to(self.device)\n        self.model['analyzer'].load_state_dict(\n            torch.load(checkpoint, map_location='cpu')['model']['generator'])\n        logging.info(f\"Successfully load analyzer from {checkpoint}.\")", "\n    \ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-c', '--config', type=str, required=True)\n    parser.add_argument(\"--tag\", type=str, required=True)\n    parser.add_argument(\"--exp_root\", type=str, default=\"exp\")\n    parser.add_argument(\"--resume\", default=\"\", type=str, nargs=\"?\",\n        help='checkpoint file path to resume training. (default=\"\")',\n    )\n    parser.add_argument('--seed', default=1337, type=int)\n    parser.add_argument('--disable_cudnn', choices=('True','False'), default='False', help='Disable CUDNN')\n    args = parser.parse_args()\n        \n    # initial train_main\n    train_main = TrainMain(args=args)   \n\n    # get dataset\n    train_main.initialize_data_loader()\n    \n    # define models, optimizers, and schedulers\n    train_main.define_model()\n    \n    # define criteria\n    train_main.define_criterion()\n\n    # define trainer\n    train_main.define_trainer()\n\n    # model initialization\n    train_main.initialize_model()\n\n    # run training loop\n    train_main.run()", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "codecTest.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\nimport os\nimport torch\nimport logging\nimport argparse\nimport soundfile as sf\n\nfrom dataloader import SingleDataset", "\nfrom dataloader import SingleDataset\nfrom models.autoencoder.AudioDec import Generator as generator_audiodec\nfrom models.vocoder.HiFiGAN import Generator as generator_hifigan\nfrom bin.test import TestGEN\n\nclass TestMain(TestGEN):\n    def __init__(self, args,):\n        super(TestMain, self).__init__(args=args,)\n        self.encoder_type = self.encoder_config.get('model_type', 'symAudioDec')\n        self.decoder_type = self.decoder_config.get('model_type', 'symAudioDec')\n        if self.encoder_config['generator_params']['input_channels'] > 1:\n            self.multi_channel = True\n        else:\n            self.multi_channel = False\n    \n\n    # LOAD DATASET\n    def load_dataset(self, subset, subset_num):\n        data_path = os.path.join(\n            self.encoder_config['data']['path'], \n            self.encoder_config['data']['subset'][subset]\n        )\n        assert os.path.exists(data_path), f\"{data_path} does not exist!\"\n        self.dataset = SingleDataset(\n            files=data_path,\n            query=\"*.wav\",\n            load_fn=sf.read,\n            return_utt_id=True,\n            subset_num=subset_num,\n        )\n        logging.info(f\"The number of utterances = {len(self.dataset)}.\")\n    \n    \n    # LOAD MODEL\n    def load_encoder(self):\n        if self.encoder_type in ['symAudioDec', 'symAudioDecUniv']:\n            encoder = generator_audiodec\n        else:     \n            raise NotImplementedError(f\"Encoder {self.encoder_type} is not supported!\")\n        self.encoder = encoder(**self.encoder_config['generator_params'])\n        self.encoder.load_state_dict(\n            torch.load(self.encoder_checkpoint, map_location='cpu')['model']['generator'])\n        self.encoder = self.encoder.eval().to(self.device)\n        logging.info(f\"Loaded Encoder from {self.encoder_checkpoint}.\")\n    \n    \n    def load_decoder(self):\n        if self.decoder_type in ['symAudioDec', 'symAudioDecUniv']:\n            decoder = generator_audiodec\n        elif self.decoder_type in ['HiFiGAN', 'UnivNet']:\n            decoder = generator_hifigan\n        else:     \n            raise NotImplementedError(f\"Decoder {self.decoder_type} is not supported!\")\n        self.decoder = decoder(**self.decoder_config['generator_params'])\n        self.decoder.load_state_dict(\n            torch.load(self.decoder_checkpoint, map_location='cpu')['model']['generator'])\n        self.decoder = self.decoder.eval().to(self.device)\n        logging.info(f\"Loaded Decoder from {self.decoder_checkpoint}.\")\n    \n\n    def encode(self, audio):\n        x = torch.tensor(audio, dtype=torch.float).to(self.device)\n        if self.multi_channel:\n            x = x.transpose(1, 0).unsqueeze(0) # (T, C) -> (1, C, T)\n        else:\n            x = x.transpose(1, 0).unsqueeze(1) # (T, C) -> (C, 1, T)\n        x = self.encoder.encoder(x)\n        z = self.encoder.projector(x)\n        zq, _, _ = self.encoder.quantizer(z)\n        return zq\n        \n    \n    def decode(self, zq):\n        if self.decoder_type in ['HiFiGAN', 'UnivNet']:\n            y = self.decoder(zq)\n        else:\n            y = self.decoder.decoder(zq)\n        return y\n    \n    \n    # INITIAL FOLDER\n    def initial_folder(self, subset, output_name):\n        # model name\n        encoder = os.path.dirname(self.encoder_checkpoint).split('/')[-1]\n        decoder = os.path.dirname(self.decoder_checkpoint).split('/')[-1]\n        # model checkpoint\n        encoder_checkpoint = os.path.basename(self.encoder_checkpoint).split('steps')[0].split('-')[-1]\n        decoder_checkpoint = os.path.basename(self.decoder_checkpoint).split('steps')[0].split('-')[-1]\n        testdir = f\"{encoder}-{decoder}_{encoder_checkpoint}-{decoder_checkpoint}\"\n        # testing set\n        setdir = self.encoder_config['data']['subset'][subset]\n        self.outdir = os.path.join(output_name, testdir, setdir)\n        if not os.path.exists(self.outdir):\n            os.makedirs(self.outdir, exist_ok=True)    ", "    \n\ndef main():\n    \"\"\"Run testing process.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--subset\", type=str, default=\"clean_test\")\n    parser.add_argument(\"--subset_num\", type=int, default=-1)\n    parser.add_argument(\"--encoder\", type=str, required=True)\n    parser.add_argument(\"--decoder\", type=str, required=True)\n    parser.add_argument(\"--output_dir\", type=str, required=True)\n    args = parser.parse_args()\n\n    # initial test_main\n    test_main = TestMain(args=args)\n\n    # load dataset\n    test_main.load_dataset(args.subset, args.subset_num)\n\n    # load model\n    test_main.load_encoder()\n    test_main.load_decoder()\n\n    # initial folder\n    test_main.initial_folder(args.subset, args.output_dir)\n    \n    # run testing\n    test_main.run()", "    \n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "losses/waveform_loss.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Waveform-based loss modules.\"\"\"", "\n\"\"\"Waveform-based loss modules.\"\"\"\n\nimport torch\n\n\nclass WaveformShapeLoss(torch.nn.Module):\n    \"\"\"Waveform shape loss.\"\"\"\n\n    def __init__(self, winlen):\n        super().__init__()\n        self.loss = torch.nn.L1Loss()\n        self.winlen = winlen\n        self.maxpool = torch.nn.MaxPool1d(self.winlen)\n\n    def forward(self, y_hat, y):\n        \"\"\"Calculate L1 loss.\n\n        Args:\n            y_hat (Tensor): Generated single tensor (B, 1, T).\n            y (Tensor): Groundtruth single tensor (B, 1, T).\n\n        Returns:\n            Tensor: L1 loss value.\n\n        \"\"\"\n        ys = self.maxpool(torch.abs(y))\n        ys_hat = self.maxpool(torch.abs(y_hat))\n        loss = self.loss(ys_hat, ys)\n        return loss", "\n\nclass MultiWindowShapeLoss(torch.nn.Module):\n    \"\"\"Multi-window-lengthe waveform shape loss.\"\"\"\n\n    def __init__(\n        self,\n        winlen=[300, 200, 100],\n    ):\n        \"\"\"Initialize Multi window shape loss module.\n\n        Args:\n            winlen (list): List of window lengths.\n\n        \"\"\"\n        super(MultiWindowShapeLoss, self).__init__()\n        self.shape_losses = torch.nn.ModuleList()\n        for wl in winlen:\n            self.shape_losses += [WaveformShapeLoss(wl)]\n\n    def forward(self, y_hat, y):\n        \"\"\"Calculate L1 loss.\n\n        Args:\n            y_hat (Tensor): Generated single tensor (B, 1, T).\n            y (Tensor): Groundtruth single tensor (B, 1, T).\n\n        Returns:\n            Tensor: L2 loss value.\n\n        \"\"\"\n        loss = 0.0\n        for f in self.shape_losses:\n            loss += f(y_hat, y)\n        loss /= len(self.shape_losses)\n\n        return loss"]}
{"filename": "losses/__init__.py", "chunked_list": ["from .adversarial_loss import *  # NOQA\nfrom .feat_match_loss import *  # NOQA\nfrom .mel_loss import *  # NOQA\nfrom .stft_loss import *  # NOQA\nfrom .waveform_loss import *  # NOQA"]}
{"filename": "losses/adversarial_loss.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2021 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n\"\"\"Adversarial loss modules.\"\"\"\n\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\n\n\nclass GeneratorAdversarialLoss(torch.nn.Module):\n    \"\"\"Generator adversarial loss module.\"\"\"\n\n    def __init__(\n        self,\n        average_by_discriminators=True,\n        loss_type=\"mse\",\n    ):\n        \"\"\"Initialize GeneratorAversarialLoss module.\"\"\"\n        super().__init__()\n        self.average_by_discriminators = average_by_discriminators\n        assert loss_type in [\"mse\", \"hinge\"], f\"{loss_type} is not supported.\"\n        if loss_type == \"mse\":\n            self.criterion = self._mse_loss\n        else:\n            self.criterion = self._hinge_loss\n\n    def forward(self, outputs):\n        \"\"\"Calcualate generator adversarial loss.\n\n        Args:\n            outputs (Tensor or list): Discriminator outputs or list of\n                discriminator outputs.\n\n        Returns:\n            Tensor: Generator adversarial loss value.\n\n        \"\"\"\n        if isinstance(outputs, (tuple, list)):\n            adv_loss = 0.0\n            for i, outputs_ in enumerate(outputs):\n                if isinstance(outputs_, (tuple, list)):\n                    # NOTE(kan-bayashi): case including feature maps\n                    outputs_ = outputs_[-1]\n                adv_loss += self.criterion(outputs_)\n            if self.average_by_discriminators:\n                adv_loss /= i + 1\n        else:\n            adv_loss = self.criterion(outputs)\n\n        return adv_loss\n\n    def _mse_loss(self, x):\n        return F.mse_loss(x, x.new_ones(x.size()))\n\n    def _hinge_loss(self, x):\n        return -x.mean()", "\n\nclass DiscriminatorAdversarialLoss(torch.nn.Module):\n    \"\"\"Discriminator adversarial loss module.\"\"\"\n\n    def __init__(\n        self,\n        average_by_discriminators=True,\n        loss_type=\"mse\",\n    ):\n        \"\"\"Initialize DiscriminatorAversarialLoss module.\"\"\"\n        super().__init__()\n        self.average_by_discriminators = average_by_discriminators\n        assert loss_type in [\"mse\", \"hinge\"], f\"{loss_type} is not supported.\"\n        if loss_type == \"mse\":\n            self.fake_criterion = self._mse_fake_loss\n            self.real_criterion = self._mse_real_loss\n        else:\n            self.fake_criterion = self._hinge_fake_loss\n            self.real_criterion = self._hinge_real_loss\n\n    def forward(self, outputs_hat, outputs):\n        \"\"\"Calcualate discriminator adversarial loss.\n\n        Args:\n            outputs_hat (Tensor or list): Discriminator outputs or list of\n                discriminator outputs calculated from generator outputs.\n            outputs (Tensor or list): Discriminator outputs or list of\n                discriminator outputs calculated from groundtruth.\n\n        Returns:\n            Tensor: Discriminator real loss value.\n            Tensor: Discriminator fake loss value.\n\n        \"\"\"\n        if isinstance(outputs, (tuple, list)):\n            real_loss = 0.0\n            fake_loss = 0.0\n            for i, (outputs_hat_, outputs_) in enumerate(zip(outputs_hat, outputs)):\n                if isinstance(outputs_hat_, (tuple, list)):\n                    # NOTE(kan-bayashi): case including feature maps\n                    outputs_hat_ = outputs_hat_[-1]\n                    outputs_ = outputs_[-1]\n                real_loss += self.real_criterion(outputs_)\n                fake_loss += self.fake_criterion(outputs_hat_)\n            if self.average_by_discriminators:\n                fake_loss /= i + 1\n                real_loss /= i + 1\n        else:\n            real_loss = self.real_criterion(outputs)\n            fake_loss = self.fake_criterion(outputs_hat)\n\n        return real_loss, fake_loss\n\n    def _mse_real_loss(self, x):\n        return F.mse_loss(x, x.new_ones(x.size()))\n\n    def _mse_fake_loss(self, x):\n        return F.mse_loss(x, x.new_zeros(x.size()))\n\n    def _hinge_real_loss(self, x):\n        return -torch.mean(torch.min(x - 1, x.new_zeros(x.size())))\n\n    def _hinge_fake_loss(self, x):\n        return -torch.mean(torch.min(-x - 1, x.new_zeros(x.size())))", ""]}
{"filename": "losses/stft_loss.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"STFT-based loss modules.\"\"\"\n\nimport torch\nimport torch.nn.functional as F\n\n\n\ndef stft(x, fft_size, hop_size, win_length, window, eps=1e-7):\n    \"\"\"Perform STFT and convert to magnitude spectrogram.\n\n    Args:\n        x (Tensor): Input signal tensor (B, T).\n        fft_size (int): FFT size.\n        hop_size (int): Hop size.\n        win_length (int): Window length.\n        window (str): Window function type.\n\n    Returns:\n        Tensor: Magnitude spectrogram (B, #frames, fft_size // 2 + 1).\n\n    \"\"\"\n    x_stft = torch.stft(x, fft_size, hop_size, win_length, window, return_complex=True)\n    x_power = x_stft.real ** 2 + x_stft.imag ** 2\n    return torch.sqrt(torch.clamp(x_power, min=eps)).transpose(2, 1)", "\n\ndef stft(x, fft_size, hop_size, win_length, window, eps=1e-7):\n    \"\"\"Perform STFT and convert to magnitude spectrogram.\n\n    Args:\n        x (Tensor): Input signal tensor (B, T).\n        fft_size (int): FFT size.\n        hop_size (int): Hop size.\n        win_length (int): Window length.\n        window (str): Window function type.\n\n    Returns:\n        Tensor: Magnitude spectrogram (B, #frames, fft_size // 2 + 1).\n\n    \"\"\"\n    x_stft = torch.stft(x, fft_size, hop_size, win_length, window, return_complex=True)\n    x_power = x_stft.real ** 2 + x_stft.imag ** 2\n    return torch.sqrt(torch.clamp(x_power, min=eps)).transpose(2, 1)", "\n\nclass SpectralConvergenceLoss(torch.nn.Module):\n    \"\"\"Spectral convergence loss module.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initilize spectral convergence loss module.\"\"\"\n        super(SpectralConvergenceLoss, self).__init__()\n\n    def forward(self, x_mag, y_mag):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\n        Returns:\n            Tensor: Spectral convergence loss value.\n\n        \"\"\"\n        return torch.norm(y_mag - x_mag, p=\"fro\") / torch.norm(y_mag, p=\"fro\")", "\n\nclass LogSTFTMagnitudeLoss(torch.nn.Module):\n    \"\"\"Log STFT magnitude loss module.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initilize los STFT magnitude loss module.\"\"\"\n        super(LogSTFTMagnitudeLoss, self).__init__()\n\n    def forward(self, x_mag, y_mag):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x_mag (Tensor): Magnitude spectrogram of predicted signal (B, #frames, #freq_bins).\n            y_mag (Tensor): Magnitude spectrogram of groundtruth signal (B, #frames, #freq_bins).\n\n        Returns:\n            Tensor: Log STFT magnitude loss value.\n\n        \"\"\"\n        return F.l1_loss(torch.log(y_mag), torch.log(x_mag))", "\n\nclass STFTLoss(torch.nn.Module):\n    \"\"\"STFT loss module.\"\"\"\n\n    def __init__(\n        self, \n        fft_size=1024, \n        hop_size=120, \n        win_length=600, \n        window=\"hann_window\",\n    ):\n        \"\"\"Initialize STFT loss module.\"\"\"\n        super(STFTLoss, self).__init__()\n        self.fft_size = fft_size\n        self.hop_size = hop_size\n        self.win_length = win_length\n        self.spectral_convergence_loss = SpectralConvergenceLoss()\n        self.log_stft_magnitude_loss = LogSTFTMagnitudeLoss()\n        self.register_buffer(\"window\", getattr(torch, window)(win_length))\n\n\n    def forward(self, x, y):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Predicted signal (B, T).\n            y (Tensor): Groundtruth signal (B, T).\n\n        Returns:\n            Tensor: Spectral convergence loss value.\n            Tensor: Log STFT magnitude loss value.\n\n        \"\"\"\n        x_mag = stft(x, self.fft_size, self.hop_size, self.win_length, self.window)\n        y_mag = stft(y, self.fft_size, self.hop_size, self.win_length, self.window)\n        sc_loss = self.spectral_convergence_loss(x_mag, y_mag)\n        mag_loss = self.log_stft_magnitude_loss(x_mag, y_mag)\n\n        return sc_loss, mag_loss", "\n\nclass MultiResolutionSTFTLoss(torch.nn.Module):\n    \"\"\"Multi resolution STFT loss module.\"\"\"\n\n    def __init__(\n        self,\n        fft_sizes=[1024, 2048, 512],\n        hop_sizes=[120, 240, 50],\n        win_lengths=[600, 1200, 240],\n        window=\"hann_window\",\n    ):\n        \"\"\"Initialize Multi resolution STFT loss module.\n\n        Args:\n            fft_sizes (list): List of FFT sizes.\n            hop_sizes (list): List of hop sizes.\n            win_lengths (list): List of window lengths.\n            window (str): Window function type.\n\n        \"\"\"\n        super(MultiResolutionSTFTLoss, self).__init__()\n        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)\n        self.stft_losses = torch.nn.ModuleList()\n        for fft_size, hop_size, win_length in zip(fft_sizes, hop_sizes, win_lengths):\n            self.stft_losses += [STFTLoss(fft_size, hop_size, win_length, window)]\n\n\n    def forward(self, x, y):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Predicted signal (B, T) or (B, #subband, T).\n            y (Tensor): Groundtruth signal (B, T) or (B, #subband, T).\n\n        Returns:\n            Tensor: Multi resolution spectral convergence loss value.\n            Tensor: Multi resolution log STFT magnitude loss value.\n\n        \"\"\"\n        if len(x.shape) == 3:\n            x = x.view(-1, x.size(2))  # (B, C, T) -> (B x C, T)\n            y = y.view(-1, y.size(2))  # (B, C, T) -> (B x C, T)\n        sc_loss = 0.0\n        mag_loss = 0.0\n        for f in self.stft_losses:\n            sc_l, mag_l = f(x, y)\n            sc_loss += sc_l\n            mag_loss += mag_l\n        sc_loss /= len(self.stft_losses)\n        mag_loss /= len(self.stft_losses)\n\n        return sc_loss, mag_loss", ""]}
{"filename": "losses/mel_loss.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Mel-spectrogram loss modules.\"\"\"\n\nimport librosa\nimport torch\nimport torch.nn.functional as F\n\n\nclass MelSpectrogram(torch.nn.Module):\n    \"\"\"Calculate Mel-spectrogram.\"\"\"\n\n    def __init__(\n        self,\n        fs=22050,\n        fft_size=1024,\n        hop_size=256,\n        win_length=None,\n        window=\"hann_window\",\n        num_mels=80,\n        fmin=80,\n        fmax=7600,\n        center=True,\n        normalized=False,\n        onesided=True,\n        eps=1e-10,\n        log_base=10.0,\n    ):\n        \"\"\"Initialize MelSpectrogram module.\"\"\"\n        super().__init__()\n        self.fft_size = fft_size\n        self.hop_size = hop_size\n        if win_length is not None:\n            self.win_length = win_length\n        else:\n            self.win_length = fft_size\n        self.center = center\n        self.normalized = normalized\n        self.onesided = onesided\n        self.register_buffer(\"window\", getattr(torch, window)(self.win_length))\n        self.eps = eps\n\n        fmin = 0 if fmin is None else fmin\n        fmax = fs / 2 if fmax is None else fmax\n        melmat = librosa.filters.mel(\n            sr=fs,\n            n_fft=fft_size,\n            n_mels=num_mels,\n            fmin=fmin,\n            fmax=fmax,\n        )\n        self.register_buffer(\"melmat\", torch.from_numpy(melmat.T).float())\n\n        self.log_base = log_base\n        if self.log_base is None:\n            self.log = torch.log\n        elif self.log_base == 2.0:\n            self.log = torch.log2\n        elif self.log_base == 10.0:\n            self.log = torch.log10\n        else:\n            raise ValueError(f\"log_base: {log_base} is not supported.\")\n\n\n    def forward(self, x):\n        \"\"\"Calculate Mel-spectrogram.\n\n        Args:\n            x (Tensor): Input waveform tensor (B, T) or (B, C, T).\n\n        Returns:\n            Tensor: Mel-spectrogram (B, #mels, #frames).\n\n        \"\"\"\n        if x.dim() == 3:\n            # (B, C, T) -> (B*C, T)\n            x = x.reshape(-1, x.size(2))\n\n        x_stft = torch.stft(x, self.fft_size, self.hop_size, self.win_length, self.window, return_complex=True)\n        x_power = x_stft.real ** 2 + x_stft.imag ** 2\n        x_amp = torch.sqrt(torch.clamp(x_power, min=self.eps)).transpose(2, 1) # (B, D, T') -> (B, T', D)\n        x_mel = torch.matmul(x_amp, self.melmat)\n        x_mel = torch.clamp(x_mel, min=self.eps)\n\n        return self.log(x_mel).transpose(1, 2) # (B, D, T')", "\n\nclass MelSpectrogram(torch.nn.Module):\n    \"\"\"Calculate Mel-spectrogram.\"\"\"\n\n    def __init__(\n        self,\n        fs=22050,\n        fft_size=1024,\n        hop_size=256,\n        win_length=None,\n        window=\"hann_window\",\n        num_mels=80,\n        fmin=80,\n        fmax=7600,\n        center=True,\n        normalized=False,\n        onesided=True,\n        eps=1e-10,\n        log_base=10.0,\n    ):\n        \"\"\"Initialize MelSpectrogram module.\"\"\"\n        super().__init__()\n        self.fft_size = fft_size\n        self.hop_size = hop_size\n        if win_length is not None:\n            self.win_length = win_length\n        else:\n            self.win_length = fft_size\n        self.center = center\n        self.normalized = normalized\n        self.onesided = onesided\n        self.register_buffer(\"window\", getattr(torch, window)(self.win_length))\n        self.eps = eps\n\n        fmin = 0 if fmin is None else fmin\n        fmax = fs / 2 if fmax is None else fmax\n        melmat = librosa.filters.mel(\n            sr=fs,\n            n_fft=fft_size,\n            n_mels=num_mels,\n            fmin=fmin,\n            fmax=fmax,\n        )\n        self.register_buffer(\"melmat\", torch.from_numpy(melmat.T).float())\n\n        self.log_base = log_base\n        if self.log_base is None:\n            self.log = torch.log\n        elif self.log_base == 2.0:\n            self.log = torch.log2\n        elif self.log_base == 10.0:\n            self.log = torch.log10\n        else:\n            raise ValueError(f\"log_base: {log_base} is not supported.\")\n\n\n    def forward(self, x):\n        \"\"\"Calculate Mel-spectrogram.\n\n        Args:\n            x (Tensor): Input waveform tensor (B, T) or (B, C, T).\n\n        Returns:\n            Tensor: Mel-spectrogram (B, #mels, #frames).\n\n        \"\"\"\n        if x.dim() == 3:\n            # (B, C, T) -> (B*C, T)\n            x = x.reshape(-1, x.size(2))\n\n        x_stft = torch.stft(x, self.fft_size, self.hop_size, self.win_length, self.window, return_complex=True)\n        x_power = x_stft.real ** 2 + x_stft.imag ** 2\n        x_amp = torch.sqrt(torch.clamp(x_power, min=self.eps)).transpose(2, 1) # (B, D, T') -> (B, T', D)\n        x_mel = torch.matmul(x_amp, self.melmat)\n        x_mel = torch.clamp(x_mel, min=self.eps)\n\n        return self.log(x_mel).transpose(1, 2) # (B, D, T')", "\n\nclass MultiMelSpectrogramLoss(torch.nn.Module):\n    \"\"\"Multi resolution Mel-spectrogram loss.\"\"\"\n\n    def __init__(\n        self,\n        fs=22050,\n        fft_sizes=[1024, 2048, 512],\n        hop_sizes=[120, 240, 50],\n        win_lengths=[600, 1200, 240],\n        window=\"hann_window\",\n        num_mels=80,\n        fmin=80,\n        fmax=7600,\n        center=True,\n        normalized=False,\n        onesided=True,\n        eps=1e-10,\n        log_base=10.0,\n    ):\n        \"\"\"Initialize Mel-spectrogram loss.\"\"\"\n        super().__init__()\n        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)\n        self.mel_transfers = torch.nn.ModuleList()\n        for fft_size, hop_size, win_length in zip(fft_sizes, hop_sizes, win_lengths):\n            self.mel_transfers += [\n                MelSpectrogram(\n                    fs=fs,\n                    fft_size=fft_size,\n                    hop_size=hop_size,\n                    win_length=win_length,\n                    window=window,\n                    num_mels=num_mels,\n                    fmin=fmin,\n                    fmax=fmax,\n                    center=center,\n                    normalized=normalized,\n                    onesided=onesided,\n                    eps=eps,\n                    log_base=log_base,\n                )\n            ]\n\n\n    def forward(self, y_hat, y):\n        \"\"\"Calculate Mel-spectrogram loss.\n\n        Args:\n            y_hat (Tensor): Generated single tensor (B, C, T).\n            y (Tensor): Groundtruth single tensor (B, C, T).\n\n        Returns:\n            Tensor: Mel-spectrogram loss value.\n\n        \"\"\"\n        mel_loss = 0.0\n        for f in self.mel_transfers:\n            mel_loss += F.l1_loss(f(y_hat), f(y))\n        mel_loss /= len(self.mel_transfers)\n\n        return mel_loss"]}
{"filename": "losses/feat_match_loss.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2021 Tomoki Hayashi\n#  MIT License (https://opensource.org/licenses/MIT)\n\n\"\"\"Feature matching loss modules.\"\"\"\n\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\n\n\nclass FeatureMatchLoss(torch.nn.Module):\n    \"\"\"Feature matching loss module.\"\"\"\n\n    def __init__(\n        self,\n        average_by_layers=True,\n        average_by_discriminators=True,\n        include_final_outputs=False,\n    ):\n        \"\"\"Initialize FeatureMatchLoss module.\"\"\"\n        super().__init__()\n        self.average_by_layers = average_by_layers\n        self.average_by_discriminators = average_by_discriminators\n        self.include_final_outputs = include_final_outputs\n\n    def forward(self, feats_hat, feats):\n        \"\"\"Calcualate feature matching loss.\n\n        Args:\n            feats_hat (list): List of list of discriminator outputs\n                calcuated from generater outputs.\n            feats (list): List of list of discriminator outputs\n                calcuated from groundtruth.\n\n        Returns:\n            Tensor: Feature matching loss value.\n\n        \"\"\"\n        feat_match_loss = 0.0\n        for i, (feats_hat_, feats_) in enumerate(zip(feats_hat, feats)):\n            feat_match_loss_ = 0.0\n            if not self.include_final_outputs:\n                feats_hat_ = feats_hat_[:-1]\n                feats_ = feats_[:-1]\n            for j, (feat_hat_, feat_) in enumerate(zip(feats_hat_, feats_)):\n                feat_match_loss_ += F.l1_loss(feat_hat_, feat_.detach())\n            if self.average_by_layers:\n                feat_match_loss_ /= j + 1\n            feat_match_loss += feat_match_loss_\n        if self.average_by_discriminators:\n            feat_match_loss /= i + 1\n\n        return feat_match_loss", ""]}
{"filename": "bin/train.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Training stage template.\"\"\"\n\nimport os\nimport abc\nimport sys\nimport yaml\nimport random", "import yaml\nimport random\nimport logging\nimport torch\nimport numpy as np\n\nfrom bin.utils import load_config\n\n\nclass TrainGAN(abc.ABC):\n    def __init__(\n        self,\n        args,\n    ):\n        # set logger\n        logging.basicConfig(\n            level=logging.INFO,\n            stream=sys.stdout,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n\n        # Fix seed and make backends deterministic\n        random.seed(args.seed)\n        np.random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        if not torch.cuda.is_available():\n            self.device = torch.device('cpu')\n            logging.info(f\"device: cpu\")\n        else:\n            self.device = torch.device('cuda')\n            logging.info(f\"device: gpu\")\n            torch.cuda.manual_seed_all(args.seed)\n            if args.disable_cudnn == \"False\":\n                torch.backends.cudnn.benchmark = True\n        \n        # initialize config\n        with open(args.config, 'r') as f:\n            self.config = yaml.load(f, Loader=yaml.FullLoader)\n        self.config.update(vars(args))\n\n        # initialize model folder\n        expdir = os.path.join(args.exp_root, args.tag)\n        os.makedirs(expdir, exist_ok=True)\n        self.config[\"outdir\"] = expdir\n\n        # save config\n        with open(os.path.join(expdir, \"config.yml\"), \"w\") as f:\n            yaml.dump(self.config, f, Dumper=yaml.Dumper)\n        for key, value in self.config.items():\n            logging.info(f\"[TrainGAN] {key} = {value}\")\n        \n        # initialize attribute\n        self.resume = args.resume\n        self.data_loader = None\n        self.model = {}\n        self.criterion = None\n        self.optimizer = None\n        self.scheduler = None\n        self.trainer = None\n\n        # initialize batch_length\n        self.batch_length = self.config['batch_length']\n\n\n    @abc.abstractmethod    \n    def initialize_data_loader(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def define_model(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def define_trainer(self):\n        pass\n\n\n    @abc.abstractmethod\n    def initialize_model(self):\n        pass\n    \n\n    @abc.abstractmethod\n    def define_criterion(self):\n        pass\n\n\n    def run(self):\n        try:\n            logging.info(f\"The current training step: {self.trainer.steps}\")\n            self.trainer.train_max_steps = self.config[\"train_max_steps\"]\n            if not self.trainer._check_train_finish():\n                self.trainer.run()\n            if self.config.get(\"adv_train_max_steps\", False) and self.config.get(\"adv_batch_length\", False):\n                self.batch_length = self.config['adv_batch_length']\n                logging.info(f\"Reload dataloader for adversarial training.\")                \n                self.initialize_data_loader()\n                self.trainer.data_loader = self.data_loader\n                self.trainer.train_max_steps = self.config[\"adv_train_max_steps\"]\n                self.trainer.run()\n        finally:\n            self.trainer.save_checkpoint(\n                os.path.join(self.config[\"outdir\"], f\"checkpoint-{self.trainer.steps}steps.pkl\")\n            )\n            logging.info(f\"Successfully saved checkpoint @ {self.trainer.steps}steps.\")\n    \n    \n    def _show_setting(self):\n        logging.info(self.model['generator'])\n        logging.info(self.model['discriminator'])\n        logging.info(self.optimizer['generator'])\n        logging.info(self.optimizer['discriminator'])\n        logging.info(self.scheduler['generator'])\n        logging.info(self.scheduler['discriminator'])\n        for criterion_ in self.criterion.values():\n            logging.info(criterion_)\n    \n\n    def _load_config(self, checkpoint, config_name='config.yml'):\n        return load_config(checkpoint, config_name)", "\nclass TrainGAN(abc.ABC):\n    def __init__(\n        self,\n        args,\n    ):\n        # set logger\n        logging.basicConfig(\n            level=logging.INFO,\n            stream=sys.stdout,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n\n        # Fix seed and make backends deterministic\n        random.seed(args.seed)\n        np.random.seed(args.seed)\n        torch.manual_seed(args.seed)\n        if not torch.cuda.is_available():\n            self.device = torch.device('cpu')\n            logging.info(f\"device: cpu\")\n        else:\n            self.device = torch.device('cuda')\n            logging.info(f\"device: gpu\")\n            torch.cuda.manual_seed_all(args.seed)\n            if args.disable_cudnn == \"False\":\n                torch.backends.cudnn.benchmark = True\n        \n        # initialize config\n        with open(args.config, 'r') as f:\n            self.config = yaml.load(f, Loader=yaml.FullLoader)\n        self.config.update(vars(args))\n\n        # initialize model folder\n        expdir = os.path.join(args.exp_root, args.tag)\n        os.makedirs(expdir, exist_ok=True)\n        self.config[\"outdir\"] = expdir\n\n        # save config\n        with open(os.path.join(expdir, \"config.yml\"), \"w\") as f:\n            yaml.dump(self.config, f, Dumper=yaml.Dumper)\n        for key, value in self.config.items():\n            logging.info(f\"[TrainGAN] {key} = {value}\")\n        \n        # initialize attribute\n        self.resume = args.resume\n        self.data_loader = None\n        self.model = {}\n        self.criterion = None\n        self.optimizer = None\n        self.scheduler = None\n        self.trainer = None\n\n        # initialize batch_length\n        self.batch_length = self.config['batch_length']\n\n\n    @abc.abstractmethod    \n    def initialize_data_loader(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def define_model(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def define_trainer(self):\n        pass\n\n\n    @abc.abstractmethod\n    def initialize_model(self):\n        pass\n    \n\n    @abc.abstractmethod\n    def define_criterion(self):\n        pass\n\n\n    def run(self):\n        try:\n            logging.info(f\"The current training step: {self.trainer.steps}\")\n            self.trainer.train_max_steps = self.config[\"train_max_steps\"]\n            if not self.trainer._check_train_finish():\n                self.trainer.run()\n            if self.config.get(\"adv_train_max_steps\", False) and self.config.get(\"adv_batch_length\", False):\n                self.batch_length = self.config['adv_batch_length']\n                logging.info(f\"Reload dataloader for adversarial training.\")                \n                self.initialize_data_loader()\n                self.trainer.data_loader = self.data_loader\n                self.trainer.train_max_steps = self.config[\"adv_train_max_steps\"]\n                self.trainer.run()\n        finally:\n            self.trainer.save_checkpoint(\n                os.path.join(self.config[\"outdir\"], f\"checkpoint-{self.trainer.steps}steps.pkl\")\n            )\n            logging.info(f\"Successfully saved checkpoint @ {self.trainer.steps}steps.\")\n    \n    \n    def _show_setting(self):\n        logging.info(self.model['generator'])\n        logging.info(self.model['discriminator'])\n        logging.info(self.optimizer['generator'])\n        logging.info(self.optimizer['discriminator'])\n        logging.info(self.scheduler['generator'])\n        logging.info(self.scheduler['discriminator'])\n        for criterion_ in self.criterion.values():\n            logging.info(criterion_)\n    \n\n    def _load_config(self, checkpoint, config_name='config.yml'):\n        return load_config(checkpoint, config_name)", "        \n"]}
{"filename": "bin/utils.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n", "\n\n\"\"\"Utility modules.\"\"\"\n\nimport os\nimport yaml\n\n\ndef load_config(checkpoint, config_name='config.yml'):\n    dirname = os.path.dirname(checkpoint)\n    config_path = os.path.join(dirname, config_name)\n    with open(config_path) as f:\n        config = yaml.load(f, Loader=yaml.Loader)\n    return config", "def load_config(checkpoint, config_name='config.yml'):\n    dirname = os.path.dirname(checkpoint)\n    config_path = os.path.join(dirname, config_name)\n    with open(config_path) as f:\n        config = yaml.load(f, Loader=yaml.Loader)\n    return config\n    "]}
{"filename": "bin/stream.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os", "\nimport os\nimport abc\nimport yaml\nimport time\nimport queue\nimport threading\nimport torch\nimport torchaudio\nimport numpy as np", "import torchaudio\nimport numpy as np\n\nfrom typing import Union\n\n\nclass AudioCodec(abc.ABC):\n    def __init__(\n        self,\n        tx_device: str = \"cpu\",\n        rx_device: str = \"cpu\",\n        receptive_length: int = 8192,\n    ):\n        self.tx_device = tx_device\n        self.rx_device = rx_device\n        self.receptive_length = receptive_length\n        self.tx_encoder = None\n        self.rx_encoder = None\n        self.decoder = None\n\n\n    @abc.abstractmethod\n    def _load_encoder(self, checkpoint):\n        pass\n\n\n    @abc.abstractmethod\n    def _load_decoder(self, checkpoint):\n        pass\n\n\n    def _load_config(self, checkpoint, config_name='config.yml'):\n        dirname = os.path.dirname(checkpoint)\n        config_path = os.path.join(dirname, config_name)\n        with open(config_path) as f:\n            config = yaml.load(f, Loader=yaml.Loader)\n        return config\n\n\n    def load_transmitter(self, encoder_checkpoint):\n        # load transmitter model(s)\n        assert os.path.exists(encoder_checkpoint), f'{encoder_checkpoint} does not exist!'\n        self.tx_encoder = self._load_encoder(encoder_checkpoint)\n        self.tx_encoder.eval().to(self.tx_device)\n        self.tx_encoder.initial_encoder(self.receptive_length, self.tx_device)\n        print(\"Load tx_encoder: %s\" % (encoder_checkpoint))\n\n\n    def load_receiver(self, encoder_checkpoint, decoder_checkpoint):\n        # load receiver model(s)\n        assert os.path.exists(encoder_checkpoint), f'{encoder_checkpoint} does not exist!'\n        self.rx_encoder = self._load_encoder(encoder_checkpoint)\n        self.rx_encoder.eval().to(self.rx_device)\n        zq = self.rx_encoder.initial_encoder(self.receptive_length, self.rx_device)\n        print(\"Load rx_encoder: %s\" % (encoder_checkpoint))\n\n        assert os.path.exists(decoder_checkpoint), f'{decoder_checkpoint} does not exist!'\n        self.decoder = self._load_decoder(decoder_checkpoint)\n        self.decoder.eval().to(self.rx_device)\n        self.decoder.initial_decoder(zq)\n        print(\"Load decoder: %s\" % (decoder_checkpoint))", "\n\nclass AudioCodecStreamer(abc.ABC):\n    \"\"\"\n    Streams audio from an input microphone to headpones/speakers.\n    For each model that can be optionally provided (encoder, decoder), the input audio is processed by the forward call of these models.\n\n    Main functions (see function definition for detailed documentation):\n    * __init__\n    * enable_filedump\n    * set_tx_rx_poses\n    * run\n\n    Example usage:\n\n        streamer = AudioCodecStreamer(\n            input_device=1,\n            output_device=4,\n            frame_size=512,\n            encoder=my_encoder_network,\n            tx_device=\"cuda:0\",\n            decoder=my_decoder_network,\n            rx_device=\"cuda:1\",\n        )\n\n        streamer.enable_filedump(input_stream_file=\"input.wav\", output_stream_file=\"output.wav\")\n\n        streamer.run()\n    \"\"\"\n    def __init__(\n        self,\n        input_device: Union[str, int],\n        output_device: Union[str, int],\n        input_channels: int = 1,\n        output_channels: int = 1,\n        frame_size: int = 512,\n        sample_rate: int = 48000,\n        gain: int = 1.0,\n        max_latency: float = 0.1,\n        # Transmitter params\n        tx_encoder = None,\n        tx_device: str = \"cpu\",\n        # Receiver params\n        rx_encoder = None,\n        decoder = None,\n        rx_device: str = \"cpu\",\n    ):\n        \"\"\"\n        Sounddevice parameters\n\n        :param input_device:    int or str, name or index of the input device.\n                                To get a list of all input devices call python3 -m sounddevice.\n        :param output_device:   int or str, name or index of the output device.\n                                To get a list of all output devices call python3 -m sounddevice.\n        :param input_channels:  number of input channels, usually 1 but might be multiple microphones as well\n        :param output_channels: number of output channels, usually 2 for binaural audio\n        :param frame_size:      number of audio samples in a frame\n        :param sample_rate:     sample rate of the audio signal\n        :param gain:            linear factor to scale the input audio by\n        :param max_latency:     maximal accepted latency in seconds before frames get dropped\n\n        #######\n\n        Transmitter parameters\n\n        :param tx_encoder:      encoder network in the transimtter side\n                                Is an instance of torch.nn.Module and must be fully initialized and loaded.\n                                Must have a forward function that expects a batch x input_channels x frame_size tensor as input.\n                                Default: None (input tensor is forwarded to decoder without change)\n        :param tx_device:       device on transmitter (cpu, cuda:0, cuda:1, ...)\n\n        #######\n\n        Receiver parameters\n\n        :param rx_encoder:      encoder network in the receiver side\n                                Is an instance of torch.nn.Module and must be fully initialized and loaded.\n                                Must have a forward function that expects a batch x input_channels x frame_size tensor as input.\n                                Default: None (input tensor is forwarded to decoder without change)\n\n        :param decoder:         decoder network\n                                Is an instance of torch.nn.Module and must be fully initialized and loaded.\n                                Must have a forward function that expects a tensor of the shape produced by the encoder.\n                                Default: None (input tensor is forwarded to binauralizer without change)\n        :param rx_device:       device on receiver (cpu, cuda:0, cuda:1, ...)\n        \"\"\"\n        self.input_device = input_device\n        self.output_device = output_device\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.frame_size = frame_size\n        self.sample_rate = sample_rate\n        self.gain = gain\n        self.max_latency = max_latency\n\n        # encoder\n        self.tx_encoder = tx_encoder\n        self.tx_device = tx_device\n        print(f'Encoder device: {tx_device}')\n\n        # decoder\n        self.rx_encoder = rx_encoder\n        self.decoder = decoder\n        self.rx_device = rx_device\n        print(f'Decoder device: {rx_device}')\n\n        # queues for encoder, decoder, and output\n        self.encoder_queue = queue.Queue()\n        self.decoder_queue = queue.Queue()\n        self.output_queue = queue.Queue()\n\n        # file dump if requested\n        self.input_dump = []\n        self.output_dump = []\n        self.input_dump_filename = None\n        self.output_dump_filename = None\n\n        # streaming statistics\n        self.frame_drops = 0\n        self.n_frames = 0\n        self.encoder_times = []\n        self.decoder_times = []\n        self.latency_queue = queue.Queue()\n        self.latencies = []\n\n    @abc.abstractmethod\n    def _encode(self, x):\n        pass\n\n\n    @abc.abstractmethod\n    def _decode(self, x):\n        pass\n \n    def _run_encoder(self):\n        while threading.main_thread().is_alive():\n            try:\n                x = self.encoder_queue.get(timeout=1)\n            except:\n                continue\n            start = time.time()\n            x = x.to(self.tx_device)\n            with torch.no_grad():\n                if self.tx_encoder is not None:\n                    x = self._encode(x)\n            self.encoder_times.append(time.time() - start)\n            self.decoder_queue.put(x)\n\n\n    def _run_decoder(self):\n        while threading.main_thread().is_alive():\n            try:\n                x = self.decoder_queue.get(timeout=1)\n            except:\n                continue\n            start = time.time()\n            x = x.to(self.rx_device)\n            with torch.no_grad():\n                if (self.rx_encoder is not None) and (self.decoder is not None):\n                    x = self._decode(x)\n            self.decoder_times.append(time.time() - start)\n            self.output_queue.put(x)\n\n\n    def _process(self, data):\n        data = data * self.gain\n        input_data = torch.from_numpy(data).transpose(1, 0).contiguous()  # channels x frame_size\n\n        if self.input_dump_filename is not None:\n            self.input_dump.append(input_data)\n\n        # add batch dimension\n        input_data = input_data.unsqueeze(0)\n\n        # process data\n        self.encoder_queue.put(input_data)\n        self.latency_queue.put(time.time())\n        try:\n            output_data = self.output_queue.get_nowait()\n            latency = time.time() - self.latency_queue.get_nowait()\n            self.latencies.append(latency)\n            # clear queues if latency get too high; this will lead to frame drops\n            if latency > self.max_latency:\n                self.encoder_queue.queue.clear()\n                self.decoder_queue.queue.clear()\n                self.output_queue.queue.clear()\n                while not self.latency_queue.empty():\n                    self.frame_drops += 1\n                    self.latency_queue.get_nowait()\n        except queue.Empty:\n            output_data = torch.zeros(1, self.output_channels, self.frame_size)\n        output_data = output_data.squeeze(0).detach().cpu()\n\n        self.n_frames += 1\n\n        if self.output_dump_filename is not None:\n            self.output_dump.append(output_data)\n\n        data = output_data.transpose(1, 0).contiguous().numpy()\n\n        return data\n\n    def _callback(self, indata, outdata, frames, _time, status):\n        if status:\n            print(status)\n        outdata[:] = self._process(indata)\n\n    def _exit(self):\n        # dump data to file if required\n        if self.input_dump_filename is not None:\n            audio = torch.clamp(torch.cat(self.input_dump, dim=-1), min=-1, max=1)\n            torchaudio.save(self.input_dump_filename, audio, self.sample_rate)\n\n        if self.output_dump_filename is not None:\n            audio = torch.clamp(torch.cat(self.output_dump, dim=-1), min=-1, max=1)\n            torchaudio.save(self.output_dump_filename, audio, self.sample_rate)\n\n        # compute statistics\n        with threading.Lock():\n            encoder_mean = np.mean(np.array(self.encoder_times) * 1000.0)\n            encoder_std = np.std(np.array(self.encoder_times) * 1000.0)\n            decoder_mean = np.mean(np.array(self.decoder_times) * 1000.0)\n            decoder_std = np.std(np.array(self.decoder_times) * 1000.0)\n            latency_mean = np.mean(np.array(self.latencies) * 1000.0)\n            latency_std = np.std(np.array(self.latencies) * 1000.0)\n        frame_drops_ratio = self.frame_drops / self.n_frames\n\n        # print statistics\n        print('#' * 80)\n        print(f\"encoder processing time (ms):      {encoder_mean:.2f} +- {encoder_std:.2f}\")\n        print(f\"decoder processing time (ms):      {decoder_mean:.2f} +- {decoder_std:.2f}\")\n        print(f\"system latency (ms):               {latency_mean:.2f} +- {latency_std:.2f}\")\n        print(f\"frame drops:                       {self.frame_drops} ({frame_drops_ratio * 100:.2f}%)\")\n        print('#' * 80)\n\n\n    def enable_filedump(self, input_stream_file: str = None, output_stream_file: str = None):\n        \"\"\"\n        dumps input/output audio to file if input/output filenames are specified\n        call this function before run()\n        :param input_stream_file:   name of the file to dump input audio to\n        :param output_stream_file:  name of the file to dump output audio to\n        at least one of the files needs to be specified\n        \"\"\"\n        if input_stream_file is None and output_stream_file is None:\n            raise Exception(\"At least one of input_stream_file and output_stream_file must be specified.\")\n\n        if input_stream_file is not None:\n            if not input_stream_file[-4:] == \".wav\":\n                input_stream_file += \".wav\"\n            self.input_dump_filename = input_stream_file\n\n        if output_stream_file is not None:\n            if not output_stream_file[-4:] == \".wav\":\n                output_stream_file += \".wav\"\n            self.output_dump_filename = output_stream_file\n\n\n    def run(self, latency):\n        \"\"\"\n        start streaming from the input device and forward the processed audio to the output device\n        prints statistics about mean processing time, standard deviation of each processing pass, and percentage of buffer underflows\n        \"\"\"\n\n        # start encoder and decoder threads\n        encoder_thread = threading.Thread(target=self._run_encoder, daemon=True)\n        encoder_thread.start()\n        decoder_thread = threading.Thread(target=self._run_decoder, daemon=True)\n        decoder_thread.start()\n\n        try:\n            # import device\n            import sounddevice as sd\n            with sd.Stream(\n                device=(self.input_device, self.output_device),\n                samplerate=self.sample_rate,\n                blocksize=self.frame_size,\n                dtype=np.float32,\n                latency=latency,\n                channels=(self.input_channels, self.output_channels),\n                callback=self._callback\n            ):\n                print('### starting stream [press Return to quit] ###')\n                input()\n                self._exit()\n        except KeyboardInterrupt:\n            self._exit()\n        except Exception as e:\n            print(type(e).__name__ + ': ' + str(e))", ""]}
{"filename": "bin/test.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Testing stage template.\"\"\"\n\nimport os\nimport abc\nimport sys\nimport time\nimport yaml", "import time\nimport yaml\nimport torch\nimport logging\nimport soundfile as sf\n\nfrom tqdm import tqdm\nfrom bin.utils import load_config\n\nclass TestGEN(abc.ABC):\n    def __init__(\n        self,\n        args,\n    ):\n        # set logger\n        logging.basicConfig(\n            level=logging.INFO,\n            stream=sys.stdout,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n\n        # device\n        if not torch.cuda.is_available():\n            self.device = torch.device('cpu')\n            logging.info(f\"device: cpu\")\n        else:\n            self.device = torch.device('cuda')\n            logging.info(f\"device: gpu\")\n        \n        # initialize attribute\n        if hasattr(args, 'encoder'):\n            self.encoder_checkpoint = args.encoder\n            self.encoder_config = self._load_config(args.encoder)\n        if hasattr(args, 'decoder'):\n            self.decoder_checkpoint = args.decoder\n            self.decoder_config = self._load_config(args.decoder)\n        self.encoder = None\n        self.decoder = None\n        self.dataset = None\n        self.outdir = None\n    \n\n    @abc.abstractmethod\n    def initial_folder(self, output_name):\n        pass\n        \n\n    @abc.abstractmethod    \n    def load_dataset(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def load_encoder(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def load_decoder(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def encode(self, x):\n        pass\n    \n    \n    @abc.abstractmethod\n    def decode(self, z):\n        pass\n\n    \n    def run(self):\n        total_rtf = 0.0\n        with torch.no_grad(), tqdm(self.dataset, desc=\"[test]\") as pbar:\n            for idx, (utt_id, x) in enumerate(pbar, 1):\n                start = time.time()\n                zq = self.encode(x)\n                y = self.decode(zq)\n                y = y.squeeze(1).transpose(1, 0).cpu().numpy() # T x C\n                rtf = (time.time() - start) / (len(y) / self.decoder_config['sampling_rate'])\n                pbar.set_postfix({\"RTF\": rtf})\n                total_rtf += rtf\n\n                # output wav file\n                self._save_wav(os.path.join(self.outdir, f\"{utt_id}_output.wav\"), y)\n\n        logging.info(\n            \"Finished generation of %d utterances (RTF = %.03f).\" % (idx, (total_rtf / idx))\n        )\n    \n\n    def _save_wav(self, file_name, audio):\n        sf.write(\n            file_name,\n            audio,\n            self.decoder_config['sampling_rate'],\n            \"PCM_16\",\n        )\n    \n    \n    def _load_config(self, checkpoint, config_name='config.yml'):\n        return load_config(checkpoint, config_name)", "\nclass TestGEN(abc.ABC):\n    def __init__(\n        self,\n        args,\n    ):\n        # set logger\n        logging.basicConfig(\n            level=logging.INFO,\n            stream=sys.stdout,\n            format=\"%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s\",\n        )\n\n        # device\n        if not torch.cuda.is_available():\n            self.device = torch.device('cpu')\n            logging.info(f\"device: cpu\")\n        else:\n            self.device = torch.device('cuda')\n            logging.info(f\"device: gpu\")\n        \n        # initialize attribute\n        if hasattr(args, 'encoder'):\n            self.encoder_checkpoint = args.encoder\n            self.encoder_config = self._load_config(args.encoder)\n        if hasattr(args, 'decoder'):\n            self.decoder_checkpoint = args.decoder\n            self.decoder_config = self._load_config(args.decoder)\n        self.encoder = None\n        self.decoder = None\n        self.dataset = None\n        self.outdir = None\n    \n\n    @abc.abstractmethod\n    def initial_folder(self, output_name):\n        pass\n        \n\n    @abc.abstractmethod    \n    def load_dataset(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def load_encoder(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def load_decoder(self):\n        pass\n    \n    \n    @abc.abstractmethod\n    def encode(self, x):\n        pass\n    \n    \n    @abc.abstractmethod\n    def decode(self, z):\n        pass\n\n    \n    def run(self):\n        total_rtf = 0.0\n        with torch.no_grad(), tqdm(self.dataset, desc=\"[test]\") as pbar:\n            for idx, (utt_id, x) in enumerate(pbar, 1):\n                start = time.time()\n                zq = self.encode(x)\n                y = self.decode(zq)\n                y = y.squeeze(1).transpose(1, 0).cpu().numpy() # T x C\n                rtf = (time.time() - start) / (len(y) / self.decoder_config['sampling_rate'])\n                pbar.set_postfix({\"RTF\": rtf})\n                total_rtf += rtf\n\n                # output wav file\n                self._save_wav(os.path.join(self.outdir, f\"{utt_id}_output.wav\"), y)\n\n        logging.info(\n            \"Finished generation of %d utterances (RTF = %.03f).\" % (idx, (total_rtf / idx))\n        )\n    \n\n    def _save_wav(self, file_name, audio):\n        sf.write(\n            file_name,\n            audio,\n            self.decoder_config['sampling_rate'],\n            \"PCM_16\",\n        )\n    \n    \n    def _load_config(self, checkpoint, config_name='config.yml'):\n        return load_config(checkpoint, config_name)", ""]}
{"filename": "utils/audiodec.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os", "\nimport os\nimport torch\nfrom typing import Union\nfrom models.autoencoder.AudioDec import StreamGenerator as generator_audiodec\nfrom models.vocoder.HiFiGAN import StreamGenerator as generator_hifigan\nfrom bin.stream import AudioCodec, AudioCodecStreamer\n\n\nclass AudioDec(AudioCodec):\n    def __init__(\n        self,\n        tx_device: str = \"cpu\",\n        rx_device: str = \"cpu\",\n        receptive_length: int = 8192, # actual number is 7209 for symAD_vctk_48000_hop300\n    ):\n        super(AudioDec, self).__init__(\n            tx_device = tx_device,\n            rx_device = rx_device,\n            receptive_length = receptive_length,\n        )\n\n\n    def _load_encoder(self, checkpoint):\n        # load config\n        config = self._load_config(checkpoint)\n        # load model\n        if config['model_type'] in ['symAudioDec', 'symAudioDecUniv']:\n            encoder = generator_audiodec\n        else:\n            raise NotImplementedError(f\"Encoder type {config['model_type']} is not supported!\")\n        encoder = encoder(**config['generator_params'])\n        encoder.load_state_dict(torch.load(checkpoint, map_location='cpu')['model']['generator'])\n        return encoder\n\n\n    def _load_decoder(self, checkpoint):\n        # load config\n        config = self._load_config(checkpoint)\n        # load model\n        if config['model_type'] in ['symAudioDec', 'symAudioDecUniv']:\n            decoder = generator_audiodec\n        elif config['model_type'] in ['HiFiGAN', 'UnivNet']:\n            decoder = generator_hifigan\n        else:\n            raise NotImplementedError(f\"Decoder {config['model_type']} is not supported!\")\n        decoder = decoder(**config['generator_params'])\n        decoder.load_state_dict(torch.load(checkpoint, map_location='cpu')['model']['generator'])\n        return decoder", "\nclass AudioDec(AudioCodec):\n    def __init__(\n        self,\n        tx_device: str = \"cpu\",\n        rx_device: str = \"cpu\",\n        receptive_length: int = 8192, # actual number is 7209 for symAD_vctk_48000_hop300\n    ):\n        super(AudioDec, self).__init__(\n            tx_device = tx_device,\n            rx_device = rx_device,\n            receptive_length = receptive_length,\n        )\n\n\n    def _load_encoder(self, checkpoint):\n        # load config\n        config = self._load_config(checkpoint)\n        # load model\n        if config['model_type'] in ['symAudioDec', 'symAudioDecUniv']:\n            encoder = generator_audiodec\n        else:\n            raise NotImplementedError(f\"Encoder type {config['model_type']} is not supported!\")\n        encoder = encoder(**config['generator_params'])\n        encoder.load_state_dict(torch.load(checkpoint, map_location='cpu')['model']['generator'])\n        return encoder\n\n\n    def _load_decoder(self, checkpoint):\n        # load config\n        config = self._load_config(checkpoint)\n        # load model\n        if config['model_type'] in ['symAudioDec', 'symAudioDecUniv']:\n            decoder = generator_audiodec\n        elif config['model_type'] in ['HiFiGAN', 'UnivNet']:\n            decoder = generator_hifigan\n        else:\n            raise NotImplementedError(f\"Decoder {config['model_type']} is not supported!\")\n        decoder = decoder(**config['generator_params'])\n        decoder.load_state_dict(torch.load(checkpoint, map_location='cpu')['model']['generator'])\n        return decoder", "\n\nclass AudioDecStreamer(AudioCodecStreamer):\n    def __init__(\n        self,\n        input_device: Union[str, int],\n        output_device: Union[str, int],\n        input_channels: int = 1,\n        output_channels: int = 1,\n        frame_size: int = 512,\n        sample_rate: int = 48000,\n        gain: int = 1.0,\n        max_latency: float = 0.1,\n        # encoder params\n        tx_encoder = None,\n        tx_device: str = \"cpu\",\n        # decoder params\n        rx_encoder = None,\n        decoder = None,\n        rx_device: str = \"cpu\",\n    ):\n        super(AudioDecStreamer, self).__init__(\n            input_device = input_device,\n            output_device = output_device,\n            input_channels = input_channels,\n            output_channels = output_channels,\n            frame_size = frame_size,\n            sample_rate = sample_rate,\n            gain = gain,\n            max_latency = max_latency,\n            tx_encoder = tx_encoder,\n            tx_device = tx_device,\n            rx_encoder = rx_encoder,\n            decoder = decoder,\n            rx_device = rx_device,\n        )\n\n\n    def _encode(self, x):\n        x = self.tx_encoder.encode(x)\n        return self.tx_encoder.quantize(x)\n\n\n    def _decode(self, x):\n        x = self.rx_encoder.lookup(x)\n        return self.decoder.decode(x)", "    \n\ndef assign_model(model):\n    if model == 'libritts_v1':\n        sample_rate = 24000\n        tx_steps = 500000\n        rx_steps = 500000\n        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_libritts_24000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v1_symAD_libritts_24000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\") \n    elif model == 'libritts_sym':\n        sample_rate = 24000\n        tx_steps = 500000\n        rx_steps = 1000000\n        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_libritts_24000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_libritts_24000_hop300', f\"checkpoint-{rx_steps}steps.pkl\")\n    elif model == 'vctk_v1':\n        sample_rate = 48000\n        tx_steps = 200000\n        rx_steps = 500000\n        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v1_symAD_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\") \n    elif model == 'vctk_sym':\n        sample_rate = 48000\n        tx_steps = 200000\n        rx_steps = 700000\n        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{rx_steps}steps.pkl\") \n    elif model == 'vctk_v0':\n        sample_rate = 48000\n        tx_steps = 200000\n        rx_steps = 500000\n        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v0_symAD_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\") \n    elif model == 'vctk_v2':\n        sample_rate = 48000\n        tx_steps = 200000\n        rx_steps = 500000\n        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v2_symAD_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\") \n    elif model == 'vctk_denoise':\n        sample_rate = 48000\n        tx_steps = 200000\n        rx_steps = 500000\n        encoder_checkpoint = os.path.join('exp', 'denoise', 'symAD_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v1_symAD_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\")\n    elif model == 'vctk_univ':\n        sample_rate = 48000\n        tx_steps = 500000\n        rx_steps = 500000\n        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symADuniv_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'vocoder', 'AudioDec_v3_symADuniv_vctk_48000_hop300_clean', f\"checkpoint-{rx_steps}steps.pkl\")\n    elif model == 'vctk_univ_sym':\n        sample_rate = 48000\n        tx_steps = 500000\n        rx_steps = 1000000\n        encoder_checkpoint = os.path.join('exp', 'autoencoder', 'symADuniv_vctk_48000_hop300', f\"checkpoint-{tx_steps}steps.pkl\")\n        decoder_checkpoint = os.path.join('exp', 'autoencoder', 'symADuniv_vctk_48000_hop300', f\"checkpoint-{rx_steps}steps.pkl\")\n    else:\n        raise NotImplementedError(f'Model {model} is not supported!')\n    \n    return sample_rate, encoder_checkpoint, decoder_checkpoint", ""]}
{"filename": "layers/conv_layer.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Convolution layers.\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Conv1d1x1(nn.Conv1d):\n    \"\"\"1x1 Conv1d.\"\"\"\n\n    def __init__(self, in_channels, out_channels, bias=True):\n        super(Conv1d1x1, self).__init__(in_channels, out_channels, kernel_size=1, bias=bias)", "\n\nclass Conv1d1x1(nn.Conv1d):\n    \"\"\"1x1 Conv1d.\"\"\"\n\n    def __init__(self, in_channels, out_channels, bias=True):\n        super(Conv1d1x1, self).__init__(in_channels, out_channels, kernel_size=1, bias=bias)\n\n\nclass NonCausalConv1d(nn.Module):\n    \"\"\"1D noncausal convloution w/ 2-sides padding.\"\"\"\n\n    def __init__(\n            self, \n            in_channels, \n            out_channels, \n            kernel_size, \n            stride=1, \n            padding=-1, \n            dilation=1,\n            groups=1,\n            bias=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        if padding < 0:\n            padding = (kernel_size - 1) // 2 * dilation\n        self.dilation = dilation\n        self.conv = nn.Conv1d(\n            in_channels=in_channels, \n            out_channels=out_channels, \n            kernel_size=kernel_size,\n            stride=stride, \n            padding=padding, \n            dilation=dilation, \n            groups=groups,\n            bias=bias,\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): Float tensor variable with the shape  (B, C, T).\n        Returns:\n            Tensor: Float tensor variable with the shape (B, C, T).\n        \"\"\"\n        x = self.conv(x)\n        return x", "\nclass NonCausalConv1d(nn.Module):\n    \"\"\"1D noncausal convloution w/ 2-sides padding.\"\"\"\n\n    def __init__(\n            self, \n            in_channels, \n            out_channels, \n            kernel_size, \n            stride=1, \n            padding=-1, \n            dilation=1,\n            groups=1,\n            bias=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        if padding < 0:\n            padding = (kernel_size - 1) // 2 * dilation\n        self.dilation = dilation\n        self.conv = nn.Conv1d(\n            in_channels=in_channels, \n            out_channels=out_channels, \n            kernel_size=kernel_size,\n            stride=stride, \n            padding=padding, \n            dilation=dilation, \n            groups=groups,\n            bias=bias,\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): Float tensor variable with the shape  (B, C, T).\n        Returns:\n            Tensor: Float tensor variable with the shape (B, C, T).\n        \"\"\"\n        x = self.conv(x)\n        return x", "    \n\nclass NonCausalConvTranspose1d(nn.Module):\n    \"\"\"1D noncausal transpose convloution.\"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding=-1,\n        output_padding=-1,\n        groups=1,\n        bias=True,\n    ):\n        super().__init__()\n        if padding < 0:\n            padding = (stride+1) // 2\n        if output_padding < 0:\n            output_padding = 1 if stride % 2 else 0\n        self.deconv = nn.ConvTranspose1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            output_padding=output_padding,\n            groups=groups,\n            bias=bias,\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): Float tensor variable with the shape  (B, C, T).\n        Returns:\n            Tensor: Float tensor variable with the shape (B, C', T').\n        \"\"\"\n        x = self.deconv(x)\n        return x", "\n\nclass CausalConv1d(NonCausalConv1d):\n    \"\"\"1D causal convloution w/ 1-side padding.\"\"\"\n\n    def __init__(\n        self, \n        in_channels, \n        out_channels, \n        kernel_size, \n        stride=1, \n        dilation=1, \n        groups=1,\n        bias=True,\n        pad_buffer=None,\n    ):\n        super(CausalConv1d, self).__init__(\n            in_channels=in_channels, \n            out_channels=out_channels, \n            kernel_size=kernel_size, \n            stride=stride, \n            padding=0,\n            dilation=dilation, \n            groups=groups,\n            bias=bias,\n        )\n        self.stride = stride\n        self.pad_length = (kernel_size - 1) * dilation\n        if pad_buffer is None:\n            pad_buffer = torch.zeros(1, in_channels, self.pad_length)\n        self.register_buffer(\"pad_buffer\", pad_buffer)\n\n    def forward(self, x):\n        pad = nn.ConstantPad1d((self.pad_length, 0), 0.0)\n        x = pad(x)\n        return self.conv(x)\n    \n    def inference(self, x):\n        x = torch.cat((self.pad_buffer, x), -1)\n        self.pad_buffer = x[:, :, -self.pad_length:]\n        return self.conv(x)\n    \n    def reset_buffer(self):\n        self.pad_buffer.zero_()", "\n        \nclass CausalConvTranspose1d(NonCausalConvTranspose1d):\n    \"\"\"1D causal transpose convloution.\"\"\"\n\n    def __init__(\n        self, \n        in_channels, \n        out_channels, \n        kernel_size, \n        stride, \n        bias=True,\n        pad_buffer=None,\n    ):\n        super(CausalConvTranspose1d, self).__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=0,\n            output_padding=0,\n            bias=bias,\n        )\n        self.stride = stride\n        self.pad_length = 1\n        if pad_buffer is None:\n            pad_buffer = torch.zeros(1, in_channels, self.pad_length)\n        self.register_buffer(\"pad_buffer\", pad_buffer)\n\n    def forward(self, x):\n        pad = nn.ReplicationPad1d((self.pad_length, 0))\n        x = pad(x)\n        return self.deconv(x)[:, :, self.stride : -self.stride]\n    \n    def inference(self, x):\n        x = torch.cat((self.pad_buffer, x), -1)\n        self.pad_buffer = x[:, :, -self.pad_length:]\n        return self.deconv(x)[:, :, self.stride : -self.stride]\n    \n    def reset_buffer(self):\n        self.pad_buffer.zero_()", ""]}
{"filename": "layers/vq_module.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/lucidrains/vector-quantize-pytorch/)", "#\n# Reference (https://github.com/lucidrains/vector-quantize-pytorch/)\n\n\"\"\"Vector quantizer.\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass VectorQuantize(nn.Module):\n    \"\"\"Vector quantization w/ exponential moving averages (EMA)\"\"\"\n\n    def __init__(\n        self,\n        dim,\n        codebook_size,\n        decay = 0.8,\n        commitment = 1.,\n        eps = 1e-5,\n        n_embed = None,\n    ):\n        super().__init__()\n        n_embed = self.default(n_embed, codebook_size)\n\n        self.dim = dim\n        self.n_embed = n_embed\n        self.decay = decay\n        self.eps = eps\n        self.commitment = commitment\n\n        embed = torch.randn(dim, n_embed)\n        self.register_buffer('embed', embed)\n        self.register_buffer('cluster_size', torch.zeros(n_embed))\n        self.register_buffer('embed_avg', embed.clone())\n\n    @property\n    def codebook(self):\n        return self.embed.transpose(0, 1)\n    \n    def exists(self,val):\n        return val is not None\n\n    def default(self, val, d):\n        return val if self.exists(val) else d\n    \n    def ema_inplace(self, moving_avg, new, decay):\n        moving_avg.data.mul_(decay).add_(new, alpha = (1 - decay))\n    \n    def laplace_smoothing(self, x, n_categories, eps=1e-5):\n        return (x + eps) / (x.sum() + n_categories * eps)\n    \n    def forward(self, input):\n        dtype = input.dtype\n        flatten = input.reshape(-1, self.dim)\n        dist = (\n            flatten.pow(2).sum(1, keepdim=True)\n            - 2 * flatten @ self.embed\n            + self.embed.pow(2).sum(0, keepdim=True)\n        )\n        _, embed_ind = (-dist).max(1)\n        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(dtype)\n        embed_ind = embed_ind.view(*input.shape[:-1])\n        quantize = F.embedding(embed_ind, self.embed.transpose(0, 1))\n\n        if self.training:\n            self.ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)\n            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n            self.ema_inplace(self.embed_avg, embed_sum, self.decay)\n            cluster_size = self.laplace_smoothing(self.cluster_size, self.n_embed, self.eps) * self.cluster_size.sum()\n            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n            self.embed.data.copy_(embed_normalized)\n\n        loss = F.mse_loss(quantize.detach(), input) * self.commitment\n        quantize = input + (quantize - input).detach()\n\n        avg_probs = torch.mean(embed_onehot, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        return quantize, loss, perplexity\n    \n    def forward_index(self, input):\n        dtype = input.dtype\n        flatten = input.reshape(-1, self.dim)\n        dist = (\n            flatten.pow(2).sum(1, keepdim=True)\n            - 2 * flatten @ self.embed\n            + self.embed.pow(2).sum(0, keepdim=True)\n        )\n        _, embed_ind = (-dist).max(1)\n        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(dtype)\n        embed_ind = embed_ind.view(*input.shape[:-1])\n        quantize = F.embedding(embed_ind, self.embed.transpose(0, 1))\n        quantize = input + (quantize - input).detach()\n\n        return quantize, embed_ind", "\n\nclass VectorQuantize(nn.Module):\n    \"\"\"Vector quantization w/ exponential moving averages (EMA)\"\"\"\n\n    def __init__(\n        self,\n        dim,\n        codebook_size,\n        decay = 0.8,\n        commitment = 1.,\n        eps = 1e-5,\n        n_embed = None,\n    ):\n        super().__init__()\n        n_embed = self.default(n_embed, codebook_size)\n\n        self.dim = dim\n        self.n_embed = n_embed\n        self.decay = decay\n        self.eps = eps\n        self.commitment = commitment\n\n        embed = torch.randn(dim, n_embed)\n        self.register_buffer('embed', embed)\n        self.register_buffer('cluster_size', torch.zeros(n_embed))\n        self.register_buffer('embed_avg', embed.clone())\n\n    @property\n    def codebook(self):\n        return self.embed.transpose(0, 1)\n    \n    def exists(self,val):\n        return val is not None\n\n    def default(self, val, d):\n        return val if self.exists(val) else d\n    \n    def ema_inplace(self, moving_avg, new, decay):\n        moving_avg.data.mul_(decay).add_(new, alpha = (1 - decay))\n    \n    def laplace_smoothing(self, x, n_categories, eps=1e-5):\n        return (x + eps) / (x.sum() + n_categories * eps)\n    \n    def forward(self, input):\n        dtype = input.dtype\n        flatten = input.reshape(-1, self.dim)\n        dist = (\n            flatten.pow(2).sum(1, keepdim=True)\n            - 2 * flatten @ self.embed\n            + self.embed.pow(2).sum(0, keepdim=True)\n        )\n        _, embed_ind = (-dist).max(1)\n        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(dtype)\n        embed_ind = embed_ind.view(*input.shape[:-1])\n        quantize = F.embedding(embed_ind, self.embed.transpose(0, 1))\n\n        if self.training:\n            self.ema_inplace(self.cluster_size, embed_onehot.sum(0), self.decay)\n            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n            self.ema_inplace(self.embed_avg, embed_sum, self.decay)\n            cluster_size = self.laplace_smoothing(self.cluster_size, self.n_embed, self.eps) * self.cluster_size.sum()\n            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n            self.embed.data.copy_(embed_normalized)\n\n        loss = F.mse_loss(quantize.detach(), input) * self.commitment\n        quantize = input + (quantize - input).detach()\n\n        avg_probs = torch.mean(embed_onehot, dim=0)\n        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n\n        return quantize, loss, perplexity\n    \n    def forward_index(self, input):\n        dtype = input.dtype\n        flatten = input.reshape(-1, self.dim)\n        dist = (\n            flatten.pow(2).sum(1, keepdim=True)\n            - 2 * flatten @ self.embed\n            + self.embed.pow(2).sum(0, keepdim=True)\n        )\n        _, embed_ind = (-dist).max(1)\n        embed_onehot = F.one_hot(embed_ind, self.n_embed).type(dtype)\n        embed_ind = embed_ind.view(*input.shape[:-1])\n        quantize = F.embedding(embed_ind, self.embed.transpose(0, 1))\n        quantize = input + (quantize - input).detach()\n\n        return quantize, embed_ind", "\n\nclass ResidualVQ(nn.Module):\n    \"\"\" Residual VQ following algorithm 1. in https://arxiv.org/pdf/2107.03312.pdf \"\"\"\n\n    def __init__(\n        self,\n        *,\n        num_quantizers,\n        **kwargs\n    ):\n        super().__init__()\n        self.layers = nn.ModuleList([VectorQuantize(**kwargs) for _ in range(num_quantizers)])\n\n    def forward(self, x):\n        quantized_out = 0.\n        residual = x\n        all_losses = []\n        all_perplexities = []\n        for layer in self.layers:\n            quantized, loss, perplexity = layer(residual)\n            # Issue: https://github.com/lucidrains/vector-quantize-pytorch/issues/33\n            # We found considering only the 1st layer VQ's graident results in better performance\n            #residual = residual - quantized.detach() # considering all layers' graidents\n            residual = residual - quantized # considering only the first layer's graident \n            quantized_out = quantized_out + quantized\n            all_losses.append(loss)\n            all_perplexities.append(perplexity)\n        all_losses, all_perplexities = map(torch.stack, (all_losses, all_perplexities))\n        return quantized_out, all_losses, all_perplexities\n\n    def forward_index(self, x, flatten_idx=False):\n        quantized_out = 0.\n        residual = x\n        all_indices = []\n        for i, layer in enumerate(self.layers):\n            quantized, indices = layer.forward_index(residual)\n            #residual = residual - quantized.detach()\n            residual = residual - quantized\n            quantized_out = quantized_out + quantized\n            if flatten_idx:\n                indices += (self.codebook_size * i)\n            all_indices.append(indices)\n        all_indices= torch.stack(all_indices)\n        return quantized_out, all_indices.squeeze(1)\n    \n    def initial(self):\n        self.codebook = []\n        for layer in self.layers:\n            self.codebook.append(layer.codebook)\n        self.codebook_size = self.codebook[0].size(0)\n        self.codebook = torch.stack(self.codebook)\n        self.codebook = self.codebook.reshape(-1, self.codebook.size(-1))\n    \n    def lookup(self, indices):\n        quantized_out = F.embedding(indices, self.codebook) # Num x T x C\n        return  torch.sum(quantized_out, dim=0,keepdim=True)", ""]}
{"filename": "models/utils.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n", "\n\n\"\"\"Utility modules.\"\"\"\n\ndef check_mode(mode, method):\n    stream_modes = ['causal']\n    assert mode in stream_modes, f\"Mode {mode} does not support {method}!\"\n    "]}
{"filename": "models/vocoder/HiFiGAN.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n# Reference (https://github.com/jik876/hifi-gan)\n\n\"\"\"HiFi-GAN Modules. (Causal)\"\"\"\n\nimport logging\nimport os\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom layers.conv_layer import CausalConv1d, CausalConvTranspose1d\nfrom models.vocoder.modules.multi_fusion import MultiReceptiveField, MultiGroupConv1d\nfrom models.vocoder.modules.discriminator import HiFiGANMultiScaleDiscriminator\nfrom models.vocoder.modules.discriminator import HiFiGANMultiPeriodDiscriminator\n", "from models.vocoder.modules.discriminator import HiFiGANMultiPeriodDiscriminator\n\n\nclass Generator(nn.Module):\n    \"\"\"HiFiGAN causal generator module.\"\"\"\n\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=1,\n        channels=512,\n        kernel_size=7,\n        upsample_scales=(8, 8, 2, 2),\n        upsample_kernel_sizes=(16, 16, 4, 4),\n        resblock_kernel_sizes=(3, 7, 11),\n        resblock_dilations=[(1, 3, 5), (1, 3, 5), (1, 3, 5)],\n        groups=1,\n        bias=True,\n        use_additional_convs=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.1},\n        use_weight_norm=True,\n        stats=None,\n    ):\n        \"\"\"Initialize HiFiGANGenerator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            channels (int): Number of hidden representation channels.\n            kernel_size (int): Kernel size of initial and final conv layer.\n            upsample_scales (list): List of upsampling scales.\n            upsample_kernel_sizes (list): List of kernel sizes for upsampling layers.\n            resblock_kernel_sizes (list): List of kernel sizes for residual blocks.\n            resblock_dilations (list): List of dilation list for residual blocks.\n            groups (int): Number of groups of residual conv\n            bias (bool): Whether to add bias parameter in convolution layers.\n            use_additional_convs (bool): Whether to use additional conv layers in residual blocks.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            use_weight_norm (bool): Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n            stats (str): File name of the statistic file\n\n        \"\"\"\n        super().__init__()\n\n        # check hyperparameters are valid\n        assert kernel_size % 2 == 1, \"Kernel size must be odd number.\"\n        assert len(upsample_scales) == len(upsample_kernel_sizes)\n        assert len(resblock_dilations) == len(resblock_kernel_sizes)\n\n        # Group conv or MRF\n        if (len(resblock_dilations) == len(resblock_kernel_sizes) == 1) and (groups > 1):\n            multi_fusion = MultiGroupConv1d\n        else:\n            multi_fusion = MultiReceptiveField\n        \n        # define modules\n        self.num_upsamples = len(upsample_kernel_sizes)\n        self.input_conv = CausalConv1d(\n            in_channels,\n            channels,\n            kernel_size,\n            stride=1,\n        )\n        self.upsamples = nn.ModuleList()\n        self.blocks = nn.ModuleList()\n        self.activation_upsamples = getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params)\n        for i in range(len(upsample_kernel_sizes)):\n            assert upsample_kernel_sizes[i] == 2 * upsample_scales[i]\n            self.upsamples += [\n                CausalConvTranspose1d(\n                    channels // (2 ** i),\n                    channels // (2 ** (i + 1)),\n                    kernel_size=upsample_kernel_sizes[i],\n                    stride=upsample_scales[i],\n                )\n            ]\n            self.blocks += [\n                multi_fusion(\n                    channels=channels // (2 ** (i + 1)),\n                    resblock_kernel_sizes=resblock_kernel_sizes,\n                    resblock_dilations=resblock_dilations,\n                    groups=groups,\n                    bias=bias,\n                    use_additional_convs=use_additional_convs,\n                    nonlinear_activation=nonlinear_activation,\n                    nonlinear_activation_params=nonlinear_activation_params,\n                )\n            ]\n        self.activation_output1 = nn.LeakyReLU()\n        self.activation_output2 = nn.Tanh()\n        self.output_conv = CausalConv1d(\n            channels // (2 ** (i + 1)),\n            out_channels,\n            kernel_size,\n            stride=1,\n        )\n\n        # load stats\n        if stats is not None:\n            self.register_stats(stats)\n            self.norm = True\n        else:\n            self.norm = False\n        logging.info(f\"Input normalization: {self.norm}\")\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n        # reset parameters\n        self.reset_parameters()\n\n    \n    def forward(self, c):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            c (Tensor): Input tensor (B, in_channels, T).\n\n        Returns:\n            Tensor: Output tensor (B, out_channels, T).\n\n        \"\"\"\n        if self.norm:\n            c = (c.transpose(2, 1) - self.mean) / self.scale\n            c = c.transpose(2, 1)\n        c = self.input_conv(c)\n        for i in range(self.num_upsamples):\n            c = self.upsamples[i](self.activation_upsamples(c))\n            c = self.blocks[i](c)\n        c = self.output_conv(self.activation_output1(c))\n        c = self.activation_output2(c)\n\n        return c\n    \n\n    def reset_parameters(self):\n        \"\"\"Reset parameters.\n\n        This initialization follows the official implementation manner.\n        https://github.com/jik876/hifi-gan/blob/master/models.py\n\n        \"\"\"\n\n        def _reset_parameters(m):\n            if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d)):\n                m.weight.data.normal_(0.0, 0.01)\n                logging.debug(f\"Reset parameters in {m}.\")\n\n        self.apply(_reset_parameters)\n\n\n    def remove_weight_norm(self):\n        \"\"\"Remove weight normalization module from all of the layers.\"\"\"\n\n        def _remove_weight_norm(m):\n            try:\n                logging.debug(f\"Weight norm is removed from {m}.\")\n                nn.utils.remove_weight_norm(m)\n            except ValueError:  # this module didn't have weight norm\n                return\n\n        self.apply(_remove_weight_norm)\n\n\n    def apply_weight_norm(self):\n        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n\n        def _apply_weight_norm(m):\n            if isinstance(m, nn.Conv1d) or isinstance(\n                m, nn.ConvTranspose1d\n            ):\n                nn.utils.weight_norm(m)\n                logging.debug(f\"Weight norm is applied to {m}.\")\n\n        self.apply(_apply_weight_norm)\n\n\n    def register_stats(self, stats):\n        \"\"\"Register stats for de-normalization as buffer.\n\n        Args:\n            stats (str): Path of statistics file (\".npy\" or \".h5\").\n\n        \"\"\"\n        assert stats.endswith(\".h5\") or stats.endswith(\".npy\")\n        assert os.path.exists(stats), f\"Stats {stats} does not exist!\"\n        mean = np.load(stats)[0].reshape(-1)\n        scale = np.load(stats)[1].reshape(-1)\n        self.register_buffer(\"mean\", torch.from_numpy(mean).float())\n        self.register_buffer(\"scale\", torch.from_numpy(scale).float())\n        logging.info(\"Successfully registered stats as buffer.\")", "\n\nclass StreamGenerator(Generator):\n    \"\"\"HiFiGAN streaming generator.\"\"\"\n\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=1,\n        channels=512,\n        kernel_size=7,\n        upsample_scales=(8, 8, 2, 2),\n        upsample_kernel_sizes=(16, 16, 4, 4),\n        resblock_kernel_sizes=(3, 7, 11),\n        resblock_dilations=[(1, 3, 5), (1, 3, 5), (1, 3, 5)],\n        groups=1,\n        bias=True,\n        use_additional_convs=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.1},\n        use_weight_norm=True,\n        stats=None,\n    ):\n\n        super(StreamGenerator, self).__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            channels=channels,\n            kernel_size=kernel_size,\n            upsample_scales=upsample_scales,\n            upsample_kernel_sizes=upsample_kernel_sizes,\n            resblock_kernel_sizes=resblock_kernel_sizes,\n            resblock_dilations=resblock_dilations,\n            groups=groups,\n            bias=bias,\n            use_additional_convs=use_additional_convs,\n            nonlinear_activation=nonlinear_activation,\n            nonlinear_activation_params=nonlinear_activation_params,\n            use_weight_norm=use_weight_norm,\n            stats=stats,\n        )\n        self.reset_buffer()\n\n    \n    def initial_decoder(self, c):\n        self.decode(c)\n    \n\n    def decode(self, c):\n        c = self.decode_norm(c)\n        c = self.decode_input(c.transpose(2, 1))\n        c = self.decode_upsample(c)\n        c = self.decode_output(c)\n        return c\n    \n\n    def decode_norm(self, c):\n        if self.norm:\n            c = (c - self.mean) / self.scale\n        return c \n\n\n    def decode_input(self, c):\n        c = self.input_conv.inference(c)\n        return c\n\n\n    def decode_upsample(self, c):        \n        for i in range(self.num_upsamples):\n            c = self.upsamples[i].inference(self.activation_upsamples(c))\n            c = self.blocks[i].inference(c)\n        return c\n\n\n    def decode_output(self, c):\n        c = self.output_conv.inference(self.activation_output1(c))\n        return self.activation_output2(c)\n    \n\n    def reset_buffer(self):\n        \"\"\"Apply weight normalization module from all layers.\"\"\"\n\n        def _reset_buffer(m):\n            if isinstance(m, CausalConv1d) or isinstance(m, CausalConvTranspose1d):\n                m.reset_buffer()\n        self.apply(_reset_buffer)", "\n\nclass Discriminator(nn.Module):\n    \"\"\"HiFi-GAN multi-scale + multi-period discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        # Multi-scale discriminator related\n        scales=3,\n        scale_downsample_pooling=\"AvgPool1d\",\n        scale_downsample_pooling_params={\n            \"kernel_size\": 4,\n            \"stride\": 2,\n            \"padding\": 2,\n        },\n        scale_discriminator_params={\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [15, 41, 5, 3],\n            \"channels\": 128,\n            \"max_downsample_channels\": 1024,\n            \"max_groups\": 16,\n            \"bias\": True,\n            \"downsample_scales\": [2, 2, 4, 4, 1],\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n        },\n        follow_official_norm=True,\n        # Multi-period discriminator related\n        periods=[2, 3, 5, 7, 11],\n        period_discriminator_params={\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [5, 3],\n            \"channels\": 32,\n            \"downsample_scales\": [3, 3, 3, 3, 1],\n            \"max_downsample_channels\": 1024,\n            \"bias\": True,\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n            \"use_weight_norm\": True,\n            \"use_spectral_norm\": False,\n        },\n    ):\n        \"\"\"Initilize HiFiGAN multi-scale + multi-period discriminator module.\n\n        Args:\n            scales (int): Number of multi-scales.\n            scale_downsample_pooling (str): Pooling module name for downsampling of the inputs.\n            scale_downsample_pooling_params (dict): Parameters for the above pooling module.\n            scale_discriminator_params (dict): Parameters for hifi-gan scale discriminator module.\n            follow_official_norm (bool): Whether to follow the norm setting of the official\n                implementaion. The first discriminator uses spectral norm and the other\n                discriminators use weight norm.\n            periods (list): List of periods.\n            period_discriminator_params (dict): Parameters for hifi-gan period discriminator module.\n                The period parameter will be overwritten.\n\n        \"\"\"\n        super().__init__()\n        self.msd = HiFiGANMultiScaleDiscriminator(\n            scales=scales,\n            downsample_pooling=scale_downsample_pooling,\n            downsample_pooling_params=scale_downsample_pooling_params,\n            discriminator_params=scale_discriminator_params,\n            follow_official_norm=follow_official_norm,\n        )\n        self.mpd = HiFiGANMultiPeriodDiscriminator(\n            periods=periods,\n            discriminator_params=period_discriminator_params,\n        )\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, C, T).\n\n        Returns:\n            List: List of list of each discriminator outputs,\n                which consists of each layer output tensors.\n                Multi scale and multi period ones are concatenated.\n\n        \"\"\"\n        (batch, channel, time) = x.size()\n        if channel != 1:\n            x = x.reshape(batch * channel, 1, time)\n        msd_outs = self.msd(x)\n        mpd_outs = self.mpd(x)\n        return msd_outs + mpd_outs", ""]}
{"filename": "models/vocoder/UnivNet.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n# Reference (https://github.com/chomeyama/SiFiGAN)\n\n\"\"\"HiFi-GAN Modules. (Causal)\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n", "import torch.nn.functional as F\n\nfrom models.vocoder.modules.discriminator import UnivNetMultiResolutionSpectralDiscriminator\nfrom models.vocoder.modules.discriminator import HiFiGANMultiPeriodDiscriminator\n\n\nclass Discriminator(nn.Module):\n    \"\"\"UnivNet multi-resolution spectrogram + multi-period discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        # Multi-resolution spectrogram discriminator related\n        fft_sizes=[1024, 2048, 512],\n        hop_sizes=[120, 240, 50],\n        win_lengths=[600, 1200, 240],\n        window=\"hann_window\",\n        spectral_discriminator_params={\n            \"channels\": 32,\n            \"kernel_sizes\": [(3, 9), (3, 9), (3, 9), (3, 9), (3, 3), (3, 3)],\n            \"strides\": [(1, 1), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1)],\n            \"bias\": True,\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.2},\n        },\n        # Multi-period discriminator related\n        periods=[2, 3, 5, 7, 11],\n        period_discriminator_params={\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [5, 3],\n            \"channels\": 32,\n            \"downsample_scales\": [3, 3, 3, 3, 1],\n            \"max_downsample_channels\": 1024,\n            \"bias\": True,\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n            \"use_weight_norm\": True,\n            \"use_spectral_norm\": False,\n        },\n        flat_channel=False,\n    ):\n        \"\"\"Initilize HiFiGAN multi-scale + multi-period discriminator module.\n\n        Args:\n            fft_sizes (list): FFT sizes for each spectral discriminator.\n            hop_sizes (list): Hop sizes for each spectral discriminator.\n            win_lengths (list): Window lengths for each spectral discriminator.\n            window (stt): Name of window function.\n            sperctral_discriminator_params (dict): Parameters for hifi-gan scale discriminator module.\n            periods (list): List of periods.\n            period_discriminator_params (dict): Parameters for hifi-gan period discriminator module.\n                The period parameter will be overwritten.\n            flat_channel (bool):set true to flat multi-channel input to one-channel with multi-batch\n\n        \"\"\"\n        super().__init__()\n        self.flat_channel = flat_channel\n        self.mrsd = UnivNetMultiResolutionSpectralDiscriminator(\n            fft_sizes=fft_sizes,\n            hop_sizes=hop_sizes,\n            win_lengths=win_lengths,\n            window=window,\n            discriminator_params=spectral_discriminator_params,\n        )\n        self.mpd = HiFiGANMultiPeriodDiscriminator(\n            periods=periods,\n            discriminator_params=period_discriminator_params,\n        )\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, C, T).\n\n        Returns:\n            List: List of list of each discriminator outputs,\n                which consists of each layer output tensors.\n                Multi scale and multi period ones are concatenated.\n\n        \"\"\"\n        (batch, channel, time) = x.size()\n        if channel != 1 and self.flat_channel:\n            x = x.reshape(batch * channel, 1, time)\n        mrsd_outs = self.mrsd(x)\n        mpd_outs = self.mpd(x)\n        return mrsd_outs + mpd_outs", ""]}
{"filename": "models/vocoder/modules/multi_fusion.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n# Reference (https://github.com/r9y9/wavenet_vocoder)\n# Reference (https://github.com/jik876/hifi-gan)\n\n\"\"\"Multi-fusion modules.\"\"\"\n\nimport math\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nfrom layers.conv_layer import CausalConv1d, Conv1d1x1\nfrom models.vocoder.modules.residual_block import HiFiGANResidualBlock\n\n\nclass MultiReceptiveField(nn.Module):\n    \"\"\"Multi-receptive field module in HiFiGAN.\"\"\"\n\n    def __init__(\n        self,\n        channels=512,\n        resblock_kernel_sizes=(3, 7, 11),\n        resblock_dilations=[(1, 3, 5), (1, 3, 5), (1, 3, 5)],\n        groups=1,\n        bias=True,\n        use_additional_convs=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.1},\n    ):\n        assert len(resblock_kernel_sizes) == len(resblock_dilations)\n        super().__init__()\n        self.num_blocks = len(resblock_kernel_sizes)\n\n        self.blocks = nn.ModuleList()\n        for i in range(self.num_blocks):\n            self.blocks += [\n                HiFiGANResidualBlock(\n                    kernel_size=resblock_kernel_sizes[i],\n                    channels=channels,\n                    dilations=resblock_dilations[i],\n                    groups=groups,\n                    bias=bias,\n                    use_additional_convs=use_additional_convs,\n                    nonlinear_activation=nonlinear_activation,\n                    nonlinear_activation_params=nonlinear_activation_params,\n                )\n            ]\n    \n    def forward(self, c):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            c (Tensor): Input tensor (B, channels, T).\n\n        Returns:\n            Tensor: Output tensor (B, channels, T).\n\n        \"\"\"\n        cs = 0.0  # initialize\n        for i in range(self.num_blocks):\n            cs += self.blocks[i](c)\n        c = cs / self.num_blocks\n\n        return c\n    \n    def inference(self, c):\n        cs = 0.0  # initialize\n        for i in range(self.num_blocks):\n            cs += self.blocks[i].inference(c)\n        c = cs / self.num_blocks\n        \n        return c", "        \n\nclass MultiGroupConv1d(HiFiGANResidualBlock):\n    \"\"\"Multi-group convolution module.\"\"\"\n\n    def __init__(\n        self,\n        channels=512,\n        resblock_kernel_sizes=(3),\n        resblock_dilations=[(1, 3, 5)],\n        groups=3,\n        bias=True,\n        use_additional_convs=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.1},\n    ):\n        assert len(resblock_kernel_sizes) == len(resblock_dilations) == 1\n        super(MultiGroupConv1d, self).__init__(\n            kernel_size=resblock_kernel_sizes[0],\n            channels=channels*groups,\n            dilations=resblock_dilations[0],\n            groups=groups,\n            bias=bias,\n            use_additional_convs=use_additional_convs,\n            nonlinear_activation=nonlinear_activation,\n            nonlinear_activation_params=nonlinear_activation_params,\n        )\n        self.groups = groups\n        self.conv_out = Conv1d1x1(\n            in_channels=channels*groups, \n            out_channels=channels,\n            bias=False,\n        )\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, channels, T).\n\n        Returns:\n            Tensor: Output tensor (B, channels, T).\n\n        \"\"\"\n        x = x.repeat(1, self.groups, 1) # (B, n*C, T)\n        for idx in range(self.num_layer):\n            xt = self.convs1[idx](self.activation(x))\n            if self.use_additional_convs:\n                xt = self.convs2[idx](self.activation(xt))\n            x = xt + x\n        x = self.conv_out(x) # (B, C, T)\n        return x\n    \n    def inference(self, x):\n        x = x.repeat(1, self.groups, 1) # (B, n*C, T)\n        for idx in range(self.num_layer):\n            xt = self.convs1[idx].inference(self.activation(x))\n            if self.use_additional_convs:\n                xt = self.convs2[idx].inference(self.activation(xt))\n            x = xt + x\n        x = self.conv_out(x) # (B, C, T)\n        return x"]}
{"filename": "models/vocoder/modules/discriminator.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n# Reference (https://github.com/jik876/hifi-gan)\n# Reference (https://github.com/chomeyama/SiFiGAN)\n\n\"\"\"GAN-based Discriminators\"\"\"\n\nimport copy\nimport logging\n", "import logging\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchaudio.functional import spectrogram\n\n\nclass HiFiGANPeriodDiscriminator(nn.Module):\n    \"\"\"HiFiGAN period discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        period=3,\n        kernel_sizes=[5, 3],\n        channels=32,\n        downsample_scales=[3, 3, 3, 3, 1],\n        max_downsample_channels=1024,\n        bias=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.1},\n        use_weight_norm=True,\n        use_spectral_norm=False,\n    ):\n        \"\"\"Initialize HiFiGANPeriodDiscriminator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            period (int): Period.\n            kernel_sizes (list): Kernel sizes of initial conv layers and the final conv layer.\n            channels (int): Number of initial channels.\n            downsample_scales (list): List of downsampling scales.\n            max_downsample_channels (int): Number of maximum downsampling channels.\n            use_additional_convs (bool): Whether to use additional conv layers in residual blocks.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            use_weight_norm (bool): Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n            use_spectral_norm (bool): Whether to use spectral norm.\n                If set to true, it will be applied to all of the conv layers.\n\n        \"\"\"\n        super().__init__()\n        assert len(kernel_sizes) == 2\n        assert kernel_sizes[0] % 2 == 1, \"Kernel size must be odd number.\"\n        assert kernel_sizes[1] % 2 == 1, \"Kernel size must be odd number.\"\n\n        self.period = period\n        self.convs = nn.ModuleList()\n        in_chs = in_channels\n        out_chs = channels\n        for downsample_scale in downsample_scales:\n            self.convs += [\n                torch.nn.Sequential(\n                    torch.nn.Conv2d(\n                        in_chs,\n                        out_chs,\n                        (kernel_sizes[0], 1),\n                        (downsample_scale, 1),\n                        padding=((kernel_sizes[0] - 1) // 2, 0),\n                    ),\n                    getattr(torch.nn, nonlinear_activation)(\n                        **nonlinear_activation_params\n                    ),\n                )\n            ]\n            in_chs = out_chs\n            # NOTE(kan-bayashi): Use downsample_scale + 1?\n            out_chs = min(out_chs * 4, max_downsample_channels)\n        self.output_conv = torch.nn.Conv2d(\n            out_chs,\n            out_channels,\n            (kernel_sizes[1] - 1, 1),\n            1,\n            padding=((kernel_sizes[1] - 1) // 2, 0),\n        )\n\n        if use_weight_norm and use_spectral_norm:\n            raise ValueError(\"Either use use_weight_norm or use_spectral_norm.\")\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n        # apply spectral norm\n        if use_spectral_norm:\n            self.apply_spectral_norm()\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            c (Tensor): Input tensor (B, in_channels, T).\n\n        Returns:\n            list: List of each layer's tensors.\n\n        \"\"\"\n        # transform 1d to 2d -> (B, C, T/P, P)\n        b, c, t = x.shape\n        if t % self.period != 0:\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t += n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        # forward conv\n        outs = []\n        for layer in self.convs:\n            x = layer(x)\n            outs += [x]\n        x = self.output_conv(x)\n        x = torch.flatten(x, 1, -1)\n        outs += [x]\n\n        return outs\n\n    def apply_weight_norm(self):\n        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f\"Weight norm is applied to {m}.\")\n\n        self.apply(_apply_weight_norm)\n\n    def apply_spectral_norm(self):\n        \"\"\"Apply spectral normalization module from all of the layers.\"\"\"\n\n        def _apply_spectral_norm(m):\n            if isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.spectral_norm(m)\n                logging.debug(f\"Spectral norm is applied to {m}.\")\n\n        self.apply(_apply_spectral_norm)", "\nclass HiFiGANPeriodDiscriminator(nn.Module):\n    \"\"\"HiFiGAN period discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        period=3,\n        kernel_sizes=[5, 3],\n        channels=32,\n        downsample_scales=[3, 3, 3, 3, 1],\n        max_downsample_channels=1024,\n        bias=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.1},\n        use_weight_norm=True,\n        use_spectral_norm=False,\n    ):\n        \"\"\"Initialize HiFiGANPeriodDiscriminator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            period (int): Period.\n            kernel_sizes (list): Kernel sizes of initial conv layers and the final conv layer.\n            channels (int): Number of initial channels.\n            downsample_scales (list): List of downsampling scales.\n            max_downsample_channels (int): Number of maximum downsampling channels.\n            use_additional_convs (bool): Whether to use additional conv layers in residual blocks.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            use_weight_norm (bool): Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n            use_spectral_norm (bool): Whether to use spectral norm.\n                If set to true, it will be applied to all of the conv layers.\n\n        \"\"\"\n        super().__init__()\n        assert len(kernel_sizes) == 2\n        assert kernel_sizes[0] % 2 == 1, \"Kernel size must be odd number.\"\n        assert kernel_sizes[1] % 2 == 1, \"Kernel size must be odd number.\"\n\n        self.period = period\n        self.convs = nn.ModuleList()\n        in_chs = in_channels\n        out_chs = channels\n        for downsample_scale in downsample_scales:\n            self.convs += [\n                torch.nn.Sequential(\n                    torch.nn.Conv2d(\n                        in_chs,\n                        out_chs,\n                        (kernel_sizes[0], 1),\n                        (downsample_scale, 1),\n                        padding=((kernel_sizes[0] - 1) // 2, 0),\n                    ),\n                    getattr(torch.nn, nonlinear_activation)(\n                        **nonlinear_activation_params\n                    ),\n                )\n            ]\n            in_chs = out_chs\n            # NOTE(kan-bayashi): Use downsample_scale + 1?\n            out_chs = min(out_chs * 4, max_downsample_channels)\n        self.output_conv = torch.nn.Conv2d(\n            out_chs,\n            out_channels,\n            (kernel_sizes[1] - 1, 1),\n            1,\n            padding=((kernel_sizes[1] - 1) // 2, 0),\n        )\n\n        if use_weight_norm and use_spectral_norm:\n            raise ValueError(\"Either use use_weight_norm or use_spectral_norm.\")\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n        # apply spectral norm\n        if use_spectral_norm:\n            self.apply_spectral_norm()\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            c (Tensor): Input tensor (B, in_channels, T).\n\n        Returns:\n            list: List of each layer's tensors.\n\n        \"\"\"\n        # transform 1d to 2d -> (B, C, T/P, P)\n        b, c, t = x.shape\n        if t % self.period != 0:\n            n_pad = self.period - (t % self.period)\n            x = F.pad(x, (0, n_pad), \"reflect\")\n            t += n_pad\n        x = x.view(b, c, t // self.period, self.period)\n\n        # forward conv\n        outs = []\n        for layer in self.convs:\n            x = layer(x)\n            outs += [x]\n        x = self.output_conv(x)\n        x = torch.flatten(x, 1, -1)\n        outs += [x]\n\n        return outs\n\n    def apply_weight_norm(self):\n        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f\"Weight norm is applied to {m}.\")\n\n        self.apply(_apply_weight_norm)\n\n    def apply_spectral_norm(self):\n        \"\"\"Apply spectral normalization module from all of the layers.\"\"\"\n\n        def _apply_spectral_norm(m):\n            if isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.spectral_norm(m)\n                logging.debug(f\"Spectral norm is applied to {m}.\")\n\n        self.apply(_apply_spectral_norm)", "\n\nclass HiFiGANMultiPeriodDiscriminator(nn.Module):\n    \"\"\"HiFiGAN multi-period discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        periods=[2, 3, 5, 7, 11],\n        discriminator_params={\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [5, 3],\n            \"channels\": 32,\n            \"downsample_scales\": [3, 3, 3, 3, 1],\n            \"max_downsample_channels\": 1024,\n            \"bias\": True,\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n            \"use_weight_norm\": True,\n            \"use_spectral_norm\": False,\n        },\n    ):\n        \"\"\"Initialize HiFiGANMultiPeriodDiscriminator module.\n\n        Args:\n            periods (list): List of periods.\n            discriminator_params (dict): Parameters for hifi-gan period discriminator module.\n                The period parameter will be overwritten.\n\n        \"\"\"\n        super().__init__()\n        self.discriminators = nn.ModuleList()\n        for period in periods:\n            params = copy.deepcopy(discriminator_params)\n            params[\"period\"] = period\n            self.discriminators += [HiFiGANPeriodDiscriminator(**params)]\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n\n        Returns:\n            List: List of list of each discriminator outputs, which consists of each layer output tensors.\n\n        \"\"\"\n        outs = []\n        for f in self.discriminators:\n            outs += [f(x)]\n\n        return outs", "\n\nclass HiFiGANScaleDiscriminator(nn.Module):\n    \"\"\"HiFi-GAN scale discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        kernel_sizes=[15, 41, 5, 3],\n        channels=128,\n        max_downsample_channels=1024,\n        max_groups=16,\n        bias=True,\n        downsample_scales=[2, 2, 4, 4, 1],\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.1},\n        use_weight_norm=True,\n        use_spectral_norm=False,\n    ):\n        \"\"\"Initilize HiFiGAN scale discriminator module.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_sizes (list): List of four kernel sizes. The first will be used for the first conv layer,\n                and the second is for downsampling part, and the remaining two are for output layers.\n            channels (int): Initial number of channels for conv layer.\n            max_downsample_channels (int): Maximum number of channels for downsampling layers.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            downsample_scales (list): List of downsampling scales.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            use_weight_norm (bool): Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n            use_spectral_norm (bool): Whether to use spectral norm.\n                If set to true, it will be applied to all of the conv layers.\n\n        \"\"\"\n        super().__init__()\n        self.layers = nn.ModuleList()\n\n        # check kernel size is valid\n        assert len(kernel_sizes) == 4\n        for ks in kernel_sizes:\n            assert ks % 2 == 1\n\n        # add first layer\n        self.layers += [\n            torch.nn.Sequential(\n                torch.nn.Conv1d(\n                    in_channels,\n                    channels,\n                    # NOTE(kan-bayashi): Use always the same kernel size\n                    kernel_sizes[0],\n                    bias=bias,\n                    padding=(kernel_sizes[0] - 1) // 2,\n                ),\n                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n            )\n        ]\n\n        # add downsample layers\n        in_chs = channels\n        out_chs = channels\n        # NOTE(kan-bayashi): Remove hard coding?\n        groups = 4\n        for downsample_scale in downsample_scales:\n            self.layers += [\n                torch.nn.Sequential(\n                    torch.nn.Conv1d(\n                        in_chs,\n                        out_chs,\n                        kernel_size=kernel_sizes[1],\n                        stride=downsample_scale,\n                        padding=(kernel_sizes[1] - 1) // 2,\n                        groups=groups,\n                        bias=bias,\n                    ),\n                    getattr(torch.nn, nonlinear_activation)(\n                        **nonlinear_activation_params\n                    ),\n                )\n            ]\n            in_chs = out_chs\n            # NOTE(kan-bayashi): Remove hard coding?\n            out_chs = min(in_chs * 2, max_downsample_channels)\n            # NOTE(kan-bayashi): Remove hard coding?\n            groups = min(groups * 4, max_groups)\n\n        # add final layers\n        out_chs = min(in_chs * 2, max_downsample_channels)\n        self.layers += [\n            torch.nn.Sequential(\n                torch.nn.Conv1d(\n                    in_chs,\n                    out_chs,\n                    kernel_size=kernel_sizes[2],\n                    stride=1,\n                    padding=(kernel_sizes[2] - 1) // 2,\n                    bias=bias,\n                ),\n                getattr(torch.nn, nonlinear_activation)(**nonlinear_activation_params),\n            )\n        ]\n        self.layers += [\n            torch.nn.Conv1d(\n                out_chs,\n                out_channels,\n                kernel_size=kernel_sizes[3],\n                stride=1,\n                padding=(kernel_sizes[3] - 1) // 2,\n                bias=bias,\n            ),\n        ]\n\n        if use_weight_norm and use_spectral_norm:\n            raise ValueError(\"Either use use_weight_norm or use_spectral_norm.\")\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n        # apply spectral norm\n        if use_spectral_norm:\n            self.apply_spectral_norm()\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n\n        Returns:\n            List: List of output tensors of each layer.\n\n        \"\"\"\n        outs = []\n        for f in self.layers:\n            x = f(x)\n            outs += [x]\n\n        return outs\n\n    def apply_weight_norm(self):\n        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f\"Weight norm is applied to {m}.\")\n\n        self.apply(_apply_weight_norm)\n\n    def apply_spectral_norm(self):\n        \"\"\"Apply spectral normalization module from all of the layers.\"\"\"\n\n        def _apply_spectral_norm(m):\n            if isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.spectral_norm(m)\n                logging.debug(f\"Spectral norm is applied to {m}.\")\n\n        self.apply(_apply_spectral_norm)", "\n\nclass HiFiGANMultiScaleDiscriminator(nn.Module):\n    \"\"\"HiFi-GAN multi-scale discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        scales=3,\n        downsample_pooling=\"AvgPool1d\",\n        # follow the official implementation setting\n        downsample_pooling_params={\n            \"kernel_size\": 4,\n            \"stride\": 2,\n            \"padding\": 2,\n        },\n        discriminator_params={\n            \"in_channels\": 1,\n            \"out_channels\": 1,\n            \"kernel_sizes\": [15, 41, 5, 3],\n            \"channels\": 128,\n            \"max_downsample_channels\": 1024,\n            \"max_groups\": 16,\n            \"bias\": True,\n            \"downsample_scales\": [2, 2, 4, 4, 1],\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.1},\n        },\n        follow_official_norm=False,\n    ):\n        \"\"\"Initilize HiFiGAN multi-scale discriminator module.\n\n        Args:\n            scales (int): Number of multi-scales.\n            downsample_pooling (str): Pooling module name for downsampling of the inputs.\n            downsample_pooling_params (dict): Parameters for the above pooling module.\n            discriminator_params (dict): Parameters for hifi-gan scale discriminator module.\n            follow_official_norm (bool): Whether to follow the norm setting of the official\n                implementaion. The first discriminator uses spectral norm and the other\n                discriminators use weight norm.\n\n        \"\"\"\n        super().__init__()\n        self.discriminators = nn.ModuleList()\n\n        # add discriminators\n        for i in range(scales):\n            params = copy.deepcopy(discriminator_params)\n            if follow_official_norm:\n                if i == 0:\n                    params[\"use_weight_norm\"] = False\n                    params[\"use_spectral_norm\"] = True\n                else:\n                    params[\"use_weight_norm\"] = True\n                    params[\"use_spectral_norm\"] = False\n            self.discriminators += [HiFiGANScaleDiscriminator(**params)]\n        self.pooling = getattr(torch.nn, downsample_pooling)(\n            **downsample_pooling_params\n        )\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n\n        Returns:\n            List: List of list of each discriminator outputs, which consists of each layer output tensors.\n\n        \"\"\"\n        outs = []\n        for f in self.discriminators:\n            outs += [f(x)]\n            x = self.pooling(x)\n\n        return outs", "\n\nclass UnivNetSpectralDiscriminator(nn.Module):\n    \"\"\"UnivNet spectral discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        fft_size,\n        hop_size,\n        win_length,\n        window=\"hann_window\",\n        kernel_sizes=[(3, 9), (3, 9), (3, 9), (3, 9), (3, 3), (3, 3)],\n        strides=[(1, 1), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1)],\n        channels=32,\n        bias=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.2},\n        use_weight_norm=True,\n    ):\n        \"\"\"Initilize HiFiGAN scale discriminator module.\n        Args:\n            fft_size (list): FFT size.\n            hop_size (int): Hop size.\n            win_length (int): Window length.\n            window (stt): Name of window function.\n            kernel_sizes (list): List of kernel sizes in down-sampling CNNs.\n            strides (list): List of stride sizes in down-sampling CNNs.\n            channels (int): Number of channels for conv layer.\n            bias (bool): Whether to add bias parameter in convolution layers.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n            use_weight_norm (bool): Whether to use weight norm.\n                If set to true, it will be applied to all of the conv layers.\n        \"\"\"\n        super().__init__()\n\n        self.fft_size = fft_size\n        self.hop_size = hop_size\n        self.win_length = win_length\n        self.register_buffer(\"window\", getattr(torch, window)(win_length))\n\n        self.layers = nn.ModuleList()\n\n        # check kernel size is valid\n        assert len(kernel_sizes) == len(strides)\n\n        # add first layer\n        self.layers += [\n            nn.Sequential(\n                nn.Conv2d(\n                    1,\n                    channels,\n                    kernel_sizes[0],\n                    stride=strides[0],\n                    bias=bias,\n                ),\n                getattr(nn, nonlinear_activation)(**nonlinear_activation_params),\n            )\n        ]\n\n        for i in range(1, len(kernel_sizes) - 2):\n            self.layers += [\n                nn.Sequential(\n                    nn.Conv2d(\n                        channels,\n                        channels,\n                        kernel_size=kernel_sizes[i],\n                        stride=strides[i],\n                        bias=bias,\n                    ),\n                    getattr(nn, nonlinear_activation)(**nonlinear_activation_params),\n                )\n            ]\n\n        # add final layers\n        self.layers += [\n            nn.Sequential(\n                nn.Conv2d(\n                    channels,\n                    channels,\n                    kernel_size=kernel_sizes[-2],\n                    stride=strides[-2],\n                    bias=bias,\n                ),\n                getattr(nn, nonlinear_activation)(**nonlinear_activation_params),\n            )\n        ]\n        self.layers += [\n            nn.Conv2d(\n                channels,\n                1,\n                kernel_size=kernel_sizes[-1],\n                stride=strides[-1],\n                bias=bias,\n            )\n        ]\n\n        # apply weight norm\n        if use_weight_norm:\n            self.apply_weight_norm()\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n        Returns:\n            List: List of output tensors of each layer.\n        \"\"\"\n        x = spectrogram(\n            x,\n            pad=self.win_length // 2,\n            window=self.window,\n            n_fft=self.fft_size,\n            hop_length=self.hop_size,\n            win_length=self.win_length,\n            power=1.0,\n            normalized=False,\n        ).transpose(-1, -2)\n\n        for f in self.layers:\n            x = f(x)\n\n        return x\n\n    def apply_weight_norm(self):\n        \"\"\"Apply weight normalization module from all of the layers.\"\"\"\n\n        def _apply_weight_norm(m):\n            if isinstance(m, torch.nn.Conv2d):\n                torch.nn.utils.weight_norm(m)\n                logging.debug(f\"Weight norm is applied to {m}.\")\n\n        self.apply(_apply_weight_norm)", "\n\nclass UnivNetMultiResolutionSpectralDiscriminator(nn.Module):\n    \"\"\"UnivNet multi-resolution spectral discriminator module.\"\"\"\n\n    def __init__(\n        self,\n        fft_sizes=[1024, 2048, 512],\n        hop_sizes=[120, 240, 50],\n        win_lengths=[600, 1200, 240],\n        window=\"hann_window\",\n        discriminator_params={\n            \"channels\": 32,\n            \"kernel_sizes\": [(3, 9), (3, 9), (3, 9), (3, 9), (3, 3), (3, 3)],\n            \"strides\": [(1, 1), (1, 2), (1, 2), (1, 2), (1, 1), (1, 1)],\n            \"bias\": True,\n            \"nonlinear_activation\": \"LeakyReLU\",\n            \"nonlinear_activation_params\": {\"negative_slope\": 0.2},\n        },\n    ):\n        \"\"\"Initilize UnivNetMultiResolutionSpectralDiscriminator module.\n        Args:\n            fft_sizes (list): FFT sizes for each spectral discriminator.\n            hop_sizes (list): Hop sizes for each spectral discriminator.\n            win_lengths (list): Window lengths for each spectral discriminator.\n            window (stt): Name of window function.\n            discriminator_params (dict): Parameters for univ-net spectral discriminator module.\n        \"\"\"\n        super().__init__()\n        assert len(fft_sizes) == len(hop_sizes) == len(win_lengths)\n        self.discriminators = nn.ModuleList()\n\n        # add discriminators\n        for i in range(len(fft_sizes)):\n            params = copy.deepcopy(discriminator_params)\n            self.discriminators += [\n                UnivNetSpectralDiscriminator(\n                    fft_size=fft_sizes[i],\n                    hop_size=hop_sizes[i],\n                    win_length=win_lengths[i],\n                    window=window,\n                    **params,\n                )\n            ]\n\n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n        Args:\n            x (Tensor): Input noise signal (B, 1, T).\n        Returns:\n            List: List of list of each discriminator outputs, which consists of each layer output tensors.\n        \"\"\"\n        outs = []\n        for f in self.discriminators:\n            out = f(x)\n            outs.append(out)\n\n        return outs"]}
{"filename": "models/vocoder/modules/residual_block.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n# Reference (https://github.com/r9y9/wavenet_vocoder)\n# Reference (https://github.com/jik876/hifi-gan)\n\n\"\"\"Residual block modules.\"\"\"\n\nimport math\n\nimport torch", "\nimport torch\nimport torch.nn as nn\nfrom layers.conv_layer import CausalConv1d, Conv1d1x1   \n\n\nclass HiFiGANResidualBlock(nn.Module):\n    \"\"\"Causal Residual block module in HiFiGAN.\"\"\"\n\n    def __init__(\n        self,\n        kernel_size=3,\n        channels=512,\n        dilations=(1, 3, 5),\n        groups=1,\n        bias=True,\n        use_additional_convs=True,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.1},\n    ):\n        \"\"\"Initialize CausalResidualBlock module.\n\n        Args:\n            kernel_size (int): Kernel size of dilation convolution layer.\n            channels (int): Number of channels for convolution layer.\n            dilations (List[int]): List of dilation factors.\n            use_additional_convs (bool): Whether to use additional convolution layers.\n            groups (int): The group number of conv1d (default: 1)\n            bias (bool): Whether to add bias parameter in convolution layers.\n            nonlinear_activation (str): Activation function module name.\n            nonlinear_activation_params (dict): Hyperparameters for activation function.\n\n        \"\"\"\n        super().__init__()\n        self.use_additional_convs = use_additional_convs\n        self.convs1 = nn.ModuleList()\n        if use_additional_convs:\n            self.convs2 = nn.ModuleList()\n        assert kernel_size % 2 == 1, \"Kernel size must be odd number.\"\n        self.activation = getattr(nn, nonlinear_activation)(**nonlinear_activation_params)\n        for dilation in dilations:\n            self.convs1 += [\n                CausalConv1d(\n                    in_channels=channels,\n                    out_channels=channels,\n                    kernel_size=kernel_size,\n                    stride=1,\n                    dilation=dilation,\n                    groups=groups,\n                    bias=bias,\n                )\n            ]\n            if use_additional_convs:\n                self.convs2 += [\n                    CausalConv1d(\n                        in_channels=channels,\n                        out_channels=channels,\n                        kernel_size=kernel_size,\n                        stride=1,\n                        dilation=1,\n                        groups=groups,\n                        bias=bias,\n                    )\n                ]\n        self.num_layer = len(self.convs1)\n    \n    def forward(self, x):\n        \"\"\"Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor (B, channels, T).\n\n        Returns:\n            Tensor: Output tensor (B, channels, T).\n\n        \"\"\"\n        for idx in range(self.num_layer):\n            xt = self.convs1[idx](self.activation(x))\n            if self.use_additional_convs:\n                xt = self.convs2[idx](self.activation(xt))\n            x = xt + x\n        return x\n    \n    def inference(self, x):\n        for idx in range(self.num_layer):\n            xt = self.convs1[idx].inference(self.activation(x))\n            if self.use_additional_convs:\n                xt = self.convs2[idx].inference(self.activation(xt))\n            x = xt + x\n        return x"]}
{"filename": "models/autoencoder/AudioDec.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"AudioDec model.\"\"\"\n\nimport torch\nimport inspect\n\nfrom layers.conv_layer import CausalConv1d, CausalConvTranspose1d\nfrom models.autoencoder.modules.encoder import Encoder", "from layers.conv_layer import CausalConv1d, CausalConvTranspose1d\nfrom models.autoencoder.modules.encoder import Encoder\nfrom models.autoencoder.modules.decoder import Decoder\nfrom models.autoencoder.modules.projector import Projector\nfrom models.autoencoder.modules.quantizer import Quantizer\nfrom models.utils import check_mode\n\n\n### GENERATOR ###\nclass Generator(torch.nn.Module):\n    \"\"\"AudioDec generator.\"\"\"\n\n    def __init__(\n        self,\n        input_channels=1,\n        output_channels=1,\n        encode_channels=32,\n        decode_channels=32,\n        code_dim=64,\n        codebook_num=8,\n        codebook_size=1024,\n        bias=True,\n        enc_ratios=(2, 4, 8, 16),\n        dec_ratios=(16, 8, 4, 2),\n        enc_strides=(3, 4, 5, 5),\n        dec_strides=(5, 5, 4, 3),\n        mode='causal',\n        codec='audiodec',\n        projector='conv1d',\n        quantier='residual_vq',\n    ):\n        super().__init__()\n        if codec == 'audiodec':\n            encoder = Encoder\n            decoder = Decoder\n        else:\n            raise NotImplementedError(f\"Codec ({codec}) is not supported!\")\n        self.mode = mode\n        self.input_channels = input_channels\n\n        self.encoder = encoder(\n            input_channels=input_channels,\n            encode_channels=encode_channels,\n            channel_ratios=enc_ratios,\n            strides=enc_strides,\n            kernel_size=7,\n            bias=bias,\n            mode=self.mode,\n        )\n\n        self.decoder = decoder(\n            code_dim=code_dim,\n            output_channels=output_channels,\n            decode_channels=decode_channels,\n            channel_ratios=dec_ratios,\n            strides=dec_strides,\n            kernel_size=7,\n            bias=bias,\n            mode=self.mode,\n        )\n\n        self.projector = Projector(\n            input_channels=self.encoder.out_channels,\n            code_dim=code_dim,\n            kernel_size=3,\n            stride=1,\n            bias=False,\n            mode=self.mode,\n            model=projector,\n        )\n\n        self.quantizer = Quantizer(\n            code_dim=code_dim,\n            codebook_num=codebook_num,\n            codebook_size=codebook_size,\n            model=quantier,\n        )\n\n    def forward(self, x):\n        (batch, channel, length) = x.size()\n        if channel != self.input_channels: \n            x = x.reshape(-1, self.input_channels, length) # (B, C, T) -> (B', C', T)\n        x = self.encoder(x)\n        z = self.projector(x)\n        zq, vqloss, perplexity = self.quantizer(z)\n        y = self.decoder(zq)\n        return y, zq, z, vqloss, perplexity", "### GENERATOR ###\nclass Generator(torch.nn.Module):\n    \"\"\"AudioDec generator.\"\"\"\n\n    def __init__(\n        self,\n        input_channels=1,\n        output_channels=1,\n        encode_channels=32,\n        decode_channels=32,\n        code_dim=64,\n        codebook_num=8,\n        codebook_size=1024,\n        bias=True,\n        enc_ratios=(2, 4, 8, 16),\n        dec_ratios=(16, 8, 4, 2),\n        enc_strides=(3, 4, 5, 5),\n        dec_strides=(5, 5, 4, 3),\n        mode='causal',\n        codec='audiodec',\n        projector='conv1d',\n        quantier='residual_vq',\n    ):\n        super().__init__()\n        if codec == 'audiodec':\n            encoder = Encoder\n            decoder = Decoder\n        else:\n            raise NotImplementedError(f\"Codec ({codec}) is not supported!\")\n        self.mode = mode\n        self.input_channels = input_channels\n\n        self.encoder = encoder(\n            input_channels=input_channels,\n            encode_channels=encode_channels,\n            channel_ratios=enc_ratios,\n            strides=enc_strides,\n            kernel_size=7,\n            bias=bias,\n            mode=self.mode,\n        )\n\n        self.decoder = decoder(\n            code_dim=code_dim,\n            output_channels=output_channels,\n            decode_channels=decode_channels,\n            channel_ratios=dec_ratios,\n            strides=dec_strides,\n            kernel_size=7,\n            bias=bias,\n            mode=self.mode,\n        )\n\n        self.projector = Projector(\n            input_channels=self.encoder.out_channels,\n            code_dim=code_dim,\n            kernel_size=3,\n            stride=1,\n            bias=False,\n            mode=self.mode,\n            model=projector,\n        )\n\n        self.quantizer = Quantizer(\n            code_dim=code_dim,\n            codebook_num=codebook_num,\n            codebook_size=codebook_size,\n            model=quantier,\n        )\n\n    def forward(self, x):\n        (batch, channel, length) = x.size()\n        if channel != self.input_channels: \n            x = x.reshape(-1, self.input_channels, length) # (B, C, T) -> (B', C', T)\n        x = self.encoder(x)\n        z = self.projector(x)\n        zq, vqloss, perplexity = self.quantizer(z)\n        y = self.decoder(zq)\n        return y, zq, z, vqloss, perplexity", "\n\n# STREAMING\nclass StreamGenerator(Generator):\n    \"\"\"AudioDec streaming generator.\"\"\"\n\n    def __init__(\n        self,\n        input_channels=1,\n        output_channels=1,\n        encode_channels=32,\n        decode_channels=32,\n        code_dim=64,\n        codebook_num=8,\n        codebook_size=1024,\n        bias=True,\n        enc_ratios=(2, 4, 8, 16),\n        dec_ratios=(16, 8, 4, 2),\n        enc_strides=(3, 4, 5, 5),\n        dec_strides=(5, 5, 4, 3),\n        mode='causal',\n        codec='audiodec',\n        projector='conv1d',\n        quantier='residual_vq',\n    ):\n        super(StreamGenerator, self).__init__(\n            input_channels=input_channels,\n            output_channels=output_channels,\n            encode_channels=encode_channels,\n            decode_channels=decode_channels,\n            code_dim=code_dim,\n            codebook_num=codebook_num,\n            codebook_size=codebook_size,\n            bias=bias,\n            enc_ratios=enc_ratios,\n            dec_ratios=dec_ratios,\n            enc_strides=enc_strides,\n            dec_strides=dec_strides,\n            mode=mode,\n            codec=codec,\n            projector=projector,\n            quantier=quantier,\n        )\n        check_mode(mode, \"AudioDec Streamer\")\n        self.reset_buffer()\n\n\n    def initial_encoder(self, receptive_length, device):\n        self.quantizer.initial()\n        z = self.encode(torch.zeros(1, self.input_channels, receptive_length).to(device))\n        idx = self.quantize(z)\n        zq = self.lookup(idx)\n        return zq\n\n\n    def initial_decoder(self, zq):\n        self.decode(zq)\n\n\n    def encode(self, x):\n        (batch, channel, length) = x.size()\n        if channel != self.input_channels: \n            x = x.reshape(-1, self.input_channels, length) # (B, C, T) -> (B', C', T)\n        x = self.encoder.encode(x)\n        z = self.projector.encode(x)\n        return z\n\n\n    def quantize(self, z):\n        zq, idx = self.quantizer.encode(z)\n        return idx\n\n\n    def lookup(self, idx):\n        return self.quantizer.decode(idx)\n\n\n    def decode(self, zq):\n        return self.decoder.decode(zq.transpose(2, 1))\n\n\n    def reset_buffer(self):\n        \"\"\"Apply weight normalization module from all layers.\"\"\"\n\n        def _reset_buffer(m):\n            if isinstance(m, CausalConv1d) or isinstance(m, CausalConvTranspose1d):\n                m.reset_buffer()\n        self.apply(_reset_buffer)", ""]}
{"filename": "models/autoencoder/modules/decoder.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://ieeexplore.ieee.org/document/9625818)", "#\n# Reference (https://ieeexplore.ieee.org/document/9625818)\n\n\"\"\"Decoder modules.\"\"\"\n\nimport torch\nimport inspect\n\nfrom layers.conv_layer import NonCausalConv1d, NonCausalConvTranspose1d\nfrom layers.conv_layer import CausalConv1d, CausalConvTranspose1d", "from layers.conv_layer import NonCausalConv1d, NonCausalConvTranspose1d\nfrom layers.conv_layer import CausalConv1d, CausalConvTranspose1d\nfrom models.autoencoder.modules.residual_unit import NonCausalResidualUnit\nfrom models.autoencoder.modules.residual_unit import CausalResidualUnit\nfrom models.utils import check_mode\n\n\nclass DecoderBlock(torch.nn.Module):\n    \"\"\" Decoder block (upsampling) \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        dilations=(1, 3, 9),\n        bias=True,\n        mode='causal',\n    ):\n        super().__init__()\n        self.mode = mode\n        if self.mode == 'noncausal':\n            ResidualUnit = NonCausalResidualUnit\n            ConvTranspose1d = NonCausalConvTranspose1d\n        elif self.mode == 'causal':\n            ResidualUnit = CausalResidualUnit\n            ConvTranspose1d = CausalConvTranspose1d\n        else:\n            raise NotImplementedError(f\"Mode ({self.mode}) is not supported!\")\n\n        self.conv = ConvTranspose1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(2 * stride),\n            stride=stride,\n            bias=bias,\n        )\n\n        self.res_units = torch.nn.ModuleList()\n        for idx, dilation in enumerate(dilations):\n            self.res_units += [\n                ResidualUnit(out_channels, out_channels, dilation=dilation)]\n        self.num_res = len(self.res_units)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        for idx in range(self.num_res):\n            x = self.res_units[idx](x)\n        return x\n    \n    def inference(self, x):\n        check_mode(self.mode, inspect.stack()[0][3])\n        x = self.conv.inference(x)\n        for idx in range(self.num_res):\n            x = self.res_units[idx].inference(x)\n        return x", "\n\nclass Decoder(torch.nn.Module):\n    def __init__(self,\n        code_dim, \n        output_channels,\n        decode_channels,\n        channel_ratios=(16, 8, 4, 2),\n        strides=(5, 5, 4, 3),\n        kernel_size=7,\n        bias=True,\n        mode='causal',\n    ):\n        super().__init__()\n        assert len(channel_ratios) == len(strides)\n        self.mode = mode\n        if self.mode == 'noncausal':\n            Conv1d = NonCausalConv1d\n        elif self.mode == 'causal':\n            Conv1d = CausalConv1d\n        else:\n            raise NotImplementedError(f\"Mode ({self.mode}) is not supported!\")\n\n        self.conv1 = Conv1d(\n            in_channels=code_dim, \n            out_channels=(decode_channels * channel_ratios[0]), \n            kernel_size=kernel_size, \n            stride=1, \n            bias=False)\n\n        self.conv_blocks = torch.nn.ModuleList()\n        for idx, stride in enumerate(strides):\n            in_channels = decode_channels * channel_ratios[idx]\n            if idx < (len(channel_ratios)-1):\n                out_channels = decode_channels * channel_ratios[idx+1]\n            else:\n                out_channels = decode_channels\n            self.conv_blocks += [\n                DecoderBlock(in_channels, out_channels, stride, bias=bias, mode=self.mode)]\n        self.num_blocks = len(self.conv_blocks)\n\n        self.conv2 = Conv1d(out_channels, output_channels, kernel_size, 1, bias=False)\n\n    def forward(self, z):\n        x = self.conv1(z)\n        for i in range(self.num_blocks):\n            x = self.conv_blocks[i](x)\n        x = self.conv2(x)\n        return x\n    \n    def decode(self, z):\n        check_mode(self.mode, inspect.stack()[0][3])\n        x = self.conv1.inference(z)\n        for i in range(self.num_blocks):\n            x = self.conv_blocks[i].inference(x)\n        x = self.conv2.inference(x)\n        return x"]}
{"filename": "models/autoencoder/modules/residual_unit.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://ieeexplore.ieee.org/document/9625818)", "#\n# Reference (https://ieeexplore.ieee.org/document/9625818)\n\n\"\"\"Residual Units.\"\"\"\n\nimport torch\nimport torch.nn as nn\n\nfrom layers.conv_layer import Conv1d1x1, NonCausalConv1d, CausalConv1d\n", "from layers.conv_layer import Conv1d1x1, NonCausalConv1d, CausalConv1d\n\n\nclass NonCausalResidualUnit(nn.Module):\n    def __init__(\n        self, \n        in_channels, \n        out_channels, \n        kernel_size=7,\n        dilation=1,\n        bias=False,\n        nonlinear_activation=\"ELU\",\n        nonlinear_activation_params={}, \n    ):\n        super().__init__()\n        self.activation = getattr(nn, nonlinear_activation)(**nonlinear_activation_params)\n        self.conv1 = NonCausalConv1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=1,\n            dilation=dilation,\n            bias=bias,\n        )\n        self.conv2 = Conv1d1x1(out_channels, out_channels, bias)\n\n    def forward(self, x):\n        y = self.conv1(self.activation(x))\n        y = self.conv2(self.activation(y))\n        return x + y", "\n\nclass CausalResidualUnit(NonCausalResidualUnit):\n    def __init__(\n        self, \n        in_channels, \n        out_channels, \n        kernel_size=7,\n        dilation=1,\n        bias=False,\n        nonlinear_activation=\"ELU\",\n        nonlinear_activation_params={}, \n    ):\n        super(CausalResidualUnit, self).__init__(\n            in_channels=in_channels, \n            out_channels=out_channels, \n            kernel_size=kernel_size, \n            dilation=dilation, \n            bias=bias,\n            nonlinear_activation=nonlinear_activation,\n            nonlinear_activation_params=nonlinear_activation_params, \n        )\n        self.conv1 = CausalConv1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=1,\n            dilation=dilation,\n            bias=bias,\n        )\n    \n    def inference(self, x):\n        y = self.conv1.inference(self.activation(x))\n        y = self.conv2(self.activation(y))\n        return x + y"]}
{"filename": "models/autoencoder/modules/encoder.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://ieeexplore.ieee.org/document/9625818)", "#\n# Reference (https://ieeexplore.ieee.org/document/9625818)\n\n\"\"\"Encoder modules.\"\"\"\n\nimport torch\nimport inspect\n\nfrom layers.conv_layer import NonCausalConv1d\nfrom layers.conv_layer import CausalConv1d", "from layers.conv_layer import NonCausalConv1d\nfrom layers.conv_layer import CausalConv1d\nfrom models.autoencoder.modules.residual_unit import NonCausalResidualUnit\nfrom models.autoencoder.modules.residual_unit import CausalResidualUnit\nfrom models.utils import check_mode\n\n\nclass EncoderBlock(torch.nn.Module):\n    \"\"\" Encoder block (downsampling) \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        dilations=(1, 3, 9),\n        bias=True,\n        mode='causal',\n    ):\n        super().__init__()\n        self.mode = mode\n        if self.mode == 'noncausal':\n            ResidualUnit = NonCausalResidualUnit\n            Conv1d = NonCausalConv1d\n        elif self.mode == 'causal':\n            ResidualUnit = CausalResidualUnit\n            Conv1d = CausalConv1d\n        else:\n            raise NotImplementedError(f\"Mode ({self.mode}) is not supported!\")\n\n        self.res_units = torch.nn.ModuleList()\n        for dilation in dilations:\n            self.res_units += [\n                ResidualUnit(in_channels, in_channels, dilation=dilation)]\n        self.num_res = len(self.res_units)\n\n        self.conv = Conv1d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(2 * stride),\n            stride=stride,\n            bias=bias,\n        )\n        \n    def forward(self, x):\n        for idx in range(self.num_res):\n            x = self.res_units[idx](x)\n        x = self.conv(x)\n        return x\n    \n    def inference(self, x):\n        check_mode(self.mode, inspect.stack()[0][3])\n        for idx in range(self.num_res):\n            x = self.res_units[idx].inference(x)\n        x = self.conv.inference(x)\n        return x", "\n\nclass Encoder(torch.nn.Module):\n    def __init__(self,\n        input_channels,\n        encode_channels,\n        channel_ratios=(2, 4, 8, 16),\n        strides=(3, 4, 5, 5),\n        kernel_size=7,\n        bias=True,\n        mode='causal',\n    ):\n        super().__init__()\n        assert len(channel_ratios) == len(strides)\n        self.mode = mode\n        if self.mode == 'noncausal':\n            Conv1d = NonCausalConv1d\n        elif self.mode == 'causal':\n            Conv1d = CausalConv1d\n        else:\n            raise NotImplementedError(f\"Mode ({self.mode}) is not supported!\")\n\n        self.conv = Conv1d(\n            in_channels=input_channels, \n            out_channels=encode_channels, \n            kernel_size=kernel_size, \n            stride=1, \n            bias=False)\n\n        self.conv_blocks = torch.nn.ModuleList()\n        in_channels = encode_channels\n        for idx, stride in enumerate(strides):\n            out_channels = encode_channels * channel_ratios[idx]\n            self.conv_blocks += [\n                EncoderBlock(in_channels, out_channels, stride, bias=bias, mode=self.mode)]\n            in_channels = out_channels\n        self.num_blocks = len(self.conv_blocks)\n        self.out_channels = out_channels\n    \n    def forward(self, x): \n        x = self.conv(x)\n        for i in range(self.num_blocks):\n            x = self.conv_blocks[i](x)\n        return x\n    \n    def encode(self, x):\n        check_mode(self.mode, inspect.stack()[0][3])\n        x = self.conv.inference(x)\n        for i in range(self.num_blocks):\n            x = self.conv_blocks[i].inference(x)\n        return x", "\n\n"]}
{"filename": "models/autoencoder/modules/quantizer.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch", "\nimport torch\n\nfrom layers.vq_module import ResidualVQ\n\n\nclass Quantizer(torch.nn.Module):\n    def __init__(self,\n        code_dim,\n        codebook_num,\n        codebook_size,\n        model='residual_vq',\n        ):\n        super().__init__()\n        # speech\n        if model == 'residual_vq':\n            self.codebook = ResidualVQ(dim=code_dim, num_quantizers=codebook_num, codebook_size=codebook_size)\n        else:\n            raise NotImplementedError(f\"Model ({model}) is not supported!\")\n\n    def initial(self):\n        self.codebook.initial()    \n    \n    def forward(self, z):\n        zq, vqloss, perplexity = self.codebook(z.transpose(2, 1))\n        zq = zq.transpose(2, 1)        \n        return zq, vqloss, perplexity\n    \n    def inference(self, z):  \n        zq, indices = self.codebook.forward_index(z.transpose(2, 1))\n        zq = zq.transpose(2, 1)\n        return zq, indices\n    \n    def encode(self, z):  \n        zq, indices = self.codebook.forward_index(z.transpose(2, 1), flatten_idx=True)\n        return zq, indices\n    \n    def decode(self, indices):  \n        z = self.codebook.lookup(indices)\n        return z", ""]}
{"filename": "models/autoencoder/modules/projector.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Projector modules.\"\"\"", "\n\"\"\"Projector modules.\"\"\"\n\nimport torch\nimport inspect\n\nfrom layers.conv_layer import NonCausalConv1d\nfrom layers.conv_layer import CausalConv1d\nfrom models.utils import check_mode\n", "from models.utils import check_mode\n\n\nclass Projector(torch.nn.Module):\n    def __init__(self,\n        input_channels,\n        code_dim, \n        kernel_size=3,\n        stride=1,\n        bias=False,\n        mode='causal',\n        model='conv1d',\n    ):\n        super().__init__()\n        self.mode = mode\n        if self.mode == 'noncausal':\n            Conv1d = NonCausalConv1d\n        elif self.mode == 'causal':\n            Conv1d = CausalConv1d\n        else:\n            raise NotImplementedError(f\"Mode ({mode}) is not supported!\")\n\n        if model == 'conv1d':\n            self.project = Conv1d(input_channels, code_dim, kernel_size=kernel_size, stride=stride, bias=bias)\n        elif model == 'conv1d_bn':\n            self.project = torch.nn.Sequential(\n                Conv1d(input_channels, code_dim, kernel_size=kernel_size, stride=stride, bias=bias),\n                torch.nn.BatchNorm1d(code_dim)\n            )\n        else:\n            raise NotImplementedError(f\"Model ({model}) is not supported!\")\n        \n    def forward(self, x): \n        return self.project(x)\n    \n    def encode(self, x):\n        check_mode(self.mode, inspect.stack()[0][3])\n        return self.project.inference(x)", "\n\n"]}
{"filename": "dataloader/collater.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Customized collater modules for Pytorch DataLoader.\"\"\"\n\nimport torch\nimport numpy as np\n\n\nclass CollaterAudio(object):\n    \"\"\"Customized collater for loading single audio.\"\"\"\n\n    def __init__(\n        self,\n        batch_length=9600,\n    ):\n        \"\"\"\n        Args:\n            batch_length (int): The length of audio signal batch.\n\n        \"\"\"\n        self.batch_length = batch_length\n\n\n    def __call__(self, batch):\n        # filter short batch\n        xs = [b for b in batch if len(b) > self.batch_length]\n        \n        # random cut\n        starts, ends = self._random_segment(xs)\n        x_batch = self._cut(xs, starts, ends)\n        \n        return x_batch\n    \n\n    def _random_segment(self, xs):\n        x_lengths = [len(x) for x in xs]\n        start_offsets = np.array(\n            [\n                np.random.randint(0, xl - self.batch_length)\n                for xl in x_lengths\n            ]\n        )\n        starts = start_offsets\n        ends = starts + self.batch_length\n        return starts, ends\n    \n\n    def _cut(self, xs, starts, ends):\n        x_batch = np.array([x[start:end] for x, start, end in zip(xs, starts, ends)])\n        x_batch = torch.tensor(x_batch, dtype=torch.float).transpose(2, 1)  # (B, C, T)\n        return x_batch", "\nclass CollaterAudio(object):\n    \"\"\"Customized collater for loading single audio.\"\"\"\n\n    def __init__(\n        self,\n        batch_length=9600,\n    ):\n        \"\"\"\n        Args:\n            batch_length (int): The length of audio signal batch.\n\n        \"\"\"\n        self.batch_length = batch_length\n\n\n    def __call__(self, batch):\n        # filter short batch\n        xs = [b for b in batch if len(b) > self.batch_length]\n        \n        # random cut\n        starts, ends = self._random_segment(xs)\n        x_batch = self._cut(xs, starts, ends)\n        \n        return x_batch\n    \n\n    def _random_segment(self, xs):\n        x_lengths = [len(x) for x in xs]\n        start_offsets = np.array(\n            [\n                np.random.randint(0, xl - self.batch_length)\n                for xl in x_lengths\n            ]\n        )\n        starts = start_offsets\n        ends = starts + self.batch_length\n        return starts, ends\n    \n\n    def _cut(self, xs, starts, ends):\n        x_batch = np.array([x[start:end] for x, start, end in zip(xs, starts, ends)])\n        x_batch = torch.tensor(x_batch, dtype=torch.float).transpose(2, 1)  # (B, C, T)\n        return x_batch", "\n\nclass CollaterAudioPair(CollaterAudio):\n    \"\"\"Customized collater for loading audio pair.\"\"\"\n\n    def __init__(\n        self,\n        batch_length=9600,\n    ):\n        super().__init__(\n            batch_length=batch_length\n        )\n\n\n    def __call__(self, batch):\n        batch = [\n            b for b in batch if (len(b[0]) > self.batch_length) and (len(b[0]) == len(b[1]))\n        ]\n        assert len(batch) > 0, f\"No qualified audio pairs.!\"\n        xs, ns = [b[0] for b in batch], [b[1] for b in batch]\n\n        # random cut\n        starts, ends = self._random_segment(xs)\n        x_batch = self._cut(xs, starts, ends)\n        n_batch = self._cut(ns, starts, ends)\n        \n        return n_batch, x_batch # (input, output)", "\n\n"]}
{"filename": "dataloader/dataset.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"PyTorch compatible dataset modules.\"\"\"\n\nimport os\nimport soundfile as sf\nfrom torch.utils.data import Dataset\nfrom dataloader.utils import find_files\n", "from dataloader.utils import find_files\n\n\nclass SingleDataset(Dataset):\n    def __init__(\n        self,\n        files,\n        query=\"*.wav\",\n        load_fn=sf.read,\n        return_utt_id=False,\n        subset_num=-1,\n    ):\n        self.return_utt_id = return_utt_id\n        self.load_fn = load_fn\n        self.subset_num = subset_num\n\n        self.filenames = self._load_list(files, query)\n        self.utt_ids = self._load_ids(self.filenames)\n\n\n    def __getitem__(self, idx):\n        utt_id = self.utt_ids[idx]\n        data = self._data(idx)\n                \n        if self.return_utt_id:\n            items = utt_id, data\n        else:\n            items = data\n\n        return items\n\n\n    def __len__(self):\n        return len(self.filenames)\n    \n    \n    def _read_list(self, listfile):\n        filenames = []\n        with open(listfile) as f:\n            for line in f:\n                line = line.strip()\n                if len(line):\n                    filenames.append(line)\n        return filenames\n    \n\n    def _load_list(self, files, query):\n        if isinstance(files, list):\n            filenames = files\n        else:\n            if os.path.isdir(files):\n                filenames = sorted(find_files(files, query))\n            elif os.path.isfile(files):\n                filenames = sorted(self._read_list(files))\n            else:\n                raise ValueError(f\"{files} is not a list / existing folder or file!\")\n            \n        if self.subset_num > 0:\n            filenames = filenames[:self.subset_num]\n        assert len(filenames) != 0, f\"File list in empty!\"\n        return filenames\n    \n    \n    def _load_ids(self, filenames):\n        utt_ids = [\n            os.path.splitext(os.path.basename(f))[0] for f in filenames\n        ]\n        return utt_ids\n    \n\n    def _data(self, idx):\n        return self._load_data(self.filenames[idx], self.load_fn)\n    \n\n    def _load_data(self, filename, load_fn):\n        if load_fn == sf.read:\n            data, _ = load_fn(filename, always_2d=True) # (T, C)\n        else:\n            data = load_fn(filename)\n        return data", "\n\nclass MultiDataset(SingleDataset):\n    def __init__(\n        self,\n        multi_files,\n        queries,\n        load_fns,\n        return_utt_id=False,\n        subset_num=-1,\n    ):\n        errmsg = f\"multi_files({len(multi_files)}), queries({len(queries)}), and load_fns({len(load_fns)}) are length mismatched!\"\n        assert len(multi_files) == len(queries) == len(load_fns), errmsg\n        super(MultiDataset, self).__init__(\n            files=multi_files,\n            query=queries,\n            load_fn=load_fns,\n            return_utt_id=return_utt_id,\n            subset_num=subset_num,\n        )\n        self._check_length(self.filenames)\n    \n\n    def _load_list(self, multi_files, queries):\n        multi_filenames = []\n        if isinstance(multi_files, list):\n            for files, query in zip(multi_files, queries):\n                multi_filenames.append(super()._load_list(files, query))\n        else:\n            raise ValueError(f\"{multi_files} should be a list!\")\n        \n        return multi_filenames\n    \n    \n    def _load_ids(self, multi_filenames):\n        return super()._load_ids(multi_filenames[0])\n\n\n    def _data(self, idx):\n        filenames = [\n            f[idx] for f in self.filenames\n        ]\n        data = []\n        for filename, load_fn in zip(filenames, self.load_fn):\n            data.append(self._load_data(filename, load_fn))\n        return data\n    \n\n    def _check_length(self, multi_filenames):\n        errmsg = f\"Not all lists have the same number of files!\"\n        self.file_num = len(multi_filenames[0])\n        assert all(len(x)==self.file_num for x in multi_filenames), errmsg\n        \n    \n    def __len__(self):\n        return self.file_num", "\n"]}
{"filename": "dataloader/__init__.py", "chunked_list": ["from .dataset import *  # NOQA\nfrom .collater import *  # NOQA\nfrom .utils import * # NOQA"]}
{"filename": "dataloader/utils.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\nimport os\nimport fnmatch\nimport logging\nimport numpy as np\n\n\ndef find_files(root_dir, query=\"*.wav\", include_root_dir=True):\n    \"\"\"Find files recursively.\n        Args:\n            root_dir (str): Root root_dir to find.\n            query (str): Query to find.\n            include_root_dir (bool): If False, root_dir name is not included.\n        Returns:\n            list: List of found filenames.\n    \"\"\"\n    files = []\n    for root, dirnames, filenames in os.walk(root_dir, followlinks=True):\n        for filename in fnmatch.filter(filenames, query):\n            files.append(os.path.join(root, filename))\n    if not include_root_dir:\n        files = [file_.replace(root_dir + \"/\", \"\") for file_ in files]\n\n    return files", "\ndef find_files(root_dir, query=\"*.wav\", include_root_dir=True):\n    \"\"\"Find files recursively.\n        Args:\n            root_dir (str): Root root_dir to find.\n            query (str): Query to find.\n            include_root_dir (bool): If False, root_dir name is not included.\n        Returns:\n            list: List of found filenames.\n    \"\"\"\n    files = []\n    for root, dirnames, filenames in os.walk(root_dir, followlinks=True):\n        for filename in fnmatch.filter(filenames, query):\n            files.append(os.path.join(root, filename))\n    if not include_root_dir:\n        files = [file_.replace(root_dir + \"/\", \"\") for file_ in files]\n\n    return files", "\n\ndef load_files(data_path, query=\"*.wav\", num_core=40):\n    # sort all files    \n    file_list = sorted(find_files(data_path, query))\n    logging.info(f\"The number of {os.path.basename(data_path)} files = {len(file_list)}.\")\n    # divide\n    if num_core < len(file_list):\n        file_lists = np.array_split(file_list, num_core)\n        file_lists = [f_list.tolist() for f_list in file_lists]\n    else:\n        file_lists = [file_list]\n    return file_lists"]}
{"filename": "trainer/vocoder.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Training flow of GAN-based vocoder.\"\"\"\n\nimport logging\nimport torch\nfrom trainer.trainerGAN import TrainerGAN\n\n\nclass Trainer(TrainerGAN):\n    def __init__(\n        self,\n        steps,\n        epochs,\n        data_loader,\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        config,\n        device=torch.device(\"cpu\"),\n    ):\n        super(Trainer, self).__init__(\n           steps=steps,\n           epochs=epochs,\n           data_loader=data_loader,\n           model=model,\n           criterion=criterion,\n           optimizer=optimizer,\n           scheduler=scheduler,\n           config=config,\n           device=device,\n        )\n        self.fix_analyzer = False\n        self.generator_start = config.get(\"generator_train_start_steps\", 0)\n        self.discriminator_start = config.get(\"discriminator_train_start_steps\", 0)\n\n\n    def _train_step(self, batch):\n        \"\"\"Train model one step.\"\"\"\n        mode = 'train'\n        x = batch\n        x = x.to(self.device)\n\n        # fix analyzer\n        if not self.fix_analyzer:    \n            for parameter in self.model[\"analyzer\"].parameters():\n                parameter.requires_grad = False\n            self.fix_analyzer = True\n            logging.info(\"Analyzer is fixed!\")\n        self.model[\"analyzer\"].eval()\n\n        #######################\n        #      Generator      #\n        #######################\n        if self.steps > self.generator_start:\n            # initialize generator loss\n            gen_loss = 0.0\n\n            # main genertor operation\n            e = self.model[\"analyzer\"].encoder(x)\n            z = self.model[\"analyzer\"].projector(e)\n            zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n            y_ = self.model[\"generator\"](zq)\n\n            # metric loss\n            gen_loss += self._metric_loss(y_, x, mode=mode)\n\n            # adversarial loss\n            if self.steps > self.discriminator_start:\n                p_ = self.model[\"discriminator\"](y_)\n                if self.config[\"use_feat_match_loss\"]:\n                    with torch.no_grad():\n                        p = self.model[\"discriminator\"](x)\n                else:\n                    p = None\n                gen_loss += self._adv_loss(p_, p, mode=mode)\n                \n            # update generator\n            self._record_loss('generator_loss', gen_loss, mode=mode)\n            self._update_generator(gen_loss)\n\n        #######################\n        #    Discriminator    #\n        #######################\n        if self.steps > self.discriminator_start:\n            # re-compute y_ which leads better quality\n            with torch.no_grad():\n                e = self.model[\"analyzer\"].encoder(x)\n                z = self.model[\"analyzer\"].projector(e)\n                zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n                y_ = self.model[\"generator\"](zq)\n            p = self.model[\"discriminator\"](x)\n            p_ = self.model[\"discriminator\"](y_.detach())\n\n            # discriminator loss & update discriminator\n            self._update_discriminator(self._dis_loss(p_, p, mode=mode))\n\n        # update counts\n        self.steps += 1\n        self.tqdm.update(1)\n        self._check_train_finish()\n\n\n    @torch.no_grad()\n    def _eval_step(self, batch):\n        \"\"\"Single step of evaluation.\"\"\"\n        mode = 'eval'\n        x = batch\n        x = x.to(self.device)\n\n        # initialize generator loss\n        gen_loss = 0.0\n\n        # main genertor operation\n        e = self.model[\"analyzer\"].encoder(x)\n        z = self.model[\"analyzer\"].projector(e)\n        zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n        y_ = self.model[\"generator\"](zq)\n\n        # metric loss\n        gen_loss += self._metric_loss(y_, x, mode=mode)\n\n        # adversarial loss & feature matching loss\n        if self.steps > self.discriminator_start:\n            p_ = self.model[\"discriminator\"](y_)\n            if self.config[\"use_feat_match_loss\"]:\n                p = self.model[\"discriminator\"](x)\n            else:\n                p = None\n            gen_loss += self._adv_loss(p_, p, mode=mode)\n\n            # discriminator loss\n            self._dis_loss(p_, p, mode=mode)\n\n        # generator loss\n        self._record_loss('generator_loss', gen_loss, mode=mode)", "\n\nclass Trainer(TrainerGAN):\n    def __init__(\n        self,\n        steps,\n        epochs,\n        data_loader,\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        config,\n        device=torch.device(\"cpu\"),\n    ):\n        super(Trainer, self).__init__(\n           steps=steps,\n           epochs=epochs,\n           data_loader=data_loader,\n           model=model,\n           criterion=criterion,\n           optimizer=optimizer,\n           scheduler=scheduler,\n           config=config,\n           device=device,\n        )\n        self.fix_analyzer = False\n        self.generator_start = config.get(\"generator_train_start_steps\", 0)\n        self.discriminator_start = config.get(\"discriminator_train_start_steps\", 0)\n\n\n    def _train_step(self, batch):\n        \"\"\"Train model one step.\"\"\"\n        mode = 'train'\n        x = batch\n        x = x.to(self.device)\n\n        # fix analyzer\n        if not self.fix_analyzer:    \n            for parameter in self.model[\"analyzer\"].parameters():\n                parameter.requires_grad = False\n            self.fix_analyzer = True\n            logging.info(\"Analyzer is fixed!\")\n        self.model[\"analyzer\"].eval()\n\n        #######################\n        #      Generator      #\n        #######################\n        if self.steps > self.generator_start:\n            # initialize generator loss\n            gen_loss = 0.0\n\n            # main genertor operation\n            e = self.model[\"analyzer\"].encoder(x)\n            z = self.model[\"analyzer\"].projector(e)\n            zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n            y_ = self.model[\"generator\"](zq)\n\n            # metric loss\n            gen_loss += self._metric_loss(y_, x, mode=mode)\n\n            # adversarial loss\n            if self.steps > self.discriminator_start:\n                p_ = self.model[\"discriminator\"](y_)\n                if self.config[\"use_feat_match_loss\"]:\n                    with torch.no_grad():\n                        p = self.model[\"discriminator\"](x)\n                else:\n                    p = None\n                gen_loss += self._adv_loss(p_, p, mode=mode)\n                \n            # update generator\n            self._record_loss('generator_loss', gen_loss, mode=mode)\n            self._update_generator(gen_loss)\n\n        #######################\n        #    Discriminator    #\n        #######################\n        if self.steps > self.discriminator_start:\n            # re-compute y_ which leads better quality\n            with torch.no_grad():\n                e = self.model[\"analyzer\"].encoder(x)\n                z = self.model[\"analyzer\"].projector(e)\n                zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n                y_ = self.model[\"generator\"](zq)\n            p = self.model[\"discriminator\"](x)\n            p_ = self.model[\"discriminator\"](y_.detach())\n\n            # discriminator loss & update discriminator\n            self._update_discriminator(self._dis_loss(p_, p, mode=mode))\n\n        # update counts\n        self.steps += 1\n        self.tqdm.update(1)\n        self._check_train_finish()\n\n\n    @torch.no_grad()\n    def _eval_step(self, batch):\n        \"\"\"Single step of evaluation.\"\"\"\n        mode = 'eval'\n        x = batch\n        x = x.to(self.device)\n\n        # initialize generator loss\n        gen_loss = 0.0\n\n        # main genertor operation\n        e = self.model[\"analyzer\"].encoder(x)\n        z = self.model[\"analyzer\"].projector(e)\n        zq, _, _ = self.model[\"analyzer\"].quantizer(z)\n        y_ = self.model[\"generator\"](zq)\n\n        # metric loss\n        gen_loss += self._metric_loss(y_, x, mode=mode)\n\n        # adversarial loss & feature matching loss\n        if self.steps > self.discriminator_start:\n            p_ = self.model[\"discriminator\"](y_)\n            if self.config[\"use_feat_match_loss\"]:\n                p = self.model[\"discriminator\"](x)\n            else:\n                p = None\n            gen_loss += self._adv_loss(p_, p, mode=mode)\n\n            # discriminator loss\n            self._dis_loss(p_, p, mode=mode)\n\n        # generator loss\n        self._record_loss('generator_loss', gen_loss, mode=mode)", "\n"]}
{"filename": "trainer/trainerGAN.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Template GAN training flow.\"\"\"\n\nimport logging\nimport os\nimport abc\nimport torch\n", "import torch\n\nfrom collections import defaultdict\nfrom tensorboardX import SummaryWriter\nfrom tqdm import tqdm\n\n\nclass TrainerGAN(abc.ABC):\n    def __init__(\n        self,\n        steps,\n        epochs,\n        data_loader,\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        config,\n        device=torch.device(\"cpu\"),\n    ):\n        \"\"\"Initialize trainer.\n\n        Args:\n            steps (int): Initial global steps.\n            epochs (int): Initial global epochs.\n            data_loader (dict): Dict of data loaders. It must contrain \"train\" and \"dev\" loaders.\n            model (dict): Dict of models. It must contrain \"generator\" and \"discriminator\" models.\n            criterion (dict): Dict of criterions. It must contrain \"stft\" and \"mse\" criterions.\n            optimizer (dict): Dict of optimizers. It must contrain \"generator\" and \"discriminator\" optimizers.\n            scheduler (dict): Dict of schedulers. It must contrain \"generator\" and \"discriminator\" schedulers.\n            config (dict): Config dict loaded from yaml format configuration file.\n            device (torch.deive): Pytorch device instance.\n\n        \"\"\"\n        self.steps = steps\n        self.epochs = epochs\n        self.data_loader = data_loader\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.config = config\n        self.device = device\n        self.writer = SummaryWriter(config[\"outdir\"])\n        self.total_train_loss = defaultdict(float)\n        self.total_eval_loss = defaultdict(float)\n        self.train_max_steps = config.get(\"train_max_steps\", 0)\n\n    \n    @abc.abstractmethod\n    def _train_step(self, batch):\n        \"\"\"Single step of training.\"\"\"\n        pass\n        \n\n    @abc.abstractmethod\n    def _eval_step(self, batch):\n        \"\"\"Single step of evaluation.\"\"\"\n        pass\n\n\n    def run(self):\n        \"\"\"Run training.\"\"\"\n        self.finish_train = False\n        self.tqdm = tqdm(\n            initial=self.steps, total=self.train_max_steps, desc=\"[train]\"\n        )\n        while True:\n            self._train_epoch()\n\n            # check whether training is finished\n            if self.finish_train:\n                break\n\n        self.tqdm.close()\n        logging.info(\"Finished training.\")\n\n\n    def save_checkpoint(self, checkpoint_path):\n        \"\"\"Save checkpoint.\n\n        Args:\n            checkpoint_path (str): Checkpoint path to be saved.\n\n        \"\"\"\n        state_dict = {\n            \"optimizer\": {\n                \"generator\": self.optimizer[\"generator\"].state_dict(),\n                \"discriminator\": self.optimizer[\"discriminator\"].state_dict(),\n            },\n            \"scheduler\": {\n                \"generator\": self.scheduler[\"generator\"].state_dict(),\n                \"discriminator\": self.scheduler[\"discriminator\"].state_dict(),\n            },\n            \"steps\": self.steps,\n            \"epochs\": self.epochs,\n        }\n        state_dict[\"model\"] = {\n            \"generator\": self.model[\"generator\"].state_dict(),\n            \"discriminator\": self.model[\"discriminator\"].state_dict(),\n        }\n\n        if not os.path.exists(os.path.dirname(checkpoint_path)):\n            os.makedirs(os.path.dirname(checkpoint_path))\n        torch.save(state_dict, checkpoint_path)\n\n\n    def load_checkpoint(self, checkpoint_path, strict=True, load_only_params=False, load_discriminator=True):\n        \"\"\"Load checkpoint.\n\n        Args:\n            checkpoint_path (str): Checkpoint path to be loaded.\n            load_only_params (bool): Whether to load only model parameters.\n            load_discriminator (bool): Whether to load optimizer and scheduler of the discriminators.\n\n        \"\"\"\n        state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n        self.model[\"generator\"].load_state_dict(\n            state_dict[\"model\"][\"generator\"], strict=strict)\n        self.model[\"discriminator\"].load_state_dict(\n            state_dict[\"model\"][\"discriminator\"], strict=strict)\n        if not load_only_params:\n            self.steps = state_dict[\"steps\"]\n            self.epochs = state_dict[\"epochs\"]\n            self.optimizer[\"generator\"].load_state_dict(\n                state_dict[\"optimizer\"][\"generator\"])\n            self.scheduler[\"generator\"].load_state_dict(\n                state_dict[\"scheduler\"][\"generator\"])\n            if load_discriminator:\n                self.optimizer[\"discriminator\"].load_state_dict(\n                    state_dict[\"optimizer\"][\"discriminator\"])\n                self.scheduler[\"discriminator\"].load_state_dict(\n                    state_dict[\"scheduler\"][\"discriminator\"])\n        \n\n    def _train_epoch(self):\n        \"\"\"One epoch of training.\"\"\"\n        for train_steps_per_epoch, batch in enumerate(self.data_loader[\"train\"], 1):\n            # train one step\n            self._train_step(batch)\n\n            # check interval\n            self._check_log_interval()\n            self._check_eval_interval()\n            self._check_save_interval()\n\n            # check whether training is finished\n            if self.finish_train:\n                return\n\n        # update\n        self.epochs += 1\n        self.train_steps_per_epoch = train_steps_per_epoch\n        if train_steps_per_epoch > 200:\n            logging.info(\n                f\"(Steps: {self.steps}) Finished {self.epochs} epoch training \"\n                f\"({self.train_steps_per_epoch} steps per epoch).\"\n            )\n\n\n    def _eval_epoch(self):\n        \"\"\"One epoch of evaluation.\"\"\"\n        logging.info(f\"(Steps: {self.steps}) Start evaluation.\")\n        # change mode\n        for key in self.model.keys():\n            self.model[key].eval()\n\n        # calculate loss for each batch\n        for eval_steps_per_epoch, batch in enumerate(\n            tqdm(self.data_loader[\"dev\"], desc=\"[eval]\"), 1\n        ):\n            # eval one step\n            self._eval_step(batch)\n\n        logging.info(\n            f\"(Steps: {self.steps}) Finished evaluation \"\n            f\"({eval_steps_per_epoch} steps per epoch).\"\n        )\n\n        # average loss\n        for key in self.total_eval_loss.keys():\n            self.total_eval_loss[key] /= eval_steps_per_epoch\n            logging.info(\n                f\"(Steps: {self.steps}) {key} = {self.total_eval_loss[key]:.4f}.\"\n            )\n\n        # record\n        self._write_to_tensorboard(self.total_eval_loss)\n\n        # reset\n        self.total_eval_loss = defaultdict(float)\n\n        # restore mode\n        for key in self.model.keys():\n            self.model[key].train()\n\n\n    def _metric_loss(self, predict_y, natural_y, mode='train'):\n        \"\"\"Metric losses.\"\"\"\n        metric_loss=0.0\n\n        # mel spectrogram loss\n        if self.config.get('use_mel_loss', False):\n            mel_loss = self.criterion[\"mel\"](predict_y, natural_y)\n            mel_loss *= self.config[\"lambda_mel_loss\"]\n            self._record_loss('mel_loss', mel_loss, mode=mode)\n            metric_loss += mel_loss\n        \n        # multi-resolution sfft loss\n        if self.config.get('use_stft_loss', False):\n            sc_loss, mag_loss = self.criterion[\"stft\"](predict_y, natural_y)\n            sc_loss *= self.config[\"lambda_stft_loss\"]\n            mag_loss *= self.config[\"lambda_stft_loss\"]\n            self._record_loss('spectral_convergence_loss', sc_loss, mode=mode)\n            self._record_loss('log_stft_magnitude_loss', mag_loss, mode=mode)\n            metric_loss += (sc_loss + mag_loss)\n\n        # waveform shape loss\n        if self.config.get(\"use_shape_loss\", False):\n            shape_loss = self.criterion[\"shape\"](predict_y, natural_y)\n            shape_loss *= self.config[\"lambda_shape_loss\"]\n            self._record_loss('shape_loss', shape_loss, mode=mode)\n            metric_loss += shape_loss\n        \n        return metric_loss\n    \n\n    def _adv_loss(self, predict_p, natural_p=None, mode='train'):\n        \"\"\"Adversarial loss.\"\"\"\n        adv_loss = self.criterion[\"gen_adv\"](predict_p)\n\n        # feature matching loss\n        if natural_p is not None:\n            fm_loss = self.criterion[\"feat_match\"](predict_p, natural_p)\n            self._record_loss('feature_matching_loss', fm_loss, mode=mode)\n            adv_loss += self.config[\"lambda_feat_match\"] * fm_loss\n\n        adv_loss *= self.config[\"lambda_adv\"]\n        self._record_loss('adversarial_loss', adv_loss, mode=mode)\n\n        return adv_loss\n    \n\n    def _dis_loss(self, predict_p, natural_p, mode='train'):\n        \"\"\"Discriminator loss.\"\"\"\n        real_loss, fake_loss = self.criterion[\"dis_adv\"](predict_p, natural_p)\n        dis_loss = real_loss + fake_loss\n        self._record_loss('real_loss', real_loss, mode=mode)\n        self._record_loss('fake_loss', fake_loss, mode=mode)\n        self._record_loss('discriminator_loss', dis_loss, mode=mode)\n\n        return dis_loss\n\n\n    def _update_generator(self, gen_loss):\n        \"\"\"Update generator.\"\"\"\n        self.optimizer[\"generator\"].zero_grad()\n        gen_loss.backward()\n        if self.config[\"generator_grad_norm\"] > 0:\n            torch.nn.utils.clip_grad_norm_(\n                self.model[\"generator\"].parameters(),\n                self.config[\"generator_grad_norm\"],\n            )\n        self.optimizer[\"generator\"].step()\n        self.scheduler[\"generator\"].step()  \n    \n\n    def _update_discriminator(self, dis_loss):\n        \"\"\"Update discriminator.\"\"\"\n        self.optimizer[\"discriminator\"].zero_grad()\n        dis_loss.backward()\n        if self.config[\"discriminator_grad_norm\"] > 0:\n            torch.nn.utils.clip_grad_norm_(\n                self.model[\"discriminator\"].parameters(),\n                self.config[\"discriminator_grad_norm\"],\n            )\n        self.optimizer[\"discriminator\"].step()\n        self.scheduler[\"discriminator\"].step()\n    \n\n    def _record_loss(self, name, loss, mode='train'):\n        \"\"\"Record loss.\"\"\"\n        if torch.is_tensor(loss):\n            loss = loss.item()\n\n        if mode == 'train':\n            self.total_train_loss[f\"train/{name}\"] += loss\n        elif mode == 'eval':\n            self.total_eval_loss[f\"eval/{name}\"] += loss\n        else:\n            raise NotImplementedError(f\"Mode ({mode}) is not supported!\")\n\n\n    def _write_to_tensorboard(self, loss):\n        \"\"\"Write to tensorboard.\"\"\"\n        for key, value in loss.items():\n            self.writer.add_scalar(key, value, self.steps)\n\n\n    def _check_save_interval(self):\n        if self.steps and (self.steps % self.config[\"save_interval_steps\"] == 0):\n            self.save_checkpoint(\n                os.path.join(self.config[\"outdir\"], f\"checkpoint-{self.steps}steps.pkl\")\n            )\n            logging.info(f\"Successfully saved checkpoint @ {self.steps} steps.\")\n\n\n    def _check_eval_interval(self):\n        if self.steps % self.config[\"eval_interval_steps\"] == 0:\n            self._eval_epoch()\n\n\n    def _check_log_interval(self):\n        if self.steps % self.config[\"log_interval_steps\"] == 0:\n            for key in self.total_train_loss.keys():\n                self.total_train_loss[key] /= self.config[\"log_interval_steps\"]\n                logging.info(\n                    f\"(Steps: {self.steps}) {key} = {self.total_train_loss[key]:.4f}.\"\n                )\n            self._write_to_tensorboard(self.total_train_loss)\n\n            # reset\n            self.total_train_loss = defaultdict(float)\n\n\n    def _check_train_finish(self):\n        if self.steps >= self.train_max_steps:\n            self.finish_train = True\n        else:\n            self.finish_train = False\n        return self.finish_train", "\n\nclass TrainerVQGAN(TrainerGAN):\n    def __init__(\n        self,\n        steps,\n        epochs,\n        data_loader,\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        config,\n        device=torch.device(\"cpu\"),\n    ):\n\n        super(TrainerVQGAN, self).__init__(\n            steps=steps,\n            epochs=epochs,\n            data_loader=data_loader,\n            model=model,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            config=config,\n            device=device,\n        )\n\n\n    # perplexity info\n    def _perplexity(self, perplexity, label=None, mode='train'):\n        if label:\n            name = f\"{mode}/ppl_{label}\"\n        else:\n            name = f\"{mode}/ppl\"\n        if torch.numel(perplexity) > 1:\n            perplexity = perplexity.tolist()\n            for idx, ppl in enumerate(perplexity):\n                self._record_loss(f\"{name}_{idx}\", ppl, mode=mode)\n        else:\n            self._record_loss(name, perplexity, mode=mode)\n\n\n    # vq loss\n    def _vq_loss(self, vqloss, label=None, mode='train'):\n        if label:\n            name = f\"{mode}/vqloss_{label}\"\n        else:\n            name = f\"{mode}/vqloss\"\n        vqloss = torch.sum(vqloss)\n        vqloss *= self.config[\"lambda_vq_loss\"]\n        self._record_loss(name, vqloss, mode=mode)\n\n        return vqloss", "    \n"]}
{"filename": "trainer/autoencoder.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Training flow of symmetric codec.\"\"\"\n\nimport logging\nimport torch\nfrom trainer.trainerGAN import TrainerVQGAN\n\n\nclass Trainer(TrainerVQGAN):\n    def __init__(\n        self,\n        steps,\n        epochs,\n        data_loader,\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        config,\n        device=torch.device(\"cpu\"),\n    ):\n        super(Trainer, self).__init__(\n           steps=steps,\n           epochs=epochs,\n           data_loader=data_loader,\n           model=model,\n           criterion=criterion,\n           optimizer=optimizer,\n           scheduler=scheduler,\n           config=config,\n           device=device,\n        )\n        self.fix_encoder = False\n        self.paradigm = config.get('paradigm', 'efficient') \n        self.generator_start = config.get('start_steps', {}).get('generator', 0)\n        self.discriminator_start = config.get('start_steps', {}).get('discriminator', 200000)\n\n\n    def _train_step(self, batch):\n        \"\"\"Single step of training.\"\"\"\n        mode = 'train'\n        x = batch\n        x = x.to(self.device)\n\n        # check generator step\n        if self.steps < self.generator_start:\n            self.generator_train = False\n        else:\n            self.generator_train = True\n            \n        # check discriminator step\n        if self.steps < self.discriminator_start:\n            self.discriminator_train = False\n        else:\n            self.discriminator_train = True\n            if (not self.fix_encoder) and (self.paradigm == 'efficient'):\n                # fix encoder, quantizer, and codebook\n                for parameter in self.model[\"generator\"].encoder.parameters():\n                    parameter.requires_grad = False\n                for parameter in self.model[\"generator\"].projector.parameters():\n                    parameter.requires_grad = False\n                for parameter in self.model[\"generator\"].quantizer.parameters():\n                    parameter.requires_grad = False\n                self.fix_encoder = True\n                logging.info(\"Encoder, projector, quantizer, and codebook are fixed\")\n        \n        # check codebook updating\n        if self.fix_encoder:\n            self.model[\"generator\"].quantizer.codebook.eval()\n\n        #######################\n        #      Generator      #\n        #######################\n        if self.generator_train:\n            # initialize generator loss\n            gen_loss = 0.0\n\n            # main genertor operation\n            y_, zq, z, vqloss, perplexity = self.model[\"generator\"](x)\n\n            # perplexity info\n            self._perplexity(perplexity, mode=mode)\n\n            # vq loss\n            gen_loss += self._vq_loss(vqloss, mode=mode)\n            \n            # metric loss\n            gen_loss += self._metric_loss(y_, x, mode=mode)\n            \n            # adversarial loss\n            if self.discriminator_train:\n                p_ = self.model[\"discriminator\"](y_)\n                if self.config[\"use_feat_match_loss\"]:\n                    with torch.no_grad():\n                        p = self.model[\"discriminator\"](x)\n                else:\n                    p = None\n                gen_loss += self._adv_loss(p_, p, mode=mode)\n\n            # update generator\n            self._record_loss('generator_loss', gen_loss, mode=mode)\n            self._update_generator(gen_loss)\n\n        #######################\n        #    Discriminator    #\n        #######################\n        if self.discriminator_train:\n            # re-compute y_ which leads better quality\n            with torch.no_grad():\n                y_, _, _, _, _ = self.model[\"generator\"](x)\n            \n            p = self.model[\"discriminator\"](x)\n            p_ = self.model[\"discriminator\"](y_.detach())\n\n            # discriminator loss & update discriminator\n            self._update_discriminator(self._dis_loss(p_, p, mode=mode))\n\n        # update counts\n        self.steps += 1\n        self.tqdm.update(1)\n        self._check_train_finish()\n\n\n    @torch.no_grad()\n    def _eval_step(self, batch):\n        \"\"\"Single step of evaluation.\"\"\"\n        mode = 'eval'\n        x = batch\n        x = x.to(self.device)\n        \n        # initialize generator loss\n        gen_loss = 0.0\n\n        # main genertor operation\n        y_, zq, z, vqloss, perplexity = self.model[\"generator\"](x)\n\n        # perplexity info\n        self._perplexity(perplexity, mode=mode)\n\n        # vq_loss\n        gen_loss += self._vq_loss(vqloss, mode=mode)\n        \n        # metric loss\n        gen_loss += self._metric_loss(y_, x, mode=mode)\n\n        if self.discriminator_train:\n            # adversarial loss\n            p_ = self.model[\"discriminator\"](y_)\n            p = self.model[\"discriminator\"](x)\n            gen_loss += self._adv_loss(p_, p, mode=mode)\n\n            # discriminator loss\n            self._dis_loss(p_, p, mode=mode)\n\n        # generator loss\n        self._record_loss('generator_loss', gen_loss, mode=mode)", "\n\nclass Trainer(TrainerVQGAN):\n    def __init__(\n        self,\n        steps,\n        epochs,\n        data_loader,\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        config,\n        device=torch.device(\"cpu\"),\n    ):\n        super(Trainer, self).__init__(\n           steps=steps,\n           epochs=epochs,\n           data_loader=data_loader,\n           model=model,\n           criterion=criterion,\n           optimizer=optimizer,\n           scheduler=scheduler,\n           config=config,\n           device=device,\n        )\n        self.fix_encoder = False\n        self.paradigm = config.get('paradigm', 'efficient') \n        self.generator_start = config.get('start_steps', {}).get('generator', 0)\n        self.discriminator_start = config.get('start_steps', {}).get('discriminator', 200000)\n\n\n    def _train_step(self, batch):\n        \"\"\"Single step of training.\"\"\"\n        mode = 'train'\n        x = batch\n        x = x.to(self.device)\n\n        # check generator step\n        if self.steps < self.generator_start:\n            self.generator_train = False\n        else:\n            self.generator_train = True\n            \n        # check discriminator step\n        if self.steps < self.discriminator_start:\n            self.discriminator_train = False\n        else:\n            self.discriminator_train = True\n            if (not self.fix_encoder) and (self.paradigm == 'efficient'):\n                # fix encoder, quantizer, and codebook\n                for parameter in self.model[\"generator\"].encoder.parameters():\n                    parameter.requires_grad = False\n                for parameter in self.model[\"generator\"].projector.parameters():\n                    parameter.requires_grad = False\n                for parameter in self.model[\"generator\"].quantizer.parameters():\n                    parameter.requires_grad = False\n                self.fix_encoder = True\n                logging.info(\"Encoder, projector, quantizer, and codebook are fixed\")\n        \n        # check codebook updating\n        if self.fix_encoder:\n            self.model[\"generator\"].quantizer.codebook.eval()\n\n        #######################\n        #      Generator      #\n        #######################\n        if self.generator_train:\n            # initialize generator loss\n            gen_loss = 0.0\n\n            # main genertor operation\n            y_, zq, z, vqloss, perplexity = self.model[\"generator\"](x)\n\n            # perplexity info\n            self._perplexity(perplexity, mode=mode)\n\n            # vq loss\n            gen_loss += self._vq_loss(vqloss, mode=mode)\n            \n            # metric loss\n            gen_loss += self._metric_loss(y_, x, mode=mode)\n            \n            # adversarial loss\n            if self.discriminator_train:\n                p_ = self.model[\"discriminator\"](y_)\n                if self.config[\"use_feat_match_loss\"]:\n                    with torch.no_grad():\n                        p = self.model[\"discriminator\"](x)\n                else:\n                    p = None\n                gen_loss += self._adv_loss(p_, p, mode=mode)\n\n            # update generator\n            self._record_loss('generator_loss', gen_loss, mode=mode)\n            self._update_generator(gen_loss)\n\n        #######################\n        #    Discriminator    #\n        #######################\n        if self.discriminator_train:\n            # re-compute y_ which leads better quality\n            with torch.no_grad():\n                y_, _, _, _, _ = self.model[\"generator\"](x)\n            \n            p = self.model[\"discriminator\"](x)\n            p_ = self.model[\"discriminator\"](y_.detach())\n\n            # discriminator loss & update discriminator\n            self._update_discriminator(self._dis_loss(p_, p, mode=mode))\n\n        # update counts\n        self.steps += 1\n        self.tqdm.update(1)\n        self._check_train_finish()\n\n\n    @torch.no_grad()\n    def _eval_step(self, batch):\n        \"\"\"Single step of evaluation.\"\"\"\n        mode = 'eval'\n        x = batch\n        x = x.to(self.device)\n        \n        # initialize generator loss\n        gen_loss = 0.0\n\n        # main genertor operation\n        y_, zq, z, vqloss, perplexity = self.model[\"generator\"](x)\n\n        # perplexity info\n        self._perplexity(perplexity, mode=mode)\n\n        # vq_loss\n        gen_loss += self._vq_loss(vqloss, mode=mode)\n        \n        # metric loss\n        gen_loss += self._metric_loss(y_, x, mode=mode)\n\n        if self.discriminator_train:\n            # adversarial loss\n            p_ = self.model[\"discriminator\"](y_)\n            p = self.model[\"discriminator\"](x)\n            gen_loss += self._adv_loss(p_, p, mode=mode)\n\n            # discriminator loss\n            self._dis_loss(p_, p, mode=mode)\n\n        # generator loss\n        self._record_loss('generator_loss', gen_loss, mode=mode)", "\n        \n\n       \n\n"]}
{"filename": "trainer/denoise.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)", "#\n# Reference (https://github.com/kan-bayashi/ParallelWaveGAN/)\n\n\"\"\"Training flow of symmetric codec.\"\"\"\n\nimport logging\nimport torch\nfrom trainer.trainerGAN import TrainerVQGAN\n\n\nclass Trainer(TrainerVQGAN):\n    def __init__(\n        self,\n        steps,\n        epochs,\n        data_loader,\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        config,\n        device=torch.device(\"cpu\"),\n    ):\n        super(Trainer, self).__init__(\n           steps=steps,\n           epochs=epochs,\n           data_loader=data_loader,\n           model=model,\n           criterion=criterion,\n           optimizer=optimizer,\n           scheduler=scheduler,\n           config=config,\n           device=device,\n        )\n        # fix quantizer\n        for parameter in self.model[\"generator\"].quantizer.parameters():\n            parameter.requires_grad = False\n        # fix decoder\n        for parameter in self.model[\"generator\"].decoder.parameters():\n            parameter.requires_grad = False\n        logging.info(\"Quantizer, codebook, and decoder are fixed\")\n\n\n    def _train_step(self, batch):\n        \"\"\"Single step of training.\"\"\"\n        mode = 'train'\n        x_n, x_c = batch\n        x_n = x_n.to(self.device)\n        x_c = x_c.to(self.device)\n        \n        # fix codebook\n        self.model[\"generator\"].quantizer.codebook.eval()\n        \n        # initialize generator loss\n        gen_loss = 0.0\n\n        # main genertor operation\n        y_nc, zq, z, vqloss, perplexity = self.model[\"generator\"](x_n)\n\n        # perplexity info\n        self._perplexity(perplexity, mode=mode)\n\n        # vq loss\n        gen_loss += self._vq_loss(vqloss, mode=mode)\n        \n        # metric loss\n        gen_loss += self._metric_loss(y_nc, x_c, mode=mode)\n\n        # update generator\n        self._record_loss('generator_loss', gen_loss, mode=mode)\n        self._update_generator(gen_loss)\n\n        # update counts\n        self.steps += 1\n        self.tqdm.update(1)\n        self._check_train_finish()\n\n\n    @torch.no_grad()\n    def _eval_step(self, batch):\n        \"\"\"Single step of evaluation.\"\"\"\n        mode = 'eval'\n        x_n, x_c = batch\n        x_n = x_n.to(self.device)\n        x_c = x_c.to(self.device)\n        \n        # initialize generator loss\n        gen_loss = 0.0\n\n        # main genertor operation\n        y_nc, zq, z, vqloss, perplexity = self.model[\"generator\"](x_n)\n\n        # perplexity info\n        self._perplexity(perplexity, mode=mode)\n\n        # vq_loss\n        gen_loss += self._vq_loss(vqloss, mode=mode)\n        \n        # metric loss\n        gen_loss += self._metric_loss(y_nc, x_c, mode=mode)\n\n        # generator loss\n        self._record_loss('generator_loss', gen_loss, mode=mode)", "\n\nclass Trainer(TrainerVQGAN):\n    def __init__(\n        self,\n        steps,\n        epochs,\n        data_loader,\n        model,\n        criterion,\n        optimizer,\n        scheduler,\n        config,\n        device=torch.device(\"cpu\"),\n    ):\n        super(Trainer, self).__init__(\n           steps=steps,\n           epochs=epochs,\n           data_loader=data_loader,\n           model=model,\n           criterion=criterion,\n           optimizer=optimizer,\n           scheduler=scheduler,\n           config=config,\n           device=device,\n        )\n        # fix quantizer\n        for parameter in self.model[\"generator\"].quantizer.parameters():\n            parameter.requires_grad = False\n        # fix decoder\n        for parameter in self.model[\"generator\"].decoder.parameters():\n            parameter.requires_grad = False\n        logging.info(\"Quantizer, codebook, and decoder are fixed\")\n\n\n    def _train_step(self, batch):\n        \"\"\"Single step of training.\"\"\"\n        mode = 'train'\n        x_n, x_c = batch\n        x_n = x_n.to(self.device)\n        x_c = x_c.to(self.device)\n        \n        # fix codebook\n        self.model[\"generator\"].quantizer.codebook.eval()\n        \n        # initialize generator loss\n        gen_loss = 0.0\n\n        # main genertor operation\n        y_nc, zq, z, vqloss, perplexity = self.model[\"generator\"](x_n)\n\n        # perplexity info\n        self._perplexity(perplexity, mode=mode)\n\n        # vq loss\n        gen_loss += self._vq_loss(vqloss, mode=mode)\n        \n        # metric loss\n        gen_loss += self._metric_loss(y_nc, x_c, mode=mode)\n\n        # update generator\n        self._record_loss('generator_loss', gen_loss, mode=mode)\n        self._update_generator(gen_loss)\n\n        # update counts\n        self.steps += 1\n        self.tqdm.update(1)\n        self._check_train_finish()\n\n\n    @torch.no_grad()\n    def _eval_step(self, batch):\n        \"\"\"Single step of evaluation.\"\"\"\n        mode = 'eval'\n        x_n, x_c = batch\n        x_n = x_n.to(self.device)\n        x_c = x_c.to(self.device)\n        \n        # initialize generator loss\n        gen_loss = 0.0\n\n        # main genertor operation\n        y_nc, zq, z, vqloss, perplexity = self.model[\"generator\"](x_n)\n\n        # perplexity info\n        self._perplexity(perplexity, mode=mode)\n\n        # vq_loss\n        gen_loss += self._vq_loss(vqloss, mode=mode)\n        \n        # metric loss\n        gen_loss += self._metric_loss(y_nc, x_c, mode=mode)\n\n        # generator loss\n        self._record_loss('generator_loss', gen_loss, mode=mode)", "\n        \n\n       \n\n"]}
