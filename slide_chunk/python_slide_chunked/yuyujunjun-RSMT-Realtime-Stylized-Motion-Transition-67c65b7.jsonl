{"filename": "train_deephase.py", "chunked_list": ["\nimport os\nfrom argparse import ArgumentParser\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning import loggers\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom pytorch_lightning.profiler import SimpleProfiler\nfrom pytorch_lightning.utilities.seed import seed_everything", "from pytorch_lightning.profiler import SimpleProfiler\nfrom pytorch_lightning.utilities.seed import seed_everything\n\nfrom src.Datasets.DeepPhaseDataModule import Style100DataModule\nfrom src.Datasets.Style100Processor import StyleLoader, Swap100StyJoints\nfrom src.Net.DeepPhaseNet import DeepPhaseNet, Application\nfrom src.utils import BVH_mod as BVH\nfrom src.utils.locate_model import locate_model\nfrom src.utils.motion_process import subsample\n", "from src.utils.motion_process import subsample\n\n\n#from src.Datasets.DataSetProperty import lafan1_property,cmu_property\ndef setup_seed(seed:int):\n    seed_everything(seed,True)\ndef test_model():\n    dict = {}\n    dict['limit_train_batches'] = 1.\n    dict['limit_val_batches'] = 1.\n    return dict", "def detect_nan_par():\n    '''track_grad_norm\": 'inf'''\n    return { \"detect_anomaly\":True}\ndef select_gpu_par():\n    return {\"accelerator\":'gpu', \"auto_select_gpus\":True, \"devices\":-1}\n\ndef create_common_states(prefix:str):\n    log_name = prefix+'/'\n    '''test upload'''\n    parser = ArgumentParser()\n    parser.add_argument(\"--dev_run\", action=\"store_true\")\n    parser.add_argument(\"--version\", type=str, default=\"-1\")\n    parser.add_argument(\"--resume\", action=\"store_true\")\n    parser.add_argument(\"--n_phases\",type=int,default=10)\n    parser.add_argument(\"--epoch\",type=str,default = '')\n    parser.add_argument(\"--test\",action=\"store_true\")\n    args = parser.parse_args()\n    ckpt_path = \"results/\"\n    if (args.version != \"-1\"):\n        version = args.version\n    else:\n        version = None\n    '''Create Loggers tensorboard'''\n    if args.dev_run:\n        log_name += \"dev_run\"\n    else:\n        log_name += \"myResults\"\n    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"tensorboard_logs/\", name=log_name, version=version)\n\n    ckpt_path = os.path.join(ckpt_path, log_name, str(tb_logger.version))\n    if (args.resume == True):\n        resume_from_checkpoint = os.path.join(os.path.join(ckpt_path, \"last.ckpt\"))  # results/version/last.ckpt\"\n    else:\n        resume_from_checkpoint = None\n    checkpoint_callback = [ModelCheckpoint(dirpath=ckpt_path + \"/\", save_top_k=-1, save_last=True, every_n_epochs=1,save_weights_only=True),\n                           ModelCheckpoint(dirpath=ckpt_path + \"/\", save_top_k=1, monitor=\"val_loss\", save_last=False, every_n_epochs=1)]\n    '''Train'''\n    checkpoint_callback[0].CHECKPOINT_NAME_LAST = \"last\"\n    profiler = SimpleProfiler()\n    trainer_dict = {\n        \"callbacks\":checkpoint_callback,\n        \"profiler\":profiler,\n        \"logger\":tb_logger\n    }\n    return args,trainer_dict,resume_from_checkpoint,ckpt_path", "\ndef read_style_bvh(style,content,clip=None):\n    swap_joints = Swap100StyJoints()\n    anim = BVH.read_bvh(os.path.join(\"MotionData/100STYLE/\",style,style+\"_\"+content+\".bvh\"),remove_joints=swap_joints)\n    if (clip != None):\n        anim.quats = anim.quats[clip[0]:clip[1], ...]\n        anim.hip_pos = anim.hip_pos[clip[0]:clip[1], ...]\n    anim = subsample(anim,ratio=2)\n    return anim\n\ndef training_style100():\n    args, trainer_dict, resume_from_checkpoint, ckpt_path = create_common_states(\"deephase_sty\")\n    '''Create the model'''\n    frequency = 30\n    window = 61\n\n    style_loader = StyleLoader()\n    batch_size = 32\n    data_module = Style100DataModule( batch_size=batch_size,shuffle=True,data_loader=style_loader,window_size=window)\n    model = DeepPhaseNet(args.n_phases, data_module.skeleton, window, 1.0 / frequency,batch_size=batch_size)  # or model = pl.LightningModule().load_from_checkpoint(PATH)\n    if (args.test == False):\n        if (args.dev_run):\n            trainer = Trainer(**trainer_dict, **test_model(),\n                              **select_gpu_par(), precision=32,\n                              log_every_n_steps=50, flush_logs_every_n_steps=500, max_epochs=30,\n                              weights_summary='full', auto_lr_find=True)\n        else:\n\n            trainer = Trainer(**trainer_dict, max_epochs=500, **select_gpu_par(), log_every_n_steps=50,#limit_train_batches=0.1,\n                              flush_logs_every_n_steps=500, resume_from_checkpoint=resume_from_checkpoint)\n        trainer.fit(model, datamodule=data_module)\n    # trainer.test(ckpt_path='best')\n    else:\n        anim = read_style_bvh(\"WildArms\", \"FW\",[509,1009])\n\n        check_file = ckpt_path + \"/\"\n\n        modelfile = locate_model(check_file, args.epoch)\n\n        model = DeepPhaseNet.load_from_checkpoint(modelfile)\n        model = model.cuda()\n\n        data_module.setup()\n\n        app = Application(model, data_module)\n        app = app.float()\n        anim = subsample(anim, 1)\n        app.setAnim(anim)\n        app.forward()\n\n        BVH.save_bvh(\"source.bvh\",anim)", "\ndef training_style100():\n    args, trainer_dict, resume_from_checkpoint, ckpt_path = create_common_states(\"deephase_sty\")\n    '''Create the model'''\n    frequency = 30\n    window = 61\n\n    style_loader = StyleLoader()\n    batch_size = 32\n    data_module = Style100DataModule( batch_size=batch_size,shuffle=True,data_loader=style_loader,window_size=window)\n    model = DeepPhaseNet(args.n_phases, data_module.skeleton, window, 1.0 / frequency,batch_size=batch_size)  # or model = pl.LightningModule().load_from_checkpoint(PATH)\n    if (args.test == False):\n        if (args.dev_run):\n            trainer = Trainer(**trainer_dict, **test_model(),\n                              **select_gpu_par(), precision=32,\n                              log_every_n_steps=50, flush_logs_every_n_steps=500, max_epochs=30,\n                              weights_summary='full', auto_lr_find=True)\n        else:\n\n            trainer = Trainer(**trainer_dict, max_epochs=500, **select_gpu_par(), log_every_n_steps=50,#limit_train_batches=0.1,\n                              flush_logs_every_n_steps=500, resume_from_checkpoint=resume_from_checkpoint)\n        trainer.fit(model, datamodule=data_module)\n    # trainer.test(ckpt_path='best')\n    else:\n        anim = read_style_bvh(\"WildArms\", \"FW\",[509,1009])\n\n        check_file = ckpt_path + \"/\"\n\n        modelfile = locate_model(check_file, args.epoch)\n\n        model = DeepPhaseNet.load_from_checkpoint(modelfile)\n        model = model.cuda()\n\n        data_module.setup()\n\n        app = Application(model, data_module)\n        app = app.float()\n        anim = subsample(anim, 1)\n        app.setAnim(anim)\n        app.forward()\n\n        BVH.save_bvh(\"source.bvh\",anim)", "\n\ndef readBVH(filename,dataset_property):\n    remove_joints = (dataset_property['remove_joints'])\n    if (remove_joints != None):\n        remove_joints = remove_joints()\n    filename = dataset_property[\"test_path\"] + filename\n    return BVH.read_bvh(filename, remove_joints=remove_joints, Tpose=-1, remove_gap=dataset_property['remove_gap'])\n\nif __name__ == '__main__':\n    setup_seed(3407)\n    training_style100()", "\nif __name__ == '__main__':\n    setup_seed(3407)\n    training_style100()\n\n\n"]}
{"filename": "train_styleVAE.py", "chunked_list": ["#import argparse\nimport copy\nimport os\nimport re\nfrom argparse import ArgumentParser\n\nimport pytorch_lightning as pl\nimport torch\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning import loggers", "from pytorch_lightning import Trainer\nfrom pytorch_lightning import loggers\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom pytorch_lightning.profiler import SimpleProfiler\nfrom pytorch_lightning.utilities.seed import seed_everything\n\nfrom src.Datasets.BaseLoader import WindowBasedLoader\nfrom src.Net.StyleVAENet import StyleVAENet\nfrom src.utils import BVH_mod as BVH\n", "from src.utils import BVH_mod as BVH\n\n\ndef setup_seed(seed:int):\n    seed_everything(seed,True)\ndef test_model():\n    dict = {}\n    #dict['fast_dev_run'] = 1 # only run 1 train, val, test batch and program ends\n    dict['limit_train_batches'] = 0.1\n    dict['limit_val_batches'] = 0.7\n    return dict", "def detect_nan_par():\n    '''track_grad_norm\": 'inf'''\n    return { \"detect_anomaly\":True}\ndef select_gpu_par():\n    return {\"accelerator\":'gpu', \"auto_select_gpus\":True, \"devices\":-1}\n\ndef create_common_states(prefix:str):\n    log_name = prefix+'/'\n    '''test upload'''\n    parser = ArgumentParser()\n    parser.add_argument(\"--dev_run\", action=\"store_true\")\n    parser.add_argument(\"--version\", type=str, default=\"-1\")\n    parser.add_argument(\"--epoch\",type=str,default=\"last\")\n    parser.add_argument(\"--resume\", action=\"store_true\")\n    parser.add_argument(\"--test\",action=\"store_true\")\n    args = parser.parse_args()\n    ckpt_path_prefix = \"results/\"\n    if (args.version != \"-1\"):\n        version = args.version\n    else:\n        version = None\n    '''Create Loggers tensorboard'''\n    if args.dev_run:\n        log_name += \"dev_run\"\n    else:\n        log_name += \"myResults\"\n    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"tensorboard_logs/\", name=log_name, version=None)\n    load_ckpt_path = os.path.join(ckpt_path_prefix, prefix+'/myResults', str(version))\n    save_ckpt_path = os.path.join(ckpt_path_prefix, log_name, str(tb_logger.version))\n\n    if (args.resume == True):\n        check_file = load_ckpt_path+\"/\"\n        if (args.epoch == \"last\"):\n            check_file += \"last.ckpt\"\n        else:\n            dirs = os.listdir(check_file)\n            for dir in dirs:\n                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n                out = re.findall(st, dir)\n                if (len(out) > 0):\n                    check_file += out[0]\n                    print(check_file)\n                    break\n        resume_from_checkpoint = check_file  # results/version/last.ckpt\"\n    else:\n        resume_from_checkpoint = None\n    checkpoint_callback = [ModelCheckpoint(dirpath=save_ckpt_path + \"/\", save_top_k=-1, save_last=True, every_n_epochs=5),\n                           ModelCheckpoint(dirpath=save_ckpt_path + \"/\", save_top_k=1, monitor=\"val_loss\", save_last=False, every_n_epochs=1,save_weights_only=True),\n                          # EMA(0.99)\n                           ]\n    '''Train'''\n    checkpoint_callback[0].CHECKPOINT_NAME_LAST = \"last\"\n    profiler = SimpleProfiler()#PyTorchProfiler(filename=\"profiler\")\n    trainer_dict = {\n        \"callbacks\":checkpoint_callback,\n        \"profiler\":profiler,\n        \"logger\":tb_logger\n    }\n    return args,trainer_dict,resume_from_checkpoint,load_ckpt_path", "def training_style100():\n    from src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\n    from src.Datasets.Style100Processor import StyleLoader\n    from src.Net.StyleVAENet import Application,VAEMode\n    prefix = \"StyleVAE2\"\n    data_set = \"style100\"\n    prefix += \"_\" + data_set\n    args, trainer_dict, resume_from_checkpoint, ckpt_path = create_common_states(prefix)\n    resume_from_checkpoint = None\n    loader = WindowBasedLoader(61, 21, 1)\n    dt = 1. / 30.\n    phase_dim = 10\n    phase_file = \"+phase_gv10\"\n    latent_size = 32\n    net_mode = VAEMode.SINGLE\n\n    batch_size = 32\n    if (args.test == False):\n        '''Create the model'''\n        style_loader = StyleLoader()\n        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),style_file_name=None, dt=dt, batch_size=batch_size, mirror=0.0)  # when apply phase, should avoid mirror\n        model = StyleVAENet(data_module.skeleton,  phase_dim=phase_dim, latent_size=latent_size,batch_size=batch_size,mode='pretrain',net_mode=net_mode)\n        if (args.dev_run):\n            trainer = Trainer(**trainer_dict, **test_model(),\n                              **select_gpu_par(), precision=32, reload_dataloaders_every_n_epochs=1,#gradient_clip_val=1.0,#**detect_nan_par(),\n                              log_every_n_steps=5, flush_logs_every_n_steps=10,\n                              weights_summary='full')\n        else:\n\n            trainer = Trainer(**trainer_dict, max_epochs=10000, reload_dataloaders_every_n_epochs=1,gradient_clip_val=1.0,#**detect_nan_par(),\n                              **select_gpu_par(), log_every_n_steps=50,\n                              flush_logs_every_n_steps=100)\n        trainer.fit(model, datamodule=data_module)\n\n    else:\n\n        style_loader = StyleLoader()\n        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),None, dt=dt, batch_size=batch_size, mirror=0.0)\n        data_module.setup()\n        check_file = ckpt_path + \"/\"\n        if (args.epoch == \"last\"):\n            check_file += \"last.ckpt\"\n            print(check_file)\n        else:\n            dirs = os.listdir(check_file)\n            for dir in dirs:\n                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n                out = re.findall(st, dir)\n                if (len(out) > 0):\n                    check_file += out[0]\n                    print(check_file)\n                    break\n        model = StyleVAENet.load_from_checkpoint(check_file, moe_decoder=None,pose_channels=6,net_mode=net_mode,strict=False)\n        model = model.cuda()\n        src_motion = data_module.test_set.dataset[\"HighKnees\"][0]\n        source = BVH.read_bvh(\"source.bvh\")\n        '''check if space can produce netural space: encoding=False, style=kick'''\n        data_module.mirror = 0\n        model = model.cpu()\n        model.eval()\n        app = Application(model, data_module)\n        app = app.float()\n        app.setSource(src_motion)\n        output = copy.deepcopy(source)\n        output.hip_pos, output.quats = app.forward(seed=3000,encoding=True)\n        BVH.save_bvh(\"test_net.bvh\", output)\n        source.hip_pos, source.quats = app.get_source()\n        BVH.save_bvh(\"source.bvh\", source)\n        torch.save(model, ckpt_path + \"/m_save_model_\" + str(args.epoch))", "\n\nif __name__ == '__main__':\n    setup_seed(3407)\n    training_style100()\n\n\n"]}
{"filename": "add_phase_to_dataset.py", "chunked_list": ["import torch\n\nimport src.Datasets.BaseLoader as mBaseLoader\nfrom src.Datasets.DeepPhaseDataModule import DeephaseDataSet, Style100DataModule\nfrom src.Datasets.Style100Processor import StyleLoader\nfrom src.Net.DeepPhaseNet import Application\nfrom src.Net.DeepPhaseNet import DeepPhaseNet\n\n\nclass PhaseMotionStyle100Processor(mBaseLoader.BasedDataProcessor):\n    def __init__(self,window,dt,model_path:str):\n        from src.Datasets.DeepPhaseDataModule import DeepPhaseProcessor\n        super(PhaseMotionStyle100Processor, self).__init__()\n        self.processor = DeepPhaseProcessor(dt)\n        #self.processor = DeepPhaseProcessorPCA(dt)\n        #self.attribute = 'pos'#'gv'\n        self.window = window\n        self.model = DeepPhaseNet.load_from_checkpoint(model_path,style_loader=None)\n    def __call__(self, dict,skeleton,motion_datalaoder= None):\n        offsets, hip_pos, quats = dict[\"offsets\"],dict[\"hip_pos\"],dict[\"quats\"]\n        style_loader = StyleLoader()\n        data_module = Style100DataModule(batch_size=32, shuffle=True, data_loader=style_loader, window_size=self.window)\n        # data_module.setup()# load std\n        #stat = style_loader.load_part_to_binary(\"deepphase_vp_statistics\")\n        app = Application(self.model, data_module)\n        self.app = app.float()\n\n        gv = self.processor(dict,skeleton,style_loader)['gv']\n        gv = torch.from_numpy(gv).cuda()\n        phase = {key:[] for key in [\"A\",\"S\",\"B\",\"F\"]}\n        h=[]\n        q=[]\n        o=[]\n        for i in range(len(offsets)):\n            print(\"{} in {},length:{}\".format(i,len(offsets),hip_pos[i].shape[0]))\n            if(hip_pos[i].shape[0]<=self.window): #gv = hip_pos[i].shape[0]-1\n                continue\n            dataset = DeephaseDataSet([gv[i]], self.window)\n            print(\"dataset length: {}\".format(len(dataset)))\n            if(len(dataset)==0):\n                continue\n            self.app.Net.to(\"cuda\")\n            phases = self.app.calculate_statistic_for_dataset(dataset)\n            key_frame = self.window // 2   # 61th or 31th,\n\n            use_pos= False\n            if(use_pos):\n                clip = lambda x:x[key_frame:-key_frame]\n            else:\n                '''gv\u7684\u7b2c60\u5e27\u5b9e\u9645\u4e0a\u662f\u7b2c61\u5e27\u51cf60\uff0c\u6211\u4eec\u5e94\u8be5\u4fdd\u7559\u7b2c61\u5e27'''\n                clip = lambda x: x[key_frame+1:-key_frame+1]\n            # o[i] = clip(o[i])\n            o.append(offsets[i])\n            h.append(clip(hip_pos[i]))\n            q.append(clip(quats[i]))\n            #offsets[i]=None\n            #hip_pos[i]=None\n            #quats[i]=None\n            for key in phases:\n                phase[key].append(phases[key])\n\n        return {\"offsets\": o, \"hip_pos\": h, \"quats\": q, **phase}", "\nclass PhaseMotionStyle100Processor(mBaseLoader.BasedDataProcessor):\n    def __init__(self,window,dt,model_path:str):\n        from src.Datasets.DeepPhaseDataModule import DeepPhaseProcessor\n        super(PhaseMotionStyle100Processor, self).__init__()\n        self.processor = DeepPhaseProcessor(dt)\n        #self.processor = DeepPhaseProcessorPCA(dt)\n        #self.attribute = 'pos'#'gv'\n        self.window = window\n        self.model = DeepPhaseNet.load_from_checkpoint(model_path,style_loader=None)\n    def __call__(self, dict,skeleton,motion_datalaoder= None):\n        offsets, hip_pos, quats = dict[\"offsets\"],dict[\"hip_pos\"],dict[\"quats\"]\n        style_loader = StyleLoader()\n        data_module = Style100DataModule(batch_size=32, shuffle=True, data_loader=style_loader, window_size=self.window)\n        # data_module.setup()# load std\n        #stat = style_loader.load_part_to_binary(\"deepphase_vp_statistics\")\n        app = Application(self.model, data_module)\n        self.app = app.float()\n\n        gv = self.processor(dict,skeleton,style_loader)['gv']\n        gv = torch.from_numpy(gv).cuda()\n        phase = {key:[] for key in [\"A\",\"S\",\"B\",\"F\"]}\n        h=[]\n        q=[]\n        o=[]\n        for i in range(len(offsets)):\n            print(\"{} in {},length:{}\".format(i,len(offsets),hip_pos[i].shape[0]))\n            if(hip_pos[i].shape[0]<=self.window): #gv = hip_pos[i].shape[0]-1\n                continue\n            dataset = DeephaseDataSet([gv[i]], self.window)\n            print(\"dataset length: {}\".format(len(dataset)))\n            if(len(dataset)==0):\n                continue\n            self.app.Net.to(\"cuda\")\n            phases = self.app.calculate_statistic_for_dataset(dataset)\n            key_frame = self.window // 2   # 61th or 31th,\n\n            use_pos= False\n            if(use_pos):\n                clip = lambda x:x[key_frame:-key_frame]\n            else:\n                '''gv\u7684\u7b2c60\u5e27\u5b9e\u9645\u4e0a\u662f\u7b2c61\u5e27\u51cf60\uff0c\u6211\u4eec\u5e94\u8be5\u4fdd\u7559\u7b2c61\u5e27'''\n                clip = lambda x: x[key_frame+1:-key_frame+1]\n            # o[i] = clip(o[i])\n            o.append(offsets[i])\n            h.append(clip(hip_pos[i]))\n            q.append(clip(quats[i]))\n            #offsets[i]=None\n            #hip_pos[i]=None\n            #quats[i]=None\n            for key in phases:\n                phase[key].append(phases[key])\n\n        return {\"offsets\": o, \"hip_pos\": h, \"quats\": q, **phase}", "def add_phase_to_100Style(info):\n    phase_processor = PhaseMotionStyle100Processor(info[\"window\"], info['dt'], info[\"model_path\"])\n    bloader = mBaseLoader.StreamBasedLoader(1)\n    style_loader = StyleLoader()\n    style_loader.setup(bloader,mBaseLoader.BasedDataProcessor())\n    style_loader.process_from_binary()\n    def add_phase(motions):\n        for style in motions.keys():\n            print(style+\"----------\")\n            for content in motions[style].keys():\n                print(content)\n                motions[style][content] = phase_processor(motions[style][content],style_loader.skeleton)\n        return motions\n    style_loader.train_motions = add_phase(style_loader.train_motions)\n    style_loader.test_motions = add_phase(style_loader.test_motions)\n    style_loader.save_dataset(\"+phase_gv10\")", "\n    # style_loader.process_from_binary(argument=False)\n    # style_loader.train_motions = add_phase(style_loader.train_motions)\n    # style_loader.test_motions = add_phase(style_loader.test_motions)\n    # style_loader.save_dataset(\"no_augement+phase_gv10\")"]}
{"filename": "process_dataset.py", "chunked_list": ["import os\nimport src.Datasets.BaseLoader as mBaseLoader\nfrom src.Datasets.BatchProcessor import BatchRotateYCenterXZ\nimport torch\nimport numpy as np\nimport random\nfrom src.Datasets.Style100Processor import StyleLoader,Swap100StyJoints,bvh_to_binary,save_skeleton\nimport src.utils.BVH_mod as BVH\nfrom src.utils.motion_process import subsample\nfrom src.Datasets.BaseLoader import BasedDataProcessor,BasedLoader,DataSetType,MotionDataLoader,WindowBasedLoader\nclass TransitionProcessor(BasedDataProcessor):\n    def __init__(self,ref_id):\n        super(TransitionProcessor, self).__init__()\n        self.process = BatchRotateYCenterXZ()\n        self.ref_id = ref_id\n    def __call__(self, dict, skeleton,motionDataLoader,ratio=1.0):\n        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n        stat = self._calculate_stat(offsets,hip_pos,quats,skeleton,ratio)\n        return {\"offsets\":offsets,\"hip_pos\":hip_pos,\"quats\":quats,**stat}\n    def _concat(self,local_quat,offsets,hip_pos,ratio=0.2):\n        if(ratio>=1.0):\n            local_quat = torch.from_numpy(np.concatenate(local_quat, axis=0))\n            offsets = torch.from_numpy(np.concatenate((offsets), axis=0))\n            hip_pos = torch.from_numpy(np.concatenate((hip_pos), axis=0))\n        else:\n            length = int(len(local_quat)*ratio)+1\n            idx = []\n            for i in range(length):\n                idx.append(random.randrange(0,len(local_quat)))\n            sample = lambda x:[x[i] for i in idx]\n\n            local_quat = torch.from_numpy(np.concatenate(sample(local_quat), axis=0))\n            offsets = torch.from_numpy(np.concatenate(sample(offsets), axis=0))\n            hip_pos =torch.from_numpy(np.concatenate(sample(hip_pos), axis=0))\n        return local_quat,offsets,hip_pos\n    def calculate_pos_statistic(self, pos):\n        '''pos:N,T,J,3'''\n        mean = np.mean(pos, axis=(0, 1))\n        std = np.std(pos, axis=(0, 1))\n        std = np.mean(std)\n        return mean, std\n\n    def calculate_rotation_mean(self,rotation):\n        mean = np.mean(rotation,axis=(0,1))\n        std = np.std(rotation,axis=(0,1))\n        std = np.mean(std)\n        return mean ,std\n    def calculate_statistic(self, local_pos,local_rot):\n        pos_mean, pos_std = self.calculate_pos_statistic(local_pos[:,:,1:,:].cpu().numpy())\n        vel_mean, vel_std = self.calculate_pos_statistic((local_pos[:, 1:, 1:, :] - local_pos[:, :-1, 1:, :]).cpu().numpy())\n        hipv_mean, hipv_std = self.calculate_pos_statistic((local_pos[:, 1:, 0:1, :] - local_pos[:, :-1, 0:1, :]).cpu().numpy())\n        rot_mean,rot_std = self.calculate_rotation_mean(local_rot[:,:,1:,:].cpu().numpy())\n        rotv_mean,rotv_std = self.calculate_rotation_mean((local_rot[:,1:,1:,:]-local_rot[:,:-1,1:,:]).cpu().numpy())\n        hipr_mean, hipr_std = self.calculate_rotation_mean(local_rot[:, :, 0:1, :].cpu().numpy())\n        hiprv_mean, hiprv_std = self.calculate_rotation_mean((local_rot[:, 1:, 0:1, :] - local_rot[:, :-1, 0:1, :]).cpu().numpy())\n\n        return {\"pos_stat\": [pos_mean, pos_std], \"rot_stat\":[rot_mean,rot_std],\"vel_stat\":[vel_mean,vel_std],\n                \"rotv_stat\":[rotv_mean,rotv_std],\"hipv_stat\":[hipv_mean,hipv_std],\"hipr_stat\":[hipr_mean,hipr_std],\"hiprv_stat\":[hiprv_mean,hiprv_std]}\n    def _calculate_stat(self,offsets,hip_pos,local_quat,skeleton,ratio):\n        local_quat, offsets, hip_pos = self._concat(local_quat,offsets,hip_pos,ratio)\n        global_positions, global_rotations = skeleton.forward_kinematics(local_quat, offsets, hip_pos)\n        local_pos,local_rot = self.process(global_positions,local_quat, self.ref_id)\n\n        return self.calculate_statistic(local_pos,local_rot)", "from src.utils.motion_process import subsample\nfrom src.Datasets.BaseLoader import BasedDataProcessor,BasedLoader,DataSetType,MotionDataLoader,WindowBasedLoader\nclass TransitionProcessor(BasedDataProcessor):\n    def __init__(self,ref_id):\n        super(TransitionProcessor, self).__init__()\n        self.process = BatchRotateYCenterXZ()\n        self.ref_id = ref_id\n    def __call__(self, dict, skeleton,motionDataLoader,ratio=1.0):\n        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n        stat = self._calculate_stat(offsets,hip_pos,quats,skeleton,ratio)\n        return {\"offsets\":offsets,\"hip_pos\":hip_pos,\"quats\":quats,**stat}\n    def _concat(self,local_quat,offsets,hip_pos,ratio=0.2):\n        if(ratio>=1.0):\n            local_quat = torch.from_numpy(np.concatenate(local_quat, axis=0))\n            offsets = torch.from_numpy(np.concatenate((offsets), axis=0))\n            hip_pos = torch.from_numpy(np.concatenate((hip_pos), axis=0))\n        else:\n            length = int(len(local_quat)*ratio)+1\n            idx = []\n            for i in range(length):\n                idx.append(random.randrange(0,len(local_quat)))\n            sample = lambda x:[x[i] for i in idx]\n\n            local_quat = torch.from_numpy(np.concatenate(sample(local_quat), axis=0))\n            offsets = torch.from_numpy(np.concatenate(sample(offsets), axis=0))\n            hip_pos =torch.from_numpy(np.concatenate(sample(hip_pos), axis=0))\n        return local_quat,offsets,hip_pos\n    def calculate_pos_statistic(self, pos):\n        '''pos:N,T,J,3'''\n        mean = np.mean(pos, axis=(0, 1))\n        std = np.std(pos, axis=(0, 1))\n        std = np.mean(std)\n        return mean, std\n\n    def calculate_rotation_mean(self,rotation):\n        mean = np.mean(rotation,axis=(0,1))\n        std = np.std(rotation,axis=(0,1))\n        std = np.mean(std)\n        return mean ,std\n    def calculate_statistic(self, local_pos,local_rot):\n        pos_mean, pos_std = self.calculate_pos_statistic(local_pos[:,:,1:,:].cpu().numpy())\n        vel_mean, vel_std = self.calculate_pos_statistic((local_pos[:, 1:, 1:, :] - local_pos[:, :-1, 1:, :]).cpu().numpy())\n        hipv_mean, hipv_std = self.calculate_pos_statistic((local_pos[:, 1:, 0:1, :] - local_pos[:, :-1, 0:1, :]).cpu().numpy())\n        rot_mean,rot_std = self.calculate_rotation_mean(local_rot[:,:,1:,:].cpu().numpy())\n        rotv_mean,rotv_std = self.calculate_rotation_mean((local_rot[:,1:,1:,:]-local_rot[:,:-1,1:,:]).cpu().numpy())\n        hipr_mean, hipr_std = self.calculate_rotation_mean(local_rot[:, :, 0:1, :].cpu().numpy())\n        hiprv_mean, hiprv_std = self.calculate_rotation_mean((local_rot[:, 1:, 0:1, :] - local_rot[:, :-1, 0:1, :]).cpu().numpy())\n\n        return {\"pos_stat\": [pos_mean, pos_std], \"rot_stat\":[rot_mean,rot_std],\"vel_stat\":[vel_mean,vel_std],\n                \"rotv_stat\":[rotv_mean,rotv_std],\"hipv_stat\":[hipv_mean,hipv_std],\"hipr_stat\":[hipr_mean,hipr_std],\"hiprv_stat\":[hiprv_mean,hiprv_std]}\n    def _calculate_stat(self,offsets,hip_pos,local_quat,skeleton,ratio):\n        local_quat, offsets, hip_pos = self._concat(local_quat,offsets,hip_pos,ratio)\n        global_positions, global_rotations = skeleton.forward_kinematics(local_quat, offsets, hip_pos)\n        local_pos,local_rot = self.process(global_positions,local_quat, self.ref_id)\n\n        return self.calculate_statistic(local_pos,local_rot)", "def read_style_bvh(style,content,clip=None):\n    swap_joints = Swap100StyJoints()\n    anim = BVH.read_bvh(os.path.join(\"MotionData/100STYLE/\",style,style+\"_\"+content+\".bvh\"),remove_joints=swap_joints)\n    if (clip != None):\n        anim.quats = anim.quats[clip[0]:clip[1], ...]\n        anim.hip_pos = anim.hip_pos[clip[0]:clip[1], ...]\n    anim = subsample(anim,ratio=2)\n    return anim\n\ndef processStyle100Benchmark( window, overlap):\n    style_loader = StyleLoader()\n    processor = None\n    bloader = mBaseLoader.WindowBasedLoader(window=window, overlap=overlap, subsample=1)\n    style_loader.setup(bloader, processor)\n    style_loader.load_dataset(\"+phase_gv10\")\n\n    def split_window(motions):\n        for style in motions.keys():\n            styles = []\n            # print(style)\n            if len(motions[style].keys()):\n                dict = motions[style].copy()\n\n            for content in motions[style].keys():\n                motions[style][content] = bloader.append_dicts(motions[style][content])\n            for content in dict.keys():\n                if dict[content]['hip_pos'][0].shape[0]>=120:\n                    o = dict[content]['offsets'][0]\n                    h = dict[content]['hip_pos'][0][0:120]\n                    q = dict[content]['quats'][0][0:120]\n                    styles.append({\"offsets\": o, \"hip_pos\": h, \"quats\": q})\n            motions[style]['style'] = styles\n        result = {}\n        for style_name in motions.keys():\n            # print(motions.keys())\n            o, h, q, a, s, b, f = [], [], [], [], [], [], []\n            for content_name in motions[style_name]:\n                if content_name == 'style':\n                    continue\n                dict = motions[style_name][content_name]\n                o += dict['offsets']\n                h += dict['hip_pos']\n                q += dict['quats']\n                a += dict['A']\n                s += dict['S']\n                b += dict['B']\n                f += dict['F']\n\n            # i += 1\n            style = motions[style_name]['style']\n            motion = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f}\n            result[style_name] = {\"motion\":motion, \"style\":style}\n        return result\n    style_loader.test_dict = split_window(style_loader.test_motions)\n    style_loader.save_to_binary(\"style100_benchmark_65_25\", style_loader.test_dict)", "\ndef processStyle100Benchmark( window, overlap):\n    style_loader = StyleLoader()\n    processor = None\n    bloader = mBaseLoader.WindowBasedLoader(window=window, overlap=overlap, subsample=1)\n    style_loader.setup(bloader, processor)\n    style_loader.load_dataset(\"+phase_gv10\")\n\n    def split_window(motions):\n        for style in motions.keys():\n            styles = []\n            # print(style)\n            if len(motions[style].keys()):\n                dict = motions[style].copy()\n\n            for content in motions[style].keys():\n                motions[style][content] = bloader.append_dicts(motions[style][content])\n            for content in dict.keys():\n                if dict[content]['hip_pos'][0].shape[0]>=120:\n                    o = dict[content]['offsets'][0]\n                    h = dict[content]['hip_pos'][0][0:120]\n                    q = dict[content]['quats'][0][0:120]\n                    styles.append({\"offsets\": o, \"hip_pos\": h, \"quats\": q})\n            motions[style]['style'] = styles\n        result = {}\n        for style_name in motions.keys():\n            # print(motions.keys())\n            o, h, q, a, s, b, f = [], [], [], [], [], [], []\n            for content_name in motions[style_name]:\n                if content_name == 'style':\n                    continue\n                dict = motions[style_name][content_name]\n                o += dict['offsets']\n                h += dict['hip_pos']\n                q += dict['quats']\n                a += dict['A']\n                s += dict['S']\n                b += dict['B']\n                f += dict['F']\n\n            # i += 1\n            style = motions[style_name]['style']\n            motion = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f}\n            result[style_name] = {\"motion\":motion, \"style\":style}\n        return result\n    style_loader.test_dict = split_window(style_loader.test_motions)\n    style_loader.save_to_binary(\"style100_benchmark_65_25\", style_loader.test_dict)", "\ndef processTransitionPhaseDatasetForStyle100(window,overlap):\n    style_loader = StyleLoader()\n    window_loader = mBaseLoader.WindowBasedLoader(window, overlap, 1)\n    processor = None #MotionPuzzleProcessor()\n    style_loader.setup(window_loader, processor)\n    style_loader.load_dataset(\"+phase_gv10\")\n    def split_window(motions):\n        #motions = style_loader.all_motions\n        for style in motions.keys():\n            for content in motions[style].keys():\n                motions[style][content] = window_loader.append_dicts(motions[style][content])\n        return motions\n    style_loader.train_motions = split_window(style_loader.train_motions)\n    style_loader.test_motions = split_window(style_loader.test_motions)\n    #style_loader.save_part_to_binary(\"motionpuzzle_statistics\", [\"pos_stat\", \"vel_stat\", \"rot_stat\"])\n    style_loader.save_dataset(\"+phase_gv10\" + window_loader.get_postfix_str())\n    print()", "\ndef processDeepPhaseForStyle100(window,overlap):\n    from src.Datasets.DeepPhaseDataModule import DeepPhaseProcessor\n    style_loader = StyleLoader()\n    window_loader = mBaseLoader.WindowBasedLoader(window,overlap,1)\n    processor = DeepPhaseProcessor(1./30)\n    style_loader.setup(window_loader,processor)\n    style_loader.process_from_binary()\n    style_loader.save_train_test_dataset(\"deep_phase_gv\")\n\ndef splitStyle100TrainTestSet():\n    style_loader = StyleLoader()\n    print(\"Divide the data set to train set and test set\")\n    style_loader.split_from_binary()\n    print(\"argument datasets\")\n    style_loader.augment_dataset()\n    print(\"down\")", "\ndef splitStyle100TrainTestSet():\n    style_loader = StyleLoader()\n    print(\"Divide the data set to train set and test set\")\n    style_loader.split_from_binary()\n    print(\"argument datasets\")\n    style_loader.augment_dataset()\n    print(\"down\")\n\nif __name__ == '__main__':\n    from argparse import ArgumentParser\n    parser = ArgumentParser()\n    parser.add_argument(\"--preprocess\", action=\"store_true\")\n    parser.add_argument(\"--train_phase_model\", action=\"store_true\")\n    parser.add_argument(\"--add_phase_to_dataset\", action=\"store_true\")\n    parser.add_argument(\"--model_path\",type=str,default=\"./results/deephase_sty/myResults/31/epoch=161-step=383778-v1.ckpt\")\n    parser.add_argument(\"--train_manifold_model\", action=\"store_true\")\n    parser.add_argument(\"--train_sampler_model\", action=\"store_true\")\n    parser.add_argument(\"--benchmarks\", action=\"store_true\")\n    args = parser.parse_args()\n    if(args.preprocess==True):\n        print(\"######## convert all bvh files to binary files################\")\n        bvh_to_binary()\n        save_skeleton()\n        print(\"\\nConvert down\\n\")\n        print(\"Divide the dataset to train set and test set, and then argument datasets.\")\n        splitStyle100TrainTestSet()\n    elif(args.train_phase_model==True):\n        processDeepPhaseForStyle100(62,2)\n    elif(args.add_phase_to_dataset==True):\n        from add_phase_to_dataset import add_phase_to_100Style\n        style100_info = {\n            \"model_path\":args.model_path,\n            \"dt\": 1. / 30,\n            \"window\": 61\n        }\n        add_phase_to_100Style(style100_info)\n    elif(args.train_manifold_model==True):\n        processTransitionPhaseDatasetForStyle100(61,21)\n    elif(args.train_sampler_model==True):\n        processTransitionPhaseDatasetForStyle100(120,0)\n    elif(args.benchmarks==True):\n        processStyle100Benchmark(65,25)", "\nif __name__ == '__main__':\n    from argparse import ArgumentParser\n    parser = ArgumentParser()\n    parser.add_argument(\"--preprocess\", action=\"store_true\")\n    parser.add_argument(\"--train_phase_model\", action=\"store_true\")\n    parser.add_argument(\"--add_phase_to_dataset\", action=\"store_true\")\n    parser.add_argument(\"--model_path\",type=str,default=\"./results/deephase_sty/myResults/31/epoch=161-step=383778-v1.ckpt\")\n    parser.add_argument(\"--train_manifold_model\", action=\"store_true\")\n    parser.add_argument(\"--train_sampler_model\", action=\"store_true\")\n    parser.add_argument(\"--benchmarks\", action=\"store_true\")\n    args = parser.parse_args()\n    if(args.preprocess==True):\n        print(\"######## convert all bvh files to binary files################\")\n        bvh_to_binary()\n        save_skeleton()\n        print(\"\\nConvert down\\n\")\n        print(\"Divide the dataset to train set and test set, and then argument datasets.\")\n        splitStyle100TrainTestSet()\n    elif(args.train_phase_model==True):\n        processDeepPhaseForStyle100(62,2)\n    elif(args.add_phase_to_dataset==True):\n        from add_phase_to_dataset import add_phase_to_100Style\n        style100_info = {\n            \"model_path\":args.model_path,\n            \"dt\": 1. / 30,\n            \"window\": 61\n        }\n        add_phase_to_100Style(style100_info)\n    elif(args.train_manifold_model==True):\n        processTransitionPhaseDatasetForStyle100(61,21)\n    elif(args.train_sampler_model==True):\n        processTransitionPhaseDatasetForStyle100(120,0)\n    elif(args.benchmarks==True):\n        processStyle100Benchmark(65,25)", "\n\n"]}
{"filename": "Running_LongSeq.py", "chunked_list": ["import torch\nfrom pytorch3d.transforms import quaternion_apply, quaternion_multiply, quaternion_invert\nfrom src.Datasets.Style100Processor import StyleLoader\nfrom src.geometry.quaternions import or6d_to_quat, quat_to_or6D, from_to_1_0_0\nfrom src.utils import BVH_mod as BVH\nfrom src.utils.BVH_mod import Skeleton, find_secondary_axis\n\n\ndef load_model():\n    model = torch.load('./results/Transitionv2_style100/myResults/141/m_save_model_198')\n    return model", "def load_model():\n    model = torch.load('./results/Transitionv2_style100/myResults/141/m_save_model_198')\n    return model\ndef load_dataSet():\n    loader = StyleLoader()\n    loader.load_dataset(\"+phase_gv10\")\n    return loader\nclass BatchRotateYCenterXZ(torch.nn.Module):\n    def __init__(self):\n        super(BatchRotateYCenterXZ, self).__init__()\n    def forward(self,global_positions,global_quats,ref_frame_id):\n        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id+1, 5:6, :] - global_positions[:, ref_frame_id:ref_frame_id+1, 1:2, :],\n                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device),dim=-1)\n        root_rotation = from_to_1_0_0(ref_vector)\n\n        ref_hip = torch.mean(global_positions[:,:,0:1,[0,2]],dim=(1),keepdim=True)\n        global_positions[...,[0,2]] = global_positions[...,[0,2]] - ref_hip\n        global_positions = quaternion_apply(root_rotation, global_positions)\n        global_quats = quaternion_multiply(root_rotation, global_quats)\n        \"\"\" Local Space \"\"\"\n\n        return global_positions,global_quats, ref_hip,root_rotation", "\nclass TransformSeq():\n    def __init__(self):\n        self.trans = BatchRotateYCenterXZ()\n        pass\n    def transform(self,glb_pos,glb_quat):\n        pos,quats, ref_hip,root_rotation = self.trans(glb_pos,glb_quat,9)\n        self.ref_hip = ref_hip\n        self.inv_rot = quaternion_invert(root_rotation)\n        return pos,quats\n    def un_transform(self,glb_pos,glb_quat):\n        glb_pos = quaternion_apply(self.inv_rot,glb_pos)\n        glb_rot = quaternion_multiply(self.inv_rot,glb_quat)\n        glb_pos[...,[0,2]] = glb_pos[...,[0,2]]+self.ref_hip\n        return glb_pos,glb_rot", "class RunningLongSeq():\n    def __init__(self,model,X,Q,A,S,tar_X,tar_Q,pos_offset,skeleton):\n        self.window = 60\n        self.source_idx = 0\n        self.out_idx = 0\n        self.X = X.cuda()\n        self.Q = Q.cuda()\n\n        self.pos_offset = pos_offset.cuda()\n        self.tar_pos, self.tar_rot = skeleton.forward_kinematics(tar_Q[:, :120].cuda(),  self.pos_offset, tar_X[:, :120].cuda())\n        self.skeleton = skeleton\n        self.transform = TransformSeq()\n        self.model = model.cuda()\n        self.phases = model.phase_op.phaseManifold(A, S)\n        self.outX = X[:,:10].cuda()\n        self.outQ = Q[:,:10].cuda()\n        self.outPhase = self.phases[:,:10].cuda()\n\n        tar_id = [50,90,130,170,210,250,290,330]\n        self.time = [40,40,40,40,40,80,40,40]\n        get_tar_property = lambda property: [property[:,tar_id[i]-2:tar_id[i]] for i in range(len(tar_id))]\n        self.X_tar = get_tar_property(self.X)\n        self.Q_tar = get_tar_property(self.Q)\n        self.X_tar[3][:, :, :, [0, 2]] = self.X_tar[2][:, :, :, [0]] + 20\n        self.tar_id = 0\n        pass\n    def next_seq(self,length):\n        X = self.outX[:,self.out_idx:self.out_idx+10]\n        Q = self.outQ[:,self.out_idx:self.out_idx+10]\n        phases = self.phases[:,self.out_idx:self.out_idx+10]#self.outPhase[:,self.out_idx:self.out_idx+10]\n        X_tar = self.X_tar[self.tar_id]#torch.cat((self.X_tar[self.tar_id]),dim=1)\n        Q_tar = self.Q_tar[self.tar_id]#torch.cat((self.Q_tar[self.tar_id]),dim=1)\n        X = torch.cat((X,X_tar),dim=1)\n        Q = torch.cat((Q,Q_tar),dim=1)\n        gp,gq = self.skeleton.forward_kinematics(Q,self.pos_offset,X)\n        gp,gq = self.transform.transform(gp,gq)\n        gq,gp,pred_phase = synthesize(self.model,gp,gq,phases,self.tar_pos,self.tar_rot,self.pos_offset,self.skeleton,length,12)\n        gp,gq = self.transform.un_transform(gp,gq)\n        X,Q = self.skeleton.inverse_kinematics(gq,gp)\n        self.outX = torch.cat((self.outX,X),dim=1)\n        self.outQ = torch.cat((self.outQ,Q),dim=1)\n        self.outPhase = torch.cat((self.outPhase,pred_phase),dim=1)\n        self.out_idx += length\n        self.tar_id+=1\n    def iteration(self):\n      #  time = [40,40,40,40,40,80,40]\n        for i in range(len(self.X_tar)):\n            self.next_seq(self.time[i])\n    def get_source(self):\n        return self.X.cpu().squeeze(0).numpy()[10:],self.Q.cpu().squeeze(0).numpy()[10:]\n    def get_results(self):\n        return self.outX.cpu().squeeze(0).numpy()[10:],self.outQ.cpu().squeeze(0).numpy()[10:]\n    def get_target(self):\n        Xs = []\n        Qs = []\n        for i in range(len(self.X_tar)):\n            X = self.X_tar[i][0,1:]\n            Q = self.Q_tar[i][0,1:]\n            X = X.repeat(self.time[i],1,1)\n            Q = Q.repeat(self.time[i],1,1)\n            Xs.append(X)\n            Qs.append(Q)\n        Xs = torch.cat(Xs,dim=0)\n        Qs = torch.cat(Qs,dim=0)\n        return Xs.cpu().numpy(),Qs.cpu().numpy()", "\n\ndef synthesize(model, gp, gq, phases, tar_pos, tar_quat, pos_offset, skeleton: Skeleton, length, target_id, ifnoise=False):\n    model = model.eval()\n    model = model.cuda()\n    #quats = Q\n    offsets = pos_offset\n  #  hip_pos = X\n   # gp, gq = skeleton.forward_kinematics(quats, offsets, hip_pos)\n    loc_rot = quat_to_or6D(gq)\n    if ifnoise:\n        noise = None\n    else:\n        noise = torch.zeros(size=(gp.shape[0], 512), dtype=gp.dtype, device=gp.device).cuda()\n    tar_quat = quat_to_or6D(tar_quat)\n    target_style = model.get_film_code(tar_pos.cuda(), tar_quat.cuda())  # use random style seq\n    # target_style = model.get_film_code(gp.cuda(), loc_rot.cuda())\n   # F = S[:, 1:] - S[:, :-1]\n  #  F = model.phase_op.remove_F_discontiny(F)\n   # F = F / model.phase_op.dt\n   # phases = model.phase_op.phaseManifold(A, S)\n    pred_pos, pred_rot, pred_phase, _ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), None,\n                                                            None,\n                                                            target_style, noise, start_id=10, target_id=target_id,\n                                                            length=length, phase_schedule=1.)\n    pred_pos, pred_rot = pred_pos, pred_rot\n    rot_pos = model.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n    pred_pos[:, :, model.rot_rep_idx] = rot_pos[:, :, model.rot_rep_idx]\n    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n    pred_pos, pred_rot = model.regu_pose(pred_pos, edge_len, pred_rot)\n\n    GQ = skeleton.inverse_pos_to_rot(or6d_to_quat(pred_rot), pred_pos, offsets, find_secondary_axis(offsets))\n    GX = skeleton.global_rot_to_global_pos(GQ, offsets, pred_pos[:, :, 0:1, :])\n    phases = pred_phase[3]\n    return GQ, GX, phases", "\n\nif __name__ ==\"__main__\":\n    model = load_model()\n    loader = load_dataSet()\n    anim = BVH.read_bvh(\"source.bvh\")\n    motions = loader.train_motions['LeftHop'][\"BR\"]\n    tar_motions = loader.train_motions['Neutral'][\"FR\"]\n\n    def extract_property(motions):\n        X = torch.from_numpy(motions['hip_pos'][0]).unsqueeze(0)\n        Q = torch.from_numpy(motions['quats'][0]).unsqueeze(0)\n        A = torch.from_numpy(motions['A'][0]).unsqueeze(0)/0.1\n        S = torch.from_numpy(motions['S'][0]).unsqueeze(0)\n        return X,Q,A,S\n\n\n    X, Q, A, S = extract_property(motions)\n    pos_offset = torch.from_numpy(motions['offsets'][0]).unsqueeze(0)\n    with torch.no_grad():\n        running_machine = RunningLongSeq(model,X,Q,A,S,X,Q,pos_offset,anim.skeleton)\n        running_machine.iteration()\n        anim.hip_pos,anim.quats = running_machine.get_source()\n        BVH.save_bvh(\"source.bvh\",anim)\n        anim.hip_pos,anim.quats = running_machine.get_results()\n        BVH.save_bvh(\"results.bvh\",anim)\n        anim.hip_pos, anim.quats = running_machine.get_target()\n        BVH.save_bvh(\"target.bvh\", anim)", "\n\n"]}
{"filename": "benchmark.py", "chunked_list": ["import time\nimport pickle\nfrom src.Datasets.BatchProcessor import BatchProcessDatav2\nfrom src.geometry.quaternions import or6d_to_quat, quat_to_or6D, from_to_1_0_0\nfrom scipy import linalg\nfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport os\nfrom src.utils.BVH_mod import Skeleton, find_secondary_axis", "import os\nfrom src.utils.BVH_mod import Skeleton, find_secondary_axis\nfrom src.utils.np_vector import interpolate_local, remove_quat_discontinuities\nfrom src.Datasets.Style100Processor import StyleLoader\nimport src.Datasets.BaseLoader as mBaseLoader\nimport torch\nimport numpy as np\nimport random\nimport torch.nn.functional as F\ndef load_model(args):\n    model_dict, function_dict = {}, {}\n    model_dict[args.model_name] = torch.load(args.model_path)\n    if(hasattr(model_dict[args.model_name],\"predict_phase\")):\n        model_dict[args.model_name].predict_phase = False\n    model_dict[args.model_name].test = True\n    function_dict[args.model_name] = eval_sample\n    return model_dict, function_dict", "import torch.nn.functional as F\ndef load_model(args):\n    model_dict, function_dict = {}, {}\n    model_dict[args.model_name] = torch.load(args.model_path)\n    if(hasattr(model_dict[args.model_name],\"predict_phase\")):\n        model_dict[args.model_name].predict_phase = False\n    model_dict[args.model_name].test = True\n    function_dict[args.model_name] = eval_sample\n    return model_dict, function_dict\n", "\n\ndef eval_sample(model, X, Q, A, S, tar_pos, tar_quat, pos_offset, skeleton: Skeleton, length, target_id, ifnoise=False):\n    model = model.eval()\n    model = model.cuda()\n    quats = Q\n    offsets = pos_offset\n    hip_pos = X\n    gp, gq = skeleton.forward_kinematics(quats, offsets, hip_pos)\n    loc_rot = quat_to_or6D(gq)\n    if ifnoise:\n        noise = None\n    else:\n        noise = torch.zeros(size=(gp.shape[0], 512), dtype=gp.dtype, device=gp.device).cuda()\n    tar_quat = quat_to_or6D(tar_quat)\n    target_style = model.get_film_code(tar_pos.cuda(), tar_quat.cuda())   # use random style seq\n    # target_style = model.get_film_code(gp.cuda(), loc_rot.cuda())\n    F = S[:, 1:] - S[:, :-1]\n    F = model.phase_op.remove_F_discontiny(F)\n    F = F / model.phase_op.dt\n    phases = model.phase_op.phaseManifold(A, S)\n\n    if(hasattr(model,\"predict_phase\") and model.predict_phase):\n        pred_pos, pred_rot, pred_phase, _,_ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(),\n                                                            F.cuda(),\n                                                            target_style, noise, start_id=10, target_id=target_id,\n                                                            length=length, phase_schedule=1.)\n    else:\n        pred_pos, pred_rot, pred_phase, _ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(),\n                                                            F.cuda(),\n                                                            target_style, noise, start_id=10, target_id=target_id,\n                                                            length=length, phase_schedule=1.)\n    pred_pos, pred_rot = pred_pos, pred_rot\n    rot_pos = model.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n    pred_pos[:, :, model.rot_rep_idx] = rot_pos[:, :, model.rot_rep_idx]\n    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n    pred_pos, pred_rot = model.regu_pose(pred_pos, edge_len, pred_rot)\n\n    GQ = skeleton.inverse_pos_to_rot(or6d_to_quat(pred_rot), pred_pos, offsets, find_secondary_axis(offsets))\n    GX = skeleton.global_rot_to_global_pos(GQ, offsets, pred_pos[:, :, 0:1, :])\n\n    return GQ, GX", "\n\n\ndef renderplot(name, ylabel, lengths, res):\n    for key, l in res:\n        if l == lengths[0]:\n            result = [res[(key, n)] for n in lengths]\n            plt.plot(lengths, result, label=key)\n    plt.xlabel('Lengths')\n    plt.ylabel(ylabel)\n    plt.title(name)\n    plt.legend()\n    plt.savefig(name + '.png')\n    plt.close()", "\ndef calculate_fid(embeddings, gt_embeddings):\n    if type(embeddings) == torch.Tensor:\n        embeddings = embeddings.detach().cpu().numpy()\n        gt_embeddings = gt_embeddings.detach().cpu().numpy()\n    mu1 = np.mean(embeddings, axis=0)\n    sigma1 = np.cov(embeddings, rowvar=False)\n    mu2 = np.mean(gt_embeddings, axis=0)\n    sigma2 = np.cov(gt_embeddings, rowvar=False)\n    return calculate_frechet_distance(mu1, sigma1, mu2, sigma2)", "\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    Stable version by Dougal J. Sutherland.\n    Params:\n    -- mu1   : Numpy array containing the activations of a layer of the\n               inception net (like returned by the function 'get_predictions')\n               for generated samples.\n    -- mu2   : The sample mean over activations, precalculated on an\n               representative data set.\n    -- sigma1: The covariance matrix over activations for generated samples.\n    -- sigma2: The covariance matrix over activations, precalculated on an\n               representative data set.\n    Returns:\n    --   : The Frechet Distance.\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        'Training and test mean vectors have different lengths'\n    assert sigma1.shape == sigma2.shape, \\\n        'Training and test covariances have different dimensions'\n\n    diff = mu1 - mu2\n\n    # Product might be almost singular\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False, blocksize=1024)\n    if not np.isfinite(covmean).all():\n        msg = ('fid calculation produces singular product; '\n               'adding %s to diagonal of cov estimates') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    # Numerical error might give slight imaginary component\n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError('Imaginary component {}'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1)\n            + np.trace(sigma2) - 2 * tr_covmean)", "\nclass BatchRotateYCenterXZ(torch.nn.Module):\n    def __init__(self):\n        super(BatchRotateYCenterXZ, self).__init__()\n\n    def forward(self, global_positions, global_quats, ref_frame_id):\n        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id + 1, 5:6, :] - global_positions[:,\n                                                                                              ref_frame_id:ref_frame_id + 1,\n                                                                                              1:2, :],\n                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device),\n                                 dim=-1)\n        root_rotation = from_to_1_0_0(ref_vector)\n\n        # center\n        ref_hip = torch.mean(global_positions[:, :, 0:1, [0, 2]], dim=(1), keepdim=True)\n        global_positions[..., [0, 2]] = global_positions[..., [0, 2]] - ref_hip\n        global_positions = quaternion_apply(root_rotation, global_positions)\n        global_quats = quaternion_multiply(root_rotation, global_quats)\n\n        return global_positions, global_quats", "\nclass BenchmarkDataSet(Dataset):\n    def __init__(self, data, style_keys):\n        super(BenchmarkDataSet, self).__init__()\n        o, h, q, a, s, b, f, style = [], [], [], [], [], [], [], []\n        for style_name in style_keys:\n            dict = data[style_name]['motion']\n            o += dict['offsets']\n            h += dict['hip_pos']\n            q += dict['quats']\n            a += dict['A']\n            s += dict['S']\n            b += dict['B']\n            f += dict['F']\n            for i in range(len(dict['offsets'])):\n                style.append(random.sample(data[style_name]['style'], 1)[0])\n\n        motion = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f, \"style\": style}\n        self.data = motion\n\n    def __getitem__(self, item):\n        keys = [\"hip_pos\", \"quats\", \"offsets\", \"A\", \"S\", \"B\", \"F\"]\n        dict = {key: self.data[key][item][0] for key in keys}\n        dict[\"style\"] = self.data[\"style\"][item]\n        return {**dict}\n\n    def __len__(self):\n        return len(self.data[\"offsets\"])", "\ndef skating_loss(pred_seq):\n    num_joints = 23\n    # pred_seq = pred_seq.transpose(1, 2)\n    pred_seq = pred_seq.view(pred_seq.shape[0], pred_seq.shape[1], num_joints, 3)\n\n    foot_seq = pred_seq[:, :, [3, 4, 7, 8], :]\n    v = torch.sqrt(\n        torch.sum((foot_seq[:, 1:, :, [0, 1, 2]] - foot_seq[:, :-1, :, [0, 1, 2]]) ** 2, dim=3, keepdim=True))\n    #  v[v<1]=0\n    # v=torch.abs(foot_seq[:,1:,:,[0,2]]-foot_seq[:,:-1,:,[0,2]])\n    ratio = torch.abs(foot_seq[:, 1:, :, 1:2]) / 2.5  # 2.5\n    exp = torch.clamp(2 - torch.pow(2, ratio), 0, 1)\n    c = np.sum(exp.detach().numpy() > 0)\n\n    s = (v * exp)\n    c = max(c, 1)\n    m = torch.sum(s) / c\n    # m = torch.max(s)\n    return m", "\nclass Condition():\n    def __init__(self, length, x, t):\n        self.length = length\n        self.x = x\n        self.t = t\n\n    def get_name(self):\n        return \"{}_{}_{}\".format(self.length, self.x, self.t)\n", "\n\nclass GetLoss():\n    def __init__(self):\n        pass\n\n    def __call__(self, data, mA, mB):\n        return 0.0\n\ndef flatjoints(x):\n    \"\"\"\n    Shorthand for a common reshape pattern. Collapses all but the two first dimensions of a tensor.\n    :param x: Data tensor of at least 3 dimensions.\n    :return: The flattened tensor.\n    \"\"\"\n    return x.reshape((x.shape[0], x.shape[1], -1))", "\ndef flatjoints(x):\n    \"\"\"\n    Shorthand for a common reshape pattern. Collapses all but the two first dimensions of a tensor.\n    :param x: Data tensor of at least 3 dimensions.\n    :return: The flattened tensor.\n    \"\"\"\n    return x.reshape((x.shape[0], x.shape[1], -1))\n\ndef fast_npss(gt_seq, pred_seq):\n    \"\"\"\n    Computes Normalized Power Spectrum Similarity (NPSS).\n\n    This is the metric proposed by Gropalakrishnan et al (2019).\n    This implementation uses numpy parallelism for improved performance.\n\n    :param gt_seq: ground-truth array of shape : (Batchsize, Timesteps, Dimension)\n    :param pred_seq: shape : (Batchsize, Timesteps, Dimension)\n    :return: The average npss metric for the batch\n    \"\"\"\n    # Fourier coefficients along the time dimension\n    gt_fourier_coeffs = np.real(np.fft.fft(gt_seq, axis=1))\n    pred_fourier_coeffs = np.real(np.fft.fft(pred_seq, axis=1))\n\n    # Square of the Fourier coefficients\n    gt_power = np.square(gt_fourier_coeffs)\n    pred_power = np.square(pred_fourier_coeffs)\n\n    # Sum of powers over time dimension\n    gt_total_power = np.sum(gt_power, axis=1)\n    pred_total_power = np.sum(pred_power, axis=1)\n\n    # Normalize powers with totals\n    gt_norm_power = gt_power / gt_total_power[:, np.newaxis, :]\n    pred_norm_power = pred_power / pred_total_power[:, np.newaxis, :]\n\n    # Cumulative sum over time\n    cdf_gt_power = np.cumsum(gt_norm_power, axis=1)\n    cdf_pred_power = np.cumsum(pred_norm_power, axis=1)\n\n    # Earth mover distance\n    emd = np.linalg.norm((cdf_pred_power - cdf_gt_power), ord=1, axis=1)\n\n    # Weighted EMD\n    power_weighted_emd = np.average(emd, weights=gt_total_power)\n\n    return power_weighted_emd", "\ndef fast_npss(gt_seq, pred_seq):\n    \"\"\"\n    Computes Normalized Power Spectrum Similarity (NPSS).\n\n    This is the metric proposed by Gropalakrishnan et al (2019).\n    This implementation uses numpy parallelism for improved performance.\n\n    :param gt_seq: ground-truth array of shape : (Batchsize, Timesteps, Dimension)\n    :param pred_seq: shape : (Batchsize, Timesteps, Dimension)\n    :return: The average npss metric for the batch\n    \"\"\"\n    # Fourier coefficients along the time dimension\n    gt_fourier_coeffs = np.real(np.fft.fft(gt_seq, axis=1))\n    pred_fourier_coeffs = np.real(np.fft.fft(pred_seq, axis=1))\n\n    # Square of the Fourier coefficients\n    gt_power = np.square(gt_fourier_coeffs)\n    pred_power = np.square(pred_fourier_coeffs)\n\n    # Sum of powers over time dimension\n    gt_total_power = np.sum(gt_power, axis=1)\n    pred_total_power = np.sum(pred_power, axis=1)\n\n    # Normalize powers with totals\n    gt_norm_power = gt_power / gt_total_power[:, np.newaxis, :]\n    pred_norm_power = pred_power / pred_total_power[:, np.newaxis, :]\n\n    # Cumulative sum over time\n    cdf_gt_power = np.cumsum(gt_norm_power, axis=1)\n    cdf_pred_power = np.cumsum(pred_norm_power, axis=1)\n\n    # Earth mover distance\n    emd = np.linalg.norm((cdf_pred_power - cdf_gt_power), ord=1, axis=1)\n\n    # Weighted EMD\n    power_weighted_emd = np.average(emd, weights=gt_total_power)\n\n    return power_weighted_emd", "\nclass GetLastRotLoss(GetLoss):\n    def __init__(self):\n        super(GetLastRotLoss, self).__init__()\n\n    def __call__(self, data, mA, mB):\n        a = data[mA]['rot']\n        b = data[mB]['rot']\n        return torch.mean(torch.sqrt(\n            torch.sum((a[:, -1:, ...] - b[:, -1:, ...]) ** 2.0, dim=(2, 3))))", "\nclass GetNPSS(GetLoss):\n    def __init__(self):\n        super(GetNPSS, self).__init__()\n\n    def __call__(self, data, mA, mB):\n        a = data[mA]['rot']\n        b = data[mB]['rot']\n        return fast_npss(flatjoints(a), flatjoints(b))\n\nclass GetLastPosLoss(GetLoss):\n    def __init__(self, x_mean, x_std):\n        super(GetLastPosLoss, self).__init__()\n        self.x_mean = x_mean\n        self.x_std = x_std\n\n    def __call__(self, data, mA, mB):\n        a = data[mA]['pos'].flatten(-2, -1)\n        b = data[mB]['pos'].flatten(-2, -1)\n        a = (a - self.x_mean) / self.x_std\n        b = (b - self.x_mean) / self.x_std\n        return torch.mean(torch.sqrt(\n            torch.sum((a[:, -1:, ...] - b[:, -1:, ...]) ** 2.0, dim=(2))))", "\nclass GetLastPosLoss(GetLoss):\n    def __init__(self, x_mean, x_std):\n        super(GetLastPosLoss, self).__init__()\n        self.x_mean = x_mean\n        self.x_std = x_std\n\n    def __call__(self, data, mA, mB):\n        a = data[mA]['pos'].flatten(-2, -1)\n        b = data[mB]['pos'].flatten(-2, -1)\n        a = (a - self.x_mean) / self.x_std\n        b = (b - self.x_mean) / self.x_std\n        return torch.mean(torch.sqrt(\n            torch.sum((a[:, -1:, ...] - b[:, -1:, ...]) ** 2.0, dim=(2))))", "\nclass GetPosLoss(GetLoss):\n    def __init__(self, x_mean, x_std):\n        super(GetPosLoss, self).__init__()\n        self.x_mean = x_mean\n        self.x_std = x_std\n\n    def __call__(self, data, mA, mB):\n        a = data[mA]['pos'].flatten(-2, -1)\n        b = data[mB]['pos'].flatten(-2, -1)\n        a = (a - self.x_mean) / self.x_std\n        b = (b - self.x_mean) / self.x_std\n        return torch.mean(torch.sqrt(\n            torch.sum((a[:, :, ...] - b[:, :, ...]) ** 2.0, dim=(2))))", "\nclass GetContactLoss(GetLoss):\n    def __init__(self):\n        super(GetContactLoss, self).__init__()\n\n    def __call__(self, data, mA, mB=None):\n        return skating_loss(data[mA]['pos'])\n\n\ndef calculate_diversity(data):\n    loss_function = torch.nn.MSELoss()\n    N = data.shape[-1]\n    loss = 0\n    for i in range(N):\n        for j in range(i + 1, N):\n            loss += loss_function(data[..., i], data[..., j]).detach().cpu().numpy()\n    loss /= (N * N / 2)\n    return loss", "\ndef calculate_diversity(data):\n    loss_function = torch.nn.MSELoss()\n    N = data.shape[-1]\n    loss = 0\n    for i in range(N):\n        for j in range(i + 1, N):\n            loss += loss_function(data[..., i], data[..., j]).detach().cpu().numpy()\n    loss /= (N * N / 2)\n    return loss", "\nclass GetDiversity(GetLoss):\n    def __init__(self, style_seq):\n        super(GetDiversity, self).__init__()\n        self.style_seq = style_seq\n\n    def __call__(self, data, mA, mB=None):\n        pos = data[mA]['pos']\n        DIVs = []\n        start_id = 0\n        for length in self.style_seq:\n            if length:\n                pos_seq = pos[start_id:start_id + length]\n                div = calculate_diversity(pos_seq)\n            else:\n                div = 0\n            DIVs.append(div)\n            start_id += length\n        dic = dict(zip(range(1, len(DIVs) + 1), DIVs))\n        for key in dic.keys():\n            print(dic[key])\n        DIVs = np.array(DIVs)\n        weight = np.array(self.style_seq)\n        # mask = self.style_seq != 0\n        # weight = self.style_seq[mask]\n        avg_div = np.average(DIVs, weights=weight)\n        return avg_div", "\n\nclass GetFID(GetLoss):\n    def __init__(self, style_seq):\n        super(GetFID, self).__init__()\n        self.style_seq = style_seq\n        self.norm_factor = 1\n    def __call__(self, data, mA, mB):\n        if mA == mB:\n            return 0.0\n        import time\n        t = time.time()\n        FIDs = []\n        start_id_noaug = 0\n        start_id_aug = 0\n        # latentA = data[mB]['latent'][start_id:start_id + self.style_seq[0]]\n        for i, length_noaug in enumerate(self.style_seq):\n            if length_noaug:\n                length_aug = length_noaug\n                # latentA = data[mB]['latent'][start_id:start_id + length//2].flatten(1, -1)\n                # latentB = data[mB]['latent'][start_id + length//2:start_id + length].flatten(1, -1)\n                latentA = data[mA]['latent'][start_id_noaug:start_id_noaug + length_noaug] / self.norm_factor\n                # latentA = latentA.flatten(1, -1)\n                # latentA = latentA.flatten(0, 1)  # v\n                latentA = latentA[:, :, :, 0].transpose(1, 2).flatten(0, 1)  # latent\n                # latentA = latentA.flatten(0, 1).flatten(1, 2)  # phase\n                latentB = data[mB]['latent'][start_id_aug:start_id_aug + length_aug] / self.norm_factor\n                latentB = latentB[:, :, :, 0].transpose(1, 2).flatten(0, 1)  # latent\n                # latentB = latentB.flatten(0, 1).flatten(1, 2)  # phase\n                # latentB = latentB.flatten(0, 1)   # v\n                # latentB = latentB.flatten(1, -1)\n                fid = calculate_fid(latentA, latentB)\n            else:\n                fid = 0\n            FIDs.append(fid)\n            start_id_noaug += length_noaug\n            start_id_aug += length_aug\n        round_func = lambda x: round(x, 2)\n        dic = dict(zip(range(1, len(FIDs) + 1), map(round_func, FIDs)))\n        for key in dic.keys():\n            print(dic[key])\n        FIDs = np.array(FIDs)\n        weight = np.array(self.style_seq)\n        # mask = self.style_seq != 0\n        # weight = self.style_seq[mask]\n        avg_fid = np.average(FIDs, weights=weight)\n        max_fid = np.max(FIDs)\n        print(avg_fid)\n        print(max_fid)\n        print(f'cost : {time.time() - t:.4f}s')\n        # return [avg_fid, max_fid]\n        return avg_fid", "\nclass GetAcc(GetLoss):\n    def __init__(self, gt_label, style_seq):\n        super(GetAcc, self).__init__()\n        self.style_seq = style_seq\n        self.gt_label = gt_label\n        pass\n\n    def __call__(self, data, mA, mB):\n        print(mA)\n        start_id_noaug = 0\n        correct = 0\n        correct_list = []\n        for i, length_noaug in enumerate(self.style_seq):\n\n            gt_label = self.gt_label[i]\n            if length_noaug:\n                pred_label = data[mA]['label'][start_id_noaug:start_id_noaug + length_noaug]\n                correct_num = ((pred_label == gt_label).sum()).detach().cpu().numpy()\n                correct += correct_num\n                correct_list.append(correct_num / length_noaug)\n                start_id_noaug += length_noaug\n            else:\n                correct_list.append(0.)\n\n        for i in correct_list:\n            print(i)\n        print('###########################')\n        return correct / sum(self.style_seq)", "\ndef reconstruct_motion(models, function, X, Q, A, S, tar_pos, tar_quat, offsets, skeleton: Skeleton,\n                       condition: Condition, n_past=10, n_future=10):\n    # \"\"\"\n    # Evaluate naive baselines (zero-velocity and interpolation) for transition generation on given data.\n    # :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n    # :param Q: Local quaternions array of shape (B, T, J, 4)\n    # :param x_mean : Mean vector of local positions of shape (1, J*3, 1)\n    # :param x_std: Standard deviation vector of local positions (1, J*3, 1)\n    # :param offsets: Local bone offsets tensor of shape (1, 1, J-1, 3)\n    # :param parents: List of bone parents indices defining the hierarchy\n    # :param out_path: optional path for saving the results\n    # :param n_past: Number of frames used as past context\n    # :param n_future: Number of frames used as future context (only the first frame is used as the target)\n    # :return: Results dictionary\n    # \"\"\"\n\n    data = {}\n\n    torch.cuda.empty_cache()\n    location_x = condition.x\n    duration_t = condition.t\n    n_trans = int(condition.length * duration_t)\n    target_id = condition.length + n_past\n\n\n    X[:, 12:, 0, [0, 2]] = X[:, 12:, 0, [0, 2]] + (\n            X[:, target_id:target_id + 1, 0, [0, 2]] - X[:, 10:11, 0, [0, 2]]) * location_x\n\n    # Format the data for the current transition lengths. The number of samples and the offset stays unchanged.\n    curr_window = target_id + n_future\n    curr_x = X[:, :curr_window, ...]\n    curr_q = Q[:, :curr_window, ...]\n    global_poses, global_quats = skeleton.forward_kinematics(curr_q, offsets, curr_x)\n\n\n\n    def remove_disconti(quats):\n        ref_quat = global_quats[:, n_past - 1:n_past]\n        return remove_quat_discontinuities(torch.cat((ref_quat, quats), dim=1))[:, 1:]\n\n    def get_gt_pose(n_past, target_id):\n        trans_gt_global_poses, trans_gt_global_quats = global_poses[:, n_past: target_id, ...], global_quats[:,\n                                                                                                n_past: target_id, ...]\n        trans_gt_global_quats = remove_disconti(trans_gt_global_quats)\n        return trans_gt_global_poses, trans_gt_global_quats\n\n\n    gt_global_pos, gt_global_quats = get_gt_pose(n_past, target_id)\n    data['gt'] = {}\n    data['gt']['pos'] = gt_global_pos\n    data['gt']['rot'] = gt_global_quats\n    data['gt']['offsets'] = offsets\n\n    for key in models:\n        resQ, resX = function[key](models[key], X, Q, A, S, tar_pos, tar_quat, offsets, skeleton, n_trans, target_id)\n        resQ = remove_disconti(resQ)\n        data[key] = {}\n        data[key]['rot'] = resQ\n        data[key]['pos'] = resX\n\n        # data[key]['phase'] = F.normalize(phase[3], dim=-1)\n        data[key][\"offsets\"] = offsets\n\n        ############################# calculate diversity #################################\n        # data['gt']['phase'] = F.normalize(gt_phase, dim=-1)\n        # data['div_test'] = {}\n        # num_samples = 10\n        # N,T,J,C = X.shape\n        # X = X.repeat(num_samples,1,1,1)\n        # Q = Q.repeat(num_samples,1,1,1)\n        # A = A.repeat(num_samples,1,1,1)\n        # S = S.repeat(num_samples,1,1,1)\n        # tar_pos = tar_pos.repeat(num_samples,1,1,1)\n        # tar_quat = tar_quat.repeat(num_samples,1,1,1)\n        # offsets = offsets.repeat(num_samples,1,1)\n        # x, q = [], []\n        # resQ, resX = function[key](models[key], X, Q, A, S, tar_pos, tar_quat, offsets, skeleton, n_trans,\n        #                         target_id, ifnoise=True)\n        #\n        # data['div_test']['pos'] = resX.view(num_samples,N,resX.shape[1],resX.shape[2],3).permute(1,2,3,4,0).cpu()\n        # data['div_test']['rot'] = resQ.view(num_samples,N,resX.shape[1],resX.shape[2],4).permute(1,2,3,4,0).cpu()\n        ####################################################################################\n\n    return data, global_poses[:, n_past - 1:n_past, :, :], global_quats[:, n_past - 1:n_past, :, :]", "\n\ndef save_data_to_binary(path, data):\n    f = open('./benchmark_result/'+ path, \"wb+\")\n    pos = {}\n    for condition in data.keys():\n        pos[condition] = {}\n        for method in data[condition].keys():\n            pos[condition][method] = data[condition][method]['pos']\n            data[condition][method]['pos'] = data[condition][method]['pos'][:,:,0:1,:]#b,t,1,3\n    pickle.dump(data, f)\n    for condition in data.keys():\n        for method in data[condition].keys():\n            data[condition][method]['pos'] = pos[condition][method]\n\n    f.close()", "\ndef load_data_from_binary(path, skeleton:Skeleton):\n    # condition{method{data}}\n    f = open('./benchmark_result/'+path, \"rb\")\n    data = pickle.load(f)\n    for condition in data.keys():\n        for method in data[condition].keys():\n            pos = data[condition][method]['pos'] #b,t,1,3\n            rot = data[condition][method]['rot'] #b,t,23,4\n            offsets = data[condition][method]['offsets']\n            data[condition][method]['pos'] = skeleton.global_rot_to_global_pos(rot, offsets, pos)\n    f.close()\n    return data", "\ndef print_result(metrics, out_path=None):\n    def format(dt, name, value):\n        s = \"{0: <16}\"\n        for i in range(len(value)):\n            s += \" & {\" + str(i + 1) + \":\" + dt + \"}\"\n        return s.format(name, *value)\n\n    print()\n    for metric_name in metrics.keys():\n        if metric_name != \"NPSS\":\n            print(\"=== {} losses ===\".format(metric_name))\n            conditions = list(metrics[metric_name].keys())\n            print(format(\"<6\", \"Conditions\", conditions))\n            all_methods = list(metrics[metric_name][conditions[0]].keys())\n            for method_name in all_methods:\n                method_metric = [metrics[metric_name][condition][method_name] for condition in conditions]\n                print(format(\"6.3f\", method_name, method_metric))\n        else:\n            print(\"=== NPSS ===\".format(metric_name))\n            conditions = list(metrics[metric_name].keys())\n            print(format(\"<6\", \"Conditions\", conditions))\n            all_methods = list(metrics[metric_name][conditions[0]].keys())\n            for method_name in all_methods:\n                method_metric = [metrics[metric_name][condition][method_name] for condition in conditions]\n                # print(method_name)\n                # print(method_metric)\n                print(format(\"6.5f\", method_name, method_metric))\n    if out_path is not None:\n        res_txt_file = open(os.path.join(out_path, 'benchmark.txt'), \"a\")\n        for metric_name in metrics.keys():\n            res_txt_file.write(\"\\n=== {} losses ===\\n\".format(metric_name))\n            conditions = list(metrics[metric_name].keys())\n            res_txt_file.write(format(\"<6\", \"Conditions\", conditions) + \"\\n\")\n            all_methods = list(metrics[metric_name][conditions[0]].keys())\n            for method_name in all_methods:\n                method_metric = [metrics[metric_name][condition][method_name] for condition in conditions]\n                res_txt_file.write(format(\"6.3f\", method_name, method_metric) + \"\\n\")\n        print(\"\\n\")\n        res_txt_file.close()", "\n\ndef get_vel(pos):\n    return pos[:, 1:] - pos[:, :-1]\n\n\ndef get_gt_latent(style_encoder, rot, pos, batch_size=1000):\n    glb_vel, glb_pos, glb_rot, root_rotation = BatchProcessDatav2().forward(rot, pos)\n    stard_id = 0\n    data_size = glb_vel.shape[0]\n    init = True\n    while stard_id < data_size:\n        length = min(batch_size, data_size - stard_id)\n        mp_batch = {}\n        mp_batch['glb_rot'] = quat_to_or6D(glb_rot[stard_id: stard_id + length]).cuda()\n        mp_batch['glb_pos'] = glb_pos[stard_id: stard_id + length].cuda()\n        mp_batch['glb_vel'] = glb_vel[stard_id: stard_id + length].cuda()\n        latent = style_encoder.cal_latent(mp_batch).cpu()\n        if init:\n            output = torch.empty((data_size,) + latent.shape[1:])\n            init = False\n        output[stard_id: stard_id + length] = latent\n        stard_id += length\n    return output", "\n\n\ndef calculate_stat(conditions, dataLoader, data_size, function, models, skeleton, load_from_dict = False,data_name = None, window_size=1500):\n    ##################################\n    # condition{method{data}}\n    if load_from_dict:\n        print('load from calculated stat ...')\n        t = time.time()\n        outputs = load_data_from_binary(data_name, skeleton)\n        print('loading data costs {} s'.format(time.time() - t))\n    else:\n        outputs = {}\n        with torch.no_grad():\n            for condition in conditions:\n                print('Reconstructing motions for condition: length={}, x={}, t={} ...'.format(condition.length, condition.x,condition.t))\n                start_id = 0\n                outputs[condition.get_name()] = {}\n                outputs[condition.get_name()] = {\"gt\": {}, \"div_test\": {}}  # , \"interp\":{}, \"zerov\":{}}\n                output = outputs[condition.get_name()]\n                for key in models.keys():\n                    output[key] = {}\n                for batch in dataLoader:\n\n                    \"\"\"\n                    :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n                    :param Q: Local quaternions array of shape (B, T, J, 4)\n                    \"\"\"\n                    t1 = time.time()\n\n                    A = batch['A'] / 0.1\n                    S = batch['S']\n\n                    gp, gq = skeleton.forward_kinematics(batch['quats'], batch['offsets'], batch['hip_pos'])\n                    tar_gp, tar_gq = skeleton.forward_kinematics(batch['style']['quats'], batch['style']['offsets'],\n                                                                 batch['style']['hip_pos'])\n\n                    local_pos, global_quat = BatchRotateYCenterXZ().forward(gp, gq, 10)\n                    tar_pos, tar_quat = BatchRotateYCenterXZ().forward(tar_gp, tar_gq, 10)\n                    local_quat = skeleton.inverse_kinematics_quats(global_quat)\n                    local_quat = (remove_quat_discontinuities(local_quat.cpu()))\n                    hip_pos = local_pos[:, :, 0:1, :]\n                    data, last_pos, last_rot = reconstruct_motion(models, function, hip_pos.cuda(), local_quat.cuda(),\n                                                                  A.cuda(), S.cuda(), tar_pos.cuda(),\n                                                                  tar_quat.cuda(), batch['offsets'].cuda(), skeleton,\n                                                                  condition)\n                    t2 = time.time()\n\n                    if start_id == 0:\n                        for method in data.keys():\n                            for prop in data[method].keys():\n                                output[method][prop] = torch.empty((data_size,) + data[method][prop].shape[1:])\n                    for method in data.keys():\n                        batch_size = data[method]['pos'].shape[0]\n                        props = list(data[method].keys())\n                        for prop in props:\n                            output[method][prop][start_id:start_id + batch_size] = data[method][prop]\n                            del data[method][prop]\n                    print(\"batch_id : {} - {}\".format(start_id, start_id + batch_size))\n                    start_id += batch_size\n                    print(\"time cost t2 - t1 : {}\".format(t2 - t1))\n            # print('saving data to binary file ...')\n            # save_data_to_binary(data_name, outputs)\n    for condition in conditions:\n        if condition.length == 40:\n            print('Reconstructing latent for condition: length={}, x={}, t={} ...'.format(condition.length, condition.x, condition.t))\n            t = time.time()\n            data = outputs[condition.get_name()]\n            for method in data.keys():\n                if method == \"div_test\":# or method == \"gt\":\n                    continue\n                start_id = 0\n                length = data[method]['rot'].shape[0]\n                while start_id < length:\n                    window = min(window_size, length-start_id)\n                    rot, pos = data[method]['rot'][start_id:start_id+window], data[method]['pos'][start_id:start_id+window]\n                    # rot, pos = torch.cat((last_rot, data[method]['rot']), dim=1), torch.cat((last_pos, data[method]['pos']), dim=1)\n                    # glb_vel,glb_pos,glb_rot,root_rotation = BatchProcessDatav2().forward(data[method]['rot'], data[method]['pos'])\n                    glb_vel, glb_pos, glb_rot, root_rotation = BatchProcessDatav2().forward(rot, pos)\n                    # data[method][\"latent\"] = glb_vel.flatten(-2,-1).cpu()    # use vel\n                    mp_batch = {}\n                    mp_batch['glb_rot'] = quat_to_or6D(glb_rot).cuda()\n                    mp_batch['glb_pos'] = glb_pos.cuda()\n                    mp_batch['glb_vel'] = glb_vel.cuda()\n                    # latent = style_encoder.cal_latent(mp_batch).cpu() # use latent\n                    # label = style_encoder.cal_label(mp_batch).cpu()\n                    # if start_id == 0:\n                    #     data[method]['latent'] = torch.empty((length,) + latent.shape[1:])\n                    #     # data[method]['label'] = torch.empty((length,) + label.shape[1:])\n                    #\n                    # data[method]['latent'][start_id:start_id + window] = latent\n                    # data[method]['label'][start_id:start_id + window] = label\n                    # del latent\n                    # del latent\n                    # del label\n                    start_id += window\n            print('cost {} s'.format(time.time() - t))\n    return outputs", "\n\n\ndef calculate_benchmark(data, benchmarks):\n    metrics = {}\n    for metric_key in benchmarks.keys():\n        print('Computing errors for {}'.format(metric_key))\n        metrics[metric_key] = {}\n        if metric_key == \"Diversity\":\n            for condition in data.keys():\n                print(\"condition : {}\".format(condition))\n                metrics[metric_key][condition] = {}\n                metric = benchmarks[metric_key]\n                metrics[metric_key][condition][\"div_test\"] = metric(data[condition], \"div_test\")\n        else:\n            for condition in data.keys():\n                print(\"condition : {}\".format(condition))\n                # if metric_key == 'FID' and condition.length != 40:\n                #     continue\n                metrics[metric_key][condition] = {}\n                metric = benchmarks[metric_key]\n                for method in data[condition].keys():\n                    if method == \"div_test\":\n                        continue\n                    metrics[metric_key][condition][method] = metric(data[condition], method, \"gt\")\n    return metrics", "\ndef benchmarks(args,load_stat=False, data_name = None):\n\n    # set dataset\n    style_start = 0\n    style_end = 90\n    batch_size = 500\n    style_loader = StyleLoader()\n    print('loading dataset ...')\n    stat_file = 'style100_benchmark_65_25'\n    style_loader.load_from_binary(stat_file)\n    style_loader.load_skeleton_only()\n    skeleton = style_loader.skeleton\n    stat_dict = style_loader.load_part_to_binary(\"style100_benchmark_stat\")\n    mean, std = stat_dict['pos_stat']\n    mean, std = torch.from_numpy(mean).view(23 * 3), torch.from_numpy(std).view(23 * 3)\n\n    # set style\n    style_keys = list(style_loader.all_motions.keys())[style_start:style_end]\n    dataSet = BenchmarkDataSet(style_loader.all_motions, style_keys)\n\n    data_size = len(dataSet)\n    dataLoader = DataLoader(dataSet, batch_size=batch_size, num_workers=0, shuffle=False)\n    style_seqs = [len(style_loader.all_motions[style_key]['motion']['quats']) for style_key in style_keys]\n\n    gt_style = []\n\n    models, function = load_model(args)\n    for key in models:\n        for param in models[key].parameters():\n            param.requires_grad = False\n    # set condition and metrics for the benchmarks\n    conditions, metrics = Set_Condition_Metric(gt_style, mean, std, style_seqs)\n\n    outputs = calculate_stat(conditions, dataLoader, data_size, function, models, skeleton, load_from_dict=load_stat, data_name=data_name)\n    metrics_stat = calculate_benchmark(outputs, metrics)\n    print_result(metrics_stat, './')", "\n\ndef Set_Condition_Metric(gt_style, mean, std, style_seqs):\n    pos_loss = GetLastPosLoss(mean, std)\n    gp_loss = GetPosLoss(mean,std)\n    rot_loss = GetLastRotLoss()\n    contact_loss = GetContactLoss()\n    npss_loss = GetNPSS()\n    fid = GetFID(style_seqs)\n    diversity = GetDiversity(style_seqs)\n    acc = GetAcc(gt_style, style_seqs)\n    # set metrics\n    # for example: metrics = {\"Diversity\" : diversity}#{\"LastPos\": pos_loss, \"Contact\": contact_loss}\n    metrics = {\"NPSS\": npss_loss, \"gp\": gp_loss, \"Contact\": contact_loss}\n    # set conditions\n    conditions = [Condition(10, 0, 1), Condition(20, 0, 1), Condition(40, 0, 1),]\n    return conditions, metrics", "\n\nif __name__ == '__main__':\n    random.seed(0)\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(\"--model_path\", type=str,default='./results/Transitionv2_style100/myResults/117/m_save_model_205')\n    parser.add_argument(\"--model_name\",type=str,default=\"RSMT\")\n    args = parser.parse_args()\n   # args.model_path = './results/Transitionv2_style100/myResults/128/m_save_model_last'\n    benchmarks(args,load_stat=False, data_name='benchmark_test_data.dat')"]}
{"filename": "benchmarkStyle100_withStyle.py", "chunked_list": ["import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply\nfrom torch.utils.data import DataLoader, Dataset\n\nimport src.Datasets.BaseLoader as mBaseLoader\nimport src.Net.TransitionNet", "import src.Datasets.BaseLoader as mBaseLoader\nimport src.Net.TransitionNet\nfrom src.Datasets.BaseLoader import BasedDataProcessor\nfrom src.Net.TransitionPhaseNet import TransitionNet_phase\nfrom src.geometry.quaternions import quat_inv, quat_mul, quat_mul_vec, from_to_1_0_0\nfrom src.utils.BVH_mod import Skeleton\nfrom src.utils.np_vector import interpolate_local, remove_quat_discontinuities\n\n\nclass BatchRotateYCenterXZ(torch.nn.Module):\n    def __init__(self):\n        super(BatchRotateYCenterXZ, self).__init__()\n    def forward(self,global_positions,global_quats,ref_frame_id):\n        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id+1, 5:6, :] - global_positions[:, ref_frame_id:ref_frame_id+1, 1:2, :],\n                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device),dim=-1)\n        root_rotation = from_to_1_0_0(ref_vector)\n        ref_hip = torch.mean(global_positions[:,:,0:1,[0,2]],dim=(1),keepdim=True)\n        global_positions[...,[0,2]] = global_positions[...,[0,2]] - ref_hip\n        global_positions = quaternion_apply(root_rotation, global_positions)\n        global_quats = quaternion_multiply(root_rotation, global_quats)\n\n        return global_positions,global_quats", "\nclass BatchRotateYCenterXZ(torch.nn.Module):\n    def __init__(self):\n        super(BatchRotateYCenterXZ, self).__init__()\n    def forward(self,global_positions,global_quats,ref_frame_id):\n        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id+1, 5:6, :] - global_positions[:, ref_frame_id:ref_frame_id+1, 1:2, :],\n                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device),dim=-1)\n        root_rotation = from_to_1_0_0(ref_vector)\n        ref_hip = torch.mean(global_positions[:,:,0:1,[0,2]],dim=(1),keepdim=True)\n        global_positions[...,[0,2]] = global_positions[...,[0,2]] - ref_hip\n        global_positions = quaternion_apply(root_rotation, global_positions)\n        global_quats = quaternion_multiply(root_rotation, global_quats)\n\n        return global_positions,global_quats", "\n\nclass BenchmarkProcessor(BasedDataProcessor):\n    def __init__(self):\n        super(BenchmarkProcessor, self).__init__()\n        # self.process = BatchRotateYCenterXZ()\n\n    def __call__(self, dict, skeleton, motionDataLoader, ratio=1.0):\n        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n        stat = self._calculate_stat(offsets, hip_pos, quats, skeleton, 1.0)\n        return {\"offsets\": offsets, \"hip_pos\": hip_pos, \"quats\": quats, **stat}\n\n    def _concat(self, local_quat, offsets, hip_pos, ratio=0.2):\n        local_quat = torch.from_numpy(np.concatenate((local_quat), axis=0))\n        offsets = torch.from_numpy(np.concatenate((offsets), axis=0))\n        hip_pos = torch.from_numpy(np.concatenate((hip_pos), axis=0))\n        return local_quat, offsets, hip_pos\n\n    def calculate_pos_statistic(self, pos):\n        '''pos:N,T,J,3'''\n        mean = np.mean(pos, axis=(0, 1))\n        std = np.std(pos, axis=(0, 1))\n\n        return mean, std\n\n    def _calculate_stat(self, offsets, hip_pos, local_quat, skeleton, ratio):\n        local_quat, offsets, hip_pos = self._concat(local_quat, offsets, hip_pos, ratio)\n        local_quat = local_quat.to(torch.float64)\n        offsets = offsets.to(torch.float64)\n        hip_pos = hip_pos.to(torch.float64)\n        global_positions, global_rotations = skeleton.forward_kinematics(local_quat, offsets, hip_pos)\n        global_positions, global_quats, root_rotation = BatchRotateYCenterXZ().forward(global_positions,\n                                                                                       global_rotations, 10)\n        # global_positions = torch.cat((local_positions[:,:,0:1,:],local_positions[:,:,1:]+local_positions[:,:,0:1,:]),dim=-2)\n        pos_mean, pos_std = self.calculate_pos_statistic(global_positions.cpu().numpy())\n        pos_mean.astype(np.float)\n        pos_std.astype(np.float)\n        return {\"pos_stat\": [pos_mean, pos_std]}", "\n\nclass BenchmarkDataSet(Dataset):\n    def __init__(self, data, style_keys):\n        super(BenchmarkDataSet, self).__init__()\n        o, h, q, a, s, b, f, style = [], [], [], [], [], [], [], []\n        for style_name in style_keys:\n            dict = data[style_name]['motion']\n            o += dict['offsets']\n            h += dict['hip_pos']\n            q += dict['quats']\n            a += dict['A']\n            s += dict['S']\n            b += dict['B']\n            f += dict['F']\n            for i in range(len(dict['offsets'])):\n                style.append( data[style_name]['style'])\n\n\n        motion = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f, \"style\":style}\n        self.data = motion\n        # motions = {\"offsets\": o, \"hip_pos\": h, \"quats\": q, \"A\": a, \"S\": s, \"B\": b, \"F\": f}\n\n    def __getitem__(self, item):\n\n        keys = [\"hip_pos\",\"quats\",\"offsets\",\"A\",\"S\",\"B\",\"F\"]\n        dict = {key: self.data[key][item][0] for key in keys}\n        dict[\"style\"] = self.data[\"style\"][item]\n        return {**dict}\n\n    def __len__(self):\n        return len(self.data[\"offsets\"])", "\ndef eval_sample(model, X, Q, x_mean, x_std, pos_offset, skeleton: Skeleton, length, param):\n    # FOR EXAMPLE\n    model = model.eval()\n    # target frame\n    target_id = None\n    if \"target_id\" in param:\n        target_id = param[\"target_id\"]\n\n    pred_hips = torch.zeros(size=(X.shape[0], length, 1, 3))\n    pred_quats = torch.zeros(size=(X.shape[0], length, 22, 4))\n    GX, GQ = skeleton.forward_kinematics(pred_quats, pos_offset, pred_hips)\n\n    x_mean = x_mean.view(skeleton.num_joints, 3)\n    x_std = x_std.view(skeleton.num_joints, 3)\n    # normalized\n    GX = (GX - x_mean.flatten(-2, -1)) / x_std.flatten(-2, -1)\n    GX = GX.transpose(1, 2)\n\n    return GQ, GX", "\n\ndef load_model():\n    model_dict, function_dict, param_dict = {}, {}, {}\n\n    model_dict['Transition'] = torch.load('./results/Transitionv2_style100/myResults/117/m_save_model_205')\n    function_dict['Transition'] = src.Net.TransitionPhaseNet.eval_sample\n    return model_dict, function_dict, param_dict\n\n\ndef catPosition(pos, pos_offset):\n    pos = pos.view(pos.shape[0], pos.shape[1], 1, 3)\n    return torch.cat((pos, pos_offset), 2)", "\n\ndef catPosition(pos, pos_offset):\n    pos = pos.view(pos.shape[0], pos.shape[1], 1, 3)\n    return torch.cat((pos, pos_offset), 2)\n\n\ndef torch_fk(lrot, lpos, parents, squeeze=True):\n    \"\"\"\n    Performs Forward Kinematics (FK) on local quaternions and local positions to retrieve global representations\n\n    :param lrot: tensor of local quaternions with shape (..., Nb of joints, 4)\n    :param lpos: tensor of local positions with shape (..., Nb of joints, 3)\n    :param parents: list of parents indices\n    :return: tuple of tensors of global quaternion, global positions\n    \"\"\"\n\n    gp, gr = [lpos[..., :1, :]], [lrot[..., :1, :]]\n\n    for i in range(1, len(parents)):\n        gp.append(quat_mul_vec(gr[parents[i]], lpos[..., i:i + 1, :]) + gp[parents[i]])\n\n        gr.append(quat_mul(gr[parents[i]], lrot[..., i:i + 1, :]))\n\n    rot, pos = torch.cat(gr, dim=-2), torch.cat(gp, dim=-2)\n    if (squeeze):\n        rot = rot.reshape(lrot.shape[0], lrot.shape[1], -1)\n        pos = pos.reshape(lrot.shape[0], lrot.shape[1], -1)\n\n    return rot, pos", "\n\ndef torch_quat_normalize(x, eps=1e-12):\n    assert x.shape[-1] == 3 or x.shape[-1] == 4 or x.shape[-1] == 2\n    # lgth =  torch.sqrt(torch.sum(x*x,dim=-1,keepdim=True))\n    lgth = torch.norm(x, dim=-1, keepdim=True)\n    return x / (lgth + eps)\n\n\ndef torch_ik_rot(grot, parents):\n    return torch.cat((\n        grot[..., :1, :],\n        quat_mul(quat_inv(grot[..., parents[1:], :]), grot[..., 1:, :]),\n    ), dim=-2)", "\ndef torch_ik_rot(grot, parents):\n    return torch.cat((\n        grot[..., :1, :],\n        quat_mul(quat_inv(grot[..., parents[1:], :]), grot[..., 1:, :]),\n    ), dim=-2)\n\n\ndef skating_loss(pred_seq):\n    num_joints = 23\n    pred_seq = pred_seq.transpose(1, 2)\n    pred_seq = pred_seq.view(pred_seq.shape[0], pred_seq.shape[1], num_joints, 3)\n\n    foot_seq = pred_seq[:, :, [3, 4, 7, 8], :]\n    v = torch.sqrt(\n        torch.sum((foot_seq[:, 1:, :, [0, 1, 2]] - foot_seq[:, :-1, :, [0, 1, 2]]) ** 2, dim=3, keepdim=True))\n    #  v[v<1]=0\n    # v=torch.abs(foot_seq[:,1:,:,[0,2]]-foot_seq[:,:-1,:,[0,2]])\n    ratio = torch.abs(foot_seq[:, 1:, :, 1:2]) / 2.5  # 2.5\n    exp = torch.clamp(2 - torch.pow(2, ratio), 0, 1)\n    c = np.sum(exp.detach().numpy() > 0)\n\n    s = (v * exp)\n    c = max(c, 1)\n    m = torch.sum(s) / c\n    # m = torch.max(s)\n    return m", "def skating_loss(pred_seq):\n    num_joints = 23\n    pred_seq = pred_seq.transpose(1, 2)\n    pred_seq = pred_seq.view(pred_seq.shape[0], pred_seq.shape[1], num_joints, 3)\n\n    foot_seq = pred_seq[:, :, [3, 4, 7, 8], :]\n    v = torch.sqrt(\n        torch.sum((foot_seq[:, 1:, :, [0, 1, 2]] - foot_seq[:, :-1, :, [0, 1, 2]]) ** 2, dim=3, keepdim=True))\n    #  v[v<1]=0\n    # v=torch.abs(foot_seq[:,1:,:,[0,2]]-foot_seq[:,:-1,:,[0,2]])\n    ratio = torch.abs(foot_seq[:, 1:, :, 1:2]) / 2.5  # 2.5\n    exp = torch.clamp(2 - torch.pow(2, ratio), 0, 1)\n    c = np.sum(exp.detach().numpy() > 0)\n\n    s = (v * exp)\n    c = max(c, 1)\n    m = torch.sum(s) / c\n    # m = torch.max(s)\n    return m", "\n\ndef fast_npss(gt_seq, pred_seq):\n    \"\"\"\n    Computes Normalized Power Spectrum Similarity (NPSS).\n\n    This is the metric proposed by Gropalakrishnan et al (2019).\n    This implementation uses numpy parallelism for improved performance.\n\n    :param gt_seq: ground-truth array of shape : (Batchsize, Timesteps, Dimension)\n    :param pred_seq: shape : (Batchsize, Timesteps, Dimension)\n    :return: The average npss metric for the batch\n    \"\"\"\n    # Fourier coefficients along the time dimension\n    gt_fourier_coeffs = np.real(np.fft.fft(gt_seq, axis=1))\n    pred_fourier_coeffs = np.real(np.fft.fft(pred_seq, axis=1))\n\n    # Square of the Fourier coefficients\n    gt_power = np.square(gt_fourier_coeffs)\n    pred_power = np.square(pred_fourier_coeffs)\n\n    # Sum of powers over time dimension\n    gt_total_power = np.sum(gt_power, axis=1)\n    pred_total_power = np.sum(pred_power, axis=1)\n\n    # Normalize powers with totals\n    gt_norm_power = gt_power / gt_total_power[:, np.newaxis, :]\n    pred_norm_power = pred_power / pred_total_power[:, np.newaxis, :]\n\n    # Cumulative sum over time\n    cdf_gt_power = np.cumsum(gt_norm_power, axis=1)\n    cdf_pred_power = np.cumsum(pred_norm_power, axis=1)\n\n    # Earth mover distance\n    emd = np.linalg.norm((cdf_pred_power - cdf_gt_power), ord=1, axis=1)\n\n    # Weighted EMD\n    power_weighted_emd = np.average(emd, weights=gt_total_power)\n\n    return power_weighted_emd", "\n\ndef flatjoints(x):\n    \"\"\"\n    Shorthand for a common reshape pattern. Collapses all but the two first dimensions of a tensor.\n    :param x: Data tensor of at least 3 dimensions.\n    :return: The flattened tensor.\n    \"\"\"\n    return x.reshape((x.shape[0], x.shape[1], -1))\n", "\n\ndef benchmark_interpolation(models, function, params, X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton: Skeleton, trans_lengths,\n                            benchmarks, n_past=10, n_future=10):\n    \"\"\"\n    Evaluate naive baselines (zero-velocity and interpolation) for transition generation on given data.\n    :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n    :param Q: Local quaternions array of shape (B, T, J, 4)\n    :param x_mean : Mean vector of local positions of shape (1, J*3, 1)\n    :param x_std: Standard deviation vector of local positions (1, J*3, 1)\n    :param offsets: Local bone offsets tensor of shape (1, 1, J-1, 3)\n    :param parents: List of bone parents indices defining the hierarchy\n    :param out_path: optional path for saving the results\n    :param n_past: Number of frames used as past context\n    :param n_future: Number of frames used as future context (only the first frame is used as the target)\n    :return: Results dictionary\n    \"\"\"\n\n    # trans_lengths = [ 30,30,30,30]\n    n_joints = skeleton.num_joints\n    res_quat = {}\n    res_pos = {}\n    res_npss = {}\n    res_contact = {}\n\n    resultQ = {}\n    resultX = {}\n\n    for n_trans in trans_lengths:\n\n        torch.cuda.empty_cache()\n        print('Computing errors for transition length = {}...'.format(n_trans))\n        target_id = n_trans + n_past\n\n        # Format the data for the current transition lengths. The number of samples and the offset stays unchanged.\n        curr_window = n_trans + n_past + n_future\n        curr_x = X[:, :curr_window, ...]\n        curr_q = Q[:, :curr_window, ...]\n        # ref_quat = Q[:,curr_window-1:curr_window]\n\n        # pos_offset = offsets.unsqueeze(1)[:,:,1:,:]\n        batchsize = curr_x.shape[0]\n        gt_local_quats = curr_q\n        gt_roots = curr_x  # torch.unsqueeze(curr_x,dim=2)\n        gt_local_poses = gt_roots  # torch.cat((gt_roots, gt_offsets), dim=2)\n        global_poses, global_quats = skeleton.forward_kinematics(gt_local_quats, offsets, gt_local_poses)\n        ref_quat = global_quats[:, n_past - 1:n_past]\n        trans_gt_local_poses = gt_local_poses[:, n_past: n_past + n_trans, ...]\n        trans_gt_local_quats = gt_local_quats[:, n_past: n_past + n_trans, ...]\n        # Local to global with Forward Kinematics (FK)\n        trans_gt_global_poses, trans_gt_global_quats = skeleton.forward_kinematics(trans_gt_local_quats, offsets,\n                                                                                   trans_gt_local_poses)  # torch_fk(trans_gt_local_quats, trans_gt_local_poses, skeleton.parents)\n        trans_gt_global_poses = trans_gt_global_poses.reshape(\n            (trans_gt_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\n        def remove_disconti(quats):\n            return remove_quat_discontinuities(torch.cat((ref_quat, quats), dim=1))[:, 1:]\n\n        trans_gt_global_quats = remove_disconti(trans_gt_global_quats)\n        # Normalize\n\n        trans_gt_global_poses = (trans_gt_global_poses - x_mean) / x_std\n        # trans_gt_global_poses = trans_gt_global_poses.reshape(\n        #     (trans_gt_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n        # Zero-velocity pos/quats\n        zerov_trans_local_quats, zerov_trans_local_poses = torch.zeros_like(trans_gt_local_quats), torch.zeros_like(\n            trans_gt_local_poses)\n        zerov_trans_local_quats[:, :, :, :] = gt_local_quats[:, n_past - 1:n_past, :, :]\n        zerov_trans_local_poses[:, :, :, :] = gt_local_poses[:, n_past - 1:n_past, :, :]\n        # To global\n        trans_zerov_global_poses, trans_zerov_global_quats = skeleton.forward_kinematics(zerov_trans_local_quats,\n                                                                                         offsets,\n                                                                                         zerov_trans_local_poses)  # torch_fk(zerov_trans_local_quats, zerov_trans_local_poses, skeleton.parents)\n        trans_zerov_global_poses = trans_zerov_global_poses.reshape(\n            (trans_zerov_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n        # Normalize\n        trans_zerov_global_poses = (trans_zerov_global_poses - x_mean) / x_std\n        trans_zerov_global_quats = remove_disconti(trans_zerov_global_quats)\n\n        # trans_zerov_global_poses = trans_zerov_global_poses.reshape(\n        #     (trans_zerov_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n        resultQ['zerov'] = trans_zerov_global_quats\n        resultX['zerov'] = trans_zerov_global_poses\n        # Interpolation pos/quats\n        r, q = curr_x[:, :, :], curr_q\n        r = r.cpu().data.numpy()\n        q = q.cpu().data.numpy()\n\n        \"\"\"duration test\"\"\"\n        inter_root, inter_local_quats = interpolate_local(r, q, n_trans, n_trans + n_past)\n        # inter_root, inter_local_quats = interpolate_local(X.unsqueeze(dim=2).data.numpy(), Q.data.numpy(), n_trans, 30+n_past)\n\n        inter_root = torch.Tensor(inter_root)\n        inter_local_quats = torch.Tensor(inter_local_quats)\n\n        trans_inter_root = inter_root[:, 1:-1, :, :]\n        inter_local_quats = inter_local_quats[:, 1:-1, :, :]\n        # To global\n        trans_interp_global_poses, trans_interp_global_quats = skeleton.forward_kinematics(inter_local_quats, offsets,\n                                                                                           trans_inter_root)  # torch_fk(inter_local_quats, trans_inter_local_poses, skeleton.parents)\n        trans_interp_global_poses = trans_interp_global_poses.reshape(\n            (trans_interp_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n        # Normalize\n        trans_interp_global_poses = (trans_interp_global_poses - x_mean) / x_std\n        # trans_interp_global_poses = trans_interp_global_poses.reshape(\n        #     (trans_interp_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n        resultQ['interp'] = trans_interp_global_quats\n        resultX['interp'] = trans_interp_global_poses\n        # robust motion in between pos/quats\n        # model,model_input,x_mean,x_std,pos_offset,parents,length, param)\n\n        for key in models:\n            if (key in params):\n                resQ, resX = function[key](models[key], X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton, n_trans, params[key])\n            else:\n                resQ, resX = function[key](models[key], X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton, n_trans, {})\n            resultQ[key] = resQ\n            resultX[key] = resX\n\n        # Local quaternion loss\n        if (\"gq\" in benchmarks):\n            for key in resultQ:\n                resultQ[key] = remove_disconti(\n                    resultQ[key].view(batchsize, n_trans, n_joints, 4))  # .flatten(start_dim=-2,end_dim=-1)\n            trans_gt_global_quats = remove_disconti(\n                trans_gt_global_quats.view(batchsize, n_trans, n_joints, 4))  # .flatten(start_dim=-2,end_dim=-1)\n            for key in resultQ:\n                res_quat[(key, n_trans)] = torch.mean(\n                    torch.sqrt(torch.sum((resultQ[key] - trans_gt_global_quats) ** 2.0, dim=(2, 3))))\n        if (\"gp\" in benchmarks):\n            for key in resultX:\n                res_pos[(key, n_trans)] = torch.mean(\n                    torch.sqrt(torch.sum((resultX[key] - trans_gt_global_poses) ** 2.0, dim=1)))\n\n        # NPSS loss on global quaternions\n        if (\"npss\" in benchmarks):\n            for key in resultQ:\n                resultQ[key] = resultQ[key].cpu().data.numpy()\n            for key in resultQ:\n                res_npss[(key, n_trans)] = fast_npss(flatjoints(trans_gt_global_quats), flatjoints(resultQ[key]))\n\n        # foot skating artifacts\n        if (\"contact\" in benchmarks):\n            for key in resultX:\n                # resultX[key] = resultX[key].reshape(\n                #     (resultX[key].shape[0], -1, n_joints , 3))\n                resultX[key] = resultX[key] * x_std + x_mean\n            # trans_gt_global_poses = trans_gt_global_poses.reshape(\n            #     (trans_gt_global_poses.shape[0], -1, n_joints, 3))\n            trans_gt_global_poses_unnomarilized = trans_gt_global_poses * x_std + x_mean\n            res_contact[('gt', n_trans)] = skating_loss(trans_gt_global_poses_unnomarilized)\n            for key in resultX:\n                res_contact[(key, n_trans)] = skating_loss(resultX[key])\n\n    ### === Global quat losses ===\n\n    return res_quat, res_pos, res_npss, res_contact", "\n\ndef print_result(res_quat, res_pos, res_npss, res_contact, trans_lengths, out_path=None):\n    def format(dt, name, value):\n        s = \"{0: <16}\"\n        for i in range(len(value)):\n            s += \" & {\" + str(i + 1) + \":\" + dt + \"}\"\n        return s.format(name, *value)\n\n    print()\n    print(\"=== Global quat losses ===\")\n    print(format(\"6d\", \"Lengths\", trans_lengths))\n    for key, l in res_quat:\n        if (l == trans_lengths[0]):\n            avg_loss = [res_quat[(key, n)] for n in trans_lengths]\n            print(format(\"6.3f\", key, avg_loss))\n    print()\n\n    renderplot('GlobalQuatLosses', 'loss', trans_lengths, res_quat)\n\n    ### === Global pos losses ===\n    print(\"=== Global pos losses ===\")\n    print(format(\"6d\", \"Lengths\", trans_lengths))\n    for key, l in res_pos:\n        if l == trans_lengths[0]:\n            print(format(\"6.3f\", key, [res_pos[(key, n)] for n in trans_lengths]))\n    print()\n\n    renderplot('GlobalPositionLosses', 'loss', trans_lengths, res_pos)\n\n    ### === NPSS on global quats ===\n    print(\"=== NPSS on global quats ===\")\n    print(format(\"6.4f\", \"Lengths\", trans_lengths))\n    for key, l in res_npss:\n        if l == trans_lengths[0]:\n            print(format(\"6.5f\", key, [res_npss[(key, n)] for n in trans_lengths]))\n\n    renderplot('NPSSonGlobalQuats', 'NPSS', trans_lengths, res_npss)\n\n    ### === Contact loss ===\n    print(\"=== Contact loss ===\")\n    print(format(\"6d\", \"Lengths\", trans_lengths))\n    for key, l in res_contact:\n        if l == trans_lengths[0]:\n            print(format(\"6.3f\", key, [res_contact[(key, n)] for n in trans_lengths]))\n    renderplot('ContactLoss', 'loss', trans_lengths, res_contact)\n\n    # Write to file is desired\n    if out_path is not None:\n        res_txt_file = open(os.path.join(out_path, 'h36m_transitions_benchmark.txt'), \"a\")\n        res_txt_file.write(\"=== Global quat losses ===\\n\")\n        res_txt_file.write(format(\"6d\", \"Lengths\", trans_lengths) + \"\\n\")\n        for key in res_quat:\n            res_txt_file.write(format(\"6.3f\", key, [res_quat[(key, n)] for n in trans_lengths]) + \"\\n\")\n        res_txt_file.write(\"\\n\")\n        res_txt_file.write(\"=== Global pos losses ===\" + \"\\n\")\n        res_txt_file.write(format(\"6d\", \"Lengths\", trans_lengths) + \"\\n\")\n        for key in res_pos:\n            res_txt_file.write(format(\"6.3f\", key, [res_pos[(key, n)] for n in trans_lengths]) + \"\\n\")\n        res_txt_file.write(\"\\n\")\n        res_txt_file.write(\"=== NPSS on global quats ===\" + \"\\n\")\n        res_txt_file.write(format(\"6d\", \"Lengths\", trans_lengths) + \"\\n\")\n        for key in res_npss:\n            res_txt_file.write(format(\"6.3f\", key, [res_npss[(key, n)] for n in trans_lengths]) + \"\\n\")\n        res_txt_file.write(\"\\n\")\n        res_txt_file.write(format(\"6d\", \"Lengths\", trans_lengths) + \"\\n\")\n        for key in res_contact:\n            res_txt_file.write(format(\"6.3f\", key, [res_contact[(key, n)] for n in trans_lengths]) + \"\\n\")\n        print(\"\\n\")\n        res_txt_file.close()", "\nfrom src.Datasets.Style100Processor import StyleLoader\ndef benchmarks():\n    loader = mBaseLoader.WindowBasedLoader(65, 25, 1)\n    # motionloader = mBaseLoader.MotionDataLoader(lafan1_property)\n    style_loader = StyleLoader()\n    processor = BenchmarkProcessor()\n    style_loader.setup(loader, processor)\n    stat_dict = style_loader.load_part_to_binary( \"style100_benchmark_stat\")\n    mean, std = stat_dict['pos_stat']  # ,motionloader.test_dict['pos_std'] #load train x_mean, x_std  size (J * 3)\n    mean, std = torch.from_numpy(mean).view(23*3,1), torch.from_numpy(std).view(23*3,1)\n    style_loader.load_from_binary( \"style100_benchmark_\" + loader.get_postfix_str())\n    #style_loader.load_from_binary( \"test+phase_gv10_ori__61_21\" )\n    style_keys = list(style_loader.all_motions.keys())[0:90]\n    dataSet = BenchmarkDataSet(style_loader.all_motions,style_keys)\n    # dataLoader = DataLoader(dataSet, batch_size=len(dataSet),num_workers=0)\n    dataLoader = DataLoader(dataSet, 100, num_workers=0)\n    style_loader.load_skeleton_only()\n    skeleton = style_loader.skeleton\n    segment = 1\n    num_joints = 22\n\n    benchmarks = [\"gq\", \"gp\", \"npss\", \"contact\"]\n    models, function, params = load_model()\n    trans_lengths = [5, 15, 30]\n    res_quat = None\n\n    for key in models:\n        for param in models[key].parameters():\n            param.requires_grad = False\n\n    iter = 0\n    with torch.no_grad():\n        for batch in dataLoader:\n\n            \"\"\"\n            :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n            :param Q: Local quaternions array of shape (B, T, J, 4)\n            \"\"\"\n            iter += 1\n\n            X = batch['hip_pos']\n            A = batch['A']/0.1\n            S = batch['S']\n            B = batch['B']\n            F = batch['F']\n            gp, gq = skeleton.forward_kinematics(batch['quats'], batch['offsets'], batch['hip_pos'])\n            tar_gp, tar_gq = skeleton.forward_kinematics(batch['style']['quats'], batch['style']['offsets'], batch['style']['hip_pos'])\n            hip_pos = batch['hip_pos']\n            local_quat = batch['quats']\n            local_pos, global_quat = BatchRotateYCenterXZ().forward(gp, gq, 10)\n            tar_pos, tar_quat = BatchRotateYCenterXZ().forward(tar_gp, tar_gq, 10)\n            local_quat = skeleton.inverse_kinematics_quats(global_quat)\n            local_quat = (remove_quat_discontinuities(local_quat.cpu()))\n            hip_pos = local_pos[:, :, 0:1, :]\n            quat, pos, npss, contact = benchmark_interpolation(models, function, params, hip_pos, local_quat, A, S, tar_pos, tar_quat, mean, std,\n                                                               batch['offsets'], skeleton, trans_lengths, benchmarks)\n            if (res_quat == None):\n                res_quat, res_pos, res_npss, res_contact = quat, pos, npss, contact\n            else:\n                for key in res_quat:\n                    res_quat[key] += quat[key]\n                    res_pos[key] += pos[key]\n                    res_npss[key] += npss[key]\n                    res_contact[key] += contact[key]\n\n    for key in res_quat:\n        res_quat[key] /= iter\n        res_pos[key] /= iter\n        res_npss[key] /= iter\n        res_contact[key] /= iter\n    print_result(res_quat, res_pos, res_npss, res_contact, trans_lengths)", "\n\ndef renderplot(name, ylabel, lengths, res):\n    # plt.plot(lengths, interp, label='Interp')\n    for key, l in res:\n        if l == lengths[0]:\n            result = [res[(key, n)] for n in lengths]\n            plt.plot(lengths, result, label=key)\n    plt.xlabel('Lengths')\n    plt.ylabel(ylabel)\n    plt.title(name)\n    plt.legend()\n    plt.savefig(name + '.png')\n    plt.close()", "\n\n\ndef duration_interpolation(models, function, params, X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton: Skeleton,\n                            benchmarks, trans_lengths, n_past=10, n_future=10):\n    \"\"\"\n    Evaluate naive baselines (zero-velocity and interpolation) for transition generation on given data.\n    :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n    :param Q: Local quaternions array of shape (B, T, J, 4)\n    :param x_mean : Mean vector of local positions of shape (1, J*3, 1)\n    :param x_std: Standard deviation vector of local positions (1, J*3, 1)\n    :param offsets: Local bone offsets tensor of shape (1, 1, J-1, 3)\n    :param parents: List of bone parents indices defining the hierarchy\n    :param out_path: optional path for saving the results\n    :param n_past: Number of frames used as past context\n    :param n_future: Number of frames used as future context (only the first frame is used as the target)\n    :return: Results dictionary\n    \"\"\"\n\n    # trans_lengths = [ 30,30,30,30]\n    n_joints = skeleton.num_joints\n    res_quat = {}\n    res_pos = {}\n    res_npss = {}\n    res_contact = {}\n\n    resultQ = {}\n    resultX = {}\n    # trans_lengths = [8,15,60,3000]\n    n_trans = 30\n    for length in trans_lengths:\n\n        torch.cuda.empty_cache()\n        print('Computing errors for transition length = {}...'.format(length))\n        target_id = n_trans + n_past\n\n        # Format the data for the current transition lengths. The number of samples and the offset stays unchanged.\n        curr_window = n_trans + n_past + n_future\n        curr_x = X[:, :curr_window, ...]\n        curr_q = Q[:, :curr_window, ...]\n        # ref_quat = Q[:,curr_window-1:curr_window]\n\n        # pos_offset = offsets.unsqueeze(1)[:,:,1:,:]\n        batchsize = curr_x.shape[0]\n        # gt_offsets = pos_offset#.expand(curr_x.shape[0], curr_x.shape[1], num_joints - 1, 3)\n        # trans_offsets=gt_offsets[:,0: 1,...].repeat(1,n_trans,1,1)\n        # Ground-truth positions/quats/eulers\n        gt_local_quats = curr_q\n        gt_roots = curr_x  # torch.unsqueeze(curr_x,dim=2)\n        gt_local_poses = gt_roots  # torch.cat((gt_roots, gt_offsets), dim=2)\n        global_poses, global_quats = skeleton.forward_kinematics(gt_local_quats, offsets, gt_local_poses)\n        ref_quat = global_quats[:, n_past - 1:n_past]\n        trans_gt_local_poses = gt_local_poses[:, n_past: n_past + n_trans, ...]\n        trans_gt_local_quats = gt_local_quats[:, n_past: n_past + n_trans, ...]\n        # Local to global with Forward Kinematics (FK)\n        trans_gt_global_poses, trans_gt_global_quats = skeleton.forward_kinematics(trans_gt_local_quats, offsets,\n                                                                                   trans_gt_local_poses)  # torch_fk(trans_gt_local_quats, trans_gt_local_poses, skeleton.parents)\n        trans_gt_global_poses = trans_gt_global_poses.reshape(\n            (trans_gt_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\n        def remove_disconti(quats):\n            return remove_quat_discontinuities(torch.cat((ref_quat, quats), dim=1))[:, 1:]\n\n        trans_gt_global_quats = remove_disconti(trans_gt_global_quats)\n        # Normalize\n\n        trans_gt_global_poses = (trans_gt_global_poses - x_mean) / x_std\n        # trans_gt_global_poses = trans_gt_global_poses.reshape(\n        #     (trans_gt_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n\n        # Interpolation pos/quats\n        r, q = curr_x[:, :, :], curr_q\n        r = r.cpu().data.numpy()\n        q = q.cpu().data.numpy()\n\n        \"\"\"duration test\"\"\"\n        inter_root, inter_local_quats = interpolate_local(r, q, length, n_trans + n_past)\n        # inter_root, inter_local_quats = interpolate_local(X.unsqueeze(dim=2).data.numpy(), Q.data.numpy(), n_trans, 30+n_past)\n\n        inter_root = torch.Tensor(inter_root)\n        inter_local_quats = torch.Tensor(inter_local_quats)\n\n        trans_inter_root = inter_root[:, 1:-1, :, :]\n        inter_local_quats = inter_local_quats[:, 1:-1, :, :]\n        # To global\n        trans_interp_global_poses, trans_interp_global_quats = skeleton.forward_kinematics(inter_local_quats, offsets,\n                                                                                           trans_inter_root)  # torch_fk(inter_local_quats, trans_inter_local_poses, skeleton.parents)\n        trans_interp_global_poses = trans_interp_global_poses.reshape(\n            (trans_interp_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n        # Normalize\n        trans_interp_global_poses = (trans_interp_global_poses - x_mean) / x_std\n        # trans_interp_global_poses = trans_interp_global_poses.reshape(\n        #     (trans_interp_global_poses.shape[0], -1, n_joints * 3)).transpose(1, 2)\n        resultQ['interp'] = trans_interp_global_quats\n        resultX['interp'] = trans_interp_global_poses\n        # robust motion in between pos/quats\n        # model,model_input,x_mean,x_std,pos_offset,parents,length, param)\n\n        for key in models:\n            if (key in params):\n                resQ, resX = function[key](models[key], X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton, length, params[key])\n            else:\n                resQ, resX = function[key](models[key], X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, offsets, skeleton, length, {})\n            resultQ[key] = resQ\n            resultX[key] = resX\n\n        # foot skating artifacts\n        if (\"contact\" in benchmarks):\n            for key in resultX:\n                # resultX[key] = resultX[key].reshape(\n                #     (resultX[key].shape[0], -1, n_joints , 3))\n                resultX[key] = resultX[key] * x_std + x_mean\n            # trans_gt_global_poses = trans_gt_global_poses.reshape(\n            #     (trans_gt_global_poses.shape[0], -1, n_joints, 3))\n            trans_gt_global_poses_unnomarilized = trans_gt_global_poses * x_std + x_mean\n            res_contact[('gt', length)] = skating_loss(trans_gt_global_poses_unnomarilized)\n            for key in resultX:\n                res_contact[(key, length)] = skating_loss(resultX[key])\n\n    ### === Global quat losses ===\n\n    return  res_contact", "\ndef duration_test():\n    loader = mBaseLoader.WindowBasedLoader(65, 25, 1)\n    # motionloader = mBaseLoader.MotionDataLoader(lafan1_property)\n    style_loader = StyleLoader()\n    #    processor = TransitionProcessor(10)\n    processor = BenchmarkProcessor()\n    style_loader.setup(loader, processor)\n    stat_dict = style_loader.load_part_to_binary( \"style100_benchmark_stat\")\n    mean, std = stat_dict['pos_stat']  # ,motionloader.test_dict['pos_std'] #load train x_mean, x_std  size (J * 3)\n    mean, std = torch.from_numpy(mean).view(23*3,1), torch.from_numpy(std).view(23*3,1)\n    style_loader.load_from_binary( \"style100_benchmark_\" + loader.get_postfix_str())\n    #style_loader.load_from_binary( \"test+phase_gv10_ori__61_21\" )\n    style_keys = list(style_loader.all_motions.keys())[0:90]\n    dataSet = BenchmarkDataSet(style_loader.all_motions,style_keys)\n    dataLoader = DataLoader(dataSet, batch_size=len(dataSet),num_workers=0)\n    style_loader.load_skeleton_only()\n    skeleton = style_loader.skeleton\n\n\n    benchmarks = [\"contact\"]\n\n    models, function, params = load_model()\n    # trans_lengths = [8, 15, 60, 120]\n\n    trans_lengths = [30]\n    res_contact = None\n\n    for key in models:\n        for param in models[key].parameters():\n            param.requires_grad = False\n\n    iter = 0\n    with torch.no_grad():\n        for batch in dataLoader:\n\n            \"\"\"\n            :param X: Local positions array of shape (Batchsize, Timesteps, 1, 3)\n            :param Q: Local quaternions array of shape (B, T, J, 4)\n            \"\"\"\n            iter += 1\n\n            X = batch['hip_pos']\n            A = batch['A']/0.1\n            S = batch['S']\n            B = batch['B']\n            F = batch['F']\n            gp, gq = skeleton.forward_kinematics(batch['quats'], batch['offsets'], batch['hip_pos'])\n            tar_gp, tar_gq = skeleton.forward_kinematics(batch['style']['quats'], batch['style']['offsets'], batch['style']['hip_pos'])\n            hip_pos = batch['hip_pos']\n            local_quat = batch['quats']\n            # X=X.cpu()\n            # Q=Q.view(Q.shape[0],Q.shape[1],num_joints,4).cpu()\n            local_pos, global_quat = BatchRotateYCenterXZ().forward(gp, gq, 10)\n            local_pos[:, 12:, :, [0, 2]] = local_pos[:, 12:, :, [0, 2]] + (\n                    local_pos[:, 40:41, 0:1, [0, 2]] - local_pos[:, 10:11, 0:1, [0, 2]]) * 2\n            tar_pos, tar_quat = BatchRotateYCenterXZ().forward(tar_gp, tar_gq, 10)\n            local_quat = skeleton.inverse_kinematics_quats(global_quat)\n            local_quat = (remove_quat_discontinuities(local_quat.cpu()))\n            hip_pos = local_pos[:, :, 0:1, :]\n            contact = duration_interpolation(models, function, params, hip_pos, local_quat, A, S, tar_pos, tar_quat, mean, std,\n                                                               batch['offsets'], skeleton, benchmarks,trans_lengths)\n            if (res_contact == None):\n                res_contact = contact\n            else:\n                for key in res_contact:\n                    res_contact[key] += contact[key]\n\n    for key in res_contact:\n        res_contact[key] /= iter\n\n    def format(dt, name, value):\n        s = \"{0: <16}\"\n        for i in range(len(value)):\n            s += \" & {\" + str(i + 1) + \":\" + dt + \"}\"\n        return s.format(name, *value)\n    print(\"=== Contact loss ===\")\n    print(format(\"6d\", \"Lengths\", trans_lengths))\n    for key, l in res_contact:\n        if l == trans_lengths[0]:\n            print(format(\"6.3f\", key, [res_contact[(key, n)] for n in trans_lengths]))", "\nif __name__ == '__main__':\n    benchmarks()\n\n    # duration_test()"]}
{"filename": "train_transitionNet.py", "chunked_list": ["import copy\nimport os\nimport re\nfrom argparse import ArgumentParser\nimport pytorch_lightning as pl\nimport torch\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning import loggers\nfrom pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom pytorch_lightning.profiler import SimpleProfiler", "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\nfrom pytorch_lightning.profiler import SimpleProfiler\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom src.Datasets.BaseLoader import WindowBasedLoader\nfrom src.Datasets.Style100Processor import StyleLoader, Swap100StyJoints\nfrom src.utils import BVH_mod as BVH\nfrom src.utils.motion_process import subsample\n\n\ndef setup_seed(seed:int):\n    seed_everything(seed,True)", "\ndef setup_seed(seed:int):\n    seed_everything(seed,True)\ndef test_model():\n    dict = {}\n    #dict['fast_dev_run'] = 1 # only run 1 train, val, test batch and program ends\n    dict['limit_train_batches'] = 1.\n    dict['limit_val_batches'] = 1.\n    return dict\ndef detect_nan_par():\n    '''track_grad_norm\": 'inf'''\n    return { \"detect_anomaly\":True}", "def detect_nan_par():\n    '''track_grad_norm\": 'inf'''\n    return { \"detect_anomaly\":True}\ndef select_gpu_par():\n    return {\"accelerator\":'gpu', \"auto_select_gpus\":True, \"devices\":-1}\n\ndef create_common_states(prefix:str):\n    log_name = prefix+'/'\n    '''test upload'''\n    parser = ArgumentParser()\n    parser.add_argument(\"--dev_run\", action=\"store_true\")\n    parser.add_argument(\"--version\", type=str, default=\"-1\")\n    parser.add_argument(\"--epoch\",type=str,default=\"last\")\n    parser.add_argument(\"--resume\", action=\"store_true\")\n    parser.add_argument(\"--test\",action=\"store_true\")\n    parser.add_argument(\"--moe_model\",type=str,default=\"./results/StyleVAE2_style100/myResults/55/m_save_model_332\")\n    parser.add_argument(\"--pretrained\",action=\"store_true\")\n    parser.add_argument(\"--predict_phase\",action=\"store_true\")\n    args = parser.parse_args()\n    ckpt_path_prefix = \"results/\"\n    if (args.version != \"-1\"):\n        version = args.version\n    else:\n        version = None\n    '''Create Loggers tensorboard'''\n    if args.dev_run:\n        log_name += \"dev_run\"\n    else:\n        log_name += \"myResults\"\n    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"tensorboard_logs/\", name=log_name, version=None)\n    #load_ckpt_path = os.path.join(ckpt_path_prefix, prefix+'/myResults', str(version))\n    load_ckpt_path = os.path.join(ckpt_path_prefix, prefix+'/myResults', str(version))\n    save_ckpt_path = os.path.join(ckpt_path_prefix, log_name, str(tb_logger.version))\n\n    if (args.resume == True):\n        check_file = load_ckpt_path+\"/\"\n        if (args.epoch == \"last\"):\n            check_file += \"last.ckpt\"\n        else:\n            dirs = os.listdir(check_file)\n            for dir in dirs:\n                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n                out = re.findall(st, dir)\n                if (len(out) > 0):\n                    check_file += out[0]\n                    print(check_file)\n                    break\n        resume_from_checkpoint = check_file  # results/version/last.ckpt\"\n    else:\n        resume_from_checkpoint = None\n    checkpoint_callback = [ModelCheckpoint(dirpath=save_ckpt_path + \"/\", save_top_k=-1, save_last=False, every_n_epochs=2,save_weights_only=True),\n                           ModelCheckpoint(dirpath=save_ckpt_path + \"/\", save_top_k=1, monitor=\"val_loss\", save_last=True, every_n_epochs=1,save_weights_only=True),\n                          # EMA(0.99)\n                           ]\n    '''Train'''\n    checkpoint_callback[0].CHECKPOINT_NAME_LAST = \"last\"\n    profiler = SimpleProfiler()#PyTorchProfiler(filename=\"profiler\")\n    trainer_dict = {\n        \"callbacks\":checkpoint_callback,\n        \"profiler\":profiler,\n        \"logger\":tb_logger\n    }\n    return args,trainer_dict,load_ckpt_path", "def read_style_bvh(style,content,clip=None):\n    swap_joints = Swap100StyJoints()\n    anim = BVH.read_bvh(os.path.join(\"MotionData/100STYLE/\",style,style+\"_\"+content+\".bvh\"),remove_joints=swap_joints)\n    if (clip != None):\n        anim.quats = anim.quats[clip[0]:clip[1], ...]\n        anim.hip_pos = anim.hip_pos[clip[0]:clip[1], ...]\n    anim = subsample(anim,ratio=2)\n    return anim\n\ndef training_style100_phase():\n    from src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\n    from src.Net.TransitionPhaseNet import TransitionNet_phase,Application_phase\n    prefix = \"Transitionv2\"\n    data_set = \"style100\"\n    prefix += \"_\" + data_set\n    args, trainer_dict, ckpt_path = create_common_states(prefix)\n    moe_net = torch.load(args.moe_model)\n\n    if(args.pretrained==True):\n        from src.utils.locate_model import locate_model\n        pretrained_file = locate_model(ckpt_path+\"/\",args.epoch)\n        pre_trained = torch.load(pretrained_file)\n    else:\n        pre_trained = None\n\n\n\n    loader = WindowBasedLoader(61, 21, 1)\n    dt = 1. / 30.\n    phase_dim = 10\n    phase_file = \"+phase_gv10\"\n    style_file_name = phase_file + WindowBasedLoader(120,0,1).get_postfix_str()\n    if (args.test == False):\n        '''Create the model'''\n        style_loader = StyleLoader()\n\n        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),style_file_name, dt=dt,\n                                         batch_size=32,mirror=0.0) # when apply phase, should avoid mirror\n        stat = style_loader.load_part_to_binary(\"motion_statistics\")\n        mode = \"pretrain\"\n\n        model = TransitionNet_phase(moe_net, data_module.skeleton, pose_channels=9,stat=stat ,phase_dim=phase_dim,\n                               dt=dt,mode=mode,pretrained_model=pre_trained,predict_phase=args.predict_phase)\n\n        if (args.dev_run):\n            trainer = Trainer(**trainer_dict, **test_model(),\n                              **select_gpu_par(), precision=32,reload_dataloaders_every_n_epochs=1,\n                              log_every_n_steps=5, flush_logs_every_n_steps=10,\n                              weights_summary='full')\n        else:\n\n            trainer = Trainer(**trainer_dict, max_epochs=10000,reload_dataloaders_every_n_epochs=1,gradient_clip_val=1.0,\n                              **select_gpu_par(), log_every_n_steps=50,check_val_every_n_epoch=2,\n                              flush_logs_every_n_steps=100)\n        trainer.fit(model, datamodule=data_module)\n    else:\n\n        style_loader = StyleLoader()\n        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),style_file_name, dt=dt,batch_size=32,mirror=0.0)\n        data_module.setup()\n\n        check_file = ckpt_path + \"/\"\n        if (args.epoch == \"last\"):\n            check_file += \"last.ckpt\"\n            print(check_file)\n        else:\n            dirs = os.listdir(check_file)\n            for dir in dirs:\n                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n                out = re.findall(st, dir)\n                if (len(out) > 0):\n                    check_file += out[0]\n                    print(check_file)\n                    break\n        model = TransitionNet_phase.load_from_checkpoint(check_file, moe_decoder=moe_net, pose_channels=9,phase_dim=phase_dim,\n                               dt=dt,mode='fine_tune',strict=False)\n        model = model.cuda()\n        data_module.mirror = 0\n        app = Application_phase(model, data_module)\n        model.eval()\n\n        app = app.float()\n\n        key = \"HighKnees\"\n        sty_key = \"HighKnees\"\n        cid = 61\n        sid = 4\n        src_motion = app.data_module.test_set.dataset[key][cid]\n        target_motion = app.data_module.test_set_sty.dataset[sty_key][sid]\n\n        app.setSource(src_motion)\n        app.setTarget(target_motion)\n        source = BVH.read_bvh(\"source.bvh\")\n        output = copy.deepcopy(source)\n\n        output.hip_pos, output.quats = app.forward(t=2., x=0.)\n        BVH.save_bvh(\"test_net.bvh\", output)\n        output.hip_pos, output.quats = app.get_source()\n        BVH.save_bvh(\"source.bvh\", output)\n        torch.save(model, ckpt_path + \"/m_save_model_\" + str(args.epoch))", "\ndef training_style100_phase():\n    from src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\n    from src.Net.TransitionPhaseNet import TransitionNet_phase,Application_phase\n    prefix = \"Transitionv2\"\n    data_set = \"style100\"\n    prefix += \"_\" + data_set\n    args, trainer_dict, ckpt_path = create_common_states(prefix)\n    moe_net = torch.load(args.moe_model)\n\n    if(args.pretrained==True):\n        from src.utils.locate_model import locate_model\n        pretrained_file = locate_model(ckpt_path+\"/\",args.epoch)\n        pre_trained = torch.load(pretrained_file)\n    else:\n        pre_trained = None\n\n\n\n    loader = WindowBasedLoader(61, 21, 1)\n    dt = 1. / 30.\n    phase_dim = 10\n    phase_file = \"+phase_gv10\"\n    style_file_name = phase_file + WindowBasedLoader(120,0,1).get_postfix_str()\n    if (args.test == False):\n        '''Create the model'''\n        style_loader = StyleLoader()\n\n        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),style_file_name, dt=dt,\n                                         batch_size=32,mirror=0.0) # when apply phase, should avoid mirror\n        stat = style_loader.load_part_to_binary(\"motion_statistics\")\n        mode = \"pretrain\"\n\n        model = TransitionNet_phase(moe_net, data_module.skeleton, pose_channels=9,stat=stat ,phase_dim=phase_dim,\n                               dt=dt,mode=mode,pretrained_model=pre_trained,predict_phase=args.predict_phase)\n\n        if (args.dev_run):\n            trainer = Trainer(**trainer_dict, **test_model(),\n                              **select_gpu_par(), precision=32,reload_dataloaders_every_n_epochs=1,\n                              log_every_n_steps=5, flush_logs_every_n_steps=10,\n                              weights_summary='full')\n        else:\n\n            trainer = Trainer(**trainer_dict, max_epochs=10000,reload_dataloaders_every_n_epochs=1,gradient_clip_val=1.0,\n                              **select_gpu_par(), log_every_n_steps=50,check_val_every_n_epoch=2,\n                              flush_logs_every_n_steps=100)\n        trainer.fit(model, datamodule=data_module)\n    else:\n\n        style_loader = StyleLoader()\n        data_module = StyleVAE_DataModule(style_loader, phase_file + loader.get_postfix_str(),style_file_name, dt=dt,batch_size=32,mirror=0.0)\n        data_module.setup()\n\n        check_file = ckpt_path + \"/\"\n        if (args.epoch == \"last\"):\n            check_file += \"last.ckpt\"\n            print(check_file)\n        else:\n            dirs = os.listdir(check_file)\n            for dir in dirs:\n                st = \"epoch=\" + args.epoch + \"-step=\\d+.ckpt\"\n                out = re.findall(st, dir)\n                if (len(out) > 0):\n                    check_file += out[0]\n                    print(check_file)\n                    break\n        model = TransitionNet_phase.load_from_checkpoint(check_file, moe_decoder=moe_net, pose_channels=9,phase_dim=phase_dim,\n                               dt=dt,mode='fine_tune',strict=False)\n        model = model.cuda()\n        data_module.mirror = 0\n        app = Application_phase(model, data_module)\n        model.eval()\n\n        app = app.float()\n\n        key = \"HighKnees\"\n        sty_key = \"HighKnees\"\n        cid = 61\n        sid = 4\n        src_motion = app.data_module.test_set.dataset[key][cid]\n        target_motion = app.data_module.test_set_sty.dataset[sty_key][sid]\n\n        app.setSource(src_motion)\n        app.setTarget(target_motion)\n        source = BVH.read_bvh(\"source.bvh\")\n        output = copy.deepcopy(source)\n\n        output.hip_pos, output.quats = app.forward(t=2., x=0.)\n        BVH.save_bvh(\"test_net.bvh\", output)\n        output.hip_pos, output.quats = app.get_source()\n        BVH.save_bvh(\"source.bvh\", output)\n        torch.save(model, ckpt_path + \"/m_save_model_\" + str(args.epoch))", "\nif __name__ == '__main__':\n    setup_seed(3407)\n    training_style100_phase()\n\n\n"]}
{"filename": "src/__init__.py", "chunked_list": ["import os\nimport sys\nBASEPATH = os.path.dirname(__file__)\nsys.path.insert(0, BASEPATH)"]}
{"filename": "src/Datasets/BaseLoader.py", "chunked_list": ["import os\nimport pickle\nimport random\nimport numpy as np\nimport src.utils.BVH_mod as BVH\nfrom src.utils.motion_process import subsample\nfrom enum import Enum\nclass DataSetType(Enum):\n    Train = 0\n    Test = 1", "'''Loader decides the data structure in the memory'''\ndef subsample_dict(data,ratio):\n    if(ratio<=1):\n        return data\n    for key in data.keys():\n        if(key!=\"offsets\"):\n            data[key] = data[key][::ratio,...]\n    return data\nclass BasedLoader():\n    def __init__(self,sample_ratio:int):\n        self.sample_ratio = sample_ratio\n\n    def _append(self, offsets, hip_pos, local_quat):\n       # return {\"offsets\":[offsets],\"hip_pos\":[hip_pos],\"quat\":[local_quat]}\n        return [offsets],[hip_pos],[local_quat]\n        pass\n    def _append_dict(self,dict,non_temporal_keys):\n        return dict\n    def _subsample(self,anim):\n        if(self.sample_ratio>1):\n            anim = subsample(anim,self.sample_ratio)\n        return anim\n    def load_anim(self, anim):\n        anim = self._subsample(anim)\n        return self._append(anim.offsets,anim.hip_pos,anim.quats)\n    def load_data(self,offsets,hip_pos,quats):\n        return self._append(offsets,hip_pos,quats)\n    def load_dict(self,data,non_temporal_keys=[\"offsets\"]):\n        data = subsample_dict(data,self.sample_ratio)\n        return self._append_dict(data,non_temporal_keys)", "class BasedLoader():\n    def __init__(self,sample_ratio:int):\n        self.sample_ratio = sample_ratio\n\n    def _append(self, offsets, hip_pos, local_quat):\n       # return {\"offsets\":[offsets],\"hip_pos\":[hip_pos],\"quat\":[local_quat]}\n        return [offsets],[hip_pos],[local_quat]\n        pass\n    def _append_dict(self,dict,non_temporal_keys):\n        return dict\n    def _subsample(self,anim):\n        if(self.sample_ratio>1):\n            anim = subsample(anim,self.sample_ratio)\n        return anim\n    def load_anim(self, anim):\n        anim = self._subsample(anim)\n        return self._append(anim.offsets,anim.hip_pos,anim.quats)\n    def load_data(self,offsets,hip_pos,quats):\n        return self._append(offsets,hip_pos,quats)\n    def load_dict(self,data,non_temporal_keys=[\"offsets\"]):\n        data = subsample_dict(data,self.sample_ratio)\n        return self._append_dict(data,non_temporal_keys)", "\n\n\nclass WindowBasedLoader(BasedLoader):\n    def __init__(self,window,overlap,subsample):\n        super(WindowBasedLoader, self).__init__(subsample)\n        self.window = window\n        self.overlap = overlap\n\n    def _append(self,offsets, hip_pos, local_quat):\n        # Sliding windows\n        step = self.window - self.overlap\n        i = 0\n        o=[]\n        h=[]\n        q=[]\n        while i + self.window < local_quat.shape[0]:\n            clip = lambda x: x[i:i + self.window , ...]\n            o.append(offsets[np.newaxis, :, :].astype(np.float32))\n            h.append(clip(hip_pos)[np.newaxis, ...].astype(np.float32))\n            q.append(clip(local_quat)[np.newaxis, ...].astype(np.float32))\n            i += step\n        return o,h,q\n    def append_dicts(self,dict,non_temporal_keys=[\"offsets\"]):\n        temporal_keys = [i for i in dict.keys() if i not in non_temporal_keys]\n        length = len(dict[temporal_keys[0]])\n        output = {key:[] for key in dict.keys()}\n        for i in range(length):\n            x = {key:dict[key][i] for key in dict.keys()}\n            x = self._append_dict(x,non_temporal_keys)\n            for key in dict.keys():\n                output[key] = output[key]+x[key]\n        return output\n    def _append_dict(self,dict,non_temporal_keys=[\"offsets\"]):\n        step = self.window - self.overlap\n        temporal_keys = [i for i in dict.keys() if i not in non_temporal_keys]\n        length = dict[temporal_keys[0]].shape[0]\n        i = 0\n        output = {key:[] for key in dict.keys()}\n        while i + self.window < length:\n            clip = lambda x: x[i:i + self.window, ...]\n            for key in temporal_keys:\n                output[key].append(clip(dict[key])[np.newaxis,:,:].astype(np.float32))\n            for key in non_temporal_keys:\n                output[key].append(dict[key][np.newaxis,:,:].astype(np.float32))\n            i += step\n        return output\n    def get_postfix_str(self):\n        return \"_\"+str(self.window)+\"_\"+str(self.overlap)", "\nclass StreamBasedLoader(BasedLoader):\n    def __init__(self,sample_ratio):\n        super(StreamBasedLoader, self).__init__(sample_ratio)\n        pass\n\n'''Processor decides the training data '''\nclass BasedDataProcessor():\n    def __init__(self):\n        pass\n    def _concat(self,offsets,hip_pos,quats):\n        offsets = np.concatenate(offsets,axis = 0)\n        local_quat = np.concatenate(quats,axis = 0)\n        hip_pos = np.concatenate(hip_pos,axis = 0)\n        return offsets,hip_pos,local_quat\n    def __call__(self, dict,skeleton,motion_data_loader):\n        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n        return {\"offsets\":offsets,\"hip_pos\":hip_pos,\"quats\":quats}", "'''\u8bfb\u6587\u4ef6\u7684\u5de5\u5177\u7c7b\uff0c\u5982\u679c\u4ecebvh\u6587\u4ef6\u4e2d\u8bfb\u53d6\uff0c\u5219\u9700\u8981\u914d\u7f6eloader\u548cprocessor\uff0c\u4ee5\u505a\u7b80\u5355\u7684\u6570\u636e\u5904\u7406\u548c\u5143\u6570\u636e\u8ba1\u7b97'''\nclass MotionDataLoader():\n    def __init__(self,property:dict):\n       # self.dict = None\n\n        self.loader= self.processor = None\n        self.skeleton = None\n        self.dataset_property = property\n        self.file_prefix = []\n    def setup(self,loader:BasedLoader=None,processor:BasedDataProcessor=None):\n        self.loader = loader\n        self.processor = processor\n    def get_path(self,name:DataSetType):\n        if(name==DataSetType.Train):\n            return self.dataset_property[\"train_path\"]\n        elif(name == DataSetType.Test):\n            return self.dataset_property['test_path']\n        else:\n            assert False\n    def _set_dict(self,name:DataSetType,dict):\n        if (name == DataSetType.Train):\n            self.train_dict = dict\n        else:\n            self.test_dict = dict\n    def get_dict(self,name:DataSetType):\n        if (name == DataSetType.Train):\n            return self.train_dict\n        else:\n            return self.test_dict\n    def _process_set(self,name:DataSetType):\n\n        path = self.get_path(name)\n        remove_joints = (self.dataset_property['remove_joints'])\n        if (remove_joints != None):\n            remove_joints = remove_joints()\n        self.files = []\n        self.files = collect_bvh_files(path, self.files)\n        self.file_prefix = [0]\n        list_count = 0\n\n        o = []\n        h = []\n        q = []\n        # buffer = DataBuffer()\n        for file in self.files:\n            if file.endswith(\".bvh\"):\n                offsets, hip_pos, quats = self._load_from_bvh(file,remove_joints)\n\n                list_count += len(hip_pos)\n                self.file_prefix.append(list_count)\n                o+=(offsets)\n                h+=(hip_pos)\n                q+=(quats)\n\n\n\n        assert list_count == len(q)\n        data = {\"offsets\":o,\"hip_pos\":h,\"quats\":q}\n        self._set_dict(name,self.processor(data,self.skeleton,self))\n        return o,h,q\n    def load_from_bvh_list(self,name:DataSetType):\n        # data_file = os.path.join(path,\"data\")\n        if(name == DataSetType.Test and self.get_path(DataSetType.Train)==self.get_path(DataSetType.Test)):\n            assert False\n        return self._process_set(name)\n    def load_from_bvh_file(self,name:DataSetType,file):\n        path = self.get_path(name)\n        remove_joints = (self.dataset_property['remove_joints'])\n        if (remove_joints != None):\n            remove_joints = remove_joints()\n        o = []\n        h = []\n        q = []\n        offsets,hip_pos,quats = self._load_from_bvh(os.path.join(path,file),remove_joints)\n        o += (offsets)\n        h += (hip_pos)\n        q += (quats)\n        self._set_dict(name,self.processor(o,h,q,self.skeleton,self))\n    def _load_from_bvh(self,file,remove_joints):\n        if (self.loader == None or self.processor == None):\n            assert False\n        #print(file)\n        anim = BVH.read_bvh(file, remove_joints=remove_joints, Tpose=-1, remove_gap=self.dataset_property['remove_gap'])\n        if (self.skeleton == None):\n            self.skeleton = anim.skeleton\n        return self.loader.load_anim(anim)\n    def get_skeleton(self):\n        if(self.skeleton==None):\n            self._load_skeleton(self.get_path(DataSetType.Train))\n\n    def _load_skeleton(self,path):\n        f = open(path +'/'+ 'skeleton', 'rb')\n        self.skeleton = pickle.load(f)\n        f.close()\n    def _save_skeleton(self,path):\n        f = open(path +'/'+ 'skeleton', 'wb+')\n        pickle.dump(self.skeleton, f)\n        f.close()\n    def load_skeleton_only(self,name:DataSetType):\n        path = self.get_path(name)\n        self._load_skeleton(path)\n\n    def save_part_to_binary(self,name:DataSetType,filename,keys):\n        path = self.get_path(name)\n        parseName = \"train_\" if name == DataSetType.Train else \"test_\"\n        dict = self.get_dict(name)\n        dict = {key:dict[key] for key in keys}\n        f = open(path + '/' + parseName + filename + \".dat\", \"wb+\")\n        pickle.dump(dict, f)\n        f.close()\n    def load_part_from_binary(self,name:DataSetType,filename):\n        path = self.get_path(name)\n        parseName = \"train_\" if name == DataSetType.Train else \"test_\"\n        f = open(path + '/' + parseName + filename + \".dat\", 'rb')\n        dict = pickle.load(f)\n        f.close()\n        return dict\n    def save_to_binary(self,name:DataSetType,filename):\n        path = self.get_path(name)\n        self._save_skeleton(path)\n        parseName = \"train_\" if name == DataSetType.Train else \"test_\"\n        dict = self.get_dict(name)\n        f = open(path+'/'+parseName+filename+\".dat\",\"wb+\")\n        pickle.dump(dict,f)\n        f.close()\n\n    def load_from_binary(self,name:DataSetType,filename):\n        path = self.get_path(name)\n        self._load_skeleton(path)\n        parseName = \"train_\" if name == DataSetType.Train else \"test_\"\n        f = open(path + '/' + parseName + filename + \".dat\", 'rb')\n        dict = pickle.load(f)\n        f.close()\n        self._set_dict(name,dict)", "\n\n\n\n\n\n\ndef collect_bvh_files(path,l:list):\n    print(path)\n    files = os.listdir(path)\n    for file in files:\n        file = os.path.join(path,file)\n        if os.path.isdir(file):\n            l = collect_bvh_files(file,l)\n        elif file.endswith(\".bvh\"):\n            l.append(file)\n    return l", ""]}
{"filename": "src/Datasets/DeepPhaseDataModule.py", "chunked_list": ["\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nimport torch.utils.data\nfrom torch.utils.data import DataLoader\n\n\nfrom src.Datasets.BaseDataSet import StreamDataSetHelper\nfrom src.Datasets.BaseLoader import BasedDataProcessor", "from src.Datasets.BaseDataSet import StreamDataSetHelper\nfrom src.Datasets.BaseLoader import BasedDataProcessor\nfrom src.Datasets.BatchProcessor import BatchProcessData, BatchProcessDatav2\nfrom src.Datasets.Style100Processor import StyleLoader\n\n\nclass DeepPhaseProcessor(BasedDataProcessor):\n    def __init__(self,dt):\n        super(DeepPhaseProcessor, self).__init__()\n        self.process = BatchProcessData()\n        self.dt = dt\n    def gpu_fk(self,offsets,hip_pos,local_quat,skeleton):\n        quat = torch.from_numpy(local_quat).float().cuda()\n        offsets = torch.from_numpy(offsets).float().cuda()\n        hip_pos = torch.from_numpy(hip_pos).float().cuda()\n        gp,gq = skeleton.forward_kinematics(quat,offsets,hip_pos)\n\n        return gp,gq[...,0:1,:]\n    def transform_single(self,offsets,hip_pos,local_quat,skeleton):\n        gp,gq = self.gpu_fk(offsets,hip_pos,local_quat,skeleton)\n        dict = self.process(gq.unsqueeze(0),gp.unsqueeze(0))\n        lp = dict['local_pos'][0]\n        relative_gv = (lp[:,1:]-lp[:,:-1])/self.dt\n        torch.cuda.empty_cache()\n\n        return relative_gv\n\n    #((V_i in R_i) - (V_(i - 1) in R_(i - 1))) / dt,\n    def __call__(self, dict,skeleton,motionDataLoader):\n        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n        dict = {\"gv\":[]}\n        if(len(offsets)>1):\n            offsets = np.concatenate(offsets,axis=0)\n            hip_pos = np.concatenate(hip_pos,axis=0)\n            quats = np.concatenate(quats,axis=0)\n        else:\n            offsets = offsets[0][np.newaxis,...]\n            hip_pos = hip_pos[0][np.newaxis,...]\n            quats = quats[0][np.newaxis,...]\n        gv = self.transform_single(offsets, hip_pos,quats, skeleton)\n        gv = gv.cpu().numpy()\n        dict['gv']=gv\n\n        return dict", "\n\n\nclass DeepPhaseProcessorv2(BasedDataProcessor):\n    def __init__(self,dt):\n        super(DeepPhaseProcessorv2, self).__init__()\n        self.process = BatchProcessDatav2()\n        self.dt = dt\n    def gpu_fk(self,offsets,hip_pos,local_quat,skeleton):\n        quat = torch.from_numpy(local_quat).float().cuda()\n        offsets = torch.from_numpy(offsets).float().cuda()\n        hip_pos = torch.from_numpy(hip_pos).float().cuda()\n        gp,gq = skeleton.forward_kinematics(quat,offsets,hip_pos)\n\n        return gp,gq[...,0:1,:]\n    def transform_single(self,offsets,hip_pos,local_quat,skeleton):\n        gp,gq = self.gpu_fk(offsets,hip_pos,local_quat,skeleton)\n        glb_vel,glb_pos,glb_rot,root_rotatioin = self.process.forward(gq.unsqueeze(0),gp.unsqueeze(0))\n        relative_gv = glb_vel[0]/self.dt\n        torch.cuda.empty_cache()\n        return relative_gv\n\n    #((V_i in R_i) - (V_(i - 1) in R_(i - 1))) / dt,\n    def __call__(self, dict,skeleton,motionDataLoader):\n        offsets, hip_pos, quats = dict[\"offsets\"], dict[\"hip_pos\"], dict[\"quats\"]\n        dict = {\"gv\":[]}\n        all_std = 0\n        length = 0\n        for i in range(len(offsets)):\n            N,J,D = quats[i].shape\n            dict[\"gv\"].append(self.transform_single(offsets[i],hip_pos[i],quats[i],skeleton))\n            all_std = all_std + dict['gv'][-1].std() * dict['gv'][-1].shape[0]\n            length = length + dict['gv'][-1].shape[0]\n        all_std = all_std/length\n        dict['std'] = all_std\n        return dict", "class DeephaseDatasetWindow(torch.utils.data.Dataset):\n    def __init__(self,buffer):\n        self.buffer = buffer\n    def __getitem__(self, item):\n        return self.buffer[item]\n    def __len__(self):\n        return self.buffer.shape[0]\nclass DeephaseDataSet(torch.utils.data.Dataset):\n    def __init__(self,buffer,window_size):\n        self.window = window_size\n        # first remove sequence less than window\n        idx = [i for i in range(len(buffer)) if buffer[i].shape[0]>=window_size]\n        buffer = [buffer[i] for i in idx]\n\n        self.streamer = StreamDataSetHelper(buffer,window_size)\n        self.buffer = buffer\n\n\n    def get_window(self,item):\n        idx, frame_idx = self.streamer[item]\n        return idx,[frame_idx,frame_idx+self.window]\n    def __getitem__(self, item):\n        idx,frame_idx = self.streamer[item]\n        return self.buffer[idx][frame_idx:frame_idx+self.window]\n    def __len__(self):\n        return len(self.streamer)", "class Style100DataModule(pl.LightningDataModule):\n    def __init__(self,batch_size,shuffle,data_loader:StyleLoader,window_size):\n        super(Style100DataModule, self).__init__()\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        self.data_loader = data_loader\n\n        data_loader.load_skeleton_only()\n        self.skeleton = data_loader.skeleton\n        self.window = window_size\n    def setup(self,stage: [str] = None) :\n        use_gv=True\n        if(use_gv):\n            self.data_loader.load_train_test_dataset(\"deep_phase_gv\")\n            self.test_set = DeephaseDatasetWindow(self.data_loader.test_dict['gv'])\n            self.train_set = DeephaseDatasetWindow(self.data_loader.train_dict['gv'])\n            self.scale = 1.\n        else:\n            self.data_loader.load_train_test_dataset(\"deep_phase_pca\")\n            self.test_set = DeephaseDataSet(self.data_loader.test_dict['pos'], self.window)\n            self.train_set = DeephaseDataSet(self.data_loader.train_dict['pos'], self.window)\n            self.scale = 1.\n\n\n        self.val_set = self.test_set\n\n\n    def train_dataloader(self):\n        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=self.shuffle, num_workers=0, drop_last=True)\n\n    def val_dataloader(self):  # \u9700\u8981shuffle\u786e\u4fdd\u76f8\u540cbatch\u5185\u7684\u98ce\u683c\u4e0d\u76f8\u540c\n        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=0, drop_last=True)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=16, drop_last=True)\n    def on_after_batch_transfer(self, batch, dataloader_idx: int) :\n        self.scale= 1.\n        return on_after_batch_transfer(batch,self.scale)", "\n\n\n\ndef on_after_batch_transfer(batch,scale):\n    use_gv = True\n    if (use_gv):\n        batch = batch.permute(0, 2, 3, 1).contiguous()\n        B, J, D, N = batch.shape\n        batch = batch/30.\n        mean = torch.mean(batch, dim=(-1), keepdim=True)\n        batch = batch - mean\n\n    batch = batch.to(torch.float)\n    return batch", ""]}
{"filename": "src/Datasets/StyleVAE_DataModule.py", "chunked_list": ["\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import DataLoader\n\nfrom src.Datasets.BatchProcessor import BatchRotateYCenterXZ\nfrom src.Datasets.Style100Processor import StyleLoader\nfrom src.Datasets.augmentation import BatchMirror", "from src.Datasets.Style100Processor import StyleLoader\nfrom src.Datasets.augmentation import BatchMirror\nfrom src.Module.PhaseModule import PhaseOperator\n\n\nclass Style100Dataset_phase(torch.utils.data.Dataset):\n    def __init__(self, dataset:dict,batch_size,is_train,keep_equal=True):\n        self.is_train = is_train\n        self.dataset = dataset\n        self.n_styles = len(self.dataset.keys())\n        self.style_ids = list(self.dataset.keys())\n        self.expand_()\n        self.batch_size = batch_size\n        self.equal_style = keep_equal\n\n        size = [len(self.dataset[key]) for key in self.dataset]\n\n        if (keep_equal):\n            min_size = min(size)\n            min_batches = min_size//batch_size\n            self.sizes = [min_batches for i in range(self.n_styles)]\n            self.len = min_batches*self.n_styles\n        else:\n\n            self.sizes = [size[i]//batch_size for i in range(self.n_styles)]\n            self.len = sum(self.sizes)\n\n        self.expand_dataset = {\"data\":[],\"sty\":[]}\n        self.shuffle_()\n        self.style_to_idx = {}\n        for i in range(len(self.style_ids)):\n            self.style_to_idx[self.style_ids[i]]=i\n        self.style_batch_idx = [0 for i in range(len(self.style_ids))]\n        self.style_batch_start = [0]\n        for i in range(len(self.sizes)):\n            self.style_batch_start.append(self.sizes[i]+self.style_batch_start[i])\n\n\n\n        pass\n    def get_style_batch_for_train(self,style):\n        style_id = self.style_to_idx[style]\n        self.style_batch_idx[style_id] += 1\n        assert(self.sizes[style_id]>0)\n        if(self.style_batch_idx[style_id]>=self.sizes[style_id]):\n            self.style_batch_idx[style_id] = 0\n        start_idx = self.style_batch_start[style_id]\n        idx = self.style_batch_idx[style_id]\n        item = start_idx+idx\n        return self.expand_dataset['data'][item],self.expand_dataset['sty'][item]\n\n\n\n    def get_style_batch(self,style,style_id,batch_size):\n        motions = self.dataset[style]\n        length = len(motions)\n        idx = np.arange(0,length)\n        np.random.shuffle(idx)\n        sub_idx = idx[:batch_size]\n        sub_motions = [motions[j] for j in sub_idx]\n        for i in range(len(sub_motions)):\n            dict = {key:torch.from_numpy(sub_motions[i][3][key]).unsqueeze(0).cuda() for key in sub_motions[i][3].keys()}\n            sub_motions[i] = [torch.from_numpy(sub_motions[i][j]).unsqueeze(0).cuda() for j in range(3)]+[dict]\n        return {\"data\":sub_motions,'sty':style_id}\n    def expand_(self):\n        for style in self.dataset:\n            motions = self.dataset[style]\n            expand_ = lambda key : sum([motions[con][key] for con in motions],[])\n            q = expand_('quats')\n            o = expand_('offsets')\n            h = expand_('hip_pos')\n            A = expand_(\"A\")\n            S = expand_(\"S\")\n            B = expand_(\"B\")\n            F = expand_(\"F\")\n\n            self.dataset[style] = [(q[i],o[i],h[i],{\"A\":A[i],\"S\":S[i],\"B\":B[i],\"F\":F[i]}) for i in range(len(q))]#\n\n    def shuffle_(self):\n        #self.expand_dataset['data'].clear()\n        #self.expand_dataset['sty'].clear()\n        self.expand_dataset = {\"data\": [], \"sty\": []}\n        for style_id,style in enumerate(self.dataset):\n            motions = self.dataset[style]\n            length = len(motions)\n            idx = np.arange(0,length)\n            np.random.shuffle(idx)\n            sub_motions = []\n\n            for i in range(0,len(idx),self.batch_size):\n                if(i+self.batch_size>len(idx)):\n                    break\n                sub_idx = idx[i:i+self.batch_size]\n                sub_motions.append([motions[j] for j in sub_idx])\n            self.expand_dataset['data']+=(sub_motions[:self.sizes[style_id]])\n            self.expand_dataset['sty']+=[self.style_ids[style_id] for i in range(self.sizes[style_id])]\n\n\n\n\n    def __getitem__(self, item):\n\n\n        return self.expand_dataset['data'][item],self.expand_dataset['sty'][item],self.is_train\n\n\n    def __len__(self):\n        return self.len", "\nclass StyleVAE_DataModule(pl.LightningDataModule):\n    def __init__(self,dataloader:StyleLoader,filename,style_file_name,dt, batch_size = 32,shuffle=True,mirror=0.,use_phase = True):\n        super(StyleVAE_DataModule, self).__init__()\n        self.loader = dataloader\n        self.data_file = filename\n        self.style_file_name = style_file_name\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.loader.load_skeleton_only()\n        self.skeleton = self.loader.skeleton\n        self.processor = BatchRotateYCenterXZ()\n        self.phase_op = PhaseOperator(dt)\n        self.mirror = mirror\n\n        self.batch_mirror = BatchMirror(self.skeleton, mirror_prob=mirror)\n        self.use_phase = use_phase\n        if(style_file_name==None):\n            self.use_sty = False\n        else:\n            self.use_sty=True\n    def prepare_data(self) -> None:\n        # download, tokenize, etc\n        pass\n\n    def setup(self, stage: [str] = None) -> None:\n        self.loader.load_dataset(self.data_file)\n        self.train_set = Style100Dataset_phase(self.loader.train_motions, self.batch_size,True)\n        self.test_set = Style100Dataset_phase(self.loader.test_motions, self.batch_size,False, keep_equal=False)\n        self.val_set = self.test_set\n        if(self.use_sty):\n            self.loader.load_dataset(self.style_file_name)\n            self.train_set_sty = Style100Dataset_phase(self.loader.train_motions, self.batch_size,True,keep_equal=False)\n            self.test_set_sty = Style100Dataset_phase(self.loader.test_motions, 4,False, keep_equal=False)\n            self.val_set = self.test_set\n\n        pass\n\n    def train_dataloader(self):\n        torch.cuda.empty_cache()\n        self.train_set.shuffle_()\n        cons = DataLoader(self.train_set, batch_size=1, shuffle=self.shuffle, num_workers=0, drop_last=False)\n\n        return cons\n\n    def val_dataloader(self):  # \u9700\u8981shuffle\u786e\u4fdd\u76f8\u540cbatch\u5185\u7684\u98ce\u683c\u4e0d\u76f8\u540c\n        cons = DataLoader(self.test_set, batch_size=1, shuffle=self.shuffle, num_workers=0, drop_last=False)\n\n        return cons\n\n    def test_dataloader(self):\n        cons = DataLoader(self.test_set, batch_size=1, shuffle=self.shuffle, num_workers=0,\n                          drop_last=False)\n\n        return cons\n\n    def transfer_mannual(self, batch, dataloader_idx: int, use_phase=True,use_sty=True):\n\n        def get_data(batch, idx):\n            data = [batch[0][i][idx].squeeze(1) for i in range(len(batch[0]))]\n            data = torch.cat(data, dim=0)\n            return data\n\n        def get_phase(batch, str):\n            data = [batch[0][i][3][str].squeeze(1) for i in range(len(batch[0]))]\n            data = torch.cat(data, dim=0)\n            return data\n\n        if ('con' in batch):\n            quat = torch.cat((get_data(batch['con'], 0), get_data(batch['sty'], 0)), dim=0)\n            hip_pos = torch.cat((get_data(batch['con'], 2), get_data(batch['sty'], 2)), dim=0)\n            offsets = torch.cat((get_data(batch['con'], 1), get_data(batch['sty'], 1)), dim=0)\n            sty = [batch['con'][1], batch['sty'][1]]\n            if (use_phase):\n                A = torch.cat((get_phase(batch['con'], \"A\"), get_phase(batch['sty'], 'A')), dim=0)\n                S = torch.cat((get_phase(batch['con'], \"S\"), get_phase(batch['sty'], 'S')), dim=0)\n                A =  A/0.1\n                phase = self.phase_op.phaseManifold(A, S)\n            else:\n                phase = None\n                A = S = None\n\n\n        else:\n            hip_pos = get_data(batch, 2)\n            quat = get_data(batch, 0)\n            offsets = get_data(batch, 1)\n            if (use_phase):\n                A = get_phase(batch, 'A')  # ['A']\n                S = get_phase(batch, 'S')  # ['S']\n                A = A/0.1\n                phase = self.phase_op.phaseManifold(A, S)\n            else:\n                A=S=None\n                phase = None\n            sty = [batch[1]]\n\n        gp, gq = self.skeleton.forward_kinematics(quat, offsets, hip_pos)\n        if(use_sty):\n            style_name = sty[0][0]\n            is_train = batch[2][0]\n            if(is_train==False):# is not train\n                style_batch = self.test_set_sty.get_style_batch_for_train(style_name)\n            else:\n                style_batch = self.train_set_sty.get_style_batch_for_train(style_name)\n            assert(style_batch[1]==style_name)\n            def get_style_batch(batch,idx):\n                style_hip = [batch[0][i][idx] for i in range(len(batch[0]))]\n                style_hip = np.concatenate(style_hip,axis=0)\n                return torch.from_numpy(style_hip).to(gp.device)\n            style_hip = get_style_batch(style_batch,2)\n            style_quat = get_style_batch(style_batch, 0)\n            style_offsets = get_style_batch(style_batch, 1)\n            style_gp,style_gq = self.skeleton.forward_kinematics(style_quat,style_offsets,style_hip)\n            if(is_train==False):# the test set is too small so there is not enough batch\n                style_gp = style_gp.unsqueeze(0).expand((8,)+style_gp.shape).flatten(0,1)\n                style_gq = style_gq.unsqueeze(0).expand((8,)+style_gq.shape).flatten(0,1)\n        else:\n            style_gp = style_gq = None\n\n        local_pos, local_rot = self.processor.forward(gp, gq, 10)\n\n\n        return {\"local_pos\": local_pos, \"local_rot\": local_rot, \"offsets\": offsets, \"label\": sty, \"phase\": phase,'A':A,'S':S,\"sty_pos\":style_gp,\"sty_rot\":style_gq}\n    def on_after_batch_transfer(self, batch, dataloader_idx: int) :\n        return self.transfer_mannual(batch,dataloader_idx,self.use_phase,use_sty=self.use_sty)", "\n\n"]}
{"filename": "src/Datasets/__init__.py", "chunked_list": [""]}
{"filename": "src/Datasets/augmentation.py", "chunked_list": ["import random\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply, quaternion_invert\nfrom src.geometry.quaternions import slerp, from_to_1_0_0\nfrom src.geometry.vector import normalize_vector\nfrom src.utils.BVH_mod import Skeleton\ndef find_Yrotation_to_align_with_Xplus(q):\n    '''Warning: the quat must be along y axis'''\n    \"\"\"\n    :param q: Quats tensor for current rotations (B, 4)\n    :return y_rotation: Quats tensor of rotations to apply to q to align with X+\n    \"\"\"\n    shape = list(q.shape)\n    q = q.reshape(-1,4)\n    mask = torch.tensor(np.array([[1.0, 0.0, 1.0]]), dtype=q.dtype,device=q.device).expand(q.shape[0], -1)\n    ref_vector = torch.tensor(np.array([[1.0, 0.0, 0.0]]), dtype=q.dtype,device=q.device).expand(q.shape[0], -1)\n    forward = mask * quaternion_apply(q, ref_vector)\n    #forward = normalize_vector(forward)\n    #target =  torch.tensor(np.array([[1, 0, 0]]), dtype=torch.float,device=q.device).expand(q.shape[0], -1)\n\n    #y_rotation = normalize_vector(from_to_quaternion(forward, target))\n    y_rotation = normalize_vector(from_to_1_0_0(forward))\n\n\n\n    return y_rotation.view(shape)", "from src.utils.BVH_mod import Skeleton\ndef find_Yrotation_to_align_with_Xplus(q):\n    '''Warning: the quat must be along y axis'''\n    \"\"\"\n    :param q: Quats tensor for current rotations (B, 4)\n    :return y_rotation: Quats tensor of rotations to apply to q to align with X+\n    \"\"\"\n    shape = list(q.shape)\n    q = q.reshape(-1,4)\n    mask = torch.tensor(np.array([[1.0, 0.0, 1.0]]), dtype=q.dtype,device=q.device).expand(q.shape[0], -1)\n    ref_vector = torch.tensor(np.array([[1.0, 0.0, 0.0]]), dtype=q.dtype,device=q.device).expand(q.shape[0], -1)\n    forward = mask * quaternion_apply(q, ref_vector)\n    #forward = normalize_vector(forward)\n    #target =  torch.tensor(np.array([[1, 0, 0]]), dtype=torch.float,device=q.device).expand(q.shape[0], -1)\n\n    #y_rotation = normalize_vector(from_to_quaternion(forward, target))\n    y_rotation = normalize_vector(from_to_1_0_0(forward))\n\n\n\n    return y_rotation.view(shape)", "def angle_between_quats_along_y(from_quat:torch.Tensor,to_quat:torch.Tensor):\n    \"\"\"\n        Quats tensor for current rotations (B, 4)\n        :return\n    \"\"\"\n\n    #mask = np.tile(np.array([[1.0, 0.0, 1.0]], dtype=np.float),(from_quat.shape[0],1))\n    initial_direction = torch.tensor(np.array([[0.0, 0.0, 1.0]]), dtype=torch.float,device=from_quat.device).expand(from_quat.shape[:-1]+(3,))\n    quat = quaternion_multiply(to_quat,quaternion_invert(from_quat))\n    dir = quaternion_apply(quat,initial_direction)\n\n    return torch.atan2(dir[...,0],dir[...,2])", "\n\n\n\n\nclass TemporalScale(torch.nn.Module):\n    def __init__(self,prob):\n        super(TemporalScale, self).__init__()\n        self.prob = prob\n    def forward(self,hip_pos,quat):\n        if (random.random() > self.prob):\n            return hip_pos, quat\n            # hip_pos = batch['hip_pos']\n            # quat = batch['quat']\n        batch_size = hip_pos.shape[0]\n        T = hip_pos.shape[1]\n        num_joints = quat.shape[2]\n        delta_t = random.randint(T // 2, T)\n        start_t = random.randint(0, T - delta_t)\n        hip_pos = hip_pos[:, start_t:delta_t + start_t, :, :]\n        quat = quat[:, start_t:delta_t + start_t, :, :]\n        if delta_t < 0.75 * T:\n            gamma = random.random() + 1  # gamma \\in [1,2]\n        else:\n            gamma = random.random() * 0.5 + 0.5  # gamma \\in [0.5,1]\n        new_delta_t = int(delta_t * gamma)\n        new_hip_pos = torch.zeros((batch_size, new_delta_t, 1, 3), device=hip_pos.device, dtype=hip_pos.dtype)\n        new_quat = torch.zeros((batch_size, new_delta_t, num_joints, 4), device=hip_pos.device, dtype=hip_pos.dtype)\n        '''scale'''\n        ratio = delta_t / new_delta_t\n        idx = torch.arange(0, new_delta_t, device=quat.device).type(torch.long)\n        ori_frame_step = idx * ratio\n        start_frame = ori_frame_step.to(torch.long)\n        neighbor_frame = start_frame + 1\n        neighbor_frame = torch.where(neighbor_frame >= quat.shape[1], neighbor_frame - 1, neighbor_frame)\n        weight = start_frame - ori_frame_step + 1\n        weight = weight.view(1, -1, 1, 1)\n\n        new_quat[:, idx, :, :] = slerp(quat[:, start_frame, :, :], quat[:, neighbor_frame, :, :], 1 - weight)\n        new_hip_pos[:, idx, :, :] = hip_pos[:, start_frame, :, :] * weight + (1 - weight) * hip_pos[:, neighbor_frame,:, :]\n\n        '''padding or cutting'''\n        if new_delta_t > T:\n            new_quat = new_quat[:, :T, :, :]\n            new_hip_pos = new_hip_pos[:, :T, :, :]\n        # elif new_delta_t < self.T:\n        #     new_quat = pad_to_window(new_quat, window=self.T)\n        #     new_hip_pos = pad_to_window(new_hip_pos, window=self.T)\n\n        return new_hip_pos, new_quat", "\n\n\n\nclass BatchMirror(torch.nn.Module):\n    def __init__(self, skeleton, mirror_prob=0.5):\n        super().__init__()\n        self.mirroring = Mirror(skeleton=skeleton,dtype=torch.float)\n        self.mirror_probability = mirror_prob\n        self.nb_joints = skeleton.num_joints\n\n    def forward(self, global_pos,global_rot):\n        if torch.rand(1)[0] < self.mirror_probability:\n            batch_size = global_pos.shape[0]\n            original_positions = global_pos.reshape(-1, self.nb_joints, 3)\n            original_rotations = global_rot.reshape(-1, self.nb_joints, 4)\n            mirrored_positions, mirrored_rotations = self.mirroring.forward(original_positions, original_rotations)\n            global_pos = mirrored_positions.view(batch_size, -1, self.nb_joints, 3)\n            global_rot = mirrored_rotations.view(batch_size, -1, self.nb_joints, 4)\n        return global_pos,global_rot", "\n\n\nclass Mirror(torch.nn.Module):\n    def __init__(self, skeleton: Skeleton,dtype:torch.dtype):\n        super().__init__()\n        self.register_buffer('reflection_matrix', torch.eye(3,dtype=dtype))\n        # quaternions must be in w, x, y, z order\n        if(skeleton.sys==\"x\"):# x\n            self.reflection_matrix[0, 0] = -1\n            self.register_buffer('quat_indices', torch.tensor([2, 3]))\n        elif(skeleton.sys=='z'):# z\n            self.reflection_matrix[2, 2] = -1\n            self.register_buffer('quat_indices', torch.tensor([1, 2]))\n        elif(skeleton.sys=='y'):\n            self.reflection_matrix[1,1] = -1\n            self.register_buffer('quat_indices', torch.tensor([1, 3]))\n        else:\n            assert(0,\"can't mirror\")\n\n\n\n\n        self.register_buffer('swap_index_list', torch.LongTensor(skeleton.bone_pair_indices))\n\n    def forward(self, joint_positions: torch.Tensor = None, joint_rotations: torch.Tensor = None):#t position in global space\n        self.reflection_matrix = self.reflection_matrix.to(joint_positions.device)\n        if joint_positions is None:\n            new_positions = None\n        else:\n            new_positions = self._swap_tensor(joint_positions)\n            new_positions = torch.matmul(self.reflection_matrix, new_positions.permute(1, 2, 0)).permute(2, 0, 1)\n            new_positions = new_positions.contiguous()\n\n        if joint_rotations is None:\n            new_rotations = None\n        else:\n            new_rotations = self._swap_tensor(joint_rotations)\n            new_rotations[:, :, self.quat_indices] *= -1\n            new_rotations = new_rotations.contiguous()\n\n        return new_positions, new_rotations\n\n    def _swap_tensor(self, tensor: torch.Tensor) -> torch.Tensor:\n        return tensor[:, self.swap_index_list, :].view_as(tensor)", "\n\n"]}
{"filename": "src/Datasets/Style100Processor.py", "chunked_list": ["import pickle\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom src.Datasets.BaseLoader import BasedLoader, BasedDataProcessor\nfrom src.utils.BVH_mod import read_bvh\nfrom src.utils.motion_process import subsample\n", "from src.utils.motion_process import subsample\n\n\nclass Swap100StyJoints():\n    def __call__(self,quats,offsets,parents,names):\n        order = [\"Hips\",\n                 \"LeftHip\",\"LeftKnee\",\"LeftAnkle\",\"LeftToe\",\n                 \"RightHip\",\"RightKnee\",\"RightAnkle\",\"RightToe\",\n                 \"Chest\",\"Chest2\",\"Chest3\",\"Chest4\",\"Neck\",\"Head\",\n                 \"LeftCollar\",\"LeftShoulder\",\"LeftElbow\",\"LeftWrist\",\n                 \"RightCollar\",\"RightShoulder\",\"RightElbow\",\"RightWrist\"]\n        #order = {name:i for i,name in enumerate(order)}\n        ori_order = {name:i for i,name in enumerate(names)}\n        tar_order = {name:i for i,name in enumerate(order)}\n        n_quats = np.empty_like(quats)\n        n_offsets = np.empty_like(offsets)\n        n_parents = parents.copy()\n        for i,name in enumerate(order):\n            or_idx = ori_order[name]\n            n_quats[:, i] = quats[:, or_idx]\n            n_offsets[ i] = offsets[ or_idx]\n            if(i!=or_idx):\n                par_name = names[parents[or_idx]]\n                par_id = tar_order[par_name]\n            else:\n                par_id = parents[or_idx]\n            n_parents[i] = par_id\n        return n_quats,n_offsets,n_parents,order", "\n\n\n#dataset_list = pd.read_csv(root_dir+\"Dataset_List.csv\")\n#anim = read_bvh(\"./MotionData/lafan1/train/aiming1_subject4.bvh\")\n#root_dir = \"./MotionData/100STYLE/\"\n#anim = read_bvh(root_dir+\"Aeroplane/Aeroplane_BR.bvh\",remove_joints=Swap100StyJoints())\n#save_bvh(\"example.bvh\",anim)\ndef bvh_to_binary():\n    root_dir = \"./MotionData/100STYLE/\"\n    frame_cuts = pd.read_csv(root_dir + \"Frame_Cuts.csv\")\n    n_styles = len(frame_cuts.STYLE_NAME)\n    style_name = [frame_cuts.STYLE_NAME[i] for i in range(n_styles)]\n    content_name = [\"BR\", \"BW\", \"FR\", \"FW\", \"ID\", \"SR\", \"SW\", \"TR1\", \"TR2\", \"TR3\"]\n\n    def extractSeqRange(start,end):\n        start = start.astype('Int64')\n        end = end.astype('Int64')\n\n        return [[(start[i]),(end[i])] for i in range(len(start))]\n    content_range = {name:extractSeqRange(frame_cuts[name+\"_START\"],frame_cuts[name+\"_STOP\"]) for name in content_name}\n    def clip_anim(anim,start,end):\n        anim.quats = anim.quats[start:end]\n        anim.hip_pos = anim.hip_pos[start:end]\n        return anim\n    for i in range(n_styles):\n        anim_style = {}\n        folder = root_dir + style_name[i] + \"/\"\n        for content in content_name:\n            ran = content_range[content][i]\n            if(type(content_range[content][i][0])!=type(pd.NA)):\n                file = folder+style_name[i]+\"_\"+content+\".bvh\"\n                anim = read_bvh(file,remove_joints=Swap100StyJoints())\n                anim = clip_anim(anim,ran[0],ran[1])\n                anim = subsample(anim,2)\n                anim_style[content] = {\"quats\":anim.quats.astype(np.float32),\"offsets\":anim.offsets.astype(np.float32),\"hips\":anim.hip_pos.astype(np.float32)}\n        f = open(folder+\"binary.dat\",\"wb+\")\n        pickle.dump(anim_style,f)\n        f.close()", "def bvh_to_binary():\n    root_dir = \"./MotionData/100STYLE/\"\n    frame_cuts = pd.read_csv(root_dir + \"Frame_Cuts.csv\")\n    n_styles = len(frame_cuts.STYLE_NAME)\n    style_name = [frame_cuts.STYLE_NAME[i] for i in range(n_styles)]\n    content_name = [\"BR\", \"BW\", \"FR\", \"FW\", \"ID\", \"SR\", \"SW\", \"TR1\", \"TR2\", \"TR3\"]\n\n    def extractSeqRange(start,end):\n        start = start.astype('Int64')\n        end = end.astype('Int64')\n\n        return [[(start[i]),(end[i])] for i in range(len(start))]\n    content_range = {name:extractSeqRange(frame_cuts[name+\"_START\"],frame_cuts[name+\"_STOP\"]) for name in content_name}\n    def clip_anim(anim,start,end):\n        anim.quats = anim.quats[start:end]\n        anim.hip_pos = anim.hip_pos[start:end]\n        return anim\n    for i in range(n_styles):\n        anim_style = {}\n        folder = root_dir + style_name[i] + \"/\"\n        for content in content_name:\n            ran = content_range[content][i]\n            if(type(content_range[content][i][0])!=type(pd.NA)):\n                file = folder+style_name[i]+\"_\"+content+\".bvh\"\n                anim = read_bvh(file,remove_joints=Swap100StyJoints())\n                anim = clip_anim(anim,ran[0],ran[1])\n                anim = subsample(anim,2)\n                anim_style[content] = {\"quats\":anim.quats.astype(np.float32),\"offsets\":anim.offsets.astype(np.float32),\"hips\":anim.hip_pos.astype(np.float32)}\n        f = open(folder+\"binary.dat\",\"wb+\")\n        pickle.dump(anim_style,f)\n        f.close()", "def save_skeleton():\n    root_dir = \"./MotionData/100STYLE/\"\n    anim = read_bvh(root_dir+\"Aeroplane/Aeroplane_BR.bvh\",remove_joints=Swap100StyJoints())\n    f = open(root_dir+\"skeleton\",\"wb+\")\n    pickle.dump(anim.skeleton,f)\n    f.close()\ndef read_binary():\n    root_dir = \"./MotionData/100STYLE/\"\n    frame_cuts = pd.read_csv(root_dir + \"Frame_Cuts.csv\")\n    n_styles = len(frame_cuts.STYLE_NAME)\n    style_name = [frame_cuts.STYLE_NAME[i] for i in range(n_styles)]\n    content_name = [\"BR\", \"BW\", \"FR\", \"FW\", \"ID\", \"SR\", \"SW\", \"TR1\", \"TR2\", \"TR3\"]\n    motion_styles={}\n    for i in range(n_styles):\n        #anim_style = {}\n        folder = root_dir + style_name[i] + \"/\"\n        f = open(folder + \"binary.dat\", \"rb\")\n        anim_style = pickle.load(f)\n        f.close()\n        motion_styles[style_name[i]]=anim_style\n    return motion_styles,style_name", "#bvh_to_binary()\n#save_skeleton()\n\n\nclass StyleLoader():\n    def __init__(self,root_dir = \"./MotionData/100STYLE/\"):\n        self.root_dir = root_dir\n\n    def setup(self,loader:BasedLoader,processor:BasedDataProcessor):\n        self.loader = loader\n        self.processor = processor\n    def save_part_to_binary(self,filename,keys):\n        path = self.root_dir\n        dict = {key:self.train_dict[key] for key in keys}\n        f = open(path + '/' + filename + \".dat\", \"wb+\")\n        pickle.dump(dict, f)\n        f.close()\n    def load_part_to_binary(self,filename):\n        path = \"./\"#self.root_dir\n        f = open(path + '/' + filename + \".dat\", \"rb\")\n        stat = pickle.load(f)\n        f.close()\n        return stat\n    # dataset: all motions, the motions are splited into windows\n    def save_dataset(self,filename):\n        path = self.root_dir\n        f = open(path + '/train' + filename + \".dat\", \"wb\")\n        pickle.dump(self.train_motions, f)\n        f.close()\n        f = open(path + '/test' + filename + \".dat\", \"wb\")\n        pickle.dump(self.test_motions, f)\n        f.close()\n    def load_dataset(self,filename):\n        path = self.root_dir\n        f = open(path + '/train' + filename + \".dat\", \"rb\")\n        self.train_motions = pickle.load(f)\n        f.close()\n        f = open(path + '/test' + filename + \".dat\", \"rb\")\n        self.test_motions = pickle.load(f)\n        f.close()\n    # save train set and test set\n    def save_train_test_dataset(self,filename):\n        path = self.root_dir\n        dict = {\"train\": self.train_dict, \"test\": self.test_dict}\n        f = open(path+'/'+filename+\".dat\",\"wb+\")\n        pickle.dump(dict,f)\n        f.close()\n    def load_train_test_dataset(self,filename):\n        path = self.root_dir\n        f = open(path + '/' + filename + \".dat\", \"rb\")\n        dict = pickle.load( f)#{\"train\": self.train_dict, \"test\": self.test_dict}\n        f.close()\n        self.train_dict = dict['train']\n        self.test_dict = dict['test']\n\n    def load_style_code(self,filename):\n        path = self.root_dir\n        f = open(path + '/' + filename + \"_stycode.dat\", \"rb\")\n        self.style_codes = pickle.load(f)\n        f.close()\n    def save_style_code(self,filename,style_codes):\n        path = self.root_dir\n        f = open(path + '/' + filename + \"_stycode.dat\", \"wb+\")\n        pickle.dump(style_codes, f)\n        f.close()\n    def load_from_binary(self,filename):\n        path = self.root_dir\n        f = open(path + '/' + filename + \".dat\", \"rb\")\n        self.all_motions = pickle.load(f)\n        f.close()\n    def save_to_binary(self,filename,all_motions):\n        path = self.root_dir\n        f = open(path + '/' + filename + \".dat\", \"wb+\")\n        pickle.dump(all_motions, f)\n        f.close()\n    def augment_dataset(self):\n        from src.Datasets.augmentation import TemporalScale,BatchMirror\n        folder = \"./MotionData/100STYLE/\"\n        self._load_skeleton(folder)\n        mirror = BatchMirror(self.skeleton,1.)\n        scale = TemporalScale(1.)\n\n        def augment_motions(motions):\n            for style in motions.keys():\n                content_keys = list(motions[style].keys())\n                for content in content_keys:\n                    seq = motions[style][content]\n                    quats = torch.from_numpy(seq['quats']).unsqueeze(0).float().cuda()\n                    offsets = torch.from_numpy(seq['offsets']).unsqueeze(0).float().cuda()\n                    hips = torch.from_numpy(seq['hips']).unsqueeze(0).float().cuda()\n                    # mirror\n                    gp,gq = self.skeleton.forward_kinematics(quats,offsets,hips)\n                    gp,gq = mirror(gp,gq)\n                    mirror_hips,mirror_quats = self.skeleton.inverse_kinematics(gq,gp)\n                    mirror_hips,mirror_quats = mirror_hips.squeeze(0).cpu().numpy(),mirror_quats.squeeze(0).cpu().numpy()\n                    motions[style][\"mr_\"+content] = {\"quats\":mirror_quats,\"offsets\":seq['offsets'],\"hips\":mirror_hips}\n                    # scale\n                    sc_hips,sc_quats = scale(hips,quats)\n                    sc_hips, sc_quats = sc_hips.squeeze(0).cpu().numpy(), sc_quats.squeeze(0).cpu().numpy()\n                    motions[style][\"sca_\" + content] = {\"quats\": sc_quats, \"offsets\": seq['offsets'], \"hips\": sc_hips}\n            return motions\n\n        f = open(folder + \"train_binary.dat\", 'rb')\n        self.train_motions = pickle.load(f)\n        f.close()\n        f = open(folder + 'test_binary.dat', 'rb')\n        self.test_motions = pickle.load(f)\n        f.close()\n\n        self.train_motions = augment_motions(self.train_motions)\n        self.test_motions = augment_motions(self.test_motions)\n        f = open(folder + \"train_binary_agument.dat\", \"wb\")\n        pickle.dump(self.train_motions, f)\n        f.close()\n        f = open(folder + \"test_binary_agument.dat\", \"wb\")\n        pickle.dump(self.test_motions, f)\n        f.close()\n\n\n\n    def split_from_binary(self):\n        folder = \"./MotionData/100STYLE/\"\n        self.load_skeleton_only()\n        self.all_motions, self.style_names = read_binary()\n\n        self.train_motions = {}\n        self.test_motions = {}\n        # \u955c\u50cf\u6570\u636e\u96c6\uff1a\n        from src.Datasets.BatchProcessor import BatchMirror\n        batch_mirror = BatchMirror(self.skeleton, 1.)\n        for style in self.style_names[:-10]:\n            self.train_motions[style]={}\n            self.test_motions[style]={}\n            for content in self.all_motions[style].keys():\n                seq = self.all_motions[style][content]\n\n                length = seq['quats'].shape[0]\n                if(length>2000):\n                    test_length = length//10\n                    self.train_motions[style][content]={}\n                    self.train_motions[style][content]['quats'] = seq['quats'][:-test_length]\n                    self.train_motions[style][content]['offsets'] = seq['offsets']#[:-test_length]\n                    self.train_motions[style][content]['hips'] = seq['hips'][:-test_length]\n                    self.test_motions[style][content]={}\n                    self.test_motions[style][content]['quats'] = seq['quats'][-test_length:]\n                    self.test_motions[style][content]['offsets'] = seq['offsets']#[-test_length:]\n                    self.test_motions[style][content]['hips'] = seq['hips'][-test_length:]\n                else:\n                    self.train_motions[style][content] = seq\n        for style in self.style_names[-10:]:\n            self.test_motions[style] = self.all_motions[style]\n\n\n\n        f = open(folder + \"train_binary.dat\", \"wb\")\n        pickle.dump(self.train_motions,f)\n        f.close()\n        f = open(folder+\"test_binary.dat\",\"wb\")\n        pickle.dump(self.test_motions,f)\n        f.close()\n    # read from each file\n    def process_from_binary(self,argument = True):\n        folder = \"./MotionData/100STYLE/\"\n        if(argument):\n            f = open(folder+\"train_binary_agument.dat\",'rb')\n        else:\n            f = open(folder+\"train_binary.dat\",'rb')\n        self.train_motions = pickle.load(f)\n        f.close()\n        if (argument):\n            f = open(folder + \"test_binary_agument.dat\", 'rb')\n        else:\n            f = open(folder + \"test_binary.dat\", 'rb')\n\n        self.test_motions = pickle.load(f)\n        f.close()\n        self.train_dict = self._process_dataset(self.train_motions)\n        self.test_dict = self._process_dataset(self.test_motions)\n        print(\"process done\")\n        #self.loader.load_data()\n\n    def _process_dataset(self,motions):\n        o, h, q, s = [], [], [], []\n        for style_name in motions.keys():\n            for content_name in motions[style_name]:\n                dict = motions[style_name][content_name]\n                off, hip, quat = self.loader.load_data(dict['offsets'], dict['hips'], dict['quats'])\n                dict['offsets'], dict['hip_pos'], dict['quats'] = off, hip, quat\n                del dict['hips']\n                o += off\n                h += hip\n                q += quat\n                s += [style_name for i in range(2*len(off))]\n                # motions[style_name][content_name] = dict\n        self.load_skeleton_only()\n\n        train_set = self.processor({\"offsets\": o, \"hip_pos\": h, \"quats\": q}, self.skeleton, self)\n\n        train_set[\"style\"] = s\n        return train_set\n    def _load_skeleton(self,path):\n        f = open(path +'/'+ 'skeleton', 'rb')\n        self.skeleton = pickle.load(f)\n        f.close()\n    def load_skeleton_only(self):\n        self._load_skeleton(self.root_dir)", "\n\n"]}
{"filename": "src/Datasets/BatchProcessor.py", "chunked_list": ["import random\n\nimport numpy as np\nimport pytorch3d\nimport torch.utils.data\nfrom pytorch3d.transforms import quaternion_apply, quaternion_multiply\n\nfrom src.Datasets.BaseLoader import BasedDataProcessor\nfrom src.Datasets.augmentation import angle_between_quats_along_y\nfrom src.geometry.quaternions import quat_to_or6D, or6d_to_quat, from_to_1_0_0", "from src.Datasets.augmentation import angle_between_quats_along_y\nfrom src.geometry.quaternions import quat_to_or6D, or6d_to_quat, from_to_1_0_0\n\n\nclass BatchProcessDatav2(torch.nn.Module):\n    def __init__(self):\n        super(BatchProcessDatav2, self).__init__()\n    def forward(self,glb_rot,glb_pos):\n        dir = glb_pos[..., 5:6, :] - glb_pos[..., 1:2, :]\n        ref_vector = torch.cat((-dir[...,2:3],torch.zeros_like(dir[...,1:2]),dir[...,0:1]),dim=-1)\n        root_rotation = from_to_1_0_0(ref_vector)\n        glb_vel = quaternion_apply(root_rotation[:,:-1], glb_pos[:,1:]-glb_pos[:,:-1])\n        glb_rot = quaternion_multiply(root_rotation,glb_rot)\n        glb_pos = glb_pos.clone()\n        glb_pos[...,[0,2]] = glb_pos[...,[0,2]]-glb_pos[...,0:1,[0,2]]\n        glb_pos = quaternion_apply(root_rotation,glb_pos)\n        return glb_vel,glb_pos,glb_rot,root_rotation", "class BatchUnProcessDatav2(torch.nn.Module):\n    # we don't need use it in training\n    def __init__(self):\n        super(BatchUnProcessDatav2, self).__init__()\n    def forward(self,glb_pos,glb_rot,glb_vel,root_rotation):\n        # glb_pos: N,T,J,3\n        inverse_rot = pytorch3d.transforms.quaternion_invert(root_rotation)\n        glb_pos = quaternion_apply(inverse_rot,glb_pos)\n        out_pos = torch.empty(size=glb_rot.shape[:-1]+(3,),dtype=glb_pos.dtype,device=glb_pos.device)\n        out_pos[:,0:1,:,:] = glb_pos[:,0:1,:,:]\n\n        glb_vel = quaternion_apply(inverse_rot[:,:-1],glb_vel)\n        for i in range(0,glb_vel.shape[1]):\n            out_pos[:,i+1,...] = out_pos[:,i]+glb_vel[:,i]\n        glb_rot = quaternion_multiply(inverse_rot,glb_rot)\n        #glb_pos = out_pos\n        glb_pos[...,[0,2]]=out_pos[:,:,0:1,[0,2]]+glb_pos[...,[0,2]]\n        return  glb_pos,glb_rot", "\n\n\nclass BatchProcessData(torch.nn.Module):#(B,T,J,dim)\n    def __init__(self):\n        super(BatchProcessData, self).__init__()\n    def forward(self,global_rotations,global_positions):\n        ref_vector = torch.cross(global_positions[...,5:6,:]-global_positions[...,1:2,:],torch.tensor([0,1,0],dtype=global_positions.dtype,device=global_positions.device),dim=-1)\n\n        root_rotation = from_to_1_0_0(ref_vector)\n        \"\"\" Local Space \"\"\"\n        local_positions = global_positions.clone()\n        local_positions[..., 0] = local_positions[..., 0] - local_positions[..., 0:1, 0]\n        local_positions[...,2] = local_positions[..., 2] - local_positions[..., 0:1, 2]\n\n        local_positions = quaternion_apply(root_rotation, local_positions)\n        local_velocities = quaternion_apply(root_rotation[:,:-1], (global_positions[:,1:] - global_positions[:,:-1]))\n        local_rotations = quaternion_multiply(root_rotation, global_rotations)\n\n        local_rotations = quat_to_or6D(local_rotations)\n        if torch.isnan(local_rotations).any():\n            print(\"6D\")\n            assert False\n        root_global_velocity_XZ = global_positions[:,1:, 0:1] - global_positions[:,:-1, 0:1]\n        root_global_velocity_XZ[..., 1] = 0\n        root_velocity = quaternion_apply(root_rotation[:,:-1, :], (root_global_velocity_XZ))\n        #root_rvelocity = angle_between_quats_along_y(global_rotations[:,:-1, 0:1, :], global_rotations[:,1:, 0:1, :])\n        root_rvelocity = angle_between_quats_along_y(root_rotation[:,1:, 0:1, :], root_rotation[:,:-1, 0:1, :])\n        dict = {\"root_rotation\":root_rotation,\n            \"local_pos\":local_positions[:,:,...],\"local_vel\":local_velocities,\"local_rot\":local_rotations[:,:,...],\"root_vel\":root_velocity[...,[0,2]],\"root_rvel\":root_rvelocity.unsqueeze(-1)}\n\n        return dict", "class UnProcessData(torch.nn.Module):\n    def __init__(self):\n        super(UnProcessData, self).__init__()\n\n    def get_global_rotation(self,root_rvelocity):\n        '''root_rvelocity:  N,T,...,C'''\n        #r = torch.zeros_like(root_rvelocity)  # BxTx1 or Bx1 or BxTx1x1\n        shape = list(root_rvelocity.shape)\n        shape[1]+=1\n        r = torch.zeros(shape,device=root_rvelocity.device)\n        for i in range(1, r.shape[1]):\n                r[:, i] = r[:, i - 1] + root_rvelocity[:, i - 1]\n        shape = [1 for i in range(r.dim())]\n        shape[-1] = 3\n        axis = torch.zeros(size = shape,device=r.device)\n        axis[...,0:3] = torch.tensor([0,1,0])\n        rotation = pytorch3d.transforms.axis_angle_to_quaternion(axis * r)  # B,T,1,4\n        return rotation\n    def get_global_xz(self,global_hip_rot,hip_velocity):\n        root_velocity3D = torch.zeros(size=(hip_velocity.shape[0], hip_velocity.shape[1], 1, 3),device=global_hip_rot.device,dtype=hip_velocity.dtype)\n        root_velocity3D[..., [0, 2]] = hip_velocity.clone()\n        root_velocity3D = quaternion_apply(global_hip_rot[:,:-1,...], root_velocity3D)\n        shape = list(root_velocity3D.shape)\n        shape[1]+=1\n        root_pos_XZ = torch.zeros(shape,device=root_velocity3D.device)\n        for i in range(1, (root_velocity3D.shape[1]+1)):\n            root_pos_XZ[:, i, 0, :] = root_pos_XZ[:, i - 1, 0, :] + root_velocity3D[:, i - 1, 0, :]\n        return root_pos_XZ\n\n\n\n    def forward(self,batch):\n        num_joints = batch['local_pos'].shape[-2]\n        batch_size = batch['local_pos'].shape[0]\n        root_rvelocity = batch['root_rvel']\n        local_positions = batch['local_pos']\n        local_rotations = batch['local_rot']\n        local_rotations = or6d_to_quat(local_rotations)\n        root_velocity = batch['root_vel']\n\n\n\n        rotation = self.get_global_rotation(root_rvelocity)\n        '''transform to global rotation'''\n        global_rot = quaternion_multiply(rotation,local_rotations)\n        relative_pos = quaternion_apply(rotation,local_positions)\n        global_pos_xz = self.get_global_xz(rotation,root_velocity)\n        '''transform to global pos'''\n        global_pos = relative_pos + global_pos_xz\n        #return relative_pos,local_rotations\n       # global_pos = local_positions\n      #  global_rot = local_rotations\n        batch['global_pos'] = global_pos\n        batch['global_rot'] = global_rot\n        return global_pos,global_rot", "\nclass BatchRotateYCenterXZ(torch.nn.Module):\n    def __init__(self):\n        super(BatchRotateYCenterXZ, self).__init__()\n    def forward(self,global_positions,global_quats,ref_frame_id):\n        ref_vector = torch.cross(global_positions[:, ref_frame_id:ref_frame_id+1, 5:6, :] - global_positions[:, ref_frame_id:ref_frame_id+1, 1:2, :],\n                                 torch.tensor([0, 1, 0], dtype=global_positions.dtype, device=global_positions.device).view(1,1,1,3),dim=-1)\n        root_rotation = from_to_1_0_0(ref_vector)\n\n        ref_hip = torch.mean(global_positions[:,:,0:1,[0,2]],dim=(1),keepdim=True)\n        global_positions[...,[0,2]] = global_positions[...,[0,2]] - ref_hip\n        global_positions = quaternion_apply(root_rotation, global_positions)\n        global_quats = quaternion_multiply(root_rotation, global_quats)\n        \"\"\" Local Space \"\"\"\n\n\n        return global_positions,global_quats", "\n"]}
{"filename": "src/Datasets/BaseDataSet.py", "chunked_list": ["import torch\nimport torch.utils.data\nclass StreamDataSetHelper():\n    '''first dimsion of data is seq length'''\n    def __init__(self,example_data_list,window_size = 1):\n        self.length = 0\n        self.prefix_list = [0]\n        for i in range(len(example_data_list)):\n            self.length+=example_data_list[i].shape[0]-window_size+1\n            self.prefix_list.append(self.length)\n    def __binary_search(self,idx,start,end):\n        middle = (start+end)//2\n        if(self.prefix_list[middle]<=idx and self.prefix_list[middle+1]>idx):\n            return middle\n        elif(self.prefix_list[middle+1]<=idx):\n            return self.__binary_search(idx,middle+1,end)\n        elif(self.prefix_list[middle]>idx):\n            return self.__binary_search(idx,start,middle)\n        else:\n            assert False\n\n    def __len__(self):\n        return self.length\n    def __getitem__(self, item):\n\n        results = self.__binary_search(item,0,len(self.prefix_list)-1)\n        assert results>=0\n        return results,item-self.prefix_list[results]", "\n"]}
{"filename": "src/Net/DeepPhaseNet.py", "chunked_list": ["from torch import nn\nfrom torch._lowrank import pca_lowrank\nimport pytorch_lightning as pl\nfrom src.Net.CommonOperation import CommonOperator\nfrom src.utils.Drawer import Drawer\nfrom src.Datasets.DeepPhaseDataModule import DeephaseDataSet\nimport numpy as np\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdamW(Optimizer):\n    \"\"\"Implements Adam algorithm.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        #super(AdamW, self).__init__(params, defaults)\n        super().__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsgrad = group['amsgrad']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.mul_(1 - group['weight_decay']).addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss", "\n\nclass AdamW(Optimizer):\n    \"\"\"Implements Adam algorithm.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        #super(AdamW, self).__init__(params, defaults)\n        super().__init__(params, defaults)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsgrad = group['amsgrad']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsgrad:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n                p.data.mul_(1 - group['weight_decay']).addcdiv_(exp_avg, denom, value=-step_size)\n\n        return loss", "from torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport math\nimport torch\n\n\nclass ReduceMaxLROnRestart:\n    def __init__(self, ratio=0.75):\n        self.ratio = ratio\n\n    def __call__(self, eta_min, eta_max):\n        return eta_min, eta_max * self.ratio", "\n\nclass ExpReduceMaxLROnIteration:\n    def __init__(self, gamma=1):\n        self.gamma = gamma\n\n    def __call__(self, eta_min, eta_max, iterations):\n        return eta_min, eta_max * self.gamma ** iterations\n\n\nclass CosinePolicy:\n    def __call__(self, t_cur, restart_period):\n        return 0.5 * (1. + math.cos(math.pi *\n                                    (t_cur / restart_period)))", "\n\nclass CosinePolicy:\n    def __call__(self, t_cur, restart_period):\n        return 0.5 * (1. + math.cos(math.pi *\n                                    (t_cur / restart_period)))\n\n\nclass ArccosinePolicy:\n    def __call__(self, t_cur, restart_period):\n        return (math.acos(max(-1, min(1, 2 * t_cur\n                                      / restart_period - 1))) / math.pi)", "class ArccosinePolicy:\n    def __call__(self, t_cur, restart_period):\n        return (math.acos(max(-1, min(1, 2 * t_cur\n                                      / restart_period - 1))) / math.pi)\n\n\nclass TriangularPolicy:\n    def __init__(self, triangular_step=0.5):\n        self.triangular_step = triangular_step\n\n    def __call__(self, t_cur, restart_period):\n        inflection_point = self.triangular_step * restart_period\n        point_of_triangle = (t_cur / inflection_point\n                             if t_cur < inflection_point\n                             else 1.0 - (t_cur - inflection_point)\n                             / (restart_period - inflection_point))\n        return point_of_triangle", "\n\nclass CyclicLRWithRestarts(_LRScheduler):\n    \"\"\"Decays learning rate with cosine annealing, normalizes weight decay\n    hyperparameter value, implements restarts.\n    https://arxiv.org/abs/1711.05101\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        batch_size: minibatch size\n        epoch_size: training samples per epoch\n        restart_period: epoch count in the first restart period\n        t_mult: multiplication factor by which the next restart period will expand/shrink\n        policy: [\"cosine\", \"arccosine\", \"triangular\", \"triangular2\", \"exp_range\"]\n        min_lr: minimum allowed learning rate\n        verbose: print a message on every restart\n        gamma: exponent used in \"exp_range\" policy\n        eta_on_restart_cb: callback executed on every restart, adjusts max or min lr\n        eta_on_iteration_cb: callback executed on every iteration, adjusts max or min lr\n        triangular_step: adjusts ratio of increasing/decreasing phases for triangular policy\n\n\n    Example:\n        >>> scheduler = CyclicLRWithRestarts(optimizer, 32, 1024, restart_period=5, t_mult=1.2)\n        >>> for epoch in range(100):\n        >>>     scheduler.step()\n        >>>     train(...)\n        >>>         ...\n        >>>         optimizer.zero_grad()\n        >>>         loss.backward()\n        >>>         optimizer.step()\n        >>>         scheduler.batch_step()\n        >>>     validate(...)\n    \"\"\"\n\n    def __init__(self, optimizer, batch_size, epoch_size, restart_period=100,\n                 t_mult=2, last_epoch=-1, verbose=False,\n                 policy=\"cosine\", policy_fn=None, min_lr=1e-7,\n                 eta_on_restart_cb=None, eta_on_iteration_cb=None,\n                 gamma=1.0, triangular_step=0.5):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n\n        self.optimizer = optimizer\n\n        if last_epoch == -1:\n            for group in optimizer.param_groups:\n                group.setdefault('initial_lr', group['lr'])\n                group.setdefault('minimum_lr', min_lr)\n        else:\n            for i, group in enumerate(optimizer.param_groups):\n                if 'initial_lr' not in group:\n                    raise KeyError(\"param 'initial_lr' is not specified \"\n                                   \"in param_groups[{}] when resuming an\"\n                                   \" optimizer\".format(i))\n\n        self.base_lrs = [group['initial_lr'] for group\n                         in optimizer.param_groups]\n\n        self.min_lrs = [group['minimum_lr'] for group\n                        in optimizer.param_groups]\n\n        self.base_weight_decays = [group['weight_decay'] for group\n                                   in optimizer.param_groups]\n\n        self.policy = policy\n        self.eta_on_restart_cb = eta_on_restart_cb\n        self.eta_on_iteration_cb = eta_on_iteration_cb\n        if policy_fn is not None:\n            self.policy_fn = policy_fn\n        elif self.policy == \"cosine\":\n            self.policy_fn = CosinePolicy()\n        elif self.policy == \"arccosine\":\n            self.policy_fn = ArccosinePolicy()\n        elif self.policy == \"triangular\":\n            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n        elif self.policy == \"triangular2\":\n            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n            self.eta_on_restart_cb = ReduceMaxLROnRestart(ratio=0.5)\n        elif self.policy == \"exp_range\":\n            self.policy_fn = TriangularPolicy(triangular_step=triangular_step)\n            self.eta_on_iteration_cb = ExpReduceMaxLROnIteration(gamma=gamma)\n\n        self.last_epoch = last_epoch\n        self.batch_size = batch_size\n        self.epoch_size = epoch_size\n\n        self.iteration = 0\n        self.total_iterations = 0\n\n        self.t_mult = t_mult\n        self.verbose = verbose\n        self.restart_period = math.ceil(restart_period)\n        self.restarts = 0\n        self.t_epoch = -1\n        self.epoch = -1\n\n        self.eta_min = 0\n        self.eta_max = 1\n\n        self.end_of_period = False\n        self.batch_increments = []\n        self._set_batch_increment()\n\n    def _on_restart(self):\n        if self.eta_on_restart_cb is not None:\n            self.eta_min, self.eta_max = self.eta_on_restart_cb(self.eta_min,\n                                                                self.eta_max)\n\n    def _on_iteration(self):\n        if self.eta_on_iteration_cb is not None:\n            self.eta_min, self.eta_max = self.eta_on_iteration_cb(self.eta_min,\n                                                                  self.eta_max,\n                                                                  self.total_iterations)\n\n    def get_lr(self, t_cur):\n        eta_t = (self.eta_min + (self.eta_max - self.eta_min)\n                 * self.policy_fn(t_cur, self.restart_period))\n\n        weight_decay_norm_multi = math.sqrt(self.batch_size /\n                                            (self.epoch_size *\n                                             self.restart_period))\n\n        lrs = [min_lr + (base_lr - min_lr) * eta_t for base_lr, min_lr\n               in zip(self.base_lrs, self.min_lrs)]\n        weight_decays = [base_weight_decay * eta_t * weight_decay_norm_multi\n                         for base_weight_decay in self.base_weight_decays]\n\n        if (self.t_epoch + 1) % self.restart_period < self.t_epoch:\n            self.end_of_period = True\n\n        if self.t_epoch % self.restart_period < self.t_epoch:\n            if self.verbose:\n                print(\"Restart {} at epoch {}\".format(self.restarts + 1,\n                                                      self.last_epoch))\n            self.restart_period = math.ceil(self.restart_period * self.t_mult)\n            self.restarts += 1\n            self.t_epoch = 0\n            self._on_restart()\n            self.end_of_period = False\n\n        return zip(lrs, weight_decays)\n\n    def _set_batch_increment(self):\n        d, r = divmod(self.epoch_size, self.batch_size)\n        batches_in_epoch = d + 2 if r > 0 else d + 1\n        self.iteration = 0\n        self.batch_increments = torch.linspace(0, 1, batches_in_epoch).tolist()\n\n    def step(self):\n        self.last_epoch += 1\n        self.t_epoch += 1\n        self._set_batch_increment()\n        self.batch_step()\n\n    def batch_step(self):\n        try:\n            t_cur = self.t_epoch + self.batch_increments[self.iteration]\n            self._on_iteration()\n            self.iteration += 1\n            self.total_iterations += 1\n        except (IndexError):\n            raise StopIteration(\"Epoch size and batch size used in the \"\n                                \"training loop and while initializing \"\n                                \"scheduler should be the same.\")\n\n        for param_group, (lr, weight_decay) in zip(self.optimizer.param_groups,\n                                                   self.get_lr(t_cur)):\n            param_group['lr'] = lr\n            param_group['weight_decay'] = weight_decay\n        return lr", "\ndef PhaseManifold(A,S):\n    shape = list(A.shape)\n    shape[-1]*=2\n    output = torch.empty((shape))\n    output[...,::2] = A*torch.cos(2*torch.pi*S)\n    output[...,1::2] = A*torch.sin(2*torch.pi*S)\n    return output\n\n", "\n\n\nclass PAE_AI4Animation(nn.Module):\n    def __init__(self,n_phases, n_joints,length, key_range=1., window=2.0):\n        super(PAE_AI4Animation, self).__init__()\n        embedding_channels = n_phases\n        input_channels = (n_joints)*3\n        time_range = length\n        self.n_phases = n_phases\n        self.input_channels = input_channels\n        self.embedding_channels = embedding_channels\n        self.time_range = time_range\n        self.key_range = key_range\n\n        self.window = window\n        self.time_scale = key_range / time_range\n\n        self.tpi = nn.Parameter(torch.from_numpy(np.array([2.0 * np.pi], dtype=np.float32)), requires_grad=False)\n        self.args = nn.Parameter(\n            torch.from_numpy(np.linspace(-self.window / 2, self.window / 2, self.time_range, dtype=np.float32)),\n            requires_grad=False)\n        self.freqs = nn.Parameter(torch.fft.rfftfreq(time_range)[1:] * (time_range * self.time_scale) / self.window,\n                               requires_grad=False)  # Remove DC frequency\n\n        intermediate_channels = int(input_channels / 3)\n\n        self.conv1 = nn.Conv1d(input_channels, intermediate_channels, time_range, stride=1,\n                               padding=int((time_range - 1) / 2), dilation=1, groups=1, bias=True, padding_mode='zeros')\n        self.bn_conv1 = nn.BatchNorm1d(num_features=intermediate_channels)\n        self.conv2 = nn.Conv1d(intermediate_channels, embedding_channels, time_range, stride=1,\n                               padding=int((time_range - 1) / 2), dilation=1, groups=1, bias=True, padding_mode='zeros')\n        self.bn_conv2 = nn.BatchNorm1d(num_features=embedding_channels)\n\n        self.fc = torch.nn.ModuleList()\n        self.bn = torch.nn.ModuleList()\n        for i in range(embedding_channels):\n            self.fc.append(nn.Linear(time_range, 2))\n            self.bn.append(nn.BatchNorm1d(num_features=2))\n        self.parallel_fc0 = nn.Linear(time_range,embedding_channels)\n        self.parallel_fc1 = nn.Linear(time_range,embedding_channels)\n\n        self.deconv1 = nn.Conv1d(embedding_channels, intermediate_channels, time_range, stride=1,\n                                 padding=int((time_range - 1) / 2), dilation=1, groups=1, bias=True,\n                                 padding_mode='zeros')\n        self.bn_deconv1 = nn.BatchNorm1d(num_features=intermediate_channels)\n        self.deconv2 = nn.Conv1d(intermediate_channels, input_channels, time_range, stride=1,\n                                 padding=int((time_range - 1) / 2), dilation=1, groups=1, bias=True,\n                                 padding_mode='zeros')\n\n    def atan2(self, y, x):\n        tpi = self.tpi\n        ans = torch.atan(y / x)\n        ans = torch.where((x < 0) * (y >= 0), ans + 0.5 * tpi, ans)\n        ans = torch.where((x < 0) * (y < 0), ans - 0.5 * tpi, ans)\n        return ans\n\n    # Returns the frequency for a function over a time window in s\n    def FFT(self, function, dim):\n        rfft = torch.fft.rfft(function, dim=dim)\n        magnitudes = rfft.abs()\n        spectrum = magnitudes[:, :, 1:]  # Spectrum without DC component\n        power = spectrum ** 2\n\n        # Frequency\n        freq = torch.sum(self.freqs * power, dim=dim) / torch.sum(power, dim=dim)\n        freq = freq / self.time_scale\n\n        # Amplitude\n        amp = 2 * torch.sqrt(torch.sum(power, dim=dim)) / self.time_range\n\n        # Offset\n        offset = rfft.real[:, :, 0] / self.time_range  # DC component\n\n        return freq, amp, offset\n\n    def forward(self, x):\n        y = x\n\n        # Signal Embedding\n        y = y.reshape(y.shape[0], self.input_channels, self.time_range)\n\n        y = self.conv1(y)\n        y = self.bn_conv1(y)\n        y = torch.tanh(y)\n\n        y = self.conv2(y)\n        y = self.bn_conv2(y)\n        y = torch.tanh(y)\n\n        latent = y  # Save latent for returning\n\n        # Frequency, Amplitude, Offset\n        f, a, b = self.FFT(y, dim=2)\n\n        # Phase\n        sx = self.parallel_fc0(y).diagonal(dim1=-2,dim2=-1).unsqueeze(-1).contiguous()\n        sy = self.parallel_fc1(y).diagonal(dim1=-2,dim2=-1).unsqueeze(-1).contiguous()\n        v = torch.cat([sx,sy],dim=-1) # B x M x 2\n        tv = torch.empty_like(v)\n        for i in range(self.embedding_channels):\n            tv[:,i,:] = self.bn[i](v[:,i,:])\n        p = self.atan2(tv[:,:,1],tv[:,:,0])/self.tpi\n\n        #################### the original code ####################\n        # p = torch.empty((y.shape[0], self.embedding_channels), dtype=torch.float32, device=y.device)\n        # for i in range(self.embedding_channels):\n        #     v = self.fc[i](y[:, i, :])\n        #     v = self.bn[i](v)\n        #     p[:, i] = self.atan2(v[:, 1], v[:, 0]) / self.tpi\n        ###########################################################\n\n\n        # Parameters\n        p = p.unsqueeze(2)\n        f = f.unsqueeze(2)\n        a = a.unsqueeze(2)\n        b = b.unsqueeze(2)\n        params = [p, f, a, b]  # Save parameters for returning\n\n        # Latent Reconstruction\n        y = a * torch.sin(self.tpi * (f * self.args + p)) + b\n\n        signal = y  # Save signal for returning\n\n        # Signal Reconstruction\n        y = self.deconv1(y)\n        y = self.bn_deconv1(y)\n        y = torch.tanh(y)\n\n        y = self.deconv2(y)\n\n        return y, p, a, f, b", "\n\n\n\n\nclass DeepPhaseNet(pl.LightningModule):\n    def __init__(self,n_phase,skeleton,length,dt,batch_size):\n        super(DeepPhaseNet, self).__init__()\n       # self.automatic_optimization = False\n        self.save_hyperparameters(ignore=['style_loader'])\n        self.lr = 1e-3\n        self.weight_decay = 1e-4\n        self.dt = dt\n        self.skeleton  = skeleton\n        self.model = PAE_AI4Animation(n_phase,skeleton.num_joints,length)\n        self.mse_loss = nn.MSELoss()\n        self.oper = CommonOperator(batch_size)\n    def transform_to_pca(self,input):\n        # input: N,T,J,D\n        input = input*self.dt\n        std = torch.std(input,dim=[-2,-1],keepdim=True)\n        mean = torch.mean(input,dim=[-2,-1],keepdim=True)\n        input = (input-mean)/std\n        input = input.flatten(1,2)\n        return input\n    def forward(self,input):\n        input = input.flatten(1,2)\n        Y,S,A,F,B = self.model(input)\n        loss ={ \"loss\":self.mse_loss(input,Y)}\n        return loss,input,Y\n    def training_step(self, batch,batch_idx):\n        loss,_,_ = self.forward(batch)\n        self.oper.log_dict(self,loss,\"train_\")\n        return loss['loss']\n    def validation_step(self, batch,batch_idx) :\n        loss,input,Y = self.forward(batch)\n        self.oper.log_dict(self, loss, \"val_\")\n        return loss['loss']\n    def test_step(self,batch,batch_idx):\n        loss = self.forward(batch)\n        self.oper.log_dict(self,loss,\"test_\")\n        return loss['loss']\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        scheduler = CyclicLRWithRestarts(optimizer=optimizer, batch_size=32,\n                                          epoch_size=75830, restart_period=10,\n                                          t_mult=2, policy=\"cosine\", verbose=True)\n        return [optimizer], {'scheduler': scheduler, 'interval': 'step', 'frequency': 1}", "'''\u589e\u52a0\u4e00\u4e2aloss\u8ba1\u7b97\uff0c\u786e\u4fdd\u8f93\u5165\u548c\u7f51\u7edc\u8bad\u7ec3\u7684\u65f6\u5019\u662f\u4e00\u81f4\u7684'''\nfrom src.Datasets.DeepPhaseDataModule import DeepPhaseProcessor\nclass Application(nn.Module):\n    def __init__(self,Net:DeepPhaseNet,datamodule):\n        super(Application, self).__init__()\n        self.Net = Net\n        self.drawer = Drawer()\n        self.module = datamodule\n        self.processor = DeepPhaseProcessor (1/30.)\n        self.window = datamodule.window\n    def draw_phase(self,label,input,channel,feature):\n        '''input: Batch x M x 1'''\n        self.drawer.ax[channel,feature].set_xlabel(label)\n        self.drawer.draw(input.cpu().numpy(),channel=channel,feature=feature)\n    def draw_dict(self,input:dict):\n        size = len(input)\n        print(self.Net.model.n_phases)\n        self.drawer.create_canvas(self.Net.model.n_phases,size)\n        feature_id = 0\n        for key in input.keys():\n            data = input[key]\n            for phase in range(data.shape[1]):\n                self.draw_phase(key,data[:,phase],phase,feature_id)\n            feature_id+=1\n\n    def _transform_anim(self,anim):\n        offsets,hip_pos,quats = anim.offsets,anim.hip_pos,anim.quats#self.loader(anim)\n        offsets = np.expand_dims(offsets,axis=0)\n        hip_pos = np.expand_dims(hip_pos,axis=0)\n        quats = np.expand_dims(quats,axis=0)\n        gv = self.processor.transform_single(offsets,hip_pos,quats,self.module.skeleton)\n        batch = gv\n        return batch\n    def setAnim(self,anim):\n        self.input = self._transform_anim(anim).to(torch.float32)#.cpu()\n        self.dataset = DeephaseDataSet(self.input,self.window)\n    def forward_window(self,batch):\n        input = batch\n        input = self.module.on_after_batch_transfer(input,0)\n        return self.calculate_phase(input)\n    def calculate_FFT(self,embedding):\n        N = embedding.shape[-1]\n        model = self.Net.model\n        A, F, B = model.fft_layer(embedding, N)\n        return A,F,B\n    def calculate_phase(self,x):\n        N = x.shape[-1]\n        model = self.Net.model\n        self.Net = self.Net.to(self.Net.device)\n        x = x.to(self.Net.device)\n        self.Net.eval()\n        x = x.flatten(1,2 )\n        Y,S,A,F,B = model.forward(x)\n        print(\"mseLoss:{}\".format(self.Net.mse_loss(Y,x)))\n        return {\n            \"S\":S,\"A\":A,\"F\":F,\"B\":B\n        }\n    def calculate_statistic_for_dataset(self,dataset):\n        self.Net.eval()\n        with torch.no_grad():\n            phases = {\"F\": [], \"A\": [], \"S\": [], \"B\": []}\n            step = 1\n            length = len(dataset)\n            input = []\n            for i in range(length):\n                input.append(dataset[i].unsqueeze(0))\n\n            batch = torch.cat(input, 0)\n            batch = batch.to(\"cuda\")\n            phase = self.forward_window(batch)\n            for key in phase.keys():\n                phase[key] = phase[key].cpu().numpy()\n            return phase\n    def forward(self):\n        import matplotlib.pyplot as plt\n        self.Net.eval()\n        with torch.no_grad():\n            start = 0\n            length = max(self.window,len(self.dataset)-self.window-10)\n            input = []\n            for i in range(length):\n                input.append(self.dataset[i+start].unsqueeze(0))\n\n            batch = torch.cat(input,0)\n            phase = self.forward_window(batch)\n            self.draw_dict(phase)\n            self.drawer.show()\n\n            phase = PhaseManifold(phase['A'][:,:,0],phase['S'][:,:,0])\n            U,S,V = pca_lowrank(phase)\n            proj = torch.matmul(phase,V)\n            proj = proj[:,:2]\n            c = np.arange(0,proj.shape[0],step=1.)\n            plt.scatter(proj[:,0],proj[:,1],c=c)\n            plt.show()", "\n\n\n"]}
{"filename": "src/Net/CommonOperation.py", "chunked_list": ["from torch import nn\nimport pytorch_lightning as pl\nimport torch\nimport math\nclass CommonOperator():\n    def __init__(self,batch_size):\n        self.batch_size = batch_size\n        self.steps_per_epoch = None\n        pass\n\n    def add_prefix_to_loss(self, loss:dict, prefix:str):\n        output = {}\n        for key, value in loss.items():\n            if(type(value)==torch.Tensor):\n                output[prefix + key] = value.detach()\n            else:\n                output[prefix + key] = value\n        return output\n    def set_lr(self,lr,optimizer):\n        for pg in optimizer.param_groups:\n            pg['lr'] = lr\n    def log_dict(self,obj:pl.LightningModule,loss:dict,prefix:str):\n        loss_ = self.add_prefix_to_loss(loss,prefix)\n        obj.log_dict(loss_,prog_bar=True,logger=True,batch_size=self.batch_size)\n        return loss_\n\n    def add_weight_decay(self,model,lr, weight_decay=1e-5, skip_list=()):\n        decay = []\n        no_decay = []\n        for name, param in model.named_parameters():\n            if not param.requires_grad:\n                continue\n            if len(param.shape) == 1 or name in skip_list:\n                no_decay.append(param)\n            else:\n                decay.append(param)\n        return [\n            {'params': no_decay, 'weight_decay': 0.,\"lr\":lr},\n            {'params': decay, 'weight_decay': weight_decay,\"lr\":lr}]\n    def num_training_steps(self,obj:pl.LightningModule) -> int:\n        \"\"\"Total training steps inferred from datamodule and devices.\"\"\"\n        # warning: will call train_dataloader()\n        if(self.steps_per_epoch==None):\n            dataset = obj.trainer._data_connector._train_dataloader_source.dataloader()\n            self.steps_per_epoch=len(dataset)//obj.trainer.accumulate_grad_batches\n        return self.steps_per_epoch\n\n    def get_progress(self,obj:pl.LightningModule,target_epoch:float,start_epoch=0.):\n        steps_per_epoch = self.num_training_steps(obj)\n        return max(min((obj.global_step-start_epoch*steps_per_epoch)/(target_epoch*steps_per_epoch-start_epoch*steps_per_epoch),1.0),0.0)\n    def collect_models(self,obj:nn.Module):\n        models = []\n        for i, module in obj._modules.items():\n            if (sum(p.numel() for p in module.parameters()) > 0):\n                models.append(i)\n        return models", "\n"]}
{"filename": "src/Net/__init__.py", "chunked_list": ["import os\nimport sys\nBASEPATH = os.path.dirname(__file__)\nsys.path.insert(0, BASEPATH)"]}
{"filename": "src/Net/StyleVAENet.py", "chunked_list": ["\nimport random\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\nfrom src.Module.MoEModule import MultipleExpertsLinear", "from src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\nfrom src.Module.MoEModule import MultipleExpertsLinear\nfrom src.Module.PhaseModule import PhaseOperator\nfrom src.Net.CommonOperation import CommonOperator\n\n_EPS32 = torch.finfo(torch.float32).eps\n_EPS32_2 = _EPS32*2\n\nfrom src.Module.VAEModule import VAE_Linear\n", "from src.Module.VAEModule import VAE_Linear\n\nfrom src.geometry.quaternions import quat_to_or6D,or6d_to_quat\nfrom src.Datasets.BatchProcessor import BatchProcessDatav2\n\n\nclass MoeGateDecoder(nn.Module):\n    def __init__(self,  style_dims, n_joints,n_pos_joints, condition_size, phase_dim, latent_size, num_experts):\n\n        super(MoeGateDecoder, self).__init__()\n        out_channels = 6 * n_joints + n_pos_joints*3# + phase_dim*4\n        gate_in = phase_dim*2 + latent_size#+ condition_size\n        self.gate = nn.Sequential(nn.Linear(gate_in, 128), nn.ELU(), nn.Linear(128, 128),nn.ELU(), nn.Linear(128, num_experts), nn.Softmax(-1))\n        self.linears = nn.ModuleList([MultipleExpertsLinear(condition_size+latent_size, 512, num_experts), MultipleExpertsLinear(512+latent_size, 512, num_experts)])\n        self.act = nn.ModuleList([nn.ELU(), nn.ELU(), nn.ELU()])\n        self.mlp = MultipleExpertsLinear(512, out_channels, num_experts)\n        self.phase_dims = phase_dim\n        self.out_channels = out_channels\n\n\n\n    def forward(self,  latent, condition, phase):\n        '''input: N,C->decoder'''\n        '''phase: N,C->gate network'''\n        '''x: pose+latent'''\n\n        x = condition\n        coefficients = self.gate(torch.cat((phase.flatten(-2,-1),latent),dim=-1))  # self.gate(torch.cat((x,hn),dim=-1))#self.gate(torch.cat((condition,hn,phase.flatten(-2,-1)),dim=-1))#self.gate(torch.cat((hn,condition),dim=-1))##self.gate(torch.cat((phase.flatten(-2,-1),contact),dim=-1))###\n        x = torch.cat((x,latent),dim=-1)\n        x = self.linears[0](x,coefficients)\n        x = self.act[0](x)\n        x = torch.cat((x, latent), dim=-1)\n        x = self.linears[1](x,coefficients)\n        x = self.act[1](x)\n        out = self.mlp(x,coefficients)\n        pred_pose = out\n        return pred_pose,coefficients", "from enum import Enum\nclass VAEMode(Enum):\n        MULTI = 1\n        SINGLE = 2\nclass MultiVAEOperator():\n    def __init__(self):\n        pass\nclass SingleVAEOperator():\n    def __init__(self):\n        pass\n    def shift_encode_two_frame(this,self,last_v,last_rots,nxt_v,nxt_rots,contact,last_phase,next_phase):\n        co_input = torch.cat((last_v, last_rots, nxt_v, nxt_rots,last_phase.flatten(-2,-1),next_phase.flatten(-2,-1)), dim=-1)\n        z,mu,log_var = self.embedding_encoder(co_input)\n        return z,mu,log_var", "\n\nclass IAN_FilmGenerator2(nn.Module):\n    def __init__(self,in_channel):\n        super(IAN_FilmGenerator2, self).__init__()\n        self.conv_pre = nn.Conv1d(in_channel,512,1)\n        self.conv_pre2 = nn.Conv1d(512,512,3)\n        self.conv0 = nn.Conv1d(512,512,3)\n        self.conv1 = nn.Conv1d(512,512,5)\n        self.act = nn.ReLU()\n    def forward(self,x):\n        # hope transformer can aggregate the sharp feature, different from the netural pose\n        # from motion\n        x = self.conv_pre(x)\n        x = self.act(x)\n        x = self.conv_pre2(x)\n        x = self.act(x)\n        x = self.conv0(x)\n        x = self.act(x)\n        x = self.conv1(x)\n        x = self.act(x)\n        y2 = x\n        y2 = y2.transpose(1,2)\n        return [y2]", "\nclass EmbeddingTwoFrameEncoder(nn.Module):\n    def __init__(self,num_joints,num_pos_joints,latent_dims,phase_dims):\n        super(EmbeddingTwoFrameEncoder, self).__init__()\n        in_channels = (6*num_joints+6*num_pos_joints)*2 #+latent_dims\n        self.linears = nn.ModuleList([nn.Linear(in_channels,512),nn.Linear(512,512),nn.Linear(512,512)])\n        self.act = nn.ModuleList([nn.ELU(),nn.ELU(),nn.ELU()])\n        self.mlp = VAE_Linear(512,latent_dims)\n    def forward(self, condition):\n        '''input: N,C->decoder'''\n        '''phase: N,C->gate network'''\n        '''x: pose+latent'''\n        x = condition\n        x = self.linears[0](x)\n        x = self.act[0](x)\n        x = self.linears[1](x)\n        x = self.act[1](x)\n        x = self.linears[2](x)\n        x = self.act[2](x)\n        latent,mu,log_var = self.mlp(x)\n        return latent,mu,log_var", "\nclass StyleVAENet(pl.LightningModule):\n\n    def __init__(self, skeleton, phase_dim:int=20,latent_size = 64,batch_size=64,mode=\"pretrain\",net_mode=VAEMode.SINGLE):\n        style_level_dim = [512,512]\n        self.rot_rep_idx = [1, 5, 9, 10, 11, 12, 13, 15, 19]\n        self.pos_rep_idx = [idx for idx in np.arange(0, skeleton.num_joints) if idx not in self.rot_rep_idx]\n        '''input channel: n_joints*dimensions'''\n        super(StyleVAENet, self).__init__()\n        self.lr = self.init_lr = 1e-3\n        self.automatic_optimization = True\n        self.dt = 1./30.\n        self.phase_op = PhaseOperator(self.dt)\n        self.style_level_dim = style_level_dim\n        self.save_hyperparameters(ignore=['mode','net_mode'])\n        self.skeleton = skeleton\n        self.mode = mode\n        self.net_mode = net_mode\n\n        self.vae_op = SingleVAEOperator() if net_mode==VAEMode.SINGLE else MultiVAEOperator()\n\n        self.batch_processor = BatchProcessDatav2()\n        self.embedding_encoder = EmbeddingTwoFrameEncoder(skeleton.num_joints, len(self.pos_rep_idx), latent_size,phase_dim)\n        self.decoder = MoeGateDecoder(style_level_dim, skeleton.num_joints,len(self.pos_rep_idx),  9+len(self.pos_rep_idx)*6+self.skeleton.num_joints*6, phase_dim, latent_size, num_experts=8)\n        self.l1Loss = nn.L1Loss()\n        self.mse_loss = nn.MSELoss()\n        self.common_operator = CommonOperator(batch_size=batch_size)\n        self.scheduled_prob = 1.0\n        self.latent_size = latent_size\n        self.style_level_dim = style_level_dim\n        self.sigma = 0.3\n        self.initialize = True\n\n    def transform_batch_to_VAE(self, batch):\n        local_pos, local_rot = batch['local_pos'], batch['local_rot']\n        edge_len = torch.norm(batch['offsets'][:, 1:], dim=-1, keepdim=True)\n        return local_pos, quat_to_or6D(local_rot), edge_len, (batch['phase'])\n    def kl_loss(self, mu, log_var):\n        return -0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp()) / np.prod(mu.shape)\n    def shift_running(self, local_pos, local_rots, phases, As, Ss, contacts, style_code):\n        '''pose: N,T,J,9'''\n        '''hip: N,T,3'''\n        '''phases: N,T,M'''\n        '''style_code: [N,C,T,J| N,C,T,J]'''\n        N, T, n_joints, C = local_pos.shape\n        output_pos = torch.empty(size=(N, T - 2, n_joints, 3), device=local_pos.device)\n        output_rot = torch.empty(size=(N, T - 2, n_joints, 6), device=local_rots.device)\n        output_mu = torch.empty(size=(N, T - 2, self.latent_size), device=local_rots.device)\n        local_pos = local_pos[:, :, self.pos_rep_idx]\n\n        # last_S = Ss[:,0]\n        output_phase = torch.empty(size=(N, T - 2, phases.shape[-2], 2), device=phases.device)\n        output_A = torch.empty(size=(N, T - 2, phases.shape[-2], 1), device=phases.device)\n        output_F = torch.empty(size=(N, T - 2, phases.shape[-2], 1), device=phases.device)\n        output_sphase = torch.empty(size=(N, T - 2, phases.shape[-2], 2), device=phases.device)\n        last_g_pos, last_g_rot = local_pos[:, 1], local_rots[:, 1]\n        last_g_v = local_pos[:, 1] - local_pos[:, 0]\n        last_phase = phases[:, 1]\n\n        kl_loss = 0.\n        step = 0.\n        for t in range(1, T - 1):  # slice+1: we discard the first frame\n            last_rel_pos = (last_g_pos - last_g_pos[:, 0:1]).flatten(-2,-1)\n            last_l_v = last_g_v.flatten(-2, -1)\n            last_l_rot = last_g_rot.flatten(-2,-1)\n            next_rel_pos = (local_pos[:, t + 1] - local_pos[:, t+1, 0:1]).flatten(-2,-1)\n            next_l_v = (local_pos[:, t + 1] - last_g_pos).flatten(-2, -1)\n            next_l_rot = local_rots[:, t + 1].flatten(-2, -1)\n            hip_l_v = next_l_v[..., 0:3]\n            hip_l_r = next_l_rot[..., 0:6].clone()\n            condition_no_style = torch.cat((last_rel_pos, last_l_v, hip_l_v, last_l_rot, hip_l_r), dim=-1)\n            embedding_input = torch.cat( (last_rel_pos, next_rel_pos, last_l_v, next_l_v, last_l_rot, next_l_rot), dim=-1)\n            latent, mu, log_var = self.embedding_encoder( embedding_input)\n            output_mu[:, t - 1] = latent\n            kl_loss = kl_loss + self.kl_loss(mu, log_var)\n            step += 1\n            pred_pose_, coefficients = self.decoder(latent, condition_no_style, phases[:,t+1])\n            pred_l_v, pred_l_rot_v = pred_pose_[..., :len(self.pos_rep_idx) * 3].view(-1, len(self.pos_rep_idx),3), pred_pose_[..., len(self.pos_rep_idx) * 3:].view( -1, self.skeleton.num_joints, 6)\n            last_l_rot = last_l_rot.view(-1, self.skeleton.num_joints, 6)\n            pred_g_v = pred_l_v\n            pred_g_v[:, 0] = local_pos[:, t + 1, 0] - last_g_pos[:, 0]\n            pred_pos = pred_g_v + last_g_pos\n            pred_rot = pred_l_rot_v + last_l_rot\n            pred_rot[:, 0:1] = local_rots[:, t + 1, 0:1]\n\n            output_pos[:, t - 1, self.pos_rep_idx] = pred_pos\n            output_rot[:, t - 1] = pred_rot\n            last_g_pos, last_g_rot, last_g_v = pred_pos, pred_rot, pred_g_v\n        if (step > 0):\n            kl_loss = kl_loss / step\n        return output_pos, output_rot, kl_loss, [output_phase, output_A, output_F, output_sphase]\n\n    def regu_pose(self, pos, edge_len, rot):\n        import src.geometry.inverse_kinematics as ik\n        from src.geometry.quaternions import normalized_or6d\n        # In our setting, the spine idx can not be affected by this function because the bone length is constant\n        pos = ik.scale_pos_to_bonelen(pos, edge_len, self.skeleton._level_joints, self.skeleton._level_joints_parents)\n        rot = normalized_or6d(rot)\n        return pos, rot\n    def get_gt_contact(self,gt_v):\n        eps = 0.5\n        eps_bound = 1.\n        def contact_smooth(x, idx, eps, bound):\n            # x must be in range of [eps,bound]\n            x = (x - eps) / (bound - eps)\n            x2 = x * x\n            x3 = x2 * x\n            contact = 2 * (x3) - 3 * (x2) + 1\n            return contact * idx\n        key_joint = [3, 4, 7, 8]\n        gt_fv = gt_v[:, :, key_joint]\n        gt_fv = torch.sum(gt_fv.pow(2), dim=-1).sqrt()\n        # first we need gt_contact\n        idx = (gt_fv < eps)  # * (pred_fv>gt_fv) # get penality idx\n        idx_smooth = (gt_fv >= eps) * (gt_fv < eps_bound)\n        gt_contact = contact_smooth(gt_fv, idx_smooth, eps, eps_bound) + torch.ones_like(gt_fv) * idx\n        return gt_contact\n\n    def contact_foot_loss(self,contact,pred_v):\n        key_joints = [3, 4, 7, 8]\n        pred_fv = pred_v[:, :, key_joints]\n        pred_fv = (torch.sum(pred_fv.pow(2), dim=-1) + 1e-6)  # .sqrt()\n        contact_idx = torch.where(contact < 0.01, 0, contact)\n        # torch.where(contact>=0.01,-contact,0) # only those who is 100% fixed, we don't apply position constrain\n        contact_num = torch.count_nonzero(contact_idx)\n        pred_fv = pred_fv * contact_idx\n        contact_loss = pred_fv.sum() / max(contact_num, 1)\n        return contact_loss\n    def rot_to_pos(self,rot,offsets,hip):\n        if (rot.shape[-1] == 6):\n            rot = or6d_to_quat(rot)\n        rot_pos = self.skeleton.global_rot_to_global_pos(rot, offsets, hip)\n        return rot_pos\n    def phase_loss(self,gt_phase,gt_A,gt_F,phase,A,F,sphase):\n        amp_scale = 1.\n        loss = {\"phase\": self.mse_loss(gt_phase*amp_scale,phase*amp_scale),\"A\":self.mse_loss(gt_A*amp_scale,A*amp_scale),\"F\":self.mse_loss(gt_F,F),\"slerp_phase\":self.mse_loss(gt_phase*amp_scale,sphase*amp_scale)}\n        return loss\n    def shared_forward_single(self,batch,base_epoch = 30,edge_mean =21.):\n        N = batch['local_pos'].shape[0] // 2\n        local_pos, local_rots, edge_len, phases = self.transform_batch_to_VAE(batch)\n        A = batch['A']\n        S = batch['S']\n\n        src_code = None\n        self.length = 25\n        start_idx = np.random.randint(0, 60-self.length-1)\n        end_idx = start_idx + self.length\n        local_pos = local_pos[:, start_idx:end_idx]\n        local_rots = local_rots[:, start_idx:end_idx]\n        phases = phases[:, start_idx:end_idx]\n        A = A[:, start_idx:end_idx]\n        S = S[:, start_idx:end_idx]\n\n        F = S[:,1:]-S[:,:-1]\n        F = self.phase_op.remove_F_discontiny(F)\n        F = F / self.phase_op.dt\n\n        src_pos = local_pos[:N]\n        src_rots = local_rots[:N]\n        src_edge_len = edge_len[:N]\n        src_phases = phases[:N]\n        src_A = A[:N]\n        src_F = F[:N]\n\n        ########################################################################################\n        pred_pos, pred_rot,kl, pred_phase = self.shift_running(src_pos, src_rots,src_phases, src_A, src_F,None,style_code=src_code)\n        rot_pos = self.rot_to_pos(pred_rot,batch['offsets'][:N],pred_pos[:,:,0:1])\n        pred_pos[:,:,self.rot_rep_idx] = rot_pos[:, :, self.rot_rep_idx]\n        if (self.stage != \"training\" or random.random() < 0.1):\n            pred_pos, pred_rot = self.regu_pose(pred_pos, src_edge_len, pred_rot)\n        # in loss function, we consider the end effector's position more\n        gt_contact = self.get_gt_contact(src_pos[:,3:,:] - src_pos[:,2:-1,:]).detach()\n        contact_loss = self.contact_foot_loss(gt_contact,pred_pos[:,1:]-pred_pos[:,:-1])\n        pos_loss = self.mse_loss(src_pos[:,2:2+self.length,:]/edge_mean,pred_pos[:,:,:]/edge_mean)\n        rot_loss = self.mse_loss(src_rots[:,2:2+self.length],pred_rot)\n\n        vae_loss = {\"pos\":pos_loss,\"rot\":rot_loss,\"kl\":kl,\"ct\":contact_loss}\n\n        epoch = self.common_operator.get_progress(self,1,0)\n        if(epoch>=1):\n            vae_loss['loss'] = vae_loss['pos'] + vae_loss['rot'] + kl*0.001 + vae_loss[\"ct\"]*0.1# + (vae_loss[\"phase\"] + vae_loss['A'] + vae_loss['F']+ vae_loss['slerp_phase']) * 0.5\n        else:\n            vae_loss['loss'] = vae_loss['pos'] + vae_loss['rot'] + kl * 0.001\n\n        return vae_loss\n\n\n\n    def validation_step(self, batch, batch_idx):\n        self.scheduled_prob = 1.\n        self.scheduled_phase = 1.\n        self.length = 30\n        self.stage = \"validation\"\n        loss = self.shared_forward_single(batch)\n        self.common_operator.log_dict(self, loss, \"val_\")\n        return loss['loss']\n\n    def test_step(self, batch, batch_idx):\n        self.stage = \"test\"\n        loss = self.shared_forward_single(batch)\n        self.common_operator.log_dict(self, loss, \"test_\")\n        return loss\n    def update_lr(self):\n        base_epoch = 20\n        if (self.mode == 'fine_tune'):\n            self.scheduled_prob = 1.\n            # first 20 epoch ,we increase lr to self.lr\n            if (self.current_epoch < base_epoch):\n                progress = self.common_operator.get_progress(self, base_epoch, 0)\n                lr = self.lr * progress\n            else:\n                progress = self.common_operator.get_progress(self, 400, base_epoch)\n                lr = (1 - progress)*self.lr+progress*1e-5\n            opt = self.optimizers()\n            self.common_operator.set_lr(lr, opt)\n            self.log(\"lr\", lr, logger=True)\n        else:\n            lr = self.lr\n            #first 40 epoch, we use the original lr\n            #then we decay the lr to zero until 200 epoch\n            if(self.mode=='second'):\n                base_epoch = 0\n            progress = self.common_operator.get_progress(self, 300, base_epoch)\n            decay = (1 - progress)*self.lr+progress*1e-5\n            lr = decay\n            opt = self.optimizers()\n            self.common_operator.set_lr(lr, opt)\n            self.log(\"lr\", lr, logger=True)\n    def training_step(self, batch, batch_idx):\n\n        self.stage = \"training\"\n        self.scheduled_prob = self.common_operator.get_progress(self,target_epoch=50,start_epoch=0)\n\n        if(self.mode!=\"pretrain\"):\n            self.scheduled_prob = 1.\n\n        self.update_lr()\n        loss = self.shared_forward_single(batch)\n\n        self.common_operator.log_dict(self, loss, \"train_\")\n        self.log(\"prob\",self.scheduled_prob,logger=True)\n        self.log('prob_ph',self.scheduled_phase,logger=True)\n        return loss['loss']\n\n    def configure_optimizers(self):\n\n        models = self.common_operator.collect_models(self)\n        trained_model = []\n\n        def weight_decay(model_name, lr, weight_decay):\n            trained_model.append(model_name)\n            model = getattr(self, model_name)\n            return self.common_operator.add_weight_decay(model, lr, weight_decay)\n        lr = self.lr\n\n        params = weight_decay(\"decoder\", lr, 0) + weight_decay(\"embedding_encoder\",lr,0)\n        optimizer = torch.optim.Adam(params, lr=self.lr, betas=(0.5, 0.9),amsgrad=True)\n\n        non_train_model = [i for i in models if i not in trained_model]\n        if (len(non_train_model) > 0):\n            print(\"warning:there are some models not trained:{}\".format(non_train_model))\n            input(\"Please confirm that by Enter\")\n        if(self.mode=='fine_tune'):\n            return [optimizer]\n\n        else:\n            return [optimizer]", "\nfrom src.utils.BVH_mod import find_secondary_axis\nclass Application(nn.Module):\n    def __init__(self,net:StyleVAENet,data_module:StyleVAE_DataModule):\n        super(Application, self).__init__()\n        self.Net = net\n        self.data_module = data_module\n        self.src_batch = None\n        self.target_batch = None\n        self.skeleton = data_module.skeleton\n\n    def transform_batch(self, motion, label):\n        batch = [[], label]\n        batch[0] = [[motion[0], motion[1], motion[2]]]\n        for i, value in enumerate(batch[0][0]):\n            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n        for key in motion[3].keys():\n            motion[3][key] = torch.from_numpy(motion[3][key]).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n        batch[0][0].append(motion[3])\n        batch = self.data_module.transfer_mannual(batch, 0, use_phase=True,use_sty=False)\n        return batch\n\n    def transform_anim(self, anim, label):\n        batch = [[], label]\n        batch[0] = [[anim.quats, anim.offsets, anim.hip_pos]]\n        for i, value in enumerate(batch[0][0]):\n            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n        batch = self.data_module.transfer_mannual(batch, 0, use_phase=False,use_sty=False)\n        return batch\n\n    def setSource(self, anim):\n        if (type(anim) == tuple):\n            self.src_batch = self.transform_batch(anim, 0)\n        else:\n            self.src_batch = self.transform_anim(anim, 0)\n        self.offsets = self.src_batch['offsets']\n        self.tangent = find_secondary_axis(self.offsets)\n    def _get_transform_ori_motion(self, batch):\n        global_pos = batch['local_pos']\n        global_rot = batch['local_rot']\n\n        # global_pos,global_rot = self.skeleton.forward_kinematics(local_rot,self.offsets,global_pos[:,:,0:1,:])\n        global_rot = self.skeleton.inverse_pos_to_rot(global_rot, global_pos, self.offsets, self.tangent)\n        # lp, lq = self.skeleton.inverse_kinematics((global_rot), (global_pos))\n        # or\n        lp, lq = self.skeleton.inverse_kinematics(global_rot, global_pos)\n        return lp[0].cpu().numpy(), lq[0].cpu().numpy()\n\n    def get_source(self):\n        batch = {'local_pos':self.src_batch['local_pos'][:,16:46],\"local_rot\":self.src_batch['local_rot'][:,16:46]}\n        return self._get_transform_ori_motion(batch)\n\n    def draw_foot_vel(self, pos, pred_pos):\n        import matplotlib.pyplot as plt\n        def draw(pos,ax,key_joints):\n            foot_vel = ((pos[:, 1:, key_joints] - pos[:, :-1, key_joints]) ** 2).sum(dim=-1).sqrt()\n            ax.plot(foot_vel[0, :].cpu())\n        fig,ax = plt.subplots(2,2,figsize=(2*1.5,2*1.))\n        joints = [17,18,21,22]\n        ax[0,0].set_xlabel(\"3\")\n        draw(pos,ax[0,0],joints[0])\n        draw(pred_pos,ax[0,0],joints[0])\n\n        ax[0, 1].set_xlabel(\"4\")\n        draw(pos, ax[0, 1], joints[1])\n        draw(pred_pos, ax[0, 1], joints[1])\n\n        ax[1, 0].set_xlabel(\"7\")\n        draw(pos, ax[1, 0], joints[2])\n        draw(pred_pos, ax[1, 0], joints[2])\n\n        ax[1, 1].set_xlabel(\"8\")\n        draw(pos, ax[1, 1], joints[3])\n        draw(pred_pos, ax[1, 1], joints[3])\n\n        plt.show()\n\n    def forward(self,seed,encoding=True):\n        import matplotlib.pyplot as plt\n        from torch._lowrank import pca_lowrank\n        self.Net.eval()\n        with torch.no_grad():\n            loc_pos, loc_rot, edge_len, phases= self.Net.transform_batch_to_VAE(self.src_batch)\n            A = self.src_batch['A']\n            S = self.src_batch['S']\n            F = S[:,1:]-S[:,:-1]\n            F = self.Net.phase_op.remove_F_discontiny(F)\n            F = F / self.Net.phase_op.dt\n            torch.random.manual_seed(seed)\n            pred_pos, pred_rot,kl,pred_phase = self.Net.shift_running(loc_pos, loc_rot, phases,A,F, None,\n                                                                                        None)\n            def draw_projection(pred_mu):\n                U, S, V = pca_lowrank(pred_mu)\n                proj = torch.matmul(pred_mu, V)\n                proj = proj[:, :2]\n                c = np.arange(0, proj.shape[0], step=1.)\n                proj = proj.cpu()\n                plt.scatter(proj[:, 0], proj[:, 1], c=c)\n                plt.show()\n\n            draw_projection(phases[0].squeeze(0).flatten(-2,-1))\n            loss = {}\n            rot_pos = self.Net.rot_to_pos(pred_rot, self.src_batch['offsets'], pred_pos[:, :, 0:1])\n            pred_pos[:, :, self.Net.rot_rep_idx] = rot_pos[:, :, self.Net.rot_rep_idx]\n            # output_pos,output_rot = pred_pos,pred_rot\n            output_pos, output_rot = self.Net.regu_pose(pred_pos, edge_len, pred_rot)\n\n            loss['pos']=self.Net.mse_loss(output_pos[:]/21,loc_pos[:,2:]/21)\n            loss['rot']=self.Net.mse_loss(output_rot[:,],loc_rot[:,2:])\n            print(loss)\n            output_pos = torch.cat((loc_pos[:, :2, ...], output_pos), dim=1)\n            output_rot = torch.cat((loc_rot[:, :2, ...], output_rot), dim=1)\n            output_rot = or6d_to_quat(output_rot)\n\n            batch = {}\n\n            batch['local_rot'] = output_rot#or6d_to_quat(output_rot)\n            batch['local_pos'] = output_pos\n            self.draw_foot_vel(loc_pos[:,2:], output_pos)\n\n            return self._get_transform_ori_motion(batch)", "\n\n\n\n\n\n\n"]}
{"filename": "src/Net/TransitionNet.py", "chunked_list": ["import math\nimport random\n\nimport torch\nfrom torch import nn\n\n\nclass PosEncoding(nn.Module):\n    def positional_encoding(self,tta, basis=10000., dimensions=256):\n        z = torch.zeros(dimensions, device='cuda')  # .type(torch.float16)\n        indices = torch.arange(0, dimensions, 2, device='cuda').type(torch.float32)\n        z[indices.long()] = torch.sin(tta / torch.pow(basis, indices / dimensions))\n        z[indices.long() + 1] = torch.cos(tta / torch.pow(basis, indices / dimensions))\n        return z\n    def __init__(self,max_tta,dims):\n        super(PosEncoding, self).__init__()\n        ttaEncoding = torch.empty(max_tta, dims)\n        for tta in range(max_tta):\n            ttaEncoding[tta] = self.positional_encoding(tta,basis=10000.,dimensions=dims)\n        self.register_buffer(\"ttaEncoding\", ttaEncoding)\n    def forward(self,id):\n        return self.ttaEncoding[id]", "def add_pos_info(latent,embedings,tta,t_max):\n    t = min(tta, t_max)\n    pos = embedings(t)\n    #pos = torch.cat((pos,pos,pos),dim=-1)\n    return latent+pos\n\ndef multi_concat(context_state,context_offset,context_target,embeddings,embeddings512,noise_per_sequence,tta,t_max):\n    t = torch.where(tta < t_max, tta, t_max)\n    embeddings_vector = []\n    embeddings512_vector = []\n    for i in range(t.shape[0]):\n        embeddings_vector.append(embeddings(t[i]).view(1,1,-1))\n        embeddings512_vector.append(embeddings512(t[i]).view(1,1,-1))\n    embeddings_vector = torch.cat(embeddings_vector,dim=1) # 1, 8 ,256\n    embeddings512_vector = torch.cat(embeddings512_vector,dim=1)\n    h_target = context_target + embeddings_vector\n    h_state = context_state + embeddings512_vector\n    h_offset = context_offset + embeddings_vector\n\n    lambda_tar = torch.where(t>30,1.,torch.where(t<5.,0,(t-5.)/25.))\n    # if tta >= 30:\n    #     lambda_tar = 1.\n    # elif tta < 5:\n    #     lambda_tar = 0.\n    # else:\n    #     lambda_tar = (tta - 5.) / 25.\n    h_target = torch.cat((h_offset, h_target), dim=-1)\n    h_target = h_target + lambda_tar.view(1,-1,1) * noise_per_sequence.unsqueeze(1)\n\n    return torch.cat((h_state, h_target), dim=-1), h_target", "def concat(context_state,context_offset,context_target,embeddings,embeddings512,noise_per_sequence,tta,t_max):\n#    t = torch.min(tta,t_max) # MAXFRAME+10-5\n    t = min(tta,t_max)\n    h_target = context_target + embeddings(t)\n    h_state = context_state + embeddings512(t)\n    h_offset = context_offset + embeddings(t)\n    if tta >= 30:\n        lambda_tar = 1.\n    elif tta < 5:\n        lambda_tar = 0.\n    else:\n        lambda_tar = (tta - 5.) / 25.\n    h_target = torch.cat((h_offset, h_target), dim=-1)\n    h_target = h_target + lambda_tar * noise_per_sequence\n\n    return torch.cat((h_state,h_target),dim=-1),h_target", "\nclass SeqScheduler():\n    def __init__(self,initial_seq,max_seq):\n        self.initial_seq = initial_seq\n        self.max_seq = max_seq\n       # self.epoch = epoch\n    def progress(self,t:float):\n        t = min(max(t,0.),1.)\n        out = (self.max_seq-self.initial_seq)*t+self.initial_seq\n        return int(out)\n    def range(self,t:float):\n        upper = self.progress(t)\n        return random.randint(self.initial_seq,upper)", "\nclass ATNBlock(nn.Module):\n    def __init__(self,content_dims,style_dims):\n        super(ATNBlock, self).__init__()\n        in_ch = content_dims\n        self.in_ch = content_dims\n        self.sty_ch = style_dims\n        self.f = nn.Linear(content_dims, style_dims)\n        self.g = nn.Linear(style_dims, style_dims)\n        self.h = nn.Linear(style_dims, style_dims)\n        self.sm = nn.Softmax(dim=-2)\n        self.k = nn.Linear(style_dims, in_ch)\n        self.norm = nn.InstanceNorm2d(style_dims,affine=False)\n        self.norm_content = nn.InstanceNorm1d(content_dims,affine=False) #LayerNorm(keep the same as AdaIn)\n       # self.s = []\n    def forward(self, fs, fd,pos, first):\n        #return fd\n        N,T,C = fs.shape\n        N,C = fd.shape\n        x = fd\n        s_sty = fs\n        # N,C : fd.shape\n        # N,C,T : fs.shape\n        b = s_sty.shape[0]\n\n        F = self.f(self.norm_content(x)).unsqueeze(-1) # N,C,1\n        if(first):\n            G = self.g(self.norm(s_sty)) #N,T,C\n            self.G = G.view(b, -1, self.sty_ch)  # N,T,C\n            #s_sty_pos = s_sty+pos_embedding.view(1,C,1)\n            self.H = self.h(s_sty).transpose(1,2) #N,C,T\n        #H = H.view(b, self.sty_ch, -1)  # N,C,T\n\n\n        F = F.view(b, self.sty_ch, -1) #N,C,1\n\n        S = torch.bmm(self.G,F) #N,T,1\n\n        S = self.sm(S/math.sqrt(self.G.shape[-1]))\n        # self.s.append(S)\n        O = torch.bmm(self.H, S) # N,C,1\n\n        O = O.view(x.shape[:-1]+(self.sty_ch,))\n\n        O = self.k(O)\n        O += x\n        return O", "\nclass AdaInNorm2D(nn.Module):\n    r\"\"\"MLP(fs.mean(),fs.std()) -> instanceNorm(fd)\"\"\"\n    def __init__(self,style_dims,content_dim,n_joints=0):\n\n        super(AdaInNorm2D, self).__init__()\n\n        self.affine2 = nn.Linear(style_dims, style_dims)\n        self.act = nn.ELU()#nn.LeakyReLU(0.2)\n        self.affine3 = nn.Linear(style_dims, style_dims)\n        self.affine4 = nn.Linear(style_dims, content_dim * 2)\n        self.norm = nn.InstanceNorm1d(512)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, s, d ,pos_emedding,first):\n        if(first):\n            N,T,C = s.shape\n            s = torch.mean(s,dim=1) #N,C\n            s = self.affine2(s)\n            s = self.act(s)\n            s = self.dropout(s)\n            s = self.affine4(s)\n            self.gamma, self.beta = torch.chunk(s, chunks=2, dim=1)\n\n        d = self.norm(d)\n        return (1 + self.gamma) * d + self.beta", ""]}
{"filename": "src/Net/TransitionPhaseNet.py", "chunked_list": ["import random\n\nimport numpy as np\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom src.Datasets.BatchProcessor import BatchProcessDatav2\nfrom src.Module.Utilities import PLU\nfrom src.Net.CommonOperation import CommonOperator", "from src.Module.Utilities import PLU\nfrom src.Net.CommonOperation import CommonOperator\n# from src.Datasets.TransitionDataModule import Transition_DataModule, BatchRotateYCenterXZ\nfrom src.geometry.quaternions import normalized_or6d\nfrom src.geometry.quaternions import quat_to_or6D, or6d_to_quat\nfrom src.utils.BVH_mod import Skeleton, find_secondary_axis\n\n\ndef eval_sample(model, X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, pos_offset, skeleton: Skeleton, length, param):\n    # FOR EXAMPLE\n    model = model.eval()\n    model = model.cuda()\n    quats = Q\n    offsets = pos_offset\n    hip_pos = X\n    dict = {\"hip_pos\": X, 'offsets': Q, 'quats': Q}\n    gp, gq = skeleton.forward_kinematics(quats, offsets, hip_pos)\n    loc_rot = quat_to_or6D(gq)\n    if \"target_id\" in param:\n        target_id = param[\"target_id\"]\n    else:\n        target_id = length + 10\n    noise = torch.zeros(size=(gp.shape[0], 512), dtype=gp.dtype, device=gp.device)\n    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n    tar_quat = quat_to_or6D(tar_quat)\n    target_style = model.get_film_code(tar_pos.cuda(),tar_quat.cuda())\n    F = S[:, 1:] - S[:, :-1]\n    F = model.phase_op.remove_F_discontiny(F)\n    F = F / model.phase_op.dt\n    phases = model.phase_op.phaseManifold(A, S)\n    if(model.predict_phase==True):\n     pred_pos, pred_rot, pred_phase, _,_ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(), F.cuda(), target_style, None,\n                                                               start_id=10,\n                                                               target_id=target_id, length=length,\n                                                               phase_schedule=1.)\n    else:\n        pred_pos, pred_rot, pred_phase, _ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(),\n                                                                   F.cuda(), target_style, None,\n                                                                   start_id=10,\n                                                                   target_id=target_id, length=length,\n                                                                   phase_schedule=1.)\n    pred_pos,pred_rot = pred_pos.cpu(),pred_rot.cpu()\n    rot_pos = model.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n    pred_pos[:, :, model.rot_rep_idx] = rot_pos[:, :, model.rot_rep_idx]\n    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n    pred_pos, pred_rot = model.regu_pose(pred_pos, edge_len, pred_rot)\n\n    GQ = skeleton.inverse_pos_to_rot(or6d_to_quat(pred_rot), pred_pos, offsets, find_secondary_axis(offsets))\n    GX = skeleton.global_rot_to_global_pos(GQ, offsets, pred_pos[:, :, 0:1, :]).flatten(-2, -1)\n    x_mean = x_mean.view(skeleton.num_joints, 3)\n    x_std = x_std.view(skeleton.num_joints, 3)\n    GX = (GX - x_mean.flatten(-2, -1)) / x_std.flatten(-2, -1)\n    GX = GX.transpose(1, 2)\n    return GQ, GX", "def eval_sample(model, X, Q,A,S,tar_pos,tar_quat, x_mean, x_std, pos_offset, skeleton: Skeleton, length, param):\n    # FOR EXAMPLE\n    model = model.eval()\n    model = model.cuda()\n    quats = Q\n    offsets = pos_offset\n    hip_pos = X\n    dict = {\"hip_pos\": X, 'offsets': Q, 'quats': Q}\n    gp, gq = skeleton.forward_kinematics(quats, offsets, hip_pos)\n    loc_rot = quat_to_or6D(gq)\n    if \"target_id\" in param:\n        target_id = param[\"target_id\"]\n    else:\n        target_id = length + 10\n    noise = torch.zeros(size=(gp.shape[0], 512), dtype=gp.dtype, device=gp.device)\n    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n    tar_quat = quat_to_or6D(tar_quat)\n    target_style = model.get_film_code(tar_pos.cuda(),tar_quat.cuda())\n    F = S[:, 1:] - S[:, :-1]\n    F = model.phase_op.remove_F_discontiny(F)\n    F = F / model.phase_op.dt\n    phases = model.phase_op.phaseManifold(A, S)\n    if(model.predict_phase==True):\n     pred_pos, pred_rot, pred_phase, _,_ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(), F.cuda(), target_style, None,\n                                                               start_id=10,\n                                                               target_id=target_id, length=length,\n                                                               phase_schedule=1.)\n    else:\n        pred_pos, pred_rot, pred_phase, _ = model.shift_running(gp.cuda(), loc_rot.cuda(), phases.cuda(), A.cuda(),\n                                                                   F.cuda(), target_style, None,\n                                                                   start_id=10,\n                                                                   target_id=target_id, length=length,\n                                                                   phase_schedule=1.)\n    pred_pos,pred_rot = pred_pos.cpu(),pred_rot.cpu()\n    rot_pos = model.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n    pred_pos[:, :, model.rot_rep_idx] = rot_pos[:, :, model.rot_rep_idx]\n    edge_len = torch.norm(offsets[:, 1:], dim=-1, keepdim=True)\n    pred_pos, pred_rot = model.regu_pose(pred_pos, edge_len, pred_rot)\n\n    GQ = skeleton.inverse_pos_to_rot(or6d_to_quat(pred_rot), pred_pos, offsets, find_secondary_axis(offsets))\n    GX = skeleton.global_rot_to_global_pos(GQ, offsets, pred_pos[:, :, 0:1, :]).flatten(-2, -1)\n    x_mean = x_mean.view(skeleton.num_joints, 3)\n    x_std = x_std.view(skeleton.num_joints, 3)\n    GX = (GX - x_mean.flatten(-2, -1)) / x_std.flatten(-2, -1)\n    GX = GX.transpose(1, 2)\n    return GQ, GX", "\nfrom src.Module.PhaseModule import PhaseOperator\nclass StateEncoder(nn.Module):\n    def __init__(self,inchannels):\n        super(StateEncoder, self).__init__()\n        self.layers = torch.nn.ModuleList([torch.nn.Linear(inchannels, 512),torch.nn.Linear(512,256)])\n        self.acts = torch.nn.ModuleList([PLU(),PLU()])\n\n    def forward(self,x):\n        for i in range(len(self.layers)):\n            x = self.layers[i](x)\n            x = self.acts[i](x)\n        return x", "class StyleEmbedding(torch.nn.Module):\n    def __init__(self,in_channels):\n        super(StyleEmbedding, self).__init__()\n        from src.Net.TransitionNet import AdaInNorm2D,ATNBlock\n        self.adains = nn.ModuleList([AdaInNorm2D(512, 512, 0)])\n        self.atns = nn.ModuleList([ATNBlock(512, 512)])\n        self.linears = nn.ModuleList([nn.Linear(in_channels, 512)])\n        self.act = nn.ELU()\n\n    def FILM(self, idx, s, x,pos_encoding, first):\n        x = self.adains[idx](s, x,pos_encoding, first)\n        x = self.act(x)\n        x = self.atns[idx](s, x,pos_encoding, first)\n        return x\n\n\n    def forward(self, fs, condition,pos_encoding, first):\n        '''input: N,C->decoder'''\n        '''phase: N,C->gate network'''\n        '''x: pose+latent'''\n        # just for sure it has style\n        x = condition\n        x = self.linears[0](x)\n        x = self.FILM(0,fs[0],x,None,first)\n        x = self.act(x)\n\n\n        return x", "class MoeStylePhasePredictor(nn.Module):\n    def __init__(self, rnn_size, phase_dim, num_experts):\n        from src.Net.TransitionNet import AdaInNorm2D,ATNBlock\n        from src.Module.PhaseModule import PhaseSegement\n        super(MoeStylePhasePredictor, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(rnn_size+phase_dim*2+512,rnn_size),nn.Linear(rnn_size,512),nn.Linear(512,512)])\n        self.adains = nn.ModuleList([AdaInNorm2D(512,512,0)])\n        self.atns = nn.ModuleList([ATNBlock(512, 512)])\n        self.act = nn.ModuleList([nn.ELU(), nn.ELU(), nn.ELU()])\n        self.mlp = nn.Linear(512,phase_dim*4+9+32)\n        self.phase_predcitor = PhaseSegement(phase_dim)\n        self.phase_dims = phase_dim\n\n\n    def FILM(self, idx, s, x, pos_encoding, first):\n        x = self.adains[idx](s, x, pos_encoding, first)\n        x = self.act[idx](x)\n        x = self.atns[idx](s, x, pos_encoding, first)\n        return x\n\n    def forward(self, fs, condition, phase,offset_latent,first):\n        '''input: N,C->decoder'''\n        '''phase: N,C->gate network'''\n        '''x: pose+latent'''\n\n        x = condition\n        x = torch.cat((x,phase.flatten(-2,-1),offset_latent),dim=-1)\n        x = self.linears[0](x)\n        x = self.act[0](x)\n        x = self.linears[1](x)\n        x = self.FILM(0,fs[0],x,None,first)\n        x = self.act[1](x)\n        x = self.linears[2](x)\n        x = self.act[2](x)\n        out = self.mlp(x)\n        phase,hip,latent = out[:,:self.phase_dims*4],out[:,self.phase_dims*4:self.phase_dims*4+9],out[:,self.phase_dims*4+9:]\n        phase, A, F = self.phase_predcitor(phase)\n        hip_v,hip_rv = hip[:,:3],hip[:,3:]\n        return phase, A, F, hip_v,hip_rv,latent", "class PredictInitialState(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv_layer = nn.Sequential(\n            nn.Conv1d(1024,1024,5),\n           # nn.AvgPool1d(2),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Conv1d(1024, 1024, 3),\n          #  nn.AvgPool1d(2),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Conv1d(1024, 1024, 2),\n            nn.ReLU()\n        )\n\n        #self.transform = nn.Transformer(1024)\n       # self.bn = lambda v:(v[:,0]**2+v[:,1]**2)\n        self.mlp = nn.Linear(1024,10*3)\n        self.phase_op = PhaseOperator(1 / 30.)\n    def forward(self, x):\n        def bn(v):\n            len = torch.sqrt(v[...,0]**2+v[...,1]**2+1e-6)\n            return v/len.unsqueeze(-1)\n        x = x.transpose(1,2)\n        x = self.conv_layer(x)\n        x = x.squeeze(-1)\n\n        # x = self.transform(x,x)\n        # x = x[:,-1]\n        out = self.mlp(x)\n        phase = out#.view(out.shape[0],10,3)\n        A,S = phase[:,:10].unsqueeze(-1),phase[:,10:30].unsqueeze(-1)\n        S = S.view(S.shape[0],10,2)\n        S = bn(S)\n        S = torch.atan2(S[...,1],S[...,0]).unsqueeze(-1)/self.phase_op.tpi\n\n\n       #  S = torch.atan2(S[:,:10],S[:,10:])/3.14159\n        phase = self.phase_op.phaseManifold(A,S)\n        return  phase,A,S", "\nclass TransitionNet_phase(pl.LightningModule):\n    \"\"\"Sequence-to-sequence model for human motion prediction\"\"\"\n    def __init__(self, moe_decoder: nn.Module,skeleton, pose_channels,stat,\n                  dt,  phase_dim=20, rnn_size=1024, dropout=0.3, past_seq=10,mode='pretrain',predict_phase=False,pretrained_model=None):\n        from src.Net.StyleVAENet import IAN_FilmGenerator2\n        from src.Net.TransitionNet import SeqScheduler,PosEncoding\n        super(TransitionNet_phase, self).__init__()\n        self.rot_rep_idx = [1, 5, 9, 10, 11, 12, 13, 15, 19]\n        self.pos_rep_idx = [idx for idx in np.arange(0, skeleton.num_joints) if idx not in self.rot_rep_idx]\n        self.mode = mode\n        self.test = True\n        self.predict_phase = predict_phase\n        pos_mean, pos_std = stat['pos_stat']\n        vel_mean, vel_std = stat['vel_stat']\n        rot_mean, rot_std = stat[\"rot_stat\"]\n        register_numpy = lambda x, s: self.register_buffer(s, torch.from_numpy(x).float())\n        register_scalar = lambda x, s: self.register_buffer(s, torch.Tensor([x]).float())\n        register_numpy(vel_mean, \"vel_mean\")\n        register_numpy(pos_mean, \"pos_mean\")\n        register_scalar(vel_std, \"vel_std\")\n        register_numpy(rot_mean, \"rot_mean\")\n        register_scalar(rot_std, \"rot_std\")\n        register_scalar(pos_std, \"pos_std\")\n        self.automatic_optimization = True\n        max_seq_length = 40\n        self.seq_scheduler = SeqScheduler(20, 40)\n        self.seq_scheduler2 = SeqScheduler(5,20)\n        self.max_seq = max_seq_length\n        self.past_seq = past_seq\n        self.learned_embedding = False\n        self.batch_processor = BatchProcessDatav2()\n        self.phase_dim = phase_dim\n        self.dt = dt\n        #self.VAE_op = VAE_Pose_Operator(skeleton)\n        self.phase_op = PhaseOperator(dt)\n        self.lr = self.init_lr = 1e-3\n        self.normal_method = 'zscore'\n        self.input_channel = pose_channels\n        self.save_hyperparameters(ignore=['moe_decoder','mode','pretrained_model'])\n        self.skeleton = skeleton\n        num_joints = skeleton.num_joints\n        self.num_joints = num_joints\n        self.state_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*6 # +phase_dim*2# c_t, root velocity, q_t\n        self.offset_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*3  # offset from target, root offset, pose offset\n        self.target_input_size = (num_joints) * 6 + len(self.pos_rep_idx)*6 # + phase_dim*2# q_target\n        self.dropout = dropout\n        self.state_encoder = StateEncoder(self.state_input_size)\n        self.offset_encoder = StateEncoder(self.offset_input_size)\n        self.target_encoder = StateEncoder(self.target_input_size)\n        self.embedding_style = StyleEmbedding(256)\n        self.LSTMCell = torch.nn.LSTMCell(1024, rnn_size)\n        self.initial_state_predictor = PredictInitialState()\n        self.pose_channels = pose_channels\n        self.phase_predictor = MoeStylePhasePredictor(rnn_size, phase_dim,8)\n        self.film_generator = IAN_FilmGenerator2(12 * 22)\n        self.decoder = moe_decoder.decoder\n\n        self.sigma_target = 0.5\n        self.max_tta = past_seq + max_seq_length - 5\n        if (self.learned_embedding):\n            self.embedding = nn.Embedding(self.max_tta + 1, 256)\n            self.embedding512 = nn.Embedding(self.max_tta+1,512)\n        else:\n            self.embedding = PosEncoding(self.max_tta + 1, 256)\n            self.embedding512 = PosEncoding(self.max_tta + 1, 512)\n        if (pretrained_model != None):\n            if(type(pretrained_model)==dict):\n                self.load_state_dict(pretrained_model['state_dict'], strict=False)\n            else:\n                self.load_state_dict(pretrained_model.state_dict(), strict=False)\n\n        self.l1_loss = nn.L1Loss()\n        self.mse_loss = nn.MSELoss()\n        self.common_operator = CommonOperator(batch_size=32)\n    def target_encoding(self, target_rots,target_pos, target_v):\n        return self.target_encoder(torch.cat((target_pos.flatten(-2,-1),target_v.flatten(-2,-1),target_rots.flatten(-2,-1)), dim=-1))\n\n    def state_encoding(self, pos,vel, rot):\n        return self.state_encoder(torch.cat((pos.flatten(-2,-1),vel.flatten(-2,-1), rot.flatten(-2,-1)), dim=-1))\n\n    def offset_encoding(self, pos, rot):\n        return self.offset_encoder(torch.cat((pos.flatten(-2,-1),rot.flatten(-2,-1)), dim=-1))\n\n    def regu_pose(self, pos, edge_len, rot):\n        import src.geometry.inverse_kinematics as ik\n        pos = ik.scale_pos_to_bonelen(pos, edge_len, self.skeleton._level_joints, self.skeleton._level_joints_parents)\n        rot = normalized_or6d(rot)\n        return pos, rot\n\n\n    def shift_running(self, local_pos, local_rots,phases,As,Fs, style_code, noise_per_sequence, start_id, target_id,length,phase_schedule=1.):\n        from src.Net.TransitionNet import concat\n        if not length:\n            length = target_id - start_id\n\n        '''pose: N,T,J,9'''\n        '''hip: N,T,3'''\n        '''phases: N,T,M'''\n        '''style_code: [N,C,T,J| N,C,T,J]'''\n        J = local_pos.shape[-2]\n        N, T, J, C = local_pos.shape\n        device = local_pos.device\n\n        output_pos = torch.empty(size=(N, length,  (J),3), device=device)\n        output_rot = torch.empty(size=(N, length,  (J),6), device=device)\n        output_phase = torch.empty(size=(N, length, phases.shape[-2], 2), device=phases.device)\n        output_sphase = torch.empty(size=(N, length, phases.shape[-2], 2), device=phases.device)\n        output_A = torch.empty(size=(N, length, phases.shape[-2], 1), device=phases.device)\n        output_F = torch.empty(size=(N, length, phases.shape[-2], 1), device=phases.device)\n        #\n        hn = torch.zeros(N, 1024, device=device)\n        cn = torch.zeros(N, 1024, device=device)\n\n        if (noise_per_sequence == None):\n            noise_per_sequence = torch.normal(0, 0.5, size=(N, 512), device=device)\n        offset_t = torch.scalar_tensor(start_id + length - 1, dtype=torch.int32, device=device)\n        tmax = torch.scalar_tensor(self.max_tta, dtype=torch.int32, device=device)\n        local_pos = local_pos[:,:,self.pos_rep_idx]\n        last_g_v = local_pos[:, 1] - local_pos[:, 0]\n        target_g_pos, target_g_rots = local_pos[:, target_id-1, :], local_rots[:,target_id-1, :]\n        target_g_v = local_pos[:,target_id-1]-local_pos[:,target_id-2]\n        last_g_pos, last_g_rot  = local_pos[:, 1, :], local_rots[:, 1, :]\n\n\n        latent_loss = 0.\n\n        target_latent = self.target_encoding(target_g_rots, target_g_pos-target_g_pos[:,0:1], target_g_v)\n\n        encode_first = True\n        if(hasattr(self,\"predict_phase\")==False or self.predict_phase==False):\n            for i in range(1,start_id-1):\n                last_l_v, target_l_v = last_g_v, target_g_v\n                last_l_pos, target_l_pos = last_g_pos, target_g_pos\n                last_l_rot, target_l_rot = last_g_rot, target_g_rots\n                offset_pos = target_l_pos - last_l_pos\n                offset_rot = target_l_rot - last_l_rot\n                offset_t = offset_t - 1\n\n                state_latent = self.state_encoding(last_l_pos-last_l_pos[:,0:1], last_l_v, last_l_rot)\n                offset_latent = self.offset_encoding(offset_pos, offset_rot)\n                state_latent = self.embedding_style(style_code, state_latent, None, encode_first)\n                latent,h_target = concat(state_latent, offset_latent, target_latent, self.embedding,self.embedding512, noise_per_sequence, offset_t, tmax)\n                encode_first = False\n                (hn, cn) = self.LSTMCell(latent, (hn, cn))\n                last_g_pos,last_g_rot  = local_pos[:,i+1],local_rots[:,i+1]\n                last_g_v = local_pos[:,i+1]-local_pos[:,i]\n                last_phase = phases[:,i+1]\n        else:\n            last_l_v, target_l_v = local_pos[:,1:start_id-1]-local_pos[:,0:start_id-2], target_g_v.repeat(1,start_id-2,1,1)\n            last_l_pos,target_l_pos = local_pos[:,1:start_id-1],target_g_pos.unsqueeze(1).repeat(1,start_id-2,1,1)\n            last_l_rot,target_l_rot = local_rots[:,1:start_id-1],target_g_rots.unsqueeze(1).repeat(1,start_id-2,1,1)\n            offset_pos = target_l_pos-last_l_pos\n            offset_rot = target_l_rot-last_l_rot\n            last_l_pos = last_l_pos.flatten(0,1)\n            last_l_v = last_l_v.flatten(0,1)\n            last_l_rot = last_l_rot.flatten(0,1)\n            offset_pos = offset_pos.flatten(0,1)\n            offset_rot = offset_rot.flatten(0,1)\n\n            state_latent = self.state_encoding(last_l_pos - last_l_pos[:, 0:1], last_l_v, last_l_rot)\n            offset_latent = self.offset_encoding(offset_pos, offset_rot)\n            style_code[0] = style_code[0].unsqueeze(1).repeat(1,start_id-2,1,1).flatten(0,1)\n            state_latent = self.embedding_style(style_code, state_latent, None, first=True)\n\n            target_latent = target_latent.unsqueeze(1).repeat(1,start_id-2,1).flatten(0,1)\n            ## recover\n            recover = lambda x: x.view((N,start_id-2)+x.shape[1:])\n            state_latent = recover(state_latent)\n            offset_latent = recover(offset_latent)\n            target_latent = recover(target_latent)\n            style_code[0] = recover(style_code[0])\n\n            offset_ts = []\n            for i in range(1,start_id-1):\n                offset_t = offset_t - 1\n                offset_ts.append(offset_t.view(1))\n            offset_ts = torch.cat(offset_ts,dim=0)\n\n            from src.Net.TransitionNet import multi_concat\n            latent, h_target = multi_concat(state_latent, offset_latent, target_latent, self.embedding, self.embedding512,noise_per_sequence,offset_ts, tmax)\n            pre_phase, preA, preS = self.initial_state_predictor(latent)\n\n            style_code[0] = style_code[0][:,0]\n            target_latent = target_latent[:,0]\n\n            # prepare for predicting the first frame\n            last_g_pos,last_g_rot  = local_pos[:,start_id-1],local_rots[:,start_id-1]\n            last_g_v = local_pos[:,start_id-1]-local_pos[:,start_id-2]\n            last_phase = pre_phase\n           # last_phase = phases[:,start_id-1]\n            for i in range(1,start_id-1):\n                (hn, cn) = self.LSTMCell(latent[:,i-1], (hn, cn))\n\n        first = True\n        step = 0\n        for i in range(start_id-1, start_id-1+length):\n            last_l_v, target_l_v = last_g_v, target_g_v\n            last_l_pos, target_l_pos = last_g_pos, target_g_pos\n            last_l_rot, target_l_rot = last_g_rot, target_g_rots\n            offset_pos = target_l_pos - last_l_pos\n            offset_rot = target_l_rot - last_l_rot\n            offset_t = offset_t - 1\n\n            state_latent = self.state_encoding(last_l_pos - last_l_pos[:, 0:1], last_l_v, last_l_rot)\n            offset_latent = self.offset_encoding(offset_pos, offset_rot)\n            state_latent = self.embedding_style(style_code, state_latent, None, encode_first)\n            encode_first=False\n            latent,h_target = concat(state_latent, offset_latent, target_latent, self.embedding,self.embedding512, noise_per_sequence, offset_t,tmax)\n            (hn, cn) = self.LSTMCell(latent, (hn, cn))\n\n            input_clip = hn\n            pred_phase,pred_A,pred_F,hip_l_v,hip_l_rv,latent = self.phase_predictor(style_code,input_clip,last_phase,h_target,first)\n            hip_l_r = hip_l_rv + last_l_rot[:,0]\n            condition_no_style = torch.cat(((last_l_pos - last_l_pos[:, 0:1]).flatten(-2,-1), last_l_v.flatten(-2,-1), hip_l_v, last_l_rot.flatten(-2,-1), hip_l_r), dim=-1)\n            nxt_phase = self.phase_op.next_phase(last_phase, pred_A, pred_F)\n            slerp_phase = self.phase_op.slerp(nxt_phase, pred_phase)\n            pred_pose_, coefficients = self.decoder(latent, condition_no_style,slerp_phase)\n            pred_l_v, pred_l_rot_v = pred_pose_[..., :len(self.pos_rep_idx) * 3], pred_pose_[..., len(self.pos_rep_idx) * 3:]\n            pred_l_v = pred_l_v.view(-1,len(self.pos_rep_idx),3)\n            pred_l_rot_v = pred_l_rot_v.view(-1, self.skeleton.num_joints, 6)\n\n            pred_g_v = pred_l_v\n            pred_g_v[:,0] = hip_l_v\n            pred_rot = pred_l_rot_v+last_l_rot\n            pred_rot[:,0] = hip_l_r\n            pred_pos = pred_g_v + last_g_pos\n\n            output_pos[:,step, self.pos_rep_idx] = pred_pos\n            output_rot[:, step] = pred_rot\n            output_phase[:,step] = pred_phase\n            output_A[:,step] = pred_A\n            output_F[:,step] = pred_F\n            output_sphase[:,step] = slerp_phase\n\n            last_g_pos, last_g_rot = pred_pos, pred_rot\n            last_g_v = pred_g_v\n            last_phase = slerp_phase\n\n            first = False\n            step+=1\n\n        latent_loss = latent_loss/length\n        #output = output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss#,output_hip_pos,output_hip_rot#, output_phase,output_A,output_F\n        if(hasattr(self,\"predict_phase\") and self.predict_phase):\n            return output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss,[pre_phase,preA,preS]\n        else:\n            return output_pos, output_rot,[output_phase,output_A,output_F,output_sphase],latent_loss\n\n\n    def phase_loss(self, gt_phase, gt_A, gt_F, phase, A, F, sphase):\n        loss = {\"phase\": self.mse_loss(gt_phase, phase), \"A\": self.mse_loss(gt_A, A), \"F\": self.mse_loss(gt_F, F),\n                \"slerp_phase\": self.mse_loss(gt_phase, sphase)}\n        return loss\n    def weight_sum_loss(self, loss, weight):\n        l = 0\n        for key in loss.keys():\n            l = l + loss[key] * weight[key] if key in weight.keys() else l + loss[key]\n        return l\n    def rot_to_pos(self,rot,offsets,hip):\n        if (rot.shape[-1] == 6):\n            rot = or6d_to_quat(rot)\n        rot_pos = self.skeleton.global_rot_to_global_pos(rot, offsets, hip)\n        return rot_pos\n\n    def get_gt_contact(self,gt_v):\n        eps = 0.5\n        eps_bound = 1.\n        def contact_smooth(x, idx, eps, bound):\n            # x must be in range of [eps,bound]\n            x = (x - eps) / (bound - eps)\n            x2 = x * x\n            x3 = x2 * x\n            contact = 2 * (x3) - 3 * (x2) + 1\n            return contact * idx\n        key_joint = [3, 4, 7, 8]\n        gt_fv = gt_v[:, :, key_joint]\n        gt_fv = torch.sum(gt_fv.pow(2), dim=-1).sqrt()\n        # first we need gt_contact\n        idx = (gt_fv < eps)  # * (pred_fv>gt_fv) # get penality idx\n        idx_smooth = (gt_fv >= eps) * (gt_fv < eps_bound)\n        gt_contact = contact_smooth(gt_fv, idx_smooth, eps, eps_bound) + torch.ones_like(gt_fv) * idx\n        return gt_contact\n    def contact_foot_loss(self,contact,pred_v):\n        key_joints = [3, 4, 7, 8]\n        pred_fv = pred_v[:, :, key_joints]\n        pred_fv = (torch.sum(pred_fv.pow(2), dim=-1) + 1e-6)  # .sqrt()\n        contact_idx = torch.where(contact < 0.01, 0, contact)\n        # torch.where(contact>=0.01,-contact,0) # only those who is 100% fixed, we don't apply position constrain\n        contact_num = torch.count_nonzero(contact_idx)\n        pred_fv = pred_fv * contact_idx\n        contact_loss = pred_fv.sum() / max(contact_num, 1)\n        return contact_loss\n    def shared_forward(self, batch, seq, optimizer=None, d_optimizer=None):\n        N = batch['local_pos'].shape[0] #// 2\n        style_code = self.get_film_code(batch['sty_pos'][:N], batch['sty_rot'][:N])\n        A = batch['A']\n        S = batch['S']\n        F = S[:, 1:] - S[:, :-1]\n        F = self.phase_op.remove_F_discontiny(F)\n        F = F / self.phase_op.dt\n        local_pos, local_rots, edge_len, phases = self.transform_batch_to_VAE(batch)\n        # source\n        local_pos = local_pos[:N]\n        local_rots = local_rots[:N]\n        edge_len = edge_len[:N]\n        phases = phases[:N]\n        A = A[:N]\n        F = F[:N]\n        offsets = batch['offsets'][:N]\n        noise=None\n        start_id = 10\n        target_id = 10+seq\n\n        output = self.shift_running(local_pos, local_rots, phases, A, F,style_code, noise,\n                                                                                start_id=start_id, target_id=target_id,length=seq,\n                                                                                phase_schedule=self.schedule_phase)\n        if(self.predict_phase):\n            pred_pos, pred_rot, pred_phase, latent_loss, first_phase = output\n        else:\n            pred_pos, pred_rot, pred_phase, latent_loss = output\n        rot_pos = self.rot_to_pos(pred_rot, offsets, pred_pos[:, :, 0:1])\n        pred_pos[:, :, self.rot_rep_idx] = rot_pos[:, :, self.rot_rep_idx]\n\n        if (self.test == True or random.random() < 0.1):\n            pred_pos, pred_rot = self.regu_pose(pred_pos, edge_len, pred_rot)\n        edge_mean = 21.\n\n        glb_loss = self.l1_loss(local_pos[:, start_id:target_id, :]/edge_mean, pred_pos[:, :, :]/edge_mean)\n        glb_rot_loss = self.l1_loss(local_rots[:, start_id:target_id], pred_rot)\n        last_pos_loss = self.l1_loss(local_pos[:,target_id-1,:]/edge_mean,pred_pos[:,-1]/edge_mean)\n        last_rot_loss = self.l1_loss(local_rots[:,target_id-1,:],pred_rot[:,-1])\n        gt_contact = self.get_gt_contact(local_pos[:,start_id+1:target_id]-local_pos[:,start_id:target_id-1])\n        contact_loss = self.contact_foot_loss(gt_contact,pred_pos[:,1:]-pred_pos[:,:-1])\n\n        phase_loss = self.phase_loss(phases[:, start_id:target_id], A[:, start_id:target_id], F[:, start_id-1:target_id-1], pred_phase[0][:, :],pred_phase[1][:, :], pred_phase[2][:, :], pred_phase[3])\n        loss = {\"glb_pos\": glb_loss, \"glb_rot\": glb_rot_loss, \"lst_pos\": last_pos_loss, \"lst_rot\": last_rot_loss,\n                **phase_loss, \"fv\": contact_loss}\n        if(self.predict_phase):\n            pre_phase,pre_A,pre_S = first_phase\n            first_phase_loss = {\"f_ph\": self.mse_loss(phases[:,start_id-1], pre_phase), \"f_A\": self.mse_loss(A[:,start_id-1], pre_A), \"f_S\": self.mse_loss(S[:,start_id-1], pre_S)}\n            loss = {**loss,**first_phase_loss}\n\n\n        new_weight = {\"glb_pos\":1.,\"glb_rot\":1., \"fv\":0.01,\"lst_pos\":1.0,\"lst_rot\":0.5,\"phase\": 0.5, \"A\": 0.5, \"F\": 0.5, \"slerp_phase\": 0.5,\"f_ph\":0.5,\"f_A\":0.5,\"f_S\":0.5}\n        loss['loss'] = self.weight_sum_loss(loss, new_weight)\n\n        return loss, pred_pos, pred_rot\n\n\n    def un_normalized(self, batch):\n        batch['glb_vel'] = batch[\"glb_vel\"] * self.vel_std + self.vel_mean\n        batch['glb_rot'] = batch['glb_rot'] * self.rot_std+self.rot_mean\n        batch['glb_pos'] = batch['glb_pos'] * self.pos_std+self.pos_mean\n        return batch\n\n\n    def normalized(self, batch):\n        batch['glb_rot'] = (batch['glb_rot']-self.rot_mean)/self.rot_std\n        batch['glb_vel'] = (batch['glb_vel']-self.vel_mean)/self.vel_std\n        batch['glb_pos'] = (batch['glb_pos']-self.pos_mean)/self.pos_std\n        return batch\n\n    def transform_batch_to_filmEncoder(self,glb_pos,glb_rot):\n        glb_vel, glb_pos, glb_rot, root_rotation = self.batch_processor.forward(glb_rot, glb_pos)\n        glb_rot = quat_to_or6D(glb_rot)\n        batch = {'glb_vel': glb_vel, 'glb_rot': glb_rot, 'glb_pos': glb_pos}\n\n        batch = self.normalized(batch)\n        batch = {key: batch[key][:, :, 1:] for key in batch.keys()}\n        return self.transform_batch_to_input(batch)\n    def transform_batch_to_input(self,batch):\n        glb_vel, glb_rot, glb_pos = batch['glb_vel'],batch['glb_rot'], batch['glb_pos']\n        data = torch.cat((glb_pos[:,1:],glb_vel,glb_rot[:,1:]),dim=-1)\n        data  = data.permute(0,3,1,2).contiguous() # N,C,T,J\n        return data\n    def get_film_code(self,glb_pos,glb_rot):\n        if (glb_rot.shape[-1] == 6):\n            glb_rot = or6d_to_quat(glb_rot)\n        data = self.transform_batch_to_filmEncoder(glb_pos, glb_rot)\n        data = data.transpose(-2, -1).contiguous().flatten(1, 2)\n        if self.test ==False:\n            idx = torch.randperm(data.shape[0])\n        else:\n            idx = torch.arange(data.shape[0])\n        return self.film_generator(data[idx])#,idx\n\n\n\n    def transform_batch_to_VAE(self, batch):\n        local_pos, local_rot = batch['local_pos'], batch['local_rot']\n        edge_len = torch.norm(batch['offsets'][:, 1:], dim=-1, keepdim=True)\n        return local_pos, quat_to_or6D(local_rot), edge_len,batch['phase']\n\n    def validation_step(self, batch, batch_idx):\n        self.scheduled_prob = 1.\n        self.schedule_phase = 1.\n        self.style_phase = 1.\n        self.test =True\n        loss,pred_pos,pred_rot = self.shared_forward(batch, self.seq_scheduler.max_seq)\n        self.common_operator.log_dict(self, loss, \"val_\")\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        self.test=True\n        self.scheduled_prob = 1.\n        self.schedule_phase = 1.\n        self.style_phase = 1.\n        loss, pred_pos, pred_rot = self.shared_forward(batch, self.seq_scheduler.max_seq)\n        self.common_operator.log_dict(self, loss, \"test_\")\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        self.test =False\n        if(self.mode=='pretrain'):\n            progress = self.common_operator.get_progress(self, target_epoch=3,start_epoch=0)\n            self.schedule_phase = self.common_operator.get_progress(self,target_epoch=4,start_epoch=1)\n            self.style_phase = self.common_operator.get_progress(self,target_epoch=6,start_epoch=3)\n        else:\n            progress = 1\n            self.schedule_phase = 1.\n        length = self.seq_scheduler.range(progress)\n        '''calculate loss'''\n        loss,pred_pos,pred_rot = self.shared_forward(batch, length)\n        self.common_operator.log_dict(self, loss, \"train_\")\n        self.log(\"length\", length, logger=True)\n        return loss['loss']\n\n    def configure_optimizers(self):\n        models = self.common_operator.collect_models(self)\n        trained_model = []\n        for param in self.parameters():\n            param.requires_grad = False\n        def weight_decay(model_name,lr, weight_decay):\n            trained_model.append(model_name)\n            model = getattr(self,model_name)\n            for param in model.parameters():\n                param.requires_grad = True\n            return self.common_operator.add_weight_decay(model,lr, weight_decay)\n\n        lr = self.lr\n        if(self.mode=='pretrain' and self.predict_phase==False):\n            params = weight_decay(\"film_generator\",lr,1e-4)+weight_decay(\"target_encoder\", lr, 0) + weight_decay(\"embedding_style\",lr,0)+\\\n                     weight_decay(\"offset_encoder\", lr, 0) +\\\n                     weight_decay(\"state_encoder\", lr, 0) + \\\n                     weight_decay(\"LSTMCell\", lr, 0) + weight_decay(\"phase_predictor\",lr,0)#+weight_decay(\"decoder\",lr,0)\n        elif(self.predict_phase == True):\n            params = weight_decay(\"initial_state_predictor\",lr,1e-4)\n        elif(self.mode=='fine_tune'):\n            lr = self.lr*0.1\n            params = weight_decay(\"film_generator\", lr, 1e-4) + weight_decay(\"target_encoder\", lr, 0) + weight_decay(\n                \"embedding_style\", lr, 0) + \\\n                     weight_decay(\"offset_encoder\", lr, 0) + \\\n                     weight_decay(\"state_encoder\", lr, 0) + \\\n                     weight_decay(\"LSTMCell\", lr, 0) + weight_decay(\"phase_predictor\", lr, 0)\n\n\n        optimizer = torch.optim.Adam(params, lr=lr, betas=(0.5, 0.9), amsgrad=True)\n\n        non_train_model=[i for i in models if i not in trained_model]\n        if(len(non_train_model)>0):\n            import warnings\n            warnings.warn(\"warning:there are some models not trained:{}\".format(non_train_model))\n\n        return [optimizer]", "\nfrom src.Datasets.StyleVAE_DataModule import StyleVAE_DataModule\nclass Application_phase(nn.Module):\n    def __init__(self, net: TransitionNet_phase, data_module: StyleVAE_DataModule):\n        super(Application_phase, self).__init__()\n        self.Net = net\n        self.data_module = data_module\n        self.data_loader = data_module.loader\n        self.src_batch = None\n        self.target_batch = None\n\n        self.skeleton = data_module.skeleton\n    def transform_batch(self,motion,label):\n        batch = [[],label]\n        batch[0] = [[motion[0],motion[1],motion[2]]]\n        for i, value in enumerate(batch[0][0]):\n            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n        phase = {}\n        for key in motion[3].keys():\n            phase[key] = torch.from_numpy(motion[3][key]).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n        batch[0][0].append(phase)\n        batch = self.data_module.transfer_mannual(batch,0,use_phase=True,use_sty=False)\n        return batch\n    def transform_anim(self, anim, label):\n        batch = [[], label]\n        batch[0] = [[anim.quats, anim.offsets, anim.hip_pos]]\n        for i, value in enumerate(batch[0][0]):\n            batch[0][0][i] = torch.from_numpy(value).unsqueeze(0).unsqueeze(0).type(self.Net.dtype).to(self.Net.device)\n        batch = self.data_module.transfer_mannual(batch, 0,use_phase=False)\n\n        # batch = normalized(self.Net,batch,True)\n        return batch\n\n    def setSource(self, anim):\n        if(type(anim)==tuple):\n            self.src_batch = self.transform_batch(anim,0)\n        else:\n            self.src_batch = self.transform_anim(anim, 0)\n        self.offsets = self.src_batch['offsets']\n        self.tangent = find_secondary_axis(self.offsets)\n\n\n\n    def setTarget(self, anim):\n        if (type(anim) == tuple):\n            self.target_anim = self.transform_batch(anim, 2)\n        else:\n            self.target_anim = self.transform_anim(anim, 2)\n\n    def _get_transform_ori_motion(self, batch):\n        global_pos = batch['local_pos']\n        global_rot = batch['local_rot']\n        global_rot = self.skeleton.inverse_pos_to_rot(global_rot, global_pos, self.offsets, self.tangent)\n        lp, lq = self.skeleton.inverse_kinematics(global_rot, global_pos)\n        return lp[0].cpu().numpy(), lq[0].cpu().numpy()\n\n    def get_source(self):\n        return self._get_transform_ori_motion(self.src_batch)\n    def get_target(self):\n        return self._get_transform_ori_motion(self.target_anim)\n\n    def forward(self,t,x):\n        self.Net.schedule_phase = 1.\n        self.Net.style_phase = 1.\n        seq = self.Net.seq_scheduler.max_seq\n        # seq = 10\n        self.Net.eval()\n        with torch.no_grad():\n            loc_pos, loc_rot, edge_len,phases = self.Net.transform_batch_to_VAE(self.src_batch)\n            tar_pos,tar_rot ,_,_= self.Net.transform_batch_to_VAE(self.target_anim)\n            target_style = self.Net.get_film_code(tar_pos,tar_rot)\n\n            A = self.src_batch['A']\n            S = self.src_batch['S']\n            F = S[:,1:]-S[:,:-1]\n            F = self.Net.phase_op.remove_F_discontiny(F)\n            F = F/self.Net.phase_op.dt\n            if x !=1 :\n                loc_pos[:, 12:, :, [0, 2]] = loc_pos[:, 12:, :, [0, 2]] + (\n                            loc_pos[:, 10 + seq, 0, [0, 2]] - loc_pos[:, 10, 0, [0, 2]]) * x\n            if(self.Net.predict_phase):\n                pred_pos, pred_rot, pred_phase, _,_ = self.Net.shift_running(loc_pos, loc_rot, phases, A, F,\n                                                                          target_style, None, start_id=10,\n                                                                          target_id=10 + seq, length=int(seq * t),\n                                                                          phase_schedule=1.)\n            else:\n                pred_pos, pred_rot,pred_phase,_= self.Net.shift_running(loc_pos, loc_rot, phases,A,F,target_style, None, start_id=10,\n                                                            target_id=10 + seq,length = int(seq*t) ,phase_schedule=1.)\n\n            rot_pos = self.Net.rot_to_pos(pred_rot,self.src_batch['offsets'],pred_pos[:,:,0:1])\n            pred_pos[:,:,self.Net.rot_rep_idx] = rot_pos[:,:,self.Net.rot_rep_idx]\n            output_pos, output_rot = self.Net.regu_pose(pred_pos, edge_len, pred_rot)\n\n            output_pos = torch.cat((loc_pos[:, :10, ...], output_pos, loc_pos[:, 10 + seq:, ...]), dim=1)\n            output_rot = torch.cat((loc_rot[:, :10, ...], output_rot, loc_rot[:, 10 + seq:, ...]), dim=1)\n            batch = {}\n            batch['local_rot'] = or6d_to_quat(output_rot)\n            batch['local_pos'] = output_pos\n            return self._get_transform_ori_motion(batch)", "            # output = self.src\n\n"]}
{"filename": "src/utils/locate_model.py", "chunked_list": ["import re\nimport os\ndef locate_model(check_file:str,epoch):\n    if(epoch=='last'):\n        check_file+=\"last.ckpt\"\n        return check_file\n    dirs = os.listdir(check_file)\n    for dir in dirs:\n        st = \"epoch=\" + epoch + \"-step=\\d+.ckpt\"\n        out = re.findall(st, dir)\n        if (len(out) > 0):\n            check_file += out[0]\n            print(check_file)\n            return check_file", ""]}
{"filename": "src/utils/motion_process.py", "chunked_list": ["import numpy as np\nfrom np_vector import quat_mul,quat_mul_vec,quat_inv,normalize,quat_between\nfrom BVH_mod import  Anim\ndef angle_between_quats_along_y(from_quat,to_quat):\n    \"\"\"\n        Quats tensor for current rotations (B, 4)\n        :return\n    \"\"\"\n    #mask = np.tile(np.array([[1.0, 0.0, 1.0]], dtype=np.float),(from_quat.shape[0],1))\n    initial_direction = np.array([[0.0, 0.0, 1.0]], dtype=np.float)\n    quat = quat_mul(to_quat,quat_inv(from_quat))\n    dir = quat_mul_vec(quat,initial_direction)\n   # dir[...,1] = 0\n   # dir = normalize(dir)\n    return np.arctan2(dir[...,0],dir[...,2])", "def find_Yrotation_to_align_with_Xplus(q):\n    \"\"\"\n\n    :param q: Quats tensor for current rotations (B, 4)\n    :return y_rotation: Quats tensor of rotations to apply to q to align with X+\n    \"\"\"\n    mask = np.array([[1.0, 0.0, 1.0]], dtype=np.float)#(q.shape[0], -1)\n    mask = np.tile(mask,(q.shape[0],q.shape[1],1))\n    '''\u7528\u6765\u5ea6\u91cfquat\u7684\u65b9\u5411'''\n    initial_direction = np.tile(np.array([[1.0, 0.0, 0.0]], dtype=np.float),(q.shape[0],q.shape[1],1))\n    forward = mask * quat_mul_vec(q, initial_direction)\n    forward = normalize(forward)\n    y_rotation = normalize(quat_between(forward, np.array([[1, 0, 0]])))\n    return y_rotation", "'''We keep batch version only'''\ndef extract_feet_contacts(pos:np.array , lfoot_idx, rfoot_idx, velfactor=0.02):\n    assert len(pos.shape)==4\n    \"\"\"\n    Extracts binary tensors of feet contacts\n    :param pos: tensor of global positions of shape (Timesteps, Joints, 3)\n    :param lfoot_idx: indices list of left foot joints\n    :param rfoot_idx: indices list of right foot joints\n    :param velfactor: velocity threshold to consider a joint moving or not\n    :return: float tensors of left foot contacts and right foot contacts\n    \"\"\"\n    lfoot_xyz = (pos[:,1:, lfoot_idx, :] - pos[:,:-1, lfoot_idx, :]) ** 2\n    contacts_l = (np.sum(lfoot_xyz, axis=-1) < velfactor).astype(np.float)\n\n    rfoot_xyz = (pos[:,1:, rfoot_idx, :] - pos[:,:-1, rfoot_idx, :]) ** 2\n    contacts_r = ((np.sum(rfoot_xyz, axis=-1)) < velfactor).astype(np.float)\n\n    # Duplicate the last frame for shape consistency\n    contacts_l = np.concatenate([contacts_l, contacts_l[:,-1:]], axis=1)\n    contacts_r = np.concatenate([contacts_r, contacts_r[:,-1:]], axis=1)\n\n    return [contacts_l,contacts_r]", "\ndef subsample(anim:Anim,ratio = 2):\n    anim.quats = anim.quats[::ratio,...]\n    anim.hip_pos = anim.hip_pos[::ratio,...]\n    return anim"]}
{"filename": "src/utils/np_vector.py", "chunked_list": ["import numpy\nimport numpy as np\nimport pytorch3d.transforms\n_FLOAT_EPS = np.finfo(np.float32).eps\n_EPS4 = _FLOAT_EPS * 4.0\n_EPS16 = _FLOAT_EPS * 16.0\ndef clamp_mean(x,axis):\n    x = np.asarray(x,dtype=np.float64)\n    return np.mean(x,axis=axis)\ndef clamp_std(x,axis):\n    x = np.asarray(x,dtype=np.float64)\n    std = np.std(x,axis=axis)\n    return np.where(np.abs(std) < _EPS4, _EPS4, std)", "def clamp_std(x,axis):\n    x = np.asarray(x,dtype=np.float64)\n    std = np.std(x,axis=axis)\n    return np.where(np.abs(std) < _EPS4, _EPS4, std)\ndef length(x, axis=-1, keepdims=True):\n    \"\"\"\n    Computes vector norm along a tensor axis(axes)\n\n    :param x: tensor\n    :param axis: axis(axes) along which to compute the norm\n    :param keepdims: indicates if the dimension(s) on axis should be kept\n    :return: The length or vector of lengths.\n    \"\"\"\n    lgth = np.sqrt(np.sum(x * x, axis=axis, keepdims=keepdims))\n    return lgth", "\n\ndef normalize(x, axis=-1):\n    \"\"\"\n    Normalizes a tensor over some axis (axes)\n\n    :param x: data tensor\n    :param axis: axis(axes) along which to compute the norm\n    :param eps: epsilon to prevent numerical instabilities\n    :return: The normalized tensor\n    \"\"\"\n    x = np.array(x,dtype=np.float64)\n    res = x / (length(x, axis=axis) + _EPS4)\n    return res", "\n\ndef quat_normalize(x):\n    \"\"\"\n    Normalizes a quaternion tensor\n\n    :param x: data tensor\n    :param eps: epsilon to prevent numerical instabilities\n    :return: The normalized quaternions tensor\n    \"\"\"\n    res = normalize(x)\n    return res", "\n\ndef angle_axis_to_quat(angle, axis):\n    \"\"\"\n    Converts from and angle-axis representation to a quaternion representation\n\n    :param angle: angles tensor\n    :param axis: axis tensor\n    :return: quaternion tensor\n    \"\"\"\n    c = np.cos(angle / 2.0)[..., np.newaxis]\n    s = np.sin(angle / 2.0)[..., np.newaxis]\n    q = np.concatenate([c, s * axis], axis=-1)\n    return q", "\n\ndef euler_to_quat(e, order='zyx'):\n    \"\"\"\n\n    Converts from an euler representation to a quaternion representation\n\n    :param e: euler tensor\n    :param order: order of euler rotations\n    :return: quaternion tensor\n    \"\"\"\n    axis = {\n        'x': np.asarray([1, 0, 0], dtype=np.float32),\n        'y': np.asarray([0, 1, 0], dtype=np.float32),\n        'z': np.asarray([0, 0, 1], dtype=np.float32)}\n\n    q0 = angle_axis_to_quat(e[..., 0], axis[order[0]])\n    q1 = angle_axis_to_quat(e[..., 1], axis[order[1]])\n    q2 = angle_axis_to_quat(e[..., 2], axis[order[2]])\n\n    return quat_mul(q0, quat_mul(q1, q2))", "\n\ndef quat_inv(q):\n    \"\"\"\n    Inverts a tensor of quaternions\n\n    :param q: quaternion tensor\n    :return: tensor of inverted quaternions\n    \"\"\"\n    res = np.asarray([1, -1, -1, -1], dtype=np.float32) * q\n    return res", "\n\n# def quat_fk(lrot, lpos, parents):\n#     \"\"\"\n#     Performs Forward Kinematics (FK) on local quaternions and local positions to retrieve global representations\n#\n#     :param lrot: tensor of local quaternions with shape (..., Nb of joints, 4)\n#     :param lpos: tensor of local positions with shape (..., Nb of joints, 3)\n#     :param parents: list of parents indices\n#     :return: tuple of tensors of global quaternion, global positions", "#     :param parents: list of parents indices\n#     :return: tuple of tensors of global quaternion, global positions\n#     \"\"\"\n#     gp, gr = [lpos[..., :1, :]], [lrot[..., :1, :]]\n#     for i in range(1, len(parents)):\n#         gp.append(quat_mul_vec(gr[parents[i]], lpos[..., i:i+1, :]) + gp[parents[i]])\n#         gr.append(quat_mul    (gr[parents[i]], lrot[..., i:i+1, :]))\n#\n#     res = np.concatenate(gr, axis=-2), np.concatenate(gp, axis=-2)\n#     return res", "#     res = np.concatenate(gr, axis=-2), np.concatenate(gp, axis=-2)\n#     return res\n#\n#\n# def quat_ik(grot, gpos, parents):\n#     \"\"\"\n#     Performs Inverse Kinematics (IK) on global quaternions and global positions to retrieve local representations\n#\n#     :param grot: tensor of global quaternions with shape (..., Nb of joints, 4)\n#     :param gpos: tensor of global positions with shape (..., Nb of joints, 3)", "#     :param grot: tensor of global quaternions with shape (..., Nb of joints, 4)\n#     :param gpos: tensor of global positions with shape (..., Nb of joints, 3)\n#     :param parents: list of parents indices\n#     :return: tuple of tensors of local quaternion, local positions\n#     \"\"\"\n#     res = [\n#         np.concatenate([\n#             grot[..., :1, :],\n#             quat_mul(quat_inv(grot[..., parents[1:], :]), grot[..., 1:, :]),\n#         ], axis=-2),", "#             quat_mul(quat_inv(grot[..., parents[1:], :]), grot[..., 1:, :]),\n#         ], axis=-2),\n#         np.concatenate([\n#             gpos[..., :1, :],\n#             quat_mul_vec(\n#                 quat_inv(grot[..., parents[1:], :]),\n#                 gpos[..., 1:, :] - gpos[..., parents[1:], :]),\n#         ], axis=-2)\n#     ]\n#", "#     ]\n#\n#     return res\n\n\ndef quat_mul(x, y):\n    \"\"\"\n    Performs quaternion multiplication on arrays of quaternions\n\n    :param x: tensor of quaternions of shape (..., Nb of joints, 4)\n    :param y: tensor of quaternions of shape (..., Nb of joints, 4)\n    :return: The resulting quaternions\n    \"\"\"\n    x0, x1, x2, x3 = x[..., 0:1], x[..., 1:2], x[..., 2:3], x[..., 3:4]\n    y0, y1, y2, y3 = y[..., 0:1], y[..., 1:2], y[..., 2:3], y[..., 3:4]\n\n    res = np.concatenate([\n        y0 * x0 - y1 * x1 - y2 * x2 - y3 * x3,\n        y0 * x1 + y1 * x0 - y2 * x3 + y3 * x2,\n        y0 * x2 + y1 * x3 + y2 * x0 - y3 * x1,\n        y0 * x3 - y1 * x2 + y2 * x1 + y3 * x0], axis=-1)\n\n    return res", "\n\ndef quat_mul_vec(q, x):\n    \"\"\"\n    Performs multiplication of an array of 3D vectors by an array of quaternions (rotation).\n\n    :param q: tensor of quaternions of shape (..., Nb of joints, 4)\n    :param x: tensor of vectors of shape (..., Nb of joints, 3)\n    :return: the resulting array of rotated vectors\n    \"\"\"\n    t = 2.0 * np.cross(q[..., 1:], x)\n    res = x + q[..., 0][..., np.newaxis] * t + np.cross(q[..., 1:], t)\n\n    return res", "\n\ndef quat_slerp(x, y, a):\n    \"\"\"\n    Perfroms spherical linear interpolation (SLERP) between x and y, with proportion a\n\n    :param x: quaternion tensor\n    :param y: quaternion tensor\n    :param a: indicator (between 0 and 1) of completion of the interpolation.\n    :return: tensor of interpolation results\n    \"\"\"\n    len = np.sum(x * y, axis=-1)\n\n    neg = len < 0.0\n    len[neg] = -len[neg]\n    y[neg] = -y[neg]\n\n    a = np.zeros_like(x[..., 0]) + a\n    amount0 = np.zeros(a.shape)\n    amount1 = np.zeros(a.shape)\n\n    linear = (1.0 - len) < 0.01\n    omegas = np.arccos(len[~linear])\n    sinoms = np.sin(omegas)\n\n    amount0[linear] = 1.0 - a[linear]\n    amount0[~linear] = np.sin((1.0 - a[~linear]) * omegas) / sinoms\n\n    amount1[linear] = a[linear]\n    amount1[~linear] = np.sin(a[~linear] * omegas) / sinoms\n    res = amount0[..., np.newaxis] * x + amount1[..., np.newaxis] * y\n\n    return res", "\n\ndef quat_between(x, y):\n    \"\"\"\n    Quaternion rotations between two 3D-vector arrays\n\n    :param x: tensor of 3D vectors\n    :param y: tensor of 3D vetcors\n    :return: tensor of quaternions\n    \"\"\"\n    res = np.concatenate([\n        np.sqrt(np.sum(x * x, axis=-1) * np.sum(y * y, axis=-1))[..., np.newaxis] +\n        np.sum(x * y, axis=-1)[..., np.newaxis],\n        np.cross(x, y)], axis=-1)\n\n    return res", "\n\ndef interpolate_local(lcl_r_mb,lcl_q_mb,length,target):\n    # Extract last past frame and target frame\n    start_lcl_r_mb = lcl_r_mb[:, 10 - 1, :, :][:, None, :, :]  # (B, 1, J, 3)\n    end_lcl_r_mb = lcl_r_mb[:, target, :, :][:, None, :, :]\n    start_lcl_q_mb = lcl_q_mb[:, 10 - 1, :, :]\n    end_lcl_q_mb = lcl_q_mb[:, target, :, :]\n    # LERP Local Positions:\n    n_trans = length\n    interp_ws = np.linspace(0.0, 1.0, num=n_trans + 2, dtype=np.float32)\n    offset = end_lcl_r_mb - start_lcl_r_mb\n    const_trans = np.tile(start_lcl_r_mb, [1, n_trans + 2, 1, 1])\n    inter_lcl_r_mb = const_trans + (interp_ws)[None, :, None, None] * offset\n\n    # SLERP Local Quats:\n    interp_ws = np.linspace(0.0, 1.0, num=n_trans + 2, dtype=np.float32)\n    inter_lcl_q_mb = np.stack(\n        [(quat_normalize(quat_slerp(quat_normalize(start_lcl_q_mb), quat_normalize(end_lcl_q_mb), w))) for w in\n         interp_ws], axis=1)\n\n    return inter_lcl_r_mb, inter_lcl_q_mb", "def interpolate_local_(lcl_r_mb, lcl_q_mb, n_past, n_future):\n    \"\"\"\n    Performs interpolation between 2 frames of an animation sequence.\n\n    The 2 frames are indirectly specified through n_past and n_future.\n    SLERP is performed on the quaternions\n    LERP is performed on the root's positions.\n\n    :param lcl_r_mb:  Local/Global root positions (B, T, 1, 3)\n    :param lcl_q_mb:  Local quaternions (B, T, J, 4)\n    :param n_past:    Number of frames of past context\n    :param n_future:  Number of frames of future context\n    :return: Interpolated root and quats\n    \"\"\"\n\n    # Extract last past frame and target frame\n    start_lcl_r_mb = lcl_r_mb[:, n_past - 1, :, :][:, None, :, :]  # (B, 1, J, 3)\n    end_lcl_r_mb = lcl_r_mb[:, -n_future, :, :][:, None, :, :]\n\n    start_lcl_q_mb = lcl_q_mb[:, n_past - 1, :, :]\n    end_lcl_q_mb = lcl_q_mb[:, -n_future, :, :]\n\n    # LERP Local Positions:\n    n_trans = lcl_r_mb.shape[1] - (n_past + n_future)\n    interp_ws = np.linspace(0.0, 1.0, num=n_trans + 2, dtype=np.float32)\n    offset = end_lcl_r_mb - start_lcl_r_mb\n\n    const_trans    = np.tile(start_lcl_r_mb, [1, n_trans + 2, 1, 1])\n    inter_lcl_r_mb = const_trans + (interp_ws)[None, :, None, None] * offset\n\n    # SLERP Local Quats:\n    interp_ws = np.linspace(0.0, 1.0, num=n_trans + 2, dtype=np.float32)\n    inter_lcl_q_mb = np.stack(\n        [(quat_normalize(quat_slerp(quat_normalize(start_lcl_q_mb), quat_normalize(end_lcl_q_mb), w))) for w in\n         interp_ws], axis=1)\n\n    return inter_lcl_r_mb, inter_lcl_q_mb", "\nimport torch\ndef remove_quat_discontinuities(rotations:torch.tensor):\n    \"\"\"\n\n    Removing quat discontinuities on the time dimension (removing flips)\n\n    :param rotations: Array of quaternions of shape (T, J, 4)\n    :return: The processed array without quaternion inversion.\n    \"\"\"\n    rotations = rotations.clone()\n    rots_inv = -rotations\n    for i in range(1, rotations.shape[1]):\n        replace_mask = torch.sum(rotations[:, i - 1:i, ...] * rotations[:, i:i + 1, ...],\n                                 dim=-1, keepdim=True) < \\\n                       torch.sum(rotations[:, i - 1:i, ...] * rots_inv[:, i:i + 1, ...],\n                                 dim=-1, keepdim=True)\n        replace_mask = replace_mask.squeeze(1).type_as(rotations)\n        rotations[:, i, ...] = replace_mask * rots_inv[:, i, ...] + (1.0 - replace_mask) * rotations[:, i, ...]\n    return rotations", "\n\n# Orient the data according to the las past keframe\n# def rotate_at_frame(X, Q, parents, n_past=10):\n#     \"\"\"\n#     Re-orients the animation data according to the last frame of past context.\n#\n#     :param X: tensor of local positions of shape (Batchsize, Timesteps, Joints, 3)\n#     :param Q: tensor of local quaternions (Batchsize, Timesteps, Joints, 4)\n#     :param parents: list of parents' indices", "#     :param Q: tensor of local quaternions (Batchsize, Timesteps, Joints, 4)\n#     :param parents: list of parents' indices\n#     :param n_past: number of frames in the past context\n#     :return: The rotated positions X and quaternions Q\n#     \"\"\"\n#     # Get global quats and global poses (FK)\n#     global_q, global_x = quat_fk(Q, X, parents)\n#\n#     key_glob_Q = global_q[:, n_past - 1: n_past, 0:1, :]  # (B, 1, 1, 4)\n#     forward = np.array([1, 0, 1])[np.newaxis, np.newaxis, np.newaxis, :] \\", "#     key_glob_Q = global_q[:, n_past - 1: n_past, 0:1, :]  # (B, 1, 1, 4)\n#     forward = np.array([1, 0, 1])[np.newaxis, np.newaxis, np.newaxis, :] \\\n#                  * quat_mul_vec(key_glob_Q, np.array([0, 1, 0])[np.newaxis, np.newaxis, np.newaxis, :])\n#     forward = normalize(forward)\n#     yrot = quat_normalize(quat_between(np.array([1, 0, 0]), forward))#1,0,0\n#     new_glob_Q = quat_mul(quat_inv(yrot), global_q)\n#     new_glob_X = quat_mul_vec(quat_inv(yrot), global_x)\n#\n#     # back to local quat-pos\n#     Q, X = quat_ik(new_glob_Q, new_glob_X, parents)", "#     # back to local quat-pos\n#     Q, X = quat_ik(new_glob_Q, new_glob_X, parents)\n#\n#     return X, Q\n\n\ndef extract_feet_contacts(pos, lfoot_idx, rfoot_idx, velfactor=0.02):\n    \"\"\"\n    Extracts binary tensors of feet contacts\n\n    :param pos: tensor of global positions of shape (Timesteps, Joints, 3)\n    :param lfoot_idx: indices list of left foot joints\n    :param rfoot_idx: indices list of right foot joints\n    :param velfactor: velocity threshold to consider a joint moving or not\n    :return: binary tensors of left foot contacts and right foot contacts\n    \"\"\"\n    lfoot_xyz = (pos[1:, lfoot_idx, :] - pos[:-1, lfoot_idx, :]) ** 2\n    contacts_l = (np.sqrt(np.sum(lfoot_xyz, axis=-1)) < velfactor)\n\n    rfoot_xyz = (pos[1:, rfoot_idx, :] - pos[:-1, rfoot_idx, :]) ** 2\n    contacts_r = (np.sqrt(np.sum(rfoot_xyz, axis=-1)) < velfactor)\n\n    # Duplicate the last frame for shape consistency\n    contacts_l = np.concatenate([contacts_l, contacts_l[-1:]], axis=0)\n    contacts_r = np.concatenate([contacts_r, contacts_r[-1:]], axis=0)\n\n    return contacts_l, contacts_r", "\n##########\n# My own code\n#########\n# axis sequences for Euler angles\n\ndef quat_to_mat(quat):\n\n    \"\"\" Convert Quaternion to Euler Angles.  See rotation.py for notes \"\"\"\n    quat = np.asarray(quat, dtype=np.float64)\n    assert quat.shape[-1] == 4, \"Invalid shape quat {}\".format(quat.shape)\n\n    w, x, y, z = quat[..., 0], quat[..., 1], quat[..., 2], quat[..., 3]\n    Nq = np.sum(quat * quat, axis=-1)\n    s = 2.0 / Nq\n    X, Y, Z = x * s, y * s, z * s\n    wX, wY, wZ = w * X, w * Y, w * Z\n    xX, xY, xZ = x * X, x * Y, x * Z\n    yY, yZ, zZ = y * Y, y * Z, z * Z\n\n    mat = np.empty(quat.shape[:-1] + (3, 3), dtype=np.float64)\n    mat[..., 0, 0] = 1.0 - (yY + zZ)\n    mat[..., 0, 1] = xY - wZ\n    mat[..., 0, 2] = xZ + wY\n    mat[..., 1, 0] = xY + wZ\n    mat[..., 1, 1] = 1.0 - (xX + zZ)\n    mat[..., 1, 2] = yZ - wX\n    mat[..., 2, 0] = xZ - wY\n    mat[..., 2, 1] = yZ + wX\n    mat[..., 2, 2] = 1.0 - (xX + yY)\n    return np.where((Nq > _FLOAT_EPS)[..., np.newaxis, np.newaxis], mat, np.eye(3))", "def mat_to_euler(mat):\n    \"\"\" Convert Rotation Matrix to Euler Angles.  See rotation.py for notes \"\"\"\n    mat = np.asarray(mat, dtype=np.float64)\n    assert mat.shape[-2:] == (3, 3), \"Invalid shape matrix {}\".format(mat)\n\n    cy = np.sqrt(mat[..., 2, 2] * mat[..., 2, 2] + mat[..., 1, 2] * mat[..., 1, 2])\n    condition = cy > _EPS4\n    euler = np.empty(mat.shape[:-1], dtype=np.float64)\n    euler[..., 2] = np.where(condition,\n                             -np.arctan2(mat[..., 0, 1], mat[..., 0, 0]),\n                             -np.arctan2(-mat[..., 1, 0], mat[..., 1, 1]))\n    euler[..., 1] = np.where(condition,\n                             -np.arctan2(-mat[..., 0, 2], cy),\n                             -np.arctan2(-mat[..., 0, 2], cy))\n    euler[..., 0] = np.where(condition,\n                             -np.arctan2(mat[..., 1, 2], mat[..., 2, 2]),\n                             0.0)\n    return euler", "def quat_to_euler(quat):\n    \"\"\" Convert Quaternion to Euler Angles.  See rotation.py for notes \"\"\"\n    return mat_to_euler(quat_to_mat(quat))\n\n"]}
{"filename": "src/utils/__init__.py", "chunked_list": ["import os\nimport sys\nBASEPATH = os.path.dirname(__file__)\nsys.path.insert(0, BASEPATH)"]}
{"filename": "src/utils/Drawer.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport matplotlib\n#matplotlib.use('TKAgg')\nclass Drawer():\n    def __init__(self):\n        str_dict = {\"point\":\"o\",}\n        pass\n    def create_canvas(self,m,n):\n        self.fig,self.ax = plt.subplots(m,n,figsize=(m,n*2))\n\n    def get_window(self,window):\n        return self.ax[window]\n\n    def draw(self,*args,channel,feature):\n\n        self.ax[channel,feature].plot(*args)\n    def scatter(self,*args,channel,feature):\n        self.ax[channel,feature].scatter(*args)\n    def show(self):\n        plt.show()"]}
{"filename": "src/utils/BVH_mod.py", "chunked_list": ["import re\nimport numpy as np\nimport src.geometry.forward_kinematics as fk\nimport src.geometry.inverse_kinematics as ik\nimport torch\nfrom src.geometry.vector import find_secondary_axis\nfrom np_vector import euler_to_quat,remove_quat_discontinuities,quat_to_euler\nimport pytorch3d.transforms as trans\n\n", "\n\nchannelmap = {\n    'Xrotation' : 'x',\n    'Yrotation' : 'y',\n    'Zrotation' : 'z'   \n}\n\nchannelmap_inv = {\n    'x': 'Xrotation',", "channelmap_inv = {\n    'x': 'Xrotation',\n    'y': 'Yrotation',\n    'z': 'Zrotation',\n}\n\nordermap = {\n    'x' : 0,\n    'y' : 1,\n    'z' : 2,", "    'y' : 1,\n    'z' : 2,\n}\ndef find_sys_with_prefix(names,name,l_prefix,r_prefix):\n    if (name.lower().startswith(l_prefix)):\n        l_joint_name = name[len(l_prefix):]\n        for rName in names:\n            if(rName.lower().startswith(r_prefix)):\n                r_joint_name = rName[len(r_prefix):]\n                if(l_joint_name == r_joint_name):\n                    return rName\n    return \"\"", "def find_sys(names,name):\n    sys_part = find_sys_with_prefix(names,name,\"l\",\"r\")\n    if(sys_part==\"\"):\n        sys_part = find_sys_with_prefix(names, name, \"left\", \"right\")\n    return sys_part\ndef find_level_recursive(map,parent,node,level_joint,level_joint_parent,level):\n    if(level not in level_joint):\n        level_joint[level]=[]\n    if(level not in level_joint_parent):\n        level_joint_parent[level]=[]\n    level_joint[level].append(node)\n    level_joint_parent[level].append(parent)\n    for child_id in map[node][\"children\"]:\n        find_level_recursive(map,node,child_id,level_joint,level_joint_parent,level+1)", "def find_link_recursive(map,node,link):\n    for child_id in map[node][\"children\"]:\n        link.append((node,child_id))\n        find_link_recursive(map,child_id,link)\nclass Skeleton(object):\n    def __init__(self,parents,names,offsets,symmetry=None):\n        self.num_joints = len(parents)\n        self.root = 0\n        self.parents = parents\n        self.names = names\n        self.hierarchy = {}\n        self.name_to_id = {}\n        self.__build_name_map()\n        self.bone_pair_indices = symmetry\n        self._level_joints = []\n        self._level_joints_parents=[]\n        if(symmetry==None):\n            self.__predict_sysmetry()\n        self.predict_sysmetry_axis(offsets)\n        self.__build_hierarchy()\n        self.__build_level_map()\n        self.self_link = [(i,i) for i in range(len(parents))]\n        self.neighbor_link =[]\n        find_link_recursive(self.hierarchy,self.root,self.neighbor_link)\n        self.build_end_effector()\n\n\n    def __eq__(self, other):\n        if(other==None):\n            return False\n        return self.parents == other.parents and self.names == other.names and self.bone_pair_indices == other.bone_pair_indices\n    def build_end_effector(self):\n        self.multi_children_idx = []\n        self.end_effector_idx = []\n        for i in range(len(self.names)):\n            child = [*self.hierarchy[i]['children'].keys()]\n            if(len(child)>1):\n                self.multi_children_idx.append(i)\n            elif(len(child)==0):\n                self.end_effector_idx.append(i)\n\n    def __build_name_map(self):\n        self.hierarchy[-1] = {\"name\":\"\"}\n        self.name_to_id[''] = -1\n        for idx, name in enumerate(self.names):\n            self.hierarchy[idx] = {\"name\": name}\n            self.name_to_id[name] = idx\n    def predict_sysmetry_axis(self,offsets):\n        offsets = offsets[self.bone_pair_indices]-offsets\n        x = np.max(np.abs(offsets)[...,0])\n        y = np.max(np.abs(offsets)[...,1])\n        z = np.max(np.abs(offsets)[...,2])\n        if(x>y and x>z):\n            self.sys = \"x\"\n        elif(y>z and y>x):\n            self.sys = 'y'\n        elif(z>y and z>x):\n            self.sys = 'z'\n        else:\n            assert(0,\"Unpredicition\")\n    def __predict_sysmetry(self):\n        '''\u6839\u636e\u9aa8\u9abc\u540d\u5b57\u731c\u6d4b\u5bf9\u79f0\u9aa8\u9abcidx\uff0c\u53ef\u80fd\u4e0d\u51c6\u786e\uff0c\u5982\u679c\u6ca1\u6709\u5bf9\u79f0\u9aa8\u9abc\uff0c\u5219\u8be5\u4f4d\u7f6e\u4e3a-1'''\n        self.bone_pair_indices = [-1 for _ in range(len(self.parents))]\n        for idx, name in enumerate(self.names):\n            if(idx in self.bone_pair_indices):\n                continue\n            sys_name = (find_sys(self.names,name))\n            sys_idx = self.name_to_id[sys_name]\n            self.bone_pair_indices[idx] = sys_idx\n            if(sys_idx==-1):\n                self.bone_pair_indices[idx] = idx\n            else:\n                self.bone_pair_indices[sys_idx] = idx\n\n\n\n    def __build_hierarchy(self):\n        '''\u6784\u5efa\u9aa8\u9abc\u540d\u5b57\uff0cidx\uff0cparent,children\uff0c\u5bf9\u79f0\u7b49\u7d22\u5f15'''\n        mapping = self.hierarchy\n        for idx, (name,par_id) in enumerate(zip(self.names, self.parents)):\n            children_id = []\n            children = {}\n            for child_id,child_parent in enumerate(self.parents):\n                if(child_parent==idx):\n                    children_id.append(child_id)\n                    children[child_id] = mapping[child_id]\n            mapping[idx][\"parent\"] = mapping[par_id]\n            mapping[idx][\"children\"] = children\n            if(self.bone_pair_indices[idx]!=-1):\n                mapping[idx][\"sys\"] = mapping[self.bone_pair_indices[idx]]\n\n    def __build_level_map(self):\n        level_joint = {}\n        level_joint_parent = {}\n\n        find_level_recursive(self.hierarchy,-1,self.root,level_joint,level_joint_parent,0)\n        for i in range(len(level_joint)):\n            self._level_joints.append(level_joint[i])\n            self._level_joints_parents.append(level_joint_parent[i])\n\n    def forward_kinematics(self,local_rotations: torch.Tensor, local_offsets: torch.Tensor,hip_positions:torch.Tensor):\n        #gp,gq =  fk.forward_kinematics_quats(local_rotations, local_offsets,hip_positions, self._level_joints, self._level_joints_parents)\n        gp,gq =  fk.forward_kinematics_quats(local_rotations, local_offsets,hip_positions, self.parents)\n        return gp,gq\n    def inverse_kinematics_quats(self,joints_global_rotation:torch.Tensor):\n        return ik.inverse_kinematics_only_quats(joints_global_rotation, self.parents)\n    def inverse_kinematics(self,joints_global_rotations: torch.Tensor, joints_global_positions: torch.Tensor,output_other_dim = False):\n        lp,lq =  ik.inverse_kinematics_quats(joints_global_rotations, joints_global_positions, self.parents)\n        if(output_other_dim):\n            return lp,lq\n        else:\n            return lp[...,0:1,:],lq\n    def global_rot_to_global_pos(self,global_rot,offsets,hip):\n        # quick version of fk(ik()), only for global_rotation\n        pos_shape = global_rot.shape[:-1]+(3,)\n        offsets = offsets.unsqueeze(-3).expand(pos_shape)\n        #hip = torch.zeros_like(offsets[...,:1,:])\n        gp = torch.empty(pos_shape,dtype=global_rot.dtype,device = global_rot.device)#[hip]\n        gp[...,0:1,:] = hip[...,0:1,:]\n        offset = trans.quaternion_apply(global_rot[..., self.parents[1:], :], offsets[..., 1:, :])\n\n        for level in range(1, len(self._level_joints)):\n            local_bone_indices = self._level_joints[level]\n            local_bone_indices_1 = [i-1 for i in local_bone_indices]\n            parent_bone_indices = self._level_joints_parents[level]\n            gp[...,local_bone_indices,:] = offset[...,local_bone_indices_1,:]+gp[...,parent_bone_indices,:]\n\n        # for i in range(1, len(self.parents)):\n        #     gp.append(offset[..., i-1:i, :] + gp[self.parents[i]])\n        # gp = torch.cat(gp, dim=-2)\n        return gp\n    def inverse_pos_to_rot(self,joints_global_rotations,joints_global_positions,offsets,tangent):\n        edge_len = torch.norm(offsets[:,1:],dim=-1,keepdim=True)\n        joints_global_positions = ik.scale_pos_to_bonelen(joints_global_positions,edge_len,self._level_joints,self._level_joints_parents)\n        parent_rot = self.pos_to_parent_rot(joints_global_rotations,joints_global_positions,offsets,tangent)\n        out = self.reduce_parent_rot(parent_rot)\n        special_idx = [*self.end_effector_idx,*self.multi_children_idx,*[9,10,11,12,13,15,19]]\n        out[...,special_idx,:] = joints_global_rotations[...,special_idx,:]\n        return out\n\n    def pos_to_parent_rot(self,joints_global_rotations:torch.Tensor,joints_global_positions:torch.Tensor,local_offsets:torch.Tensor,secondary_offsets:torch.Tensor):\n        return ik.change_pos_to_rot(joints_global_rotations, joints_global_positions, local_offsets, secondary_offsets,self.parents)\n    def reduce_parent_rot(self,parent_rot):\n        shape = list(parent_rot.shape)\n        shape[-2] += 1\n        rot = torch.empty(size=shape,device=parent_rot.device,dtype=parent_rot.dtype)\n        #children = [[*self.hierarchy[i]['children'].keys()] for i in range(len(self.names))]\n        idx = []\n        child_idx = []\n        for i in range(len(self.names)):\n            child = [*self.hierarchy[i]['children'].keys()]\n            if(len(child)==1):\n                idx.append(i)\n                child_idx.append(child[0]-1)\n        rot[...,idx,:] = parent_rot[...,child_idx,:]\n            # else:\n            #     rot[..., i, :] = torch.zeros_like(rot[..., i, :])\n        return rot", "\n\n\nclass SkeletonLevel():\n    def __init__(self,n_joints,edges,root,body_parts,last_n_joints, pooling_list=None,unpooling_list=None):\n        self.n_joints = n_joints\n        self.last_n_joints = last_n_joints\n        self.edges = edges\n        self.pooling_list = pooling_list\n        self.unpooling_list = unpooling_list\n        self.root = root\n        self.body_parts = body_parts", "\n\n\n\n\n\n\nclass Anim(object):\n\n    \"\"\"\n    A very basic animation object\n    \"\"\"\n    def __init__(self, quats, pos, offsets, parents, names,remove_joints = None,Tpose = -1,remove_gap:float = 0.,filename= \"\"):\n        \"\"\"\n        :param quats: local quaternions tensor\n        :param pos: local positions tensor\n        :param offsets: local joint offsets\n        :param parents: bone hierarchy\n        :param names: bone names\n        :param valid_names: useful for remove unnecessary joints\n        \"\"\"\n        from src.Datasets.BatchProcessor import BatchProcessData\n        process = BatchProcessData()\n        self.quats = quats\n        self.hip_pos = pos\n        self.offsets = offsets\n\n        self.parents = parents\n        self.names = names\n        if(remove_joints!=None):\n            self.quats,self.offsets,self.parents,self.names = remove_joints(self.quats,self.offsets,self.parents,self.names)\n\n        self.skeleton = Skeleton(self.parents,self.names,self.offsets)\n        if(remove_gap>0.):\n\n            gp,gq = self.skeleton.forward_kinematics(torch.from_numpy(self.quats),torch.from_numpy(self.offsets),torch.from_numpy(self.hip_pos))\n\n            idx = check_continuty(gp,remove_gap)\n            #print(filename)\n\n            idx = list(set(idx))\n\n            idx.sort()\n            dict = process(gq.unsqueeze(0),gp.unsqueeze(0))\n            lp = dict['local_pos'][0]\n            idx1 = check_continuty(lp,remove_gap)\n            idx1 = list(set(idx1))\n            idx1.sort()\n            if(idx[-1]>10 or idx1[-1]>10):\n                print(filename)\n                print()\n            idx = max(idx[-1],idx1[-1])+1\n            self.quats = self.quats[idx:]\n            self.hip_pos = self.hip_pos[idx:]\n        self.secondary_offsets = find_secondary_axis(torch.from_numpy(self.offsets)).numpy()\n    def forward_kinamatics(self):\n        return self.skeleton.forward_kinematics(torch.from_numpy(self.quats),torch.from_numpy(self.offsets),torch.from_numpy(self.hip_pos))", "\n\n\n\n\n\n\ndef check_continuty(x,threshold):\n    nei = (x[1:]-x[:-1]).abs()\n    index = np.where(nei>threshold)[0]\n    index = (set(index))\n\n    vel = (nei[1:]-nei[:-1]).abs()\n    id = np.where(vel>threshold/10)[0]\n    id = (set(id))\n\n    index = index.intersection(id)\n    index = list(index)\n    index.sort()\n    if (len(index) == 0):\n        return [-1]\n    seq_start = index[-1]+1\n    if(seq_start>10):\n        print({key:float(nei[key].max()) for key in index})\n\n       # print(\"remove gap:{}, max gap{}, remove gap{}\".format(seq_start,nei.max(),nei[seq_start-1].max()))\n\n    return index", "def remove_zero_seq(hip_pos):\n    '''\u628a\u4e00\u6bb5\u5e8f\u5217\u4e2dhip_pos\u5728\u539f\u70b9\u4e4b\u524d\u7684\u5b50\u5e8f\u5217\u79fb\u9664'''\n    '''[seq,1,3]'''\n    index = np.where(np.dot(np.abs(hip_pos),np.array([1,1,1]))==0)\n    if(index[0].shape[0]==0):\n        return 0\n    seq_start = index[0][-1]\n    return seq_start\n\ndef read_bvh(filename, start=None, end=None, order=None,remove_joints = None,Tpose = -1,remove_gap :float=0.):\n    print(filename)\n    \"\"\"\n    Reads a BVH file and extracts animation information.\n\n    :param filename: BVh filename\n    :param start: start frame\n    :param end: end frame\n    :param order: order of euler rotations\n    :return: A simple Anim object conatining the extracted information.\n    \"\"\"\n\n    f = open(filename, \"r\")\n\n    i = 0\n    active = -1\n    end_site = False\n\n    names = []\n    orients = np.array([]).reshape((0, 4))\n    offsets = np.array([]).reshape((0, 3))\n    parents = np.array([], dtype=int)\n\n    # Parse the  file, line by line\n    for line in f:\n\n        if \"HIERARCHY\" in line: continue\n        if \"MOTION\" in line: continue\n\n        rmatch = re.match(r\"ROOT (\\w+)\", line)\n        if rmatch:\n            names.append(rmatch.group(1))\n            offsets = np.append(offsets, np.array([[0, 0, 0]]), axis=0)\n            orients = np.append(orients, np.array([[1, 0, 0, 0]]), axis=0)\n            parents = np.append(parents, active)\n            active = (len(parents) - 1)\n            continue\n\n        if \"{\" in line: continue\n\n        if \"}\" in line:\n            if end_site:\n                end_site = False\n            else:\n                active = parents[active]\n            continue\n\n        offmatch = re.match(r\"\\s*OFFSET\\s+([\\-\\d\\.e]+)\\s+([\\-\\d\\.e]+)\\s+([\\-\\d\\.e]+)\", line)\n        if offmatch:\n            if not end_site:\n                offsets[active] = np.array([list(map(float, offmatch.groups()))])\n            continue\n\n        chanmatch = re.match(r\"\\s*CHANNELS\\s+(\\d+)\", line)\n        if chanmatch:\n            channels = int(chanmatch.group(1))\n            if order is None:\n                channelis = 0 if channels == 3 else 3\n                channelie = 3 if channels == 3 else 6\n                parts = line.split()[2 + channelis:2 + channelie]\n                if any([p not in channelmap for p in parts]):\n                    continue\n                order = \"\".join([channelmap[p] for p in parts])\n            continue\n\n        jmatch = re.match(\"\\s*JOINT\\s+(\\w+)\", line)\n        if jmatch:\n            names.append(jmatch.group(1))\n            offsets = np.append(offsets, np.array([[0, 0, 0]]), axis=0)\n            orients = np.append(orients, np.array([[1, 0, 0, 0]]), axis=0)\n            parents = np.append(parents, active)\n            active = (len(parents) - 1)\n            continue\n\n        if \"End Site\" in line:\n            end_site = True\n            continue\n\n        fmatch = re.match(\"\\s*Frames:\\s+(\\d+)\", line)\n        if fmatch:\n            if start and end:\n                fnum = (end - start) - 1\n            else:\n                fnum = int(fmatch.group(1))\n            positions = offsets[np.newaxis].repeat(fnum, axis=0)\n            rotations = np.zeros((fnum, len(orients), 3))\n            continue\n\n        fmatch = re.match(\"\\s*Frame Time:\\s+([\\d\\.]+)\", line)\n        if fmatch:\n            frametime = float(fmatch.group(1))\n            continue\n\n        if (start and end) and (i < start or i >= end - 1):\n            i += 1\n            continue\n\n        dmatch = line.strip().split(' ')\n        if dmatch:\n            data_block = np.array(list(map(float, dmatch)))\n            N = len(parents)\n            fi = i - start if start else i\n            if channels == 3:\n                positions[fi, 0:1] = data_block[0:3]\n                rotations[fi, :] = data_block[3:].reshape(N, 3)\n            elif channels == 6:\n                data_block = data_block.reshape(N, 6)\n                positions[fi, :] = data_block[:, 0:3]\n                rotations[fi, :] = data_block[:, 3:6]\n            elif channels == 9:\n                positions[fi, 0] = data_block[0:3]\n                data_block = data_block[3:].reshape(N - 1, 9)\n                rotations[fi, 1:] = data_block[:, 3:6]\n                positions[fi, 1:] += data_block[:, 0:3] * data_block[:, 6:9]\n            else:\n                raise Exception(\"Too many channels! %i\" % channels)\n\n            i += 1\n\n    f.close()\n\n    rotations = euler_to_quat(np.radians(rotations), order=order)\n    rotations = remove_quat_discontinuities(torch.from_numpy(rotations).unsqueeze(0))[0].numpy()\n\n    return Anim(rotations, positions[...,0:1,:], offsets, parents, names,remove_joints,Tpose=Tpose,remove_gap=remove_gap,filename=filename)", "\ndef read_bvh(filename, start=None, end=None, order=None,remove_joints = None,Tpose = -1,remove_gap :float=0.):\n    print(filename)\n    \"\"\"\n    Reads a BVH file and extracts animation information.\n\n    :param filename: BVh filename\n    :param start: start frame\n    :param end: end frame\n    :param order: order of euler rotations\n    :return: A simple Anim object conatining the extracted information.\n    \"\"\"\n\n    f = open(filename, \"r\")\n\n    i = 0\n    active = -1\n    end_site = False\n\n    names = []\n    orients = np.array([]).reshape((0, 4))\n    offsets = np.array([]).reshape((0, 3))\n    parents = np.array([], dtype=int)\n\n    # Parse the  file, line by line\n    for line in f:\n\n        if \"HIERARCHY\" in line: continue\n        if \"MOTION\" in line: continue\n\n        rmatch = re.match(r\"ROOT (\\w+)\", line)\n        if rmatch:\n            names.append(rmatch.group(1))\n            offsets = np.append(offsets, np.array([[0, 0, 0]]), axis=0)\n            orients = np.append(orients, np.array([[1, 0, 0, 0]]), axis=0)\n            parents = np.append(parents, active)\n            active = (len(parents) - 1)\n            continue\n\n        if \"{\" in line: continue\n\n        if \"}\" in line:\n            if end_site:\n                end_site = False\n            else:\n                active = parents[active]\n            continue\n\n        offmatch = re.match(r\"\\s*OFFSET\\s+([\\-\\d\\.e]+)\\s+([\\-\\d\\.e]+)\\s+([\\-\\d\\.e]+)\", line)\n        if offmatch:\n            if not end_site:\n                offsets[active] = np.array([list(map(float, offmatch.groups()))])\n            continue\n\n        chanmatch = re.match(r\"\\s*CHANNELS\\s+(\\d+)\", line)\n        if chanmatch:\n            channels = int(chanmatch.group(1))\n            if order is None:\n                channelis = 0 if channels == 3 else 3\n                channelie = 3 if channels == 3 else 6\n                parts = line.split()[2 + channelis:2 + channelie]\n                if any([p not in channelmap for p in parts]):\n                    continue\n                order = \"\".join([channelmap[p] for p in parts])\n            continue\n\n        jmatch = re.match(\"\\s*JOINT\\s+(\\w+)\", line)\n        if jmatch:\n            names.append(jmatch.group(1))\n            offsets = np.append(offsets, np.array([[0, 0, 0]]), axis=0)\n            orients = np.append(orients, np.array([[1, 0, 0, 0]]), axis=0)\n            parents = np.append(parents, active)\n            active = (len(parents) - 1)\n            continue\n\n        if \"End Site\" in line:\n            end_site = True\n            continue\n\n        fmatch = re.match(\"\\s*Frames:\\s+(\\d+)\", line)\n        if fmatch:\n            if start and end:\n                fnum = (end - start) - 1\n            else:\n                fnum = int(fmatch.group(1))\n            positions = offsets[np.newaxis].repeat(fnum, axis=0)\n            rotations = np.zeros((fnum, len(orients), 3))\n            continue\n\n        fmatch = re.match(\"\\s*Frame Time:\\s+([\\d\\.]+)\", line)\n        if fmatch:\n            frametime = float(fmatch.group(1))\n            continue\n\n        if (start and end) and (i < start or i >= end - 1):\n            i += 1\n            continue\n\n        dmatch = line.strip().split(' ')\n        if dmatch:\n            data_block = np.array(list(map(float, dmatch)))\n            N = len(parents)\n            fi = i - start if start else i\n            if channels == 3:\n                positions[fi, 0:1] = data_block[0:3]\n                rotations[fi, :] = data_block[3:].reshape(N, 3)\n            elif channels == 6:\n                data_block = data_block.reshape(N, 6)\n                positions[fi, :] = data_block[:, 0:3]\n                rotations[fi, :] = data_block[:, 3:6]\n            elif channels == 9:\n                positions[fi, 0] = data_block[0:3]\n                data_block = data_block[3:].reshape(N - 1, 9)\n                rotations[fi, 1:] = data_block[:, 3:6]\n                positions[fi, 1:] += data_block[:, 0:3] * data_block[:, 6:9]\n            else:\n                raise Exception(\"Too many channels! %i\" % channels)\n\n            i += 1\n\n    f.close()\n\n    rotations = euler_to_quat(np.radians(rotations), order=order)\n    rotations = remove_quat_discontinuities(torch.from_numpy(rotations).unsqueeze(0))[0].numpy()\n\n    return Anim(rotations, positions[...,0:1,:], offsets, parents, names,remove_joints,Tpose=Tpose,remove_gap=remove_gap,filename=filename)", "    \n\n    \n#anim.pos: only root position writen into file, anim.offset: write all joints to file\ndef save_bvh(filename, anim, names=None, frametime=1.0 / 30.0, order='xyz', positions=False, orients=True):\n    \"\"\"\n    Saves an Animation to file as BVH\n\n    Parameters\n    ----------\n    filename: str\n        File to be saved to\n\n    anim : Animation\n        Animation to save\n\n    names : [str]\n        List of joint names\n\n    order : str\n        Optional Specifier for joint order.\n        Given as string E.G 'xyz', 'zxy'\n\n    frametime : float\n        Optional Animation Frame time\n\n    positions : bool\n        Optional specfier to save bone\n        positions for each frame\n\n    orients : bool\n        Multiply joint orients to the rotations\n        before saving.\n\n    \"\"\"\n\n    if names is None:\n        if anim.names is None:\n            names = [\"joint_\" + str(i) for i in range(len(anim.parents))]\n        else:\n            names = anim.names\n    print(filename)\n    with open(filename, 'w') as f:\n\n        t = \"\"\n        f.write(\"%sHIERARCHY\\n\" % t)\n        f.write(\"%sROOT %s\\n\" % (t, names[0]))\n        f.write(\"%s{\\n\" % t)\n        t += '\\t'\n\n        f.write(\"%sOFFSET %f %f %f\\n\" % (t, anim.offsets[0, 0], anim.offsets[0, 1], anim.offsets[0, 2]))\n        f.write(\"%sCHANNELS 6 Xposition Yposition Zposition %s %s %s \\n\" %\n                (t, channelmap_inv[order[0]], channelmap_inv[order[1]], channelmap_inv[order[2]]))\n        num_joints=anim.parents.shape[0]\n        assert num_joints==anim.offsets.shape[0]\n        seq_length=anim.quats.shape[0]\n        assert seq_length==anim.hip_pos.shape[0]\n        for i in range(anim.parents.shape[0]):#iterate joints\n            if anim.parents[i] == 0:\n                t = save_joint(f, anim, names, t, i, order=order, positions=positions)\n\n        t = t[:-1]\n        f.write(\"%s}\\n\" % t)\n\n        f.write(\"MOTION\\n\")\n        f.write(\"Frames: %i\\n\" % anim.quats.shape[0]);\n        f.write(\"Frame Time: %f\\n\" % frametime);\n\n\n        # if orients:\n        #    rots = np.degrees((-anim.orients[np.newaxis] * anim.rotations).euler(order=order[::-1]))\n        # else:\n        #    rots = np.degrees(anim.rotations.euler(order=order[::-1]))\n        rots = np.degrees(quat_to_euler(anim.quats))\n        poss = anim.hip_pos\n        if len(poss.shape)==2:\n            poss=poss[:,np.newaxis,:]\n        for i in range(poss.shape[0]):#frame\n            for j in range(anim.parents.shape[0]):#joints\n\n                if positions or j == 0:\n\n                    f.write(\"%f %f %f %f %f %f \" % (\n                        poss[i, j, 0], poss[i, j, 1], poss[i, j, 2],\n                        rots[i, j, ordermap[order[0]]], rots[i, j, ordermap[order[1]]], rots[i, j, ordermap[order[2]]]))\n\n                else:\n\n                    f.write(\"%f %f %f \" % (\n                        rots[i, j, ordermap[order[0]]], rots[i, j, ordermap[order[1]]], rots[i, j, ordermap[order[2]]]))\n\n            f.write(\"\\n\")", "\n\ndef save_joint(f, anim, names, t, i, order='zyx', positions=False):\n    f.write(\"%sJOINT %s\\n\" % (t, names[i]))\n    f.write(\"%s{\\n\" % t)\n    t += '\\t'\n\n    f.write(\"%sOFFSET %f %f %f\\n\" % (t, anim.offsets[i, 0], anim.offsets[i, 1], anim.offsets[i, 2]))\n\n    if positions:\n        f.write(\"%sCHANNELS 6 Xposition Yposition Zposition %s %s %s \\n\" % (t,\n                                                                            channelmap_inv[order[0]],\n                                                                            channelmap_inv[order[1]],\n                                                                            channelmap_inv[order[2]]))\n    else:\n        f.write(\"%sCHANNELS 3 %s %s %s\\n\" % (t,\n                                             channelmap_inv[order[0]], channelmap_inv[order[1]],\n                                             channelmap_inv[order[2]]))\n\n    end_site = True\n\n    for j in range(anim.parents.shape[0]):\n        if anim.parents[j] == i:\n            t = save_joint(f, anim, names, t, j, order=order, positions=positions)\n            end_site = False\n\n    if end_site:\n        f.write(\"%sEnd Site\\n\" % t)\n        f.write(\"%s{\\n\" % t)\n        t += '\\t'\n        f.write(\"%sOFFSET %f %f %f\\n\" % (t, 0.0, 0.0, 0.0))\n        t = t[:-1]\n        f.write(\"%s}\\n\" % t)\n\n    t = t[:-1]\n    f.write(\"%s}\\n\" % t)\n\n    return t", ""]}
{"filename": "src/Module/Utilities.py", "chunked_list": ["from torch import nn\nimport torch\nclass PLU(torch.nn.Module):\n  def __init__(self,w=None,c=None):\n    super(PLU, self).__init__()\n    if w==None:\n      self.w=torch.nn.Parameter(torch.Tensor([0.1,]))\n    else:\n      self.w=torch.nn.Parameter(torch.Tensor([w,]))\n    self.w.requires_grad=True\n    if(c == None):\n      self.c=torch.nn.Parameter(torch.Tensor([1]))\n    else:\n      self.c=torch.nn.Parameter(torch.Tensor([c]))\n    self.c.requires_grad=True\n\n  def forward(self, x):\n        # max(w(x+c)-c,min(w(x-c)+c,x))\n\n        return torch.max(self.w * (x + self.c) - self.c, torch.min(self.w * (x - self.c) + self.c, x))", "''' from [Liu et al.2019]'''\n\n\n\n\n\n\n\n\n", "\n\n\n"]}
{"filename": "src/Module/__init__.py", "chunked_list": ["import os\nimport sys\nBASEPATH = os.path.dirname(__file__)\nsys.path.insert(0, BASEPATH)"]}
{"filename": "src/Module/MoEModule.py", "chunked_list": ["import torch\nfrom torch import nn\nimport math\nclass MultipleExpertsLinear(nn.Module):\n    def __init__(self,in_channels,out_channels,nums):\n        super(MultipleExpertsLinear, self).__init__()\n        self.num_experts = nums\n        weight_list = []\n        bias_list = []\n        for i in range(nums):\n            weight = nn.Parameter(torch.empty(out_channels,in_channels))\n            bias = nn.Parameter(torch.empty(out_channels))\n            weight,bias = self.reset_parameter(weight,bias)\n            weight_list.append(weight.unsqueeze(0))\n            bias_list.append(bias.unsqueeze(0))\n        self.weight = nn.Parameter(torch.cat(weight_list,dim=0))\n        self.bias = nn.Parameter(torch.cat(bias_list,dim=0))\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        #self.linear = torch.nn.Linear(in_channels,out_channels)\n    def reset_parameter(self,weight,bias):\n        nn.init.kaiming_uniform_(weight, a=math.sqrt(5))\n        if bias is not None:\n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            nn.init.uniform_(bias, -bound, bound)\n        return weight,bias\n    def forward(self,x,coefficients):\n        '''coefficients: N,num_experts'''\n        '''x: N,C'''\n        mixed_weight = torch.einsum(\"bc,cmn->bmn\",(coefficients,self.weight))\n\n        mixed_bias = torch.matmul(coefficients,self.bias).unsqueeze(2)# N, out_channels, 1\n        x = x.unsqueeze(2)  # N,1,in_channels\n        out = torch.baddbmm(mixed_bias,mixed_weight,x).squeeze(2)\n        return out", "\n\n"]}
{"filename": "src/Module/PhaseModule.py", "chunked_list": ["import torch\nfrom src.Module.Utilities import PLU\n\nclass PhaseSegement(torch.nn.Module):\n    def __init__(self,phase_dims):\n        super(PhaseSegement, self).__init__()\n        self.phase_dims = phase_dims\n        self.A_eps = torch.scalar_tensor(1e-6)\n\n        self.F_act = torch.nn.Tanh()\n    def forward(self,out):\n        # x:[...,4*phase_dims]\n        phase, A, F = out[..., :2 * self.phase_dims], out[..., 2 * self.phase_dims:3 * self.phase_dims], out[...,3 * self.phase_dims:4 * self.phase_dims]\n        return phase.view(phase.shape[:-1]+(self.phase_dims,2)), A.unsqueeze(-1),F.unsqueeze(-1)", "class PhaseOperator():\n    def __init__(self,dt):\n        self.dt = dt\n        self._EPS32 = torch.finfo(torch.float32).eps\n        self.tpi = 2 * torch.pi\n        #self.phase_dim = phase_dim\n        pass\n    def phaseManifold(self,A,S):\n        assert A.shape[-1] == 1\n        out = torch.empty(size=(A.shape[:-1]+(2,)),device=A.device,dtype=A.dtype)\n        out[...,0:1] = A*torch.cos(self.tpi * S)\n        out[...,1:2] = A*torch.sin(self.tpi * S)\n        return out\n    def remove_F_discontiny(self,F):\n        F = torch.where(F <= -0.5, 1 + F, F)\n        F = torch.where(F >= 0.5, F - 1, F)\n        return F\n    def getA(self,phase):\n        # phase should be (...,phase_dim,2)\n        #phase = phase.view(phase.shape[:-1]+(self.phase_dim, 2))\n        A = torch.norm(phase, dim=-1,keepdim=True)\n        return A\n    def normalized_phase(self,p):\n        A0 = torch.norm(p, dim=-1, keepdim=True)\n        A0 = torch.where(A0.abs() < self._EPS32, self._EPS32, A0)\n        return p/A0\n\n    def next_phase(self,last_phase,A,F):\n        theta = self.tpi * F * self.dt\n        #dA = self.dt * A\n        #last_phase = last_phase.view(last_phase.shape[0], self.phase_dim, 2)\n        cos = torch.cos(theta)\n        sin = torch.sin(theta)\n        R = torch.stack([cos, sin, -sin, cos], dim=-1).reshape(cos.shape[:-1] + (2, 2))  # N,10,2,2\n\n        # normalized\n        baseA = torch.norm(last_phase,dim=-1,keepdim=True)\n        baseA = torch.clamp_min(baseA,self._EPS32)\n        last_phase = last_phase/baseA\n        pred_phase = torch.matmul(last_phase.unsqueeze(dim=-2), R).squeeze(dim=-2)\n        pred_phase = A*pred_phase\n        # if (torch.isnan(last_phase).any() or torch.isinf(last_phase).any()):\n        #     print(\"nan in last_phase\")\n        # elif (torch.isnan(R).any()):\n        #     print(\"nan in R\")\n        # elif(torch.isnan(pred_phase).any()):\n        #     print(\"nan in pred_phase\")\n\n        #pred_phase = pred_phase/baseA\n        #pred_phase = pred_phase * (dA+baseA)\n        return pred_phase\n\n    def slerp(self,p0,p1):\n\n        # only work for t = 0.5, if p0 \\approx -p1, the results might have numerical error, we assume p0 is similar to p1\n        A0 = torch.norm(p0,dim=-1,keepdim=True)\n        A1 = torch.norm(p1,dim=-1,keepdim=True)\n        A0 = torch.where(A0.abs()<self._EPS32,self._EPS32,A0)\n        A1 = torch.where(A1.abs()<self._EPS32,self._EPS32,A1)\n        # normalized\n        p0 = p0/A0\n        p1 = p1/A1\n        p = self.normalized_phase(0.5*(p0+p1))\n\n        A = 0.5*(A0+A1)\n        return A*p"]}
{"filename": "src/Module/VAEModule.py", "chunked_list": ["import torch\nfrom torch import  nn\nimport torch.nn.functional as F\nfrom src.geometry.quaternions import normalized_or6d\n_EPS32 = torch.finfo(torch.float32).eps\n\nclass VAE_Conv(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size):\n        super(VAE_Conv, self).__init__()\n        assert kernel_size%2==1\n        padding = kernel_size//2\n        self.latent_size = out_channels\n        self.encode = nn.Conv1d(in_channels,2*out_channels,kernel_size,padding=padding,padding_mode=\"reflect\")\n    def reparameterize(self,mu,log_sigma,epsilon = None):\n        std = torch.exp(0.5*log_sigma)\n        if epsilon==None:\n            epsilon = torch.randn_like(std)\n        return mu+std*epsilon\n    def forward(self,input):\n        x = self.encode(input)\n        mu = x[:,:self.latent_size]\n        log_var = x[:,self.latent_size:]\n        return self.reparameterize(mu,log_var),mu,log_var", "class VAE_Linear(nn.Module):\n    def __init__(self,in_channels,out_channels,output_ori=True):\n        super(VAE_Linear, self).__init__()\n        self.output_ori = output_ori\n        self.encode = nn.Linear(in_channels,2*out_channels)\n        self.N = out_channels\n        #self.encode_logvar = nn.Linear(in_channels,out_channels)\n    def reparameterize(self,mu,log_sigma,epsilon = None):\n        std = torch.exp(0.5*log_sigma)\n        if epsilon==None:\n            epsilon = torch.randn_like(std)\n        return mu+std*epsilon\n    def forward(self,input):\n        mu_logvar = self.encode(input)\n        mu,logvar = mu_logvar[:,:self.N],mu_logvar[:,self.N:]\n        #logvar = self.encode_logvar(input)\n        if(self.output_ori):\n            return self.reparameterize(mu,logvar),mu,logvar\n        else:\n            return self.reparameterize(mu,logvar)", ""]}
{"filename": "src/geometry/rotations.py", "chunked_list": ["from pytorch3d.transforms import rotation_6d_to_matrix, quaternion_to_matrix\nimport torch.nn.functional as F\nfrom src.geometry.vector import cross_product, normalize_vector\nimport torch\nimport numpy as np\n\n# m: batch*3*3\n# out: batch*4*4\ndef get_4x4_rotation_matrix_from_3x3_rotation_matrix(m):\n    batch_size = m.shape[0]\n\n    row4 = torch.autograd.Variable(torch.zeros(batch_size, 1, 3).type_as(m))\n    m43 = torch.cat((m, row4), 1)  # batch*4,3\n    col4 = torch.autograd.Variable(torch.zeros(batch_size, 4, 1).type_as(m))\n    col4[:, 3, 0] = col4[:, 3, 0] + 1\n    out = torch.cat((m43, col4), 2)  # batch*4*4\n\n    return out", "def get_4x4_rotation_matrix_from_3x3_rotation_matrix(m):\n    batch_size = m.shape[0]\n\n    row4 = torch.autograd.Variable(torch.zeros(batch_size, 1, 3).type_as(m))\n    m43 = torch.cat((m, row4), 1)  # batch*4,3\n    col4 = torch.autograd.Variable(torch.zeros(batch_size, 4, 1).type_as(m))\n    col4[:, 3, 0] = col4[:, 3, 0] + 1\n    out = torch.cat((m43, col4), 2)  # batch*4*4\n\n    return out", "\n\n# matrices batch*3*3\n# both matrix are orthogonal rotation matrices\n# out theta between 0 to 180 degree batch\ndef compute_geodesic_distance_from_two_matrices(m1, m2):\n    m = torch.bmm(m1, m2.transpose(1, 2))  # batch*3*3\n\n    theta = compute_angle_from_rotation_matrix(m)\n    # theta = torch.min(theta, 2*np.pi - theta)\n\n    return theta", "\n\n# matrices batch*3*3\n# both matrix are orthogonal rotation matrices\n# out theta between 0 to 180 degree batch\ndef compute_angle_from_rotation_matrix(m):\n    batch = m.shape[0]\n\n    cos = (m[:, 0, 0] + m[:, 1, 1] + m[:, 2, 2] - 1) / 2\n    cos = torch.min(cos, torch.autograd.Variable(torch.ones(batch).type_as(m)))\n    cos = torch.max(cos, torch.autograd.Variable(torch.ones(batch).type_as(m)) * -1)\n\n    theta = torch.acos(cos)\n\n    return theta", "\n\n# axis: size (batch, 3)\n# output quat order is w, x, y, z\ndef get_random_rotation_around_axis(axis, return_quaternion=False):\n    batch = axis.shape[0]\n    axis = normalize_vector(axis)  # batch*3\n    theta = torch.FloatTensor(axis.shape[0]).uniform_(-np.pi, np.pi).type_as(axis)  # [0, pi] #[-180, 180]\n    sin = torch.sin(theta)\n    qw = torch.cos(theta)\n    qx = axis[:, 0] * sin\n    qy = axis[:, 1] * sin\n    qz = axis[:, 2] * sin\n\n    quaternion = torch.cat((qw.view(batch, 1), qx.view(batch, 1), qy.view(batch, 1), qz.view(batch, 1)), 1)\n    matrix = quaternion_to_matrix(quaternion)\n\n    if (return_quaternion == True):\n        return matrix, quaternion\n    else:\n        return matrix", "\n\n# axisAngle batch*4 angle, x,y,z\n# output quat order is w, x, y, z\ndef get_random_rotation_matrices_around_random_axis(batch, return_quaternion=False):\n    axis = torch.autograd.Variable(torch.randn(batch.shape[0], 3).type_as(batch))\n    return get_random_rotation_around_axis(axis, return_quaternion=return_quaternion)\n\n\n# matrices batch*3*3", "\n# matrices batch*3*3\n# both matrix are orthogonal rotation matrices\n# out theta between 0 to 180 degree batch\ndef compute_geodesic_distance_from_two_matrices(m1, m2):\n    batch = m1.shape[0]\n    m = torch.bmm(m1, m2.transpose(1, 2))  # batch*3*3\n\n    cos = (m[:, 0, 0] + m[:, 1, 1] + m[:, 2, 2] - 1) / 2\n    cos = torch.min(cos, torch.autograd.Variable(torch.ones(batch).type_as(m1)))\n    cos = torch.max(cos, torch.autograd.Variable(torch.ones(batch).type_as(m1)) * -1)\n\n    theta = torch.acos(cos)\n\n    # theta = torch.min(theta, 2*np.pi - theta)\n\n    return theta", "\n\ndef geodesic_loss(gt_r_matrix, out_r_matrix):\n    theta = compute_geodesic_distance_from_two_matrices(gt_r_matrix, out_r_matrix)\n    error = theta.mean()\n    return error\n\n\ndef geodesic_loss_matrix3x3_matrix3x3(gt_r_matrix, out_r_matrix):\n    return geodesic_loss(gt_r_matrix, out_r_matrix)", "def geodesic_loss_matrix3x3_matrix3x3(gt_r_matrix, out_r_matrix):\n    return geodesic_loss(gt_r_matrix, out_r_matrix)\n\n\ndef geodesic_loss_quat_ortho6d(quaternions, ortho6d):\n\n    assert quaternions.shape[-1] == 4, \"quaternions should have the last dimension length be 4\"\n    assert ortho6d.shape[-1] == 6, \"ortho6d should have the last dimension length be 6\"\n\n    # quat -> matrix3x3\n    matrix3x3_a = quaternion_to_matrix(quaternions)\n\n    # ortho6d -> matrix3x3\n    matrix3x3_b = rotation_6d_to_matrix(ortho6d)\n\n    return geodesic_loss_matrix3x3_matrix3x3(matrix3x3_a, matrix3x3_b)", "\n\ndef rotation_6d_to_matrix_no_cross(d6: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    This is the pytorch3d implementation of the conversion,\n    but avoids the torch.cross() operator that cannot be exported with opset_version >= 11\n    \"\"\"\n    a1, a2 = d6[..., :3], d6[..., 3:]\n    b1 = F.normalize(a1, dim=-1)\n    b2 = a2 - (b1 * a2).sum(-1, keepdim=True) * b1\n    b2 = F.normalize(b2, dim=-1)\n    b3 = cross_product(b1, b2)\n    return torch.stack((b1, b2, b3), dim=-2)", ""]}
{"filename": "src/geometry/inverse_kinematics.py", "chunked_list": ["from typing import List\n\nimport torch\nimport numpy as np\nfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply, quaternion_invert\n\n# performs differentiable forward kinematics\n# local_transforms: local transform matrix of each transform (B, J, 4, 4)\n# level_transforms: a list of hierarchy levels, ordered by distance to the root transforms in the hierarchy\n#                   each elements of the list is a list transform indexes", "# level_transforms: a list of hierarchy levels, ordered by distance to the root transforms in the hierarchy\n#                   each elements of the list is a list transform indexes\n#                   for instance level_transforms[3] contains a list of indices of all the transforms that are 3 levels\n#                   below the root transforms the indices should match these of the provided local transforms\n#                   for instance if level_transforms[4, 1] is equal to 7, it means that local_transform[:, 7, :, :]\n#                   contains the local matrix of a transform that is 4 levels deeper than the root transform\n#                   (ie there are 4 transforms above it in the hierarchy)\n# level_transform_parents: similar to level_transforms but contains the parent indices\n#                   for instance if level_transform_parents[4, 1] is equal to 5, it means that local_transform[:, 5, :, :]\n#                   contains the local matrix of the parent transform of the transform contained in level_transforms[4, 1]", "#                   for instance if level_transform_parents[4, 1] is equal to 5, it means that local_transform[:, 5, :, :]\n#                   contains the local matrix of the parent transform of the transform contained in level_transforms[4, 1]\n# out: world transform matrix of each transform in the batch (B, J, 4, 4). Order is the same as input\n\n\ndef invert_transform_hierarchy(global_transforms: torch.Tensor, level_transforms: List[List[int]],\n                               level_transform_parents: List[List[int]]):\n    # used to store local transforms\n    local_transforms = global_transforms.clone()\n\n    # then process all children transforms\n    for level in range(len(level_transforms)-1, 0, -1):\n        parent_bone_indices = level_transform_parents[level]\n        local_bone_indices = level_transforms[level]\n        parent_level_transforms = global_transforms[..., parent_bone_indices, :, :]\n        local_level_transforms = global_transforms[..., local_bone_indices, :, :]\n        local_matrix = torch.matmul(torch.inverse(parent_level_transforms),\n                                    local_level_transforms)\n        local_transforms[..., local_bone_indices, :, :] = local_matrix.type_as(local_transforms)\n\n    return local_transforms", "\n\n# Compute a transform matrix combining a rotation and translation (rotation is applied first)\n# translations: translation vectors (N, 3)\n# rotations: rotation matrices (N, 3, 3)\n# out: transform matrix (N, 4, 4)\ndef get_transform_matrix(translations: torch.Tensor, rotations: torch.Tensor):    \n    out_shape = np.array(rotations.shape)\n    out_shape[-2:] = 4    \n    out = torch.zeros(list(out_shape)).type_as(rotations)\n    \n    out[..., 0:3, 0:3] = rotations\n    out[..., 0:3, 3] = translations\n    out[..., 3, 3] = 1.0\n    return out", "\n\n# performs differentiable inverse kinematics\n# global_rotations: global rotation matrices of each joint in the batch (B, J, 3, 3)\n# global_offset: local position offset of each joint in the batch (B, J, 3).\n#               This corresponds to the offset with respect to the parent joint when no rotation is applied\n# level_joints: a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n#               each elements of the list is a list transform indexes\n#               for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n#               below the root joints the indices should match these of the provided local rotations", "#               for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n#               below the root joints the indices should match these of the provided local rotations\n#               for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n#               contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n#               (ie there are 4 joints above it in the hierarchy)\n# level_joint_parents: similar to level_transforms but contains the parent indices\n#                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n#                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\ndef inverse_kinematics(global_rotations: torch.Tensor, global_offsets: torch.Tensor,\n                       level_joints: List[List[int]], level_joint_parents: List[List[int]]):\n\n    # compute global transformation matrix for each joints\n    global_transforms = get_transform_matrix(translations=global_offsets, rotations=global_rotations)\n    # used to store local transforms\n    local_transforms = global_transforms.clone()\n    \n    # then process all children transforms\n    for level in range(len(level_joints)-1, 0, -1):\n        parent_bone_indices = level_joint_parents[level]\n        local_bone_indices = level_joints[level]\n        parent_level_transforms = global_transforms[..., parent_bone_indices, :, :]\n        local_level_transforms = global_transforms[..., local_bone_indices, :, :]\n        local_matrix = torch.matmul(torch.inverse(parent_level_transforms),\n                                    local_level_transforms)\n        local_transforms[..., local_bone_indices, :, :] = local_matrix.type_as(local_transforms)\n\n    return local_transforms", "def inverse_kinematics(global_rotations: torch.Tensor, global_offsets: torch.Tensor,\n                       level_joints: List[List[int]], level_joint_parents: List[List[int]]):\n\n    # compute global transformation matrix for each joints\n    global_transforms = get_transform_matrix(translations=global_offsets, rotations=global_rotations)\n    # used to store local transforms\n    local_transforms = global_transforms.clone()\n    \n    # then process all children transforms\n    for level in range(len(level_joints)-1, 0, -1):\n        parent_bone_indices = level_joint_parents[level]\n        local_bone_indices = level_joints[level]\n        parent_level_transforms = global_transforms[..., parent_bone_indices, :, :]\n        local_level_transforms = global_transforms[..., local_bone_indices, :, :]\n        local_matrix = torch.matmul(torch.inverse(parent_level_transforms),\n                                    local_level_transforms)\n        local_transforms[..., local_bone_indices, :, :] = local_matrix.type_as(local_transforms)\n\n    return local_transforms", "\ndef inverse_kinematics_only_quats(joints_global_rotations: torch.Tensor, parents):\n    \"\"\"\n    Performs differentiable inverse kinematics with quaternion-based rotations\n    :param joints_global_rotations: tensor of global quaternions of shape (..., J, 4)\n    :param joints_global_positions: tensor of global positions of shape (..., J, 3)\n    :param a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n                           each elements of the list is a list transform indexes\n                           for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n                           below the root joints the indices should match these of the provided local rotations\n                           for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n                           contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n                           (ie there are 4 joints above it in the hierarchy)\n    :param level_joint_parents: similar to level_transforms but contains the parent indices\n                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n    :return:\n        joints_local_positions: tensor of local bone offsets of shape (..., J, 3)\n        joints_local_rotations: tensor of local quaternions of shape (..., J, 4)\n    \"\"\"\n\n\n    joint_local_quats = joints_global_rotations.clone()\n    invert_global_rotations = quaternion_invert(joint_local_quats)\n    joint_local_quats = torch.cat((\n        joint_local_quats[..., :1, :],\n        quaternion_multiply(invert_global_rotations[..., parents[1:], :], joint_local_quats[..., 1:, :]),\n    ), dim=-2)\n    return joint_local_quats", "\ndef inverse_kinematics_quats(joints_global_rotations: torch.Tensor,joints_global_positions, parents):\n    \"\"\"\n    Performs differentiable inverse kinematics with quaternion-based rotations\n    :param joints_global_rotations: tensor of global quaternions of shape (..., J, 4)\n    :param joints_global_positions: tensor of global positions of shape (..., J, 3)\n    :param a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n                           each elements of the list is a list transform indexes\n                           for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n                           below the root joints the indices should match these of the provided local rotations\n                           for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n                           contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n                           (ie there are 4 joints above it in the hierarchy)\n    :param level_joint_parents: similar to level_transforms but contains the parent indices\n                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n    :return:\n        joints_local_positions: tensor of local bone offsets of shape (..., J, 3)\n        joints_local_rotations: tensor of local quaternions of shape (..., J, 4)\n    \"\"\"\n\n\n    joint_local_quats = joints_global_rotations.clone()\n    invert_global_rotations = quaternion_invert(joint_local_quats)\n    joint_local_quats = torch.cat((\n        joint_local_quats[..., :1, :],\n        quaternion_multiply(invert_global_rotations[..., parents[1:], :], joint_local_quats[..., 1:, :]),\n    ), dim=-2)\n    joint_local_pos = torch.cat([joints_global_positions[...,:1,:],\n                                 quaternion_apply(invert_global_rotations[..., parents[1:], :],joints_global_positions[...,1:,:]-joints_global_positions[...,parents[1:],:])],dim=-2)\n    return joint_local_pos,joint_local_quats", "\n\nfrom src.geometry.vector import normalize_vector\nfrom src.geometry.quaternions import from_to_quaternion,quat_to_or6D\ndef scale_pos_to_bonelen(glb_pos:torch.Tensor,edge_len:torch.Tensor,level_joints: List[List[int]], level_joint_parents: List[List[int]]):\n    edge_len = edge_len.unsqueeze(1).expand(glb_pos.shape[:2]+(glb_pos.shape[2]-1,3))\n    _EPS32 = torch.finfo(torch.float32).eps\n    #for level in range(len(level_joints)-1,0,-1):\n    # import modifications\n    for level in range(1,len(level_joints)):\n        parent_bone_indices = level_joint_parents[level]\n        local_bone_indices = level_joints[level]\n        local_bone_indices_1 = [i-1 for i in local_bone_indices]\n        parent_pos = glb_pos[:,:,parent_bone_indices]\n        local_pos = glb_pos[:,:,local_bone_indices]\n        dir = local_pos - parent_pos\n        length = torch.norm(dir,dim=-1,keepdim=True)\n        length = torch.where(length<_EPS32,_EPS32,length)\n        local_pos = dir/length*edge_len[:,:,local_bone_indices_1]+parent_pos\n        glb_pos[:,:,local_bone_indices] = local_pos\n    return glb_pos", "def change_pos_to_rot(joints_global_rotations: torch.Tensor, joints_global_positions: torch.Tensor,offsets:torch.Tensor,secondary_offsets:torch.Tensor,parent:list):\n    \"\"\"\n    Performs differentiable inverse kinematics with quaternion-based rotations\n    :param joints_global_rotations: tensor of global quaternions of shape (..., J, 4)\n    :param joints_global_positions: tensor of global positions of shape (..., J, 3)\n    :param a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n                           each elements of the list is a list transform indexes\n                           for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n                           below the root joints the indices should match these of the provided local rotations\n                           for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n                           contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n                           (ie there are 4 joints above it in the hierarchy)\n    :param level_joint_parents: similar to level_transforms but contains the parent indices\n                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n    :return:\n        joints_local_positions: tensor of local bone offsets of shape (..., J, 3)\n        joints_local_rotations: tensor of local quaternions of shape (..., J, 4)\n    \"\"\"\n\n    #joint_local_positions = joints_global_positions.clone()\n    joint_local_quats = joints_global_rotations.clone()\n    joint_offset = joints_global_positions[...,1:,:] - joints_global_positions[...,parent[1:],:]\n    joint_offset = normalize_vector(joint_offset)\n    offsets = offsets[...,1:,:].unsqueeze(-3).expand(joint_offset.shape)\n    offsets = normalize_vector(offsets)\n    secondary_offsets = secondary_offsets[...,1:,:].unsqueeze(-3).expand(joint_offset.shape)\n    joints_global_rotations = from_to_quaternion(offsets,joint_offset)\n    # find plane P of joint offset\n    tangent = quaternion_apply(joints_global_rotations, secondary_offsets)\n    #co_tangent = normalize_vector(torch.cross(joint_offset,tangent))\n    # project the secondary offsets to plane\n    init_ore = quaternion_apply(joint_local_quats[...,parent[1:],:],secondary_offsets)\n\n    dot = lambda x,y: (x*y).sum(-1,keepdim=True)\n    init_ore = init_ore - dot(init_ore,joint_offset)*joint_offset\n    #now init_ore and tangent should be in plane P\n    rot = from_to_quaternion(tangent,init_ore)\n    joints_global_rotations = quaternion_multiply(rot,joints_global_rotations)\n\n    return joints_global_rotations", "\n\n\n"]}
{"filename": "src/geometry/quaternions.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\nfrom src.geometry.vector import normalize_vector\nimport numpy as np\nimport pytorch3d.transforms\ndef rotation_6d_to_matrix_no_normalized(d6: torch.Tensor) -> torch.Tensor:\n    a1, a2 = d6[..., :3], d6[..., 3:]\n    # b1 = F.normalize(a1, dim=-1)\n    # b2 = a2 - (b1 * a2).sum(-1, keepdim=True) * b1\n    # b2 = F.normalize(b2, dim=-1)\n    # b3 = torch.cross(b1, b2, dim=-1)\n    return torch.stack((a1, a2), dim=-2)", "\ndef rotation6d_multiply(r1,r2):\n    r1 = pytorch3d.transforms.rotation_6d_to_matrix(r1)\n    r2 = pytorch3d.transforms.rotation_6d_to_matrix(r2)\n    return pytorch3d.transforms.matrix_to_rotation_6d(torch.matmul(r1,r2))\ndef rotation6d_apply(r,p):\n    #p :...3\n    r = pytorch3d.transforms.rotation_6d_to_matrix(r)\n    p = p.unsqueeze(-1)#...3,1\n    return torch.matmul(r,p).squeeze(-1)\ndef rotation6d_inverse(r):\n    r = pytorch3d.transforms.rotation_6d_to_matrix(r)\n    inv_r = torch.transpose(r,-2,-1)\n    return pytorch3d.transforms.matrix_to_rotation_6d(inv_r)", "def rotation6d_inverse(r):\n    r = pytorch3d.transforms.rotation_6d_to_matrix(r)\n    inv_r = torch.transpose(r,-2,-1)\n    return pytorch3d.transforms.matrix_to_rotation_6d(inv_r)\ndef quat_to_or6D(quat):\n\n    assert(quat.shape[-1]==4)\n    return pytorch3d.transforms.matrix_to_rotation_6d(pytorch3d.transforms.quaternion_to_matrix(quat))\ndef or6d_to_quat(mat):\n    assert (mat.shape[-1] == 6)\n    return pytorch3d.transforms.matrix_to_quaternion(pytorch3d.transforms.rotation_6d_to_matrix(mat))", "def or6d_to_quat(mat):\n    assert (mat.shape[-1] == 6)\n    return pytorch3d.transforms.matrix_to_quaternion(pytorch3d.transforms.rotation_6d_to_matrix(mat))\ndef normalized_or6d(d6):\n    a1, a2 = d6[..., :3], d6[..., 3:]\n    b1 = F.normalize(a1, dim=-1)\n    b2 = a2 - (b1 * a2).sum(-1, keepdim=True) * b1\n    b2 = F.normalize(b2, dim=-1)\n    return torch.cat((b1, b2), dim=-1)\n#\u79fb\u9664\u56db\u5143\u6570\u4e0d\u8fde\u7eed\u7684\u95ee\u9898\ndef remove_quat_discontinuities(rotations):\n    rotations = rotations.clone()\n    rots_inv = -rotations\n    for i in range(1, rotations.shape[1]):\n        replace_mask = torch.sum(rotations[:, i-1:i, ...] * rotations[:, i:i+1, ...], \n                                 dim=-1, keepdim=True) < \\\n                       torch.sum(rotations[:, i-1:i, ...] * rots_inv[:, i:i+1, ...], \n                                 dim=-1, keepdim=True)\n        replace_mask = replace_mask.squeeze(1).type_as(rotations)\n        rotations[:, i, ...] = replace_mask * rots_inv[:, i, ...] + (1.0 - replace_mask) * rotations[:, i, ...]\n    return rotations", "#\u79fb\u9664\u56db\u5143\u6570\u4e0d\u8fde\u7eed\u7684\u95ee\u9898\ndef remove_quat_discontinuities(rotations):\n    rotations = rotations.clone()\n    rots_inv = -rotations\n    for i in range(1, rotations.shape[1]):\n        replace_mask = torch.sum(rotations[:, i-1:i, ...] * rotations[:, i:i+1, ...], \n                                 dim=-1, keepdim=True) < \\\n                       torch.sum(rotations[:, i-1:i, ...] * rots_inv[:, i:i+1, ...], \n                                 dim=-1, keepdim=True)\n        replace_mask = replace_mask.squeeze(1).type_as(rotations)\n        rotations[:, i, ...] = replace_mask * rots_inv[:, i, ...] + (1.0 - replace_mask) * rotations[:, i, ...]\n    return rotations", "\ndef from_to_1_0_0(v_from):\n    '''v_from: B,3'''\n    v_to_unit = torch.tensor(np.array([[1, 0, 0]]), dtype=v_from.dtype, device=v_from.device).expand(v_from.shape)\n    y = torch.tensor(np.array([[0,1,0]]),dtype=v_from.dtype,device=v_from.device).expand(v_from.shape)\n    z_to_unit = torch.tensor(np.array([[0,0,1]]),dtype=v_from.dtype,device=v_from.device).expand(v_from.shape)\n\n    v_from_unit = normalize_vector(v_from)\n    z_from_unit = normalize_vector(torch.cross(v_from_unit,y,dim=-1))\n    to_transform = torch.stack([v_to_unit,y,z_to_unit],dim=-1)\n    from_transform = torch.stack([v_from_unit,y,z_from_unit],dim=-1)\n    shape = to_transform.shape\n    to_transform = to_transform.view(-1,3,3)\n    from_transform = from_transform.view(-1,3,3)\n    r = torch.matmul(to_transform,from_transform.transpose(1,2)).view(shape)\n    rq = pytorch3d.transforms.matrix_to_quaternion(r)\n\n    # w = (v_from_unit * v_to_unit).sum(dim=1) + 1\n    # '''can't cross if two directions are exactly inverse'''\n    # xyz = torch.cross(v_from_unit, v_to_unit, dim=1)\n    # '''if exactly inverse, the rotation should be (0,0,1,0), around yaxis 180'''\n    # xyz[...,1] = torch.where(w==0,torch.tensor([1],dtype=v_from.dtype,device=xyz.device),xyz[...,1])\n    # q = torch.cat([w.unsqueeze(1), xyz], dim=1)\n    return rq", "\n\n# returns quaternion so that v_from rotated by this quaternion equals v_to\n# v_... are vectors of size (..., 3)\n# returns quaternion in w, x, y, z order, of size (..., 4)\n# note: such a rotation is not unique, there is an infinite number of solutions\n# this implementation returns the shortest arc\ndef from_to_quaternion(v_from, v_to):\n    v_from_unit = normalize_vector(v_from)\n    v_to_unit = normalize_vector(v_to)\n\n    w = (v_from_unit * v_to_unit).sum(dim=-1)+1\n    '''can't cross if two directions are exactly inverse'''\n    xyz = torch.cross(v_from_unit, v_to_unit, dim=-1)\n\n\n    q = torch.cat([w.unsqueeze(-1), xyz], dim=-1)\n    return normalize_vector(q)", "\ndef quat_inv(quat):\n    return pytorch3d.transforms.quaternion_invert(quat)\ndef quat_mul(quat0,quat1):\n    return pytorch3d.transforms.quaternion_multiply(quat0,quat1)\ndef quat_mul_vec(quat,vec):\n    return pytorch3d.transforms.quaternion_apply(quat,vec)\ndef slerp(q0, q1, t):\n    \"\"\"\n    Spherical Linear Interpolation of quaternions\n    https://www.euclideanspace.com/maths/algebra/realNormedAlgebra/quaternions/slerp/index.htm\n    :param q0: Start quats (w, x, y, z) : shape = (B, J, 4)\n    :param q1: End quats (w, x, y, z) : shape = (B, J, 4)\n    :param t:  Step (in [0, 1]) : shape = (B, J, 1)\n    :return: Interpolated quat (w, x, y, z) : shape = (B, J, 4)\n    \"\"\"\n  #  q0 = q0.unsqueeze(1)\n  #  q1 = q1.unsqueeze(1)\n    \n    # Dot product\n    q = q0*q1\n    cos_half_theta = torch.sum(q, dim=-1, keepdim=True)\n   # t = t.view(1,-1,1,1)\n    # Make sure we take the shortest path :\n    q1_antipodal = -q1\n    q1 = torch.where(cos_half_theta < 0, q1_antipodal, q1)\n    cos_half_theta = torch.where(cos_half_theta < 0,-cos_half_theta,cos_half_theta)\n    half_theta = torch.acos(cos_half_theta)\n    # torch.sin must be safer here\n    sin_half_theta = torch.sqrt(1.0 - cos_half_theta * cos_half_theta)\n    ratio_a = torch.sin((1 - t) * half_theta) / sin_half_theta\n    ratio_b = torch.sin(t * half_theta) / sin_half_theta\n    \n    qt = ratio_a * q0 + ratio_b * q1    \n    # If the angle was constant, prevent nans by picking the original quat:\n    qt = torch.where(torch.abs(cos_half_theta) >= 1.0-1e-8, q0, qt)\n    return qt", ""]}
{"filename": "src/geometry/__init__.py", "chunked_list": [""]}
{"filename": "src/geometry/vector.py", "chunked_list": ["import torch\nimport numpy as np\n_EPS64 = torch.finfo(torch.float64).eps\n_4EPS64 = _EPS64 * 4.0\n_EPS32 = torch.finfo(torch.float32).eps\n_4EPS32 = _EPS32 * 4.0\n# batch*n\ndef find_secondary_axis(v):\n    v = normalize_vector(v)\n    refx = torch.tensor(np.array([[1, 0, 0]]), dtype=v.dtype, device=v.device).expand(v.shape)\n    refy =  torch.tensor(np.array([[0, 1, 0]]), dtype=v.dtype, device=v.device).expand(v.shape)\n    dot = lambda x,y:(x*y).sum(-1,keepdim=True)\n    refxv,refyv = dot(v,refx).abs(),dot(v,refy).abs()    # find proper axis\n    ref = torch.where(refxv>refyv,refy,refx)\n    pred = torch.cross(v,ref,dim=-1)\n    pred = normalize_vector(pred)\n    return pred", "\ndef normalize_vector(v, return_mag=False):\n    if(v.dtype==torch.float64):\n        eps = _EPS64\n    else:\n        eps = _EPS32\n    v_mag = torch.norm(v,dim=-1,keepdim=True)# torch.sqrt(v.pow(2).sum(-1,keepdim=True))#+eps\n\n    #v_mag = torch.max(v_mag, torch.autograd.Variable(torch.FloatTensor([eps]).type_as(v)))\n    v = v / v_mag\n    if return_mag:\n        return v, v_mag[:, 0]\n    else:\n        return v", "\n\ndef cross_product(u, v):\n    \"\"\"\n    Cross operation on batched vectors of shape (..., 3)\n    \"\"\"\n    i = u[..., 1] * v[..., 2] - u[..., 2] * v[..., 1]\n    j = u[..., 2] * v[..., 0] - u[..., 0] * v[..., 2]\n    k = u[..., 0] * v[..., 1] - u[..., 1] * v[..., 0]\n    out = torch.cat((i.unsqueeze(-1), j.unsqueeze(-1), k.unsqueeze(-1)), dim=-1)\n    return out", ""]}
{"filename": "src/geometry/forward_kinematics.py", "chunked_list": ["from typing import List\n\nimport torch\nfrom pytorch3d.transforms import quaternion_multiply, quaternion_apply\n\ndef forward_transform_hierarchy(local_transforms: torch.Tensor, level_transforms: List[List[int]],\n                                level_transform_parents: List[List[int]]):\n    # used to store world transforms\n    world_transforms = torch.zeros_like(local_transforms)\n\n    # initialize root transforms\n    world_transforms[:, level_transforms[0]] = local_transforms[:, level_transforms[0], :, :]\n\n    # then process all children transforms\n    for level in range(1, len(level_transforms)):\n        parent_bone_indices = level_transform_parents[level]\n        local_bone_indices = level_transforms[level]\n        parent_level_transforms = world_transforms[:, parent_bone_indices]\n        local_level_transforms = local_transforms[:, local_bone_indices]\n        global_matrix = torch.matmul(parent_level_transforms, local_level_transforms)\n        world_transforms[:, local_bone_indices] = global_matrix.type_as(world_transforms)\n\n    return world_transforms", "\n\n# Compute a transform matrix combining a rotation and translation (rotation is applied first)\n# translations: translation vectors (N, 3)\n# rotations: rotation matrices (N, 3, 3)\n# out: transform matrix (N, 4, 4)\ndef get_transform_matrix(translations: torch.Tensor, rotations: torch.Tensor):\n    batch_size = rotations.shape[0]\n\n    out = torch.zeros((batch_size, 4, 4)).type_as(rotations)\n    out[:, 0:3, 0:3] = rotations\n    out[:, 0:3, 3] = translations\n    out[:, 3, 3] = 1.0\n\n    return out", "\n\n\n\ndef forward_kinematics(local_rotations: torch.Tensor, local_offsets: torch.Tensor,\n                       level_joints: List[List[int]], level_joint_parents: List[List[int]]):\n    batch_size = local_rotations.shape[0]\n    joints_nbr = local_rotations.shape[1]\n\n    # compute local transformation matrix for each joints\n    local_transforms = get_transform_matrix(translations=local_offsets.view(-1, 3), rotations=local_rotations.view(-1, 3, 3))\n    local_transforms = local_transforms.view(batch_size, joints_nbr, 4, 4)\n\n    return forward_transform_hierarchy(local_transforms, level_joints, level_joint_parents)", "def forward_kinematics_quats(local_rotations: torch.Tensor, local_offsets: torch.Tensor, hip_positions:torch.Tensor,\n                       parents):\n    \"\"\"\n    Performs FK to retrieve global positions and rotations in quaternion format\n    :param local_rotations: tensor of local rotations of shape (..., J, 4)\n    :param local_offsets: tensor of local bone offsets of shape (..., J, 3)\n    :param level_joints:  a list of hierarchy levels, ordered by distance to the root joints in the hierarchy\n                           each elements of the list is a list transform indexes\n                           for instance level_joints[3] contains a list of indices of all the joints that are 3 levels\n                           below the root joints the indices should match these of the provided local rotations\n                           for instance if level_joints[4, 1] is equal to 7, it means that local_rotations[:, 7, :, :]\n                           contains the local rotation matrix of a joint that is 4 levels deeper than the root joint\n                           (ie there are 4 joints above it in the hierarchy)\n    :param level_joint_parents: similar to level_transforms but contains the parent indices\n                      for instance if level_joint_parents[4, 1] is equal to 5, it means that local_rotations[:, 5, :, :]\n                      contains the local rotation matrix of the parent joint of the joint contained in level_joints[4, 1]\n    :return:\n        global_positions : tensor of global bone positions of shape (..., J, 3)\n        global_quats     : tensor of global bone quaternions of shape (..., J, 4)\n    \"\"\"\n\n    quat_shape = list(local_rotations.shape)\n    pos_shape = quat_shape.copy()\n    pos_shape[-1] = 3\n    global_quats = local_rotations.clone()\n    local_offsets = local_offsets.unsqueeze(-3).expand(pos_shape)\n    global_positions = [hip_positions]\n    global_quats = [global_quats[...,:1,:]]\n    #global_positions[...,0:1,:] = hip_positions.clone()\n    for level in range(1, len(parents)):\n\n        global_positions.append( quaternion_apply(global_quats[parents[level]], local_offsets[..., level:level+1, :]) + global_positions[parents[level]])\n        global_quats.append( quaternion_multiply(global_quats[parents[level]], local_rotations[..., level:level+1, :]))\n    global_positions = torch.cat(global_positions,dim=-2)\n    global_quats = torch.cat(global_quats,dim=-2)\n    return global_positions, global_quats", "\n"]}
