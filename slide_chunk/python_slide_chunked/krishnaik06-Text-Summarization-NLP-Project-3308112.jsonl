{"filename": "setup.py", "chunked_list": ["import setuptools\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as f:\n    long_description = f.read()\n\n\n__version__ = \"0.0.0\"\n\nREPO_NAME = \"Text-Summarizer-Project\"\nAUTHOR_USER_NAME = \"entbappy\"", "REPO_NAME = \"Text-Summarizer-Project\"\nAUTHOR_USER_NAME = \"entbappy\"\nSRC_REPO = \"textSummarizer\"\nAUTHOR_EMAIL = \"entbappy73@gmail.com\"\n\n\n\nsetuptools.setup(\n    name=SRC_REPO,\n    version=__version__,", "    name=SRC_REPO,\n    version=__version__,\n    author=AUTHOR_USER_NAME,\n    author_email=AUTHOR_EMAIL,\n    description=\"A small python package for NLP app\",\n    long_description=long_description,\n    long_description_content=\"text/markdown\",\n    url=f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}\",\n    project_urls={\n        \"Bug Tracker\": f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}/issues\",", "    project_urls={\n        \"Bug Tracker\": f\"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}/issues\",\n    },\n    package_dir={\"\": \"src\"},\n    packages=setuptools.find_packages(where=\"src\")\n)"]}
{"filename": "main.py", "chunked_list": ["from textSummarizer.pipeline.stage_01_data_ingestion import DataIngestionTrainingPipeline\nfrom textSummarizer.pipeline.stage_02_data_validation import DataValidationTrainingPipeline\nfrom textSummarizer.pipeline.stage_03_data_transformation import DataTransformationTrainingPipeline\nfrom textSummarizer.pipeline.stage_04_model_trainer import ModelTrainerTrainingPipeline\nfrom textSummarizer.pipeline.stage_05_model_evaluation import ModelEvaluationTrainingPipeline\nfrom textSummarizer.logging import logger\n\n\nSTAGE_NAME = \"Data Ingestion stage\"\ntry:\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \n   data_ingestion = DataIngestionTrainingPipeline()\n   data_ingestion.main()\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\nexcept Exception as e:\n        logger.exception(e)\n        raise e", "STAGE_NAME = \"Data Ingestion stage\"\ntry:\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \n   data_ingestion = DataIngestionTrainingPipeline()\n   data_ingestion.main()\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\nexcept Exception as e:\n        logger.exception(e)\n        raise e\n", "\n\n\n\nSTAGE_NAME = \"Data Validation stage\"\ntry:\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \n   data_validation = DataValidationTrainingPipeline()\n   data_validation.main()\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\nexcept Exception as e:\n        logger.exception(e)\n        raise e", "\n\n\nSTAGE_NAME = \"Data Transformation stage\"\ntry:\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\") \n   data_transformation = DataTransformationTrainingPipeline()\n   data_transformation.main()\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\nexcept Exception as e:\n        logger.exception(e)\n        raise e", "\n\n\nSTAGE_NAME = \"Model Trainer stage\"\ntry: \n   logger.info(f\"*******************\")\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n   model_trainer = ModelTrainerTrainingPipeline()\n   model_trainer.main()\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\nexcept Exception as e:\n        logger.exception(e)\n        raise e", "\n\n\n\nSTAGE_NAME = \"Model Evaluation stage\"\ntry: \n   logger.info(f\"*******************\")\n   logger.info(f\">>>>>> stage {STAGE_NAME} started <<<<<<\")\n   model_evaluation = ModelEvaluationTrainingPipeline()\n   model_evaluation.main()\n   logger.info(f\">>>>>> stage {STAGE_NAME} completed <<<<<<\\n\\nx==========x\")\nexcept Exception as e:\n        logger.exception(e)\n        raise e", "\n\n\n\n\n"]}
{"filename": "app.py", "chunked_list": ["from fastapi import FastAPI\nimport uvicorn\nimport sys\nimport os\nfrom fastapi.templating import Jinja2Templates\nfrom starlette.responses import RedirectResponse\nfrom fastapi.responses import Response\nfrom textSummarizer.pipeline.prediction import PredictionPipeline\n\n", "\n\ntext:str = \"What is Text Summarization?\"\n\napp = FastAPI()\n\n@app.get(\"/\", tags=[\"authentication\"])\nasync def index():\n    return RedirectResponse(url=\"/docs\")\n", "    return RedirectResponse(url=\"/docs\")\n\n\n\n@app.get(\"/train\")\nasync def training():\n    try:\n        os.system(\"python main.py\")\n        return Response(\"Training successful !!\")\n\n    except Exception as e:\n        return Response(f\"Error Occurred! {e}\")", "    \n\n\n\n@app.post(\"/predict\")\nasync def predict_route(text):\n    try:\n\n        obj = PredictionPipeline()\n        text = obj.predict(text)\n        return text\n    except Exception as e:\n        raise e", "    \n\nif __name__==\"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n"]}
{"filename": "template.py", "chunked_list": ["import os\nfrom pathlib import Path\nimport logging\n\nlogging.basicConfig(level=logging.INFO, format='[%(asctime)s]: %(message)s:')\n\n\nproject_name = \"textSummarizer\"\n\nlist_of_files = [", "\nlist_of_files = [\n    \".github/workflows/.gitkeep\",\n    f\"src/{project_name}/__init__.py\",\n    f\"src/{project_name}/conponents/__init__.py\",\n    f\"src/{project_name}/utils/__init__.py\",\n    f\"src/{project_name}/utils/common.py\",\n    f\"src/{project_name}/logging/__init__.py\",\n    f\"src/{project_name}/config/__init__.py\",\n    f\"src/{project_name}/config/configuration.py\",", "    f\"src/{project_name}/config/__init__.py\",\n    f\"src/{project_name}/config/configuration.py\",\n    f\"src/{project_name}/pipeline/__init__.py\",\n    f\"src/{project_name}/entity/__init__.py\",\n    f\"src/{project_name}/constants/__init__.py\",\n    \"config/config.yaml\",\n    \"params.yaml\",\n    \"app.py\",\n    \"main.py\",\n    \"Dockerfile\",", "    \"main.py\",\n    \"Dockerfile\",\n    \"requirements.txt\",\n    \"setup.py\",\n    \"research/trials.ipynb\",\n\n]\n\n\nfor filepath in list_of_files:\n    filepath = Path(filepath)\n    filedir, filename = os.path.split(filepath)\n\n    if filedir != \"\":\n        os.makedirs(filedir, exist_ok=True)\n        logging.info(f\"Creating directory:{filedir} for the file {filename}\")\n\n    \n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n        with open(filepath,'w') as f:\n            pass\n            logging.info(f\"Creating empty file: {filepath}\")\n\n\n    \n    else:\n        logging.info(f\"{filename} is already exists\")", "\nfor filepath in list_of_files:\n    filepath = Path(filepath)\n    filedir, filename = os.path.split(filepath)\n\n    if filedir != \"\":\n        os.makedirs(filedir, exist_ok=True)\n        logging.info(f\"Creating directory:{filedir} for the file {filename}\")\n\n    \n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\n        with open(filepath,'w') as f:\n            pass\n            logging.info(f\"Creating empty file: {filepath}\")\n\n\n    \n    else:\n        logging.info(f\"{filename} is already exists\")", "\n"]}
{"filename": "test.py", "chunked_list": [""]}
{"filename": "src/textSummarizer/__init__.py", "chunked_list": [""]}
{"filename": "src/textSummarizer/entity/__init__.py", "chunked_list": ["from dataclasses import dataclass\nfrom pathlib import Path\n\n@dataclass(frozen=True)\nclass DataIngestionConfig:\n    root_dir: Path\n    source_URL: str\n    local_data_file: Path\n    unzip_dir: Path\n", "\n\n\n@dataclass(frozen=True)\nclass DataValidationConfig:\n    root_dir: Path\n    STATUS_FILE: str\n    ALL_REQUIRED_FILES: list\n\n", "\n\n\n@dataclass(frozen=True)\nclass DataTransformationConfig:\n    root_dir: Path\n    data_path: Path\n    tokenizer_name: Path\n\n", "\n\n\n@dataclass(frozen=True)\nclass ModelTrainerConfig:\n    root_dir: Path\n    data_path: Path\n    model_ckpt: Path\n    num_train_epochs: int\n    warmup_steps: int\n    per_device_train_batch_size: int\n    weight_decay: float\n    logging_steps: int\n    evaluation_strategy: str\n    eval_steps: int\n    save_steps: float\n    gradient_accumulation_steps: int", "\n\n\n@dataclass(frozen=True)\nclass ModelEvaluationConfig:\n    root_dir: Path\n    data_path: Path\n    model_path: Path\n    tokenizer_path: Path\n    metric_file_name: Path"]}
{"filename": "src/textSummarizer/utils/__init__.py", "chunked_list": [""]}
{"filename": "src/textSummarizer/utils/common.py", "chunked_list": ["import os\nfrom box.exceptions import BoxValueError\nimport yaml\nfrom textSummarizer.logging import logger\nfrom ensure import ensure_annotations\nfrom box import ConfigBox\nfrom pathlib import Path\nfrom typing import Any\n\n", "\n\n\n@ensure_annotations\ndef read_yaml(path_to_yaml: Path) -> ConfigBox:\n    \"\"\"reads yaml file and returns\n\n    Args:\n        path_to_yaml (str): path like input\n\n    Raises:\n        ValueError: if yaml file is empty\n        e: empty file\n\n    Returns:\n        ConfigBox: ConfigBox type\n    \"\"\"\n    try:\n        with open(path_to_yaml) as yaml_file:\n            content = yaml.safe_load(yaml_file)\n            logger.info(f\"yaml file: {path_to_yaml} loaded successfully\")\n            return ConfigBox(content)\n    except BoxValueError:\n        raise ValueError(\"yaml file is empty\")\n    except Exception as e:\n        raise e", "    \n\n\n@ensure_annotations\ndef create_directories(path_to_directories: list, verbose=True):\n    \"\"\"create list of directories\n\n    Args:\n        path_to_directories (list): list of path of directories\n        ignore_log (bool, optional): ignore if multiple dirs is to be created. Defaults to False.\n    \"\"\"\n    for path in path_to_directories:\n        os.makedirs(path, exist_ok=True)\n        if verbose:\n            logger.info(f\"created directory at: {path}\")", "\n\n\n@ensure_annotations\ndef get_size(path: Path) -> str:\n    \"\"\"get size in KB\n\n    Args:\n        path (Path): path of the file\n\n    Returns:\n        str: size in KB\n    \"\"\"\n    size_in_kb = round(os.path.getsize(path)/1024)\n    return f\"~ {size_in_kb} KB\"", "\n    \n"]}
{"filename": "src/textSummarizer/config/__init__.py", "chunked_list": [""]}
{"filename": "src/textSummarizer/config/configuration.py", "chunked_list": ["from textSummarizer.constants import *\nfrom textSummarizer.utils.common import read_yaml, create_directories\nfrom textSummarizer.entity import (DataIngestionConfig,\n                                   DataValidationConfig,\n                                   DataTransformationConfig,\n                                   ModelTrainerConfig,\n                                   ModelEvaluationConfig)\n\n\nclass ConfigurationManager:\n    def __init__(\n        self,\n        config_filepath = CONFIG_FILE_PATH,\n        params_filepath = PARAMS_FILE_PATH):\n\n        self.config = read_yaml(config_filepath)\n        self.params = read_yaml(params_filepath)\n\n        create_directories([self.config.artifacts_root])\n\n    \n\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\n        config = self.config.data_ingestion\n\n        create_directories([config.root_dir])\n\n        data_ingestion_config = DataIngestionConfig(\n            root_dir=config.root_dir,\n            source_URL=config.source_URL,\n            local_data_file=config.local_data_file,\n            unzip_dir=config.unzip_dir \n        )\n\n        return data_ingestion_config\n    \n\n\n    def get_data_validation_config(self) -> DataValidationConfig:\n        config = self.config.data_validation\n\n        create_directories([config.root_dir])\n\n        data_validation_config = DataValidationConfig(\n            root_dir=config.root_dir,\n            STATUS_FILE=config.STATUS_FILE,\n            ALL_REQUIRED_FILES=config.ALL_REQUIRED_FILES,\n        )\n\n        return data_validation_config\n    \n\n    def get_data_transformation_config(self) -> DataTransformationConfig:\n        config = self.config.data_transformation\n\n        create_directories([config.root_dir])\n\n        data_transformation_config = DataTransformationConfig(\n            root_dir=config.root_dir,\n            data_path=config.data_path,\n            tokenizer_name = config.tokenizer_name\n        )\n\n        return data_transformation_config\n    \n\n\n    def get_model_trainer_config(self) -> ModelTrainerConfig:\n        config = self.config.model_trainer\n        params = self.params.TrainingArguments\n\n        create_directories([config.root_dir])\n\n        model_trainer_config = ModelTrainerConfig(\n            root_dir=config.root_dir,\n            data_path=config.data_path,\n            model_ckpt = config.model_ckpt,\n            num_train_epochs = params.num_train_epochs,\n            warmup_steps = params.warmup_steps,\n            per_device_train_batch_size = params.per_device_train_batch_size,\n            weight_decay = params.weight_decay,\n            logging_steps = params.logging_steps,\n            evaluation_strategy = params.evaluation_strategy,\n            eval_steps = params.evaluation_strategy,\n            save_steps = params.save_steps,\n            gradient_accumulation_steps = params.gradient_accumulation_steps\n        )\n\n        return model_trainer_config\n    \n\n    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n        config = self.config.model_evaluation\n\n        create_directories([config.root_dir])\n\n        model_evaluation_config = ModelEvaluationConfig(\n            root_dir=config.root_dir,\n            data_path=config.data_path,\n            model_path = config.model_path,\n            tokenizer_path = config.tokenizer_path,\n            metric_file_name = config.metric_file_name\n           \n        )\n\n        return model_evaluation_config", "\nclass ConfigurationManager:\n    def __init__(\n        self,\n        config_filepath = CONFIG_FILE_PATH,\n        params_filepath = PARAMS_FILE_PATH):\n\n        self.config = read_yaml(config_filepath)\n        self.params = read_yaml(params_filepath)\n\n        create_directories([self.config.artifacts_root])\n\n    \n\n    def get_data_ingestion_config(self) -> DataIngestionConfig:\n        config = self.config.data_ingestion\n\n        create_directories([config.root_dir])\n\n        data_ingestion_config = DataIngestionConfig(\n            root_dir=config.root_dir,\n            source_URL=config.source_URL,\n            local_data_file=config.local_data_file,\n            unzip_dir=config.unzip_dir \n        )\n\n        return data_ingestion_config\n    \n\n\n    def get_data_validation_config(self) -> DataValidationConfig:\n        config = self.config.data_validation\n\n        create_directories([config.root_dir])\n\n        data_validation_config = DataValidationConfig(\n            root_dir=config.root_dir,\n            STATUS_FILE=config.STATUS_FILE,\n            ALL_REQUIRED_FILES=config.ALL_REQUIRED_FILES,\n        )\n\n        return data_validation_config\n    \n\n    def get_data_transformation_config(self) -> DataTransformationConfig:\n        config = self.config.data_transformation\n\n        create_directories([config.root_dir])\n\n        data_transformation_config = DataTransformationConfig(\n            root_dir=config.root_dir,\n            data_path=config.data_path,\n            tokenizer_name = config.tokenizer_name\n        )\n\n        return data_transformation_config\n    \n\n\n    def get_model_trainer_config(self) -> ModelTrainerConfig:\n        config = self.config.model_trainer\n        params = self.params.TrainingArguments\n\n        create_directories([config.root_dir])\n\n        model_trainer_config = ModelTrainerConfig(\n            root_dir=config.root_dir,\n            data_path=config.data_path,\n            model_ckpt = config.model_ckpt,\n            num_train_epochs = params.num_train_epochs,\n            warmup_steps = params.warmup_steps,\n            per_device_train_batch_size = params.per_device_train_batch_size,\n            weight_decay = params.weight_decay,\n            logging_steps = params.logging_steps,\n            evaluation_strategy = params.evaluation_strategy,\n            eval_steps = params.evaluation_strategy,\n            save_steps = params.save_steps,\n            gradient_accumulation_steps = params.gradient_accumulation_steps\n        )\n\n        return model_trainer_config\n    \n\n    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n        config = self.config.model_evaluation\n\n        create_directories([config.root_dir])\n\n        model_evaluation_config = ModelEvaluationConfig(\n            root_dir=config.root_dir,\n            data_path=config.data_path,\n            model_path = config.model_path,\n            tokenizer_path = config.tokenizer_path,\n            metric_file_name = config.metric_file_name\n           \n        )\n\n        return model_evaluation_config", ""]}
{"filename": "src/textSummarizer/logging/__init__.py", "chunked_list": ["import os\nimport sys\nimport logging\n\nlogging_str = \"[%(asctime)s: %(levelname)s: %(module)s: %(message)s]\"\nlog_dir = \"logs\"\nlog_filepath = os.path.join(log_dir,\"running_logs.log\")\nos.makedirs(log_dir, exist_ok=True)\n\n", "\n\n\nlogging.basicConfig(\n    level= logging.INFO,\n    format= logging_str,\n\n    handlers=[\n        logging.FileHandler(log_filepath),\n        logging.StreamHandler(sys.stdout)", "        logging.FileHandler(log_filepath),\n        logging.StreamHandler(sys.stdout)\n    ]\n)\n\nlogger = logging.getLogger(\"textSummarizerLogger\")"]}
{"filename": "src/textSummarizer/pipeline/stage_03_data_transformation.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\nfrom textSummarizer.conponents.data_transformation import DataTransformation\nfrom textSummarizer.logging import logger\n\n\nclass DataTransformationTrainingPipeline:\n    def __init__(self):\n        pass\n\n    def main(self):\n        config = ConfigurationManager()\n        data_transformation_config = config.get_data_transformation_config()\n        data_transformation = DataTransformation(config=data_transformation_config)\n        data_transformation.convert()"]}
{"filename": "src/textSummarizer/pipeline/stage_04_model_trainer.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\nfrom textSummarizer.conponents.model_trainer import ModelTrainer\nfrom textSummarizer.logging import logger\n\n\nclass ModelTrainerTrainingPipeline:\n    def __init__(self):\n        pass\n\n    def main(self):\n        config = ConfigurationManager()\n        model_trainer_config = config.get_model_trainer_config()\n        model_trainer_config = ModelTrainer(config=model_trainer_config)\n        model_trainer_config.train()"]}
{"filename": "src/textSummarizer/pipeline/prediction.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline\n\n\nclass PredictionPipeline:\n    def __init__(self):\n        self.config = ConfigurationManager().get_model_evaluation_config()\n\n\n    \n    def predict(self,text):\n        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n        gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n\n        pipe = pipeline(\"summarization\", model=self.config.model_path,tokenizer=tokenizer)\n\n        print(\"Dialogue:\")\n        print(text)\n\n        output = pipe(text, **gen_kwargs)[0][\"summary_text\"]\n        print(\"\\nModel Summary:\")\n        print(output)\n\n        return output"]}
{"filename": "src/textSummarizer/pipeline/__init__.py", "chunked_list": [""]}
{"filename": "src/textSummarizer/pipeline/stage_01_data_ingestion.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\nfrom textSummarizer.conponents.data_ingestion import DataIngestion\nfrom textSummarizer.logging import logger\n\n\nclass DataIngestionTrainingPipeline:\n    def __init__(self):\n        pass\n\n    def main(self):\n        config = ConfigurationManager()\n        data_ingestion_config = config.get_data_ingestion_config()\n        data_ingestion = DataIngestion(config=data_ingestion_config)\n        data_ingestion.download_file()\n        data_ingestion.extract_zip_file()", ""]}
{"filename": "src/textSummarizer/pipeline/stage_02_data_validation.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\nfrom textSummarizer.conponents.data_validation import DataValiadtion\nfrom textSummarizer.logging import logger\n\n\nclass DataValidationTrainingPipeline:\n    def __init__(self):\n        pass\n\n    def main(self):\n        config = ConfigurationManager()\n        data_validation_config = config.get_data_validation_config()\n        data_validation = DataValiadtion(config=data_validation_config)\n        data_validation.validate_all_files_exist()"]}
{"filename": "src/textSummarizer/pipeline/stage_05_model_evaluation.py", "chunked_list": ["from textSummarizer.config.configuration import ConfigurationManager\nfrom textSummarizer.conponents.model_evaluation import ModelEvaluation\nfrom textSummarizer.logging import logger\n\n\n\n\nclass ModelEvaluationTrainingPipeline:\n    def __init__(self):\n        pass\n\n    def main(self):\n        config = ConfigurationManager()\n        model_evaluation_config = config.get_model_evaluation_config()\n        model_evaluation_config = ModelEvaluation(config=model_evaluation_config)\n        model_evaluation_config.evaluate()"]}
{"filename": "src/textSummarizer/constants/__init__.py", "chunked_list": ["from pathlib import Path\n\nCONFIG_FILE_PATH = Path(\"config/config.yaml\")\nPARAMS_FILE_PATH = Path(\"params.yaml\")"]}
{"filename": "src/textSummarizer/conponents/data_validation.py", "chunked_list": ["import os\nfrom textSummarizer.logging import logger\nfrom textSummarizer.entity import DataValidationConfig\n\nclass DataValiadtion:\n    def __init__(self, config: DataValidationConfig):\n        self.config = config\n\n\n    \n    def validate_all_files_exist(self)-> bool:\n        try:\n            validation_status = None\n\n            all_files = os.listdir(os.path.join(\"artifacts\",\"data_ingestion\",\"samsum_dataset\"))\n\n            for file in all_files:\n                if file not in self.config.ALL_REQUIRED_FILES:\n                    validation_status = False\n                    with open(self.config.STATUS_FILE, 'w') as f:\n                        f.write(f\"Validation status: {validation_status}\")\n                else:\n                    validation_status = True\n                    with open(self.config.STATUS_FILE, 'w') as f:\n                        f.write(f\"Validation status: {validation_status}\")\n\n            return validation_status\n        \n        except Exception as e:\n            raise e", ""]}
{"filename": "src/textSummarizer/conponents/model_trainer.py", "chunked_list": ["from transformers import TrainingArguments, Trainer\nfrom transformers import DataCollatorForSeq2Seq\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom datasets import load_dataset, load_from_disk\nfrom textSummarizer.entity import ModelTrainerConfig\nimport torch\nimport os\n\n\nclass ModelTrainer:\n    def __init__(self, config: ModelTrainerConfig):\n        self.config = config\n\n\n    \n    def train(self):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n        \n        #loading data \n        dataset_samsum_pt = load_from_disk(self.config.data_path)\n\n        # trainer_args = TrainingArguments(\n        #     output_dir=self.config.root_dir, num_train_epochs=self.config.num_train_epochs, warmup_steps=self.config.warmup_steps,\n        #     per_device_train_batch_size=self.config.per_device_train_batch_size, per_device_eval_batch_size=self.config.per_device_train_batch_size,\n        #     weight_decay=self.config.weight_decay, logging_steps=self.config.logging_steps,\n        #     evaluation_strategy=self.config.evaluation_strategy, eval_steps=self.config.eval_steps, save_steps=1e6,\n        #     gradient_accumulation_steps=self.config.gradient_accumulation_steps\n        # ) \n\n\n        trainer_args = TrainingArguments(\n            output_dir=self.config.root_dir, num_train_epochs=1, warmup_steps=500,\n            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n            weight_decay=0.01, logging_steps=10,\n            evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n            gradient_accumulation_steps=16\n        ) \n\n        trainer = Trainer(model=model_pegasus, args=trainer_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])\n        \n        trainer.train()\n\n        ## Save model\n        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n        ## Save tokenizer\n        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))", "\nclass ModelTrainer:\n    def __init__(self, config: ModelTrainerConfig):\n        self.config = config\n\n\n    \n    def train(self):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        tokenizer = AutoTokenizer.from_pretrained(self.config.model_ckpt)\n        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n        seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n        \n        #loading data \n        dataset_samsum_pt = load_from_disk(self.config.data_path)\n\n        # trainer_args = TrainingArguments(\n        #     output_dir=self.config.root_dir, num_train_epochs=self.config.num_train_epochs, warmup_steps=self.config.warmup_steps,\n        #     per_device_train_batch_size=self.config.per_device_train_batch_size, per_device_eval_batch_size=self.config.per_device_train_batch_size,\n        #     weight_decay=self.config.weight_decay, logging_steps=self.config.logging_steps,\n        #     evaluation_strategy=self.config.evaluation_strategy, eval_steps=self.config.eval_steps, save_steps=1e6,\n        #     gradient_accumulation_steps=self.config.gradient_accumulation_steps\n        # ) \n\n\n        trainer_args = TrainingArguments(\n            output_dir=self.config.root_dir, num_train_epochs=1, warmup_steps=500,\n            per_device_train_batch_size=1, per_device_eval_batch_size=1,\n            weight_decay=0.01, logging_steps=10,\n            evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n            gradient_accumulation_steps=16\n        ) \n\n        trainer = Trainer(model=model_pegasus, args=trainer_args,\n                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])\n        \n        trainer.train()\n\n        ## Save model\n        model_pegasus.save_pretrained(os.path.join(self.config.root_dir,\"pegasus-samsum-model\"))\n        ## Save tokenizer\n        tokenizer.save_pretrained(os.path.join(self.config.root_dir,\"tokenizer\"))", ""]}
{"filename": "src/textSummarizer/conponents/__init__.py", "chunked_list": [""]}
{"filename": "src/textSummarizer/conponents/model_evaluation.py", "chunked_list": ["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom datasets import load_dataset, load_from_disk, load_metric\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom textSummarizer.entity import ModelEvaluationConfig\n\n\n\n\nclass ModelEvaluation:\n    def __init__(self, config: ModelEvaluationConfig):\n        self.config = config\n\n\n    \n    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\n        \"\"\"split the dataset into smaller batches that we can process simultaneously\n        Yield successive batch-sized chunks from list_of_elements.\"\"\"\n        for i in range(0, len(list_of_elements), batch_size):\n            yield list_of_elements[i : i + batch_size]\n\n    \n    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \n                               batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n                               column_text=\"article\", \n                               column_summary=\"highlights\"):\n        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n        for article_batch, target_batch in tqdm(\n            zip(article_batches, target_batches), total=len(article_batches)):\n            \n            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n                            padding=\"max_length\", return_tensors=\"pt\")\n            \n            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                            attention_mask=inputs[\"attention_mask\"].to(device), \n                            length_penalty=0.8, num_beams=8, max_length=128)\n            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n            \n            # Finally, we decode the generated texts, \n            # replace the  token, and add the decoded texts with the references to the metric.\n            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n                                    clean_up_tokenization_spaces=True) \n                for s in summaries]      \n            \n            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n            \n            \n            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n            \n        #  Finally compute and return the ROUGE scores.\n        score = metric.compute()\n        return score\n\n\n    def evaluate(self):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n       \n        #loading data \n        dataset_samsum_pt = load_from_disk(self.config.data_path)\n\n\n        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n  \n        rouge_metric = load_metric('rouge')\n\n        score = self.calculate_metric_on_test_ds(\n        dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n            )\n\n        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n\n        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )\n        df.to_csv(self.config.metric_file_name, index=False)", "\n\nclass ModelEvaluation:\n    def __init__(self, config: ModelEvaluationConfig):\n        self.config = config\n\n\n    \n    def generate_batch_sized_chunks(self,list_of_elements, batch_size):\n        \"\"\"split the dataset into smaller batches that we can process simultaneously\n        Yield successive batch-sized chunks from list_of_elements.\"\"\"\n        for i in range(0, len(list_of_elements), batch_size):\n            yield list_of_elements[i : i + batch_size]\n\n    \n    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, \n                               batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\", \n                               column_text=\"article\", \n                               column_summary=\"highlights\"):\n        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n        for article_batch, target_batch in tqdm(\n            zip(article_batches, target_batches), total=len(article_batches)):\n            \n            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n                            padding=\"max_length\", return_tensors=\"pt\")\n            \n            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                            attention_mask=inputs[\"attention_mask\"].to(device), \n                            length_penalty=0.8, num_beams=8, max_length=128)\n            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n            \n            # Finally, we decode the generated texts, \n            # replace the  token, and add the decoded texts with the references to the metric.\n            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n                                    clean_up_tokenization_spaces=True) \n                for s in summaries]      \n            \n            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n            \n            \n            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n            \n        #  Finally compute and return the ROUGE scores.\n        score = metric.compute()\n        return score\n\n\n    def evaluate(self):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n       \n        #loading data \n        dataset_samsum_pt = load_from_disk(self.config.data_path)\n\n\n        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n  \n        rouge_metric = load_metric('rouge')\n\n        score = self.calculate_metric_on_test_ds(\n        dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n            )\n\n        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n\n        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )\n        df.to_csv(self.config.metric_file_name, index=False)", "\n        \n\n"]}
{"filename": "src/textSummarizer/conponents/data_transformation.py", "chunked_list": ["import os\nfrom textSummarizer.logging import logger\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset, load_from_disk\nfrom textSummarizer.entity import DataTransformationConfig\n\n\n\nclass DataTransformation:\n    def __init__(self, config: DataTransformationConfig):\n        self.config = config\n        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n\n\n    \n    def convert_examples_to_features(self,example_batch):\n        input_encodings = self.tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n        \n        with self.tokenizer.as_target_tokenizer():\n            target_encodings = self.tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n            \n        return {\n            'input_ids' : input_encodings['input_ids'],\n            'attention_mask': input_encodings['attention_mask'],\n            'labels': target_encodings['input_ids']\n        }\n    \n\n    def convert(self):\n        dataset_samsum = load_from_disk(self.config.data_path)\n        dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched = True)\n        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir,\"samsum_dataset\"))", "class DataTransformation:\n    def __init__(self, config: DataTransformationConfig):\n        self.config = config\n        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n\n\n    \n    def convert_examples_to_features(self,example_batch):\n        input_encodings = self.tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n        \n        with self.tokenizer.as_target_tokenizer():\n            target_encodings = self.tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n            \n        return {\n            'input_ids' : input_encodings['input_ids'],\n            'attention_mask': input_encodings['attention_mask'],\n            'labels': target_encodings['input_ids']\n        }\n    \n\n    def convert(self):\n        dataset_samsum = load_from_disk(self.config.data_path)\n        dataset_samsum_pt = dataset_samsum.map(self.convert_examples_to_features, batched = True)\n        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir,\"samsum_dataset\"))", "\n\n"]}
{"filename": "src/textSummarizer/conponents/data_ingestion.py", "chunked_list": ["import os\nimport urllib.request as request\nimport zipfile\nfrom textSummarizer.logging import logger\nfrom textSummarizer.utils.common import get_size\nfrom pathlib import Path\nfrom textSummarizer.entity import DataIngestionConfig\n\n\nclass DataIngestion:\n    def __init__(self, config: DataIngestionConfig):\n        self.config = config\n\n\n    \n    def download_file(self):\n        if not os.path.exists(self.config.local_data_file):\n            filename, headers = request.urlretrieve(\n                url = self.config.source_URL,\n                filename = self.config.local_data_file\n            )\n            logger.info(f\"{filename} download! with following info: \\n{headers}\")\n        else:\n            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")  \n\n        \n    \n    def extract_zip_file(self):\n        \"\"\"\n        zip_file_path: str\n        Extracts the zip file into the data directory\n        Function returns None\n        \"\"\"\n        unzip_path = self.config.unzip_dir\n        os.makedirs(unzip_path, exist_ok=True)\n        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n            zip_ref.extractall(unzip_path)", "\nclass DataIngestion:\n    def __init__(self, config: DataIngestionConfig):\n        self.config = config\n\n\n    \n    def download_file(self):\n        if not os.path.exists(self.config.local_data_file):\n            filename, headers = request.urlretrieve(\n                url = self.config.source_URL,\n                filename = self.config.local_data_file\n            )\n            logger.info(f\"{filename} download! with following info: \\n{headers}\")\n        else:\n            logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")  \n\n        \n    \n    def extract_zip_file(self):\n        \"\"\"\n        zip_file_path: str\n        Extracts the zip file into the data directory\n        Function returns None\n        \"\"\"\n        unzip_path = self.config.unzip_dir\n        os.makedirs(unzip_path, exist_ok=True)\n        with zipfile.ZipFile(self.config.local_data_file, 'r') as zip_ref:\n            zip_ref.extractall(unzip_path)"]}
