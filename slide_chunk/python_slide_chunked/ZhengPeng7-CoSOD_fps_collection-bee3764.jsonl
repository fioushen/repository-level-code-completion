{"filename": "test_fps.py", "chunked_list": ["from time import time\nimport torch\n\ndef test_fps(model, size=256, batch_size=2):\n    # Init model\n\n    # print('111!!!!')\n    model.cuda()\n    model.eval()\n\n    N = 2\n    time_total = 0.\n    buf_iter = -500\n    # print('!!!!')\n    with torch.no_grad():\n        for i in range(1+buf_iter, 1000+1):\n            # print(i, end=', ')\n            inputs = torch.randn(batch_size, 3, size, size).float().cuda()\n            time_st = time()\n            _ = model(inputs)\n            if i > 0:\n                time_latest = time() - time_st\n                time_total += time_latest\n                if i % 300 == 0:\n                    print(i, 'time_avg: {:.4f}s, time_curr: {:.4f}s.'.format(time_total / i / N, time_latest / N))\n    return time_total / i / N", ""]}
{"filename": "CoSOD_CoADNet/code/backbone.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\nfrom CoSOD_CoADNet.code.modules import CAM\nfrom CoSOD_CoADNet.code.ops import CU\nimport torch.utils.model_zoo as model_zoo\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=(1, 1), residual=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=dilation[1], bias=False, dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out", "\n    \nclass DRN_A(nn.Module):\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(DRN_A, self).__init__()\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n        self.avgpool = nn.AvgPool2d(28, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                     nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                     nn.BatchNorm2d(planes * block.expansion))\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=(dilation, dilation)))\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x", "\n\ndef Build_DRN_A_50(pretrained=False, **kwargs):\n    model = DRN_A(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet50-19c8e357.pth'))\n    return model\n\n\nclass Backbone_Wrapper_VGG16(nn.Module):\n    def __init__(self):\n        super(Backbone_Wrapper_VGG16, self).__init__()\n        bb = models.vgg16_bn(pretrained=False).features\n        self.C1 = nn.Sequential()\n        self.C2 = nn.Sequential()\n        self.C3 = nn.Sequential()\n        self.C4 = nn.Sequential()\n        self.C5 = nn.Sequential()\n        for layer_index, (name, sub_module) in enumerate(bb.named_children()):\n            if layer_index>=0 and layer_index<=5:\n                self.C1.add_module(name, sub_module)           \n            if layer_index>=7 and layer_index <= 12:\n                self.C2.add_module(name, sub_module)\n            if layer_index>=14 and layer_index <= 22:\n                self.C3.add_module(name, sub_module)\n            if layer_index>=24 and layer_index <= 32:\n                self.C4.add_module(name, sub_module)\n            if layer_index>=34 and layer_index <= 42:\n                self.C5.add_module(name, sub_module)\n        # channels = [64, 128, 256, 512, 512]\n        self.C4_Att = CAM(512, 8)\n        self.C5_Att = CAM(512, 8)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n    def forward(self, image):\n        # image: [B, 3, 128, 128]\n        ftr_s = self.C3(self.pool(self.C2(self.C1(image)))) # [B, 256, 64, 64]\n        ftr_d = self.C5_Att(self.C5(self.pool(self.C4_Att(self.C4(self.pool(ftr_s)))))) # [B, 512, 16, 16]\n        return ftr_d", "\nclass Backbone_Wrapper_VGG16(nn.Module):\n    def __init__(self):\n        super(Backbone_Wrapper_VGG16, self).__init__()\n        bb = models.vgg16_bn(pretrained=False).features\n        self.C1 = nn.Sequential()\n        self.C2 = nn.Sequential()\n        self.C3 = nn.Sequential()\n        self.C4 = nn.Sequential()\n        self.C5 = nn.Sequential()\n        for layer_index, (name, sub_module) in enumerate(bb.named_children()):\n            if layer_index>=0 and layer_index<=5:\n                self.C1.add_module(name, sub_module)           \n            if layer_index>=7 and layer_index <= 12:\n                self.C2.add_module(name, sub_module)\n            if layer_index>=14 and layer_index <= 22:\n                self.C3.add_module(name, sub_module)\n            if layer_index>=24 and layer_index <= 32:\n                self.C4.add_module(name, sub_module)\n            if layer_index>=34 and layer_index <= 42:\n                self.C5.add_module(name, sub_module)\n        # channels = [64, 128, 256, 512, 512]\n        self.C4_Att = CAM(512, 8)\n        self.C5_Att = CAM(512, 8)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n    def forward(self, image):\n        # image: [B, 3, 128, 128]\n        ftr_s = self.C3(self.pool(self.C2(self.C1(image)))) # [B, 256, 64, 64]\n        ftr_d = self.C5_Att(self.C5(self.pool(self.C4_Att(self.C4(self.pool(ftr_s)))))) # [B, 512, 16, 16]\n        return ftr_d", "    \n    \nclass Backbone_Wrapper_ResNet50(nn.Module):\n    def __init__(self):\n        super(Backbone_Wrapper_ResNet50, self).__init__()\n        bb = models.resnet50(pretrained=False)\n        bb.conv1.stride = (1, 1)\n        self.C1 = nn.Sequential(bb.conv1, bb.bn1, bb.relu)\n        self.C2 = bb.layer1\n        self.C3 = bb.layer2\n        self.C4 = bb.layer3\n        self.C5 = bb.layer4\n        # channels = [64, 256, 512, 1024, 2048]\n        self.C4_Att = CAM(1024, 8)\n        self.C5_Att = CAM(2048, 16)\n        self.squeeze_channels = CU(2048, 1024, 1, True, 'relu')\n    def forward(self, image):\n        # image: [B, 3, 128, 128]\n        ftr_s = self.C3(self.C2(self.C1(image))) # [B, 512, 64, 64]\n        ftr_d = self.C5_Att(self.C5(self.C4_Att(self.C4(ftr_s)))) # [B, 2048, 16, 16]\n        ftr_d = self.squeeze_channels(ftr_d) # [B, 1024, 16, 16]\n        return ftr_d", "    \n    \nclass Backbone_Wrapper_Dilated_ResNet50(nn.Module):\n    def __init__(self):\n        super(Backbone_Wrapper_Dilated_ResNet50, self).__init__()\n        bb = Build_DRN_A_50(pretrained=False)\n        bb.conv1.stride = (1, 1)\n        self.C1 = nn.Sequential(bb.conv1, bb.bn1, bb.relu, bb.maxpool)\n        self.C2 = bb.layer1\n        self.C3 = bb.layer2\n        self.C4 = bb.layer3\n        self.C5 = bb.layer4\n        # channels = [64, 256, 512, 1024, 2048]\n        self.C4_Att = CAM(1024, 8)\n        self.C5_Att = CAM(2048, 16)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        self.squeeze_channels = CU(2048, 1024, 1, True, 'relu')\n    def forward(self, image):\n        # image: [B, 3, 128, 128]\n        ftr_s = self.C2(self.C1(image)) # [B, 256, 64, 64]\n        ftr_d = self.C5_Att(self.C5(self.C4_Att(self.C4(self.pool(self.C3(ftr_s)))))) # [B, 2048, 16, 16]\n        ftr_d = self.squeeze_channels(ftr_d) # [B, 1024, 16, 16]\n        return ftr_d", "    \n    \n    \n"]}
{"filename": "CoSOD_CoADNet/code/network.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\nfrom CoSOD_CoADNet.code.ops import *\nfrom CoSOD_CoADNet.code.misc import *\nfrom CoSOD_CoADNet.code.modules import *\nfrom CoSOD_CoADNet.code.backbone import Backbone_Wrapper_VGG16, Backbone_Wrapper_ResNet50, Backbone_Wrapper_Dilated_ResNet50\n\n\nclass CoADNet_VGG16(nn.Module):\n    def __init__(self, mode, compute_loss):\n        super(CoADNet_VGG16, self).__init__()\n        assert mode in ['train', 'test']\n        self.mode = mode\n        self.compute_loss = compute_loss\n        self.M = 5\n        self.D = 512\n        self.S = 4\n        assert np.mod(self.D, self.S) == 0\n        self.backbone = Backbone_Wrapper_VGG16()\n        self.IaSH = IaSH('VGG16')\n        self.online_intra_saliency_guidance = OIaSG()\n        self.lca = nn.ModuleList([LCA(self.D//self.S, 1, [1, 3, 5]) for s in range(self.S)])\n        self.gca = nn.ModuleList([GCA(self.D//self.S, 1) for s in range(self.S)])\n        self.block_fusion = BlockFusion(self.D, 2)\n        self.ggd = GGD(self.D, 1)\n        decode_dims = [self.D, 512, 256, 128]\n        self.gcpd_1 = GCPD(decode_dims[0], decode_dims[1], 4)\n        self.gcpd_2 = GCPD(decode_dims[1], decode_dims[2], 1)\n        self.gcpd_3 = GCPD(decode_dims[2], decode_dims[3], 1)\n        self.cosal_head = CoSH(decode_dims[3])\n    def forward(self, gi, si=None, gl=None, sl=None):\n        # gi: [Bg, M, 3, 128, 128]\n        # si: [Bs, 3, 128, 128]\n        # gl: [Bg, M, 1, 128, 128]\n        # sl: [Bs, 1, 128, 128]\n        M = self.M # number of groups\n        D = self.D # backbone feature dimension\n        S = self.S # number of blocks\n        H, W = gi.size(3), gi.size(4)\n        if self.mode == 'train':\n            assert si is not None\n            assert gl is not None\n            assert sl is not None\n            Bg = gi.size(0)\n            Bs = si.size(0)\n            si_ftr = self.backbone(si) # [Bs, 512, 16, 16]\n            si_sm = self.IaSH(si_ftr) # [Bs, 1, 64, 64]\n        if self.mode == 'test':\n            assert si is None\n            assert gl is None\n            assert sl is None\n            Bg = gi.size(0)\n        # feature extrcation on gi\n        gi_ftr = self.backbone(gi.view(-1, 3, H, W)) # [Bg*M, D, 16, 16]\n        gi_sm = self.IaSH(gi_ftr) # [Bg*M, 1, 64, 64]\n        gi_ftr_g = self.online_intra_saliency_guidance(DS(gi_sm, 4), gi_ftr).view(Bg, M, D, H//8, W//8) # [Bg, M, D, 16, 16]\n        # block-wise group shuffling\n        # re-organize a tensor \"gi_ftr_g\" into a tuple \"shuffled_blocks\"\n        # \"shuffled_blocks[sid] (sid=1,...,S)\" is a tensor with dimension of [Bg, M*D/S, 16, 16]\n        shuffled_blocks = torch.chunk(self.block_wise_shuffle(gi_ftr_g, S), S, dim=1) \n        # local & global aggregation\n        bag = []\n        for sid in range(S):\n            blk = self.attentional_aggregation(shuffled_blocks[sid], M) # [Bg, D/S, 16, 16]\n            blk = self.lca[sid](blk) # [Bg, D/S, 16, 16]\n            blk = self.gca[sid](blk) # [Bg, D/S, 16, 16]\n            bag.append(blk)\n        concat_blk = torch.cat(bag, dim=1) # [Bg, D, 16, 16]\n        gs = self.block_fusion(concat_blk) # [Bg, D, 16, 16]\n        # gated group distribution\n        X_0 = []\n        for mid in range(M):\n            gi_ftr_cosal = self.ggd(gs, gi_ftr_g[:, mid, :, :, :]) # [Bg, D, 16, 16]\n            X_0.append(gi_ftr_cosal.unsqueeze(1)) # [Bg, 1, D, 16, 16]\n        X_0 = torch.cat(X_0, dim=1) # [Bg, M, D, 16, 16]\n        # feature decoding\n        X_1 = self.gcpd_1(X_0) # [Bg, M, 512, 32, 32]\n        X_2 = self.gcpd_2(X_1) # [Bg, M, 256, 64, 64]\n        X_3 = self.gcpd_3(X_2) # [Bg, M, 256, 128, 128]\n        csm = self.cosal_head(X_3) # [Bg, M, 1, 128, 128]\n        if self.mode=='train' and self.compute_loss==True:\n            cosod_loss = self.compute_cosod_loss(csm, gl)\n            sod_loss = self.compute_sod_loss(si_sm, sl)\n            return csm, cosod_loss, sod_loss\n        else:\n            return csm\n    def block_wise_shuffle(self, gi_ftr_g, S):\n        # gi_ftr_g: [Bg, M, D, fh, fw]\n        Bg, M, D, fh, fw = gi_ftr_g.size()\n        assert np.mod(D, S) == 0\n        return gi_ftr_g.view(Bg, M, S, D//S, fh, fw).transpose(1, 2).contiguous().view(Bg, M, D, fh, fw).view(Bg, M*D, fh, fw)\n    def attentional_aggregation(self, blk, M):\n        # blk: [B, M*D/S, fh, fw]\n        # blk_agg: # [B, D/S, fh, fw]\n        blk = F.softmax(blk, dim=1) * blk # [B, M*D/S, fh, fw]\n        blk_bag = torch.chunk(blk, M, dim=1) # blk_bag[mid]: [B, D/S, H, W], mid=1,...,M\n        blk_agg = blk_bag[0] # [B, D/S, fh, fw]\n        for mid in range(1, M):\n            blk_agg = blk_agg + blk_bag[mid] # [B, D/S, fh, fw]\n        return blk_agg\n    def compute_cosod_loss(self, csm, gl):\n        # csm: [Bg, M, 1, 128, 128]\n        # gl: [Bg, M, 1, 128, 128]\n        Bg, M, _, H, W = gl.size()\n        cm = csm.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n        gt = gl.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n        return F.binary_cross_entropy(cm, gt)\n    def compute_sod_loss(self, si_sm, sl):\n        # si_sm: [Bs, 1, 64, 64]\n        # sl: [Bs, 1, 128, 128]\n        return F.binary_cross_entropy(si_sm, DS(sl, 2, 'max'))", "\n    \n    \nclass CoADNet_ResNet50(nn.Module):\n    def __init__(self, mode, compute_loss):\n        super(CoADNet_ResNet50, self).__init__()\n        assert mode in ['train', 'test']\n        self.mode = mode\n        self.compute_loss = compute_loss\n        self.M = 5\n        self.D = 1024\n        self.S = 8\n        assert np.mod(self.D, self.S) == 0\n        self.backbone = Backbone_Wrapper_Dilated_ResNet50()\n        self.IaSH = IaSH('ResNet50')\n        self.online_intra_saliency_guidance = OIaSG()\n        self.lca = nn.ModuleList([LCA(self.D//self.S, 4, [1, 3, 5]) for s in range(self.S)])\n        self.gca = nn.ModuleList([GCA(self.D//self.S, 2) for s in range(self.S)])\n        self.block_fusion = BlockFusion(self.D, 8)\n        self.ggd = GGD(self.D, 8)\n        decode_dims = [self.D, 512, 256, 128]\n        self.gcpd_1 = GCPD(decode_dims[0], decode_dims[1], 8)\n        self.gcpd_2 = GCPD(decode_dims[1], decode_dims[2], 2)\n        self.gcpd_3 = GCPD(decode_dims[2], decode_dims[3], 2)\n        self.cosal_head = CoSH(decode_dims[3])\n    def forward(self, gi, si=None, gl=None, sl=None):\n        # gi: [Bg, M, 3, 128, 128]\n        # si: [Bs, 3, 128, 128]\n        # gl: [Bg, M, 1, 128, 128]\n        # sl: [Bs, 1, 128, 128]\n        M = self.M # number of groups\n        D = self.D # backbone feature dimension\n        S = self.S # number of blocks\n        H, W = gi.size(3), gi.size(4)\n        if self.mode == 'train':\n            assert si is not None\n            assert gl is not None\n            assert sl is not None\n            Bg = gi.size(0)\n            Bs = si.size(0)\n            si_ftr = self.backbone(si) # [Bs, 1024, 16, 16]\n            si_sm = self.IaSH(si_ftr) # [Bs, 1, 64, 64]\n        if self.mode == 'test':\n            assert si is None\n            assert gl is None\n            assert sl is None\n            Bg = gi.size(0)\n        # feature extrcation on gi\n        gi_ftr = self.backbone(gi.view(-1, 3, H, W)) # [Bg*M, D, 16, 16]\n        gi_sm = self.IaSH(gi_ftr) # [Bg*M, 1, 64, 64]\n        gi_ftr_g = self.online_intra_saliency_guidance(DS(gi_sm, 4), gi_ftr).view(Bg, M, D, H//8, W//8) # [Bg, M, D, 16, 16]\n        # block-wise group shuffling\n        # re-organize a tensor \"gi_ftr_g\" into a tuple \"shuffled_blocks\"\n        # \"shuffled_blocks[sid] (sid=1,...,S)\" is a tensor with dimension of [Bg, M*D/S, 16, 16]\n        shuffled_blocks = torch.chunk(self.block_wise_shuffle(gi_ftr_g, S), S, dim=1) \n        # local & global aggregation\n        bag = []\n        for sid in range(S):\n            blk = self.attentional_aggregation(shuffled_blocks[sid], M) # [Bg, D/S, 16, 16]\n            blk = self.lca[sid](blk) # [Bg, D/S, 16, 16]\n            blk = self.gca[sid](blk) # [Bg, D/S, 16, 16]\n            bag.append(blk)\n        concat_blk = torch.cat(bag, dim=1) # [Bg, D, 16, 16]\n        gs = self.block_fusion(concat_blk) # [Bg, D, 16, 16]\n        # gated group distribution\n        X_0 = []\n        for mid in range(M):\n            gi_ftr_cosal = self.ggd(gs, gi_ftr_g[:, mid, :, :, :]) # [Bg, D, 16, 16]\n            X_0.append(gi_ftr_cosal.unsqueeze(1)) # [Bg, 1, D, 16, 16]\n        X_0 = torch.cat(X_0, dim=1) # [Bg, M, D, 16, 16]\n        # feature decoding\n        X_1 = self.gcpd_1(X_0) # [Bg, M, 512, 32, 32]\n        X_2 = self.gcpd_2(X_1) # [Bg, M, 256, 64, 64]\n        X_3 = self.gcpd_3(X_2) # [Bg, M, 256, 128, 128]\n        csm = self.cosal_head(X_3) # [Bg, M, 1, 128, 128]\n        if self.mode=='train' and self.compute_loss==True:\n            cosod_loss = self.compute_cosod_loss(csm, gl)\n            sod_loss = self.compute_sod_loss(si_sm, sl)\n            return csm, cosod_loss, sod_loss\n        else:\n            return csm\n    def block_wise_shuffle(self, gi_ftr_g, S):\n        # gi_ftr_g: [Bg, M, D, fh, fw]\n        Bg, M, D, fh, fw = gi_ftr_g.size()\n        assert np.mod(D, S) == 0\n        return gi_ftr_g.view(Bg, M, S, D//S, fh, fw).transpose(1, 2).contiguous().view(Bg, M, D, fh, fw).view(Bg, M*D, fh, fw)\n    def attentional_aggregation(self, blk, M):\n        # blk: [B, M*D/S, fh, fw]\n        # blk_agg: # [B, D/S, fh, fw]\n        blk = F.softmax(blk, dim=1) * blk # [B, M*D/S, fh, fw]\n        blk_bag = torch.chunk(blk, M, dim=1) # blk_bag[mid]: [B, D/S, H, W], mid=1,...,M\n        blk_agg = blk_bag[0] # [B, D/S, fh, fw]\n        for mid in range(1, M):\n            blk_agg = blk_agg + blk_bag[mid] # [B, D/S, fh, fw]\n        return blk_agg\n    def compute_cosod_loss(self, csm, gl):\n        # csm: [Bg, M, 1, 128, 128]\n        # gl: [Bg, M, 1, 128, 128]\n        Bg, M, _, H, W = gl.size()\n        cm = csm.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n        gt = gl.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n        return F.binary_cross_entropy(cm, gt)\n    def compute_sod_loss(self, si_sm, sl):\n        # si_sm: [Bs, 1, 64, 64]\n        # sl: [Bs, 1, 128, 128]\n        return F.binary_cross_entropy(si_sm, DS(sl, 2, 'max'))", "    \n    \n    \nclass CoADNet_Dilated_ResNet50(nn.Module):\n    def __init__(self, mode='test', compute_loss=False):\n        super(CoADNet_Dilated_ResNet50, self).__init__()\n        assert mode in ['train', 'test']\n        self.mode = mode\n        self.compute_loss = compute_loss\n        self.M = 2\n        self.D = 1024\n        self.S = 8\n        assert np.mod(self.D, self.S) == 0\n        self.backbone = Backbone_Wrapper_Dilated_ResNet50()\n        self.IaSH = IaSH('Dilated_ResNet50')\n        self.online_intra_saliency_guidance = OIaSG()\n        self.lca = nn.ModuleList([LCA(self.D//self.S, 4, [1, 3, 5]) for s in range(self.S)])\n        self.gca = nn.ModuleList([GCA(self.D//self.S, 2) for s in range(self.S)])\n        self.block_fusion = BlockFusion(self.D, 8)\n        self.ggd = GGD(self.D, 8)\n        decode_dims = [self.D, 512, 256, 128]\n        self.gcpd_1 = GCPD(decode_dims[0], decode_dims[1], 8)\n        self.gcpd_2 = GCPD(decode_dims[1], decode_dims[2], 2)\n        self.gcpd_3 = GCPD(decode_dims[2], decode_dims[3], 2)\n        self.cosal_head = CoSH(decode_dims[3])\n    def forward(self, gi, si=None, gl=None, sl=None):\n        # gi: [Bg, M, 3, 128, 128]\n        # si: [Bs, 3, 128, 128]\n        # gl: [Bg, M, 1, 128, 128]\n        # sl: [Bs, 1, 128, 128]\n        M = self.M # number of groups\n        D = self.D # backbone feature dimension\n        S = self.S # number of blocks\n        gi = gi.unsqueeze(0)\n        H, W = gi.size(3), gi.size(4)\n        if self.mode == 'train':\n            assert si is not None\n            assert gl is not None\n            assert sl is not None\n            Bg = gi.size(0)\n            Bs = si.size(0)\n            si_ftr = self.backbone(si) # [Bs, 1024, 16, 16]\n            si_sm = self.IaSH(si_ftr) # [Bs, 1, 64, 64]\n        if self.mode == 'test':\n            assert si is None\n            assert gl is None\n            assert sl is None\n            Bg = gi.size(0)\n        # feature extrcation on gi\n        gi_ftr = self.backbone(gi.view(-1, 3, H, W)) # [Bg*M, D, 16, 16]\n        gi_sm = self.IaSH(gi_ftr) # [Bg*M, 1, 64, 64]\n        gi_ftr_g = self.online_intra_saliency_guidance(DS(gi_sm, 4), gi_ftr).view(Bg, M, D, H//8, W//8) # [Bg, M, D, 16, 16]\n        # block-wise group shuffling\n        # re-organize a tensor \"gi_ftr_g\" into a tuple \"shuffled_blocks\"\n        # \"shuffled_blocks[sid] (sid=1,...,S)\" is a tensor with dimension of [Bg, M*D/S, 16, 16]\n        shuffled_blocks = torch.chunk(self.block_wise_shuffle(gi_ftr_g, S), S, dim=1) \n        # local & global aggregation\n        bag = []\n        for sid in range(S):\n            blk = self.attentional_aggregation(shuffled_blocks[sid], M) # [Bg, D/S, 16, 16]\n            blk = self.lca[sid](blk) # [Bg, D/S, 16, 16]\n            blk = self.gca[sid](blk) # [Bg, D/S, 16, 16]\n            bag.append(blk)\n        concat_blk = torch.cat(bag, dim=1) # [Bg, D, 16, 16]\n        gs = self.block_fusion(concat_blk) # [Bg, D, 16, 16]\n        # gated group distribution\n        X_0 = []\n        for mid in range(M):\n            gi_ftr_cosal = self.ggd(gs, gi_ftr_g[:, mid, :, :, :]) # [Bg, D, 16, 16]\n            X_0.append(gi_ftr_cosal.unsqueeze(1)) # [Bg, 1, D, 16, 16]\n        X_0 = torch.cat(X_0, dim=1) # [Bg, M, D, 16, 16]\n        # feature decoding\n        X_1 = self.gcpd_1(X_0) # [Bg, M, 512, 32, 32]\n        X_2 = self.gcpd_2(X_1) # [Bg, M, 256, 64, 64]\n        X_3 = self.gcpd_3(X_2) # [Bg, M, 256, 128, 128]\n        csm = self.cosal_head(X_3) # [Bg, M, 1, 128, 128]\n        if self.mode=='train' and self.compute_loss==True:\n            cosod_loss = self.compute_cosod_loss(csm, gl)\n            sod_loss = self.compute_sod_loss(si_sm, sl)\n            return csm, cosod_loss, sod_loss\n        else:\n            return csm\n    def block_wise_shuffle(self, gi_ftr_g, S):\n        # gi_ftr_g: [Bg, M, D, fh, fw]\n        Bg, M, D, fh, fw = gi_ftr_g.size()\n        assert np.mod(D, S) == 0\n        return gi_ftr_g.view(Bg, M, S, D//S, fh, fw).transpose(1, 2).contiguous().view(Bg, M, D, fh, fw).view(Bg, M*D, fh, fw)\n    def attentional_aggregation(self, blk, M):\n        # blk: [B, M*D/S, fh, fw]\n        # blk_agg: # [B, D/S, fh, fw]\n        blk = F.softmax(blk, dim=1) * blk # [B, M*D/S, fh, fw]\n        blk_bag = torch.chunk(blk, M, dim=1) # blk_bag[mid]: [B, D/S, H, W], mid=1,...,M\n        blk_agg = blk_bag[0] # [B, D/S, fh, fw]\n        for mid in range(1, M):\n            blk_agg = blk_agg + blk_bag[mid] # [B, D/S, fh, fw]\n        return blk_agg\n    def compute_cosod_loss(self, csm, gl):\n        # csm: [Bg, M, 1, 128, 128]\n        # gl: [Bg, M, 1, 128, 128]\n        Bg, M, _, H, W = gl.size()\n        cm = csm.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n        gt = gl.view(Bg*M, 1, H, W) # [Bg*M, 1, H, W]\n        return F.binary_cross_entropy(cm, gt)\n    def compute_sod_loss(self, si_sm, sl):\n        # si_sm: [Bs, 1, 64, 64]\n        # sl: [Bs, 1, 128, 128]\n        return F.binary_cross_entropy(si_sm, DS(sl, 2, 'max'))", "    \n    \n    "]}
{"filename": "CoSOD_CoADNet/code/ops.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\n\n\ndef US(x, up_scale_factor):\n    # up-scale features\n    # x: [B, C, H, W]\n    # y: [B, C, H_us, W_us]\n    isinstance(up_scale_factor, int)\n    y = F.interpolate(x, scale_factor=2, mode='bilinear')\n    return y", "\n\ndef DS(x, down_scale_factor, ds_mode='max'):\n    # down-scale features\n    # x: [B, C, H, W]\n    # y: [B, C, H_ds, W_ds]\n    isinstance(down_scale_factor, int)\n    assert ds_mode in ['max', 'avg']\n    if ds_mode == 'max':\n        y = F.max_pool2d(x, down_scale_factor)\n    if ds_mode == 'avg':\n        y = F.avg_pool2d(x, down_scale_factor)\n    return y", "\n\ndef GAP(x):\n    # global average pooling\n    # x: [B, C, H, W]\n    # y: [B, C, 1, 1]\n    y = F.adaptive_avg_pool2d(x, (1, 1))\n    return y\n\n\nclass CU(nn.Module):\n    # Convolution Unit\n    def __init__(self, ic, oc, ks, is_bn, na):\n        # ic: input channels\n        # oc: output channels\n        # ks: kernel size\n        # is_bn: True/False\n        # na: non-linear activation\n        super(CU, self).__init__()\n        assert isinstance(ic, int)\n        assert isinstance(oc, int)\n        assert isinstance(ks, int)\n        assert isinstance(is_bn, bool)\n        assert isinstance(na, str)\n        assert np.mod(ks + 1, 2) == 0\n        assert na in ['none', 'relu', 'sigmoid']\n        self.is_bn = is_bn\n        self.na = na\n        self.convolution = nn.Conv2d(ic, oc, ks, padding=(ks-1)//2, bias=False)\n        if self.is_bn:\n            self.batch_normalization = nn.BatchNorm2d(oc)\n        if self.na == 'relu':\n            self.activation = nn.ReLU(inplace=True)\n        if self.na == 'sigmoid':\n            self.activation = nn.Sigmoid()\n    def forward(self, x):\n        # x: [B, ic, H, W]\n        # y: [B, oc, H, W]\n        y = self.convolution(x)\n        if self.is_bn:\n            y = self.batch_normalization(y)\n        if self.na != 'none':\n            y = self.activation(y)\n        return y", "\n\nclass CU(nn.Module):\n    # Convolution Unit\n    def __init__(self, ic, oc, ks, is_bn, na):\n        # ic: input channels\n        # oc: output channels\n        # ks: kernel size\n        # is_bn: True/False\n        # na: non-linear activation\n        super(CU, self).__init__()\n        assert isinstance(ic, int)\n        assert isinstance(oc, int)\n        assert isinstance(ks, int)\n        assert isinstance(is_bn, bool)\n        assert isinstance(na, str)\n        assert np.mod(ks + 1, 2) == 0\n        assert na in ['none', 'relu', 'sigmoid']\n        self.is_bn = is_bn\n        self.na = na\n        self.convolution = nn.Conv2d(ic, oc, ks, padding=(ks-1)//2, bias=False)\n        if self.is_bn:\n            self.batch_normalization = nn.BatchNorm2d(oc)\n        if self.na == 'relu':\n            self.activation = nn.ReLU(inplace=True)\n        if self.na == 'sigmoid':\n            self.activation = nn.Sigmoid()\n    def forward(self, x):\n        # x: [B, ic, H, W]\n        # y: [B, oc, H, W]\n        y = self.convolution(x)\n        if self.is_bn:\n            y = self.batch_normalization(y)\n        if self.na != 'none':\n            y = self.activation(y)\n        return y", "    \n    \nclass FC(nn.Module):\n    # Fully-Connected Layer\n    def __init__(self, ic, oc, is_bn, na):\n        super(FC, self).__init__()\n        # ic: input channels\n        # oc: output channels\n        # is_bn: True/False\n        # na: non-linear activation\n        assert isinstance(ic, int)\n        assert isinstance(oc, int)\n        assert isinstance(is_bn, bool)\n        assert isinstance(na, str)\n        assert na in ['none', 'relu', 'sigmoid']\n        self.is_bn = is_bn\n        self.na = na\n        self.linear = nn.Linear(ic, oc, bias=False)\n        if self.is_bn:\n            self.batch_normalization = nn.BatchNorm1d(oc)\n        if self.na == 'relu':\n            self.activation = nn.ReLU(inplace=True)\n        if self.na == 'sigmoid':\n            self.activation = nn.Sigmoid()\n    def forward(self, x):\n        # x: [B, ic]\n        # y: [B, oc]\n        y = self.linear(x)\n        if self.is_bn:\n            y = self.batch_normalization(y)\n        if self.na != 'none':\n            y = self.activation(y)\n        return y", "    \n    \nclass DilConv3(nn.Module):\n    #  Dilated Convolution with 3*3 kernel size\n    def __init__(self, ic, oc, is_bn, na, dr):\n        super(DilConv3, self).__init__()\n        # ic: input channels\n        # oc: output channels\n        # is_bn: True/False\n        # na: non-linear activation\n        # dr: dilation rate\n        assert isinstance(ic, int)\n        assert isinstance(oc, int)\n        assert isinstance(is_bn, bool)\n        assert isinstance(na, str)\n        assert isinstance(dr, int)\n        assert na in ['none', 'relu', 'sigmoid']\n        self.is_bn = is_bn\n        self.na = na\n        self.dil_conv = nn.Conv2d(ic, oc, kernel_size=3, padding=dr, dilation=dr, bias=False)\n        if self.is_bn:\n            self.batch_normalization = nn.BatchNorm2d(oc)\n        if self.na == 'relu':\n            self.activation = nn.ReLU(inplace=True)\n        if self.na == 'sigmoid':\n            self.activation = nn.Sigmoid()\n    def forward(self, x):\n        # x: [B, ic, H, W]\n        # y: [B, oc, H, W]\n        y = self.dil_conv(x)\n        if self.is_bn:\n            y = self.batch_normalization(y)\n        if self.na != 'none':\n            y = self.activation(y) \n        return y", "    \n    \n    "]}
{"filename": "CoSOD_CoADNet/code/misc.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\n\n\ndef align_number(number, N):\n    assert type(number) == int\n    num_str = str(number)\n    assert len(num_str) <= N\n    return (N - len(num_str)) * '0' + num_str\n\n\ndef min_max_normalize(x):\n    x_normed = (x - np.min(x)) / (np.max(x)-np.min(x))\n    return x_normed", "\n\ndef min_max_normalize(x):\n    x_normed = (x - np.min(x)) / (np.max(x)-np.min(x))\n    return x_normed\n\n\ndef visualize_image_tensor(image_tensor):\n    # image_tensor: [3, H, W]\n    # image_pil: PIL.Image object\n    unloader = transforms.ToPILImage()\n    image_tensor = image_tensor.cpu()\n    image_pil = unloader(image_tensor)\n    return image_pil", "\n\ndef visualize_label_tensor(label_tensor):\n    # label_tensor: [1, H, W]\n    # label_pil: PIL.Image object\n    label_numpy = label_tensor.squeeze(0).cpu().data.numpy() # (H, W)\n    label_numpy = min_max_normalize(label_numpy) * 255\n    label_pil = Image.fromarray(label_numpy).convert('L')\n    return label_pil\n", "\n\ndef binarize_label(label_tensor, threshold):\n    assert label_tensor.min()>=0 and label_tensor.max()<=1\n    assert threshold>=0 and threshold<=1\n    label_tensor[label_tensor < threshold] = 0\n    label_tensor[label_tensor >= threshold] = 1\n    return label_tensor\n\n\ndef random_crop_tensor(x, cropped_h, cropped_w):\n    # x: [in_channels, input_h, input_w]\n    # y: [in_channels, cropped_h, cropped_w]\n    in_channels, input_h, input_w = x.size()\n    assert input_h>cropped_h and input_w>cropped_w\n    h_start = np.random.randint(0, (input_h-cropped_h))\n    w_start = np.random.randint(0, (input_w-cropped_w))\n    y = x[:, h_start:(h_start+cropped_h), w_start:(w_start+cropped_w)] # [in_channels, cropped_h, cropped_w]\n    return y", "\n\ndef random_crop_tensor(x, cropped_h, cropped_w):\n    # x: [in_channels, input_h, input_w]\n    # y: [in_channels, cropped_h, cropped_w]\n    in_channels, input_h, input_w = x.size()\n    assert input_h>cropped_h and input_w>cropped_w\n    h_start = np.random.randint(0, (input_h-cropped_h))\n    w_start = np.random.randint(0, (input_w-cropped_w))\n    y = x[:, h_start:(h_start+cropped_h), w_start:(w_start+cropped_w)] # [in_channels, cropped_h, cropped_w]\n    return y", "\n\ndef random_crop_image_label(image, label, expansion_ratio):\n    # image & label: PIL.Image\n    # image_cropped, label_cropped: torch.tensor, [3, height, width] & [1, height, width]\n    assert image.height==label.height and image.width==label.width\n    assert expansion_ratio > 0\n    height, width = image.height, image.width\n    exp_height, exp_width = int(height*(1+expansion_ratio)), int(width*(1+expansion_ratio))\n    to_tensor = transforms.ToTensor()\n    image = to_tensor(image.resize((exp_height, exp_width))) # [3, exp_height, exp_width]\n    label = to_tensor(label.resize((exp_height, exp_width))) # [1, exp_height, exp_width]\n    image_concat_label = torch.cat((image, label), dim=0) # [4, exp_height, exp_width]\n    image_concat_label_cropped = random_crop_tensor(image_concat_label, height, width) # [4, height, width]\n    image_cropped = image_concat_label_cropped[0:3, :, :] # [3, height, width]\n    label_cropped = image_concat_label_cropped[3:4, :, :] # [1, height, width]\n    label_cropped = binarize_label(label_cropped, 0.50) # [1, height, width]\n    return image_cropped, label_cropped", "\n\n# def read_image(path_list_every, return_list):\n#     for load_path in path_list_every:\n#         return_list.append(Image.open(load_path))\n\n        \n# def image_loader_multi_processing(path_list, num_cores):\n#     num_total = len(path_list)\n#     num_every = int(num_total / num_cores) + 1", "#     num_total = len(path_list)\n#     num_every = int(num_total / num_cores) + 1\n#     path_list_every = []\n#     for index in range(num_cores):\n#         index_start = index * num_every\n#         index_end = (index + 1) * num_every\n#         path_list_every.append(path_list[index_start:index_end])\n#     manager = Manager()\n#     return_list = manager.list()\n#     jobs = []", "#     return_list = manager.list()\n#     jobs = []\n#     for i in range(num_cores):\n#         prcs = Process(target=read_image, args=(path_list_every[i], return_list))\n#         jobs.append(prcs)\n#         prcs.start()\n#     for jb in jobs:\n#         jb.join()\n#     return return_list\n", "#     return return_list\n\n\ndef load_image_as_tensor(image_full_path):\n    loader = transforms.ToTensor()\n    image_tensor = loader(Image.open(image_full_path)) # [C, H, W], [0, 1]\n    image_tensor_batch = image_tensor.unsqueeze(0) # add batch dimension, [1, C, H, W]\n    return image_tensor_batch\n\n\ndef unload(x):\n    y = x.squeeze().cpu().data.numpy()\n    return y", "\n\ndef unload(x):\n    y = x.squeeze().cpu().data.numpy()\n    return y\n\n\ndef convert2img(x):\n    return Image.fromarray(x*255).convert('L')\n", "\n\ndef save_smap(smap, path, negative_threshold=0.0):\n    # smap: [1, H, W]\n    if torch.max(smap) <= negative_threshold:\n        smap[smap<negative_threshold] = 0\n        smap = convert2img(unload(smap))\n    else:\n        smap = convert2img(min_max_normalize(unload(smap)))\n    smap.save(path)", "    \n\ndef cache_model(model, path, multi_gpu):\n    if multi_gpu:\n        torch.save(model.module.state_dict(), path)\n    else:\n        torch.save(model.state_dict(), path)\n    \n    ", "    "]}
{"filename": "CoSOD_CoADNet/code/modules.py", "chunked_list": ["from CoSOD_CoADNet.code.common_packages import *\nfrom CoSOD_CoADNet.code.ops import *\nfrom CoSOD_CoADNet.code.misc import *\n\n\nclass CAM(nn.Module): \n    # Channel Attention Module\n    def __init__(self, in_channels, squeeze_ratio):\n        super(CAM, self).__init__()\n        inter_channels = in_channels // squeeze_ratio\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        fc_1 = FC(in_channels, inter_channels, False, 'relu')\n        fc_2 = FC(inter_channels, in_channels, False, 'none')\n        self.fc = nn.Sequential(fc_1, fc_2)\n    def forward(self, x):\n        # x: [B, in_channels, fh, fw]\n        # y: [B, in_channels, fh, fw]\n        avg_pooled = self.avg_pool(x).squeeze(-1).squeeze(-1) # [B, in_channels]\n        max_pooled = self.max_pool(x).squeeze(-1).squeeze(-1) # [B, in_channels]\n        ap_weights = self.fc(avg_pooled) # [B, in_channels]\n        mp_weights = self.fc(max_pooled) # [B, in_channels]\n        weights = F.sigmoid(ap_weights + mp_weights) # [B, in_channels]\n        y = x * weights.unsqueeze(-1).unsqueeze(-1) + x\n        return y", "    \n    \nclass SAM(nn.Module): \n    # Spatial Attention Module\n    def __init__(self, conv_ks):\n        super(SAM, self).__init__()\n        assert conv_ks>=3 and np.mod(conv_ks+1, 2)==0\n        self.conv = CU(ic=2, oc=1, ks=conv_ks, is_bn=False, na='sigmoid')\n    def forward(self, x):\n        # x: [B, ic, fh, fw]\n        # y: [B, ic, fh, fw]\n        avg_pooled = torch.mean(x, dim=1, keepdim=True) # [B, 1, fh, fw]\n        max_pooled = torch.max(x, dim=1, keepdim=True)[0] # [B, 1, fh, fw]\n        cat_pooled = torch.cat((avg_pooled, max_pooled), dim=1) # [B, 2, fh, fw]\n        weights = self.conv(cat_pooled) # [B, 1, fh, fw]\n        y = x * weights + x\n        return y", "    \n    \nclass LCA(nn.Module): \n    # Local Context Aggregation\n    def __init__(self, in_channels, squeeze_ratio, dr_list):\n        super(LCA, self).__init__()\n        inter_channels = in_channels // squeeze_ratio\n        self.conv_1 = CU(in_channels, inter_channels, 1, True, 'relu')\n        self.conv_2 = DilConv3(in_channels, inter_channels, True, 'relu', dr_list[0])\n        self.conv_3 = DilConv3(in_channels, inter_channels, True, 'relu', dr_list[1])\n        self.conv_4 = DilConv3(in_channels, inter_channels, True, 'relu', dr_list[2])\n        self.fusion = CU(inter_channels*4, in_channels, 1, True, 'relu')\n    def forward(self, x):\n        # x: [B, in_channels, fh, fw]\n        # y: [B, in_channels, fh, fw]\n        x_1 = self.conv_1(x) # [B, inter_channels, fh, fw]\n        x_2 = self.conv_2(x) # [B, inter_channels, fh, fw]\n        x_3 = self.conv_3(x) # [B, inter_channels, fh, fw]\n        x_4 = self.conv_4(x) # [B, inter_channels, fh, fw]\n        x_f = self.fusion(torch.cat((x_1, x_2, x_3, x_4), dim=1)) # [B, in_channels, fh, fw]\n        y = x_f + x\n        return y", "    \n    \nclass GCA(nn.Module):\n    # Global Context Aggregation\n    def __init__(self, in_channels, squeeze_ratio):\n        super(GCA, self).__init__()\n        self.map_q = CU(in_channels, in_channels//squeeze_ratio, 1, False, 'none')\n        self.map_k = CU(in_channels, in_channels//squeeze_ratio, 1, False, 'none')\n        self.map_v = CU(in_channels, in_channels, 1, False, 'none')\n        self.gamma = nn.Parameter(torch.zeros(1))\n    def forward(self, ftr):\n        # ftr: [B, C, H, W]\n        # ftr_fusion: # [B, C, H, W]\n        B, C, H, W = ftr.size()\n        N = H * W\n        ftr_q = self.map_q(ftr).view(B, -1, N).transpose(1, 2).contiguous() # [B, N, C']\n        ftr_k = self.map_k(ftr).view(B, -1, N) # [B, C', N]\n        aff_mat = F.softmax(torch.bmm(ftr_q, ftr_k), dim=1) # [B, N, N]\n        ftr_v = self.map_v(ftr).view(B, -1, N) # [B, C, N]\n        ftr_gca = torch.bmm(ftr_v, aff_mat).view(B, C, H, W) # [B, C, H, W]\n        ftr_fusion = ftr + ftr_gca * self.gamma # [B, C, H, W]\n        return ftr_fusion", "    \n    \nclass IaSH(nn.Module):\n    # Intra-Saliency Head\n    def __init__(self, bb_type):\n        super(IaSH, self).__init__()\n        assert bb_type in ['VGG16', 'ResNet50', 'Dilated_ResNet50']\n        self.bb_type = bb_type\n        upsample_2x = nn.Upsample(scale_factor=2, mode='bilinear')\n        if bb_type == 'VGG16':\n            self.conv_1 = nn.Sequential(upsample_2x, CU(512, 256, 3, True, 'relu'))\n            self.conv_2 = nn.Sequential(upsample_2x, CU(256, 128, 3, True, 'relu'))\n            self.output = nn.Sequential(CU(128, 128, 3, True, 'relu'), CU(128, 64, 3, True, 'relu'), CU(64, 1, 3, False, 'sigmoid'))\n        if bb_type == 'ResNet50':\n            self.conv_1 = nn.Sequential(CU(1024, 512, 1, True, 'relu'), upsample_2x, CU(512, 256, 3, True, 'relu'))\n            self.conv_2 = nn.Sequential(upsample_2x, CU(256, 128, 3, True, 'relu'))\n            self.output = nn.Sequential(CU(128, 128, 3, True, 'relu'), CU(128, 64, 3, True, 'relu'), CU(64, 1, 3, False, 'sigmoid'))\n        if bb_type == 'Dilated_ResNet50':          \n            self.conv_1 = nn.Sequential(CU(1024, 512, 1, True, 'relu'), upsample_2x, CU(512, 256, 3, True, 'relu'))\n            self.conv_2 = nn.Sequential(upsample_2x, CU(256, 128, 3, True, 'relu'))\n            self.output = nn.Sequential(CU(128, 128, 3, True, 'relu'), CU(128, 64, 3, True, 'relu'), CU(64, 1, 3, False, 'sigmoid'))\n    def forward(self, si_ftr):\n        # si_ftr: [Bs, D, 16, 16]\n        sm = self.output(self.conv_2(self.conv_1(si_ftr))) # [Bs, 1, 64, 64]\n        return sm", "    \n    \nclass OIaSG(nn.Module):\n    # Online-Intra Saliency Guidance\n    def __init__(self):\n        super(OIaSG, self).__init__()\n        self.fusion_1 = CU(ic=2, oc=1, ks=3, is_bn=False, na='sigmoid')\n        self.fusion_2 = CU(ic=2, oc=1, ks=3, is_bn=False, na='sigmoid')\n    def forward(self, gi_sm, gi_ftr_d):\n        # gi_sm: [Bg*M, 1, 16, 16]\n        # gi_ftr_d: [Bg*M, Cd, 16, 16]\n        ftr_avg = torch.mean(gi_ftr_d, dim=1, keepdim=True) # [Bg*M, 1, 16, 16]\n        ftr_max = torch.max(gi_ftr_d, dim=1, keepdim=True)[0] # [Bg*M, 1, 16, 16]\n        ftr_concat = torch.cat((ftr_avg, ftr_max), dim=1) # [Bg*M, 2, 16, 16]\n        ftr_fusion = self.fusion_1(ftr_concat) # [Bg*M, 1, 16, 16]\n        gm = self.fusion_2(torch.cat((ftr_fusion, gi_sm), dim=1)) # [Bg*M, 1, 16, 16]\n        intra_sal_ftr = gi_ftr_d + gi_ftr_d * gm # [Bg*M, Cd, 16, 16]\n        return intra_sal_ftr", "    \n    \nclass BlockFusion(nn.Module):\n    def __init__(self, in_channels, squeeze_ratio):\n        super(BlockFusion, self).__init__()\n        inter_channels = in_channels // squeeze_ratio\n        conv_1 = CU(in_channels, in_channels, 1, True, 'relu')\n        conv_2 = CU(in_channels, inter_channels, 3, True, 'relu')\n        conv_3 = CU(inter_channels, in_channels, 3, True, 'relu')\n        self.conv = nn.Sequential(conv_1, conv_2, conv_3)\n    def forward(self, ftr):\n        # ftr: [B, C, H, W]\n        # ftr_fusion: [B, C, H, W]\n        ftr_fusion = self.conv(ftr) # [B, C, H, W]\n        return ftr_fusion + ftr", "    \n    \nclass GGD(nn.Module):\n    # Gated Group Distribution\n    def __init__(self, in_channels, squeeze_ratio):\n        super(GGD, self).__init__()\n        inter_channels = in_channels // squeeze_ratio\n        self.squeeze_channel = nn.Sequential(CU(in_channels*2, in_channels//2, 1, True, 'relu'), CAM(in_channels//2, 8))\n        conv_1 = CU(in_channels//2, inter_channels, 3, True, 'relu')\n        conv_2 = CU(inter_channels, in_channels, 3, False, 'sigmoid')\n        self.gie = nn.Sequential(conv_1, conv_2)\n    def forward(self, G, U):\n        # G: [Bg, D, H, W]\n        # U: [Bg, D, H, W] \n        P = self.gie(self.squeeze_channel(torch.cat((G, U), dim=1))) # [B, D, H, W]\n        ftr_cosal = (G - U) * P + U # [B, D, H, W], \"G*P+U*(1-P)\"\n        return ftr_cosal", "    \n    \nclass GCPD(nn.Module):\n    # Group Consistency Preserving Decoder\n    def __init__(self, in_channels, out_channels, squeeze_ratio):\n        super(GCPD, self).__init__()\n        tf_1 = CU(in_channels, in_channels//2, 1, True, 'relu')\n        tf_2 = nn.Upsample(scale_factor=2, mode='bilinear')\n        tf_3 = CU(in_channels//2, in_channels//2, 3, True, 'relu')\n        self.transform = nn.Sequential(tf_1, tf_2, tf_3)\n        inter_channels = in_channels // squeeze_ratio\n        smlp_1 = FC(in_channels, inter_channels, False, 'relu')\n        smlp_2 = FC(inter_channels, in_channels//2, False, 'sigmoid')\n        self.smlp = nn.Sequential(smlp_1, smlp_2)\n        self.conv = CU(in_channels//2, out_channels, 3, True, 'relu')\n    def forward(self, X):\n        # X: [B, M, ic, fh, fw]\n        # X_d: [B, M, oc, fh*2, fw*2]\n        B, M, ic, fh, fw = X.size()\n        X_t = self.transform(X.view(-1, ic, fh, fw)) # [B*M, ic//2, fh*2, fw*2]\n        V = GAP(X_t).squeeze(-1).squeeze(-1).view(B, M, ic//2) # [B, M, ic//2]\n        y = torch.sum(V * F.softmax(V, dim=1), dim=1) # [B, ic//2]\n        V_cat_y = torch.cat((V, y.unsqueeze(1).repeat(1, M, 1)), dim=-1) # [B, M, ic]\n        A = self.smlp(V_cat_y.view(-1, ic)) # [B, M, ic//2]\n        X_t = X_t * A.unsqueeze(-1).unsqueeze(-1) + X_t # [B*M, ic//2, fh*2, fw*2]\n        X_d = self.conv(X_t).view(B, M, -1, fh*2, fw*2) # [B, M, oc, fh*2, fw*2]\n        return X_d", "    \n    \nclass CoSH(nn.Module):\n    # Co-Saliency Head\n    def __init__(self, in_channels):\n        super(CoSH, self).__init__()\n        dims = [in_channels, in_channels, 64, 1]\n        head_1 = CU(dims[0], dims[1], 3, True, 'relu')\n        head_2 = CU(dims[1], dims[2], 3, True, 'relu')\n        head_3 = CU(dims[2], dims[3], 3, False, 'sigmoid')\n        self.head = nn.Sequential(head_1, head_2, head_3)\n    def forward(self, x):\n        # x: [B, M, in_channels, H, W]\n        # y: [B, M, 1, H, W]\n        B, M, C, H, W = x.size()\n        y = self.head(x.view(-1, C, H, W)).view(B, M, 1, H, W)\n        return y", "    \n    \n"]}
{"filename": "CoSOD_CoADNet/code/common_packages.py", "chunked_list": ["import os\nimport sys\nimport math\nimport time\nimport numpy as np\nfrom PIL import Image\nfrom numpy import random\nfrom multiprocessing import Process, Queue, Pool, Pipe, Manager\n\n", "\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils import data\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torchvision import models, transforms\n", "from torchvision import models, transforms\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n"]}
{"filename": "GCAGC_CVPR2020/model3/model2_graph4_hrnet_agcm.py", "chunked_list": ["from torch.optim import Adam\nimport math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n#from resnext import ResNeXt101\nfrom torch.autograd import Variable\nfrom .default import _C as config", "from torch.autograd import Variable\nfrom .default import _C as config\nfrom .cls_hrnet import get_cls_net\nmodel5 = get_cls_net(config)\nmodel5.cuda()\n###############\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.prnet = model5\n        self.gc4_1 = GraphConvolution(384,  192)\n        self.gc4_2 = GraphConvolution(192,  192)\n        self.gc3_1 = GraphConvolution(192, 96)\n        self.gc3_2 = GraphConvolution(96,  96)\n        self.gc2_1 = GraphConvolution(96,  48)\n        self.gc2_2 = GraphConvolution(48,  48)\n        self.group_size=5\n        # decoder\n        de_in_channels=int(48+96+192)\n        de_layers = make_decoder_layers(decoder_archs['d16'], de_in_channels, batch_norm=True)\n        self.decoder = DOCSDecoderNet(de_layers)\n#        self.initialize_weights()\n    def forward(self, img):\n        with torch.no_grad():\n           fea = self.prnet(img)           #### layer4=(B*N, C, H, W)  \n        out4 = unsqz_fea(fea[3])        #### fea5=(B,N,392,H,W)\n        out3 = unsqz_fea(fea[2])        #### fea5=(B,N,192,H,W)\n        out2 = unsqz_fea(fea[1])\n        for a in range(out4.shape[0]):\n            ######################## layer4\n            feat4 = out4[a].permute(0, 2, 3, 1).reshape(-1, out4.shape[2]) # (N*H*W,C)           \n            adj4 = torch.mm(feat4, torch.t(feat4)) # (N*H*W, N*H*W)\n            adj4 = row_normalize(adj4)\n            gc4 = F.relu(self.gc4_1(feat4, adj4))\n            gc4 = F.relu(self.gc4_2(gc4, adj4))    # (N*H*W,192)\n            gc4 = gc4.reshape(out4[a].shape[0], out4[a].shape[2], out4[a].shape[3], 192)\n            gc44 = gc4.permute(0, 3, 1, 2)       # (N,192,H,W)\n            gc4 = F.upsample(gc44, scale_factor=4, mode='bilinear')\n            ######################### layer3\n            feat3=out3[a].permute(0,2,3,1).reshape(-1, out3.shape[2])\n            adj3=torch.mm(feat3, torch.t(feat3))\n            adj3=row_normalize(adj3)\n            gc3=F.relu(self.gc3_1(feat3, adj3))\n            gc3=F.relu(self.gc3_2(gc3, adj3))   # (5*14*14,96)\n            gc3 = gc3.reshape(out3[a].shape[0], out3[a].shape[2], out3[a].shape[3], 96)\n            gc3 = gc3.permute(0, 3, 1, 2)       # (N,96,H,W) \n            gc3 = F.upsample(gc3, scale_factor=2, mode='bilinear')\n            ######################### layer2\n            feat2=out2[a].permute(0,2,3,1).reshape(-1, out2.shape[2])\n            adj2=torch.mm(feat2, torch.t(feat2))            \n            adj2=row_normalize(adj2)\n            gc2=F.relu(self.gc2_1(feat2, adj2))\n            gc2=F.relu(self.gc2_2(gc2, adj2))\n            gc2 = gc2.reshape(out2[a].shape[0], out2[a].shape[2], out2[a].shape[3], 48)\n            gc2 = gc2.permute(0, 3, 1, 2)\n            #########################\n            gc_fuse = torch.cat((gc4, gc3, gc2), dim=1)\n            if a == 0:\n                gcx_out = gc_fuse\n                gc4_out = gc44\n            else:\n                gcx_out = torch.cat((gcx_out, gc_fuse), dim=0)\n                gc4_out = torch.cat((gc4_out, gc44), dim=0)\n        spa_masks = spatial_optimize(gc4_out, self.group_size).cuda()\n        spa_masks = F.upsample(spa_masks, scale_factor=4, mode='bilinear')\n        spa_masks = spa_masks.expand_as(gcx_out)\n        ######################\n        out_final=self.decoder(spa_masks + gcx_out)\n        out_final=F.upsample(out_final, size=img.size()[2:], mode='bilinear')\n        out_final=out_final.sigmoid().squeeze()\n        return out_final,fea[3], fea[2], fea[1]", "        \n#    def initialize_weights(self):\n#        pretrained_dict=torch.load('/home/litengpeng/CODE/semantic-segmentation/CPD-HR2/hrnetv2_w48_imagenet_pretrained.pth')\n#        net_dict=self.prnet.state_dict()\n#        for key,value in pretrained_dict.items():\n#          if key in net_dict.keys():\n#             net_dict[key]=value\n#        net_dict.update(net_dict)\n#        self.prnet.load_state_dict(net_dict)    \n############## unsupervised masks", "#        self.prnet.load_state_dict(net_dict)    \n############## unsupervised masks\n############## unsupervised masks\ndef norm(x,dim):\n    squared_norm=(x**2).sum(dim=dim, keepdim=True)\n    normed=x/torch.sqrt(squared_norm)\n    return normed\ndef spatial_optimize(fmap, group_size):\n    N, H, W = 2, 8, 8\n    fmap_split = torch.split(fmap, group_size, dim=0)\n    for i in range(len(fmap_split)):\n        cur_fmap = fmap_split[i]\n        with torch.no_grad():\n            spatial_x = cur_fmap.permute(0, 2, 3, 1).contiguous().view(-1, cur_fmap.size(1)).transpose(1, 0)\n            spatial_x = norm(spatial_x, dim=0)\n            spatial_x_t = spatial_x.transpose(1, 0)\n            G = torch.mm(spatial_x_t , spatial_x) - 1\n            G = G.detach().cpu()\n\n        with torch.enable_grad():\n            # N*H*W\n            # M = 5 * 7 * 7\n            M = N * H * W\n            spatial_s = nn.Parameter(torch.sqrt(M * torch.ones((M, 1))) / M, requires_grad=True)\n            spatial_s_t = spatial_s.transpose(1, 0)\n            spatial_s_optimizer = Adam([spatial_s], 0.01)\n\n            for iter in range(200):\n                f_spa_loss = -1 * torch.sum(torch.mm(torch.mm(spatial_s_t, G), spatial_s))\n                spatial_s_d = torch.sqrt(torch.sum(spatial_s ** 2))\n                if spatial_s_d >= 1:\n                    d_loss = -1 * torch.log(2 - spatial_s_d)\n                else:\n                    d_loss = -1 * torch.log(spatial_s_d)\n\n                all_loss = 50 * d_loss + f_spa_loss\n#                if iter%20==0:\n#                   print('iter: [%.4f], loss: [%.4f], dloss:[%.4f], floss: [%.4f]' %(iter, all_loss, d_loss, f_spa_loss))\n\n                spatial_s_optimizer.zero_grad()\n                all_loss.backward()\n                spatial_s_optimizer.step()\n\n        result_map = spatial_s.data.view(N, 1, H, W)\n\n        if i == 0:\n            spa_mask = result_map\n        else:\n            spa_mask = torch.cat(([spa_mask, result_map]), dim=0)\n\n    return spa_mask", "##################### unsupervised masks\n##################### unsupervised masks\ndef row_normalize(mx):\n    \"\"\"Row-normalize sparse matrix\"\"\"\n    rowsum = torch.sum(mx, dim=1)\n    r_inv = 1 / (rowsum + 1e-10)\n    r_mat_inv = torch.diag(r_inv)\n    mx = torch.mm(r_mat_inv, mx)\n    return mx\ndef unsqz_fea(dim4_data):\n    split_data = torch.split(dim4_data, 5, dim=0)\n    for i in range(len(split_data)):\n        if i == 0:\n            dim5_data = split_data[i].unsqueeze(dim=0)\n        else:\n            dim5_data = torch.cat((dim5_data, split_data[i].unsqueeze(dim=0)), dim=0)\n    return dim5_data", "def unsqz_fea(dim4_data):\n    split_data = torch.split(dim4_data, 5, dim=0)\n    for i in range(len(split_data)):\n        if i == 0:\n            dim5_data = split_data[i].unsqueeze(dim=0)\n        else:\n            dim5_data = torch.cat((dim5_data, split_data[i].unsqueeze(dim=0)), dim=0)\n    return dim5_data\n\ndef sqz_fea(dim5_data):\n    if dim5_data.size(1) == 1:\n        return dim5_data.squeeze()\n    else:\n        b = dim5_data.size(0)\n        for i in range(b):\n            if i == 0:\n                new_dim4_data = dim5_data[i, :, :, :, :]\n            else:\n                new_dim4_data = torch.cat((new_dim4_data, dim5_data[i, :, :, :, :]), dim=0)\n    return new_dim4_data    ", "\ndef sqz_fea(dim5_data):\n    if dim5_data.size(1) == 1:\n        return dim5_data.squeeze()\n    else:\n        b = dim5_data.size(0)\n        for i in range(b):\n            if i == 0:\n                new_dim4_data = dim5_data[i, :, :, :, :]\n            else:\n                new_dim4_data = torch.cat((new_dim4_data, dim5_data[i, :, :, :, :]), dim=0)\n    return new_dim4_data    ", "################################\nclass GraphConvolution(Module):\n    \"\"\"\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        support = torch.mm(input, self.weight)\n        output = torch.mm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output", "#################\ndecoder_archs = {\n    'd16': [336, 'd128', 128, 128, 'd64', 64, 64, 'c1']\n}\n\ndef make_decoder_layers(cfg, in_channels, batch_norm=True):\n    layers = []\n    for v in cfg:\n        if type(v) is str:\n            if v[0] == 'd':\n                v = int(v[1:])\n                convtrans2d = nn.ConvTranspose2d(in_channels, v, kernel_size=4, stride=2, padding=1)\n                if batch_norm:\n                    layers += [convtrans2d, nn.BatchNorm2d(v), nn.ReLU()]\n                else:\n                    layers += [convtrans2d, nn.ReLU()]\n                in_channels = v\n            elif v[0] == 'c':\n                v = int(v[1:])\n                layers += [nn.Conv2d(in_channels, v, kernel_size=3, padding=1)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU()]\n            else:\n                layers += [conv2d, nn.ReLU()]\n            in_channels = v\n    return nn.Sequential(*layers)", "class DOCSDecoderNet(nn.Module):\n    def __init__(self, features):\n        super(DOCSDecoderNet, self).__init__()\n        self.features = features\n\n    def forward(self, x):\n        return self.features(x)"]}
{"filename": "GCAGC_CVPR2020/model3/model2_graph4_hrnet.py", "chunked_list": ["import math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n#from resnext import ResNeXt101\nfrom torch.autograd import Variable\nfrom default import _C as config\nfrom cls_hrnet import get_cls_net", "from default import _C as config\nfrom cls_hrnet import get_cls_net\nmodel5 = get_cls_net(config)\nmodel5.cuda()\n###############\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.prnet = model5\n        self.gc4_1 = GraphConvolution(384,  192)\n        self.gc4_2 = GraphConvolution(192,  192)\n        self.gc3_1 = GraphConvolution(192, 96)\n        self.gc3_2 = GraphConvolution(96,  96)\n        self.gc2_1 = GraphConvolution(96,  48)\n        self.gc2_2 = GraphConvolution(48,  48)\n        # \n        #self.pro3=nn.Conv2d(192, 192, 1)\n        # decoder\n        de_in_channels=int(48+96+192)\n        de_layers = make_decoder_layers(decoder_archs['d16'], de_in_channels, batch_norm=True)\n        self.decoder = DOCSDecoderNet(de_layers)\n#        self.decoder=nn.Sequential(nn.Conv2d(64, 64, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.ConvTranspose2d(64, 1, 64, 32, 16))\n#        self.initialize_weights()\n    def forward(self, img):\n        with torch.no_grad():\n            fea = self.prnet(img)       #### layer4=(B*N, C, H, W)  \n        out4 = unsqz_fea(fea[3])        #### fea5=(B,N,392,H,W)\n        out3 = unsqz_fea(fea[2])        #### fea5=(B,N,192,H,W)\n        out2 = unsqz_fea(fea[1])\n        for a in range(out4.shape[0]):\n            ######################## layer4\n            feat4 = out4[a].permute(0, 2, 3, 1).reshape(-1, out4.shape[2]) # (N*H*W,C)           \n            adj4 = torch.mm(feat4, torch.t(feat4)) # (N*H*W, N*H*W)\n            adj4 = row_normalize(adj4)\n            gc4 = F.relu(self.gc4_1(feat4, adj4))\n            gc4 = F.relu(self.gc4_2(gc4, adj4))    # (N*H*W,192)\n            gc4 = gc4.reshape(out4[a].shape[0], out4[a].shape[2], out4[a].shape[3], 192)\n            gc4 = gc4.permute(0, 3, 1, 2)       # (N,192,H,W)\n            gc4 = F.upsample(gc4, scale_factor=4, mode='bilinear')\n            ######################### layer3\n            feat3=out3[a].permute(0,2,3,1).reshape(-1, out3.shape[2])\n            adj3=torch.mm(feat3, torch.t(feat3))\n            adj3=row_normalize(adj3)\n            gc3=F.relu(self.gc3_1(feat3, adj3))\n            gc3=F.relu(self.gc3_2(gc3, adj3))   # (5*14*14,96)\n            gc3 = gc3.reshape(out3[a].shape[0], out3[a].shape[2], out3[a].shape[3], 96)\n            gc3 = gc3.permute(0, 3, 1, 2)       # (N,96,H,W) \n            gc3 = F.upsample(gc3, scale_factor=2, mode='bilinear')\n            ######################### layer2\n            feat2=out2[a].permute(0,2,3,1).reshape(-1, out2.shape[2])\n            adj2=torch.mm(feat2, torch.t(feat2))            \n            adj2=row_normalize(adj2)\n            gc2=F.relu(self.gc2_1(feat2, adj2))\n            gc2=F.relu(self.gc2_2(gc2, adj2))\n            gc2 = gc2.reshape(out2[a].shape[0], out2[a].shape[2], out2[a].shape[3], 48)\n            gc2 = gc2.permute(0, 3, 1, 2)\n            #########################\n            gc_fuse = torch.cat((gc4, gc3, gc2), dim=1)\n            if a == 0:\n                gcx_out = gc_fuse\n            else:\n                gcx_out = torch.cat((gcx_out, gc_fuse), dim=0)\n        ######################\n        out_final=self.decoder(gcx_out)\n        out_final=F.upsample(out_final, size=img.size()[2:], mode='bilinear')\n        out_final=out_final.sigmoid().squeeze()\n        return out_final, fea[3], fea[2], fea[1]", "        \n#    def initialize_weights(self):\n#        pretrained_dict=torch.load('/home/litengpeng/CODE/semantic-segmentation/CPD-HR2/hrnetv2_w48_imagenet_pretrained.pth')\n#        net_dict=self.prnet.state_dict()\n#        for key,value in pretrained_dict.items():\n#          if key in net_dict.keys():\n#             net_dict[key]=value\n#        net_dict.update(net_dict)\n#        self.prnet.load_state_dict(net_dict)    \ndef row_normalize(mx):\n    \"\"\"Row-normalize sparse matrix\"\"\"\n    rowsum = torch.sum(mx, dim=1)\n    r_inv = 1 / (rowsum + 1e-10)\n    r_mat_inv = torch.diag(r_inv)\n    mx = torch.mm(r_mat_inv, mx)\n    return mx", "#        self.prnet.load_state_dict(net_dict)    \ndef row_normalize(mx):\n    \"\"\"Row-normalize sparse matrix\"\"\"\n    rowsum = torch.sum(mx, dim=1)\n    r_inv = 1 / (rowsum + 1e-10)\n    r_mat_inv = torch.diag(r_inv)\n    mx = torch.mm(r_mat_inv, mx)\n    return mx\ndef unsqz_fea(dim4_data):\n    split_data = torch.split(dim4_data, 5, dim=0)\n    for i in range(len(split_data)):\n        if i == 0:\n            dim5_data = split_data[i].unsqueeze(dim=0)\n        else:\n            dim5_data = torch.cat((dim5_data, split_data[i].unsqueeze(dim=0)), dim=0)\n    return dim5_data", "def unsqz_fea(dim4_data):\n    split_data = torch.split(dim4_data, 5, dim=0)\n    for i in range(len(split_data)):\n        if i == 0:\n            dim5_data = split_data[i].unsqueeze(dim=0)\n        else:\n            dim5_data = torch.cat((dim5_data, split_data[i].unsqueeze(dim=0)), dim=0)\n    return dim5_data\n\ndef sqz_fea(dim5_data):\n    if dim5_data.size(1) == 1:\n        return dim5_data.squeeze()\n    else:\n        b = dim5_data.size(0)\n        for i in range(b):\n            if i == 0:\n                new_dim4_data = dim5_data[i, :, :, :, :]\n            else:\n                new_dim4_data = torch.cat((new_dim4_data, dim5_data[i, :, :, :, :]), dim=0)\n    return new_dim4_data    ", "\ndef sqz_fea(dim5_data):\n    if dim5_data.size(1) == 1:\n        return dim5_data.squeeze()\n    else:\n        b = dim5_data.size(0)\n        for i in range(b):\n            if i == 0:\n                new_dim4_data = dim5_data[i, :, :, :, :]\n            else:\n                new_dim4_data = torch.cat((new_dim4_data, dim5_data[i, :, :, :, :]), dim=0)\n    return new_dim4_data    ", "################################\nclass GraphConvolution(Module):\n    \"\"\"\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        support = torch.mm(input, self.weight)\n        output = torch.mm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output", "#################\ndecoder_archs = {\n    'd16': [336, 'd128', 128, 128, 'd64', 64, 64, 'c1']\n}\n\ndef make_decoder_layers(cfg, in_channels, batch_norm=True):\n    layers = []\n    for v in cfg:\n        if type(v) is str:\n            if v[0] == 'd':\n                v = int(v[1:])\n                convtrans2d = nn.ConvTranspose2d(in_channels, v, kernel_size=4, stride=2, padding=1)\n                if batch_norm:\n                    layers += [convtrans2d, nn.BatchNorm2d(v), nn.ReLU()]\n                else:\n                    layers += [convtrans2d, nn.ReLU()]\n                in_channels = v\n            elif v[0] == 'c':\n                v = int(v[1:])\n                layers += [nn.Conv2d(in_channels, v, kernel_size=3, padding=1)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU()]\n            else:\n                layers += [conv2d, nn.ReLU()]\n            in_channels = v\n    return nn.Sequential(*layers)", "class DOCSDecoderNet(nn.Module):\n    def __init__(self, features):\n        super(DOCSDecoderNet, self).__init__()\n        self.features = features\n\n    def forward(self, x):\n        return self.features(x)\n#####################\n# def pca_group(fmap, group_size):\n#     fmap_split = torch.split(fmap, group_size, dim=0)", "# def pca_group(fmap, group_size):\n#     fmap_split = torch.split(fmap, group_size, dim=0)\n#     for i in range(len(fmap_split)):\n#         cur_fmap = fmap_split[i]\n#         p4pca = cur_fmap.permute(0, 2, 3, 1).contiguous().view(-1, cur_fmap.size()[1])\n#         p4pca_mean = torch.mean(p4pca, dim=0)\n#         p4pca = p4pca - p4pca_mean.expand_as(p4pca)\n#         U, S, V = torch.svd(torch.t(p4pca))\n#         C = torch.mm(p4pca, U[:, 0].view(-1, 1))\n#         a4pca = C.view(cur_fmap.size()[0], cur_fmap.size()[2], cur_fmap.size()[3], 1)", "#         C = torch.mm(p4pca, U[:, 0].view(-1, 1))\n#         a4pca = C.view(cur_fmap.size()[0], cur_fmap.size()[2], cur_fmap.size()[3], 1)\n#         cur_mask = a4pca.permute(0, 3, 1, 2)\n#         # ddd = cur_mask.detach().cpu().numpy()\n#         if i == 0:\n#             pca_mask = cur_mask\n#         else:\n#             pca_mask = torch.cat(([pca_mask, cur_mask]), dim=0)\n#     # ddd = pca_mask.detach().cpu().numpy()\n#     return pca_mask", "#     # ddd = pca_mask.detach().cpu().numpy()\n#     return pca_mask"]}
{"filename": "GCAGC_CVPR2020/model3/model2_graph4_hrnet_sal.py", "chunked_list": ["import math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \\\n    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding\n#from resnext import ResNeXt101\nfrom torch.autograd import Variable\nfrom .model2_graph4_hrnet_agcm import Model\nmodel6 = Model()", "from .model2_graph4_hrnet_agcm import Model\nmodel6 = Model()\nmodel6.cuda()\nclass Model2(nn.Module):\n    def __init__(self):\n        super(Model2, self).__init__()\n        self.cosalnet = model6\n        self.jpu = JPU([96, 96, 96, 192, 384], width=96, norm_layer=nn.BatchNorm2d)\n        de_in_channels=int(96*4)\n        de_layers = make_decoder_layers(decoder_archs['d16'], de_in_channels, batch_norm=True)\n        self.decodersal = DOCSDecoderNet(de_layers)\n#        self.initialize_weights()\n    def forward(self, img):  \n        cosalmap, fea3, fea2, fea1 = self.cosalnet(img)\n        with torch.no_grad():\n          feat= self.jpu(fea1, fea2, fea3)\n          pred=self.decodersal(feat)\n          pred=F.upsample(pred, size=img.size()[2:], mode='bilinear')\n        #cosalmap=F.upsample(cosalmap, img.size()[2:], mode='bilinear')\n        return pred, cosalmap", "    \n#    def initialize_weights(self):\n#        pretrained_dict=torch.load('/home/litengpeng/CODE/cosal/aaai19/mine1-cosal/model_path/graph4_decoder_hrnet/hrnet_iter67500.pth')\n#        net_dict=self.cosalnet.state_dict()\n#        for key,value in pretrained_dict.items():\n#          if key in net_dict.keys():\n#             net_dict[key]=value\n#        net_dict.update(net_dict)\n#        self.cosalnet.load_state_dict(net_dict)    \n        \nclass JPU(nn.Module):\n    def __init__(self, in_channels, width=96, norm_layer=nn.BatchNorm2d):\n        super(JPU, self).__init__()\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(inplace=True))\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels[-3], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(inplace=True))\n        \n        self.dilation1 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=1, dilation=1, bias=False),\n                                       norm_layer(width),\n                                       nn.ReLU(inplace=True))\n        self.dilation2 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=2, dilation=2, bias=False),\n                                       norm_layer(width),\n                                       nn.ReLU(inplace=True))\n        self.dilation3 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=4, dilation=4, bias=False),\n                                       norm_layer(width),\n                                       nn.ReLU(inplace=True))\n        self.dilation4 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=8, dilation=8, bias=False),\n                                       norm_layer(width),\n                                       nn.ReLU(inplace=True))\n\n    def forward(self, *inputs):\n        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2]), self.conv3(inputs[-3])]\n        _, _, h, w = feats[-1].size()\n        feats[-2] = F.upsample(feats[-2], (h, w), mode='bilinear')\n        feats[-3] = F.upsample(feats[-3], (h, w), mode='bilinear')\n        feat = torch.cat(feats, dim=1)   #### channel 128*3=284\n        feat = torch.cat([self.dilation1(feat), self.dilation2(feat), self.dilation3(feat), self.dilation4(feat)], dim=1)\n\n        return feat  ##### channel 128*4=512", "#        self.cosalnet.load_state_dict(net_dict)    \n        \nclass JPU(nn.Module):\n    def __init__(self, in_channels, width=96, norm_layer=nn.BatchNorm2d):\n        super(JPU, self).__init__()\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels[-1], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(inplace=True))\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels[-2], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels[-3], width, 3, padding=1, bias=False),\n            norm_layer(width),\n            nn.ReLU(inplace=True))\n        \n        self.dilation1 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=1, dilation=1, bias=False),\n                                       norm_layer(width),\n                                       nn.ReLU(inplace=True))\n        self.dilation2 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=2, dilation=2, bias=False),\n                                       norm_layer(width),\n                                       nn.ReLU(inplace=True))\n        self.dilation3 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=4, dilation=4, bias=False),\n                                       norm_layer(width),\n                                       nn.ReLU(inplace=True))\n        self.dilation4 = nn.Sequential(SeparableConv2d(3*width, width, kernel_size=3, padding=8, dilation=8, bias=False),\n                                       norm_layer(width),\n                                       nn.ReLU(inplace=True))\n\n    def forward(self, *inputs):\n        feats = [self.conv5(inputs[-1]), self.conv4(inputs[-2]), self.conv3(inputs[-3])]\n        _, _, h, w = feats[-1].size()\n        feats[-2] = F.upsample(feats[-2], (h, w), mode='bilinear')\n        feats[-3] = F.upsample(feats[-3], (h, w), mode='bilinear')\n        feat = torch.cat(feats, dim=1)   #### channel 128*3=284\n        feat = torch.cat([self.dilation1(feat), self.dilation2(feat), self.dilation3(feat), self.dilation4(feat)], dim=1)\n\n        return feat  ##### channel 128*4=512", "\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=False, BatchNorm=nn.BatchNorm2d):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding, dilation, groups=inplanes, bias=bias)\n        self.bn = BatchNorm(inplanes)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.pointwise(x)\n        return x", "#################\ndecoder_archs = {\n    'd16': [96*4, 'd256', 256, 256, 'd128', 128, 128, 'd64', 64, 64, 'c1']\n}\n\ndef make_decoder_layers(cfg, in_channels, batch_norm=True):\n    layers = []\n    for v in cfg:\n        if type(v) is str:\n            if v[0] == 'd':\n                v = int(v[1:])\n                convtrans2d = nn.ConvTranspose2d(in_channels, v, kernel_size=4, stride=2, padding=1)\n                if batch_norm:\n                    layers += [convtrans2d, nn.BatchNorm2d(v), nn.ReLU()]\n                else:\n                    layers += [convtrans2d, nn.ReLU()]\n                in_channels = v\n            elif v[0] == 'c':\n                v = int(v[1:])\n                layers += [nn.Conv2d(in_channels, v, kernel_size=3, padding=1)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU()]\n            else:\n                layers += [conv2d, nn.ReLU()]\n            in_channels = v\n    return nn.Sequential(*layers)", "class DOCSDecoderNet(nn.Module):\n    def __init__(self, features):\n        super(DOCSDecoderNet, self).__init__()\n        self.features = features\n\n    def forward(self, x):\n        return self.features(x)"]}
{"filename": "GCAGC_CVPR2020/model3/cls_hrnet.py", "chunked_list": ["# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# Modified by Ke Sun (sunk@mail.ustc.edu.cn)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function", "from __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport logging\nimport functools\n\nimport numpy as np\n\nimport torch", "\nimport torch\nimport torch.nn as nn\nimport torch._utils\nimport torch.nn.functional as F\n\nBN_MOMENTUM = 0.1\nlogger = logging.getLogger(__name__)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)", "\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out", "class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out", "\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n                               momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out", "\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(False)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(num_channels[branch_index] * block.expansion,\n                            momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride, downsample))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j],\n                                  num_inchannels[i],\n                                  1,\n                                  1,\n                                  0,\n                                  bias=False),\n                        nn.BatchNorm2d(num_inchannels[i], \n                                       momentum=BN_MOMENTUM),\n                        nn.Upsample(scale_factor=2**(j-i), mode='nearest')))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i-j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_outchannels_conv3x3, \n                                            momentum=BN_MOMENTUM)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_outchannels_conv3x3,\n                                            momentum=BN_MOMENTUM),\n                                nn.ReLU(False)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse", "\n\nblocks_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass HighResolutionNet(nn.Module):\n\n    def __init__(self, cfg, **kwargs):\n        super(HighResolutionNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(Bottleneck, 64, 64, 4)\n\n        self.stage2_cfg = cfg['MODEL']['EXTRA']['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [256], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = cfg['MODEL']['EXTRA']['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = cfg['MODEL']['EXTRA']['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        # Classification Head\n#        self.incre_modules, self.downsamp_modules, \\\n#            self.final_layer = self._make_head(pre_stage_channels)\n#\n#        self.classifier = nn.Linear(2048, 1000)\n\n#    def _make_head(self, pre_stage_channels):\n#        head_block = Bottleneck\n#        head_channels = [32, 64, 128, 256]\n#\n#        # Increasing the #channels on each resolution \n#        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n#        incre_modules = []\n#        for i, channels  in enumerate(pre_stage_channels):\n#            incre_module = self._make_layer(head_block,\n#                                            channels,\n#                                            head_channels[i],\n#                                            1,\n#                                            stride=1)\n#            incre_modules.append(incre_module)\n#        incre_modules = nn.ModuleList(incre_modules)\n#            \n#        # downsampling modules\n#        downsamp_modules = []\n#        for i in range(len(pre_stage_channels)-1):\n#            in_channels = head_channels[i] * head_block.expansion\n#            out_channels = head_channels[i+1] * head_block.expansion\n#\n#            downsamp_module = nn.Sequential(\n#                nn.Conv2d(in_channels=in_channels,\n#                          out_channels=out_channels,\n#                          kernel_size=3,\n#                          stride=2,\n#                          padding=1),\n#                nn.BatchNorm2d(out_channels, momentum=BN_MOMENTUM),\n#                nn.ReLU(inplace=True)\n#            )\n#\n#            downsamp_modules.append(downsamp_module)\n#        downsamp_modules = nn.ModuleList(downsamp_modules)\n#\n#        final_layer = nn.Sequential(\n#            nn.Conv2d(\n#                in_channels=head_channels[3] * head_block.expansion,\n#                out_channels=2048,\n#                kernel_size=1,\n#                stride=1,\n#                padding=0\n#            ),\n#            nn.BatchNorm2d(2048, momentum=BN_MOMENTUM),\n#            nn.ReLU(inplace=True)\n#        )\n#\n#        return incre_modules, downsamp_modules, final_layer\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        nn.BatchNorm2d(\n                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i+1-num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i-num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        nn.BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                HighResolutionModule(num_branches,\n                                      block,\n                                      num_blocks,\n                                      num_inchannels,\n                                      num_channels,\n                                      fuse_method,\n                                      reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        # Classification Head\n#        y = self.incre_modules[0](y_list[0])\n#        for i in range(len(self.downsamp_modules)):\n#            y = self.incre_modules[i+1](y_list[i+1]) + \\\n#                        self.downsamp_modules[i](y)\n#\n#        y = self.final_layer(y)\n#\n#        y = F.avg_pool2d(y, kernel_size=y.size()\n#                             [2:]).view(y.size(0), -1)\n#            \n#        y = self.classifier(y)\n\n        return y_list\n\n    def init_weights(self, pretrained='',):\n        logger.info('=> init weights from normal distribution')\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained)\n            logger.info('=> loading pretrained model {}'.format(pretrained))\n            model_dict = self.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n            for k, _ in pretrained_dict.items():\n                logger.info(\n                    '=> loading {} pretrained model {}'.format(k, pretrained))\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)", "class HighResolutionNet(nn.Module):\n\n    def __init__(self, cfg, **kwargs):\n        super(HighResolutionNet, self).__init__()\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(Bottleneck, 64, 64, 4)\n\n        self.stage2_cfg = cfg['MODEL']['EXTRA']['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [256], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = cfg['MODEL']['EXTRA']['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = cfg['MODEL']['EXTRA']['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        # Classification Head\n#        self.incre_modules, self.downsamp_modules, \\\n#            self.final_layer = self._make_head(pre_stage_channels)\n#\n#        self.classifier = nn.Linear(2048, 1000)\n\n#    def _make_head(self, pre_stage_channels):\n#        head_block = Bottleneck\n#        head_channels = [32, 64, 128, 256]\n#\n#        # Increasing the #channels on each resolution \n#        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n#        incre_modules = []\n#        for i, channels  in enumerate(pre_stage_channels):\n#            incre_module = self._make_layer(head_block,\n#                                            channels,\n#                                            head_channels[i],\n#                                            1,\n#                                            stride=1)\n#            incre_modules.append(incre_module)\n#        incre_modules = nn.ModuleList(incre_modules)\n#            \n#        # downsampling modules\n#        downsamp_modules = []\n#        for i in range(len(pre_stage_channels)-1):\n#            in_channels = head_channels[i] * head_block.expansion\n#            out_channels = head_channels[i+1] * head_block.expansion\n#\n#            downsamp_module = nn.Sequential(\n#                nn.Conv2d(in_channels=in_channels,\n#                          out_channels=out_channels,\n#                          kernel_size=3,\n#                          stride=2,\n#                          padding=1),\n#                nn.BatchNorm2d(out_channels, momentum=BN_MOMENTUM),\n#                nn.ReLU(inplace=True)\n#            )\n#\n#            downsamp_modules.append(downsamp_module)\n#        downsamp_modules = nn.ModuleList(downsamp_modules)\n#\n#        final_layer = nn.Sequential(\n#            nn.Conv2d(\n#                in_channels=head_channels[3] * head_block.expansion,\n#                out_channels=2048,\n#                kernel_size=1,\n#                stride=1,\n#                padding=0\n#            ),\n#            nn.BatchNorm2d(2048, momentum=BN_MOMENTUM),\n#            nn.ReLU(inplace=True)\n#        )\n#\n#        return incre_modules, downsamp_modules, final_layer\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  3,\n                                  1,\n                                  1,\n                                  bias=False),\n                        nn.BatchNorm2d(\n                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i+1-num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i-num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(\n                            inchannels, outchannels, 3, 2, 1, bias=False),\n                        nn.BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride, downsample))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                HighResolutionModule(num_branches,\n                                      block,\n                                      num_blocks,\n                                      num_inchannels,\n                                      num_channels,\n                                      fuse_method,\n                                      reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        # Classification Head\n#        y = self.incre_modules[0](y_list[0])\n#        for i in range(len(self.downsamp_modules)):\n#            y = self.incre_modules[i+1](y_list[i+1]) + \\\n#                        self.downsamp_modules[i](y)\n#\n#        y = self.final_layer(y)\n#\n#        y = F.avg_pool2d(y, kernel_size=y.size()\n#                             [2:]).view(y.size(0), -1)\n#            \n#        y = self.classifier(y)\n\n        return y_list\n\n    def init_weights(self, pretrained='',):\n        logger.info('=> init weights from normal distribution')\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        if os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained)\n            logger.info('=> loading pretrained model {}'.format(pretrained))\n            model_dict = self.state_dict()\n            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                               if k in model_dict.keys()}\n            for k, _ in pretrained_dict.items():\n                logger.info(\n                    '=> loading {} pretrained model {}'.format(k, pretrained))\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)", "\n\ndef get_cls_net(config, **kwargs):\n    model = HighResolutionNet(config, **kwargs)\n    model.init_weights()\n    return model\n"]}
{"filename": "GCAGC_CVPR2020/model3/__init__.py", "chunked_list": [""]}
{"filename": "GCAGC_CVPR2020/model3/default.py", "chunked_list": ["\n# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Ke Sun (sunk@mail.ustc.edu.cn)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function", "from __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom yacs.config import CfgNode as CN\n\n\n_C = CN()\n#", "_C = CN()\n#\n# # common params for NETWORK\n# _C.MODEL = CN()\n# _C.MODEL.NAME = 'seg_hrnet'\n# _C.MODEL.PRETRAINED = ''\n# _C.MODEL.EXTRA = CN(new_allowed=True)\n#\n# # stage2\n# _C.MODEL.EXTRA.STAGE2 = CN()", "# # stage2\n# _C.MODEL.EXTRA.STAGE2 = CN()\n# _C.MODEL.EXTRA.STAGE2.NUM_MODULES = 1\n# _C.MODEL.EXTRA.STAGE2.NUM_BRANCHES = 2\n# _C.MODEL.EXTRA.STAGE2.BLOCK = 'BASIC'\n# _C.MODEL.EXTRA.STAGE2.NUM_BLOCKS = [4, 4]\n# _C.MODEL.EXTRA.STAGE2.NUM_CHANNELS = [48, 96]\n# _C.MODEL.EXTRA.STAGE2.FUSE_METHOD = 'SUM'\n#\n# # stage3", "#\n# # stage3\n# _C.MODEL.EXTRA.STAGE3 = CN()\n# _C.MODEL.EXTRA.STAGE3.NUM_MODULES = 4\n# _C.MODEL.EXTRA.STAGE3.NUM_BRANCHES = 3\n# _C.MODEL.EXTRA.STAGE3.BLOCK = 'BASIC'\n# _C.MODEL.EXTRA.STAGE3.NUM_BLOCKS = [4, 4, 4]\n# _C.MODEL.EXTRA.STAGE3.NUM_CHANNELS = [48, 96, 192]\n# _C.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n#", "# _C.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n#\n# # stage4\n# _C.MODEL.EXTRA.STAGE4 = CN()\n# _C.MODEL.EXTRA.STAGE4.NUM_MODULES = 3\n# _C.MODEL.EXTRA.STAGE4.NUM_BRANCHES = 4\n# _C.MODEL.EXTRA.STAGE4.BLOCK = 'BASIC'\n# _C.MODEL.EXTRA.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\n# _C.MODEL.EXTRA.STAGE4.NUM_CHANNELS = [48, 96, 192, 384]\n# _C.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'", "# _C.MODEL.EXTRA.STAGE4.NUM_CHANNELS = [48, 96, 192, 384]\n# _C.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'\n\n_C.MODEL = CN()\n_C.MODEL.EXTRA = CN(new_allowed=True)\n_C.MODEL.PRETRAINED_LAYERS = ['*']\n_C.MODEL.STEM_INPLANES = 64\n_C.MODEL.FINAL_CONV_KERNEL = 1\n_C.MODEL.WITH_HEAD = True\n", "_C.MODEL.WITH_HEAD = True\n\n_C.MODEL.EXTRA.STAGE2 = CN()\n_C.MODEL.EXTRA.STAGE2.NUM_MODULES = 1\n_C.MODEL.EXTRA.STAGE2.NUM_BRANCHES = 2\n_C.MODEL.EXTRA.STAGE2.NUM_BLOCKS = [4, 4]\n_C.MODEL.EXTRA.STAGE2.NUM_CHANNELS = [48, 96]\n_C.MODEL.EXTRA.STAGE2.BLOCK = 'BASIC'\n_C.MODEL.EXTRA.STAGE2.FUSE_METHOD = 'SUM'\n", "_C.MODEL.EXTRA.STAGE2.FUSE_METHOD = 'SUM'\n\n_C.MODEL.EXTRA.STAGE3 = CN()\n_C.MODEL.EXTRA.STAGE3.NUM_MODULES = 4\n_C.MODEL.EXTRA.STAGE3.NUM_BRANCHES = 3\n_C.MODEL.EXTRA.STAGE3.NUM_BLOCKS = [4, 4, 4]\n_C.MODEL.EXTRA.STAGE3.NUM_CHANNELS = [48, 96, 192]\n_C.MODEL.EXTRA.STAGE3.BLOCK = 'BASIC'\n_C.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n", "_C.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n\n_C.MODEL.EXTRA.STAGE4 = CN()\n_C.MODEL.EXTRA.STAGE4.NUM_MODULES = 3\n_C.MODEL.EXTRA.STAGE4.NUM_BRANCHES = 4\n_C.MODEL.EXTRA.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\n_C.MODEL.EXTRA.STAGE4.NUM_CHANNELS = [48, 96, 192, 384]\n_C.MODEL.EXTRA.STAGE4.BLOCK = 'BASIC'\n_C.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'\n", "_C.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'\n\n\n\n"]}
{"filename": "MCCL/config.py", "chunked_list": ["import os\n\n\nclass Config():\n    def __init__(self) -> None:\n        # Backbone\n        self.bb = ['cnn-vgg16', 'cnn-vgg16bn', 'cnn-resnet50', 'trans-pvt'][3]\n        self.pvt_weights = ['../bb_weights/pvt_v2_b2.pth', ''][0]\n        # BN\n        self.use_bn = self.bb not in ['cnn-vgg16']\n        # Augmentation\n        self.preproc_methods = ['flip', 'enhance', 'rotate', 'crop', 'pepper'][:3]\n\n        # Components\n        self.consensus = ['', 'GCAM', 'GWM', 'SGS'][1]\n        self.dec_blk = ['ResBlk'][0]\n        self.GCAM_metric = ['online', 'offline', ''][0] if self.consensus else ''\n        # Training\n        self.batch_size = 48\n        self.loadN = 2\n        self.dec_att = ['', 'ASPP'][0]\n        self.auto_pad = ['', 'fixed', 'adaptive'][0]\n        self.optimizer = ['Adam', 'AdamW'][1]\n        self.lr = 1e-4\n        self.freeze = True\n        self.lr_decay_epochs = [-20]    # Set to negative N to decay the lr in the last N-th epoch.\n        self.forward_per_dataset = True\n        # Adv\n        self.lambda_adv_g = 10. * 1        # turn to 0 to avoid adv training\n        self.lambda_adv_d = 3. * (self.lambda_adv_g > 0)\n        # Loss\n        losses = ['sal']\n        self.loss = losses[:]\n        self.cls_mask_operation = ['x', '+', 'c'][0]\n        # Loss + Triplet Loss\n        self.lambdas_sal_last = {\n            # not 0 means opening this loss\n            # original rate -- 1 : 30 : 1.5 : 0.2, bce x 30\n            'bce': 30 * 1,          # high performance\n            'iou': 0.5 * 1,         # 0 / 255\n            'ssim': 1 * 0,          # help contours\n            'mse': 150 * 0,         # can smooth the saliency map\n            'reg': 100 * 0,\n            'triplet': 3 * 1,\n        }\n\n        self.db_output_decoder = False\n        self.refine = False\n        self.db_output_refiner = False\n        # Triplet Loss\n        self.triplet = ['_x5', 'mask'][:1]\n        self.triplet_loss_margin = 0.1\n\n\n        # Intermediate Layers\n        self.lambdas_sal_others = {\n            'bce': 0,\n            'iou': 0.,\n            'ssim': 0,\n            'mse': 0,\n            'reg': 0,\n            'triplet': 0,\n        }\n        self.output_number = 1\n        self.loss_sal_layers = 4              # used to be last 4 layers\n        self.loss_cls_mask_last_layers = 1         # used to be last 4 layers\n        if 'keep in range':\n            self.loss_sal_layers = min(self.output_number, self.loss_sal_layers)\n            self.loss_cls_mask_last_layers = min(self.output_number, self.loss_cls_mask_last_layers)\n            self.output_number = min(self.output_number, max(self.loss_sal_layers, self.loss_cls_mask_last_layers))\n            if self.output_number == 1:\n                for cri in self.lambdas_sal_others:\n                    self.lambdas_sal_others[cri] = 0\n        self.conv_after_itp = False\n        self.complex_lateral_connection = False\n\n        # to control the quantitive level of each single loss by number of output branches.\n        self.loss_cls_mask_ratio_by_last_layers = 4 / self.loss_cls_mask_last_layers\n        for loss_sal in self.lambdas_sal_last.keys():\n            loss_sal_ratio_by_last_layers = 4 / (int(bool(self.lambdas_sal_others[loss_sal])) * (self.loss_sal_layers - 1) + 1)\n            self.lambdas_sal_last[loss_sal] *= loss_sal_ratio_by_last_layers\n            self.lambdas_sal_others[loss_sal] *= loss_sal_ratio_by_last_layers\n        self.lambda_cls_mask = 2.5 * self.loss_cls_mask_ratio_by_last_layers\n        self.lambda_cls = 3.\n        self.lambda_contrast = 250.\n\n        # others\n        self.self_supervision = False\n        self.label_smoothing = False\n\n        self.validation = False\n        self.rand_seed = 7", "        # run_sh_file = [f for f in os.listdir('.') if 'go' in f and '.sh' in f] + [os.path.join('..', f) for f in os.listdir('..') if 'gco' in f and '.sh' in f]\n        # with open(run_sh_file[0], 'r') as f:\n        #     lines = f.readlines()\n        #     self.val_last = int([l.strip() for l in lines if 'val_last=' in l][0].split('=')[-1])\n        #     self.save_step = int([l.strip() for l in lines if 'step=' in l][0].split('=')[-1])\n"]}
{"filename": "MCCL/models/pvt.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\nfrom timm.models.vision_transformer import _cfg\nfrom timm.models.registry import register_model\n", "from timm.models.registry import register_model\n\nimport math\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        x = self.fc1(x)\n        x = self.dwconv(x, H, W)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x", "\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n        super().__init__()\n        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        self.sr_ratio = sr_ratio\n        if sr_ratio > 1:\n            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n            self.norm = nn.LayerNorm(dim)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n\n        if self.sr_ratio > 1:\n            x_ = x.permute(0, 2, 1).reshape(B, C, H, W)\n            x_ = self.sr(x_).reshape(B, C, -1).permute(0, 2, 1)\n            x_ = self.norm(x_)\n            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        else:\n            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        k, v = kv[0], kv[1]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n\n        return x", "\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x, H, W):\n        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))\n\n        return x", "\n\nclass OverlapPatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n        self.num_patches = self.H * self.W\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        self.norm = nn.LayerNorm(embed_dim)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.proj(x)\n        _, _, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)\n        x = self.norm(x)\n\n        return x, H, W", "\n\nclass PyramidVisionTransformerImpr(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512],\n                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm,\n                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n        super().__init__()\n        self.num_classes = num_classes\n        self.depths = depths\n\n        # patch_embed\n        self.patch_embed1 = OverlapPatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n                                              embed_dim=embed_dims[0])\n        self.patch_embed2 = OverlapPatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n                                              embed_dim=embed_dims[1])\n        self.patch_embed3 = OverlapPatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n                                              embed_dim=embed_dims[2])\n        self.patch_embed4 = OverlapPatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n                                              embed_dim=embed_dims[3])\n\n        # transformer encoder\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n        cur = 0\n        self.block1 = nn.ModuleList([Block(\n            dim=embed_dims[0], num_heads=num_heads[0], mlp_ratio=mlp_ratios[0], qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[0])\n            for i in range(depths[0])])\n        self.norm1 = norm_layer(embed_dims[0])\n\n        cur += depths[0]\n        self.block2 = nn.ModuleList([Block(\n            dim=embed_dims[1], num_heads=num_heads[1], mlp_ratio=mlp_ratios[1], qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[1])\n            for i in range(depths[1])])\n        self.norm2 = norm_layer(embed_dims[1])\n\n        cur += depths[1]\n        self.block3 = nn.ModuleList([Block(\n            dim=embed_dims[2], num_heads=num_heads[2], mlp_ratio=mlp_ratios[2], qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[2])\n            for i in range(depths[2])])\n        self.norm3 = norm_layer(embed_dims[2])\n\n        cur += depths[2]\n        self.block4 = nn.ModuleList([Block(\n            dim=embed_dims[3], num_heads=num_heads[3], mlp_ratio=mlp_ratios[3], qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[cur + i], norm_layer=norm_layer,\n            sr_ratio=sr_ratios[3])\n            for i in range(depths[3])])\n        self.norm4 = norm_layer(embed_dims[3])\n\n        # classification head\n        # self.head = nn.Linear(embed_dims[3], num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n            fan_out //= m.groups\n            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n            if m.bias is not None:\n                m.bias.data.zero_()\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = 1\n            #load_checkpoint(self, pretrained, map_location='cpu', strict=False, logger=logger)\n\n    def reset_drop_path(self, drop_path_rate):\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n        cur = 0\n        for i in range(self.depths[0]):\n            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[0]\n        for i in range(self.depths[1]):\n            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[1]\n        for i in range(self.depths[2]):\n            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n\n        cur += self.depths[2]\n        for i in range(self.depths[3]):\n            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n\n    def freeze_patch_emb(self):\n        self.patch_embed1.requires_grad = False\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed1', 'pos_embed2', 'pos_embed3', 'pos_embed4', 'cls_token'}  # has pos_embed may be better\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    # def _get_pos_embed(self, pos_embed, patch_embed, H, W):\n    #     if H * W == self.patch_embed1.num_patches:\n    #         return pos_embed\n    #     else:\n    #         return F.interpolate(\n    #             pos_embed.reshape(1, patch_embed.H, patch_embed.W, -1).permute(0, 3, 1, 2),\n    #             size=(H, W), mode=\"bilinear\").reshape(1, -1, H * W).permute(0, 2, 1)\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        outs = []\n\n        # stage 1\n        x, H, W = self.patch_embed1(x)\n        for i, blk in enumerate(self.block1):\n            x = blk(x, H, W)\n        x = self.norm1(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 2\n        x, H, W = self.patch_embed2(x)\n        for i, blk in enumerate(self.block2):\n            x = blk(x, H, W)\n        x = self.norm2(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 3\n        x, H, W = self.patch_embed3(x)\n        for i, blk in enumerate(self.block3):\n            x = blk(x, H, W)\n        x = self.norm3(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        # stage 4\n        x, H, W = self.patch_embed4(x)\n        for i, blk in enumerate(self.block4):\n            x = blk(x, H, W)\n        x = self.norm4(x)\n        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n        outs.append(x)\n\n        return outs\n\n        # return x.mean(dim=1)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        # x = self.head(x)\n\n        return x", "\n\nclass DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super(DWConv, self).__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W)\n        x = self.dwconv(x)\n        x = x.flatten(2).transpose(1, 2)\n\n        return x", "\n\ndef _conv_filter(state_dict, patch_size=16):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    for k, v in state_dict.items():\n        if 'patch_embed.proj.weight' in k:\n            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n        out_dict[k] = v\n\n    return out_dict", "\n\n@register_model\nclass pvt_v2_b0(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b0, self).__init__(\n            patch_size=4, embed_dims=[32, 64, 160, 256], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n", "\n\n\n@register_model\nclass pvt_v2_b1(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b1, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)", "\n@register_model\nclass pvt_v2_b2(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b2, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n\n@register_model\nclass pvt_v2_b3(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b3, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)", "\n@register_model\nclass pvt_v2_b3(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b3, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n\n@register_model\nclass pvt_v2_b4(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b4, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)", "\n@register_model\nclass pvt_v2_b4(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b4, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n\n", "\n\n@register_model\nclass pvt_v2_b5(PyramidVisionTransformerImpr):\n    def __init__(self, **kwargs):\n        super(pvt_v2_b5, self).__init__(\n            patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 6, 40, 3], sr_ratios=[8, 4, 2, 1],\n            drop_rate=0.0, drop_path_rate=0.1)\n", ""]}
{"filename": "MCCL/models/modules.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# import fvcore.nn.weight_init as weight_init\nfrom functools import partial\n\nfrom MCCL.config import Config\n\n", "\n\nconfig = Config()\n\n\nclass ResBlk(nn.Module):\n    def __init__(self, channel_in=64, channel_out=64, groups=0):\n        super(ResBlk, self).__init__()\n        self.conv_in = nn.Conv2d(channel_in, 64, 3, 1, 1)\n        self.relu_in = nn.ReLU(inplace=True)\n        if config.dec_att == 'ASPP':\n            self.dec_att = ASPP(channel_in=64)\n        self.conv_out = nn.Conv2d(64, channel_out, 3, 1, 1)\n        if config.use_bn:\n            self.bn_in = nn.BatchNorm2d(64)\n            self.bn_out = nn.BatchNorm2d(channel_out)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        if config.use_bn:\n            x = self.bn_in(x)\n        x = self.relu_in(x)\n        if config.dec_att:\n            x = self.dec_att(x)\n        x = self.conv_out(x)\n        if config.use_bn:\n            x = self.bn_out(x)\n        return x", "\n\nclass _ASPPModule(nn.Module):\n    def __init__(self, channel_in, planes, kernel_size, padding, dilation):\n        super(_ASPPModule, self).__init__()\n        self.atrous_conv = nn.Conv2d(channel_in, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=dilation, bias=False)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.atrous_conv(x)\n        x = self.bn(x)\n\n        return self.relu(x)", "\nclass ASPP(nn.Module):\n    def __init__(self, channel_in=64, output_stride=16):\n        super(ASPP, self).__init__()\n        self.down_scale = 1\n        self.channel_inter = 256 // self.down_scale\n        if output_stride == 16:\n            dilations = [1, 6, 12, 18]\n        elif output_stride == 8:\n            dilations = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = _ASPPModule(channel_in, self.channel_inter, 1, padding=0, dilation=dilations[0])\n        self.aspp2 = _ASPPModule(channel_in, self.channel_inter, 3, padding=dilations[1], dilation=dilations[1])\n        self.aspp3 = _ASPPModule(channel_in, self.channel_inter, 3, padding=dilations[2], dilation=dilations[2])\n        self.aspp4 = _ASPPModule(channel_in, self.channel_inter, 3, padding=dilations[3], dilation=dilations[3])\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(channel_in, self.channel_inter, 1, stride=1, bias=False),\n                                             nn.BatchNorm2d(self.channel_inter),\n                                             nn.ReLU(inplace=True))\n        self.conv1 = nn.Conv2d(self.channel_inter * 5, channel_in, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(channel_in)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        return self.dropout(x)", "\n\nclass CoAttLayer(nn.Module):\n    def __init__(self, channel_in=512):\n        super(CoAttLayer, self).__init__()\n\n        self.all_attention = GCAM(channel_in)\n    \n    def forward(self, x):\n        if self.training:\n            if config.loadN > 1:\n                channel_per_class = x.shape[0] // config.loadN\n                x_per_class_corr_list = []\n                for idx in range(0, x.shape[0], channel_per_class):\n                    x_per_class = x[idx:idx+channel_per_class]\n\n                    x_new_per_class = self.all_attention(x_per_class)\n\n                    x_per_class_proto = torch.mean(x_new_per_class, (0, 2, 3), True).view(1, -1)\n                    x_per_class_proto = x_per_class_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\n                    x_per_class_corr = x_per_class * x_per_class_proto\n                    x_per_class_corr_list.append(x_per_class_corr)\n                weighted_x = torch.cat(x_per_class_corr_list, dim=0)\n            else:\n                x_new = self.all_attention(x)\n                x_proto = torch.mean(x_new, (0, 2, 3), True).view(1, -1)\n                x_proto = x_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n                weighted_x = x * x_proto\n\n        else:\n            x_new = self.all_attention(x)\n            x_proto = torch.mean(x_new, (0, 2, 3), True).view(1, -1)\n            x_proto = x_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n            weighted_x = x * x_proto\n        return weighted_x", "\n\nclass GCAM(nn.Module):\n    def __init__(self, channel_in=512):\n\n        super(GCAM, self).__init__()\n        self.query_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n        self.key_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\n        self.scale = 1.0 / (channel_in ** 0.5)\n\n        self.conv6 = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\n        # for layer in [self.query_transform, self.key_transform, self.conv6]:\n        #     weight_init.c2_msra_fill(layer)\n\n\n    def forward(self, x):\n        # x: B,C,H,W\n        # x_query: B,C,HW\n        B, C, H5, W5 = x.size()\n\n        x_query = self.query_transform(x).view(B, C, -1)\n\n        # x_query: B,HW,C\n        x_query = torch.transpose(x_query, 1, 2).contiguous().view(-1, C) # BHW, C\n        # x_key: B,C,HW\n        x_key = self.key_transform(x).view(B, C, -1)\n\n        x_key = torch.transpose(x_key, 0, 1).contiguous().view(C, -1) # C, BHW\n\n        # W = Q^T K: B,HW,HW\n        x_w = torch.matmul(x_query, x_key) #* self.scale # BHW, BHW\n        x_w = x_w.view(B*H5*W5, B, H5*W5)\n        x_w = torch.max(x_w, -1).values # BHW, B\n        x_w = x_w.mean(-1)\n        #x_w = torch.mean(x_w, -1).values # BHW\n        x_w = x_w.view(B, -1) * self.scale # B, HW\n        x_w = F.softmax(x_w, dim=-1) # B, HW\n        x_w = x_w.view(B, H5, W5).unsqueeze(1) # B, 1, H, W\n \n        x = x * x_w\n        x = self.conv6(x)\n\n        return x", ""]}
{"filename": "MCCL/models/GCoNet.py", "chunked_list": ["from collections import OrderedDict\nimport torch\nfrom torch.functional import norm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import vgg16, vgg16_bn\nfrom torchvision.models import resnet50\n\nfrom MCCL.models.modules import ResBlk, CoAttLayer\nfrom MCCL.models.pvt import pvt_v2_b2", "from MCCL.models.modules import ResBlk, CoAttLayer\nfrom MCCL.models.pvt import pvt_v2_b2\nfrom MCCL.config import Config\n\n\nclass MCCL(nn.Module):\n    def __init__(self):\n        super(MCCL, self).__init__()\n        self.config = Config()\n        bb = self.config.bb\n        if bb == 'cnn-vgg16':\n            bb_net = list(vgg16(pretrained=False).children())[0]\n            bb_convs = OrderedDict({\n                'conv1': bb_net[:4],\n                'conv2': bb_net[4:9],\n                'conv3': bb_net[9:16],\n                'conv4': bb_net[16:23]\n            })\n        elif bb == 'cnn-vgg16bn':\n            bb_net = list(vgg16_bn(pretrained=False).children())[0]\n            bb_convs = OrderedDict({\n                'conv1': bb_net[:6],\n                'conv2': bb_net[6:13],\n                'conv3': bb_net[13:23],\n                'conv4': bb_net[23:33]\n            })\n        elif bb == 'cnn-resnet50':\n            bb_net = list(resnet50(pretrained=False).children())\n            bb_convs = OrderedDict({\n                'conv1': nn.Sequential(*bb_net[0:3]),\n                'conv2': bb_net[4],\n                'conv3': bb_net[5],\n                'conv4': bb_net[6]\n            })\n        elif bb == 'trans-pvt':\n            self.bb = pvt_v2_b2()\n            # if self.config.pvt_weights:\n            #     save_model = torch.load(self.config.pvt_weights)\n            #     model_dict = self.bb.state_dict()\n            #     state_dict = {k: v for k, v in save_model.items() if k in model_dict.keys()}\n            #     model_dict.update(state_dict)\n            #     self.bb.load_state_dict(model_dict)\n\n        if 'cnn-' in bb:\n            self.bb = nn.Sequential(bb_convs)\n        lateral_channels_in = {\n            'cnn-vgg16': [512, 256, 128, 64],\n            'cnn-vgg16bn': [512, 256, 128, 64],\n            'cnn-resnet50': [1024, 512, 256, 64],\n            'trans-pvt': [512, 320, 128, 64],\n        }\n\n        if self.config.consensus == 'GCAM':\n            self.co_x4 = CoAttLayer(channel_in=lateral_channels_in[bb][0])\n        elif self.config.consensus == 'SGS':\n            self.co_x4 = SGS(channel_in=lateral_channels_in[bb][0])\n        elif self.config.consensus == 'GWM':\n            self.co_x4 = GWM(channel_in=lateral_channels_in[bb][0])\n\n        if self.config.dec_blk == 'ResBlk':\n            DecBlk = ResBlk\n\n        self.top_layer = DecBlk(lateral_channels_in[bb][0], lateral_channels_in[bb][1])\n\n        self.dec_layer4 = DecBlk(lateral_channels_in[bb][1], lateral_channels_in[bb][1])\n        self.lat_layer4 = nn.Conv2d(lateral_channels_in[bb][1], lateral_channels_in[bb][1], 1, 1, 0)\n\n        self.dec_layer3 = DecBlk(lateral_channels_in[bb][1], lateral_channels_in[bb][2])\n        self.lat_layer3 = nn.Conv2d(lateral_channels_in[bb][2], lateral_channels_in[bb][2], 1, 1, 0)\n\n        self.dec_layer2 = DecBlk(lateral_channels_in[bb][2], lateral_channels_in[bb][3])\n        self.lat_layer2 = nn.Conv2d(lateral_channels_in[bb][3], lateral_channels_in[bb][3], 1, 1, 0)\n\n        self.dec_layer1 = DecBlk(lateral_channels_in[bb][3], lateral_channels_in[bb][3]//2)\n        self.conv_out1 = nn.Sequential(nn.Conv2d(lateral_channels_in[bb][3]//2, 1, 1, 1, 0))\n\n\n    def forward(self, x):\n        ########## Encoder ##########\n\n        if 'trans' in self.config.bb:\n            x1, x2, x3, x4 = self.bb(x)\n        else:\n            x1 = self.bb.conv1(x)\n            x2 = self.bb.conv2(x1)\n            x3 = self.bb.conv3(x2)\n            x4 = self.bb.conv4(x3)\n\n        if self.config.consensus:\n            x4 = self.co_x4(x4)\n        p4 = self.top_layer(x4)\n\n        ########## Decoder ##########\n        scaled_preds = []\n\n        p4 = self.dec_layer4(p4)\n        p4 = F.interpolate(p4, size=x3.shape[2:], mode='bilinear', align_corners=True)\n        p3 = p4 + self.lat_layer4(x3)\n\n        p3 = self.dec_layer3(p3)\n        p3 = F.interpolate(p3, size=x2.shape[2:], mode='bilinear', align_corners=True)\n        p2 = p3 + self.lat_layer3(x2)\n\n        p2 = self.dec_layer2(p2)\n        p2 = F.interpolate(p2, size=x1.shape[2:], mode='bilinear', align_corners=True)\n        p1 = p2 + self.lat_layer2(x1)\n\n        p1 = self.dec_layer1(p1)\n        p1 = F.interpolate(p1, size=x.shape[2:], mode='bilinear', align_corners=True)\n        if self.config.db_output_decoder:\n            p1_out = self.db_output_decoder(p1)\n        else:\n            p1_out = self.conv_out1(p1)\n        scaled_preds.append(p1_out)\n\n        if self.training:\n            return_values = [scaled_preds, x4]\n            return return_values\n        else:\n            return scaled_preds", ""]}
{"filename": "CADC/parameter.py", "chunked_list": ["import os\n\n################# Training #################\n# Your path for pretrained vgg model\nload_model = './pretrained_models/vgg16_20M.caffemodel.pth'\n# Your path for COCO9213\nimg_root_coco = './Data/COCO9213-os/img/'\ngt_root_coco = './Data/COCO9213-os/gt/'\n# Your path for DUTS Class\nimg_root = './Data/DUTS_class/img/'", "# Your path for DUTS Class\nimg_root = './Data/DUTS_class/img/'\ngt_root = './Data/DUTS_class/gt/'\n# Your path for our synethsis data\nimg_syn_root = './Data/DUTS_class_syn/img_png_seamless_cloning_add_naive/img/'\nimg_ReverseSyn_root = './Data/DUTS_class_syn/img_png_seamless_cloning_add_naive_reverse/img/'\ngt_ReverseSyn_root = './Data/DUTS_class_syn/img_png_seamless_cloning_add_naive_reverse/gt/'\n\n# save model path\nsave_model_dir = './checkpoint/'", "# save model path\nsave_model_dir = './checkpoint/'\n# if not os.path.exists(save_model_dir):\n#     os.makedirs(save_model_dir)\n\n# settings\ngpu_id = \"0\"\nmax_num = 14\ndec_channels = 64\n", "dec_channels = 64\n\nimg_size = 256\nscale_size = 288\nbatch_size = 1\nlr = 0.01\nepochs = 300\ntrain_steps = 40000\nlr_decay_gamma = 0.1\nstepvalue1 = 20000", "lr_decay_gamma = 0.1\nstepvalue1 = 20000\nstepvalue2 = 30000\nloss_weights = [1, 0.8, 0.8, 0.5, 0.5, 0.5]\nbn_momentum = 0.001\n\n\n################# Testing #################\n# save output path\n# save_test_path_root = './Preds/'", "# save output path\n# save_test_path_root = './Preds/'\n# if not os.path.exists(save_test_path_root):\n#     os.makedirs(save_test_path_root)\n\n# testing your own trained model\ntest_model = 'iterations40000.pth'\ntest_model_dir = save_model_dir + test_model\n\n# testing our pretrained CADC model", "\n# testing our pretrained CADC model\n# test_model_dir = 'checkpoint/CADC.pth'\n\n# test dataset path\ntest_dir_img = ['./Data/CoCA/image',\n                './Data/CoSal2015/Image',\n                './Data/CoSOD3k/Image',\n                './Data/MSRC/Image',\n                ]", "                './Data/MSRC/Image',\n                ]\n\n"]}
{"filename": "CADC/ImageBranchEncoder/__init__.py", "chunked_list": ["from .ImageBranchEncoder import ImageBranchEncoder"]}
{"filename": "CADC/ImageBranchEncoder/ImageBranchEncoder.py", "chunked_list": ["import torch.nn as nn\n\n\nclass ImageBranchEncoder(nn.Module):\n    def __init__(self, n_channels):\n\n        super(ImageBranchEncoder, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(n_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n        )\n        self.conv2 = nn.Sequential(\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n        )\n        self.conv3 = nn.Sequential(\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True),\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n        )\n        self.conv4 = nn.Sequential(\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True),\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n        )\n        self.conv5 = nn.Sequential(\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, dilation=2, stride=1, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, dilation=2, stride=1, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, dilation=2, stride=1, padding=2),\n        )\n        self.fc6 = nn.Sequential(\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, dilation=12, padding=12),\n            nn.ReLU(inplace=True),\n        )\n\n        self.dropout = nn.Dropout(0.5)\n\n        self.fc7 = nn.Sequential(\n            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=1),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        out_conv1 = self.conv1(x)\n        out_conv2 = self.conv2(out_conv1)\n        out_conv3 = self.conv3(out_conv2)\n        out_conv4 = self.conv4(out_conv3)\n        out_conv5 = self.conv5(out_conv4)\n        x6 = self.fc6(out_conv5)\n        x7 = self.fc7(self.dropout(x6))\n\n        return out_conv1, out_conv2, out_conv3, out_conv4, out_conv5, x7", ""]}
{"filename": "CADC/ImageBranchDecoder/ImageBranchDecoder.py", "chunked_list": ["import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom CADC.parameter import *\nfrom CADC.Modules import KernelModule\nfrom CADC.Modules import Transformer\nfrom CADC.Modules import poolingModule\n\n\nclass spatialAttDecoder_module(nn.Module):\n    def __init__(self, in_channels, out_channels, fusing=True):\n        super(spatialAttDecoder_module, self).__init__()\n        if fusing:\n            self.enc_fea_proc = nn.Sequential(\n                nn.BatchNorm2d(in_channels, momentum=bn_momentum),\n                nn.ReLU(inplace=True),\n            )\n            in_channels = in_channels + dec_channels\n        self.decoding1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n            nn.ReLU(inplace=True),\n        )\n        self.decoding2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n            nn.ReLU(inplace=True),\n        )\n        self.SpatialAtt = nn.Sequential(\n            nn.Conv2d(out_channels, 1, kernel_size=3, stride=1, padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, enc_fea, dec_fea=None):\n        if dec_fea is not None:\n            enc_fea = self.enc_fea_proc(enc_fea)\n            if dec_fea.size(2) != enc_fea.size(2):\n                dec_fea = F.upsample(dec_fea, size=[enc_fea.size(2), enc_fea.size(3)], mode='bilinear', align_corners=True)\n\n            spatial_att = self.SpatialAtt(dec_fea)\n            enc_fea = enc_fea*spatial_att\n            enc_fea = torch.cat([enc_fea, dec_fea], dim=1)\n        output = self.decoding1(enc_fea)\n        output = self.decoding2(output)\n\n        return output", "\nclass spatialAttDecoder_module(nn.Module):\n    def __init__(self, in_channels, out_channels, fusing=True):\n        super(spatialAttDecoder_module, self).__init__()\n        if fusing:\n            self.enc_fea_proc = nn.Sequential(\n                nn.BatchNorm2d(in_channels, momentum=bn_momentum),\n                nn.ReLU(inplace=True),\n            )\n            in_channels = in_channels + dec_channels\n        self.decoding1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n            nn.ReLU(inplace=True),\n        )\n        self.decoding2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n            nn.ReLU(inplace=True),\n        )\n        self.SpatialAtt = nn.Sequential(\n            nn.Conv2d(out_channels, 1, kernel_size=3, stride=1, padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, enc_fea, dec_fea=None):\n        if dec_fea is not None:\n            enc_fea = self.enc_fea_proc(enc_fea)\n            if dec_fea.size(2) != enc_fea.size(2):\n                dec_fea = F.upsample(dec_fea, size=[enc_fea.size(2), enc_fea.size(3)], mode='bilinear', align_corners=True)\n\n            spatial_att = self.SpatialAtt(dec_fea)\n            enc_fea = enc_fea*spatial_att\n            enc_fea = torch.cat([enc_fea, dec_fea], dim=1)\n        output = self.decoding1(enc_fea)\n        output = self.decoding2(output)\n\n        return output", "\n\nclass decoder_module(nn.Module):\n    def __init__(self, in_channels, out_channels, fusing=True):\n        super(decoder_module, self).__init__()\n        if fusing:\n            self.enc_fea_proc = nn.Sequential(\n                nn.BatchNorm2d(in_channels, momentum=bn_momentum),\n                nn.ReLU(inplace=True),\n            )\n            self.poolingModule = poolingModule()\n            self.Transformer = Transformer(in_channels)\n\n            self.bn_relu = nn.Sequential(\n                nn.BatchNorm1d(in_channels),\n                nn.ReLU(inplace=True),\n            )\n            self.KernelModule = KernelModule(in_channels)\n\n            in_channels = 3*dec_channels\n\n        self.decoding1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n            nn.ReLU(inplace=True),\n        )\n        self.decoding2 = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(out_channels, momentum=bn_momentum),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, enc_fea, dec_fea=None):\n        if dec_fea is not None:\n\n            # [1] Consensus Feature Aggregation\n            output = self.poolingModule(enc_fea) # [N, C, 46]\n            Transformer_output, affinity = self.Transformer(output)\n            # Transformer_output [N, 46, C]   affinity (Nx46)x(Nx46)\n            Transformer_output = self.bn_relu(Transformer_output.permute(0, 2, 1)) # [N, C, 46]\n\n            # [2] Consensus-aware Kernel Construction and Search\n            enc_fea = self.enc_fea_proc(enc_fea)\n            enc_fea = self.KernelModule(Transformer_output, enc_fea, affinity)\n\n            if dec_fea.size(2) != enc_fea.size(2):\n                dec_fea = F.upsample(dec_fea, size=[enc_fea.size(2), enc_fea.size(3)], mode='bilinear', align_corners=True)\n            enc_fea = torch.cat([enc_fea, dec_fea], dim=1)\n        output = self.decoding1(enc_fea)\n        output = self.decoding2(output)\n\n        return output", "\n\nclass ImageBranchDecoder(nn.Module):\n    def __init__(self):\n\n        super(ImageBranchDecoder, self).__init__()\n        channels = [64, 128, 256, 512, 512, 512]\n\n        self.decoder6 = decoder_module(dec_channels*2, dec_channels, False)\n        self.decoder5 = decoder_module(channels[4], dec_channels)\n        self.decoder4 = decoder_module(channels[3], dec_channels)\n        self.decoder3 = decoder_module(channels[2], dec_channels)\n        self.decoder2 = spatialAttDecoder_module(channels[1], dec_channels)\n        self.decoder1 = spatialAttDecoder_module(channels[0], dec_channels)\n\n        self.conv_loss6 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n        self.conv_loss5 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n        self.conv_loss4 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n        self.conv_loss3 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n        self.conv_loss2 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n        self.conv_loss1 = nn.Conv2d(in_channels=dec_channels, out_channels=1, kernel_size=3, padding=1)\n\n    def forward(self, enc_fea, kenerled_afteraspp, Transformer_output, affinity):\n\n        encoder_conv1, encoder_conv2, encoder_conv3, encoder_conv4, encoder_conv5, x7 = enc_fea\n\n        dec_fea_6 = self.decoder6(kenerled_afteraspp)\n        mask6 = self.conv_loss6(dec_fea_6)\n\n        dec_fea_5 = self.decoder5(encoder_conv5, dec_fea_6)\n        mask5 = self.conv_loss5(dec_fea_5)\n\n        dec_fea_4 = self.decoder4(encoder_conv4, dec_fea_5)\n        mask4 = self.conv_loss4(dec_fea_4)\n\n        dec_fea_3 = self.decoder3(encoder_conv3, dec_fea_4)\n        mask3 = self.conv_loss3(dec_fea_3)\n\n        dec_fea_2 = self.decoder2(encoder_conv2, dec_fea_3)\n        mask2 = self.conv_loss2(dec_fea_2)\n\n        dec_fea_1 = self.decoder1(encoder_conv1, dec_fea_2)\n        mask1 = self.conv_loss1(dec_fea_1)\n\n        return mask6, mask5, mask4, mask3, mask2, mask1", ""]}
{"filename": "CADC/ImageBranchDecoder/__init__.py", "chunked_list": ["from .ImageBranchDecoder import ImageBranchDecoder"]}
{"filename": "CADC/CoSODNet/__init__.py", "chunked_list": ["from .CoSODNet import CoSODNet\n"]}
{"filename": "CADC/CoSODNet/CoSODNet.py", "chunked_list": ["from CADC.ImageBranchEncoder import ImageBranchEncoder\nfrom CADC.ImageBranchDecoder import ImageBranchDecoder\nimport torch.nn as nn\nimport torch\nfrom torch.nn import BatchNorm2d as bn\nfrom CADC.Modules import KernelModule\nfrom CADC.Modules import Transformer\nfrom CADC.Modules import poolingModule\n\n\nclass _DenseAsppBlock(nn.Sequential):\n    \"\"\" ConvNet block for building DenseASPP. \"\"\"\n\n    def __init__(self, input_num, num1, num2, dilation_rate):\n        super(_DenseAsppBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)\n        self.bn1 = bn(num1, momentum=0.0003)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n                               dilation=dilation_rate, padding=dilation_rate)\n        self.bn2 = bn(num2, momentum=0.0003)\n        self.relu2 = nn.ReLU(inplace=True)\n\n    def forward(self, input):\n\n        feature = self.relu1(self.bn1(self.conv1(input)))\n        feature = self.relu2(self.bn2(self.conv2(feature)))\n\n        return feature", "\n\nclass _DenseAsppBlock(nn.Sequential):\n    \"\"\" ConvNet block for building DenseASPP. \"\"\"\n\n    def __init__(self, input_num, num1, num2, dilation_rate):\n        super(_DenseAsppBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)\n        self.bn1 = bn(num1, momentum=0.0003)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n                               dilation=dilation_rate, padding=dilation_rate)\n        self.bn2 = bn(num2, momentum=0.0003)\n        self.relu2 = nn.ReLU(inplace=True)\n\n    def forward(self, input):\n\n        feature = self.relu1(self.bn1(self.conv1(input)))\n        feature = self.relu2(self.bn2(self.conv2(feature)))\n\n        return feature", "\n\nclass DASPPmodule(nn.Module):\n    def __init__(self):\n        super(DASPPmodule, self).__init__()\n        num_features = 512\n        d_feature1 = 176\n        d_feature0 = num_features//2\n\n        self.AvgPool = nn.Sequential(\n            nn.AvgPool2d([32, 32], [32, 32]),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Upsample(size=32, mode='nearest'),\n        )\n        self.ASPP_2 = _DenseAsppBlock(input_num=num_features, num1=d_feature0, num2=d_feature1,\n                                      dilation_rate=2)\n\n        self.ASPP_4 = _DenseAsppBlock(input_num=num_features + d_feature1 * 1, num1=d_feature0, num2=d_feature1,\n                                      dilation_rate=4)\n\n        self.ASPP_8 = _DenseAsppBlock(input_num=num_features + d_feature1 * 2, num1=d_feature0, num2=d_feature1,\n                                       dilation_rate=8)\n\n        self.afterASPP = nn.Sequential(\n            nn.Conv2d(in_channels=512*2 + 176*3, out_channels=512, kernel_size=1),)\n\n    def forward(self, encoder_fea):\n\n        imgAvgPool = self.AvgPool(encoder_fea)\n\n        aspp2 = self.ASPP_2(encoder_fea)\n        feature = torch.cat([aspp2, encoder_fea], dim=1)\n\n        aspp4 = self.ASPP_4(feature)\n        feature = torch.cat([aspp4, feature], dim=1)\n\n        aspp8 = self.ASPP_8(feature)\n        feature = torch.cat([aspp8, feature], dim=1)\n\n        asppFea = torch.cat([feature, imgAvgPool], dim=1)\n        AfterASPP = self.afterASPP(asppFea)\n\n        return AfterASPP", "\n\nclass CoSODNet(nn.Module):\n    def __init__(self, n_channels=3, mode='train'):\n        super(CoSODNet, self).__init__()\n\n        self.mode = mode\n        self.ImageBranchEncoder = ImageBranchEncoder(n_channels)\n\n        self.ImageBranch_fc7_1 = nn.Sequential(\n            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n        )\n        self.ImageBranch_DASPP = DASPPmodule()\n\n        self.poolingModule = poolingModule()\n        self.Transformer = Transformer(512)\n\n        self.bn_relu = nn.Sequential(\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n        )\n\n        self.KernelModule = KernelModule()\n\n        self.ImageBranchDecoder = ImageBranchDecoder()\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight),\n                nn.init.constant_(m.bias, 0),\n\n    def forward(self, image_Input):\n        if self.mode == 'train':\n            preds = self._train_forward(image_Input)\n        else:\n            preds = self._test_forward(image_Input)\n\n        return preds\n\n    def _train_forward(self, image_Input):\n        outputs_image = self._forward(image_Input)\n        return outputs_image\n\n    def _test_forward(self, image_Input):\n\n        with torch.no_grad():\n            outputs_image = self._forward(image_Input)\n            return outputs_image\n\n    def _forward(self, image_Input):\n        N, _, _, _ = image_Input.size()\n        image_feas = self.ImageBranchEncoder(image_Input)\n        afteraspp = self.ImageBranch_DASPP(self.ImageBranch_fc7_1(image_feas[-1]))\n\n        ####################### Consensus Feature Aggregation #######################\n        output = self.poolingModule(afteraspp)  # [N, C, 46]\n        Transformer_output, affinity = self.Transformer(output)\n        # Transformer_output [N, 46, C]   affinity (Nx46)x(Nx46)\n        Transformer_output = self.bn_relu(Transformer_output.permute(0, 2, 1))  # [N, C, 46]\n\n        _, cha, _ = output.size()\n\n        ####################### Consensus-aware Kernel Construction and Search #######################\n        kenerled_afteraspp = self.KernelModule(Transformer_output, afteraspp, affinity)\n\n        outputs_image = self.ImageBranchDecoder(image_feas, kenerled_afteraspp, Transformer_output, affinity)\n\n        return outputs_image\n\n    def init_parameters(self, pretrain_vgg16_1024):\n\n        conv_blocks = [self.ImageBranchEncoder.conv1,\n                       self.ImageBranchEncoder.conv2,\n                       self.ImageBranchEncoder.conv3,\n                       self.ImageBranchEncoder.conv4,\n                       self.ImageBranchEncoder.conv5,\n                       self.ImageBranchEncoder.fc6,\n                       self.ImageBranchEncoder.fc7]\n        listkey = [['conv1_1', 'conv1_2'], ['conv2_1', 'conv2_2'], ['conv3_1', 'conv3_2', 'conv3_3'],\n                   ['conv4_1', 'conv4_2', 'conv4_3'], ['conv5_1', 'conv5_2', 'conv5_3'], ['fc6'], ['fc7']]\n\n        for idx, conv_block in enumerate(conv_blocks):\n            num_conv = 0\n            for l2 in conv_block:\n                if isinstance(l2, nn.Conv2d):\n                    num_conv += 1\n                    l2.weight.data = pretrain_vgg16_1024[str(listkey[idx][num_conv - 1]) + '.weight']\n                    l2.bias.data = pretrain_vgg16_1024[str(listkey[idx][num_conv - 1]) + '.bias'].squeeze(0).squeeze(0).squeeze(0).squeeze(0)\n        return self", ""]}
{"filename": "CADC/Modules/Modules.py", "chunked_list": ["import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\nclass KernelModule(nn.Module):\n    def __init__(self, channel=512):\n        super(KernelModule, self).__init__()\n\n        self.encoder_fea_channel = channel\n        self.self_att = nn.Sequential(\n            nn.Linear(46 * self.encoder_fea_channel, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 46)\n        )\n\n        self.generate_depthwise_Adakernel = nn.Sequential(\n            nn.Linear(46, 46),\n            nn.BatchNorm1d(46),\n            nn.PReLU(num_parameters=1, init=0.1),\n            nn.Linear(46, 9)\n        )\n\n        self.generate_pointwise_Adakernel = nn.Sequential(\n            nn.Linear(self.encoder_fea_channel, self.encoder_fea_channel),\n            nn.BatchNorm1d(self.encoder_fea_channel),\n            nn.PReLU(num_parameters=1, init=0.1),\n            nn.Linear(self.encoder_fea_channel, self.encoder_fea_channel * 64)\n        )\n\n        self.att_depthwise_part1 = nn.Sequential(\n            nn.Linear(46 * self.encoder_fea_channel, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, self.encoder_fea_channel)\n        )\n\n        self.att_depthwise_part2 = nn.Sequential(\n            nn.Linear(46 * self.encoder_fea_channel, 1024),\n            nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 46)\n        )\n\n        self.generate_depthwise_Cokernel = nn.Sequential(\n            nn.Linear(46, 46),\n            nn.BatchNorm1d(46),\n            nn.PReLU(num_parameters=1, init=0.1),\n            nn.Linear(46, 9)\n        )\n\n        self.generate_pointwise_Cokernel = nn.Sequential(\n            nn.Linear(self.encoder_fea_channel, self.encoder_fea_channel),\n            nn.PReLU(num_parameters=1, init=0.1),\n            nn.Linear(self.encoder_fea_channel, self.encoder_fea_channel * 64)\n        )\n\n    def forward(self, output, enc_fea, affinity):\n\n        N, cha, _ = output.size()\n\n        ##################### Adaptive Kernel Construction #####################\n        # [1] depthwise adaptive kernels\n        depthwise_Adakernel = self.generate_depthwise_Adakernel(output.view(N * cha, 46))\n        depthwise_Adakernel = depthwise_Adakernel.reshape(N, cha, 3, 3)\n        depthwise_Adakernel = depthwise_Adakernel.unsqueeze(2)\n\n        # [2] pointwise adaptive kernels\n        att = F.softmax(self.self_att(output.view(-1, 46 * cha)), dim=1)  # N x 46\n        att = att.unsqueeze(2) # N x 46 x 1\n        fea = ((output.permute(0, 2, 1)) * att).sum(dim=1).reshape(N, cha) # N x 512\n        pointwise_Adakernel = self.generate_pointwise_Adakernel(fea)\n        pointwise_Adakernel = pointwise_Adakernel.reshape(N, 64, cha, 1, 1)\n\n\n        ##################### Common Kernel Construction #####################\n        # [1] pointwise common kernel\n        affinity_att = affinity.mean(dim=0)  # [N*46]\n        fea_for_pointwise = torch.matmul(affinity_att.unsqueeze(0),\n                                         output.permute(0, 2, 1).contiguous().view(N * 46, cha))  # [1, C]\n        pointwise_Cokernel = self.generate_pointwise_Cokernel(fea_for_pointwise)\n        pointwise_Cokernel = pointwise_Cokernel.reshape(64, cha, 1, 1)\n\n        # [2] depthwise common kernel\n        att_part1_depthwise = self.att_depthwise_part1(output.permute(0, 2, 1).contiguous().view(N, 46 * cha))\n        att_part1_depthwise = F.softmax(att_part1_depthwise, dim=1) # alpha1 [N, C]\n\n        att_part2_depthwise = self.att_depthwise_part2(output.permute(0, 2, 1).contiguous().view(N, 46 * cha))\n        att_part2_depthwise = F.softmax(att_part2_depthwise, dim=1)  # alpha2 [N, 46]\n\n        att_for_depthwise = (output * att_part1_depthwise.unsqueeze(2)).sum(dim=1) # [N, 46]\n        att_for_depthwise = (att_for_depthwise * att_part2_depthwise).sum(dim=1) # [N]\n        att_for_depthwise = F.softmax(att_for_depthwise).unsqueeze(0)  # alpha3 [1 x N]\n\n        # att_for_depthwise [1 x N]   output [N, C, 46]\n        fea_for_depthwise = torch.matmul(att_for_depthwise, output.view(N, cha * 46))  # [1, C*46]\n\n        depthwise_Cokernel = self.generate_depthwise_Cokernel(fea_for_depthwise.reshape(cha, 46))\n        depthwise_Cokernel = depthwise_Cokernel.reshape(cha, 3, 3)\n        depthwise_Cokernel = depthwise_Cokernel.unsqueeze(1)\n\n\n        ##################### Searching via Adaptive Kernel #####################\n        _, _, H, W = enc_fea.size()\n\n        Adpkenerled_enc_fea = torch.cuda.FloatTensor(N, 64, H, W)\n\n        for num in range(N):\n            tmp_fea = F.conv2d(enc_fea[num, :, :, :].unsqueeze(0), depthwise_Adakernel[num, :, :, :, :], stride=1,\n                               padding=1, groups=cha)\n            Adpkenerled_enc_fea[num, :, :, :] = F.conv2d(tmp_fea, pointwise_Adakernel[num, :, :, :, :], stride=1,\n                                                           padding=0)\n\n        ##################### Searching via Common Kernel #####################\n        Cokenerled_enc_fea = F.conv2d(enc_fea, depthwise_Cokernel, stride=1, padding=1, groups=cha)\n        Cokenerled_enc_fea = F.conv2d(Cokenerled_enc_fea, pointwise_Cokernel, stride=1, padding=0)\n\n        kenerled_afteraspp = torch.cat([Adpkenerled_enc_fea, Cokenerled_enc_fea], dim=1)\n\n        return kenerled_afteraspp", "\n\nclass Transformer(nn.Module):\n    def __init__(self, in_channels):\n        super(Transformer, self).__init__()\n\n        self.in_channels = in_channels\n        self.inter_channels = self.in_channels // 2\n\n        self.bn_relu = nn.Sequential(\n            nn.BatchNorm1d(self.in_channels),\n            nn.ReLU(inplace=True),\n        )\n\n        self.theta = nn.Linear(self.in_channels, self.inter_channels)\n        self.phi = nn.Linear(self.in_channels, self.inter_channels)\n        self.g = nn.Linear(self.in_channels, self.inter_channels)\n\n        self.W = nn.Linear(self.inter_channels, self.in_channels)\n\n    def forward(self, ori_feature):\n        # ori_feature N x C x 46\n        feature = self.bn_relu(ori_feature)\n        feature = feature.permute(0, 2, 1)\n\n        # feature N x 46 x C\n        N, num, c = feature.size()\n\n        x_theta = self.theta(feature.contiguous().view(-1, c))\n        x_phi = self.phi(feature.contiguous().view(-1, c))\n        x_phi = x_phi.permute(1, 0)\n        attention = torch.matmul(x_theta, x_phi)\n\n        for k in range(N):\n            attention[k*46:k*46+46, k*46:k*46+46] = -1000\n\n        # (Nx46)x(Nx46)\n        f_div_C = F.softmax(attention, dim=-1)\n\n        g_x = self.g((feature.contiguous().view(-1, c)))\n        y = torch.matmul(f_div_C, g_x)\n        # (Nx46)xc/2\n\n        W_y = self.W(y).contiguous().view(N, num, c)\n\n        att_fea = ori_feature.permute(0, 2, 1) + W_y\n\n        return att_fea, f_div_C", "\n\nclass poolingModule(nn.Module):\n    def __init__(self):\n        super(poolingModule, self).__init__()\n\n        self.maxpool1 = torch.nn.AdaptiveMaxPool2d((1,1))\n        self.maxpool2 = torch.nn.AdaptiveMaxPool2d((3,3))\n        self.maxpool3 = torch.nn.AdaptiveMaxPool2d((6,6))\n\n    def forward(self, feature):\n        batch_size, cha, _, _ = feature.size()\n        maxpool_fea1 = self.maxpool1(feature).view(batch_size, cha, -1)\n        maxpool_fea2 = self.maxpool2(feature).view(batch_size, cha, -1)\n        maxpool_fea3 = self.maxpool3(feature).view(batch_size, cha, -1)\n\n        maxpool_fea = torch.cat([maxpool_fea1, maxpool_fea2, maxpool_fea3], dim=2)\n\n        return maxpool_fea", "\n"]}
{"filename": "CADC/Modules/__init__.py", "chunked_list": ["from .Modules import KernelModule\nfrom .Modules import Transformer\nfrom .Modules import poolingModule"]}
{"filename": "DCFM/util.py", "chunked_list": ["import logging\nimport os\nimport torch\nimport shutil\nfrom torchvision import transforms\nimport numpy as np\nimport random\nimport cv2\n\n\nclass Logger():\n    def __init__(self, path=\"log.txt\"):\n        self.logger = logging.getLogger('DCFM')\n        self.file_handler = logging.FileHandler(path, \"w\")\n        self.stdout_handler = logging.StreamHandler()\n        self.stdout_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n        self.file_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n        self.logger.addHandler(self.file_handler)\n        self.logger.addHandler(self.stdout_handler)\n        self.logger.setLevel(logging.INFO)\n        self.logger.propagate = False\n    \n    def info(self, txt):\n        self.logger.info(txt)\n    \n    def close(self):\n        self.file_handler.close()\n        self.stdout_handler.close()", "\n\nclass Logger():\n    def __init__(self, path=\"log.txt\"):\n        self.logger = logging.getLogger('DCFM')\n        self.file_handler = logging.FileHandler(path, \"w\")\n        self.stdout_handler = logging.StreamHandler()\n        self.stdout_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n        self.file_handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(message)s'))\n        self.logger.addHandler(self.file_handler)\n        self.logger.addHandler(self.stdout_handler)\n        self.logger.setLevel(logging.INFO)\n        self.logger.propagate = False\n    \n    def info(self, txt):\n        self.logger.info(txt)\n    \n    def close(self):\n        self.file_handler.close()\n        self.stdout_handler.close()", "\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0.0\n        self.avg = 0.0\n        self.sum = 0.0\n        self.count = 0.0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count", "\n\ndef save_checkpoint(state, path, filename=\"checkpoint.pth\"):\n    torch.save(state, os.path.join(path, filename))\n\n\ndef save_tensor_img(tenor_im, path):\n    im = tenor_im.cpu().clone()\n    im = im.squeeze(0)\n    tensor2pil = transforms.ToPILImage()\n    im = tensor2pil(im)\n    im.save(path)", "\n\ndef save_tensor_merge(tenor_im, tensor_mask, path, colormap='HOT'):\n    im = tenor_im.cpu().detach().clone()\n    im = im.squeeze(0).numpy()\n    im = ((im - np.min(im)) / (np.max(im) - np.min(im) + 1e-20)) * 255\n    im = np.array(im,np.uint8)\n    mask = tensor_mask.cpu().detach().clone()\n    mask = mask.squeeze(0).numpy()\n    mask = ((mask - np.min(mask)) / (np.max(mask) - np.min(mask) + 1e-20)) * 255\n    mask = np.clip(mask, 0, 255)\n    mask = np.array(mask, np.uint8)\n    if colormap == 'HOT':\n        mask = cv2.applyColorMap(mask[0,:,:], cv2.COLORMAP_HOT)\n    elif colormap == 'PINK':\n        mask = cv2.applyColorMap(mask[0,:,:], cv2.COLORMAP_PINK)\n    elif colormap == 'BONE':\n        mask = cv2.applyColorMap(mask[0,:,:], cv2.COLORMAP_BONE)\n    # exec('cv2.applyColorMap(mask[0,:,:], cv2.COLORMAP_' + colormap+')')\n    im = im.transpose((1, 2, 0))\n    im = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n    mix = cv2.addWeighted(im, 0.3, mask, 0.7, 0)\n    cv2.imwrite(path, mix)", "\n\ndef set_seed(seed):\n     torch.manual_seed(seed)\n     torch.cuda.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n     np.random.seed(seed)\n     random.seed(seed)\n     torch.backends.cudnn.deterministic = True\n     torch.backends.cudnn.benchmark = False", "\n\n"]}
{"filename": "DCFM/models/main.py", "chunked_list": ["import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom DCFM.models.vgg import VGG_Backbone\nfrom DCFM.util import *\n\n\ndef weights_init(module):\n    if isinstance(module, nn.Conv2d):\n        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.ones_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Linear):\n        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)", "\n\nclass EnLayer(nn.Module):\n    def __init__(self, in_channel=64):\n        super(EnLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        return x", "\n\nclass LatLayer(nn.Module):\n    def __init__(self, in_channel):\n        super(LatLayer, self).__init__()\n        self.convlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.convlayer(x)\n        return x", "\n\nclass DSLayer(nn.Module):\n    def __init__(self, in_channel=64):\n        super(DSLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.predlayer = nn.Sequential(\n            nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0))#, nn.Sigmoid())\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        x = self.predlayer(x)\n        return x", "\n\nclass half_DSLayer(nn.Module):\n    def __init__(self, in_channel=512):\n        super(half_DSLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, int(in_channel/4), kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.predlayer = nn.Sequential(\n            nn.Conv2d(int(in_channel/4), 1, kernel_size=1, stride=1, padding=0)) #, nn.Sigmoid())\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        x = self.predlayer(x)\n        return x", "\n\nclass AugAttentionModule(nn.Module):\n    def __init__(self, input_channels=512):\n        super(AugAttentionModule, self).__init__()\n        self.query_transform = nn.Sequential(\n            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n        )\n        self.key_transform = nn.Sequential(\n            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n        )\n        self.value_transform = nn.Sequential(\n            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n        )\n        self.scale = 1.0 / (input_channels ** 0.5)\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        B, C, H, W = x.size()\n        x = self.conv(x)\n        x_query = self.query_transform(x).view(B, C, -1).permute(0, 2, 1)  # B,HW,C\n        # x_key: C,BHW\n        x_key = self.key_transform(x).view(B, C, -1)  # B, C,HW\n        # x_value: BHW, C\n        x_value = self.value_transform(x).view(B, C, -1).permute(0, 2, 1)  # B,HW,C\n        attention_bmm = torch.bmm(x_query, x_key)*self.scale # B, HW, HW\n        attention = F.softmax(attention_bmm, dim=-1)\n        attention_sort = torch.sort(attention_bmm, dim=-1, descending=True)[1]\n        attention_sort = torch.sort(attention_sort, dim=-1)[1]\n        #####\n        attention_positive_num = torch.ones_like(attention).cuda()\n        attention_positive_num[attention_bmm < 0] = 0\n        att_pos_mask = attention_positive_num.clone()\n        attention_positive_num = torch.sum(attention_positive_num, dim=-1, keepdim=True).expand_as(attention_sort)\n        attention_sort_pos = attention_sort.float().clone()\n        apn = attention_positive_num-1\n        attention_sort_pos[attention_sort > apn] = 0\n        attention_mask = ((attention_sort_pos+1)**3)*att_pos_mask + (1-att_pos_mask)\n        out = torch.bmm(attention*attention_mask, x_value)\n        out = out.view(B, H, W, C).permute(0, 3, 1, 2)\n        return out+x", "\n\nclass AttLayer(nn.Module):\n    def __init__(self, input_channels=512):\n        super(AttLayer, self).__init__()\n        self.query_transform = nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0)\n        self.key_transform = nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0)\n        self.scale = 1.0 / (input_channels ** 0.5)\n        self.conv = nn.Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0)\n\n    def correlation(self, x5, seeds):\n        B, C, H5, W5 = x5.size()\n        if self.training:\n            correlation_maps = F.conv2d(x5, weight=seeds)  # B,B,H,W\n        else:\n            correlation_maps = torch.relu(F.conv2d(x5, weight=seeds))  # B,B,H,W\n        correlation_maps = correlation_maps.mean(1).view(B, -1)\n        min_value = torch.min(correlation_maps, dim=1, keepdim=True)[0]\n        max_value = torch.max(correlation_maps, dim=1, keepdim=True)[0]\n        correlation_maps = (correlation_maps - min_value) / (max_value - min_value + 1e-12)  # shape=[B, HW]\n        correlation_maps = correlation_maps.view(B, 1, H5, W5)  # shape=[B, 1, H, W]\n        return correlation_maps\n\n    def forward(self, x5):\n        # x: B,C,H,W\n        x5 = self.conv(x5)+x5\n        B, C, H5, W5 = x5.size()\n        x_query = self.query_transform(x5).view(B, C, -1)\n        # x_query: B,HW,C\n        x_query = torch.transpose(x_query, 1, 2).contiguous().view(-1, C)  # BHW, C\n        # x_key: B,C,HW\n        x_key = self.key_transform(x5).view(B, C, -1)\n        x_key = torch.transpose(x_key, 0, 1).contiguous().view(C, -1)  # C, BHW\n        # W = Q^T K: B,HW,HW\n        x_w1 = torch.matmul(x_query, x_key) * self.scale # BHW, BHW\n        x_w = x_w1.view(B * H5 * W5, B, H5 * W5)\n        x_w = torch.max(x_w, -1).values  # BHW, B\n        x_w = x_w.mean(-1)\n        x_w = x_w.view(B, -1)   # B, HW\n        x_w = F.softmax(x_w, dim=-1)  # B, HW\n        #####  mine ######\n        # x_w_max = torch.max(x_w, -1)\n        # max_indices0 = x_w_max.indices.unsqueeze(-1).unsqueeze(-1)\n        norm0 = F.normalize(x5, dim=1)\n        # norm = norm0.view(B, C, -1)\n        # max_indices = max_indices0.expand(B, C, -1)\n        # seeds = torch.gather(norm, 2, max_indices).unsqueeze(-1)\n        x_w = x_w.unsqueeze(1)\n        x_w_max = torch.max(x_w, -1).values.unsqueeze(2).expand_as(x_w)\n        mask = torch.zeros_like(x_w).cuda()\n        mask[x_w == x_w_max] = 1\n        mask = mask.view(B, 1, H5, W5)\n        seeds = norm0 * mask\n        seeds = seeds.sum(3).sum(2).unsqueeze(2).unsqueeze(3)\n        cormap = self.correlation(norm0, seeds)\n        x51 = x5 * cormap\n        proto1 = torch.mean(x51, (0, 2, 3), True)\n        return x5, proto1, x5*proto1+x51, mask", "\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.toplayer = nn.Sequential(\n            nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0))\n        self.latlayer4 = LatLayer(in_channel=512)\n        self.latlayer3 = LatLayer(in_channel=256)\n        self.latlayer2 = LatLayer(in_channel=128)\n        self.latlayer1 = LatLayer(in_channel=64)\n\n        self.enlayer4 = EnLayer()\n        self.enlayer3 = EnLayer()\n        self.enlayer2 = EnLayer()\n        self.enlayer1 = EnLayer()\n\n        self.dslayer4 = DSLayer()\n        self.dslayer3 = DSLayer()\n        self.dslayer2 = DSLayer()\n        self.dslayer1 = DSLayer()\n\n    def _upsample_add(self, x, y):\n        [_, _, H, W] = y.size()\n        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)\n        return x + y\n\n    def forward(self, weighted_x5, x4, x3, x2, x1, H, W):\n        preds = []\n        p5 = self.toplayer(weighted_x5)\n        p4 = self._upsample_add(p5, self.latlayer4(x4))\n        p4 = self.enlayer4(p4)\n        _pred = self.dslayer4(p4)\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear', align_corners=False))\n\n        p3 = self._upsample_add(p4, self.latlayer3(x3))\n        p3 = self.enlayer3(p3)\n        _pred = self.dslayer3(p3)\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear', align_corners=False))\n\n        p2 = self._upsample_add(p3, self.latlayer2(x2))\n        p2 = self.enlayer2(p2)\n        _pred = self.dslayer2(p2)\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear', align_corners=False))\n\n        p1 = self._upsample_add(p2, self.latlayer1(x1))\n        p1 = self.enlayer1(p1)\n        _pred = self.dslayer1(p1)\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear', align_corners=False))\n        return preds", "\n\nclass DCFMNet(nn.Module):\n    \"\"\" Class for extracting activations and\n    registering gradients from targetted intermediate layers \"\"\"\n    def __init__(self, mode='train'):\n        super(DCFMNet, self).__init__()\n        self.gradients = None\n        self.backbone = VGG_Backbone()\n        self.mode = mode\n        self.aug = AugAttentionModule()\n        self.fusion = AttLayer(512)\n        self.decoder = Decoder()\n\n    def set_mode(self, mode):\n        self.mode = mode\n\n    def forward(self, x, gt):\n        if self.mode == 'train':\n            preds = self._forward(x, gt)\n        else:\n            with torch.no_grad():\n                preds = self._forward(x, gt)\n\n        return preds\n\n    def featextract(self, x):\n        x1 = self.backbone.conv1(x)\n        x2 = self.backbone.conv2(x1)\n        x3 = self.backbone.conv3(x2)\n        x4 = self.backbone.conv4(x3)\n        x5 = self.backbone.conv5(x4)\n        return x5, x4, x3, x2, x1\n\n    def _forward(self, x, gt):\n        [B, _, H, W] = x.size()\n        x5, x4, x3, x2, x1 = self.featextract(x)\n        feat, proto, weighted_x5, cormap = self.fusion(x5)\n        feataug = self.aug(weighted_x5)\n        preds = self.decoder(feataug, x4, x3, x2, x1, H, W)\n        if self.training:\n            gt = F.interpolate(gt, size=weighted_x5.size()[2:], mode='bilinear', align_corners=False)\n            feat_pos, proto_pos, weighted_x5_pos, cormap_pos = self.fusion(x5 * gt)\n            feat_neg, proto_neg, weighted_x5_neg, cormap_neg = self.fusion(x5*(1-gt))\n            return preds, proto, proto_pos, proto_neg\n        return preds", "\n\nclass DCFM(nn.Module):\n    def __init__(self, mode='train'):\n        super(DCFM, self).__init__()\n        set_seed(123)\n        self.dcfmnet = DCFMNet()\n        self.mode = mode\n\n    def set_mode(self, mode):\n        self.mode = mode\n        self.dcfmnet.set_mode(self.mode)\n\n    def forward(self, x, gt=None):\n        ########## Co-SOD ############\n        preds = self.dcfmnet(x, gt)\n        return preds", "\n"]}
{"filename": "DCFM/models/vgg.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport os\n\n\nclass VGG_Backbone(nn.Module):\n    # VGG16 with two branches\n    # pooling layer at the front of block\n    def __init__(self):\n        super(VGG_Backbone, self).__init__()\n        conv1 = nn.Sequential()\n        conv1.add_module('conv1_1', nn.Conv2d(3, 64, 3, 1, 1))\n        conv1.add_module('relu1_1', nn.ReLU(inplace=True))\n        conv1.add_module('conv1_2', nn.Conv2d(64, 64, 3, 1, 1))\n        conv1.add_module('relu1_2', nn.ReLU(inplace=True))\n        self.conv1 = conv1\n\n        conv2 = nn.Sequential()\n        conv2.add_module('pool1', nn.MaxPool2d(2, stride=2))\n        conv2.add_module('conv2_1', nn.Conv2d(64, 128, 3, 1, 1))\n        conv2.add_module('relu2_1', nn.ReLU())\n        conv2.add_module('conv2_2', nn.Conv2d(128, 128, 3, 1, 1))\n        conv2.add_module('relu2_2', nn.ReLU())\n        self.conv2 = conv2\n\n        conv3 = nn.Sequential()\n        conv3.add_module('pool2', nn.MaxPool2d(2, stride=2))\n        conv3.add_module('conv3_1', nn.Conv2d(128, 256, 3, 1, 1))\n        conv3.add_module('relu3_1', nn.ReLU())\n        conv3.add_module('conv3_2', nn.Conv2d(256, 256, 3, 1, 1))\n        conv3.add_module('relu3_2', nn.ReLU())\n        conv3.add_module('conv3_3', nn.Conv2d(256, 256, 3, 1, 1))\n        conv3.add_module('relu3_3', nn.ReLU())\n        self.conv3 = conv3\n\n        conv4 = nn.Sequential()\n        conv4.add_module('pool3', nn.MaxPool2d(2, stride=2))\n        conv4.add_module('conv4_1', nn.Conv2d(256, 512, 3, 1, 1))\n        conv4.add_module('relu4_1', nn.ReLU())\n        conv4.add_module('conv4_2', nn.Conv2d(512, 512, 3, 1, 1))\n        conv4.add_module('relu4_2', nn.ReLU())\n        conv4.add_module('conv4_3', nn.Conv2d(512, 512, 3, 1, 1))\n        conv4.add_module('relu4_3', nn.ReLU())\n        self.conv4 = conv4\n\n        conv5 = nn.Sequential()\n        conv5.add_module('pool4', nn.MaxPool2d(2, stride=2))\n        conv5.add_module('conv5_1', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_1', nn.ReLU())\n        conv5.add_module('conv5_2', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_2', nn.ReLU())\n        conv5.add_module('conv5_3', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_3', nn.ReLU())\n        self.conv5 = conv5\n\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 1000),\n        )\n        \n        # pre_train = torch.load(os.path.dirname(__file__) + '/vgg16-397923af.pth')\n        # self._initialize_weights(pre_train)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x1 = self.conv4_1(x)\n        x1 = self.conv5_1(x1)\n        x1 = self.avgpool(x1)\n        _x1 = x1.view(x1.size(0), -1)\n        pred_vector = self.classifier(_x1)\n\n        x2 = self.conv4_2(x)\n        x2 = self.conv5_2(x2)\n        return x1, pred_vector, x2\n\n    def _initialize_weights(self, pre_train):\n        keys = list(pre_train.keys())\n        self.conv1.conv1_1.weight.data.copy_(pre_train[keys[0]])\n        self.conv1.conv1_2.weight.data.copy_(pre_train[keys[2]])\n        self.conv2.conv2_1.weight.data.copy_(pre_train[keys[4]])\n        self.conv2.conv2_2.weight.data.copy_(pre_train[keys[6]])\n        self.conv3.conv3_1.weight.data.copy_(pre_train[keys[8]])\n        self.conv3.conv3_2.weight.data.copy_(pre_train[keys[10]])\n        self.conv3.conv3_3.weight.data.copy_(pre_train[keys[12]])\n        self.conv4.conv4_1.weight.data.copy_(pre_train[keys[14]])\n        self.conv4.conv4_2.weight.data.copy_(pre_train[keys[16]])\n        self.conv4.conv4_3.weight.data.copy_(pre_train[keys[18]])\n        self.conv5.conv5_1.weight.data.copy_(pre_train[keys[20]])\n        self.conv5.conv5_2.weight.data.copy_(pre_train[keys[22]])\n        self.conv5.conv5_3.weight.data.copy_(pre_train[keys[24]])\n\n        self.conv1.conv1_1.bias.data.copy_(pre_train[keys[1]])\n        self.conv1.conv1_2.bias.data.copy_(pre_train[keys[3]])\n        self.conv2.conv2_1.bias.data.copy_(pre_train[keys[5]])\n        self.conv2.conv2_2.bias.data.copy_(pre_train[keys[7]])\n        self.conv3.conv3_1.bias.data.copy_(pre_train[keys[9]])\n        self.conv3.conv3_2.bias.data.copy_(pre_train[keys[11]])\n        self.conv3.conv3_3.bias.data.copy_(pre_train[keys[13]])\n        self.conv4.conv4_1.bias.data.copy_(pre_train[keys[15]])\n        self.conv4.conv4_2.bias.data.copy_(pre_train[keys[17]])\n        self.conv4.conv4_3.bias.data.copy_(pre_train[keys[19]])\n        self.conv5.conv5_1.bias.data.copy_(pre_train[keys[21]])\n        self.conv5.conv5_2.bias.data.copy_(pre_train[keys[23]])\n        self.conv5.conv5_3.bias.data.copy_(pre_train[keys[25]])\n\n        self.classifier[0].weight.data.copy_(pre_train[keys[26]])\n        self.classifier[0].bias.data.copy_(pre_train[keys[27]])\n        self.classifier[3].weight.data.copy_(pre_train[keys[28]])\n        self.classifier[3].bias.data.copy_(pre_train[keys[29]])\n        self.classifier[6].weight.data.copy_(pre_train[keys[30]])\n        self.classifier[6].bias.data.copy_(pre_train[keys[31]])", ""]}
{"filename": "GCoNet/models/vgg.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport os\n\n\nclass VGG_Backbone(nn.Module):\n    # VGG16 with two branches\n    # pooling layer at the front of block\n    def __init__(self):\n        super(VGG_Backbone, self).__init__()\n        conv1 = nn.Sequential()\n        conv1.add_module('conv1_1', nn.Conv2d(3, 64, 3, 1, 1))\n        conv1.add_module('relu1_1', nn.ReLU(inplace=True))\n        conv1.add_module('conv1_2', nn.Conv2d(64, 64, 3, 1, 1))\n        conv1.add_module('relu1_2', nn.ReLU(inplace=True))\n        self.conv1 = conv1\n\n        conv2 = nn.Sequential()\n        conv2.add_module('pool1', nn.MaxPool2d(2, stride=2))\n        conv2.add_module('conv2_1', nn.Conv2d(64, 128, 3, 1, 1))\n        conv2.add_module('relu2_1', nn.ReLU())\n        conv2.add_module('conv2_2', nn.Conv2d(128, 128, 3, 1, 1))\n        conv2.add_module('relu2_2', nn.ReLU())\n        self.conv2 = conv2\n\n        conv3 = nn.Sequential()\n        conv3.add_module('pool2', nn.MaxPool2d(2, stride=2))\n        conv3.add_module('conv3_1', nn.Conv2d(128, 256, 3, 1, 1))\n        conv3.add_module('relu3_1', nn.ReLU())\n        conv3.add_module('conv3_2', nn.Conv2d(256, 256, 3, 1, 1))\n        conv3.add_module('relu3_2', nn.ReLU())\n        conv3.add_module('conv3_3', nn.Conv2d(256, 256, 3, 1, 1))\n        conv3.add_module('relu3_3', nn.ReLU())\n        self.conv3 = conv3\n\n        conv4 = nn.Sequential()\n        conv4.add_module('pool3', nn.MaxPool2d(2, stride=2))\n        conv4.add_module('conv4_1', nn.Conv2d(256, 512, 3, 1, 1))\n        conv4.add_module('relu4_1', nn.ReLU())\n        conv4.add_module('conv4_2', nn.Conv2d(512, 512, 3, 1, 1))\n        conv4.add_module('relu4_2', nn.ReLU())\n        conv4.add_module('conv4_3', nn.Conv2d(512, 512, 3, 1, 1))\n        conv4.add_module('relu4_3', nn.ReLU())\n        self.conv4 = conv4\n\n        conv5 = nn.Sequential()\n        conv5.add_module('pool4', nn.MaxPool2d(2, stride=2))\n        conv5.add_module('conv5_1', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_1', nn.ReLU())\n        conv5.add_module('conv5_2', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_2', nn.ReLU())\n        conv5.add_module('conv5_3', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_3', nn.ReLU())\n        self.conv5 = conv5\n\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 1000),\n        )\n        \n        # pre_train = torch.load(os.path.dirname(__file__) + '/vgg16-397923af.pth')\n        # self._initialize_weights(pre_train)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x1 = self.conv4_1(x)\n        x1 = self.conv5_1(x1)\n        x1 = self.avgpool(x1)\n        _x1 = x1.view(x1.size(0), -1)\n        pred_vector = self.classifier(_x1)\n\n        x2 = self.conv4_2(x)\n        x2 = self.conv5_2(x2)\n        return x1, pred_vector, x2\n\n    def _initialize_weights(self, pre_train):\n        keys = list(pre_train.keys())\n        self.conv1.conv1_1.weight.data.copy_(pre_train[keys[0]])\n        self.conv1.conv1_2.weight.data.copy_(pre_train[keys[2]])\n        self.conv2.conv2_1.weight.data.copy_(pre_train[keys[4]])\n        self.conv2.conv2_2.weight.data.copy_(pre_train[keys[6]])\n        self.conv3.conv3_1.weight.data.copy_(pre_train[keys[8]])\n        self.conv3.conv3_2.weight.data.copy_(pre_train[keys[10]])\n        self.conv3.conv3_3.weight.data.copy_(pre_train[keys[12]])\n        self.conv4.conv4_1.weight.data.copy_(pre_train[keys[14]])\n        self.conv4.conv4_2.weight.data.copy_(pre_train[keys[16]])\n        self.conv4.conv4_3.weight.data.copy_(pre_train[keys[18]])\n        self.conv5.conv5_1.weight.data.copy_(pre_train[keys[20]])\n        self.conv5.conv5_2.weight.data.copy_(pre_train[keys[22]])\n        self.conv5.conv5_3.weight.data.copy_(pre_train[keys[24]])\n\n        self.conv1.conv1_1.bias.data.copy_(pre_train[keys[1]])\n        self.conv1.conv1_2.bias.data.copy_(pre_train[keys[3]])\n        self.conv2.conv2_1.bias.data.copy_(pre_train[keys[5]])\n        self.conv2.conv2_2.bias.data.copy_(pre_train[keys[7]])\n        self.conv3.conv3_1.bias.data.copy_(pre_train[keys[9]])\n        self.conv3.conv3_2.bias.data.copy_(pre_train[keys[11]])\n        self.conv3.conv3_3.bias.data.copy_(pre_train[keys[13]])\n        self.conv4.conv4_1.bias.data.copy_(pre_train[keys[15]])\n        self.conv4.conv4_2.bias.data.copy_(pre_train[keys[17]])\n        self.conv4.conv4_3.bias.data.copy_(pre_train[keys[19]])\n        self.conv5.conv5_1.bias.data.copy_(pre_train[keys[21]])\n        self.conv5.conv5_2.bias.data.copy_(pre_train[keys[23]])\n        self.conv5.conv5_3.bias.data.copy_(pre_train[keys[25]])\n\n        self.classifier[0].weight.data.copy_(pre_train[keys[26]])\n        self.classifier[0].bias.data.copy_(pre_train[keys[27]])\n        self.classifier[3].weight.data.copy_(pre_train[keys[28]])\n        self.classifier[3].bias.data.copy_(pre_train[keys[29]])\n        self.classifier[6].weight.data.copy_(pre_train[keys[30]])\n        self.classifier[6].bias.data.copy_(pre_train[keys[31]])", ""]}
{"filename": "GCoNet/models/GCoNet.py", "chunked_list": ["import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom GCoNet.models.vgg import VGG_Backbone\nimport numpy as np\nimport torch.optim as optim\nfrom torchvision.models import vgg16\n# import fvcore.nn.weight_init as weight_init\n\n\nclass EnLayer(nn.Module):\n    def __init__(self, in_channel=64):\n        super(EnLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        return x", "\n\nclass EnLayer(nn.Module):\n    def __init__(self, in_channel=64):\n        super(EnLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        return x", "\n\nclass LatLayer(nn.Module):\n    def __init__(self, in_channel):\n        super(LatLayer, self).__init__()\n        self.convlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.convlayer(x)\n        return x", "\n\nclass DSLayer(nn.Module):\n    def __init__(self, in_channel=64):\n        super(DSLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.predlayer = nn.Sequential(\n            nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        x = self.predlayer(x)\n        return x", "\nclass half_DSLayer(nn.Module):\n    def __init__(self, in_channel=512):\n        super(half_DSLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, int(in_channel/4), kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            #nn.Conv2d(int(in_channel/2), int(in_channel/4), kernel_size=3, stride=1, padding=1),\n            #nn.ReLU(inplace=True),\n        )\n        self.predlayer = nn.Sequential(\n            nn.Conv2d(int(in_channel/4), 1, kernel_size=1, stride=1, padding=0)) #, nn.Sigmoid())\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        x = self.predlayer(x)\n        return x", "\n\nclass AllAttLayer(nn.Module):\n    def __init__(self, input_channels=512):\n\n        super(AllAttLayer, self).__init__()\n        self.query_transform = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n        self.key_transform = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n\n        self.scale = 1.0 / (input_channels ** 0.5)\n\n        self.conv6 = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n\n        # for layer in [self.query_transform, self.key_transform, self.conv6]:\n        #     weight_init.c2_msra_fill(layer)\n\n    def forward(self, x5):\n        # x: B,C,H,W\n        # x_query: B,C,HW\n        B, C, H5, W5 = x5.size()\n\n        x_query = self.query_transform(x5).view(B, C, -1)\n\n        # x_query: B,HW,C\n        x_query = torch.transpose(x_query, 1, 2).contiguous().view(-1, C) # BHW, C\n        # x_key: B,C,HW\n        x_key = self.key_transform(x5).view(B, C, -1)\n\n        x_key = torch.transpose(x_key, 0, 1).contiguous().view(C, -1) # C, BHW\n\n        # W = Q^T K: B,HW,HW\n        x_w = torch.matmul(x_query, x_key) #* self.scale # BHW, BHW\n        x_w = x_w.view(B*H5*W5, B, H5*W5)\n        x_w = torch.max(x_w, -1).values # BHW, B\n        x_w = x_w.mean(-1)\n        #x_w = torch.mean(x_w, -1).values # BHW\n        x_w = x_w.view(B, -1) * self.scale # B, HW\n        x_w = F.softmax(x_w, dim=-1) # B, HW\n        x_w = x_w.view(B, H5, W5).unsqueeze(1) # B, 1, H, W\n \n        x5 = x5 * x_w\n        x5 = self.conv6(x5)\n\n        return x5", "\nclass CoAttLayer(nn.Module):\n    def __init__(self, input_channels=512):\n\n        super(CoAttLayer, self).__init__()\n\n        self.all_attention = AllAttLayer(input_channels)\n        self.conv_output = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n        self.conv_transform = Conv2d(input_channels, input_channels, kernel_size=1, stride=1, padding=0) \n        self.fc_transform = nn.Linear(input_channels, input_channels)\n\n        # for layer in [self.conv_output, self.conv_transform, self.fc_transform]:\n        #     weight_init.c2_msra_fill(layer)\n    \n    def forward(self, x5):\n        if self.training:\n            f_begin = 0\n            f_end = int(x5.shape[0] / 2)\n            s_begin = f_end\n            s_end = int(x5.shape[0])\n\n            x5_1 = x5[f_begin: f_end]\n            x5_2 = x5[s_begin: s_end]\n\n            x5_new_1 = self.all_attention(x5_1)\n            x5_new_2 = self.all_attention(x5_2)\n\n            x5_1_proto = torch.mean(x5_new_1, (0, 2, 3), True).view(1, -1)\n            x5_1_proto = x5_1_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\n            x5_2_proto = torch.mean(x5_new_2, (0, 2, 3), True).view(1, -1)\n            x5_2_proto = x5_2_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\n            x5_11 = x5_1 * x5_1_proto\n            x5_22 = x5_2 * x5_2_proto\n            weighted_x5 = torch.cat([x5_11, x5_22], dim=0)\n\n            x5_12 = x5_1 * x5_2_proto\n            x5_21 = x5_2 * x5_1_proto\n            neg_x5 = torch.cat([x5_12, x5_21], dim=0)\n        else:\n\n            x5_new = self.all_attention(x5)\n            x5_proto = torch.mean(x5_new, (0, 2, 3), True).view(1, -1)\n            x5_proto = x5_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\n            weighted_x5 = x5 * x5_proto #* cweight\n            neg_x5 = None\n        return weighted_x5, neg_x5", "\n\nclass Conv2d(torch.nn.Conv2d):\n    \"\"\"\n    A wrapper around :class:`torch.nn.Conv2d` to support more features.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Extra keyword arguments supported in addition to those in `torch.nn.Conv2d`:\n        Args:\n            norm (nn.Module, optional): a normalization layer\n            activation (callable(Tensor) -> Tensor): a callable activation function\n        It assumes that norm layer is used before activation.\n        \"\"\"\n        norm = kwargs.pop(\"norm\", None)\n        activation = kwargs.pop(\"activation\", None)\n        super().__init__(*args, **kwargs)\n\n        self.norm = norm\n        self.activation = activation\n\n    def forward(self, x):\n        if x.numel() == 0 and self.training:\n            # https://github.com/pytorch/pytorch/issues/12013\n            assert not isinstance(\n                self.norm, torch.nn.SyncBatchNorm\n            ), \"SyncBatchNorm does not support empty inputs!\"\n\n        x = super().forward(x)\n        if self.norm is not None:\n            x = self.norm(x)\n        if self.activation is not None:\n            x = self.activation(x)\n        return x", "\nclass GINet(nn.Module):\n    \"\"\" Class for extracting activations and \n    registering gradients from targetted intermediate layers \"\"\"\n    def __init__(self, mode='train'):\n        super(GINet, self).__init__()\n        self.gradients = None\n        self.backbone = VGG_Backbone()\n        self.mode = mode\n\n        self.toplayer = nn.Sequential(\n            nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0))\n\n        self.latlayer4 = LatLayer(in_channel=512)\n        self.latlayer3 = LatLayer(in_channel=256)\n        self.latlayer2 = LatLayer(in_channel=128)\n        self.latlayer1 = LatLayer(in_channel=64)\n\n        self.enlayer4 = EnLayer()\n        self.enlayer3 = EnLayer()\n        self.enlayer2 = EnLayer()\n        self.enlayer1 = EnLayer()\n\n        self.dslayer4 = DSLayer()\n        self.dslayer3 = DSLayer()\n        self.dslayer2 = DSLayer()\n        self.dslayer1 = DSLayer()\n\n        self.pred_layer = half_DSLayer(512)\n\n        self.co_x5 = CoAttLayer()\n\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Linear(512, 291)\n\n        # for layer in [self.classifier]:\n        #     weight_init.c2_msra_fill(layer)\n\n    def set_mode(self, mode):\n        self.mode = mode\n\n    def _upsample_add(self, x, y):\n        [_, _, H, W] = y.size()\n        return F.interpolate(\n            x, size=(H, W), mode='bilinear', align_corners=True) + y\n\n    def _fg_att(self, feat, pred):\n        [_, _, H, W] = feat.size()\n        pred = F.interpolate(pred,\n                             size=(H, W),\n                             mode='bilinear',\n                             align_corners=True)\n        return feat * pred\n\n    def forward(self, x):\n        if self.mode == 'train':\n            preds = self._forward(x)\n        else:\n            with torch.no_grad():\n                preds = self._forward(x)\n\n        return preds\n\n    def _forward(self, x):\n        [_, _, H, W] = x.size()\n        x1 = self.backbone.conv1(x)\n        x2 = self.backbone.conv2(x1)\n        x3 = self.backbone.conv3(x2)\n        x4 = self.backbone.conv4(x3)\n        x5 = self.backbone.conv5(x4)\n\n        _x5 = self.avgpool(x5)\n        _x5 = _x5.view(_x5.size(0), -1)\n        pred_cls = self.classifier(_x5)\n\n        weighted_x5, neg_x5 = self.co_x5(x5)\n       \n        cam = torch.mean(weighted_x5, dim=1).unsqueeze(1)\n        cam = cam.sigmoid()\n        if self.training:\n            ########## contrastive branch #########\n            cat_x5 = torch.cat([weighted_x5, neg_x5], dim=0)\n            pred_x5 = self.pred_layer(cat_x5)\n            pred_x5 = F.interpolate(pred_x5,\n                              size=(H, W),\n                              mode='bilinear',\n                              align_corners=True)\n\n        ########## Up-Sample ##########\n        preds = []\n        p5 = self.toplayer(weighted_x5)\n        _pred = cam\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear',\n                          align_corners=True))\n\n        p4 = self._upsample_add(p5, self.latlayer4(x4)) \n        p4 = self.enlayer4(p4)\n        _pred = self.dslayer4(p4)\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear',\n                          align_corners=True))\n\n        p3 = self._upsample_add(p4, self.latlayer3(x3)) \n        p3 = self.enlayer3(p3)\n        _pred = self.dslayer3(p3)\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear',\n                          align_corners=True))\n\n        p2 = self._upsample_add(p3, self.latlayer2(x2)) \n        p2 = self.enlayer2(p2)\n        _pred = self.dslayer2(p2)\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear',\n                          align_corners=True))\n\n        p1 = self._upsample_add(p2, self.latlayer1(x1)) \n        p1 = self.enlayer1(p1)\n        _pred = self.dslayer1(p1)\n        preds.append(\n            F.interpolate(_pred,\n                          size=(H, W),\n                          mode='bilinear',\n                          align_corners=True))\n\n        if self.training:\n            return preds, pred_cls, pred_x5\n        else:\n            return preds", "\n\nclass GCoNet(nn.Module):\n    def __init__(self, mode='train'):\n        super(GCoNet, self).__init__()\n        self.co_classifier = vgg16(pretrained=False).eval()\n        self.ginet = GINet()\n        self.mode = mode\n\n    def set_mode(self, mode):\n        self.mode = mode\n        self.ginet.set_mode(self.mode)\n\n    def forward(self, x):\n        ########## Co-SOD ############\n        preds = self.ginet(x)\n\n        return preds", "\n"]}
{"filename": "GCoNet_plus/config.py", "chunked_list": ["import os\n\n\nclass Config():\n    def __init__(self) -> None:\n        # Backbone\n        self.bb = ['vgg16', 'vgg16bn', 'resnet50'][1]\n        # BN\n        self.use_bn = 'bn' in self.bb or 'resnet' in self.bb\n        # Augmentation\n        self.preproc_methods = ['flip', 'enhance', 'rotate', 'crop', 'pepper'][:3]\n\n        # Mask\n        losses = ['sal', 'cls', 'contrast', 'cls_mask']\n        self.loss = losses[:]\n        self.cls_mask_operation = ['x', '+', 'c'][0]\n        # Loss + Triplet Loss\n        self.lambdas_sal_last = {\n            # not 0 means opening this loss\n            # original rate -- 1 : 30 : 1.5 : 0.2, bce x 30\n            'bce': 30 * 1,          # high performance\n            'iou': 0.5 * 1,         # 0 / 255\n            'ssim': 1 * 0,          # help contours\n            'mse': 150 * 0,         # can smooth the saliency map\n            'reg': 100 * 0,\n            'triplet': 3 * 1 * ('cls' in self.loss),\n        }\n\n        # DB\n        self.db_output_decoder = True\n        self.db_k = 300\n        self.db_k_alpha = 1\n        self.split_mask = True and 'cls_mask' in self.loss\n        self.db_mask = False and self.split_mask\n\n        # Triplet Loss\n        self.triplet = ['_x5', 'mask'][:1]\n        self.triplet_loss_margin = 0.1\n        # Adv\n        self.lambda_adv = 0.        # turn to 0 to avoid adv training\n\n        # Refiner\n        self.refine = [0, 1, 4][0]         # 0 -- no refinement, 1 -- only output mask for refinement, 4 -- but also raw input.\n        if self.refine:\n            self.batch_size = 16\n        else:\n            if self.bb != 'vgg16':\n                self.batch_size = 26\n            else:\n                self.batch_size = 48\n        self.db_output_refiner = False and self.refine\n\n        # Intermediate Layers\n        self.lambdas_sal_others = {\n            'bce': 0,\n            'iou': 0.,\n            'ssim': 0,\n            'mse': 0,\n            'reg': 0,\n            'triplet': 0,\n        }\n        self.output_number = 1\n        self.loss_sal_layers = 4              # used to be last 4 layers\n        self.loss_cls_mask_last_layers = 1         # used to be last 4 layers\n        if 'keep in range':\n            self.loss_sal_layers = min(self.output_number, self.loss_sal_layers)\n            self.loss_cls_mask_last_layers = min(self.output_number, self.loss_cls_mask_last_layers)\n            self.output_number = min(self.output_number, max(self.loss_sal_layers, self.loss_cls_mask_last_layers))\n            if self.output_number == 1:\n                for cri in self.lambdas_sal_others:\n                    self.lambdas_sal_others[cri] = 0\n        self.conv_after_itp = False\n        self.complex_lateral_connection = False\n\n        # to control the quantitive level of each single loss by number of output branches.\n        self.loss_cls_mask_ratio_by_last_layers = 4 / self.loss_cls_mask_last_layers\n        for loss_sal in self.lambdas_sal_last.keys():\n            loss_sal_ratio_by_last_layers = 4 / (int(bool(self.lambdas_sal_others[loss_sal])) * (self.loss_sal_layers - 1) + 1)\n            self.lambdas_sal_last[loss_sal] *= loss_sal_ratio_by_last_layers\n            self.lambdas_sal_others[loss_sal] *= loss_sal_ratio_by_last_layers\n        self.lambda_cls_mask = 2.5 * self.loss_cls_mask_ratio_by_last_layers\n        self.lambda_cls = 3.\n        self.lambda_contrast = 250.\n\n        # Performance of GCoNet\n        self.val_measures = {\n            'Emax': {'CoCA': 0.760, 'CoSOD3k': 0.860, 'CoSal2015': 0.887},\n            'Smeasure': {'CoCA': 0.673, 'CoSOD3k': 0.802, 'CoSal2015': 0.845},\n            'Fmax': {'CoCA': 0.544, 'CoSOD3k': 0.777, 'CoSal2015': 0.847},\n        }\n\n        # others\n        self.GAM = True\n        if not self.GAM and 'contrast' in self.loss:\n            self.loss.remove('contrast')\n        self.lr = 1e-4 * (self.batch_size / 16)\n        self.relation_module = ['GAM', 'ICE', 'NonLocal', 'MHA'][0]\n        self.self_supervision = False\n        self.label_smoothing = False\n        self.freeze = True\n\n        self.validation = False\n        self.decay_step_size = 3000\n        self.rand_seed = 7", "        # run_sh_file = [f for f in os.listdir('.') if 'gco' in f and '.sh' in f] + [os.path.join('..', f) for f in os.listdir('..') if 'gco' in f and '.sh' in f]\n        # with open(run_sh_file[0], 'r') as f:\n        #     self.val_last = int([l.strip() for l in f.readlines() if 'val_last=' in l][0].split('=')[-1])"]}
{"filename": "GCoNet_plus/models/modules.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# import fvcore.nn.weight_init as weight_init\n\nfrom GCoNet_plus.config import Config\n\n\nconfig = Config()", "\nconfig = Config()\n\n\nclass ResBlk(nn.Module):\n    def __init__(self, channel_in=64, channel_out=64):\n        super(ResBlk, self).__init__()\n        self.conv_in = nn.Conv2d(channel_in, 64, 3, 1, 1)\n        self.relu_in = nn.ReLU(inplace=True)\n        self.conv_out = nn.Conv2d(64, channel_out, 3, 1, 1)\n        if config.use_bn:\n            self.bn_in = nn.BatchNorm2d(64)\n            self.bn_out = nn.BatchNorm2d(channel_out)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        if config.use_bn:\n            x = self.bn_in(x)\n        x = self.relu_in(x)\n        x = self.conv_out(x)\n        if config.use_bn:\n            x = self.bn_out(x)\n        return x", "\n\nclass DSLayer(nn.Module):\n    def __init__(self, channel_in=64, channel_out=1, activation_out='relu'):\n        super(DSLayer, self).__init__()\n        self.activation_out = activation_out\n        self.conv1 = nn.Conv2d(channel_in, 64, kernel_size=3, stride=1, padding=1)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n        self.relu2 = nn.ReLU(inplace=True)\n        if activation_out:\n            self.pred_conv = nn.Conv2d(64, channel_out, kernel_size=1, stride=1, padding=0)\n            self.pred_relu = nn.ReLU(inplace=True)\n        else:\n            self.pred_conv = nn.Conv2d(64, channel_out, kernel_size=1, stride=1, padding=0)\n\n        if config.use_bn:\n            self.bn1 = nn.BatchNorm2d(64)\n            self.bn2 = nn.BatchNorm2d(64)\n            self.pred_bn = nn.BatchNorm2d(channel_out)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        if config.use_bn:\n            x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        if config.use_bn:\n            x = self.bn2(x)\n        x = self.relu2(x)\n\n        x = self.pred_conv(x)\n        if config.use_bn:\n            x = self.pred_bn(x)\n        if self.activation_out:\n            x = self.pred_relu(x)\n        return x", "\n\nclass half_DSLayer(nn.Module):\n    def __init__(self, channel_in=512):\n        super(half_DSLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(channel_in, int(channel_in//4), kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True)\n        )\n        self.predlayer = nn.Sequential(\n            nn.Conv2d(int(channel_in//4), 1, kernel_size=1, stride=1, padding=0),\n        )\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        x = self.predlayer(x)\n        return x", "\n\nclass CoAttLayer(nn.Module):\n    def __init__(self, channel_in=512):\n        super(CoAttLayer, self).__init__()\n\n        self.all_attention = eval(Config().relation_module + '(channel_in)')\n        self.conv_output = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n        self.conv_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n        self.fc_transform = nn.Linear(channel_in, channel_in)\n\n        # for layer in [self.conv_output, self.conv_transform, self.fc_transform]:\n        #     weight_init.c2_msra_fill(layer)\n    \n    def forward(self, x5):\n        if self.training:\n            f_begin = 0\n            f_end = int(x5.shape[0] / 2)\n            s_begin = f_end\n            s_end = int(x5.shape[0])\n\n            x5_1 = x5[f_begin: f_end]\n            x5_2 = x5[s_begin: s_end]\n\n            x5_new_1 = self.all_attention(x5_1)\n            x5_new_2 = self.all_attention(x5_2)\n\n            x5_1_proto = torch.mean(x5_new_1, (0, 2, 3), True).view(1, -1)\n            x5_1_proto = x5_1_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\n            x5_2_proto = torch.mean(x5_new_2, (0, 2, 3), True).view(1, -1)\n            x5_2_proto = x5_2_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\n            x5_11 = x5_1 * x5_1_proto\n            x5_22 = x5_2 * x5_2_proto\n            weighted_x5 = torch.cat([x5_11, x5_22], dim=0)\n\n            x5_12 = x5_1 * x5_2_proto\n            x5_21 = x5_2 * x5_1_proto\n            neg_x5 = torch.cat([x5_12, x5_21], dim=0)\n        else:\n\n            x5_new = self.all_attention(x5)\n            x5_proto = torch.mean(x5_new, (0, 2, 3), True).view(1, -1)\n            x5_proto = x5_proto.unsqueeze(-1).unsqueeze(-1) # 1, C, 1, 1\n\n            weighted_x5 = x5 * x5_proto #* cweight\n            neg_x5 = None\n        return weighted_x5, neg_x5", "\n\nclass ICE(nn.Module):\n    # The Integrity Channel Enhancement (ICE) module\n    # _X means in X-th column\n    def __init__(self, channel_in=512):\n        super(ICE, self).__init__()\n        self.conv_1 = nn.Conv2d(channel_in, channel_in, 3, 1, 1)\n        self.conv_2 = nn.Conv1d(channel_in, channel_in, 3, 1, 1)\n        self.conv_3 = nn.Conv2d(channel_in*3, channel_in, 3, 1, 1)\n\n        self.fc_2 = nn.Linear(channel_in, channel_in)\n        self.fc_3 = nn.Linear(channel_in, channel_in)\n\n    def forward(self, x):\n        x_1, x_2, x_3 = x, x, x\n\n        x_1 = x_1 * x_2 * x_3\n        x_2 = x_1 + x_2 + x_3\n        x_3 = torch.cat((x_1, x_2, x_3), dim=1)\n\n        V = self.conv_1(x_1)\n\n        bs, c, h, w = x_2.shape\n        K = self.conv_2(x_2.view(bs, c, h*w))\n        Q_prime = self.conv_3(x_3)\n        Q_prime = torch.norm(Q_prime, dim=(-2, -1)).view(bs, c, 1, 1)\n        Q_prime = Q_prime.view(bs, -1)\n        Q_prime = self.fc_3(Q_prime)\n        Q_prime = torch.softmax(Q_prime, dim=-1)\n        Q_prime = Q_prime.unsqueeze(1)\n\n        Q = torch.matmul(Q_prime, K)\n\n        x_2 = torch.nn.functional.cosine_similarity(K, Q, dim=-1)\n        x_2 = torch.sigmoid(x_2)\n        x_2 = self.fc_2(x_2)\n        x_2 = x_2.unsqueeze(-1).unsqueeze(-1)\n        x_1 = V * x_2 + V\n\n        return x_1", "\n\nclass GAM(nn.Module):\n    def __init__(self, channel_in=512):\n\n        super(GAM, self).__init__()\n        self.query_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n        self.key_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\n        self.scale = 1.0 / (channel_in ** 0.5)\n\n        self.conv6 = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0) \n\n        # for layer in [self.query_transform, self.key_transform, self.conv6]:\n        #     weight_init.c2_msra_fill(layer)\n\n    def forward(self, x5):\n        # x: B,C,H,W\n        # x_query: B,C,HW\n        B, C, H5, W5 = x5.size()\n\n        x_query = self.query_transform(x5).view(B, C, -1)\n\n        # x_query: B,HW,C\n        x_query = torch.transpose(x_query, 1, 2).contiguous().view(-1, C) # BHW, C\n        # x_key: B,C,HW\n        x_key = self.key_transform(x5).view(B, C, -1)\n\n        x_key = torch.transpose(x_key, 0, 1).contiguous().view(C, -1) # C, BHW\n\n        # W = Q^T K: B,HW,HW\n        x_w = torch.matmul(x_query, x_key) #* self.scale # BHW, BHW\n        x_w = x_w.view(B*H5*W5, B, H5*W5)\n        x_w = torch.max(x_w, -1).values # BHW, B\n        x_w = x_w.mean(-1)\n        #x_w = torch.mean(x_w, -1).values # BHW\n        x_w = x_w.view(B, -1) * self.scale # B, HW\n        x_w = F.softmax(x_w, dim=-1) # B, HW\n        x_w = x_w.view(B, H5, W5).unsqueeze(1) # B, 1, H, W\n \n        x5 = x5 * x_w\n        x5 = self.conv6(x5)\n\n        return x5", "\n\nclass MHA(nn.Module):\n    '''\n    Scaled dot-product attention\n    '''\n\n    def __init__(self, d_model=512, d_k=512, d_v=512, h=8, dropout=.1, channel_in=512):\n        '''\n        :param d_model: Output dimensionality of the model\n        :param d_k: Dimensionality of queries and keys\n        :param d_v: Dimensionality of values\n        :param h: Number of heads\n        '''\n        super(MHA, self).__init__()\n        self.query_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0)\n        self.key_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0)\n        self.value_transform = nn.Conv2d(channel_in, channel_in, kernel_size=1, stride=1, padding=0)\n        self.fc_q = nn.Linear(d_model, h * d_k)\n        self.fc_k = nn.Linear(d_model, h * d_k)\n        self.fc_v = nn.Linear(d_model, h * d_v)\n        self.fc_o = nn.Linear(h * d_v, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n        self.d_model = d_model\n        self.d_k = d_k\n        self.d_v = d_v\n        self.h = h\n\n        self.init_weights()\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x, attention_mask=None, attention_weights=None):\n        '''\n        Computes\n        :param queries: Queries (b_s, nq, d_model)\n        :param keys: Keys (b_s, nk, d_model)\n        :param values: Values (b_s, nk, d_model)\n        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n        :return:\n        '''\n        B, C, H, W = x.size()\n        queries = self.query_transform(x).view(B, -1, C)\n        keys = self.query_transform(x).view(B, -1, C)\n        values = self.query_transform(x).view(B, -1, C)\n\n        b_s, nq = queries.shape[:2]\n        nk = keys.shape[1]\n\n        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n\n        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n        if attention_weights is not None:\n            att = att * attention_weights\n        if attention_mask is not None:\n            att = att.masked_fill(attention_mask, -np.inf)\n        att = torch.softmax(att, -1)\n        att = self.dropout(att)\n\n        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n        out = self.fc_o(out).view(B, C, H, W)  # (b_s, nq, d_model)\n        return out", "\n\nclass NonLocal(nn.Module):\n    def __init__(self, channel_in=512, inter_channels=None, dimension=2, sub_sample=True, bn_layer=True):\n        super(NonLocal, self).__init__()\n\n        assert dimension in [1, 2, 3]\n        self.dimension = dimension\n        self.sub_sample = sub_sample\n\n        self.channel_in = channel_in\n        self.inter_channels = inter_channels\n\n        if self.inter_channels is None:\n            self.inter_channels = channel_in // 2\n            if self.inter_channels == 0:\n                self.inter_channels = 1\n\n        self.g = nn.Conv2d(self.channel_in, self.inter_channels, 1, 1, 0)\n\n        if bn_layer:\n            self.W = nn.Sequential(\n                nn.Conv2d(self.inter_channels, self.channel_in, kernel_size=1, stride=1, padding=0),\n                nn.BatchNorm2d(self.channel_in)\n            )\n            nn.init.constant_(self.W[1].weight, 0)\n            nn.init.constant_(self.W[1].bias, 0)\n        else:\n            self.W = nn.Conv2d(self.inter_channels, self.channel_in, kernel_size=1, stride=1, padding=0)\n            nn.init.constant_(self.W.weight, 0)\n            nn.init.constant_(self.W.bias, 0)\n\n        self.theta = nn.Conv2d(self.channel_in, self.inter_channels, kernel_size=1, stride=1, padding=0)\n        self.phi = nn.Conv2d(self.channel_in, self.inter_channels, kernel_size=1, stride=1, padding=0)\n\n        if sub_sample:\n            self.g = nn.Sequential(self.g, nn.MaxPool2d(kernel_size=(2, 2)))\n            self.phi = nn.Sequential(self.phi, nn.MaxPool2d(kernel_size=(2, 2)))\n\n    def forward(self, x, return_nl_map=False):\n        \"\"\"\n        :param x: (b, c, t, h, w)\n        :param return_nl_map: if True return z, nl_map, else only return z.\n        :return:\n        \"\"\"\n\n        batch_size = x.size(0)\n\n        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n        f = torch.matmul(theta_x, phi_x)\n        f_div_C = F.softmax(f, dim=-1)\n\n        y = torch.matmul(f_div_C, g_x)\n        y = y.permute(0, 2, 1).contiguous()\n        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n        W_y = self.W(y)\n        z = W_y + x\n\n        if return_nl_map:\n            return z, f_div_C\n        return z", "\n\nclass DBHead(nn.Module):\n    def __init__(self, channel_in=32, channel_out=1, k=config.db_k):\n        super().__init__()\n        self.k = k\n        self.binarize = nn.Sequential(\n            nn.Conv2d(channel_in, channel_in, 3, 1, 1),\n            *[nn.BatchNorm2d(channel_in), nn.ReLU(inplace=True)] if config.use_bn else nn.ReLU(inplace=True),\n            nn.Conv2d(channel_in, channel_in, 3, 1, 1),\n            *[nn.BatchNorm2d(channel_in), nn.ReLU(inplace=True)] if config.use_bn else nn.ReLU(inplace=True),\n            nn.Conv2d(channel_in, channel_out, 1, 1, 0),\n            nn.Sigmoid()\n        )\n\n        self.thresh = nn.Sequential(\n            nn.Conv2d(channel_in, channel_in, 3, padding=1),\n            *[nn.BatchNorm2d(channel_in), nn.ReLU(inplace=True)] if config.use_bn else nn.ReLU(inplace=True),\n            nn.Conv2d(channel_in, channel_in, 3, 1, 1),\n            *[nn.BatchNorm2d(channel_in), nn.ReLU(inplace=True)] if config.use_bn else nn.ReLU(inplace=True),\n            nn.Conv2d(channel_in, channel_out, 1, 1, 0),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        shrink_maps = self.binarize(x)\n        threshold_maps = self.thresh(x)\n        binary_maps = self.step_function(shrink_maps, threshold_maps)\n        return binary_maps\n\n    def step_function(self, x, y):\n        if config.db_k_alpha != 1:\n            z = x - y\n            mask_neg_inv = 1 - 2 * (z < 0)\n            a = torch.exp(-self.k * (torch.pow(z * mask_neg_inv + 1e-16, 1/config.k_alpha) * mask_neg_inv))\n        else:\n            a = torch.exp(-self.k * (x - y))\n        if torch.isinf(a).any():\n            a = torch.exp(-50 * (x - y))\n        return torch.reciprocal(1 + a)", "\n\nclass RefUnet(nn.Module):\n    # Refinement\n    def __init__(self, in_ch, inc_ch):\n        super(RefUnet, self).__init__()\n        self.conv0 = nn.Conv2d(in_ch, inc_ch, 3, padding=1)\n        self.conv1 = nn.Conv2d(inc_ch, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.pool1 = nn.MaxPool2d(2, 2, ceil_mode=True)\n        self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.pool2 = nn.MaxPool2d(2, 2, ceil_mode=True)\n        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn3 = nn.BatchNorm2d(64)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        self.pool3 = nn.MaxPool2d(2, 2, ceil_mode=True)\n        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn4 = nn.BatchNorm2d(64)\n        self.relu4 = nn.ReLU(inplace=True)\n\n        self.pool4 = nn.MaxPool2d(2, 2, ceil_mode=True)\n        #####\n        self.conv5 = nn.Conv2d(64, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn5 = nn.BatchNorm2d(64)\n        self.relu5 = nn.ReLU(inplace=True)\n        #####\n        self.conv_d4 = nn.Conv2d(128, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn_d4 = nn.BatchNorm2d(64)\n        self.relu_d4 = nn.ReLU(inplace=True)\n\n        self.conv_d3 = nn.Conv2d(128, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn_d3 = nn.BatchNorm2d(64)\n        self.relu_d3 = nn.ReLU(inplace=True)\n\n        self.conv_d2 = nn.Conv2d(128, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn_d2 = nn.BatchNorm2d(64)\n        self.relu_d2 = nn.ReLU(inplace=True)\n\n        self.conv_d1 = nn.Conv2d(128, 64, 3, padding=1)\n        if config.use_bn:\n            self.bn_d1 = nn.BatchNorm2d(64)\n        self.relu_d1 = nn.ReLU(inplace=True)\n\n        self.conv_d0 = nn.Conv2d(64, 1, 3, padding=1)\n\n        self.upscore2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        if config.db_output_refiner:\n            self.db_output_refiner = DBHead(64)\n\n\n    def forward(self, x):\n        hx = x\n        hx = self.conv1(self.conv0(hx))\n        if config.use_bn:\n            hx = self.bn1(hx)\n        hx1 = self.relu1(hx)\n        hx = self.conv2(self.pool1(hx1))\n        if config.use_bn:\n            hx = self.bn2(hx)\n        hx2 = self.relu2(hx)\n        hx = self.conv3(self.pool2(hx2))\n        if config.use_bn:\n            hx = self.bn3(hx)\n        hx3 = self.relu3(hx)\n        hx = self.conv4(self.pool3(hx3))\n        if config.use_bn:\n            hx = self.bn4(hx)\n        hx4 = self.relu4(hx)\n        hx = self.conv5(self.pool4(hx4))\n        if config.use_bn:\n            hx = self.bn5(hx)\n        hx5 = self.relu5(hx)\n        hx = self.upscore2(hx5)\n        d4 = self.conv_d4(torch.cat((hx, hx4), 1))\n        if config.use_bn:\n            d4 = self.bn_d4(d4)\n        d4 = self.relu_d4(d4)\n        hx = self.upscore2(d4)\n        d3 = self.conv_d3(torch.cat((hx, hx3), 1))\n        if config.use_bn:\n            d3 = self.bn_d3(d3)\n        d3 = self.relu_d3(d3)\n        hx = self.upscore2(d3)\n        d2 = self.conv_d2(torch.cat((hx, hx2), 1))\n        if config.use_bn:\n            d2 = self.bn_d2(d2)\n        d2 = self.relu_d2(d2)\n        hx = self.upscore2(d2)\n        d1 = self.conv_d1(torch.cat((hx, hx1), 1))\n        if config.use_bn:\n            d1 = self.bn_d1(d1)\n        d1 = self.relu_d1(d1)\n        if config.db_output_refiner:\n            x = self.db_output_refiner(d1)\n        else:\n            residual = self.conv_d0(d1)\n            x = x + residual\n        return x", ""]}
{"filename": "GCoNet_plus/models/GCoNet.py", "chunked_list": ["from collections import OrderedDict\nimport torch\nfrom torch.functional import norm\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import vgg16, vgg16_bn\n# import fvcore.nn.weight_init as weight_init\nfrom torchvision.models import resnet50\n\nfrom GCoNet_plus.models.modules import ResBlk, DSLayer, half_DSLayer, CoAttLayer, RefUnet, DBHead", "\nfrom GCoNet_plus.models.modules import ResBlk, DSLayer, half_DSLayer, CoAttLayer, RefUnet, DBHead\n\nfrom GCoNet_plus.config import Config\n\n\nclass GCoNet_plus(nn.Module):\n    def __init__(self):\n        super(GCoNet_plus, self).__init__()\n        self.config = Config()\n        bb = self.config.bb\n        if bb == 'vgg16':\n            bb_net = list(vgg16(pretrained=False).children())[0]\n            bb_convs = OrderedDict({\n                'conv1': bb_net[:4],\n                'conv2': bb_net[4:9],\n                'conv3': bb_net[9:16],\n                'conv4': bb_net[16:23],\n                'conv5': bb_net[23:30]\n            })\n            channel_scale = 1\n        elif bb == 'resnet50':\n            bb_net = list(resnet50(pretrained=False).children())\n            bb_convs = OrderedDict({\n                'conv1': nn.Sequential(*bb_net[0:3]),\n                'conv2': bb_net[4],\n                'conv3': bb_net[5],\n                'conv4': bb_net[6],\n                'conv5': bb_net[7]\n            })\n            channel_scale = 4\n        elif bb == 'vgg16bn':\n            bb_net = list(vgg16_bn(pretrained=False).children())[0]\n            bb_convs = OrderedDict({\n                'conv1': bb_net[:6],\n                'conv2': bb_net[6:13],\n                'conv3': bb_net[13:23],\n                'conv4': bb_net[23:33],\n                'conv5': bb_net[33:43]\n            })\n            channel_scale = 1\n        self.bb = nn.Sequential(bb_convs)\n        lateral_channels_in = [512, 512, 256, 128, 64] if 'vgg16' in bb else [2048, 1024, 512, 256, 64]\n\n        # channel_scale_latlayer = channel_scale // 2 if bb == 'resnet50' else 1\n        # channel_last = 32\n\n        ch_decoder = lateral_channels_in[0]//2//channel_scale\n        self.top_layer = ResBlk(lateral_channels_in[0], ch_decoder)\n        self.enlayer5 = ResBlk(ch_decoder, ch_decoder)\n        if self.config.conv_after_itp:\n            self.dslayer5 = DSLayer(ch_decoder, ch_decoder)\n        self.latlayer5 = ResBlk(lateral_channels_in[1], ch_decoder) if self.config.complex_lateral_connection else nn.Conv2d(lateral_channels_in[1], ch_decoder, 1, 1, 0)\n\n        ch_decoder //= 2\n        self.enlayer4 = ResBlk(ch_decoder*2, ch_decoder)\n        if self.config.conv_after_itp:\n            self.dslayer4 = DSLayer(ch_decoder, ch_decoder)\n        self.latlayer4 = ResBlk(lateral_channels_in[2], ch_decoder) if self.config.complex_lateral_connection else nn.Conv2d(lateral_channels_in[2], ch_decoder, 1, 1, 0)\n        if self.config.output_number >= 4:\n            self.conv_out4 = nn.Sequential(nn.Conv2d(ch_decoder, 32, 1, 1, 0), nn.ReLU(inplace=True), nn.Conv2d(32, 1, 1, 1, 0))\n\n        ch_decoder //= 2\n        self.enlayer3 = ResBlk(ch_decoder*2, ch_decoder)\n        if self.config.conv_after_itp:\n            self.dslayer3 = DSLayer(ch_decoder, ch_decoder)\n        self.latlayer3 = ResBlk(lateral_channels_in[3], ch_decoder) if self.config.complex_lateral_connection else nn.Conv2d(lateral_channels_in[3], ch_decoder, 1, 1, 0)\n        if self.config.output_number >= 3:\n            self.conv_out3 = nn.Sequential(nn.Conv2d(ch_decoder, 32, 1, 1, 0), nn.ReLU(inplace=True), nn.Conv2d(32, 1, 1, 1, 0))\n\n        ch_decoder //= 2\n        self.enlayer2 = ResBlk(ch_decoder*2, ch_decoder)\n        if self.config.conv_after_itp:\n            self.dslayer2 = DSLayer(ch_decoder, ch_decoder)\n        self.latlayer2 = ResBlk(lateral_channels_in[4], ch_decoder) if self.config.complex_lateral_connection else nn.Conv2d(lateral_channels_in[4], ch_decoder, 1, 1, 0)\n        if self.config.output_number >= 2:\n            self.conv_out2 = nn.Sequential(nn.Conv2d(ch_decoder, 32, 1, 1, 0), nn.ReLU(inplace=True), nn.Conv2d(32, 1, 1, 1, 0))\n\n        self.enlayer1 = ResBlk(ch_decoder, ch_decoder)\n        self.conv_out1 = nn.Sequential(nn.Conv2d(ch_decoder, 1, 1, 1, 0))\n\n        if self.config.GAM:\n            self.co_x5 = CoAttLayer(channel_in=lateral_channels_in[0])\n\n        if 'contrast' in self.config.loss:\n            self.pred_layer = half_DSLayer(lateral_channels_in[0])\n\n        if {'cls', 'cls_mask'} & set(self.config.loss):\n            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n            self.classifier = nn.Linear(lateral_channels_in[0], 291)       # DUTS_class has 291 classes\n            # for layer in [self.classifier]:\n            #     weight_init.c2_msra_fill(layer)\n        if self.config.split_mask:\n            self.sgm = nn.Sigmoid()\n        if self.config.refine:\n            self.refiner = nn.Sequential(RefUnet(self.config.refine, 64))\n        if self.config.split_mask:\n            self.conv_out_mask = nn.Sequential(nn.Conv2d(ch_decoder, 1, 1, 1, 0))\n        if self.config.db_mask:\n            self.db_mask = DBHead(32)\n        if self.config.db_output_decoder:\n            self.db_output_decoder = DBHead(32)\n        if self.config.cls_mask_operation == 'c':\n            self.conv_cat_mask = nn.Conv2d(4, 3, 1, 1, 0)\n\n    def forward(self, x):\n        ########## Encoder ##########\n\n        [N, _, H, W] = x.size()\n        x1 = self.bb.conv1(x)\n        x2 = self.bb.conv2(x1)\n        x3 = self.bb.conv3(x2)\n        x4 = self.bb.conv4(x3)\n        x5 = self.bb.conv5(x4)\n\n        if 'cls' in self.config.loss:\n            _x5 = self.avgpool(x5)\n            _x5 = _x5.view(_x5.size(0), -1)\n            pred_cls = self.classifier(_x5)\n\n        if self.config.GAM:\n            weighted_x5, neg_x5 = self.co_x5(x5)\n            if 'contrast' in self.config.loss:\n                if self.training:\n                    ########## contrastive branch #########\n                    cat_x5 = torch.cat([weighted_x5, neg_x5], dim=0)\n                    pred_contrast = self.pred_layer(cat_x5)\n                    pred_contrast = F.interpolate(pred_contrast, size=(H, W), mode='bilinear', align_corners=True)\n            p5 = self.top_layer(weighted_x5)\n        else:\n            p5 = self.top_layer(x5)\n\n        ########## Decoder ##########\n        scaled_preds = []\n        p5 = self.enlayer5(p5)\n        p5 = F.interpolate(p5, size=x4.shape[2:], mode='bilinear', align_corners=True)\n        if self.config.conv_after_itp:\n            p5 = self.dslayer5(p5)\n        p4 = p5 + self.latlayer5(x4)\n\n        p4 = self.enlayer4(p4)\n        p4 = F.interpolate(p4, size=x3.shape[2:], mode='bilinear', align_corners=True)\n        if self.config.conv_after_itp:\n            p4 = self.dslayer4(p4)\n        if self.config.output_number >= 4:\n            p4_out = self.conv_out4(p4)\n            scaled_preds.append(p4_out)\n        p3 = p4 + self.latlayer4(x3)\n\n        p3 = self.enlayer3(p3)\n        p3 = F.interpolate(p3, size=x2.shape[2:], mode='bilinear', align_corners=True)\n        if self.config.conv_after_itp:\n            p3 = self.dslayer3(p3)\n        if self.config.output_number >= 3:\n            p3_out = self.conv_out3(p3)\n            scaled_preds.append(p3_out)\n        p2 = p3 + self.latlayer3(x2)\n\n        p2 = self.enlayer2(p2)\n        p2 = F.interpolate(p2, size=x1.shape[2:], mode='bilinear', align_corners=True)\n        if self.config.conv_after_itp:\n            p2 = self.dslayer2(p2)\n        if self.config.output_number >= 2:\n            p2_out = self.conv_out2(p2)\n            scaled_preds.append(p2_out)\n        p1 = p2 + self.latlayer2(x1)\n\n        p1 = self.enlayer1(p1)\n        p1 = F.interpolate(p1, size=x.shape[2:], mode='bilinear', align_corners=True)\n        if self.config.db_output_decoder:\n            p1_out = self.db_output_decoder(p1)\n        else:\n            p1_out = self.conv_out1(p1)\n        scaled_preds.append(p1_out)\n\n        if self.config.refine == 1:\n            scaled_preds.append(self.refiner(p1_out))\n        elif self.config.refine == 4:\n            scaled_preds.append(self.refiner(torch.cat([x, p1_out], dim=1)))\n\n        if 'cls_mask' in self.config.loss:\n            pred_cls_masks = []\n            norm_features_mask = []\n            input_features = [x, x1, x2, x3][:self.config.loss_cls_mask_last_layers]\n            bb_lst = [self.bb.conv1, self.bb.conv2, self.bb.conv3, self.bb.conv4, self.bb.conv5]\n            for idx_out in range(self.config.loss_cls_mask_last_layers):\n                if idx_out:\n                    mask_output = scaled_preds[-(idx_out+1+int(bool(self.config.refine)))]\n                else:\n                    if self.config.split_mask:\n                        if self.config.db_mask:\n                            mask_output = self.db_mask(p1)\n                        else:\n                            mask_output = self.sgm(self.conv_out_mask(p1))\n\n                if self.config.cls_mask_operation == 'x':\n                    masked_features = input_features[idx_out] * mask_output\n                elif self.config.cls_mask_operation == '+':\n                    masked_features = input_features[idx_out] + mask_output\n                elif self.config.cls_mask_operation == 'c':\n                    masked_features = self.conv_cat_mask(torch.cat((input_features[idx_out], mask_output), dim=1))\n                norm_feature_mask = self.avgpool(\n                    nn.Sequential(*bb_lst[idx_out:])(\n                        masked_features\n                    )\n                ).view(N, -1)\n                norm_features_mask.append(norm_feature_mask)\n                pred_cls_masks.append(\n                    self.classifier(\n                        norm_feature_mask\n                    )\n                )\n\n        if self.training:\n            return_values = []\n            if {'sal', 'cls', 'contrast', 'cls_mask'} == set(self.config.loss):\n                return_values = [scaled_preds, pred_cls, pred_contrast, pred_cls_masks]\n            elif {'sal', 'cls', 'contrast'} == set(self.config.loss):\n                return_values = [scaled_preds, pred_cls, pred_contrast]\n            elif {'sal', 'cls', 'cls_mask'} == set(self.config.loss):\n                return_values = [scaled_preds, pred_cls, pred_cls_masks]\n            elif {'sal', 'cls'} == set(self.config.loss):\n                return_values = [scaled_preds, pred_cls]\n            elif {'sal', 'contrast'} == set(self.config.loss):\n                return_values = [scaled_preds, pred_contrast]\n            elif {'sal', 'cls_mask'} == set(self.config.loss):\n                return_values = [scaled_preds, pred_cls_masks]\n            else:\n                return_values = [scaled_preds]\n            \n            if self.config.lambdas_sal_last['triplet']:\n                norm_features = []\n                if '_x5' in self.config.triplet:\n                    norm_features.append(_x5)\n                if 'mask' in self.config.triplet:\n                    norm_features.append(norm_features_mask[0])\n                return_values.append(norm_features)\n            return return_values\n        else:\n            return scaled_preds", ""]}
{"filename": "gicd/models/__init__.py", "chunked_list": ["from .GICD import GICD"]}
{"filename": "gicd/models/vgg.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\n\nclass VGG_Backbone(nn.Module):\n    def __init__(self):\n        super(VGG_Backbone, self).__init__()\n        conv1 = nn.Sequential()\n        conv1.add_module('conv1_1', nn.Conv2d(3, 64, 3, 1, 1))\n        conv1.add_module('relu1_1', nn.ReLU(inplace=True))\n        conv1.add_module('conv1_2', nn.Conv2d(64, 64, 3, 1, 1))\n        conv1.add_module('relu1_2', nn.ReLU(inplace=True))\n        self.conv1 = conv1\n\n        conv2 = nn.Sequential()\n        conv2.add_module('pool1', nn.MaxPool2d(2, stride=2))\n        conv2.add_module('conv2_1', nn.Conv2d(64, 128, 3, 1, 1))\n        conv2.add_module('relu2_1', nn.ReLU())\n        conv2.add_module('conv2_2', nn.Conv2d(128, 128, 3, 1, 1))\n        conv2.add_module('relu2_2', nn.ReLU())\n        self.conv2 = conv2\n\n        conv3 = nn.Sequential()\n        conv3.add_module('pool2', nn.MaxPool2d(2, stride=2))\n        conv3.add_module('conv3_1', nn.Conv2d(128, 256, 3, 1, 1))\n        conv3.add_module('relu3_1', nn.ReLU())\n        conv3.add_module('conv3_2', nn.Conv2d(256, 256, 3, 1, 1))\n        conv3.add_module('relu3_2', nn.ReLU())\n        conv3.add_module('conv3_3', nn.Conv2d(256, 256, 3, 1, 1))\n        conv3.add_module('relu3_3', nn.ReLU())\n        self.conv3 = conv3\n\n        conv4 = nn.Sequential()\n        conv4.add_module('pool3', nn.MaxPool2d(2, stride=2))\n        conv4.add_module('conv4_1', nn.Conv2d(256, 512, 3, 1, 1))\n        conv4.add_module('relu4_1', nn.ReLU())\n        conv4.add_module('conv4_2', nn.Conv2d(512, 512, 3, 1, 1))\n        conv4.add_module('relu4_2', nn.ReLU())\n        conv4.add_module('conv4_3', nn.Conv2d(512, 512, 3, 1, 1))\n        conv4.add_module('relu4_3', nn.ReLU())\n        self.conv4 = conv4\n\n        conv5 = nn.Sequential()\n        conv5.add_module('pool4', nn.MaxPool2d(2, stride=2))\n        conv5.add_module('conv5_1', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_1', nn.ReLU())\n        conv5.add_module('conv5_2', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_2', nn.ReLU())\n        conv5.add_module('conv5_3', nn.Conv2d(512, 512, 3, 1, 1))\n        conv5.add_module('relu5_3', nn.ReLU())\n        self.conv5 = conv5\n\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 1000),\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x1 = self.conv4_1(x)\n        x1 = self.conv5_1(x1)\n        x1 = self.avgpool(x1)\n        _x1 = x1.view(x1.size(0), -1)\n        pred_vector = self.classifier(_x1)\n\n        x2 = self.conv4_2(x)\n        x2 = self.conv5_2(x2)\n        return x1, pred_vector, x2", ""]}
{"filename": "gicd/models/GICD.py", "chunked_list": ["import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom gicd.models.vgg import VGG_Backbone\nimport numpy as np\nimport torch.optim as optim\nfrom torchvision.models import vgg16\n\n\nclass EnLayer(nn.Module):\n    def __init__(self, in_channel=64):\n        super(EnLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        return x", "\nclass EnLayer(nn.Module):\n    def __init__(self, in_channel=64):\n        super(EnLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        return x", "\n\nclass LatLayer(nn.Module):\n    def __init__(self, in_channel):\n        super(LatLayer, self).__init__()\n        self.convlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, x):\n        x = self.convlayer(x)\n        return x", "\n\nclass DSLayer(nn.Module):\n    def __init__(self, in_channel=64):\n        super(DSLayer, self).__init__()\n        self.enlayer = nn.Sequential(\n            nn.Conv2d(in_channel, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.predlayer = nn.Sequential(\n            nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n\n    def forward(self, x):\n        x = self.enlayer(x)\n        x = self.predlayer(x)\n        return x", "\n\nclass GINet(nn.Module):\n    def __init__(self, mode='train'):\n        super(GINet, self).__init__()\n        self.gradients = None\n        self.backbone = VGG_Backbone()\n\n        self.toplayer = nn.Sequential(\n            nn.Conv2d(512, 64, kernel_size=1, stride=1, padding=0),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=1, stride=1, padding=0))\n\n        self.latlayer4 = LatLayer(in_channel=512)\n        self.latlayer3 = LatLayer(in_channel=256)\n        self.latlayer2 = LatLayer(in_channel=128)\n        self.latlayer1 = LatLayer(in_channel=64)\n\n        self.enlayer4 = EnLayer()\n        self.enlayer3 = EnLayer()\n        self.enlayer2 = EnLayer()\n        self.enlayer1 = EnLayer()\n\n        self.dslayer4 = DSLayer()\n        self.dslayer3 = DSLayer()\n        self.dslayer2 = DSLayer()\n        self.dslayer1 = DSLayer()\n\n    def save_gradient(self, grad):\n        self.gradients = grad\n\n    def _upsample_add(self, x, y):\n        [_, _, H, W] = y.size()\n        return F.interpolate(\n            x, size=(H, W), mode='bilinear', align_corners=True) + y\n\n    def _fg_att(self, feat, pred):\n        [_, _, H, W] = feat.size()\n        pred = F.interpolate(pred,\n                             size=(H, W),\n                             mode='bilinear',\n                             align_corners=True)\n        return feat * pred\n\n    def forward(self, x, co_coding):\n        [_, _, H, W] = x.size()\n        with torch.no_grad():\n            x1 = self.backbone.conv1(x)\n            x2 = self.backbone.conv2(x1)\n            x3 = self.backbone.conv3(x2)\n            x4 = self.backbone.conv4(x3)\n\n        x5 = self.backbone.conv5(x4)\n        x5.requires_grad_()\n        x5.register_hook(self.save_gradient)\n        x5_p = self.backbone.avgpool(x5)\n        _x5_p = x5_p.view(x5_p.size(0), -1)\n        pred_vector = self.backbone.classifier(_x5_p)\n\n        co_coding = co_coding.requires_grad_()\n        similarity = torch.sum(co_coding.cuda() * pred_vector)\n\n        similarity.backward(retain_graph=True)\n        cweight = F.adaptive_avg_pool2d(self.gradients, (1, 1))\n        cweight = F.relu(cweight)\n        cweight = (cweight - torch.min(cweight)) / (torch.max(cweight) -\n                                                    torch.min(cweight) + 1e-20)\n        weighted_x5 = x5 * cweight\n        cam = torch.mean(weighted_x5, dim=1).unsqueeze(1)\n        cam = torch.relu(cam)\n        cam = cam - torch.min(cam)\n        cam = cam / (torch.max(cam) + 1e-6)\n        cam = torch.clamp(cam, 0, 1)\n\n        with torch.no_grad():\n            ########## Up-Sample ##########\n            preds = []\n            p5 = self.toplayer(weighted_x5)\n            _pred = cam\n            preds.append(\n                F.interpolate(_pred,\n                              size=(H, W),\n                              mode='bilinear',\n                              align_corners=True))\n\n            p4 = self._upsample_add(p5, self.latlayer4(self._fg_att(x4,\n                                                                    _pred)))\n            p4 = self.enlayer4(p4)\n            _pred = self.dslayer4(p4)\n            preds.append(\n                F.interpolate(_pred,\n                              size=(H, W),\n                              mode='bilinear',\n                              align_corners=True))\n\n            p3 = self._upsample_add(p4, self.latlayer3(self._fg_att(x3,\n                                                                    _pred)))\n            p3 = self.enlayer3(p3)\n            _pred = self.dslayer3(p3)\n            preds.append(\n                F.interpolate(_pred,\n                              size=(H, W),\n                              mode='bilinear',\n                              align_corners=True))\n\n            p2 = self._upsample_add(p3, self.latlayer2(self._fg_att(x2,\n                                                                    _pred)))\n            p2 = self.enlayer2(p2)\n            _pred = self.dslayer2(p2)\n            preds.append(\n                F.interpolate(_pred,\n                              size=(H, W),\n                              mode='bilinear',\n                              align_corners=True))\n\n            p1 = self._upsample_add(p2, self.latlayer1(self._fg_att(x1,\n                                                                    _pred)))\n            p1 = self.enlayer1(p1)\n            _pred = self.dslayer1(p1)\n            preds.append(\n                F.interpolate(_pred,\n                              size=(H, W),\n                              mode='bilinear',\n                              align_corners=True))\n\n        return preds", "\n\nclass GICD(nn.Module):\n    def __init__(self, mode='test'):\n        super(GICD, self).__init__()\n        self.co_classifier = vgg16(pretrained=0).eval()\n        self.ginet = GINet()\n\n    def forward(self, x):\n        x = x.unsqueeze(0)\n        [_, N, _, _, _] = x.size()\n        with torch.no_grad():\n            ######### Co-Classify ########\n            co_coding = 0\n            for inum in range(N):\n                co_coding += self.co_classifier(\n                    x[:, inum, :, :, :]).cpu().data.numpy()\n            co_coding = torch.from_numpy(co_coding)\n            co_coding = F.softmax(co_coding, dim=1)  #\u6ce8\u610f\u4e86 dim\n\n        ########## Co-SOD ############\n        preds = []\n        for inum in range(N):\n            ipreds = self.ginet(x[:, inum, :, :, :], co_coding)\n\n            preds.append(ipreds)\n        return preds", ""]}
{"filename": "ICNet/ICNet/network.py", "chunked_list": ["import torch\nimport time\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import init\nfrom os.path import join\nnp.set_printoptions(suppress=True, threshold=1e5)\n\n\"\"\"", "\n\"\"\"\nresize:\n    Resize tensor (shape=[N, C, H, W]) to the target size (default: 224*224).\n\"\"\"\ndef resize(input, target_size=(224, 224)):\n    return F.interpolate(input, (target_size[0], target_size[1]), mode='bilinear', align_corners=True)\n\n\"\"\"\nweights_init:", "\"\"\"\nweights_init:\n    Weights initialization.\n\"\"\"\ndef weights_init(module):\n    if isinstance(module, nn.Conv2d):\n        init.normal_(module.weight, 0, 0.01)\n        if module.bias is not None:\n            init.constant_(module.bias, 0)\n    elif isinstance(module, nn.BatchNorm2d):\n        init.constant_(module.weight, 1)\n        init.constant_(module.bias, 0)", "\n\n\"\"\"\"\nVGG16:\n    VGG16 backbone.\n\"\"\" \nclass VGG16(nn.Module):\n    def __init__(self):\n        super(VGG16, self).__init__()\n        layers = []\n        in_channel = 3\n        vgg_out_channels = (64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M')\n        for out_channel in vgg_out_channels:\n            if out_channel == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                conv2d = nn.Conv2d(in_channel, out_channel, 3, 1, 1)\n                layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channel = out_channel\n        self.vgg = nn.ModuleList(layers)\n        self.table = {'conv1_1': 0, 'conv1_2': 2, 'conv1_2_mp': 4,\n                      'conv2_1': 5, 'conv2_2': 7, 'conv2_2_mp': 9,\n                      'conv3_1': 10, 'conv3_2': 12, 'conv3_3': 14, 'conv3_3_mp': 16,\n                      'conv4_1': 17, 'conv4_2': 19, 'conv4_3': 21, 'conv4_3_mp': 23,\n                      'conv5_1': 24, 'conv5_2': 26, 'conv5_3': 28, 'conv5_3_mp': 30, 'final': 31}\n\n    def forward(self, feats, start_layer_name, end_layer_name):\n        start_idx = self.table[start_layer_name]\n        end_idx = self.table[end_layer_name]\n        for idx in range(start_idx, end_idx):\n            feats = self.vgg[idx](feats)\n        return feats", "\n\n\"\"\"\nPrediction:\n    Compress the channel of input features to 1, then predict maps with sigmoid function.\n\"\"\"\nclass Prediction(nn.Module):\n    def __init__(self, in_channel):\n        super(Prediction, self).__init__()\n        self.pred = nn.Sequential(nn.Conv2d(in_channel, 1, 1), nn.Sigmoid())\n\n    def forward(self, feats):\n        pred = self.pred(feats)\n        return pred", "\n\n\"\"\"\nRes:\n    Two convolutional layers with residual structure.\n\"\"\"\nclass Res(nn.Module):\n    def __init__(self, in_channel):\n        super(Res, self).__init__()\n        self.conv = nn.Sequential(nn.Conv2d(in_channel, in_channel, 3, 1, 1), \n                                  nn.BatchNorm2d(in_channel), nn.ReLU(inplace=True),\n                                  nn.Conv2d(in_channel, in_channel, 3, 1, 1))\n\n    def forward(self, feats):\n        feats = feats + self.conv(feats)\n        feats = F.relu(feats, inplace=True)\n        return feats", "\n\"\"\"\nCosal_Module:\n    Given features extracted from the VGG16 backbone,\n    exploit SISMs to build intra cues and inter cues.\n\"\"\"\nclass Cosal_Module(nn.Module):\n    def __init__(self, H, W):\n        super(Cosal_Module, self).__init__()\n        self.cosal_feat = Cosal_Sub_Module(H, W)\n        self.conv = nn.Sequential(nn.Conv2d(256, 128, 1), Res(128))\n\n    def forward(self, feats, SISMs):\n        # Get foreground co-saliency features.\n        fore_cosal_feats = self.cosal_feat(feats, SISMs)\n\n        # Get background co-saliency features.\n        back_cosal_feats = self.cosal_feat(feats, 1.0 - SISMs)\n        \n        # Fuse foreground and background co-saliency features\n        # to generate co-saliency enhanced features.\n        cosal_enhanced_feats = self.conv(torch.cat([fore_cosal_feats, back_cosal_feats], dim=1))\n        return cosal_enhanced_feats", "\n\"\"\"\nCosal_Sub_Module:\n  * The kernel module of ICNet.\n    Generate foreground/background co-salient features by using SISMs.\n\"\"\"\nclass Cosal_Sub_Module(nn.Module):\n    def __init__(self, H, W):\n        super(Cosal_Sub_Module, self).__init__()\n        channel = H * W\n        self.conv = nn.Sequential(nn.Conv2d(channel, 128, 1), Res(128))\n\n    def forward(self, feats, SISMs):\n        N, C, H, W = feats.shape\n        HW = H * W\n        \n        # Resize SISMs to the same size as the input feats.\n        SISMs = resize(SISMs, [H, W])  # shape=[N, 1, H, W]\n        \n        # NFs: L2-normalized features.\n        NFs = F.normalize(feats, dim=1)  # shape=[N, C, H, W]\n\n        def CFM(SIVs, NFs):\n            # Compute correlation maps [Figure 4] between SIVs and pixel-wise feature vectors in NFs by inner product.\n            # We implement this process by ``F.conv2d()'', which takes SIVs as 1*1 kernels to convolve NFs.\n            correlation_maps = F.conv2d(NFs, weight=SIVs)  # shape=[N, N, H, W]\n            \n            # Vectorize and normalize correlation maps.\n            correlation_maps = F.normalize(correlation_maps.reshape(N, N, HW), dim=2)  # shape=[N, N, HW]\n            \n            # Compute the weight vectors [Equation 2].\n            correlation_matrix = torch.matmul(correlation_maps, correlation_maps.permute(0, 2, 1))  # shape=[N, N, N]\n            weight_vectors = correlation_matrix.sum(dim=2).softmax(dim=1)  # shape=[N, N]\n\n            # Fuse correlation maps with the weight vectors to build co-salient attention (CSA) maps.\n            CSA_maps = torch.sum(correlation_maps * weight_vectors.view(N, N, 1), dim=1)  # shape=[N, HW]\n            \n            # Max-min normalize CSA maps.\n            min_value = torch.min(CSA_maps, dim=1, keepdim=True)[0]\n            max_value = torch.max(CSA_maps, dim=1, keepdim=True)[0]\n            CSA_maps = (CSA_maps - min_value) / (max_value - min_value + 1e-12)  # shape=[N, HW]\n            CSA_maps = CSA_maps.view(N, 1, H, W)  # shape=[N, 1, H, W]\n            return CSA_maps\n\n        def get_SCFs(NFs):\n            NFs = NFs.view(N, C, HW)  # shape=[N, C, HW]\n            SCFs = torch.matmul(NFs.permute(0, 2, 1), NFs).view(N, -1, H, W)  # shape=[N, HW, H, W]\n            return SCFs\n\n        # Compute SIVs [Section 3.2, Equation 1].\n        SIVs = F.normalize((NFs * SISMs).mean(dim=3).mean(dim=2), dim=1).view(N, C, 1, 1)  # shape=[N, C, 1, 1]\n\n        # Compute co-salient attention (CSA) maps [Section 3.3].\n        CSA_maps = CFM(SIVs, NFs)  # shape=[N, 1, H, W]\n\n        # Compute self-correlation features (SCFs) [Section 3.4].\n        SCFs = get_SCFs(NFs)  # shape=[N, HW, H, W]\n\n        # Rearrange the channel order of SCFs to obtain RSCFs [Section 3.4].\n        evidence = CSA_maps.view(N, HW)  # shape=[N, HW]\n        indices = torch.argsort(evidence, dim=1, descending=True).view(N, HW, 1, 1).repeat(1, 1, H, W)  # shape=[N, HW, H, W]\n        RSCFs = torch.gather(SCFs, dim=1, index=indices)  # shape=[N, HW, H, W]\n        cosal_feat = self.conv(RSCFs * CSA_maps)  # shape=[N, 128, H, W]\n        return cosal_feat", "\n\"\"\"\nDecoder_Block:\n    U-net like decoder block that fuses co-saliency features and low-level features for upsampling. \n\"\"\"\nclass Decoder_Block(nn.Module):\n    def __init__(self, in_channel):\n        super(Decoder_Block, self).__init__()\n        self.cmprs = nn.Conv2d(in_channel, 32, 1)\n        self.merge_conv = nn.Sequential(nn.Conv2d(96, 96, 3, 1, 1), nn.BatchNorm2d(96), nn.ReLU(inplace=True),\n                                        nn.Conv2d(96, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(inplace=True))\n        self.pred = Prediction(32)\n\n    def forward(self, low_level_feats, cosal_map, SISMs, old_feats):\n        _, _, H, W = low_level_feats.shape\n        # Adjust cosal_map, SISMs and old_feats to the same spatial size as low_level_feats.\n        cosal_map = resize(cosal_map, [H, W])\n        SISMs = resize(SISMs, [H, W])\n        old_feats = resize(old_feats, [H, W])\n\n        # Predict co-saliency maps with the size of H*W.\n        cmprs = self.cmprs(low_level_feats)\n        new_feats = self.merge_conv(torch.cat([cmprs * cosal_map, \n                                               cmprs * SISMs, \n                                               old_feats], dim=1))\n        new_cosal_map = self.pred(new_feats)\n        return new_feats, new_cosal_map", "\n\n\"\"\"\nICNet:\n    The entire ICNet.\n    Given a group of images and corresponding SISMs, ICNet outputs a group of co-saliency maps (predictions) at once.\n\"\"\"\nclass ICNet(nn.Module):\n    def __init__(self):\n        super(ICNet, self).__init__()\n        self.vgg = VGG16()\n        self.s = 8 # Change size from 224x224 to 256x256, s: 7 -> 8\n        self.Co6 = Cosal_Module(self.s*(2**0), self.s*(2**0))\n        self.Co5 = Cosal_Module(self.s*(2**1), self.s*(2**1))\n        self.Co4 = Cosal_Module(self.s*(2**2), self.s*(2**2))\n        self.conv6_cmprs = nn.Sequential(nn.MaxPool2d(2, 2), nn.Conv2d(512, 128, 1),\n                                         nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n                                         nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n                                         nn.Conv2d(128, 128, 3, 1, 1))\n        self.conv5_cmprs = nn.Conv2d(512, 256, 1)\n        self.conv4_cmprs = nn.Conv2d(512, 256, 1)\n\n        self.merge_co_56 = Res(128)\n        self.merge_co_45 = nn.Sequential(Res(128), nn.Conv2d(128, 32, 1))\n        self.get_pred_4 = Prediction(32)\n        self.refine_3 = Decoder_Block(256)\n        self.refine_2 = Decoder_Block(128)\n        self.refine_1 = Decoder_Block(64)\n\n    def forward(self, image_group, SISMs=None, is_training=False):\n        if SISMs is None:\n            # May cost a little extra time.\n            SISMs = image_group[:, :1, ...]\n        # Extract features from the VGG16 backbone.\n        conv1_2 = self.vgg(image_group, 'conv1_1', 'conv1_2_mp') # shape=[N, 64, 224, 224]\n        conv2_2 = self.vgg(conv1_2, 'conv1_2_mp', 'conv2_2_mp')  # shape=[N, 128, 112, 112]\n        conv3_3 = self.vgg(conv2_2, 'conv2_2_mp', 'conv3_3_mp')  # shape=[N, 256, 56, 56]\n        conv4_3 = self.vgg(conv3_3, 'conv3_3_mp', 'conv4_3_mp')  # shape=[N, 512, 28, 28]\n        conv5_3 = self.vgg(conv4_3, 'conv4_3_mp', 'conv5_3_mp')  # shape=[N, 512, 14, 14]\n\n        # Compress the channels of high-level features.\n        conv6_cmprs = self.conv6_cmprs(conv5_3)  # shape=[N, 128, 7, 7]\n        conv5_cmprs = self.conv5_cmprs(conv5_3)  # shape=[N, 256, 14, 14]\n        conv4_cmprs = self.conv4_cmprs(conv4_3)  # shape=[N, 256, 28, 28]\n\n        # Obtain co-saliancy features.\n        cosal_feat_6 = self.Co6(conv6_cmprs, SISMs) # shape=[N, 128, 7, 7]\n        cosal_feat_5 = self.Co5(conv5_cmprs, SISMs) # shape=[N, 128, 14, 14]\n        cosal_feat_4 = self.Co4(conv4_cmprs, SISMs) # shape=[N, 128, 28, 28]\n        \n        # Merge co-saliancy features and predict co-saliency maps with size of 28*28 (i.e., \"cosal_map_4\").\n        feat_56 = self.merge_co_56(cosal_feat_5 + resize(cosal_feat_6, [self.s*(2**1), self.s*(2**1)])) # shape=[N, 128, 14, 14]\n        feat_45 = self.merge_co_45(cosal_feat_4 + resize(feat_56, [self.s*(2**2), self.s*(2**2)]))      # shape=[N, 128, 28, 28]\n        cosal_map_4 = self.get_pred_4(feat_45)                                    # shape=[N, 1, 28, 28]\n\n        # Obtain co-saliency maps with size of 224*224 (i.e., \"cosal_map_1\") by progressively upsampling.\n        feat_34, cosal_map_3 = self.refine_3(conv3_3, cosal_map_4, SISMs, feat_45)\n        feat_23, cosal_map_2 = self.refine_2(conv2_2, cosal_map_4, SISMs, feat_34)\n        _, cosal_map_1 = self.refine_1(conv1_2, cosal_map_4, SISMs, feat_23)      # shape=[N, 1, 224, 224]\n\n        # Return predicted co-saliency maps.\n        if is_training:\n            preds_list = [resize(cosal_map_4), resize(cosal_map_3), resize(cosal_map_2), cosal_map_1]\n            return preds_list\n        else:\n            preds = cosal_map_1\n            return preds", ""]}
