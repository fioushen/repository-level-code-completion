{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\nsetup(\n    name=\"m3drefclip\",\n    py_modules=[\"m3drefclip\"],\n    version=\"1.0\",\n    author=\"Yiming Zhang\",\n    description=\"M3DRef-CLIP\",\n    packages=find_packages(include=(\"m3drefclip*\")),\n    install_requires=[", "    packages=find_packages(include=(\"m3drefclip*\")),\n    install_requires=[\n        f\"clip @ git+ssh://git@github.com/eamonn-zh/CLIP.git\", \"lightning\", \"wandb\", \"scipy\", \"hydra-core\",\n        \"h5py\", \"open3d\", \"pandas\"\n    ]\n)\n"]}
{"filename": "train.py", "chunked_list": ["import os\nimport torch\nimport hydra\nimport lightning.pytorch as pl\nfrom m3drefclip.data.data_module import DataModule\nfrom lightning.pytorch.callbacks import LearningRateMonitor\nfrom m3drefclip.callback.gpu_cache_clean_callback import GPUCacheCleanCallback\nfrom m3drefclip.callback.lr_decay_callback import LrDecayCallback\n\n\ndef init_callbacks(cfg):\n    checkpoint_monitor = hydra.utils.instantiate(cfg.checkpoint_monitor)\n    gpu_cache_clean_monitor = GPUCacheCleanCallback()\n    lr_decay_callback = LrDecayCallback()\n    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n    return [checkpoint_monitor, gpu_cache_clean_monitor, lr_decay_callback, lr_monitor]", "\n\ndef init_callbacks(cfg):\n    checkpoint_monitor = hydra.utils.instantiate(cfg.checkpoint_monitor)\n    gpu_cache_clean_monitor = GPUCacheCleanCallback()\n    lr_decay_callback = LrDecayCallback()\n    lr_monitor = LearningRateMonitor(logging_interval=\"epoch\")\n    return [checkpoint_monitor, gpu_cache_clean_monitor, lr_decay_callback, lr_monitor]\n\n", "\n\n@hydra.main(version_base=None, config_path=\"config\", config_name=\"global_config\")\ndef main(cfg):\n    # fix the seed\n    pl.seed_everything(cfg.train_seed, workers=True)\n\n    # create directories for training outputs\n    os.makedirs(os.path.join(cfg.experiment_output_path, \"training\"), exist_ok=True)\n\n    # initialize data\n    data_module = DataModule(cfg.data)\n\n    # initialize model\n    model = hydra.utils.instantiate(cfg.model.model_name, cfg)\n\n    # load the pre-trained detector\n    if \"detector_path\" in cfg:\n        detector_weights = torch.load(cfg.detector_path)[\"state_dict\"]\n        model.detector.load_state_dict(detector_weights)\n\n    # initialize logger\n    logger = hydra.utils.instantiate(cfg.logger)\n\n    # initialize callbacks\n    callbacks = init_callbacks(cfg)\n\n    # initialize trainer\n    trainer = pl.Trainer(callbacks=callbacks, logger=logger, **cfg.trainer)\n\n    # check the checkpoint\n    if cfg.ckpt_path is not None:\n        assert os.path.exists(cfg.ckpt_path), \"Error: Checkpoint path does not exist.\"\n\n    # start training\n    trainer.fit(model=model, datamodule=data_module, ckpt_path=cfg.ckpt_path)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "evaluate.py", "chunked_list": ["from tqdm import tqdm\nimport numpy as np\nimport hydra\nimport torch\nimport json\nimport csv\nimport os\n\n\ndef generate_gt_scanrefer(split, lang_input_path, scene_root_path):\n    gt_dict = {}\n    scene_ids = {}\n    with open(lang_input_path, \"r\") as f:\n        raw_data = json.load(f)\n    for query in tqdm(raw_data, desc=\"Initializing ground truths\"):\n        scene_id = query[\"scene_id\"]\n        scene_ids[scene_id] = True\n        object_id = int(query[\"object_id\"])\n        scene_data = torch.load(os.path.join(scene_root_path, split, f\"{scene_id}.pth\"))\n        if \"object_ids\" not in query:\n            # for ScanRefer and Nr3D\n            object_ids = [object_id]\n        corners = scene_data[\"aabb_corner_xyz\"][np.in1d(scene_data[\"aabb_obj_ids\"], np.array(object_ids))]\n        aabb_min_max_bound = np.stack((corners.min(1), corners.max(1)), axis=1)\n        gt_dict[(scene_id, object_id, int(query[\"ann_id\"]))] = {\n            \"aabb_bound\": aabb_min_max_bound,\n            \"eval_type\": query[\"eval_type\"]\n        }\n    scene_ids = list(scene_ids.keys())\n    return gt_dict, scene_ids", "\ndef generate_gt_scanrefer(split, lang_input_path, scene_root_path):\n    gt_dict = {}\n    scene_ids = {}\n    with open(lang_input_path, \"r\") as f:\n        raw_data = json.load(f)\n    for query in tqdm(raw_data, desc=\"Initializing ground truths\"):\n        scene_id = query[\"scene_id\"]\n        scene_ids[scene_id] = True\n        object_id = int(query[\"object_id\"])\n        scene_data = torch.load(os.path.join(scene_root_path, split, f\"{scene_id}.pth\"))\n        if \"object_ids\" not in query:\n            # for ScanRefer and Nr3D\n            object_ids = [object_id]\n        corners = scene_data[\"aabb_corner_xyz\"][np.in1d(scene_data[\"aabb_obj_ids\"], np.array(object_ids))]\n        aabb_min_max_bound = np.stack((corners.min(1), corners.max(1)), axis=1)\n        gt_dict[(scene_id, object_id, int(query[\"ann_id\"]))] = {\n            \"aabb_bound\": aabb_min_max_bound,\n            \"eval_type\": query[\"eval_type\"]\n        }\n    scene_ids = list(scene_ids.keys())\n    return gt_dict, scene_ids", "\n\ndef generate_gt_nr3d(split, lang_input_path, scene_root_path):\n    gt_dict = {}\n    scene_ids = {}\n    tmp_ann_id_count = {}\n    raw_data = []\n    with open(lang_input_path, \"r\") as f:\n        csv_data = csv.DictReader(f)\n        for row in csv_data:\n            raw_data.append(row)\n\n    for query in tqdm(raw_data, desc=\"Initializing ground truths\"):\n        scene_id = query[\"scan_id\"]\n        scene_ids[scene_id] = True\n        object_id = int(query[\"target_id\"])\n\n        scene_obj_key = (scene_id, object_id)\n        if scene_obj_key not in tmp_ann_id_count:\n            tmp_ann_id_count[scene_obj_key] = 0\n        else:\n            tmp_ann_id_count[scene_obj_key] += 1\n\n        scene_data = torch.load(os.path.join(scene_root_path, split, f\"{scene_id}.pth\"))\n        object_ids = [object_id]\n        corners = scene_data[\"aabb_corner_xyz\"][np.in1d(scene_data[\"aabb_obj_ids\"], np.array(object_ids))]\n        aabb_min_max_bound = np.stack((corners.min(1), corners.max(1)), axis=1)\n\n        is_easy = query[\"is_easy\"] == \"True\"\n        is_view_dep = query[\"is_view_dep\"] == \"True\"\n        if is_easy and is_view_dep:\n            eval_type = \"easy_dep\"\n        elif is_easy:\n            eval_type = \"easy_indep\"\n        elif is_view_dep:\n            eval_type = \"hard_dep\"\n        else:\n            eval_type = \"hard_indep\"\n\n        gt_dict[(scene_id, object_id, tmp_ann_id_count[scene_obj_key])] = {\n            \"aabb_bound\": aabb_min_max_bound,\n            \"eval_type\": eval_type\n        }\n    scene_ids = list(scene_ids.keys())\n    return gt_dict, scene_ids", "\n\ndef parse_prediction(scene_ids, pred_file_root_path):\n    pred_dict = {}\n    for scene_id in scene_ids:\n        with open(os.path.join(pred_file_root_path, f\"{scene_id}.json\"), \"r\") as f:\n            scene_predictions = json.load(f)\n        for scene_prediction in scene_predictions:\n            corners = np.array(scene_prediction[\"aabb\"], dtype=np.float32)\n            aabb_min_max_bound = np.stack((corners.min(1), corners.max(1)), axis=1)\n            pred_dict[(scene_id, int(scene_prediction[\"object_id\"]), int(scene_prediction[\"ann_id\"]))] = {\n                \"aabb_bound\": aabb_min_max_bound\n            }\n    return pred_dict", "\n\n@hydra.main(version_base=None, config_path=\"config\", config_name=\"global_config\")\ndef main(cfg):\n    split = cfg.data.evaluation.split\n\n    # prepare gt\n    lang_input_path = getattr(cfg.data.lang_metadata, f\"{split}_language_data\")\n\n    assert os.path.exists(cfg.pred_path), f\"Error: Predictions file path {cfg.pred_path} does not exist.\"\n\n    if cfg.data.lang_dataset == \"ScanRefer\":\n        gt_data, scene_ids = generate_gt_scanrefer(split, lang_input_path, cfg.data.scene_dataset_path)\n    elif cfg.data.lang_dataset == \"Nr3D\":\n        gt_data, scene_ids = generate_gt_nr3d(split, lang_input_path, cfg.data.scene_dataset_path)\n    else:\n        raise NotImplementedError\n\n    # prepare predictions\n    pred_data = parse_prediction(scene_ids, cfg.pred_path)\n\n    evaluator = hydra.utils.instantiate(cfg.data.evaluator, verbose=True)\n    evaluator.set_ground_truths(gt_data)\n\n    _ = evaluator.evaluate(pred_data)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "test.py", "chunked_list": ["import os\nimport hydra\nimport lightning.pytorch as pl\nfrom m3drefclip.data.data_module import DataModule\n\n\n@hydra.main(version_base=None, config_path=\"config\", config_name=\"global_config\")\ndef main(cfg):\n    # fix the seed\n    pl.seed_everything(cfg.test_seed, workers=True)\n\n    # create directories for inference outputs\n    os.makedirs(os.path.join(cfg.pred_path, cfg.data.inference.split), exist_ok=True)\n\n    # initialize data\n    data_module = DataModule(cfg.data)\n\n    # initialize model\n    cfg.data.evaluator.verbose = True  # print out evaluation results after inference\n    model = hydra.utils.instantiate(cfg.model.model_name, cfg)\n\n    # initialize trainer\n    trainer = pl.Trainer(accelerator=cfg.trainer.accelerator, devices=1, max_epochs=1, logger=False)\n\n    # check the checkpoint\n    assert cfg.ckpt_path is not None, \"Error: Checkpoint path is not provided.\"\n    assert os.path.exists(cfg.ckpt_path), f\"Error: Checkpoint path {cfg.ckpt_path} does not exist.\"\n\n    # start inference\n    trainer.test(model=model, datamodule=data_module, ckpt_path=cfg.ckpt_path)", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "m3drefclip/evaluation/referit3d_evaluator.py", "chunked_list": ["import torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom m3drefclip.util.utils import get_batch_aabb_pair_ious\nfrom m3drefclip.evaluation.general_evaluator import GeneralEvaluator\n\n\nIOU_THRESHOLD = 0.9  # referit3d uses GT boxes, a dummy iou here\n\n\nclass ReferIt3DEvaluator(GeneralEvaluator):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.evaluation_types = {\"easy_dep\": 0, \"easy_indep\": 1, \"hard_dep\": 2, \"hard_indep\": 3}\n        self.evaluation_types_comb = {\"easy\": (0, 1), \"hard\": (2, 3), \"view_dep\": (0, 2), \"view_indep\": (1, 3)}\n\n    def _print_results(self, results):\n        print(f\"{'=' * 55}\")\n        print(\"{0:<12}{1:<12}{2:<12}{3:<12}{4:<12}\".format(\"easy\", \"hard\", \"view-dep\", \"view-indep\", \"overall\"))\n        print(f\"{'-' * 55}\")\n        line_1_str = ''\n        for sub_group_type, score in results.items():\n            line_1_str += '{:<12.1f}'.format(score * 100)\n        print(line_1_str)\n        print(f\"{'=' * 55}\")\n\n    def evaluate(self, predictions):\n        all_gt_info_len = len(self.ground_truths)\n        eval_type_mask = np.empty(all_gt_info_len, dtype=np.uint8)\n        tps = np.zeros(all_gt_info_len, dtype=bool)\n        iterator = enumerate(tqdm(predictions.items(), desc=\"Evaluating\") if self.verbose else predictions.items())\n        for i, (key, value) in iterator:\n            eval_type_mask[i] = self.evaluation_types[self.ground_truths[key][\"eval_type\"]]\n            tps[i] = self._evaluate_one_query(value, self.ground_truths[key])\n        results = {}\n        for sub_group in self.evaluation_types_comb.keys():\n            selected_indices = np.isin(eval_type_mask, np.array(self.evaluation_types_comb[sub_group], dtype=np.uint8))\n            if np.any(selected_indices):\n                results[sub_group] = np.count_nonzero(tps[selected_indices]) / np.count_nonzero(selected_indices)\n            else:\n                results[sub_group] = np.nan\n        results[\"overall\"] = np.count_nonzero(tps) / tps.shape[0]\n\n        if self.verbose:\n            self._print_results(results)\n\n        return {self.metric_name: results}\n\n    def _evaluate_one_query(self, pred_info, gt_info):\n        # initialize true positives\n        tp = 0\n\n        # TODO: convert to batch process\n        iou = get_batch_aabb_pair_ious(\n            torch.from_numpy(pred_info[\"aabb_bound\"]), torch.from_numpy(gt_info[\"aabb_bound\"])\n        )[0].item()\n        if iou >= IOU_THRESHOLD:\n            tp += 1\n        return tp", "\n\nclass ReferIt3DEvaluator(GeneralEvaluator):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.evaluation_types = {\"easy_dep\": 0, \"easy_indep\": 1, \"hard_dep\": 2, \"hard_indep\": 3}\n        self.evaluation_types_comb = {\"easy\": (0, 1), \"hard\": (2, 3), \"view_dep\": (0, 2), \"view_indep\": (1, 3)}\n\n    def _print_results(self, results):\n        print(f\"{'=' * 55}\")\n        print(\"{0:<12}{1:<12}{2:<12}{3:<12}{4:<12}\".format(\"easy\", \"hard\", \"view-dep\", \"view-indep\", \"overall\"))\n        print(f\"{'-' * 55}\")\n        line_1_str = ''\n        for sub_group_type, score in results.items():\n            line_1_str += '{:<12.1f}'.format(score * 100)\n        print(line_1_str)\n        print(f\"{'=' * 55}\")\n\n    def evaluate(self, predictions):\n        all_gt_info_len = len(self.ground_truths)\n        eval_type_mask = np.empty(all_gt_info_len, dtype=np.uint8)\n        tps = np.zeros(all_gt_info_len, dtype=bool)\n        iterator = enumerate(tqdm(predictions.items(), desc=\"Evaluating\") if self.verbose else predictions.items())\n        for i, (key, value) in iterator:\n            eval_type_mask[i] = self.evaluation_types[self.ground_truths[key][\"eval_type\"]]\n            tps[i] = self._evaluate_one_query(value, self.ground_truths[key])\n        results = {}\n        for sub_group in self.evaluation_types_comb.keys():\n            selected_indices = np.isin(eval_type_mask, np.array(self.evaluation_types_comb[sub_group], dtype=np.uint8))\n            if np.any(selected_indices):\n                results[sub_group] = np.count_nonzero(tps[selected_indices]) / np.count_nonzero(selected_indices)\n            else:\n                results[sub_group] = np.nan\n        results[\"overall\"] = np.count_nonzero(tps) / tps.shape[0]\n\n        if self.verbose:\n            self._print_results(results)\n\n        return {self.metric_name: results}\n\n    def _evaluate_one_query(self, pred_info, gt_info):\n        # initialize true positives\n        tp = 0\n\n        # TODO: convert to batch process\n        iou = get_batch_aabb_pair_ious(\n            torch.from_numpy(pred_info[\"aabb_bound\"]), torch.from_numpy(gt_info[\"aabb_bound\"])\n        )[0].item()\n        if iou >= IOU_THRESHOLD:\n            tp += 1\n        return tp", ""]}
{"filename": "m3drefclip/evaluation/scanrefer_evaluator.py", "chunked_list": ["import os\nimport json\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom m3drefclip.util.utils import get_batch_aabb_pair_ious\nfrom m3drefclip.evaluation.general_evaluator import GeneralEvaluator\n\n\nclass ScanReferEvaluator(GeneralEvaluator):\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.evaluation_types = {\"unique\": 0, \"multiple\": 1}\n\n    def _print_results(self, iou_25_results, iou_50_results):\n        print(f\"{'=' * 43}\")\n        print(\"{0:<12}{1:<12}{2:<12}{3:<12}\".format(\"IoU\", \"unique\", \"multiple\", \"overall\"))\n        print(f\"{'-' * 43}\")\n        line_1_str = '{:<12}'.format(\"0.25\")\n        for sub_group_type, score in iou_25_results.items():\n            line_1_str += '{:<12.1f}'.format(score * 100)\n        print(line_1_str)\n        line_2_str = '{:<12}'.format(\"0.50\")\n        for sub_group_type, score in iou_50_results.items():\n            line_2_str += '{:<12.1f}'.format(score * 100)\n        print(line_2_str)\n        print(f\"{'=' * 43}\")\n\n    def evaluate(self, predictions):\n        all_gt_info_len = len(self.ground_truths)\n        eval_type_mask = np.empty(all_gt_info_len, dtype=bool)\n        iou_25_tps = np.zeros(all_gt_info_len, dtype=bool)\n        iou_50_tps = np.zeros(all_gt_info_len, dtype=bool)\n        iterator = enumerate(tqdm(predictions.items(), desc=\"Evaluating\") if self.verbose else predictions.items())\n        for i, (key, value) in iterator:\n            eval_type_mask[i] = self.evaluation_types[self.ground_truths[key][\"eval_type\"]]\n            iou_25_tps[i], iou_50_tps[i] = self._evaluate_one_query(value, self.ground_truths[key])\n        iou_25_results = {}\n        iou_50_results = {}\n        for sub_group in self.evaluation_types.keys():\n            selected_indices = eval_type_mask == self.evaluation_types[sub_group]\n            if np.any(selected_indices):\n                iou_25_results[sub_group] = np.count_nonzero(iou_25_tps[selected_indices]) / np.count_nonzero(selected_indices)\n                iou_50_results[sub_group] = np.count_nonzero(iou_50_tps[selected_indices]) / np.count_nonzero(selected_indices)\n            else:\n                iou_25_results[sub_group] = np.nan\n                iou_50_results[sub_group] = np.nan\n        iou_25_results[\"overall\"] = np.count_nonzero(iou_25_tps) / iou_25_tps.shape[0]\n        iou_50_results[\"overall\"] = np.count_nonzero(iou_50_tps) / iou_50_tps.shape[0]\n\n        if self.verbose:\n            self._print_results(iou_25_results, iou_50_results)\n\n        return {f\"{self.metric_name}@0.25\": iou_25_results, f\"{self.metric_name}@0.5\": iou_50_results}\n\n    def _evaluate_one_query(self, pred_info, gt_info):\n        # initialize true positives\n        iou_25_tp = 0\n        iou_50_tp = 0\n\n        # TODO: convert to batch process\n        iou = get_batch_aabb_pair_ious(\n            torch.from_numpy(pred_info[\"aabb_bound\"]), torch.from_numpy(gt_info[\"aabb_bound\"])\n        )[0].item()\n        if iou >= 0.25:\n            iou_25_tp += 1\n        if iou >= 0.5:\n            iou_50_tp += 1\n        return iou_25_tp, iou_50_tp", "\nclass ScanReferEvaluator(GeneralEvaluator):\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.evaluation_types = {\"unique\": 0, \"multiple\": 1}\n\n    def _print_results(self, iou_25_results, iou_50_results):\n        print(f\"{'=' * 43}\")\n        print(\"{0:<12}{1:<12}{2:<12}{3:<12}\".format(\"IoU\", \"unique\", \"multiple\", \"overall\"))\n        print(f\"{'-' * 43}\")\n        line_1_str = '{:<12}'.format(\"0.25\")\n        for sub_group_type, score in iou_25_results.items():\n            line_1_str += '{:<12.1f}'.format(score * 100)\n        print(line_1_str)\n        line_2_str = '{:<12}'.format(\"0.50\")\n        for sub_group_type, score in iou_50_results.items():\n            line_2_str += '{:<12.1f}'.format(score * 100)\n        print(line_2_str)\n        print(f\"{'=' * 43}\")\n\n    def evaluate(self, predictions):\n        all_gt_info_len = len(self.ground_truths)\n        eval_type_mask = np.empty(all_gt_info_len, dtype=bool)\n        iou_25_tps = np.zeros(all_gt_info_len, dtype=bool)\n        iou_50_tps = np.zeros(all_gt_info_len, dtype=bool)\n        iterator = enumerate(tqdm(predictions.items(), desc=\"Evaluating\") if self.verbose else predictions.items())\n        for i, (key, value) in iterator:\n            eval_type_mask[i] = self.evaluation_types[self.ground_truths[key][\"eval_type\"]]\n            iou_25_tps[i], iou_50_tps[i] = self._evaluate_one_query(value, self.ground_truths[key])\n        iou_25_results = {}\n        iou_50_results = {}\n        for sub_group in self.evaluation_types.keys():\n            selected_indices = eval_type_mask == self.evaluation_types[sub_group]\n            if np.any(selected_indices):\n                iou_25_results[sub_group] = np.count_nonzero(iou_25_tps[selected_indices]) / np.count_nonzero(selected_indices)\n                iou_50_results[sub_group] = np.count_nonzero(iou_50_tps[selected_indices]) / np.count_nonzero(selected_indices)\n            else:\n                iou_25_results[sub_group] = np.nan\n                iou_50_results[sub_group] = np.nan\n        iou_25_results[\"overall\"] = np.count_nonzero(iou_25_tps) / iou_25_tps.shape[0]\n        iou_50_results[\"overall\"] = np.count_nonzero(iou_50_tps) / iou_50_tps.shape[0]\n\n        if self.verbose:\n            self._print_results(iou_25_results, iou_50_results)\n\n        return {f\"{self.metric_name}@0.25\": iou_25_results, f\"{self.metric_name}@0.5\": iou_50_results}\n\n    def _evaluate_one_query(self, pred_info, gt_info):\n        # initialize true positives\n        iou_25_tp = 0\n        iou_50_tp = 0\n\n        # TODO: convert to batch process\n        iou = get_batch_aabb_pair_ious(\n            torch.from_numpy(pred_info[\"aabb_bound\"]), torch.from_numpy(gt_info[\"aabb_bound\"])\n        )[0].item()\n        if iou >= 0.25:\n            iou_25_tp += 1\n        if iou >= 0.5:\n            iou_50_tp += 1\n        return iou_25_tp, iou_50_tp", ""]}
{"filename": "m3drefclip/evaluation/general_evaluator.py", "chunked_list": ["from abc import ABC, abstractmethod\n\n\nclass GeneralEvaluator(ABC):\n\n    def __init__(self, metric_name, gts_path=None, verbose=True):\n        self.verbose = verbose  # print progress bar and results or not\n        self.metric_name = metric_name\n        self.ground_truths = None\n        # if gts_path is not None, load ground truth files from disk, set it manually otherwise.\n        if gts_path is not None:\n            self._set_ground_truths_from_files(gts_path)\n\n    def set_ground_truths(self, ground_truths):\n        self.ground_truths = ground_truths\n\n\n    @abstractmethod\n    def _print_results(self, *args, **kwargs):\n        pass\n\n    @abstractmethod\n    def evaluate(self, predictions):\n        pass", "\n"]}
{"filename": "m3drefclip/data/data_module.py", "chunked_list": ["import torch\nimport MinkowskiEngine as ME\nimport lightning.pytorch as pl\nfrom importlib import import_module\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data._utils.collate import default_collate\n\n\nclass DataModule(pl.LightningDataModule):\n    def __init__(self, data_cfg):\n        super().__init__()\n        self.data_cfg = data_cfg\n        dataset_name = data_cfg.lang_dataset\n        self.dataset = getattr(import_module(f\"m3drefclip.data.dataset.{dataset_name.lower()}\"), dataset_name)\n\n    def setup(self, stage=None):\n        if stage == \"fit\":\n            self.train_set = self.dataset(self.data_cfg, \"train\")\n            self.val_set = self.dataset(self.data_cfg, \"val\")\n        if stage == \"test\":\n            self.test_set = self.dataset(self.data_cfg, self.data_cfg.inference.split)\n        if stage == \"predict\":\n            self.test_set = self.dataset(self.data_cfg, \"test\")\n\n    def train_dataloader(self):\n        self.train_set.shuffle_chunks()  # shuffle language data chunks after each epoch\n        return DataLoader(self.train_set, batch_size=self.data_cfg.dataloader.batch_size, shuffle=True, pin_memory=True,\n                          collate_fn=_collate_fn, num_workers=self.data_cfg.dataloader.num_workers, drop_last=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_set, batch_size=self.data_cfg.dataloader.batch_size, pin_memory=True,\n                          collate_fn=_collate_fn, num_workers=self.data_cfg.dataloader.num_workers)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, batch_size=self.data_cfg.dataloader.batch_size, pin_memory=True,\n                          collate_fn=_collate_fn, num_workers=self.data_cfg.dataloader.num_workers)\n\n    def predict_dataloader(self):\n        return DataLoader(self.test_set, batch_size=self.data_cfg.dataloader.batch_size, pin_memory=True,\n                          collate_fn=_collate_fn, num_workers=self.data_cfg.dataloader.num_workers)", "class DataModule(pl.LightningDataModule):\n    def __init__(self, data_cfg):\n        super().__init__()\n        self.data_cfg = data_cfg\n        dataset_name = data_cfg.lang_dataset\n        self.dataset = getattr(import_module(f\"m3drefclip.data.dataset.{dataset_name.lower()}\"), dataset_name)\n\n    def setup(self, stage=None):\n        if stage == \"fit\":\n            self.train_set = self.dataset(self.data_cfg, \"train\")\n            self.val_set = self.dataset(self.data_cfg, \"val\")\n        if stage == \"test\":\n            self.test_set = self.dataset(self.data_cfg, self.data_cfg.inference.split)\n        if stage == \"predict\":\n            self.test_set = self.dataset(self.data_cfg, \"test\")\n\n    def train_dataloader(self):\n        self.train_set.shuffle_chunks()  # shuffle language data chunks after each epoch\n        return DataLoader(self.train_set, batch_size=self.data_cfg.dataloader.batch_size, shuffle=True, pin_memory=True,\n                          collate_fn=_collate_fn, num_workers=self.data_cfg.dataloader.num_workers, drop_last=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_set, batch_size=self.data_cfg.dataloader.batch_size, pin_memory=True,\n                          collate_fn=_collate_fn, num_workers=self.data_cfg.dataloader.num_workers)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, batch_size=self.data_cfg.dataloader.batch_size, pin_memory=True,\n                          collate_fn=_collate_fn, num_workers=self.data_cfg.dataloader.num_workers)\n\n    def predict_dataloader(self):\n        return DataLoader(self.test_set, batch_size=self.data_cfg.dataloader.batch_size, pin_memory=True,\n                          collate_fn=_collate_fn, num_workers=self.data_cfg.dataloader.num_workers)", "\n\ndef _collate_fn(batch):\n    data_dict = {}\n\n    # default collation\n    default_collate_item_names = (\"scene_id\", \"object_id\", \"ann_id\", \"clip_tokens\", \"scene_center_xyz\")\n    default_collate_data = []\n\n    point_count_total = 0\n    all_point_count_total = 0\n    aabb_count_total = 0\n\n    data_dict[\"point_count_offsets\"] = torch.zeros(size=(len(batch) + 1,), dtype=torch.int32)\n    data_dict[\"aabb_count_offsets\"] = torch.zeros(size=(len(batch) + 1,), dtype=torch.int32)\n    data_dict[\"all_point_count_offsets\"] = torch.zeros(size=(len(batch) + 1,), dtype=torch.int32)\n    data_dict[\"eval_type\"] = []\n\n    vert_batch_ids = []\n\n    for i, b in enumerate(batch):\n        # organize default collation\n        default_collate_data.append({k: b[k] for k in default_collate_item_names})\n\n        # pre-calculate the numbers of total points and aabbs for sparse collation\n        point_count_total += b[\"point_xyz\"].shape[0]\n        all_point_count_total += b[\"all_point_xyz\"].shape[0]\n        data_dict[\"all_point_count_offsets\"][i + 1] = all_point_count_total\n        aabb_count_total += b[\"gt_aabb_min_max_bounds\"].shape[0]\n        data_dict[\"point_count_offsets\"][i + 1] = point_count_total\n        data_dict[\"aabb_count_offsets\"][i + 1] = aabb_count_total\n        data_dict[\"eval_type\"].append(b[\"eval_type\"])\n        vert_batch_ids.append(\n            torch.full((b[\"point_xyz\"].shape[0],), fill_value=i, dtype=torch.uint8)\n        )\n\n    data_dict[\"vert_batch_ids\"] = torch.cat(vert_batch_ids, dim=0)\n    data_dict.update(default_collate(default_collate_data))\n    lang_chunk_size = data_dict[\"ann_id\"].shape[1]\n\n    # sparse collation\n    data_dict[\"point_xyz\"] = torch.empty(size=(point_count_total, 3), dtype=torch.float32)\n\n    data_dict[\"all_point_xyz\"] = torch.empty(size=(all_point_count_total, 3), dtype=torch.float32)\n    data_dict[\"all_point_rgb\"] = torch.empty_like(data_dict[\"all_point_xyz\"])\n\n    data_dict[\"instance_ids\"] = torch.empty(size=(point_count_total,), dtype=torch.int16)\n    data_dict[\"sem_labels\"] = torch.empty_like(data_dict[\"instance_ids\"], dtype=torch.long)\n    data_dict[\"instance_centers\"] = torch.empty_like(data_dict[\"point_xyz\"])\n\n    data_dict[\"gt_aabb_min_max_bounds\"] = torch.empty(size=(aabb_count_total, 2, 3), dtype=torch.float32)\n    data_dict[\"gt_aabb_obj_ids\"] = torch.empty(size=(aabb_count_total, ), dtype=torch.int16)\n\n    data_dict[\"gt_target_obj_id_mask\"] = torch.empty(\n        size=(aabb_count_total, lang_chunk_size), dtype=torch.bool\n    )\n\n    num_voxel_batch = 0\n    voxel_xyz_list = []\n    voxel_features_list = []\n    voxel_point_map_list = []\n    instance_num_point = []\n\n    num_instances = 0\n    for i, b in enumerate(batch):\n        batch_points_start_idx = data_dict[\"point_count_offsets\"][i]\n        batch_points_end_idx = data_dict[\"point_count_offsets\"][i+1]\n\n        data_dict[\"all_point_xyz\"][data_dict[\"all_point_count_offsets\"][i]:data_dict[\"all_point_count_offsets\"][i+1]] = \\\n            torch.from_numpy(b[\"all_point_xyz\"])\n        data_dict[\"all_point_rgb\"][\n        data_dict[\"all_point_count_offsets\"][i]:data_dict[\"all_point_count_offsets\"][i+1]] = torch.from_numpy(\n            b[\"all_point_rgb\"])\n        data_dict[\"point_xyz\"][batch_points_start_idx:batch_points_end_idx] = torch.from_numpy(b[\"point_xyz\"])\n\n        instance_ids_tmp = torch.from_numpy(b[\"instance_ids\"])\n        instance_ids_tmp[instance_ids_tmp != -1] += num_instances\n        num_instances += b[\"num_instances\"]\n        data_dict[\"instance_ids\"][batch_points_start_idx:batch_points_end_idx] = instance_ids_tmp\n\n        data_dict[\"sem_labels\"][batch_points_start_idx:batch_points_end_idx] = torch.from_numpy(b[\"sem_labels\"])\n        data_dict[\"instance_centers\"][batch_points_start_idx:batch_points_end_idx] = torch.from_numpy(\n            b[\"instance_centers\"]\n        )\n        instance_num_point.append(torch.from_numpy(b[\"instance_num_point\"]))\n\n        voxel_point_map_list.append(b[\"voxel_point_map\"] + num_voxel_batch)\n        num_voxel_batch += b[\"voxel_xyz\"].shape[0]\n\n        voxel_xyz_list.append(b[\"voxel_xyz\"])\n        voxel_features_list.append(b[\"voxel_features\"])\n\n        batch_aabbs_start_idx = data_dict[\"aabb_count_offsets\"][i]\n        batch_aabbs_end_idx = data_dict[\"aabb_count_offsets\"][i+1]\n        data_dict[\"gt_aabb_min_max_bounds\"][batch_aabbs_start_idx:batch_aabbs_end_idx] = \\\n            torch.from_numpy(b[\"gt_aabb_min_max_bounds\"])\n        data_dict[\"gt_aabb_obj_ids\"][batch_aabbs_start_idx:batch_aabbs_end_idx] = \\\n            torch.from_numpy(b[\"gt_aabb_obj_ids\"])\n        data_dict[\"gt_target_obj_id_mask\"][batch_aabbs_start_idx:batch_aabbs_end_idx] = \\\n            torch.from_numpy(b[\"gt_target_obj_id_mask\"]).permute(dims=(1, 0))\n\n    data_dict[\"instance_num_point\"] = torch.cat(instance_num_point, dim=0)\n\n    data_dict[\"voxel_xyz\"], data_dict[\"voxel_features\"] = ME.utils.sparse_collate(\n        coords=voxel_xyz_list, feats=voxel_features_list\n    )\n    data_dict[\"voxel_point_map\"] = torch.cat(voxel_point_map_list, dim=0)\n    return data_dict", ""]}
{"filename": "m3drefclip/data/dataset/nr3d.py", "chunked_list": ["from m3drefclip.data.dataset.general_dataset import GeneralDataset\nimport numpy as np\nimport torch\nimport clip\nimport csv\n\n\nclass Nr3D(GeneralDataset):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _load_language_data(self):\n        # create a map, skip invalid labels to make the final semantic labels consecutive\n        filtered_label_map = {}\n        for i, valid_id in enumerate(self.data_cfg.scene_metadata.valid_semantic_mapping):\n            filtered_label_map[valid_id] = i\n\n        file_path = getattr(self.data_cfg.lang_metadata, f\"{self.split}_language_data\")\n\n        raw_data = []\n        with open(file_path, \"r\") as f:\n            csv_data = csv.DictReader(f)\n            for row in csv_data:\n                raw_data.append(row)\n\n        # TODO\n        tmp_ann_id_count = {}\n        self.language_data = {}\n        scene_ids = {}\n        for item in raw_data:\n            scene_ids[item[\"scan_id\"]] = True\n            # TODO\n            scene_obj_key = (item[\"scan_id\"], int(item[\"target_id\"]))\n            if scene_obj_key not in tmp_ann_id_count:\n                tmp_ann_id_count[scene_obj_key] = 0\n            else:\n                tmp_ann_id_count[scene_obj_key] += 1\n            if item[\"scan_id\"] not in self.language_data:\n                self.language_data[item[\"scan_id\"]] = []\n\n            is_easy = item[\"is_easy\"] == \"True\"\n            is_view_dep = item[\"is_view_dep\"] == \"True\"\n            if is_easy and is_view_dep:\n                eval_type = \"easy_dep\"\n            elif is_easy:\n                eval_type = \"easy_indep\"\n            elif is_view_dep:\n                eval_type = \"hard_dep\"\n            else:\n                eval_type = \"hard_indep\"\n            self.language_data[item[\"scan_id\"]].append(\n                {\n                    \"object_id\": int(item[\"target_id\"]),\n                    \"object_name\": item[\"instance_type\"],\n                    \"ann_id\": tmp_ann_id_count[scene_obj_key],\n                    \"eval_type\": eval_type,\n                    \"clip_tokens\": clip.tokenize(item[\"utterance\"].strip(), truncate=True)[0]\n                }\n            )\n        self.scene_ids = list(scene_ids.keys())\n\n    def __getitem__(self, index):\n        data_dict = super().__getitem__(index)  # get scene data from parent class\n\n        # prepare language data\n        scene_id = self.scene_ids[self.chunk_scene_indices[index]]\n        language_data_indices = self.chunk_lang_indices[index]\n        language_data_in_scene = self.language_data[scene_id]\n        num_language_data_in_scene = len(language_data_in_scene)\n\n        data_dict[\"ann_id\"] = np.empty(shape=self.data_cfg.chunk_size, dtype=np.uint8)\n        data_dict[\"object_id\"] = np.empty(shape=self.data_cfg.chunk_size, dtype=np.int16)\n\n        data_dict[\"gt_target_obj_id_mask\"] = np.zeros(\n            shape=(self.data_cfg.chunk_size, data_dict[\"gt_aabb_obj_ids\"].shape[0]), dtype=bool\n        )\n        data_dict[\"clip_tokens\"] = torch.empty(size=(self.data_cfg.chunk_size, 77), dtype=torch.int32)\n        data_dict[\"eval_type\"] = []\n        for i, index in enumerate(language_data_indices):\n            real_idx = index % num_language_data_in_scene  # pad the last chunk\n            data = language_data_in_scene[real_idx]\n            data_dict[\"ann_id\"][i] = data[\"ann_id\"]\n            data_dict[\"object_id\"][i] = data[\"object_id\"]\n            data_dict[\"gt_target_obj_id_mask\"][i] = np.in1d(data_dict[\"gt_aabb_obj_ids\"], data[\"object_id\"])\n            data_dict[\"clip_tokens\"][i] = data[\"clip_tokens\"]\n            data_dict[\"eval_type\"].append(data[\"eval_type\"])\n        return data_dict", ""]}
{"filename": "m3drefclip/data/dataset/general_dataset.py", "chunked_list": ["import os\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport random\nimport math\nimport h5py\nimport MinkowskiEngine as ME\nfrom abc import abstractmethod\nfrom torch.utils.data import Dataset", "from abc import abstractmethod\nfrom torch.utils.data import Dataset\n\n\nclass GeneralDataset(Dataset):\n    def __init__(self, data_cfg, split):\n        self.data_cfg = data_cfg\n        self.split = split\n\n        # load language data from disk to memory\n        self._load_language_data()\n\n        # load scene data from disk to memory\n        self._load_scene_data()\n\n        # pack scene and language data\n        self._pack_data_to_chunks()\n\n    def _open_hdf5(self):\n        self.multiview_data = h5py.File(self.data_cfg.scene_metadata.scene_multiview_file, \"r\", libver=\"latest\")\n\n    def _load_scene_data(self):\n        scene_data_path = self.data_cfg.scene_dataset_path\n        self.all_scene_data = {}\n        for scene_id in tqdm(self.scene_ids, desc=f\"Loading {self.split} data from disk\"):\n            scene_path = os.path.join(scene_data_path, self.split, f\"{scene_id}.pth\")\n            scene_data = torch.load(scene_path)\n            scene_data[\"rgb\"] = scene_data[\"rgb\"].astype(np.float32) / 127.5 - 1  # scale rgb to [-1, 1]\n            self.all_scene_data[scene_id] = scene_data\n\n    @abstractmethod\n    def _load_language_data(self):\n        # this function is overridden by child class\n        pass\n\n    def _pack_data_to_chunks(self):\n        # this array maintains lists of pointers pointing to language and scene data\n        self.chunk_lang_indices = np.empty(shape=(0, self.data_cfg.chunk_size), dtype=np.uint16)\n        self.chunk_scene_indices = np.empty(shape=0, dtype=np.uint16)\n        for i, scene_id in enumerate(self.scene_ids):\n            num_of_chunks = math.ceil(len(self.language_data[scene_id]) / self.data_cfg.chunk_size)\n            all_lang_indices = np.arange(num_of_chunks * self.data_cfg.chunk_size, dtype=np.uint16) # max 65535\n            np.random.shuffle(all_lang_indices)\n            chunked_lang_indices = np.split(all_lang_indices, num_of_chunks)\n            chunked_scene_indices = np.full(shape=num_of_chunks, fill_value=i, dtype=np.uint16)\n            self.chunk_lang_indices = np.concatenate((self.chunk_lang_indices, chunked_lang_indices), axis=0)\n            self.chunk_scene_indices = np.concatenate((self.chunk_scene_indices, chunked_scene_indices), axis=0)\n\n    def _get_xyz_augment_matrix(self):\n        aug_settings = self.data_cfg.scene_augmentation\n        m = np.eye(3, dtype=np.float32)\n        if self.split == \"train\" and aug_settings.jitter_xyz:\n            m += np.random.randn(3, 3) * 0.1\n        if self.split == \"train\" and aug_settings.flip_x and random.random() > 0.5:\n            m[0][0] *= -1\n        if self.split == \"train\" and aug_settings.rotate_z:\n            rad = random.choice((0, 0.5, 1, 1.5)) * np.pi  # randomly rotate around z-axis by 0, 90, 180, 270 degrees\n            c = np.cos(rad)\n            s = np.sin(rad)\n            m = m @ np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n        return m.astype(np.float32)\n\n    @abstractmethod\n    def _augment_language(self):\n        # this function is overridden by child class\n        pass\n\n    def shuffle_chunks(self):\n        # called after each epoch\n        self._pack_data_to_chunks()\n\n    def __len__(self):\n        return len(self.chunk_lang_indices)\n\n    def __getitem__(self, index):\n        data_dict = {}\n        scene_id = self.scene_ids[self.chunk_scene_indices[index]]\n        scene_data = self.all_scene_data[scene_id]\n        scene_center_xyz = scene_data[\"xyz\"].mean(axis=0)\n\n        original_num_points = scene_data[\"xyz\"].shape[0]\n        choices = np.ones(shape=original_num_points, dtype=bool)\n\n        # sample points\n        if self.split == \"train\" and original_num_points > self.data_cfg.max_num_point:\n            choices = np.random.choice(original_num_points, self.data_cfg.max_num_point, replace=False)\n\n        # augment the whole scene (only applicable for the train set)\n        xyz_augment_matrix = self._get_xyz_augment_matrix()\n        data_dict[\"point_xyz\"] = (scene_data[\"xyz\"] - scene_center_xyz)[choices] @ xyz_augment_matrix\n        point_normal = scene_data[\"normal\"][choices] @ np.linalg.inv(xyz_augment_matrix).transpose()\n        point_rgb = scene_data[\"rgb\"][choices]\n        data_dict[\"instance_ids\"] = scene_data[\"instance_ids\"][choices]\n        data_dict[\"sem_labels\"] = scene_data[\"sem_labels\"][choices].astype(np.int64)\n\n        data_dict[\"scene_center_xyz\"] = scene_center_xyz  # used to recover the original pointcloud coordinates\n\n        instance_num_point = []  # (nInst), int\n        unique_instance_ids = np.unique(data_dict[\"instance_ids\"])\n        unique_instance_ids = unique_instance_ids[unique_instance_ids != -1]\n        data_dict[\"num_instances\"] = unique_instance_ids.shape[0]\n        instance_centers = np.empty(shape=(data_dict[\"point_xyz\"].shape[0], 3), dtype=np.float32)\n\n        for index, i in enumerate(unique_instance_ids):\n            assert index == i  # make sure it is consecutive\n            inst_i_idx = np.where(data_dict[\"instance_ids\"] == i)[0]\n            mean_xyz_i = data_dict[\"point_xyz\"][inst_i_idx].mean(0)  # instance_info\n            instance_centers[inst_i_idx] = mean_xyz_i  # offset\n            instance_num_point.append(inst_i_idx.size)  # instance_num_point\n\n        data_dict[\"instance_num_point\"] = np.array(instance_num_point, dtype=np.int32)\n        data_dict[\"instance_centers\"] = instance_centers\n\n        # TODO\n        data_dict[\"all_point_xyz\"] = (scene_data[\"xyz\"] - scene_center_xyz) @ xyz_augment_matrix\n        data_dict[\"all_point_rgb\"] = (scene_data[\"rgb\"] + 1) / 2\n\n        # augment axis-aligned bounding boxes in the scene\n        augmented_gt_aabb_corners_tmp = (scene_data[\"aabb_corner_xyz\"] - scene_center_xyz) @ xyz_augment_matrix\n        data_dict[\"gt_aabb_min_max_bounds\"] = np.stack(\n            (augmented_gt_aabb_corners_tmp.min(1), augmented_gt_aabb_corners_tmp.max(1)), axis=1\n        )\n        data_dict[\"gt_aabb_obj_ids\"] = scene_data[\"aabb_obj_ids\"]\n\n        # quantize points to voxels\n        point_features = np.empty(shape=(data_dict[\"point_xyz\"].shape[0], 0), dtype=np.float32)\n        if self.data_cfg.point_features.use_rgb:\n            point_features = np.concatenate((point_features, point_rgb), axis=1)\n        if self.data_cfg.point_features.use_normal:\n            point_features = np.concatenate((point_features, point_normal), axis=1)\n        if self.data_cfg.point_features.use_multiview:\n            if not hasattr(self, 'multiview_data'):\n                self._open_hdf5()\n            point_features = np.concatenate((point_features, self.multiview_data[scene_id][()][choices]), axis=1)\n\n        point_features = np.concatenate((point_features, data_dict[\"point_xyz\"]), axis=1)\n\n        data_dict[\"voxel_xyz\"], data_dict[\"voxel_features\"], _, data_dict[\"voxel_point_map\"] = ME.utils.sparse_quantize(\n            coordinates=data_dict[\"point_xyz\"] - data_dict[\"point_xyz\"].min(axis=0), features=point_features, return_index=True,\n            return_inverse=True, quantization_size=self.data_cfg.voxel_size\n        )\n        data_dict[\"scene_id\"] = scene_id\n        return data_dict", ""]}
{"filename": "m3drefclip/data/dataset/scanrefer.py", "chunked_list": ["from m3drefclip.data.dataset.general_dataset import GeneralDataset\nimport numpy as np\nimport torch\nimport clip\nimport json\n\n\nclass ScanRefer(GeneralDataset):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def _load_language_data(self):\n        # create a map, skip invalid labels to make the final semantic labels consecutive\n        filtered_label_map = {}\n        for i, valid_id in enumerate(self.data_cfg.scene_metadata.valid_semantic_mapping):\n            filtered_label_map[valid_id] = i\n\n        file_path = getattr(self.data_cfg.lang_metadata, f\"{self.split}_language_data\")\n        with open(file_path, \"r\") as f:\n            raw_data = json.load(f)\n\n        self.language_data = {}\n        scene_ids = {}\n        for item in raw_data:\n            scene_ids[item[\"scene_id\"]] = True\n            if item[\"scene_id\"] not in self.language_data:\n                self.language_data[item[\"scene_id\"]] = []\n\n            object_name = item[\"object_name\"].replace(\"_\", \" \")\n            self.language_data[item[\"scene_id\"]].append(\n                {\n                    \"object_id\": int(item[\"object_id\"]),\n                    \"object_name\": object_name,\n                    \"ann_id\": int(item[\"ann_id\"]),\n                    \"eval_type\": item[\"eval_type\"],\n                    \"clip_tokens\": clip.tokenize(item[\"description\"].strip(), truncate=True)[0]\n                }\n            )\n        self.scene_ids = list(scene_ids.keys())\n\n    def __getitem__(self, index):\n        data_dict = super().__getitem__(index)  # get scene data from parent class\n        # prepare language data\n        scene_id = self.scene_ids[self.chunk_scene_indices[index]]\n        language_data_indices = self.chunk_lang_indices[index]\n        language_data_in_scene = self.language_data[scene_id]\n        num_language_data_in_scene = len(language_data_in_scene)\n\n        data_dict[\"ann_id\"] = np.empty(shape=self.data_cfg.chunk_size, dtype=np.uint8)\n        data_dict[\"object_id\"] = np.empty(shape=self.data_cfg.chunk_size, dtype=np.int16)\n\n        data_dict[\"gt_target_obj_id_mask\"] = np.zeros(\n            shape=(self.data_cfg.chunk_size, data_dict[\"gt_aabb_obj_ids\"].shape[0]), dtype=bool\n        )\n        data_dict[\"clip_tokens\"] = torch.empty(size=(self.data_cfg.chunk_size, 77), dtype=torch.int32)\n        data_dict[\"eval_type\"] = []\n\n        for i, index in enumerate(language_data_indices):\n            real_idx = index % num_language_data_in_scene  # pad the last chunk\n            data = language_data_in_scene[real_idx]\n            data_dict[\"ann_id\"][i] = data[\"ann_id\"]\n            data_dict[\"object_id\"][i] = data[\"object_id\"]\n            data_dict[\"gt_target_obj_id_mask\"][i] = np.in1d(data_dict[\"gt_aabb_obj_ids\"], data[\"object_id\"])\n            data_dict[\"clip_tokens\"][i] = data[\"clip_tokens\"]\n            data_dict[\"eval_type\"].append(data[\"eval_type\"])\n        return data_dict", ""]}
{"filename": "m3drefclip/model/m3dref_clip.py", "chunked_list": ["import os\nimport json\nimport clip\nimport torch\nimport hydra\nimport numpy as np\nfrom tqdm import tqdm\nimport lightning.pytorch as pl\nfrom m3drefclip.common_ops.functions import common_ops\nfrom m3drefclip.model.vision_module.pointgroup import PointGroup", "from m3drefclip.common_ops.functions import common_ops\nfrom m3drefclip.model.vision_module.pointgroup import PointGroup\nfrom m3drefclip.model.cross_modal_module.match_module import MatchModule\nfrom m3drefclip.model.vision_module.object_renderer import ObjectRenderer\nfrom m3drefclip.model.vision_module.clip_image_encoder import CLIPImageEncoder\n\n\nclass M3DRefCLIP(pl.LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.save_hyperparameters()\n        self.dataset_name = cfg.data.lang_dataset\n\n        # vision modules\n        input_channel = 3 + 3 * cfg.data.point_features.use_rgb + 3 * cfg.data.point_features.use_normal + \\\n                        128 * cfg.data.point_features.use_multiview\n        self.detector = PointGroup(\n            input_channel=input_channel, output_channel=cfg.model.network.detector.output_channel,\n            max_proposals=cfg.model.network.max_num_proposals, semantic_class=cfg.data.semantic_class,\n            use_gt=cfg.model.network.detector.use_gt_proposal\n        )\n\n        # loss\n        if self.dataset_name in (\"ScanRefer\", \"Nr3D\"):\n            self.ref_loss = hydra.utils.instantiate(\n                cfg.model.loss.reference_ce_loss, chunk_size=cfg.data.chunk_size,\n                max_num_proposals=cfg.model.network.max_num_proposals\n            )\n        else:\n            raise NotImplementedError\n\n        self.clip_model = clip.load(cfg.model.network.clip_model, device=self.device)[0]\n\n        # freeze CLIP\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n\n        if self.hparams.cfg.model.network.use_2d_feature:\n            self.object_renderer = ObjectRenderer(**cfg.model.network.object_renderer)\n            self.clip_image = CLIPImageEncoder(clip_model=self.clip_model, **cfg.model.network.clip_img_encoder)\n\n        self.text_encoder = hydra.utils.instantiate(cfg.model.network.clip_word_encoder, clip_model=self.clip_model)\n\n        self.match_module = MatchModule(\n            **cfg.model.network.matching_module,\n            input_channel=cfg.model.network.detector.output_channel *\n                          self.hparams.cfg.model.network.use_3d_features +\n                          self.hparams.cfg.model.network.use_2d_feature *\n                          self.hparams.cfg.model.network.clip_img_encoder.output_channel\n        )\n\n        self.contrastive_loss = hydra.utils.instantiate(cfg.model.loss.contrastive_loss)\n\n        # evaluator\n        self.evaluator = hydra.utils.instantiate(cfg.data.evaluator)\n        self.val_test_step_outputs = []\n\n    def forward(self, data_dict):\n        output_dict = self.detector(data_dict)\n        batch_size = len(data_dict[\"scene_id\"])\n        if self.hparams.cfg.model.network.use_3d_features:\n            aabb_features = output_dict[\"aabb_features\"]\n        else:\n            aabb_features = torch.empty(\n                size=(output_dict[\"aabb_features\"].shape[0], 0),\n                dtype=output_dict[\"aabb_features\"].dtype, device=self.device\n            )\n        data_dict[\"lang_attention_mask\"] = None\n        if self.hparams.cfg.model.network.use_2d_feature:\n            rendered_imgs = self.object_renderer(data_dict, output_dict)\n            img_features = self.clip_image(rendered_imgs.permute(dims=(0, 3, 1, 2)))\n            views = len(self.hparams.cfg.model.network.object_renderer.eye)\n            aabb_img_features = torch.nn.functional.avg_pool1d(\n                img_features.permute(1, 0), kernel_size=views, stride=views\n            ).permute(1, 0)\n            # TODO: adjust mask\n            # data_dict[\"lang_attention_mask\"] = data_dict[\"lang_attention_mask\"][:, :77]  # CLIP context length\n            # concatenate 2D and 3D features\n            aabb_features = torch.nn.functional.normalize(torch.cat((aabb_features, aabb_img_features), dim=1), dim=1)\n\n        output_dict[\"aabb_features\"] = common_ops.convert_sparse_tensor_to_dense(\n            aabb_features, output_dict[\"proposal_batch_offsets\"],\n            self.hparams.cfg.model.network.max_num_proposals\n        )\n\n        output_dict[\"pred_aabb_min_max_bounds\"] = common_ops.convert_sparse_tensor_to_dense(\n            output_dict[\"pred_aabb_min_max_bounds\"].reshape(-1, 6), output_dict[\"proposal_batch_offsets\"],\n            self.hparams.cfg.model.network.max_num_proposals\n        ).reshape(batch_size, self.hparams.cfg.model.network.max_num_proposals, 2, 3)\n\n        self.text_encoder(data_dict, output_dict)\n\n        \"\"\"\n        cross-modal fusion\n        \"\"\"\n        self.match_module(data_dict, output_dict)\n        return output_dict\n\n    def _loss(self, data_dict, output_dict):\n        loss_dict = self.detector.loss(data_dict, output_dict)\n\n        # reference loss\n        loss_dict[\"reference_loss\"] = self.ref_loss(\n            output_dict,\n            output_dict[\"pred_aabb_min_max_bounds\"],\n            output_dict[\"pred_aabb_scores\"],\n            data_dict[\"gt_aabb_min_max_bounds\"],\n            data_dict[\"gt_target_obj_id_mask\"].permute(dims=(1, 0)),\n            data_dict[\"aabb_count_offsets\"],\n        )\n        if self.hparams.cfg.model.network.use_contrastive_loss:\n            # contrastive loss\n            loss_dict[\"contrastive_loss\"] = self.contrastive_loss(\n                output_dict[\"aabb_features_inter\"],\n                output_dict[\"sentence_features\"],\n                output_dict[\"gt_labels\"]\n            )\n        return loss_dict\n\n    def configure_optimizers(self):\n        optimizer = hydra.utils.instantiate(self.hparams.cfg.model.optimizer, params=self.parameters())\n        return optimizer\n\n    def training_step(self, data_dict, idx):\n        output_dict = self(data_dict)\n        loss_dict = self._loss(data_dict, output_dict)\n\n        # calculate the total loss and log\n        total_loss = 0\n        for loss_name, loss_value in loss_dict.items():\n            total_loss += loss_value\n            self.log(f\"train_loss/{loss_name}\", loss_value, on_step=True, on_epoch=False)\n        self.log(f\"train_loss/total_loss\", total_loss, on_step=True, on_epoch=False)\n        return total_loss\n\n    def validation_step(self, data_dict, idx):\n        output_dict = self(data_dict)\n        loss_dict = self._loss(data_dict, output_dict)\n\n        # calculate the total loss and log\n        total_loss = 0\n        for loss_name, loss_value in loss_dict.items():\n            total_loss += loss_value\n            self.log(f\"val_loss/{loss_name}\", loss_value, on_step=True, on_epoch=False)\n        self.log(f\"val_loss/total_loss\", total_loss, on_step=True, on_epoch=False)\n\n        # get predictions and gts\n        self.val_test_step_outputs.append((self._parse_pred_results(data_dict, output_dict), self._parse_gt(data_dict)))\n\n    def test_step(self, data_dict, idx):\n        output_dict = self(data_dict)\n        self.val_test_step_outputs.append(\n            (self._parse_pred_results(data_dict, output_dict), self._parse_gt(data_dict))\n        )\n\n    def on_validation_epoch_end(self):\n        total_pred_results = {}\n        total_gt_results = {}\n        for pred_results, gt_results in self.val_test_step_outputs:\n            total_pred_results.update(pred_results)\n            total_gt_results.update(gt_results)\n        self.val_test_step_outputs.clear()\n        self.evaluator.set_ground_truths(total_gt_results)\n        results = self.evaluator.evaluate(total_pred_results)\n\n        # log\n        for metric_name, result in results.items():\n            for breakdown, value in result.items():\n                self.log(f\"val_eval/{metric_name}_{breakdown}\", value)\n\n    def on_test_epoch_end(self):\n        total_pred_results = {}\n        total_gt_results = {}\n        for pred_results, gt_results in self.val_test_step_outputs:\n            total_pred_results.update(pred_results)\n            total_gt_results.update(gt_results)\n        self.val_test_step_outputs.clear()\n        self._save_predictions(total_pred_results)\n\n    def _parse_pred_results(self, data_dict, output_dict):\n        batch_size, lang_chunk_size = data_dict[\"ann_id\"].shape\n        if self.dataset_name in (\"ScanRefer\", \"Nr3D\"):\n            pred_aabb_score_masks = (output_dict[\"pred_aabb_scores\"].argmax(dim=1)).reshape(\n                shape=(batch_size, lang_chunk_size, -1)\n            )\n        else:\n            raise NotImplementedError\n\n        pred_results = {}\n        for i in range(batch_size):\n            for j in range(lang_chunk_size):\n                pred_aabbs = output_dict[\"pred_aabb_min_max_bounds\"][i][pred_aabb_score_masks[i, j]]\n                pred_results[\n                    (data_dict[\"scene_id\"][i], data_dict[\"object_id\"][i][j].item(),\n                     data_dict[\"ann_id\"][i][j].item())\n                ] = {\n                    \"aabb_bound\": (pred_aabbs + data_dict[\"scene_center_xyz\"][i]).cpu().numpy()\n                }\n        return pred_results\n\n    def _parse_gt(self, data_dict):\n        batch_size, lang_chunk_size = data_dict[\"ann_id\"].shape\n        gts = {}\n        gt_target_obj_id_masks = data_dict[\"gt_target_obj_id_mask\"].permute(1, 0)\n        for i in range(batch_size):\n            aabb_start_idx = data_dict[\"aabb_count_offsets\"][i]\n            aabb_end_idx = data_dict[\"aabb_count_offsets\"][i + 1]\n            for j in range(lang_chunk_size):\n                gts[\n                    (data_dict[\"scene_id\"][i], data_dict[\"object_id\"][i][j].item(),\n                     data_dict[\"ann_id\"][i][j].item())\n                ] = {\n                    \"aabb_bound\":\n                        (data_dict[\"gt_aabb_min_max_bounds\"][aabb_start_idx:aabb_end_idx][gt_target_obj_id_masks[j]\n                    [aabb_start_idx:aabb_end_idx]] + data_dict[\"scene_center_xyz\"][i]).cpu().numpy(),\n                    \"eval_type\": data_dict[\"eval_type\"][i][j]\n                }\n        return gts\n\n    def _save_predictions(self, predictions):\n        scene_pred = {}\n        for key, value in predictions.items():\n            scene_id = key[0]\n            if key[0] not in scene_pred:\n                scene_pred[scene_id] = []\n            min_point = value[\"aabb_bound\"][:, 0]\n            max_point = value[\"aabb_bound\"][:, 1]\n            corners = np.array(np.meshgrid(\n                np.linspace(min_point[:, 0], max_point[:, 0], 2),\n                np.linspace(min_point[:, 1], max_point[:, 1], 2),\n                np.linspace(min_point[:, 2], max_point[:, 2], 2)\n            )).T.reshape(-1, 8, 3)\n            scene_pred[scene_id].append({\n                \"object_id\": key[1],\n                \"ann_id\": key[2],\n                \"aabb\": corners.tolist()\n            })\n        prediction_output_root_path = os.path.join(\n            self.hparams.cfg.pred_path, self.hparams.cfg.data.inference.split\n        )\n        os.makedirs(prediction_output_root_path, exist_ok=True)\n        for scene_id in tqdm(scene_pred.keys(), desc=\"Saving predictions\"):\n            with open(os.path.join(prediction_output_root_path, f\"{scene_id}.json\"), \"w\") as f:\n                json.dump(scene_pred[scene_id], f, indent=2)\n        self.print(f\"==> Complete. Saved at: {os.path.abspath(prediction_output_root_path)}\")", ""]}
{"filename": "m3drefclip/model/language_module/clip_word_encoder.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport lightning.pytorch as pl\n\n\nclass CLIPWordEncoder(pl.LightningModule):\n    def __init__(self, clip_model, output_channel, dropout):\n        super().__init__()\n        self.clip_model = clip_model\n        self.mlp = nn.Sequential(\n            nn.Linear(self.clip_model.visual.output_dim, output_channel),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(output_channel, output_channel),\n        )\n        self.text_projection = nn.Parameter(\n            torch.empty(\n                size=(self.clip_model.visual.output_dim, output_channel),\n                device=self.device,\n                dtype=torch.float32\n            )\n        )\n        self._weight_initialization(output_channel)\n\n    def _weight_initialization(self, output_channel):\n        nn.init.normal_(self.text_projection, std=output_channel**-0.5)\n\n    def forward(self, data_dict, output_dict):\n        clip_tokens = data_dict[\"clip_tokens\"].flatten(start_dim=0, end_dim=1)\n        word_features, sentence_features = self.clip_model.encode_text(clip_tokens)\n        word_features = nn.functional.normalize(word_features, dim=2)\n        output_dict[\"word_features\"] = self.mlp(word_features)\n        output_dict[\"sentence_features\"] = sentence_features @ self.text_projection", ""]}
{"filename": "m3drefclip/model/module/tiny_unet.py", "chunked_list": ["import torch.nn as nn\nimport lightning.pytorch as pl\nimport MinkowskiEngine as ME\nfrom m3drefclip.model.module.common import ResidualBlock, UBlock\n\n\nclass TinyUnet(pl.LightningModule):\n    def __init__(self, channel):\n        super().__init__()\n\n        # 1. U-Net\n        self.unet = nn.Sequential(\n            UBlock([channel, 2 * channel], ME.MinkowskiBatchNorm, 2, ResidualBlock),\n            ME.MinkowskiBatchNorm(channel),\n            ME.MinkowskiReLU(inplace=True)\n        )\n\n    def forward(self, proposals_voxel_feats):\n        return self.unet(proposals_voxel_feats)", ""]}
{"filename": "m3drefclip/model/module/backbone.py", "chunked_list": ["import torch.nn as nn\nimport lightning.pytorch as pl\nimport MinkowskiEngine as ME\nfrom m3drefclip.model.module.common import ResidualBlock, UBlock\n\n\nclass Backbone(pl.LightningModule):\n    def __init__(self, input_channel, output_channel, block_channels, block_reps, sem_classes):\n        super().__init__()\n\n        # 1. U-Net\n        self.unet = nn.Sequential(\n            ME.MinkowskiConvolution(in_channels=input_channel, out_channels=output_channel, kernel_size=3, dimension=3),\n            UBlock([output_channel * c for c in block_channels], ME.MinkowskiBatchNorm, block_reps, ResidualBlock),\n            ME.MinkowskiBatchNorm(output_channel),\n            ME.MinkowskiReLU(inplace=True)\n        )\n\n        # 2.1 semantic prediction branch\n        self.semantic_branch = nn.Sequential(\n            nn.Linear(output_channel, output_channel),\n            nn.BatchNorm1d(output_channel),\n            nn.ReLU(inplace=True),\n            nn.Linear(output_channel, sem_classes)\n        )\n\n        # 2.2 offset prediction branch\n        self.offset_branch = nn.Sequential(\n            nn.Linear(output_channel, output_channel),\n            nn.BatchNorm1d(output_channel),\n            nn.ReLU(inplace=True),\n            nn.Linear(output_channel, 3)\n        )\n\n    def forward(self, voxel_features, voxel_coordinates, v2p_map):\n        x = ME.SparseTensor(features=voxel_features, coordinates=voxel_coordinates, device=self.device)\n        unet_out = self.unet(x)\n        point_features = unet_out.features[v2p_map]\n        semantic_scores = self.semantic_branch(point_features)\n        point_offsets = self.offset_branch(point_features)\n        return point_features, semantic_scores, point_offsets", ""]}
{"filename": "m3drefclip/model/module/__init__.py", "chunked_list": ["from .tiny_unet import TinyUnet\n"]}
{"filename": "m3drefclip/model/module/common.py", "chunked_list": ["import torch.nn as nn\nimport MinkowskiEngine as ME\nfrom collections import OrderedDict\nimport lightning.pytorch as pl\n\n\nclass ResidualBlock(pl.LightningModule):\n\n    def __init__(self, in_channels, out_channels, dimension, norm_fn=None):\n        super().__init__()\n        self.downsample = None\n        if norm_fn is None:\n            norm_fn = ME.MinkowskiBatchNorm\n        if in_channels != out_channels:\n            self.downsample = nn.Sequential(\n                ME.MinkowskiConvolution(in_channels, out_channels, kernel_size=1, dimension=dimension)\n            )\n\n        self.conv_branch = nn.Sequential(\n            norm_fn(in_channels),\n            ME.MinkowskiReLU(inplace=True),\n            ME.MinkowskiConvolution(in_channels, out_channels, kernel_size=3, dimension=dimension),\n            norm_fn(out_channels),\n            ME.MinkowskiReLU(inplace=True),\n            ME.MinkowskiConvolution(out_channels, out_channels, kernel_size=3, dimension=dimension)\n        )\n\n    def forward(self, x):\n        identity = x\n        x = self.conv_branch(x)\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n        x += identity\n        return x", "\n\nclass UBlock(pl.LightningModule):\n\n    def __init__(self, n_planes, norm_fn, block_reps, block):\n        super().__init__()\n        self.nPlanes = n_planes\n        self.D = 3\n        blocks = {'block{}'.format(i): block(n_planes[0], n_planes[0], self.D, norm_fn) for i in range(block_reps)}\n        blocks = OrderedDict(blocks)\n        self.blocks = nn.Sequential(blocks)\n\n        if len(n_planes) > 1:\n            self.conv = nn.Sequential(\n                norm_fn(n_planes[0]),\n                ME.MinkowskiReLU(inplace=True),\n                ME.MinkowskiConvolution(n_planes[0], n_planes[1], kernel_size=2, stride=2, dimension=self.D)\n            )\n            self.u = UBlock(n_planes[1:], norm_fn, block_reps, block)\n            self.deconv = nn.Sequential(\n                norm_fn(n_planes[1]),\n                ME.MinkowskiReLU(inplace=True),\n                ME.MinkowskiConvolutionTranspose(n_planes[1], n_planes[0], kernel_size=2, stride=2, dimension=self.D)\n            )\n            blocks_tail = {'block{}'.format(i): block(n_planes[0] * (2 - i), n_planes[0], self.D, norm_fn) for i in\n                           range(block_reps)}\n            blocks_tail = OrderedDict(blocks_tail)\n            self.blocks_tail = nn.Sequential(blocks_tail)\n\n    def forward(self, x):\n        out = self.blocks(x)\n        identity = out\n        if len(self.nPlanes) > 1:\n            out = self.conv(out)\n            out = self.u(out)\n            out = self.deconv(out)\n            out = ME.cat(identity, out)\n            out = self.blocks_tail(out)\n        return out", "\n"]}
{"filename": "m3drefclip/model/cross_modal_module/match_module.py", "chunked_list": ["import lightning.pytorch as pl\nimport torch\nimport torch.nn as nn\nfrom m3drefclip.model.cross_modal_module.attention import MultiHeadAttention\n\n\nclass MatchModule(pl.LightningModule):\n    def __init__(self, feat_channel, input_channel, head, depth):\n        super().__init__()\n        self.depth = depth - 1\n        self.features_concat = nn.Sequential(\n            nn.Conv1d(input_channel, feat_channel, 1),\n            nn.BatchNorm1d(feat_channel),\n            nn.PReLU(feat_channel),\n            nn.Conv1d(feat_channel, feat_channel, 1),\n        )\n        self.self_attn = nn.ModuleList(\n            MultiHeadAttention(\n                d_model=feat_channel,\n                h=head,\n                d_k=feat_channel // head,\n                d_v=feat_channel // head,\n                dropout=0.1\n            ) for _ in range(depth)\n        )\n        self.cross_attn = nn.ModuleList(\n            MultiHeadAttention(\n                d_model=feat_channel,\n                h=head,\n                d_k=feat_channel // head,\n                d_v=feat_channel // head,\n                dropout=0.1\n            ) for _ in range(depth)\n        )\n        self.match = nn.Sequential(\n            nn.Conv1d(feat_channel, feat_channel, 1),\n            nn.BatchNorm1d(feat_channel),\n            nn.PReLU(),\n            nn.Conv1d(feat_channel, feat_channel, 1),\n            nn.BatchNorm1d(feat_channel),\n            nn.PReLU(),\n            nn.Conv1d(feat_channel, 1, 1)\n        )\n\n    def forward(self, data_dict, output_dict):\n        batch_size, chunk_size = data_dict[\"ann_id\"].shape[0:2]\n        num_proposals = output_dict[\"pred_aabb_min_max_bounds\"].shape[1]\n        # attention weight\n        attention_weights = self._calculate_spatial_weight(output_dict)\n        aabb_features = self.features_concat(output_dict['aabb_features'].permute(0, 2, 1)).permute(0, 2, 1)\n        output_dict[\"aabb_features_inter\"] = aabb_features\n        aabb_features = self.self_attn[0](\n            aabb_features, aabb_features, aabb_features, attention_weights=attention_weights, way=\"add\"\n        )\n        aabb_features = aabb_features.unsqueeze(dim=1).expand(-1, chunk_size, -1, -1).flatten(start_dim=0, end_dim=1)\n        attention_weights = attention_weights.unsqueeze(dim=1).expand(-1, chunk_size, -1, -1, -1).reshape(\n            batch_size * chunk_size, attention_weights.shape[1], num_proposals, num_proposals\n        )\n        aabb_features = self.cross_attn[0](\n            aabb_features, output_dict[\"word_features\"], output_dict[\"word_features\"],\n            attention_mask=data_dict[\"lang_attention_mask\"]\n        )\n        for i in range(1, self.depth + 1):\n            aabb_features = self.self_attn[i](\n                aabb_features, aabb_features, aabb_features, attention_weights=attention_weights, way=\"add\"\n            )\n            aabb_features = self.cross_attn[i](\n                aabb_features, output_dict[\"word_features\"], output_dict[\"word_features\"],\n                attention_mask=data_dict[\"lang_attention_mask\"]\n            )\n        # match\n        aabb_features = aabb_features.permute(0, 2, 1).contiguous()\n        output_dict[\"pred_aabb_scores\"] = self.match(aabb_features).flatten(start_dim=0, end_dim=1)\n\n    def _calculate_spatial_weight(self, output_dict):\n        \"\"\"\n        Reference: https://github.com/zlccccc/3DVG-Transformer\n        \"\"\"\n        objects_center = output_dict[\"pred_aabb_min_max_bounds\"].mean(dim=2)\n        num_proposals = objects_center.shape[1]\n        center_a = objects_center.unsqueeze(dim=1).repeat(1, num_proposals, 1, 1)\n        center_b = objects_center.unsqueeze(dim=2).repeat(1, 1, num_proposals, 1)\n        dist = (center_a - center_b).pow(2)\n        dist = torch.sqrt(dist.sum(dim=-1)).unsqueeze(dim=1)\n        dist_weights = 1 / (dist + 1e-2)\n\n        # mask placeholders\n        tmp_unsqueezed = output_dict[\"proposal_masks_dense\"].unsqueeze(-1)\n        dist_weights *= (tmp_unsqueezed.transpose(1, 2) * tmp_unsqueezed).unsqueeze(dim=1)\n\n        dist_weights += torch.finfo(torch.float32).eps  # prevent zeros\n        norm = dist_weights.sum(dim=2, keepdim=True)\n        dist_weights = dist_weights / norm\n        zeros = torch.zeros_like(dist_weights)\n        dist_weights = torch.cat([dist_weights, -dist, zeros, zeros], dim=1).detach()\n        return dist_weights", ""]}
{"filename": "m3drefclip/model/cross_modal_module/attention.py", "chunked_list": ["import lightning.pytorch as pl\nfrom torch import nn\nimport torch\nimport math\n\n\nclass ScaledDotProductAttention(pl.LightningModule):\n    \"\"\"\n    Scaled dot-product attention\n    Reference: https://github.com/zlccccc/3DVG-Transformer\n    \"\"\"\n\n    def __init__(self, d_model, d_k, d_v, h):\n        \"\"\"\n        :param d_model: Output dimensionality of the model\n        :param d_k: Dimensionality of queries and keys\n        :param d_v: Dimensionality of values\n        :param h: Number of heads\n        \"\"\"\n        super().__init__()\n        self.fc_q = nn.Linear(d_model, h * d_k)\n        self.fc_k = nn.Linear(d_model, h * d_k)\n        self.fc_v = nn.Linear(d_model, h * d_v)\n        self.fc_o = nn.Linear(h * d_v, d_model)\n        self.d_k = d_k\n        self.d_v = d_v\n        self.h = h\n\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.xavier_uniform_(self.fc_q.weight)\n        nn.init.xavier_uniform_(self.fc_k.weight)\n        nn.init.xavier_uniform_(self.fc_v.weight)\n        nn.init.xavier_uniform_(self.fc_o.weight)\n        nn.init.constant_(self.fc_q.bias, 0)\n        nn.init.constant_(self.fc_k.bias, 0)\n        nn.init.constant_(self.fc_v.bias, 0)\n        nn.init.constant_(self.fc_o.bias, 0)\n\n    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None, way='add'):\n        \"\"\"\n        Computes\n        :param queries: Queries (b_s, nq, d_model)\n        :param keys: Keys (b_s, nk, d_model)\n        :param values: Values (b_s, nk, d_model)\n        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n        :return:\n        \"\"\"\n        batch_size, nq = queries.shape[:2]\n        nk = keys.shape[1]\n        q = self.fc_q(queries)\n        q = q.view(batch_size, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n        k = self.fc_k(keys).view(batch_size, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n        v = self.fc_v(values).view(batch_size, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n\n        att = torch.matmul(q, k) / math.sqrt(self.d_k)  # (b_s, h, nq, nk)\n        if attention_weights is not None:\n            if way == 'mul':\n                att = att * attention_weights\n            elif way == 'add':\n                att = att + attention_weights\n            else:\n                raise NotImplementedError(way)\n        if attention_mask is not None:\n            att = att.masked_fill(attention_mask, -torch.inf)\n        att = torch.softmax(att, dim=-1)\n        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(batch_size, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n        out = self.fc_o(out)  # (b_s, nq, d_model)\n        return out", "\n\nclass MultiHeadAttention(pl.LightningModule):\n    \"\"\"\n    Reference: https://github.com/zlccccc/3DVG-Transformer\n    \"\"\"\n\n    def __init__(self, d_model, d_k, d_v, h, dropout):\n        super().__init__()\n        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None, way='add'):\n        out = self.attention(queries, keys, values, attention_mask, attention_weights, way)\n        out = self.dropout(out)\n        out = self.layer_norm(queries + out)\n        return out", ""]}
{"filename": "m3drefclip/model/vision_module/pointgroup.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom m3drefclip.common_ops.functions import pointgroup_ops, common_ops\nfrom m3drefclip.model.module import TinyUnet\nfrom m3drefclip.model.module.backbone import Backbone\nimport lightning.pytorch as pl\nimport MinkowskiEngine as ME\nfrom m3drefclip.loss.pt_offset_loss import PTOffsetLoss\n\n\nclass PointGroup(pl.LightningModule):\n    def __init__(self, input_channel, output_channel, max_proposals, semantic_class, use_gt):\n        super().__init__()\n\n        self.backbone = Backbone(\n            input_channel=input_channel, output_channel=output_channel, block_channels=[1, 2, 3, 4, 5, 6, 7],\n            block_reps=2, sem_classes=semantic_class\n        )\n\n        \"\"\"\n            ScoreNet Block\n        \"\"\"\n        self.score_net = TinyUnet(output_channel)\n        self.score_branch = nn.Linear(output_channel, 1)\n        self.output_channel = output_channel\n        self.max_proposals = max_proposals\n        self.use_gt = use_gt\n\n    def forward(self, data_dict):\n\n        batch_size = len(data_dict[\"scene_id\"])\n        output_dict = {}\n\n        point_features, output_dict[\"semantic_scores\"], output_dict[\"point_offsets\"] = self.backbone(\n            data_dict[\"voxel_features\"], data_dict[\"voxel_xyz\"], data_dict[\"voxel_point_map\"]\n        )\n\n        if not self.use_gt:\n            # get prooposal clusters\n            semantic_preds = output_dict[\"semantic_scores\"].argmax(1).to(torch.int16)\n\n            # set mask\n            semantic_preds_mask = torch.ones_like(semantic_preds, dtype=torch.bool)\n            for class_label in [0, 1]:\n                semantic_preds_mask = semantic_preds_mask & (semantic_preds != class_label)\n            object_idxs = torch.nonzero(semantic_preds_mask).view(-1)\n\n            batch_idxs_ = data_dict[\"vert_batch_ids\"][object_idxs]\n            batch_offsets_ = torch.cumsum(torch.bincount(batch_idxs_ + 1), dim=0, dtype=torch.int32)\n            coords_ = data_dict[\"point_xyz\"][object_idxs]\n            pt_offsets_ = output_dict[\"point_offsets\"][object_idxs]\n\n            semantic_preds_cpu = semantic_preds[object_idxs].cpu()\n\n            idx_shift, start_len_shift = common_ops.ballquery_batch_p(\n                coords_ + pt_offsets_, batch_idxs_, batch_offsets_, 0.03, 300\n            )\n            cluster_obj_idxs_shift, cluster_point_idxs_shift, proposals_offset_shift = pointgroup_ops.pg_bfs_cluster(\n                semantic_preds_cpu, idx_shift.cpu(), start_len_shift.cpu(), 50\n            )\n\n            cluster_obj_idxs_shift = cluster_obj_idxs_shift.to(self.device)\n            cluster_point_idxs_shift = cluster_point_idxs_shift.to(self.device)\n            proposals_offset_shift = proposals_offset_shift.to(self.device)\n\n            cluster_point_idxs_shift = object_idxs[cluster_point_idxs_shift]\n\n            proposals_batch_id_shift_all = data_dict[\"vert_batch_ids\"][cluster_point_idxs_shift]\n\n            idx, start_len = common_ops.ballquery_batch_p(coords_, batch_idxs_, batch_offsets_, 0.03, 50)\n\n            cluster_obj_idxs, cluster_point_idxs, proposals_offset = pointgroup_ops.pg_bfs_cluster(\n                semantic_preds_cpu, idx.cpu(), start_len.cpu(), 50\n            )\n\n            cluster_obj_idxs = cluster_obj_idxs.to(self.device)\n            cluster_point_idxs = cluster_point_idxs.to(self.device)\n\n            proposals_offset = proposals_offset.to(self.device)\n            cluster_point_idxs = object_idxs[cluster_point_idxs]\n\n            proposals_batch_id_all_tmp = data_dict[\"vert_batch_ids\"][cluster_point_idxs]\n\n            cluster_obj_idxs_shift += (proposals_offset.size(0) - 1)\n            proposals_offset_shift += proposals_offset[-1]\n            # proposals_idx = torch.cat((proposals_idx, proposals_idx_shift), dim=0)\n\n            cluster_obj_idxs = torch.cat((cluster_obj_idxs, cluster_obj_idxs_shift), dim=0)\n            cluster_point_idxs = torch.cat((cluster_point_idxs, cluster_point_idxs_shift), dim=0)\n\n            proposals_offset = torch.cat((proposals_offset, proposals_offset_shift[1:]))\n\n            proposals_batch_id_all = torch.cat((proposals_batch_id_all_tmp, proposals_batch_id_shift_all[1:]))\n        else:\n            unique_obj_ids = torch.unique(data_dict[\"instance_ids\"])\n            unique_obj_ids = unique_obj_ids[unique_obj_ids != -1]\n            proposal_point_idx_list = []\n            proposal_idx_list = []\n            proposals_offset = torch.empty(size=(len(unique_obj_ids) + 1, ), dtype=torch.int32, device=self.device)\n            proposals_offset[0] = 0\n            for i, inst_id in enumerate(unique_obj_ids):\n                point_idx = torch.where(data_dict[\"instance_ids\"] == inst_id)[0]\n                proposal_point_idx_list.append(point_idx)\n                proposal_idx_list.append(\n                    torch.full(size=(point_idx.shape[0], ), fill_value=i, device=self.device, dtype=torch.int32)\n                )\n                proposals_offset[i + 1] = proposals_offset[i] + point_idx.shape[0]\n\n            cluster_obj_idxs = torch.cat(proposal_idx_list)\n            cluster_point_idxs = torch.cat(proposal_point_idx_list)\n            # proposals_idx = torch.hstack(\n            #     (torch.cat(proposal_idx_list).unsqueeze(-1), torch.cat(proposal_point_idx_list).unsqueeze(-1))\n            # )\n            proposals_batch_id_all = data_dict[\"vert_batch_ids\"][cluster_point_idxs]\n\n        # proposals voxelization again\n        proposals_voxel_feats, proposals_p2v_map, aabb_min_max_bound = clusters_voxelization(\n            cluster_obj_idxs=cluster_obj_idxs,\n            cluster_point_idxs=cluster_point_idxs,\n            clusters_offset=proposals_offset,\n            feats=point_features,\n            coords=data_dict[\"point_xyz\"],\n            scale=50,\n            spatial_shape=14,\n            device=self.device\n        )\n\n        # score\n        score_feats = self.score_net(proposals_voxel_feats)\n        pt_score_feats = score_feats.features[proposals_p2v_map]  # (sumNPoint, C)\n        proposals_score_feats = common_ops.roipool(pt_score_feats, proposals_offset)  # (nProposal, C)\n\n        if not self.use_gt:\n            proposals_scores = self.score_branch(proposals_score_feats).view(-1)\n        else:\n            proposals_scores = torch.ones(proposals_score_feats.shape[0], dtype=torch.float32, device=self.device)\n        proposals_batch_id = proposals_batch_id_all[proposals_offset[:-1].long()]\n        output_dict[\"proposal_scores\"] = (proposals_scores, cluster_point_idxs, proposals_offset)\n        max_num_proposal = self.max_proposals\n\n        total_proposals = 0\n        proposals_batch_offset = torch.zeros(size=(batch_size + 1,), dtype=torch.int16, device=self.device)\n\n        proposal_batch_idx_list = []\n        for b in range(batch_size):\n            proposal_batch_idx = torch.nonzero(proposals_batch_id == b).squeeze(-1)\n            proposal_batch_idx_list.append(proposal_batch_idx)\n            pred_num = len(proposal_batch_idx) if len(proposal_batch_idx) < max_num_proposal else max_num_proposal\n            total_proposals += pred_num\n            proposals_batch_offset[b + 1] = total_proposals\n\n        proposal_features = torch.zeros(\n            size=(total_proposals, self.output_channel), dtype=torch.float32, device=self.device\n        )\n\n        proposal_masks_dense = torch.zeros(\n            size=(batch_size, max_num_proposal), dtype=torch.bool, device=self.device\n        )\n\n        pred_aabb_min_max_bounds = torch.zeros(size=(total_proposals, 2, 3), dtype=torch.float32, device=self.device)\n\n        # convert to batch\n        for b in range(batch_size):\n            proposal_batch_idx = proposal_batch_idx_list[b]\n\n            start_idx = proposals_batch_offset[b]\n            end_idx = proposals_batch_offset[b + 1]\n\n            pred_num = end_idx - start_idx\n\n            rearrange_ids = torch.randperm(pred_num)\n\n            proposal_idx_sorted = proposal_batch_idx[torch.argsort(proposals_scores[proposal_batch_idx], descending=True)][0:pred_num]\n\n            proposal_features[start_idx:end_idx] = proposals_score_feats[proposal_idx_sorted][rearrange_ids]\n            pred_aabb_min_max_bounds[start_idx:end_idx] = aabb_min_max_bound[proposal_idx_sorted][rearrange_ids]\n\n            proposal_masks_dense[b, 0:pred_num] = True\n\n        output_dict[\"aabb_features\"] = proposal_features\n        output_dict[\"pred_aabb_min_max_bounds\"] = pred_aabb_min_max_bounds\n        output_dict[\"proposal_batch_offsets\"] = proposals_batch_offset\n        output_dict[\"proposal_masks_dense\"] = proposal_masks_dense\n        return output_dict\n\n    def loss(self, data_dict, output_dict):\n        losses = {}\n\n        # semantic loss\n        losses[\"semantic_loss\"] = nn.functional.cross_entropy(\n            output_dict[\"semantic_scores\"], data_dict[\"sem_labels\"], ignore_index=-1\n        )\n\n        if self.use_gt:\n            return losses\n\n        # offset loss\n        gt_offsets = data_dict[\"instance_centers\"] - data_dict[\"point_xyz\"]\n        valid = data_dict[\"instance_ids\"] != -1\n        pt_offset_criterion = PTOffsetLoss()\n        losses[\"offset_norm_loss\"], losses[\"offset_dir_loss\"] = pt_offset_criterion(\n            output_dict[\"point_offsets\"], gt_offsets, valid_mask=valid\n        )\n\n        # score loss\n        scores, cluster_point_idxs, proposals_offset = output_dict[\"proposal_scores\"]\n\n        ious = common_ops.get_iou(\n            cluster_point_idxs, proposals_offset,\n            data_dict[\"instance_ids\"], data_dict[\"instance_num_point\"]\n        )\n\n        gt_scores = get_segmented_scores(ious.max(1)[0], 0.75, 0.25)\n        losses[\"score_loss\"] = nn.functional.binary_cross_entropy_with_logits(scores.view(-1), gt_scores)\n        return losses", "\n\nclass PointGroup(pl.LightningModule):\n    def __init__(self, input_channel, output_channel, max_proposals, semantic_class, use_gt):\n        super().__init__()\n\n        self.backbone = Backbone(\n            input_channel=input_channel, output_channel=output_channel, block_channels=[1, 2, 3, 4, 5, 6, 7],\n            block_reps=2, sem_classes=semantic_class\n        )\n\n        \"\"\"\n            ScoreNet Block\n        \"\"\"\n        self.score_net = TinyUnet(output_channel)\n        self.score_branch = nn.Linear(output_channel, 1)\n        self.output_channel = output_channel\n        self.max_proposals = max_proposals\n        self.use_gt = use_gt\n\n    def forward(self, data_dict):\n\n        batch_size = len(data_dict[\"scene_id\"])\n        output_dict = {}\n\n        point_features, output_dict[\"semantic_scores\"], output_dict[\"point_offsets\"] = self.backbone(\n            data_dict[\"voxel_features\"], data_dict[\"voxel_xyz\"], data_dict[\"voxel_point_map\"]\n        )\n\n        if not self.use_gt:\n            # get prooposal clusters\n            semantic_preds = output_dict[\"semantic_scores\"].argmax(1).to(torch.int16)\n\n            # set mask\n            semantic_preds_mask = torch.ones_like(semantic_preds, dtype=torch.bool)\n            for class_label in [0, 1]:\n                semantic_preds_mask = semantic_preds_mask & (semantic_preds != class_label)\n            object_idxs = torch.nonzero(semantic_preds_mask).view(-1)\n\n            batch_idxs_ = data_dict[\"vert_batch_ids\"][object_idxs]\n            batch_offsets_ = torch.cumsum(torch.bincount(batch_idxs_ + 1), dim=0, dtype=torch.int32)\n            coords_ = data_dict[\"point_xyz\"][object_idxs]\n            pt_offsets_ = output_dict[\"point_offsets\"][object_idxs]\n\n            semantic_preds_cpu = semantic_preds[object_idxs].cpu()\n\n            idx_shift, start_len_shift = common_ops.ballquery_batch_p(\n                coords_ + pt_offsets_, batch_idxs_, batch_offsets_, 0.03, 300\n            )\n            cluster_obj_idxs_shift, cluster_point_idxs_shift, proposals_offset_shift = pointgroup_ops.pg_bfs_cluster(\n                semantic_preds_cpu, idx_shift.cpu(), start_len_shift.cpu(), 50\n            )\n\n            cluster_obj_idxs_shift = cluster_obj_idxs_shift.to(self.device)\n            cluster_point_idxs_shift = cluster_point_idxs_shift.to(self.device)\n            proposals_offset_shift = proposals_offset_shift.to(self.device)\n\n            cluster_point_idxs_shift = object_idxs[cluster_point_idxs_shift]\n\n            proposals_batch_id_shift_all = data_dict[\"vert_batch_ids\"][cluster_point_idxs_shift]\n\n            idx, start_len = common_ops.ballquery_batch_p(coords_, batch_idxs_, batch_offsets_, 0.03, 50)\n\n            cluster_obj_idxs, cluster_point_idxs, proposals_offset = pointgroup_ops.pg_bfs_cluster(\n                semantic_preds_cpu, idx.cpu(), start_len.cpu(), 50\n            )\n\n            cluster_obj_idxs = cluster_obj_idxs.to(self.device)\n            cluster_point_idxs = cluster_point_idxs.to(self.device)\n\n            proposals_offset = proposals_offset.to(self.device)\n            cluster_point_idxs = object_idxs[cluster_point_idxs]\n\n            proposals_batch_id_all_tmp = data_dict[\"vert_batch_ids\"][cluster_point_idxs]\n\n            cluster_obj_idxs_shift += (proposals_offset.size(0) - 1)\n            proposals_offset_shift += proposals_offset[-1]\n            # proposals_idx = torch.cat((proposals_idx, proposals_idx_shift), dim=0)\n\n            cluster_obj_idxs = torch.cat((cluster_obj_idxs, cluster_obj_idxs_shift), dim=0)\n            cluster_point_idxs = torch.cat((cluster_point_idxs, cluster_point_idxs_shift), dim=0)\n\n            proposals_offset = torch.cat((proposals_offset, proposals_offset_shift[1:]))\n\n            proposals_batch_id_all = torch.cat((proposals_batch_id_all_tmp, proposals_batch_id_shift_all[1:]))\n        else:\n            unique_obj_ids = torch.unique(data_dict[\"instance_ids\"])\n            unique_obj_ids = unique_obj_ids[unique_obj_ids != -1]\n            proposal_point_idx_list = []\n            proposal_idx_list = []\n            proposals_offset = torch.empty(size=(len(unique_obj_ids) + 1, ), dtype=torch.int32, device=self.device)\n            proposals_offset[0] = 0\n            for i, inst_id in enumerate(unique_obj_ids):\n                point_idx = torch.where(data_dict[\"instance_ids\"] == inst_id)[0]\n                proposal_point_idx_list.append(point_idx)\n                proposal_idx_list.append(\n                    torch.full(size=(point_idx.shape[0], ), fill_value=i, device=self.device, dtype=torch.int32)\n                )\n                proposals_offset[i + 1] = proposals_offset[i] + point_idx.shape[0]\n\n            cluster_obj_idxs = torch.cat(proposal_idx_list)\n            cluster_point_idxs = torch.cat(proposal_point_idx_list)\n            # proposals_idx = torch.hstack(\n            #     (torch.cat(proposal_idx_list).unsqueeze(-1), torch.cat(proposal_point_idx_list).unsqueeze(-1))\n            # )\n            proposals_batch_id_all = data_dict[\"vert_batch_ids\"][cluster_point_idxs]\n\n        # proposals voxelization again\n        proposals_voxel_feats, proposals_p2v_map, aabb_min_max_bound = clusters_voxelization(\n            cluster_obj_idxs=cluster_obj_idxs,\n            cluster_point_idxs=cluster_point_idxs,\n            clusters_offset=proposals_offset,\n            feats=point_features,\n            coords=data_dict[\"point_xyz\"],\n            scale=50,\n            spatial_shape=14,\n            device=self.device\n        )\n\n        # score\n        score_feats = self.score_net(proposals_voxel_feats)\n        pt_score_feats = score_feats.features[proposals_p2v_map]  # (sumNPoint, C)\n        proposals_score_feats = common_ops.roipool(pt_score_feats, proposals_offset)  # (nProposal, C)\n\n        if not self.use_gt:\n            proposals_scores = self.score_branch(proposals_score_feats).view(-1)\n        else:\n            proposals_scores = torch.ones(proposals_score_feats.shape[0], dtype=torch.float32, device=self.device)\n        proposals_batch_id = proposals_batch_id_all[proposals_offset[:-1].long()]\n        output_dict[\"proposal_scores\"] = (proposals_scores, cluster_point_idxs, proposals_offset)\n        max_num_proposal = self.max_proposals\n\n        total_proposals = 0\n        proposals_batch_offset = torch.zeros(size=(batch_size + 1,), dtype=torch.int16, device=self.device)\n\n        proposal_batch_idx_list = []\n        for b in range(batch_size):\n            proposal_batch_idx = torch.nonzero(proposals_batch_id == b).squeeze(-1)\n            proposal_batch_idx_list.append(proposal_batch_idx)\n            pred_num = len(proposal_batch_idx) if len(proposal_batch_idx) < max_num_proposal else max_num_proposal\n            total_proposals += pred_num\n            proposals_batch_offset[b + 1] = total_proposals\n\n        proposal_features = torch.zeros(\n            size=(total_proposals, self.output_channel), dtype=torch.float32, device=self.device\n        )\n\n        proposal_masks_dense = torch.zeros(\n            size=(batch_size, max_num_proposal), dtype=torch.bool, device=self.device\n        )\n\n        pred_aabb_min_max_bounds = torch.zeros(size=(total_proposals, 2, 3), dtype=torch.float32, device=self.device)\n\n        # convert to batch\n        for b in range(batch_size):\n            proposal_batch_idx = proposal_batch_idx_list[b]\n\n            start_idx = proposals_batch_offset[b]\n            end_idx = proposals_batch_offset[b + 1]\n\n            pred_num = end_idx - start_idx\n\n            rearrange_ids = torch.randperm(pred_num)\n\n            proposal_idx_sorted = proposal_batch_idx[torch.argsort(proposals_scores[proposal_batch_idx], descending=True)][0:pred_num]\n\n            proposal_features[start_idx:end_idx] = proposals_score_feats[proposal_idx_sorted][rearrange_ids]\n            pred_aabb_min_max_bounds[start_idx:end_idx] = aabb_min_max_bound[proposal_idx_sorted][rearrange_ids]\n\n            proposal_masks_dense[b, 0:pred_num] = True\n\n        output_dict[\"aabb_features\"] = proposal_features\n        output_dict[\"pred_aabb_min_max_bounds\"] = pred_aabb_min_max_bounds\n        output_dict[\"proposal_batch_offsets\"] = proposals_batch_offset\n        output_dict[\"proposal_masks_dense\"] = proposal_masks_dense\n        return output_dict\n\n    def loss(self, data_dict, output_dict):\n        losses = {}\n\n        # semantic loss\n        losses[\"semantic_loss\"] = nn.functional.cross_entropy(\n            output_dict[\"semantic_scores\"], data_dict[\"sem_labels\"], ignore_index=-1\n        )\n\n        if self.use_gt:\n            return losses\n\n        # offset loss\n        gt_offsets = data_dict[\"instance_centers\"] - data_dict[\"point_xyz\"]\n        valid = data_dict[\"instance_ids\"] != -1\n        pt_offset_criterion = PTOffsetLoss()\n        losses[\"offset_norm_loss\"], losses[\"offset_dir_loss\"] = pt_offset_criterion(\n            output_dict[\"point_offsets\"], gt_offsets, valid_mask=valid\n        )\n\n        # score loss\n        scores, cluster_point_idxs, proposals_offset = output_dict[\"proposal_scores\"]\n\n        ious = common_ops.get_iou(\n            cluster_point_idxs, proposals_offset,\n            data_dict[\"instance_ids\"], data_dict[\"instance_num_point\"]\n        )\n\n        gt_scores = get_segmented_scores(ious.max(1)[0], 0.75, 0.25)\n        losses[\"score_loss\"] = nn.functional.binary_cross_entropy_with_logits(scores.view(-1), gt_scores)\n        return losses", "\n\ndef get_segmented_scores(scores, fg_thresh=1.0, bg_thresh=0.0):\n    \"\"\"\n    Args:\n        scores: (N), float, 0~1\n\n    Returns:\n        segmented_scores: (N), float 0~1, >fg_thresh: 1, <bg_thresh: 0, mid: linear\n    \"\"\"\n    fg_mask = scores > fg_thresh\n    bg_mask = scores < bg_thresh\n    interval_mask = (fg_mask == 0) & (bg_mask == 0)\n\n    segmented_scores = (fg_mask > 0).float()\n    k = 1 / (fg_thresh - bg_thresh)\n    b = bg_thresh / (bg_thresh - fg_thresh)\n    segmented_scores[interval_mask] = scores[interval_mask] * k + b\n\n    return segmented_scores", "\n\ndef clusters_voxelization(cluster_obj_idxs, cluster_point_idxs, clusters_offset, feats, coords, scale, spatial_shape, device):\n    batch_idx = cluster_obj_idxs\n    c_idxs = cluster_point_idxs\n    feats = feats[c_idxs]\n    clusters_coords = coords[c_idxs]\n\n    clusters_coords_mean = common_ops.sec_mean(clusters_coords, clusters_offset)  # (nCluster, 3)\n    clusters_coords_mean_all = torch.index_select(clusters_coords_mean, 0, batch_idx)  # (sumNPoint, 3)\n    clusters_coords -= clusters_coords_mean_all\n\n    clusters_coords_min = common_ops.sec_min(clusters_coords, clusters_offset)\n    clusters_coords_max = common_ops.sec_max(clusters_coords, clusters_offset)\n\n    aabb_min_max_bound = torch.stack(\n        tensors=(clusters_coords_min + clusters_coords_mean, clusters_coords_max + clusters_coords_mean), dim=1\n    )\n\n    # 0.01 to ensure voxel_coords < spatial_shape\n    clusters_scale = 1 / ((clusters_coords_max - clusters_coords_min) / spatial_shape).max(1)[0] - 0.01\n    clusters_scale = torch.clamp(clusters_scale, min=None, max=scale)\n\n    min_xyz = clusters_coords_min * clusters_scale[:, None]\n    max_xyz = clusters_coords_max * clusters_scale[:, None]\n\n    clusters_scale = torch.index_select(clusters_scale, 0, batch_idx)\n\n    clusters_coords = clusters_coords * clusters_scale[:, None]\n\n    range = max_xyz - min_xyz\n    offset = -min_xyz + torch.clamp(spatial_shape - range - 0.001, min=0) * torch.rand(3, device=device)\n    offset += torch.clamp(spatial_shape - range + 0.001, max=0) * torch.rand(3, device=device)\n    offset = torch.index_select(offset, 0, batch_idx)\n    clusters_coords += offset\n\n    batched_xyz = torch.cat((cluster_obj_idxs.unsqueeze(-1), clusters_coords.int()), dim=1)\n\n    voxel_xyz, voxel_features, _, voxel_point_map = ME.utils.sparse_quantize(\n        batched_xyz, feats, return_index=True, return_inverse=True, device=device.type\n    )\n\n    clusters_voxel_feats = ME.SparseTensor(features=voxel_features, coordinates=voxel_xyz, device=device)\n    return clusters_voxel_feats, voxel_point_map, aabb_min_max_bound", ""]}
{"filename": "m3drefclip/model/vision_module/object_renderer.py", "chunked_list": ["import torch\nimport lightning.pytorch as pl\nfrom m3drefclip.common_ops.functions import common_ops\nfrom pytorch3d.structures import Pointclouds\nfrom pytorch3d.renderer import (\n    look_at_view_transform,\n    FoVOrthographicCameras,\n    PointsRasterizationSettings,\n    PointsRenderer,\n    PointsRasterizer,", "    PointsRenderer,\n    PointsRasterizer,\n    AlphaCompositor\n)\n\n\nclass ObjectRenderer(pl.LightningModule):\n    def __init__(self, eye, rasterizer_setting):\n        super().__init__()\n        self.R, self.T = look_at_view_transform(eye=eye, at=((0, 0, 0),), up=((0, 0, 1),), device=self.device)\n        self.image_size = rasterizer_setting.image_size\n        self.views = len(eye)\n        self.renderer = PointsRenderer(\n            rasterizer=PointsRasterizer(\n                cameras=None, raster_settings=PointsRasterizationSettings(**rasterizer_setting)\n            ), compositor=AlphaCompositor()\n        )\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def forward(self, data_dict, output_dict):\n        batch_size = len(data_dict[\"scene_id\"])\n        total_num_aabbs = output_dict[\"pred_aabb_min_max_bounds\"].shape[0]\n        output_imgs = torch.zeros(\n            size=(total_num_aabbs * self.views, self.image_size, self.image_size, 3),\n            dtype=torch.float32, device=self.device\n        )\n\n        for i in range(batch_size):\n            batch_points_start_idx = data_dict[\"all_point_count_offsets\"][i]\n            batch_points_end_idx = data_dict[\"all_point_count_offsets\"][i + 1]\n            current_pcd_xyz = data_dict[\"all_point_xyz\"][batch_points_start_idx:batch_points_end_idx]\n            current_pcd_rgb = data_dict[\"all_point_rgb\"][batch_points_start_idx:batch_points_end_idx]\n\n            pred_aabb_start_idx = output_dict[\"proposal_batch_offsets\"][i]\n            pred_aabb_end_idx = output_dict[\"proposal_batch_offsets\"][i + 1]\n\n            output_masks = common_ops.crop_pcd_from_aabbs(\n                output_dict[\"pred_aabb_min_max_bounds\"][pred_aabb_start_idx:pred_aabb_end_idx],\n                current_pcd_xyz\n            )\n            aabb_xyz_list = []\n            aabb_rgb_list = []\n            for obj_i in range(output_masks.shape[0]):\n                current_obj_point_indicies = output_masks[obj_i]\n                if not current_obj_point_indicies.any():\n                    obj_pcd_xyz = torch.empty(size=(0, 3), device=self.device, dtype=torch.float32)\n                    obj_pcd_rgb = torch.empty(size=(0, 3), device=self.device, dtype=torch.float32)\n                else:\n                    obj_pcd_xyz = current_pcd_xyz[current_obj_point_indicies]\n                    obj_pcd_rgb = current_pcd_rgb[current_obj_point_indicies]\n                    obj_pcd_xyz -= obj_pcd_xyz.mean(dim=0)\n                    obj_pcd_xyz /= obj_pcd_xyz.abs().max()\n                for _ in range(self.views):\n                    aabb_xyz_list.append(obj_pcd_xyz)\n                    aabb_rgb_list.append(obj_pcd_rgb)\n            pytorch3d_pcd = Pointclouds(points=aabb_xyz_list, features=aabb_rgb_list)\n            pytorch3d_pcd.device = self.device\n            num_aabbs = len(aabb_xyz_list) // self.views\n\n            R = self.R.expand(num_aabbs, -1, -1, -1).flatten(0, 1)\n            T = self.T.expand(num_aabbs, -1, -1).flatten(0, 1)\n\n            output_imgs[pred_aabb_start_idx * self.views:pred_aabb_end_idx * self.views] = self.renderer(\n                pytorch3d_pcd, dtype=torch.float32, device=self.device,\n                cameras=FoVOrthographicCameras(device=self.device, R=R, T=T, znear=0.01)\n            )\n\n            # pytorch3d_pcd = None\n            # aabb_xyz_list = None\n            # aabb_rgb_list = None\n            # torch.cuda.empty_cache()\n\n        return output_imgs", ""]}
{"filename": "m3drefclip/model/vision_module/clip_image_encoder.py", "chunked_list": ["import torch.nn as nn\nfrom torchvision.transforms import Normalize\nimport lightning.pytorch as pl\n\n\nclass CLIPImageEncoder(pl.LightningModule):\n    def __init__(self, clip_model, output_channel, dropout):\n        super().__init__()\n        self.clip_model = clip_model\n        self.mlp = nn.Sequential(\n            nn.Linear(self.clip_model.visual.output_dim, output_channel),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(output_channel, output_channel),\n        )\n\n    def forward(self, x):\n        output = self.clip_model.encode_image(\n            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))(x)\n        )\n        output = nn.functional.normalize(output, dim=1)\n        return self.mlp(output)", ""]}
{"filename": "m3drefclip/util/utils.py", "chunked_list": ["import torch\n\n\ndef get_batch_aabb_pair_ious(batch_boxes_1_bound, batch_boxes_2_bound):\n    box_1_x_min, box_1_y_min, box_1_z_min = torch.tensor_split(batch_boxes_1_bound[:, 0], 3, dim=1)\n    box_1_x_max, box_1_y_max, box_1_z_max = torch.tensor_split(batch_boxes_1_bound[:, 1], 3, dim=1)\n\n    box_2_x_min, box_2_y_min, box_2_z_min = torch.tensor_split(batch_boxes_2_bound[:, 0], 3, dim=1)\n    box_2_x_max, box_2_y_max, box_2_z_max = torch.tensor_split(batch_boxes_2_bound[:, 1], 3, dim=1)\n\n    x_a = torch.maximum(box_1_x_min, box_2_x_min)\n    y_a = torch.maximum(box_1_y_min, box_2_y_min)\n    z_a = torch.maximum(box_1_z_min, box_2_z_min)\n    x_b = torch.minimum(box_1_x_max, box_2_x_max)\n    y_b = torch.minimum(box_1_y_max, box_2_y_max)\n    z_b = torch.minimum(box_1_z_max, box_2_z_max)\n\n    zero_tensor = torch.zeros_like(x_a)\n    intersection_volume = torch.maximum((x_b - x_a), zero_tensor) * torch.maximum((y_b - y_a), zero_tensor) * \\\n                          torch.maximum((z_b - z_a), zero_tensor)\n    box_1_volume = (box_1_x_max - box_1_x_min) * (box_1_y_max - box_1_y_min) * (box_1_z_max - box_1_z_min)\n    box_2_volume = (box_2_x_max - box_2_x_min) * (box_2_y_max - box_2_y_min) * (box_2_z_max - box_2_z_min)\n    iou = intersection_volume / (box_1_volume + box_2_volume - intersection_volume + torch.finfo(torch.float32).eps)\n    return iou.flatten()", ""]}
{"filename": "m3drefclip/callback/lr_decay_callback.py", "chunked_list": ["from lightning.pytorch.callbacks import Callback\nfrom math import cos, pi\n\n\nclass LrDecayCallback(Callback):\n    def on_train_epoch_end(self, trainer, pl_module):\n        # cosine learning rate decay\n        current_epoch = trainer.current_epoch\n        start_epoch = pl_module.hparams.cfg.model.lr_decay.start_epoch\n        if trainer.current_epoch < start_epoch:\n            return\n        end_epoch = pl_module.hparams.cfg.trainer.max_epochs\n        clip = 1e-6\n        for param_group in trainer.optimizers[0].param_groups:\n            param_group['lr'] = clip + 0.5 * (pl_module.hparams.cfg.model.optimizer.lr - clip) * \\\n                                (1 + cos(pi * ((current_epoch - start_epoch) / (end_epoch - start_epoch))))", ""]}
{"filename": "m3drefclip/callback/gpu_cache_clean_callback.py", "chunked_list": ["from lightning.pytorch.callbacks import Callback\nimport torch\n\n\nclass GPUCacheCleanCallback(Callback):\n    def on_train_batch_start(self, *args, **kwargs):\n        torch.cuda.empty_cache()\n\n    def on_validation_batch_start(self, *args, **kwargs):\n        torch.cuda.empty_cache()\n\n    def on_test_batch_start(self, *args, **kwargs):\n        torch.cuda.empty_cache()\n\n    def on_predict_batch_start(self, *args, **kwargs):\n        torch.cuda.empty_cache()", ""]}
{"filename": "m3drefclip/loss/reference_loss.py", "chunked_list": ["import lightning.pytorch as pl\nimport torch\nimport torch.nn as nn\nfrom scipy.optimize import linear_sum_assignment\nfrom m3drefclip.util.utils import get_batch_aabb_pair_ious\n\n\nclass RefBCELoss(pl.LightningModule):\n    def __init__(self, iou_threshold, matching_strategy, chunk_size, max_num_proposals):\n        super().__init__()\n        self.criterion = nn.MultiLabelSoftMarginLoss()\n        self.iou_threshold = iou_threshold\n        self.matching_strategy = matching_strategy\n        self.chunk_size = chunk_size\n        self.max_num_proposals = max_num_proposals\n\n    def forward(self, output_dict, pred_aabbs, pred_scores, target, gt_target_objs_mask, gt_aabb_offset):\n        batch_size = pred_aabbs.shape[0]\n        output_dict[\"gt_labels\"] = torch.zeros(\n            size=(batch_size, self.chunk_size, self.max_num_proposals), dtype=torch.float32, device=self.device\n        )\n        for batch_i in range(batch_size):\n            aabb_start_idx = gt_aabb_offset[batch_i]\n            aabb_end_idx = gt_aabb_offset[batch_i + 1]\n            for lang_i in range(self.chunk_size):\n                single_aabb_mask = gt_target_objs_mask[lang_i, aabb_start_idx:aabb_end_idx]\n                if torch.count_nonzero(single_aabb_mask) == 0:\n                    continue\n                curr_gt_aabb = target[aabb_start_idx:aabb_end_idx][single_aabb_mask]\n\n                iou_matrix = torch.zeros(\n                    size=(curr_gt_aabb.shape[0], self.max_num_proposals), dtype=pred_aabbs.dtype, device=self.device\n                )\n                for i, gt_aabb in enumerate(curr_gt_aabb):\n                    ious = get_batch_aabb_pair_ious(\n                        pred_aabbs[batch_i], gt_aabb.tile(dims=(self.max_num_proposals, 1, 1))\n                    )\n                    if self.matching_strategy == \"all\":\n                        filtered_ious_indices = torch.where(ious >= self.iou_threshold)[0]\n                        if filtered_ious_indices.shape[0] == 0:\n                            continue\n                        output_dict[\"gt_labels\"][batch_i, lang_i, filtered_ious_indices] = 1\n                    elif self.matching_strategy == \"hungarian\":\n                        iou_matrix[i] = ious * -1\n                    else:\n                        raise NotImplementedError\n                if self.matching_strategy == \"hungarian\":\n                    # TODO: implement pytorch gpu version\n                    row_idx, col_idx = linear_sum_assignment(iou_matrix.cpu())\n                    for index in range(len(row_idx)):\n                        if (iou_matrix[row_idx[index], col_idx[index]] * -1) >= self.iou_threshold:\n                            output_dict[\"gt_labels\"][batch_i, lang_i, col_idx[index]] = 1\n        # self.criterion.weight = gt_aabb_dense_mask.repeat_interleave(lang_chunk_size, dim=0)\n        return self.criterion(pred_scores, output_dict[\"gt_labels\"].flatten(start_dim=0, end_dim=1))", "\n\nclass RefCELoss(pl.LightningModule):\n    def __init__(self, iou_threshold, chunk_size, max_num_proposals):\n        super().__init__()\n        self.criterion = nn.CrossEntropyLoss()\n        self.iou_threshold = iou_threshold\n        self.chunk_size = chunk_size\n        self.max_num_proposals = max_num_proposals\n\n    def forward(self, output_dict, pred_aabb_min_max_bounds, pred_scores, gt_aabb_min_max_bounds, gt_target_objs_mask, gt_aabb_offset):\n        batch_size = pred_aabb_min_max_bounds.shape[0]\n        output_dict[\"gt_labels\"] = torch.zeros(\n            size=(batch_size, self.chunk_size, self.max_num_proposals), dtype=torch.float32, device=self.device\n        )\n        gt_aabb_min_max_bounds_filtered = torch.empty(\n            size=(batch_size, self.chunk_size, 2, 3), dtype=torch.float32, device=self.device\n        )\n        for batch_i in range(batch_size):\n            aabb_start_idx = gt_aabb_offset[batch_i]\n            aabb_end_idx = gt_aabb_offset[batch_i + 1]\n            # there should be only one GT aabb\n            gt_aabb_min_max_bounds_filtered[batch_i] = torch.einsum(\n                \"abc,da->dbc\", gt_aabb_min_max_bounds[aabb_start_idx:aabb_end_idx],\n                gt_target_objs_mask[:, aabb_start_idx:aabb_end_idx].float()\n            )\n            # for lang_i in range(self.chunk_size):\n            #     single_aabb_mask = gt_target_objs_mask[lang_i, aabb_start_idx:aabb_end_idx]\n            #     # there should be only one GT aabb\n            #     gt_aabb_min_max_bounds_filtered[batch_i, lang_i] = gt_aabb_min_max_bounds[aabb_start_idx:aabb_end_idx][single_aabb_mask][0]\n\n        ious = get_batch_aabb_pair_ious(\n            pred_aabb_min_max_bounds.unsqueeze(1).expand(size=(-1, self.chunk_size, -1, -1, -1)).reshape(-1, 2, 3),\n            gt_aabb_min_max_bounds_filtered.unsqueeze(2).expand(size=(-1, -1, self.max_num_proposals, -1, -1)).reshape(-1, 2, 3)\n        ).reshape(batch_size, self.chunk_size, -1)\n\n        iou, index = ious.max(dim=2)\n        passed_threshold = torch.zeros(size=(batch_size, self.chunk_size), dtype=torch.float32, device=self.device)\n        passed_threshold[iou >= self.iou_threshold] = 1\n        output_dict[\"gt_labels\"].scatter_(2, index.unsqueeze(-1), passed_threshold.unsqueeze(-1))\n        return self.criterion(pred_scores, output_dict[\"gt_labels\"].flatten(start_dim=0, end_dim=1))", "\n"]}
{"filename": "m3drefclip/loss/pt_offset_loss.py", "chunked_list": ["import torch\nimport lightning.pytorch as pl\nimport torch.nn.functional as F\n\n\nclass PTOffsetLoss(pl.LightningModule):\n    \n    def __init__(self):\n        super(PTOffsetLoss, self).__init__()\n\n    def forward(self, pred_offsets, gt_offsets, valid_mask):\n        \"\"\"Point-wise offset prediction losses in norm and direction\n\n        Args:\n            pred_offsets (torch.Tensor): predicted point offsets, (B, 3), float32, cuda\n            gt_offsets (torch.Tensor): GT point offsets, (B, 3), float32, cuda\n            valid_mask (torch.Tensor): indicate valid points involving in loss, (B,), bool, cuda\n\n        Returns:\n            torch.Tensor: [description]\n        \"\"\"\n        if valid_mask.count_nonzero() == 0:\n            # for invalid points, don't calculate loss\n            return 0, 0\n\n        valid_pred_offsets = pred_offsets[valid_mask]\n        valid_gt_offsets = gt_offsets[valid_mask]\n        pt_diff = valid_pred_offsets - valid_gt_offsets  # (N, 3)\n        pt_dist = torch.sum(torch.abs(pt_diff), dim=-1)  # (N)\n\n        normalized_gt_offsets = F.normalize(valid_gt_offsets, p=2, dim=1, eps=torch.finfo(valid_gt_offsets.dtype).eps)\n        normalized_pt_offsets = F.normalize(valid_pred_offsets, p=2, dim=1, eps=torch.finfo(valid_gt_offsets.dtype).eps)\n        direction_diff = - (normalized_gt_offsets * normalized_pt_offsets).sum(-1)  # (N)\n\n        offset_norm_loss = torch.mean(pt_dist)\n        offset_direction_loss = torch.mean(direction_diff)\n\n        return offset_norm_loss, offset_direction_loss", ""]}
{"filename": "m3drefclip/loss/contrastive_loss.py", "chunked_list": ["import lightning.pytorch as pl\nimport torch.nn as nn\nimport torch\n\n\nclass SinglePairContrastiveLoss(pl.LightningModule):\n    def __init__(self, temperature, split_batch):\n        super().__init__()\n        self.logit_scale = nn.Parameter(torch.tensor(temperature, device=self.device, dtype=torch.float32))\n        self.split_batch = split_batch\n\n    def forward(self, aabb_features, sentence_features, gt_labels, multiple_gt=False):\n        aabb_features_filtered = torch.einsum(\"abc,adb->adc\", aabb_features, gt_labels).flatten(0, 1)\n\n        # for multiple gt labels, it takes the average of their features as the final feature\n        gt_count = torch.count_nonzero(gt_labels, dim=2).flatten(0, 1)\n        gt_mask = gt_count != 0\n\n        if not gt_mask.any():\n            return 0.0\n\n        aabb_features_filtered = aabb_features_filtered[gt_mask] / gt_count[gt_mask].unsqueeze(-1)\n\n        # normalized features\n        normalized_aabb_features = nn.functional.normalize(aabb_features_filtered, dim=1)\n        normalized_sentence_features = nn.functional.normalize(sentence_features[gt_mask], dim=1)\n\n        logit_scale = self.logit_scale.exp()\n        logits_1 = logit_scale * normalized_aabb_features @ normalized_sentence_features.t()\n        logits_2 = logit_scale * normalized_sentence_features @ normalized_aabb_features.t()\n\n        labels = torch.arange(normalized_aabb_features.shape[0], device=self.device, dtype=torch.uint8)  # max 255\n        loss_a = nn.functional.cross_entropy(logits_1, labels)\n        loss_b = nn.functional.cross_entropy(logits_2, labels)\n        return (loss_a + loss_b) / 2", ""]}
{"filename": "m3drefclip/common_ops/setup.py", "chunked_list": ["from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='COMMON_OPS',\n    version=\"1.0\",\n    ext_modules=[\n        CUDAExtension('COMMON_OPS', [\n            'src/common_ops_api.cpp',\n            'src/common_ops.cpp',", "            'src/common_ops_api.cpp',\n            'src/common_ops.cpp',\n            'src/cuda.cu'\n        ], extra_compile_args={'cxx': ['-g'], 'nvcc': ['-O2']})\n    ],\n    cmdclass={'build_ext': BuildExtension}\n)\n"]}
{"filename": "m3drefclip/common_ops/functions/pointgroup_ops.py", "chunked_list": ["from torch.autograd import Function\nimport COMMON_OPS\n\n\nclass PGBFSCluster(Function):\n    @staticmethod\n    def forward(ctx, semantic_label, ball_query_idxs, start_len, threshold):\n        \"\"\"\n        :param ctx:\n        :param semantic_label: (N), int16\n        :param ball_query_idxs: (nActive), int\n        :param start_len: (N, 2), int\n        :return: cluster_idxs:  int (sumNPoint, 2), dim 0 for cluster_id, dim 1 for corresponding point idxs in N\n        :return: cluster_offsets: int (nCluster + 1)\n        \"\"\"\n\n        N = start_len.size(0)\n\n        assert semantic_label.is_contiguous()\n        assert ball_query_idxs.is_contiguous()\n        assert start_len.is_contiguous()\n\n        cluster_obj_idxs, cluster_point_idxs, cluster_offsets = COMMON_OPS.pg_bfs_cluster(\n            semantic_label, ball_query_idxs, start_len, N, threshold\n        )\n\n        return cluster_obj_idxs, cluster_point_idxs, cluster_offsets\n\n    @staticmethod\n    def backward(ctx, a=None):\n        return None", "\n\npg_bfs_cluster = PGBFSCluster.apply\n"]}
{"filename": "m3drefclip/common_ops/functions/common_ops.py", "chunked_list": ["\nimport torch\nfrom torch.autograd import Function\nimport COMMON_OPS\n\n\nclass BallQueryBatchP(Function):\n    @staticmethod\n    def forward(ctx, coords, batch_idxs, batch_offsets, radius, meanActive):\n        \"\"\"\n        :param ctx:\n        :param coords: (n, 3) float\n        :param batch_idxs: (n) uint8\n        :param batch_offsets: (B+1) int\n        :param radius: float\n        :param meanActive: int\n        :return: idx (nActive), int\n        :return: start_len (n, 2), int\n        \"\"\"\n\n        n = coords.size(0)\n\n        assert coords.is_contiguous() and coords.is_cuda\n        assert batch_idxs.is_contiguous() and batch_idxs.is_cuda\n        assert batch_offsets.is_contiguous() and batch_offsets.is_cuda\n\n        while True:\n            idx = torch.zeros(n * meanActive, dtype=torch.int32, device=\"cuda\")\n            start_len = torch.zeros((n, 2), dtype=torch.int32, device=\"cuda\")\n            nActive = COMMON_OPS.ballquery_batch_p(coords, batch_idxs, batch_offsets, idx, start_len, n, meanActive, radius)\n            if nActive <= n * meanActive:\n                break\n            meanActive = int(nActive // n + 1)\n        idx = idx[:nActive]\n\n        return idx, start_len\n\n    @staticmethod\n    def backward(ctx, a=None, b=None):\n        return None, None, None", "\n\nballquery_batch_p = BallQueryBatchP.apply\n\n\nclass SecMean(Function):\n    @staticmethod\n    def forward(ctx, inp, offsets):\n        \"\"\"\n        :param ctx:\n        :param inp: (N, C) float\n        :param offsets: (nProposal + 1) int\n        :return: out (nProposal, C) float\n        \"\"\"\n        nProposal = offsets.size(0) - 1\n        C = inp.size(1)\n\n        assert inp.is_contiguous()\n        assert offsets.is_contiguous()\n\n        out = torch.zeros((nProposal, C), dtype=torch.float32, device=inp.device)\n\n        COMMON_OPS.sec_mean(inp, offsets, out, nProposal, C)\n\n        return out\n\n    @staticmethod\n    def backward(ctx, a=None):\n        return None, None", "\n\nsec_mean = SecMean.apply\n\n\nclass SecMin(Function):\n    @staticmethod\n    def forward(ctx, inp, offsets):\n        '''\n        :param ctx:\n        :param inp: (N, C) float\n        :param offsets: (nProposal + 1) int\n        :return: out (nProposal, C) float\n        '''\n        nProposal = offsets.size(0) - 1\n        C = inp.size(1)\n\n        assert inp.is_contiguous()\n        assert offsets.is_contiguous()\n\n        out = torch.zeros((nProposal, C), dtype=torch.float32, device=\"cuda\")\n\n        COMMON_OPS.sec_min(inp, offsets, out, nProposal, C)\n\n        return out\n\n    @staticmethod\n    def backward(ctx, a=None):\n        return None, None", "\n\nsec_min = SecMin.apply\n\n\nclass SecMax(Function):\n    @staticmethod\n    def forward(ctx, inp, offsets):\n        '''\n        :param ctx:\n        :param inp: (N, C) float\n        :param offsets: (nProposal + 1) int\n        :return: out (nProposal, C) float\n        '''\n        nProposal = offsets.size(0) - 1\n        C = inp.size(1)\n\n        assert inp.is_contiguous()\n        assert offsets.is_contiguous()\n\n        out = torch.zeros((nProposal, C), dtype=torch.float32, device=\"cuda\")\n\n        COMMON_OPS.sec_max(inp, offsets, out, nProposal, C)\n\n        return out\n\n    @staticmethod\n    def backward(ctx, a=None):\n        return None, None", "\n\nsec_max = SecMax.apply\n\nclass RoiPool(Function):\n    @staticmethod\n    def forward(ctx, feats, proposals_offset):\n        '''\n        :param ctx:\n        :param feats: (sumNPoint, C) float\n        :param proposals_offset: (nProposal + 1) int\n        :return: output_feats (nProposal, C) float\n        '''\n        nProposal = proposals_offset.size(0) - 1\n        sumNPoint, C = feats.size()\n\n        assert feats.is_contiguous()\n        assert proposals_offset.is_contiguous()\n\n        output_feats = torch.zeros((nProposal, C), dtype=torch.float32, device=\"cuda\")\n        output_maxidx = torch.zeros((nProposal, C), dtype=torch.int32, device=\"cuda\")\n\n        COMMON_OPS.roipool_fp(feats, proposals_offset, output_feats, output_maxidx, nProposal, C)\n\n        ctx.for_backwards = (output_maxidx, proposals_offset, sumNPoint)\n\n        return output_feats\n\n    @staticmethod\n    def backward(ctx, d_output_feats):\n        nProposal, C = d_output_feats.size()\n\n        output_maxidx, proposals_offset, sumNPoint = ctx.for_backwards\n\n        d_feats = torch.zeros((sumNPoint, C), dtype=torch.float32, device=\"cuda\")\n\n        COMMON_OPS.roipool_bp(d_feats, proposals_offset, output_maxidx, d_output_feats.contiguous(), nProposal, C)\n\n        return d_feats, None", "\n\nroipool = RoiPool.apply\n\nclass GetIoU(Function):\n    @staticmethod\n    def forward(ctx, proposals_idx, proposals_offset, instance_ids, instance_pointnum):\n        '''\n        :param ctx:\n        :param proposals_idx: (sumNPoint), int\n        :param proposals_offset: (nProposal + 1), int\n        :param instance_ids: (N), int16, 0~total_nInst-1, -1\n        :param instance_pointnum: (total_nInst), int\n        :return: proposals_iou: (nProposal, total_nInst), float\n        '''\n        nInstance = instance_pointnum.size(0)\n        nProposal = proposals_offset.size(0) - 1\n\n        assert proposals_idx.is_contiguous() and proposals_idx.is_cuda\n        assert proposals_offset.is_contiguous() and proposals_offset.is_cuda\n        assert instance_ids.is_contiguous() and instance_ids.is_cuda\n        assert instance_pointnum.is_contiguous() and instance_pointnum.is_cuda\n\n        proposals_iou = torch.zeros((nProposal, nInstance), dtype=torch.float32, device=\"cuda\")\n\n        COMMON_OPS.get_iou(proposals_idx, proposals_offset, instance_ids, instance_pointnum, proposals_iou, nInstance,\n                           nProposal)\n\n        return proposals_iou\n\n    @staticmethod\n    def backward(ctx, a=None):\n        return None, None, None, None", "\n\nget_iou = GetIoU.apply\n\n\ndef crop_pcd_from_aabbs(aabb_min_max_bounds, scene_points_xyz):\n    output_masks = torch.zeros(\n        size=(aabb_min_max_bounds.shape[0], scene_points_xyz.shape[0]), dtype=torch.bool, device=scene_points_xyz.device\n    )\n    COMMON_OPS.crop_pcds_from_aabbs(\n        aabb_min_max_bounds.contiguous(), scene_points_xyz.contiguous(), output_masks\n    )\n    return output_masks", "\n\ndef convert_sparse_tensor_to_dense(sparse_info, idx_offsets, max_num_aabbs):\n    dense_aabb_info = torch.zeros(\n        size=(idx_offsets.shape[0] - 1, max_num_aabbs) + sparse_info.shape[1:],\n        dtype=sparse_info.dtype, device=sparse_info.device\n    )\n    for i in range(idx_offsets.shape[0] - 1):\n        aabb_start_idx = idx_offsets[i]\n        aabb_end_idx = idx_offsets[i + 1]\n        dense_aabb_info[i][0:aabb_end_idx - aabb_start_idx] = sparse_info[aabb_start_idx:aabb_end_idx]\n    return dense_aabb_info", ""]}
{"filename": "dataset/scannetv2/preprocess_all_data.py", "chunked_list": ["import os\nimport csv\nimport json\nimport torch\nimport hydra\nimport numpy as np\nimport open3d as o3d\nfrom os import cpu_count\nfrom functools import partial\nfrom tqdm.contrib.concurrent import process_map", "from functools import partial\nfrom tqdm.contrib.concurrent import process_map\n\n\ndef get_semantic_mapping_file(file_path, mapping_name):\n    label_mapping = {}\n    mapping_col_idx = {\n        \"nyu40\": 4,\n        \"eigen13\": 5,\n        \"mpcat40\": 16\n    }\n    with open(file_path, \"r\") as f:\n        tsv_file = csv.reader(f, delimiter=\"\\t\")\n        next(tsv_file)  # skip the header\n        for line in tsv_file:\n            label_mapping[line[1]] = int(line[mapping_col_idx[mapping_name]])\n    return label_mapping", "\ndef read_axis_align_matrix(file_path):\n    axis_align_matrix = None\n    with open(file_path, \"r\") as f:\n        for line in f:\n            line_content = line.strip()\n            if 'axisAlignment' in line_content:\n                axis_align_matrix = [float(x) for x in line_content.strip('axisAlignment = ').split(' ')]\n                axis_align_matrix = np.array(axis_align_matrix).reshape((4, 4))\n                break\n    return axis_align_matrix", "\n\ndef read_mesh_file(file_path, axis_align_matrix):\n    mesh = o3d.io.read_triangle_mesh(file_path)\n    if axis_align_matrix is not None:\n        mesh.transform(axis_align_matrix)  # align the mesh\n    mesh.compute_vertex_normals()\n    return np.asarray(mesh.vertices, dtype=np.float32), \\\n           np.rint(np.asarray(mesh.vertex_colors) * 255).astype(np.uint8), \\\n           np.asarray(mesh.vertex_normals, dtype=np.float32)", "\n\ndef get_semantic_labels(obj_name_to_segs, seg_to_verts, num_verts, label_map, valid_semantic_mapping):\n    # create a map, skip invalid labels to make the final semantic labels consecutive\n    filtered_label_map = {}\n    for i, valid_id in enumerate(valid_semantic_mapping):\n        filtered_label_map[valid_id] = i\n\n    semantic_labels = np.full(shape=num_verts, fill_value=-1, dtype=np.int8)  # max value: 127\n    for label, segs in obj_name_to_segs.items():\n        for seg in segs:\n            verts = seg_to_verts[seg]\n            if label_map[label] not in filtered_label_map:\n                semantic_labels[verts] = 19\n            elif label_map[label] == 22:\n                semantic_labels[verts] = -1\n            else:\n                semantic_labels[verts] = filtered_label_map[label_map[label]]\n    return semantic_labels", "\n\ndef read_agg_file(file_path, label_map, invalid_ids):\n    object_id_to_segs = {}\n    obj_name_to_segs = {}\n    with open(file_path, \"r\") as f:\n        data = json.load(f)\n    for group in data['segGroups']:\n        object_name = group['label']\n        if object_name not in label_map:\n            object_name = \"case\"  # TODO: randomly assign a name mapped to \"objects\"\n        if label_map[object_name] in invalid_ids:\n            # skip room architecture\n            continue\n        segments = group['segments']\n        object_id_to_segs[group[\"objectId\"]] = segments\n        if object_name in obj_name_to_segs:\n            obj_name_to_segs[object_name].extend(segments)\n        else:\n            obj_name_to_segs[object_name] = segments.copy()\n    return object_id_to_segs, obj_name_to_segs", "\n\ndef read_seg_file(file_path):\n    seg2verts = {}\n    with open(file_path, \"r\") as f:\n        data = json.load(f)\n    for vert, seg in enumerate(data['segIndices']):\n        if seg not in seg2verts:\n            seg2verts[seg] = []\n        seg2verts[seg].append(vert)\n    return seg2verts", "\n\ndef get_instance_ids(object_id2segs, seg2verts, sem_labels):\n    instance_ids = np.full(shape=len(sem_labels), fill_value=-1, dtype=np.int16)\n    for object_id, segs in object_id2segs.items():\n        for seg in segs:\n            verts = seg2verts[seg]\n            instance_ids[verts] = object_id\n    return instance_ids\n", "\n\ndef get_aabbs(xyz, instance_ids):\n    unique_inst_ids = np.unique(instance_ids)\n    unique_inst_ids = unique_inst_ids[unique_inst_ids != -1]  # skip the invalid id\n    num_of_aabb = unique_inst_ids.shape[0]\n    aabb_corner_points = np.empty(shape=(num_of_aabb, 8, 3), dtype=np.float32)\n    aabb_obj_ids = np.empty(shape=num_of_aabb, dtype=np.int16)\n\n    combinations = np.array(np.meshgrid([0, 1], [0, 1], [0, 1], copy=False), dtype=np.float32).T.reshape(-1, 3)\n    for i, instance_id in enumerate(unique_inst_ids):\n        point_indices = instance_ids == instance_id\n        object_points = xyz[point_indices]\n        min_corner = object_points.min(axis=0)\n        max_corner = object_points.max(axis=0)\n        corner_points = min_corner + (max_corner - min_corner) * combinations\n        aabb_corner_points[i] = corner_points\n        aabb_obj_ids[i] = instance_id\n    return aabb_corner_points, aabb_obj_ids", "\n\ndef process_one_scene(scene, cfg, split, label_map, invalid_ids, valid_semantic_mapping):\n    mesh_file_path = os.path.join(cfg.data.raw_scene_path, scene, scene + '_vh_clean_2.ply')\n    agg_file_path = os.path.join(cfg.data.raw_scene_path, scene, scene + '.aggregation.json')\n    seg_file_path = os.path.join(cfg.data.raw_scene_path, scene, scene + '_vh_clean_2.0.010000.segs.json')\n    meta_file_path = os.path.join(cfg.data.raw_scene_path, scene, scene + '.txt')\n\n    # read meta_file\n    axis_align_matrix = read_axis_align_matrix(meta_file_path)\n\n    # read mesh file\n    xyz, rgb, normal = read_mesh_file(mesh_file_path, axis_align_matrix)\n\n    num_verts = len(xyz)\n\n    sem_labels = None\n    object_ids = None\n    aabb_obj_ids = None\n    aabb_corner_xyz = None\n    if os.path.exists(agg_file_path) and os.path.exists(seg_file_path):\n        # read seg_file\n        seg2verts = read_seg_file(seg_file_path)\n        # read agg_file\n        object_id_to_segs, obj_name_to_segs = read_agg_file(agg_file_path, label_map, invalid_ids)\n        # get semantic labels\n        sem_labels = get_semantic_labels(obj_name_to_segs, seg2verts, num_verts, label_map, valid_semantic_mapping)\n        # get instance ids\n        object_ids = get_instance_ids(object_id_to_segs, seg2verts, sem_labels)\n        # get axis-aligned bounding boxes\n        aabb_corner_xyz, aabb_obj_ids = get_aabbs(xyz, object_ids)\n\n    torch.save(\n        {\"xyz\": xyz, \"rgb\": rgb, \"normal\": normal, \"sem_labels\": sem_labels,\n         \"instance_ids\": object_ids, \"aabb_obj_ids\": aabb_obj_ids, \"aabb_corner_xyz\": aabb_corner_xyz},\n        os.path.join(cfg.data.scene_dataset_path, split, f\"{scene}.pth\")\n    )", "\n\n@hydra.main(version_base=None, config_path=\"../../config\", config_name=\"global_config\")\ndef main(cfg):\n    max_workers = cpu_count() if \"workers\" not in cfg else cfg.workers\n    print(f\"\\nUsing {max_workers} CPU threads.\")\n\n    # read semantic label mapping file\n    label_map = get_semantic_mapping_file(cfg.data.scene_metadata.label_mapping_file,\n                                          cfg.data.scene_metadata.semantic_mapping_name)\n\n    for split in (\"train\", \"val\", \"test\"):\n        output_path = os.path.join(cfg.data.scene_dataset_path, split)\n        if os.path.exists(output_path):\n            print(f\"\\nWarning: output directory {os.path.abspath(output_path)} exists, \"\n                  f\"existing files will be overwritten.\")\n        os.makedirs(output_path, exist_ok=True)\n        with open(getattr(cfg.data.scene_metadata, f\"{split}_scene_ids\")) as f:\n            split_list = [line.strip() for line in f]\n        print(f\"==> Processing {split} split ...\")\n        process_map(\n            partial(\n                process_one_scene, cfg=cfg, split=split, label_map=label_map,\n                invalid_ids=cfg.data.scene_metadata.invalid_semantic_labels,\n                valid_semantic_mapping=cfg.data.scene_metadata.valid_semantic_mapping\n            ), split_list, chunksize=1, max_workers=max_workers\n        )\n        print(f\"==> Complete. Saved at: {os.path.abspath(output_path)}\\n\")", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "dataset/nr3d/add_evaluation_labels.py", "chunked_list": ["\"\"\"\nThis help file adds evaluation labels (easy / hard / view-dep / view-indep) to original Nr3D data, it follows the\nofficial code logic. Please refer to\nhttps://github.com/referit3d/referit3d/blob/eccv/referit3d/analysis/deepnet_predictions.py\n\"\"\"\n\nfrom tqdm.contrib.concurrent import process_map\nimport pandas as pd\nimport hydra\nimport ast", "import hydra\nimport ast\n\n\ndef decode_stimulus_string(s):\n    if len(s.split('-', maxsplit=4)) == 4:\n        scene_id, instance_label, n_objects, target_id = \\\n            s.split('-', maxsplit=4)\n        distractors_ids = \"\"\n    else:\n        scene_id, instance_label, n_objects, target_id, distractors_ids = \\\n            s.split('-', maxsplit=4)\n    instance_label = instance_label.replace('_', ' ')\n    n_objects = int(n_objects)\n    target_id = int(target_id)\n    distractors_ids = [int(i) for i in distractors_ids.split('-') if i != '']\n    assert len(distractors_ids) == n_objects - 1\n    return scene_id, instance_label, n_objects, target_id, distractors_ids", "\n\ndef get_view_dependent_mask(df):\n    target_words = {\n        'front', 'behind', 'back', 'right', 'left', 'facing', 'leftmost', 'rightmost', 'looking', 'across'\n    }\n    return df.tokens.apply(lambda x: len(set(ast.literal_eval(x)).intersection(target_words)) > 0)\n\n\ndef get_easy_mask(df):\n    return df.stimulus_id.apply(lambda x: decode_stimulus_string(x)[2]) <= 2", "\ndef get_easy_mask(df):\n    return df.stimulus_id.apply(lambda x: decode_stimulus_string(x)[2]) <= 2\n\n\ndef add_evaluation_labels_to_csv(file_path):\n    df = pd.read_csv(file_path)\n    easy_mask = get_easy_mask(df)\n    view_dependent_mask = get_view_dependent_mask(df)\n    df[\"is_easy\"] = easy_mask\n    df[\"is_view_dep\"] = view_dependent_mask\n    df.to_csv(file_path, index=False)", "\n\n@hydra.main(version_base=None, config_path=\"../../config\", config_name=\"global_config\")\ndef main(cfg):\n    print(\"\\nDefault: using all CPU cores.\")\n    file_paths = []\n    for split in (\"train\", \"test\"):\n        file_paths.append(getattr(cfg.data.lang_metadata, f\"{split}_language_data\"))\n    process_map(add_evaluation_labels_to_csv, file_paths, chunksize=1)\n    print(f\"==> Complete.\")", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "dataset/scanrefer/convert_output_to_benchmark_format.py", "chunked_list": ["import hydra\nimport json\nimport os\n\n\n@hydra.main(version_base=None, config_path=\"../../config\", config_name=\"global_config\")\ndef main(cfg):\n    # we need to get unique/multiple labels from the dataset json\n    unique_multiple_dict = {}\n    with open(cfg.data.lang_metadata.test_language_data, \"r\") as f:\n        dataset_json = json.load(f)\n    for item in dataset_json:\n        unique_multiple_dict[item[\"scene_id\"], int(item[\"object_id\"]), int(item[\"ann_id\"])] = 1 if item[\"eval_type\"] == \"multiple\" else 0\n\n    outputs = []\n\n    prediction_file_names = os.listdir(cfg.pred_path)\n    for prediction_file_name in prediction_file_names:\n        file_path = os.path.join(cfg.pred_path, prediction_file_name)\n        with open(file_path, \"r\") as f:\n            input_json = json.load(f)\n        for item in input_json:\n            output_item = {\n                \"scene_id\": prediction_file_name[:-5],\n                \"object_id\": item[\"object_id\"],\n                \"ann_id\": item[\"ann_id\"],\n                \"bbox\": item[\"aabb\"][0],\n                \"unique_multiple\": unique_multiple_dict[(prediction_file_name[:-5], item[\"object_id\"], item[\"ann_id\"])],\n                \"others\": 0  # dummy value\n            }\n            outputs.append(output_item)\n\n    os.makedirs(cfg.output_path, exist_ok=True)\n    with open(os.path.join(cfg.output_path, \"benchmark_output.json\"), \"w\") as f:\n        json.dump(outputs, f)\n\n    print(f\"==> Complete. Saved at: {os.path.abspath(cfg.output_path)}\")", "\n\nif __name__ == '__main__':\n    print(\"\\nThis script converts M3DRef-CLIP predictions to ScanRefer hidden test benchmark format.\")\n    main()\n\n"]}
{"filename": "dataset/scanrefer/add_evaluation_labels.py", "chunked_list": ["\"\"\"\nThis help file adds evaluation labels (unique / multiple) to original ScanRefer data, it follows the\nofficial code logic. Please refer to\nhttps://github.com/daveredrum/ScanRefer/blob/master/lib/dataset.py\n\"\"\"\n\nfrom tqdm.contrib.concurrent import process_map\nfrom functools import partial\nimport hydra\nimport json", "import hydra\nimport json\nimport csv\n\n\ndef get_semantic_mapping_file(file_path, mapping_name):\n    label_mapping = {}\n    mapping_col_idx = {\n        \"nyu40\": 4,\n        \"eigen13\": 5,\n        \"mpcat40\": 16\n    }\n    with open(file_path, \"r\") as f:\n        tsv_file = csv.reader(f, delimiter=\"\\t\")\n        next(tsv_file)  # skip the header\n        for line in tsv_file:\n            label_mapping[line[1]] = int(line[mapping_col_idx[mapping_name]])\n    return label_mapping", "\n\ndef add_unique_multiple_labels_to_json(file_path, label_mapping, valid_semantic_mapping):\n    with open(file_path, \"r\") as f:\n        scanrefer_json_data = json.load(f)\n    obj_cache = {}\n    sem_cache = {}\n    for item in scanrefer_json_data:\n        if (item[\"scene_id\"], item[\"object_id\"]) in obj_cache:\n            continue\n        obj_name = item[\"object_name\"].replace(\"_\", \" \")\n        sem_label = 39\n        if obj_name in label_mapping:\n            sem_label = label_mapping[obj_name]\n        if sem_label not in valid_semantic_mapping:\n            sem_label = 39\n        if (item['scene_id'], sem_label) not in sem_cache:\n            sem_cache[(item['scene_id'], sem_label)] = 0\n        sem_cache[(item['scene_id'], sem_label)] += 1\n        obj_cache[(item[\"scene_id\"], item[\"object_id\"])] = True\n\n    for item in scanrefer_json_data:\n        scene_id = item['scene_id']\n        obj_name = item[\"object_name\"].replace(\"_\", \" \")\n        sem_label = 39\n        if obj_name in label_mapping:\n            sem_label = label_mapping[obj_name]\n        if sem_label not in valid_semantic_mapping:\n            sem_label = 39\n        assert sem_cache[(scene_id, sem_label)] >= 1\n        item[\"eval_type\"] = \"unique\" if sem_cache[(scene_id, sem_label)] == 1 else \"multiple\"\n    # save the new json\n    with open(file_path, \"w\") as f:\n        json.dump(scanrefer_json_data, f, indent=2)", "\n\n@hydra.main(version_base=None, config_path=\"../../config\", config_name=\"global_config\")\ndef main(cfg):\n    print(\"\\nDefault: using all CPU cores.\")\n    label_mapping = get_semantic_mapping_file(cfg.data.scene_metadata.label_mapping_file, \"nyu40\")\n    file_paths = []\n    for split in (\"train\", \"val\", \"test\"):\n        file_paths.append(getattr(cfg.data.lang_metadata, f\"{split}_language_data\"))\n\n    process_map(\n        partial(\n            add_unique_multiple_labels_to_json, label_mapping=label_mapping,\n            valid_semantic_mapping=cfg.data.scene_metadata.valid_semantic_mapping\n        ), file_paths, chunksize=1\n    )\n\n    print(f\"==> Complete.\")", "\n\nif __name__ == '__main__':\n    main()\n"]}
