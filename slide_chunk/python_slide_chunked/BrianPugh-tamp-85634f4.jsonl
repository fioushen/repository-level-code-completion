{"filename": "build.py", "chunked_list": ["import os\nimport shutil\nfrom pathlib import Path\n\nallowed_to_fail = os.environ.get(\"CIBUILDWHEEL\", \"0\") != \"1\"\n\nprofile = os.environ.get(\"TAMP_PROFILE\", \"0\") == \"1\"\n\n\ndef build_cython_extensions():\n    import Cython.Compiler.Options\n    from Cython.Build import build_ext, cythonize\n    from Cython.Compiler.Options import get_directive_defaults\n    from setuptools import Extension\n    from setuptools.dist import Distribution\n\n    Cython.Compiler.Options.annotate = True\n\n    define_macros = []\n\n    if profile:\n        print(\"Setting profiling configuration.\")\n        directive_defaults = get_directive_defaults()\n        directive_defaults[\"linetrace\"] = True\n        directive_defaults[\"binding\"] = True\n\n        define_macros.append((\"CYTHON_TRACE\", \"1\"))\n\n    if os.name == \"nt\":  # Windows\n        extra_compile_args = [\n            \"/O2\",\n        ]\n    else:  # UNIX-based systems\n        extra_compile_args = [\n            \"-O3\",\n            \"-Werror\",\n            \"-Wno-unreachable-code-fallthrough\",\n            \"-Wno-deprecated-declarations\",\n            \"-Wno-parentheses-equality\",\n        ]\n    include_dirs = [\"tamp/_c_src/\", \"tamp/\"]\n\n    extensions = [\n        Extension(\n            \"tamp._c_compressor\",\n            [\n                \"tamp/_c_src/tamp/common.c\",\n                \"tamp/_c_src/tamp/compressor.c\",\n                \"tamp/_c_compressor.pyx\",\n            ],\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            language=\"c\",\n            define_macros=define_macros,\n        ),\n        Extension(\n            \"tamp._c_decompressor\",\n            [\n                \"tamp/_c_src/tamp/common.c\",\n                \"tamp/_c_src/tamp/decompressor.c\",\n                \"tamp/_c_decompressor.pyx\",\n            ],\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            language=\"c\",\n            define_macros=define_macros,\n        ),\n        Extension(\n            \"tamp._c_common\",\n            [\n                \"tamp/_c_common.pyx\",\n            ],\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            language=\"c\",\n            define_macros=define_macros,\n        ),\n    ]\n\n    include_dirs = set()\n    for extension in extensions:\n        include_dirs.update(extension.include_dirs)\n    include_dirs = list(include_dirs)\n\n    ext_modules = cythonize(extensions, include_path=include_dirs, language_level=3, annotate=True)\n    dist = Distribution({\"ext_modules\": ext_modules})\n    cmd = build_ext(dist)\n    cmd.ensure_finalized()\n    cmd.run()\n\n    for output in cmd.get_outputs():\n        output = Path(output)\n        relative_extension = output.relative_to(cmd.build_lib)\n        shutil.copyfile(output, relative_extension)", "\ndef build_cython_extensions():\n    import Cython.Compiler.Options\n    from Cython.Build import build_ext, cythonize\n    from Cython.Compiler.Options import get_directive_defaults\n    from setuptools import Extension\n    from setuptools.dist import Distribution\n\n    Cython.Compiler.Options.annotate = True\n\n    define_macros = []\n\n    if profile:\n        print(\"Setting profiling configuration.\")\n        directive_defaults = get_directive_defaults()\n        directive_defaults[\"linetrace\"] = True\n        directive_defaults[\"binding\"] = True\n\n        define_macros.append((\"CYTHON_TRACE\", \"1\"))\n\n    if os.name == \"nt\":  # Windows\n        extra_compile_args = [\n            \"/O2\",\n        ]\n    else:  # UNIX-based systems\n        extra_compile_args = [\n            \"-O3\",\n            \"-Werror\",\n            \"-Wno-unreachable-code-fallthrough\",\n            \"-Wno-deprecated-declarations\",\n            \"-Wno-parentheses-equality\",\n        ]\n    include_dirs = [\"tamp/_c_src/\", \"tamp/\"]\n\n    extensions = [\n        Extension(\n            \"tamp._c_compressor\",\n            [\n                \"tamp/_c_src/tamp/common.c\",\n                \"tamp/_c_src/tamp/compressor.c\",\n                \"tamp/_c_compressor.pyx\",\n            ],\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            language=\"c\",\n            define_macros=define_macros,\n        ),\n        Extension(\n            \"tamp._c_decompressor\",\n            [\n                \"tamp/_c_src/tamp/common.c\",\n                \"tamp/_c_src/tamp/decompressor.c\",\n                \"tamp/_c_decompressor.pyx\",\n            ],\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            language=\"c\",\n            define_macros=define_macros,\n        ),\n        Extension(\n            \"tamp._c_common\",\n            [\n                \"tamp/_c_common.pyx\",\n            ],\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            language=\"c\",\n            define_macros=define_macros,\n        ),\n    ]\n\n    include_dirs = set()\n    for extension in extensions:\n        include_dirs.update(extension.include_dirs)\n    include_dirs = list(include_dirs)\n\n    ext_modules = cythonize(extensions, include_path=include_dirs, language_level=3, annotate=True)\n    dist = Distribution({\"ext_modules\": ext_modules})\n    cmd = build_ext(dist)\n    cmd.ensure_finalized()\n    cmd.run()\n\n    for output in cmd.get_outputs():\n        output = Path(output)\n        relative_extension = output.relative_to(cmd.build_lib)\n        shutil.copyfile(output, relative_extension)", "\n\ntry:\n    build_cython_extensions()\nexcept Exception:\n    if not allowed_to_fail:\n        raise\n"]}
{"filename": "tamp/compressor_viper.py", "chunked_list": ["\"\"\"Micropython optimized for performance over readability.\n\"\"\"\nfrom io import BytesIO\n\nimport micropython\nfrom micropython import const\n\nfrom . import ExcessBitsError, bit_size, compute_min_pattern_size, initialize_dictionary\n\n# encodes [2, 15] pattern lengths", "\n# encodes [2, 15] pattern lengths\n_HUFFMAN_CODES = b\"\\x00\\x03\\x08\\x0b\\x14$&+KT\\x94\\x95\\xaa'\"\n# These bit lengths pre-add the 1 bit for the 0-value is_literal flag.\n_HUFFMAN_BITS = b\"\\x02\\x03\\x05\\x05\\x06\\x07\\x07\\x07\\x08\\x08\\x09\\x09\\x09\\x07\"\n_FLUSH_CODE = const(0xAB)  # 8 bits\n\n\ndef _f_write(f, number, size):\n    f.write(number.to_bytes(size, \"big\"))", "def _f_write(f, number, size):\n    f.write(number.to_bytes(size, \"big\"))\n\n\nclass Compressor:\n    def __init__(\n        self,\n        f,\n        *,\n        window=10,\n        literal=8,\n        dictionary=None,\n    ):\n        self._close_f_on_close = False\n        if not hasattr(f, \"write\"):  # It's probably a path-like object.\n            f = open(str(f), \"wb\")\n            self._close_f_on_close = True\n\n        self.window_bits = window\n        self.literal_bits = literal\n\n        self.min_pattern_size = compute_min_pattern_size(window, literal)\n\n        # Window Buffer\n        if dictionary:\n            if bit_size(len(dictionary) - 1) != window:\n                raise ValueError\n            self.window_buf = dictionary\n        else:\n            self.window_buf = initialize_dictionary(1 << window)\n        self.window_pos = 0\n\n        # Input Buffer\n        self.input_buf = bytearray(16)\n        self.input_size = 0\n        self.input_pos = 0\n\n        # Write header\n        self.f = f\n        self.f_buf = ((window - 8) << 5 | (literal - 5) << 3 | int(bool(dictionary)) << 2) << (22)\n        self.f_pos = 8\n\n    @micropython.viper\n    def _compress_input_buffer_single(self) -> int:\n        bytes_written = 0\n        # Viper-ize everything\n        input_buf = ptr8(self.input_buf)\n        input_size = int(self.input_size)\n        input_pos = int(self.input_pos)\n\n        literal_bits = int(self.literal_bits)\n        literal_flag = 1 << literal_bits\n\n        f_buf = int(self.f_buf)\n        f_pos = int(self.f_pos)\n        huffman_bits_ptr8 = ptr8(_HUFFMAN_BITS)\n        huffman_codes_ptr8 = ptr8(_HUFFMAN_CODES)\n\n        window_bits = int(self.window_bits)\n        window_buf = ptr8(self.window_buf)\n        window_size = 1 << int(window_bits)\n        window_pos = int(self.window_pos)\n\n        min_pattern_size = int(self.min_pattern_size)\n\n        f = self.f\n\n        # Find largest match\n        search_i = int(0)\n        match_size = int(0)\n\n        if input_size >= min_pattern_size:\n            for window_index in range(window_size - min_pattern_size + 1):\n                if input_buf[input_pos] != window_buf[window_index]:\n                    continue  # Significant speed short-cut\n\n                input_index = (input_pos + 1) & 0xF\n                if input_buf[input_index] != window_buf[window_index + 1]:\n                    continue  # Small Speed short-cut\n\n                current_match_size = int(2)\n                for k in range(current_match_size, input_size):\n                    input_index = (input_pos + k) & 0xF\n                    if input_buf[input_index] != window_buf[window_index + k] or window_index + k >= window_size:\n                        break\n                    current_match_size = k + 1\n                if current_match_size > match_size:\n                    match_size = current_match_size\n                    search_i = window_index\n\n                    if match_size == input_size:\n                        break\n\n        # Write out a literal or a token\n        if match_size >= min_pattern_size:\n            huffman_index = match_size - min_pattern_size\n            # Adds up to 9 bits\n            f_pos += huffman_bits_ptr8[huffman_index]\n            f_buf |= huffman_codes_ptr8[huffman_index] << (30 - f_pos)\n\n            if f_pos >= 16:\n                _f_write(f, f_buf >> 14, 2)\n                f_buf = (f_buf & 0x3FFF) << 16\n                f_pos -= 16\n                bytes_written += 2\n\n            # Adds up to 15 bits\n            f_pos += window_bits\n            f_buf |= search_i << (30 - f_pos)\n        else:\n            # Adds up to 9 bits\n            match_size = 1\n            if input_buf[input_pos] >> literal_bits:\n                raise ExcessBitsError\n\n            f_pos += literal_bits + 1\n            f_buf |= (input_buf[input_pos] | literal_flag) << (30 - f_pos)\n\n        if f_pos >= 16:\n            _f_write(f, f_buf >> 14, 2)\n            f_buf = (f_buf & 0x3FFF) << 16\n            f_pos -= 16\n            bytes_written += 2\n\n        for _ in range(match_size):  # Copy pattern into buffer\n            window_buf[window_pos] = input_buf[input_pos]\n            input_pos = (input_pos + 1) & 0xF\n            window_pos = (window_pos + 1) % window_size\n\n        input_size -= match_size\n\n        self.input_size = input_size\n        self.input_pos = input_pos\n        self.window_pos = window_pos\n\n        self.f_pos = f_pos\n        self.f_buf = f_buf\n\n        return bytes_written\n\n    @micropython.viper\n    def write(self, data) -> int:\n        bytes_written = 0\n\n        data_l = int(len(data))\n        data_p = ptr8(data)\n\n        input_buf = ptr8(self.input_buf)\n\n        max_pattern_size = int(self.min_pattern_size) + 13\n\n        for i in range(data_l):\n            input_size = int(self.input_size)\n            input_pos = int(self.input_pos)\n\n            pos = (input_pos + input_size) & 0xF\n            input_buf[pos] = data_p[i]\n            input_size += 1\n            self.input_size = input_size\n\n            if input_size == max_pattern_size:\n                bytes_written += int(self._compress_input_buffer_single())\n        return bytes_written\n\n    def flush(self, write_token=True):\n        bytes_written = 0\n        while self.input_size:\n            bytes_written += self._compress_input_buffer_single()\n\n        if self.f_pos > 0 and write_token:\n            self.f_pos += 9\n            self.f_buf |= _FLUSH_CODE << (30 - self.f_pos)\n\n        while self.f_pos > 0:\n            _f_write(self.f, self.f_buf >> 22, 1)\n            self.f_buf = (self.f_buf & 0x3FFFFF) << 8\n            self.f_pos -= 8\n            bytes_written += 1\n\n        self.f_pos = 0\n\n        return bytes_written\n\n    def close(self):\n        bytes_written = self.flush(write_token=False)\n        if self._close_f_on_close:\n            self.f.close()\n        return bytes_written\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()", "\n\nclass TextCompressor(Compressor):\n    def write(self, data: str) -> int:\n        return super().write(data.encode())\n\n\ndef compress(data, *args, **kwargs):\n    with BytesIO() as f:\n        cls = TextCompressor if isinstance(data, str) else Compressor\n        c = cls(f, *args, **kwargs)\n        c.write(data)\n        c.flush(write_token=False)\n        f.seek(0)\n        return f.read()", ""]}
{"filename": "tamp/decompressor_viper.py", "chunked_list": ["from io import BytesIO\n\nimport micropython\nfrom micropython import const\n\nfrom . import compute_min_pattern_size, initialize_dictionary\n\n_HUFFMAN_TABLE = b\"BBBBBBBBBBBBBBBBeeee\\x8a\\x8bxxffffmmmmTTTTTTTTyy\\x8c\\x8fggggCCCCCCCCCCCCCCCC\"\n\n_FLUSH = const(15)", "\n_FLUSH = const(15)\n\n\nclass Decompressor:\n    def __init__(self, f, *, dictionary=None):\n        self._close_f_on_close = False\n        if not hasattr(f, \"read\"):  # It's probably a path-like object.\n            f = open(str(f), \"rb\")\n            self._close_f_on_close = True\n\n        self.f = f\n        self.f_buf = 0\n        self.f_pos = 0\n\n        # Read Header\n        header = self.f.read(1)[0]\n        self.w_bits = (header >> 5) + 8\n        self.literal_bits = ((header >> 3) & 0b11) + 5\n        uses_custom_dictionary = header & 0b100\n        reserved = header & 0b10\n        more_header_bytes = header & 0b1\n\n        if reserved or more_header_bytes:\n            raise NotImplementedError\n\n        if uses_custom_dictionary:\n            if not dictionary:\n                raise ValueError\n            self.w_buf = dictionary\n        else:\n            self.w_buf = initialize_dictionary(1 << self.w_bits)\n        self.w_pos = 0\n\n        self.min_pattern_size = compute_min_pattern_size(self.w_bits, self.literal_bits)\n\n        self.overflow_buf = bytearray(self.min_pattern_size + 13)\n        self.overflow_pos = 0\n        self.overflow_size = 0\n\n    def readinto(self, buf):\n        raise NotImplementedError\n\n    @micropython.viper\n    def _decompress_into(self, out) -> int:\n        \"\"\"Attempt to fill out, will place into overflow.\"\"\"\n        out_capacity = int(len(out))  # const\n        out_buf = ptr8(out)\n\n        huffman_table = ptr8(_HUFFMAN_TABLE)\n\n        literal_bits = int(self.literal_bits)\n\n        overflow_buf = self.overflow_buf\n        overflow_buf_ptr = ptr8(overflow_buf)\n        overflow_pos = int(self.overflow_pos)\n        overflow_size = int(self.overflow_size)\n\n        w_buf = self.w_buf\n        w_buf_ptr = ptr8(w_buf)\n        w_pos = int(self.w_pos)\n        w_bits = int(self.w_bits)\n        w_mask = (1 << w_bits) - 1\n\n        f_pos = int(self.f_pos)\n        f_buf = int(self.f_buf)\n        f = self.f\n\n        min_pattern_size = int(self.min_pattern_size)\n        int(len(overflow_buf))\n\n        # Copy overflow into out if available\n        overflow_copy = int(min(overflow_size, out_capacity))\n        for i in range(overflow_copy):\n            out[i] = overflow_buf_ptr[overflow_pos + i]\n        out_pos = overflow_copy\n        overflow_pos += overflow_copy\n        overflow_size -= overflow_copy\n\n        full_mask = int(0x3FFFFFFF)\n\n        # Decompress more\n        try:\n            while out_pos < out_capacity:\n                overflow_pos = 0  # overflow_size is also always zero here\n\n                if f_pos == 0:\n                    f_buf = int(f.read(1)[0]) << 22  # Will raise IndexError if out of data.\n                    f_pos = 8\n\n                # re-use is_literal flag as match_size so we don't need to explicitly set it\n                match_size = f_buf >> 29\n                # Don't update f_buf & f_pos here, in case there's not enough data available.\n\n                if match_size:\n                    if f_pos < (literal_bits + 1):\n                        f_buf |= int(f.read(1)[0]) << (22 - f_pos)\n                        f_pos += 8\n\n                    # Now update buffers from is_literal\n                    f_buf = (f_buf << 1) & full_mask\n\n                    c = f_buf >> (30 - literal_bits)\n                    f_buf = (f_buf << literal_bits) & full_mask\n                    f_pos -= literal_bits + 1\n\n                    out_buf[out_pos] = c\n                    w_buf_ptr[w_pos] = c\n                    out_pos += 1\n                else:\n                    # Read and decode Huffman; we'll need the bits regardless\n                    if f_pos < 9:\n                        f_buf |= int(f.read(1)[0]) << (22 - f_pos)\n                        f_pos += 8\n\n                    if f_buf >> 28:\n                        code = (f_buf >> 21) & 0x7F\n                        code = 33 if code >= 64 else huffman_table[code]\n                        match_size = code & 0xF\n                        if match_size == _FLUSH:\n                            f_buf = f_pos = 0\n                            continue\n                        delta = code >> 4\n                    else:\n                        match_size = 0\n                        delta = 1\n\n                    token_bits = 1 + delta + w_bits\n\n                    while f_pos < token_bits:\n                        f_buf |= int(f.read(1)[0]) << (22 - f_pos)\n                        f_pos += 8\n\n                    # We now absolutely have enough data to decode token.\n\n                    match_size += min_pattern_size\n                    index = (f_buf >> (30 - token_bits)) & w_mask\n                    f_buf = (f_buf << token_bits) & full_mask\n                    f_pos -= token_bits\n\n                    # Copy entire match to overflow\n                    for i in range(match_size):\n                        overflow_buf_ptr[i] = w_buf_ptr[index + i]\n                    overflow_pos = 0\n                    overflow_size = match_size\n\n                    # Copy available amount to out_buf\n                    for i in range(match_size):\n                        if out_pos < out_capacity:\n                            out_buf[out_pos] = overflow_buf_ptr[overflow_pos]\n                            out_pos += 1\n                            overflow_pos += 1\n                            overflow_size -= 1\n                        # Copy overflow to window\n                        w_buf_ptr[(w_pos + i) & w_mask] = overflow_buf_ptr[i]\n                w_pos = (w_pos + match_size) & w_mask\n        except IndexError:\n            pass\n\n        self.w_pos = w_pos\n        self.overflow_pos = overflow_pos\n        self.overflow_size = overflow_size\n        self.f_buf = f_buf\n        self.f_pos = f_pos\n\n        return out_pos\n\n    def read(self, size=-1):\n        \"\"\"Return at most ``size`` bytes.\"\"\"\n        if size == 0:\n            out = bytearray()\n        elif size < 0:\n            # Read into a bunch of chunks, then do a single join.\n            chunks = []\n            chunk_size = 1024\n            decompressed_bytes = 0\n            while True:\n                chunk = bytearray(chunk_size)\n                chunk_decompressed_bytes = self._decompress_into(chunk)\n                if chunk_decompressed_bytes != chunk_size:\n                    chunk = chunk[:chunk_decompressed_bytes]\n                chunks.append(chunk)\n\n                if chunk_decompressed_bytes != chunk_size:\n                    break\n\n            out = b\"\".join(chunks)\n        else:\n            out = bytearray(size)\n            decompressed_bytes = self._decompress_into(out)\n            if decompressed_bytes != size:\n                out = out[:decompressed_bytes]\n\n        return out\n\n    def close(self):\n        if self._close_f_on_close:\n            self.f.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()", "\n\nclass TextDecompressor(Decompressor):\n    def read(self, *args, **kwargs) -> str:\n        return super().read(*args, **kwargs).decode()\n\n\ndef decompress(data, *args, **kwargs):\n    with BytesIO(data) as f:\n        d = Decompressor(f, *args, **kwargs)\n        return d.read()", ""]}
{"filename": "tamp/__main__.py", "chunked_list": ["from .cli.main import run_app\n\nrun_app(prog_name=\"tamp\")\n"]}
{"filename": "tamp/__init__.py", "chunked_list": ["# Don't manually change, let poetry-dynamic-versioning-plugin handle it.\n__version__ = \"0.0.0\"\n\n\nclass ExcessBitsError(Exception):\n    \"\"\"Provided data has more bits than expected ``literal`` bits.\"\"\"\n\n\ndef bit_size(value):\n    for i in range(32):\n        if not value:\n            return i\n        value >>= 1\n    return -1", "def bit_size(value):\n    for i in range(32):\n        if not value:\n            return i\n        value >>= 1\n    return -1\n\n\ndef _xorshift32(seed):\n    while True:\n        seed ^= (seed << 13) & 0xFFFFFFFF\n        seed ^= (seed >> 17) & 0xFFFFFFFF\n        seed ^= (seed << 5) & 0xFFFFFFFF\n        yield seed", "def _xorshift32(seed):\n    while True:\n        seed ^= (seed << 13) & 0xFFFFFFFF\n        seed ^= (seed >> 17) & 0xFFFFFFFF\n        seed ^= (seed << 5) & 0xFFFFFFFF\n        yield seed\n\n\ndef initialize_dictionary(size, seed=None):\n    if seed is None:\n        seed = 3758097560\n    elif seed == 0:\n        return bytearray(size)\n\n    chars = b\" \\x000ei>to<ans\\nr/.\"  # 16 most common chars in dataset\n\n    def _gen_stream(xorshift32):\n        for _ in range(size >> 3):\n            value = next(xorshift32)\n            for _ in range(8):\n                yield chars[value & 0x0F]\n                value >>= 4\n\n    return bytearray(_gen_stream(_xorshift32(seed)))", "def initialize_dictionary(size, seed=None):\n    if seed is None:\n        seed = 3758097560\n    elif seed == 0:\n        return bytearray(size)\n\n    chars = b\" \\x000ei>to<ans\\nr/.\"  # 16 most common chars in dataset\n\n    def _gen_stream(xorshift32):\n        for _ in range(size >> 3):\n            value = next(xorshift32)\n            for _ in range(8):\n                yield chars[value & 0x0F]\n                value >>= 4\n\n    return bytearray(_gen_stream(_xorshift32(seed)))", "\n\ndef compute_min_pattern_size(window, literal):\n    \"\"\"Compute whether the minimum pattern length should be 2 or 3.\"\"\"\n    \"\"\"\n    # Easy to understand version; commented out for smaller optimized version;\n    if window > 15 or window < 8:\n        raise ValueError\n    if literal == 5:\n        return 2 + (window > 10)\n    elif literal == 6:\n        return 2 + (window > 12)\n    elif literal == 7:\n        return 2 + (window > 14)\n    elif literal == 8:\n        return 2\n    else:\n        raise ValueError\n    \"\"\"\n    if not (7 < window < 16 and 4 < literal < 9):\n        raise ValueError\n\n    return 2 + (window > (10 + ((literal - 5) << 1)))", "\n\ntry:\n    from .compressor_viper import Compressor, TextCompressor, compress\nexcept ImportError:\n    try:\n        from ._c_compressor import Compressor, TextCompressor, compress\n    except ImportError:\n        try:\n            from .compressor import Compressor, TextCompressor, compress\n        except ImportError:\n            pass", "\ntry:\n    from .decompressor_viper import Decompressor, TextDecompressor, decompress\nexcept ImportError:\n    try:\n        from ._c_decompressor import Decompressor, TextDecompressor, decompress\n    except ImportError:\n        try:\n            from .decompressor import Decompressor, TextDecompressor, decompress\n        except ImportError:\n            pass", "\n\ndef open(f, mode=\"rb\", **kwargs):\n    if \"r\" in mode and \"w\" in mode:\n        raise ValueError\n\n    if \"r\" in mode:  # Decompressor\n        if \"b\" in mode:\n            return Decompressor(f, **kwargs)\n        else:\n            return TextDecompressor(f, **kwargs)\n    elif \"w\" in mode:  # Compressor\n        if \"b\" in mode:\n            return Compressor(f, **kwargs)\n        else:\n            return TextCompressor(f, **kwargs)\n    else:\n        raise ValueError", ""]}
{"filename": "tamp/decompressor.py", "chunked_list": ["from io import BytesIO\n\nfrom . import compute_min_pattern_size, initialize_dictionary\n\n_FLUSH = object()\n\n# Each key here are the huffman codes or'd with 0x80\n# This is so that each lookup is easy/quick.\nhuffman_lookup = {\n    0b0: 0,", "huffman_lookup = {\n    0b0: 0,\n    0b11: 1,\n    0b1000: 2,\n    0b1011: 3,\n    0b10100: 4,\n    0b100100: 5,\n    0b100110: 6,\n    0b101011: 7,\n    0b1001011: 8,", "    0b101011: 7,\n    0b1001011: 8,\n    0b1010100: 9,\n    0b10010100: 10,\n    0b10010101: 11,\n    0b10101010: 12,\n    0b100111: 13,\n    0b10101011: _FLUSH,\n}\n", "}\n\n\nclass BitReader:\n    \"\"\"Reads bits from a stream.\"\"\"\n\n    def __init__(self, f, close_f_on_close=False):\n        self.close_f_on_close = close_f_on_close\n        self.f = f\n        self.clear()\n\n    def read_huffman(self):\n        proposed_code = 0\n        lookup = huffman_lookup\n        read = self.read\n        for _ in range(8):\n            proposed_code |= read(1)\n            try:\n                return lookup[proposed_code]\n            except KeyError:\n                proposed_code <<= 1\n        raise RuntimeError(\"Unable to decode huffman code. Should never happen.\")\n\n    def read(self, num_bits):\n        while self.bit_pos < num_bits:\n            byte = self.f.read(1)\n            if not byte:\n                raise EOFError\n            byte_value = int.from_bytes(byte, \"little\")\n            self.buffer |= byte_value << (24 - self.bit_pos)\n            self.bit_pos += 8\n\n            if self.backup_buffer is not None and self.backup_bit_pos is not None:\n                self.backup_buffer |= byte_value << (24 - self.backup_bit_pos)\n                self.backup_bit_pos += 8\n\n        result = self.buffer >> (32 - num_bits)\n        mask = (1 << (32 - num_bits)) - 1\n        self.buffer = (self.buffer & mask) << num_bits\n        self.bit_pos -= num_bits\n\n        return result\n\n    def clear(self):\n        self.buffer = 0\n        self.bit_pos = 0\n\n        self.backup_buffer = None\n        self.backup_bit_pos = None\n\n    def close(self):\n        self.f.close()\n        if self.close_f_on_close:\n            self.f.close()\n\n    def __len__(self):\n        return self.bit_pos\n\n    def __enter__(self):\n        # Context manager to restore all read bits in a session if any read fails.\n\n        # backup the buffer\n        self.backup_buffer = self.buffer\n        self.backup_bit_pos = self.bit_pos\n        return self\n\n    def __exit__(self, exception_type, exception_value, exception_traceback):\n        if exception_type is not None:\n            # restore buffers\n            self.buffer = self.backup_buffer\n            self.bit_pos = self.backup_bit_pos\n\n        self.backup_buffer = None\n        self.backup_bit_pos = None", "\n\nclass RingBuffer:\n    def __init__(self, buffer):\n        self.buffer = buffer\n        self.size = len(buffer)\n        self.pos = 0  # Always pointing to the byte-to-be-overwritten\n        self.index = self.buffer.index\n\n    def write_byte(self, byte):  # ~10% of time\n        self.buffer[self.pos] = byte\n        self.pos += 1\n        if self.pos == self.size:\n            self.pos = 0\n\n    def write_bytes(self, data):\n        for byte in data:\n            self.write_byte(byte)", "\n\nclass Decompressor:\n    \"\"\"Decompresses a file or stream of tamp-compressed data.\n\n    Can be used as a context manager to automatically handle file\n    opening and closing::\n\n        with tamp.Decompressor(\"compressed.tamp\") as f:\n            decompressed_data = f.read()\n    \"\"\"\n\n    def __init__(self, f, *, dictionary=None):\n        \"\"\"\n        Parameters\n        ----------\n        f: Union[file, str]\n            File-like object to read compressed bytes from.\n        dictionary: bytearray\n            Use dictionary inplace as window buffer.\n        \"\"\"\n        if not hasattr(f, \"read\"):  # It's probably a path-like object.\n            f = open(str(f), \"rb\")\n            close_f_on_close = True\n        else:\n            close_f_on_close = False\n\n        self._bit_reader = BitReader(f, close_f_on_close=close_f_on_close)\n\n        # Read Header\n        self.window_bits = self._bit_reader.read(3) + 8\n        self.literal_bits = self._bit_reader.read(2) + 5\n        uses_custom_dictionary = self._bit_reader.read(1)\n        reserved = self._bit_reader.read(1)\n        more_header_bytes = self._bit_reader.read(1)\n\n        if reserved:\n            raise NotImplementedError\n\n        if more_header_bytes:\n            raise NotImplementedError\n\n        if uses_custom_dictionary ^ bool(dictionary):\n            raise ValueError\n\n        self._window_buffer = RingBuffer(\n            buffer=dictionary if dictionary else initialize_dictionary(1 << self.window_bits),\n        )\n\n        self.min_pattern_size = compute_min_pattern_size(self.window_bits, self.literal_bits)\n\n        self.overflow = bytearray()\n\n    def read(self, size=-1) -> bytearray:\n        \"\"\"Decompresses data to bytes.\n\n        Parameters\n        ----------\n        size: int\n            Maximum number of bytes to return.\n            If a negative value is provided, all data will be returned.\n            Defaults to ``-1``.\n\n        Returns\n        -------\n        bytearray\n            Decompressed data.\n        \"\"\"\n        if size < 0:\n            size = 0xFFFFFFFF\n\n        if len(self.overflow) > size:\n            out = self.overflow[:size]\n            self.overflow = self.overflow[size:]\n            return out\n        elif self.overflow:\n            out = self.overflow\n            self.overflow = bytearray()\n        else:\n            out = bytearray()\n\n        while len(out) < size:\n            try:\n                with self._bit_reader:\n                    is_literal = self._bit_reader.read(1)\n\n                    if is_literal:\n                        c = self._bit_reader.read(self.literal_bits)\n                        self._window_buffer.write_byte(c)\n                        out.append(c)\n                    else:\n                        match_size = self._bit_reader.read_huffman()\n                        if match_size is _FLUSH:\n                            self._bit_reader.clear()\n                            continue\n                        match_size += self.min_pattern_size\n                        index = self._bit_reader.read(self.window_bits)\n\n                        string = self._window_buffer.buffer[index : index + match_size]\n                        self._window_buffer.write_bytes(string)\n\n                        out.extend(string)\n                        if len(out) > size:\n                            self.overflow[:] = out[size:]\n                            out = out[:size]\n                            break\n            except EOFError:\n                break\n\n        return out\n\n    def close(self):\n        \"\"\"Closes the input file or stream.\"\"\"\n        self._bit_reader.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()", "\n\nclass TextDecompressor(Decompressor):\n    \"\"\"Decompresses a file or stream of tamp-compressed data into text.\"\"\"\n\n    def read(self, *args, **kwargs) -> str:\n        \"\"\"Decompresses data to text.\n\n        Parameters\n        ----------\n        size: int\n            Maximum number of bytes to return.\n            If a negative value is provided, all data will be returned.\n            Defaults to ``-1``.\n\n        Returns\n        -------\n        str\n            Decompressed text.\n        \"\"\"\n        return super().read(*args, **kwargs).decode()", "\n\ndef decompress(data: bytes, *args, **kwargs) -> bytearray:\n    \"\"\"Single-call to decompress data.\n\n    Parameters\n    ----------\n    data: bytes\n        Plaintext data to compress.\n    *args: tuple\n        Passed along to :class:`Decompressor`.\n    **kwargs : dict\n        Passed along to :class:`Decompressor`.\n\n    Returns\n    -------\n    bytearray\n        Decompressed data.\n    \"\"\"\n    with BytesIO(data) as f:\n        d = Decompressor(f, *args, **kwargs)\n        return d.read()", ""]}
{"filename": "tamp/compressor.py", "chunked_list": ["from collections import deque\nfrom io import BytesIO\n\ntry:\n    from typing import Union\nexcept ImportError:\n    pass\n\nfrom . import ExcessBitsError, bit_size, compute_min_pattern_size, initialize_dictionary\n\ntry:\n    from micropython import const\nexcept ImportError:\n\n    def const(x):\n        return x  # noqa: E721", "from . import ExcessBitsError, bit_size, compute_min_pattern_size, initialize_dictionary\n\ntry:\n    from micropython import const\nexcept ImportError:\n\n    def const(x):\n        return x  # noqa: E721\n\n", "\n\n# encodes [min_pattern_bytes, min_pattern_bytes + 13] pattern lengths\nhuffman_codes = b\"\\x00\\x03\\x08\\x0b\\x14$&+KT\\x94\\x95\\xaa'\"\n# These bit lengths pre-add the 1 bit for the 0-value is_literal flag.\nhuffman_bits = b\"\\x02\\x03\\x05\\x05\\x06\\x07\\x07\\x07\\x08\\x08\\x09\\x09\\x09\\x07\"\nFLUSH_CODE = const(0xAB)  # 8 bits\n\n\nclass BitWriter:\n    \"\"\"Writes bits to a stream.\"\"\"\n\n    def __init__(self, f, close_f_on_close=False):\n        self.close_f_on_close = close_f_on_close\n        self.f = f\n        self.buffer = 0  # Basically a uint24\n        self.bit_pos = 0\n\n    def write_huffman(self, pattern_size):\n        return self.write(huffman_codes[pattern_size], huffman_bits[pattern_size])\n\n    def write(self, bits, num_bits, flush=True):\n        bits &= (1 << num_bits) - 1\n        self.bit_pos += num_bits\n        self.buffer |= bits << (32 - self.bit_pos)\n\n        bytes_written = 0\n        if flush:\n            while self.bit_pos >= 8:\n                byte = self.buffer >> 24\n                self.f.write(byte.to_bytes(1, \"big\"))\n                self.buffer = (self.buffer & 0xFFFFFF) << 8\n                self.bit_pos -= 8\n                bytes_written += 1\n        return bytes_written\n\n    def flush(self, write_token=True):\n        bytes_written = 0\n        if self.bit_pos > 0 and write_token:\n            bytes_written += self.write(FLUSH_CODE, 9)\n\n        while self.bit_pos > 0:\n            byte = (self.buffer >> 24) & 0xFF\n            self.f.write(byte.to_bytes(1, \"big\"))\n            self.bit_pos = 0\n            self.buffer = 0\n            bytes_written += 1\n\n        self.f.flush()\n\n        return bytes_written\n\n    def close(self):\n        self.flush(write_token=False)\n        if self.close_f_on_close:\n            self.f.close()", "\nclass BitWriter:\n    \"\"\"Writes bits to a stream.\"\"\"\n\n    def __init__(self, f, close_f_on_close=False):\n        self.close_f_on_close = close_f_on_close\n        self.f = f\n        self.buffer = 0  # Basically a uint24\n        self.bit_pos = 0\n\n    def write_huffman(self, pattern_size):\n        return self.write(huffman_codes[pattern_size], huffman_bits[pattern_size])\n\n    def write(self, bits, num_bits, flush=True):\n        bits &= (1 << num_bits) - 1\n        self.bit_pos += num_bits\n        self.buffer |= bits << (32 - self.bit_pos)\n\n        bytes_written = 0\n        if flush:\n            while self.bit_pos >= 8:\n                byte = self.buffer >> 24\n                self.f.write(byte.to_bytes(1, \"big\"))\n                self.buffer = (self.buffer & 0xFFFFFF) << 8\n                self.bit_pos -= 8\n                bytes_written += 1\n        return bytes_written\n\n    def flush(self, write_token=True):\n        bytes_written = 0\n        if self.bit_pos > 0 and write_token:\n            bytes_written += self.write(FLUSH_CODE, 9)\n\n        while self.bit_pos > 0:\n            byte = (self.buffer >> 24) & 0xFF\n            self.f.write(byte.to_bytes(1, \"big\"))\n            self.bit_pos = 0\n            self.buffer = 0\n            bytes_written += 1\n\n        self.f.flush()\n\n        return bytes_written\n\n    def close(self):\n        self.flush(write_token=False)\n        if self.close_f_on_close:\n            self.f.close()", "\n\nclass RingBuffer:\n    def __init__(self, buffer):\n        self.buffer = buffer\n        self.size = len(buffer)\n        self.pos = 0  # Always pointing to the byte-to-be-overwritten\n\n    def write_byte(self, byte):  # ~10% of time\n        self.buffer[self.pos] = byte\n        self.pos = (self.pos + 1) % self.size\n\n    def write_bytes(self, data):\n        for byte in data:\n            self.write_byte(byte)\n\n    def index(self, pattern, start):\n        return self.buffer.index(pattern, start)", "\n\nclass Compressor:\n    \"\"\"Compresses data to a file or stream.\"\"\"\n\n    def __init__(\n        self,\n        f,\n        *,\n        window=10,\n        literal=8,\n        dictionary=None,\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        window: int\n            Size of window buffer in bits.\n            Defaults to 10 (1024 byte buffer).\n        literal: int\n            Size of literals in bits.\n            Defaults to 8.\n        dictionary: Optional[bytearray]\n            Use the given initialized buffer inplace.\n            At decompression time, the same buffer must be provided.\n            ``window`` must agree with the dictionary size.\n        \"\"\"\n        if not hasattr(f, \"write\"):  # It's probably a path-like object.\n            # TODO: then close it on close\n            f = open(str(f), \"wb\")\n            close_f_on_close = True\n        else:\n            close_f_on_close = False\n\n        self._bit_writer = BitWriter(f, close_f_on_close=close_f_on_close)\n        if dictionary and bit_size(len(dictionary) - 1) != window:\n            raise ValueError(\"Dictionary-window size mismatch.\")\n\n        self.window_bits = window\n        self.literal_bits = literal\n\n        self.min_pattern_size = compute_min_pattern_size(window, literal)\n        self.max_pattern_size = self.min_pattern_size + 13\n\n        self.literal_flag = 1 << self.literal_bits\n\n        self._window_buffer = RingBuffer(\n            buffer=dictionary if dictionary else initialize_dictionary(1 << window),\n        )\n\n        self._input_buffer = deque(maxlen=self.max_pattern_size)\n\n        # Callbacks for debugging/metric collection; can be externally set.\n        self.token_cb = None\n        self.literal_cb = None\n        self.flush_cb = None\n\n        # Write header\n        self._bit_writer.write(window - 8, 3, flush=False)\n        self._bit_writer.write(literal - 5, 2, flush=False)\n        self._bit_writer.write(bool(dictionary), 1, flush=False)\n        self._bit_writer.write(0, 1, flush=False)  # Reserved\n        self._bit_writer.write(0, 1, flush=False)  # No other header bytes\n\n    def _compress_input_buffer_single(self) -> int:\n        target = bytes(self._input_buffer)\n        bytes_written = 0\n        search_i = 0\n        match_size = 1\n        for match_size in range(self.min_pattern_size, len(target) + 1):\n            match = target[:match_size]\n            try:\n                search_i = self._window_buffer.index(match, search_i)\n            except ValueError:\n                # Not Found\n                match_size -= 1\n                break\n        match = target[:match_size]\n\n        if match_size >= self.min_pattern_size:\n            if self.token_cb:\n                self.token_cb(\n                    search_i,\n                    match_size,\n                    match,\n                )\n            bytes_written += self._bit_writer.write_huffman(match_size - self.min_pattern_size)\n            bytes_written += self._bit_writer.write(search_i, self.window_bits)\n            self._window_buffer.write_bytes(match)\n\n            for _ in range(match_size):\n                self._input_buffer.popleft()\n        else:\n            char = self._input_buffer.popleft()\n            if self.literal_cb:\n                self.literal_cb(char)\n            if char >> self.literal_bits:\n                raise ExcessBitsError\n\n            bytes_written += self._bit_writer.write(char | self.literal_flag, self.literal_bits + 1)\n            self._window_buffer.write_byte(char)\n\n        return bytes_written\n\n    def write(self, data: bytes) -> int:\n        \"\"\"Compress ``data`` to stream.\n\n        Parameters\n        ----------\n        data: bytes\n            Data to be compressed.\n\n        Returns\n        -------\n        int\n            Number of compressed bytes written.\n            May be zero when data is filling up internal buffers.\n        \"\"\"\n        bytes_written = 0\n\n        for char in data:\n            self._input_buffer.append(char)\n            if len(self._input_buffer) == self._input_buffer.maxlen:\n                bytes_written += self._compress_input_buffer_single()\n\n        return bytes_written\n\n    def flush(self, write_token: bool = True) -> int:\n        \"\"\"Flushes internal buffers.\n\n        Parameters\n        ----------\n        write_token: bool\n            If appropriate, write a ``FLUSH`` token.\n            Defaults to ``True``.\n\n        Returns\n        -------\n        int\n            Number of compressed bytes flushed.\n        \"\"\"\n        bytes_written = 0\n        if self.flush_cb:\n            self.flush_cb()\n        while self._input_buffer:\n            bytes_written += self._compress_input_buffer_single()\n        bytes_written += self._bit_writer.flush(write_token=write_token)\n        return bytes_written\n\n    def close(self) -> int:\n        \"\"\"Flushes internal buffers and close the output file or stream.\n\n        Returns\n        -------\n        int\n            Number of compressed bytes flushed.\n        \"\"\"\n        bytes_written = 0\n        bytes_written += self.flush(write_token=False)\n        self._bit_writer.close()\n        return bytes_written\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()", "\n\nclass TextCompressor(Compressor):\n    \"\"\"Compresses text to a file or stream.\"\"\"\n\n    def write(self, data: str) -> int:\n        return super().write(data.encode())\n\n\ndef compress(data: Union[bytes, str], *args, **kwargs) -> bytes:\n    \"\"\"Single-call to compress data.\n\n    Parameters\n    ----------\n    data: Union[str, bytes]\n        Data to compress.\n    *args: tuple\n        Passed along to :class:`Compressor`.\n    **kwargs : dict\n        Passed along to :class:`Compressor`.\n\n    Returns\n    -------\n    bytes\n        Compressed data\n    \"\"\"\n    with BytesIO() as f:\n        if isinstance(data, str):\n            c = TextCompressor(f, *args, **kwargs)\n            c.write(data)\n        else:\n            c = Compressor(f, *args, **kwargs)\n            c.write(data)\n        c.flush(write_token=False)\n        f.seek(0)\n        return f.read()", "\ndef compress(data: Union[bytes, str], *args, **kwargs) -> bytes:\n    \"\"\"Single-call to compress data.\n\n    Parameters\n    ----------\n    data: Union[str, bytes]\n        Data to compress.\n    *args: tuple\n        Passed along to :class:`Compressor`.\n    **kwargs : dict\n        Passed along to :class:`Compressor`.\n\n    Returns\n    -------\n    bytes\n        Compressed data\n    \"\"\"\n    with BytesIO() as f:\n        if isinstance(data, str):\n            c = TextCompressor(f, *args, **kwargs)\n            c.write(data)\n        else:\n            c = Compressor(f, *args, **kwargs)\n            c.write(data)\n        c.flush(write_token=False)\n        f.seek(0)\n        return f.read()", ""]}
{"filename": "tamp/cli/main.py", "chunked_list": ["import sys\nfrom pathlib import Path\nfrom typing import Callable, Optional\n\nimport typer\nfrom typer import Argument, Option\n\nimport tamp\n\napp = typer.Typer(no_args_is_help=True, pretty_exceptions_enable=False, add_completion=False)", "\napp = typer.Typer(no_args_is_help=True, pretty_exceptions_enable=False, add_completion=False)\n\n\ndef version_callback(value: bool):\n    if not value:\n        return\n    print(tamp.__version__)\n    raise typer.Exit()\n", "\n\n@app.callback()\ndef common(\n    version: bool = Option(\n        None,\n        \"--version\",\n        \"-v\",\n        callback=version_callback,\n        help=\"Print tamp version.\",\n    ),\n):\n    pass", "\n\ndef read(input_: Optional[Path]) -> bytes:\n    data = sys.stdin.buffer.read() if input_ is None else input_.read_bytes()\n    if not data:\n        raise ValueError(\"No data provided.\")\n    return data\n\n\ndef write(output: Optional[Path], data: bytes):\n    if output is None:\n        sys.stdout.buffer.write(data)\n    else:\n        output.write_bytes(data)", "\ndef write(output: Optional[Path], data: bytes):\n    if output is None:\n        sys.stdout.buffer.write(data)\n    else:\n        output.write_bytes(data)\n\n\n@app.command()\ndef compress(\n    input_path: Optional[Path] = Argument(\n        None,\n        exists=True,\n        readable=True,\n        dir_okay=False,\n        show_default=False,\n        help=\"Input file to compress or decompress. Defaults to stdin.\",\n    ),\n    output_path: Optional[Path] = Option(\n        None,\n        \"--output\",\n        \"-o\",\n        exists=False,\n        writable=True,\n        dir_okay=True,\n        show_default=False,\n        help=\"Output file. Defaults to stdout.\",\n    ),\n    window: int = Option(\n        10,\n        \"-w\",\n        \"--window\",\n        min=8,\n        max=15,\n        help=\"Number of bits used to represent the dictionary window.\",\n    ),\n    literal: int = Option(\n        8,\n        \"-l\",\n        \"--literal\",\n        min=5,\n        max=8,\n        help=\"Number of bits used to represent a literal.\",\n    ),\n):\n    \"\"\"Compress an input file or stream.\"\"\"\n    input_bytes = read(input_path)\n    output_bytes = tamp.compress(\n        input_bytes,\n        window=window,\n        literal=literal,\n    )\n    write(output_path, output_bytes)", "@app.command()\ndef compress(\n    input_path: Optional[Path] = Argument(\n        None,\n        exists=True,\n        readable=True,\n        dir_okay=False,\n        show_default=False,\n        help=\"Input file to compress or decompress. Defaults to stdin.\",\n    ),\n    output_path: Optional[Path] = Option(\n        None,\n        \"--output\",\n        \"-o\",\n        exists=False,\n        writable=True,\n        dir_okay=True,\n        show_default=False,\n        help=\"Output file. Defaults to stdout.\",\n    ),\n    window: int = Option(\n        10,\n        \"-w\",\n        \"--window\",\n        min=8,\n        max=15,\n        help=\"Number of bits used to represent the dictionary window.\",\n    ),\n    literal: int = Option(\n        8,\n        \"-l\",\n        \"--literal\",\n        min=5,\n        max=8,\n        help=\"Number of bits used to represent a literal.\",\n    ),\n):\n    \"\"\"Compress an input file or stream.\"\"\"\n    input_bytes = read(input_path)\n    output_bytes = tamp.compress(\n        input_bytes,\n        window=window,\n        literal=literal,\n    )\n    write(output_path, output_bytes)", "\n\n@app.command()\ndef decompress(\n    input_path: Optional[Path] = Argument(\n        None,\n        exists=True,\n        readable=True,\n        dir_okay=False,\n        show_default=False,\n        help=\"Input file. If not provided, reads from stdin.\",\n    ),\n    output_path: Optional[Path] = Option(\n        None,\n        \"--output\",\n        \"-o\",\n        exists=False,\n        writable=True,\n        dir_okay=True,\n        show_default=False,\n        help=\"Output file. Defaults to stdout.\",\n    ),\n):\n    \"\"\"Decompress an input file or stream.\"\"\"\n    input_bytes = read(input_path)\n    output_bytes = tamp.decompress(input_bytes)\n    write(output_path, output_bytes)", "\n\ndef run_app(*args, **kwargs):\n    app(*args, **kwargs)\n"]}
{"filename": "tamp/cli/__init__.py", "chunked_list": [""]}
{"filename": "tools/on-device-compression-benchmark.py", "chunked_list": ["\"\"\"Micropython code to be ran on-device.\n\"\"\"\nimport uprofiler\n\nuprofiler.print_period = 0\n\nimport tamp\n\n\n@uprofiler.profile\ndef main():\n    block_size = 1024\n    decompressed_len = 0\n    compressed_len = 0\n\n    with open(\"enwik8-100kb\", \"rb\") as input_file, tamp.open(\"enwik8-100kb.tamp\", \"wb\") as compressed_f:\n        while chunk := input_file.read(block_size):\n            decompressed_len += len(chunk)\n            compressed_len += compressed_f.write(chunk)\n        compressed_len += compressed_f.flush(write_token=False)\n    print(f\"{decompressed_len=:,}\")\n    print(f\"{compressed_len=:,}\")", "\n@uprofiler.profile\ndef main():\n    block_size = 1024\n    decompressed_len = 0\n    compressed_len = 0\n\n    with open(\"enwik8-100kb\", \"rb\") as input_file, tamp.open(\"enwik8-100kb.tamp\", \"wb\") as compressed_f:\n        while chunk := input_file.read(block_size):\n            decompressed_len += len(chunk)\n            compressed_len += compressed_f.write(chunk)\n        compressed_len += compressed_f.flush(write_token=False)\n    print(f\"{decompressed_len=:,}\")\n    print(f\"{compressed_len=:,}\")", "\n\nif __name__ == \"__main__\":\n    main()\n    uprofiler.print_results()\n"]}
{"filename": "tools/analysis.py", "chunked_list": ["import argparse\nimport pickle\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom huffman import huffman_coding\n\n\ndef _generate_ticks(data, num_ticks=20):\n    step_size = (len(data) - 1) / (num_ticks - 1)\n    xticks_positions = [round(i * step_size) for i in range(num_ticks)]\n    xticks_labels = [str(int(pos)) for pos in xticks_positions]\n    return (xticks_positions, xticks_labels)", "\ndef _generate_ticks(data, num_ticks=20):\n    step_size = (len(data) - 1) / (num_ticks - 1)\n    xticks_positions = [round(i * step_size) for i in range(num_ticks)]\n    xticks_labels = [str(int(pos)) for pos in xticks_positions]\n    return (xticks_positions, xticks_labels)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"window_bits\", type=int)\n    parser.add_argument(\"--plot\", action=\"store_true\")\n    parser.add_argument(\"--pdb\", action=\"store_true\")\n    args = parser.parse_args()\n\n    with Path(f\"build/results-w{args.window_bits}.pkl\").open(\"rb\") as f:\n        r = pickle.load(f)  # noqa: S301\n        for k, v in r.items():\n            if isinstance(v, list):\n                r[k] = np.array(v)\n\n    r[\"token_sizes\"] = r[\"token_sizes\"] / r[\"token_sizes\"].sum()\n    r[\"token_distances\"] = r[\"token_distances\"] / r[\"token_distances\"].sum()\n\n    for k, v in r.items():\n        if isinstance(v, np.ndarray):\n            continue\n        print(f\"{k}: {v:,}\")\n\n    literal_prob = r[\"n_literals\"] / (r[\"n_literals\"] + r[\"n_tokens\"])\n    print(f\"{literal_prob=}\")\n\n    min_pattern_size = r[\"token_sizes\"].nonzero()[0].min()\n    max_pattern_size = r[\"token_sizes\"].nonzero()[0].max()\n\n    probs = {i: r[\"token_sizes\"][i] for i in range(min_pattern_size, max_pattern_size + 1)}\n\n    huffman_codes = huffman_coding(probs)\n\n    bits_per_symbols = {}\n    shortest_symbol_size = 100\n    for k, v in sorted(huffman_codes.items()):\n        symbol_size = len(v)\n        bits_per_symbols[k] = symbol_size\n        if symbol_size < shortest_symbol_size:\n            shortest_symbol_size = symbol_size\n\n    # if huffman_codes[shortest_symbol] == \"1\":\n    #    # Invert all codes;\n    #    for symbol, code in huffman_codes.items():\n    #        huffman_codes[symbol] = code.replace(\"0\",\"Z\").replace(\"1\", \"0\").replace(\"Z\", \"1\")\n\n    average_bits_per_symbol = 0\n    for code in bits_per_symbols:\n        average_bits_per_symbol += bits_per_symbols[code] * probs[code]\n\n    print(f\"Huffman pattern size code: {average_bits_per_symbol=}\")\n\n    print(\"Huffman codes:\")\n    huffman_code_array = b\"\"\n    huffman_bit_size_array = []\n    for char, code in sorted(huffman_codes.items()):\n        print(f\"{char}: {code}\")\n        huffman_code_array += int(code, 2).to_bytes(1, \"little\")\n        huffman_bit_size_array.append(len(code) + 1)  # plus 1 for implicit is_literal flag.\n    huffman_bit_size_array = tuple(huffman_bit_size_array)\n    print(f\"{huffman_bit_size_array=}\")\n    print(f\"{huffman_code_array=}\")\n\n    offset_weighted_average = np.dot(r[\"token_distances\"], np.arange(len(r[\"token_distances\"])))\n    print(f\"50% of offsets are under {offset_weighted_average}.\")\n\n    if args.pdb:\n        breakpoint()\n\n    if args.plot:\n        plt.rc(\"font\", size=30)\n\n        plt.subplot(1, 2, 1)\n        data = r[\"token_distances\"]\n        indices = range(len(data))\n        plt.bar(indices, data, width=1)\n        plt.xticks(*_generate_ticks(data))\n        plt.xlabel(\"Offset\")\n        plt.ylabel(\"Occurrence\")\n        plt.title(f\"Token Distances (w={args.window_bits})\")\n\n        plt.subplot(1, 2, 2)\n        indices = np.array(sorted(huffman_codes.keys()))\n        data = [probs[x] for x in indices]\n        plt.bar(indices, data, width=1, align=\"center\")\n        plt.yticks(np.arange(0, 0.55, 0.05))\n        plt.xticks(\n            indices,\n            indices,\n        )\n        plt.xlim(0, max(indices))\n        plt.xlabel(\"Match Size\")\n        plt.ylabel(\"Occurrence\")\n        plt.title(f\"Match Sizes (enwik8, w={args.window_bits})\")\n\n        plt.grid(True)\n\n        plt.show()", "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"window_bits\", type=int)\n    parser.add_argument(\"--plot\", action=\"store_true\")\n    parser.add_argument(\"--pdb\", action=\"store_true\")\n    args = parser.parse_args()\n\n    with Path(f\"build/results-w{args.window_bits}.pkl\").open(\"rb\") as f:\n        r = pickle.load(f)  # noqa: S301\n        for k, v in r.items():\n            if isinstance(v, list):\n                r[k] = np.array(v)\n\n    r[\"token_sizes\"] = r[\"token_sizes\"] / r[\"token_sizes\"].sum()\n    r[\"token_distances\"] = r[\"token_distances\"] / r[\"token_distances\"].sum()\n\n    for k, v in r.items():\n        if isinstance(v, np.ndarray):\n            continue\n        print(f\"{k}: {v:,}\")\n\n    literal_prob = r[\"n_literals\"] / (r[\"n_literals\"] + r[\"n_tokens\"])\n    print(f\"{literal_prob=}\")\n\n    min_pattern_size = r[\"token_sizes\"].nonzero()[0].min()\n    max_pattern_size = r[\"token_sizes\"].nonzero()[0].max()\n\n    probs = {i: r[\"token_sizes\"][i] for i in range(min_pattern_size, max_pattern_size + 1)}\n\n    huffman_codes = huffman_coding(probs)\n\n    bits_per_symbols = {}\n    shortest_symbol_size = 100\n    for k, v in sorted(huffman_codes.items()):\n        symbol_size = len(v)\n        bits_per_symbols[k] = symbol_size\n        if symbol_size < shortest_symbol_size:\n            shortest_symbol_size = symbol_size\n\n    # if huffman_codes[shortest_symbol] == \"1\":\n    #    # Invert all codes;\n    #    for symbol, code in huffman_codes.items():\n    #        huffman_codes[symbol] = code.replace(\"0\",\"Z\").replace(\"1\", \"0\").replace(\"Z\", \"1\")\n\n    average_bits_per_symbol = 0\n    for code in bits_per_symbols:\n        average_bits_per_symbol += bits_per_symbols[code] * probs[code]\n\n    print(f\"Huffman pattern size code: {average_bits_per_symbol=}\")\n\n    print(\"Huffman codes:\")\n    huffman_code_array = b\"\"\n    huffman_bit_size_array = []\n    for char, code in sorted(huffman_codes.items()):\n        print(f\"{char}: {code}\")\n        huffman_code_array += int(code, 2).to_bytes(1, \"little\")\n        huffman_bit_size_array.append(len(code) + 1)  # plus 1 for implicit is_literal flag.\n    huffman_bit_size_array = tuple(huffman_bit_size_array)\n    print(f\"{huffman_bit_size_array=}\")\n    print(f\"{huffman_code_array=}\")\n\n    offset_weighted_average = np.dot(r[\"token_distances\"], np.arange(len(r[\"token_distances\"])))\n    print(f\"50% of offsets are under {offset_weighted_average}.\")\n\n    if args.pdb:\n        breakpoint()\n\n    if args.plot:\n        plt.rc(\"font\", size=30)\n\n        plt.subplot(1, 2, 1)\n        data = r[\"token_distances\"]\n        indices = range(len(data))\n        plt.bar(indices, data, width=1)\n        plt.xticks(*_generate_ticks(data))\n        plt.xlabel(\"Offset\")\n        plt.ylabel(\"Occurrence\")\n        plt.title(f\"Token Distances (w={args.window_bits})\")\n\n        plt.subplot(1, 2, 2)\n        indices = np.array(sorted(huffman_codes.keys()))\n        data = [probs[x] for x in indices]\n        plt.bar(indices, data, width=1, align=\"center\")\n        plt.yticks(np.arange(0, 0.55, 0.05))\n        plt.xticks(\n            indices,\n            indices,\n        )\n        plt.xlim(0, max(indices))\n        plt.xlabel(\"Match Size\")\n        plt.ylabel(\"Occurrence\")\n        plt.title(f\"Match Sizes (enwik8, w={args.window_bits})\")\n\n        plt.grid(True)\n\n        plt.show()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "tools/on-device-decompression-benchmark.py", "chunked_list": ["\"\"\"Micropython code to be ran on-device.\n\"\"\"\nimport uprofiler\n\nuprofiler.print_period = 0\n\nimport tamp\n\n\n@uprofiler.profile\ndef main():\n    block_size = 1024\n    decompressed_len = 0\n\n    with tamp.open(\"enwik8-100kb.tamp\", \"rb\") as input_file, open(\"enwik8-100kb-decompressed\", \"wb\") as output_file:\n        while True:\n            chunk = input_file.read(block_size)\n            decompressed_len += len(chunk)\n            output_file.write(chunk)\n            if len(chunk) < block_size:\n                break\n    print(f\"{decompressed_len=:,}\")", "\n@uprofiler.profile\ndef main():\n    block_size = 1024\n    decompressed_len = 0\n\n    with tamp.open(\"enwik8-100kb.tamp\", \"rb\") as input_file, open(\"enwik8-100kb-decompressed\", \"wb\") as output_file:\n        while True:\n            chunk = input_file.read(block_size)\n            decompressed_len += len(chunk)\n            output_file.write(chunk)\n            if len(chunk) < block_size:\n                break\n    print(f\"{decompressed_len=:,}\")", "\n\nif __name__ == \"__main__\":\n    main()\n    uprofiler.print_results()\n"]}
{"filename": "tools/zlib_decompress.py", "chunked_list": ["import sys\nimport zlib\n\n\ndef main():\n    compressed_data = sys.stdin.buffer.read()\n    zlib.decompress(compressed_data)\n\n\nif __name__ == \"__main__\":\n    main()", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "tools/zlib_compress.py", "chunked_list": ["import argparse\nimport sys\nimport zlib\nfrom pathlib import Path\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"input\", type=Path)\n    args = parser.parse_args()\n    with args.input.open(\"rb\") as f:\n        decompressed_data = f.read()\n        compressobj = zlib.compressobj(level=9, wbits=10, memLevel=1, strategy=zlib.Z_DEFAULT_STRATEGY)\n        compressed_data = compressobj.compress(decompressed_data)\n        compressed_data += compressobj.flush()\n    sys.stdout.buffer.write(compressed_data)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "tools/collect-data.py", "chunked_list": ["import argparse\nimport pickle\nimport time\nfrom io import BytesIO\nfrom pathlib import Path\n\nfrom tamp.compressor import Compressor\n\n\ndef timeit(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(f\"Function {func.__name__} took {end - start:.5f} seconds to execute.\")\n        return result\n\n    return wrapper", "\ndef timeit(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(f\"Function {func.__name__} took {end - start:.5f} seconds to execute.\")\n        return result\n\n    return wrapper", "\n\n@timeit\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"window_bits\", type=int)\n    args = parser.parse_args()\n\n    window_size = 1 << args.window_bits\n\n    decompressed = Path(\"build/enwik8\").read_bytes()\n\n    results = {\n        \"n_literals\": 0,\n        \"n_tokens\": 0,\n        \"n_flushes\": 0,\n        \"token_distances\": [0] * window_size,\n        \"token_sizes\": [0] * 20,\n        \"decompressed_size\": 0,\n        \"compressed_size\": 0,\n        \"ratio\": 0,\n    }\n\n    def token_cb(offset, match_size, string):\n        results[\"n_tokens\"] += 1\n        results[\"token_distances\"][offset] += 1\n        results[\"token_sizes\"][match_size] += 1\n\n    def literal_cb(char):\n        results[\"n_literals\"] += 1\n\n    def flush_cb():\n        results[\"n_flushes\"] += 1\n\n    with BytesIO() as compressed_out:\n        compressor = Compressor(\n            compressed_out,\n            window=args.window_bits,\n        )\n        compressor.token_cb = token_cb\n        compressor.literal_cb = literal_cb\n        compressor.flush_cb = flush_cb\n\n        compressor.write(decompressed)\n        compressor.flush()\n\n        results[\"decompressed_size\"] = len(decompressed)\n        results[\"compressed_size\"] = compressed_out.tell()\n        results[\"ratio\"] = results[\"decompressed_size\"] / results[\"compressed_size\"]\n\n    with Path(f\"build/results-w{args.window_bits}.pkl\").open(\"wb\") as f:\n        pickle.dump(results, f)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "tools/huffman.py", "chunked_list": ["import heapq\nfrom typing import Any, Dict, Union\n\n\nclass Node:\n    def __init__(self, char, prob):\n        self.char = char\n        self.prob = prob\n        self.left = None\n        self.right = None\n\n    def __lt__(self, other):\n        return self.prob < other.prob", "\n\ndef create_priority_queue(probs):\n    queue = []\n    if not isinstance(probs, dict):\n        probs = dict(zip(range(len(probs)), probs))\n\n    for token, prob in probs.items():\n        heapq.heappush(queue, Node(token, prob))\n    return queue", "\n\ndef build_huffman_tree(queue):\n    while len(queue) > 1:\n        left = heapq.heappop(queue)\n        right = heapq.heappop(queue)\n        internal_node = Node(None, left.prob + right.prob)\n        internal_node.left = left\n        internal_node.right = right\n        heapq.heappush(queue, internal_node)\n    return heapq.heappop(queue)", "\n\ndef generate_huffman_codes(node, code=\"\"):\n    if node is None:\n        return {}\n    if node.char is not None:\n        return {node.char: code}\n    codes = {}\n    codes.update(generate_huffman_codes(node.left, code + \"0\"))\n    codes.update(generate_huffman_codes(node.right, code + \"1\"))\n    return codes", "\n\ndef huffman_coding(probs: Union[list, Dict[Any, float]]) -> Dict[int, str]:\n    priority_queue = create_priority_queue(probs)\n    huffman_tree_root = build_huffman_tree(priority_queue)\n    huffman_codes = generate_huffman_codes(huffman_tree_root)\n    return huffman_codes\n"]}
{"filename": "tools/huffman_jump_table.py", "chunked_list": ["\"\"\"Goal to increase hufman decoding speed via lookup table.\n\n1. Look at first bit, if it's 0, then stop decoding and return 0. Maybe do the samething for 2nd bit.\n2. Else, read the next 7 bits, and use the value to index into a table.\n3. Use the upper 4 bits to express the number of bits decoded.\n4. Use the lower 4 bits to express the decoded value.\n\"\"\"\n_FLUSH = 15\n\ntable_bits = 7", "\ntable_bits = 7\n\nhuffman_lookup = {\n    \"1\": 1,\n    \"000\": 2,\n    \"011\": 3,\n    \"0100\": 4,\n    \"00100\": 5,\n    \"00110\": 6,", "    \"00100\": 5,\n    \"00110\": 6,\n    \"01011\": 7,\n    \"001011\": 8,\n    \"010100\": 9,\n    \"0010100\": 10,\n    \"0010101\": 11,\n    \"0101010\": 12,\n    \"00111\": 13,\n    \"0101011\": _FLUSH,", "    \"00111\": 13,\n    \"0101011\": _FLUSH,\n}\n\ntable_size = 1 << table_bits\n\nUNPOPULATED = object()\nc_table = [UNPOPULATED] * table_size\npy_table = [UNPOPULATED] * table_size\n\nfor k, v in huffman_lookup.items():\n    n_bits = len(k)\n    n_pad = table_bits - n_bits\n    for j in range(1 << n_pad):\n        index_bin_str = k\n        if n_pad:\n            index_bin_str += format(j, f\"0{n_pad}b\")\n        index = int(index_bin_str, 2)\n        c_table[index] = v | (n_bits << 4)\n        py_table[index] = v | ((n_bits + 1) << 4)", "py_table = [UNPOPULATED] * table_size\n\nfor k, v in huffman_lookup.items():\n    n_bits = len(k)\n    n_pad = table_bits - n_bits\n    for j in range(1 << n_pad):\n        index_bin_str = k\n        if n_pad:\n            index_bin_str += format(j, f\"0{n_pad}b\")\n        index = int(index_bin_str, 2)\n        c_table[index] = v | (n_bits << 4)\n        py_table[index] = v | ((n_bits + 1) << 4)", "\nprint(f\"const uint8_t HUFFMAN_TABLE[{table_size}] = {{{str(c_table)[1:-1]}}};\")\nprint(f\"_HUFFMAN_TABLE = {bytes(py_table)}\")\n"]}
{"filename": "tools/find_seed.py", "chunked_list": ["import argparse\nimport collections\nimport contextlib\nimport multiprocessing\nimport random\nfrom io import BytesIO\nfrom pathlib import Path\n\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom tamp import initialize_dictionary\nfrom tamp.compressor import Compressor\n\n\ndef random_slices(data, num_slices, slice_size):\n    slices = []\n    for _ in range(num_slices):\n        start_index = random.randint(0, len(data) - slice_size)  # noqa: S311\n        end_index = start_index + slice_size\n        slices.append(data[start_index:end_index])\n\n    return slices", "\n\ndef compute_size(seed, data):\n    size = 0\n    for window_bits, chunks in data.items():\n        for chunk in chunks:\n            with BytesIO() as f:\n                dictionary = initialize_dictionary(1 << window_bits, seed=seed)\n                compressor = Compressor(f, window=window_bits, dictionary=dictionary)\n                compressor.write(chunk)\n                compressor.flush()\n                size += f.tell()\n    return size", "\n\ndef find_seed(best_seed, best_size, lock, data, start_index, end_index):\n    random.seed()\n    with contextlib.suppress(KeyboardInterrupt):\n        generator = range(start_index, end_index)\n        if start_index == 0:\n            generator = tqdm(generator)\n        for seed in generator:\n            size = compute_size(seed, data)\n            with lock:\n                if size < best_size.value:\n                    best_seed.value = seed\n                    best_size.value = size\n                    print(f\"{seed=} {size=}\")", "\n\ndef read_data():\n    input_folder = Path(\"build/silesia\")\n    files = list(input_folder.glob(\"*\"))\n    data_list = [x.read_bytes() for x in files]\n    return data_list\n\n\ndef generate_data(data_list, chunks_per_source):\n    sliced_data = {\n        8: [],\n        9: [],\n        10: [],\n    }\n    for data in data_list:\n        for k in sliced_data:\n            sliced_data[k].extend(random_slices(data, chunks_per_source, 1 << k))\n    return sliced_data", "\ndef generate_data(data_list, chunks_per_source):\n    sliced_data = {\n        8: [],\n        9: [],\n        10: [],\n    }\n    for data in data_list:\n        for k in sliced_data:\n            sliced_data[k].extend(random_slices(data, chunks_per_source, 1 << k))\n    return sliced_data", "\n\ndef character_finder(data_list, n):\n    counter = collections.Counter(b\"\".join(data_list))\n    most_common = counter.most_common(n)\n    common_bytes = bytes(x[0] for x in most_common)\n    print(f\"{common_bytes=}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--seed\", type=int, default=-1, help=\"Confirm seed performance.\")\n    parser.add_argument(\"--character-finder\", type=int, default=-1)\n    parser.add_argument(\"--processes\", type=int, default=8)\n    args = parser.parse_args()\n\n    chunks_per_source = 200\n\n    random.seed(100)\n\n    data_list = read_data()\n    sliced_data = generate_data(data_list, chunks_per_source)\n\n    uncompressed_size = 0\n    total_chunks = 0\n    for v in sliced_data.values():\n        uncompressed_size += len(b\"\".join(v))\n        total_chunks += len(v)\n    print(f\"{uncompressed_size=} {total_chunks=}\")\n\n    if args.seed >= 0:\n        seed = args.seed\n        size = compute_size(args.seed, sliced_data)\n        print(f\"{seed=} {size=}\")\n        return\n\n    if args.character_finder >= 0:\n        character_finder(data_list, args.character_finder)\n        return\n\n    shared_best_seed = multiprocessing.Value(\"i\", 0)\n    shared_best_size = multiprocessing.Value(\"i\", 10000000000000)\n    lock = multiprocessing.Lock()\n\n    intervals = list(range(0, 0xFFFFFFFF, 0xFFFFFFFF // args.processes))\n    processes = []\n    for start_index, end_index in zip(intervals[:-1], intervals[1:]):\n        processes.append(\n            multiprocessing.Process(\n                target=find_seed,\n                args=(\n                    shared_best_seed,\n                    shared_best_size,\n                    lock,\n                    sliced_data,\n                    start_index,\n                    end_index,\n                ),\n            )\n        )\n\n    with contextlib.suppress(KeyboardInterrupt):\n        for process in processes:\n            process.start()\n\n        for process in processes:\n            process.join()", "\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--seed\", type=int, default=-1, help=\"Confirm seed performance.\")\n    parser.add_argument(\"--character-finder\", type=int, default=-1)\n    parser.add_argument(\"--processes\", type=int, default=8)\n    args = parser.parse_args()\n\n    chunks_per_source = 200\n\n    random.seed(100)\n\n    data_list = read_data()\n    sliced_data = generate_data(data_list, chunks_per_source)\n\n    uncompressed_size = 0\n    total_chunks = 0\n    for v in sliced_data.values():\n        uncompressed_size += len(b\"\".join(v))\n        total_chunks += len(v)\n    print(f\"{uncompressed_size=} {total_chunks=}\")\n\n    if args.seed >= 0:\n        seed = args.seed\n        size = compute_size(args.seed, sliced_data)\n        print(f\"{seed=} {size=}\")\n        return\n\n    if args.character_finder >= 0:\n        character_finder(data_list, args.character_finder)\n        return\n\n    shared_best_seed = multiprocessing.Value(\"i\", 0)\n    shared_best_size = multiprocessing.Value(\"i\", 10000000000000)\n    lock = multiprocessing.Lock()\n\n    intervals = list(range(0, 0xFFFFFFFF, 0xFFFFFFFF // args.processes))\n    processes = []\n    for start_index, end_index in zip(intervals[:-1], intervals[1:]):\n        processes.append(\n            multiprocessing.Process(\n                target=find_seed,\n                args=(\n                    shared_best_seed,\n                    shared_best_size,\n                    lock,\n                    sliced_data,\n                    start_index,\n                    end_index,\n                ),\n            )\n        )\n\n    with contextlib.suppress(KeyboardInterrupt):\n        for process in processes:\n            process.start()\n\n        for process in processes:\n            process.join()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "tools/profiler.py", "chunked_list": ["from io import BytesIO\n\nimport line_profiler\n\nimport tamp\nimport tamp._c_compressor\n\nprofile = line_profiler.LineProfiler(tamp.Compressor.write)\n\nwith open(\"build/enwik8\", \"rb\") as f, BytesIO() as f_compressed:\n    data = f.read()\n    compressor = tamp._c_compressor.Compressor(f_compressed)\n    profile.runcall(compressor.write, data)", "\nwith open(\"build/enwik8\", \"rb\") as f, BytesIO() as f_compressed:\n    data = f.read()\n    compressor = tamp._c_compressor.Compressor(f_compressed)\n    profile.runcall(compressor.write, data)\n\nprofile.print_stats()\n"]}
{"filename": "tools/micropython-compression-benchmark.py", "chunked_list": ["from pathlib import Path\n\nimport tamp\n\nchunk_size = 1 << 20\n\ncompressor = tamp.Compressor(\"build/enwik8.tamp\")\n\nwith open(\"build/enwik8\", \"rb\") as f:\n    i = 0\n    while True:\n        i += 1\n        print(f\"Chunk {i}\")\n        chunk = f.read(chunk_size)\n        if not chunk:\n            break\n        compressor.write(chunk)\n    compressor.flush(write_token=False)", "with open(\"build/enwik8\", \"rb\") as f:\n    i = 0\n    while True:\n        i += 1\n        print(f\"Chunk {i}\")\n        chunk = f.read(chunk_size)\n        if not chunk:\n            break\n        compressor.write(chunk)\n    compressor.flush(write_token=False)", "print(\"Complete!\")\n"]}
{"filename": ".belay/dependencies/dev/fnmatch/__init__.py", "chunked_list": ["\"\"\"Filename matching with shell patterns.\n\nfnmatch(FILENAME, PATTERN) matches according to the local convention.\nfnmatchcase(FILENAME, PATTERN) always takes case in account.\n\nThe functions operate by translating the pattern into a regular\nexpression.  They cache the compiled regular expressions for speed.\n\nThe function translate(PATTERN) returns a regular expression\ncorresponding to PATTERN.  (It does not compile it.)", "The function translate(PATTERN) returns a regular expression\ncorresponding to PATTERN.  (It does not compile it.)\n\"\"\"\nimport re\n\ntry:\n    from os.path import normcase\nexcept ImportError:\n\n    def normcase(s):\n        \"\"\"\n        From os.path.normcase\n        Normalize the case of a pathname. On Windows, convert all characters\n        in the pathname to lowercase, and also convert forward slashes to\n        backward slashes. On other operating systems, return the path unchanged.\n        \"\"\"\n        return s", "\n\n__all__ = [\"filter\", \"fnmatch\", \"fnmatchcase\", \"translate\"]\n\nCOMPAT = re.__name__ == \"ure\"\n\n\ndef fnmatch(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN.\n\n    Patterns are Unix shell style:\n\n    *       matches everything\n    ?       matches any single character\n    [seq]   matches any character in seq\n    [!seq]  matches any char not in seq\n\n    An initial period in FILENAME is not special.\n    Both FILENAME and PATTERN are first case-normalized\n    if the operating system requires it.\n    If you don't want this, use fnmatchcase(FILENAME, PATTERN).\n    \"\"\"\n    name = normcase(name)\n    pat = normcase(pat)\n    return fnmatchcase(name, pat)", "\n\n# @functools.lru_cache(maxsize=256, typed=True)\ndef _compile_pattern(pat):\n    if isinstance(pat, bytes):\n        pat_str = str(pat, \"ISO-8859-1\")\n        res_str = translate(pat_str)\n        res = bytes(res_str, \"ISO-8859-1\")\n    else:\n        res = translate(pat)\n    if COMPAT:\n        if res.startswith(\"(?ms)\"):\n            res = res[5:]\n        if res.endswith(\"\\\\Z\"):\n            res = res[:-2] + \"$\"\n    return re.compile(res).match", "\n\ndef filter(names, pat):\n    \"\"\"Return the subset of the list NAMES that match PAT.\"\"\"\n    result = []\n    pat = normcase(pat)\n    match = _compile_pattern(pat)\n    for name in names:\n        if match(normcase(name)):\n            result.append(name)\n    return result", "\n\ndef fnmatchcase(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN, including case.\n\n    This is a version of fnmatch() which doesn't case-normalize\n    its arguments.\n    \"\"\"\n    match = _compile_pattern(pat)\n    return match(name) is not None", "\n\ndef translate(pat):\n    \"\"\"Translate a shell PATTERN to a regular expression.\n\n    There is no way to quote meta-characters.\n    \"\"\"\n\n    i, n = 0, len(pat)\n    res = \"\"\n    while i < n:\n        c = pat[i]\n        i = i + 1\n        if c == \"*\":\n            res = res + \".*\"\n        elif c == \"?\":\n            res = res + \".\"\n        elif c == \"[\":\n            j = i\n            if j < n and pat[j] == \"!\":\n                j = j + 1\n            if j < n and pat[j] == \"]\":\n                j = j + 1\n            while j < n and pat[j] != \"]\":\n                j = j + 1\n            if j >= n:\n                res = res + \"\\\\[\"\n            else:\n                stuff = pat[i:j].replace(\"\\\\\", \"\\\\\\\\\")\n                i = j + 1\n                if stuff[0] == \"!\":\n                    stuff = \"^\" + stuff[1:]\n                elif stuff[0] == \"^\":\n                    stuff = \"\\\\\" + stuff\n                res = \"%s[%s]\" % (res, stuff)\n        else:\n            try:\n                res = res + re.escape(c)\n            except AttributeError:\n                # Using ure rather than re-pcre\n                res = res + re_escape(c)\n    # Original patterns is undefined, see http://bugs.python.org/issue21464\n    return \"(?ms)\" + res + \"\\Z\"", "\n\ndef re_escape(pattern):\n    # Replacement minimal re.escape for ure compatibility\n    return re.sub(r\"([\\^\\$\\.\\|\\?\\*\\+\\(\\)\\[\\\\])\", r\"\\\\\\1\", pattern)\n"]}
{"filename": ".belay/dependencies/dev/uprofiler/__init__.py", "chunked_list": ["from time import ticks_diff, ticks_us  # type: ignore[reportGeneralTypeIssues]\n\n_t_import = ticks_us()\n\n_BOLD = \"\\033[1m\"\n_RESET = \"\\033[0m\"\n\n# Default print period\nprint_period = 1\n", "print_period = 1\n\n\ndef _ticks_delta(t_start):\n    return ticks_diff(ticks_us(), t_start)\n\n\nclass _Counter:\n    registry = {}\n\n    def __init__(self, name, print_period):\n        self.name = name\n        self.print_period = print_period\n        self.n = 0\n        self.t_time_us = 0\n\n        self.registry[name] = self\n\n    def record(self, delta):\n        self.n += 1\n        self.t_time_us += delta\n\n    @property\n    def average(self):\n        return self.t_time_us / self.n\n\n    def __str__(self):\n        t_time_ms = self.t_time_us / 1000\n        return f\"{self.name: 24.24} {self.n : >8} calls {t_time_ms:>12.3f}ms total {t_time_ms/self.n:>12.3f}ms average\"\n\n    def print(self):\n        pp = self.print_period\n        if pp is None:\n            pp = print_period\n\n        if pp > 0 and self.n % pp == 0:\n            print(self)", "\n\n# Can't use class; micropython will have issues decorating methods then.\ndef profile(f=None, *, name=None, print_period=None):\n    \"\"\"Function/Method decorator.\"\"\"\n    if f is None:\n        # decorated with arguments\n        return lambda x: profile(x, name=name, print_period=print_period)\n\n    if name is None:\n        # TODO: I think micropython 1.20.0 will have __qualname__\n        name = f.__name__\n\n    try:\n        counter = _Counter.registry[name]\n    except KeyError:\n        counter = _Counter(name, print_period)\n\n    def inner(*args, **kwargs):\n        t_start = ticks_us()\n        result = f(*args, **kwargs)\n        delta = _ticks_delta(t_start)\n        counter.record(delta)\n        counter.print()\n        return result\n\n    return inner", "\n\ndef _table_formatter(name, calls, total_pct, total_ms, avg_ms):\n    return f\"{name: 32.32} {calls: >8} {total_pct: >10} {total_ms: >13} {avg_ms: >13}\"\n\n\ndef print_results():\n    \"\"\"Print summary.\n\n    To be called at end of script.\n    \"\"\"\n    t_total_ms = ticks_diff(ticks_us(), _t_import) / 1000\n\n    print()\n    print(f\"{_BOLD}Total-Time:{_RESET} {t_total_ms:6.3f}ms\")\n\n    header = _table_formatter(\n        \"Name\", \"Calls\", \"Total (%)\", \"Total (ms)\", \"Average (ms)\"\n    )\n\n    print(_BOLD + header + _RESET)\n    print(\"-\" * len(header))\n\n    counters = _Counter.registry.values()\n    counters = sorted(counters, key=lambda x: x.t_time_us, reverse=True)\n    for counter in counters:\n        t_counter_total_ms = counter.t_time_us / 1000\n        print(\n            _table_formatter(\n                name=counter.name,\n                calls=counter.n,\n                total_pct=round(100 * t_counter_total_ms / t_total_ms, 2),\n                total_ms=t_counter_total_ms,\n                avg_ms=t_counter_total_ms / counter.n if counter.n else 0,\n            )\n        )", ""]}
{"filename": ".belay/dependencies/dev/tempfile/__init__.py", "chunked_list": ["import errno\nimport os\nimport random\nimport shutil\n\n_ascii_letters = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\n\ndef _get_candidate_name(size=8):\n    return \"\".join(random.choice(_ascii_letters) for _ in range(size))", "def _get_candidate_name(size=8):\n    return \"\".join(random.choice(_ascii_letters) for _ in range(size))\n\n\ndef _sanitize_inputs(suffix, prefix, dir):\n    if dir is None:\n        dir = \"/tmp\"\n    if suffix is None:\n        suffix = \"\"\n    if prefix is None:\n        prefix = \"\"\n    return suffix, prefix, dir", "\n\ndef _try(action, *args, **kwargs):\n    try:\n        action(*args, **kwargs)\n        return True\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise e\n    return False", "\n\ndef mkdtemp(suffix=None, prefix=None, dir=None):\n    suffix, prefix, dir = _sanitize_inputs(suffix, prefix, dir)\n\n    _try(os.mkdir, dir)\n\n    while True:\n        name = _get_candidate_name()\n        file = dir + \"/\" + prefix + name + suffix\n        if _try(os.mkdir, file):\n            return file", "\n\nclass TemporaryDirectory:\n    def __init__(self, suffix=None, prefix=None, dir=None):\n        self.name = mkdtemp(suffix, prefix, dir)\n\n    def __repr__(self):\n        return \"<{} {!r}>\".format(self.__class__.__name__, self.name)\n\n    def __enter__(self):\n        return self.name\n\n    def __exit__(self, exc, value, tb):\n        self.cleanup()\n\n    def cleanup(self):\n        _try(shutil.rmtree, self.name)", ""]}
{"filename": ".belay/dependencies/dev/collections/deque.py", "chunked_list": ["class deque:\n    def __init__(self, iterable=None, maxlen=None):\n        if iterable is None:\n            self.q = []\n        else:\n            self.q = list(iterable)\n        if maxlen is not None:\n            if not isinstance(maxlen, int):\n                raise TypeError(\"\" \"an integer is required\")\n            if maxlen < 0:\n                raise ValueError(\"\" \"maxlen must be non-negative\")\n        self.__maxlen = maxlen\n\n    def popleft(self):\n        return self.q.pop(0)\n\n    def pop(self):\n        return self.q.pop()\n\n    def remove(self, a):\n        return self.q.remove(a)\n\n    def append(self, a):\n        self.q.append(a)\n        if self.__maxlen is not None and len(self.q) > self.__maxlen:\n            self.popleft()\n\n    def appendleft(self, a):\n        self.q.insert(0, a)\n        if self.__maxlen is not None and len(self.q) > self.__maxlen:\n            self.pop()\n\n    def extend(self, a):\n        if len(self.q) + len(a) > self.__maxlen:\n            raise IndexError\n        self.q.extend(a)\n\n    def clear(self):\n        self.q.clear()\n\n    @property\n    def maxlen(self):\n        return self.__maxlen\n\n    def __len__(self):\n        return len(self.q)\n\n    def __bool__(self):\n        return bool(self.q)\n\n    def __iter__(self):\n        yield from self.q\n\n    def __str__(self):\n        return \"deque({})\".format(self.q)\n\n    def __getitem__(self, idx):\n        return self.q[idx]", ""]}
{"filename": ".belay/dependencies/dev/collections/__init__.py", "chunked_list": ["# Replace built-in collections module.\nfrom ucollections import *\n\n# Provide optional dependencies (which may be installed separately).\ntry:\n    from .defaultdict import defaultdict\nexcept ImportError:\n    pass\ntry:\n    from .deque import deque\nexcept ImportError:\n    pass", "try:\n    from .deque import deque\nexcept ImportError:\n    pass\n\n\nclass MutableMapping:\n    pass\n", ""]}
{"filename": ".belay/dependencies/dev/shutil/__init__.py", "chunked_list": ["# Reimplement, because CPython3.3 impl is rather bloated\nimport os\nfrom collections import namedtuple\n\n_ntuple_diskusage = namedtuple(\"usage\", (\"total\", \"used\", \"free\"))\n\n\ndef rmtree(d):\n    if not d:\n        raise ValueError\n\n    for name, type, *_ in os.ilistdir(d):\n        path = d + \"/\" + name\n        if type & 0x4000:  # dir\n            rmtree(path)\n        else:  # file\n            os.unlink(path)\n    os.rmdir(d)", "\n\ndef copyfileobj(src, dest, length=512):\n    if hasattr(src, \"readinto\"):\n        buf = bytearray(length)\n        while True:\n            sz = src.readinto(buf)\n            if not sz:\n                break\n            if sz == length:\n                dest.write(buf)\n            else:\n                b = memoryview(buf)[:sz]\n                dest.write(b)\n    else:\n        while True:\n            buf = src.read(length)\n            if not buf:\n                break\n            dest.write(buf)", "\n\ndef disk_usage(path):\n    bit_tuple = os.statvfs(path)\n    blksize = bit_tuple[0]  # system block size\n    total = bit_tuple[2] * blksize\n    free = bit_tuple[3] * blksize\n    used = total - free\n\n    return _ntuple_diskusage(total, used, free)", ""]}
{"filename": ".belay/dependencies/dev/pathlib/__init__.py", "chunked_list": ["import errno\nimport os\n\nfrom micropython import const\n\n_SEP = const(\"/\")\n\n\ndef _mode_if_exists(path):\n    try:\n        return os.stat(path)[0]\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            return 0\n        raise e", "def _mode_if_exists(path):\n    try:\n        return os.stat(path)[0]\n    except OSError as e:\n        if e.errno == errno.ENOENT:\n            return 0\n        raise e\n\n\ndef _clean_segment(segment):\n    segment = str(segment)\n    if not segment:\n        return \".\"\n    segment = segment.rstrip(_SEP)\n    if not segment:\n        return _SEP\n    while True:\n        no_double = segment.replace(_SEP + _SEP, _SEP)\n        if no_double == segment:\n            break\n        segment = no_double\n    return segment", "\ndef _clean_segment(segment):\n    segment = str(segment)\n    if not segment:\n        return \".\"\n    segment = segment.rstrip(_SEP)\n    if not segment:\n        return _SEP\n    while True:\n        no_double = segment.replace(_SEP + _SEP, _SEP)\n        if no_double == segment:\n            break\n        segment = no_double\n    return segment", "\n\nclass Path:\n    def __init__(self, *segments):\n        segments_cleaned = []\n        for segment in segments:\n            segment = _clean_segment(segment)\n            if segment[0] == _SEP:\n                segments_cleaned = [segment]\n            elif segment == \".\":\n                continue\n            else:\n                segments_cleaned.append(segment)\n\n        self._path = _clean_segment(_SEP.join(segments_cleaned))\n\n    def __truediv__(self, other):\n        return Path(self._path, str(other))\n\n    def __repr__(self):\n        return f'{type(self).__name__}(\"{self._path}\")'\n\n    def __str__(self):\n        return self._path\n\n    def __eq__(self, other):\n        return self.absolute() == Path(other).absolute()\n\n    def absolute(self):\n        path = self._path\n        cwd = os.getcwd()\n        if not path or path == \".\":\n            return cwd\n        if path[0] == _SEP:\n            return path\n        return _SEP + path if cwd == _SEP else cwd + _SEP + path\n\n    def resolve(self):\n        return self.absolute()\n\n    def open(self, mode=\"r\", encoding=None):\n        return open(self._path, mode, encoding=encoding)\n\n    def exists(self):\n        return bool(_mode_if_exists(self._path))\n\n    def mkdir(self, parents=False, exist_ok=False):\n        try:\n            os.mkdir(self._path)\n            return\n        except OSError as e:\n            if e.errno == errno.EEXIST and exist_ok:\n                return\n            elif e.errno == errno.ENOENT and parents:\n                pass  # handled below\n            else:\n                raise e\n\n        segments = self._path.split(_SEP)\n        progressive_path = \"\"\n        if segments[0] == \"\":\n            segments = segments[1:]\n            progressive_path = _SEP\n        for segment in segments:\n            progressive_path += _SEP + segment\n            try:\n                os.mkdir(progressive_path)\n            except OSError as e:\n                if e.errno != errno.EEXIST:\n                    raise e\n\n    def is_dir(self):\n        return bool(_mode_if_exists(self._path) & 0x4000)\n\n    def is_file(self):\n        return bool(_mode_if_exists(self._path) & 0x8000)\n\n    def _glob(self, path, pattern, recursive):\n        # Currently only supports a single \"*\" pattern.\n        n_wildcards = pattern.count(\"*\")\n        n_single_wildcards = pattern.count(\"?\")\n\n        if n_single_wildcards:\n            raise NotImplementedError(\"? single wildcards not implemented.\")\n\n        if n_wildcards == 0:\n            raise ValueError\n        elif n_wildcards > 1:\n            raise NotImplementedError(\"Multiple * wildcards not implemented.\")\n\n        prefix, suffix = pattern.split(\"*\")\n\n        for name, mode, *_ in os.ilistdir(path):\n            full_path = path + _SEP + name\n            if name.startswith(prefix) and name.endswith(suffix):\n                yield full_path\n            if recursive and mode & 0x4000:  # is_dir\n                yield from self._glob(full_path, pattern, recursive=recursive)\n\n    def glob(self, pattern):\n        \"\"\"Iterate over this subtree and yield all existing files (of any\n        kind, including directories) matching the given relative pattern.\n\n        Currently only supports a single \"*\" pattern.\n        \"\"\"\n        return self._glob(self._path, pattern, recursive=False)\n\n    def rglob(self, pattern):\n        return self._glob(self._path, pattern, recursive=True)\n\n    def stat(self):\n        return os.stat(self._path)\n\n    def read_bytes(self):\n        with open(self._path, \"rb\") as f:\n            return f.read()\n\n    def read_text(self, encoding=None):\n        with open(self._path, \"r\", encoding=encoding) as f:\n            return f.read()\n\n    def rename(self, target):\n        os.rename(self._path, target)\n\n    def rmdir(self):\n        os.rmdir(self._path)\n\n    def touch(self, exist_ok=True):\n        if self.exists():\n            if exist_ok:\n                return  # TODO: should update timestamp\n            else:\n                # In lieue of FileExistsError\n                raise OSError(errno.EEXIST)\n        with open(self._path, \"w\"):\n            pass\n\n    def unlink(self, missing_ok=False):\n        try:\n            os.unlink(self._path)\n        except OSError as e:\n            if not (missing_ok and e.errno == errno.ENOENT):\n                raise e\n\n    def write_bytes(self, data):\n        with open(self._path, \"wb\") as f:\n            f.write(data)\n\n    def write_text(self, data, encoding=None):\n        with open(self._path, \"w\", encoding=encoding) as f:\n            f.write(data)\n\n    def with_suffix(self, suffix):\n        index = -len(self.suffix) or None\n        return Path(self._path[:index] + suffix)\n\n    @property\n    def stem(self):\n        return self.name.rsplit(\".\", 1)[0]\n\n    @property\n    def parent(self):\n        tokens = self._path.rsplit(_SEP, 1)\n        if len(tokens) == 2:\n            if not tokens[0]:\n                tokens[0] = _SEP\n            return Path(tokens[0])\n        return Path(\".\")\n\n    @property\n    def name(self):\n        return self._path.rsplit(_SEP, 1)[-1]\n\n    @property\n    def suffix(self):\n        elems = self._path.rsplit(\".\", 1)\n        return \"\" if len(elems) == 1 else \".\" + elems[1]", ""]}
{"filename": ".belay/dependencies/dev/unittest/__main__.py", "chunked_list": ["# Extension for \"unittest\" that adds the ability to run via \"micropython -m unittest\".\n\nimport argparse\nimport os\nimport sys\nfrom fnmatch import fnmatch\nfrom micropython import const\n\nfrom unittest import TestRunner, TestResult, TestSuite\n", "from unittest import TestRunner, TestResult, TestSuite\n\n\n# Run a single test in a clean environment.\ndef _run_test_module(runner: TestRunner, module_name: str, *extra_paths: list[str]):\n    module_snapshot = {k: v for k, v in sys.modules.items()}\n    path_snapshot = sys.path[:]\n    try:\n        for path in reversed(extra_paths):\n            if path:\n                sys.path.insert(0, path)\n\n        module = __import__(module_name)\n        suite = TestSuite(module_name)\n        suite._load_module(module)\n        return runner.run(suite)\n    finally:\n        sys.path[:] = path_snapshot\n        sys.modules.clear()\n        sys.modules.update(module_snapshot)", "\n\n_DIR_TYPE = const(0x4000)\n\n\ndef _run_all_in_dir(runner: TestRunner, path: str, pattern: str, top: str):\n    result = TestResult()\n    for fname, ftype, *_ in os.ilistdir(path):\n        if fname in (\"..\", \".\"):\n            continue\n        if ftype == _DIR_TYPE:\n            result += _run_all_in_dir(\n                runner=runner,\n                path=\"/\".join((path, fname)),\n                pattern=pattern,\n                top=top,\n            )\n        if fnmatch(fname, pattern):\n            module_name = fname.rsplit(\".\", 1)[0]\n            result += _run_test_module(runner, module_name, path, top)\n    return result", "\n\n# Implements discovery inspired by https://docs.python.org/3/library/unittest.html#test-discovery\ndef _discover(runner: TestRunner):\n    parser = argparse.ArgumentParser()\n    # parser.add_argument(\n    #     \"-v\",\n    #     \"--verbose\",\n    #     action=\"store_true\",\n    #     help=\"Verbose output\",\n    # )\n    parser.add_argument(\n        \"-s\",\n        \"--start-directory\",\n        dest=\"start\",\n        default=\".\",\n        help=\"Directory to start discovery\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--pattern \",\n        dest=\"pattern\",\n        default=\"test*.py\",\n        help=\"Pattern to match test files\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--top-level-directory\",\n        dest=\"top\",\n        help=\"Top level directory of project (defaults to start directory)\",\n    )\n    args = parser.parse_args(args=sys.argv[2:])\n\n    path = args.start\n    top = args.top or path\n\n    return _run_all_in_dir(\n        runner=runner,\n        path=path,\n        pattern=args.pattern,\n        top=top,\n    )", "\n\n# TODO: Use os.path for path handling.\nPATH_SEP = getattr(os, \"sep\", \"/\")\n\n\n# foo/bar/x.y.z --> foo/bar, x.y\ndef _dirname_filename_no_ext(path):\n    # Workaround: The Windows port currently reports \"/\" for os.sep\n    # (and MicroPython doesn't have os.altsep). So for now just\n    # always work with os.sep (i.e. \"/\").\n    path = path.replace(\"\\\\\", PATH_SEP)\n\n    split = path.rsplit(PATH_SEP, 1)\n    if len(split) == 1:\n        dirname, filename = \"\", split[0]\n    else:\n        dirname, filename = split\n    return dirname, filename.rsplit(\".\", 1)[0]", "\n\ndef discover_main():\n    runner = TestRunner()\n\n    if len(sys.argv) == 1 or (\n        len(sys.argv) >= 2\n        and _dirname_filename_no_ext(sys.argv[0])[1] == \"unittest\"\n        and sys.argv[1] == \"discover\"\n    ):\n        # No args, or `python -m unittest discover ...`.\n        result = _discover(runner)\n    else:\n        result = TestResult()\n        for test_spec in sys.argv[1:]:\n            try:\n                os.stat(test_spec)\n                # File exists, strip extension and import with its parent directory in sys.path.\n                dirname, module_name = _dirname_filename_no_ext(test_spec)\n                res = _run_test_module(runner, module_name, dirname)\n            except OSError:\n                # Not a file, treat as named module to import.\n                res = _run_test_module(runner, test_spec)\n\n            result += res\n\n    if not result.testsRun:\n        # If tests are run their results are already printed.\n        # Ensure an appropriate output is printed if no tests are found.\n        runner.run(TestSuite())\n\n    # Terminate with non zero return code in case of failures.\n    sys.exit(result.failuresNum + result.errorsNum)", "\n\ndiscover_main()\n"]}
{"filename": ".belay/dependencies/dev/unittest/__init__.py", "chunked_list": ["import io\nimport os\nimport sys\n\ntry:\n    import traceback\nexcept ImportError:\n    traceback = None\n\n\nclass SkipTest(Exception):\n    pass", "\n\nclass SkipTest(Exception):\n    pass\n\n\nclass AssertRaisesContext:\n    def __init__(self, exc):\n        self.expected = exc\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        self.exception = exc_value\n        if exc_type is None:\n            assert False, \"%r not raised\" % self.expected\n        if issubclass(exc_type, self.expected):\n            # store exception for later retrieval\n            self.exception = exc_value\n            return True\n        return False", "\n\n# These are used to provide required context to things like subTest\n__current_test__ = None\n__test_result__ = None\n\n\nclass SubtestContext:\n    def __init__(self, msg=None, params=None):\n        self.msg = msg\n        self.params = params\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *exc_info):\n        if exc_info[0] is not None:\n            # Exception raised\n            global __test_result__, __current_test__\n            test_details = __current_test__\n            if self.msg:\n                test_details += (f\" [{self.msg}]\",)\n            if self.params:\n                detail = \", \".join(f\"{k}={v}\" for k, v in self.params.items())\n                test_details += (f\" ({detail})\",)\n\n            _handle_test_exception(test_details, __test_result__, exc_info, False)\n        # Suppress the exception as we've captured it above\n        return True", "\n\nclass NullContext:\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n\nclass TestCase:\n    def __init__(self):\n        pass\n\n    def addCleanup(self, func, *args, **kwargs):\n        if not hasattr(self, \"_cleanups\"):\n            self._cleanups = []\n        self._cleanups.append((func, args, kwargs))\n\n    def doCleanups(self):\n        if hasattr(self, \"_cleanups\"):\n            while self._cleanups:\n                func, args, kwargs = self._cleanups.pop()\n                func(*args, **kwargs)\n\n    def subTest(self, msg=None, **params):\n        return SubtestContext(msg=msg, params=params)\n\n    def skipTest(self, reason):\n        raise SkipTest(reason)\n\n    def fail(self, msg=\"\"):\n        assert False, msg\n\n    def assertEqual(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"%r vs (expected) %r\" % (x, y)\n        assert x == y, msg\n\n    def assertNotEqual(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"%r not expected to be equal %r\" % (x, y)\n        assert x != y, msg\n\n    def assertLessEqual(self, x, y, msg=None):\n        if msg is None:\n            msg = \"%r is expected to be <= %r\" % (x, y)\n        assert x <= y, msg\n\n    def assertGreaterEqual(self, x, y, msg=None):\n        if msg is None:\n            msg = \"%r is expected to be >= %r\" % (x, y)\n        assert x >= y, msg\n\n    def assertAlmostEqual(self, x, y, places=None, msg=\"\", delta=None):\n        if x == y:\n            return\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n\n        if delta is not None:\n            if abs(x - y) <= delta:\n                return\n            if not msg:\n                msg = \"%r != %r within %r delta\" % (x, y, delta)\n        else:\n            if places is None:\n                places = 7\n            if round(abs(y - x), places) == 0:\n                return\n            if not msg:\n                msg = \"%r != %r within %r places\" % (x, y, places)\n\n        assert False, msg\n\n    def assertNotAlmostEqual(self, x, y, places=None, msg=\"\", delta=None):\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n\n        if delta is not None:\n            if not (x == y) and abs(x - y) > delta:\n                return\n            if not msg:\n                msg = \"%r == %r within %r delta\" % (x, y, delta)\n        else:\n            if places is None:\n                places = 7\n            if not (x == y) and round(abs(y - x), places) != 0:\n                return\n            if not msg:\n                msg = \"%r == %r within %r places\" % (x, y, places)\n\n        assert False, msg\n\n    def assertIs(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"%r is not %r\" % (x, y)\n        assert x is y, msg\n\n    def assertIsNot(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"%r is %r\" % (x, y)\n        assert x is not y, msg\n\n    def assertIsNone(self, x, msg=\"\"):\n        if not msg:\n            msg = \"%r is not None\" % x\n        assert x is None, msg\n\n    def assertIsNotNone(self, x, msg=\"\"):\n        if not msg:\n            msg = \"%r is None\" % x\n        assert x is not None, msg\n\n    def assertTrue(self, x, msg=\"\"):\n        if not msg:\n            msg = \"Expected %r to be True\" % x\n        assert x, msg\n\n    def assertFalse(self, x, msg=\"\"):\n        if not msg:\n            msg = \"Expected %r to be False\" % x\n        assert not x, msg\n\n    def assertIn(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"Expected %r to be in %r\" % (x, y)\n        assert x in y, msg\n\n    def assertIsInstance(self, x, y, msg=\"\"):\n        assert isinstance(x, y), msg\n\n    def assertRaises(self, exc, func=None, *args, **kwargs):\n        if func is None:\n            return AssertRaisesContext(exc)\n\n        try:\n            func(*args, **kwargs)\n        except Exception as e:\n            if isinstance(e, exc):\n                return\n            raise\n\n        assert False, \"%r not raised\" % exc\n\n    def assertWarns(self, warn):\n        return NullContext()", "\n\nclass TestCase:\n    def __init__(self):\n        pass\n\n    def addCleanup(self, func, *args, **kwargs):\n        if not hasattr(self, \"_cleanups\"):\n            self._cleanups = []\n        self._cleanups.append((func, args, kwargs))\n\n    def doCleanups(self):\n        if hasattr(self, \"_cleanups\"):\n            while self._cleanups:\n                func, args, kwargs = self._cleanups.pop()\n                func(*args, **kwargs)\n\n    def subTest(self, msg=None, **params):\n        return SubtestContext(msg=msg, params=params)\n\n    def skipTest(self, reason):\n        raise SkipTest(reason)\n\n    def fail(self, msg=\"\"):\n        assert False, msg\n\n    def assertEqual(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"%r vs (expected) %r\" % (x, y)\n        assert x == y, msg\n\n    def assertNotEqual(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"%r not expected to be equal %r\" % (x, y)\n        assert x != y, msg\n\n    def assertLessEqual(self, x, y, msg=None):\n        if msg is None:\n            msg = \"%r is expected to be <= %r\" % (x, y)\n        assert x <= y, msg\n\n    def assertGreaterEqual(self, x, y, msg=None):\n        if msg is None:\n            msg = \"%r is expected to be >= %r\" % (x, y)\n        assert x >= y, msg\n\n    def assertAlmostEqual(self, x, y, places=None, msg=\"\", delta=None):\n        if x == y:\n            return\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n\n        if delta is not None:\n            if abs(x - y) <= delta:\n                return\n            if not msg:\n                msg = \"%r != %r within %r delta\" % (x, y, delta)\n        else:\n            if places is None:\n                places = 7\n            if round(abs(y - x), places) == 0:\n                return\n            if not msg:\n                msg = \"%r != %r within %r places\" % (x, y, places)\n\n        assert False, msg\n\n    def assertNotAlmostEqual(self, x, y, places=None, msg=\"\", delta=None):\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n\n        if delta is not None:\n            if not (x == y) and abs(x - y) > delta:\n                return\n            if not msg:\n                msg = \"%r == %r within %r delta\" % (x, y, delta)\n        else:\n            if places is None:\n                places = 7\n            if not (x == y) and round(abs(y - x), places) != 0:\n                return\n            if not msg:\n                msg = \"%r == %r within %r places\" % (x, y, places)\n\n        assert False, msg\n\n    def assertIs(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"%r is not %r\" % (x, y)\n        assert x is y, msg\n\n    def assertIsNot(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"%r is %r\" % (x, y)\n        assert x is not y, msg\n\n    def assertIsNone(self, x, msg=\"\"):\n        if not msg:\n            msg = \"%r is not None\" % x\n        assert x is None, msg\n\n    def assertIsNotNone(self, x, msg=\"\"):\n        if not msg:\n            msg = \"%r is None\" % x\n        assert x is not None, msg\n\n    def assertTrue(self, x, msg=\"\"):\n        if not msg:\n            msg = \"Expected %r to be True\" % x\n        assert x, msg\n\n    def assertFalse(self, x, msg=\"\"):\n        if not msg:\n            msg = \"Expected %r to be False\" % x\n        assert not x, msg\n\n    def assertIn(self, x, y, msg=\"\"):\n        if not msg:\n            msg = \"Expected %r to be in %r\" % (x, y)\n        assert x in y, msg\n\n    def assertIsInstance(self, x, y, msg=\"\"):\n        assert isinstance(x, y), msg\n\n    def assertRaises(self, exc, func=None, *args, **kwargs):\n        if func is None:\n            return AssertRaisesContext(exc)\n\n        try:\n            func(*args, **kwargs)\n        except Exception as e:\n            if isinstance(e, exc):\n                return\n            raise\n\n        assert False, \"%r not raised\" % exc\n\n    def assertWarns(self, warn):\n        return NullContext()", "\n\ndef skip(msg):\n    def _decor(fun):\n        # We just replace original fun with _inner\n        def _inner(self):\n            raise SkipTest(msg)\n\n        return _inner\n\n    return _decor", "\n\ndef skipIf(cond, msg):\n    if not cond:\n        return lambda x: x\n    return skip(msg)\n\n\ndef skipUnless(cond, msg):\n    if cond:\n        return lambda x: x\n    return skip(msg)", "def skipUnless(cond, msg):\n    if cond:\n        return lambda x: x\n    return skip(msg)\n\n\ndef expectedFailure(test):\n    def test_exp_fail(*args, **kwargs):\n        try:\n            test(*args, **kwargs)\n        except:\n            pass\n        else:\n            assert False, \"unexpected success\"\n\n    return test_exp_fail", "\n\nclass TestSuite:\n    def __init__(self, name=\"\"):\n        self._tests = []\n        self.name = name\n\n    def addTest(self, cls):\n        self._tests.append(cls)\n\n    def run(self, result):\n        for c in self._tests:\n            _run_suite(c, result, self.name)\n        return result\n\n    def _load_module(self, mod):\n        for tn in dir(mod):\n            c = getattr(mod, tn)\n            if isinstance(c, object) and isinstance(c, type) and issubclass(c, TestCase):\n                self.addTest(c)\n            elif tn.startswith(\"test\") and callable(c):\n                self.addTest(c)", "\n\nclass TestRunner:\n    def run(self, suite: TestSuite):\n        res = TestResult()\n        suite.run(res)\n\n        res.printErrors()\n        print(\"----------------------------------------------------------------------\")\n        print(\"Ran %d tests\\n\" % res.testsRun)\n        if res.failuresNum > 0 or res.errorsNum > 0:\n            print(\"FAILED (failures=%d, errors=%d)\" % (res.failuresNum, res.errorsNum))\n        else:\n            msg = \"OK\"\n            if res.skippedNum > 0:\n                msg += \" (skipped=%d)\" % res.skippedNum\n            print(msg)\n\n        return res", "\n\nTextTestRunner = TestRunner\n\n\nclass TestResult:\n    def __init__(self):\n        self.errorsNum = 0\n        self.failuresNum = 0\n        self.skippedNum = 0\n        self.testsRun = 0\n        self.errors = []\n        self.failures = []\n        self.skipped = []\n        self._newFailures = 0\n\n    def wasSuccessful(self):\n        return self.errorsNum == 0 and self.failuresNum == 0\n\n    def printErrors(self):\n        if self.errors or self.failures:\n            print()\n            self.printErrorList(self.errors)\n            self.printErrorList(self.failures)\n\n    def printErrorList(self, lst):\n        sep = \"----------------------------------------------------------------------\"\n        for c, e in lst:\n            detail = \" \".join((str(i) for i in c))\n            print(\"======================================================================\")\n            print(f\"FAIL: {detail}\")\n            print(sep)\n            print(e)\n\n    def __repr__(self):\n        # Format is compatible with CPython.\n        return \"<unittest.result.TestResult run=%d errors=%d failures=%d>\" % (\n            self.testsRun,\n            self.errorsNum,\n            self.failuresNum,\n        )\n\n    def __add__(self, other):\n        self.errorsNum += other.errorsNum\n        self.failuresNum += other.failuresNum\n        self.skippedNum += other.skippedNum\n        self.testsRun += other.testsRun\n        self.errors.extend(other.errors)\n        self.failures.extend(other.failures)\n        self.skipped.extend(other.skipped)\n        return self", "\n\ndef _capture_exc(exc, exc_traceback):\n    buf = io.StringIO()\n    if hasattr(sys, \"print_exception\"):\n        sys.print_exception(exc, buf)\n    elif traceback is not None:\n        traceback.print_exception(None, exc, exc_traceback, file=buf)\n    return buf.getvalue()\n", "\n\ndef _handle_test_exception(\n    current_test: tuple, test_result: TestResult, exc_info: tuple, verbose=True\n):\n    exc = exc_info[1]\n    traceback = exc_info[2]\n    ex_str = _capture_exc(exc, traceback)\n    if isinstance(exc, AssertionError):\n        test_result.failuresNum += 1\n        test_result.failures.append((current_test, ex_str))\n        if verbose:\n            print(\" FAIL\")\n    else:\n        test_result.errorsNum += 1\n        test_result.errors.append((current_test, ex_str))\n        if verbose:\n            print(\" ERROR\")\n    test_result._newFailures += 1", "\n\ndef _run_suite(c, test_result: TestResult, suite_name=\"\"):\n    if isinstance(c, TestSuite):\n        c.run(test_result)\n        return\n\n    if isinstance(c, type):\n        o = c()\n    else:\n        o = c\n    set_up_class = getattr(o, \"setUpClass\", lambda: None)\n    tear_down_class = getattr(o, \"tearDownClass\", lambda: None)\n    set_up = getattr(o, \"setUp\", lambda: None)\n    tear_down = getattr(o, \"tearDown\", lambda: None)\n    exceptions = []\n    try:\n        suite_name += \".\" + c.__qualname__\n    except AttributeError:\n        pass\n\n    def run_one(test_function):\n        global __test_result__, __current_test__\n        print(\"%s (%s) ...\" % (name, suite_name), end=\"\")\n        set_up()\n        __test_result__ = test_result\n        test_container = f\"({suite_name})\"\n        __current_test__ = (name, test_container)\n        try:\n            test_result._newFailures = 0\n            test_result.testsRun += 1\n            test_function()\n            # No exception occurred, test passed\n            if test_result._newFailures:\n                print(\" FAIL\")\n            else:\n                print(\" ok\")\n        except SkipTest as e:\n            reason = e.args[0]\n            print(\" skipped:\", reason)\n            test_result.skippedNum += 1\n            test_result.skipped.append((name, c, reason))\n        except Exception as ex:\n            _handle_test_exception(\n                current_test=(name, c), test_result=test_result, exc_info=(type(ex), ex, None)\n            )\n            # Uncomment to investigate failure in detail\n            # raise\n        finally:\n            __test_result__ = None\n            __current_test__ = None\n            tear_down()\n            try:\n                o.doCleanups()\n            except AttributeError:\n                pass\n\n    set_up_class()\n    try:\n        if hasattr(o, \"runTest\"):\n            name = str(o)\n            run_one(o.runTest)\n            return\n\n        for name in dir(o):\n            if name.startswith(\"test\"):\n                m = getattr(o, name)\n                if not callable(m):\n                    continue\n                run_one(m)\n\n        if callable(o):\n            name = o.__name__\n            run_one(o)\n    finally:\n        tear_down_class()\n\n    return exceptions", "\n\n# This supports either:\n#\n# >>> import mytest\n# >>> unitttest.main(mytest)\n#\n# >>> unittest.main(\"mytest\")\n#\n# Or, a script that ends with:", "#\n# Or, a script that ends with:\n# if __name__ == \"__main__\":\n#     unittest.main()\n# e.g. run via `mpremote run mytest.py`\ndef main(module=\"__main__\", testRunner=None):\n    if testRunner is None:\n        testRunner = TestRunner()\n    elif isinstance(testRunner, type):\n        testRunner = testRunner()\n\n    if isinstance(module, str):\n        module = __import__(module)\n    suite = TestSuite(module.__name__)\n    suite._load_module(module)\n    return testRunner.run(suite)", ""]}
{"filename": ".belay/dependencies/dev/argparse/__init__.py", "chunked_list": ["\"\"\"\nMinimal and functional version of CPython's argparse module.\n\"\"\"\n\nimport sys\nfrom ucollections import namedtuple\n\n\nclass _ArgError(BaseException):\n    pass", "class _ArgError(BaseException):\n    pass\n\n\nclass _Arg:\n    def __init__(self, names, dest, action, nargs, const, default, help):\n        self.names = names\n        self.dest = dest\n        self.action = action\n        self.nargs = nargs\n        self.const = const\n        self.default = default\n        self.help = help\n\n    def parse(self, optname, args):\n        # parse args for this arg\n        if self.action == \"store\":\n            if self.nargs is None:\n                if args:\n                    return args.pop(0)\n                else:\n                    raise _ArgError(\"expecting value for %s\" % optname)\n            elif self.nargs == \"?\":\n                if args:\n                    return args.pop(0)\n                else:\n                    return self.default\n            else:\n                if self.nargs == \"*\":\n                    n = -1\n                elif self.nargs == \"+\":\n                    if not args:\n                        raise _ArgError(\"expecting value for %s\" % optname)\n                    n = -1\n                else:\n                    n = int(self.nargs)\n                ret = []\n                stop_at_opt = True\n                while args and n != 0:\n                    if stop_at_opt and args[0].startswith(\"-\") and args[0] != \"-\":\n                        if args[0] == \"--\":\n                            stop_at_opt = False\n                            args.pop(0)\n                        else:\n                            break\n                    else:\n                        ret.append(args.pop(0))\n                        n -= 1\n                if n > 0:\n                    raise _ArgError(\"expecting value for %s\" % optname)\n                return ret\n        elif self.action == \"store_const\":\n            return self.const\n        else:\n            assert False", "\n\ndef _dest_from_optnames(opt_names):\n    dest = opt_names[0]\n    for name in opt_names:\n        if name.startswith(\"--\"):\n            dest = name\n            break\n    return dest.lstrip(\"-\").replace(\"-\", \"_\")\n", "\n\nclass ArgumentParser:\n    def __init__(self, *, description=\"\"):\n        self.description = description\n        self.opt = []\n        self.pos = []\n\n    def add_argument(self, *args, **kwargs):\n        action = kwargs.get(\"action\", \"store\")\n        if action == \"store_true\":\n            action = \"store_const\"\n            const = True\n            default = kwargs.get(\"default\", False)\n        elif action == \"store_false\":\n            action = \"store_const\"\n            const = False\n            default = kwargs.get(\"default\", True)\n        else:\n            const = kwargs.get(\"const\", None)\n            default = kwargs.get(\"default\", None)\n        if args and args[0].startswith(\"-\"):\n            list = self.opt\n            dest = kwargs.get(\"dest\")\n            if dest is None:\n                dest = _dest_from_optnames(args)\n        else:\n            list = self.pos\n            dest = kwargs.get(\"dest\")\n            if dest is None:\n                dest = args[0]\n            if not args:\n                args = [dest]\n        list.append(\n            _Arg(\n                args,\n                dest,\n                action,\n                kwargs.get(\"nargs\", None),\n                const,\n                default,\n                kwargs.get(\"help\", \"\"),\n            )\n        )\n\n    def usage(self, full):\n        # print short usage\n        print(\"usage: %s [-h]\" % sys.argv[0], end=\"\")\n\n        def render_arg(arg):\n            if arg.action == \"store\":\n                if arg.nargs is None:\n                    return \" %s\" % arg.dest\n                if isinstance(arg.nargs, int):\n                    return \" %s(x%d)\" % (arg.dest, arg.nargs)\n                else:\n                    return \" %s%s\" % (arg.dest, arg.nargs)\n            else:\n                return \"\"\n\n        for opt in self.opt:\n            print(\" [%s%s]\" % (\", \".join(opt.names), render_arg(opt)), end=\"\")\n        for pos in self.pos:\n            print(render_arg(pos), end=\"\")\n        print()\n\n        if not full:\n            return\n\n        # print full information\n        print()\n        if self.description:\n            print(self.description)\n        if self.pos:\n            print(\"\\npositional args:\")\n            for pos in self.pos:\n                print(\"  %-16s%s\" % (pos.names[0], pos.help))\n        print(\"\\noptional args:\")\n        print(\"  -h, --help      show this message and exit\")\n        for opt in self.opt:\n            print(\"  %-16s%s\" % (\", \".join(opt.names) + render_arg(opt), opt.help))\n\n    def parse_args(self, args=None):\n        return self._parse_args_impl(args, False)\n\n    def parse_known_args(self, args=None):\n        return self._parse_args_impl(args, True)\n\n    def _parse_args_impl(self, args, return_unknown):\n        if args is None:\n            args = sys.argv[1:]\n        else:\n            args = args[:]\n        try:\n            return self._parse_args(args, return_unknown)\n        except _ArgError as e:\n            self.usage(False)\n            print(\"error:\", e)\n            sys.exit(2)\n\n    def _parse_args(self, args, return_unknown):\n        # add optional args with defaults\n        arg_dest = []\n        arg_vals = []\n        for opt in self.opt:\n            arg_dest.append(opt.dest)\n            arg_vals.append(opt.default)\n\n        # deal with unknown arguments, if needed\n        unknown = []\n\n        def consume_unknown():\n            while args and not args[0].startswith(\"-\"):\n                unknown.append(args.pop(0))\n\n        # parse all args\n        parsed_pos = False\n        while args or not parsed_pos:\n            if args and args[0].startswith(\"-\") and args[0] != \"-\" and args[0] != \"--\":\n                # optional arg\n                a = args.pop(0)\n                if a in (\"-h\", \"--help\"):\n                    self.usage(True)\n                    sys.exit(0)\n                found = False\n                for i, opt in enumerate(self.opt):\n                    if a in opt.names:\n                        arg_vals[i] = opt.parse(a, args)\n                        found = True\n                        break\n                if not found:\n                    if return_unknown:\n                        unknown.append(a)\n                        consume_unknown()\n                    else:\n                        raise _ArgError(\"unknown option %s\" % a)\n            else:\n                # positional arg\n                if parsed_pos:\n                    if return_unknown:\n                        unknown = unknown + args\n                        break\n                    else:\n                        raise _ArgError(\"extra args: %s\" % \" \".join(args))\n                for pos in self.pos:\n                    arg_dest.append(pos.dest)\n                    arg_vals.append(pos.parse(pos.names[0], args))\n                parsed_pos = True\n                if return_unknown:\n                    consume_unknown()\n\n        # build and return named tuple with arg values\n        values = namedtuple(\"args\", arg_dest)(*arg_vals)\n        return (values, unknown) if return_unknown else values", ""]}
{"filename": "tests/test_compressor.py", "chunked_list": ["\"\"\"Tamp compressor tests.\n\nOur custom huffman size table:\n\n   huffman_coding = {\n            2: 0b0,\n            3: 0b11,\n            4: 0b1000,\n            5: 0b1011,\n            6: 0b10100,", "            5: 0b1011,\n            6: 0b10100,\n            7: 0b100100,\n            8: 0b100110,\n            9: 0b101011,\n           10: 0b1001011,\n           11: 0b1010100,\n           12: 0b10010100,\n           13: 0b10010101,\n           14: 0b10101010,", "           13: 0b10010101,\n           14: 0b10101010,\n           15: 0b100111,\n      \"FLUSH\": 0b10101011,\n   }\n\"\"\"\n\nimport io\nimport unittest\n", "import unittest\n\nfrom tamp import ExcessBitsError\n\ntry:\n    import micropython\nexcept ImportError:\n    micropython = None\n\n", "\n\nCompressors = []\ncompresses = []\n\nif micropython:\n    from tamp.compressor_viper import Compressor as ViperCompressor\n    from tamp.compressor_viper import compress as viper_compress\n\n    Compressors.append(ViperCompressor)\n    compresses.append(viper_compress)\nelse:\n    from tamp.compressor import Compressor as PyCompressor\n    from tamp.compressor import compress as py_compress\n\n    Compressors.append(PyCompressor)\n    compresses.append(py_compress)\n\n    try:\n        from tamp._c_compressor import Compressor as CCompressor\n        from tamp._c_compressor import compress as c_compress\n\n        Compressors.append(CCompressor)\n        compresses.append(c_compress)\n    except ImportError:\n        pass", "\n\nclass TestCompressor(unittest.TestCase):\n    def test_compressor_default(self):\n        for Compressor in Compressors:\n            with self.subTest(Compressor=Compressor):\n                test_string = b\"foo foo foo\"\n\n                expected = bytes(\n                    # fmt: off\n                    [\n                        0b010_11_0_0_0,  # header (window_bits=10, literal_bits=8)\n                        0b1_0110011,    # literal \"f\"\n                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n                                        # size=2 -> 0b0\n                                        # 131 -> 0b0010000011\n                        0b00011_1_00,   # literal \" \"\n                        0b100000_0_1,   # There is now \"foo \" at index 0\n                        0b000_00000,    # size=4 -> 0b1000\n                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n                        0b00000000,     # index 0 -> 0b0000000000\n                        0b00_000000,    # 6 bits of zero-padding\n                    ]\n                    # fmt: on\n                )\n\n                bytes_written = 0\n                with io.BytesIO() as f:\n                    compressor = Compressor(f)\n                    bytes_written += compressor.write(test_string)\n                    bytes_written += compressor.flush(write_token=False)\n\n                    f.seek(0)\n                    actual = f.read()\n                    compressor.close()\n                self.assertEqual(actual, expected)\n                self.assertEqual(bytes_written, len(expected))\n\n                # Test Context Manager\n                bytes_written = 0\n                with io.BytesIO() as f, Compressor(f) as compressor:\n                    bytes_written += compressor.write(test_string)\n                    bytes_written += compressor.flush(write_token=False)\n\n                    f.seek(0)\n                    actual = f.read()\n                self.assertEqual(actual, expected)\n                self.assertEqual(bytes_written, len(expected))\n\n    def test_compressor_input_buffer(self):\n        for Compressor in Compressors:\n            with self.subTest(Compressor=Compressor):\n                expected = bytes(\n                    # fmt: off\n                    [\n                        0b010_11_0_0_0,  # header (window_bits=10, literal_bits=8)\n                        0b1_0110011,    # literal \"f\"\n                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n                                        # size=2 -> 0b0\n                                        # 131 -> 0b0010000011\n                        0b00011_1_00,   # literal \" \"\n                        0b100000_0_1,   # There is now \"foo \" at index 0\n                        0b000_00000,    # size=4 -> 0b1000\n                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n                        0b00000000,     # index 0 -> 0b0000000000\n                        0b00_000000,    # 6 bits of zero-padding\n                    ]\n                    # fmt: on\n                )\n\n                with io.BytesIO() as f:\n                    compressor = Compressor(f)\n                    compressor.write(b\"f\")\n                    compressor.write(b\"oo\")\n                    compressor.write(b\" fo\")\n                    compressor.write(b\"o foo\")\n                    compressor.flush(write_token=False)\n\n                    f.seek(0)\n                    actual = f.read()\n                self.assertEqual(actual, expected)\n\n    def test_compressor_7bit(self):\n        for Compressor in Compressors:\n            with self.subTest(Compressor=Compressor):\n                test_string = b\"foo foo foo\"\n\n                expected = bytes(\n                    # fmt: off\n                    [\n                        0b010_10_0_0_0,  # header (window_bits=10, literal_bits=7)\n                        0b1_1100110,    # literal \"f\"\n                        0b0_0_001000,   # the pre-init buffer contains \"oo \" at index 131\n                                        # size=2 -> 0b0\n                                        # 131 -> 0b0010000011\n                        0b0011_1_010,   # literal \" \"\n                        0b0000_0_100,   # size=4 -> 0b1000\n                        0b0_0000000,\n                        0b000_0_11_00,  # Just \"foo\" at index 0; size=3 -> 0b11\n                        0b000000000,    # index 0 -> 0b0000000000\n                        # no padding!\n                    ]\n                    # fmt: on\n                )\n                with io.BytesIO() as f:\n                    compressor = Compressor(f, literal=7)\n                    compressor.write(test_string)\n                    compressor.flush(write_token=False)\n\n                    f.seek(0)\n                    actual = f.read()\n                self.assertEqual(actual, expected)\n\n    def test_compressor_predefined_dictionary(self):\n        for Compressor in Compressors:\n            with self.subTest(Compressor=Compressor):\n                test_string = b\"foo foo foo\"\n\n                init_string = b\"foo foo foo\"\n                dictionary = bytearray(1 << 8)\n                dictionary[: len(init_string)] = init_string\n\n                expected = bytes(\n                    # fmt: off\n                    [\n                        0b000_10_1_0_0,  # header (window_bits=8, literal_bits=7, dictionary provided)\n                        0b0_1010100,     # match-size 11\n                        0b00000000,      # At index 0\n                        # no padding!\n                    ]\n                    # fmt: on\n                )\n\n                with io.BytesIO() as f:\n                    compressor = Compressor(f, window=8, literal=7, dictionary=dictionary)\n                    compressor.write(test_string)\n                    compressor.flush(write_token=False)\n\n                    f.seek(0)\n                    actual = f.read()\n                self.assertEqual(actual, expected)\n\n    def test_compressor_predefined_dictionary_incorrect_size(self):\n        for Compressor in Compressors:\n            with self.subTest(Compressor=Compressor):\n                dictionary = bytearray(1 << 8)\n                with io.BytesIO() as f, self.assertRaises(ValueError):\n                    Compressor(f, window=9, literal=7, dictionary=dictionary)\n\n    def test_oob_2_byte_pattern(self):\n        \"\"\"Viper implementation had a bug where a pattern of length 2 could be detected at the end of a string (going out of bounds by 1 byte).\"\"\"\n        for Compressor in Compressors:\n            with self.subTest(Compressor=Compressor):\n                test_string_extended = bytearray(b\"Q\\x00Q\\x00\")\n                test_string = memoryview(test_string_extended)[:3]  # b\"Q\\x00Q\"\n\n                with io.BytesIO() as f:\n                    compressor = Compressor(f)\n                    compressor.write(test_string)\n                    compressor.flush(write_token=False)\n\n                    f.seek(0)\n                    actual = f.read()\n\n                # Q == 0b0101_0001\n                expected = bytes(\n                    [\n                        0b010_11_00_0,\n                        0b1_0101_000,\n                        0b1_1_0000_00,\n                        0b00_1_0101_0,\n                        0b001_00000,\n                    ]\n                )\n                assert actual == expected\n\n    def test_excess_bits(self):\n        for Compressor in Compressors:\n            with self.subTest(Compressor=Compressor), io.BytesIO() as f:\n                compressor = Compressor(f, literal=7)\n\n                with self.assertRaises(ExcessBitsError):\n                    compressor.write(b\"\\xFF\")\n                    compressor.flush()\n\n    def test_single_shot_compress_text(self):\n        for compress in compresses:\n            with self.subTest(compress=compress):\n                expected = bytes(\n                    # fmt: off\n                    [\n                        0b010_11_0_0_0,  # header (window_bits=10, literal_bits=8)\n                        0b1_0110011,    # literal \"f\"\n                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n                                        # size=2 -> 0b0\n                                        # 131 -> 0b0010000011\n                        0b00011_1_00,   # literal \" \"\n                        0b100000_0_1,   # There is now \"foo \" at index 0\n                        0b000_00000,    # size=4 -> 0b1000\n                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n                        0b00000000,     # index 0 -> 0b0000000000\n                        0b00_000000,    # 6 bits of zero-padding\n                    ]\n                    # fmt: on\n                )\n                self.assertEqual(compress(\"foo foo foo\"), expected)\n\n    def test_single_shot_compress_binary(self):\n        for compress in compresses:\n            with self.subTest(compress=compress):\n                expected = bytes(\n                    # fmt: off\n                    [\n                        0b010_11_0_0_0,  # header (window_bits=10, literal_bits=8)\n                        0b1_0110011,    # literal \"f\"\n                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n                                        # size=2 -> 0b0\n                                        # 131 -> 0b0010000011\n                        0b00011_1_00,   # literal \" \"\n                        0b100000_0_1,   # There is now \"foo \" at index 0\n                        0b000_00000,    # size=4 -> 0b1000\n                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n                        0b00000000,     # index 0 -> 0b0000000000\n                        0b00_000000,    # 6 bits of zero-padding\n                    ]\n                    # fmt: on\n                )\n                self.assertEqual(compress(b\"foo foo foo\"), expected)\n\n    def test_invalid_conf(self):\n        for Compressor in Compressors:\n            with self.subTest(Compressor=Compressor), io.BytesIO() as f:\n                with self.assertRaises(ValueError):\n                    Compressor(f, literal=4)\n                with self.assertRaises(ValueError):\n                    Compressor(f, window=16)", ""]}
{"filename": "tests/test_bit_writer_reader.py", "chunked_list": ["import io\nimport random\nimport unittest\n\nfrom tamp.compressor import BitWriter\nfrom tamp.decompressor import BitReader\n\n\nclass TestBitWriterAndReader(unittest.TestCase):\n    def test_auto_bit_writer_and_reader(self):\n        # Generate a list of random chunks of bits (1~16 bits)\n        num_chunks = 1000\n        n_bits = [random.randint(1, 16) for _ in range(num_chunks)]\n        data = []\n        for n_bit in n_bits:\n            mask = (1 << n_bit) - 1\n            data.append(random.randint(0, 1 << 32) & mask)\n        chunks = list(zip(data, n_bits))\n\n        # Write the chunks of bits using BitWriter\n        with io.BytesIO() as f:\n            writer = BitWriter(f)\n            for bits, num_bits in chunks:\n                writer.write(bits, num_bits)\n            writer.flush(write_token=False)\n\n            # Read the chunks of bits back using BitReader\n            f.seek(0)\n            reader = BitReader(f)\n            for original_bits, num_bits in chunks:\n                read_bits = reader.read(num_bits)\n                self.assertEqual(read_bits, original_bits)\n\n    def test_writer_correct_size_no_flush_token(self):\n        for i in range(1, 8 + 1):\n            with io.BytesIO() as f:\n                writer = BitWriter(f)\n                writer.write(0xFFFF, i)\n                writer.flush(write_token=False)\n\n                self.assertEqual(f.tell(), 1)\n        for i in range(9, 16 + 1):\n            with io.BytesIO() as f:\n                writer = BitWriter(f)\n                writer.write(0xFFFF, i)\n                writer.flush(write_token=False)\n\n                self.assertEqual(f.tell(), 2)", "class TestBitWriterAndReader(unittest.TestCase):\n    def test_auto_bit_writer_and_reader(self):\n        # Generate a list of random chunks of bits (1~16 bits)\n        num_chunks = 1000\n        n_bits = [random.randint(1, 16) for _ in range(num_chunks)]\n        data = []\n        for n_bit in n_bits:\n            mask = (1 << n_bit) - 1\n            data.append(random.randint(0, 1 << 32) & mask)\n        chunks = list(zip(data, n_bits))\n\n        # Write the chunks of bits using BitWriter\n        with io.BytesIO() as f:\n            writer = BitWriter(f)\n            for bits, num_bits in chunks:\n                writer.write(bits, num_bits)\n            writer.flush(write_token=False)\n\n            # Read the chunks of bits back using BitReader\n            f.seek(0)\n            reader = BitReader(f)\n            for original_bits, num_bits in chunks:\n                read_bits = reader.read(num_bits)\n                self.assertEqual(read_bits, original_bits)\n\n    def test_writer_correct_size_no_flush_token(self):\n        for i in range(1, 8 + 1):\n            with io.BytesIO() as f:\n                writer = BitWriter(f)\n                writer.write(0xFFFF, i)\n                writer.flush(write_token=False)\n\n                self.assertEqual(f.tell(), 1)\n        for i in range(9, 16 + 1):\n            with io.BytesIO() as f:\n                writer = BitWriter(f)\n                writer.write(0xFFFF, i)\n                writer.flush(write_token=False)\n\n                self.assertEqual(f.tell(), 2)", "\n\nclass TestHuffmanReader(unittest.TestCase):\n    def test_always_valid(self):\n        \"\"\"There should never be an instance where 8bits have been read and\n        no huffman code has been decoded. This verifies that.\n        \"\"\"\n        random_bytes = bytes(random.randint(0, 255) for _ in range(1024 * 1024))\n        with io.BytesIO(random_bytes) as f:\n            reader = BitReader(f)\n            reader.read_huffman()", ""]}
{"filename": "tests/test_compressor_helpers.py", "chunked_list": ["import unittest\n\nfrom tamp import bit_size, compute_min_pattern_size\n\n\nclass TestCompressorHelpers(unittest.TestCase):\n    def test_bit_size(self):\n        self.assertEqual(bit_size(0b11), 2)\n\n    def test_bit_size_excess(self):\n        self.assertEqual(bit_size(1 << 32), -1)\n\n    def test_min_pattern_size(self):\n        self.assertEqual(compute_min_pattern_size(window=10, literal=8), 2)\n        self.assertEqual(compute_min_pattern_size(window=15, literal=5), 3)\n\n    def test_min_pattern_size_out_of_range(self):\n        with self.assertRaises(ValueError):\n            compute_min_pattern_size(0, 0)", ""]}
{"filename": "tests/test_cli.py", "chunked_list": ["import tempfile\nimport unittest\nfrom pathlib import Path\n\ntry:\n    import micropython\nexcept ImportError:\n    micropython = None\n\ntry:\n    from typer.testing import CliRunner\n\n    from tamp.cli.main import app\nexcept ImportError:\n    pass\nelse:\n    runner = CliRunner()", "\ntry:\n    from typer.testing import CliRunner\n\n    from tamp.cli.main import app\nexcept ImportError:\n    pass\nelse:\n    runner = CliRunner()\n", "\ncompressed_foo_foo_foo = bytes(\n    # fmt: off\n    [\n        0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n        0b1_0110011,    # literal \"f\"\n        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n                        # size=2 -> 0b0\n                        # 131 -> 0b0010000011\n        0b00011_1_00,   # literal \" \"", "                        # 131 -> 0b0010000011\n        0b00011_1_00,   # literal \" \"\n        0b100000_0_1,   # There is now \"foo \" at index 0\n        0b000_00000,    # size=4 -> 0b1000\n        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n        0b00000000,     # index 0 -> 0b0000000000\n        0b00_000000,    # 6 bits of zero-padding\n    ]\n    # fmt: on\n)", "    # fmt: on\n)\n\n\n@unittest.skipIf(micropython is not None, \"not running cpython\")\nclass TestCli(unittest.TestCase):\n    def test_compress_file_to_stdout(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_dir = Path(tmp_dir)\n            test_file = tmp_dir / \"test_input.bin\"\n            test_file.write_bytes(b\"foo foo foo\")\n            result = runner.invoke(app, [\"compress\", str(test_file)])\n            self.assertEqual(result.exit_code, 0)\n            self.assertEqual(result.stdout_bytes, compressed_foo_foo_foo)\n\n    def test_compress_stdin_to_stdout(self):\n        result = runner.invoke(app, [\"compress\"], input=\"foo foo foo\")\n        self.assertEqual(result.exit_code, 0)\n        self.assertEqual(result.stdout_bytes, compressed_foo_foo_foo)\n\n    def test_decompress_file_to_stdout(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_dir = Path(tmp_dir)\n            test_file = tmp_dir / \"test_input.tamp\"\n            test_file.write_bytes(compressed_foo_foo_foo)\n            result = runner.invoke(app, [\"decompress\", str(test_file)])\n            self.assertEqual(result.exit_code, 0)\n            self.assertEqual(result.stdout, \"foo foo foo\")\n\n    def test_decompress_stdin_to_stdout(self):\n        result = runner.invoke(app, [\"decompress\"], input=compressed_foo_foo_foo)\n        self.assertEqual(result.exit_code, 0)\n        self.assertEqual(result.stdout, \"foo foo foo\")\n\n    def test_decompress_stdin_to_file(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            tmp_dir = Path(tmp_dir)\n            test_file = tmp_dir / \"test_output.txt\"\n\n            result = runner.invoke(app, [\"decompress\", \"-o\", str(test_file)], input=compressed_foo_foo_foo)\n            self.assertEqual(result.exit_code, 0)\n            self.assertEqual(test_file.read_text(), \"foo foo foo\")\n\n    def test_version(self):\n        result = runner.invoke(app, [\"--version\"])\n        self.assertEqual(result.exit_code, 0)", ""]}
{"filename": "tests/test_compressor_decompressor.py", "chunked_list": ["import random\nimport unittest\nfrom io import BytesIO\n\ntry:\n    import micropython\nexcept ImportError:\n    micropython = None\n\nfrom tamp.compressor import Compressor as PyCompressor", "\nfrom tamp.compressor import Compressor as PyCompressor\nfrom tamp.decompressor import Decompressor as PyDecompressor\n\nif micropython is None:\n    ViperCompressor = None\n    ViperDecompressor = None\nelse:\n    from tamp.compressor_viper import Compressor as ViperCompressor\n    from tamp.decompressor_viper import Decompressor as ViperDecompressor", "\ntry:\n    from tamp._c_compressor import Compressor as CCompressor\n    from tamp._c_decompressor import Decompressor as CDecompressor\nexcept ImportError:\n    CCompressor = None\n    CDecompressor = None\n\n\nCompressors = (PyCompressor, CCompressor, ViperCompressor)", "\nCompressors = (PyCompressor, CCompressor, ViperCompressor)\nDecompressors = (PyDecompressor, CDecompressor, ViperDecompressor)\n\n\ndef walk_compressors_decompressors():\n    \"\"\"Iterate over all available compressor/decompressor combintations.\"\"\"\n    for Compressor in Compressors:\n        if Compressor is None:\n            continue\n\n        for Decompressor in Decompressors:\n            if Decompressor is None:\n                continue\n\n            yield (Compressor, Decompressor)", "\n\ntale_of_two_cities = b\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way - in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\"\n\n\nclass TestCompressorAndDecompressor(unittest.TestCase):\n    def _autotest(self, num_bytes, n_bits, compressor_kwargs=None):\n        if compressor_kwargs is None:\n            compressor_kwargs = {}\n\n        data = bytearray(random.randint(0, (1 << n_bits) - 1) for x in range(num_bytes))\n\n        for Compressor, Decompressor in walk_compressors_decompressors():\n            # Compress/Decompress random data\n            with BytesIO() as f, self.subTest(\n                data=\"Random\",\n                Compressor=Compressor,\n                Decompressor=Decompressor,\n            ):\n                c = Compressor(f, **compressor_kwargs)\n                c.write(data)\n                c.flush()\n\n                f.seek(0)\n                d = Decompressor(f)\n                actual = d.read()\n\n                self.assertEqual(actual, data)\n\n            # Compress/Decompress\n            data = bytearray(1 for _ in range(num_bytes))\n            with BytesIO() as f, self.subTest(\n                data=\"Sequential\",\n                Compressor=Compressor,\n                Decompressor=Decompressor,\n            ):\n                c = Compressor(f, **compressor_kwargs)\n                c.write(data)\n                c.flush()\n\n                f.seek(0)\n                d = Decompressor(f)\n                actual = d.read()\n\n                self.assertEqual(len(actual), len(data))\n                self.assertEqual(actual, data)\n\n    def test_default(self):\n        self._autotest(10_000, 8)\n\n    def test_7bit(self):\n        self._autotest(10_000, 7, compressor_kwargs={\"literal\": 7})\n\n    def test_6bit(self):\n        self._autotest(10_000, 6, compressor_kwargs={\"literal\": 6})\n\n    def test_5bit(self):\n        self._autotest(10_000, 5, compressor_kwargs={\"literal\": 5})\n\n    def test_tale_of_two_cities(self):\n        assert len(tale_of_two_cities) > (1 << 8)\n        for Compressor, Decompressor in walk_compressors_decompressors():\n            with BytesIO() as f:\n                c = Compressor(f, window=8)\n                c.write(tale_of_two_cities)\n                c.flush()\n\n                f.seek(0)\n                d = Decompressor(f)\n                actual = d.read()\n\n                assert actual == tale_of_two_cities", "\n\nif __name__ == \"__main__\":\n    unittest.main()\n"]}
{"filename": "tests/test_file_interface.py", "chunked_list": ["import unittest\nfrom pathlib import Path\nfrom tempfile import TemporaryDirectory\n\nimport tamp\n\n\nclass TestFileInterface(unittest.TestCase):\n    def test_open_wb(self):\n        with TemporaryDirectory() as tmp_dir:\n            tmp_dir = Path(tmp_dir)\n            fn = tmp_dir / \"file.tamp\"\n            f = tamp.open(fn, \"wb\")\n            self.assertIsInstance(f, tamp.Compressor)\n            f.close()\n\n    def test_open_wb_then_rb(self):\n        with TemporaryDirectory() as tmp_dir:\n            tmp_dir = Path(tmp_dir)\n            fn = tmp_dir / \"file.tamp\"\n\n            f = tamp.open(fn, \"wb\")\n            f.close()\n\n            f = tamp.open(fn, \"rb\")\n            self.assertIsInstance(f, tamp.Decompressor)\n            f.close()\n\n    def test_open_context_manager_read_write(self):\n        with TemporaryDirectory() as tmp_dir:\n            tmp_dir = Path(tmp_dir)\n            fn = tmp_dir / \"file.tamp\"\n\n            test_string = b\"test string is best string\"\n            with tamp.open(fn, \"wb\") as f:\n                f.write(test_string)\n\n            with tamp.open(fn, \"rb\") as f:\n                actual = f.read()\n\n            assert test_string == actual\n\n    def test_encoding(self):\n        with TemporaryDirectory() as tmp_dir:\n            tmp_dir = Path(tmp_dir)\n            fn = tmp_dir / \"file.tamp\"\n\n            test_string = \"test string is best string\"\n            with tamp.open(fn, \"w\") as f:\n                f.write(test_string)\n\n            with tamp.open(fn, \"r\") as f:\n                actual = f.read()\n\n            assert test_string == actual\n\n    def test_bad_modes(self):\n        with self.assertRaises(ValueError):\n            tamp.open(None, \"abc\")  # type: ignore[reportGeneralTypeIssues]\n\n        with self.assertRaises(ValueError):\n            tamp.open(None, \"rw\")  # type: ignore[reportGeneralTypeIssues]", ""]}
{"filename": "tests/test_pseudorandom.py", "chunked_list": ["import unittest\n\nimport tamp.compressor\nimport tamp.decompressor\nfrom tamp import initialize_dictionary\n\ntry:\n    import micropython\nexcept ImportError:\n    micropython = None", "\n\nclass TestPseudoRandom(unittest.TestCase):\n    def test_256_compressor_zero_seed(self):\n        self.assertEqual(initialize_dictionary(256, seed=0), bytearray(256))\n\n    def test_256_compressor_nonzero_seed(self):\n        self.assertNotEqual(initialize_dictionary(256, seed=1), bytearray(256))\n\n    def test_256_compressor(self):\n        actual = tamp.compressor.initialize_dictionary(256)\n        self.assertEqual(len(actual), 256)\n        self.assertEqual(\n            actual,\n            b\"\\x00.//r.0. t>\\n/>snas.trnr i\\x00r/a\\x00snat./.r\\x00i o.s tneo>.as>\\na.ta\\x00 aa\\x00\\x00\\x000oe ri\\x00a>eatsi\\n.\\ni.str\\n//snesr.ost<  \\x00\\ni\\neoa\\x00se0.o\\n\\n>aori>n0.>./.oonen0<\\x00<r o\\n\\naas0< ai\\n0\\x00na\\x00e><.\\noas to \\n></se>>ts/oreatinter.n0 >s\\n/.e.><. r si<>/<san\\x00ae t 0.r.o/0./a r/ttn nn.<re.t0 \\x00r\\x00ro\",\n        )\n\n    def test_256_decompressor(self):\n        actual = tamp.decompressor.initialize_dictionary(256)\n        self.assertEqual(len(actual), 256)\n        self.assertEqual(\n            actual,\n            b\"\\x00.//r.0. t>\\n/>snas.trnr i\\x00r/a\\x00snat./.r\\x00i o.s tneo>.as>\\na.ta\\x00 aa\\x00\\x00\\x000oe ri\\x00a>eatsi\\n.\\ni.str\\n//snesr.ost<  \\x00\\ni\\neoa\\x00se0.o\\n\\n>aori>n0.>./.oonen0<\\x00<r o\\n\\naas0< ai\\n0\\x00na\\x00e><.\\noas to \\n></se>>ts/oreatinter.n0 >s\\n/.e.><. r si<>/<san\\x00ae t 0.r.o/0./a r/ttn nn.<re.t0 \\x00r\\x00ro\",\n        )\n\n    @unittest.skipIf(micropython is None, \"not running micropython\")\n    def test_256_compressor_viper(self):\n        import tamp.compressor_viper\n\n        actual = tamp.compressor_viper.initialize_dictionary(256)\n        self.assertEqual(len(actual), 256)\n        self.assertEqual(\n            actual,\n            b\"\\x00.//r.0. t>\\n/>snas.trnr i\\x00r/a\\x00snat./.r\\x00i o.s tneo>.as>\\na.ta\\x00 aa\\x00\\x00\\x000oe ri\\x00a>eatsi\\n.\\ni.str\\n//snesr.ost<  \\x00\\ni\\neoa\\x00se0.o\\n\\n>aori>n0.>./.oonen0<\\x00<r o\\n\\naas0< ai\\n0\\x00na\\x00e><.\\noas to \\n></se>>ts/oreatinter.n0 >s\\n/.e.><. r si<>/<san\\x00ae t 0.r.o/0./a r/ttn nn.<re.t0 \\x00r\\x00ro\",\n        )\n\n    @unittest.skipIf(micropython is None, \"not running micropython\")\n    def test_256_decompressor_viper(self):\n        import tamp.decompressor_viper\n\n        actual = tamp.decompressor_viper.initialize_dictionary(256)\n        self.assertEqual(len(actual), 256)\n        self.assertEqual(\n            actual,\n            b\"\\x00.//r.0. t>\\n/>snas.trnr i\\x00r/a\\x00snat./.r\\x00i o.s tneo>.as>\\na.ta\\x00 aa\\x00\\x00\\x000oe ri\\x00a>eatsi\\n.\\ni.str\\n//snesr.ost<  \\x00\\ni\\neoa\\x00se0.o\\n\\n>aori>n0.>./.oonen0<\\x00<r o\\n\\naas0< ai\\n0\\x00na\\x00e><.\\noas to \\n></se>>ts/oreatinter.n0 >s\\n/.e.><. r si<>/<san\\x00ae t 0.r.o/0./a r/ttn nn.<re.t0 \\x00r\\x00ro\",\n        )", "\n\nif __name__ == \"__main__\":\n    unittest.main()\n"]}
{"filename": "tests/test_decompressor.py", "chunked_list": ["import unittest\nfrom io import BytesIO\n\ntry:\n    import micropython\nexcept ImportError:\n    micropython = None\n\nDecompressors = []\ndecompresses = []", "Decompressors = []\ndecompresses = []\n\nif micropython is None:\n    from tamp.decompressor import Decompressor as PyDecompressor\n    from tamp.decompressor import decompress as py_decompress\n\n    Decompressors.append(PyDecompressor)\n    decompresses.append(py_decompress)\n\n    try:\n        from tamp._c_decompressor import Decompressor as CDecompressor\n        from tamp._c_decompressor import decompress as c_decompress\n\n        Decompressors.append(CDecompressor)\n        decompresses.append(c_decompress)\n    except ImportError:\n        pass\n\nelse:\n    from tamp.decompressor_viper import Decompressor as ViperDecompressor\n    from tamp.decompressor_viper import decompress as viper_decompress\n\n    Decompressors.append(ViperDecompressor)\n    decompresses.append(viper_decompress)", "\n\nclass TestDecompressor(unittest.TestCase):\n    def test_decompressor_basic(self):\n        for Decompressor in Decompressors:\n            with self.subTest(Decompressor=Decompressor):\n                expected = b\"foo foo foo\"\n\n                compressed = bytes(\n                    # fmt: off\n                    [\n                        0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n                        0b1_0110011,    # literal \"f\"\n                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n                                        # size=2 -> 0b0\n                                        # 131 -> 0b0010000011\n                        0b00011_1_00,   # literal \" \"\n                        0b100000_0_1,   # There is now \"foo \" at index 0\n                        0b000_00000,    # size=4 -> 0b1000\n                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n                        0b00000000,     # index 0 -> 0b0000000000\n                        0b00_000000,    # 6 bits of zero-padding\n                    ]\n                    # fmt: on\n                )\n\n                with BytesIO(compressed) as f:\n                    decompressor = Decompressor(f)\n                    actual = decompressor.read()\n\n                self.assertEqual(actual, expected)\n\n    def test_decompressor_restricted_read_size(self):\n        for Decompressor in Decompressors:\n            with self.subTest(Decompressor=Decompressor):\n                compressed = bytes(\n                    # fmt: off\n                    [\n                        0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n                        0b1_0110011,    # literal \"f\"\n                        0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n                                        # size=2 -> 0b0\n                                        # 131 -> 0b0010000011\n                        0b00011_1_00,   # literal \" \"\n                        0b100000_0_1,   # There is now \"foo \" at index 0\n                        0b000_00000,    # size=4 -> 0b1000\n                        0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n                        0b00000000,     # index 0 -> 0b0000000000\n                        0b00_000000,    # 6 bits of zero-padding\n                    ]\n                    # fmt: on\n                )\n\n                with BytesIO(compressed) as f:\n                    decompressor = Decompressor(f)\n                    self.assertEqual(decompressor.read(4), b\"foo \")\n                    self.assertEqual(decompressor.read(2), b\"fo\")\n                    self.assertEqual(decompressor.read(-1), b\"o foo\")\n\n    def test_decompressor_flushing(self):\n        for decompress in decompresses:\n            with self.subTest(decompress=decompress):\n                compressed = bytes(\n                    # fmt: off\n                    [\n                        0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n                        0b1_0101000,  # literal 'Q'\n                        0b1_0_101010,  # FLUSH_CODE\n                        0b11_000000,   # FLUSH CONTINUE\n                        0b1_0101011,  # literal 'W'\n                        0b1_0_101010,  # FLUSH_CODE\n                        0b11_000000,   # FLUSH CONTINUE\n\n                    ]\n                    # fmt: on\n                )\n                decoded = decompress(compressed)\n                self.assertEqual(decoded, b\"QW\")\n\n    def test_decompressor_missing_dict(self):\n        for Decompressor in Decompressors:\n            with self.subTest(Decompressor=Decompressor), self.assertRaises(ValueError), BytesIO(\n                bytes([0b000_10_1_0_0])\n            ) as f:\n                Decompressor(f)\n\n    def test_decompressor_full_output_dst_immediately_after_src(self):\n        # Decompressor's perspective of window\n        # compressed data: b\"z\"\n        #    * 0 write literal \"a\"   -> b\"abcd\"\n        #    * 1 write pattern \"abc\"\n        custom_dictionary = bytearray(1024)\n        custom_dictionary_init = b\"abcd\"\n        custom_dictionary[: len(custom_dictionary_init)] = custom_dictionary_init\n\n        data = bytes(\n            [\n                # fmt: off\n\n                # header (window_bits=10, literal_bits=8, custom)\n                0b010_11_1_0_0,\n                0b1_0110000,       # literal \"a\"\n                0b1_0_11_0000,  # token \"abc\"\n                0b000000_00,\n                # 2-bit padding\n                # fmt: on\n            ]\n        )\n\n        for Decompressor in Decompressors:\n            with self.subTest(Decompressor=Decompressor):\n                with BytesIO(data) as f:\n                    # Sanity check that without limiting output, it decompresses correctly.\n                    decompressor = Decompressor(f, dictionary=bytearray(custom_dictionary))\n                    self.assertEqual(decompressor.read(), b\"aabc\")\n\n                with BytesIO(data) as f:\n                    decompressor = Decompressor(f, dictionary=bytearray(custom_dictionary))\n                    self.assertEqual(decompressor.read(1), b\"a\")\n                    self.assertEqual(decompressor.read(1), b\"a\")\n                    self.assertEqual(decompressor.read(1), b\"b\")\n                    self.assertEqual(decompressor.read(1), b\"c\")\n\n    def test_decompressor_partial_token(self):\n        compressed = bytes(\n            # fmt: off\n            [\n                0b010_11_00_0,  # header (window_bits=10, literal_bits=8)\n                0b1_0110011,    # literal \"f\"\n                0b0_0_0_00100,  # the pre-init buffer contains \"oo\" at index 131\n                                # size=2 -> 0b0\n                                # 131 -> 0b0010000011\n                0b00011_1_00,   # literal \" \"\n                0b100000_0_1,   # There is now \"foo \" at index 0\n                0b000_00000,    # size=4 -> 0b1000\n                ####################### - stream-break here\n                0b00000_0_11,   # Just \"foo\" at index 0; size=3 -> 0b11\n                0b00000000,     # index 0 -> 0b0000000000\n                0b00_000000,    # 6 bits of zero-padding\n            ]\n            # fmt: on\n        )\n\n        expected = b\"foo foo foo\"\n\n        for Decompressor in Decompressors:\n            with self.subTest(Decompressor=Decompressor), BytesIO(compressed[:6]) as f:\n                decompressor = Decompressor(f)\n                read0 = decompressor.read()\n\n                f.write(compressed[6:])\n                f.seek(6)\n\n                read1 = decompressor.read()\n                self.assertEqual(read0 + read1, expected)", ""]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\nimport importlib\nimport inspect", "import importlib\nimport inspect\nimport sys\nfrom datetime import date\nfrom pathlib import Path\n\nimport git\n\nsys.path.insert(0, str(Path(\"../..\").absolute()))\n", "sys.path.insert(0, str(Path(\"../..\").absolute()))\n\n\nfrom tamp import __version__\n\ngit_repo = git.Repo(\".\", search_parent_directories=True)\ngit_commit = git_repo.head.commit\n\n# -- Project information -----------------------------------------------------\n", "# -- Project information -----------------------------------------------------\n\nproject = \"tamp\"\ncopyright = f\"{date.today().year}, Brian Pugh\"\nauthor = \"Brian Pugh\"\n\n# The short X.Y version.\nversion = __version__\n# The full version, including alpha/beta/rc tags\nrelease = __version__", "# The full version, including alpha/beta/rc tags\nrelease = __version__\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [", "# ones.\nextensions = [\n    \"sphinx_rtd_theme\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.linkcode\",\n    \"sphinx_copybutton\",\n]\n\n# Add any paths that contain templates here, relative to this directory.", "\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\nsmartquotes = False", "\nsmartquotes = False\n\n# Napoleon settings\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = False\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False", "napoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\nnapoleon_preprocess_types = False\nnapoleon_type_aliases = None\nnapoleon_attr_annotations = True", "napoleon_type_aliases = None\nnapoleon_attr_annotations = True\n\n# Autodoc\nautodoc_default_options = {\n    \"members\": True,\n    \"member-order\": \"bysource\",\n    \"undoc-members\": True,\n    \"exclude-members\": \"__weakref__\",\n    \"inherited-members\": True,", "    \"exclude-members\": \"__weakref__\",\n    \"inherited-members\": True,\n}\nautoclass_content = \"both\"\n\n\n# LinkCode\ncode_url = f\"https://github.com/brianpugh/tamp/blob/{git_commit}\"\n\n\ndef linkcode_resolve(domain, info):\n    \"\"\"Link code to github.\n\n    Modified from:\n        https://github.com/python-websockets/websockets/blob/778a1ca6936ac67e7a3fe1bbe585db2eafeaa515/docs/conf.py#L100-L134\n    \"\"\"\n    # Non-linkable objects from the starter kit in the tutorial.\n    if domain == \"js\":\n        return\n\n    if domain != \"py\":\n        raise ValueError(\"expected only Python objects\")\n\n    if not info.get(\"module\"):\n        # Documented via py:function::\n        return\n\n    mod = importlib.import_module(info[\"module\"])\n    if \".\" in info[\"fullname\"]:\n        objname, attrname = info[\"fullname\"].split(\".\")\n        obj = getattr(mod, objname)\n        try:\n            # object is a method of a class\n            obj = getattr(obj, attrname)\n        except AttributeError:\n            # object is an attribute of a class\n            return None\n    else:\n        obj = getattr(mod, info[\"fullname\"])\n\n    try:\n        file = inspect.getsourcefile(obj)\n        lines = inspect.getsourcelines(obj)\n    except TypeError:\n        # e.g. object is a typing.Union\n        return None\n    except OSError:\n        # Source code is not available (e.g. cython)\n        return None\n    if file is None:\n        return None\n    file = Path(file).resolve().relative_to(git_repo.working_dir)\n    if file.parts[0] != \"tamp\":\n        # e.g. object is a typing.NewType\n        return None\n    start, end = lines[1], lines[1] + len(lines[0]) - 1\n\n    return f\"{code_url}/{file}#L{start}-L{end}\"", "\n\ndef linkcode_resolve(domain, info):\n    \"\"\"Link code to github.\n\n    Modified from:\n        https://github.com/python-websockets/websockets/blob/778a1ca6936ac67e7a3fe1bbe585db2eafeaa515/docs/conf.py#L100-L134\n    \"\"\"\n    # Non-linkable objects from the starter kit in the tutorial.\n    if domain == \"js\":\n        return\n\n    if domain != \"py\":\n        raise ValueError(\"expected only Python objects\")\n\n    if not info.get(\"module\"):\n        # Documented via py:function::\n        return\n\n    mod = importlib.import_module(info[\"module\"])\n    if \".\" in info[\"fullname\"]:\n        objname, attrname = info[\"fullname\"].split(\".\")\n        obj = getattr(mod, objname)\n        try:\n            # object is a method of a class\n            obj = getattr(obj, attrname)\n        except AttributeError:\n            # object is an attribute of a class\n            return None\n    else:\n        obj = getattr(mod, info[\"fullname\"])\n\n    try:\n        file = inspect.getsourcefile(obj)\n        lines = inspect.getsourcelines(obj)\n    except TypeError:\n        # e.g. object is a typing.Union\n        return None\n    except OSError:\n        # Source code is not available (e.g. cython)\n        return None\n    if file is None:\n        return None\n    file = Path(file).resolve().relative_to(git_repo.working_dir)\n    if file.parts[0] != \"tamp\":\n        # e.g. object is a typing.NewType\n        return None\n    start, end = lines[1], lines[1] + len(lines[0]) - 1\n\n    return f\"{code_url}/{file}#L{start}-L{end}\"", "\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Add any paths that contain custom static files (such as style sheets) here,", "\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\nhtml_title = project\nhtml_logo = \"../../assets/logo_300w.png\"\nhtml_favicon = \"../../assets/favicon-16x16.png\"\n", "html_favicon = \"../../assets/favicon-16x16.png\"\n\nhtml_theme_options = {\n    # \"analytics_id\": \"G-XXXXXXXXXX\",  # Provided by Google in your dashboard\n    # \"analytics_anonymize_ip\": False,\n    \"logo_only\": True,\n    \"display_version\": False,\n    \"prev_next_buttons_location\": \"bottom\",\n    \"style_external_links\": False,\n    \"vcs_pageview_mode\": \"\",", "    \"style_external_links\": False,\n    \"vcs_pageview_mode\": \"\",\n    \"style_nav_header_background\": \"white\",\n    # Toc options\n    \"collapse_navigation\": True,\n    \"sticky_navigation\": True,\n    \"navigation_depth\": 4,\n    \"includehidden\": True,\n    \"titles_only\": False,\n}", "    \"titles_only\": False,\n}\n\nhtml_context = {\n    # Github options\n    \"display_github\": True,\n    \"github_user\": \"BrianPugh\",\n    \"github_repo\": \"tamp\",\n    \"github_version\": \"main\",\n    \"conf_py_path\": \"/docs/source/\",", "    \"github_version\": \"main\",\n    \"conf_py_path\": \"/docs/source/\",\n}\n\nhtml_css_files = [\n    \"custom.css\",\n]\n"]}
