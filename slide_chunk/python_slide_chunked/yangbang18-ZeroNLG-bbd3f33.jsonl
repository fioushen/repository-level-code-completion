{"filename": "setup.py", "chunked_list": ["import os\nfrom setuptools import setup, find_packages\n\n\ndef get_install_requirements():\n    requirements = []\n    requirements_file = os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), \"requirements.txt\"\n    )\n    with open(requirements_file) as f_req:\n        for line in f_req:\n            line = line.strip()\n            if not line.startswith(\"#\") and len(line) > 0:\n                requirements.append(line)\n\n    return requirements", "\nsetup(\n    name=\"zeronlg\",\n    version=\"1.0.0\",\n    author=\"Bang Yang\",\n    author_email=\"yangbang@pku.edu.cn\",\n    description=\"Zero-Shot Multimodal and Multilingual Natural Language Generation\",\n    license=\"Apache License 2.0\",\n    packages=find_packages(),\n    python_requires=\">=3.6.0\",", "    packages=find_packages(),\n    python_requires=\">=3.6.0\",\n    install_requires=get_install_requirements(),\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\"\n    ],", "        \"Topic :: Scientific/Engineering :: Artificial Intelligence\"\n    ],\n)\n"]}
{"filename": "configs.py", "chunked_list": ["# root paths to load raw images or videos\nimage_video_root = {\n    'coco': 'data/MSCOCO',\n    'flickr30k': 'data/Flickr30k',\n    'msrvtt': 'data/MSRVTT',\n    'vatex': 'data/VATEX',\n}\n\nnum_frames = 8 # the number of frames to be uniformly sampled for each video\n", "num_frames = 8 # the number of frames to be uniformly sampled for each video\n\nannotation_root = 'data/annotations'\ncorpus_root = 'data/corpus'\n\n# generation settings for visual captioning in different languages\nauto_settings = {\n    'en': dict(\n        max_length=20,\n        min_length=3,", "        max_length=20,\n        min_length=3,\n        repetition_penalty=1.0,\n    ),\n    'zh': dict(\n        max_length=30,\n        min_length=3,\n        repetition_penalty=1.0,\n    ),\n    'de': dict(", "    ),\n    'de': dict(\n        max_length=15,\n        min_length=3,\n        repetition_penalty=2.0,\n    ),\n    'fr': dict(\n        max_length=20,\n        min_length=3,\n        repetition_penalty=2.0,", "        min_length=3,\n        repetition_penalty=2.0,\n    ),\n}\n"]}
{"filename": "infer_translate.py", "chunked_list": ["\nimport os\nimport time\nimport datetime\nimport argparse\nimport logging\n\nimport configs\n\nfrom torch.utils.data import DataLoader", "\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler\nfrom zeronlg import ZeroNLG, TranslateDataset, TranslateEvaluator\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])", "                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nlogger = logging.getLogger(__name__)\n\ntry:\n    ROOT = configs.annotation_translate_root\nexcept:\n    ROOT = configs.annotation_root\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, required=True)\n    parser.add_argument('--dataset', type=str, default='flickr30k')\n    \n    # Data paths and attributes\n    parser.add_argument('--data_root', type=str, help='If not specified, default to {ROOT}/{dataset}')\n    parser.add_argument('--folder_format', type=str, default='{source}-{target}')\n    parser.add_argument('--image_list_format', type=str, default='{mode}_images.txt',\n        help='To run multimodal machine translation, you should specify a file that stores relative path of images'\n    )\n    \n    # Dataloader settings\n    parser.add_argument('--batch_size', type=int, default=64)\n    \n    # Evaluation settings\n    parser.add_argument('--source', type=str, default='en', help='source language')\n    parser.add_argument('--target', type=str, default='zh', help='target language')\n    parser.add_argument('--unidirectional', action='store_true', help='if specified, only evaluating source -> target')\n    parser.add_argument('--no_score', action='store_true', help='do not calculate scores')\n    parser.add_argument('--modes', type=str, nargs='+', default=['test'], help='evaluation modes: [\"val\"], [\"test\"], [\"val\", \"test\"]')\n    parser.add_argument('--num_beams', type=int, default=3)\n    parser.add_argument('--max_length', type=int, default=128)\n    parser.add_argument('--min_length', type=int, default=5)\n    parser.add_argument('--repetition_penalty', type=float, default=1.0)\n\n    # Output settings\n    parser.add_argument('--output_path', type=str, help='If not specified, output_path will be ${model}/evaluations_translate/${source}-${target}')\n    parser.add_argument('--no_suffix_folder', action='store_true', help='If True, the suffix `evaluations_translate/${source}-${target}` will not be joined to the output path')\n    parser.add_argument('--print_sent', action='store_true')\n    args = parser.parse_args()\n\n    data_root = args.data_root or os.path.join(ROOT, args.dataset)\n    read_path1 = os.path.join(data_root, args.folder_format.format(source=args.source, target=args.target))\n    read_path2 = os.path.join(data_root, args.folder_format.format(source=args.target, target=args.source))\n    assert os.path.exists(read_path1) or os.path.exists(read_path2), f'{read_path1} or {read_path2} do not exist!'\n    read_path = read_path1 if os.path.exists(read_path1) else read_path2\n\n    if not os.path.exists(args.model):\n        assert args.output_path, \"you are training to load a model from hugginface hub, please specify --output_path\"\n\n    output_path = args.output_path or args.model\n    if not args.no_suffix_folder: \n        output_path = os.path.join(output_path, 'evaluations_translate', f'{args.source}-{args.target}')\n\n    os.makedirs(output_path, exist_ok=True)\n    logger.addHandler(logging.FileHandler(os.path.join(output_path, 'log.txt'), 'w', encoding='utf-8'))\n    logger.info(f'output path: {output_path}')\n\n    assert args.modes in [['val'], ['test'], ['val', 'test']]\n\n    logger.info(f'Creating model from {args.model}')\n    model = ZeroNLG(args.model)\n\n    # prepare evaluation settings\n    evaluation_settings = {\n        k: getattr(args, k) \n        for k in ['num_beams', 'max_length', 'min_length', 'repetition_penalty']\n    }\n    logger.info(f'Evaluation settings: {evaluation_settings}')\n\n    # start evaluation\n    start_time = time.time()\n    for mode in args.modes:\n        if args.unidirectional:\n            sources = [args.source]\n            targets = [args.target]\n        else:\n            sources = [args.source, args.target]\n            targets = [args.target, args.source]\n\n        for source, target in zip(sources, targets):\n\n            source_path = os.path.join(read_path, f'{mode}.{source}')\n            target_path = os.path.join(read_path, f'{mode}.{target}')\n\n            dataset = TranslateDataset(\n                source_language=source,\n                target_language=target,\n                source_path=source_path,\n                target_path=target_path,\n                logger=logger\n            )\n\n            loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n            \n            evaluator = TranslateEvaluator(\n                loader=loader,\n                evaluation_settings=evaluation_settings,\n                mode=mode, \n                logger=logger\n            )\n\n            evaluator(model, output_path=output_path, no_score=args.no_score, print_sent=args.print_sent)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Time {}'.format(total_time_str))", "\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, required=True)\n    parser.add_argument('--dataset', type=str, default='flickr30k')\n    \n    # Data paths and attributes\n    parser.add_argument('--data_root', type=str, help='If not specified, default to {ROOT}/{dataset}')\n    parser.add_argument('--folder_format', type=str, default='{source}-{target}')\n    parser.add_argument('--image_list_format', type=str, default='{mode}_images.txt',\n        help='To run multimodal machine translation, you should specify a file that stores relative path of images'\n    )\n    \n    # Dataloader settings\n    parser.add_argument('--batch_size', type=int, default=64)\n    \n    # Evaluation settings\n    parser.add_argument('--source', type=str, default='en', help='source language')\n    parser.add_argument('--target', type=str, default='zh', help='target language')\n    parser.add_argument('--unidirectional', action='store_true', help='if specified, only evaluating source -> target')\n    parser.add_argument('--no_score', action='store_true', help='do not calculate scores')\n    parser.add_argument('--modes', type=str, nargs='+', default=['test'], help='evaluation modes: [\"val\"], [\"test\"], [\"val\", \"test\"]')\n    parser.add_argument('--num_beams', type=int, default=3)\n    parser.add_argument('--max_length', type=int, default=128)\n    parser.add_argument('--min_length', type=int, default=5)\n    parser.add_argument('--repetition_penalty', type=float, default=1.0)\n\n    # Output settings\n    parser.add_argument('--output_path', type=str, help='If not specified, output_path will be ${model}/evaluations_translate/${source}-${target}')\n    parser.add_argument('--no_suffix_folder', action='store_true', help='If True, the suffix `evaluations_translate/${source}-${target}` will not be joined to the output path')\n    parser.add_argument('--print_sent', action='store_true')\n    args = parser.parse_args()\n\n    data_root = args.data_root or os.path.join(ROOT, args.dataset)\n    read_path1 = os.path.join(data_root, args.folder_format.format(source=args.source, target=args.target))\n    read_path2 = os.path.join(data_root, args.folder_format.format(source=args.target, target=args.source))\n    assert os.path.exists(read_path1) or os.path.exists(read_path2), f'{read_path1} or {read_path2} do not exist!'\n    read_path = read_path1 if os.path.exists(read_path1) else read_path2\n\n    if not os.path.exists(args.model):\n        assert args.output_path, \"you are training to load a model from hugginface hub, please specify --output_path\"\n\n    output_path = args.output_path or args.model\n    if not args.no_suffix_folder: \n        output_path = os.path.join(output_path, 'evaluations_translate', f'{args.source}-{args.target}')\n\n    os.makedirs(output_path, exist_ok=True)\n    logger.addHandler(logging.FileHandler(os.path.join(output_path, 'log.txt'), 'w', encoding='utf-8'))\n    logger.info(f'output path: {output_path}')\n\n    assert args.modes in [['val'], ['test'], ['val', 'test']]\n\n    logger.info(f'Creating model from {args.model}')\n    model = ZeroNLG(args.model)\n\n    # prepare evaluation settings\n    evaluation_settings = {\n        k: getattr(args, k) \n        for k in ['num_beams', 'max_length', 'min_length', 'repetition_penalty']\n    }\n    logger.info(f'Evaluation settings: {evaluation_settings}')\n\n    # start evaluation\n    start_time = time.time()\n    for mode in args.modes:\n        if args.unidirectional:\n            sources = [args.source]\n            targets = [args.target]\n        else:\n            sources = [args.source, args.target]\n            targets = [args.target, args.source]\n\n        for source, target in zip(sources, targets):\n\n            source_path = os.path.join(read_path, f'{mode}.{source}')\n            target_path = os.path.join(read_path, f'{mode}.{target}')\n\n            dataset = TranslateDataset(\n                source_language=source,\n                target_language=target,\n                source_path=source_path,\n                target_path=target_path,\n                logger=logger\n            )\n\n            loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n            \n            evaluator = TranslateEvaluator(\n                loader=loader,\n                evaluation_settings=evaluation_settings,\n                mode=mode, \n                logger=logger\n            )\n\n            evaluator(model, output_path=output_path, no_score=args.no_score, print_sent=args.print_sent)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Time {}'.format(total_time_str))", "\n'''\npython infer_translate.py --model zeronlg-4langs-mt --output_path output/zeronlg-4langs-mt --dataset flickr30k --source zh --target de\n'''\n"]}
{"filename": "train.py", "chunked_list": ["import os\nimport logging\nimport torch.nn as nn\nimport argparse\n\nfrom torch.utils.data import DataLoader\n\nfrom sentence_transformers import LoggingHandler\nfrom sentence_transformers.models import Pooling\n", "from sentence_transformers.models import Pooling\n\nfrom zeronlg import Framework\nfrom zeronlg.losses import LossManager\nfrom zeronlg.datasets import PretrainDataset\nfrom zeronlg.models import Dense, Projector, Decoder, Transformer\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,", "                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nlogger = logging.getLogger(__name__)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    # Model settings\n    parser.add_argument('--teacher_model_name', type=str, default='clip-ViT-B-32', \n                        choices=['clip-ViT-B-32', 'clip-ViT-B-16', 'clip-ViT-L-14'], \n                        help='Monolingual teacher model')\n    parser.add_argument('--student_model_name', type=str, default='distilbert-base-multilingual-cased',  \n                        help='Multilingual student model we use to imitate the teacher model\\' outputs')\n    parser.add_argument('--decoder_name', type=str, default='bert-base-multilingual-cased', \n                        help='Multilingual student model for NLG')\n    # Student's encoder settings\n    parser.add_argument('--max_seq_length', type=int, default=128, \n                        help='Student model max. lengths for inputs (number of word pieces)')\n    \n    # Student's decoder settings\n    parser.add_argument('--num_hidden_layers', type=int, default=3)\n    parser.add_argument('--use_pretrained_decoder', action='store_true', \n                        help='Whether to load pre-trianed weights of the decoder; \\\n                              Note that we will add randomly initialized cross-attention layers to the decdoder, \\\n                              even if you specify `use_pretrained_decoder` to True')\n    parser.add_argument('--decoder_max_seq_length', type=int, default=128, \n                        help='Student model max. lengths for decoder inputs (number of word pieces)')\n\n    # Data settings\n    parser.add_argument('--train_corpus_format', type=str, default=\"data/corpus/multilingual_cc3m/4langs/cc3m_{}-{}.tsv\")\n    parser.add_argument('--source_language', type=str, default='en', choices=['en'], \n                        help='Our teacher model accepts English (en) sentences')\n    parser.add_argument('--target_languages', type=str, nargs='+', default=['en', 'zh', 'de', 'fr'], \n                        help='The languages to be learned by the student model')\n    parser.add_argument('--max_sentences', type=int, help='maximun number of sentences per file')\n    parser.add_argument('--weights', type=int, nargs='+', help='If more than one dataset is loaded with load_data: With which frequency should data be sampled from this dataset?')\n    parser.add_argument('--numpy_path', type=str, help='Path to a numpy file that stores sentence embeddings')\n    parser.add_argument('--num_workers', type=int, default=4, help='# workers to load data; only activated when `numpy_path` is specified')\n\n    # Training settings\n    parser.add_argument('--use_amp', action='store_true', help='Whether use automatic mixed precision (amp) to speed up training')\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n    parser.add_argument('--inference_batch_size', type=int, help='Batch size at inference; if not speficied, set to batch_size')\n    parser.add_argument('--epochs', type=int, default=3, help='Train for x epochs')\n    parser.add_argument('--warmup_steps', type=int, default=5000, help='Warumup steps')\n    parser.add_argument('--lr', type=float, default=2e-5, help='Learning rate')\n    parser.add_argument('--eps', type=float, default=1e-6)\n    parser.add_argument('--weight_decay', type=float, default=0.01)\n    parser.add_argument('--scheduler', type=str, default='warmupconstant', \n                        choices=['constantlr', 'warmupconstant', 'warmuplinear', 'warmupcosine', 'warmupcosinewithhardrestarts'])\n\n    # Output settings\n    parser.add_argument('--output_path', type=str, help='The exact output path to save training info and checkpoints')\n    parser.add_argument('--output_root', type=str, default='output/2stages')\n    parser.add_argument('--exp_name', type=str, default='debug', help='Experiment name; If `output_path` is not specified, the output path will be {output_root}/{exp_name}')\n    parser.add_argument('--log_every', type=int, default=500)\n    \n    # Method-related settings\n    parser.add_argument('--scales', type=float, nargs='+', default=[1.0, 0.0, 0.0, 0.0], \n                        help='Scales of loss_mse, loss_at_teacher, loss_at_student, loss_contrastive')\n    parser.add_argument('--use_masking', action='store_true',\n                        help='Wheter to apply input corruption, i.e., randomly mask encoder\\'s input tokens')\n    parser.add_argument('--mask_prob', type=float, default=0.15,\n                        help='Probability to mask tokens')\n    parser.add_argument('--noise_std', type=float, default=0,\n                        help='Standard deviation of gaussian noise; 0 means not applying feature corruption')\n    parser.add_argument('--noise_prob', type=float, default=0, \n                        help='Probability to add gaussian noise; only activated when it is larger than 0')\n    parser.add_argument('--student_emb_keyname', type=str, default='sentence_embedding', \n                        choices=['sentence_embedding', 'token_embeddings'],\n                        help='If set to `sentence_embedding`, decoder will generate texts solely based on a global vector; \\\n                              Otherwise, decoder will base text generation on a sequence of token features. \\\n                              We find that set this arg to `token_embeddings` benefit machine translation a lot')\n    \n    # Model settings\n    parser.add_argument('--no_frozen', action='store_true')\n    parser.add_argument('--freeze_transformer', action='store_true')\n    args = parser.parse_args()\n\n    if not args.inference_batch_size:\n        args.inference_batch_size = args.batch_size\n    \n    assert len(args.scales) == 4\n\n    output_path = args.output_path or os.path.join(args.output_root, args.exp_name)\n    os.makedirs(output_path, exist_ok=True)\n\n    # saving training logs to {output_path}/log.txt\n    logger.addHandler(logging.FileHandler(os.path.join(output_path, 'log.txt'), 'w', encoding='utf-8'))\n\n    # log necessary information\n    logger.info(f\"Output path: {output_path}\")\n    logger.info(f\"Target languages: {args.target_languages}\")\n    logger.info(f'Loss scales: {args.scales}')\n    logger.info(f'Noise std: {args.noise_std} {f\"(noise prob: {args.noise_prob})\" if args.noise_prob > 0 else \"\"}')\n    if args.use_amp:\n        logger.info('Use amp for speeding up training')\n    if args.use_masking:\n        logger.info(f'Random masking: {args.use_masking} (prob: {args.mask_prob})')\n\n    ######## Teacher model ########\n    logger.info(f\"Load teacher model: {args.teacher_model_name}\")\n    teacher_model = Framework(args.teacher_model_name)\n    logger.info(f'Teacher model architecture: \\n {teacher_model}')\n\n    # freeze teacher model\n    for p in teacher_model.parameters():\n        p.requires_grad = False\n    \n    dim_teacher = teacher_model._last_module().get_sentence_embedding_dimension()\n\n    ######## Student model ########\n    logger.info(f\"Create student model from {args.student_model_name}\")\n\n    if args.student_model_name in ['distilbert-base-multilingual-cased']:\n        # a transformer model for encoding\n        encoder = Transformer(\n            args.student_model_name, \n            max_seq_length=args.max_seq_length,\n        )\n        dim_enc = encoder.get_word_embedding_dimension()\n\n        pooling_model = Pooling(dim_enc)\n        dense_model = Dense(dim_enc, dim_teacher, bias=False, activation_function=nn.modules.linear.Identity())\n        modules = [encoder, pooling_model, dense_model]\n    else:\n        student_model = Framework(args.student_model_name, load_sbert_only=True)\n        modules = student_model.get_modules()\n\n    attend_to = []\n    if args.scales[1]:\n        attend_to.append('teacher')\n    if args.scales[2]:\n        attend_to.append('student')\n    \n    if isinstance(modules[-1], Dense):\n        # only encoding modules included now\n        dim_student = modules[-1].get_sentence_embedding_dimension()\n        assert dim_teacher == dim_student\n        assert isinstance(modules[-1], Dense)\n\n        # check if we need to add decoding modules\n        if args.scales[1] or args.scales[2]:\n\n            if 'bert' not in args.decoder_name.lower():\n                raise NotImplementedError('You should take care of `num_hidden_layers`')\n\n            decoder = Decoder(\n                model_name_or_path=args.decoder_name,\n                model_args={\n                    'is_decoder': True, \n                    'add_cross_attention': True,\n                    'num_hidden_layers': args.num_hidden_layers},\n                from_pretrained=args.use_pretrained_decoder,\n                attend_to=attend_to,\n                teacher_model_name=args.teacher_model_name,\n                max_seq_length=args.decoder_max_seq_length,\n            )\n            dim_dec = decoder.get_word_embedding_dimension()\n\n            projector = Projector(dim_teacher, dim_dec, noise_std=args.noise_std, noise_prob=args.noise_prob, student_emb_keyname=args.student_emb_keyname)\n            modules.extend([projector, decoder])\n    else:\n        # both encoding and decoding modules included\n        assert student_model.get_module_attribute('teacher_model_name') == args.teacher_model_name\n\n        student_model.set_module_attribute(Projector, 'noise_std', args.noise_std)\n        \n        # check if we need to keep decoding modules\n        if args.scales[1] or args.scales[2]:\n            student_model.set_module_attribute(Decoder, 'attend_to', attend_to)\n        else:\n            logger.info('Training does not need the decoder, ignore it')\n            modules = student_model.get_encoding_modules()\n\n    if args.scales[0] == 0 and not args.no_frozen:\n        logger.info('Freeze the multimodal encoder of the student model')\n        for idx, module in enumerate(modules):\n            if isinstance(module, Projector):\n                break\n        for module in modules[:idx]:\n            for p in module.parameters():\n                p.requires_grad = False\n    elif args.scales[0] == 0 and args.freeze_transformer:\n        logger.info('Freeze the transformer of the multimodal encoder of the student model')\n        module = modules[0]\n        assert isinstance(module, Transformer)\n        for p in module.parameters():\n            p.requires_grad = False\n\n    student_model = Framework(modules=modules, logger=logger)\n    student_model.set_module_attribute(Dense, 'proj_token_embs', args.student_emb_keyname == 'token_embeddings')\n\n    if args.scales[0] == 0 and args.scales[1] == 0 and args.scales[3] == 0:\n        logger.info('Training does not need the teacher model, set it to None')\n        teacher_model = None\n    \n    if args.scales[0] == 0 and args.scales[2] == 0 and args.scales[3] == 0:\n        logger.info('Training does not need the multimodal encoder, ignore it')\n        student_model = Framework(modules=student_model.get_decoding_modules(), logger=logger)\n\n    logger.info(f'Student model architecture: \\n {student_model}')\n    logger.info(f\"Total Params: {sum(p.numel() for p in student_model.parameters())}\")\n    logger.info(f\"Trainable Params: {sum(p.numel() for p in student_model.parameters() if p.requires_grad)}\")\n\n    ###### Read Parallel Sentences Dataset ######\n    train_data = PretrainDataset( \n        teacher_model=teacher_model, \n        batch_size=args.inference_batch_size, \n        use_embedding_cache=True,\n        logger=logger,\n        numpy_path=args.numpy_path,\n    )\n\n    if not args.weights:\n        args.weights = [100] * len(args.target_languages)\n    else:\n        assert isinstance(args.weights, list)\n        if len(args.weights) == 1:\n            args.weights = args.weights * len(args.target_languages)\n        else:\n            assert len(args.weights) == len(args.target_languages)\n\n    for lang, weight in zip(args.target_languages, args.weights):\n        train_corpus = args.train_corpus_format.format(args.source_language, lang)\n        if lang == args.source_language:\n            langs=[lang]\n            train_corpus = train_corpus.replace(f'-{args.source_language}.', '.')\n            train_data.load_data(train_corpus, max_sentences=args.max_sentences, max_sentence_length=None, exclude_source=False, langs=langs, weight=weight)\n        else:\n            langs = [args.source_language, lang]\n            # we set exclude_source to True, because we do not want too many samples in the source language for training\n            train_data.load_data(train_corpus, max_sentences=args.max_sentences, max_sentence_length=None, exclude_source=True, langs=langs, weight=weight)\n\n    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=args.batch_size, num_workers=args.num_workers if args.numpy_path else 0)\n\n    ###### Define the training objective ######\n    train_loss = LossManager(student_model, *args.scales)\n\n    ###### Start training ######\n    student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n        epochs=args.epochs,\n        warmup_steps=args.warmup_steps,\n        optimizer_params= {'lr': args.lr, 'eps': args.eps},\n        weight_decay=args.weight_decay,\n        output_path=output_path,\n        checkpoint_path=output_path,\n        checkpoint_save_steps=None, # save checkpoints every epoch, rather than spcific number of steps\n        log_every=args.log_every,\n        use_amp=args.use_amp,\n        scheduler=args.scheduler,\n        seed=args.seed,\n        use_masking=args.use_masking,\n        mask_prob=args.mask_prob,\n    )", ""]}
{"filename": "infer_retrieval.py", "chunked_list": ["import os\nimport time\nimport datetime\nimport argparse\nimport logging\nimport configs\n\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler\nfrom zeronlg import ZeroNLG, CaptionDatasetForRetrieval, RetrievalEvaluator", "from sentence_transformers import LoggingHandler\nfrom zeronlg import ZeroNLG, CaptionDatasetForRetrieval, RetrievalEvaluator\nfrom zeronlg.utils import get_formatted_string\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nlogger = logging.getLogger(__name__)\n\ntry:\n    ROOT = configs.annotation_retrieval_root\nexcept:\n    ROOT = configs.annotation_root", "logger = logging.getLogger(__name__)\n\ntry:\n    ROOT = configs.annotation_retrieval_root\nexcept:\n    ROOT = configs.annotation_root\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, required=True)\n    parser.add_argument('--clip_model_name', type=str)\n    # Data paths and attributes\n    parser.add_argument('--data_root', type=str, default=ROOT)\n    parser.add_argument('--dataset', type=str, default='coco')\n    parser.add_argument('--val_file', type=str, help='If not specified, use val_file_format')\n    parser.add_argument('--test_file', type=str, help='If not specified, use test_file_format')\n    parser.add_argument('--pickle_path', type=str, help='If not specified, use pickle_path_format')\n    parser.add_argument('--val_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/val.json'))\n    parser.add_argument('--test_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/test.json'))\n    parser.add_argument('--pickle_path_format', type=str, default=os.path.join(ROOT, '{dataset}/{clip_model_name}_{mode}.pkl'))\n    \n    # Dataloader settings\n    parser.add_argument('--batch_size', type=int, default=64)\n    \n    # Evaluation settings\n    parser.add_argument('--modes', type=str, nargs='+', default=['test'], help='evaluation modes: [\"val\"], [\"test\"], [\"val\", \"test\"]')\n    parser.add_argument('--lang', type=str, default='en', help='which language to be generated?')\n    parser.add_argument('--num_frames', type=int, default=configs.num_frames)\n    parser.add_argument('--mean_pooling', action='store_true')\n\n    # Output settings\n    parser.add_argument('--output_path', type=str, help='If not specified, output_path will be {model}/evaluations_retrieval/{dataset}/{lang}')\n    parser.add_argument('--no_suffix_folder', action='store_true', help='If True, the suffix `evaluations_retrieval/{dataset}/{lang}` will not be joined to the output path')\n    args = parser.parse_args()\n\n    if not os.path.exists(args.model):\n        assert args.output_path, \"you are training to load a model from hugginface hub, please specify --output_path\"\n\n    output_path = args.output_path or args.model\n    if not args.no_suffix_folder: \n        output_path = os.path.join(output_path, 'evaluations_retrieval', args.dataset, args.lang)\n\n    os.makedirs(output_path, exist_ok=True)\n    logger.addHandler(logging.FileHandler(os.path.join(output_path, 'log.txt'), 'w', encoding='utf-8'))\n    logger.info(f'output path: {output_path}')\n\n    assert args.modes in [['val'], ['test'], ['val', 'test']]\n\n    logger.info(f'Creating model from {args.model}')\n    model = ZeroNLG(args.model, args.clip_model_name)\n\n    # start evaluation\n    start_time = time.time()\n    for mode in args.modes:\n        ann_rpath = get_formatted_string(vars(args), f\"{mode}_file\", assigned_keys=['dataset', 'lang'])\n        logger.info(f'Load dataset from {ann_rpath}')\n\n        pickle_path = get_formatted_string(vars(args), 'pickle_path', assigned_kwargs=dict(\n            dataset=args.dataset, clip_model_name=model.clip_model_name, mode=mode,\n        ))\n        \n        dataset = CaptionDatasetForRetrieval(\n            vision_root=configs.image_video_root[args.dataset],\n            ann_rpath=ann_rpath,\n            num_frames=args.num_frames,\n            lang=args.lang,\n            clip_model=model.clip_model,\n            pickle_path=pickle_path,\n            logger=logger,\n            mean_pooling=args.mean_pooling,\n        )\n        logger.info(f'There are {len(dataset)} vision inputs')\n\n        loader = DataLoader(\n            dataset,\n            batch_size=args.batch_size,\n            shuffle=False,\n            collate_fn=dataset.collate_fn,\n        )\n\n        evaluator = RetrievalEvaluator(\n            loader=loader,\n            mode=mode,\n            logger=logger,\n            # for MS-COCO dataset, we additionally run 1K test\n            # the original 5K test will be also run\n            n_fold=5 if args.dataset == 'coco' else 1, \n        )\n\n        evaluator(model, output_path=output_path)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Time {}'.format(total_time_str))", "\n'''\npython infer_retrieval.py --model zeronlg-4langs-vc --output_path output/zeronlg-4langs-vc --dataset msrvtt --lang en --modes val test\n'''\n"]}
{"filename": "train_caption.py", "chunked_list": ["\nimport os\nimport argparse\nimport logging\nimport torch\nimport numpy as np\nimport configs\n\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler", "from torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler\n\nimport zeronlg\nfrom zeronlg import CaptionDataset, CaptionEvaluator\nfrom zeronlg.models import Projector, Decoder, CLIPModel\nfrom zeronlg.utils import get_formatted_string\nfrom zeronlg.losses import LossManager\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',", "\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nlogger = logging.getLogger(__name__)\n\ntry:\n    ROOT = configs.annotation_caption_root\nexcept:\n    ROOT = configs.annotation_root", "\n\nclass Framework(zeronlg.Framework):\n    def smart_batching_collate(self, batch):\n        texts = [b['text'] for b in batch]\n        langs = [b['lang'] for b in batch]\n        embs = np.array([b['emb'] for b in batch])\n        \n        features = {}\n        features.update(self.decoder_tokenize(texts, langs))\n        features['source_embedding'] = torch.FloatTensor(embs)\n\n        labels = None\n        return features, labels", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str)\n    parser.add_argument('--teacher_model_name', type=str, default='clip-ViT-B-32', \n                    choices=['clip-ViT-B-32', 'clip-ViT-B-16', 'clip-ViT-L-14'], \n                    help='Monolingual teacher model')\n    parser.add_argument('--use_clip_tokens', type=int, help='Whether use token-level visual embeddings?')\n    parser.add_argument('--mean_pooling', action='store_true', help='average visual embeddings over the time axis')\n\n    parser.add_argument('--decoder_name', type=str, default='bert-base-multilingual-cased')\n    parser.add_argument('--num_hidden_layers', type=int, default=3)\n    parser.add_argument('--hidden_dropout_prob', type=float, default=0.1)\n    parser.add_argument('--use_pretrained_decoder', action='store_true')\n    parser.add_argument('--decoder_max_seq_length', type=int, default=128, help='Student model max. lengths for inputs (number of word pieces)')\n    parser.add_argument('--freeze_word_embs', action='store_true')\n\n    # Data paths and attributes\n    parser.add_argument('--dataset', type=str, default='coco')\n    parser.add_argument('--lang', type=str, default='en', help='Language')\n    parser.add_argument('--data_root', type=str, default=ROOT)\n    parser.add_argument('--train_file', type=str, help='If not specified, use train_file_format')\n    parser.add_argument('--val_file', type=str, help='If not specified, use val_file_format')\n    parser.add_argument('--val_gt_file', type=str, help='If not specified, use val_gt_file_format')\n    parser.add_argument('--test_file', type=str, help='If not specified, use test_file_format')\n    parser.add_argument('--test_gt_file', type=str, help='If not specified, use test_gt_file_format')\n    parser.add_argument('--pickle_path', type=str, help='If not specified, use pickle_path_format')\n    parser.add_argument('--subset', type=str)\n    parser.add_argument('--train_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/train.json'))\n    parser.add_argument('--val_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/val.json'))\n    parser.add_argument('--val_gt_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/val_gt.json'))\n    parser.add_argument('--test_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/test.json'))\n    parser.add_argument('--test_gt_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/test_gt.json'))\n    parser.add_argument('--pickle_path_format', type=str, default=os.path.join(ROOT, '{dataset}/{clip_model_name}_{mode}{postfix}.pkl'))\n    parser.add_argument('--subset_path_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/subsets/{subset}.json'))\n\n    # Training settings\n    parser.add_argument('--use_amp', action='store_true')\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=32)\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--warmup_steps', type=int, default=5000, help='Warumup steps')\n    parser.add_argument('--lr', type=float, default=2e-5, help='Learning rate')\n    parser.add_argument('--eps', type=float, default=1e-6)\n    parser.add_argument('--weight_decay', type=float, default=0.01)\n    parser.add_argument('--scheduler', type=str, default='warmupconstant', choices=['constantlr', 'warmupconstant', 'warmuplinear', 'warmupcosine', 'warmupcosinewithhardrestarts'])\n    parser.add_argument('--auto', action='store_true')\n    \n    # Evaluation settings\n    parser.add_argument('--num_beams', type=int, default=3)\n    parser.add_argument('--max_length', type=int, default=30)\n    parser.add_argument('--min_length', type=int, default=5)\n    parser.add_argument('--repetition_penalty', type=float, default=1.0)\n\n    # Output settings\n    parser.add_argument('--output_path', type=str, required=True)\n    parser.add_argument('--log_every', type=int, default=200)\n    args = parser.parse_args()\n\n    os.makedirs(args.output_path, exist_ok=True)\n    logger.addHandler(logging.FileHandler(os.path.join(args.output_path, 'log.txt'), 'w', encoding='utf-8'))\n\n    if args.subset:\n        args.train_file_format = args.subset_path_format\n\n    ##############################################################################\n    logger.info('Creating models')\n    if args.model:\n        logger.info(f'Load the student model from {args.model}')\n        student_model = Framework(args.model)\n        student_model.set_module_attribute(Projector, 'noise_std', 0.0)\n        student_model.set_module_attribute(Decoder, 'attend_to', ['teacher'])\n\n        use_clip_tokens = bool(args.use_clip_tokens or student_model.get_module_attribute('use_clip_tokens', False))\n        student_model.set_module_attribute(Decoder, 'use_clip_tokens', use_clip_tokens)\n\n        student_model = Framework(\n            modules=student_model.get_decoding_modules(), \n            freeze_word_embeddings=args.freeze_word_embs,\n            logger=logger\n        )\n        teacher_model_name = student_model.get_module_attribute('teacher_model_name')\n        logger.info(f'Load the teacher model from {teacher_model_name}')\n        teacher_model = Framework(teacher_model_name)\n        teacher_model.set_module_attribute(CLIPModel, 'use_clip_tokens', use_clip_tokens)\n    else:\n        teacher_model_name = args.teacher_model_name\n        logger.info(f'Load the teacher model from {teacher_model_name}')\n        teacher_model = Framework(teacher_model_name)\n        teacher_model.set_module_attribute(CLIPModel, 'use_clip_tokens', args.use_clip_tokens)\n        \n        logger.info(f'Create the randomly initialized student model')\n        decoder = Decoder(\n            model_name_or_path=args.decoder_name,\n            model_args={\n                'is_decoder': True, \n                'add_cross_attention': True,\n                'num_hidden_layers': args.num_hidden_layers,\n                'hidden_dropout_prob': args.hidden_dropout_prob},\n            from_pretrained=args.use_pretrained_decoder,\n            attend_to=['teacher'],\n            teacher_model_name=teacher_model_name,\n            use_clip_tokens=args.use_clip_tokens,\n            max_seq_length=args.decoder_max_seq_length,\n        )\n        dim_tea = teacher_model._last_module().get_sentence_embedding_dimension()\n        dim_dec = decoder.get_word_embedding_dimension()\n\n        projector = Projector(dim_tea, dim_dec)\n        student_model = Framework(modules=[projector, decoder], logger=logger)\n\n    logger.info(f'Student model architecture: \\n {student_model}')\n    logger.info(f\"Total Params: {sum(p.numel() for p in student_model.parameters())}\")\n    logger.info(f\"Trainable Params: {sum(p.numel() for p in student_model.parameters() if p.requires_grad)}\")\n\n    teacher_model.eval()\n    for p in teacher_model.parameters():\n        p.requires_grad = False\n\n    ##############################################################################\n    logger.info('Creating dataloaders')\n\n    loaders = []\n    for mode in ['train', 'val', 'test']:\n        ann_rpath = get_formatted_string(vars(args), f\"{mode}_file\", assigned_keys=['dataset', 'lang', 'subset'])\n\n        pickle_path = get_formatted_string(vars(args), 'pickle_path', assigned_kwargs=dict(\n            dataset=args.dataset, clip_model_name=teacher_model_name, mode=mode, postfix='_tokens' if student_model.get_module_attribute('use_clip_tokens', False) else ''\n        ))\n\n        dataset = CaptionDataset(\n            vision_root=configs.image_video_root[args.dataset],\n            ann_rpath=ann_rpath,\n            lang=args.lang,\n            clip_model=teacher_model,\n            pickle_path=pickle_path,\n            logger=logger,\n            mean_pooling=args.mean_pooling\n        )\n\n        loader = DataLoader(\n            dataset=dataset,\n            batch_size=args.batch_size,\n            shuffle=True if mode == 'train' else False,\n            collate_fn=dataset.collate_fn,\n            # NOTE: do not set `num_workers`, it will raise an error about `using cuda with multiprocessing`\n        )\n\n        logger.info(f'{mode} #data: {len(dataset)}, #batches: {len(loader)}')\n        loaders.append(loader)\n    \n    train_loader, val_loader, test_loader = loaders\n\n    ##############################################################################\n    evaluation_settings = {\n        k: getattr(args, k) \n        for k in ['lang', 'num_beams', 'max_length', 'min_length', 'repetition_penalty']\n    }\n    if args.auto:\n        evaluation_settings.update(configs.auto_settings[args.lang])\n\n    logger.info(f'Evaluation settings: {evaluation_settings}')\n\n    gt_file_path = get_formatted_string(vars(args), \"val_gt_file\", assigned_keys=['dataset', 'lang'])\n    evaluator = CaptionEvaluator(\n        loader=val_loader,\n        gt_file_path=gt_file_path,\n        evaluation_settings=evaluation_settings,\n        mode='val',\n        logger=logger,\n        with_epoch=True,\n    )\n\n    train_loss = LossManager(student_model, loss_mse_scale=0, loss_at_teacher_scale=1)\n\n    student_model.fit(train_objectives=[(train_loader, train_loss)],\n        epochs=args.epochs,\n        evaluator=evaluator,\n        save_best_model=True,\n        warmup_steps=args.warmup_steps,\n        optimizer_params= {'lr': args.lr, 'eps': args.eps},\n        weight_decay=args.weight_decay,\n        output_path=args.output_path,\n        log_every=args.log_every,\n        use_amp=args.use_amp,\n        scheduler=args.scheduler,\n        seed=args.seed,\n    )\n\n    ##############################################################################\n    gt_file_path = get_formatted_string(vars(args), \"test_gt_file\", assigned_keys=['dataset', 'lang'])\n    evaluator = CaptionEvaluator(\n        loader=test_loader,\n        gt_file_path=gt_file_path,\n        evaluation_settings=evaluation_settings,\n        mode='test',\n        logger=logger,\n    )\n\n    student_model = Framework(args.output_path)\n    evaluator(student_model, os.path.join(args.output_path, 'eval'))\n\n    train_loader.dataset.save_pickle()\n    val_loader.dataset.save_pickle()\n    test_loader.dataset.save_pickle()", ""]}
{"filename": "infer_caption.py", "chunked_list": ["\nimport os\nimport time\nimport datetime\nimport argparse\nimport logging\nimport configs\n\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler", "from torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler\nfrom zeronlg import ZeroNLG, CaptionDataset, CaptionEvaluator\nfrom zeronlg.utils import get_formatted_string\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])", "                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nlogger = logging.getLogger(__name__)\n\ntry:\n    ROOT = configs.annotation_caption_root\nexcept:\n    ROOT = configs.annotation_root\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, required=True)\n    parser.add_argument('--use_clip_tokens', type=int, help='Whether use token-level visual embeddings?')\n    # Data paths and attributes\n    parser.add_argument('--data_root', type=str, default=ROOT)\n    parser.add_argument('--dataset', type=str, default='coco')\n    parser.add_argument('--val_file', type=str, help='If not specified, use val_file_format')\n    parser.add_argument('--val_gt_file', type=str, help='If not specified, use val_gt_file_format')\n    parser.add_argument('--test_file', type=str, help='If not specified, use test_file_format')\n    parser.add_argument('--test_gt_file', type=str, help='If not specified, use test_gt_file_format')\n    parser.add_argument('--pickle_path', type=str, help='If not specified, use pickle_path_format')\n    parser.add_argument('--val_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/val.json'))\n    parser.add_argument('--val_gt_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/val_gt.json'))\n    parser.add_argument('--test_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/test.json'))\n    parser.add_argument('--test_gt_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/test_gt.json'))\n    parser.add_argument('--pickle_path_format', type=str, default=os.path.join(ROOT, '{dataset}/{clip_model_name}_{mode}{postfix}.pkl'))\n    \n    # Dataloader settings\n    parser.add_argument('--batch_size', type=int, default=64)\n    \n    # Evaluation settings\n    parser.add_argument('--auto', action='store_true', help='whether to use the auto_settings')\n    parser.add_argument('--no_score', action='store_true', help='do not calculate caption scores')\n    parser.add_argument('--modes', type=str, nargs='+', default=['test'], help='evaluation modes: [\"val\"], [\"test\"], [\"val\", \"test\"]')\n    parser.add_argument('--lang', type=str, default='en', help='which language to be generated?')\n    parser.add_argument('--num_beams', type=int, default=3)\n    parser.add_argument('--max_length', type=int, default=30)\n    parser.add_argument('--min_length', type=int, default=5)\n    parser.add_argument('--repetition_penalty', type=float, default=1.0)\n    parser.add_argument('--num_frames', type=int, default=configs.num_frames)\n    parser.add_argument('--mean_pooling', action='store_true')\n\n    # Output settings\n    parser.add_argument('--output_path', type=str, help='If not specified, output_path will be {model}/evaluations_caption/{dataset}/{lang}')\n    parser.add_argument('--no_suffix_folder', action='store_true', help='If True, the suffix `evaluations_caption/{dataset}/{lang}` will not be joined to the output path')\n    parser.add_argument('--print_sent', action='store_true')\n    args = parser.parse_args()\n\n    if not os.path.exists(args.model):\n        assert args.output_path, \"you are training to load a model from hugginface hub, please specify --output_path\"\n\n    output_path = args.output_path or args.model\n    if not args.no_suffix_folder: \n        output_path = os.path.join(output_path, 'evaluations_caption', args.dataset, args.lang)\n\n    os.makedirs(output_path, exist_ok=True)\n    logger.addHandler(logging.FileHandler(os.path.join(output_path, 'log.txt'), 'w', encoding='utf-8'))\n    logger.info(f'output path: {output_path}')\n\n    assert args.modes in [['val'], ['test'], ['val', 'test']]\n\n    logger.info(f'Creating model from {args.model}')\n    model = ZeroNLG(args.model, use_clip_tokens=args.use_clip_tokens)\n\n    # prepare evaluation settings\n    evaluation_settings = {\n        k: getattr(args, k) \n        for k in ['lang', 'num_beams', 'max_length', 'min_length', 'repetition_penalty']\n    }\n    if args.auto:\n        evaluation_settings.update(configs.auto_settings[args.lang])\n    logger.info(f'Evaluation settings: {evaluation_settings}')\n\n    # start evaluation\n    start_time = time.time()\n    for mode in args.modes:\n        ann_rpath = get_formatted_string(vars(args), f\"{mode}_file\", assigned_keys=['dataset', 'lang'])\n        logger.info(f'Load dataset from {ann_rpath}')\n\n        pickle_path = get_formatted_string(vars(args), 'pickle_path', assigned_kwargs=dict(\n            dataset=args.dataset, clip_model_name=model.clip_model_name, mode=mode, postfix='_tokens' if model.use_clip_tokens else ''\n        ))\n\n        dataset = CaptionDataset(\n            vision_root=configs.image_video_root[args.dataset],\n            ann_rpath=ann_rpath,\n            num_frames=args.num_frames,\n            lang=args.lang,\n            clip_model=model.clip_model,\n            pickle_path=pickle_path,\n            logger=logger,\n            mean_pooling=args.mean_pooling,\n        )\n        logger.info(f'There are {len(dataset)} vision inputs')\n\n        loader = DataLoader(\n            dataset,\n            batch_size=args.batch_size,\n            shuffle=False,\n            collate_fn=dataset.collate_fn,\n        )\n\n        gt_file_path = get_formatted_string(vars(args), f\"{mode}_gt_file\", assigned_keys=['dataset', 'lang'])\n        evaluator = CaptionEvaluator(\n            loader=loader,\n            gt_file_path=gt_file_path,\n            evaluation_settings=evaluation_settings,\n            mode=mode,\n            logger=logger\n        )\n\n        evaluator(model, output_path=output_path, no_score=args.no_score, print_sent=args.print_sent)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Time {}'.format(total_time_str))", "\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, required=True)\n    parser.add_argument('--use_clip_tokens', type=int, help='Whether use token-level visual embeddings?')\n    # Data paths and attributes\n    parser.add_argument('--data_root', type=str, default=ROOT)\n    parser.add_argument('--dataset', type=str, default='coco')\n    parser.add_argument('--val_file', type=str, help='If not specified, use val_file_format')\n    parser.add_argument('--val_gt_file', type=str, help='If not specified, use val_gt_file_format')\n    parser.add_argument('--test_file', type=str, help='If not specified, use test_file_format')\n    parser.add_argument('--test_gt_file', type=str, help='If not specified, use test_gt_file_format')\n    parser.add_argument('--pickle_path', type=str, help='If not specified, use pickle_path_format')\n    parser.add_argument('--val_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/val.json'))\n    parser.add_argument('--val_gt_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/val_gt.json'))\n    parser.add_argument('--test_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/test.json'))\n    parser.add_argument('--test_gt_file_format', type=str, default=os.path.join(ROOT, '{dataset}/{lang}/test_gt.json'))\n    parser.add_argument('--pickle_path_format', type=str, default=os.path.join(ROOT, '{dataset}/{clip_model_name}_{mode}{postfix}.pkl'))\n    \n    # Dataloader settings\n    parser.add_argument('--batch_size', type=int, default=64)\n    \n    # Evaluation settings\n    parser.add_argument('--auto', action='store_true', help='whether to use the auto_settings')\n    parser.add_argument('--no_score', action='store_true', help='do not calculate caption scores')\n    parser.add_argument('--modes', type=str, nargs='+', default=['test'], help='evaluation modes: [\"val\"], [\"test\"], [\"val\", \"test\"]')\n    parser.add_argument('--lang', type=str, default='en', help='which language to be generated?')\n    parser.add_argument('--num_beams', type=int, default=3)\n    parser.add_argument('--max_length', type=int, default=30)\n    parser.add_argument('--min_length', type=int, default=5)\n    parser.add_argument('--repetition_penalty', type=float, default=1.0)\n    parser.add_argument('--num_frames', type=int, default=configs.num_frames)\n    parser.add_argument('--mean_pooling', action='store_true')\n\n    # Output settings\n    parser.add_argument('--output_path', type=str, help='If not specified, output_path will be {model}/evaluations_caption/{dataset}/{lang}')\n    parser.add_argument('--no_suffix_folder', action='store_true', help='If True, the suffix `evaluations_caption/{dataset}/{lang}` will not be joined to the output path')\n    parser.add_argument('--print_sent', action='store_true')\n    args = parser.parse_args()\n\n    if not os.path.exists(args.model):\n        assert args.output_path, \"you are training to load a model from hugginface hub, please specify --output_path\"\n\n    output_path = args.output_path or args.model\n    if not args.no_suffix_folder: \n        output_path = os.path.join(output_path, 'evaluations_caption', args.dataset, args.lang)\n\n    os.makedirs(output_path, exist_ok=True)\n    logger.addHandler(logging.FileHandler(os.path.join(output_path, 'log.txt'), 'w', encoding='utf-8'))\n    logger.info(f'output path: {output_path}')\n\n    assert args.modes in [['val'], ['test'], ['val', 'test']]\n\n    logger.info(f'Creating model from {args.model}')\n    model = ZeroNLG(args.model, use_clip_tokens=args.use_clip_tokens)\n\n    # prepare evaluation settings\n    evaluation_settings = {\n        k: getattr(args, k) \n        for k in ['lang', 'num_beams', 'max_length', 'min_length', 'repetition_penalty']\n    }\n    if args.auto:\n        evaluation_settings.update(configs.auto_settings[args.lang])\n    logger.info(f'Evaluation settings: {evaluation_settings}')\n\n    # start evaluation\n    start_time = time.time()\n    for mode in args.modes:\n        ann_rpath = get_formatted_string(vars(args), f\"{mode}_file\", assigned_keys=['dataset', 'lang'])\n        logger.info(f'Load dataset from {ann_rpath}')\n\n        pickle_path = get_formatted_string(vars(args), 'pickle_path', assigned_kwargs=dict(\n            dataset=args.dataset, clip_model_name=model.clip_model_name, mode=mode, postfix='_tokens' if model.use_clip_tokens else ''\n        ))\n\n        dataset = CaptionDataset(\n            vision_root=configs.image_video_root[args.dataset],\n            ann_rpath=ann_rpath,\n            num_frames=args.num_frames,\n            lang=args.lang,\n            clip_model=model.clip_model,\n            pickle_path=pickle_path,\n            logger=logger,\n            mean_pooling=args.mean_pooling,\n        )\n        logger.info(f'There are {len(dataset)} vision inputs')\n\n        loader = DataLoader(\n            dataset,\n            batch_size=args.batch_size,\n            shuffle=False,\n            collate_fn=dataset.collate_fn,\n        )\n\n        gt_file_path = get_formatted_string(vars(args), f\"{mode}_gt_file\", assigned_keys=['dataset', 'lang'])\n        evaluator = CaptionEvaluator(\n            loader=loader,\n            gt_file_path=gt_file_path,\n            evaluation_settings=evaluation_settings,\n            mode=mode,\n            logger=logger\n        )\n\n        evaluator(model, output_path=output_path, no_score=args.no_score, print_sent=args.print_sent)\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    print('Time {}'.format(total_time_str))", "\n'''\npython infer_caption.py --model zeronlg-4langs-vc --output_path output/zeronlg-4langs-vc --dataset msrvtt --lang en --modes val test\n'''\n"]}
{"filename": "zeronlg/__init__.py", "chunked_list": ["__LIBRARY_NAME__ = 'zeronlg'\n__version__ = \"1.0.0\"\n__HUGGINGFACE_HUB_NAME__ = 'yangbang18'\n\n\nfrom .Framework import Framework\nfrom .ZeroNLG import ZeroNLG\nfrom .losses import LossManager\nfrom .datasets import (\n    PretrainDataset, ", "from .datasets import (\n    PretrainDataset, \n    CaptionDataset, \n    CaptionDatasetForRetrieval,\n    TranslateDataset\n)\nfrom .evaluation import (\n    CaptionEvaluator, \n    TranslateEvaluator, \n    RetrievalEvaluator", "    TranslateEvaluator, \n    RetrievalEvaluator\n)\n"]}
{"filename": "zeronlg/ZeroNLG.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\nfrom PIL import Image\nfrom typing import List, Optional, Union, Dict, Any, Tuple\nfrom sentence_transformers.util import batch_to_device\n\nfrom . import Framework\nfrom .utils import process_images\nfrom .models import Decoder, CLIPModel", "from .utils import process_images\nfrom .models import Decoder, CLIPModel\n\n\nSUPPORTED_TASKS = ['caption', 'translate']\n\n\nclass ZeroNLG(nn.Module):\n    def __init__(self, \n                 multilingual_model: Union[str, Framework], \n                 clip_model: Union[str, Framework, None] = None, \n                 use_clip_tokens: Optional[bool] = None,\n                 load_clip_model: bool = True,\n                 device: Union[str, torch.device, None] = None,\n        ):\n        super().__init__()\n        self.use_clip_tokens = use_clip_tokens\n\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self._target_device = torch.device(device)\n\n        if type(multilingual_model) is str:\n            self.multilingual_model = Framework(multilingual_model, device=self.device)\n        else:\n            self.multilingual_model = multilingual_model\n\n        self.clip_model_name = None\n        if clip_model is None or clip_model == '':\n            self.clip_model_name = self.multilingual_model.get_module_attribute('teacher_model_name') \\\n                    or self.multilingual_model.get_module_attribute('clip_model_name')\n        elif type(clip_model) is str:\n            self.clip_model_name = clip_model\n        else:\n            assert isinstance(clip_model, Framework)\n            self.clip_model = clip_model\n\n        if load_clip_model:\n            self._load_clip_model()\n\n    def forward(self,\n            images: Union[str, List[str], Image.Image, List[Image.Image], None] = None,\n            texts: Union[str, List[str], None] = None, \n            image_embs: Optional[Tensor] = None,\n            text_embs: Optional[Tensor] = None,\n            num_frames: int=8,\n            lang: Optional[str] = None, \n            num_beams: int = 3,\n            max_length: int = 30,\n            min_length: int = 5,\n            repetition_penalty: float = 1.0,\n            return_all: bool = False,\n            attend_to: Union[str, List[str], None] = None,\n            task: Optional[str] = None,\n            **kwargs,\n        ) -> Union[List[str], Dict[str, Any]]:\n\n        if task:\n            assert task in SUPPORTED_TASKS\n        else:\n            raise ValueError(f'Please either pass the argument `task` ({SUPPORTED_TASKS}) or call the corresponding forward function')\n\n        forward_func_name = f'forward_{task}'\n        forward_func = getattr(self, forward_func_name, None)\n        assert forward_func is not None, f\"Please implement the function {forward_func_name} in ZeroNLG\"\n\n        return forward_func(\n            images=images,\n            texts=texts,\n            image_embs=image_embs,\n            text_embs=text_embs,\n            num_frames=num_frames,\n            lang=lang,\n            num_beams=num_beams,\n            max_length=max_length,\n            min_length=min_length,\n            repetition_penalty=repetition_penalty,\n            return_all=return_all,\n            attend_to=attend_to,\n            **kwargs\n        )\n    \n    def forward_caption(self, \n            images: Union[str, List[str], Image.Image, List[Image.Image], None] = None,\n            image_embs: Optional[Tensor] = None,\n            num_frames: int = 8,\n            lang: Optional[str] = None, \n            num_beams: int = 3,\n            max_length: int = 30,\n            min_length: int = 5,\n            repetition_penalty: float = 1.0,\n            return_all: bool = False,\n            attend_to: Union[str, List[str], None] = None,\n            **kwargs\n        ) -> Union[List[str], Dict[str, Any]]:\n\n        # the decoder should always attend to teacher's outputs in the cross-attention layers\n        attend_to = ['teacher']\n\n        # prepare clip image embeddings\n        image_embs = self.get_image_embeddings(images, image_embs, num_frames, normalize=False, mean_pooling=False)\n\n        # we re-define multilingual model to exclude encoding modules, which are useless for visual captioning\n        multilingual_model = self.multilingual_model.get_decoding_model(self.device)\n        multilingual_model.eval()\n\n        # features that will be passed to the multilingual model\n        features = dict(\n            source_embedding=image_embs,\n            attend_to=attend_to,\n            num_beams=num_beams,\n            max_length=max_length,\n            min_length=min_length,\n            repetition_penalty=repetition_penalty,\n        )\n        features.update(kwargs) # kwargs for generation\n        features['decoder_input_ids'] = self.get_bos_input_ids(batch_size=image_embs.size(0), lang=lang)\n        features = batch_to_device(features, self.device)\n\n        with torch.no_grad():\n            outputs = multilingual_model(features)\n\n        if return_all:\n            return outputs\n\n        # return only the caption\n        return outputs['results_at_teacher']\n\n    def forward_translate(self, \n            texts: Union[str, List[str], Tuple[str], None] = None, \n            text_embs: Optional[Tensor] = None,\n            lang: Optional[str] = None, \n            num_beams: int = 3,\n            max_length: int = 30,\n            min_length: int = 5,\n            repetition_penalty: float = 1.0,\n            return_all: bool = False,\n            attend_to: Union[str, List[str], None] = None,\n            **other_generate_kwargs\n        ) -> Union[List[str], Dict[str, Any]]:\n\n        # by default, the decoder attend to student's output in cross-attention layers\n        attend_to = attend_to or ['student']\n        if type(attend_to) is str:\n            attend_to = [attend_to]\n        if text_embs is not None:\n            # given that you have provided clip's text text_embs, the decoder must attend to clip (teacher)\n            attend_to = ['teacher']\n        assert len(attend_to) == 1, 'attend_to should be one of \"teacher\", \"student\", [\"teacher\"], [\"student\"]'\n        \n        assert texts is not None or text_embs is not None, \"you should specify either texts or text_embs\"\n\n        # prepare tokenized text features for the multilingual encoder\n        # or clip text embeddings\n        tokenized_features, source_embedding = {}, None\n        if text_embs is None:\n            texts = [texts] if not isinstance(texts, (list, tuple)) else texts\n            batch_size = len(texts)\n\n            if attend_to == ['teacher']:\n                # extract clip text embeddigns\n                self._load_clip_model() # load clip model if it has not been loaded\n                self.clip_model = self.clip_model.to(self.device)\n                self.clip_model.eval()\n                source_embedding = self.clip_model.encode(texts, batch_size, show_progress_bar=False, convert_to_tensor=True)\n            else:\n                tokenized_features = self.multilingual_model.tokenize(texts)\n        else:            \n            source_embedding = text_embs\n            batch_size = text_embs.size(0)\n\n        # re-define multilingual model if necessary\n        if attend_to == ['teacher']:\n            multilingual_model = self.multilingual_model.get_decoding_model(self.device)\n        else:\n            multilingual_model = self.multilingual_model.to(self.device)\n        multilingual_model.eval()\n\n        # features that will be passed to the multilingual model\n        features = dict(\n            source_embedding=source_embedding,\n            attend_to=attend_to,\n            num_beams=num_beams,\n            max_length=max_length,\n            min_length=min_length,\n            repetition_penalty=repetition_penalty,\n            **tokenized_features,\n        )\n        features.update(other_generate_kwargs)\n        features['decoder_input_ids'] = self.get_bos_input_ids(batch_size=batch_size, lang=lang)\n        features = batch_to_device(features, self.device)\n\n        with torch.no_grad():\n            outputs = multilingual_model(features)\n\n        if return_all:\n            return outputs\n\n        # return only the translated text\n        return outputs[f'results_at_{attend_to[0]}']\n\n    def get_image_embeddings(self,\n            images: Union[str, List[str], Image.Image, List[Image.Image], None] = None,\n            image_embs: Optional[Tensor] = None,\n            num_frames: int = 8,\n            mean_pooling: bool = False,\n            normalize: bool = False,\n            batch_size: Optional[int] = None,\n            **kwargs,\n        ) -> Tensor:\n        \"\"\"Extract CLIP image embeddings\"\"\"\n\n        if image_embs is None:\n            assert images is not None, \"you should specify either images or image_embs\"\n            \n            self._load_clip_model() # load clip model if it has not been loaded\n            self.clip_model = self.clip_model.to(self.device)\n            self.clip_model.eval()\n\n            images, is_video, num_frames, num_samples = process_images(images, num_frames)\n            \n            batch_size = batch_size or num_samples\n            \n            image_embs = self.clip_model.encode(\n                images, batch_size, output_value='token_embeddings' if self.use_clip_tokens else 'sentence_embedding',\n                show_progress_bar=False, convert_to_tensor=True, device=self.device)\n            \n            if isinstance(image_embs, list):\n                image_embs = torch.stack(image_embs, dim=0).to(self.device)\n\n            if is_video:\n                image_embs = image_embs.view(batch_size, num_frames, -1, image_embs.size(-1)).squeeze(2)\n        else:\n            image_embs = image_embs.to(self.device)\n            batch_size = image_embs.size(0)\n\n        if image_embs.ndim == 1:\n            image_embs = image_embs.unsqueeze(0)\n        \n        if image_embs.ndim > 2 and mean_pooling:\n            # averaged over the time axis\n            image_embs = image_embs.mean(dim=1)\n        \n        if normalize:\n            image_embs = F.normalize(image_embs, dim=-1)\n        \n        return image_embs\n    \n    def get_text_embeddings(self, \n            texts: Union[str, List[str], None] = None, \n            text_embs: Optional[Tensor] = None,\n            normalize: bool = False,\n            batch_size: Optional[int] = None,\n            **kwargs,\n        ) -> Tensor:\n        \"\"\"Extract CLIP text embeddings\"\"\"\n\n        # we re-define multilingual model to exclude useless decoding modules\n        multilingual_model = self.multilingual_model.get_encoding_model(self.device)\n        multilingual_model.eval()\n\n        texts = [texts] if not isinstance(texts, (list, tuple)) else texts\n        batch_size = batch_size or len(texts)\n\n        if text_embs is None:\n            text_embs = multilingual_model.encode(texts, batch_size, show_progress_bar=False, convert_to_tensor=True, device=self.device)\n        else:\n            text_embs = text_embs.to(self.device)\n        \n        if text_embs.ndim == 1:\n            text_embs = text_embs.unsqueeze(0)\n        \n        if normalize:\n            text_embs = F.normalize(text_embs, dim=-1)\n\n        return text_embs\n\n    def get_bos_input_ids(self, batch_size: int, lang: Optional[str] = None) -> Tensor:\n        for module in self.multilingual_model.get_modules():\n            if isinstance(module, Decoder):\n                return module.get_bos_input_ids(batch_size=batch_size, lang=lang)\n    \n    def _load_clip_model(self):\n        if not hasattr(self, 'clip_model'):\n            try:\n                # in this case, the multilignual model is actually a monolingual CLIP model\n                print(self.multilingual_model)\n                assert isinstance(self.multilingual_model._first_module(), CLIPModel)\n                self.clip_model = self.multilingual_model.get_encoding_model(device=self.device)\n            except:\n                assert self.clip_model_name is not None, \"you are trying to use a clip model, whose name can not be obtained from the multilingual model;\\\n                    Maybe you should pass the argument `clip_model_name` when defining a ZeroNLG model\"\n                assert type(self.clip_model_name) is str\n                self.clip_model = Framework(self.clip_model_name, device=self.device)\n            \n            self.use_clip_tokens = self.use_clip_tokens or self.multilingual_model.get_module_attribute('use_clip_tokens', False)\n            self.clip_model.set_module_attribute(CLIPModel, 'use_clip_tokens', self.use_clip_tokens)\n\n    @property\n    def device(self):\n        return self._target_device", ""]}
{"filename": "zeronlg/Framework.py", "chunked_list": ["import os\nimport torch\nimport json\nimport time\nimport shutil\nimport logging\nimport datetime\nimport random\nimport numpy as np\nimport transformers", "import numpy as np\nimport transformers\n\nimport stat\nimport tempfile\n\nimport torch\nimport torch.backends.cudnn as cudnn\nfrom torch import nn\nfrom torch.optim import Optimizer", "from torch import nn\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\n\nfrom tqdm.autonotebook import trange\nfrom collections import OrderedDict\nfrom distutils.dir_util import copy_tree\nfrom huggingface_hub import HfApi, HfFolder, Repository\nfrom typing import List, Dict, Tuple, Iterable, Type, Callable, Optional, Union\n", "from typing import List, Dict, Tuple, Iterable, Type, Callable, Optional, Union\n\nfrom sentence_transformers import SentenceTransformer, LoggingHandler\nfrom sentence_transformers import __version__ as __sbert_version__\nfrom sentence_transformers.evaluation import SentenceEvaluator\nfrom sentence_transformers.util import batch_to_device, fullname, import_from_string\nfrom sentence_transformers.model_card_templates import ModelCardTemplate\nfrom sentence_transformers.models import Pooling\n\nfrom .models import Transformer, Projector, Decoder", "\nfrom .models import Transformer, Projector, Decoder\nfrom .utils import MetricLogger, random_masking_, get_cache_folder\nfrom . import __LIBRARY_NAME__, __version__, __HUGGINGFACE_HUB_NAME__\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])", "                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\n\nglobal_logger = logging.getLogger(__name__)\n\n\nsbert_mappings = {\n    'sentence_transformers.models.Transformer': 'zeronlg.models.Transformer',\n    'sentence_transformers.models.Dense': 'zeronlg.models.Dense',\n    'sentence_transformers.models.CLIPModel': 'zeronlg.models.CLIPModel',", "    'sentence_transformers.models.Dense': 'zeronlg.models.Dense',\n    'sentence_transformers.models.CLIPModel': 'zeronlg.models.CLIPModel',\n    'models.Dense': 'zeronlg.models.Dense',\n    'models.Projector': 'zeronlg.models.Projector',\n    'models.Decoder': 'zeronlg.models.Decoder',\n}\n\n\nclass Framework(SentenceTransformer):\n    def __init__(self, \n                 model_name_or_path: Optional[str] = None, \n                 modules: Optional[Iterable[nn.Module]] = None, \n                 device: Optional[str] = None, \n                 cache_folder: Optional[str] = get_cache_folder(), \n                 use_auth_token: Union[bool, str, None] = None,\n                 tie_word_embeddings: bool = True,\n                 freeze_word_embeddings: bool = False,\n                 logger: logging.Logger = None,\n                 load_sbert_only: bool = False,\n                 ):\n\n        # check if we need to prefix `model_name_or_path` with __HUGGINGFACE_HUB_NAME__\n        if model_name_or_path \\\n            and 'zeronlg' in model_name_or_path.lower() \\\n            and '/' not in model_name_or_path \\\n            and not os.path.exists(model_name_or_path):\n            model_name_or_path = os.path.join(__HUGGINGFACE_HUB_NAME__, model_name_or_path)\n\n        super().__init__(model_name_or_path, modules, device, cache_folder, use_auth_token)\n\n        self.model_name_or_path = model_name_or_path\n        self.logger = logger or global_logger\n        self.load_sbert_only = load_sbert_only\n        \n        if tie_word_embeddings:\n            self._tie_word_embeddings()\n        \n        if freeze_word_embeddings:\n            self._freeze_word_embeddings()\n        \n        #Save some model info\n        if '__version__' not in self._model_config:\n            self._model_config['__version__'] = {\n                'sentence_transformers': __sbert_version__,\n                'transformers': transformers.__version__,\n                'pytorch': torch.__version__,\n                __LIBRARY_NAME__: __version__,\n            }\n        elif __LIBRARY_NAME__ not in self._model_config['__version__']:\n            self._model_config['__version__'][__LIBRARY_NAME__] = __version__\n        \n    def _tie_word_embeddings(self):\n        encoder_module, decoder_module = None, None\n        for module in self.get_modules():\n            if isinstance(module, Transformer):\n                encoder_module = module\n            if isinstance(module, Decoder):\n                decoder_module = module\n        \n        if encoder_module is not None and decoder_module is not None:\n            encoder_input_word_embs = encoder_module.auto_model.get_input_embeddings()\n            decoder_input_word_embs = decoder_module.auto_model.get_input_embeddings()\n            decoder_output_word_embs = decoder_module.auto_model.get_output_embeddings()\n            decoder_module.auto_model._tie_or_clone_weights(decoder_input_word_embs, encoder_input_word_embs)\n            decoder_module.auto_model._tie_or_clone_weights(decoder_output_word_embs, encoder_input_word_embs)\n    \n    def _freeze_word_embeddings(self):\n        for module in self.get_modules():\n            if isinstance(module, (Transformer, Decoder)):\n                for embs in [\n                        module.auto_model.get_input_embeddings(), \n                        module.auto_model.get_output_embeddings()\n                    ]:\n                    if embs is not None:\n                        for p in embs.parameters():\n                            p.requires_grad = False\n    \n    def _load_sbert_model(self, model_path):\n        \"\"\"\n        Loads a full sentence-transformers model\n        \"\"\"\n        # Check if the config_sentence_transformers.json file exists (exists since v2 of the framework)\n        config_sentence_transformers_json_path = os.path.join(model_path, 'config_sentence_transformers.json')\n        if os.path.exists(config_sentence_transformers_json_path):\n            with open(config_sentence_transformers_json_path) as fIn:\n                self._model_config = json.load(fIn)\n\n            # Yang B. modification: additionally check version of zeronlg\n            for package_name, version in zip(['sentence_transformers', __LIBRARY_NAME__], [__sbert_version__, __version__]):\n                if '__version__' in self._model_config \\\n                    and package_name in self._model_config['__version__'] \\\n                    and self._model_config['__version__'][package_name] > version:\n                    self.logger.warning(\n                        f\"You try to use a {package_name} model that was created with version {self._model_config['__version__'][package_name]}, however, your version is {version}. \\\n                        This might cause unexpected behavior or errors.\\n\\n\\n\")\n\n        # Check if a readme exists\n        model_card_path = os.path.join(model_path, 'README.md')\n        if os.path.exists(model_card_path):\n            try:\n                with open(model_card_path, encoding='utf8') as fIn:\n                    self._model_card_text = fIn.read()\n            except:\n                pass\n\n        # Load the modules of sentence transformer\n        modules_json_path = os.path.join(model_path, 'modules.json')\n        with open(modules_json_path) as fIn:\n            modules_config = json.load(fIn)\n\n        modules = OrderedDict()\n        for module_config in modules_config:\n            # Yang B. modification: apply mappings, make it compatible to new implementations\n            module_type = sbert_mappings.get(module_config['type'], module_config['type'])\n            module_class = import_from_string(module_type)\n            module = module_class.load(os.path.join(model_path, module_config['path']))\n            modules[module_config['name']] = module\n\n        return modules\n\n    def _load_auto_model(self, model_name_or_path):\n        \"\"\"\n        Creates a simple Transformer + Mean Pooling model and returns the modules\n        \"\"\"\n        # Yang B. modification: check if we automatically load non-sbert model\n        if self.load_sbert_only:\n            raise FileNotFoundError(\"No sentence-transformers model found with name {}, and you set `load_sbert_only` to True\".format(model_name_or_path))\n\n        self.logger.warning(\"No sentence-transformers model found with name {}. Creating a new one with MEAN pooling.\".format(model_name_or_path))\n        transformer_model = Transformer(model_name_or_path)\n        pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), 'mean')\n        return [transformer_model, pooling_model]\n\n    def set_module_attribute(self, module_class, key, value):\n        for module in self.get_modules():\n            if isinstance(module, module_class):\n                setattr(module, key, value)\n    \n    def get_module_attribute(self, key, default_value=None):\n        for module in self.get_modules():\n            if hasattr(module, key):\n                return getattr(module, key)\n            \n        if key == 'teacher_model_name':\n            if 'clip-ViT-B-32' in self.model_name_or_path:\n                return 'clip-ViT-B-32'\n            if 'clip-ViT-B-16' in self.model_name_or_path:\n                return 'clip-ViT-B-16'\n            if 'clip-ViT-L-14' in self.model_name_or_path:\n                return 'clip-ViT-L-14'\n\n        return default_value\n\n    def get_modules(self):\n        return [self._modules[_] for _ in iter(self._modules)]\n    \n    def _get_specific_model(self, before=True, instances=(Projector, Decoder), device=None, return_modules_only: bool = False):\n        \"\"\"only keep related modules\"\"\"\n        modules = self.get_modules()\n        idx = 0\n        for module in modules:\n            if isinstance(module, instances):\n                break\n            idx += 1\n\n        device = device or self.device\n\n        if before:\n            # get modules < idx\n            if idx == 0:\n                return None  \n            if return_modules_only:\n                return modules[:idx]\n            model = Framework(modules=modules[:idx], device=device)\n        else:\n            # get modules >= idx\n            if idx == len(modules):\n                return None\n            if return_modules_only:\n                return modules[idx:]\n            model = Framework(modules=modules[idx:], device=device)\n\n        model.to(device)\n        return model\n\n    def get_encoding_model(self, device=None):\n        \"\"\"return a model that contains modules only related to encoding\"\"\"\n        return self._get_specific_model(before=True, instances=(Projector, Decoder), device=device or self._target_device)\n    \n    def get_encoding_modules(self) -> List[nn.Module]:\n        \"\"\"return modules only related to encoding\"\"\"\n        return self._get_specific_model(before=True, instances=(Projector, Decoder), return_modules_only=True)\n\n    def get_decoding_model(self, device=None):\n        \"\"\"return a model that contains modules only related to decoding\"\"\"\n        return self._get_specific_model(before=False, instances=(Projector, Decoder), device=device or self._target_device)\n    \n    def get_decoding_modules(self) -> List[nn.Module]:\n        \"\"\"return modules only related to decoding\"\"\"\n        return self._get_specific_model(before=False, instances=(Projector, Decoder), return_modules_only=True)\n\n    def tokenize(self, texts: Union[List[str], List[Dict], List[Tuple[str, str]]]):\n        module = self._first_module()\n        if hasattr(module, 'tokenize'):\n            return module.tokenize(texts)\n        return {}\n\n    def decoder_tokenize(self, texts: List[str], langs: Optional[List[str]]=None):\n        module = self._last_module()\n        if isinstance(module, Decoder):\n            return module.tokenize(texts, langs)\n        return {}\n    \n    @property\n    def tokenizer(self):\n        \"\"\"Property to get the tokenizer that is used by this model\"\"\"\n        module = self._first_module()\n        if hasattr(module, 'tokenizer'):\n            return module.tokenizer\n        return None\n    \n    @property\n    def decoder_tokenizer(self):\n        \"\"\"Property to get the decoder tokenizer that is used by this model\"\"\"\n        module = self._last_module()\n        if isinstance(module, Decoder):\n            return module.tokenizer\n        return None\n    \n    @property\n    def device(self):\n        return self._target_device\n\n    def smart_batching_collate(self, batch):\n        \"\"\"Transforms a batch of InputExample to features requested by this model\"\"\"\n        texts = []\n        labels = []\n        langs = []\n\n        for example in batch:\n            texts.append(example.trg_text)\n            if example.label is not None:\n                labels.append(example.label)\n            if example.lang:\n                langs.append(example.lang)\n\n        labels = torch.tensor(np.array(labels)) if len(labels) else None\n\n        features = {}\n\n        # prepare input_ids, attention_mask, ...\n        tokenized_results = self.tokenize(texts)\n\n        # mask tokenized results if specified\n        if getattr(self, 'use_masking', False):\n            # self.use_masking and self.mask_prob is defined in Framework.fit\n            random_masking_(\n                tokenizer=self.tokenizer, \n                tokenized_results=tokenized_results, \n                mask_prob=getattr(self, 'mask_prob', 0.15)\n            )\n\n        features.update(tokenized_results) \n\n        # prepare decoder_input_ids, decoder_attention_mask, ...\n        features.update(self.decoder_tokenize(texts, langs if len(langs) else None)) \n\n        features['source_embedding'] = labels # used for decoding (optional)\n        \n        return features, labels\n\n    def fit(self,\n            train_objectives: Iterable[Tuple[DataLoader, nn.Module]],\n            evaluator: SentenceEvaluator = None,\n            epochs: int = 1,\n            steps_per_epoch = None,\n            scheduler: str = 'WarmupLinear',\n            warmup_steps: int = 10000,\n            optimizer_class: Type[Optimizer] = torch.optim.AdamW,\n            optimizer_params : Dict[str, object]= {'lr': 2e-5},\n            weight_decay: float = 0.01,\n            evaluation_steps: int = 0,\n            output_path: str = None,\n            save_best_model: bool = True,\n            max_grad_norm: float = 1,\n            use_amp: bool = False,\n            callback: Callable[[float, int, int], None] = None,\n            show_progress_bar: bool = True,\n            checkpoint_path: str = None,\n            checkpoint_save_steps: int = 500,\n            checkpoint_save_total_limit: int = 0,\n            log_every: int = 500,\n            seed: int = 42,\n            use_masking: bool = False,\n            mask_prob: float = 0.15,\n            ):\n        \"\"\"\n        Train the model with the given training objective\n        Each training objective is sampled in turn for one batch.\n        We sample only as many batches from each objective as there are in the smallest one\n        to make sure of equal training with each dataset.\n\n        :param train_objectives: Tuples of (DataLoader, LossFunction). Pass more than one for multi-task learning\n        :param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n        :param epochs: Number of epochs for training\n        :param steps_per_epoch: Number of training steps per epoch. If set to None (default), one epoch is equal the DataLoader size from train_objectives.\n        :param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n        :param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n        :param optimizer_class: Optimizer\n        :param optimizer_params: Optimizer parameters\n        :param weight_decay: Weight decay for model parameters\n        :param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n        :param output_path: Storage path for the model and evaluation files\n        :param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n        :param max_grad_norm: Used for gradient normalization.\n        :param use_amp: Use Automatic Mixed Precision (AMP). Only for Pytorch >= 1.6.0\n        :param callback: Callback function that is invoked after each evaluation.\n                It must accept the following three parameters in this order:\n                `score`, `epoch`, `steps`\n        :param show_progress_bar: If True, output a tqdm progress bar\n        :param checkpoint_path: Folder to save checkpoints during training\n        :param checkpoint_save_steps: Will save a checkpoint after so many steps\n        :param checkpoint_save_total_limit: Total number of checkpoints to store\n        \"\"\"\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n        cudnn.benchmark = True\n\n        self.use_masking = use_masking\n        self.mask_prob = mask_prob\n\n        ##Add info to model card\n        #info_loss_functions = \"\\n\".join([\"- {} with {} training examples\".format(str(loss), len(dataloader)) for dataloader, loss in train_objectives])\n        info_loss_functions =  []\n        for dataloader, loss in train_objectives:\n            info_loss_functions.extend(ModelCardTemplate.get_train_objective_info(dataloader, loss))\n        info_loss_functions = \"\\n\\n\".join([text for text in info_loss_functions])\n\n        info_fit_parameters = json.dumps({\"evaluator\": fullname(evaluator), \"epochs\": epochs, \"steps_per_epoch\": steps_per_epoch, \"scheduler\": scheduler, \"warmup_steps\": warmup_steps, \"optimizer_class\": str(optimizer_class),  \"optimizer_params\": optimizer_params, \"weight_decay\": weight_decay, \"evaluation_steps\": evaluation_steps, \"max_grad_norm\": max_grad_norm }, indent=4, sort_keys=True)\n        self._model_card_text = None\n        self._model_card_vars['{TRAINING_SECTION}'] = ModelCardTemplate.__TRAINING_SECTION__.replace(\"{LOSS_FUNCTIONS}\", info_loss_functions).replace(\"{FIT_PARAMETERS}\", info_fit_parameters)\n\n\n        if use_amp:\n            from torch.cuda.amp import autocast\n            scaler = torch.cuda.amp.GradScaler()\n\n        self.to(self.device)\n\n        dataloaders = [dataloader for dataloader, _ in train_objectives]\n\n        # Use smart batching\n        for dataloader in dataloaders:\n            dataloader.collate_fn = self.smart_batching_collate\n\n        loss_models = [loss for _, loss in train_objectives]\n        for loss_model in loss_models:\n            loss_model.to(self.device)\n\n        self.best_score = -9999999\n\n        if steps_per_epoch is None or steps_per_epoch == 0:\n            steps_per_epoch = min([len(dataloader) for dataloader in dataloaders])\n\n        num_train_steps = int(steps_per_epoch * epochs)\n\n        # Prepare optimizers\n        optimizers = []\n        schedulers = []\n        for loss_model in loss_models:\n            param_optimizer = list(loss_model.named_parameters())\n\n            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n            optimizer_grouped_parameters = [\n                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n            ]\n\n            optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n            scheduler_obj = self._get_scheduler(optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n\n            optimizers.append(optimizer)\n            schedulers.append(scheduler_obj)\n\n\n        global_step = 0\n        data_iterators = [iter(dataloader) for dataloader in dataloaders]\n\n        num_train_objectives = len(train_objectives)\n\n        skip_scheduler = False\n        train_start_time = time.time()\n        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n            \n            training_steps = 0\n            metric_logger = MetricLogger(delimiter=\"  \")\n            start_time = time.time()\n\n            for loss_model in loss_models:\n                loss_model.zero_grad()\n                loss_model.train()\n\n            for _ in trange(steps_per_epoch, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n                for train_idx in range(num_train_objectives):\n                    loss_model = loss_models[train_idx]\n                    optimizer = optimizers[train_idx]\n                    scheduler = schedulers[train_idx]\n                    data_iterator = data_iterators[train_idx]\n\n                    try:\n                        data = next(data_iterator)\n                    except StopIteration:\n                        data_iterator = iter(dataloaders[train_idx])\n                        data_iterators[train_idx] = data_iterator\n                        data = next(data_iterator)\n\n                    features, labels = data\n                    labels = labels.to(self.device) if labels is not None else None\n                    features = batch_to_device(features, self.device)\n\n                    if use_amp:\n                        with autocast():\n                            loss_value, loss_msg_dict = loss_model(features, labels)\n\n                        scale_before_step = scaler.get_scale()\n                        scaler.scale(loss_value).backward()\n                        scaler.unscale_(optimizer)\n                        torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n                        scaler.step(optimizer)\n                        scaler.update()\n\n                        skip_scheduler = scaler.get_scale() != scale_before_step\n                    else:\n                        loss_value, loss_msg_dict = loss_model(features, labels)\n                        loss_value.backward()\n                        torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n                        optimizer.step()\n                    \n                    metric_logger.update(**loss_msg_dict)\n\n                    optimizer.zero_grad()\n\n                    if not skip_scheduler:\n                        scheduler.step()\n\n                training_steps += 1\n                global_step += 1\n\n                if log_every > 0 and global_step % log_every == 0:\n                    self.log_training_info(metric_logger, epoch, training_steps, steps_per_epoch)\n\n                if evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n                    self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n\n                    for loss_model in loss_models:\n                        loss_model.zero_grad()\n                        loss_model.train()\n\n                    info = f\"[BEST] {self.best_score}\"\n                    self.logger.info(info)\n\n                if checkpoint_path is not None and checkpoint_save_steps is not None and checkpoint_save_steps > 0 and global_step % checkpoint_save_steps == 0:\n                    self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n            \n            metric_logger.synchronize_between_processes()\n            info = f\"Averaged stats: {metric_logger.global_avg()}\"\n            self.logger.info(info)\n            time_string = 'Train epoch time: ' + str(datetime.timedelta(seconds=int(time.time() - start_time)))\n            self.logger.info(time_string)\n\n            self._eval_during_training(evaluator, output_path, save_best_model, epoch, -1, callback)\n\n            if checkpoint_path is not None and checkpoint_save_steps is None:\n                self._save_checkpoint_epoch(checkpoint_path, checkpoint_save_total_limit, epoch)\n\n        if evaluator is None and output_path is not None:   #No evaluator, but output path: save final model version\n            self.save(output_path)\n\n        if checkpoint_path is not None and checkpoint_save_steps is not None:\n            self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n        \n        time_string = 'Train time: ' + str(datetime.timedelta(seconds=int(time.time() - train_start_time)))\n        self.logger.info(time_string)\n    \n    def log_training_info(self, \n            metric_logger: MetricLogger, \n            epoch: int, \n            step: int, \n            steps_per_epoch: int,\n            delimiter: str = '  ',\n            ):\n        \n        _msg = [\n            'Epoch: {epoch} [{step:' + f'{len(str(steps_per_epoch))}' + 'd} / {steps_per_epoch}]',\n            '{meters}',\n        ]\n\n        if torch.cuda.is_available():\n            _msg.append('max mem: {memory:.0f}')\n            MB = 1024.0 * 1024.0\n            info = delimiter.join(_msg).format(\n                epoch=epoch, \n                step=step, \n                steps_per_epoch=steps_per_epoch, \n                meters=str(metric_logger), \n                memory=torch.cuda.max_memory_allocated() / MB\n            )\n        else:\n            info = delimiter.join(_msg).format(\n                epoch=epoch, \n                step=step, \n                steps_per_epoch=steps_per_epoch, \n                meters=str(metric_logger)\n            )\n        \n        self.logger.info(info)\n    \n    def _save_checkpoint_epoch(self, checkpoint_path, checkpoint_save_total_limit, epoch):\n        # Store new checkpoint\n        self.save(os.path.join(checkpoint_path, str(epoch)))\n\n        # Delete old checkpoints\n        if checkpoint_save_total_limit is not None and checkpoint_save_total_limit > 0:\n            old_checkpoints = []\n            for subdir in os.listdir(checkpoint_path):\n                if subdir.isdigit():\n                    old_checkpoints.append({'epoch': int(subdir), 'path': os.path.join(checkpoint_path, subdir)})\n\n            if len(old_checkpoints) > checkpoint_save_total_limit:\n                old_checkpoints = sorted(old_checkpoints, key=lambda x: x['epoch'])\n                shutil.rmtree(old_checkpoints[0]['path'])\n\n    @staticmethod\n    def load(input_path):\n        return Framework(input_path)\n    \n    def save_to_hub(self,\n                repo_name: str,\n                private: Optional[bool] = None,\n                commit_message: str = \"Add new ZeroNLG model.\",\n                local_model_path: Optional[str] = None,\n                exist_ok: bool = False,\n                replace_model_card: bool = False,\n                train_datasets: Optional[List[str]] = None):\n        \"\"\"\n        Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.\n\n        Yang B. modification: \n        1) delete organization to avoid bugs;\n\n        :param repo_name: Repository name for your model in the Hub.\n        :param private: Set to true, for hosting a prive model\n        :param commit_message: Message to commit while pushing.\n        :param local_model_path: Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded\n        :param exist_ok: If true, saving to an existing repository is OK. If false, saving only to a new repository is possible\n        :param replace_model_card: If true, replace an existing model card in the hub with the automatically created model card\n        :param train_datasets: Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.\n        :return: The url of the commit of your model in the given repository.\n        \"\"\"\n        token = HfFolder.get_token()\n        if token is None:\n            raise ValueError(\"You must login to the Hugging Face hub on this computer by typing `transformers-cli login`.\")\n\n        endpoint = \"https://huggingface.co\"\n        repo_url = HfApi(endpoint=endpoint).create_repo(\n                repo_name,\n                token=token,\n                private=private,\n                repo_type=None,\n                exist_ok=exist_ok,\n            )\n        full_model_name = repo_url[len(endpoint)+1:].strip(\"/\")\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            # First create the repo (and clone its content if it's nonempty).\n            self.logger.info(\"Create repository and clone it if it exists\")\n            repo = Repository(tmp_dir, clone_from=repo_url)\n\n            # If user provides local files, copy them.\n            if local_model_path:\n                copy_tree(local_model_path, tmp_dir)\n            else:  # Else, save model directly into local repo.\n                create_model_card = False # TODO: zeronlg model card\n                self.save(tmp_dir, model_name=full_model_name, create_model_card=create_model_card, train_datasets=train_datasets)\n\n            #Find files larger 5M and track with git-lfs\n            large_files = []\n            for root, dirs, files in os.walk(tmp_dir):\n                for filename in files:\n                    file_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(file_path, tmp_dir)\n\n                    if os.path.getsize(file_path) > (5 * 1024 * 1024):\n                        large_files.append(rel_path)\n\n            if len(large_files) > 0:\n                self.logger.info(\"Track files with git lfs: {}\".format(\", \".join(large_files)))\n                repo.lfs_track(large_files)\n\n            self.logger.info(\"Push model to the hub. This might take a while\")\n            push_return = repo.push_to_hub(commit_message=commit_message)\n\n            def on_rm_error(func, path, exc_info):\n                # path contains the path of the file that couldn't be removed\n                # let's just assume that it's read-only and unlink it.\n                try:\n                    os.chmod(path, stat.S_IWRITE)\n                    os.unlink(path)\n                except:\n                    pass\n\n            # Remove .git folder. On Windows, the .git folder might be read-only and cannot be deleted\n            # Hence, try to set write permissions on error\n            try:\n                for f in os.listdir(tmp_dir):\n                    shutil.rmtree(os.path.join(tmp_dir, f), onerror=on_rm_error)\n            except Exception as e:\n                self.logger.warning(\"Error when deleting temp folder: {}\".format(str(e)))\n                pass\n\n        return push_return", "class Framework(SentenceTransformer):\n    def __init__(self, \n                 model_name_or_path: Optional[str] = None, \n                 modules: Optional[Iterable[nn.Module]] = None, \n                 device: Optional[str] = None, \n                 cache_folder: Optional[str] = get_cache_folder(), \n                 use_auth_token: Union[bool, str, None] = None,\n                 tie_word_embeddings: bool = True,\n                 freeze_word_embeddings: bool = False,\n                 logger: logging.Logger = None,\n                 load_sbert_only: bool = False,\n                 ):\n\n        # check if we need to prefix `model_name_or_path` with __HUGGINGFACE_HUB_NAME__\n        if model_name_or_path \\\n            and 'zeronlg' in model_name_or_path.lower() \\\n            and '/' not in model_name_or_path \\\n            and not os.path.exists(model_name_or_path):\n            model_name_or_path = os.path.join(__HUGGINGFACE_HUB_NAME__, model_name_or_path)\n\n        super().__init__(model_name_or_path, modules, device, cache_folder, use_auth_token)\n\n        self.model_name_or_path = model_name_or_path\n        self.logger = logger or global_logger\n        self.load_sbert_only = load_sbert_only\n        \n        if tie_word_embeddings:\n            self._tie_word_embeddings()\n        \n        if freeze_word_embeddings:\n            self._freeze_word_embeddings()\n        \n        #Save some model info\n        if '__version__' not in self._model_config:\n            self._model_config['__version__'] = {\n                'sentence_transformers': __sbert_version__,\n                'transformers': transformers.__version__,\n                'pytorch': torch.__version__,\n                __LIBRARY_NAME__: __version__,\n            }\n        elif __LIBRARY_NAME__ not in self._model_config['__version__']:\n            self._model_config['__version__'][__LIBRARY_NAME__] = __version__\n        \n    def _tie_word_embeddings(self):\n        encoder_module, decoder_module = None, None\n        for module in self.get_modules():\n            if isinstance(module, Transformer):\n                encoder_module = module\n            if isinstance(module, Decoder):\n                decoder_module = module\n        \n        if encoder_module is not None and decoder_module is not None:\n            encoder_input_word_embs = encoder_module.auto_model.get_input_embeddings()\n            decoder_input_word_embs = decoder_module.auto_model.get_input_embeddings()\n            decoder_output_word_embs = decoder_module.auto_model.get_output_embeddings()\n            decoder_module.auto_model._tie_or_clone_weights(decoder_input_word_embs, encoder_input_word_embs)\n            decoder_module.auto_model._tie_or_clone_weights(decoder_output_word_embs, encoder_input_word_embs)\n    \n    def _freeze_word_embeddings(self):\n        for module in self.get_modules():\n            if isinstance(module, (Transformer, Decoder)):\n                for embs in [\n                        module.auto_model.get_input_embeddings(), \n                        module.auto_model.get_output_embeddings()\n                    ]:\n                    if embs is not None:\n                        for p in embs.parameters():\n                            p.requires_grad = False\n    \n    def _load_sbert_model(self, model_path):\n        \"\"\"\n        Loads a full sentence-transformers model\n        \"\"\"\n        # Check if the config_sentence_transformers.json file exists (exists since v2 of the framework)\n        config_sentence_transformers_json_path = os.path.join(model_path, 'config_sentence_transformers.json')\n        if os.path.exists(config_sentence_transformers_json_path):\n            with open(config_sentence_transformers_json_path) as fIn:\n                self._model_config = json.load(fIn)\n\n            # Yang B. modification: additionally check version of zeronlg\n            for package_name, version in zip(['sentence_transformers', __LIBRARY_NAME__], [__sbert_version__, __version__]):\n                if '__version__' in self._model_config \\\n                    and package_name in self._model_config['__version__'] \\\n                    and self._model_config['__version__'][package_name] > version:\n                    self.logger.warning(\n                        f\"You try to use a {package_name} model that was created with version {self._model_config['__version__'][package_name]}, however, your version is {version}. \\\n                        This might cause unexpected behavior or errors.\\n\\n\\n\")\n\n        # Check if a readme exists\n        model_card_path = os.path.join(model_path, 'README.md')\n        if os.path.exists(model_card_path):\n            try:\n                with open(model_card_path, encoding='utf8') as fIn:\n                    self._model_card_text = fIn.read()\n            except:\n                pass\n\n        # Load the modules of sentence transformer\n        modules_json_path = os.path.join(model_path, 'modules.json')\n        with open(modules_json_path) as fIn:\n            modules_config = json.load(fIn)\n\n        modules = OrderedDict()\n        for module_config in modules_config:\n            # Yang B. modification: apply mappings, make it compatible to new implementations\n            module_type = sbert_mappings.get(module_config['type'], module_config['type'])\n            module_class = import_from_string(module_type)\n            module = module_class.load(os.path.join(model_path, module_config['path']))\n            modules[module_config['name']] = module\n\n        return modules\n\n    def _load_auto_model(self, model_name_or_path):\n        \"\"\"\n        Creates a simple Transformer + Mean Pooling model and returns the modules\n        \"\"\"\n        # Yang B. modification: check if we automatically load non-sbert model\n        if self.load_sbert_only:\n            raise FileNotFoundError(\"No sentence-transformers model found with name {}, and you set `load_sbert_only` to True\".format(model_name_or_path))\n\n        self.logger.warning(\"No sentence-transformers model found with name {}. Creating a new one with MEAN pooling.\".format(model_name_or_path))\n        transformer_model = Transformer(model_name_or_path)\n        pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), 'mean')\n        return [transformer_model, pooling_model]\n\n    def set_module_attribute(self, module_class, key, value):\n        for module in self.get_modules():\n            if isinstance(module, module_class):\n                setattr(module, key, value)\n    \n    def get_module_attribute(self, key, default_value=None):\n        for module in self.get_modules():\n            if hasattr(module, key):\n                return getattr(module, key)\n            \n        if key == 'teacher_model_name':\n            if 'clip-ViT-B-32' in self.model_name_or_path:\n                return 'clip-ViT-B-32'\n            if 'clip-ViT-B-16' in self.model_name_or_path:\n                return 'clip-ViT-B-16'\n            if 'clip-ViT-L-14' in self.model_name_or_path:\n                return 'clip-ViT-L-14'\n\n        return default_value\n\n    def get_modules(self):\n        return [self._modules[_] for _ in iter(self._modules)]\n    \n    def _get_specific_model(self, before=True, instances=(Projector, Decoder), device=None, return_modules_only: bool = False):\n        \"\"\"only keep related modules\"\"\"\n        modules = self.get_modules()\n        idx = 0\n        for module in modules:\n            if isinstance(module, instances):\n                break\n            idx += 1\n\n        device = device or self.device\n\n        if before:\n            # get modules < idx\n            if idx == 0:\n                return None  \n            if return_modules_only:\n                return modules[:idx]\n            model = Framework(modules=modules[:idx], device=device)\n        else:\n            # get modules >= idx\n            if idx == len(modules):\n                return None\n            if return_modules_only:\n                return modules[idx:]\n            model = Framework(modules=modules[idx:], device=device)\n\n        model.to(device)\n        return model\n\n    def get_encoding_model(self, device=None):\n        \"\"\"return a model that contains modules only related to encoding\"\"\"\n        return self._get_specific_model(before=True, instances=(Projector, Decoder), device=device or self._target_device)\n    \n    def get_encoding_modules(self) -> List[nn.Module]:\n        \"\"\"return modules only related to encoding\"\"\"\n        return self._get_specific_model(before=True, instances=(Projector, Decoder), return_modules_only=True)\n\n    def get_decoding_model(self, device=None):\n        \"\"\"return a model that contains modules only related to decoding\"\"\"\n        return self._get_specific_model(before=False, instances=(Projector, Decoder), device=device or self._target_device)\n    \n    def get_decoding_modules(self) -> List[nn.Module]:\n        \"\"\"return modules only related to decoding\"\"\"\n        return self._get_specific_model(before=False, instances=(Projector, Decoder), return_modules_only=True)\n\n    def tokenize(self, texts: Union[List[str], List[Dict], List[Tuple[str, str]]]):\n        module = self._first_module()\n        if hasattr(module, 'tokenize'):\n            return module.tokenize(texts)\n        return {}\n\n    def decoder_tokenize(self, texts: List[str], langs: Optional[List[str]]=None):\n        module = self._last_module()\n        if isinstance(module, Decoder):\n            return module.tokenize(texts, langs)\n        return {}\n    \n    @property\n    def tokenizer(self):\n        \"\"\"Property to get the tokenizer that is used by this model\"\"\"\n        module = self._first_module()\n        if hasattr(module, 'tokenizer'):\n            return module.tokenizer\n        return None\n    \n    @property\n    def decoder_tokenizer(self):\n        \"\"\"Property to get the decoder tokenizer that is used by this model\"\"\"\n        module = self._last_module()\n        if isinstance(module, Decoder):\n            return module.tokenizer\n        return None\n    \n    @property\n    def device(self):\n        return self._target_device\n\n    def smart_batching_collate(self, batch):\n        \"\"\"Transforms a batch of InputExample to features requested by this model\"\"\"\n        texts = []\n        labels = []\n        langs = []\n\n        for example in batch:\n            texts.append(example.trg_text)\n            if example.label is not None:\n                labels.append(example.label)\n            if example.lang:\n                langs.append(example.lang)\n\n        labels = torch.tensor(np.array(labels)) if len(labels) else None\n\n        features = {}\n\n        # prepare input_ids, attention_mask, ...\n        tokenized_results = self.tokenize(texts)\n\n        # mask tokenized results if specified\n        if getattr(self, 'use_masking', False):\n            # self.use_masking and self.mask_prob is defined in Framework.fit\n            random_masking_(\n                tokenizer=self.tokenizer, \n                tokenized_results=tokenized_results, \n                mask_prob=getattr(self, 'mask_prob', 0.15)\n            )\n\n        features.update(tokenized_results) \n\n        # prepare decoder_input_ids, decoder_attention_mask, ...\n        features.update(self.decoder_tokenize(texts, langs if len(langs) else None)) \n\n        features['source_embedding'] = labels # used for decoding (optional)\n        \n        return features, labels\n\n    def fit(self,\n            train_objectives: Iterable[Tuple[DataLoader, nn.Module]],\n            evaluator: SentenceEvaluator = None,\n            epochs: int = 1,\n            steps_per_epoch = None,\n            scheduler: str = 'WarmupLinear',\n            warmup_steps: int = 10000,\n            optimizer_class: Type[Optimizer] = torch.optim.AdamW,\n            optimizer_params : Dict[str, object]= {'lr': 2e-5},\n            weight_decay: float = 0.01,\n            evaluation_steps: int = 0,\n            output_path: str = None,\n            save_best_model: bool = True,\n            max_grad_norm: float = 1,\n            use_amp: bool = False,\n            callback: Callable[[float, int, int], None] = None,\n            show_progress_bar: bool = True,\n            checkpoint_path: str = None,\n            checkpoint_save_steps: int = 500,\n            checkpoint_save_total_limit: int = 0,\n            log_every: int = 500,\n            seed: int = 42,\n            use_masking: bool = False,\n            mask_prob: float = 0.15,\n            ):\n        \"\"\"\n        Train the model with the given training objective\n        Each training objective is sampled in turn for one batch.\n        We sample only as many batches from each objective as there are in the smallest one\n        to make sure of equal training with each dataset.\n\n        :param train_objectives: Tuples of (DataLoader, LossFunction). Pass more than one for multi-task learning\n        :param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n        :param epochs: Number of epochs for training\n        :param steps_per_epoch: Number of training steps per epoch. If set to None (default), one epoch is equal the DataLoader size from train_objectives.\n        :param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n        :param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n        :param optimizer_class: Optimizer\n        :param optimizer_params: Optimizer parameters\n        :param weight_decay: Weight decay for model parameters\n        :param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n        :param output_path: Storage path for the model and evaluation files\n        :param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n        :param max_grad_norm: Used for gradient normalization.\n        :param use_amp: Use Automatic Mixed Precision (AMP). Only for Pytorch >= 1.6.0\n        :param callback: Callback function that is invoked after each evaluation.\n                It must accept the following three parameters in this order:\n                `score`, `epoch`, `steps`\n        :param show_progress_bar: If True, output a tqdm progress bar\n        :param checkpoint_path: Folder to save checkpoints during training\n        :param checkpoint_save_steps: Will save a checkpoint after so many steps\n        :param checkpoint_save_total_limit: Total number of checkpoints to store\n        \"\"\"\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n        cudnn.benchmark = True\n\n        self.use_masking = use_masking\n        self.mask_prob = mask_prob\n\n        ##Add info to model card\n        #info_loss_functions = \"\\n\".join([\"- {} with {} training examples\".format(str(loss), len(dataloader)) for dataloader, loss in train_objectives])\n        info_loss_functions =  []\n        for dataloader, loss in train_objectives:\n            info_loss_functions.extend(ModelCardTemplate.get_train_objective_info(dataloader, loss))\n        info_loss_functions = \"\\n\\n\".join([text for text in info_loss_functions])\n\n        info_fit_parameters = json.dumps({\"evaluator\": fullname(evaluator), \"epochs\": epochs, \"steps_per_epoch\": steps_per_epoch, \"scheduler\": scheduler, \"warmup_steps\": warmup_steps, \"optimizer_class\": str(optimizer_class),  \"optimizer_params\": optimizer_params, \"weight_decay\": weight_decay, \"evaluation_steps\": evaluation_steps, \"max_grad_norm\": max_grad_norm }, indent=4, sort_keys=True)\n        self._model_card_text = None\n        self._model_card_vars['{TRAINING_SECTION}'] = ModelCardTemplate.__TRAINING_SECTION__.replace(\"{LOSS_FUNCTIONS}\", info_loss_functions).replace(\"{FIT_PARAMETERS}\", info_fit_parameters)\n\n\n        if use_amp:\n            from torch.cuda.amp import autocast\n            scaler = torch.cuda.amp.GradScaler()\n\n        self.to(self.device)\n\n        dataloaders = [dataloader for dataloader, _ in train_objectives]\n\n        # Use smart batching\n        for dataloader in dataloaders:\n            dataloader.collate_fn = self.smart_batching_collate\n\n        loss_models = [loss for _, loss in train_objectives]\n        for loss_model in loss_models:\n            loss_model.to(self.device)\n\n        self.best_score = -9999999\n\n        if steps_per_epoch is None or steps_per_epoch == 0:\n            steps_per_epoch = min([len(dataloader) for dataloader in dataloaders])\n\n        num_train_steps = int(steps_per_epoch * epochs)\n\n        # Prepare optimizers\n        optimizers = []\n        schedulers = []\n        for loss_model in loss_models:\n            param_optimizer = list(loss_model.named_parameters())\n\n            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n            optimizer_grouped_parameters = [\n                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n            ]\n\n            optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n            scheduler_obj = self._get_scheduler(optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n\n            optimizers.append(optimizer)\n            schedulers.append(scheduler_obj)\n\n\n        global_step = 0\n        data_iterators = [iter(dataloader) for dataloader in dataloaders]\n\n        num_train_objectives = len(train_objectives)\n\n        skip_scheduler = False\n        train_start_time = time.time()\n        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n            \n            training_steps = 0\n            metric_logger = MetricLogger(delimiter=\"  \")\n            start_time = time.time()\n\n            for loss_model in loss_models:\n                loss_model.zero_grad()\n                loss_model.train()\n\n            for _ in trange(steps_per_epoch, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n                for train_idx in range(num_train_objectives):\n                    loss_model = loss_models[train_idx]\n                    optimizer = optimizers[train_idx]\n                    scheduler = schedulers[train_idx]\n                    data_iterator = data_iterators[train_idx]\n\n                    try:\n                        data = next(data_iterator)\n                    except StopIteration:\n                        data_iterator = iter(dataloaders[train_idx])\n                        data_iterators[train_idx] = data_iterator\n                        data = next(data_iterator)\n\n                    features, labels = data\n                    labels = labels.to(self.device) if labels is not None else None\n                    features = batch_to_device(features, self.device)\n\n                    if use_amp:\n                        with autocast():\n                            loss_value, loss_msg_dict = loss_model(features, labels)\n\n                        scale_before_step = scaler.get_scale()\n                        scaler.scale(loss_value).backward()\n                        scaler.unscale_(optimizer)\n                        torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n                        scaler.step(optimizer)\n                        scaler.update()\n\n                        skip_scheduler = scaler.get_scale() != scale_before_step\n                    else:\n                        loss_value, loss_msg_dict = loss_model(features, labels)\n                        loss_value.backward()\n                        torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n                        optimizer.step()\n                    \n                    metric_logger.update(**loss_msg_dict)\n\n                    optimizer.zero_grad()\n\n                    if not skip_scheduler:\n                        scheduler.step()\n\n                training_steps += 1\n                global_step += 1\n\n                if log_every > 0 and global_step % log_every == 0:\n                    self.log_training_info(metric_logger, epoch, training_steps, steps_per_epoch)\n\n                if evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n                    self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n\n                    for loss_model in loss_models:\n                        loss_model.zero_grad()\n                        loss_model.train()\n\n                    info = f\"[BEST] {self.best_score}\"\n                    self.logger.info(info)\n\n                if checkpoint_path is not None and checkpoint_save_steps is not None and checkpoint_save_steps > 0 and global_step % checkpoint_save_steps == 0:\n                    self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n            \n            metric_logger.synchronize_between_processes()\n            info = f\"Averaged stats: {metric_logger.global_avg()}\"\n            self.logger.info(info)\n            time_string = 'Train epoch time: ' + str(datetime.timedelta(seconds=int(time.time() - start_time)))\n            self.logger.info(time_string)\n\n            self._eval_during_training(evaluator, output_path, save_best_model, epoch, -1, callback)\n\n            if checkpoint_path is not None and checkpoint_save_steps is None:\n                self._save_checkpoint_epoch(checkpoint_path, checkpoint_save_total_limit, epoch)\n\n        if evaluator is None and output_path is not None:   #No evaluator, but output path: save final model version\n            self.save(output_path)\n\n        if checkpoint_path is not None and checkpoint_save_steps is not None:\n            self._save_checkpoint(checkpoint_path, checkpoint_save_total_limit, global_step)\n        \n        time_string = 'Train time: ' + str(datetime.timedelta(seconds=int(time.time() - train_start_time)))\n        self.logger.info(time_string)\n    \n    def log_training_info(self, \n            metric_logger: MetricLogger, \n            epoch: int, \n            step: int, \n            steps_per_epoch: int,\n            delimiter: str = '  ',\n            ):\n        \n        _msg = [\n            'Epoch: {epoch} [{step:' + f'{len(str(steps_per_epoch))}' + 'd} / {steps_per_epoch}]',\n            '{meters}',\n        ]\n\n        if torch.cuda.is_available():\n            _msg.append('max mem: {memory:.0f}')\n            MB = 1024.0 * 1024.0\n            info = delimiter.join(_msg).format(\n                epoch=epoch, \n                step=step, \n                steps_per_epoch=steps_per_epoch, \n                meters=str(metric_logger), \n                memory=torch.cuda.max_memory_allocated() / MB\n            )\n        else:\n            info = delimiter.join(_msg).format(\n                epoch=epoch, \n                step=step, \n                steps_per_epoch=steps_per_epoch, \n                meters=str(metric_logger)\n            )\n        \n        self.logger.info(info)\n    \n    def _save_checkpoint_epoch(self, checkpoint_path, checkpoint_save_total_limit, epoch):\n        # Store new checkpoint\n        self.save(os.path.join(checkpoint_path, str(epoch)))\n\n        # Delete old checkpoints\n        if checkpoint_save_total_limit is not None and checkpoint_save_total_limit > 0:\n            old_checkpoints = []\n            for subdir in os.listdir(checkpoint_path):\n                if subdir.isdigit():\n                    old_checkpoints.append({'epoch': int(subdir), 'path': os.path.join(checkpoint_path, subdir)})\n\n            if len(old_checkpoints) > checkpoint_save_total_limit:\n                old_checkpoints = sorted(old_checkpoints, key=lambda x: x['epoch'])\n                shutil.rmtree(old_checkpoints[0]['path'])\n\n    @staticmethod\n    def load(input_path):\n        return Framework(input_path)\n    \n    def save_to_hub(self,\n                repo_name: str,\n                private: Optional[bool] = None,\n                commit_message: str = \"Add new ZeroNLG model.\",\n                local_model_path: Optional[str] = None,\n                exist_ok: bool = False,\n                replace_model_card: bool = False,\n                train_datasets: Optional[List[str]] = None):\n        \"\"\"\n        Uploads all elements of this Sentence Transformer to a new HuggingFace Hub repository.\n\n        Yang B. modification: \n        1) delete organization to avoid bugs;\n\n        :param repo_name: Repository name for your model in the Hub.\n        :param private: Set to true, for hosting a prive model\n        :param commit_message: Message to commit while pushing.\n        :param local_model_path: Path of the model locally. If set, this file path will be uploaded. Otherwise, the current model will be uploaded\n        :param exist_ok: If true, saving to an existing repository is OK. If false, saving only to a new repository is possible\n        :param replace_model_card: If true, replace an existing model card in the hub with the automatically created model card\n        :param train_datasets: Datasets used to train the model. If set, the datasets will be added to the model card in the Hub.\n        :return: The url of the commit of your model in the given repository.\n        \"\"\"\n        token = HfFolder.get_token()\n        if token is None:\n            raise ValueError(\"You must login to the Hugging Face hub on this computer by typing `transformers-cli login`.\")\n\n        endpoint = \"https://huggingface.co\"\n        repo_url = HfApi(endpoint=endpoint).create_repo(\n                repo_name,\n                token=token,\n                private=private,\n                repo_type=None,\n                exist_ok=exist_ok,\n            )\n        full_model_name = repo_url[len(endpoint)+1:].strip(\"/\")\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            # First create the repo (and clone its content if it's nonempty).\n            self.logger.info(\"Create repository and clone it if it exists\")\n            repo = Repository(tmp_dir, clone_from=repo_url)\n\n            # If user provides local files, copy them.\n            if local_model_path:\n                copy_tree(local_model_path, tmp_dir)\n            else:  # Else, save model directly into local repo.\n                create_model_card = False # TODO: zeronlg model card\n                self.save(tmp_dir, model_name=full_model_name, create_model_card=create_model_card, train_datasets=train_datasets)\n\n            #Find files larger 5M and track with git-lfs\n            large_files = []\n            for root, dirs, files in os.walk(tmp_dir):\n                for filename in files:\n                    file_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(file_path, tmp_dir)\n\n                    if os.path.getsize(file_path) > (5 * 1024 * 1024):\n                        large_files.append(rel_path)\n\n            if len(large_files) > 0:\n                self.logger.info(\"Track files with git lfs: {}\".format(\", \".join(large_files)))\n                repo.lfs_track(large_files)\n\n            self.logger.info(\"Push model to the hub. This might take a while\")\n            push_return = repo.push_to_hub(commit_message=commit_message)\n\n            def on_rm_error(func, path, exc_info):\n                # path contains the path of the file that couldn't be removed\n                # let's just assume that it's read-only and unlink it.\n                try:\n                    os.chmod(path, stat.S_IWRITE)\n                    os.unlink(path)\n                except:\n                    pass\n\n            # Remove .git folder. On Windows, the .git folder might be read-only and cannot be deleted\n            # Hence, try to set write permissions on error\n            try:\n                for f in os.listdir(tmp_dir):\n                    shutil.rmtree(os.path.join(tmp_dir, f), onerror=on_rm_error)\n            except Exception as e:\n                self.logger.warning(\"Error when deleting temp folder: {}\".format(str(e)))\n                pass\n\n        return push_return", ""]}
{"filename": "zeronlg/losses/LossManager.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\nfrom typing import Iterable, Dict, Tuple\nfrom ..Framework import Framework\n\n\nclass LossManager(nn.Module):\n    def __init__(self, \n                 model: Framework, \n                 loss_mse_scale: float = 1.0, \n                 loss_at_teacher_scale: float = 0.0, \n                 loss_at_student_scale: float = 0.0, \n                 loss_contrastive_scale: float = 0.0,\n                 ):\n        \"\"\"\n        :param model: Framework based on sentence-transformers\n        :loss_mse_scale: The scale of MSE loss between teacher's and student's sentence embeddings\n        :loss_at_teacher_scale: The scale of cross-entropy loss of decoding on teacher's sentence embeddings (translation)\n        :loss_at_teacher_scale: The scale of cross-entropy loss of decoding on student's sentence embeddings (auto-encoding)\n        :loss_contrastive_scale: The scale of contrastive loss between teacher's and student's sentence embeddings\n        \"\"\"\n        super(LossManager, self).__init__()\n        self.model = model\n\n        self.loss_fct = nn.MSELoss()\n\n        self.loss_mse_scale = loss_mse_scale\n        self.loss_at_teacher_scale = loss_at_teacher_scale\n        self.loss_at_student_scale = loss_at_student_scale\n        self.loss_contrastive_scale = loss_contrastive_scale\n\n        if loss_contrastive_scale > 0:\n            self.temp = nn.Parameter(torch.ones([]) * 0.07) # identical to CLIP\n    \n    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor) -> Tuple[Tensor, Dict[str, float]]:\n        outputs = self.model(sentence_features)\n\n        loss, loss_msg_dict = 0, {}\n        for name in ['mse', 'at_teacher', 'at_student', 'contrastive']:\n            this_loss, this_dict = getattr(self, f'forward_{name}')(outputs, labels)\n            loss += this_loss\n            loss_msg_dict.update(this_dict)\n\n        return loss, loss_msg_dict\n\n    def forward_mse(self, \n            outputs: Dict[str, Tensor], \n            labels: Tensor,\n            name: str = 'loss_mse'\n            ) -> Tuple[Tensor, Dict[str, float]]:\n        \"\"\"\n        Computes the MSE loss between the computed sentence embedding and a target sentence embedding. This loss\n        is used when extending sentence embeddings to new languages as described in our publication\n        Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation: https://arxiv.org/abs/2004.09813\n\n        For an example, see the documentation on extending language models to new languages.\n        \"\"\"\n        if self.loss_mse_scale > 0:\n            loss = self.loss_fct(outputs['sentence_embedding'], labels)\n            return self.loss_mse_scale * loss, {name: loss.detach().cpu().item()}\n        \n        return 0, {}\n\n    def forward_at_teacher(self, \n            outputs: Dict[str, Tensor], \n            labels: Tensor,\n            name: str = 'loss_at_teacher',\n            ) -> Tuple[Tensor, Dict[str, float]]:\n        \n        if self.loss_at_teacher_scale > 0:\n            loss = outputs['loss_at_teacher']\n            return self.loss_at_teacher_scale * loss, {name: loss.detach().cpu().item()}\n        \n        return 0, {}\n\n    def forward_at_student(self,\n            outputs: Dict[str, Tensor], \n            labels: Tensor,\n            name: str = 'loss_at_student',\n            ) -> Tuple[Tensor, Dict[str, float]]:\n\n        if self.loss_at_student_scale > 0:\n            loss = outputs['loss_at_student']\n            return self.loss_at_student_scale * loss, {name: loss.detach().cpu().item()}\n        \n        return 0, {}\n\n    def forward_contrastive(self,\n            outputs: Dict[str, Tensor], \n            labels: Tensor,\n            name: str = 'loss_cl'\n            ) -> Tuple[Tensor, Dict[str, float]]:\n\n        if self.loss_contrastive_scale > 0:\n            with torch.no_grad():\n                self.temp.clamp_(0.001, 0.5)\n        \n            feats_student = F.normalize(outputs['sentence_embedding'], dim=-1)\n            feats_teacher = F.normalize(labels, dim=-1)\n            logits_s2t = feats_student @ feats_teacher.t() / self.temp\n            logits_t2s = feats_teacher @ feats_student.t() / self.temp\n            \n            cl_labels = torch.arange(logits_s2t.size(0), device=logits_s2t.device)\n\n            loss_s2t = F.cross_entropy(logits_s2t, cl_labels, reduction='mean')\n            loss_t2s = F.cross_entropy(logits_t2s, cl_labels, reduction='mean')\n            loss = (loss_s2t + loss_t2s) / 2\n            \n            return self.loss_contrastive_scale * loss, {\n                name: loss.detach().cpu().item(),\n                'temp': self.temp.detach().cpu().item(),\n            }\n        \n        return 0, {}", ""]}
{"filename": "zeronlg/losses/__init__.py", "chunked_list": ["from .LossManager import LossManager\n"]}
{"filename": "zeronlg/evaluation/CaptionEvaluator.py", "chunked_list": ["import os\nimport json\nimport torch\nimport logging\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler\nfrom typing import Dict, Any, Union\n\nfrom .. import Framework, ZeroNLG", "\nfrom .. import Framework, ZeroNLG\nfrom ..datasets import CaptionDataset\nfrom ..utils import coco_caption_eval\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])", "                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nglobal_logger = logging.getLogger(__name__)\n\n\nclass CaptionEvaluator:\n    def __init__(self, \n            loader: DataLoader, \n            gt_file_path: str, \n            evaluation_settings: Dict[str, Any] = {'lang': 'en'}, \n            mode: str = 'val', \n            logger: logging.Logger = None, \n            monitor: str = 'CIDEr', \n            with_epoch: bool = False, \n            with_steps: bool = False,\n            auto_save: bool = True\n        ):\n        super().__init__()\n        assert mode in ['val', 'test']\n        assert 'lang' in evaluation_settings\n        assert isinstance(loader.dataset, CaptionDataset)\n        assert loader.dataset.clip_model is not None\n\n        self.loader = loader\n        self.gt_file_path = gt_file_path\n        self.evaluation_settings = evaluation_settings\n        self.mode = mode\n        self.logger = logger or global_logger\n        self.monitor = monitor\n        self.with_epoch = with_epoch\n        self.with_steps = with_steps\n        self.auto_save = auto_save\n\n    def log(self, msg):\n        self.logger.info(msg)\n    \n    @torch.no_grad()\n    def __call__(self, \n            model: Union[Framework, ZeroNLG, str], \n            output_path: str = None, \n            epoch: int = -1, \n            steps: int = -1, \n            no_score: bool=False, \n            print_sent: bool=False\n        ) -> float:\n\n        prefix = [self.mode]\n        if self.with_epoch:\n            prefix.append(f'epoch{epoch}')\n        if self.with_steps:\n            prefix.append(f'steps{steps}')\n        prefix = '_'.join(prefix)\n\n        if output_path:\n            result_file = os.path.join(output_path, f'{prefix}_captions.json')\n            detailed_scores_file = os.path.join(output_path, f'{prefix}_detailed_scores.json')\n            scores_file = os.path.join(output_path, f'{prefix}_scores.json')\n\n        if type(model) is str:\n            zeronlg = ZeroNLG(model)\n        elif isinstance(model, Framework):\n            zeronlg = ZeroNLG(\n                multilingual_model=model,\n                device=model.device,\n                load_clip_model=False,\n            )\n        else:\n            assert isinstance(model, ZeroNLG)\n            zeronlg = model\n\n        results = []\n        for batch in tqdm(self.loader):\n            image_ids, image_embs = batch\n\n            outputs = zeronlg.forward_caption(\n                image_embs=image_embs,\n                **self.evaluation_settings,\n            )\n\n            for caption, image_id in zip(outputs, image_ids):\n                results.append({\"image_id\": image_id, \"caption\": caption})\n                if print_sent:\n                    print(image_id, caption)\n        \n        if output_path:\n            self.log(f'Save caption results to {result_file}')\n            json.dump(results, open(result_file, 'w'))\n        \n        if self.auto_save:\n            self.loader.dataset.save_pickle()\n\n        if not no_score:\n            coco_test = coco_caption_eval(self.gt_file_path, result_file, eval_lang=self.evaluation_settings['lang'])\n            \n            if output_path:\n                self.log(f'Save detailed scores to {detailed_scores_file}')\n                json.dump(coco_test.evalImgs, open(detailed_scores_file, 'w'))\n\n            if output_path:\n                self.log(f'Save scores to {scores_file}')\n                json.dump(coco_test.eval, open(scores_file, 'w'))\n\n            for k, v in coco_test.eval.items():\n                self.log(f'[{prefix}] {k} {v}')\n\n            score = coco_test.eval[self.monitor]\n            return score", ""]}
{"filename": "zeronlg/evaluation/RetrievalEvaluator.py", "chunked_list": ["import os\nimport json\nimport logging\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler\nfrom typing import Dict, Union, List, Any\n", "from typing import Dict, Union, List, Any\n\nfrom .. import Framework, ZeroNLG\nfrom ..datasets import CaptionDatasetForRetrieval\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])", "                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nglobal_logger = logging.getLogger(__name__)\n\n\ndef construct_gts(annotation: List[Dict[str, Any]]):\n    img2text = {}\n    text2img = {}\n\n    text_id = 0\n    for img_id, ann in enumerate(annotation):\n        assert 'caption' in ann, f'The annotation item {ann} does not contain the key `caption`'\n\n        captions = ann['caption']\n        if not isinstance(captions, (list, tuple)):\n            captions = [captions]\n\n        img2text[img_id] = []\n        for _ in captions:\n            img2text[img_id].append(text_id)\n            text2img[text_id] = img_id\n            text_id += 1\n    \n    return img2text, text2img", "\n\n@torch.no_grad()\ndef retrieval_evaluation(\n        scores_i2t: np.ndarray, \n        scores_t2i: np.ndarray, \n        annotation: List[Dict[str, Any]],\n        topk: int = 10,\n    ):\n    img2text, text2img = construct_gts(annotation)\n\n    # Images->Text\n    ranks = np.zeros(scores_i2t.shape[0])\n    topk_inds_i2t = np.zeros((scores_i2t.shape[0], topk))\n    for index, score in enumerate(scores_i2t):\n        inds = np.argsort(score)[::-1]\n        # Score\n        rank = 1e20\n        for i in img2text[index]:\n            tmp = np.where(inds == i)[0][0]\n            if tmp < rank:\n                rank = tmp\n        ranks[index] = rank\n        topk_inds_i2t[index] = inds[:topk]\n\n    # Compute metrics\n    tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n    tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n    tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n\n    # Text->Images\n    ranks = np.zeros(scores_t2i.shape[0])\n    topk_inds_t2i = np.zeros((scores_t2i.shape[0], topk))\n    for index, score in enumerate(scores_t2i):\n        inds = np.argsort(score)[::-1]\n        ranks[index] = np.where(inds == text2img[index])[0][0]\n        topk_inds_t2i[index] = inds[:topk]\n\n    # Compute metrics\n    ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n    ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n    ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n\n    tr_mean = (tr1 + tr5 + tr10) / 3\n    ir_mean = (ir1 + ir5 + ir10) / 3\n    r_mean = (tr_mean + ir_mean) / 2\n\n    scores = {\n        'txt_r1': tr1,\n        'txt_r5': tr5,\n        'txt_r10': tr10,\n        'txt_r_mean': tr_mean,\n        'img_r1': ir1,\n        'img_r5': ir5,\n        'img_r10': ir10,\n        'img_r_mean': ir_mean,\n        'r_mean': r_mean\n    }\n    \n    return scores, topk_inds_i2t, topk_inds_t2i", "\n\n@torch.no_grad()\ndef retrieval_evaluation_n_fold(\n        scores_i2t: np.ndarray, \n        scores_t2i: np.ndarray, \n        annotation: List[Dict[str, Any]],\n        n_fold: int = 5,\n    ):\n    n_fold_annotations = np.array_split(annotation, n_fold)\n\n    all_tr = [[], [], []]\n    all_ir = [[], [], []]\n    all_mean = [[], [], []]\n    i_begin, t_begin = 0, 0\n    for i in range(n_fold):\n        img2text, text2img = construct_gts(n_fold_annotations[i])\n\n        i_end = i_begin + len(img2text)\n        t_end = t_begin + len(text2img)\n\n        # Images->Text\n        ranks = np.zeros(len(img2text))\n        for index, score in enumerate(scores_i2t[i_begin:i_end, t_begin:t_end]):\n            inds = np.argsort(score)[::-1]\n            # Score\n            rank = 1e20\n            for i in img2text[index]:\n                tmp = np.where(inds == i)[0][0]\n                if tmp < rank:\n                    rank = tmp\n            ranks[index] = rank\n\n        # Compute metrics\n        tr1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n        tr5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n        tr10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n        all_tr[0].append(tr1)\n        all_tr[1].append(tr5)\n        all_tr[2].append(tr10)\n\n        # Text->Images\n        ranks = np.zeros(len(text2img))\n        for index, score in enumerate(scores_t2i[t_begin:t_end, i_begin:i_end]):\n            inds = np.argsort(score)[::-1]\n            ranks[index] = np.where(inds == text2img[index])[0][0]\n\n        # Compute metrics\n        ir1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n        ir5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n        ir10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n        all_ir[0].append(ir1)\n        all_ir[1].append(ir5)\n        all_ir[2].append(ir10)\n\n        tr_mean = (tr1 + tr5 + tr10) / 3\n        ir_mean = (ir1 + ir5 + ir10) / 3\n        r_mean = (tr_mean + ir_mean) / 2\n        all_mean[0].append(tr_mean)\n        all_mean[1].append(ir_mean)\n        all_mean[2].append(r_mean)\n\n        i_begin, t_begin = i_end, t_end\n\n    scores = {\n        f'{n_fold}fold_txt_r1': np.array(all_tr[0]).mean(),\n        f'{n_fold}fold_txt_r5': np.array(all_tr[1]).mean(),\n        f'{n_fold}fold_txt_r10': np.array(all_tr[2]).mean(),\n        f'{n_fold}fold_txt_r_mean': np.array(all_mean[0]).mean(),\n        f'{n_fold}fold_img_r1': np.array(all_ir[0]).mean(),\n        f'{n_fold}fold_img_r5': np.array(all_ir[1]).mean(),\n        f'{n_fold}fold_img_r10': np.array(all_ir[2]).mean(),\n        f'{n_fold}fold_img_r_mean': np.array(all_mean[1]).mean(),\n        f'{n_fold}fold_r_mean': np.array(all_mean[2]).mean()\n    }\n    return scores", "\n\nclass RetrievalEvaluator:\n    def __init__(self, \n            loader: DataLoader, \n            mode: str = 'val', \n            logger: logging.Logger = None, \n            monitor: str = 'r_mean',\n            with_epoch: bool = False, \n            with_steps: bool = False,\n            auto_save: bool = True,\n            n_fold: int = 1,\n        ):\n        super().__init__()\n        assert mode in ['val', 'test']\n        assert isinstance(loader.dataset, CaptionDatasetForRetrieval)\n        assert loader.dataset.clip_model is not None\n\n        self.loader = loader\n        self.mode = mode\n        self.logger = logger or global_logger\n        self.monitor = monitor\n        self.with_epoch = with_epoch\n        self.with_steps = with_steps\n        self.auto_save = auto_save\n        self.n_fold = n_fold\n\n    def log(self, msg):\n        self.logger.info(msg)\n    \n    @torch.no_grad()\n    def __call__(self, \n            model: Union[Framework, ZeroNLG, str], \n            output_path: str = None, \n            epoch: int = -1, \n            steps: int = -1, \n            **kwargs,\n        ) -> float:\n\n        prefix = [self.mode]\n        if self.with_epoch:\n            prefix.append(f'epoch{epoch}')\n        if self.with_steps:\n            prefix.append(f'steps{steps}')\n        prefix = '_'.join(prefix)\n\n        if output_path:\n            result_i2t_file = os.path.join(output_path, f'{prefix}_i2t.npy')\n            result_t2i_file = os.path.join(output_path, f'{prefix}_t2i.npy')\n            scores_file = os.path.join(output_path, f'{prefix}_scores.json')\n\n        if type(model) is str:\n            zeronlg = ZeroNLG(model)\n        elif isinstance(model, Framework):\n            zeronlg = ZeroNLG(\n                multilingual_model=model,\n                device=model.device,\n                load_clip_model=False,\n            )\n        else:\n            assert isinstance(model, ZeroNLG)\n            zeronlg = model\n\n        all_image_embs, all_text_embs = [], []\n        for batch in tqdm(self.loader):\n            _, image_embs, texts = batch\n\n            image_embs = zeronlg.get_image_embeddings(\n                image_embs=image_embs,\n                normalize=True,\n                mean_pooling=True, # TODO: we now always apply mean pooling\n            )\n\n            text_embs = zeronlg.get_text_embeddings(\n                texts=texts,\n                normalize=True,\n                batch_size=image_embs.size(0),\n            )\n\n            all_image_embs.append(image_embs)\n            all_text_embs.append(text_embs)\n        \n        all_image_embs = torch.cat(all_image_embs, dim=0)\n        all_text_embs = torch.cat(all_text_embs, dim=0)\n\n        scores_i2t = all_image_embs @ all_text_embs.t()\n        scores_t2i = all_text_embs @ all_image_embs.t()\n\n        scores, topk_inds_i2t, topk_inds_t2i = retrieval_evaluation(\n            scores_i2t=scores_i2t.cpu().numpy(),\n            scores_t2i=scores_t2i.cpu().numpy(),\n            annotation=self.loader.dataset.annotation,\n        )\n\n        if self.n_fold > 1:\n            # MSCOCO 1K test\n            scores.update(\n                retrieval_evaluation_n_fold(\n                    scores_i2t=scores_i2t.cpu().numpy(),\n                    scores_t2i=scores_t2i.cpu().numpy(),\n                    annotation=self.loader.dataset.annotation,\n                    n_fold=self.n_fold,\n                )\n            )\n        \n        for k, v in scores.items():\n            self.log(f'[{prefix}] {k} {v}')\n\n        if output_path:\n            self.log(f'Save results to {result_i2t_file}, {result_t2i_file}')\n            np.save(result_i2t_file, topk_inds_i2t)\n            np.save(result_t2i_file, topk_inds_t2i)\n\n            self.log(f'Save scores to {scores_file}')\n            json.dump(scores, open(scores_file, 'w'))\n        \n        if self.auto_save:\n            self.loader.dataset.save_pickle()\n\n        score = scores[self.monitor]\n        return score", ""]}
{"filename": "zeronlg/evaluation/__init__.py", "chunked_list": ["from .CaptionEvaluator import CaptionEvaluator\nfrom .TranslateEvaluator import TranslateEvaluator\nfrom .RetrievalEvaluator import RetrievalEvaluator"]}
{"filename": "zeronlg/evaluation/TranslateEvaluator.py", "chunked_list": ["import os\nimport json\nimport torch\nimport logging\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import LoggingHandler\nfrom typing import Dict, Any, Union\n\nfrom .. import Framework, ZeroNLG", "\nfrom .. import Framework, ZeroNLG\nfrom ..datasets import TranslateDataset\nfrom ..utils import translate_eval\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])", "                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nglobal_logger = logging.getLogger(__name__)\n\n\nclass TranslateEvaluator:\n    def __init__(self, \n            loader: DataLoader, \n            evaluation_settings: Dict[str, Any] = {'lang': 'en'}, \n            mode: str = 'val', \n            logger: logging.Logger = None, \n            monitor: str = 'BLEU', \n            with_epoch: bool = False, \n            with_steps: bool = False,\n        ):\n        super().__init__()\n        assert mode in ['val', 'test']\n        assert isinstance(loader.dataset, TranslateDataset)\n\n        self.loader = loader\n        self.evaluation_settings = evaluation_settings\n        self.mode = mode\n        self.logger = logger or global_logger\n        self.monitor = monitor\n        self.with_epoch = with_epoch\n        self.with_steps = with_steps\n\n    def log(self, msg):\n        self.logger.info(msg)\n    \n    @torch.no_grad()\n    def __call__(self, \n            model: Union[Framework, ZeroNLG], \n            output_path: str = None, \n            epoch: int = -1, \n            steps: int = -1, \n            no_score: bool=False, \n            print_sent: bool=False\n        ) -> float:\n        \n        prefix = [self.mode]\n        if self.with_epoch:\n            prefix.append(f'epoch{epoch}')\n        if self.with_steps:\n            prefix.append(f'steps{steps}')\n        prefix = '_'.join(prefix)\n\n        source = self.loader.dataset.source_language\n        target = self.loader.dataset.target_language\n        self.evaluation_settings['lang'] = target\n\n        if output_path:\n            result_file = os.path.join(output_path, f'{prefix}_translations_{source}-{target}.json')\n            scores_file = os.path.join(output_path, f'{prefix}_scores_{source}-{target}.json')\n\n        if isinstance(model, Framework):\n            zeronlg = ZeroNLG(\n                multilingual_model=model,\n                device=model.device,\n                load_clip_model=False,\n            )\n        else:\n            assert isinstance(model, ZeroNLG)\n            zeronlg = model\n\n        results, gts = [], []\n        for batch in tqdm(self.loader):\n            source_sentences, target_sentences = batch\n\n            outputs = zeronlg.forward_translate(\n                texts=source_sentences,\n                **self.evaluation_settings,\n            )\n\n            results.extend(outputs)\n            gts.extend(target_sentences)\n\n            if print_sent:\n                for src, trg, pred in zip(source_sentences, target_sentences, outputs):\n                    print(f'[SRC] {src}; [TRG] {trg}; [PRED] {pred}')\n        \n        if output_path:\n            self.log(f'Save translation results to {result_file}')\n            json.dump(results, open(result_file, 'w'))\n\n        if not no_score:\n            scores = translate_eval(gts=gts, res=results, eval_lang=target)\n            \n            if output_path:\n                self.log(f'Save scores to {scores_file}')\n                json.dump(scores, open(scores_file, 'w'))\n\n            for k, v in scores.items():\n                self.log(f'[{prefix}] [{source} -> {target}] {k} {v}')\n            \n            score = scores[self.monitor]\n            return score", ""]}
{"filename": "zeronlg/utils/metriclogger.py", "chunked_list": ["import time\nimport datetime\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom collections import defaultdict, deque\n\nfrom .distributed import is_dist_avail_and_initialized\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        if np.isnan(value):\n            # There occurs gradient overflow in apex.amp\n            return\n        \n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        if len(self.deque) == 0:\n            return 0\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        if len(self.deque) == 0:\n            return 0\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        if self.count == 0:\n            return 0\n        return self.total / self.count\n\n    @property\n    def max(self):\n        if len(self.deque) == 0:\n            return 0\n        return max(self.deque)\n\n    @property\n    def value(self):\n        if len(self.deque) == 0:\n            return 0\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)", "\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        if np.isnan(value):\n            # There occurs gradient overflow in apex.amp\n            return\n        \n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        if len(self.deque) == 0:\n            return 0\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        if len(self.deque) == 0:\n            return 0\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        if self.count == 0:\n            return 0\n        return self.total / self.count\n\n    @property\n    def max(self):\n        if len(self.deque) == 0:\n            return 0\n        return max(self.deque)\n\n    @property\n    def value(self):\n        if len(self.deque) == 0:\n            return 0\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value)", "\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\", window_size=1, fmt='{value:.4f}'):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n        self.window_size = window_size\n        self.fmt = fmt\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if k not in self.meters:\n                self.add_meter(k, SmoothedValue(window_size=self.window_size, fmt=self.fmt))\n\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                \"{}: {}\".format(name, str(meter))\n            )\n        return self.delimiter.join(loss_str)\n\n    def global_avg(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                \"{}: {:.4f}\".format(name, meter.global_avg)\n            )\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, loader, print_freq, header=None):\n        if not header:\n            header = ''\n\n        dataset_len = len(loader)\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt='{avg:.4f}')\n        data_time = SmoothedValue(fmt='{avg:.4f}')\n        space_fmt = ':' + str(len(str(dataset_len))) + 'd'\n\n        _msg = [\n            '[{0' + space_fmt + '}/{1}]',\n            'eta: {eta}',\n            '{meters}',\n            'time: {time}',\n            'data: {data}'\n        ]\n        if torch.cuda.is_available():\n            _msg.append('max mem: {memory:.0f}')\n        _msg = self.delimiter.join(_msg)\n        MB = 1024.0 * 1024.0\n        iterable = iter(loader)\n        total_train_steps = dataset_len\n        \n        start_step = 0\n\n        for i in range(start_step, total_train_steps):\n            obj = next(iterable)\n\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n\n            log_msg = header + \" \" + _msg\n            if (i % dataset_len) % print_freq == 0 or i == dataset_len - 1:\n                eta_seconds = iter_time.global_avg * (dataset_len - i % dataset_len)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(log_msg.format(\n                        i % dataset_len, dataset_len, eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time),\n                        memory=torch.cuda.max_memory_allocated() / MB))\n                else:\n                    print(log_msg.format(\n                        i % dataset_len, dataset_len, eta=eta_string,\n                        meters=str(self),\n                        time=str(iter_time), data=str(data_time)))\n\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print('{} Total time: {} ({:.4f} s / it)'.format(\n            header, total_time_str, total_time / dataset_len))", "\n"]}
{"filename": "zeronlg/utils/op_vision.py", "chunked_list": ["import requests\nimport numpy as np\nfrom PIL import Image\nfrom typing import List, Tuple, Union\n\n\ndef get_uniform_frame_ids(\n        num_total_frames: int, \n        num_frames: int,\n    ) -> List[int]:\n    \n    if num_total_frames <= num_frames:\n        frame_ids = [_ for _ in range(num_total_frames)]\n        frame_ids = frame_ids + [frame_ids[-1]] * (num_frames - num_total_frames)\n        return frame_ids\n\n    # there will be num_frames intervals\n    ids = np.linspace(0, num_total_frames, num_frames + 1)\n    frame_ids = []\n    for i in range(num_frames):\n        # get the middle frame index of each interval\n        frame_ids.append(round((ids[i] + ids[i+1]) / 2))\n    return frame_ids", "\n\ndef process_images(\n        images: Union[str, List[str], Image.Image, List[Image.Image], List[List[Image.Image]]], \n        num_frames: int = 8\n    ) -> Tuple[List[Image.Image], bool, int, int]:\n\n    images = [images] if not isinstance(images, list) else images\n    batch_size = len(images)\n\n    num_images_per_input, is_video = None, False\n    if type(images[0]) is str:\n        if images[0].startswith(\"http://\") or images[0].startswith(\"https://\"):\n            # load images from remote URLs\n            images = [Image.open(requests.get(item, stream=True).raw) for item in images]\n        elif images[0].endswith('.mp4') or images[0].endswith('.avi'):\n            # load local videos\n            import decord\n            is_video = True\n            frames = []\n            for item in images:\n                reader = decord.VideoReader(item)\n                this_frames = reader.get_batch(get_uniform_frame_ids(len(reader), num_frames)).asnumpy()\n                this_frames = [Image.fromarray(frame) for frame in this_frames]\n                frames.extend(this_frames)\n            images = frames\n        else:\n            # load local images\n            images = [Image.open(item) for item in images]\n    elif isinstance(images[0], list):\n        assert isinstance(images[0][0], Image.Image), type(images[0][0])\n        num_images_per_input = len(images[0])\n        is_video = num_images_per_input > 1\n        images = [images[i][j] for i in range(len(images)) for j in range(num_images_per_input)]\n    else:\n        assert isinstance(images[0], Image.Image), type(images[0])\n    \n    return images, is_video, num_images_per_input or num_frames, batch_size", ""]}
{"filename": "zeronlg/utils/distributed.py", "chunked_list": ["import os\nimport torch\nimport torch.distributed as dist\n\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print", "\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()", "\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()", "def get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)", "\n\ndef save_on_master(*args, **kwargs):\n    if is_main_process():\n        torch.save(*args, **kwargs)\n\n\ndef init_distributed_mode(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n    elif 'SLURM_PROCID' in os.environ:\n        args.rank = int(os.environ['SLURM_PROCID'])\n        args.gpu = args.rank % torch.cuda.device_count()\n    else:\n        print('Not using distributed mode')\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = 'nccl'\n    print('| distributed init (rank {}): {}'.format(\n        args.rank, args.dist_url), flush=True)\n    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n                                         world_size=args.world_size, rank=args.rank)\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)", ""]}
{"filename": "zeronlg/utils/op_text.py", "chunked_list": ["import requests\nimport torch\nfrom torch import Tensor\nfrom PIL import Image\nfrom typing import List, Tuple, Union, Dict\n\n\ndef random_masking_(\n        tokenizer,\n        tokenized_results: Dict[str, Tensor],\n        mask_prob: float = 0.15,\n        replace_prob: float = 0.8,\n        random_prob: float = 0.1, \n    ):\n    \"\"\"\n    in-place operation to randomlly mask the input ids of tokenized results\n\n    :param tokenized_results: Obtained by tokenzier\n    :param mask_prob: Probability to mask a token\n    :param replace_prob: Probability to replace masked token with [MASK]\n    :param random_prob: Probability to replace masked token with a random token\n    \"\"\"\n    if len(tokenized_results) == 0:\n        return\n    \n    input_ids = tokenized_results['input_ids']\n    special_tokens_mask = [tokenizer.get_special_tokens_mask(ids.tolist(), already_has_special_tokens=True) for ids in input_ids]\n    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n\n    probability_matrix = torch.full(input_ids.shape, mask_prob)\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n\n    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n    indices_replaced = torch.bernoulli(torch.full(input_ids.shape, replace_prob)).bool() & masked_indices\n    input_ids[indices_replaced] = tokenizer.mask_token_id\n\n    # 10% of the time, we replace masked input tokens with random word\n    current_prob = random_prob / (1 - replace_prob)\n    indices_random = torch.bernoulli(torch.full(input_ids.shape, current_prob)).bool() & masked_indices & ~indices_replaced\n    random_words = torch.randint(len(tokenizer), input_ids.shape, dtype=torch.long)\n    input_ids[indices_random] = random_words[indices_random]", ""]}
{"filename": "zeronlg/utils/__init__.py", "chunked_list": ["from .eval import coco_caption_eval, translate_eval\nfrom .metriclogger import MetricLogger\nfrom .op_text import random_masking_\nfrom .op_vision import get_uniform_frame_ids, process_images\nfrom .io import get_cache_folder, get_formatted_string\n"]}
{"filename": "zeronlg/utils/eval.py", "chunked_list": ["import os\nimport re\nimport jieba\nimport string\n\nimport logging\nimport subprocess\nimport requests\nimport wget\nimport psutil", "import wget\nimport psutil\nimport time\nimport json\nimport socket\nimport glob\nimport sys\ntry:\n    from urlparse import urlparse\nexcept ImportError:\n    from urllib.parse import urlparse", "\nfrom stanfordcorenlp import StanfordCoreNLP\n\nfrom pycocotools.coco import COCO\nfrom pycocoevalcap.eval import COCOEvalCap\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.spice.spice import Spice", "from pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.spice.spice import Spice\nfrom transformers.models.bert.tokenization_bert import BasicTokenizer\nfrom typing import Dict\n\nfrom .io import get_cache_folder\n\n\nCORENLP = 'stanford-corenlp-4.5.2'\n", "CORENLP = 'stanford-corenlp-4.5.2'\n\n\nclass MyStanfordCoreNLP(StanfordCoreNLP):\n    def __init__(self, path_or_host, port=9000, memory='4g', lang='en', timeout=1500, quiet=True,\n                 logging_level=logging.WARNING, auto_download=True):\n        \n        self.port = port\n        self.memory = memory\n        self.lang = lang\n        self.timeout = timeout\n        self.quiet = quiet\n        self.logging_level = logging_level\n\n        logging.basicConfig(level=self.logging_level)\n\n        # Check args\n        self._check_args()\n\n        if path_or_host.startswith('http'):\n            self.url = path_or_host + ':' + str(port)\n            self.path_or_host = path_or_host\n            logging.info('Using an existing server {}'.format(self.url))\n        else:\n            self.path_or_host = os.path.join(path_or_host, CORENLP)\n\n            # Check Java\n            if not subprocess.call(['java', '-version'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT) == 0:\n                raise RuntimeError('Java not found.')\n\n            # Check if the dir exists\n            if not os.path.isdir(self.path_or_host):\n                if auto_download:\n                    try:\n                        logging.info(f'Downloading {CORENLP} to {self.path_or_host} ...')\n                        zip, jars = None, []\n                        for url in [\n                            f'https://nlp.stanford.edu/software/{CORENLP}.zip',\n                            f'https://nlp.stanford.edu/software/{CORENLP}-models-german.jar',\n                            f'https://nlp.stanford.edu/software/{CORENLP}-models-french.jar',\n                        ]:\n                            fn = url.split('/')[-1]\n\n                            if fn.endswith('.jar'):\n                                jars.append(fn)\n                            else:\n                                assert zip is None\n                                assert fn.endswith('.zip')\n                                zip = fn\n\n                            wget.download(url, fn)\n                        \n                        root = os.path.dirname(self.path_or_host)\n                        os.system(f'mkdir -p {root}')\n                        os.system(f'unzip {zip} -d {root}')\n                        for jar in jars:\n                            os.system(f'mv {jar} {os.path.join(root, zip.replace(\".zip\", \"\"))}')\n                        os.system(f'rm {zip}')\n                    except:\n                        raise ConnectionError('can not automatically download corenlp-4.5.2')\n                else:\n                    raise IOError(str(self.path_or_host) + ' is not a directory.')\n            directory = os.path.normpath(self.path_or_host) + os.sep\n            self.class_path_dir = directory\n\n            # Check if the language specific model file exists\n            # We rewrite the file format compaired with the implementation in 3.9.1.1\n            switcher = {\n                'en': 'stanford-corenlp-[0-9].[0-9].[0-9]-models.jar',\n                'zh': 'stanford-corenlp-[0-9].[0-9].[0-9]-models-chinese.jar',\n                'ar': 'stanford-corenlp-[0-9].[0-9].[0-9]-models-arabic.jar',\n                'fr': 'stanford-corenlp-[0-9].[0-9].[0-9]-models-french.jar',\n                'de': 'stanford-corenlp-[0-9].[0-9].[0-9]-models-german.jar',\n                'es': 'stanford-corenlp-[0-9].[0-9].[0-9]-models-spanish.jar',\n            }\n            jars = {\n                'en': 'stanford-corenlp-x.x.x-models.jar',\n                'zh': 'stanford-corenlp-x.x.x-models-chinese.jar',\n                'ar': 'stanford-corenlp-x.x.x-models-arabic.jar',\n                'fr': 'stanford-corenlp-x.x.x-models-french.jar',\n                'de': 'stanford-corenlp-x.x.x-models-german.jar',\n                'es': 'stanford-corenlp-x.x.x-models-spanish.jar',\n            }\n            if len(glob.glob(directory + switcher.get(self.lang))) <= 0:\n                raise IOError(jars.get(\n                    self.lang) + ' not exists. You should download and place it in the ' + directory + ' first.')\n\n            # We disable port checking because it will raise an error when running on Mac\n\n            # # If port not set, auto select\n            # if self.port is None:\n            #     for port_candidate in range(9000, 65535):\n            #         if port_candidate not in [conn.laddr[1] for conn in psutil.net_connections()]:\n            #             self.port = port_candidate\n            #             break\n\n            # # Check if the port is in use\n            # if self.port in [conn.laddr[1] for conn in psutil.net_connections()]:\n            #     raise IOError('Port ' + str(self.port) + ' is already in use.')\n\n            # Start native server\n            logging.info('Initializing native server...')\n            cmd = \"java\"\n            java_args = \"-Xmx{}\".format(self.memory)\n            java_class = \"edu.stanford.nlp.pipeline.StanfordCoreNLPServer\"\n            class_path = '\"{}*\"'.format(directory)\n\n            args = [cmd, java_args, '-cp', class_path, java_class, '-port', str(self.port)]\n\n            args = ' '.join(args)\n\n            logging.info(args)\n\n            # Silence\n            with open(os.devnull, 'w') as null_file:\n                out_file = None\n                if self.quiet:\n                    out_file = null_file\n\n                self.p = subprocess.Popen(args, shell=True, stdout=out_file, stderr=subprocess.STDOUT)\n                logging.info('Server shell PID: {}'.format(self.p.pid))\n\n            self.url = 'http://localhost:' + str(self.port)\n\n        # Wait until server starts\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        host_name = urlparse(self.url).hostname\n        time.sleep(1)  # OSX, not tested\n        while sock.connect_ex((host_name, self.port)):\n            logging.info('Waiting until the server is available.')\n            time.sleep(1)\n        logging.info('The server is available.')\n\n    def _request(self, annotators=None, data=None, *args, **kwargs):\n        if sys.version_info.major >= 3:\n            data = data.encode('utf-8')\n\n        properties = {'annotators': annotators, 'outputFormat': 'json'}\n        params = {'properties': str(properties), 'pipelineLanguage': self.lang}\n        if 'pattern' in kwargs:\n            params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': self.lang}\n\n        # logging.info(params)\n        r = requests.post(self.url, params=params, data=data, headers={'Connection': 'close'})\n        r_dict = json.loads(r.text)\n\n        return r_dict", "\n\n\nzh_punctuation = \"\\\u3010.*?\u3011+|\\\u300a.*?\u300b+|\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[\u2014\u2014\uff01\\\uff0c\u3002=\uff1f\u3001\uff1a\u201c\u201d\u2018\u2019\uffe5\u2026\u2026()\u300a\u300b\u3010\u3011\uff5e]\"\n\n\ndef tokenize_zh_sentence(text:str):\n    text = ' '.join(jieba.cut(text))\n    text = re.sub(zh_punctuation, '', text).strip()\n    text = re.sub(r\"\\s{2,}\", ' ', text)\n    return text", "\n\ndef tokenize_sentence(text: str, nlp: MyStanfordCoreNLP=None, punctuation=string.punctuation, do_lower_case=True):\n    if nlp is None:\n        tokenizer = BasicTokenizer(strip_accents=False, do_lower_case=do_lower_case)\n        tokens = tokenizer.tokenize(text)\n    else:\n        if do_lower_case:\n            text = text.lower()\n        tokens = [token for token in nlp.word_tokenize(text) if token not in punctuation]\n    return ' '.join(tokens)", "\n\nclass MyCOCOEvalCap(COCOEvalCap):\n    def __init__(self, coco, cocoRes, lang='zh', cache_folder=get_cache_folder()):\n        super().__init__(coco, cocoRes)\n        self.lang = lang\n        if self.lang != 'zh':\n            self.nlp = MyStanfordCoreNLP(cache_folder, lang=lang)\n\n    def evaluate(self):\n        imgIds = self.params['image_id']\n        # imgIds = self.coco.getImgIds()\n        gts = {}\n        res = {}\n        for imgId in imgIds:\n            if self.lang == 'zh':\n                gts[imgId] = [tokenize_zh_sentence(item['caption']) for item in self.coco.imgToAnns[imgId]]\n                res[imgId] = [tokenize_zh_sentence(''.join(item['caption'].split(' '))) for item in self.cocoRes.imgToAnns[imgId]]\n            else:\n                gts[imgId] = [tokenize_sentence(item['caption'], self.nlp) for item in self.coco.imgToAnns[imgId]]\n                res[imgId] = [tokenize_sentence(item['caption'], self.nlp) for item in self.cocoRes.imgToAnns[imgId]]\n\n        # =================================================\n        # Set up scorers\n        # =================================================\n        print('setting up scorers...')\n        scorers = [\n            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n            (Meteor(),\"METEOR\"),\n            (Rouge(), \"ROUGE_L\"),\n            (Cider(), \"CIDEr\"),\n            # (Spice(), \"SPICE\") \n            # SPICE will call English-related tokenizer and NER, so we don't evaluate it on other language.\n        ]\n\n        # =================================================\n        # Compute scores\n        # =================================================\n        for scorer, method in scorers:\n            print('computing %s score...'%(scorer.method()))\n            score, scores = scorer.compute_score(gts, res)\n            if type(method) == list:\n                for sc, scs, m in zip(score, scores, method):\n                    self.setEval(sc, m)\n                    self.setImgToEvalImgs(scs, gts.keys(), m)\n                    print(\"%s: %0.3f\"%(m, sc))\n            else:\n                self.setEval(score, method)\n                self.setImgToEvalImgs(scores, gts.keys(), method)\n                print(\"%s: %0.3f\"%(method, score))\n        self.setEvalImgs()", "\n\ndef coco_caption_eval(annotation_file, results_file, eval_lang='en', cache_folder=get_cache_folder()):\n    assert os.path.exists(annotation_file)\n\n    # create coco object and coco_result object\n    coco = COCO(annotation_file)\n    coco_result = coco.loadRes(results_file)\n\n    if eval_lang == 'en':\n        # we keep using the original evaluation toolkit for visual captioning in English\n        coco_eval = COCOEvalCap(coco, coco_result)\n    else:\n        coco_eval = MyCOCOEvalCap(coco, coco_result, eval_lang, cache_folder=cache_folder)\n\n    # evaluate results\n    # SPICE will take a few minutes the first time, but speeds up due to caching\n    coco_eval.evaluate()\n\n    # print output evaluation scores\n    for metric, score in coco_eval.eval.items():\n        print(f'{metric}: {score:.3f}', flush=True)\n\n    return coco_eval", "\n\ndef translate_eval(gts, res, eval_lang='en', cache_folder=get_cache_folder()) -> Dict[str, float]:\n    assert isinstance(gts, (list, tuple))\n    assert isinstance(res, (list, tuple))\n    assert len(gts) == len(res)\n\n    if eval_lang == 'zh':\n        gts = [[tokenize_zh_sentence(item).split()] for item in gts]\n        res = [tokenize_zh_sentence(''.join(item.split())).split() for item in res]\n    else:\n        nlp = MyStanfordCoreNLP(cache_folder, lang=eval_lang)\n        gts = [[tokenize_sentence(item, nlp).split()] for item in gts]\n        res = [tokenize_sentence(item, nlp).split() for item in res]\n\n    from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n    score = corpus_bleu(gts, res, smoothing_function=SmoothingFunction().method1)\n    return {\n        'BLEU': score,\n    }", ""]}
{"filename": "zeronlg/utils/io.py", "chunked_list": ["import os\nimport json\nimport torch.distributed as dist\nfrom .distributed import (\n    is_dist_avail_and_initialized, \n    is_main_process, \n    get_rank, \n    get_world_size,\n)\n", ")\n\n\ndef read_json(rpath: str):\n    result = []\n    with open(rpath, 'rt') as f:\n        for line in f:\n            result.append(json.loads(line.strip()))\n\n    return result", "\n\ndef write_json(result: list, wpath: str):\n    with open(wpath, 'wt') as f:\n        for res in result:\n            f.write(json.dumps(res) + '\\n')\n\n\ndef collect_result(result, filename, local_wdir, save_result=False, remove_duplicate='', do_not_collect=False):\n    assert isinstance(result, list)\n    write_json(result, os.path.join(local_wdir,'%s_rank%d.json' % (filename, get_rank())))\n    if is_dist_avail_and_initialized():\n        dist.barrier()\n\n    if do_not_collect:\n        return None\n\n    result = []\n    final_result_file = ''\n    if is_main_process():\n        # combine results from all processes\n        for rank in range(get_world_size()):\n            result += read_json(os.path.join(local_wdir, '%s_rank%d.json' % (filename, rank)))\n\n        if remove_duplicate:  # for evaluating captioning tasks\n            result_new = []\n            id_list = set()\n            for res in result:\n                if res[remove_duplicate] not in id_list:\n                    id_list.add(res[remove_duplicate])\n                    result_new.append(res)\n            result = result_new\n\n        if save_result:\n            final_result_file = os.path.join(local_wdir, '%s.json' % filename)\n            json.dump(result, open(final_result_file, 'w'), indent=4)\n            print('result file saved to %s' % final_result_file)\n\n    if is_dist_avail_and_initialized():\n        dist.barrier()\n\n    return final_result_file if save_result else result", "def collect_result(result, filename, local_wdir, save_result=False, remove_duplicate='', do_not_collect=False):\n    assert isinstance(result, list)\n    write_json(result, os.path.join(local_wdir,'%s_rank%d.json' % (filename, get_rank())))\n    if is_dist_avail_and_initialized():\n        dist.barrier()\n\n    if do_not_collect:\n        return None\n\n    result = []\n    final_result_file = ''\n    if is_main_process():\n        # combine results from all processes\n        for rank in range(get_world_size()):\n            result += read_json(os.path.join(local_wdir, '%s_rank%d.json' % (filename, rank)))\n\n        if remove_duplicate:  # for evaluating captioning tasks\n            result_new = []\n            id_list = set()\n            for res in result:\n                if res[remove_duplicate] not in id_list:\n                    id_list.add(res[remove_duplicate])\n                    result_new.append(res)\n            result = result_new\n\n        if save_result:\n            final_result_file = os.path.join(local_wdir, '%s.json' % filename)\n            json.dump(result, open(final_result_file, 'w'), indent=4)\n            print('result file saved to %s' % final_result_file)\n\n    if is_dist_avail_and_initialized():\n        dist.barrier()\n\n    return final_result_file if save_result else result", "\n\ndef get_cache_folder(cache_folder: str=None):\n    if cache_folder is None:\n        cache_folder = os.getenv('ZERONLG_HOME')\n        if cache_folder is None:\n            try:\n                from torch.hub import _get_torch_home\n\n                torch_cache_home = _get_torch_home()\n            except ImportError:\n                torch_cache_home = os.path.expanduser(os.getenv('TORCH_HOME', os.path.join(os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))\n\n            cache_folder = os.path.join(torch_cache_home, 'zeronlg')\n    return cache_folder", "\n\ndef get_formatted_string(kwargs, key, assigned_keys=None, assigned_kwargs=None, format_key=None) -> str:\n    if format_key is None:\n        format_key = f'{key}_format'\n\n    if kwargs.get(key, None) is None:\n        assert format_key in kwargs\n        \n        if assigned_kwargs is None:\n            assert assigned_keys is not None\n            assigned_kwargs = {k: kwargs[k] for k in assigned_keys}\n\n        return kwargs[format_key].format(**assigned_kwargs)\n\n    return kwargs[key]", ""]}
{"filename": "zeronlg/models/Transformer.py", "chunked_list": ["import os\nimport json\n\nfrom torch import nn\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig, T5Config\nfrom typing import List, Dict, Optional, Union, Tuple\nfrom sentence_transformers.util import snapshot_download\nfrom ..utils import get_cache_folder\nfrom .. import __LIBRARY_NAME__, __version__\n", "from .. import __LIBRARY_NAME__, __version__\n\n\n# Derived from sentence_transformers.models.Transformer\n# (https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/Transformer.py)\nclass Transformer(nn.Module):\n    \"\"\"Huggingface AutoModel to generate token embeddings.\n    Loads the correct class, e.g. BERT / RoBERTa etc.\n\n    :param model_name_or_path: Huggingface models name (https://huggingface.co/models)\n    :param max_seq_length: Truncate any inputs longer than max_seq_length\n    :param model_args: Arguments (key, value pairs) passed to the Huggingface Transformers model\n    :param cache_folder: Cache folder to store/load HuggingFace transformers models\n    :param tokenizer_args: Arguments (key, value pairs) passed to the Huggingface Tokenizer model\n    :param do_lower_case: If true, lowercases the input (independent if the model is cased or not)\n    :param tokenizer_name_or_path: Name or path of the tokenizer. When None, then model_name_or_path is used\n    :param use_auth_token: HuggingFace authentication token to download private models.\n    \"\"\"\n    def __init__(self, \n                 model_name_or_path: str, \n                 max_seq_length: Optional[int] = None,\n                 model_args: Dict = {}, \n                 cache_folder: Optional[str] = None,\n                 tokenizer_args: Dict = {}, \n                 do_lower_case: bool = False,\n                 tokenizer_name_or_path : str = None,\n                 use_auth_token: Union[bool, str, None] = None\n                 ):\n        \"\"\"\n        Yang B. modification: \n        1) change the parameter `cache_dir` to `cache_folder`\n        2) add a new parameter `use_auth_token`\n        3) call sentence_transformers.util.snapshot_download to download transformers\n        \"\"\"\n        super(Transformer, self).__init__()\n        self.config_keys = ['max_seq_length', 'do_lower_case']\n        self.do_lower_case = do_lower_case\n\n        cache_folder = get_cache_folder(cache_folder)\n        if os.path.exists(model_name_or_path):\n            model_path = model_name_or_path\n        else:\n            model_path = os.path.join(cache_folder, model_name_or_path.replace('/', '_'))\n        \n        if not os.path.exists(os.path.join(model_path, 'config.json')):\n            storage_path = snapshot_download(model_name_or_path,\n                            cache_dir=cache_folder,\n                            library_name=__LIBRARY_NAME__,\n                            library_version=__version__,\n                            ignore_files=['flax_model.msgpack', 'rust_model.ot', 'tf_model.h5'],\n                            use_auth_token=use_auth_token)\n            assert model_path == storage_path\n        \n        assert os.path.exists(model_path)\n\n        config = AutoConfig.from_pretrained(model_path, **model_args)\n        self._load_model(model_path, config)\n\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path or model_path, **tokenizer_args)\n\n        #No max_seq_length set. Try to infer from model\n        if max_seq_length is None:\n            if hasattr(self.auto_model, \"config\") and hasattr(self.auto_model.config, \"max_position_embeddings\") and hasattr(self.tokenizer, \"model_max_length\"):\n                max_seq_length = min(self.auto_model.config.max_position_embeddings, self.tokenizer.model_max_length)\n\n        self.max_seq_length = max_seq_length\n\n        if tokenizer_name_or_path is not None:\n            self.auto_model.config.tokenizer_class = self.tokenizer.__class__.__name__\n\n\n    def _load_model(self, model_name_or_path, config, cache_dir=None):\n        \"\"\"Loads the transformer model\"\"\"\n        if isinstance(config, T5Config):\n            self._load_t5_model(model_name_or_path, config, cache_dir)\n        else:\n            self.auto_model = AutoModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n\n    def _load_t5_model(self, model_name_or_path, config, cache_dir=None):\n        \"\"\"Loads the encoder model from T5\"\"\"\n        from transformers import T5EncoderModel\n        T5EncoderModel._keys_to_ignore_on_load_unexpected = [\"decoder.*\"]\n        self.auto_model = T5EncoderModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n\n    def __repr__(self):\n        return \"Transformer({}) with Transformer model: {} \".format(self.get_config_dict(), self.auto_model.__class__.__name__)\n\n    def forward(self, features):\n        \"\"\"Returns token_embeddings, cls_token\"\"\"\n        trans_features = {'input_ids': features['input_ids'], 'attention_mask': features['attention_mask']}\n        if 'token_type_ids' in features:\n            trans_features['token_type_ids'] = features['token_type_ids']\n\n        output_states = self.auto_model(**trans_features, return_dict=False)\n        output_tokens = output_states[0]\n\n        features.update({'token_embeddings': output_tokens, 'attention_mask': features['attention_mask']})\n\n        if self.auto_model.config.output_hidden_states:\n            all_layer_idx = 2\n            if len(output_states) < 3: #Some models only output last_hidden_states and all_hidden_states\n                all_layer_idx = 1\n\n            hidden_states = output_states[all_layer_idx]\n            features.update({'all_layer_embeddings': hidden_states})\n\n        return features\n\n    def get_word_embedding_dimension(self) -> int:\n        return self.auto_model.config.hidden_size\n\n    def tokenize(self, texts: Union[List[str], List[Dict], List[Tuple[str, str]]]):\n        \"\"\"\n        Tokenizes a text and maps tokens to token-ids\n        \"\"\"\n        output = {}\n        if isinstance(texts[0], str):\n            to_tokenize = [texts]\n        elif isinstance(texts[0], dict):\n            to_tokenize = []\n            output['text_keys'] = []\n            for lookup in texts:\n                text_key, text = next(iter(lookup.items()))\n                to_tokenize.append(text)\n                output['text_keys'].append(text_key)\n            to_tokenize = [to_tokenize]\n        else:\n            batch1, batch2 = [], []\n            for text_tuple in texts:\n                batch1.append(text_tuple[0])\n                batch2.append(text_tuple[1])\n            to_tokenize = [batch1, batch2]\n\n        #strip\n        to_tokenize = [[str(s).strip() for s in col] for col in to_tokenize]\n\n        #Lowercase\n        if self.do_lower_case:\n            to_tokenize = [[s.lower() for s in col] for col in to_tokenize]\n\n        output.update(self.tokenizer(*to_tokenize, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_seq_length))\n        return output\n\n\n    def get_config_dict(self):\n        return {key: self.__dict__[key] for key in self.config_keys}\n\n    def save(self, output_path: str):\n        self.auto_model.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n\n        with open(os.path.join(output_path, 'sentence_bert_config.json'), 'w') as fOut:\n            json.dump(self.get_config_dict(), fOut, indent=2)\n\n    @staticmethod\n    def load(input_path: str):\n        #Old classes used other config names than 'sentence_bert_config.json'\n        for config_name in ['sentence_bert_config.json', 'sentence_roberta_config.json', 'sentence_distilbert_config.json', 'sentence_camembert_config.json', 'sentence_albert_config.json', 'sentence_xlm-roberta_config.json', 'sentence_xlnet_config.json']:\n            sbert_config_path = os.path.join(input_path, config_name)\n            if os.path.exists(sbert_config_path):\n                break\n\n        with open(sbert_config_path) as fIn:\n            config = json.load(fIn)\n        return Transformer(model_name_or_path=input_path, **config)", ""]}
{"filename": "zeronlg/models/Dense.py", "chunked_list": ["import torch\nfrom torch import Tensor\nfrom torch import nn\nfrom typing import Dict\nimport os\nimport json\nfrom sentence_transformers.util import fullname, import_from_string\n\n\n# Derived from sentence_transformers.models.Dense", "\n# Derived from sentence_transformers.models.Dense\n# (https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/Dense.py)\nclass Dense(nn.Module):\n    \"\"\"Feed-forward function with  activiation function.\n\n    This layer takes a fixed-sized sentence embedding and passes it through a feed-forward layer. Can be used to generate deep averaging networks (DAN).\n\n    :param in_features: Size of the input dimension\n    :param out_features: Output size\n    :param bias: Add a bias vector\n    :param activation_function: Pytorch activation function applied on output\n    :param init_weight: Initial value for the matrix of the linear layer\n    :param init_bias: Initial value for the bias of the linear layer\n    :param proj_token_embs: If True, project token embeddings in addition to the sentence embedding\n    \"\"\"\n    def __init__(self, \n                 in_features: int, \n                 out_features: int, \n                 bias: bool = True, \n                 activation_function=nn.Tanh(), \n                 init_weight: Tensor = None, \n                 init_bias: Tensor = None,\n                 proj_token_embs: bool = False,\n                 **kwargs,\n                ):\n        \"\"\"\n        Yang B. modification:\n        1) add a new parameter `proj_token_embs`, which allow projecting token embeddings if speficied\n        2) save `proj_token_embs` to config_dict\n        \"\"\"\n        super(Dense, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bias = bias\n        self.activation_function = activation_function\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n\n        if init_weight is not None:\n            self.linear.weight = nn.Parameter(init_weight)\n\n        if init_bias is not None:\n            self.linear.bias = nn.Parameter(init_bias)\n        \n        self.proj_token_embs = proj_token_embs\n\n    def forward(self, features: Dict[str, Tensor]):\n        features.update({'sentence_embedding': self.activation_function(self.linear(features['sentence_embedding']))})\n        if self.proj_token_embs:\n            features.update({'token_embeddings': self.activation_function(self.linear(features['token_embeddings']))})\n        return features\n\n    def get_sentence_embedding_dimension(self) -> int:\n        return self.out_features\n\n    def save(self, output_path):\n        with open(os.path.join(output_path, 'config.json'), 'w') as fOut:\n            json.dump(self.get_config_dict(), fOut)\n\n        torch.save(self.state_dict(), os.path.join(output_path, 'pytorch_model.bin'))\n\n    def get_config_dict(self):\n        return {'in_features': self.in_features, 'out_features': self.out_features, 'bias': self.bias, 'activation_function': fullname(self.activation_function), 'proj_token_embs': self.proj_token_embs}\n    \n    def __repr__(self):\n        return \"Dense({})\".format(self.get_config_dict())\n\n    @staticmethod\n    def load(input_path):\n        with open(os.path.join(input_path, 'config.json')) as fIn:\n            config = json.load(fIn)\n\n        config['activation_function'] = import_from_string(config['activation_function'])()\n        model = Dense(**config)\n        model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n        return model", ""]}
{"filename": "zeronlg/models/Decoder.py", "chunked_list": ["import os\nimport json\nimport torch\nimport transformers\nfrom torch import nn\nfrom typing import List, Dict, Optional, Union\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\nfrom sentence_transformers.util import snapshot_download\nfrom ..utils import get_cache_folder\nfrom .. import __LIBRARY_NAME__, __version__", "from ..utils import get_cache_folder\nfrom .. import __LIBRARY_NAME__, __version__\n\n\n# map a language to a specific token, \n# which will be used as the begin-of-sentence (BOS) token to guide the decoder\nLANG2TOKEN = {\n    'en': '[unused1]',\n    'zh': '[unused2]',\n    'de': '[unused3]',", "    'zh': '[unused2]',\n    'de': '[unused3]',\n    'fr': '[unused4]',\n} # Note: do not modify the existing mappings in LANG2TOKEN, you can instead add new ones\n\n\nclass Decoder(nn.Module):\n    \"\"\"Huggingface AutoModelForCausalLM for decoding.\n    Loads the correct class, e.g. BERT / RoBERTa etc.\n\n    :param model_name_or_path: Huggingface models name (https://huggingface.co/models)\n    :param model_args: Arguments (key, value pairs) passed to the Huggingface Transformers model\n    :param max_seq_length: Truncate any inputs longer than max_seq_length\n    :param cache_dir: Cache dir for Huggingface Transformers to store/load models\n    :param tokenizer_args: Arguments (key, value pairs) passed to the Huggingface Tokenizer model\n    :param do_lower_case: If true, lowercases the input (independent if the model is cased or not)\n    :param tokenizer_name_or_path: Name or path of the tokenizer. When None, then model_name_or_path is used\n    :param from_pretrained: If true, load pre-trained weights (deafult to true)\n    :param attend_to: A list of string(s) to specify which encoder(s) will the decoder attend to,\n                      e.g., ['student'] means taking the newly-trained encoder's outputs as the cross-attention inputs;\n                            ['teacher'] means taking the (frozen) pre-trained encoder's outputs as the cross-attention inputs;\n                            ['student', 'teacher'] ...\n    :param teacher_model_name: The name of the teacher model, which will be stored into the module config and used for re-loading\n    \"\"\"\n    def __init__(self, \n                 model_name_or_path: str, \n                 model_args: Dict = {}, \n                 max_seq_length: Optional[int] = None,\n                 cache_folder: Optional[str] = None,\n                 tokenizer_args: Dict = {}, \n                 do_lower_case: bool = False,\n                 tokenizer_name_or_path : str = None,\n                 from_pretrained: bool = True,\n                 use_auth_token: Union[bool, str, None] = None,\n                 attend_to: List[str] = ['student'],\n                 teacher_model_name: str = None,\n                 use_clip_tokens: Optional[bool] = None,\n                 ):\n        super().__init__()\n        self.config_keys = ['max_seq_length', 'do_lower_case', 'attend_to', 'teacher_model_name', 'use_clip_tokens']\n        self.do_lower_case = do_lower_case\n        self.teacher_model_name = teacher_model_name\n        self.use_clip_tokens = bool(use_clip_tokens or False)\n\n        assert isinstance(attend_to, (list, tuple))\n        self.attend_to = list(set(attend_to))\n\n        cache_folder = get_cache_folder(cache_folder)\n        if os.path.exists(model_name_or_path):\n            model_path = model_name_or_path\n        else:\n            model_path = os.path.join(cache_folder, model_name_or_path.replace('/', '_'))\n        \n        if not os.path.exists(os.path.join(model_path, 'config.json')):\n            storage_path = snapshot_download(model_name_or_path,\n                            cache_dir=cache_folder,\n                            library_name=__LIBRARY_NAME__,\n                            library_version=__version__,\n                            ignore_files=['flax_model.msgpack', 'rust_model.ot', 'tf_model.h5'],\n                            use_auth_token=use_auth_token)\n            assert model_path == storage_path\n        \n        assert os.path.exists(model_path)\n\n        config = AutoConfig.from_pretrained(model_path, **model_args)\n        assert config.is_decoder is True\n        assert config.add_cross_attention is True\n\n        self._load_model(model_path, config, from_pretrained)\n        self.auto_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation\n\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path or model_path, **tokenizer_args)\n        self.vocab = self.tokenizer.get_vocab()\n        \n        self.bos_token_id = self.tokenizer.bos_token_id or self.tokenizer.cls_token_id\n        self.eos_token_id = self.tokenizer.eos_token_id or self.tokenizer.sep_token_id\n        self.pad_token_id = self.tokenizer.pad_token_id\n\n        # No max_seq_length set. Try to infer from model\n        if max_seq_length is None:\n            if hasattr(self.auto_model, \"config\") and hasattr(self.auto_model.config, \"max_position_embeddings\") and hasattr(self.tokenizer, \"model_max_length\"):\n                max_seq_length = min(self.auto_model.config.max_position_embeddings, self.tokenizer.model_max_length)\n\n        self.max_seq_length = max_seq_length\n\n        if tokenizer_name_or_path is not None:\n            self.auto_model.config.tokenizer_class = self.tokenizer.__class__.__name__\n\n        # determine the behavior during generation based on the version of transformers\n        self.should_repeat = True\n        version = [int(item) for item in transformers.__version__.split('.')]\n        if version[0] > 4 or (version[0] == 4 and version[1] >=27):\n            # after 4.27.0, we should not repeat encoder's outputs by `num_beams` * `num_return_sequences` times\n            self.should_repeat = False\n\n    def _load_model(self, model_name_or_path, config, from_pretrained):\n        \"\"\"Loads the transformer model\"\"\"\n        if from_pretrained:\n            self.auto_model = AutoModelForCausalLM.from_pretrained(model_name_or_path, config=config)\n        else:\n            self.auto_model = AutoModelForCausalLM.from_config(config)\n\n    def __repr__(self):\n        return \"Decoder({}) with Transformer model: {} \".format(self.get_config_dict(), self.auto_model.__class__.__name__)\n\n    def get_encoder_attention_mask(self, encoder_hidden_states, features, attend_to, n_repeats=1):\n        encoder_attention_mask = None\n        if encoder_hidden_states.size(1) != 1 and attend_to == 'student':\n            encoder_attention_mask = features['attention_mask']\n        \n        if encoder_attention_mask is not None:\n            encoder_attention_mask = encoder_attention_mask.repeat_interleave(n_repeats, dim=0)\n        \n        return encoder_attention_mask\n\n    def prepare_encoder_hidden_states(self, states):\n        B = states.size(0)\n        D = states.size(-1)\n        if states.ndim == 2:\n            states = states.unsqueeze(1)\n        if states.ndim == 4:\n            states = states.view(B, -1, D)\n        return states\n\n    def forward(self, features):\n        \"\"\"Returns losses if self.training is True, otherwise generation results\"\"\"\n\n        attend_to = features.get('attend_to', None) or self.attend_to\n\n        if not isinstance(attend_to, (list, tuple)):\n            attend_to = [attend_to]\n\n        if self.training:\n            inputs = {\n                'input_ids': features['decoder_input_ids'], \n                'attention_mask': features.get('decoder_attention_mask', None),\n                'token_type_ids': features.get('decoder_token_type_ids', None),\n            }\n            inputs['labels'] = inputs['input_ids'].masked_fill(inputs['input_ids'] == self.tokenizer.pad_token_id, -100)\n\n            for at in attend_to:\n                assert at in ['teacher', 'student'], f\"`attend_to` == {at} is not supported during training yet\"\n\n                # if attending to the teacher's hidden states, it denotes a English -> English/Chinese/German/French process\n                # if attending to the student's hidden states, it denotes a auto-encoding process\n                source = features[f'{at}_hidden_states']\n                \n                inputs['encoder_hidden_states'] = self.prepare_encoder_hidden_states(source)\n                inputs['encoder_attention_mask'] = self.get_encoder_attention_mask(source, features, at)\n                \n                output_states = self.auto_model(**inputs, return_dict=False)\n                loss = output_states[0]\n\n                features.update({f'loss_at_{at}': loss})\n        else:\n            # if we do not filter those unused keys, there will be an error for transformers==4.27.1\n            ignore_keys = ['sentence_embedding', 'source_embedding', 'attend_to', 'decoder_input_ids', 'student_hidden_states', 'teacher_hidden_states', 'token_embeddings', 'num_frames']\n            generate_kwargs = {k: v for k, v in features.items() if k not in ignore_keys}\n\n            for at in attend_to:\n                assert at in ['student', 'teacher', 'both']\n                if at == 'both':\n                    # multimodal machine translation\n                    encoder_hidden_states = torch.cat([features['teacher_hidden_states'], features['student_hidden_states']], dim=1)\n                else:\n                    # visual captioning (at == 'teacher') or machine translation (at == 'student')\n                    encoder_hidden_states = features[f'{at}_hidden_states']\n                encoder_hidden_states = self.prepare_encoder_hidden_states(encoder_hidden_states)\n\n                if self.should_repeat:\n                    n_repeats = generate_kwargs['num_beams']\n                    if generate_kwargs.get('do_sample', False):\n                        n_repeats *= generate_kwargs.get('num_return_sequences', 1)\n                    encoder_hidden_states = encoder_hidden_states.repeat_interleave(n_repeats, dim=0)\n                else:\n                    n_repeats = 1\n                encoder_attention_mask = self.get_encoder_attention_mask(encoder_hidden_states, features, at, n_repeats=n_repeats)\n\n                inputs = {\n                    'input_ids': features['decoder_input_ids'], \n                    'encoder_hidden_states': encoder_hidden_states,\n                    'attention_mask': None,\n                    'encoder_attention_mask': encoder_attention_mask,\n                    'eos_token_id': self.eos_token_id,\n                    'pad_token_id': self.pad_token_id,\n                }\n                generate_kwargs.update(inputs)\n\n                outputs = self.auto_model.generate(**generate_kwargs)\n                features[f'results_at_{at}'] =  self._get_captions(outputs)\n\n        return features\n\n    def get_word_embedding_dimension(self) -> int:\n        return self.auto_model.config.hidden_size\n\n    def tokenize(self, texts: List[str], langs: Optional[List[str]]=None):\n        \"\"\"Tokenizes texts and maps tokens to token-ids\"\"\"\n        to_tokenize = texts\n        #strip\n        to_tokenize = [str(text).strip() for text in to_tokenize]\n\n        #Lowercase\n        if self.do_lower_case:\n            to_tokenize = [text.lower() for text in to_tokenize]\n\n        outputs = self.tokenizer(to_tokenize, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_seq_length)\n\n        for input_ids in outputs['input_ids']:\n            assert self.bos_token_id in input_ids\n            assert self.eos_token_id in input_ids\n\n        if langs:\n            assert len(outputs['input_ids']) == len(langs)\n            for input_ids, lang in zip(outputs['input_ids'], langs):\n                assert lang in LANG2TOKEN, f\"{lang} not in LANG2TOKEN {LANG2TOKEN.keys()}\"\n                \n                lang_token_id = self.vocab.get(LANG2TOKEN[lang], None)\n                if not lang_token_id:\n                    raise NotImplementedError(f'The special token of {lang}, i.e., {LANG2TOKEN[lang]}, is not found in the vocab; You may call tokenizer.add_tokens')\n                \n                index_of_bos_token_id = input_ids.numpy().tolist().index(self.bos_token_id)\n                # override the first bos token with lang token\n                input_ids[index_of_bos_token_id] = lang_token_id\n                \n        return {f'decoder_{k}': v for k, v in outputs.items()}\n    \n    def get_bos_input_ids(self, batch_size: int, lang: Optional[str]=None):\n        bos = self.bos_token_id if lang is None else self.vocab[LANG2TOKEN[lang]]\n        return torch.LongTensor([[bos] for _ in range(batch_size)])\n\n    def get_config_dict(self):\n        return {key: self.__dict__[key] for key in self.config_keys}\n\n    def save(self, output_path: str):\n        self.auto_model.save_pretrained(output_path)\n        self.tokenizer.save_pretrained(output_path)\n\n        with open(os.path.join(output_path, 'decoder_config.json'), 'w') as fOut:\n            json.dump(self.get_config_dict(), fOut, indent=2)\n\n    @staticmethod\n    def load(input_path: str):\n        sbert_config_path = os.path.join(input_path, 'decoder_config.json')\n\n        with open(sbert_config_path) as fIn:\n            config = json.load(fIn)\n        return Decoder(model_name_or_path=input_path, **config)\n    \n    def _get_captions(self, caption_ids):\n        captions = []\n        for i, output in enumerate(caption_ids):\n            # skip the bos token, which can be not a special token, e.g., [unused1]\n            caption = self.tokenizer.decode(output[1:], skip_special_tokens=True)\n            captions.append(caption)\n        return captions\n\n    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):\n        input_shape = input_ids.shape\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_shape)\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        assert kwargs.get('encoder_hidden_states', None) is not None\n        \n        return {\n            \"input_ids\": input_ids, \n            \"attention_mask\": attention_mask, \n            \"past_key_values\": past, \n            'encoder_hidden_states': kwargs['encoder_hidden_states'],\n            'encoder_attention_mask': kwargs['encoder_attention_mask'],\n        }", ""]}
{"filename": "zeronlg/models/__init__.py", "chunked_list": ["from .CLIPModel import CLIPModel\nfrom .Dense import Dense\nfrom .Projector import Projector\nfrom .Decoder import Decoder\nfrom .Transformer import Transformer"]}
{"filename": "zeronlg/models/CLIPModel.py", "chunked_list": ["import os\nimport json\nimport transformers\nimport torch\nfrom torch import nn\nfrom PIL import Image\n\n\n# Adapted from sentence_transformers.models.CLIPModel\n# (https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/CLIPModel.py)\nclass CLIPModel(nn.Module):\n    def __init__(self,  model_name: str = \"openai/clip-vit-base-patch32\", processor_name = None, use_clip_tokens: bool = False):\n        \"\"\"\n        Yang B. modification: \n        1) add truncation=True and max_length=77 in CLIPModel.tokenize to avoid bugs\n        2) add function: get_word_embedding_dimension and get_sentence_embedding_dimension\n        3) add an extra argument `use_clip_tokens` to return token-level embeddings in addition to the global-level vector\n        4) save configs for re-loading\n        \"\"\"\n        super(CLIPModel, self).__init__()\n        self.config_keys = ['use_clip_tokens']\n\n        if processor_name is None:\n            processor_name = model_name\n\n        self.model = transformers.CLIPModel.from_pretrained(model_name)\n        self.processor = transformers.CLIPProcessor.from_pretrained(processor_name)\n        self.use_clip_tokens = use_clip_tokens\n\n    def __repr__(self):\n        return \"CLIPModel({})\".format(self.get_config_dict())\n\n    def forward(self, features):\n        image_embeds = []\n        text_embeds = []\n\n        if 'pixel_values' in features:\n            vision_outputs = self.model.vision_model(pixel_values=features['pixel_values'], return_dict=False)\n            last_hidden_state, pooled_output, *_ = vision_outputs\n            image_embeds = self.model.visual_projection(pooled_output)\n            if self.use_clip_tokens:\n                image_token_embeds = self.model.visual_projection(last_hidden_state)\n                \n        if 'input_ids' in features:\n            text_outputs = self.model.text_model(\n                input_ids=features.get('input_ids'),\n                attention_mask=features.get('attention_mask', None),\n                position_ids=features.get('position_ids', None),\n                output_attentions=features.get('output_attentions', None),\n                output_hidden_states=features.get('output_hidden_states', None),\n                return_dict=False,\n            )\n            last_hidden_state, pooled_output, *_ = text_outputs\n            text_embeds = self.model.text_projection(pooled_output)\n            if self.use_clip_tokens:\n                text_token_embeds = self.model.text_projection(last_hidden_state)\n\n        sentence_embedding = []\n        image_features = iter(image_embeds)\n        text_features = iter(text_embeds)\n\n        for idx, input_type in enumerate(features['image_text_info']):\n            if input_type == 0:\n                sentence_embedding.append(next(image_features))\n            else:\n                sentence_embedding.append(next(text_features))\n\n        features['sentence_embedding'] = torch.stack(sentence_embedding).float()\n\n        if self.use_clip_tokens:\n            prev_input_type = None\n            for input_type in features['image_text_info']:\n                if prev_input_type is None:\n                    prev_input_type = input_type\n                else:\n                    assert prev_input_type == input_type\n            \n            # (batch_size, num_tokens, D)\n            if prev_input_type == 0:\n                features['token_embeddings'] = image_token_embeds\n                features['attention_mask'] = torch.ones(*image_token_embeds.shape[:2])\n            else:\n                features['token_embeddings'] = text_token_embeds\n\n        return features\n\n    def tokenize(self, texts):\n        images = []\n        texts_values = []\n        image_text_info = []\n\n        for idx, data in enumerate(texts):\n            if isinstance(data, Image.Image):  # An Image\n                images.append(data)\n                image_text_info.append(0)\n            else:  # A text\n                texts_values.append(data)\n                image_text_info.append(1)\n\n        if len(texts_values) == 0:\n            texts_values = None\n        if len(images) == 0:\n            images = None\n\n        inputs = self.processor(text=texts_values, images=images, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n        inputs['image_text_info'] = image_text_info\n        return inputs\n    \n    def get_word_embedding_dimension(self) -> int:\n        return self.model.text_embed_dim\n    \n    def get_sentence_embedding_dimension(self) -> int:\n        return self.model.projection_dim\n    \n    def get_config_dict(self):\n        return {key: self.__dict__[key] for key in self.config_keys}\n\n    def save(self, output_path: str):\n        self.model.save_pretrained(output_path)\n        self.processor.save_pretrained(output_path)\n\n        with open(os.path.join(output_path, 'clip_config.json'), 'w') as fOut:\n            json.dump(self.get_config_dict(), fOut, indent=2)\n\n    @staticmethod\n    def load(input_path: str):\n        config = {}\n        config_path = os.path.join(input_path, 'clip_config.json')\n        if os.path.exists(config_path):\n            config = json.load(open(config_path))\n\n        return CLIPModel(model_name=input_path, **config)", "# Adapted from sentence_transformers.models.CLIPModel\n# (https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/CLIPModel.py)\nclass CLIPModel(nn.Module):\n    def __init__(self,  model_name: str = \"openai/clip-vit-base-patch32\", processor_name = None, use_clip_tokens: bool = False):\n        \"\"\"\n        Yang B. modification: \n        1) add truncation=True and max_length=77 in CLIPModel.tokenize to avoid bugs\n        2) add function: get_word_embedding_dimension and get_sentence_embedding_dimension\n        3) add an extra argument `use_clip_tokens` to return token-level embeddings in addition to the global-level vector\n        4) save configs for re-loading\n        \"\"\"\n        super(CLIPModel, self).__init__()\n        self.config_keys = ['use_clip_tokens']\n\n        if processor_name is None:\n            processor_name = model_name\n\n        self.model = transformers.CLIPModel.from_pretrained(model_name)\n        self.processor = transformers.CLIPProcessor.from_pretrained(processor_name)\n        self.use_clip_tokens = use_clip_tokens\n\n    def __repr__(self):\n        return \"CLIPModel({})\".format(self.get_config_dict())\n\n    def forward(self, features):\n        image_embeds = []\n        text_embeds = []\n\n        if 'pixel_values' in features:\n            vision_outputs = self.model.vision_model(pixel_values=features['pixel_values'], return_dict=False)\n            last_hidden_state, pooled_output, *_ = vision_outputs\n            image_embeds = self.model.visual_projection(pooled_output)\n            if self.use_clip_tokens:\n                image_token_embeds = self.model.visual_projection(last_hidden_state)\n                \n        if 'input_ids' in features:\n            text_outputs = self.model.text_model(\n                input_ids=features.get('input_ids'),\n                attention_mask=features.get('attention_mask', None),\n                position_ids=features.get('position_ids', None),\n                output_attentions=features.get('output_attentions', None),\n                output_hidden_states=features.get('output_hidden_states', None),\n                return_dict=False,\n            )\n            last_hidden_state, pooled_output, *_ = text_outputs\n            text_embeds = self.model.text_projection(pooled_output)\n            if self.use_clip_tokens:\n                text_token_embeds = self.model.text_projection(last_hidden_state)\n\n        sentence_embedding = []\n        image_features = iter(image_embeds)\n        text_features = iter(text_embeds)\n\n        for idx, input_type in enumerate(features['image_text_info']):\n            if input_type == 0:\n                sentence_embedding.append(next(image_features))\n            else:\n                sentence_embedding.append(next(text_features))\n\n        features['sentence_embedding'] = torch.stack(sentence_embedding).float()\n\n        if self.use_clip_tokens:\n            prev_input_type = None\n            for input_type in features['image_text_info']:\n                if prev_input_type is None:\n                    prev_input_type = input_type\n                else:\n                    assert prev_input_type == input_type\n            \n            # (batch_size, num_tokens, D)\n            if prev_input_type == 0:\n                features['token_embeddings'] = image_token_embeds\n                features['attention_mask'] = torch.ones(*image_token_embeds.shape[:2])\n            else:\n                features['token_embeddings'] = text_token_embeds\n\n        return features\n\n    def tokenize(self, texts):\n        images = []\n        texts_values = []\n        image_text_info = []\n\n        for idx, data in enumerate(texts):\n            if isinstance(data, Image.Image):  # An Image\n                images.append(data)\n                image_text_info.append(0)\n            else:  # A text\n                texts_values.append(data)\n                image_text_info.append(1)\n\n        if len(texts_values) == 0:\n            texts_values = None\n        if len(images) == 0:\n            images = None\n\n        inputs = self.processor(text=texts_values, images=images, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n        inputs['image_text_info'] = image_text_info\n        return inputs\n    \n    def get_word_embedding_dimension(self) -> int:\n        return self.model.text_embed_dim\n    \n    def get_sentence_embedding_dimension(self) -> int:\n        return self.model.projection_dim\n    \n    def get_config_dict(self):\n        return {key: self.__dict__[key] for key in self.config_keys}\n\n    def save(self, output_path: str):\n        self.model.save_pretrained(output_path)\n        self.processor.save_pretrained(output_path)\n\n        with open(os.path.join(output_path, 'clip_config.json'), 'w') as fOut:\n            json.dump(self.get_config_dict(), fOut, indent=2)\n\n    @staticmethod\n    def load(input_path: str):\n        config = {}\n        config_path = os.path.join(input_path, 'clip_config.json')\n        if os.path.exists(config_path):\n            config = json.load(open(config_path))\n\n        return CLIPModel(model_name=input_path, **config)", ""]}
{"filename": "zeronlg/models/Projector.py", "chunked_list": ["import os\nimport torch\nimport json\nimport random\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\nfrom typing import Dict\n\n\nclass Projector(nn.Module):\n    \"\"\"\n    This layer takes fixed-sized embedding(s) named \n    'sentence_embedding' (from the student model) and/or 'source_embedding' (from the teacher model) \n    as inputs, and outputs 'student_hidden_states' and/or 'teacher_hidden_states' respectively.\n\n    Pipeline:\n    1) applying L2 normalization, \n    2) (optional) adding gaussian noises and again applying L2 normalization,\n    3) applying the feed-forward process in the order of `Linear-Dropout-LayerNorm` \n\n    :param in_features: Size of the input dimension\n    :param out_features: Output size\n    :param bias: Add a bias vector\n    :param dropout: Probability of dropout (default to 0.1)\n    :param noise_std: Standard deviation of the gaussian noise (defaut to 0)\n    :param noise_prob: Probability to add gaussian noise (default to 0, which is equivalent to 1)\n    :param student_emb_keyname: Features of the specific key to be mapped to `student_hidden_states`\n    :param teacher_emb_keyname: Features of the specific key to be mapped to `teacher_hidden_states`\n    \"\"\"\n    def __init__(self, \n                 in_features: int, \n                 out_features: int, bias: bool = True, \n                 dropout: float = 0.1, \n                 noise_std: float = 0.0, \n                 noise_prob: float = 0.0, \n                 student_emb_keyname: str = 'sentence_embedding',\n                 teacher_emb_keyname: str = 'source_embedding',\n                 **kwargs\n                 ):\n        super(Projector, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bias = bias\n        self.noise_std = noise_std\n        self.noise_prob = noise_prob\n        self.dropout = dropout\n\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n        self.drop = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(out_features)\n\n        self.student_emb_keyname = student_emb_keyname\n        self.teacher_emb_keyname = teacher_emb_keyname\n\n    def forward(self, features: Dict[str, Tensor]):\n        for src_name, trg_name in zip(\n                [self.student_emb_keyname, self.teacher_emb_keyname], \n                ['student_hidden_states', 'teacher_hidden_states']\n            ):\n            if src_name in features and features[src_name] is not None:\n                # 1): L2 normalization\n                feats = F.normalize(features[src_name], dim=-1)\n                \n                # 2): Gaussian noise & L2 normalization\n                if self.noise_std > 0 and self.training:\n                    if self.noise_prob == 0 or (random.random() < self.noise_prob):\n                        feats = feats + (torch.randn(feats.shape).to(feats.device) * self.noise_std)\n                        feats = F.normalize(feats, dim=-1)\n                \n                # 3): Feed forward in the order of `Linear-Dropout-LayerNorm`\n                feats = self.norm(self.drop(self.linear(feats)))\n\n                if feats.dim() == 2:\n                    feats = feats.unsqueeze(1) # (batch_size, 1, out_features)\n            \n                features[trg_name] = feats\n        \n        return features\n\n    def get_config_dict(self):\n        return {\n            'in_features': self.in_features, \n            'out_features': self.out_features, \n            'bias': self.bias, \n            'noise_std': self.noise_std, \n            'dropout': self.dropout, \n            'noise_prob': self.noise_prob,\n            'student_emb_keyname': self.student_emb_keyname,\n            'teacher_emb_keyname': self.teacher_emb_keyname,\n        }\n\n    def save(self, output_path):\n        with open(os.path.join(output_path, 'config.json'), 'w') as fOut:\n            json.dump(self.get_config_dict(), fOut)\n\n        torch.save(self.state_dict(), os.path.join(output_path, 'pytorch_model.bin'))\n\n    def __repr__(self):\n        return \"Projector({})\".format(self.get_config_dict())\n\n    @staticmethod\n    def load(input_path):\n        with open(os.path.join(input_path, 'config.json')) as fIn:\n            config = json.load(fIn)\n\n        model = Projector(**config)\n        model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n        return model", "\nclass Projector(nn.Module):\n    \"\"\"\n    This layer takes fixed-sized embedding(s) named \n    'sentence_embedding' (from the student model) and/or 'source_embedding' (from the teacher model) \n    as inputs, and outputs 'student_hidden_states' and/or 'teacher_hidden_states' respectively.\n\n    Pipeline:\n    1) applying L2 normalization, \n    2) (optional) adding gaussian noises and again applying L2 normalization,\n    3) applying the feed-forward process in the order of `Linear-Dropout-LayerNorm` \n\n    :param in_features: Size of the input dimension\n    :param out_features: Output size\n    :param bias: Add a bias vector\n    :param dropout: Probability of dropout (default to 0.1)\n    :param noise_std: Standard deviation of the gaussian noise (defaut to 0)\n    :param noise_prob: Probability to add gaussian noise (default to 0, which is equivalent to 1)\n    :param student_emb_keyname: Features of the specific key to be mapped to `student_hidden_states`\n    :param teacher_emb_keyname: Features of the specific key to be mapped to `teacher_hidden_states`\n    \"\"\"\n    def __init__(self, \n                 in_features: int, \n                 out_features: int, bias: bool = True, \n                 dropout: float = 0.1, \n                 noise_std: float = 0.0, \n                 noise_prob: float = 0.0, \n                 student_emb_keyname: str = 'sentence_embedding',\n                 teacher_emb_keyname: str = 'source_embedding',\n                 **kwargs\n                 ):\n        super(Projector, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bias = bias\n        self.noise_std = noise_std\n        self.noise_prob = noise_prob\n        self.dropout = dropout\n\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n        self.drop = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(out_features)\n\n        self.student_emb_keyname = student_emb_keyname\n        self.teacher_emb_keyname = teacher_emb_keyname\n\n    def forward(self, features: Dict[str, Tensor]):\n        for src_name, trg_name in zip(\n                [self.student_emb_keyname, self.teacher_emb_keyname], \n                ['student_hidden_states', 'teacher_hidden_states']\n            ):\n            if src_name in features and features[src_name] is not None:\n                # 1): L2 normalization\n                feats = F.normalize(features[src_name], dim=-1)\n                \n                # 2): Gaussian noise & L2 normalization\n                if self.noise_std > 0 and self.training:\n                    if self.noise_prob == 0 or (random.random() < self.noise_prob):\n                        feats = feats + (torch.randn(feats.shape).to(feats.device) * self.noise_std)\n                        feats = F.normalize(feats, dim=-1)\n                \n                # 3): Feed forward in the order of `Linear-Dropout-LayerNorm`\n                feats = self.norm(self.drop(self.linear(feats)))\n\n                if feats.dim() == 2:\n                    feats = feats.unsqueeze(1) # (batch_size, 1, out_features)\n            \n                features[trg_name] = feats\n        \n        return features\n\n    def get_config_dict(self):\n        return {\n            'in_features': self.in_features, \n            'out_features': self.out_features, \n            'bias': self.bias, \n            'noise_std': self.noise_std, \n            'dropout': self.dropout, \n            'noise_prob': self.noise_prob,\n            'student_emb_keyname': self.student_emb_keyname,\n            'teacher_emb_keyname': self.teacher_emb_keyname,\n        }\n\n    def save(self, output_path):\n        with open(os.path.join(output_path, 'config.json'), 'w') as fOut:\n            json.dump(self.get_config_dict(), fOut)\n\n        torch.save(self.state_dict(), os.path.join(output_path, 'pytorch_model.bin'))\n\n    def __repr__(self):\n        return \"Projector({})\".format(self.get_config_dict())\n\n    @staticmethod\n    def load(input_path):\n        with open(os.path.join(input_path, 'config.json')) as fIn:\n            config = json.load(fIn)\n\n        model = Projector(**config)\n        model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n        return model", ""]}
{"filename": "zeronlg/datasets/__init__.py", "chunked_list": ["from .PretrainDataset import PretrainDataset\nfrom .CaptionDataset import CaptionDataset, CaptionDatasetForRetrieval\nfrom .TranslateDataset import TranslateDataset"]}
{"filename": "zeronlg/datasets/TranslateDataset.py", "chunked_list": ["import logging\nfrom sentence_transformers import LoggingHandler\nfrom torch.utils.data import Dataset\nfrom typing import Optional, List\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])", "                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nglobal_logger = logging.getLogger(__name__)\n\n\nclass TranslateDataset(Dataset):\n    def __init__(self, \n                 source_language: str, \n                 target_language: str,\n                 source_path: Optional[str] = None,\n                 target_path: Optional[str] = None,\n                 source_sentences: Optional[List[str]] = None, \n                 target_sentences: Optional[List[str]] = None,\n                 logger: Optional[logging.Logger] = None\n                 ) -> None:\n        \n        self.logger = logger or global_logger\n        \n        assert source_path is not None or source_sentences is not None\n        assert target_path is not None or target_sentences is not None\n\n        if source_sentences is None:\n            self.log(f'Loading source sentences ({source_language}) from {source_path}')\n            source_sentences = open(source_path, 'r', encoding='utf8').read().strip().split('\\n')\n        \n        if target_sentences is None:\n            self.log(f'Loading target sentences ({target_language}) from {target_path}')\n            target_sentences = open(target_path, 'r', encoding='utf8').read().strip().split('\\n')\n        \n        assert len(source_sentences) == len(target_sentences), \\\n            f\"#source sents: {len(source_sentences)}; #target sents: {target_sentences}\"\n        \n        self.source_language = source_language\n        self.source_sentences = source_sentences\n        self.target_language = target_language\n        self.target_sentences = target_sentences\n    \n    def log(self, msg):\n        self.logger.info(msg)\n    \n    def __len__(self):\n        return len(self.source_sentences)\n    \n    def __getitem__(self, index):\n        return self.source_sentences[index], self.target_sentences[index]", ""]}
{"filename": "zeronlg/datasets/PretrainDataset.py", "chunked_list": ["import os\nimport logging\nimport gzip\nimport random\nimport torch\nimport numpy as np\n\nfrom sentence_transformers import LoggingHandler\nfrom torch.utils.data import Dataset\nfrom typing import Union, List", "from torch.utils.data import Dataset\nfrom typing import Union, List\nfrom .. import Framework\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\n", "                    handlers=[LoggingHandler()])\n\nglobal_logger = logging.getLogger(__name__)\n\n\nclass InputExample:\n    \"\"\"\n    Structure for one input example\n    \"\"\"\n    def __init__(self, \n                 sid: str = '', \n                 src_text: str = None,\n                 trg_text: str = None,  \n                 label: Union[int, float, torch.Tensor, np.ndarray] = 0, \n                 lang: str = None, \n                 ):\n        \"\"\"\n        :param sid: id for the example\n        :param src_text: the source sentence\n        :param trg_text: the target sentence\n        :param label: the label for the target sentence\n        :param lang: the language of the target sentence\n        \"\"\"\n        self.sid = sid\n        self.src_text = src_text\n        self.trg_text = trg_text\n        self.label = label\n        self.lang = lang\n\n    def __str__(self):\n        return \"<InputExample> label: {}, src_text: {}, trg_text: {}, lang: {}\".format(\n            str(self.label), str(self.src_text), str(self.trg_text), str(self.lang))", "\n\nclass PretrainDataset(Dataset):\n    \"\"\"\n    Adapted from sentence_transformers.datasets.ParallelSentencesDataset\n    (https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/datasets/ParallelSentencesDataset.py)\n\n    This dataset reader can be used to read-in parallel sentences, i.e., it reads in a file with tab-seperated sentences with the same\n    sentence in different languages. For example, the file can look like this (EN\\tDE\\tES):\n    hello world     hallo welt  hola mundo\n    second sentence zweiter satz    segunda oraci\u00f3n\n\n    The sentence in the first column will be mapped to a sentence embedding using the given the embedder. For example,\n    embedder is a mono-lingual sentence embedding method for English. The sentences in the other languages will also be\n    mapped to this English sentence embedding.\n\n    When getting a sample from the dataset, we get one sentence with the according sentence embedding for this sentence.\n\n    teacher_model can be any class that implement an encode function. The encode function gets a list of sentences and\n    returns a list of sentence embeddings\n    \"\"\"\n\n    def __init__(self, \n                 teacher_model: Framework, \n                 batch_size: int = 8, \n                 use_embedding_cache: bool = True, \n                 # Yang B. modification: add extra arguments\n                 target_languages: List[str]=None, \n                 logger: logging.Logger=None,\n                 numpy_path: str = None,\n                 ):\n        \"\"\"\n        :param teacher_model: Teacher model, that provides the sentence embeddings for the first column in the dataset file\n        :param batch_size: The number of sentences used for embedding extraction per iteration\n        :param use_embedding_cache: Cache extracted embeddins for speeding up (if the training lasts multiple epochs)\n\n        :param target_languages: Columns that are not satisfied with the specific target languages will be ignored\n        :param logger: If not specified, use the global logger\n        :param numpy_path: Path to a numpy file that stores sentence embeddings\n        \"\"\"\n        self.teacher_model = teacher_model\n        self.datasets = []\n        self.datasets_iterator = []\n        self.datasets_tokenized = []\n        self.dataset_indices = []\n        self.copy_dataset_indices = []\n        self.cache = []\n        self.batch_size = batch_size\n        self.use_embedding_cache = use_embedding_cache\n        self.embedding_cache = {}\n        self.source2index = {}\n        self.num_sentences = 0\n\n        self.target_languages = target_languages\n        self.logger = logger or global_logger\n        self.numpy_path = numpy_path\n        if self.numpy_path:\n            self.logger.info(f'loading embedding cache from {self.numpy_path}')\n            self.embedding_cache = np.load(self.numpy_path)\n        if target_languages:\n            self.logger.info(f'Target languges during training: {str(self.target_languages)}')\n\n    def load_data(self, \n                  filepath: str, \n                  weight: int = 100, \n                  max_sentences: int = None, \n                  max_sentence_length: int = None, \n                  # Yang B. modification: add extra arguments\n                  first_line_is_lang: bool = False, \n                  langs: List[str]=None, \n                  exclude_source: bool=False\n                  ):\n        \"\"\"\n        Reads in a tab-seperated .txt/.csv/.tsv or .gz file. The different columns contain the different translations of the sentence in the first column\n\n        :param filepath: Filepath to the file\n        :param weight: If more than one dataset is loaded with load_data: With which frequency should data be sampled from this dataset?\n        :param max_sentences: Max number of lines to be read from filepath\n        :param max_sentence_length: Skip the example if one of the sentences is has more characters than max_sentence_length\n        :param batch_size: Size for encoding parallel sentences\n\n        :param first_line_is_lang: Whether the first line is the header that indicates the languages of each column (default to False)\n        :param langs: The specific languages of all columns (default to None)\n        :param exclude_source: Whether exclude sentences in the source langugage (i.e., the first column) as targets (default to False)\n        :return:\n        \"\"\"\n        logger = self.logger\n\n        logger.info(\"Load \"+filepath)\n        parallel_sentences = []\n        first_line_flag = True\n\n        # Yang B. modification: record parallel languages of this data if specified\n        if (first_line_is_lang or langs):\n            if not hasattr(self, 'langs_of_data'):\n                self.langs_of_data = []\n            if langs:\n                self.langs_of_data.append(langs)\n                logger.info(f\"There are {len(langs)} langauges: {langs}\")\n        elif hasattr(self, 'langs_of_data'):\n            # it means that you have a inconsistent behavior when calling this function.\n            raise ValueError('You should pass `first_line_is_lang` = True or specify langs for all data')\n        \n        with gzip.open(filepath, 'rt', encoding='utf8') if filepath.endswith('.gz') else open(filepath, encoding='utf8') as fIn:\n            count = 0\n            for line in fIn:\n                sentences = line.strip().split(\"\\t\")\n\n                # check languages\n                if first_line_flag and first_line_is_lang:\n                    first_line_flag = False\n                    if langs:\n                        assert len(langs) == len(sentences)\n                        for lang1, lang2 in zip(langs, sentences):\n                            assert lang1 == lang2\n                    else:\n                        self.langs_of_data.append(sentences)\n                        logger.info(f\"There are {len(sentences)} langauges: {sentences}\")\n                    continue\n                \n                if hasattr(self, 'langs_of_data'):\n                    # ensure that each line has the same number of sentences as that of languages\n                    assert len(sentences) == len(self.langs_of_data[-1])\n            \n                if max_sentence_length is not None and max_sentence_length > 0 and max([len(sent) for sent in sentences]) > max_sentence_length:\n                    continue\n\n                parallel_sentences.append(sentences)\n                count += 1\n                if max_sentences is not None and max_sentences > 0 and count >= max_sentences:\n                    break\n\n        # show statistics and an example\n        logger.info(f\"There are {count} lines, one of which is {parallel_sentences[0]}\")\n\n        self.add_dataset(parallel_sentences, weight, max_sentences, max_sentence_length, exclude_source)\n    \n    def add_dataset(self, \n                    parallel_sentences: List[List[str]], \n                    weight: int = 100, \n                    max_sentences: int = None, \n                    max_sentence_length: int = 128, \n                    # Yang B. modification: add extra arguments\n                    exclude_source: bool = False\n                    ):\n        sentences_map = {}\n        for idx, sentences in enumerate(parallel_sentences):\n            if max_sentence_length is not None and max_sentence_length > 0 and max([len(sent) for sent in sentences]) > max_sentence_length:\n                continue\n\n            source_sentence = sentences[0]\n            \n            self.source2index[source_sentence] = idx\n\n            if source_sentence not in sentences_map:\n                sentences_map[source_sentence] = set()\n\n            if hasattr(self, 'langs_of_data'):\n                langs = self.langs_of_data[-1]\n            else:\n                langs = [None for _ in range(len(sentences))]\n\n            # whether we exclude the source sentences as a part of targets\n            # we carry out this operation to avoid imbalanced language distribution\n            # e.g., if we add datasets of columns A-B, A-C, and A-D with `exclude_source = False`\n            # then A: B: C: D = 3: 1: 1: 1\n            start = 1 if exclude_source else 0\n            for i in range(start, len(sentences)):\n                sent = sentences[i]\n                lang = langs[i]\n                if self.target_languages and lang not in self.target_languages:\n                    continue\n                sentences_map[source_sentence].add((sent, lang))\n\n            if max_sentences is not None and max_sentences > 0 and len(sentences_map) >= max_sentences:\n                break\n\n        if len(sentences_map) == 0:\n            return\n\n        self.num_sentences += sum([len(sentences_map[sent]) for sent in sentences_map])\n\n        dataset_id = len(self.datasets)\n        self.datasets.append(list(sentences_map.items()))\n        self.datasets_iterator.append(0)\n        self.dataset_indices.extend([dataset_id] * weight)\n    \n    def generate_data(self):\n        # Yang B. modification: add the language of each target sentence (if available) and the source text into the InputExample\n        source_sentences_list = []\n        target_sentences_list = []\n        target_languages_list = []\n\n        for data_idx in self.dataset_indices:\n            src_sentence, trg_sentences = self.next_entry(data_idx)\n            source_sentences_list.append(src_sentence)\n            target_sentences_list.append([item[0] for item in trg_sentences])\n            target_languages_list.append([item[1] for item in trg_sentences])\n\n        #Generate embeddings\n        src_embeddings = self.get_embeddings(source_sentences_list)\n\n        for src_sentence, src_embedding, trg_sentences, trg_languages, data_idx in zip(\n                source_sentences_list, \n                src_embeddings, \n                target_sentences_list, \n                target_languages_list, \n                self.dataset_indices\n            ):\n            for trg_sentence, trg_language in zip(trg_sentences, trg_languages):\n                self.cache.append(\n                    InputExample(\n                        src_text=src_sentence, \n                        trg_text=trg_sentence, \n                        label=src_embedding, \n                        lang=trg_language,\n                    )\n                )\n\n        random.shuffle(self.cache)\n    \n    def next_entry(self, data_idx):\n        source, target_sentences = self.datasets[data_idx][self.datasets_iterator[data_idx]]\n\n        self.datasets_iterator[data_idx] += 1\n        if self.datasets_iterator[data_idx] >= len(self.datasets[data_idx]): #Restart iterator\n            self.datasets_iterator[data_idx] = 0\n            random.shuffle(self.datasets[data_idx])\n\n        return source, target_sentences\n\n    def get_embeddings(self, sentences):\n        # Yang B. modification: if we have loaded numpy file, directly return embeddings\n        if self.numpy_path:\n            embeddings = [self.embedding_cache[self.source2index[source]] for source in sentences]\n            return embeddings\n\n        if self.teacher_model is None:\n            return [None for sent in sentences]\n\n        if not self.use_embedding_cache:\n            return self.teacher_model.encode(sentences, batch_size=self.batch_size, show_progress_bar=False, convert_to_numpy=True)\n\n        #Use caching\n        new_sentences = []\n        for sent in sentences:\n            if sent not in self.embedding_cache:\n                new_sentences.append(sent)\n\n        if len(new_sentences) > 0:\n            new_embeddings = self.teacher_model.encode(new_sentences, batch_size=self.batch_size, show_progress_bar=False, convert_to_numpy=True)\n            for sent, embedding in zip(new_sentences, new_embeddings):\n                self.embedding_cache[sent] = embedding\n\n        return [self.embedding_cache[sent] for sent in sentences]\n\n    def __len__(self):\n        return self.num_sentences\n\n    def __getitem__(self, idx):\n        if len(self.cache) == 0:\n            self.generate_data()\n\n        return self.cache.pop()", ""]}
{"filename": "zeronlg/datasets/CaptionDataset.py", "chunked_list": ["import os\nimport logging\nimport torch\nimport numpy as np\nimport json\nimport pickle\nimport decord\n\nfrom sentence_transformers import LoggingHandler\nfrom torch.utils.data import Dataset", "from sentence_transformers import LoggingHandler\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom .. import Framework\nfrom ..utils import get_uniform_frame_ids\n\n\nlogging.basicConfig(format='%(asctime)s - %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,", "                    datefmt='%Y-%m-%d %H:%M:%S',\n                    level=logging.INFO,\n                    handlers=[LoggingHandler()])\nglobal_logger = logging.getLogger(__name__)\n\n\nclass CaptionDataset(Dataset):\n    def __init__(self, \n                 vision_root: str, \n                 ann_rpath: str, \n                 num_frames: int = 8, \n                 lang: str = None, \n                 clip_model: Framework = None, \n                 pickle_path: str = None, \n                 logger: logging.Logger = None, \n                 return_images: bool = False, \n                 mean_pooling: bool = False,\n                 ):\n        \n        if return_images:\n            assert pickle_path is None\n        else:\n            assert clip_model is not None\n\n        self.vision_root = vision_root\n        self.lang = lang\n        self.num_frames = num_frames\n        self.clip_model = clip_model\n        self.logger = logger or global_logger\n        self.return_images = return_images\n        self.mean_pooling = mean_pooling\n\n        self.annotation = json.load(open(ann_rpath, 'r'))\n        assert 'image' in self.annotation[0], f'{self.annotation[0]} does not contain the key `image`'\n        \n        self.pickle_path = pickle_path\n        self.has_been_updated = False\n        if pickle_path is not None and os.path.exists(pickle_path):\n            self.log(f'Load CLIP embs from {pickle_path}')\n            self.rpath2emb = pickle.load(open(pickle_path, 'rb'))\n        else:\n            self.log(f'CLIP embs does not exist: {pickle_path}')\n            self.rpath2emb = {}\n\n        self.rpath2images = {}\n\n    def get_item(self, image_id):\n        for index, ann in enumerate(self.annotation):\n            if ann['image_id'] == image_id:\n                return self.__getitem__(index)\n\n    def __len__(self):\n        return len(self.annotation)\n    \n    def __getitem__(self, index):\n        out = {}\n        ann = self.annotation[index]\n\n        out['image_id'] = ann['image_id']\n\n        if 'caption' in ann:\n            out['text'] = ann['caption']\n\n        rpath = ann['image']\n\n        if rpath not in self.rpath2emb:\n            self.has_been_updated = True\n            try:\n                image_path = os.path.join(self.vision_root, rpath)\n                image = Image.open(image_path).convert('RGB')\n                images = [image]\n            except:\n                video_path = os.path.join(self.vision_root, rpath)\n                reader = decord.VideoReader(video_path)\n                images = reader.get_batch(get_uniform_frame_ids(len(reader), self.num_frames)).asnumpy()\n                images = [Image.fromarray(image) for image in images]\n            \n            if self.clip_model is not None:\n                output_value = 'token_embeddings' if self.clip_model.get_module_attribute('use_clip_tokens', False) else 'sentence_embedding'\n                emb = self.clip_model.encode(images, output_value=output_value, show_progress_bar=False)\n                if isinstance(emb, list):\n                    emb = torch.stack(emb, dim=0).cpu().numpy()\n                out['emb'] = emb\n                self.rpath2emb[rpath] = emb\n\n            if self.return_images:\n                out['images'] = images\n                if self.clip_model is not None:\n                    self.rpath2images[rpath] = images\n        else:\n            out['emb'] = self.rpath2emb[rpath]\n            if self.return_images:\n                out['images'] = self.rpath2images[rpath]\n        \n        out['lang'] = self.lang\n        return out\n\n    def log(self, msg):\n        self.logger.info(msg)\n    \n    def save_pickle(self):\n        if self.has_been_updated and self.pickle_path is not None:\n            self.log(f'Save CLIP embs to {self.pickle_path}')\n            with open(self.pickle_path, 'wb') as wf:\n                pickle.dump(self.rpath2emb, wf)\n        \n        self.has_been_updated = False\n    \n    def collate_fn(self, batch):\n        out = {}\n        for key in batch[0].keys():\n            out[key] = [b[key] for b in batch]\n        \n        image_ids = [b['image_id'] for b in batch]\n        if 'emb' in batch[0]:\n            embs = torch.FloatTensor(np.array([b['emb'] for b in batch]))\n            if self.mean_pooling:\n                embs = embs.mean(1, keepdims=True)\n            return image_ids, embs\n        else:\n            images = [b['images'] for b in batch]\n            return image_ids, images", "\n\nclass CaptionDatasetForRetrieval(CaptionDataset):\n    def __init__(self, \n                vision_root: str, \n                ann_rpath: str, \n                num_frames: int = 8, \n                lang: str = None, \n                clip_model: Framework = None, \n                pickle_path: str = None, \n                logger: logging.Logger = None, \n                return_images: bool = False, \n                mean_pooling: bool = False,\n                ):\n        super().__init__(\n            vision_root=vision_root, \n            ann_rpath=ann_rpath, \n            num_frames=num_frames, \n            lang=lang, \n            clip_model=clip_model, \n            pickle_path=pickle_path, \n            logger=logger, \n            return_images=return_images, \n            mean_pooling=True, # TODO: we now always apply mean pooling\n        )\n    \n    def collate_fn(self, batch):\n        out = {}\n        for key in batch[0].keys():\n            out[key] = [b[key] for b in batch]\n        \n        image_ids = [b['image_id'] for b in batch]\n\n        texts = []\n        for b in batch:\n            if isinstance(b['text'], (list, tuple)):\n                texts.extend(b['text'])\n            else:\n                texts.append(b[texts])\n\n        if 'emb' in batch[0]:\n            embs = torch.FloatTensor(np.array([b['emb'] for b in batch]))\n            if self.mean_pooling:\n                embs = embs.mean(1, keepdims=True)\n            return image_ids, embs, texts\n        else:\n            images = [b['images'] for b in batch]\n            return image_ids, images, texts", ""]}
{"filename": "tests/test_eval_translate.py", "chunked_list": ["import unittest\nfrom zeronlg.utils import translate_eval\n\n\nclass EvalTranslateTest(unittest.TestCase):\n    def test_chinese(self):\n        gts = ['\u4e00\u4e2a\u5973\u5b69\u5728\u821e\u53f0\u4e0a\u5531\u6b4c']\n        res = ['\u4e00\u4e2a\u5973\u5b69\u5728\u5531\u6b4c']\n\n        score = translate_eval(gts, res, eval_lang='zh')\n        print('zh', score)\n    \n    # if you run this test for the first time, it may download stanford-corenlp-4.5.2\n    def test_english(self):\n        gts = ['a girl is singing on the stage']\n        res = ['a girl is singing on the stage']\n\n        score = translate_eval(gts, res, eval_lang='en')\n        print('en', score)\n        assert score['BLEU'] == 1.0\n    \n    def test_german(self):\n        gts = ['ein M\u00e4dchen singt auf der B\u00fchne']\n        res = ['ein M\u00e4dchen singt']\n\n        score = translate_eval(gts, res, eval_lang='de')\n        print('de', score)\n    \n    def test_french(self):\n        gts = ['Une fille chante sur sc\u00e8ne']\n        res = ['Une fille chante']\n\n        score = translate_eval(gts, res, eval_lang='fr')\n        print('fr', score)", "    \n\nif \"__main__\" == __name__:\n    unittest.main()\n"]}
{"filename": "tests/test_eval_caption.py", "chunked_list": ["import os\nimport json\nimport shutil\nimport unittest\nfrom zeronlg.utils import coco_caption_eval\nfrom pathlib import Path\n\n\nclass EvalCaptionTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.root = Path(__file__).parent.joinpath(self.__class__.__name__)\n        os.makedirs(self.root, exist_ok=True)\n                \n    def tearDown(self) -> None:\n        shutil.rmtree(self.root)\n\n    # if you run this test for the first time, it may download stanford-corenlp-3.6.0 required by the SPICE metric\n    def test_english(self):\n        annotation_file = os.path.join(self.root, 'gts.json')\n        with open(annotation_file, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"a girl is singing on the stage\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"a beautiful girl is showing her talents\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"a boy is running on the road and wearing a red bag\",\n                            \"id\": -1,\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"a black boy looks happy and runs\",\n                            \"id\": -1,\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 100},\n                        {'id': 101}\n                    ]\n                }, wf\n            )\n\n        results_file = os.path.join(self.root, 'res.json')\n        with open(results_file, 'w') as wf:\n            json.dump([\n                {'image_id': 100, 'caption': 'a beautiful girl is singing'},\n                {'image_id': 101, 'caption': 'a handsome boy is running'}\n            ], wf)\n\n        coco_eval = coco_caption_eval(annotation_file, results_file, eval_lang='en')\n        print('en', coco_eval.eval)\n    \n    def test_chinese(self):\n        annotation_file = os.path.join(self.root, 'gts.json')\n        with open(annotation_file, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"\u4e00\u4e2a\u5973\u5b69\u5728\u821e\u53f0\u4e0a\u5531\u6b4c\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"\u4e00\u4e2a\u6f02\u4eae\u7684\u5973\u5b69\u5728\u5c55\u793a\u5979\u7684\u624d\u534e\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"\u4e00\u4e2a\u7a7f\u7740\u7ea2\u8272\u4e66\u5305\u7684\u7537\u5b69\u5728\u8def\u4e0a\u8dd1\",\n                            \"id\": -1,\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"\u4e00\u4e2a\u9ed1\u76ae\u80a4\u7537\u5b69\u770b\u8d77\u6765\u5f88\u5f00\u5fc3\u5e76\u5728\u5954\u8dd1\",\n                            \"id\": -1,\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 100},\n                        {'id': 101}\n                    ]\n                }, wf\n            )\n\n        results_file = os.path.join(self.root, 'res.json')\n        with open(results_file, 'w') as wf:\n            json.dump([\n                {'image_id': 100, 'caption': '\u4e00\u4e2a\u6f02\u4eae\u7684\u5973\u5b69\u5728\u5531\u6b4c'},\n                {'image_id': 101, 'caption': '\u4e00\u4e2a\u5e05\u6c14\u7684\u7537\u5b69\u5728\u5954\u8dd1'}\n            ], wf)\n\n        coco_eval = coco_caption_eval(annotation_file, results_file, eval_lang='zh')\n        print('zh', coco_eval.eval)\n    \n    def test_german(self):\n        annotation_file = os.path.join(self.root, 'gts.json')\n        with open(annotation_file, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"Ein M\u00e4dchen singt auf der B\u00fchne\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"Ein sch\u00f6nes M\u00e4dchen zeigt ihre Talente\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"Ein Junge in einer roten Schultasche lief auf der Stra\u00dfe\",\n                            \"id\": -1,\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"Ein schwarzh\u00e4utiger Junge sieht gl\u00fccklich aus und l\u00e4uft\",\n                            \"id\": -1,\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 100},\n                        {'id': 101}\n                    ]\n                }, wf\n            )\n\n        results_file = os.path.join(self.root, 'res.json')\n        with open(results_file, 'w') as wf:\n            json.dump([\n                {'image_id': 100, 'caption': 'Ein sch\u00f6nes M\u00e4dchen singt'},\n                {'image_id': 101, 'caption': 'Ein h\u00fcbscher Junge rennt'}\n            ], wf)\n\n        coco_eval = coco_caption_eval(annotation_file, results_file, eval_lang='de')\n        print('de', coco_eval.eval)\n\n    def test_french(self):\n        annotation_file = os.path.join(self.root, 'gts.json')\n        with open(annotation_file, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"Une fille chante sur sc\u00e8ne\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"Une belle fille montre son talent\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"Gar\u00e7on dans un cartable rouge courant sur la route\",\n                            \"id\": -1,\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"Un gar\u00e7on \u00e0 la peau noire semble heureux et court\",\n                            \"id\": -1,\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 100},\n                        {'id': 101}\n                    ]\n                }, wf\n            )\n\n        results_file = os.path.join(self.root, 'res.json')\n        with open(results_file, 'w') as wf:\n            json.dump([\n                {'image_id': 100, 'caption': 'Une belle fille chantant'},\n                {'image_id': 101, 'caption': 'Un beau gar\u00e7on qui court'}\n            ], wf)\n\n        coco_eval = coco_caption_eval(annotation_file, results_file, eval_lang='fr')\n        print('fr', coco_eval.eval)", "class EvalCaptionTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.root = Path(__file__).parent.joinpath(self.__class__.__name__)\n        os.makedirs(self.root, exist_ok=True)\n                \n    def tearDown(self) -> None:\n        shutil.rmtree(self.root)\n\n    # if you run this test for the first time, it may download stanford-corenlp-3.6.0 required by the SPICE metric\n    def test_english(self):\n        annotation_file = os.path.join(self.root, 'gts.json')\n        with open(annotation_file, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"a girl is singing on the stage\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"a beautiful girl is showing her talents\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"a boy is running on the road and wearing a red bag\",\n                            \"id\": -1,\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"a black boy looks happy and runs\",\n                            \"id\": -1,\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 100},\n                        {'id': 101}\n                    ]\n                }, wf\n            )\n\n        results_file = os.path.join(self.root, 'res.json')\n        with open(results_file, 'w') as wf:\n            json.dump([\n                {'image_id': 100, 'caption': 'a beautiful girl is singing'},\n                {'image_id': 101, 'caption': 'a handsome boy is running'}\n            ], wf)\n\n        coco_eval = coco_caption_eval(annotation_file, results_file, eval_lang='en')\n        print('en', coco_eval.eval)\n    \n    def test_chinese(self):\n        annotation_file = os.path.join(self.root, 'gts.json')\n        with open(annotation_file, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"\u4e00\u4e2a\u5973\u5b69\u5728\u821e\u53f0\u4e0a\u5531\u6b4c\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"\u4e00\u4e2a\u6f02\u4eae\u7684\u5973\u5b69\u5728\u5c55\u793a\u5979\u7684\u624d\u534e\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"\u4e00\u4e2a\u7a7f\u7740\u7ea2\u8272\u4e66\u5305\u7684\u7537\u5b69\u5728\u8def\u4e0a\u8dd1\",\n                            \"id\": -1,\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"\u4e00\u4e2a\u9ed1\u76ae\u80a4\u7537\u5b69\u770b\u8d77\u6765\u5f88\u5f00\u5fc3\u5e76\u5728\u5954\u8dd1\",\n                            \"id\": -1,\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 100},\n                        {'id': 101}\n                    ]\n                }, wf\n            )\n\n        results_file = os.path.join(self.root, 'res.json')\n        with open(results_file, 'w') as wf:\n            json.dump([\n                {'image_id': 100, 'caption': '\u4e00\u4e2a\u6f02\u4eae\u7684\u5973\u5b69\u5728\u5531\u6b4c'},\n                {'image_id': 101, 'caption': '\u4e00\u4e2a\u5e05\u6c14\u7684\u7537\u5b69\u5728\u5954\u8dd1'}\n            ], wf)\n\n        coco_eval = coco_caption_eval(annotation_file, results_file, eval_lang='zh')\n        print('zh', coco_eval.eval)\n    \n    def test_german(self):\n        annotation_file = os.path.join(self.root, 'gts.json')\n        with open(annotation_file, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"Ein M\u00e4dchen singt auf der B\u00fchne\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"Ein sch\u00f6nes M\u00e4dchen zeigt ihre Talente\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"Ein Junge in einer roten Schultasche lief auf der Stra\u00dfe\",\n                            \"id\": -1,\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"Ein schwarzh\u00e4utiger Junge sieht gl\u00fccklich aus und l\u00e4uft\",\n                            \"id\": -1,\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 100},\n                        {'id': 101}\n                    ]\n                }, wf\n            )\n\n        results_file = os.path.join(self.root, 'res.json')\n        with open(results_file, 'w') as wf:\n            json.dump([\n                {'image_id': 100, 'caption': 'Ein sch\u00f6nes M\u00e4dchen singt'},\n                {'image_id': 101, 'caption': 'Ein h\u00fcbscher Junge rennt'}\n            ], wf)\n\n        coco_eval = coco_caption_eval(annotation_file, results_file, eval_lang='de')\n        print('de', coco_eval.eval)\n\n    def test_french(self):\n        annotation_file = os.path.join(self.root, 'gts.json')\n        with open(annotation_file, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"Une fille chante sur sc\u00e8ne\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 100,\n                            \"caption\": \"Une belle fille montre son talent\",\n                            \"id\": -1\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"Gar\u00e7on dans un cartable rouge courant sur la route\",\n                            \"id\": -1,\n                        },\n                        {\n                            \"image_id\": 101,\n                            \"caption\": \"Un gar\u00e7on \u00e0 la peau noire semble heureux et court\",\n                            \"id\": -1,\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 100},\n                        {'id': 101}\n                    ]\n                }, wf\n            )\n\n        results_file = os.path.join(self.root, 'res.json')\n        with open(results_file, 'w') as wf:\n            json.dump([\n                {'image_id': 100, 'caption': 'Une belle fille chantant'},\n                {'image_id': 101, 'caption': 'Un beau gar\u00e7on qui court'}\n            ], wf)\n\n        coco_eval = coco_caption_eval(annotation_file, results_file, eval_lang='fr')\n        print('fr', coco_eval.eval)", "\n\nif \"__main__\" == __name__:\n    unittest.main()\n"]}
{"filename": "tests/test_evaluator_translate.py", "chunked_list": ["import os\nimport shutil\nimport unittest\n\nfrom zeronlg import TranslateDataset, TranslateEvaluator, ZeroNLG\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\n\n\nclass EvaluatorTranslateTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.root = Path(__file__).parent.joinpath(self.__class__.__name__)\n        os.makedirs(self.root, exist_ok=True)\n        self.model = ZeroNLG('zeronlg-4langs-vc')\n                \n    def tearDown(self) -> None:\n        shutil.rmtree(self.root)\n\n    def test_en2zh(self):\n        evaluation_settings = {'lang': 'zh'}\n        \n        dataset = TranslateDataset(\n            source_language='en',\n            target_language='zh',\n            source_sentences=['a girl is singing', 'a boy is running'],\n            target_sentences=['\u4e00\u4e2a\u5973\u5b69\u5728\u5531\u6b4c', '\u4e00\u4e2a\u7537\u5b69\u5728\u5954\u8dd1'],\n        )\n\n        loader = DataLoader(\n            dataset,\n            batch_size=2,\n            shuffle=False,\n        )\n\n        evaluator = TranslateEvaluator(\n            loader=loader,\n            evaluation_settings=evaluation_settings,\n            mode='val'\n        )\n\n        score = evaluator(\n            model=self.model,\n            output_path=self.root,\n            epoch=0,\n            steps=100,\n            print_sent=True\n        )\n        print(score)\n\n\n        loader = DataLoader(\n            dataset,\n            batch_size=1,\n            shuffle=False,\n        )\n        evaluator = TranslateEvaluator(\n            loader=loader,\n            evaluation_settings=evaluation_settings,\n            mode='val'\n        )\n        score = evaluator(\n            model=self.model,\n            output_path=self.root,\n            epoch=0,\n            steps=100,\n            print_sent=True\n        )\n        print(score)\n\n    def test_zh2en(self):\n        evaluation_settings = {'lang': 'en'}\n        \n        dataset = TranslateDataset(\n            source_language='zh',\n            target_language='en',\n            source_sentences=['\u4e00\u4e2a\u5973\u5b69\u5728\u5531\u6b4c', '\u4e00\u4e2a\u7537\u5b69\u5728\u5954\u8dd1'],\n            target_sentences=['a girl is singing', 'a boy is running'],\n        )\n\n        loader = DataLoader(\n            dataset,\n            batch_size=2,\n            shuffle=False,\n        )\n\n        evaluator = TranslateEvaluator(\n            loader=loader,\n            evaluation_settings=evaluation_settings,\n            mode='val'\n        )\n\n        score = evaluator(\n            model=self.model,\n            output_path=self.root,\n            epoch=0,\n            steps=100,\n            print_sent=True\n        )\n        print(score)", "\nclass EvaluatorTranslateTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.root = Path(__file__).parent.joinpath(self.__class__.__name__)\n        os.makedirs(self.root, exist_ok=True)\n        self.model = ZeroNLG('zeronlg-4langs-vc')\n                \n    def tearDown(self) -> None:\n        shutil.rmtree(self.root)\n\n    def test_en2zh(self):\n        evaluation_settings = {'lang': 'zh'}\n        \n        dataset = TranslateDataset(\n            source_language='en',\n            target_language='zh',\n            source_sentences=['a girl is singing', 'a boy is running'],\n            target_sentences=['\u4e00\u4e2a\u5973\u5b69\u5728\u5531\u6b4c', '\u4e00\u4e2a\u7537\u5b69\u5728\u5954\u8dd1'],\n        )\n\n        loader = DataLoader(\n            dataset,\n            batch_size=2,\n            shuffle=False,\n        )\n\n        evaluator = TranslateEvaluator(\n            loader=loader,\n            evaluation_settings=evaluation_settings,\n            mode='val'\n        )\n\n        score = evaluator(\n            model=self.model,\n            output_path=self.root,\n            epoch=0,\n            steps=100,\n            print_sent=True\n        )\n        print(score)\n\n\n        loader = DataLoader(\n            dataset,\n            batch_size=1,\n            shuffle=False,\n        )\n        evaluator = TranslateEvaluator(\n            loader=loader,\n            evaluation_settings=evaluation_settings,\n            mode='val'\n        )\n        score = evaluator(\n            model=self.model,\n            output_path=self.root,\n            epoch=0,\n            steps=100,\n            print_sent=True\n        )\n        print(score)\n\n    def test_zh2en(self):\n        evaluation_settings = {'lang': 'en'}\n        \n        dataset = TranslateDataset(\n            source_language='zh',\n            target_language='en',\n            source_sentences=['\u4e00\u4e2a\u5973\u5b69\u5728\u5531\u6b4c', '\u4e00\u4e2a\u7537\u5b69\u5728\u5954\u8dd1'],\n            target_sentences=['a girl is singing', 'a boy is running'],\n        )\n\n        loader = DataLoader(\n            dataset,\n            batch_size=2,\n            shuffle=False,\n        )\n\n        evaluator = TranslateEvaluator(\n            loader=loader,\n            evaluation_settings=evaluation_settings,\n            mode='val'\n        )\n\n        score = evaluator(\n            model=self.model,\n            output_path=self.root,\n            epoch=0,\n            steps=100,\n            print_sent=True\n        )\n        print(score)", "\n\nif \"__main__\" == __name__:\n    unittest.main()\n"]}
{"filename": "tests/test_evaluator_caption.py", "chunked_list": ["import os\nimport json\nimport shutil\nimport unittest\n\nfrom zeronlg import CaptionDataset, CaptionEvaluator, ZeroNLG\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nimport wget\n", "import wget\n\n\nclass EvaluatorCaptionTest(unittest.TestCase):\n    def setUp(self) -> None:\n        self.root = Path(__file__).parent.joinpath(self.__class__.__name__)\n        os.makedirs(self.root, exist_ok=True)\n\n        self.model = ZeroNLG('zeronlg-4langs-vc')\n\n        wget.download(\n            'https://img0.baidu.com/it/u=2179151004,2612321767&fm=253&fmt=auto&app=138&f=JPEG?w=889&h=500', \n            os.path.join(self.root, '0.jpg')\n        )\n\n        ann_rpath = os.path.join(self.root, 'ann.json')\n        with open(ann_rpath, 'w') as wf:\n            json.dump([{'image': '0.jpg'}], wf)\n        \n        gt_file_path = os.path.join(self.root, 'gts.json')\n        with open(gt_file_path, 'w') as wf:\n            json.dump(\n                {\n                    'annotations': [\n                        {\n                            \"image_id\": 0,\n                            \"caption\": \"two white and cute dogs\",\n                            \"id\": -1\n                        },\n                    ],\n                    \"images\": [\n                        {'id': 0},\n                    ]\n                }, wf\n            )\n        \n        evaluation_settings = {'lang': 'en'}\n        \n        dataset = CaptionDataset(\n            vision_root=self.root,\n            ann_rpath=ann_rpath,\n            lang=evaluation_settings['lang'],\n            clip_model=self.model.clip_model,\n        )\n\n        loader = DataLoader(\n            dataset,\n            batch_size=1,\n            shuffle=False,\n            collate_fn=dataset.collate_fn\n        )\n\n        self.evaluator = CaptionEvaluator(\n            loader=loader,\n            gt_file_path=gt_file_path,\n            evaluation_settings=evaluation_settings,\n            mode='val'\n        )\n                \n    def tearDown(self) -> None:\n        shutil.rmtree(self.root)\n\n    def test(self):\n        score = self.evaluator(\n            model=self.model,\n            output_path=self.root,\n            epoch=0,\n            steps=100,\n            print_sent=True\n        )\n        print(score)", "\n\nif \"__main__\" == __name__:\n    unittest.main()\n"]}
