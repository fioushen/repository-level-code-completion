{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nwith open('README.md', 'r') as f:\n    long_description = f.read()\n\nsetup(\n    name='gravit',\n    version='1.0.0',\n    description='Graph learning framework for long-term Video undersTanding',\n    long_description=long_description,", "    description='Graph learning framework for long-term Video undersTanding',\n    long_description=long_description,\n    license='Apache License 2.0',\n    author='Kyle Min',\n    author_email='kyle.min@intel.com',\n    packages=find_packages(),\n    python_requires='>=3.7',\n    install_requires=['pyyaml', 'pandas', 'torch', 'torch-geometric>=2.0.3'],\n    scripts=['data/generate_graph.py',\n             'tools/train_context_reasoning.py',", "    scripts=['data/generate_graph.py',\n             'tools/train_context_reasoning.py',\n             'tools/evaluate.py']\n)\n"]}
{"filename": "tools/train_context_reasoning.py", "chunked_list": ["import os\nimport yaml\nimport torch\nimport torch.optim as optim\nfrom torch_geometric.loader import DataLoader\nfrom gravit.utils.parser import get_args, get_cfg\nfrom gravit.utils.logger import get_logger\nfrom gravit.models import build_model, get_loss_func\nfrom gravit.datasets import GraphDataset\n", "from gravit.datasets import GraphDataset\n\n\ndef train(cfg):\n    \"\"\"\n    Run the training process given the configuration\n    \"\"\"\n\n    # Input and output paths\n    path_graphs = os.path.join(cfg['root_data'], f'graphs/{cfg[\"graph_name\"]}')\n    path_result = os.path.join(cfg['root_result'], f'{cfg[\"exp_name\"]}')\n    os.makedirs(path_result, exist_ok=True)\n\n    # Prepare the logger and save the current configuration for future reference\n    logger = get_logger(path_result, file_name='train')\n    logger.info(cfg['exp_name'])\n    logger.info('Saving the configuration file')\n    with open(os.path.join(path_result, 'cfg.yaml'), 'w') as f:\n        yaml.dump(cfg, f, default_flow_style=False, sort_keys=False)\n\n    # Build a model and prepare the data loaders\n    logger.info('Preparing a model and data loaders')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = build_model(cfg, device)\n    train_loader = DataLoader(GraphDataset(os.path.join(path_graphs, 'train')), batch_size=cfg['batch_size'], shuffle=True)\n    val_loader = DataLoader(GraphDataset(os.path.join(path_graphs, 'val')))\n\n    # Prepare the experiment\n    loss_func = get_loss_func(cfg['loss_name'])\n    optimizer = optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg['wd'])\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg['sch_param'])\n\n    # Run the training process\n    logger.info('Training process started')\n\n    min_loss_val = float('inf')\n    for epoch in range(1, cfg['num_epoch']+1):\n        model.train()\n\n        # Train for a single epoch\n        loss_sum = 0.\n        for data in train_loader:\n            optimizer.zero_grad()\n\n            x, c, y = data.x.to(device), data.c.to(device), data.y.to(device)\n            edge_index = data.edge_index.to(device)\n            edge_attr = data.edge_attr.to(device)\n\n            logits = model(x, c, edge_index, edge_attr)\n\n            loss = loss_func(logits, y)\n            loss.backward()\n            loss_sum += loss.item()\n            optimizer.step()\n\n        # Adjust the learning rate\n        scheduler.step()\n\n        loss_train = loss_sum / len(train_loader)\n\n        # Get the validation loss\n        loss_val = val(val_loader, model, device, loss_func)\n\n        # Save the best-performing checkpoint\n        if loss_val < min_loss_val:\n            min_loss_val = loss_val\n            epoch_best = epoch\n            torch.save(model.state_dict(), os.path.join(path_result, 'ckpt_best.pt'))\n\n        # Log the losses for every epoch\n        logger.info(f'Epoch [{epoch:03d}|{cfg[\"num_epoch\"]:03d}] loss_train: {loss_train:.4f}, loss_val: {loss_val:.4f}, best: epoch {epoch_best:03d}')\n\n    logger.info('Training finished')", "\n\ndef val(val_loader, model, device, loss_func):\n    \"\"\"\n    Run a single validation process\n    \"\"\"\n\n    model.eval()\n    loss_sum = 0\n    with torch.no_grad():\n        for data in val_loader:\n            x, c, y = data.x.to(device), data.c.to(device), data.y.to(device)\n            edge_index = data.edge_index.to(device)\n            edge_attr = data.edge_attr.to(device)\n\n            logits = model(x, c, edge_index, edge_attr)\n            loss = loss_func(logits, y)\n            loss_sum += loss.item()\n\n    return loss_sum / len(val_loader)", "\n\nif __name__ == \"__main__\":\n    args = get_args()\n    cfg = get_cfg(args)\n\n    train(cfg)\n"]}
{"filename": "tools/evaluate.py", "chunked_list": ["import os\nimport yaml\nimport torch\nimport argparse\nfrom torch_geometric.loader import DataLoader\nfrom gravit.utils.parser import get_cfg\nfrom gravit.utils.logger import get_logger\nfrom gravit.models import build_model\nfrom gravit.datasets import GraphDataset\nfrom gravit.utils.formatter import get_formatting_data_dict, get_formatted_preds", "from gravit.datasets import GraphDataset\nfrom gravit.utils.formatter import get_formatting_data_dict, get_formatted_preds\nfrom gravit.utils.eval_tool import get_eval_score\n\n\ndef evaluate(cfg):\n    \"\"\"\n    Run the evaluation process given the configuration\n    \"\"\"\n\n    # Input and output paths\n    path_graphs = os.path.join(cfg['root_data'], f'graphs/{cfg[\"graph_name\"]}')\n    path_result = os.path.join(cfg['root_result'], f'{cfg[\"exp_name\"]}')\n\n    # Prepare the logger\n    logger = get_logger(path_result, file_name='eval')\n    logger.info(cfg['exp_name'])\n\n    # Build a model and prepare the data loaders\n    logger.info('Preparing a model and data loaders')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = build_model(cfg, device)\n    val_loader = DataLoader(GraphDataset(os.path.join(path_graphs, 'val')))\n    num_val_graphs = len(val_loader)\n\n    # Load the trained model\n    logger.info('Loading the trained model')\n    state_dict = torch.load(os.path.join(path_result, 'ckpt_best.pt'), map_location=torch.device('cpu'))\n    model.load_state_dict(state_dict)\n    model.eval()\n\n    # Load the feature files to properly format the evaluation result\n    logger.info('Retrieving the formatting dictionary')\n    data_dict = get_formatting_data_dict(cfg['root_data'], cfg['graph_name'])\n\n    # Run the evaluation process\n    logger.info('Evaluation process started')\n\n    preds_all = []\n    with torch.no_grad():\n        for i, data in enumerate(val_loader, 1):\n            g = data.g.tolist()\n            x, c = data.x.to(device), data.c.to(device)\n            edge_index = data.edge_index.to(device)\n            edge_attr = data.edge_attr.to(device)\n\n            logits = model(x, c, edge_index, edge_attr)\n\n            # Change the format of the model output\n            preds = get_formatted_preds(cfg['eval_type'], data_dict, logits, g)\n            preds_all.extend(preds)\n\n            logger.info(f'[{i:04d}|{num_val_graphs:04d}] processed')\n\n    # Compute the evaluation score\n    logger.info('Computing the evaluation score')\n    eval_score = get_eval_score(cfg['root_data'], cfg['eval_type'], preds_all)\n    logger.info(f'{cfg[\"eval_type\"]} evaluation finished: {eval_score:.2f}%')", "\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Evaluate the trained model from the experiment \"exp_name\"\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--root_data',     type=str,   help='Root directory to the data', default='./data')\n    parser.add_argument('--root_result',   type=str,   help='Root directory to output', default='./results')\n    parser.add_argument('--exp_name',      type=str,   help='Name of the experiment', required=True)\n    parser.add_argument('--eval_type',     type=str,   help='Type of the evaluation', required=True)\n\n    args = parser.parse_args()\n\n    path_result = os.path.join(args.root_result, args.exp_name)\n    if not os.path.isdir(path_result):\n        raise ValueError(f'Please run the training experiment \"{args.exp_name}\" first')\n\n    args.cfg = os.path.join(path_result, 'cfg.yaml')\n    cfg = get_cfg(args)\n\n    evaluate(cfg)", ""]}
{"filename": "data/generate_graph.py", "chunked_list": ["import os\nimport glob\nimport torch\nimport pickle  #nosec\nimport argparse\nimport numpy as np\nfrom functools import partial\nfrom multiprocessing import Pool\nfrom torch_geometric.data import Data\n", "from torch_geometric.data import Data\n\n\ndef _get_time_windows(list_fts, time_span):\n    \"\"\"\n    Get the time windows from the list of frame_timestamps\n    Each window is a subset of frame_timestamps where its time span is not greater than \"time_span\"\n\n    e.g.\n    input:\n        list_fts:    [902, 903, 904, 905, 910, 911, 912, 913, 914, 917]\n        time_span:   3\n    output:\n        twd_all:     [[902, 903, 904], [905], [910, 911, 912], [913, 914], [917]]\n    \"\"\"\n\n    twd_all = []\n\n    start = end = 0\n    while end < len(list_fts):\n        while end < len(list_fts) and list_fts[end] < list_fts[start] + time_span:\n            end += 1\n\n        twd_all.append(list_fts[start:end])\n        start = end\n\n    return twd_all", "\n\ndef generate_graph(data_file, path_graphs, sp):\n    \"\"\"\n    Generate graphs of a single video\n    Time span of each graph is not greater than \"time_span\"\n    \"\"\"\n\n    video_id = os.path.splitext(os.path.basename(data_file))[0]\n    with open(data_file, 'rb') as f:\n        data = pickle.load(f)  #nosec\n\n    # Get a list of frame_timestamps\n    list_fts = sorted([float(frame_timestamp) for frame_timestamp in data.keys()])\n\n    # Get the time windows where the time span of each window is not greater than \"time_span\"\n    twd_all = _get_time_windows(list_fts, args.time_span)\n\n    # Iterate over every time window\n    num_graph = 0\n    for twd in twd_all:\n        # Skip the training graphs without any temporal edges\n        if sp == 'train' and len(twd) == 1:\n            continue\n\n        # Get lists of the timestamps, features, coordinates, labels, person_ids, and global_ids for a given time window\n        timestamp, feature, coord, label, person_id, global_id = [], [], [], [], [], []\n        for fts in twd:\n            for entity in data[f'{fts:g}']:\n                timestamp.append(fts)\n                feature.append(entity['feature'])\n                x1, y1, x2, y2 = [float(c) for c in entity['person_box'].split(',')]\n                coord.append(np.array([(x1+x2)/2, (y1+y2)/2, x2-x1, y2-y1], dtype=np.float32))\n                label.append(entity['label'])\n                person_id.append(entity['person_id'])\n                global_id.append(entity['global_id'])\n\n        # Get a list of the edge information: these are for edge_index and edge_attr\n        node_source = []\n        node_target = []\n        edge_attr = []\n        for i in range(len(timestamp)):\n            for j in range(len(timestamp)):\n                # Time difference between the i-th and j-th nodes\n                time_diff = timestamp[i] - timestamp[j]\n\n                # If the edge connection mode is csi, nodes having the same identity are connected across the frames\n                # If the edge connection mode is cdi, temporally-distant nodes with different identities are also connected\n                if args.ec_mode == 'csi':\n                    id_condition = person_id[i] == person_id[j]\n                elif args.ec_mode == 'cdi':\n                    id_condition = True\n\n                # The edge ij connects the i-th node and j-th node\n                # Positive edge_attr indicates that the edge ij is backward (negative: forward)\n                if time_diff == 0 or (abs(time_diff) <= args.tau and id_condition):\n                    node_source.append(i)\n                    node_target.append(j)\n                    edge_attr.append(np.sign(time_diff))\n\n        # x: features\n        # c: coordinates of person_box\n        # g: global_ids\n        # edge_index: information on how the graph nodes are connected\n        # edge_attr: information about whether the edge is spatial (0) or temporal (positive: backward, negative: forward)\n        # y: labels\n        graphs = Data(x = torch.tensor(np.array(feature, dtype=np.float32), dtype=torch.float32),\n                      c = torch.tensor(np.array(coord, dtype=np.float32), dtype=torch.float32),\n                      g = torch.tensor(global_id, dtype=torch.long),\n                      edge_index = torch.tensor(np.array([node_source, node_target], dtype=np.int64), dtype=torch.long),\n                      edge_attr = torch.tensor(edge_attr, dtype=torch.float32),\n                      y = torch.tensor(np.array(label, dtype=np.float32), dtype=torch.float32))\n\n        num_graph += 1\n        torch.save(graphs, os.path.join(path_graphs, f'{video_id}_{num_graph:04d}.pt'))\n\n    return num_graph", "\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Generate graphs from the extracted features\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n    # Default paths for the training process\n    parser.add_argument('--root_data',     type=str,   help='Root directory to the data', default='./data')\n    parser.add_argument('--features',      type=str,   help='Name of the features', required=True)\n\n    # Two options for the edge connection mode:\n    # csi: Connect the nodes only with the same identities across the frames\n    # cdi: Connect different identities across the frames\n    parser.add_argument('--ec_mode',       type=str,   help='Edge connection mode (csi | cdi)', required=True)\n    parser.add_argument('--time_span',     type=float, help='Maximum time span for each graph in seconds', required=True)\n    parser.add_argument('--tau',           type=float, help='Maximum time difference between neighboring nodes in seconds', required=True)\n\n    global args\n    args = parser.parse_args()\n\n    # Iterate over train/val splits\n    print ('This process might take a few minutes')\n    for sp in ['train', 'val']:\n        path_graphs = os.path.join(args.root_data, f'graphs/{args.features}_{args.ec_mode}_{args.time_span}_{args.tau}/{sp}')\n        os.makedirs(path_graphs, exist_ok=True)\n\n        list_data_files = sorted(glob.glob(os.path.join(args.root_data, f'features/{args.features}/{sp}/*.pkl')))\n\n        with Pool(processes=20) as pool:\n            num_graph = pool.map(partial(generate_graph, path_graphs=path_graphs, sp=sp), list_data_files)\n\n        print (f'Graph generation for {sp} is finished (number of graphs: {sum(num_graph)})')", ""]}
{"filename": "data/annotations/merge_ava_activespeaker.py", "chunked_list": ["import os\nimport csv\nimport glob\n\n\ndef merge_csv_files(path_annts, sp):\n    \"\"\"\n    Merge multiple csv files into a single file\n    \"\"\"\n\n    csv_files = sorted(glob.glob(os.path.join(path_annts, '*.csv')))\n    data = []\n    for csv_file in csv_files:\n        with open(csv_file) as f:\n            reader = csv.reader(f)\n            data.extend(list(reader))\n\n    with open(f'data/annotations/ava_activespeaker_{sp}_v1.0.csv', 'w') as f:\n        writer = csv.writer(f, delimiter =',')\n        writer.writerows(data)", "\n\nif __name__ == \"__main__\":\n    path_annts = 'data/annotations/ava_activespeaker_test_v1.0'\n    sp = 'val'\n\n    merge_csv_files(path_annts, sp)\n"]}
{"filename": "gravit/__init__.py", "chunked_list": ["from pkg_resources import get_distribution\n\ntry:\n    __version__ = get_distribution('gravit').version\nexcept:\n    __version__ = '1.0.0'\n"]}
{"filename": "gravit/utils/formatter.py", "chunked_list": ["import os\nimport glob\nimport torch\nimport pickle  #nosec\n\n\ndef get_formatting_data_dict(root_data, graph_name, sp='val'):\n    \"\"\"\n    Get a dictionary that is used to format the results following the formatting rules of the evaluation tool\n    \"\"\"\n\n    # Get a list of the feature files\n    features = '_'.join(graph_name.split('_')[:-3])\n    list_data_files = sorted(glob.glob(os.path.join(root_data, f'features/{features}/{sp}/*.pkl')))\n\n    data_dict = {}\n    for data_file in list_data_files:\n        video_id = os.path.splitext(os.path.basename(data_file))[0]\n\n        with open(data_file, 'rb') as f:\n            data = pickle.load(f) #nosec\n\n        # Get a list of frame_timestamps\n        list_fts = sorted([float(frame_timestamp) for frame_timestamp in data.keys()])\n\n        # Iterate over all the frame_timestamps and retrieve the required data for evaluation\n        for fts in list_fts:\n            frame_timestamp = f'{fts:g}'\n            for entity in data[frame_timestamp]:\n                data_dict[entity['global_id']] = {'video_id': video_id,\n                                                  'frame_timestamp': frame_timestamp,\n                                                  'person_box': entity['person_box'],\n                                                  'person_id': entity['person_id']}\n\n    return data_dict", "\n\ndef get_formatted_preds(eval_type, data_dict, logits, global_ids):\n    \"\"\"\n    Get a list of formatted predictions from the model output, which is used to compute the evaluation score\n    \"\"\"\n\n    # Compute scores from the logits\n    scores_all = torch.sigmoid(logits.detach().cpu()).numpy()\n\n    # Iterate over all the nodes and get the formatted predictions for evaluation\n    preds = []\n    for scores, global_id in zip(scores_all, global_ids):\n        data = data_dict[global_id]\n        video_id = data['video_id']\n        frame_timestamp = float(data['frame_timestamp'])\n        x1, y1, x2, y2 = [float(c) for c in data['person_box'].split(',')]\n\n        if eval_type == 'AVA_ASD':\n            # Line formatted following Challenge #2: http://activity-net.org/challenges/2019/tasks/guest_ava.html\n            person_id = data['person_id']\n            score = scores.item()\n            pred = [video_id, frame_timestamp, x1, y1, x2, y2, 'SPEAKING_AUDIBLE', person_id, score]\n            preds.append(pred)\n\n        elif eval_type == 'AVA_AL':\n            # Line formatted following Challenge #1: http://activity-net.org/challenges/2019/tasks/guest_ava.html\n            for action_id, score in enumerate(scores, 1):\n                pred = [video_id, frame_timestamp, x1, y1, x2, y2, action_id, score]\n                preds.append(pred)\n\n    return preds", ""]}
{"filename": "gravit/utils/eval_tool.py", "chunked_list": ["# This code is based the official ActivityNet repository: https://github.com/activitynet/ActivityNet\n# The owner of the official ActivityNet repository: ActivityNet\n# Copyright (c) 2015 ActivityNet\n# Licensed under The MIT License\n# Please refer to https://github.com/activitynet/ActivityNet/blob/master/LICENSE\n\nimport os\nimport numpy as np\nimport pandas as pd\n", "import pandas as pd\n\nfrom collections import defaultdict\nimport csv\nimport decimal\nimport heapq\nfrom .ava import object_detection_evaluation\nfrom .ava import standard_fields\n\n\ndef compute_average_precision(precision, recall):\n  \"\"\"Compute Average Precision according to the definition in VOCdevkit.\n  Precision is modified to ensure that it does not decrease as recall\n  decrease.\n  Args:\n    precision: A float [N, 1] numpy array of precisions\n    recall: A float [N, 1] numpy array of recalls\n  Raises:\n    ValueError: if the input is not of the correct format\n  Returns:\n    average_precison: The area under the precision recall curve. NaN if\n      precision and recall are None.\n  \"\"\"\n  if precision is None:\n    if recall is not None:\n      raise ValueError(\"If precision is None, recall must also be None\")\n    return np.NAN\n\n  if not isinstance(precision, np.ndarray) or not isinstance(\n      recall, np.ndarray):\n    raise ValueError(\"precision and recall must be numpy array\")\n  if precision.dtype != float or recall.dtype != float:\n    raise ValueError(\"input must be float numpy array.\")\n  if len(precision) != len(recall):\n    raise ValueError(\"precision and recall must be of the same size.\")\n  if not precision.size:\n    return 0.0\n  if np.amin(precision) < 0 or np.amax(precision) > 1:\n    raise ValueError(\"Precision must be in the range of [0, 1].\")\n  if np.amin(recall) < 0 or np.amax(recall) > 1:\n    raise ValueError(\"recall must be in the range of [0, 1].\")\n  if not all(recall[i] <= recall[i + 1] for i in range(len(recall) - 1)):\n    raise ValueError(\"recall must be a non-decreasing array\")\n\n  recall = np.concatenate([[0], recall, [1]])\n  precision = np.concatenate([[0], precision, [0]])\n\n  # Smooth precision to be monotonically decreasing.\n  for i in range(len(precision) - 2, -1, -1):\n    precision[i] = np.maximum(precision[i], precision[i + 1])\n\n  indices = np.where(recall[1:] != recall[:-1])[0] + 1\n  average_precision = np.sum(\n      (recall[indices] - recall[indices - 1]) * precision[indices])\n  return average_precision", "\n\ndef compute_average_precision(precision, recall):\n  \"\"\"Compute Average Precision according to the definition in VOCdevkit.\n  Precision is modified to ensure that it does not decrease as recall\n  decrease.\n  Args:\n    precision: A float [N, 1] numpy array of precisions\n    recall: A float [N, 1] numpy array of recalls\n  Raises:\n    ValueError: if the input is not of the correct format\n  Returns:\n    average_precison: The area under the precision recall curve. NaN if\n      precision and recall are None.\n  \"\"\"\n  if precision is None:\n    if recall is not None:\n      raise ValueError(\"If precision is None, recall must also be None\")\n    return np.NAN\n\n  if not isinstance(precision, np.ndarray) or not isinstance(\n      recall, np.ndarray):\n    raise ValueError(\"precision and recall must be numpy array\")\n  if precision.dtype != float or recall.dtype != float:\n    raise ValueError(\"input must be float numpy array.\")\n  if len(precision) != len(recall):\n    raise ValueError(\"precision and recall must be of the same size.\")\n  if not precision.size:\n    return 0.0\n  if np.amin(precision) < 0 or np.amax(precision) > 1:\n    raise ValueError(\"Precision must be in the range of [0, 1].\")\n  if np.amin(recall) < 0 or np.amax(recall) > 1:\n    raise ValueError(\"recall must be in the range of [0, 1].\")\n  if not all(recall[i] <= recall[i + 1] for i in range(len(recall) - 1)):\n    raise ValueError(\"recall must be a non-decreasing array\")\n\n  recall = np.concatenate([[0], recall, [1]])\n  precision = np.concatenate([[0], precision, [0]])\n\n  # Smooth precision to be monotonically decreasing.\n  for i in range(len(precision) - 2, -1, -1):\n    precision[i] = np.maximum(precision[i], precision[i + 1])\n\n  indices = np.where(recall[1:] != recall[:-1])[0] + 1\n  average_precision = np.sum(\n      (recall[indices] - recall[indices - 1]) * precision[indices])\n  return average_precision", "\n\ndef load_csv(filename, column_names):\n  \"\"\"Loads CSV from the filename using given column names.\n  Adds uid column.\n  Args:\n    filename: Path to the CSV file to load.\n    column_names: A list of column names for the data.\n  Returns:\n    df: A Pandas DataFrame containing the data.\n  \"\"\"\n  # Here and elsewhere, df indicates a DataFrame variable.\n  df = pd.read_csv(filename, header=None, names=column_names)\n  # Creates a unique id from frame timestamp and entity id.\n  df[\"uid\"] = (df[\"frame_timestamp\"].map(str) + \":\" + df[\"entity_id\"])\n  return df", "\n\ndef eq(a, b, tolerance=1e-09):\n  \"\"\"Returns true if values are approximately equal.\"\"\"\n  return abs(a - b) <= tolerance\n\n\ndef merge_groundtruth_and_predictions(df_groundtruth, df_predictions):\n  \"\"\"Merges groundtruth and prediction DataFrames.\n  The returned DataFrame is merged on uid field and sorted in descending order\n  by score field. Bounding boxes are checked to make sure they match between\n  groundtruth and predictions.\n  Args:\n    df_groundtruth: A DataFrame with groundtruth data.\n    df_predictions: A DataFrame with predictions data.\n  Returns:\n    df_merged: A merged DataFrame, with rows matched on uid column.\n  \"\"\"\n  if df_groundtruth[\"uid\"].count() != df_predictions[\"uid\"].count():\n    raise ValueError(\n        \"Groundtruth and predictions CSV must have the same number of \"\n        \"unique rows.\")\n\n  if df_predictions[\"label\"].unique() != [\"SPEAKING_AUDIBLE\"]:\n    raise ValueError(\n        \"Predictions CSV must contain only SPEAKING_AUDIBLE label.\")\n\n  if df_predictions[\"score\"].count() < df_predictions[\"uid\"].count():\n    raise ValueError(\"Predictions CSV must contain score value for every row.\")\n\n  # Merges groundtruth and predictions on uid, validates that uid is unique\n  # in both frames, and sorts the resulting frame by the predictions score.\n  df_merged = df_groundtruth.merge(\n      df_predictions,\n      on=\"uid\",\n      suffixes=(\"_groundtruth\", \"_prediction\"),\n      validate=\"1:1\").sort_values(\n          by=[\"score\"], ascending=False).reset_index()\n  # Validates that bounding boxes in ground truth and predictions match for the\n  # same uids.\n  df_merged[\"bounding_box_correct\"] = np.where(\n      eq(df_merged[\"entity_box_x1_groundtruth\"],\n         df_merged[\"entity_box_x1_prediction\"])\n      & eq(df_merged[\"entity_box_x2_groundtruth\"],\n           df_merged[\"entity_box_x2_prediction\"])\n      & eq(df_merged[\"entity_box_y1_groundtruth\"],\n           df_merged[\"entity_box_y1_prediction\"])\n      & eq(df_merged[\"entity_box_y2_groundtruth\"],\n           df_merged[\"entity_box_y2_prediction\"]), True, False)\n\n  if (~df_merged[\"bounding_box_correct\"]).sum() > 0:\n    raise ValueError(\n        \"Mismatch between groundtruth and predictions bounding boxes found at \"\n        + str(list(df_merged[~df_merged[\"bounding_box_correct\"]][\"uid\"])))\n\n  return df_merged", "\n\ndef get_all_positives(df_merged):\n  \"\"\"Counts all positive examples in the groundtruth dataset.\"\"\"\n  return df_merged[df_merged[\"label_groundtruth\"] ==\n                   \"SPEAKING_AUDIBLE\"][\"uid\"].count()\n\n\ndef calculate_precision_recall(df_merged):\n  \"\"\"Calculates precision and recall arrays going through df_merged row-wise.\"\"\"\n  all_positives = get_all_positives(df_merged)\n\n  # Populates each row with 1 if this row is a true positive\n  # (at its score level).\n  df_merged[\"is_tp\"] = np.where(\n      (df_merged[\"label_groundtruth\"] == \"SPEAKING_AUDIBLE\") &\n      (df_merged[\"label_prediction\"] == \"SPEAKING_AUDIBLE\"), 1, 0)\n\n  # Counts true positives up to and including that row.\n  df_merged[\"tp\"] = df_merged[\"is_tp\"].cumsum()\n\n  # Calculates precision for every row counting true positives up to\n  # and including that row over the index (1-based) of that row.\n  df_merged[\"precision\"] = df_merged[\"tp\"] / (df_merged.index + 1)\n\n  # Calculates recall for every row counting true positives up to\n  # and including that row over all positives in the groundtruth dataset.\n  df_merged[\"recall\"] = df_merged[\"tp\"] / all_positives\n\n  return np.array(df_merged[\"precision\"]), np.array(df_merged[\"recall\"])", "def calculate_precision_recall(df_merged):\n  \"\"\"Calculates precision and recall arrays going through df_merged row-wise.\"\"\"\n  all_positives = get_all_positives(df_merged)\n\n  # Populates each row with 1 if this row is a true positive\n  # (at its score level).\n  df_merged[\"is_tp\"] = np.where(\n      (df_merged[\"label_groundtruth\"] == \"SPEAKING_AUDIBLE\") &\n      (df_merged[\"label_prediction\"] == \"SPEAKING_AUDIBLE\"), 1, 0)\n\n  # Counts true positives up to and including that row.\n  df_merged[\"tp\"] = df_merged[\"is_tp\"].cumsum()\n\n  # Calculates precision for every row counting true positives up to\n  # and including that row over the index (1-based) of that row.\n  df_merged[\"precision\"] = df_merged[\"tp\"] / (df_merged.index + 1)\n\n  # Calculates recall for every row counting true positives up to\n  # and including that row over all positives in the groundtruth dataset.\n  df_merged[\"recall\"] = df_merged[\"tp\"] / all_positives\n\n  return np.array(df_merged[\"precision\"]), np.array(df_merged[\"recall\"])", "\n\ndef run_evaluation_asd(predictions, groundtruth):\n  \"\"\"Runs AVA Active Speaker evaluation, returns average precision result.\"\"\"\n  column_names=[\n      \"video_id\", \"frame_timestamp\", \"entity_box_x1\", \"entity_box_y1\",\n      \"entity_box_x2\", \"entity_box_y2\", \"label\", \"entity_id\"\n  ]\n  df_groundtruth = load_csv(groundtruth, column_names=column_names)\n  df_predictions = pd.DataFrame(predictions, columns=column_names+[\"score\"])\n  # Creates a unique id from frame timestamp and entity id.\n  df_predictions[\"uid\"] = (df_predictions[\"frame_timestamp\"].map(str) + \":\" + df_predictions[\"entity_id\"])\n\n  df_merged = merge_groundtruth_and_predictions(df_groundtruth, df_predictions)\n  precision, recall = calculate_precision_recall(df_merged)\n\n  return compute_average_precision(precision, recall)", "\n\ndef make_image_key(video_id, timestamp):\n  \"\"\"Returns a unique identifier for a video id & timestamp.\"\"\"\n  return \"%s,%.6f\" % (video_id, decimal.Decimal(timestamp))\n\n\ndef read_csv(csv_file, class_whitelist=None, capacity=0):\n  \"\"\"Loads boxes and class labels from a CSV file in the AVA format.\n  CSV file format described at https://research.google.com/ava/download.html.\n  Args:\n    csv_file: A file object.\n    class_whitelist: If provided, boxes corresponding to (integer) class labels\n      not in this set are skipped.\n    capacity: Maximum number of labeled boxes allowed for each example. Default\n      is 0 where there is no limit.\n  Returns:\n    boxes: A dictionary mapping each unique image key (string) to a list of\n      boxes, given as coordinates [y1, x1, y2, x2].\n    labels: A dictionary mapping each unique image key (string) to a list of\n      integer class lables, matching the corresponding box in `boxes`.\n    scores: A dictionary mapping each unique image key (string) to a list of\n      score values lables, matching the corresponding label in `labels`. If\n      scores are not provided in the csv, then they will default to 1.0.\n    all_keys: A set of all image keys found in the csv file.\n  \"\"\"\n  entries = defaultdict(list)\n  boxes = defaultdict(list)\n  labels = defaultdict(list)\n  scores = defaultdict(list)\n  all_keys = set()\n  reader = csv.reader(csv_file)\n  for row in reader:\n    assert len(row) in [2, 7, 8], \"Wrong number of columns: \" + row\n    image_key = make_image_key(row[0], row[1])\n    all_keys.add(image_key)\n    # Rows with 2 tokens (videoid,timestatmp) indicates images with no detected\n    # / ground truth actions boxes. Add them to all_keys, so we can score\n    # appropriately, but otherwise skip the box creation steps.\n    if len(row) == 2:\n      continue\n    x1, y1, x2, y2 = [float(n) for n in row[2:6]]\n    action_id = int(row[6])\n    if class_whitelist and action_id not in class_whitelist:\n      continue\n    score = 1.0\n    if len(row) == 8:\n      score = float(row[7])\n    if capacity < 1 or len(entries[image_key]) < capacity:\n      heapq.heappush(entries[image_key], (score, action_id, y1, x1, y2, x2))\n    elif score > entries[image_key][0][0]:\n      heapq.heapreplace(entries[image_key], (score, action_id, y1, x1, y2, x2))\n  for image_key in entries:\n    # Evaluation API assumes boxes with descending scores\n    entry = sorted(entries[image_key], key=lambda tup: -tup[0])\n    for item in entry:\n      score, action_id, y1, x1, y2, x2 = item\n      boxes[image_key].append([y1, x1, y2, x2])\n      labels[image_key].append(action_id)\n      scores[image_key].append(score)\n  return boxes, labels, scores, all_keys", "\n\ndef read_detections(detections, class_whitelist, capacity=50):\n  \"\"\"\n  Loads boxes and class labels from a list of detections in the AVA format.\n  \"\"\"\n  entries = defaultdict(list)\n  boxes = defaultdict(list)\n  labels = defaultdict(list)\n  scores = defaultdict(list)\n  for row in detections:\n    image_key = make_image_key(row[0], row[1])\n    x1, y1, x2, y2 = row[2:6]\n    action_id = int(row[6])\n    if class_whitelist and action_id not in class_whitelist:\n      continue\n    score = float(row[7])\n    if capacity < 1 or len(entries[image_key]) < capacity:\n      heapq.heappush(entries[image_key], (score, action_id, y1, x1, y2, x2))\n    elif score > entries[image_key][0][0]:\n      heapq.heapreplace(entries[image_key], (score, action_id, y1, x1, y2, x2))\n  for image_key in entries:\n    # Evaluation API assumes boxes with descending scores\n    entry = sorted(entries[image_key], key=lambda tup: -tup[0])\n    for item in entry:\n      score, action_id, y1, x1, y2, x2 = item\n      boxes[image_key].append([y1, x1, y2, x2])\n      labels[image_key].append(action_id)\n      scores[image_key].append(score)\n  return boxes, labels, scores", "\n\ndef read_labelmap(labelmap_file):\n  \"\"\"Reads a labelmap without the dependency on protocol buffers.\n  Args:\n    labelmap_file: A file object containing a label map protocol buffer.\n  Returns:\n    labelmap: The label map in the form used by the object_detection_evaluation\n      module - a list of {\"id\": integer, \"name\": classname } dicts.\n    class_ids: A set containing all of the valid class id integers.\n  \"\"\"\n  labelmap = []\n  class_ids = set()\n  name = \"\"\n  class_id = \"\"\n  for line in labelmap_file:\n    if line.startswith(\"  name:\"):\n      name = line.split('\"')[1]\n    elif line.startswith(\"  id:\") or line.startswith(\"  label_id:\"):\n      class_id = int(line.strip().split(\" \")[-1])\n      labelmap.append({\"id\": class_id, \"name\": name})\n      class_ids.add(class_id)\n  return labelmap, class_ids", "\n\ndef run_evaluation_al(detections, groundtruth, labelmap):\n  \"\"\"\n  Runs AVA Actions evaluation, returns mean average precision result\n  \"\"\"\n  with open(labelmap, 'r') as f:\n    categories, class_whitelist = read_labelmap(f)\n\n  pascal_evaluator = object_detection_evaluation.PascalDetectionEvaluator(categories)\n\n  # Reads the ground truth data.\n  with open(groundtruth, 'r') as f:\n    boxes, labels, _, included_keys = read_csv(f, class_whitelist)\n  for image_key in boxes:\n    pascal_evaluator.add_single_ground_truth_image_info(\n        image_key, {\n            standard_fields.InputDataFields.groundtruth_boxes:\n                np.array(boxes[image_key], dtype=float),\n            standard_fields.InputDataFields.groundtruth_classes:\n                np.array(labels[image_key], dtype=int),\n            standard_fields.InputDataFields.groundtruth_difficult:\n                np.zeros(len(boxes[image_key]), dtype=bool)\n        })\n\n  # Reads detections data.\n  boxes, labels, scores = read_detections(detections, class_whitelist)\n  for image_key in boxes:\n    if image_key not in included_keys:\n      continue\n    pascal_evaluator.add_single_detected_image_info(\n        image_key, {\n            standard_fields.DetectionResultFields.detection_boxes:\n                np.array(boxes[image_key], dtype=float),\n            standard_fields.DetectionResultFields.detection_classes:\n                np.array(labels[image_key], dtype=int),\n            standard_fields.DetectionResultFields.detection_scores:\n                np.array(scores[image_key], dtype=float)\n        })\n\n  metrics = pascal_evaluator.evaluate()\n  return metrics['PascalBoxes_Precision/mAP@0.5IOU']", "\n\ndef get_eval_score(root_data, eval_type, preds):\n  \"\"\"\n  Compute the evaluation score\n  \"\"\"\n\n  # Path to the annotations\n  path_annts = os.path.join(root_data, 'annotations')\n\n  if eval_type == 'AVA_ASD':\n    groundtruth = os.path.join(path_annts, 'ava_activespeaker_val_v1.0.csv')\n    score = run_evaluation_asd(preds, groundtruth)\n  elif eval_type == 'AVA_AL':\n    groundtruth = os.path.join(path_annts, 'ava_val_v2.2.csv')\n    labelmap = os.path.join(path_annts, 'ava_action_list_v2.2_for_activitynet_2019.pbtxt')\n    score = run_evaluation_al(preds, groundtruth, labelmap)\n\n  return score*100.", ""]}
{"filename": "gravit/utils/parser.py", "chunked_list": ["import yaml\nimport argparse\n\n\ndef get_args():\n    \"\"\"\n    Get the command-line arguments for the configuration\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--cfg',           type=str,   help='Path to the configuration file', required=True)\n\n    # Additional arguments from the command line override the configuration from cfg file\n\n    # Root directories for the training process\n    parser.add_argument('--root_data',     type=str,   help='Root directory to the data', default='./data')\n    parser.add_argument('--root_result',   type=str,   help='Root directory to output', default='./results')\n\n    # Names required for the training process\n    parser.add_argument('--exp_name',      type=str,   help='Name of the experiment')\n    parser.add_argument('--model_name',    type=str,   help='Name of the model')\n    parser.add_argument('--graph_name',    type=str,   help='Name of the graphs')\n    parser.add_argument('--loss_name',     type=str,   help='Name of the loss function')\n\n    # Other hyper-parameters\n    parser.add_argument('--num_modality',  type=int,   help='Number of input modalities')\n    parser.add_argument('--channel1',      type=int,   help='Filter dimension of the first GCN layers')\n    parser.add_argument('--channel2',      type=int,   help='Filter dimension of the rest GCN layers')\n    parser.add_argument('--proj_dim',      type=int,   help='Dimension of the projected spatial feature')\n    parser.add_argument('--final_dim',     type=int,   help='Dimension of the final output')\n    parser.add_argument('--dropout',       type=float, help='Dropout for the last GCN layers')\n    parser.add_argument('--lr',            type=float, help='Initial learning rate')\n    parser.add_argument('--wd',            type=float, help='Weight decay value for regularization')\n    parser.add_argument('--batch_size',    type=int,   help='Batch size during the training process')\n    parser.add_argument('--sch_param',     type=int,   help='Parameter for lr_scheduler')\n    parser.add_argument('--num_epoch',     type=int,   help='Total number of epochs')\n\n    return parser.parse_args()", "\n\ndef get_cfg(args):\n    \"\"\"\n    Initialize the configuration given the optional command-line arguments\n    \"\"\"\n\n    with open(args.cfg, 'r') as f:\n        cfg = yaml.safe_load(f)\n        delattr(args, 'cfg')\n\n    for k, v in vars(args).items():\n        if v is None:\n            if k not in cfg:\n                raise ValueError(f'Please specify \"{k}\" in your command-line arguments or in the configuration file')\n        else:\n            cfg[k] = v\n\n    return cfg", ""]}
{"filename": "gravit/utils/logger.py", "chunked_list": ["import os\nimport logging\n\ndef get_logger(path_result, file_name, file_mode='w'):\n    \"\"\"\n    Get the logger that logs runtime messages under \"path_result\"\n    \"\"\"\n\n    logging.basicConfig(format='%(asctime)s.%(msecs)03d %(message)s',\n                        datefmt='%m/%d/%Y %H:%M:%S',\n                        level=logging.DEBUG,\n                        handlers=[logging.StreamHandler(),\n                        logging.FileHandler(filename=os.path.join(path_result, f'{file_name}.log'), mode=file_mode)])\n\n    return logging.getLogger()", ""]}
{"filename": "gravit/utils/__init__.py", "chunked_list": [""]}
{"filename": "gravit/utils/ava/np_mask_ops.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Operations for [N, height, width] numpy arrays representing masks.\n\nExample mask operations that are supported:", "\nExample mask operations that are supported:\n  * Areas: compute mask areas\n  * IOU: pairwise intersection-over-union scores\n\"\"\"\nimport numpy as np\n\nEPSILON = 1e-7\n\n\ndef area(masks):\n  \"\"\"Computes area of masks.\n\n  Args:\n    masks: Numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N*1] representing mask areas.\n\n  Raises:\n    ValueError: If masks.dtype is not np.uint8\n  \"\"\"\n  if masks.dtype != np.uint8:\n    raise ValueError('Masks type should be np.uint8')\n  return np.sum(masks, axis=(1, 2), dtype=np.float32)", "\n\ndef area(masks):\n  \"\"\"Computes area of masks.\n\n  Args:\n    masks: Numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N*1] representing mask areas.\n\n  Raises:\n    ValueError: If masks.dtype is not np.uint8\n  \"\"\"\n  if masks.dtype != np.uint8:\n    raise ValueError('Masks type should be np.uint8')\n  return np.sum(masks, axis=(1, 2), dtype=np.float32)", "\n\ndef intersection(masks1, masks2):\n  \"\"\"Compute pairwise intersection areas between masks.\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding M masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  \"\"\"\n  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:\n    raise ValueError('masks1 and masks2 should be of type np.uint8')\n  n = masks1.shape[0]\n  m = masks2.shape[0]\n  answer = np.zeros([n, m], dtype=np.float32)\n  for i in np.arange(n):\n    for j in np.arange(m):\n      answer[i, j] = np.sum(np.minimum(masks1[i], masks2[j]), dtype=np.float32)\n  return answer", "\n\ndef iou(masks1, masks2):\n  \"\"\"Computes pairwise intersection-over-union between mask collections.\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  \"\"\"\n  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:\n    raise ValueError('masks1 and masks2 should be of type np.uint8')\n  intersect = intersection(masks1, masks2)\n  area1 = area(masks1)\n  area2 = area(masks2)\n  union = np.expand_dims(area1, axis=1) + np.expand_dims(\n      area2, axis=0) - intersect\n  return intersect / np.maximum(union, EPSILON)", "\n\ndef ioa(masks1, masks2):\n  \"\"\"Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two masks, mask1 and mask2 is defined as\n  their intersection area over mask2's area. Note that ioa is not symmetric,\n  that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n\n  Args:\n    masks1: a numpy array with shape [N, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n    masks2: a numpy array with shape [M, height, width] holding N masks. Masks\n      values are of type np.uint8 and values are in {0,1}.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n\n  Raises:\n    ValueError: If masks1 and masks2 are not of type np.uint8.\n  \"\"\"\n  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:\n    raise ValueError('masks1 and masks2 should be of type np.uint8')\n  intersect = intersection(masks1, masks2)\n  areas = np.expand_dims(area(masks2), axis=0)\n  return intersect / (areas + EPSILON)", ""]}
{"filename": "gravit/utils/ava/metrics.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Functions for computing metrics like precision, recall, CorLoc and etc.\"\"\"\nfrom __future__ import division\n", "from __future__ import division\n\nimport numpy as np\n\n\ndef compute_precision_recall(scores, labels, num_gt):\n  \"\"\"Compute precision and recall.\n\n  Args:\n    scores: A float numpy array representing detection score\n    labels: A boolean numpy array representing true/false positive labels\n    num_gt: Number of ground truth instances\n\n  Raises:\n    ValueError: if the input is not of the correct format\n\n  Returns:\n    precision: Fraction of positive instances over detected ones. This value is\n      None if no ground truth labels are present.\n    recall: Fraction of detected positive instance over all positive instances.\n      This value is None if no ground truth labels are present.\n\n  \"\"\"\n  if not isinstance(\n      labels, np.ndarray) or labels.dtype != bool or len(labels.shape) != 1:\n    raise ValueError(\"labels must be single dimension bool numpy array\")\n\n  if not isinstance(\n      scores, np.ndarray) or len(scores.shape) != 1:\n    raise ValueError(\"scores must be single dimension numpy array\")\n\n  if num_gt < np.sum(labels):\n    raise ValueError(\"Number of true positives must be smaller than num_gt.\")\n\n  if len(scores) != len(labels):\n    raise ValueError(\"scores and labels must be of the same size.\")\n\n  if num_gt == 0:\n    return None, None\n\n  sorted_indices = np.argsort(scores)\n  sorted_indices = sorted_indices[::-1]\n  labels = labels.astype(int)\n  true_positive_labels = labels[sorted_indices]\n  false_positive_labels = 1 - true_positive_labels\n  cum_true_positives = np.cumsum(true_positive_labels)\n  cum_false_positives = np.cumsum(false_positive_labels)\n  precision = cum_true_positives.astype(float) / (\n      cum_true_positives + cum_false_positives)\n  recall = cum_true_positives.astype(float) / num_gt\n  return precision, recall", "\n\ndef compute_average_precision(precision, recall):\n  \"\"\"Compute Average Precision according to the definition in VOCdevkit.\n\n  Precision is modified to ensure that it does not decrease as recall\n  decrease.\n\n  Args:\n    precision: A float [N, 1] numpy array of precisions\n    recall: A float [N, 1] numpy array of recalls\n\n  Raises:\n    ValueError: if the input is not of the correct format\n\n  Returns:\n    average_precison: The area under the precision recall curve. NaN if\n      precision and recall are None.\n\n  \"\"\"\n  if precision is None:\n    if recall is not None:\n      raise ValueError(\"If precision is None, recall must also be None\")\n    return np.NAN\n\n  if not isinstance(precision, np.ndarray) or not isinstance(recall,\n                                                             np.ndarray):\n    raise ValueError(\"precision and recall must be numpy array\")\n  if precision.dtype != float or recall.dtype != float:\n    raise ValueError(\"input must be float numpy array.\")\n  if len(precision) != len(recall):\n    raise ValueError(\"precision and recall must be of the same size.\")\n  if not precision.size:\n    return 0.0\n  if np.amin(precision) < 0 or np.amax(precision) > 1:\n    raise ValueError(\"Precision must be in the range of [0, 1].\")\n  if np.amin(recall) < 0 or np.amax(recall) > 1:\n    raise ValueError(\"recall must be in the range of [0, 1].\")\n  if not all(recall[i] <= recall[i + 1] for i in range(len(recall) - 1)):\n    raise ValueError(\"recall must be a non-decreasing array\")\n\n  recall = np.concatenate([[0], recall, [1]])\n  precision = np.concatenate([[0], precision, [0]])\n\n  # Preprocess precision to be a non-decreasing array\n  for i in range(len(precision) - 2, -1, -1):\n    precision[i] = np.maximum(precision[i], precision[i + 1])\n\n  indices = np.where(recall[1:] != recall[:-1])[0] + 1\n  average_precision = np.sum(\n      (recall[indices] - recall[indices - 1]) * precision[indices])\n  return average_precision", "\n\ndef compute_cor_loc(num_gt_imgs_per_class,\n                    num_images_correctly_detected_per_class):\n  \"\"\"Compute CorLoc according to the definition in the following paper.\n\n  https://www.robots.ox.ac.uk/~vgg/rg/papers/deselaers-eccv10.pdf\n\n  Returns nans if there are no ground truth images for a class.\n\n  Args:\n    num_gt_imgs_per_class: 1D array, representing number of images containing\n        at least one object instance of a particular class\n    num_images_correctly_detected_per_class: 1D array, representing number of\n        images that are correctly detected at least one object instance of a\n        particular class\n\n  Returns:\n    corloc_per_class: A float numpy array represents the corloc score of each\n      class\n  \"\"\"\n  # Divide by zero expected for classes with no gt examples.\n  with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n    return np.where(\n        num_gt_imgs_per_class == 0, np.nan,\n        num_images_correctly_detected_per_class / num_gt_imgs_per_class)", ""]}
{"filename": "gravit/utils/ava/__init__.py", "chunked_list": [""]}
{"filename": "gravit/utils/ava/np_box_list.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Numpy BoxList classes and functions.\"\"\"\n\nimport numpy as np", "\nimport numpy as np\n\n\nclass BoxList(object):\n  \"\"\"Box collection.\n\n  BoxList represents a list of bounding boxes as numpy array, where each\n  bounding box is represented as a row of 4 numbers,\n  [y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes within a\n  given list correspond to a single image.\n\n  Optionally, users can add additional related fields (such as\n  objectness/classification scores).\n  \"\"\"\n\n  def __init__(self, data):\n    \"\"\"Constructs box collection.\n\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n\n    Raises:\n      ValueError: if bbox data is not a numpy array\n      ValueError: if invalid dimensions for bbox data\n    \"\"\"\n    if not isinstance(data, np.ndarray):\n      raise ValueError('data must be a numpy array.')\n    if len(data.shape) != 2 or data.shape[1] != 4:\n      raise ValueError('Invalid dimensions for box data.')\n    if data.dtype != np.float32 and data.dtype != np.float64:\n      raise ValueError('Invalid data type for box data: float is required.')\n    if not self._is_valid_boxes(data):\n      raise ValueError('Invalid box data. data must be a numpy array of '\n                       'N*[y_min, x_min, y_max, x_max]')\n    self.data = {'boxes': data}\n\n  def num_boxes(self):\n    \"\"\"Return number of boxes held in collections.\"\"\"\n    return self.data['boxes'].shape[0]\n\n  def get_extra_fields(self):\n    \"\"\"Return all non-box fields.\"\"\"\n    return [k for k in self.data.keys() if k != 'boxes']\n\n  def has_field(self, field):\n    return field in self.data\n\n  def add_field(self, field, field_data):\n    \"\"\"Add data to a specified field.\n\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n      field_data: a numpy array of [N, ...] representing the data associated\n          with the field.\n    Raises:\n      ValueError: if the field is already exist or the dimension of the field\n          data does not matches the number of boxes.\n    \"\"\"\n    if self.has_field(field):\n      raise ValueError('Field ' + field + 'already exists')\n    if len(field_data.shape) < 1 or field_data.shape[0] != self.num_boxes():\n      raise ValueError('Invalid dimensions for field data')\n    self.data[field] = field_data\n\n  def get(self):\n    \"\"\"Convenience function for accesssing box coordinates.\n\n    Returns:\n      a numpy array of shape [N, 4] representing box corners\n    \"\"\"\n    return self.get_field('boxes')\n\n  def get_field(self, field):\n    \"\"\"Accesses data associated with the specified field in the box collection.\n\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n\n    Returns:\n      a numpy 1-d array representing data of an associated field\n\n    Raises:\n      ValueError: if invalid field\n    \"\"\"\n    if not self.has_field(field):\n      raise ValueError('field {} does not exist'.format(field))\n    return self.data[field]\n\n  def get_coordinates(self):\n    \"\"\"Get corner coordinates of boxes.\n\n    Returns:\n     a list of 4 1-d numpy arrays [y_min, x_min, y_max, x_max]\n    \"\"\"\n    box_coordinates = self.get()\n    y_min = box_coordinates[:, 0]\n    x_min = box_coordinates[:, 1]\n    y_max = box_coordinates[:, 2]\n    x_max = box_coordinates[:, 3]\n    return [y_min, x_min, y_max, x_max]\n\n  def _is_valid_boxes(self, data):\n    \"\"\"Check whether data fullfills the format of N*[ymin, xmin, ymax, xmin].\n\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n\n    Returns:\n      a boolean indicating whether all ymax of boxes are equal or greater than\n          ymin, and all xmax of boxes are equal or greater than xmin.\n    \"\"\"\n    if data.shape[0] > 0:\n      for i in range(data.shape[0]):\n        if data[i, 0] > data[i, 2] or data[i, 1] > data[i, 3]:\n          return False\n    return True", ""]}
{"filename": "gravit/utils/ava/np_box_list_ops.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Bounding Box List operations for Numpy BoxLists.\n\nExample box operations that are supported:", "\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n\"\"\"\nimport numpy as np\n\nfrom . import np_box_list\nfrom . import np_box_ops\n", "from . import np_box_ops\n\n\nclass SortOrder(object):\n  \"\"\"Enum class for sort order.\n\n  Attributes:\n    ascend: ascend order.\n    descend: descend order.\n  \"\"\"\n  ASCEND = 1\n  DESCEND = 2", "\n\ndef area(boxlist):\n  \"\"\"Computes area of boxes.\n\n  Args:\n    boxlist: BoxList holding N boxes\n\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  \"\"\"\n  y_min, x_min, y_max, x_max = boxlist.get_coordinates()\n  return (y_max - y_min) * (x_max - x_min)", "\n\ndef intersection(boxlist1, boxlist2):\n  \"\"\"Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  \"\"\"\n  return np_box_ops.intersection(boxlist1.get(), boxlist2.get())", "\n\ndef iou(boxlist1, boxlist2):\n  \"\"\"Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  \"\"\"\n  return np_box_ops.iou(boxlist1.get(), boxlist2.get())", "\n\ndef ioa(boxlist1, boxlist2):\n  \"\"\"Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2's area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  \"\"\"\n  return np_box_ops.ioa(boxlist1.get(), boxlist2.get())", "\n\ndef gather(boxlist, indices, fields=None):\n  \"\"\"Gather boxes from BoxList according to indices and return new BoxList.\n\n  By default, gather returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the boxlist (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    indices: a 1-d numpy array of type int_\n    fields: (optional) list of fields to also gather from.  If None (default),\n        all fields are gathered from.  Pass an empty fields list to only gather\n        the box coordinates.\n\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n        specified by indices\n\n  Raises:\n    ValueError: if specified field is not contained in boxlist or if the\n        indices are not of type int_\n  \"\"\"\n  if indices.size:\n    if np.amax(indices) >= boxlist.num_boxes() or np.amin(indices) < 0:\n      raise ValueError('indices are out of valid range.')\n  subboxlist = np_box_list.BoxList(boxlist.get()[indices, :])\n  if fields is None:\n    fields = boxlist.get_extra_fields()\n  for field in fields:\n    extra_field_data = boxlist.get_field(field)\n    subboxlist.add_field(field, extra_field_data[indices, ...])\n  return subboxlist", "\n\ndef sort_by_field(boxlist, field, order=SortOrder.DESCEND):\n  \"\"\"Sort boxes and associated fields according to a scalar field.\n\n  A common use case is reordering the boxes according to descending scores.\n\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: A BoxList field for sorting and reordering the BoxList.\n    order: (Optional) 'descend' or 'ascend'. Default is descend.\n\n  Returns:\n    sorted_boxlist: A sorted BoxList with the field in the specified order.\n\n  Raises:\n    ValueError: if specified field does not exist or is not of single dimension.\n    ValueError: if the order is not either descend or ascend.\n  \"\"\"\n  if not boxlist.has_field(field):\n    raise ValueError('Field ' + field + ' does not exist')\n  if len(boxlist.get_field(field).shape) != 1:\n    raise ValueError('Field ' + field + 'should be single dimension.')\n  if order != SortOrder.DESCEND and order != SortOrder.ASCEND:\n    raise ValueError('Invalid sort order')\n\n  field_to_sort = boxlist.get_field(field)\n  sorted_indices = np.argsort(field_to_sort)\n  if order == SortOrder.DESCEND:\n    sorted_indices = sorted_indices[::-1]\n  return gather(boxlist, sorted_indices)", "\n\ndef non_max_suppression(boxlist,\n                        max_output_size=10000,\n                        iou_threshold=1.0,\n                        score_threshold=-10.0):\n  \"\"\"Non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes. In each iteration, the detected bounding box with\n  highest score in the available pool is selected.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n      representing detection scores. All scores belong to the same class.\n    max_output_size: maximum number of retained boxes\n    iou_threshold: intersection over union threshold.\n    score_threshold: minimum score threshold. Remove the boxes with scores\n                     less than this value. Default value is set to -10. A very\n                     low threshold to pass pretty much all the boxes, unless\n                     the user sets a different score threshold.\n\n  Returns:\n    a BoxList holding M boxes where M <= max_output_size\n  Raises:\n    ValueError: if 'scores' field does not exist\n    ValueError: if threshold is not in [0, 1]\n    ValueError: if max_output_size < 0\n  \"\"\"\n  if not boxlist.has_field('scores'):\n    raise ValueError('Field scores does not exist')\n  if iou_threshold < 0. or iou_threshold > 1.0:\n    raise ValueError('IOU threshold must be in [0, 1]')\n  if max_output_size < 0:\n    raise ValueError('max_output_size must be bigger than 0.')\n\n  boxlist = filter_scores_greater_than(boxlist, score_threshold)\n  if boxlist.num_boxes() == 0:\n    return boxlist\n\n  boxlist = sort_by_field(boxlist, 'scores')\n\n  # Prevent further computation if NMS is disabled.\n  if iou_threshold == 1.0:\n    if boxlist.num_boxes() > max_output_size:\n      selected_indices = np.arange(max_output_size)\n      return gather(boxlist, selected_indices)\n    else:\n      return boxlist\n\n  boxes = boxlist.get()\n  num_boxes = boxlist.num_boxes()\n  # is_index_valid is True only for all remaining valid boxes,\n  is_index_valid = np.full(num_boxes, 1, dtype=bool)\n  selected_indices = []\n  num_output = 0\n  for i in range(num_boxes):\n    if num_output < max_output_size:\n      if is_index_valid[i]:\n        num_output += 1\n        selected_indices.append(i)\n        is_index_valid[i] = False\n        valid_indices = np.where(is_index_valid)[0]\n        if valid_indices.size == 0:\n          break\n\n        intersect_over_union = np_box_ops.iou(\n            np.expand_dims(boxes[i, :], axis=0), boxes[valid_indices, :])\n        intersect_over_union = np.squeeze(intersect_over_union, axis=0)\n        is_index_valid[valid_indices] = np.logical_and(\n            is_index_valid[valid_indices],\n            intersect_over_union <= iou_threshold)\n  return gather(boxlist, np.array(selected_indices))", "\n\ndef multi_class_non_max_suppression(boxlist, score_thresh, iou_thresh,\n                                    max_output_size):\n  \"\"\"Multi-class version of non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n      representing detection scores.  This scores field is a tensor that can\n      be 1 dimensional (in the case of a single class) or 2-dimensional, which\n      which case we assume that it takes the shape [num_boxes, num_classes].\n      We further assume that this rank is known statically and that\n      scores.shape[1] is also known (i.e., the number of classes is fixed\n      and known at graph construction time).\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap\n      with previously selected boxes are removed).\n    max_output_size: maximum number of retained boxes per class.\n\n  Returns:\n    a BoxList holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not have\n      a valid scores field.\n  \"\"\"\n  if not 0 <= iou_thresh <= 1.0:\n    raise ValueError('thresh must be between 0 and 1')\n  if not isinstance(boxlist, np_box_list.BoxList):\n    raise ValueError('boxlist must be a BoxList')\n  if not boxlist.has_field('scores'):\n    raise ValueError('input boxlist must have \\'scores\\' field')\n  scores = boxlist.get_field('scores')\n  if len(scores.shape) == 1:\n    scores = np.reshape(scores, [-1, 1])\n  elif len(scores.shape) == 2:\n    if scores.shape[1] is None:\n      raise ValueError('scores field must have statically defined second '\n                       'dimension')\n  else:\n    raise ValueError('scores field must be of rank 1 or 2')\n  num_boxes = boxlist.num_boxes()\n  num_scores = scores.shape[0]\n  num_classes = scores.shape[1]\n\n  if num_boxes != num_scores:\n    raise ValueError('Incorrect scores field length: actual vs expected.')\n\n  selected_boxes_list = []\n  for class_idx in range(num_classes):\n    boxlist_and_class_scores = np_box_list.BoxList(boxlist.get())\n    class_scores = np.reshape(scores[0:num_scores, class_idx], [-1])\n    boxlist_and_class_scores.add_field('scores', class_scores)\n    boxlist_filt = filter_scores_greater_than(boxlist_and_class_scores,\n                                              score_thresh)\n    nms_result = non_max_suppression(boxlist_filt,\n                                     max_output_size=max_output_size,\n                                     iou_threshold=iou_thresh,\n                                     score_threshold=score_thresh)\n    nms_result.add_field(\n        'classes', np.zeros_like(nms_result.get_field('scores')) + class_idx)\n    selected_boxes_list.append(nms_result)\n  selected_boxes = concatenate(selected_boxes_list)\n  sorted_boxes = sort_by_field(selected_boxes, 'scores')\n  return sorted_boxes", "\n\ndef scale(boxlist, y_scale, x_scale):\n  \"\"\"Scale box coordinates in x and y dimensions.\n\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: float\n    x_scale: float\n\n  Returns:\n    boxlist: BoxList holding N boxes\n  \"\"\"\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  y_min = y_scale * y_min\n  y_max = y_scale * y_max\n  x_min = x_scale * x_min\n  x_max = x_scale * x_max\n  scaled_boxlist = np_box_list.BoxList(np.hstack([y_min, x_min, y_max, x_max]))\n\n  fields = boxlist.get_extra_fields()\n  for field in fields:\n    extra_field_data = boxlist.get_field(field)\n    scaled_boxlist.add_field(field, extra_field_data)\n\n  return scaled_boxlist", "\n\ndef clip_to_window(boxlist, window):\n  \"\"\"Clip bounding boxes to a window.\n\n  This op clips input bounding boxes (represented by bounding box\n  corners) to a window, optionally filtering out boxes that do not\n  overlap at all with the window.\n\n  Args:\n    boxlist: BoxList holding M_in boxes\n    window: a numpy array of shape [4] representing the\n            [y_min, x_min, y_max, x_max] window to which the op\n            should clip boxes.\n\n  Returns:\n    a BoxList holding M_out boxes where M_out <= M_in\n  \"\"\"\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  win_y_min = window[0]\n  win_x_min = window[1]\n  win_y_max = window[2]\n  win_x_max = window[3]\n  y_min_clipped = np.fmax(np.fmin(y_min, win_y_max), win_y_min)\n  y_max_clipped = np.fmax(np.fmin(y_max, win_y_max), win_y_min)\n  x_min_clipped = np.fmax(np.fmin(x_min, win_x_max), win_x_min)\n  x_max_clipped = np.fmax(np.fmin(x_max, win_x_max), win_x_min)\n  clipped = np_box_list.BoxList(\n      np.hstack([y_min_clipped, x_min_clipped, y_max_clipped, x_max_clipped]))\n  clipped = _copy_extra_fields(clipped, boxlist)\n  areas = area(clipped)\n  nonzero_area_indices = np.reshape(np.nonzero(np.greater(areas, 0.0)),\n                                    [-1]).astype(np.int32)\n  return gather(clipped, nonzero_area_indices)", "\n\ndef prune_non_overlapping_boxes(boxlist1, boxlist2, minoverlap=0.0):\n  \"\"\"Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n\n  For each box in boxlist1, we want its IOA to be more than minoverlap with\n  at least one of the boxes in boxlist2. If it does not, we remove it.\n\n  Args:\n    boxlist1: BoxList holding N boxes.\n    boxlist2: BoxList holding M boxes.\n    minoverlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n\n  Returns:\n    A pruned boxlist with size [N', 4].\n  \"\"\"\n  intersection_over_area = ioa(boxlist2, boxlist1)  # [M, N] tensor\n  intersection_over_area = np.amax(intersection_over_area, axis=0)  # [N] tensor\n  keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))\n  keep_inds = np.nonzero(keep_bool)[0]\n  new_boxlist1 = gather(boxlist1, keep_inds)\n  return new_boxlist1", "\n\ndef prune_outside_window(boxlist, window):\n  \"\"\"Prunes bounding boxes that fall outside a given window.\n\n  This function prunes bounding boxes that even partially fall outside the given\n  window. See also ClipToWindow which only prunes bounding boxes that fall\n  completely outside the window, and clips any bounding boxes that partially\n  overflow.\n\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a numpy array of size 4, representing [ymin, xmin, ymax, xmax]\n            of the window.\n\n  Returns:\n    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in.\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  \"\"\"\n\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  win_y_min = window[0]\n  win_x_min = window[1]\n  win_y_max = window[2]\n  win_x_max = window[3]\n  coordinate_violations = np.hstack([np.less(y_min, win_y_min),\n                                     np.less(x_min, win_x_min),\n                                     np.greater(y_max, win_y_max),\n                                     np.greater(x_max, win_x_max)])\n  valid_indices = np.reshape(\n      np.where(np.logical_not(np.max(coordinate_violations, axis=1))), [-1])\n  return gather(boxlist, valid_indices), valid_indices", "\n\ndef concatenate(boxlists, fields=None):\n  \"\"\"Concatenate list of BoxLists.\n\n  This op concatenates a list of input BoxLists into a larger BoxList.  It also\n  handles concatenation of BoxList fields as long as the field tensor shapes\n  are equal except for the first dimension.\n\n  Args:\n    boxlists: list of BoxList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxList in the list are included in the\n      concatenation.\n\n  Returns:\n    a BoxList with number of boxes equal to\n      sum([boxlist.num_boxes() for boxlist in BoxList])\n  Raises:\n    ValueError: if boxlists is invalid (i.e., is not a list, is empty, or\n      contains non BoxList objects), or if requested fields are not contained in\n      all boxlists\n  \"\"\"\n  if not isinstance(boxlists, list):\n    raise ValueError('boxlists should be a list')\n  if not boxlists:\n    raise ValueError('boxlists should have nonzero length')\n  for boxlist in boxlists:\n    if not isinstance(boxlist, np_box_list.BoxList):\n      raise ValueError('all elements of boxlists should be BoxList objects')\n  concatenated = np_box_list.BoxList(\n      np.vstack([boxlist.get() for boxlist in boxlists]))\n  if fields is None:\n    fields = boxlists[0].get_extra_fields()\n  for field in fields:\n    first_field_shape = boxlists[0].get_field(field).shape\n    first_field_shape = first_field_shape[1:]\n    for boxlist in boxlists:\n      if not boxlist.has_field(field):\n        raise ValueError('boxlist must contain all requested fields')\n      field_shape = boxlist.get_field(field).shape\n      field_shape = field_shape[1:]\n      if field_shape != first_field_shape:\n        raise ValueError('field %s must have same shape for all boxlists '\n                         'except for the 0th dimension.' % field)\n    concatenated_field = np.concatenate(\n        [boxlist.get_field(field) for boxlist in boxlists], axis=0)\n    concatenated.add_field(field, concatenated_field)\n  return concatenated", "\n\ndef filter_scores_greater_than(boxlist, thresh):\n  \"\"\"Filter to keep only boxes with score exceeding a given threshold.\n\n  This op keeps the collection of boxes whose corresponding scores are\n  greater than the input threshold.\n\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a 'scores' field\n      representing detection scores.\n    thresh: scalar threshold\n\n  Returns:\n    a BoxList holding M boxes where M <= N\n\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not\n      have a scores field\n  \"\"\"\n  if not isinstance(boxlist, np_box_list.BoxList):\n    raise ValueError('boxlist must be a BoxList')\n  if not boxlist.has_field('scores'):\n    raise ValueError('input boxlist must have \\'scores\\' field')\n  scores = boxlist.get_field('scores')\n  if len(scores.shape) > 2:\n    raise ValueError('Scores should have rank 1 or 2')\n  if len(scores.shape) == 2 and scores.shape[1] != 1:\n    raise ValueError('Scores should have rank 1 or have shape '\n                     'consistent with [None, 1]')\n  high_score_indices = np.reshape(np.where(np.greater(scores, thresh)),\n                                  [-1]).astype(np.int32)\n  return gather(boxlist, high_score_indices)", "\n\ndef change_coordinate_frame(boxlist, window):\n  \"\"\"Change coordinate frame of the boxlist to be relative to window's frame.\n\n  Given a window of the form [ymin, xmin, ymax, xmax],\n  changes bounding box coordinates from boxlist to be relative to this window\n  (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n\n  An example use case is data augmentation: where we are given groundtruth\n  boxes (boxlist) and would like to randomly crop the image to some\n  window (window). In this case we need to change the coordinate frame of\n  each groundtruth box to be relative to this new window.\n\n  Args:\n    boxlist: A BoxList object holding N boxes.\n    window: a size 4 1-D numpy array.\n\n  Returns:\n    Returns a BoxList object with N boxes.\n  \"\"\"\n  win_height = window[2] - window[0]\n  win_width = window[3] - window[1]\n  boxlist_new = scale(\n      np_box_list.BoxList(boxlist.get() -\n                          [window[0], window[1], window[0], window[1]]),\n      1.0 / win_height, 1.0 / win_width)\n  _copy_extra_fields(boxlist_new, boxlist)\n\n  return boxlist_new", "\n\ndef _copy_extra_fields(boxlist_to_copy_to, boxlist_to_copy_from):\n  \"\"\"Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to.\n\n  Args:\n    boxlist_to_copy_to: BoxList to which extra fields are copied.\n    boxlist_to_copy_from: BoxList from which fields are copied.\n\n  Returns:\n    boxlist_to_copy_to with extra fields.\n  \"\"\"\n  for field in boxlist_to_copy_from.get_extra_fields():\n    boxlist_to_copy_to.add_field(field, boxlist_to_copy_from.get_field(field))\n  return boxlist_to_copy_to", "\n\ndef _update_valid_indices_by_removing_high_iou_boxes(\n    selected_indices, is_index_valid, intersect_over_union, threshold):\n  max_iou = np.max(intersect_over_union[:, selected_indices], axis=1)\n  return np.logical_and(is_index_valid, max_iou <= threshold)\n"]}
{"filename": "gravit/utils/ava/per_image_evaluation.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Evaluate Object Detection result on a single image.\n\nAnnotate each detected result as true positives or false positive according to\na predefined IOU ratio. Non Maximum Supression is used by default. Multi class", "Annotate each detected result as true positives or false positive according to\na predefined IOU ratio. Non Maximum Supression is used by default. Multi class\ndetection is supported by default.\nBased on the settings, per image evaluation is either performed on boxes or\non object masks.\n\"\"\"\nimport numpy as np\n\nfrom . import np_box_list\nfrom . import np_box_list_ops", "from . import np_box_list\nfrom . import np_box_list_ops\nfrom . import np_box_mask_list\nfrom . import np_box_mask_list_ops\n\n\nclass PerImageEvaluation(object):\n  \"\"\"Evaluate detection result of a single image.\"\"\"\n\n  def __init__(self,\n               num_groundtruth_classes,\n               matching_iou_threshold=0.5):\n    \"\"\"Initialized PerImageEvaluation by evaluation parameters.\n\n    Args:\n      num_groundtruth_classes: Number of ground truth object classes\n      matching_iou_threshold: A ratio of area intersection to union, which is\n          the threshold to consider whether a detection is true positive or not\n    \"\"\"\n    self.matching_iou_threshold = matching_iou_threshold\n    self.num_groundtruth_classes = num_groundtruth_classes\n\n  def compute_object_detection_metrics(\n      self, detected_boxes, detected_scores, detected_class_labels,\n      groundtruth_boxes, groundtruth_class_labels,\n      groundtruth_is_difficult_list, groundtruth_is_group_of_list,\n      detected_masks=None, groundtruth_masks=None):\n    \"\"\"Evaluates detections as being tp, fp or ignored from a single image.\n\n    The evaluation is done in two stages:\n     1. All detections are matched to non group-of boxes; true positives are\n        determined and detections matched to difficult boxes are ignored.\n     2. Detections that are determined as false positives are matched against\n        group-of boxes and ignored if matched.\n\n    Args:\n      detected_boxes: A float numpy array of shape [N, 4], representing N\n          regions of detected object regions.\n          Each row is of the format [y_min, x_min, y_max, x_max]\n      detected_scores: A float numpy array of shape [N, 1], representing\n          the confidence scores of the detected N object instances.\n      detected_class_labels: A integer numpy array of shape [N, 1], repreneting\n          the class labels of the detected N object instances.\n      groundtruth_boxes: A float numpy array of shape [M, 4], representing M\n          regions of object instances in ground truth\n      groundtruth_class_labels: An integer numpy array of shape [M, 1],\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag\n      detected_masks: (optional) A uint8 numpy array of shape\n        [N, height, width]. If not None, the metrics will be computed based\n        on masks.\n      groundtruth_masks: (optional) A uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      scores: A list of C float numpy arrays. Each numpy array is of\n          shape [K, 1], representing K scores detected with object class\n          label c\n      tp_fp_labels: A list of C boolean numpy arrays. Each numpy array\n          is of shape [K, 1], representing K True/False positive label of\n          object instances detected with class label c\n    \"\"\"\n    detected_boxes, detected_scores, detected_class_labels, detected_masks = (\n        self._remove_invalid_boxes(detected_boxes, detected_scores,\n                                   detected_class_labels, detected_masks))\n    scores, tp_fp_labels = self._compute_tp_fp(\n        detected_boxes=detected_boxes,\n        detected_scores=detected_scores,\n        detected_class_labels=detected_class_labels,\n        groundtruth_boxes=groundtruth_boxes,\n        groundtruth_class_labels=groundtruth_class_labels,\n        groundtruth_is_difficult_list=groundtruth_is_difficult_list,\n        groundtruth_is_group_of_list=groundtruth_is_group_of_list,\n        detected_masks=detected_masks,\n        groundtruth_masks=groundtruth_masks)\n\n    return scores, tp_fp_labels\n\n  def _compute_tp_fp(self, detected_boxes, detected_scores,\n                     detected_class_labels, groundtruth_boxes,\n                     groundtruth_class_labels, groundtruth_is_difficult_list,\n                     groundtruth_is_group_of_list,\n                     detected_masks=None, groundtruth_masks=None):\n    \"\"\"Labels true/false positives of detections of an image across all classes.\n\n    Args:\n      detected_boxes: A float numpy array of shape [N, 4], representing N\n          regions of detected object regions.\n          Each row is of the format [y_min, x_min, y_max, x_max]\n      detected_scores: A float numpy array of shape [N, 1], representing\n          the confidence scores of the detected N object instances.\n      detected_class_labels: A integer numpy array of shape [N, 1], repreneting\n          the class labels of the detected N object instances.\n      groundtruth_boxes: A float numpy array of shape [M, 4], representing M\n          regions of object instances in ground truth\n      groundtruth_class_labels: An integer numpy array of shape [M, 1],\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag\n      detected_masks: (optional) A np.uint8 numpy array of shape\n        [N, height, width]. If not None, the scores will be computed based\n        on masks.\n      groundtruth_masks: (optional) A np.uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      result_scores: A list of float numpy arrays. Each numpy array is of\n          shape [K, 1], representing K scores detected with object class\n          label c\n      result_tp_fp_labels: A list of boolean numpy array. Each numpy array is of\n          shape [K, 1], representing K True/False positive label of object\n          instances detected with class label c\n\n    Raises:\n      ValueError: If detected masks is not None but groundtruth masks are None,\n        or the other way around.\n    \"\"\"\n    if detected_masks is not None and groundtruth_masks is None:\n      raise ValueError(\n          'Detected masks is available but groundtruth masks is not.')\n    if detected_masks is None and groundtruth_masks is not None:\n      raise ValueError(\n          'Groundtruth masks is available but detected masks is not.')\n\n    result_scores = []\n    result_tp_fp_labels = []\n    for i in range(self.num_groundtruth_classes):\n      groundtruth_is_difficult_list_at_ith_class = (\n          groundtruth_is_difficult_list[groundtruth_class_labels == i])\n      groundtruth_is_group_of_list_at_ith_class = (\n          groundtruth_is_group_of_list[groundtruth_class_labels == i])\n      (gt_boxes_at_ith_class, gt_masks_at_ith_class,\n       detected_boxes_at_ith_class, detected_scores_at_ith_class,\n       detected_masks_at_ith_class) = self._get_ith_class_arrays(\n           detected_boxes, detected_scores, detected_masks,\n           detected_class_labels, groundtruth_boxes, groundtruth_masks,\n           groundtruth_class_labels, i)\n      scores, tp_fp_labels = self._compute_tp_fp_for_single_class(\n          detected_boxes=detected_boxes_at_ith_class,\n          detected_scores=detected_scores_at_ith_class,\n          groundtruth_boxes=gt_boxes_at_ith_class,\n          groundtruth_is_difficult_list=\n          groundtruth_is_difficult_list_at_ith_class,\n          groundtruth_is_group_of_list=\n          groundtruth_is_group_of_list_at_ith_class,\n          detected_masks=detected_masks_at_ith_class,\n          groundtruth_masks=gt_masks_at_ith_class)\n      result_scores.append(scores)\n      result_tp_fp_labels.append(tp_fp_labels)\n    return result_scores, result_tp_fp_labels\n\n  def _get_overlaps_and_scores_box_mode(\n      self,\n      detected_boxes,\n      detected_scores,\n      groundtruth_boxes,\n      groundtruth_is_group_of_list):\n    \"\"\"Computes overlaps and scores between detected and groudntruth boxes.\n\n    Args:\n      detected_boxes: A numpy array of shape [N, 4] representing detected box\n          coordinates\n      detected_scores: A 1-d numpy array of length N representing classification\n          score\n      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth\n          box coordinates\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag. If a groundtruth box\n          is group-of box, every detection matching this box is ignored.\n\n    Returns:\n      iou: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If\n          gt_non_group_of_boxlist.num_boxes() == 0 it will be None.\n      ioa: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If\n          gt_group_of_boxlist.num_boxes() == 0 it will be None.\n      scores: The score of the detected boxlist.\n      num_boxes: Number of non-maximum suppressed detected boxes.\n    \"\"\"\n    detected_boxlist = np_box_list.BoxList(detected_boxes)\n    detected_boxlist.add_field('scores', detected_scores)\n    gt_non_group_of_boxlist = np_box_list.BoxList(\n        groundtruth_boxes[~groundtruth_is_group_of_list])\n    iou = np_box_list_ops.iou(detected_boxlist, gt_non_group_of_boxlist)\n    scores = detected_boxlist.get_field('scores')\n    num_boxes = detected_boxlist.num_boxes()\n    return iou, None, scores, num_boxes\n\n  def _compute_tp_fp_for_single_class(\n      self, detected_boxes, detected_scores, groundtruth_boxes,\n      groundtruth_is_difficult_list, groundtruth_is_group_of_list,\n      detected_masks=None, groundtruth_masks=None):\n    \"\"\"Labels boxes detected with the same class from the same image as tp/fp.\n\n    Args:\n      detected_boxes: A numpy array of shape [N, 4] representing detected box\n          coordinates\n      detected_scores: A 1-d numpy array of length N representing classification\n          score\n      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth\n          box coordinates\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not. If a\n          groundtruth box is difficult, every detection matching this box\n          is ignored.\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box has group-of tag. If a groundtruth box\n          is group-of box, every detection matching this box is ignored.\n      detected_masks: (optional) A uint8 numpy array of shape\n        [N, height, width]. If not None, the scores will be computed based\n        on masks.\n      groundtruth_masks: (optional) A uint8 numpy array of shape\n        [M, height, width].\n\n    Returns:\n      Two arrays of the same size, containing all boxes that were evaluated as\n      being true positives or false positives; if a box matched to a difficult\n      box or to a group-of box, it is ignored.\n\n      scores: A numpy array representing the detection scores.\n      tp_fp_labels: a boolean numpy array indicating whether a detection is a\n          true positive.\n    \"\"\"\n    if detected_boxes.size == 0:\n      return np.array([], dtype=float), np.array([], dtype=bool)\n\n    (iou, _, scores,\n     num_detected_boxes) = self._get_overlaps_and_scores_box_mode(\n         detected_boxes=detected_boxes,\n         detected_scores=detected_scores,\n         groundtruth_boxes=groundtruth_boxes,\n         groundtruth_is_group_of_list=groundtruth_is_group_of_list)\n\n    if groundtruth_boxes.size == 0:\n      return scores, np.zeros(num_detected_boxes, dtype=bool)\n\n    tp_fp_labels = np.zeros(num_detected_boxes, dtype=bool)\n    is_matched_to_difficult_box = np.zeros(num_detected_boxes, dtype=bool)\n    is_matched_to_group_of_box = np.zeros(num_detected_boxes, dtype=bool)\n\n    # The evaluation is done in two stages:\n    # 1. All detections are matched to non group-of boxes; true positives are\n    #    determined and detections matched to difficult boxes are ignored.\n    # 2. Detections that are determined as false positives are matched against\n    #    group-of boxes and ignored if matched.\n\n    # Tp-fp evaluation for non-group of boxes (if any).\n    if iou.shape[1] > 0:\n      groundtruth_nongroup_of_is_difficult_list = groundtruth_is_difficult_list[\n          ~groundtruth_is_group_of_list]\n      max_overlap_gt_ids = np.argmax(iou, axis=1)\n      is_gt_box_detected = np.zeros(iou.shape[1], dtype=bool)\n      for i in range(num_detected_boxes):\n        gt_id = max_overlap_gt_ids[i]\n        if iou[i, gt_id] >= self.matching_iou_threshold:\n          if not groundtruth_nongroup_of_is_difficult_list[gt_id]:\n            if not is_gt_box_detected[gt_id]:\n              tp_fp_labels[i] = True\n              is_gt_box_detected[gt_id] = True\n          else:\n            is_matched_to_difficult_box[i] = True\n\n    return scores[~is_matched_to_difficult_box\n                  & ~is_matched_to_group_of_box], tp_fp_labels[\n                      ~is_matched_to_difficult_box\n                      & ~is_matched_to_group_of_box]\n\n  def _get_ith_class_arrays(self, detected_boxes, detected_scores,\n                            detected_masks, detected_class_labels,\n                            groundtruth_boxes, groundtruth_masks,\n                            groundtruth_class_labels, class_index):\n    \"\"\"Returns numpy arrays belonging to class with index `class_index`.\n\n    Args:\n      detected_boxes: A numpy array containing detected boxes.\n      detected_scores: A numpy array containing detected scores.\n      detected_masks: A numpy array containing detected masks.\n      detected_class_labels: A numpy array containing detected class labels.\n      groundtruth_boxes: A numpy array containing groundtruth boxes.\n      groundtruth_masks: A numpy array containing groundtruth masks.\n      groundtruth_class_labels: A numpy array containing groundtruth class\n        labels.\n      class_index: An integer index.\n\n    Returns:\n      gt_boxes_at_ith_class: A numpy array containing groundtruth boxes labeled\n        as ith class.\n      gt_masks_at_ith_class: A numpy array containing groundtruth masks labeled\n        as ith class.\n      detected_boxes_at_ith_class: A numpy array containing detected boxes\n        corresponding to the ith class.\n      detected_scores_at_ith_class: A numpy array containing detected scores\n        corresponding to the ith class.\n      detected_masks_at_ith_class: A numpy array containing detected masks\n        corresponding to the ith class.\n    \"\"\"\n    selected_groundtruth = (groundtruth_class_labels == class_index)\n    gt_boxes_at_ith_class = groundtruth_boxes[selected_groundtruth]\n    if groundtruth_masks is not None:\n      gt_masks_at_ith_class = groundtruth_masks[selected_groundtruth]\n    else:\n      gt_masks_at_ith_class = None\n    selected_detections = (detected_class_labels == class_index)\n    detected_boxes_at_ith_class = detected_boxes[selected_detections]\n    detected_scores_at_ith_class = detected_scores[selected_detections]\n    if detected_masks is not None:\n      detected_masks_at_ith_class = detected_masks[selected_detections]\n    else:\n      detected_masks_at_ith_class = None\n    return (gt_boxes_at_ith_class, gt_masks_at_ith_class,\n            detected_boxes_at_ith_class, detected_scores_at_ith_class,\n            detected_masks_at_ith_class)\n\n  def _remove_invalid_boxes(self, detected_boxes, detected_scores,\n                            detected_class_labels, detected_masks=None):\n    \"\"\"Removes entries with invalid boxes.\n\n    A box is invalid if either its xmax is smaller than its xmin, or its ymax\n    is smaller than its ymin.\n\n    Args:\n      detected_boxes: A float numpy array of size [num_boxes, 4] containing box\n        coordinates in [ymin, xmin, ymax, xmax] format.\n      detected_scores: A float numpy array of size [num_boxes].\n      detected_class_labels: A int32 numpy array of size [num_boxes].\n      detected_masks: A uint8 numpy array of size [num_boxes, height, width].\n\n    Returns:\n      valid_detected_boxes: A float numpy array of size [num_valid_boxes, 4]\n        containing box coordinates in [ymin, xmin, ymax, xmax] format.\n      valid_detected_scores: A float numpy array of size [num_valid_boxes].\n      valid_detected_class_labels: A int32 numpy array of size\n        [num_valid_boxes].\n      valid_detected_masks: A uint8 numpy array of size\n        [num_valid_boxes, height, width].\n    \"\"\"\n    valid_indices = np.logical_and(detected_boxes[:, 0] < detected_boxes[:, 2],\n                                   detected_boxes[:, 1] < detected_boxes[:, 3])\n    detected_boxes = detected_boxes[valid_indices]\n    detected_scores = detected_scores[valid_indices]\n    detected_class_labels = detected_class_labels[valid_indices]\n    if detected_masks is not None:\n      detected_masks = detected_masks[valid_indices]\n    return [\n        detected_boxes, detected_scores, detected_class_labels, detected_masks\n    ]", ""]}
{"filename": "gravit/utils/ava/standard_fields.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Contains classes specifying naming conventions used for object detection.\n\n", "\n\nSpecifies:\n  InputDataFields: standard fields used by reader/preprocessor/batcher.\n  DetectionResultFields: standard fields returned by object detector.\n  BoxListFields: standard field used by BoxList\n  TfExampleFields: standard fields for tf-example data format (go/tf-example).\n\"\"\"\n\n\nclass InputDataFields(object):\n  \"\"\"Names for the input tensors.\n\n  Holds the standard data field names to use for identifying input tensors. This\n  should be used by the decoder to identify keys for the returned tensor_dict\n  containing input tensors. And it should be used by the model to identify the\n  tensors it needs.\n\n  Attributes:\n    image: image.\n    original_image: image in the original input size.\n    key: unique key corresponding to image.\n    source_id: source of the original image.\n    filename: original filename of the dataset (without common path).\n    groundtruth_image_classes: image-level class labels.\n    groundtruth_boxes: coordinates of the ground truth boxes in the image.\n    groundtruth_classes: box-level class labels.\n    groundtruth_label_types: box-level label types (e.g. explicit negative).\n    groundtruth_is_crowd: [DEPRECATED, use groundtruth_group_of instead]\n      is the groundtruth a single object or a crowd.\n    groundtruth_area: area of a groundtruth segment.\n    groundtruth_difficult: is a `difficult` object\n    groundtruth_group_of: is a `group_of` objects, e.g. multiple objects of the\n      same class, forming a connected group, where instances are heavily\n      occluding each other.\n    proposal_boxes: coordinates of object proposal boxes.\n    proposal_objectness: objectness score of each proposal.\n    groundtruth_instance_masks: ground truth instance masks.\n    groundtruth_instance_boundaries: ground truth instance boundaries.\n    groundtruth_instance_classes: instance mask-level class labels.\n    groundtruth_keypoints: ground truth keypoints.\n    groundtruth_keypoint_visibilities: ground truth keypoint visibilities.\n    groundtruth_label_scores: groundtruth label scores.\n    groundtruth_weights: groundtruth weight factor for bounding boxes.\n    num_groundtruth_boxes: number of groundtruth boxes.\n    true_image_shapes: true shapes of images in the resized images, as resized\n      images can be padded with zeros.\n  \"\"\"\n  image = 'image'\n  original_image = 'original_image'\n  key = 'key'\n  source_id = 'source_id'\n  filename = 'filename'\n  groundtruth_image_classes = 'groundtruth_image_classes'\n  groundtruth_boxes = 'groundtruth_boxes'\n  groundtruth_classes = 'groundtruth_classes'\n  groundtruth_label_types = 'groundtruth_label_types'\n  groundtruth_is_crowd = 'groundtruth_is_crowd'\n  groundtruth_area = 'groundtruth_area'\n  groundtruth_difficult = 'groundtruth_difficult'\n  groundtruth_group_of = 'groundtruth_group_of'\n  proposal_boxes = 'proposal_boxes'\n  proposal_objectness = 'proposal_objectness'\n  groundtruth_instance_masks = 'groundtruth_instance_masks'\n  groundtruth_instance_boundaries = 'groundtruth_instance_boundaries'\n  groundtruth_instance_classes = 'groundtruth_instance_classes'\n  groundtruth_keypoints = 'groundtruth_keypoints'\n  groundtruth_keypoint_visibilities = 'groundtruth_keypoint_visibilities'\n  groundtruth_label_scores = 'groundtruth_label_scores'\n  groundtruth_weights = 'groundtruth_weights'\n  num_groundtruth_boxes = 'num_groundtruth_boxes'\n  true_image_shape = 'true_image_shape'", "\n\nclass InputDataFields(object):\n  \"\"\"Names for the input tensors.\n\n  Holds the standard data field names to use for identifying input tensors. This\n  should be used by the decoder to identify keys for the returned tensor_dict\n  containing input tensors. And it should be used by the model to identify the\n  tensors it needs.\n\n  Attributes:\n    image: image.\n    original_image: image in the original input size.\n    key: unique key corresponding to image.\n    source_id: source of the original image.\n    filename: original filename of the dataset (without common path).\n    groundtruth_image_classes: image-level class labels.\n    groundtruth_boxes: coordinates of the ground truth boxes in the image.\n    groundtruth_classes: box-level class labels.\n    groundtruth_label_types: box-level label types (e.g. explicit negative).\n    groundtruth_is_crowd: [DEPRECATED, use groundtruth_group_of instead]\n      is the groundtruth a single object or a crowd.\n    groundtruth_area: area of a groundtruth segment.\n    groundtruth_difficult: is a `difficult` object\n    groundtruth_group_of: is a `group_of` objects, e.g. multiple objects of the\n      same class, forming a connected group, where instances are heavily\n      occluding each other.\n    proposal_boxes: coordinates of object proposal boxes.\n    proposal_objectness: objectness score of each proposal.\n    groundtruth_instance_masks: ground truth instance masks.\n    groundtruth_instance_boundaries: ground truth instance boundaries.\n    groundtruth_instance_classes: instance mask-level class labels.\n    groundtruth_keypoints: ground truth keypoints.\n    groundtruth_keypoint_visibilities: ground truth keypoint visibilities.\n    groundtruth_label_scores: groundtruth label scores.\n    groundtruth_weights: groundtruth weight factor for bounding boxes.\n    num_groundtruth_boxes: number of groundtruth boxes.\n    true_image_shapes: true shapes of images in the resized images, as resized\n      images can be padded with zeros.\n  \"\"\"\n  image = 'image'\n  original_image = 'original_image'\n  key = 'key'\n  source_id = 'source_id'\n  filename = 'filename'\n  groundtruth_image_classes = 'groundtruth_image_classes'\n  groundtruth_boxes = 'groundtruth_boxes'\n  groundtruth_classes = 'groundtruth_classes'\n  groundtruth_label_types = 'groundtruth_label_types'\n  groundtruth_is_crowd = 'groundtruth_is_crowd'\n  groundtruth_area = 'groundtruth_area'\n  groundtruth_difficult = 'groundtruth_difficult'\n  groundtruth_group_of = 'groundtruth_group_of'\n  proposal_boxes = 'proposal_boxes'\n  proposal_objectness = 'proposal_objectness'\n  groundtruth_instance_masks = 'groundtruth_instance_masks'\n  groundtruth_instance_boundaries = 'groundtruth_instance_boundaries'\n  groundtruth_instance_classes = 'groundtruth_instance_classes'\n  groundtruth_keypoints = 'groundtruth_keypoints'\n  groundtruth_keypoint_visibilities = 'groundtruth_keypoint_visibilities'\n  groundtruth_label_scores = 'groundtruth_label_scores'\n  groundtruth_weights = 'groundtruth_weights'\n  num_groundtruth_boxes = 'num_groundtruth_boxes'\n  true_image_shape = 'true_image_shape'", "\n\nclass DetectionResultFields(object):\n  \"\"\"Naming conventions for storing the output of the detector.\n\n  Attributes:\n    source_id: source of the original image.\n    key: unique key corresponding to image.\n    detection_boxes: coordinates of the detection boxes in the image.\n    detection_scores: detection scores for the detection boxes in the image.\n    detection_classes: detection-level class labels.\n    detection_masks: contains a segmentation mask for each detection box.\n    detection_boundaries: contains an object boundary for each detection box.\n    detection_keypoints: contains detection keypoints for each detection box.\n    num_detections: number of detections in the batch.\n  \"\"\"\n\n  source_id = 'source_id'\n  key = 'key'\n  detection_boxes = 'detection_boxes'\n  detection_scores = 'detection_scores'\n  detection_classes = 'detection_classes'\n  detection_masks = 'detection_masks'\n  detection_boundaries = 'detection_boundaries'\n  detection_keypoints = 'detection_keypoints'\n  num_detections = 'num_detections'", "\n\nclass BoxListFields(object):\n  \"\"\"Naming conventions for BoxLists.\n\n  Attributes:\n    boxes: bounding box coordinates.\n    classes: classes per bounding box.\n    scores: scores per bounding box.\n    weights: sample weights per bounding box.\n    objectness: objectness score per bounding box.\n    masks: masks per bounding box.\n    boundaries: boundaries per bounding box.\n    keypoints: keypoints per bounding box.\n    keypoint_heatmaps: keypoint heatmaps per bounding box.\n  \"\"\"\n  boxes = 'boxes'\n  classes = 'classes'\n  scores = 'scores'\n  weights = 'weights'\n  objectness = 'objectness'\n  masks = 'masks'\n  boundaries = 'boundaries'\n  keypoints = 'keypoints'\n  keypoint_heatmaps = 'keypoint_heatmaps'", "\n\nclass TfExampleFields(object):\n  \"\"\"TF-example proto feature names for object detection.\n\n  Holds the standard feature names to load from an Example proto for object\n  detection.\n\n  Attributes:\n    image_encoded: JPEG encoded string\n    image_format: image format, e.g. \"JPEG\"\n    filename: filename\n    channels: number of channels of image\n    colorspace: colorspace, e.g. \"RGB\"\n    height: height of image in pixels, e.g. 462\n    width: width of image in pixels, e.g. 581\n    source_id: original source of the image\n    object_class_text: labels in text format, e.g. [\"person\", \"cat\"]\n    object_class_label: labels in numbers, e.g. [16, 8]\n    object_bbox_xmin: xmin coordinates of groundtruth box, e.g. 10, 30\n    object_bbox_xmax: xmax coordinates of groundtruth box, e.g. 50, 40\n    object_bbox_ymin: ymin coordinates of groundtruth box, e.g. 40, 50\n    object_bbox_ymax: ymax coordinates of groundtruth box, e.g. 80, 70\n    object_view: viewpoint of object, e.g. [\"frontal\", \"left\"]\n    object_truncated: is object truncated, e.g. [true, false]\n    object_occluded: is object occluded, e.g. [true, false]\n    object_difficult: is object difficult, e.g. [true, false]\n    object_group_of: is object a single object or a group of objects\n    object_depiction: is object a depiction\n    object_is_crowd: [DEPRECATED, use object_group_of instead]\n      is the object a single object or a crowd\n    object_segment_area: the area of the segment.\n    object_weight: a weight factor for the object's bounding box.\n    instance_masks: instance segmentation masks.\n    instance_boundaries: instance boundaries.\n    instance_classes: Classes for each instance segmentation mask.\n    detection_class_label: class label in numbers.\n    detection_bbox_ymin: ymin coordinates of a detection box.\n    detection_bbox_xmin: xmin coordinates of a detection box.\n    detection_bbox_ymax: ymax coordinates of a detection box.\n    detection_bbox_xmax: xmax coordinates of a detection box.\n    detection_score: detection score for the class label and box.\n  \"\"\"\n  image_encoded = 'image/encoded'\n  image_format = 'image/format'  # format is reserved keyword\n  filename = 'image/filename'\n  channels = 'image/channels'\n  colorspace = 'image/colorspace'\n  height = 'image/height'\n  width = 'image/width'\n  source_id = 'image/source_id'\n  object_class_text = 'image/object/class/text'\n  object_class_label = 'image/object/class/label'\n  object_bbox_ymin = 'image/object/bbox/ymin'\n  object_bbox_xmin = 'image/object/bbox/xmin'\n  object_bbox_ymax = 'image/object/bbox/ymax'\n  object_bbox_xmax = 'image/object/bbox/xmax'\n  object_view = 'image/object/view'\n  object_truncated = 'image/object/truncated'\n  object_occluded = 'image/object/occluded'\n  object_difficult = 'image/object/difficult'\n  object_group_of = 'image/object/group_of'\n  object_depiction = 'image/object/depiction'\n  object_is_crowd = 'image/object/is_crowd'\n  object_segment_area = 'image/object/segment/area'\n  object_weight = 'image/object/weight'\n  instance_masks = 'image/segmentation/object'\n  instance_boundaries = 'image/boundaries/object'\n  instance_classes = 'image/segmentation/object/class'\n  detection_class_label = 'image/detection/label'\n  detection_bbox_ymin = 'image/detection/bbox/ymin'\n  detection_bbox_xmin = 'image/detection/bbox/xmin'\n  detection_bbox_ymax = 'image/detection/bbox/ymax'\n  detection_bbox_xmax = 'image/detection/bbox/xmax'\n  detection_score = 'image/detection/score'", ""]}
{"filename": "gravit/utils/ava/label_map_util.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Label map utility functions.\"\"\"\n\nimport logging\n", "import logging\n\n# from google.protobuf import text_format\n# from google3.third_party.tensorflow_models.object_detection.protos import string_int_label_map_pb2\n\n\ndef _validate_label_map(label_map):\n  \"\"\"Checks if a label map is valid.\n\n  Args:\n    label_map: StringIntLabelMap to validate.\n\n  Raises:\n    ValueError: if label map is invalid.\n  \"\"\"\n  for item in label_map.item:\n    if item.id < 1:\n      raise ValueError('Label map ids should be >= 1.')", "\n\ndef create_category_index(categories):\n  \"\"\"Creates dictionary of COCO compatible categories keyed by category id.\n\n  Args:\n    categories: a list of dicts, each of which has the following keys:\n      'id': (required) an integer id uniquely identifying this category.\n      'name': (required) string representing category name\n        e.g., 'cat', 'dog', 'pizza'.\n\n  Returns:\n    category_index: a dict containing the same entries as categories, but keyed\n      by the 'id' field of each category.\n  \"\"\"\n  category_index = {}\n  for cat in categories:\n    category_index[cat['id']] = cat\n  return category_index", "\n\ndef get_max_label_map_index(label_map):\n  \"\"\"Get maximum index in label map.\n\n  Args:\n    label_map: a StringIntLabelMapProto\n\n  Returns:\n    an integer\n  \"\"\"\n  return max([item.id for item in label_map.item])", "\n\ndef convert_label_map_to_categories(label_map,\n                                    max_num_classes,\n                                    use_display_name=True):\n  \"\"\"Loads label map proto and returns categories list compatible with eval.\n\n  This function loads a label map and returns a list of dicts, each of which\n  has the following keys:\n    'id': (required) an integer id uniquely identifying this category.\n    'name': (required) string representing category name\n      e.g., 'cat', 'dog', 'pizza'.\n  We only allow class into the list if its id-label_id_offset is\n  between 0 (inclusive) and max_num_classes (exclusive).\n  If there are several items mapping to the same id in the label map,\n  we will only keep the first one in the categories list.\n\n  Args:\n    label_map: a StringIntLabelMapProto or None.  If None, a default categories\n      list is created with max_num_classes categories.\n    max_num_classes: maximum number of (consecutive) label indices to include.\n    use_display_name: (boolean) choose whether to load 'display_name' field\n      as category name.  If False or if the display_name field does not exist,\n      uses 'name' field as category names instead.\n  Returns:\n    categories: a list of dictionaries representing all possible categories.\n  \"\"\"\n  categories = []\n  list_of_ids_already_added = []\n  if not label_map:\n    label_id_offset = 1\n    for class_id in range(max_num_classes):\n      categories.append({\n          'id': class_id + label_id_offset,\n          'name': 'category_{}'.format(class_id + label_id_offset)\n      })\n    return categories\n  for item in label_map.item:\n    if not 0 < item.id <= max_num_classes:\n      logging.info('Ignore item %d since it falls outside of requested '\n                   'label range.', item.id)\n      continue\n    if use_display_name and item.HasField('display_name'):\n      name = item.display_name\n    else:\n      name = item.name\n    if item.id not in list_of_ids_already_added:\n      list_of_ids_already_added.append(item.id)\n      categories.append({'id': item.id, 'name': name})\n  return categories", "\n\ndef load_labelmap(path):\n  \"\"\"Loads label map proto.\n\n  Args:\n    path: path to StringIntLabelMap proto text file.\n  Returns:\n    a StringIntLabelMapProto\n  \"\"\"\n  with open(path, 'r') as fid:\n    label_map_string = fid.read()\n    label_map = string_int_label_map_pb2.StringIntLabelMap()\n    try:\n      text_format.Merge(label_map_string, label_map)\n    except text_format.ParseError:\n      label_map.ParseFromString(label_map_string)\n  _validate_label_map(label_map)\n  return label_map", "\n\ndef get_label_map_dict(label_map_path, use_display_name=False):\n  \"\"\"Reads a label map and returns a dictionary of label names to id.\n\n  Args:\n    label_map_path: path to label_map.\n    use_display_name: whether to use the label map items' display names as keys.\n\n  Returns:\n    A dictionary mapping label names to id.\n  \"\"\"\n  label_map = load_labelmap(label_map_path)\n  label_map_dict = {}\n  for item in label_map.item:\n    if use_display_name:\n      label_map_dict[item.display_name] = item.id\n    else:\n      label_map_dict[item.name] = item.id\n  return label_map_dict", "\n\ndef create_category_index_from_labelmap(label_map_path):\n  \"\"\"Reads a label map and returns a category index.\n\n  Args:\n    label_map_path: Path to `StringIntLabelMap` proto text file.\n\n  Returns:\n    A category index, which is a dictionary that maps integer ids to dicts\n    containing categories, e.g.\n    {1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}, ...}\n  \"\"\"\n  label_map = load_labelmap(label_map_path)\n  max_num_classes = max(item.id for item in label_map.item)\n  categories = convert_label_map_to_categories(label_map, max_num_classes)\n  return create_category_index(categories)", "\n\ndef create_class_agnostic_category_index():\n  \"\"\"Creates a category index with a single `object` class.\"\"\"\n  return {1: {'id': 1, 'name': 'object'}}\n"]}
{"filename": "gravit/utils/ava/object_detection_evaluation.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"object_detection_evaluation module.\n\nObjectDetectionEvaluation is a class which manages ground truth information of a\nobject detection dataset, and computes frequently used detection metrics such as", "ObjectDetectionEvaluation is a class which manages ground truth information of a\nobject detection dataset, and computes frequently used detection metrics such as\nPrecision, Recall, CorLoc of the provided detection results.\nIt supports the following operations:\n1) Add ground truth information of images sequentially.\n2) Add detection result of images sequentially.\n3) Evaluate detection metrics on already inserted detection results.\n4) Write evaluation result into a pickle file for future processing or\n   visualization.\n", "   visualization.\n\nNote: This module operates on numpy boxes and box lists.\n\"\"\"\n\nfrom abc import ABCMeta\nfrom abc import abstractmethod\nimport collections\nimport logging\nimport numpy as np", "import logging\nimport numpy as np\n\nfrom . import standard_fields\nfrom . import label_map_util\nfrom . import metrics\nfrom . import per_image_evaluation\n\n\nclass DetectionEvaluator(object):\n  \"\"\"Interface for object detection evalution classes.\n\n  Example usage of the Evaluator:\n  ------------------------------\n  evaluator = DetectionEvaluator(categories)\n\n  # Detections and groundtruth for image 1.\n  evaluator.add_single_groundtruth_image_info(...)\n  evaluator.add_single_detected_image_info(...)\n\n  # Detections and groundtruth for image 2.\n  evaluator.add_single_groundtruth_image_info(...)\n  evaluator.add_single_detected_image_info(...)\n\n  metrics_dict = evaluator.evaluate()\n  \"\"\"\n  __metaclass__ = ABCMeta\n\n  def __init__(self, categories):\n    \"\"\"Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n    \"\"\"\n    self._categories = categories\n\n  @abstractmethod\n  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary of groundtruth numpy arrays required\n        for evaluations.\n    \"\"\"\n    pass\n\n  @abstractmethod\n  def add_single_detected_image_info(self, image_id, detections_dict):\n    \"\"\"Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary of detection numpy arrays required\n        for evaluation.\n    \"\"\"\n    pass\n\n  @abstractmethod\n  def evaluate(self):\n    \"\"\"Evaluates detections and returns a dictionary of metrics.\"\"\"\n    pass\n\n  @abstractmethod\n  def clear(self):\n    \"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"\n    pass", "\nclass DetectionEvaluator(object):\n  \"\"\"Interface for object detection evalution classes.\n\n  Example usage of the Evaluator:\n  ------------------------------\n  evaluator = DetectionEvaluator(categories)\n\n  # Detections and groundtruth for image 1.\n  evaluator.add_single_groundtruth_image_info(...)\n  evaluator.add_single_detected_image_info(...)\n\n  # Detections and groundtruth for image 2.\n  evaluator.add_single_groundtruth_image_info(...)\n  evaluator.add_single_detected_image_info(...)\n\n  metrics_dict = evaluator.evaluate()\n  \"\"\"\n  __metaclass__ = ABCMeta\n\n  def __init__(self, categories):\n    \"\"\"Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n    \"\"\"\n    self._categories = categories\n\n  @abstractmethod\n  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary of groundtruth numpy arrays required\n        for evaluations.\n    \"\"\"\n    pass\n\n  @abstractmethod\n  def add_single_detected_image_info(self, image_id, detections_dict):\n    \"\"\"Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary of detection numpy arrays required\n        for evaluation.\n    \"\"\"\n    pass\n\n  @abstractmethod\n  def evaluate(self):\n    \"\"\"Evaluates detections and returns a dictionary of metrics.\"\"\"\n    pass\n\n  @abstractmethod\n  def clear(self):\n    \"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"\n    pass", "\n\nclass ObjectDetectionEvaluator(DetectionEvaluator):\n  \"\"\"A class to evaluate detections.\"\"\"\n\n  def __init__(self,\n               categories,\n               matching_iou_threshold=0.5,\n               evaluate_corlocs=False,\n               metric_prefix=None,\n               use_weighted_mean_ap=False,\n               evaluate_masks=False):\n    \"\"\"Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n      matching_iou_threshold: IOU threshold to use for matching groundtruth\n        boxes to detection boxes.\n      evaluate_corlocs: (optional) boolean which determines if corloc scores\n        are to be returned or not.\n      metric_prefix: (optional) string prefix for metric name; if None, no\n        prefix is used.\n      use_weighted_mean_ap: (optional) boolean which determines if the mean\n        average precision is computed directly from the scores and tp_fp_labels\n        of all classes.\n      evaluate_masks: If False, evaluation will be performed based on boxes.\n        If True, mask evaluation will be performed instead.\n\n    Raises:\n      ValueError: If the category ids are not 1-indexed.\n    \"\"\"\n    super(ObjectDetectionEvaluator, self).__init__(categories)\n    self._num_classes = max([cat['id'] for cat in categories])\n    if min(cat['id'] for cat in categories) < 1:\n      raise ValueError('Classes should be 1-indexed.')\n    self._matching_iou_threshold = matching_iou_threshold\n    self._use_weighted_mean_ap = use_weighted_mean_ap\n    self._label_id_offset = 1\n    self._evaluate_masks = evaluate_masks\n    self._evaluation = ObjectDetectionEvaluation(\n        num_groundtruth_classes=self._num_classes,\n        matching_iou_threshold=self._matching_iou_threshold,\n        use_weighted_mean_ap=self._use_weighted_mean_ap,\n        label_id_offset=self._label_id_offset)\n    self._image_ids = set([])\n    self._evaluate_corlocs = evaluate_corlocs\n    self._metric_prefix = (metric_prefix + '_') if metric_prefix else ''\n\n  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary containing -\n        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array\n          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of\n          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.InputDataFields.groundtruth_classes: integer numpy array\n          of shape [num_boxes] containing 1-indexed groundtruth classes for the\n          boxes.\n        standard_fields.InputDataFields.groundtruth_difficult: Optional length\n          M numpy boolean array denoting whether a ground truth box is a\n          difficult instance or not. This field is optional to support the case\n          that no boxes are difficult.\n        standard_fields.InputDataFields.groundtruth_instance_masks: Optional\n          numpy array of shape [num_boxes, height, width] with values in {0, 1}.\n\n    Raises:\n      ValueError: On adding groundtruth for an image more than once. Will also\n        raise error if instance masks are not in groundtruth dictionary.\n    \"\"\"\n    if image_id in self._image_ids:\n      raise ValueError('Image with id {} already added.'.format(image_id))\n\n    groundtruth_classes = (\n        groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes] -\n        self._label_id_offset)\n    # If the key is not present in the groundtruth_dict or the array is empty\n    # (unless there are no annotations for the groundtruth on this image)\n    # use values from the dictionary or insert None otherwise.\n    if (standard_fields.InputDataFields.groundtruth_difficult in\n        groundtruth_dict.keys() and\n        (groundtruth_dict[standard_fields.InputDataFields.groundtruth_difficult]\n         .size or not groundtruth_classes.size)):\n      groundtruth_difficult = groundtruth_dict[\n          standard_fields.InputDataFields.groundtruth_difficult]\n    else:\n      groundtruth_difficult = None\n      if not len(self._image_ids) % 1000:\n        logging.warn(\n            'image %s does not have groundtruth difficult flag specified',\n            image_id)\n    groundtruth_masks = None\n    if self._evaluate_masks:\n      if (standard_fields.InputDataFields.groundtruth_instance_masks not in\n          groundtruth_dict):\n        raise ValueError('Instance masks not in groundtruth dictionary.')\n      groundtruth_masks = groundtruth_dict[\n          standard_fields.InputDataFields.groundtruth_instance_masks]\n    self._evaluation.add_single_ground_truth_image_info(\n        image_key=image_id,\n        groundtruth_boxes=groundtruth_dict[\n            standard_fields.InputDataFields.groundtruth_boxes],\n        groundtruth_class_labels=groundtruth_classes,\n        groundtruth_is_difficult_list=groundtruth_difficult,\n        groundtruth_masks=groundtruth_masks)\n    self._image_ids.update([image_id])\n\n  def add_single_detected_image_info(self, image_id, detections_dict):\n    \"\"\"Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      detections_dict: A dictionary containing -\n        standard_fields.DetectionResultFields.detection_boxes: float32 numpy\n          array of shape [num_boxes, 4] containing `num_boxes` detection boxes\n          of the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.DetectionResultFields.detection_scores: float32 numpy\n          array of shape [num_boxes] containing detection scores for the boxes.\n        standard_fields.DetectionResultFields.detection_classes: integer numpy\n          array of shape [num_boxes] containing 1-indexed detection classes for\n          the boxes.\n        standard_fields.DetectionResultFields.detection_masks: uint8 numpy\n          array of shape [num_boxes, height, width] containing `num_boxes` masks\n          of values ranging between 0 and 1.\n\n    Raises:\n      ValueError: If detection masks are not in detections dictionary.\n    \"\"\"\n    detection_classes = (\n        detections_dict[standard_fields.DetectionResultFields.detection_classes]\n        - self._label_id_offset)\n    detection_masks = None\n    if self._evaluate_masks:\n      if (standard_fields.DetectionResultFields.detection_masks not in\n          detections_dict):\n        raise ValueError('Detection masks not in detections dictionary.')\n      detection_masks = detections_dict[\n          standard_fields.DetectionResultFields.detection_masks]\n    self._evaluation.add_single_detected_image_info(\n        image_key=image_id,\n        detected_boxes=detections_dict[\n            standard_fields.DetectionResultFields.detection_boxes],\n        detected_scores=detections_dict[\n            standard_fields.DetectionResultFields.detection_scores],\n        detected_class_labels=detection_classes,\n        detected_masks=detection_masks)\n\n  def evaluate(self):\n    \"\"\"Compute evaluation result.\n\n    Returns:\n      A dictionary of metrics with the following fields -\n\n      1. summary_metrics:\n        'Precision/mAP@<matching_iou_threshold>IOU': mean average precision at\n        the specified IOU threshold.\n\n      2. per_category_ap: category specific results with keys of the form\n        'PerformanceByCategory/mAP@<matching_iou_threshold>IOU/category'.\n    \"\"\"\n    (per_class_ap, mean_ap, _, _, per_class_corloc, mean_corloc) = (\n        self._evaluation.evaluate())\n    pascal_metrics = {\n        self._metric_prefix +\n        'Precision/mAP@{}IOU'.format(self._matching_iou_threshold):\n            mean_ap\n    }\n    if self._evaluate_corlocs:\n      pascal_metrics[self._metric_prefix + 'Precision/meanCorLoc@{}IOU'.format(\n          self._matching_iou_threshold)] = mean_corloc\n    category_index = label_map_util.create_category_index(self._categories)\n    for idx in range(per_class_ap.size):\n      if idx + self._label_id_offset in category_index:\n        display_name = (\n            self._metric_prefix + 'PerformanceByCategory/AP@{}IOU/{}'.format(\n                self._matching_iou_threshold,\n                category_index[idx + self._label_id_offset]['name']))\n        pascal_metrics[display_name] = per_class_ap[idx]\n\n        # Optionally add CorLoc metrics.classes\n        if self._evaluate_corlocs:\n          display_name = (\n              self._metric_prefix + 'PerformanceByCategory/CorLoc@{}IOU/{}'\n              .format(self._matching_iou_threshold,\n                      category_index[idx + self._label_id_offset]['name']))\n          pascal_metrics[display_name] = per_class_corloc[idx]\n\n    return pascal_metrics\n\n  def clear(self):\n    \"\"\"Clears the state to prepare for a fresh evaluation.\"\"\"\n    self._evaluation = ObjectDetectionEvaluation(\n        num_groundtruth_classes=self._num_classes,\n        matching_iou_threshold=self._matching_iou_threshold,\n        use_weighted_mean_ap=self._use_weighted_mean_ap,\n        label_id_offset=self._label_id_offset)\n    self._image_ids.clear()", "\n\nclass PascalDetectionEvaluator(ObjectDetectionEvaluator):\n  \"\"\"A class to evaluate detections using PASCAL metrics.\"\"\"\n\n  def __init__(self, categories, matching_iou_threshold=0.5):\n    super(PascalDetectionEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold=matching_iou_threshold,\n        evaluate_corlocs=False,\n        metric_prefix='PascalBoxes',\n        use_weighted_mean_ap=False)", "\n\nclass WeightedPascalDetectionEvaluator(ObjectDetectionEvaluator):\n  \"\"\"A class to evaluate detections using weighted PASCAL metrics.\n\n  Weighted PASCAL metrics computes the mean average precision as the average\n  precision given the scores and tp_fp_labels of all classes. In comparison,\n  PASCAL metrics computes the mean average precision as the mean of the\n  per-class average precisions.\n\n  This definition is very similar to the mean of the per-class average\n  precisions weighted by class frequency. However, they are typically not the\n  same as the average precision is not a linear function of the scores and\n  tp_fp_labels.\n  \"\"\"\n\n  def __init__(self, categories, matching_iou_threshold=0.5):\n    super(WeightedPascalDetectionEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold=matching_iou_threshold,\n        evaluate_corlocs=False,\n        metric_prefix='WeightedPascalBoxes',\n        use_weighted_mean_ap=True)", "\n\nclass PascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):\n  \"\"\"A class to evaluate instance masks using PASCAL metrics.\"\"\"\n\n  def __init__(self, categories, matching_iou_threshold=0.5):\n    super(PascalInstanceSegmentationEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold=matching_iou_threshold,\n        evaluate_corlocs=False,\n        metric_prefix='PascalMasks',\n        use_weighted_mean_ap=False,\n        evaluate_masks=True)", "\n\nclass WeightedPascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):\n  \"\"\"A class to evaluate instance masks using weighted PASCAL metrics.\n\n  Weighted PASCAL metrics computes the mean average precision as the average\n  precision given the scores and tp_fp_labels of all classes. In comparison,\n  PASCAL metrics computes the mean average precision as the mean of the\n  per-class average precisions.\n\n  This definition is very similar to the mean of the per-class average\n  precisions weighted by class frequency. However, they are typically not the\n  same as the average precision is not a linear function of the scores and\n  tp_fp_labels.\n  \"\"\"\n\n  def __init__(self, categories, matching_iou_threshold=0.5):\n    super(WeightedPascalInstanceSegmentationEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold=matching_iou_threshold,\n        evaluate_corlocs=False,\n        metric_prefix='WeightedPascalMasks',\n        use_weighted_mean_ap=True,\n        evaluate_masks=True)", "\n\nclass OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):\n  \"\"\"A class to evaluate detections using Open Images V2 metrics.\n\n    Open Images V2 introduce group_of type of bounding boxes and this metric\n    handles those boxes appropriately.\n  \"\"\"\n\n  def __init__(self,\n               categories,\n               matching_iou_threshold=0.5,\n               evaluate_corlocs=False):\n    \"\"\"Constructor.\n\n    Args:\n      categories: A list of dicts, each of which has the following keys -\n        'id': (required) an integer id uniquely identifying this category.\n        'name': (required) string representing category name e.g., 'cat', 'dog'.\n      matching_iou_threshold: IOU threshold to use for matching groundtruth\n        boxes to detection boxes.\n      evaluate_corlocs: if True, additionally evaluates and returns CorLoc.\n    \"\"\"\n    super(OpenImagesDetectionEvaluator, self).__init__(\n        categories,\n        matching_iou_threshold,\n        evaluate_corlocs,\n        metric_prefix='OpenImagesV2')\n\n  def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):\n    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_id: A unique string/integer identifier for the image.\n      groundtruth_dict: A dictionary containing -\n        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array\n          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of\n          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.\n        standard_fields.InputDataFields.groundtruth_classes: integer numpy array\n          of shape [num_boxes] containing 1-indexed groundtruth classes for the\n          boxes.\n        standard_fields.InputDataFields.groundtruth_group_of: Optional length\n          M numpy boolean array denoting whether a groundtruth box contains a\n          group of instances.\n\n    Raises:\n      ValueError: On adding groundtruth for an image more than once.\n    \"\"\"\n    if image_id in self._image_ids:\n      raise ValueError('Image with id {} already added.'.format(image_id))\n\n    groundtruth_classes = (\n        groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes] -\n        self._label_id_offset)\n    # If the key is not present in the groundtruth_dict or the array is empty\n    # (unless there are no annotations for the groundtruth on this image)\n    # use values from the dictionary or insert None otherwise.\n    if (standard_fields.InputDataFields.groundtruth_group_of in\n        groundtruth_dict.keys() and\n        (groundtruth_dict[standard_fields.InputDataFields.groundtruth_group_of]\n         .size or not groundtruth_classes.size)):\n      groundtruth_group_of = groundtruth_dict[\n          standard_fields.InputDataFields.groundtruth_group_of]\n    else:\n      groundtruth_group_of = None\n      if not len(self._image_ids) % 1000:\n        logging.warn(\n            'image %s does not have groundtruth group_of flag specified',\n            image_id)\n    self._evaluation.add_single_ground_truth_image_info(\n        image_id,\n        groundtruth_dict[standard_fields.InputDataFields.groundtruth_boxes],\n        groundtruth_classes,\n        groundtruth_is_difficult_list=None,\n        groundtruth_is_group_of_list=groundtruth_group_of)\n    self._image_ids.update([image_id])", "\n\nObjectDetectionEvalMetrics = collections.namedtuple(\n    'ObjectDetectionEvalMetrics', [\n        'average_precisions', 'mean_ap', 'precisions', 'recalls', 'corlocs',\n        'mean_corloc'\n    ])\n\n\nclass ObjectDetectionEvaluation(object):\n  \"\"\"Internal implementation of Pascal object detection metrics.\"\"\"\n\n  def __init__(self,\n               num_groundtruth_classes,\n               matching_iou_threshold=0.5,\n               nms_iou_threshold=1.0,\n               nms_max_output_boxes=10000,\n               use_weighted_mean_ap=False,\n               label_id_offset=0):\n    if num_groundtruth_classes < 1:\n      raise ValueError('Need at least 1 groundtruth class for evaluation.')\n\n    self.per_image_eval = per_image_evaluation.PerImageEvaluation(\n        num_groundtruth_classes=num_groundtruth_classes,\n        matching_iou_threshold=matching_iou_threshold)\n    self.num_class = num_groundtruth_classes\n    self.use_weighted_mean_ap = use_weighted_mean_ap\n    self.label_id_offset = label_id_offset\n\n    self.groundtruth_boxes = {}\n    self.groundtruth_class_labels = {}\n    self.groundtruth_masks = {}\n    self.groundtruth_is_difficult_list = {}\n    self.groundtruth_is_group_of_list = {}\n    self.num_gt_instances_per_class = np.zeros(self.num_class, dtype=int)\n    self.num_gt_imgs_per_class = np.zeros(self.num_class, dtype=int)\n\n    self._initialize_detections()\n\n  def _initialize_detections(self):\n    self.detection_keys = set()\n    self.scores_per_class = [[] for _ in range(self.num_class)]\n    self.tp_fp_labels_per_class = [[] for _ in range(self.num_class)]\n    self.num_images_correctly_detected_per_class = np.zeros(self.num_class)\n    self.average_precision_per_class = np.empty(self.num_class, dtype=float)\n    self.average_precision_per_class.fill(np.nan)\n    self.precisions_per_class = []\n    self.recalls_per_class = []\n    self.corloc_per_class = np.ones(self.num_class, dtype=float)\n\n  def clear_detections(self):\n    self._initialize_detections()\n\n  def add_single_ground_truth_image_info(self,\n                                         image_key,\n                                         groundtruth_boxes,\n                                         groundtruth_class_labels,\n                                         groundtruth_is_difficult_list=None,\n                                         groundtruth_is_group_of_list=None,\n                                         groundtruth_masks=None):\n    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_key: A unique string/integer identifier for the image.\n      groundtruth_boxes: float32 numpy array of shape [num_boxes, 4]\n        containing `num_boxes` groundtruth boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      groundtruth_class_labels: integer numpy array of shape [num_boxes]\n        containing 0-indexed groundtruth classes for the boxes.\n      groundtruth_is_difficult_list: A length M numpy boolean array denoting\n        whether a ground truth box is a difficult instance or not. To support\n        the case that no boxes are difficult, it is by default set as None.\n      groundtruth_is_group_of_list: A length M numpy boolean array denoting\n          whether a ground truth box is a group-of box or not. To support\n          the case that no boxes are groups-of, it is by default set as None.\n      groundtruth_masks: uint8 numpy array of shape\n        [num_boxes, height, width] containing `num_boxes` groundtruth masks.\n        The mask values range from 0 to 1.\n    \"\"\"\n    if image_key in self.groundtruth_boxes:\n      logging.warn(\n          'image %s has already been added to the ground truth database.',\n          image_key)\n      return\n\n    self.groundtruth_boxes[image_key] = groundtruth_boxes\n    self.groundtruth_class_labels[image_key] = groundtruth_class_labels\n    self.groundtruth_masks[image_key] = groundtruth_masks\n    if groundtruth_is_difficult_list is None:\n      num_boxes = groundtruth_boxes.shape[0]\n      groundtruth_is_difficult_list = np.zeros(num_boxes, dtype=bool)\n    self.groundtruth_is_difficult_list[\n        image_key] = groundtruth_is_difficult_list.astype(dtype=bool)\n    if groundtruth_is_group_of_list is None:\n      num_boxes = groundtruth_boxes.shape[0]\n      groundtruth_is_group_of_list = np.zeros(num_boxes, dtype=bool)\n    self.groundtruth_is_group_of_list[\n        image_key] = groundtruth_is_group_of_list.astype(dtype=bool)\n\n    self._update_ground_truth_statistics(\n        groundtruth_class_labels,\n        groundtruth_is_difficult_list.astype(dtype=bool),\n        groundtruth_is_group_of_list.astype(dtype=bool))\n\n  def add_single_detected_image_info(self,\n                                     image_key,\n                                     detected_boxes,\n                                     detected_scores,\n                                     detected_class_labels,\n                                     detected_masks=None):\n    \"\"\"Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_key: A unique string/integer identifier for the image.\n      detected_boxes: float32 numpy array of shape [num_boxes, 4]\n        containing `num_boxes` detection boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      detected_scores: float32 numpy array of shape [num_boxes] containing\n        detection scores for the boxes.\n      detected_class_labels: integer numpy array of shape [num_boxes] containing\n        0-indexed detection classes for the boxes.\n      detected_masks: np.uint8 numpy array of shape [num_boxes, height, width]\n        containing `num_boxes` detection masks with values ranging\n        between 0 and 1.\n\n    Raises:\n      ValueError: if the number of boxes, scores and class labels differ in\n        length.\n    \"\"\"\n    if (len(detected_boxes) != len(detected_scores) or\n        len(detected_boxes) != len(detected_class_labels)):\n      raise ValueError('detected_boxes, detected_scores and '\n                       'detected_class_labels should all have same lengths. Got'\n                       '[%d, %d, %d]' % len(detected_boxes),\n                       len(detected_scores), len(detected_class_labels))\n\n    if image_key in self.detection_keys:\n      logging.warn(\n          'image %s has already been added to the detection result database',\n          image_key)\n      return\n\n    self.detection_keys.add(image_key)\n    if image_key in self.groundtruth_boxes:\n      groundtruth_boxes = self.groundtruth_boxes[image_key]\n      groundtruth_class_labels = self.groundtruth_class_labels[image_key]\n      # Masks are popped instead of look up. The reason is that we do not want\n      # to keep all masks in memory which can cause memory overflow.\n      groundtruth_masks = self.groundtruth_masks.pop(image_key)\n      groundtruth_is_difficult_list = self.groundtruth_is_difficult_list[\n          image_key]\n      groundtruth_is_group_of_list = self.groundtruth_is_group_of_list[\n          image_key]\n    else:\n      groundtruth_boxes = np.empty(shape=[0, 4], dtype=float)\n      groundtruth_class_labels = np.array([], dtype=int)\n      if detected_masks is None:\n        groundtruth_masks = None\n      else:\n        groundtruth_masks = np.empty(shape=[0, 1, 1], dtype=float)\n      groundtruth_is_difficult_list = np.array([], dtype=bool)\n      groundtruth_is_group_of_list = np.array([], dtype=bool)\n    scores, tp_fp_labels = (\n        self.per_image_eval.compute_object_detection_metrics(\n            detected_boxes=detected_boxes,\n            detected_scores=detected_scores,\n            detected_class_labels=detected_class_labels,\n            groundtruth_boxes=groundtruth_boxes,\n            groundtruth_class_labels=groundtruth_class_labels,\n            groundtruth_is_difficult_list=groundtruth_is_difficult_list,\n            groundtruth_is_group_of_list=groundtruth_is_group_of_list,\n            detected_masks=detected_masks,\n            groundtruth_masks=groundtruth_masks))\n\n    for i in range(self.num_class):\n      if scores[i].shape[0] > 0:\n        self.scores_per_class[i].append(scores[i])\n        self.tp_fp_labels_per_class[i].append(tp_fp_labels[i])\n\n  def _update_ground_truth_statistics(self, groundtruth_class_labels,\n                                      groundtruth_is_difficult_list,\n                                      groundtruth_is_group_of_list):\n    \"\"\"Update grouth truth statitistics.\n\n    1. Difficult boxes are ignored when counting the number of ground truth\n    instances as done in Pascal VOC devkit.\n    2. Difficult boxes are treated as normal boxes when computing CorLoc related\n    statitistics.\n\n    Args:\n      groundtruth_class_labels: An integer numpy array of length M,\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a group-of box or not\n    \"\"\"\n    for class_index in range(self.num_class):\n      num_gt_instances = np.sum(groundtruth_class_labels[\n          ~groundtruth_is_difficult_list\n          & ~groundtruth_is_group_of_list] == class_index)\n      self.num_gt_instances_per_class[class_index] += num_gt_instances\n      if np.any(groundtruth_class_labels == class_index):\n        self.num_gt_imgs_per_class[class_index] += 1\n\n  def evaluate(self):\n    \"\"\"Compute evaluation result.\n\n    Returns:\n      A named tuple with the following fields -\n        average_precision: float numpy array of average precision for\n            each class.\n        mean_ap: mean average precision of all classes, float scalar\n        precisions: List of precisions, each precision is a float numpy\n            array\n        recalls: List of recalls, each recall is a float numpy array\n        corloc: numpy float array\n        mean_corloc: Mean CorLoc score for each class, float scalar\n    \"\"\"\n    if self.use_weighted_mean_ap:\n      all_scores = np.array([], dtype=float)\n      all_tp_fp_labels = np.array([], dtype=bool)\n\n    for class_index in range(self.num_class):\n      if self.num_gt_instances_per_class[class_index] == 0:\n        continue\n      if not self.scores_per_class[class_index]:\n        scores = np.array([], dtype=float)\n        tp_fp_labels = np.array([], dtype=bool)\n      else:\n        scores = np.concatenate(self.scores_per_class[class_index])\n        tp_fp_labels = np.concatenate(self.tp_fp_labels_per_class[class_index])\n      if self.use_weighted_mean_ap:\n        all_scores = np.append(all_scores, scores)\n        all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)\n      precision, recall = metrics.compute_precision_recall(\n          scores, tp_fp_labels, self.num_gt_instances_per_class[class_index])\n      self.precisions_per_class.append(precision)\n      self.recalls_per_class.append(recall)\n      average_precision = metrics.compute_average_precision(precision, recall)\n      self.average_precision_per_class[class_index] = average_precision\n\n    self.corloc_per_class = metrics.compute_cor_loc(\n        self.num_gt_imgs_per_class,\n        self.num_images_correctly_detected_per_class)\n\n    if self.use_weighted_mean_ap:\n      num_gt_instances = np.sum(self.num_gt_instances_per_class)\n      precision, recall = metrics.compute_precision_recall(\n          all_scores, all_tp_fp_labels, num_gt_instances)\n      mean_ap = metrics.compute_average_precision(precision, recall)\n    else:\n      mean_ap = np.nanmean(self.average_precision_per_class)\n    mean_corloc = np.nanmean(self.corloc_per_class)\n    return ObjectDetectionEvalMetrics(\n        self.average_precision_per_class, mean_ap, self.precisions_per_class,\n        self.recalls_per_class, self.corloc_per_class, mean_corloc)", "\nclass ObjectDetectionEvaluation(object):\n  \"\"\"Internal implementation of Pascal object detection metrics.\"\"\"\n\n  def __init__(self,\n               num_groundtruth_classes,\n               matching_iou_threshold=0.5,\n               nms_iou_threshold=1.0,\n               nms_max_output_boxes=10000,\n               use_weighted_mean_ap=False,\n               label_id_offset=0):\n    if num_groundtruth_classes < 1:\n      raise ValueError('Need at least 1 groundtruth class for evaluation.')\n\n    self.per_image_eval = per_image_evaluation.PerImageEvaluation(\n        num_groundtruth_classes=num_groundtruth_classes,\n        matching_iou_threshold=matching_iou_threshold)\n    self.num_class = num_groundtruth_classes\n    self.use_weighted_mean_ap = use_weighted_mean_ap\n    self.label_id_offset = label_id_offset\n\n    self.groundtruth_boxes = {}\n    self.groundtruth_class_labels = {}\n    self.groundtruth_masks = {}\n    self.groundtruth_is_difficult_list = {}\n    self.groundtruth_is_group_of_list = {}\n    self.num_gt_instances_per_class = np.zeros(self.num_class, dtype=int)\n    self.num_gt_imgs_per_class = np.zeros(self.num_class, dtype=int)\n\n    self._initialize_detections()\n\n  def _initialize_detections(self):\n    self.detection_keys = set()\n    self.scores_per_class = [[] for _ in range(self.num_class)]\n    self.tp_fp_labels_per_class = [[] for _ in range(self.num_class)]\n    self.num_images_correctly_detected_per_class = np.zeros(self.num_class)\n    self.average_precision_per_class = np.empty(self.num_class, dtype=float)\n    self.average_precision_per_class.fill(np.nan)\n    self.precisions_per_class = []\n    self.recalls_per_class = []\n    self.corloc_per_class = np.ones(self.num_class, dtype=float)\n\n  def clear_detections(self):\n    self._initialize_detections()\n\n  def add_single_ground_truth_image_info(self,\n                                         image_key,\n                                         groundtruth_boxes,\n                                         groundtruth_class_labels,\n                                         groundtruth_is_difficult_list=None,\n                                         groundtruth_is_group_of_list=None,\n                                         groundtruth_masks=None):\n    \"\"\"Adds groundtruth for a single image to be used for evaluation.\n\n    Args:\n      image_key: A unique string/integer identifier for the image.\n      groundtruth_boxes: float32 numpy array of shape [num_boxes, 4]\n        containing `num_boxes` groundtruth boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      groundtruth_class_labels: integer numpy array of shape [num_boxes]\n        containing 0-indexed groundtruth classes for the boxes.\n      groundtruth_is_difficult_list: A length M numpy boolean array denoting\n        whether a ground truth box is a difficult instance or not. To support\n        the case that no boxes are difficult, it is by default set as None.\n      groundtruth_is_group_of_list: A length M numpy boolean array denoting\n          whether a ground truth box is a group-of box or not. To support\n          the case that no boxes are groups-of, it is by default set as None.\n      groundtruth_masks: uint8 numpy array of shape\n        [num_boxes, height, width] containing `num_boxes` groundtruth masks.\n        The mask values range from 0 to 1.\n    \"\"\"\n    if image_key in self.groundtruth_boxes:\n      logging.warn(\n          'image %s has already been added to the ground truth database.',\n          image_key)\n      return\n\n    self.groundtruth_boxes[image_key] = groundtruth_boxes\n    self.groundtruth_class_labels[image_key] = groundtruth_class_labels\n    self.groundtruth_masks[image_key] = groundtruth_masks\n    if groundtruth_is_difficult_list is None:\n      num_boxes = groundtruth_boxes.shape[0]\n      groundtruth_is_difficult_list = np.zeros(num_boxes, dtype=bool)\n    self.groundtruth_is_difficult_list[\n        image_key] = groundtruth_is_difficult_list.astype(dtype=bool)\n    if groundtruth_is_group_of_list is None:\n      num_boxes = groundtruth_boxes.shape[0]\n      groundtruth_is_group_of_list = np.zeros(num_boxes, dtype=bool)\n    self.groundtruth_is_group_of_list[\n        image_key] = groundtruth_is_group_of_list.astype(dtype=bool)\n\n    self._update_ground_truth_statistics(\n        groundtruth_class_labels,\n        groundtruth_is_difficult_list.astype(dtype=bool),\n        groundtruth_is_group_of_list.astype(dtype=bool))\n\n  def add_single_detected_image_info(self,\n                                     image_key,\n                                     detected_boxes,\n                                     detected_scores,\n                                     detected_class_labels,\n                                     detected_masks=None):\n    \"\"\"Adds detections for a single image to be used for evaluation.\n\n    Args:\n      image_key: A unique string/integer identifier for the image.\n      detected_boxes: float32 numpy array of shape [num_boxes, 4]\n        containing `num_boxes` detection boxes of the format\n        [ymin, xmin, ymax, xmax] in absolute image coordinates.\n      detected_scores: float32 numpy array of shape [num_boxes] containing\n        detection scores for the boxes.\n      detected_class_labels: integer numpy array of shape [num_boxes] containing\n        0-indexed detection classes for the boxes.\n      detected_masks: np.uint8 numpy array of shape [num_boxes, height, width]\n        containing `num_boxes` detection masks with values ranging\n        between 0 and 1.\n\n    Raises:\n      ValueError: if the number of boxes, scores and class labels differ in\n        length.\n    \"\"\"\n    if (len(detected_boxes) != len(detected_scores) or\n        len(detected_boxes) != len(detected_class_labels)):\n      raise ValueError('detected_boxes, detected_scores and '\n                       'detected_class_labels should all have same lengths. Got'\n                       '[%d, %d, %d]' % len(detected_boxes),\n                       len(detected_scores), len(detected_class_labels))\n\n    if image_key in self.detection_keys:\n      logging.warn(\n          'image %s has already been added to the detection result database',\n          image_key)\n      return\n\n    self.detection_keys.add(image_key)\n    if image_key in self.groundtruth_boxes:\n      groundtruth_boxes = self.groundtruth_boxes[image_key]\n      groundtruth_class_labels = self.groundtruth_class_labels[image_key]\n      # Masks are popped instead of look up. The reason is that we do not want\n      # to keep all masks in memory which can cause memory overflow.\n      groundtruth_masks = self.groundtruth_masks.pop(image_key)\n      groundtruth_is_difficult_list = self.groundtruth_is_difficult_list[\n          image_key]\n      groundtruth_is_group_of_list = self.groundtruth_is_group_of_list[\n          image_key]\n    else:\n      groundtruth_boxes = np.empty(shape=[0, 4], dtype=float)\n      groundtruth_class_labels = np.array([], dtype=int)\n      if detected_masks is None:\n        groundtruth_masks = None\n      else:\n        groundtruth_masks = np.empty(shape=[0, 1, 1], dtype=float)\n      groundtruth_is_difficult_list = np.array([], dtype=bool)\n      groundtruth_is_group_of_list = np.array([], dtype=bool)\n    scores, tp_fp_labels = (\n        self.per_image_eval.compute_object_detection_metrics(\n            detected_boxes=detected_boxes,\n            detected_scores=detected_scores,\n            detected_class_labels=detected_class_labels,\n            groundtruth_boxes=groundtruth_boxes,\n            groundtruth_class_labels=groundtruth_class_labels,\n            groundtruth_is_difficult_list=groundtruth_is_difficult_list,\n            groundtruth_is_group_of_list=groundtruth_is_group_of_list,\n            detected_masks=detected_masks,\n            groundtruth_masks=groundtruth_masks))\n\n    for i in range(self.num_class):\n      if scores[i].shape[0] > 0:\n        self.scores_per_class[i].append(scores[i])\n        self.tp_fp_labels_per_class[i].append(tp_fp_labels[i])\n\n  def _update_ground_truth_statistics(self, groundtruth_class_labels,\n                                      groundtruth_is_difficult_list,\n                                      groundtruth_is_group_of_list):\n    \"\"\"Update grouth truth statitistics.\n\n    1. Difficult boxes are ignored when counting the number of ground truth\n    instances as done in Pascal VOC devkit.\n    2. Difficult boxes are treated as normal boxes when computing CorLoc related\n    statitistics.\n\n    Args:\n      groundtruth_class_labels: An integer numpy array of length M,\n          representing M class labels of object instances in ground truth\n      groundtruth_is_difficult_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a difficult instance or not\n      groundtruth_is_group_of_list: A boolean numpy array of length M denoting\n          whether a ground truth box is a group-of box or not\n    \"\"\"\n    for class_index in range(self.num_class):\n      num_gt_instances = np.sum(groundtruth_class_labels[\n          ~groundtruth_is_difficult_list\n          & ~groundtruth_is_group_of_list] == class_index)\n      self.num_gt_instances_per_class[class_index] += num_gt_instances\n      if np.any(groundtruth_class_labels == class_index):\n        self.num_gt_imgs_per_class[class_index] += 1\n\n  def evaluate(self):\n    \"\"\"Compute evaluation result.\n\n    Returns:\n      A named tuple with the following fields -\n        average_precision: float numpy array of average precision for\n            each class.\n        mean_ap: mean average precision of all classes, float scalar\n        precisions: List of precisions, each precision is a float numpy\n            array\n        recalls: List of recalls, each recall is a float numpy array\n        corloc: numpy float array\n        mean_corloc: Mean CorLoc score for each class, float scalar\n    \"\"\"\n    if self.use_weighted_mean_ap:\n      all_scores = np.array([], dtype=float)\n      all_tp_fp_labels = np.array([], dtype=bool)\n\n    for class_index in range(self.num_class):\n      if self.num_gt_instances_per_class[class_index] == 0:\n        continue\n      if not self.scores_per_class[class_index]:\n        scores = np.array([], dtype=float)\n        tp_fp_labels = np.array([], dtype=bool)\n      else:\n        scores = np.concatenate(self.scores_per_class[class_index])\n        tp_fp_labels = np.concatenate(self.tp_fp_labels_per_class[class_index])\n      if self.use_weighted_mean_ap:\n        all_scores = np.append(all_scores, scores)\n        all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)\n      precision, recall = metrics.compute_precision_recall(\n          scores, tp_fp_labels, self.num_gt_instances_per_class[class_index])\n      self.precisions_per_class.append(precision)\n      self.recalls_per_class.append(recall)\n      average_precision = metrics.compute_average_precision(precision, recall)\n      self.average_precision_per_class[class_index] = average_precision\n\n    self.corloc_per_class = metrics.compute_cor_loc(\n        self.num_gt_imgs_per_class,\n        self.num_images_correctly_detected_per_class)\n\n    if self.use_weighted_mean_ap:\n      num_gt_instances = np.sum(self.num_gt_instances_per_class)\n      precision, recall = metrics.compute_precision_recall(\n          all_scores, all_tp_fp_labels, num_gt_instances)\n      mean_ap = metrics.compute_average_precision(precision, recall)\n    else:\n      mean_ap = np.nanmean(self.average_precision_per_class)\n    mean_corloc = np.nanmean(self.corloc_per_class)\n    return ObjectDetectionEvalMetrics(\n        self.average_precision_per_class, mean_ap, self.precisions_per_class,\n        self.recalls_per_class, self.corloc_per_class, mean_corloc)", ""]}
{"filename": "gravit/utils/ava/np_box_mask_list_ops.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Operations for np_box_mask_list.BoxMaskList.\n\nExample box operations that are supported:", "\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n\"\"\"\nimport numpy as np\n\nfrom . import np_box_list_ops\nfrom . import np_box_mask_list\nfrom . import np_mask_ops", "from . import np_box_mask_list\nfrom . import np_mask_ops\n\n\ndef box_list_to_box_mask_list(boxlist):\n  \"\"\"Converts a BoxList containing 'masks' into a BoxMaskList.\n\n  Args:\n    boxlist: An np_box_list.BoxList object.\n\n  Returns:\n    An np_box_mask_list.BoxMaskList object.\n\n  Raises:\n    ValueError: If boxlist does not contain `masks` as a field.\n  \"\"\"\n  if not boxlist.has_field('masks'):\n    raise ValueError('boxlist does not contain mask field.')\n  box_mask_list = np_box_mask_list.BoxMaskList(\n      box_data=boxlist.get(),\n      mask_data=boxlist.get_field('masks'))\n  extra_fields = boxlist.get_extra_fields()\n  for key in extra_fields:\n    if key != 'masks':\n      box_mask_list.data[key] = boxlist.get_field(key)\n  return box_mask_list", "\n\ndef area(box_mask_list):\n  \"\"\"Computes area of masks.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes and masks\n\n  Returns:\n    a numpy array with shape [N*1] representing mask areas\n  \"\"\"\n  return np_mask_ops.area(box_mask_list.get_masks())", "\n\ndef intersection(box_mask_list1, box_mask_list2):\n  \"\"\"Compute pairwise intersection areas between masks.\n\n  Args:\n    box_mask_list1: BoxMaskList holding N boxes and masks\n    box_mask_list2: BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  \"\"\"\n  return np_mask_ops.intersection(box_mask_list1.get_masks(),\n                                  box_mask_list2.get_masks())", "\n\ndef iou(box_mask_list1, box_mask_list2):\n  \"\"\"Computes pairwise intersection-over-union between box and mask collections.\n\n  Args:\n    box_mask_list1: BoxMaskList holding N boxes and masks\n    box_mask_list2: BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  \"\"\"\n  return np_mask_ops.iou(box_mask_list1.get_masks(),\n                         box_mask_list2.get_masks())", "\n\ndef ioa(box_mask_list1, box_mask_list2):\n  \"\"\"Computes pairwise intersection-over-area between box and mask collections.\n\n  Intersection-over-area (ioa) between two masks mask1 and mask2 is defined as\n  their intersection area over mask2's area. Note that ioa is not symmetric,\n  that is, IOA(mask1, mask2) != IOA(mask2, mask1).\n\n  Args:\n    box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks\n    box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  \"\"\"\n  return np_mask_ops.ioa(box_mask_list1.get_masks(), box_mask_list2.get_masks())", "\n\ndef gather(box_mask_list, indices, fields=None):\n  \"\"\"Gather boxes from np_box_mask_list.BoxMaskList according to indices.\n\n  By default, gather returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the box_mask_list (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes\n    indices: a 1-d numpy array of type int_\n    fields: (optional) list of fields to also gather from.  If None (default),\n        all fields are gathered from.  Pass an empty fields list to only gather\n        the box coordinates.\n\n  Returns:\n    subbox_mask_list: a np_box_mask_list.BoxMaskList corresponding to the subset\n        of the input box_mask_list specified by indices\n\n  Raises:\n    ValueError: if specified field is not contained in box_mask_list or if the\n        indices are not of type int_\n  \"\"\"\n  if fields is not None:\n    if 'masks' not in fields:\n      fields.append('masks')\n  return box_list_to_box_mask_list(\n      np_box_list_ops.gather(\n          boxlist=box_mask_list, indices=indices, fields=fields))", "\n\ndef sort_by_field(box_mask_list, field,\n                  order=np_box_list_ops.SortOrder.DESCEND):\n  \"\"\"Sort boxes and associated fields according to a scalar field.\n\n  A common use case is reordering the boxes according to descending scores.\n\n  Args:\n    box_mask_list: BoxMaskList holding N boxes.\n    field: A BoxMaskList field for sorting and reordering the BoxMaskList.\n    order: (Optional) 'descend' or 'ascend'. Default is descend.\n\n  Returns:\n    sorted_box_mask_list: A sorted BoxMaskList with the field in the specified\n      order.\n  \"\"\"\n  return box_list_to_box_mask_list(\n      np_box_list_ops.sort_by_field(\n          boxlist=box_mask_list, field=field, order=order))", "\n\ndef non_max_suppression(box_mask_list,\n                        max_output_size=10000,\n                        iou_threshold=1.0,\n                        score_threshold=-10.0):\n  \"\"\"Non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes. In each iteration, the detected bounding box with\n  highest score in the available pool is selected.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes.  Must contain\n      a 'scores' field representing detection scores. All scores belong to the\n      same class.\n    max_output_size: maximum number of retained boxes\n    iou_threshold: intersection over union threshold.\n    score_threshold: minimum score threshold. Remove the boxes with scores\n                     less than this value. Default value is set to -10. A very\n                     low threshold to pass pretty much all the boxes, unless\n                     the user sets a different score threshold.\n\n  Returns:\n    an np_box_mask_list.BoxMaskList holding M boxes where M <= max_output_size\n\n  Raises:\n    ValueError: if 'scores' field does not exist\n    ValueError: if threshold is not in [0, 1]\n    ValueError: if max_output_size < 0\n  \"\"\"\n  if not box_mask_list.has_field('scores'):\n    raise ValueError('Field scores does not exist')\n  if iou_threshold < 0. or iou_threshold > 1.0:\n    raise ValueError('IOU threshold must be in [0, 1]')\n  if max_output_size < 0:\n    raise ValueError('max_output_size must be bigger than 0.')\n\n  box_mask_list = filter_scores_greater_than(box_mask_list, score_threshold)\n  if box_mask_list.num_boxes() == 0:\n    return box_mask_list\n\n  box_mask_list = sort_by_field(box_mask_list, 'scores')\n\n  # Prevent further computation if NMS is disabled.\n  if iou_threshold == 1.0:\n    if box_mask_list.num_boxes() > max_output_size:\n      selected_indices = np.arange(max_output_size)\n      return gather(box_mask_list, selected_indices)\n    else:\n      return box_mask_list\n\n  masks = box_mask_list.get_masks()\n  num_masks = box_mask_list.num_boxes()\n\n  # is_index_valid is True only for all remaining valid boxes,\n  is_index_valid = np.full(num_masks, 1, dtype=bool)\n  selected_indices = []\n  num_output = 0\n  for i in range(num_masks):\n    if num_output < max_output_size:\n      if is_index_valid[i]:\n        num_output += 1\n        selected_indices.append(i)\n        is_index_valid[i] = False\n        valid_indices = np.where(is_index_valid)[0]\n        if valid_indices.size == 0:\n          break\n\n        intersect_over_union = np_mask_ops.iou(\n            np.expand_dims(masks[i], axis=0), masks[valid_indices])\n        intersect_over_union = np.squeeze(intersect_over_union, axis=0)\n        is_index_valid[valid_indices] = np.logical_and(\n            is_index_valid[valid_indices],\n            intersect_over_union <= iou_threshold)\n  return gather(box_mask_list, np.array(selected_indices))", "\n\ndef multi_class_non_max_suppression(box_mask_list, score_thresh, iou_thresh,\n                                    max_output_size):\n  \"\"\"Multi-class version of non maximum suppression.\n\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n\n  Args:\n    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes.  Must contain a\n      'scores' field representing detection scores.  This scores field is a\n      tensor that can be 1 dimensional (in the case of a single class) or\n      2-dimensional, in which case we assume that it takes the\n      shape [num_boxes, num_classes]. We further assume that this rank is known\n      statically and that scores.shape[1] is also known (i.e., the number of\n      classes is fixed and known at graph construction time).\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap\n      with previously selected boxes are removed).\n    max_output_size: maximum number of retained boxes per class.\n\n  Returns:\n    a box_mask_list holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input box_mask_list does\n      not have a valid scores field.\n  \"\"\"\n  if not 0 <= iou_thresh <= 1.0:\n    raise ValueError('thresh must be between 0 and 1')\n  if not isinstance(box_mask_list, np_box_mask_list.BoxMaskList):\n    raise ValueError('box_mask_list must be a box_mask_list')\n  if not box_mask_list.has_field('scores'):\n    raise ValueError('input box_mask_list must have \\'scores\\' field')\n  scores = box_mask_list.get_field('scores')\n  if len(scores.shape) == 1:\n    scores = np.reshape(scores, [-1, 1])\n  elif len(scores.shape) == 2:\n    if scores.shape[1] is None:\n      raise ValueError('scores field must have statically defined second '\n                       'dimension')\n  else:\n    raise ValueError('scores field must be of rank 1 or 2')\n\n  num_boxes = box_mask_list.num_boxes()\n  num_scores = scores.shape[0]\n  num_classes = scores.shape[1]\n\n  if num_boxes != num_scores:\n    raise ValueError('Incorrect scores field length: actual vs expected.')\n\n  selected_boxes_list = []\n  for class_idx in range(num_classes):\n    box_mask_list_and_class_scores = np_box_mask_list.BoxMaskList(\n        box_data=box_mask_list.get(),\n        mask_data=box_mask_list.get_masks())\n    class_scores = np.reshape(scores[0:num_scores, class_idx], [-1])\n    box_mask_list_and_class_scores.add_field('scores', class_scores)\n    box_mask_list_filt = filter_scores_greater_than(\n        box_mask_list_and_class_scores, score_thresh)\n    nms_result = non_max_suppression(\n        box_mask_list_filt,\n        max_output_size=max_output_size,\n        iou_threshold=iou_thresh,\n        score_threshold=score_thresh)\n    nms_result.add_field(\n        'classes',\n        np.zeros_like(nms_result.get_field('scores')) + class_idx)\n    selected_boxes_list.append(nms_result)\n  selected_boxes = np_box_list_ops.concatenate(selected_boxes_list)\n  sorted_boxes = np_box_list_ops.sort_by_field(selected_boxes, 'scores')\n  return box_list_to_box_mask_list(boxlist=sorted_boxes)", "\n\ndef prune_non_overlapping_masks(box_mask_list1, box_mask_list2, minoverlap=0.0):\n  \"\"\"Prunes the boxes in list1 that overlap less than thresh with list2.\n\n  For each mask in box_mask_list1, we want its IOA to be more than minoverlap\n  with at least one of the masks in box_mask_list2. If it does not, we remove\n  it. If the masks are not full size image, we do the pruning based on boxes.\n\n  Args:\n    box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks.\n    box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks.\n    minoverlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n\n  Returns:\n    A pruned box_mask_list with size [N', 4].\n  \"\"\"\n  intersection_over_area = ioa(box_mask_list2, box_mask_list1)  # [M, N] tensor\n  intersection_over_area = np.amax(intersection_over_area, axis=0)  # [N] tensor\n  keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))\n  keep_inds = np.nonzero(keep_bool)[0]\n  new_box_mask_list1 = gather(box_mask_list1, keep_inds)\n  return new_box_mask_list1", "\n\ndef concatenate(box_mask_lists, fields=None):\n  \"\"\"Concatenate list of box_mask_lists.\n\n  This op concatenates a list of input box_mask_lists into a larger\n  box_mask_list.  It also\n  handles concatenation of box_mask_list fields as long as the field tensor\n  shapes are equal except for the first dimension.\n\n  Args:\n    box_mask_lists: list of np_box_mask_list.BoxMaskList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxMaskList in the list are included in the\n      concatenation.\n\n  Returns:\n    a box_mask_list with number of boxes equal to\n      sum([box_mask_list.num_boxes() for box_mask_list in box_mask_list])\n  Raises:\n    ValueError: if box_mask_lists is invalid (i.e., is not a list, is empty, or\n      contains non box_mask_list objects), or if requested fields are not\n      contained in all box_mask_lists\n  \"\"\"\n  if fields is not None:\n    if 'masks' not in fields:\n      fields.append('masks')\n  return box_list_to_box_mask_list(\n      np_box_list_ops.concatenate(boxlists=box_mask_lists, fields=fields))", "\n\ndef filter_scores_greater_than(box_mask_list, thresh):\n  \"\"\"Filter to keep only boxes and masks with score exceeding a given threshold.\n\n  This op keeps the collection of boxes and masks whose corresponding scores are\n  greater than the input threshold.\n\n  Args:\n    box_mask_list: BoxMaskList holding N boxes and masks.  Must contain a\n      'scores' field representing detection scores.\n    thresh: scalar threshold\n\n  Returns:\n    a BoxMaskList holding M boxes and masks where M <= N\n\n  Raises:\n    ValueError: if box_mask_list not a np_box_mask_list.BoxMaskList object or\n      if it does not have a scores field\n  \"\"\"\n  if not isinstance(box_mask_list, np_box_mask_list.BoxMaskList):\n    raise ValueError('box_mask_list must be a BoxMaskList')\n  if not box_mask_list.has_field('scores'):\n    raise ValueError('input box_mask_list must have \\'scores\\' field')\n  scores = box_mask_list.get_field('scores')\n  if len(scores.shape) > 2:\n    raise ValueError('Scores should have rank 1 or 2')\n  if len(scores.shape) == 2 and scores.shape[1] != 1:\n    raise ValueError('Scores should have rank 1 or have shape '\n                     'consistent with [None, 1]')\n  high_score_indices = np.reshape(np.where(np.greater(scores, thresh)),\n                                  [-1]).astype(np.int32)\n  return gather(box_mask_list, high_score_indices)", ""]}
{"filename": "gravit/utils/ava/np_box_ops.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Operations for [N, 4] numpy arrays representing bounding boxes.\n\nExample box operations that are supported:", "\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n\"\"\"\nimport numpy as np\n\n\ndef area(boxes):\n  \"\"\"Computes area of boxes.\n\n  Args:\n    boxes: Numpy array with shape [N, 4] holding N boxes\n\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  \"\"\"\n  return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])", "def area(boxes):\n  \"\"\"Computes area of boxes.\n\n  Args:\n    boxes: Numpy array with shape [N, 4] holding N boxes\n\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  \"\"\"\n  return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])", "\n\ndef intersection(boxes1, boxes2):\n  \"\"\"Compute pairwise intersection areas between boxes.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes\n    boxes2: a numpy array with shape [M, 4] holding M boxes\n\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  \"\"\"\n  [y_min1, x_min1, y_max1, x_max1] = np.split(boxes1, 4, axis=1)\n  [y_min2, x_min2, y_max2, x_max2] = np.split(boxes2, 4, axis=1)\n\n  all_pairs_min_ymax = np.minimum(y_max1, np.transpose(y_max2))\n  all_pairs_max_ymin = np.maximum(y_min1, np.transpose(y_min2))\n  intersect_heights = np.maximum(\n      np.zeros(all_pairs_max_ymin.shape),\n      all_pairs_min_ymax - all_pairs_max_ymin)\n  all_pairs_min_xmax = np.minimum(x_max1, np.transpose(x_max2))\n  all_pairs_max_xmin = np.maximum(x_min1, np.transpose(x_min2))\n  intersect_widths = np.maximum(\n      np.zeros(all_pairs_max_xmin.shape),\n      all_pairs_min_xmax - all_pairs_max_xmin)\n  return intersect_heights * intersect_widths", "\n\ndef iou(boxes1, boxes2):\n  \"\"\"Computes pairwise intersection-over-union between box collections.\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  \"\"\"\n  intersect = intersection(boxes1, boxes2)\n  area1 = area(boxes1)\n  area2 = area(boxes2)\n  union = np.expand_dims(area1, axis=1) + np.expand_dims(\n      area2, axis=0) - intersect\n  return intersect / union", "\n\ndef ioa(boxes1, boxes2):\n  \"\"\"Computes pairwise intersection-over-area between box collections.\n\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2's area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  \"\"\"\n  intersect = intersection(boxes1, boxes2)\n  areas = np.expand_dims(area(boxes2), axis=0)\n  return intersect / areas", ""]}
{"filename": "gravit/utils/ava/np_box_mask_list.py", "chunked_list": ["# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Numpy BoxMaskList classes and functions.\"\"\"\n\nimport numpy as np", "\nimport numpy as np\nfrom . import np_box_list\n\n\nclass BoxMaskList(np_box_list.BoxList):\n  \"\"\"Convenience wrapper for BoxList with masks.\n\n  BoxMaskList extends the np_box_list.BoxList to contain masks as well.\n  In particular, its constructor receives both boxes and masks. Note that the\n  masks correspond to the full image.\n  \"\"\"\n\n  def __init__(self, box_data, mask_data):\n    \"\"\"Constructs box collection.\n\n    Args:\n      box_data: a numpy array of shape [N, 4] representing box coordinates\n      mask_data: a numpy array of shape [N, height, width] representing masks\n        with values are in {0,1}. The masks correspond to the full\n        image. The height and the width will be equal to image height and width.\n\n    Raises:\n      ValueError: if bbox data is not a numpy array\n      ValueError: if invalid dimensions for bbox data\n      ValueError: if mask data is not a numpy array\n      ValueError: if invalid dimension for mask data\n    \"\"\"\n    super(BoxMaskList, self).__init__(box_data)\n    if not isinstance(mask_data, np.ndarray):\n      raise ValueError('Mask data must be a numpy array.')\n    if len(mask_data.shape) != 3:\n      raise ValueError('Invalid dimensions for mask data.')\n    if mask_data.dtype != np.uint8:\n      raise ValueError('Invalid data type for mask data: uint8 is required.')\n    if mask_data.shape[0] != box_data.shape[0]:\n      raise ValueError('There should be the same number of boxes and masks.')\n    self.data['masks'] = mask_data\n\n  def get_masks(self):\n    \"\"\"Convenience function for accessing masks.\n\n    Returns:\n      a numpy array of shape [N, height, width] representing masks\n    \"\"\"\n    return self.get_field('masks')", "\n"]}
{"filename": "gravit/models/losses.py", "chunked_list": ["import torch.nn as nn\n\n\n_LOSSES = {\n          'cross_entropy':    nn.CrossEntropyLoss,\n          'bce':              nn.BCELoss,\n          'bce_logit':        nn.BCEWithLogitsLoss,\n          'mse':              nn.MSELoss\n          }\n", "          }\n\n\ndef get_loss_func(loss_name):\n    \"\"\"\n    Get the loss function corresponding to \"loss_name\"\n    \"\"\"\n\n    if loss_name not in _LOSSES:\n        raise ValueError(f'Loss {loss_name} is not implemented in models/losses.py')\n\n    return _LOSSES[loss_name]()", ""]}
{"filename": "gravit/models/__init__.py", "chunked_list": ["from .build import build_model\nfrom .losses import get_loss_func\n"]}
{"filename": "gravit/models/build.py", "chunked_list": ["from .context_reasoning import *\n\n\ndef build_model(cfg, device):\n    \"\"\"\n    Build the model corresponding to \"model_name\"\n    \"\"\"\n\n    model_name = cfg['model_name']\n    model = globals()[model_name](cfg)\n\n    return model.to(device)", ""]}
{"filename": "gravit/models/context_reasoning/__init__.py", "chunked_list": ["from .spell import SPELL\n"]}
{"filename": "gravit/models/context_reasoning/spell.py", "chunked_list": ["import torch\nfrom torch.nn import Module, Sequential, ReLU, Dropout\nfrom torch_geometric.nn import Linear, EdgeConv, SAGEConv, BatchNorm\n\n\nclass SPELL(Module):\n    def __init__(self, cfg):\n        super(SPELL, self).__init__()\n        self.num_modality = cfg['num_modality']\n        channels = [cfg['channel1'], cfg['channel2']]\n        proj_dim = cfg['proj_dim']\n        final_dim = cfg['final_dim']\n        dropout = cfg['dropout']\n\n        self.layerspf = Linear(-1, proj_dim) # projection layer for spatial features\n        self.layer011 = Linear(-1, channels[0])\n        if self.num_modality == 2:\n            self.layer012 = Linear(-1, channels[0])\n\n        self.batch01 = BatchNorm(channels[0])\n        self.relu = ReLU()\n        self.dropout = Dropout(dropout)\n\n        self.layer11 = EdgeConv(Sequential(Linear(2*channels[0], channels[0]), ReLU(), Linear(channels[0], channels[0])))\n        self.batch11 = BatchNorm(channels[0])\n        self.layer12 = EdgeConv(Sequential(Linear(2*channels[0], channels[0]), ReLU(), Linear(channels[0], channels[0])))\n        self.batch12 = BatchNorm(channels[0])\n        self.layer13 = EdgeConv(Sequential(Linear(2*channels[0], channels[0]), ReLU(), Linear(channels[0], channels[0])))\n        self.batch13 = BatchNorm(channels[0])\n\n        self.layer21 = SAGEConv(channels[0], channels[1])\n        self.batch21 = BatchNorm(channels[1])\n\n        self.layer31 = SAGEConv(channels[1], final_dim)\n        self.layer32 = SAGEConv(channels[1], final_dim)\n        self.layer33 = SAGEConv(channels[1], final_dim)\n\n\n    def forward(self, x, c, edge_index, edge_attr):\n        feature_dim = x.shape[1]\n        x_visual = self.layer011(torch.cat((x[:, :feature_dim//self.num_modality], self.layerspf(c)), dim=1))\n        if self.num_modality == 1:\n            x = x_visual\n        elif self.num_modality == 2:\n            x_audio = self.layer012(x[:, feature_dim//self.num_modality:])\n            x = x_visual + x_audio\n\n        x = self.batch01(x)\n        x = self.relu(x)\n\n        edge_index_f = edge_index[:, edge_attr<=0]\n        edge_index_b = edge_index[:, edge_attr>=0]\n\n        # Forward-graph stream\n        x1 = self.layer11(x, edge_index_f)\n        x1 = self.batch11(x1)\n        x1 = self.relu(x1)\n        x1 = self.dropout(x1)\n        x1 = self.layer21(x1, edge_index_f)\n        x1 = self.batch21(x1)\n        x1 = self.relu(x1)\n        x1 = self.dropout(x1)\n\n        # Backward-graph stream\n        x2 = self.layer12(x, edge_index_b)\n        x2 = self.batch12(x2)\n        x2 = self.relu(x2)\n        x2 = self.dropout(x2)\n        x2 = self.layer21(x2, edge_index_b)\n        x2 = self.batch21(x2)\n        x2 = self.relu(x2)\n        x2 = self.dropout(x2)\n\n        # Undirected-graph stream\n        x3 = self.layer13(x, edge_index)\n        x3 = self.batch13(x3)\n        x3 = self.relu(x3)\n        x3 = self.dropout(x3)\n        x3 = self.layer21(x3, edge_index)\n        x3 = self.batch21(x3)\n        x3 = self.relu(x3)\n        x3 = self.dropout(x3)\n\n        x1 = self.layer31(x1, edge_index_f)\n        x2 = self.layer32(x2, edge_index_b)\n        x3 = self.layer33(x3, edge_index)\n\n        return x1+x2+x3", ""]}
{"filename": "gravit/datasets/dataset_context_reasoning.py", "chunked_list": ["import os\nimport glob\nimport torch\nfrom torch_geometric.data import Data, Dataset\n\n\nclass GraphDataset(Dataset):\n    \"\"\"\n    General class for graph dataset\n    \"\"\"\n\n    def __init__(self, path_graphs):\n        super(GraphDataset, self).__init__()\n        self.all_graphs = sorted(glob.glob(os.path.join(path_graphs, '*.pt')))\n\n    def len(self):\n        return len(self.all_graphs)\n\n    def get(self, idx):\n        data = torch.load(self.all_graphs[idx])\n        return data", ""]}
{"filename": "gravit/datasets/__init__.py", "chunked_list": ["from .dataset_context_reasoning import GraphDataset\n"]}
