{"filename": "setup.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"VERSION\", \"r\") as fh:\n    version = fh.read()", "\nwith open(\"VERSION\", \"r\") as fh:\n    version = fh.read()\n\nwith open('requirements.txt') as fh:\n    requirements = fh.read().splitlines()\n\nsetuptools.setup(\n    name=\"ENID\",\n    author=\"Simone Magnani\",", "    name=\"ENID\",\n    author=\"Simone Magnani\",\n    author_email=\"simonemagnani.96@gmail.com\",\n    version=version,\n    description=\"Enhancing Network Intrusion Detection: An Online Methodology for Performance Analysis\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/s41m0n/enid\",\n    packages=setuptools.find_packages(exclude=(\"tests\",)),\n    license=\"Apache-2.0\",", "    packages=setuptools.find_packages(exclude=(\"tests\",)),\n    license=\"Apache-2.0\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: Apache-2.0 License\",\n        \"Operating System :: Linux\",\n    ],\n    install_requires=requirements,\n    include_package_data=True,\n    python_requires='>=3.8'", "    include_package_data=True,\n    python_requires='>=3.8'\n)\n"]}
{"filename": "enid/offline.py", "chunked_list": ["import argparse\nimport os\n\nfrom .datasetter import DatasetConfig\nfrom .lib.utility import dump_json_data, get_logger, load_json_data, set_seed\nfrom .trainer import ModelConfig, ResultTrain\n\n_logger = get_logger(__name__)\n\n\ndef main(args_list):\n    # registering cli parameters, which can be shown with the -h\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        'de_models_dir', help='path to the model directory containing the models to test')\n    parser.add_argument(\n        '-r', '--retest', help='overwrite previous result', action=\"store_true\")\n    args = parser.parse_args(args_list).__dict__\n\n    models_dir = os.path.join(\n        args[\"de_models_dir\"], 'models')\n\n    models_config = ModelConfig(\n        **load_json_data(os.path.join(models_dir, \"conf.json\")))\n    dataset_config = DatasetConfig(\n        **load_json_data(os.path.join(args[\"de_models_dir\"], os.pardir, \"conf.json\")))\n\n    for c in models_config.train_params.all_train_combs():\n        name = models_config.detection_engine.model_name(**c)\n        new_path = os.path.join(models_dir, name)\n        if not args[\"retest\"] and os.path.isfile(os.path.join(new_path, \"results.json\")):\n            _logger.info(\n                f\"Offline results for {name} already present, skipping\")\n            continue\n\n        params = load_json_data(os.path.join(new_path, \"params.json\"))\n\n        fholder, params, model = models_config.detection_engine.load_model(\n            params, os.path.join(args[\"de_models_dir\"], \"models\"))\n        set_seed()\n        (_, _, _), (_, _, _), (xts, yts, pt) = models_config.detection_engine._load_dataset(\n            os.path.join(args[\"de_models_dir\"], os.pardir), fholder, **params.__dict__)\n\n        _logger.info(f\"Testing {name}\")\n        y_pred = models_config.detection_engine.predict(\n            model, xts, **params.__dict__)\n\n        res = ResultTrain(\n            _threshold=models_config.train_params.malicious_threshold,\n            _ypred=y_pred, _ytrue=yts)\n        res.update(dataset_config.offline, pt)\n        dump_json_data(res, os.path.join(new_path, \"results.json\"))", "\n\ndef main(args_list):\n    # registering cli parameters, which can be shown with the -h\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        'de_models_dir', help='path to the model directory containing the models to test')\n    parser.add_argument(\n        '-r', '--retest', help='overwrite previous result', action=\"store_true\")\n    args = parser.parse_args(args_list).__dict__\n\n    models_dir = os.path.join(\n        args[\"de_models_dir\"], 'models')\n\n    models_config = ModelConfig(\n        **load_json_data(os.path.join(models_dir, \"conf.json\")))\n    dataset_config = DatasetConfig(\n        **load_json_data(os.path.join(args[\"de_models_dir\"], os.pardir, \"conf.json\")))\n\n    for c in models_config.train_params.all_train_combs():\n        name = models_config.detection_engine.model_name(**c)\n        new_path = os.path.join(models_dir, name)\n        if not args[\"retest\"] and os.path.isfile(os.path.join(new_path, \"results.json\")):\n            _logger.info(\n                f\"Offline results for {name} already present, skipping\")\n            continue\n\n        params = load_json_data(os.path.join(new_path, \"params.json\"))\n\n        fholder, params, model = models_config.detection_engine.load_model(\n            params, os.path.join(args[\"de_models_dir\"], \"models\"))\n        set_seed()\n        (_, _, _), (_, _, _), (xts, yts, pt) = models_config.detection_engine._load_dataset(\n            os.path.join(args[\"de_models_dir\"], os.pardir), fholder, **params.__dict__)\n\n        _logger.info(f\"Testing {name}\")\n        y_pred = models_config.detection_engine.predict(\n            model, xts, **params.__dict__)\n\n        res = ResultTrain(\n            _threshold=models_config.train_params.malicious_threshold,\n            _ypred=y_pred, _ytrue=yts)\n        res.update(dataset_config.offline, pt)\n        dump_json_data(res, os.path.join(new_path, \"results.json\"))", ""]}
{"filename": "enid/datasetter.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMain file for creating a Dataser from a series of preprocessed pcap files.\nPcap can belong to different datasets and categories. This program takes into\naccount the creation of a unified and balanced train, validation and test set\nfor the training and the offline testing of the generated models.", "account the creation of a unified and balanced train, validation and test set\nfor the training and the offline testing of the generated models.\nIn addition, a PCAP file from the testing samples is created for the further\nonline testing methodology.\n\nThe algorithm for the creation of the dataset aims at taking an equally number\nof benign and malicious samples. However, within the same type (e.g., malicious)\nit is possible to still have a different amount of samples (e.g., 1000 ddos,\n10 sqli, 1 botnet).\n\"\"\"", "10 sqli, 1 botnet).\n\"\"\"\nimport argparse\nimport math\nimport multiprocessing\nimport os\nimport pickle\nimport random\nimport time\nfrom copy import deepcopy", "import time\nfrom copy import deepcopy\nfrom dataclasses import dataclass, field, fields\nfrom typing import Any, Dict, List, Type\n\nfrom pypacker.ppcap import Reader, Writer\n\nfrom .lib import ATTACK_LABELS\nfrom .lib.definitions import DetectionEngine\nfrom .lib.identifiers import BaseKey, str_to_key", "from .lib.definitions import DetectionEngine\nfrom .lib.identifiers import BaseKey, str_to_key\nfrom .lib.utility import (UpdatableDataclass, all_subclasses, create_dir,\n                          dump_json_data, get_logger, load_json_data)\nfrom .preprocesser import CategoryConfig, PcapConfig, PreprocessedConfig\n\n_logger = get_logger(__name__)\n\n\n@dataclass\nclass SingleDatasetTestConfig(PcapConfig, UpdatableDataclass):\n    categories: Dict[str, CategoryConfig] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for k, v in self.categories.items():\n            self.categories[k] = CategoryConfig(**v)", "\n@dataclass\nclass SingleDatasetTestConfig(PcapConfig, UpdatableDataclass):\n    categories: Dict[str, CategoryConfig] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for k, v in self.categories.items():\n            self.categories[k] = CategoryConfig(**v)\n\n", "\n\n@dataclass\nclass DatasetTestConfig(PcapConfig, UpdatableDataclass):\n    duration: int = 0\n    datasets: Dict[str, SingleDatasetTestConfig] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for k, v in self.datasets.items():\n            self.datasets[k] = SingleDatasetTestConfig(**v)", "\n\n@dataclass\nclass BaseConfig(UpdatableDataclass):\n    taken: int = field(default=0)\n    train_taken: int = field(default=0)\n    val_taken: int = field(default=0)\n    test_taken: int = field(default=0)\n\n", "\n\n@dataclass\nclass TrainBaseConfig:\n    benign: BaseConfig = field(default_factory=BaseConfig)\n    malicious: BaseConfig = field(default_factory=BaseConfig)\n\n    def __post_init__(self):\n        if isinstance(self.benign, dict):\n            self.benign = BaseConfig(**self.benign)\n        if isinstance(self.malicious, dict):\n            self.malicious = BaseConfig(**self.malicious)", "\n\n@dataclass\nclass TrainCategoryConfig(TrainBaseConfig):\n    captures: Dict[str, TrainBaseConfig] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for k, v in self.captures.items():\n            if isinstance(v, dict):\n                self.captures[k] = TrainBaseConfig(**v)", "\n\n@dataclass\nclass TrainDatasetConfig(TrainBaseConfig):\n    categories: Dict[str, TrainCategoryConfig] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for k, v in self.categories.items():\n            if isinstance(v, dict):\n                self.categories[k] = TrainCategoryConfig(**v)", "\n\n@dataclass\nclass DatasetTrainConfig(TrainBaseConfig):\n    validation_percentage: float = field(default=0.1)\n    test_percentage: float = field(default=0.1)\n    max_to_take: int = field(default=0)\n    datasets: Dict[str, TrainDatasetConfig] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for k, v in self.datasets.items():\n            if isinstance(v, dict):\n                self.datasets[k] = TrainDatasetConfig(**v)", "\n\n@dataclass\nclass DatasetConfig:\n    name: str = field(default=\"\")\n    time_window: int = 0\n    additional_params: Dict[str, Any] = field(default_factory=dict)\n    key_cls: Type[BaseKey] = field(default=None)\n    offline: DatasetTrainConfig = field(default_factory=DatasetTrainConfig)\n    online: DatasetTestConfig = field(default_factory=DatasetTestConfig)\n    attackers: List[Type[BaseKey]] = field(default_factory=list)\n    preprocessed_configs: Dict[str, PreprocessedConfig] = field(\n        default_factory=dict)\n    paths: Dict[str, str] = field(default_factory=dict)\n\n    def __post_init__(self):\n        self.name = \"\"\n        des = set()\n        if isinstance(self.key_cls, str):\n            self.key_cls = str_to_key(self.key_cls)\n        for i, k in enumerate(sorted(self.preprocessed_configs.keys())):\n            if isinstance(self.preprocessed_configs[k], dict):\n                self.preprocessed_configs[k] = PreprocessedConfig(\n                    **self.preprocessed_configs[k])\n            self.name += self.preprocessed_configs[k].family + \"-\"\n            des.add(self.preprocessed_configs[k].detection_engine.__name__)\n            if not self.key_cls:\n                self.key_cls = self.preprocessed_configs[k].key_cls\n            if not self.key_cls == self.preprocessed_configs[k].key_cls:\n                raise Exception(\"Key cls does not match\", self.key_cls,\n                                self.preprocessed_configs[k].key_cls)\n            if not self.time_window:\n                self.time_window = self.preprocessed_configs[k].time_window\n            if not self.time_window == self.preprocessed_configs[k].time_window:\n                raise Exception(\"Time Windows does not match\")\n            if i + 1 == len(self.preprocessed_configs):\n                self.name += self.preprocessed_configs[k].detection_engine.__name__\n        if not DetectionEngine.intersect(des):\n            raise Exception(\"Do not intersect\")\n        if isinstance(self.online, dict):\n            self.online = DatasetTestConfig(**self.online)\n        if isinstance(self.offline, dict):\n            self.offline = DatasetTrainConfig(**self.offline)\n        conf_names = list(self.preprocessed_configs.keys())\n        if not all(\n                self.preprocessed_configs[x].time_window == self.preprocessed_configs[conf_names[0]].time_window\n                for x in conf_names):\n            raise ValueError(\"Non son compatibili TW\")\n        if not all(\n                self.preprocessed_configs[x].additional_params == self.preprocessed_configs[conf_names[0]].additional_params\n                for x in conf_names):\n            raise ValueError(\"Non son compatibili FL\")\n        for i, v in enumerate(self.attackers):\n            tmp = None\n            if isinstance(v, BaseKey):\n                break\n            if not tmp:\n                tmp = next(y for y in all_subclasses(BaseKey)\n                           if len(v) == len(fields(y)) and all(p.name in v for p in fields(y)))\n            self.attackers[i] = tmp.create(**v)", "\n\ndef load_packets(pcap, dataset, category, capture):\n    \"\"\"Method to load all raw packets from a pcap into a buffer\"\"\"\n    _logger.info(f\"Started Loading packets of {pcap}\")\n    init_ts = 0\n    all_pkts = []\n    for i, (ts, pkt) in enumerate(Reader(filename=pcap)):\n        if i == 0:\n            init_ts = ts\n        all_pkts.append((ts - init_ts, pkt, dataset, category, capture))\n    _logger.info(f\"Finished Loading packets of {pcap}\")\n    return all_pkts", "\n\ndef async_combined(unordered, target_dir):\n    \"\"\"Method to sort all packets belonging to the provided pcaps by arrival time and\n    creating a unified capture file\"\"\"\n    _logger.info(\"Start Combined Async load\")\n    pkts = []\n    [pkts.extend(x) for x in unordered]\n    pkts = sorted(pkts, key=lambda x: x[0])\n    tmp = []\n    with Writer(filename=os.path.join(target_dir, \"combined.pcap\")) as w:\n        new_ts = time.time_ns()\n        for i, x in enumerate(pkts):\n            w.write(x[1], ts=new_ts+x[0])\n            tmp.append((x[2], x[3], x[4]))\n            if i % 50000 == 0:\n                _logger.info(f\"Report Combined Async Load {100*i/len(pkts)}%\")\n    with open(os.path.join(target_dir, \"combined.pickle\"), \"wb\") as fp:\n        pickle.dump(tmp, fp)\n    _logger.info(\"Finished Combined Async load\")", "\n\ndef async_join(conf: DatasetTrainConfig, preprocessed: Dict[str, PreprocessedConfig],\n               paths: Dict[str, str], target_dir, de: DetectionEngine):\n    \"\"\"Method to joining all portions of train, validation and test processed data into the final one\"\"\"\n    _logger.info(\"Async Join Start\")\n    for dataset, v in conf.datasets.items():\n        for category, vv in v.categories.items():\n            for capture, vvv in vv.captures.items():\n                for label, ttype in enumerate([\"benign\", \"malicious\"]):\n                    t: BaseConfig = getattr(vvv, ttype)\n                    if not t.taken:\n                        continue\n                    spath = os.path.join(paths[dataset], category, capture)\n                    available = getattr(\n                        preprocessed[dataset].categories[category].captures[capture], ttype)\n                    indexes = random.sample(range(available), t.taken)\n                    de.append_to_dataset(spath, target_dir, ttype, label,\n                                         indexes[:t.train_taken],\n                                         indexes[t.train_taken:t.train_taken +\n                                                 t.val_taken],\n                                         indexes[t.train_taken+t.val_taken:])\n    _logger.info(\"Async Join End\")", "\n\ndef main(args_list):\n    # registering cli args\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        '-b', '--benign', help='preprocessed directories with benign flows', type=str, nargs=\"+\", required=True)\n    parser.add_argument(\n        '-m', '--malicious', help='preprocessed directories with malicious flows', type=str, nargs=\"+\", required=True)\n    parser.add_argument(\n        '-pc', '--per-category', help='per category creation of the dataset', action=\"store_true\")\n    parser.add_argument(\n        '-no', '--no-online', help='no online test', action=\"store_true\")\n    parser.add_argument(\n        '-tp', '--test-percentage', help='percentage of test in dataset', type=float, default=0.1)\n    parser.add_argument(\n        '-vp', '--validation-percentage', help='percentage of validation within the entire train set',\n        type=float, default=0.1)\n    parser.add_argument(\n        '-p', '--parallel', help='number of parallel executions', type=int, default=os.cpu_count())\n    args = parser.parse_args(args_list).__dict__\n\n    conf: DatasetConfig = DatasetConfig()\n    conf.offline.test_percentage = args[\"test_percentage\"]\n    conf.offline.validation_percentage = args[\"validation_percentage\"]\n    detection_engine: DetectionEngine = None\n\n    for c in set(args[\"benign\"] + args[\"malicious\"]):\n        tmp = PreprocessedConfig(\n            **load_json_data(os.path.join(c, \"conf.json\")))\n        conf.preprocessed_configs[tmp.family] = tmp\n        detection_engine = tmp.detection_engine\n        conf.paths[tmp.family] = c\n        conf.attackers += ATTACK_LABELS[tmp.family](\n            tmp.captures_config.path)\n        conf.additional_params = tmp.additional_params\n    conf.__post_init__()\n\n    target_dir = os.path.join(\n        \"datasets\", conf.name, \"{}-{}\".format(\"percategory\" if args[\"per_category\"] else \"combined\",\n                                              \"offline\" if args[\"no_online\"] else \"online\"))\n    create_dir(target_dir, overwrite=False)\n\n    # Chosing portions for the online simulation according to their maximum number\n    # of benign and malicious samples (which are maximised)\n    test_pcaps = []\n    cop = deepcopy(conf.preprocessed_configs)\n    if not args[\"no_online\"]:\n        for ttype in (\"benign\", \"malicious\"):\n            for dpath in args[ttype]:\n                dataset_name = next(\n                    k for k, v in conf.paths.items() if v == dpath)\n                conf.online.datasets[dataset_name] = SingleDatasetTestConfig()\n                for cat, vals in conf.preprocessed_configs[dataset_name].categories.items():\n                    conf.online.datasets[dataset_name].categories[cat] = CategoryConfig(\n                    )\n                    chosen = max(vals.captures, key=lambda x: getattr(\n                        vals.captures[x], ttype))\n                    tmp = cop[dataset_name].categories[cat].captures.pop(\n                        chosen)\n                    conf.online.datasets[dataset_name].categories[cat].captures[chosen] = tmp\n                    test_pcaps.append((os.path.join(\n                        conf.preprocessed_configs[dataset_name].captures_config.path, cat, chosen),\n                        cop[dataset_name].family, cat, chosen))\n                    conf.online.datasets[dataset_name].categories[cat].update(\n                        tmp)\n                    conf.online.datasets[dataset_name].update(tmp)\n                    conf.online.update(tmp)\n\n    with multiprocessing.Pool(maxtasksperchild=1, processes=args[\"parallel\"]) as pool:\n        pkts = None\n        tasks = []\n        if not args[\"no_online\"]:\n            pkts = pool.starmap_async(load_packets, test_pcaps)\n\n        if not args[\"per_category\"]:\n            # Creating balanced train, validation and test portion for training\n            tmp = [(vvv.benign, vvv.malicious) for v in cop.values() for vv in v.categories.values()\n                   for vvv in vv.captures.values()]\n            conf.offline.max_to_take = min(\n                sum([v[0] for v in tmp]), sum([v[1] for v in tmp]))\n            for ttype in (\"benign\", \"malicious\"):\n                asd = {}\n                for dpath in args[ttype]:\n                    dname = next(\n                        k for k, v in conf.paths.items() if v == dpath)\n                    asd.update({(dname, kk): sum([getattr(vvv, ttype) for vvv in vv.captures.values()])\n                                for kk, vv in cop[dname].categories.items()})\n                asd = {k: v for k, v in asd.items() if v}\n                asd = {k: asd[k] for k in sorted(asd, key=asd.get)}\n                macina(conf, asd, cop, ttype)\n\n            tasks.append(pool.apply_async(async_join, (conf.offline, conf.preprocessed_configs, conf.paths,\n                                                       target_dir, detection_engine)))\n            if not args[\"no_online\"]:\n                conf.online.duration = max(vvv.duration for v in conf.online.datasets.values(\n                ) for vv in v.categories.values() for vvv in vv.captures.values())\n            _logger.info(\"Dumping configuration with updated stats\")\n            dump_json_data(conf, os.path.join(target_dir, \"conf.json\"))\n            if not args[\"no_online\"]:\n                pkts = pkts.get()\n                tasks.append(pool.apply_async(\n                    async_combined, (pkts, target_dir)))\n        else:\n            asd = {}\n            for dpath in args[\"benign\"]:\n                dname = next(k for k, v in conf.paths.items() if v == dpath)\n                asd.update({(dname, kk): sum([vvv.benign for vvv in vv.captures.values()])\n                            for kk, vv in cop[dname].categories.items()})\n            for dpath in args[\"malicious\"]:\n                dname = dataset_name = next(\n                    k for k, v in conf.paths.items() if v == dpath)\n                for cat, v in conf.preprocessed_configs[dname].categories.items():\n                    confi: DatasetConfig = deepcopy(conf)\n                    confi.offline.max_to_take = min(\n                        v.malicious, sum(v for v in asd.values()))\n                    macina(confi, asd, cop, \"benign\")\n                    macina(confi, {(dname, cat): v.malicious},\n                           cop, \"malicious\")\n                    _logger.info(\"Dumping configuration with updated stats\")\n                    ts = os.path.join(target_dir, dname, cat)\n                    create_dir(ts)\n                    dump_json_data(confi, os.path.join(ts, \"conf.json\"))\n                    tasks.append(pool.apply_async(async_join, (confi.offline, conf.preprocessed_configs, conf.paths,\n                                                               ts, detection_engine)))\n        _logger.info(\"Waiting for last tasks ...\")\n        for t in tasks:\n            t.get()", "\n\ndef macina(conf: DatasetConfig, asd, cop, ttype):\n    take_for_each_cat = math.floor(conf.offline.max_to_take/len(asd))\n    so_so_far = 0\n    for ii, (dataset_name, cat) in enumerate(asd.keys()):\n        take_for_each_pcap = math.floor(\n            take_for_each_cat/len(cop[dataset_name].categories[cat].captures))\n        if dataset_name not in conf.offline.datasets:\n            conf.offline.datasets[dataset_name] = TrainDatasetConfig()\n        if cat not in conf.offline.datasets[dataset_name].categories:\n            conf.offline.datasets[dataset_name].categories[cat] = TrainCategoryConfig(\n            )\n\n        so_far = 0\n        for i, (name, vals) in enumerate(sorted(cop[dataset_name].categories[cat].captures.items(),\n                                                key=lambda x: getattr(x[1], ttype))):\n            if name not in conf.offline.datasets[dataset_name].categories[cat].captures:\n                conf.offline.datasets[dataset_name].categories[cat].captures[name] = TrainBaseConfig(\n                )\n            taken = min(getattr(vals, ttype), take_for_each_pcap)\n            so_far += taken\n            if taken != take_for_each_pcap and i+1 != len(conf.preprocessed_configs[dataset_name].categories[cat].captures):\n                take_for_each_pcap = math.floor((take_for_each_cat - so_far) / (len(\n                    conf.preprocessed_configs[dataset_name].categories[cat].captures) - i - 1))\n            test_start = math.floor(\n                taken * (1 - conf.offline.test_percentage))\n            val_start = math.floor(\n                test_start * (1 - conf.offline.validation_percentage))\n            tmp = BaseConfig(\n                taken, val_start, test_start - val_start, taken - test_start)\n            setattr(\n                conf.offline.datasets[dataset_name].categories[cat].captures[name], ttype, tmp)\n            getattr(\n                conf.offline.datasets[dataset_name].categories[cat], ttype).update(tmp)\n            getattr(\n                conf.offline.datasets[dataset_name], ttype).update(tmp)\n            getattr(conf.offline, ttype).update(tmp)\n        so_so_far += so_far\n        if so_far != take_for_each_cat and ii+1 != len(asd):\n            take_for_each_cat = math.floor(\n                (conf.offline.max_to_take - so_so_far) / (len(asd)-ii-1))", ""]}
{"filename": "enid/__main__.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport argparse\nimport importlib\nimport os\n\nfrom .lib.definitions import DetectionEngine", "\nfrom .lib.definitions import DetectionEngine\nfrom .lib.utility import camel_to_snake, silence, set_seed\n\nif __name__ == '__main__':\n    silence()\n    set_seed()\n    de_with_main_list = DetectionEngine.list_all(only_main=True)\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"operation\", help=\"Select the operation to perform\", type=str,\n                        choices=de_with_main_list + [x.replace(\".py\", \"\") for x in os.listdir(os.path.dirname(__file__))\n                                                     if not x.startswith(\"_\") and x.endswith(\".py\")\n                                                     and os.path.isfile(os.path.join(os.path.dirname(__file__), x))])\n    parser.add_argument(\"rest\", nargs=argparse.REMAINDER)\n    args = parser.parse_args().__dict__\n    if args[\"operation\"] not in de_with_main_list:\n        mod = importlib.import_module(\".{}\".format(\n            args[\"operation\"]), package=__package__)\n        mod.main(args[\"rest\"])\n    else:\n        args[\"operation\"] = camel_to_snake(args[\"operation\"])\n        parser = argparse.ArgumentParser(\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        parser.add_argument(\"action\", help=\"Main action\", type=str,\n                            choices=[x.replace(\".py\", \"\") for x in os.listdir(\n                                os.path.join(os.path.dirname(__file__), \"lib\", \"engines\", args[\"operation\"], \"main\"))\n                                if not x.startswith(\"_\") and x.endswith(\".py\")\n                                and os.path.isfile(\n                                os.path.join(os.path.dirname(__file__), \"lib\", \"engines\", args[\"operation\"], \"main\", x))])\n        parser.add_argument(\"rest\", nargs=argparse.REMAINDER)\n        args_nested = parser.parse_args(args[\"rest\"]).__dict__\n        mod = importlib.import_module(\".lib.engines.{}.main.{}\".format(\n            args[\"operation\"], args_nested[\"action\"]), package=__package__)\n        mod.main(args_nested[\"rest\"])", ""]}
{"filename": "enid/comparator.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMain program to debug the online test results produced with the\nDebugLevel.ENHANCED. Given a certain threshold of packets provided,\nthis program outputs all the benign flows that have been erroneously\nclassified and their n\u00b0 of following packets blocked, and the malicious", "this program outputs all the benign flows that have been erroneously\nclassified and their n\u00b0 of following packets blocked, and the malicious\nsessions not detected and their n\u00b0 packets that reached the application\nin the following intervals.\n\"\"\"\nimport argparse\nimport os\n\nfrom .datasetter import DatasetConfig\nfrom .lib.identifiers import str_to_key", "from .datasetter import DatasetConfig\nfrom .lib.identifiers import str_to_key\nfrom .lib.utility import load_json_data\n\n\ndef _load_history(path):\n    s = load_json_data(path)\n    for k in s:\n        key_cls = str_to_key(s[k][\"analysis_state\"][\"current_key\"])\n        s[k][\"session_map\"] = {key_cls.create(\n            **j[\"key\"]): j[\"value\"] for j in s[k][\"session_map\"]}\n        s[k][\"black_map\"] = {key_cls.create(\n            **j[\"key\"]): j[\"value\"] for j in s[k][\"black_map\"]}\n    return s", "\n\ndef main(args_list):\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        \"files\", help=\"test history files to compare\", nargs=\"2+\", type=str)\n    parser.add_argument(\"-t\", \"--threshold\", type=int, default=0)\n\n    args = parser.parse_args(args_list).__dict__\n    files = {os.path.basename(x): _load_history(x) for x in args[\"files\"]}\n    malicious = {}\n    [malicious.update(DatasetConfig(**load_json_data(os.path.join(x, os.pardir,\n                      os.pardir, os.pardir, \"conf.json\"))).attackers) for x in args[\"files\"]]\n\n    already_handled = {}\n    timewindows = list(range(0, max(len(x) for x in files.values())))\n\n    for i, k in enumerate(timewindows):\n        seen_sess = set()\n        [seen_sess.update(v[k][\"session_map\"].keys()) for v in files.values()]\n        for s in seen_sess:\n            if s not in already_handled and all(s in v[k][\"session_map\"] for v in files.values()) and\\\n                (not all(v[k][\"session_map\"][s][\"prediction\"] > 0.5 for v in files.values()) or\n                 not all(v[k][\"session_map\"][s][\"prediction\"] < 0.5 for v in files.values())):\n                already_handled[s] = True\n                if getattr(s, \"dataset\", None):\n                    is_malicious_key = s.cast(\n                        malicious[s.dataset][0].__class__) in malicious[s.dataset]\n                else:\n                    is_malicious_key = any(\n                        s.cast(p[0].__class__) in p for p in malicious.values())\n                print(\"---------------------\")\n                print(\"Detected difference in time interval\", k)\n                print(f\"Key (is_ddos={is_malicious_key}):\", s)\n                print()\n                for k, v in files.items():\n                    if v[k][\"session_map\"][s][\"prediction\"] <= 0.5:\n                        continue\n                    fut_handled = sum(\n                        v[tw][\"black_map\"][s] if s in v[tw][\"black_map\"] else 0 for tw in timewindows[i+1:])\n\n                    if fut_handled < args[\"threshold\"]:\n                        continue\n                    print(\"File:\", k)\n                    print(\"SessionValue:\", v[k][\"session_map\"][s])\n                    print(\"Features:\", [(k[\"key\"], k[\"value\"])\n                          for k in v[k][\"analysis_state\"][\"current_features\"][\"value\"]])\n                    print(\"Future Packets Mitigated:\", fut_handled, \"in the following\", len(\n                        timewindows[i+1:]), \"time intervals\")\n                    print()\n                print(\"---------------------\")\n                input()", ""]}
{"filename": "enid/__init__.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"]}
{"filename": "enid/plotter.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMain file for creating plots of the achieved results.\nThis program can create all kinds of plots, according to the\nparameters of the Detection Model used and the configuration of the NIDS.\nIn particular, the following results can be plotted:", "parameters of the Detection Model used and the configuration of the NIDS.\nIn particular, the following results can be plotted:\n1. Models complexity\n2. Models training history (loss and accuracy)\n3. Models features relevances\n4. Models train results\n5. Models train results detailed for each type of granularity (dataset/category/pcap)\n6. Test results of the various configurations\n7. Test results of the various configuration for each type of granularity\n", "7. Test results of the various configuration for each type of granularity\n\nTrain and Test (both offline and online) results can be plotted by condensing parameters\nand creating boxplot, in case of multiple dimensions of the Engine (e.g., packets P and features F)\n\"\"\"\nimport argparse\nimport math\nimport multiprocessing\nimport os\nfrom dataclasses import dataclass", "import os\nfrom dataclasses import dataclass\nfrom multiprocessing.pool import ThreadPool\nfrom typing import Type\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom cycler import cycler\nfrom matplotlib.figure import Figure\nfrom matplotlib.lines import Line2D", "from matplotlib.figure import Figure\nfrom matplotlib.lines import Line2D\n\nfrom .lib.definitions import DetectionEngine, FeaturesHolder\nfrom .lib.metrics import TestMetric, TrainMetric\nfrom .lib.utility import (create_dir, get_logger, handler,\n                          load_json_data, snake_to_camel)\nfrom .trainer import ModelConfig, DatasetConfig\n\n_logger = get_logger(__name__)", "\n_logger = get_logger(__name__)\n\n\n@dataclass\nclass PlotArgs:\n    \"\"\"PlotArgs dataclass to contain personalised arguments for adjusting plots\"\"\"\n    logx: bool = False\n    logy: bool = False\n    fit: bool = False\n    offlegend: bool = False\n    hlegend: bool = False\n    seplegend: bool = False\n    minx: float = None\n    maxx: float = None\n    miny: float = None\n    maxy: float = None\n    xgrid: bool = True\n    ygrid: bool = True\n    figsizex: float = 4\n    figsizey: float = 3\n    ylabelchars: int = 35\n    xlabelchars: int = 35\n    style: str = \"science\"\n\n    def __post_init__(self):\n        plt.style.use(self.style)\n\n    def adjust(self, fig: Figure = None, ax: plt.Axes = None, xlabel: str = None, ylabel: str = None,\n               path: str = None, suptitle: str = None, title: str = None):\n        if ax:\n            if self.xgrid:\n                ax.grid(True, axis='x')\n            if self.ygrid:\n                ax.grid(True, axis='y')\n            if self.logy:\n                ax.set_yscale(\"log\")\n            if self.maxy is not None or self.miny is not None:\n                ax.set_ylim(top=self.maxy, bottom=self.miny)\n            if self.maxx is not None or self.minx is not None:\n                ax.set_xlim(right=self.maxx, left=self.minx)\n            if self.logx:\n                ax.set_xscale(\"log\")\n            if xlabel:\n                ax.set_xlabel(xlabel)\n            if ylabel:\n                ax.set_ylabel(ylabel)\n            if title:\n                ax.set_title(title)\n\n            if self.offlegend:\n                leg = ax.get_legend()\n                if leg:\n                    leg.remove()\n            else:\n                params = {}\n                if self.hlegend:\n                    params = {\n                        \"loc\": \"upper center\",\n                        \"ncol\": math.ceil(math.sqrt(len(ax.lines))) if not self.seplegend else len(ax.lines),\n                        \"bbox_to_anchor\": (0.5, 1.25) if not self.seplegend else None\n                    }\n                else:\n                    params = {\"loc\": \"center left\",\n                              \"ncol\": 1, \"bbox_to_anchor\": (1, 0.5)}\n                if self.seplegend:\n                    params.pop(\"loc\")\n                    params.pop(\"bbox_to_anchor\")\n                ax.legend(**params)\n                if self.seplegend:\n                    label_params = ax.get_legend_handles_labels()\n                    figl = plt.figure(figsize=(3, 2))\n                    figl.legend(*label_params, loc=\"center\", ncol=params[\"ncol\"], bbox_to_anchor=(0.5, 0.5),\n                                fontsize=40, markerscale=4, handlelength=1.5)\n                    figl.savefig(f\"{path}_legend.pdf\",\n                                 bbox_inches='tight', pad_inches=0.0)\n                    plt.close(figl)\n                    ax.get_legend().remove()\n        if fig:\n            if self.figsizex:\n                fig.set_figwidth(self.figsizex)\n            if self.figsizey:\n                fig.set_figheight(self.figsizey)\n            if suptitle:\n                fig.suptitle(suptitle)\n            if path:\n                fig.savefig(f\"{path}.pdf\")\n                plt.close(fig)", "\n\ndef _labelify(params, exclude):\n    \"\"\"Function to transform Detection model parameters into symbols\n    E.g.: features -> f; packets_per_session -> p\n    \"\"\"\n    return '-'.join(f\"{v}\" + \"\\\\textit{\" + k[0] + \"}\" for k, v in params.items() if k != exclude)\n\n\ndef _plottify_metric_name(name: str, max_len):\n    \"\"\"Method to adjust metric name for plotting according to the length provided\"\"\"\n    name = name.replace(\"percentage\", \"_(\\\\%)_\")\n    name = name.replace(\"_per_\", \"_/_\")\n    name = snake_to_camel(name, join_char=' ')\n    for x in (\"Tpr\", \"Tnr\", \"Fpr\", \"Fnr\"):\n        name = name.replace(x, x.upper())\n    if len(name) > max_len:\n        try:\n            post = name.index(\" \", max_len, len(name))\n        except ValueError:\n            post = len(name)\n        name = name[:post] + \"\\n\" + name[post:]\n    return name", "\ndef _plottify_metric_name(name: str, max_len):\n    \"\"\"Method to adjust metric name for plotting according to the length provided\"\"\"\n    name = name.replace(\"percentage\", \"_(\\\\%)_\")\n    name = name.replace(\"_per_\", \"_/_\")\n    name = snake_to_camel(name, join_char=' ')\n    for x in (\"Tpr\", \"Tnr\", \"Fpr\", \"Fnr\"):\n        name = name.replace(x, x.upper())\n    if len(name) > max_len:\n        try:\n            post = name.index(\" \", max_len, len(name))\n        except ValueError:\n            post = len(name)\n        name = name[:post] + \"\\n\" + name[post:]\n    return name", "\n\ndef _plot_features_relevance(c, de: Type[DetectionEngine], path: str, plot_args: PlotArgs):\n    name = de.model_name(**c)\n    _logger.info(f\"Starting plotting relevance of {name}\")\n    rel: FeaturesHolder = de.features_holder_cls(**load_json_data(os.path.join(\n        path, \"models\", name, \"relevance.json\")))\n    if not rel:\n        _logger.info(f\"No relevance for {name}\")\n        return\n    xx, names = [], []\n    for x in de.features_holder_cls.ALLOWED:\n        asd = rel.get_feature_value(x)\n        if asd is not None and not isinstance(asd, (list, tuple)):\n            xx.append(asd)\n            names.append(\"\\\\textbf{\" + x.__name__ + \"}\")\n        else:\n            xx.append(0)\n            names.append(x.__name__)\n    fig, ax = plt.subplots()\n    ax.barh(names, xx)\n    plot_args.adjust(fig=fig, ax=ax, xlabel=\"Activation Score\",\n                     path=os.path.join(path, \"charts\", \"models\", \"features_relevance\", name))\n    _logger.info(f\"Finished plotting relevance of {name}\")", "\n\ndef _plot_train_histories(c, de: DetectionEngine, path: str, plot_args: PlotArgs):\n    name = de.model_name(**c)\n    _logger.info(f\"Starting plotting histories of {name}\")\n    hs = load_json_data(os.path.join(path, \"models\", name, \"history.json\"))\n    if not hs:\n        _logger.info(f\"No history for {name}\")\n        return\n    fig, ax = plt.subplots(2, 1, sharex=True)\n    fig.subplots_adjust(hspace=.0)\n    plot_args.offlegend = True\n    ax[0].plot(hs['loss'], 'b', label=\"Train\")\n    ax[0].plot(hs[\"val_loss\"], 'r', label=\"Validation\")\n    plot_args.adjust(fig=fig, ax=ax[0], ylabel='Loss')\n    ax[1].plot(hs['accuracy'], 'b', label=\"Train\")\n    ax[1].plot(hs[\"val_accuracy\"], 'r', label=\"Validation\")\n    plot_args.adjust(fig=fig, ax=ax[1], xlabel=\"Epocs\", ylabel='Accuracy')\n    handles, labels = ax[0].get_legend_handles_labels()\n    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(\n        0.5, 1), ncol=len(labels), bbox_transform=fig.transFigure)\n    plot_args.adjust(fig=fig, path=os.path.join(\n        path, \"charts\", \"models\", \"histories\", name))\n    _logger.info(f\"Finished plotting histories of {name}\")", "\n\ndef _plot_models_complexity(agg_by_name, models_conf: ModelConfig, path, plot_args: PlotArgs):\n    \"\"\"Function to plot the model complexity in terms of trainable parameters\"\"\"\n    _logger.info(\n        f\"Started plotting complexity using {agg_by_name} as x-axis\")\n    store_path = os.path.join(\n        os.path.dirname(os.path.normpath(path)),\n        \"charts\", \"models\", \"complexity\", f\"trainable_by_{agg_by_name}\")\n    fig, ax = plt.subplots()\n\n    values = sorted(getattr(models_conf.train_params, agg_by_name))\n    values_tick = list(range(len(values)))\n    all_combs = models_conf.train_params.all_train_combs(exclude=agg_by_name)\n    ax.set_prop_cycle(cycler(linestyle=[\":\"]*len(all_combs)) +\n                      cycler(color=[plt.cm.nipy_spectral(i) for i in np.linspace(0, 1, len(all_combs))]) +\n                      cycler(marker=sorted([x for x, v in Line2D.markers.items() if v != \"nothing\" and x not in (\"|\", \"_\")],\n                                           reverse=True, key=lambda x: str(x))[:len(all_combs)]))\n\n    def asd(*args):\n        name = models_conf.detection_engine.model_name(**{\n            agg_by_name: args[1],\n            **args[0]})\n        return args[0], args[1], models_conf.detection_engine.parameters(\n            **load_json_data(os.path.join(path, name, \"params.json\")))\n\n    with ThreadPool() as pool:\n        r = pool.starmap(asd, [(c, v) for c in all_combs for v in values])\n    for c in all_combs:\n        tmp = []\n        for v in values:\n            x = next(x for x in r if x[0] == c and x[1] == v)\n            tmp.append(x[2])\n        ax.plot(values_tick, tmp, label=_labelify(c, agg_by_name))\n\n    ax.set_xticks(values_tick)\n    ax.set_xticklabels(values)\n    ax.tick_params(axis='x', which='minor', bottom=False, top=False)\n    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    plot_args.adjust(fig=fig, ax=ax, xlabel=\"\\\\textit{\" + agg_by_name[0] + \"}\",\n                     ylabel=\"Trainable Parameters\", path=store_path)\n    _logger.info(\n        f\"Finished plotting complexity using {agg_by_name} as x-axis\")", "\n\ndef _plot_results_metrics(agg_by_name, models_conf: ModelConfig,\n                          path, metric, plot_args: PlotArgs, add_name=\"\", depth=tuple()):\n    \"\"\"Function used to print result metrics for both offline and online testing\"\"\"\n    _logger.info(\n        f\"Started plotting metric {metric} depth={depth} using {agg_by_name} as x-axis\")\n    store_name = os.path.join(os.path.dirname(os.path.normpath(\n        path)), \"charts\", os.path.basename(path), \"results\", add_name)\n    for x in depth:\n        store_name = os.path.join(store_name, x)\n    store_name = os.path.join(\n        store_name, f\"total_by_{agg_by_name}\" if agg_by_name else \"\", metric)\n\n    fig, ax = plt.subplots()\n\n    values = sorted(getattr(models_conf.train_params, agg_by_name))\n    m_name = _plottify_metric_name(metric, plot_args.ylabelchars)\n    values_tick = values if plot_args.logx else list(range(len(values)))\n    all_combs = models_conf.train_params.all_train_combs(exclude=agg_by_name)\n    ax.set_prop_cycle(cycler(linestyle=[\":\"]*len(all_combs)) +\n                      cycler(color=[plt.cm.nipy_spectral(i) for i in np.linspace(0, 1, len(all_combs))]) +\n                      cycler(marker=(sorted([x for x, v in Line2D.markers.items() if v != \"nothing\" and x != \"|\"],\n                                            reverse=True, key=lambda x: str(x))*len(all_combs))[:len(all_combs)]))\n    for c in all_combs:\n        tmp = []\n        for v in values:\n            name = models_conf.detection_engine.model_name(\n                **c, **{agg_by_name: v})\n            j = load_json_data(os.path.join(\n                path, name, add_name, \"results.json\"))\n            if not depth:\n                tmp.append(j[metric])\n            elif len(depth) == 1:\n                tmp.append(j[\"datasets\"][depth[0]][metric])\n            elif len(depth) == 2:\n                tmp.append(j[\"datasets\"][depth[0]]\n                           [\"categories\"][depth[1]][metric])\n            else:\n                tmp.append(j[\"datasets\"][depth[0]][\"categories\"]\n                           [depth[1]][\"captures\"][depth[2]][metric])\n        if plot_args.fit:\n            aasd = ax.scatter(values, tmp, label=_labelify(c, agg_by_name))\n            ax.plot(values, np.polyval(np.polyfit(values, tmp, 2),\n                    values), color=aasd.get_facecolor()[0])\n        else:\n            ax.plot(values_tick, tmp, label=_labelify(c, agg_by_name))\n\n    ax.set_xticks(values_tick)\n    ax.set_xticklabels(values)\n    ax.tick_params(axis='x', which='minor', bottom=False, top=False)\n    plot_args.adjust(\n        fig=fig, ax=ax, xlabel=\"\\\\textit{\" + agg_by_name[0] + \"}\", ylabel=m_name, path=store_name)\n    _logger.info(\n        f\"Finished plotting metric {metric} depth={depth} agg by {agg_by_name}\")", "\n\ndef _plot_results_metrics_boxplot(agg_by_name, models_conf: ModelConfig,\n                                  path, metric, test_name, plot_args: PlotArgs, depth=tuple()):\n    \"\"\"Function for plotting resulting metrics condensed \"\"\"\n    _logger.info(f\"Started Plotting boxplot of {metric}\")\n    fig, ax = plt.subplots()\n    vals = []\n    m_name = _plottify_metric_name(metric, plot_args.ylabelchars)\n    asd = sorted(getattr(models_conf.train_params, agg_by_name))\n    asd_tick = asd if plot_args.logx else list(range(len(asd)))\n    for v in sorted(getattr(models_conf.train_params, agg_by_name)):\n        tmp = []\n        for c in [x for x in models_conf.train_params.all_train_combs() if x[agg_by_name] == v]:\n            name = models_conf.detection_engine.model_name(**c)\n            j = load_json_data(os.path.join(\n                path, test_name, name, \"results.json\"))\n            if not depth:\n                tmp.append(j[metric])\n            elif len(depth) == 1:\n                tmp.append(j[\"datasets\"][depth[0]][metric])\n            elif len(depth) == 2:\n                tmp.append(j[\"datasets\"][depth[0]]\n                           [\"categories\"][depth[1]][metric])\n            else:\n                tmp.append(j[\"datasets\"][depth[0]][\"categories\"]\n                           [depth[1]][\"captures\"][depth[2]][metric])\n        vals.append(tmp)\n    ret = ax.boxplot(vals, positions=asd_tick)\n    if plot_args.fit:\n        xs = asd\n        ys = np.array([q.get_ydata()[0] for q in ret['medians']])\n        ys = ys[~np.isnan(ys)]\n        polyval = np.polyval(np.polyfit(xs, ys, 2), xs)\n        ax.plot(asd_tick, [polyval[q] for q in asd_tick])\n    for v, vv, fl in zip(asd, vals, ret['fliers']):\n        off = -0.5\n        for c, vvv in zip([x for x in models_conf.train_params.all_train_combs() if x[agg_by_name] == v], vv):\n            if vvv in fl.get_ydata():\n                ax.text(fl.get_xdata()[0]+off, vvv,\n                        _labelify(c, agg_by_name), va='center', ha='left')\n                off *= -1\n            off = -0.5 if off < 0 else 0.25\n    ax.minorticks_off()\n    ax.set_xticks(asd_tick)\n    ax.set_xticklabels([str(k) for k in asd])\n    path = os.path.join(path, \"charts\", test_name, \"results\")\n    for x in depth:\n        path = os.path.join(path, x)\n    path = os.path.join(path, f\"condensed_by_{agg_by_name}\", metric)\n    plot_args.adjust(\n        fig=fig, ax=ax, xlabel=\"\\\\textit{\" + agg_by_name[0] + \"}\", ylabel=m_name, path=path)\n    _logger.info(f\"Finished Plotting boxplot of {metric} agg by {agg_by_name}\")", "\n\ndef main(args_list):\n    # registering cli parameters\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        'de_models_dir', help='path to the directory containing results', type=str)\n    parser.add_argument(\n        '-mf', '--model-complexity', help='display features relevance', action=\"store_true\")\n    parser.add_argument(\n        '-fr', '--features-relevance', help='display features relevance', action=\"store_true\")\n    parser.add_argument(\n        '-th', '--train-histories', help='display train histories', action=\"store_true\")\n    parser.add_argument(\n        '-tr', '--train-results', help='display train results', action=\"store_true\")\n    parser.add_argument(\n        '-trd', '--train-results-detailed', help='display chart per-capture', action=\"store_true\")\n    parser.add_argument(\n        '-ts', '--test-results', help='display test results', action=\"store_true\")\n    parser.add_argument(\n        '-tsd', '--test-results-detailed', help='display chart per-capture in test', action=\"store_true\")\n    parser.add_argument(\n        '-tl', '--transfer-learning', help='transfer learning', action=\"store_true\")\n    parser.add_argument(\n        '-tld', '--transfer-learning-detailed', help='transfer learning detailed', action=\"store_true\")\n    parser.add_argument(\n        '-b', '--boxplot', help='condense in boxplot', action=\"store_true\")\n    args = parser.parse_args(args_list).__dict__\n\n    dataset_conf: DatasetConfig = DatasetConfig(\n        **load_json_data(os.path.join(args[\"de_models_dir\"], os.pardir, \"conf.json\")))\n    models_conf: ModelConfig = ModelConfig(\n        **load_json_data(os.path.join(args[\"de_models_dir\"], \"models\", \"conf.json\")))\n\n    star_tasks = []\n    plot_args = PlotArgs()\n\n    if args[\"model_complexity\"]:\n        # print complexity of the model in terms of trainable parameters\n        tmp = os.path.join(args[\"de_models_dir\"], \"models\")\n        for k in models_conf.train_params.train_combs():\n            if len(getattr(models_conf.train_params, k)) <= 1:\n                continue\n            create_dir(os.path.join(\n                args[\"de_models_dir\"], \"charts\", \"models\", \"complexity\"))\n            star_tasks.append(\n                (_plot_models_complexity, (k, models_conf, tmp, plot_args)))\n\n    if args[\"train_histories\"] and models_conf:\n        # print train histories, showing loss and accuracy of the train vs validation set\n        create_dir(os.path.join(\n            args[\"de_models_dir\"], \"charts\", \"models\", \"histories\"))\n        [star_tasks.append((_plot_train_histories, (c, models_conf.detection_engine, args[\"de_models_dir\"], plot_args)))\n         for c in models_conf.train_params.all_train_combs()]\n\n    if args[\"features_relevance\"] and models_conf:\n        # print relevance of each feature in each configuration of the NIDS\n        create_dir(os.path.join(\n            args[\"de_models_dir\"], \"charts\", \"models\", \"features_relevance\"))\n        [star_tasks.append((_plot_features_relevance, (c, models_conf.detection_engine, args[\"de_models_dir\"], plot_args)))\n         for c in models_conf.train_params.all_train_combs()]\n\n    if args[\"train_results\"]:\n        # print train results for each parameter (e.g., feature F in the x-axes, packets P in the x-axes)\n        tmp = os.path.join(args[\"de_models_dir\"], \"models\")\n        for k in models_conf.train_params.train_combs():\n            if len(getattr(models_conf.train_params, k)) <= 1:\n                continue\n            create_dir(os.path.join(\n                args[\"de_models_dir\"], \"charts\", \"models\", \"results\", f\"total_by_{k}\"))\n            [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args)))\n                for m in TrainMetric.get_metrics()]\n\n            if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                       for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                # print train results condensing plots by the parameters of interest\n                # (e.g., each box in the boxplot refers to a specific features F' and all models with that F'\n                # but other different parameters, such as all packets P)\n                create_dir(os.path.join(\n                    args[\"de_models_dir\"], \"charts\", \"models\", \"results\", f\"condensed_by_{k}\"))\n                [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                    m, \"models\", plot_args)))\n                 for m in TrainMetric.get_metrics()]\n\n    if args[\"train_results_detailed\"]:\n        tmp = os.path.join(args[\"de_models_dir\"], \"models\")\n        for k in models_conf.train_params.train_combs():\n            if len(getattr(models_conf.train_params, k)) <= 1:\n                continue\n            # print train results detailed at the DATASET granularity for each parameter\n            # (e.g., feature F in the x-axes, packets P in the x-axes)\n            for dataset_name, v in dataset_conf.offline.datasets.items():\n                create_dir(os.path.join(\n                    args[\"de_models_dir\"], \"charts\", \"models\", \"results\", dataset_name, f\"total_by_{k}\"))\n                [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args, \"\", (dataset_name,))))\n                    for m in TrainMetric.get_metrics()]\n\n                # print train results detailed at the DATASET granularity condensing plots by the parameters of interest\n                # (e.g., each box in the boxplot refers to a specific features F' and all models with that F'\n                # but other different parameters, such as all packets P)\n                if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                           for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                    create_dir(os.path.join(\n                        args[\"de_models_dir\"], \"charts\", \"models\", \"results\", dataset_name, f\"condensed_by_{k}\"))\n                    [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                        m, \"models\", plot_args, (dataset_name,))))\n                     for m in TrainMetric.get_metrics()]\n\n                # print train results detailed at the CATEGORY granularity for each parameter\n                # (e.g., feature F in the x-axes, packets P in the x-axes)\n                for c, vv in v.categories.items():\n                    create_dir(os.path.join(\n                        args[\"de_models_dir\"], \"charts\", \"models\", \"results\", dataset_name, c, f\"total_by_{k}\"))\n                    [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args, \"\", (dataset_name, c))))\n                        for m in TrainMetric.get_metrics()]\n\n                    # print train results detailed at the CATEGORY granularity condensing plots by the parameters of interest\n                    # (e.g., each box in the boxplot refers to a specific features F' and all models with that F'\n                    # but other different parameters, such as all packets P)\n                    if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                               for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                        create_dir(os.path.join(\n                            args[\"de_models_dir\"], \"charts\", \"models\", \"results\", dataset_name, c, f\"condensed_by_{k}\"))\n                        [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                            m, \"models\", plot_args, (dataset_name, c))))\n                         for m in TrainMetric.get_metrics()]\n\n                    # print train results detailed at the PCAP granularity for each parameter\n                    # (e.g., feature F in the x-axes, packets P in the x-axes)\n                    for capture in vv.captures:\n                        create_dir(os.path.join(\n                            args[\"de_models_dir\"], \"charts\", \"models\", \"results\", dataset_name, c, capture, f\"total_by_{k}\"))\n                        [star_tasks.append((_plot_results_metrics,\n                                            (k, models_conf, tmp, m, plot_args, \"\", (dataset_name, c, capture))))\n                            for m in TrainMetric.get_metrics()]\n                        # print train results detailed at the PCAP granularity condensing plots by the parameters of interest\n                        # (e.g., each box in the boxplot refers to a specific features F' and all models with that F'\n                        # but other different parameters, such as all packets P)\n                        if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                                   for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                            create_dir(os.path.join(\n                                args[\"de_models_dir\"], \"charts\", \"models\", \"results\", dataset_name,\n                                c, capture, f\"condensed_by_{k}\"))\n                            [star_tasks.append((_plot_results_metrics_boxplot,\n                                                (k, models_conf, args[\"de_models_dir\"],\n                                                 m, \"models\", plot_args, (dataset_name, c, capture))))\n                             for m in TrainMetric.get_metrics()]\n\n    if args[\"test_results\"]:\n        # look for folder with results of either one of the two test types (NORMAL and THROUGHPUT)\n        for x in os.listdir(args[\"de_models_dir\"]):\n            tmp = os.path.join(args[\"de_models_dir\"], x)\n            if not os.path.isdir(tmp) or (x != \"normal_test\" and x != \"throughput_test\") or\\\n                    not os.path.isfile(os.path.join(tmp, \"conf.json\")):\n                continue\n\n            # print test results for each parameter (e.g., feature F in the x-axes, packets P in the x-axes)\n            for k in models_conf.train_params.train_combs():\n                if len(getattr(models_conf.train_params, k)) <= 1:\n                    continue\n                create_dir(os.path.join(\n                    args[\"de_models_dir\"], \"charts\", x, \"results\", f\"total_by_{k}\"))\n                [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args)))\n                    for m in TestMetric.get_metrics()]\n\n                # print test results condensing plots by the parameters of interest\n                # (e.g., each box in the boxplot refers to a specific features F' and all models with that F'\n                # but other different parameters, such as all packets P)\n                if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                           for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                    create_dir(os.path.join(\n                        args[\"de_models_dir\"], \"charts\", x, \"results\", f\"condensed_by_{k}\"))\n                    [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                        m, x, plot_args)))\n                     for m in TestMetric.get_metrics()]\n\n    if args[\"test_results_detailed\"]:\n        # look for folder with results of either one of the two test types (NORMAL and THROUGHPUT)\n        for x in os.listdir(args[\"de_models_dir\"]):\n            tmp = os.path.join(args[\"de_models_dir\"], x)\n            if not os.path.isdir(tmp) or (x != \"normal_test\" and x != \"throughput_test\") or\\\n                    not os.path.isfile(os.path.join(tmp, \"conf.json\")):\n                continue\n\n            # print test results for each parameter (e.g., feature F in the x-axes, packets P in the x-axes)\n            for k in models_conf.train_params.train_combs():\n                if len(getattr(models_conf.train_params, k)) <= 1:\n                    continue\n                for dataset_name, v in dataset_conf.online.datasets.items():\n                    create_dir(os.path.join(\n                        args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, f\"total_by_{k}\"))\n                    [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args, \"\", (dataset_name,))))\n                        for m in TestMetric.get_metrics()]\n\n                    # print test results detailed at the DATASET granularity condensing plots by the parameters of interest\n                    # (e.g., each box in the boxplot refers to a specific features F' and all models with that F'\n                    # but other different parameters, such as all packets P)\n                    if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                               for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                        create_dir(os.path.join(\n                            args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, f\"condensed_by_{k}\"))\n                        [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                            m, x, plot_args, (dataset_name,))))\n                         for m in TestMetric.get_metrics()]\n\n                    # print test results detailed at the CATEGORY granularity for each parameter\n                    # (e.g., feature F in the x-axes, packets P in the x-axes)\n                    for c, vv in v.categories.items():\n                        create_dir(os.path.join(\n                            args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, c, f\"total_by_{k}\"))\n                        [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args, \"\", (dataset_name, c))))\n                            for m in TestMetric.get_metrics()]\n\n                        # print test results detailed at the CATEGORY granularity condensing plots by the parameters\n                        # of interest (e.g., each box in the boxplot refers to a specific features F' and all models\n                        # with that F' but other different parameters, such as all packets P)\n                        if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                                   for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                            create_dir(os.path.join(\n                                args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, c, f\"condensed_by_{k}\"))\n                            [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                                m, x, plot_args, (dataset_name, c))))\n                             for m in TestMetric.get_metrics()]\n\n                        # print test results detailed at the PCAP granularity for each parameter\n                        # (e.g., feature F in the x-axes, packets P in the x-axes)\n                        for capture in vv.captures:\n                            create_dir(os.path.join(\n                                args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, c, capture, f\"total_by_{k}\"))\n                            [star_tasks.append((_plot_results_metrics,\n                                                (k, models_conf, tmp, m, plot_args, \"\", (dataset_name, c, capture))))\n                                for m in TestMetric.get_metrics()]\n                            # print test results detailed at the PCAP granularity condensing plots by the parameters\n                            # of interest (e.g., each box in the boxplot refers to a specific features F' and\n                            # all models with that F' but other different parameters, such as all packets P)\n                            if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                                       for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                                create_dir(os.path.join(\n                                    args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name,\n                                    c, capture, f\"condensed_by_{k}\"))\n                                [star_tasks.append((_plot_results_metrics_boxplot,\n                                                    (k, models_conf, args[\"de_models_dir\"],\n                                                     m, x, plot_args, (dataset_name, c, capture))))\n                                 for m in TestMetric.get_metrics()]\n\n    if args[\"transfer_learning\"]:\n        # look for folder with results of either a transfer learning\n        for x in os.listdir(args[\"de_models_dir\"]):\n            tmp = os.path.join(args[\"de_models_dir\"], x)\n            if not os.path.isdir(tmp) or not x.startswith(\"transfer\"):\n                continue\n\n            # print test results for each parameter (e.g., feature F in the x-axes, packets P in the x-axes)\n            for k in models_conf.train_params.train_combs():\n                if len(getattr(models_conf.train_params, k)) <= 1:\n                    continue\n                create_dir(os.path.join(\n                    args[\"de_models_dir\"], \"charts\", x, \"results\", f\"total_by_{k}\"))\n                [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args)))\n                    for m in TrainMetric.get_metrics()]\n\n                # print test results condensing plots by the parameters of interest\n                # (e.g., each box in the boxplot refers to a specific features F' and all models with that F'\n                # but other different parameters, such as all packets P)\n                if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                           for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                    create_dir(os.path.join(\n                        args[\"de_models_dir\"], \"charts\", x, \"results\", f\"condensed_by_{k}\"))\n                    [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                        m, x, plot_args)))\n                     for m in TrainMetric.get_metrics()]\n\n    if args[\"transfer_learning_detailed\"]:\n        # look for folder with results of either one of the two test types (NORMAL and THROUGHPUT)\n        for x in os.listdir(args[\"de_models_dir\"]):\n            tmp = os.path.join(args[\"de_models_dir\"], x)\n            if not os.path.isdir(tmp) or not x.startswith(\"transfer\"):\n                continue\n\n            other_conf = DatasetConfig(\n                **load_json_data(os.path.join(tmp, \"conf.json\")))\n            # print test results for each parameter (e.g., feature F in the x-axes, packets P in the x-axes)\n            for k in models_conf.train_params.train_combs():\n                if len(getattr(models_conf.train_params, k)) <= 1:\n                    continue\n\n                for dataset_name, v in other_conf.offline.datasets.items():\n                    create_dir(os.path.join(\n                        args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, f\"total_by_{k}\"))\n                    [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args, \"\", (dataset_name,))))\n                        for m in TrainMetric.get_metrics()]\n\n                    # print test results detailed at the DATASET granularity condensing plots by the parameters of interest\n                    # (e.g., each box in the boxplot refers to a specific features F' and all models with that F'\n                    # but other different parameters, such as all packets P)\n                    if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                               for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                        create_dir(os.path.join(\n                            args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, f\"condensed_by_{k}\"))\n                        [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                            m, x, plot_args, (dataset_name,))))\n                         for m in TrainMetric.get_metrics()]\n\n                    # print test results detailed at the CATEGORY granularity for each parameter\n                    # (e.g., feature F in the x-axes, packets P in the x-axes)\n                    for c, vv in v.categories.items():\n                        create_dir(os.path.join(\n                            args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, c, f\"total_by_{k}\"))\n                        [star_tasks.append((_plot_results_metrics, (k, models_conf, tmp, m, plot_args, \"\", (dataset_name, c))))\n                            for m in TrainMetric.get_metrics()]\n\n                        # print test results detailed at the CATEGORY granularity condensing plots by the parameters\n                        # of interest (e.g., each box in the boxplot refers to a specific features F' and all models\n                        # with that F' but other different parameters, such as all packets P)\n                        if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                                   for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                            create_dir(os.path.join(\n                                args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, c, f\"condensed_by_{k}\"))\n                            [star_tasks.append((_plot_results_metrics_boxplot, (k, models_conf, args[\"de_models_dir\"],\n                                                                                m, x, plot_args, (dataset_name, c))))\n                             for m in TrainMetric.get_metrics()]\n\n                        # print test results detailed at the PCAP granularity for each parameter\n                        # (e.g., feature F in the x-axes, packets P in the x-axes)\n                        for capture in vv.captures:\n                            create_dir(os.path.join(\n                                args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name, c, capture, f\"total_by_{k}\"))\n                            [star_tasks.append((_plot_results_metrics,\n                                                (k, models_conf, tmp, m, plot_args, \"\", (dataset_name, c, capture))))\n                                for m in TrainMetric.get_metrics()]\n                            # print test results detailed at the PCAP granularity condensing plots by the parameters\n                            # of interest (e.g., each box in the boxplot refers to a specific features F' and\n                            # all models with that F' but other different parameters, such as all packets P)\n                            if args[\"boxplot\"] and sum(len(getattr(models_conf.train_params, kk))\n                                                       for kk in models_conf.train_params.train_combs() if kk != k) > 1:\n                                create_dir(os.path.join(\n                                    args[\"de_models_dir\"], \"charts\", x, \"results\", dataset_name,\n                                    c, capture, f\"condensed_by_{k}\"))\n                                [star_tasks.append((_plot_results_metrics_boxplot,\n                                                    (k, models_conf, args[\"de_models_dir\"],\n                                                     m, x, plot_args, (dataset_name, c, capture))))\n                                 for m in TrainMetric.get_metrics()]\n\n    with multiprocessing.Pool(maxtasksperchild=1) as pool:\n        pool.starmap(handler, star_tasks)", ""]}
{"filename": "enid/preprocesser.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMain file for Processing input network captures into data according to the specified\nDetection Model and parameters. This creates the files ready to be used for the creation of the dataset.\nFor each capture/category/dataset, this program records the n\u00b0 of benign and malicious samples and\nthe respective network packets, information used later during the dataset creation.", "For each capture/category/dataset, this program records the n\u00b0 of benign and malicious samples and\nthe respective network packets, information used later during the dataset creation.\nA key for grouping sessions needs to be chosen, whether considering only L3 or also L4.\n\"\"\"\nimport argparse\nimport multiprocessing\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, Type\n", "from typing import Any, Dict, Type\n\nfrom .lib import ATTACK_LABELS\nfrom .lib.definitions import DetectionEngine, TestType, TrafficAnalyser\nfrom .lib.identifiers import BaseKey, str_to_key\nfrom .lib.utility import (UpdatableDataclass, add_param_to_parser,\n                          all_subclasses, create_dir, dump_json_data,\n                          get_logger, get_param_for_method, load_json_data)\nfrom .splitter import CaptureConfig\n", "from .splitter import CaptureConfig\n\n_logger = get_logger(__name__)\n\n\n@dataclass\nclass PcapConfig:\n    benign: int = 0\n    malicious: int = 0\n    unique_benign: int = 0\n    unique_malicious: int = 0\n    benign_packets: int = 0\n    malicious_packets: int = 0\n    duration: int = 0", "\n\n@dataclass\nclass CategoryConfig(PcapConfig, UpdatableDataclass):\n    captures: Dict[str, PcapConfig] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for k in list(self.captures.keys()):\n            self.captures[k] = PcapConfig(**self.captures[k])\n", "\n\n@dataclass\nclass PreprocessedConfig(PcapConfig, UpdatableDataclass):\n    family: str = \"\"\n    time_window: int = 0\n    key_cls: Type[BaseKey] = None\n    detection_engine: Type[DetectionEngine] = None\n    additional_params: Dict[str, Any] = field(default_factory=dict)\n    categories: Dict[str, CategoryConfig] = field(default_factory=dict)\n    captures_config: CaptureConfig = field(default_factory=CaptureConfig)\n\n    def __post_init__(self):\n        if isinstance(self.captures_config, dict):\n            self.captures_config = CaptureConfig(**self.captures_config)\n        for k in list(self.categories.keys()):\n            self.categories[k] = CategoryConfig(**self.categories[k])\n        if isinstance(self.detection_engine, str):\n            self.detection_engine = DetectionEngine.import_de(\n                self.detection_engine)\n        if isinstance(self.key_cls, str):\n            self.key_cls = str_to_key(self.key_cls)", "\n\ndef process_pcap(target_dir, cap, dataset, cat, pcap, time_window, additional_params,\n                 attackers, de: Type[DetectionEngine], key_cls: Type[BaseKey]):\n    \"\"\"Function for parsing a pcap\"\"\"\n    target_ds = os.path.join(target_dir, cat, pcap)\n    pcap_file = os.path.join(cap, cat, pcap)\n    if not os.path.isfile(pcap_file):\n        return None\n\n    t = de.traffic_analyser_cls(\n        de, attackers, time_window, key_cls, TestType.PROCESSING,\n        dump_path=target_ds,\n        **additional_params)\n\n    _logger.info(f\"Starting processing {pcap_file}\")\n    # Creating a fake traffic analysers with no Filtering and Classificator components.\n    tot_time = TrafficAnalyser.generate_packets(pcap_file, t, pcap_file, labels=(dataset, cat, pcap))\n    _logger.info(f\"Finished processing {pcap_file}\")\n    p = PcapConfig(t.processing_stats.tot_benign, t.processing_stats.tot_malicious,\n                   t.processing_stats.unique_benign, t.processing_stats.unique_malicious,\n                   t.processing_stats.tot_benign_packets, t.processing_stats.tot_malicious_packets,\n                   duration=tot_time)\n    return target_dir, cat, pcap, p", "\n\ndef main(args_list):\n    # registering cli args, shown with the -h flag\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('time_window',\n                        help='time window of interest in nanoseconds', type=int)\n    parser.add_argument('key',\n                        help='Session Key to use', choices=[x.__name__ for x in all_subclasses(BaseKey)], type=str)\n    parser.add_argument(\n        '-p', '--parallel', help='number of parallel executions', type=int, default=os.cpu_count())\n    parser.add_argument(\"detection_engine\", help=\"Select the detection engine to use\", type=str,\n                        choices=DetectionEngine.list_all())\n    parser.add_argument(\"rest\", nargs=argparse.REMAINDER)\n\n    args = parser.parse_args(args_list).__dict__\n\n    de: Type[DetectionEngine] = DetectionEngine.import_de(\n        args[\"detection_engine\"])\n\n    # registering Detection Model - specific arguments\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    params_for_method = get_param_for_method(\n        de.model_name, exclude_super_cls=DetectionEngine)\n    for pname, (ptype, pdef) in params_for_method.items():\n        add_param_to_parser(parser, pname, ptype, pdef, \"Parameter\")\n    parser.add_argument(\n        'captures', help='capture directories', type=str, nargs=\"+\")\n    args.update(parser.parse_args(args[\"rest\"]).__dict__)\n\n    additional_params = {k: args[k] for k in params_for_method}\n\n    key_cls = str_to_key(args[\"key\"])\n    star_tasks = []\n    prep_configs: Dict[str, PreprocessedConfig] = {}\n\n    for cap in args[\"captures\"]:\n        dataset = os.path.basename(os.path.normpath(cap))\n        output_name = \"{}-{}t-{}\".format(\n            de.model_name(**additional_params), args[\"time_window\"], dataset)\n        target_dir = os.path.join(\n            \"preprocessed\", args[\"detection_engine\"], output_name)\n        create_dir(target_dir, overwrite=False)\n\n        malicious = ATTACK_LABELS[dataset](cap)\n\n        capture_conf = load_json_data(\n            os.path.join(cap, \"conf.json\"), fail=False)\n        if not capture_conf:\n            capture_conf = CaptureConfig(path=cap)\n\n        prep_configs[target_dir] = PreprocessedConfig(\n            key_cls=key_cls,\n            family=dataset,\n            additional_params=additional_params, time_window=args[\"time_window\"],\n            detection_engine=args[\"detection_engine\"], categories={},\n            captures_config=capture_conf)\n\n        for cat in os.listdir(cap):\n            if not os.path.isdir(os.path.join(cap, cat)):\n                continue\n            create_dir(os.path.join(target_dir, cat), overwrite=False)\n            for pcap in os.listdir(os.path.join(cap, cat)):\n                if not pcap.endswith(\".pcap\"):\n                    continue\n                star_tasks.append((target_dir, cap, dataset, cat, pcap, args[\"time_window\"],\n                                   additional_params, malicious, de, key_cls))\n\n    with multiprocessing.Pool(maxtasksperchild=1, processes=args[\"parallel\"]) as pool:\n        # concatenate all results of all launched processes\n        for ret in pool.starmap(process_pcap, star_tasks):\n            if not ret:\n                continue\n            td, cat, pcap, p = ret\n            if cat not in prep_configs[td].categories:\n                prep_configs[td].categories[cat] = CategoryConfig()\n            prep_configs[td].categories[cat].captures[pcap] = p\n            prep_configs[td].categories[cat].update(p)\n            prep_configs[td].update(p)\n\n    for td, val in prep_configs.items():\n        _logger.info(f\"Dumping {td} configuration with updated pcaps stats\")\n        dump_json_data(val, os.path.join(td, \"conf.json\"))", ""]}
{"filename": "enid/online.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMain file for Testing the generated models' configurations.\nThis file executes in parallel on different processes the online\ntesting methodology with the Traffic Filter, the Feature Extractor,\nand the Classifier all active. For each configuration, the entire pipeline", "testing methodology with the Traffic Filter, the Feature Extractor,\nand the Classifier all active. For each configuration, the entire pipeline\nis adjusted accordingly to the set of parameters involved.\n\"\"\"\nimport argparse\nimport multiprocessing\nimport os\n\nfrom dataclasses import dataclass, field\n", "from dataclasses import dataclass, field\n\nfrom .lib.definitions import TestType, TrafficAnalyser, DebugLevel\nfrom .lib.utility import create_dir, dump_json_data, load_json_data\nfrom .trainer import DatasetConfig, ModelConfig\n\n\n@dataclass\nclass TestConfig:\n    debug: DebugLevel = DebugLevel.NONE\n    enforce_timewindows_delay: int = field(default=1)\n    sessions: int = None\n    is_throughput: bool = False\n    is_adaptiveness_enabled: bool = field(default=False)\n\n    def __post_init__(self):\n        if isinstance(self.debug, int):\n            self.debug = DebugLevel(self.debug)", "class TestConfig:\n    debug: DebugLevel = DebugLevel.NONE\n    enforce_timewindows_delay: int = field(default=1)\n    sessions: int = None\n    is_throughput: bool = False\n    is_adaptiveness_enabled: bool = field(default=False)\n\n    def __post_init__(self):\n        if isinstance(self.debug, int):\n            self.debug = DebugLevel(self.debug)", "\n\ndef _run_async_adaptiveness(models_conf: ModelConfig, dataset_conf: DatasetConfig, test_conf: TestConfig, basedir, c):\n\n    name = models_conf.detection_engine.model_name(**c)\n    create_dir(os.path.join(basedir, name))\n    t = models_conf.detection_engine.traffic_analyser_cls(\n        models_conf.detection_engine,\n        dataset_conf.attackers,\n        dataset_conf.time_window,\n        dataset_conf.key_cls,\n        TestType.THROUGHPUT if test_conf.is_throughput else TestType.NORMAL,\n        models_dir=os.path.join(basedir, os.pardir, \"models\"),\n        debug=test_conf.debug,\n        enforce_timewindows_delay=test_conf.enforce_timewindows_delay,\n        is_adaptiveness_enabled=test_conf.is_adaptiveness_enabled,\n        sessions_per_timewindow=test_conf.sessions,\n        **c)\n    TrafficAnalyser.generate_packets(os.path.join(\n        basedir, os.pardir, os.pardir, \"combined.pcap\"), t)\n    dump_json_data(t.results, os.path.join(basedir, name, \"results.json\"))\n    if test_conf.debug != DebugLevel.NONE:\n        dump_json_data(t.debug_data, os.path.join(\n            basedir, name, \"history.json\"))", "\n\ndef main(args_list):\n    # registering cli parameters, which can be shown with the -h\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        'de_models_dir', help='path to the model directory containing the models to test')\n    parser.add_argument(\n        '-d', '--debug', help='debug mode', type=int, default=0)\n    parser.add_argument(\n        '-dd', '--detection-delay', help='number of time windows to wait before performing detection+mitigation',\n        type=int, default=0)\n    parser.add_argument(\n        '-s', '--sessions', help='number of monitored sessions at once', type=int, default=None)\n    parser.add_argument(\n        '-t', '--throughput', help='test throughput', action=\"store_true\")\n    parser.add_argument(\n        '-a', '--adaptiveness', help='enable adaptiveness to auto adjust', action=\"store_true\")\n    parser.add_argument(\n        '-p', '--parallel', help='number of parallel executions', type=int, default=os.cpu_count())\n\n    args = parser.parse_args(args_list).__dict__\n\n    # loading configurations\n    models_config = ModelConfig(\n        **load_json_data(os.path.join(args[\"de_models_dir\"], \"models\", \"conf.json\")))\n\n    dataset_config = DatasetConfig(\n        **load_json_data(os.path.join(args[\"de_models_dir\"], os.pardir, \"conf.json\")))\n\n    test_config = TestConfig(\n        args[\"debug\"], args[\"detection_delay\"], args[\"sessions\"], args[\"throughput\"], args[\"adaptiveness\"])\n\n    dir_basename = os.path.join(\n        args[\"de_models_dir\"], \"throughput_test\" if test_config.is_throughput else \"normal_test\")\n\n    create_dir(dir_basename, overwrite=False)\n\n    dump_json_data(test_config, os.path.join(dir_basename, \"conf.json\"))\n\n    # create an async process to test each configuration standalone\n    with multiprocessing.Pool(maxtasksperchild=1, processes=args[\"parallel\"]) as pool:\n        pool.starmap(_run_async_adaptiveness, [\n            (models_config, dataset_config, test_config, dir_basename, c)\n            for c in models_config.train_params.all_train_combs()])", ""]}
{"filename": "enid/splitter.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMain file for Splitting the provided traffic captures in a smaller size.\nEach directory is considered a category containing certain type of traffic\n(e.g., benign-traffic, malicious-ddos, malicious-sqli).\nEach pcap is split in smaller pcaps of the provided size: is its dimension is not", "(e.g., benign-traffic, malicious-ddos, malicious-sqli).\nEach pcap is split in smaller pcaps of the provided size: is its dimension is not\nat least 3 times the provided size, then the pcap is split in 3 using its own dimension.\nThis is to prevent having unfair split of the pcap within the Train, Val, and Test set\nof the dataset.\n\"\"\"\nimport argparse\nimport math\nimport multiprocessing\nimport os", "import multiprocessing\nimport os\nfrom dataclasses import dataclass\n\nfrom pypacker.ppcap import Reader, Writer\n\nfrom .lib.utility import create_dir, dump_json_data, get_logger\n\n_logger = get_logger(__name__)\n", "_logger = get_logger(__name__)\n\n\n@dataclass\nclass CaptureConfig:\n    path: str\n    size: int = 0\n\n\n# could have used tcpdump -r {} -w {} -C {}, but don't want external dependencies\ndef _splitter(src_pcap, dst_pcap, pcap_size):\n    i = curr_bytes = dump_bytes = 0\n    buf = []\n    w = Writer(f\"{dst_pcap}{i}.pcap\")\n    _logger.info(\"Splitting {} in {}\".format(src_pcap, pcap_size))\n    for ts, raw in Reader(src_pcap):\n        buf.append((ts, raw))\n        curr_bytes += len(raw) + 16  # 16 bytes of timestamp\n        dump_bytes += len(raw) + 16\n        if dump_bytes > 2 * 1024**2 or curr_bytes >= pcap_size:  # dump data every 1MB of buffer\n            for x, y in buf:\n                w.write(y, ts=x)\n            buf.clear()\n            dump_bytes = 0\n        if curr_bytes >= pcap_size:\n            w.close()\n            curr_bytes = 0\n            i += 1\n            w = Writer(f\"{dst_pcap}{i}.pcap\")\n    if buf:\n        for x, y in buf:\n            w.write(y, ts=x)\n    w.close()\n    _logger.info(\"Finished {}\".format(src_pcap))", "\n# could have used tcpdump -r {} -w {} -C {}, but don't want external dependencies\ndef _splitter(src_pcap, dst_pcap, pcap_size):\n    i = curr_bytes = dump_bytes = 0\n    buf = []\n    w = Writer(f\"{dst_pcap}{i}.pcap\")\n    _logger.info(\"Splitting {} in {}\".format(src_pcap, pcap_size))\n    for ts, raw in Reader(src_pcap):\n        buf.append((ts, raw))\n        curr_bytes += len(raw) + 16  # 16 bytes of timestamp\n        dump_bytes += len(raw) + 16\n        if dump_bytes > 2 * 1024**2 or curr_bytes >= pcap_size:  # dump data every 1MB of buffer\n            for x, y in buf:\n                w.write(y, ts=x)\n            buf.clear()\n            dump_bytes = 0\n        if curr_bytes >= pcap_size:\n            w.close()\n            curr_bytes = 0\n            i += 1\n            w = Writer(f\"{dst_pcap}{i}.pcap\")\n    if buf:\n        for x, y in buf:\n            w.write(y, ts=x)\n    w.close()\n    _logger.info(\"Finished {}\".format(src_pcap))", "\n\ndef main(args_list):\n    # registering cli parameters, to be shown with the -h flag\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        'path', help='capture directory', type=str)\n    parser.add_argument(\n        '-s', '--size', help='size to truncate in bytes', type=int, default=300*1024**2)\n    args = parser.parse_args(args_list).__dict__\n\n    conf = CaptureConfig(path=args[\"path\"], size=args[\"size\"])\n\n    dump_json_data(conf, os.path.join(conf.path, \"conf.json\"))\n\n    pcaps = [x.replace(\".pcap\", \"\")\n             for x in os.listdir(conf.path) if x.endswith(\".pcap\")]\n\n    star_tasks = []\n    for cat in pcaps:\n        dst_dir = os.path.join(conf.path, cat)\n        create_dir(dst_dir, overwrite=True)\n        src_pcap = os.path.join(conf.path, \"{}.pcap\".format(cat))\n        dst_pcap = os.path.join(dst_dir, cat)\n        pcap_size = os.path.getsize(src_pcap)\n        # if pcap size is not at least 3 times the provided size, then\n        # split the pcap in 3 according to the pcap size/3\n        # otherwise, split pcaps using the provided size\n        if pcap_size / conf.size < 3:\n            pcap_size = math.ceil(pcap_size/3)\n        else:\n            pcap_size = conf.size\n        star_tasks.append((src_pcap, dst_pcap, pcap_size))\n\n    with multiprocessing.Pool(maxtasksperchild=1) as pool:\n        pool.starmap(_splitter, star_tasks)", ""]}
{"filename": "enid/trainer.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"_\nMain file for Training the desired Detection Models.\nGiven the combination of hyperparameters and iterable parameters\nfor generating models with less information (e.g., maximum number of packets P and\nfeatures extracted F), this file will invoke the train method of the", "for generating models with less information (e.g., maximum number of packets P and\nfeatures extracted F), this file will invoke the train method of the\nDetection Models untill all models are created. Every generated model is tested\noffline against the test portion of the chosen dataset, registering results at different\ngranularities (i.e., for the entire dataset, for the single category of captures,\nfor the single pcap).\n\"\"\"\nimport argparse\nimport os\nfrom dataclasses import dataclass, field", "import os\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Type\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .datasetter import DatasetConfig, DatasetTrainConfig\nfrom .lib.definitions import DeParams, DetectionEngine\nfrom .lib.metrics import TrainMetric", "from .lib.definitions import DeParams, DetectionEngine\nfrom .lib.metrics import TrainMetric\nfrom .lib.utility import (SplitType, add_param_to_parser, create_dir,\n                          dump_json_data, get_logger, get_param_for_method,\n                          load_json_data, splittable_int)\n\n_logger = get_logger(__name__)\n\n\n@dataclass\nclass ResultCategory(TrainMetric):\n    captures: Dict[str, TrainMetric] = field(default_factory=dict)\n\n    def __post_init__(self):\n        super().__post_init__()\n        for k in self.captures:\n            if not isinstance(self.captures[k], TrainMetric):\n                self.captures[k] = TrainMetric(**self.captures[k])", "\n@dataclass\nclass ResultCategory(TrainMetric):\n    captures: Dict[str, TrainMetric] = field(default_factory=dict)\n\n    def __post_init__(self):\n        super().__post_init__()\n        for k in self.captures:\n            if not isinstance(self.captures[k], TrainMetric):\n                self.captures[k] = TrainMetric(**self.captures[k])", "\n\n@dataclass\nclass ResultDataset(TrainMetric):\n    categories: Dict[str, ResultCategory] = field(default_factory=dict)\n\n    def __post_init__(self):\n        super().__post_init__()\n        for k in self.categories:\n            if not isinstance(self.categories[k], ResultCategory):\n                self.categories[k] = ResultCategory(**self.categories[k])", "\n\n@dataclass\nclass ResultTrain(TrainMetric):\n    datasets: Dict[str, ResultDataset] = field(default_factory=dict)\n\n    def __post_init__(self):\n        super().__post_init__()\n        for k in self.datasets:\n            if not isinstance(self.datasets[k], ResultDataset):\n                self.datasets[k] = ResultDataset(**self.datasets[k])\n\n    def update(self, test_dict: DatasetTrainConfig, pt):\n        # registering results at each granularity\n        if len(pt) != len(self._ypred):\n            return\n        i = 0\n        for dataset_name, v in test_dict.datasets.items():\n            self.datasets[dataset_name] = ResultDataset()\n            for cat, val in v.categories.items():\n                self.datasets[dataset_name].categories[cat] = ResultCategory()\n                for pcap, target in val.captures.items():\n                    next_i = i + target.benign.test_taken + target.malicious.test_taken\n                    indexes = np.where(\n                        np.logical_and(pt >= i, pt < next_i))\n                    y_pred_slice = self._ypred[indexes]\n                    y_test_slice = self._ytrue[indexes]\n                    self.datasets[dataset_name].categories[cat].captures[pcap] = TrainMetric(\n                        _threshold=[\n                            (self._threshold[0][0], y_pred_slice.size)],\n                        _ypred=y_pred_slice,\n                        _ytrue=y_test_slice)\n                    self.datasets[dataset_name].categories[cat].update(\n                        self.datasets[dataset_name].categories[cat].captures[pcap])\n                    i = next_i\n                self.datasets[dataset_name].update(\n                    self.datasets[dataset_name].categories[cat])", "\n\n@dataclass\nclass ModelConfig:\n    detection_engine: Type[DetectionEngine]\n    train_params: Type[DeParams]\n    split_type: SplitType = field(default=SplitType.NONE)\n\n    def __post_init__(self):\n        if isinstance(self.detection_engine, str):\n            self.detection_engine = DetectionEngine.import_de(\n                self.detection_engine)\n        if isinstance(self.train_params, dict):\n            self.train_params = self.detection_engine.de_params(\n                **self.train_params)\n        if isinstance(self.split_type, int):\n            self.split_type = SplitType(self.split_type)\n\n    def join(self, other: \"ModelConfig\") -> bool:\n        if self.detection_engine != other.detection_engine or str(self.train_params) != str(other.train_params):\n            return False\n        return True", "\n\ndef train(dataset_path: str, de: Type[DetectionEngine],\n          models_dir, test_dict: DatasetTrainConfig, train_params: Type[DeParams], c,\n          split_type: SplitType, split_chunk):\n    \"\"\"Function for training a given configuration\"\"\"\n\n    tf.keras.backend.clear_session()\n    name = de.model_name(**c)\n    new_path = os.path.join(models_dir, name)\n    # check if configuration already trained\n    if os.path.isdir(new_path) and all(\n            os.path.isfile(os.path.join(new_path, x))\n            for x in (\"results.json\", \"relevance.json\", \"history.json\", \"params.json\")):\n        _logger.info(\n            f\"Model {name} already trained, skipping\")\n        return\n    create_dir(new_path, overwrite=True)\n\n    # calling the Detection Model specific train method\n    hs, hp, best_model, xts, yts, features_holder, pt = de.train(\n        dataset_path, new_path, train_params, split_type, split_chunk, **c)\n\n    _logger.info(f\"Testing {name}\")\n    y_pred = de.predict(best_model, xts, **hp.__dict__)\n\n    dump_json_data(features_holder, os.path.join(new_path, \"relevance.json\"))\n    dump_json_data(hs, os.path.join(new_path, \"history.json\"))\n    dump_json_data(hp, os.path.join(new_path, \"params.json\"))\n\n    res = ResultTrain(\n        _threshold=hp.malicious_threshold,\n        _ypred=y_pred, _ytrue=yts)\n    res.update(test_dict, pt)\n\n    dump_json_data(res, os.path.join(new_path, \"results.json\"))", "\n\ndef main(args_list):\n    # registering cli parameters to be shown with the -h flag\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        'dataset', help='path to the dataset directory', type=str)\n    parser.add_argument(\n        'output', help='output name', type=str)\n    parser.add_argument(\n        '-s', '--split-type', help='split type', type=splittable_int, default=SplitType.NONE)\n\n    parser.add_argument(\"detection_engine\", help=\"Select the detection engine to use\", type=str,\n                        choices=DetectionEngine.list_all())\n    parser.add_argument(\"rest\", nargs=argparse.REMAINDER)\n\n    args = parser.parse_args(args_list).__dict__\n\n    de: Type[DetectionEngine] = DetectionEngine.import_de(\n        args[\"detection_engine\"])\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    add_comb_params = get_param_for_method(\n        de.model_name, exclude_super_cls=DetectionEngine, ignore_default=True)\n    params_for_method = {k: v for k, v in get_param_for_method(\n        de.de_params).items() if k not in add_comb_params}\n\n    # registering nested cli parameters once chosen the detection model (i.e., LucidCnn)\n    # a nested -h flag is available for showing the help menu\n    for pname, (ptype, pdef) in add_comb_params.items():\n        parser.add_argument(\n            f\"--{pname}\", help=\"Parameter for creating models\", type=ptype, nargs=\"+\",\n            **{\"default\": tuple(range(len(de.features_holder_cls.ALLOWED), 0, -1))}\n            if pname == \"features\" else {\"required\": True})\n\n    for pname, (ptype, pdef) in params_for_method.items():\n        add_param_to_parser(parser, pname, ptype, pdef,\n                            \"Parameter for Training\")\n\n    args.update(parser.parse_args(args[\"rest\"]).__dict__)\n\n    dataset_config = DatasetConfig(\n        **load_json_data(os.path.join(args[\"dataset\"], \"conf.json\")))\n\n    # check if Detection Models are compatible\n    if not DetectionEngine.intersect([args[\"detection_engine\"]] +\n                                     [v.detection_engine.__name__ for v in dataset_config.preprocessed_configs.values()]):\n        raise ValueError(\"Error with the DE\")\n\n    models_config = ModelConfig(\n        de, de.de_params(\n            **{a: args[a] for a in params_for_method if a not in add_comb_params},\n            **{a: sorted(args[a], reverse=True) for a in add_comb_params}), args[\"split_type\"])\n\n    models_dir = os.path.join(\n        args[\"dataset\"], args[\"output\"], 'models')\n\n    create_dir(models_dir, overwrite=False)\n\n    dump_json_data(models_config, os.path.join(models_dir, \"conf.json\"))\n\n    combs = models_config.train_params.all_train_combs()\n    for i, c in enumerate(combs):\n        train(args[\"dataset\"], de, models_dir, dataset_config.offline,\n              models_config.train_params, c, models_config.split_type, split_chunk=(i, len(combs)))", ""]}
{"filename": "enid/register.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nMain file for registering new Detection Engine to a previous ENID installation\n\"\"\"\nimport argparse\nimport os", "import argparse\nimport os\nimport shutil\n\nfrom .lib.definitions import DetectionEngine\nfrom .lib.utility import get_logger, snake_to_camel\n\n_logger = get_logger(__name__)\n\n\ndef main(args_list):\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('path',\n                        help='path to the de to register', type=str)\n    args = parser.parse_args(args_list).__dict__\n\n    args[\"path\"] = os.path.normpath(args[\"path\"])\n\n    if os.path.isfile(args[\"path\"]) and args[\"path\"].endswith(\".py\"):\n        de_name = os.path.basename(args[\"path\"]).replace(\".py\", \"\")\n    elif os.path.isdir(args[\"path\"]) and os.path.isfile(os.path.join(args[\"path\"], \"__init__.py\")):\n        de_name = os.path.basename(args[\"path\"])\n    else:\n        raise RuntimeError(\"Unsupported format for registering plugin\")\n    de_name_camelised = snake_to_camel(de_name)\n\n    dest_path = os.path.join(os.path.dirname(\n        __file__), \"lib\", \"engines\", de_name)\n\n    _logger.info(\n        f\"Copying Detection Engine {de_name_camelised} to directory {dest_path}\")\n    shutil.copytree(args[\"path\"], dest_path, dirs_exist_ok=True)\n    try:\n        _logger.info(\"Checking validity...\")\n        DetectionEngine.import_de(de_name_camelised)\n        _logger.info(\"Engine successfully installed!\")\n    except Exception as e:\n        _logger.info(f\"Invalid Engine, removing. Why: {e}\")\n        shutil.rmtree(dest_path)", "\n\ndef main(args_list):\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('path',\n                        help='path to the de to register', type=str)\n    args = parser.parse_args(args_list).__dict__\n\n    args[\"path\"] = os.path.normpath(args[\"path\"])\n\n    if os.path.isfile(args[\"path\"]) and args[\"path\"].endswith(\".py\"):\n        de_name = os.path.basename(args[\"path\"]).replace(\".py\", \"\")\n    elif os.path.isdir(args[\"path\"]) and os.path.isfile(os.path.join(args[\"path\"], \"__init__.py\")):\n        de_name = os.path.basename(args[\"path\"])\n    else:\n        raise RuntimeError(\"Unsupported format for registering plugin\")\n    de_name_camelised = snake_to_camel(de_name)\n\n    dest_path = os.path.join(os.path.dirname(\n        __file__), \"lib\", \"engines\", de_name)\n\n    _logger.info(\n        f\"Copying Detection Engine {de_name_camelised} to directory {dest_path}\")\n    shutil.copytree(args[\"path\"], dest_path, dirs_exist_ok=True)\n    try:\n        _logger.info(\"Checking validity...\")\n        DetectionEngine.import_de(de_name_camelised)\n        _logger.info(\"Engine successfully installed!\")\n    except Exception as e:\n        _logger.info(f\"Invalid Engine, removing. Why: {e}\")\n        shutil.rmtree(dest_path)", ""]}
{"filename": "enid/lib/utility.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utility file containing all the function used among all the programs and modules.\"\"\"\nimport datetime\nimport importlib\nimport inspect\nimport json", "import inspect\nimport json\nimport logging\nimport math\nimport os\nimport pkgutil\nimport re\nimport shutil\nimport sys\nimport typing", "import sys\nimport typing\nfrom ctypes import Array, Structure\nfrom ctypes import Union as CUnion\nfrom ctypes import _Pointer, _SimpleCData\nfrom dataclasses import dataclass, fields, is_dataclass\nfrom enum import Enum\nfrom json import JSONEncoder\nfrom typing import Any, Callable, Dict, List, Tuple\n", "from typing import Any, Callable, Dict, List, Tuple\n\nimport numpy as np\nimport psutil\nfrom pypacker.layer12.ethernet import Ethernet\n\n\ndef set_seed():\n    \"Function for resetting the seed if \"\n    seed = os.environ.get(\"PYTHONHASHSEED\", None)\n\n    if seed is not None:\n        import tensorflow as tf\n        import random\n\n        seed = int(seed)\n        random.seed(seed)\n        np.random.seed(seed)\n        tf.keras.utils.set_random_seed(seed)", "\n\ndef silence():\n    import warnings\n    import tensorflow as tf\n    import tensorflow.python.util.deprecation as deprecation\n    from sklearn.exceptions import UndefinedMetricWarning\n\n    \"\"\"Function for silencing warning and tensorflow logging\"\"\"\n    warnings.filterwarnings(action='ignore', category=UndefinedMetricWarning)\n    warnings.filterwarnings(action='ignore', category=RuntimeWarning)\n    warnings.filterwarnings(action='ignore', category=UserWarning)\n    np.seterr(all=\"ignore\")\n    deprecation._PRINT_DEPRECATION_WARNINGS = False\n    tf.get_logger().setLevel('ERROR')\n    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)", "\n\nclass EthInPcap(Ethernet):\n    \"\"\"Personalised class to insert a reference to the dataset, category and pcap\"\"\"\n\n    def __init__(self, timestamp: int, dataset: str, category: str, pcap: str, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.timestamp = timestamp\n        self.dataset = dataset\n        self.category = category\n        self.pcap = pcap", "\n\nclass CDataJSONEncoder(JSONEncoder):\n    \"\"\"A JSON Encoder that puts small containers on single lines.\"\"\"\n\n    CONTAINER_TYPES = (list, tuple, dict)\n    \"\"\"Container datatypes include primitives or other containers.\"\"\"\n\n    MAX_WIDTH = 120\n    \"\"\"Maximum width of a container that might be put on a single line.\"\"\"\n\n    MAX_ITEMS = 15\n    \"\"\"Maximum number of items in container that might be put on single line.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # using this class without indentation is pointless\n        if kwargs.get(\"indent\") is None:\n            kwargs.update({\"indent\": 1})\n        super().__init__(*args, **kwargs)\n        self.indentation_level = 0\n\n    def encode(self, o):\n        \"\"\"Encode JSON object *o* with respect to single line lists.\"\"\"\n        o = self.default(o)\n\n        if isinstance(o, (list, tuple)):\n            if self._put_on_single_line(o):\n                return \"[\" + \", \".join(self.encode(el) for el in o) + \"]\"\n            else:\n                self.indentation_level += 1\n                output = [self.indent_str + self.encode(el) for el in o]\n                self.indentation_level -= 1\n                return \"[\\n\" + \",\\n\".join(output) + \"\\n\" + self.indent_str + \"]\"\n        elif isinstance(o, dict):\n            if o:\n                if self._put_on_single_line(o):\n                    return \"{ \" + \", \".join(f\"{self.encode(k)}: {self.encode(el)}\" for k, el in o.items()) + \" }\"\n                else:\n                    self.indentation_level += 1\n                    output = [\n                        self.indent_str + f\"{json.dumps(k)}: {self.encode(v)}\" for k, v in o.items()]\n                    self.indentation_level -= 1\n                    return \"{\\n\" + \",\\n\".join(output) + \"\\n\" + self.indent_str + \"}\"\n            else:\n                return \"{}\"\n        elif isinstance(o, str):  # escape newlines\n            o = o.replace(\"\\n\", \"\\\\n\")\n            return f'\"{o}\"'\n        else:\n            return json.dumps(o)\n\n    def iterencode(self, o, **kwargs):\n        \"\"\"Required to also work with `json.dump`.\"\"\"\n        return self.encode(o)\n\n    def _put_on_single_line(self, o):\n        return self._primitives_only(o) and len(o) <= self.MAX_ITEMS and len(str(o)) - 2 <= self.MAX_WIDTH\n\n    def _primitives_only(self, o: typing.Union[list, tuple, dict]):\n        if isinstance(o, (list, tuple)):\n            return not any(isinstance(el, self.CONTAINER_TYPES) for el in o)\n        elif isinstance(o, dict):\n            return not any(isinstance(el, self.CONTAINER_TYPES) for el in o.values())\n\n    @property\n    def indent_str(self) -> str:\n        if isinstance(self.indent, int):\n            return \" \" * (self.indentation_level * self.indent)\n        elif isinstance(self.indent, str):\n            return self.indentation_level * self.indent\n        else:\n            raise ValueError(\n                f\"indent must either be of type int or str (is: {type(self.indent)})\")\n\n    def default(self, obj):\n        if inspect.isclass(obj):\n            return obj.__name__\n\n        if isinstance(obj, (Array, list)):\n            return [self.default(e) for e in obj]\n\n        if isinstance(obj, _Pointer):\n            return self.default(obj.contents) if obj else None\n\n        if isinstance(obj, _SimpleCData):\n            return self.default(obj.value)\n\n        if isinstance(obj, (bool, int, float, str)):\n            return obj\n\n        if obj is None:\n            return obj\n\n        if isinstance(obj, Enum):\n            return obj.value\n\n        if isinstance(obj, (Structure, CUnion)):\n            result = {}\n            anonymous = getattr(obj, '_anonymous_', [])\n\n            for key, *_ in getattr(obj, '_fields_', []):\n                value = getattr(obj, key)\n\n                # private fields don't encode\n                if key.startswith('_'):\n                    continue\n\n                if key in anonymous:\n                    result.update(self.default(value))\n                else:\n                    result[key] = self.default(value)\n\n            return result\n\n        if is_dataclass(obj):\n            if hasattr(obj, \"to_json\"):\n                return obj.to_json()\n            else:\n                return {k.name: self.default(getattr(obj, k.name)) for k in fields(obj)}\n\n        if isinstance(obj, dict):\n            if obj and not isinstance(next(iter(obj), None), (int, float, str, bool)):\n                return [{'key': self.default(k), 'value': self.default(v)} for k, v in obj.items()]\n            else:\n                return {k: self.default(v) for k, v in obj.items()}\n\n        if isinstance(obj, tuple):\n            if hasattr(obj, \"_asdict\"):\n                return self.default(obj._asdict())\n            else:\n                return [self.default(e) for e in obj]\n\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n\n        if isinstance(obj, np.integer):\n            return int(obj)\n\n        if isinstance(obj, np.floating):\n            return float(obj)\n\n        return JSONEncoder.default(self, obj)", "\n\ndef get_best_comb(n_comb):\n    \"\"\"Function to get the best n\u00b0 of combinations during a randomised\n    search of hyperparameters\"\"\"\n    return n_comb if n_comb < 50 else 50\n\n\ndef get_best_dispatch(size: int):\n    \"\"\"Function to get the best dispatch n\u00b0 of workers while doing\n    a randomised search of hyperparameters\"\"\"\n    total = psutil.virtual_memory().total\n    n_times = 0\n\n    while True:\n        if size * n_times > total:\n            break\n        n_times += 1\n    return min(math.floor(math.sqrt(n_times)), os.cpu_count())*2 or 1", "def get_best_dispatch(size: int):\n    \"\"\"Function to get the best dispatch n\u00b0 of workers while doing\n    a randomised search of hyperparameters\"\"\"\n    total = psutil.virtual_memory().total\n    n_times = 0\n\n    while True:\n        if size * n_times > total:\n            break\n        n_times += 1\n    return min(math.floor(math.sqrt(n_times)), os.cpu_count())*2 or 1", "\n\ndef handler(x, y):\n    \"\"\"Utility to call function x with all parameters inside the arg y\"\"\"\n    return x(*y)\n\n\ndef sort_keep_comb(x, y):\n    \"\"\"Sort array x and y while preserving their labels and returning\n    the permutations used\"\"\"\n    p = np.random.permutation(len(x))\n    return x[p], y[p], p", "\n\ndef create_dir(name, overwrite=None):\n    \"\"\"Function to create a directory and, in case, overwrite or\n    backup the old one if already present\"\"\"\n    try:\n        os.makedirs(name)\n    except FileExistsError:\n        if overwrite is False:\n            shutil.move(\n                name, f\"{name}_{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}______backup\")\n            os.makedirs(name)\n        elif overwrite is True:\n            shutil.rmtree(name)\n            os.makedirs(name)", "\n\ndef load_json_data(path: str, fail=True):\n    \"\"\"Function to load json file into a dictionary\"\"\"\n    if os.path.isfile(path):\n        with open(path, \"r\") as fp:\n            return json.load(fp)\n    elif fail:\n        raise FileNotFoundError(\"File {} not found\".format(path))\n    else:\n        return {}", "\n\ndef dump_json_data(data, path: str = None):\n    \"\"\"Function to dump dictionary or dataclass into json file\n    with the custom encoder\"\"\"\n    if path is None:\n        return json.dumps(data, cls=CDataJSONEncoder)\n    with open(path, 'w') as fp:\n        json.dump(data, fp, indent=2, cls=CDataJSONEncoder)\n", "\n\ndef snake_to_camel(name, join_char=''):\n    \"\"\"Function to transform a string from snake to camel case\"\"\"\n    return join_char.join(word.title() for word in name.split('_'))\n\n\ndef camel_to_snake(name):\n    \"\"\"Function to transform a string from camel case to snake\"\"\"\n    return re.sub(r'(?<!^)(?=[A-Z])', '_', name).lower().replace(\" \", \"\")", "\n\ndef safe_division(a, b, default=None):\n    \"\"\"Function to perform a safe division, meaning no exceptions are thrown\n    in case of a division by 0 or infinite number\"\"\"\n    ret = np.divide(a, b)\n    if default is not None:\n        return np.nan_to_num(ret, copy=False, nan=default, posinf=default, neginf=default)\n    return ret\n", "\n\ndef get_logger(name: str, filepath: str = None, log_level: int = logging.INFO) -> logging.Logger:\n    \"\"\"Function to create a logger, or return the existing one\"\"\"\n    logger = logging.getLogger(name)\n    logger.setLevel(log_level)\n    formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    handlers = [logging.StreamHandler()]\n    if filepath:\n        handlers.append(logging.FileHandler(filepath, mode=\"w\"))\n    for h in handlers:\n        h.setLevel(log_level)\n        h.setFormatter(formatter)\n        logger.addHandler(h)\n    return logger", "\n\ndef nullable_int(val: str) -> int:\n    \"\"\"Utility int checker function to accept None as\n    cli argument instead of an int\"\"\"\n    ret = int(val)\n    if ret < 1:\n        return None\n    return ret\n", "\n\nclass SplitType(Enum):\n    EQUALLY = 0\n    PROPORTIONED = 1\n    NONE = 2\n\n\ndef splittable_int(val: str) -> int:\n    \"\"\"Utility function to convert int into SplitType Enum\"\"\"\n    return SplitType(int(val))", "def splittable_int(val: str) -> int:\n    \"\"\"Utility function to convert int into SplitType Enum\"\"\"\n    return SplitType(int(val))\n\n\ndef all_subclasses(cls, recursive=False):\n    \"\"\"Function to return all subclasses of a given class, optionally\n    searching for recursive ones.\"\"\"\n    if recursive:\n        recursive_import(sys.modules[cls.__module__])\n    return set(cls.__subclasses__()).union(\n        [s for c in cls.__subclasses__() for s in all_subclasses(c)])", "\n\n@dataclass\nclass UpdatableDataclass:\n    \"\"\"Base dataclass to support the update method, which updates the fields\n    of the current one by looking at all the fields of the provided one\"\"\"\n\n    def update(self, other):\n        for k in fields(other):\n            k = k.name\n            if not hasattr(self, k):\n                continue\n            if isinstance(getattr(other, k), dict):\n                for kk, vv in getattr(other, k).items():\n                    getattr(self, k).setdefault(kk, 0)\n                    getattr(self, k)[kk] += vv\n            elif isinstance(getattr(other, k), list):\n                setattr(self, k, getattr(self, k) + getattr(other, k))\n            elif isinstance(getattr(other, k), np.ndarray):\n                setattr(self, k, np.concatenate(\n                    (getattr(self, k), getattr(other, k)), axis=0, dtype=getattr(self, k).dtype))\n            elif is_dataclass(getattr(other, k)):\n                getattr(self, k).update(getattr(other, k))\n            else:\n                setattr(self, k, getattr(self, k) + getattr(other, k))", "\n\ndef recursive_import(base_module):\n    \"\"\"Function to recursively import all modules within a base module\"\"\"\n    if not hasattr(base_module, \"__path__\"):\n        return\n\n    for _, modname, ispkg in pkgutil.walk_packages(\n            path=base_module.__path__,\n            prefix=base_module.__name__ + \".\",\n            onerror=lambda x: None):\n\n        if modname in sys.modules:\n            continue\n\n        if ispkg:\n            recursive_import(modname)\n        else:\n            try:\n                sys.modules[modname] = importlib.import_module(modname)\n            except Exception as e:\n                print(\"Exception while importing\", modname, e)", "\n\ndef get_param_for_method(method: Callable, exclude_super_cls=None, ignore_default=False) -> Dict[str, Tuple[Any, Any]]:\n    \"\"\"Function to return all params and info for a specific method. Optionally, all the parameters shared with the\n    provided super class are ignored.\"\"\"\n    if not exclude_super_cls:\n        super_ones = []\n    elif inspect.isclass(method):\n        super_ones = inspect.signature(exclude_super_cls).parameters\n    else:\n        super_ones = inspect.signature(\n            getattr(exclude_super_cls, method.__name__)).parameters\n    return {v.name: (v.annotation if v.annotation != inspect._empty else Any, v.default)\n            for v in inspect.signature(method).parameters.values() if v.name != \"kwargs\" and (\n                v.name not in super_ones or ignore_default or (\n                    not ignore_default and v.default != super_ones[v.name].default))}", "\n\ndef get_max_comb(tmp: Dict[str, Tuple[Any]]):\n    \"\"\"Return the maximum combination of key-values in a dictionary\"\"\"\n    if not tmp:\n        return {}\n    if isinstance(tmp, (list, tuple)):\n        max = tmp[0]\n        for x in tmp:\n            if all(x[k] >= max[k] for k in x):\n                max = x\n        return max\n    return {k: max(v) for k, v in tmp.items()}", "\n\ndef get_all_dict_comb(tmp: Dict[str, Tuple[Any]]):\n    \"\"\"Return all combination of key-values in a dictionary\"\"\"\n    import itertools\n    keys, values = [], []\n    for k, v in tmp.items():\n        keys.append(k)\n        values.append(v if isinstance(v, (tuple, list)) else [v])\n    return [dict(zip(keys, v)) for v in itertools.product(*values)]", "\n\ndef add_param_to_parser(parser, pname, ptype, pdef, help=\"\"):\n    \"\"\"Function to add a cli parameter\"\"\"\n    other = {\"help\": help}\n    if hasattr(ptype, \"__origin__\") and issubclass(ptype.__origin__, (Tuple, List)):\n        other[\"nargs\"] = \"+\"\n        ptype = ptype.__args__[0]\n\n    if ptype == int and pdef is None:\n        ptype = nullable_int\n    if ptype == bool:\n        parser.add_argument(f'--{pname}', **other, action=\"store_{}\".format(\n            str(not pdef).lower() if pdef != inspect._empty and pdef is not None else \"true\"))\n    elif pdef != inspect._empty:\n        parser.add_argument(f'--{pname}', **other,  type=ptype, default=pdef)\n    else:\n        parser.add_argument(pname, **other, type=ptype)", ""]}
{"filename": "enid/lib/definitions.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"File containing the most important definitions of classes and behaviours.\"\"\"\nimport importlib\nimport os\nimport pickle\nimport time", "import pickle\nimport time\nfrom abc import ABC, abstractclassmethod, abstractmethod\nfrom dataclasses import dataclass, field, fields, replace\nfrom enum import Enum\nfrom itertools import cycle\nfrom typing import Dict, List, OrderedDict, Tuple, Type, Union\n\nimport numpy as np\nfrom pypacker.layer3.ip import IP", "import numpy as np\nfrom pypacker.layer3.ip import IP\nfrom pypacker.ppcap import Reader\n\nfrom .identifiers import BaseKey, TwoIPsProtoPortsKey, str_to_key\nfrom .metrics import (BaseStats, ComputationalRequirements, ResultTest,\n                      ResultTestCategory, ResultTestDataset, Stats, TestMetric)\nfrom .utility import (CDataJSONEncoder, EthInPcap, SplitType, camel_to_snake,\n                      get_all_dict_comb, get_logger, load_json_data,\n                      safe_division, snake_to_camel)", "                      get_all_dict_comb, get_logger, load_json_data,\n                      safe_division, snake_to_camel)\n\n\n@dataclass\nclass BaseFeature(ABC):\n    \"\"\"BaseFeature class. All instances must comply to this interface.\"\"\"\n    value: int = 0\n\n    @property\n    @abstractmethod\n    def computational_requirements(cls) -> Tuple[ComputationalRequirements]:\n        \"\"\"Method to return a list or tuple of computational requirements.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def memory_requirements(cls) -> int:\n        \"\"\"Method to return the memory bytes corresponding to the feature\"\"\"\n        pass\n\n    @classmethod\n    def create(cls, eth: EthInPcap):\n        \"\"\"Method to create a new instance of the feature from the packet\"\"\"\n        ret = cls()\n        ret.extract(eth)\n        return ret\n\n    @abstractmethod\n    def extract(self, eth: EthInPcap):\n        \"\"\"Method to extract/update the current instance of the feature from the packet\"\"\"\n        pass\n\n    def to_json(self):\n        \"\"\"Method to return the feature in a json-like format\"\"\"\n        return self.value", "\n\n@dataclass\nclass AggBaseFeature(BaseFeature):\n    \"\"\"AggbaseFeature class that represents the base class for all features that are\n    statistics instead of single values, hence updated throughout the time.\"\"\"\n    @classmethod\n    def create(cls, eth: EthInPcap, is_fwd=False):\n        ret = cls()\n        ret.extract(eth, is_fwd)\n        return ret\n\n    @abstractmethod\n    def extract(self, eth: EthInPcap, is_fwd=False):\n        pass", "\n\n@dataclass\nclass SessionValue:\n    \"\"\"Class to hold the values monitored from a flow\"\"\"\n    prediction: float = 0\n    metered_packets: int = 0\n    unmetered_packets: int = 0\n    value: Union[Tuple[AggBaseFeature],\n                 List[Tuple[BaseFeature]]] = field(default_factory=list)\n\n    @property\n    def total_packets(self):\n        return self.metered_packets + self.unmetered_packets", "\n\nclass Ticker:\n    \"\"\"Class to represent a ticker that hold a list of tasks and everytime the tick method\n    is called their age is reduced until zero.\"\"\"\n\n    def __init__(self, missing_tw, to_blacklist) -> None:\n        self.missing_tw: int = missing_tw\n        self.to_blacklist: List[Type[BaseKey]] = to_blacklist\n\n    def tick(self):\n        self.missing_tw -= 1\n\n    @property\n    def ready(self):\n        return self.missing_tw < 0", "\n\n@dataclass\nclass FeaturesHolder(ABC):\n    \"\"\"Abstract class for holding a features and defining which ones are\n    allowed within a Detection Model.\"\"\"\n    value: OrderedDict[Type[BaseFeature], Union[float,\n                                                Tuple[float, float]]] = field(default=None)\n\n    def __iter__(self):\n        for x in range(len(self.ALLOWED), 0, -1):\n            yield x\n\n    @classmethod\n    @property\n    @abstractmethod\n    def ALLOWED(cls) -> Tuple[Type[BaseFeature]]:\n        \"\"\"All the allowed ones\"\"\"\n        pass\n\n    @property\n    def computational_requirements(self):\n        \"\"\"Method to return the cpu requirements of extracting the current ones\"\"\"\n        return tuple(k for x in self.value for k in x.computational_requirements)\n\n    def get_feature_value(self, y: BaseFeature):\n        return self.value[y] if y in self.value else None\n\n    @property\n    def memory_requirements(self):\n        \"\"\"Method to return the memory requirements of extracting the current ones\"\"\"\n        return sum(x.memory_requirements for x in self.value)\n\n    @abstractmethod\n    def pop_less_relevant(self, key_depth_class: Type[BaseKey] = None) -> Type[BaseFeature]:\n        \"\"\"Abstract method to remove the least important features from the\n        currently active ones\"\"\"\n        pass\n\n    @property\n    def n_total(self) -> int:\n        \"\"\"All the allowed ones\"\"\"\n        return len(self.ALLOWED)\n\n    @property\n    def n_current(self) -> int:\n        \"\"\"Only the current ones\"\"\"\n        return len(self.value)", "\n\n@dataclass\nclass DeParams:\n    \"\"\"Base class to hold the parameters of a Detection Model\"\"\"\n    packets_per_session: int = None\n    features: int = None\n    max_packets_per_session: int = None\n    max_features: int = None\n    malicious_threshold: int = 0\n    key_depth_class: Type[BaseKey] = TwoIPsProtoPortsKey\n\n    def to_json(self):\n        \"\"\"Method to dump the class in a json-style\"\"\"\n        e = CDataJSONEncoder()\n        return {k.name: e.default(getattr(self, k.name)) for k in fields(self) if k.repr}\n\n    def __post_init__(self):\n        if isinstance(self.key_depth_class, str):\n            self.key_depth_class = str_to_key(self.key_depth_class)\n\n    def all_train_combs(self, exclude=None):\n        \"\"\"Method to return all combination of trainable models\"\"\"\n        return get_all_dict_comb({k: getattr(self, k) for k in self.train_combs() if k != exclude})\n\n    def previous_one(self, pps, ftrs):\n        \"\"\"Method to return the previous model parameters and the 'distance'\n        with the current one\"\"\"\n        is_max_features = ftrs == max(self.features)\n        if pps is None:\n            if is_max_features:\n                return False\n            prev_features = self.features[self.features.index(ftrs)-1]\n            return pps, prev_features, prev_features - ftrs, 0\n\n        is_max_packets = pps == max(self.packets_per_session)\n        if is_max_features and is_max_packets:\n            return False\n        if is_max_packets:\n            prev_features = self.features[self.features.index(ftrs)-1]\n            return pps, prev_features, prev_features - ftrs, 0\n        return self.packets_per_session[self.packets_per_session.index(pps)-1], ftrs, 0\n\n    def train_combs(self):\n        \"\"\"Method for returning the combination of parameters for generating models\"\"\"\n        return tuple(x for x in (\"packets_per_session\", \"features\") if getattr(self, x) is not None)", "\n\n@dataclass\nclass AnalysisState:\n    \"\"\"Base class to represent the state of an analysis\"\"\"\n    time_window: int = None\n    sessions_per_timewindow: int = None\n    enforce_timewindows_delay: int = 0\n    is_adaptiveness_enabled: bool = False\n\n    current_key: Type[BaseKey] = field(default=None)\n    current_features: Type[FeaturesHolder] = field(default=None)\n    params: DeParams = field(default=None)\n\n    def __post_init__(self):\n        if isinstance(self.current_key, str):\n            self.current_key = str_to_key(self.current_key)\n        if isinstance(self.current_features, dict):\n            self.current_features = self.__class__.current_features.__class__(\n                **self.current_features)\n        if isinstance(self.params, dict):\n            self.params = self.__class__.params.__class__(**self.params)", "\n\nclass TestType(Enum):\n    \"\"\"Enumeration to hold the type of the online test\"\"\"\n    PROCESSING = 0\n    THROUGHPUT = 1\n    NORMAL = 2\n\n\nclass DebugLevel(Enum):\n    \"\"\"Enumeration to hold the debug level of the online test\"\"\"\n    NONE = 0\n    BASE = 1\n    ENHANCED = 2", "\nclass DebugLevel(Enum):\n    \"\"\"Enumeration to hold the debug level of the online test\"\"\"\n    NONE = 0\n    BASE = 1\n    ENHANCED = 2\n\n\n@dataclass\nclass BaseProcessingData:\n    \"\"\"Base data that need to be generated from a processing method while parsing pcap\n    with the preprocesser.py program\"\"\"\n    tot_benign: int = 0\n    tot_malicious: int = 0\n    unique_benign: int = 0\n    unique_malicious: int = 0\n    tot_benign_packets: int = 0\n    tot_malicious_packets: int = 0", "@dataclass\nclass BaseProcessingData:\n    \"\"\"Base data that need to be generated from a processing method while parsing pcap\n    with the preprocesser.py program\"\"\"\n    tot_benign: int = 0\n    tot_malicious: int = 0\n    unique_benign: int = 0\n    unique_malicious: int = 0\n    tot_benign_packets: int = 0\n    tot_malicious_packets: int = 0", "\n\nclass TrafficAnalyser(ABC):\n    \"\"\"Traffic Analyser that includes the Filtering mechanism to drop packets matching\n    the blacklist and the Feature Extraction mechanisms. In addition to that, it contains a\n    reference to the Detection Model of interests and all the parameters used.\"\"\"\n    analysis_state_cls: Type[AnalysisState] = AnalysisState\n    session_value_cls: Type[SessionValue] = SessionValue\n\n    def __init__(self, detection_engine_cls: Type[\"DetectionEngine\"],\n                 attackers: Dict[str, Tuple[Type[BaseKey]]],\n                 time_window: int, current_key: Type[BaseKey], test_type: TestType,\n                 models_dir: str = None, dump_path: str = None,\n                 enforce_timewindows_delay: int = None,\n                 is_adaptiveness_enabled: bool = None, sessions_per_timewindow=None,\n                 debug: DebugLevel = DebugLevel.NONE, **kwargs):\n\n        if test_type == TestType.PROCESSING:\n            params = detection_engine_cls.de_params(\n                packets_per_session=kwargs.get(\"packets_per_session\", None),\n                features=len(detection_engine_cls.features_holder_cls.ALLOWED))\n            hold = detection_engine_cls.features_holder_cls()\n        else:\n            name = detection_engine_cls.model_name(**kwargs)\n            hold = detection_engine_cls.features_holder_cls(**load_json_data(\n                os.path.join(models_dir, name, \"relevance.json\")))\n            params = detection_engine_cls.de_params(**load_json_data(\n                os.path.join(models_dir, name, \"params.json\")))\n\n        self.analysis_state = self.analysis_state_cls(\n            time_window=time_window,\n            sessions_per_timewindow=sessions_per_timewindow,\n            enforce_timewindows_delay=enforce_timewindows_delay,\n            is_adaptiveness_enabled=is_adaptiveness_enabled,\n            current_key=current_key,\n            current_features=hold,\n            params=params)\n\n        self.test_type = test_type\n        self.dump_path = dump_path\n        self.start_time_window: int = None\n\n        self.blacklist_times = {}\n        self.extraction_times = {}\n\n        self.black_map: Dict[Type[BaseKey], bool] = {}\n        self.seen_sessions_previous_prediction: Dict[Type[BaseKey], str] = {}\n\n        self.current_untracked_map: Dict[Type[BaseKey], int] = {}\n        self.current_black_map: Dict[Type[BaseKey], int] = {}\n        self.current_session_map: Dict[Type[BaseKey], Type[SessionValue]] = {}\n\n        self.enforce_tasks: List[Ticker] = []\n        self.de: Type[DetectionEngine] = detection_engine_cls(\n            self.analysis_state, models_dir)\n        if test_type != TestType.PROCESSING:\n            a, b, c = self.de.load_model(params.__dict__, self.de.base_dir)\n            self.de.model = c\n            self.analysis_state.params = b\n            self.analysis_state.current_features = a\n            self.de._init_scale_method()\n        self.results = ResultTest()\n        self.attackers = {ds: (\n            next(x for x in attackers if x.dataset == ds).__class__,\n            {x: None for x in attackers if x.dataset == ds}) for ds in set([x.dataset for x in attackers])}\n        self.n_timewindow = 0\n        if test_type == TestType.PROCESSING:\n            self.processing_stats = detection_engine_cls.processing_data_cls()\n        self.debug = debug\n        if debug != DebugLevel.NONE:\n            self.debug_data = {}\n\n    @abstractmethod\n    def _extract(self, sess_id: Type[BaseKey], eth: EthInPcap):\n        \"\"\"Method defined by the TrafficAnalyser of each Detection Model for\n        the feature extraction of the sessions under monitoring.\"\"\"\n        pass\n\n    def _terminate_timewindow(self):\n        \"\"\"Method invoked at the end of each time window. Here the data is used\n        for the classification, and the malicious sessions are blacklisted accordingly.\n        In addition, the method to compute costs and statistics is called.\"\"\"\n        self.n_timewindow += 1\n        de_cpu, conversion_time, preprocessing_time, predict_time, de_mem, y_pred = 0, 0, 0, 0, 0, None\n        # check if at least 1 session is monitored.\n        if self.current_session_map:\n\n            data, conversion_time, preprocessing_time = self.de.preprocess_samples(\n                self)\n            predict_time = time.clock_gettime_ns(time.CLOCK_PROCESS_CPUTIME_ID)\n            y_pred = self.de.predict(\n                self.de.model, data, **self.analysis_state.params.__dict__)\n            predict_time = time.clock_gettime_ns(\n                time.CLOCK_PROCESS_CPUTIME_ID) - predict_time\n\n            de_mem = round(data.nbytes/len(y_pred))\n            de_cpu = self.de.parameters(model=self.de.model)\n            tmp = []\n            # assign the prediction to each flow and create the list of flows to be blacklisted\n            for p, (sess_id, v) in zip(y_pred, self.current_session_map.items()):\n                v.prediction = p.item()\n                if v.prediction > self.analysis_state.params.malicious_threshold and sess_id not in self.black_map:\n                    tmp.append(sess_id)\n            if tmp:\n                # insert the list as a Ticker task\n                self.enforce_tasks.append(\n                    Ticker(self.analysis_state.enforce_timewindows_delay, tmp))\n\n        if self.enforce_tasks:\n            # decrease all tasks by 1 and apply rules of all the ready ones\n            [x.tick() for x in self.enforce_tasks]\n            if self.enforce_tasks[0].ready:\n                t = self.enforce_tasks.pop(0)\n                for x in t.to_blacklist:\n                    if x not in self.black_map:\n                        self.black_map[x] = True\n\n        # compute costs and statistics for both the current time window and the global results\n        self._compute_cost(de_cpu, de_mem, self.blacklist_times, self.extraction_times,\n                           conversion_time, preprocessing_time, predict_time)\n\n        if self.analysis_state.is_adaptiveness_enabled:\n            raise NotImplementedError(\"To be implemented\")\n\n    @abstractmethod\n    def _terminate_timewindow_preprocessing(self):\n        \"\"\"Method defined by the TrafficAnalyser of each Detection Model to be\n        invoked at the end of a monitoring time window while preprocessing data.\"\"\"\n        raise NotImplementedError()\n\n    def _new_packet_preprocesser(self, ts, eth: EthInPcap = None):\n        \"\"\"Method used to handle a packet while preprocessing data\"\"\"\n        # set current start of the window if not already set\n        if not self.start_time_window:\n            self.start_time_window = ts\n\n        # check whether time window is finished\n        if self.analysis_state.time_window > 0 and ts - self.start_time_window > self.analysis_state.time_window:\n            # invoke method and clear all data structures (except global blacklist)\n            self._terminate_timewindow_preprocessing()\n            self.current_session_map.clear()\n            self.start_time_window = None\n\n        # check if IP packet or valid ethernet buffer\n        if eth is None or not eth[IP]:\n            return None\n\n        # compute the session identifier\n        sess_id = self.analysis_state.current_key.extract(eth)\n        # check if it is possible to monitor the session\n        if sess_id not in self.current_session_map:\n            self.current_session_map[sess_id] = self.session_value_cls()\n\n        # check if session already reached the max number of monitored packets\n        if self.analysis_state.params.packets_per_session and\\\n                self.current_session_map[sess_id].metered_packets == self.analysis_state.params.packets_per_session:\n            self.current_session_map[sess_id].unmetered_packets += 1\n            return\n\n        self.current_session_map[sess_id].metered_packets += 1\n\n        # compute and execute the feature extraction\n        self._extract(sess_id, eth)\n\n    def _new_packet(self, ts, eth: EthInPcap = None):\n        \"\"\"Method used to handle a new ethernet packet\"\"\"\n        # set current start of the window if not already set\n        if not self.start_time_window:\n            self.start_time_window = ts\n\n        # check whether time window is finished\n        if self.analysis_state.time_window > 0 and ts - self.start_time_window > self.analysis_state.time_window:\n            # invoke method and clear all data structures (except global blacklist)\n            self._terminate_timewindow()\n            self.current_session_map.clear()\n            self.current_black_map.clear()\n            self.current_untracked_map.clear()\n            self.extraction_times = {}\n            self.blacklist_times = {}\n            self.start_time_window = None\n\n        # check if IP packet or valid ethernet buffer\n        if eth is None or not eth[IP]:\n            return None\n\n        # compute the session identifier\n        sess_id = self.analysis_state.current_key.extract(eth)\n\n        # compute time for the blacklist lookup\n        t = time.clock_gettime_ns(time.CLOCK_PROCESS_CPUTIME_ID)\n        is_blacklisted = sess_id in self.black_map\n        t = time.clock_gettime_ns(time.CLOCK_PROCESS_CPUTIME_ID) - t\n        if (sess_id.dataset, sess_id.category, sess_id.pcap) not in self.blacklist_times:\n            self.blacklist_times[(\n                sess_id.dataset, sess_id.category, sess_id.pcap)] = 0\n        self.blacklist_times[(\n            sess_id.dataset, sess_id.category, sess_id.pcap)] += t\n\n        # block packet if blacklisted and NORMAL test\n        if not self.test_type == TestType.THROUGHPUT and is_blacklisted:\n            if sess_id not in self.current_black_map:\n                self.current_black_map[sess_id] = 0\n            self.current_black_map[sess_id] += 1\n            return\n\n        #  skip the analysis of the packet if the session is not monitored\n        if sess_id in self.current_untracked_map:\n            self.current_untracked_map[sess_id] += 1\n            return\n\n        # check if it is possible to monitor the session\n        if sess_id not in self.current_session_map:\n            # check if enough space\n            if self.analysis_state.sessions_per_timewindow and\\\n                    len(self.current_session_map) == self.analysis_state.sessions_per_timewindow:\n                self.current_untracked_map[sess_id] = 1\n                return\n            self.current_session_map[sess_id] = self.session_value_cls()\n\n        # check if session already reached the max number of monitored packets\n        if self.analysis_state.params.packets_per_session and\\\n                self.current_session_map[sess_id].metered_packets == self.analysis_state.params.packets_per_session:\n            self.current_session_map[sess_id].unmetered_packets += 1\n            return\n\n        self.current_session_map[sess_id].metered_packets += 1\n\n        # compute and execute the feature extraction\n        t = time.clock_gettime_ns(time.CLOCK_PROCESS_CPUTIME_ID)\n        self._extract(sess_id, eth)\n        t = time.clock_gettime_ns(time.CLOCK_PROCESS_CPUTIME_ID) - t\n        if (sess_id.dataset, sess_id.category, sess_id.pcap) not in self.extraction_times:\n            self.extraction_times[(\n                sess_id.dataset, sess_id.category, sess_id.pcap)] = 0\n        self.extraction_times[(\n            sess_id.dataset, sess_id.category, sess_id.pcap)] += t\n\n    @staticmethod\n    def generate_packets(pcap, analyser: \"TrafficAnalyser\", identifier=None, labels=None) -> int:\n        \"\"\"Method to generate packets  for the analyser provided from the pcap.\n        If present, this function tries to load the packets' labels, such as the\n        dataset category and pcap of belonging.\"\"\"\n        if identifier is None:\n            identifier = analyser.de.model_name(\n                **analyser.analysis_state.params.__dict__)\n        logger = get_logger(identifier)\n        if not labels:\n            with open(pcap.replace(\".pcap\", \".pickle\"), \"rb\") as fp:\n                labels = pickle.load(fp)\n            method = analyser._new_packet\n        else:\n            labels = cycle([labels])\n            method = analyser._new_packet_preprocesser\n        tot_bytes = os.path.getsize(pcap)\n        curr_bytes = 0\n\n        tot_time = 0\n        for curr_pkts, ((s_dataset, s_category, s_pcap), (ts, buf)) in enumerate(zip(labels, Reader(filename=pcap))):\n            if curr_pkts == 0:\n                tot_time = ts\n            curr_bytes += len(buf) + 16  # timestamp in nanoseconds\n            if curr_pkts % 50000 == 0:\n                logger.info(\"Read {}% bytes ({}/{}) and packet n\u00b0{}\".format(\n                    round(curr_bytes*100/tot_bytes, 2), curr_bytes, tot_bytes, curr_pkts))\n            try:\n                eth = EthInPcap(ts, s_dataset, s_category, s_pcap, buf)\n            except Exception:\n                eth = None\n\n            method(ts, eth=eth)\n        if analyser.test_type == TestType.PROCESSING:\n            analyser._terminate_timewindow_preprocessing()\n        else:\n            analyser._terminate_timewindow()\n            analyser.results.update()\n\n        logger.info(\"Finished\")\n        return ts - tot_time\n\n    def _get_sess_type(self, sess_id):\n        \"\"\"Method to return 0/1 whether the session identifier is malicious\"\"\"\n        return int(sess_id.cast(self.attackers[sess_id.dataset][0]) in self.attackers[sess_id.dataset][1])\n\n    def _compute_cost(self, de_cpu, de_mem, blacklist_times, extraction_times, conversion_time,\n                      preprocessing_time, predict_time, **kwargs):\n        \"\"\"Method for computing costs and statistics from the results of the current time\n        interval. Results are also propagated to the global results of the test accordingly.\"\"\"\n        tw_res = ResultTest()\n\n        # retrieve key and feature computational costs\n        key_comp_req = ComputationalRequirements.requirements_to_cost(\n            self.analysis_state.current_key.computational_requirements)\n        feat_comp_req = ComputationalRequirements.requirements_to_cost(\n            self.analysis_state.current_features.computational_requirements, ignore_depth=self.analysis_state.current_key)\n\n        if self.current_session_map:\n            pcap = {}\n            n_samples = len(self.current_session_map)\n            # adjust times to refer to a single flow\n            predict_time = safe_division(predict_time, n_samples, default=0.0)\n            preprocessing_time = safe_division(\n                preprocessing_time, n_samples, default=0.0)\n            conversion_time = safe_division(\n                conversion_time, n_samples, default=0.0)\n            for sess_id, v in self.current_session_map.items():\n                is_malicious = self._get_sess_type(sess_id)\n                prev_prediction = self.seen_sessions_previous_prediction.get(\n                    sess_id, None)\n                is_predicted_malicious = v.prediction > self.analysis_state.params.malicious_threshold\n\n                ttype = TestMetric.get_type_from_pred_true(\n                    is_predicted_malicious, is_malicious)\n\n                # check if dataset, category and pcap already present in results\n                if sess_id.dataset not in tw_res.datasets:\n                    tw_res.datasets[sess_id.dataset] = ResultTestDataset()\n                if sess_id.category not in tw_res.datasets[sess_id.dataset].categories:\n                    tw_res.datasets[sess_id.dataset].categories[sess_id.category] = ResultTestCategory(\n                    )\n                if sess_id.pcap not in tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures:\n                    tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures[sess_id.pcap] = TestMetric(\n                    )\n                if (sess_id.dataset, sess_id.category, sess_id.pcap) not in pcap:\n                    pcap[(sess_id.dataset, sess_id.category, sess_id.pcap)] = (\n                        [], [])\n                # appending ytrue and ypred\n                pcap[(sess_id.dataset, sess_id.category, sess_id.pcap)\n                     ][0].append(is_malicious)\n                pcap[(sess_id.dataset, sess_id.category, sess_id.pcap)\n                     ][1].append(v.prediction)\n\n                # update times\n                t = tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures[sess_id.pcap]\n                t.times.conversion_time += conversion_time\n                t.times.predict_time += predict_time\n                t.times.preprocessing_time += preprocessing_time\n\n                # depending by the nature of the session (TP, FN, etc.) update metrics accordingly\n                t1: BaseStats = getattr(\n                    getattr(\n                        tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures[sess_id.pcap],\n                        f\"{ttype}_stats\"),\n                    \"new\" if prev_prediction is None else\n                    \"from_same\" if prev_prediction == ttype else \"from_opposite\")\n                t1.traffic_analyser_memory += self.analysis_state.current_key.memory_requirements + \\\n                    self.analysis_state.current_features.memory_requirements * \\\n                    (1 if isinstance(v.value[0],\n                     AggBaseFeature) else v.metered_packets)\n                t1.detection_engine_memory += de_mem\n                t1.detection_engine_cpu += de_cpu\n                t1.traffic_analyser_cpu += key_comp_req * (v.metered_packets+v.unmetered_packets) +\\\n                    feat_comp_req * v.metered_packets\n                t1.sessions += 1\n\n                t1.metered_packets += v.metered_packets\n                t1.unmetered_packets += v.unmetered_packets\n                if v.unmetered_packets:\n                    t1.sessions_with_unmetered_packets += 1\n                if is_predicted_malicious and prev_prediction not in ('tp', 'fp'):\n                    t1.mitigated_sessions += 1\n                    t1.mitigator_memory += self.analysis_state.current_key.memory_requirements\n                self.seen_sessions_previous_prediction[sess_id] = ttype\n\n            # Update extraction times for the right pcap\n            for (d, c, p), v in extraction_times.items():\n                tw_res.datasets[d].categories[c].captures[p].times.extraction_time += v\n\n            # update ytrue, ypred and threshold accordingly\n            for (d, c, p), v in pcap.items():\n                tw_res.datasets[d].categories[c].captures[p]._ytrue = np.array(\n                    v[0], dtype=np.float64)\n                tw_res.datasets[d].categories[c].captures[p]._ypred = np.array(\n                    v[1], dtype=np.float64)\n                tw_res.datasets[d].categories[c].captures[p]._threshold = [\n                    (self.analysis_state.params.malicious_threshold, len(v[0]))]\n\n        # for each untracked session update stats\n        for sess_id, v in self.current_untracked_map.items():\n            if sess_id.dataset not in tw_res.datasets:\n                tw_res.datasets[sess_id.dataset] = ResultTestDataset()\n            if sess_id.category not in tw_res.datasets[sess_id.dataset].categories:\n                tw_res.datasets[sess_id.dataset].categories[sess_id.category] = ResultTestCategory(\n                )\n            if sess_id.pcap not in tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures:\n                tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures[sess_id.pcap] = TestMetric(\n                    _threshold=[(self.analysis_state.params.malicious_threshold, 0)])\n            t2: Stats = getattr(tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures[sess_id.pcap],\n                                'tp_stats' if self._get_sess_type(sess_id) else 'fp_stats')\n            t2.ignored_sessions += 1\n            t2.ignored_packets += v\n\n        # foreach session that appeared in the blacklist in the current time interval\n        # update stats accordingly\n        for sess_id, v in self.current_black_map.items():\n            if sess_id.dataset not in tw_res.datasets:\n                tw_res.datasets[sess_id.dataset] = ResultTestDataset()\n            if sess_id.category not in tw_res.datasets[sess_id.dataset].categories:\n                tw_res.datasets[sess_id.dataset].categories[sess_id.category] = ResultTestCategory(\n                )\n            if sess_id.pcap not in tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures:\n                tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures[sess_id.pcap] = TestMetric(\n                    _threshold=[(self.analysis_state.params.malicious_threshold, 0)])\n            t3: Stats = getattr(tw_res.datasets[sess_id.dataset].categories[sess_id.category].captures[sess_id.pcap],\n                                'tp_stats' if self._get_sess_type(sess_id) else 'fp_stats')\n            t3.mitigated_packets += v\n            t3.mitigated_sessions_reappeared += 1\n            t3.mitigator_cpu += key_comp_req * v\n\n        # update blacklist times of the pcap\n        for (d, c, p), v in blacklist_times.items():\n            tw_res.datasets[d].categories[c].captures[p].times.blacklist_time += v\n\n        # update global results without recomputing metrics (too expensive, done only at the end of the test)\n        self.results.update(tw_res)\n        # check the debug level and in case recompute metrics and update debug data structures\n        if self.debug == DebugLevel.BASE:\n            tw_res.update()\n            self.debug_data[self.n_timewindow] = replace(tw_res)\n        elif self.debug == DebugLevel.ENHANCED:\n            tw_res.update()\n            self.debug_data[self.n_timewindow] = {\n                \"result\": replace(tw_res),\n                \"analysis_state\": replace(self.analysis_state),\n                \"session_map\": self.current_session_map.copy(),\n                \"untracked_map\": self.current_untracked_map.copy(),\n                \"black_map\": self.current_black_map.copy()\n            }", "\n\nclass DetectionEngine(ABC):\n    \"\"\"Main Abstract class for representing a Detection Engine\"\"\"\n\n    # class to be used when processing data\n    processing_data_cls = BaseProcessingData\n    # class to be used for the parameters of the model\n    de_params = DeParams\n\n    def __init__(self, analysis_state: Type[AnalysisState], base_dir: str) -> None:\n        self.model = None\n        self.analysis_state = analysis_state\n        self.base_dir: str = base_dir\n\n    def __del__(self):\n        del self.model\n\n    @staticmethod\n    def list_all(only_main=False):\n        \"\"\" Method to list all Detection Models available in this framework\"\"\"\n        ret = []\n        basepath = os.path.join(os.path.dirname(__file__), \"engines\")\n        for x in os.listdir(basepath):\n            if (only_main and os.path.isdir(os.path.join(\n                basepath, x, \"main\"))) or (not only_main and not x.startswith(\"_\") and\n                                           (x.endswith(\".py\") or os.path.isdir(os.path.join(basepath, x)))):\n                ret.append(snake_to_camel(x.replace(\".py\", \"\")))\n        return ret\n\n    @abstractclassmethod\n    def load_model(cls, params: Union[DeParams, Dict], basedir: str):\n        \"\"\"Abstract method to be implemented, used for loading the model\"\"\"\n        raise NotImplementedError()\n\n    def _init_scale_method(self):\n        pass\n\n    @staticmethod\n    def _load_dataset(basename, features_holder: FeaturesHolder, packets_per_session=None,\n                      max_packets_per_session=None, max_features=None,\n                      split_type=SplitType.NONE, split_chunk: Tuple[int, int] = tuple(), **kwargs):\n        raise NotImplementedError()\n\n    @staticmethod\n    @abstractmethod\n    def _fed_adapt_layer(src_w, dst_w, global_f, local_f, pad=False):\n        raise NotImplementedError()\n\n    @abstractclassmethod\n    def parameters(cls, model=None, **kwargs) -> int:\n        \"\"\"Method to be implemented and return the complexity of the model in terms of\n        trainable parameters\"\"\"\n        raise NotImplementedError()\n\n    @classmethod\n    @property\n    @abstractmethod\n    def features_holder_cls(cls) -> Type[FeaturesHolder]:\n        \"\"\"Feature Holder class to be specified when implementing the Detection Model\"\"\"\n        pass\n\n    @classmethod\n    @property\n    @abstractmethod\n    def traffic_analyser_cls(cls) -> Type[TrafficAnalyser]:\n        \"\"\"The Traffic Analyser class to be specified when implementing the Detection Model\"\"\"\n        pass\n\n    @abstractclassmethod\n    def _get_arch(*args, **kwargs):\n        \"\"\"Method to get an instance of the model according to the provided arguments\"\"\"\n        pass\n\n    @abstractmethod\n    def preprocess_samples(self, ta: Type[TrafficAnalyser], skip: bool = False) -> Tuple[np.ndarray, int, int]:\n        \"\"\"Method to preprocess samples captured by the analyser\"\"\"\n        pass\n\n    @classmethod\n    def predict(cls, model, data, **kwargs) -> np.ndarray:\n        \"\"\"Method for classifying the data\"\"\"\n        pass\n\n    @abstractclassmethod\n    def model_name(cls, features: int = None, **kwargs) -> str:\n        \"\"\"Method for returning the name of a model given the parameters provided\"\"\"\n        pass\n\n    @staticmethod\n    def import_de(name: str) -> Type[\"DetectionEngine\"]:\n        \"\"\"Method for importing and returning the Detection Engine class provided as string\"\"\"\n        return getattr(importlib.import_module('.engines.{}'.format(camel_to_snake(name)), package=\"enid.lib\"), name)\n\n    @staticmethod\n    @abstractmethod\n    def append_to_dataset(source, dest, ttype, label, indexes_tr, indexes_val, indexes_ts, **kwargs):\n        \"\"\"Method for appending single processed data to the dataset\"\"\"\n        pass\n\n    @abstractclassmethod\n    def train(cls, dataset_path: str, models_dir: str, params: Type[DeParams], split_type: SplitType,\n              split_chunk: Tuple[int, int], **kwargs):\n        \"\"\"Method for training an instance of the model\"\"\"\n        pass\n\n    @staticmethod\n    def intersect(engines_list: List[Type[\"DetectionEngine\"]]) -> bool:\n        \"\"\"Method to check whether the provided Engines intersect, meaning\n        they can be used with the same preprocessed data\"\"\"\n        engines_list = [x if isinstance(\n            x, DetectionEngine) else DetectionEngine.import_de(x) for x in engines_list]\n        return all(set(engines_list[0].features_holder_cls.ALLOWED) == set(elem.features_holder_cls.ALLOWED)\n                   for elem in engines_list) and\\\n            all(engines_list[0].append_to_dataset.__code__ ==\n                elem.append_to_dataset.__code__ and\n                engines_list[0].traffic_analyser_cls._new_packet.__code__ == elem.traffic_analyser_cls._new_packet.__code__\n                for elem in engines_list)", ""]}
{"filename": "enid/lib/metrics.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"File for defining metrics used within the offline and online test\"\"\"\nimport math\nfrom dataclasses import dataclass, field, fields\nfrom enum import Enum\nfrom typing import Dict, List, Tuple, Type, Union", "from enum import Enum\nfrom typing import Dict, List, Tuple, Type, Union\n\nimport numpy as np\nfrom sklearn.metrics import (accuracy_score, average_precision_score,\n                             balanced_accuracy_score, confusion_matrix,\n                             f1_score, log_loss, precision_recall_curve,\n                             precision_score, recall_score, roc_auc_score,\n                             roc_curve)\n", "                             roc_curve)\n\nfrom .utility import CDataJSONEncoder, UpdatableDataclass, safe_division\n\n\nclass ComputationalRequirements(Enum):\n    \"\"\"Enumeration for representing the computational requirements of features and key fields\"\"\"\n    REQUIRED_L2 = 3\n    REQUIRED_L3 = REQUIRED_L2 + 3\n    REQUIRED_L4 = REQUIRED_L3 + 5\n    HASH_COMPUTATION = 3\n    BASE_MATH_OP = 1\n    ENHANCED_MATH_OP = 2\n    TIMER = 5\n\n    @staticmethod\n    def requirements_to_cost(requirements_lists: Tuple[\"ComputationalRequirements\"], ignore_depth: bool = False):\n        cost = 0\n\n        if ignore_depth is not True:\n            if isinstance(ignore_depth, bool):\n                target = requirements_lists\n            else:\n                from .definitions import BaseKey\n                ignore_depth: BaseKey = ignore_depth\n                target = tuple(x for x in (ComputationalRequirements.REQUIRED_L4,\n                                           ComputationalRequirements.REQUIRED_L3,\n                                           ComputationalRequirements.REQUIRED_L2)\n                               if x not in ignore_depth.computational_requirements)\n\n            if ComputationalRequirements.REQUIRED_L4 in target:\n                cost += ComputationalRequirements.REQUIRED_L4.value\n            elif ComputationalRequirements.REQUIRED_L3 in target:\n                cost += ComputationalRequirements.REQUIRED_L3.value\n            elif ComputationalRequirements.REQUIRED_L2 in target:\n                cost += ComputationalRequirements.REQUIRED_L2.value\n\n        for y in (ComputationalRequirements.HASH_COMPUTATION, ComputationalRequirements.BASE_MATH_OP,\n                  ComputationalRequirements.ENHANCED_MATH_OP, ComputationalRequirements.TIMER):\n            cost += y.value*requirements_lists.count(y)\n\n        return cost", "\n\n@dataclass\nclass TrainMetric:\n    \"\"\"Class for holding the train metrics\"\"\"\n    tp: int = 0\n    fp: int = 0\n    tn: int = 0\n    fn: int = 0\n\n    tpr: float = 0\n    fpr: float = 0\n    tnr: float = 0\n    fnr: float = 0\n\n    log_loss: float = 0\n    mcc: float = 0\n    cohen_k: float = 0\n    gmean: float = 0\n    f1_score: float = 0\n    accuracy: float = 0\n    balanced_accuracy: float = 0\n    roc_auc: float = 0\n    recall: float = 0\n    precision: float = 0\n    average_precision: float = 0\n\n    best_roc_threshold: float = 0\n    best_roc_gmean: float = 0\n    best_precision_recall_threshold: float = 0\n    best_precision_recall_f1score: float = 0\n\n    _threshold: List[Tuple[float, int]] = field(\n        default_factory=list, repr=False)\n    _ypred: np.ndarray = field(default=np.array(\n        [], dtype=np.float64), repr=False)\n    _ytrue: np.ndarray = field(default=np.array(\n        [], dtype=np.float64), repr=False)\n\n    def to_json(self):\n        \"\"\"Method to dump the class in a json-style\"\"\"\n        e = CDataJSONEncoder()\n        return {k.name: e.default(getattr(self, k.name)) for k in fields(self) if k.repr}\n\n    @staticmethod\n    def get_best_threshold_roc(y_pred, y_true):\n        fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n        # calculate the g-mean for each threshold\n        gmeans = np.sqrt(tpr * (1-fpr))\n        np.nan_to_num(gmeans, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n        # locate the index of the largest g-mean\n        ix = np.argmax(gmeans)\n        return thresholds[ix], gmeans[ix]\n\n    @staticmethod\n    def get_best_threshold_f1(y_pred, y_true):\n        precision, recall, thresholds = precision_recall_curve(\n            y_true, y_pred)\n        # convert to f score\n        fscore = (2 * precision * recall) / (precision + recall)\n        np.nan_to_num(fscore, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n        # locate the index of the largest f score\n        ix = np.argmax(fscore)\n        return thresholds[ix], fscore[ix]\n\n    def __post_init__(self):\n        \"\"\"Method called after initialisation. If the class is not empty,\n        this method computes all the metrics.\"\"\"\n        if not isinstance(self._threshold, list):\n            self._threshold = [(self._threshold, self._ytrue.size)]\n\n        if self._ypred.size == 0 or self._ytrue.size == 0:\n            return\n\n        self.best_roc_threshold, self.best_roc_gmean = self.get_best_threshold_roc(\n            self._ypred, self._ytrue)\n        self.best_precision_recall_threshold, self.best_precision_recall_f1score = self.get_best_threshold_f1(\n            self._ypred, self._ytrue)\n\n        self.log_loss = log_loss(self._ytrue, self._ypred, labels=[0, 1])\n        ypred = np.copy(self._ypred)\n\n        # Since it might be possible that the threshold changed during the time,\n        # here we make sure to use the right threshold for the various\n        # bunch of data\n        prev_index = 0\n        for (t, index) in self._threshold:\n            if prev_index - index != 0:\n                ypred[prev_index:index] = ypred[prev_index:index] > t\n                prev_index = index\n\n        self.tn, self.fp, self.fn, self.tp = confusion_matrix(\n            self._ytrue, ypred, labels=[0, 1]).ravel()\n        self.tpr = safe_division(self.tp, self.tp + self.fn, default=0.0)\n        self.tnr = safe_division(self.tn, self.tn + self.fp, default=0.0)\n        self.fpr = safe_division(self.fp, self.fp + self.tn, default=0.0)\n        self.fnr = safe_division(self.fn, self.fn + self.tp, default=0.0)\n\n        self.mcc = safe_division(self.tn*self.tp-self.fn*self.fp, math.sqrt(\n            (self.tp+self.fp) * (self.tp + self.fn) * (self.tn + self.fp) * (self.tn + self.fn)),\n            default=0.0)\n        self.cohen_k = safe_division(2*(self.tp*self.tn - self.fn * self.fp),\n                                     (self.tp + self.fp) * (self.fp + self.tn) *\n                                     (self.tp + self.fn) * (self.fn + self.tn), default=0.0)\n        self.gmean = np.sqrt(self.tpr * (1-self.fpr))\n        self.f1_score = f1_score(self._ytrue, ypred, labels=[0, 1])\n        self.accuracy = accuracy_score(self._ytrue, ypred)\n        self.balanced_accuracy = balanced_accuracy_score(self._ytrue, ypred)\n        try:\n            self.roc_auc = roc_auc_score(self._ytrue, ypred, labels=[0, 1])\n        except ValueError:\n            self.roc_auc = 0.0\n        self.recall = recall_score(self._ytrue, ypred, labels=[0, 1])\n        self.precision = precision_score(self._ytrue, ypred, labels=[0, 1])\n        self.average_precision = average_precision_score(self._ytrue, ypred)\n\n    def update(self, other: \"TrainMetric\"):\n        \"\"\"Method to update such a class with another given one\"\"\"\n        i = self._threshold[-1][1] if self._threshold else 0\n        for (t, j) in other._threshold:\n            self._threshold.append((t, i+j))\n        self._ypred = np.concatenate(\n            (self._ypred, other._ypred), axis=0, dtype=np.float64)\n        self._ytrue = np.concatenate(\n            (self._ytrue, other._ytrue), axis=0, dtype=np.float64)\n        self.__post_init__()\n\n    @classmethod\n    def get_metrics(cls):\n        \"\"\"Method to return all the metrics defined in this class\"\"\"\n        return [k.name for k in fields(cls) if k.repr and k.name not in (\"tp\", \"tn\", \"fp\", \"fn\")]", "\n\n@dataclass\nclass BaseStats(UpdatableDataclass):\n    \"\"\"Base statistic class holding stats concerning a certain type of flows (new, etc.)\"\"\"\n    sessions: int = 0\n    mitigated_sessions: int = 0\n\n    metered_packets: int = 0\n    unmetered_packets: int = 0\n    sessions_with_unmetered_packets: int = 0\n\n    mitigator_memory: int = 0\n    traffic_analyser_memory: int = 0\n    detection_engine_memory: int = 0\n    traffic_analyser_cpu: int = 0\n    detection_engine_cpu: int = 0", "\n\n@dataclass\nclass Stats(UpdatableDataclass):\n    \"\"\"Stats class to hold information about all kind of flows within a certain\n    type (TP, FP).\"\"\"\n    # newly monitored\n    new: BaseStats = field(default_factory=BaseStats)\n    # already monitored and with the same prediction\n    from_same: BaseStats = field(default_factory=BaseStats)\n    # already monitored but with the opposite prediction\n    from_opposite: BaseStats = field(default_factory=BaseStats)\n\n    ignored_sessions: int = 0\n    ignored_packets: int = 0\n\n    mitigated_packets: int = 0\n    mitigated_sessions_reappeared: int = 0\n    mitigator_cpu: int = 0\n\n    def __post_init__(self):\n        for k in (\"new\", \"from_same\", \"from_opposite\"):\n            if isinstance(getattr(self, k), dict):\n                setattr(self, k, BaseStats(**getattr(self, k)))", "\n\n@dataclass\nclass Times(UpdatableDataclass):\n    \"\"\"Class to hold times recorded during the online test\"\"\"\n    blacklist_time: float = 0\n    extraction_time: float = 0\n    preprocessing_time: float = 0\n    conversion_time: float = 0\n    predict_time: float = 0\n\n    @property\n    def no_conversion(self):\n        return self.blacklist_time + self.extraction_time + self.preprocessing_time + self.predict_time\n\n    @property\n    def total(self):\n        return self.no_conversion + self.conversion_time", "\n\n@dataclass\nclass TestMetric(TrainMetric):\n    \"\"\"Class to hold all metrics used within the online test. Additional information\n    concerning each nature of flows (TP, etc.) are mantained separetely.\"\"\"\n    flows_per_second: float = 0.0\n    packets_per_second: float = 0.0\n    packets_fnr_time_window: float = 0.0\n    packets_fnr_early_mitigation: float = 0.0\n    packets_tpr_time_window: float = 0.0\n    packets_tpr_early_mitigation: float = 0.0\n    packets_tnr_time_window: float = 0.0\n    packets_tnr_early_mitigation: float = 0.0\n    packets_fpr_time_window: float = 0.0\n    packets_fpr_early_mitigation: float = 0.0\n    benign_traffic_metered_percentage: float = 0.0\n    malicious_traffic_metered_percentage: float = 0.0\n    traffic_metered_percentage: float = 0.0\n    flows_with_unmetered_packets_percentage: float = 0.0\n    benign_flows_with_unmetered_packets_percentage: float = 0.0\n    malicious_flows_with_unmetered_packets_percentage: float = 0.0\n    estimated_traffic_analyser_memory_bytes: int = 0\n    estimated_traffic_analyser_cpu_instructions: int = 0\n    estimated_detection_engine_memory_bytes: int = 0\n    estimated_detection_engine_cpu_instructions: int = 0\n    estimated_mitigator_memory_bytes: int = 0\n    estimated_mitigator_cpu_instructions: int = 0\n    estimated_memory_bytes: int = 0\n    estimated_cpu_instructions: int = 0\n\n    times: Times = field(default_factory=Times)\n    tp_stats: Stats = field(default_factory=Stats)\n    fp_stats: Stats = field(default_factory=Stats)\n    tn_stats: Stats = field(default_factory=Stats)\n    fn_stats: Stats = field(default_factory=Stats)\n\n    def __post_init__(self):\n        \"\"\"If the class is not empty, then compute all the metrics\"\"\"\n        super().__post_init__()\n        for k in (\"tn_stats\", \"fn_stats\", \"tp_stats\", \"fp_stats\"):\n            if not isinstance(getattr(self, k), Stats):\n                setattr(self, k, Stats(**getattr(self, k)))\n        if not isinstance(self.times, Times):\n            self.times = Times(**self.times)\n        if self._ypred.size == 0:\n            return\n        self.flows_with_unmetered_packets_percentage = safe_division(self.get_stats_for(\n            \"sessions_with_unmetered_packets\"), self.tp + self.tn + self.fp + self.fn)\n        self.benign_flows_with_unmetered_packets_percentage = safe_division(self.get_stats_for(\n            \"sessions_with_unmetered_packets\", (\"tn_stats\", \"fp_stats\")), self.tn + self.fp)\n        self.malicious_flows_with_unmetered_packets_percentage = safe_division(self.get_stats_for(\n            \"sessions_with_unmetered_packets\", (\"tp_stats\", \"fn_stats\")), self.tp + self.fn)\n        self.estimated_traffic_analyser_memory_bytes = self.get_stats_for(\n            \"traffic_analyser_memory\")\n        self.estimated_detection_engine_memory_bytes = self.get_stats_for(\n            \"detection_engine_memory\")\n        self.estimated_mitigator_memory_bytes = self.get_stats_for(\n            \"mitigator_memory\")\n        self.estimated_memory_bytes = self.estimated_traffic_analyser_memory_bytes + \\\n            self.estimated_mitigator_memory_bytes + \\\n            self.estimated_detection_engine_memory_bytes\n        self.estimated_mitigator_cpu_instructions = self.get_stats_for(\n            \"mitigator_cpu\")\n        self.estimated_traffic_analyser_cpu_instructions = self.get_stats_for(\n            \"traffic_analyser_cpu\")\n        self.estimated_detection_engine_cpu_instructions = self.get_stats_for(\n            \"detection_engine_cpu\")\n        self.estimated_cpu_instructions = self.estimated_mitigator_cpu_instructions + \\\n            self.estimated_traffic_analyser_cpu_instructions + \\\n            self.estimated_detection_engine_cpu_instructions\n        self.flows_per_second = safe_division(\n            (self.tp + self.tn + self.fp + self.fn) * 10**9, self.times.no_conversion)\n        self.packets_per_second = safe_division(\n            self.total_packets * 10**9, self.times.no_conversion)\n        self.packets_fnr_time_window = safe_division(self.get_stats_for(\n            (\"metered_packets\", \"unmetered_packets\"), (\"tp_stats\", \"fn_stats\")), self.total_malicious_packets)\n        self.packets_fnr_early_mitigation = safe_division(self.get_stats_for(\n            \"metered_packets\", \"tp_stats\") + self.get_stats_for((\"metered_packets\", \"unmetered_packets\"), \"fn_stats\"),\n            self.total_malicious_packets)\n        self.packets_tpr_time_window = 1 - self.packets_fnr_time_window\n        self.packets_tpr_early_mitigation = 1 - self.packets_fnr_early_mitigation\n        self.packets_tnr_time_window = safe_division(self.get_stats_for(\n            (\"metered_packets\", \"unmetered_packets\"), (\"tn_stats\", \"fp_stats\")), self.total_benign_packets)\n        self.packets_tnr_early_mitigation = safe_division(self.get_stats_for(\n            \"metered_packets\", \"fp_stats\") + self.get_stats_for((\"metered_packets\", \"unmetered_packets\"), \"tn_stats\"),\n            self.total_benign_packets)\n        self.packets_fpr_time_window = 1 - self.packets_tnr_time_window\n        self.packets_fpr_early_mitigation = 1 - self.packets_tnr_early_mitigation\n        self.benign_traffic_metered_percentage = safe_division(self.get_stats_for(\n            \"metered_packets\", (\"tn_stats\", \"fp_stats\")), self.total_benign_packets)\n        self.malicious_traffic_metered_percentage = safe_division(self.get_stats_for(\n            \"metered_packets\", (\"tp_stats\", \"fn_stats\")), self.total_malicious_packets)\n        self.traffic_metered_percentage = safe_division(\n            self.get_stats_for(\"metered_packets\"), self.total_packets)\n\n    def get_stats_for(\n            self,\n            attrs: Union[str, Tuple[str]],\n            natures: Union[str, Tuple[str]] = (\n                \"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\"),\n            types: Union[str, Tuple[str]] = (\"new\", \"from_same\", \"from_opposite\")):\n        \"\"\"Methods to return the desired values from all attributes matching the provided ones\"\"\"\n        if not isinstance(attrs, (tuple, list)):\n            attrs = (attrs, )\n        if not isinstance(natures, (tuple, list)):\n            natures = (natures, )\n        if not isinstance(types, (tuple, list)):\n            types = (types, )\n\n        third_nested = tuple(\n            x for x in attrs if next((y for y in fields(BaseStats) if x == y.name), False))\n        second_nested = tuple(\n            x for x in attrs if next((y for y in fields(Stats) if x == y.name), False))\n\n        return sum(getattr(getattr(getattr(self, x), y), z) for x in natures for y in types for z in third_nested) +\\\n            sum(getattr(getattr(self, x), z)\n                for x in natures for z in second_nested)\n\n    @classmethod\n    def get_metrics(cls):\n        \"\"\"Method to return all the metrics defined by this class\"\"\"\n        return [k.name for k in fields(cls) if k.repr and k.name not in (\n            \"tp\", \"tn\", \"fp\", \"fn\", \"tp_stats\", \"tn_stats\", \"fp_stats\", \"fn_stats\", \"times\")]\n\n    @staticmethod\n    def get_type_from_pred_true(is_predicted_malicious: bool, is_malicious: bool) -> str:\n        \"\"\"Method to return the nature of a flow given its true value and the predicted one\"\"\"\n        if is_malicious and is_predicted_malicious:\n            return \"tp\"\n        elif is_predicted_malicious and is_malicious != is_predicted_malicious:\n            return \"fp\"\n        elif not is_predicted_malicious and not is_malicious:\n            return \"tn\"\n        elif not is_predicted_malicious and is_malicious != is_predicted_malicious:\n            return \"fn\"\n        else:\n            raise ValueError(\"Do not know how to infer\")\n\n    def update(self, other: \"TestMetric\"):\n        \"\"\"Method to update this instance with another one\"\"\"\n        [getattr(self, x).update(getattr(other, x))\n         for x in (\"times\", \"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\")]\n        super().update(other)\n\n    @property\n    def total_malicious_packets(self):\n        return self.get_stats_for((\"metered_packets\", \"unmetered_packets\", \"mitigated_packets\"),\n                                  (\"tp_stats\", \"fn_stats\"))\n\n    @property\n    def total_benign_packets(self):\n        return self.get_stats_for((\"metered_packets\", \"unmetered_packets\", \"mitigated_packets\", \"ignored_packets\"),\n                                  (\"tn_stats\", \"fp_stats\"))\n\n    @property\n    def total_packets(self):\n        return self.get_stats_for((\"metered_packets\", \"unmetered_packets\", \"mitigated_packets\", \"ignored_packets\"))", "\n\n@dataclass\nclass ResultTestCategory(TestMetric):\n    captures: Dict[str, TestMetric] = field(default_factory=dict)\n\n    def __post_init__(self):\n        super().__post_init__()\n        for k in self.captures:\n            if not isinstance(self.captures[k], TestMetric):\n                self.captures[k] = TestMetric(**self.captures[k])", "\n\n@dataclass\nclass ResultTestDataset(TestMetric):\n    categories: Dict[str, ResultTestCategory] = field(default_factory=dict)\n\n    def __post_init__(self):\n        super().__post_init__()\n        for k in self.categories:\n            if not isinstance(self.categories[k], ResultTestCategory):\n                self.categories[k] = ResultTestCategory(**self.datasets[k])", "\n\n@dataclass\nclass ResultTest(TestMetric):\n    \"\"\"Class holding results of the online test divided by granularity.\"\"\"\n    datasets: Dict[str, ResultTestDataset] = field(default_factory=dict)\n\n    def __post_init__(self):\n        super().__post_init__()\n        for k in self.datasets:\n            if not isinstance(self.datasets[k], ResultTestDataset):\n                self.datasets[k] = ResultTestDataset(**self.datasets[k])\n\n    def update(self, other: Type[\"ResultTest\"] = None):\n        \"\"\"Method to update such a class with the provided one. If the other one is not provided, then\n        this method recomputes the data of this class by starting from the most inner ones (pcap granularity)\n        untill the more generic ones are created.\"\"\"\n        if other:\n            # for each pcap in other, update pcap of this class.\n            # note that we update only pcap, as the category/dataset/general data\n            # is updated by invoking this method with no argument\n            for k, v in other.datasets.items():\n                if k not in self.datasets:\n                    self.datasets[k] = ResultTestDataset()\n                for kk, vv in v.categories.items():\n                    if kk not in self.datasets[k].categories:\n                        self.datasets[k].categories[kk] = ResultTestCategory()\n                    for kkk, vvv in vv.captures.items():\n                        if kkk not in self.datasets[k].categories[kk].captures:\n                            self.datasets[k].categories[kk].captures[kkk] = TestMetric(\n                            )\n                        tresh = vvv._threshold[-1][0]\n                        self.datasets[k].categories[kk].captures[kkk]._ypred = np.concatenate(\n                            (self.datasets[k].categories[kk].captures[kkk]._ypred, vvv._ypred), axis=0, dtype=np.float64)\n                        self.datasets[k].categories[kk].captures[kkk]._ytrue = np.concatenate(\n                            (self.datasets[k].categories[kk].captures[kkk]._ytrue, vvv._ytrue), axis=0, dtype=np.float64)\n                        self.datasets[k].categories[kk].captures[kkk]._threshold.append(\n                            (tresh, len(self.datasets[k].categories[kk].captures[kkk]._ytrue)))\n                        [getattr(self.datasets[k].categories[kk].captures[kkk], x).update(getattr(\n                            vvv, x)) for x in (\"times\", \"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\")]\n            for k, v in self.datasets.items():\n                for kk, vv in v.categories.items():\n                    for kkk, vvv in vv.captures.items():\n                        if k not in other.datasets or kk not in other.datasets[k].categories or\\\n                                kkk not in other.datasets[k].categories[kk].captures:\n                            self.datasets[k].categories[kk].captures[kkk]._threshold.append(\n                                (tresh, len(self.datasets[k].categories[kk].captures[kkk]._ytrue)))\n        else:\n            # Recompute stats starting by the pcap granularity until the more generic one\n            self._ypred = np.array([], dtype=np.float64)\n            self._ytrue = np.array([], dtype=np.float64)\n            self.times = Times()\n            [setattr(self, x, Stats())\n             for x in (\"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\")]\n\n            for v in self.datasets.values():\n                v._ypred = np.array([], dtype=np.float64)\n                v._ytrue = np.array([], dtype=np.float64)\n                v.times = Times()\n                [setattr(v, x, Stats())\n                 for x in (\"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\")]\n\n                for vv in v.categories.values():\n                    vv._ypred = np.array([], dtype=np.float64)\n                    vv._ytrue = np.array([], dtype=np.float64)\n                    vv.times = Times()\n                    [setattr(vv, x, Stats()) for x in (\n                        \"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\")]\n\n                    for vvv in vv.captures.values():\n                        vvv.__post_init__()\n                        vv._ypred = np.concatenate(\n                            (vv._ypred, vvv._ypred), axis=0, dtype=np.float64)\n                        vv._ytrue = np.concatenate(\n                            (vv._ytrue, vvv._ytrue), axis=0, dtype=np.float64)\n                        [getattr(vv, x).update(getattr(vvv, x)) for x in (\n                            \"times\", \"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\")]\n                    it = next(vvv._threshold for vvv in vv.captures.values())\n                    vv._threshold = [(it[i][0], sum(\n                        vvv._threshold[i][1] for vvv in vv.captures.values())) for i in range(len(it))]\n                    vv.__post_init__()\n\n                    v._ypred = np.concatenate(\n                        (v._ypred, vv._ypred), axis=0, dtype=np.float64)\n                    v._ytrue = np.concatenate(\n                        (v._ytrue, vv._ytrue), axis=0, dtype=np.float64)\n                    [getattr(v, x).update(getattr(vv, x)) for x in (\n                        \"times\", \"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\")]\n\n                it = next(vv._threshold for vv in v.categories.values())\n                v._threshold = [(it[i][0], sum(vv._threshold[i][1]\n                                 for vv in v.categories.values())) for i in range(len(it))]\n                v.__post_init__()\n\n                self._ypred = np.concatenate(\n                    (self._ypred, v._ypred), axis=0, dtype=np.float64)\n                self._ytrue = np.concatenate(\n                    (self._ytrue, v._ytrue), axis=0, dtype=np.float64)\n                [getattr(self, x).update(getattr(v, x)) for x in (\n                    \"times\", \"tp_stats\", \"fp_stats\", \"tn_stats\", \"fn_stats\")]\n            it = next(v._threshold for v in self.datasets.values())\n            self._threshold = [(it[i][0], sum(v._threshold[i][1]\n                                for v in self.datasets.values())) for i in range(len(it))]\n            self.__post_init__()", ""]}
{"filename": "enid/lib/__init__.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFile containing the definitions of the labels for the supported datasets\nand the function to load their respective malicious flows' IDs.\n\"\"\"\nimport itertools", "\"\"\"\nimport itertools\nimport multiprocessing\nimport os\n\nfrom lxml import etree\n\nfrom .identifiers import TwoIPsKey, TwoIPsProtoPortsKey\n\n\ndef __internal_2012(xml_file):\n    attackers = set()\n    for child in etree.parse(xml_file).getroot():\n        if child.find('Tag').text == \"Normal\":\n            continue\n        protocol_string = child.find('protocolName').text.upper()\n        if \"TCP\" in protocol_string:\n            proto = \"TCP\"\n        elif \"UDP\" in protocol_string:\n            proto = \"UDP\"\n        elif \"ICMP\" in protocol_string:\n            proto = \"ICMP\"\n        else:\n            continue\n        attackers.add(TwoIPsProtoPortsKey.create(\n            \"IDS2012\", \"\", \"\",\n            ip=child.find('source').text,\n            ip1=child.find('destination').text,\n            proto=proto,\n            port=int(child.find('sourcePort').text),\n            port1=int(child.find('destinationPort').text)))\n    return attackers", "\n\ndef __internal_2012(xml_file):\n    attackers = set()\n    for child in etree.parse(xml_file).getroot():\n        if child.find('Tag').text == \"Normal\":\n            continue\n        protocol_string = child.find('protocolName').text.upper()\n        if \"TCP\" in protocol_string:\n            proto = \"TCP\"\n        elif \"UDP\" in protocol_string:\n            proto = \"UDP\"\n        elif \"ICMP\" in protocol_string:\n            proto = \"ICMP\"\n        else:\n            continue\n        attackers.add(TwoIPsProtoPortsKey.create(\n            \"IDS2012\", \"\", \"\",\n            ip=child.find('source').text,\n            ip1=child.find('destination').text,\n            proto=proto,\n            port=int(child.find('sourcePort').text),\n            port1=int(child.find('destinationPort').text)))\n    return attackers", "\n\ndef parse_xml_label_file_IDS2012(dir_path):\n    ret = set()\n    with multiprocessing.Pool(maxtasksperchild=1) as pool:\n        for k in pool.map(__internal_2012, [os.path.join(dir_path, x) for x in os.listdir(dir_path) if \".xml\" in x]):\n            ret.update(k)\n    return list(ret)\n\n", "\n\nATTACK_LABELS = {\n    'IDS2012': parse_xml_label_file_IDS2012,\n    'IDS2017': lambda _: tuple(\n        TwoIPsKey.create(\"IDS2017\", \"\", \"\", ip=x, ip1=y) for x, y in itertools.product(\n            ['172.16.0.1'], ['192.168.10.50'])),\n    'IDS2018': lambda _: tuple(\n        TwoIPsKey.create(\"IDS2018\", \"\", \"\", ip=x, ip1=y) for x, y in itertools.product(\n            ['18.218.115.60', '18.219.9.1', '18.219.32.43', '18.218.55.126', '52.14.136.135',", "        TwoIPsKey.create(\"IDS2018\", \"\", \"\", ip=x, ip1=y) for x, y in itertools.product(\n            ['18.218.115.60', '18.219.9.1', '18.219.32.43', '18.218.55.126', '52.14.136.135',\n                '18.219.5.43', '18.216.200.189', '18.218.229.235', '18.218.11.51', '18.216.24.42'],\n            ['18.218.83.150', '172.31.69.28'])),\n    'IDS2019': lambda _: tuple(\n        TwoIPsKey.create(\"IDS2019\", \"\", \"\", ip=x, ip1=y) for x, y in itertools.product(\n            ['172.16.0.5'], ['192.168.50.1', '192.168.50.4'])),\n    'BENIGN': lambda _: tuple(),\n}\n", "}\n"]}
{"filename": "enid/lib/identifiers.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFile containing the definitions of all the field and keys usable withing this framework.\n\"\"\"\nimport ipaddress\nimport socket", "import ipaddress\nimport socket\nimport sys\nfrom abc import ABC, abstractclassmethod, abstractmethod\nfrom dataclasses import dataclass, fields, field\nfrom typing import ClassVar, Tuple, Type\n\nfrom pypacker.layer3.ip import IP\nfrom pypacker.layer4.tcp import TCP\nfrom pypacker.layer4.udp import UDP", "from pypacker.layer4.tcp import TCP\nfrom pypacker.layer4.udp import UDP\nfrom pypacker.layer12.ethernet import Ethernet\n\nfrom .metrics import ComputationalRequirements\n\n\nclass SourceIP(int):\n    \"\"\"Type representing the Source IP address\"\"\"\n    def __new__(cls, x=0, *args, **kwargs):\n        if isinstance(x, (str, bytes)):\n            x = int(ipaddress.IPv4Address(x))\n        elif isinstance(x, Ethernet):\n            x = cls.extract(x)\n        return int.__new__(cls, x, *args, **kwargs)\n\n    @staticmethod\n    def extract(eth: Ethernet):\n        return SourceIP(eth[IP].src)\n\n    def __repr__(self):\n        return str(ipaddress.IPv4Address(self))", "\n\nclass DestinationIP(SourceIP):\n    \"\"\"Type representing the Destination IP address\"\"\"\n    @staticmethod\n    def extract(eth: Ethernet):\n        return DestinationIP(eth[IP].dst)\n\n\nclass IPProtocol(int):\n    \"\"\"Type representing the L4 protocol\"\"\"\n    PROTOCOLS: ClassVar[dict] = {int(num): name[len(\"IPPROTO_\"):] for name, num in vars(\n        socket).items() if name.startswith(\"IPPROTO_\")}\n\n    def __new__(cls, x=0, *args, **kwargs):\n        if isinstance(x, str):\n            x = next(k for k, v in IPProtocol.PROTOCOLS.items()\n                     if v == x.upper())\n        elif isinstance(x, Ethernet):\n            x = cls.extract(x)\n        return int.__new__(cls, x, *args, **kwargs)\n\n    @staticmethod\n    def extract(eth: Ethernet):\n        return IPProtocol(eth[IP].p)\n\n    def __repr__(self):\n        return IPProtocol.PROTOCOLS[self]", "\nclass IPProtocol(int):\n    \"\"\"Type representing the L4 protocol\"\"\"\n    PROTOCOLS: ClassVar[dict] = {int(num): name[len(\"IPPROTO_\"):] for name, num in vars(\n        socket).items() if name.startswith(\"IPPROTO_\")}\n\n    def __new__(cls, x=0, *args, **kwargs):\n        if isinstance(x, str):\n            x = next(k for k, v in IPProtocol.PROTOCOLS.items()\n                     if v == x.upper())\n        elif isinstance(x, Ethernet):\n            x = cls.extract(x)\n        return int.__new__(cls, x, *args, **kwargs)\n\n    @staticmethod\n    def extract(eth: Ethernet):\n        return IPProtocol(eth[IP].p)\n\n    def __repr__(self):\n        return IPProtocol.PROTOCOLS[self]", "\n\nclass SourcePort(int):\n    \"\"\"Type representing the Source L4 port\"\"\"\n    def __new__(cls, x=0, *args, **kwargs):\n        if isinstance(x, str):\n            x = int(x)\n        elif isinstance(x, Ethernet):\n            x = cls.extract(x)\n        return int.__new__(cls, x, *args, **kwargs)\n\n    @staticmethod\n    def extract(eth: Ethernet):\n        if eth[TCP]:\n            return SourcePort(eth[TCP].sport)\n        elif eth[UDP]:\n            return SourcePort(eth[UDP].sport)\n        return SourcePort(0)", "\n\nclass DestinationPort(SourcePort):\n    \"\"\"Type representing the Destination L4 port\"\"\"\n    @staticmethod\n    def extract(eth: Ethernet):\n        if eth[TCP]:\n            return DestinationPort(eth[TCP].dport)\n        elif eth[UDP]:\n            return DestinationPort(eth[UDP].dport)\n        return DestinationPort(0)", "\n\nclass Dataset(str):\n    \"\"\"Type representing the Dataset. Note that this\n    field is important, especially when mixing datasets\n    in order to avoid conflicts of session identifiers within 2\n    different datasets, which may be of different natures.\"\"\"\n    def __new__(cls, x=\"\", *args, **kwargs):\n        if isinstance(x, Ethernet):\n            x = cls.extract(x)\n        return str.__new__(cls, x, *args, **kwargs)\n\n    @staticmethod\n    def extract(eth: Ethernet):\n        return Dataset(eth.dataset)", "\n\nclass Category(str):\n    \"\"\"Type representing the Category.\"\"\"\n    def __new__(cls, x=\"\", *args, **kwargs):\n        if isinstance(x, Ethernet):\n            x = cls.extract(x)\n        return str.__new__(cls, x, *args, **kwargs)\n\n    @staticmethod\n    def extract(eth: Ethernet):\n        return Category(eth.category)", "\n\nclass Pcap(str):\n    \"\"\"Type representing the Pcap.\"\"\"\n    def __new__(cls, x=\"\", *args, **kwargs):\n        if isinstance(x, Ethernet):\n            x = cls.extract(x)\n        return str.__new__(cls, x, *args, **kwargs)\n\n    @staticmethod\n    def extract(eth: Ethernet):\n        return Pcap(eth.pcap)", "\n\n@dataclass(frozen=True)\nclass BaseKey(ABC):\n    \"\"\"Base key class, containing at least the dataset, category and pcap fields.\n    Note that, unless the dataset, the pcap and category are not used for hashing and\n    comparison, as it is supposed that within the same dataset a sessions is always the\n    same and does not change in nature (e.g., from benign to malicious).\"\"\"\n    dataset: Dataset\n    category: Category = field(hash=False, compare=False)\n    pcap: Pcap = field(hash=False, compare=False)\n\n    @classmethod\n    def extract(cls, eth: Ethernet):\n        return cls.create(**{k.name: k.type.extract(eth) for k in fields(cls)})\n\n    @property\n    @abstractmethod\n    def computational_requirements(cls) -> Tuple[ComputationalRequirements]:\n        \"\"\"Method to return the memory requirements for extracting the current ones\"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def memory_requirements(self) -> int:\n        \"\"\"Method to return the memory requirements for extracting the current ones\"\"\"\n        pass\n\n    @abstractclassmethod\n    def create(cls, dataset: Dataset, category: Category, pcap: Pcap, **kwargs):\n        raise NotImplementedError()\n\n    def cast(self, other_cls):\n        if self.__class__ == other_cls:\n            return self\n        return other_cls.create(**{k.name: getattr(self, k.name, None) for k in fields(other_cls)})\n\n    def to_json(self):\n        return {x.name: str(getattr(self, x.name)) for x in fields(self)}", "\n\n@dataclass(frozen=True)\nclass NoKey(BaseKey):\n    \"\"\"Identifier that groups all the traffic into a unique stats\"\"\"\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = tuple(\n    )\n    memory_requirements: ClassVar[int] = 0\n\n    @classmethod\n    def create(cls, dataset, category, pcap, **kwargs):\n        return cls(Dataset(dataset), Category(category), Pcap(pcap))", "\n\n@dataclass(frozen=True)\nclass SingleIPKey(BaseKey):\n    \"\"\"Identifier that group the traffic by the source IP address\"\"\"\n    ip: SourceIP\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (ComputationalRequirements.REQUIRED_L3.value,\n                                                                              ComputationalRequirements.BASE_MATH_OP.value)\n    memory_requirements: ClassVar[int] = 4\n\n    @classmethod\n    def create(cls, dataset, category, pcap, ip, **kwargs):\n        return cls(Dataset(dataset), Category(category), Pcap(pcap), SourceIP(ip))", "\n\n@dataclass(frozen=True)\nclass TwoIPsKey(SingleIPKey):\n    \"\"\"Identifier that group the traffic by the source and destination IP addresses\"\"\"\n    ip1: DestinationIP\n\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = SingleIPKey.computational_requirements + \\\n        (ComputationalRequirements.BASE_MATH_OP.value,\n         ComputationalRequirements.HASH_COMPUTATION.value)\n    memory_requirements: ClassVar[int] = SingleIPKey.memory_requirements + 4\n\n    @classmethod\n    def create(cls, dataset, category, pcap, ip, ip1, **kwargs):\n        ip, ip1 = SourceIP(ip), DestinationIP(ip1)\n        if ip1 > ip:\n            ip, ip1 = ip1, ip\n        return cls(Dataset(dataset), Category(category), Pcap(pcap), ip, ip1)", "\n\n@dataclass(frozen=True)\nclass TwoIPsProtoKey(TwoIPsKey):\n    \"\"\"Identifier that group the traffic by the source and destination IP addresses plus the L4 protocol\"\"\"\n    proto: IPProtocol\n\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = TwoIPsKey.computational_requirements + \\\n        (ComputationalRequirements.BASE_MATH_OP.value, )\n    memory_requirements: ClassVar[int] = TwoIPsKey.memory_requirements + 1\n\n    @classmethod\n    def create(cls, dataset, category, pcap, ip, ip1, proto, **kwargs):\n        ip, ip1 = SourceIP(ip), DestinationIP(ip1)\n        if ip1 > ip:\n            ip, ip1 = ip1, ip\n        return cls(Dataset(dataset), Category(category), Pcap(pcap), ip, ip1, IPProtocol(proto))", "\n\n@dataclass(frozen=True)\nclass TwoIPsProtoPortsKey(TwoIPsProtoKey):\n    \"\"\"Identifier that group the traffic by the source and destination IP addresses,\n    source and destination L4 ports and the L4 protocol\"\"\"\n    port: SourcePort\n    port1: DestinationPort\n\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = TwoIPsProtoKey.computational_requirements +\\\n        (ComputationalRequirements.BASE_MATH_OP.value,)\n    memory_requirements: ClassVar[int] = TwoIPsProtoKey.memory_requirements + 4\n\n    @classmethod\n    def create(cls, dataset, category, pcap, ip, ip1, port, port1, proto, **kwargs):\n        ip, ip1, port, port1 = SourceIP(ip), DestinationIP(\n            ip1), SourcePort(port), DestinationPort(port1)\n        if ip1 > ip:\n            ip, ip1, port, port1 = ip1, ip, port1, port\n        return cls(Dataset(dataset), Category(category), Pcap(pcap), ip, ip1, IPProtocol(proto), port, port1)", "\n\ndef str_to_key(name) -> Type[BaseKey]:\n    \"\"\"Function to return the Key class corresponding to the name provided\"\"\"\n    ret = getattr(sys.modules[__name__], name)\n    if not issubclass(ret, BaseKey):\n        raise ValueError(f\"{name} is not a BaseKey instance\")\n    return ret\n", ""]}
{"filename": "enid/lib/engines/lucid_rnn.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"File defining the LucidRnn Detection Engine\"\"\"\nfrom .lucid_mlp import LucidMlp, tf\n\n\nclass LucidRnn(LucidMlp):\n    \"\"\"Detection Engine composed of:\n    - Lucid traffic processing mechanism\n    - RNN (LSTM) as model architecture\n    - Same training parameters and combination defined in LucidCnn\n    \"\"\"\n\n    @classmethod\n    def _get_arch(\n            cls, packets_per_session: int = None, features: int = None,\n            max_packets_per_session: int = None, max_features: int = None,\n            kernels: int = 64, dropout: float = 0.2,\n            learning_rate: float = 0.001, **kwargs):\n        if max_packets_per_session:\n            packets_per_session = max_packets_per_session\n        if max_features:\n            features = max_features\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.LSTM(kernels, input_shape=(\n                packets_per_session, features), name=\"TARGET\"),\n            tf.keras.layers.Dropout(dropout, name=\"Dropout\"),\n            tf.keras.layers.Activation(\n                tf.keras.activations.relu, name=\"ReLu\"),\n            tf.keras.layers.Dense(1, name='FinalDense'),\n            tf.keras.layers.Activation(\n                tf.keras.activations.sigmoid, name=\"Sigmoid\")\n        ], name=cls.model_name(packets_per_session=packets_per_session, features=features))\n\n        if learning_rate:\n            model.compile(loss=tf.keras.metrics.binary_crossentropy,\n                          optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                          metrics=[\"accuracy\"])\n\n        return model", "\nclass LucidRnn(LucidMlp):\n    \"\"\"Detection Engine composed of:\n    - Lucid traffic processing mechanism\n    - RNN (LSTM) as model architecture\n    - Same training parameters and combination defined in LucidCnn\n    \"\"\"\n\n    @classmethod\n    def _get_arch(\n            cls, packets_per_session: int = None, features: int = None,\n            max_packets_per_session: int = None, max_features: int = None,\n            kernels: int = 64, dropout: float = 0.2,\n            learning_rate: float = 0.001, **kwargs):\n        if max_packets_per_session:\n            packets_per_session = max_packets_per_session\n        if max_features:\n            features = max_features\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.LSTM(kernels, input_shape=(\n                packets_per_session, features), name=\"TARGET\"),\n            tf.keras.layers.Dropout(dropout, name=\"Dropout\"),\n            tf.keras.layers.Activation(\n                tf.keras.activations.relu, name=\"ReLu\"),\n            tf.keras.layers.Dense(1, name='FinalDense'),\n            tf.keras.layers.Activation(\n                tf.keras.activations.sigmoid, name=\"Sigmoid\")\n        ], name=cls.model_name(packets_per_session=packets_per_session, features=features))\n\n        if learning_rate:\n            model.compile(loss=tf.keras.metrics.binary_crossentropy,\n                          optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                          metrics=[\"accuracy\"])\n\n        return model", ""]}
{"filename": "enid/lib/engines/__init__.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"]}
{"filename": "enid/lib/engines/lucid_mlp.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"File defining the LucidMlp Detection Engine\"\"\"\nfrom dataclasses import dataclass, field\n\nimport numpy as np\n", "import numpy as np\n\nfrom .lucid_cnn import LucidCnn, LucidDeParams, tf\n\n\n@dataclass\nclass MlpDeParams(LucidDeParams):\n    \"\"\"Class defining parameters for the Detection Model\"\"\"\n    regularization: bool = field(default=None, init=False, repr=False)\n", "\n\nclass LucidMlp(LucidCnn):\n    \"\"\"Detection Engine composed of:\n    - Lucid traffic processing mechanism\n    - Mlp (Fully-Connected) as model architecture\n    - Same training parameters and combination defined in LucidCnn\n    \"\"\"\n    de_params = MlpDeParams\n\n    @staticmethod\n    def _fed_adapt_layer(src_w, dst_w, global_f, local_f, pad=False):\n        if src_w.shape == dst_w.shape:\n            return src_w\n        indexes = [i for i, f in enumerate(global_f) if f in local_f]\n        if pad:\n            ret = np.zeros(\n                dst_w.shape if dst_w.shape[0] > src_w.shape[0] else src_w)\n            ret[indexes, ...] = src_w[indexes, ...]\n            return ret\n        return src_w[indexes, ...]\n\n    @classmethod\n    def _get_arch(\n            cls, packets_per_session: int = None, features: int = None,\n            max_packets_per_session: int = None, max_features: int = None,\n            kernels: int = 64, dropout: float = 0.2,\n            learning_rate: float = 0.001, **kwargs):\n        if max_packets_per_session:\n            packets_per_session = max_packets_per_session\n        if max_features:\n            features = max_features\n\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.InputLayer(\n                input_shape=(packets_per_session, features), name=\"Input\"),\n            tf.keras.layers.Flatten(name=\"Flatten\"),\n            tf.keras.layers.Dense(kernels, name='TARGET'),\n            tf.keras.layers.Dropout(dropout, name=\"Dropout\"),\n            tf.keras.layers.Activation(\n                tf.keras.activations.relu, name=\"ReLu\"),\n            tf.keras.layers.Dense(1, name='FinalDense'),\n            tf.keras.layers.Activation(\n                tf.keras.activations.sigmoid, name=\"Sigmoid\")\n        ], name=cls.model_name(packets_per_session=packets_per_session, features=features))\n\n        if learning_rate:\n            model.compile(loss=tf.keras.metrics.binary_crossentropy,\n                          optimizer=tf.keras.optimizers.Adam(\n                              learning_rate=learning_rate),\n                          metrics=[\"accuracy\"])\n\n        return model", ""]}
{"filename": "enid/lib/engines/lucid_cnn/__init__.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"File defining the LucidCnn Detection Engine, its TrafficAnalyser and all\nthe parameters used.\"\"\"\nimport copy\nimport math\nimport os", "import math\nimport os\nimport time\nfrom dataclasses import dataclass, field, fields, replace\nfrom functools import lru_cache\nfrom typing import Dict, List, Tuple, Type, Union\n\nimport h5py\nimport numpy as np\nimport tensorflow as tf", "import numpy as np\nimport tensorflow as tf\nfrom pypacker.layer12.ethernet import Ethernet\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom ...definitions import (AnalysisState, BaseProcessingData, DeParams,\n                            DetectionEngine, TrafficAnalyser)\nfrom ...identifiers import BaseKey\nfrom ...metrics import TrainMetric\nfrom ...utility import (SplitType, get_all_dict_comb, get_best_comb,", "from ...metrics import TrainMetric\nfrom ...utility import (SplitType, get_all_dict_comb, get_best_comb,\n                        get_best_dispatch, get_logger, load_json_data,\n                        set_seed, sort_keep_comb)\nfrom .features import LucidFeaturesHolder, Time\n\n_logger = get_logger(__name__)\n\n\n@dataclass\nclass LucidDeParams(DeParams):\n    \"\"\"Class defining parameters for the Detection Model\"\"\"\n    rerank_at_feature: int = None\n    rerank_at_packet: int = None\n    train_with_less: bool = False\n    prune_zero_first: bool = False\n    learn_only_from_benign: bool = False\n    rank_metric: str = \"f1_score\"\n    max_loss: int = 20\n    epochs: int = 500\n    malicious_threshold: int = 0.5\n\n    learning_rate: List[float] = (0.1, 0.01, 0.001)\n    batch_size: List[int] = (512, 1024, 2048)\n    kernels: List[int] = (8, 16, 32, 64)\n    dropout: List[float] = (0.2, 0.5, 0.8)\n    regularization: List[str] = (\"l1\", \"l2\")  # used only for CNN\n\n    @property\n    def is_to_train(self):\n        \"\"\"Return true if the current model is to train according to the parameters\"\"\"\n        return self.train_with_less or (not self.max_packets_per_session and not self.max_features) or\\\n            (self.max_packets_per_session and self.max_features and self.packets_per_session == self.max_packets_per_session\n             and self.features == self.max_features) or\\\n            (self.max_packets_per_session and not self.max_features and\n                self.packets_per_session == self.max_packets_per_session) or\\\n            (self.max_features and not self.max_packets_per_session and\n                self.features == self.max_features)\n\n    @property\n    def is_to_rerank(self):\n        \"\"\"Return true if the features need to be reranked\"\"\"\n        return (not self.rerank_at_packet and not self.rerank_at_feature) or\\\n            (self.rerank_at_packet and self.rerank_at_feature and self.packets_per_session == self.rerank_at_packet and\n                self.features == self.rerank_at_feature) or\\\n            (not self.rerank_at_packet and self.rerank_at_feature and self.features == self.rerank_at_feature) or\\\n            (not self.rerank_at_feature and self.rerank_at_packet and self.packets_per_session ==\n             self.rerank_at_packet)\n\n    @property\n    def is_load_previous_rank(self):\n        \"\"\"Return true if needed to load the previous features\"\"\"\n        return self.rerank_at_feature == self.features and self.rerank_at_packet != self.packets_per_session", "\n@dataclass\nclass LucidDeParams(DeParams):\n    \"\"\"Class defining parameters for the Detection Model\"\"\"\n    rerank_at_feature: int = None\n    rerank_at_packet: int = None\n    train_with_less: bool = False\n    prune_zero_first: bool = False\n    learn_only_from_benign: bool = False\n    rank_metric: str = \"f1_score\"\n    max_loss: int = 20\n    epochs: int = 500\n    malicious_threshold: int = 0.5\n\n    learning_rate: List[float] = (0.1, 0.01, 0.001)\n    batch_size: List[int] = (512, 1024, 2048)\n    kernels: List[int] = (8, 16, 32, 64)\n    dropout: List[float] = (0.2, 0.5, 0.8)\n    regularization: List[str] = (\"l1\", \"l2\")  # used only for CNN\n\n    @property\n    def is_to_train(self):\n        \"\"\"Return true if the current model is to train according to the parameters\"\"\"\n        return self.train_with_less or (not self.max_packets_per_session and not self.max_features) or\\\n            (self.max_packets_per_session and self.max_features and self.packets_per_session == self.max_packets_per_session\n             and self.features == self.max_features) or\\\n            (self.max_packets_per_session and not self.max_features and\n                self.packets_per_session == self.max_packets_per_session) or\\\n            (self.max_features and not self.max_packets_per_session and\n                self.features == self.max_features)\n\n    @property\n    def is_to_rerank(self):\n        \"\"\"Return true if the features need to be reranked\"\"\"\n        return (not self.rerank_at_packet and not self.rerank_at_feature) or\\\n            (self.rerank_at_packet and self.rerank_at_feature and self.packets_per_session == self.rerank_at_packet and\n                self.features == self.rerank_at_feature) or\\\n            (not self.rerank_at_packet and self.rerank_at_feature and self.features == self.rerank_at_feature) or\\\n            (not self.rerank_at_feature and self.rerank_at_packet and self.packets_per_session ==\n             self.rerank_at_packet)\n\n    @property\n    def is_load_previous_rank(self):\n        \"\"\"Return true if needed to load the previous features\"\"\"\n        return self.rerank_at_feature == self.features and self.rerank_at_packet != self.packets_per_session", "\n\n@dataclass\nclass ProcessingData(BaseProcessingData):\n    indexes_benign: List = field(default_factory=list)\n    indexes_malicious: List = field(default_factory=list)\n\n\n@dataclass\nclass LucidAnalysisState(AnalysisState):\n    current_features: Type[LucidFeaturesHolder] = field(default=None)\n    params: LucidDeParams = field(default=None)", "@dataclass\nclass LucidAnalysisState(AnalysisState):\n    current_features: Type[LucidFeaturesHolder] = field(default=None)\n    params: LucidDeParams = field(default=None)\n\n\nclass LucidTrafficAnalyser(TrafficAnalyser):\n    \"\"\"Traffic Analyser class that implements the extraction mechanism and the\n    termination of a time window while processing data.\"\"\"\n    analysis_state_cls: Type[AnalysisState] = LucidAnalysisState\n\n    def _extract(self, sess_id: Type[BaseKey], eth: Ethernet):\n        self.current_session_map[sess_id].value.append(\n            tuple(y.create(eth) for y in self.analysis_state.current_features.value))\n\n    def _terminate_timewindow_preprocessing(self):\n        if not self.current_session_map:\n            return\n        asd = next((x for x in self.attackers.values()), None)\n        for i, k in enumerate(self.current_session_map):\n            target = \"benign\"\n            if asd and k.cast(asd[0]) in asd[1]:\n                target = \"malicious\"\n            setattr(self.processing_stats, f\"tot_{target}_packets\",\n                    getattr(self.processing_stats, f\"tot_{target}_packets\") + self.current_session_map[k].metered_packets +\n                    self.current_session_map[k].unmetered_packets)\n\n            if k not in self.seen_sessions_previous_prediction:\n                setattr(self.processing_stats, f\"unique_{target}\", getattr(\n                    self.processing_stats, f\"unique_{target}\") + 1)\n                self.seen_sessions_previous_prediction[k] = True\n            setattr(self.processing_stats, f\"tot_{target}\", getattr(\n                self.processing_stats, f\"tot_{target}\") + 1)\n            getattr(self.processing_stats, f\"indexes_{target}\").append(i)\n        preprocessed, _, _ = self.de.preprocess_samples(self)\n        with h5py.File(f\"{self.dump_path}.h5\", 'a') as hf:\n            for ttype in [\"benign\", \"malicious\"]:\n                if not len(getattr(self.processing_stats, f\"indexes_{ttype}\")):\n                    continue\n                vals = preprocessed[getattr(\n                    self.processing_stats, f\"indexes_{ttype}\")]\n                getattr(self.processing_stats, f\"indexes_{ttype}\").clear()\n                if ttype in hf:\n                    hf[ttype].resize(\n                        (hf[ttype].shape[0] + vals.shape[0]), axis=0)\n                    hf[ttype][-vals.shape[0]:] = vals\n                else:\n                    hf.create_dataset(ttype, data=vals,\n                                      maxshape=(None, *vals.shape[1:]))", "\n\nclass LucidCnn(DetectionEngine):\n    \"\"\"Detection Engine composed of:\n    - Lucid traffic processing mechanism\n    - CNN as detection model\n    - Custom ranking mechanism\n    \"\"\"\n\n    features_holder_cls = LucidFeaturesHolder\n    traffic_analyser_cls = LucidTrafficAnalyser\n    processing_data_cls = ProcessingData\n    de_params = LucidDeParams\n\n    def __init__(self, analysis_state: LucidAnalysisState, base_dir) -> None:\n        super().__init__(analysis_state, base_dir)\n        self.analysis_state: LucidAnalysisState\n        self.maxs: np.ndarray = None\n        self.indexes = None\n        self.adjust_timestamp: int = None\n        self._init_scale_method()\n\n    @classmethod\n    def predict(cls, model, data, batch_size: int = None, **kwargs) -> np.ndarray:\n        return np.squeeze(model.predict(data, batch_size=batch_size,\n                                        verbose=0).astype(np.float64), axis=1)\n\n    @classmethod\n    def model_name(cls, packets_per_session: int, features: int = None, **kwargs):\n        if not features:\n            features = len(cls.features_holder_cls.ALLOWED)\n        return \"{}p-{}f\".format(packets_per_session, features)\n\n    @staticmethod\n    @lru_cache\n    def _internal_load(basename):\n        \"\"\"Internal method to load the dataset asynchronously\"\"\"\n        ret = []\n        for name in (\"train\", \"validation\", \"test\"):\n            with h5py.File(os.path.join(basename, f\"{name}.h5\"), 'r') as dataset:\n                x = dataset[\"set_x\"][:].astype(np.float64)\n                y = dataset[\"set_y\"][:].astype(np.float64)\n            ret.append(list(sort_keep_comb(x, y)))\n        return ret\n\n    @staticmethod\n    def _fed_adapt_layer(src_w, dst_w, global_f, local_f, pad=False):\n        if src_w.shape == dst_w.shape:\n            return src_w\n        indexes = [i for i, f in enumerate(global_f) if f in local_f]\n        if pad:\n            ret = np.zeros(\n                (max(dst_w.shape[0], src_w.shape[0]),\n                 max(dst_w.shape[1], src_w.shape[1]),\n                 *dst_w.shape[2:]))\n            ret[:src_w.shape[0], indexes, ...] = src_w[:, :, ...]\n            return ret\n        return src_w[:min(src_w.shape[0], dst_w.shape[0]), indexes, ...]\n\n    @staticmethod\n    def _load_dataset(basename, features_holder: LucidFeaturesHolder, packets_per_session=None,\n                      max_packets_per_session=None, max_features=None,\n                      split_type=SplitType.NONE, split_chunk: Tuple[int, int] = tuple(), **kwargs):\n        \"\"\"Load dataset and format it according the current parameters features/packets\"\"\"\n        not_current_indexes = [i for i, k in enumerate(features_holder.ALLOWED)\n                               if k not in features_holder.value]\n        current_indexes = [i for i, k in enumerate(features_holder.ALLOWED)\n                           if k in features_holder.value]\n\n        ret = copy.deepcopy(LucidCnn._internal_load(basename))\n        # if max_packets or max_features then keep their position but with the\n        # value in that coord set to 0\n        for i in range(len(ret)):\n            if split_type == SplitType.EQUALLY:\n                n_samples_per_type = math.floor(\n                    len(ret[i][0])/2/split_chunk[1])\n                indexes = np.where(ret[i][1] == 1.0)[\n                    0][split_chunk[0]*n_samples_per_type:(split_chunk[0]+1)*n_samples_per_type]\n                indexes = np.concatenate((indexes, np.where(ret[i][1] == 0.0)[\n                                         0][split_chunk[0]*n_samples_per_type:(split_chunk[0]+1)*n_samples_per_type]))\n                ret[i][0] = ret[i][0][indexes, ...]\n                ret[i][1] = ret[i][1][indexes, ...]\n            elif split_type == SplitType.NONE:\n                pass\n            else:\n                raise NotImplementedError()\n\n            if not packets_per_session:  # Ho solo features aggregate, non pacchetti\n                if max_features:\n                    ret[i][0][:, not_current_indexes] = 0.0\n                else:\n                    ret[i][0] = ret[i][0][:, current_indexes]\n            else:  # Ho anche pacchetti come dimensione\n                if max_packets_per_session:\n                    ret[i][0] = ret[i][0][:, :max_packets_per_session, :]\n                    ret[i][0][:, packets_per_session:, :] = 0.0\n                else:\n                    ret[i][0] = ret[i][0][:, :packets_per_session, :]\n                if max_features:\n                    ret[i][0][:, :, not_current_indexes] = 0.0\n                else:\n                    ret[i][0] = ret[i][0][:, :, current_indexes]\n        return ret[0], ret[1], ret[2]\n\n    @classmethod\n    def load_model(cls, params: Union[Dict, LucidDeParams], base_dir: str):\n        if isinstance(params, LucidDeParams):\n            params = params.__dict\n        current_name = cls.model_name(**params)\n        current_features = cls.features_holder_cls(**load_json_data(\n            os.path.join(base_dir, current_name, \"relevance.json\")))\n        params = LucidDeParams(**load_json_data(os.path.join(\n            base_dir, current_name, \"params.json\")))\n\n        max_par = {\n            \"packets_per_session\": params.max_packets_per_session or params.packets_per_session,\n            \"features\": params.max_features or params.features}\n        model = cls._get_arch(\n            **max_par, **{k: v for k, v in params.__dict__.items() if k not in max_par})\n        model.load_weights(os.path.join(\n            base_dir, cls.model_name(**max_par), \"weights.h5\"))\n        return current_features, params, model\n\n    def preprocess_samples(self, ta: LucidTrafficAnalyser):\n        is_nested = isinstance(\n            next(x for x in ta.current_session_map.values()).value, list)\n\n        # convert data into input-compliant\n        conversion_time = time.clock_gettime_ns(time.CLOCK_PROCESS_CPUTIME_ID)\n        if is_nested:\n            data = np.zeros((\n                len(ta.current_session_map),\n                self.analysis_state.params.max_packets_per_session or self.analysis_state.params.packets_per_session,\n                self.analysis_state.params.max_features or self.analysis_state.params.features),\n                dtype=np.float64)\n            for i, v in enumerate(ta.current_session_map.values()):\n                for ii, vv in enumerate(v.value):\n                    for iii, vvv in zip(self.indexes, vv):\n                        data[i, ii, iii] = vvv.value\n        else:\n            data = np.zeros((len(ta.current_session_map),\n                             self.analysis_state.params.max_features or self.analysis_state.params.features),\n                            dtype=np.float64)\n            for i, v in enumerate(ta.current_session_map.values()):\n                for ii, vv in zip(self.indexes, v.value):\n                    data[i, ii] = vv.value\n\n        conversion_time = time.clock_gettime_ns(\n            time.CLOCK_PROCESS_CPUTIME_ID) - conversion_time\n        preprocessing_time = time.clock_gettime_ns(\n            time.CLOCK_PROCESS_CPUTIME_ID)\n        # adjust timestamp\n        if self.adjust_timestamp:\n            data[:, :, self.adjust_timestamp] -= data[:,\n                                                      [0], self.adjust_timestamp]\n        # set min to zero\n        data[data < 0] = 0.0\n        # scale between 0 and max value\n        data[..., :] /= self.maxs\n        # remove nan\n        np.nan_to_num(data, copy=False)\n        preprocessing_time = time.clock_gettime_ns(\n            time.CLOCK_PROCESS_CPUTIME_ID) - preprocessing_time\n        return data, conversion_time, preprocessing_time\n\n    @classmethod\n    def _compute_features_weights(\n            cls, features_holder: LucidFeaturesHolder, x: np.ndarray, y: np.ndarray,\n            model, malicious_threshold, batch_size, metric, only_active=True):\n        \"\"\"Method to rank the features. Given this metric, the method\n        computes the rank of each feature by:\n        1. Computing the baseline value of the metric with the entire input set as it is\n        2. For each feature, the algorithm sets that feature to zero and computes the new metric\n        3. The rank of each feature is given by the performance loss/gain given by the difference\n        between the new value and the baseline.\"\"\"\n        baseline = TrainMetric(\n            _threshold=malicious_threshold, _ytrue=y,\n            _ypred=cls.predict(model, x, batch_size=batch_size))\n        minimize = metric in (\"log_loss\", \"fp\", \"fpr\", \"fn\", \"fnr\")\n\n        if only_active:\n            indexes = list(range(features_holder.n_current))\n        else:\n            indexes = [i for i, k in enumerate(\n                features_holder.ALLOWED) if k in features_holder.value]\n        for i, k in zip(indexes, features_holder.value):\n            x_tmp = np.copy(x)\n            x_tmp[..., i] = 0.0\n\n            t = TrainMetric(\n                _threshold=malicious_threshold, _ytrue=y,\n                _ypred=np.squeeze(model.predict(\n                    x_tmp, batch_size=batch_size, verbose=0), axis=1).astype(np.float64))\n            v_base = getattr(baseline, metric)\n            v_current = getattr(t, metric)\n            if v_base == 0 or v_current == 0:\n                # fallback accuracy metric as not possible to compute\n                _logger.info(f\"Metric for feature {k} switched to fallback accuracy,\"\n                             f\" as baseline={v_base} and current={v_current}\")\n                features_holder.value[k] = baseline.accuracy - t.accuracy\n            elif minimize:\n                features_holder.value[k] = v_current - v_base\n            else:\n                features_holder.value[k] = v_base - v_current\n\n    @classmethod\n    def parameters(cls, model=None, features=None, packets_per_session=None,\n                   max_features=None, max_packets_per_session=None, **params):\n        \"\"\"Method to return the number of trainable parameters of the model.\"\"\"\n        if model is None:\n            model = cls._get_arch(packets_per_session=max_packets_per_session or packets_per_session,\n                                  features=max_features or features, **params)\n        return int(np.sum([tf.keras.backend.count_params(p) for p in model.trainable_weights]))\n\n    @classmethod\n    def _get_arch(\n            cls, packets_per_session: int = None, features: int = None,\n            max_packets_per_session: int = None, max_features: int = None,\n            kernels: int = 64, dropout: float = 0.2,\n            regularization: str = \"l2\", learning_rate: float = 0.001,  **kwargs):\n        \"\"\"Method that defines the architecture of the CNN model\"\"\"\n        if max_packets_per_session:\n            packets_per_session = max_packets_per_session\n        if max_features:\n            features = max_features\n\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Reshape((packets_per_session, features, 1),\n                                    input_shape=(packets_per_session, features)),\n            # kernel size = (minimo tra n\u00b0 packets e 3, n\u00b0 features) - input shape = (n\u00b0pacchetti, n\u00b0features, 1)\n            tf.keras.layers.Conv2D(kernels, (min(packets_per_session, 3), features),\n                                   kernel_regularizer=regularization, name='TARGET'),\n            tf.keras.layers.Dropout(dropout, name=\"Dropout\"),\n            tf.keras.layers.Activation(tf.keras.activations.relu, name=\"ReLu\"),\n            # pool size = (massimo tra 1 e n\u00b0pacchetti-2, 1)\n            tf.keras.layers.GlobalMaxPooling2D(name=\"MaxPooling\"),\n            tf.keras.layers.Flatten(name=\"Flatten\"),\n            tf.keras.layers.Dense(1, name='FinalDense'),\n            tf.keras.layers.Activation(\n                tf.keras.activations.sigmoid, name=\"Sigmoid\")\n        ], name=cls.model_name(packets_per_session=packets_per_session, features=features))\n\n        if learning_rate:\n            model.compile(loss=tf.keras.metrics.binary_crossentropy,\n                          optimizer=tf.keras.optimizers.Adam(\n                              learning_rate=learning_rate),\n                          metrics=[\"accuracy\"])\n        return model\n\n    def _init_scale_method(self):\n        \"\"\"Method to compute the max value of each feature for the scaling, and set the max value of the\n        Time feature to the time window\"\"\"\n        Time.limit = self.analysis_state.time_window if self.analysis_state.time_window > 0 else (\n            1 << 64)\n\n        # check if need to keep features indexes also for those inactive\n        if self.analysis_state.params.max_features is not None:\n            self.indexes = [i for i, f in enumerate(\n                self.features_holder_cls.ALLOWED) if f in self.analysis_state.current_features.value]\n            self.maxs = np.array(\n                [x.limit for x in self.analysis_state.current_features.ALLOWED], dtype=np.float64)\n            self.adjust_timestamp = next((i for i, k in enumerate(\n                self.analysis_state.current_features.ALLOWED) if k == Time), None)\n        else:\n            self.indexes = list(\n                range(self.analysis_state.params.features))\n            self.maxs = np.array(\n                [x.limit for x in self.analysis_state.current_features.value], dtype=np.float64)\n            self.adjust_timestamp = next((i for i, k in enumerate(\n                self.analysis_state.current_features.value) if k == Time), None)\n\n    @staticmethod\n    def append_to_dataset(source, dest, ttype, label, indexes_tr, indexes_val, indexes_ts, **kwargs):\n        source += \".h5\"\n        with h5py.File(source, 'r') as dataset:\n            t = dataset[ttype][:]\n\n        for f, iii in zip((\"train\", \"validation\", \"test\"), (indexes_tr, indexes_val, indexes_ts)):\n            with h5py.File(os.path.join(dest, f\"{f}.h5\"), 'a') as new_dataset:\n                t_tmp = t[iii, ...]\n                plus_shape = len(iii)\n                if 'set_x' in new_dataset:\n                    new_dataset['set_x'].resize(\n                        (new_dataset['set_x'].shape[0] + plus_shape), axis=0)\n                    new_dataset['set_x'][-plus_shape:] = t_tmp\n                    new_dataset['set_y'].resize(\n                        (new_dataset['set_y'].shape[0] + plus_shape), axis=0)\n                    new_dataset['set_y'][-plus_shape:\n                                         ] = np.array([label]*plus_shape, dtype=np.float64)\n                else:\n                    new_dataset.create_dataset(\n                        'set_x', data=t_tmp, maxshape=(None, *t_tmp.shape[1:]))\n                    new_dataset.create_dataset('set_y', data=np.array(\n                        [label]*plus_shape, dtype=np.float64), maxshape=(None,))\n\n    @classmethod\n    def train(cls, dataset_path: str, models_dir, train_param: LucidDeParams,\n              split_type: SplitType, split_chunk: Tuple[int, int],\n              packets_per_session: int, features: int,\n              autoencoder=False):\n        previous = train_param.previous_one(packets_per_session, features)\n        # start from a previous features holder and pop less relevant feature\n        # untill the current number of features is matched\n        if previous is False:\n            features_holder = cls.features_holder_cls()\n        else:\n            pname = cls.model_name(packets_per_session=previous[0],\n                                   features=previous[1])\n            features_holder = cls.features_holder_cls(**load_json_data(\n                os.path.join(models_dir, os.pardir, pname, \"relevance.json\")))\n            to_pop = previous[2]\n            while to_pop:\n                features_holder.pop_less_relevant(\n                    **train_param.__dict__)\n                to_pop -= 1\n\n        hyper = {k.name: getattr(train_param, k.name) for k in fields(\n            train_param) if isinstance(k.default, (tuple, list))}\n\n        # set current pair of trained values\n        train_param = replace(\n            train_param, **{\"packets_per_session\": packets_per_session, \"features\": features})\n\n        set_seed()\n        _logger.info(\"Loading dataset\")\n        # load the dataset according to the current parameters\n        (xt, yt, _), (xv, yv, _), (xts, yts, pt) = cls._load_dataset(\n            dataset_path, features_holder, packets_per_session=packets_per_session,\n            max_packets_per_session=train_param.max_packets_per_session,\n            max_features=train_param.max_features,\n            split_type=split_type, split_chunk=split_chunk)\n        set_seed()\n        if train_param.learn_only_from_benign:\n            xt = xt[np.where(yt == 0.0)[0], ...]\n            yt = yt[np.where(yt == 0.0)[0], ...]\n            xvv = xv\n            yvv = yv\n            xv = xv[np.where(yv == 0.0)[0], ...]\n            yv = yv[np.where(yv == 0.0)[0], ...]\n\n        if autoencoder:\n            yt = np.reshape(\n                xt, (xt.shape[0], np.prod([x for x in xt.shape[1:]])))\n            yv = np.reshape(\n                xv, (xv.shape[0], np.prod([x for x in xv.shape[1:]])))\n\n        name = cls.model_name(\n            packets_per_session=packets_per_session, features=features)\n\n        combs = get_all_dict_comb(hyper)\n\n        n_len = len(combs)\n\n        if train_param.is_to_train:\n            if n_len > 1:\n                n_comb = get_best_comb(n_len)\n                n_dispatch = get_best_dispatch(\n                    xt.nbytes+yt.nbytes+xv.nbytes+yv.nbytes)\n                _logger.info(\"RandomizedSearchCV model {} on {} combinations over {} and dispatching {} jobs\".\n                             format(name, n_comb, n_len, n_dispatch))\n                rnd_search_cv = RandomizedSearchCV(\n                    tf.keras.wrappers.scikit_learn.KerasClassifier(\n                        build_fn=cls._get_arch, packets_per_session=train_param.max_packets_per_session or packets_per_session,\n                        features=train_param.max_features or features, verbose=0),\n                    hyper, cv=[(slice(None), slice(None))],\n                    n_iter=n_comb, refit=False, verbose=0, n_jobs=-1, pre_dispatch=n_dispatch)\n                rnd_search_cv.fit(x=xt, y=yt, epochs=train_param.epochs, validation_data=(xv, yv),\n                                  callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                                              mode=\"min\",\n                                                                              patience=train_param.max_loss)])\n                hyper = rnd_search_cv.best_params_\n            else:\n                hyper = combs[0]\n\n            [setattr(train_param, k, v) for k, v in hyper.items()]\n\n            _logger.info(\n                f\"Fitting Model {name} using best parameters\")\n            best_model = cls._get_arch(packets_per_session=train_param.max_packets_per_session or packets_per_session,\n                                       features=train_param.max_features or features, **hyper)\n\n            hs = best_model.fit(x=xt, y=yt,\n                                validation_data=(xv, yv),\n                                batch_size=train_param.batch_size,\n                                epochs=train_param.epochs, verbose=1,\n                                callbacks=[\n                                    tf.keras.callbacks.EarlyStopping(\n                                        monitor='val_loss', mode=\"min\", patience=train_param.max_loss, min_delta=0.01),\n                                    tf.keras.callbacks.ModelCheckpoint(\n                                        filepath=os.path.join(\n                                            models_dir, \"weights.h5\"),\n                                        monitor='val_loss', mode='min',\n                                        save_best_only=True, save_weights_only=True)]).history\n        else:\n            hyper = {k: v for k, v in load_json_data(os.path.join(models_dir, os.pardir, cls.model_name(\n                packets_per_session=train_param.max_packets_per_session or packets_per_session,\n                features=train_param.max_features or features), \"params.json\")).items() if k in hyper}\n            [setattr(train_param, k, v) for k, v in hyper.items()]\n            best_model = cls._get_arch(packets_per_session=train_param.max_packets_per_session or packets_per_session,\n                                       features=train_param.max_features or features, **hyper)\n            hs = []\n            models_dir = os.path.join(models_dir, os.pardir, cls.model_name(\n                packets_per_session=train_param.max_packets_per_session, features=train_param.max_features or features))\n\n        best_model.load_weights(os.path.join(models_dir, \"weights.h5\"))\n\n        if train_param.malicious_threshold < 0:\n            train_param.malicious_threshold = TrainMetric.get_best_threshold_roc(\n                cls.predict(best_model, xv, **hyper), yv)\n\n        if train_param.is_to_rerank:\n            _logger.info(f\"Computing features importances for {name}\")\n            cls._compute_features_weights(features_holder, xvv, yvv, best_model,\n                                          train_param.malicious_threshold, train_param.batch_size,\n                                          metric=train_param.rank_metric,\n                                          only_active=train_param.max_features is None)\n\n        if train_param.is_load_previous_rank:\n            previous = cls.model_name(packets_per_session=train_param.rerank_at_packet,\n                                      features=train_param.rerank_at_feature)\n            features_holder = features_holder.__class__(**load_json_data(\n                os.path.join(models_dir, os.pardir, previous, \"relevance.json\")))\n\n        return hs, train_param, best_model, xts, yts, features_holder, pt", ""]}
{"filename": "enid/lib/engines/lucid_cnn/features.py", "chunked_list": ["# Copyright 2023 ENID\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFile that defines the features required by Lucid, their extraction mechanisms\nand parameters. Also, the FeatureHolder class with the method for popping the\nless relevant feature is defined.\n\"\"\"", "less relevant feature is defined.\n\"\"\"\nimport sys\nfrom collections import OrderedDict\nfrom dataclasses import dataclass, field\nfrom typing import ClassVar\nfrom typing import OrderedDict as OrderedDictType\nfrom typing import Tuple, Type\n\nfrom pypacker.layer3.icmp import ICMP", "\nfrom pypacker.layer3.icmp import ICMP\nfrom pypacker.layer3.ip import IP\nfrom pypacker.layer4.ssl import SSL\nfrom pypacker.layer4.tcp import TCP\nfrom pypacker.layer4.udp import UDP\nfrom pypacker.layer12.arp import ARP\nfrom pypacker.layer12.ethernet import Ethernet\nfrom pypacker.layer567.dns import DNS\nfrom pypacker.layer567.http import HTTP", "from pypacker.layer567.dns import DNS\nfrom pypacker.layer567.http import HTTP\nfrom pypacker.layer567.telnet import Telnet\nfrom pypacker.pypacker import Packet\n\nfrom ...definitions import (BaseFeature, BaseKey, ComputationalRequirements,\n                            FeaturesHolder)\n\n# Lucid supported protocols used in previous work\n_SUPPORTED_PROTOCOLS: Tuple[Type[Packet]] = (", "# Lucid supported protocols used in previous work\n_SUPPORTED_PROTOCOLS: Tuple[Type[Packet]] = (\n    HTTP, Telnet, SSL, UDP, TCP, IP, ICMP, DNS, Ethernet, ARP)\n\n\n@dataclass\nclass HighestLayer(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L4, ComputationalRequirements.ENHANCED_MATH_OP,\n        ComputationalRequirements.HASH_COMPUTATION, ComputationalRequirements.ENHANCED_MATH_OP)\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 10\n\n    def extract(self, eth: Ethernet):\n        self.value = 1 << 10 - \\\n            next(i for i, x in enumerate(_SUPPORTED_PROTOCOLS) if x in eth)", "\n\n@dataclass\nclass Protocols(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L4, ComputationalRequirements.ENHANCED_MATH_OP,\n        ComputationalRequirements.HASH_COMPUTATION) +\\\n        tuple(ComputationalRequirements.BASE_MATH_OP for _ in range(\n            len(_SUPPORTED_PROTOCOLS)))\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 10\n\n    def extract(self, eth: Ethernet):\n        protocols = tuple(x.__class__ for x in eth)\n        self.value = int(\n            ''.join(\"1\" if p in protocols else \"0\" for p in _SUPPORTED_PROTOCOLS), 2)", "\n\n@dataclass\nclass Time(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.TIMER,)\n    memory_requirements: ClassVar[int] = 8\n    limit: ClassVar[int] = None\n\n    def extract(self, eth: Ethernet):\n        self.value = eth.timestamp", "\n\n@dataclass\nclass IcmpType(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L3, ComputationalRequirements.BASE_MATH_OP)\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 16\n\n    def extract(self, eth: Ethernet):\n        if eth[ICMP]:\n            self.value = eth[ICMP].type", "\n\n@dataclass\nclass UDPLength(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L4, ComputationalRequirements.BASE_MATH_OP)\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 16\n\n    def extract(self, eth: Ethernet):\n        if eth[UDP]:\n            self.value = eth[UDP].ulen - 8", "\n\n@dataclass\nclass TCPWindow(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L4, ComputationalRequirements.BASE_MATH_OP)\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 16\n\n    def extract(self, eth: Ethernet):\n        if eth[TCP]:\n            self.value = eth[TCP].win", "\n\n@dataclass\nclass TCPFlags(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L4, ComputationalRequirements.BASE_MATH_OP)\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 16\n\n    def extract(self, eth: Ethernet):\n        if eth[TCP]:\n            self.value = eth[TCP].flags", "\n\n@dataclass\nclass TCPLength(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L4, ComputationalRequirements.BASE_MATH_OP)\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 16\n\n    def extract(self, eth: Ethernet):\n        if eth[TCP]:\n            self.value = eth[IP].len - (eth[IP].hl << 2)", "\n\n@dataclass\nclass IPFlags(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L3, ComputationalRequirements.BASE_MATH_OP)\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 16\n\n    def extract(self, eth: Ethernet):\n        self.value = eth[IP].flags", "\n\n@dataclass\nclass IPLength(BaseFeature):\n    computational_requirements: ClassVar[Tuple[ComputationalRequirements]] = (\n        ComputationalRequirements.REQUIRED_L3, ComputationalRequirements.BASE_MATH_OP)\n    memory_requirements: ClassVar[int] = 2\n    limit: ClassVar[int] = 1 << 16\n\n    def extract(self, eth: Ethernet):\n        self.value = eth[IP].len", "\n\n@dataclass\nclass LucidFeaturesHolder(FeaturesHolder):\n    ALLOWED: ClassVar[Tuple[Type[BaseFeature]]] = (HighestLayer, Protocols, Time,\n                                                   IcmpType, UDPLength,\n                                                   TCPWindow, TCPFlags, TCPLength,\n                                                   IPFlags, IPLength)\n    value: OrderedDictType[Type[BaseFeature], float] = field(\n        default_factory=OrderedDict)\n\n    def __post_init__(self):\n        if not self.value:\n            self.value = OrderedDict.fromkeys(self.ALLOWED, None)\n        elif isinstance(self.value, (list, tuple)):\n            if isinstance(self.value[0], dict):\n                self.value = OrderedDict(\n                    [(getattr(sys.modules[self.__module__], k[\"key\"]), k[\"value\"]) for k in self.value])\n            else:\n                self.value = OrderedDict.fromkeys(self.value, None)\n\n    def pop_less_relevant(self, prune_zero_first=False, key_depth_class: Type[BaseKey] = None, **kwargs):\n        \"\"\"Method for popping the less relevant feature from the current ones.\n        The method looks iteratively at the following attributes, unless only 1\n        feature is remained and removed:\n        1. If prune zero first, then consider all those with relevance=0, else\n        take all current into account\n        2. Get all features with minimum importance\n        3. Get all features with max CPU instructions\n        4. Get all features with max memory\n\n        If still more than 1 feature remains, then pop the first one.\n        \"\"\"\n        def _internal_loop(rest, cond, is_backed):\n            tmpk = []\n            tmpv = sys.maxsize if cond == 0 else sys.maxsize*-1\n            for k in rest:\n                if cond == 0:\n                    if is_backed:\n                        v = self.value[k][1]\n                    elif isinstance(self.value[k], (list, tuple)):\n                        v = self.value[k][0]\n                    else:\n                        v = self.value[k]\n                elif cond == 1:\n                    if key_depth_class is None:\n                        v = ComputationalRequirements.requirements_to_cost(\n                            k.computational_requirements, ignore_depth=True)\n                    else:\n                        v = ComputationalRequirements.requirements_to_cost(\n                            k.computational_requirements, ignore_depth=key_depth_class)\n                elif cond == 2:\n                    v = k.memory_requirements\n                else:\n                    raise Exception()\n                if (cond == 0 and v < tmpv) or ((cond == 1 or cond == 2) and v > tmpv):\n                    tmpv = v\n                    tmpk = [k]\n                elif v == tmpv:\n                    tmpk.append(k)\n            return tmpk\n\n        # used only in case all features' relevances were 0, so a backup metric is used.\n        is_backed_up = sum(1 for v in self.value.values()\n                           if isinstance(v, (tuple, list))) == len(self.value)\n\n        # get all features with no importance (0)\n        if prune_zero_first:\n            mink = [k for k, v in self.value.items() if (is_backed_up and v[1] == 0) or\n                    (not is_backed_up and v == 0)] or list(self.value.keys())\n            if len(mink) == 1:\n                return self.value.pop(mink[0])\n        else:\n            mink = list(self.value.keys())\n        # get all features with minimum importance\n        mink = _internal_loop(mink, 0, is_backed_up)\n        if len(mink) == 1:\n            return self.value.pop(mink[0])\n        # get all features with max CPU\n        mink = _internal_loop(mink, 1, is_backed_up)\n        if len(mink) == 1:\n            return self.value.pop(mink[0])\n        # get all features with max memory\n        mink = _internal_loop(mink, 2, is_backed_up)\n        return self.value.pop(mink[0])", ""]}
