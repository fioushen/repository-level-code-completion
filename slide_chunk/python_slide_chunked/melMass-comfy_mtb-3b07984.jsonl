{"filename": "endpoint.py", "chunked_list": ["from .utils import here, run_command, comfy_mode\nfrom aiohttp import web\nfrom .log import mklog\nimport sys\n\nendlog = mklog(\"mtb endpoint\")\n\n# - ACTIONS\nimport requirements\n", "import requirements\n\n\n\ndef ACTIONS_installDependency(dependency_names=None):\n    if dependency_names is None:\n        return {\"error\": \"No dependency name provided\"}\n    endlog.debug(f\"Received Install Dependency request for {dependency_names}\")\n    reqs = []\n    if comfy_mode == \"embeded\":\n        reqs = list(requirements.parse((here / \"reqs_portable.txt\").read_text()))\n    else:\n        reqs = list(requirements.parse((here / \"reqs.txt\").read_text()))\n    print([x.specs for x in reqs])\n    print(\n        \"\\n\".join([f\"{x.line} {''.join(x.specs[0] if x.specs else '')}\" for x in reqs])\n    )\n    for dependency_name in dependency_names:\n        for req in reqs:\n            if req.name == dependency_name:\n                endlog.debug(f\"Dependency {dependency_name} installed\")\n                break\n    return {\"success\": True}", "\n\ndef ACTIONS_getStyles(style_name=None):\n    from .nodes.conditions import StylesLoader\n\n    styles = StylesLoader.options\n    match_list = [\"name\"]\n    if styles:\n        filtered_styles = {\n            key: value\n            for key, value in styles.items()\n            if not key.startswith(\"__\") and key not in match_list\n        }\n        if style_name:\n            return filtered_styles.get(style_name, {\"error\": \"Style not found\"})\n        return filtered_styles\n    return {\"error\": \"No styles found\"}", "\n\nasync def do_action(request) -> web.Response:\n    endlog.debug(\"Init action request\")\n    request_data = await request.json()\n    name = request_data.get(\"name\")\n    args = request_data.get(\"args\")\n\n    endlog.debug(f\"Received action request: {name} {args}\")\n", "    endlog.debug(f\"Received action request: {name} {args}\")\n\n    method_name = f\"ACTIONS_{name}\"\n    method = globals().get(method_name)\n\n    if callable(method):\n        result = method(args) if args else method()\n        endlog.debug(f\"Action result: {result}\")\n        return web.json_response({\"result\": result})\n", "\n    available_methods = [\n        attr[len(\"ACTIONS_\") :] for attr in globals() if attr.startswith(\"ACTIONS_\")\n    ]\n\n    return web.json_response(\n        {\"error\": \"Invalid method name.\", \"available_methods\": available_methods}\n    )\n\n", "\n\n# - HTML UTILS\n\n\ndef dependencies_button(name, dependencies):\n    deps = \",\".join([f\"'{x}'\" for x in dependencies])\n    return f\"\"\"\n        <button class=\"dependency-button\" onclick=\"window.mtb_action('installDependency',[{deps}])\">Install {name} deps</button>\n        \"\"\"", "\n\ndef render_table(table_dict, sort=True, title=None):\n    table_dict = sorted(\n        table_dict.items(), key=lambda item: item[0]\n    )  # Sort the dictionary by keys\n\n    table_rows = \"\"\n    for name, item in table_dict:\n        if isinstance(item, dict):\n            if \"dependencies\" in item:\n                table_rows += f\"<tr><td>{name}</td><td>\"\n                table_rows += f\"{dependencies_button(name,item['dependencies'])}\"\n\n                table_rows += \"</td></tr>\"\n            else:\n                table_rows += f\"<tr><td>{name}</td><td>{render_table(item)}</td></tr>\"\n        # elif isinstance(item, str):\n        #     table_rows += f\"<tr><td>{name}</td><td>{item}</td></tr>\"\n        else:\n            table_rows += f\"<tr><td>{name}</td><td>{item}</td></tr>\"\n\n    return f\"\"\"\n        <div class=\"table-container\">\n        {\"\" if title is None else f\"<h1>{title}</h1>\"}\n        <table>\n            <thead>\n                <tr>\n                    <th>Name</th>\n                    <th>Description</th>\n                </tr>\n            </thead>\n            <tbody>\n                {table_rows}\n            </tbody>\n        </table>      \n        </div>\n        \"\"\"", "\n\ndef render_base_template(title, content):\n    css_content = \"\"\n    css_path = here / \"html\" / \"style.css\"\n    if css_path:\n        with open(css_path, \"r\") as css_file:\n            css_content = css_file.read()\n\n    github_icon_svg = \"\"\"<svg xmlns=\"http://www.w3.org/2000/svg\" fill=\"whitesmoke\" height=\"3em\" viewBox=\"0 0 496 512\"><path d=\"M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z\"/></svg>\"\"\"\n    return f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>{title}</title>\n        <style>\n            {css_content}\n        </style>\n    </head>\n    <script type=\"module\">\n        import {{ api }} from '/scripts/api.js'\n        const mtb_action = async (action, args) =>{{\n            console.log(`Sending ${{action}} with args: ${{args}}`)\n            }}\n        window.mtb_action = async (action, args) =>{{\n            console.log(`Sending ${{action}} with args: ${{args}} to the API`)\n            const res = await api.fetchApi('/actions', {{\n                method: 'POST',\n                body: JSON.stringify({{\n                  name: action,\n                  args,\n                }}),\n            }})\n\n              const output = await res.json()\n              console.debug(`Received ${{action}} response:`, output)\n              if (output?.result?.error){{\n                  alert(`An error occured: {{output?.result?.error}}`)\n              }}\n              return output?.result\n        }}\n    </script>\n    <body>\n        <header>\n        <a href=\"/\">Back to Comfy</a>\n        <div class=\"mtb_logo\">\n            <img src=\"https://repository-images.githubusercontent.com/649047066/a3eef9a7-20dd-4ef9-b839-884502d4e873\" alt=\"Comfy MTB Logo\" height=\"70\" width=\"128\">\n            <span class=\"title\">Comfy MTB</span></div>\n            <a style=\"width:128px;text-align:center\" href=\"https://www.github.com/melmass/comfy_mtb\">\n                {github_icon_svg}\n            </a>\n        </header>\n\n        <main>\n            {content}\n        </main>\n\n        <footer>\n            <!-- Shared footer content here -->\n        </footer>\n    </body>\n    \n    </html>\n    \"\"\"", ""]}
{"filename": "log.py", "chunked_list": ["import logging\nimport re\nimport os\n\nbase_log_level = logging.DEBUG if os.environ.get(\"MTB_DEBUG\") else logging.INFO\n\n\n# Custom object that discards the output\nclass NullWriter:\n    def write(self, text):\n        pass", "class NullWriter:\n    def write(self, text):\n        pass\n\n\nclass Formatter(logging.Formatter):\n    grey = \"\\x1b[38;20m\"\n    cyan = \"\\x1b[36;20m\"\n    purple = \"\\x1b[35;20m\"\n    yellow = \"\\x1b[33;20m\"\n    red = \"\\x1b[31;20m\"\n    bold_red = \"\\x1b[31;1m\"\n    reset = \"\\x1b[0m\"\n    # format = \"%(asctime)s - [%(name)s] - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\"\n    format = \"[%(name)s] | %(levelname)s -> %(message)s\"\n\n    FORMATS = {\n        logging.DEBUG: purple + format + reset,\n        logging.INFO: cyan + format + reset,\n        logging.WARNING: yellow + format + reset,\n        logging.ERROR: red + format + reset,\n        logging.CRITICAL: bold_red + format + reset,\n    }\n\n    def format(self, record):\n        log_fmt = self.FORMATS.get(record.levelno)\n        formatter = logging.Formatter(log_fmt)\n        return formatter.format(record)", "\n\ndef mklog(name, level=base_log_level):\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    for handler in logger.handlers:\n        logger.removeHandler(handler)\n\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    ch.setFormatter(Formatter())\n    logger.addHandler(ch)\n\n    # Disable log propagation\n    logger.propagate = False\n\n    return logger", "\n\n# - The main app logger\nlog = mklog(__package__, base_log_level)\n\n\ndef log_user(arg):\n    print(\"\\033[34mComfy MTB Utils:\\033[0m {arg}\")\n\n\ndef get_summary(docstring):\n    return docstring.strip().split(\"\\n\\n\", 1)[0]", "\n\ndef get_summary(docstring):\n    return docstring.strip().split(\"\\n\\n\", 1)[0]\n\n\ndef blue_text(text):\n    return f\"\\033[94m{text}\\033[0m\"\n\n\ndef cyan_text(text):\n    return f\"\\033[96m{text}\\033[0m\"", "\n\ndef cyan_text(text):\n    return f\"\\033[96m{text}\\033[0m\"\n\n\ndef get_label(label):\n    words = re.findall(r\"(?:^|[A-Z])[a-z]*\", label)\n    return \" \".join(words).strip()\n", ""]}
{"filename": "__init__.py", "chunked_list": ["#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n###\n# File: __init__.py\n# Project: comfy_mtb\n# Author: Mel Massadian\n# Copyright (c) 2023 Mel Massadian\n#\n###\nimport os", "###\nimport os\n\n# todo: don't override this if the user has that setup already\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\nos.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n\nimport traceback\nfrom .log import log, blue_text, cyan_text, get_summary, get_label\nfrom .utils import here", "from .log import log, blue_text, cyan_text, get_summary, get_label\nfrom .utils import here\nfrom .utils import comfy_dir\nimport importlib\nimport os\nimport ast\nimport json\n\nNODE_CLASS_MAPPINGS = {}\nNODE_DISPLAY_NAME_MAPPINGS = {}", "NODE_CLASS_MAPPINGS = {}\nNODE_DISPLAY_NAME_MAPPINGS = {}\nNODE_CLASS_MAPPINGS_DEBUG = {}\n\n__version__ = \"0.1.4\"\n\n\ndef extract_nodes_from_source(filename):\n    source_code = \"\"\n\n    with open(filename, \"r\") as file:\n        source_code = file.read()\n\n    nodes = []\n\n    try:\n        parsed = ast.parse(source_code)\n        for node in ast.walk(parsed):\n            if isinstance(node, ast.Assign) and len(node.targets) == 1:\n                target = node.targets[0]\n                if isinstance(target, ast.Name) and target.id == \"__nodes__\":\n                    value = ast.get_source_segment(source_code, node.value)\n                    node_value = ast.parse(value).body[0].value\n                    if isinstance(node_value, (ast.List, ast.Tuple)):\n                        nodes.extend(\n                            element.id\n                            for element in node_value.elts\n                            if isinstance(element, ast.Name)\n                        )\n                    break\n    except SyntaxError:\n        log.error(\"Failed to parse\")\n    return nodes", "\n\ndef load_nodes():\n    errors = []\n    nodes = []\n    nodes_failed = []\n\n    for filename in (here / \"nodes\").iterdir():\n        if filename.suffix == \".py\":\n            module_name = filename.stem\n\n            try:\n                module = importlib.import_module(\n                    f\".nodes.{module_name}\", package=__package__\n                )\n                _nodes = getattr(module, \"__nodes__\")\n                nodes.extend(_nodes)\n                log.debug(f\"Imported {module_name} nodes\")\n\n            except AttributeError:\n                pass  # wip nodes\n            except Exception:\n                error_message = traceback.format_exc().splitlines()[-1]\n                errors.append(\n                    f\"Failed to import module {module_name} because {error_message}\"\n                )\n                # Read __nodes__ variable from the source file\n                nodes_failed.extend(extract_nodes_from_source(filename))\n\n    if errors:\n        log.info(\n            f\"Some nodes failed to load:\\n\\t\"\n            + \"\\n\\t\".join(errors)\n            + \"\\n\\n\"\n            + \"Check that you properly installed the dependencies.\\n\"\n            + \"If you think this is a bug, please report it on the github page (https://github.com/melMass/comfy_mtb/issues)\"\n        )\n\n    return (nodes, nodes_failed)", "\n\n# - REGISTER WEB EXTENSIONS\nweb_extensions_root = comfy_dir / \"web\" / \"extensions\"\nweb_mtb = web_extensions_root / \"mtb\"\n\nif web_mtb.exists():\n    log.debug(f\"Web extensions folder found at {web_mtb}\")\n    if not os.path.islink(web_mtb.as_posix()):\n        log.warn(\n            f\"Web extensions folder at {web_mtb} is not a symlink, if updating please delete it before\"\n        )\n\n\nelif web_extensions_root.exists():\n    web_tgt = here / \"web\"\n    src = web_tgt.as_posix()\n    dst = web_mtb.as_posix()\n    try:\n        if os.name == \"nt\":\n            import _winapi\n\n            _winapi.CreateJunction(src, dst)\n        else:\n            os.symlink(web_tgt.as_posix(), web_mtb.as_posix())\n\n    except OSError:\n        log.warn(f\"Failed to create symlink to {web_mtb}, trying to copy it\")\n        try:\n            import shutil\n\n            shutil.copytree(web_tgt, web_mtb)\n            log.info(f\"Successfully copied {web_tgt} to {web_mtb}\")\n        except Exception as e:\n            log.warn(\n                f\"Failed to symlink and copy {web_tgt} to {web_mtb}. Please copy the folder manually.\"\n            )\n            log.warn(e)\n\n    except Exception as e:\n        log.warn(\n            f\"Failed to create symlink to {web_mtb}. Please copy the folder manually.\"\n        )\n        log.warn(e)\nelse:\n    log.warn(\n        f\"Comfy root probably not found automatically, please copy the folder {web_mtb} manually in the web/extensions folder of ComfyUI\"\n    )", "\n# - REGISTER NODES\nnodes, failed = load_nodes()\nfor node_class in nodes:\n    class_name = node_class.__name__\n    node_label = f\"{get_label(class_name)} (mtb)\"\n    NODE_CLASS_MAPPINGS[node_label] = node_class\n    NODE_DISPLAY_NAME_MAPPINGS[class_name] = node_label\n    NODE_CLASS_MAPPINGS_DEBUG[node_label] = node_class.__doc__\n    # TODO: I removed this, I find it more convenient to write without spaces, but it breaks every of my workflows\n    # TODO (cont): and until I find a way to automate the conversion, I'll leave it like this\n\n    if os.environ.get(\"MTB_EXPORT\"):\n        with open(here / \"node_list.json\", \"w\") as f:\n            f.write(\n                json.dumps(\n                    {\n                        k: NODE_CLASS_MAPPINGS_DEBUG[k]\n                        for k in sorted(NODE_CLASS_MAPPINGS_DEBUG.keys())\n                    },\n                    indent=4,\n                )\n            )", "\nlog.info(\n    f\"Loaded the following nodes:\\n\\t\"\n    + \"\\n\\t\".join(\n        f\"{cyan_text(k)}: {blue_text(get_summary(doc)) if doc else '-'}\"\n        for k, doc in NODE_CLASS_MAPPINGS_DEBUG.items()\n    )\n)\n\n# - ENDPOINT", "\n# - ENDPOINT\nfrom server import PromptServer\nfrom .log import log\nfrom aiohttp import web\nfrom importlib import reload\nimport logging\nfrom .endpoint import endlog\n\nif hasattr(PromptServer, \"instance\"):\n    restore_deps = [\"basicsr\"]\n    swap_deps = [\"insightface\", \"onnxruntime\"]\n\n    node_dependency_mapping = {\n        \"FaceSwap\": swap_deps,\n        \"LoadFaceSwapModel\": swap_deps,\n        \"LoadFaceAnalysisModel\": restore_deps,\n    }\n\n    @PromptServer.instance.routes.get(\"/mtb/status\")\n    async def get_full_library(request):\n        from . import endpoint\n\n        reload(endpoint)\n\n        endlog.debug(\"Getting node registration status\")\n        # Check if the request prefers HTML content\n        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n            # # Return an HTML page\n            html_response = endpoint.render_table(\n                NODE_CLASS_MAPPINGS_DEBUG, title=\"Registered\"\n            )\n            html_response += endpoint.render_table(\n                {\n                    k: {\"dependencies\": node_dependency_mapping.get(k)}\n                    if node_dependency_mapping.get(k)\n                    else \"-\"\n                    for k in failed\n                },\n                title=\"Failed to load\",\n            )\n\n            return web.Response(\n                text=endpoint.render_base_template(\"MTB\", html_response),\n                content_type=\"text/html\",\n            )\n\n        return web.json_response(\n            {\n                \"registered\": NODE_CLASS_MAPPINGS_DEBUG,\n                \"failed\": failed,\n            }\n        )\n\n    @PromptServer.instance.routes.post(\"/mtb/debug\")\n    async def set_debug(request):\n        json_data = await request.json()\n        enabled = json_data.get(\"enabled\")\n        if enabled:\n            os.environ[\"MTB_DEBUG\"] = \"true\"\n            log.setLevel(logging.DEBUG)\n            log.debug(\"Debug mode set from API (/mtb/debug POST route)\")\n\n        elif \"MTB_DEBUG\" in os.environ:\n            # del os.environ[\"MTB_DEBUG\"]\n            os.environ.pop(\"MTB_DEBUG\")\n            log.setLevel(logging.INFO)\n\n        return web.json_response(\n            {\"message\": f\"Debug mode {'set' if enabled else 'unset'}\"}\n        )\n\n    @PromptServer.instance.routes.get(\"/mtb\")\n    async def get_home(request):\n        from . import endpoint\n\n        reload(endpoint)\n        # Check if the request prefers HTML content\n        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n            # # Return an HTML page\n            html_response = \"\"\"\n            <div class=\"flex-container menu\">\n                <a href=\"/mtb/debug\">debug</a>\n                <a href=\"/mtb/status\">status</a>\n            </div>            \n            \"\"\"\n            return web.Response(\n                text=endpoint.render_base_template(\"MTB\", html_response),\n                content_type=\"text/html\",\n            )\n\n        # Return JSON for other requests\n        return web.json_response({\"message\": \"Welcome to MTB!\"})\n\n    @PromptServer.instance.routes.get(\"/mtb/debug\")\n    async def get_debug(request):\n        from . import endpoint\n\n        reload(endpoint)\n        enabled = \"MTB_DEBUG\" in os.environ\n        # Check if the request prefers HTML content\n        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n            # # Return an HTML page\n            html_response = f\"\"\"\n                <h1>MTB Debug Status: {'Enabled' if enabled else 'Disabled'}</h1>\n            \"\"\"\n            return web.Response(\n                text=endpoint.render_base_template(\"Debug\", html_response),\n                content_type=\"text/html\",\n            )\n\n        # Return JSON for other requests\n        return web.json_response({\"enabled\": enabled})\n\n    @PromptServer.instance.routes.get(\"/mtb/actions\")\n    async def no_route(request):\n        from . import endpoint\n\n        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n            html_response = \"\"\"\n            <h1>Actions has no get for now...</h1>\n            \"\"\"\n            return web.Response(\n                text=endpoint.render_base_template(\"Actions\", html_response),\n                content_type=\"text/html\",\n            )\n        return web.json_response({\"message\": \"actions has no get for now\"})\n\n    @PromptServer.instance.routes.post(\"/mtb/actions\")\n    async def do_action(request):\n        from . import endpoint\n\n        reload(endpoint)\n\n        return await endpoint.do_action(request)", "\nif hasattr(PromptServer, \"instance\"):\n    restore_deps = [\"basicsr\"]\n    swap_deps = [\"insightface\", \"onnxruntime\"]\n\n    node_dependency_mapping = {\n        \"FaceSwap\": swap_deps,\n        \"LoadFaceSwapModel\": swap_deps,\n        \"LoadFaceAnalysisModel\": restore_deps,\n    }\n\n    @PromptServer.instance.routes.get(\"/mtb/status\")\n    async def get_full_library(request):\n        from . import endpoint\n\n        reload(endpoint)\n\n        endlog.debug(\"Getting node registration status\")\n        # Check if the request prefers HTML content\n        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n            # # Return an HTML page\n            html_response = endpoint.render_table(\n                NODE_CLASS_MAPPINGS_DEBUG, title=\"Registered\"\n            )\n            html_response += endpoint.render_table(\n                {\n                    k: {\"dependencies\": node_dependency_mapping.get(k)}\n                    if node_dependency_mapping.get(k)\n                    else \"-\"\n                    for k in failed\n                },\n                title=\"Failed to load\",\n            )\n\n            return web.Response(\n                text=endpoint.render_base_template(\"MTB\", html_response),\n                content_type=\"text/html\",\n            )\n\n        return web.json_response(\n            {\n                \"registered\": NODE_CLASS_MAPPINGS_DEBUG,\n                \"failed\": failed,\n            }\n        )\n\n    @PromptServer.instance.routes.post(\"/mtb/debug\")\n    async def set_debug(request):\n        json_data = await request.json()\n        enabled = json_data.get(\"enabled\")\n        if enabled:\n            os.environ[\"MTB_DEBUG\"] = \"true\"\n            log.setLevel(logging.DEBUG)\n            log.debug(\"Debug mode set from API (/mtb/debug POST route)\")\n\n        elif \"MTB_DEBUG\" in os.environ:\n            # del os.environ[\"MTB_DEBUG\"]\n            os.environ.pop(\"MTB_DEBUG\")\n            log.setLevel(logging.INFO)\n\n        return web.json_response(\n            {\"message\": f\"Debug mode {'set' if enabled else 'unset'}\"}\n        )\n\n    @PromptServer.instance.routes.get(\"/mtb\")\n    async def get_home(request):\n        from . import endpoint\n\n        reload(endpoint)\n        # Check if the request prefers HTML content\n        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n            # # Return an HTML page\n            html_response = \"\"\"\n            <div class=\"flex-container menu\">\n                <a href=\"/mtb/debug\">debug</a>\n                <a href=\"/mtb/status\">status</a>\n            </div>            \n            \"\"\"\n            return web.Response(\n                text=endpoint.render_base_template(\"MTB\", html_response),\n                content_type=\"text/html\",\n            )\n\n        # Return JSON for other requests\n        return web.json_response({\"message\": \"Welcome to MTB!\"})\n\n    @PromptServer.instance.routes.get(\"/mtb/debug\")\n    async def get_debug(request):\n        from . import endpoint\n\n        reload(endpoint)\n        enabled = \"MTB_DEBUG\" in os.environ\n        # Check if the request prefers HTML content\n        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n            # # Return an HTML page\n            html_response = f\"\"\"\n                <h1>MTB Debug Status: {'Enabled' if enabled else 'Disabled'}</h1>\n            \"\"\"\n            return web.Response(\n                text=endpoint.render_base_template(\"Debug\", html_response),\n                content_type=\"text/html\",\n            )\n\n        # Return JSON for other requests\n        return web.json_response({\"enabled\": enabled})\n\n    @PromptServer.instance.routes.get(\"/mtb/actions\")\n    async def no_route(request):\n        from . import endpoint\n\n        if \"text/html\" in request.headers.get(\"Accept\", \"\"):\n            html_response = \"\"\"\n            <h1>Actions has no get for now...</h1>\n            \"\"\"\n            return web.Response(\n                text=endpoint.render_base_template(\"Actions\", html_response),\n                content_type=\"text/html\",\n            )\n        return web.json_response({\"message\": \"actions has no get for now\"})\n\n    @PromptServer.instance.routes.post(\"/mtb/actions\")\n    async def do_action(request):\n        from . import endpoint\n\n        reload(endpoint)\n\n        return await endpoint.do_action(request)", "\n\n# - WAS Dictionary\nMANIFEST = {\n    \"name\": \"MTB Nodes\",  # The title that will be displayed on Node Class menu,. and Node Class view\n    \"version\": (0, 1, 0),  # Version of the custom_node or sub module\n    \"author\": \"Mel Massadian\",  # Author or organization of the custom_node or sub module\n    \"project\": \"https://github.com/melMass/comfy_mtb\",  # The address that the `name` value will link to on Node Class Views\n    \"description\": \"Set of nodes that enhance your animation workflow and provide a range of useful tools including features such as manipulating bounding boxes, perform color corrections, swap faces in images, interpolate frames for smooth animation, export to ProRes format, apply various image operations, work with latent spaces, generate QR codes, and create normal and height maps for textures.\",\n}", "    \"description\": \"Set of nodes that enhance your animation workflow and provide a range of useful tools including features such as manipulating bounding boxes, perform color corrections, swap faces in images, interpolate frames for smooth animation, export to ProRes format, apply various image operations, work with latent spaces, generate QR codes, and create normal and height maps for textures.\",\n}\n"]}
{"filename": "install.py", "chunked_list": ["import requests\nimport os\nimport ast\nimport argparse\nimport sys\nimport subprocess\nfrom importlib import import_module\nimport platform\nfrom pathlib import Path\nimport sys", "from pathlib import Path\nimport sys\nimport stat\nimport threading\nimport signal\nfrom contextlib import suppress\nfrom queue import Queue, Empty\nfrom contextlib import contextmanager\n\nhere = Path(__file__).parent", "\nhere = Path(__file__).parent\nexecutable = sys.executable\n\n# - detect mode\nmode = None\nif os.environ.get(\"COLAB_GPU\"):\n    mode = \"colab\"\nelif \"python_embeded\" in executable:\n    mode = \"embeded\"\nelif \".venv\" in executable:\n    mode = \"venv\"", "\n\nif mode is None:\n    mode = \"unknown\"\n\n# - Constants\nrepo_url = \"https://github.com/melmass/comfy_mtb.git\"\nrepo_owner = \"melmass\"\nrepo_name = \"comfy_mtb\"\nshort_platform = {", "repo_name = \"comfy_mtb\"\nshort_platform = {\n    \"windows\": \"win_amd64\",\n    \"linux\": \"linux_x86_64\",\n}\ncurrent_platform = platform.system().lower()\n\n# region ansi\n# ANSI escape sequences for text styling\nANSI_FORMATS = {", "# ANSI escape sequences for text styling\nANSI_FORMATS = {\n    \"reset\": \"\\033[0m\",\n    \"bold\": \"\\033[1m\",\n    \"dim\": \"\\033[2m\",\n    \"italic\": \"\\033[3m\",\n    \"underline\": \"\\033[4m\",\n    \"blink\": \"\\033[5m\",\n    \"reverse\": \"\\033[7m\",\n    \"strike\": \"\\033[9m\",", "    \"reverse\": \"\\033[7m\",\n    \"strike\": \"\\033[9m\",\n}\n\nANSI_COLORS = {\n    \"black\": \"\\033[30m\",\n    \"red\": \"\\033[31m\",\n    \"green\": \"\\033[32m\",\n    \"yellow\": \"\\033[33m\",\n    \"blue\": \"\\033[34m\",", "    \"yellow\": \"\\033[33m\",\n    \"blue\": \"\\033[34m\",\n    \"magenta\": \"\\033[35m\",\n    \"cyan\": \"\\033[36m\",\n    \"white\": \"\\033[37m\",\n    \"bright_black\": \"\\033[30;1m\",\n    \"bright_red\": \"\\033[31;1m\",\n    \"bright_green\": \"\\033[32;1m\",\n    \"bright_yellow\": \"\\033[33;1m\",\n    \"bright_blue\": \"\\033[34;1m\",", "    \"bright_yellow\": \"\\033[33;1m\",\n    \"bright_blue\": \"\\033[34;1m\",\n    \"bright_magenta\": \"\\033[35;1m\",\n    \"bright_cyan\": \"\\033[36;1m\",\n    \"bright_white\": \"\\033[37;1m\",\n    \"bg_black\": \"\\033[40m\",\n    \"bg_red\": \"\\033[41m\",\n    \"bg_green\": \"\\033[42m\",\n    \"bg_yellow\": \"\\033[43m\",\n    \"bg_blue\": \"\\033[44m\",", "    \"bg_yellow\": \"\\033[43m\",\n    \"bg_blue\": \"\\033[44m\",\n    \"bg_magenta\": \"\\033[45m\",\n    \"bg_cyan\": \"\\033[46m\",\n    \"bg_white\": \"\\033[47m\",\n    \"bg_bright_black\": \"\\033[40;1m\",\n    \"bg_bright_red\": \"\\033[41;1m\",\n    \"bg_bright_green\": \"\\033[42;1m\",\n    \"bg_bright_yellow\": \"\\033[43;1m\",\n    \"bg_bright_blue\": \"\\033[44;1m\",", "    \"bg_bright_yellow\": \"\\033[43;1m\",\n    \"bg_bright_blue\": \"\\033[44;1m\",\n    \"bg_bright_magenta\": \"\\033[45;1m\",\n    \"bg_bright_cyan\": \"\\033[46;1m\",\n    \"bg_bright_white\": \"\\033[47;1m\",\n}\n\n\ndef apply_format(text, *formats):\n    \"\"\"Apply ANSI escape sequences for the specified formats to the given text.\"\"\"\n    formatted_text = text\n    for format in formats:\n        formatted_text = f\"{ANSI_FORMATS.get(format, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n    return formatted_text", "def apply_format(text, *formats):\n    \"\"\"Apply ANSI escape sequences for the specified formats to the given text.\"\"\"\n    formatted_text = text\n    for format in formats:\n        formatted_text = f\"{ANSI_FORMATS.get(format, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n    return formatted_text\n\n\ndef apply_color(text, color=None, background=None):\n    \"\"\"Apply ANSI escape sequences for the specified color and background to the given text.\"\"\"\n    formatted_text = text\n    if color:\n        formatted_text = f\"{ANSI_COLORS.get(color, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n    if background:\n        formatted_text = f\"{ANSI_COLORS.get(background, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n    return formatted_text", "def apply_color(text, color=None, background=None):\n    \"\"\"Apply ANSI escape sequences for the specified color and background to the given text.\"\"\"\n    formatted_text = text\n    if color:\n        formatted_text = f\"{ANSI_COLORS.get(color, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n    if background:\n        formatted_text = f\"{ANSI_COLORS.get(background, '')}{formatted_text}{ANSI_FORMATS.get('reset', '')}\"\n    return formatted_text\n\n\ndef print_formatted(text, *formats, color=None, background=None, **kwargs):\n    \"\"\"Print the given text with the specified formats, color, and background.\"\"\"\n    formatted_text = apply_format(text, *formats)\n    formatted_text = apply_color(formatted_text, color, background)\n    file = kwargs.get(\"file\", sys.stdout)\n    header = \"[mtb install] \"\n\n    # Handle console encoding for Unicode characters (utf-8)\n    encoded_header = header.encode(sys.stdout.encoding, errors=\"replace\").decode(\n        sys.stdout.encoding\n    )\n    encoded_text = formatted_text.encode(sys.stdout.encoding, errors=\"replace\").decode(\n        sys.stdout.encoding\n    )\n\n    print(\n        \" \" * len(encoded_header)\n        if kwargs.get(\"no_header\")\n        else apply_color(apply_format(encoded_header, \"bold\"), color=\"yellow\"),\n        encoded_text,\n        file=file,\n    )", "\n\ndef print_formatted(text, *formats, color=None, background=None, **kwargs):\n    \"\"\"Print the given text with the specified formats, color, and background.\"\"\"\n    formatted_text = apply_format(text, *formats)\n    formatted_text = apply_color(formatted_text, color, background)\n    file = kwargs.get(\"file\", sys.stdout)\n    header = \"[mtb install] \"\n\n    # Handle console encoding for Unicode characters (utf-8)\n    encoded_header = header.encode(sys.stdout.encoding, errors=\"replace\").decode(\n        sys.stdout.encoding\n    )\n    encoded_text = formatted_text.encode(sys.stdout.encoding, errors=\"replace\").decode(\n        sys.stdout.encoding\n    )\n\n    print(\n        \" \" * len(encoded_header)\n        if kwargs.get(\"no_header\")\n        else apply_color(apply_format(encoded_header, \"bold\"), color=\"yellow\"),\n        encoded_text,\n        file=file,\n    )", "\n\n# endregion\n\n\n# region utils\ndef enqueue_output(out, queue):\n    for char in iter(lambda: out.read(1), b\"\"):\n        queue.put(char)\n    out.close()", "\n\ndef run_command(cmd, ignored_lines_start=None):\n    if ignored_lines_start is None:\n        ignored_lines_start = []\n\n    if isinstance(cmd, str):\n        shell_cmd = cmd\n    elif isinstance(cmd, list):\n        shell_cmd = \"\"\n        for arg in cmd:\n            if isinstance(arg, Path):\n                arg = arg.as_posix()\n            shell_cmd += f\"{arg} \"\n    else:\n        raise ValueError(\n            \"Invalid 'cmd' argument. It must be a string or a list of arguments.\"\n        )\n\n    process = subprocess.Popen(\n        shell_cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True,\n        shell=True,\n    )\n\n    # Create separate threads to read standard output and standard error streams\n    stdout_queue = Queue()\n    stderr_queue = Queue()\n    stdout_thread = threading.Thread(\n        target=enqueue_output, args=(process.stdout, stdout_queue)\n    )\n    stderr_thread = threading.Thread(\n        target=enqueue_output, args=(process.stderr, stderr_queue)\n    )\n    stdout_thread.daemon = True\n    stderr_thread.daemon = True\n    stdout_thread.start()\n    stderr_thread.start()\n\n    interrupted = False\n\n    def signal_handler(signum, frame):\n        nonlocal interrupted\n        interrupted = True\n        print(\"Command execution interrupted.\")\n\n    # Register the signal handler for keyboard interrupts (SIGINT)\n    signal.signal(signal.SIGINT, signal_handler)\n\n    stdout_buffer = \"\"\n    stderr_buffer = \"\"\n\n    # Process output from both streams until the process completes or interrupted\n    while not interrupted and (\n        process.poll() is None or not stdout_queue.empty() or not stderr_queue.empty()\n    ):\n        with suppress(Empty):\n            stdout_char = stdout_queue.get_nowait()\n            stdout_buffer += stdout_char\n            if stdout_char == \"\\n\":\n                if not any(\n                    stdout_buffer.startswith(ign) for ign in ignored_lines_start\n                ):\n                    print(stdout_buffer.strip())\n                stdout_buffer = \"\"\n        with suppress(Empty):\n            stderr_char = stderr_queue.get_nowait()\n            stderr_buffer += stderr_char\n            if stderr_char == \"\\n\":\n                print(stderr_buffer.strip())\n                stderr_buffer = \"\"\n\n    # Print any remaining content in buffers\n    if stdout_buffer and not any(\n        stdout_buffer.startswith(ign) for ign in ignored_lines_start\n    ):\n        print(stdout_buffer.strip())\n    if stderr_buffer:\n        print(stderr_buffer.strip())\n\n    return_code = process.returncode\n\n    if return_code == 0 and not interrupted:\n        print(\"Command executed successfully!\")\n    else:\n        if not interrupted:\n            print(f\"Command failed with return code: {return_code}\")", "\n\n# endregion\n\ntry:\n    import requirements\nexcept ImportError:\n    print_formatted(\"Installing requirements-parser...\", \"italic\", color=\"yellow\")\n    run_command([sys.executable, \"-m\", \"pip\", \"install\", \"requirements-parser\"])\n    import requirements\n\n    print_formatted(\"Done.\", \"italic\", color=\"green\")", "\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    print_formatted(\"Installing tqdm...\", \"italic\", color=\"yellow\")\n    run_command([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"tqdm\"])\n    from tqdm import tqdm\n\npip_map = {\n    \"onnxruntime-gpu\": \"onnxruntime\",", "pip_map = {\n    \"onnxruntime-gpu\": \"onnxruntime\",\n    \"opencv-contrib\": \"cv2\",\n    \"tb-nightly\": \"tensorboard\",\n    \"protobuf\": \"google.protobuf\",\n    # Add more mappings as needed\n}\n\n\ndef is_pipe():\n    if not sys.stdin.isatty():\n        return False\n    if sys.platform == \"win32\":\n        try:\n            import msvcrt\n\n            return msvcrt.get_osfhandle(0) != -1\n        except ImportError:\n            return False\n    else:\n        try:\n            mode = os.fstat(0).st_mode\n            return (\n                stat.S_ISFIFO(mode)\n                or stat.S_ISREG(mode)\n                or stat.S_ISBLK(mode)\n                or stat.S_ISSOCK(mode)\n            )\n        except OSError:\n            return False", "\ndef is_pipe():\n    if not sys.stdin.isatty():\n        return False\n    if sys.platform == \"win32\":\n        try:\n            import msvcrt\n\n            return msvcrt.get_osfhandle(0) != -1\n        except ImportError:\n            return False\n    else:\n        try:\n            mode = os.fstat(0).st_mode\n            return (\n                stat.S_ISFIFO(mode)\n                or stat.S_ISREG(mode)\n                or stat.S_ISBLK(mode)\n                or stat.S_ISSOCK(mode)\n            )\n        except OSError:\n            return False", "\n\n@contextmanager\ndef suppress_std():\n    with open(os.devnull, \"w\") as devnull:\n        old_stdout = sys.stdout\n        old_stderr = sys.stderr\n        sys.stdout = devnull\n        sys.stderr = devnull\n\n        try:\n            yield\n        finally:\n            sys.stdout = old_stdout\n            sys.stderr = old_stderr", "\n\n# Get the version from __init__.py\ndef get_local_version():\n    init_file = os.path.join(os.path.dirname(__file__), \"__init__.py\")\n    if os.path.isfile(init_file):\n        with open(init_file, \"r\") as f:\n            tree = ast.parse(f.read())\n            for node in ast.walk(tree):\n                if isinstance(node, ast.Assign):\n                    for target in node.targets:\n                        if (\n                            isinstance(target, ast.Name)\n                            and target.id == \"__version__\"\n                            and isinstance(node.value, ast.Str)\n                        ):\n                            return node.value.s\n    return None", "\n\ndef download_file(url, file_name):\n    with requests.get(url, stream=True) as response:\n        response.raise_for_status()\n        total_size = int(response.headers.get(\"content-length\", 0))\n        with open(file_name, \"wb\") as file, tqdm(\n            desc=file_name.stem,\n            total=total_size,\n            unit=\"B\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as progress_bar:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n                progress_bar.update(len(chunk))", "\n\ndef get_requirements(path: Path):\n    with open(path.resolve(), \"r\") as requirements_file:\n        requirements_txt = requirements_file.read()\n\n    try:\n        parsed_requirements = requirements.parse(requirements_txt)\n    except AttributeError:\n        print_formatted(\n            f\"Failed to parse {path}. Please make sure the file is correctly formatted.\",\n            \"bold\",\n            color=\"red\",\n        )\n\n        return\n\n    return parsed_requirements", "\n\ndef try_import(requirement):\n    dependency = requirement.name.strip()\n    import_name = pip_map.get(dependency, dependency)\n    installed = False\n\n    pip_name = dependency\n    pip_spec = \"\".join(specs[0]) if (specs := requirement.specs) else \"\"\n    try:\n        with suppress_std():\n            import_module(import_name)\n        print_formatted(\n            f\"\\t\u2705 Package {pip_name} already installed (import name: '{import_name}').\",\n            \"bold\",\n            color=\"green\",\n            no_header=True,\n        )\n        installed = True\n    except ImportError:\n        print_formatted(\n            f\"\\t\u26d4 Package {pip_name} is missing (import name: '{import_name}').\",\n            \"bold\",\n            color=\"red\",\n            no_header=True,\n        )\n\n    return (installed, pip_name, pip_spec, import_name)", "\n\ndef import_or_install(requirement, dry=False):\n    installed, pip_name, pip_spec, import_name = try_import(requirement)\n\n    pip_install_name = pip_name + pip_spec\n\n    if not installed:\n        print_formatted(f\"Installing package {pip_name}...\", \"italic\", color=\"yellow\")\n        if dry:\n            print_formatted(\n                f\"Dry-run: Package {pip_install_name} would be installed (import name: '{import_name}').\",\n                color=\"yellow\",\n            )\n        else:\n            try:\n                run_command([sys.executable, \"-m\", \"pip\", \"install\", pip_install_name])\n                print_formatted(\n                    f\"Package {pip_install_name} installed successfully using pip package name  (import name: '{import_name}')\",\n                    \"bold\",\n                    color=\"green\",\n                )\n            except subprocess.CalledProcessError as e:\n                print_formatted(\n                    f\"Failed to install package {pip_install_name} using pip package name  (import name: '{import_name}'). Error: {str(e)}\",\n                    \"bold\",\n                    color=\"red\",\n                )", "\n\ndef get_github_assets(tag=None):\n    if tag:\n        tag_url = (\n            f\"https://api.github.com/repos/{repo_owner}/{repo_name}/releases/tags/{tag}\"\n        )\n    else:\n        tag_url = (\n            f\"https://api.github.com/repos/{repo_owner}/{repo_name}/releases/latest\"\n        )\n    response = requests.get(tag_url)\n    if response.status_code == 404:\n        # print_formatted(\n        #     f\"Tag version '{apply_color(version,'cyan')}' not found for {owner}/{repo} repository.\"\n        # )\n        print_formatted(\"Error retrieving the release assets.\", color=\"red\")\n        sys.exit()\n\n    tag_data = response.json()\n    tag_name = tag_data[\"name\"]\n\n    return tag_data, tag_name", "\n\n# Install dependencies from requirements.txt\ndef install_dependencies(dry=False):\n    parsed_requirements = get_requirements(here / \"reqs.txt\")\n    if not parsed_requirements:\n        return\n    print_formatted(\n        \"Installing dependencies from reqs.txt...\", \"italic\", color=\"yellow\"\n    )\n\n    for requirement in parsed_requirements:\n        import_or_install(requirement, dry=dry)", "\n\nif __name__ == \"__main__\":\n    full = False\n    if len(sys.argv) == 1:\n        print_formatted(\n            \"No arguments provided, doing a full install/update...\",\n            \"italic\",\n            color=\"yellow\",\n        )\n\n        full = True\n\n    # Parse command-line arguments\n    parser = argparse.ArgumentParser(description=\"Comfy_mtb install script\")\n    parser.add_argument(\n        \"--path\",\n        \"-p\",\n        type=str,\n        help=\"Path to clone the repository to (i.e the absolute path to ComfyUI/custom_nodes)\",\n    )\n    parser.add_argument(\n        \"--wheels\", \"-w\", action=\"store_true\", help=\"Install wheel dependencies\"\n    )\n    parser.add_argument(\n        \"--requirements\", \"-r\", action=\"store_true\", help=\"Install requirements.txt\"\n    )\n    parser.add_argument(\n        \"--dry\",\n        action=\"store_true\",\n        help=\"Print what will happen without doing it (still making requests to the GH Api)\",\n    )\n\n    # - keep\n    # parser.add_argument(\n    #     \"--version\",\n    #     default=get_local_version(),\n    #     help=\"Version to check against the GitHub API\",\n    # )\n    print_formatted(\"mtb install\", \"bold\", color=\"yellow\")\n\n    args = parser.parse_args()\n\n    # wheels_directory = here / \"wheels\"\n    print_formatted(f\"Detected environment: {apply_color(mode,'cyan')}\")\n\n    if args.path:\n        clone_dir = Path(args.path)\n        if not clone_dir.exists():\n            print_formatted(\n                \"The path provided does not exist on disk... It must be pointing to ComfyUI's custom_nodes directory\"\n            )\n            sys.exit()\n\n        else:\n            repo_dir = clone_dir / repo_name\n            if not repo_dir.exists():\n                print_formatted(f\"Cloning to {repo_dir}...\", \"italic\", color=\"yellow\")\n                run_command([\"git\", \"clone\", \"--recursive\", repo_url, repo_dir])\n            else:\n                print_formatted(\n                    f\"Directory {repo_dir} already exists, we will update it...\"\n                )\n                run_command([\"git\", \"pull\", \"-C\", repo_dir])\n        # os.chdir(clone_dir)\n        here = clone_dir\n        full = True\n\n    # Install dependencies from requirements.txt\n    # if args.requirements or mode == \"venv\":\n\n    # if (not args.wheels and mode not in [\"colab\", \"embeded\"]) and not full:\n    #     print_formatted(\n    #         \"Skipping wheel installation. Use --wheels to install wheel dependencies. (only needed for Comfy embed)\",\n    #         \"italic\",\n    #         color=\"yellow\",\n    #     )\n\n    #     install_dependencies(dry=args.dry)\n    #     sys.exit()\n\n    # if mode in [\"colab\", \"embeded\"]:\n    #     print_formatted(\n    #         f\"Downloading and installing release wheels since we are in a Comfy {apply_color(mode,'cyan')} environment\",\n    #         \"italic\",\n    #         color=\"yellow\",\n    #     )\n    # if full:\n    #     print_formatted(\n    #         f\"Downloading and installing release wheels since no arguments where provided\",\n    #         \"italic\",\n    #         color=\"yellow\",\n    #     )\n\n    print_formatted(\"Checking environment...\", \"italic\", color=\"yellow\")\n    missing_deps = []\n    if parsed_requirements := get_requirements(here / \"reqs.txt\"):\n        for requirement in parsed_requirements:\n            installed, pip_name, pip_spec, import_name = try_import(requirement)\n            if not installed:\n                missing_deps.append(pip_name.split(\"-\")[0])\n\n    if not missing_deps:\n        print_formatted(\n            \"All requirements are already installed. Enjoy \ud83d\ude80\",\n            \"italic\",\n            color=\"green\",\n        )\n        sys.exit()\n\n    # # - Get the tag version from the GitHub API\n    # tag_data, tag_name = get_github_assets(tag=None)\n\n    # # - keep\n    # version = args.version\n    # # Compare the local and tag versions\n    # if version and tag_name:\n    #     if re.match(r\"v?(\\d+(\\.\\d+)+)\", version) and re.match(\n    #         r\"v?(\\d+(\\.\\d+)+)\", tag_name\n    #     ):\n    #         version_parts = [int(part) for part in version.lstrip(\"v\").split(\".\")]\n    #         tag_version_parts = [int(part) for part in tag_name.lstrip(\"v\").split(\".\")]\n\n    #         if version_parts > tag_version_parts:\n    #             print_formatted(\n    #                 f\"Local version ({version}) is greater than the release version ({tag_name}).\",\n    #                 \"bold\",\n    #                 \"yellow\",\n    #             )\n    #             sys.exit()\n\n    # matching_assets = [\n    #     asset\n    #     for asset in tag_data[\"assets\"]\n    #     if asset[\"name\"].endswith(\".whl\")\n    #     and (\n    #         \"any\" in asset[\"name\"] or short_platform[current_platform] in asset[\"name\"]\n    #     )\n    # ]\n    # if not matching_assets:\n    #     print_formatted(\n    #         f\"Unsupported operating system: {current_platform}\", color=\"yellow\"\n    #     )\n    # wheel_order_asset = next(\n    #     (asset for asset in tag_data[\"assets\"] if asset[\"name\"] == \"wheel_order.txt\"),\n    #     None,\n    # )\n    # if wheel_order_asset is not None:\n    #     print_formatted(\n    #         \"\u2699\ufe0f Sorting the release wheels using wheels order\", \"italic\", color=\"yellow\"\n    #     )\n    #     response = requests.get(wheel_order_asset[\"browser_download_url\"])\n    #     if response.status_code == 200:\n    #         wheel_order = [line.strip() for line in response.text.splitlines()]\n\n    #         def get_order_index(val):\n    #             try:\n    #                 return wheel_order.index(val)\n    #             except ValueError:\n    #                 return len(wheel_order)\n\n    #         matching_assets = sorted(\n    #             matching_assets,\n    #             key=lambda x: get_order_index(x[\"name\"].split(\"-\")[0]),\n    #         )\n    #     else:\n    #         print(\"Failed to fetch wheel_order.txt. Status code:\", response.status_code)\n\n    # missing_deps_urls = []\n    # for whl_file in matching_assets:\n    #     # check if installed\n    #     missing_deps_urls.append(whl_file[\"browser_download_url\"])\n\n    install_cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n\n    # - Install all deps\n    if not args.dry:\n        if platform.system() == \"Windows\":\n            wheel_cmd = install_cmd + [\"-r\", (here / \"reqs_windows.txt\")]\n        else:\n            wheel_cmd = install_cmd + [\"-r\", (here / \"reqs.txt\")]\n\n        run_command(wheel_cmd)\n        print_formatted(\n            \"\u2705 Successfully installed all dependencies.\", \"italic\", color=\"green\"\n        )\n    else:\n        print_formatted(\n            f\"Would have run the following command:\\n\\t{apply_color(' '.join(install_cmd),'cyan')}\",\n            \"italic\",\n            color=\"yellow\",\n        )", ""]}
{"filename": "utils.py", "chunked_list": ["from PIL import Image\nimport numpy as np\nimport torch\nfrom pathlib import Path\nimport sys\nfrom typing import List\nimport signal\nfrom contextlib import suppress\nfrom queue import Queue, Empty\nimport subprocess", "from queue import Queue, Empty\nimport subprocess\nimport threading\nimport os\nimport math\nimport functools\nimport socket\nimport requests\n\ntry:\n    from .log import log\nexcept ImportError:\n    try:\n        from log import log\n\n        log.warn(\"Imported log without relative path\")\n    except ImportError:\n        import logging\n\n        log = logging.getLogger(\"comfy mtb utils\")\n        log.warn(\"[comfy mtb] You probably called the file outside a module.\")", "\ntry:\n    from .log import log\nexcept ImportError:\n    try:\n        from log import log\n\n        log.warn(\"Imported log without relative path\")\n    except ImportError:\n        import logging\n\n        log = logging.getLogger(\"comfy mtb utils\")\n        log.warn(\"[comfy mtb] You probably called the file outside a module.\")", "\n\nclass IPChecker:\n    def __init__(self):\n        self.ips = list(self.get_local_ips())\n        log.debug(f\"Found {len(self.ips)} local ips\")\n        self.checked_ips = set()\n\n    def get_working_ip(self, test_url_template):\n        for ip in self.ips:\n            if ip not in self.checked_ips:\n                self.checked_ips.add(ip)\n                test_url = test_url_template.format(ip)\n                if self._test_url(test_url):\n                    return ip\n        return None\n\n    @staticmethod\n    def get_local_ips(prefix=\"192.168.\"):\n        hostname = socket.gethostname()\n        log.debug(f\"Getting local ips for {hostname}\")\n        for info in socket.getaddrinfo(hostname, None):\n            # Filter out IPv6 addresses if you only want IPv4\n            log.debug(info)\n            # if info[1] == socket.SOCK_STREAM and\n            if info[0] == socket.AF_INET and info[4][0].startswith(prefix):\n                yield info[4][0]\n\n    def _test_url(self, url):\n        try:\n            response = requests.get(url)\n            return response.status_code == 200\n        except Exception:\n            return False", "\n\n# region MISC Utilities\n@functools.lru_cache(maxsize=1)\ndef get_server_info():\n    from comfy.cli_args import args\n\n    ip_checker = IPChecker()\n    base_url = args.listen\n    if base_url == \"0.0.0.0\":\n        log.debug(\"Server set to 0.0.0.0, we will try to resolve the host IP\")\n        base_url = ip_checker.get_working_ip(f\"http://{{}}:{args.port}/history\")\n        log.debug(f\"Setting ip to {base_url}\")\n    return (base_url, args.port)", "\n\ndef hex_to_rgb(hex_color):\n    try:\n        hex_color = hex_color.lstrip(\"#\")\n        return tuple(int(hex_color[i : i + 2], 16) for i in (0, 2, 4))\n    except ValueError:\n        log.error(f\"Invalid hex color: {hex_color}\")\n        return (0, 0, 0)\n", "\n\ndef add_path(path, prepend=False):\n    if isinstance(path, list):\n        for p in path:\n            add_path(p, prepend)\n        return\n\n    if isinstance(path, Path):\n        path = path.resolve().as_posix()\n\n    if path not in sys.path:\n        if prepend:\n            sys.path.insert(0, path)\n        else:\n            sys.path.append(path)", "\n\ndef enqueue_output(out, queue):\n    for line in iter(out.readline, b\"\"):\n        queue.put(line)\n    out.close()\n\n\ndef run_command(cmd):\n    if isinstance(cmd, str):\n        shell_cmd = cmd\n    elif isinstance(cmd, list):\n        shell_cmd = \"\"\n        for arg in cmd:\n            if isinstance(arg, Path):\n                arg = arg.as_posix()\n            shell_cmd += f\"{arg} \"\n    else:\n        raise ValueError(\n            \"Invalid 'cmd' argument. It must be a string or a list of arguments.\"\n        )\n\n    process = subprocess.Popen(\n        shell_cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True,\n        shell=True,\n    )\n\n    # Create separate threads to read standard output and standard error streams\n    stdout_queue = Queue()\n    stderr_queue = Queue()\n    stdout_thread = threading.Thread(\n        target=enqueue_output, args=(process.stdout, stdout_queue)\n    )\n    stderr_thread = threading.Thread(\n        target=enqueue_output, args=(process.stderr, stderr_queue)\n    )\n    stdout_thread.daemon = True\n    stderr_thread.daemon = True\n    stdout_thread.start()\n    stderr_thread.start()\n\n    interrupted = False\n\n    def signal_handler(signum, frame):\n        nonlocal interrupted\n        interrupted = True\n        print(\"Command execution interrupted.\")\n\n    # Register the signal handler for keyboard interrupts (SIGINT)\n    signal.signal(signal.SIGINT, signal_handler)\n\n    # Process output from both streams until the process completes or interrupted\n    while not interrupted and (\n        process.poll() is None or not stdout_queue.empty() or not stderr_queue.empty()\n    ):\n        with suppress(Empty):\n            stdout_line = stdout_queue.get_nowait()\n            if stdout_line.strip() != \"\":\n                print(stdout_line.strip())\n        with suppress(Empty):\n            stderr_line = stderr_queue.get_nowait()\n            if stderr_line.strip() != \"\":\n                print(stderr_line.strip())\n    return_code = process.returncode\n\n    if return_code == 0 and not interrupted:\n        print(\"Command executed successfully!\")\n    else:\n        if not interrupted:\n            print(f\"Command failed with return code: {return_code}\")", "def run_command(cmd):\n    if isinstance(cmd, str):\n        shell_cmd = cmd\n    elif isinstance(cmd, list):\n        shell_cmd = \"\"\n        for arg in cmd:\n            if isinstance(arg, Path):\n                arg = arg.as_posix()\n            shell_cmd += f\"{arg} \"\n    else:\n        raise ValueError(\n            \"Invalid 'cmd' argument. It must be a string or a list of arguments.\"\n        )\n\n    process = subprocess.Popen(\n        shell_cmd,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        universal_newlines=True,\n        shell=True,\n    )\n\n    # Create separate threads to read standard output and standard error streams\n    stdout_queue = Queue()\n    stderr_queue = Queue()\n    stdout_thread = threading.Thread(\n        target=enqueue_output, args=(process.stdout, stdout_queue)\n    )\n    stderr_thread = threading.Thread(\n        target=enqueue_output, args=(process.stderr, stderr_queue)\n    )\n    stdout_thread.daemon = True\n    stderr_thread.daemon = True\n    stdout_thread.start()\n    stderr_thread.start()\n\n    interrupted = False\n\n    def signal_handler(signum, frame):\n        nonlocal interrupted\n        interrupted = True\n        print(\"Command execution interrupted.\")\n\n    # Register the signal handler for keyboard interrupts (SIGINT)\n    signal.signal(signal.SIGINT, signal_handler)\n\n    # Process output from both streams until the process completes or interrupted\n    while not interrupted and (\n        process.poll() is None or not stdout_queue.empty() or not stderr_queue.empty()\n    ):\n        with suppress(Empty):\n            stdout_line = stdout_queue.get_nowait()\n            if stdout_line.strip() != \"\":\n                print(stdout_line.strip())\n        with suppress(Empty):\n            stderr_line = stderr_queue.get_nowait()\n            if stderr_line.strip() != \"\":\n                print(stderr_line.strip())\n    return_code = process.returncode\n\n    if return_code == 0 and not interrupted:\n        print(\"Command executed successfully!\")\n    else:\n        if not interrupted:\n            print(f\"Command failed with return code: {return_code}\")", "\n\n# todo use the requirements library\nreqs_map = {\n    \"onnxruntime\": \"onnxruntime-gpu==1.15.1\",\n    \"basicsr\": \"basicsr==1.4.2\",\n    \"rembg\": \"rembg==2.0.50\",\n    \"qrcode\": \"qrcode[pil]\",\n}\n", "}\n\n\ndef import_install(package_name):\n    from pip._internal import main as pip_main\n\n    try:\n        __import__(package_name)\n    except ImportError:\n        package_spec = reqs_map.get(package_name)\n        if package_spec is None:\n            print(f\"Installing {package_name}\")\n            package_spec = package_name\n\n        pip_main([\"install\", package_spec])\n        __import__(package_name)", "\n\n# endregion\n\n\n# region GLOBAL VARIABLES\n# - detect mode\ncomfy_mode = None\nif os.environ.get(\"COLAB_GPU\"):\n    comfy_mode = \"colab\"\nelif \"python_embeded\" in sys.executable:\n    comfy_mode = \"embeded\"\nelif \".venv\" in sys.executable:\n    comfy_mode = \"venv\"", "if os.environ.get(\"COLAB_GPU\"):\n    comfy_mode = \"colab\"\nelif \"python_embeded\" in sys.executable:\n    comfy_mode = \"embeded\"\nelif \".venv\" in sys.executable:\n    comfy_mode = \"venv\"\n\n# - Get the absolute path of the parent directory of the current script\nhere = Path(__file__).parent.resolve()\n", "here = Path(__file__).parent.resolve()\n\n# - Construct the absolute path to the ComfyUI directory\ncomfy_dir = here.parent.parent\n\n# - Construct the path to the font file\nfont_path = here / \"font.ttf\"\n\n# - Add extern folder to path\nextern_root = here / \"extern\"", "# - Add extern folder to path\nextern_root = here / \"extern\"\nadd_path(extern_root)\nfor pth in extern_root.iterdir():\n    if pth.is_dir():\n        add_path(pth)\n\n# - Add the ComfyUI directory and custom nodes path to the sys.path list\nadd_path(comfy_dir)\nadd_path((comfy_dir / \"custom_nodes\"))", "add_path(comfy_dir)\nadd_path((comfy_dir / \"custom_nodes\"))\n\nPIL_FILTER_MAP = {\n    \"nearest\": Image.Resampling.NEAREST,\n    \"box\": Image.Resampling.BOX,\n    \"bilinear\": Image.Resampling.BILINEAR,\n    \"hamming\": Image.Resampling.HAMMING,\n    \"bicubic\": Image.Resampling.BICUBIC,\n    \"lanczos\": Image.Resampling.LANCZOS,", "    \"bicubic\": Image.Resampling.BICUBIC,\n    \"lanczos\": Image.Resampling.LANCZOS,\n}\n# endregion\n\n\n# region TENSOR UTILITIES\ndef tensor2pil(image: torch.Tensor) -> List[Image.Image]:\n    batch_count = image.size(0) if len(image.shape) > 3 else 1\n    if batch_count > 1:\n        out = []\n        for i in range(batch_count):\n            out.extend(tensor2pil(image[i]))\n        return out\n\n    return [\n        Image.fromarray(\n            np.clip(255.0 * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8)\n        )\n    ]", "\n\ndef pil2tensor(image: Image.Image | List[Image.Image]) -> torch.Tensor:\n    if isinstance(image, list):\n        return torch.cat([pil2tensor(img) for img in image], dim=0)\n\n    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)\n\n\ndef np2tensor(img_np: np.ndarray | List[np.ndarray]) -> torch.Tensor:\n    if isinstance(img_np, list):\n        return torch.cat([np2tensor(img) for img in img_np], dim=0)\n\n    return torch.from_numpy(img_np.astype(np.float32) / 255.0).unsqueeze(0)", "\ndef np2tensor(img_np: np.ndarray | List[np.ndarray]) -> torch.Tensor:\n    if isinstance(img_np, list):\n        return torch.cat([np2tensor(img) for img in img_np], dim=0)\n\n    return torch.from_numpy(img_np.astype(np.float32) / 255.0).unsqueeze(0)\n\n\ndef tensor2np(tensor: torch.Tensor) -> List[np.ndarray]:\n    batch_count = tensor.size(0) if len(tensor.shape) > 3 else 1\n    if batch_count > 1:\n        out = []\n        for i in range(batch_count):\n            out.extend(tensor2np(tensor[i]))\n        return out\n\n    return [np.clip(255.0 * tensor.cpu().numpy().squeeze(), 0, 255).astype(np.uint8)]", "def tensor2np(tensor: torch.Tensor) -> List[np.ndarray]:\n    batch_count = tensor.size(0) if len(tensor.shape) > 3 else 1\n    if batch_count > 1:\n        out = []\n        for i in range(batch_count):\n            out.extend(tensor2np(tensor[i]))\n        return out\n\n    return [np.clip(255.0 * tensor.cpu().numpy().squeeze(), 0, 255).astype(np.uint8)]\n", "\n\n# endregion\n\n\n# region MODEL Utilities\ndef download_antelopev2():\n    antelopev2_url = \"https://drive.google.com/uc?id=18wEUfMNohBJ4K3Ly5wpTejPfDzp-8fI8\"\n\n    try:\n        import gdown\n\n        import folder_paths\n\n        log.debug(\"Loading antelopev2 model\")\n\n        dest = Path(folder_paths.models_dir) / \"insightface\"\n        archive = dest / \"antelopev2.zip\"\n        final_path = dest / \"models\" / \"antelopev2\"\n        if not final_path.exists():\n            log.info(f\"antelopev2 not found, downloading to {dest}\")\n            gdown.download(\n                antelopev2_url,\n                archive.as_posix(),\n                resume=True,\n            )\n\n            log.info(f\"Unzipping antelopev2 to {final_path}\")\n\n            if archive.exists():\n                # we unzip it\n                import zipfile\n\n                with zipfile.ZipFile(archive.as_posix(), \"r\") as zip_ref:\n                    zip_ref.extractall(final_path.parent.as_posix())\n\n    except Exception as e:\n        log.error(\n            f\"Could not load or download antelopev2 model, download it manually from {antelopev2_url}\"\n        )\n        raise e", "\n\n# endregion\n\n\n# region UV Utilities\n\n\ndef create_uv_map_tensor(width=512, height=512):\n    u = torch.linspace(0.0, 1.0, steps=width)\n    v = torch.linspace(0.0, 1.0, steps=height)\n\n    U, V = torch.meshgrid(u, v)\n\n    uv_map = torch.zeros(height, width, 3, dtype=torch.float32)\n    uv_map[:, :, 0] = U.t()\n    uv_map[:, :, 1] = V.t()\n\n    return uv_map.unsqueeze(0)", "def create_uv_map_tensor(width=512, height=512):\n    u = torch.linspace(0.0, 1.0, steps=width)\n    v = torch.linspace(0.0, 1.0, steps=height)\n\n    U, V = torch.meshgrid(u, v)\n\n    uv_map = torch.zeros(height, width, 3, dtype=torch.float32)\n    uv_map[:, :, 0] = U.t()\n    uv_map[:, :, 1] = V.t()\n\n    return uv_map.unsqueeze(0)", "\n\n# endregion\n\n\n# region ANIMATION Utilities\ndef apply_easing(value, easing_type):\n    if value < 0 or value > 1:\n        raise ValueError(\"The value should be between 0 and 1.\")\n\n    if easing_type == \"Linear\":\n        return value\n\n    # Back easing functions\n    def easeInBack(t):\n        s = 1.70158\n        return t * t * ((s + 1) * t - s)\n\n    def easeOutBack(t):\n        s = 1.70158\n        return ((t - 1) * t * ((s + 1) * t + s)) + 1\n\n    def easeInOutBack(t):\n        s = 1.70158 * 1.525\n        if t < 0.5:\n            return (t * t * (t * (s + 1) - s)) * 2\n        return ((t - 2) * t * ((s + 1) * t + s) + 2) * 2\n\n    # Elastic easing functions\n    def easeInElastic(t):\n        if t == 0:\n            return 0\n        if t == 1:\n            return 1\n        p = 0.3\n        s = p / 4\n        return -(math.pow(2, 10 * (t - 1)) * math.sin((t - 1 - s) * (2 * math.pi) / p))\n\n    def easeOutElastic(t):\n        if t == 0:\n            return 0\n        if t == 1:\n            return 1\n        p = 0.3\n        s = p / 4\n        return math.pow(2, -10 * t) * math.sin((t - s) * (2 * math.pi) / p) + 1\n\n    def easeInOutElastic(t):\n        if t == 0:\n            return 0\n        if t == 1:\n            return 1\n        p = 0.3 * 1.5\n        s = p / 4\n        t = t * 2\n        if t < 1:\n            return -0.5 * (\n                math.pow(2, 10 * (t - 1)) * math.sin((t - 1 - s) * (2 * math.pi) / p)\n            )\n        return (\n            0.5 * math.pow(2, -10 * (t - 1)) * math.sin((t - 1 - s) * (2 * math.pi) / p)\n            + 1\n        )\n\n    # Bounce easing functions\n    def easeInBounce(t):\n        return 1 - easeOutBounce(1 - t)\n\n    def easeOutBounce(t):\n        if t < (1 / 2.75):\n            return 7.5625 * t * t\n        elif t < (2 / 2.75):\n            t -= 1.5 / 2.75\n            return 7.5625 * t * t + 0.75\n        elif t < (2.5 / 2.75):\n            t -= 2.25 / 2.75\n            return 7.5625 * t * t + 0.9375\n        else:\n            t -= 2.625 / 2.75\n            return 7.5625 * t * t + 0.984375\n\n    def easeInOutBounce(t):\n        if t < 0.5:\n            return easeInBounce(t * 2) * 0.5\n        return easeOutBounce(t * 2 - 1) * 0.5 + 0.5\n\n    # Quart easing functions\n    def easeInQuart(t):\n        return t * t * t * t\n\n    def easeOutQuart(t):\n        t -= 1\n        return -(t**2 * t * t - 1)\n\n    def easeInOutQuart(t):\n        t *= 2\n        if t < 1:\n            return 0.5 * t * t * t * t\n        t -= 2\n        return -0.5 * (t**2 * t * t - 2)\n\n    # Cubic easing functions\n    def easeInCubic(t):\n        return t * t * t\n\n    def easeOutCubic(t):\n        t -= 1\n        return t**2 * t + 1\n\n    def easeInOutCubic(t):\n        t *= 2\n        if t < 1:\n            return 0.5 * t * t * t\n        t -= 2\n        return 0.5 * (t**2 * t + 2)\n\n    # Circ easing functions\n    def easeInCirc(t):\n        return -(math.sqrt(1 - t * t) - 1)\n\n    def easeOutCirc(t):\n        t -= 1\n        return math.sqrt(1 - t**2)\n\n    def easeInOutCirc(t):\n        t *= 2\n        if t < 1:\n            return -0.5 * (math.sqrt(1 - t**2) - 1)\n        t -= 2\n        return 0.5 * (math.sqrt(1 - t**2) + 1)\n\n    # Sine easing functions\n    def easeInSine(t):\n        return -math.cos(t * (math.pi / 2)) + 1\n\n    def easeOutSine(t):\n        return math.sin(t * (math.pi / 2))\n\n    def easeInOutSine(t):\n        return -0.5 * (math.cos(math.pi * t) - 1)\n\n    easing_functions = {\n        \"Sine In\": easeInSine,\n        \"Sine Out\": easeOutSine,\n        \"Sine In/Out\": easeInOutSine,\n        \"Quart In\": easeInQuart,\n        \"Quart Out\": easeOutQuart,\n        \"Quart In/Out\": easeInOutQuart,\n        \"Cubic In\": easeInCubic,\n        \"Cubic Out\": easeOutCubic,\n        \"Cubic In/Out\": easeInOutCubic,\n        \"Circ In\": easeInCirc,\n        \"Circ Out\": easeOutCirc,\n        \"Circ In/Out\": easeInOutCirc,\n        \"Back In\": easeInBack,\n        \"Back Out\": easeOutBack,\n        \"Back In/Out\": easeInOutBack,\n        \"Elastic In\": easeInElastic,\n        \"Elastic Out\": easeOutElastic,\n        \"Elastic In/Out\": easeInOutElastic,\n        \"Bounce In\": easeInBounce,\n        \"Bounce Out\": easeOutBounce,\n        \"Bounce In/Out\": easeInOutBounce,\n    }\n\n    function_ease = easing_functions.get(easing_type)\n    if function_ease:\n        return function_ease(value)\n\n    log.error(f\"Unknown easing type: {easing_type}\")\n    log.error(f\"Available easing types: {list(easing_functions.keys())}\")\n    raise ValueError(f\"Unknown easing type: {easing_type}\")", "\n\n# endregion\n"]}
{"filename": "nodes/deep_bump.py", "chunked_list": ["import onnxruntime as ort\nimport numpy as np\nimport pathlib\nimport onnxruntime as ort\nimport numpy as np\nfrom .. import utils as utils_inference\nfrom ..log import log\n\n# Disable MS telemetry\nort.disable_telemetry_events()", "# Disable MS telemetry\nort.disable_telemetry_events()\n\n\n# - COLOR to NORMALS\ndef color_to_normals(color_img, overlap, progress_callback):\n    \"\"\"Computes a normal map from the given color map. 'color_img' must be a numpy array\n    in C,H,W format (with C as RGB). 'overlap' must be one of 'SMALL', 'MEDIUM', 'LARGE'.\n    \"\"\"\n\n    # Remove alpha & convert to grayscale\n    img = np.mean(color_img[:3], axis=0, keepdimss=True)\n\n    # Split image in tiles\n    log.debug(\"DeepBump Color \u2192 Normals : tilling\")\n    tile_size = 256\n    overlaps = {\n        \"SMALL\": tile_size // 6,\n        \"MEDIUM\": tile_size // 4,\n        \"LARGE\": tile_size // 2,\n    }\n    stride_size = tile_size - overlaps[overlap]\n    tiles, paddings = utils_inference.tiles_split(\n        img, (tile_size, tile_size), (stride_size, stride_size)\n    )\n\n    # Load model\n    log.debug(\"DeepBump Color \u2192 Normals : loading model\")\n    addon_path = str(pathlib.Path(__file__).parent.absolute())\n    ort_session = ort.InferenceSession(f\"{addon_path}/models/deepbump256.onnx\")\n\n    # Predict normal map for each tile\n    log.debug(\"DeepBump Color \u2192 Normals : generating\")\n    pred_tiles = utils_inference.tiles_infer(\n        tiles, ort_session, progress_callback=progress_callback\n    )\n\n    # Merge tiles\n    log.debug(\"DeepBump Color \u2192 Normals : merging\")\n    pred_img = utils_inference.tiles_merge(\n        pred_tiles,\n        (stride_size, stride_size),\n        (3, img.shape[1], img.shape[2]),\n        paddings,\n    )\n\n    # Normalize each pixel to unit vector\n    pred_img = utils_inference.normalize(pred_img)\n\n    return pred_img", "\n\n# - NORMALS to CURVATURE\ndef conv_1d(array, kernel_1d):\n    \"\"\"Performs row by row 1D convolutions of the given 2D image with the given 1D kernel.\"\"\"\n\n    # Input kernel length must be odd\n    k_l = len(kernel_1d)\n    assert k_l % 2 != 0\n    # Convolution is repeat-padded\n    extended = np.pad(array, k_l // 2, mode=\"wrap\")\n    # Output has same size as input (padded, valid-mode convolution)\n    output = np.empty(array.shape)\n    for i in range(array.shape[0]):\n        output[i] = np.convolve(extended[i + (k_l // 2)], kernel_1d, mode=\"valid\")\n\n    return output * -1", "\n\ndef gaussian_kernel(length, sigma):\n    \"\"\"Returns a 1D gaussian kernel of size 'length'.\"\"\"\n\n    space = np.linspace(-(length - 1) / 2, (length - 1) / 2, length)\n    kernel = np.exp(-0.5 * np.square(space) / np.square(sigma))\n    return kernel / np.sum(kernel)\n\n\ndef normalize(np_array):\n    \"\"\"Normalize all elements of the given numpy array to [0,1]\"\"\"\n\n    return (np_array - np.min(np_array)) / (np.max(np_array) - np.min(np_array))", "\n\ndef normalize(np_array):\n    \"\"\"Normalize all elements of the given numpy array to [0,1]\"\"\"\n\n    return (np_array - np.min(np_array)) / (np.max(np_array) - np.min(np_array))\n\n\ndef normals_to_curvature(normals_img, blur_radius, progress_callback):\n    \"\"\"Computes a curvature map from the given normal map. 'normals_img' must be a numpy array\n    in C,H,W format (with C as RGB). 'blur_radius' must be one of 'SMALLEST', 'SMALLER', 'SMALL',\n    'MEDIUM', 'LARGE', 'LARGER', 'LARGEST'.\"\"\"\n\n    # Convolutions on normal map red & green channels\n    if progress_callback is not None:\n        progress_callback(0, 4)\n    diff_kernel = np.array([-1, 0, 1])\n    h_conv = conv_1d(normals_img[0, :, :], diff_kernel)\n    if progress_callback is not None:\n        progress_callback(1, 4)\n    v_conv = conv_1d(-1 * normals_img[1, :, :].T, diff_kernel).T\n    if progress_callback is not None:\n        progress_callback(2, 4)\n\n    # Sum detected edges\n    edges_conv = h_conv + v_conv\n\n    # Blur radius size is proportional to img sizes\n    blur_factors = {\n        \"SMALLEST\": 1 / 256,\n        \"SMALLER\": 1 / 128,\n        \"SMALL\": 1 / 64,\n        \"MEDIUM\": 1 / 32,\n        \"LARGE\": 1 / 16,\n        \"LARGER\": 1 / 8,\n        \"LARGEST\": 1 / 4,\n    }\n    assert blur_radius in blur_factors\n    blur_radius_px = int(np.mean(normals_img.shape[1:3]) * blur_factors[blur_radius])\n\n    # If blur radius too small, do not blur\n    if blur_radius_px < 2:\n        edges_conv = normalize(edges_conv)\n        return np.stack([edges_conv, edges_conv, edges_conv])\n\n    # Make sure blur kernel length is odd\n    if blur_radius_px % 2 == 0:\n        blur_radius_px += 1\n\n    # Blur curvature with separated convolutions\n    sigma = blur_radius_px // 8\n    if sigma == 0:\n        sigma = 1\n    g_kernel = gaussian_kernel(blur_radius_px, sigma)\n    h_blur = conv_1d(edges_conv, g_kernel)\n    if progress_callback is not None:\n        progress_callback(3, 4)\n    v_blur = conv_1d(h_blur.T, g_kernel).T\n    if progress_callback is not None:\n        progress_callback(4, 4)\n\n    # Normalize to [0,1]\n    curvature = normalize(v_blur)\n\n    # Expand single channel the three channels (RGB)\n    return np.stack([curvature, curvature, curvature])", "def normals_to_curvature(normals_img, blur_radius, progress_callback):\n    \"\"\"Computes a curvature map from the given normal map. 'normals_img' must be a numpy array\n    in C,H,W format (with C as RGB). 'blur_radius' must be one of 'SMALLEST', 'SMALLER', 'SMALL',\n    'MEDIUM', 'LARGE', 'LARGER', 'LARGEST'.\"\"\"\n\n    # Convolutions on normal map red & green channels\n    if progress_callback is not None:\n        progress_callback(0, 4)\n    diff_kernel = np.array([-1, 0, 1])\n    h_conv = conv_1d(normals_img[0, :, :], diff_kernel)\n    if progress_callback is not None:\n        progress_callback(1, 4)\n    v_conv = conv_1d(-1 * normals_img[1, :, :].T, diff_kernel).T\n    if progress_callback is not None:\n        progress_callback(2, 4)\n\n    # Sum detected edges\n    edges_conv = h_conv + v_conv\n\n    # Blur radius size is proportional to img sizes\n    blur_factors = {\n        \"SMALLEST\": 1 / 256,\n        \"SMALLER\": 1 / 128,\n        \"SMALL\": 1 / 64,\n        \"MEDIUM\": 1 / 32,\n        \"LARGE\": 1 / 16,\n        \"LARGER\": 1 / 8,\n        \"LARGEST\": 1 / 4,\n    }\n    assert blur_radius in blur_factors\n    blur_radius_px = int(np.mean(normals_img.shape[1:3]) * blur_factors[blur_radius])\n\n    # If blur radius too small, do not blur\n    if blur_radius_px < 2:\n        edges_conv = normalize(edges_conv)\n        return np.stack([edges_conv, edges_conv, edges_conv])\n\n    # Make sure blur kernel length is odd\n    if blur_radius_px % 2 == 0:\n        blur_radius_px += 1\n\n    # Blur curvature with separated convolutions\n    sigma = blur_radius_px // 8\n    if sigma == 0:\n        sigma = 1\n    g_kernel = gaussian_kernel(blur_radius_px, sigma)\n    h_blur = conv_1d(edges_conv, g_kernel)\n    if progress_callback is not None:\n        progress_callback(3, 4)\n    v_blur = conv_1d(h_blur.T, g_kernel).T\n    if progress_callback is not None:\n        progress_callback(4, 4)\n\n    # Normalize to [0,1]\n    curvature = normalize(v_blur)\n\n    # Expand single channel the three channels (RGB)\n    return np.stack([curvature, curvature, curvature])", "\n\n# - NORMALS to HEIGHT\ndef normals_to_grad(normals_img):\n    return (normals_img[0] - 0.5) * 2, (normals_img[1] - 0.5) * 2\n\n\ndef copy_flip(grad_x, grad_y):\n    \"\"\"Concat 4 flipped copies of input gradients (makes them wrap).\n    Output is twice bigger in both dimensions.\"\"\"\n\n    grad_x_top = np.hstack([grad_x, -np.flip(grad_x, axis=1)])\n    grad_x_bottom = np.hstack([np.flip(grad_x, axis=0), -np.flip(grad_x)])\n    new_grad_x = np.vstack([grad_x_top, grad_x_bottom])\n\n    grad_y_top = np.hstack([grad_y, np.flip(grad_y, axis=1)])\n    grad_y_bottom = np.hstack([-np.flip(grad_y, axis=0), -np.flip(grad_y)])\n    new_grad_y = np.vstack([grad_y_top, grad_y_bottom])\n\n    return new_grad_x, new_grad_y", "\n\ndef frankot_chellappa(grad_x, grad_y, progress_callback=None):\n    \"\"\"Frankot-Chellappa depth-from-gradient algorithm.\"\"\"\n\n    if progress_callback is not None:\n        progress_callback(0, 3)\n\n    rows, cols = grad_x.shape\n\n    rows_scale = (np.arange(rows) - (rows // 2 + 1)) / (rows - rows % 2)\n    cols_scale = (np.arange(cols) - (cols // 2 + 1)) / (cols - cols % 2)\n\n    u_grid, v_grid = np.meshgrid(cols_scale, rows_scale)\n\n    u_grid = np.fft.ifftshift(u_grid)\n    v_grid = np.fft.ifftshift(v_grid)\n\n    if progress_callback is not None:\n        progress_callback(1, 3)\n\n    grad_x_F = np.fft.fft2(grad_x)\n    grad_y_F = np.fft.fft2(grad_y)\n\n    if progress_callback is not None:\n        progress_callback(2, 3)\n\n    nominator = (-1j * u_grid * grad_x_F) + (-1j * v_grid * grad_y_F)\n    denominator = (u_grid**2) + (v_grid**2) + 1e-16\n\n    Z_F = nominator / denominator\n    Z_F[0, 0] = 0.0\n\n    Z = np.real(np.fft.ifft2(Z_F))\n\n    if progress_callback is not None:\n        progress_callback(3, 3)\n\n    return (Z - np.min(Z)) / (np.max(Z) - np.min(Z))", "\n\ndef normals_to_height(normals_img, seamless, progress_callback):\n    \"\"\"Computes a height map from the given normal map. 'normals_img' must be a numpy array\n    in C,H,W format (with C as RGB). 'seamless' is a bool that should indicates if 'normals_img'\n    is seamless.\"\"\"\n\n    # Flip height axis\n    flip_img = np.flip(normals_img, axis=1)\n\n    # Get gradients from normal map\n    grad_x, grad_y = normals_to_grad(flip_img)\n    grad_x = np.flip(grad_x, axis=0)\n    grad_y = np.flip(grad_y, axis=0)\n\n    # If non-seamless chosen, expand gradients\n    if not seamless:\n        grad_x, grad_y = copy_flip(grad_x, grad_y)\n\n    # Compute height\n    pred_img = frankot_chellappa(-grad_x, grad_y, progress_callback=progress_callback)\n\n    # Cut to valid part if gradients were expanded\n    if not seamless:\n        height, width = normals_img.shape[1], normals_img.shape[2]\n        pred_img = pred_img[:height, :width]\n\n    # Expand single channel the three channels (RGB)\n    return np.stack([pred_img, pred_img, pred_img])", "\n\n# - ADDON\nclass DeepBump:\n    \"\"\"Normal & height maps generation from single pictures\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"mode\": (\n                    [\"Color to Normals\", \"Normals to Curvature\", \"Normals to Height\"],\n                ),\n                \"color_to_normals_overlap\": ([\"SMALL\", \"MEDIUM\", \"LARGE\"],),\n                \"normals_to_curvature_blur_radius\": (\n                    [\n                        \"SMALLEST\",\n                        \"SMALLER\",\n                        \"SMALL\",\n                        \"MEDIUM\",\n                        \"LARGE\",\n                        \"LARGER\",\n                        \"LARGEST\",\n                    ],\n                ),\n                \"normals_to_height_seamless\": (\"BOOLEAN\", {\"default\": False}),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"apply\"\n\n    CATEGORY = \"mtb/textures\"\n\n    def apply(\n        self,\n        image,\n        mode=\"Color to Normals\",\n        color_to_normals_overlap=\"SMALL\",\n        normals_to_curvature_blur_radius=\"SMALL\",\n        normals_to_height_seamless=True,\n    ):\n        image = utils_inference.tensor2pil(image)\n\n        in_img = np.transpose(image, (2, 0, 1)) / 255\n\n        log.debug(f\"Input image shape: {in_img.shape}\")\n\n        # Apply processing\n        if mode == \"Color to Normals\":\n            out_img = color_to_normals(in_img, color_to_normals_overlap, None)\n        if mode == \"Normals to Curvature\":\n            out_img = normals_to_curvature(\n                in_img, normals_to_curvature_blur_radius, None\n            )\n        if mode == \"Normals to Height\":\n            out_img = normals_to_height(in_img, normals_to_height_seamless, None)\n\n        out_img = (np.transpose(out_img, (1, 2, 0)) * 255).astype(np.uint8)\n\n        return (utils_inference.pil2tensor(out_img),)", "\n\n__nodes__ = [DeepBump]\n"]}
{"filename": "nodes/faceenhance.py", "chunked_list": ["from gfpgan import GFPGANer\nimport cv2\nimport numpy as np\nimport os\nfrom pathlib import Path\nimport folder_paths\nfrom ..utils import pil2tensor, np2tensor, tensor2np\n\nfrom basicsr.utils import imwrite\n", "from basicsr.utils import imwrite\n\n\nfrom PIL import Image\nimport torch\nfrom ..log import NullWriter, log\nfrom comfy import model_management\nimport comfy\nimport comfy.utils\nfrom typing import Tuple", "import comfy.utils\nfrom typing import Tuple\n\n\nclass LoadFaceEnhanceModel:\n    \"\"\"Loads a GFPGan or RestoreFormer model for face enhancement.\"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n    @classmethod\n    def get_models_root(cls):\n        fr = Path(folder_paths.models_dir) / \"face_restore\"\n        if fr.exists():\n            return (fr, None)\n\n        um = Path(folder_paths.models_dir) / \"upscale_models\"\n        return (fr, um) if um.exists() else (None, None)\n\n    @classmethod\n    def get_models(cls):\n        fr_models_path, um_models_path = cls.get_models_root()\n\n        if fr_models_path is None and um_models_path is None:\n            log.warning(\"Face restoration models not found.\")\n            return []\n        if not fr_models_path.exists():\n            log.warning(\n                f\"No Face Restore checkpoints found at {fr_models_path} (if you've used mtb before these checkpoints were saved in upscale_models before)\"\n            )\n            log.warning(\n                \"For now we fallback to upscale_models but this will be removed in a future version\"\n            )\n            if um_models_path.exists():\n                return [\n                    x\n                    for x in um_models_path.iterdir()\n                    if x.name.endswith(\".pth\")\n                    and (\"GFPGAN\" in x.name or \"RestoreFormer\" in x.name)\n                ]\n            return []\n\n        return [\n            x\n            for x in fr_models_path.iterdir()\n            if x.name.endswith(\".pth\")\n            and (\"GFPGAN\" in x.name or \"RestoreFormer\" in x.name)\n        ]\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"model_name\": (\n                    [x.name for x in cls.get_models()],\n                    {\"default\": \"None\"},\n                ),\n                \"upscale\": (\"INT\", {\"default\": 1}),\n            },\n            \"optional\": {\"bg_upsampler\": (\"UPSCALE_MODEL\", {\"default\": None})},\n        }\n\n    RETURN_TYPES = (\"FACEENHANCE_MODEL\",)\n    RETURN_NAMES = (\"model\",)\n    FUNCTION = \"load_model\"\n    CATEGORY = \"mtb/facetools\"\n\n    def load_model(self, model_name, upscale=2, bg_upsampler=None):\n        basic = \"RestoreFormer\" not in model_name\n\n        fr_root, um_root = self.get_models_root()\n\n        if bg_upsampler is not None:\n            log.warning(\n                f\"Upscale value overridden to {bg_upsampler.scale} from bg_upsampler\"\n            )\n            upscale = bg_upsampler.scale\n            bg_upsampler = BGUpscaleWrapper(bg_upsampler)\n\n        sys.stdout = NullWriter()\n        model = GFPGANer(\n            model_path=(\n                (fr_root if fr_root.exists() else um_root) / model_name\n            ).as_posix(),\n            upscale=upscale,\n            arch=\"clean\" if basic else \"RestoreFormer\",  # or original for v1.0 only\n            channel_multiplier=2,  # 1 for v1.0 only\n            bg_upsampler=bg_upsampler,\n        )\n\n        sys.stdout = sys.__stdout__\n        return (model,)", "\n\nclass BGUpscaleWrapper:\n    def __init__(self, upscale_model) -> None:\n        self.upscale_model = upscale_model\n\n    def enhance(self, img: Image.Image, outscale=2):\n        device = model_management.get_torch_device()\n        self.upscale_model.to(device)\n\n        tile = 128 + 64\n        overlap = 8\n\n        imgt = np2tensor(img)\n        imgt = imgt.movedim(-1, -3).to(device)\n\n        steps = imgt.shape[0] * comfy.utils.get_tiled_scale_steps(\n            imgt.shape[3], imgt.shape[2], tile_x=tile, tile_y=tile, overlap=overlap\n        )\n\n        log.debug(f\"Steps: {steps}\")\n\n        pbar = comfy.utils.ProgressBar(steps)\n\n        s = comfy.utils.tiled_scale(\n            imgt,\n            lambda a: self.upscale_model(a),\n            tile_x=tile,\n            tile_y=tile,\n            overlap=overlap,\n            upscale_amount=self.upscale_model.scale,\n            pbar=pbar,\n        )\n\n        self.upscale_model.cpu()\n        s = torch.clamp(s.movedim(-3, -1), min=0, max=1.0)\n        return (tensor2np(s)[0],)", "\n\nimport sys\n\n\nclass RestoreFace:\n    \"\"\"Uses GFPGan to restore faces\"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"restore\"\n    CATEGORY = \"mtb/facetools\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"model\": (\"FACEENHANCE_MODEL\",),\n                # Input are aligned faces\n                \"aligned\": (\"BOOLEAN\", {\"default\": False}),\n                # Only restore the center face\n                \"only_center_face\": (\"BOOLEAN\", {\"default\": False}),\n                # Adjustable weights\n                \"weight\": (\"FLOAT\", {\"default\": 0.5}),\n                \"save_tmp_steps\": (\"BOOLEAN\", {\"default\": True}),\n            }\n        }\n\n    def do_restore(\n        self,\n        image: torch.Tensor,\n        model: GFPGANer,\n        aligned,\n        only_center_face,\n        weight,\n        save_tmp_steps,\n    ) -> torch.Tensor:\n        pimage = tensor2np(image)[0]\n        width, height = pimage.shape[1], pimage.shape[0]\n        source_img = cv2.cvtColor(np.array(pimage), cv2.COLOR_RGB2BGR)\n\n        sys.stdout = NullWriter()\n        cropped_faces, restored_faces, restored_img = model.enhance(\n            source_img,\n            has_aligned=aligned,\n            only_center_face=only_center_face,\n            paste_back=True,\n            # TODO: weight has no effect in 1.3 and 1.4 (only tested these for now...)\n            weight=weight,\n        )\n        sys.stdout = sys.__stdout__\n        log.warning(f\"Weight value has no effect for now. (value: {weight})\")\n\n        if save_tmp_steps:\n            self.save_intermediate_images(cropped_faces, restored_faces, height, width)\n        output = None\n        if restored_img is not None:\n            output = Image.fromarray(cv2.cvtColor(restored_img, cv2.COLOR_BGR2RGB))\n            # imwrite(restored_img, save_restore_path)\n\n        return pil2tensor(output)\n\n    def restore(\n        self,\n        image: torch.Tensor,\n        model: GFPGANer,\n        aligned=False,\n        only_center_face=False,\n        weight=0.5,\n        save_tmp_steps=True,\n    ) -> Tuple[torch.Tensor]:\n        out = [\n            self.do_restore(\n                image[i], model, aligned, only_center_face, weight, save_tmp_steps\n            )\n            for i in range(image.size(0))\n        ]\n\n        return (torch.cat(out, dim=0),)\n\n    def get_step_image_path(self, step, idx):\n        (\n            full_output_folder,\n            filename,\n            counter,\n            _subfolder,\n            _filename_prefix,\n        ) = folder_paths.get_save_image_path(\n            f\"{step}_{idx:03}\",\n            folder_paths.temp_directory,\n        )\n        file = f\"{filename}_{counter:05}_.png\"\n\n        return os.path.join(full_output_folder, file)\n\n    def save_intermediate_images(self, cropped_faces, restored_faces, height, width):\n        for idx, (cropped_face, restored_face) in enumerate(\n            zip(cropped_faces, restored_faces)\n        ):\n            face_id = idx + 1\n            file = self.get_step_image_path(\"cropped_faces\", face_id)\n            imwrite(cropped_face, file)\n\n            file = self.get_step_image_path(\"cropped_faces_restored\", face_id)\n            imwrite(restored_face, file)\n\n            file = self.get_step_image_path(\"cropped_faces_compare\", face_id)\n\n            # save comparison image\n            cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n            imwrite(cmp_img, file)", "\n\n__nodes__ = [RestoreFace, LoadFaceEnhanceModel]\n"]}
{"filename": "nodes/latent_processing.py", "chunked_list": ["import torch\n\n\nclass LatentLerp:\n    \"\"\"Linear interpolation (blend) between two latent vectors\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"A\": (\"LATENT\",),\n                \"B\": (\"LATENT\",),\n                \"t\": (\"FLOAT\", {\"default\": 0.5, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01}),\n            }\n        }\n\n    RETURN_TYPES = (\"LATENT\",)\n    FUNCTION = \"lerp_latent\"\n\n    CATEGORY = \"mtb/latent\"\n\n    def lerp_latent(self, A, B, t):\n        a = A.copy()\n        b = B.copy()\n\n        torch.lerp(a[\"samples\"], b[\"samples\"], t, out=a[\"samples\"])\n\n        return (a,)", "\n\n__nodes__ = [\n    LatentLerp,\n]\n"]}
{"filename": "nodes/transform.py", "chunked_list": ["import torch\nimport torchvision.transforms.functional as TF\nfrom ..utils import log, hex_to_rgb, tensor2pil, pil2tensor\nfrom math import sqrt, ceil\nfrom typing import cast\nfrom PIL import Image\n\n\nclass TransformImage:\n    \"\"\"Save torch tensors (image, mask or latent) to disk, useful to debug things outside comfy\n\n\n    it return a tensor representing the transformed images with the same shape as the input tensor\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"x\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096}),\n                \"y\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096}),\n                \"zoom\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.001, \"step\": 0.01}),\n                \"angle\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -360, \"max\": 360}),\n                \"shear\": (\n                    \"FLOAT\",\n                    {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096},\n                ),\n                \"border_handling\": (\n                    [\"edge\", \"constant\", \"reflect\", \"symmetric\"],\n                    {\"default\": \"edge\"},\n                ),\n                \"constant_color\": (\"COLOR\", {\"default\": \"#000000\"}),\n            },\n        }\n\n    FUNCTION = \"transform\"\n    RETURN_TYPES = (\"IMAGE\",)\n    CATEGORY = \"mtb/transform\"\n\n    def transform(\n        self,\n        image: torch.Tensor,\n        x: float,\n        y: float,\n        zoom: float,\n        angle: float,\n        shear: float,\n        border_handling=\"edge\",\n        constant_color=None,\n    ):\n        x = int(x)\n        y = int(y)\n        angle = int(angle)\n\n        log.debug(f\"Zoom: {zoom} | x: {x}, y: {y}, angle: {angle}, shear: {shear}\")\n\n        if image.size(0) == 0:\n            return (torch.zeros(0),)\n        transformed_images = []\n        frames_count, frame_height, frame_width, frame_channel_count = image.size()\n\n        new_height, new_width = int(frame_height * zoom), int(frame_width * zoom)\n\n        log.debug(f\"New height: {new_height}, New width: {new_width}\")\n\n        # - Calculate diagonal of the original image\n        diagonal = sqrt(frame_width**2 + frame_height**2)\n        max_padding = ceil(diagonal * zoom - min(frame_width, frame_height))\n        # Calculate padding for zoom\n        pw = int(frame_width - new_width)\n        ph = int(frame_height - new_height)\n\n        pw += abs(max_padding)\n        ph += abs(max_padding)\n\n        padding = [max(0, pw + x), max(0, ph + y), max(0, pw - x), max(0, ph - y)]\n\n        constant_color = hex_to_rgb(constant_color)\n        log.debug(f\"Fill Tuple: {constant_color}\")\n\n        for img in tensor2pil(image):\n            img = TF.pad(\n                img,  # transformed_frame,\n                padding=padding,\n                padding_mode=border_handling,\n                fill=constant_color or 0,\n            )\n\n            img = cast(\n                Image.Image,\n                TF.affine(img, angle=angle, scale=zoom, translate=[x, y], shear=shear),\n            )\n\n            left = abs(padding[0])\n            upper = abs(padding[1])\n            right = img.width - abs(padding[2])\n            bottom = img.height - abs(padding[3])\n\n            # log.debug(\"crop is [:,top:bottom, left:right] for tensors\")\n            log.debug(\"crop is [left, top, right, bottom] for PIL\")\n            log.debug(f\"crop is {left}, {upper}, {right}, {bottom}\")\n            img = img.crop((left, upper, right, bottom))\n\n            transformed_images.append(img)\n\n        return (pil2tensor(transformed_images),)", "class TransformImage:\n    \"\"\"Save torch tensors (image, mask or latent) to disk, useful to debug things outside comfy\n\n\n    it return a tensor representing the transformed images with the same shape as the input tensor\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"x\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096}),\n                \"y\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096}),\n                \"zoom\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.001, \"step\": 0.01}),\n                \"angle\": (\"FLOAT\", {\"default\": 0, \"step\": 1, \"min\": -360, \"max\": 360}),\n                \"shear\": (\n                    \"FLOAT\",\n                    {\"default\": 0, \"step\": 1, \"min\": -4096, \"max\": 4096},\n                ),\n                \"border_handling\": (\n                    [\"edge\", \"constant\", \"reflect\", \"symmetric\"],\n                    {\"default\": \"edge\"},\n                ),\n                \"constant_color\": (\"COLOR\", {\"default\": \"#000000\"}),\n            },\n        }\n\n    FUNCTION = \"transform\"\n    RETURN_TYPES = (\"IMAGE\",)\n    CATEGORY = \"mtb/transform\"\n\n    def transform(\n        self,\n        image: torch.Tensor,\n        x: float,\n        y: float,\n        zoom: float,\n        angle: float,\n        shear: float,\n        border_handling=\"edge\",\n        constant_color=None,\n    ):\n        x = int(x)\n        y = int(y)\n        angle = int(angle)\n\n        log.debug(f\"Zoom: {zoom} | x: {x}, y: {y}, angle: {angle}, shear: {shear}\")\n\n        if image.size(0) == 0:\n            return (torch.zeros(0),)\n        transformed_images = []\n        frames_count, frame_height, frame_width, frame_channel_count = image.size()\n\n        new_height, new_width = int(frame_height * zoom), int(frame_width * zoom)\n\n        log.debug(f\"New height: {new_height}, New width: {new_width}\")\n\n        # - Calculate diagonal of the original image\n        diagonal = sqrt(frame_width**2 + frame_height**2)\n        max_padding = ceil(diagonal * zoom - min(frame_width, frame_height))\n        # Calculate padding for zoom\n        pw = int(frame_width - new_width)\n        ph = int(frame_height - new_height)\n\n        pw += abs(max_padding)\n        ph += abs(max_padding)\n\n        padding = [max(0, pw + x), max(0, ph + y), max(0, pw - x), max(0, ph - y)]\n\n        constant_color = hex_to_rgb(constant_color)\n        log.debug(f\"Fill Tuple: {constant_color}\")\n\n        for img in tensor2pil(image):\n            img = TF.pad(\n                img,  # transformed_frame,\n                padding=padding,\n                padding_mode=border_handling,\n                fill=constant_color or 0,\n            )\n\n            img = cast(\n                Image.Image,\n                TF.affine(img, angle=angle, scale=zoom, translate=[x, y], shear=shear),\n            )\n\n            left = abs(padding[0])\n            upper = abs(padding[1])\n            right = img.width - abs(padding[2])\n            bottom = img.height - abs(padding[3])\n\n            # log.debug(\"crop is [:,top:bottom, left:right] for tensors\")\n            log.debug(\"crop is [left, top, right, bottom] for PIL\")\n            log.debug(f\"crop is {left}, {upper}, {right}, {bottom}\")\n            img = img.crop((left, upper, right, bottom))\n\n            transformed_images.append(img)\n\n        return (pil2tensor(transformed_images),)", "\n\n__nodes__ = [TransformImage]\n"]}
{"filename": "nodes/image_interpolation.py", "chunked_list": ["from typing import List\nfrom pathlib import Path\nimport os\nimport glob\nimport folder_paths\nfrom ..log import log\nimport torch\nfrom frame_interpolation.eval import util, interpolator\nimport numpy as np\nimport comfy", "import numpy as np\nimport comfy\nimport comfy.utils\nimport tensorflow as tf\nimport comfy.model_management as model_management\n\n\nclass LoadFilmModel:\n    \"\"\"Loads a FILM model\"\"\"\n\n    @staticmethod\n    def get_models() -> List[Path]:\n        models_path = os.path.join(folder_paths.models_dir, \"FILM/*\")\n        models = glob.glob(models_path)\n        models = [Path(x) for x in models if x.endswith(\".onnx\") or x.endswith(\".pth\")]\n        return models\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"film_model\": (\n                    [\"L1\", \"Style\", \"VGG\"],\n                    {\"default\": \"Style\"},\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\"FILM_MODEL\",)\n    FUNCTION = \"load_model\"\n    CATEGORY = \"mtb/frame iterpolation\"\n\n    def load_model(self, film_model: str):\n        model_path = Path(folder_paths.models_dir) / \"FILM\" / film_model\n        if not (model_path / \"saved_model.pb\").exists():\n            model_path = model_path / \"saved_model\"\n\n        if not model_path.exists():\n            log.error(f\"Model {model_path} does not exist\")\n            raise ValueError(f\"Model {model_path} does not exist\")\n\n        log.info(f\"Loading model {model_path}\")\n\n        return (interpolator.Interpolator(model_path.as_posix(), None),)", "\n\nclass FilmInterpolation:\n    \"\"\"Google Research FILM frame interpolation for large motion\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                \"interpolate\": (\"INT\", {\"default\": 2, \"min\": 1, \"max\": 50}),\n                \"film_model\": (\"FILM_MODEL\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"do_interpolation\"\n    CATEGORY = \"mtb/frame iterpolation\"\n\n    def do_interpolation(\n        self,\n        images: torch.Tensor,\n        interpolate: int,\n        film_model: interpolator.Interpolator,\n    ):\n        n = images.size(0)\n        # check if images is an empty tensor and return it...\n        if n == 0:\n            return (images,)\n\n        # check if tensorflow GPU is available\n        available_gpus = tf.config.list_physical_devices(\"GPU\")\n        if not len(available_gpus):\n            log.warning(\n                \"Tensorflow GPU not available, falling back to CPU this will be very slow\"\n            )\n        else:\n            log.debug(f\"Tensorflow GPU available, using {available_gpus}\")\n\n        num_frames = (n - 1) * (2 ** (interpolate) - 1)\n        log.debug(f\"Will interpolate into {num_frames} frames\")\n\n        in_frames = [images[i] for i in range(n)]\n        out_tensors = []\n\n        pbar = comfy.utils.ProgressBar(num_frames)\n\n        for frame in util.interpolate_recursively_from_memory(\n            in_frames, interpolate, film_model\n        ):\n            out_tensors.append(\n                torch.from_numpy(frame) if isinstance(frame, np.ndarray) else frame\n            )\n            model_management.throw_exception_if_processing_interrupted()\n            pbar.update(1)\n\n        out_tensors = torch.cat([tens.unsqueeze(0) for tens in out_tensors], dim=0)\n\n        log.debug(f\"Returning {len(out_tensors)} tensors\")\n        log.debug(f\"Output shape {out_tensors.shape}\")\n        log.debug(f\"Output type {out_tensors.dtype}\")\n        return (out_tensors,)", "\n\nclass ConcatImages:\n    \"\"\"Add images to batch\"\"\"\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"concat_images\"\n    CATEGORY = \"mtb/image\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"imageA\": (\"IMAGE\",),\n                \"imageB\": (\"IMAGE\",),\n            },\n        }\n\n    @classmethod\n    def concatenate_tensors(cls, A: torch.Tensor, B: torch.Tensor):\n        # Get the batch sizes of A and B\n        batch_size_A = A.size(0)\n        batch_size_B = B.size(0)\n\n        # Concatenate the tensors along the batch dimension\n        concatenated = torch.cat((A, B), dim=0)\n\n        # Update the batch size in the concatenated tensor\n        concatenated_size = list(concatenated.size())\n        concatenated_size[0] = batch_size_A + batch_size_B\n        concatenated = concatenated.view(*concatenated_size)\n\n        return concatenated\n\n    def concat_images(self, imageA: torch.Tensor, imageB: torch.Tensor):\n        log.debug(f\"Concatenating A ({imageA.shape}) and B ({imageB.shape})\")\n        return (self.concatenate_tensors(imageA, imageB),)", "\n\n__nodes__ = [LoadFilmModel, FilmInterpolation, ConcatImages]\n"]}
{"filename": "nodes/faceswap.py", "chunked_list": ["# region imports\nimport onnxruntime\nfrom pathlib import Path\nfrom PIL import Image\nfrom typing import List, Set, Union, Optional\nimport cv2\nimport folder_paths\nimport glob\nimport insightface\nimport numpy as np", "import insightface\nimport numpy as np\nimport os\nimport torch\nfrom insightface.model_zoo.inswapper import INSwapper\nfrom ..utils import pil2tensor, tensor2pil, download_antelopev2\nfrom ..log import mklog, NullWriter\nimport sys\nimport comfy.model_management as model_management\n", "import comfy.model_management as model_management\n\n\n# endregion\n\nlog = mklog(__name__)\n\n\nclass LoadFaceAnalysisModel:\n    \"\"\"Loads a face analysis model\"\"\"\n\n    models = []\n\n    @staticmethod\n    def get_models() -> List[str]:\n        models_path = os.path.join(folder_paths.models_dir, \"insightface/*\")\n        models = glob.glob(models_path)\n        models = [\n            Path(x).name for x in models if x.endswith(\".onnx\") or x.endswith(\".pth\")\n        ]\n        return models\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"faceswap_model\": (\n                    [\"antelopev2\", \"buffalo_l\", \"buffalo_m\", \"buffalo_sc\"],\n                    {\"default\": \"buffalo_l\"},\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\"FACE_ANALYSIS_MODEL\",)\n    FUNCTION = \"load_model\"\n    CATEGORY = \"mtb/facetools\"\n\n    def load_model(self, faceswap_model: str):\n        if faceswap_model == \"antelopev2\":\n            download_antelopev2()\n\n        face_analyser = insightface.app.FaceAnalysis(\n            name=faceswap_model,\n            root=os.path.join(folder_paths.models_dir, \"insightface\"),\n        )\n        return (face_analyser,)", "class LoadFaceAnalysisModel:\n    \"\"\"Loads a face analysis model\"\"\"\n\n    models = []\n\n    @staticmethod\n    def get_models() -> List[str]:\n        models_path = os.path.join(folder_paths.models_dir, \"insightface/*\")\n        models = glob.glob(models_path)\n        models = [\n            Path(x).name for x in models if x.endswith(\".onnx\") or x.endswith(\".pth\")\n        ]\n        return models\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"faceswap_model\": (\n                    [\"antelopev2\", \"buffalo_l\", \"buffalo_m\", \"buffalo_sc\"],\n                    {\"default\": \"buffalo_l\"},\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\"FACE_ANALYSIS_MODEL\",)\n    FUNCTION = \"load_model\"\n    CATEGORY = \"mtb/facetools\"\n\n    def load_model(self, faceswap_model: str):\n        if faceswap_model == \"antelopev2\":\n            download_antelopev2()\n\n        face_analyser = insightface.app.FaceAnalysis(\n            name=faceswap_model,\n            root=os.path.join(folder_paths.models_dir, \"insightface\"),\n        )\n        return (face_analyser,)", "\n\nclass LoadFaceSwapModel:\n    \"\"\"Loads a faceswap model\"\"\"\n\n    @staticmethod\n    def get_models() -> List[Path]:\n        models_path = os.path.join(folder_paths.models_dir, \"insightface/*\")\n        models = glob.glob(models_path)\n        models = [Path(x) for x in models if x.endswith(\".onnx\") or x.endswith(\".pth\")]\n        return models\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"faceswap_model\": (\n                    [x.name for x in cls.get_models()],\n                    {\"default\": \"None\"},\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\"FACESWAP_MODEL\",)\n    FUNCTION = \"load_model\"\n    CATEGORY = \"mtb/facetools\"\n\n    def load_model(self, faceswap_model: str):\n        model_path = os.path.join(\n            folder_paths.models_dir, \"insightface\", faceswap_model\n        )\n        log.info(f\"Loading model {model_path}\")\n        return (\n            INSwapper(\n                model_path,\n                onnxruntime.InferenceSession(\n                    path_or_bytes=model_path,\n                    providers=onnxruntime.get_available_providers(),\n                ),\n            ),\n        )", "\n\n# region roop node\nclass FaceSwap:\n    \"\"\"Face swap using deepinsight/insightface models\"\"\"\n\n    model = None\n    model_path = None\n\n    def __init__(self) -> None:\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"reference\": (\"IMAGE\",),\n                \"faces_index\": (\"STRING\", {\"default\": \"0\"}),\n                \"faceanalysis_model\": (\"FACE_ANALYSIS_MODEL\", {\"default\": \"None\"}),\n                \"faceswap_model\": (\"FACESWAP_MODEL\", {\"default\": \"None\"}),\n            },\n            \"optional\": {},\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"swap\"\n    CATEGORY = \"mtb/facetools\"\n\n    def swap(\n        self,\n        image: torch.Tensor,\n        reference: torch.Tensor,\n        faces_index: str,\n        faceanalysis_model,\n        faceswap_model,\n    ):\n        def do_swap(img):\n            model_management.throw_exception_if_processing_interrupted()\n            img = tensor2pil(img)[0]\n            ref = tensor2pil(reference)[0]\n            face_ids = {\n                int(x) for x in faces_index.strip(\",\").split(\",\") if x.isnumeric()\n            }\n            sys.stdout = NullWriter()\n            swapped = swap_face(faceanalysis_model, ref, img, faceswap_model, face_ids)\n            sys.stdout = sys.__stdout__\n            return pil2tensor(swapped)\n\n        batch_count = image.size(0)\n\n        log.info(f\"Running insightface swap (batch size: {batch_count})\")\n\n        if reference.size(0) != 1:\n            raise ValueError(\"Reference image must have batch size 1\")\n        if batch_count == 1:\n            image = do_swap(image)\n\n        else:\n            image_batch = [do_swap(image[i]) for i in range(batch_count)]\n            image = torch.cat(image_batch, dim=0)\n\n        return (image,)", "\n\n# endregion\n\n\n# region face swap utils\ndef get_face_single(\n    face_analyser, img_data: np.ndarray, face_index=0, det_size=(640, 640)\n):\n    face_analyser.prepare(ctx_id=0, det_size=det_size)\n    face = face_analyser.get(img_data)\n\n    if len(face) == 0 and det_size[0] > 320 and det_size[1] > 320:\n        log.debug(\"No face ed, trying again with smaller image\")\n        det_size_half = (det_size[0] // 2, det_size[1] // 2)\n        return get_face_single(\n            face_analyser, img_data, face_index=face_index, det_size=det_size_half\n        )\n\n    try:\n        return sorted(face, key=lambda x: x.bbox[0])[face_index]\n    except IndexError:\n        return None", "\n\ndef swap_face(\n    face_analyser,\n    source_img: Union[Image.Image, List[Image.Image]],\n    target_img: Union[Image.Image, List[Image.Image]],\n    face_swapper_model,\n    faces_index: Optional[Set[int]] = None,\n) -> Image.Image:\n    if faces_index is None:\n        faces_index = {0}\n    log.debug(f\"Swapping faces: {faces_index}\")\n    result_image = target_img\n\n    if face_swapper_model is not None:\n        cv_source_img = cv2.cvtColor(np.array(source_img), cv2.COLOR_RGB2BGR)\n        cv_target_img = cv2.cvtColor(np.array(target_img), cv2.COLOR_RGB2BGR)\n        source_face = get_face_single(face_analyser, cv_source_img, face_index=0)\n        if source_face is not None:\n            result = cv_target_img\n\n            for face_num in faces_index:\n                target_face = get_face_single(\n                    face_analyser, cv_target_img, face_index=face_num\n                )\n                if target_face is not None:\n                    sys.stdout = NullWriter()\n                    result = face_swapper_model.get(result, target_face, source_face)\n                    sys.stdout = sys.__stdout__\n                else:\n                    log.warning(f\"No target face found for {face_num}\")\n\n            result_image = Image.fromarray(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n        else:\n            log.warning(\"No source face found\")\n    else:\n        log.error(\"No face swap model provided\")\n    return result_image", "\n\n# endregion face swap utils\n\n\n__nodes__ = [FaceSwap, LoadFaceSwapModel, LoadFaceAnalysisModel]\n"]}
{"filename": "nodes/__init__.py", "chunked_list": [""]}
{"filename": "nodes/crop.py", "chunked_list": ["import torch\nfrom ..utils import tensor2pil, pil2tensor, tensor2np, np2tensor\nfrom PIL import Image, ImageFilter, ImageDraw, ImageChops\nimport numpy as np\n\nfrom ..log import log\n\n\nclass Bbox:\n    \"\"\"The bounding box (BBOX) custom type used by other nodes\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                # \"bbox\": (\"BBOX\",),\n                \"x\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n                \"width\": (\n                    \"INT\",\n                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n                ),\n                \"height\": (\n                    \"INT\",\n                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"BBOX\",)\n    FUNCTION = \"do_crop\"\n    CATEGORY = \"mtb/crop\"\n\n    def do_crop(self, x, y, width, height):  # bbox\n        return (x, y, width, height)", "class Bbox:\n    \"\"\"The bounding box (BBOX) custom type used by other nodes\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                # \"bbox\": (\"BBOX\",),\n                \"x\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n                \"width\": (\n                    \"INT\",\n                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n                ),\n                \"height\": (\n                    \"INT\",\n                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"BBOX\",)\n    FUNCTION = \"do_crop\"\n    CATEGORY = \"mtb/crop\"\n\n    def do_crop(self, x, y, width, height):  # bbox\n        return (x, y, width, height)", "        # return bbox\n\n\nclass BboxFromMask:\n    \"\"\"From a mask extract the bounding box\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n            },\n            \"optional\": {\n                \"image\": (\"IMAGE\",),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"BBOX\",\n        \"IMAGE\",\n    )\n    RETURN_NAMES = (\n        \"bbox\",\n        \"image (optional)\",\n    )\n    FUNCTION = \"extract_bounding_box\"\n    CATEGORY = \"mtb/crop\"\n\n    def extract_bounding_box(self, mask: torch.Tensor, image=None):\n        # if image != None:\n        #     if mask.size(0) != image.size(0):\n        #         if mask.size(0) != 1:\n        #             log.error(\n        #                 f\"Batch count mismatch for mask and image, it can either be 1 mask for X images, or X masks for X images (mask: {mask.shape} | image: {image.shape})\"\n        #             )\n\n        #             raise Exception(\n        #                 f\"Batch count mismatch for mask and image, it can either be 1 mask for X images, or X masks for X images (mask: {mask.shape} | image: {image.shape})\"\n        #             )\n\n        _mask = tensor2pil(1.0 - mask)[0]\n\n        # we invert it\n        alpha_channel = np.array(_mask)\n\n        non_zero_indices = np.nonzero(alpha_channel)\n\n        min_x, max_x = np.min(non_zero_indices[1]), np.max(non_zero_indices[1])\n        min_y, max_y = np.min(non_zero_indices[0]), np.max(non_zero_indices[0])\n\n        # Create a bounding box tuple\n        if image != None:\n            # Convert the image to a NumPy array\n            imgs = tensor2np(image)\n            out = []\n            for img in imgs:\n                # Crop the image from the bounding box\n                img = img[min_y:max_y, min_x:max_x, :]\n                log.debug(f\"Cropped image to shape {img.shape}\")\n                out.append(img)\n\n            image = np2tensor(out)\n            log.debug(f\"Cropped images shape: {image.shape}\")\n        bounding_box = (min_x, min_y, max_x - min_x, max_y - min_y)\n        return (\n            bounding_box,\n            image,\n        )", "\n\nclass Crop:\n    \"\"\"Crops an image and an optional mask to a given bounding box\n\n    The bounding box can be given as a tuple of (x, y, width, height) or as a BBOX type\n    The BBOX input takes precedence over the tuple input\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n            },\n            \"optional\": {\n                \"mask\": (\"MASK\",),\n                \"x\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n                \"y\": (\"INT\", {\"default\": 0, \"max\": 10000000, \"min\": 0, \"step\": 1}),\n                \"width\": (\n                    \"INT\",\n                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n                ),\n                \"height\": (\n                    \"INT\",\n                    {\"default\": 256, \"max\": 10000000, \"min\": 0, \"step\": 1},\n                ),\n                \"bbox\": (\"BBOX\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\", \"MASK\", \"BBOX\")\n    FUNCTION = \"do_crop\"\n\n    CATEGORY = \"mtb/crop\"\n\n    def do_crop(\n        self, image: torch.Tensor, mask=None, x=0, y=0, width=256, height=256, bbox=None\n    ):\n        image = image.numpy()\n        if mask:\n            mask = mask.numpy()\n\n        if bbox != None:\n            x, y, width, height = bbox\n\n        cropped_image = image[:, y : y + height, x : x + width, :]\n        cropped_mask = mask[y : y + height, x : x + width] if mask != None else None\n        crop_data = (x, y, width, height)\n\n        return (\n            torch.from_numpy(cropped_image),\n            torch.from_numpy(cropped_mask) if mask != None else None,\n            crop_data,\n        )", "\n\n# def calculate_intersection(rect1, rect2):\n#     x_left = max(rect1[0], rect2[0])\n#     y_top = max(rect1[1], rect2[1])\n#     x_right = min(rect1[2], rect2[2])\n#     y_bottom = min(rect1[3], rect2[3])\n\n#     return (x_left, y_top, x_right, y_bottom)\n", "#     return (x_left, y_top, x_right, y_bottom)\n\n\ndef bbox_check(bbox, target_size=None):\n    if not target_size:\n        return bbox\n\n    new_bbox = (\n        bbox[0],\n        bbox[1],\n        min(target_size[0] - bbox[0], bbox[2]),\n        min(target_size[1] - bbox[1], bbox[3]),\n    )\n    if new_bbox != bbox:\n        log.warn(f\"BBox too big, constrained to {new_bbox}\")\n\n    return new_bbox", "\n\ndef bbox_to_region(bbox, target_size=None):\n    bbox = bbox_check(bbox, target_size)\n\n    # to region\n    return (bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3])\n\n\nclass Uncrop:\n    \"\"\"Uncrops an image to a given bounding box\n\n    The bounding box can be given as a tuple of (x, y, width, height) or as a BBOX type\n    The BBOX input takes precedence over the tuple input\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"crop_image\": (\"IMAGE\",),\n                \"bbox\": (\"BBOX\",),\n                \"border_blending\": (\n                    \"FLOAT\",\n                    {\"default\": 0.25, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"do_crop\"\n\n    CATEGORY = \"mtb/crop\"\n\n    def do_crop(self, image, crop_image, bbox, border_blending):\n        def inset_border(image, border_width=20, border_color=(0)):\n            width, height = image.size\n            bordered_image = Image.new(image.mode, (width, height), border_color)\n            bordered_image.paste(image, (0, 0))\n            draw = ImageDraw.Draw(bordered_image)\n            draw.rectangle(\n                (0, 0, width - 1, height - 1), outline=border_color, width=border_width\n            )\n            return bordered_image\n\n        single = image.size(0) == 1\n        if image.size(0) != crop_image.size(0):\n            if not single:\n                raise ValueError(\n                    \"The Image batch count is greater than 1, but doesn't match the crop_image batch count. If using batches they should either match or only crop_image must be greater than 1\"\n                )\n\n        images = tensor2pil(image)\n        crop_imgs = tensor2pil(crop_image)\n        out_images = []\n        for i, crop in enumerate(crop_imgs):\n            if single:\n                img = images[0]\n            else:\n                img = images[i]\n\n            # uncrop the image based on the bounding box\n            bb_x, bb_y, bb_width, bb_height = bbox\n\n            paste_region = bbox_to_region((bb_x, bb_y, bb_width, bb_height), img.size)\n            # log.debug(f\"Paste region: {paste_region}\")\n            # new_region = adjust_paste_region(img.size, paste_region)\n            # log.debug(f\"Adjusted paste region: {new_region}\")\n            # # Check if the adjusted paste region is different from the original\n\n            crop_img = crop.convert(\"RGB\")\n\n            log.debug(f\"Crop image size: {crop_img.size}\")\n            log.debug(f\"Image size: {img.size}\")\n\n            if border_blending > 1.0:\n                border_blending = 1.0\n            elif border_blending < 0.0:\n                border_blending = 0.0\n\n            blend_ratio = (max(crop_img.size) / 2) * float(border_blending)\n\n            blend = img.convert(\"RGBA\")\n            mask = Image.new(\"L\", img.size, 0)\n\n            mask_block = Image.new(\"L\", (bb_width, bb_height), 255)\n            mask_block = inset_border(mask_block, int(blend_ratio / 2), (0))\n\n            mask.paste(mask_block, paste_region)\n            log.debug(f\"Blend size: {blend.size} | kind {blend.mode}\")\n            log.debug(f\"Crop image size: {crop_img.size} | kind {crop_img.mode}\")\n            log.debug(f\"BBox: {paste_region}\")\n            blend.paste(crop_img, paste_region)\n\n            mask = mask.filter(ImageFilter.BoxBlur(radius=blend_ratio / 4))\n            mask = mask.filter(ImageFilter.GaussianBlur(radius=blend_ratio / 4))\n\n            blend.putalpha(mask)\n            img = Image.alpha_composite(img.convert(\"RGBA\"), blend)\n            out_images.append(img.convert(\"RGB\"))\n\n        return (pil2tensor(out_images),)", "\nclass Uncrop:\n    \"\"\"Uncrops an image to a given bounding box\n\n    The bounding box can be given as a tuple of (x, y, width, height) or as a BBOX type\n    The BBOX input takes precedence over the tuple input\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"crop_image\": (\"IMAGE\",),\n                \"bbox\": (\"BBOX\",),\n                \"border_blending\": (\n                    \"FLOAT\",\n                    {\"default\": 0.25, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"do_crop\"\n\n    CATEGORY = \"mtb/crop\"\n\n    def do_crop(self, image, crop_image, bbox, border_blending):\n        def inset_border(image, border_width=20, border_color=(0)):\n            width, height = image.size\n            bordered_image = Image.new(image.mode, (width, height), border_color)\n            bordered_image.paste(image, (0, 0))\n            draw = ImageDraw.Draw(bordered_image)\n            draw.rectangle(\n                (0, 0, width - 1, height - 1), outline=border_color, width=border_width\n            )\n            return bordered_image\n\n        single = image.size(0) == 1\n        if image.size(0) != crop_image.size(0):\n            if not single:\n                raise ValueError(\n                    \"The Image batch count is greater than 1, but doesn't match the crop_image batch count. If using batches they should either match or only crop_image must be greater than 1\"\n                )\n\n        images = tensor2pil(image)\n        crop_imgs = tensor2pil(crop_image)\n        out_images = []\n        for i, crop in enumerate(crop_imgs):\n            if single:\n                img = images[0]\n            else:\n                img = images[i]\n\n            # uncrop the image based on the bounding box\n            bb_x, bb_y, bb_width, bb_height = bbox\n\n            paste_region = bbox_to_region((bb_x, bb_y, bb_width, bb_height), img.size)\n            # log.debug(f\"Paste region: {paste_region}\")\n            # new_region = adjust_paste_region(img.size, paste_region)\n            # log.debug(f\"Adjusted paste region: {new_region}\")\n            # # Check if the adjusted paste region is different from the original\n\n            crop_img = crop.convert(\"RGB\")\n\n            log.debug(f\"Crop image size: {crop_img.size}\")\n            log.debug(f\"Image size: {img.size}\")\n\n            if border_blending > 1.0:\n                border_blending = 1.0\n            elif border_blending < 0.0:\n                border_blending = 0.0\n\n            blend_ratio = (max(crop_img.size) / 2) * float(border_blending)\n\n            blend = img.convert(\"RGBA\")\n            mask = Image.new(\"L\", img.size, 0)\n\n            mask_block = Image.new(\"L\", (bb_width, bb_height), 255)\n            mask_block = inset_border(mask_block, int(blend_ratio / 2), (0))\n\n            mask.paste(mask_block, paste_region)\n            log.debug(f\"Blend size: {blend.size} | kind {blend.mode}\")\n            log.debug(f\"Crop image size: {crop_img.size} | kind {crop_img.mode}\")\n            log.debug(f\"BBox: {paste_region}\")\n            blend.paste(crop_img, paste_region)\n\n            mask = mask.filter(ImageFilter.BoxBlur(radius=blend_ratio / 4))\n            mask = mask.filter(ImageFilter.GaussianBlur(radius=blend_ratio / 4))\n\n            blend.putalpha(mask)\n            img = Image.alpha_composite(img.convert(\"RGBA\"), blend)\n            out_images.append(img.convert(\"RGB\"))\n\n        return (pil2tensor(out_images),)", "\n\n__nodes__ = [BboxFromMask, Bbox, Crop, Uncrop]\n"]}
{"filename": "nodes/image_processing.py", "chunked_list": ["import torch\nfrom skimage.filters import gaussian\nfrom skimage.util import compare_images\nimport numpy as np\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom ..utils import tensor2pil, pil2tensor, tensor2np\nimport torch\nimport folder_paths\nfrom PIL.PngImagePlugin import PngInfo", "import folder_paths\nfrom PIL.PngImagePlugin import PngInfo\nimport json\nimport os\nimport math\n\n\n# try:\n#     from cv2.ximgproc import guidedFilter\n# except ImportError:", "#     from cv2.ximgproc import guidedFilter\n# except ImportError:\n#     log.warning(\"cv2.ximgproc.guidedFilter not found, use opencv-contrib-python\")\n\n\nclass ColorCorrect:\n    \"\"\"Various color correction methods\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"clamp\": ([True, False], {\"default\": True}),\n                \"gamma\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.0, \"max\": 5.0, \"step\": 0.01},\n                ),\n                \"contrast\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.0, \"max\": 5.0, \"step\": 0.01},\n                ),\n                \"exposure\": (\n                    \"FLOAT\",\n                    {\"default\": 0.0, \"min\": -5.0, \"max\": 5.0, \"step\": 0.01},\n                ),\n                \"offset\": (\n                    \"FLOAT\",\n                    {\"default\": 0.0, \"min\": -5.0, \"max\": 5.0, \"step\": 0.01},\n                ),\n                \"hue\": (\n                    \"FLOAT\",\n                    {\"default\": 0.0, \"min\": -0.5, \"max\": 0.5, \"step\": 0.01},\n                ),\n                \"saturation\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.0, \"max\": 5.0, \"step\": 0.01},\n                ),\n                \"value\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.0, \"max\": 5.0, \"step\": 0.01},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"correct\"\n    CATEGORY = \"mtb/image processing\"\n\n    @staticmethod\n    def gamma_correction_tensor(image, gamma):\n        gamma_inv = 1.0 / gamma\n        return image.pow(gamma_inv)\n\n    @staticmethod\n    def contrast_adjustment_tensor(image, contrast):\n        contrasted = (image - 0.5) * contrast + 0.5\n        return torch.clamp(contrasted, 0.0, 1.0)\n\n    @staticmethod\n    def exposure_adjustment_tensor(image, exposure):\n        return image * (2.0**exposure)\n\n    @staticmethod\n    def offset_adjustment_tensor(image, offset):\n        return image + offset\n\n    @staticmethod\n    def hsv_adjustment(image: torch.Tensor, hue, saturation, value):\n        images = tensor2pil(image)\n        out = []\n        for img in images:\n            hsv_image = img.convert(\"HSV\")\n\n            h, s, v = hsv_image.split()\n\n            h = h.point(lambda x: (x + hue * 255) % 256)\n            s = s.point(lambda x: int(x * saturation))\n            v = v.point(lambda x: int(x * value))\n\n            hsv_image = Image.merge(\"HSV\", (h, s, v))\n            rgb_image = hsv_image.convert(\"RGB\")\n            out.append(rgb_image)\n        return pil2tensor(out)\n\n    @staticmethod\n    def hsv_adjustment_tensor_not_working(image: torch.Tensor, hue, saturation, value):\n        \"\"\"Abandonning for now\"\"\"\n        image = image.squeeze(0).permute(2, 0, 1)\n\n        max_val, _ = image.max(dim=0, keepdim=True)\n        min_val, _ = image.min(dim=0, keepdim=True)\n        delta = max_val - min_val\n\n        hue_image = torch.zeros_like(max_val)\n        mask = delta != 0.0\n\n        r, g, b = image[0], image[1], image[2]\n        hue_image[mask & (max_val == r)] = ((g - b) / delta)[\n            mask & (max_val == r)\n        ] % 6.0\n        hue_image[mask & (max_val == g)] = ((b - r) / delta)[\n            mask & (max_val == g)\n        ] + 2.0\n        hue_image[mask & (max_val == b)] = ((r - g) / delta)[\n            mask & (max_val == b)\n        ] + 4.0\n\n        saturation_image = delta / (max_val + 1e-7)\n        value_image = max_val\n\n        hue_image = (hue_image + hue) % 1.0\n        saturation_image = torch.where(\n            mask, saturation * saturation_image, saturation_image\n        )\n        value_image = value * value_image\n\n        c = value_image * saturation_image\n        x = c * (1 - torch.abs((hue_image % 2) - 1))\n        m = value_image - c\n\n        prime_image = torch.zeros_like(image)\n        prime_image[0] = torch.where(\n            max_val == r, c, torch.where(max_val == g, x, prime_image[0])\n        )\n        prime_image[1] = torch.where(\n            max_val == r, x, torch.where(max_val == g, c, prime_image[1])\n        )\n        prime_image[2] = torch.where(\n            max_val == g, x, torch.where(max_val == b, c, prime_image[2])\n        )\n\n        rgb_image = prime_image + m\n\n        rgb_image = rgb_image.permute(1, 2, 0).unsqueeze(0)\n\n        return rgb_image\n\n    def correct(\n        self,\n        image: torch.Tensor,\n        clamp: bool,\n        gamma: float = 1.0,\n        contrast: float = 1.0,\n        exposure: float = 0.0,\n        offset: float = 0.0,\n        hue: float = 0.0,\n        saturation: float = 1.0,\n        value: float = 1.0,\n    ):\n        # Apply color correction operations\n        image = self.gamma_correction_tensor(image, gamma)\n        image = self.contrast_adjustment_tensor(image, contrast)\n        image = self.exposure_adjustment_tensor(image, exposure)\n        image = self.offset_adjustment_tensor(image, offset)\n        image = self.hsv_adjustment(image, hue, saturation, value)\n\n        if clamp:\n            image = torch.clamp(image, 0.0, 1.0)\n\n        return (image,)", "\n\nclass ImageCompare:\n    \"\"\"Compare two images and return a difference image\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"imageA\": (\"IMAGE\",),\n                \"imageB\": (\"IMAGE\",),\n                \"mode\": (\n                    [\"checkerboard\", \"diff\", \"blend\"],\n                    {\"default\": \"checkerboard\"},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"compare\"\n    CATEGORY = \"mtb/image\"\n\n    def compare(self, imageA: torch.Tensor, imageB: torch.Tensor, mode):\n        imageA = imageA.numpy()\n        imageB = imageB.numpy()\n\n        imageA = imageA.squeeze()\n        imageB = imageB.squeeze()\n\n        image = compare_images(imageA, imageB, method=mode)\n\n        image = np.expand_dims(image, axis=0)\n        return (torch.from_numpy(image),)", "\n\nimport requests\n\n\nclass LoadImageFromUrl:\n    \"\"\"Load an image from the given URL\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"url\": (\n                    \"STRING\",\n                    {\n                        \"default\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Example.jpg/800px-Example.jpg\"\n                    },\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"load\"\n    CATEGORY = \"mtb/IO\"\n\n    def load(self, url):\n        # get the image from the url\n        image = Image.open(requests.get(url, stream=True).raw)\n        return (pil2tensor(image),)", "\n\nclass Blur:\n    \"\"\"Blur an image using a Gaussian filter.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"sigmaX\": (\n                    \"FLOAT\",\n                    {\"default\": 3.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01},\n                ),\n                \"sigmaY\": (\n                    \"FLOAT\",\n                    {\"default\": 3.0, \"min\": 0.0, \"max\": 10.0, \"step\": 0.01},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"blur\"\n    CATEGORY = \"mtb/image processing\"\n\n    def blur(self, image: torch.Tensor, sigmaX, sigmaY):\n        image = image.numpy()\n        image = image.transpose(1, 2, 3, 0)\n        image = gaussian(image, sigma=(sigmaX, sigmaY, 0, 0))\n        image = image.transpose(3, 0, 1, 2)\n        return (torch.from_numpy(image),)", "\n\n# https://github.com/lllyasviel/AdverseCleaner/blob/main/clean.py\n# def deglaze_np_img(np_img):\n#     y = np_img.copy()\n#     for _ in range(64):\n#         y = cv2.bilateralFilter(y, 5, 8, 8)\n#     for _ in range(4):\n#         y = guidedFilter(np_img, y, 4, 16)\n#     return y", "#         y = guidedFilter(np_img, y, 4, 16)\n#     return y\n\n\n# class DeglazeImage:\n#     \"\"\"Remove adversarial noise from images\"\"\"\n\n#     @classmethod\n#     def INPUT_TYPES(cls):\n#         return {\"required\": {\"image\": (\"IMAGE\",)}}", "#     def INPUT_TYPES(cls):\n#         return {\"required\": {\"image\": (\"IMAGE\",)}}\n\n#     CATEGORY = \"mtb/image processing\"\n\n#     RETURN_TYPES = (\"IMAGE\",)\n#     FUNCTION = \"deglaze_image\"\n\n#     def deglaze_image(self, image):\n#         return (np2tensor(deglaze_np_img(tensor2np(image))),)", "#     def deglaze_image(self, image):\n#         return (np2tensor(deglaze_np_img(tensor2np(image))),)\n\n\nclass MaskToImage:\n    \"\"\"Converts a mask (alpha) to an RGB image with a color and background\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"mask\": (\"MASK\",),\n                \"color\": (\"COLOR\",),\n                \"background\": (\"COLOR\", {\"default\": \"#000000\"}),\n            }\n        }\n\n    CATEGORY = \"mtb/generate\"\n\n    RETURN_TYPES = (\"IMAGE\",)\n\n    FUNCTION = \"render_mask\"\n\n    def render_mask(self, mask, color, background):\n        mask = tensor2np(mask)\n        mask = Image.fromarray(mask).convert(\"L\")\n\n        image = Image.new(\"RGBA\", mask.size, color=color)\n        # apply the mask\n        image = Image.composite(\n            image, Image.new(\"RGBA\", mask.size, color=background), mask\n        )\n\n        # image = ImageChops.multiply(image, mask)\n        # apply over background\n        # image = Image.alpha_composite(Image.new(\"RGBA\", image.size, color=background), image)\n\n        image = pil2tensor(image.convert(\"RGB\"))\n\n        return (image,)", "\n\nclass ColoredImage:\n    \"\"\"Constant color image of given size\"\"\"\n\n    def __init__(self) -> None:\n        pass\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"color\": (\"COLOR\",),\n                \"width\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": 8160}),\n                \"height\": (\"INT\", {\"default\": 512, \"min\": 16, \"max\": 8160}),\n            }\n        }\n\n    CATEGORY = \"mtb/generate\"\n\n    RETURN_TYPES = (\"IMAGE\",)\n\n    FUNCTION = \"render_img\"\n\n    def render_img(self, color, width, height):\n        image = Image.new(\"RGB\", (width, height), color=color)\n\n        image = pil2tensor(image)\n\n        return (image,)", "\n\nclass ImagePremultiply:\n    \"\"\"Premultiply image with mask\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"mask\": (\"MASK\",),\n                \"invert\": (\"BOOLEAN\", {\"default\": False}),\n            }\n        }\n\n    CATEGORY = \"mtb/image\"\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"premultiply\"\n\n    def premultiply(self, image, mask, invert):\n        images = tensor2pil(image)\n        if invert:\n            masks = tensor2pil(mask)  # .convert(\"L\")\n        else:\n            masks = tensor2pil(1.0 - mask)\n\n        single = False\n        if len(mask) == 1:\n            single = True\n\n        masks = [x.convert(\"L\") for x in masks]\n\n        out = []\n        for i, img in enumerate(images):\n            cur_mask = masks[0] if single else masks[i]\n\n            img.putalpha(cur_mask)\n            out.append(img)\n\n        # if invert:\n        #     image = Image.composite(image,Image.new(\"RGBA\", image.size, color=(0,0,0,0)), mask)\n        # else:\n        #     image = Image.composite(Image.new(\"RGBA\", image.size, color=(0,0,0,0)), image, mask)\n\n        return (pil2tensor(out),)", "\n\nclass ImageResizeFactor:\n    \"\"\"Extracted mostly from WAS Node Suite, with a few edits (most notably multiple image support) and less features.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"factor\": (\n                    \"FLOAT\",\n                    {\"default\": 2, \"min\": 0.01, \"max\": 16.0, \"step\": 0.01},\n                ),\n                \"supersample\": (\"BOOLEAN\", {\"default\": True}),\n                \"resampling\": (\n                    [\n                        \"nearest\",\n                        \"linear\",\n                        \"bilinear\",\n                        \"bicubic\",\n                        \"trilinear\",\n                        \"area\",\n                        \"nearest-exact\",\n                    ],\n                    {\"default\": \"nearest\"},\n                ),\n            },\n            \"optional\": {\n                \"mask\": (\"MASK\",),\n            },\n        }\n\n    CATEGORY = \"mtb/image\"\n    RETURN_TYPES = (\"IMAGE\", \"MASK\")\n    FUNCTION = \"resize\"\n\n    def resize(\n        self,\n        image: torch.Tensor,\n        factor: float,\n        supersample: bool,\n        resampling: str,\n        mask=None,\n    ):\n        # Check if the tensor has the correct dimension\n        if len(image.shape) not in [3, 4]:  # HxWxC or BxHxWxC\n            raise ValueError(\"Expected image tensor of shape (H, W, C) or (B, H, W, C)\")\n\n        # Transpose to CxHxW or BxCxHxW for PyTorch\n        if len(image.shape) == 3:\n            image = image.permute(2, 0, 1).unsqueeze(0)  # CxHxW\n        else:\n            image = image.permute(0, 3, 1, 2)  # BxCxHxW\n\n        # Compute new dimensions\n        B, C, H, W = image.shape\n        new_H, new_W = int(H * factor), int(W * factor)\n\n        align_corner_filters = (\"linear\", \"bilinear\", \"bicubic\", \"trilinear\")\n        # Resize the image\n        resized_image = F.interpolate(\n            image,\n            size=(new_H, new_W),\n            mode=resampling,\n            align_corners=resampling in align_corner_filters,\n        )\n\n        # Optionally supersample\n        if supersample:\n            resized_image = F.interpolate(\n                resized_image,\n                scale_factor=2,\n                mode=resampling,\n                align_corners=resampling in align_corner_filters,\n            )\n\n        # Transpose back to the original format: BxHxWxC or HxWxC\n        if len(image.shape) == 4:\n            resized_image = resized_image.permute(0, 2, 3, 1)\n        else:\n            resized_image = resized_image.squeeze(0).permute(1, 2, 0)\n\n        # Apply mask if provided\n        if mask is not None:\n            if len(mask.shape) != len(resized_image.shape):\n                raise ValueError(\n                    \"Mask tensor should have the same dimensions as the image tensor\"\n                )\n            resized_image = resized_image * mask\n\n        return (resized_image,)", "\n\nclass SaveImageGrid:\n    \"\"\"Save all the images in the input batch as a grid of images.\"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                \"filename_prefix\": (\"STRING\", {\"default\": \"ComfyUI\"}),\n                \"save_intermediate\": (\"BOOLEAN\", {\"default\": False}),\n            },\n            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n        }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"mtb/IO\"\n\n    def create_image_grid(self, image_list):\n        total_images = len(image_list)\n\n        # Calculate the grid size based on the square root of the total number of images\n        grid_size = (\n            int(math.sqrt(total_images)),\n            int(math.ceil(math.sqrt(total_images))),\n        )\n\n        # Get the size of the first image to determine the grid size\n        image_width, image_height = image_list[0].size\n\n        # Create a new blank image to hold the grid\n        grid_width = grid_size[0] * image_width\n        grid_height = grid_size[1] * image_height\n        grid_image = Image.new(\"RGB\", (grid_width, grid_height))\n\n        # Iterate over the images and paste them onto the grid\n        for i, image in enumerate(image_list):\n            x = (i % grid_size[0]) * image_width\n            y = (i // grid_size[0]) * image_height\n            grid_image.paste(image, (x, y, x + image_width, y + image_height))\n\n        return grid_image\n\n    def save_images(\n        self,\n        images,\n        filename_prefix=\"Grid\",\n        save_intermediate=False,\n        prompt=None,\n        extra_pnginfo=None,\n    ):\n        (\n            full_output_folder,\n            filename,\n            counter,\n            subfolder,\n            filename_prefix,\n        ) = folder_paths.get_save_image_path(\n            filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0]\n        )\n        image_list = []\n        batch_counter = counter\n\n        metadata = PngInfo()\n        if prompt is not None:\n            metadata.add_text(\"prompt\", json.dumps(prompt))\n        if extra_pnginfo is not None:\n            for x in extra_pnginfo:\n                metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\n        for idx, image in enumerate(images):\n            i = 255.0 * image.cpu().numpy()\n            img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n            image_list.append(img)\n\n            if save_intermediate:\n                file = f\"{filename}_batch-{idx:03}_{batch_counter:05}_.png\"\n                img.save(\n                    os.path.join(full_output_folder, file),\n                    pnginfo=metadata,\n                    compress_level=4,\n                )\n\n            batch_counter += 1\n\n        file = f\"{filename}_{counter:05}_.png\"\n        grid = self.create_image_grid(image_list)\n        grid.save(\n            os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=4\n        )\n\n        results = [{\"filename\": file, \"subfolder\": subfolder, \"type\": self.type}]\n        return {\"ui\": {\"images\": results}}", "\n\n__nodes__ = [\n    ColorCorrect,\n    ImageCompare,\n    Blur,\n    # DeglazeImage,\n    MaskToImage,\n    ColoredImage,\n    ImagePremultiply,", "    ColoredImage,\n    ImagePremultiply,\n    ImageResizeFactor,\n    SaveImageGrid,\n    LoadImageFromUrl,\n]\n"]}
{"filename": "nodes/mask.py", "chunked_list": ["from rembg import remove\nfrom ..utils import pil2tensor, tensor2pil\nfrom PIL import Image\nimport comfy.utils\n\n\nclass ImageRemoveBackgroundRembg:\n    \"\"\"Removes the background from the input using Rembg.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"alpha_matting\": (\n                    \"BOOLEAN\",\n                    {\"default\": False},\n                ),\n                \"alpha_matting_foreground_threshold\": (\n                    \"INT\",\n                    {\"default\": 240, \"min\": 0, \"max\": 255},\n                ),\n                \"alpha_matting_background_threshold\": (\n                    \"INT\",\n                    {\"default\": 10, \"min\": 0, \"max\": 255},\n                ),\n                \"alpha_matting_erode_size\": (\n                    \"INT\",\n                    {\"default\": 10, \"min\": 0, \"max\": 255},\n                ),\n                \"post_process_mask\": (\n                    \"BOOLEAN\",\n                    {\"default\": False},\n                ),\n                \"bgcolor\": (\n                    \"COLOR\",\n                    {\"default\": \"#000000\"},\n                ),\n            },\n        }\n\n    RETURN_TYPES = (\n        \"IMAGE\",\n        \"MASK\",\n        \"IMAGE\",\n    )\n    RETURN_NAMES = (\n        \"Image (rgba)\",\n        \"Mask\",\n        \"Image\",\n    )\n    FUNCTION = \"remove_background\"\n    CATEGORY = \"mtb/image\"\n\n    # bgcolor: Optional[Tuple[int, int, int, int]]\n    def remove_background(\n        self,\n        image,\n        alpha_matting,\n        alpha_matting_foreground_threshold,\n        alpha_matting_background_threshold,\n        alpha_matting_erode_size,\n        post_process_mask,\n        bgcolor,\n    ):\n        pbar = comfy.utils.ProgressBar(image.size(0))\n        images = tensor2pil(image)\n\n        out_img = []\n        out_mask = []\n        out_img_on_bg = []\n\n        for img in images:\n            img_rm = remove(\n                data=img,\n                alpha_matting=alpha_matting,\n                alpha_matting_foreground_threshold=alpha_matting_foreground_threshold,\n                alpha_matting_background_threshold=alpha_matting_background_threshold,\n                alpha_matting_erode_size=alpha_matting_erode_size,\n                session=None,\n                only_mask=False,\n                post_process_mask=post_process_mask,\n                bgcolor=None,\n            )\n\n            # extract the alpha to a new image\n            mask = img_rm.getchannel(3)\n\n            # add our bgcolor behind the image\n            image_on_bg = Image.new(\"RGBA\", img_rm.size, bgcolor)\n\n            image_on_bg.paste(img_rm, mask=mask)\n\n            out_img.append(img_rm)\n            out_mask.append(mask)\n            out_img_on_bg.append(image_on_bg)\n\n            pbar.update(1)\n\n        return (pil2tensor(out_img), pil2tensor(out_mask), pil2tensor(out_img_on_bg))", "\n\n__nodes__ = [\n    ImageRemoveBackgroundRembg,\n]\n"]}
{"filename": "nodes/graph_utils.py", "chunked_list": ["from ..log import log\nfrom PIL import Image\nimport urllib.request\nimport urllib.parse\nimport torch\nimport json\nfrom ..utils import pil2tensor, apply_easing, get_server_info\nimport io\nimport numpy as np\n", "import numpy as np\n\n\ndef get_image(filename, subfolder, folder_type):\n    log.debug(\n        f\"Getting image {filename} from foldertype {folder_type} {f'in subfolder: {subfolder}' if subfolder else ''}\"\n    )\n    data = {\"filename\": filename, \"subfolder\": subfolder, \"type\": folder_type}\n    base_url, port = get_server_info()\n\n    url_values = urllib.parse.urlencode(data)\n    url = f\"http://{base_url}:{port}/view?{url_values}\"\n    log.debug(f\"Fetching image from {url}\")\n    with urllib.request.urlopen(url) as response:\n        return io.BytesIO(response.read())", "\n\nclass GetBatchFromHistory:\n    \"\"\"Very experimental node to load images from the history of the server.\n\n    Queue items without output are ignored in the count.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"enable\": (\"BOOLEAN\", {\"default\": True}),\n                \"count\": (\"INT\", {\"default\": 1, \"min\": 0}),\n                \"offset\": (\"INT\", {\"default\": 0, \"min\": -1e9, \"max\": 1e9}),\n                \"internal_count\": (\"INT\", {\"default\": 0}),\n            },\n            \"optional\": {\n                \"passthrough_image\": (\"IMAGE\",),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"images\",)\n    CATEGORY = \"mtb/animation\"\n    FUNCTION = \"load_from_history\"\n\n    def load_from_history(\n        self,\n        enable=True,\n        count=0,\n        offset=0,\n        internal_count=0,  # hacky way to invalidate the node\n        passthrough_image=None,\n    ):\n        if not enable or count == 0:\n            if passthrough_image is not None:\n                log.debug(\"Using passthrough image\")\n                return (passthrough_image,)\n            log.debug(\"Load from history is disabled for this iteration\")\n            return (torch.zeros(0),)\n        frames = []\n\n        base_url, port = get_server_info()\n\n        history_url = f\"http://{base_url}:{port}/history\"\n        log.debug(f\"Fetching history from {history_url}\")\n        output = torch.zeros(0)\n        with urllib.request.urlopen(history_url) as response:\n            output = self.load_batch_frames(response, offset, count, frames)\n\n        if output.size(0) == 0:\n            log.warn(\"No output found in history\")\n\n        return (output,)\n\n    def load_batch_frames(self, response, offset, count, frames):\n        history = json.loads(response.read())\n\n        output_images = []\n\n        for run in history.values():\n            for node_output in run[\"outputs\"].values():\n                if \"images\" in node_output:\n                    for image in node_output[\"images\"]:\n                        image_data = get_image(\n                            image[\"filename\"], image[\"subfolder\"], image[\"type\"]\n                        )\n                        output_images.append(image_data)\n\n        if not output_images:\n            return torch.zeros(0)\n\n        # Directly get desired range of images\n        start_index = max(len(output_images) - offset - count, 0)\n        end_index = len(output_images) - offset\n        selected_images = output_images[start_index:end_index]\n\n        frames = [Image.open(image) for image in selected_images]\n\n        if not frames:\n            return torch.zeros(0)\n        elif len(frames) != count:\n            log.warning(f\"Expected {count} images, got {len(frames)} instead\")\n\n        return pil2tensor(frames)", "\n\nclass AnyToString:\n    \"\"\"Tries to take any input and convert it to a string\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\"input\": (\"*\")},\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"do_str\"\n    CATEGORY = \"mtb/converters\"\n\n    def do_str(self, input):\n        if isinstance(input, str):\n            return (input,)\n        elif isinstance(input, torch.Tensor):\n            return (f\"Tensor of shape {input.shape} and dtype {input.dtype}\",)\n        elif isinstance(input, Image.Image):\n            return (f\"PIL Image of size {input.size} and mode {input.mode}\",)\n        elif isinstance(input, np.ndarray):\n            return (f\"Numpy array of shape {input.shape} and dtype {input.dtype}\",)\n\n        elif isinstance(input, dict):\n            return (f\"Dictionary of {len(input)} items, with keys {input.keys()}\",)\n\n        else:\n            log.debug(f\"Falling back to string conversion of {input}\")\n            return (str(input),)", "\n\nclass StringReplace:\n    \"\"\"Basic string replacement\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"string\": (\"STRING\", {\"forceInput\": True}),\n                \"old\": (\"STRING\", {\"default\": \"\"}),\n                \"new\": (\"STRING\", {\"default\": \"\"}),\n            }\n        }\n\n    FUNCTION = \"replace_str\"\n    RETURN_TYPES = (\"STRING\",)\n    CATEGORY = \"mtb/string\"\n\n    def replace_str(self, string: str, old: str, new: str):\n        log.debug(f\"Current string: {string}\")\n        log.debug(f\"Find string: {old}\")\n        log.debug(f\"Replace string: {new}\")\n\n        string = string.replace(old, new)\n\n        log.debug(f\"New string: {string}\")\n\n        return (string,)", "\n\nclass FitNumber:\n    \"\"\"Fit the input float using a source and target range\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"value\": (\"FLOAT\", {\"default\": 0, \"forceInput\": True}),\n                \"clamp\": (\"BOOLEAN\", {\"default\": False}),\n                \"source_min\": (\"FLOAT\", {\"default\": 0.0}),\n                \"source_max\": (\"FLOAT\", {\"default\": 1.0}),\n                \"target_min\": (\"FLOAT\", {\"default\": 0.0}),\n                \"target_max\": (\"FLOAT\", {\"default\": 1.0}),\n                \"easing\": (\n                    [\n                        \"Linear\",\n                        \"Sine In\",\n                        \"Sine Out\",\n                        \"Sine In/Out\",\n                        \"Quart In\",\n                        \"Quart Out\",\n                        \"Quart In/Out\",\n                        \"Cubic In\",\n                        \"Cubic Out\",\n                        \"Cubic In/Out\",\n                        \"Circ In\",\n                        \"Circ Out\",\n                        \"Circ In/Out\",\n                        \"Back In\",\n                        \"Back Out\",\n                        \"Back In/Out\",\n                        \"Elastic In\",\n                        \"Elastic Out\",\n                        \"Elastic In/Out\",\n                        \"Bounce In\",\n                        \"Bounce Out\",\n                        \"Bounce In/Out\",\n                    ],\n                    {\"default\": \"Linear\"},\n                ),\n            }\n        }\n\n    FUNCTION = \"set_range\"\n    RETURN_TYPES = (\"FLOAT\",)\n    CATEGORY = \"mtb/math\"\n\n    def set_range(\n        self,\n        value: float,\n        clamp: bool,\n        source_min: float,\n        source_max: float,\n        target_min: float,\n        target_max: float,\n        easing: str,\n    ):\n        normalized_value = (value - source_min) / (source_max - source_min)\n\n        eased_value = apply_easing(normalized_value, easing)\n\n        # - Convert the eased value to the target range\n        res = target_min + (target_max - target_min) * eased_value\n\n        if clamp:\n            if target_min > target_max:\n                res = max(min(res, target_min), target_max)\n            else:\n                res = max(min(res, target_max), target_min)\n\n        return (res,)", "\n\n__nodes__ = [StringReplace, FitNumber, GetBatchFromHistory, AnyToString]\n"]}
{"filename": "nodes/number.py", "chunked_list": ["class IntToBool:\n    \"\"\"Basic int to bool conversion\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"int\": (\n                    \"INT\",\n                    {\n                        \"default\": 0,\n                    },\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"BOOLEAN\",)\n    FUNCTION = \"int_to_bool\"\n    CATEGORY = \"mtb/number\"\n\n    def int_to_bool(self, int):\n        return (bool(int),)", "\n\nclass IntToNumber:\n    \"\"\"Node addon for the WAS Suite. Converts a \"comfy\" INT to a NUMBER.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"int\": (\n                    \"INT\",\n                    {\n                        \"default\": 0,\n                        \"min\": -1e9,\n                        \"max\": 1e9,\n                        \"step\": 1,\n                        \"forceInput\": True,\n                    },\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"NUMBER\",)\n    FUNCTION = \"int_to_number\"\n    CATEGORY = \"mtb/number\"\n\n    def int_to_number(self, int):\n        return (int,)", "\n\nclass FloatToNumber:\n    \"\"\"Node addon for the WAS Suite. Converts a \"comfy\" FLOAT to a NUMBER.\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"float\": (\n                    \"FLOAT\",\n                    {\n                        \"default\": 0,\n                        \"min\": -1e9,\n                        \"max\": 1e9,\n                        \"step\": 1,\n                        \"forceInput\": True,\n                    },\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"NUMBER\",)\n    FUNCTION = \"float_to_number\"\n    CATEGORY = \"mtb/number\"\n\n    def float_to_number(self, float):\n        return (float,)\n\n        return (int,)", "\n\n__nodes__ = [\n    FloatToNumber,\n    IntToBool,\n    IntToNumber,\n]\n"]}
{"filename": "nodes/io.py", "chunked_list": ["from ..utils import tensor2np, PIL_FILTER_MAP\nimport uuid\nimport folder_paths\nfrom ..log import log\nimport comfy.model_management as model_management\nimport subprocess\nimport torch\nfrom pathlib import Path\nimport numpy as np\nfrom PIL import Image", "import numpy as np\nfrom PIL import Image\nfrom typing import Optional, List\n\n\nclass ExportWithFfmpeg:\n    \"\"\"Export with FFmpeg (Experimental)\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                # \"frames\": (\"FRAMES\",),\n                \"fps\": (\"FLOAT\", {\"default\": 24, \"min\": 1}),\n                \"prefix\": (\"STRING\", {\"default\": \"export\"}),\n                \"format\": ([\"mov\", \"mp4\", \"mkv\", \"avi\"], {\"default\": \"mov\"}),\n                \"codec\": (\n                    [\"prores_ks\", \"libx264\", \"libx265\"],\n                    {\"default\": \"prores_ks\"},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"VIDEO\",)\n    OUTPUT_NODE = True\n    FUNCTION = \"export_prores\"\n    CATEGORY = \"mtb/IO\"\n\n    def export_prores(\n        self,\n        images: torch.Tensor,\n        fps: float,\n        prefix: str,\n        format: str,\n        codec: str,\n    ):\n        if images.size(0) == 0:\n            return (\"\",)\n        output_dir = Path(folder_paths.get_output_directory())\n        pix_fmt = \"rgb48le\" if codec == \"prores_ks\" else \"yuv420p\"\n        file_ext = format\n        file_id = f\"{prefix}_{uuid.uuid4()}.{file_ext}\"\n\n        log.debug(f\"Exporting to {output_dir / file_id}\")\n\n        frames = tensor2np(images)\n        log.debug(f\"Frames type {type(frames[0])}\")\n        log.debug(f\"Exporting {len(frames)} frames\")\n\n        frames = [frame.astype(np.uint16) * 257 for frame in frames]\n\n        height, width, _ = frames[0].shape\n\n        out_path = (output_dir / file_id).as_posix()\n\n        # Prepare the FFmpeg command\n        command = [\n            \"ffmpeg\",\n            \"-y\",\n            \"-f\",\n            \"rawvideo\",\n            \"-vcodec\",\n            \"rawvideo\",\n            \"-s\",\n            f\"{width}x{height}\",\n            \"-pix_fmt\",\n            pix_fmt,\n            \"-r\",\n            str(fps),\n            \"-i\",\n            \"-\",\n            \"-c:v\",\n            codec,\n            \"-r\",\n            str(fps),\n            \"-y\",\n            out_path,\n        ]\n\n        process = subprocess.Popen(command, stdin=subprocess.PIPE)\n\n        for frame in frames:\n            model_management.throw_exception_if_processing_interrupted()\n            process.stdin.write(frame.tobytes())\n\n        process.stdin.close()\n        process.wait()\n\n        return (out_path,)", "\n\ndef prepare_animated_batch(\n    batch: torch.Tensor,\n    pingpong=False,\n    resize_by=1.0,\n    resample_filter: Optional[Image.Resampling] = None,\n    image_type=np.uint8,\n) -> List[Image.Image]:\n    images = tensor2np(batch)\n    images = [frame.astype(image_type) for frame in images]\n\n    height, width, _ = batch[0].shape\n\n    if pingpong:\n        reversed_frames = images[::-1]\n        images.extend(reversed_frames)\n    pil_images = [Image.fromarray(frame) for frame in images]\n\n    # Resize frames if necessary\n    if abs(resize_by - 1.0) > 1e-6:\n        new_width = int(width * resize_by)\n        new_height = int(height * resize_by)\n        pil_images_resized = [\n            frame.resize((new_width, new_height), resample=resample_filter)\n            for frame in pil_images\n        ]\n        pil_images = pil_images_resized\n\n    return pil_images", "\n\n# todo: deprecate for apng\nclass SaveGif:\n    \"\"\"Save the images from the batch as a GIF\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"image\": (\"IMAGE\",),\n                \"fps\": (\"INT\", {\"default\": 12, \"min\": 1, \"max\": 120}),\n                \"resize_by\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.1}),\n                \"optimize\": (\"BOOLEAN\", {\"default\": False}),\n                \"pingpong\": (\"BOOLEAN\", {\"default\": False}),\n            },\n            \"optional\": {\n                \"resample_filter\": (list(PIL_FILTER_MAP.keys()),),\n            },\n        }\n\n    RETURN_TYPES = ()\n    OUTPUT_NODE = True\n    CATEGORY = \"mtb/IO\"\n    FUNCTION = \"save_gif\"\n\n    def save_gif(\n        self,\n        image,\n        fps=12,\n        resize_by=1.0,\n        optimize=False,\n        pingpong=False,\n        resample_filter=None,\n    ):\n        if image.size(0) == 0:\n            return (\"\",)\n\n        if resample_filter is not None:\n            resample_filter = PIL_FILTER_MAP.get(resample_filter)\n\n        pil_images = prepare_animated_batch(\n            image,\n            pingpong,\n            resize_by,\n            resample_filter,\n        )\n\n        ruuid = uuid.uuid4()\n        ruuid = ruuid.hex[:10]\n        out_path = f\"{folder_paths.output_directory}/{ruuid}.gif\"\n\n        # Create the GIF from PIL images\n        pil_images[0].save(\n            out_path,\n            save_all=True,\n            append_images=pil_images[1:],\n            optimize=optimize,\n            duration=int(1000 / fps),\n            loop=0,\n        )\n\n        results = [{\"filename\": f\"{ruuid}.gif\", \"subfolder\": \"\", \"type\": \"output\"}]\n        return {\"ui\": {\"gif\": results}}", "\n\n__nodes__ = [SaveGif, ExportWithFfmpeg]\n"]}
{"filename": "nodes/debug.py", "chunked_list": ["from ..utils import tensor2pil\nfrom ..log import log\nimport io, base64\nimport torch\nimport folder_paths\nfrom typing import Optional\nfrom pathlib import Path\n\n\nclass Debug:\n    \"\"\"Experimental node to debug any Comfy values, support for more types and widgets is planned\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\"anything_1\": (\"*\")},\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"do_debug\"\n    CATEGORY = \"mtb/debug\"\n    OUTPUT_NODE = True\n\n    def do_debug(self, **kwargs):\n        output = {\n            \"ui\": {\"b64_images\": [], \"text\": []},\n            \"result\": (\"A\"),\n        }\n        for k, v in kwargs.items():\n            anything = v\n            text = \"\"\n            if isinstance(anything, torch.Tensor):\n                log.debug(f\"Tensor: {anything.shape}\")\n\n                # write the images to temp\n\n                image = tensor2pil(anything)\n                b64_imgs = []\n                for im in image:\n                    buffered = io.BytesIO()\n                    im.save(buffered, format=\"PNG\")\n                    b64_imgs.append(\n                        \"data:image/png;base64,\"\n                        + base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n                    )\n\n                output[\"ui\"][\"b64_images\"] += b64_imgs\n                log.debug(f\"Input {k} contains {len(b64_imgs)} images\")\n            elif isinstance(anything, bool):\n                log.debug(f\"Input {k} contains boolean: {anything}\")\n                output[\"ui\"][\"text\"] += [\"True\" if anything else \"False\"]\n            else:\n                text = str(anything)\n                log.debug(f\"Input {k} contains text: {text}\")\n                output[\"ui\"][\"text\"] += [text]\n\n        return output", "\nclass Debug:\n    \"\"\"Experimental node to debug any Comfy values, support for more types and widgets is planned\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\"anything_1\": (\"*\")},\n        }\n\n    RETURN_TYPES = (\"STRING\",)\n    FUNCTION = \"do_debug\"\n    CATEGORY = \"mtb/debug\"\n    OUTPUT_NODE = True\n\n    def do_debug(self, **kwargs):\n        output = {\n            \"ui\": {\"b64_images\": [], \"text\": []},\n            \"result\": (\"A\"),\n        }\n        for k, v in kwargs.items():\n            anything = v\n            text = \"\"\n            if isinstance(anything, torch.Tensor):\n                log.debug(f\"Tensor: {anything.shape}\")\n\n                # write the images to temp\n\n                image = tensor2pil(anything)\n                b64_imgs = []\n                for im in image:\n                    buffered = io.BytesIO()\n                    im.save(buffered, format=\"PNG\")\n                    b64_imgs.append(\n                        \"data:image/png;base64,\"\n                        + base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n                    )\n\n                output[\"ui\"][\"b64_images\"] += b64_imgs\n                log.debug(f\"Input {k} contains {len(b64_imgs)} images\")\n            elif isinstance(anything, bool):\n                log.debug(f\"Input {k} contains boolean: {anything}\")\n                output[\"ui\"][\"text\"] += [\"True\" if anything else \"False\"]\n            else:\n                text = str(anything)\n                log.debug(f\"Input {k} contains text: {text}\")\n                output[\"ui\"][\"text\"] += [text]\n\n        return output", "\n\nclass SaveTensors:\n    \"\"\"Save torch tensors (image, mask or latent) to disk, useful to debug things outside comfy\"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"mtb/debug\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"filename_prefix\": (\"STRING\", {\"default\": \"ComfyPickle\"}),\n            },\n            \"optional\": {\n                \"image\": (\"IMAGE\",),\n                \"mask\": (\"MASK\",),\n                \"latent\": (\"LATENT\",),\n            },\n        }\n\n    FUNCTION = \"save\"\n    OUTPUT_NODE = True\n    RETURN_TYPES = ()\n    CATEGORY = \"mtb/debug\"\n\n    def save(\n        self,\n        filename_prefix,\n        image: Optional[torch.Tensor] = None,\n        mask: Optional[torch.Tensor] = None,\n        latent: Optional[torch.Tensor] = None,\n    ):\n        (\n            full_output_folder,\n            filename,\n            counter,\n            subfolder,\n            filename_prefix,\n        ) = folder_paths.get_save_image_path(filename_prefix, self.output_dir)\n        full_output_folder = Path(full_output_folder)\n        if image is not None:\n            image_file = f\"{filename}_image_{counter:05}.pt\"\n            torch.save(image, full_output_folder / image_file)\n            # np.save(full_output_folder/ image_file, image.cpu().numpy())\n\n        if mask is not None:\n            mask_file = f\"{filename}_mask_{counter:05}.pt\"\n            torch.save(mask, full_output_folder / mask_file)\n            # np.save(full_output_folder/ mask_file, mask.cpu().numpy())\n\n        if latent is not None:\n            # for latent we must use pickle\n            latent_file = f\"{filename}_latent_{counter:05}.pt\"\n            torch.save(latent, full_output_folder / latent_file)\n            # pickle.dump(latent, open(full_output_folder/ latent_file, \"wb\"))\n\n            # np.save(full_output_folder/ latent_file, latent[\"\"].cpu().numpy())\n\n        return f\"{filename_prefix}_{counter:05}\"", "\n\n__nodes__ = [Debug, SaveTensors]\n"]}
{"filename": "nodes/conditions.py", "chunked_list": ["from ..utils import here\nfrom ..log import log\nimport folder_paths\nfrom pathlib import Path\nimport shutil\nimport csv\n\n\nclass InterpolateClipSequential:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"base_text\": (\"STRING\", {\"multiline\": True}),\n                \"text_to_replace\": (\"STRING\", {\"default\": \"\"}),\n                \"clip\": (\"CLIP\",),\n                \"interpolation_strength\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"interpolate_encodings_sequential\"\n\n    CATEGORY = \"mtb/conditioning\"\n\n    def interpolate_encodings_sequential(\n        self, base_text, text_to_replace, clip, interpolation_strength, **replacements\n    ):\n        log.debug(f\"Received interpolation_strength: {interpolation_strength}\")\n\n        # - Ensure interpolation strength is within [0, 1]\n        interpolation_strength = max(0.0, min(1.0, interpolation_strength))\n\n        # - Check if replacements were provided\n        if not replacements:\n            raise ValueError(\"At least one replacement should be provided.\")\n\n        num_replacements = len(replacements)\n        log.debug(f\"Number of replacements: {num_replacements}\")\n\n        segment_length = 1.0 / num_replacements\n        log.debug(f\"Calculated segment_length: {segment_length}\")\n\n        # - Find the segment that the interpolation_strength falls into\n        segment_index = min(\n            int(interpolation_strength // segment_length), num_replacements - 1\n        )\n        log.debug(f\"Segment index: {segment_index}\")\n\n        # - Calculate the local strength within the segment\n        local_strength = (\n            interpolation_strength - (segment_index * segment_length)\n        ) / segment_length\n        log.debug(f\"Local strength: {local_strength}\")\n\n        # - If it's the first segment, interpolate between base_text and the first replacement\n        if segment_index == 0:\n            replacement_text = list(replacements.values())[0]\n            log.debug(\"Using the base text a the base blend\")\n            # -  Start with the base_text condition\n            tokens = clip.tokenize(base_text)\n            cond_from, pooled_from = clip.encode_from_tokens(tokens, return_pooled=True)\n        else:\n            base_replace = list(replacements.values())[segment_index - 1]\n            log.debug(f\"Using {base_replace} a the base blend\")\n\n            # - Start with the base_text condition replaced by the closest replacement\n            tokens = clip.tokenize(base_text.replace(text_to_replace, base_replace))\n            cond_from, pooled_from = clip.encode_from_tokens(tokens, return_pooled=True)\n\n            replacement_text = list(replacements.values())[segment_index]\n\n        interpolated_text = base_text.replace(text_to_replace, replacement_text)\n        tokens = clip.tokenize(interpolated_text)\n        cond_to, pooled_to = clip.encode_from_tokens(tokens, return_pooled=True)\n\n        # - Linearly interpolate between the two conditions\n        interpolated_condition = (\n            1.0 - local_strength\n        ) * cond_from + local_strength * cond_to\n        interpolated_pooled = (\n            1.0 - local_strength\n        ) * pooled_from + local_strength * pooled_to\n\n        return ([[interpolated_condition, {\"pooled_output\": interpolated_pooled}]],)", "class InterpolateClipSequential:\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"base_text\": (\"STRING\", {\"multiline\": True}),\n                \"text_to_replace\": (\"STRING\", {\"default\": \"\"}),\n                \"clip\": (\"CLIP\",),\n                \"interpolation_strength\": (\n                    \"FLOAT\",\n                    {\"default\": 1.0, \"min\": 0.0, \"max\": 1.0, \"step\": 0.01},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"CONDITIONING\",)\n    FUNCTION = \"interpolate_encodings_sequential\"\n\n    CATEGORY = \"mtb/conditioning\"\n\n    def interpolate_encodings_sequential(\n        self, base_text, text_to_replace, clip, interpolation_strength, **replacements\n    ):\n        log.debug(f\"Received interpolation_strength: {interpolation_strength}\")\n\n        # - Ensure interpolation strength is within [0, 1]\n        interpolation_strength = max(0.0, min(1.0, interpolation_strength))\n\n        # - Check if replacements were provided\n        if not replacements:\n            raise ValueError(\"At least one replacement should be provided.\")\n\n        num_replacements = len(replacements)\n        log.debug(f\"Number of replacements: {num_replacements}\")\n\n        segment_length = 1.0 / num_replacements\n        log.debug(f\"Calculated segment_length: {segment_length}\")\n\n        # - Find the segment that the interpolation_strength falls into\n        segment_index = min(\n            int(interpolation_strength // segment_length), num_replacements - 1\n        )\n        log.debug(f\"Segment index: {segment_index}\")\n\n        # - Calculate the local strength within the segment\n        local_strength = (\n            interpolation_strength - (segment_index * segment_length)\n        ) / segment_length\n        log.debug(f\"Local strength: {local_strength}\")\n\n        # - If it's the first segment, interpolate between base_text and the first replacement\n        if segment_index == 0:\n            replacement_text = list(replacements.values())[0]\n            log.debug(\"Using the base text a the base blend\")\n            # -  Start with the base_text condition\n            tokens = clip.tokenize(base_text)\n            cond_from, pooled_from = clip.encode_from_tokens(tokens, return_pooled=True)\n        else:\n            base_replace = list(replacements.values())[segment_index - 1]\n            log.debug(f\"Using {base_replace} a the base blend\")\n\n            # - Start with the base_text condition replaced by the closest replacement\n            tokens = clip.tokenize(base_text.replace(text_to_replace, base_replace))\n            cond_from, pooled_from = clip.encode_from_tokens(tokens, return_pooled=True)\n\n            replacement_text = list(replacements.values())[segment_index]\n\n        interpolated_text = base_text.replace(text_to_replace, replacement_text)\n        tokens = clip.tokenize(interpolated_text)\n        cond_to, pooled_to = clip.encode_from_tokens(tokens, return_pooled=True)\n\n        # - Linearly interpolate between the two conditions\n        interpolated_condition = (\n            1.0 - local_strength\n        ) * cond_from + local_strength * cond_to\n        interpolated_pooled = (\n            1.0 - local_strength\n        ) * pooled_from + local_strength * pooled_to\n\n        return ([[interpolated_condition, {\"pooled_output\": interpolated_pooled}]],)", "\n\nclass SmartStep:\n    \"\"\"Utils to control the steps start/stop of the KAdvancedSampler in percentage\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"step\": (\n                    \"INT\",\n                    {\"default\": 20, \"min\": 1, \"max\": 10000, \"step\": 1},\n                ),\n                \"start_percent\": (\n                    \"INT\",\n                    {\"default\": 0, \"min\": 0, \"max\": 100, \"step\": 1},\n                ),\n                \"end_percent\": (\n                    \"INT\",\n                    {\"default\": 0, \"min\": 0, \"max\": 100, \"step\": 1},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"INT\", \"INT\", \"INT\")\n    RETURN_NAMES = (\"step\", \"start\", \"end\")\n    FUNCTION = \"do_step\"\n    CATEGORY = \"mtb/conditioning\"\n\n    def do_step(self, step, start_percent, end_percent):\n        start = int(step * start_percent / 100)\n        end = int(step * end_percent / 100)\n\n        return (step, start, end)", "\n\ndef install_default_styles(force=False):\n    styles_dir = Path(folder_paths.base_path) / \"styles\"\n    styles_dir.mkdir(parents=True, exist_ok=True)\n    default_style = here / \"styles.csv\"\n    dest_style = styles_dir / \"default.csv\"\n\n    if force or not dest_style.exists():\n        log.debug(f\"Copying default style to {dest_style}\")\n        shutil.copy2(default_style.as_posix(), dest_style.as_posix())\n\n    return dest_style", "\n\nclass StylesLoader:\n    \"\"\"Load csv files and populate a dropdown from the rows (\u00e0 la A111)\"\"\"\n\n    options = {}\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        if not cls.options:\n            input_dir = Path(folder_paths.base_path) / \"styles\"\n            if not input_dir.exists():\n                install_default_styles()\n\n            if not (files := [f for f in input_dir.iterdir() if f.suffix == \".csv\"]):\n                log.warn(\n                    \"No styles found in the styles folder, place at least one csv file in the styles folder at the root of ComfyUI (for instance ComfyUI/styles/mystyle.csv)\"\n                )\n\n            for file in files:\n                with open(file, \"r\", encoding=\"utf8\") as f:\n                    parsed = csv.reader(f)\n                    for row in parsed:\n                        log.debug(f\"Adding style {row[0]}\")\n                        cls.options[row[0]] = (row[1], row[2])\n\n        else:\n            log.debug(f\"Using cached styles (count: {len(cls.options)})\")\n\n        return {\n            \"required\": {\n                \"style_name\": (list(cls.options.keys()),),\n            }\n        }\n\n    CATEGORY = \"mtb/conditioning\"\n\n    RETURN_TYPES = (\"STRING\", \"STRING\")\n    RETURN_NAMES = (\"positive\", \"negative\")\n    FUNCTION = \"load_style\"\n\n    def load_style(self, style_name):\n        return (self.options[style_name][0], self.options[style_name][1])", "\n\n__nodes__ = [SmartStep, StylesLoader, InterpolateClipSequential]\n"]}
{"filename": "nodes/animation.py", "chunked_list": ["from ..log import log\n\n\nclass AnimationBuilder:\n    \"\"\"Convenient way to manage basic animation maths at the core of many of my workflows\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"total_frames\": (\"INT\", {\"default\": 100, \"min\": 0}),\n                # \"fps\": (\"INT\", {\"default\": 12, \"min\": 0}),\n                \"scale_float\": (\"FLOAT\", {\"default\": 1.0, \"min\": 0.0}),\n                \"loop_count\": (\"INT\", {\"default\": 1, \"min\": 0}),\n                \"raw_iteration\": (\"INT\", {\"default\": 0, \"min\": 0}),\n                \"raw_loop\": (\"INT\", {\"default\": 0, \"min\": 0}),\n            },\n        }\n\n    RETURN_TYPES = (\"INT\", \"FLOAT\", \"INT\", \"BOOLEAN\")\n    RETURN_NAMES = (\"frame\", \"0-1 (scaled)\", \"count\", \"loop_ended\")\n    CATEGORY = \"mtb/animation\"\n    FUNCTION = \"build_animation\"\n\n    def build_animation(\n        self,\n        total_frames=100,\n        # fps=12,\n        scale_float=1.0,\n        loop_count=1,  # set in js\n        raw_iteration=0,  # set in js\n        raw_loop=0,  # set in js\n    ):\n        frame = raw_iteration % (total_frames)\n        scaled = (frame / (total_frames - 1)) * scale_float\n        # if frame == 0:\n        #     log.debug(\"Reseting history\")\n        #     PromptServer.instance.prompt_queue.wipe_history()\n        log.debug(f\"frame: {frame}/{total_frames}  scaled: {scaled}\")\n\n        return (frame, scaled, raw_loop, (frame == (total_frames - 1)))", "\n\n__nodes__ = [AnimationBuilder]\n"]}
{"filename": "nodes/video.py", "chunked_list": ["import os\nimport re\nimport torch\nimport numpy as np\nimport hashlib\nfrom PIL import Image, ImageOps\nfrom PIL.PngImagePlugin import PngInfo\nimport folder_paths\nfrom pathlib import Path\nimport json", "from pathlib import Path\nimport json\n\nfrom ..log import log\n\n\nclass LoadImageSequence:\n    \"\"\"Load an image sequence from a folder. The current frame is used to determine which image to load.\n\n    Usually used in conjunction with the `Primitive` node set to increment to load a sequence of images from a folder.\n    Use -1 to load all matching frames as a batch.\n    \"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"path\": (\"STRING\", {\"default\": \"videos/####.png\"}),\n                \"current_frame\": (\n                    \"INT\",\n                    {\"default\": 0, \"min\": -1, \"max\": 9999999},\n                ),\n            }\n        }\n\n    CATEGORY = \"mtb/IO\"\n    FUNCTION = \"load_image\"\n    RETURN_TYPES = (\n        \"IMAGE\",\n        \"MASK\",\n        \"INT\",\n    )\n    RETURN_NAMES = (\n        \"image\",\n        \"mask\",\n        \"current_frame\",\n    )\n\n    def load_image(self, path=None, current_frame=0):\n        load_all = current_frame == -1\n\n        if load_all:\n            log.debug(f\"Loading all frames from {path}\")\n            frames = resolve_all_frames(path)\n            log.debug(f\"Found {len(frames)} frames\")\n\n            imgs = []\n            masks = []\n\n            for frame in frames:\n                img, mask = img_from_path(frame)\n                imgs.append(img)\n                masks.append(mask)\n\n            out_img = torch.cat(imgs, dim=0)\n            out_mask = torch.cat(masks, dim=0)\n\n            return (\n                out_img,\n                out_mask,\n            )\n\n        log.debug(f\"Loading image: {path}, {current_frame}\")\n        print(f\"Loading image: {path}, {current_frame}\")\n        resolved_path = resolve_path(path, current_frame)\n        image_path = folder_paths.get_annotated_filepath(resolved_path)\n        image, mask = img_from_path(image_path)\n        return (\n            image,\n            mask,\n            current_frame,\n        )\n\n    @staticmethod\n    def IS_CHANGED(path=\"\", current_frame=0):\n        print(f\"Checking if changed: {path}, {current_frame}\")\n        resolved_path = resolve_path(path, current_frame)\n        image_path = folder_paths.get_annotated_filepath(resolved_path)\n        if os.path.exists(image_path):\n            m = hashlib.sha256()\n            with open(image_path, \"rb\") as f:\n                m.update(f.read())\n            return m.digest().hex()\n        return \"NONE\"", "\n    # @staticmethod\n    # def VALIDATE_INPUTS(path=\"\", current_frame=0):\n\n    #     print(f\"Validating inputs: {path}, {current_frame}\")\n    #     resolved_path = resolve_path(path, current_frame)\n    #     if not folder_paths.exists_annotated_filepath(resolved_path):\n    #         return f\"Invalid image file: {resolved_path}\"\n    #     return True\n", "    #     return True\n\n\nimport glob\n\n\ndef img_from_path(path):\n    img = Image.open(path)\n    img = ImageOps.exif_transpose(img)\n    image = img.convert(\"RGB\")\n    image = np.array(image).astype(np.float32) / 255.0\n    image = torch.from_numpy(image)[None,]\n    if \"A\" in img.getbands():\n        mask = np.array(img.getchannel(\"A\")).astype(np.float32) / 255.0\n        mask = 1.0 - torch.from_numpy(mask)\n    else:\n        mask = torch.zeros((64, 64), dtype=torch.float32, device=\"cpu\")\n    return (\n        image,\n        mask,\n    )", "\n\ndef resolve_all_frames(pattern):\n    folder_path, file_pattern = os.path.split(pattern)\n\n    log.debug(f\"Resolving all frames in {folder_path}\")\n    frames = []\n    hash_count = file_pattern.count(\"#\")\n    frame_pattern = re.sub(r\"#+\", \"*\", file_pattern)\n\n    log.debug(f\"Found pattern: {frame_pattern}\")\n\n    matching_files = glob.glob(os.path.join(folder_path, frame_pattern))\n\n    log.debug(f\"Found {len(matching_files)} matching files\")\n\n    frame_regex = re.escape(file_pattern).replace(r\"\\#\", r\"(\\d+)\")\n\n    frame_number_regex = re.compile(frame_regex)\n\n    for file in matching_files:\n        match = frame_number_regex.search(file)\n        if match:\n            frame_number = match.group(1)\n            log.debug(f\"Found frame number: {frame_number}\")\n            # resolved_file = pattern.replace(\"*\" * frame_number.count(\"#\"), frame_number)\n            frames.append(file)\n\n    frames.sort()  # Sort frames alphabetically\n    return frames", "\n\ndef resolve_path(path, frame):\n    hashes = path.count(\"#\")\n    padded_number = str(frame).zfill(hashes)\n    return re.sub(\"#+\", padded_number, path)\n\n\nclass SaveImageSequence:\n    \"\"\"Save an image sequence to a folder. The current frame is used to determine which image to save.\n\n    This is merely a wrapper around the `save_images` function with formatting for the output folder and filename.\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                \"filename_prefix\": (\"STRING\", {\"default\": \"Sequence\"}),\n                \"current_frame\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 9999999}),\n            },\n            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n        }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"mtb/IO\"\n\n    def save_images(\n        self,\n        images,\n        filename_prefix=\"Sequence\",\n        current_frame=0,\n        prompt=None,\n        extra_pnginfo=None,\n    ):\n        # full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n        # results = list()\n        # for image in images:\n        #     i = 255. * image.cpu().numpy()\n        #     img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n        #     metadata = PngInfo()\n        #     if prompt is not None:\n        #         metadata.add_text(\"prompt\", json.dumps(prompt))\n        #     if extra_pnginfo is not None:\n        #         for x in extra_pnginfo:\n        #             metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\n        #     file = f\"{filename}_{counter:05}_.png\"\n        #     img.save(os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=4)\n        #     results.append({\n        #         \"filename\": file,\n        #         \"subfolder\": subfolder,\n        #         \"type\": self.type\n        #     })\n        #     counter += 1\n\n        if len(images) > 1:\n            raise ValueError(\"Can only save one image at a time\")\n\n        resolved_path = Path(self.output_dir) / filename_prefix\n        resolved_path.mkdir(parents=True, exist_ok=True)\n\n        resolved_img = resolved_path / f\"{filename_prefix}_{current_frame:05}.png\"\n\n        output_image = images[0].cpu().numpy()\n        img = Image.fromarray(np.clip(output_image * 255.0, 0, 255).astype(np.uint8))\n        metadata = PngInfo()\n        if prompt is not None:\n            metadata.add_text(\"prompt\", json.dumps(prompt))\n        if extra_pnginfo is not None:\n            for x in extra_pnginfo:\n                metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\n        img.save(resolved_img, pnginfo=metadata, compress_level=4)\n        return {\n            \"ui\": {\n                \"images\": [\n                    {\n                        \"filename\": resolved_img.name,\n                        \"subfolder\": resolved_path.name,\n                        \"type\": self.type,\n                    }\n                ]\n            }\n        }", "class SaveImageSequence:\n    \"\"\"Save an image sequence to a folder. The current frame is used to determine which image to save.\n\n    This is merely a wrapper around the `save_images` function with formatting for the output folder and filename.\n    \"\"\"\n\n    def __init__(self):\n        self.output_dir = folder_paths.get_output_directory()\n        self.type = \"output\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"images\": (\"IMAGE\",),\n                \"filename_prefix\": (\"STRING\", {\"default\": \"Sequence\"}),\n                \"current_frame\": (\"INT\", {\"default\": 0, \"min\": 0, \"max\": 9999999}),\n            },\n            \"hidden\": {\"prompt\": \"PROMPT\", \"extra_pnginfo\": \"EXTRA_PNGINFO\"},\n        }\n\n    RETURN_TYPES = ()\n    FUNCTION = \"save_images\"\n\n    OUTPUT_NODE = True\n\n    CATEGORY = \"mtb/IO\"\n\n    def save_images(\n        self,\n        images,\n        filename_prefix=\"Sequence\",\n        current_frame=0,\n        prompt=None,\n        extra_pnginfo=None,\n    ):\n        # full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir, images[0].shape[1], images[0].shape[0])\n        # results = list()\n        # for image in images:\n        #     i = 255. * image.cpu().numpy()\n        #     img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n        #     metadata = PngInfo()\n        #     if prompt is not None:\n        #         metadata.add_text(\"prompt\", json.dumps(prompt))\n        #     if extra_pnginfo is not None:\n        #         for x in extra_pnginfo:\n        #             metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\n        #     file = f\"{filename}_{counter:05}_.png\"\n        #     img.save(os.path.join(full_output_folder, file), pnginfo=metadata, compress_level=4)\n        #     results.append({\n        #         \"filename\": file,\n        #         \"subfolder\": subfolder,\n        #         \"type\": self.type\n        #     })\n        #     counter += 1\n\n        if len(images) > 1:\n            raise ValueError(\"Can only save one image at a time\")\n\n        resolved_path = Path(self.output_dir) / filename_prefix\n        resolved_path.mkdir(parents=True, exist_ok=True)\n\n        resolved_img = resolved_path / f\"{filename_prefix}_{current_frame:05}.png\"\n\n        output_image = images[0].cpu().numpy()\n        img = Image.fromarray(np.clip(output_image * 255.0, 0, 255).astype(np.uint8))\n        metadata = PngInfo()\n        if prompt is not None:\n            metadata.add_text(\"prompt\", json.dumps(prompt))\n        if extra_pnginfo is not None:\n            for x in extra_pnginfo:\n                metadata.add_text(x, json.dumps(extra_pnginfo[x]))\n\n        img.save(resolved_img, pnginfo=metadata, compress_level=4)\n        return {\n            \"ui\": {\n                \"images\": [\n                    {\n                        \"filename\": resolved_img.name,\n                        \"subfolder\": resolved_path.name,\n                        \"type\": self.type,\n                    }\n                ]\n            }\n        }", "\n\n__nodes__ = [\n    LoadImageSequence,\n    SaveImageSequence,\n]\n"]}
{"filename": "nodes/generate.py", "chunked_list": ["import qrcode\nfrom ..utils import pil2tensor\nfrom ..utils import comfy_dir\nfrom typing import cast\nfrom PIL import Image\nfrom ..log import log\n\n# class MtbExamples:\n#     \"\"\"MTB Example Images\"\"\"\n", "#     \"\"\"MTB Example Images\"\"\"\n\n#     def __init__(self):\n#         pass\n\n#     @classmethod\n#     @lru_cache(maxsize=1)\n#     def get_root(cls):\n#         return here / \"examples\" / \"samples\"\n", "#         return here / \"examples\" / \"samples\"\n\n#     @classmethod\n#     def INPUT_TYPES(cls):\n#         input_dir = cls.get_root()\n#         files = [f.name for f in input_dir.iterdir() if f.is_file()]\n#         return {\n#             \"required\": {\"image\": (sorted(files),)},\n#         }\n", "#         }\n\n#     RETURN_TYPES = (\"IMAGE\", \"MASK\")\n#     FUNCTION = \"do_mtb_examples\"\n#     CATEGORY = \"fun\"\n\n#     def do_mtb_examples(self, image, index):\n#         image_path = (self.get_root() / image).as_posix()\n\n#         i = Image.open(image_path)", "\n#         i = Image.open(image_path)\n#         i = ImageOps.exif_transpose(i)\n#         image = i.convert(\"RGB\")\n#         image = np.array(image).astype(np.float32) / 255.0\n#         image = torch.from_numpy(image)[None,]\n#         if \"A\" in i.getbands():\n#             mask = np.array(i.getchannel(\"A\")).astype(np.float32) / 255.0\n#             mask = 1.0 - torch.from_numpy(mask)\n#         else:", "#             mask = 1.0 - torch.from_numpy(mask)\n#         else:\n#             mask = torch.zeros((64, 64), dtype=torch.float32, device=\"cpu\")\n#         return (image, mask)\n\n#     @classmethod\n#     def IS_CHANGED(cls, image):\n#         image_path = (cls.get_root() / image).as_posix()\n\n#         m = hashlib.sha256()", "\n#         m = hashlib.sha256()\n#         with open(image_path, \"rb\") as f:\n#             m.update(f.read())\n#         return m.digest().hex()\n\n\nclass UnsplashImage:\n    \"\"\"Unsplash Image given a keyword and a size\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"width\": (\"INT\", {\"default\": 512, \"max\": 8096, \"min\": 0, \"step\": 1}),\n                \"height\": (\"INT\", {\"default\": 512, \"max\": 8096, \"min\": 0, \"step\": 1}),\n                \"random_seed\": (\"INT\", {\"default\": 0, \"max\": 1e5, \"min\": 0, \"step\": 1}),\n            },\n            \"optional\": {\n                \"keyword\": (\"STRING\", {\"default\": \"nature\"}),\n            },\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"do_unsplash_image\"\n    CATEGORY = \"mtb/generate\"\n\n    def do_unsplash_image(self, width, height, random_seed, keyword=None):\n        import requests\n        import io\n\n        base_url = \"https://source.unsplash.com/random/\"\n\n        if width and height:\n            base_url += f\"/{width}x{height}\"\n\n        if keyword:\n            keyword = keyword.replace(\" \", \"%20\")\n            base_url += f\"?{keyword}&{random_seed}\"\n        else:\n            base_url += f\"?&{random_seed}\"\n        try:\n            log.debug(f\"Getting unsplash image from {base_url}\")\n            response = requests.get(base_url)\n            response.raise_for_status()\n\n            image = Image.open(io.BytesIO(response.content))\n            return (\n                pil2tensor(\n                    image,\n                ),\n            )\n\n        except requests.exceptions.RequestException as e:\n            print(\"Error retrieving image:\", e)\n            return (None,)", "\n\nclass QrCode:\n    \"\"\"Basic QR Code generator\"\"\"\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        return {\n            \"required\": {\n                \"url\": (\"STRING\", {\"default\": \"https://www.github.com\"}),\n                \"width\": (\n                    \"INT\",\n                    {\"default\": 256, \"max\": 8096, \"min\": 0, \"step\": 1},\n                ),\n                \"height\": (\n                    \"INT\",\n                    {\"default\": 256, \"max\": 8096, \"min\": 0, \"step\": 1},\n                ),\n                \"error_correct\": ((\"L\", \"M\", \"Q\", \"H\"), {\"default\": \"L\"}),\n                \"box_size\": (\"INT\", {\"default\": 10, \"max\": 8096, \"min\": 0, \"step\": 1}),\n                \"border\": (\"INT\", {\"default\": 4, \"max\": 8096, \"min\": 0, \"step\": 1}),\n                \"invert\": ((\"BOOLEAN\",), {\"default\": False}),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    FUNCTION = \"do_qr\"\n    CATEGORY = \"mtb/generate\"\n\n    def do_qr(self, url, width, height, error_correct, box_size, border, invert):\n        log.warning(\n            \"This node will soon be deprecated, there are much better alternatives like https://github.com/coreyryanhanson/comfy-qr\"\n        )\n        if error_correct == \"L\" or error_correct not in [\"M\", \"Q\", \"H\"]:\n            error_correct = qrcode.constants.ERROR_CORRECT_L\n        elif error_correct == \"M\":\n            error_correct = qrcode.constants.ERROR_CORRECT_M\n        elif error_correct == \"Q\":\n            error_correct = qrcode.constants.ERROR_CORRECT_Q\n        else:\n            error_correct = qrcode.constants.ERROR_CORRECT_H\n\n        qr = qrcode.QRCode(\n            version=1,\n            error_correction=error_correct,\n            box_size=box_size,\n            border=border,\n        )\n        qr.add_data(url)\n        qr.make(fit=True)\n\n        back_color = (255, 255, 255) if invert else (0, 0, 0)\n        fill_color = (0, 0, 0) if invert else (255, 255, 255)\n\n        code = img = qr.make_image(back_color=back_color, fill_color=fill_color)\n\n        # that we now resize without filtering\n        code = code.resize((width, height), Image.NEAREST)\n\n        return (pil2tensor(code),)", "\n\ndef bbox_dim(bbox):\n    left, upper, right, lower = bbox\n    width = right - left\n    height = lower - upper\n    return width, height\n\n\nclass TextToImage:\n    \"\"\"Utils to convert text to image using a font\n\n\n    The tool looks for any .ttf file in the Comfy folder hierarchy.\n    \"\"\"\n\n    fonts = {}\n\n    def __init__(self):\n        # - This is executed when the graph is executed, we could conditionaly reload fonts there\n        pass\n\n    @classmethod\n    def CACHE_FONTS(cls):\n        font_extensions = [\"*.ttf\", \"*.otf\", \"*.woff\", \"*.woff2\", \"*.eot\"]\n        fonts = []\n\n        for extension in font_extensions:\n            fonts.extend(comfy_dir.glob(f\"**/{extension}\"))\n\n        if not fonts:\n            log.warn(\n                \"> No fonts found in the comfy folder, place at least one font file somewhere in ComfyUI's hierarchy\"\n            )\n        else:\n            log.debug(f\"> Found {len(fonts)} fonts\")\n\n        for font in fonts:\n            log.debug(f\"Adding font {font}\")\n            cls.fonts[font.stem] = font.as_posix()\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        if not cls.fonts:\n            cls.CACHE_FONTS()\n        else:\n            log.debug(f\"Using cached fonts (count: {len(cls.fonts)})\")\n        return {\n            \"required\": {\n                \"text\": (\n                    \"STRING\",\n                    {\"default\": \"Hello world!\"},\n                ),\n                \"font\": ((sorted(cls.fonts.keys())),),\n                \"wrap\": (\n                    \"INT\",\n                    {\"default\": 120, \"min\": 0, \"max\": 8096, \"step\": 1},\n                ),\n                \"font_size\": (\n                    \"INT\",\n                    {\"default\": 12, \"min\": 1, \"max\": 2500, \"step\": 1},\n                ),\n                \"width\": (\n                    \"INT\",\n                    {\"default\": 512, \"min\": 1, \"max\": 8096, \"step\": 1},\n                ),\n                \"height\": (\n                    \"INT\",\n                    {\"default\": 512, \"min\": 1, \"max\": 8096, \"step\": 1},\n                ),\n                # \"position\": ([\"INT\"], {\"default\": 0, \"min\": 0, \"max\": 100, \"step\": 1}),\n                \"color\": (\n                    \"COLOR\",\n                    {\"default\": \"black\"},\n                ),\n                \"background\": (\n                    \"COLOR\",\n                    {\"default\": \"white\"},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"image\",)\n    FUNCTION = \"text_to_image\"\n    CATEGORY = \"mtb/generate\"\n\n    def text_to_image(\n        self, text, font, wrap, font_size, width, height, color, background\n    ):\n        from PIL import Image, ImageDraw, ImageFont\n        import textwrap\n\n        font = self.fonts[font]\n        font = cast(ImageFont.FreeTypeFont, ImageFont.truetype(font, font_size))\n        if wrap == 0:\n            wrap = width / font_size\n        lines = textwrap.wrap(text, width=wrap)\n        log.debug(f\"Lines: {lines}\")\n        line_height = bbox_dim(font.getbbox(\"hg\"))[1]\n        img_height = height  # line_height * len(lines)\n        img_width = width  # max(font.getsize(line)[0] for line in lines)\n\n        img = Image.new(\"RGBA\", (img_width, img_height), background)\n        draw = ImageDraw.Draw(img)\n        y_text = 0\n        # - bbox is [left, upper, right, lower]\n        for line in lines:\n            width, height = bbox_dim(font.getbbox(line))\n            draw.text((0, y_text), line, color, font=font)\n            y_text += height\n\n        # img.save(os.path.join(folder_paths.base_path, f'{str(uuid.uuid4())}.png'))\n        return (pil2tensor(img),)", "\nclass TextToImage:\n    \"\"\"Utils to convert text to image using a font\n\n\n    The tool looks for any .ttf file in the Comfy folder hierarchy.\n    \"\"\"\n\n    fonts = {}\n\n    def __init__(self):\n        # - This is executed when the graph is executed, we could conditionaly reload fonts there\n        pass\n\n    @classmethod\n    def CACHE_FONTS(cls):\n        font_extensions = [\"*.ttf\", \"*.otf\", \"*.woff\", \"*.woff2\", \"*.eot\"]\n        fonts = []\n\n        for extension in font_extensions:\n            fonts.extend(comfy_dir.glob(f\"**/{extension}\"))\n\n        if not fonts:\n            log.warn(\n                \"> No fonts found in the comfy folder, place at least one font file somewhere in ComfyUI's hierarchy\"\n            )\n        else:\n            log.debug(f\"> Found {len(fonts)} fonts\")\n\n        for font in fonts:\n            log.debug(f\"Adding font {font}\")\n            cls.fonts[font.stem] = font.as_posix()\n\n    @classmethod\n    def INPUT_TYPES(cls):\n        if not cls.fonts:\n            cls.CACHE_FONTS()\n        else:\n            log.debug(f\"Using cached fonts (count: {len(cls.fonts)})\")\n        return {\n            \"required\": {\n                \"text\": (\n                    \"STRING\",\n                    {\"default\": \"Hello world!\"},\n                ),\n                \"font\": ((sorted(cls.fonts.keys())),),\n                \"wrap\": (\n                    \"INT\",\n                    {\"default\": 120, \"min\": 0, \"max\": 8096, \"step\": 1},\n                ),\n                \"font_size\": (\n                    \"INT\",\n                    {\"default\": 12, \"min\": 1, \"max\": 2500, \"step\": 1},\n                ),\n                \"width\": (\n                    \"INT\",\n                    {\"default\": 512, \"min\": 1, \"max\": 8096, \"step\": 1},\n                ),\n                \"height\": (\n                    \"INT\",\n                    {\"default\": 512, \"min\": 1, \"max\": 8096, \"step\": 1},\n                ),\n                # \"position\": ([\"INT\"], {\"default\": 0, \"min\": 0, \"max\": 100, \"step\": 1}),\n                \"color\": (\n                    \"COLOR\",\n                    {\"default\": \"black\"},\n                ),\n                \"background\": (\n                    \"COLOR\",\n                    {\"default\": \"white\"},\n                ),\n            }\n        }\n\n    RETURN_TYPES = (\"IMAGE\",)\n    RETURN_NAMES = (\"image\",)\n    FUNCTION = \"text_to_image\"\n    CATEGORY = \"mtb/generate\"\n\n    def text_to_image(\n        self, text, font, wrap, font_size, width, height, color, background\n    ):\n        from PIL import Image, ImageDraw, ImageFont\n        import textwrap\n\n        font = self.fonts[font]\n        font = cast(ImageFont.FreeTypeFont, ImageFont.truetype(font, font_size))\n        if wrap == 0:\n            wrap = width / font_size\n        lines = textwrap.wrap(text, width=wrap)\n        log.debug(f\"Lines: {lines}\")\n        line_height = bbox_dim(font.getbbox(\"hg\"))[1]\n        img_height = height  # line_height * len(lines)\n        img_width = width  # max(font.getsize(line)[0] for line in lines)\n\n        img = Image.new(\"RGBA\", (img_width, img_height), background)\n        draw = ImageDraw.Draw(img)\n        y_text = 0\n        # - bbox is [left, upper, right, lower]\n        for line in lines:\n            width, height = bbox_dim(font.getbbox(line))\n            draw.text((0, y_text), line, color, font=font)\n            y_text += height\n\n        # img.save(os.path.join(folder_paths.base_path, f'{str(uuid.uuid4())}.png'))\n        return (pil2tensor(img),)", "\n\n__nodes__ = [\n    QrCode,\n    UnsplashImage,\n    TextToImage\n    #  MtbExamples,\n]\n", ""]}
{"filename": "scripts/download_models.py", "chunked_list": ["import os\nimport requests\nfrom rich.console import Console\nfrom tqdm import tqdm\nimport subprocess\nimport sys\n\ntry:\n    import folder_paths\nexcept ModuleNotFoundError:\n    import sys\n\n    sys.path.append(os.path.join(os.path.dirname(__file__), \"../../..\"))\n    import folder_paths", "\nmodels_to_download = {\n    \"DeepBump\": {\n        \"size\": 25.5,\n        \"download_url\": \"https://github.com/HugoTini/DeepBump/raw/master/deepbump256.onnx\",\n        \"destination\": \"deepbump\",\n    },\n    \"Face Swap\": {\n        \"size\": 660,\n        \"download_url\": [", "        \"size\": 660,\n        \"download_url\": [\n            \"https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_mobilenet0.25_Final.pth\",\n            \"https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth\",\n            \"https://huggingface.co/deepinsight/inswapper/resolve/main/inswapper_128.onnx\",\n        ],\n        \"destination\": \"insightface\",\n    },\n    \"GFPGAN (face enhancement)\": {\n        \"size\": 332,", "    \"GFPGAN (face enhancement)\": {\n        \"size\": 332,\n        \"download_url\": [\n            \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\",\n            \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\n            # TODO: provide a way to selectively download models from \"packs\"\n            # https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth\n            # https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\n            # https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth\n        ],", "            # https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth\n        ],\n        \"destination\": \"face_restore\",\n    },\n    \"FILM: Frame Interpolation for Large Motion\": {\n        \"size\": 402,\n        \"download_url\": [\n            \"https://drive.google.com/drive/folders/131_--QrieM4aQbbLWrUtbO2cGbX8-war\"\n        ],\n        \"destination\": \"FILM\",", "        ],\n        \"destination\": \"FILM\",\n    },\n}\n\nconsole = Console()\n\nfrom urllib.parse import urlparse\nfrom pathlib import Path\n", "from pathlib import Path\n\n\ndef download_model(download_url, destination):\n    if isinstance(download_url, list):\n        for url in download_url:\n            download_model(url, destination)\n        return\n\n    filename = os.path.basename(urlparse(download_url).path)\n    response = None\n    if \"drive.google.com\" in download_url:\n        try:\n            import gdown\n        except ImportError:\n            print(\"Installing gdown\")\n            subprocess.check_call(\n                [\n                    sys.executable,\n                    \"-m\",\n                    \"pip\",\n                    \"install\",\n                    \"git+https://github.com/melMass/gdown@main\",\n                ]\n            )\n            import gdown\n\n        if \"/folders/\" in download_url:\n            # download folder\n            try:\n                gdown.download_folder(download_url, output=destination, resume=True)\n            except TypeError:\n                gdown.download_folder(download_url, output=destination)\n\n            return\n        # download from google drive\n        gdown.download(download_url, destination, quiet=False, resume=True)\n        return\n\n    response = requests.get(download_url, stream=True)\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    destination_path = os.path.join(destination, filename)\n    with open(destination_path, \"wb\") as file:\n        with tqdm(\n            total=total_size, unit=\"B\", unit_scale=True, desc=destination_path, ncols=80\n        ) as progress_bar:\n            for data in response.iter_content(chunk_size=4096):\n                file.write(data)\n                progress_bar.update(len(data))\n\n    console.print(\n        f\"Downloaded model from {download_url} to {destination_path}\",\n        style=\"bold green\",\n    )", "\n\ndef ask_user_for_downloads(models_to_download):\n    console.print(\"Choose models to download:\")\n    choices = {}\n    for i, model_name in enumerate(models_to_download.keys(), start=1):\n        choices[str(i)] = model_name\n        console.print(f\"{i}. {model_name}\")\n\n    console.print(\n        \"Enter the numbers of the models you want to download (comma-separated):\"\n    )\n    user_input = console.input(\">> \")\n    selected_models = user_input.split(\",\")\n    models_to_download_selected = {}\n\n    for choice in selected_models:\n        choice = choice.strip()\n\n        if choice in choices:\n            model_name = choices[choice]\n            models_to_download_selected[model_name] = models_to_download[model_name]\n\n        elif choice == \"\":\n            # download all\n            models_to_download_selected = models_to_download\n        else:\n            console.print(f\"Invalid choice: {choice}. Skipping.\")\n\n    return models_to_download_selected", "\n\ndef handle_interrupt():\n    console.print(\"Interrupted by user.\", style=\"bold red\")\n\n\ndef main(models_to_download, skip_input=False):\n    try:\n        models_to_download_selected = {}\n\n        def check_destination(urls, destination):\n            if isinstance(urls, list):\n                for url in urls:\n                    check_destination(url, destination)\n                return\n\n            filename = os.path.basename(urlparse(urls).path)\n            destination = os.path.join(folder_paths.models_dir, destination)\n\n            if not os.path.exists(destination):\n                os.makedirs(destination)\n\n            destination_path = os.path.join(destination, filename)\n            if os.path.exists(destination_path):\n                url_name = os.path.basename(urlparse(urls).path)\n                console.print(\n                    f\"Checkpoint '{url_name}' for {model_name} already exists in '{destination}'\"\n                )\n            else:\n                model_details[\"destination\"] = destination\n                models_to_download_selected[model_name] = model_details\n\n        for model_name, model_details in models_to_download.items():\n            destination = model_details[\"destination\"]\n            download_url = model_details[\"download_url\"]\n\n            check_destination(download_url, destination)\n\n        if not models_to_download_selected:\n            console.print(\"No new models to download.\")\n            return\n\n        models_to_download_selected = (\n            ask_user_for_downloads(models_to_download_selected)\n            if not skip_input\n            else models_to_download_selected\n        )\n\n        for model_name, model_details in models_to_download_selected.items():\n            download_url = model_details[\"download_url\"]\n            destination = model_details[\"destination\"]\n            console.print(f\"Downloading {model_name}...\")\n            download_model(download_url, destination)\n\n    except KeyboardInterrupt:\n        handle_interrupt()", "\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-y\", \"--yes\", action=\"store_true\", help=\"skip user input\")\n\n    args = parser.parse_args()\n    main(models_to_download, args.yes)", ""]}
{"filename": "scripts/comfy_meta.py", "chunked_list": ["import argparse\nimport json\nfrom PIL import Image, PngImagePlugin\nfrom rich.console import Console\nfrom rich import print\nfrom rich_argparse import RichHelpFormatter\nimport os\nfrom pathlib import Path\n\nconsole = Console()", "\nconsole = Console()\n\n# BNK_CutoffSetRegions\n# BNK_CutoffRegionsToConditioning\n# BNK_CutoffBasePrompt\n\n\n# Extracts metadata from a PNG image and returns it as a dictionary\ndef extract_metadata(image_path):\n    image = Image.open(image_path)\n    prompt = image.info.get(\"prompt\", \"\")\n    workflow = image.info.get(\"workflow\", \"\")\n\n    if workflow:\n        workflow = json.loads(workflow)\n\n    if prompt:\n        prompt = json.loads(prompt)\n\n    console.print(f\"Metadata extracted from [cyan]{image_path}[/cyan].\")\n\n    return {\n        \"prompt\": prompt,\n        \"workflow\": workflow,\n    }", "# Extracts metadata from a PNG image and returns it as a dictionary\ndef extract_metadata(image_path):\n    image = Image.open(image_path)\n    prompt = image.info.get(\"prompt\", \"\")\n    workflow = image.info.get(\"workflow\", \"\")\n\n    if workflow:\n        workflow = json.loads(workflow)\n\n    if prompt:\n        prompt = json.loads(prompt)\n\n    console.print(f\"Metadata extracted from [cyan]{image_path}[/cyan].\")\n\n    return {\n        \"prompt\": prompt,\n        \"workflow\": workflow,\n    }", "\n\n# Embeds metadata into a PNG image\ndef embed_metadata(image_path, metadata):\n    image = Image.open(image_path)\n    o_metadata = image.info\n\n    pnginfo = PngImagePlugin.PngInfo()\n    if prompt := metadata.get(\"prompt\"):\n        pnginfo.add_text(\"prompt\", json.dumps(prompt))\n    elif \"prompt\" in o_metadata:\n        pnginfo.add_text(\"prompt\", o_metadata[\"prompt\"])\n\n    if workflow := metadata.get(\"workflow\"):\n        pnginfo.add_text(\"workflow\", json.dumps(workflow))\n    elif \"workflow\" in o_metadata:\n        pnginfo.add_text(\"workflow\", o_metadata[\"workflow\"])\n\n    imgp = Path(image_path)\n    output = imgp.with_stem(f\"{imgp.stem}_comfy_embed\")\n    index = 1\n    while output.exists():\n        output = imgp.with_stem(f\"{imgp.stem}_{index}_comfy_embed\").with_suffix(\".png\")\n        index += 1\n\n    image.save(output, pnginfo=pnginfo)\n    console.print(f\"Metadata embedded into [cyan]{output}[/cyan].\")", "\n\n# CLI subcommand: extract\ndef extract(args):\n    input_files = []\n    for input_path in args.input:\n        if os.path.isdir(input_path):\n            folder_path = input_path\n            input_files.extend(\n                [\n                    os.path.join(folder_path, file_name)\n                    for file_name in os.listdir(folder_path)\n                    if file_name.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n                ]\n            )\n        else:\n            input_files.append(input_path)\n\n    if len(input_files) == 1:\n        metadata = extract_metadata(input_files[0])\n        if args.print_output:\n            print(json.dumps(metadata, indent=4))\n        else:\n            if not args.output:\n                output = Path(input_files[0]).with_suffix(\".json\")\n                index = 1\n                while output.exists():\n                    output = (\n                        Path(input_files[0])\n                        .with_stem(f\"{Path(input_files[0]).stem}_{index}\")\n                        .with_suffix(\".json\")\n                    )\n                    index += 1\n            else:\n                output = args.output\n            with open(output, \"w\") as file:\n                json.dump(metadata, file, indent=4)\n            console.print(f\"Metadata extracted and saved to [cyan]{output}[/cyan].\")\n    else:\n        metadata_dict = {}\n        for input_file in input_files:\n            metadata = extract_metadata(input_file)\n            filename = os.path.basename(input_file)\n            output = (\n                Path(args.output) / f\"{filename}.json\"\n                if args.output\n                else Path(input_file).with_suffix(\".json\")\n            )\n            index = 1\n            while output.exists():\n                output = Path(args.output).parent / f\"{filename}_{index}.json\"\n                index += 1\n            with open(output, \"w\") as file:\n                json.dump(metadata, file, indent=4)\n            metadata_dict[filename] = metadata\n        if args.output:\n            with open(args.output, \"w\") as file:\n                json.dump(metadata_dict, file, indent=4)\n            console.print(\n                f\"Metadata extracted and saved to [cyan]{args.output}[/cyan].\"\n            )\n        else:\n            console.print(\"Multiple metadata files created.\")", "\n\n# CLI subcommand: embed\ndef embed(args):\n    input_files = []\n    for input_path in args.input:\n        if os.path.isdir(input_path):\n            folder_path = input_path\n            input_files.extend(\n                [\n                    os.path.join(folder_path, file_name)\n                    for file_name in os.listdir(folder_path)\n                    if file_name.lower().endswith(\".json\")\n                ]\n            )\n        else:\n            input_files.append(input_path)\n\n    for input_file in input_files:\n        with open(input_file) as file:\n            metadata = json.load(file)\n        image_path = input_file.replace(\".json\", \".png\")\n        if args.output:\n            output_dir = args.output\n            if os.path.isdir(output_dir):\n                output_path = os.path.join(output_dir, os.path.basename(image_path))\n                index = 1\n                while os.path.exists(output_path):\n                    output_path = os.path.join(\n                        output_dir,\n                        f\"{os.path.basename(image_path)}_{index}.png\",\n                    )\n                    index += 1\n            else:\n                output_path = output_dir\n        else:\n            output_path = image_path.replace(\".png\", \"_comfy_embed.png\")\n\n        embed_metadata(image_path, metadata)\n        # os.rename(image_path, output_path)\n        console.print(f\"Metadata embedded into [cyan]{output_path}[/cyan].\")", "\n\nif __name__ == \"__main__\":\n    # Create the main CLI parser\n    parser = argparse.ArgumentParser(\n        prog=\"image-metadata-cli\", formatter_class=RichHelpFormatter\n    )\n    subparsers = parser.add_subparsers(title=\"subcommands\")\n\n    # Parser for the \"extract\" subcommand\n    extract_parser = subparsers.add_parser(\n        \"extract\",\n        help=\"Extract metadata from PNG image(s) or folder\",\n        formatter_class=RichHelpFormatter,\n    )\n    extract_parser.add_argument(\n        \"input\", nargs=\"+\", help=\"Input PNG image file(s) or folder path\"\n    )\n    extract_parser.add_argument(\n        \"--print\",\n        dest=\"print_output\",\n        action=\"store_true\",\n        help=\"Print the output to stdout\",\n    )\n    extract_parser.add_argument(\"--output\", help=\"Output JSON file(s) or directory\")\n    extract_parser.set_defaults(func=extract)\n\n    # Parser for the \"embed\" subcommand\n    embed_parser = subparsers.add_parser(\n        \"embed\",\n        help=\"Embed metadata into PNG image(s) or folder\",\n        formatter_class=RichHelpFormatter,\n    )\n    embed_parser.add_argument(\n        \"input\", nargs=\"+\", help=\"Input JSON file(s) or folder path\"\n    )\n    embed_parser.add_argument(\"--output\", help=\"Output PNG image file(s) or directory\")\n    embed_parser.set_defaults(func=embed)\n\n    # Parse the command-line arguments and execute the appropriate subcommand\n    args = parser.parse_args()\n    if hasattr(args, \"func\"):\n        try:\n            args.func(args)\n        except ValueError as e:\n            console.print(f\"[bold red]Error:[/bold red] {str(e)}\")\n    else:\n        parser.print_help()", ""]}
{"filename": "scripts/a111_extract.py", "chunked_list": ["from pathlib import Path\nfrom PIL import Image\nfrom PIL.PngImagePlugin import PngImageFile, PngInfo\nimport json\nfrom pprint import pprint\nimport argparse\nfrom rich.console import Console\nfrom rich.progress import Progress\nfrom rich_argparse import RichHelpFormatter\n", "from rich_argparse import RichHelpFormatter\n\n\ndef parse_a111(params, verbose=False):\n    # params = [p.split(\": \") for p in params.split(\"\\n\")]\n    params = params.split(\"\\n\")\n\n    prompt = params[0].strip()\n    neg = params[1].split(\":\")[1].strip()\n\n    settings = {}\n    try:\n        settings = {\n            s.split(\":\")[0].strip(): s.split(\":\")[1].strip()\n            for s in params[2].split(\",\")\n        }\n\n    except IndexError:\n        settings = {\"raw\": params[2].strip()}\n\n    if verbose:\n        print(f\"PROMPT: {prompt}\")\n        print(f\"NEG: {neg}\")\n        print(\"SETTINGS:\")\n        pprint(settings, indent=4)\n\n    return {\"prompt\": prompt, \"negative\": neg, \"settings\": settings}", "\n\nimport glob\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Crude metadata extractor from A111 pngs\",\n        formatter_class=RichHelpFormatter\n    )\n    parser.add_argument(\"inputs\", nargs=\"*\", help=\"Input image files\")\n    parser.add_argument(\"--output\", help=\"Output JSON file\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Verbose mode\")\n    parser.add_argument(\n        \"--glob\", help=\"Enable glob pattern matching\", metavar=\"PATTERN\"\n    )\n\n    args = parser.parse_args()\n\n    # - checks\n    if not args.glob and not args.inputs:\n        parser.error(\"Either --glob flag or inputs must be provided.\")\n    if args.glob:\n        glob_pattern = args.glob\n        try:\n            pattern_path = str(Path(glob_pattern).expanduser().resolve())\n\n            if not any(glob.glob(pattern_path)):\n                raise ValueError(f\"No files found for glob pattern: {glob_pattern}\")\n        except Exception as e:\n            console = Console()\n            console.print(\n                f\"[bold red]Error: Invalid glob pattern '{glob_pattern}': {e}[/bold red]\"\n            )\n\n            exit(1)\n    else:\n        glob_pattern = None\n\n    input_files = []\n\n    if glob_pattern:\n        input_files = list(glob.glob(str(Path(glob_pattern).expanduser().resolve())))\n    else:\n        input_files = [Path(p) for p in args.inputs]\n\n    console = Console()\n    console.print(\"Input Files:\", style=\"bold\", end=\" \")\n    console.print(f\"{len(input_files):03d} files\", style=\"cyan\")\n    # for input_file in args.inputs:\n    #     console.print(f\"- {input_file}\", style=\"cyan\")\n    console.print(\"\\nOutput File:\", style=\"bold\", end=\" \")\n    console.print(f\"{Path(args.output).resolve().absolute()}\", style=\"cyan\")\n\n    with Progress(console=console, auto_refresh=True) as progress:\n        # files = Path(pth).rglob(\"*.png\")\n        unique_info = {}\n        last = None\n\n        task = progress.add_task(\"[cyan]Extracting meta...\", total=len(input_files) + 1)\n        for p in input_files:\n            im = Image.open(p)\n            parsed = parse_a111(im.info[\"parameters\"], args.verbose)\n\n            if parsed != last:\n                unique_info[Path(p).stem] = parsed\n\n            last = parsed\n            progress.update(task, advance=1)\n            progress.refresh()\n\n        unique_info = json.dumps(unique_info, indent=4)\n        with open(args.output, \"w\") as f:\n            f.write(unique_info)\n            progress.update(task, advance=1)\n            progress.refresh()\n\n    console.print(\"\\nProcessing completed!\", style=\"bold green\")", ""]}
{"filename": "scripts/interpolate_frames.py", "chunked_list": ["import glob\nfrom pathlib import Path\nimport uuid\nimport sys\nfrom typing import List\n\nsys.path.append((Path(__file__).parent / \"extern\").as_posix())\n\n\nimport argparse", "\nimport argparse\nfrom rich_argparse import RichHelpFormatter\nfrom rich.console import Console\nfrom rich.progress import Progress\n\nimport numpy as np\nimport subprocess\n\n\ndef write_prores_444_video(output_file, frames: List[np.ndarray], fps):\n    # Convert float images to the range of 0-65535 (12-bit color depth)\n    frames = [(frame * 65535).clip(0, 65535).astype(np.uint16) for frame in frames]\n\n    height, width, _ = frames[0].shape\n\n    # Prepare the FFmpeg command\n    command = [\n        \"ffmpeg\",\n        \"-y\",  # Overwrite output file if it already exists\n        \"-f\",\n        \"rawvideo\",\n        \"-vcodec\",\n        \"rawvideo\",\n        \"-s\",\n        f\"{width}x{height}\",\n        \"-pix_fmt\",\n        \"rgb48le\",\n        \"-r\",\n        str(fps),\n        \"-i\",\n        \"-\",\n        \"-c:v\",\n        \"prores_ks\",\n        \"-profile:v\",\n        \"4\",\n        \"-pix_fmt\",\n        \"yuva444p10le\",\n        \"-r\",\n        str(fps),\n        \"-y\",  # Overwrite output file if it already exists\n        output_file,\n    ]\n\n    process = subprocess.Popen(command, stdin=subprocess.PIPE)\n\n    for frame in frames:\n        process.stdin.write(frame.tobytes())\n\n    process.stdin.close()\n    process.wait()", "\n\ndef write_prores_444_video(output_file, frames: List[np.ndarray], fps):\n    # Convert float images to the range of 0-65535 (12-bit color depth)\n    frames = [(frame * 65535).clip(0, 65535).astype(np.uint16) for frame in frames]\n\n    height, width, _ = frames[0].shape\n\n    # Prepare the FFmpeg command\n    command = [\n        \"ffmpeg\",\n        \"-y\",  # Overwrite output file if it already exists\n        \"-f\",\n        \"rawvideo\",\n        \"-vcodec\",\n        \"rawvideo\",\n        \"-s\",\n        f\"{width}x{height}\",\n        \"-pix_fmt\",\n        \"rgb48le\",\n        \"-r\",\n        str(fps),\n        \"-i\",\n        \"-\",\n        \"-c:v\",\n        \"prores_ks\",\n        \"-profile:v\",\n        \"4\",\n        \"-pix_fmt\",\n        \"yuva444p10le\",\n        \"-r\",\n        str(fps),\n        \"-y\",  # Overwrite output file if it already exists\n        output_file,\n    ]\n\n    process = subprocess.Popen(command, stdin=subprocess.PIPE)\n\n    for frame in frames:\n        process.stdin.write(frame.tobytes())\n\n    process.stdin.close()\n    process.wait()", "\n\nif __name__ == \"__main__\":\n    default_output = f\"./output_{uuid.uuid4()}.mov\"\n    parser = argparse.ArgumentParser(\n        description=\"FILM frame interpolation\", formatter_class=RichHelpFormatter\n    )\n    parser.add_argument(\"inputs\", nargs=\"*\", help=\"Input image files\")\n    parser.add_argument(\"--output\", help=\"Output JSON file\", default=default_output)\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Verbose mode\")\n    parser.add_argument(\n        \"--glob\", help=\"Enable glob pattern matching\", metavar=\"PATTERN\"\n    )\n    parser.add_argument(\n        \"--interpolate\", type=int, default=4, help=\"Time for interpolated frames\"\n    )\n    parser.add_argument(\"--fps\", type=int, default=30, help=\"Out FPS\")\n    align = 64\n    block_width = 2\n    block_height = 2\n\n    args = parser.parse_args()\n\n    # - checks\n    if not args.glob and not args.inputs:\n        parser.error(\"Either --glob flag or inputs must be provided.\")\n    if args.glob:\n        glob_pattern = args.glob\n        try:\n            pattern_path = str(Path(glob_pattern).expanduser().resolve())\n\n            if not any(glob.glob(pattern_path)):\n                raise ValueError(f\"No files found for glob pattern: {glob_pattern}\")\n        except Exception as e:\n            console = Console()\n            console.print(\n                f\"[bold red]Error: Invalid glob pattern '{glob_pattern}': {e}[/bold red]\"\n            )\n\n            exit(1)\n    else:\n        glob_pattern = None\n\n    input_files: List[Path] = []\n\n    if glob_pattern:\n        input_files = [\n            Path(p)\n            for p in list(glob.glob(str(Path(glob_pattern).expanduser().resolve())))\n        ]\n    else:\n        input_files = [Path(p) for p in args.inputs]\n\n    console = Console()\n    console.print(\"Input Files:\", style=\"bold\", end=\" \")\n    console.print(f\"{len(input_files):03d} files\", style=\"cyan\")\n    # for input_file in args.inputs:\n    #     console.print(f\"- {input_file}\", style=\"cyan\")\n    console.print(\"\\nOutput File:\", style=\"bold\", end=\" \")\n    console.print(f\"{Path(args.output).resolve().absolute()}\", style=\"cyan\")\n\n    with Progress(console=console, auto_refresh=True) as progress:\n        from frame_interpolation.eval import util\n        from frame_interpolation.eval import util, interpolator\n\n        # files = Path(pth).rglob(\"*.png\")\n\n        model = interpolator.Interpolator(\n            \"G:/MODELS/FILM/pretrained_models/film_net/Style\", None\n        )  # [2,2]\n\n        task = progress.add_task(\"[cyan]Interpolating frames...\", total=1)\n\n        frames = list(\n            util.interpolate_recursively_from_files(\n                [x.as_posix() for x in input_files], args.interpolate, model\n            )\n        )\n\n        # mediapy.write_video(args.output, frames, fps=args.fps)\n        write_prores_444_video(args.output, frames, fps=args.fps)\n        progress.update(task, advance=1)\n        progress.refresh()", ""]}
{"filename": "scripts/get_deps.py", "chunked_list": ["import os\nimport ast\nimport json\nimport sys\nfrom rich.console import Console\nfrom rich.table import Table\nfrom rich.progress import Progress\n\nconsole = Console(stderr=True)\n", "console = Console(stderr=True)\n\n\ndef get_imported_modules(filename):\n    with open(filename, \"r\") as file:\n        tree = ast.parse(file.read())\n\n    imported_modules = []\n    for node in ast.walk(tree):\n        if isinstance(node, ast.Import):\n            imported_modules.extend(\n                (alias.name, alias.name in sys.builtin_module_names)\n                for alias in node.names\n            )\n        elif isinstance(node, ast.ImportFrom):\n            if node.module:\n                imported_modules.append(\n                    (node.module, node.module in sys.builtin_module_names)\n                )\n\n    return imported_modules", "\n\ndef list_imported_modules(folder):\n    modules = []\n\n    file_count = sum(len(files) for _, _, files in os.walk(folder))\n    progress = Progress()\n\n    task = progress.add_task(\"[cyan]Scanning files...\", total=file_count)\n\n    for root, _, files in os.walk(folder):\n        for file in files:\n            if file.endswith(\".py\"):\n                file_path = os.path.join(root, file)\n                imported_modules = get_imported_modules(file_path)\n                modules.extend(imported_modules)\n            progress.update(task, advance=1)\n\n    progress.stop()\n\n    return modules", "\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        console.print(\n            \"[bold red]Please provide the folder path as a command-line argument.[/bold red]\"\n        )\n        sys.exit(1)\n\n    # folder_path = input(\"Enter the folder path: \")\n    # while not os.path.isdir(folder_path):\n    #     console.print(\"[bold red]Invalid folder path![/bold red]\")\n    #     folder_path = input(\"Enter the folder path: \")\n    folder_path = sys.argv[1]\n    if not os.path.isdir(folder_path):\n        console.print(\"[bold red]Invalid folder path![/bold red]\")\n        sys.exit(1)\n\n    console.print(\"[bold green]=== Python Imported Modules ===[/bold green]\\n\")\n    console.print(f\"Scanning folder: [bold]{folder_path}[/bold]\\n\")\n\n    imported_modules = list_imported_modules(folder_path)\n\n    console.print(f\"\\n[bold green]Imported Modules:[/bold green]\\n\")\n\n    table = Table(show_header=True, header_style=\"bold cyan\")\n    table.add_column(\"Module\")\n    table.add_column(\"Type\")\n\n    for module, is_builtin in imported_modules:\n        module_type = \"Built-in\" if is_builtin else \"External\"\n        table.add_row(module, module_type)\n\n    console.print(table)\n\n    json_data = json.dumps(\n        [\n            {\"module\": module, \"type\": \"Built-in\" if is_builtin else \"External\"}\n            for module, is_builtin in imported_modules\n        ],\n        indent=4,\n    )\n    print(json_data)", ""]}
