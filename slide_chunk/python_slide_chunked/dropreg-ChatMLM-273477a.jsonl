{"filename": "xlmr/scripts/process/prepare_xxl_ckpt.py", "chunked_list": ["from fairseq import checkpoint_utils\nimport torch\n\n\ndef read_ckpt():\n    local_path = \"/opt/data/private/data/xlmr/xlmr.xl/parallel_8/model-model_part-1.pt\"\n    with open(local_path, \"rb\") as f:\n        state_all = torch.load(f, map_location=torch.device(\"cpu\"))\n    import pdb; pdb.set_trace()\n\ndef split_ckpt(para_block=8):\n    \n    for rank in range(para_block):\n        \n        local_path = \"/opt/data/private/data/xlmr/xlmr.xxl/model.pt\"\n        with open(local_path, \"rb\") as f:\n            state = torch.load(f, map_location=torch.device(\"cpu\"))\n\n        vocab_size = 250880\n        embed_size = 4096\n        fc_size = 4096 * 4\n        para_vocab_size = vocab_size // para_block\n        para_embed_size = embed_size // para_block\n        para_fc_size = fc_size // para_block\n        \n        print(para_vocab_size, para_embed_size, para_fc_size)\n        \n        # rescale vocab embedding:\n        embed_weight = state['model']['encoder.sentence_encoder.embed_tokens.weight']\n        temp_size = embed_weight.size(0)\n        embed_weight_copy = torch.zeros(vocab_size, embed_size).type_as(embed_weight)\n        embed_weight_copy[:temp_size, :] = embed_weight\n        embed_weight = embed_weight_copy\n        print(embed_weight.size())\n\n        # rescale output mapping bias:\n        embed_bias = state['model']['encoder.lm_head.bias']\n        embed_bias_copy = torch.zeros(vocab_size).type_as(embed_weight)\n        embed_bias_copy[:temp_size] = embed_bias\n        embed_bias = embed_bias_copy\n        print(embed_bias.size())\n\n        for k in list(state['model'].keys()):\n            \n            if \"layer_norm\" in k or \"embed_positions\" in k or \"version\" in k or \"layernorm_embedding\" in k:\n                print(k)\n                state['model'][k] = state['model'][k].clone()\n                continue\n\n            if \"encoder.sentence_encoder.embed_tokens.weight\" in k:\n                start_dim = rank * para_vocab_size\n                end_dim = (rank + 1) * para_vocab_size\n                print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                state['model'][k] = embed_weight[start_dim:end_dim, :].clone()\n            elif \"encoder.sentence_encoder.layers\" in k:\n                if \"fc\" in k:\n                    start_dim = rank * para_fc_size\n                    end_dim = (rank + 1) * para_fc_size\n                    print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                    if \"fc1.weight\" in k:\n                        state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n                    elif \"fc1.bias\" in k:\n                        state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n                    elif \"fc2.weight\" in k:\n                        state['model'][k] = state['model'][k][:, start_dim:end_dim].clone()\n                else:\n                    start_dim = rank * para_embed_size\n                    end_dim = (rank + 1) * para_embed_size\n                    print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                    if \"self_attn.out_proj.weight\" in k:\n                        state['model'][k] = state['model'][k][:, start_dim:end_dim].clone()\n                    elif \"self_attn.out_proj.bias\" in k:\n                        state['model'][k] = state['model'][k].clone()\n                    else:\n                        if \"self_attn.q_proj.weight\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n                        elif \"self_attn.q_proj.bias\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n                        elif \"self_attn.k_proj.weight\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n                        elif \"self_attn.k_proj.bias\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n                        elif \"self_attn.v_proj.weight\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n                        elif \"self_attn.v_proj.bias\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n                        else:\n                            print(state['model'][k].size())\n                            print(k)\n                            raise NotImplementedError\n            elif \"lm_head.weight\" in k:\n                start_dim = rank * para_vocab_size\n                end_dim = (rank + 1) * para_vocab_size\n                state['model'][k] = embed_weight[start_dim:end_dim, :].clone()\n            elif \"lm_head.bias\" in k:\n                state['model'][k] = embed_bias.clone()\n            elif \"lm_head.dense.weight\" in k:\n                start_dim = rank * para_embed_size\n                end_dim = (rank + 1) * para_embed_size\n                print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n            elif \"lm_head.dense.bias\" in k:\n                start_dim = rank * para_embed_size\n                end_dim = (rank + 1) * para_embed_size\n                print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n            else:\n                print(state['model'][k].size())\n                print(k)\n                raise NotImplementedError\n        torch.save(state, \"/opt/data/private/data/xlmr/xlmr.xxl/parallel_8/model-model_part-{}.pt\".format(rank))", "\ndef split_ckpt(para_block=8):\n    \n    for rank in range(para_block):\n        \n        local_path = \"/opt/data/private/data/xlmr/xlmr.xxl/model.pt\"\n        with open(local_path, \"rb\") as f:\n            state = torch.load(f, map_location=torch.device(\"cpu\"))\n\n        vocab_size = 250880\n        embed_size = 4096\n        fc_size = 4096 * 4\n        para_vocab_size = vocab_size // para_block\n        para_embed_size = embed_size // para_block\n        para_fc_size = fc_size // para_block\n        \n        print(para_vocab_size, para_embed_size, para_fc_size)\n        \n        # rescale vocab embedding:\n        embed_weight = state['model']['encoder.sentence_encoder.embed_tokens.weight']\n        temp_size = embed_weight.size(0)\n        embed_weight_copy = torch.zeros(vocab_size, embed_size).type_as(embed_weight)\n        embed_weight_copy[:temp_size, :] = embed_weight\n        embed_weight = embed_weight_copy\n        print(embed_weight.size())\n\n        # rescale output mapping bias:\n        embed_bias = state['model']['encoder.lm_head.bias']\n        embed_bias_copy = torch.zeros(vocab_size).type_as(embed_weight)\n        embed_bias_copy[:temp_size] = embed_bias\n        embed_bias = embed_bias_copy\n        print(embed_bias.size())\n\n        for k in list(state['model'].keys()):\n            \n            if \"layer_norm\" in k or \"embed_positions\" in k or \"version\" in k or \"layernorm_embedding\" in k:\n                print(k)\n                state['model'][k] = state['model'][k].clone()\n                continue\n\n            if \"encoder.sentence_encoder.embed_tokens.weight\" in k:\n                start_dim = rank * para_vocab_size\n                end_dim = (rank + 1) * para_vocab_size\n                print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                state['model'][k] = embed_weight[start_dim:end_dim, :].clone()\n            elif \"encoder.sentence_encoder.layers\" in k:\n                if \"fc\" in k:\n                    start_dim = rank * para_fc_size\n                    end_dim = (rank + 1) * para_fc_size\n                    print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                    if \"fc1.weight\" in k:\n                        state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n                    elif \"fc1.bias\" in k:\n                        state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n                    elif \"fc2.weight\" in k:\n                        state['model'][k] = state['model'][k][:, start_dim:end_dim].clone()\n                else:\n                    start_dim = rank * para_embed_size\n                    end_dim = (rank + 1) * para_embed_size\n                    print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                    if \"self_attn.out_proj.weight\" in k:\n                        state['model'][k] = state['model'][k][:, start_dim:end_dim].clone()\n                    elif \"self_attn.out_proj.bias\" in k:\n                        state['model'][k] = state['model'][k].clone()\n                    else:\n                        if \"self_attn.q_proj.weight\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n                        elif \"self_attn.q_proj.bias\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n                        elif \"self_attn.k_proj.weight\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n                        elif \"self_attn.k_proj.bias\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n                        elif \"self_attn.v_proj.weight\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n                        elif \"self_attn.v_proj.bias\" in k:\n                            state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n                        else:\n                            print(state['model'][k].size())\n                            print(k)\n                            raise NotImplementedError\n            elif \"lm_head.weight\" in k:\n                start_dim = rank * para_vocab_size\n                end_dim = (rank + 1) * para_vocab_size\n                state['model'][k] = embed_weight[start_dim:end_dim, :].clone()\n            elif \"lm_head.bias\" in k:\n                state['model'][k] = embed_bias.clone()\n            elif \"lm_head.dense.weight\" in k:\n                start_dim = rank * para_embed_size\n                end_dim = (rank + 1) * para_embed_size\n                print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                state['model'][k] = state['model'][k][start_dim:end_dim, :].clone()\n            elif \"lm_head.dense.bias\" in k:\n                start_dim = rank * para_embed_size\n                end_dim = (rank + 1) * para_embed_size\n                print(\"convert {} from {} to {} \".format(k, start_dim, end_dim))\n                state['model'][k] = state['model'][k][start_dim:end_dim].clone()\n            else:\n                print(state['model'][k].size())\n                print(k)\n                raise NotImplementedError\n        torch.save(state, \"/opt/data/private/data/xlmr/xlmr.xxl/parallel_8/model-model_part-{}.pt\".format(rank))", "\n\nsplit_ckpt(para_block=8)\n\n# read_ckpt()\n"]}
{"filename": "xlmr/scripts/process/merge_xxl_ckpt.py", "chunked_list": ["from fairseq import checkpoint_utils\nimport torch\n\n\ndef merge_ckpt(para_block=8):\n    \n    state = None\n    for rank in range(para_block):\n        \n        local_path = \"/opt/data/private/ckpt/nar_chat/megatron8_ft/checkpoint9-model_part-{}.pt\".format(rank)\n        with open(local_path, \"rb\") as f:\n            ckpt_state = torch.load(f, map_location=torch.device(\"cpu\"))\n\n        if state is None:\n            state = ckpt_state\n            continue\n        else:\n            ckpt_state = ckpt_state['model']    \n        \n        for k in list(ckpt_state.keys()):\n            print(rank, k, ckpt_state[k].size())\n\n            if \"embed_tokens\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"embed_positions\" in k:\n                continue\n            elif \"decoder.layers\" in k and \"q_proj.weight\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"decoder.layers\" in k and \"q_proj.bias\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"decoder.layers\" in k and \"k_proj.weight\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"decoder.layers\" in k and \"k_proj.bias\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"decoder.layers\" in k and \"v_proj.weight\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"decoder.layers\" in k and \"v_proj.bias\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"decoder.layers\" in k and \"out_proj.weight\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=-1)\n            elif \"decoder.layers\" in k and \"out_proj.bias\" in k:\n                continue\n            elif \"decoder.layers\" in k and \"fc1.weight\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"decoder.layers\" in k and \"fc1.bias\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"decoder.layers\" in k and \"fc2.weight\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=-1)\n            elif \"decoder.layers\" in k and \"fc2.bias\" in k:\n                continue\n            elif \"norm\" in k:\n                continue\n            elif \"lm_head.weight\" in k: \n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"lm_head.bias\" in k:\n                continue\n            elif \"lm_head.dense.weight\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"lm_head.dense.bias\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            elif \"embed_length.weight\" in k:\n                state['model'][k] = torch.cat([state['model'][k], ckpt_state[k]], dim=0)\n            else:\n                import pdb; pdb.set_trace()\n    torch.save(state, \"/opt/data/private/ckpt/nar_chat/megatron8_ft/model.pt\")", "\nmerge_ckpt(para_block=8)\n"]}
{"filename": "xlmr/src/generate2.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nTranslate pre-processed data with a trained model.\n\"\"\"\n\nimport ast\nimport logging", "import ast\nimport logging\nimport math\nimport os\nimport sys\nfrom argparse import Namespace\nfrom itertools import chain\n\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nfrom omegaconf import DictConfig\n\nfrom fairseq import checkpoint_utils, options, scoring, tasks, utils\nfrom fairseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom fairseq.logging import progress_bar\nfrom fairseq.logging.meters import StopwatchMeter, TimeMeter\n\n\ndef main(cfg: DictConfig):\n\n    if isinstance(cfg, Namespace):\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    assert cfg.common_eval.path is not None, \"--path required for generation!\"\n    assert (\n        not cfg.generation.sampling or cfg.generation.nbest == cfg.generation.beam\n    ), \"--sampling requires --nbest to be equal to --beam\"\n    assert (\n        cfg.generation.replace_unk is None or cfg.dataset.dataset_impl == \"raw\"\n    ), \"--replace-unk requires a raw text dataset (--dataset-impl=raw)\"\n\n    if cfg.common_eval.results_path is not None:\n        os.makedirs(cfg.common_eval.results_path, exist_ok=True)\n        output_path = os.path.join(\n            cfg.common_eval.results_path,\n            \"generate-{}.txt\".format(cfg.dataset.gen_subset),\n        )\n        with open(output_path, \"w\", buffering=1, encoding=\"utf-8\") as h:\n            return _main(cfg, h)\n    else:\n        return _main(cfg, sys.stdout)", "\n\ndef main(cfg: DictConfig):\n\n    if isinstance(cfg, Namespace):\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    assert cfg.common_eval.path is not None, \"--path required for generation!\"\n    assert (\n        not cfg.generation.sampling or cfg.generation.nbest == cfg.generation.beam\n    ), \"--sampling requires --nbest to be equal to --beam\"\n    assert (\n        cfg.generation.replace_unk is None or cfg.dataset.dataset_impl == \"raw\"\n    ), \"--replace-unk requires a raw text dataset (--dataset-impl=raw)\"\n\n    if cfg.common_eval.results_path is not None:\n        os.makedirs(cfg.common_eval.results_path, exist_ok=True)\n        output_path = os.path.join(\n            cfg.common_eval.results_path,\n            \"generate-{}.txt\".format(cfg.dataset.gen_subset),\n        )\n        with open(output_path, \"w\", buffering=1, encoding=\"utf-8\") as h:\n            return _main(cfg, h)\n    else:\n        return _main(cfg, sys.stdout)", "\n\ndef get_symbols_to_strip_from_output(generator):\n    if hasattr(generator, \"symbols_to_strip_from_output\"):\n        return generator.symbols_to_strip_from_output\n    else:\n        return {generator.eos}\n\n\ndef _main(cfg: DictConfig, output_file):\n    logging.basicConfig(\n        format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n        stream=output_file,\n    )\n    logger = logging.getLogger(\"fairseq_cli.generate\")\n\n    utils.import_user_module(cfg.common)\n\n    if cfg.dataset.max_tokens is None and cfg.dataset.batch_size is None:\n        cfg.dataset.max_tokens = 12000\n    logger.info(cfg)\n\n    # Fix seed for stochastic decoding\n    if cfg.common.seed is not None and not cfg.generation.no_seed_provided:\n        np.random.seed(cfg.common.seed)\n        utils.set_torch_seed(cfg.common.seed)\n\n    use_cuda = torch.cuda.is_available() and not cfg.common.cpu\n\n    # Load dataset splits\n    task = tasks.setup_task(cfg.task)\n\n    # Set dictionaries\n    try:\n        src_dict = getattr(task, \"source_dictionary\", None)\n    except NotImplementedError:\n        src_dict = None\n    tgt_dict = task.target_dictionary\n\n    overrides = ast.literal_eval(cfg.common_eval.model_overrides)\n\n    # Load ensemble\n    logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n    models, saved_cfg = checkpoint_utils.load_model_ensemble(\n        utils.split_paths(cfg.common_eval.path),\n        arg_overrides=overrides,\n        task=task,\n        suffix=cfg.checkpoint.checkpoint_suffix,\n        strict=(cfg.checkpoint.checkpoint_shard_count == 1),\n        num_shards=cfg.checkpoint.checkpoint_shard_count,\n    )\n\n    # loading the dataset should happen after the checkpoint has been loaded so we can give it the saved task config\n    task.load_dataset(cfg.dataset.gen_subset, task_cfg=saved_cfg.task)\n\n    if cfg.generation.lm_path is not None:\n        overrides[\"data\"] = cfg.task.data\n\n        try:\n            lms, _ = checkpoint_utils.load_model_ensemble(\n                [cfg.generation.lm_path], arg_overrides=overrides, task=None\n            )\n        except:\n            logger.warning(\n                f\"Failed to load language model! Please make sure that the language model dict is the same \"\n                f\"as target dict and is located in the data dir ({cfg.task.data})\"\n            )\n            raise\n\n        assert len(lms) == 1\n    else:\n        lms = [None]\n\n    # Optimize ensemble for generation\n    for model in chain(models, lms):\n        if model is None:\n            continue\n        if cfg.common.fp16:\n            model.half()\n        if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n            model.cuda()\n        model.prepare_for_inference_(cfg)\n\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(cfg.generation.replace_unk)\n\n    # Load dataset (possibly sharded)\n    itr = task.get_batch_iterator(\n        dataset=task.dataset(cfg.dataset.gen_subset),\n        max_tokens=cfg.dataset.max_tokens,\n        max_sentences=cfg.dataset.batch_size,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), *[m.max_positions() for m in models]\n        ),\n        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n        seed=cfg.common.seed,\n        num_shards=cfg.distributed_training.distributed_world_size,\n        shard_id=cfg.distributed_training.distributed_rank,\n        num_workers=cfg.dataset.num_workers,\n        data_buffer_size=cfg.dataset.data_buffer_size,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_interval=cfg.common.log_interval,\n        default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n    )\n\n    # Initialize generator\n    gen_timer = StopwatchMeter()\n\n    extra_gen_cls_kwargs = {\"lm_model\": lms[0], \"lm_weight\": cfg.generation.lm_weight}\n    generator = task.build_generator(\n        models, cfg.generation, extra_gen_cls_kwargs=extra_gen_cls_kwargs\n    )\n\n    # Handle tokenization and BPE\n    tokenizer = task.build_tokenizer(cfg.tokenizer)\n    bpe = task.build_bpe(cfg.bpe)\n\n    def decode_fn(x):\n        if bpe is not None:\n            x = bpe.decode(x)\n        if tokenizer is not None:\n            x = tokenizer.decode(x)\n        return x\n\n    scorer = scoring.build_scorer(cfg.scoring, tgt_dict)\n\n    num_sentences = 0\n    has_target = True\n    wps_meter = TimeMeter()\n    for sample in progress:\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n        if \"net_input\" not in sample:\n            continue\n\n        prefix_tokens = None\n        if cfg.generation.prefix_size > 0:\n            prefix_tokens = sample[\"target\"][:, : cfg.generation.prefix_size]\n\n        constraints = None\n        if \"constraints\" in sample:\n            constraints = sample[\"constraints\"]\n\n        gen_timer.start()\n        hypos = task.inference_step(\n            generator,\n            models,\n            sample,\n            prefix_tokens=prefix_tokens,\n            constraints=constraints,\n        )\n        num_generated_tokens = sum(len(h[0][\"tokens\"]) for h in hypos)\n        gen_timer.stop(num_generated_tokens)\n\n        for i, sample_id in enumerate(sample[\"id\"].tolist()):\n            has_target = sample[\"target\"] is not None\n\n            # Remove padding\n            if \"src_tokens\" in sample[\"net_input\"]:\n                src_tokens = utils.strip_pad(\n                    sample[\"net_input\"][\"src_tokens\"][i, :], tgt_dict.pad()\n                )\n            else:\n                src_tokens = None\n\n            target_tokens = None\n            if has_target:\n                target_tokens = (\n                    utils.strip_pad(sample[\"target\"][i, :], tgt_dict.pad()).int().cpu()\n                )\n\n            # Either retrieve the original sentences or regenerate them from tokens.\n            if align_dict is not None:\n                src_str = task.dataset(cfg.dataset.gen_subset).src.get_original_text(\n                    sample_id\n                )\n                target_str = task.dataset(cfg.dataset.gen_subset).tgt.get_original_text(\n                    sample_id\n                )\n            else:\n                if src_dict is not None:\n                    src_str = src_dict.string(src_tokens, cfg.common_eval.post_process)\n                else:\n                    src_str = \"\"\n                if has_target:\n                    target_str = tgt_dict.string(\n                        target_tokens,\n                        cfg.common_eval.post_process,\n                        escape_unk=True,\n                        extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n                            generator\n                        ),\n                    )\n\n            src_str = decode_fn(src_str)\n            if has_target:\n                target_str = decode_fn(target_str)\n\n            if not cfg.common_eval.quiet:\n                if src_dict is not None:\n                    print(\"S-{}\\t{}\".format(sample_id, src_str), file=output_file)\n                if has_target:\n                    print(\"T-{}\\t{}\".format(sample_id, target_str), file=output_file)\n\n            # Process top predictions\n            for j, hypo in enumerate(hypos[i][: cfg.generation.nbest]):\n                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n                    src_str=src_str,\n                    alignment=hypo[\"alignment\"],\n                    align_dict=align_dict,\n                    tgt_dict=tgt_dict,\n                    remove_bpe=cfg.common_eval.post_process,\n                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n                )\n                detok_hypo_str = decode_fn(hypo_str)\n                if not cfg.common_eval.quiet:\n                    score = hypo[\"score\"] / math.log(2)  # convert to base 2\n                    # original hypothesis (after tokenization and BPE)\n                    print(\n                        \"H-{}\\t{}\\t{}\".format(sample_id, score, hypo_str),\n                        file=output_file,\n                    )\n                    # detokenized hypothesis\n                    print(\n                        \"D-{}\\t{}\\t{}\".format(sample_id, score, detok_hypo_str),\n                        file=output_file,\n                    )\n                    print(\n                        \"P-{}\\t{}\".format(\n                            sample_id,\n                            \" \".join(\n                                map(\n                                    lambda x: \"{:.4f}\".format(x),\n                                    # convert from base e to base 2\n                                    hypo[\"positional_scores\"]\n                                    .div_(math.log(2))\n                                    .tolist(),\n                                )\n                            ),\n                        ),\n                        file=output_file,\n                    )\n\n                    if cfg.generation.print_alignment == \"hard\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\n                                        \"{}-{}\".format(src_idx, tgt_idx)\n                                        for src_idx, tgt_idx in alignment\n                                    ]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n                    if cfg.generation.print_alignment == \"soft\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\",\".join(src_probs) for src_probs in alignment]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.print_step:\n                        print(\n                            \"I-{}\\t{}\".format(sample_id, hypo[\"steps\"]),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.retain_iter_history:\n                        for step, h in enumerate(hypo[\"history\"]):\n                            _, h_str, _ = utils.post_process_prediction(\n                                hypo_tokens=h[\"tokens\"].int().cpu(),\n                                src_str=src_str,\n                                alignment=None,\n                                align_dict=None,\n                                tgt_dict=tgt_dict,\n                                remove_bpe=None,\n                            )\n                            print(\n                                \"E-{}_{}\\t{}\".format(sample_id, step, h_str),\n                                file=output_file,\n                            )\n\n                # Score only the top hypothesis\n                if has_target and j == 0:\n                    if (\n                        align_dict is not None\n                        or cfg.common_eval.post_process is not None\n                    ):\n                        # Convert back to tokens for evaluation with unk replacement and/or without BPE\n                        target_tokens = tgt_dict.encode_line(\n                            target_str, add_if_not_exist=True\n                        )\n                        hypo_tokens = tgt_dict.encode_line(\n                            detok_hypo_str, add_if_not_exist=True\n                        )\n                    if hasattr(scorer, \"add_string\"):\n                        scorer.add_string(target_str, detok_hypo_str)\n                    else:\n                        scorer.add(target_tokens, hypo_tokens)\n\n        wps_meter.update(num_generated_tokens)\n        progress.log({\"wps\": round(wps_meter.avg)})\n        num_sentences += (\n            sample[\"nsentences\"] if \"nsentences\" in sample else sample[\"id\"].numel()\n        )\n\n    logger.info(\"NOTE: hypothesis and token scores are output in base 2\")\n    logger.info(\n        \"Translated {:,} sentences ({:,} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)\".format(\n            num_sentences,\n            gen_timer.n,\n            gen_timer.sum,\n            num_sentences / gen_timer.sum,\n            1.0 / gen_timer.avg,\n        )\n    )\n    if has_target:\n        if cfg.bpe and not cfg.generation.sacrebleu:\n            if cfg.common_eval.post_process:\n                logger.warning(\n                    \"BLEU score is being computed by splitting detokenized string on spaces, this is probably not what you want. Use --sacrebleu for standard 13a BLEU tokenization\"\n                )\n            else:\n                logger.warning(\n                    \"If you are using BPE on the target side, the BLEU score is computed on BPE tokens, not on proper words.  Use --sacrebleu for standard 13a BLEU tokenization\"\n                )\n        # use print to be consistent with other main outputs: S-, H-, T-, D- and so on\n        print(\n            \"Generate {} with beam={}: {}\".format(\n                cfg.dataset.gen_subset, cfg.generation.beam, scorer.result_string()\n            ),\n            file=output_file,\n        )\n\n    return scorer", "\ndef _main(cfg: DictConfig, output_file):\n    logging.basicConfig(\n        format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n        stream=output_file,\n    )\n    logger = logging.getLogger(\"fairseq_cli.generate\")\n\n    utils.import_user_module(cfg.common)\n\n    if cfg.dataset.max_tokens is None and cfg.dataset.batch_size is None:\n        cfg.dataset.max_tokens = 12000\n    logger.info(cfg)\n\n    # Fix seed for stochastic decoding\n    if cfg.common.seed is not None and not cfg.generation.no_seed_provided:\n        np.random.seed(cfg.common.seed)\n        utils.set_torch_seed(cfg.common.seed)\n\n    use_cuda = torch.cuda.is_available() and not cfg.common.cpu\n\n    # Load dataset splits\n    task = tasks.setup_task(cfg.task)\n\n    # Set dictionaries\n    try:\n        src_dict = getattr(task, \"source_dictionary\", None)\n    except NotImplementedError:\n        src_dict = None\n    tgt_dict = task.target_dictionary\n\n    overrides = ast.literal_eval(cfg.common_eval.model_overrides)\n\n    # Load ensemble\n    logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n    models, saved_cfg = checkpoint_utils.load_model_ensemble(\n        utils.split_paths(cfg.common_eval.path),\n        arg_overrides=overrides,\n        task=task,\n        suffix=cfg.checkpoint.checkpoint_suffix,\n        strict=(cfg.checkpoint.checkpoint_shard_count == 1),\n        num_shards=cfg.checkpoint.checkpoint_shard_count,\n    )\n\n    # loading the dataset should happen after the checkpoint has been loaded so we can give it the saved task config\n    task.load_dataset(cfg.dataset.gen_subset, task_cfg=saved_cfg.task)\n\n    if cfg.generation.lm_path is not None:\n        overrides[\"data\"] = cfg.task.data\n\n        try:\n            lms, _ = checkpoint_utils.load_model_ensemble(\n                [cfg.generation.lm_path], arg_overrides=overrides, task=None\n            )\n        except:\n            logger.warning(\n                f\"Failed to load language model! Please make sure that the language model dict is the same \"\n                f\"as target dict and is located in the data dir ({cfg.task.data})\"\n            )\n            raise\n\n        assert len(lms) == 1\n    else:\n        lms = [None]\n\n    # Optimize ensemble for generation\n    for model in chain(models, lms):\n        if model is None:\n            continue\n        if cfg.common.fp16:\n            model.half()\n        if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n            model.cuda()\n        model.prepare_for_inference_(cfg)\n\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(cfg.generation.replace_unk)\n\n    # Load dataset (possibly sharded)\n    itr = task.get_batch_iterator(\n        dataset=task.dataset(cfg.dataset.gen_subset),\n        max_tokens=cfg.dataset.max_tokens,\n        max_sentences=cfg.dataset.batch_size,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), *[m.max_positions() for m in models]\n        ),\n        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n        seed=cfg.common.seed,\n        num_shards=cfg.distributed_training.distributed_world_size,\n        shard_id=cfg.distributed_training.distributed_rank,\n        num_workers=cfg.dataset.num_workers,\n        data_buffer_size=cfg.dataset.data_buffer_size,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_interval=cfg.common.log_interval,\n        default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n    )\n\n    # Initialize generator\n    gen_timer = StopwatchMeter()\n\n    extra_gen_cls_kwargs = {\"lm_model\": lms[0], \"lm_weight\": cfg.generation.lm_weight}\n    generator = task.build_generator(\n        models, cfg.generation, extra_gen_cls_kwargs=extra_gen_cls_kwargs\n    )\n\n    # Handle tokenization and BPE\n    tokenizer = task.build_tokenizer(cfg.tokenizer)\n    bpe = task.build_bpe(cfg.bpe)\n\n    def decode_fn(x):\n        if bpe is not None:\n            x = bpe.decode(x)\n        if tokenizer is not None:\n            x = tokenizer.decode(x)\n        return x\n\n    scorer = scoring.build_scorer(cfg.scoring, tgt_dict)\n\n    num_sentences = 0\n    has_target = True\n    wps_meter = TimeMeter()\n    for sample in progress:\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n        if \"net_input\" not in sample:\n            continue\n\n        prefix_tokens = None\n        if cfg.generation.prefix_size > 0:\n            prefix_tokens = sample[\"target\"][:, : cfg.generation.prefix_size]\n\n        constraints = None\n        if \"constraints\" in sample:\n            constraints = sample[\"constraints\"]\n\n        gen_timer.start()\n        hypos = task.inference_step(\n            generator,\n            models,\n            sample,\n            prefix_tokens=prefix_tokens,\n            constraints=constraints,\n        )\n        num_generated_tokens = sum(len(h[0][\"tokens\"]) for h in hypos)\n        gen_timer.stop(num_generated_tokens)\n\n        for i, sample_id in enumerate(sample[\"id\"].tolist()):\n            has_target = sample[\"target\"] is not None\n\n            # Remove padding\n            if \"src_tokens\" in sample[\"net_input\"]:\n                src_tokens = utils.strip_pad(\n                    sample[\"net_input\"][\"src_tokens\"][i, :], tgt_dict.pad()\n                )\n            else:\n                src_tokens = None\n\n            target_tokens = None\n            if has_target:\n                target_tokens = (\n                    utils.strip_pad(sample[\"target\"][i, :], tgt_dict.pad()).int().cpu()\n                )\n\n            # Either retrieve the original sentences or regenerate them from tokens.\n            if align_dict is not None:\n                src_str = task.dataset(cfg.dataset.gen_subset).src.get_original_text(\n                    sample_id\n                )\n                target_str = task.dataset(cfg.dataset.gen_subset).tgt.get_original_text(\n                    sample_id\n                )\n            else:\n                if src_dict is not None:\n                    src_str = src_dict.string(src_tokens, cfg.common_eval.post_process)\n                else:\n                    src_str = \"\"\n                if has_target:\n                    target_str = tgt_dict.string(\n                        target_tokens,\n                        cfg.common_eval.post_process,\n                        escape_unk=True,\n                        extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n                            generator\n                        ),\n                    )\n\n            src_str = decode_fn(src_str)\n            if has_target:\n                target_str = decode_fn(target_str)\n\n            if not cfg.common_eval.quiet:\n                if src_dict is not None:\n                    print(\"S-{}\\t{}\".format(sample_id, src_str), file=output_file)\n                if has_target:\n                    print(\"T-{}\\t{}\".format(sample_id, target_str), file=output_file)\n\n            # Process top predictions\n            for j, hypo in enumerate(hypos[i][: cfg.generation.nbest]):\n                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n                    src_str=src_str,\n                    alignment=hypo[\"alignment\"],\n                    align_dict=align_dict,\n                    tgt_dict=tgt_dict,\n                    remove_bpe=cfg.common_eval.post_process,\n                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n                )\n                detok_hypo_str = decode_fn(hypo_str)\n                if not cfg.common_eval.quiet:\n                    score = hypo[\"score\"] / math.log(2)  # convert to base 2\n                    # original hypothesis (after tokenization and BPE)\n                    print(\n                        \"H-{}\\t{}\\t{}\".format(sample_id, score, hypo_str),\n                        file=output_file,\n                    )\n                    # detokenized hypothesis\n                    print(\n                        \"D-{}\\t{}\\t{}\".format(sample_id, score, detok_hypo_str),\n                        file=output_file,\n                    )\n                    print(\n                        \"P-{}\\t{}\".format(\n                            sample_id,\n                            \" \".join(\n                                map(\n                                    lambda x: \"{:.4f}\".format(x),\n                                    # convert from base e to base 2\n                                    hypo[\"positional_scores\"]\n                                    .div_(math.log(2))\n                                    .tolist(),\n                                )\n                            ),\n                        ),\n                        file=output_file,\n                    )\n\n                    if cfg.generation.print_alignment == \"hard\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\n                                        \"{}-{}\".format(src_idx, tgt_idx)\n                                        for src_idx, tgt_idx in alignment\n                                    ]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n                    if cfg.generation.print_alignment == \"soft\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\",\".join(src_probs) for src_probs in alignment]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.print_step:\n                        print(\n                            \"I-{}\\t{}\".format(sample_id, hypo[\"steps\"]),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.retain_iter_history:\n                        for step, h in enumerate(hypo[\"history\"]):\n                            _, h_str, _ = utils.post_process_prediction(\n                                hypo_tokens=h[\"tokens\"].int().cpu(),\n                                src_str=src_str,\n                                alignment=None,\n                                align_dict=None,\n                                tgt_dict=tgt_dict,\n                                remove_bpe=None,\n                            )\n                            print(\n                                \"E-{}_{}\\t{}\".format(sample_id, step, h_str),\n                                file=output_file,\n                            )\n\n                # Score only the top hypothesis\n                if has_target and j == 0:\n                    if (\n                        align_dict is not None\n                        or cfg.common_eval.post_process is not None\n                    ):\n                        # Convert back to tokens for evaluation with unk replacement and/or without BPE\n                        target_tokens = tgt_dict.encode_line(\n                            target_str, add_if_not_exist=True\n                        )\n                        hypo_tokens = tgt_dict.encode_line(\n                            detok_hypo_str, add_if_not_exist=True\n                        )\n                    if hasattr(scorer, \"add_string\"):\n                        scorer.add_string(target_str, detok_hypo_str)\n                    else:\n                        scorer.add(target_tokens, hypo_tokens)\n\n        wps_meter.update(num_generated_tokens)\n        progress.log({\"wps\": round(wps_meter.avg)})\n        num_sentences += (\n            sample[\"nsentences\"] if \"nsentences\" in sample else sample[\"id\"].numel()\n        )\n\n    logger.info(\"NOTE: hypothesis and token scores are output in base 2\")\n    logger.info(\n        \"Translated {:,} sentences ({:,} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)\".format(\n            num_sentences,\n            gen_timer.n,\n            gen_timer.sum,\n            num_sentences / gen_timer.sum,\n            1.0 / gen_timer.avg,\n        )\n    )\n    if has_target:\n        if cfg.bpe and not cfg.generation.sacrebleu:\n            if cfg.common_eval.post_process:\n                logger.warning(\n                    \"BLEU score is being computed by splitting detokenized string on spaces, this is probably not what you want. Use --sacrebleu for standard 13a BLEU tokenization\"\n                )\n            else:\n                logger.warning(\n                    \"If you are using BPE on the target side, the BLEU score is computed on BPE tokens, not on proper words.  Use --sacrebleu for standard 13a BLEU tokenization\"\n                )\n        # use print to be consistent with other main outputs: S-, H-, T-, D- and so on\n        print(\n            \"Generate {} with beam={}: {}\".format(\n                cfg.dataset.gen_subset, cfg.generation.beam, scorer.result_string()\n            ),\n            file=output_file,\n        )\n\n    return scorer", "\n\ndef cli_main():\n    parser = options.get_generation_parser()\n    # TODO: replace this workaround with refactoring of `AudioPretraining`\n    parser.add_argument(\n        \"--arch\",\n        \"-a\",\n        metavar=\"ARCH\",\n        default=\"wav2vec2\",\n        help=\"Model architecture. For constructing tasks that rely on \"\n        \"model args (e.g. `AudioPretraining`)\",\n    )\n    args = options.parse_args_and_arch(parser)\n    main(args)", "\n\nif __name__ == \"__main__\":\n    cli_main()"]}
{"filename": "xlmr/src/nar_generate.py", "chunked_list": ["#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nTranslate pre-processed data with a trained model.\n\"\"\"\n\nimport ast", "\nimport ast\nimport logging\nimport math\nimport os\nimport sys\nfrom argparse import Namespace\nfrom itertools import chain\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig\n\nfrom fairseq import checkpoint_utils, options, scoring, tasks, utils\nfrom fairseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom fairseq.logging import progress_bar\nfrom fairseq.logging.meters import StopwatchMeter, TimeMeter\nimport utils as distributed_utils", "from fairseq.logging.meters import StopwatchMeter, TimeMeter\nimport utils as distributed_utils\n\n\ndef main(cfg: DictConfig):\n\n    if isinstance(cfg, Namespace):\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    assert cfg.common_eval.path is not None, \"--path required for generation!\"\n    assert (\n        not cfg.generation.sampling or cfg.generation.nbest == cfg.generation.beam\n    ), \"--sampling requires --nbest to be equal to --beam\"\n    assert (\n        cfg.generation.replace_unk is None or cfg.dataset.dataset_impl == \"raw\"\n    ), \"--replace-unk requires a raw text dataset (--dataset-impl=raw)\"\n\n    if cfg.common_eval.results_path is not None:\n        os.makedirs(cfg.common_eval.results_path, exist_ok=True)\n        output_path = os.path.join(\n            cfg.common_eval.results_path,\n            \"generate-{}.txt\".format(cfg.dataset.gen_subset),\n        )\n        with open(output_path, \"w\", buffering=1, encoding=\"utf-8\") as h:\n            return _main(cfg, h)\n    else:\n        return _main(cfg, sys.stdout)", "\n\ndef get_symbols_to_strip_from_output(generator):\n    if hasattr(generator, \"symbols_to_strip_from_output\"):\n        return generator.symbols_to_strip_from_output\n    else:\n        return {generator.eos}\n\n\ndef _main(cfg: DictConfig, output_file):\n    logging.basicConfig(\n        format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n        stream=output_file,\n    )\n    logger = logging.getLogger(\"fairseq_cli.generate\")\n\n    utils.import_user_module(cfg.common)\n\n    if cfg.dataset.max_tokens is None and cfg.dataset.batch_size is None:\n        cfg.dataset.max_tokens = 12000\n    logger.info(cfg)\n\n    # Fix seed for stochastic decoding\n    if cfg.common.seed is not None and not cfg.generation.no_seed_provided:\n        np.random.seed(cfg.common.seed)\n        utils.set_torch_seed(cfg.common.seed)\n\n    use_cuda = torch.cuda.is_available() and not cfg.common.cpu\n\n    # Load dataset splits\n    task = tasks.setup_task(cfg.task)\n\n    # Set dictionaries\n    try:\n        src_dict = getattr(task, \"source_dictionary\", None)\n    except NotImplementedError:\n        src_dict = None\n    tgt_dict = task.target_dictionary\n\n    overrides = ast.literal_eval(cfg.common_eval.model_overrides)\n\n    # Load ensemble\n    logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n    models, saved_cfg = checkpoint_utils.load_model_ensemble(\n        utils.split_paths(cfg.common_eval.path),\n        arg_overrides=overrides,\n        task=task,\n        suffix=cfg.checkpoint.checkpoint_suffix,\n        strict=(cfg.checkpoint.checkpoint_shard_count == 1),\n        num_shards=cfg.checkpoint.checkpoint_shard_count,\n    )\n\n    # loading the dataset should happen after the checkpoint has been loaded so we can give it the saved task config\n    task.load_dataset(cfg.dataset.gen_subset, task_cfg=saved_cfg.task)\n\n    if cfg.generation.lm_path is not None:\n        overrides[\"data\"] = cfg.task.data\n\n        try:\n            lms, _ = checkpoint_utils.load_model_ensemble(\n                [cfg.generation.lm_path], arg_overrides=overrides, task=None\n            )\n        except:\n            logger.warning(\n                f\"Failed to load language model! Please make sure that the language model dict is the same \"\n                f\"as target dict and is located in the data dir ({cfg.task.data})\"\n            )\n            raise\n\n        assert len(lms) == 1\n    else:\n        lms = [None]\n\n    # Optimize ensemble for generation\n    for model in chain(models, lms):\n        if model is None:\n            continue\n        if cfg.common.fp16:\n            model.half()\n        if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n            model.cuda()\n        model.prepare_for_inference_(cfg)\n\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(cfg.generation.replace_unk)\n\n    # Load dataset (possibly sharded)\n    itr = task.get_batch_iterator(\n        dataset=task.dataset(cfg.dataset.gen_subset),\n        max_tokens=cfg.dataset.max_tokens,\n        max_sentences=cfg.dataset.batch_size,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), *[m.max_positions() for m in models]\n        ),\n        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n        seed=cfg.common.seed,\n        num_shards=cfg.distributed_training.distributed_world_size,\n        shard_id=cfg.distributed_training.distributed_rank,\n        num_workers=cfg.dataset.num_workers,\n        data_buffer_size=cfg.dataset.data_buffer_size,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_interval=cfg.common.log_interval,\n        default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n    )\n\n    # Initialize generator\n    gen_timer = StopwatchMeter()\n\n    extra_gen_cls_kwargs = {\"lm_model\": lms[0], \"lm_weight\": cfg.generation.lm_weight}\n    generator = task.build_generator(\n        models, cfg.generation, extra_gen_cls_kwargs=extra_gen_cls_kwargs\n    )\n\n    # Handle tokenization and BPE\n    tokenizer = task.build_tokenizer(cfg.tokenizer)\n    bpe = task.build_bpe(cfg.bpe)\n\n    def decode_fn(x):\n        if bpe is not None:\n            x = bpe.decode(x)\n        if tokenizer is not None:\n            x = tokenizer.decode(x)\n        return x\n\n    scorer = scoring.build_scorer(cfg.scoring, tgt_dict)\n\n    num_sentences = 0\n    has_target = True\n    wps_meter = TimeMeter()\n    for sample in progress:\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n        if \"net_input\" not in sample:\n            continue\n\n        prefix_tokens = None\n        if cfg.generation.prefix_size > 0:\n            prefix_tokens = sample[\"target\"][:, : cfg.generation.prefix_size]\n\n        constraints = None\n        if \"constraints\" in sample:\n            constraints = sample[\"constraints\"]\n\n        gen_timer.start()\n        hypos = task.inference_step(\n            generator,\n            models,\n            sample,\n            prefix_tokens=prefix_tokens,\n            constraints=constraints,\n        )\n        num_generated_tokens = sum(len(h[0][\"tokens\"]) for h in hypos)\n        gen_timer.stop(num_generated_tokens)\n\n        for i, sample_id in enumerate(sample[\"id\"].tolist()):\n            has_target = sample[\"target\"] is not None\n\n            # Remove padding\n            if \"src_tokens\" in sample[\"net_input\"]:\n                src_tokens = utils.strip_pad(\n                    sample[\"net_input\"][\"src_tokens\"][i, :], tgt_dict.pad()\n                )\n            else:\n                src_tokens = None\n\n            target_tokens = None\n            if has_target:\n                target_tokens = (\n                    utils.strip_pad(sample[\"net_input\"][\"xlmr_tgt_item\"][i, :], tgt_dict.pad()).int().cpu()\n                )\n\n            # Either retrieve the original sentences or regenerate them from tokens.\n            if align_dict is not None:\n                src_str = task.dataset(cfg.dataset.gen_subset).src.get_original_text(\n                    sample_id\n                )\n                target_str = task.dataset(cfg.dataset.gen_subset).tgt.get_original_text(\n                    sample_id\n                )\n            else:\n                if src_dict is not None:\n                    src_str = src_dict.string(src_tokens, cfg.common_eval.post_process)\n                else:\n                    src_str = \"\"\n                if has_target:\n                    target_str = tgt_dict.string(\n                        target_tokens,\n                        cfg.common_eval.post_process,\n                        escape_unk=True,\n                        extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n                            generator\n                        ),\n                    )\n\n            src_str = decode_fn(src_str)\n            if has_target:\n                target_str = decode_fn(target_str)\n\n            if not cfg.common_eval.quiet:\n                if src_dict is not None:\n                    print(\"S-{}\\t{}\".format(sample_id, src_str), file=output_file)\n                if has_target:\n                    print(\"T-{}\\t{}\".format(sample_id, target_str), file=output_file)\n\n            # Process top predictions\n            for j, hypo in enumerate(hypos[i][: cfg.generation.nbest]):\n                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n                    src_str=src_str,\n                    alignment=hypo[\"alignment\"],\n                    align_dict=align_dict,\n                    tgt_dict=tgt_dict,\n                    remove_bpe=cfg.common_eval.post_process,\n                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n                )\n                detok_hypo_str = decode_fn(hypo_str)\n                if not cfg.common_eval.quiet:\n                    score = hypo[\"score\"] / math.log(2)  # convert to base 2\n                    # original hypothesis (after tokenization and BPE)\n                    print(\n                        \"H-{}\\t{}\\t{}\".format(sample_id, score, hypo_str),\n                        file=output_file,\n                    )\n                    # detokenized hypothesis\n                    print(\n                        \"D-{}\\t{}\\t{}\".format(sample_id, score, detok_hypo_str),\n                        file=output_file,\n                    )\n                    print(\n                        \"P-{}\\t{}\".format(\n                            sample_id,\n                            \" \".join(\n                                map(\n                                    lambda x: \"{:.4f}\".format(x),\n                                    # convert from base e to base 2\n                                    hypo[\"positional_scores\"]\n                                    .div_(math.log(2))\n                                    .tolist(),\n                                )\n                            ),\n                        ),\n                        file=output_file,\n                    )\n\n                    if cfg.generation.print_alignment == \"hard\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\n                                        \"{}-{}\".format(src_idx, tgt_idx)\n                                        for src_idx, tgt_idx in alignment\n                                    ]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n                    if cfg.generation.print_alignment == \"soft\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\",\".join(src_probs) for src_probs in alignment]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.print_step:\n                        print(\n                            \"I-{}\\t{}\".format(sample_id, hypo[\"steps\"]),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.retain_iter_history:\n                        for step, h in enumerate(hypo[\"history\"]):\n                            _, h_str, _ = utils.post_process_prediction(\n                                hypo_tokens=h[\"tokens\"].int().cpu(),\n                                src_str=src_str,\n                                alignment=None,\n                                align_dict=None,\n                                tgt_dict=tgt_dict,\n                                remove_bpe=None,\n                            )\n                            print(\n                                \"E-{}_{}\\t{}\".format(sample_id, step, h_str),\n                                file=output_file,\n                            )\n\n                # Score only the top hypothesis\n                if has_target and j == 0:\n                    if (\n                        align_dict is not None\n                        or cfg.common_eval.post_process is not None\n                    ):\n                        # Convert back to tokens for evaluation with unk replacement and/or without BPE\n                        target_tokens = tgt_dict.encode_line(\n                            target_str, add_if_not_exist=True\n                        )\n                        hypo_tokens = tgt_dict.encode_line(\n                            detok_hypo_str, add_if_not_exist=True\n                        )\n                    if hasattr(scorer, \"add_string\"):\n                        scorer.add_string(target_str, detok_hypo_str)\n                    else:\n                        scorer.add(target_tokens, hypo_tokens)\n\n        wps_meter.update(num_generated_tokens)\n        progress.log({\"wps\": round(wps_meter.avg)})\n        num_sentences += (\n            sample[\"nsentences\"] if \"nsentences\" in sample else sample[\"id\"].numel()\n        )\n\n    logger.info(\"NOTE: hypothesis and token scores are output in base 2\")\n    logger.info(\n        \"Translated {:,} sentences ({:,} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)\".format(\n            num_sentences,\n            gen_timer.n,\n            gen_timer.sum,\n            num_sentences / gen_timer.sum,\n            1.0 / gen_timer.avg,\n        )\n    )\n    if has_target:\n        if cfg.bpe and not cfg.generation.sacrebleu:\n            if cfg.common_eval.post_process:\n                logger.warning(\n                    \"BLEU score is being computed by splitting detokenized string on spaces, this is probably not what you want. Use --sacrebleu for standard 13a BLEU tokenization\"\n                )\n            else:\n                logger.warning(\n                    \"If you are using BPE on the target side, the BLEU score is computed on BPE tokens, not on proper words.  Use --sacrebleu for standard 13a BLEU tokenization\"\n                )\n        # use print to be consistent with other main outputs: S-, H-, T-, D- and so on\n        print(\n            \"Generate {} with beam={}: {}\".format(\n                cfg.dataset.gen_subset, cfg.generation.beam, scorer.result_string()\n            ),\n            file=output_file,\n        )\n\n    return scorer", "\ndef _main(cfg: DictConfig, output_file):\n    logging.basicConfig(\n        format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n        stream=output_file,\n    )\n    logger = logging.getLogger(\"fairseq_cli.generate\")\n\n    utils.import_user_module(cfg.common)\n\n    if cfg.dataset.max_tokens is None and cfg.dataset.batch_size is None:\n        cfg.dataset.max_tokens = 12000\n    logger.info(cfg)\n\n    # Fix seed for stochastic decoding\n    if cfg.common.seed is not None and not cfg.generation.no_seed_provided:\n        np.random.seed(cfg.common.seed)\n        utils.set_torch_seed(cfg.common.seed)\n\n    use_cuda = torch.cuda.is_available() and not cfg.common.cpu\n\n    # Load dataset splits\n    task = tasks.setup_task(cfg.task)\n\n    # Set dictionaries\n    try:\n        src_dict = getattr(task, \"source_dictionary\", None)\n    except NotImplementedError:\n        src_dict = None\n    tgt_dict = task.target_dictionary\n\n    overrides = ast.literal_eval(cfg.common_eval.model_overrides)\n\n    # Load ensemble\n    logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n    models, saved_cfg = checkpoint_utils.load_model_ensemble(\n        utils.split_paths(cfg.common_eval.path),\n        arg_overrides=overrides,\n        task=task,\n        suffix=cfg.checkpoint.checkpoint_suffix,\n        strict=(cfg.checkpoint.checkpoint_shard_count == 1),\n        num_shards=cfg.checkpoint.checkpoint_shard_count,\n    )\n\n    # loading the dataset should happen after the checkpoint has been loaded so we can give it the saved task config\n    task.load_dataset(cfg.dataset.gen_subset, task_cfg=saved_cfg.task)\n\n    if cfg.generation.lm_path is not None:\n        overrides[\"data\"] = cfg.task.data\n\n        try:\n            lms, _ = checkpoint_utils.load_model_ensemble(\n                [cfg.generation.lm_path], arg_overrides=overrides, task=None\n            )\n        except:\n            logger.warning(\n                f\"Failed to load language model! Please make sure that the language model dict is the same \"\n                f\"as target dict and is located in the data dir ({cfg.task.data})\"\n            )\n            raise\n\n        assert len(lms) == 1\n    else:\n        lms = [None]\n\n    # Optimize ensemble for generation\n    for model in chain(models, lms):\n        if model is None:\n            continue\n        if cfg.common.fp16:\n            model.half()\n        if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n            model.cuda()\n        model.prepare_for_inference_(cfg)\n\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(cfg.generation.replace_unk)\n\n    # Load dataset (possibly sharded)\n    itr = task.get_batch_iterator(\n        dataset=task.dataset(cfg.dataset.gen_subset),\n        max_tokens=cfg.dataset.max_tokens,\n        max_sentences=cfg.dataset.batch_size,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), *[m.max_positions() for m in models]\n        ),\n        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n        seed=cfg.common.seed,\n        num_shards=cfg.distributed_training.distributed_world_size,\n        shard_id=cfg.distributed_training.distributed_rank,\n        num_workers=cfg.dataset.num_workers,\n        data_buffer_size=cfg.dataset.data_buffer_size,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_interval=cfg.common.log_interval,\n        default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n    )\n\n    # Initialize generator\n    gen_timer = StopwatchMeter()\n\n    extra_gen_cls_kwargs = {\"lm_model\": lms[0], \"lm_weight\": cfg.generation.lm_weight}\n    generator = task.build_generator(\n        models, cfg.generation, extra_gen_cls_kwargs=extra_gen_cls_kwargs\n    )\n\n    # Handle tokenization and BPE\n    tokenizer = task.build_tokenizer(cfg.tokenizer)\n    bpe = task.build_bpe(cfg.bpe)\n\n    def decode_fn(x):\n        if bpe is not None:\n            x = bpe.decode(x)\n        if tokenizer is not None:\n            x = tokenizer.decode(x)\n        return x\n\n    scorer = scoring.build_scorer(cfg.scoring, tgt_dict)\n\n    num_sentences = 0\n    has_target = True\n    wps_meter = TimeMeter()\n    for sample in progress:\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n        if \"net_input\" not in sample:\n            continue\n\n        prefix_tokens = None\n        if cfg.generation.prefix_size > 0:\n            prefix_tokens = sample[\"target\"][:, : cfg.generation.prefix_size]\n\n        constraints = None\n        if \"constraints\" in sample:\n            constraints = sample[\"constraints\"]\n\n        gen_timer.start()\n        hypos = task.inference_step(\n            generator,\n            models,\n            sample,\n            prefix_tokens=prefix_tokens,\n            constraints=constraints,\n        )\n        num_generated_tokens = sum(len(h[0][\"tokens\"]) for h in hypos)\n        gen_timer.stop(num_generated_tokens)\n\n        for i, sample_id in enumerate(sample[\"id\"].tolist()):\n            has_target = sample[\"target\"] is not None\n\n            # Remove padding\n            if \"src_tokens\" in sample[\"net_input\"]:\n                src_tokens = utils.strip_pad(\n                    sample[\"net_input\"][\"src_tokens\"][i, :], tgt_dict.pad()\n                )\n            else:\n                src_tokens = None\n\n            target_tokens = None\n            if has_target:\n                target_tokens = (\n                    utils.strip_pad(sample[\"net_input\"][\"xlmr_tgt_item\"][i, :], tgt_dict.pad()).int().cpu()\n                )\n\n            # Either retrieve the original sentences or regenerate them from tokens.\n            if align_dict is not None:\n                src_str = task.dataset(cfg.dataset.gen_subset).src.get_original_text(\n                    sample_id\n                )\n                target_str = task.dataset(cfg.dataset.gen_subset).tgt.get_original_text(\n                    sample_id\n                )\n            else:\n                if src_dict is not None:\n                    src_str = src_dict.string(src_tokens, cfg.common_eval.post_process)\n                else:\n                    src_str = \"\"\n                if has_target:\n                    target_str = tgt_dict.string(\n                        target_tokens,\n                        cfg.common_eval.post_process,\n                        escape_unk=True,\n                        extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n                            generator\n                        ),\n                    )\n\n            src_str = decode_fn(src_str)\n            if has_target:\n                target_str = decode_fn(target_str)\n\n            if not cfg.common_eval.quiet:\n                if src_dict is not None:\n                    print(\"S-{}\\t{}\".format(sample_id, src_str), file=output_file)\n                if has_target:\n                    print(\"T-{}\\t{}\".format(sample_id, target_str), file=output_file)\n\n            # Process top predictions\n            for j, hypo in enumerate(hypos[i][: cfg.generation.nbest]):\n                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n                    src_str=src_str,\n                    alignment=hypo[\"alignment\"],\n                    align_dict=align_dict,\n                    tgt_dict=tgt_dict,\n                    remove_bpe=cfg.common_eval.post_process,\n                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n                )\n                detok_hypo_str = decode_fn(hypo_str)\n                if not cfg.common_eval.quiet:\n                    score = hypo[\"score\"] / math.log(2)  # convert to base 2\n                    # original hypothesis (after tokenization and BPE)\n                    print(\n                        \"H-{}\\t{}\\t{}\".format(sample_id, score, hypo_str),\n                        file=output_file,\n                    )\n                    # detokenized hypothesis\n                    print(\n                        \"D-{}\\t{}\\t{}\".format(sample_id, score, detok_hypo_str),\n                        file=output_file,\n                    )\n                    print(\n                        \"P-{}\\t{}\".format(\n                            sample_id,\n                            \" \".join(\n                                map(\n                                    lambda x: \"{:.4f}\".format(x),\n                                    # convert from base e to base 2\n                                    hypo[\"positional_scores\"]\n                                    .div_(math.log(2))\n                                    .tolist(),\n                                )\n                            ),\n                        ),\n                        file=output_file,\n                    )\n\n                    if cfg.generation.print_alignment == \"hard\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\n                                        \"{}-{}\".format(src_idx, tgt_idx)\n                                        for src_idx, tgt_idx in alignment\n                                    ]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n                    if cfg.generation.print_alignment == \"soft\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\",\".join(src_probs) for src_probs in alignment]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.print_step:\n                        print(\n                            \"I-{}\\t{}\".format(sample_id, hypo[\"steps\"]),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.retain_iter_history:\n                        for step, h in enumerate(hypo[\"history\"]):\n                            _, h_str, _ = utils.post_process_prediction(\n                                hypo_tokens=h[\"tokens\"].int().cpu(),\n                                src_str=src_str,\n                                alignment=None,\n                                align_dict=None,\n                                tgt_dict=tgt_dict,\n                                remove_bpe=None,\n                            )\n                            print(\n                                \"E-{}_{}\\t{}\".format(sample_id, step, h_str),\n                                file=output_file,\n                            )\n\n                # Score only the top hypothesis\n                if has_target and j == 0:\n                    if (\n                        align_dict is not None\n                        or cfg.common_eval.post_process is not None\n                    ):\n                        # Convert back to tokens for evaluation with unk replacement and/or without BPE\n                        target_tokens = tgt_dict.encode_line(\n                            target_str, add_if_not_exist=True\n                        )\n                        hypo_tokens = tgt_dict.encode_line(\n                            detok_hypo_str, add_if_not_exist=True\n                        )\n                    if hasattr(scorer, \"add_string\"):\n                        scorer.add_string(target_str, detok_hypo_str)\n                    else:\n                        scorer.add(target_tokens, hypo_tokens)\n\n        wps_meter.update(num_generated_tokens)\n        progress.log({\"wps\": round(wps_meter.avg)})\n        num_sentences += (\n            sample[\"nsentences\"] if \"nsentences\" in sample else sample[\"id\"].numel()\n        )\n\n    logger.info(\"NOTE: hypothesis and token scores are output in base 2\")\n    logger.info(\n        \"Translated {:,} sentences ({:,} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)\".format(\n            num_sentences,\n            gen_timer.n,\n            gen_timer.sum,\n            num_sentences / gen_timer.sum,\n            1.0 / gen_timer.avg,\n        )\n    )\n    if has_target:\n        if cfg.bpe and not cfg.generation.sacrebleu:\n            if cfg.common_eval.post_process:\n                logger.warning(\n                    \"BLEU score is being computed by splitting detokenized string on spaces, this is probably not what you want. Use --sacrebleu for standard 13a BLEU tokenization\"\n                )\n            else:\n                logger.warning(\n                    \"If you are using BPE on the target side, the BLEU score is computed on BPE tokens, not on proper words.  Use --sacrebleu for standard 13a BLEU tokenization\"\n                )\n        # use print to be consistent with other main outputs: S-, H-, T-, D- and so on\n        print(\n            \"Generate {} with beam={}: {}\".format(\n                cfg.dataset.gen_subset, cfg.generation.beam, scorer.result_string()\n            ),\n            file=output_file,\n        )\n\n    return scorer", "\n\ndef cli_main():\n    parser = options.get_generation_parser()\n    # TODO: replace this workaround with refactoring of `AudioPretraining`\n    parser.add_argument(\n        \"--arch\",\n        \"-a\",\n        metavar=\"ARCH\",\n        default=\"wav2vec2\",\n        help=\"Model architecture. For constructing tasks that rely on \"\n        \"model args (e.g. `AudioPretraining`)\",\n    )\n    args = options.parse_args_and_arch(parser)\n    if args.model_parallel_size > 1:\n        print(\"run megatron mode...\")\n        distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n    else:\n        main(args)", "\n\nif __name__ == \"__main__\":\n    cli_main()\n"]}
{"filename": "xlmr/src/webapp.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom model.llama_model import LLaMA\nimport argparse\nimport gradio as gr\n", "import gradio as gr\n\n\n\ndef sample_demo(alpaca):\n\n    @torch.no_grad()\n    def process(prompt):\n        prompt_text = \"## Instruction:\\n{}\\n\\n## Response:\".format(prompt)\n        print(\"Received:\\n\", prompt_text)\n        eval_kwargs = dict(beam=1, sampling=True, sampling_topp=0.95, temperature=0.8, min_len=512)\n        prompts = [prompt_text]\n        results = alpaca.sample(prompts, **eval_kwargs)[0]\n        print(\"Generated:\\n\", results[0])\n        return str(results[0])\n\n    demo = gr.Interface(\n        title = \"Efficient Alpaca\",\n        thumbnail = \"https://github.com/dropreg/efficient_alpaca/blob/main/efficient_alpaca_logo.PNG\",\n        fn = process,\n        inputs = gr.Textbox(lines=10, placeholder=\"Your prompt here...\"),\n        outputs = \"text\",\n    )\n\n    demo.launch(share=True)", "\ndef demo(alpaca):\n\n    @torch.no_grad()\n    def process(prompt, temperature, topp):\n        prompt_text = \"## Instruction:\\n{}\\n\\n## Response:\".format(prompt)\n        print(\"Received:\\n\", prompt_text)\n        eval_kwargs = dict(sampling=True, sampling_topp=topp, temperature=temperature)\n        prompts = [prompt_text]\n        results = alpaca.sample(prompts, **eval_kwargs)[0]\n        print(\"Generated:\\n\", results[0])\n        return str(results[0])\n\n    with gr.Blocks() as demo:\n        gr.Markdown(\n            \"\"\"\n            <p align=\"center\" width=\"100%\">\n            <img src=\"https://github.com/dropreg/efficient_alpaca/raw/main/efficient_alpaca_logo.PNG\" alt=\"Efficient-Alpaca\" style=\"width: 40%; min-width: 200px; display: block; margin: auto;\">\n            </p>\n            \"\"\")\n\n        with gr.Row():\n            with gr.Column():\n                model_input = gr.Textbox(lines=15, placeholder='Input something', label='Input')\n                with gr.Row():\n                    gen = gr.Button(\"Generate\")\n                    clr = gr.Button(\"Clear\")\n\n            outputs = gr.Textbox(lines=15, label='Output')\n                \n        gr.Markdown(\n            \"\"\"\n            Generation Parameter\n            \"\"\")\n        with gr.Row():\n            with gr.Column():\n                temperature = gr.Slider(maximum=1, value=0.8, minimum=0, label='Temperature')\n                topp = gr.Slider(maximum=1, value=0.95, minimum=0, label='Top P')\n        \n        inputs = [model_input, temperature, topp]\n        gen.click(fn=process, inputs=inputs, outputs=outputs)\n        clr.click(fn=lambda value: gr.update(value=\"\"), inputs=clr, outputs=model_input)\n                \n        gr.Markdown(\n            \"\"\"\n            Our project can be found from [Efficient Alpaca](https://github.com/dropreg/efficient_alpaca)\n            \"\"\")\n\n    demo.launch(share=True)", "\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-dir\",\n        required=True,\n        type=str,\n        default=\"alpaca_lora\",\n        help=\"path containing model file and src_dict.txt\",\n    )\n    parser.add_argument(\n        \"--model-file\",\n        default=\"checkpoint_best.pt\",\n        help=\"where in model_dir are weights saved\",\n    )\n    parser.add_argument(\n        \"--lora-model-inf\",\n        default=\"\",\n        help=\"where in model_dir are weights saved\",\n    )\n    parser.add_argument(\n        \"--lora-tuning\",\n        action=\"store_true\",\n        default=False,\n        help=\"if true use XSUM_KWARGS else CNN_KWARGS\",\n    )\n\n    parser.add_argument(\"--bpe\",)\n    parser.add_argument(\"--sentencepiece-model\")\n    args = parser.parse_args()\n    \n    kwargs = {\n        \"user_dir\": \"alpaca/src\", \n        \"lora_model_inf\": args.lora_model_inf,\n        \"bpe\": args.bpe,\n        \"sentencepiece_model\": args.sentencepiece_model,\n        \"source_lang\": 'src',\n        \"target_lang\": 'tgt',\n        \"lora_tuning\": args.lora_tuning,\n        \"task\": \"seq2seq_lora_task\",\n    }\n    alpaca = LLaMA.from_pretrained(\n        model_name_or_path=args.model_dir,\n        checkpoint_file=args.model_file,\n        **kwargs,\n    )\n\n    alpaca = alpaca.eval()\n    if torch.cuda.is_available():\n        alpaca = alpaca.half().cuda()\n    \n    demo(alpaca)", ""]}
{"filename": "xlmr/src/megatron_trainer.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nTrain a network across multiple GPUs.\n\"\"\"\n\nfrom fairseq.dataclass.configs import FairseqConfig", "\nfrom fairseq.dataclass.configs import FairseqConfig\nimport utils as distributed_utils\nfrom trainer import Trainer\nfrom fairscale.nn.model_parallel.random import get_cuda_rng_tracker\n\nclass MegatronTrainer(Trainer):\n    \"\"\"Main class for model parallel with data parallel training.\"\"\"\n\n    def __init__(self, cfg: FairseqConfig, task, model, criterion, **kwargs):\n        super().__init__(cfg, task, model, criterion, **kwargs)\n    \n    def clip_grad_norm(self, clip_norm):\n        def _aggregate_model_parallel_grad_norm(total_norm):\n            total_norm = total_norm**2\n            distributed_utils.all_reduce(\n                total_norm, group=distributed_utils.get_model_parallel_group()\n            )\n            total_norm = total_norm**0.5\n            return total_norm\n\n        return self.optimizer.clip_grad_norm(\n            clip_norm,\n            aggregate_norm_fn=_aggregate_model_parallel_grad_norm,\n        )\n    \n    def save_checkpoint(self, filename, extra_state):\n        \"\"\"Save all training state in a checkpoint file.\"\"\"\n        extra_state[\"rng_tracker_states\"] = get_cuda_rng_tracker().get_states()\n        super().save_checkpoint(filename, extra_state)\n\n    def load_checkpoint(\n        self,\n        filename,\n        reset_optimizer=False,\n        reset_lr_scheduler=False,\n        optimizer_overrides=None,\n        reset_meters=False,\n    ):\n        extra_state = super().load_checkpoint(\n            filename,\n            reset_optimizer=reset_optimizer,\n            reset_lr_scheduler=reset_lr_scheduler,\n            optimizer_overrides=optimizer_overrides,\n            reset_meters=reset_meters,\n        )\n        if extra_state is not None and \"rng_tracker_states\" in extra_state:\n            get_cuda_rng_tracker().set_states(extra_state[\"rng_tracker_states\"])\n        return extra_state", ""]}
{"filename": "xlmr/src/inference.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom model.xlmr_model import NARXLMR\n\nimport argparse\nimport logging", "import argparse\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@torch.no_grad()\ndef generate(xlmr_model):\n    \n    # load from txt\n    # prompts = [\n    #     \"\u7ed9\u6211\u8bb2\u4e00\u4e2a\u66f9\u64cd\u7684\u6545\u4e8b.\",\n    #     \"\u4e2d\u56fd\u7684\u6700\u597d\u7684\u5341\u6240\u5927\u5b66\u6392\u540d\u662f\u4ec0\u4e48\uff1f\",\n    # ]\n\n    prompts = [\n        \"Who is the author of the Lord of the Rings\",\n        \"Give me a story about Snow White\",\n        \"Give me a story about cao cao.\",\n        \"Write a short story in third person narration about a protagonist who has to make an important career decision.\",\n    ]\n\n\n    # load from files\n    # prompts = open(\"alpaca/scripts/assert/test.src\").readlines()\n\n    eval_kwargs = dict(sampling=True, sampling_topp=0.95, temperature=0.8)\n    for prompt in prompts:\n        print(\"-----\" * 20)\n        prompt_text = prompt\n        print(prompt_text)\n        output = xlmr_model.sample([prompt_text], **eval_kwargs)[0][0]\n        print(output)", "\ndef main():\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-dir\",\n        required=True,\n        type=str,\n        default=\"\",\n        help=\"path containing model file\",\n    )\n    parser.add_argument(\n        \"--model-file\",\n        default=\"\",\n        help=\"where in model_dir are weights saved\",\n    )\n    parser.add_argument(\n        \"--lora-model-inf\",\n        default=\"\",\n        help=\"where in model_dir are weights saved\",\n    )\n    parser.add_argument(\n        \"--lora-tuning\",\n        action=\"store_true\",\n        default=False,\n        help=\"if true use XSUM_KWARGS else CNN_KWARGS\",\n    )\n\n    parser.add_argument(\"--bpe\",)\n    parser.add_argument(\"--sentencepiece-model\")\n    args = parser.parse_args()\n    \n    kwargs = {\n        \"user_dir\": \"xlmr/src\",\n        \"bpe\": args.bpe,\n        \"sentencepiece_model\": args.sentencepiece_model,\n        \"source_lang\": 'src',\n        \"target_lang\": 'tgt',\n        \"task\": \"seq2seq_ft_task\",\n        \"iter_decode_max_iter\": 9,\n    }\n    xlmr_model = NARXLMR.from_pretrained(\n        model_name_or_path=args.model_dir,\n        checkpoint_file=args.model_file,\n        **kwargs,\n    )\n    xlmr_model = xlmr_model.eval()\n    if torch.cuda.is_available():\n        xlmr_model = xlmr_model.half().cuda()\n    \n    generate(xlmr_model)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "xlmr/src/__init__.py", "chunked_list": ["\ntry:\n    from .model import xlmr_model\nexcept:\n    print(\"have been load xlmr model\")\nfrom .loss import cmlm_loss\nfrom .task import seq2seq_ft_task\nfrom .fsdp import cpu_adam, fully_sharded_data_parallel\n", ""]}
{"filename": "xlmr/src/utils.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport io\nimport logging\nimport os\nimport pickle\nimport random", "import pickle\nimport random\nimport socket\nimport struct\nimport subprocess\nimport warnings\nfrom argparse import Namespace\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional", "from dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional\n\nimport torch\nimport torch.distributed as dist\nfrom fairseq.dataclass.configs import DistributedTrainingConfig, FairseqConfig\nfrom omegaconf import open_dict\n\ntry:\n    import torch_xla.core.xla_model as xm\nexcept ImportError:\n    xm = None", "try:\n    import torch_xla.core.xla_model as xm\nexcept ImportError:\n    xm = None\n\n\n# Flag to indicate if we're using Megatron\n# NOTE: this is a temporary hack until we move away from Megatron's model parallel init\n_USE_MEGATRON = False\n", "_USE_MEGATRON = False\n\n# Whether to use XLA ops (e.g., on TPUs) instead of CUDA ops.\n_USE_XLA = False\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef is_master(cfg: DistributedTrainingConfig):\n    return cfg.distributed_rank == 0", "\ndef is_master(cfg: DistributedTrainingConfig):\n    return cfg.distributed_rank == 0\n\n\ndef infer_init_method(cfg: DistributedTrainingConfig, force_distributed=False):\n    if cfg.distributed_init_method is not None or cfg.tpu:\n        return\n\n    num_pipelines_per_node = None\n    if cfg.pipeline_model_parallel:\n        num_pipeline_devices, num_pipelines_per_node = _pipeline_parallel_pre_init(cfg)\n\n    if cfg.distributed_world_size == 1:\n        return\n    if all(\n        key in os.environ\n        for key in [\"MASTER_ADDR\", \"MASTER_PORT\", \"WORLD_SIZE\", \"RANK\"]\n    ):\n        # support torch.distributed.launch\n        _infer_torch_distributed_launch_init(cfg)\n    else:\n        # we can determine the init method automatically for Slurm\n        if not _infer_slurm_init(cfg, num_pipelines_per_node):\n            if cfg.distributed_port <= 0 or force_distributed:\n                _infer_single_node_init(cfg)\n        elif cfg.distributed_port <= 0:\n            _infer_single_node_init(cfg)\n\n    if cfg.pipeline_model_parallel:\n        _pipeline_parallel_post_init(cfg, num_pipeline_devices, num_pipelines_per_node)\n    elif not cfg.distributed_no_spawn:\n        with open_dict(cfg):\n            cfg.distributed_num_procs = min(\n                torch.cuda.device_count(), cfg.distributed_world_size\n            )\n    else:\n        if cfg.device_id > 0:\n            logger.info(\n                \"setting CUDA device={} on rank {}\".format(\n                    cfg.device_id, cfg.distributed_rank\n                )\n            )\n            torch.cuda.set_device(cfg.device_id)", "\n\ndef _infer_torch_distributed_launch_init(cfg: DistributedTrainingConfig):\n    cfg.distributed_init_method = \"env://\"\n    cfg.distributed_world_size = int(os.environ[\"WORLD_SIZE\"])\n    cfg.distributed_rank = int(os.environ[\"RANK\"])\n    cfg.device_id = cfg.distributed_rank % torch.cuda.device_count()\n    # processes are created by torch.distributed.launch\n    cfg.distributed_no_spawn = True\n", "\n\ndef _infer_slurm_init(cfg: DistributedTrainingConfig, num_pipelines_per_node):\n    node_list = os.environ.get(\"SLURM_STEP_NODELIST\")\n    if node_list is None:\n        node_list = os.environ.get(\"SLURM_JOB_NODELIST\")\n    if node_list is not None:\n        try:\n            hostnames = subprocess.check_output(\n                [\"scontrol\", \"show\", \"hostnames\", node_list]\n            )\n            cfg.distributed_init_method = \"tcp://{host}:{port}\".format(\n                host=hostnames.split()[0].decode(\"utf-8\"),\n                port=cfg.distributed_port,\n            )\n            nnodes = int(os.environ.get(\"SLURM_NNODES\"))\n            ntasks_per_node = os.environ.get(\"SLURM_NTASKS_PER_NODE\")\n            if ntasks_per_node is not None:\n                ntasks_per_node = int(ntasks_per_node)\n            else:\n                ntasks = int(os.environ.get(\"SLURM_NTASKS\"))\n                nnodes = int(os.environ.get(\"SLURM_NNODES\"))\n                assert ntasks % nnodes == 0\n                ntasks_per_node = int(ntasks / nnodes)\n            if ntasks_per_node == 1:\n                gpus_per_node = torch.cuda.device_count()\n                node_id = int(os.environ.get(\"SLURM_NODEID\"))\n                cfg.distributed_rank = node_id * gpus_per_node\n                cfg.distributed_world_size = nnodes * gpus_per_node\n            elif cfg.pipeline_model_parallel:\n                assert ntasks_per_node == num_pipelines_per_node, (\n                    \"SLURM --ntasks-per-node must match number of pipelines per \"\n                    \"node (={})\".format(num_pipelines_per_node)\n                )\n                cfg.distributed_no_spawn = True\n                # For 4-way MP on nodes with 8 GPUs, ranks will be [0, 1] on\n                # the first node, [1, 2] on the second node, etc. This\n                # matches torch.distributed.launch.\n                node_id = int(os.environ.get(\"SLURM_NODEID\"))\n                local_id = int(os.environ.get(\"SLURM_LOCALID\"))\n                cfg.distributed_rank = node_id * num_pipelines_per_node + local_id\n                # In the above example, device_id will always be in [0, 1],\n                # which also matches torch.distributed.launch.\n                cfg.device_id = local_id\n                # We also want to set distributed_world_size to be the total\n                # number of pipelines across all nodes.\n                cfg.distributed_world_size = nnodes * num_pipelines_per_node\n            else:\n                assert (\n                    ntasks_per_node == cfg.distributed_world_size // nnodes\n                ), f\"{ntasks_per_node}, {cfg.distributed_world_size}, {nnodes}\"\n                cfg.distributed_no_spawn = True\n                cfg.distributed_rank = int(os.environ.get(\"SLURM_PROCID\"))\n                cfg.device_id = int(os.environ.get(\"SLURM_LOCALID\"))\n            logger.info(f\"Rank {cfg.distributed_rank}, device_id: {cfg.device_id}\")\n            return True\n        except subprocess.CalledProcessError as e:  # scontrol failed\n            raise e\n        except FileNotFoundError:  # Slurm is not installed\n            pass\n\n    return False", "\n\ndef _infer_single_node_init(cfg: DistributedTrainingConfig):\n    assert (\n        cfg.distributed_world_size <= torch.cuda.device_count()\n    ), f\"world size is {cfg.distributed_world_size} but have {torch.cuda.device_count()} available devices\"\n\n    if cfg.distributed_port <= 0:\n        jobid = os.environ.get(\"SLURM_JOB_ID\")\n        task_id = os.environ.get(\"SLURM_ARRAY_TASK_ID\")\n\n        if jobid is not None:\n            if task_id is not None:\n                jobid += str(task_id)\n            jobid = int(jobid)\n            rng = random.Random(jobid)\n            port = rng.randint(10000, 60000)\n        else:\n            port = random.randint(10000, 60000)\n\n        cfg.distributed_port = port\n    cfg.distributed_init_method = \"tcp://localhost:{port}\".format(\n        port=cfg.distributed_port\n    )", "\n\ndef _pipeline_parallel_pre_init(cfg: DistributedTrainingConfig):\n    from fairseq import utils\n\n    balance_exists = (\n        cfg.pipeline_balance is not None\n        or cfg.pipeline_encoder_balance is not None\n        or cfg.pipeline_decoder_balance is not None\n    )\n    devices_exist = (\n        cfg.pipeline_devices is not None\n        or cfg.pipeline_encoder_devices is not None\n        or cfg.pipeline_decoder_devices is not None\n    )\n    if not balance_exists:\n        raise ValueError(\n            \"--pipeline-balance is currently required for pipeline model parallelism\"\n        )\n    if not devices_exist:\n        raise ValueError(\n            \"--pipeline-devices is currently required for pipeline model parallelism\"\n        )\n\n    cfg.pipeline_balance = utils.eval_str_list(cfg.pipeline_balance, type=int)\n    if cfg.pipeline_devices is not None:\n        cfg.pipeline_devices = utils.eval_str_list(cfg.pipeline_devices, type=int)\n        num_pipeline_devices = len(set(cfg.pipeline_devices))\n    else:\n        cfg.pipeline_encoder_devices = utils.eval_str_list(\n            cfg.pipeline_encoder_devices, type=int\n        )\n        cfg.pipeline_decoder_devices = utils.eval_str_list(\n            cfg.pipeline_decoder_devices, type=int\n        )\n        num_pipeline_devices = len(\n            set(cfg.pipeline_encoder_devices + cfg.pipeline_decoder_devices)\n        )\n    gpus_per_node = torch.cuda.device_count()\n    assert (\n        gpus_per_node >= num_pipeline_devices\n        and gpus_per_node % num_pipeline_devices == 0\n    ), (\n        \"the number of unique device IDs in --pipeline-devices must evenly divide \"\n        \"the number of GPUs per node (multi-node pipelining is not yet supported)\"\n    )\n    num_pipelines_per_node = gpus_per_node // num_pipeline_devices\n    return num_pipeline_devices, num_pipelines_per_node", "\n\ndef _pipeline_parallel_post_init(\n    cfg: DistributedTrainingConfig, num_pipeline_devices, num_pipelines_per_node\n):\n    if not cfg.distributed_no_spawn:\n        # When distributed_no_spawn is False, we expect distributed_rank and\n        # distributed_world_size to be based on the total number of GPUs, so\n        # we need to correct them to be based on the number of pipelines.\n        assert cfg.distributed_world_size % num_pipeline_devices == 0\n        cfg.distributed_world_size = cfg.distributed_world_size // num_pipeline_devices\n        # In the case of 4-way MP on nodes with 8 GPUs, we want\n        # distributed_rank to be the starting GPU index for each pipeline\n        # i.e., 0, 2, ...\n        gpus_per_node = torch.cuda.device_count()\n        assert cfg.distributed_rank % gpus_per_node == 0\n        assert cfg.distributed_rank % num_pipeline_devices == 0\n\n        with open_dict(cfg):\n            cfg.distributed_rank = cfg.distributed_rank // num_pipeline_devices\n            # launch one process per pipeline\n            cfg.distributed_num_procs = num_pipelines_per_node\n\n    # if we have 4-way MP on a node with 8 GPUs, we want device_ids to be 0\n    # and 4, indicating the starting device IDs for each pipeline\n    cfg.device_id *= num_pipeline_devices\n\n    if cfg.device_id > 0:\n        # if there's multiple pipelines on a node (e.g., 4-way MP on an 8\n        # GPU node), we need to adjust pipeline_devices accordingly\n        logger.debug(\n            \"setting CUDA device={} on rank {}\".format(\n                cfg.device_id, cfg.distributed_rank\n            )\n        )\n        torch.cuda.set_device(cfg.device_id)\n        with open_dict(cfg):\n            cfg.pipeline_devices = [cfg.device_id + d for d in cfg.pipeline_devices]\n        logger.info(\n            \"setting pipeline_devices={} on rank {}\".format(\n                cfg.pipeline_devices, cfg.distributed_rank\n            )\n        )", "\n\ndef distributed_init(cfg: FairseqConfig):\n    if isinstance(cfg, Namespace):\n        from fairseq.dataclass.utils import convert_namespace_to_omegaconf\n\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    if not cfg.common.tpu:\n        if torch.distributed.is_available() and torch.distributed.is_initialized():\n            warnings.warn(\n                \"Distributed is already initialized, cannot initialize twice!\"\n            )\n        else:\n            logger.info(\n                \"distributed init (rank {}): {}\".format(\n                    cfg.distributed_training.distributed_rank,\n                    cfg.distributed_training.distributed_init_method,\n                )\n            )\n            dist.init_process_group(\n                backend=cfg.distributed_training.distributed_backend,\n                init_method=cfg.distributed_training.distributed_init_method,\n                world_size=cfg.distributed_training.distributed_world_size,\n                rank=cfg.distributed_training.distributed_rank,\n            )\n            logger.info(\n                \"initialized host {} as rank {}\".format(\n                    socket.gethostname(),\n                    cfg.distributed_training.distributed_rank,\n                )\n            )\n\n            # perform a dummy all-reduce to initialize the NCCL communicator\n            if torch.cuda.is_available():\n                dist.all_reduce(torch.zeros(1).cuda())\n\n        cfg.distributed_training.distributed_rank = torch.distributed.get_rank()\n    else:\n        assert xm.xrt_world_size() == cfg.distributed_training.distributed_world_size\n        global _USE_XLA\n        _USE_XLA = True\n        cfg.distributed_training.device_id = xm.get_local_ordinal()\n        cfg.distributed_training.distributed_rank = xm.get_ordinal()\n        xm.rendezvous(\"distributed_init\")  # wait for all workers\n\n    if is_master(cfg.distributed_training):\n        logging.getLogger().setLevel(logging.INFO)\n    else:\n        logging.getLogger().setLevel(logging.WARNING)\n\n    if cfg.common.model_parallel_size > 1:\n        global _USE_MEGATRON\n        _USE_MEGATRON = True\n        from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n        initialize_model_parallel(cfg.common.model_parallel_size)\n        from fairscale.nn.model_parallel.random import model_parallel_cuda_manual_seed\n        model_parallel_cuda_manual_seed(cfg.common.seed)\n        model_part_number = get_model_parallel_rank()\n        cfg.checkpoint.checkpoint_suffix += \"-model_part-{0}\".format(model_part_number)\n\n    if hasattr(cfg, \"model\") and getattr(cfg.model, \"base_layers\", 0) > 0:\n        cfg.checkpoint.checkpoint_suffix = (\n            f\"-rank-{cfg.distributed_training.distributed_rank}\"\n        )\n\n    return cfg.distributed_training.distributed_rank", "\n\ndef distributed_main(i, main, cfg: FairseqConfig, kwargs):\n    cfg.distributed_training.device_id = i\n    if torch.cuda.is_available() and not cfg.common.cpu and not cfg.common.tpu:\n        torch.cuda.set_device(cfg.distributed_training.device_id)\n    if cfg.distributed_training.distributed_rank is None:  # torch.multiprocessing.spawn\n        cfg.distributed_training.distributed_rank = kwargs.pop(\"start_rank\", 0) + i\n\n    cfg.distributed_training.distributed_rank = distributed_init(cfg)\n\n    after_distributed_init_fn = kwargs.pop(\"after_distributed_init_fn\", None)\n    if after_distributed_init_fn:\n        cfg = after_distributed_init_fn(cfg)\n\n    main(cfg, **kwargs)\n\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier(get_global_group())", "\n\ndef call_main(cfg: FairseqConfig, main, **kwargs):\n    if cfg.distributed_training.distributed_init_method is None:\n        infer_init_method(cfg.distributed_training)\n\n    if cfg.distributed_training.distributed_init_method is not None:\n        # distributed training\n        if not cfg.distributed_training.distributed_no_spawn:\n            start_rank = cfg.distributed_training.distributed_rank\n            cfg.distributed_training.distributed_rank = None  # assign automatically\n            kwargs[\"start_rank\"] = start_rank\n\n            torch.multiprocessing.spawn(\n                fn=distributed_main,\n                args=(main, cfg, kwargs),\n                nprocs=min(\n                    torch.cuda.device_count(),\n                    cfg.distributed_training.distributed_world_size,\n                ),\n                join=True,\n            )\n        else:\n            distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)\n    elif cfg.common.tpu and cfg.distributed_training.distributed_world_size > 1:\n        import torch_xla.distributed.xla_multiprocessing as xmp\n\n        torch.multiprocessing.set_sharing_strategy(\"file_system\")\n        xmp.spawn(\n            fn=distributed_main,\n            args=(main, cfg, kwargs),\n            # tpu-comment:\n            #   8 devices in one TPU VM, is the max processes to be spawned.\n            #   The rest is driven by xm.distributed.xla_dist\n            nprocs=min(cfg.distributed_training.distributed_world_size, 8),\n        )\n    else:\n        # single GPU main\n        main(cfg, **kwargs)", "\n\ndef use_xla():\n    global _USE_XLA\n    return _USE_XLA\n\n\ndef new_groups(grouped_ranks: List[List[int]]):\n    if use_xla():\n        return (\"tpu\", grouped_ranks)\n    else:\n        groups = [dist.new_group(g) for g in grouped_ranks]\n        my_group_idx = _find_my_group_index(grouped_ranks)\n        return groups[my_group_idx]", "\n\ndef _find_my_group_index(grouped_ranks):\n    my_rank = get_global_rank()\n    for i, group in enumerate(grouped_ranks):\n        if my_rank in group:\n            return i\n    raise RuntimeError\n\n\ndef _find_my_group(grouped_ranks):\n    index = _find_my_group_index(grouped_ranks)\n    return grouped_ranks[index]", "\n\ndef _find_my_group(grouped_ranks):\n    index = _find_my_group_index(grouped_ranks)\n    return grouped_ranks[index]\n\n\ndef get_rank(group):\n    if use_xla():\n        assert group[0] == \"tpu\"\n        my_group = _find_my_group(group[1])\n        return my_group.index(get_global_rank())\n    else:\n        return dist.get_rank(group=group)", "\n\ndef get_world_size(group):\n    if use_xla():\n        assert group[0] == \"tpu\"\n        my_group = _find_my_group(group[1])\n        return len(my_group)\n    elif torch.distributed.is_initialized():\n        return dist.get_world_size(group=group)\n    else:\n        return 1", "\n\ndef get_global_group():\n    if use_xla():\n        return new_groups([list(range(get_global_world_size()))])\n    elif torch.distributed.is_initialized():\n        if not hasattr(get_global_group, \"_global_group\"):\n            # ideally we could use torch.distributed.group.WORLD, but it seems\n            # to cause random NCCL hangs in some cases\n            get_global_group._global_group = dist.new_group()\n        return get_global_group._global_group\n    else:\n        return None", "\n\ndef get_global_rank():\n    if use_xla():\n        return xm.get_ordinal()\n    elif torch.distributed.is_initialized():\n        return torch.distributed.get_rank()\n    else:\n        return 0\n", "\n\ndef get_global_world_size():\n    if use_xla():\n        return xm.xrt_world_size()\n    elif torch.distributed.is_initialized():\n        return torch.distributed.get_world_size()\n    else:\n        return 1\n", "\n\ndef get_data_parallel_group():\n    \"\"\"Get the data parallel group the caller rank belongs to.\"\"\"\n    global _USE_MEGATRON\n    if _USE_MEGATRON:\n        from fairscale.nn.model_parallel import initialize as mpu\n        return mpu.get_data_parallel_group()\n    else:\n        return get_global_group()", "\n\ndef get_data_parallel_rank():\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    return get_rank(get_data_parallel_group())\n\n\ndef get_data_parallel_world_size():\n    \"\"\"Return world size for the data parallel group.\"\"\"\n    return get_world_size(get_data_parallel_group())", "\n\ndef get_model_parallel_group():\n    global _USE_MEGATRON\n    if _USE_MEGATRON:\n        from fairscale.nn.model_parallel import initialize as mpu\n        return mpu.get_model_parallel_group()\n    else:\n        return None\n", "\n\ndef get_model_parallel_rank():\n    \"\"\"Return my rank for the model parallel group.\"\"\"\n    return get_rank(get_model_parallel_group())\n\n\ndef get_model_parallel_world_size():\n    \"\"\"Return world size for the model parallel group.\"\"\"\n    return get_world_size(get_model_parallel_group())", "\n\ndef all_reduce(tensor, group, op=\"sum\"):\n    if use_xla():\n        assert isinstance(group, tuple) and group[0] == \"tpu\"\n        tensor = [tensor]  # wrap in a list to make xm.all_reduce in-place\n        return xm.all_reduce(op, tensor, groups=group[1])[0]\n    else:\n        if op == \"sum\":\n            op = dist.ReduceOp.SUM\n        elif op == \"max\":\n            op = dist.ReduceOp.MAX\n        else:\n            raise NotImplementedError\n        dist.all_reduce(tensor, op=op, group=group)\n        return tensor", "\n\ndef broadcast(tensor, src, group):\n    if use_xla():\n        # XLA doesn't support broadcast, hack it with all_reduce\n        if get_rank(group) != src:\n            tensor.zero_()\n        all_reduce(tensor, group)\n    else:\n        dist.broadcast(tensor, src=src, group=group)", "\n\ndef all_to_all(tensor, group):\n    \"\"\"Perform an all-to-all operation on a 1D Tensor.\"\"\"\n    assert tensor.dim() == 1\n    split_count = get_world_size(group=group)\n    assert tensor.numel() % split_count == 0\n    if use_xla():\n        assert isinstance(group, tuple) and group[0] == \"tpu\"\n        return xm.all_to_all(\n            tensor,\n            split_dimension=0,\n            concat_dimension=0,\n            split_count=split_count,\n            groups=group[1],\n        )\n    else:\n        output = torch.zeros_like(tensor)\n        dist.all_to_all_single(output, tensor, group=group)\n        return output", "\n\ndef all_gather(tensor, group, return_tensor=False):\n    \"\"\"Perform an all-gather operation.\"\"\"\n    if use_xla():\n        result = xm.all_gather(tensor, groups=group[1])\n        world_size = get_world_size(group=group)\n        result = result.view(world_size, *tensor.size())\n        if return_tensor:\n            return result\n        else:\n            return [result[i] for i in range(world_size)]\n    else:\n        world_size = get_world_size(group=group)\n        rank = get_rank(group=group)\n        tensor_list = [\n            tensor if i == rank else torch.empty_like(tensor) for i in range(world_size)\n        ]\n        dist.all_gather(tensor_list, tensor, group=group)\n        if return_tensor:\n            return torch.stack(tensor_list, dim=0)\n        else:\n            return tensor_list", "\n\ndef all_gather_list(data, group=None, max_size=16384):\n    \"\"\"Gathers arbitrary data from all nodes into a list.\n\n    Similar to :func:`~torch.distributed.all_gather` but for arbitrary Python\n    data. Note that *data* must be picklable and any CUDA tensors will be moved\n    to CPU and returned on CPU as well.\n\n    Args:\n        data (Any): data from the local worker to be gathered on other workers\n        group: group of the collective\n        max_size (int, optional): maximum size of the data to be gathered\n            across workers\n    \"\"\"\n    from fairseq import utils\n\n    if group is None:\n        group = get_global_group()\n    rank = get_rank(group=group)\n    world_size = get_world_size(group=group)\n\n    buffer_size = max_size * world_size\n    if (\n        not hasattr(all_gather_list, \"_buffer\")\n        or all_gather_list._buffer.numel() < buffer_size\n    ):\n        all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)\n        all_gather_list._cpu_buffer = torch.ByteTensor(max_size).pin_memory()\n    buffer = all_gather_list._buffer\n    buffer.zero_()\n    cpu_buffer = all_gather_list._cpu_buffer\n\n    data = utils.move_to_cpu(data)\n    enc = pickle.dumps(data)\n    enc_size = len(enc)\n    header_size = 4  # size of header that contains the length of the encoded data\n    size = header_size + enc_size\n    if size > max_size:\n        raise ValueError(\n            \"encoded data size ({}) exceeds max_size ({})\".format(size, max_size)\n        )\n\n    header = struct.pack(\">I\", enc_size)\n    cpu_buffer[:size] = torch.ByteTensor(list(header + enc))\n    start = rank * max_size\n    buffer[start : start + size].copy_(cpu_buffer[:size])\n\n    all_reduce(buffer, group=group)\n\n    buffer = buffer.cpu()\n    try:\n        result = []\n        for i in range(world_size):\n            out_buffer = buffer[i * max_size : (i + 1) * max_size]\n            (enc_size,) = struct.unpack(\">I\", bytes(out_buffer[:header_size].tolist()))\n            if enc_size > 0:\n                result.append(\n                    pickle.loads(\n                        bytes(out_buffer[header_size : header_size + enc_size].tolist())\n                    )\n                )\n        return result\n    except pickle.UnpicklingError:\n        raise Exception(\n            \"Unable to unpickle data from other workers. all_gather_list requires all \"\n            \"workers to enter the function together, so this error usually indicates \"\n            \"that the workers have fallen out of sync somehow. Workers can fall out of \"\n            \"sync if one of them runs out of memory, or if there are other conditions \"\n            \"in your training script that can cause one worker to finish an epoch \"\n            \"while other workers are still iterating over their portions of the data. \"\n            \"Try rerunning with --ddp-backend=legacy_ddp and see if that helps.\"\n        )", "\n\ndef all_reduce_dict(data: Mapping[str, Any], device, group) -> Dict[str, Any]:\n    \"\"\"\n    AllReduce a dictionary of values across workers. We separately\n    reduce items that are already on the device and items on CPU for\n    better performance.\n\n    Args:\n        data (Mapping[str, Any]): dictionary of data to all-reduce, but\n            cannot be a nested dictionary\n        device (torch.device): device for the reduction\n        group: group of the collective\n    \"\"\"\n    data_keys = list(data.keys())\n\n    # We want to separately reduce items that are already on the\n    # device and items on CPU for performance reasons.\n    cpu_data = OrderedDict()\n    device_data = OrderedDict()\n    for k in data_keys:\n        t = data[k]\n        if not torch.is_tensor(t):\n            cpu_data[k] = torch.tensor(t, dtype=torch.double)\n        elif t.device.type != device.type:\n            cpu_data[k] = t.to(dtype=torch.double)\n        else:\n            device_data[k] = t.to(dtype=torch.double)\n\n    def _all_reduce_dict(data: OrderedDict):\n        if len(data) == 0:\n            return data\n        buf = torch.cat([t.view(-1) for t in data.values()]).to(device=device)\n        all_reduce(buf, group=group)\n        split_buf = torch.split(buf.clone(), [t.numel() for t in data.values()])\n        reduced_data = [t.view_as(orig) for t, orig in zip(split_buf, data.values())]\n        return OrderedDict(zip(data.keys(), reduced_data))\n\n    cpu_data = _all_reduce_dict(cpu_data)\n    device_data = _all_reduce_dict(device_data)\n\n    def get_from_stack(key):\n        if key in cpu_data:\n            return cpu_data[key]\n        elif key in device_data:\n            return device_data[key]\n        raise KeyError\n\n    return OrderedDict([(key, get_from_stack(key)) for key in data_keys])", "\n\ndef broadcast_tensors(\n    tensors: Optional[List[torch.Tensor]],\n    src_rank: int,\n    group: object,\n    dist_device: Optional[torch.device] = None,\n) -> List[torch.Tensor]:\n    \"\"\"\n    Broadcasts a list of tensors without other (non-src) ranks needing to know\n    the dtypes/shapes of the tensors.\n    \"\"\"\n    if dist_device is None:\n        if torch.distributed.get_backend(group) == \"nccl\":\n            dist_device = torch.device(\"cuda\")\n        else:\n            dist_device = torch.device(\"cpu\")\n\n    # share metadata first to simplify transfer\n    is_src_rank = get_rank(group) == src_rank\n    if is_src_rank:\n        metadata = [\n            {\"size\": t.size(), \"dtype\": t.dtype, \"device\": t.device} for t in tensors\n        ]\n        metadata = _broadcast_object_slow(metadata, src_rank, group, dist_device)\n    else:\n        metadata = _broadcast_object_slow(None, src_rank, group, dist_device)\n\n    out_tensors = []\n    for i, meta in enumerate(metadata):\n        if is_src_rank:\n            tensor = tensors[i]\n            broadcast(tensors[i].to(dist_device), src=src_rank, group=group)\n        else:\n            tensor = torch.zeros(\n                [meta[\"size\"].numel()], dtype=meta[\"dtype\"], device=dist_device\n            )\n            broadcast(tensor, src=src_rank, group=group)\n        tensor = tensor.view(meta[\"size\"]).to(meta[\"device\"])\n        out_tensors.append(tensor)\n    return out_tensors", "\n\ndef broadcast_object(\n    obj: Any,\n    src_rank: int,\n    group: object,\n    dist_device: Optional[torch.device] = None,\n) -> Any:\n    \"\"\"Broadcast an arbitrary Python object to other workers.\"\"\"\n    if dist_device is None:\n        if torch.distributed.get_backend(group) == \"nccl\":\n            dist_device = torch.device(\"cuda\")\n        else:\n            dist_device = torch.device(\"cpu\")\n\n    if get_rank(group) == src_rank:\n        # split the tensors from the non-tensors so we can broadcast them\n        # directly, avoiding unnecessary serialization/deserialization\n        tensors = []\n        obj = _split_tensors_from_obj(obj, tensors)\n        obj = _broadcast_object_slow(obj, src_rank, group, dist_device)\n        tensors = broadcast_tensors(tensors, src_rank, group, dist_device)\n    else:\n        obj = _broadcast_object_slow(None, src_rank, group, dist_device)\n        tensors = broadcast_tensors(None, src_rank, group, dist_device)\n    return _put_tensors_in_obj(obj, tensors)", "\n\ndef _broadcast_object_slow(\n    obj: Any,\n    src_rank: int,\n    group: object,\n    dist_device: torch.device,\n) -> Any:\n    if get_rank(group) == src_rank:\n        # Emit data\n        buffer = io.BytesIO()\n        torch.save(obj, buffer)\n        buffer = torch.ByteTensor(buffer.getbuffer()).to(dist_device)\n        length = torch.LongTensor([len(buffer)]).to(dist_device)\n        broadcast(length, src=src_rank, group=group)\n        broadcast(buffer, src=src_rank, group=group)\n    else:\n        # Fetch from the source\n        length = torch.LongTensor([0]).to(dist_device)\n        broadcast(length, src=src_rank, group=group)\n        buffer = torch.ByteTensor(int(length.item())).to(dist_device)\n        broadcast(buffer, src=src_rank, group=group)\n        buffer = io.BytesIO(buffer.cpu().numpy())\n        obj = torch.load(buffer, map_location=\"cpu\")\n    return obj", "\n\n@dataclass(frozen=True)\nclass _TensorPlaceholder:\n    index: int\n\n\ndef _split_tensors_from_obj(obj: Any, tensors: List[torch.Tensor]) -> Any:\n    if torch.is_tensor(obj):\n        placeholder = _TensorPlaceholder(index=len(tensors))\n        tensors.append(obj)\n        return placeholder\n    elif isinstance(obj, dict):\n        return {k: _split_tensors_from_obj(v, tensors) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [_split_tensors_from_obj(v, tensors) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(_split_tensors_from_obj(v, tensors) for v in obj)\n    elif isinstance(obj, set):\n        return {_split_tensors_from_obj(v, tensors) for v in obj}\n    else:\n        return obj", "\n\ndef _put_tensors_in_obj(obj: Any, tensors: List[torch.Tensor]) -> Any:\n    if isinstance(obj, _TensorPlaceholder):\n        return tensors[obj.index]\n    elif isinstance(obj, dict):\n        return {k: _put_tensors_in_obj(v, tensors) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [_put_tensors_in_obj(v, tensors) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(_put_tensors_in_obj(v, tensors) for v in obj)\n    elif isinstance(obj, set):\n        return {_put_tensors_in_obj(v, tensors) for v in obj}\n    else:\n        return obj"]}
{"filename": "xlmr/src/train_fsdp.py", "chunked_list": ["#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nTrain a new model on one or across multiple GPUs.\n\"\"\"\n\nimport argparse", "\nimport argparse\nimport logging\nimport math\nimport os\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\n# We need to setup root logger before importing any fairseq libraries.\nlogging.basicConfig(", "# We need to setup root logger before importing any fairseq libraries.\nlogging.basicConfig(\n    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\"fairseq_cli.train\")\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom fairseq import checkpoint_utils, options, quantization_utils, tasks, utils\nfrom fairseq.data import data_utils, iterators\nfrom fairseq.data.plasma_utils import PlasmaStore\nfrom fairseq.dataclass.configs import FairseqConfig\nfrom fairseq.dataclass.initialize import add_defaults", "from fairseq.dataclass.configs import FairseqConfig\nfrom fairseq.dataclass.initialize import add_defaults\nfrom fairseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom fairseq.distributed import utils as distributed_utils\nfrom fairseq.file_io import PathManager\nfrom fairseq.logging import meters, metrics, progress_bar\nfrom fairseq.model_parallel.megatron_trainer import MegatronTrainer\nfrom trainer import Trainer\nfrom fsdp.fully_sharded_data_parallel import fsdp_enable_wrap, fsdp_wrap\n", "from fsdp.fully_sharded_data_parallel import fsdp_enable_wrap, fsdp_wrap\n\n\ndef main(cfg: FairseqConfig) -> None:\n    if isinstance(cfg, argparse.Namespace):\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    utils.import_user_module(cfg.common)\n    add_defaults(cfg)\n\n    if (\n        distributed_utils.is_master(cfg.distributed_training)\n        and \"job_logging_cfg\" in cfg\n    ):\n        # make hydra logging work with ddp (see # see https://github.com/facebookresearch/hydra/issues/1126)\n        logging.config.dictConfig(OmegaConf.to_container(cfg.job_logging_cfg))\n\n    assert (\n        cfg.dataset.max_tokens is not None or cfg.dataset.batch_size is not None\n    ), \"Must specify batch size either with --max-tokens or --batch-size\"\n    metrics.reset()\n\n    if cfg.common.log_file is not None:\n        handler = logging.FileHandler(filename=cfg.common.log_file)\n        logger.addHandler(handler)\n\n    np.random.seed(cfg.common.seed)\n    utils.set_torch_seed(cfg.common.seed)\n\n    if distributed_utils.is_master(cfg.distributed_training):\n        checkpoint_utils.verify_checkpoint_directory(cfg.checkpoint.save_dir)\n\n    # Print args\n    logger.info(cfg)\n\n    if cfg.checkpoint.write_checkpoints_asynchronously:\n        try:\n            import iopath  # noqa: F401\n        except ImportError:\n            logging.exception(\n                \"Asynchronous checkpoint writing is specified but iopath is \"\n                \"not installed: `pip install iopath`\"\n            )\n            return\n\n    # Setup task, e.g., translation, language modeling, etc.\n    task = tasks.setup_task(cfg.task)\n\n    assert cfg.criterion, \"Please specify criterion to train a model\"\n\n    if not torch.distributed.is_initialized():\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = '30000'\n        torch.distributed.init_process_group(\"nccl\", init_method='env://', rank=0, world_size=1)\n\n    # Build model and criterion\n    if cfg.distributed_training.ddp_backend == \"fully_sharded\":\n        with fsdp_enable_wrap(cfg.distributed_training):\n            model = fsdp_wrap(task.build_model(cfg.model))\n    else:\n        model = task.build_model(cfg.model)\n    criterion = task.build_criterion(cfg.criterion)\n    logger.info(model)\n    logger.info(\"task: {}\".format(task.__class__.__name__))\n    logger.info(\"model: {}\".format(model.__class__.__name__))\n    logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n    logger.info(\n        \"num. shared model params: {:,} (num. trained: {:,})\".format(\n            sum(\n                p.numel() for p in model.parameters() if not getattr(p, \"expert\", False)\n            ),\n            sum(\n                p.numel()\n                for p in model.parameters()\n                if not getattr(p, \"expert\", False) and p.requires_grad\n            ),\n        )\n    )\n\n    logger.info(\n        \"num. expert model params: {} (num. trained: {})\".format(\n            sum(p.numel() for p in model.parameters() if getattr(p, \"expert\", False)),\n            sum(\n                p.numel()\n                for p in model.parameters()\n                if getattr(p, \"expert\", False) and p.requires_grad\n            ),\n        )\n    )\n\n    # Load valid dataset (we load training data below, based on the latest checkpoint)\n    # We load the valid dataset AFTER building the model\n    if not cfg.dataset.disable_validation:\n        data_utils.raise_if_valid_subsets_unintentionally_ignored(cfg)\n        if cfg.dataset.combine_valid_subsets:\n            task.load_dataset(\"valid\", combine=True, epoch=1)\n        else:\n            for valid_sub_split in cfg.dataset.valid_subset.split(\",\"):\n                task.load_dataset(valid_sub_split, combine=False, epoch=1)\n\n    # (optionally) Configure quantization\n    if cfg.common.quantization_config_path is not None:\n        quantizer = quantization_utils.Quantizer(\n            config_path=cfg.common.quantization_config_path,\n            max_epoch=cfg.optimization.max_epoch,\n            max_update=cfg.optimization.max_update,\n        )\n    else:\n        quantizer = None\n\n    # Build trainer\n    if cfg.common.model_parallel_size == 1:\n        trainer = Trainer(cfg, task, model, criterion, quantizer)\n    else:\n        trainer = MegatronTrainer(cfg, task, model, criterion)\n    logger.info(\n        \"training on {} devices (GPUs/TPUs)\".format(\n            cfg.distributed_training.distributed_world_size\n        )\n    )\n    logger.info(\n        \"max tokens per device = {} and max sentences per device = {}\".format(\n            cfg.dataset.max_tokens,\n            cfg.dataset.batch_size,\n        )\n    )\n\n    # Load the latest checkpoint if one is available and restore the\n    # corresponding train iterator\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(\n        cfg.checkpoint,\n        trainer,\n        # don't cache epoch iterators for sharded datasets\n        disable_iterator_cache=task.has_sharded_data(\"train\"),\n    )\n    if cfg.common.tpu:\n        import torch_xla.core.xla_model as xm\n\n        xm.rendezvous(\"load_checkpoint\")  # wait for all workers\n\n    max_epoch = cfg.optimization.max_epoch or math.inf\n    lr = trainer.get_lr()\n\n    # TODO: a dry run on validation set to pin the memory\n    valid_subsets = cfg.dataset.valid_subset.split(\",\")\n    if not cfg.dataset.disable_validation:\n        for subset in valid_subsets:\n            logger.info('begin dry-run validation on \"{}\" subset'.format(subset))\n            itr = trainer.get_valid_iterator(subset).next_epoch_itr(\n                shuffle=False, set_dataset_epoch=False  # use a fixed valid set\n            )\n            if cfg.common.tpu:\n                itr = utils.tpu_data_loader(itr)\n            for _ in itr:\n                pass\n    # TODO: end of dry run section\n\n    train_meter = meters.StopwatchMeter()\n    train_meter.start()\n    while epoch_itr.next_epoch_idx <= max_epoch:\n        if lr <= cfg.optimization.stop_min_lr:\n            logger.info(\n                f\"stopping training because current learning rate ({lr}) is smaller \"\n                \"than or equal to minimum learning rate \"\n                f\"(--stop-min-lr={cfg.optimization.stop_min_lr})\"\n            )\n            break\n\n        # train for one epoch\n        valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n        if should_stop:\n            break\n\n        # only use first validation loss to update the learning rate\n        lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n\n        epoch_itr = trainer.get_train_iterator(\n            epoch_itr.next_epoch_idx,\n            # sharded data: get train iterator for next epoch\n            load_dataset=task.has_sharded_data(\"train\"),\n            # don't cache epoch iterators for sharded datasets\n            disable_iterator_cache=task.has_sharded_data(\"train\"),\n        )\n    train_meter.stop()\n    logger.info(\"done training in {:.1f} seconds\".format(train_meter.sum))\n\n    # ioPath implementation to wait for all asynchronous file writes to complete.\n    if cfg.checkpoint.write_checkpoints_asynchronously:\n        logger.info(\n            \"ioPath PathManager waiting for all asynchronous checkpoint \"\n            \"writes to finish.\"\n        )\n        PathManager.async_close()\n        logger.info(\"ioPath PathManager finished waiting.\")", "\n\ndef should_stop_early(cfg: DictConfig, valid_loss: float) -> bool:\n    # skip check if no validation was done in the current epoch\n    if valid_loss is None:\n        return False\n    if cfg.checkpoint.patience <= 0:\n        return False\n\n    def is_better(a, b):\n        return a > b if cfg.checkpoint.maximize_best_checkpoint_metric else a < b\n\n    prev_best = getattr(should_stop_early, \"best\", None)\n    if prev_best is None or is_better(valid_loss, prev_best):\n        should_stop_early.best = valid_loss\n        should_stop_early.num_runs = 0\n        return False\n    else:\n        should_stop_early.num_runs += 1\n        if should_stop_early.num_runs >= cfg.checkpoint.patience:\n            logger.info(\n                \"early stop since valid performance hasn't improved for last {} runs\".format(\n                    cfg.checkpoint.patience\n                )\n            )\n            return True\n        else:\n            return False", "\n\n@metrics.aggregate(\"train\")\ndef train(\n    cfg: DictConfig, trainer: Trainer, task: tasks.FairseqTask, epoch_itr\n) -> Tuple[List[Optional[float]], bool]:\n    \"\"\"Train the model for one epoch and return validation losses.\"\"\"\n    # Initialize data iterator\n    itr = epoch_itr.next_epoch_itr(\n        fix_batches_to_gpus=cfg.distributed_training.fix_batches_to_gpus,\n        shuffle=(epoch_itr.next_epoch_idx > cfg.dataset.curriculum),\n    )\n    update_freq = (\n        cfg.optimization.update_freq[epoch_itr.epoch - 1]\n        if epoch_itr.epoch <= len(cfg.optimization.update_freq)\n        else cfg.optimization.update_freq[-1]\n    )\n    itr = iterators.GroupedIterator(\n        itr,\n        update_freq,\n        skip_remainder_batch=cfg.optimization.skip_remainder_batch,\n    )\n    if cfg.common.tpu:\n        itr = utils.tpu_data_loader(itr)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_file=cfg.common.log_file,\n        log_interval=cfg.common.log_interval,\n        epoch=epoch_itr.epoch,\n        aim_repo=(\n            cfg.common.aim_repo\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        aim_run_hash=(\n            cfg.common.aim_run_hash\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        aim_param_checkpoint_dir=cfg.checkpoint.save_dir,\n        tensorboard_logdir=(\n            cfg.common.tensorboard_logdir\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n        wandb_project=(\n            cfg.common.wandb_project\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        wandb_run_name=os.environ.get(\n            \"WANDB_NAME\", os.path.basename(cfg.checkpoint.save_dir)\n        ),\n        azureml_logging=(\n            cfg.common.azureml_logging\n            if distributed_utils.is_master(cfg.distributed_training)\n            else False\n        ),\n    )\n    progress.update_config(_flatten_config(cfg))\n\n    trainer.begin_epoch(epoch_itr.epoch)\n\n    valid_subsets = cfg.dataset.valid_subset.split(\",\")\n    should_stop = False\n    num_updates = trainer.get_num_updates()\n    logger.info(\"Start iterating over samples\")\n    for i, samples in enumerate(progress):\n        with metrics.aggregate(\"train_inner\"), torch.autograd.profiler.record_function(\n            \"train_step-%d\" % i\n        ):\n            log_output = trainer.train_step(samples)\n\n        if log_output is not None:  # not OOM, overflow, ...\n            # log mid-epoch stats\n            num_updates = trainer.get_num_updates()\n            if num_updates % cfg.common.log_interval == 0:\n                stats = get_training_stats(metrics.get_smoothed_values(\"train_inner\"))\n                progress.log(stats, tag=\"train_inner\", step=num_updates)\n\n                # reset mid-epoch stats after each log interval\n                # the end-of-epoch stats will still be preserved\n                metrics.reset_meters(\"train_inner\")\n\n        end_of_epoch = not itr.has_next()\n        valid_losses, should_stop = validate_and_save(\n            cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch\n        )\n\n        if should_stop:\n            break\n\n    # log end-of-epoch stats\n    logger.info(\"end of epoch {} (average epoch stats below)\".format(epoch_itr.epoch))\n    stats = get_training_stats(metrics.get_smoothed_values(\"train\"))\n    progress.print(stats, tag=\"train\", step=num_updates)\n\n    # reset epoch-level meters\n    metrics.reset_meters(\"train\")\n    return valid_losses, should_stop", "\n\ndef _flatten_config(cfg: DictConfig):\n    config = OmegaConf.to_container(cfg)\n    # remove any legacy Namespaces and replace with a single \"args\"\n    namespace = None\n    for k, v in list(config.items()):\n        if isinstance(v, argparse.Namespace):\n            namespace = v\n            del config[k]\n    if namespace is not None:\n        config[\"args\"] = vars(namespace)\n    return config", "\n\ndef validate_and_save(\n    cfg: DictConfig,\n    trainer: Trainer,\n    task: tasks.FairseqTask,\n    epoch_itr,\n    valid_subsets: List[str],\n    end_of_epoch: bool,\n) -> Tuple[List[Optional[float]], bool]:\n    num_updates = trainer.get_num_updates()\n    max_update = cfg.optimization.max_update or math.inf\n\n    # Stopping conditions (and an additional one based on validation loss later\n    # on)\n    should_stop = False\n    if num_updates >= max_update:\n        should_stop = True\n        logger.info(\n            f\"Stopping training due to \"\n            f\"num_updates: {num_updates} >= max_update: {max_update}\"\n        )\n\n    training_time_hours = trainer.cumulative_training_time() / (60 * 60)\n    if (\n        cfg.optimization.stop_time_hours > 0\n        and training_time_hours > cfg.optimization.stop_time_hours\n    ):\n        should_stop = True\n        logger.info(\n            f\"Stopping training due to \"\n            f\"cumulative_training_time: {training_time_hours} > \"\n            f\"stop_time_hours: {cfg.optimization.stop_time_hours} hour(s)\"\n        )\n\n    do_save = (\n        (end_of_epoch and epoch_itr.epoch % cfg.checkpoint.save_interval == 0)\n        or should_stop\n        or (\n            cfg.checkpoint.save_interval_updates > 0\n            and num_updates > 0\n            and num_updates % cfg.checkpoint.save_interval_updates == 0\n            and num_updates >= cfg.dataset.validate_after_updates\n        )\n    )\n    do_validate = (\n        (\n            (not end_of_epoch and do_save)  # validate during mid-epoch saves\n            or (end_of_epoch and epoch_itr.epoch % cfg.dataset.validate_interval == 0)\n            or should_stop\n            or (\n                cfg.dataset.validate_interval_updates > 0\n                and num_updates > 0\n                and num_updates % cfg.dataset.validate_interval_updates == 0\n            )\n        )\n        and not cfg.dataset.disable_validation\n        and num_updates >= cfg.dataset.validate_after_updates\n    )\n\n    # Validate\n    valid_losses = [None]\n    if do_validate:\n        valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\n\n    should_stop |= should_stop_early(cfg, valid_losses[0])\n\n    # Save checkpoint\n    if do_save or should_stop:\n        cp_path = checkpoint_utils.save_checkpoint(\n            cfg.checkpoint, trainer, epoch_itr, valid_losses[0]\n        )\n        if cp_path is not None and hasattr(task, \"post_save\"):\n            task.post_save(cp_path, num_updates)\n\n    return valid_losses, should_stop", "\n\ndef get_training_stats(stats: Dict[str, Any]) -> Dict[str, Any]:\n    stats[\"wall\"] = round(metrics.get_meter(\"default\", \"wall\").elapsed_time, 0)\n    return stats\n\n\ndef validate(\n    cfg: DictConfig,\n    trainer: Trainer,\n    task: tasks.FairseqTask,\n    epoch_itr,\n    subsets: List[str],\n) -> List[Optional[float]]:\n    \"\"\"Evaluate the model on the validation set(s) and return the losses.\"\"\"\n\n    if cfg.dataset.fixed_validation_seed is not None:\n        # set fixed seed for every validation\n        utils.set_torch_seed(cfg.dataset.fixed_validation_seed)\n\n    trainer.begin_valid_epoch(epoch_itr.epoch)\n    valid_losses = []\n    for subset_idx, subset in enumerate(subsets):\n        logger.info('begin validation on \"{}\" subset'.format(subset))\n\n        # Initialize data iterator\n        itr = trainer.get_valid_iterator(subset).next_epoch_itr(\n            shuffle=False, set_dataset_epoch=False  # use a fixed valid set\n        )\n        if cfg.common.tpu:\n            itr = utils.tpu_data_loader(itr)\n        progress = progress_bar.progress_bar(\n            itr,\n            log_format=cfg.common.log_format,\n            log_interval=cfg.common.log_interval,\n            epoch=epoch_itr.epoch,\n            prefix=f\"valid on '{subset}' subset\",\n            aim_repo=(\n                cfg.common.aim_repo\n                if distributed_utils.is_master(cfg.distributed_training)\n                else None\n            ),\n            aim_run_hash=(\n                cfg.common.aim_run_hash\n                if distributed_utils.is_master(cfg.distributed_training)\n                else None\n            ),\n            aim_param_checkpoint_dir=cfg.checkpoint.save_dir,\n            tensorboard_logdir=(\n                cfg.common.tensorboard_logdir\n                if distributed_utils.is_master(cfg.distributed_training)\n                else None\n            ),\n            default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n            wandb_project=(\n                cfg.common.wandb_project\n                if distributed_utils.is_master(cfg.distributed_training)\n                else None\n            ),\n            wandb_run_name=os.environ.get(\n                \"WANDB_NAME\", os.path.basename(cfg.checkpoint.save_dir)\n            ),\n        )\n\n        # create a new root metrics aggregator so validation metrics\n        # don't pollute other aggregators (e.g., train meters)\n        with metrics.aggregate(new_root=True) as agg:\n            for i, sample in enumerate(progress):\n                if (\n                    cfg.dataset.max_valid_steps is not None\n                    and i > cfg.dataset.max_valid_steps\n                ):\n                    break\n                trainer.valid_step(sample)\n\n        # log validation stats\n        # only tracking the best metric on the 1st validation subset\n        tracking_best = subset_idx == 0\n        stats = get_valid_stats(cfg, trainer, agg.get_smoothed_values(), tracking_best)\n\n        if hasattr(task, \"post_validate\"):\n            task.post_validate(trainer.get_model(), stats, agg)\n\n        progress.print(stats, tag=subset, step=trainer.get_num_updates())\n\n        valid_losses.append(stats[cfg.checkpoint.best_checkpoint_metric])\n    return valid_losses", "\n\ndef get_valid_stats(\n    cfg: DictConfig,\n    trainer: Trainer,\n    stats: Dict[str, Any],\n    tracking_best: bool,\n) -> Dict[str, Any]:\n    stats[\"num_updates\"] = trainer.get_num_updates()\n    if tracking_best and hasattr(checkpoint_utils.save_checkpoint, \"best\"):\n        key = \"best_{0}\".format(cfg.checkpoint.best_checkpoint_metric)\n        best_function = max if cfg.checkpoint.maximize_best_checkpoint_metric else min\n        stats[key] = best_function(\n            checkpoint_utils.save_checkpoint.best,\n            stats[cfg.checkpoint.best_checkpoint_metric],\n        )\n    return stats", "\n\ndef cli_main(\n    modify_parser: Optional[Callable[[argparse.ArgumentParser], None]] = None\n) -> None:\n    parser = options.get_training_parser()\n    args = options.parse_args_and_arch(parser, modify_parser=modify_parser)\n\n    cfg = convert_namespace_to_omegaconf(args)\n\n    if cfg.common.use_plasma_view:\n        server = PlasmaStore(path=cfg.common.plasma_path)\n        logger.info(\n            f\"Started plasma server pid {server.server.pid} {cfg.common.plasma_path}\"\n        )\n\n    if args.profile:\n        with torch.cuda.profiler.profile():\n            with torch.autograd.profiler.emit_nvtx():\n                distributed_utils.call_main(cfg, main)\n    else:\n        distributed_utils.call_main(cfg, main)", "\n    # if cfg.common.use_plasma_view:\n    #     server.server.kill()\n\n\nif __name__ == \"__main__\":\n    cli_main()\n"]}
{"filename": "xlmr/src/train_megatron.py", "chunked_list": ["#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nTrain a new model on one or across multiple GPUs.\n\"\"\"\n\nimport argparse", "\nimport argparse\nimport logging\nimport math\nimport os\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple\n\n# We need to setup root logger before importing any fairseq libraries.\nlogging.basicConfig(", "# We need to setup root logger before importing any fairseq libraries.\nlogging.basicConfig(\n    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\"fairseq_cli.train\")\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig, OmegaConf\nfrom fairseq.utils import safe_getattr, safe_hasattr\n\nfrom fairseq import checkpoint_utils, options, quantization_utils, tasks, utils\nfrom fairseq.data import data_utils, iterators\nfrom fairseq.data.plasma_utils import PlasmaStore\nfrom fairseq.dataclass.configs import FairseqConfig", "from fairseq.data.plasma_utils import PlasmaStore\nfrom fairseq.dataclass.configs import FairseqConfig\nfrom fairseq.dataclass.initialize import add_defaults\nfrom fairseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom fairseq.distributed import fsdp_enable_wrap, fsdp_wrap\nfrom fairseq.file_io import PathManager\nfrom fairseq.logging import meters, metrics, progress_bar\nfrom megatron_trainer import MegatronTrainer\nfrom trainer import Trainer\nimport utils as distributed_utils", "from trainer import Trainer\nimport utils as distributed_utils\n\n\ndef main(cfg: FairseqConfig) -> None:\n    if isinstance(cfg, argparse.Namespace):\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    utils.import_user_module(cfg.common)\n    add_defaults(cfg)\n\n    if (\n        distributed_utils.is_master(cfg.distributed_training)\n        and \"job_logging_cfg\" in cfg\n    ):\n        # make hydra logging work with ddp (see # see https://github.com/facebookresearch/hydra/issues/1126)\n        logging.config.dictConfig(OmegaConf.to_container(cfg.job_logging_cfg))\n\n    assert (\n        cfg.dataset.max_tokens is not None or cfg.dataset.batch_size is not None\n    ), \"Must specify batch size either with --max-tokens or --batch-size\"\n    metrics.reset()\n\n    if cfg.common.log_file is not None:\n        handler = logging.FileHandler(filename=cfg.common.log_file)\n        logger.addHandler(handler)\n\n    np.random.seed(cfg.common.seed)\n    utils.set_torch_seed(cfg.common.seed)\n\n    if distributed_utils.is_master(cfg.distributed_training):\n        checkpoint_utils.verify_checkpoint_directory(cfg.checkpoint.save_dir)\n\n    # Print args\n    logger.info(cfg)\n\n    if cfg.checkpoint.write_checkpoints_asynchronously:\n        try:\n            import iopath  # noqa: F401\n        except ImportError:\n            logging.exception(\n                \"Asynchronous checkpoint writing is specified but iopath is \"\n                \"not installed: `pip install iopath`\"\n            )\n            return\n\n    # Setup task, e.g., translation, language modeling, etc.\n    task = tasks.setup_task(cfg.task)\n\n    assert cfg.criterion, \"Please specify criterion to train a model\"\n\n    # Build model and criterion\n    if cfg.distributed_training.ddp_backend == \"fully_sharded\":\n        with fsdp_enable_wrap(cfg.distributed_training):\n            model = fsdp_wrap(task.build_model(cfg.model))\n    else:\n        model = task.build_model(cfg.model)\n    criterion = task.build_criterion(cfg.criterion)\n    logger.info(model)\n    logger.info(\"task: {}\".format(task.__class__.__name__))\n    logger.info(\"model: {}\".format(model.__class__.__name__))\n    logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n    logger.info(\n        \"num. shared model params: {:,} (num. trained: {:,})\".format(\n            sum(\n                p.numel() for p in model.parameters() if not getattr(p, \"expert\", False)\n            ),\n            sum(\n                p.numel()\n                for p in model.parameters()\n                if not getattr(p, \"expert\", False) and p.requires_grad\n            ),\n        )\n    )\n\n    logger.info(\n        \"num. expert model params: {} (num. trained: {})\".format(\n            sum(p.numel() for p in model.parameters() if getattr(p, \"expert\", False)),\n            sum(\n                p.numel()\n                for p in model.parameters()\n                if getattr(p, \"expert\", False) and p.requires_grad\n            ),\n        )\n    )\n\n    # Load valid dataset (we load training data below, based on the latest checkpoint)\n    # We load the valid dataset AFTER building the model\n    if not cfg.dataset.disable_validation:\n        data_utils.raise_if_valid_subsets_unintentionally_ignored(cfg)\n        if cfg.dataset.combine_valid_subsets:\n            task.load_dataset(\"valid\", combine=True, epoch=1)\n        else:\n            for valid_sub_split in cfg.dataset.valid_subset.split(\",\"):\n                task.load_dataset(valid_sub_split, combine=False, epoch=1)\n\n    # (optionally) Configure quantization\n    if cfg.common.quantization_config_path is not None:\n        quantizer = quantization_utils.Quantizer(\n            config_path=cfg.common.quantization_config_path,\n            max_epoch=cfg.optimization.max_epoch,\n            max_update=cfg.optimization.max_update,\n        )\n    else:\n        quantizer = None\n\n    # Build trainer\n    if cfg.common.model_parallel_size == 1:\n        trainer = Trainer(cfg, task, model, criterion, quantizer)\n    else:\n        trainer = MegatronTrainer(cfg, task, model, criterion)\n    logger.info(\n        \"training on {} devices (GPUs/TPUs)\".format(\n            cfg.distributed_training.distributed_world_size\n        )\n    )\n    logger.info(\n        \"max tokens per device = {} and max sentences per device = {}\".format(\n            cfg.dataset.max_tokens,\n            cfg.dataset.batch_size,\n        )\n    )\n\n    # Load the latest checkpoint if one is available and restore the\n    # corresponding train iterator\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(\n        cfg.checkpoint,\n        trainer,\n        # don't cache epoch iterators for sharded datasets\n        disable_iterator_cache=task.has_sharded_data(\"train\"),\n    )\n    if cfg.common.tpu:\n        import torch_xla.core.xla_model as xm\n\n        xm.rendezvous(\"load_checkpoint\")  # wait for all workers\n\n    max_epoch = cfg.optimization.max_epoch or math.inf\n    lr = trainer.get_lr()\n\n    # TODO: a dry run on validation set to pin the memory\n    valid_subsets = cfg.dataset.valid_subset.split(\",\")\n    if not cfg.dataset.disable_validation:\n        for subset in valid_subsets:\n            logger.info('begin dry-run validation on \"{}\" subset'.format(subset))\n            itr = trainer.get_valid_iterator(subset).next_epoch_itr(\n                shuffle=False, set_dataset_epoch=False  # use a fixed valid set\n            )\n            if cfg.common.tpu:\n                itr = utils.tpu_data_loader(itr)\n            for _ in itr:\n                pass\n    # TODO: end of dry run section\n\n    train_meter = meters.StopwatchMeter()\n    train_meter.start()\n    while epoch_itr.next_epoch_idx <= max_epoch:\n        if lr <= cfg.optimization.stop_min_lr:\n            logger.info(\n                f\"stopping training because current learning rate ({lr}) is smaller \"\n                \"than or equal to minimum learning rate \"\n                f\"(--stop-min-lr={cfg.optimization.stop_min_lr})\"\n            )\n            break\n\n        # train for one epoch\n        valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n        if should_stop:\n            break\n\n        # only use first validation loss to update the learning rate\n        lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n\n        epoch_itr = trainer.get_train_iterator(\n            epoch_itr.next_epoch_idx,\n            # sharded data: get train iterator for next epoch\n            load_dataset=task.has_sharded_data(\"train\"),\n            # don't cache epoch iterators for sharded datasets\n            disable_iterator_cache=task.has_sharded_data(\"train\"),\n        )\n    train_meter.stop()\n    logger.info(\"done training in {:.1f} seconds\".format(train_meter.sum))\n\n    # ioPath implementation to wait for all asynchronous file writes to complete.\n    if cfg.checkpoint.write_checkpoints_asynchronously:\n        logger.info(\n            \"ioPath PathManager waiting for all asynchronous checkpoint \"\n            \"writes to finish.\"\n        )\n        PathManager.async_close()\n        logger.info(\"ioPath PathManager finished waiting.\")", "\n\ndef should_stop_early(cfg: DictConfig, valid_loss: float) -> bool:\n    # skip check if no validation was done in the current epoch\n    if valid_loss is None:\n        return False\n    if cfg.checkpoint.patience <= 0:\n        return False\n\n    def is_better(a, b):\n        return a > b if cfg.checkpoint.maximize_best_checkpoint_metric else a < b\n\n    prev_best = getattr(should_stop_early, \"best\", None)\n    if prev_best is None or is_better(valid_loss, prev_best):\n        should_stop_early.best = valid_loss\n        should_stop_early.num_runs = 0\n        return False\n    else:\n        should_stop_early.num_runs += 1\n        if should_stop_early.num_runs >= cfg.checkpoint.patience:\n            logger.info(\n                \"early stop since valid performance hasn't improved for last {} runs\".format(\n                    cfg.checkpoint.patience\n                )\n            )\n            return True\n        else:\n            return False", "\n\n@metrics.aggregate(\"train\")\ndef train(\n    cfg: DictConfig, trainer: Trainer, task: tasks.FairseqTask, epoch_itr\n) -> Tuple[List[Optional[float]], bool]:\n    \"\"\"Train the model for one epoch and return validation losses.\"\"\"\n    # Initialize data iterator\n    itr = epoch_itr.next_epoch_itr(\n        fix_batches_to_gpus=cfg.distributed_training.fix_batches_to_gpus,\n        shuffle=(epoch_itr.next_epoch_idx > cfg.dataset.curriculum),\n    )\n    update_freq = (\n        cfg.optimization.update_freq[epoch_itr.epoch - 1]\n        if epoch_itr.epoch <= len(cfg.optimization.update_freq)\n        else cfg.optimization.update_freq[-1]\n    )\n    itr = iterators.GroupedIterator(\n        itr,\n        update_freq,\n        skip_remainder_batch=cfg.optimization.skip_remainder_batch,\n    )\n    if cfg.common.tpu:\n        itr = utils.tpu_data_loader(itr)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_file=cfg.common.log_file,\n        log_interval=cfg.common.log_interval,\n        epoch=epoch_itr.epoch,\n        aim_repo=(\n            cfg.common.aim_repo\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        aim_run_hash=(\n            cfg.common.aim_run_hash\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        aim_param_checkpoint_dir=cfg.checkpoint.save_dir,\n        tensorboard_logdir=(\n            cfg.common.tensorboard_logdir\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n        wandb_project=(\n            cfg.common.wandb_project\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        wandb_run_name=os.environ.get(\n            \"WANDB_NAME\", os.path.basename(cfg.checkpoint.save_dir)\n        ),\n        azureml_logging=(\n            cfg.common.azureml_logging\n            if distributed_utils.is_master(cfg.distributed_training)\n            else False\n        ),\n    )\n    progress.update_config(_flatten_config(cfg))\n\n    trainer.begin_epoch(epoch_itr.epoch)\n\n    valid_subsets = cfg.dataset.valid_subset.split(\",\")\n    should_stop = False\n    num_updates = trainer.get_num_updates()\n    logger.info(\"Start iterating over samples\")\n    for i, samples in enumerate(progress):\n        with metrics.aggregate(\"train_inner\"), torch.autograd.profiler.record_function(\n            \"train_step-%d\" % i\n        ):\n            log_output = trainer.train_step(samples)\n\n        if log_output is not None:  # not OOM, overflow, ...\n            # log mid-epoch stats\n            num_updates = trainer.get_num_updates()\n            if num_updates % cfg.common.log_interval == 0:\n                stats = get_training_stats(metrics.get_smoothed_values(\"train_inner\"))\n                progress.log(stats, tag=\"train_inner\", step=num_updates)\n\n                # reset mid-epoch stats after each log interval\n                # the end-of-epoch stats will still be preserved\n                metrics.reset_meters(\"train_inner\")\n\n        end_of_epoch = not itr.has_next()\n        valid_losses, should_stop = validate_and_save(\n            cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch\n        )\n\n        if should_stop:\n            break\n\n    # log end-of-epoch stats\n    logger.info(\"end of epoch {} (average epoch stats below)\".format(epoch_itr.epoch))\n    stats = get_training_stats(metrics.get_smoothed_values(\"train\"))\n    progress.print(stats, tag=\"train\", step=num_updates)\n\n    # reset epoch-level meters\n    metrics.reset_meters(\"train\")\n    return valid_losses, should_stop", "\n\ndef _flatten_config(cfg: DictConfig):\n    config = OmegaConf.to_container(cfg)\n    # remove any legacy Namespaces and replace with a single \"args\"\n    namespace = None\n    for k, v in list(config.items()):\n        if isinstance(v, argparse.Namespace):\n            namespace = v\n            del config[k]\n    if namespace is not None:\n        config[\"args\"] = vars(namespace)\n    return config", "\n\ndef validate_and_save(\n    cfg: DictConfig,\n    trainer: Trainer,\n    task: tasks.FairseqTask,\n    epoch_itr,\n    valid_subsets: List[str],\n    end_of_epoch: bool,\n) -> Tuple[List[Optional[float]], bool]:\n    num_updates = trainer.get_num_updates()\n    max_update = cfg.optimization.max_update or math.inf\n\n    # Stopping conditions (and an additional one based on validation loss later\n    # on)\n    should_stop = False\n    if num_updates >= max_update:\n        should_stop = True\n        logger.info(\n            f\"Stopping training due to \"\n            f\"num_updates: {num_updates} >= max_update: {max_update}\"\n        )\n\n    training_time_hours = trainer.cumulative_training_time() / (60 * 60)\n    if (\n        cfg.optimization.stop_time_hours > 0\n        and training_time_hours > cfg.optimization.stop_time_hours\n    ):\n        should_stop = True\n        logger.info(\n            f\"Stopping training due to \"\n            f\"cumulative_training_time: {training_time_hours} > \"\n            f\"stop_time_hours: {cfg.optimization.stop_time_hours} hour(s)\"\n        )\n\n    do_save = (\n        (end_of_epoch and epoch_itr.epoch % cfg.checkpoint.save_interval == 0)\n        or should_stop\n        or (\n            cfg.checkpoint.save_interval_updates > 0\n            and num_updates > 0\n            and num_updates % cfg.checkpoint.save_interval_updates == 0\n            and num_updates >= cfg.dataset.validate_after_updates\n        )\n    )\n    do_validate = (\n        (\n            (not end_of_epoch and do_save)  # validate during mid-epoch saves\n            or (end_of_epoch and epoch_itr.epoch % cfg.dataset.validate_interval == 0)\n            or should_stop\n            or (\n                cfg.dataset.validate_interval_updates > 0\n                and num_updates > 0\n                and num_updates % cfg.dataset.validate_interval_updates == 0\n            )\n        )\n        and not cfg.dataset.disable_validation\n        and num_updates >= cfg.dataset.validate_after_updates\n    )\n\n    # Validate\n    valid_losses = [None]\n    if do_validate:\n        valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\n\n    should_stop |= should_stop_early(cfg, valid_losses[0])\n    \n    # Save checkpoint\n    if do_save or should_stop:\n        \n        if safe_getattr(task, \"lora_tuning\", False) and \"model_part\" in trainer.checkpoint_suffix and \"model_part-0\" not in trainer.checkpoint_suffix:\n            print(\"skip to save checkpoint checkpoint{}.pt\".format(trainer.checkpoint_suffix))\n        else:\n            cp_path = checkpoint_utils.save_checkpoint(\n                cfg.checkpoint, trainer, epoch_itr, valid_losses[0]\n            )\n            if cp_path is not None and hasattr(task, \"post_save\"):\n                task.post_save(cp_path, num_updates)\n\n    return valid_losses, should_stop", "\n\ndef get_training_stats(stats: Dict[str, Any]) -> Dict[str, Any]:\n    stats[\"wall\"] = round(metrics.get_meter(\"default\", \"wall\").elapsed_time, 0)\n    return stats\n\n\ndef validate(\n    cfg: DictConfig,\n    trainer: Trainer,\n    task: tasks.FairseqTask,\n    epoch_itr,\n    subsets: List[str],\n) -> List[Optional[float]]:\n    \"\"\"Evaluate the model on the validation set(s) and return the losses.\"\"\"\n\n    if cfg.dataset.fixed_validation_seed is not None:\n        # set fixed seed for every validation\n        utils.set_torch_seed(cfg.dataset.fixed_validation_seed)\n\n    trainer.begin_valid_epoch(epoch_itr.epoch)\n    valid_losses = []\n    for subset_idx, subset in enumerate(subsets):\n        logger.info('begin validation on \"{}\" subset'.format(subset))\n\n        # Initialize data iterator\n        itr = trainer.get_valid_iterator(subset).next_epoch_itr(\n            shuffle=False, set_dataset_epoch=False  # use a fixed valid set\n        )\n        if cfg.common.tpu:\n            itr = utils.tpu_data_loader(itr)\n        progress = progress_bar.progress_bar(\n            itr,\n            log_format=cfg.common.log_format,\n            log_interval=cfg.common.log_interval,\n            epoch=epoch_itr.epoch,\n            prefix=f\"valid on '{subset}' subset\",\n            aim_repo=(\n                cfg.common.aim_repo\n                if distributed_utils.is_master(cfg.distributed_training)\n                else None\n            ),\n            aim_run_hash=(\n                cfg.common.aim_run_hash\n                if distributed_utils.is_master(cfg.distributed_training)\n                else None\n            ),\n            aim_param_checkpoint_dir=cfg.checkpoint.save_dir,\n            tensorboard_logdir=(\n                cfg.common.tensorboard_logdir\n                if distributed_utils.is_master(cfg.distributed_training)\n                else None\n            ),\n            default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n            wandb_project=(\n                cfg.common.wandb_project\n                if distributed_utils.is_master(cfg.distributed_training)\n                else None\n            ),\n            wandb_run_name=os.environ.get(\n                \"WANDB_NAME\", os.path.basename(cfg.checkpoint.save_dir)\n            ),\n        )\n\n        # create a new root metrics aggregator so validation metrics\n        # don't pollute other aggregators (e.g., train meters)\n        with metrics.aggregate(new_root=True) as agg:\n            for i, sample in enumerate(progress):\n                if (\n                    cfg.dataset.max_valid_steps is not None\n                    and i > cfg.dataset.max_valid_steps\n                ):\n                    break\n                trainer.valid_step(sample)\n\n        # log validation stats\n        # only tracking the best metric on the 1st validation subset\n        tracking_best = subset_idx == 0\n        stats = get_valid_stats(cfg, trainer, agg.get_smoothed_values(), tracking_best)\n\n        if hasattr(task, \"post_validate\"):\n            task.post_validate(trainer.get_model(), stats, agg)\n\n        progress.print(stats, tag=subset, step=trainer.get_num_updates())\n\n        valid_losses.append(stats[cfg.checkpoint.best_checkpoint_metric])\n    return valid_losses", "\n\ndef get_valid_stats(\n    cfg: DictConfig,\n    trainer: Trainer,\n    stats: Dict[str, Any],\n    tracking_best: bool,\n) -> Dict[str, Any]:\n    stats[\"num_updates\"] = trainer.get_num_updates()\n    if tracking_best and hasattr(checkpoint_utils.save_checkpoint, \"best\"):\n        key = \"best_{0}\".format(cfg.checkpoint.best_checkpoint_metric)\n        best_function = max if cfg.checkpoint.maximize_best_checkpoint_metric else min\n        stats[key] = best_function(\n            checkpoint_utils.save_checkpoint.best,\n            stats[cfg.checkpoint.best_checkpoint_metric],\n        )\n    return stats", "\n\ndef cli_main(\n    modify_parser: Optional[Callable[[argparse.ArgumentParser], None]] = None\n) -> None:\n    parser = options.get_training_parser()\n    args = options.parse_args_and_arch(parser, modify_parser=modify_parser)\n\n    cfg = convert_namespace_to_omegaconf(args)\n\n    if cfg.common.use_plasma_view:\n        server = PlasmaStore(path=cfg.common.plasma_path)\n        logger.info(\n            f\"Started plasma server pid {server.server.pid} {cfg.common.plasma_path}\"\n        )\n\n    if args.profile:\n        with torch.cuda.profiler.profile():\n            with torch.autograd.profiler.emit_nvtx():\n                distributed_utils.call_main(cfg, main)\n    else:\n        distributed_utils.call_main(cfg, main)", "\n    # if cfg.common.use_plasma_view:\n    #     server.server.kill()\n\n\nif __name__ == \"__main__\":\n    cli_main()\n"]}
{"filename": "xlmr/src/generate.py", "chunked_list": ["#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nTranslate pre-processed data with a trained model.\n\"\"\"\n\nimport ast", "\nimport ast\nimport logging\nimport math\nimport os\nimport sys\nfrom argparse import Namespace\nfrom itertools import chain\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nfrom omegaconf import DictConfig\n\nfrom fairseq import checkpoint_utils, options, scoring, tasks, utils\nfrom fairseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom fairseq.logging import progress_bar\nfrom fairseq.logging.meters import StopwatchMeter, TimeMeter\nimport utils as distributed_utils", "from fairseq.logging.meters import StopwatchMeter, TimeMeter\nimport utils as distributed_utils\n\n\ndef main(cfg: DictConfig):\n\n    if isinstance(cfg, Namespace):\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    assert cfg.common_eval.path is not None, \"--path required for generation!\"\n    assert (\n        not cfg.generation.sampling or cfg.generation.nbest == cfg.generation.beam\n    ), \"--sampling requires --nbest to be equal to --beam\"\n    assert (\n        cfg.generation.replace_unk is None or cfg.dataset.dataset_impl == \"raw\"\n    ), \"--replace-unk requires a raw text dataset (--dataset-impl=raw)\"\n\n    if cfg.common_eval.results_path is not None:\n        os.makedirs(cfg.common_eval.results_path, exist_ok=True)\n        output_path = os.path.join(\n            cfg.common_eval.results_path,\n            \"generate-{}.txt\".format(cfg.dataset.gen_subset),\n        )\n        with open(output_path, \"w\", buffering=1, encoding=\"utf-8\") as h:\n            return _main(cfg, h)\n    else:\n        return _main(cfg, sys.stdout)", "\n\ndef get_symbols_to_strip_from_output(generator):\n    if hasattr(generator, \"symbols_to_strip_from_output\"):\n        return generator.symbols_to_strip_from_output\n    else:\n        return {generator.eos}\n\n\ndef _main(cfg: DictConfig, output_file):\n    logging.basicConfig(\n        format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n        stream=output_file,\n    )\n    logger = logging.getLogger(\"fairseq_cli.generate\")\n\n    utils.import_user_module(cfg.common)\n\n    if cfg.dataset.max_tokens is None and cfg.dataset.batch_size is None:\n        cfg.dataset.max_tokens = 12000\n    logger.info(cfg)\n\n    # Fix seed for stochastic decoding\n    if cfg.common.seed is not None and not cfg.generation.no_seed_provided:\n        np.random.seed(cfg.common.seed)\n        utils.set_torch_seed(cfg.common.seed)\n\n    use_cuda = torch.cuda.is_available() and not cfg.common.cpu\n\n    # Load dataset splits\n    task = tasks.setup_task(cfg.task)\n\n    # Set dictionaries\n    try:\n        src_dict = getattr(task, \"source_dictionary\", None)\n    except NotImplementedError:\n        src_dict = None\n    tgt_dict = task.target_dictionary\n\n    overrides = ast.literal_eval(cfg.common_eval.model_overrides)\n\n    logger.info(\"cfg.checkpoint.checkpoint_suffix {}\".format(cfg.checkpoint.checkpoint_suffix))\n    # Load ensemble\n    logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n    models, saved_cfg = checkpoint_utils.load_model_ensemble(\n        utils.split_paths(cfg.common_eval.path),\n        arg_overrides=overrides,\n        task=task,\n        suffix=cfg.checkpoint.checkpoint_suffix,\n        strict=(cfg.checkpoint.checkpoint_shard_count == 1),\n        num_shards=cfg.checkpoint.checkpoint_shard_count,\n    )\n\n    # loading the dataset should happen after the checkpoint has been loaded so we can give it the saved task config\n    task.load_dataset(cfg.dataset.gen_subset, task_cfg=saved_cfg.task)\n\n    if cfg.generation.lm_path is not None:\n        overrides[\"data\"] = cfg.task.data\n\n        try:\n            lms, _ = checkpoint_utils.load_model_ensemble(\n                [cfg.generation.lm_path], arg_overrides=overrides, task=None\n            )\n        except:\n            logger.warning(\n                f\"Failed to load language model! Please make sure that the language model dict is the same \"\n                f\"as target dict and is located in the data dir ({cfg.task.data})\"\n            )\n            raise\n\n        assert len(lms) == 1\n    else:\n        lms = [None]\n\n    # Optimize ensemble for generation\n    for model in chain(models, lms):\n        if model is None:\n            continue\n        if cfg.common.fp16:\n            model.half()\n        if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n            model.cuda()\n        model.prepare_for_inference_(cfg)\n\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(cfg.generation.replace_unk)\n    \n    # Load dataset (possibly sharded)\n    itr = task.get_batch_iterator(\n        dataset=task.dataset(cfg.dataset.gen_subset),\n        max_tokens=cfg.dataset.max_tokens,\n        max_sentences=cfg.dataset.batch_size,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), *[m.max_positions() for m in models]\n        ),\n        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n        seed=cfg.common.seed,\n        num_shards=cfg.distributed_training.distributed_world_size,\n        shard_id=cfg.distributed_training.distributed_rank,\n        num_workers=cfg.dataset.num_workers,\n        data_buffer_size=cfg.dataset.data_buffer_size,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_interval=cfg.common.log_interval,\n        default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n    )\n\n    # Initialize generator\n    gen_timer = StopwatchMeter()\n    generator = task.build_generator(models, args=cfg.generation)\n\n    # Handle tokenization and BPE\n    tokenizer = task.build_tokenizer(cfg.tokenizer)\n    bpe = task.build_bpe(cfg.bpe)\n    \n    def decode_fn(x):\n        if bpe is not None:\n            x = bpe.decode(x.tolist())\n        if tokenizer is not None:\n            x = tokenizer.decode(x)\n        return x\n\n    scorer = scoring.build_scorer(cfg.scoring, tgt_dict)\n\n    num_sentences = 0\n    has_target = True\n    wps_meter = TimeMeter()\n    for sample in progress:\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n        if \"net_input\" not in sample:\n            continue\n\n        prefix_tokens = None\n        if cfg.generation.prefix_size > 0:\n            prefix_tokens = sample[\"target\"][:, : cfg.generation.prefix_size]\n\n        constraints = None\n        if \"constraints\" in sample:\n            constraints = sample[\"constraints\"]\n\n        gen_timer.start()\n        hypos = task.inference_step(\n            generator,\n            models,\n            sample,\n            prefix_tokens=prefix_tokens,\n            constraints=constraints,\n        )\n        num_generated_tokens = sum(len(h[0][\"tokens\"]) for h in hypos)\n        gen_timer.stop(num_generated_tokens)\n\n        for i, sample_id in enumerate(sample[\"id\"].tolist()):\n            has_target = sample[\"target\"] is not None\n\n            # Remove padding\n            if \"src_tokens\" in sample[\"net_input\"]:\n                src_tokens = utils.strip_pad(\n                    sample[\"net_input\"][\"src_tokens\"][i, :], tgt_dict.pad()\n                )\n            else:\n                src_tokens = None\n\n            target_tokens = None\n            if has_target:\n                target_tokens = (\n                    utils.strip_pad(sample[\"target\"][i, :], tgt_dict.pad()).int().cpu()\n                )\n\n            # Either retrieve the original sentences or regenerate them from tokens.\n            if align_dict is not None:\n                src_str = task.dataset(cfg.dataset.gen_subset).src.get_original_text(\n                    sample_id\n                )\n                target_str = task.dataset(cfg.dataset.gen_subset).tgt.get_original_text(\n                    sample_id\n                )\n            else:\n                if src_dict is not None:\n                    src_str = src_dict.string(src_tokens, cfg.common_eval.post_process)\n                else:\n                    src_str = \"\"\n                if has_target:\n                    target_str = tgt_dict.string(\n                        target_tokens,\n                        cfg.common_eval.post_process,\n                        escape_unk=True,\n                        extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n                            generator\n                        ),\n                    )\n\n            src_str = decode_fn(src_tokens)\n            if has_target:\n                target_str = decode_fn(target_tokens)\n\n            if \"-model_part\" in cfg.checkpoint.checkpoint_suffix and \"-model_part-0\" not in cfg.checkpoint.checkpoint_suffix:\n                print(distributed_utils.get_model_parallel_rank())\n                continue\n\n            if not cfg.common_eval.quiet:\n                if src_dict is not None:\n                    print(\"S-{}\\t{}\".format(sample_id, src_str), file=output_file)\n                # if has_target:\n                #     print(\"T-{}\\t{}\".format(sample_id, target_str), file=output_file)\n\n            # Process top predictions\n            for j, hypo in enumerate(hypos[i][: cfg.generation.nbest]):\n                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n                    src_str=src_str,\n                    alignment=hypo[\"alignment\"],\n                    align_dict=align_dict,\n                    tgt_dict=tgt_dict,\n                    remove_bpe=cfg.common_eval.post_process,\n                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n                )\n                detok_hypo_str = decode_fn(hypo_tokens)\n                if not cfg.common_eval.quiet:\n                    score = hypo[\"score\"] / math.log(2)  # convert to base 2\n                    # original hypothesis (after tokenization and BPE)\n                    print(\n                        \"H-{}\\t{}\\t{}\".format(sample_id, score, hypo_str),\n                        file=output_file,\n                    )\n                    # detokenized hypothesis\n                    print(\n                        \"D-{}\\t{}\\t{}\".format(sample_id, score, detok_hypo_str),\n                        file=output_file,\n                    )\n                    # print(\n                    #     \"P-{}\\t{}\".format(\n                    #         sample_id,\n                    #         \" \".join(\n                    #             map(\n                    #                 lambda x: \"{:.4f}\".format(x),\n                    #                 # convert from base e to base 2\n                    #                 hypo[\"positional_scores\"]\n                    #                 .div_(math.log(2))\n                    #                 .tolist(),\n                    #             )\n                    #         ),\n                    #     ),\n                    #     file=output_file,\n                    # )\n\n                    if cfg.generation.print_alignment == \"hard\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\n                                        \"{}-{}\".format(src_idx, tgt_idx)\n                                        for src_idx, tgt_idx in alignment\n                                    ]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n                    if cfg.generation.print_alignment == \"soft\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\",\".join(src_probs) for src_probs in alignment]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.print_step:\n                        print(\n                            \"I-{}\\t{}\".format(sample_id, hypo[\"steps\"]),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.retain_iter_history:\n                        for step, h in enumerate(hypo[\"history\"]):\n                            _, h_str, _ = utils.post_process_prediction(\n                                hypo_tokens=h[\"tokens\"].int().cpu(),\n                                src_str=src_str,\n                                alignment=None,\n                                align_dict=None,\n                                tgt_dict=tgt_dict,\n                                remove_bpe=None,\n                            )\n                            print(\n                                \"E-{}_{}\\t{}\".format(sample_id, step, h_str),\n                                file=output_file,\n                            )\n\n                # Score only the top hypothesis\n                if has_target and j == 0:\n                    if (\n                        align_dict is not None\n                        or cfg.common_eval.post_process is not None\n                    ):\n                        # Convert back to tokens for evaluation with unk replacement and/or without BPE\n                        target_tokens = tgt_dict.encode_line(\n                            target_str, add_if_not_exist=True\n                        )\n                        hypo_tokens = tgt_dict.encode_line(\n                            detok_hypo_str, add_if_not_exist=True\n                        )\n                    if hasattr(scorer, \"add_string\"):\n                        scorer.add_string(target_str, detok_hypo_str)\n                    else:\n                        scorer.add(target_tokens, hypo_tokens)\n\n        wps_meter.update(num_generated_tokens)\n        progress.log({\"wps\": round(wps_meter.avg)})\n        num_sentences += (\n            sample[\"nsentences\"] if \"nsentences\" in sample else sample[\"id\"].numel()\n        )\n\n    logger.info(\"NOTE: hypothesis and token scores are output in base 2\")\n    logger.info(\n        \"Translated {:,} sentences ({:,} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)\".format(\n            num_sentences,\n            gen_timer.n,\n            gen_timer.sum,\n            num_sentences / gen_timer.sum,\n            1.0 / gen_timer.avg,\n        )\n    )\n    # if has_target:\n    #     if cfg.bpe and not cfg.generation.sacrebleu:\n    #         if cfg.common_eval.post_process:\n    #             logger.warning(\n    #                 \"BLEU score is being computed by splitting detokenized string on spaces, this is probably not what you want. Use --sacrebleu for standard 13a BLEU tokenization\"\n    #             )\n    #         else:\n    #             logger.warning(\n    #                 \"If you are using BPE on the target side, the BLEU score is computed on BPE tokens, not on proper words.  Use --sacrebleu for standard 13a BLEU tokenization\"\n    #             )\n    #     # use print to be consistent with other main outputs: S-, H-, T-, D- and so on\n    #     print(\n    #         \"Generate {} with beam={}: {}\".format(\n    #             cfg.dataset.gen_subset, cfg.generation.beam, scorer.result_string()\n    #         ),\n    #         file=output_file,\n    #     )\n\n    return scorer", "\ndef _main(cfg: DictConfig, output_file):\n    logging.basicConfig(\n        format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n        stream=output_file,\n    )\n    logger = logging.getLogger(\"fairseq_cli.generate\")\n\n    utils.import_user_module(cfg.common)\n\n    if cfg.dataset.max_tokens is None and cfg.dataset.batch_size is None:\n        cfg.dataset.max_tokens = 12000\n    logger.info(cfg)\n\n    # Fix seed for stochastic decoding\n    if cfg.common.seed is not None and not cfg.generation.no_seed_provided:\n        np.random.seed(cfg.common.seed)\n        utils.set_torch_seed(cfg.common.seed)\n\n    use_cuda = torch.cuda.is_available() and not cfg.common.cpu\n\n    # Load dataset splits\n    task = tasks.setup_task(cfg.task)\n\n    # Set dictionaries\n    try:\n        src_dict = getattr(task, \"source_dictionary\", None)\n    except NotImplementedError:\n        src_dict = None\n    tgt_dict = task.target_dictionary\n\n    overrides = ast.literal_eval(cfg.common_eval.model_overrides)\n\n    logger.info(\"cfg.checkpoint.checkpoint_suffix {}\".format(cfg.checkpoint.checkpoint_suffix))\n    # Load ensemble\n    logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n    models, saved_cfg = checkpoint_utils.load_model_ensemble(\n        utils.split_paths(cfg.common_eval.path),\n        arg_overrides=overrides,\n        task=task,\n        suffix=cfg.checkpoint.checkpoint_suffix,\n        strict=(cfg.checkpoint.checkpoint_shard_count == 1),\n        num_shards=cfg.checkpoint.checkpoint_shard_count,\n    )\n\n    # loading the dataset should happen after the checkpoint has been loaded so we can give it the saved task config\n    task.load_dataset(cfg.dataset.gen_subset, task_cfg=saved_cfg.task)\n\n    if cfg.generation.lm_path is not None:\n        overrides[\"data\"] = cfg.task.data\n\n        try:\n            lms, _ = checkpoint_utils.load_model_ensemble(\n                [cfg.generation.lm_path], arg_overrides=overrides, task=None\n            )\n        except:\n            logger.warning(\n                f\"Failed to load language model! Please make sure that the language model dict is the same \"\n                f\"as target dict and is located in the data dir ({cfg.task.data})\"\n            )\n            raise\n\n        assert len(lms) == 1\n    else:\n        lms = [None]\n\n    # Optimize ensemble for generation\n    for model in chain(models, lms):\n        if model is None:\n            continue\n        if cfg.common.fp16:\n            model.half()\n        if use_cuda and not cfg.distributed_training.pipeline_model_parallel:\n            model.cuda()\n        model.prepare_for_inference_(cfg)\n\n    # Load alignment dictionary for unknown word replacement\n    # (None if no unknown word replacement, empty if no path to align dictionary)\n    align_dict = utils.load_align_dict(cfg.generation.replace_unk)\n    \n    # Load dataset (possibly sharded)\n    itr = task.get_batch_iterator(\n        dataset=task.dataset(cfg.dataset.gen_subset),\n        max_tokens=cfg.dataset.max_tokens,\n        max_sentences=cfg.dataset.batch_size,\n        max_positions=utils.resolve_max_positions(\n            task.max_positions(), *[m.max_positions() for m in models]\n        ),\n        ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n        required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n        seed=cfg.common.seed,\n        num_shards=cfg.distributed_training.distributed_world_size,\n        shard_id=cfg.distributed_training.distributed_rank,\n        num_workers=cfg.dataset.num_workers,\n        data_buffer_size=cfg.dataset.data_buffer_size,\n    ).next_epoch_itr(shuffle=False)\n    progress = progress_bar.progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_interval=cfg.common.log_interval,\n        default_log_format=(\"tqdm\" if not cfg.common.no_progress_bar else \"simple\"),\n    )\n\n    # Initialize generator\n    gen_timer = StopwatchMeter()\n    generator = task.build_generator(models, args=cfg.generation)\n\n    # Handle tokenization and BPE\n    tokenizer = task.build_tokenizer(cfg.tokenizer)\n    bpe = task.build_bpe(cfg.bpe)\n    \n    def decode_fn(x):\n        if bpe is not None:\n            x = bpe.decode(x.tolist())\n        if tokenizer is not None:\n            x = tokenizer.decode(x)\n        return x\n\n    scorer = scoring.build_scorer(cfg.scoring, tgt_dict)\n\n    num_sentences = 0\n    has_target = True\n    wps_meter = TimeMeter()\n    for sample in progress:\n        sample = utils.move_to_cuda(sample) if use_cuda else sample\n        if \"net_input\" not in sample:\n            continue\n\n        prefix_tokens = None\n        if cfg.generation.prefix_size > 0:\n            prefix_tokens = sample[\"target\"][:, : cfg.generation.prefix_size]\n\n        constraints = None\n        if \"constraints\" in sample:\n            constraints = sample[\"constraints\"]\n\n        gen_timer.start()\n        hypos = task.inference_step(\n            generator,\n            models,\n            sample,\n            prefix_tokens=prefix_tokens,\n            constraints=constraints,\n        )\n        num_generated_tokens = sum(len(h[0][\"tokens\"]) for h in hypos)\n        gen_timer.stop(num_generated_tokens)\n\n        for i, sample_id in enumerate(sample[\"id\"].tolist()):\n            has_target = sample[\"target\"] is not None\n\n            # Remove padding\n            if \"src_tokens\" in sample[\"net_input\"]:\n                src_tokens = utils.strip_pad(\n                    sample[\"net_input\"][\"src_tokens\"][i, :], tgt_dict.pad()\n                )\n            else:\n                src_tokens = None\n\n            target_tokens = None\n            if has_target:\n                target_tokens = (\n                    utils.strip_pad(sample[\"target\"][i, :], tgt_dict.pad()).int().cpu()\n                )\n\n            # Either retrieve the original sentences or regenerate them from tokens.\n            if align_dict is not None:\n                src_str = task.dataset(cfg.dataset.gen_subset).src.get_original_text(\n                    sample_id\n                )\n                target_str = task.dataset(cfg.dataset.gen_subset).tgt.get_original_text(\n                    sample_id\n                )\n            else:\n                if src_dict is not None:\n                    src_str = src_dict.string(src_tokens, cfg.common_eval.post_process)\n                else:\n                    src_str = \"\"\n                if has_target:\n                    target_str = tgt_dict.string(\n                        target_tokens,\n                        cfg.common_eval.post_process,\n                        escape_unk=True,\n                        extra_symbols_to_ignore=get_symbols_to_strip_from_output(\n                            generator\n                        ),\n                    )\n\n            src_str = decode_fn(src_tokens)\n            if has_target:\n                target_str = decode_fn(target_tokens)\n\n            if \"-model_part\" in cfg.checkpoint.checkpoint_suffix and \"-model_part-0\" not in cfg.checkpoint.checkpoint_suffix:\n                print(distributed_utils.get_model_parallel_rank())\n                continue\n\n            if not cfg.common_eval.quiet:\n                if src_dict is not None:\n                    print(\"S-{}\\t{}\".format(sample_id, src_str), file=output_file)\n                # if has_target:\n                #     print(\"T-{}\\t{}\".format(sample_id, target_str), file=output_file)\n\n            # Process top predictions\n            for j, hypo in enumerate(hypos[i][: cfg.generation.nbest]):\n                hypo_tokens, hypo_str, alignment = utils.post_process_prediction(\n                    hypo_tokens=hypo[\"tokens\"].int().cpu(),\n                    src_str=src_str,\n                    alignment=hypo[\"alignment\"],\n                    align_dict=align_dict,\n                    tgt_dict=tgt_dict,\n                    remove_bpe=cfg.common_eval.post_process,\n                    extra_symbols_to_ignore=get_symbols_to_strip_from_output(generator),\n                )\n                detok_hypo_str = decode_fn(hypo_tokens)\n                if not cfg.common_eval.quiet:\n                    score = hypo[\"score\"] / math.log(2)  # convert to base 2\n                    # original hypothesis (after tokenization and BPE)\n                    print(\n                        \"H-{}\\t{}\\t{}\".format(sample_id, score, hypo_str),\n                        file=output_file,\n                    )\n                    # detokenized hypothesis\n                    print(\n                        \"D-{}\\t{}\\t{}\".format(sample_id, score, detok_hypo_str),\n                        file=output_file,\n                    )\n                    # print(\n                    #     \"P-{}\\t{}\".format(\n                    #         sample_id,\n                    #         \" \".join(\n                    #             map(\n                    #                 lambda x: \"{:.4f}\".format(x),\n                    #                 # convert from base e to base 2\n                    #                 hypo[\"positional_scores\"]\n                    #                 .div_(math.log(2))\n                    #                 .tolist(),\n                    #             )\n                    #         ),\n                    #     ),\n                    #     file=output_file,\n                    # )\n\n                    if cfg.generation.print_alignment == \"hard\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\n                                        \"{}-{}\".format(src_idx, tgt_idx)\n                                        for src_idx, tgt_idx in alignment\n                                    ]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n                    if cfg.generation.print_alignment == \"soft\":\n                        print(\n                            \"A-{}\\t{}\".format(\n                                sample_id,\n                                \" \".join(\n                                    [\",\".join(src_probs) for src_probs in alignment]\n                                ),\n                            ),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.print_step:\n                        print(\n                            \"I-{}\\t{}\".format(sample_id, hypo[\"steps\"]),\n                            file=output_file,\n                        )\n\n                    if cfg.generation.retain_iter_history:\n                        for step, h in enumerate(hypo[\"history\"]):\n                            _, h_str, _ = utils.post_process_prediction(\n                                hypo_tokens=h[\"tokens\"].int().cpu(),\n                                src_str=src_str,\n                                alignment=None,\n                                align_dict=None,\n                                tgt_dict=tgt_dict,\n                                remove_bpe=None,\n                            )\n                            print(\n                                \"E-{}_{}\\t{}\".format(sample_id, step, h_str),\n                                file=output_file,\n                            )\n\n                # Score only the top hypothesis\n                if has_target and j == 0:\n                    if (\n                        align_dict is not None\n                        or cfg.common_eval.post_process is not None\n                    ):\n                        # Convert back to tokens for evaluation with unk replacement and/or without BPE\n                        target_tokens = tgt_dict.encode_line(\n                            target_str, add_if_not_exist=True\n                        )\n                        hypo_tokens = tgt_dict.encode_line(\n                            detok_hypo_str, add_if_not_exist=True\n                        )\n                    if hasattr(scorer, \"add_string\"):\n                        scorer.add_string(target_str, detok_hypo_str)\n                    else:\n                        scorer.add(target_tokens, hypo_tokens)\n\n        wps_meter.update(num_generated_tokens)\n        progress.log({\"wps\": round(wps_meter.avg)})\n        num_sentences += (\n            sample[\"nsentences\"] if \"nsentences\" in sample else sample[\"id\"].numel()\n        )\n\n    logger.info(\"NOTE: hypothesis and token scores are output in base 2\")\n    logger.info(\n        \"Translated {:,} sentences ({:,} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)\".format(\n            num_sentences,\n            gen_timer.n,\n            gen_timer.sum,\n            num_sentences / gen_timer.sum,\n            1.0 / gen_timer.avg,\n        )\n    )\n    # if has_target:\n    #     if cfg.bpe and not cfg.generation.sacrebleu:\n    #         if cfg.common_eval.post_process:\n    #             logger.warning(\n    #                 \"BLEU score is being computed by splitting detokenized string on spaces, this is probably not what you want. Use --sacrebleu for standard 13a BLEU tokenization\"\n    #             )\n    #         else:\n    #             logger.warning(\n    #                 \"If you are using BPE on the target side, the BLEU score is computed on BPE tokens, not on proper words.  Use --sacrebleu for standard 13a BLEU tokenization\"\n    #             )\n    #     # use print to be consistent with other main outputs: S-, H-, T-, D- and so on\n    #     print(\n    #         \"Generate {} with beam={}: {}\".format(\n    #             cfg.dataset.gen_subset, cfg.generation.beam, scorer.result_string()\n    #         ),\n    #         file=output_file,\n    #     )\n\n    return scorer", "\n\ndef cli_main():\n    parser = options.get_generation_parser()\n    # TODO: replace this workaround with refactoring of `AudioPretraining`\n    parser.add_argument(\n        \"--arch\",\n        \"-a\",\n        metavar=\"ARCH\",\n        default=\"wav2vec2\",\n        help=\"Model architecture. For constructing tasks that rely on \"\n        \"model args (e.g. `AudioPretraining`)\",\n    )\n    args = options.parse_args_and_arch(parser)\n\n    if args.model_parallel_size > 1:\n        print(\"run megatron mode...\")\n        distributed_utils.call_main(convert_namespace_to_omegaconf(args), main)\n    else:\n        main(args)", "\n\nif __name__ == \"__main__\":\n    cli_main()\n"]}
{"filename": "xlmr/src/trainer.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nTrain a network across multiple GPUs.\n\"\"\"\n\nimport contextlib", "\nimport contextlib\nimport logging\nimport os\nimport sys\nimport time\nfrom argparse import Namespace\nfrom itertools import chain\nfrom typing import Any, Dict, List\n", "from typing import Any, Dict, List\n\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom fairseq import checkpoint_utils, models, optim, utils\nfrom fairseq.dataclass.configs import FairseqConfig\nfrom fairseq.dataclass.utils import convert_namespace_to_omegaconf\n# from fairseq.distributed import utils as distributed_utils\nfrom fairseq.file_io import PathManager", "# from fairseq.distributed import utils as distributed_utils\nfrom fairseq.file_io import PathManager\nfrom fairseq.logging import meters, metrics\nfrom fairseq.models.ema import build_ema\nfrom fairseq.nan_detector import NanDetector\nfrom fairseq.optim import lr_scheduler\nfrom fairseq.utils import safe_hasattr\nimport utils as distributed_utils\n\n", "\n\nlogger = logging.getLogger(__name__)\n\n\nclass Trainer(object):\n    \"\"\"Main class for data parallel training.\n\n    This class supports synchronous distributed data parallel training,\n    where multiple workers each have a full model replica and gradients\n    are accumulated across workers before each update. We use\n    :class:`~torch.nn.parallel.DistributedDataParallel` to handle\n    communication of the gradients across workers.\n    \"\"\"\n\n    def __init__(self, cfg: FairseqConfig, task, model, criterion, quantizer=None):\n\n        if isinstance(cfg, Namespace):\n            logger.warning(\n                \"argparse.Namespace configuration is deprecated! Automatically converting to OmegaConf\"\n            )\n            cfg = convert_namespace_to_omegaconf(cfg)\n\n        self.cfg = cfg\n        self.task = task\n\n        # catalog shared parameters\n        shared_params = _catalog_shared_params(model)\n        self.tpu = cfg.common.tpu\n        self.cuda = torch.cuda.is_available() and not cfg.common.cpu and not self.tpu\n        if self.cuda:\n            self.device = torch.device(\"cuda\")\n        elif self.tpu:\n            self.device = utils.get_tpu_device()\n        else:\n            self.device = torch.device(\"cpu\")\n\n        if self.is_fsdp:\n            import fairscale\n\n            if self.cfg.common.bf16:\n                raise ValueError(\n                    \"FullyShardedDataParallel is not compatible with --bf16 or \"\n                    \"--memory-efficient-bf16\"\n                )\n            if self.cfg.distributed_training.zero_sharding != \"none\":\n                raise ValueError(\n                    \"FullyShardedDataParallel is not compatible with --zero-sharding \"\n                    \"option (it's already built in)\"\n                )\n            if (\n                max(self.cfg.optimization.update_freq) > 1\n                and fairscale.__version__ < \"0.4.0\"\n            ):\n                raise RuntimeError(\n                    \"Please update to fairscale 0.4.0 or newer when combining \"\n                    \"--update-freq with FullyShardedDataParallel\"\n                )\n        else:\n            if (\n                hasattr(self.cfg.distributed_training, \"cpu_offload\")\n                and self.cfg.distributed_training.cpu_offload\n            ):\n                raise ValueError(\"--cpu-offload requires --ddp-backend=fully_sharded\")\n\n        # copy model and criterion to current device/dtype\n        self._criterion = criterion\n        self._model = model\n        if not self.is_fsdp:\n            if cfg.common.fp16:\n                assert not cfg.common.amp, \"Cannot use fp16 and AMP together\"\n                self._criterion = self._criterion.half()\n                self._model = self._model.half()\n            elif cfg.common.bf16:\n                self._criterion = self._criterion.to(dtype=torch.bfloat16)\n                self._model = self._model.to(dtype=torch.bfloat16)\n            elif cfg.common.amp:\n                self._amp_retries = 0\n        if (\n            not cfg.distributed_training.pipeline_model_parallel\n            # the DistributedFairseqModel wrapper will handle moving to device,\n            # so only handle cases which don't use the wrapper\n            and not self.use_distributed_wrapper\n        ):\n            self._criterion = self._criterion.to(device=self.device)\n            self._model = self._model.to(device=self.device)\n        self.pipeline_model_parallel = cfg.distributed_training.pipeline_model_parallel\n        self.last_device = None\n        if self.cuda and self.pipeline_model_parallel:\n            self.last_device = torch.device(\n                cfg.distributed_training.pipeline_devices[-1]\n            )\n\n        # check that shared parameters are preserved after device transfer\n        for shared_param in shared_params:\n            ref = _get_module_by_path(self._model, shared_param[0])\n            for path in shared_param[1:]:\n                logger.info(\n                    \"detected shared parameter: {} <- {}\".format(shared_param[0], path)\n                )\n                _set_module_by_path(self._model, path, ref)\n\n        self._dummy_batch = None  # indicates we don't have a dummy batch at first\n        self._lr_scheduler = None\n        self._num_updates = 0\n        self._num_xla_compiles = 0  # for TPUs\n        self._optim_history = None\n        self._optimizer = None\n        self._warn_once = set()\n        self._wrapped_criterion = None\n        self._wrapped_model = None\n        self._ema = None\n\n        # TODO(myleott): support tpu\n        if self.cuda and self.data_parallel_world_size > 1:\n            self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)\n        else:\n            self._grad_norm_buf = None\n\n        self.quantizer = quantizer\n        if self.quantizer is not None:\n            self.quantizer.set_trainer(self)\n\n        # get detailed cuda environment\n        if self.cuda:\n            self.cuda_env = utils.CudaEnvironment()\n            if self.data_parallel_world_size > 1:\n                self.cuda_env_arr = distributed_utils.all_gather_list(\n                    self.cuda_env, group=distributed_utils.get_global_group()\n                )\n            else:\n                self.cuda_env_arr = [self.cuda_env]\n            if self.data_parallel_rank == 0:\n                utils.CudaEnvironment.pretty_print_cuda_env_list(self.cuda_env_arr)\n        else:\n            self.cuda_env = None\n            self.cuda_env_arr = None\n\n        metrics.log_start_time(\"wall\", priority=790, round=0)\n\n        self._start_time = time.time()\n        self._previous_training_time = 0\n        self._cumulative_training_time = None\n\n    def reinitialize(self):\n        \"\"\"Reinitialize the Trainer, typically after model params change.\"\"\"\n        self._lr_scheduler = None\n        self._optimizer = None\n        self._wrapped_criterion = None\n        self._wrapped_model = None\n\n    @property\n    def data_parallel_world_size(self):\n        if self.cfg.distributed_training.distributed_world_size == 1:\n            return 1\n        return distributed_utils.get_data_parallel_world_size()\n\n    @property\n    def data_parallel_process_group(self):\n        return distributed_utils.get_data_parallel_group()\n\n    @property\n    def data_parallel_rank(self):\n        if self.cfg.distributed_training.distributed_world_size == 1:\n            return 0\n        return distributed_utils.get_data_parallel_rank()\n\n    @property\n    def is_data_parallel_master(self):\n        # NOTE: this returns true for all model parallel replicas with data\n        # parallel rank 0\n        return self.data_parallel_rank == 0\n\n    @property\n    def use_distributed_wrapper(self) -> bool:\n        return (\n            self.data_parallel_world_size > 1 and not self.cfg.optimization.use_bmuf\n        ) or (self.is_fsdp and self.cfg.distributed_training.cpu_offload)\n\n    @property\n    def should_save_checkpoint_on_current_rank(self) -> bool:\n        \"\"\"Indicates whether to save checkpoints on the current DDP rank.\"\"\"\n        if (\n            self.is_fsdp and self.cfg.distributed_training.use_sharded_state\n        ) or getattr(self.cfg.model, \"base_layers\", 0) > 0:\n            return True\n        else:\n            return self.is_data_parallel_master\n\n    @property\n    def always_call_state_dict_during_save_checkpoint(self) -> bool:\n        if self.is_fsdp and not self.cfg.distributed_training.use_sharded_state:\n            # FSDP calls communication collective when consolidating checkpoints\n            return True\n        else:\n            return False\n\n    @property\n    def checkpoint_suffix(self) -> str:\n        \"\"\"Suffix to add to the checkpoint file name.\"\"\"\n        if self.is_fsdp and self.cfg.distributed_training.use_sharded_state:\n            return self.cfg.checkpoint.checkpoint_suffix + \"-shard{0}\".format(\n                self.data_parallel_rank\n            )\n        else:\n            return self.cfg.checkpoint.checkpoint_suffix or \"\"\n\n    @property\n    def criterion(self):\n        if self._wrapped_criterion is None:\n            if utils.has_parameters(self._criterion) and self.use_distributed_wrapper:\n                self._wrapped_criterion = models.DistributedFairseqModel(\n                    self.cfg.distributed_training,\n                    self._criterion,\n                    process_group=self.data_parallel_process_group,\n                    device=self.device,\n                )\n            else:\n                self._wrapped_criterion = self._criterion\n        return self._wrapped_criterion\n\n    @property\n    def model(self):\n        if self._wrapped_model is None:\n            if self.use_distributed_wrapper:\n                self._wrapped_model = models.DistributedFairseqModel(\n                    self.cfg.distributed_training,\n                    self._model,\n                    process_group=self.data_parallel_process_group,\n                    device=self.device,\n                )\n            else:\n                self._wrapped_model = self._model\n        return self._wrapped_model\n\n    @property\n    def ema(self):\n        if self._ema is None:\n            self._build_ema()\n        return self._ema\n\n    def _build_ema(self):\n        if self.cfg.ema.store_ema:\n            self._ema = build_ema(self._model, self.cfg.ema, self.device)\n            logger.info(\"Exponential Moving Average Shadow Model is initialized.\")\n\n    @property\n    def optimizer(self):\n        if self._optimizer is None:\n            self._build_optimizer()\n        return self._optimizer\n\n    @property\n    def lr_scheduler(self):\n        if self._lr_scheduler is None:\n            self._build_optimizer()  # this will initialize self._lr_scheduler\n        return self._lr_scheduler\n\n    def _build_optimizer(self):\n\n        if (\n            self.cfg.optimization.debug_param_names\n            and self.cfg.common.fp16_no_flatten_grads\n        ):\n            params = []\n            self.param_names = []\n\n            for n, p in chain(\n                self.model.named_parameters(), self.criterion.named_parameters()\n            ):\n                if p.requires_grad:\n                    params.append(p)\n                    self.param_names.append(n)\n        else:\n            params = list(\n                filter(\n                    lambda p: p.requires_grad,\n                    chain(self.model.parameters(), self.criterion.parameters()),\n                )\n            )\n\n        if self.is_fsdp and self.cfg.common.fp16:\n            # FullyShardedDataParallel always uses MemoryEfficientFP16 wrapper,\n            # mostly for the grad scaling. But if we don't have the\n            # --memory-efficient-fp16 flag set, then we're effectively doing\n            # regular --fp16 and can allow the use of optimizers that would\n            # otherwise be unsupported by MemoryEfficientFP16Optimizer.\n            allow_unsupported = not self.cfg.common.memory_efficient_fp16\n            self._optimizer = optim.MemoryEfficientFP16Optimizer.build_optimizer(\n                self.cfg, params, allow_unsupported=allow_unsupported\n            )\n        elif self.cfg.common.fp16 or self.cfg.common.bf16 or self.cfg.common.amp:\n            if self.cuda and torch.cuda.get_device_capability(0)[0] < 7:\n                logger.info(\n                    \"NOTE: your device does NOT support faster training with --fp16 or --amp, \"\n                    \"please switch to FP32 which is likely to be faster\"\n                )\n            if (\n                self.cfg.common.memory_efficient_fp16\n                or self.cfg.common.memory_efficient_bf16\n            ):\n                self._optimizer = optim.MemoryEfficientFP16Optimizer.build_optimizer(\n                    self.cfg, params\n                )\n            elif self.cfg.common.amp:\n                self._optimizer = optim.AMPOptimizer.build_optimizer(self.cfg, params)\n            else:\n                self._optimizer = optim.FP16Optimizer.build_optimizer(self.cfg, params)\n        else:\n            if self.cuda and torch.cuda.get_device_capability(0)[0] >= 7:\n                logger.info(\n                    \"NOTE: your device may support faster training with --fp16 or --amp\"\n                )\n            self._optimizer = optim.build_optimizer(self.cfg.optimizer, params)\n\n        if self.is_fsdp:\n            assert (\n                not self.cfg.optimization.use_bmuf\n            ), \"--ddp-backend=fully_sharded is not compatible with BMUF\"\n            assert self._optimizer.supports_flat_params, (\n                \"--ddp-backend=fully_sharded is only compatible with pointwise \"\n                \"optimizers (e.g., Adam, AdamW, Adadelta, Adamax, SGD, etc.). \"\n                \"However, the sharding will result in slightly different results when \"\n                \"using non-pointwise optimizers (e.g., Adagrad, Adafactor, LAMB)\"\n            )\n\n        if self.cfg.optimization.use_bmuf:\n            self._optimizer = optim.FairseqBMUF(\n                self.cfg.bmuf,\n                self._optimizer,\n            )\n\n        if self.cfg.distributed_training.zero_sharding == \"os\":\n            if (\n                self.cfg.common.fp16\n                and not self.cfg.common.memory_efficient_fp16\n                and not self.cfg.common.memory_efficient_bf16\n            ) and not self.cfg.common.fp16_no_flatten_grads:\n                raise ValueError(\n                    \"ZeRO is incomptabile with fp16 and flattened grads. \"\n                    \"Please use --fp16-no-flatten-grads\"\n                )\n            else:\n                optim.shard_(self._optimizer, self.data_parallel_process_group)\n\n        # We should initialize the learning rate scheduler immediately after\n        # building the optimizer, so that the initial learning rate is set.\n        self._lr_scheduler = lr_scheduler.build_lr_scheduler(\n            self.cfg.lr_scheduler,\n            self.optimizer,\n        )\n        self._lr_scheduler.step_update(0)\n\n    @property\n    def is_fsdp(self):\n        return self.cfg.distributed_training.ddp_backend == \"fully_sharded\"\n\n    def consolidate_optimizer(self):\n        \"\"\"For OSS, we need to consolidate the state dict.\"\"\"\n        if self.cfg.checkpoint.no_save_optimizer_state:\n            return\n        self._gathered_optim_state = None\n        if hasattr(self.optimizer.optimizer, \"consolidate_state_dict\"):\n            self.optimizer.optimizer.consolidate_state_dict()\n        elif self.is_fsdp and not self.model.use_sharded_state:\n            st = self.model.gather_full_optim_state_dict(\n                self.optimizer\n            )  # only returns on rank 0\n            self._gathered_optim_state = st\n\n    def state_dict(self):\n        state_dict = {\n            \"args\": None,  # legacy\n            \"cfg\": (\n                OmegaConf.to_container(self.cfg, resolve=True, enum_to_str=True)\n                if OmegaConf.is_config(self.cfg)\n                else self.cfg\n            ),\n            \"model\": self.model.state_dict(),\n            \"criterion\": (\n                self.criterion.state_dict()\n                if utils.has_parameters(self.criterion)\n                else None\n            ),\n            \"optimizer_history\": (self._optim_history or [])\n            + [\n                {\n                    \"criterion_name\": self.get_criterion().__class__.__name__,\n                    \"optimizer_name\": self.optimizer.__class__.__name__,\n                    \"lr_scheduler_state\": self.lr_scheduler.state_dict(),\n                    \"num_updates\": self.get_num_updates(),\n                }\n            ],\n            \"task_state\": self.task.state_dict() if self.task is not None else {},\n            \"extra_state\": {\n                \"metrics\": metrics.state_dict(),\n                \"previous_training_time\": self.cumulative_training_time(),\n            },\n        }\n        if self.cfg.ema.store_ema:\n            # Save EMA model state as extra state\n            state_dict[\"extra_state\"][\"ema\"] = self.ema.get_model().state_dict()\n            if self.cfg.ema.ema_fp32:\n                # Save EMA params in fp32\n                state_dict[\"extra_state\"][\"ema_fp32_params\"] = self.ema.fp32_params\n        if not self.cfg.checkpoint.no_save_optimizer_state:\n            if self._gathered_optim_state is not None:\n                state_dict[\"last_optimizer_state\"] = self._gathered_optim_state\n                self._gathered_optim_state = None\n            else:\n                state_dict[\"last_optimizer_state\"] = self.optimizer.state_dict()\n        if self.is_fsdp:\n            # save meta data for recombining checkpoint upon loading\n            state_dict[\"fsdp_metadata\"] = self.model.local_metadata_dict()\n        return state_dict\n\n    def save_checkpoint(self, filename, extra_state):\n        \"\"\"Save all training state in a checkpoint file.\"\"\"\n        if self.should_save_checkpoint_on_current_rank:\n\n            logger.info(f\"Saving checkpoint to {os.path.abspath(filename)}\")\n            # call state_dict on all ranks in case it needs internal communication\n            state_dict = utils.move_to_cpu(self.state_dict())\n            state_dict[\"extra_state\"].update(extra_state)\n            checkpoint_utils.torch_persistent_save(\n                state_dict,\n                filename,\n                async_write=self.cfg.checkpoint.write_checkpoints_asynchronously,\n            )\n            logger.info(f\"Finished saving checkpoint to {os.path.abspath(filename)}\")\n            return os.path.abspath(filename)\n        return None\n\n    def load_checkpoint(\n        self,\n        filename,\n        reset_optimizer=False,\n        reset_lr_scheduler=False,\n        optimizer_overrides=None,\n        reset_meters=False,\n    ):\n        \"\"\"\n        Load all training state from a checkpoint file.\n        rank = 0 will load the checkpoint, and then broadcast it to all\n        other ranks.\n        \"\"\"\n        extra_state, self._optim_history, last_optim_state = None, [], None\n\n        logger.info(f\"Preparing to load checkpoint {filename}\")\n        is_distributed = self.data_parallel_world_size > 1\n        bexists = PathManager.isfile(filename)\n        if bexists:\n            load_on_all_ranks = (\n                self.cfg.checkpoint.load_checkpoint_on_all_dp_ranks\n                # TPUs don't support broadcast yet, so load checkpoints\n                # on every worker for now\n                or self.tpu\n                # FSDP requires loading checkpoint shards on all ranks\n                or (self.is_fsdp and self.cfg.distributed_training.use_sharded_state)\n                or getattr(self.cfg.model, \"base_layers\", 0) > 0\n            )\n\n            if load_on_all_ranks or self.data_parallel_rank == 0:\n                state = checkpoint_utils.load_checkpoint_to_cpu(\n                    filename, load_on_all_ranks=load_on_all_ranks\n                )\n                last_optim_state = state.get(\"last_optimizer_state\", None)\n\n                # If doing zero_sharding, do not broadcast global optimizer\n                # state. Later we will broadcast sharded states to each rank\n                # to avoid memory from exploding.\n                if (\n                    not load_on_all_ranks\n                    and self.cfg.distributed_training.zero_sharding == \"os\"\n                    and \"last_optimizer_state\" in state\n                    and is_distributed\n                ):\n                    state[\"last_optimizer_state\"] = \"SHARDED\"\n            else:\n                last_optim_state = None\n                state = None\n\n            if is_distributed and not load_on_all_ranks:\n                state = distributed_utils.broadcast_object(\n                    state,\n                    src_rank=0,\n                    group=self.data_parallel_process_group,\n                    dist_device=self.device,\n                )\n                if self.data_parallel_rank > 0:\n                    last_optim_state = state.get(\"last_optimizer_state\", None)\n\n            # load model parameters\n            try:\n                if (\n                    \"optimizer_history\" in state\n                    and len(state[\"optimizer_history\"]) > 0\n                    and \"num_updates\" in state[\"optimizer_history\"][-1]\n                ):\n                    self.model.set_num_updates(\n                        state[\"optimizer_history\"][-1][\"num_updates\"]\n                    )\n\n                # this is the code related to AdaPrune\n                # In short, it removes redundant heads in multi-head attention module based on heads importance provided\n                # For more info, please refer to the paper: https://openreview.net/forum?id=_CMSV7FTzGI\n                # The idea of prune in mha can be summarized as\n                # Fine tune model (e.g. roberta encoder) on a certain datasets with regularization\n                # After the model is trained. User could use get_reserve_head_index and _adaptive_prune_heads functions to get the top X heads with most importance.\n                # Then user uses the rank to prune a new roberta encoder and save the pruned ckpt manually.\n                # User will fine tune the the new roberta encoder via the ckpt saved above\n                # To get rid of registering different pruned version of Roberta, I use the argument --mha-heads-to-keep to prune the Roberta model into a pruned version which matches the pruned ckpt.\n                if (\n                    safe_hasattr(self.model, \"args\")\n                    and safe_hasattr(self.model.args, \"mha_heads_to_keep\")\n                    and self.model.args.mha_heads_to_keep != -1\n                ):\n                    logger.info(\n                        f\"Prune model: keep {self.model.args.mha_heads_to_keep} heads for each multihead attention module\"\n                    )\n                    for layer in self.model.encoder.sentence_encoder.layers:\n                        reserve_head_index = layer.self_attn._get_reserve_head_index(\n                            num_heads_to_keep=self.model.args.mha_heads_to_keep\n                        )\n                        layer.self_attn._adaptive_prune_heads(\n                            reserve_head_index=reserve_head_index\n                        )\n                        layer.self_attn._set_skip_embed_dim_check()\n                    logger.info(self.model)\n                # this is the code related to AdaPrune\n                # In short, it removes redundant units in feedforward layer in each transformer layer based on importance\n                # For more info, please refer to the paper: https://openreview.net/forum?id=_CMSV7FTzGI\n                # The idea of prune in ffn can be summarized as\n                # Fine tune model (e.g. roberta encoder) on a certain datasets with regularization\n                # After the model is trained. User could use _get_fc_rank and _prune_fc_layer functions to get the top X units with most importance.\n                # Then user uses the rank to prune a new roberta encoder and save the pruned ckpt manually.\n                # User will fine tune the the new roberta encoder via the ckpt saved above\n                # To get rid of registering different pruned version of Roberta, I use the argument --ffn-blocks-to-remove to prune the Roberta model into a pruned version which matches the pruned ckpt.\n                if (\n                    safe_hasattr(self.model, \"args\")\n                    and safe_hasattr(self.model.args, \"ffn_blocks_to_remove\")\n                    and self.model.args.ffn_blocks_to_remove != -1\n                ):\n                    logger.info(\n                        f\"Prune model: remove {self.model.args.ffn_blocks_to_remove} ffn blocks for each transformer layer\"\n                    )\n                    for layer in self.model.encoder.sentence_encoder.layers:\n                        remove_index = layer._get_fc_rank(\n                            remove_num=self.model.args.ffn_blocks_to_remove\n                        )\n                        layer._prune_fc_layer(remove_index=remove_index)\n                    logger.info(self.model)\n\n                if self.is_fsdp:\n                    self.model.upgrade_state_dict_named(state[\"model\"], '')\n                    self.model.load_state_dict(state[\"model\"], strict=True, model_cfg=self.cfg.model)\n                else:\n                    self.model.load_state_dict(\n                        state[\"model\"], strict=True, model_cfg=self.cfg.model\n                    )\n                # save memory for later steps\n                del state[\"model\"]\n                if utils.has_parameters(self.get_criterion()):\n                    self.get_criterion().load_state_dict(\n                        state[\"criterion\"], strict=True\n                    )\n                    del state[\"criterion\"]\n\n            except Exception:\n                raise Exception(\n                    \"Cannot load model parameters from checkpoint {}; \"\n                    \"please ensure that the architectures match.\".format(filename)\n                )\n            extra_state = state[\"extra_state\"]\n            self._optim_history = state[\"optimizer_history\"]\n\n        if last_optim_state is not None and not reset_optimizer:\n            # rebuild optimizer after loading model, since params may have changed\n            self._build_optimizer()\n\n            # only reload optimizer and lr_scheduler if they match\n            last_optim = self._optim_history[-1]\n            assert (\n                last_optim[\"criterion_name\"] == self.get_criterion().__class__.__name__\n            ), f\"Criterion does not match; please reset the optimizer (--reset-optimizer). {last_optim['criterion_name']} vs {self.get_criterion().__class__.__name__}\"\n            assert (\n                last_optim[\"optimizer_name\"] == self.optimizer.__class__.__name__\n            ), f\"Optimizer does not match; please reset the optimizer (--reset-optimizer). {last_optim['optimizer_name']} vs {self.optimizer.__class__.__name__}\"\n\n            if not reset_lr_scheduler:\n                self.lr_scheduler.load_state_dict(last_optim[\"lr_scheduler_state\"])\n\n            if self.is_fsdp and not self.model.use_sharded_state:\n                # if use_sharded_state, the last_optim_state is already sharded, skip this\n                last_optim_state = self.model.get_shard_from_optim_state_dict(\n                    last_optim_state\n                )\n            elif not load_on_all_ranks and is_distributed:\n                last_optim_state = self.optimizer.broadcast_global_state_dict(\n                    last_optim_state\n                )\n\n            self.optimizer.load_state_dict(last_optim_state, optimizer_overrides)\n\n            self.set_num_updates(last_optim[\"num_updates\"])\n\n        if extra_state is not None:\n            itr_state = extra_state[\"train_iterator\"]\n            epoch = itr_state[\"epoch\"]\n\n            if \"previous_training_time\" in extra_state:\n                self._previous_training_time = extra_state[\"previous_training_time\"]\n                self._start_time = time.time()\n\n            self.lr_step(epoch)\n\n            if (\n                itr_state.get(\"version\", 1) >= 2\n                and itr_state[\"iterations_in_epoch\"] == 0\n            ):\n                # reset meters at start of epoch\n                reset_meters = True\n\n            if \"metrics\" in extra_state and not reset_meters:\n                metrics.load_state_dict(extra_state[\"metrics\"])\n\n                # reset TimeMeters, since their start times don't make sense anymore\n                for meter in metrics.get_meters(\"default\"):\n                    if isinstance(meter, meters.TimeMeter):\n                        meter.reset()\n\n            if self.cfg.ema.store_ema:\n                if \"ema\" not in extra_state:\n                    logger.warn(\n                        \"EMA not found in checkpoint. But store_ema is True. \"\n                        \"EMA is re-initialized from checkpoint.\"\n                    )\n                    self.ema.restore(\n                        state[\"model\"], build_fp32_params=self.cfg.ema.ema_fp32\n                    )\n                else:\n                    logger.info(\"Loading EMA from checkpoint\")\n                    self.ema.restore(extra_state[\"ema\"], build_fp32_params=False)\n\n                    if self.cfg.ema.ema_fp32:\n                        if \"ema_fp32_params\" in extra_state:\n                            logger.info(\"Loading EMA fp32 params from checkpoint\")\n                            self.ema.build_fp32_params(extra_state[\"ema_fp32_params\"])\n                        else:\n                            logger.info(\n                                \"Building EMA fp32 params from EMA model in checkpoint\"\n                            )\n                            self.ema.build_fp32_params()\n\n            logger.info(\n                \"Loaded checkpoint {} (epoch {} @ {} updates)\".format(\n                    filename, epoch, self.get_num_updates()\n                )\n            )\n\n        else:\n            logger.info(\"No existing checkpoint found {}\".format(filename))\n\n        return extra_state\n\n    def get_train_iterator(\n        self,\n        epoch,\n        combine=True,\n        load_dataset=True,\n        data_selector=None,\n        shard_batch_itr=True,\n        disable_iterator_cache=False,\n    ):\n        \"\"\"Return an EpochBatchIterator over the training set for a given epoch.\"\"\"\n        if load_dataset:\n            logger.info(\"loading train data for epoch {}\".format(epoch))\n            self.task.load_dataset(\n                self.cfg.dataset.train_subset,\n                epoch=epoch,\n                combine=combine,\n                data_selector=data_selector,\n                tpu=self.tpu,\n            )\n        batch_iterator = self.task.get_batch_iterator(\n            dataset=self.task.dataset(self.cfg.dataset.train_subset),\n            max_tokens=self.cfg.dataset.max_tokens,\n            max_sentences=self.cfg.dataset.batch_size,\n            max_positions=utils.resolve_max_positions(\n                self.task.max_positions(),\n                self.model.max_positions(),\n                self.cfg.dataset.max_tokens,\n            ),\n            ignore_invalid_inputs=True,\n            required_batch_size_multiple=self.cfg.dataset.required_batch_size_multiple,\n            seed=(self.cfg.common.seed + epoch)\n            if self.cfg.dataset.update_ordered_indices_seed\n            else self.cfg.common.seed,\n            num_shards=self.data_parallel_world_size if shard_batch_itr else 1,\n            shard_id=self.data_parallel_rank if shard_batch_itr else 0,\n            num_workers=self.cfg.dataset.num_workers,\n            epoch=epoch,\n            data_buffer_size=self.cfg.dataset.data_buffer_size,\n            disable_iterator_cache=disable_iterator_cache,\n            skip_remainder_batch=self.cfg.optimization.skip_remainder_batch,\n            grouped_shuffling=self.cfg.dataset.grouped_shuffling,\n            update_epoch_batch_itr=self.cfg.dataset.update_epoch_batch_itr,\n        )\n        self.reset_dummy_batch(batch_iterator.first_batch)\n        return batch_iterator\n\n    def get_valid_iterator(\n        self,\n        subset,\n        disable_iterator_cache=False,\n    ):\n        \"\"\"Return an EpochBatchIterator over given validation subset for a given epoch.\"\"\"\n        batch_iterator = self.task.get_batch_iterator(\n            dataset=self.task.dataset(subset),\n            max_tokens=self.cfg.dataset.max_tokens_valid,\n            max_sentences=self.cfg.dataset.batch_size_valid,\n            max_positions=utils.resolve_max_positions(\n                self.task.max_positions(),\n                self.model.max_positions(),\n            ),\n            ignore_invalid_inputs=self.cfg.dataset.skip_invalid_size_inputs_valid_test,\n            required_batch_size_multiple=self.cfg.dataset.required_batch_size_multiple,\n            seed=self.cfg.common.seed,\n            num_shards=self.data_parallel_world_size,\n            shard_id=self.data_parallel_rank,\n            num_workers=self.cfg.dataset.num_workers,\n            # always pass a fixed \"epoch\" to keep validation data consistent\n            # across training epochs\n            epoch=1,\n            data_buffer_size=self.cfg.dataset.data_buffer_size,\n            disable_iterator_cache=disable_iterator_cache,\n            skip_remainder_batch=False,\n        )\n        self.reset_dummy_batch(batch_iterator.first_batch)\n        return batch_iterator\n\n    def begin_epoch(self, epoch):\n        \"\"\"Called at the beginning of each epoch.\"\"\"\n        logger.info(\"begin training epoch {}\".format(epoch))\n\n        self.lr_step_begin_epoch(epoch)\n\n        if self.quantizer is not None:\n            self.quantizer.begin_epoch(epoch)\n\n        # task specific setup per epoch\n        self.task.begin_epoch(epoch, self.get_model())\n\n        if self.tpu:\n            import torch_xla.core.xla_model as xm\n\n            xm.rendezvous(\"begin_epoch\")  # wait for all workers\n            xm.mark_step()\n\n    def begin_valid_epoch(self, epoch):\n        \"\"\"Called at the beginning of each validation epoch.\"\"\"\n\n        # task specific setup per validation epoch\n        self.task.begin_valid_epoch(epoch, self.get_model())\n\n    def reset_dummy_batch(self, batch):\n        self._dummy_batch = batch\n\n    @metrics.aggregate(\"train\")\n    def train_step(self, samples, raise_oom=False):\n        \"\"\"Do forward, backward and parameter update.\"\"\"\n        self._set_seed()\n        self.model.train()\n        self.criterion.train()\n        self.zero_grad()\n\n        metrics.log_start_time(\"train_wall\", priority=800, round=0)\n\n        # If EMA is enabled through store_ema=True\n        # and task.uses_ema is True, pass the EMA model as a keyword\n        # argument to the task.\n        extra_kwargs = {}\n        if self.cfg.ema.store_ema and getattr(self.task, \"uses_ema\", False):\n            extra_kwargs[\"ema_model\"] = self.ema.get_model()\n\n        has_oom = False\n\n        # forward and backward pass\n        logging_outputs, sample_size, ooms = [], 0, 0\n        for i, sample in enumerate(samples):  # delayed update loop\n            sample, is_dummy_batch = self._prepare_sample(sample)\n\n            def maybe_no_sync():\n                \"\"\"\n                Whenever *samples* contains more than one mini-batch, we\n                want to accumulate gradients locally and only call\n                all-reduce in the last backwards pass.\n                \"\"\"\n                if (\n                    self.data_parallel_world_size > 1\n                    and hasattr(self.model, \"no_sync\")\n                    and i < len(samples) - 1\n                    # The no_sync context manager results in increased memory\n                    # usage with FSDP, since full-size gradients will be\n                    # accumulated on each GPU. It's typically a better tradeoff\n                    # to do the extra communication with FSDP.\n                    and not self.is_fsdp\n                ):\n                    return self.model.no_sync()\n                else:\n                    return contextlib.ExitStack()  # dummy contextmanager\n\n            try:\n                with maybe_no_sync():\n                    # forward and backward\n                    loss, sample_size_i, logging_output = self.task.train_step(\n                        sample=sample,\n                        model=self.model,\n                        criterion=self.criterion,\n                        optimizer=self.optimizer,\n                        update_num=self.get_num_updates(),\n                        ignore_grad=is_dummy_batch,\n                        **extra_kwargs,\n                    )\n                    del loss\n\n                logging_outputs.append(logging_output)\n                sample_size += sample_size_i\n\n                # emptying the CUDA cache after the first step can\n                # reduce the chance of OOM\n                if self.cuda and self.get_num_updates() == 0:\n                    torch.cuda.empty_cache()\n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    self._log_oom(e)\n                    has_oom = True\n                    if raise_oom:\n                        raise e\n                else:\n                    raise e\n            except Exception:\n                self.consolidate_optimizer()\n                self.save_checkpoint(\n                    os.path.join(self.cfg.checkpoint.save_dir, \"crash.pt\"), {}\n                )\n                raise\n\n            if has_oom:\n                logger.warning(\n                    \"attempting to recover from OOM in forward/backward pass\"\n                )\n                ooms += 1\n                self.zero_grad()\n                if self.cuda:\n                    torch.cuda.empty_cache()\n\n                if self.cfg.distributed_training.distributed_world_size == 1:\n                    return None\n\n            if self.tpu and i < len(samples) - 1:\n                # tpu-comment: every XLA operation before marking step is\n                # appended to the IR graph, and processing too many batches\n                # before marking step can lead to OOM errors.\n                # To handle gradient accumulation use case, we explicitly\n                # mark step here for every forward pass without a backward pass\n                self._xla_markstep_and_send_to_cpu()\n\n        if is_dummy_batch:\n            if torch.is_tensor(sample_size):\n                sample_size.zero_()\n            else:\n                sample_size *= 0.0\n\n        if torch.is_tensor(sample_size):\n            sample_size = sample_size.float()\n        else:\n            sample_size = float(sample_size)\n\n        # gather logging outputs from all replicas\n        if self._sync_stats():\n            train_time = self._local_cumulative_training_time()\n            (\n                logging_outputs,\n                (\n                    sample_size,\n                    ooms,\n                    total_train_time,\n                ),\n            ) = self._aggregate_logging_outputs(\n                logging_outputs, sample_size, ooms, train_time, ignore=is_dummy_batch\n            )\n            self._cumulative_training_time = (\n                total_train_time / self.data_parallel_world_size\n            )\n\n        overflow = False\n        try:\n            with torch.autograd.profiler.record_function(\"reduce-grads\"):\n                # reduce gradients across workers\n                self.optimizer.all_reduce_grads(self.model)\n                if utils.has_parameters(self.criterion):\n                    self.optimizer.all_reduce_grads(self.criterion)\n\n            with torch.autograd.profiler.record_function(\"multiply-grads\"):\n                # multiply gradients by (data_parallel_size / sample_size) since\n                # DDP normalizes by the number of data parallel workers for\n                # improved fp16 precision.\n                # Thus we get (sum_of_gradients / sample_size) at the end.\n                # In case of fp16, this step also undoes loss scaling.\n                # (Debugging note: Some optimizers perform this scaling on the\n                # fly, so inspecting model.parameters() or optimizer.params may\n                # still show the original, unscaled gradients.)\n                numer = (\n                    self.data_parallel_world_size\n                    if not self.cfg.optimization.use_bmuf or self._sync_stats()\n                    else 1\n                )\n                self.optimizer.multiply_grads(numer / (sample_size or 1.0))\n                # Note: (sample_size or 1.0) handles the case of a zero gradient, in a\n                # way that avoids CPU/device transfers in case sample_size is a GPU or\n                # TPU object. The assumption is that the gradient itself is also 0.\n\n            with torch.autograd.profiler.record_function(\"clip-grads\"):\n                # clip grads\n                grad_norm = self.clip_grad_norm(self.cfg.optimization.clip_norm)\n\n            # check that grad norms are consistent across workers\n            # on tpu check tensor is slow\n            if not self.tpu:\n                if (\n                    not self.cfg.optimization.use_bmuf\n                    and self.cfg.distributed_training.ddp_backend != \"slowmo\"\n                ):\n                    self._check_grad_norms(grad_norm)\n                if not torch.isfinite(grad_norm).all():\n                    # in case of AMP, if gradients are Nan/Inf then\n                    # optimizer step is still required\n                    if self.cfg.common.amp:\n                        overflow = True\n                    else:\n                        # check local gradnorm single GPU case, trigger NanDetector\n                        raise FloatingPointError(\"gradients are Nan/Inf\")\n\n            with torch.autograd.profiler.record_function(\"optimizer\"):\n                # take an optimization step\n                self.task.optimizer_step(\n                    self.optimizer, model=self.model, update_num=self.get_num_updates()\n                )\n                if self.cfg.common.amp and overflow:\n                    if self._amp_retries == self.cfg.common.amp_batch_retries:\n                        logger.info(\"AMP: skipping this batch.\")\n                        self._amp_retries = 0\n                    else:\n                        self._amp_retries += 1\n                        return self.train_step(\n                            samples, raise_oom\n                        )  # recursion to feed in same batch\n\n        except FloatingPointError:\n\n            self.consolidate_optimizer()\n            self.save_checkpoint(\n                os.path.join(self.cfg.checkpoint.save_dir, \"crash.pt\"), {}\n            )\n\n            # re-run the forward and backward pass with hooks attached to print\n            # out where it fails\n            self.zero_grad()\n            with NanDetector(self.get_model()):\n                for _, sample in enumerate(samples):\n                    sample, _ = self._prepare_sample(sample)\n                    self.task.train_step(\n                        sample,\n                        self.model,\n                        self.criterion,\n                        self.optimizer,\n                        self.get_num_updates(),\n                        ignore_grad=False,\n                        **extra_kwargs,\n                    )\n            raise\n        except OverflowError as e:\n            overflow = True\n            logger.info(\n                f\"NOTE: gradient overflow detected, ignoring gradient, {str(e)}\"\n            )\n\n            if hasattr(self, \"param_names\") and hasattr(\n                self.optimizer, \"fp32_optimizer\"\n            ):\n                for p, n in zip(self.optimizer.fp32_optimizer.params, self.param_names):\n                    if torch.isinf(p.grad).any() or torch.isnan(p.grad).any():\n                        logger.info(f\"overflow in param {n}\")\n\n            grad_norm = torch.tensor(0.0).cuda()\n            self.zero_grad()\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                self._log_oom(e)\n                logger.error(\"OOM during optimization, irrecoverable\")\n            raise e\n\n        # Some distributed wrappers (e.g., SlowMo) need access to the optimizer\n        # after the step\n        if hasattr(self.model, \"perform_slowmo\"):\n            self.model.perform_slowmo(\n                self.optimizer.optimizer, getattr(self.optimizer, \"fp32_params\", None)\n            )\n\n        logging_output = None\n        if not overflow or self.cfg.distributed_training.ddp_backend == \"slowmo\":\n            self.set_num_updates(self.get_num_updates() + 1)\n\n            if self.cfg.ema.store_ema:\n                # Step EMA forward with new model.\n                self.ema.step(\n                    self.get_model(),\n                    self.get_num_updates(),\n                )\n                metrics.log_scalar(\n                    \"ema_decay\",\n                    self.ema.get_decay(),\n                    priority=10000,\n                    round=5,\n                    weight=0,\n                )\n\n            if self.tpu:\n                import torch_xla.core.xla_model as xm\n\n                # mark step on TPUs\n                self._xla_markstep_and_send_to_cpu()\n\n                # only log stats every log_interval steps\n                # this causes wps to be misreported when log_interval > 1\n                logging_output = {}\n                if self.get_num_updates() % self.cfg.common.log_interval == 0:\n                    # log memory usage\n                    mem_info = xm.get_memory_info(self.device)\n                    gb_free = mem_info[\"kb_free\"] / 1024 / 1024\n                    gb_total = mem_info[\"kb_total\"] / 1024 / 1024\n                    metrics.log_scalar(\n                        \"gb_free\", gb_free, priority=1500, round=1, weight=0\n                    )\n                    metrics.log_scalar(\n                        \"gb_total\", gb_total, priority=1600, round=1, weight=0\n                    )\n                    logging_outputs = self._xla_markstep_and_send_to_cpu(\n                        logging_outputs\n                    )\n                    logging_output = self._reduce_and_log_stats(\n                        logging_outputs, sample_size, grad_norm\n                    )\n\n                # log whenever there's an XLA compilation, since these\n                # slow down training and may indicate opportunities for\n                # optimization\n                self._check_xla_compilation()\n            else:\n                if self.cuda and self.cuda_env is not None:\n                    # log minimum free memory over the iteration\n                    gb_used = torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024\n                    torch.cuda.reset_peak_memory_stats()\n                    gb_free = self.cuda_env.total_memory_in_GB - gb_used\n                    metrics.log_scalar(\n                        \"gb_free\", gb_free, priority=1500, round=1, weight=0\n                    )\n\n                # log stats\n                logging_output = self._reduce_and_log_stats(\n                    logging_outputs, sample_size, grad_norm\n                )\n\n                # clear CUDA cache to reduce memory fragmentation\n                if (\n                    self.cuda\n                    and self.cfg.common.empty_cache_freq > 0\n                    and (\n                        (self.get_num_updates() + self.cfg.common.empty_cache_freq - 1)\n                        % self.cfg.common.empty_cache_freq\n                    )\n                    == 0\n                ):\n                    torch.cuda.empty_cache()\n\n        if self.cfg.common.fp16 or self.cfg.common.amp:\n            metrics.log_scalar(\n                \"loss_scale\",\n                (\n                    self.optimizer.scaler.loss_scale\n                    if self.cfg.common.fp16\n                    else self.optimizer.scaler.get_scale()\n                ),\n                priority=700,\n                round=4,\n                weight=0,\n            )\n\n        metrics.log_stop_time(\"train_wall\")\n        return logging_output\n\n    @metrics.aggregate(\"valid\")\n    def valid_step(self, sample, raise_oom=False):\n        \"\"\"Do forward pass in evaluation mode.\"\"\"\n        if self.tpu:\n            import torch_xla.core.xla_model as xm\n\n            xm.rendezvous(\"valid_step\")  # wait for all workers\n\n        # If EMA is enabled through store_ema=True\n        # and task.uses_ema is True, pass the EMA model as a keyword\n        # argument to the task.\n        extra_kwargs = {}\n        if self.cfg.ema.store_ema and getattr(self.task, \"uses_ema\", False):\n            extra_kwargs[\"ema_model\"] = self.ema.get_model()\n\n        with torch.no_grad():\n            self.model.eval()\n            self.criterion.eval()\n\n            sample, is_dummy_batch = self._prepare_sample(sample)\n\n            try:\n                _loss, sample_size, logging_output = self.task.valid_step(\n                    sample, self.model, self.criterion, **extra_kwargs\n                )\n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    self._log_oom(e)\n                    if not raise_oom:\n                        logger.warning(\n                            \"ran out of memory in validation step, retrying batch\"\n                        )\n                        for p in self.model.parameters():\n                            if p.grad is not None:\n                                p.grad = None  # free some memory\n                        if self.cuda:\n                            torch.cuda.empty_cache()\n                        return self.valid_step(sample, raise_oom=True)\n                raise e\n\n            logging_outputs = [logging_output]\n            if is_dummy_batch:\n                if torch.is_tensor(sample_size):\n                    sample_size.zero_()\n                else:\n                    sample_size *= 0.0\n\n        # gather logging outputs from all replicas\n        if self.data_parallel_world_size > 1:\n            logging_outputs, (sample_size,) = self._aggregate_logging_outputs(\n                logging_outputs,\n                sample_size,\n                ignore=is_dummy_batch,\n            )\n\n        # log validation stats\n        if self.tpu:\n            logging_outputs = self._xla_markstep_and_send_to_cpu(logging_outputs)\n        logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\n\n        return logging_output\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def lr_step_begin_epoch(self, epoch):\n        \"\"\"Adjust the learning rate at the beginning of the epoch.\"\"\"\n        self.lr_scheduler.step_begin_epoch(epoch)\n        # prefer updating the LR based on the number of steps\n        return self.lr_step_update()\n\n    def lr_step(self, epoch, val_loss=None):\n        \"\"\"Adjust the learning rate at the end of the epoch.\"\"\"\n        self.lr_scheduler.step(epoch, val_loss)\n        # prefer updating the LR based on the number of steps\n        return self.lr_step_update()\n\n    def lr_step_update(self):\n        \"\"\"Update the learning rate after each update.\"\"\"\n        new_lr = self.lr_scheduler.step_update(self.get_num_updates())\n        if isinstance(new_lr, dict):\n            for k, v in new_lr.items():\n                metrics.log_scalar(f\"lr_{k}\", v, weight=0, priority=300)\n            new_lr = new_lr.get(\"default\", next(iter(new_lr.values())))\n        else:\n            metrics.log_scalar(\"lr\", new_lr, weight=0, priority=300)\n        return new_lr\n\n    def get_lr(self):\n        \"\"\"Get the current learning rate.\"\"\"\n        return self.optimizer.get_lr()\n\n    def get_model(self):\n        \"\"\"Get the (non-wrapped) model instance.\"\"\"\n        return self._model\n\n    def get_criterion(self):\n        \"\"\"Get the (non-wrapped) criterion instance.\"\"\"\n        return self._criterion\n\n    def get_meter(self, name):\n        \"\"\"[deprecated] Get a specific meter by name.\"\"\"\n        from fairseq import meters\n\n        if \"get_meter\" not in self._warn_once:\n            self._warn_once.add(\"get_meter\")\n            utils.deprecation_warning(\n                \"Trainer.get_meter is deprecated. Please use fairseq.metrics instead.\"\n            )\n\n        train_meters = metrics.get_meters(\"train\")\n        if train_meters is None:\n            train_meters = {}\n\n        if name == \"train_loss\" and \"loss\" in train_meters:\n            return train_meters[\"loss\"]\n        elif name == \"train_nll_loss\":\n            # support for legacy train.py, which assumed this meter is\n            # always initialized\n            m = train_meters.get(\"nll_loss\", None)\n            return m or meters.AverageMeter()\n        elif name == \"wall\":\n            # support for legacy train.py, which assumed this meter is\n            # always initialized\n            m = metrics.get_meter(\"default\", \"wall\")\n            return m or meters.TimeMeter()\n        elif name == \"wps\":\n            m = metrics.get_meter(\"train\", \"wps\")\n            return m or meters.TimeMeter()\n        elif name in {\"valid_loss\", \"valid_nll_loss\"}:\n            # support for legacy train.py, which assumed these meters\n            # are always initialized\n            k = name[len(\"valid_\") :]\n            m = metrics.get_meter(\"valid\", k)\n            return m or meters.AverageMeter()\n        elif name == \"oom\":\n            return meters.AverageMeter()\n        elif name in train_meters:\n            return train_meters[name]\n        return None\n\n    def get_num_updates(self):\n        \"\"\"Get the number of parameters updates.\"\"\"\n        return self._num_updates\n\n    def set_num_updates(self, num_updates):\n        \"\"\"Set the number of parameters updates.\"\"\"\n        self._num_updates = num_updates\n        self.lr_step_update()\n        if self.quantizer:\n            self.quantizer.step_update(self._num_updates)\n        metrics.log_scalar(\"num_updates\", self._num_updates, weight=0, priority=200)\n\n    def clip_grad_norm(self, clip_norm):\n        def agg_norm_fn(total_norm):\n            total_norm = total_norm.cuda().float() ** 2\n            total_norm = distributed_utils.all_reduce(\n                total_norm, group=self.data_parallel_process_group\n            )\n            return total_norm**0.5\n\n        should_agg_norm = self.is_fsdp and (\n            self.data_parallel_process_group is not None\n            or torch.distributed.is_initialized()\n        )\n        return self.optimizer.clip_grad_norm(\n            clip_norm, aggregate_norm_fn=agg_norm_fn if should_agg_norm else None\n        )\n\n    def cumulative_training_time(self):\n        if self._cumulative_training_time is None:\n            # single GPU\n            return self._local_cumulative_training_time()\n        else:\n            return self._cumulative_training_time\n\n    def _local_cumulative_training_time(self):\n        \"\"\"Aggregate training time in seconds.\"\"\"\n        return time.time() - self._start_time + self._previous_training_time\n\n    def _fp_convert_sample(self, sample):\n        def apply_half(t):\n            if t.dtype is torch.float32:\n                return t.to(dtype=torch.half)\n            return t\n\n        def apply_bfloat16(t):\n            if t.dtype is torch.float32:\n                return t.to(dtype=torch.bfloat16)\n            return t\n\n        if self.cfg.common.fp16:\n            sample = utils.apply_to_sample(apply_half, sample)\n\n        if self.cfg.common.bf16:\n            sample = utils.apply_to_sample(apply_bfloat16, sample)\n\n        return sample\n\n    def _prepare_sample(self, sample, is_dummy=False):\n        if sample == \"DUMMY\":\n            raise Exception(\n                \"Trying to use an uninitialized 'dummy' batch. This usually indicates \"\n                \"that the total number of batches is smaller than the number of \"\n                \"participating GPUs. Try reducing the batch size or using fewer GPUs.\"\n            )\n\n        if sample is None or len(sample) == 0:\n            assert (\n                self._dummy_batch is not None and len(self._dummy_batch) > 0\n            ), \"Invalid dummy batch: {}\".format(self._dummy_batch)\n            sample, _ = self._prepare_sample(self._dummy_batch, is_dummy=True)\n            return sample, True\n\n        # Given that PCIe/NVLink bandwidth is significantly smaller than DRAM bandwidth\n        # it makes sense to do the format conversion on the CPU and then transfer\n        # a smaller buffer to the device. This also saves GPU memory capacity.\n\n        if self.cfg.common.on_cpu_convert_precision:\n            sample = self._fp_convert_sample(sample)\n\n        if self.cuda:\n            if self.pipeline_model_parallel:\n                if \"target\" in sample:\n                    sample[\"target\"] = utils.move_to_cuda(\n                        sample[\"target\"], device=self.last_device\n                    )\n            else:\n                sample = utils.move_to_cuda(sample)\n        elif self.tpu and is_dummy:\n            # the dummy batch may not be on the appropriate device\n            sample = utils.move_to_cuda(sample, device=self.device)\n\n        if not self.cfg.common.on_cpu_convert_precision:\n            sample = self._fp_convert_sample(sample)\n\n        if self._dummy_batch == \"DUMMY\":\n            self._dummy_batch = sample\n\n        return sample, False\n\n    def _set_seed(self):\n        # Set seed based on args.seed and the update number so that we get\n        # reproducible results when resuming from checkpoints\n        seed = self.cfg.common.seed + self.get_num_updates()\n        utils.set_torch_seed(seed)\n\n    def _sync_stats(self):\n        # Return True if it's using multiple GPUs and DDP or multiple GPUs with\n        # BMUF and it's a bmuf sync with warmup iterations completed before.\n        if self.data_parallel_world_size == 1:\n            return False\n        elif self.cfg.optimization.use_bmuf:\n            return (\n                self.get_num_updates() + 1\n            ) % self.cfg.bmuf.global_sync_iter == 0 and (\n                self.get_num_updates() + 1\n            ) > self.cfg.bmuf.warmup_iterations\n        else:\n            return True\n\n    def _log_oom(self, exc):\n        msg = \"OOM: Ran out of memory with exception: {}\".format(exc)\n        logger.warning(msg)\n        if torch.cuda.is_available() and hasattr(torch.cuda, \"memory_summary\"):\n            for device_idx in range(torch.cuda.device_count()):\n                logger.warning(torch.cuda.memory_summary(device=device_idx))\n        sys.stderr.flush()\n\n    def _aggregate_logging_outputs(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        if self.task.__class__.logging_outputs_can_be_summed(self.get_criterion()):\n            return self._fast_stat_sync_sum(\n                logging_outputs, *extra_stats_to_sum, ignore=ignore\n            )\n        else:\n            return self._all_gather_list_sync(\n                logging_outputs, *extra_stats_to_sum, ignore=ignore\n            )\n\n    def _all_gather_list_sync(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        \"\"\"\n        Sync logging outputs across workers. all_gather_list_sync is\n        suitable when logging outputs are complex types.\n        \"\"\"\n        if self.tpu:\n            raise NotImplementedError\n        if ignore:\n            logging_outputs = []\n        results = list(\n            zip(\n                *distributed_utils.all_gather_list(\n                    [logging_outputs] + list(extra_stats_to_sum),\n                    max_size=getattr(self.cfg.common, \"all_gather_list_size\", 16384),\n                    group=self.data_parallel_process_group,\n                )\n            )\n        )\n        logging_outputs, extra_stats_to_sum = results[0], results[1:]\n        logging_outputs = list(chain.from_iterable(logging_outputs))\n        extra_stats_to_sum = [sum(s) for s in extra_stats_to_sum]\n        return logging_outputs, extra_stats_to_sum\n\n    def _fast_stat_sync_sum(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        \"\"\"\n        Sync logging outputs across workers. fast_stat_sync_sum is\n        faster than all_gather_list_sync, but is only suitable when\n        logging outputs are scalars and can be summed. Note that\n        *logging_outputs* cannot contain any nested dicts/lists.\n        \"\"\"\n        data = {}\n        for i, stat in enumerate(extra_stats_to_sum):\n            data[\"extra_stats_\" + str(i)] = stat\n        if len(logging_outputs) > 0:\n            log_keys = list(logging_outputs[0].keys())\n            for k in log_keys:\n                if not ignore:\n                    v = sum(log[k] for log in logging_outputs if k in log)\n                else:\n                    v = logging_outputs[0][k]\n                    v = torch.zeros_like(v) if torch.is_tensor(v) else 0\n                data[\"logging_outputs_\" + k] = v\n        else:\n            log_keys = None\n\n        data = distributed_utils.all_reduce_dict(\n            data, device=self.device, group=self.data_parallel_process_group\n        )\n\n        extra_stats_to_sum = [\n            data[\"extra_stats_\" + str(i)] for i in range(len(extra_stats_to_sum))\n        ]\n        if log_keys is not None:\n            logging_outputs = [{k: data[\"logging_outputs_\" + k] for k in log_keys}]\n        else:\n            logging_outputs = []\n        return logging_outputs, extra_stats_to_sum\n\n    def _check_grad_norms(self, grad_norm):\n        \"\"\"Check that grad norms are consistent across workers.\"\"\"\n        if self._grad_norm_buf is not None:\n            self._grad_norm_buf.zero_()\n            self._grad_norm_buf[self.data_parallel_rank] = grad_norm\n            distributed_utils.all_reduce(\n                self._grad_norm_buf, group=self.data_parallel_process_group\n            )\n\n            def is_consistent(tensor):\n                max_abs_diff = torch.max(torch.abs(tensor - tensor[0]))\n                return (\n                    (\n                        torch.isfinite(tensor).all()\n                        and (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()\n                    )\n                    or (self.cfg.common.amp and not torch.isfinite(tensor).all())\n                    # in case of amp non-finite grads are fine\n                )\n\n            if not is_consistent(self._grad_norm_buf):\n                pretty_detail = \"\\n\".join(\n                    \"rank {:3d} = {:.8f}\".format(r, n)\n                    for r, n in enumerate(self._grad_norm_buf.tolist())\n                )\n                error_detail = \"grad_norm across the workers:\\n{}\\n\".format(\n                    pretty_detail\n                )\n                # use FloatingPointError to trigger NanDetector\n                raise FloatingPointError(\n                    \"Fatal error: gradients are inconsistent between workers. \"\n                    \"Try --ddp-backend=legacy_ddp. \"\n                    \"Or are you mixing up different generation of GPUs in training?\"\n                    + \"\\n\"\n                    + \"-\" * 80\n                    + \"\\n{}\\n\".format(error_detail)\n                    + \"-\" * 80\n                )\n\n    def _reduce_and_log_stats(self, logging_outputs, sample_size, grad_norm=None):\n        if grad_norm is not None and (\n            not torch.is_tensor(grad_norm) or torch.isfinite(grad_norm)\n        ):\n            metrics.log_speed(\"ups\", 1.0, priority=100, round=2)\n            metrics.log_scalar(\"gnorm\", grad_norm, priority=400, round=3)\n            if self.cfg.optimization.clip_norm > 0:\n                metrics.log_scalar(\n                    \"clip\",\n                    torch.where(\n                        grad_norm > self.cfg.optimization.clip_norm,\n                        grad_norm.new_tensor(100),\n                        grad_norm.new_tensor(0),\n                    ),\n                    priority=500,\n                    round=1,\n                )\n\n        with metrics.aggregate() as agg:\n            if logging_outputs is not None:\n                self.task.reduce_metrics(logging_outputs, self.get_criterion())\n                del logging_outputs\n\n            # extra warning for criterions that don't properly log a loss value\n            if \"loss\" not in agg:\n                if \"loss\" not in self._warn_once:\n                    self._warn_once.add(\"loss\")\n                    logger.warning(\n                        \"Criterion.reduce_metrics did not log a 'loss' value, \"\n                        \"which may break some functionality\"\n                    )\n                metrics.log_scalar(\"loss\", -1)\n\n            # support legacy interface\n            if self.tpu:\n                logging_output = {}\n            else:\n                logging_output = agg.get_smoothed_values()\n                logging_output[\"sample_size\"] = sample_size\n                for key_to_delete in [\"ppl\", \"wps\", \"wpb\", \"bsz\"]:\n                    if key_to_delete in logging_output:\n                        del logging_output[key_to_delete]\n            return logging_output\n\n    def _check_xla_compilation(self):\n        import torch_xla.debug.metrics as met\n\n        compile_stats = met.metric_data(\"CompileTime\")\n        if compile_stats is None:\n            return\n        num_xla_compiles = compile_stats[0]\n        if num_xla_compiles > self._num_xla_compiles:\n            logger.warning(\n                \"XLA compilation detected on device #{}; too many of these can lead \"\n                \"to slow training, but we expect a few in the beginning\".format(\n                    self.cfg.distributed_training.distributed_rank\n                )\n            )\n        self._num_xla_compiles = num_xla_compiles\n\n    def _xla_markstep_and_send_to_cpu(self, data=None):\n        import torch_xla.core.xla_model as xm\n\n        xm.mark_step()\n        if data is not None:\n            from fairseq.utils import xla_device_to_cpu\n\n            return xla_device_to_cpu(data)", "\n\ndef _catalog_shared_params(module, memo=None, prefix=\"\"):\n    if memo is None:\n        first_call = True\n        memo = {}\n    else:\n        first_call = False\n    for name, param in module._parameters.items():\n        param_prefix = prefix + (\".\" if prefix else \"\") + name\n        if param not in memo:\n            memo[param] = []\n        memo[param].append(param_prefix)\n    for name, m in module._modules.items():\n        if m is None:\n            continue\n        submodule_prefix = prefix + (\".\" if prefix else \"\") + name\n        _catalog_shared_params(m, memo, submodule_prefix)\n    if first_call:\n        return [x for x in memo.values() if len(x) > 1]", "\n\ndef _get_module_by_path(module, path):\n    path = path.split(\".\")\n    for name in path:\n        module = getattr(module, name)\n    return module\n\n\ndef _set_module_by_path(module, path, value):\n    path = path.split(\".\")\n    for name in path[:-1]:\n        module = getattr(module, name)\n    setattr(module, path[-1], value)", "\ndef _set_module_by_path(module, path, value):\n    path = path.split(\".\")\n    for name in path[:-1]:\n        module = getattr(module, name)\n    setattr(module, path[-1], value)\n"]}
{"filename": "xlmr/src/task/seq2seq_dataset.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport numpy as np\nimport torch\nfrom fairseq.data import FairseqDataset, data_utils", "import torch\nfrom fairseq.data import FairseqDataset, data_utils\nfrom fairseq.utils import new_arange\nimport math\nfrom fairseq.utils import new_arange\n\nlogger = logging.getLogger(__name__)\n\ndef collate(\n    samples,\n    pad_idx,\n    eos_idx,\n    left_pad_source=True,\n    left_pad_target=False,\n    input_feeding=True,\n    pad_to_length=None,\n    pad_to_multiple=1,\n):\n    if len(samples) == 0:\n        return {}\n\n    def merge(key, left_pad, move_eos_to_beginning=False, pad_to_length=None):\n        return data_utils.collate_tokens(\n            [s[key] for s in samples],\n            pad_idx,\n            eos_idx,\n            left_pad,\n            move_eos_to_beginning,\n            pad_to_length=pad_to_length,\n            pad_to_multiple=pad_to_multiple,\n        )\n\n    def check_alignment(alignment, src_len, tgt_len):\n        if alignment is None or len(alignment) == 0:\n            return False\n        if (\n            alignment[:, 0].max().item() >= src_len - 1\n            or alignment[:, 1].max().item() >= tgt_len - 1\n        ):\n            logger.warning(\"alignment size mismatch found, skipping alignment!\")\n            return False\n        return True\n\n    def compute_alignment_weights(alignments):\n        \"\"\"\n        Given a tensor of shape [:, 2] containing the source-target indices\n        corresponding to the alignments, a weight vector containing the\n        inverse frequency of each target index is computed.\n        For e.g. if alignments = [[5, 7], [2, 3], [1, 3], [4, 2]], then\n        a tensor containing [1., 0.5, 0.5, 1] should be returned (since target\n        index 3 is repeated twice)\n        \"\"\"\n        align_tgt = alignments[:, 1]\n        _, align_tgt_i, align_tgt_c = torch.unique(\n            align_tgt, return_inverse=True, return_counts=True\n        )\n        align_weights = align_tgt_c[align_tgt_i[np.arange(len(align_tgt))]]\n        return 1.0 / align_weights.float()\n\n    id = torch.LongTensor([s[\"id\"] for s in samples])\n    src_tokens = merge(\n        \"source\",\n        left_pad=left_pad_source,\n        pad_to_length=pad_to_length[\"source\"] if pad_to_length is not None else None,\n    )\n    # sort by descending source length\n    src_lengths = torch.LongTensor(\n        [s[\"source\"].ne(pad_idx).long().sum() for s in samples]\n    )\n    src_lengths, sort_order = src_lengths.sort(descending=True)\n    id = id.index_select(0, sort_order)\n    src_tokens = src_tokens.index_select(0, sort_order)\n    \n    def merge_data(data_name, sort_order):\n        prepared_data = merge(\n            data_name,\n            left_pad=left_pad_target,\n            pad_to_length=pad_to_length[data_name]\n            if pad_to_length is not None\n            else None,\n        )\n        return prepared_data.index_select(0, sort_order)\n    \n    if samples[0].get(\"target\", None) is not None:\n        target = merge(\n            \"target\",\n            left_pad=left_pad_target,\n            pad_to_length=pad_to_length[\"target\"]\n            if pad_to_length is not None\n            else None,\n        )\n        target = target.index_select(0, sort_order)\n        tgt_lengths = torch.LongTensor(\n            [s[\"target\"].ne(pad_idx).long().sum() for s in samples]\n        ).index_select(0, sort_order)\n        ntokens = tgt_lengths.sum().item()\n    else:\n        target = None\n        ntokens = src_lengths.sum().item()\n\n    batch = {\n        \"id\": id,\n        \"nsentences\": len(samples),\n        \"ntokens\": ntokens,\n        \"net_input\": {\n            \"src_tokens\": src_tokens,\n            \"src_lengths\": src_lengths,\n        },\n        \"target\": target,\n    }\n\n    if samples[0].get(\"alignment\", None) is not None:\n        bsz, tgt_sz = batch[\"target\"].shape\n        src_sz = batch[\"net_input\"][\"src_tokens\"].shape[1]\n\n        offsets = torch.zeros((len(sort_order), 2), dtype=torch.long)\n        offsets[:, 1] += torch.arange(len(sort_order), dtype=torch.long) * tgt_sz\n        if left_pad_source:\n            offsets[:, 0] += src_sz - src_lengths\n        if left_pad_target:\n            offsets[:, 1] += tgt_sz - tgt_lengths\n\n        alignments = [\n            alignment + offset\n            for align_idx, offset, src_len, tgt_len in zip(\n                sort_order, offsets, src_lengths, tgt_lengths\n            )\n            for alignment in [samples[align_idx][\"alignment\"].view(-1, 2)]\n            if check_alignment(alignment, src_len, tgt_len)\n        ]\n\n        if len(alignments) > 0:\n            alignments = torch.cat(alignments, dim=0)\n            align_weights = compute_alignment_weights(alignments)\n\n            batch[\"alignments\"] = alignments\n            batch[\"align_weights\"] = align_weights\n\n    if samples[0].get(\"constraints\", None) is not None:\n        # Collate the packed constraints across the samples, padding to\n        # the length of the longest sample.\n        lens = [sample.get(\"constraints\").size(0) for sample in samples]\n        max_len = max(lens)\n        constraints = torch.zeros((len(samples), max(lens))).long()\n        for i, sample in enumerate(samples):\n            constraints[i, 0 : lens[i]] = samples[i].get(\"constraints\")\n        batch[\"constraints\"] = constraints\n\n    return batch", "def collate(\n    samples,\n    pad_idx,\n    eos_idx,\n    left_pad_source=True,\n    left_pad_target=False,\n    input_feeding=True,\n    pad_to_length=None,\n    pad_to_multiple=1,\n):\n    if len(samples) == 0:\n        return {}\n\n    def merge(key, left_pad, move_eos_to_beginning=False, pad_to_length=None):\n        return data_utils.collate_tokens(\n            [s[key] for s in samples],\n            pad_idx,\n            eos_idx,\n            left_pad,\n            move_eos_to_beginning,\n            pad_to_length=pad_to_length,\n            pad_to_multiple=pad_to_multiple,\n        )\n\n    def check_alignment(alignment, src_len, tgt_len):\n        if alignment is None or len(alignment) == 0:\n            return False\n        if (\n            alignment[:, 0].max().item() >= src_len - 1\n            or alignment[:, 1].max().item() >= tgt_len - 1\n        ):\n            logger.warning(\"alignment size mismatch found, skipping alignment!\")\n            return False\n        return True\n\n    def compute_alignment_weights(alignments):\n        \"\"\"\n        Given a tensor of shape [:, 2] containing the source-target indices\n        corresponding to the alignments, a weight vector containing the\n        inverse frequency of each target index is computed.\n        For e.g. if alignments = [[5, 7], [2, 3], [1, 3], [4, 2]], then\n        a tensor containing [1., 0.5, 0.5, 1] should be returned (since target\n        index 3 is repeated twice)\n        \"\"\"\n        align_tgt = alignments[:, 1]\n        _, align_tgt_i, align_tgt_c = torch.unique(\n            align_tgt, return_inverse=True, return_counts=True\n        )\n        align_weights = align_tgt_c[align_tgt_i[np.arange(len(align_tgt))]]\n        return 1.0 / align_weights.float()\n\n    id = torch.LongTensor([s[\"id\"] for s in samples])\n    src_tokens = merge(\n        \"source\",\n        left_pad=left_pad_source,\n        pad_to_length=pad_to_length[\"source\"] if pad_to_length is not None else None,\n    )\n    # sort by descending source length\n    src_lengths = torch.LongTensor(\n        [s[\"source\"].ne(pad_idx).long().sum() for s in samples]\n    )\n    src_lengths, sort_order = src_lengths.sort(descending=True)\n    id = id.index_select(0, sort_order)\n    src_tokens = src_tokens.index_select(0, sort_order)\n    \n    def merge_data(data_name, sort_order):\n        prepared_data = merge(\n            data_name,\n            left_pad=left_pad_target,\n            pad_to_length=pad_to_length[data_name]\n            if pad_to_length is not None\n            else None,\n        )\n        return prepared_data.index_select(0, sort_order)\n    \n    if samples[0].get(\"target\", None) is not None:\n        target = merge(\n            \"target\",\n            left_pad=left_pad_target,\n            pad_to_length=pad_to_length[\"target\"]\n            if pad_to_length is not None\n            else None,\n        )\n        target = target.index_select(0, sort_order)\n        tgt_lengths = torch.LongTensor(\n            [s[\"target\"].ne(pad_idx).long().sum() for s in samples]\n        ).index_select(0, sort_order)\n        ntokens = tgt_lengths.sum().item()\n    else:\n        target = None\n        ntokens = src_lengths.sum().item()\n\n    batch = {\n        \"id\": id,\n        \"nsentences\": len(samples),\n        \"ntokens\": ntokens,\n        \"net_input\": {\n            \"src_tokens\": src_tokens,\n            \"src_lengths\": src_lengths,\n        },\n        \"target\": target,\n    }\n\n    if samples[0].get(\"alignment\", None) is not None:\n        bsz, tgt_sz = batch[\"target\"].shape\n        src_sz = batch[\"net_input\"][\"src_tokens\"].shape[1]\n\n        offsets = torch.zeros((len(sort_order), 2), dtype=torch.long)\n        offsets[:, 1] += torch.arange(len(sort_order), dtype=torch.long) * tgt_sz\n        if left_pad_source:\n            offsets[:, 0] += src_sz - src_lengths\n        if left_pad_target:\n            offsets[:, 1] += tgt_sz - tgt_lengths\n\n        alignments = [\n            alignment + offset\n            for align_idx, offset, src_len, tgt_len in zip(\n                sort_order, offsets, src_lengths, tgt_lengths\n            )\n            for alignment in [samples[align_idx][\"alignment\"].view(-1, 2)]\n            if check_alignment(alignment, src_len, tgt_len)\n        ]\n\n        if len(alignments) > 0:\n            alignments = torch.cat(alignments, dim=0)\n            align_weights = compute_alignment_weights(alignments)\n\n            batch[\"alignments\"] = alignments\n            batch[\"align_weights\"] = align_weights\n\n    if samples[0].get(\"constraints\", None) is not None:\n        # Collate the packed constraints across the samples, padding to\n        # the length of the longest sample.\n        lens = [sample.get(\"constraints\").size(0) for sample in samples]\n        max_len = max(lens)\n        constraints = torch.zeros((len(samples), max(lens))).long()\n        for i, sample in enumerate(samples):\n            constraints[i, 0 : lens[i]] = samples[i].get(\"constraints\")\n        batch[\"constraints\"] = constraints\n\n    return batch", "\n\nclass LanguagePairDataset(FairseqDataset):\n    \n    def __init__(\n        self,\n        src,\n        src_sizes,\n        src_dict,\n        tgt=None,\n        tgt_sizes=None,\n        tgt_dict=None,\n        left_pad_source=True,\n        left_pad_target=False,\n        shuffle=True,\n        input_feeding=True,\n        remove_eos_from_source=False,\n        append_eos_to_target=False,\n        align_dataset=None,\n        constraints=None,\n        append_bos=False,\n        eos=None,\n        num_buckets=0,\n        src_lang_id=None,\n        tgt_lang_id=None,\n        pad_to_multiple=1,\n    ):\n        if tgt_dict is not None:\n            assert src_dict.pad() == tgt_dict.pad()\n            assert src_dict.eos() == tgt_dict.eos()\n            assert src_dict.unk() == tgt_dict.unk()\n        if tgt is not None:\n            assert len(src) == len(\n                tgt\n            ), \"Source and target must contain the same number of examples\"\n        self.src = src\n        self.tgt = tgt\n        self.src_sizes = np.array(src_sizes)\n        self.tgt_sizes = np.array(tgt_sizes) if tgt_sizes is not None else None\n        self.sizes = (\n            np.vstack((self.src_sizes, self.tgt_sizes)).T\n            if self.tgt_sizes is not None\n            else self.src_sizes\n        )\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n        self.left_pad_source = left_pad_source\n        self.left_pad_target = left_pad_target\n        self.shuffle = shuffle\n        self.input_feeding = input_feeding\n        self.remove_eos_from_source = remove_eos_from_source\n        self.append_eos_to_target = append_eos_to_target\n        self.align_dataset = align_dataset\n        if self.align_dataset is not None:\n            assert (\n                self.tgt_sizes is not None\n            ), \"Both source and target needed when alignments are provided\"\n        self.constraints = constraints\n        self.append_bos = append_bos\n        self.eos = eos if eos is not None else src_dict.eos()\n        self.src_lang_id = src_lang_id\n        self.tgt_lang_id = tgt_lang_id\n        if num_buckets > 0:\n            from fairseq.data import BucketPadLengthDataset\n\n            self.src = BucketPadLengthDataset(\n                self.src,\n                sizes=self.src_sizes,\n                num_buckets=num_buckets,\n                pad_idx=self.src_dict.pad(),\n                left_pad=self.left_pad_source,\n            )\n            self.src_sizes = self.src.sizes\n            logger.info(\"bucketing source lengths: {}\".format(list(self.src.buckets)))\n            if self.tgt is not None:\n                self.tgt = BucketPadLengthDataset(\n                    self.tgt,\n                    sizes=self.tgt_sizes,\n                    num_buckets=num_buckets,\n                    pad_idx=self.tgt_dict.pad(),\n                    left_pad=self.left_pad_target,\n                )\n                self.tgt_sizes = self.tgt.sizes\n                logger.info(\n                    \"bucketing target lengths: {}\".format(list(self.tgt.buckets))\n                )\n\n            # determine bucket sizes using self.num_tokens, which will return\n            # the padded lengths (thanks to BucketPadLengthDataset)\n            num_tokens = np.vectorize(self.num_tokens, otypes=[np.long])\n            self.bucketed_num_tokens = num_tokens(np.arange(len(self.src)))\n            self.buckets = [\n                (None, num_tokens) for num_tokens in np.unique(self.bucketed_num_tokens)\n            ]\n        else:\n            self.buckets = None\n        self.pad_to_multiple = pad_to_multiple\n            \n    def get_batch_shapes(self):\n        return self.buckets\n\n    def __getitem__(self, index):\n        tgt_item = self.tgt[index] if self.tgt is not None else None\n        src_item = self.src[index]\n        \n        example = {\n            \"id\": index,\n            \"source\": src_item,\n            \"target\": tgt_item,\n        }\n\n        if self.align_dataset is not None:\n            example[\"alignment\"] = self.align_dataset[index]\n        if self.constraints is not None:\n            example[\"constraints\"] = self.constraints[index]\n        return example\n\n    def __len__(self):\n        return len(self.src)\n\n    def collater(self, samples, pad_to_length=None):\n        res = collate(\n            samples,\n            pad_idx=self.src_dict.pad(),\n            eos_idx=self.eos,\n            left_pad_source=self.left_pad_source,\n            left_pad_target=self.left_pad_target,\n            input_feeding=self.input_feeding,\n            pad_to_length=pad_to_length,\n            pad_to_multiple=self.pad_to_multiple,\n        )\n        if self.src_lang_id is not None or self.tgt_lang_id is not None:\n            src_tokens = res[\"net_input\"][\"src_tokens\"]\n            bsz = src_tokens.size(0)\n            if self.src_lang_id is not None:\n                res[\"net_input\"][\"src_lang_id\"] = (\n                    torch.LongTensor([[self.src_lang_id]]).expand(bsz, 1).to(src_tokens)\n                )\n            if self.tgt_lang_id is not None:\n                res[\"tgt_lang_id\"] = (\n                    torch.LongTensor([[self.tgt_lang_id]]).expand(bsz, 1).to(src_tokens)\n                )\n        return res\n\n    def num_tokens(self, index):\n        \"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"\n        return max(\n            self.src_sizes[index],\n            self.tgt_sizes[index] if self.tgt_sizes is not None else 0,\n        )\n\n    def num_tokens_vec(self, indices):\n        \"\"\"Return the number of tokens for a set of positions defined by indices.\n        This value is used to enforce ``--max-tokens`` during batching.\"\"\"\n        sizes = self.src_sizes[indices]\n        if self.tgt_sizes is not None:\n            sizes = np.maximum(sizes, self.tgt_sizes[indices])\n        return sizes\n\n    def size(self, index):\n        \"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"\n        return (\n            self.src_sizes[index],\n            self.tgt_sizes[index] if self.tgt_sizes is not None else 0,\n        )\n\n    def ordered_indices(self):\n        \"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"\n        if self.shuffle:\n            indices = np.random.permutation(len(self)).astype(np.int64)\n        else:\n            indices = np.arange(len(self), dtype=np.int64)\n        if self.buckets is None:\n            # sort by target length, then source length\n            if self.tgt_sizes is not None:\n                indices = indices[np.argsort(self.tgt_sizes[indices], kind=\"mergesort\")]\n            return indices[np.argsort(self.src_sizes[indices], kind=\"mergesort\")]\n        else:\n            # sort by bucketed_num_tokens, which is:\n            #   max(padded_src_len, padded_tgt_len)\n            return indices[\n                np.argsort(self.bucketed_num_tokens[indices], kind=\"mergesort\")\n            ]\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.src, \"supports_prefetch\", False) and (\n            getattr(self.tgt, \"supports_prefetch\", False) or self.tgt is None\n        )\n\n    def prefetch(self, indices):\n        self.src.prefetch(indices)\n        if self.tgt is not None:\n            self.tgt.prefetch(indices)\n        if self.align_dataset is not None:\n            self.align_dataset.prefetch(indices)\n\n    def filter_indices_by_size(self, indices, max_sizes):\n        \"\"\"Filter a list of sample indices. Remove those that are longer\n            than specified in max_sizes.\n\n        Args:\n            indices (np.array): original array of sample indices\n            max_sizes (int or list[int] or tuple[int]): max sample size,\n                can be defined separately for src and tgt (then list or tuple)\n\n        Returns:\n            np.array: filtered sample array\n            list: list of removed indices\n        \"\"\"\n        return data_utils.filter_paired_dataset_indices_by_size(\n            self.src_sizes,\n            self.tgt_sizes,\n            indices,\n            max_sizes,\n        )", ""]}
{"filename": "xlmr/src/task/seq2seq_ft_task.py", "chunked_list": ["import torch\nimport itertools\nimport os\nimport logging\nfrom typing import Dict, Optional\n\nfrom dataclasses import dataclass, field\nfrom fairseq import utils\nfrom fairseq.tasks.translation import TranslationTask\nfrom fairseq.utils import new_arange", "from fairseq.tasks.translation import TranslationTask\nfrom fairseq.utils import new_arange\nfrom fairseq.tasks import FairseqTask, register_task\nfrom fairseq.tasks.translation import TranslationConfig\nfrom fairseq.data import (\n    AppendTokenDataset,\n    ConcatDataset,\n    PrependTokenDataset,\n    StripTokenDataset,\n    TruncateDataset,", "    StripTokenDataset,\n    TruncateDataset,\n    data_utils,\n    indexed_dataset,\n)\nfrom fairseq.data import iterators\nfrom .seq2seq_dataset import LanguagePairDataset\nfrom fairseq.utils import safe_getattr, safe_hasattr\nfrom fairseq.data import Dictionary\n", "from fairseq.data import Dictionary\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_langpair_dataset(\n    data_path,\n    split,\n    src,\n    src_dict,\n    tgt,\n    tgt_dict,\n    combine,\n    dataset_impl,\n    upsample_primary,\n    left_pad_source,\n    left_pad_target,\n    max_source_positions,\n    max_target_positions,\n    prepend_bos=False,\n    load_alignments=False,\n    truncate_source=False,\n    append_source_id=False,\n    num_buckets=0,\n    shuffle=True,\n    pad_to_multiple=1,\n    prepend_bos_src=None,\n):\n    def split_exists(split, src, tgt, lang, data_path):\n        filename = os.path.join(data_path, \"{}.{}-{}.{}\".format(split, src, tgt, lang))\n        return indexed_dataset.dataset_exists(filename, impl=dataset_impl)\n\n    src_datasets = []\n    tgt_datasets = []\n\n    for k in itertools.count():\n        split_k = split + (str(k) if k > 0 else \"\")\n\n        # infer langcode\n        if split_exists(split_k, src, tgt, src, data_path):\n            prefix = os.path.join(data_path, \"{}.{}-{}.\".format(split_k, src, tgt))\n        elif split_exists(split_k, tgt, src, src, data_path):\n            prefix = os.path.join(data_path, \"{}.{}-{}.\".format(split_k, tgt, src))\n        else:\n            if k > 0:\n                break\n            else:\n                raise FileNotFoundError(\n                    \"Dataset not found: {} ({})\".format(split, data_path)\n                )\n\n        src_dataset = data_utils.load_indexed_dataset(\n            prefix + src, src_dict, dataset_impl\n        )\n        if truncate_source:\n            src_dataset = AppendTokenDataset(\n                TruncateDataset(\n                    StripTokenDataset(src_dataset, src_dict.eos()),\n                    max_source_positions - 2,\n                ),\n                src_dict.eos(),\n            )\n        src_datasets.append(src_dataset)\n        \n        tgt_dataset = data_utils.load_indexed_dataset(\n            prefix + tgt, tgt_dict, dataset_impl\n        )\n        if truncate_source:\n            tgt_dataset = AppendTokenDataset(\n                TruncateDataset(\n                    StripTokenDataset(tgt_dataset, tgt_dict.eos()),\n                    max_target_positions - 2,\n                ),\n                tgt_dict.eos(),\n            )\n\n        if tgt_dataset is not None:\n            tgt_datasets.append(tgt_dataset)\n\n        logger.info(\n            \"{} {} {}-{} {} examples\".format(\n                data_path, split_k, src, tgt, len(src_datasets[-1])\n            )\n        )\n\n        if not combine:\n            break\n\n    assert len(src_datasets) == len(tgt_datasets) or len(tgt_datasets) == 0\n\n    if len(src_datasets) == 1:\n        src_dataset = src_datasets[0]\n        tgt_dataset = tgt_datasets[0] if len(tgt_datasets) > 0 else None\n    else:\n        sample_ratios = [1] * len(src_datasets)\n        sample_ratios[0] = upsample_primary\n        src_dataset = ConcatDataset(src_datasets, sample_ratios)\n        if len(tgt_datasets) > 0:\n            tgt_dataset = ConcatDataset(tgt_datasets, sample_ratios)\n        else:\n            tgt_dataset = None\n\n    if prepend_bos:\n        assert hasattr(src_dict, \"bos_index\") and hasattr(tgt_dict, \"bos_index\")\n        src_dataset = PrependTokenDataset(src_dataset, src_dict.bos())\n        if tgt_dataset is not None:\n            tgt_dataset = PrependTokenDataset(tgt_dataset, tgt_dict.bos())\n    elif prepend_bos_src is not None:\n        logger.info(f\"prepending src bos: {prepend_bos_src}\")\n        src_dataset = PrependTokenDataset(src_dataset, prepend_bos_src)\n\n    eos = None\n    if append_source_id:\n        src_dataset = AppendTokenDataset(\n            src_dataset, src_dict.index(\"[{}]\".format(src))\n        )\n        if tgt_dataset is not None:\n            tgt_dataset = AppendTokenDataset(\n                tgt_dataset, tgt_dict.index(\"[{}]\".format(tgt))\n            )\n        eos = tgt_dict.index(\"[{}]\".format(tgt))\n\n    align_dataset = None\n    if load_alignments:\n        align_path = os.path.join(data_path, \"{}.align.{}-{}\".format(split, src, tgt))\n        if indexed_dataset.dataset_exists(align_path, impl=dataset_impl):\n            align_dataset = data_utils.load_indexed_dataset(\n                align_path, None, dataset_impl\n            )\n\n    tgt_dataset_sizes = tgt_dataset.sizes if tgt_dataset is not None else None\n    return LanguagePairDataset(\n        src_dataset,\n        src_dataset.sizes,\n        src_dict,\n        tgt_dataset,\n        tgt_dataset_sizes,\n        tgt_dict,\n        left_pad_source=left_pad_source,\n        left_pad_target=left_pad_target,\n        align_dataset=align_dataset,\n        eos=eos,\n        num_buckets=num_buckets,\n        shuffle=shuffle,\n        pad_to_multiple=pad_to_multiple,\n    )", "\n@dataclass\nclass FTTaskConfig(TranslationConfig):\n\n    megatron_model: bool = field(\n        default=False,\n        metadata={\"help\": \"using megatron-lm to split model\"},\n    )\n\n@register_task(\"seq2seq_ft_task\", dataclass=FTTaskConfig)\nclass Seq2SeqFineTuningTask(TranslationTask):\n\n    def __init__(self, cfg, src_dict, tgt_dict):\n        super().__init__(cfg, src_dict, tgt_dict)\n        self.megatron_model = safe_getattr(cfg, \"megatron_model\", False)\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        dictionary = Dictionary.load(filename)\n        dictionary.add_symbol(\"<mask>\")\n        return dictionary\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        paths = utils.split_paths(self.cfg.data)\n        data_path = paths[(epoch - 1) % len(paths)]\n        src, tgt = self.cfg.source_lang, self.cfg.target_lang\n\n        self.cfg.left_pad_source = False\n        self.cfg.left_pad_target = False\n        self.datasets[split] = load_langpair_dataset(\n            data_path,\n            split,\n            src,\n            self.src_dict,\n            tgt,\n            self.tgt_dict,\n            combine=combine,\n            dataset_impl=self.cfg.dataset_impl,\n            upsample_primary=self.cfg.upsample_primary,\n            left_pad_source=self.cfg.left_pad_source,\n            left_pad_target=self.cfg.left_pad_target,\n            max_source_positions=self.cfg.max_source_positions,\n            max_target_positions=self.cfg.max_target_positions,\n            truncate_source=self.cfg.truncate_source,\n            shuffle=(split != \"test\"),\n            prepend_bos=True,\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths, constraints=None):\n        return LanguagePairDataset(\n            src_tokens,\n            src_lengths,\n            self.source_dictionary,\n            tgt_dict=self.target_dictionary,\n            constraints=constraints,\n        )\n        \n    def build_generator(self, models, args=None, **kwargs):\n        from generator.iterative_refinement_generator import IterativeRefinementGenerator\n        return IterativeRefinementGenerator(\n            self.target_dictionary,\n            eos_penalty=getattr(args, \"iter_decode_eos_penalty\", 0.0),\n            max_iter=getattr(args, \"iter_decode_max_iter\", 9),\n            beam_size=getattr(args, \"iter_decode_with_beam\", 1),\n            reranking=getattr(args, \"iter_decode_with_external_reranker\", False),\n            decoding_format=getattr(args, \"decoding_format\", None),\n            adaptive=not getattr(args, \"iter_decode_force_max_iter\", True),\n            retain_history=getattr(args, \"retain_iter_history\", False),\n        )", "\n@register_task(\"seq2seq_ft_task\", dataclass=FTTaskConfig)\nclass Seq2SeqFineTuningTask(TranslationTask):\n\n    def __init__(self, cfg, src_dict, tgt_dict):\n        super().__init__(cfg, src_dict, tgt_dict)\n        self.megatron_model = safe_getattr(cfg, \"megatron_model\", False)\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        dictionary = Dictionary.load(filename)\n        dictionary.add_symbol(\"<mask>\")\n        return dictionary\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        paths = utils.split_paths(self.cfg.data)\n        data_path = paths[(epoch - 1) % len(paths)]\n        src, tgt = self.cfg.source_lang, self.cfg.target_lang\n\n        self.cfg.left_pad_source = False\n        self.cfg.left_pad_target = False\n        self.datasets[split] = load_langpair_dataset(\n            data_path,\n            split,\n            src,\n            self.src_dict,\n            tgt,\n            self.tgt_dict,\n            combine=combine,\n            dataset_impl=self.cfg.dataset_impl,\n            upsample_primary=self.cfg.upsample_primary,\n            left_pad_source=self.cfg.left_pad_source,\n            left_pad_target=self.cfg.left_pad_target,\n            max_source_positions=self.cfg.max_source_positions,\n            max_target_positions=self.cfg.max_target_positions,\n            truncate_source=self.cfg.truncate_source,\n            shuffle=(split != \"test\"),\n            prepend_bos=True,\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths, constraints=None):\n        return LanguagePairDataset(\n            src_tokens,\n            src_lengths,\n            self.source_dictionary,\n            tgt_dict=self.target_dictionary,\n            constraints=constraints,\n        )\n        \n    def build_generator(self, models, args=None, **kwargs):\n        from generator.iterative_refinement_generator import IterativeRefinementGenerator\n        return IterativeRefinementGenerator(\n            self.target_dictionary,\n            eos_penalty=getattr(args, \"iter_decode_eos_penalty\", 0.0),\n            max_iter=getattr(args, \"iter_decode_max_iter\", 9),\n            beam_size=getattr(args, \"iter_decode_with_beam\", 1),\n            reranking=getattr(args, \"iter_decode_with_external_reranker\", False),\n            decoding_format=getattr(args, \"decoding_format\", None),\n            adaptive=not getattr(args, \"iter_decode_force_max_iter\", True),\n            retain_history=getattr(args, \"retain_iter_history\", False),\n        )", ""]}
{"filename": "xlmr/src/generator/sequence_generator.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nimport sys\nfrom typing import Dict, List, Optional\n\nimport torch", "\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom fairseq import search, utils\nfrom fairseq.data import data_utils\nfrom fairseq.models import FairseqIncrementalDecoder\nfrom fairseq.ngram_repeat_block import NGramRepeatBlock\n", "from fairseq.ngram_repeat_block import NGramRepeatBlock\n\n\nclass SequenceGenerator(nn.Module):\n    def __init__(\n        self,\n        models,\n        tgt_dict,\n        beam_size=1,\n        max_len_a=0,\n        max_len_b=200,\n        max_len=0,\n        min_len=1,\n        normalize_scores=True,\n        len_penalty=1.0,\n        unk_penalty=0.0,\n        temperature=1.0,\n        match_source_len=False,\n        no_repeat_ngram_size=0,\n        search_strategy=None,\n        eos=None,\n        symbols_to_strip_from_output=None,\n        lm_model=None,\n        lm_weight=1.0,\n        tokens_to_suppress=(),\n    ):\n        \"\"\"Generates translations of a given source sentence.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models,\n                currently support fairseq.models.TransformerModel for scripting\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            max_len (int, optional): the maximum length of the generated output\n                (not including end-of-sentence)\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            normalize_scores (bool, optional): normalize scores by the length\n                of the output (default: True)\n            len_penalty (float, optional): length penalty, where <1.0 favors\n                shorter, >1.0 favors longer sentences (default: 1.0)\n            unk_penalty (float, optional): unknown word penalty, where <0\n                produces more unks, >0 produces fewer (default: 0.0)\n            temperature (float, optional): temperature, where values\n                >1.0 produce more uniform samples and values <1.0 produce\n                sharper samples (default: 1.0)\n            match_source_len (bool, optional): outputs should match the source\n                length (default: False)\n        \"\"\"\n        super().__init__()\n        if isinstance(models, EnsembleModel):\n            self.model = models\n        else:\n            self.model = EnsembleModel(models)\n        self.tgt_dict = tgt_dict\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos() if eos is None else eos\n        self.symbols_to_strip_from_output = (\n            symbols_to_strip_from_output.union({self.eos})\n            if symbols_to_strip_from_output is not None\n            else {self.eos}\n        )\n\n        self.token_indices_to_suppress: Optional[Tensor] = None\n        token_indices_to_suppress = []\n        for token_string in tokens_to_suppress:\n            token_index = tgt_dict.index(token_string)\n            assert token_index != self.unk\n            token_indices_to_suppress.append(token_index)\n        if len(token_indices_to_suppress) > 0:\n            self.token_indices_to_suppress = torch.Tensor(\n                token_indices_to_suppress\n            ).long()\n\n        self.vocab_size = len(tgt_dict)\n        self.beam_size = beam_size\n        # the max beam size is the dictionary size - 1, since we never select pad\n        self.beam_size = min(beam_size, self.vocab_size - 1)\n        self.model.set_decoder_beam_size(self.beam_size)\n        self.max_len_a = max_len_a\n        self.max_len_b = max_len_b\n        self.min_len = min_len\n        self.max_len = max_len or self.model.max_decoder_positions()\n\n        self.normalize_scores = normalize_scores\n        self.len_penalty = len_penalty\n        self.unk_penalty = unk_penalty\n        self.temperature = temperature\n        self.match_source_len = match_source_len\n\n        if no_repeat_ngram_size > 0:\n            self.repeat_ngram_blocker = NGramRepeatBlock(no_repeat_ngram_size)\n        else:\n            self.repeat_ngram_blocker = None\n\n        assert temperature > 0, \"--temperature must be greater than 0\"\n\n        self.search = (\n            search.BeamSearch(tgt_dict) if search_strategy is None else search_strategy\n        )\n        # We only need to set src_lengths in LengthConstrainedBeamSearch.\n        # As a module attribute, setting it would break in multithread\n        # settings when the model is shared.\n        self.should_set_src_lengths = (\n            hasattr(self.search, \"needs_src_lengths\") and self.search.needs_src_lengths\n        )\n\n        self.model.eval()\n\n        self.lm_model = lm_model\n        self.lm_weight = lm_weight\n        if self.lm_model is not None:\n            self.lm_model.eval()\n\n    def cuda(self):\n        self.model.cuda()\n        return self\n\n    @torch.no_grad()\n    def forward(\n        self,\n        sample: Dict[str, Dict[str, Tensor]],\n        prefix_tokens: Optional[Tensor] = None,\n        bos_token: Optional[int] = None,\n    ):\n        \"\"\"Generate a batch of translations.\n\n        Args:\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n            bos_token (int, optional): beginning of sentence token\n                (default: self.eos)\n        \"\"\"\n        return self._generate(sample, prefix_tokens, bos_token=bos_token)\n\n    # TODO(myleott): unused, deprecate after pytorch-translate migration\n    def generate_batched_itr(self, data_itr, beam_size=None, cuda=False, timer=None):\n        \"\"\"Iterate over a batched dataset and yield individual translations.\n        Args:\n            cuda (bool, optional): use GPU for generation\n            timer (StopwatchMeter, optional): time generations\n        \"\"\"\n        for sample in data_itr:\n            s = utils.move_to_cuda(sample) if cuda else sample\n            if \"net_input\" not in s:\n                continue\n            input = s[\"net_input\"]\n            # model.forward normally channels prev_output_tokens into the decoder\n            # separately, but SequenceGenerator directly calls model.encoder\n            encoder_input = {\n                k: v for k, v in input.items() if k != \"prev_output_tokens\"\n            }\n            if timer is not None:\n                timer.start()\n            with torch.no_grad():\n                hypos = self.generate(encoder_input)\n            if timer is not None:\n                timer.stop(sum(len(h[0][\"tokens\"]) for h in hypos))\n            for i, id in enumerate(s[\"id\"].data):\n                # remove padding\n                src = utils.strip_pad(input[\"src_tokens\"].data[i, :], self.pad)\n                ref = (\n                    utils.strip_pad(s[\"target\"].data[i, :], self.pad)\n                    if s[\"target\"] is not None\n                    else None\n                )\n                yield id, src, ref, hypos[i]\n\n    @torch.no_grad()\n    def generate(\n        self, models, sample: Dict[str, Dict[str, Tensor]], **kwargs\n    ) -> List[List[Dict[str, Tensor]]]:\n        \"\"\"Generate translations. Match the api of other fairseq generators.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n            constraints (torch.LongTensor, optional): force decoder to include\n                the list of constraints\n            bos_token (int, optional): beginning of sentence token\n                (default: self.eos)\n        \"\"\"\n        return self._generate(sample, **kwargs)\n\n    def _generate(\n        self,\n        sample: Dict[str, Dict[str, Tensor]],\n        prefix_tokens: Optional[Tensor] = None,\n        constraints: Optional[Tensor] = None,\n        bos_token: Optional[int] = None,\n    ):\n        incremental_states = torch.jit.annotate(\n            List[Dict[str, Dict[str, Optional[Tensor]]]],\n            [\n                torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {})\n                for i in range(self.model.models_size)\n            ],\n        )\n        net_input = sample[\"net_input\"]\n\n        if \"src_tokens\" in net_input:\n            src_tokens = net_input[\"src_tokens\"]\n            # length of the source text being the character length except EndOfSentence and pad\n            # if src_lengths exists in net_input (speech_to_text dataset case), then use it\n            if \"src_lengths\" in net_input:\n                src_lengths = net_input[\"src_lengths\"]\n            else:\n                src_lengths = (\n                    (src_tokens.ne(self.eos) & src_tokens.ne(self.pad))\n                    .long()\n                    .sum(dim=1)\n                )\n        elif \"source\" in net_input:\n            src_tokens = net_input[\"source\"]\n            src_lengths = (\n                net_input[\"padding_mask\"].size(-1) - net_input[\"padding_mask\"].sum(-1)\n                if net_input[\"padding_mask\"] is not None\n                else torch.tensor(src_tokens.size(-1)).to(src_tokens)\n            )\n        elif \"features\" in net_input:\n            src_tokens = net_input[\"features\"]\n            src_lengths = (\n                net_input[\"padding_mask\"].size(-1) - net_input[\"padding_mask\"].sum(-1)\n                if net_input[\"padding_mask\"] is not None\n                else torch.tensor(src_tokens.size(-1)).to(src_tokens)\n            )\n        else:\n            raise Exception(\n                \"expected src_tokens or source in net input. input keys: \"\n                + str(net_input.keys())\n            )\n \n        # bsz: total number of sentences in beam\n        # Note that src_tokens may have more than 2 dimensions (i.e. audio features)\n        bsz, src_len = src_tokens.size()[:2]\n        beam_size = self.beam_size\n\n        if constraints is not None and not self.search.supports_constraints:\n            raise NotImplementedError(\n                \"Target-side constraints were provided, but search method doesn't support them\"\n            )\n\n        # Initialize constraints, when active\n        self.search.init_constraints(constraints, beam_size)\n\n        max_len: int = -1\n        if self.match_source_len:\n            max_len = src_lengths.max().item()\n        else:\n            max_len = min(\n                int(self.max_len_a * src_len + self.max_len_b),\n                self.max_len - 1,\n            )\n        assert (\n            self.min_len <= max_len\n        ), \"min_len cannot be larger than max_len, please adjust these!\"\n        # compute the encoder output for each beam\n        with torch.autograd.profiler.record_function(\"EnsembleModel: forward_encoder\"):\n            encoder_outs = self.model.forward_encoder(net_input)\n\n        # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores\n        new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n        new_order = new_order.to(src_tokens.device).long()\n        encoder_outs = self.model.reorder_encoder_out(encoder_outs, new_order)\n        # ensure encoder_outs is a List.\n        assert encoder_outs is not None\n\n        # initialize buffers\n        scores = (\n            torch.zeros(bsz * beam_size, max_len + 1).to(src_tokens).float()\n        )  # +1 for eos; pad is never chosen for scoring\n        tokens = (\n            torch.zeros(bsz * beam_size, max_len + 2)\n            .to(src_tokens)\n            .long()\n            .fill_(self.pad)\n        )  # +2 for eos and pad\n        if bos_token is not None:\n            bos_new_token = bos_token.repeat(beam_size, 1)\n        tokens[:, 0] = self.eos if bos_token is None else bos_new_token.squeeze()\n        attn: Optional[Tensor] = None\n\n        # A list that indicates candidates that should be ignored.\n        # For example, suppose we're sampling and have already finalized 2/5\n        # samples. Then cands_to_ignore would mark 2 positions as being ignored,\n        # so that we only finalize the remaining 3 samples.\n        cands_to_ignore = (\n            torch.zeros(bsz, beam_size).to(src_tokens).eq(-1)\n        )  # forward and backward-compatible False mask\n\n        # list of completed sentences\n        finalized = torch.jit.annotate(\n            List[List[Dict[str, Tensor]]],\n            [torch.jit.annotate(List[Dict[str, Tensor]], []) for i in range(bsz)],\n        )  # contains lists of dictionaries of infomation about the hypothesis being finalized at each step\n\n        # a boolean array indicating if the sentence at the index is finished or not\n        finished = [False for i in range(bsz)]\n        num_remaining_sent = bsz  # number of sentences remaining\n\n        # number of candidate hypos per step\n        cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n\n        # offset arrays for converting between different indexing schemes\n        bbsz_offsets = (\n            (torch.arange(0, bsz) * beam_size)\n            .unsqueeze(1)\n            .type_as(tokens)\n            .to(src_tokens.device)\n        )\n        cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)\n\n        reorder_state: Optional[Tensor] = None\n        batch_idxs: Optional[Tensor] = None\n\n        original_batch_idxs: Optional[Tensor] = None\n        if \"id\" in sample and isinstance(sample[\"id\"], Tensor):\n            original_batch_idxs = sample[\"id\"]\n        else:\n            original_batch_idxs = torch.arange(0, bsz).type_as(tokens)\n\n        for step in range(max_len + 1):  # one extra step for EOS marker\n            # reorder decoder internal states based on the prev choice of beams\n            if reorder_state is not None:\n                if batch_idxs is not None:\n                    # update beam indices to take into account removed sentences\n                    corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(\n                        batch_idxs\n                    )\n                    reorder_state.view(-1, beam_size).add_(\n                        corr.unsqueeze(-1) * beam_size\n                    )\n                    original_batch_idxs = original_batch_idxs[batch_idxs]\n                self.model.reorder_incremental_state(incremental_states, reorder_state)\n                encoder_outs = self.model.reorder_encoder_out(\n                    encoder_outs, reorder_state\n                )\n            with torch.autograd.profiler.record_function(\n                \"EnsembleModel: forward_decoder\"\n            ):\n\n                lprobs, avg_attn_scores = self.model.forward_decoder(\n                    tokens[:, : step + 1],\n                    encoder_outs,\n                    incremental_states,\n                    self.temperature,\n                )\n            if self.lm_model is not None:\n                lm_out = self.lm_model(tokens[:, : step + 1])\n                probs = self.lm_model.get_normalized_probs(\n                    lm_out, log_probs=True, sample=None\n                )\n                probs = probs[:, -1, :] * self.lm_weight\n                lprobs += probs\n\n            lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)\n\n            lprobs[:, self.pad] = -math.inf  # never select pad\n            lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty\n\n            # handle max length constraint\n            if step >= max_len:\n                lprobs[:, : self.eos] = -math.inf\n                lprobs[:, self.eos + 1 :] = -math.inf\n\n            # handle prefix tokens (possibly with different lengths)\n            if (\n                prefix_tokens is not None\n                and step < prefix_tokens.size(1)\n                and step < max_len\n            ):\n                lprobs, tokens, scores = self._prefix_tokens(\n                    step, lprobs, scores, tokens, prefix_tokens, beam_size\n                )\n            else:\n                if step < self.min_len:\n                    # minimum length constraint (does not apply if using prefix_tokens)\n                    lprobs[:, self.eos] = -math.inf\n\n                if self.token_indices_to_suppress is not None:\n                    lprobs[:, self.token_indices_to_suppress] = -math.inf\n\n            # Record attention scores, only support avg_attn_scores is a Tensor\n            if avg_attn_scores is not None:\n                if attn is None:\n                    attn = torch.empty(\n                        bsz * beam_size, avg_attn_scores.size(1), max_len + 2\n                    ).to(scores)\n                attn[:, :, step + 1].copy_(avg_attn_scores)\n\n            scores = scores.type_as(lprobs)\n            eos_bbsz_idx = torch.empty(0).to(\n                tokens\n            )  # indices of hypothesis ending with eos (finished sentences)\n            eos_scores = torch.empty(0).to(\n                scores\n            )  # scores of hypothesis ending with eos (finished sentences)\n\n            if self.should_set_src_lengths:\n                self.search.set_src_lengths(src_lengths)\n\n            if self.repeat_ngram_blocker is not None:\n                lprobs = self.repeat_ngram_blocker(tokens, lprobs, bsz, beam_size, step)\n\n            # Shape: (batch, cand_size)\n            cand_scores, cand_indices, cand_beams = self.search.step(\n                step,\n                lprobs.view(bsz, -1, self.vocab_size),\n                scores.view(bsz, beam_size, -1)[:, :, :step],\n                tokens[:, : step + 1],\n                original_batch_idxs,\n            )\n\n            # cand_bbsz_idx contains beam indices for the top candidate\n            # hypotheses, with a range of values: [0, bsz*beam_size),\n            # and dimensions: [bsz, cand_size]\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n\n            # finalize hypotheses that end in eos\n            # Shape of eos_mask: (batch size, beam size)\n            eos_mask = cand_indices.eq(self.eos) & cand_scores.ne(-math.inf)\n            eos_mask[:, :beam_size][cands_to_ignore] = torch.tensor(0).to(eos_mask)\n\n            # only consider eos when it's among the top beam_size indices\n            # Now we know what beam item(s) to finish\n            # Shape: 1d list of absolute-numbered\n            eos_bbsz_idx = torch.masked_select(\n                cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size]\n            )\n\n            finalized_sents: List[int] = []\n            if eos_bbsz_idx.numel() > 0:\n                eos_scores = torch.masked_select(\n                    cand_scores[:, :beam_size], mask=eos_mask[:, :beam_size]\n                )\n\n                finalized_sents = self.finalize_hypos(\n                    step,\n                    eos_bbsz_idx,\n                    eos_scores,\n                    tokens,\n                    scores,\n                    finalized,\n                    finished,\n                    beam_size,\n                    attn,\n                    src_lengths,\n                    max_len,\n                )\n                num_remaining_sent -= len(finalized_sents)\n\n            assert num_remaining_sent >= 0\n            if num_remaining_sent == 0:\n                break\n            if self.search.stop_on_max_len and step >= max_len:\n                break\n            assert step < max_len, f\"{step} < {max_len}\"\n\n            # Remove finalized sentences (ones for which {beam_size}\n            # finished hypotheses have been generated) from the batch.\n            if len(finalized_sents) > 0:\n                new_bsz = bsz - len(finalized_sents)\n\n                # construct batch_idxs which holds indices of batches to keep for the next pass\n                batch_mask = torch.ones(\n                    bsz, dtype=torch.bool, device=cand_indices.device\n                )\n                batch_mask[finalized_sents] = False\n                # TODO replace `nonzero(as_tuple=False)` after TorchScript supports it\n                batch_idxs = torch.arange(\n                    bsz, device=cand_indices.device\n                ).masked_select(batch_mask)\n\n                # Choose the subset of the hypothesized constraints that will continue\n                self.search.prune_sentences(batch_idxs)\n\n                eos_mask = eos_mask[batch_idxs]\n                cand_beams = cand_beams[batch_idxs]\n                bbsz_offsets.resize_(new_bsz, 1)\n                cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n                cand_scores = cand_scores[batch_idxs]\n                cand_indices = cand_indices[batch_idxs]\n\n                if prefix_tokens is not None:\n                    prefix_tokens = prefix_tokens[batch_idxs]\n                src_lengths = src_lengths[batch_idxs]\n                cands_to_ignore = cands_to_ignore[batch_idxs]\n\n                scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n                tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n                if attn is not None:\n                    attn = attn.view(bsz, -1)[batch_idxs].view(\n                        new_bsz * beam_size, attn.size(1), -1\n                    )\n                bsz = new_bsz\n            else:\n                batch_idxs = None\n\n            # Set active_mask so that values > cand_size indicate eos hypos\n            # and values < cand_size indicate candidate active hypos.\n            # After, the min values per row are the top candidate active hypos\n\n            # Rewrite the operator since the element wise or is not supported in torchscript.\n\n            eos_mask[:, :beam_size] = ~((~cands_to_ignore) & (~eos_mask[:, :beam_size]))\n            active_mask = torch.add(\n                eos_mask.type_as(cand_offsets) * cand_size,\n                cand_offsets[: eos_mask.size(1)],\n            )\n\n            # get the top beam_size active hypotheses, which are just\n            # the hypos with the smallest values in active_mask.\n            # {active_hypos} indicates which {beam_size} hypotheses\n            # from the list of {2 * beam_size} candidates were\n            # selected. Shapes: (batch size, beam size)\n            new_cands_to_ignore, active_hypos = torch.topk(\n                active_mask, k=beam_size, dim=1, largest=False\n            )\n\n            # update cands_to_ignore to ignore any finalized hypos.\n            cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n            # Make sure there is at least one active item for each sentence in the batch.\n            assert (~cands_to_ignore).any(dim=1).all()\n\n            # update cands_to_ignore to ignore any finalized hypos\n\n            # {active_bbsz_idx} denotes which beam number is continued for each new hypothesis (a beam\n            # can be selected more than once).\n            active_bbsz_idx = torch.gather(cand_bbsz_idx, dim=1, index=active_hypos)\n            active_scores = torch.gather(cand_scores, dim=1, index=active_hypos)\n\n            active_bbsz_idx = active_bbsz_idx.view(-1)\n            active_scores = active_scores.view(-1)\n\n            # copy tokens and scores for active hypotheses\n\n            # Set the tokens for each beam (can select the same row more than once)\n            tokens[:, : step + 1] = torch.index_select(\n                tokens[:, : step + 1], dim=0, index=active_bbsz_idx\n            )\n            # Select the next token for each of them\n            tokens.view(bsz, beam_size, -1)[:, :, step + 1] = torch.gather(\n                cand_indices, dim=1, index=active_hypos\n            )\n            if step > 0:\n                scores[:, :step] = torch.index_select(\n                    scores[:, :step], dim=0, index=active_bbsz_idx\n                )\n            scores.view(bsz, beam_size, -1)[:, :, step] = torch.gather(\n                cand_scores, dim=1, index=active_hypos\n            )\n\n            # Update constraints based on which candidates were selected for the next beam\n            self.search.update_constraints(active_hypos)\n\n            # copy attention for active hypotheses\n            if attn is not None:\n                attn[:, :, : step + 2] = torch.index_select(\n                    attn[:, :, : step + 2], dim=0, index=active_bbsz_idx\n                )\n\n            # reorder incremental state in decoder\n            reorder_state = active_bbsz_idx\n\n        # sort by score descending\n        for sent in range(len(finalized)):\n            scores = torch.tensor(\n                [float(elem[\"score\"].item()) for elem in finalized[sent]]\n            )\n            _, sorted_scores_indices = torch.sort(scores, descending=True)\n            finalized[sent] = [finalized[sent][ssi] for ssi in sorted_scores_indices]\n            finalized[sent] = torch.jit.annotate(\n                List[Dict[str, Tensor]], finalized[sent]\n            )\n        return finalized\n\n    def _prefix_tokens(\n        self, step: int, lprobs, scores, tokens, prefix_tokens, beam_size: int\n    ):\n        \"\"\"Handle prefix tokens\"\"\"\n        prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n        prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n        prefix_mask = prefix_toks.ne(self.pad)\n        lprobs[prefix_mask] = torch.tensor(-math.inf).to(lprobs)\n        lprobs[prefix_mask] = lprobs[prefix_mask].scatter(\n            -1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lprobs[prefix_mask]\n        )\n        # if prefix includes eos, then we should make sure tokens and\n        # scores are the same across all beams\n        eos_mask = prefix_toks.eq(self.eos)\n        if eos_mask.any():\n            # validate that the first beam matches the prefix\n            first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[\n                :, 0, 1 : step + 1\n            ]\n            eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n            target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n            assert (first_beam == target_prefix).all()\n\n            # copy tokens, scores and lprobs from the first beam to all beams\n            tokens = self.replicate_first_beam(tokens, eos_mask_batch_dim, beam_size)\n            scores = self.replicate_first_beam(scores, eos_mask_batch_dim, beam_size)\n            lprobs = self.replicate_first_beam(lprobs, eos_mask_batch_dim, beam_size)\n        return lprobs, tokens, scores\n\n    def replicate_first_beam(self, tensor, mask, beam_size: int):\n        tensor = tensor.view(-1, beam_size, tensor.size(-1))\n        tensor[mask] = tensor[mask][:, :1, :]\n        return tensor.view(-1, tensor.size(-1))\n\n    def finalize_hypos(\n        self,\n        step: int,\n        bbsz_idx,\n        eos_scores,\n        tokens,\n        scores,\n        finalized: List[List[Dict[str, Tensor]]],\n        finished: List[bool],\n        beam_size: int,\n        attn: Optional[Tensor],\n        src_lengths,\n        max_len: int,\n    ):\n        \"\"\"Finalize hypothesis, store finalized information in `finalized`, and change `finished` accordingly.\n        A sentence is finalized when {beam_size} finished items have been collected for it.\n\n        Returns number of sentences (not beam items) being finalized.\n        These will be removed from the batch and not processed further.\n        Args:\n            bbsz_idx (Tensor):\n        \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n\n        # clone relevant token and attention tensors.\n        # tokens is (batch * beam, max_len). So the index_select\n        # gets the newly EOS rows, then selects cols 1..{step + 2}\n        tokens_clone = tokens.index_select(0, bbsz_idx)[\n            :, 1 : step + 2\n        ]  # skip the first index, which is EOS\n\n        tokens_clone[:, step] = self.eos\n        attn_clone = (\n            attn.index_select(0, bbsz_idx)[:, :, 1 : step + 2]\n            if attn is not None\n            else None\n        )\n\n        # compute scores per token position\n        pos_scores = scores.index_select(0, bbsz_idx)[:, : step + 1]\n        pos_scores[:, step] = eos_scores\n        # convert from cumulative to per-position scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n\n        # normalize sentence-level scores\n        if self.normalize_scores:\n            eos_scores /= (step + 1) ** self.len_penalty\n\n        # cum_unfin records which sentences in the batch are finished.\n        # It helps match indexing between (a) the original sentences\n        # in the batch and (b) the current, possibly-reduced set of\n        # sentences.\n        cum_unfin: List[int] = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n        cum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int).to(bbsz_idx)\n\n        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode=\"trunc\")\n        sent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)\n\n        # Create a set of \"{sent}{unfin_idx}\", where\n        # \"unfin_idx\" is the index in the current (possibly reduced)\n        # list of sentences, and \"sent\" is the index in the original,\n        # unreduced batch\n        # For every finished beam item\n        # sentence index in the current (possibly reduced) batch\n        seen = (sent << 32) + unfin_idx\n        unique_seen: List[int] = torch.unique(seen).tolist()\n\n        if self.match_source_len:\n            condition = step > torch.index_select(src_lengths, 0, unfin_idx)\n            eos_scores = torch.where(condition, torch.tensor(-math.inf), eos_scores)\n        sent_list: List[int] = sent.tolist()\n        for i in range(bbsz_idx.size()[0]):\n            # An input sentence (among those in a batch) is finished when\n            # beam_size hypotheses have been collected for it\n            if len(finalized[sent_list[i]]) < beam_size:\n                if attn_clone is not None:\n                    # remove padding tokens from attn scores\n                    hypo_attn = attn_clone[i]\n                else:\n                    hypo_attn = torch.empty(0)\n\n                finalized[sent_list[i]].append(\n                    {\n                        \"tokens\": tokens_clone[i],\n                        \"score\": eos_scores[i],\n                        \"attention\": hypo_attn,  # src_len x tgt_len\n                        \"alignment\": torch.empty(0),\n                        \"positional_scores\": pos_scores[i],\n                    }\n                )\n\n        newly_finished: List[int] = []\n        for unique_s in unique_seen:\n            # check termination conditions for this sentence\n            unique_sent: int = unique_s >> 32\n            unique_unfin_idx: int = unique_s - (unique_sent << 32)\n\n            if not finished[unique_sent] and self.is_finished(\n                step, unique_unfin_idx, max_len, len(finalized[unique_sent]), beam_size\n            ):\n                finished[unique_sent] = True\n                newly_finished.append(unique_unfin_idx)\n\n        return newly_finished\n\n    def is_finished(\n        self,\n        step: int,\n        unfin_idx: int,\n        max_len: int,\n        finalized_sent_len: int,\n        beam_size: int,\n    ):\n        \"\"\"\n        Check whether decoding for a sentence is finished, which\n        occurs when the list of finalized sentences has reached the\n        beam size, or when we reach the maximum length.\n        \"\"\"\n        assert finalized_sent_len <= beam_size\n        if finalized_sent_len == beam_size or step == max_len:\n            return True\n        return False", "\n\nclass EnsembleModel(nn.Module):\n    \"\"\"A wrapper around an ensemble of models.\"\"\"\n\n    def __init__(self, models):\n        super().__init__()\n        self.models_size = len(models)\n        # method '__len__' is not supported in ModuleList for torch script\n        self.single_model = models[0]\n        self.models = nn.ModuleList(models)\n\n        self.has_incremental = True\n\n    def forward(self):\n        pass\n\n    def has_encoder(self):\n        return hasattr(self.single_model, \"encoder\")\n\n    def has_incremental_states(self):\n        return self.has_incremental\n\n    def max_decoder_positions(self):\n        return min(\n            [\n                m.max_decoder_positions()\n                for m in self.models\n                if hasattr(m, \"max_decoder_positions\")\n            ]\n            + [sys.maxsize]\n        )\n\n    def set_decoder_beam_size(self, beam_size):\n        \"\"\"Set beam size for efficient beamable enc-dec attention.\"\"\"\n        if beam_size > 1:\n            for model in self.models:\n                if hasattr(model, \"set_beam_size\"):\n                    model.set_beam_size(beam_size)\n\n    @torch.jit.export\n    def forward_encoder(self, net_input: Dict[str, Tensor]):\n        return [model.forward_encoder(net_input) for model in self.models]\n\n    @torch.jit.export\n    def forward_decoder(\n        self,\n        tokens,\n        encoder_outs: List[Dict[str, List[Tensor]]],\n        incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]],\n        temperature: float = 1.0,\n    ):\n        log_probs = []\n        avg_attn: Optional[Tensor] = None\n        encoder_out: Optional[Dict[str, List[Tensor]]] = None\n        for i, model in enumerate(self.models):\n            \n            encoder_out = encoder_outs[i]\n            # decode each model\n            if self.has_incremental_states():\n                decoder_out = model.forward_decoder(\n                    tokens,\n                    encoder_out=encoder_out,\n                    incremental_state=incremental_states[i],\n                )\n                incremental_states[i] = decoder_out[2]\n            else:\n                if hasattr(model, \"decoder\"):\n                    decoder_out = model.forward_decoder(tokens, encoder_out=encoder_out)\n                else:\n                    decoder_out = model.forward(tokens)\n\n            attn: Optional[Tensor] = None\n            decoder_len = len(decoder_out) - 1\n            if decoder_len > 1 and decoder_out[1] is not None:\n                if isinstance(decoder_out[1], Tensor):\n                    attn = decoder_out[1]\n                else:\n                    attn_holder = decoder_out[1][\"attn\"]\n                    if isinstance(attn_holder, Tensor):\n                        attn = attn_holder\n                    elif attn_holder is not None:\n                        attn = attn_holder[0]\n                if attn is not None:\n                    attn = attn[:, -1, :]\n\n            decoder_out_tuple = (\n                decoder_out[0][:, -1:, :].div_(temperature),\n                None if decoder_len <= 1 else decoder_out[1],\n            )\n            probs = model.get_normalized_probs(\n                decoder_out_tuple, log_probs=True, sample=None\n            )\n            probs = probs[:, -1, :]\n            if self.models_size == 1:\n                return probs, None\n\n            log_probs.append(probs)\n            if attn is not None:\n                if avg_attn is None:\n                    avg_attn = attn\n                else:\n                    avg_attn.add_(attn)\n\n        avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(\n            self.models_size\n        )\n\n        if avg_attn is not None:\n            avg_attn.div_(self.models_size)\n        return avg_probs, avg_attn\n\n    @torch.jit.export\n    def reorder_encoder_out(\n        self, encoder_outs: Optional[List[Dict[str, List[Tensor]]]], new_order\n    ):\n        \"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"\n        new_outs: List[Dict[str, List[Tensor]]] = []\n        for i, model in enumerate(self.models):\n            assert encoder_outs is not None\n            new_outs.append(\n                model.reorder_encoder_out(encoder_outs[i], new_order)\n            )\n        return new_outs\n        \n    @torch.jit.export\n    def reorder_incremental_state(\n        self,\n        incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]],\n        new_order,\n    ):\n        if not self.has_incremental_states():\n            return\n        for i, model in enumerate(self.models):\n            model.reorder_incremental_state(\n                incremental_states[i], new_order\n            )", "\n\nclass SequenceGeneratorWithAlignment(SequenceGenerator):\n    def __init__(\n        self, models, tgt_dict, left_pad_target=False, print_alignment=\"hard\", **kwargs\n    ):\n        \"\"\"Generates translations of a given source sentence.\n\n        Produces alignments following \"Jointly Learning to Align and\n        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n\n        Args:\n            left_pad_target (bool, optional): Whether or not the\n                hypothesis should be left padded or not when they are\n                teacher forced for generating alignments.\n        \"\"\"\n        super().__init__(EnsembleModelWithAlignment(models), tgt_dict, **kwargs)\n        self.left_pad_target = left_pad_target\n\n        if print_alignment == \"hard\":\n            self.extract_alignment = utils.extract_hard_alignment\n        elif print_alignment == \"soft\":\n            self.extract_alignment = utils.extract_soft_alignment\n\n    @torch.no_grad()\n    def generate(self, models, sample, **kwargs):\n        finalized = super()._generate(sample, **kwargs)\n\n        src_tokens = sample[\"net_input\"][\"src_tokens\"]\n        bsz = src_tokens.shape[0]\n        beam_size = self.beam_size\n        (\n            src_tokens,\n            src_lengths,\n            prev_output_tokens,\n            tgt_tokens,\n        ) = self._prepare_batch_for_alignment(sample, finalized)\n        if any(getattr(m, \"full_context_alignment\", False) for m in self.model.models):\n            attn = self.model.forward_align(src_tokens, src_lengths, prev_output_tokens)\n        else:\n            attn = [\n                finalized[i // beam_size][i % beam_size][\"attention\"].transpose(1, 0)\n                for i in range(bsz * beam_size)\n            ]\n\n        if src_tokens.device != \"cpu\":\n            src_tokens = src_tokens.to(\"cpu\")\n            tgt_tokens = tgt_tokens.to(\"cpu\")\n            attn = [i.to(\"cpu\") for i in attn]\n\n        # Process the attn matrix to extract hard alignments.\n        for i in range(bsz * beam_size):\n            alignment = self.extract_alignment(\n                attn[i], src_tokens[i], tgt_tokens[i], self.pad, self.eos\n            )\n            finalized[i // beam_size][i % beam_size][\"alignment\"] = alignment\n        return finalized\n\n    def _prepare_batch_for_alignment(self, sample, hypothesis):\n        src_tokens = sample[\"net_input\"][\"src_tokens\"]\n        bsz = src_tokens.shape[0]\n        src_tokens = (\n            src_tokens[:, None, :]\n            .expand(-1, self.beam_size, -1)\n            .contiguous()\n            .view(bsz * self.beam_size, -1)\n        )\n        src_lengths = sample[\"net_input\"][\"src_lengths\"]\n        src_lengths = (\n            src_lengths[:, None]\n            .expand(-1, self.beam_size)\n            .contiguous()\n            .view(bsz * self.beam_size)\n        )\n        prev_output_tokens = data_utils.collate_tokens(\n            [beam[\"tokens\"] for example in hypothesis for beam in example],\n            self.pad,\n            self.eos,\n            self.left_pad_target,\n            move_eos_to_beginning=True,\n        )\n        tgt_tokens = data_utils.collate_tokens(\n            [beam[\"tokens\"] for example in hypothesis for beam in example],\n            self.pad,\n            self.eos,\n            self.left_pad_target,\n            move_eos_to_beginning=False,\n        )\n        return src_tokens, src_lengths, prev_output_tokens, tgt_tokens", "\n\nclass EnsembleModelWithAlignment(EnsembleModel):\n    \"\"\"A wrapper around an ensemble of models.\"\"\"\n\n    def __init__(self, models):\n        super().__init__(models)\n\n    def forward_align(self, src_tokens, src_lengths, prev_output_tokens):\n        avg_attn = None\n        for model in self.models:\n            decoder_out = model(src_tokens, src_lengths, prev_output_tokens)\n            attn = decoder_out[1][\"attn\"][0]\n            if avg_attn is None:\n                avg_attn = attn\n            else:\n                avg_attn.add_(attn)\n        if len(self.models) > 1:\n            avg_attn.div_(len(self.models))\n        return avg_attn", ""]}
{"filename": "xlmr/src/generator/iterative_refinement_generator.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import namedtuple\n\nimport numpy as np\nimport torch\nfrom fairseq import utils", "import torch\nfrom fairseq import utils\n\n\nDecoderOut = namedtuple(\n    \"IterativeRefinementDecoderOut\",\n    [\"output_tokens\", \"output_scores\", \"attn\", \"step\", \"max_step\", \"history\"],\n)\n\n\nclass IterativeRefinementGenerator(object):\n    def __init__(\n        self,\n        tgt_dict,\n        models=None,\n        eos_penalty=0.0,\n        max_iter=10,\n        max_ratio=2,\n        beam_size=1,\n        decoding_format=None,\n        retain_dropout=False,\n        adaptive=True,\n        retain_history=False,\n        reranking=False,\n    ):\n        \"\"\"\n        Generates translations based on iterative refinement.\n\n        Args:\n            tgt_dict: target dictionary\n            eos_penalty: if > 0.0, it penalized early-stopping in decoding\n            max_iter: maximum number of refinement iterations\n            max_ratio: generate sequences of maximum length ax, where x is the source length\n            decoding_format: decoding mode in {'unigram', 'ensemble', 'vote', 'dp', 'bs'}\n            retain_dropout: retaining dropout in the inference\n            adaptive: decoding with early stop\n        \"\"\"\n        self.bos = tgt_dict.bos()\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.eos_penalty = eos_penalty\n        self.max_iter = max_iter\n        self.max_ratio = max_ratio\n        self.beam_size = beam_size\n        self.reranking = reranking\n        self.decoding_format = decoding_format\n        self.retain_dropout = retain_dropout\n        self.retain_history = retain_history\n        self.adaptive = adaptive\n        self.models = models\n\n    def generate_batched_itr(\n        self,\n        data_itr,\n        maxlen_a=None,\n        maxlen_b=None,\n        cuda=False,\n        timer=None,\n        prefix_size=0,\n    ):\n        \"\"\"Iterate over a batched dataset and yield individual translations.\n\n        Args:\n            maxlen_a/b: generate sequences of maximum length ax + b,\n                where x is the source sentence length.\n            cuda: use GPU for generation\n            timer: StopwatchMeter for timing generations.\n        \"\"\"\n\n        for sample in data_itr:\n            if \"net_input\" not in sample:\n                continue\n            if timer is not None:\n                timer.start()\n            with torch.no_grad():\n                hypos = self.generate(\n                    self.models,\n                    sample,\n                    prefix_tokens=sample[\"target\"][:, :prefix_size]\n                    if prefix_size > 0\n                    else None,\n                )\n            if timer is not None:\n                timer.stop(sample[\"ntokens\"])\n            for i, id in enumerate(sample[\"id\"]):\n                # remove padding\n                src = utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i, :], self.pad)\n                ref = utils.strip_pad(sample[\"target\"][i, :], self.pad)\n                yield id, src, ref, hypos[i]\n\n    @torch.no_grad()\n    def generate(self, models, sample, prefix_tokens=None, constraints=None):\n        if constraints is not None:\n            raise NotImplementedError(\n                \"Constrained decoding with the IterativeRefinementGenerator is not supported\"\n            )\n\n        # TODO: iterative refinement generator does not support ensemble for now.\n        if not self.retain_dropout:\n            for model in models:\n                model.eval()\n\n        model, reranker = models[0], None\n        if self.reranking:\n            assert len(models) > 1, \"Assuming the last checkpoint is the reranker\"\n            assert (\n                self.beam_size > 1\n            ), \"Reranking requires multiple translation for each example\"\n\n            reranker = models[-1]\n            models = models[:-1]\n\n        if len(models) > 1 and hasattr(model, \"enable_ensemble\"):\n            assert model.allow_ensemble, \"{} does not support ensembling\".format(\n                model.__class__.__name__\n            )\n            model.enable_ensemble(models)\n\n        # TODO: better encoder inputs?\n        src_lengths = sample[\"net_input\"][\"src_lengths\"]\n        src_tokens = sample[\"net_input\"][\"src_tokens\"]\n        tgt_tokens = sample[\"target\"]\n        bsz, src_len = src_tokens.size()\n        \n        # initialize\n        encoder_out = model.forward_encoder(src_tokens)\n        prev_decoder_out = model.initialize_output_tokens(encoder_out, src_tokens, tgt_tokens)\n        \n        if self.beam_size > 1:\n            assert (\n                model.allow_length_beam\n            ), \"{} does not support decoding with length beam.\".format(\n                model.__class__.__name__\n            )\n\n            # regenerate data based on length-beam\n            length_beam_order = (\n                utils.new_arange(src_tokens, self.beam_size, bsz).t().reshape(-1)\n            )\n            encoder_out = model.reorder_encoder_out(\n                encoder_out, length_beam_order\n            )\n            prev_decoder_out = model.regenerate_length_beam(\n                prev_decoder_out, self.beam_size\n            )\n            bsz = bsz * self.beam_size\n\n        sent_idxs = torch.arange(bsz)\n        prev_output_tokens = prev_decoder_out.output_tokens.clone()\n\n        if self.retain_history:\n            prev_decoder_out = prev_decoder_out._replace(history=[prev_output_tokens])\n\n        finalized = [[] for _ in range(bsz)]\n\n        def is_a_loop(x, y, s, a):\n            b, l_x, l_y = x.size(0), x.size(1), y.size(1)\n            if l_x > l_y:\n                y = torch.cat([y, x.new_zeros(b, l_x - l_y).fill_(self.pad)], 1)\n                s = torch.cat([s, s.new_zeros(b, l_x - l_y)], 1)\n                if a is not None:\n                    a = torch.cat([a, a.new_zeros(b, l_x - l_y, a.size(2))], 1)\n            elif l_x < l_y:\n                x = torch.cat([x, y.new_zeros(b, l_y - l_x).fill_(self.pad)], 1)\n            return (x == y).all(1), y, s, a\n\n        def finalized_hypos(step, prev_out_token, prev_out_score, prev_out_attn):\n            cutoff = prev_out_token.ne(self.pad)\n            tokens = prev_out_token[cutoff]\n            if prev_out_score is None:\n                scores, score = None, None\n            else:\n                scores = prev_out_score[cutoff]\n                score = scores.mean()\n\n            if prev_out_attn is None:\n                hypo_attn, alignment = None, None\n            else:\n                hypo_attn = prev_out_attn[cutoff]\n                alignment = hypo_attn.max(dim=1)[1]\n            return {\n                \"steps\": step,\n                \"tokens\": tokens,\n                \"positional_scores\": scores,\n                \"score\": score,\n                \"hypo_attn\": hypo_attn,\n                \"alignment\": alignment,\n            }\n\n        for step in range(self.max_iter + 1):\n\n            decoder_options = {\n                \"eos_penalty\": self.eos_penalty,\n                \"max_ratio\": self.max_ratio,\n                \"decoding_format\": self.decoding_format,\n            }\n            prev_decoder_out = prev_decoder_out._replace(\n                step=step,\n                max_step=self.max_iter + 1,\n            )\n\n            decoder_out = model.forward_decoder(\n                prev_decoder_out, encoder_out, **decoder_options\n            )\n\n            if self.adaptive:\n                # terminate if there is a loop\n                terminated, out_tokens, out_scores, out_attn = is_a_loop(\n                    prev_output_tokens,\n                    decoder_out.output_tokens,\n                    decoder_out.output_scores,\n                    decoder_out.attn,\n                )\n                decoder_out = decoder_out._replace(\n                    output_tokens=out_tokens,\n                    output_scores=out_scores,\n                    attn=out_attn,\n                )\n\n            else:\n                terminated = decoder_out.output_tokens.new_zeros(\n                    decoder_out.output_tokens.size(0)\n                ).bool()\n\n            if step == self.max_iter:  # reach last iteration, terminate\n                terminated.fill_(1)\n\n            # collect finalized sentences\n            finalized_idxs = sent_idxs[terminated.cpu()]\n            finalized_tokens = decoder_out.output_tokens[terminated]\n            finalized_scores = decoder_out.output_scores[terminated]\n            finalized_attn = (\n                None\n                if (decoder_out.attn is None or decoder_out.attn.size(0) == 0)\n                else decoder_out.attn[terminated]\n            )\n\n            if self.retain_history:\n                finalized_history_tokens = [h[terminated] for h in decoder_out.history]\n\n            for i in range(finalized_idxs.size(0)):\n                finalized[finalized_idxs[i]] = [\n                    finalized_hypos(\n                        step,\n                        finalized_tokens[i],\n                        finalized_scores[i],\n                        None if finalized_attn is None else finalized_attn[i],\n                    )\n                ]\n\n                if self.retain_history:\n                    finalized[finalized_idxs[i]][0][\"history\"] = []\n                    for j in range(len(finalized_history_tokens)):\n                        finalized[finalized_idxs[i]][0][\"history\"].append(\n                            finalized_hypos(\n                                step, finalized_history_tokens[j][i], None, None\n                            )\n                        )\n                \n            # check if all terminated\n            if terminated.sum() == terminated.size(0):\n                break\n\n            # for next step\n            not_terminated = ~terminated\n            prev_decoder_out = decoder_out._replace(\n                output_tokens=decoder_out.output_tokens[not_terminated],\n                output_scores=decoder_out.output_scores[not_terminated],\n                attn=decoder_out.attn[not_terminated]\n                if (decoder_out.attn is not None and decoder_out.attn.size(0) > 0)\n                else None,\n                history=[h[not_terminated] for h in decoder_out.history]\n                if decoder_out.history is not None\n                else None,\n            )\n            encoder_out = model.reorder_encoder_out(\n                encoder_out, not_terminated.nonzero(as_tuple=False).squeeze()\n            )\n            sent_idxs = sent_idxs[not_terminated.cpu()]\n            prev_output_tokens = prev_decoder_out.output_tokens.clone()\n\n        if self.beam_size > 1:\n            if reranker is not None:\n                finalized = self.rerank(\n                    reranker, finalized, [src_tokens, src_lengths], self.beam_size\n                )\n\n            # aggregate information from length beam\n            finalized = [\n                finalized[\n                    np.argmax(\n                        [\n                            finalized[self.beam_size * i + j][0][\"score\"].cpu()\n                            for j in range(self.beam_size)\n                        ]\n                    )\n                    + self.beam_size * i\n                ]\n                for i in range(len(finalized) // self.beam_size)\n            ]\n\n        return finalized\n\n    def rerank(self, reranker, finalized, encoder_input, beam_size):\n        def rebuild_batch(finalized):\n            finalized_tokens = [f[0][\"tokens\"] for f in finalized]\n            finalized_maxlen = max(f.size(0) for f in finalized_tokens)\n            final_output_tokens = (\n                finalized_tokens[0]\n                .new_zeros(len(finalized_tokens), finalized_maxlen)\n                .fill_(self.pad)\n            )\n            for i, f in enumerate(finalized_tokens):\n                final_output_tokens[i, : f.size(0)] = f\n            return final_output_tokens\n\n        final_output_tokens = rebuild_batch(finalized)\n        final_output_tokens[\n            :, 0\n        ] = self.eos  # autoregressive model assumes starting with EOS\n\n        reranker_encoder_out = reranker.encoder(*encoder_input)\n        length_beam_order = (\n            utils.new_arange(\n                final_output_tokens, beam_size, reranker_encoder_out.encoder_out.size(1)\n            )\n            .t()\n            .reshape(-1)\n        )\n        reranker_encoder_out = reranker.encoder.reorder_encoder_out(\n            reranker_encoder_out, length_beam_order\n        )\n        reranking_scores = reranker.get_normalized_probs(\n            reranker.decoder(final_output_tokens[:, :-1], reranker_encoder_out),\n            True,\n            None,\n        )\n        reranking_scores = reranking_scores.gather(2, final_output_tokens[:, 1:, None])\n        reranking_masks = final_output_tokens[:, 1:].ne(self.pad)\n        reranking_scores = (\n            reranking_scores[:, :, 0].masked_fill_(~reranking_masks, 0).sum(1)\n        )\n        reranking_scores = reranking_scores / reranking_masks.sum(1).type_as(\n            reranking_scores\n        )\n\n        for i in range(len(finalized)):\n            finalized[i][0][\"score\"] = reranking_scores[i]\n\n        return finalized", "\n\nclass IterativeRefinementGenerator(object):\n    def __init__(\n        self,\n        tgt_dict,\n        models=None,\n        eos_penalty=0.0,\n        max_iter=10,\n        max_ratio=2,\n        beam_size=1,\n        decoding_format=None,\n        retain_dropout=False,\n        adaptive=True,\n        retain_history=False,\n        reranking=False,\n    ):\n        \"\"\"\n        Generates translations based on iterative refinement.\n\n        Args:\n            tgt_dict: target dictionary\n            eos_penalty: if > 0.0, it penalized early-stopping in decoding\n            max_iter: maximum number of refinement iterations\n            max_ratio: generate sequences of maximum length ax, where x is the source length\n            decoding_format: decoding mode in {'unigram', 'ensemble', 'vote', 'dp', 'bs'}\n            retain_dropout: retaining dropout in the inference\n            adaptive: decoding with early stop\n        \"\"\"\n        self.bos = tgt_dict.bos()\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.eos_penalty = eos_penalty\n        self.max_iter = max_iter\n        self.max_ratio = max_ratio\n        self.beam_size = beam_size\n        self.reranking = reranking\n        self.decoding_format = decoding_format\n        self.retain_dropout = retain_dropout\n        self.retain_history = retain_history\n        self.adaptive = adaptive\n        self.models = models\n\n    def generate_batched_itr(\n        self,\n        data_itr,\n        maxlen_a=None,\n        maxlen_b=None,\n        cuda=False,\n        timer=None,\n        prefix_size=0,\n    ):\n        \"\"\"Iterate over a batched dataset and yield individual translations.\n\n        Args:\n            maxlen_a/b: generate sequences of maximum length ax + b,\n                where x is the source sentence length.\n            cuda: use GPU for generation\n            timer: StopwatchMeter for timing generations.\n        \"\"\"\n\n        for sample in data_itr:\n            if \"net_input\" not in sample:\n                continue\n            if timer is not None:\n                timer.start()\n            with torch.no_grad():\n                hypos = self.generate(\n                    self.models,\n                    sample,\n                    prefix_tokens=sample[\"target\"][:, :prefix_size]\n                    if prefix_size > 0\n                    else None,\n                )\n            if timer is not None:\n                timer.stop(sample[\"ntokens\"])\n            for i, id in enumerate(sample[\"id\"]):\n                # remove padding\n                src = utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i, :], self.pad)\n                ref = utils.strip_pad(sample[\"target\"][i, :], self.pad)\n                yield id, src, ref, hypos[i]\n\n    @torch.no_grad()\n    def generate(self, models, sample, prefix_tokens=None, constraints=None):\n        if constraints is not None:\n            raise NotImplementedError(\n                \"Constrained decoding with the IterativeRefinementGenerator is not supported\"\n            )\n\n        # TODO: iterative refinement generator does not support ensemble for now.\n        if not self.retain_dropout:\n            for model in models:\n                model.eval()\n\n        model, reranker = models[0], None\n        if self.reranking:\n            assert len(models) > 1, \"Assuming the last checkpoint is the reranker\"\n            assert (\n                self.beam_size > 1\n            ), \"Reranking requires multiple translation for each example\"\n\n            reranker = models[-1]\n            models = models[:-1]\n\n        if len(models) > 1 and hasattr(model, \"enable_ensemble\"):\n            assert model.allow_ensemble, \"{} does not support ensembling\".format(\n                model.__class__.__name__\n            )\n            model.enable_ensemble(models)\n\n        # TODO: better encoder inputs?\n        src_lengths = sample[\"net_input\"][\"src_lengths\"]\n        src_tokens = sample[\"net_input\"][\"src_tokens\"]\n        tgt_tokens = sample[\"target\"]\n        bsz, src_len = src_tokens.size()\n        \n        # initialize\n        encoder_out = model.forward_encoder(src_tokens)\n        prev_decoder_out = model.initialize_output_tokens(encoder_out, src_tokens, tgt_tokens)\n        \n        if self.beam_size > 1:\n            assert (\n                model.allow_length_beam\n            ), \"{} does not support decoding with length beam.\".format(\n                model.__class__.__name__\n            )\n\n            # regenerate data based on length-beam\n            length_beam_order = (\n                utils.new_arange(src_tokens, self.beam_size, bsz).t().reshape(-1)\n            )\n            encoder_out = model.reorder_encoder_out(\n                encoder_out, length_beam_order\n            )\n            prev_decoder_out = model.regenerate_length_beam(\n                prev_decoder_out, self.beam_size\n            )\n            bsz = bsz * self.beam_size\n\n        sent_idxs = torch.arange(bsz)\n        prev_output_tokens = prev_decoder_out.output_tokens.clone()\n\n        if self.retain_history:\n            prev_decoder_out = prev_decoder_out._replace(history=[prev_output_tokens])\n\n        finalized = [[] for _ in range(bsz)]\n\n        def is_a_loop(x, y, s, a):\n            b, l_x, l_y = x.size(0), x.size(1), y.size(1)\n            if l_x > l_y:\n                y = torch.cat([y, x.new_zeros(b, l_x - l_y).fill_(self.pad)], 1)\n                s = torch.cat([s, s.new_zeros(b, l_x - l_y)], 1)\n                if a is not None:\n                    a = torch.cat([a, a.new_zeros(b, l_x - l_y, a.size(2))], 1)\n            elif l_x < l_y:\n                x = torch.cat([x, y.new_zeros(b, l_y - l_x).fill_(self.pad)], 1)\n            return (x == y).all(1), y, s, a\n\n        def finalized_hypos(step, prev_out_token, prev_out_score, prev_out_attn):\n            cutoff = prev_out_token.ne(self.pad)\n            tokens = prev_out_token[cutoff]\n            if prev_out_score is None:\n                scores, score = None, None\n            else:\n                scores = prev_out_score[cutoff]\n                score = scores.mean()\n\n            if prev_out_attn is None:\n                hypo_attn, alignment = None, None\n            else:\n                hypo_attn = prev_out_attn[cutoff]\n                alignment = hypo_attn.max(dim=1)[1]\n            return {\n                \"steps\": step,\n                \"tokens\": tokens,\n                \"positional_scores\": scores,\n                \"score\": score,\n                \"hypo_attn\": hypo_attn,\n                \"alignment\": alignment,\n            }\n\n        for step in range(self.max_iter + 1):\n\n            decoder_options = {\n                \"eos_penalty\": self.eos_penalty,\n                \"max_ratio\": self.max_ratio,\n                \"decoding_format\": self.decoding_format,\n            }\n            prev_decoder_out = prev_decoder_out._replace(\n                step=step,\n                max_step=self.max_iter + 1,\n            )\n\n            decoder_out = model.forward_decoder(\n                prev_decoder_out, encoder_out, **decoder_options\n            )\n\n            if self.adaptive:\n                # terminate if there is a loop\n                terminated, out_tokens, out_scores, out_attn = is_a_loop(\n                    prev_output_tokens,\n                    decoder_out.output_tokens,\n                    decoder_out.output_scores,\n                    decoder_out.attn,\n                )\n                decoder_out = decoder_out._replace(\n                    output_tokens=out_tokens,\n                    output_scores=out_scores,\n                    attn=out_attn,\n                )\n\n            else:\n                terminated = decoder_out.output_tokens.new_zeros(\n                    decoder_out.output_tokens.size(0)\n                ).bool()\n\n            if step == self.max_iter:  # reach last iteration, terminate\n                terminated.fill_(1)\n\n            # collect finalized sentences\n            finalized_idxs = sent_idxs[terminated.cpu()]\n            finalized_tokens = decoder_out.output_tokens[terminated]\n            finalized_scores = decoder_out.output_scores[terminated]\n            finalized_attn = (\n                None\n                if (decoder_out.attn is None or decoder_out.attn.size(0) == 0)\n                else decoder_out.attn[terminated]\n            )\n\n            if self.retain_history:\n                finalized_history_tokens = [h[terminated] for h in decoder_out.history]\n\n            for i in range(finalized_idxs.size(0)):\n                finalized[finalized_idxs[i]] = [\n                    finalized_hypos(\n                        step,\n                        finalized_tokens[i],\n                        finalized_scores[i],\n                        None if finalized_attn is None else finalized_attn[i],\n                    )\n                ]\n\n                if self.retain_history:\n                    finalized[finalized_idxs[i]][0][\"history\"] = []\n                    for j in range(len(finalized_history_tokens)):\n                        finalized[finalized_idxs[i]][0][\"history\"].append(\n                            finalized_hypos(\n                                step, finalized_history_tokens[j][i], None, None\n                            )\n                        )\n                \n            # check if all terminated\n            if terminated.sum() == terminated.size(0):\n                break\n\n            # for next step\n            not_terminated = ~terminated\n            prev_decoder_out = decoder_out._replace(\n                output_tokens=decoder_out.output_tokens[not_terminated],\n                output_scores=decoder_out.output_scores[not_terminated],\n                attn=decoder_out.attn[not_terminated]\n                if (decoder_out.attn is not None and decoder_out.attn.size(0) > 0)\n                else None,\n                history=[h[not_terminated] for h in decoder_out.history]\n                if decoder_out.history is not None\n                else None,\n            )\n            encoder_out = model.reorder_encoder_out(\n                encoder_out, not_terminated.nonzero(as_tuple=False).squeeze()\n            )\n            sent_idxs = sent_idxs[not_terminated.cpu()]\n            prev_output_tokens = prev_decoder_out.output_tokens.clone()\n\n        if self.beam_size > 1:\n            if reranker is not None:\n                finalized = self.rerank(\n                    reranker, finalized, [src_tokens, src_lengths], self.beam_size\n                )\n\n            # aggregate information from length beam\n            finalized = [\n                finalized[\n                    np.argmax(\n                        [\n                            finalized[self.beam_size * i + j][0][\"score\"].cpu()\n                            for j in range(self.beam_size)\n                        ]\n                    )\n                    + self.beam_size * i\n                ]\n                for i in range(len(finalized) // self.beam_size)\n            ]\n\n        return finalized\n\n    def rerank(self, reranker, finalized, encoder_input, beam_size):\n        def rebuild_batch(finalized):\n            finalized_tokens = [f[0][\"tokens\"] for f in finalized]\n            finalized_maxlen = max(f.size(0) for f in finalized_tokens)\n            final_output_tokens = (\n                finalized_tokens[0]\n                .new_zeros(len(finalized_tokens), finalized_maxlen)\n                .fill_(self.pad)\n            )\n            for i, f in enumerate(finalized_tokens):\n                final_output_tokens[i, : f.size(0)] = f\n            return final_output_tokens\n\n        final_output_tokens = rebuild_batch(finalized)\n        final_output_tokens[\n            :, 0\n        ] = self.eos  # autoregressive model assumes starting with EOS\n\n        reranker_encoder_out = reranker.encoder(*encoder_input)\n        length_beam_order = (\n            utils.new_arange(\n                final_output_tokens, beam_size, reranker_encoder_out.encoder_out.size(1)\n            )\n            .t()\n            .reshape(-1)\n        )\n        reranker_encoder_out = reranker.encoder.reorder_encoder_out(\n            reranker_encoder_out, length_beam_order\n        )\n        reranking_scores = reranker.get_normalized_probs(\n            reranker.decoder(final_output_tokens[:, :-1], reranker_encoder_out),\n            True,\n            None,\n        )\n        reranking_scores = reranking_scores.gather(2, final_output_tokens[:, 1:, None])\n        reranking_masks = final_output_tokens[:, 1:].ne(self.pad)\n        reranking_scores = (\n            reranking_scores[:, :, 0].masked_fill_(~reranking_masks, 0).sum(1)\n        )\n        reranking_scores = reranking_scores / reranking_masks.sum(1).type_as(\n            reranking_scores\n        )\n\n        for i in range(len(finalized)):\n            finalized[i][0][\"score\"] = reranking_scores[i]\n\n        return finalized", ""]}
{"filename": "xlmr/src/generator/search.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nfrom fairseq.token_generation_constraints import (\n    ConstraintState,\n    OrderedConstraintState,\n    UnorderedConstraintState,\n)\nfrom torch import Tensor\n\n\nclass Search(nn.Module):\n    def __init__(self, tgt_dict):\n        super().__init__()\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.src_lengths = torch.tensor(-1)\n        self.supports_constraints = False\n        self.stop_on_max_len = False\n\n    def step(\n        self, step, lprobs, scores, prev_output_tokens=None, original_batch_idxs=None\n    ):\n        \"\"\"Take a single search step.\n\n        Args:\n            step: the current search step, starting at 0\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n            scores: (bsz x input_beam_size x step)\n                the historical model scores of each hypothesis up to this point\n            prev_output_tokens: (bsz x step)\n                the previously generated oputput tokens\n            original_batch_idxs: (bsz)\n                the tensor with the batch indices, in the range [0, bsz)\n                this is useful in case there has been applied a re-ordering\n                and we need to know the orignal indices\n\n        Return: A tuple of (scores, indices, beams) where:\n            scores: (bsz x output_beam_size)\n                the scores of the chosen elements; output_beam_size can be\n                larger than input_beam_size, e.g., we may return\n                2*input_beam_size to account for EOS\n            indices: (bsz x output_beam_size)\n                the indices of the chosen elements\n            beams: (bsz x output_beam_size)\n                the hypothesis ids of the chosen elements, in the range [0, input_beam_size)\n        \"\"\"\n        raise NotImplementedError\n\n    @torch.jit.export\n    def set_src_lengths(self, src_lengths):\n        self.src_lengths = src_lengths\n\n    @torch.jit.export\n    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int):\n        \"\"\"Initialize constraint states for constrained decoding (if supported).\n\n        Args:\n            batch_constraints: (torch.Tensor, optional)\n                the list of constraints, in packed form\n            beam_size: (int)\n                the beam size\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"\n        pass\n\n    def prune_sentences(self, batch_idxs: Tensor):\n        \"\"\"\n        Removes constraint states for completed sentences (if supported).\n        This is called from sequence_generator._generate() when sentences are\n        deleted from the batch.\n\n        Args:\n            batch_idxs: Indices of *sentences* whose constraint state should be *kept*.\n        \"\"\"\n        pass\n\n    def update_constraints(self, active_hypos: Tensor):\n        \"\"\"\n        Updates the constraint states by selecting the beam items that are retained.\n        This is called at each time step of sequence_generator._generate() when\n        the set of 2 * {beam_size} candidate hypotheses are reduced to the beam size.\n\n        Args:\n            active_hypos: (batch size, beam size)\n              list of integers denoting, for each sentence, which beam candidate items\n              should be kept.\n        \"\"\"\n        pass", "\n\nclass Search(nn.Module):\n    def __init__(self, tgt_dict):\n        super().__init__()\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.src_lengths = torch.tensor(-1)\n        self.supports_constraints = False\n        self.stop_on_max_len = False\n\n    def step(\n        self, step, lprobs, scores, prev_output_tokens=None, original_batch_idxs=None\n    ):\n        \"\"\"Take a single search step.\n\n        Args:\n            step: the current search step, starting at 0\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n            scores: (bsz x input_beam_size x step)\n                the historical model scores of each hypothesis up to this point\n            prev_output_tokens: (bsz x step)\n                the previously generated oputput tokens\n            original_batch_idxs: (bsz)\n                the tensor with the batch indices, in the range [0, bsz)\n                this is useful in case there has been applied a re-ordering\n                and we need to know the orignal indices\n\n        Return: A tuple of (scores, indices, beams) where:\n            scores: (bsz x output_beam_size)\n                the scores of the chosen elements; output_beam_size can be\n                larger than input_beam_size, e.g., we may return\n                2*input_beam_size to account for EOS\n            indices: (bsz x output_beam_size)\n                the indices of the chosen elements\n            beams: (bsz x output_beam_size)\n                the hypothesis ids of the chosen elements, in the range [0, input_beam_size)\n        \"\"\"\n        raise NotImplementedError\n\n    @torch.jit.export\n    def set_src_lengths(self, src_lengths):\n        self.src_lengths = src_lengths\n\n    @torch.jit.export\n    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int):\n        \"\"\"Initialize constraint states for constrained decoding (if supported).\n\n        Args:\n            batch_constraints: (torch.Tensor, optional)\n                the list of constraints, in packed form\n            beam_size: (int)\n                the beam size\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"\n        pass\n\n    def prune_sentences(self, batch_idxs: Tensor):\n        \"\"\"\n        Removes constraint states for completed sentences (if supported).\n        This is called from sequence_generator._generate() when sentences are\n        deleted from the batch.\n\n        Args:\n            batch_idxs: Indices of *sentences* whose constraint state should be *kept*.\n        \"\"\"\n        pass\n\n    def update_constraints(self, active_hypos: Tensor):\n        \"\"\"\n        Updates the constraint states by selecting the beam items that are retained.\n        This is called at each time step of sequence_generator._generate() when\n        the set of 2 * {beam_size} candidate hypotheses are reduced to the beam size.\n\n        Args:\n            active_hypos: (batch size, beam size)\n              list of integers denoting, for each sentence, which beam candidate items\n              should be kept.\n        \"\"\"\n        pass", "\n\nclass BeamSearch(Search):\n    def __init__(self, tgt_dict):\n        super().__init__(tgt_dict)\n        self.constraint_states = None\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores: Optional[Tensor],\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n        else:\n            # make probs contain cumulative scores for each hypothesis\n            assert scores is not None\n            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n\n        top_prediction = torch.topk(\n            lprobs.view(bsz, -1),\n            k=min(\n                # Take the best 2 x beam_size predictions. We'll choose the first\n                # beam_size of these which don't predict eos to continue with.\n                beam_size * 2,\n                lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad\n            ),\n        )\n        scores_buf = top_prediction[0]\n        indices_buf = top_prediction[1]\n        # Project back into relative indices and beams\n        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode=\"trunc\")\n        indices_buf = indices_buf.fmod(vocab_size)\n\n        # At this point, beams_buf and indices_buf are single-dim and contain relative indices\n        return scores_buf, indices_buf, beams_buf", "\n\nclass PrefixConstrainedBeamSearch(Search):\n    def __init__(self, tgt_dict, prefix_allowed_tokens_fn):\n        super().__init__(tgt_dict)\n        self.prefix_allowed_tokens_fn = prefix_allowed_tokens_fn\n        self.stop_on_max_len = True\n\n    @torch.jit.export\n    def apply_mask(self, x, prev_output_tokens, original_batch_idxs):\n        beam_size = x.shape[0] // original_batch_idxs.shape[0]\n        original_batch_idxs = (\n            original_batch_idxs.unsqueeze(-1).repeat((1, beam_size)).flatten().tolist()\n        )\n\n        mask = torch.full_like(x, -math.inf)\n        for sent_i, (sent, batch_i) in enumerate(\n            zip(prev_output_tokens, original_batch_idxs)\n        ):\n            mask[sent_i, :, self.prefix_allowed_tokens_fn(batch_i, sent)] = 0\n\n        return mask\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs: Tensor,\n        scores: Tensor,\n        prev_output_tokens: Tensor,\n        original_batch_idxs: Tensor,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n\n        lprobs += self.apply_mask(\n            lprobs.view(bsz * beam_size, 1, vocab_size),\n            prev_output_tokens,\n            original_batch_idxs,\n        ).view(bsz, beam_size, vocab_size)\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n        else:\n            # make probs contain cumulative scores for each hypothesis\n            assert scores is not None\n            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n\n        top_prediction = torch.topk(\n            lprobs.view(bsz, -1),\n            k=min(\n                # Take the best beam_size predictions. We'll choose the first\n                # beam_size of these which don't predict eos to continue with.\n                beam_size,\n                lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad\n            ),\n        )\n        scores_buf = top_prediction[0]\n        indices_buf = top_prediction[1]\n        beams_buf = indices_buf // vocab_size\n        indices_buf = indices_buf.fmod(vocab_size)\n        return scores_buf, indices_buf, beams_buf", "\n\nclass LexicallyConstrainedBeamSearch(Search):\n    \"\"\"Implements lexically constrained beam search as described in\n\n        Fast Lexically Constrained Decoding with Dynamic Beam\n        Allocation for Neural Machine Translation.  Post & Vilar,\n        NAACL 2018.  https://www.aclweb.org/anthology/N18-1119/\n\n    and\n\n        Improved Lexically Constrained Decoding for Translation and\n        Monolingual Rewriting. Hu et al, NAACL\n        2019. https://www.aclweb.org/anthology/N19-1090/\n\n    This is accomplished by maintaining, for each beam hypothesis, a\n    ConstraintState object (see constraints.py) that tracks which\n    constraints have been generated and using this information to\n    shape the beam for each input sentence.\n    \"\"\"\n\n    def __init__(self, tgt_dict, representation):\n        super().__init__(tgt_dict)\n        self.representation = representation\n        self.vocab_size = len(tgt_dict)\n        self.num_cands = 0\n        self.supports_constraints = True\n\n    @torch.jit.export\n    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int):\n        self.constraint_states = []\n        for constraint_tensor in batch_constraints:\n            if self.representation == \"ordered\":\n                constraint_state = OrderedConstraintState.create(constraint_tensor)\n            elif self.representation == \"unordered\":\n                constraint_state = UnorderedConstraintState.create(constraint_tensor)\n\n            self.constraint_states.append([constraint_state for i in range(beam_size)])\n\n    @torch.jit.export\n    def prune_sentences(self, batch_idxs: Tensor):\n        self.constraint_states = [\n            self.constraint_states[i] for i in batch_idxs.tolist()\n        ]\n\n    @torch.jit.export\n    def update_constraints(self, active_hypos: Tensor):\n        if self.constraint_states:\n            batch_size = active_hypos.size(0)\n            for sentid in range(batch_size):\n                self.constraint_states[sentid] = [\n                    self.constraint_states[sentid][i] for i in active_hypos[sentid]\n                ]\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs: Tensor,\n        scores: Optional[Tensor],\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        \"\"\"\n        A constrained step builds a large candidates list from the following:\n        - the top 2 * {beam_size} items over the whole beam\n        - for each item in the beam\n          - the top {each_k} (default 1)\n          - all next constraints\n        We then compute the constrained state of each beam item, and assign\n        stripe codes: 0 to the best in each bank, 1 to the 2nd-best, and so\n        on. We then sort by (stripe, score), and truncate the list at\n        2 * beam size.\n\n        Args:\n            step: the decoder step\n            lprobs: (batch size, beam size, target vocab)\n                the target-vocab distributions for each item in the beam.\n        Retrun: A tuple of (scores, indices, beams, constraints) where:\n            scores: (batch, output beam size)\n                the scores of the chosen elements\n            indices: (batch, output beam size)\n                the target vocab indices of the chosen elements\n            beams: (batch, output beam size)\n                the 0-indexed hypothesis ids of the chosen elements\n            constraints: (batch, output beam size)\n                the new constraint states\n        \"\"\"\n        each_k = 1\n        device = lprobs.device\n\n        batch_size, beam_size, vocab_size = lprobs.size()\n\n        self.num_cands = min(\n            # Just take the k-best. We'll get another k from the 1-best from each\n            # row, plus more from the constraints\n            beam_size * 2,\n            lprobs.view(batch_size, -1).size(1) - 1,  # -1 so we never select pad\n        )\n\n        # STEP 0: Preliminary. Prevent EOS for unfinished hyps across all batch items\n        constraint_states = self.constraint_states\n        if constraint_states and step > 0:\n            not_finished_indices = []\n            for sentno, sent_constraints in enumerate(constraint_states):\n                for beamno, state in enumerate(sent_constraints):\n                    index = sentno * beam_size + beamno\n                    if not state.finished:\n                        not_finished_indices.append(index)\n            not_finished_indices = torch.tensor(not_finished_indices)\n            if not_finished_indices.numel() > 0:\n                lprobs.view(batch_size * beam_size, -1)[\n                    not_finished_indices, self.eos\n                ] = -math.inf\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam entry for each batch item\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n        else:\n            # make probs contain cumulative scores for each hypothesis\n            assert scores is not None\n            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n\n        top_prediction = torch.topk(\n            lprobs.view(batch_size, -1),\n            self.num_cands,\n        )\n        scores_buf, indices_buf = top_prediction\n        # Project back into relative indices and beams\n        beams_buf = indices_buf // vocab_size\n        indices_buf = indices_buf.fmod(vocab_size)\n\n        # Short circuit if there are no constraints in this batch\n        if not constraint_states:\n            return scores_buf, indices_buf, beams_buf\n\n        # STEP 1: get top-1 from each hypothesis across all sentences in the batch\n        if step > 0:\n            top_scores, top_indices = torch.topk(\n                lprobs.view(batch_size * beam_size, -1),\n                k=each_k,\n                dim=1,\n            )\n            top_scores = top_scores.view(batch_size, -1)\n            top_indices = top_indices.view(batch_size, -1)\n            scores_buf = torch.cat((scores_buf, top_scores), dim=1)\n            indices_buf = torch.cat((indices_buf, top_indices), dim=1)\n            new_beams = torch.arange(0, beam_size, device=device).repeat(batch_size, 1)\n            beams_buf = torch.cat((beams_buf, new_beams), dim=1)\n\n        # Now, process sentences in the batch one by one.\n        new_scores_buf = torch.zeros((batch_size, 2 * beam_size), device=device)\n        new_indices_buf = torch.zeros((batch_size, 2 * beam_size), device=device).long()\n        new_beams_buf = torch.zeros((batch_size, 2 * beam_size), device=device).long()\n        for sentno, states in enumerate(constraint_states):\n            scores, indices, beams, new_states = self.step_sentence(\n                step,\n                sentno,\n                lprobs[sentno],\n                constraint_states[sentno],\n                beams_buf[sentno].clone(),\n                indices_buf[sentno].clone(),\n                scores_buf[sentno].clone(),\n            )\n            new_scores_buf[sentno] = scores\n            new_indices_buf[sentno] = indices\n            new_beams_buf[sentno] = beams\n            self.constraint_states[sentno] = new_states\n\n        return new_scores_buf, new_indices_buf, new_beams_buf\n\n    @torch.jit.export\n    def step_sentence(\n        self,\n        step: int,\n        sentno: int,\n        lprobs: Tensor,\n        constraint_states: List[List[ConstraintState]],\n        beams_buf: Tensor,\n        indices_buf: Tensor,\n        scores_buf: Tensor,\n    ):\n        \"\"\"Does per-sentence processing. Adds all constraints for each\n        hypothesis to the list of candidates; then removes duplicates,\n        sorts, and dynamically stripes across the banks. All tensor inputs\n        are collapsed to those pertaining to a single input sentence.\n        \"\"\"\n        device = lprobs.device\n\n        # STEP 2: Add all constraints for each beam item\n        for beamno, state in enumerate(constraint_states):\n            next_tokens = torch.tensor(list(state.next_tokens()), device=device).long()\n            if next_tokens.numel() != 0:\n                indices_buf = torch.cat((indices_buf, next_tokens))\n                next_beams = (\n                    torch.tensor(beamno, device=device)\n                    .repeat(next_tokens.size(0))\n                    .long()\n                )\n                beams_buf = torch.cat((beams_buf, next_beams))\n                next_values = lprobs[beamno].take(next_tokens.view(-1))\n                scores_buf = torch.cat((scores_buf, next_values))\n\n            # At the 0th time step, there is just one beam item\n            if step == 0:\n                break\n\n        # STEP 3: Compute the \"bank\" for each candidate. This is the\n        # number of constraints it's generated. We need this so that\n        # we can do round-robin allocation of the beam across these\n        # banks. If C is the number of constraints, we select the best\n        # item in bank C, then the best in bank C-1, etc, followed by\n        # the 2nd-best in bank C, the 2nd-best in bank C-1, etc, and so\n        # on, until the maximum beam size. We accomplish this by\n        # creating a sort key and striping across the banks.\n\n        # Compute the new states for all candidates\n        cands_size = indices_buf.size(0)\n        constraint_states = [\n            constraint_states[beams_buf[i]].advance(indices_buf[i])\n            for i in range(cands_size)\n        ]\n\n        banks = torch.tensor([state.bank for state in constraint_states], device=device)\n\n        # STEP 4: Sort\n        num_constraint_tokens = len(state.tokens)\n\n        # Sort by keys (bank, score) (i.e., sort banks together, and scores\n        # within banks). AFAIK pytorch doesn't support either stable sort or\n        # multi-key sorting, so we have to hack this.\n        MAX_SCORE = -100\n        sort_key = (num_constraint_tokens - banks) * MAX_SCORE + scores_buf\n        sort_values, sort_indices = sort_key.sort(dim=0, descending=True)\n        scores_buf = scores_buf[sort_indices]\n        indices_buf = indices_buf[sort_indices]\n        beams_buf = beams_buf[sort_indices]\n        banks = banks[sort_indices]\n\n        # Sort the constraints to follow suit\n        constraint_states = [constraint_states[i] for i in sort_indices]\n\n        # STEP 5: Remove duplicates. The topk calls (overall and\n        # per-row) plus the per-row generation of constraints will\n        # produce duplicates. Here we remove them.\n\n        def roll(t):\n            \"\"\"Rolls a 1d tensor left by 1.\n\n            [0, 1, 2, 3, 4] becomes [4, 0, 1, 2, 3]\n            \"\"\"\n            return torch.cat((t[-1].unsqueeze(0), t[0:-1]), dim=0)\n\n        # We map candidates (beam, token_id) to a single dimension.\n        # This is then shifted by 1. We can then easily identify\n        # duplicates and create a mask that identifies unique\n        # extensions.\n        uniques_mask = beams_buf * (self.vocab_size + 1) + indices_buf\n        uniques_mask = roll(uniques_mask) != uniques_mask\n\n        # Use the mask to pare down the data structures\n        scores_buf = torch.masked_select(scores_buf, uniques_mask)\n        indices_buf = torch.masked_select(indices_buf, uniques_mask)\n        beams_buf = torch.masked_select(beams_buf, uniques_mask)\n        banks = torch.masked_select(banks, uniques_mask)\n        i = 1\n        for mask in uniques_mask[1:]:\n            if not mask:\n                constraint_states.pop(i)\n            i += mask\n\n        # STEP 6: Assign IDs round-robin across banks, sort, and\n        # truncate. Now that the candidates are sorted by (bank,\n        # score) and uniqed, we dynamically allocate the {beam_size}\n        # beam by striping across the candidates. These stripes will\n        # be used as sort keys to do round-robin selection. This is\n        # accomplished in a single pass with offsets. Sorting by\n        # highest-banks (furthest-along hypotheses) first ensures\n        # progress through the constraints.\n        #\n        # e.g., BANKS: 3 3 3 2 2 2 2 1 1 1 0 0\n        # OLD STRIPES: 0 1 2 0 1 2 3 0 1 2 0 1\n        # NEW STRIPES: 0 1+4 2+8 0+1 1+5 2+9 3+11 0+2 1+6 2+10 0+3 1+7\n        #            = 0 5 10 1 6 11 13 2 7 12 3 8\n        #\n        # Sorting by this then gives the following banks:\n        #\n        #             3 2 1 0 3 2 1 0 3 2 1 2\n        #\n        # We'll take the top {beam_size} of these.\n        stripe_offsets = [offset * (len(banks) + 1) for offset in range(len(banks) + 1)]\n        stripes = torch.zeros_like(banks)\n        cur_bank_count = -1\n        cur_bank = banks[0]\n        for i, bank in enumerate(banks):\n            if bank != cur_bank:\n                cur_bank_count = 0\n                cur_bank = bank\n            else:\n                cur_bank_count += 1\n            stripes[i] = num_constraint_tokens - bank + stripe_offsets[cur_bank_count]\n\n        # STEP 7: Sort by the stripes values\n        sort_values, sort_indices = stripes.sort(dim=0)\n        scores_buf = scores_buf[sort_indices]\n        indices_buf = indices_buf[sort_indices]\n        beams_buf = beams_buf[sort_indices]\n        constraint_states = [constraint_states[i] for i in sort_indices]\n\n        # STEP 8: Truncate to the candidates size!\n        scores_buf = scores_buf[: self.num_cands]\n        indices_buf = indices_buf[: self.num_cands]\n        beams_buf = beams_buf[: self.num_cands]\n\n        return scores_buf, indices_buf, beams_buf, constraint_states", "\n\nclass LengthConstrainedBeamSearch(Search):\n    def __init__(self, tgt_dict, min_len_a, min_len_b, max_len_a, max_len_b):\n        super().__init__(tgt_dict)\n        self.min_len_a = min_len_a\n        self.min_len_b = min_len_b\n        self.max_len_a = max_len_a\n        self.max_len_b = max_len_b\n        self.beam = BeamSearch(tgt_dict)\n        self.needs_src_lengths = True\n\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores,\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        min_lens = self.min_len_a * self.src_lengths + self.min_len_b\n        max_lens = self.max_len_a * self.src_lengths + self.max_len_b\n        lprobs[step < min_lens, :, self.eos] = -math.inf\n        lprobs[step >= max_lens, :, self.eos] = 0\n        return self.beam.step(step, lprobs, scores)", "\n\nclass DiverseBeamSearch(Search):\n    \"\"\"Diverse Beam Search.\n\n    See \"Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence\n    Models\" for details.\n\n    We only implement the Hamming Diversity penalty here, which performed best\n    in the original paper.\n    \"\"\"\n\n    def __init__(self, tgt_dict, num_groups, diversity_strength):\n        super().__init__(tgt_dict)\n        self.num_groups = num_groups\n        self.diversity_strength = -diversity_strength\n        self.beam = BeamSearch(tgt_dict)\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores,\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n        if beam_size % self.num_groups != 0:\n            raise ValueError(\n                \"DiverseBeamSearch requires --beam to be divisible by the number of groups\"\n            )\n\n        # initialize diversity penalty\n        diversity_buf = torch.zeros(lprobs[:, 0, :].size()).to(lprobs)\n\n        scores_G, indices_G, beams_G = [], [], []\n        for g in range(self.num_groups):\n            lprobs_g = lprobs[:, g :: self.num_groups, :]\n            scores_g = scores[:, g :: self.num_groups, :] if step > 0 else None\n\n            # apply diversity penalty\n            if g > 0:\n                lprobs_g = torch.add(\n                    lprobs_g,\n                    other=diversity_buf.unsqueeze(1),\n                    alpha=self.diversity_strength,\n                )\n            else:\n                lprobs_g = lprobs_g.contiguous()\n\n            scores_buf, indices_buf, beams_buf = self.beam.step(\n                step, lprobs_g, scores_g\n            )\n            beams_buf.mul_(self.num_groups).add_(g)\n\n            scores_G.append(scores_buf.clone())\n            indices_G.append(indices_buf.clone())\n            beams_G.append(beams_buf.clone())\n\n            # update diversity penalty\n            diversity_buf.scatter_add_(\n                1, indices_buf, torch.ones(indices_buf.size()).to(diversity_buf)\n            )\n\n        # interleave results from different groups\n        scores_buf = torch.stack(scores_G, dim=2).view(bsz, -1)\n        indices_buf = torch.stack(indices_G, dim=2).view(bsz, -1)\n        beams_buf = torch.stack(beams_G, dim=2).view(bsz, -1)\n        return scores_buf, indices_buf, beams_buf", "\n\nclass Sampling(Search):\n    sampling_topk: int\n    sampling_topp: float\n\n    def __init__(self, tgt_dict, sampling_topk=-1, sampling_topp=-1.0):\n        super().__init__(tgt_dict)\n        self.sampling_topk = sampling_topk\n        self.sampling_topp = sampling_topp\n\n    def _sample_topp(self, lprobs):\n        \"\"\"Sample among the smallest set of elements whose cumulative probability mass exceeds p.\n\n        See `\"The Curious Case of Neural Text Degeneration\"\n        (Holtzman et al., 2019) <https://arxiv.org/abs/1904.09751>`_.\n\n        Args:\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n\n        Return: A tuple of (trimed_probs, truncated_indices) where:\n            trimed_probs: (bsz x input_beam_size x ?)\n                the model's probabilities over the elements selected to sample from. The\n                width of the third dimension is determined by top-P.\n            truncated_indices: (bsz x input_beam_size x ?)\n                the indices of the chosen elements.\n        \"\"\"\n        probs = lprobs.exp_()\n\n        # sort the last dimension (vocab dimension) in descending order\n        sorted_probs, sorted_indices = probs.sort(descending=True)\n\n        # compute a mask to indicate the words to be included in the top-P set.\n        cumsum_probs = sorted_probs.cumsum(dim=2)\n        mask = cumsum_probs.lt(self.sampling_topp)\n\n        # note that mask was computed by 'lt'. One more word needs to be included\n        # so that the cumulative probability mass can exceed p.\n        cumsum_mask = mask.cumsum(dim=2)\n        last_included = cumsum_mask[:, :, -1:]\n        last_included.clamp_(0, mask.size()[2] - 1)\n        mask = mask.scatter_(2, last_included, 1)\n\n        # truncate unnecessary dims.\n        max_dim = last_included.max()\n        truncated_mask = mask[:, :, : max_dim + 1]\n        truncated_probs = sorted_probs[:, :, : max_dim + 1]\n        truncated_indices = sorted_indices[:, :, : max_dim + 1]\n\n        # trim the words that are not in top-P by setting their probabilities\n        # to 0, so that they would not be sampled later.\n        trim_mask = ~truncated_mask\n        trimed_probs = truncated_probs.masked_fill_(trim_mask, 0)\n        return trimed_probs, truncated_indices\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores,\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n\n        if self.sampling_topp > 0:\n            # only sample from the smallest set of words whose cumulative probability mass exceeds p\n            probs, top_indices = self._sample_topp(lprobs)\n        elif self.sampling_topk > 0:\n            # only sample from top-k candidates\n            lprobs, top_indices = lprobs.topk(self.sampling_topk)\n            probs = lprobs.exp_()\n        else:\n            probs = lprobs.exp_()\n\n            # dummy data to be consistent with true branch for type check\n            top_indices = torch.empty(0).to(probs)\n        # sample\n        if step == 0:\n            indices_buf = torch.multinomial(\n                probs.view(bsz, -1),\n                beam_size,\n                replacement=True,\n            ).view(bsz, beam_size)\n        else:\n            indices_buf = torch.multinomial(\n                probs.view(bsz * beam_size, -1),\n                1,\n                replacement=True,\n            ).view(bsz, beam_size)\n\n        if step == 0:\n            # expand to beam size\n            probs = probs.expand(bsz, beam_size, -1)\n\n        # gather scores\n        scores_buf = torch.gather(probs, dim=2, index=indices_buf.unsqueeze(-1))\n        scores_buf = scores_buf.log_().view(bsz, -1)\n\n        # remap indices if using top-k or top-P sampling\n        if self.sampling_topk > 0 or self.sampling_topp > 0:\n            indices_buf = torch.gather(\n                top_indices.expand(bsz, beam_size, -1),\n                dim=2,\n                index=indices_buf.unsqueeze(-1),\n            ).squeeze(2)\n\n        if step == 0:\n            beams_buf = indices_buf.new_zeros(bsz, beam_size)\n        else:\n            beams_buf = torch.arange(0, beam_size).to(indices_buf).repeat(bsz, 1)\n            # make scores cumulative\n            scores_buf.add_(\n                torch.gather(scores[:, :, step - 1], dim=1, index=beams_buf)\n            )\n\n        return scores_buf, indices_buf, beams_buf", "\n\nclass DiverseSiblingsSearch(Search):\n    \"\"\"\n    Beam search with diverse siblings.\n\n    See \"A Simple, Fast Diverse Decoding Algorithm for Neural Generation\" for details.\n    https://arxiv.org/abs/1611.08562\n\n    1/ Calculate hypotheses for each beam\n    2/ Intra-sibling ordering\n    3/ Rewrite scores\n    4/ Choose top K hypotheses\n\n    if diversity_rate == 0 is equivalent to BeamSearch\n    \"\"\"\n\n    def __init__(self, tgt_dict, diversity_rate):\n        super().__init__(tgt_dict)\n        self.diversity_rate = diversity_rate\n        self.beam = BeamSearch(tgt_dict)\n\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores,\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n        k = min(\n            # Take the best 2 x beam_size predictions. We'll choose the first\n            # beam_size of these which don't predict eos to continue with.\n            beam_size * 2,\n            lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad\n        )\n        s_list: List[Tensor]\n        i_list: List[Tensor]\n        s_list = [torch.empty(0).to(lprobs) for i in range(beam_size)]\n        i_list = [torch.LongTensor().to(device=lprobs.device) for i in range(beam_size)]\n        sibling_score = torch.arange(1, k + 1).to(lprobs) * self.diversity_rate\n\n        if step == 0:\n            return self.beam.step(step, lprobs, scores)\n        lprobs.add_(scores[:, :, step - 1].unsqueeze(-1))\n\n        # 1/ Calculate hypotheses for each beam\n        for i in range(beam_size):\n            torch.topk(lprobs[:, i, :].view(bsz, -1), k, out=(s_list[i], i_list[i]))\n            i_list[i].fmod_(vocab_size)\n\n            # 2/ Intra-sibling ordering by default from topk + 3/ Rewrite scores\n            s_list[i].sub_(sibling_score)\n\n        # 4/ Choose top K hypotheses\n        indices = torch.stack(i_list, dim=1).view(bsz, -1)\n\n        final_scores = torch.empty(0).to(lprobs)\n        final_indices = torch.LongTensor().to(device=lprobs.device)\n        final_beams = torch.LongTensor().to(device=lprobs.device)\n        (final_scores, final_indices) = torch.topk(\n            torch.stack(s_list, dim=1).view(bsz, -1),\n            k,\n        )\n\n        final_beams = final_indices // k\n\n        for i in range(bsz):\n            final_indices[i] = indices[i][final_indices[i]]\n\n        return final_scores, final_indices, final_beams", ""]}
{"filename": "xlmr/src/model/hub_interface.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nimport logging\nfrom typing import Dict, List\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.data import encoders\nfrom fairseq.hub_utils import GeneratorHubInterface\nfrom omegaconf import open_dict\n", "from omegaconf import open_dict\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass XLMRHubInterface(GeneratorHubInterface):\n\n    def __init__(self, cfg, task, model):\n        super().__init__(cfg, task, [model])\n        self.model = self.models[0]\n\n    def encode(\n        self, sentence: str, *addl_sentences, no_separator=True\n    ) -> torch.LongTensor:\n        bpe_sentence = \"<s> \" + self.bpe.encode(sentence)\n        tokens = self.task.target_dictionary.encode_line(bpe_sentence, append_eos=False)\n        return tokens.long()\n        \n    def decode(self, tokens: torch.LongTensor):\n        tokens = tokens.cpu().numpy()\n        sentences = [self.bpe.decode(self.task.target_dictionary.string(tokens))]\n        return sentences\n        \n    def sample(\n        self, sentences: List[str], **kwargs\n    ) -> List[str]:\n        tokenized_sentences = [self.encode(sentence) for sentence in sentences]\n        batched_hypos = self.generate(tokenized_sentences, **kwargs)\n        return [self.decode(hypos[0][\"tokens\"]) for hypos in batched_hypos]\n\n    def generate(\n        self,\n        tokenized_sentences: List[torch.LongTensor],\n        **kwargs\n    ) -> List[List[Dict[str, torch.Tensor]]]:\n    \n        generator = self.task.build_generator(\n            self.models,\n            **kwargs,\n        )\n\n        results = []\n        for batch in self._build_batches(tokenized_sentences, skip_invalid_size_inputs=False):\n            batch = utils.apply_to_sample(lambda t: t.to(self.device), batch)\n            translations = self.task.inference_step(\n                generator, self.models, batch,\n            )\n            for id, hypos in zip(batch[\"id\"].tolist(), translations):\n                results.append((id, hypos))\n\n        # sort output to match input order\n        outputs = [hypos for _, hypos in sorted(results, key=lambda x: x[0])]\n        return outputs", ""]}
{"filename": "xlmr/src/model/xlmr_megatron.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, List, Optional, Tuple\nimport os\nimport math\nimport logging\n", "import logging\n\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom torch.nn import Linear\nfrom fairscale.nn.model_parallel import initialize as mpu\nfrom fairscale.nn.model_parallel.initialize import initialize_model_parallel\nfrom fairscale.nn.model_parallel.mappings import (", "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\nfrom fairscale.nn.model_parallel.mappings import (\n    scatter_to_model_parallel_region, \n    gather_from_model_parallel_region, \n    copy_to_model_parallel_region\n) \nimport fairscale.nn.model_parallel.initialize as fs_init\nfrom fairscale.nn.model_parallel.layers import (\n    ParallelEmbedding,\n    RowParallelLinear,", "    ParallelEmbedding,\n    RowParallelLinear,\n    ColumnParallelLinear,\n    VocabParallelEmbedding,\n)\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\nfrom fairseq.modules.checkpoint_activations import checkpoint_wrapper\nfrom fsdp.fully_sharded_data_parallel import fsdp_enable_wrap, fsdp_wrap\nfrom fairseq.modules import (\n    LayerNorm,", "from fairseq.modules import (\n    LayerNorm,\n    PositionalEmbedding,\n)\nlogger = logging.getLogger(__name__)\n\n\ndef _mean_pooling(enc_feats, src_masks):\n    # enc_feats: T x B x C\n    # src_masks: B x T or None\n    if src_masks is None:\n        enc_feats = enc_feats.mean(0)\n    else:\n        src_masks = (~src_masks).transpose(0, 1).type_as(enc_feats)\n        enc_feats = (\n            (enc_feats / src_masks.sum(0)[None, :, None]) * src_masks[:, :, None]\n        ).sum(0)\n    return enc_feats", "\nclass XLMRMegatron(nn.Module):\n\n    def __init__(self, cfg, src_dict, tgt_dict, embed_tokens):\n        super().__init__()\n        \n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n\n        self.embed_tokens = embed_tokens\n        self.embed_dim = cfg.decoder_embed_dim\n        self.num_layers = cfg.decoder_layers\n        self.normalize_before = cfg.decoder_normalize_before\n\n        self.pad = self.tgt_dict.pad()\n        self.bos = self.tgt_dict.bos()\n        self.eos = self.tgt_dict.eos()\n        self.unk = self.tgt_dict.unk()\n        \n        self.embed_positions = (\n            PositionalEmbedding(512,\n                self.embed_dim,\n                padding_idx=self.pad,\n                learned=True,\n            )\n        )\n\n        self.layers = torch.nn.ModuleList()\n        self.layers.extend(\n            [\n                self.build_decoder_layer(cfg)\n                for _ in range(self.num_layers)\n            ]\n        )\n        self.layer_norm = LayerNorm(self.embed_dim)\n\n        self.lm_head = XLMRHead(\n            embed_dim=self.embed_dim,\n            output_dim=len(self.tgt_dict),\n            weight=self.embed_tokens.weight,\n        )\n\n        def _vocab_init(tensor, **kwargs):\n            nn.init.normal_(tensor, mean=0.0, std=0.02)\n        self.embed_length = VocabParallelEmbedding(512, self.embed_dim, init_method=_vocab_init)\n        self.dropout_module = FairseqDropout(0.1)\n\n    def forward_length(self, enc_feats, src_masks):\n        enc_feats = _mean_pooling(enc_feats.transpose(0, 1), src_masks)\n        enc_feats = copy_to_model_parallel_region(enc_feats)\n        length_out = F.linear(enc_feats, self.embed_length.weight)\n        length_out = gather_from_model_parallel_region(length_out).contiguous()\n        return F.log_softmax(length_out, -1)\n    \n    def forward_length_prediction(self, length_out, tgt_tokens=None):\n        if tgt_tokens is not None:\n            # obtain the length target\n            tgt_lengs = tgt_tokens.ne(self.pad).sum(1).long()\n            length_tgt = tgt_lengs\n            length_tgt = length_tgt.clamp(min=0, max=512)\n        else:\n            length_out = gather_from_model_parallel_region(length_out).contiguous()\n            pred_lengs = length_out.max(-1)[1]\n            length_tgt = pred_lengs\n        return length_tgt\n\n    def build_decoder_layer(self, cfg):\n        layer = XLMRTransformerLayer(cfg)\n        checkpoint = cfg.checkpoint_activations\n        if checkpoint:\n            offload_to_cpu = cfg.offload_activations\n            layer = checkpoint_wrapper(layer, offload_to_cpu=offload_to_cpu)\n        min_params_to_wrap = cfg.min_params_to_wrap if not checkpoint else 0\n        layer = fsdp_wrap(layer, min_num_params=min_params_to_wrap)\n        return layer\n\n    def output_layer(self, x):\n        return self.lm_head(x).float()\n\n    def forward(self, source, target):\n        \n        src_embed = self.embed_tokens(source)\n        src_x = src_embed + self.embed_positions(source)\n\n        tgt_embed = self.embed_tokens(target)\n        tgt_x = tgt_embed + self.embed_positions(target)\n\n        if not self.normalize_before:\n            src_x = self.layer_norm(src_x)\n            tgt_x = self.layer_norm(tgt_x)\n\n        src_x = self.dropout_module(src_x)\n        tgt_x = self.dropout_module(tgt_x)\n        src_key_padding_mask = source.eq(self.pad)\n        tgt_key_padding_mask = target.eq(self.pad)\n        \n        hidden_state = [src_x]\n\n        tgt_start_idx = src_x.size(1)\n        x = torch.cat([src_x, tgt_x], dim=1)\n        key_padding_mask = torch.cat([src_key_padding_mask, tgt_key_padding_mask], dim=1)\n\n        for i, layer in enumerate(self.layers):\n            x = layer(\n                x,\n                key_padding_mask,\n            )\n            hidden_state.append(x[:,:tgt_start_idx,:])\n\n        if self.normalize_before:\n            src_x = self.layer_norm(x[:,:tgt_start_idx,:])\n            tgt_x = self.layer_norm(x[:,tgt_start_idx:,:])\n        \n        return src_x, tgt_x, src_key_padding_mask, tgt_key_padding_mask, hidden_state\n\n    def forward_enc(self, tokens):\n        \n        embed = self.embed_tokens(tokens)\n        x = embed + self.embed_positions(tokens)\n\n        if not self.normalize_before:\n            x = self.layer_norm(x)\n\n        x = self.dropout_module(x)\n        key_padding_mask = tokens.eq(self.pad)\n        \n        hidden_state = [x]\n        for i, layer in enumerate(self.layers):\n            \n            x = layer(\n                x,\n                key_padding_mask,\n            )\n            hidden_state.append(x)\n\n        if self.normalize_before:\n            x = self.layer_norm(x)\n        \n        return x, key_padding_mask, hidden_state\n\n    def forward_dec(self, encoder_out, tokens):\n        \n        embed = self.embed_tokens(tokens)\n        x = embed + self.embed_positions(tokens)\n\n        if not self.normalize_before:\n            x = self.layer_norm(x)\n\n        x = self.dropout_module(x)\n        tgt_key_padding_mask = tokens.eq(self.pad)\n        \n        hidden_state = [x]\n        tgt_start_idx = encoder_out[\"encoder_padding_mask\"][0].size(1)\n        key_padding_mask = torch.cat([encoder_out[\"encoder_padding_mask\"][0], tgt_key_padding_mask], dim=1)\n        for i, layer in enumerate(self.layers):\n            \n            x_concat = torch.cat([encoder_out[\"encoder_states\"][i], x], dim=1)\n            \n            x = layer(\n                x_concat,\n                key_padding_mask,\n            )[:, tgt_start_idx:, :]\n            hidden_state.append(x[:, tgt_start_idx:, :])\n\n        if self.normalize_before:\n            x = self.layer_norm(x)\n        \n        return x, key_padding_mask, hidden_state", "    \n\nclass XLMRHead(nn.Module):\n    \"\"\"Head for masked language modeling.\"\"\"\n\n    def __init__(self, embed_dim, output_dim, weight=None):\n        super().__init__()\n        self.dense = ColumnParallelLinear(\n            embed_dim,\n            embed_dim,\n            bias=True,\n            gather_output=True,\n            init_method=lambda x: x,\n        )\n        self.activation_fn = utils.get_activation_fn(\"gelu\")\n        self.layer_norm = LayerNorm(embed_dim)\n        \n        if weight is None:\n            weight = nn.Linear(embed_dim, output_dim, bias=False).weight\n        self.weight = weight\n        self.bias = nn.Parameter(torch.zeros(output_dim))\n\n    def forward(self, features):\n        x = self.dense(features)\n        x = self.activation_fn(x)\n        x = self.layer_norm(x)\n\n        x = copy_to_model_parallel_region(x)\n        x = F.linear(x, self.weight)\n        x = gather_from_model_parallel_region(x).contiguous()\n        x = x + self.bias\n        return x", "\n\nclass XLMRTransformerLayer(nn.Module):\n\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embed_dim = cfg.decoder_embed_dim\n        self.num_heads = cfg.decoder_attention_heads\n        self.ffn_embed_dim = cfg.decoder_ffn_embed_dim\n\n        self.self_attn = XLMRAttention(self.num_heads, self.embed_dim)\n\n        self.activation_fn = utils.get_activation_fn(\"gelu\")\n        self.fc1 = ColumnParallelLinear(self.embed_dim, self.ffn_embed_dim, gather_output=False, init_method=lambda x: x)\n        self.fc2 = RowParallelLinear(self.ffn_embed_dim, self.embed_dim, input_is_parallel=True, init_method=lambda x: x)\n        \n        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n        \n        self.normalize_before = cfg.decoder_normalize_before\n        self.dropout_module = FairseqDropout(0.1)\n        \n    def forward(\n        self,\n        x: Tensor,\n        key_padding_mask: Optional[Tensor],\n    ):\n        \n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        x = residual + self.dropout_module(self.self_attn(x, key_padding_mask))\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n        \n        x = residual + self.dropout_module(self.fc2(self.activation_fn(self.fc1(x))))\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        return x", "\n\nclass XLMRAttention(nn.Module):\n\n    def __init__(self, num_heads, embed_dim):\n        super().__init__()\n\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.head_dim = embed_dim // num_heads\n        self.local_num_heads = self.num_heads // fs_init.get_model_parallel_world_size()\n        \n        self.scaling = self.head_dim**-0.5\n\n        self.q_proj = ColumnParallelLinear(\n            self.embed_dim,\n            self.embed_dim,\n            bias=True,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.k_proj = ColumnParallelLinear(\n            self.embed_dim,\n            self.embed_dim,\n            bias=True,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.v_proj = ColumnParallelLinear(\n            self.embed_dim,\n            self.embed_dim,\n            bias=True,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.out_proj = RowParallelLinear(\n            self.embed_dim,\n            self.embed_dim,\n            bias=True,\n            input_is_parallel=True,\n            init_method=lambda x: x,\n        )\n        self.dropout_module = FairseqDropout(0.1)\n\n    def forward(self, query, key_padding_mask):\n        \n        bsz, src_len, embed_dim = query.size()\n        q = self.q_proj(query)\n        k = self.k_proj(query)\n        v = self.v_proj(query)\n        q *= self.scaling\n\n        q = q.view(bsz, src_len, self.local_num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(bsz, src_len, self.local_num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(bsz, src_len, self.local_num_heads, self.head_dim).transpose(1, 2)\n        attn_scores = torch.matmul(q, k.transpose(2, 3))\n\n        if key_padding_mask is not None:\n            attn_scores = attn_scores.masked_fill(\n                key_padding_mask.unsqueeze(1).unsqueeze(2),\n                float(\"-inf\")\n            )\n        attn_softmax_scores = F.softmax(attn_scores.float(), dim=-1).type_as(q)\n        output = torch.matmul(self.dropout_module(attn_softmax_scores), v)\n        output = output.transpose(1, 2).contiguous().view(bsz, src_len, -1)\n        return self.out_proj(output)", "\n"]}
{"filename": "xlmr/src/model/xlmr_model.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Tuple\nimport os\nfrom omegaconf import II", "import os\nfrom omegaconf import II\nimport math\nimport logging\n\nimport torch \nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom fairseq import options, utils\nfrom fairseq.dataclass import ChoiceEnum, FairseqDataclass", "from fairseq import options, utils\nfrom fairseq.dataclass import ChoiceEnum, FairseqDataclass\nfrom fairseq.models import (\n    BaseFairseqModel,\n    register_model,\n    register_model_architecture,\n)\nfrom fairseq.models.transformer import DEFAULT_MIN_PARAMS_TO_WRAP\nfrom .hub_interface import XLMRHubInterface\nfrom .xlmr_transformer import XLMRTransformer", "from .hub_interface import XLMRHubInterface\nfrom .xlmr_transformer import XLMRTransformer\nfrom .xlmr_megatron import XLMRMegatron\nfrom fairseq.utils import safe_getattr, safe_hasattr\nfrom fairseq.utils import new_arange\nfrom generator.iterative_refinement_generator import DecoderOut\nfrom fairscale.nn.model_parallel import initialize as mpu\nfrom fairscale.nn.model_parallel.layers import ParallelEmbedding, VocabParallelEmbedding\nfrom sentencepiece import SentencePieceProcessor\n", "from sentencepiece import SentencePieceProcessor\n\nlogger = logging.getLogger(__name__)\n\n\ndef _skeptical_unmasking(output_scores, output_masks, p):\n    sorted_index = output_scores.sort(-1)[1]\n    boundary_len = (\n        (output_masks.sum(1, keepdim=True).type_as(output_scores) - 2) * p\n    ).long()\n    skeptical_mask = new_arange(output_masks) < boundary_len\n    return skeptical_mask.scatter(1, sorted_index, skeptical_mask)", "\n\n@dataclass\nclass XLMRConfig(FairseqDataclass):\n\n    dropout: float = field(default=0.1, metadata={\"help\": \"dropout probability\"})\n    attention_dropout: float = field(\n        default=0.0, metadata={\"help\": \"dropout probability for attention weights\"}\n    )\n\n    decoder_embed_dim: int = field(\n        default=4096, metadata={\"help\": \"decoder embedding dimension\"}\n    )\n    decoder_ffn_embed_dim: int = field(\n        default=16384, metadata={\"help\": \"decoder embedding dimension for FFN\"}\n    )\n    decoder_layers: int = field(default=48, metadata={\"help\": \"num decoder layers\"})\n    decoder_attention_heads: int = field(\n        default=32, metadata={\"help\": \"num decoder attention heads\"}\n    )\n    decoder_normalize_before: bool = field(\n        default=True, metadata={\"help\": \"norm before\"}\n    )\n    \n    max_target_positions: Optional[int] = II(\"task.max_target_positions\")\n    checkpoint_activations: bool = field(\n        default=False, metadata={\"help\": \"checkpoint activations at each layer\"}\n    )\n    offload_activations: bool = field(\n        default=False,\n        metadata={\"help\": \"move checkpointed activations to CPU after they are used.\"},\n    )\n    min_params_to_wrap: int = field(\n        default=DEFAULT_MIN_PARAMS_TO_WRAP,\n        metadata={\n            \"help\": (\"minimum number of params for a layer to be wrapped with FSDP()\")\n        },\n    )", "\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim**-0.5)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\n@register_model(\"nar_xlmr\", dataclass=XLMRConfig)\nclass NARXLMR(BaseFairseqModel):\n    \n    def __init__(self, decoder):\n        super().__init__()\n        self.decoder = decoder\n        self.mask_idx = self.decoder.tgt_dict.index('<mask>')\n    \n    @classmethod\n    def build_model(cls, args, task):\n        \"\"\"Build a new model instance.\"\"\"\n        xlmr_base(args)\n        \n        logger.info(\"rescale [src] dictionary: {} types and [tgt] dictionary: {} types\".format(\n            len(task.source_dictionary), len(task.target_dictionary)))\n        \n        task.megatron_model = False\n        if safe_getattr(task, \"megatron_model\", False):\n            cls.initialize_model_parallel()\n            \n            task.source_dictionary.pad_to_multiple_(torch.distributed.get_world_size() * 8)\n            task.target_dictionary.pad_to_multiple_(torch.distributed.get_world_size() * 8)\n            \n            embed_tokens = cls.build_megatron_embedding(args, task.target_dictionary, args.decoder_embed_dim)\n            decoder = XLMRMegatron(\n                args,\n                task.source_dictionary,\n                task.target_dictionary,\n                embed_tokens\n            )\n        else:\n\n            embed_tokens = cls.build_embedding(args, task.source_dictionary, args.decoder_embed_dim)\n            decoder = XLMRTransformer(\n                args,\n                task.source_dictionary,\n                task.target_dictionary,\n                embed_tokens\n            )\n        \n        return cls(decoder)\n        \n    @classmethod\n    def initialize_model_parallel(cls):\n        logger.info(\"llama model init process group\")\n\n        if not torch.distributed.is_initialized():\n            torch.distributed.init_process_group(\"nccl\")\n\n        if not mpu.model_parallel_is_initialized():\n            ws = torch.distributed.get_world_size()\n            mpu.initialize_model_parallel(ws)\n\n    @classmethod\n    def build_megatron_embedding(cls, args, dictionary, embed_dim):\n        return VocabParallelEmbedding(len(dictionary), embed_dim, init_method=lambda x: x)\n        \n    @classmethod\n    def build_embedding(cls, cfg, dictionary, embed_dim):\n        return Embedding(len(dictionary), embed_dim, dictionary.pad())\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_name_or_path,\n        checkpoint_file,\n        **kwargs\n    ):\n        from fairseq import hub_utils\n\n        x = hub_utils.from_pretrained(\n            model_name_or_path,\n            checkpoint_file,\n            **kwargs,\n        )\n        return XLMRHubInterface(x[\"args\"], x[\"task\"], x[\"models\"][0])\n\n    def forward(self, source, target_mask):\n        \n        src_x, tgt_x, src_key_padding_mask, tgt_key_padding_mask, hidden_state = self.decoder(source, target_mask)\n        tgt_out = self.decoder.output_layer(tgt_x)\n\n        length_out = self.decoder.forward_length(src_x, src_key_padding_mask)\n        length_tgt = self.decoder.forward_length_prediction(length_out, target_mask)\n        return tgt_out, length_out, length_tgt\n\n    def forward_encoder(self, source):\n        \n        src_x, src_padding, src_hiddens = self.decoder.forward_enc(source)\n        return {\n            \"encoder_out\": [src_x],\n            \"encoder_padding_mask\": [src_padding],\n            \"encoder_states\": src_hiddens,\n            \"src_tokens\": [source],\n        }\n\n    def nucleus_sampling(self, probs, output_tokens, step, max_step):\n        \n        nucleus_p = 0.9\n        temperature = (1.0 - step / max_step) * 2.0\n        probs = F.softmax(probs / temperature, dim=-1)\n        raw_indices_buf = probs.max(-1)[1].unsqueeze(-1)\n        \n        if nucleus_p > 0:\n            sorted_probs, sorted_indices = probs.sort(descending=True)\n            cumsum_probs = sorted_probs.cumsum(dim=2)\n            mask = cumsum_probs.lt(nucleus_p)\n\n            cumsum_mask = mask.cumsum(dim=2)\n            last_included = cumsum_mask[:, :, -1:]\n            last_included.clamp_(0, mask.size()[2] - 1)\n            mask = mask.scatter_(2, last_included, 1)\n            \n            max_dim = last_included.max()\n            truncated_mask = mask[:, :, : max_dim + 1]\n            truncated_probs = sorted_probs[:, :, : max_dim + 1]\n            truncated_indices = sorted_indices[:, :, : max_dim + 1]\n            trimed_probs = truncated_probs.masked_fill_(~truncated_mask, 0)\n        else:\n            trimed_probs, truncated_indices = probs.topk(nucleus_k)\n        \n        bsz, seq_len, _ = trimed_probs.size()\n        select_buf = torch.multinomial(trimed_probs.view(bsz * seq_len, -1), 1, replacement=True).view(bsz, seq_len)\n        scores_buf = torch.gather(trimed_probs, dim=2, index=select_buf.unsqueeze(-1))\n        indices_buf = torch.gather(truncated_indices, dim=2, index=select_buf.unsqueeze(-1))\n        return torch.log(scores_buf), indices_buf\n\n    def forward_decoder(self, decoder_out, encoder_out, decoding_format=None, **kwargs):\n        step = decoder_out.step\n        max_step = decoder_out.max_step\n\n        output_tokens = decoder_out.output_tokens\n        output_scores = decoder_out.output_scores\n        history = decoder_out.history\n        \n        output_masks = output_tokens.eq(self.mask_idx)\n        tgt_x, tgt_padding_mask, _ = self.decoder.forward_dec(encoder_out, output_tokens)\n        tgt_out = self.decoder.output_layer(tgt_x)\n\n        _scores, _tokens = self.nucleus_sampling(tgt_out, output_tokens, step, max_step)\n        # _scores, _tokens = F.log_softmax(tgt_out, -1).max(-1)\n\n        output_tokens.masked_scatter_(output_masks, _tokens[output_masks])\n        output_scores.masked_scatter_(output_masks, _scores[output_masks])\n\n        if history is not None:\n            history.append(output_tokens.clone())\n\n        # skeptical decoding (depend on the maximum decoding steps.)\n        if (step + 1) < max_step:\n            skeptical_mask = _skeptical_unmasking(\n                output_scores, output_tokens.ne(self.decoder.tgt_dict.pad()), 1 - (step + 1) / max_step\n            )\n\n            output_tokens.masked_fill_(skeptical_mask, self.mask_idx)\n            output_scores.masked_fill_(skeptical_mask, 0.0)\n\n            if history is not None:\n                history.append(output_tokens.clone())\n\n        return decoder_out._replace(\n            output_tokens=output_tokens,\n            output_scores=output_scores,\n            attn=None,\n            history=history,\n        )\n\n    def initialize_output_tokens(self, encoder_out, src_tokens, tgt_tokens=None):\n        length_tgt = self.decoder.forward_length_prediction(\n            self.decoder.forward_length(encoder_out[\"encoder_out\"][0], encoder_out[\"encoder_padding_mask\"][0]),\n            tgt_tokens=tgt_tokens,\n        )\n        max_length = length_tgt.clamp_(min=2).max()\n        idx_length = utils.new_arange(src_tokens, max_length)\n        initial_output_tokens = src_tokens.new_zeros(\n            src_tokens.size(0), max_length\n        ).fill_(self.decoder.pad)\n        initial_output_tokens.masked_fill_(\n            idx_length[None, :] < length_tgt[:, None], self.mask_idx\n        )\n        initial_output_tokens[:, 0] = self.decoder.bos\n        initial_output_tokens.scatter_(1, length_tgt[:, None] - 1, self.decoder.eos)\n\n        initial_output_scores = initial_output_tokens.new_zeros(\n            *initial_output_tokens.size()\n        ).type_as(encoder_out[\"encoder_out\"][0]).float()\n\n        return DecoderOut(\n            output_tokens=initial_output_tokens,\n            output_scores=initial_output_scores,\n            attn=None,\n            step=0,\n            max_step=0,\n            history=None,\n        )\n\n    @torch.jit.export\n    def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):\n        \n        if len(encoder_out[\"encoder_out\"]) == 0:\n            new_encoder_out = []\n        else:\n            new_encoder_out = [encoder_out[\"encoder_out\"][0].index_select(0, new_order)]\n        \n        if len(encoder_out[\"encoder_padding_mask\"]) == 0:\n            new_encoder_padding_mask = []\n        else:\n            new_encoder_padding_mask = [\n                encoder_out[\"encoder_padding_mask\"][0].index_select(0, new_order)\n            ]\n        \n        encoder_states = encoder_out[\"encoder_states\"]\n        if len(encoder_states) > 0:\n            for idx, state in enumerate(encoder_states):\n                encoder_states[idx] = state.index_select(0, new_order)\n\n        if len(encoder_out[\"src_tokens\"]) == 0:\n            src_tokens = []\n        else:\n            src_tokens = [(encoder_out[\"src_tokens\"][0]).index_select(0, new_order)]\n        \n        return {\n            \"encoder_out\": new_encoder_out,  # T x B x C\n            \"encoder_padding_mask\": new_encoder_padding_mask,  # B x T\n            \"encoder_states\": encoder_states,  # List[T x B x C]\n            \"src_tokens\": src_tokens,  # B x T\n        }\n        \n    def upgrade_state_dict_named(self, state_dict, name):\n        \n        for k in list(state_dict.keys()):\n                        \n            if \"version\" in k:\n                del state_dict[k]\n                continue\n\n            if \"encoder.sentence_encoder\" in k:\n                new_k = k.replace(\"encoder.sentence_encoder\", \"decoder\")\n                state_dict[new_k] = state_dict[k]\n                del state_dict[k]\n\n            if \"encoder.lm_head\" in k:\n                new_k = k.replace(\"encoder.lm_head\", \"decoder.lm_head\")\n                state_dict[new_k] = state_dict[k]\n                del state_dict[k]   \n\n        if \"decoder.embed_length.weight\" not in state_dict:\n            state_dict[\"decoder.embed_length.weight\"] = self.decoder.embed_length.weight\n\n        super().upgrade_state_dict_named(state_dict, name)", "@register_model(\"nar_xlmr\", dataclass=XLMRConfig)\nclass NARXLMR(BaseFairseqModel):\n    \n    def __init__(self, decoder):\n        super().__init__()\n        self.decoder = decoder\n        self.mask_idx = self.decoder.tgt_dict.index('<mask>')\n    \n    @classmethod\n    def build_model(cls, args, task):\n        \"\"\"Build a new model instance.\"\"\"\n        xlmr_base(args)\n        \n        logger.info(\"rescale [src] dictionary: {} types and [tgt] dictionary: {} types\".format(\n            len(task.source_dictionary), len(task.target_dictionary)))\n        \n        task.megatron_model = False\n        if safe_getattr(task, \"megatron_model\", False):\n            cls.initialize_model_parallel()\n            \n            task.source_dictionary.pad_to_multiple_(torch.distributed.get_world_size() * 8)\n            task.target_dictionary.pad_to_multiple_(torch.distributed.get_world_size() * 8)\n            \n            embed_tokens = cls.build_megatron_embedding(args, task.target_dictionary, args.decoder_embed_dim)\n            decoder = XLMRMegatron(\n                args,\n                task.source_dictionary,\n                task.target_dictionary,\n                embed_tokens\n            )\n        else:\n\n            embed_tokens = cls.build_embedding(args, task.source_dictionary, args.decoder_embed_dim)\n            decoder = XLMRTransformer(\n                args,\n                task.source_dictionary,\n                task.target_dictionary,\n                embed_tokens\n            )\n        \n        return cls(decoder)\n        \n    @classmethod\n    def initialize_model_parallel(cls):\n        logger.info(\"llama model init process group\")\n\n        if not torch.distributed.is_initialized():\n            torch.distributed.init_process_group(\"nccl\")\n\n        if not mpu.model_parallel_is_initialized():\n            ws = torch.distributed.get_world_size()\n            mpu.initialize_model_parallel(ws)\n\n    @classmethod\n    def build_megatron_embedding(cls, args, dictionary, embed_dim):\n        return VocabParallelEmbedding(len(dictionary), embed_dim, init_method=lambda x: x)\n        \n    @classmethod\n    def build_embedding(cls, cfg, dictionary, embed_dim):\n        return Embedding(len(dictionary), embed_dim, dictionary.pad())\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_name_or_path,\n        checkpoint_file,\n        **kwargs\n    ):\n        from fairseq import hub_utils\n\n        x = hub_utils.from_pretrained(\n            model_name_or_path,\n            checkpoint_file,\n            **kwargs,\n        )\n        return XLMRHubInterface(x[\"args\"], x[\"task\"], x[\"models\"][0])\n\n    def forward(self, source, target_mask):\n        \n        src_x, tgt_x, src_key_padding_mask, tgt_key_padding_mask, hidden_state = self.decoder(source, target_mask)\n        tgt_out = self.decoder.output_layer(tgt_x)\n\n        length_out = self.decoder.forward_length(src_x, src_key_padding_mask)\n        length_tgt = self.decoder.forward_length_prediction(length_out, target_mask)\n        return tgt_out, length_out, length_tgt\n\n    def forward_encoder(self, source):\n        \n        src_x, src_padding, src_hiddens = self.decoder.forward_enc(source)\n        return {\n            \"encoder_out\": [src_x],\n            \"encoder_padding_mask\": [src_padding],\n            \"encoder_states\": src_hiddens,\n            \"src_tokens\": [source],\n        }\n\n    def nucleus_sampling(self, probs, output_tokens, step, max_step):\n        \n        nucleus_p = 0.9\n        temperature = (1.0 - step / max_step) * 2.0\n        probs = F.softmax(probs / temperature, dim=-1)\n        raw_indices_buf = probs.max(-1)[1].unsqueeze(-1)\n        \n        if nucleus_p > 0:\n            sorted_probs, sorted_indices = probs.sort(descending=True)\n            cumsum_probs = sorted_probs.cumsum(dim=2)\n            mask = cumsum_probs.lt(nucleus_p)\n\n            cumsum_mask = mask.cumsum(dim=2)\n            last_included = cumsum_mask[:, :, -1:]\n            last_included.clamp_(0, mask.size()[2] - 1)\n            mask = mask.scatter_(2, last_included, 1)\n            \n            max_dim = last_included.max()\n            truncated_mask = mask[:, :, : max_dim + 1]\n            truncated_probs = sorted_probs[:, :, : max_dim + 1]\n            truncated_indices = sorted_indices[:, :, : max_dim + 1]\n            trimed_probs = truncated_probs.masked_fill_(~truncated_mask, 0)\n        else:\n            trimed_probs, truncated_indices = probs.topk(nucleus_k)\n        \n        bsz, seq_len, _ = trimed_probs.size()\n        select_buf = torch.multinomial(trimed_probs.view(bsz * seq_len, -1), 1, replacement=True).view(bsz, seq_len)\n        scores_buf = torch.gather(trimed_probs, dim=2, index=select_buf.unsqueeze(-1))\n        indices_buf = torch.gather(truncated_indices, dim=2, index=select_buf.unsqueeze(-1))\n        return torch.log(scores_buf), indices_buf\n\n    def forward_decoder(self, decoder_out, encoder_out, decoding_format=None, **kwargs):\n        step = decoder_out.step\n        max_step = decoder_out.max_step\n\n        output_tokens = decoder_out.output_tokens\n        output_scores = decoder_out.output_scores\n        history = decoder_out.history\n        \n        output_masks = output_tokens.eq(self.mask_idx)\n        tgt_x, tgt_padding_mask, _ = self.decoder.forward_dec(encoder_out, output_tokens)\n        tgt_out = self.decoder.output_layer(tgt_x)\n\n        _scores, _tokens = self.nucleus_sampling(tgt_out, output_tokens, step, max_step)\n        # _scores, _tokens = F.log_softmax(tgt_out, -1).max(-1)\n\n        output_tokens.masked_scatter_(output_masks, _tokens[output_masks])\n        output_scores.masked_scatter_(output_masks, _scores[output_masks])\n\n        if history is not None:\n            history.append(output_tokens.clone())\n\n        # skeptical decoding (depend on the maximum decoding steps.)\n        if (step + 1) < max_step:\n            skeptical_mask = _skeptical_unmasking(\n                output_scores, output_tokens.ne(self.decoder.tgt_dict.pad()), 1 - (step + 1) / max_step\n            )\n\n            output_tokens.masked_fill_(skeptical_mask, self.mask_idx)\n            output_scores.masked_fill_(skeptical_mask, 0.0)\n\n            if history is not None:\n                history.append(output_tokens.clone())\n\n        return decoder_out._replace(\n            output_tokens=output_tokens,\n            output_scores=output_scores,\n            attn=None,\n            history=history,\n        )\n\n    def initialize_output_tokens(self, encoder_out, src_tokens, tgt_tokens=None):\n        length_tgt = self.decoder.forward_length_prediction(\n            self.decoder.forward_length(encoder_out[\"encoder_out\"][0], encoder_out[\"encoder_padding_mask\"][0]),\n            tgt_tokens=tgt_tokens,\n        )\n        max_length = length_tgt.clamp_(min=2).max()\n        idx_length = utils.new_arange(src_tokens, max_length)\n        initial_output_tokens = src_tokens.new_zeros(\n            src_tokens.size(0), max_length\n        ).fill_(self.decoder.pad)\n        initial_output_tokens.masked_fill_(\n            idx_length[None, :] < length_tgt[:, None], self.mask_idx\n        )\n        initial_output_tokens[:, 0] = self.decoder.bos\n        initial_output_tokens.scatter_(1, length_tgt[:, None] - 1, self.decoder.eos)\n\n        initial_output_scores = initial_output_tokens.new_zeros(\n            *initial_output_tokens.size()\n        ).type_as(encoder_out[\"encoder_out\"][0]).float()\n\n        return DecoderOut(\n            output_tokens=initial_output_tokens,\n            output_scores=initial_output_scores,\n            attn=None,\n            step=0,\n            max_step=0,\n            history=None,\n        )\n\n    @torch.jit.export\n    def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):\n        \n        if len(encoder_out[\"encoder_out\"]) == 0:\n            new_encoder_out = []\n        else:\n            new_encoder_out = [encoder_out[\"encoder_out\"][0].index_select(0, new_order)]\n        \n        if len(encoder_out[\"encoder_padding_mask\"]) == 0:\n            new_encoder_padding_mask = []\n        else:\n            new_encoder_padding_mask = [\n                encoder_out[\"encoder_padding_mask\"][0].index_select(0, new_order)\n            ]\n        \n        encoder_states = encoder_out[\"encoder_states\"]\n        if len(encoder_states) > 0:\n            for idx, state in enumerate(encoder_states):\n                encoder_states[idx] = state.index_select(0, new_order)\n\n        if len(encoder_out[\"src_tokens\"]) == 0:\n            src_tokens = []\n        else:\n            src_tokens = [(encoder_out[\"src_tokens\"][0]).index_select(0, new_order)]\n        \n        return {\n            \"encoder_out\": new_encoder_out,  # T x B x C\n            \"encoder_padding_mask\": new_encoder_padding_mask,  # B x T\n            \"encoder_states\": encoder_states,  # List[T x B x C]\n            \"src_tokens\": src_tokens,  # B x T\n        }\n        \n    def upgrade_state_dict_named(self, state_dict, name):\n        \n        for k in list(state_dict.keys()):\n                        \n            if \"version\" in k:\n                del state_dict[k]\n                continue\n\n            if \"encoder.sentence_encoder\" in k:\n                new_k = k.replace(\"encoder.sentence_encoder\", \"decoder\")\n                state_dict[new_k] = state_dict[k]\n                del state_dict[k]\n\n            if \"encoder.lm_head\" in k:\n                new_k = k.replace(\"encoder.lm_head\", \"decoder.lm_head\")\n                state_dict[new_k] = state_dict[k]\n                del state_dict[k]   \n\n        if \"decoder.embed_length.weight\" not in state_dict:\n            state_dict[\"decoder.embed_length.weight\"] = self.decoder.embed_length.weight\n\n        super().upgrade_state_dict_named(state_dict, name)", "\ndef xlmr_base_architecture(args):\n\n    args.dropout = safe_getattr(args, \"dropout\", 0.1)\n    args.attention_dropout = safe_getattr(args, \"attention_dropout\", 0.1)\n    \n    args.decoder_embed_dim = safe_getattr(args, \"decoder_embed_dim\", 768)\n    args.decoder_ffn_embed_dim = safe_getattr(args, \"decoder_ffn_embed_dim\", 768 * 4)\n    args.decoder_layers = safe_getattr(args, \"decoder_layers\", 12)\n    args.decoder_attention_heads = safe_getattr(args, \"decoder_attention_heads\", 12)\n    args.decoder_normalize_before = getattr(args, \"decoder_normalize_before\", True)\n\n    args.max_source_positions = safe_getattr(args, \"max_source_positions\", 512)\n    args.max_target_positions = safe_getattr(args, \"max_target_positions\", 512)", "\n\ndef xlmr_xl_architecture(args):\n\n    args.dropout = safe_getattr(args, \"dropout\", 0.1)\n    args.attention_dropout = safe_getattr(args, \"attention_dropout\", 0.1)\n    \n    args.decoder_embed_dim = safe_getattr(args, \"decoder_embed_dim\", 2560)\n    args.decoder_ffn_embed_dim = safe_getattr(args, \"decoder_ffn_embed_dim\", 2560 * 4)\n    args.decoder_layers = safe_getattr(args, \"decoder_layers\", 36)\n    args.decoder_attention_heads = safe_getattr(args, \"decoder_attention_heads\", 32)\n    args.decoder_normalize_before = getattr(args, \"decoder_normalize_before\", True)\n\n    args.max_source_positions = safe_getattr(args, \"max_source_positions\", 512)\n    args.max_target_positions = safe_getattr(args, \"max_target_positions\", 512)", "\n\ndef xlmr_xxl_architecture(args):\n\n    args.dropout = safe_getattr(args, \"dropout\", 0.1)\n    args.attention_dropout = safe_getattr(args, \"attention_dropout\", 0.1)\n    \n    args.decoder_embed_dim = safe_getattr(args, \"decoder_embed_dim\", 4096)\n    args.decoder_ffn_embed_dim = safe_getattr(args, \"decoder_ffn_embed_dim\", 4096 * 4)\n    args.decoder_layers = safe_getattr(args, \"decoder_layers\", 48)\n    args.decoder_attention_heads = safe_getattr(args, \"decoder_attention_heads\", 32)\n    args.decoder_normalize_before = getattr(args, \"decoder_normalize_before\", True)\n\n    args.max_source_positions = safe_getattr(args, \"max_source_positions\", 512)\n    args.max_target_positions = safe_getattr(args, \"max_target_positions\", 512)", "\n@register_model_architecture(\"nar_xlmr\", \"nar_xlmr_base\")\ndef xlmr_base(args):\n    xlmr_base_architecture(args)\n\n@register_model_architecture(\"nar_xlmr\", \"nar_xlmr_xl\")\ndef xlmr_xl(args):\n    xlmr_xl_architecture(args)\n\n@register_model_architecture(\"nar_xlmr\", \"nar_xlmr_xxl\")\ndef xlmr_xxl(args):\n    xlmr_xxl_architecture(args)", "\n@register_model_architecture(\"nar_xlmr\", \"nar_xlmr_xxl\")\ndef xlmr_xxl(args):\n    xlmr_xxl_architecture(args)"]}
{"filename": "xlmr/src/model/xlmr_transformer.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, List, Optional, Tuple\nimport os\nimport math\nimport logging\n", "import logging\n\nimport torch\nfrom torch import Tensor, nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom torch.nn import Linear\nfrom fsdp.fully_sharded_data_parallel import fsdp_enable_wrap, fsdp_wrap\nfrom fairseq.modules.checkpoint_activations import checkpoint_wrapper\nfrom fairseq.modules.fairseq_dropout import FairseqDropout", "from fairseq.modules.checkpoint_activations import checkpoint_wrapper\nfrom fairseq.modules.fairseq_dropout import FairseqDropout\nimport numpy as np\nfrom fairseq.modules import (\n    LayerNorm,\n    PositionalEmbedding,\n)\nlogger = logging.getLogger(__name__)\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim**-0.5)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m", "\ndef Embedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim**-0.5)\n    nn.init.constant_(m.weight[padding_idx], 0)\n    return m\n\ndef _mean_pooling(enc_feats, src_masks):\n    # enc_feats: T x B x C\n    # src_masks: B x T or None\n    if src_masks is None:\n        enc_feats = enc_feats.mean(0)\n    else:\n        src_masks = (~src_masks).transpose(0, 1).type_as(enc_feats)\n        enc_feats = (\n            (enc_feats / src_masks.sum(0)[None, :, None]) * src_masks[:, :, None]\n        ).sum(0)\n    return enc_feats", "\nclass XLMRTransformer(nn.Module):\n\n    def __init__(self, cfg, src_dict, tgt_dict, embed_tokens):\n        super().__init__()\n        \n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n\n        self.embed_tokens = embed_tokens\n        self.embed_dim = cfg.decoder_embed_dim\n        self.num_layers = cfg.decoder_layers\n        self.normalize_before = cfg.decoder_normalize_before\n\n        self.pad = self.tgt_dict.pad()\n        self.bos = self.tgt_dict.bos()\n        self.eos = self.tgt_dict.eos()\n        self.unk = self.tgt_dict.unk()\n        \n        self.embed_positions = (\n            PositionalEmbedding(512,\n                self.embed_dim,\n                padding_idx=self.pad,\n                learned=True,\n            )\n        )\n\n        self.layers = torch.nn.ModuleList()\n        self.layers.extend(\n            [\n                self.build_decoder_layer(cfg)\n                for _ in range(self.num_layers)\n            ]\n        )\n        self.layer_norm = LayerNorm(self.embed_dim)\n\n        self.lm_head = XLMRHead(\n            embed_dim=self.embed_dim,\n            output_dim=len(self.tgt_dict),\n            weight=self.embed_tokens.weight,\n        )\n        self.embed_length = Embedding(512, self.embed_dim, None)\n        self.dropout_module = FairseqDropout(0.1)\n\n    def forward_length(self, enc_feats, src_masks):\n        enc_feats = _mean_pooling(enc_feats.transpose(0, 1), src_masks)\n        length_out = F.linear(enc_feats, self.embed_length.weight)\n        return F.log_softmax(length_out, -1)\n    \n    def forward_length_prediction(self, length_out, tgt_tokens=None):\n        if tgt_tokens is not None:\n            # obtain the length target\n            tgt_lengs = tgt_tokens.ne(self.pad).sum(1).long()\n            length_tgt = tgt_lengs\n            length_tgt = length_tgt.clamp(min=0, max=512)\n        else:\n            pred_lengs = length_out.max(-1)[1]\n            length_tgt = pred_lengs\n        return length_tgt\n\n    def build_decoder_layer(self, cfg):\n        layer = XLMRTransformerLayer(cfg)\n        checkpoint = cfg.checkpoint_activations\n        if checkpoint:\n            offload_to_cpu = cfg.offload_activations\n            layer = checkpoint_wrapper(layer, offload_to_cpu=offload_to_cpu)\n        min_params_to_wrap = cfg.min_params_to_wrap if not checkpoint else 0\n        layer = fsdp_wrap(layer, min_num_params=min_params_to_wrap)\n        return layer\n\n    def output_layer(self, x):\n        return self.lm_head(x).float()\n\n    def forward(self, source, target):\n        \n        src_embed = self.embed_tokens(source)\n        src_x = src_embed + self.embed_positions(source)\n\n        tgt_embed = self.embed_tokens(target)\n        tgt_x = tgt_embed + self.embed_positions(target)\n\n        if not self.normalize_before:\n            src_x = self.layer_norm(src_x)\n            tgt_x = self.layer_norm(tgt_x)\n\n        src_x = self.dropout_module(src_x)\n        tgt_x = self.dropout_module(tgt_x)\n        src_key_padding_mask = source.eq(self.pad)\n        tgt_key_padding_mask = target.eq(self.pad)\n        \n        hidden_state = [src_x]\n\n        tgt_start_idx = src_x.size(1)\n        x = torch.cat([src_x, tgt_x], dim=1)\n        key_padding_mask = torch.cat([src_key_padding_mask, tgt_key_padding_mask], dim=1)\n\n        for i, layer in enumerate(self.layers):\n            x = layer(\n                x,\n                key_padding_mask,\n            )\n            hidden_state.append(x[:,:tgt_start_idx,:])\n\n        if self.normalize_before:\n            src_x = self.layer_norm(x[:,:tgt_start_idx,:])\n            tgt_x = self.layer_norm(x[:,tgt_start_idx:,:])\n        \n        return src_x, tgt_x, src_key_padding_mask, tgt_key_padding_mask, hidden_state\n\n    def forward_enc(self, tokens):\n        \n        embed = self.embed_tokens(tokens)\n        x = embed + self.embed_positions(tokens)\n\n        if not self.normalize_before:\n            x = self.layer_norm(x)\n\n        x = self.dropout_module(x)\n        key_padding_mask = tokens.eq(self.pad)\n        \n        hidden_state = [x]\n        for i, layer in enumerate(self.layers):\n            \n            x = layer(\n                x,\n                key_padding_mask,\n            )\n            hidden_state.append(x)\n\n        if self.normalize_before:\n            x = self.layer_norm(x)\n        \n        return x, key_padding_mask, hidden_state\n\n    def forward_dec(self, encoder_out, tokens):\n        \n        embed = self.embed_tokens(tokens)\n        x = embed + self.embed_positions(tokens)\n\n        if not self.normalize_before:\n            x = self.layer_norm(x)\n\n        x = self.dropout_module(x)\n        tgt_key_padding_mask = tokens.eq(self.pad)\n        \n        hidden_state = [x]\n        tgt_start_idx = encoder_out[\"encoder_padding_mask\"][0].size(1)\n        key_padding_mask = torch.cat([encoder_out[\"encoder_padding_mask\"][0], tgt_key_padding_mask], dim=1)\n        for i, layer in enumerate(self.layers):\n            \n            x_concat = torch.cat([encoder_out[\"encoder_states\"][i], x], dim=1)\n            \n            x = layer(\n                x_concat,\n                key_padding_mask,\n            )[:, tgt_start_idx:, :]\n            hidden_state.append(x[:, tgt_start_idx:, :])\n\n        if self.normalize_before:\n            x = self.layer_norm(x)\n        \n        return x, key_padding_mask, hidden_state", "    \n\nclass XLMRHead(nn.Module):\n    \"\"\"Head for masked language modeling.\"\"\"\n\n    def __init__(self, embed_dim, output_dim, weight=None):\n        super().__init__()\n        self.dense = nn.Linear(embed_dim, embed_dim)\n        self.activation_fn = utils.get_activation_fn(\"gelu\")\n        self.layer_norm = LayerNorm(embed_dim)\n        \n        if weight is None:\n            weight = nn.Linear(embed_dim, output_dim, bias=False).weight\n        self.weight = weight\n        self.bias = nn.Parameter(torch.zeros(output_dim))\n\n    def forward(self, features):\n        x = self.dense(features)\n        x = self.activation_fn(x)\n        x = self.layer_norm(x)\n        x = F.linear(x, self.weight) + self.bias\n        return x", "\n\nclass XLMRTransformerLayer(nn.Module):\n\n    def __init__(self, cfg):\n        super().__init__()\n\n        self.embed_dim = cfg.decoder_embed_dim\n        self.num_heads = cfg.decoder_attention_heads\n        self.ffn_embed_dim = cfg.decoder_ffn_embed_dim\n\n        self.self_attn = XLMRAttention(self.num_heads, self.embed_dim)\n\n        self.activation_fn = utils.get_activation_fn(\"gelu\")\n        self.fc1 = nn.Linear(self.embed_dim, self.ffn_embed_dim)\n        self.fc2 = nn.Linear(self.ffn_embed_dim, self.embed_dim)\n\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n        \n        self.normalize_before = cfg.decoder_normalize_before\n        self.dropout_module = FairseqDropout(0.1)\n        \n    def forward(\n        self,\n        x: Tensor,\n        key_padding_mask: Optional[Tensor],\n    ):\n        \n        residual = x\n        if self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        x = residual + self.dropout_module(self.self_attn(x, key_padding_mask))\n        if not self.normalize_before:\n            x = self.self_attn_layer_norm(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.final_layer_norm(x)\n        \n        x = residual + self.dropout_module(self.fc2(self.activation_fn(self.fc1(x))))\n        if not self.normalize_before:\n            x = self.final_layer_norm(x)\n        return x", "\n\nclass XLMRAttention(nn.Module):\n\n    def __init__(self, num_heads, embed_dim):\n        super().__init__()\n\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.head_dim = embed_dim // num_heads\n\n        self.scaling = self.head_dim**-0.5\n        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n        self.dropout_module = FairseqDropout(0.1)\n\n    def forward(self, query, key_padding_mask):\n        \n        bsz, src_len, embed_dim = query.size()\n        q = self.q_proj(query)\n        k = self.k_proj(query)\n        v = self.v_proj(query)\n        q *= self.scaling\n\n        q = q.view(bsz, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(bsz, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(bsz, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n        attn_scores = torch.matmul(q, k.transpose(2, 3))\n\n        if key_padding_mask is not None:\n            attn_scores = attn_scores.masked_fill(\n                key_padding_mask.unsqueeze(1).unsqueeze(2),\n                float(\"-inf\")\n            )\n        attn_softmax_scores = F.softmax(attn_scores.float(), dim=-1).type_as(q)\n        output = torch.matmul(self.dropout_module(attn_softmax_scores), v)\n        output = output.transpose(1, 2).contiguous().view(bsz, src_len, -1)\n        return self.out_proj(output)", "\n"]}
{"filename": "xlmr/src/fsdp/cpu_adam.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nfrom collections.abc import Collection\nfrom dataclasses import dataclass, field\nfrom typing import List\n", "from typing import List\n\nimport torch\nfrom fairseq.dataclass import FairseqDataclass\nfrom fairseq.optim import FairseqOptimizer, register_optimizer\nfrom omegaconf import II, DictConfig\n\n\ntry:\n    import deepspeed\n\n    has_deepspeed = True\nexcept ImportError as e:\n    has_deepspeed = False", "try:\n    import deepspeed\n\n    has_deepspeed = True\nexcept ImportError as e:\n    has_deepspeed = False\n\n\ndef _get_cpu_adam():\n    try:\n        from deepspeed.ops.op_builder import CPUAdamBuilder\n\n        return CPUAdamBuilder().load()\n    except ImportError:\n        # fbcode\n        from deepspeed.ops.adam import DeepSpeedCPUAdam as ds_opt_adam\n\n        return ds_opt_adam", "def _get_cpu_adam():\n    try:\n        from deepspeed.ops.op_builder import CPUAdamBuilder\n\n        return CPUAdamBuilder().load()\n    except ImportError:\n        # fbcode\n        from deepspeed.ops.adam import DeepSpeedCPUAdam as ds_opt_adam\n\n        return ds_opt_adam", "\n\n@dataclass\nclass FairseqCPUAdamConfig(FairseqDataclass):\n    adam_betas: str = field(\n        default=\"(0.9, 0.999)\", metadata={\"help\": \"betas for Adam optimizer\"}\n    )\n    adam_eps: float = field(\n        default=1e-8, metadata={\"help\": \"epsilon for Adam optimizer\"}\n    )\n    weight_decay: float = field(default=0.0, metadata={\"help\": \"weight decay\"})\n    fp16_adam_stats: bool = field(\n        default=False, metadata={\"help\": \"use FP16 stats (with automatic scaling)\"}\n    )\n    # TODO common vars below in parent\n    lr: List[float] = II(\"optimization.lr\")", "\n\n@register_optimizer(\"new_cpu_adam\", dataclass=FairseqCPUAdamConfig)\nclass FairseqCPUAdam(FairseqOptimizer):\n    \"\"\"Adam optimizer for fairseq, optimized for CPU tensors.\n\n    Important note: this optimizer corresponds to the \"AdamW\" variant of\n    Adam in its weight decay behavior. As such, it is most closely\n    analogous to torch.optim.AdamW from PyTorch.\n    \"\"\"\n\n    def __init__(self, cfg: DictConfig, params):\n        super().__init__(cfg)\n        self._optimizer = CPUAdam(params, **self.optimizer_config)\n\n    @property\n    def optimizer_config(self):\n        \"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"\n        return {\n            \"lr\": self.cfg.lr[0]\n            if isinstance(self.cfg.lr, Collection)\n            else self.cfg.lr,\n            \"betas\": eval(self.cfg.adam_betas),\n            \"eps\": self.cfg.adam_eps,\n            \"weight_decay\": self.cfg.weight_decay,\n            \"use_fp16_stats\": self.cfg.fp16_adam_stats,\n        }", "\n\nclass CPUAdam(torch.optim.Optimizer):\n\n    optimizer_id = 0\n\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        bias_correction=True,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        use_fp16_stats=False,\n    ):\n        defaults = {\n            \"lr\": lr,\n            \"bias_correction\": bias_correction,\n            \"betas\": betas,\n            \"eps\": eps,\n            \"weight_decay\": weight_decay,\n        }\n        super().__init__(params, defaults)\n\n        self.use_fp16_stats = use_fp16_stats\n        self.FLOAT16_MAX = 65504.0\n\n        if not has_deepspeed:\n            raise ImportError(\"Please install DeepSpeed: pip install deepspeed\")\n\n        self.opt_id = CPUAdam.optimizer_id\n        CPUAdam.optimizer_id = CPUAdam.optimizer_id + 1\n\n        self.ds_opt_adam = _get_cpu_adam()\n        adamw_mode = True\n        self.ds_opt_adam.create_adam(\n            self.opt_id, lr, betas[0], betas[1], eps, weight_decay, adamw_mode, True\n        )\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return True\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        torch.cuda.synchronize()\n\n        for group_id, group in enumerate(self.param_groups):\n            for param_id, p in enumerate(group[\"params\"]):\n                if p.grad is None:\n                    continue\n\n                state = self.state[p]\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    dtype = torch.float16 if self.use_fp16_stats else p.data.dtype\n                    # gradient momentums\n                    state[\"exp_avg\"] = torch.zeros_like(\n                        p.data, dtype=dtype, device=\"cpu\"\n                    )\n                    # gradient variances\n                    state[\"exp_avg_sq\"] = torch.zeros_like(\n                        p.data, dtype=dtype, device=\"cpu\"\n                    )\n                    if self.use_fp16_stats:\n                        assert torch.is_floating_point(p.data)\n                        state[\"exp_avg_scale\"] = 1.0\n                        state[\"exp_avg_sq_scale\"] = 1.0\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n\n                p_data_bak = p.data  # backup of the original data pointer\n\n                p.data = p.data.to(dtype=torch.float32, device=\"cpu\")\n                p.grad.data = p.grad.data.to(dtype=torch.float32, device=\"cpu\")\n\n                if self.use_fp16_stats:\n                    exp_avg = exp_avg.float() * state[\"exp_avg_scale\"]\n                    exp_avg_sq = exp_avg_sq.float() * state[\"exp_avg_sq_scale\"]\n\n                state[\"step\"] += 1\n                beta1, beta2 = group[\"betas\"]\n\n                self.ds_opt_adam.adam_update(\n                    self.opt_id,\n                    state[\"step\"],\n                    group[\"lr\"],\n                    beta1,\n                    beta2,\n                    group[\"eps\"],\n                    group[\"weight_decay\"],\n                    group[\"bias_correction\"],\n                    p.data,\n                    p.grad.data,\n                    exp_avg,\n                    exp_avg_sq,\n                )\n\n                if p_data_bak.data_ptr() != p.data.data_ptr():\n                    p_data_bak.copy_(p.data)\n                    p.data = p_data_bak\n\n                if self.use_fp16_stats:\n\n                    def inf_norm(t):\n                        return torch.norm(t, float(\"inf\"))\n\n                    # from github.com/openai/jukebox/blob/master/jukebox/utils/fp16.py\n                    state[\"exp_avg_scale\"], state[\"exp_avg_sq_scale\"] = (\n                        1e-8 + inf_norm(exp_avg) / self.FLOAT16_MAX,\n                        1e-8 + inf_norm(exp_avg_sq) / self.FLOAT16_MAX,\n                    )\n                    state[\"exp_avg\"], state[\"exp_avg_sq\"] = (\n                        (exp_avg / state[\"exp_avg_scale\"]).half(),\n                        (exp_avg_sq / state[\"exp_avg_sq_scale\"]).half(),\n                    )\n\n        return loss", ""]}
{"filename": "xlmr/src/fsdp/fully_sharded_data_parallel.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nfrom typing import Optional\nimport os\nimport torch\nfrom fairseq.dataclass.configs import DistributedTrainingConfig", "import torch\nfrom fairseq.dataclass.configs import DistributedTrainingConfig\nfrom fairseq.distributed import utils as dist_utils\nfrom typing import Any, Dict, Optional, Set, cast\ntry:\n    from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n    from fairscale.nn.data_parallel import TrainingState\n    has_FSDP = True\nexcept ImportError:\n    FSDP = torch.nn.Module\n    has_FSDP = False", "\ndef free_storage_(data: torch.Tensor):\n    if data.storage().size() > 0:\n        assert data.storage_offset() == 0\n        data.storage().resize_(0)\n\n\nclass FullyShardedDataParallel(FSDP):\n    \"\"\"\n    A small wrapper around fairscale's FullyShardedDataParallel (FSDP) with some\n    fairseq-specific checkpoint saving/loading logic.\n\n    Args:\n        use_sharded_state (bool): if True, then ``state_dict`` will return\n            ``FSDP.local_state_dict`` and ``load_state_dict`` will call\n            ``FSDP.load_local_state_dict``. Otherwise, ``state_dict`` will\n            return the full model weights on data parallel rank 0 (empty on\n            other ranks) and ``load_state_dict`` will broadcast model weights\n            from rank 0 to other ranks.\n    \"\"\"\n\n    def __init__(self, *args, use_sharded_state: bool = False, **kwargs):\n        if not has_FSDP:\n            raise ImportError(\n                \"Cannot find FullyShardedDataParallel. \"\n                \"Please install fairscale with: pip install fairscale\"\n            )\n        super().__init__(*args, **kwargs)\n        self.use_sharded_state = use_sharded_state\n        \n        if dist_utils.get_world_size(group=dist_utils.get_data_parallel_group()) < 4 and \\\n            \"NVIDIA GeForce RTX 3090\" in torch.cuda.get_device_name():\n            self.alpaca_force_full_precision = False\n        else:\n            self.alpaca_force_full_precision = True\n\n    @property\n    def unwrapped_module(self) -> torch.nn.Module:\n        if self.flatten_parameters:\n            return self.module.module\n        else:\n            return self.module\n\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n        if self.use_sharded_state:\n            return super().local_state_dict(\n                destination=destination, prefix=prefix, keep_vars=keep_vars\n            )\n        else:\n            if self.rank == 0:\n                return super().state_dict(\n                    destination=destination, prefix=prefix, keep_vars=keep_vars\n                )\n            else:\n                # We must call state_dict() due to use of communication\n                # primitives. But we don't use the result.\n                super().state_dict()\n                return destination or {}\n\n    def load_state_dict(self, state_dict, strict=True, model_cfg=None):\n        if self.use_sharded_state:\n            return super().load_local_state_dict(state_dict, strict=strict)\n        else:\n            state_dict = dist_utils.broadcast_object(\n                state_dict, src_rank=0, group=self.process_group,\n            )\n            return super().load_state_dict(state_dict, strict=strict)\n\n    @contextlib.contextmanager\n    def summon_full_params(self, recurse: bool = True, volatile: bool = False):\n        if recurse:\n            with contextlib.ExitStack() as stack:\n                # Summon all params for any nested FSDP instances.\n                for module in self.modules():\n                    if isinstance(module, FullyShardedDataParallel):\n                        stack.enter_context(module.summon_full_params(recurse=False, volatile=volatile))\n                # Yield to the caller, with full params in all nested instances.\n                yield\n            # Exiting from the ExitStack will re-shard params.\n            return\n        else:\n            torch.cuda.synchronize()\n            self._lazy_init()\n            self.assert_state(TrainingState.IDLE)\n            # Set the state so that we assert when trying to go into fwd/bwd.\n            self.training_state = TrainingState.SUMMON_FULL_PARAMS\n            full_tensors = self._rebuild_full_params(force_full_precision=self.alpaca_force_full_precision)\n            assert full_tensors is not None\n            with contextlib.ExitStack() as stack:\n                if self.module.is_flattened:\n                    # Update flattened views to point to fully-sized tensors. We\n                    # use self.params instead of full_tensors since the\n                    # latter may contain padding.\n                    stack.enter_context(\n                        self.module.unflatten_params(\n                            flat_params=[p.data for p in self.params[: self._num_flatten_params]]\n                        )\n                    )\n                try:\n                    yield\n                finally:\n                    stack.close()\n                    non_shared_params = self.params\n                    # filter out shared params for all but the owner FSDP module.\n                    if len(full_tensors) < len(non_shared_params):\n                        non_shared_params = self.non_shared_params()\n                    assert len(full_tensors) == len(\n                        non_shared_params\n                    ), f\"{len(full_tensors)} vs. {len(non_shared_params)}\"\n                    for p, (full_tensor, safe_to_free) in zip(non_shared_params, full_tensors):\n                        if not volatile:\n                            # Copy any changes made to the full params back into\n                            # the corresponding local shards.\n                            local_shard, _ = self._get_shard(full_tensor)\n                            p._fp32_shard.copy_(local_shard.view_as(p._fp32_shard))\n                        if safe_to_free:\n                            free_storage_(full_tensor)\n                    self.has_full_params = False\n                    self._use_fp32_param_shard()\n                    self.training_state = TrainingState.IDLE", "\n\nclass DummyProcessGroup:\n    def __init__(self, rank: int, size: int):\n        self._rank = rank\n        self._size = size\n\n    def rank(self) -> int:\n        return self._rank\n\n    def size(self) -> int:\n        return self._size", "\n\n@contextlib.contextmanager\ndef fsdp_enable_wrap(cfg: DistributedTrainingConfig):\n    try:\n        from fairscale.nn import enable_wrap\n    except ImportError:\n        raise ImportError(\n            \"Cannot find FullyShardedDataParallel. \"\n            \"Please install fairscale with: pip install fairscale\"\n        )\n    if cfg.memory_efficient_fp16:\n        assert cfg.fp16  # memory_efficient_fp16 should imply fp16\n    group = dist_utils.get_data_parallel_group()\n    if group is None and cfg.distributed_world_size == 1:\n        group = DummyProcessGroup(rank=0, size=1)\n    fsdp_config = {\n        \"process_group\": group,\n        \"reshard_after_forward\": not cfg.no_reshard_after_forward,\n        \"mixed_precision\": cfg.fp16 and not cfg.memory_efficient_fp16,\n        \"fp32_reduce_scatter\": cfg.fp32_reduce_scatter,\n        \"flatten_parameters\": not cfg.not_fsdp_flatten_parameters,\n        \"cpu_offload\": cfg.cpu_offload,\n        \"compute_dtype\": torch.float16 if cfg.fp16 else torch.float32,\n        \"bucket_cap_mb\": cfg.bucket_cap_mb,\n        \"state_dict_device\": torch.device(\"cpu\"),  # reduce GPU mem usage\n    }\n    with enable_wrap(\n        wrapper_cls=FullyShardedDataParallel,\n        use_sharded_state=cfg.use_sharded_state,\n        **fsdp_config,\n    ):\n        yield", "\n\ndef fsdp_wrap(module, min_num_params: Optional[int] = None, **kwargs):\n    \"\"\"\n    Helper to wrap layers/modules in FSDP. This falls back to a no-op if\n    fairscale is not available.\n\n    Args:\n        module (nn.Module): module to (maybe) wrap\n        min_num_params (int, Optional): minimum number of layer params to wrap\n    \"\"\"\n    try:\n        from fairscale.nn import wrap\n\n        if min_num_params is not None:\n            num_params = sum(p.numel() for p in module.parameters())\n            if num_params >= min_num_params:\n                return wrap(module, **kwargs)\n            else:\n                return module\n        else:\n            return wrap(module, **kwargs)\n    except ImportError:\n        return module", ""]}
{"filename": "xlmr/src/loss/cmlm_loss.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom dataclasses import dataclass, field\n\nimport torch\nfrom fairseq import metrics, utils", "import torch\nfrom fairseq import metrics, utils\nfrom fairseq.criterions import FairseqCriterion, register_criterion\nfrom fairseq.dataclass import FairseqDataclass\nimport torch.nn.functional as F\nfrom fairseq.utils import new_arange\nfrom fairscale.nn.model_parallel.cross_entropy import vocab_parallel_cross_entropy\n\n\n@register_criterion(\"cmlm_loss\")\nclass CMLMLabelSmoothedCrossEntropyCriterion(FairseqCriterion):\n    def __init__(\n        self,\n        task,\n    ):\n        super().__init__(task)\n        self.eps = 0.1\n        self.pad = task.tgt_dict.pad()\n        self.bos = task.tgt_dict.bos()\n        self.eos = task.tgt_dict.eos()\n        self.unk = task.tgt_dict.unk()\n        self.mask = task.tgt_dict.index('<mask>')\n\n    def _random_mask(self, target_tokens):\n        target_masks = (\n            target_tokens.ne(self.pad) & target_tokens.ne(self.bos) & target_tokens.ne(self.eos)\n        )\n        target_score = target_tokens.clone().float().uniform_()\n        target_score.masked_fill_(~target_masks, 2.0)\n        target_length = target_masks.sum(1).float()\n        target_length = target_length * target_length.clone().uniform_()\n        target_length = target_length + 1  # make sure to mask at least one token.\n        \n        _, target_rank = target_score.sort(1)\n        target_cutoff = new_arange(target_rank) < target_length[:, None].long()\n        prev_target_tokens = target_tokens.masked_fill(\n            target_cutoff.scatter(1, target_rank, target_cutoff), self.mask\n        )\n        return prev_target_tokens\n\n    def forward(self, model, sample, reduce=True):\n        source = sample[\"net_input\"][\"src_tokens\"]\n        target = sample[\"target\"]\n        target_mask = self._random_mask(target)\n        mask = (target != target_mask)\n\n        output, length_out, length_tgt = model(source, target_mask)\n        loss, nll_loss = self.label_smooth_loss(output[mask], target[mask])\n        # print(length_out.size(), length_tgt)\n        # length_loss = self.length_loss(length_out, length_tgt)\n        # loss += 0.1 * length_loss\n        \n        sample_size = 1\n        logging_output = {\n            \"loss\": loss.data,\n            \"nll_loss\": nll_loss.data,\n            \"ntokens\": sample[\"ntokens\"],\n            \"nsentences\": sample[\"target\"].size(0),\n            \"sample_size\": sample_size,\n        }\n        return loss, sample_size, logging_output\n        \n    def label_smooth_loss(self, net_out, net_target):\n        net_logits = F.log_softmax(net_out, dim=-1)\n        nll_loss = F.nll_loss(net_logits, net_target, reduction=\"none\").float().mean()\n        loss = nll_loss * (1. - self.eps) - net_logits.float().mean() * self.eps\n        return loss, nll_loss\n\n    def length_loss(self, length_out, length_tgt):\n        length_logits = F.log_softmax(length_out, dim=-1)\n        length_loss = F.nll_loss(length_logits, length_tgt, reduction=\"none\").float().mean()\n        return length_loss\n\n\n    @classmethod\n    def reduce_metrics(cls, logging_outputs) -> None:\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n        nll_loss_sum = sum(log.get(\"nll_loss\", 0) for log in logging_outputs)\n        ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n        metrics.log_scalar(\n            \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n        )\n        metrics.log_scalar(\n            \"nll_loss\", nll_loss_sum / sample_size / math.log(2), sample_size, round=3\n        )\n        metrics.log_derived(\n            \"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg)\n        )\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        \"\"\"\n        return True", "\n@register_criterion(\"cmlm_loss\")\nclass CMLMLabelSmoothedCrossEntropyCriterion(FairseqCriterion):\n    def __init__(\n        self,\n        task,\n    ):\n        super().__init__(task)\n        self.eps = 0.1\n        self.pad = task.tgt_dict.pad()\n        self.bos = task.tgt_dict.bos()\n        self.eos = task.tgt_dict.eos()\n        self.unk = task.tgt_dict.unk()\n        self.mask = task.tgt_dict.index('<mask>')\n\n    def _random_mask(self, target_tokens):\n        target_masks = (\n            target_tokens.ne(self.pad) & target_tokens.ne(self.bos) & target_tokens.ne(self.eos)\n        )\n        target_score = target_tokens.clone().float().uniform_()\n        target_score.masked_fill_(~target_masks, 2.0)\n        target_length = target_masks.sum(1).float()\n        target_length = target_length * target_length.clone().uniform_()\n        target_length = target_length + 1  # make sure to mask at least one token.\n        \n        _, target_rank = target_score.sort(1)\n        target_cutoff = new_arange(target_rank) < target_length[:, None].long()\n        prev_target_tokens = target_tokens.masked_fill(\n            target_cutoff.scatter(1, target_rank, target_cutoff), self.mask\n        )\n        return prev_target_tokens\n\n    def forward(self, model, sample, reduce=True):\n        source = sample[\"net_input\"][\"src_tokens\"]\n        target = sample[\"target\"]\n        target_mask = self._random_mask(target)\n        mask = (target != target_mask)\n\n        output, length_out, length_tgt = model(source, target_mask)\n        loss, nll_loss = self.label_smooth_loss(output[mask], target[mask])\n        # print(length_out.size(), length_tgt)\n        # length_loss = self.length_loss(length_out, length_tgt)\n        # loss += 0.1 * length_loss\n        \n        sample_size = 1\n        logging_output = {\n            \"loss\": loss.data,\n            \"nll_loss\": nll_loss.data,\n            \"ntokens\": sample[\"ntokens\"],\n            \"nsentences\": sample[\"target\"].size(0),\n            \"sample_size\": sample_size,\n        }\n        return loss, sample_size, logging_output\n        \n    def label_smooth_loss(self, net_out, net_target):\n        net_logits = F.log_softmax(net_out, dim=-1)\n        nll_loss = F.nll_loss(net_logits, net_target, reduction=\"none\").float().mean()\n        loss = nll_loss * (1. - self.eps) - net_logits.float().mean() * self.eps\n        return loss, nll_loss\n\n    def length_loss(self, length_out, length_tgt):\n        length_logits = F.log_softmax(length_out, dim=-1)\n        length_loss = F.nll_loss(length_logits, length_tgt, reduction=\"none\").float().mean()\n        return length_loss\n\n\n    @classmethod\n    def reduce_metrics(cls, logging_outputs) -> None:\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n        nll_loss_sum = sum(log.get(\"nll_loss\", 0) for log in logging_outputs)\n        ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n        metrics.log_scalar(\n            \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n        )\n        metrics.log_scalar(\n            \"nll_loss\", nll_loss_sum / sample_size / math.log(2), sample_size, round=3\n        )\n        metrics.log_derived(\n            \"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg)\n        )\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        \"\"\"\n        return True", ""]}
