{"filename": "inference.py", "chunked_list": ["import argparse\nimport os\nimport torch\nfrom torch import autocast\nfrom diffusers import DDIMScheduler\nfrom diffusers_ import StableDiffusionPipeline\nfrom accelerate.utils import set_seed\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"text-inversion-model\",\n        help=\"The directory where the target embeddings are saved.\",\n    )\n    parser.add_argument(\n        \"--target_txt\",\n        type=str,\n        help=\"Target prompt.\",\n    )\n    parser.add_argument(\n        \"--pretrained_model_name\",\n        type=str,\n        default=\"CompVis/stable-diffusion-v1-4\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        help=\"Seed\",\n    )\n    parser.add_argument(\n        \"--image_num\",\n        type=int,\n        help=\"Seed\",\n    )\n    parser.add_argument(\n        \"--inference_train_step\",\n        type=int,\n        help=\"Seed\",\n    )\n    args = parser.parse_args()\n    return args", "\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"text-inversion-model\",\n        help=\"The directory where the target embeddings are saved.\",\n    )\n    parser.add_argument(\n        \"--target_txt\",\n        type=str,\n        help=\"Target prompt.\",\n    )\n    parser.add_argument(\n        \"--pretrained_model_name\",\n        type=str,\n        default=\"CompVis/stable-diffusion-v1-4\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        help=\"Seed\",\n    )\n    parser.add_argument(\n        \"--image_num\",\n        type=int,\n        help=\"Seed\",\n    )\n    parser.add_argument(\n        \"--inference_train_step\",\n        type=int,\n        help=\"Seed\",\n    )\n    args = parser.parse_args()\n    return args", "\nfrom transformers import CLIPTextModel, CLIPTokenizer\n\n\ndef main():\n    args = parse_args()\n    \n    if args.seed is not None:\n        set_seed(args.seed)\n        g_cuda = torch.Generator(device='cuda')\n        g_cuda.manual_seed(args.seed)\n    \n    \n    # Load pretrained models    \n    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False) \n    pipe = StableDiffusionPipeline.from_pretrained(args.pretrained_model_name, scheduler=scheduler, torch_dtype=torch.float16).to(\"cuda\")\n    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name, subfolder=\"tokenizer\", use_auth_token=True)\n    CLIP_text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name, subfolder=\"text_encoder\", use_auth_token=True)\n    \n    \n    # Encode the target text.\n    text_ids_tgt = tokenizer(args.target_txt, padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids\n    CLIP_text_encoder.to('cuda', dtype=torch.float32)\n    with torch.inference_mode():\n        target_embedding = CLIP_text_encoder(text_ids_tgt.to('cuda'))[0].to('cuda')\n    del CLIP_text_encoder    \n    \n    \n    # Concat target and hiper embeddings\n    hiper_embeddings = torch.load(os.path.join(args.output_dir, 'hiper_embeddings_step{}.pt'.format(args.inference_train_step))).to(\"cuda\")\n    n_hiper = hiper_embeddings.shape[1]\n    inference_embeddings =torch.cat([target_embedding[:, :-n_hiper], hiper_embeddings*0.8], 1)\n    \n\n    # Generate target images \n    num_samples = 3\n    guidance_scale = 7.5 \n    num_inference_steps = 50 \n    height = 512\n    width = 512 \n\n    with autocast(\"cuda\"), torch.inference_mode():        \n        model_path = os.path.join(args.output_dir, 'result')\n        os.makedirs(model_path, exist_ok=True)\n        for idx, embd in enumerate([inference_embeddings]):\n            for i in range(args.image_num//num_samples+1):\n                images = pipe(\n                    text_embeddings=embd,\n                    height=height,\n                    width=width,\n                    num_images_per_prompt=num_samples,\n                    num_inference_steps=num_inference_steps,\n                    guidance_scale=guidance_scale,\n                    generator=g_cuda\n                ).images\n                for j in range(len(images)):\n                    image = images[j]\n                    image.save(model_path+'/{}_seed{}_{}.png'.format(args.target_txt,args.seed,i*num_samples+j))\n                    if i*num_samples+j == args.image_num:\n                        break", "        \n        \n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "train.py", "chunked_list": ["import argparse\nimport os\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed", "from accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler, UNet2DConditionModel\nfrom diffusers_ import StableDiffusionPipeline\nfrom huggingface_hub import HfFolder, Repository, whoami\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\nfrom tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer", "from tqdm.auto import tqdm\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom torch import autocast\nfrom utils import inf_save\n\n\nlogger = get_logger(__name__)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--input_image\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to input image to edit.\",\n    )\n    parser.add_argument(\n        \"--target_text\",\n        type=str,\n        default=None,\n        help=\"The target text describing the output image.\",\n    )\n    parser.add_argument(\n        \"--source_text\",\n        type=str,\n        default=None,\n        help=\"The source text describing the input image.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"output\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=512,\n        help=(\n            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n            \" resolution\"\n        ),\n    )\n    parser.add_argument(\n        \"--center_crop\", action=\"store_true\", help=\"Whether to center crop images before resizing to resolution\"\n    )\n    parser.add_argument(\n        \"--n_hiper\",\n        type=int,\n        default=5,\n        help=\"Number of hiper embedding\",\n    )\n    parser.add_argument(\n        \"--emb_train_steps\",\n        type=int,\n        default=1500,\n        help=\"Total number of training steps to perform.\",\n    )\n    parser.add_argument(\n        \"--emb_learning_rate\",\n        type=float,\n        default=1e-3,\n        help=\"Learning rate for optimizing the embeddings.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--use_8bit_adam\", action=\"store_false\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n    )\n    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n    parser.add_argument(\n        \"--mixed_precision\",\n        type=str,\n        default=\"fp16\",\n        choices=[\"no\", \"fp16\", \"bf16\"],\n        help=(\n            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )\n\n    args = parser.parse_args()\n    return args", "\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n    parser.add_argument(\n        \"--pretrained_model_name\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"model identifier from huggingface.co/models.\",\n    )\n    parser.add_argument(\n        \"--input_image\",\n        type=str,\n        default=None,\n        required=True,\n        help=\"Path to input image to edit.\",\n    )\n    parser.add_argument(\n        \"--target_text\",\n        type=str,\n        default=None,\n        help=\"The target text describing the output image.\",\n    )\n    parser.add_argument(\n        \"--source_text\",\n        type=str,\n        default=None,\n        help=\"The source text describing the input image.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"output\",\n        help=\"The output directory where the model predictions and checkpoints will be written.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n    parser.add_argument(\n        \"--resolution\",\n        type=int,\n        default=512,\n        help=(\n            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n            \" resolution\"\n        ),\n    )\n    parser.add_argument(\n        \"--center_crop\", action=\"store_true\", help=\"Whether to center crop images before resizing to resolution\"\n    )\n    parser.add_argument(\n        \"--n_hiper\",\n        type=int,\n        default=5,\n        help=\"Number of hiper embedding\",\n    )\n    parser.add_argument(\n        \"--emb_train_steps\",\n        type=int,\n        default=1500,\n        help=\"Total number of training steps to perform.\",\n    )\n    parser.add_argument(\n        \"--emb_learning_rate\",\n        type=float,\n        default=1e-3,\n        help=\"Learning rate for optimizing the embeddings.\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n    )\n    parser.add_argument(\n        \"--use_8bit_adam\", action=\"store_false\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n    )\n    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n    parser.add_argument(\n        \"--mixed_precision\",\n        type=str,\n        default=\"fp16\",\n        choices=[\"no\", \"fp16\", \"bf16\"],\n        help=(\n            \"Whether to use mixed precision. Choose\"\n            \"between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >= 1.10.\"\n            \"and an Nvidia Ampere GPU.\"\n        ),\n    )\n\n    args = parser.parse_args()\n    return args", "\n\ndef main():\n    args = parse_args()\n\n\n    # Setting\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        mixed_precision=args.mixed_precision,\n    )\n\n    if args.seed is not None:\n        set_seed(args.seed)\n        g_cuda = torch.Generator(device='cuda')\n        g_cuda.manual_seed(args.seed)\n    \n    os.makedirs(args.output_dir, exist_ok=True)\n    if args.use_8bit_adam:\n        try:\n            import bitsandbytes as bnb\n        except ImportError:\n            raise ImportError(\n                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n            )\n        optimizer_class = bnb.optim.Adam8bit\n    else:\n        optimizer_class = torch.optim.Adam\n\n    weight_dtype = torch.float32\n    if args.mixed_precision == \"fp16\":\n        weight_dtype = torch.float16\n    elif args.mixed_precision == \"bf16\":\n        weight_dtype = torch.bfloat16\n        \n        \n    # Load pretrained models    \n    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name, subfolder=\"tokenizer\", use_auth_token=True)\n    CLIP_text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name, subfolder=\"text_encoder\", use_auth_token=True)\n    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name, subfolder=\"vae\", use_auth_token=True)\n    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name, subfolder=\"unet\", use_auth_token=True)\n    noise_scheduler = DDPMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n    \n\n    # Encode the input image.\n    vae.to(accelerator.device, dtype=weight_dtype)\n    input_image = Image.open(args.input_image).convert(\"RGB\")\n    image_transforms = transforms.Compose(\n        [\n            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n            transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]),\n        ]\n    )\n    init_image = image_transforms(input_image)\n    init_image = init_image[None].to(device=accelerator.device, dtype=weight_dtype)\n    with torch.inference_mode():\n        init_latents = vae.encode(init_image).latent_dist.sample()\n        init_latents = 0.18215 * init_latents\n\n\n    # Encode the source and target text.\n    CLIP_text_encoder.to(accelerator.device, dtype=weight_dtype)\n    text_ids_tgt = tokenizer(args.target_text, padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length, return_tensors=\"pt\").input_ids\n    text_ids_tgt = text_ids_tgt.to(device=accelerator.device)\n\n    text_ids_src = tokenizer(args.source_text,padding=\"max_length\",truncation=True,max_length=tokenizer.model_max_length,return_tensors=\"pt\").input_ids\n    text_ids_src = text_ids_src.to(device=accelerator.device)\n    with torch.inference_mode():\n        target_embeddings = CLIP_text_encoder(text_ids_tgt)[0].float()\n        source_embeddings = CLIP_text_encoder(text_ids_src)[0].float()\n    \n    \n    # del vae, CLIP_text_encoder\n    del vae, CLIP_text_encoder\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\n    # For inference\n    ddim_scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n    pipe = StableDiffusionPipeline.from_pretrained(args.pretrained_model_name, scheduler=ddim_scheduler, torch_dtype=torch.float16).to(\"cuda\")\n    num_samples = 1 \n    guidance_scale = 7.5 \n    num_inference_steps = 50\n    height = 512\n    width = 512\n    \n    \n    # Optimize hiper embedding\n    n_hiper = args.n_hiper\n    hiper_embeddings = source_embeddings[:,-n_hiper:].clone().detach()\n    src_embeddings = source_embeddings[:,:-n_hiper].clone().detach()\n    tgt_embeddings = target_embeddings[:,:-n_hiper].clone().detach()\n    hiper_embeddings.requires_grad_(True)\n    \n    \n    optimizer = optimizer_class(\n        [hiper_embeddings],  \n        lr=args.emb_learning_rate,\n        betas=(args.adam_beta1, args.adam_beta2),\n        eps=args.adam_epsilon,\n    )\n\n    unet, optimizer = accelerator.prepare(unet, optimizer)\n\n\n    def train_loop(pbar, optimizer, hiper_embeddings):\n        for step in pbar:\n            with accelerator.accumulate(unet):\n                \n                noise = torch.randn_like(init_latents)\n                bsz = init_latents.shape[0]\n                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=init_latents.device)\n                timesteps = timesteps.long()\n\n                noisy_latents = noise_scheduler.add_noise(init_latents, noise, timesteps)\n\n                source_embeddings = torch.cat([src_embeddings, hiper_embeddings], 1)\n                noise_pred = unet(noisy_latents, timesteps, source_embeddings).sample\n                loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n\n                accelerator.backward(loss)\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n            \n            # Check inference\n            if step%100==0:\n                inf_emb = []\n                inf_emb.append(torch.cat([src_embeddings, hiper_embeddings.clone().detach()], 1))\n                inf_emb.append(torch.cat([tgt_embeddings, hiper_embeddings.clone().detach()*0.8], 1))\n                inf_emb.append(target_embeddings)\n                \n                inf_images=[]    \n                inf_images.append(input_image)            \n                with autocast(\"cuda\"), torch.inference_mode():     \n                    for i in range(3):   \n                        images = pipe(text_embeddings=inf_emb[i], height=height, width=width, num_images_per_prompt=num_samples,\n                            num_inference_steps=num_inference_steps, guidance_scale=guidance_scale, generator=g_cuda).images\n                        inf_images.append(images[0])\n\n                save_name = os.path.join(args.output_dir, 'training_results_seed{}_step{}.png'.format(args.seed, step))\n                inf_save(inf_images, ['source_img', '[src, hper]', '[tgt, hper]', '[tgt]'], save_name)\n                del images\n                \n                torch.save(hiper_embeddings.cpu(), os.path.join(args.output_dir, \"hiper_embeddings_step{}.pt\".format(step)))\n\n        accelerator.wait_for_everyone()\n    \n    progress_bar = tqdm(range(args.emb_train_steps), disable=not accelerator.is_local_main_process)\n    progress_bar.set_description(\"Optimizing embedding\")\n    \n    train_loop(progress_bar, optimizer, hiper_embeddings)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "utils.py", "chunked_list": ["import numpy as np\nimport torch\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\nfrom typing import Optional, Union, Tuple, List, Callable, Dict\nfrom tqdm.notebook import tqdm\n\n#codes for 'save_image' and 'text_under_image' are from \n# https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_stable.ipynb\n\ndef save_images(images, num_rows=1, offset_ratio=0.02, name=None):\n    if type(images) is list:\n        num_empty = len(images) % num_rows\n    elif images.ndim == 4:\n        num_empty = images.shape[0] % num_rows\n    else:\n        images = [images]\n        num_empty = 0\n\n    empty_images = np.ones(images[0].shape, dtype=np.uint8) * 255\n    images = [image.astype(np.uint8) for image in images] + [empty_images] * num_empty\n    num_items = len(images)\n\n    h, w, c = images[0].shape\n    offset = int(h * offset_ratio)\n    num_cols = num_items // num_rows\n    image_ = np.ones((h * num_rows + offset * (num_rows - 1),\n                      w * num_cols + offset * (num_cols - 1), 3), dtype=np.uint8) * 255\n    for i in range(num_rows):\n        for j in range(num_cols):\n            image_[i * (h + offset): i * (h + offset) + h:, j * (w + offset): j * (w + offset) + w] = images[\n                i * num_cols + j]\n\n    pil_img = Image.fromarray(image_)\n    pil_img.save(name)", "# https://github.com/google/prompt-to-prompt/blob/main/prompt-to-prompt_stable.ipynb\n\ndef save_images(images, num_rows=1, offset_ratio=0.02, name=None):\n    if type(images) is list:\n        num_empty = len(images) % num_rows\n    elif images.ndim == 4:\n        num_empty = images.shape[0] % num_rows\n    else:\n        images = [images]\n        num_empty = 0\n\n    empty_images = np.ones(images[0].shape, dtype=np.uint8) * 255\n    images = [image.astype(np.uint8) for image in images] + [empty_images] * num_empty\n    num_items = len(images)\n\n    h, w, c = images[0].shape\n    offset = int(h * offset_ratio)\n    num_cols = num_items // num_rows\n    image_ = np.ones((h * num_rows + offset * (num_rows - 1),\n                      w * num_cols + offset * (num_cols - 1), 3), dtype=np.uint8) * 255\n    for i in range(num_rows):\n        for j in range(num_cols):\n            image_[i * (h + offset): i * (h + offset) + h:, j * (w + offset): j * (w + offset) + w] = images[\n                i * num_cols + j]\n\n    pil_img = Image.fromarray(image_)\n    pil_img.save(name)", "    \n    \ndef text_under_image(image: np.ndarray, text: str, text_color: Tuple[int, int, int] = (0, 0, 0)):\n    h, w, c = image.shape\n    offset = int(h * .2)\n    img = np.ones((h + offset, w, c), dtype=np.uint8) * 255\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    img[:h] = image\n    textsize = cv2.getTextSize(text, font, 1, 2)[0]\n    text_x, text_y = (w - textsize[0]) // 2, h + offset - textsize[1] // 2\n    cv2.putText(img, text, (text_x, text_y ), font, 1, text_color, 2)\n    return img", "\n\ndef inf_save(inf_img, name, save_name):\n    images = []\n    for i in range(len(inf_img)):\n        image = np.array(inf_img[i].resize((256,256)))\n        image = text_under_image(image, name[i])\n        images.append(image)\n    save_images(np.stack(images, axis=0), name = save_name)\n    ", "    \n"]}
{"filename": "diffusers_/configuration_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2022 The HuggingFace Inc. team.\n# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" ConfigMixin base class and utilities.\"\"\"\nimport dataclasses\nimport functools", "import dataclasses\nimport functools\nimport importlib\nimport inspect\nimport json\nimport os\nimport re\nfrom collections import OrderedDict\nfrom typing import Any, Dict, Tuple, Union\n", "from typing import Any, Dict, Tuple, Union\n\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\nfrom requests import HTTPError\n\nfrom . import __version__\nfrom .utils import DIFFUSERS_CACHE, HUGGINGFACE_CO_RESOLVE_ENDPOINT, DummyObject, deprecate, logging\n\n", "\n\nlogger = logging.get_logger(__name__)\n\n_re_configuration_file = re.compile(r\"config\\.(.*)\\.json\")\n\n\nclass FrozenDict(OrderedDict):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        for key, value in self.items():\n            setattr(self, key, value)\n\n        self.__frozen = True\n\n    def __delitem__(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n\n    def setdefault(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n\n    def pop(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n\n    def update(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n\n    def __setattr__(self, name, value):\n        if hasattr(self, \"__frozen\") and self.__frozen:\n            raise Exception(f\"You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.\")\n        super().__setattr__(name, value)\n\n    def __setitem__(self, name, value):\n        if hasattr(self, \"__frozen\") and self.__frozen:\n            raise Exception(f\"You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.\")\n        super().__setitem__(name, value)", "\n\nclass ConfigMixin:\n    r\"\"\"\n    Base class for all configuration classes. Stores all configuration parameters under `self.config` Also handles all\n    methods for loading/downloading/saving classes inheriting from [`ConfigMixin`] with\n        - [`~ConfigMixin.from_config`]\n        - [`~ConfigMixin.save_config`]\n\n    Class attributes:\n        - **config_name** (`str`) -- A filename under which the config should stored when calling\n          [`~ConfigMixin.save_config`] (should be overridden by parent class).\n        - **ignore_for_config** (`List[str]`) -- A list of attributes that should not be saved in the config (should be\n          overridden by subclass).\n        - **has_compatibles** (`bool`) -- Whether the class has compatible classes (should be overridden by subclass).\n        - **_deprecated_kwargs** (`List[str]`) -- Keyword arguments that are deprecated. Note that the init function\n          should only have a `kwargs` argument if at least one argument is deprecated (should be overridden by\n          subclass).\n    \"\"\"\n    config_name = None\n    ignore_for_config = []\n    has_compatibles = False\n\n    _deprecated_kwargs = []\n\n    def register_to_config(self, **kwargs):\n        if self.config_name is None:\n            raise NotImplementedError(f\"Make sure that {self.__class__} has defined a class name `config_name`\")\n        # Special case for `kwargs` used in deprecation warning added to schedulers\n        # TODO: remove this when we remove the deprecation warning, and the `kwargs` argument,\n        # or solve in a more general way.\n        kwargs.pop(\"kwargs\", None)\n        for key, value in kwargs.items():\n            try:\n                setattr(self, key, value)\n            except AttributeError as err:\n                logger.error(f\"Can't set {key} with value {value} for {self}\")\n                raise err\n\n        if not hasattr(self, \"_internal_dict\"):\n            internal_dict = kwargs\n        else:\n            previous_dict = dict(self._internal_dict)\n            internal_dict = {**self._internal_dict, **kwargs}\n            logger.debug(f\"Updating config from {previous_dict} to {internal_dict}\")\n\n        self._internal_dict = FrozenDict(internal_dict)\n\n    def save_config(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n        \"\"\"\n        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the\n        [`~ConfigMixin.from_config`] class method.\n\n        Args:\n            save_directory (`str` or `os.PathLike`):\n                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        # If we save using the predefined names, we can load using `from_config`\n        output_config_file = os.path.join(save_directory, self.config_name)\n\n        self.to_json_file(output_config_file)\n        logger.info(f\"Configuration saved in {output_config_file}\")\n\n    @classmethod\n    def from_config(cls, config: Union[FrozenDict, Dict[str, Any]] = None, return_unused_kwargs=False, **kwargs):\n        r\"\"\"\n        Instantiate a Python class from a config dictionary\n\n        Parameters:\n            config (`Dict[str, Any]`):\n                A config dictionary from which the Python class will be instantiated. Make sure to only load\n                configuration files of compatible classes.\n            return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n                Whether kwargs that are not consumed by the Python class should be returned or not.\n\n            kwargs (remaining dictionary of keyword arguments, *optional*):\n                Can be used to update the configuration object (after it being loaded) and initiate the Python class.\n                `**kwargs` will be directly passed to the underlying scheduler/model's `__init__` method and eventually\n                overwrite same named arguments of `config`.\n\n        Examples:\n\n        ```python\n        >>> from diffusers import DDPMScheduler, DDIMScheduler, PNDMScheduler\n\n        >>> # Download scheduler from huggingface.co and cache.\n        >>> scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cifar10-32\")\n\n        >>> # Instantiate DDIM scheduler class with same config as DDPM\n        >>> scheduler = DDIMScheduler.from_config(scheduler.config)\n\n        >>> # Instantiate PNDM scheduler class with same config as DDPM\n        >>> scheduler = PNDMScheduler.from_config(scheduler.config)\n        ```\n        \"\"\"\n        # <===== TO BE REMOVED WITH DEPRECATION\n        # TODO(Patrick) - make sure to remove the following lines when config==\"model_path\" is deprecated\n        if \"pretrained_model_name_or_path\" in kwargs:\n            config = kwargs.pop(\"pretrained_model_name_or_path\")\n\n        if config is None:\n            raise ValueError(\"Please make sure to provide a config as the first positional argument.\")\n        # ======>\n\n        if not isinstance(config, dict):\n            deprecation_message = \"It is deprecated to pass a pretrained model name or path to `from_config`.\"\n            if \"Scheduler\" in cls.__name__:\n                deprecation_message += (\n                    f\"If you were trying to load a scheduler, please use {cls}.from_pretrained(...) instead.\"\n                    \" Otherwise, please make sure to pass a configuration dictionary instead. This functionality will\"\n                    \" be removed in v1.0.0.\"\n                )\n            elif \"Model\" in cls.__name__:\n                deprecation_message += (\n                    f\"If you were trying to load a model, please use {cls}.load_config(...) followed by\"\n                    f\" {cls}.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary\"\n                    \" instead. This functionality will be removed in v1.0.0.\"\n                )\n            deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n            config, kwargs = cls.load_config(pretrained_model_name_or_path=config, return_unused_kwargs=True, **kwargs)\n\n        init_dict, unused_kwargs, hidden_dict = cls.extract_init_dict(config, **kwargs)\n\n        # Allow dtype to be specified on initialization\n        if \"dtype\" in unused_kwargs:\n            init_dict[\"dtype\"] = unused_kwargs.pop(\"dtype\")\n\n        # add possible deprecated kwargs\n        for deprecated_kwarg in cls._deprecated_kwargs:\n            if deprecated_kwarg in unused_kwargs:\n                init_dict[deprecated_kwarg] = unused_kwargs.pop(deprecated_kwarg)\n\n        # Return model and optionally state and/or unused_kwargs\n        model = cls(**init_dict)\n\n        # make sure to also save config parameters that might be used for compatible classes\n        model.register_to_config(**hidden_dict)\n\n        # add hidden kwargs of compatible classes to unused_kwargs\n        unused_kwargs = {**unused_kwargs, **hidden_dict}\n\n        if return_unused_kwargs:\n            return (model, unused_kwargs)\n        else:\n            return model\n\n    @classmethod\n    def get_config_dict(cls, *args, **kwargs):\n        deprecation_message = (\n            f\" The function get_config_dict is deprecated. Please use {cls}.load_config instead. This function will be\"\n            \" removed in version v1.0.0\"\n        )\n        deprecate(\"get_config_dict\", \"1.0.0\", deprecation_message, standard_warn=False)\n        return cls.load_config(*args, **kwargs)\n\n    @classmethod\n    def load_config(\n        cls, pretrained_model_name_or_path: Union[str, os.PathLike], return_unused_kwargs=False, **kwargs\n    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n        r\"\"\"\n        Instantiate a Python class from a config dictionary\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n\n                    - A string, the *model id* of a model repo on huggingface.co. Valid model ids should have an\n                      organization name, like `google/ddpm-celebahq-256`.\n                    - A path to a *directory* containing model weights saved using [`~ConfigMixin.save_config`], e.g.,\n                      `./my_model_directory/`.\n\n            cache_dir (`Union[str, os.PathLike]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            resume_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n                file exists.\n            proxies (`Dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n            output_loading_info(`bool`, *optional*, defaults to `False`):\n                Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n            local_files_only(`bool`, *optional*, defaults to `False`):\n                Whether or not to only look at local files (i.e., do not try to download the model).\n            use_auth_token (`str` or *bool*, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n                when running `transformers-cli login` (stored in `~/.huggingface`).\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n                identifier allowed by git.\n            subfolder (`str`, *optional*, defaults to `\"\"`):\n                In case the relevant files are located inside a subfolder of the model repo (either remote in\n                huggingface.co or downloaded locally), you can specify the folder name here.\n\n        <Tip>\n\n         It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n         models](https://huggingface.co/docs/hub/models-gated#gated-models).\n\n        </Tip>\n\n        <Tip>\n\n        Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n        use this method in a firewalled environment.\n\n        </Tip>\n        \"\"\"\n        cache_dir = kwargs.pop(\"cache_dir\", DIFFUSERS_CACHE)\n        force_download = kwargs.pop(\"force_download\", False)\n        resume_download = kwargs.pop(\"resume_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n        local_files_only = kwargs.pop(\"local_files_only\", False)\n        revision = kwargs.pop(\"revision\", None)\n        _ = kwargs.pop(\"mirror\", None)\n        subfolder = kwargs.pop(\"subfolder\", None)\n\n        user_agent = {\"file_type\": \"config\"}\n\n        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n\n        if cls.config_name is None:\n            raise ValueError(\n                \"`self.config_name` is not defined. Note that one should not load a config from \"\n                \"`ConfigMixin`. Please make sure to define `config_name` in a class inheriting from `ConfigMixin`\"\n            )\n\n        if os.path.isfile(pretrained_model_name_or_path):\n            config_file = pretrained_model_name_or_path\n        elif os.path.isdir(pretrained_model_name_or_path):\n            if os.path.isfile(os.path.join(pretrained_model_name_or_path, cls.config_name)):\n                # Load from a PyTorch checkpoint\n                config_file = os.path.join(pretrained_model_name_or_path, cls.config_name)\n            elif subfolder is not None and os.path.isfile(\n                os.path.join(pretrained_model_name_or_path, subfolder, cls.config_name)\n            ):\n                config_file = os.path.join(pretrained_model_name_or_path, subfolder, cls.config_name)\n            else:\n                raise EnvironmentError(\n                    f\"Error no file named {cls.config_name} found in directory {pretrained_model_name_or_path}.\"\n                )\n        else:\n            try:\n                # Load from URL or cache if already cached\n                config_file = hf_hub_download(\n                    pretrained_model_name_or_path,\n                    filename=cls.config_name,\n                    cache_dir=cache_dir,\n                    force_download=force_download,\n                    proxies=proxies,\n                    resume_download=resume_download,\n                    local_files_only=local_files_only,\n                    use_auth_token=use_auth_token,\n                    user_agent=user_agent,\n                    subfolder=subfolder,\n                    revision=revision,\n                )\n\n            except RepositoryNotFoundError:\n                raise EnvironmentError(\n                    f\"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier\"\n                    \" listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a\"\n                    \" token having permission to this repo with `use_auth_token` or log in with `huggingface-cli\"\n                    \" login`.\"\n                )\n            except RevisionNotFoundError:\n                raise EnvironmentError(\n                    f\"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for\"\n                    \" this model name. Check the model page at\"\n                    f\" 'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions.\"\n                )\n            except EntryNotFoundError:\n                raise EnvironmentError(\n                    f\"{pretrained_model_name_or_path} does not appear to have a file named {cls.config_name}.\"\n                )\n            except HTTPError as err:\n                raise EnvironmentError(\n                    \"There was a specific connection error when trying to load\"\n                    f\" {pretrained_model_name_or_path}:\\n{err}\"\n                )\n            except ValueError:\n                raise EnvironmentError(\n                    f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it\"\n                    f\" in the cached files and it looks like {pretrained_model_name_or_path} is not the path to a\"\n                    f\" directory containing a {cls.config_name} file.\\nCheckout your internet connection or see how to\"\n                    \" run the library in offline mode at\"\n                    \" 'https://huggingface.co/docs/diffusers/installation#offline-mode'.\"\n                )\n            except EnvironmentError:\n                raise EnvironmentError(\n                    f\"Can't load config for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\n                    \"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\n                    f\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\n                    f\"containing a {cls.config_name} file\"\n                )\n\n        try:\n            # Load config dict\n            config_dict = cls._dict_from_json_file(config_file)\n        except (json.JSONDecodeError, UnicodeDecodeError):\n            raise EnvironmentError(f\"It looks like the config file at '{config_file}' is not a valid JSON file.\")\n\n        if return_unused_kwargs:\n            return config_dict, kwargs\n\n        return config_dict\n\n    @staticmethod\n    def _get_init_keys(cls):\n        return set(dict(inspect.signature(cls.__init__).parameters).keys())\n\n    @classmethod\n    def extract_init_dict(cls, config_dict, **kwargs):\n        # 0. Copy origin config dict\n        original_dict = {k: v for k, v in config_dict.items()}\n\n        # 1. Retrieve expected config attributes from __init__ signature\n        expected_keys = cls._get_init_keys(cls)\n        expected_keys.remove(\"self\")\n        # remove general kwargs if present in dict\n        if \"kwargs\" in expected_keys:\n            expected_keys.remove(\"kwargs\")\n        # remove flax internal keys\n        if hasattr(cls, \"_flax_internal_args\"):\n            for arg in cls._flax_internal_args:\n                expected_keys.remove(arg)\n\n        # 2. Remove attributes that cannot be expected from expected config attributes\n        # remove keys to be ignored\n        if len(cls.ignore_for_config) > 0:\n            expected_keys = expected_keys - set(cls.ignore_for_config)\n\n        # load diffusers library to import compatible and original scheduler\n        diffusers_library = importlib.import_module(__name__.split(\".\")[0])\n\n        if cls.has_compatibles:\n            compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]\n        else:\n            compatible_classes = []\n\n        expected_keys_comp_cls = set()\n        for c in compatible_classes:\n            expected_keys_c = cls._get_init_keys(c)\n            expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)\n        expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)\n        config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}\n\n        # remove attributes from orig class that cannot be expected\n        orig_cls_name = config_dict.pop(\"_class_name\", cls.__name__)\n        if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):\n            orig_cls = getattr(diffusers_library, orig_cls_name)\n            unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys\n            config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}\n\n        # remove private attributes\n        config_dict = {k: v for k, v in config_dict.items() if not k.startswith(\"_\")}\n\n        # 3. Create keyword arguments that will be passed to __init__ from expected keyword arguments\n        init_dict = {}\n        for key in expected_keys:\n            # if config param is passed to kwarg and is present in config dict\n            # it should overwrite existing config dict key\n            if key in kwargs and key in config_dict:\n                config_dict[key] = kwargs.pop(key)\n\n            if key in kwargs:\n                # overwrite key\n                init_dict[key] = kwargs.pop(key)\n            elif key in config_dict:\n                # use value from config dict\n                init_dict[key] = config_dict.pop(key)\n\n        # 4. Give nice warning if unexpected values have been passed\n        if len(config_dict) > 0:\n            logger.warning(\n                f\"The config attributes {config_dict} were passed to {cls.__name__}, \"\n                \"but are not expected and will be ignored. Please verify your \"\n                f\"{cls.config_name} configuration file.\"\n            )\n\n        # 5. Give nice info if config attributes are initiliazed to default because they have not been passed\n        passed_keys = set(init_dict.keys())\n        if len(expected_keys - passed_keys) > 0:\n            logger.info(\n                f\"{expected_keys - passed_keys} was not found in config. Values will be initialized to default values.\"\n            )\n\n        # 6. Define unused keyword arguments\n        unused_kwargs = {**config_dict, **kwargs}\n\n        # 7. Define \"hidden\" config parameters that were saved for compatible classes\n        hidden_config_dict = {k: v for k, v in original_dict.items() if k not in init_dict}\n\n        return init_dict, unused_kwargs, hidden_config_dict\n\n    @classmethod\n    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n            text = reader.read()\n        return json.loads(text)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__} {self.to_json_string()}\"\n\n    @property\n    def config(self) -> Dict[str, Any]:\n        \"\"\"\n        Returns the config of the class as a frozen dictionary\n\n        Returns:\n            `Dict[str, Any]`: Config of the class.\n        \"\"\"\n        return self._internal_dict\n\n    def to_json_string(self) -> str:\n        \"\"\"\n        Serializes this instance to a JSON string.\n\n        Returns:\n            `str`: String containing all the attributes that make up this configuration instance in JSON format.\n        \"\"\"\n        config_dict = self._internal_dict if hasattr(self, \"_internal_dict\") else {}\n        config_dict[\"_class_name\"] = self.__class__.__name__\n        config_dict[\"_diffusers_version\"] = __version__\n\n        return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n\n    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n        \"\"\"\n        Save this instance to a JSON file.\n\n        Args:\n            json_file_path (`str` or `os.PathLike`):\n                Path to the JSON file in which this configuration instance's parameters will be saved.\n        \"\"\"\n        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n            writer.write(self.to_json_string())", "\n\ndef register_to_config(init):\n    r\"\"\"\n    Decorator to apply on the init of classes inheriting from [`ConfigMixin`] so that all the arguments are\n    automatically sent to `self.register_for_config`. To ignore a specific argument accepted by the init but that\n    shouldn't be registered in the config, use the `ignore_for_config` class variable\n\n    Warning: Once decorated, all private arguments (beginning with an underscore) are trashed and not sent to the init!\n    \"\"\"\n\n    @functools.wraps(init)\n    def inner_init(self, *args, **kwargs):\n        # Ignore private kwargs in the init.\n        init_kwargs = {k: v for k, v in kwargs.items() if not k.startswith(\"_\")}\n        config_init_kwargs = {k: v for k, v in kwargs.items() if k.startswith(\"_\")}\n        if not isinstance(self, ConfigMixin):\n            raise RuntimeError(\n                f\"`@register_for_config` was applied to {self.__class__.__name__} init method, but this class does \"\n                \"not inherit from `ConfigMixin`.\"\n            )\n\n        ignore = getattr(self, \"ignore_for_config\", [])\n        # Get positional arguments aligned with kwargs\n        new_kwargs = {}\n        signature = inspect.signature(init)\n        parameters = {\n            name: p.default for i, (name, p) in enumerate(signature.parameters.items()) if i > 0 and name not in ignore\n        }\n        for arg, name in zip(args, parameters.keys()):\n            new_kwargs[name] = arg\n\n        # Then add all kwargs\n        new_kwargs.update(\n            {\n                k: init_kwargs.get(k, default)\n                for k, default in parameters.items()\n                if k not in ignore and k not in new_kwargs\n            }\n        )\n        new_kwargs = {**config_init_kwargs, **new_kwargs}\n        getattr(self, \"register_to_config\")(**new_kwargs)\n        init(self, *args, **init_kwargs)\n\n    return inner_init", "\n\ndef flax_register_to_config(cls):\n    original_init = cls.__init__\n\n    @functools.wraps(original_init)\n    def init(self, *args, **kwargs):\n        if not isinstance(self, ConfigMixin):\n            raise RuntimeError(\n                f\"`@register_for_config` was applied to {self.__class__.__name__} init method, but this class does \"\n                \"not inherit from `ConfigMixin`.\"\n            )\n\n        # Ignore private kwargs in the init. Retrieve all passed attributes\n        init_kwargs = {k: v for k, v in kwargs.items()}\n\n        # Retrieve default values\n        fields = dataclasses.fields(self)\n        default_kwargs = {}\n        for field in fields:\n            # ignore flax specific attributes\n            if field.name in self._flax_internal_args:\n                continue\n            if type(field.default) == dataclasses._MISSING_TYPE:\n                default_kwargs[field.name] = None\n            else:\n                default_kwargs[field.name] = getattr(self, field.name)\n\n        # Make sure init_kwargs override default kwargs\n        new_kwargs = {**default_kwargs, **init_kwargs}\n        # dtype should be part of `init_kwargs`, but not `new_kwargs`\n        if \"dtype\" in new_kwargs:\n            new_kwargs.pop(\"dtype\")\n\n        # Get positional arguments aligned with kwargs\n        for i, arg in enumerate(args):\n            name = fields[i].name\n            new_kwargs[name] = arg\n\n        getattr(self, \"register_to_config\")(**new_kwargs)\n        original_init(self, *args, **kwargs)\n\n    cls.__init__ = init\n    return cls", ""]}
{"filename": "diffusers_/modeling_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2022 The HuggingFace Inc. team.\n# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom functools import partial", "import os\nfrom functools import partial\nfrom typing import Callable, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import Tensor, device\n\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\nfrom requests import HTTPError", "from huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError, RevisionNotFoundError\nfrom requests import HTTPError\n\nfrom . import __version__\nfrom .utils import (\n    CONFIG_NAME,\n    DIFFUSERS_CACHE,\n    HUGGINGFACE_CO_RESOLVE_ENDPOINT,\n    WEIGHTS_NAME,\n    is_accelerate_available,", "    WEIGHTS_NAME,\n    is_accelerate_available,\n    is_torch_version,\n    logging,\n)\n\n\nlogger = logging.get_logger(__name__)\n\n\nif is_torch_version(\">=\", \"1.9.0\"):\n    _LOW_CPU_MEM_USAGE_DEFAULT = True\nelse:\n    _LOW_CPU_MEM_USAGE_DEFAULT = False", "\n\nif is_torch_version(\">=\", \"1.9.0\"):\n    _LOW_CPU_MEM_USAGE_DEFAULT = True\nelse:\n    _LOW_CPU_MEM_USAGE_DEFAULT = False\n\n\nif is_accelerate_available():\n    import accelerate\n    from accelerate.utils import set_module_tensor_to_device\n    from accelerate.utils.versions import is_torch_version", "if is_accelerate_available():\n    import accelerate\n    from accelerate.utils import set_module_tensor_to_device\n    from accelerate.utils.versions import is_torch_version\n\n\ndef get_parameter_device(parameter: torch.nn.Module):\n    try:\n        return next(parameter.parameters()).device\n    except StopIteration:\n        # For torch.nn.DataParallel compatibility in PyTorch 1.5\n\n        def find_tensor_attributes(module: torch.nn.Module) -> List[Tuple[str, Tensor]]:\n            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n            return tuples\n\n        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n        first_tuple = next(gen)\n        return first_tuple[1].device", "\n\ndef get_parameter_dtype(parameter: torch.nn.Module):\n    try:\n        return next(parameter.parameters()).dtype\n    except StopIteration:\n        # For torch.nn.DataParallel compatibility in PyTorch 1.5\n\n        def find_tensor_attributes(module: torch.nn.Module) -> List[Tuple[str, Tensor]]:\n            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n            return tuples\n\n        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n        first_tuple = next(gen)\n        return first_tuple[1].dtype", "\n\ndef load_state_dict(checkpoint_file: Union[str, os.PathLike]):\n    \"\"\"\n    Reads a PyTorch checkpoint file, returning properly formatted errors if they arise.\n    \"\"\"\n    try:\n        return torch.load(checkpoint_file, map_location=\"cpu\")\n    except Exception as e:\n        try:\n            with open(checkpoint_file) as f:\n                if f.read().startswith(\"version\"):\n                    raise OSError(\n                        \"You seem to have cloned a repository without having git-lfs installed. Please install \"\n                        \"git-lfs and run `git lfs install` followed by `git lfs pull` in the folder \"\n                        \"you cloned.\"\n                    )\n                else:\n                    raise ValueError(\n                        f\"Unable to locate the file {checkpoint_file} which is necessary to load this pretrained \"\n                        \"model. Make sure you have saved the model properly.\"\n                    ) from e\n        except (UnicodeDecodeError, ValueError):\n            raise OSError(\n                f\"Unable to load weights from pytorch checkpoint file for '{checkpoint_file}' \"\n                f\"at '{checkpoint_file}'. \"\n                \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.\"\n            )", "\n\ndef _load_state_dict_into_model(model_to_load, state_dict):\n    # Convert old format to new format if needed from a PyTorch state_dict\n    # copy state_dict so _load_from_state_dict can modify it\n    state_dict = state_dict.copy()\n    error_msgs = []\n\n    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n    # so we need to apply the function recursively.\n    def load(module: torch.nn.Module, prefix=\"\"):\n        args = (state_dict, prefix, {}, True, [], [], error_msgs)\n        module._load_from_state_dict(*args)\n\n        for name, child in module._modules.items():\n            if child is not None:\n                load(child, prefix + name + \".\")\n\n    load(model_to_load)\n\n    return error_msgs", "\n\nclass ModelMixin(torch.nn.Module):\n    r\"\"\"\n    Base class for all models.\n\n    [`ModelMixin`] takes care of storing the configuration of the models and handles methods for loading, downloading\n    and saving models.\n\n        - **config_name** ([`str`]) -- A filename under which the model should be stored when calling\n          [`~modeling_utils.ModelMixin.save_pretrained`].\n    \"\"\"\n    config_name = CONFIG_NAME\n    _automatically_saved_args = [\"_diffusers_version\", \"_class_name\", \"_name_or_path\"]\n    _supports_gradient_checkpointing = False\n\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def is_gradient_checkpointing(self) -> bool:\n        \"\"\"\n        Whether gradient checkpointing is activated for this model or not.\n\n        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n        activations\".\n        \"\"\"\n        return any(hasattr(m, \"gradient_checkpointing\") and m.gradient_checkpointing for m in self.modules())\n\n    def enable_gradient_checkpointing(self):\n        \"\"\"\n        Activates gradient checkpointing for the current model.\n\n        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n        activations\".\n        \"\"\"\n        if not self._supports_gradient_checkpointing:\n            raise ValueError(f\"{self.__class__.__name__} does not support gradient checkpointing.\")\n        self.apply(partial(self._set_gradient_checkpointing, value=True))\n\n    def disable_gradient_checkpointing(self):\n        \"\"\"\n        Deactivates gradient checkpointing for the current model.\n\n        Note that in other frameworks this feature can be referred to as \"activation checkpointing\" or \"checkpoint\n        activations\".\n        \"\"\"\n        if self._supports_gradient_checkpointing:\n            self.apply(partial(self._set_gradient_checkpointing, value=False))\n\n    def save_pretrained(\n        self,\n        save_directory: Union[str, os.PathLike],\n        is_main_process: bool = True,\n        save_function: Callable = torch.save,\n    ):\n        \"\"\"\n        Save a model and its configuration file to a directory, so that it can be re-loaded using the\n        `[`~modeling_utils.ModelMixin.from_pretrained`]` class method.\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n            is_main_process (`bool`, *optional*, defaults to `True`):\n                Whether the process calling this is the main process or not. Useful when in distributed training like\n                TPUs and need to call this function on all processes. In this case, set `is_main_process=True` only on\n                the main process to avoid race conditions.\n            save_function (`Callable`):\n                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n                need to replace `torch.save` by another method.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            logger.error(f\"Provided path ({save_directory}) should be a directory, not a file\")\n            return\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        model_to_save = self\n\n        # Attach architecture to the config\n        # Save the config\n        if is_main_process:\n            model_to_save.save_config(save_directory)\n\n        # Save the model\n        state_dict = model_to_save.state_dict()\n\n        # Clean the folder from a previous save\n        for filename in os.listdir(save_directory):\n            full_filename = os.path.join(save_directory, filename)\n            # If we have a shard file that is not going to be replaced, we delete it, but only from the main process\n            # in distributed settings to avoid race conditions.\n            if filename.startswith(WEIGHTS_NAME[:-4]) and os.path.isfile(full_filename) and is_main_process:\n                os.remove(full_filename)\n\n        # Save the model\n        save_function(state_dict, os.path.join(save_directory, WEIGHTS_NAME))\n\n        logger.info(f\"Model weights saved in {os.path.join(save_directory, WEIGHTS_NAME)}\")\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n        r\"\"\"\n        Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train\n        the model, you should first set it back in training mode with `model.train()`.\n\n        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n        task.\n\n        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n        weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n\n                    - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n                      Valid model ids should have an organization name, like `google/ddpm-celebahq-256`.\n                    - A path to a *directory* containing model weights saved using [`~ModelMixin.save_config`], e.g.,\n                      `./my_model_directory/`.\n\n            cache_dir (`Union[str, os.PathLike]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            torch_dtype (`str` or `torch.dtype`, *optional*):\n                Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n                will be automatically derived from the model's weights.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            resume_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n                file exists.\n            proxies (`Dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n            output_loading_info(`bool`, *optional*, defaults to `False`):\n                Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n            local_files_only(`bool`, *optional*, defaults to `False`):\n                Whether or not to only look at local files (i.e., do not try to download the model).\n            use_auth_token (`str` or *bool*, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n                when running `diffusers-cli login` (stored in `~/.huggingface`).\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n                identifier allowed by git.\n            subfolder (`str`, *optional*, defaults to `\"\"`):\n                In case the relevant files are located inside a subfolder of the model repo (either remote in\n                huggingface.co or downloaded locally), you can specify the folder name here.\n\n            mirror (`str`, *optional*):\n                Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n                Please refer to the mirror site for more information.\n            device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n                A map that specifies where each submodule should go. It doesn't need to be refined to each\n                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n                same device.\n\n                To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n                more information about each option see [designing a device\n                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n            low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n                Speed up model loading by not initializing the weights and only loading the pre-trained weights. This\n                also tries to not use more than 1x model size in CPU memory (including peak memory) while loading the\n                model. This is only supported when torch version >= 1.9.0. If you are using an older version of torch,\n                setting this argument to `True` will raise an error.\n\n        <Tip>\n\n         It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n         models](https://huggingface.co/docs/hub/models-gated#gated-models).\n\n        </Tip>\n\n        <Tip>\n\n        Activate the special [\"offline-mode\"](https://huggingface.co/diffusers/installation.html#offline-mode) to use\n        this method in a firewalled environment.\n\n        </Tip>\n\n        \"\"\"\n        cache_dir = kwargs.pop(\"cache_dir\", DIFFUSERS_CACHE)\n        ignore_mismatched_sizes = kwargs.pop(\"ignore_mismatched_sizes\", False)\n        force_download = kwargs.pop(\"force_download\", False)\n        resume_download = kwargs.pop(\"resume_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        output_loading_info = kwargs.pop(\"output_loading_info\", False)\n        local_files_only = kwargs.pop(\"local_files_only\", False)\n        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n        revision = kwargs.pop(\"revision\", None)\n        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n        subfolder = kwargs.pop(\"subfolder\", None)\n        device_map = kwargs.pop(\"device_map\", None)\n        low_cpu_mem_usage = kwargs.pop(\"low_cpu_mem_usage\", _LOW_CPU_MEM_USAGE_DEFAULT)\n\n        if low_cpu_mem_usage and not is_accelerate_available():\n            low_cpu_mem_usage = False\n            logger.warning(\n                \"Cannot initialize model with low cpu memory usage because `accelerate` was not found in the\"\n                \" environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install\"\n                \" `accelerate` for faster and less memory-intense model loading. You can do so with: \\n```\\npip\"\n                \" install accelerate\\n```\\n.\"\n            )\n\n        if device_map is not None and not is_accelerate_available():\n            raise NotImplementedError(\n                \"Loading and dispatching requires `accelerate`. Please make sure to install accelerate or set\"\n                \" `device_map=None`. You can install accelerate with `pip install accelerate`.\"\n            )\n\n        # Check if we can handle device_map and dispatching the weights\n        if device_map is not None and not is_torch_version(\">=\", \"1.9.0\"):\n            raise NotImplementedError(\n                \"Loading and dispatching requires torch >= 1.9.0. Please either update your PyTorch version or set\"\n                \" `device_map=None`.\"\n            )\n\n        if low_cpu_mem_usage is True and not is_torch_version(\">=\", \"1.9.0\"):\n            raise NotImplementedError(\n                \"Low memory initialization requires torch >= 1.9.0. Please either update your PyTorch version or set\"\n                \" `low_cpu_mem_usage=False`.\"\n            )\n\n        if low_cpu_mem_usage is False and device_map is not None:\n            raise ValueError(\n                f\"You cannot set `low_cpu_mem_usage` to `False` while using device_map={device_map} for loading and\"\n                \" dispatching. Please make sure to set `low_cpu_mem_usage=True`.\"\n            )\n\n        user_agent = {\n            \"diffusers\": __version__,\n            \"file_type\": \"model\",\n            \"framework\": \"pytorch\",\n        }\n\n        # Load config if we don't provide a configuration\n        config_path = pretrained_model_name_or_path\n\n        # This variable will flag if we're loading a sharded checkpoint. In this case the archive file is just the\n        # Load model\n        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n        if os.path.isdir(pretrained_model_name_or_path):\n            if os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n                # Load from a PyTorch checkpoint\n                model_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n            elif subfolder is not None and os.path.isfile(\n                os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)\n            ):\n                model_file = os.path.join(pretrained_model_name_or_path, subfolder, WEIGHTS_NAME)\n            else:\n                raise EnvironmentError(\n                    f\"Error no file named {WEIGHTS_NAME} found in directory {pretrained_model_name_or_path}.\"\n                )\n        else:\n            try:\n                # Load from URL or cache if already cached\n                model_file = hf_hub_download(\n                    pretrained_model_name_or_path,\n                    filename=WEIGHTS_NAME,\n                    cache_dir=cache_dir,\n                    force_download=force_download,\n                    proxies=proxies,\n                    resume_download=resume_download,\n                    local_files_only=local_files_only,\n                    use_auth_token=use_auth_token,\n                    user_agent=user_agent,\n                    subfolder=subfolder,\n                    revision=revision,\n                )\n\n            except RepositoryNotFoundError:\n                raise EnvironmentError(\n                    f\"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier \"\n                    \"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a \"\n                    \"token having permission to this repo with `use_auth_token` or log in with `huggingface-cli \"\n                    \"login`.\"\n                )\n            except RevisionNotFoundError:\n                raise EnvironmentError(\n                    f\"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for \"\n                    \"this model name. Check the model page at \"\n                    f\"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions.\"\n                )\n            except EntryNotFoundError:\n                raise EnvironmentError(\n                    f\"{pretrained_model_name_or_path} does not appear to have a file named {WEIGHTS_NAME}.\"\n                )\n            except HTTPError as err:\n                raise EnvironmentError(\n                    \"There was a specific connection error when trying to load\"\n                    f\" {pretrained_model_name_or_path}:\\n{err}\"\n                )\n            except ValueError:\n                raise EnvironmentError(\n                    f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it\"\n                    f\" in the cached files and it looks like {pretrained_model_name_or_path} is not the path to a\"\n                    f\" directory containing a file named {WEIGHTS_NAME} or\"\n                    \" \\nCheckout your internet connection or see how to run the library in\"\n                    \" offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.\"\n                )\n            except EnvironmentError:\n                raise EnvironmentError(\n                    f\"Can't load the model for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\n                    \"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\n                    f\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\n                    f\"containing a file named {WEIGHTS_NAME}\"\n                )\n\n            # restore default dtype\n\n        if low_cpu_mem_usage:\n            # Instantiate model with empty weights\n            with accelerate.init_empty_weights():\n                config, unused_kwargs = cls.load_config(\n                    config_path,\n                    cache_dir=cache_dir,\n                    return_unused_kwargs=True,\n                    force_download=force_download,\n                    resume_download=resume_download,\n                    proxies=proxies,\n                    local_files_only=local_files_only,\n                    use_auth_token=use_auth_token,\n                    revision=revision,\n                    subfolder=subfolder,\n                    device_map=device_map,\n                    **kwargs,\n                )\n                model = cls.from_config(config, **unused_kwargs)\n\n            # if device_map is Non,e load the state dict on move the params from meta device to the cpu\n            if device_map is None:\n                param_device = \"cpu\"\n                state_dict = load_state_dict(model_file)\n                # move the parms from meta device to cpu\n                for param_name, param in state_dict.items():\n                    set_module_tensor_to_device(model, param_name, param_device, value=param)\n            else:  # else let accelerate handle loading and dispatching.\n                # Load weights and dispatch according to the device_map\n                # by deafult the device_map is None and the weights are loaded on the CPU\n                accelerate.load_checkpoint_and_dispatch(model, model_file, device_map)\n\n            loading_info = {\n                \"missing_keys\": [],\n                \"unexpected_keys\": [],\n                \"mismatched_keys\": [],\n                \"error_msgs\": [],\n            }\n        else:\n            config, unused_kwargs = cls.load_config(\n                config_path,\n                cache_dir=cache_dir,\n                return_unused_kwargs=True,\n                force_download=force_download,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=use_auth_token,\n                revision=revision,\n                subfolder=subfolder,\n                device_map=device_map,\n                **kwargs,\n            )\n            model = cls.from_config(config, **unused_kwargs)\n\n            state_dict = load_state_dict(model_file)\n            model, missing_keys, unexpected_keys, mismatched_keys, error_msgs = cls._load_pretrained_model(\n                model,\n                state_dict,\n                model_file,\n                pretrained_model_name_or_path,\n                ignore_mismatched_sizes=ignore_mismatched_sizes,\n            )\n\n            loading_info = {\n                \"missing_keys\": missing_keys,\n                \"unexpected_keys\": unexpected_keys,\n                \"mismatched_keys\": mismatched_keys,\n                \"error_msgs\": error_msgs,\n            }\n\n        if torch_dtype is not None and not isinstance(torch_dtype, torch.dtype):\n            raise ValueError(\n                f\"{torch_dtype} needs to be of type `torch.dtype`, e.g. `torch.float16`, but is {type(torch_dtype)}.\"\n            )\n        elif torch_dtype is not None:\n            model = model.to(torch_dtype)\n\n        model.register_to_config(_name_or_path=pretrained_model_name_or_path)\n\n        # Set model in evaluation mode to deactivate DropOut modules by default\n        model.eval()\n        if output_loading_info:\n            return model, loading_info\n\n        return model\n\n    @classmethod\n    def _load_pretrained_model(\n        cls,\n        model,\n        state_dict,\n        resolved_archive_file,\n        pretrained_model_name_or_path,\n        ignore_mismatched_sizes=False,\n    ):\n        # Retrieve missing & unexpected_keys\n        model_state_dict = model.state_dict()\n        loaded_keys = [k for k in state_dict.keys()]\n\n        expected_keys = list(model_state_dict.keys())\n\n        original_loaded_keys = loaded_keys\n\n        missing_keys = list(set(expected_keys) - set(loaded_keys))\n        unexpected_keys = list(set(loaded_keys) - set(expected_keys))\n\n        # Make sure we are able to load base models as well as derived models (with heads)\n        model_to_load = model\n\n        def _find_mismatched_keys(\n            state_dict,\n            model_state_dict,\n            loaded_keys,\n            ignore_mismatched_sizes,\n        ):\n            mismatched_keys = []\n            if ignore_mismatched_sizes:\n                for checkpoint_key in loaded_keys:\n                    model_key = checkpoint_key\n\n                    if (\n                        model_key in model_state_dict\n                        and state_dict[checkpoint_key].shape != model_state_dict[model_key].shape\n                    ):\n                        mismatched_keys.append(\n                            (checkpoint_key, state_dict[checkpoint_key].shape, model_state_dict[model_key].shape)\n                        )\n                        del state_dict[checkpoint_key]\n            return mismatched_keys\n\n        if state_dict is not None:\n            # Whole checkpoint\n            mismatched_keys = _find_mismatched_keys(\n                state_dict,\n                model_state_dict,\n                original_loaded_keys,\n                ignore_mismatched_sizes,\n            )\n            error_msgs = _load_state_dict_into_model(model_to_load, state_dict)\n\n        if len(error_msgs) > 0:\n            error_msg = \"\\n\\t\".join(error_msgs)\n            if \"size mismatch\" in error_msg:\n                error_msg += (\n                    \"\\n\\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\"\n                )\n            raise RuntimeError(f\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\n\n        if len(unexpected_keys) > 0:\n            logger.warning(\n                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when\"\n                f\" initializing {model.__class__.__name__}: {unexpected_keys}\\n- This IS expected if you are\"\n                f\" initializing {model.__class__.__name__} from the checkpoint of a model trained on another task\"\n                \" or with another architecture (e.g. initializing a BertForSequenceClassification model from a\"\n                \" BertForPreTraining model).\\n- This IS NOT expected if you are initializing\"\n                f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly\"\n                \" identical (initializing a BertForSequenceClassification model from a\"\n                \" BertForSequenceClassification model).\"\n            )\n        else:\n            logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n        if len(missing_keys) > 0:\n            logger.warning(\n                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n                f\" {pretrained_model_name_or_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n                \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n            )\n        elif len(mismatched_keys) == 0:\n            logger.info(\n                f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n                f\" {pretrained_model_name_or_path}.\\nIf your task is similar to the task the model of the\"\n                f\" checkpoint was trained on, you can already use {model.__class__.__name__} for predictions\"\n                \" without further training.\"\n            )\n        if len(mismatched_keys) > 0:\n            mismatched_warning = \"\\n\".join(\n                [\n                    f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n                    for key, shape1, shape2 in mismatched_keys\n                ]\n            )\n            logger.warning(\n                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n                f\" {pretrained_model_name_or_path} and are newly initialized because the shapes did not\"\n                f\" match:\\n{mismatched_warning}\\nYou should probably TRAIN this model on a down-stream task to be\"\n                \" able to use it for predictions and inference.\"\n            )\n\n        return model, missing_keys, unexpected_keys, mismatched_keys, error_msgs\n\n    @property\n    def device(self) -> device:\n        \"\"\"\n        `torch.device`: The device on which the module is (assuming that all the module parameters are on the same\n        device).\n        \"\"\"\n        return get_parameter_device(self)\n\n    @property\n    def dtype(self) -> torch.dtype:\n        \"\"\"\n        `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\n        \"\"\"\n        return get_parameter_dtype(self)\n\n    def num_parameters(self, only_trainable: bool = False, exclude_embeddings: bool = False) -> int:\n        \"\"\"\n        Get number of (optionally, trainable or non-embeddings) parameters in the module.\n\n        Args:\n            only_trainable (`bool`, *optional*, defaults to `False`):\n                Whether or not to return only the number of trainable parameters\n\n            exclude_embeddings (`bool`, *optional*, defaults to `False`):\n                Whether or not to return only the number of non-embeddings parameters\n\n        Returns:\n            `int`: The number of parameters.\n        \"\"\"\n\n        if exclude_embeddings:\n            embedding_param_names = [\n                f\"{name}.weight\"\n                for name, module_type in self.named_modules()\n                if isinstance(module_type, torch.nn.Embedding)\n            ]\n            non_embedding_parameters = [\n                parameter for name, parameter in self.named_parameters() if name not in embedding_param_names\n            ]\n            return sum(p.numel() for p in non_embedding_parameters if p.requires_grad or not only_trainable)\n        else:\n            return sum(p.numel() for p in self.parameters() if p.requires_grad or not only_trainable)", "\n\ndef unwrap_model(model: torch.nn.Module) -> torch.nn.Module:\n    \"\"\"\n    Recursively unwraps a model from potential containers (as used in distributed training).\n\n    Args:\n        model (`torch.nn.Module`): The model to unwrap.\n    \"\"\"\n    # since there could be multiple levels of wrapping, unwrap recursively\n    if hasattr(model, \"module\"):\n        return unwrap_model(model.module)\n    else:\n        return model", ""]}
{"filename": "diffusers_/hub_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2022 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nimport shutil", "import os\nimport shutil\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, Optional, Union\nfrom uuid import uuid4\n\nfrom huggingface_hub import HfFolder, Repository, whoami\n\nfrom . import __version__", "\nfrom . import __version__\nfrom .utils import ENV_VARS_TRUE_VALUES, deprecate, logging\nfrom .utils.import_utils import (\n    _flax_version,\n    _jax_version,\n    _onnxruntime_version,\n    _torch_version,\n    is_flax_available,\n    is_modelcards_available,", "    is_flax_available,\n    is_modelcards_available,\n    is_onnx_available,\n    is_torch_available,\n)\n\n\nif is_modelcards_available():\n    from modelcards import CardData, ModelCard\n", "\n\nlogger = logging.get_logger(__name__)\n\n\nMODEL_CARD_TEMPLATE_PATH = Path(__file__).parent / \"utils\" / \"model_card_template.md\"\nSESSION_ID = uuid4().hex\nDISABLE_TELEMETRY = os.getenv(\"DISABLE_TELEMETRY\", \"\").upper() in ENV_VARS_TRUE_VALUES\n\n\ndef http_user_agent(user_agent: Union[Dict, str, None] = None) -> str:\n    \"\"\"\n    Formats a user-agent string with basic info about a request.\n    \"\"\"\n    ua = f\"diffusers/{__version__}; python/{sys.version.split()[0]}; session_id/{SESSION_ID}\"\n    if DISABLE_TELEMETRY:\n        return ua + \"; telemetry/off\"\n    if is_torch_available():\n        ua += f\"; torch/{_torch_version}\"\n    if is_flax_available():\n        ua += f\"; jax/{_jax_version}\"\n        ua += f\"; flax/{_flax_version}\"\n    if is_onnx_available():\n        ua += f\"; onnxruntime/{_onnxruntime_version}\"\n    # CI will set this value to True\n    if os.environ.get(\"DIFFUSERS_IS_CI\", \"\").upper() in ENV_VARS_TRUE_VALUES:\n        ua += \"; is_ci/true\"\n    if isinstance(user_agent, dict):\n        ua += \"; \" + \"; \".join(f\"{k}/{v}\" for k, v in user_agent.items())\n    elif isinstance(user_agent, str):\n        ua += \"; \" + user_agent\n    return ua", "\n\ndef http_user_agent(user_agent: Union[Dict, str, None] = None) -> str:\n    \"\"\"\n    Formats a user-agent string with basic info about a request.\n    \"\"\"\n    ua = f\"diffusers/{__version__}; python/{sys.version.split()[0]}; session_id/{SESSION_ID}\"\n    if DISABLE_TELEMETRY:\n        return ua + \"; telemetry/off\"\n    if is_torch_available():\n        ua += f\"; torch/{_torch_version}\"\n    if is_flax_available():\n        ua += f\"; jax/{_jax_version}\"\n        ua += f\"; flax/{_flax_version}\"\n    if is_onnx_available():\n        ua += f\"; onnxruntime/{_onnxruntime_version}\"\n    # CI will set this value to True\n    if os.environ.get(\"DIFFUSERS_IS_CI\", \"\").upper() in ENV_VARS_TRUE_VALUES:\n        ua += \"; is_ci/true\"\n    if isinstance(user_agent, dict):\n        ua += \"; \" + \"; \".join(f\"{k}/{v}\" for k, v in user_agent.items())\n    elif isinstance(user_agent, str):\n        ua += \"; \" + user_agent\n    return ua", "\n\ndef get_full_repo_name(model_id: str, organization: Optional[str] = None, token: Optional[str] = None):\n    if token is None:\n        token = HfFolder.get_token()\n    if organization is None:\n        username = whoami(token)[\"name\"]\n        return f\"{username}/{model_id}\"\n    else:\n        return f\"{organization}/{model_id}\"", "\n\ndef init_git_repo(args, at_init: bool = False):\n    \"\"\"\n    Args:\n    Initializes a git repo in `args.hub_model_id`.\n        at_init (`bool`, *optional*, defaults to `False`):\n            Whether this function is called before any training or not. If `self.args.overwrite_output_dir` is `True`\n            and `at_init` is `True`, the path to the repo (which is `self.args.output_dir`) might be wiped out.\n    \"\"\"\n    deprecation_message = (\n        \"Please use `huggingface_hub.Repository`. \"\n        \"See `examples/unconditional_image_generation/train_unconditional.py` for an example.\"\n    )\n    deprecate(\"init_git_repo()\", \"0.10.0\", deprecation_message)\n\n    if hasattr(args, \"local_rank\") and args.local_rank not in [-1, 0]:\n        return\n    hub_token = args.hub_token if hasattr(args, \"hub_token\") else None\n    use_auth_token = True if hub_token is None else hub_token\n    if not hasattr(args, \"hub_model_id\") or args.hub_model_id is None:\n        repo_name = Path(args.output_dir).absolute().name\n    else:\n        repo_name = args.hub_model_id\n    if \"/\" not in repo_name:\n        repo_name = get_full_repo_name(repo_name, token=hub_token)\n\n    try:\n        repo = Repository(\n            args.output_dir,\n            clone_from=repo_name,\n            use_auth_token=use_auth_token,\n            private=args.hub_private_repo,\n        )\n    except EnvironmentError:\n        if args.overwrite_output_dir and at_init:\n            # Try again after wiping output_dir\n            shutil.rmtree(args.output_dir)\n            repo = Repository(\n                args.output_dir,\n                clone_from=repo_name,\n                use_auth_token=use_auth_token,\n            )\n        else:\n            raise\n\n    repo.git_pull()\n\n    # By default, ignore the checkpoint folders\n    if not os.path.exists(os.path.join(args.output_dir, \".gitignore\")):\n        with open(os.path.join(args.output_dir, \".gitignore\"), \"w\", encoding=\"utf-8\") as writer:\n            writer.writelines([\"checkpoint-*/\"])\n\n    return repo", "\n\ndef push_to_hub(\n    args,\n    pipeline,\n    repo: Repository,\n    commit_message: Optional[str] = \"End of training\",\n    blocking: bool = True,\n    **kwargs,\n) -> str:\n    \"\"\"\n    Parameters:\n    Upload *self.model* and *self.tokenizer* to the \ud83e\udd17 model hub on the repo *self.args.hub_model_id*.\n        commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n            Message to commit while pushing.\n        blocking (`bool`, *optional*, defaults to `True`):\n            Whether the function should return only when the `git push` has finished.\n        kwargs:\n            Additional keyword arguments passed along to [`create_model_card`].\n    Returns:\n        The url of the commit of your model in the given repository if `blocking=False`, a tuple with the url of the\n        commit and an object to track the progress of the commit if `blocking=True`\n    \"\"\"\n    deprecation_message = (\n        \"Please use `huggingface_hub.Repository` and `Repository.push_to_hub()`. \"\n        \"See `examples/unconditional_image_generation/train_unconditional.py` for an example.\"\n    )\n    deprecate(\"push_to_hub()\", \"0.10.0\", deprecation_message)\n\n    if not hasattr(args, \"hub_model_id\") or args.hub_model_id is None:\n        model_name = Path(args.output_dir).name\n    else:\n        model_name = args.hub_model_id.split(\"/\")[-1]\n\n    output_dir = args.output_dir\n    os.makedirs(output_dir, exist_ok=True)\n    logger.info(f\"Saving pipeline checkpoint to {output_dir}\")\n    pipeline.save_pretrained(output_dir)\n\n    # Only push from one node.\n    if hasattr(args, \"local_rank\") and args.local_rank not in [-1, 0]:\n        return\n\n    # Cancel any async push in progress if blocking=True. The commits will all be pushed together.\n    if (\n        blocking\n        and len(repo.command_queue) > 0\n        and repo.command_queue[-1] is not None\n        and not repo.command_queue[-1].is_done\n    ):\n        repo.command_queue[-1]._process.kill()\n\n    git_head_commit_url = repo.push_to_hub(commit_message=commit_message, blocking=blocking, auto_lfs_prune=True)\n    # push separately the model card to be independent from the rest of the model\n    create_model_card(args, model_name=model_name)\n    try:\n        repo.push_to_hub(commit_message=\"update model card README.md\", blocking=blocking, auto_lfs_prune=True)\n    except EnvironmentError as exc:\n        logger.error(f\"Error pushing update to the model card. Please read logs and retry.\\n${exc}\")\n\n    return git_head_commit_url", "\n\ndef create_model_card(args, model_name):\n    if not is_modelcards_available:\n        raise ValueError(\n            \"Please make sure to have `modelcards` installed when using the `create_model_card` function. You can\"\n            \" install the package with `pip install modelcards`.\"\n        )\n\n    if hasattr(args, \"local_rank\") and args.local_rank not in [-1, 0]:\n        return\n\n    hub_token = args.hub_token if hasattr(args, \"hub_token\") else None\n    repo_name = get_full_repo_name(model_name, token=hub_token)\n\n    model_card = ModelCard.from_template(\n        card_data=CardData(  # Card metadata object that will be converted to YAML block\n            language=\"en\",\n            license=\"apache-2.0\",\n            library_name=\"diffusers\",\n            tags=[],\n            datasets=args.dataset_name,\n            metrics=[],\n        ),\n        template_path=MODEL_CARD_TEMPLATE_PATH,\n        model_name=model_name,\n        repo_name=repo_name,\n        dataset_name=args.dataset_name if hasattr(args, \"dataset_name\") else None,\n        learning_rate=args.learning_rate,\n        train_batch_size=args.train_batch_size,\n        eval_batch_size=args.eval_batch_size,\n        gradient_accumulation_steps=args.gradient_accumulation_steps\n        if hasattr(args, \"gradient_accumulation_steps\")\n        else None,\n        adam_beta1=args.adam_beta1 if hasattr(args, \"adam_beta1\") else None,\n        adam_beta2=args.adam_beta2 if hasattr(args, \"adam_beta2\") else None,\n        adam_weight_decay=args.adam_weight_decay if hasattr(args, \"adam_weight_decay\") else None,\n        adam_epsilon=args.adam_epsilon if hasattr(args, \"adam_epsilon\") else None,\n        lr_scheduler=args.lr_scheduler if hasattr(args, \"lr_scheduler\") else None,\n        lr_warmup_steps=args.lr_warmup_steps if hasattr(args, \"lr_warmup_steps\") else None,\n        ema_inv_gamma=args.ema_inv_gamma if hasattr(args, \"ema_inv_gamma\") else None,\n        ema_power=args.ema_power if hasattr(args, \"ema_power\") else None,\n        ema_max_decay=args.ema_max_decay if hasattr(args, \"ema_max_decay\") else None,\n        mixed_precision=args.mixed_precision,\n    )\n\n    card_path = os.path.join(args.output_dir, \"README.md\")\n    model_card.save(card_path)", ""]}
{"filename": "diffusers_/dynamic_modules_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2022 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utilities to dynamically load objects from the Hub.\"\"\"\n\nimport importlib\nimport inspect", "import importlib\nimport inspect\nimport os\nimport re\nimport shutil\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, Optional, Union\n\nfrom huggingface_hub import HfFolder, cached_download, hf_hub_download, model_info", "\nfrom huggingface_hub import HfFolder, cached_download, hf_hub_download, model_info\n\nfrom .utils import DIFFUSERS_DYNAMIC_MODULE_NAME, HF_MODULES_CACHE, logging\n\n\nCOMMUNITY_PIPELINES_URL = (\n    \"https://raw.githubusercontent.com/huggingface/diffusers/main/examples/community/{pipeline}.py\"\n)\n", ")\n\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\ndef init_hf_modules():\n    \"\"\"\n    Creates the cache directory for modules with an init, and adds it to the Python path.\n    \"\"\"\n    # This function has already been executed if HF_MODULES_CACHE already is in the Python path.\n    if HF_MODULES_CACHE in sys.path:\n        return\n\n    sys.path.append(HF_MODULES_CACHE)\n    os.makedirs(HF_MODULES_CACHE, exist_ok=True)\n    init_path = Path(HF_MODULES_CACHE) / \"__init__.py\"\n    if not init_path.exists():\n        init_path.touch()", "\n\ndef create_dynamic_module(name: Union[str, os.PathLike]):\n    \"\"\"\n    Creates a dynamic module in the cache directory for modules.\n    \"\"\"\n    init_hf_modules()\n    dynamic_module_path = Path(HF_MODULES_CACHE) / name\n    # If the parent module does not exist yet, recursively create it.\n    if not dynamic_module_path.parent.exists():\n        create_dynamic_module(dynamic_module_path.parent)\n    os.makedirs(dynamic_module_path, exist_ok=True)\n    init_path = dynamic_module_path / \"__init__.py\"\n    if not init_path.exists():\n        init_path.touch()", "\n\ndef get_relative_imports(module_file):\n    \"\"\"\n    Get the list of modules that are relatively imported in a module file.\n\n    Args:\n        module_file (`str` or `os.PathLike`): The module file to inspect.\n    \"\"\"\n    with open(module_file, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n\n    # Imports of the form `import .xxx`\n    relative_imports = re.findall(\"^\\s*import\\s+\\.(\\S+)\\s*$\", content, flags=re.MULTILINE)\n    # Imports of the form `from .xxx import yyy`\n    relative_imports += re.findall(\"^\\s*from\\s+\\.(\\S+)\\s+import\", content, flags=re.MULTILINE)\n    # Unique-ify\n    return list(set(relative_imports))", "\n\ndef get_relative_import_files(module_file):\n    \"\"\"\n    Get the list of all files that are needed for a given module. Note that this function recurses through the relative\n    imports (if a imports b and b imports c, it will return module files for b and c).\n\n    Args:\n        module_file (`str` or `os.PathLike`): The module file to inspect.\n    \"\"\"\n    no_change = False\n    files_to_check = [module_file]\n    all_relative_imports = []\n\n    # Let's recurse through all relative imports\n    while not no_change:\n        new_imports = []\n        for f in files_to_check:\n            new_imports.extend(get_relative_imports(f))\n\n        module_path = Path(module_file).parent\n        new_import_files = [str(module_path / m) for m in new_imports]\n        new_import_files = [f for f in new_import_files if f not in all_relative_imports]\n        files_to_check = [f\"{f}.py\" for f in new_import_files]\n\n        no_change = len(new_import_files) == 0\n        all_relative_imports.extend(files_to_check)\n\n    return all_relative_imports", "\n\ndef check_imports(filename):\n    \"\"\"\n    Check if the current Python environment contains all the libraries that are imported in a file.\n    \"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n\n    # Imports of the form `import xxx`\n    imports = re.findall(\"^\\s*import\\s+(\\S+)\\s*$\", content, flags=re.MULTILINE)\n    # Imports of the form `from xxx import yyy`\n    imports += re.findall(\"^\\s*from\\s+(\\S+)\\s+import\", content, flags=re.MULTILINE)\n    # Only keep the top-level module\n    imports = [imp.split(\".\")[0] for imp in imports if not imp.startswith(\".\")]\n\n    # Unique-ify and test we got them all\n    imports = list(set(imports))\n    missing_packages = []\n    for imp in imports:\n        try:\n            importlib.import_module(imp)\n        except ImportError:\n            missing_packages.append(imp)\n\n    if len(missing_packages) > 0:\n        raise ImportError(\n            \"This modeling file requires the following packages that were not found in your environment: \"\n            f\"{', '.join(missing_packages)}. Run `pip install {' '.join(missing_packages)}`\"\n        )\n\n    return get_relative_imports(filename)", "\n\ndef get_class_in_module(class_name, module_path):\n    \"\"\"\n    Import a module on the cache directory for modules and extract a class from it.\n    \"\"\"\n    module_path = module_path.replace(os.path.sep, \".\")\n    module = importlib.import_module(module_path)\n\n    if class_name is None:\n        return find_pipeline_class(module)\n    return getattr(module, class_name)", "\n\ndef find_pipeline_class(loaded_module):\n    \"\"\"\n    Retrieve pipeline class that inherits from `DiffusionPipeline`. Note that there has to be exactly one class\n    inheriting from `DiffusionPipeline`.\n    \"\"\"\n    from .pipeline_utils import DiffusionPipeline\n\n    cls_members = dict(inspect.getmembers(loaded_module, inspect.isclass))\n\n    pipeline_class = None\n    for cls_name, cls in cls_members.items():\n        if (\n            cls_name != DiffusionPipeline.__name__\n            and issubclass(cls, DiffusionPipeline)\n            and cls.__module__.split(\".\")[0] != \"diffusers\"\n        ):\n            if pipeline_class is not None:\n                raise ValueError(\n                    f\"Multiple classes that inherit from {DiffusionPipeline.__name__} have been found:\"\n                    f\" {pipeline_class.__name__}, and {cls_name}. Please make sure to define only one in\"\n                    f\" {loaded_module}.\"\n                )\n            pipeline_class = cls\n\n    return pipeline_class", "\n\ndef get_cached_module_file(\n    pretrained_model_name_or_path: Union[str, os.PathLike],\n    module_file: str,\n    cache_dir: Optional[Union[str, os.PathLike]] = None,\n    force_download: bool = False,\n    resume_download: bool = False,\n    proxies: Optional[Dict[str, str]] = None,\n    use_auth_token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    local_files_only: bool = False,\n):\n    \"\"\"\n    Prepares Downloads a module from a local folder or a distant repo and returns its path inside the cached\n    Transformers module.\n\n    Args:\n        pretrained_model_name_or_path (`str` or `os.PathLike`):\n            This can be either:\n\n            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n              huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced\n              under a user or organization name, like `dbmdz/bert-base-german-cased`.\n            - a path to a *directory* containing a configuration file saved using the\n              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n\n        module_file (`str`):\n            The name of the module file containing the class to look for.\n        cache_dir (`str` or `os.PathLike`, *optional*):\n            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n            cache should not be used.\n        force_download (`bool`, *optional*, defaults to `False`):\n            Whether or not to force to (re-)download the configuration files and override the cached versions if they\n            exist.\n        resume_download (`bool`, *optional*, defaults to `False`):\n            Whether or not to delete incompletely received file. Attempts to resume the download if such a file exists.\n        proxies (`Dict[str, str]`, *optional*):\n            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n        use_auth_token (`str` or *bool*, *optional*):\n            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n            when running `transformers-cli login` (stored in `~/.huggingface`).\n        revision (`str`, *optional*, defaults to `\"main\"`):\n            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n            identifier allowed by git.\n        local_files_only (`bool`, *optional*, defaults to `False`):\n            If `True`, will only try to load the tokenizer configuration from local files.\n\n    <Tip>\n\n    You may pass a token in `use_auth_token` if you are not logged in (`huggingface-cli long`) and want to use private\n    or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models).\n\n    </Tip>\n\n    Returns:\n        `str`: The path to the module inside the cache.\n    \"\"\"\n    # Download and cache module_file from the repo `pretrained_model_name_or_path` of grab it if it's a local file.\n    pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n\n    module_file_or_url = os.path.join(pretrained_model_name_or_path, module_file)\n\n    if os.path.isfile(module_file_or_url):\n        resolved_module_file = module_file_or_url\n        submodule = \"local\"\n    elif pretrained_model_name_or_path.count(\"/\") == 0:\n        # community pipeline on GitHub\n        github_url = COMMUNITY_PIPELINES_URL.format(pipeline=pretrained_model_name_or_path)\n        try:\n            resolved_module_file = cached_download(\n                github_url,\n                cache_dir=cache_dir,\n                force_download=force_download,\n                proxies=proxies,\n                resume_download=resume_download,\n                local_files_only=local_files_only,\n                use_auth_token=False,\n            )\n            submodule = \"git\"\n            module_file = pretrained_model_name_or_path + \".py\"\n        except EnvironmentError:\n            logger.error(f\"Could not locate the {module_file} inside {pretrained_model_name_or_path}.\")\n            raise\n    else:\n        try:\n            # Load from URL or cache if already cached\n            resolved_module_file = hf_hub_download(\n                pretrained_model_name_or_path,\n                module_file,\n                cache_dir=cache_dir,\n                force_download=force_download,\n                proxies=proxies,\n                resume_download=resume_download,\n                local_files_only=local_files_only,\n                use_auth_token=use_auth_token,\n            )\n            submodule = os.path.join(\"local\", \"--\".join(pretrained_model_name_or_path.split(\"/\")))\n        except EnvironmentError:\n            logger.error(f\"Could not locate the {module_file} inside {pretrained_model_name_or_path}.\")\n            raise\n\n    # Check we have all the requirements in our environment\n    modules_needed = check_imports(resolved_module_file)\n\n    # Now we move the module inside our cached dynamic modules.\n    full_submodule = DIFFUSERS_DYNAMIC_MODULE_NAME + os.path.sep + submodule\n    create_dynamic_module(full_submodule)\n    submodule_path = Path(HF_MODULES_CACHE) / full_submodule\n    if submodule == \"local\" or submodule == \"git\":\n        # We always copy local files (we could hash the file to see if there was a change, and give them the name of\n        # that hash, to only copy when there is a modification but it seems overkill for now).\n        # The only reason we do the copy is to avoid putting too many folders in sys.path.\n        shutil.copy(resolved_module_file, submodule_path / module_file)\n        for module_needed in modules_needed:\n            module_needed = f\"{module_needed}.py\"\n            shutil.copy(os.path.join(pretrained_model_name_or_path, module_needed), submodule_path / module_needed)\n    else:\n        # Get the commit hash\n        # TODO: we will get this info in the etag soon, so retrieve it from there and not here.\n        if isinstance(use_auth_token, str):\n            token = use_auth_token\n        elif use_auth_token is True:\n            token = HfFolder.get_token()\n        else:\n            token = None\n\n        commit_hash = model_info(pretrained_model_name_or_path, revision=revision, token=token).sha\n\n        # The module file will end up being placed in a subfolder with the git hash of the repo. This way we get the\n        # benefit of versioning.\n        submodule_path = submodule_path / commit_hash\n        full_submodule = full_submodule + os.path.sep + commit_hash\n        create_dynamic_module(full_submodule)\n\n        if not (submodule_path / module_file).exists():\n            shutil.copy(resolved_module_file, submodule_path / module_file)\n        # Make sure we also have every file with relative\n        for module_needed in modules_needed:\n            if not (submodule_path / module_needed).exists():\n                get_cached_module_file(\n                    pretrained_model_name_or_path,\n                    f\"{module_needed}.py\",\n                    cache_dir=cache_dir,\n                    force_download=force_download,\n                    resume_download=resume_download,\n                    proxies=proxies,\n                    use_auth_token=use_auth_token,\n                    revision=revision,\n                    local_files_only=local_files_only,\n                )\n    return os.path.join(full_submodule, module_file)", "\n\ndef get_class_from_dynamic_module(\n    pretrained_model_name_or_path: Union[str, os.PathLike],\n    module_file: str,\n    class_name: Optional[str] = None,\n    cache_dir: Optional[Union[str, os.PathLike]] = None,\n    force_download: bool = False,\n    resume_download: bool = False,\n    proxies: Optional[Dict[str, str]] = None,\n    use_auth_token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    local_files_only: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Extracts a class from a module file, present in the local folder or repository of a model.\n\n    <Tip warning={true}>\n\n    Calling this function will execute the code in the module file found locally or downloaded from the Hub. It should\n    therefore only be called on trusted repos.\n\n    </Tip>\n\n    Args:\n        pretrained_model_name_or_path (`str` or `os.PathLike`):\n            This can be either:\n\n            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n              huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced\n              under a user or organization name, like `dbmdz/bert-base-german-cased`.\n            - a path to a *directory* containing a configuration file saved using the\n              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n\n        module_file (`str`):\n            The name of the module file containing the class to look for.\n        class_name (`str`):\n            The name of the class to import in the module.\n        cache_dir (`str` or `os.PathLike`, *optional*):\n            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n            cache should not be used.\n        force_download (`bool`, *optional*, defaults to `False`):\n            Whether or not to force to (re-)download the configuration files and override the cached versions if they\n            exist.\n        resume_download (`bool`, *optional*, defaults to `False`):\n            Whether or not to delete incompletely received file. Attempts to resume the download if such a file exists.\n        proxies (`Dict[str, str]`, *optional*):\n            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n        use_auth_token (`str` or `bool`, *optional*):\n            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n            when running `transformers-cli login` (stored in `~/.huggingface`).\n        revision (`str`, *optional*, defaults to `\"main\"`):\n            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n            identifier allowed by git.\n        local_files_only (`bool`, *optional*, defaults to `False`):\n            If `True`, will only try to load the tokenizer configuration from local files.\n\n    <Tip>\n\n    You may pass a token in `use_auth_token` if you are not logged in (`huggingface-cli long`) and want to use private\n    or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models).\n\n    </Tip>\n\n    Returns:\n        `type`: The class, dynamically imported from the module.\n\n    Examples:\n\n    ```python\n    # Download module `modeling.py` from huggingface.co and cache then extract the class `MyBertModel` from this\n    # module.\n    cls = get_class_from_dynamic_module(\"sgugger/my-bert-model\", \"modeling.py\", \"MyBertModel\")\n    ```\"\"\"\n    # And lastly we get the class inside our newly created module\n    final_module = get_cached_module_file(\n        pretrained_model_name_or_path,\n        module_file,\n        cache_dir=cache_dir,\n        force_download=force_download,\n        resume_download=resume_download,\n        proxies=proxies,\n        use_auth_token=use_auth_token,\n        revision=revision,\n        local_files_only=local_files_only,\n    )\n    return get_class_in_module(class_name, final_module.replace(\".py\", \"\"))", ""]}
{"filename": "diffusers_/__init__.py", "chunked_list": ["from .utils import (\n    is_torch_available,\n    is_transformers_available,\n)\n\n\n__version__ = \"0.9.0\"\n\n\nif is_torch_available() and is_transformers_available():\n    from .stable_diffusion import (\n        StableDiffusionPipeline,\n    )\nelse:\n    from .utils.dummy_torch_and_transformers_objects import *  # noqa F403", "\nif is_torch_available() and is_transformers_available():\n    from .stable_diffusion import (\n        StableDiffusionPipeline,\n    )\nelse:\n    from .utils.dummy_torch_and_transformers_objects import *  # noqa F403\n"]}
{"filename": "diffusers_/pipeline_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2022 The HuggingFace Inc. team.\n# Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport importlib\nimport inspect", "import importlib\nimport inspect\nimport os\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport numpy as np\nimport torch\n", "import torch\n\nimport diffusers\nimport PIL\nfrom huggingface_hub import snapshot_download\nfrom packaging import version\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\nfrom .configuration_utils import ConfigMixin", "\nfrom .configuration_utils import ConfigMixin\nfrom .dynamic_modules_utils import get_class_from_dynamic_module\nfrom .hub_utils import http_user_agent\nfrom .modeling_utils import _LOW_CPU_MEM_USAGE_DEFAULT\nfrom .scheduling_utils import SCHEDULER_CONFIG_NAME\nfrom .utils import (\n    CONFIG_NAME,\n    DIFFUSERS_CACHE,\n    ONNX_WEIGHTS_NAME,", "    DIFFUSERS_CACHE,\n    ONNX_WEIGHTS_NAME,\n    WEIGHTS_NAME,\n    BaseOutput,\n    deprecate,\n    is_accelerate_available,\n    is_torch_version,\n    is_transformers_available,\n    logging,\n)", "    logging,\n)\n\n\nif is_transformers_available():\n    import transformers\n    from transformers import PreTrainedModel\n\n\nINDEX_FILE = \"diffusion_pytorch_model.bin\"", "\nINDEX_FILE = \"diffusion_pytorch_model.bin\"\nCUSTOM_PIPELINE_FILE_NAME = \"pipeline.py\"\nDUMMY_MODULES_FOLDER = \"diffusers.utils\"\nTRANSFORMERS_DUMMY_MODULES_FOLDER = \"transformers.utils\"\n\n\nlogger = logging.get_logger(__name__)\n\n", "\n\nLOADABLE_CLASSES = {\n    \"diffusers\": {\n        \"ModelMixin\": [\"save_pretrained\", \"from_pretrained\"],\n        \"SchedulerMixin\": [\"save_pretrained\", \"from_pretrained\"],\n        \"DiffusionPipeline\": [\"save_pretrained\", \"from_pretrained\"],\n        \"OnnxRuntimeModel\": [\"save_pretrained\", \"from_pretrained\"],\n    },\n    \"transformers\": {", "    },\n    \"transformers\": {\n        \"PreTrainedTokenizer\": [\"save_pretrained\", \"from_pretrained\"],\n        \"PreTrainedTokenizerFast\": [\"save_pretrained\", \"from_pretrained\"],\n        \"PreTrainedModel\": [\"save_pretrained\", \"from_pretrained\"],\n        \"FeatureExtractionMixin\": [\"save_pretrained\", \"from_pretrained\"],\n        \"ProcessorMixin\": [\"save_pretrained\", \"from_pretrained\"],\n        \"ImageProcessingMixin\": [\"save_pretrained\", \"from_pretrained\"],\n    },\n    \"onnxruntime.training\": {", "    },\n    \"onnxruntime.training\": {\n        \"ORTModule\": [\"save_pretrained\", \"from_pretrained\"],\n    },\n}\n\nALL_IMPORTABLE_CLASSES = {}\nfor library in LOADABLE_CLASSES:\n    ALL_IMPORTABLE_CLASSES.update(LOADABLE_CLASSES[library])\n", "\n\n@dataclass\nclass ImagePipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for image pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]", "\n\n@dataclass\nclass AudioPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for audio pipelines.\n\n    Args:\n        audios (`np.ndarray`)\n            List of denoised samples of shape `(batch_size, num_channels, sample_rate)`. Numpy array present the\n            denoised audio samples of the diffusion pipeline.\n    \"\"\"\n\n    audios: np.ndarray", "\n\nclass DiffusionPipeline(ConfigMixin):\n    r\"\"\"\n    Base class for all models.\n\n    [`DiffusionPipeline`] takes care of storing all components (models, schedulers, processors) for diffusion pipelines\n    and handles methods for loading, downloading and saving models as well as a few methods common to all pipelines to:\n\n        - move all PyTorch modules to the device of your choice\n        - enabling/disabling the progress bar for the denoising iteration\n\n    Class attributes:\n\n        - **config_name** (`str`) -- name of the config file that will store the class and module names of all\n          components of the diffusion pipeline.\n        - **_optional_components** (List[`str`]) -- list of all components that are optional so they don't have to be\n          passed for the pipeline to function (should be overridden by subclasses).\n    \"\"\"\n    config_name = \"model_index.json\"\n    _optional_components = []\n\n    def register_modules(self, **kwargs):\n        # import it here to avoid circular import\n        from diffusers import pipelines\n\n        for name, module in kwargs.items():\n            # retrieve library\n            if module is None:\n                register_dict = {name: (None, None)}\n            else:\n                library = module.__module__.split(\".\")[0]\n\n                # check if the module is a pipeline module\n                pipeline_dir = module.__module__.split(\".\")[-2] if len(module.__module__.split(\".\")) > 2 else None\n                path = module.__module__.split(\".\")\n                is_pipeline_module = pipeline_dir in path and hasattr(pipelines, pipeline_dir)\n\n                # if library is not in LOADABLE_CLASSES, then it is a custom module.\n                # Or if it's a pipeline module, then the module is inside the pipeline\n                # folder so we set the library to module name.\n                if library not in LOADABLE_CLASSES or is_pipeline_module:\n                    library = pipeline_dir\n\n                # retrieve class_name\n                class_name = module.__class__.__name__\n\n                register_dict = {name: (library, class_name)}\n\n                # save model index config\n                self.register_to_config(**register_dict)\n\n            # set models\n            setattr(self, name, module)\n\n    def save_pretrained(self, save_directory: Union[str, os.PathLike]):\n        \"\"\"\n        Save all variables of the pipeline that can be saved and loaded as well as the pipelines configuration file to\n        a directory. A pipeline variable can be saved and loaded if its class implements both a save and loading\n        method. The pipeline can easily be re-loaded using the `[`~DiffusionPipeline.from_pretrained`]` class method.\n\n        Arguments:\n            save_directory (`str` or `os.PathLike`):\n                Directory to which to save. Will be created if it doesn't exist.\n        \"\"\"\n        self.save_config(save_directory)\n\n        model_index_dict = dict(self.config)\n        model_index_dict.pop(\"_class_name\")\n        model_index_dict.pop(\"_diffusers_version\")\n        model_index_dict.pop(\"_module\", None)\n\n        expected_modules, optional_kwargs = self._get_signature_keys(self)\n\n        def is_saveable_module(name, value):\n            if name not in expected_modules:\n                return False\n            if name in self._optional_components and value[0] is None:\n                return False\n            return True\n\n        model_index_dict = {k: v for k, v in model_index_dict.items() if is_saveable_module(k, v)}\n\n        for pipeline_component_name in model_index_dict.keys():\n            sub_model = getattr(self, pipeline_component_name)\n            model_cls = sub_model.__class__\n\n            save_method_name = None\n            # search for the model's base class in LOADABLE_CLASSES\n            for library_name, library_classes in LOADABLE_CLASSES.items():\n                library = importlib.import_module(library_name)\n                for base_class, save_load_methods in library_classes.items():\n                    class_candidate = getattr(library, base_class, None)\n                    if class_candidate is not None and issubclass(model_cls, class_candidate):\n                        # if we found a suitable base class in LOADABLE_CLASSES then grab its save method\n                        save_method_name = save_load_methods[0]\n                        break\n                if save_method_name is not None:\n                    break\n\n            if save_method_name is not None:\n                save_method = getattr(sub_model, save_method_name)\n                save_method(os.path.join(save_directory, pipeline_component_name))\n\n    def to(self, torch_device: Optional[Union[str, torch.device]] = None):\n        if torch_device is None:\n            return self\n\n        module_names, _, _ = self.extract_init_dict(dict(self.config))\n        for name in module_names.keys():\n            module = getattr(self, name)\n            if isinstance(module, torch.nn.Module):\n                if module.dtype == torch.float16 and str(torch_device) in [\"cpu\"]:\n                    logger.warning(\n                        \"Pipelines loaded with `torch_dtype=torch.float16` cannot run with `cpu` device. It\"\n                        \" is not recommended to move them to `cpu` as running them will fail. Please make\"\n                        \" sure to use an accelerator to run the pipeline in inference, due to the lack of\"\n                        \" support for`float16` operations on this device in PyTorch. Please, remove the\"\n                        \" `torch_dtype=torch.float16` argument, or use another device for inference.\"\n                    )\n                module.to(torch_device)\n        return self\n\n    @property\n    def device(self) -> torch.device:\n        r\"\"\"\n        Returns:\n            `torch.device`: The torch device on which the pipeline is located.\n        \"\"\"\n        module_names, _, _ = self.extract_init_dict(dict(self.config))\n        for name in module_names.keys():\n            module = getattr(self, name)\n            if isinstance(module, torch.nn.Module):\n                return module.device\n        return torch.device(\"cpu\")\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], **kwargs):\n        r\"\"\"\n        Instantiate a PyTorch diffusion pipeline from pre-trained pipeline weights.\n\n        The pipeline is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated).\n\n        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come\n        pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning\n        task.\n\n        The warning *Weights from XXX not used in YYY* means that the layer XXX is not used by YYY, therefore those\n        weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n\n                    - A string, the *repo id* of a pretrained pipeline hosted inside a model repo on\n                      https://huggingface.co/ Valid repo ids have to be located under a user or organization name, like\n                      `CompVis/ldm-text2im-large-256`.\n                    - A path to a *directory* containing pipeline weights saved using\n                      [`~DiffusionPipeline.save_pretrained`], e.g., `./my_pipeline_directory/`.\n            torch_dtype (`str` or `torch.dtype`, *optional*):\n                Override the default `torch.dtype` and load the model under this dtype. If `\"auto\"` is passed the dtype\n                will be automatically derived from the model's weights.\n            custom_pipeline (`str`, *optional*):\n\n                <Tip warning={true}>\n\n                    This is an experimental feature and is likely to change in the future.\n\n                </Tip>\n\n                Can be either:\n\n                    - A string, the *repo id* of a custom pipeline hosted inside a model repo on\n                      https://huggingface.co/. Valid repo ids have to be located under a user or organization name,\n                      like `hf-internal-testing/diffusers-dummy-pipeline`.\n\n                        <Tip>\n\n                         It is required that the model repo has a file, called `pipeline.py` that defines the custom\n                         pipeline.\n\n                        </Tip>\n\n                    - A string, the *file name* of a community pipeline hosted on GitHub under\n                      https://github.com/huggingface/diffusers/tree/main/examples/community. Valid file names have to\n                      match exactly the file name without `.py` located under the above link, *e.g.*\n                      `clip_guided_stable_diffusion`.\n\n                        <Tip>\n\n                         Community pipelines are always loaded from the current `main` branch of GitHub.\n\n                        </Tip>\n\n                    - A path to a *directory* containing a custom pipeline, e.g., `./my_pipeline_directory/`.\n\n                        <Tip>\n\n                         It is required that the directory has a file, called `pipeline.py` that defines the custom\n                         pipeline.\n\n                        </Tip>\n\n                For more information on how to load and create custom pipelines, please have a look at [Loading and\n                Adding Custom\n                Pipelines](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)\n\n            torch_dtype (`str` or `torch.dtype`, *optional*):\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            resume_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n                file exists.\n            proxies (`Dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n            output_loading_info(`bool`, *optional*, defaults to `False`):\n                Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n            local_files_only(`bool`, *optional*, defaults to `False`):\n                Whether or not to only look at local files (i.e., do not try to download the model).\n            use_auth_token (`str` or *bool*, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n                when running `huggingface-cli login` (stored in `~/.huggingface`).\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n                identifier allowed by git.\n            mirror (`str`, *optional*):\n                Mirror source to accelerate downloads in China. If you are from China and have an accessibility\n                problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.\n                Please refer to the mirror site for more information. specify the folder name here.\n            device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n                A map that specifies where each submodule should go. It doesn't need to be refined to each\n                parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n                same device.\n\n                To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n                more information about each option see [designing a device\n                map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n            low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n                Speed up model loading by not initializing the weights and only loading the pre-trained weights. This\n                also tries to not use more than 1x model size in CPU memory (including peak memory) while loading the\n                model. This is only supported when torch version >= 1.9.0. If you are using an older version of torch,\n                setting this argument to `True` will raise an error.\n\n            kwargs (remaining dictionary of keyword arguments, *optional*):\n                Can be used to overwrite load - and saveable variables - *i.e.* the pipeline components - of the\n                specific pipeline class. The overwritten components are then directly passed to the pipelines\n                `__init__` method. See example below for more information.\n\n        <Tip>\n\n         It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n         models](https://huggingface.co/docs/hub/models-gated#gated-models), *e.g.* `\"runwayml/stable-diffusion-v1-5\"`\n\n        </Tip>\n\n        <Tip>\n\n        Activate the special [\"offline-mode\"](https://huggingface.co/diffusers/installation.html#offline-mode) to use\n        this method in a firewalled environment.\n\n        </Tip>\n\n        Examples:\n\n        ```py\n        >>> from diffusers import DiffusionPipeline\n\n        >>> # Download pipeline from huggingface.co and cache.\n        >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")\n\n        >>> # Download pipeline that requires an authorization token\n        >>> # For more information on access tokens, please refer to this section\n        >>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)\n        >>> pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n\n        >>> # Use a different scheduler\n        >>> from diffusers import LMSDiscreteScheduler\n\n        >>> scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n        >>> pipeline.scheduler = scheduler\n        ```\n        \"\"\"\n        cache_dir = kwargs.pop(\"cache_dir\", DIFFUSERS_CACHE)\n        resume_download = kwargs.pop(\"resume_download\", False)\n        force_download = kwargs.pop(\"force_download\", False)\n        proxies = kwargs.pop(\"proxies\", None)\n        local_files_only = kwargs.pop(\"local_files_only\", False)\n        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n        revision = kwargs.pop(\"revision\", None)\n        torch_dtype = kwargs.pop(\"torch_dtype\", None)\n        custom_pipeline = kwargs.pop(\"custom_pipeline\", None)\n        provider = kwargs.pop(\"provider\", None)\n        sess_options = kwargs.pop(\"sess_options\", None)\n        device_map = kwargs.pop(\"device_map\", None)\n        low_cpu_mem_usage = kwargs.pop(\"low_cpu_mem_usage\", _LOW_CPU_MEM_USAGE_DEFAULT)\n\n        if low_cpu_mem_usage and not is_accelerate_available():\n            low_cpu_mem_usage = False\n            logger.warning(\n                \"Cannot initialize model with low cpu memory usage because `accelerate` was not found in the\"\n                \" environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install\"\n                \" `accelerate` for faster and less memory-intense model loading. You can do so with: \\n```\\npip\"\n                \" install accelerate\\n```\\n.\"\n            )\n\n        if device_map is not None and not is_torch_version(\">=\", \"1.9.0\"):\n            raise NotImplementedError(\n                \"Loading and dispatching requires torch >= 1.9.0. Please either update your PyTorch version or set\"\n                \" `device_map=None`.\"\n            )\n\n        if low_cpu_mem_usage is True and not is_torch_version(\">=\", \"1.9.0\"):\n            raise NotImplementedError(\n                \"Low memory initialization requires torch >= 1.9.0. Please either update your PyTorch version or set\"\n                \" `low_cpu_mem_usage=False`.\"\n            )\n\n        if low_cpu_mem_usage is False and device_map is not None:\n            raise ValueError(\n                f\"You cannot set `low_cpu_mem_usage` to False while using device_map={device_map} for loading and\"\n                \" dispatching. Please make sure to set `low_cpu_mem_usage=True`.\"\n            )\n\n        # 1. Download the checkpoints and configs\n        # use snapshot download here to get it working from from_pretrained\n        if not os.path.isdir(pretrained_model_name_or_path):\n            config_dict = cls.load_config(\n                pretrained_model_name_or_path,\n                cache_dir=cache_dir,\n                resume_download=resume_download,\n                force_download=force_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=use_auth_token,\n                revision=revision,\n            )\n            # make sure we only download sub-folders and `diffusers` filenames\n            folder_names = [k for k in config_dict.keys() if not k.startswith(\"_\")]\n            allow_patterns = [os.path.join(k, \"*\") for k in folder_names]\n            allow_patterns += [WEIGHTS_NAME, SCHEDULER_CONFIG_NAME, CONFIG_NAME, ONNX_WEIGHTS_NAME, cls.config_name]\n\n            # make sure we don't download flax weights\n            ignore_patterns = \"*.msgpack\"\n\n            if custom_pipeline is not None:\n                allow_patterns += [CUSTOM_PIPELINE_FILE_NAME]\n\n            if cls != DiffusionPipeline:\n                requested_pipeline_class = cls.__name__\n            else:\n                requested_pipeline_class = config_dict.get(\"_class_name\", cls.__name__)\n            user_agent = {\"pipeline_class\": requested_pipeline_class}\n            if custom_pipeline is not None:\n                user_agent[\"custom_pipeline\"] = custom_pipeline\n            user_agent = http_user_agent(user_agent)\n\n            # download all allow_patterns\n            cached_folder = snapshot_download(\n                pretrained_model_name_or_path,\n                cache_dir=cache_dir,\n                resume_download=resume_download,\n                proxies=proxies,\n                local_files_only=local_files_only,\n                use_auth_token=use_auth_token,\n                revision=revision,\n                allow_patterns=allow_patterns,\n                ignore_patterns=ignore_patterns,\n                user_agent=user_agent,\n            )\n        else:\n            cached_folder = pretrained_model_name_or_path\n\n        config_dict = cls.load_config(cached_folder)\n\n        # 2. Load the pipeline class, if using custom module then load it from the hub\n        # if we load from explicit class, let's use it\n        if custom_pipeline is not None:\n            if custom_pipeline.endswith(\".py\"):\n                path = Path(custom_pipeline)\n                # decompose into folder & file\n                file_name = path.name\n                custom_pipeline = path.parent.absolute()\n            else:\n                file_name = CUSTOM_PIPELINE_FILE_NAME\n            import ipdb; ipdb.set_trace()\n            pipeline_class = get_class_from_dynamic_module(\n                custom_pipeline, module_file=file_name, cache_dir=custom_pipeline\n            )\n        elif cls != DiffusionPipeline:\n            pipeline_class = cls\n        else:\n            diffusers_module = importlib.import_module(cls.__module__.split(\".\")[0])\n            pipeline_class = getattr(diffusers_module, config_dict[\"_class_name\"])\n\n        # To be removed in 1.0.0\n        if pipeline_class.__name__ == \"StableDiffusionInpaintPipeline\" and version.parse(\n            version.parse(config_dict[\"_diffusers_version\"]).base_version\n        ) <= version.parse(\"0.5.1\"):\n            from diffusers import StableDiffusionInpaintPipeline, StableDiffusionInpaintPipelineLegacy\n\n            pipeline_class = StableDiffusionInpaintPipelineLegacy\n\n            deprecation_message = (\n                \"You are using a legacy checkpoint for inpainting with Stable Diffusion, therefore we are loading the\"\n                f\" {StableDiffusionInpaintPipelineLegacy} class instead of {StableDiffusionInpaintPipeline}. For\"\n                \" better inpainting results, we strongly suggest using Stable Diffusion's official inpainting\"\n                \" checkpoint: https://huggingface.co/runwayml/stable-diffusion-inpainting instead or adapting your\"\n                f\" checkpoint {pretrained_model_name_or_path} to the format of\"\n                \" https://huggingface.co/runwayml/stable-diffusion-inpainting. Note that we do not actively maintain\"\n                \" the {StableDiffusionInpaintPipelineLegacy} class and will likely remove it in version 1.0.0.\"\n            )\n            deprecate(\"StableDiffusionInpaintPipelineLegacy\", \"1.0.0\", deprecation_message, standard_warn=False)\n\n        # some modules can be passed directly to the init\n        # in this case they are already instantiated in `kwargs`\n        # extract them here\n        expected_modules, optional_kwargs = cls._get_signature_keys(pipeline_class)\n        passed_class_obj = {k: kwargs.pop(k) for k in expected_modules if k in kwargs}\n        passed_pipe_kwargs = {k: kwargs.pop(k) for k in optional_kwargs if k in kwargs}\n\n        init_dict, unused_kwargs, _ = pipeline_class.extract_init_dict(config_dict, **kwargs)\n\n        # define init kwargs\n        init_kwargs = {k: init_dict.pop(k) for k in optional_kwargs if k in init_dict}\n        init_kwargs = {**init_kwargs, **passed_pipe_kwargs}\n\n        # remove `null` components\n        def load_module(name, value):\n            if value[0] is None:\n                return False\n            if name in passed_class_obj and passed_class_obj[name] is None:\n                return False\n            return True\n\n        init_dict = {k: v for k, v in init_dict.items() if load_module(k, v)}\n\n        if len(unused_kwargs) > 0:\n            logger.warning(\n                f\"Keyword arguments {unused_kwargs} are not expected by {pipeline_class.__name__} and will be ignored.\"\n            )\n\n        # import it here to avoid circular import\n        from diffusers import pipelines\n\n        # 3. Load each module in the pipeline\n        for name, (library_name, class_name) in init_dict.items():\n            # 3.1 - now that JAX/Flax is an official framework of the library, we might load from Flax names\n            if class_name.startswith(\"Flax\"):\n                class_name = class_name[4:]\n\n            is_pipeline_module = hasattr(pipelines, library_name)\n            loaded_sub_model = None\n\n            # if the model is in a pipeline module, then we load it from the pipeline\n            if name in passed_class_obj:\n                # 1. check that passed_class_obj has correct parent class\n                if not is_pipeline_module:\n                    library = importlib.import_module(library_name)\n                    class_obj = getattr(library, class_name)\n                    importable_classes = LOADABLE_CLASSES[library_name]\n                    class_candidates = {c: getattr(library, c, None) for c in importable_classes.keys()}\n\n                    expected_class_obj = None\n                    for class_name, class_candidate in class_candidates.items():\n                        if class_candidate is not None and issubclass(class_obj, class_candidate):\n                            expected_class_obj = class_candidate\n\n                    if not issubclass(passed_class_obj[name].__class__, expected_class_obj):\n                        raise ValueError(\n                            f\"{passed_class_obj[name]} is of type: {type(passed_class_obj[name])}, but should be\"\n                            f\" {expected_class_obj}\"\n                        )\n                else:\n                    logger.warning(\n                        f\"You have passed a non-standard module {passed_class_obj[name]}. We cannot verify whether it\"\n                        \" has the correct type\"\n                    )\n\n                # set passed class object\n                loaded_sub_model = passed_class_obj[name]\n            elif is_pipeline_module:\n                pipeline_module = getattr(pipelines, library_name)\n                class_obj = getattr(pipeline_module, class_name)\n                importable_classes = ALL_IMPORTABLE_CLASSES\n                class_candidates = {c: class_obj for c in importable_classes.keys()}\n            else:\n                # else we just import it from the library.\n                library = importlib.import_module(library_name)\n\n                class_obj = getattr(library, class_name)\n                importable_classes = LOADABLE_CLASSES[library_name]\n                class_candidates = {c: getattr(library, c, None) for c in importable_classes.keys()}\n\n            if loaded_sub_model is None:\n                load_method_name = None\n                for class_name, class_candidate in class_candidates.items():\n                    if class_candidate is not None and issubclass(class_obj, class_candidate):\n                        load_method_name = importable_classes[class_name][1]\n\n                if load_method_name is None:\n                    none_module = class_obj.__module__\n                    is_dummy_path = none_module.startswith(DUMMY_MODULES_FOLDER) or none_module.startswith(\n                        TRANSFORMERS_DUMMY_MODULES_FOLDER\n                    )\n                    if is_dummy_path and \"dummy\" in none_module:\n                        # call class_obj for nice error message of missing requirements\n                        class_obj()\n\n                    raise ValueError(\n                        f\"The component {class_obj} of {pipeline_class} cannot be loaded as it does not seem to have\"\n                        f\" any of the loading methods defined in {ALL_IMPORTABLE_CLASSES}.\"\n                    )\n\n                load_method = getattr(class_obj, load_method_name)\n                loading_kwargs = {}\n\n                if issubclass(class_obj, torch.nn.Module):\n                    loading_kwargs[\"torch_dtype\"] = torch_dtype\n                if issubclass(class_obj, diffusers.OnnxRuntimeModel):\n                    loading_kwargs[\"provider\"] = provider\n                    loading_kwargs[\"sess_options\"] = sess_options\n\n                is_diffusers_model = issubclass(class_obj, diffusers.ModelMixin)\n                is_transformers_model = (\n                    is_transformers_available()\n                    and issubclass(class_obj, PreTrainedModel)\n                    and version.parse(version.parse(transformers.__version__).base_version) >= version.parse(\"4.20.0\")\n                )\n\n                # When loading a transformers model, if the device_map is None, the weights will be initialized as opposed to diffusers.\n                # To make default loading faster we set the `low_cpu_mem_usage=low_cpu_mem_usage` flag which is `True` by default.\n                # This makes sure that the weights won't be initialized which significantly speeds up loading.\n                if is_diffusers_model or is_transformers_model:\n                    loading_kwargs[\"device_map\"] = device_map\n                    loading_kwargs[\"low_cpu_mem_usage\"] = low_cpu_mem_usage\n\n                # check if the module is in a subdirectory\n                if os.path.isdir(os.path.join(cached_folder, name)):\n                    loaded_sub_model = load_method(os.path.join(cached_folder, name), **loading_kwargs)\n                else:\n                    # else load from the root directory\n                    loaded_sub_model = load_method(cached_folder, **loading_kwargs)\n\n            init_kwargs[name] = loaded_sub_model  # UNet(...), # DiffusionSchedule(...)\n\n        # 4. Potentially add passed objects if expected\n        missing_modules = set(expected_modules) - set(init_kwargs.keys())\n        passed_modules = list(passed_class_obj.keys())\n        optional_modules = pipeline_class._optional_components\n        if len(missing_modules) > 0 and missing_modules <= set(passed_modules + optional_modules):\n            for module in missing_modules:\n                init_kwargs[module] = passed_class_obj.get(module, None)\n        elif len(missing_modules) > 0:\n            passed_modules = set(list(init_kwargs.keys()) + list(passed_class_obj.keys())) - optional_kwargs\n            # raise ValueError(\n            #     f\"Pipeline {pipeline_class} expected {expected_modules}, but only {passed_modules} were passed.\"\n            # )\n\n        # 5. Instantiate the pipeline\n        model = pipeline_class(**init_kwargs)\n        return model\n\n    @staticmethod\n    def _get_signature_keys(obj):\n        parameters = inspect.signature(obj.__init__).parameters\n        required_parameters = {k: v for k, v in parameters.items() if v.default == inspect._empty}\n        optional_parameters = set({k for k, v in parameters.items() if v.default != inspect._empty})\n        expected_modules = set(required_parameters.keys()) - set([\"self\"])\n        return expected_modules, optional_parameters\n\n    @property\n    def components(self) -> Dict[str, Any]:\n        r\"\"\"\n\n        The `self.components` property can be useful to run different pipelines with the same weights and\n        configurations to not have to re-allocate memory.\n\n        Examples:\n\n        ```py\n        >>> from diffusers import (\n        ...     StableDiffusionPipeline,\n        ...     StableDiffusionImg2ImgPipeline,\n        ...     StableDiffusionInpaintPipeline,\n        ... )\n\n        >>> text2img = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n        >>> img2img = StableDiffusionImg2ImgPipeline(**text2img.components)\n        >>> inpaint = StableDiffusionInpaintPipeline(**text2img.components)\n        ```\n\n        Returns:\n            A dictionaly containing all the modules needed to initialize the pipeline.\n        \"\"\"\n        expected_modules, optional_parameters = self._get_signature_keys(self)\n        components = {\n            k: getattr(self, k) for k in self.config.keys() if not k.startswith(\"_\") and k not in optional_parameters\n        }\n\n        if set(components.keys()) != expected_modules:\n            raise ValueError(\n                f\"{self} has been incorrectly initialized or {self.__class__} is incorrectly implemented. Expected\"\n                f\" {expected_modules} to be defined, but {components} are defined.\"\n            )\n\n        return components\n\n    @staticmethod\n    def numpy_to_pil(images):\n        \"\"\"\n        Convert a numpy image or a batch of images to a PIL image.\n        \"\"\"\n        if images.ndim == 3:\n            images = images[None, ...]\n        images = (images * 255).round().astype(\"uint8\")\n        if images.shape[-1] == 1:\n            # special case for grayscale (single channel) images\n            pil_images = [Image.fromarray(image.squeeze(), mode=\"L\") for image in images]\n        else:\n            pil_images = [Image.fromarray(image) for image in images]\n\n        return pil_images\n\n    def progress_bar(self, iterable):\n        if not hasattr(self, \"_progress_bar_config\"):\n            self._progress_bar_config = {}\n        elif not isinstance(self._progress_bar_config, dict):\n            raise ValueError(\n                f\"`self._progress_bar_config` should be of type `dict`, but is {type(self._progress_bar_config)}.\"\n            )\n\n        return tqdm(iterable, **self._progress_bar_config)\n\n    def set_progress_bar_config(self, **kwargs):\n        self._progress_bar_config = kwargs", ""]}
{"filename": "diffusers_/scheduling_utils.py", "chunked_list": ["# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport importlib\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Optional, Union\n", "from typing import Any, Dict, Optional, Union\n\nimport torch\n\nfrom .utils import BaseOutput\n\n\nSCHEDULER_CONFIG_NAME = \"scheduler_config.json\"\n\n", "\n\n@dataclass\nclass SchedulerOutput(BaseOutput):\n    \"\"\"\n    Base class for the scheduler's step function output.\n\n    Args:\n        prev_sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` for images):\n            Computed sample (x_{t-1}) of previous timestep. `prev_sample` should be used as next model input in the\n            denoising loop.\n    \"\"\"\n\n    prev_sample: torch.FloatTensor", "\n\nclass SchedulerMixin:\n    \"\"\"\n    Mixin containing common functions for the schedulers.\n\n    Class attributes:\n        - **_compatibles** (`List[str]`) -- A list of classes that are compatible with the parent class, so that\n          `from_config` can be used from a class different than the one used to save the config (should be overridden\n          by parent class).\n    \"\"\"\n\n    config_name = SCHEDULER_CONFIG_NAME\n    _compatibles = []\n    has_compatibles = True\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        pretrained_model_name_or_path: Dict[str, Any] = None,\n        subfolder: Optional[str] = None,\n        return_unused_kwargs=False,\n        **kwargs,\n    ):\n        r\"\"\"\n        Instantiate a Scheduler class from a pre-defined JSON configuration file inside a directory or Hub repo.\n\n        Parameters:\n            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n                Can be either:\n\n                    - A string, the *model id* of a model repo on huggingface.co. Valid model ids should have an\n                      organization name, like `google/ddpm-celebahq-256`.\n                    - A path to a *directory* containing the schedluer configurations saved using\n                      [`~SchedulerMixin.save_pretrained`], e.g., `./my_model_directory/`.\n            subfolder (`str`, *optional*):\n                In case the relevant files are located inside a subfolder of the model repo (either remote in\n                huggingface.co or downloaded locally), you can specify the folder name here.\n            return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n                Whether kwargs that are not consumed by the Python class should be returned or not.\n            cache_dir (`Union[str, os.PathLike]`, *optional*):\n                Path to a directory in which a downloaded pretrained model configuration should be cached if the\n                standard cache should not be used.\n            force_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                cached versions if they exist.\n            resume_download (`bool`, *optional*, defaults to `False`):\n                Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n                file exists.\n            proxies (`Dict[str, str]`, *optional*):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n            output_loading_info(`bool`, *optional*, defaults to `False`):\n                Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n            local_files_only(`bool`, *optional*, defaults to `False`):\n                Whether or not to only look at local files (i.e., do not try to download the model).\n            use_auth_token (`str` or *bool*, *optional*):\n                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n                when running `transformers-cli login` (stored in `~/.huggingface`).\n            revision (`str`, *optional*, defaults to `\"main\"`):\n                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n                identifier allowed by git.\n\n        <Tip>\n\n         It is required to be logged in (`huggingface-cli login`) when you want to use private or [gated\n         models](https://huggingface.co/docs/hub/models-gated#gated-models).\n\n        </Tip>\n\n        <Tip>\n\n        Activate the special [\"offline-mode\"](https://huggingface.co/transformers/installation.html#offline-mode) to\n        use this method in a firewalled environment.\n\n        </Tip>\n\n        \"\"\"\n        config, kwargs = cls.load_config(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            subfolder=subfolder,\n            return_unused_kwargs=True,\n            **kwargs,\n        )\n        return cls.from_config(config, return_unused_kwargs=return_unused_kwargs, **kwargs)\n\n    def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs):\n        \"\"\"\n        Save a scheduler configuration object to the directory `save_directory`, so that it can be re-loaded using the\n        [`~SchedulerMixin.from_pretrained`] class method.\n\n        Args:\n            save_directory (`str` or `os.PathLike`):\n                Directory where the configuration JSON file will be saved (will be created if it does not exist).\n        \"\"\"\n        self.save_config(save_directory=save_directory, push_to_hub=push_to_hub, **kwargs)\n\n    @property\n    def compatibles(self):\n        \"\"\"\n        Returns all schedulers that are compatible with this scheduler\n\n        Returns:\n            `List[SchedulerMixin]`: List of compatible schedulers\n        \"\"\"\n        return self._get_compatibles()\n\n    @classmethod\n    def _get_compatibles(cls):\n        compatible_classes_str = list(set([cls.__name__] + cls._compatibles))\n        diffusers_library = importlib.import_module(__name__.split(\".\")[0])\n        compatible_classes = [\n            getattr(diffusers_library, c) for c in compatible_classes_str if hasattr(diffusers_library, c)\n        ]\n        return compatible_classes", ""]}
{"filename": "diffusers_/utils/outputs.py", "chunked_list": ["# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nGeneric utilities\n\"\"\"\n\nfrom collections import OrderedDict", "\nfrom collections import OrderedDict\nfrom dataclasses import fields\nfrom typing import Any, Tuple\n\nimport numpy as np\n\nfrom .import_utils import is_torch_available\n\n\ndef is_tensor(x):\n    \"\"\"\n    Tests if `x` is a `torch.Tensor` or `np.ndarray`.\n    \"\"\"\n    if is_torch_available():\n        import torch\n\n        if isinstance(x, torch.Tensor):\n            return True\n\n    return isinstance(x, np.ndarray)", "\n\ndef is_tensor(x):\n    \"\"\"\n    Tests if `x` is a `torch.Tensor` or `np.ndarray`.\n    \"\"\"\n    if is_torch_available():\n        import torch\n\n        if isinstance(x, torch.Tensor):\n            return True\n\n    return isinstance(x, np.ndarray)", "\n\nclass BaseOutput(OrderedDict):\n    \"\"\"\n    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a\n    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular\n    python dictionary.\n\n    <Tip warning={true}>\n\n    You can't unpack a `BaseOutput` directly. Use the [`~utils.BaseOutput.to_tuple`] method to convert it to a tuple\n    before.\n\n    </Tip>\n    \"\"\"\n\n    def __post_init__(self):\n        class_fields = fields(self)\n\n        # Safety and consistency checks\n        if not len(class_fields):\n            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n\n        first_field = getattr(self, class_fields[0].name)\n        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n\n        if other_fields_are_none and isinstance(first_field, dict):\n            for key, value in first_field.items():\n                self[key] = value\n        else:\n            for field in class_fields:\n                v = getattr(self, field.name)\n                if v is not None:\n                    self[field.name] = v\n\n    def __delitem__(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n\n    def setdefault(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n\n    def pop(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n\n    def update(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n\n    def __getitem__(self, k):\n        if isinstance(k, str):\n            inner_dict = {k: v for (k, v) in self.items()}\n            return inner_dict[k]\n        else:\n            return self.to_tuple()[k]\n\n    def __setattr__(self, name, value):\n        if name in self.keys() and value is not None:\n            # Don't call self.__setitem__ to avoid recursion errors\n            super().__setitem__(name, value)\n        super().__setattr__(name, value)\n\n    def __setitem__(self, key, value):\n        # Will raise a KeyException if needed\n        super().__setitem__(key, value)\n        # Don't call self.__setattr__ to avoid recursion errors\n        super().__setattr__(key, value)\n\n    def to_tuple(self) -> Tuple[Any]:\n        \"\"\"\n        Convert self to a tuple containing all the attributes/keys that are not `None`.\n        \"\"\"\n        return tuple(self[k] for k in self.keys())", ""]}
{"filename": "diffusers_/utils/deprecation_utils.py", "chunked_list": ["import inspect\nimport warnings\nfrom typing import Any, Dict, Optional, Union\n\nfrom packaging import version\n\n\ndef deprecate(*args, take_from: Optional[Union[Dict, Any]] = None, standard_warn=True):\n    from .. import __version__\n\n    deprecated_kwargs = take_from\n    values = ()\n    if not isinstance(args[0], tuple):\n        args = (args,)\n\n    for attribute, version_name, message in args:\n        if version.parse(version.parse(__version__).base_version) >= version.parse(version_name):\n            raise ValueError(\n                f\"The deprecation tuple {(attribute, version_name, message)} should be removed since diffusers'\"\n                f\" version {__version__} is >= {version_name}\"\n            )\n\n        warning = None\n        if isinstance(deprecated_kwargs, dict) and attribute in deprecated_kwargs:\n            values += (deprecated_kwargs.pop(attribute),)\n            warning = f\"The `{attribute}` argument is deprecated and will be removed in version {version_name}.\"\n        elif hasattr(deprecated_kwargs, attribute):\n            values += (getattr(deprecated_kwargs, attribute),)\n            warning = f\"The `{attribute}` attribute is deprecated and will be removed in version {version_name}.\"\n        elif deprecated_kwargs is None:\n            warning = f\"`{attribute}` is deprecated and will be removed in version {version_name}.\"\n\n        if warning is not None:\n            warning = warning + \" \" if standard_warn else \"\"\n            warnings.warn(warning + message, FutureWarning)\n\n    if isinstance(deprecated_kwargs, dict) and len(deprecated_kwargs) > 0:\n        call_frame = inspect.getouterframes(inspect.currentframe())[1]\n        filename = call_frame.filename\n        line_number = call_frame.lineno\n        function = call_frame.function\n        key, value = next(iter(deprecated_kwargs.items()))\n        raise TypeError(f\"{function} in {filename} line {line_number-1} got an unexpected keyword argument `{key}`\")\n\n    if len(values) == 0:\n        return\n    elif len(values) == 1:\n        return values[0]\n    return values", ""]}
{"filename": "diffusers_/utils/testing_utils.py", "chunked_list": ["import inspect\nimport logging\nimport os\nimport random\nimport re\nimport unittest\nimport urllib.parse\nfrom distutils.util import strtobool\nfrom io import BytesIO, StringIO\nfrom pathlib import Path", "from io import BytesIO, StringIO\nfrom pathlib import Path\nfrom typing import Union\n\nimport numpy as np\n\nimport PIL.Image\nimport PIL.ImageOps\nimport requests\nfrom packaging import version", "import requests\nfrom packaging import version\n\nfrom .import_utils import is_flax_available, is_onnx_available, is_torch_available\n\n\nglobal_rng = random.Random()\n\n\nif is_torch_available():\n    import torch\n\n    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    is_torch_higher_equal_than_1_12 = version.parse(version.parse(torch.__version__).base_version) >= version.parse(\n        \"1.12\"\n    )\n\n    if is_torch_higher_equal_than_1_12:\n        # Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details\n        mps_backend_registered = hasattr(torch.backends, \"mps\")\n        torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device", "\nif is_torch_available():\n    import torch\n\n    torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    is_torch_higher_equal_than_1_12 = version.parse(version.parse(torch.__version__).base_version) >= version.parse(\n        \"1.12\"\n    )\n\n    if is_torch_higher_equal_than_1_12:\n        # Some builds of torch 1.12 don't have the mps backend registered. See #892 for more details\n        mps_backend_registered = hasattr(torch.backends, \"mps\")\n        torch_device = \"mps\" if (mps_backend_registered and torch.backends.mps.is_available()) else torch_device", "\n\ndef torch_all_close(a, b, *args, **kwargs):\n    if not is_torch_available():\n        raise ValueError(\"PyTorch needs to be installed to use this function.\")\n    if not torch.allclose(a, b, *args, **kwargs):\n        assert False, f\"Max diff is absolute {(a - b).abs().max()}. Diff tensor is {(a - b).abs()}.\"\n    return True\n\n\ndef get_tests_dir(append_path=None):\n    \"\"\"\n    Args:\n        append_path: optional path to append to the tests dir path\n    Return:\n        The full path to the `tests` dir, so that the tests can be invoked from anywhere. Optionally `append_path` is\n        joined after the `tests` dir the former is provided.\n    \"\"\"\n    # this function caller's __file__\n    caller__file__ = inspect.stack()[1][1]\n    tests_dir = os.path.abspath(os.path.dirname(caller__file__))\n\n    while not tests_dir.endswith(\"tests\"):\n        tests_dir = os.path.dirname(tests_dir)\n\n    if append_path:\n        return os.path.join(tests_dir, append_path)\n    else:\n        return tests_dir", "\n\ndef get_tests_dir(append_path=None):\n    \"\"\"\n    Args:\n        append_path: optional path to append to the tests dir path\n    Return:\n        The full path to the `tests` dir, so that the tests can be invoked from anywhere. Optionally `append_path` is\n        joined after the `tests` dir the former is provided.\n    \"\"\"\n    # this function caller's __file__\n    caller__file__ = inspect.stack()[1][1]\n    tests_dir = os.path.abspath(os.path.dirname(caller__file__))\n\n    while not tests_dir.endswith(\"tests\"):\n        tests_dir = os.path.dirname(tests_dir)\n\n    if append_path:\n        return os.path.join(tests_dir, append_path)\n    else:\n        return tests_dir", "\n\ndef parse_flag_from_env(key, default=False):\n    try:\n        value = os.environ[key]\n    except KeyError:\n        # KEY isn't set, default to `default`.\n        _value = default\n    else:\n        # KEY is set, convert it to True or False.\n        try:\n            _value = strtobool(value)\n        except ValueError:\n            # More values are supported, but let's keep the message simple.\n            raise ValueError(f\"If set, {key} must be yes or no.\")\n    return _value", "\n\n_run_slow_tests = parse_flag_from_env(\"RUN_SLOW\", default=False)\n\n\ndef floats_tensor(shape, scale=1.0, rng=None, name=None):\n    \"\"\"Creates a random float32 tensor\"\"\"\n    if rng is None:\n        rng = global_rng\n\n    total_dims = 1\n    for dim in shape:\n        total_dims *= dim\n\n    values = []\n    for _ in range(total_dims):\n        values.append(rng.random() * scale)\n\n    return torch.tensor(data=values, dtype=torch.float).view(shape).contiguous()", "\n\ndef slow(test_case):\n    \"\"\"\n    Decorator marking a test as slow.\n\n    Slow tests are skipped by default. Set the RUN_SLOW environment variable to a truthy value to run them.\n\n    \"\"\"\n    return unittest.skipUnless(_run_slow_tests, \"test is slow\")(test_case)", "\n\ndef require_torch(test_case):\n    \"\"\"\n    Decorator marking a test that requires PyTorch. These tests are skipped when PyTorch isn't installed.\n    \"\"\"\n    return unittest.skipUnless(is_torch_available(), \"test requires PyTorch\")(test_case)\n\n\ndef require_torch_gpu(test_case):\n    \"\"\"Decorator marking a test that requires CUDA and PyTorch.\"\"\"\n    return unittest.skipUnless(is_torch_available() and torch_device == \"cuda\", \"test requires PyTorch+CUDA\")(\n        test_case\n    )", "\ndef require_torch_gpu(test_case):\n    \"\"\"Decorator marking a test that requires CUDA and PyTorch.\"\"\"\n    return unittest.skipUnless(is_torch_available() and torch_device == \"cuda\", \"test requires PyTorch+CUDA\")(\n        test_case\n    )\n\n\ndef require_flax(test_case):\n    \"\"\"\n    Decorator marking a test that requires JAX & Flax. These tests are skipped when one / both are not installed\n    \"\"\"\n    return unittest.skipUnless(is_flax_available(), \"test requires JAX & Flax\")(test_case)", "def require_flax(test_case):\n    \"\"\"\n    Decorator marking a test that requires JAX & Flax. These tests are skipped when one / both are not installed\n    \"\"\"\n    return unittest.skipUnless(is_flax_available(), \"test requires JAX & Flax\")(test_case)\n\n\ndef require_onnxruntime(test_case):\n    \"\"\"\n    Decorator marking a test that requires onnxruntime. These tests are skipped when onnxruntime isn't installed.\n    \"\"\"\n    return unittest.skipUnless(is_onnx_available(), \"test requires onnxruntime\")(test_case)", "\n\ndef load_numpy(arry: Union[str, np.ndarray]) -> np.ndarray:\n    if isinstance(arry, str):\n        if arry.startswith(\"http://\") or arry.startswith(\"https://\"):\n            response = requests.get(arry)\n            response.raise_for_status()\n            arry = np.load(BytesIO(response.content))\n        elif os.path.isfile(arry):\n            arry = np.load(arry)\n        else:\n            raise ValueError(\n                f\"Incorrect path or url, URLs must start with `http://` or `https://`, and {arry} is not a valid path\"\n            )\n    elif isinstance(arry, np.ndarray):\n        pass\n    else:\n        raise ValueError(\n            \"Incorrect format used for numpy ndarray. Should be an url linking to an image, a local path, or a\"\n            \" ndarray.\"\n        )\n\n    return arry", "\n\ndef load_image(image: Union[str, PIL.Image.Image]) -> PIL.Image.Image:\n    \"\"\"\n    Args:\n    Loads `image` to a PIL Image.\n        image (`str` or `PIL.Image.Image`):\n            The image to convert to the PIL Image format.\n    Returns:\n        `PIL.Image.Image`: A PIL Image.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http://\") or image.startswith(\"https://\"):\n            image = PIL.Image.open(requests.get(image, stream=True).raw)\n        elif os.path.isfile(image):\n            image = PIL.Image.open(image)\n        else:\n            raise ValueError(\n                f\"Incorrect path or url, URLs must start with `http://` or `https://`, and {image} is not a valid path\"\n            )\n    elif isinstance(image, PIL.Image.Image):\n        image = image\n    else:\n        raise ValueError(\n            \"Incorrect format used for image. Should be an url linking to an image, a local path, or a PIL image.\"\n        )\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert(\"RGB\")\n    return image", "\n\ndef load_hf_numpy(path) -> np.ndarray:\n    if not path.startswith(\"http://\") or path.startswith(\"https://\"):\n        path = os.path.join(\n            \"https://huggingface.co/datasets/fusing/diffusers-testing/resolve/main\", urllib.parse.quote(path)\n        )\n\n    return load_numpy(path)\n", "\n\n# --- pytest conf functions --- #\n\n# to avoid multiple invocation from tests/conftest.py and examples/conftest.py - make sure it's called only once\npytest_opt_registered = {}\n\n\ndef pytest_addoption_shared(parser):\n    \"\"\"\n    This function is to be called from `conftest.py` via `pytest_addoption` wrapper that has to be defined there.\n\n    It allows loading both `conftest.py` files at once without causing a failure due to adding the same `pytest`\n    option.\n\n    \"\"\"\n    option = \"--make-reports\"\n    if option not in pytest_opt_registered:\n        parser.addoption(\n            option,\n            action=\"store\",\n            default=False,\n            help=\"generate report files. The value of this option is used as a prefix to report names\",\n        )\n        pytest_opt_registered[option] = 1", "def pytest_addoption_shared(parser):\n    \"\"\"\n    This function is to be called from `conftest.py` via `pytest_addoption` wrapper that has to be defined there.\n\n    It allows loading both `conftest.py` files at once without causing a failure due to adding the same `pytest`\n    option.\n\n    \"\"\"\n    option = \"--make-reports\"\n    if option not in pytest_opt_registered:\n        parser.addoption(\n            option,\n            action=\"store\",\n            default=False,\n            help=\"generate report files. The value of this option is used as a prefix to report names\",\n        )\n        pytest_opt_registered[option] = 1", "\n\ndef pytest_terminal_summary_main(tr, id):\n    \"\"\"\n    Generate multiple reports at the end of test suite run - each report goes into a dedicated file in the current\n    directory. The report files are prefixed with the test suite name.\n\n    This function emulates --duration and -rA pytest arguments.\n\n    This function is to be called from `conftest.py` via `pytest_terminal_summary` wrapper that has to be defined\n    there.\n\n    Args:\n    - tr: `terminalreporter` passed from `conftest.py`\n    - id: unique id like `tests` or `examples` that will be incorporated into the final reports filenames - this is\n      needed as some jobs have multiple runs of pytest, so we can't have them overwrite each other.\n\n    NB: this functions taps into a private _pytest API and while unlikely, it could break should\n    pytest do internal changes - also it calls default internal methods of terminalreporter which\n    can be hijacked by various `pytest-` plugins and interfere.\n\n    \"\"\"\n    from _pytest.config import create_terminal_writer\n\n    if not len(id):\n        id = \"tests\"\n\n    config = tr.config\n    orig_writer = config.get_terminal_writer()\n    orig_tbstyle = config.option.tbstyle\n    orig_reportchars = tr.reportchars\n\n    dir = \"reports\"\n    Path(dir).mkdir(parents=True, exist_ok=True)\n    report_files = {\n        k: f\"{dir}/{id}_{k}.txt\"\n        for k in [\n            \"durations\",\n            \"errors\",\n            \"failures_long\",\n            \"failures_short\",\n            \"failures_line\",\n            \"passes\",\n            \"stats\",\n            \"summary_short\",\n            \"warnings\",\n        ]\n    }\n\n    # custom durations report\n    # note: there is no need to call pytest --durations=XX to get this separate report\n    # adapted from https://github.com/pytest-dev/pytest/blob/897f151e/src/_pytest/runner.py#L66\n    dlist = []\n    for replist in tr.stats.values():\n        for rep in replist:\n            if hasattr(rep, \"duration\"):\n                dlist.append(rep)\n    if dlist:\n        dlist.sort(key=lambda x: x.duration, reverse=True)\n        with open(report_files[\"durations\"], \"w\") as f:\n            durations_min = 0.05  # sec\n            f.write(\"slowest durations\\n\")\n            for i, rep in enumerate(dlist):\n                if rep.duration < durations_min:\n                    f.write(f\"{len(dlist)-i} durations < {durations_min} secs were omitted\")\n                    break\n                f.write(f\"{rep.duration:02.2f}s {rep.when:<8} {rep.nodeid}\\n\")\n\n    def summary_failures_short(tr):\n        # expecting that the reports were --tb=long (default) so we chop them off here to the last frame\n        reports = tr.getreports(\"failed\")\n        if not reports:\n            return\n        tr.write_sep(\"=\", \"FAILURES SHORT STACK\")\n        for rep in reports:\n            msg = tr._getfailureheadline(rep)\n            tr.write_sep(\"_\", msg, red=True, bold=True)\n            # chop off the optional leading extra frames, leaving only the last one\n            longrepr = re.sub(r\".*_ _ _ (_ ){10,}_ _ \", \"\", rep.longreprtext, 0, re.M | re.S)\n            tr._tw.line(longrepr)\n            # note: not printing out any rep.sections to keep the report short\n\n    # use ready-made report funcs, we are just hijacking the filehandle to log to a dedicated file each\n    # adapted from https://github.com/pytest-dev/pytest/blob/897f151e/src/_pytest/terminal.py#L814\n    # note: some pytest plugins may interfere by hijacking the default `terminalreporter` (e.g.\n    # pytest-instafail does that)\n\n    # report failures with line/short/long styles\n    config.option.tbstyle = \"auto\"  # full tb\n    with open(report_files[\"failures_long\"], \"w\") as f:\n        tr._tw = create_terminal_writer(config, f)\n        tr.summary_failures()\n\n    # config.option.tbstyle = \"short\" # short tb\n    with open(report_files[\"failures_short\"], \"w\") as f:\n        tr._tw = create_terminal_writer(config, f)\n        summary_failures_short(tr)\n\n    config.option.tbstyle = \"line\"  # one line per error\n    with open(report_files[\"failures_line\"], \"w\") as f:\n        tr._tw = create_terminal_writer(config, f)\n        tr.summary_failures()\n\n    with open(report_files[\"errors\"], \"w\") as f:\n        tr._tw = create_terminal_writer(config, f)\n        tr.summary_errors()\n\n    with open(report_files[\"warnings\"], \"w\") as f:\n        tr._tw = create_terminal_writer(config, f)\n        tr.summary_warnings()  # normal warnings\n        tr.summary_warnings()  # final warnings\n\n    tr.reportchars = \"wPpsxXEf\"  # emulate -rA (used in summary_passes() and short_test_summary())\n    with open(report_files[\"passes\"], \"w\") as f:\n        tr._tw = create_terminal_writer(config, f)\n        tr.summary_passes()\n\n    with open(report_files[\"summary_short\"], \"w\") as f:\n        tr._tw = create_terminal_writer(config, f)\n        tr.short_test_summary()\n\n    with open(report_files[\"stats\"], \"w\") as f:\n        tr._tw = create_terminal_writer(config, f)\n        tr.summary_stats()\n\n    # restore:\n    tr._tw = orig_writer\n    tr.reportchars = orig_reportchars\n    config.option.tbstyle = orig_tbstyle", "\n\nclass CaptureLogger:\n    \"\"\"\n    Args:\n    Context manager to capture `logging` streams\n        logger: 'logging` logger object\n    Returns:\n        The captured output is available via `self.out`\n    Example:\n    ```python\n    >>> from diffusers import logging\n    >>> from diffusers.testing_utils import CaptureLogger\n\n    >>> msg = \"Testing 1, 2, 3\"\n    >>> logging.set_verbosity_info()\n    >>> logger = logging.get_logger(\"diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.py\")\n    >>> with CaptureLogger(logger) as cl:\n    ...     logger.info(msg)\n    >>> assert cl.out, msg + \"\\n\"\n    ```\n    \"\"\"\n\n    def __init__(self, logger):\n        self.logger = logger\n        self.io = StringIO()\n        self.sh = logging.StreamHandler(self.io)\n        self.out = \"\"\n\n    def __enter__(self):\n        self.logger.addHandler(self.sh)\n        return self\n\n    def __exit__(self, *exc):\n        self.logger.removeHandler(self.sh)\n        self.out = self.io.getvalue()\n\n    def __repr__(self):\n        return f\"captured: {self.out}\\n\"", ""]}
{"filename": "diffusers_/utils/import_utils.py", "chunked_list": ["# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nImport utilities: Utilities related to imports and our lazy inits.\n\"\"\"\nimport importlib.util\nimport operator as op", "import importlib.util\nimport operator as op\nimport os\nimport sys\nfrom collections import OrderedDict\nfrom typing import Union\n\nfrom packaging import version\nfrom packaging.version import Version, parse\n", "from packaging.version import Version, parse\n\nfrom . import logging\n\n\n# The package importlib_metadata is in a different place, depending on the python version.\nif sys.version_info < (3, 8):\n    import importlib_metadata\nelse:\n    import importlib.metadata as importlib_metadata", "\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\nENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}\nENV_VARS_TRUE_AND_AUTO_VALUES = ENV_VARS_TRUE_VALUES.union({\"AUTO\"})\n\nUSE_TF = os.environ.get(\"USE_TF\", \"AUTO\").upper()\nUSE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\nUSE_JAX = os.environ.get(\"USE_FLAX\", \"AUTO\").upper()", "USE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\nUSE_JAX = os.environ.get(\"USE_FLAX\", \"AUTO\").upper()\n\nSTR_OPERATION_TO_FUNC = {\">\": op.gt, \">=\": op.ge, \"==\": op.eq, \"!=\": op.ne, \"<=\": op.le, \"<\": op.lt}\n\n_torch_version = \"N/A\"\nif USE_TORCH in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TF not in ENV_VARS_TRUE_VALUES:\n    _torch_available = importlib.util.find_spec(\"torch\") is not None\n    if _torch_available:\n        try:\n            _torch_version = importlib_metadata.version(\"torch\")\n            logger.info(f\"PyTorch version {_torch_version} available.\")\n        except importlib_metadata.PackageNotFoundError:\n            _torch_available = False\nelse:\n    logger.info(\"Disabling PyTorch because USE_TF is set\")\n    _torch_available = False", "\n\n_tf_version = \"N/A\"\nif USE_TF in ENV_VARS_TRUE_AND_AUTO_VALUES and USE_TORCH not in ENV_VARS_TRUE_VALUES:\n    _tf_available = importlib.util.find_spec(\"tensorflow\") is not None\n    if _tf_available:\n        candidates = (\n            \"tensorflow\",\n            \"tensorflow-cpu\",\n            \"tensorflow-gpu\",\n            \"tf-nightly\",\n            \"tf-nightly-cpu\",\n            \"tf-nightly-gpu\",\n            \"intel-tensorflow\",\n            \"intel-tensorflow-avx512\",\n            \"tensorflow-rocm\",\n            \"tensorflow-macos\",\n            \"tensorflow-aarch64\",\n        )\n        _tf_version = None\n        # For the metadata, we have to look for both tensorflow and tensorflow-cpu\n        for pkg in candidates:\n            try:\n                _tf_version = importlib_metadata.version(pkg)\n                break\n            except importlib_metadata.PackageNotFoundError:\n                pass\n        _tf_available = _tf_version is not None\n    if _tf_available:\n        if version.parse(_tf_version) < version.parse(\"2\"):\n            logger.info(f\"TensorFlow found but with version {_tf_version}. Diffusers requires version 2 minimum.\")\n            _tf_available = False\n        else:\n            logger.info(f\"TensorFlow version {_tf_version} available.\")\nelse:\n    logger.info(\"Disabling Tensorflow because USE_TORCH is set\")\n    _tf_available = False", "\n_jax_version = \"N/A\"\n_flax_version = \"N/A\"\nif USE_JAX in ENV_VARS_TRUE_AND_AUTO_VALUES:\n    _flax_available = importlib.util.find_spec(\"jax\") is not None and importlib.util.find_spec(\"flax\") is not None\n    if _flax_available:\n        try:\n            _jax_version = importlib_metadata.version(\"jax\")\n            _flax_version = importlib_metadata.version(\"flax\")\n            logger.info(f\"JAX version {_jax_version}, Flax version {_flax_version} available.\")\n        except importlib_metadata.PackageNotFoundError:\n            _flax_available = False\nelse:\n    _flax_available = False", "\n\n_transformers_available = importlib.util.find_spec(\"transformers\") is not None\ntry:\n    _transformers_version = importlib_metadata.version(\"transformers\")\n    logger.debug(f\"Successfully imported transformers version {_transformers_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _transformers_available = False\n\n", "\n\n_inflect_available = importlib.util.find_spec(\"inflect\") is not None\ntry:\n    _inflect_version = importlib_metadata.version(\"inflect\")\n    logger.debug(f\"Successfully imported inflect version {_inflect_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _inflect_available = False\n\n", "\n\n_unidecode_available = importlib.util.find_spec(\"unidecode\") is not None\ntry:\n    _unidecode_version = importlib_metadata.version(\"unidecode\")\n    logger.debug(f\"Successfully imported unidecode version {_unidecode_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _unidecode_available = False\n\n", "\n\n_modelcards_available = importlib.util.find_spec(\"modelcards\") is not None\ntry:\n    _modelcards_version = importlib_metadata.version(\"modelcards\")\n    logger.debug(f\"Successfully imported modelcards version {_modelcards_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _modelcards_available = False\n\n", "\n\n_onnxruntime_version = \"N/A\"\n_onnx_available = importlib.util.find_spec(\"onnxruntime\") is not None\nif _onnx_available:\n    candidates = (\"onnxruntime\", \"onnxruntime-gpu\", \"onnxruntime-directml\", \"onnxruntime-openvino\")\n    _onnxruntime_version = None\n    # For the metadata, we have to look for both onnxruntime and onnxruntime-gpu\n    for pkg in candidates:\n        try:\n            _onnxruntime_version = importlib_metadata.version(pkg)\n            break\n        except importlib_metadata.PackageNotFoundError:\n            pass\n    _onnx_available = _onnxruntime_version is not None\n    if _onnx_available:\n        logger.debug(f\"Successfully imported onnxruntime version {_onnxruntime_version}\")", "\n\n_scipy_available = importlib.util.find_spec(\"scipy\") is not None\ntry:\n    _scipy_version = importlib_metadata.version(\"scipy\")\n    logger.debug(f\"Successfully imported transformers version {_scipy_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _scipy_available = False\n\n_accelerate_available = importlib.util.find_spec(\"accelerate\") is not None\ntry:\n    _accelerate_version = importlib_metadata.version(\"accelerate\")\n    logger.debug(f\"Successfully imported accelerate version {_accelerate_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _accelerate_available = False", "\n_accelerate_available = importlib.util.find_spec(\"accelerate\") is not None\ntry:\n    _accelerate_version = importlib_metadata.version(\"accelerate\")\n    logger.debug(f\"Successfully imported accelerate version {_accelerate_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _accelerate_available = False\n\n_xformers_available = importlib.util.find_spec(\"xformers\") is not None\ntry:\n    _xformers_version = importlib_metadata.version(\"xformers\")\n    if _torch_available:\n        import torch\n\n        if torch.__version__ < version.Version(\"1.12\"):\n            raise ValueError(\"PyTorch should be >= 1.12\")\n    logger.debug(f\"Successfully imported xformers version {_xformers_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _xformers_available = False", "_xformers_available = importlib.util.find_spec(\"xformers\") is not None\ntry:\n    _xformers_version = importlib_metadata.version(\"xformers\")\n    if _torch_available:\n        import torch\n\n        if torch.__version__ < version.Version(\"1.12\"):\n            raise ValueError(\"PyTorch should be >= 1.12\")\n    logger.debug(f\"Successfully imported xformers version {_xformers_version}\")\nexcept importlib_metadata.PackageNotFoundError:\n    _xformers_available = False", "\n\ndef is_torch_available():\n    return _torch_available\n\n\ndef is_tf_available():\n    return _tf_available\n\n\ndef is_flax_available():\n    return _flax_available", "\n\ndef is_flax_available():\n    return _flax_available\n\n\ndef is_transformers_available():\n    return _transformers_available\n\n\ndef is_inflect_available():\n    return _inflect_available", "\n\ndef is_inflect_available():\n    return _inflect_available\n\n\ndef is_unidecode_available():\n    return _unidecode_available\n\n\ndef is_modelcards_available():\n    return _modelcards_available", "\n\ndef is_modelcards_available():\n    return _modelcards_available\n\n\ndef is_onnx_available():\n    return _onnx_available\n\n\ndef is_scipy_available():\n    return _scipy_available", "\n\ndef is_scipy_available():\n    return _scipy_available\n\n\ndef is_xformers_available():\n    return _xformers_available\n\n\ndef is_accelerate_available():\n    return _accelerate_available", "\n\ndef is_accelerate_available():\n    return _accelerate_available\n\n\n# docstyle-ignore\nFLAX_IMPORT_ERROR = \"\"\"\n{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://github.com/google/flax and follow the ones that match your environment.", "{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://github.com/google/flax and follow the ones that match your environment.\n\"\"\"\n\n# docstyle-ignore\nINFLECT_IMPORT_ERROR = \"\"\"\n{0} requires the inflect library but it was not found in your environment. You can install it with pip: `pip install\ninflect`\n\"\"\"\n", "\"\"\"\n\n# docstyle-ignore\nPYTORCH_IMPORT_ERROR = \"\"\"\n{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n\"\"\"\n\n# docstyle-ignore\nONNX_IMPORT_ERROR = \"\"\"", "# docstyle-ignore\nONNX_IMPORT_ERROR = \"\"\"\n{0} requires the onnxruntime library but it was not found in your environment. You can install it with pip: `pip\ninstall onnxruntime`\n\"\"\"\n\n# docstyle-ignore\nSCIPY_IMPORT_ERROR = \"\"\"\n{0} requires the scipy library but it was not found in your environment. You can install it with pip: `pip install\nscipy`", "{0} requires the scipy library but it was not found in your environment. You can install it with pip: `pip install\nscipy`\n\"\"\"\n\n# docstyle-ignore\nTENSORFLOW_IMPORT_ERROR = \"\"\"\n{0} requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n\"\"\"\n", "\"\"\"\n\n# docstyle-ignore\nTRANSFORMERS_IMPORT_ERROR = \"\"\"\n{0} requires the transformers library but it was not found in your environment. You can install it with pip: `pip\ninstall transformers`\n\"\"\"\n\n# docstyle-ignore\nUNIDECODE_IMPORT_ERROR = \"\"\"", "# docstyle-ignore\nUNIDECODE_IMPORT_ERROR = \"\"\"\n{0} requires the unidecode library but it was not found in your environment. You can install it with pip: `pip install\nUnidecode`\n\"\"\"\n\n\nBACKENDS_MAPPING = OrderedDict(\n    [\n        (\"flax\", (is_flax_available, FLAX_IMPORT_ERROR)),", "    [\n        (\"flax\", (is_flax_available, FLAX_IMPORT_ERROR)),\n        (\"inflect\", (is_inflect_available, INFLECT_IMPORT_ERROR)),\n        (\"onnx\", (is_onnx_available, ONNX_IMPORT_ERROR)),\n        (\"scipy\", (is_scipy_available, SCIPY_IMPORT_ERROR)),\n        (\"tf\", (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n        (\"torch\", (is_torch_available, PYTORCH_IMPORT_ERROR)),\n        (\"transformers\", (is_transformers_available, TRANSFORMERS_IMPORT_ERROR)),\n        (\"unidecode\", (is_unidecode_available, UNIDECODE_IMPORT_ERROR)),\n    ]", "        (\"unidecode\", (is_unidecode_available, UNIDECODE_IMPORT_ERROR)),\n    ]\n)\n\n\ndef requires_backends(obj, backends):\n    if not isinstance(backends, (list, tuple)):\n        backends = [backends]\n\n    name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n    checks = (BACKENDS_MAPPING[backend] for backend in backends)\n    failed = [msg.format(name) for available, msg in checks if not available()]\n    if failed:\n        raise ImportError(\"\".join(failed))\n\n    if name in [\n        \"VersatileDiffusionTextToImagePipeline\",\n        \"VersatileDiffusionPipeline\",\n        \"VersatileDiffusionDualGuidedPipeline\",\n        \"StableDiffusionImageVariationPipeline\",\n    ] and is_transformers_version(\"<\", \"4.25.0.dev0\"):\n        raise ImportError(\n            f\"You need to install `transformers` from 'main' in order to use {name}: \\n```\\n pip install\"\n            \" git+https://github.com/huggingface/transformers \\n```\"\n        )", "\n\nclass DummyObject(type):\n    \"\"\"\n    Metaclass for the dummy objects. Any class inheriting from it will return the ImportError generated by\n    `requires_backend` each time a user tries to access any method of that class.\n    \"\"\"\n\n    def __getattr__(cls, key):\n        if key.startswith(\"_\"):\n            return super().__getattr__(cls, key)\n        requires_backends(cls, cls._backends)", "\n\n# This function was copied from: https://github.com/huggingface/accelerate/blob/874c4967d94badd24f893064cc3bef45f57cadf7/src/accelerate/utils/versions.py#L319\ndef compare_versions(library_or_version: Union[str, Version], operation: str, requirement_version: str):\n    \"\"\"\n    Args:\n    Compares a library version to some requirement using a given operation.\n        library_or_version (`str` or `packaging.version.Version`):\n            A library name or a version to check.\n        operation (`str`):\n            A string representation of an operator, such as `\">\"` or `\"<=\"`.\n        requirement_version (`str`):\n            The version to compare the library version against\n    \"\"\"\n    if operation not in STR_OPERATION_TO_FUNC.keys():\n        raise ValueError(f\"`operation` must be one of {list(STR_OPERATION_TO_FUNC.keys())}, received {operation}\")\n    operation = STR_OPERATION_TO_FUNC[operation]\n    if isinstance(library_or_version, str):\n        library_or_version = parse(importlib_metadata.version(library_or_version))\n    return operation(library_or_version, parse(requirement_version))", "\n\n# This function was copied from: https://github.com/huggingface/accelerate/blob/874c4967d94badd24f893064cc3bef45f57cadf7/src/accelerate/utils/versions.py#L338\ndef is_torch_version(operation: str, version: str):\n    \"\"\"\n    Args:\n    Compares the current PyTorch version to a given reference with an operation.\n        operation (`str`):\n            A string representation of an operator, such as `\">\"` or `\"<=\"`\n        version (`str`):\n            A string version of PyTorch\n    \"\"\"\n    return compare_versions(parse(_torch_version), operation, version)", "\n\ndef is_transformers_version(operation: str, version: str):\n    \"\"\"\n    Args:\n    Compares the current Transformers version to a given reference with an operation.\n        operation (`str`):\n            A string representation of an operator, such as `\">\"` or `\"<=\"`\n        version (`str`):\n            A string version of PyTorch\n    \"\"\"\n    if not _transformers_available:\n        return False\n    return compare_versions(parse(_transformers_version), operation, version)", ""]}
{"filename": "diffusers_/utils/pil_utils.py", "chunked_list": ["import PIL.Image\nimport PIL.ImageOps\nfrom packaging import version\n\n\nif version.parse(version.parse(PIL.__version__).base_version) >= version.parse(\"9.1.0\"):\n    PIL_INTERPOLATION = {\n        \"linear\": PIL.Image.Resampling.BILINEAR,\n        \"bilinear\": PIL.Image.Resampling.BILINEAR,\n        \"bicubic\": PIL.Image.Resampling.BICUBIC,\n        \"lanczos\": PIL.Image.Resampling.LANCZOS,\n        \"nearest\": PIL.Image.Resampling.NEAREST,\n    }\nelse:\n    PIL_INTERPOLATION = {\n        \"linear\": PIL.Image.LINEAR,\n        \"bilinear\": PIL.Image.BILINEAR,\n        \"bicubic\": PIL.Image.BICUBIC,\n        \"lanczos\": PIL.Image.LANCZOS,\n        \"nearest\": PIL.Image.NEAREST,\n    }", ""]}
{"filename": "diffusers_/utils/__init__.py", "chunked_list": ["# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\n\nfrom .deprecation_utils import deprecate", "\nfrom .deprecation_utils import deprecate\nfrom .import_utils import (\n    ENV_VARS_TRUE_AND_AUTO_VALUES,\n    ENV_VARS_TRUE_VALUES,\n    USE_JAX,\n    USE_TF,\n    USE_TORCH,\n    DummyObject,\n    is_accelerate_available,", "    DummyObject,\n    is_accelerate_available,\n    is_flax_available,\n    is_inflect_available,\n    is_modelcards_available,\n    is_onnx_available,\n    is_scipy_available,\n    is_tf_available,\n    is_torch_available,\n    is_torch_version,", "    is_torch_available,\n    is_torch_version,\n    is_transformers_available,\n    is_transformers_version,\n    is_unidecode_available,\n    requires_backends,\n)\nfrom .logging import get_logger\nfrom .outputs import BaseOutput\nfrom .pil_utils import PIL_INTERPOLATION", "from .outputs import BaseOutput\nfrom .pil_utils import PIL_INTERPOLATION\n\n\nif is_torch_available():\n    from .testing_utils import (\n        floats_tensor,\n        load_hf_numpy,\n        load_image,\n        load_numpy,\n        parse_flag_from_env,\n        require_torch_gpu,\n        slow,\n        torch_all_close,\n        torch_device,\n    )", "\n\nlogger = get_logger(__name__)\n\n\nhf_cache_home = os.path.expanduser(\n    os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\"))\n)\ndefault_cache_path = os.path.join(hf_cache_home, \"diffusers\")\n", "default_cache_path = os.path.join(hf_cache_home, \"diffusers\")\n\n\nCONFIG_NAME = \"config.json\"\nWEIGHTS_NAME = \"diffusion_pytorch_model.bin\"\nFLAX_WEIGHTS_NAME = \"diffusion_flax_model.msgpack\"\nONNX_WEIGHTS_NAME = \"model.onnx\"\nONNX_EXTERNAL_WEIGHTS_NAME = \"weights.pb\"\nHUGGINGFACE_CO_RESOLVE_ENDPOINT = \"https://huggingface.co\"\nDIFFUSERS_CACHE = default_cache_path", "HUGGINGFACE_CO_RESOLVE_ENDPOINT = \"https://huggingface.co\"\nDIFFUSERS_CACHE = default_cache_path\nDIFFUSERS_DYNAMIC_MODULE_NAME = \"diffusers_modules\"\nHF_MODULES_CACHE = os.getenv(\"HF_MODULES_CACHE\", os.path.join(hf_cache_home, \"modules\"))\n\n_COMPATIBLE_STABLE_DIFFUSION_SCHEDULERS = [\n    \"DDIMScheduler\",\n    \"DDPMScheduler\",\n    \"PNDMScheduler\",\n    \"LMSDiscreteScheduler\",", "    \"PNDMScheduler\",\n    \"LMSDiscreteScheduler\",\n    \"EulerDiscreteScheduler\",\n    \"EulerAncestralDiscreteScheduler\",\n    \"DPMSolverMultistepScheduler\",\n]\n"]}
{"filename": "diffusers_/utils/logging.py", "chunked_list": ["# coding=utf-8\n# Copyright 2020 Optuna, Hugging Face\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Logging utilities.\"\"\"\n\nimport logging\nimport os", "import logging\nimport os\nimport sys\nimport threading\nfrom logging import CRITICAL  # NOQA\nfrom logging import DEBUG  # NOQA\nfrom logging import ERROR  # NOQA\nfrom logging import FATAL  # NOQA\nfrom logging import INFO  # NOQA\nfrom logging import NOTSET  # NOQA", "from logging import INFO  # NOQA\nfrom logging import NOTSET  # NOQA\nfrom logging import WARN  # NOQA\nfrom logging import WARNING  # NOQA\nfrom typing import Optional\n\nfrom tqdm import auto as tqdm_lib\n\n\n_lock = threading.Lock()", "\n_lock = threading.Lock()\n_default_handler: Optional[logging.Handler] = None\n\nlog_levels = {\n    \"debug\": logging.DEBUG,\n    \"info\": logging.INFO,\n    \"warning\": logging.WARNING,\n    \"error\": logging.ERROR,\n    \"critical\": logging.CRITICAL,", "    \"error\": logging.ERROR,\n    \"critical\": logging.CRITICAL,\n}\n\n_default_log_level = logging.WARNING\n\n_tqdm_active = True\n\n\ndef _get_default_logging_level():\n    \"\"\"\n    If DIFFUSERS_VERBOSITY env var is set to one of the valid choices return that as the new default level. If it is\n    not - fall back to `_default_log_level`\n    \"\"\"\n    env_level_str = os.getenv(\"DIFFUSERS_VERBOSITY\", None)\n    if env_level_str:\n        if env_level_str in log_levels:\n            return log_levels[env_level_str]\n        else:\n            logging.getLogger().warning(\n                f\"Unknown option DIFFUSERS_VERBOSITY={env_level_str}, \"\n                f\"has to be one of: { ', '.join(log_levels.keys()) }\"\n            )\n    return _default_log_level", "\ndef _get_default_logging_level():\n    \"\"\"\n    If DIFFUSERS_VERBOSITY env var is set to one of the valid choices return that as the new default level. If it is\n    not - fall back to `_default_log_level`\n    \"\"\"\n    env_level_str = os.getenv(\"DIFFUSERS_VERBOSITY\", None)\n    if env_level_str:\n        if env_level_str in log_levels:\n            return log_levels[env_level_str]\n        else:\n            logging.getLogger().warning(\n                f\"Unknown option DIFFUSERS_VERBOSITY={env_level_str}, \"\n                f\"has to be one of: { ', '.join(log_levels.keys()) }\"\n            )\n    return _default_log_level", "\n\ndef _get_library_name() -> str:\n    return __name__.split(\".\")[0]\n\n\ndef _get_library_root_logger() -> logging.Logger:\n    return logging.getLogger(_get_library_name())\n\n\ndef _configure_library_root_logger() -> None:\n    global _default_handler\n\n    with _lock:\n        if _default_handler:\n            # This library has already configured the library root logger.\n            return\n        _default_handler = logging.StreamHandler()  # Set sys.stderr as stream.\n        _default_handler.flush = sys.stderr.flush\n\n        # Apply our default configuration to the library root logger.\n        library_root_logger = _get_library_root_logger()\n        library_root_logger.addHandler(_default_handler)\n        library_root_logger.setLevel(_get_default_logging_level())\n        library_root_logger.propagate = False", "\n\ndef _configure_library_root_logger() -> None:\n    global _default_handler\n\n    with _lock:\n        if _default_handler:\n            # This library has already configured the library root logger.\n            return\n        _default_handler = logging.StreamHandler()  # Set sys.stderr as stream.\n        _default_handler.flush = sys.stderr.flush\n\n        # Apply our default configuration to the library root logger.\n        library_root_logger = _get_library_root_logger()\n        library_root_logger.addHandler(_default_handler)\n        library_root_logger.setLevel(_get_default_logging_level())\n        library_root_logger.propagate = False", "\n\ndef _reset_library_root_logger() -> None:\n    global _default_handler\n\n    with _lock:\n        if not _default_handler:\n            return\n\n        library_root_logger = _get_library_root_logger()\n        library_root_logger.removeHandler(_default_handler)\n        library_root_logger.setLevel(logging.NOTSET)\n        _default_handler = None", "\n\ndef get_log_levels_dict():\n    return log_levels\n\n\ndef get_logger(name: Optional[str] = None) -> logging.Logger:\n    \"\"\"\n    Return a logger with the specified name.\n\n    This function is not supposed to be directly accessed unless you are writing a custom diffusers module.\n    \"\"\"\n\n    if name is None:\n        name = _get_library_name()\n\n    _configure_library_root_logger()\n    return logging.getLogger(name)", "\n\ndef get_verbosity() -> int:\n    \"\"\"\n    Return the current level for the \ud83e\udd17 Diffusers' root logger as an int.\n\n    Returns:\n        `int`: The logging level.\n\n    <Tip>\n\n    \ud83e\udd17 Diffusers has following logging levels:\n\n    - 50: `diffusers.logging.CRITICAL` or `diffusers.logging.FATAL`\n    - 40: `diffusers.logging.ERROR`\n    - 30: `diffusers.logging.WARNING` or `diffusers.logging.WARN`\n    - 20: `diffusers.logging.INFO`\n    - 10: `diffusers.logging.DEBUG`\n\n    </Tip>\"\"\"\n\n    _configure_library_root_logger()\n    return _get_library_root_logger().getEffectiveLevel()", "\n\ndef set_verbosity(verbosity: int) -> None:\n    \"\"\"\n    Set the verbosity level for the \ud83e\udd17 Diffusers' root logger.\n\n    Args:\n        verbosity (`int`):\n            Logging level, e.g., one of:\n\n            - `diffusers.logging.CRITICAL` or `diffusers.logging.FATAL`\n            - `diffusers.logging.ERROR`\n            - `diffusers.logging.WARNING` or `diffusers.logging.WARN`\n            - `diffusers.logging.INFO`\n            - `diffusers.logging.DEBUG`\n    \"\"\"\n\n    _configure_library_root_logger()\n    _get_library_root_logger().setLevel(verbosity)", "\n\ndef set_verbosity_info():\n    \"\"\"Set the verbosity to the `INFO` level.\"\"\"\n    return set_verbosity(INFO)\n\n\ndef set_verbosity_warning():\n    \"\"\"Set the verbosity to the `WARNING` level.\"\"\"\n    return set_verbosity(WARNING)", "\n\ndef set_verbosity_debug():\n    \"\"\"Set the verbosity to the `DEBUG` level.\"\"\"\n    return set_verbosity(DEBUG)\n\n\ndef set_verbosity_error():\n    \"\"\"Set the verbosity to the `ERROR` level.\"\"\"\n    return set_verbosity(ERROR)", "\n\ndef disable_default_handler() -> None:\n    \"\"\"Disable the default handler of the HuggingFace Diffusers' root logger.\"\"\"\n\n    _configure_library_root_logger()\n\n    assert _default_handler is not None\n    _get_library_root_logger().removeHandler(_default_handler)\n", "\n\ndef enable_default_handler() -> None:\n    \"\"\"Enable the default handler of the HuggingFace Diffusers' root logger.\"\"\"\n\n    _configure_library_root_logger()\n\n    assert _default_handler is not None\n    _get_library_root_logger().addHandler(_default_handler)\n", "\n\ndef add_handler(handler: logging.Handler) -> None:\n    \"\"\"adds a handler to the HuggingFace Diffusers' root logger.\"\"\"\n\n    _configure_library_root_logger()\n\n    assert handler is not None\n    _get_library_root_logger().addHandler(handler)\n", "\n\ndef remove_handler(handler: logging.Handler) -> None:\n    \"\"\"removes given handler from the HuggingFace Diffusers' root logger.\"\"\"\n\n    _configure_library_root_logger()\n\n    assert handler is not None and handler not in _get_library_root_logger().handlers\n    _get_library_root_logger().removeHandler(handler)\n", "\n\ndef disable_propagation() -> None:\n    \"\"\"\n    Disable propagation of the library log outputs. Note that log propagation is disabled by default.\n    \"\"\"\n\n    _configure_library_root_logger()\n    _get_library_root_logger().propagate = False\n", "\n\ndef enable_propagation() -> None:\n    \"\"\"\n    Enable propagation of the library log outputs. Please disable the HuggingFace Diffusers' default handler to prevent\n    double logging if the root logger has been configured.\n    \"\"\"\n\n    _configure_library_root_logger()\n    _get_library_root_logger().propagate = True", "\n\ndef enable_explicit_format() -> None:\n    \"\"\"\n    Enable explicit formatting for every HuggingFace Diffusers' logger. The explicit formatter is as follows:\n    ```\n        [LEVELNAME|FILENAME|LINE NUMBER] TIME >> MESSAGE\n    ```\n    All handlers currently bound to the root logger are affected by this method.\n    \"\"\"\n    handlers = _get_library_root_logger().handlers\n\n    for handler in handlers:\n        formatter = logging.Formatter(\"[%(levelname)s|%(filename)s:%(lineno)s] %(asctime)s >> %(message)s\")\n        handler.setFormatter(formatter)", "\n\ndef reset_format() -> None:\n    \"\"\"\n    Resets the formatting for HuggingFace Diffusers' loggers.\n\n    All handlers currently bound to the root logger are affected by this method.\n    \"\"\"\n    handlers = _get_library_root_logger().handlers\n\n    for handler in handlers:\n        handler.setFormatter(None)", "\n\ndef warning_advice(self, *args, **kwargs):\n    \"\"\"\n    This method is identical to `logger.warning()`, but if env var DIFFUSERS_NO_ADVISORY_WARNINGS=1 is set, this\n    warning will not be printed\n    \"\"\"\n    no_advisory_warnings = os.getenv(\"DIFFUSERS_NO_ADVISORY_WARNINGS\", False)\n    if no_advisory_warnings:\n        return\n    self.warning(*args, **kwargs)", "\n\nlogging.Logger.warning_advice = warning_advice\n\n\nclass EmptyTqdm:\n    \"\"\"Dummy tqdm which doesn't do anything.\"\"\"\n\n    def __init__(self, *args, **kwargs):  # pylint: disable=unused-argument\n        self._iterator = args[0] if args else None\n\n    def __iter__(self):\n        return iter(self._iterator)\n\n    def __getattr__(self, _):\n        \"\"\"Return empty function.\"\"\"\n\n        def empty_fn(*args, **kwargs):  # pylint: disable=unused-argument\n            return\n\n        return empty_fn\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type_, value, traceback):\n        return", "\n\nclass _tqdm_cls:\n    def __call__(self, *args, **kwargs):\n        if _tqdm_active:\n            return tqdm_lib.tqdm(*args, **kwargs)\n        else:\n            return EmptyTqdm(*args, **kwargs)\n\n    def set_lock(self, *args, **kwargs):\n        self._lock = None\n        if _tqdm_active:\n            return tqdm_lib.tqdm.set_lock(*args, **kwargs)\n\n    def get_lock(self):\n        if _tqdm_active:\n            return tqdm_lib.tqdm.get_lock()", "\n\ntqdm = _tqdm_cls()\n\n\ndef is_progress_bar_enabled() -> bool:\n    \"\"\"Return a boolean indicating whether tqdm progress bars are enabled.\"\"\"\n    global _tqdm_active\n    return bool(_tqdm_active)\n", "\n\ndef enable_progress_bar():\n    \"\"\"Enable tqdm progress bar.\"\"\"\n    global _tqdm_active\n    _tqdm_active = True\n\n\ndef disable_progress_bar():\n    \"\"\"Disable tqdm progress bar.\"\"\"\n    global _tqdm_active\n    _tqdm_active = False", "def disable_progress_bar():\n    \"\"\"Disable tqdm progress bar.\"\"\"\n    global _tqdm_active\n    _tqdm_active = False\n"]}
{"filename": "diffusers_/stable_diffusion/pipeline_stable_diffusion.py", "chunked_list": ["import inspect\nfrom typing import Callable, List, Optional, Union\n\nimport torch\n\nfrom diffusers.utils import is_accelerate_available\nfrom packaging import version\n\nfrom ..configuration_utils import FrozenDict\nfrom ..pipeline_utils import DiffusionPipeline", "from ..configuration_utils import FrozenDict\nfrom ..pipeline_utils import DiffusionPipeline\nfrom ..utils import deprecate, logging\nfrom . import StableDiffusionPipelineOutput\nfrom .safety_checker import StableDiffusionSafetyChecker\n\n\nlogger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\nclass StableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latens. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n    \"\"\"\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n\n    def __init__(\n        self,\n        vae,\n        text_encoder,\n        tokenizer,\n        unet,\n        scheduler,\n        safety_checker,\n        feature_extractor,\n        requires_safety_checker: bool = False,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n            )\n            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"clip_sample\"] = False\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and version.parse(\n            version.parse(unet.config._diffusers_version).base_version\n        ) < version.parse(\"0.9.0.dev0\")\n        is_unet_sample_size_less_64 = hasattr(unet.config, \"sample_size\") and unet.config.sample_size < 64\n        if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:\n            deprecation_message = (\n                \"The configuration file of the unet has set the default `sample_size` to smaller than\"\n                \" 64 which seems highly unlikely .If you're checkpoint is a fine-tuned version of any of the\"\n                \" following: \\n- CompVis/stable-diffusion-v1-4 \\n- CompVis/stable-diffusion-v1-3 \\n-\"\n                \" CompVis/stable-diffusion-v1-2 \\n- CompVis/stable-diffusion-v1-1 \\n- runwayml/stable-diffusion-v1-5\"\n                \" \\n- runwayml/stable-diffusion-inpainting \\n you should change 'sample_size' to 64 in the\"\n                \" configuration file. Please make sure to update the config accordingly as leaving `sample_size=32`\"\n                \" in the config might lead to incorrect results in future versions. If you have downloaded this\"\n                \" checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for\"\n                \" the `unet/config.json` file\"\n            )\n            deprecate(\"sample_size<64\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(unet.config)\n            new_config[\"sample_size\"] = 64\n            unet._internal_dict = FrozenDict(new_config)\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=None, #safety_checker,\n            feature_extractor=None, #feature_extractor,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.register_to_config(requires_safety_checker=requires_safety_checker)\n\n    def enable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Enable memory efficient attention as implemented in xformers.\n\n        When this option is enabled, you should observe lower GPU memory usage and a potential speed up at inference\n        time. Speed up at training time is not guaranteed.\n\n        Warning: When Memory Efficient Attention and Sliced attention are both enabled, the Memory Efficient Attention\n        is used.\n        \"\"\"\n        self.unet.set_use_memory_efficient_attention_xformers(True)\n\n    def disable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Disable memory efficient attention as implemented in xformers.\n        \"\"\"\n        self.unet.set_use_memory_efficient_attention_xformers(False)\n\n    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            if isinstance(self.unet.config.attention_head_dim, int):\n                # half the attention head size is usually a good trade-off between\n                # speed and memory\n                slice_size = self.unet.config.attention_head_dim // 2\n            else:\n                # if `attention_head_dim` is a list, take the smallest head size\n                slice_size = min(self.unet.config.attention_head_dim)\n\n        self.unet.set_attention_slice(slice_size)\n\n    def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)\n\n    def enable_sequential_cpu_offload(self):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(\"cuda\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate\n            # fix by only offloading self.safety_checker for now\n            cpu_offload(self.safety_checker.vision_model, device)\n\n    @property\n    def _execution_device(self):\n        r\"\"\"\n        Returns the device on which the pipeline's models will be executed. After calling\n        `pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module\n        hooks.\n        \"\"\"\n        if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):\n            return self.device\n        for module in self.unet.modules():\n            if (\n                hasattr(module, \"_hf_hook\")\n                and hasattr(module._hf_hook, \"execution_device\")\n                and module._hf_hook.execution_device is not None\n            ):\n                return torch.device(module._hf_hook.execution_device)\n        return self.device\n\n    def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `list(int)`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n\n        if not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n            )\n\n        if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n            attention_mask = text_inputs.attention_mask.to(device)\n        else:\n            attention_mask = None\n\n        text_embeddings = self.text_encoder(\n            text_input_ids.to(device),\n            attention_mask=attention_mask,\n        )\n        text_embeddings = text_embeddings[0]\n\n        # duplicate text embeddings for each generation per prompt, using mps friendly method\n        bs_embed, seq_len, _ = text_embeddings.shape\n        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance:\n            uncond_tokens: List[str]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif type(prompt) is not type(negative_prompt):\n                raise TypeError(\n                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n                    f\" {type(prompt)}.\"\n                )\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            elif batch_size != len(negative_prompt):\n                raise ValueError(\n                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                    \" the batch size of `prompt`.\"\n                )\n            else:\n                uncond_tokens = negative_prompt\n\n            max_length = text_input_ids.shape[-1]\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = uncond_input.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            uncond_embeddings = self.text_encoder(\n                uncond_input.input_ids.to(device),\n                attention_mask=attention_mask,\n            )\n            uncond_embeddings = uncond_embeddings[0]\n\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            seq_len = uncond_embeddings.shape[1]\n            uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt, 1)\n            uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n        return text_embeddings\n\n    def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept\n\n    def decode_latents(self, latents):\n        latents = 1 / 0.18215 * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image\n\n    def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (\u03b7) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to \u03b7 in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs\n\n    def check_inputs(self, prompt, height, width, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n    def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if latents is None:\n            if device.type == \"mps\":\n                # randn does not work reproducibly on mps\n                latents = torch.randn(shape, generator=generator, device=\"cpu\", dtype=dtype).to(device)\n            else:\n                latents = torch.randn(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = \"\",\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        text_embeddings: Optional[torch.FloatTensor] = None,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(prompt, height, width, callback_steps)\n\n        # 2. Define call parameters\n        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # 3. Encode input prompt\n\n        # if text_embeddings is None:\n        #     text_embeddings = self._encode_prompt(\n        #         prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n        #     )\n        \n        if num_images_per_prompt!=1:\n            seq_len = text_embeddings.shape[1]\n            text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n            text_embeddings = text_embeddings.view(num_images_per_prompt, seq_len, -1)\n        \n        \n        if do_classifier_free_guidance:\n            uncond_tokens = [\"\"]\n            max_length = self.tokenizer.model_max_length\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            \n            seq_len = uncond_embeddings.shape[1]\n            if num_images_per_prompt!=1:\n                uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt, 1)\n            uncond_embeddings = uncond_embeddings.view(num_images_per_prompt, seq_len, -1)\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n        # 4. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n        # 5. Prepare latent variables\n        num_channels_latents = self.unet.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            text_embeddings.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 7. Denoising loop\n        for i, t in enumerate(self.progress_bar(timesteps)):\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            # predict the noise residual\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n            # call the callback, if provided\n            if callback is not None and i % callback_steps == 0:\n                callback(i, t, latents)\n\n        # 8. Post-processing\n        image = self.decode_latents(latents)\n\n        # 9. Run safety checker\n        image, has_nsfw_concept = self.run_safety_checker(image, device, text_embeddings.dtype)\n\n        # 10. Convert to PIL\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)", "\n\nclass StableDiffusionPipeline(DiffusionPipeline):\n    r\"\"\"\n    Pipeline for text-to-image generation using Stable Diffusion.\n\n    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n\n    Args:\n        vae ([`AutoencoderKL`]):\n            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n        text_encoder ([`CLIPTextModel`]):\n            Frozen text-encoder. Stable Diffusion uses the text portion of\n            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel), specifically\n            the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant.\n        tokenizer (`CLIPTokenizer`):\n            Tokenizer of class\n            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n        unet ([`UNet2DConditionModel`]): Conditional U-Net architecture to denoise the encoded image latents.\n        scheduler ([`SchedulerMixin`]):\n            A scheduler to be used in combination with `unet` to denoise the encoded image latens. Can be one of\n            [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n        safety_checker ([`StableDiffusionSafetyChecker`]):\n            Classification module that estimates whether generated images could be considered offensive or harmful.\n            Please, refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for details.\n        feature_extractor ([`CLIPFeatureExtractor`]):\n            Model that extracts features from generated images to be used as inputs for the `safety_checker`.\n    \"\"\"\n    _optional_components = [\"safety_checker\", \"feature_extractor\"]\n\n    def __init__(\n        self,\n        vae,\n        text_encoder,\n        tokenizer,\n        unet,\n        scheduler,\n        safety_checker,\n        feature_extractor,\n        requires_safety_checker: bool = False,\n    ):\n        super().__init__()\n\n        if hasattr(scheduler.config, \"steps_offset\") and scheduler.config.steps_offset != 1:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} is outdated. `steps_offset`\"\n                f\" should be set to 1 instead of {scheduler.config.steps_offset}. Please make sure \"\n                \"to update the config accordingly as leaving `steps_offset` might led to incorrect results\"\n                \" in future versions. If you have downloaded this checkpoint from the Hugging Face Hub,\"\n                \" it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json`\"\n                \" file\"\n            )\n            deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"steps_offset\"] = 1\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if hasattr(scheduler.config, \"clip_sample\") and scheduler.config.clip_sample is True:\n            deprecation_message = (\n                f\"The configuration file of this scheduler: {scheduler} has not set the configuration `clip_sample`.\"\n                \" `clip_sample` should be set to False in the configuration file. Please make sure to update the\"\n                \" config accordingly as not setting `clip_sample` in the config might lead to incorrect results in\"\n                \" future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very\"\n                \" nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\"\n            )\n            deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(scheduler.config)\n            new_config[\"clip_sample\"] = False\n            scheduler._internal_dict = FrozenDict(new_config)\n\n        if safety_checker is None and requires_safety_checker:\n            logger.warning(\n                f\"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure\"\n                \" that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered\"\n                \" results in services or applications open to the public. Both the diffusers team and Hugging Face\"\n                \" strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling\"\n                \" it only for use-cases that involve analyzing network behavior or auditing its results. For more\"\n                \" information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\"\n            )\n\n        if safety_checker is not None and feature_extractor is None:\n            raise ValueError(\n                \"Make sure to define a feature extractor when loading {self.__class__} if you want to use the safety\"\n                \" checker. If you do not want to use the safety checker, you can pass `'safety_checker=None'` instead.\"\n            )\n\n        is_unet_version_less_0_9_0 = hasattr(unet.config, \"_diffusers_version\") and version.parse(\n            version.parse(unet.config._diffusers_version).base_version\n        ) < version.parse(\"0.9.0.dev0\")\n        is_unet_sample_size_less_64 = hasattr(unet.config, \"sample_size\") and unet.config.sample_size < 64\n        if is_unet_version_less_0_9_0 and is_unet_sample_size_less_64:\n            deprecation_message = (\n                \"The configuration file of the unet has set the default `sample_size` to smaller than\"\n                \" 64 which seems highly unlikely .If you're checkpoint is a fine-tuned version of any of the\"\n                \" following: \\n- CompVis/stable-diffusion-v1-4 \\n- CompVis/stable-diffusion-v1-3 \\n-\"\n                \" CompVis/stable-diffusion-v1-2 \\n- CompVis/stable-diffusion-v1-1 \\n- runwayml/stable-diffusion-v1-5\"\n                \" \\n- runwayml/stable-diffusion-inpainting \\n you should change 'sample_size' to 64 in the\"\n                \" configuration file. Please make sure to update the config accordingly as leaving `sample_size=32`\"\n                \" in the config might lead to incorrect results in future versions. If you have downloaded this\"\n                \" checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for\"\n                \" the `unet/config.json` file\"\n            )\n            deprecate(\"sample_size<64\", \"1.0.0\", deprecation_message, standard_warn=False)\n            new_config = dict(unet.config)\n            new_config[\"sample_size\"] = 64\n            unet._internal_dict = FrozenDict(new_config)\n\n        self.register_modules(\n            vae=vae,\n            text_encoder=text_encoder,\n            tokenizer=tokenizer,\n            unet=unet,\n            scheduler=scheduler,\n            safety_checker=None, #safety_checker,\n            feature_extractor=None, #feature_extractor,\n        )\n        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n        self.register_to_config(requires_safety_checker=requires_safety_checker)\n\n    def enable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Enable memory efficient attention as implemented in xformers.\n\n        When this option is enabled, you should observe lower GPU memory usage and a potential speed up at inference\n        time. Speed up at training time is not guaranteed.\n\n        Warning: When Memory Efficient Attention and Sliced attention are both enabled, the Memory Efficient Attention\n        is used.\n        \"\"\"\n        self.unet.set_use_memory_efficient_attention_xformers(True)\n\n    def disable_xformers_memory_efficient_attention(self):\n        r\"\"\"\n        Disable memory efficient attention as implemented in xformers.\n        \"\"\"\n        self.unet.set_use_memory_efficient_attention_xformers(False)\n\n    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n        r\"\"\"\n        Enable sliced attention computation.\n\n        When this option is enabled, the attention module will split the input tensor in slices, to compute attention\n        in several steps. This is useful to save some memory in exchange for a small speed decrease.\n\n        Args:\n            slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n                When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n                a number is provided, uses as many slices as `attention_head_dim // slice_size`. In this case,\n                `attention_head_dim` must be a multiple of `slice_size`.\n        \"\"\"\n        if slice_size == \"auto\":\n            if isinstance(self.unet.config.attention_head_dim, int):\n                # half the attention head size is usually a good trade-off between\n                # speed and memory\n                slice_size = self.unet.config.attention_head_dim // 2\n            else:\n                # if `attention_head_dim` is a list, take the smallest head size\n                slice_size = min(self.unet.config.attention_head_dim)\n\n        self.unet.set_attention_slice(slice_size)\n\n    def disable_attention_slicing(self):\n        r\"\"\"\n        Disable sliced attention computation. If `enable_attention_slicing` was previously invoked, this method will go\n        back to computing attention in one step.\n        \"\"\"\n        # set slice_size = `None` to disable `attention slicing`\n        self.enable_attention_slicing(None)\n\n    def enable_sequential_cpu_offload(self):\n        r\"\"\"\n        Offloads all models to CPU using accelerate, significantly reducing memory usage. When called, unet,\n        text_encoder, vae and safety checker have their state dicts saved to CPU and then are moved to a\n        `torch.device('meta') and loaded to GPU only when their specific submodule has its `forward` method called.\n        \"\"\"\n        if is_accelerate_available():\n            from accelerate import cpu_offload\n        else:\n            raise ImportError(\"Please install accelerate via `pip install accelerate`\")\n\n        device = torch.device(\"cuda\")\n\n        for cpu_offloaded_model in [self.unet, self.text_encoder, self.vae]:\n            if cpu_offloaded_model is not None:\n                cpu_offload(cpu_offloaded_model, device)\n\n        if self.safety_checker is not None:\n            # TODO(Patrick) - there is currently a bug with cpu offload of nn.Parameter in accelerate\n            # fix by only offloading self.safety_checker for now\n            cpu_offload(self.safety_checker.vision_model, device)\n\n    @property\n    def _execution_device(self):\n        r\"\"\"\n        Returns the device on which the pipeline's models will be executed. After calling\n        `pipeline.enable_sequential_cpu_offload()` the execution device can only be inferred from Accelerate's module\n        hooks.\n        \"\"\"\n        if self.device != torch.device(\"meta\") or not hasattr(self.unet, \"_hf_hook\"):\n            return self.device\n        for module in self.unet.modules():\n            if (\n                hasattr(module, \"_hf_hook\")\n                and hasattr(module._hf_hook, \"execution_device\")\n                and module._hf_hook.execution_device is not None\n            ):\n                return torch.device(module._hf_hook.execution_device)\n        return self.device\n\n    def _encode_prompt(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt):\n        r\"\"\"\n        Encodes the prompt into text encoder hidden states.\n\n        Args:\n            prompt (`str` or `list(int)`):\n                prompt to be encoded\n            device: (`torch.device`):\n                torch device\n            num_images_per_prompt (`int`):\n                number of images that should be generated per prompt\n            do_classifier_free_guidance (`bool`):\n                whether to use classifier free guidance or not\n            negative_prompt (`str` or `List[str]`):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n        \"\"\"\n        batch_size = len(prompt) if isinstance(prompt, list) else 1\n\n        text_inputs = self.tokenizer(\n            prompt,\n            padding=\"max_length\",\n            max_length=self.tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        text_input_ids = text_inputs.input_ids\n        untruncated_ids = self.tokenizer(prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n\n        if not torch.equal(text_input_ids, untruncated_ids):\n            removed_text = self.tokenizer.batch_decode(untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1])\n            logger.warning(\n                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n                f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n            )\n\n        if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n            attention_mask = text_inputs.attention_mask.to(device)\n        else:\n            attention_mask = None\n\n        text_embeddings = self.text_encoder(\n            text_input_ids.to(device),\n            attention_mask=attention_mask,\n        )\n        text_embeddings = text_embeddings[0]\n\n        # duplicate text embeddings for each generation per prompt, using mps friendly method\n        bs_embed, seq_len, _ = text_embeddings.shape\n        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n\n        # get unconditional embeddings for classifier free guidance\n        if do_classifier_free_guidance:\n            uncond_tokens: List[str]\n            if negative_prompt is None:\n                uncond_tokens = [\"\"] * batch_size\n            elif type(prompt) is not type(negative_prompt):\n                raise TypeError(\n                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n                    f\" {type(prompt)}.\"\n                )\n            elif isinstance(negative_prompt, str):\n                uncond_tokens = [negative_prompt]\n            elif batch_size != len(negative_prompt):\n                raise ValueError(\n                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n                    \" the batch size of `prompt`.\"\n                )\n            else:\n                uncond_tokens = negative_prompt\n\n            max_length = text_input_ids.shape[-1]\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n\n            if hasattr(self.text_encoder.config, \"use_attention_mask\") and self.text_encoder.config.use_attention_mask:\n                attention_mask = uncond_input.attention_mask.to(device)\n            else:\n                attention_mask = None\n\n            uncond_embeddings = self.text_encoder(\n                uncond_input.input_ids.to(device),\n                attention_mask=attention_mask,\n            )\n            uncond_embeddings = uncond_embeddings[0]\n\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            seq_len = uncond_embeddings.shape[1]\n            uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt, 1)\n            uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n        return text_embeddings\n\n    def run_safety_checker(self, image, device, dtype):\n        if self.safety_checker is not None:\n            safety_checker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(device)\n            image, has_nsfw_concept = self.safety_checker(\n                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n            )\n        else:\n            has_nsfw_concept = None\n        return image, has_nsfw_concept\n\n    def decode_latents(self, latents):\n        latents = 1 / 0.18215 * latents\n        image = self.vae.decode(latents).sample\n        image = (image / 2 + 0.5).clamp(0, 1)\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n        return image\n\n    def prepare_extra_step_kwargs(self, generator, eta):\n        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n        # eta (\u03b7) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n        # eta corresponds to \u03b7 in DDIM paper: https://arxiv.org/abs/2010.02502\n        # and should be between [0, 1]\n\n        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        extra_step_kwargs = {}\n        if accepts_eta:\n            extra_step_kwargs[\"eta\"] = eta\n\n        # check if the scheduler accepts generator\n        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n        if accepts_generator:\n            extra_step_kwargs[\"generator\"] = generator\n        return extra_step_kwargs\n\n    def check_inputs(self, prompt, height, width, callback_steps):\n        if not isinstance(prompt, str) and not isinstance(prompt, list):\n            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n\n        if height % 8 != 0 or width % 8 != 0:\n            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n\n        if (callback_steps is None) or (\n            callback_steps is not None and (not isinstance(callback_steps, int) or callback_steps <= 0)\n        ):\n            raise ValueError(\n                f\"`callback_steps` has to be a positive integer but is {callback_steps} of type\"\n                f\" {type(callback_steps)}.\"\n            )\n\n    def prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):\n        shape = (batch_size, num_channels_latents, height // self.vae_scale_factor, width // self.vae_scale_factor)\n        if latents is None:\n            if device.type == \"mps\":\n                # randn does not work reproducibly on mps\n                latents = torch.randn(shape, generator=generator, device=\"cpu\", dtype=dtype).to(device)\n            else:\n                latents = torch.randn(shape, generator=generator, device=device, dtype=dtype)\n        else:\n            if latents.shape != shape:\n                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {shape}\")\n            latents = latents.to(device)\n\n        # scale the initial noise by the standard deviation required by the scheduler\n        latents = latents * self.scheduler.init_noise_sigma\n        return latents\n\n    @torch.no_grad()\n    def __call__(\n        self,\n        prompt: Union[str, List[str]] = \"\",\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n        num_inference_steps: int = 50,\n        guidance_scale: float = 7.5,\n        text_embeddings: Optional[torch.FloatTensor] = None,\n        negative_prompt: Optional[Union[str, List[str]]] = None,\n        num_images_per_prompt: Optional[int] = 1,\n        eta: float = 0.0,\n        generator: Optional[torch.Generator] = None,\n        latents: Optional[torch.FloatTensor] = None,\n        output_type: Optional[str] = \"pil\",\n        return_dict: bool = True,\n        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n        callback_steps: Optional[int] = 1,\n        **kwargs,\n    ):\n        r\"\"\"\n        Function invoked when calling the pipeline for generation.\n\n        Args:\n            prompt (`str` or `List[str]`):\n                The prompt or prompts to guide the image generation.\n            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The height in pixels of the generated image.\n            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                The width in pixels of the generated image.\n            num_inference_steps (`int`, *optional*, defaults to 50):\n                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n                expense of slower inference.\n            guidance_scale (`float`, *optional*, defaults to 7.5):\n                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n                usually at the expense of lower image quality.\n            negative_prompt (`str` or `List[str]`, *optional*):\n                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n                if `guidance_scale` is less than `1`).\n            num_images_per_prompt (`int`, *optional*, defaults to 1):\n                The number of images to generate per prompt.\n            eta (`float`, *optional*, defaults to 0.0):\n                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n                [`schedulers.DDIMScheduler`], will be ignored for others.\n            generator (`torch.Generator`, *optional*):\n                A [torch generator](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make generation\n                deterministic.\n            latents (`torch.FloatTensor`, *optional*):\n                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n                tensor will ge generated by sampling using the supplied random `generator`.\n            output_type (`str`, *optional*, defaults to `\"pil\"`):\n                The output format of the generate image. Choose between\n                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n            return_dict (`bool`, *optional*, defaults to `True`):\n                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n                plain tuple.\n            callback (`Callable`, *optional*):\n                A function that will be called every `callback_steps` steps during inference. The function will be\n                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n            callback_steps (`int`, *optional*, defaults to 1):\n                The frequency at which the `callback` function will be called. If not specified, the callback will be\n                called at every step.\n\n        Returns:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n            When returning a tuple, the first element is a list with the generated images, and the second element is a\n            list of `bool`s denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, according to the `safety_checker`.\n        \"\"\"\n        # 0. Default height and width to unet\n        height = height or self.unet.config.sample_size * self.vae_scale_factor\n        width = width or self.unet.config.sample_size * self.vae_scale_factor\n\n        # 1. Check inputs. Raise error if not correct\n        self.check_inputs(prompt, height, width, callback_steps)\n\n        # 2. Define call parameters\n        batch_size = 1 if isinstance(prompt, str) else len(prompt)\n        device = self._execution_device\n        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n        # corresponds to doing no classifier free guidance.\n        do_classifier_free_guidance = guidance_scale > 1.0\n\n        # 3. Encode input prompt\n\n        # if text_embeddings is None:\n        #     text_embeddings = self._encode_prompt(\n        #         prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt\n        #     )\n        \n        if num_images_per_prompt!=1:\n            seq_len = text_embeddings.shape[1]\n            text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n            text_embeddings = text_embeddings.view(num_images_per_prompt, seq_len, -1)\n        \n        \n        if do_classifier_free_guidance:\n            uncond_tokens = [\"\"]\n            max_length = self.tokenizer.model_max_length\n            uncond_input = self.tokenizer(\n                uncond_tokens,\n                padding=\"max_length\",\n                max_length=max_length,\n                truncation=True,\n                return_tensors=\"pt\",\n            )\n            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n\n            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n            \n            seq_len = uncond_embeddings.shape[1]\n            if num_images_per_prompt!=1:\n                uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt, 1)\n            uncond_embeddings = uncond_embeddings.view(num_images_per_prompt, seq_len, -1)\n\n            # For classifier free guidance, we need to do two forward passes.\n            # Here we concatenate the unconditional and text embeddings into a single batch\n            # to avoid doing two forward passes\n            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n\n        # 4. Prepare timesteps\n        self.scheduler.set_timesteps(num_inference_steps, device=device)\n        timesteps = self.scheduler.timesteps\n\n        # 5. Prepare latent variables\n        num_channels_latents = self.unet.in_channels\n        latents = self.prepare_latents(\n            batch_size * num_images_per_prompt,\n            num_channels_latents,\n            height,\n            width,\n            text_embeddings.dtype,\n            device,\n            generator,\n            latents,\n        )\n\n        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n\n        # 7. Denoising loop\n        for i, t in enumerate(self.progress_bar(timesteps)):\n            # expand the latents if we are doing classifier free guidance\n            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n            latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n            # predict the noise residual\n            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n            # perform guidance\n            if do_classifier_free_guidance:\n                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n            # compute the previous noisy sample x_t -> x_t-1\n            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n\n            # call the callback, if provided\n            if callback is not None and i % callback_steps == 0:\n                callback(i, t, latents)\n\n        # 8. Post-processing\n        image = self.decode_latents(latents)\n\n        # 9. Run safety checker\n        image, has_nsfw_concept = self.run_safety_checker(image, device, text_embeddings.dtype)\n\n        # 10. Convert to PIL\n        if output_type == \"pil\":\n            image = self.numpy_to_pil(image)\n\n        if not return_dict:\n            return (image, has_nsfw_concept)\n\n        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)"]}
{"filename": "diffusers_/stable_diffusion/__init__.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import List, Optional, Union\n\nimport numpy as np\n\nimport PIL\nfrom PIL import Image\n\nfrom ..utils import (\n    BaseOutput,", "from ..utils import (\n    BaseOutput,\n    is_torch_available,\n    is_transformers_available,\n)\n\n\n@dataclass\nclass StableDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n        nsfw_content_detected (`List[bool]`)\n            List of flags denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, or `None` if safety checking could not be performed.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n    nsfw_content_detected: Optional[List[bool]]", "class StableDiffusionPipelineOutput(BaseOutput):\n    \"\"\"\n    Output class for Stable Diffusion pipelines.\n\n    Args:\n        images (`List[PIL.Image.Image]` or `np.ndarray`)\n            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n        nsfw_content_detected (`List[bool]`)\n            List of flags denoting whether the corresponding generated image likely represents \"not-safe-for-work\"\n            (nsfw) content, or `None` if safety checking could not be performed.\n    \"\"\"\n\n    images: Union[List[PIL.Image.Image], np.ndarray]\n    nsfw_content_detected: Optional[List[bool]]", "\n\nif is_transformers_available() and is_torch_available():\n    from .pipeline_stable_diffusion import StableDiffusionPipeline\n"]}
{"filename": "diffusers_/stable_diffusion/safety_checker.py", "chunked_list": ["# Copyright 2022 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n", "import torch.nn as nn\n\nfrom transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel\n\nfrom ..utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\n\ndef cosine_distance(image_embeds, text_embeds):\n    normalized_image_embeds = nn.functional.normalize(image_embeds)\n    normalized_text_embeds = nn.functional.normalize(text_embeds)\n    return torch.mm(normalized_image_embeds, normalized_text_embeds.t())", "\n\ndef cosine_distance(image_embeds, text_embeds):\n    normalized_image_embeds = nn.functional.normalize(image_embeds)\n    normalized_text_embeds = nn.functional.normalize(text_embeds)\n    return torch.mm(normalized_image_embeds, normalized_text_embeds.t())\n\n\nclass StableDiffusionSafetyChecker(PreTrainedModel):\n    config_class = CLIPConfig\n\n    _no_split_modules = [\"CLIPEncoderLayer\"]\n\n    def __init__(self, config: CLIPConfig):\n        super().__init__(config)\n\n        self.vision_model = CLIPVisionModel(config.vision_config)\n        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n\n        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n\n        self.concept_embeds_weights = nn.Parameter(torch.ones(17), requires_grad=False)\n        self.special_care_embeds_weights = nn.Parameter(torch.ones(3), requires_grad=False)\n\n    @torch.no_grad()\n    def forward(self, clip_input, images):\n        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n        image_embeds = self.visual_projection(pooled_output)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().float().numpy()\n        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().float().numpy()\n\n        result = []\n        batch_size = image_embeds.shape[0]\n        for i in range(batch_size):\n            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n\n            # increase this value to create a stronger `nfsw` filter\n            # at the cost of increasing the possibility of filtering benign images\n            adjustment = 0.0\n\n            for concept_idx in range(len(special_cos_dist[0])):\n                concept_cos = special_cos_dist[i][concept_idx]\n                concept_threshold = self.special_care_embeds_weights[concept_idx].item()\n                result_img[\"special_scores\"][concept_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n                if result_img[\"special_scores\"][concept_idx] > 0:\n                    result_img[\"special_care\"].append({concept_idx, result_img[\"special_scores\"][concept_idx]})\n                    adjustment = 0.01\n\n            for concept_idx in range(len(cos_dist[0])):\n                concept_cos = cos_dist[i][concept_idx]\n                concept_threshold = self.concept_embeds_weights[concept_idx].item()\n                result_img[\"concept_scores\"][concept_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n                if result_img[\"concept_scores\"][concept_idx] > 0:\n                    result_img[\"bad_concepts\"].append(concept_idx)\n\n            result.append(result_img)\n\n        has_nsfw_concepts = [len(res[\"bad_concepts\"]) > 0 for res in result]\n\n        for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n            if has_nsfw_concept:\n                images[idx] = np.zeros(images[idx].shape)  # black image\n\n        if any(has_nsfw_concepts):\n            logger.warning(\n                \"Potential NSFW content was detected in one or more images. A black image will be returned instead.\"\n                \" Try again with a different prompt and/or seed.\"\n            )\n\n        return images, has_nsfw_concepts\n\n    @torch.no_grad()\n    def forward_onnx(self, clip_input: torch.FloatTensor, images: torch.FloatTensor):\n        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n        image_embeds = self.visual_projection(pooled_output)\n\n        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds)\n        cos_dist = cosine_distance(image_embeds, self.concept_embeds)\n\n        # increase this value to create a stronger `nsfw` filter\n        # at the cost of increasing the possibility of filtering benign images\n        adjustment = 0.0\n\n        special_scores = special_cos_dist - self.special_care_embeds_weights + adjustment\n        # special_scores = special_scores.round(decimals=3)\n        special_care = torch.any(special_scores > 0, dim=1)\n        special_adjustment = special_care * 0.01\n        special_adjustment = special_adjustment.unsqueeze(1).expand(-1, cos_dist.shape[1])\n\n        concept_scores = (cos_dist - self.concept_embeds_weights) + special_adjustment\n        # concept_scores = concept_scores.round(decimals=3)\n        has_nsfw_concepts = torch.any(concept_scores > 0, dim=1)\n\n        images[has_nsfw_concepts] = 0.0  # black image\n\n        return images, has_nsfw_concepts", "class StableDiffusionSafetyChecker(PreTrainedModel):\n    config_class = CLIPConfig\n\n    _no_split_modules = [\"CLIPEncoderLayer\"]\n\n    def __init__(self, config: CLIPConfig):\n        super().__init__(config)\n\n        self.vision_model = CLIPVisionModel(config.vision_config)\n        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n\n        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n\n        self.concept_embeds_weights = nn.Parameter(torch.ones(17), requires_grad=False)\n        self.special_care_embeds_weights = nn.Parameter(torch.ones(3), requires_grad=False)\n\n    @torch.no_grad()\n    def forward(self, clip_input, images):\n        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n        image_embeds = self.visual_projection(pooled_output)\n\n        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloa16\n        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().float().numpy()\n        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().float().numpy()\n\n        result = []\n        batch_size = image_embeds.shape[0]\n        for i in range(batch_size):\n            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n\n            # increase this value to create a stronger `nfsw` filter\n            # at the cost of increasing the possibility of filtering benign images\n            adjustment = 0.0\n\n            for concept_idx in range(len(special_cos_dist[0])):\n                concept_cos = special_cos_dist[i][concept_idx]\n                concept_threshold = self.special_care_embeds_weights[concept_idx].item()\n                result_img[\"special_scores\"][concept_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n                if result_img[\"special_scores\"][concept_idx] > 0:\n                    result_img[\"special_care\"].append({concept_idx, result_img[\"special_scores\"][concept_idx]})\n                    adjustment = 0.01\n\n            for concept_idx in range(len(cos_dist[0])):\n                concept_cos = cos_dist[i][concept_idx]\n                concept_threshold = self.concept_embeds_weights[concept_idx].item()\n                result_img[\"concept_scores\"][concept_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n                if result_img[\"concept_scores\"][concept_idx] > 0:\n                    result_img[\"bad_concepts\"].append(concept_idx)\n\n            result.append(result_img)\n\n        has_nsfw_concepts = [len(res[\"bad_concepts\"]) > 0 for res in result]\n\n        for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n            if has_nsfw_concept:\n                images[idx] = np.zeros(images[idx].shape)  # black image\n\n        if any(has_nsfw_concepts):\n            logger.warning(\n                \"Potential NSFW content was detected in one or more images. A black image will be returned instead.\"\n                \" Try again with a different prompt and/or seed.\"\n            )\n\n        return images, has_nsfw_concepts\n\n    @torch.no_grad()\n    def forward_onnx(self, clip_input: torch.FloatTensor, images: torch.FloatTensor):\n        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n        image_embeds = self.visual_projection(pooled_output)\n\n        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds)\n        cos_dist = cosine_distance(image_embeds, self.concept_embeds)\n\n        # increase this value to create a stronger `nsfw` filter\n        # at the cost of increasing the possibility of filtering benign images\n        adjustment = 0.0\n\n        special_scores = special_cos_dist - self.special_care_embeds_weights + adjustment\n        # special_scores = special_scores.round(decimals=3)\n        special_care = torch.any(special_scores > 0, dim=1)\n        special_adjustment = special_care * 0.01\n        special_adjustment = special_adjustment.unsqueeze(1).expand(-1, cos_dist.shape[1])\n\n        concept_scores = (cos_dist - self.concept_embeds_weights) + special_adjustment\n        # concept_scores = concept_scores.round(decimals=3)\n        has_nsfw_concepts = torch.any(concept_scores > 0, dim=1)\n\n        images[has_nsfw_concepts] = 0.0  # black image\n\n        return images, has_nsfw_concepts", ""]}
