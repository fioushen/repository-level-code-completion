{"filename": "setup.py", "chunked_list": ["import re\nfrom pathlib import Path\n\nimport setuptools\nfrom setuptools import find_packages\n\nFILE = Path(__file__).resolve()\nPARENT = FILE.parent  # root directory\nREADME = (PARENT / \"README.md\").read_text(encoding=\"utf-8\")\n", "README = (PARENT / \"README.md\").read_text(encoding=\"utf-8\")\n\n\ndef get_version():\n    file = PARENT / \"evaluations/__init__.py\"\n    return re.search(\n        r'^__version__ = [\\'\"]([^\\'\"]*)[\\'\"]', file.read_text(encoding=\"utf-8\"), re.M\n    )[1]\n\n", "\n\nsetuptools.setup(\n    name=\"evaluations\",\n    version=get_version(),\n    author=\"Roboflow, Inc\",\n    author_email=\"support@roboflow.com\",\n    license=\"MIT\",\n    description=\"Evaluate ground truth and model predictions from Roboflow and supported zero-shot models\",\n    long_description=README,", "    description=\"Evaluate ground truth and model predictions from Roboflow and supported zero-shot models\",\n    long_description=README,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/roboflow/cvevals\",\n    install_requires=[\"numpy>=1.20.0\", \"opencv-python\", \"matplotlib\"],\n    packages=find_packages(exclude=(\"tests\",)),\n    extras_require={\n        \"dev\": [\n            \"flake8\",\n            \"black==22.3.0\",", "            \"flake8\",\n            \"black==22.3.0\",\n            \"isort\",\n            \"twine\",\n            \"pytest\",\n            \"wheel\",\n            \"mkdocs-material\",\n            \"mkdocstrings[python]\",\n        ],\n    },", "        ],\n    },\n    classifiers=[\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",", "        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3 :: Only\",\n        \"Topic :: Software Development\",\n        \"Topic :: Scientific/Engineering\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering :: Image Recognition\",\n    ],\n    keywords=\"Roboflow, computer vision, CV, computer vision evaluation\",", "    ],\n    keywords=\"Roboflow, computer vision, CV, computer vision evaluation\",\n    python_requires=\">=3.7\",\n)\n"]}
{"filename": "scripts/tighten_bounding_boxes.py", "chunked_list": ["import argparse\nimport os\nimport json\n\nfrom evaluations.dataloaders import RoboflowDataLoader\n\n# translate above to argparse\nparser = argparse.ArgumentParser()\n\nparser.add_argument(", "\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)", "    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\nparser.add_argument(\n    \"--increase_bounding_box_size\",", "parser.add_argument(\n    \"--increase_bounding_box_size\",\n    type=float,\n    required=False,\n    default=0.1,\n    help=\"Increase bounding box size by this percentage\",\n)\n\nparser.add_argument(\n    \"--sam_checkpoint\",", "parser.add_argument(\n    \"--sam_checkpoint\",\n    type=str,\n    required=True,\n    help=\"Path to SAM checkpoint\",\n)\n\nargs = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path", "\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\nif args.increase_bounding_box_size > 1.0:\n    raise ValueError(\"Increase bounding box size must be less than 1.0\")\n\nclass_names, data, model = RoboflowDataLoader(", "\nclass_names, data, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"object-detection\",\n).download_dataset()\n\nimages = data.keys()", "\nimages = data.keys()\n\nimport cv2\nimport supervision as sv\nfrom segment_anything import SamPredictor, sam_model_registry\n\nfrom evaluations.iou import box_iou\n\nsam = sam_model_registry[\"default\"](", "\nsam = sam_model_registry[\"default\"](\n    checkpoint=SAM_CHECKPOINT\n)\nimport numpy as np\n\nrecommended_boxes = {}\n\npredictor = SamPredictor(sam)\n\nfor i in images:\n    print(i)\n    bboxes = data[i][\"ground_truth\"]\n\n    img = cv2.imread(i)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    predictor.set_image(img_rgb)\n\n    input_box = bboxes[0]\n\n    annotator = sv.BoxAnnotator()\n\n    input_box = list(map(int, input_box[:4]))\n\n    # cast box as 2d np.ndarray\n    box = np.array([input_box])\n\n    box = list(map(int, input_box[:4]))\n\n    # make box bigger\n    box[0] = box[0] - int(box[2] * args.increase_bounding_box_size)\n    box[1] = box[1] - int(box[3] * args.increase_bounding_box_size)\n    box[2] = box[2] + int(box[2] * args.increase_bounding_box_size)\n    box[3] = box[3] + int(box[3] * args.increase_bounding_box_size)\n\n    masks, scores, logits = predictor.predict(\n        point_coords=None,\n        point_labels=None,\n        box=np.array(box[:4]),\n        multimask_output=False,\n    )\n\n    box_annotator = sv.BoxAnnotator(color=sv.Color.red())\n    mask_annotator = sv.MaskAnnotator(color=sv.Color.red())\n\n    detections = sv.Detections(xyxy=sv.mask_to_xyxy(masks=masks), mask=masks)\n\n    detections = detections[detections.area == np.max(detections.area)]\n\n    recommended_box = detections.xyxy[0].tolist()\n    recommended_boxes[i] = {}\n    recommended_boxes[i][\"rec\"] = recommended_box\n    recommended_boxes[i][\"gt\"] = input_box\n    recommended_boxes[i][\"iou\"] = box_iou(input_box, recommended_box)\n\n    annotator = sv.BoxAnnotator()\n\n    # merge two detections\n    detections = sv.Detections(\n        xyxy=np.concatenate([np.array([input_box]), detections.xyxy]),\n    )\n\n    annotated_image = annotator.annotate(\n        img,\n        detections=detections,\n    )\n\n    sv.plot_image(annotated_image)", "predictor = SamPredictor(sam)\n\nfor i in images:\n    print(i)\n    bboxes = data[i][\"ground_truth\"]\n\n    img = cv2.imread(i)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    predictor.set_image(img_rgb)\n\n    input_box = bboxes[0]\n\n    annotator = sv.BoxAnnotator()\n\n    input_box = list(map(int, input_box[:4]))\n\n    # cast box as 2d np.ndarray\n    box = np.array([input_box])\n\n    box = list(map(int, input_box[:4]))\n\n    # make box bigger\n    box[0] = box[0] - int(box[2] * args.increase_bounding_box_size)\n    box[1] = box[1] - int(box[3] * args.increase_bounding_box_size)\n    box[2] = box[2] + int(box[2] * args.increase_bounding_box_size)\n    box[3] = box[3] + int(box[3] * args.increase_bounding_box_size)\n\n    masks, scores, logits = predictor.predict(\n        point_coords=None,\n        point_labels=None,\n        box=np.array(box[:4]),\n        multimask_output=False,\n    )\n\n    box_annotator = sv.BoxAnnotator(color=sv.Color.red())\n    mask_annotator = sv.MaskAnnotator(color=sv.Color.red())\n\n    detections = sv.Detections(xyxy=sv.mask_to_xyxy(masks=masks), mask=masks)\n\n    detections = detections[detections.area == np.max(detections.area)]\n\n    recommended_box = detections.xyxy[0].tolist()\n    recommended_boxes[i] = {}\n    recommended_boxes[i][\"rec\"] = recommended_box\n    recommended_boxes[i][\"gt\"] = input_box\n    recommended_boxes[i][\"iou\"] = box_iou(input_box, recommended_box)\n\n    annotator = sv.BoxAnnotator()\n\n    # merge two detections\n    detections = sv.Detections(\n        xyxy=np.concatenate([np.array([input_box]), detections.xyxy]),\n    )\n\n    annotated_image = annotator.annotate(\n        img,\n        detections=detections,\n    )\n\n    sv.plot_image(annotated_image)", "\n\nwith open(\"recommended_boxes.json\", \"w\") as f:\n    json.dump(recommended_boxes, f)\n\n# calculate avg iou\nious = [v[\"iou\"] for v in recommended_boxes.values()]\nprint(np.mean(ious))\n", ""]}
{"filename": "scripts/cutout.py", "chunked_list": ["import argparse\nimport csv\nimport os\n\nimport pypandoc\n\nfrom evaluations.dataloaders import RoboflowDataLoader\n\nimport clip\nimport cv2", "import clip\nimport cv2\nimport torch\nfrom PIL import Image\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,", "    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path where your dataset will be saved\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"", "parser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\nparser.add_argument(\n    \"--generate_pdf\", type=bool, required=False, default=False, help=\"Generate PDF\"\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--interactive\", type=bool, required=False, default=False, help=\"Show false positives\"\n)\nparser.add_argument(\n    \"--fp_threshold\",\n    type=float,\n    required=False,\n    default=0.7,\n    help=\"False positive threshold\",", "    default=0.7,\n    help=\"False positive threshold\",\n)\n\nargs = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version", "ROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\nclass_names, data, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"object-detection\",\n    dataset=\"train\"", "    model_type=\"object-detection\",\n    dataset=\"train\"\n).download_dataset()\n\n\nmask_vectors = {}\nclip_vectors_by_class = {}\nbbox_size = []\nclass_distribution = {}\nwhole_image_vectors = []", "class_distribution = {}\nwhole_image_vectors = []\nvectors_by_image = {}\n\nimages = data.keys()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n\ndef run_clip_inference(mask: str) -> torch.Tensor:\n    image = preprocess(Image.fromarray(mask)).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        image_features = clip_model.encode_image(image)\n\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n\n    return image_features", "\n\ndef run_clip_inference(mask: str) -> torch.Tensor:\n    image = preprocess(Image.fromarray(mask)).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        image_features = clip_model.encode_image(image)\n\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n\n    return image_features", "\n\ndef save_report():\n    print(\"Computing images with potential false positives. This may take some time, depending on with how many images you are working.\")\n\n    potential_false_positives = []\n\n    for i in data.keys():\n        img = cv2.imread(i)\n\n        print(\"Evaluating\", i)\n\n        for j in data[i][\"ground_truth\"]:\n            x1 = int(j[0])\n            y1 = int(j[1])\n            x2 = int(j[2])\n            y2 = int(j[3])\n            class_name = class_names[int(j[4])]\n\n            if (x1, y1, x2, y2) not in mask_vectors[class_name]:\n                continue\n\n            img = cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            clip_vector = mask_vectors[class_name][(x1, y1, x2, y2)]\n\n            # show mask\n            mask = img[y1:y2, x1:x2]\n\n            # calculate distance between clip vector and class vector\n            distance = torch.dist(clip_vector, clip_vectors_by_class[class_name])\n\n            if distance > 0.7:\n                print(f\"{i} has a false positive at {x1, y1, x2, y2}\")\n                image = cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n                cv2.imshow(\"image\", image)\n\n                potential_false_positives.append(i)\n\n                if args.interactive:\n                    cv2.imshow(\"mask\", mask)\n\n                    if cv2.waitKey(0) == ord(\"q\"):\n                        args.interactive = False\n\n                    cv2.destroyAllWindows()\n\n    potential_false_positives = \"\\n\\n\".join(potential_false_positives)\n\n    report = f\"\"\"\n    # Bounding Box Size Report\n\n    ## Stats\n\n    - Dataset Used: {ROBOFLOW_PROJECT_URL}/1\n    - Number of Images: {len(images)}\n    - Number of Classes: {len(class_names)}\n\n    ## Class Distribution\n\n    {class_distribution_as_str}\n\n    ## Images with Potential False Positives\n\n    {potential_false_positives}\n    \"\"\"\n                    \n    with open(\"report.md\", \"w\") as f:\n        f.write(report)\n\n    if args.generate_pdf:\n        pypandoc.convert_file(\n            \"report.md\",\n            \"pdf\",\n            outputfile=\"report.pdf\",\n            extra_args=[\"-V\", \"geometry:margin=1in\", \"-V\", \"mainfont=Times New Roman\"],\n        )", "\n# find duplicate clip vectors\ndef find_duplicate_images():\n    # show progress bar\n\n    fps = 0\n\n    images = os.listdir(EVAL_DATA_PATH + \"train/images\")\n\n    for i in images:\n        i = EVAL_DATA_PATH + \"train/images/\" + i\n        print(\"Comparing\", i)\n\n        if vectors_by_image.get(i) is None:\n            vectors_by_image[i] = run_clip_inference(cv2.imread(i))\n\n        for j in vectors_by_image.keys():\n            if i != j:\n                distance = torch.dist(vectors_by_image[i], vectors_by_image[j])\n\n                if distance < 0.1:\n                    print(f\"{i} and {j} are duplicates\")", "\nfor i in data.keys():\n    img = cv2.imread(i)\n    print(\"Computing ground truth vectors for\", i)\n\n    for j in data[i][\"ground_truth\"]:\n        x1 = int(j[0])\n        y1 = int(j[1])\n        x2 = int(j[2])\n        y2 = int(j[3])\n        class_name = class_names[int(j[4])]\n\n        if x1 == x2 or y1 == y2:\n            print(f\"{i} has a 0x0 bounding box\")\n            continue\n\n        mask = img[y1:y2, x1:x2]\n\n        if class_name not in clip_vectors_by_class:\n            clip_vectors_by_class[class_name] = []\n\n        vector = run_clip_inference(mask)\n\n        class_distribution[class_name] = class_distribution.get(class_name, 0) + 1\n\n        if mask_vectors.get(class_name) is None:\n            mask_vectors[class_name] = {}\n\n        mask_vectors[class_name][(x1, y1, x2, y2)] = vector\n\n        clip_vectors_by_class[class_name].append(vector)\n        bbox_size.append(mask.shape[0] * mask.shape[1])\n        whole_image_clip = run_clip_inference(img)\n\n        whole_image_vectors.append(whole_image_clip)\n        vectors_by_image[i] = whole_image_clip", "\nprint(\"Computing mean class vectors\")\n\nfor i in clip_vectors_by_class.keys():\n    clip_vectors_by_class[i] = torch.mean(torch.stack(clip_vectors_by_class[i]), dim=0)\n\nwhole_image_vectors = torch.mean(torch.stack(whole_image_vectors), dim=0)\n\navg_frame_size = sum(bbox_size) / len(bbox_size)\nmega_pixels = round(avg_frame_size / 1000000, 2)", "avg_frame_size = sum(bbox_size) / len(bbox_size)\nmega_pixels = round(avg_frame_size / 1000000, 2)\n\nclass_distribution_as_str = \"\"\"\n| Class Name | Count |\n| ---------- | ----- |\n{}\"\"\".format(\n    \"\\n\".join(\n        [\n            \"| {} | {} |\".format(class_name, count)", "        [\n            \"| {} | {} |\".format(class_name, count)\n            for class_name, count in class_distribution.items()\n        ]\n    )\n)\n\nif __name__ == \"__main__\":\n    # find_duplicate_images()\n    print(\"Saving report\")\n    save_report()", ""]}
{"filename": "scripts/bbox_sizes.py", "chunked_list": ["import cv2\n\ndef find_small_bounding_boxes(data):\n    for img in data.keys():\n        for j in data[img][\"ground_truth\"]:\n            x1 = int(j[0])\n            y1 = int(j[1])\n            x2 = int(j[2])\n            y2 = int(j[3])\n\n            if x1 == x2 or y1 == y2:\n                print(f\"{img} has a 0x0 bounding box\")\n                continue\n\n            if x2 - x1 < 10 or y2 - y1 < 10:\n                print(f\"{img} has a small bounding box\")", "\ndef find_large_bounding_boxes(data):\n    for img in data.keys():\n        width, height = cv2.imread(img).shape[:2]\n\n        for j in data[img][\"ground_truth\"]:\n            x1 = int(j[0])\n            y1 = int(j[1])\n            x2 = int(j[2])\n            y2 = int(j[3])\n            \n            if x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n                print(f\"{img} has a large bounding box\")", ""]}
{"filename": "examples/dino_example.py", "chunked_list": ["import argparse\nimport os\n\nimport cv2\nfrom groundingdino.util.inference import Model\n\nfrom evaluations import Evaluator\nfrom evaluations.dataloaders import RoboflowDataLoader\n\nparser = argparse.ArgumentParser()", "\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)", "    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\nparser.add_argument(\n    \"--config_path\", type=str, required=True, help=\"Path to GroundingDINO config\"\n)\nparser.add_argument(\n    \"--weights_path\", type=str, required=True, help=\"Path to GroundingDINO weights\"\n)\nparser.add_argument(\"--box_threshold\", type=float, required=False, help=\"Box threshold\")\nparser.add_argument(", "parser.add_argument(\"--box_threshold\", type=float, required=False, help=\"Box threshold\")\nparser.add_argument(\n    \"--text_threshold\", type=float, required=False, help=\"Text threshold\"\n)\n\nargs = parser.parse_args()\n\nif not args.box_threshold:\n    BOX_THRESHOLD = 0.35\n\nif not args.text_threshold:\n    TEXT_THRESHOLD = 0.25", "\nif not args.text_threshold:\n    TEXT_THRESHOLD = 0.25\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\nCONFIG_PATH = args.config_path\nWEIGHTS_PATH = args.weights_path", "CONFIG_PATH = args.config_path\nWEIGHTS_PATH = args.weights_path\n\nIMAGE_PATH = EVAL_DATA_PATH + \"/images/valid\"\n\nclass_names, data, _ = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,", "    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n).download_dataset()\n\nmodel = Model(\n    model_config_path=CONFIG_PATH, model_checkpoint_path=WEIGHTS_PATH, device=\"cpu\"\n)\n\nall_predictions = {}\n\nfor file in os.listdir(IMAGE_PATH)[:2]:\n    file = os.path.join(IMAGE_PATH, file)\n    if file.endswith(\".jpg\"):\n        image = cv2.imread(file)\n\n        detections = model.predict_with_classes(\n            image=image,\n            classes=class_names,\n            box_threshold=BOX_THRESHOLD,\n            text_threshold=TEXT_THRESHOLD,\n        )\n\n        all_predictions[file] = {\"filename\": file, \"predictions\": detections}", "all_predictions = {}\n\nfor file in os.listdir(IMAGE_PATH)[:2]:\n    file = os.path.join(IMAGE_PATH, file)\n    if file.endswith(\".jpg\"):\n        image = cv2.imread(file)\n\n        detections = model.predict_with_classes(\n            image=image,\n            classes=class_names,\n            box_threshold=BOX_THRESHOLD,\n            text_threshold=TEXT_THRESHOLD,\n        )\n\n        all_predictions[file] = {\"filename\": file, \"predictions\": detections}", "\nevaluator = Evaluator(\n    ground_truth=data,\n    predictions=all_predictions,\n    class_names=class_names,\n    confidence_threshold=0.2,\n    mode=\"batch\",\n)\n\ncf = evaluator.eval_model_predictions()", "\ncf = evaluator.eval_model_predictions()\n\ndata = evaluator.calculate_statistics()\n\nprint(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)\nprint(\"f1 Score:\", data.f1)\n", ""]}
{"filename": "examples/roboflow_example.py", "chunked_list": ["import argparse\nimport os\n\nfrom evaluations.dataloaders import (RoboflowDataLoader,\n                                     RoboflowPredictionsDataLoader)\nfrom evaluations.roboflow import RoboflowEvaluator\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(", "\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)", "    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\n\nargs = parser.parse_args()", "\nargs = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\nclass_names, data, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,", "class_names, data, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n).download_dataset()\n\npredictions = RoboflowPredictionsDataLoader(\n    model=model,\n    model_type=\"object-detection\",", "    model=model,\n    model_type=\"object-detection\",\n    image_files=EVAL_DATA_PATH,\n    class_names=class_names,\n).process_files()\n\nevaluator = RoboflowEvaluator(\n    ground_truth=data, predictions=predictions, class_names=class_names, mode=\"batch\"\n)\n", ")\n\ncf = evaluator.eval_model_predictions()\n\nprint(cf)\n\ndata = evaluator.calculate_statistics()\n\nprint(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)", "print(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)\nprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/compare_roboflow_models.py", "chunked_list": ["import os\n\nimport pypandoc\n\nfrom evaluations import CompareEvaluations\nfrom evaluations.confusion_matrices import plot_confusion_matrix\nfrom evaluations.dataloaders import (RoboflowDataLoader,\n                                     RoboflowPredictionsDataLoader)\nfrom evaluations.roboflow import RoboflowEvaluator\n", "from evaluations.roboflow import RoboflowEvaluator\n\nMODEL_TYPE = \"object-detection\"\n\nmodels = [\n    {\n        \"workspace_url\": \"\",\n        \"project_url\": \"\",\n        \"model_version\": 8,\n    },", "        \"model_version\": 8,\n    },\n]\n\nevals = []\ndatasets = []\n\nfor m in models:\n    loader = RoboflowDataLoader(\n        workspace_url=m[\"workspace_url\"],\n        project_url=m[\"project_url\"],\n        project_version=m[\"model_version\"],\n        image_files=m[\"project_url\"],\n        model_type=MODEL_TYPE,\n    )\n\n    class_names, data, model = loader.download_dataset()\n\n    predictions = RoboflowPredictionsDataLoader(\n        model=model,\n        model_type=\"object-detection\",\n        image_files=m[\"project_url\"],\n        class_names=class_names,\n    ).process_files()\n\n    long_name = f\"{m['workspace_url']}/{m['project_url']}/{m['model_version']}\"\n\n    evals.append(\n        RoboflowEvaluator(\n            ground_truth=data,\n            predictions=predictions,\n            class_names=class_names,\n            mode=\"batch\",\n            name=long_name,\n            model_type=MODEL_TYPE,\n        )\n    )\n\n    print(loader.dataset_content)\n\n    datasets.append(loader.dataset_content)", "\nbest = CompareEvaluations(evals)\n\nfull_path = os.path.join(os.getcwd(), \"output\", \"matrices\")\n\nbest, evaluations = best.compare()\n\nplot_confusion_matrix(\n    best[4], class_names=best[3], file_name=f\"Best Confusion Matrix\", mode=\"return\"\n)", "    best[4], class_names=best[3], file_name=f\"Best Confusion Matrix\", mode=\"return\"\n)\n\nbest_cf = (\n    f\"![Confusion Matrix]({full_path}/Best Confusion Matrix.png)\" + \"{ width=400px }\"\n)\n\ncfs = [\n    f\"![Confusion Matrix]({full_path}/{c[6].replace('/', '_')}.png)\" + \"{ width=400px }\"\n    for c in evaluations", "    f\"![Confusion Matrix]({full_path}/{c[6].replace('/', '_')}.png)\" + \"{ width=400px }\"\n    for c in evaluations\n]\n\ncfs = \"\\n\\n\".join(cfs)\n\ndataset_meta = \"\"\n\nfor d in datasets:\n    preprocessing = d.preprocessing\n    augmentations = d.augmentation\n    # convert to str\n    augmentations = str(augmentations)\n    preprocessing = str(preprocessing)\n    splits = d.splits\n\n    dataset_meta += f\"### {d.name} (Version {d.version})\\n\\n\"\n\n    dataset_meta += f\"#### Preprocessing\\n\\n\"\n\n    dataset_meta += f\"> ```python\\n{preprocessing}\\n```\\n\\n\"\n\n    dataset_meta += f\"\\n\\n#### Augmentations\\n\\n\"\n\n    dataset_meta += f\"> ```python\\n{augmentations}\\n```\\n\\n\"\n\n    dataset_meta += f\"\\n\\n#### Splits\\n\\n\"\n\n    dataset_meta += \"Valid: \" + str(splits[\"valid\"]) + \"\\n\\n\"\n    dataset_meta += \"Train: \" + str(splits[\"train\"]) + \"\\n\\n\"\n    dataset_meta += \"Test: \" + str(splits[\"test\"]) + \"\\n\\n\"", "for d in datasets:\n    preprocessing = d.preprocessing\n    augmentations = d.augmentation\n    # convert to str\n    augmentations = str(augmentations)\n    preprocessing = str(preprocessing)\n    splits = d.splits\n\n    dataset_meta += f\"### {d.name} (Version {d.version})\\n\\n\"\n\n    dataset_meta += f\"#### Preprocessing\\n\\n\"\n\n    dataset_meta += f\"> ```python\\n{preprocessing}\\n```\\n\\n\"\n\n    dataset_meta += f\"\\n\\n#### Augmentations\\n\\n\"\n\n    dataset_meta += f\"> ```python\\n{augmentations}\\n```\\n\\n\"\n\n    dataset_meta += f\"\\n\\n#### Splits\\n\\n\"\n\n    dataset_meta += \"Valid: \" + str(splits[\"valid\"]) + \"\\n\\n\"\n    dataset_meta += \"Train: \" + str(splits[\"train\"]) + \"\\n\\n\"\n    dataset_meta += \"Test: \" + str(splits[\"test\"]) + \"\\n\\n\"", "\nmodel_table_data = \"\"\n\nfor c in evaluations:\n    plot_confusion_matrix(\n        c[4],\n        file_name=f\"{c[6].replace('/', '_')}\",\n        class_names=c[3],\n        mode=\"return\",\n    )\n    model_table_data += f\"| {c[6]} | {round(c[0], 2)} | {round(c[1], 2)} | {round(c[2], 2)} | {c[5]} |\\n\"", "\nreport = f\"\"\"\n# Roboflow Model Comparison\n\nTotal evaluations run: {len(evaluations)}\n\n## Models\n\n| Model | F1 Score | Precision | Recall | Confidence Threshold |\n| --- | --- | --- | --- | --- |", "| Model | F1 Score | Precision | Recall | Confidence Threshold |\n| --- | --- | --- | --- | --- |\n{model_table_data}\n\n## Datasets\n\n{dataset_meta}\n\n## Best Model (measured by f1 score)\n", "## Best Model (measured by f1 score)\n\n{best_cf}\n\n## Confusion Matrices by Model\n\n{cfs}\n\"\"\"\n\nwith open(\"roboflow_model_comparison.md\", \"w\") as f:\n    f.write(report)", "\nwith open(\"roboflow_model_comparison.md\", \"w\") as f:\n    f.write(report)\n\npypandoc.convert_file(\n    \"roboflow_model_comparison.md\",\n    \"pdf\",\n    outputfile=\"roboflow_model_comparison.pdf\",\n    extra_args=[\"--extract-media\", \".\", \"-V\", \"geometry:margin=1in\"],\n)", "    extra_args=[\"--extract-media\", \".\", \"-V\", \"geometry:margin=1in\"],\n)\n"]}
{"filename": "examples/clip_blip_albef_compare.py", "chunked_list": ["import argparse\n\nimport torch\nfrom models.blip import run_blip_albef_inference\nfrom models.clip import run_clip_inference\n\nfrom evaluations.classification import ClassificationEvaluator\nfrom evaluations.compare import CompareEvaluations\nfrom evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.dataloaders.classification import \\", "from evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.dataloaders.classification import \\\n    ClassificationFolderDataLoader\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",", "parser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\n\nargs = parser.parse_args()\n", "args = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\nIMAGE_PATH = EVAL_DATA_PATH + \"/valid\"\n\nclass_names, ground_truth, model = RoboflowDataLoader(", "\nclass_names, ground_truth, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"classification\",\n).download_dataset()\n\n# dedupe class names", "\n# dedupe class names\n\nclass_names = list(set(class_names))\n\nall_clip_predictions = {}\n\nfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n    # print(file)\n    all_clip_predictions.update(run_clip_inference(file, class_names))", "\nall_blip_predictions = {}\n\nfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n    all_blip_predictions.update(\n        run_blip_albef_inference(file, class_names, \"blip\", device)\n    )\n\nall_albef_predictions = {}\n\nfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n    all_albef_predictions.update(\n        run_blip_albef_inference(file, class_names, \"albef\", device)\n    )", "all_albef_predictions = {}\n\nfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n    all_albef_predictions.update(\n        run_blip_albef_inference(file, class_names, \"albef\", device)\n    )\n\nevals = [\n    {\"name\": \"clip\", \"predictions\": all_clip_predictions},\n    {\"name\": \"blip\", \"predictions\": all_blip_predictions},", "    {\"name\": \"clip\", \"predictions\": all_clip_predictions},\n    {\"name\": \"blip\", \"predictions\": all_blip_predictions},\n    {\"name\": \"albef\", \"predictions\": all_albef_predictions},\n]\n\nbest = CompareEvaluations(\n    [\n        ClassificationEvaluator(\n            predictions=cn[\"predictions\"],\n            ground_truth=ground_truth,", "            predictions=cn[\"predictions\"],\n            ground_truth=ground_truth,\n            class_names=class_names,\n            mode=\"batch\",\n            name=cn[\"name\"],\n        )\n    ]\n    for cn in evals\n)\n", ")\n\ncf = best.compare()\n\nprint(cf)\n"]}
{"filename": "examples/google_cloud_vision_example.py", "chunked_list": ["import argparse\nimport base64\nimport glob\nimport os\nfrom typing import Sequence\n\nimport cv2\nimport numpy as np\nimport supervision as sv\nfrom google.cloud import vision", "import supervision as sv\nfrom google.cloud import vision\n\nfrom evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.roboflow import RoboflowEvaluator\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",", "parser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\n\nargs = parser.parse_args()\n", "args = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\nVALIDATION_SET_PATH = EVAL_DATA_PATH + \"/valid/images/*.jpg\"\n\n# map class names to class ids", "\n# map class names to class ids\nclass_mappings = {\n    \"Mug\": 0,\n}\n\n\ndef analyze_image_from_uri(\n    image_uri: str,\n    feature_types: Sequence,\n) -> vision.AnnotateImageResponse:\n    client = vision.ImageAnnotatorClient()\n\n    image = vision.Image()\n\n    loaded_image = cv2.imread(image_uri)\n\n    base64_image = base64.b64encode(open(image_uri, \"rb\").read())\n\n    image.source.image_uri = \"base64://\" + base64_image.decode()\n\n    features = [vision.Feature(type_=feature_type) for feature_type in feature_types]\n\n    request = {\n        \"image\": {\"content\": base64_image.decode()},\n        \"features\": features,\n    }\n\n    response = client.annotate_image(request)\n\n    localized_object_annotations = response.localized_object_annotations\n\n    localized_object_annotations = [\n        annotation\n        for annotation in localized_object_annotations\n        if annotation.name in class_mappings.keys()\n    ]\n\n    if len(localized_object_annotations) == 0:\n        return sv.detection.core.Detections(\n            xyxy=np.array([[0, 0, 0, 0]]),\n            class_id=np.array([None]),\n            confidence=np.array([100]),\n        )\n\n    xyxys = []\n\n    image_size = loaded_image.shape\n\n    for obj in localized_object_annotations:\n        # scale up to full image size\n        x0 = obj.bounding_poly.normalized_vertices[0].x * image_size[1]\n        y0 = obj.bounding_poly.normalized_vertices[0].y * image_size[0]\n        x1 = obj.bounding_poly.normalized_vertices[2].x * image_size[1]\n        y1 = obj.bounding_poly.normalized_vertices[2].y * image_size[0]\n\n        xyxys.append([x0, y0, x1, y1])\n\n    annotations = sv.detection.core.Detections(\n        xyxy=np.array(xyxys).astype(np.float32),\n        class_id=np.array(\n            [class_mappings[obj.name] for obj in localized_object_annotations]\n        ),\n        confidence=np.array([obj.score for obj in localized_object_annotations]),\n    )\n\n    return annotations", "\n\nclass_names, data, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n).download_dataset()\n\nall_predictions = {}", "\nall_predictions = {}\n\nfor file in glob.glob(VALIDATION_SET_PATH):\n    # add base dir\n    current_dir = os.getcwd()\n\n    response = analyze_image_from_uri(file, [vision.Feature.Type.OBJECT_LOCALIZATION])\n\n    all_predictions[file] = {\"filename\": file, \"predictions\": response}", "\nevaluator = RoboflowEvaluator(\n    ground_truth=data,\n    predictions=all_predictions,\n    class_names=class_names,\n    mode=\"batch\",\n)\n\ncf = evaluator.eval_model_predictions()\n", "cf = evaluator.eval_model_predictions()\n\ndata = evaluator.calculate_statistics()\n\nprint(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)\nprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/blip_albef_compare_example.py", "chunked_list": ["import argparse\n\nimport torch\nfrom models.blip import run_blip_albef_inference\n\nfrom evaluations.classification import ClassificationEvaluator\nfrom evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.dataloaders.classification import \\\n    ClassificationFolderDataLoader\n", "    ClassificationFolderDataLoader\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,", "    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)", "    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\nparser.add_argument(\n    \"--feature_extractor\",\n    type=str,\n    required=True,\n    help=\"Feature extractor to use ('blip' or 'albef')\",", "    required=True,\n    help=\"Feature extractor to use ('blip' or 'albef')\",\n)\n\nargs = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version", "ROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\nFEATURE_EXTRACTOR = args.feature_extractor\n\nif FEATURE_EXTRACTOR not in (\"blip\", \"albef\"):\n    raise ValueError(\"feature_extractor must be either 'blip' or 'albef'\")\n\n# use validation set\nIMAGE_PATH = EVAL_DATA_PATH + \"/images/valid\"\n", "IMAGE_PATH = EVAL_DATA_PATH + \"/images/valid\"\n\nclass_names, ground_truth, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"classification\",\n).download_dataset()\n", ").download_dataset()\n\nall_predictions = {}\n\nfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n    all_predictions.update(\n        run_blip_albef_inference(file, class_names, FEATURE_EXTRACTOR, device)\n    )\n\nevaluator = ClassificationEvaluator(", "\nevaluator = ClassificationEvaluator(\n    ground_truth=ground_truth,\n    predictions=all_predictions,\n    class_names=[\"banana\", \"apple\"],\n    mode=\"batch\",\n)\n\ncf = evaluator.eval_model_predictions()\n", "cf = evaluator.eval_model_predictions()\n\ndata = evaluator.calculate_statistics()\n\nprint(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)\nprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/imagebind_example.py", "chunked_list": ["import data\nimport torch\nfrom models import imagebind_model\nfrom models.imagebind_model import ModalityType\n\nimport argparse\nimport os\n\nimport clip\nimport torch", "import clip\nimport torch\nfrom PIL import Image\n\nfrom evaluations.classification import ClassificationEvaluator\nfrom evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.dataloaders.classification import ClassificationDetections\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n", "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = imagebind_model.imagebind_huge(pretrained=True)\nmodel.eval()\nmodel.to(device)\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",", "parser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\n\nargs = parser.parse_args()\n", "args = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\n# use validation set\nIMAGE_PATH = EVAL_DATA_PATH + \"/valid\"\n", "IMAGE_PATH = EVAL_DATA_PATH + \"/valid\"\n\nclass_names, ground_truth, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"multiclass\",\n).download_dataset()\n", ").download_dataset()\n\nall_predictions = {}\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n\ndef run_imagebind_inference(class_names: str, img_filename: str) -> str:\n    \"\"\"\n    Run inference on an image using ImageBind.\n\n    Args:\n        class_names: The class names to use for inference.\n        img_filename: The filename of the image on which to run inference.\n\n    Returns:\n        The predicted class name.\n    \"\"\"\n    image_paths=[\".assets/dog_image.jpg\"]\n\n    # Load data\n    inputs = {\n        ModalityType.TEXT: data.load_and_transform_text(class_names, device),\n        ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n    }\n\n    with torch.no_grad():\n        embeddings = model(inputs)\n\n    text_result = torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1)\n\n    top_k = torch.topk(text_result, k=1, dim=-1)\n    confidence = top_k.values[0][0].item()\n\n    return class_names[top_k.indices[0][0]], confidence", "def run_imagebind_inference(class_names: str, img_filename: str) -> str:\n    \"\"\"\n    Run inference on an image using ImageBind.\n\n    Args:\n        class_names: The class names to use for inference.\n        img_filename: The filename of the image on which to run inference.\n\n    Returns:\n        The predicted class name.\n    \"\"\"\n    image_paths=[\".assets/dog_image.jpg\"]\n\n    # Load data\n    inputs = {\n        ModalityType.TEXT: data.load_and_transform_text(class_names, device),\n        ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n    }\n\n    with torch.no_grad():\n        embeddings = model(inputs)\n\n    text_result = torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1)\n\n    top_k = torch.topk(text_result, k=1, dim=-1)\n    confidence = top_k.values[0][0].item()\n\n    return class_names[top_k.indices[0][0]], confidence", "\n\nfor file in ground_truth.keys():\n    # add base dir\n    current_dir = os.getcwd()\n    file = os.path.join(current_dir, file)\n\n    # run inference\n    class_name, confidence = run_imagebind_inference(class_names, file)\n\n    result = ClassificationDetections(\n        image_id=file,\n        predicted_class_names=[class_name],\n        predicted_class_ids=[class_names.index(class_name)],\n        confidence=[1.0, confidence],\n    )\n\n    all_predictions[file] = {\"filename\": file, \"predictions\": result}", "\nevaluator = ClassificationEvaluator(\n    ground_truth=ground_truth,\n    predictions=all_predictions,\n    class_names=class_names,\n    mode=\"batch\",\n    model_type=\"multiclass\",\n)\n\ncf = evaluator.eval_model_predictions()", "\ncf = evaluator.eval_model_predictions()\n\ndata = evaluator.calculate_statistics()\n\nprint(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)\nprint(\"f1 Score:\", data.f1)\n", ""]}
{"filename": "examples/azure_example.py", "chunked_list": ["import argparse\nimport glob\nimport os\n\nimport cv2\nimport numpy as np\nimport requests\nimport supervision as sv\n\nfrom evaluations.dataloaders import RoboflowDataLoader", "\nfrom evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.roboflow import RoboflowEvaluator\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,", "    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)", "    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\nparser.add_argument(\"--azure_endpoint\", type=str, required=True, help=\"Azure endpoint\")\nparser.add_argument(\"--azure_api_key\", type=str, required=True, help=\"Azure API key\")\n\nargs = parser.parse_args()\n", "args = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\nAZURE_ENDPOINT = args.azure_endpoint\nAZURE_API_KEY = args.azure_api_key\n\nVALIDATION_SET_PATH = EVAL_DATA_PATH + \"/valid/images/*.jpg\"", "\nVALIDATION_SET_PATH = EVAL_DATA_PATH + \"/valid/images/*.jpg\"\n\n\n# map class names to class ids\nclass_mappings = {\n    \"cup\": 0,\n}\n\n\ndef run_inference_azure(image_filename):\n    with open(image_filename, \"rb\") as image_file:\n        data = image_file.read()\n\n    image = cv2.imread(image_filename)\n\n    headers = {\n        \"Content-Type\": \"application/octet-stream\",\n        \"Ocp-Apim-Subscription-Key\": AZURE_API_KEY,\n    }\n\n    response = requests.request(\"POST\", AZURE_ENDPOINT, headers=headers, data=data)\n\n    predictions = response.json()\n\n    if \"error\" in predictions.keys():\n        print(predictions[\"error\"][\"message\"])\n        return sv.detection.core.Detections(\n            xyxy=np.array([[0, 0, 0, 0]]),\n            class_id=np.array([None]),\n            confidence=np.array([100]),\n        )\n\n    predictions = predictions[\"objects\"]\n\n    # turn predictions into xyxys\n    xyxys = []\n\n    predictions = [\n        prediction\n        for prediction in predictions\n        if prediction[\"object\"] in class_mappings.keys()\n    ]\n\n    if len(predictions) == 0:\n        return sv.detection.core.Detections(\n            xyxy=np.array([[0, 0, 0, 0]]),\n            class_id=np.array([None]),\n            confidence=np.array([100]),\n        )\n\n    for obj in predictions:\n        # scale up to full image size\n        x0 = obj[\"rectangle\"][\"x\"] * image.shape[1]\n        y0 = obj[\"rectangle\"][\"y\"] * image.shape[0]\n        x1 = (obj[\"rectangle\"][\"x\"] + obj[\"rectangle\"][\"w\"]) * image.shape[1]\n        y1 = (obj[\"rectangle\"][\"y\"] + obj[\"rectangle\"][\"h\"]) * image.shape[0]\n\n        xyxys.append([x0, y0, x1, y1])\n\n    annotations = sv.detection.core.Detections(\n        xyxy=np.array(xyxys).astype(np.float32),\n        class_id=np.array([class_mappings[obj[\"object\"]] for obj in predictions]),\n        confidence=np.array([obj[\"confidence\"] for obj in predictions]),\n    )\n\n    return annotations", "\n\ndef run_inference_azure(image_filename):\n    with open(image_filename, \"rb\") as image_file:\n        data = image_file.read()\n\n    image = cv2.imread(image_filename)\n\n    headers = {\n        \"Content-Type\": \"application/octet-stream\",\n        \"Ocp-Apim-Subscription-Key\": AZURE_API_KEY,\n    }\n\n    response = requests.request(\"POST\", AZURE_ENDPOINT, headers=headers, data=data)\n\n    predictions = response.json()\n\n    if \"error\" in predictions.keys():\n        print(predictions[\"error\"][\"message\"])\n        return sv.detection.core.Detections(\n            xyxy=np.array([[0, 0, 0, 0]]),\n            class_id=np.array([None]),\n            confidence=np.array([100]),\n        )\n\n    predictions = predictions[\"objects\"]\n\n    # turn predictions into xyxys\n    xyxys = []\n\n    predictions = [\n        prediction\n        for prediction in predictions\n        if prediction[\"object\"] in class_mappings.keys()\n    ]\n\n    if len(predictions) == 0:\n        return sv.detection.core.Detections(\n            xyxy=np.array([[0, 0, 0, 0]]),\n            class_id=np.array([None]),\n            confidence=np.array([100]),\n        )\n\n    for obj in predictions:\n        # scale up to full image size\n        x0 = obj[\"rectangle\"][\"x\"] * image.shape[1]\n        y0 = obj[\"rectangle\"][\"y\"] * image.shape[0]\n        x1 = (obj[\"rectangle\"][\"x\"] + obj[\"rectangle\"][\"w\"]) * image.shape[1]\n        y1 = (obj[\"rectangle\"][\"y\"] + obj[\"rectangle\"][\"h\"]) * image.shape[0]\n\n        xyxys.append([x0, y0, x1, y1])\n\n    annotations = sv.detection.core.Detections(\n        xyxy=np.array(xyxys).astype(np.float32),\n        class_id=np.array([class_mappings[obj[\"object\"]] for obj in predictions]),\n        confidence=np.array([obj[\"confidence\"] for obj in predictions]),\n    )\n\n    return annotations", "\n\nclass_names, data, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n).download_dataset()\n\nall_predictions = {}", "\nall_predictions = {}\n\nfor file in glob.glob(VALIDATION_SET_PATH):\n    response = run_inference_azure(file)\n\n    all_predictions[file] = {\"filename\": file, \"predictions\": response}\n\nprint(all_predictions)\nprint(data)", "print(all_predictions)\nprint(data)\n\nevaluator = RoboflowEvaluator(\n    ground_truth=data,\n    predictions=all_predictions,\n    class_names=class_names,\n    mode=\"batch\",\n)\n", ")\n\ncf = evaluator.eval_model_predictions()\n\ndata = evaluator.calculate_statistics()\n\nprint(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)\nprint(\"f1 Score:\", data.f1)\n", "print(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/segmentation.py", "chunked_list": ["import argparse\nimport os\nimport supervision as sv\n\nfrom evaluations.dataloaders import (RoboflowDataLoader,\n                                     RoboflowPredictionsDataLoader)\nfrom evaluations.segmentation import SegmentationEvaluator\n\nparser = argparse.ArgumentParser()\n", "parser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"", "parser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\n", ")\n\nargs = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\nclass_names, _, model = RoboflowDataLoader(", "\nclass_names, _, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"segmentation\",\n).download_dataset()\n\nds = sv.DetectionDataset.from_yolo(", "\nds = sv.DetectionDataset.from_yolo(\n    images_directory_path=os.path.join(EVAL_DATA_PATH, \"test/images\"),\n    annotations_directory_path=os.path.join(EVAL_DATA_PATH, \"test/labels\"),\n    data_yaml_path=os.path.join(EVAL_DATA_PATH, \"data.yaml\"),\n)\n\npredictions = RoboflowPredictionsDataLoader(\n    model=model,\n    model_type=\"segmentation\",", "    model=model,\n    model_type=\"segmentation\",\n    image_files=EVAL_DATA_PATH,\n    class_names=class_names,\n).process_files()\n\n# add EVAL_DATA_PATH to predictions\n\nevaluator = SegmentationEvaluator(\n    ground_truth=ds, predictions=predictions, class_names=class_names, eval_data_path = EVAL_DATA_PATH", "evaluator = SegmentationEvaluator(\n    ground_truth=ds, predictions=predictions, class_names=class_names, eval_data_path = EVAL_DATA_PATH\n)\n\ncf = evaluator.eval_model_predictions()\n\nfrom evaluations.confusion_matrices import plot_confusion_matrix\n\nplot_confusion_matrix(\n    cf,", "plot_confusion_matrix(\n    cf,\n    class_names=class_names,\n    aggregate=True,\n)\n\n\ndata = evaluator.calculate_statistics()\n\nprint(\"Precision:\", data.precision)", "\nprint(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)\nprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/clip_compare_example.py", "chunked_list": ["import argparse\nimport os\n\nfrom models.clip import run_clip_inference\n\nfrom evaluations.classification import ClassificationEvaluator\nfrom evaluations.compare import CompareEvaluations\nfrom evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.dataloaders.classification import \\\n    ClassificationFolderDataLoader", "from evaluations.dataloaders.classification import \\\n    ClassificationFolderDataLoader\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",", "    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\n\nargs = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url", "ROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\nIMAGE_PATH = EVAL_DATA_PATH + \"/images/valid\"\n\nclass_names, ground_truth, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,", "    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"classification\",\n).download_dataset()\n\nevals = [\n    {\"classes\": [\"orange\", \"background\"], \"confidence\": 1},\n    {\"classes\": [\"pear\", \"background\"], \"confidence\": 0.9},\n]", "    {\"classes\": [\"pear\", \"background\"], \"confidence\": 0.9},\n]\n\nevaluations = []\n\nfor cn in evals:\n    all_predictions = {}\n\n    for file in ClassificationFolderDataLoader(IMAGE_PATH).get_files():\n        all_predictions.update(run_clip_inference(file, class_names))\n\n        evaluations.append(\n            ClassificationEvaluator(\n                predictions=all_predictions,\n                ground_truth=ground_truth,\n                confidence_threshold=cn[\"confidence\"],\n                class_names=class_names + cn[\"classes\"],\n                mode=\"batch\",\n            )\n        )", "\nbest = CompareEvaluations(evaluations)\n\ncf = best.compare()\n\nprint(cf)\n"]}
{"filename": "examples/dinov2_example.py", "chunked_list": ["import argparse\nimport os\n\nimport clip\nfrom evaluations import CompareEvaluations\nfrom evaluations.classification import ClassificationEvaluator\nfrom evaluations.confusion_matrices import plot_confusion_matrix\nfrom evaluations.dataloaders import RoboflowDataLoader\n\nimport models.clip as clip", "\nimport models.clip as clip\nimport models.dinov2 as dinov2\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,", "    type=str,\n    required=True,\n    help=\"Absolute path to Classification dataset (if you don't have one, this is the path in which your Roboflow dataset will be saved)\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)", "    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\n\nargs = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url", "EVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\nIMAGE_PATH = EVAL_DATA_PATH + \"/test\"\n\nclass_names, ground_truth, _ = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,", "    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"classification\",\n).download_dataset()\n\nlabels = {}\n\ndinov2_predictions = {}", "\ndinov2_predictions = {}\nclip_predictions = {}\n\nfor folder in os.listdir(IMAGE_PATH):\n    for file in os.listdir(os.path.join(IMAGE_PATH, folder)):\n        if file.endswith(\".jpg\"):\n            full_name = os.path.join(IMAGE_PATH, folder, file)\n            labels[full_name] = folder\n", "\nfiles = labels.keys()\n\nmodel = dinov2.train_dinov2_svm_model(IMAGE_PATH)\n\nprint(\n    \"DINOv2 Model Trained. Starting inference (this may take a while depending on how many images you are using).\"\n)\n\nall_predictions = {}", "\nall_predictions = {}\n\nfor file in list(ground_truth.keys())[:3]:\n    print(\"Running inference on\", file)\n    dinov2_result = dinov2.run_dinov2_inference(model, file, class_names)\n    clip_result = clip.run_clip_inference(file, class_names)\n\n    dinov2_predictions[file] = dinov2_result\n    clip_predictions[file] = clip_result", "\nprint(\"Running comparison.\")\n\nbest = CompareEvaluations(\n    [\n        ClassificationEvaluator(\n            ground_truth=ground_truth,\n            predictions=dinov2_predictions,\n            class_names=class_names,\n            mode=\"batch\",", "            class_names=class_names,\n            mode=\"batch\",\n            model_type=\"multiclass\",\n            name=\"DINOv2\",\n        ),\n        ClassificationEvaluator(\n            ground_truth=ground_truth,\n            predictions=clip_predictions,\n            class_names=class_names,\n            mode=\"batch\",", "            class_names=class_names,\n            mode=\"batch\",\n            model_type=\"multiclass\",\n            name=\"CLIP\",\n        ),\n    ]\n)\n\nhighest_eval, comparison = best.compare()\n", "highest_eval, comparison = best.compare()\n\nplot_confusion_matrix(highest_eval[4], class_names)\n\nprint(\"F1 Score:\", highest_eval[0])\nprint(\"Precision:\", highest_eval[1])\nprint(\"Recall:\", highest_eval[2])\nprint(\"Class Names:\", highest_eval[3])\nprint(\"Confidence Threshold:\", highest_eval[5])\n", "print(\"Confidence Threshold:\", highest_eval[5])\n"]}
{"filename": "examples/clip_example.py", "chunked_list": ["import argparse\nimport os\n\nimport clip\nimport torch\nfrom PIL import Image\n\nfrom evaluations.classification import ClassificationEvaluator\nfrom evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.dataloaders.classification import ClassificationDetections", "from evaluations.dataloaders import RoboflowDataLoader\nfrom evaluations.dataloaders.classification import ClassificationDetections\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",", "    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(", ")\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\n\nargs = parser.parse_args()\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url", "ROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\n# use validation set\nIMAGE_PATH = EVAL_DATA_PATH + \"/valid\"\n\nclass_names, ground_truth, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,", "    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"multiclass\",\n).download_dataset()\n\nall_predictions = {}\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"", "\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nfor file in ground_truth.keys():\n    # add base dir\n    current_dir = os.getcwd()\n    file = os.path.join(current_dir, file)\n\n    image = preprocess(Image.open(file)).unsqueeze(0).to(device)\n    text = clip.tokenize(class_names).to(device)\n\n    with torch.no_grad():\n        image_features = clip_model.encode_image(image)\n        text_features = clip_model.encode_text(text)\n\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n    values, indices = similarity[0].topk(1)\n\n    data = ClassificationDetections(\n        image_id=file,\n        predicted_class_names=[class_names[indices[0]]],\n        predicted_class_ids=[indices[0]],\n        confidence=values[0],\n    )\n\n    all_predictions[file] = {\"filename\": file, \"predictions\": data}", "\nevaluator = ClassificationEvaluator(\n    ground_truth=ground_truth,\n    predictions=all_predictions,\n    class_names=class_names,\n    mode=\"batch\",\n    model_type=\"multiclass\",\n)\n\ncf = evaluator.eval_model_predictions()", "\ncf = evaluator.eval_model_predictions()\n\ndata = evaluator.calculate_statistics()\n\nprint(\"Precision:\", data.precision)\nprint(\"Recall:\", data.recall)\nprint(\"f1 Score:\", data.f1)\n", ""]}
{"filename": "examples/dino_compare_example.py", "chunked_list": ["import argparse\nimport os\nimport numpy as np\n\nimport cv2\nfrom groundingdino.util.inference import Model\n\nimport supervision as sv\n\nfrom evaluations import CompareEvaluations, Evaluator", "\nfrom evaluations import CompareEvaluations, Evaluator\nfrom evaluations.dataloaders import RoboflowDataLoader\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--eval_data_path\",\n    type=str,\n    required=True,", "    type=str,\n    required=True,\n    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n)\nparser.add_argument(\n    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n)\nparser.add_argument(\n    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)", "    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n)\nparser.add_argument(\n    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n)\nparser.add_argument(\n    \"--config_path\", type=str, required=True, help=\"Path to GroundingDINO config\"\n)\nparser.add_argument(\n    \"--weights_path\", type=str, required=True, help=\"Path to GroundingDINO weights\"", "parser.add_argument(\n    \"--weights_path\", type=str, required=True, help=\"Path to GroundingDINO weights\"\n)\nparser.add_argument(\"--box_threshold\", type=float, required=False, help=\"Box threshold\")\nparser.add_argument(\n    \"--text_threshold\", type=float, required=False, help=\"Text threshold\"\n)\n\nargs = parser.parse_args()\n\nif not args.box_threshold:\n    BOX_THRESHOLD = 0.35", "args = parser.parse_args()\n\nif not args.box_threshold:\n    BOX_THRESHOLD = 0.35\n\nif not args.text_threshold:\n    TEXT_THRESHOLD = 0.25\n\nEVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url", "EVAL_DATA_PATH = args.eval_data_path\nROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\nROBOFLOW_PROJECT_URL = args.roboflow_project_url\nROBOFLOW_MODEL_VERSION = args.roboflow_model_version\nCONFIG_PATH = args.config_path\nWEIGHTS_PATH = args.weights_path\n\n# use validation set\nIMAGE_PATH = EVAL_DATA_PATH + \"/train/images\"\n", "IMAGE_PATH = EVAL_DATA_PATH + \"/train/images\"\n\nclass_names, ground_truth, model = RoboflowDataLoader(\n    workspace_url=ROBOFLOW_WORKSPACE_URL,\n    project_url=ROBOFLOW_PROJECT_URL,\n    project_version=ROBOFLOW_MODEL_VERSION,\n    image_files=EVAL_DATA_PATH,\n    model_type=\"object-detection\",\n    dataset=\"train\",\n).download_dataset()", "    dataset=\"train\",\n).download_dataset()\n\nground_truth = sv.DetectionDataset.from_yolo(\n    images_directory_path=os.path.join(EVAL_DATA_PATH, \"train/images\"),\n    annotations_directory_path=os.path.join(EVAL_DATA_PATH, \"train/labels\"),\n    data_yaml_path=os.path.join(EVAL_DATA_PATH, \"data.yaml\"),\n)\n\nmodel = Model(", "\nmodel = Model(\n    model_config_path=CONFIG_PATH, model_checkpoint_path=WEIGHTS_PATH, device=\"cpu\"\n)\n\ndef get_dino_predictions(class_names: dict) -> dict:\n    all_predictions = {}\n\n    class_names = [pred[\"inference\"] for pred in class_names]\n\n    for file in os.listdir(IMAGE_PATH):\n        file = os.path.join(IMAGE_PATH, file)\n        if file.endswith(\".jpg\"):\n            image = cv2.imread(file)\n\n            detections = model.predict_with_classes(\n                image=image,\n                classes=class_names,\n                box_threshold=BOX_THRESHOLD,\n                text_threshold=TEXT_THRESHOLD,\n            )\n\n            annotations = sv.detection.core.Detections(\n                xyxy=np.array(detections.xyxy).astype(np.float32),\n                class_id=np.array(detections.class_id),\n                confidence=np.array(detections.confidence)\n            )\n\n            all_predictions[file] = {\"filename\": file, \"predictions\": annotations}\n\n    return all_predictions", "\nevals = [\n    {\"classes\": [{\"ground_truth\": \"trash\", \"inference\": \"trash\"}], \"confidence\": 0.5},\n    {\"classes\": [{\"ground_truth\": \"trash\", \"inference\": \"rubbish\"}], \"confidence\": 0.5},\n    {\"classes\": [{\"ground_truth\": \"trash\", \"inference\": \"waste\"}], \"confidence\": 0.5},\n]\n\n# map eval inferences to ground truth\nfor e in evals:\n    evals[evals.index(e)][\"ground_truth\"] = ground_truth\n    evals[evals.index(e)][\"inference_class_names\"] = [e[\"classes\"][0][\"inference\"], \"background\"]", "for e in evals:\n    evals[evals.index(e)][\"ground_truth\"] = ground_truth\n    evals[evals.index(e)][\"inference_class_names\"] = [e[\"classes\"][0][\"inference\"], \"background\"]\n\nbest = CompareEvaluations(\n    [\n        Evaluator(\n            predictions=get_dino_predictions(class_names=cn[\"classes\"]),\n            ground_truth=cn[\"ground_truth\"].annotations,\n            confidence_threshold=cn[\"confidence\"],", "            ground_truth=cn[\"ground_truth\"].annotations,\n            confidence_threshold=cn[\"confidence\"],\n            class_names=cn[\"inference_class_names\"],\n            name=cn[\"classes\"][0][\"inference\"],\n            mode=\"batch\",\n            model_type=\"object-detection\"\n        )\n        for cn in evals\n    ]\n)", "    ]\n)\n\ncomparison = best.compare()\n\nprint(\"F1 Score:\", comparison[0])\nprint(\"Precision:\", comparison[1])\nprint(\"Recall:\", comparison[2])\nprint(\"Class Names:\", comparison[3])\nprint(\"Confidence Threshold:\", comparison[5])", "print(\"Class Names:\", comparison[3])\nprint(\"Confidence Threshold:\", comparison[5])"]}
{"filename": "examples/models/clip.py", "chunked_list": ["import os\n\nimport clip\nimport torch\nfrom PIL import Image\n\nfrom evaluations.dataloaders.classification import ClassificationDetections\n\n\ndef run_clip_inference(file: str, class_names: list) -> dict:\n    \"\"\"\n    Run inference on a single image using the CLIP model by OpenAI.\n\n    Args:\n\n        file (str): Path to the image file.\n        class_names (list): List of class names.\n\n    Returns:\n\n        dict: Dictionary containing the filename and the predictions.\n    \"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n    image = preprocess(Image.open(file)).unsqueeze(0).to(device)\n    text = clip.tokenize(class_names).to(device)\n\n    with torch.no_grad():\n        image_features = clip_model.encode_image(image)\n        text_features = clip_model.encode_text(text)\n\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n    values, indices = similarity[0].topk(1)\n\n    data = ClassificationDetections(\n        image_id=file,\n        predicted_class_names=[class_names[indices[0]]],\n        predicted_class_ids=[indices[0]],\n        confidence=values[0],\n    )\n\n    return {\"filename\": file, \"predictions\": data}", "\ndef run_clip_inference(file: str, class_names: list) -> dict:\n    \"\"\"\n    Run inference on a single image using the CLIP model by OpenAI.\n\n    Args:\n\n        file (str): Path to the image file.\n        class_names (list): List of class names.\n\n    Returns:\n\n        dict: Dictionary containing the filename and the predictions.\n    \"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n    image = preprocess(Image.open(file)).unsqueeze(0).to(device)\n    text = clip.tokenize(class_names).to(device)\n\n    with torch.no_grad():\n        image_features = clip_model.encode_image(image)\n        text_features = clip_model.encode_text(text)\n\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n    values, indices = similarity[0].topk(1)\n\n    data = ClassificationDetections(\n        image_id=file,\n        predicted_class_names=[class_names[indices[0]]],\n        predicted_class_ids=[indices[0]],\n        confidence=values[0],\n    )\n\n    return {\"filename\": file, \"predictions\": data}", ""]}
{"filename": "examples/models/blip.py", "chunked_list": ["import os\n\nimport torch\nfrom lavis.models import load_model_and_preprocess\nfrom PIL import Image\n\nfrom evaluations.dataloaders.classification import ClassificationDetections\n\nSUPPORTED_MODELS = (\"blip\", \"albef\")\n\ndef run_blip_albef_inference(file: str, class_names: list, model_type: str, device: str) -> dict:\n    \"\"\"\n    Run inference on a single image using the BLIP or ALBEF model by Salesforce.\n\n    Args:\n\n        file (str): Path to the image file.\n        class_names (list): List of class names.\n        model_type (str): Model type to use. Either \"blip\" or \"albef\".\n        device (str): Device to run the model on.\n\n    Returns:\n\n        dict: Dictionary containing the filename and the predictions.\n    \"\"\"\n    current_dir = os.getcwd()\n    file = os.path.join(current_dir, file)\n\n    if model_type not in SUPPORTED_MODELS:\n        raise ValueError(f\"Model type {model_type} is not supported by the run_blip_albef_inference function. Supported models are {SUPPORTED_MODELS}\")\n\n    raw_image = Image.open(\"./IMG_3322.jpeg\").convert(\"RGB\")\n\n    classes = [\"A picture of \" + c for c in class_names]\n\n    model, vis_processors, _ = load_model_and_preprocess(\n        name=model_type + \"_classification\",\n        model_type=\"base\",\n        is_eval=True,\n        device=device,\n    )\n\n    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n\n    sample = {\"image\": image, \"text_input\": classes}\n\n    image_features = model.extract_features(sample, mode=\"image\").image_embeds_proj[\n        :, 0\n    ]\n    text_features = model.extract_features(sample, mode=\"text\").text_embeds_proj[:, 0]\n\n    sims = (image_features @ text_features.t())[0] / model.temp\n    probs = torch.nn.Softmax(dim=0)(sims).tolist()\n\n    for cls_nm, prob in zip(classes, probs):\n        print(f\"{cls_nm}: \\t {prob:.3%}\")\n\n    top_prob = None\n    top_prob_confidence = -1\n\n    for cls_nm, prob in zip(classes, probs):\n        if prob > top_prob_confidence:\n            top_prob = cls_nm\n            top_prob_confidence = prob\n\n    top_prob = top_prob.lstrip(\"A picture of \")\n\n    data = ClassificationDetections(\n        image_id=file,\n        predicted_class_names=[class_names[top_prob[0]]],\n        predicted_class_ids=[top_prob[0]],\n        confidence=top_prob_confidence[0],\n    )\n\n    return {\"filename\": file, \"predictions\": data}", "SUPPORTED_MODELS = (\"blip\", \"albef\")\n\ndef run_blip_albef_inference(file: str, class_names: list, model_type: str, device: str) -> dict:\n    \"\"\"\n    Run inference on a single image using the BLIP or ALBEF model by Salesforce.\n\n    Args:\n\n        file (str): Path to the image file.\n        class_names (list): List of class names.\n        model_type (str): Model type to use. Either \"blip\" or \"albef\".\n        device (str): Device to run the model on.\n\n    Returns:\n\n        dict: Dictionary containing the filename and the predictions.\n    \"\"\"\n    current_dir = os.getcwd()\n    file = os.path.join(current_dir, file)\n\n    if model_type not in SUPPORTED_MODELS:\n        raise ValueError(f\"Model type {model_type} is not supported by the run_blip_albef_inference function. Supported models are {SUPPORTED_MODELS}\")\n\n    raw_image = Image.open(\"./IMG_3322.jpeg\").convert(\"RGB\")\n\n    classes = [\"A picture of \" + c for c in class_names]\n\n    model, vis_processors, _ = load_model_and_preprocess(\n        name=model_type + \"_classification\",\n        model_type=\"base\",\n        is_eval=True,\n        device=device,\n    )\n\n    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n\n    sample = {\"image\": image, \"text_input\": classes}\n\n    image_features = model.extract_features(sample, mode=\"image\").image_embeds_proj[\n        :, 0\n    ]\n    text_features = model.extract_features(sample, mode=\"text\").text_embeds_proj[:, 0]\n\n    sims = (image_features @ text_features.t())[0] / model.temp\n    probs = torch.nn.Softmax(dim=0)(sims).tolist()\n\n    for cls_nm, prob in zip(classes, probs):\n        print(f\"{cls_nm}: \\t {prob:.3%}\")\n\n    top_prob = None\n    top_prob_confidence = -1\n\n    for cls_nm, prob in zip(classes, probs):\n        if prob > top_prob_confidence:\n            top_prob = cls_nm\n            top_prob_confidence = prob\n\n    top_prob = top_prob.lstrip(\"A picture of \")\n\n    data = ClassificationDetections(\n        image_id=file,\n        predicted_class_names=[class_names[top_prob[0]]],\n        predicted_class_ids=[top_prob[0]],\n        confidence=top_prob_confidence[0],\n    )\n\n    return {\"filename\": file, \"predictions\": data}", ""]}
{"filename": "examples/models/dinov2.py", "chunked_list": ["import json\nimport os\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom sklearn import svm\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom evaluations.dataloaders.classification import ClassificationDetections\n\ncwd = os.getcwd()\n\nROOT_DIR = os.path.join(cwd, \"MIT-Indoor-Scene-Recognition-5/train\")\n\ndinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n", "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndinov2_vits14.to(device)\n\ntransform_image = T.Compose(\n    [T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])],\n)\n", ")\n\n\ndef load_image(img: str) -> torch.Tensor:\n    \"\"\"\n    Load an image and return a tensor that can be used as an input to DINOv2.\n    \"\"\"\n    img = Image.open(img)\n\n    transformed_img = transform_image(img)[:3].unsqueeze(0)\n\n    return transformed_img", "\n\ndef embed_image(img: str) -> torch.Tensor:\n    \"\"\"\n    Embed an image using DINOv2.\n    \"\"\"\n    with torch.no_grad():\n        return dinov2_vits14(load_image(img).to(device))\n\n\ndef compute_embeddings(files: list) -> dict:\n    \"\"\"\n    Create an index that contains all of the images in the specified list of files.\n    \"\"\"\n    all_embeddings = {}\n\n    with torch.no_grad():\n        for i, file in enumerate(tqdm(files)):\n            embeddings = dinov2_vits14(load_image(file).to(device))\n\n            all_embeddings[file] = (\n                np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()\n            )\n\n    with open(\"all_embeddings.json\", \"w\") as f:\n        f.write(json.dumps(all_embeddings))\n\n    return all_embeddings", "\n\ndef compute_embeddings(files: list) -> dict:\n    \"\"\"\n    Create an index that contains all of the images in the specified list of files.\n    \"\"\"\n    all_embeddings = {}\n\n    with torch.no_grad():\n        for i, file in enumerate(tqdm(files)):\n            embeddings = dinov2_vits14(load_image(file).to(device))\n\n            all_embeddings[file] = (\n                np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()\n            )\n\n    with open(\"all_embeddings.json\", \"w\") as f:\n        f.write(json.dumps(all_embeddings))\n\n    return all_embeddings", "\ndef train_dinov2_svm_model(training_dir):\n    \"\"\"\n    Train an SVM model using the embeddings from DINOv2.\n    \"\"\"\n    labels = {}\n\n    for folder in os.listdir(training_dir):\n        for file in os.listdir(os.path.join(training_dir, folder)):\n            if file.endswith(\".jpg\"):\n                full_name = os.path.join(training_dir, folder, file)\n                labels[full_name] = folder\n\n    files = labels.keys()\n\n    embeddings = compute_embeddings(files)\n\n    clf = svm.SVC(gamma=\"scale\")\n\n    y = [labels[file] for file in files]\n\n    embedding_list = list(embeddings.values())\n\n    clf.fit(np.array(embedding_list).reshape(-1, 384), y)\n\n    return clf", "\ndef run_dinov2_inference(model, image: str, class_names: list) -> dict:\n    \"\"\"\n    Run inference on a single image using the DINOv2 model.\n    \"\"\"\n\n    result = model.predict(embed_image(image))\n\n    data = ClassificationDetections(\n        image_id=image,\n        predicted_class_names=result,\n        predicted_class_ids=[class_names.index(result[0])],\n        confidence=1.0,\n    )\n\n    return {\"filename\": image, \"predictions\": data}"]}
{"filename": "examples/annotation_check/run_evaluator.py", "chunked_list": ["\nfrom annotation_eval import AnnotationEval\nimport argparse\n'''\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"\")\nproject = rf.workspace(\"\").project(\"\")\ndataset = project.version().download(\"voc\")\n'''\n", "'''\n\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--local_data_folder\",\n    type=str,\n    required=True,\n    help=\"Absolute path to local data folder of images and annotations \",", "    required=True,\n    help=\"Absolute path to local data folder of images and annotations \",\n)\n\nparser.add_argument(\n    \"--roboflow_data_folder\",\n    type=str,\n    required=True,\n    help=\"Absolute path to roboflow uploads of images and annotations \",\n)", "    help=\"Absolute path to roboflow uploads of images and annotations \",\n)\n\nargs = parser.parse_args()\n\nLOCAL_FOLDER = args.local_data_folder\nROBOFLOW_FOLDER = args.roboflow_data_folder\n\nif __name__ == \"__main__\":\n    eval = AnnotationEval(\n        local_folder = LOCAL_FOLDER,\n        roboflow_folder = ROBOFLOW_FOLDER)\n    eval.collect_images_labels()\n    eval.run_eval_loop()", "if __name__ == \"__main__\":\n    eval = AnnotationEval(\n        local_folder = LOCAL_FOLDER,\n        roboflow_folder = ROBOFLOW_FOLDER)\n    eval.collect_images_labels()\n    eval.run_eval_loop()\n"]}
{"filename": "examples/annotation_check/annotation_eval.py", "chunked_list": ["import json\nimport yaml\nimport pandas as pd\nimport os, glob\nimport yaml\nimport json\nimport csv\nimport xml.etree.ElementTree as ET\nfrom roboflow import Roboflow\nimport os", "from roboflow import Roboflow\nimport os\nimport argparse\n\n\nclass AnnotationEval():\n    def __init__(\n        self,\n        local_folder: str,\n        roboflow_folder: str\n    ):\n        \n        \"\"\"\n        Loads local images + labels and roboflow images + labels. \n        Checks that each label in each image locally matches the label in roboflow\n\n            Args:\n                local_folder (str): local folder\n                roboflow_folder (str): data downloaded/exported from roboflow.\n\n            Returns:\n                Similarity Score\n        \"\"\"\n        self.local_folder = local_folder\n        self.roboflow_folder = roboflow_folder\n    \n\n    def extract_text_after_last_slash(self,text):\n        last_slash_index = text.rfind('/')\n        if last_slash_index != -1:\n            text_after_last_slash = text[last_slash_index + 1:]\n            return text_after_last_slash\n        return None\n\n    def parse_xml(self,xml_string):\n        tree = ET.parse(xml_string)\n        root = tree.getroot()\n        bbox_dict = {}\n        # Find all <object> elements\n        object_elements = root.findall('.//object')\n        # Iterate over each <object> element\n        for i, object_element in enumerate(object_elements):\n            bbox = {}\n            # Extract bounding box coordinates\n            bndbox_element = object_element.find('bndbox')\n            bbox['xmin'] = round(float(bndbox_element.find('xmin').text))\n            bbox['xmax'] = round(float(bndbox_element.find('xmax').text))\n            bbox['ymin'] = round(float(bndbox_element.find('ymin').text))\n            bbox['ymax'] = round(float(bndbox_element.find('ymax').text))\n            # Extract name\n            name_element = object_element.find('name')\n            bbox['name'] = name_element.text\n            # Add bbox to the dictionary with counter as the key\n            bbox_dict[i] = bbox\n        return bbox_dict\n\n    def collect_images_labels(self):\n        \n        self.local_images = []\n        self.local_annotations = []\n        self.roboflow_annotations = []\n        \n        # Loop through each subfolder in image_folder\n        for root, dirs, files in os.walk(self.local_folder):\n            # Check if 'xml' folder exists in the current subfolder\n            if 'xml' in dirs:\n                xml_folder = os.path.join(root, 'xml')\n                # Get all XML files in xml_folder\n                #xml_files = [file for file in os.listdir(xml_folder) if file.endswith('.xml')]\n                xml_files = sorted(glob.glob(os.path.join(xml_folder, \"*.xml\")))\n                # Add the XML files to the master_list\n                self.local_annotations.extend(xml_files)\n\n        # Loop through each subfolder in image_folder\n        for root, dirs, files in os.walk(self.local_folder):\n            # Check if 'images' folder exists in the current subfolder\n            if 'images' in dirs:\n                images_folder = os.path.join(root, 'images')\n                # Get all JPEG files in the images_folder\n                #jpeg_files = [file for file in os.listdir(images_folder) if file.endswith('.jpg') or file.endswith('.jpg')]\n                jpeg_files = sorted(glob.glob(os.path.join(images_folder, \"*.jpg\")))\n\n\n                # Add the JPEG files to the local_images list\n                self.local_images.extend(jpeg_files)\n                \n        name_requirements = [\"train\",\"test\",\"valid\"]\n        # Loop through each subfolder in image_folder\n        for root, dirs, files in os.walk(self.roboflow_folder):\n            # Check if 'images' folder exists in the current subfolder\n            for folder in dirs:\n                # Check if the subfolder meets the name requirement\n                if any(req in folder for req in name_requirements):\n\n                    xml_path = os.path.join(root, folder)\n                    # Get all XML files in the subfolder\n                    #xml_files = [file for file in os.listdir(xml_path) if file.endswith('.xml')]\n                    xml_files = sorted(glob.glob(os.path.join(xml_path, \"*.xml\")))\n                    # Add the XML files to the local_images list\n                    self.roboflow_annotations.extend(xml_files)\n                    \n        # Print the local_images list\n        print('local image count',len(self.local_images))\n        print('local annotation count',len(self.local_annotations))\n        print('robfolow annotation count',len(self.roboflow_annotations))\n        return self.local_images,self.local_annotations,self.roboflow_annotations\n\n\n    def run_eval_loop(self):\n    \n        count = 0\n        loop_count = 0\n        roboflow_count = 0\n        match1 = 0\n        overall_accuracy = []\n        no_difference_count = 0\n        roboflow_key_count = 0\n        local_key_count = 0\n        key_match = 0\n\n        for image in self.local_images:\n            if count < len(self.local_images):\n                               \n                f = os.path.join(image)\n                image_hash = self.extract_text_after_last_slash(image.split(\".\")[0].replace(\"#\",\"-\"))\n\n                # split the image path to the hash\n                current_annotation = self.local_annotations[count]\n                annotation_hash = self.extract_text_after_last_slash(current_annotation.split(\".\")[0]).replace(\"#\",\"-\")\n                \n                if image_hash == annotation_hash:\n                \n                    match1 +=1 \n                \n                    for roboflow_annotation in self.roboflow_annotations:\n                        #Roboflow labels and hash\n                        roboflow_hash = ((roboflow_annotation.split(\"/\"))[-1].split('.')[0][:-4])\n                        \n                        if roboflow_hash == image_hash:\n                        \n                            roboflow_count +=1\n                                \n                            local_parsed = self.parse_xml(current_annotation)\n                            roboflow_parsed = self.parse_xml(roboflow_annotation)\n                            \n                            label_count_local = len(local_parsed)\n                            roboflow_count_local = len(roboflow_parsed)\n                            local_key_count += label_count_local\n            \n                            for key in local_parsed: \n                                \n                                if key in roboflow_parsed:\n                                    roboflow_key_count+=1\n                                    difference = 0\n                                    for sub_key in local_parsed[key]:\n                                        if sub_key in roboflow_parsed[key] and type(local_parsed[key][sub_key]) == int and (local_parsed[key][sub_key]-local_parsed[key][sub_key]) >1:\n                                            difference += (local_parsed[key][sub_key] - roboflow_parsed[key][sub_key])\n                                        if type(local_parsed[key][sub_key]) == str:\n                                            if local_parsed[key][sub_key] != roboflow_parsed[key][sub_key]:\n                                                difference += 1\n                                    if difference <=1:\n                                        no_difference_count +=1\n                                \n                                    elif difference >1:\n                                        print('PIXEL MISMATCH')\n                                        print(image_hash,annotation_hash,roboflow_hash)\n                                        print(difference)\n                    \n             \n                    count+=1\n\n                    if loop_count > len(self.local_images)*len(self.local_annotations):\n                        break\n                    \n        print('\\n')\n        print('KEY_MATCH %',str((roboflow_key_count/local_key_count)*100)+'%')\n        print('\\n')\n        print('LABEL SIMILARITY %',str((no_difference_count/roboflow_key_count)*100)+'%')\n        print('\\n')\n        print('TOTAL LABELS',local_key_count)\n        print('\\n')\n        print('TOTAL IMAGE MATCH',match1)", ""]}
{"filename": "evaluations/roboflow.py", "chunked_list": ["from .evaluator import Evaluator\n\n\nclass RoboflowEvaluator(Evaluator):\n    \"\"\"\n    Evaluate models hosted on the Roboflow platform.\n    \"\"\"\n\n    pass\n", ""]}
{"filename": "evaluations/compare.py", "chunked_list": ["from tabulate import tabulate\n\nSUPPORTED_COMPARATORS = [\"f1\", \"precision\", \"recall\"]\n\n\nclass CompareEvaluations:\n    \"\"\"\n    Compare multiple evaluations and return the best one.\n    \"\"\"\n\n    def __init__(self, evaluations, comparator: str = \"f1\"):\n        self.evaluations = evaluations\n        self.comparator = comparator\n\n    def compare(self):\n        \"\"\"\n        Compare the evaluations and return the best one according to the specified comparator.\n        \"\"\"\n        highest = -1\n        highest_eval = None\n\n        evaluations = []\n\n        for evaluation in self.evaluations:\n            cf = evaluation.eval_model_predictions()\n\n            cf = evaluation.combined_cf\n\n            data = evaluation.calculate_statistics()\n\n            results = (\n                data.f1,\n                data.precision,\n                data.recall,\n                evaluation.class_names,\n                cf,\n                evaluation.confidence_threshold,\n                evaluation.name,\n            )\n\n            if self.comparator not in SUPPORTED_COMPARATORS:\n                raise Exception(\n                    f\"Comparator {self.comparator} not supported. Please use one of {SUPPORTED_COMPARATORS}.\"\n                )\n\n            if self.comparator == \"f1\":\n                if data.f1 > highest:\n                    highest = data.f1\n                    highest_eval = results\n            elif self.comparator == \"precision\":\n                if data.precision > highest:\n                    highest = data.precision\n                    highest_eval = results\n            elif self.comparator == \"recall\":\n                if data.recall > highest:\n                    highest = data.recall\n                    highest_eval = results\n\n            evaluations.append(results)\n\n        table = tabulate(\n            [\n                [\n                    \"F1\",\n                    \"Precision\",\n                    \"Recall\",\n                    \"Class Names\",\n                    \"Confusion Matrix\",\n                    \"Confidence\",\n                    \"Name\",\n                ]\n            ]\n            + evaluations,\n            headers=\"firstrow\",\n            tablefmt=\"fancy_grid\",\n        )\n\n        print(table)\n\n        return highest_eval, evaluations", ""]}
{"filename": "evaluations/classification.py", "chunked_list": ["from .evaluator import Evaluator\n\n\nclass ClassificationEvaluator(Evaluator):\n    \"\"\"\n    Evaluate classification models.\n    \"\"\"\n\n    def compute_confusion_matrix(self, result: dict, class_names: list) -> dict:\n        \"\"\"\n        Compute a confusion matrix for a classification model.\n\n        Args:\n            result (dict): A dictionary containing the ground truth and predictions.\n            class_names (list): A list of class names.\n\n        \"\"\"\n\n        confusion_data = {}\n\n        # matrix is\n        # [[tp, fp]]\n\n        for i, _ in enumerate(class_names):\n            for j, _ in enumerate(class_names):\n                confusion_data[(i, j)] = 0\n\n        if self.model_type == \"multiclass\":\n            for i, _ in enumerate(result[0]):\n                if result[0][i] in result[1].predicted_class_names:\n                    r0index = class_names.index(result[0][i])\n                    r1index = class_names.index(result[0][i])\n                    confusion_data[(r0index, r1index)] += 1\n                else:\n                    r0index = class_names.index(result[0][i])\n                    r1index = class_names.index(result[1].predicted_class_names[i])\n                    confusion_data[(r0index, r1index)] += 1\n        else:\n            is_match = result[0][0] == result[1].predicted_class_names[0]\n\n            if is_match:\n                r0index = class_names.index(result[0][0])\n                confusion_data[(r0index, r0index)] += 1\n            else:\n                r0index = class_names.index(result[0][0])\n                r1index = class_names.index(result[1].predicted_class_names[0])\n                confusion_data[(r0index, r1index)] += 1\n\n        return confusion_data", ""]}
{"filename": "evaluations/__init__.py", "chunked_list": ["from .compare import CompareEvaluations\nfrom .evaluator import Evaluator\n\n__version__ = \"0.1.0\"\n\n__all__ = [\"Evaluator\", \"CompareEvaluations\"]\n"]}
{"filename": "evaluations/confusion_matrices.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef plot_confusion_matrix(\n    image_eval_results: dict,\n    class_names: list,\n    aggregate: bool = False,\n    file_name: str = \"\",\n    mode: str = \"interactive\",\n) -> None:\n    \"\"\"\n    Plot an aggregate confusion matrix showing the results of the evaluation.\n    \"\"\"\n    confusion = []\n\n    # get base name\n    file_name = file_name.split(\"/\")[-1]\n\n    for x in range(len(class_names)):\n        row = []\n        for y in range(len(class_names)):\n            row.append(image_eval_results[(x, y)])\n        confusion.append(row)\n\n    fig = plt.figure(figsize=(10, 10))\n\n    plt.title(\"Confusion Matrix for \" + file_name)\n\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n\n    if aggregate:\n        plt.title(\"Confusion Matrix (Aggregated)\")\n\n    heatmap = sns.heatmap(\n        confusion, annot=True, xticklabels=class_names, yticklabels=class_names, fmt=\"g\"\n    )\n\n    # axis names\n    heatmap.set_xlabel(\"Predicted\")\n    heatmap.set_ylabel(\"Actual\")\n\n    if mode == \"interactive\":\n        plt.title(file_name)\n        plt.show()\n\n    # save to ./output/matrices\n    plt.savefig(\"./output/matrices/\" + file_name + \".png\")\n\n    plt.close(fig)", ""]}
{"filename": "evaluations/eval_images.py", "chunked_list": ["import cv2\nimport numpy as np\n\n# from supervision.metrics.iou import box_iou\nfrom .iou import box_iou\n\n\ndef find_best_prediction(\n    gt_box: list, predictions: list, iou_threshold: int = 0.5\n) -> tuple:\n    \"\"\"\n    Given ground truth data and associated predictions, find the prediction with the highest IOU.\n\n    Args:\n        gt_box (list): Ground truth box coordinates.\n        predictions (list): A list of model predictions.\n        iou_threshold (list): The Intersection Over Union threshold above which the ground truth and top prediction must be to be returned.\n    \"\"\"\n    if len(predictions) == 0:\n        return None, None\n\n    ious = [box_iou(gt_box, pred_box) for pred_box in predictions]\n\n    selected_pred_idx = np.argmax(ious)\n    selected_pred = predictions[selected_pred_idx]\n\n    if ious[selected_pred_idx] > iou_threshold:\n        return [selected_pred_idx, selected_pred]\n    else:\n        return None, None", "\n\ndef draw_bounding_boxes(\n    image: np.ndarray, ground_truth: list, predictions: list, class_names: list\n) -> np.ndarray:\n    \"\"\"\n    Draw bounding boxes on an image to show the ground truth and predictions.\n\n    Args:\n        image (np.ndarray): The image to draw bounding boxes on.\n        ground_truth (list): A list of ground truth bounding boxes.\n        predictions (list): A list of predicted bounding boxes.\n        class_names (list): A list of class names.\n\n    Returns:\n        np.ndarray: The image with bounding boxes drawn on it.\n    \"\"\"\n    output_image = image.copy()\n\n    # Draw ground truth bounding boxes with green color\n    for x0, y0, x1, y1, label in ground_truth:\n        cv2.rectangle(output_image, (x0, y0), (x1, y1), (0, 255, 0), 2)\n        cv2.putText(\n            output_image,\n            str(label),\n            (x0, y0 - 5),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (0, 255, 255),\n            2,\n        )\n\n    # Draw predicted bounding boxes with red color\n    for pred in range(len(predictions.xyxy)):\n        prediction = predictions.xyxy[pred].tolist()\n        x0, y0, x1, y1 = prediction\n        class_name = class_names[int(predictions.class_id[pred])]\n        # scale to image\n        cv2.rectangle(output_image, (x0, y0), (x1, y1), (0, 0, 255), 2)\n        cv2.putText(\n            output_image,\n            str(class_name),\n            (x0, y0 - 5),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (255, 0, 255),\n            2,\n        )\n\n    # show\n    cv2.imshow(\"image\", output_image)\n\n    return output_image", "\n\ndef visualize_image_experiment(img_eval_data: dict, class_names: list) -> None:\n    \"\"\"\n    Show the ground truth and predictions of an image using OpenCV.\n\n    Args:\n        img_eval_data: Evaluation data with ground truth and predictions.\n        class_names: The list of class names.\n\n    Returns:\n        None\n    \"\"\"\n\n    img_filename = img_eval_data[\"filename\"]\n    predictions = img_eval_data[\"predictions\"]\n    ground_truth = img_eval_data[\"ground_truth\"]\n\n    print(predictions)\n\n    image = cv2.imread(img_filename)\n\n    # output_image = draw_bounding_boxes(image, ground_truth, predictions, class_names)\n\n    base_file_name = img_filename.split(\"/\")[-1]\n\n    # save to ./output/images\n\n    print(f\"Saving image to ./output/images/{base_file_name}\")", "\n    # cv2.imwrite(f\"./output/images/{base_file_name}\", output_image)\n"]}
{"filename": "evaluations/iou.py", "chunked_list": ["def box_iou(boxA: list, boxB: list) -> float:\n    \"\"\"\n    Determine the Intersection over Union (IoU) of two bounding boxes.\n\n    Args:\n\n        :boxA (list): First bounding box\n        :boxB (list): Second bounding box\n    \"\"\"\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    # compute the area of intersection rectangle\n    interArea = (xB - xA) * (yB - yA)\n\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n\n    # return the intersection over union value\n    return iou", ""]}
{"filename": "evaluations/segmentation.py", "chunked_list": ["import numpy as np\nfrom .evaluator import Evaluator\nfrom .confusion_matrices import plot_confusion_matrix\nimport cv2\n\nclass SegmentationEvaluator(Evaluator):\n    def __init__(self, ground_truth, predictions, class_names, eval_data_path, dataset = \"test\") -> None:\n        self.ground_truth = ground_truth\n        self.predictions = predictions\n        self.class_names = class_names\n        self.eval_data_path = eval_data_path\n        self.dataset = dataset\n\n    def eval_model_predictions(self) -> dict:\n        confusion_matrices = {}\n\n        class_names = self.class_names\n        predictions = self.predictions\n\n        combined_cf = {\"fn\": 0, \"fp\": 0}\n        combined_cf = {}\n\n        for i, _ in enumerate(class_names):\n            for j, _ in enumerate(class_names):\n                combined_cf[(i, j)] = 0\n\n        for image_name, prediction in self.ground_truth.annotations.items():\n            mask = prediction.mask\n\n            image_name = self.eval_data_path + \"/test/images/\" + image_name\n\n            best_mask = 0\n            best_iou = 0\n\n            cf = {}\n\n            for i, _ in enumerate(class_names):\n                for j, _ in enumerate(class_names):\n                    cf[(i, j)] = 0\n\n            if image_name not in predictions:\n                continue\n\n            for pred in predictions[image_name][\"predictions\"]:\n                class_id = pred[3]\n                pred = pred[1]\n\n                # show two masks on image\n                image = cv2.imread(image_name)\n\n                # convert img to 3 channels\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n                # convert mask to 1 channel\n                mask = mask.astype(np.uint8)\n\n                for m in mask:\n                    intersection = np.logical_and(m, pred)\n                    union = np.logical_or(m, pred)\n                    iou = np.sum(intersection) / np.sum(union)\n\n                    if iou > best_iou:\n                        best_iou = iou\n                        best_mask = class_id\n\n            # if best mask has iou > threshold, add to confusion matrix\n            if best_iou > 0.5:\n                cf[(best_mask, best_mask)] += 1\n            else:\n                background_class_id = class_names.index(\"background\")\n                cf[(background_class_id, background_class_id)] += 1\n\n            \n            for i, _ in enumerate(class_names):\n                for j, _ in enumerate(class_names):\n                    combined_cf[(i, j)] += cf[(i, j)]\n\n            confusion_matrices[image_name] = cf\n\n            plot_confusion_matrix(cf, self.class_names, False, key, self.mode)\n\n        plot_confusion_matrix(\n            combined_cf, self.class_names, True, \"aggregate\", self.mode\n        )\n\n        self.combined_cf = combined_cf\n\n        return combined_cf"]}
{"filename": "evaluations/evaluator.py", "chunked_list": ["import copy\nimport os\nfrom dataclasses import dataclass\n\nfrom .confusion_matrices import plot_confusion_matrix\nfrom .eval_images import find_best_prediction, visualize_image_experiment\n\nACCEPTED_MODES = [\"interactive\", \"batch\"]\n\n", "\n\n@dataclass\nclass EvaluatorResponse:\n    true_positives: int\n    false_positives: int\n    false_negatives: int\n    precision: float\n    recall: float\n    f1: float", "\n\nclass Evaluator:\n    \"\"\"\n    Evaluates the output of a model on a dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        predictions: dict = {},\n        ground_truth: dict = {},\n        class_names: list = [],\n        confidence_threshold: int = 0.2,\n        mode: str = \"interactive\",\n        name: str = \"\",\n        model_type: str = \"\",\n    ) -> None:\n        if mode not in ACCEPTED_MODES:\n            raise ValueError(f\"mode must be one of {ACCEPTED_MODES}\")\n\n        if model_type not in [\"multiclass\", \"classification\", \"object-detection\", \"segmentation\"]:\n            raise ValueError(\n                f\"model_type must be one of ['multiclass', 'classification', 'object-detection', 'segmentation']\"\n            )\n\n        self.confidence_threshold = confidence_threshold\n        self.mode = mode\n        self.class_names = class_names\n        self.name = name\n        self.model_type = model_type\n\n        if not os.path.exists(\"./output/images\"):\n            os.makedirs(\"./output/images\")\n\n        if not os.path.exists(\"./output/matrices\"):\n            os.makedirs(\"./output/matrices\")\n\n        merged_data = {}\n\n        for key in predictions.keys():\n            gt = ground_truth[key.split(\"/\")[-1]]\n\n            gts = [list(item) for item in gt.xyxy]\n\n            for idx, item in enumerate(gt.class_id):\n                gts[idx].append(item)\n\n            merged_data[key] = {\n                \"ground_truth\": gts,\n                \"predictions\": predictions[key][\"predictions\"],\n                \"filename\": key,\n            }\n\n        self.data = merged_data\n\n    def eval_model_predictions(self) -> dict:\n        \"\"\"\n        Compute confusion matrices for a Roboflow, Grounding DINO, and CLIP model.\n\n        Returns:\n            combined_cf (dict): A dictionary with a confusion matrix composed of the result of all inferences.\n        \"\"\"\n\n        confusion_matrices = {}\n\n        combined_cf = {\"fn\": 0, \"fp\": 0}\n        combined_cf = {}\n\n        for i, _ in enumerate(self.class_names):\n            for j, _ in enumerate(self.class_names):\n                combined_cf[(i, j)] = 0\n\n        for key, value in self.data.items():\n            print(\n                \"evaluating image predictions against ground truth\",\n                key,\n                \"...\",\n                self.class_names,\n            )\n\n            gt = value[\"ground_truth\"]\n            pred = value[\"predictions\"]\n\n            cf = self.compute_confusion_matrix([gt, pred], self.class_names)\n\n            for k, v in cf.items():\n                combined_cf[k] += v\n\n            confusion_matrices[key] = cf\n\n            if self.model_type == \"object-detection\":\n                visualize_image_experiment(value, self.class_names)\n\n            plot_confusion_matrix(cf, self.class_names, False, key, self.mode)\n\n        plot_confusion_matrix(\n            combined_cf, self.class_names, True, \"aggregate\", self.mode\n        )\n\n        self.combined_cf = combined_cf\n\n        return combined_cf\n\n    def compute_confusion_matrix(\n        self,\n        image_eval_data,\n        class_names,\n    ) -> dict:\n        # image eval data looks like:\n        # {filename: \"path/to/image.jpg\", ground_truth: [{}, {}, {}, ...], predictions: [{},{},{},...]}\n        # each ground truth is a tuple/list of (x_min, y_min, x_max, y_max, label)\n        # each prediction is a tuple/list of (x_min, y_min, x_max, y_max, label, confidence)\n\n        predictions = copy.deepcopy(image_eval_data[1])\n\n        all_predictions = []\n\n        for i in range(len(predictions)):\n            if (\n                predictions.confidence[i] > self.confidence_threshold\n                and predictions.class_id[i] is not None\n            ):\n                merged_prediction = predictions.xyxy[i].tolist() + [\n                    predictions.class_id[i]\n                ]\n\n                all_predictions.append(merged_prediction)\n\n        ground_truths = copy.deepcopy(image_eval_data[0])\n\n        confusion_data = {}\n        for i, _ in enumerate(class_names):\n            for j, _ in enumerate(class_names):\n                confusion_data[(i, j)] = 0\n\n        for gt_box in ground_truths:\n            match_idx, match = find_best_prediction(gt_box, all_predictions)\n\n            if match is None:\n                confusion_data[gt_box[4], len(class_names) - 1] += 1\n            else:\n                all_predictions.pop(match_idx)\n                confusion_data[gt_box[4], match[4]] += 1\n\n        for p in all_predictions:\n            confusion_data[len(class_names) - 1, p[4]] += 1\n\n        return confusion_data\n\n    def calculate_statistics(self) -> tuple:\n        \"\"\"\n        Calculate precision, recall, and f1 score for the evaluation.\n\n        Returns:\n            precision: The precision of the model\n            recall: The recall of the the model\n            f1: The f1 score of the model\n        \"\"\"\n        cf = self.combined_cf\n\n        # compute precision, recall, and f1 score\n        tp = 0\n        fp = 0\n        fn = 0\n\n        for x in range(len(self.class_names)):  # ground truth\n            for y in range(len(self.class_names)):  # predictions\n                if (\n                    x == len(self.class_names) - 1\n                ):  # last column / prediction with no ground truth\n                    fp += cf[(x, y)]\n                elif (\n                    y == len(self.class_names) - 1\n                ):  # bottom row / ground truth with no prediction\n                    fn += cf[(x, y)]\n                elif x == y:  # true positives across the diagonal\n                    tp += cf[(x, y)]\n                else:  # misclassification\n                    fp += cf[(x, y)]\n\n        if tp + fp == 0:\n            precision = 1\n        else:\n            precision = tp / (tp + fp)\n\n        if tp + fn == 0:\n            recall = 1\n        else:\n            recall = tp / (tp + fn)\n\n        if precision + recall == 0:\n            f1 = 0\n        else:\n            f1 = 2 * (precision * recall) / (precision + recall)\n\n        return EvaluatorResponse(\n            true_positives=tp,\n            false_positives=fp,\n            false_negatives=fn,\n            precision=precision,\n            recall=recall,\n            f1=f1,\n        )", ""]}
{"filename": "evaluations/dataloaders/yolov5.py", "chunked_list": ["import cv2\n\ndef get_ground_truth_for_image(img_filename):\n    labels = []\n    masks = []\n    label_file = img_filename.replace(\"/images/\", \"/labels/\").replace(\".jpg\", \".txt\")\n    with open(label_file, \"r\") as file:\n        for line in file:\n            # Split the line into a list of values and convert them to floats\n            values = list(map(float, line.strip().split()))\n            print(values)\n            if len(values) > 5:\n                # normalize to bbox\n                max_x = max(values[1::2])\n                min_x = min(values[1::2])\n                max_y = max(values[2::2])\n                min_y = min(values[2::2])\n\n                cx = (max_x + min_x) / 2\n                cy = (max_y + min_y) / 2\n                width = max_x - min_x\n                height = max_y - min_y\n                label = values[0]\n            else:\n                # Extract the label, and scale the coordinates and dimensions\n                label = int(values[0])\n                cx = values[1]\n                cy = values[2]\n                width = values[3]\n                height = values[4]\n\n            image = cv2.imread(img_filename)\n\n            x0 = cx - width / 2\n            y0 = cy - height / 2\n            x1 = cx + width / 2\n            y1 = cy + height / 2\n\n            # scale to non-floats\n            x0 = int(x0 * image.shape[1])\n            y0 = int(y0 * image.shape[0])\n            x1 = int(x1 * image.shape[1])\n            y1 = int(y1 * image.shape[0])\n\n            # Add the extracted data to the output list\n            labels.append((x0, y0, x1, y1, label))\n\n            if len(values) > 5:\n                # Extract the mask\n                mask = values[1:]\n\n                # Add the mask to the output list\n                masks.append(mask)\n\n    return labels, masks", ""]}
{"filename": "evaluations/dataloaders/roboflow.py", "chunked_list": ["import csv\nimport os\n\nimport cv2\nimport numpy as np\nimport roboflow\nimport yaml\nfrom supervision.detection.core import Detections\n\nfrom .dataloader import DataLoader", "\nfrom .dataloader import DataLoader\nfrom .yolov5 import get_ground_truth_for_image\n\n\nclass RoboflowDataLoader(DataLoader):\n    def __init__(\n        self,\n        workspace_url: str,\n        project_url: str,\n        project_version: int,\n        image_files: str,\n        model_type: str = \"object-detection\",\n        dataset: str = \"test\",\n    ):\n        \"\"\"\n        Load a dataset from Roboflow. Saves the result to ./dataset/\n\n        Args:\n            workspace_url (str): The Roboflow workspace URL\n            project_url (str): The Roboflow project URL\n            project_version (int): The Roboflow project version\n            model_type (str): The model type. Either \"object-detection\" or \"classification\"\n\n        Returns:\n            None\n        \"\"\"\n        self.workspace_url = workspace_url\n        self.project_url = project_url\n        self.project_version = project_version\n        self.model_type = model_type\n        self.data = {}\n        self.image_files = image_files\n        self.dataset = dataset\n\n        self.model = None\n        self.dataset_version = None\n\n    def download_dataset(self) -> None:\n        \"\"\"\n        Download a dataset from Roboflow. Saves the result to ./dataset/\n\n        Returns:\n            None\n        \"\"\"\n\n        roboflow.login()\n        rf = roboflow.Roboflow()\n\n        self.data = {}\n\n        project = rf.workspace(self.workspace_url).project(self.project_url)\n\n        self.dataset_version = project.version(self.project_version)\n        self.dataset_content = self.dataset_version\n        self.model = project.version(self.project_version).model\n\n        if self.model_type == \"classification\":\n            data_format = \"folder\"\n        elif self.model_type == \"multiclass\":\n            data_format = \"multiclass\"\n        elif self.model_type == \"object-detection\":\n            data_format = \"yolov5\"\n        elif self.model_type == \"segmentation\":\n            data_format = \"yolov5\"\n        else:\n            raise ValueError(\"Model type not supported\")\n\n        root_path = self.image_files\n\n        # download if needed\n        if not os.path.exists(root_path):\n            self.dataset_version.download(data_format, root_path)\n\n        if data_format == \"yolov5\":\n            yaml_data = os.path.join(root_path, \"data.yaml\")\n            if os.path.exists(yaml_data):\n                # load class names map\n                with open(yaml_data, \"r\") as file:\n                    dataset_yaml = yaml.safe_load(file)\n                    self.class_names = [\n                        i.replace(\"-\", \" \") for i in dataset_yaml[\"names\"]\n                    ]\n        elif data_format == \"multiclass\":\n            with open(os.path.join(root_path, \"valid/\", \"_classes.csv\")) as f:\n                reader = csv.reader(f)\n                results = list(reader)\n\n                class_names = results[0]\n\n                # first item will be \"filename\", so we need to remove it\n                self.class_names = [c.strip() for c in class_names][1:]\n\n                self.class_names.append(\"background\")\n\n                for row in results[1:]:\n                    self.data[os.path.join(root_path, \"valid/\", row[0])] = {\n                        \"filename\": os.path.join(root_path, \"valid/\", row[0]),\n                        \"predictions\": [],\n                        \"ground_truth\": [\n                            self.class_names[c - 1].strip()\n                            for c in range(1, len(row))\n                            if row[c].strip() == \"1\"\n                        ],\n                    }\n\n                return self.class_names, self.data, self.model\n        else:\n            # class names are folder names in test/\n            self.class_names = [\n                name\n                for name in os.listdir(os.path.join(root_path, \"valid\"))\n                if os.path.isdir(os.path.join(root_path, \"valid\", name))\n            ]\n\n        for root, dirs, files in os.walk(\n            self.image_files.rstrip(\"/\") + f\"/{self.dataset}/\"\n        ):\n            for file in files:\n                if file.endswith(\".jpg\"):\n                    if self.model_type == \"object-detection\" or self.model_type == \"segmentation\":\n                        ground_truth, masks = get_ground_truth_for_image(\n                            os.path.join(root, file)\n                        )\n                        if masks != []:\n                            ground_truth = masks\n                    else:\n                        # folder name\n                        ground_truth = [os.path.basename(root)]\n\n                    self.data[os.path.join(root, file)] = {\n                        \"filename\": os.path.join(root, file),\n                        \"predictions\": [],\n                        \"ground_truth\": ground_truth,\n                    }\n\n        self.class_names.append(\"background\")\n\n        return self.class_names, self.data, self.model", "\n\nclass RoboflowPredictionsDataLoader(DataLoader):\n    def __init__(self, model, model_type, image_files, class_names):\n        self.model = model\n        self.model_type = model_type\n        self.image_files = image_files\n        self.data = {}\n        self.class_names = class_names\n\n    def get_predictions_for_image(self, img_filename: str) -> list:\n        \"\"\"\n        Retrieve predictions for a Roboflow, Grounding DINO, and CLIP model for a single image.\n\n        Args:\n            img_filename (str): Path to image file\n\n        Returns:\n            predictions (list): List of predictions\n        \"\"\"\n\n        prediction_group = self.model.predict(img_filename)\n\n        image_dimensions = prediction_group.image_dims\n\n        width, height = float(image_dimensions[\"width\"]), float(\n            image_dimensions[\"height\"]\n        )\n        prediction_group = [p.json() for p in prediction_group.predictions]\n\n        predictions = []\n        class_names = []\n        confidence = []\n        masks = []\n\n        for p in prediction_group:\n            # scale predictions to 0 to 1\n            cx = p[\"x\"] / width\n            cy = p[\"y\"] / height\n            w = p[\"width\"] / width\n            h = p[\"height\"] / height\n\n            x0 = cx - w / 2\n            y0 = cy - h / 2\n            x1 = cx + w / 2\n            y1 = cy + h / 2\n\n            # multiply by image dimensions to get pixel values\n            x0 = int(x0 * width)\n            y0 = int(y0 * height)\n            x1 = int(x1 * width)\n            y1 = int(y1 * height)\n\n            if p.get(\"points\"):\n                # points are {x, y} pairs, turn into mask\n                points = p[\"points\"]\n\n                # print(points)\n\n                mask = np.zeros((int(height), int(width)), dtype=np.uint8)\n                points = np.array(\n                    [(p[\"x\"], p[\"y\"]) for p in points], dtype=np.int32\n                ).reshape((-1, 1, 2))\n\n                # entire points should be filled\n\n                cv2.fillPoly(mask, [points], 1)\n\n                # should be T or F\n                mask = mask.astype(bool)\n\n                # print # of True bools\n                print(np.count_nonzero(mask))\n                # cv2.imshow(\"mask\", mask.astype(np.uint8) * 255)\n                # if cv2.waitKey(0) == ord(\"q\"):\n                #     break\n\n                masks.append(mask)\n\n            predictions.append(\n                (\n                    x0,\n                    y0,\n                    x1,\n                    y1,\n                )\n            )\n            class_names.append(self.class_names.index(p[\"class\"]))\n            confidence.append(p[\"confidence\"])\n\n        if len(predictions) == 0:\n            return Detections.empty()\n\n        predictions = Detections(\n            xyxy=np.array(predictions),\n            class_id=np.array(class_names),\n            mask=np.array(masks),\n            confidence=np.array(confidence),\n        )\n\n        return predictions", ""]}
{"filename": "evaluations/dataloaders/dataloader.py", "chunked_list": ["import os\n\n\nclass DataLoader:\n    def __init__(self):\n        self.eval_data = {}\n        self.data = {}\n        self.image_files = None\n\n    def process_files(self) -> None:\n        \"\"\"\n        Process all input files in the dataset.\n\n        Returns:\n            None\n        \"\"\"\n\n        for root, dirs, files in os.walk(self.image_files.rstrip(\"/\") + \"/test/\"):\n            for file in files:\n                if file.endswith(\".jpg\"):\n                    file_name = os.path.join(root, file)\n                    if file_name not in self.data:\n                        self.data[file_name] = {}\n\n                    self.data[file_name][\n                        \"predictions\"\n                    ] = self.get_predictions_for_image(file_name)\n\n        return self.data", ""]}
{"filename": "evaluations/dataloaders/classification.py", "chunked_list": ["from dataclasses import dataclass\n\n\n@dataclass\nclass ClassificationDetections:\n    image_id: str\n    predicted_class_names: list\n    predicted_class_ids: list\n    confidence: float\n", ""]}
{"filename": "evaluations/dataloaders/cliploader.py", "chunked_list": ["import glob\nimport os\n\nimport clip\nimport torch\nfrom PIL import Image\n\nfrom ..evaluator import Evaluator\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"", "\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\nclass CLIPDataLoader(Evaluator):\n    \"\"\"\n    Evaluate CLIP prompts for classification tasks.\n    \"\"\"\n\n    def __init__(self, eval_data_path, class_names, data):\n        self.eval_data_path = eval_data_path\n        self.class_names = class_names\n        self.data = data\n\n        self.load_model()\n\n    def load_model(self):\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n        self.clip_model = clip_model\n        self.preprocess = preprocess\n\n    def get_ground_truth_for_image(self, img_filename: str) -> list:\n        \"\"\"\n        Get the ground truth for image in an object detection dataset.\n\n        Args:\n            img_filename (str): Path to image file\n\n        Returns:\n            list: List of ground truth labels\n        \"\"\"\n        return self.class_names\n\n    def run_clip_inference(self, filename: str) -> tuple:\n        \"\"\"\n        Run inference on an image using CLIP.\n\n        Args:\n            filename (str): path to image file\n\n        Returns:\n            top (str): Top prediction from CLIP\n            top_rf (str): Top prediction from roboflow\n        \"\"\"\n\n        image = self.preprocess(Image.open(filename)).unsqueeze(0).to(device)\n        text = clip.tokenize(self.class_names).to(device)\n\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(image)\n            text_features = self.clip_model.encode_text(text)\n\n        image_features /= image_features.norm(dim=-1, keepdim=True)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\n        _, indices = similarity[0].topk(1)\n\n        top = self.class_names[indices[0]]\n\n        return top\n\n    def get_predictions_for_image(self, img_filename: str) -> str:\n        \"\"\"\n        Retrieve predictions for a Roboflow, Grounding DINO, and CLIP model for a single image.\n\n        Args:\n            img_filename (str): Path to image file\n\n        Returns:\n            predictions (list): List of predictions\n        \"\"\"\n\n        label = self.run_clip_inference(img_filename)\n\n        return label\n\n    def process_files(self) -> None:\n        \"\"\"\n        Process all input files in the dataset.\n\n        Returns:\n            None\n        \"\"\"\n\n        for root, dirs, files in os.walk(self.eval_data_path):\n            for file in files:\n                if file.endswith(\".jpg\"):\n                    file_name = os.path.join(root, file)\n\n                    if file_name not in self.data:\n                        self.data[file_name] = {}\n\n                    self.data[file_name][\n                        \"predictions\"\n                    ] = self.get_predictions_for_image(file_name)\n                    self.data[file_name][\n                        \"ground_truth\"\n                    ] = self.get_ground_truth_for_image(file_name)\n\n        return self.data", ""]}
{"filename": "evaluations/dataloaders/__init__.py", "chunked_list": ["from .roboflow import RoboflowDataLoader, RoboflowPredictionsDataLoader\n\n__all__ = [\n    \"RoboflowDataLoader\",\n    \"JSONDataLoader\",\n    \"RoboflowPredictionsDataLoader\",\n]\n"]}
{"filename": "evaluations/annotation_check/run_evaluator.py", "chunked_list": ["\nfrom annotation_eval import AnnotationEval\nimport argparse\n'''\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"\")\nproject = rf.workspace(\"\").project(\"\")\ndataset = project.version().download(\"voc\")\n'''\n", "'''\n\n\n# translate above to argparse\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\n    \"--local_data_folder\",\n    type=str,\n    required=True,", "    type=str,\n    required=True,\n    help=\"Absolute path to local data folder of images and annotations \",\n)\n\nparser.add_argument(\n    \"--roboflow_data_folder\",\n    type=str,\n    required=True,\n    help=\"Absolute path to roboflow uploads of images and annotations \",", "    required=True,\n    help=\"Absolute path to roboflow uploads of images and annotations \",\n)\n\nargs = parser.parse_args()\n\nLOCAL_FOLDER = args.local_data_folder\nROBOFLOW_FOLDER = args.roboflow_data_folder\n\nif __name__ == \"__main__\":\n    eval = AnnotationEval(\n        local_folder = LOCAL_FOLDER,\n        roboflow_folder = ROBOFLOW_FOLDER)\n    eval.collect_images_labels()\n    eval.run_eval_loop()", "\nif __name__ == \"__main__\":\n    eval = AnnotationEval(\n        local_folder = LOCAL_FOLDER,\n        roboflow_folder = ROBOFLOW_FOLDER)\n    eval.collect_images_labels()\n    eval.run_eval_loop()\n"]}
{"filename": "evaluations/annotation_check/annotation_eval.py", "chunked_list": ["import json\nimport yaml\nimport pandas as pd\nimport os, glob\nimport yaml\nimport json\nimport csv\nimport xml.etree.ElementTree as ET\nfrom roboflow import Roboflow\nimport os", "from roboflow import Roboflow\nimport os\nimport argparse\n\n\nclass AnnotationEval():\n    def __init__(\n        self,\n        local_folder: str,\n        roboflow_folder: str\n    ):\n        \n        \"\"\"\n        Loads local images + labels and roboflow images + labels. \n        Checks that each label in each image locally matches the label in roboflow\n\n            Args:\n                local_folder (str): local folder\n                roboflow_folder (str): data downloaded/exported from roboflow.\n\n            Returns:\n                Similarity Score\n        \"\"\"\n        self.local_folder = local_folder\n        self.roboflow_folder = roboflow_folder\n    \n\n    def extract_text_after_last_slash(self,text):\n        last_slash_index = text.rfind('/')\n        if last_slash_index != -1:\n            text_after_last_slash = text[last_slash_index + 1:]\n            return text_after_last_slash\n        return None\n\n    def parse_xml(self,xml_string):\n        tree = ET.parse(xml_string)\n        root = tree.getroot()\n        bbox_dict = {}\n        # Find all <object> elements\n        object_elements = root.findall('.//object')\n        # Iterate over each <object> element\n        for i, object_element in enumerate(object_elements):\n            bbox = {}\n            # Extract bounding box coordinates\n            bndbox_element = object_element.find('bndbox')\n            bbox['xmin'] = round(float(bndbox_element.find('xmin').text))\n            bbox['xmax'] = round(float(bndbox_element.find('xmax').text))\n            bbox['ymin'] = round(float(bndbox_element.find('ymin').text))\n            bbox['ymax'] = round(float(bndbox_element.find('ymax').text))\n            # Extract name\n            name_element = object_element.find('name')\n            bbox['name'] = name_element.text\n            # Add bbox to the dictionary with counter as the key\n            bbox_dict[i] = bbox\n        return bbox_dict\n\n    def collect_images_labels(self):\n        \n        self.local_images = []\n        self.local_annotations = []\n        self.roboflow_annotations = []\n        \n        # Loop through each subfolder in image_folder\n        for root, dirs, files in os.walk(self.local_folder):\n            # Check if 'xml' folder exists in the current subfolder\n            if 'xml' in dirs:\n                xml_folder = os.path.join(root, 'xml')\n                # Get all XML files in xml_folder\n                #xml_files = [file for file in os.listdir(xml_folder) if file.endswith('.xml')]\n                xml_files = sorted(glob.glob(os.path.join(xml_folder, \"*.xml\")))\n                # Add the XML files to the master_list\n                self.local_annotations.extend(xml_files)\n\n        # Loop through each subfolder in image_folder\n        for root, dirs, files in os.walk(self.local_folder):\n            # Check if 'images' folder exists in the current subfolder\n            if 'images' in dirs:\n                images_folder = os.path.join(root, 'images')\n                # Get all JPEG files in the images_folder\n                #jpeg_files = [file for file in os.listdir(images_folder) if file.endswith('.jpg') or file.endswith('.jpg')]\n                jpeg_files = sorted(glob.glob(os.path.join(images_folder, \"*.jpg\")))\n\n\n                # Add the JPEG files to the local_images list\n                self.local_images.extend(jpeg_files)\n                \n        name_requirements = [\"train\",\"test\",\"valid\"]\n        # Loop through each subfolder in image_folder\n        for root, dirs, files in os.walk(self.roboflow_folder):\n            # Check if 'images' folder exists in the current subfolder\n            for folder in dirs:\n                # Check if the subfolder meets the name requirement\n                if any(req in folder for req in name_requirements):\n\n                    xml_path = os.path.join(root, folder)\n                    # Get all XML files in the subfolder\n                    #xml_files = [file for file in os.listdir(xml_path) if file.endswith('.xml')]\n                    xml_files = sorted(glob.glob(os.path.join(xml_path, \"*.xml\")))\n                    # Add the XML files to the local_images list\n                    self.roboflow_annotations.extend(xml_files)\n                    \n        # Print the local_images list\n        print('local image count',len(self.local_images))\n        print('local annotation count',len(self.local_annotations))\n        print('robfolow annotation count',len(self.roboflow_annotations))\n        return self.local_images,self.local_annotations,self.roboflow_annotations\n\n\n    def run_eval_loop(self):\n    \n        count = 0\n        loop_count = 0\n        roboflow_count = 0\n        match1 = 0\n        overall_accuracy = []\n        no_difference_count = 0\n        roboflow_key_count = 0\n        local_key_count = 0\n        key_match = 0\n\n        for image in self.local_images:\n            if count < len(self.local_images):\n                               \n                f = os.path.join(image)\n                image_hash = self.extract_text_after_last_slash(image.split(\".\")[0].replace(\"#\",\"-\"))\n\n                # split the image path to the hash\n                current_annotation = self.local_annotations[count]\n                annotation_hash = self.extract_text_after_last_slash(current_annotation.split(\".\")[0]).replace(\"#\",\"-\")\n                \n                if image_hash == annotation_hash:\n                \n                    match1 +=1 \n                \n                    for roboflow_annotation in self.roboflow_annotations:\n                        #Roboflow labels and hash\n                        roboflow_hash = ((roboflow_annotation.split(\"/\"))[-1].split('.')[0][:-4])\n                        \n                        if roboflow_hash == image_hash:\n                        \n                            roboflow_count +=1\n                                \n                            local_parsed = self.parse_xml(current_annotation)\n                            roboflow_parsed = self.parse_xml(roboflow_annotation)\n                            \n                            label_count_local = len(local_parsed)\n                            roboflow_count_local = len(roboflow_parsed)\n                            local_key_count += label_count_local\n            \n                            for key in local_parsed: \n                                \n                                if key in roboflow_parsed:\n                                    roboflow_key_count+=1\n                                    difference = 0\n                                    for sub_key in local_parsed[key]:\n                                        if sub_key in roboflow_parsed[key] and type(local_parsed[key][sub_key]) == int and (local_parsed[key][sub_key]-local_parsed[key][sub_key]) >1:\n                                            difference += (local_parsed[key][sub_key] - roboflow_parsed[key][sub_key])\n                                        if type(local_parsed[key][sub_key]) == str:\n                                            if local_parsed[key][sub_key] != roboflow_parsed[key][sub_key]:\n                                                difference += 1\n                                    if difference <=1:\n                                        no_difference_count +=1\n                                \n                                    elif difference >1:\n                                        print('PIXEL MISMATCH')\n                                        print(image_hash,annotation_hash,roboflow_hash)\n                                        print(difference)\n                    \n             \n                    count+=1\n\n                    if loop_count > len(self.local_images)*len(self.local_annotations):\n                        break\n                    \n        print('\\n')\n        print('KEY_MATCH %',str((roboflow_key_count/local_key_count)*100)+'%')\n        print('\\n')\n        print('LABEL SIMILARITY %',str((no_difference_count/roboflow_key_count)*100)+'%')\n        print('\\n')\n        print('TOTAL LABELS',local_key_count)\n        print('\\n')\n        print('TOTAL IMAGE MATCH',match1)", ""]}
