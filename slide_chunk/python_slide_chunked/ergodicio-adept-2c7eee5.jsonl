{"filename": "train-dispersion.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n#  research@ergodic.io\n\nimport yaml, os\n\nimport numpy as np\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n# config.update(\"jax_debug_nans\", True)", "config.update(\"jax_enable_x64\", True)\n# config.update(\"jax_debug_nans\", True)\n# config.update(\"jax_disable_jit\", True)\n\nimport jax\nfrom jax import numpy as jnp\nimport tempfile, time\nimport mlflow, optax\nimport equinox as eqx\nfrom tqdm import tqdm", "import equinox as eqx\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\n\nimport helpers\nfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\nfrom utils import misc\n\n\ndef _modify_defaults_(defaults, k0):\n    wepw = float(np.sqrt(1.0 + 3.0 * k0**2.0))\n\n    defaults[\"save\"][\"func\"][\"is_on\"] = False\n    defaults[\"physics\"][\"landau_damping\"] = True\n    defaults[\"physics\"][\"electron\"][\"trapping\"][\"kld\"] = k0\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = k0\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = wepw\n    xmax = float(2.0 * np.pi / k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"kx\"][\"kxmax\"] = k0\n\n    return defaults", "\ndef _modify_defaults_(defaults, k0):\n    wepw = float(np.sqrt(1.0 + 3.0 * k0**2.0))\n\n    defaults[\"save\"][\"func\"][\"is_on\"] = False\n    defaults[\"physics\"][\"landau_damping\"] = True\n    defaults[\"physics\"][\"electron\"][\"trapping\"][\"kld\"] = k0\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = k0\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = wepw\n    xmax = float(2.0 * np.pi / k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"kx\"][\"kxmax\"] = k0\n\n    return defaults", "\n\ndef train_loop():\n    weight_key = jax.random.PRNGKey(420)\n    dispersion_models = {\n        \"w_of_k\": eqx.nn.MLP(1, 1, 4, 3, activation=jnp.tanh, final_activation=jnp.tanh, key=weight_key)\n    }\n    optimizer = optax.adam(0.1)\n    opt_state = optimizer.init(eqx.filter(dispersion_models, eqx.is_array))\n\n    # modify config\n    num_ks = 8\n    batch_size = 4\n    k0s = np.linspace(0.2, 0.4, num_ks)\n    all_ks = np.linspace(0.01, 0.5, 1024)\n    all_ws = np.sqrt(1 + 3 * all_ks**2.0)\n    rng = np.random.default_rng(420)\n\n    mlflow.set_experiment(\"train-dispersion\")\n    with mlflow.start_run(run_name=\"disp-opt\", nested=True) as mlflow_run:\n        for epoch in range(100):\n            rng.shuffle(k0s)\n            these_batches = k0s.reshape((-1, batch_size))\n            epoch_loss = 0.0\n\n            for batch, this_batch in tqdm(enumerate(these_batches), total=len(these_batches)):\n                grads = []\n                for sim, k0 in enumerate(this_batch):\n                    with open(\"./tests/configs/resonance.yaml\", \"r\") as file:\n                        defaults = yaml.safe_load(file)\n                    mod_defaults = _modify_defaults_(defaults, float(k0))\n                    with mlflow.start_run(run_name=f\"{epoch=}-{batch=}-{sim=}\", nested=True) as mlflow_run:\n                        mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n                        misc.log_params(mod_defaults)\n\n                        mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n                        mod_defaults = helpers.get_save_quantities(mod_defaults)\n\n                        with tempfile.TemporaryDirectory() as td:\n                            # run\n                            t0 = time.time()\n                            state = helpers.init_state(mod_defaults)\n                            k0 = (k0 - 0.2) / 0.25\n\n                            def loss(models):\n                                w0 = jnp.squeeze(0.5 * models[\"w_of_k\"](jnp.array([k0])) + 1.1)\n                                mod_defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = w0\n                                vf = helpers.VectorField(mod_defaults, models=False)\n                                args = {\"driver\": mod_defaults[\"drivers\"]}\n\n                                results = diffeqsolve(\n                                    terms=ODETerm(vf),\n                                    solver=Tsit5(),\n                                    t0=mod_defaults[\"grid\"][\"tmin\"],\n                                    t1=mod_defaults[\"grid\"][\"tmax\"],\n                                    max_steps=mod_defaults[\"grid\"][\"max_steps\"],\n                                    dt0=mod_defaults[\"grid\"][\"dt\"],\n                                    y0=state,\n                                    args=args,\n                                    saveat=SaveAt(ts=mod_defaults[\"save\"][\"t\"][\"ax\"]),\n                                )\n                                nk1 = (\n                                    jnp.abs(jnp.fft.fft(results.ys[\"electron\"][\"n\"], axis=1)[:, 1])\n                                    * 2.0\n                                    / mod_defaults[\"grid\"][\"nx\"]\n                                )\n                                return -jnp.amax(nk1), results\n\n                            vg_func = eqx.filter_value_and_grad(loss, has_aux=True)\n                            (loss_val, results), grad = eqx.filter_jit(vg_func)(dispersion_models)\n                            mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4)})\n\n                            t0 = time.time()\n                            helpers.post_process(results, mod_defaults, td)\n                            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n                            # log artifacts\n                            mlflow.log_artifacts(td)\n                    grads.append(grad)\n\n                grads = misc.all_reduce_gradients(grads, len(grads))\n                updates, opt_state = optimizer.update(grads, opt_state, dispersion_models)\n                dispersion_models = eqx.apply_updates(dispersion_models, updates)\n                loss_val = float(loss_val)\n                mlflow.log_metrics({\"run_loss\": loss_val}, step=sim + epoch * len(k0s))\n                epoch_loss = epoch_loss + loss_val\n\n                # pbar.set_description(f\"{loss_val=:.2e}, {epoch_loss=:.2e}, average_loss={epoch_loss/(sim+1):.2e}\")\n\n            learned_ws = (\n                0.5 * eqx.filter_vmap(dispersion_models[\"w_of_k\"])((jnp.array(all_ks[:, None]) - 0.2) / 0.25) + 1.1\n            )\n            chosen_ws = 0.5 * eqx.filter_vmap(dispersion_models[\"w_of_k\"])((jnp.array(k0s[:, None]) - 0.2) / 0.25) + 1.1\n\n            fig, ax = plt.subplots(1, 1, figsize=(7, 4), tight_layout=True)\n            ax.plot(all_ks, all_ws, label=\"actual\")\n            ax.plot(k0s, chosen_ws, \"x\", label=\"training data\")\n            ax.plot(all_ks, np.squeeze(learned_ws), label=\"prediction\")\n            ax.legend(fontsize=14)\n            ax.grid()\n            ax.set_xlabel(r\"$k\\lambda_D$\", fontsize=14)\n            ax.set_ylabel(r\"$\\omega_0$\", fontsize=14)\n            with tempfile.TemporaryDirectory() as td:\n                fig.savefig(os.path.join(td, f\"{epoch=}\"), bbox_inches=\"tight\")\n                mlflow.log_artifacts(td, \"validation-plots\")\n            plt.close(fig)\n            mlflow.log_metrics({\"epoch_loss\": epoch_loss})\n            mlflow.log_metrics({\"val_loss\": float(np.sqrt(np.mean(np.square(all_ws - learned_ws))))})", "\n\nif __name__ == \"__main__\":\n    train_loop()\n"]}
{"filename": "run.py", "chunked_list": ["from jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n# config.update(\"jax_disable_jit\", True)\n\nimport yaml, mlflow\nfrom utils.runner import run\n\nif __name__ == \"__main__\":\n    with open(\"configs/epw.yaml\", \"r\") as fi:\n    # with open(\"tests/configs/resonance.yaml\", \"r\") as fi:\n        cfg = yaml.safe_load(fi)\n\n    mlflow.set_experiment(cfg[\"mlflow\"][\"experiment\"])\n    # modify config\n    with mlflow.start_run(run_name=cfg[\"mlflow\"][\"run\"]) as mlflow_run:\n        run(cfg)", "if __name__ == \"__main__\":\n    with open(\"configs/epw.yaml\", \"r\") as fi:\n    # with open(\"tests/configs/resonance.yaml\", \"r\") as fi:\n        cfg = yaml.safe_load(fi)\n\n    mlflow.set_experiment(cfg[\"mlflow\"][\"experiment\"])\n    # modify config\n    with mlflow.start_run(run_name=cfg[\"mlflow\"][\"run\"]) as mlflow_run:\n        run(cfg)\n", ""]}
{"filename": "run_job.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n#  research@ergodic.io\n\nimport sys, os\nfrom utils.runner import start_run\n\n\nif __name__ == \"__main__\":\n    run_id = sys.argv[1]\n    run_type = sys.argv[2]\n    start_run(run_type, run_id)", ""]}
{"filename": "train-damping.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n#  research@ergodic.io\n\nimport yaml, os\nfrom itertools import product\n\nimport numpy as np\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)", "\nconfig.update(\"jax_enable_x64\", True)\n# config.update(\"jax_debug_nans\", True)\n# config.update(\"jax_disable_jit\", True)\n\nfrom jax import numpy as jnp\nimport xarray as xr\nimport tempfile, time\nimport mlflow, optax\nimport equinox as eqx", "import mlflow, optax\nimport equinox as eqx\nfrom tqdm import tqdm\n\nimport helpers\nfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\nfrom utils import misc, plotters\n\n\ndef _modify_defaults_(defaults, k0, a0, nuee):\n    wepw = float(np.sqrt(1.0 + 3.0 * k0**2.0))\n\n    defaults[\"physics\"][\"landau_damping\"] = True\n    defaults[\"physics\"][\"electron\"][\"trapping\"][\"nuee\"] = nuee\n    defaults[\"physics\"][\"electron\"][\"trapping\"][\"kld\"] = k0\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = k0\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = wepw\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"a0\"] = a0\n    xmax = float(2.0 * np.pi / k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"kx\"][\"kxmax\"] = k0\n\n    return defaults", "\ndef _modify_defaults_(defaults, k0, a0, nuee):\n    wepw = float(np.sqrt(1.0 + 3.0 * k0**2.0))\n\n    defaults[\"physics\"][\"landau_damping\"] = True\n    defaults[\"physics\"][\"electron\"][\"trapping\"][\"nuee\"] = nuee\n    defaults[\"physics\"][\"electron\"][\"trapping\"][\"kld\"] = k0\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = k0\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = wepw\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"a0\"] = a0\n    xmax = float(2.0 * np.pi / k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"kx\"][\"kxmax\"] = k0\n\n    return defaults", "\n\ndef loss(models, this_cfg, state, actual_nk1):\n    vf = helpers.VectorField(this_cfg, models=models)\n    args = {\"driver\": this_cfg[\"drivers\"]}\n\n    results = diffeqsolve(\n        terms=ODETerm(vf),\n        solver=Tsit5(),\n        t0=this_cfg[\"grid\"][\"tmin\"],\n        t1=this_cfg[\"grid\"][\"tmax\"],\n        max_steps=this_cfg[\"grid\"][\"max_steps\"],\n        dt0=this_cfg[\"grid\"][\"dt\"],\n        y0=state,\n        args=args,\n        saveat=SaveAt(ts=this_cfg[\"save\"][\"t\"][\"ax\"], fn=this_cfg[\"save\"][\"func\"][\"callable\"]),\n    )\n    nk1 = jnp.abs(jnp.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1]) * 2.0 / this_cfg[\"grid\"][\"nx\"]\n    return jnp.mean(jnp.square((actual_nk1 - nk1) / jnp.amax(actual_nk1))), results", "\n\ndef train_loop():\n    vg_func = eqx.filter_jit(eqx.filter_value_and_grad(loss, has_aux=True))\n    with open(\"configs/damping.yaml\", \"r\") as file:\n        defaults = yaml.safe_load(file)\n    trapping_models = helpers.get_models(defaults[\"models\"])\n    optimizer = optax.adam(0.1)\n    opt_state = optimizer.init(eqx.filter(trapping_models, eqx.is_array))\n\n    batch_size = 3\n\n    # modify config\n    fks = xr.open_dataset(\"./epws.nc\")\n\n    nus = np.copy(fks.coords[r\"$\\nu_{ee}$\"].data[::3])\n    k0s = np.copy(fks.coords[\"$k_0$\"].data[::2])\n    a0s = np.copy(fks.coords[\"$a_0$\"].data[::3])\n    all_sims = np.array(list(product(nus, k0s, a0s)))\n\n    rng = np.random.default_rng(420)\n\n    train_sims = rng.choice(\n        np.arange(all_sims.shape[0]), int(0.8 * all_sims.shape[0] / batch_size) * batch_size, replace=False\n    )\n    val_sims = np.array(list(set(np.arange(all_sims.shape[0])) - set(train_sims)))\n\n    mlflow.set_experiment(\"damping-rates-epw-new-model\")\n    with mlflow.start_run(run_name=\"damping-opt\", nested=True) as mlflow_run:\n        for epoch in range(100):\n            epoch_loss = 0.0\n            rng.shuffle(train_sims)\n            train_batches = all_sims[train_sims].reshape((-1, batch_size, 3))\n            for i_batch, batch in (pbar := tqdm(enumerate(train_batches), total=len(train_batches))):\n                grads = []\n                for sim, (nuee, k0, a0) in tqdm(enumerate(batch), total=batch_size):\n                    with open(\"configs/damping.yaml\", \"r\") as file:\n                        defaults = yaml.safe_load(file)\n\n                    mod_defaults = _modify_defaults_(defaults, float(k0), float(a0), float(nuee))\n                    locs = {\"$k_0$\": k0, \"$a_0$\": a0, r\"$\\nu_{ee}$\": nuee}\n                    actual_nk1 = xr.DataArray(\n                        fks[\"n-(k_x)\"].loc[locs].data[:, 1], coords=((\"t\", fks.coords[\"t\"].data),)\n                    ).data\n                    with mlflow.start_run(run_name=f\"{epoch=}-{batch=}-{sim=}\", nested=True) as mlflow_run:\n                        mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n                        misc.log_params(mod_defaults)\n\n                        mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n                        mod_defaults = helpers.get_save_quantities(mod_defaults)\n\n                        with tempfile.TemporaryDirectory() as td:\n                            # run\n                            t0 = time.time()\n                            state = helpers.init_state(mod_defaults)\n\n                            (loss_val, results), grad = vg_func(trapping_models, mod_defaults, state, actual_nk1)\n                            mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4)})\n\n                            t0 = time.time()\n                            helpers.post_process(results, mod_defaults, td)\n                            plotters.mva(actual_nk1, mod_defaults, results, td, fks.coords)\n                            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n                            # log artifacts\n                            mlflow.log_artifacts(td)\n\n                    grads.append(grad)\n\n                grads = misc.all_reduce_gradients(grads, len(grads))\n                updates, opt_state = optimizer.update(grads, opt_state, trapping_models)\n                trapping_models = eqx.apply_updates(trapping_models, updates)\n                loss_val = float(loss_val)\n                mlflow.log_metrics({\"run_loss\": loss_val}, step=sim + epoch * 100)\n                epoch_loss = epoch_loss + loss_val\n                pbar.set_description(f\"{loss_val=:.2e}, {epoch_loss=:.2e}, average_loss={epoch_loss/(sim+1):.2e}\")\n\n            mlflow.log_metrics({\"epoch_loss\": epoch_loss})", "\n\ndef remote_train_loop():\n    with open(\"configs/damping.yaml\", \"r\") as file:\n        defaults = yaml.safe_load(file)\n    trapping_models = helpers.get_models(defaults[\"models\"])\n    optimizer = optax.adam(0.1)\n    opt_state = optimizer.init(eqx.filter(trapping_models, eqx.is_array))\n\n    batch_size = 16\n\n    # modify config\n    fks = xr.open_dataset(\"./epws.nc\")\n\n    nus = np.copy(fks.coords[r\"$\\nu_{ee}$\"].data[::3])\n    k0s = np.copy(fks.coords[\"$k_0$\"].data[::2])\n    a0s = np.copy(fks.coords[\"$a_0$\"].data[::3])\n    all_sims = np.array(list(product(nus, k0s, a0s)))\n\n    rng = np.random.default_rng(420)\n\n    train_sims = rng.choice(\n        np.arange(all_sims.shape[0]), int(0.8 * all_sims.shape[0] / batch_size) * batch_size, replace=False\n    )\n    val_sims = np.array(list(set(np.arange(all_sims.shape[0])) - set(train_sims)))\n\n    mlflow.set_experiment(\"damping-rates-epw-new-model\")\n    with mlflow.start_run(run_name=\"damping-opt\", nested=True) as mlflow_run:\n        for epoch in range(100):\n            epoch_loss = 0.0\n            rng.shuffle(train_sims)\n            train_batches = all_sims[train_sims].reshape((-1, batch_size, 3))\n            for i_batch, batch in (pbar := tqdm(enumerate(train_batches), total=len(train_batches))):\n                run_ids, job_done = [], []\n                for sim, (nuee, k0, a0) in enumerate(batch):\n                    run_ids, job_done = queue_sim(\n                        fks, nuee, k0, a0, run_ids, job_done, trapping_models, epoch, i_batch, sim, t_or_v=\"grad\"\n                    )\n                trapping_models = update_w_and_b(\n                    job_done, run_ids, optimizer, opt_state, eqx.filter(trapping_models, eqx.is_array)\n                )\n                batch_loss = float(\n                    np.average(\n                        np.array([misc.get_this_metric_of_this_run(\"loss\", queued_run_id) for queued_run_id in run_ids])\n                    )\n                )\n                mlflow.log_metrics({\"batch_loss\": batch_loss}, step=i_batch + epoch * len(train_batches))\n                epoch_loss = epoch_loss + batch_loss\n                pbar.set_description(f\"{batch_loss=:.2e}, {epoch_loss=:.2e}, average_loss={epoch_loss/(sim+1):.2e}\")\n\n            mlflow.log_metrics({\"epoch_loss\": epoch_loss}, step=epoch)\n\n            # validation\n            run_ids, job_done = [], []\n            for sim, (nuee, k0, a0) in enumerate(all_sims[val_sims]):\n                run_ids, job_done = queue_sim(\n                    fks, nuee, k0, a0, run_ids, job_done, trapping_models, epoch, 0, sim, t_or_v=\"val\"\n                )\n            wait_for_jobs(job_done, run_ids)\n            val_loss = float(\n                np.average(\n                    np.array([misc.get_this_metric_of_this_run(\"val_loss\", queued_run_id) for queued_run_id in run_ids])\n                )\n            )\n            mlflow.log_metrics({\"val_epoch_loss\": val_loss}, step=epoch)", "\n\ndef wait_for_jobs(job_done, run_ids):\n    while not all(job_done):\n        for i, run_id in enumerate(run_ids):\n            time.sleep(4.2 / len(run_ids))\n            job_done[i] = misc.is_job_done(run_id)\n\n\ndef update_w_and_b(job_done, run_ids, optimizer, opt_state, w_and_b):\n    wait_for_jobs(job_done, run_ids)\n    gradients = []\n    with tempfile.TemporaryDirectory() as td:\n        for queued_run_id in run_ids:\n            mlflow.artifacts.download_artifacts(run_id=queued_run_id, artifact_path=\"grads.eqx\", dst_path=td)\n            gradients.append(eqx.tree_deserialise_leaves(os.path.join(td, \"grads.eqx\"), w_and_b))\n\n    gradients = misc.all_reduce_gradients(gradients, len(run_ids))\n    updates, opt_state = optimizer.update(gradients, opt_state, w_and_b)\n    w_and_b = eqx.apply_updates(w_and_b, updates)\n\n    return w_and_b", "\ndef update_w_and_b(job_done, run_ids, optimizer, opt_state, w_and_b):\n    wait_for_jobs(job_done, run_ids)\n    gradients = []\n    with tempfile.TemporaryDirectory() as td:\n        for queued_run_id in run_ids:\n            mlflow.artifacts.download_artifacts(run_id=queued_run_id, artifact_path=\"grads.eqx\", dst_path=td)\n            gradients.append(eqx.tree_deserialise_leaves(os.path.join(td, \"grads.eqx\"), w_and_b))\n\n    gradients = misc.all_reduce_gradients(gradients, len(run_ids))\n    updates, opt_state = optimizer.update(gradients, opt_state, w_and_b)\n    w_and_b = eqx.apply_updates(w_and_b, updates)\n\n    return w_and_b", "\n\ndef queue_sim(fks, nuee, k0, a0, run_ids, job_done, w_and_b, epoch, i_batch, sim, t_or_v=\"grad\"):\n    with open(\"configs/damping.yaml\", \"r\") as file:\n        defaults = yaml.safe_load(file)\n\n    mod_defaults = _modify_defaults_(defaults, float(k0), float(a0), float(nuee))\n    locs = {\"$k_0$\": k0, \"$a_0$\": a0, r\"$\\nu_{ee}$\": nuee}\n    actual_nk1 = xr.DataArray(fks[\"n-(k_x)\"].loc[locs].data[:, 1], coords=((\"t\", fks.coords[\"t\"].data),))\n    with mlflow.start_run(run_name=f\"{epoch=}-batch={i_batch}-{sim=}\", nested=True) as mlflow_run:\n        with tempfile.TemporaryDirectory() as td:\n            with open(os.path.join(td, \"config.yaml\"), \"w\") as fp:\n                yaml.dump(mod_defaults, fp)\n            actual_nk1.to_netcdf(os.path.join(td, \"ground_truth.nc\"))\n            # with open(os.path.join(td, \"weights.pkl\"), \"wb\") as fi:\n            #     pickle.dump(w_and_b, fi)\n            eqx.tree_serialise_leaves(os.path.join(td, \"weights.eqx\"), w_and_b)\n\n            mlflow.log_artifacts(td)\n        misc.queue_sim(\n            {\n                \"job_name\": f\"epw-{t_or_v}-epoch-{epoch}-batch-{i_batch}-sim-{sim}\",\n                \"run_id\": mlflow_run.info.run_id,\n                \"sim_type\": \"fluid\",\n                \"run_type\": t_or_v,\n                \"machine\": \"continuum-cpu\",\n            }\n        )\n        mlflow.set_tags({\"status\": \"queued\"})\n        run_ids.append(mlflow_run.info.run_id)\n        job_done.append(False)\n\n    return run_ids, job_done", "\n\nif __name__ == \"__main__\":\n    train_loop()\n"]}
{"filename": "utils/runner.py", "chunked_list": ["from typing import Dict\nimport os, time, tempfile, yaml\n\nfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5, Solution\nfrom jax import numpy as jnp\nimport numpy as np\n\nimport mlflow\nimport equinox as eqx\nimport xarray as xr", "import equinox as eqx\nimport xarray as xr\n\nfrom adept.es1d import helpers\nfrom utils import plotters, misc\n\n\ndef start_run(run_type, run_id):\n    if run_type == \"forward\":\n        just_forward(run_id, nested=False)\n    elif run_type == \"grad\":\n        remote_gradient(run_id)\n    elif run_type == \"val\":\n        remote_val(run_id)\n    else:\n        raise NotImplementedError", "\n\ndef run(cfg: Dict) -> Solution:\n    with tempfile.TemporaryDirectory() as td:\n        with open(os.path.join(td, \"config.yaml\"), \"w\") as fi:\n            yaml.dump(cfg, fi)\n\n        # get derived quantities\n        cfg[\"grid\"] = helpers.get_derived_quantities(cfg[\"grid\"])\n        misc.log_params(cfg)\n\n        cfg[\"grid\"] = helpers.get_solver_quantities(cfg[\"grid\"])\n        cfg = helpers.get_save_quantities(cfg)\n\n        models = helpers.get_models(cfg[\"models\"])\n        state = helpers.init_state(cfg)\n\n        # run\n        t0 = time.time()\n\n        # @eqx.filter_jit\n        def _run_():\n            vf = helpers.VectorField(cfg, models=models)\n            return diffeqsolve(\n                terms=ODETerm(vf),\n                solver=Tsit5(),\n                t0=cfg[\"grid\"][\"tmin\"],\n                t1=cfg[\"grid\"][\"tmax\"],\n                max_steps=cfg[\"grid\"][\"max_steps\"],\n                dt0=cfg[\"grid\"][\"dt\"],\n                y0=state,\n                args={\"driver\": cfg[\"drivers\"]},\n                saveat=SaveAt(ts=cfg[\"save\"][\"t\"][\"ax\"], fn=cfg[\"save\"][\"func\"][\"callable\"]),\n            )\n\n        result = _run_()\n        mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4)})\n\n        t0 = time.time()\n        helpers.post_process(result, cfg, td)\n        mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n        # log artifacts\n        mlflow.log_artifacts(td)\n\n    # fin\n\n    return result", "\n\ndef remote_gradient(run_id):\n    with mlflow.start_run(run_id=run_id, nested=True) as mlflow_run:\n        with tempfile.TemporaryDirectory() as td:\n            mod_defaults = misc.get_cfg(artifact_uri=mlflow_run.info.artifact_uri, temp_path=td)\n            actual_nk1 = xr.open_dataarray(\n                misc.download_file(\"ground_truth.nc\", artifact_uri=mlflow_run.info.artifact_uri, destination_path=td)\n            )\n            mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n            misc.log_params(mod_defaults)\n            mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n            mod_defaults = helpers.get_save_quantities(mod_defaults)\n            t0 = time.time()\n\n            state = helpers.init_state(mod_defaults)\n            mod_defaults[\"models\"][\"file\"] = misc.download_file(\n                \"weights.eqx\", artifact_uri=mlflow_run.info.artifact_uri, destination_path=td\n            )\n            models = helpers.get_models(mod_defaults[\"models\"])\n\n            def loss(these_models):\n                vf = helpers.VectorField(mod_defaults, models=these_models)\n                args = {\"driver\": mod_defaults[\"drivers\"]}\n\n                results = diffeqsolve(\n                    terms=ODETerm(vf),\n                    solver=Tsit5(),\n                    t0=mod_defaults[\"grid\"][\"tmin\"],\n                    t1=mod_defaults[\"grid\"][\"tmax\"],\n                    max_steps=mod_defaults[\"grid\"][\"max_steps\"],\n                    dt0=mod_defaults[\"grid\"][\"dt\"],\n                    y0=state,\n                    args=args,\n                    saveat=SaveAt(ts=mod_defaults[\"save\"][\"t\"][\"ax\"], fn=mod_defaults[\"save\"][\"func\"][\"callable\"]),\n                )\n                nk1 = (\n                    jnp.abs(jnp.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1])\n                    * 2.0\n                    / mod_defaults[\"grid\"][\"nx\"]\n                )\n                return (\n                    jnp.mean(\n                        jnp.square(\n                            (np.log10(actual_nk1.data + 1e-20) - jnp.log10(nk1 + 1e-20))\n                            / np.log10(np.amax(actual_nk1.data))\n                        )\n                        * jnp.exp(-2 * (1 - (mod_defaults[\"save\"][\"t\"][\"ax\"] / mod_defaults[\"save\"][\"t\"][\"tmax\"])))\n                    ),\n                    results,\n                )\n\n            vg_func = eqx.filter_value_and_grad(loss, has_aux=True)\n            (loss_val, results), grad = eqx.filter_jit(vg_func)(models)\n            mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4), \"loss\": float(loss_val)})\n\n            # dump gradients\n            eqx.tree_serialise_leaves(os.path.join(td, \"grads.eqx\"), grad)\n\n            t0 = time.time()\n            helpers.post_process(results, mod_defaults, td)\n            plotters.mva(actual_nk1.data, mod_defaults, results, td, actual_nk1.coords)\n            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n            # log artifacts\n            mlflow.log_artifacts(td)\n            mlflow.set_tags({\"status\": \"completed\"})", "\n\ndef remote_val(run_id):\n    with mlflow.start_run(run_id=run_id, nested=True) as mlflow_run:\n        with tempfile.TemporaryDirectory() as td:\n            mod_defaults = misc.get_cfg(artifact_uri=mlflow_run.info.artifact_uri, temp_path=td)\n            actual_nk1 = xr.open_dataarray(\n                misc.download_file(\"ground_truth.nc\", artifact_uri=mlflow_run.info.artifact_uri, destination_path=td)\n            )\n            mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n            misc.log_params(mod_defaults)\n            mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n            mod_defaults = helpers.get_save_quantities(mod_defaults)\n            t0 = time.time()\n\n            state = helpers.init_state(mod_defaults)\n            mod_defaults[\"models\"][\"file\"] = misc.download_file(\n                \"weights.eqx\", artifact_uri=mlflow_run.info.artifact_uri, destination_path=td\n            )\n            models = helpers.get_models(mod_defaults[\"models\"])\n\n            def loss(these_models):\n                vf = helpers.VectorField(mod_defaults, models=these_models)\n                args = {\"driver\": mod_defaults[\"drivers\"]}\n                results = diffeqsolve(\n                    terms=ODETerm(vf),\n                    solver=Tsit5(),\n                    t0=mod_defaults[\"grid\"][\"tmin\"],\n                    t1=mod_defaults[\"grid\"][\"tmax\"],\n                    max_steps=mod_defaults[\"grid\"][\"max_steps\"],\n                    dt0=mod_defaults[\"grid\"][\"dt\"],\n                    y0=state,\n                    args=args,\n                    saveat=SaveAt(ts=mod_defaults[\"save\"][\"t\"][\"ax\"], fn=mod_defaults[\"save\"][\"func\"][\"callable\"]),\n                )\n                nk1 = (\n                    jnp.abs(jnp.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1])\n                    * 2.0\n                    / mod_defaults[\"grid\"][\"nx\"]\n                )\n                return (\n                    jnp.mean(\n                        jnp.square(\n                            (np.log10(actual_nk1.data + 1e-20) - jnp.log10(nk1 + 1e-20))\n                            / np.log10(np.amax(actual_nk1.data))\n                        )\n                        * jnp.exp(-2 * (1 - (mod_defaults[\"save\"][\"t\"][\"ax\"] / mod_defaults[\"save\"][\"t\"][\"tmax\"])))\n                    ),\n                    results,\n                )\n\n            loss_val, results = eqx.filter_jit(loss)(models)\n            mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4), \"val_loss\": float(loss_val)})\n\n            t0 = time.time()\n            helpers.post_process(results, mod_defaults, td)\n            plotters.mva(actual_nk1.data, mod_defaults, results, td, actual_nk1.coords)\n            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n            # log artifacts\n            mlflow.log_artifacts(td)\n            mlflow.set_tags({\"status\": \"completed\"})", ""]}
{"filename": "utils/plotters.py", "chunked_list": ["import os\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef mva(actual_nk1, mod_defaults, results, td, coords):\n    fig, ax = plt.subplots(1, 2, figsize=(10, 4), tight_layout=True)\n    ax[0].plot(coords[\"t\"].data, actual_nk1, label=\"Vlasov\")\n    ax[0].plot(\n        mod_defaults[\"save\"][\"t\"][\"ax\"],\n        (np.abs(np.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1]) * 2.0 / mod_defaults[\"grid\"][\"nx\"]),\n        label=\"NN + Fluid\",\n    )\n    ax[1].semilogy(coords[\"t\"].data, actual_nk1, label=\"Vlasov\")\n    ax[1].semilogy(\n        mod_defaults[\"save\"][\"t\"][\"ax\"],\n        (np.abs(np.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1]) * 2.0 / mod_defaults[\"grid\"][\"nx\"]),\n        label=\"NN + Fluid\",\n    )\n    ax[0].set_xlabel(r\"t ($\\omega_p^{-1}$)\", fontsize=12)\n    ax[1].set_xlabel(r\"t ($\\omega_p^{-1}$)\", fontsize=12)\n    ax[0].set_ylabel(r\"$|\\hat{n}|^{1}$\", fontsize=12)\n    ax[0].grid()\n    ax[1].grid()\n    ax[0].legend(fontsize=14)\n    fig.savefig(os.path.join(td, \"plots\", \"vlasov_v_fluid.png\"), bbox_inches=\"tight\")\n    plt.close(fig)", ""]}
{"filename": "utils/__init__.py", "chunked_list": [""]}
{"filename": "utils/nn.py", "chunked_list": ["from typing import Any, Literal, Optional, TypeVar, Union, Callable, Tuple\nimport jax.random as jrandom\nfrom jax import nn as jnn\nimport numpy as np\nfrom jaxtyping import Array\n\nfrom equinox._custom_types import PRNGKey\nfrom equinox import Module, static_field\n\n\nclass Linear(Module):\n    \"\"\"Performs a linear transformation.\"\"\"\n\n    weight: Array\n    bias: Optional[Array]\n    in_features: Union[int, Literal[\"scalar\"]] = static_field()\n    out_features: Union[int, Literal[\"scalar\"]] = static_field()\n    use_bias: bool = static_field()\n\n    def __init__(\n        self,\n        in_features: Union[int, Literal[\"scalar\"]],\n        out_features: Union[int, Literal[\"scalar\"]],\n        use_bias: bool = True,\n        *,\n        key: PRNGKey,\n    ):\n        \"\"\"**Arguments:**\n\n        - `in_features`: The input size. The input to the layer should be a vector of\n            shape `(in_features,)`\n        - `out_features`: The output size. The output from the layer will be a vector\n            of shape `(out_features,)`.\n        - `use_bias`: Whether to add on a bias as well.\n        - `key`: A `jax.random.PRNGKey` used to provide randomness for parameter\n            initialisation. (Keyword only argument.)\n\n        Note that `in_features` also supports the string `\"scalar\"` as a special value.\n        In this case the input to the layer should be of shape `()`.\n\n        Likewise `out_features` can also be a string `\"scalar\"`, in which case the\n        output from the layer will have shape `()`.\n        \"\"\"\n        super().__init__()\n        wkey, bkey = jrandom.split(key, 2)\n        in_features_ = 1 if in_features == \"scalar\" else in_features\n        out_features_ = 1 if out_features == \"scalar\" else out_features\n        std_dev = 1.0 / np.sqrt(in_features)\n        self.weight = jrandom.truncated_normal(wkey, -2.0, 2.0, (out_features_, in_features_)) * std_dev\n        if use_bias:\n            self.bias = jrandom.truncated_normal(bkey, -2.0, 2.0, (out_features_,)) * std_dev\n        else:\n            self.bias = None\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.use_bias = use_bias\n\n    def __call__(self, x: Array, *, key: Optional[PRNGKey] = None) -> Array:\n        \"\"\"**Arguments:**\n\n        - `x`: The input. Should be a JAX array of shape `(in_features,)`. (Or shape\n            `()` if `in_features=\"scalar\"`.)\n        - `key`: Ignored; provided for compatibility with the rest of the Equinox API.\n            (Keyword only argument.)\n\n        !!! info\n\n            If you want to use higher order tensors as inputs (for example featuring \"\n            \"batch dimensions) then use `jax.vmap`. For example, for an input `x` of \"\n            \"shape `(batch, in_features)`, using\n            ```python\n            linear = equinox.nn.Linear(...)\n            jax.vmap(linear)(x)\n            ```\n            will produce the appropriate output of shape `(batch, out_features)`.\n\n        **Returns:**\n\n        A JAX array of shape `(out_features,)`. (Or shape `()` if\n        `out_features=\"scalar\"`.)\n        \"\"\"\n\n        x = self.weight @ x\n        x = x + self.bias\n        return x", "\n\nclass Linear(Module):\n    \"\"\"Performs a linear transformation.\"\"\"\n\n    weight: Array\n    bias: Optional[Array]\n    in_features: Union[int, Literal[\"scalar\"]] = static_field()\n    out_features: Union[int, Literal[\"scalar\"]] = static_field()\n    use_bias: bool = static_field()\n\n    def __init__(\n        self,\n        in_features: Union[int, Literal[\"scalar\"]],\n        out_features: Union[int, Literal[\"scalar\"]],\n        use_bias: bool = True,\n        *,\n        key: PRNGKey,\n    ):\n        \"\"\"**Arguments:**\n\n        - `in_features`: The input size. The input to the layer should be a vector of\n            shape `(in_features,)`\n        - `out_features`: The output size. The output from the layer will be a vector\n            of shape `(out_features,)`.\n        - `use_bias`: Whether to add on a bias as well.\n        - `key`: A `jax.random.PRNGKey` used to provide randomness for parameter\n            initialisation. (Keyword only argument.)\n\n        Note that `in_features` also supports the string `\"scalar\"` as a special value.\n        In this case the input to the layer should be of shape `()`.\n\n        Likewise `out_features` can also be a string `\"scalar\"`, in which case the\n        output from the layer will have shape `()`.\n        \"\"\"\n        super().__init__()\n        wkey, bkey = jrandom.split(key, 2)\n        in_features_ = 1 if in_features == \"scalar\" else in_features\n        out_features_ = 1 if out_features == \"scalar\" else out_features\n        std_dev = 1.0 / np.sqrt(in_features)\n        self.weight = jrandom.truncated_normal(wkey, -2.0, 2.0, (out_features_, in_features_)) * std_dev\n        if use_bias:\n            self.bias = jrandom.truncated_normal(bkey, -2.0, 2.0, (out_features_,)) * std_dev\n        else:\n            self.bias = None\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.use_bias = use_bias\n\n    def __call__(self, x: Array, *, key: Optional[PRNGKey] = None) -> Array:\n        \"\"\"**Arguments:**\n\n        - `x`: The input. Should be a JAX array of shape `(in_features,)`. (Or shape\n            `()` if `in_features=\"scalar\"`.)\n        - `key`: Ignored; provided for compatibility with the rest of the Equinox API.\n            (Keyword only argument.)\n\n        !!! info\n\n            If you want to use higher order tensors as inputs (for example featuring \"\n            \"batch dimensions) then use `jax.vmap`. For example, for an input `x` of \"\n            \"shape `(batch, in_features)`, using\n            ```python\n            linear = equinox.nn.Linear(...)\n            jax.vmap(linear)(x)\n            ```\n            will produce the appropriate output of shape `(batch, out_features)`.\n\n        **Returns:**\n\n        A JAX array of shape `(out_features,)`. (Or shape `()` if\n        `out_features=\"scalar\"`.)\n        \"\"\"\n\n        x = self.weight @ x\n        x = x + self.bias\n        return x", "\n\nclass MLP(Module):\n    \"\"\"Standard Multi-Layer Perceptron; also known as a feed-forward network.\n\n    !!! faq\n\n        If you get a TypeError saying an object is not a valid JAX type, see the\n            [FAQ](https://docs.kidger.site/equinox/faq/).\"\"\"\n\n    layers: Tuple[Linear, ...]\n    activation: Callable\n    final_activation: Callable\n    in_size: Union[int, Literal[\"scalar\"]] = static_field()\n    out_size: Union[int, Literal[\"scalar\"]] = static_field()\n    width_size: int = static_field()\n    depth: int = static_field()\n\n    def __init__(\n        self,\n        in_size: Union[int, Literal[\"scalar\"]],\n        out_size: Union[int, Literal[\"scalar\"]],\n        width_size: int,\n        depth: int,\n        activation: Callable = jnn.relu,\n        final_activation: Callable = jnn.relu,\n        *,\n        key: PRNGKey,\n        **kwargs,\n    ):\n        \"\"\"**Arguments**:\n\n        - `in_size`: The input size. The input to the module should be a vector of\n            shape `(in_features,)`\n        - `out_size`: The output size. The output from the module will be a vector\n            of shape `(out_features,)`.\n        - `width_size`: The size of each hidden layer.\n        - `depth`: The number of hidden layers, including the output layer.\n            For example, `depth=2` results in an network with layers:\n            [`Linear(in_size, width_size)`, `Linear(width_size, width_size)`,\n            `Linear(width_size, out_size)`].\n        - `activation`: The activation function after each hidden layer. Defaults to\n            ReLU.\n        - `final_activation`: The activation function after the output layer. Defaults\n            to the identity.\n        - `key`: A `jax.random.PRNGKey` used to provide randomness for parameter\n            initialisation. (Keyword only argument.)\n\n        Note that `in_size` also supports the string `\"scalar\"` as a special value.\n        In this case the input to the module should be of shape `()`.\n\n        Likewise `out_size` can also be a string `\"scalar\"`, in which case the\n        output from the module will have shape `()`.\n        \"\"\"\n\n        super().__init__(**kwargs)\n        keys = jrandom.split(key, depth + 1)\n        layers = []\n        if depth == 0:\n            layers.append(Linear(in_size, out_size, key=keys[0]))\n        else:\n            layers.append(Linear(in_size, width_size, key=keys[0]))\n            for i in range(depth - 1):\n                layers.append(Linear(width_size, width_size, key=keys[i + 1]))\n            layers.append(Linear(width_size, out_size, key=keys[-1]))\n        self.layers = tuple(layers)\n        self.in_size = in_size\n        self.out_size = out_size\n        self.width_size = width_size\n        self.depth = depth\n        self.activation = activation\n        self.final_activation = final_activation\n\n    def __call__(self, x: Array, *, key: Optional[PRNGKey] = None) -> Array:\n        \"\"\"**Arguments:**\n\n        - `x`: A JAX array with shape `(in_size,)`. (Or shape `()` if\n            `in_size=\"scalar\"`.)\n        - `key`: Ignored; provided for compatibility with the rest of the Equinox API.\n            (Keyword only argument.)\n\n        **Returns:**\n\n        A JAX array with shape `(out_size,)`. (Or shape `()` if `out_size=\"scalar\"`.)\n        \"\"\"\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            x = self.activation(x)\n        x = self.layers[-1](x)\n        x = self.final_activation(x)\n        return x", ""]}
{"filename": "utils/misc.py", "chunked_list": ["import flatdict, mlflow, os, boto3, botocore, shutil, pickle, yaml, operator\nfrom urllib.parse import urlparse\nfrom mlflow.tracking import MlflowClient\nimport jax\nimport equinox as eqx\n\n\ndef log_params(cfg):\n    flattened_dict = dict(flatdict.FlatDict(cfg, delimiter=\".\"))\n    num_entries = len(flattened_dict.keys())\n\n    if num_entries > 100:\n        num_batches = num_entries % 100\n        fl_list = list(flattened_dict.items())\n        for i in range(num_batches):\n            end_ind = min((i + 1) * 100, num_entries)\n            trunc_dict = {k: v for k, v in fl_list[i * 100 : end_ind]}\n            mlflow.log_params(trunc_dict)\n    else:\n        mlflow.log_params(flattened_dict)", "\n\ndef get_cfg(artifact_uri, temp_path):\n    dest_file_path = download_file(\"config.yaml\", artifact_uri, temp_path)\n    with open(dest_file_path, \"r\") as file:\n        cfg = yaml.safe_load(file)\n\n    return cfg\n\n\ndef get_weights(artifact_uri, temp_path, models):\n    dest_file_path = download_file(\"weights.eqx\", artifact_uri, temp_path)\n    if dest_file_path is not None:\n        # with open(dest_file_path, \"rb\") as file:\n        #     weights = pickle.load(file)\n        # return weights\n        return eqx.tree_deserialise_leaves(dest_file_path, like=models)\n\n    else:\n        return None", "\n\ndef get_weights(artifact_uri, temp_path, models):\n    dest_file_path = download_file(\"weights.eqx\", artifact_uri, temp_path)\n    if dest_file_path is not None:\n        # with open(dest_file_path, \"rb\") as file:\n        #     weights = pickle.load(file)\n        # return weights\n        return eqx.tree_deserialise_leaves(dest_file_path, like=models)\n\n    else:\n        return None", "\n\ndef download_file(fname, artifact_uri, destination_path):\n    file_uri = mlflow.get_artifact_uri(fname)\n    dest_file_path = os.path.join(destination_path, fname)\n\n    if \"s3\" in artifact_uri:\n        s3 = boto3.client(\"s3\")\n        out = urlparse(file_uri, allow_fragments=False)\n        bucket_name = out.netloc\n        rest_of_path = out.path\n        try:\n            s3.download_file(bucket_name, rest_of_path[1:], dest_file_path)\n        except botocore.exceptions.ClientError as e:\n            return None\n    elif \"file\" in artifact_uri:\n        file_uri = file_uri[7:]\n        if os.path.exists(file_uri):\n            shutil.copyfile(file_uri, dest_file_path)\n        else:\n            return None\n    else:\n        raise NotImplementedError\n\n    return dest_file_path", "\n\ndef is_job_done(run_id):\n    return MlflowClient().get_run(run_id).data.tags[\"status\"] == \"completed\"\n\n\ndef get_this_metric_of_this_run(metric_name, run_id):\n    run = MlflowClient().get_run(run_id)\n    return run.data.metrics[metric_name]\n", "\n\ndef download_and_open_file_from_this_run(fname, run_id, destination_path):\n    mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=fname, dst_path=destination_path)\n    with open(os.path.join(destination_path, fname), \"rb\") as f:\n        this_file = pickle.load(f)\n\n    return this_file\n\n\ndef all_reduce_gradients(gradients, num):\n    if num > 1:\n\n        def _safe_add(a1, a2):\n            if a1 is None:\n                return a2\n            else:\n                return a1 + a2\n\n        def _is_none(x):\n            return x is None\n\n        def _safe_divide(a1):\n            if a1 is None:\n                return a1\n            else:\n                return a1 / num\n\n        summed_gradients = jax.tree_map(_safe_add, gradients[0], gradients[1], is_leaf=_is_none)\n        for i in range(2, num):\n            summed_gradients = jax.tree_map(_safe_add, summed_gradients, gradients[i], is_leaf=_is_none)\n\n        average_gradient = jax.tree_map(_safe_divide, summed_gradients, is_leaf=_is_none)\n    else:\n        average_gradient = gradients[0]\n\n    return average_gradient", "\n\ndef all_reduce_gradients(gradients, num):\n    if num > 1:\n\n        def _safe_add(a1, a2):\n            if a1 is None:\n                return a2\n            else:\n                return a1 + a2\n\n        def _is_none(x):\n            return x is None\n\n        def _safe_divide(a1):\n            if a1 is None:\n                return a1\n            else:\n                return a1 / num\n\n        summed_gradients = jax.tree_map(_safe_add, gradients[0], gradients[1], is_leaf=_is_none)\n        for i in range(2, num):\n            summed_gradients = jax.tree_map(_safe_add, summed_gradients, gradients[i], is_leaf=_is_none)\n\n        average_gradient = jax.tree_map(_safe_divide, summed_gradients, is_leaf=_is_none)\n    else:\n        average_gradient = gradients[0]\n\n    return average_gradient", "\n\ndef get_jq(client: boto3.client, desired_machine: str):\n    queues = client.describe_job_queues()\n    for queue in queues[\"jobQueues\"]:\n        if desired_machine == queue[\"jobQueueName\"]:\n            return queue[\"jobQueueArn\"]\n\n\ndef get_jd(client: boto3.client, sim_type: str, desired_machine: str):\n    jobdefs = client.describe_job_definitions()\n    for jobdef in jobdefs[\"jobDefinitions\"]:\n        if (\n            desired_machine in jobdef[\"jobDefinitionName\"]\n            and jobdef[\"status\"] == \"ACTIVE\"\n            and sim_type in jobdef[\"jobDefinitionName\"]\n        ):\n            return jobdef[\"jobDefinitionArn\"]", "\ndef get_jd(client: boto3.client, sim_type: str, desired_machine: str):\n    jobdefs = client.describe_job_definitions()\n    for jobdef in jobdefs[\"jobDefinitions\"]:\n        if (\n            desired_machine in jobdef[\"jobDefinitionName\"]\n            and jobdef[\"status\"] == \"ACTIVE\"\n            and sim_type in jobdef[\"jobDefinitionName\"]\n        ):\n            return jobdef[\"jobDefinitionArn\"]", "\n\ndef queue_sim(sim_request):\n    client = boto3.client(\"batch\", region_name=\"us-east-1\")\n\n    job_template = {\n        \"jobQueue\": get_jq(client, sim_request[\"machine\"]),\n        \"jobDefinition\": get_jd(client, sim_request[\"sim_type\"], sim_request[\"machine\"]),\n        \"jobName\": sim_request[\"job_name\"],\n        \"parameters\": {\"run_id\": sim_request[\"run_id\"], \"run_type\": sim_request[\"run_type\"]},\n        \"retryStrategy\": {\"attempts\": 10, \"evaluateOnExit\": [{\"action\": \"RETRY\", \"onStatusReason\": \"Host EC2*\"}]},\n    }\n\n    submissionResult = client.submit_job(**job_template)\n\n    return submissionResult", ""]}
{"filename": "tests/test_resonance_search.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n#  research@ergodic.io\nimport yaml, pytest\nfrom itertools import product\n\n\nimport numpy as np\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)", "\nconfig.update(\"jax_enable_x64\", True)\n# config.update(\"jax_debug_nans\", True)\n# config.update(\"jax_disable_jit\", True)\n\nimport jax, mlflow, diffrax, optax\nfrom jax import numpy as jnp\nimport tempfile, time\nfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\nfrom utils import misc", "from diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\nfrom utils import misc\nfrom jaxopt import OptaxSolver\nimport equinox as eqx\nfrom tqdm import tqdm\n\nfrom adept.es1d import helpers\nfrom theory.electrostatic import get_roots_to_electrostatic_dispersion\n\n\ndef load_cfg(rand_k0, gamma, adjoint):\n    with open(\"./tests/configs/resonance_search.yaml\", \"r\") as file:\n        defaults = yaml.safe_load(file)\n\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n    defaults[\"physics\"][\"electron\"][\"gamma\"] = gamma\n    defaults[\"adjoint\"] = adjoint\n\n    if gamma == \"kinetic\":\n        wepw = np.real(get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0))\n        defaults[\"mlflow\"][\"run\"] = \"kinetic\"\n    else:\n        wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n        defaults[\"mlflow\"][\"run\"] = \"bohm-gross\"\n\n    xmax = float(2.0 * np.pi / rand_k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n\n    return defaults, wepw", "\n\ndef load_cfg(rand_k0, gamma, adjoint):\n    with open(\"./tests/configs/resonance_search.yaml\", \"r\") as file:\n        defaults = yaml.safe_load(file)\n\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n    defaults[\"physics\"][\"electron\"][\"gamma\"] = gamma\n    defaults[\"adjoint\"] = adjoint\n\n    if gamma == \"kinetic\":\n        wepw = np.real(get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0))\n        defaults[\"mlflow\"][\"run\"] = \"kinetic\"\n    else:\n        wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n        defaults[\"mlflow\"][\"run\"] = \"bohm-gross\"\n\n    xmax = float(2.0 * np.pi / rand_k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n\n    return defaults, wepw", "\n\n@pytest.mark.parametrize(\"adjoint\", [\"Recursive\", \"Backsolve\"])\n@pytest.mark.parametrize(\"gamma\", [\"kinetic\", 3.0])\ndef test_resonance_search(gamma, adjoint):\n    mlflow.set_experiment(\"test-res-search\")\n    with mlflow.start_run(run_name=\"res-search-opt\") as mlflow_run:\n        vg_func, sim_k0, actual_w0 = get_vg_func(gamma, adjoint)\n\n        mod_defaults, _ = load_cfg(sim_k0, gamma, adjoint)\n        mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n        misc.log_params(mod_defaults)\n        mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n        mod_defaults = helpers.get_save_quantities(mod_defaults)\n\n        rng_key = jax.random.PRNGKey(420)\n\n        w0 = 1.1 + 0.05 * jax.random.normal(rng_key, [1], dtype=jnp.float64)\n\n        # optimizer = optax.adam(0.1)\n        # opt_state = optimizer.init(w0)\n\n        t0 = time.time()\n        optimizer = OptaxSolver(fun=vg_func, value_and_grad=True, has_aux=True, opt=optax.adam(learning_rate=0.1))\n        opt_state = optimizer.init_state(w0)\n        mlflow.log_metrics({\"init_time\": round(time.time() - t0, 4)})\n\n        mlflow.log_metrics({\"w0\": float(w0)}, step=0)\n        mlflow.log_metrics({\"actual_w0\": actual_w0}, step=0)\n        for i in tqdm(range(40)):\n            w0, opt_state, loss = run_one_step(i, w0, vg_func, mod_defaults, optimizer, opt_state)\n\n            mlflow.log_metrics({\"w0\": float(w0), \"actual_w0\": actual_w0, \"loss\": float(loss)}, step=i + 1)\n\n        print(f\"{gamma=}, {adjoint=}\")\n        print(f\"{actual_w0=}, {float(w0)=}\")\n\n        np.testing.assert_allclose(actual_w0, float(w0), rtol=0.03)", "\n\ndef run_one_step(i, w0, vg_func, mod_defaults, optimizer, opt_state):\n    with mlflow.start_run(run_name=f\"res-search-run-{i}\", nested=True) as mlflow_run:\n        mlflow.log_param(\"w0\", w0)\n        t0 = time.time()\n\n        # (loss, results), grad = vg_func(w0)\n        # updates, opt_state = optimizer.update(grad, opt_state, w0)\n        # w0 = optax.apply_updates(w0, updates)\n\n        w0, opt_state = optimizer.update(params=w0, state=opt_state)\n        loss = opt_state.error\n        results = opt_state.aux\n\n        mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4)})\n        with tempfile.TemporaryDirectory() as td:\n            t0 = time.time()\n            helpers.post_process(results, mod_defaults, td)\n            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4), \"loss\": float(loss)})\n            # log artifacts\n            mlflow.log_artifacts(td)\n\n    return w0, opt_state, loss", "\n\ndef get_vg_func(gamma, adjoint):\n    rng = np.random.default_rng(420)\n    sim_k0 = rng.uniform(0.26, 0.4)\n\n    defaults, actual_w0 = load_cfg(sim_k0, gamma, adjoint)\n    defaults[\"grid\"] = helpers.get_derived_quantities(defaults[\"grid\"])\n    misc.log_params(defaults)\n\n    defaults[\"grid\"] = helpers.get_solver_quantities(defaults[\"grid\"])\n    defaults = helpers.get_save_quantities(defaults)\n\n    pulse_dict = {\"driver\": defaults[\"drivers\"]}\n    state = helpers.init_state(defaults)\n    loss_fn = get_loss(state, pulse_dict, defaults)\n    vg_func = eqx.filter_jit(jax.value_and_grad(loss_fn, argnums=0, has_aux=True))\n\n    return vg_func, sim_k0, actual_w0", "\n\ndef get_loss(state, pulse_dict, mod_defaults):\n    if mod_defaults[\"adjoint\"] == \"Recursive\":\n        adjoint = diffrax.RecursiveCheckpointAdjoint()\n    elif mod_defaults[\"adjoint\"] == \"Backsolve\":\n        adjoint = diffrax.BacksolveAdjoint(solver=Tsit5())\n    else:\n        raise NotImplementedError\n\n    def loss(w0):\n        pulse_dict[\"driver\"][\"ex\"][\"0\"][\"w0\"] = w0\n        vf = helpers.VectorField(mod_defaults, models=False)\n        results = diffeqsolve(\n            terms=ODETerm(vf),\n            solver=Tsit5(),\n            t0=mod_defaults[\"grid\"][\"tmin\"],\n            t1=mod_defaults[\"grid\"][\"tmax\"],\n            max_steps=mod_defaults[\"grid\"][\"max_steps\"],\n            dt0=mod_defaults[\"grid\"][\"dt\"],\n            adjoint=adjoint,\n            y0=state,\n            args=pulse_dict,\n            saveat=SaveAt(ts=mod_defaults[\"save\"][\"t\"][\"ax\"]),\n        )\n        nk1 = jnp.abs(jnp.fft.fft(results.ys[\"electron\"][\"n\"], axis=1)[:, 1])\n        return -jnp.amax(nk1), results\n\n    return loss", "\n\nif __name__ == \"__main__\":\n    for gamma, adjoint in product([\"kinetic\", 3.0], [\"Recursive\", \"Backsolve\"]):\n        test_resonance_search(gamma, adjoint)\n"]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_resonance.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n#  research@ergodic.io\nimport yaml, pytest\n\nimport numpy as np\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n# config.update(\"jax_disable_jit\", True)\n", "# config.update(\"jax_disable_jit\", True)\n\nfrom jax import numpy as jnp\nimport mlflow\n\nfrom theory import electrostatic\nfrom utils.runner import run\n\n\ndef _modify_defaults_(defaults, rng, gamma):\n    rand_k0 = np.round(rng.uniform(0.25, 0.4), 3)\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n    defaults[\"physics\"][\"electron\"][\"gamma\"] = gamma\n    if gamma == \"kinetic\":\n        root = np.real(electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0))\n        defaults[\"mlflow\"][\"run\"] = \"kinetic\"\n    else:\n        root = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n        defaults[\"mlflow\"][\"run\"] = \"bohm-gross\"\n\n    xmax = float(2.0 * np.pi / rand_k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"mlflow\"][\"experiment\"] = \"test-resonance\"\n\n    return defaults, float(root)", "\ndef _modify_defaults_(defaults, rng, gamma):\n    rand_k0 = np.round(rng.uniform(0.25, 0.4), 3)\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n    defaults[\"physics\"][\"electron\"][\"gamma\"] = gamma\n    if gamma == \"kinetic\":\n        root = np.real(electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0))\n        defaults[\"mlflow\"][\"run\"] = \"kinetic\"\n    else:\n        root = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n        defaults[\"mlflow\"][\"run\"] = \"bohm-gross\"\n\n    xmax = float(2.0 * np.pi / rand_k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"mlflow\"][\"experiment\"] = \"test-resonance\"\n\n    return defaults, float(root)", "\n\n@pytest.mark.parametrize(\"gamma\", [\"kinetic\", 3.0])\ndef test_single_resonance(gamma):\n    with open(\"./tests/configs/resonance.yaml\", \"r\") as file:\n        defaults = yaml.safe_load(file)\n\n    # modify config\n    rng = np.random.default_rng()\n    mod_defaults, actual_resonance = _modify_defaults_(defaults, rng, gamma)\n\n    # run\n    mlflow.set_experiment(mod_defaults[\"mlflow\"][\"experiment\"])\n    # modify config\n    with mlflow.start_run(run_name=mod_defaults[\"mlflow\"][\"run\"]) as mlflow_run:\n        result = run(mod_defaults)\n\n    kx = (\n        np.fft.fftfreq(\n            mod_defaults[\"save\"][\"x\"][\"nx\"], d=mod_defaults[\"save\"][\"x\"][\"ax\"][2] - mod_defaults[\"save\"][\"x\"][\"ax\"][1]\n        )\n        * 2.0\n        * np.pi\n    )\n    one_over_kx = np.zeros_like(kx)\n    one_over_kx[1:] = 1.0 / kx[1:]\n    efs = jnp.real(\n        jnp.fft.ifft(\n            1j\n            * one_over_kx[None, :]\n            * jnp.fft.fft(result.ys[\"x\"][\"ion\"][\"n\"][:, :] - result.ys[\"x\"][\"electron\"][\"n\"][:, :])\n        )\n    )\n    ek1 = np.fft.fft(efs, axis=1)[:, 1]\n    env, freq = electrostatic.get_nlfs(ek1, result.ts[1] - result.ts[0])\n    frslc = slice(-80, -10)\n    print(\n        f\"Frequency check \\n\"\n        f\"measured: {np.round(np.mean(freq[frslc]), 5)}, \"\n        f\"desired: {np.round(actual_resonance, 5)}, \"\n    )\n    measured_resonance = np.mean(freq[frslc])\n    np.testing.assert_almost_equal(measured_resonance, actual_resonance, decimal=2)", "\n\nif __name__ == \"__main__\":\n    for gamma in [\"kinetic\", 3.0]:\n        test_single_resonance(gamma)\n"]}
{"filename": "tests/test_landau_damping.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n#  research@ergodic.io\nimport yaml\n\nimport numpy as np\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n# config.update(\"jax_disable_jit\", True)\n", "# config.update(\"jax_disable_jit\", True)\n\nfrom jax import numpy as jnp\nimport mlflow\n\nfrom theory import electrostatic\nfrom utils.runner import run\n\n\ndef _modify_defaults_(defaults, rng):\n    rand_k0 = np.round(rng.uniform(0.25, 0.4), 3)\n\n    wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n    root = electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0)\n    print(rand_k0, wepw, root)\n\n    defaults[\"physics\"][\"landau_damping\"] = True\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = float(wepw)\n    xmax = float(2.0 * np.pi / rand_k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"kx\"][\"kxmax\"] = rand_k0\n    defaults[\"mlflow\"][\"experiment\"] = \"test-landau-damping\"\n\n    return defaults, float(np.imag(root))", "\ndef _modify_defaults_(defaults, rng):\n    rand_k0 = np.round(rng.uniform(0.25, 0.4), 3)\n\n    wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n    root = electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0)\n    print(rand_k0, wepw, root)\n\n    defaults[\"physics\"][\"landau_damping\"] = True\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = float(wepw)\n    xmax = float(2.0 * np.pi / rand_k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"kx\"][\"kxmax\"] = rand_k0\n    defaults[\"mlflow\"][\"experiment\"] = \"test-landau-damping\"\n\n    return defaults, float(np.imag(root))", "\n\ndef test_single_resonance():\n    with open(\"./tests/configs/resonance.yaml\", \"r\") as file:\n        defaults = yaml.safe_load(file)\n\n    # modify config\n    rng = np.random.default_rng()\n    mod_defaults, actual_damping_rate = _modify_defaults_(defaults, rng)\n\n    # run\n    mlflow.set_experiment(mod_defaults[\"mlflow\"][\"experiment\"])\n    # modify config\n    with mlflow.start_run(run_name=mod_defaults[\"mlflow\"][\"run\"]) as mlflow_run:\n        result = run(mod_defaults)\n\n    kx = (\n        np.fft.fftfreq(\n            mod_defaults[\"save\"][\"x\"][\"nx\"], d=mod_defaults[\"save\"][\"x\"][\"ax\"][2] - mod_defaults[\"save\"][\"x\"][\"ax\"][1]\n        )\n        * 2.0\n        * np.pi\n    )\n    one_over_kx = np.zeros_like(kx)\n    one_over_kx[1:] = 1.0 / kx[1:]\n    efs = jnp.real(jnp.fft.ifft(1j * one_over_kx[None, :] * jnp.fft.fft(1 - result.ys[\"x\"][\"electron\"][\"n\"][:, :])))\n    ek1 = (2.0 / mod_defaults[\"grid\"][\"nx\"] * np.abs(np.fft.fft(efs, axis=1)[:, 1])) ** 2.0\n    frslc = slice(-100, -50)\n    measured_damping_rate = np.mean(np.gradient(ek1[frslc], (result.ts[1] - result.ts[0])) / ek1[frslc])\n    print(\n        f\"Landau Damping rate check \\n\"\n        f\"measured: {np.round(measured_damping_rate, 5)}, \"\n        f\"actual: {np.round(2*actual_damping_rate, 5)}, \"\n    )\n\n    np.testing.assert_almost_equal(measured_damping_rate, 2 * actual_damping_rate, decimal=2)", "\n\nif __name__ == \"__main__\":\n    test_single_resonance()\n"]}
{"filename": "tests/test_against_vlasov.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n#  research@ergodic.io\nimport yaml\n\nimport numpy as np\nfrom jax.config import config\n\nconfig.update(\"jax_enable_x64\", True)\n# config.update(\"jax_disable_jit\", True)\n", "# config.update(\"jax_disable_jit\", True)\n\nfrom jax import numpy as jnp\nimport mlflow\nimport xarray as xr\n\nfrom theory import electrostatic\nfrom utils.runner import run\n\n\ndef _modify_defaults_(defaults):\n    rand_k0 = 0.358\n\n    wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n    root = electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0)\n\n    defaults[\"physics\"][\"landau_damping\"] = True\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = float(wepw)\n    xmax = float(2.0 * np.pi / rand_k0)\n    # defaults[\"save\"][\"field\"][\"xmax_to_store\"] = float(2.0 * np.pi / rand_k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"kx\"][\"kxmax\"] = rand_k0\n    defaults[\"mlflow\"][\"experiment\"] = \"test-against-vlasov\"\n\n    return defaults, float(np.imag(root))", "\n\ndef _modify_defaults_(defaults):\n    rand_k0 = 0.358\n\n    wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n    root = electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0)\n\n    defaults[\"physics\"][\"landau_damping\"] = True\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = float(wepw)\n    xmax = float(2.0 * np.pi / rand_k0)\n    # defaults[\"save\"][\"field\"][\"xmax_to_store\"] = float(2.0 * np.pi / rand_k0)\n    defaults[\"grid\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n    defaults[\"save\"][\"kx\"][\"kxmax\"] = rand_k0\n    defaults[\"mlflow\"][\"experiment\"] = \"test-against-vlasov\"\n\n    return defaults, float(np.imag(root))", "\n\ndef test_single_resonance():\n    with open(\"./tests/configs/vlasov_comparison.yaml\", \"r\") as file:\n        defaults = yaml.safe_load(file)\n\n    # modify config\n    mod_defaults, actual_damping_rate = _modify_defaults_(defaults)\n\n    # run\n    mlflow.set_experiment(mod_defaults[\"mlflow\"][\"experiment\"])\n    # modify config\n    with mlflow.start_run(run_name=mod_defaults[\"mlflow\"][\"run\"]) as mlflow_run:\n        result = run(mod_defaults)\n\n    vds = xr.open_dataset(\"tests/vlasov-reference/all-fields-kx.nc\", engine=\"h5netcdf\")\n\n    nk1_fluid = result.ys[\"kx\"][\"electron\"][\"n\"][\"mag\"][:, 1]\n    nk1_vlasov = vds[\"n-(k_x)\"][:, 1].data\n    t_fluid = result.ts\n    t_vlasov = vds.coords[\"t\"].data\n    fluid_slc = slice(80, 160)\n    vlasov_slc = slice(700, 850)\n\n    vlasov_damping_rate = np.mean(\n        np.gradient(nk1_vlasov[vlasov_slc], (t_vlasov[1] - t_vlasov[0])) / nk1_vlasov[vlasov_slc]\n    )\n    fluid_damping_rate = np.mean(np.gradient(nk1_fluid[fluid_slc], (t_fluid[1] - t_fluid[0])) / nk1_fluid[fluid_slc])\n\n    print(f\"{vlasov_damping_rate=}, {fluid_damping_rate=}\")\n    print(f\"{np.amax(nk1_vlasov)=}, {np.amax(nk1_fluid)=}\")\n\n    np.testing.assert_almost_equal(vlasov_damping_rate, fluid_damping_rate, decimal=2)\n    np.testing.assert_allclose(np.amax(nk1_fluid), np.amax(nk1_vlasov), rtol=0.05)", "\n\nif __name__ == \"__main__\":\n    test_single_resonance()\n"]}
{"filename": "adept/__init__.py", "chunked_list": [""]}
{"filename": "adept/es1d/pushers.py", "chunked_list": ["from typing import Dict\n\nimport jax\nfrom jax import numpy as jnp\nimport numpy as np\nimport equinox as eqx\n\nfrom theory.electrostatic import get_complex_frequency_table\n\n\ndef get_envelope(p_wL, p_wR, p_L, p_R, ax):\n    return 0.5 * (jnp.tanh((ax - p_L) / p_wL) - jnp.tanh((ax - p_R) / p_wR))", "\n\ndef get_envelope(p_wL, p_wR, p_L, p_R, ax):\n    return 0.5 * (jnp.tanh((ax - p_L) / p_wL) - jnp.tanh((ax - p_R) / p_wR))\n\n\nclass WaveSolver(eqx.Module):\n    dx: float\n    c: float\n    c_sq: float\n    dt: float\n    const: float\n    one_over_const: float\n\n    def __init__(self, c: jnp.float64, dx: jnp.float64, dt: jnp.float64):\n        self.dx = dx\n        self.c = c\n        self.c_sq = c**2.0\n        c_over_dx = c / dx\n        self.dt = dt\n        self.const = c_over_dx * dt\n        # abc_const = (const - 1.0) / (const + 1.0)\n        self.one_over_const = 1.0 / dt / c_over_dx\n\n    def apply_2nd_order_abc(self, aold, a, anew):\n        \"\"\"\n        Second order absorbing boundary conditions\n\n        :param aold:\n        :param a:\n        :param anew:\n        :param dt:\n        :return:\n        \"\"\"\n\n        coeff = -1.0 / (self.one_over_const + 2.0 + self.const)\n\n        # # 2nd order ABC\n        a_left = (self.one_over_const - 2.0 + self.const) * (anew[1] + aold[0])\n        a_left += 2.0 * (self.const - self.one_over_const) * (a[0] + a[2] - anew[0] - aold[1])\n        a_left -= 4.0 * (self.one_over_const + self.const) * a[1]\n        a_left *= coeff\n        a_left -= aold[2]\n        a_left = jnp.array([a_left])\n\n        a_right = (self.one_over_const - 2.0 + self.const) * (anew[-2] + aold[-1])\n        a_right += 2.0 * (self.const - self.one_over_const) * (a[-1] + a[-3] - anew[-1] - aold[-2])\n        a_right -= 4.0 * (self.one_over_const + self.const) * a[-2]\n        a_right *= coeff\n        a_right -= aold[-3]\n        a_right = jnp.array([a_right])\n\n        # commenting out first order damping\n        # a_left = jnp.array([a[1] + abc_const * (anew[0] - a[0])])\n        # a_right = jnp.array([a[-2] + abc_const * (anew[-1] - a[-1])])\n\n        return jnp.concatenate([a_left, anew, a_right])\n\n    def __call__(self, a: jnp.ndarray, aold: jnp.ndarray, djy_array: jnp.ndarray, electron_charge: jnp.ndarray):\n        if self.c > 0:\n            d2dx2 = (a[:-2] - 2.0 * a[1:-1] + a[2:]) / self.dx**2.0\n            anew = (\n                2.0 * a[1:-1]\n                - aold[1:-1]\n                + self.dt**2.0 * (self.c_sq * d2dx2 - electron_charge * a[1:-1] + djy_array)\n            )\n            return self.apply_2nd_order_abc(aold, a, anew), a\n        else:\n            return a, aold", "\n\nclass Driver(eqx.Module):\n    xax: jax.Array\n\n    def __init__(self, xax):\n        self.xax = xax\n\n    def __call__(self, this_pulse: Dict, current_time: jnp.float64):\n        kk = this_pulse[\"k0\"]\n        ww = this_pulse[\"w0\"]\n        dw = this_pulse[\"dw0\"]\n        t_L = this_pulse[\"t_c\"] - this_pulse[\"t_w\"] * 0.5\n        t_R = this_pulse[\"t_c\"] + this_pulse[\"t_w\"] * 0.5\n        t_wL = this_pulse[\"t_r\"]\n        t_wR = this_pulse[\"t_r\"]\n        x_L = this_pulse[\"x_c\"] - this_pulse[\"x_w\"] * 0.5\n        x_R = this_pulse[\"x_c\"] + this_pulse[\"x_w\"] * 0.5\n        x_wL = this_pulse[\"x_r\"]\n        x_wR = this_pulse[\"x_r\"]\n        envelope_t = get_envelope(t_wL, t_wR, t_L, t_R, current_time)\n        envelope_x = get_envelope(x_wL, x_wR, x_L, x_R, self.xax)\n\n        return (\n            envelope_t * envelope_x * jnp.abs(kk) * this_pulse[\"a0\"] * jnp.sin(kk * self.xax - (ww + dw) * current_time)\n        )", "\n\nclass PoissonSolver(eqx.Module):\n    one_over_kx: jax.Array\n\n    def __init__(self, one_over_kx):\n        self.one_over_kx = one_over_kx\n\n    def __call__(self, dn):\n        return jnp.real(jnp.fft.ifft(1j * self.one_over_kx * jnp.fft.fft(dn)))", "\n\nclass StepAmpere(eqx.Module):\n    def __call__(self, n, u):\n        return n * u\n\n\ndef gradient(arr, kx):\n    return jnp.real(jnp.fft.ifft(1j * kx * jnp.fft.fft(arr)))\n", "\n\nclass DensityStepper(eqx.Module):\n    kx: jax.Array\n\n    def __init__(self, kx):\n        self.kx = kx\n\n    def __call__(self, n, u):\n        return -u * gradient(n, self.kx) - n * gradient(u, self.kx)", "\n\nclass VelocityStepper(eqx.Module):\n    kx: jax.Array\n    wr_corr: jax.Array\n    wis: jax.Array\n\n    def __init__(self, kx, kxr, one_over_kxr, physics):\n        self.kx = kx\n\n        wrs, wis, klds = get_complex_frequency_table(1024, True if physics[\"gamma\"] == \"kinetic\" else False)\n        wrs = jnp.array(jnp.interp(kxr, klds, wrs, left=1.0, right=wrs[-1]))\n\n        if physics[\"gamma\"] == \"kinetic\":\n            self.wr_corr = (jnp.square(wrs) - 1.0) * one_over_kxr**2.0\n        else:\n            self.wr_corr = 1.0\n\n        if physics[\"landau_damping\"]:\n            self.wis = jnp.array(jnp.interp(kxr, klds, wis, left=0.0, right=wis[-1]))\n        else:\n            self.wis = jnp.zeros_like(kxr)\n\n    def landau_damping_term(self, u):\n        return 2 * jnp.real(jnp.fft.irfft(self.wis * jnp.fft.rfft(u)))\n\n    def restoring_force_term(self, gradp_over_nm):\n        return jnp.real(jnp.fft.irfft(self.wr_corr * jnp.fft.rfft(gradp_over_nm)))\n\n    def __call__(self, n, u, p_over_m, q_over_m_times_e, delta):\n        return (\n            -u * gradient(u, self.kx)\n            - self.restoring_force_term(gradient(p_over_m, self.kx) / n)\n            - q_over_m_times_e\n            + self.landau_damping_term(u) / (1.0 + delta**2)\n        )", "\n\nclass EnergyStepper(eqx.Module):\n    kx: jax.Array\n    gamma: float\n\n    def __init__(self, kx, physics):\n        self.kx = kx\n        if physics[\"gamma\"] == \"kinetic\":\n            self.gamma = 1.0\n        else:\n            self.gamma = physics[\"gamma\"]\n\n    def __call__(self, n, u, p_over_m, q_over_m_times_e):\n        return (\n            -u * gradient(p_over_m, self.kx)\n            - self.gamma * p_over_m * gradient(u, self.kx)\n            - 2 * n * u * q_over_m_times_e\n        )", "\n\nclass ParticleTrapper(eqx.Module):\n    kxr: np.ndarray\n    kx: jax.Array\n    model_kld: float\n    wrs: jax.Array\n    wis: jax.Array\n    table_klds: jax.Array\n    norm_kld: jnp.float64\n    norm_nuee: jnp.float64\n    vph: jnp.float64\n    nu_g_model: eqx.Module\n    # nu_d_model: eqx.Module\n\n    def __init__(self, cfg, species=\"electron\", models=None):\n        nuee = cfg[\"physics\"][species][\"trapping\"][\"nuee\"]\n        if cfg[\"physics\"][species][\"gamma\"] == \"kinetic\":\n            kinetic_real_epw = True\n        else:\n            kinetic_real_epw = False\n\n        self.kxr = cfg[\"grid\"][\"kxr\"]\n        self.kx = cfg[\"grid\"][\"kx\"]\n        table_wrs, table_wis, table_klds = get_complex_frequency_table(1024, kinetic_real_epw)\n        self.model_kld = cfg[\"physics\"][species][\"trapping\"][\"kld\"]\n        self.wrs = jnp.interp(cfg[\"grid\"][\"kxr\"], table_klds, table_wrs, left=1.0, right=table_wrs[-1])\n        self.wis = jnp.interp(cfg[\"grid\"][\"kxr\"], table_klds, table_wis, left=0.0, right=0.0)\n        self.table_klds = table_klds\n        self.norm_kld = (self.model_kld - 0.26) / 0.14\n        self.norm_nuee = (jnp.log10(nuee) + 7.0) / -4.0\n        self.vph = jnp.interp(self.model_kld, table_klds, table_wrs, left=1.0, right=table_wrs[-1]) / self.model_kld\n\n        # Make models\n        if models:\n            self.nu_g_model = models[\"nu_g\"]\n        else:\n            self.nu_g_model = lambda x: 1e-3\n\n    def __call__(self, e, delta, args):\n        ek = jnp.fft.rfft(e, axis=0) * 2.0 / self.kx.size\n        norm_e = (jnp.log10(jnp.interp(self.model_kld, self.kxr, jnp.abs(ek)) + 1e-10) + 10.0) / -10.0\n        func_inputs = jnp.stack([norm_e, self.norm_kld, self.norm_nuee], axis=-1)\n        # jax.debug.print(\"{x}\", x=func_inputs)\n        growth_rates = 10 ** (3 * jnp.squeeze(self.nu_g_model(func_inputs)))\n\n        return -self.vph * gradient(delta, self.kx) + growth_rates * jnp.abs(\n            jnp.fft.irfft(ek * self.kx.size / 2.0 * self.wis)\n        ) / (1.0 + delta**2.0)", ""]}
{"filename": "adept/es1d/__init__.py", "chunked_list": ["\n"]}
{"filename": "adept/es1d/helpers.py", "chunked_list": ["from collections import defaultdict\nfrom typing import Callable, Dict\nfrom functools import partial\n\nimport os\n\nimport jax.random\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport xarray as xr", "from matplotlib import pyplot as plt\nimport xarray as xr\nfrom jax import tree_util as jtu\nfrom flatdict import FlatDict\nimport equinox as eqx\n\nfrom jax import numpy as jnp\nfrom adept.es1d import pushers\nfrom utils import nn\n", "from utils import nn\n\n\ndef save_arrays(result, td, cfg, label):\n    if label is None:\n        label = \"x\"\n        flattened_dict = dict(FlatDict(result.ys, delimiter=\"-\"))\n    else:\n        flattened_dict = dict(FlatDict(result.ys[label], delimiter=\"-\"))\n    data_vars = {\n        k: xr.DataArray(v, coords=((\"t\", cfg[\"save\"][\"t\"][\"ax\"]), (label, cfg[\"save\"][label][\"ax\"])))\n        for k, v in flattened_dict.items()\n    }\n\n    saved_arrays_xr = xr.Dataset(data_vars)\n    saved_arrays_xr.to_netcdf(os.path.join(td, \"binary\", f\"state_vs_{label}.nc\"))\n    return saved_arrays_xr", "\n\ndef plot_xrs(which, td, xrs):\n    os.makedirs(os.path.join(td, \"plots\", which))\n    os.makedirs(os.path.join(td, \"plots\", which, \"ion\"))\n    os.makedirs(os.path.join(td, \"plots\", which, \"electron\"))\n\n    for k, v in xrs.items():\n        fname = f\"{'-'.join(k.split('-')[1:])}.png\"\n        fig, ax = plt.subplots(1, 1, figsize=(7, 4), tight_layout=True)\n        v.plot(ax=ax, cmap=\"gist_ncar\")\n        ax.grid()\n        fig.savefig(os.path.join(td, \"plots\", which, k.split(\"-\")[0], fname), bbox_inches=\"tight\")\n        plt.close(fig)\n\n        if which == \"kx\":\n            os.makedirs(os.path.join(td, \"plots\", which, \"ion\", \"hue\"), exist_ok=True)\n            os.makedirs(os.path.join(td, \"plots\", which, \"electron\", \"hue\"), exist_ok=True)\n            # only plot\n            if v.coords[\"kx\"].size > 8:\n                hue_skip = v.coords[\"kx\"].size // 8\n            else:\n                hue_skip = 1\n\n            for log in [True, False]:\n                fig, ax = plt.subplots(1, 1, figsize=(7, 4), tight_layout=True)\n                v[:, ::hue_skip].plot(ax=ax, hue=\"kx\")\n                ax.set_yscale(\"log\" if log else \"linear\")\n                ax.grid()\n                fig.savefig(\n                    os.path.join(\n                        td, \"plots\", which, k.split(\"-\")[0], f\"hue\", f\"{'-'.join(k.split('-')[1:])}-log-{log}.png\"\n                    ),\n                    bbox_inches=\"tight\",\n                )\n                plt.close(fig)", "\n\ndef post_process(result, cfg: Dict, td: str) -> None:\n    os.makedirs(os.path.join(td, \"binary\"))\n    os.makedirs(os.path.join(td, \"plots\"))\n\n    if cfg[\"save\"][\"func\"][\"is_on\"]:\n        if cfg[\"save\"][\"x\"][\"is_on\"]:\n            xrs = save_arrays(result, td, cfg, label=\"x\")\n            plot_xrs(\"x\", td, xrs)\n\n        if cfg[\"save\"][\"kx\"][\"is_on\"]:\n            xrs = save_arrays(result, td, cfg, label=\"kx\")\n            plot_xrs(\"kx\", td, xrs)\n    else:\n        xrs = save_arrays(result, td, cfg, label=None)\n        plot_xrs(\"x\", td, xrs)", "\n\ndef get_derived_quantities(cfg_grid: Dict) -> Dict:\n    \"\"\"\n    This function just updates the config with the derived quantities that are only integers or strings.\n\n    This is run prior to the log params step\n\n    :param cfg_grid:\n    :return:\n    \"\"\"\n    cfg_grid[\"dx\"] = cfg_grid[\"xmax\"] / cfg_grid[\"nx\"]\n    cfg_grid[\"dt\"] = 0.05 * cfg_grid[\"dx\"]\n    cfg_grid[\"nt\"] = int(cfg_grid[\"tmax\"] / cfg_grid[\"dt\"] + 1)\n    cfg_grid[\"tmax\"] = cfg_grid[\"dt\"] * cfg_grid[\"nt\"]\n\n    if cfg_grid[\"nt\"] > 1e6:\n        cfg_grid[\"max_steps\"] = int(1e6)\n        print(r\"Only running $10^6$ steps\")\n    else:\n        cfg_grid[\"max_steps\"] = cfg_grid[\"nt\"] + 4\n\n    return cfg_grid", "\n\ndef get_solver_quantities(cfg_grid: Dict) -> Dict:\n    \"\"\"\n    This function just updates the config with the derived quantities that are arrays\n\n    This is run after the log params step\n\n    :param cfg_grid:\n    :return:\n    \"\"\"\n    cfg_grid = {\n        **cfg_grid,\n        **{\n            \"x\": jnp.linspace(\n                cfg_grid[\"xmin\"] + cfg_grid[\"dx\"] / 2, cfg_grid[\"xmax\"] - cfg_grid[\"dx\"] / 2, cfg_grid[\"nx\"]\n            ),\n            \"t\": jnp.linspace(0, cfg_grid[\"tmax\"], cfg_grid[\"nt\"]),\n            \"kx\": jnp.fft.fftfreq(cfg_grid[\"nx\"], d=cfg_grid[\"dx\"]) * 2.0 * np.pi,\n            \"kxr\": jnp.fft.rfftfreq(cfg_grid[\"nx\"], d=cfg_grid[\"dx\"]) * 2.0 * np.pi,\n        },\n    }\n\n    one_over_kx = np.zeros_like(cfg_grid[\"kx\"])\n    one_over_kx[1:] = 1.0 / cfg_grid[\"kx\"][1:]\n    cfg_grid[\"one_over_kx\"] = jnp.array(one_over_kx)\n\n    one_over_kxr = np.zeros_like(cfg_grid[\"kxr\"])\n    one_over_kxr[1:] = 1.0 / cfg_grid[\"kxr\"][1:]\n    cfg_grid[\"one_over_kxr\"] = jnp.array(one_over_kxr)\n\n    return cfg_grid", "\n\ndef get_save_quantities(cfg: Dict) -> Dict:\n    \"\"\"\n    This function updates the config with the quantities required for the diagnostics and saving routines\n\n    :param cfg:\n    :return:\n    \"\"\"\n    cfg[\"save\"][\"func\"] = {**cfg[\"save\"][\"func\"], **{\"callable\": get_save_func(cfg)}}\n    cfg[\"save\"][\"t\"][\"ax\"] = jnp.linspace(cfg[\"save\"][\"t\"][\"tmin\"], cfg[\"save\"][\"t\"][\"tmax\"], cfg[\"save\"][\"t\"][\"nt\"])\n\n    return cfg", "\n\ndef init_state(cfg: Dict) -> Dict:\n    \"\"\"\n    This function initializes the state\n\n    :param cfg:\n    :return:\n    \"\"\"\n    state = {}\n    for species in [\"ion\", \"electron\"]:\n        state[species] = dict(\n            n=jnp.ones(cfg[\"grid\"][\"nx\"]),\n            p=jnp.full(cfg[\"grid\"][\"nx\"], cfg[\"physics\"][species][\"T0\"]),\n            u=jnp.zeros(cfg[\"grid\"][\"nx\"]),\n            delta=jnp.zeros(cfg[\"grid\"][\"nx\"]),\n        )\n\n    return state", "\n\nclass VectorField(eqx.Module):\n    \"\"\"\n    This function returns the function that defines $d_state / dt$\n\n    All the pushers are chosen and initialized here and a single time-step is defined here.\n\n    We use the time-integrators provided by diffrax, and therefore, only need $d_state / dt$ here\n\n    :param cfg:\n    :return:\n    \"\"\"\n\n    cfg: Dict\n    pusher_dict: Dict\n    push_driver: Callable\n    poisson_solver: Callable\n\n    def __init__(self, cfg, models):\n        super().__init__()\n        self.cfg = cfg\n        self.pusher_dict = {\"ion\": {}, \"electron\": {}}\n        for species_name in [\"ion\", \"electron\"]:\n            self.pusher_dict[species_name][\"push_n\"] = pushers.DensityStepper(cfg[\"grid\"][\"kx\"])\n            self.pusher_dict[species_name][\"push_u\"] = pushers.VelocityStepper(\n                cfg[\"grid\"][\"kx\"], cfg[\"grid\"][\"kxr\"], cfg[\"grid\"][\"one_over_kxr\"], cfg[\"physics\"][species_name]\n            )\n            self.pusher_dict[species_name][\"push_e\"] = pushers.EnergyStepper(\n                cfg[\"grid\"][\"kx\"], cfg[\"physics\"][species_name]\n            )\n            if cfg[\"physics\"][species_name][\"trapping\"][\"is_on\"]:\n                self.pusher_dict[species_name][\"particle_trapper\"] = pushers.ParticleTrapper(cfg, species_name, models)\n\n        self.push_driver = pushers.Driver(cfg[\"grid\"][\"x\"])\n        # if \"ey\" in self.cfg[\"drivers\"]:\n        #     self.wave_solver = pushers.WaveSolver(cfg[\"grid\"][\"c\"], cfg[\"grid\"][\"dx\"], cfg[\"grid\"][\"dt\"])\n        self.poisson_solver = pushers.PoissonSolver(cfg[\"grid\"][\"one_over_kx\"])\n\n    def __call__(self, t: float, y: Dict, args: Dict):\n        \"\"\"\n        This function is used by the time integrators specified in diffrax\n\n        :param t:\n        :param y:\n        :param args:\n        :return:\n        \"\"\"\n        e = self.poisson_solver(\n            self.cfg[\"physics\"][\"ion\"][\"charge\"] * y[\"ion\"][\"n\"]\n            + self.cfg[\"physics\"][\"electron\"][\"charge\"] * y[\"electron\"][\"n\"]\n        )\n        ed = 0.0\n\n        for p_ind in self.cfg[\"drivers\"][\"ex\"].keys():\n            ed += self.push_driver(args[\"driver\"][\"ex\"][p_ind], t)\n\n        # if \"ey\" in self.cfg[\"drivers\"]:\n        #     ad = 0.0\n        #     for p_ind in self.cfg[\"drivers\"][\"ey\"].keys():\n        #         ad += self.push_driver(args[\"pulse\"][\"ey\"][p_ind], t)\n        #     a = self.wave_solver(a, aold, djy_array, charge)\n        #     total_a = y[\"a\"] + ad\n        #     ponderomotive_force = -0.5 * jnp.gradient(jnp.square(total_a), self.cfg[\"grid\"][\"dx\"])[1:-1]\n\n        total_e = e + ed  # + ponderomotive_force\n\n        dstate_dt = {\"ion\": {}, \"electron\": {}}\n        for species_name in [\"ion\", \"electron\"]:\n            n = y[species_name][\"n\"]\n            u = y[species_name][\"u\"]\n            p = y[species_name][\"p\"]\n            delta = y[species_name][\"delta\"]\n            if self.cfg[\"physics\"][species_name][\"is_on\"]:\n                q_over_m = self.cfg[\"physics\"][species_name][\"charge\"] / self.cfg[\"physics\"][species_name][\"mass\"]\n                p_over_m = p / self.cfg[\"physics\"][species_name][\"mass\"]\n\n                dstate_dt[species_name][\"n\"] = self.pusher_dict[species_name][\"push_n\"](n, u)\n                dstate_dt[species_name][\"u\"] = self.pusher_dict[species_name][\"push_u\"](\n                    n, u, p_over_m, q_over_m * total_e, delta\n                )\n                dstate_dt[species_name][\"p\"] = self.pusher_dict[species_name][\"push_e\"](n, u, p_over_m, q_over_m * e)\n            else:\n                dstate_dt[species_name][\"n\"] = jnp.zeros(self.cfg[\"grid\"][\"nx\"])\n                dstate_dt[species_name][\"u\"] = jnp.zeros(self.cfg[\"grid\"][\"nx\"])\n                dstate_dt[species_name][\"p\"] = jnp.zeros(self.cfg[\"grid\"][\"nx\"])\n\n            if self.cfg[\"physics\"][species_name][\"trapping\"][\"is_on\"]:\n                dstate_dt[species_name][\"delta\"] = self.pusher_dict[species_name][\"particle_trapper\"](e, delta, args)\n            else:\n                dstate_dt[species_name][\"delta\"] = jnp.zeros(self.cfg[\"grid\"][\"nx\"])\n\n        return dstate_dt", "\n\ndef get_save_func(cfg):\n    if cfg[\"save\"][\"func\"][\"is_on\"]:\n        if cfg[\"save\"][\"x\"][\"is_on\"]:\n            dx = (cfg[\"save\"][\"x\"][\"xmax\"] - cfg[\"save\"][\"x\"][\"xmin\"]) / cfg[\"save\"][\"x\"][\"nx\"]\n            cfg[\"save\"][\"x\"][\"ax\"] = jnp.linspace(\n                cfg[\"save\"][\"x\"][\"xmin\"] + dx / 2.0, cfg[\"save\"][\"x\"][\"xmax\"] - dx / 2.0, cfg[\"save\"][\"x\"][\"nx\"]\n            )\n\n            save_x = partial(jnp.interp, cfg[\"save\"][\"x\"][\"ax\"], cfg[\"grid\"][\"x\"])\n\n        if cfg[\"save\"][\"kx\"][\"is_on\"]:\n            cfg[\"save\"][\"kx\"][\"ax\"] = jnp.linspace(\n                cfg[\"save\"][\"kx\"][\"kxmin\"], cfg[\"save\"][\"kx\"][\"kxmax\"], cfg[\"save\"][\"kx\"][\"nkx\"]\n            )\n\n            def save_kx(field):\n                complex_field = jnp.fft.rfft(field, axis=0) * 2.0 / cfg[\"grid\"][\"nx\"]\n                interped_field = jnp.interp(cfg[\"save\"][\"kx\"][\"ax\"], cfg[\"grid\"][\"kxr\"], complex_field)\n                return {\"mag\": jnp.abs(interped_field), \"ang\": jnp.angle(interped_field)}\n\n        def save_func(t, y, args):\n            save_dict = {}\n            if cfg[\"save\"][\"x\"][\"is_on\"]:\n                save_dict[\"x\"] = jtu.tree_map(save_x, y)\n            if cfg[\"save\"][\"kx\"][\"is_on\"]:\n                save_dict[\"kx\"] = jtu.tree_map(save_kx, y)\n\n            return save_dict\n\n    else:\n        cfg[\"save\"][\"x\"][\"ax\"] = cfg[\"grid\"][\"x\"]\n        save_func = None\n\n    return save_func", "\n\ndef get_models(model_config: Dict) -> defaultdict[eqx.Module]:\n    if model_config:\n        model_keys = jax.random.split(jax.random.PRNGKey(420), len(model_config.keys()))\n        model_dict = defaultdict(eqx.Module)\n        for (term, config), this_key in zip(model_config.items(), model_keys):\n            if term == \"file\":\n                pass\n            else:\n                for act in [\"activation\", \"final_activation\"]:\n                    if config[act] == \"tanh\":\n                        config[act] = jnp.tanh\n\n                model_dict[term] = nn.MLP(**{**config, \"key\": this_key})\n        if model_config[\"file\"]:\n            model_dict = eqx.tree_deserialise_leaves(model_config[\"file\"], model_dict)\n\n        return model_dict\n    else:\n        return False", ""]}
{"filename": "theory/__init__.py", "chunked_list": [""]}
{"filename": "theory/electrostatic.py", "chunked_list": ["# MIT License\n#\n# Copyright (c) 2022 Ergodic LLC\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:", "# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER", "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom typing import Tuple\nimport numpy as np\nimport scipy\nfrom scipy import signal", "import scipy\nfrom scipy import signal\n\n\ndef get_nlfs(ek, dt):\n    \"\"\"\n    Calculate the shift in frequency with respect to a reference\n    This can be done by subtracting a signal at the reference frequency from the\n    given signal\n    :param ek:\n    :param dt:\n    :return:\n    \"\"\"\n\n    midpt = int(ek.shape[0] / 2)\n\n    window = 1\n    # Calculate hilbert transform\n    analytic_signal = signal.hilbert(window * np.real(ek))\n    # Determine envelope\n    amplitude_envelope = np.abs(analytic_signal)\n    # Phase = angle(signal)    ---- needs unwrapping because of periodicity\n    instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n    # f(t) = dphase/dt\n    instantaneous_frequency = np.gradient(instantaneous_phase, dt)  ### Sampling rate!\n    # delta_f(t) = f(t) - driver_freq\n\n    # Smooth the answer\n    b, a = signal.butter(8, 0.125)\n    instantaneous_frequency_smooth = signal.filtfilt(b, a, instantaneous_frequency, padlen=midpt)\n\n    return amplitude_envelope, instantaneous_frequency_smooth", "\n\ndef plasma_dispersion(value):\n    \"\"\"\n    This function leverages the Fadeeva function in scipy to calculate the Z function\n\n    :param value:\n    :return:\n    \"\"\"\n    return scipy.special.wofz(value) * np.sqrt(np.pi) * 1j", "\n\ndef plasma_dispersion_prime(value):\n    \"\"\"\n    This is a simple relation for Z-prime, which happens to be directly proportional to Z\n\n    :param value:\n    :return:\n    \"\"\"\n    return -2.0 * (1.0 + value * plasma_dispersion(value))", "\n\ndef get_roots_to_electrostatic_dispersion(wp_e, vth_e, k0, maxwellian_convention_factor=2.0, initial_root_guess=None):\n    \"\"\"\n    This function calculates the root of the plasma dispersion relation\n\n    :param wp_e:\n    :param vth_e:\n    :param k0:\n    :param maxwellian_convention_factor:\n    :param initial_root_guess:\n    :return:\n    \"\"\"\n    from scipy import optimize\n\n    plasma_epsilon, initial_root_guess = get_dispersion_function(\n        wp_e, vth_e, k0, maxwellian_convention_factor, initial_root_guess\n    )\n\n    epsilon_root = optimize.newton(plasma_epsilon, initial_root_guess)\n\n    return epsilon_root * k0 * vth_e * np.sqrt(maxwellian_convention_factor)", "\n\ndef get_dispersion_function(wp_e, vth_e, k0, maxwellian_convention_factor=2.0, initial_root_guess=None):\n    \"\"\"\n    This function calculates the root of the plasma dispersion relation\n\n    :param wp_e:\n    :param vth_e:\n    :param k0:\n    :param maxwellian_convention_factor:\n    :param initial_root_guess:\n    :return:\n    \"\"\"\n    if initial_root_guess is None:\n        initial_root_guess = np.sqrt(wp_e**2.0 + 3 * (k0 * vth_e) ** 2.0)\n\n    chi_e = np.power((wp_e / (vth_e * k0)), 2.0) / maxwellian_convention_factor\n\n    def plasma_epsilon(x):\n        val = 1.0 - chi_e * plasma_dispersion_prime(x)\n        return val\n\n    return plasma_epsilon, initial_root_guess", "\n\ndef calc_depsdw(kld):\n    \"\"\"\n    Used for frequency shift calculations\n\n    :param kld:\n    :return:\n    \"\"\"\n    depsdw = {}\n    wax = np.linspace(1.0, 1.5, 2048)\n\n    # Approximate epsilon\n    epsilon_approx = 1 - 1.0 / wax**2.0 - 3 * 1 / wax**2.0 * (kld / wax) ** 2.0\n\n    # Exact\n    disp_fn, _ = get_dispersion_function(1.0, 1.0, kld)\n\n    # Make array of epsilons\n    disp_arr = np.array([np.real(disp_fn(w / (kld * np.sqrt(2.0)))) for w in wax])\n\n    depsdw[\"exact\"] = np.gradient(disp_arr, wax[2] - wax[1])\n    depsdw[\"approx\"] = np.gradient(epsilon_approx, wax[2] - wax[1])\n\n    wr = np.real(\n        get_roots_to_electrostatic_dispersion(1.0, 1.0, kld, maxwellian_convention_factor=2.0, initial_root_guess=None)\n    )\n    iw = np.argmin(np.abs(wax - wr))\n\n    return depsdw[\"exact\"][iw], depsdw[\"approx\"][iw]", "\n\ndef get_complex_frequency_table(num: int, kinetic_real_epw: bool) -> Tuple[np.array, np.array, np.array]:\n    \"\"\"\n    This function creates a table of the complex plasma frequency for $0.2 < k \\lambda_D < 0.4$ in `num` steps\n\n    :param kinetic_real_epw:\n    :param num:\n    :return:\n    \"\"\"\n    klds = np.linspace(0.02, 0.4, num)\n    wrs = np.zeros(num)\n    wis = np.zeros(num)\n\n    for i, kld in enumerate(klds):\n        ww = get_roots_to_electrostatic_dispersion(1.0, 1.0, kld)\n        if kinetic_real_epw:\n            wrs[i] = np.real(ww)\n        else:\n            wrs[i] = np.sqrt(1.0 + 3.0 * kld**2.0)\n        wis[i] = np.imag(ww)\n\n    return wrs, wis, klds", ""]}
