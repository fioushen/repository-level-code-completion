{"filename": "data_split.py", "chunked_list": ["import glob\nimport pathlib\nimport os\nimport json\ndef data_split(data_path):\n    datacreate=[\"train.json\",\"val.json\",\"trainval.json\"]\n    file = glob.glob(data_path + \"/pdfs/*\")\n    l=[]\n    # resplit data by 8/2\n    split = 80*len(file)/100\n\n    for item in file:\n        f = pathlib.Path(item).stem\n        l.append(f)\n    with open (os.path.join(data_path,\"trainval.json\"),\"w\") as f:\n        f.write(json.dumps(l,ensure_ascii=False))\n    with open (os.path.join(data_path,\"train.json\"),\"w\") as f:\n        f.write(json.dumps(l[:int(split)],ensure_ascii=False))\n    with open (os.path.join(data_path,\"val.json\"),\"w\") as f:\n        f.write(json.dumps(l[-(len(file)-int(split)):],ensure_ascii=False))", "   \ndataset_path=\"/home/tip2k4/docile/data/docile/data/docile\"\ndata_split(dataset_path)\n"]}
{"filename": "docile/__init__.py", "chunked_list": ["\"\"\"Package with tools to work with the DocILE dataset and benchmark.\"\"\"\n"]}
{"filename": "docile/tools/__init__.py", "chunked_list": [""]}
{"filename": "docile/tools/dataset_browser.py", "chunked_list": ["import enum\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import List, Mapping, Optional, Tuple\n\nimport ipywidgets as widgets\nimport plotly.graph_objects as go\nfrom IPython.display import clear_output, display\n\nfrom docile.dataset import BBox, Dataset, Field", "\nfrom docile.dataset import BBox, Dataset, Field\n\n\nclass DisplayType(enum.Enum):\n    ANNOTATION = 1\n    ANNOTATION_MATCHED = 2\n    ANNOTATION_UNMATCHED = 3\n    PREDICTION = 4\n    PREDICTION_MATCHED = 5\n    PREDICTION_UNMATCHED = 6\n    TABLE_AREA = 7\n    TABLE_ROW = 8\n    TABLE_COLUMN = 9\n\n    def __str__(self) -> str:\n        # old version of enum package without StrEnum\n        d = {\n            DisplayType.ANNOTATION: \"Annotation\",\n            DisplayType.ANNOTATION_MATCHED: \"Matched Annotation\",\n            DisplayType.ANNOTATION_UNMATCHED: \"Unmatched Annotation\",\n            DisplayType.PREDICTION: \"Prediction\",\n            DisplayType.PREDICTION_MATCHED: \"Correct Prediction\",\n            DisplayType.PREDICTION_UNMATCHED: \"False Prediction\",\n            DisplayType.TABLE_AREA: \"Table Area\",\n            DisplayType.TABLE_ROW: \"Table Row\",\n            DisplayType.TABLE_COLUMN: \"Table Column\",\n        }\n        return d[self]\n\n    @property\n    def prefix(self) -> str:\n        type_to_prefix = {\n            DisplayType.ANNOTATION: \"Annotation \",\n            DisplayType.ANNOTATION_MATCHED: \"Matched annot. \",\n            DisplayType.ANNOTATION_UNMATCHED: \"Unmatched annot. \",\n            DisplayType.PREDICTION: \"Prediction \",\n            DisplayType.PREDICTION_MATCHED: \"Correct pred. \",\n            DisplayType.PREDICTION_UNMATCHED: \"False pred. \",\n            DisplayType.TABLE_AREA: \"Table area\",\n            DisplayType.TABLE_ROW: \"Table row \",\n            DisplayType.TABLE_COLUMN: \"Table column \",\n        }\n        return type_to_prefix[self]\n\n    @property\n    def color(self) -> str:\n        type_to_color = {\n            DisplayType.ANNOTATION: \"RoyalBlue\",\n            DisplayType.ANNOTATION_MATCHED: \"RoyalBlue\",\n            DisplayType.ANNOTATION_UNMATCHED: \"DarkRed\",\n            DisplayType.PREDICTION: \"Orange\",\n            DisplayType.PREDICTION_MATCHED: \"Green\",\n            DisplayType.PREDICTION_UNMATCHED: \"RED\",\n            DisplayType.TABLE_AREA: \"Yellow\",\n            DisplayType.TABLE_ROW: \"Yellow\",\n            DisplayType.TABLE_COLUMN: \"LightGreen\",\n        }\n        return type_to_color[self]", "\n\n@dataclass\nclass DisplayBox:\n    box: BBox\n    description: str\n    display_type: DisplayType\n\n    @property\n    def color(self) -> str:\n        return self.display_type.color\n\n    @property\n    def name(self) -> str:\n        return str(self.display_type)", "\n\nclass DatasetBrowser:\n    def __init__(\n        self,\n        dataset: Dataset,\n        doc_i: int = 0,\n        page_i: int = 0,\n        kile_matching: Optional[Mapping] = None,\n        lir_matching: Optional[Mapping] = None,\n        kile_predictions: Optional[Mapping] = None,\n        lir_predictions: Optional[Mapping] = None,\n        display_grid: bool = False,\n    ) -> None:\n        \"\"\"\n        Dataset browser to interactively display document annotations and optionally predictions in a jupyter notebook/lab.\n\n        Parameters\n        ----------\n        dataset\n            A Dataset from docile.dataset.\n        doc_i\n            Index of document to show, as sorted in the Dataset (not document ID!).\n        page_i\n            Index of page to show.\n        kile_matching\n            Dictionary with document IDs as keys and FieldMatching from KILE evaluation as values.\n        lir_predictions\n            Dictionary with document IDs as keys and FieldMatching from LIR evaluation as values.\n        kile_predictions\n            Dictionary with document IDs as keys and a lists of predicted KILE fields as values.\n            Note: This input is ignored if kile_matching (predictions with matching from evaluation) is provided.\n        lir_predictions\n            Dictionary with document IDs as keys and a lists of predicted LIR fields as values.\n            Note: This input is ignored if lir_matching (predictions with matching from evaluation) is provided.\n        display_grid\n            If True, show row and column annotations (imperfect, please refer to Supplementary Material for details).\n        \"\"\"\n        if kile_matching is not None and kile_predictions is not None:\n            warnings.warn(\n                \"Displaying predictions from provided kile_matching, kile_predictions are ignored.\",\n                stacklevel=1,\n            )\n        if lir_matching is not None and lir_predictions is not None:\n            warnings.warn(\n                \"Displaying predictions from provided lir_matching, lir_predictions are ignored.\",\n                stacklevel=1,\n            )\n\n        self.dataset = dataset\n        self.doc_i = doc_i\n        self.docid = self.dataset[self.doc_i].docid\n        self.page_i = page_i\n        self.kile_predictions = kile_predictions\n        self.lir_predictions = lir_predictions\n        self.kile_matching = kile_matching\n        self.lir_matching = lir_matching\n        self.display_grid = display_grid\n\n        self.button_prev_doc = widgets.Button(description=\"Previous document\")\n        self.button_next_doc = widgets.Button(description=\"Next document\")\n        self.button_prev_page = widgets.Button(description=\"Previous page\")\n        self.button_next_page = widgets.Button(description=\"Next page\")\n        self.output = widgets.Output()\n\n        def next_doc_button_clicked(_b: widgets.Button) -> None:\n            self.doc_i += 1\n            self.page_i = 0\n            self.update_output(self.doc_i, self.page_i)\n\n        def prev_doc_button_clicked(_b: widgets.Button) -> None:\n            self.doc_i -= 1\n            self.page_i = 0\n            self.update_output(self.doc_i, self.page_i)\n\n        def next_page_button_clicked(_b: widgets.Button) -> None:\n            self.page_i += 1\n            self.update_output(self.doc_i, self.page_i)\n\n        def prev_page_button_clicked(_b: widgets.Button) -> None:\n            self.page_i -= 1\n            self.update_output(self.doc_i, self.page_i)\n\n        self.button_next_doc.on_click(next_doc_button_clicked)\n        self.button_prev_doc.on_click(prev_doc_button_clicked)\n        self.button_next_page.on_click(next_page_button_clicked)\n        self.button_prev_page.on_click(prev_page_button_clicked)\n\n        buttons = widgets.HBox(\n            (\n                self.button_prev_doc,\n                self.button_next_doc,\n                self.button_prev_page,\n                self.button_next_page,\n            )\n        )\n        widgets_layout = widgets.VBox((buttons, self.output))\n        display(widgets_layout)\n        with self.output:\n            self.update_output(self.doc_i, self.page_i)\n\n    def update_output(self, doc_i: int, page_i: int) -> None:\n        self.doc_i = doc_i\n        self.docid = self.dataset[self.doc_i].docid\n        self.page_i = page_i\n        self.button_prev_doc.disabled = self.doc_i == 0\n        self.button_next_doc.disabled = self.doc_i == len(self.dataset) - 1\n        self.button_prev_page.disabled = self.page_i == 0\n        self.button_next_page.disabled = self.page_i == self.dataset[self.doc_i].page_count - 1\n\n        with self.output:\n            clear_output()\n            print(  # noqa T201\n                f\"document {self.dataset[self.doc_i].docid} ({self.doc_i+1}/{len(self.dataset)}), \"\n                f\"page {self.page_i+1}/{self.dataset[self.doc_i].page_count}\"\n            )\n            self.plot_page()\n\n    def get_displayboxes_and_resolve_overlaps(\n        self, fields_types: List[Tuple[Field, DisplayType]], merge_iou: float = 0.7\n    ) -> List[DisplayBox]:\n        # sort from largest to smallest for interactive browsing, so that smaller bboxes interact\n        # on top of the larger\n        fields_types = sorted(fields_types, key=lambda f: -f[0].bbox.area)\n\n        descriptions = []\n        for field, display_type in fields_types:\n            descriptions.append(self._get_field_description(field, display_type.prefix))\n\n        display_boxes = []\n        for i, (field, display_type) in enumerate(fields_types):\n            desc = [descriptions[i]]\n            for j, (field2, _) in enumerate(fields_types):\n                if i == j:\n                    continue\n                iou = (\n                    field.bbox.intersection(field2.bbox).area / field.bbox.union(field2.bbox).area\n                )\n                if iou > merge_iou:\n                    desc.append(descriptions[j])\n\n            display_boxes.append(\n                DisplayBox(field.bbox, description=\"<br>\".join(desc), display_type=display_type)\n            )\n        return display_boxes\n\n    @staticmethod\n    def _get_field_description(field: Field, prefix: str) -> str:\n        li_suffix = f\" @item {field.line_item_id}\" if field.line_item_id is not None else \"\"\n        multiline_text = field.text.replace(\"\\n\", \"<br>\") if field.text is not None else \"\"\n        return f\"[{prefix}{field.fieldtype}{li_suffix}]<br>{multiline_text}\"\n\n    def draw_fields(self, display_boxes: List[DisplayBox]) -> None:\n        displayed_types = set()\n        # Add field bounding boxes\n        for display_box in display_boxes:\n            x0 = display_box.box.left * self.scaled_width\n            y0 = self.scaled_height - display_box.box.top * self.scaled_height\n            x1 = display_box.box.right * self.scaled_width\n            y1 = self.scaled_height - display_box.box.bottom * self.scaled_height\n\n            self.fig.add_shape(\n                type=\"rect\",\n                x0=x0,\n                y0=y0,\n                x1=x1,\n                y1=y1,\n                line={\"color\": display_box.color},\n                name=display_box.name,\n            )\n\n            # Adding a trace with a fill, setting opacity to 0\n            self.fig.add_trace(\n                go.Scatter(\n                    x=[x0, x0, x1, x1],\n                    y=[y0, y1, y1, y0],\n                    fill=\"toself\",\n                    mode=\"lines\",\n                    text=display_box.description,\n                    name=\"\",\n                    opacity=0,\n                    showlegend=False,\n                )\n            )\n            displayed_types.add(display_box.display_type)\n\n        for t in DisplayType:\n            if t in displayed_types:\n                self.fig.add_trace(\n                    go.Scatter(\n                        x=[None],\n                        y=[None],\n                        mode=\"markers\",\n                        name=str(t),\n                        marker={\"size\": 7, \"color\": t.color, \"symbol\": \"square\"},\n                    )\n                )\n\n    def get_all_displayboxes(self) -> List[DisplayBox]:\n        annotation = self.dataset[self.doc_i].annotation\n\n        display_boxes = []\n\n        try:\n            table_grid = annotation.get_table_grid(self.page_i)\n        except KeyError:\n            table_grid = None\n        if table_grid is not None:\n            display_boxes.append(\n                DisplayBox(table_grid.bbox, \"[Table area]\", DisplayType.TABLE_AREA)\n            )\n            if self.display_grid:\n                display_boxes.extend(\n                    [\n                        DisplayBox(bbox, f\"[Table column {col_type}]\", DisplayType.TABLE_COLUMN)\n                        for bbox, col_type in table_grid.columns_bbox_with_type\n                    ]\n                )\n                display_boxes.extend(\n                    [\n                        DisplayBox(bbox, f\"[Table row {row_type}]\", DisplayType.TABLE_ROW)\n                        for bbox, row_type in table_grid.rows_bbox_with_type\n                    ]\n                )\n\n        fields_types = []\n\n        # display KILE predictions with matching (if available) or without (if not available):\n        if self.kile_matching is not None:\n            if self.docid in self.kile_matching:\n                fields_types.extend(\n                    [\n                        (f, DisplayType.PREDICTION_UNMATCHED)\n                        for f in self.kile_matching[self.docid].false_positives\n                        if f.page == self.page_i\n                    ]\n                )\n                fields_types.extend(\n                    [\n                        (f, DisplayType.ANNOTATION_UNMATCHED)\n                        for f in self.kile_matching[self.docid].false_negatives\n                        if f.page == self.page_i\n                    ]\n                )\n                fields_types.extend(\n                    [\n                        (m.pred, DisplayType.PREDICTION_MATCHED)\n                        for m in self.kile_matching[self.docid].matches\n                        if m.pred.page == self.page_i\n                    ]\n                )\n                fields_types.extend(\n                    [\n                        (m.gold, DisplayType.ANNOTATION_MATCHED)\n                        for m in self.kile_matching[self.docid].matches\n                        if m.gold.page == self.page_i\n                    ]\n                )\n        else:\n            try:\n                fields_types.extend(\n                    [(f, DisplayType.ANNOTATION) for f in annotation.page_fields(self.page_i)]\n                )\n            except KeyError:\n                # annotations not available, this can happen for test set or unlabeled set\n                pass\n            if self.kile_predictions is not None:\n                fields_types.extend(\n                    [\n                        (f, DisplayType.PREDICTION)\n                        for f in self.kile_predictions.get(self.docid, [])\n                        if f.page == self.page_i\n                    ]\n                )\n\n        # display LIR predictions with matching (if available) or without (if not available):\n        if self.lir_matching is not None:\n            if self.docid in self.lir_matching:\n                fields_types.extend(\n                    [\n                        (f, DisplayType.PREDICTION_UNMATCHED)\n                        for f in self.lir_matching[self.docid].false_positives\n                        if f.page == self.page_i\n                    ]\n                )\n                fields_types.extend(\n                    [\n                        (f, DisplayType.ANNOTATION_UNMATCHED)\n                        for f in self.lir_matching[self.docid].false_negatives\n                        if f.page == self.page_i\n                    ]\n                )\n                fields_types.extend(\n                    [\n                        (m.pred, DisplayType.PREDICTION_MATCHED)\n                        for m in self.lir_matching[self.docid].matches\n                        if m.pred.page == self.page_i\n                    ]\n                )\n                fields_types.extend(\n                    [\n                        (\n                            m.gold,\n                            DisplayType.ANNOTATION_MATCHED,\n                        )\n                        for m in self.lir_matching[self.docid].matches\n                        if m.gold.page == self.page_i\n                    ]\n                )\n        else:\n            try:\n                fields_types.extend(\n                    [(f, DisplayType.ANNOTATION) for f in annotation.page_li_fields(self.page_i)]\n                )\n            except KeyError:\n                # annotations not available, this can happen for test set or unlabeled set\n                pass\n            if self.lir_predictions is not None:\n                fields_types.extend(\n                    [\n                        (f, DisplayType.PREDICTION)\n                        for f in self.lir_predictions.get(self.docid, [])\n                        if f.page == self.page_i\n                    ]\n                )\n\n        display_boxes.extend(self.get_displayboxes_and_resolve_overlaps(fields_types=fields_types))\n        return display_boxes\n\n    def plot_page(self, scale_factor: float = 0.5) -> None:\n        img = self.dataset[self.doc_i].page_image(self.page_i)\n\n        # Create figure\n        self.fig = go.Figure()\n        # Constants\n        self.scaled_width = img.size[0] * scale_factor\n        self.scaled_height = img.size[1] * scale_factor\n\n        # Configure axes\n        self.fig.update_xaxes(visible=False, range=[0, self.scaled_width])\n\n        self.fig.update_yaxes(\n            visible=False,\n            range=[0, self.scaled_height],\n            # the scaleanchor attribute ensures that the aspect ratio stays constant\n            scaleanchor=\"x\",\n        )\n\n        # Add image\n        self.fig.add_layout_image(\n            {\n                \"x\": 0,\n                \"sizex\": self.scaled_width,\n                \"y\": self.scaled_height,\n                \"sizey\": self.scaled_height,\n                \"xref\": \"x\",\n                \"yref\": \"y\",\n                \"opacity\": 1.0,\n                \"layer\": \"below\",\n                \"sizing\": \"stretch\",\n                \"source\": img,\n            }\n        )\n\n        # prepare bboxes\n        display_boxes = self.get_all_displayboxes()\n\n        self.draw_fields(display_boxes)\n\n        # Configure other layout\n        self.fig.update_layout(\n            width=self.scaled_width,\n            height=self.scaled_height,\n            margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n            showlegend=True,\n            legend={\"yanchor\": \"top\", \"y\": 0.9, \"xanchor\": \"left\", \"x\": 1},\n        )\n        self.fig.show(config={\"doubleClick\": \"reset\"})", ""]}
{"filename": "docile/tools/print_results.py", "chunked_list": ["import argparse\nfrom pathlib import Path\nfrom typing import List, Mapping, Sequence, Tuple, Union\n\nfrom tabulate import tabulate\n\nfrom docile.evaluation import EvaluationResult\nfrom docile.evaluation.evaluate import TASK_TO_PRIMARY_METRIC_NAME\n\n\ndef _highlight_best_numbers(\n    main_metric: str,\n    headers: Sequence[str],\n    rows: Sequence[Sequence[Union[str, int]]],\n    tablefmt: str,\n    floatfmt: str,\n) -> Tuple[List[str], List[List[Union[str, int]]]]:\n    \"\"\"Return updated headers and rows, highlighting the main metric with its best numbers.\"\"\"\n    if tablefmt != \"github\":\n        raise NotImplementedError(\"Highlight only works for github style tables\")\n    main_metric_col_is = [i for i, h in enumerate(headers) if h.endswith(main_metric)]\n    headers_task = list(headers)\n    rows_task = [list(row) for row in rows]\n    for col_i in main_metric_col_is:\n        headers_task[col_i] = f\"<ins>{headers_task[col_i]}</ins>\"  # underline in github\n        max_i = max(range(len(rows_task)), key=lambda i: rows_task[i][col_i])\n        for i in range(len(rows_task)):\n            rows_task[i][col_i] = f\"{rows_task[i][col_i]:{floatfmt}}\"\n        rows_task[max_i][col_i] = f\"**{rows_task[max_i][col_i]}**\"\n    return headers_task, rows_task", "\n\ndef _highlight_best_numbers(\n    main_metric: str,\n    headers: Sequence[str],\n    rows: Sequence[Sequence[Union[str, int]]],\n    tablefmt: str,\n    floatfmt: str,\n) -> Tuple[List[str], List[List[Union[str, int]]]]:\n    \"\"\"Return updated headers and rows, highlighting the main metric with its best numbers.\"\"\"\n    if tablefmt != \"github\":\n        raise NotImplementedError(\"Highlight only works for github style tables\")\n    main_metric_col_is = [i for i, h in enumerate(headers) if h.endswith(main_metric)]\n    headers_task = list(headers)\n    rows_task = [list(row) for row in rows]\n    for col_i in main_metric_col_is:\n        headers_task[col_i] = f\"<ins>{headers_task[col_i]}</ins>\"  # underline in github\n        max_i = max(range(len(rows_task)), key=lambda i: rows_task[i][col_i])\n        for i in range(len(rows_task)):\n            rows_task[i][col_i] = f\"{rows_task[i][col_i]:{floatfmt}}\"\n        rows_task[max_i][col_i] = f\"**{rows_task[max_i][col_i]}**\"\n    return headers_task, rows_task", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--predictions-dir\",\n        type=Path,\n        help=(\n            \"Path to directory with evaluation results of your models. It should contain a \"\n            \"subdirectory for each model with {split}_results_KILE.json and/or \"\n            \"{split}_results_LIR.json files.\"\n        ),\n    )\n    parser.add_argument(\n        \"--split\",\n        type=str,\n        default=\"val\",\n        help='you can pass multiple splits separated by commas, e.g., --split=\"val,test\"',\n    )\n    parser.add_argument(\n        \"--tablefmt\",\n        type=str,\n        default=\"github\",\n        help=\"table format such as 'github' or 'latex', see `tabulate` package for more options\",\n    )\n    parser.add_argument(\"--floatfmt\", type=str, default=\".3f\")\n    parser.add_argument(\n        \"--models\",\n        type=str,\n        default=\"\",\n        help=\"models to include in the table (in the given order), separated by commas\",\n    )\n    parser.add_argument(\n        \"--show-counts\",\n        action=\"store_true\",\n        help=\"show counts of True Positive and False Positive/Negative predictions\",\n    )\n    parser.add_argument(\n        \"--highlight-best-numbers\",\n        action=\"store_true\",\n        help=(\n            \"highlight the best numbers for the main metric, only implemented for \"\n            \"--table-format=github\"\n        ),\n    )\n    args = parser.parse_args()\n\n    splits = args.split.split(\",\")\n\n    metric_names = [\"AP\", \"f1\", \"precision\", \"recall\"]\n    if args.show_counts:\n        metric_names.extend([\"TP\", \"FP\", \"FN\"])\n    headers = [\"model\"]\n    for split in splits:\n        prefix = f\"{split}-\" if len(splits) > 1 else \"\"\n        headers.extend([f\"{prefix}{m}\" for m in metric_names])\n\n    rows = {\"KILE\": [], \"LIR\": []}\n    models_paths = list(args.predictions_dir.iterdir())\n    if args.models != \"\":\n        models_paths = [args.predictions_dir / m for m in args.models.split(\",\")]\n    for model_dir in models_paths:\n        for task in [\"KILE\", \"LIR\"]:\n            row = [model_dir.name]\n            for split in splits:\n                results_path = model_dir / f\"{split}_results_{task}.json\"\n                metrics: Mapping[str, Union[str, float]] = {m: \"-\" for m in metric_names}\n                if results_path.exists():\n                    eval_result = EvaluationResult.from_file(results_path)\n                    metrics = eval_result.get_metrics(task.lower())\n                row.extend([metrics[m] for m in metric_names])\n            rows[task].append(row)\n\n    report = []\n    for task in [\"KILE\", \"LIR\"]:\n        headers_task, rows_task = headers, rows[task]\n        if args.highlight_best_numbers:\n            main_metric = TASK_TO_PRIMARY_METRIC_NAME[task.lower()]\n            headers_task, rows_task = _highlight_best_numbers(\n                main_metric, headers_task, rows_task, args.tablefmt, args.floatfmt\n            )\n        report.append(task)\n        report.append(\"=\" * len(task))\n        report.append(\"\")\n        table = tabulate(rows_task, headers_task, tablefmt=args.tablefmt, floatfmt=args.floatfmt)\n        report.extend(table.splitlines())\n        report.append(\"\")\n\n    print(\"\\n\".join(report))  # noqa T201", ""]}
{"filename": "docile/cli/evaluate.py", "chunked_list": ["from pathlib import Path\nfrom typing import List, Optional, Sequence\n\nimport click\n\nfrom docile.dataset import CachingConfig, Dataset, load_predictions\nfrom docile.evaluation import (\n    EvaluationResult,\n    NamedRange,\n    evaluate_dataset,", "    NamedRange,\n    evaluate_dataset,\n    get_evaluation_subsets,\n)\n\n\nclass NamedRangesParamType(click.ParamType):\n    \"\"\"Parameter for list of ranges.\"\"\"\n\n    name = \"named_range\"\n\n    def convert(self, value: str, param: click.Option, ctx: click.Context) -> List[NamedRange]:\n        \"\"\"\n        Convert the input value into list of named ranges.\n\n        Parameters\n        ----------\n        value\n            List of ranges in the format 'range1,range2,...' One range can be one of 'x', 'x-y'\n            or 'x+', corresponding to ranges [x, x], [x, y] and [x, infinity).\n        param\n            Click option.\n        ctx\n            Click context.\n\n        Returns\n        -------\n        named_ranges\n            List of named ranges, i.e., tuples of string (range name) and range. Range is either\n            Tuple[int, int] or Tuple[int, None] if there is no upper bound.\n        \"\"\"\n        if value == \"\":\n            return []\n        parsed_named_ranges: List[NamedRange] = []\n        for range_name in value.split(\",\"):\n            size_range = range_name.split(\"-\")\n            if range_name.isdigit():\n                parsed_named_ranges.append((range_name, (int(range_name), int(range_name))))\n            elif range_name[-1] == \"+\" and range_name[:-1].isdigit():\n                parsed_named_ranges.append((range_name, (int(range_name[:-1]), None)))\n            elif len(size_range) == 2 and all(x.isdigit() for x in size_range):\n                parsed_named_ranges.append((range_name, (int(size_range[0]), int(size_range[1]))))\n            else:\n                self.fail(\n                    f\"Cannot parse range name {range_name}. Options are 'x', 'x-y' or 'x+'\",\n                    param,\n                    ctx,\n                )\n        return parsed_named_ranges", "\n\n@click.command(\"Evaluate predictions on DocILE dataset\")\n@click.option(\n    \"-t\",\n    \"--task\",\n    type=click.Choice([\"KILE\", \"LIR\"]),\n    required=True,\n    help=\"whether to evaluate KILE or LIR task\",\n)", "    help=\"whether to evaluate KILE or LIR task\",\n)\n@click.option(\n    \"-d\",\n    \"--dataset-path\",\n    type=click.Path(exists=True, file_okay=True, dir_okay=True, path_type=Path),\n    required=True,\n    help=\"path to the zip with dataset or unzipped dataset\",\n)\n@click.option(", ")\n@click.option(\n    \"-s\",\n    \"--split\",\n    type=str,\n    required=True,\n    default=\"val\",\n    help=\"name of the dataset split to evaluate on\",\n    show_default=True,\n)", "    show_default=True,\n)\n@click.option(\n    \"-p\",\n    \"--predictions\",\n    type=click.Path(exists=True, file_okay=True, dir_okay=False, path_type=Path),\n    required=True,\n    help=\"path to the json file with predictions\",\n)\n@click.option(", ")\n@click.option(\n    \"--store-evaluation-result\",\n    type=click.Path(exists=False, file_okay=True, dir_okay=False, path_type=Path),\n    default=None,\n    help=\"path to a json file where to store the evaluation result\",\n)\n@click.option(\n    \"--iou-threshold\",\n    type=float,", "    \"--iou-threshold\",\n    type=float,\n    default=1.0,\n    help=\"IoU threshold for PCC matching, can be useful for experiments\",\n)\n@click.option(\n    \"--evaluate-x-shot-subsets\",\n    type=NamedRangesParamType(),\n    default=\"0,1-3,4+\",\n    help=(", "    default=\"0,1-3,4+\",\n    help=(\n        \"evaluate on subsets of x-shot layout clusters. Pass empty string to turn it off. \"\n        \"Format: 'range1,range2,...' where range is one of 'x', 'x-y' or 'x+'.\"\n    ),\n    show_default=True,\n)\n@click.option(\n    \"--evaluate-synthetic-subsets\",\n    is_flag=True,", "    \"--evaluate-synthetic-subsets\",\n    is_flag=True,\n    help=\"if used, evaluate also on subsets belonging to layout clusters with synthetic data\",\n)\n@click.option(\n    \"--evaluate-fieldtypes\",\n    is_flag=True,\n    help=\"show breakdown per fiedltype\",\n)\n@click.option(", ")\n@click.option(\n    \"--evaluate-also-text\",\n    is_flag=True,\n    help=\"if used, also show metrics that require exact text match for predictions\",\n)\n@click.option(\n    \"--primary-metric-only\",\n    is_flag=True,\n    help=\"if used, the script prints only the primary metric instead of a full evaluation report\",", "    is_flag=True,\n    help=\"if used, the script prints only the primary metric instead of a full evaluation report\",\n)\ndef evaluate(\n    task: str,\n    dataset_path: Path,\n    split: str,\n    predictions: Path,\n    store_evaluation_result: Optional[Path],\n    iou_threshold: float,\n    evaluate_x_shot_subsets: Sequence[NamedRange],\n    evaluate_synthetic_subsets: bool,\n    evaluate_fieldtypes: bool,\n    evaluate_also_text: bool,\n    primary_metric_only: bool,\n) -> None:\n    dataset = Dataset(split, dataset_path, cache_images=CachingConfig.OFF)\n    docid_to_predictions = load_predictions(predictions)\n    subsets = get_evaluation_subsets(dataset, evaluate_x_shot_subsets, evaluate_synthetic_subsets)\n    if task == \"KILE\":\n        evaluation_result = evaluate_dataset(\n            dataset,\n            docid_to_kile_predictions=docid_to_predictions,\n            docid_to_lir_predictions={},\n            iou_threshold=iou_threshold,\n        )\n    elif task == \"LIR\":\n        evaluation_result = evaluate_dataset(\n            dataset,\n            docid_to_kile_predictions={},\n            docid_to_lir_predictions=docid_to_predictions,\n            iou_threshold=iou_threshold,\n        )\n    else:\n        raise ValueError(f\"Unknown task {task}\")\n\n    if store_evaluation_result is not None:\n        evaluation_result.to_file(store_evaluation_result)\n\n    if primary_metric_only:\n        metric_value = evaluation_result.get_primary_metric(task.lower())\n        print(metric_value)  # noqa T201\n    else:\n        report = evaluation_result.print_report(\n            subsets=subsets,\n            include_fieldtypes=evaluate_fieldtypes,\n            include_same_text=evaluate_also_text,\n        )\n        print(report)  # noqa T201", "\n\n@click.command(\"Print evaluation previously done on DocILE dataset\")\n@click.option(\n    \"--evaluation-result-path\",\n    type=click.Path(exists=True, file_okay=True, dir_okay=False, path_type=Path),\n    required=True,\n    help=\"path to the json file with evaluation result\",\n)\n@click.option(", ")\n@click.option(\n    \"--evaluate-x-shot-subsets\",\n    type=NamedRangesParamType(),\n    default=\"0,1-3,4+\",\n    help=(\n        \"evaluate on subsets of x-shot layout clusters. Pass empty string to turn it off. \"\n        \"Format: 'range1,range2,...' where range is one of 'x', 'x-y' or 'x+'.\"\n    ),\n    show_default=True,", "    ),\n    show_default=True,\n)\n@click.option(\n    \"--evaluate-synthetic-subsets\",\n    is_flag=True,\n    help=\"if used, evaluate also on subsets belonging to layout clusters with synthetic data\",\n)\n@click.option(\n    \"--dataset-path\",", "@click.option(\n    \"--dataset-path\",\n    type=click.Path(exists=True, file_okay=True, dir_okay=True, path_type=Path),\n    default=None,\n    help=(\n        \"if --evaluate-x-shot-subsets (used by default) or --evaluate-synthetic-subsets are used, \"\n        \"you need to pass a path to the dataset\"\n    ),\n)\n@click.option(", ")\n@click.option(\n    \"--evaluate-fieldtypes\",\n    is_flag=True,\n    help=\"show breakdown per fiedltype\",\n)\n@click.option(\n    \"--evaluate-also-text\",\n    is_flag=True,\n    help=\"if used, also show metrics that require exact text match for predictions\",", "    is_flag=True,\n    help=\"if used, also show metrics that require exact text match for predictions\",\n)\ndef print_evaluation_report(\n    evaluation_result_path: Path,\n    evaluate_x_shot_subsets: Sequence[NamedRange],\n    evaluate_synthetic_subsets: bool,\n    dataset_path: Optional[Path],\n    evaluate_fieldtypes: bool,\n    evaluate_also_text: bool,\n) -> None:\n    evaluation_result = EvaluationResult.from_file(evaluation_result_path)\n    subsets: List[Dataset] = []\n    if len(evaluate_x_shot_subsets) > 0 or evaluate_synthetic_subsets:\n        if dataset_path is None:\n            raise ValueError(\n                \"You need to provide --dataset-path when --evaluate-x-shot-subsets (used by \"\n                \"default) or --evaluate-synthetic-subsets are used.\"\n            )\n        test_split_name: Optional[str] = None\n        for split_name in [\"test\", \"val\"]:\n            if evaluation_result.dataset_name.endswith(split_name):\n                test_split_name = split_name\n        if test_split_name is None:\n            raise ValueError(\n                f\"Unknown dataset {evaluation_result.dataset_name}, cannot find x-shot subsets\"\n            )\n        test = Dataset(\n            test_split_name, dataset_path, load_ocr=False, cache_images=CachingConfig.OFF\n        )\n        subsets = get_evaluation_subsets(test, evaluate_x_shot_subsets, evaluate_synthetic_subsets)\n    report = evaluation_result.print_report(\n        subsets=subsets,\n        include_fieldtypes=evaluate_fieldtypes,\n        include_same_text=evaluate_also_text,\n    )\n    print(report)  # noqa T201", "\n\nif __name__ == \"__main__\":\n    evaluate()\n"]}
{"filename": "docile/cli/__init__.py", "chunked_list": [""]}
{"filename": "docile/evaluation/line_item_matching.py", "chunked_list": ["from collections import defaultdict\nfrom typing import Dict, Iterable, List, Optional, Sequence, Tuple\n\nimport networkx\n\nfrom docile.dataset import BBox, Field\nfrom docile.evaluation.pcc import PCCSet\nfrom docile.evaluation.pcc_field_matching import FieldMatching, get_matches\n\n\nclass LineItemsGraph:\n    \"\"\"\n    Class representing the bipartite graph between prediction and gold line items.\n\n    Each edge holds the information about the field matching between the two line items. The graph\n    is used to find the maximum matching between line items that is maximizing the overall number\n    of matched fields (after excluding predictions with flag `use_only_for_ap`).\n    \"\"\"\n\n    def __init__(\n        self, pred_line_item_ids: Sequence[int], gold_line_item_ids: Sequence[int]\n    ) -> None:\n        self.G = networkx.Graph()\n        self.pred_nodes = [(0, i) for i in pred_line_item_ids]\n        self.gold_nodes = [(1, i) for i in gold_line_item_ids]\n        self.G.add_nodes_from(self.pred_nodes)\n        self.G.add_nodes_from(self.gold_nodes)\n\n    def add_edge(self, pred_li_i: int, gold_li_i: int, field_matching: FieldMatching) -> None:\n        # Only count predictions without the `use_only_for_ap` flag.\n        main_prediction_matches = len(field_matching.filter(exclude_only_for_ap=True).matches)\n        self.G.add_edge(\n            (0, pred_li_i),\n            (1, gold_li_i),\n            weight=-main_prediction_matches,\n            field_matching=field_matching,\n        )\n\n    def get_pair_field_matching(self, pred_li_i: int, gold_li_i: int) -> FieldMatching:\n        return self.G.edges[(0, pred_li_i), (1, gold_li_i)][\"field_matching\"]\n\n    def get_maximum_matching(self) -> Dict[int, int]:\n        \"\"\"\n        Return the maximum matching between the prediction and gold line items.\n\n        Returns\n        -------\n        Mapping from pred line item ids to gold line item ids. Only pairs that have non-empty field\n        matching are returned.\n        \"\"\"\n        maximum_matching = networkx.algorithms.bipartite.minimum_weight_full_matching(\n            self.G, self.pred_nodes\n        )\n        # Each node in the graph is identified as (0, i), resp. (1, i) based on which side of\n        # bipartition the node is in (p=0 .. prediction, p=1 .. gold).\n        return {\n            pred_node[1]: gold_node[1]  # remove the bipartition id part\n            for pred_node, gold_node in maximum_matching.items()\n            # keep only edges from pred to gold and if they have non-empty field matching\n            if pred_node[0] == 0\n            and len(self.get_pair_field_matching(pred_node[1], gold_node[1]).matches) != 0\n        }", "\n\nclass LineItemsGraph:\n    \"\"\"\n    Class representing the bipartite graph between prediction and gold line items.\n\n    Each edge holds the information about the field matching between the two line items. The graph\n    is used to find the maximum matching between line items that is maximizing the overall number\n    of matched fields (after excluding predictions with flag `use_only_for_ap`).\n    \"\"\"\n\n    def __init__(\n        self, pred_line_item_ids: Sequence[int], gold_line_item_ids: Sequence[int]\n    ) -> None:\n        self.G = networkx.Graph()\n        self.pred_nodes = [(0, i) for i in pred_line_item_ids]\n        self.gold_nodes = [(1, i) for i in gold_line_item_ids]\n        self.G.add_nodes_from(self.pred_nodes)\n        self.G.add_nodes_from(self.gold_nodes)\n\n    def add_edge(self, pred_li_i: int, gold_li_i: int, field_matching: FieldMatching) -> None:\n        # Only count predictions without the `use_only_for_ap` flag.\n        main_prediction_matches = len(field_matching.filter(exclude_only_for_ap=True).matches)\n        self.G.add_edge(\n            (0, pred_li_i),\n            (1, gold_li_i),\n            weight=-main_prediction_matches,\n            field_matching=field_matching,\n        )\n\n    def get_pair_field_matching(self, pred_li_i: int, gold_li_i: int) -> FieldMatching:\n        return self.G.edges[(0, pred_li_i), (1, gold_li_i)][\"field_matching\"]\n\n    def get_maximum_matching(self) -> Dict[int, int]:\n        \"\"\"\n        Return the maximum matching between the prediction and gold line items.\n\n        Returns\n        -------\n        Mapping from pred line item ids to gold line item ids. Only pairs that have non-empty field\n        matching are returned.\n        \"\"\"\n        maximum_matching = networkx.algorithms.bipartite.minimum_weight_full_matching(\n            self.G, self.pred_nodes\n        )\n        # Each node in the graph is identified as (0, i), resp. (1, i) based on which side of\n        # bipartition the node is in (p=0 .. prediction, p=1 .. gold).\n        return {\n            pred_node[1]: gold_node[1]  # remove the bipartition id part\n            for pred_node, gold_node in maximum_matching.items()\n            # keep only edges from pred to gold and if they have non-empty field matching\n            if pred_node[0] == 0\n            and len(self.get_pair_field_matching(pred_node[1], gold_node[1]).matches) != 0\n        }", "\n\ndef _get_line_item_id(field: Field) -> int:\n    if field.line_item_id is None:\n        raise ValueError(f\"No line item ID specified for LIR field {field}\")\n    return field.line_item_id\n\n\ndef _place_bbox_in_document(bbox: BBox, page: int) -> BBox:\n    \"\"\"\n    Return a bbox where y coordinates are in range [page, page+1].\n\n    This way it is possible to work with bboxes from different pages on the same document.\n    \"\"\"\n    return BBox(left=bbox.left, top=bbox.top + page, right=bbox.right, bottom=bbox.bottom + page)", "def _place_bbox_in_document(bbox: BBox, page: int) -> BBox:\n    \"\"\"\n    Return a bbox where y coordinates are in range [page, page+1].\n\n    This way it is possible to work with bboxes from different pages on the same document.\n    \"\"\"\n    return BBox(left=bbox.left, top=bbox.top + page, right=bbox.right, bottom=bbox.bottom + page)\n\n\ndef _get_covering_bbox(bboxes: Iterable[BBox]) -> BBox:\n    \"\"\"\n    Return the minimum bbox covering all input bboxes.\n\n    Raises an exception if there are no bboxes on input.\n    \"\"\"\n    lefts, tops, rights, bottoms = zip(*(bbox.to_tuple() for bbox in bboxes))\n    return BBox(min(lefts), min(tops), max(rights), max(bottoms))", "\ndef _get_covering_bbox(bboxes: Iterable[BBox]) -> BBox:\n    \"\"\"\n    Return the minimum bbox covering all input bboxes.\n\n    Raises an exception if there are no bboxes on input.\n    \"\"\"\n    lefts, tops, rights, bottoms = zip(*(bbox.to_tuple() for bbox in bboxes))\n    return BBox(min(lefts), min(tops), max(rights), max(bottoms))\n", "\n\ndef get_lir_matches(\n    predictions: Sequence[Field],\n    annotations: Sequence[Field],\n    pcc_set: PCCSet,\n    iou_threshold: float = 1,\n) -> Tuple[FieldMatching, Dict[int, int]]:\n    \"\"\"\n    Get matching of line item fields in the document.\n\n    This is similar to pcc_field_matching.get_matches but first corresponding line items are found\n    with maximum matching while optimizing the total number of matched predictions, irrespective of\n    their score.\n\n    Returns\n    -------\n    Matching of line item fields and used maximum matching between line item ids (prediction to gold).\n    \"\"\"\n    if len(predictions) == 0 or len(annotations) == 0:\n        return (FieldMatching.empty(predictions, annotations), {})\n\n    pred_line_items = defaultdict(list)\n    pred_i_to_index_in_li = {}\n    for pred_i, pred in enumerate(predictions):\n        li_i = _get_line_item_id(pred)\n        pred_i_to_index_in_li[pred_i] = len(pred_line_items[li_i])\n        pred_line_items[li_i].append(pred)\n\n    gold_line_items = defaultdict(list)\n    for gold in annotations:\n        li_i = _get_line_item_id(gold)\n        gold_line_items[li_i].append(gold)\n\n    # We precompute the covering bbox of each line item. This is used to speedup the computation\n    # since prediction/gold line items that are completely disjoint cannot have any matches.\n    pred_li_bbox = {\n        li_i: _get_covering_bbox(_place_bbox_in_document(f.bbox, f.page) for f in fields)\n        for li_i, fields in pred_line_items.items()\n    }\n    gold_li_bbox = {\n        li_i: _get_covering_bbox(_place_bbox_in_document(f.bbox, f.page) for f in fields)\n        for li_i, fields in gold_line_items.items()\n    }\n\n    # Construct complete bipartite graph between pred and gold line items.\n    line_items_graph = LineItemsGraph(list(pred_line_items.keys()), list(gold_line_items.keys()))\n    for pred_li_i, preds in pred_line_items.items():\n        for gold_li_i, golds in gold_line_items.items():\n            # If the bboxes covering the line items are disjoint, there cannot be any field matches\n            if not pred_li_bbox[pred_li_i].intersects(gold_li_bbox[gold_li_i]):\n                field_matching = FieldMatching.empty(preds, golds)\n            else:\n                field_matching = get_matches(\n                    predictions=preds,\n                    annotations=golds,\n                    pcc_set=pcc_set,\n                    iou_threshold=iou_threshold,\n                )\n\n            line_items_graph.add_edge(\n                pred_li_i=pred_li_i, gold_li_i=gold_li_i, field_matching=field_matching\n            )\n\n    maximum_matching = line_items_graph.get_maximum_matching()\n\n    # Construct matching on the field level from the line item matching.\n    ordered_predictions_with_match: List[Tuple[Field, Optional[Field]]] = []\n    for pred_i, pred in enumerate(predictions):\n        pred_li_i = _get_line_item_id(pred)\n        if pred_li_i not in maximum_matching:\n            ordered_predictions_with_match.append((pred, None))\n            continue\n\n        gold_li_i = maximum_matching[pred_li_i]\n        field_matching = line_items_graph.get_pair_field_matching(\n            pred_li_i=pred_li_i, gold_li_i=gold_li_i\n        )\n        pred_i_in_li = pred_i_to_index_in_li[pred_i]\n        ordered_predictions_with_match.append(\n            field_matching.ordered_predictions_with_match[pred_i_in_li]\n        )\n\n    false_negatives: List[Field] = []\n    maximum_matching_gold_to_pred = {v: k for k, v in maximum_matching.items()}\n    for gold_li_i, golds in gold_line_items.items():\n        if gold_li_i in maximum_matching_gold_to_pred:\n            pred_li_i = maximum_matching_gold_to_pred[gold_li_i]\n            field_matching = line_items_graph.get_pair_field_matching(\n                pred_li_i=pred_li_i, gold_li_i=gold_li_i\n            )\n            false_negatives.extend(field_matching.false_negatives)\n        else:\n            false_negatives.extend(golds)\n\n    lir_field_matching = FieldMatching(ordered_predictions_with_match, false_negatives)\n    return lir_field_matching, maximum_matching", ""]}
{"filename": "docile/evaluation/evaluation_subsets.py", "chunked_list": ["from typing import List, Optional, Sequence, Tuple\n\nfrom docile.dataset import CachingConfig, Dataset\n\nNamedRange = Tuple[str, Tuple[int, Optional[int]]]\n\n\ndef size_in_range(size: int, size_range: Tuple[int, Optional[int]]) -> bool:\n    \"\"\"\n    Test if the cluster size lies in the given range.\n\n    Parameters\n    ----------\n    size\n        Size of the cluster.\n    size_range\n        A range [start, end], start and end is inclusive. If end is None, only start is tested.\n    \"\"\"\n    return size_range[0] <= size and (size_range[1] is None or size <= size_range[1])", "\n\ndef get_x_shot_subsets(\n    test: Dataset, train: Dataset, named_ranges: Sequence[NamedRange]\n) -> List[Dataset]:\n    \"\"\"\n    Find subsets of test corresponding to x-shot clusters.\n\n    For each given range, find a subset of `test` documents from clusters with `x` samples in\n    `train` where `x` lies in that range.\n\n    Parameters\n    ----------\n    test\n        Dataset used for evaluation, find subsets of this dataset.\n    train\n        Dataset that contains all the documents seen during training. Then `x`-shot cluster is\n        defined as a cluster that has `x` documents in `train`. Notice that in docile, `trainval`\n        is considered to be the training set for `test` as both `train` and `val` splits can be\n        used during training (even if just for validation).\n    named_ranges\n        Sequence of tuples of names and ranges representing the cluster sizes in `train` to fall in\n        the corresponding subset.\n\n    Returns\n    -------\n    A sequence of datasets, one for each named range, that are subsets of `test` and whose clusters\n    have the correct number of documents in `train`.\n    \"\"\"\n    # First parse the range names to raise an exception early if they are not valid.\n    test_cluster_ids = {doc.annotation.cluster_id for doc in test}\n    range_name_to_documents = {range_name: [] for range_name, _ in named_ranges}\n    for cluster_id in test_cluster_ids:\n        train_documents = [doc for doc in train if doc.annotation.cluster_id == cluster_id]\n        test_documents = test.get_cluster(cluster_id).documents\n        for range_name, size_range in named_ranges:\n            if size_in_range(len(train_documents), size_range):\n                range_name_to_documents[range_name].extend(test_documents)\n    return [\n        Dataset.from_documents(f\"{test.split_name}-{range_name}-shot\", documents)\n        for range_name, documents in range_name_to_documents.items()\n    ]", "\n\ndef get_synthetic_subset(test: Dataset, synthetic_sources: Dataset) -> Optional[Dataset]:\n    \"\"\"\n    Get subset of test corresponding to clusters with synthetic data available.\n\n    Returns\n    -------\n    Subset with documents in clusters that have synthetic data available. Returns None if there are\n    no such documents in `test`.\n    \"\"\"\n    synthetic_cluster_ids = {doc.annotation.cluster_id for doc in synthetic_sources}\n    documents_synth = [doc for doc in test if doc.annotation.cluster_id in synthetic_cluster_ids]\n    if len(documents_synth) == 0:\n        return None\n    return Dataset.from_documents(f\"{test.split_name}-synth-clusters-only\", documents_synth)", "\n\ndef get_evaluation_subsets(\n    test: Dataset, named_ranges: Sequence[NamedRange], synthetic: bool\n) -> List[Dataset]:\n    \"\"\"\n    Find subsets corresponding to x-shot and/or synthetic clusters.\n\n    When named_ranges is given, finds x-shot clusters with respect to `trainval` for `test` and\n    w.r.t. to `train` for `val`. When synthetic is true, for each subset a variant is added that\n    includes only documents in clusters with synthetic data available.\n\n    Parameters\n    ----------\n    named_ranges\n        Sequence of tuples of names and ranges representing the cluster sizes in `train` to fall in\n        the corresponding subset.\n    synthetic\n        If true, generate subsets of documents belonging to clusters with synthetic data.\n\n    Returns\n    -------\n    List of dataset subsets, without the full `test` dataset.\n    \"\"\"\n    if len(named_ranges) == 0 and not synthetic:\n        return []\n\n    # Add the full dataset to the list so that the synth version is generated below. It is removed\n    # from the output at the end.\n    subsets = [test]\n    if len(named_ranges) > 0:\n        if test.split_name == \"test\":\n            train_split_name = \"trainval\"\n        elif test.split_name == \"val\":\n            train_split_name = \"train\"\n        else:\n            raise ValueError(f\"No default corresponding train dataset for {test}\")\n\n        train = Dataset(\n            train_split_name, test.data_paths, load_ocr=False, cache_images=CachingConfig.OFF\n        )\n        subsets.extend(get_x_shot_subsets(test, train, named_ranges))\n\n    if synthetic:\n        new_subsets = []\n        synthetic_sources = Dataset(\n            \"synthetic-sources\", test.data_paths, load_ocr=False, cache_images=CachingConfig.OFF\n        )\n        for subset in subsets:\n            new_subsets.append(subset)\n            synthetic_subset = get_synthetic_subset(subset, synthetic_sources)\n            if synthetic_subset is not None:\n                new_subsets.append(synthetic_subset)\n        subsets = new_subsets\n\n    # Remove the full test set from the output\n    return subsets[1:]", ""]}
{"filename": "docile/evaluation/evaluate.py", "chunked_list": ["import hashlib\nimport json\nimport logging\nimport operator\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Dict, List, Mapping, Optional, Sequence, Tuple, Union\n\nfrom tabulate import tabulate", "\nfrom tabulate import tabulate\nfrom tqdm import tqdm\n\nfrom docile.dataset import KILE_FIELDTYPES, LIR_FIELDTYPES, Dataset, Document, Field\nfrom docile.evaluation.average_precision import compute_average_precision\nfrom docile.evaluation.line_item_matching import get_lir_matches\nfrom docile.evaluation.pcc import get_document_pccs\nfrom docile.evaluation.pcc_field_matching import FieldMatching, get_matches\n", "from docile.evaluation.pcc_field_matching import FieldMatching, get_matches\n\nlogger = logging.getLogger(__name__)\n\n\nPredictionSortKey = Tuple[Tuple[bool, float], int, str]\n\nTASK_TO_PRIMARY_METRIC_NAME = {\"kile\": \"AP\", \"lir\": \"f1\"}\nMETRIC_NAMES = [\"AP\", \"f1\", \"precision\", \"recall\", \"TP\", \"FP\", \"FN\"]\n", "METRIC_NAMES = [\"AP\", \"f1\", \"precision\", \"recall\", \"TP\", \"FP\", \"FN\"]\n\nMAX_NUMBER_OF_PREDICTIONS_PER_PAGE = 1000\n\n\nclass PredictionsValidationError(ValueError):\n    pass\n\n\n@dataclass(frozen=True)\nclass EvaluationResult:\n    \"\"\"\n    Class with the evaluation result.\n\n    It stores the matching between predictions and annotations which can be used to (quickly)\n    compute different metrics. The following options are supported:\n    * Unmatch predictions whose text differs from the ground truth text (in the primary metric\n      this is not required).\n    * Filter predictions and annotations to a specific fieldtype.\n    * Compute metrics for a single document\n    \"\"\"\n\n    task_to_docid_to_matching: Mapping[str, Mapping[str, FieldMatching]]\n    dataset_name: str  # name of evaluated Dataset\n    iou_threshold: float  # which value was used to for the evaluation\n\n    def to_file(self, path: Path) -> None:\n        encoded_matchings = {\n            task: {docid: matching.to_dict() for docid, matching in docid_to_matching.items()}\n            for task, docid_to_matching in self.task_to_docid_to_matching.items()\n        }\n        dct = {\n            \"dataset_name\": self.dataset_name,\n            \"iou_threshold\": self.iou_threshold,\n            \"task_to_docid_to_matching\": encoded_matchings,\n        }\n        path.write_text(json.dumps(dct, indent=2))\n\n    @classmethod\n    def from_file(cls, path: Path) -> \"EvaluationResult\":\n        dct = json.loads(path.read_text())\n        matchings = {\n            task: {\n                docid: FieldMatching.from_dict(matching)\n                for docid, matching in docid_to_matching.items()\n            }\n            for task, docid_to_matching in dct[\"task_to_docid_to_matching\"].items()\n        }\n        return cls(matchings, dct[\"dataset_name\"], dct[\"iou_threshold\"])\n\n    def get_primary_metric(self, task: str) -> float:\n        \"\"\"Return the primary metric used for DocILE'23 benchmark competition.\"\"\"\n        metric = TASK_TO_PRIMARY_METRIC_NAME[task]\n        return self.get_metrics(task)[metric]\n\n    def get_metrics(\n        self,\n        task: str,\n        same_text: bool = False,\n        fieldtype: str = \"\",\n        docids: Optional[Sequence[str]] = None,\n    ) -> Dict[str, float]:\n        \"\"\"Get metrics based on several filters.\n\n        Parameters\n        ----------\n        task\n            Task name for which to return the metrics, should be \"kile\" or \"lir\".\n        same_text\n            Require predictions to have exactly the same text as the ground truth in the\n            annotation. Note that matching is done based on the location only and this is then just\n            used to unmatch predictions with wrong text. This means it can happen that a correct\n            prediction is not counted as true positive if there is another prediction in the same\n            location with wrong text that was matched to the annotation first.\n        fieldtype\n            If non-empty, restrict the predictions and annotations to this fieldtype.\n        docids\n            Only restrict to these docids (all have to be in the original dataset).\n\n        Returns\n        -------\n        Dictionary from metric name to the metric value.\n        \"\"\"\n        docid_to_matching = self.task_to_docid_to_matching[task]\n        if docids is not None:\n            if not set(docid_to_matching.keys()).issuperset(docids):\n                raise ValueError(\n                    \"Cannot evaluate on subset with documents missing in the evaluation\"\n                )\n            docid_to_matching = {docid: docid_to_matching[docid] for docid in docids}\n        docid_to_filtered_matching = {\n            docid: matching.filter(same_text=same_text, fieldtype=fieldtype)\n            for docid, matching in docid_to_matching.items()\n        }\n        return compute_metrics(docid_to_filtered_matching)\n\n    def print_report(\n        self,\n        subsets: Sequence[Union[Dataset, Document]] = (),\n        include_fieldtypes: bool = True,\n        include_same_text: bool = False,\n        show_legend: bool = True,\n        tablefmt: str = \"github\",\n        floatfmt: str = \".3f\",\n    ) -> str:\n        \"\"\"\n        Return a string with a detailed evaluation report.\n\n        Parameters\n        ----------\n        subsets\n            Print evaluation report for several subsets of the original evaluation dataset.\n        include_fieldtypes\n            Also show metrics for each fieldtype separately.\n        include_same_text\n            Also show results if exact text match is required.\n        tablefmt\n            Format in which the table should be printed. With 'github' (default) the whole report\n            can be stored as a markdown file. You can also use 'latex' to generate a LaTeX table\n            definition and other options as defined in the `tabulate` package.\n        floatfmt\n            Formatting option for floats in tables. Check `tabulate` package for details.\n\n        Returns\n        -------\n        Multi-line string with the human-readable report.\n        \"\"\"\n\n        def get_subset_docids(subset: Union[Document, Dataset]) -> Sequence[str]:\n            return [subset.docid] if isinstance(subset, Document) else subset.docids\n\n        # When there are two or more subsets, a table with subset summary is shown, followed by\n        # reports of the individual subsets (if include_fieldtypes is used). Otherwise show only\n        # report for the whole dataset or the single subset.\n        report_name = (\n            self.dataset_name\n            if len(subsets) == 0\n            else str(subsets[0])\n            if len(subsets) == 1\n            else f\"{self.dataset_name} subsets\"\n        )\n        report_docids = get_subset_docids(subsets[0]) if len(subsets) == 1 else None\n        report = [f\"Evaluation report for {report_name}\"]\n\n        iou_threshold_str = \"\"\n        if self.iou_threshold < 1:\n            iou_threshold_str = f\" [IoU threshold for PCCs = {self.iou_threshold}]\"\n        report[-1] += iou_threshold_str\n        report.append(\"=\" * len(report[-1]))\n\n        for task in sorted(self.task_to_docid_to_matching.keys()):\n            same_text_choices = [False, True] if include_same_text else [False]\n            for same_text in same_text_choices:\n                task_name = task.upper()\n                if same_text:\n                    task_name += \" (with text comparison)\"\n                report.append(task_name)\n                report.append(\"-\" * len(report[-1]))\n                summary_metrics = self.get_metrics(\n                    task=task, same_text=same_text, docids=report_docids\n                )\n                primary_metric_name = TASK_TO_PRIMARY_METRIC_NAME[task]\n                primary_metric = summary_metrics[primary_metric_name]\n                report.append(f\"Primary metric ({primary_metric_name}): {primary_metric}\")\n                report.append(\"\")\n\n                assert set(summary_metrics.keys()) == set(METRIC_NAMES)\n                if len(subsets) > 1:\n                    headers = [\"subsets\"] + METRIC_NAMES\n                    rows = [[self.dataset_name] + [summary_metrics[m] for m in METRIC_NAMES]]\n                    for subset in subsets:\n                        subset_metrics = self.get_metrics(\n                            task=task, same_text=same_text, docids=get_subset_docids(subset)\n                        )\n                        rows.append([str(subset)] + [subset_metrics[m] for m in METRIC_NAMES])\n                else:\n                    headers = [\"fieldtype\"] + METRIC_NAMES\n                    rows = [[\"**-> micro average**\"] + [summary_metrics[m] for m in METRIC_NAMES]]\n                    if include_fieldtypes:\n                        fieldtypes = KILE_FIELDTYPES if task == \"kile\" else LIR_FIELDTYPES\n                        for fieldtype in fieldtypes:\n                            metrics = self.get_metrics(\n                                task=task,\n                                same_text=same_text,\n                                fieldtype=fieldtype,\n                                docids=report_docids,\n                            )\n                            rows.append([fieldtype] + [metrics[m] for m in METRIC_NAMES])\n\n                table = tabulate(rows, headers, tablefmt=tablefmt, floatfmt=floatfmt)\n                report.extend(table.splitlines())\n                report.append(\"\")\n\n        report_str = \"\\n\".join(report)\n        if len(subsets) > 1 and include_fieldtypes:\n            # Iterate over individual subsets, including the no subset option as first.\n            for one_subset in [[]] + [[subset] for subset in subsets]:\n                report_str += \"\\n\"\n                report_str += self.print_report(\n                    subsets=one_subset,\n                    include_fieldtypes=include_fieldtypes,\n                    include_same_text=include_same_text,\n                    show_legend=False,\n                    tablefmt=tablefmt,\n                    floatfmt=floatfmt,\n                )\n\n        if show_legend:\n            report_str += \"\\n\" + self.print_legend(len(subsets) > 1, include_same_text)\n\n        return report_str\n\n    @staticmethod\n    def print_legend(show_subsets_summary: bool, include_same_text: bool) -> str:\n        legend = [\"Notes:\"]\n        if show_subsets_summary:\n            legend.append(\n                \"* '{dataset}-x-shot' means that the evaluation is restricted to documents from \"\n                \"layout clusters with `x` documents for training available. Here 'training' means \"\n                \"trainval for test and train for val.\"\n            )\n            legend.append(\n                \"* '{dataset}-synth-clusters-only' means that the evaluation is restricted to \"\n                \"documents from layout clusters for which synthetic data exists.\"\n            )\n        legend.append(\n            \"* For AP all predictions are used. For f1, precision, recall, TP, FP and FN \"\n            \"predictions explicitly marked with flag `use_only_for_ap=True` are excluded.\"\n        )\n        if include_same_text:\n            legend.append(\n                \"* '{TASK} (with text comparison)' means that matches found based on location are \"\n                \"considered as a false positive and false negative pair when their `text` is not \"\n                \"completely equal.\"\n            )\n        legend.append(\"\")\n        return \"\\n\".join(legend)", "\n@dataclass(frozen=True)\nclass EvaluationResult:\n    \"\"\"\n    Class with the evaluation result.\n\n    It stores the matching between predictions and annotations which can be used to (quickly)\n    compute different metrics. The following options are supported:\n    * Unmatch predictions whose text differs from the ground truth text (in the primary metric\n      this is not required).\n    * Filter predictions and annotations to a specific fieldtype.\n    * Compute metrics for a single document\n    \"\"\"\n\n    task_to_docid_to_matching: Mapping[str, Mapping[str, FieldMatching]]\n    dataset_name: str  # name of evaluated Dataset\n    iou_threshold: float  # which value was used to for the evaluation\n\n    def to_file(self, path: Path) -> None:\n        encoded_matchings = {\n            task: {docid: matching.to_dict() for docid, matching in docid_to_matching.items()}\n            for task, docid_to_matching in self.task_to_docid_to_matching.items()\n        }\n        dct = {\n            \"dataset_name\": self.dataset_name,\n            \"iou_threshold\": self.iou_threshold,\n            \"task_to_docid_to_matching\": encoded_matchings,\n        }\n        path.write_text(json.dumps(dct, indent=2))\n\n    @classmethod\n    def from_file(cls, path: Path) -> \"EvaluationResult\":\n        dct = json.loads(path.read_text())\n        matchings = {\n            task: {\n                docid: FieldMatching.from_dict(matching)\n                for docid, matching in docid_to_matching.items()\n            }\n            for task, docid_to_matching in dct[\"task_to_docid_to_matching\"].items()\n        }\n        return cls(matchings, dct[\"dataset_name\"], dct[\"iou_threshold\"])\n\n    def get_primary_metric(self, task: str) -> float:\n        \"\"\"Return the primary metric used for DocILE'23 benchmark competition.\"\"\"\n        metric = TASK_TO_PRIMARY_METRIC_NAME[task]\n        return self.get_metrics(task)[metric]\n\n    def get_metrics(\n        self,\n        task: str,\n        same_text: bool = False,\n        fieldtype: str = \"\",\n        docids: Optional[Sequence[str]] = None,\n    ) -> Dict[str, float]:\n        \"\"\"Get metrics based on several filters.\n\n        Parameters\n        ----------\n        task\n            Task name for which to return the metrics, should be \"kile\" or \"lir\".\n        same_text\n            Require predictions to have exactly the same text as the ground truth in the\n            annotation. Note that matching is done based on the location only and this is then just\n            used to unmatch predictions with wrong text. This means it can happen that a correct\n            prediction is not counted as true positive if there is another prediction in the same\n            location with wrong text that was matched to the annotation first.\n        fieldtype\n            If non-empty, restrict the predictions and annotations to this fieldtype.\n        docids\n            Only restrict to these docids (all have to be in the original dataset).\n\n        Returns\n        -------\n        Dictionary from metric name to the metric value.\n        \"\"\"\n        docid_to_matching = self.task_to_docid_to_matching[task]\n        if docids is not None:\n            if not set(docid_to_matching.keys()).issuperset(docids):\n                raise ValueError(\n                    \"Cannot evaluate on subset with documents missing in the evaluation\"\n                )\n            docid_to_matching = {docid: docid_to_matching[docid] for docid in docids}\n        docid_to_filtered_matching = {\n            docid: matching.filter(same_text=same_text, fieldtype=fieldtype)\n            for docid, matching in docid_to_matching.items()\n        }\n        return compute_metrics(docid_to_filtered_matching)\n\n    def print_report(\n        self,\n        subsets: Sequence[Union[Dataset, Document]] = (),\n        include_fieldtypes: bool = True,\n        include_same_text: bool = False,\n        show_legend: bool = True,\n        tablefmt: str = \"github\",\n        floatfmt: str = \".3f\",\n    ) -> str:\n        \"\"\"\n        Return a string with a detailed evaluation report.\n\n        Parameters\n        ----------\n        subsets\n            Print evaluation report for several subsets of the original evaluation dataset.\n        include_fieldtypes\n            Also show metrics for each fieldtype separately.\n        include_same_text\n            Also show results if exact text match is required.\n        tablefmt\n            Format in which the table should be printed. With 'github' (default) the whole report\n            can be stored as a markdown file. You can also use 'latex' to generate a LaTeX table\n            definition and other options as defined in the `tabulate` package.\n        floatfmt\n            Formatting option for floats in tables. Check `tabulate` package for details.\n\n        Returns\n        -------\n        Multi-line string with the human-readable report.\n        \"\"\"\n\n        def get_subset_docids(subset: Union[Document, Dataset]) -> Sequence[str]:\n            return [subset.docid] if isinstance(subset, Document) else subset.docids\n\n        # When there are two or more subsets, a table with subset summary is shown, followed by\n        # reports of the individual subsets (if include_fieldtypes is used). Otherwise show only\n        # report for the whole dataset or the single subset.\n        report_name = (\n            self.dataset_name\n            if len(subsets) == 0\n            else str(subsets[0])\n            if len(subsets) == 1\n            else f\"{self.dataset_name} subsets\"\n        )\n        report_docids = get_subset_docids(subsets[0]) if len(subsets) == 1 else None\n        report = [f\"Evaluation report for {report_name}\"]\n\n        iou_threshold_str = \"\"\n        if self.iou_threshold < 1:\n            iou_threshold_str = f\" [IoU threshold for PCCs = {self.iou_threshold}]\"\n        report[-1] += iou_threshold_str\n        report.append(\"=\" * len(report[-1]))\n\n        for task in sorted(self.task_to_docid_to_matching.keys()):\n            same_text_choices = [False, True] if include_same_text else [False]\n            for same_text in same_text_choices:\n                task_name = task.upper()\n                if same_text:\n                    task_name += \" (with text comparison)\"\n                report.append(task_name)\n                report.append(\"-\" * len(report[-1]))\n                summary_metrics = self.get_metrics(\n                    task=task, same_text=same_text, docids=report_docids\n                )\n                primary_metric_name = TASK_TO_PRIMARY_METRIC_NAME[task]\n                primary_metric = summary_metrics[primary_metric_name]\n                report.append(f\"Primary metric ({primary_metric_name}): {primary_metric}\")\n                report.append(\"\")\n\n                assert set(summary_metrics.keys()) == set(METRIC_NAMES)\n                if len(subsets) > 1:\n                    headers = [\"subsets\"] + METRIC_NAMES\n                    rows = [[self.dataset_name] + [summary_metrics[m] for m in METRIC_NAMES]]\n                    for subset in subsets:\n                        subset_metrics = self.get_metrics(\n                            task=task, same_text=same_text, docids=get_subset_docids(subset)\n                        )\n                        rows.append([str(subset)] + [subset_metrics[m] for m in METRIC_NAMES])\n                else:\n                    headers = [\"fieldtype\"] + METRIC_NAMES\n                    rows = [[\"**-> micro average**\"] + [summary_metrics[m] for m in METRIC_NAMES]]\n                    if include_fieldtypes:\n                        fieldtypes = KILE_FIELDTYPES if task == \"kile\" else LIR_FIELDTYPES\n                        for fieldtype in fieldtypes:\n                            metrics = self.get_metrics(\n                                task=task,\n                                same_text=same_text,\n                                fieldtype=fieldtype,\n                                docids=report_docids,\n                            )\n                            rows.append([fieldtype] + [metrics[m] for m in METRIC_NAMES])\n\n                table = tabulate(rows, headers, tablefmt=tablefmt, floatfmt=floatfmt)\n                report.extend(table.splitlines())\n                report.append(\"\")\n\n        report_str = \"\\n\".join(report)\n        if len(subsets) > 1 and include_fieldtypes:\n            # Iterate over individual subsets, including the no subset option as first.\n            for one_subset in [[]] + [[subset] for subset in subsets]:\n                report_str += \"\\n\"\n                report_str += self.print_report(\n                    subsets=one_subset,\n                    include_fieldtypes=include_fieldtypes,\n                    include_same_text=include_same_text,\n                    show_legend=False,\n                    tablefmt=tablefmt,\n                    floatfmt=floatfmt,\n                )\n\n        if show_legend:\n            report_str += \"\\n\" + self.print_legend(len(subsets) > 1, include_same_text)\n\n        return report_str\n\n    @staticmethod\n    def print_legend(show_subsets_summary: bool, include_same_text: bool) -> str:\n        legend = [\"Notes:\"]\n        if show_subsets_summary:\n            legend.append(\n                \"* '{dataset}-x-shot' means that the evaluation is restricted to documents from \"\n                \"layout clusters with `x` documents for training available. Here 'training' means \"\n                \"trainval for test and train for val.\"\n            )\n            legend.append(\n                \"* '{dataset}-synth-clusters-only' means that the evaluation is restricted to \"\n                \"documents from layout clusters for which synthetic data exists.\"\n            )\n        legend.append(\n            \"* For AP all predictions are used. For f1, precision, recall, TP, FP and FN \"\n            \"predictions explicitly marked with flag `use_only_for_ap=True` are excluded.\"\n        )\n        if include_same_text:\n            legend.append(\n                \"* '{TASK} (with text comparison)' means that matches found based on location are \"\n                \"considered as a false positive and false negative pair when their `text` is not \"\n                \"completely equal.\"\n            )\n        legend.append(\"\")\n        return \"\\n\".join(legend)", "\n\ndef evaluate_dataset(\n    dataset: Dataset,\n    docid_to_kile_predictions: Mapping[str, Sequence[Field]],\n    docid_to_lir_predictions: Mapping[str, Sequence[Field]],\n    iou_threshold: float = 1.0,\n) -> EvaluationResult:\n    \"\"\"\n    Evaluate the dataset on KILE and LIR using the given predictions.\n\n    If evaluating only on one of these metrics, simply provide no predictions for the second metric.\n\n    Parameters\n    ----------\n    dataset\n        Dataset with gold annotations to evaluate on.\n    docid_to_kile_predictions\n        Mapping from doc ids (in the 'dataset') to KILE predictions.\n    docid_to_lir_predictions\n        Mapping from doc ids (in the 'dataset') to LIR predictions.\n    iou_threshold\n        Necessary 'intersection / union' to accept a pair of fields as a match. The official\n        evaluation uses threshold 1.0 but lower thresholds can be used for debugging.\n\n    Returns\n    -------\n    Evaluation result containing the matched predictions. Use its `print_metrics()` method to get\n    the metrics.\n    \"\"\"\n    # Only evaluate tasks with at least 1 provided prediction.\n    task_to_docid_to_predictions = {\n        task: docid_to_predictions\n        for task, docid_to_predictions in [\n            (\"kile\", docid_to_kile_predictions),\n            (\"lir\", docid_to_lir_predictions),\n        ]\n        if sum(len(predictions) for predictions in docid_to_predictions.values()) > 0\n    }\n\n    _validate_predictions(dataset, task_to_docid_to_predictions)\n\n    tasks = task_to_docid_to_predictions.keys()\n    task_to_docid_to_matching = {task: {} for task in tasks}\n    for document in tqdm(dataset, desc=\"Run matching for documents\"):\n        pcc_set = get_document_pccs(document)\n\n        if \"kile\" in tasks:\n            kile_matching = get_matches(\n                predictions=docid_to_kile_predictions.get(document.docid, []),\n                annotations=document.annotation.fields,\n                pcc_set=pcc_set,\n                iou_threshold=iou_threshold,\n            )\n            task_to_docid_to_matching[\"kile\"][document.docid] = kile_matching\n\n        if \"lir\" in tasks:\n            lir_matching, _line_item_matching = get_lir_matches(\n                predictions=docid_to_lir_predictions.get(document.docid, []),\n                annotations=document.annotation.li_fields,\n                pcc_set=pcc_set,\n                iou_threshold=iou_threshold,\n            )\n            task_to_docid_to_matching[\"lir\"][document.docid] = lir_matching\n\n    return EvaluationResult(\n        task_to_docid_to_matching=task_to_docid_to_matching,\n        dataset_name=dataset.name,\n        iou_threshold=iou_threshold,\n    )", "\n\ndef compute_metrics(\n    docid_to_matching: Mapping[str, FieldMatching]\n) -> Dict[str, Union[int, float]]:\n    \"\"\"Compute different metrics for the given matchings between predictions and annotations.\"\"\"\n    ap = compute_average_precision(\n        sorted_predictions_matched=_sort_predictions(docid_to_matching),\n        total_annotations=sum(\n            len(matching.annotations) for matching in docid_to_matching.values()\n        ),\n    )\n\n    # Remove all predictions that were only for AP computation\n    matchings_no_ap = [\n        matching.filter(exclude_only_for_ap=True) for matching in docid_to_matching.values()\n    ]\n    total_predictions = sum(len(matching.predictions) for matching in matchings_no_ap)\n    total_annotations = sum(len(matching.annotations) for matching in matchings_no_ap)\n\n    true_positives = sum(len(matching.matches) for matching in matchings_no_ap)\n    false_positives = sum(len(matching.false_positives) for matching in matchings_no_ap)\n    false_negatives = sum(len(matching.false_negatives) for matching in matchings_no_ap)\n\n    precision = true_positives / total_predictions if total_predictions else 0.0\n    recall = true_positives / total_annotations if total_annotations else 0.0\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n\n    return {\n        \"AP\": ap,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"TP\": true_positives,\n        \"FP\": false_positives,\n        \"FN\": false_negatives,\n    }", "\n\ndef _validate_predictions(\n    dataset: Dataset,\n    task_to_docid_to_predictions: Mapping[str, Mapping[str, Sequence[Field]]],\n) -> None:\n    \"\"\"Run basic checks on the provided predictions.\"\"\"\n    if len(task_to_docid_to_predictions) == 0:\n        raise PredictionsValidationError(\n            \"You need to provide at least one prediction for at least one of the tasks.\"\n        )\n\n    for task, docid_to_predictions in task_to_docid_to_predictions.items():\n        for docid, predictions in docid_to_predictions.items():\n            page_to_predictions = Counter(pred.page for pred in predictions)\n            if any(\n                num_predictions > MAX_NUMBER_OF_PREDICTIONS_PER_PAGE\n                for num_predictions in page_to_predictions.values()\n            ):\n                raise PredictionsValidationError(\n                    f\"{task.upper()}: Exceeded limit of {MAX_NUMBER_OF_PREDICTIONS_PER_PAGE} \"\n                    f\"predictions per page for doc: {docid}\"\n                )\n\n    for task, docid_to_predictions in task_to_docid_to_predictions.items():\n        if any(\n            pred.fieldtype is None\n            for predictions in docid_to_predictions.values()\n            for pred in predictions\n        ):\n            raise PredictionsValidationError(f\"{task.upper()}: Prediction is missing 'fieldtype'.\")\n\n    for task, docid_to_predictions in task_to_docid_to_predictions.items():\n        if any(\n            not pred.bbox.has_valid_relative_coords()\n            for predictions in docid_to_predictions.values()\n            for pred in predictions\n        ):\n            raise PredictionsValidationError(\n                f\"{task.upper()}: Prediction bbox does not have valid relative coordinates.\"\n            )\n\n    for task, docid_to_predictions in task_to_docid_to_predictions.items():\n        if task == \"kile\":\n            if any(\n                pred.line_item_id is not None\n                for predictions in docid_to_predictions.values()\n                for pred in predictions\n            ):\n                raise PredictionsValidationError(\n                    f\"{task.upper()}: Prediction has extra 'line_item_id'.\"\n                )\n\n        if task == \"lir\":\n            if any(\n                pred.line_item_id is None\n                for predictions in docid_to_predictions.values()\n                for pred in predictions\n            ):\n                raise PredictionsValidationError(\n                    f\"{task.upper()}: Prediction is missing 'line_item_id'.\"\n                )\n\n    for task, docid_to_predictions in task_to_docid_to_predictions.items():\n        have_scores = sum(\n            sum(1 for f in fields if f.score is not None)\n            for fields in docid_to_predictions.values()\n        )\n        if have_scores > 0 and have_scores < sum(\n            len(fields) for fields in docid_to_predictions.values()\n        ):\n            raise PredictionsValidationError(\n                f\"{task.upper()}: Either all or no predictions should have 'score' defined\"\n            )\n\n    for task, docid_to_predictions in task_to_docid_to_predictions.items():\n        extra = len(set(docid_to_predictions.keys()).difference(dataset.docids))\n        missing = len(set(dataset.docids).difference(docid_to_predictions.keys()))\n        if extra:\n            raise PredictionsValidationError(\n                f\"{task.upper()}: Predictions provided for {extra} documents not in the dataset \"\n                f\"{dataset.name}.\"\n            )\n        if missing:\n            raise PredictionsValidationError(\n                f\"{task.upper()}: Predictions not provided for {missing}/{len(dataset)} documents. \"\n                \"Pass an empty list of predictions for these documents if this was intended.\"\n            )\n\n    for task, docid_to_predictions in task_to_docid_to_predictions.items():\n        max_ap_only_score = max(\n            (\n                pred.score\n                for predictions in docid_to_predictions.values()\n                for pred in predictions\n                if pred.use_only_for_ap and pred.score is not None\n            ),\n            default=0,\n        )\n        min_not_ap_only_score = min(\n            (\n                pred.score\n                for predictions in docid_to_predictions.values()\n                for pred in predictions\n                if not pred.use_only_for_ap and pred.score is not None\n            ),\n            default=1,\n        )\n        if max_ap_only_score > min_not_ap_only_score:\n            logger.warning(\n                f\"{task.upper()}: Found a prediction with use_only_for_ap=True that has a higher \"\n                f\"score ({max_ap_only_score}) than another prediction with use_only_for_ap=False \"\n                f\"({min_not_ap_only_score}). Note that all predictions with use_only_for_ap=True \"\n                \"will be used (matched, counted in AP) only after all of the predictions with \"\n                \"use_only_for_ap=False anyway.\"\n            )", "\n\ndef _sort_predictions(docid_to_matching: Mapping[str, FieldMatching]) -> Sequence[bool]:\n    \"\"\"\n    Collect and sort predictions from the given field matchings.\n\n    Returns\n    -------\n    Indicator for each prediction whether it was matched, sorted by the criteria explained in\n    `_get_prediction_sort_key`.\n    \"\"\"\n    sort_key_prediction_matched: List[Tuple[PredictionSortKey, bool]] = []\n    total_annotations = 0\n    for docid, matching in docid_to_matching.items():\n        for pred_i, (pred, gold) in enumerate(matching.ordered_predictions_with_match):\n            sort_key_prediction_matched.append(\n                (_get_prediction_sort_key(pred.score_sort_key, pred_i, docid), gold is not None)\n            )\n        total_annotations += len(matching.annotations)\n\n    return [\n        matched\n        for _sort_key, matched in sorted(sort_key_prediction_matched, key=operator.itemgetter(0))\n    ]", "\n\ndef _get_prediction_sort_key(\n    score_sort_key: Tuple[bool, float], prediction_i: int, docid: str\n) -> PredictionSortKey:\n    \"\"\"\n    Get a sort key for a prediction.\n\n    For evaluation purposes, predictions are sorted by these criteria (sorted by importance):\n    1.  Score from the highest to the lowest.\n    2.  Original order in which the predictions were passed in.\n    3.  The document id. Document id is hashed together with the prediction_i to make sure\n        documents are not always sorted in the same order (for different prediction indices) which\n        would make some documents more important for the evaluation than others.\n\n    Parameters\n    ----------\n    score\n        Prediction score (confidence).\n    prediction_i\n        The original rank of the prediction for the document as given on the input.\n    docid\n        Document ID\n\n    Returns\n    -------\n    A tuple whose ordering corresponds to the criteria described above.\n    \"\"\"\n    hashed_docid = hashlib.sha1(docid.encode())\n    hashed_docid.update(prediction_i.to_bytes(8, \"little\"))\n    return (score_sort_key, prediction_i, hashed_docid.hexdigest()[:16])", ""]}
{"filename": "docile/evaluation/pcc_field_matching.py", "chunked_list": ["import dataclasses\nimport itertools\nfrom collections import defaultdict\nfrom typing import Any, Dict, List, Mapping, Optional, Sequence, Tuple, Union\n\nfrom docile.dataset import BBox, Field\nfrom docile.evaluation.pcc import PCCSet\n\n# Small value for robust >= on floats.\nEPS = 1e-6", "# Small value for robust >= on floats.\nEPS = 1e-6\n\n\n@dataclasses.dataclass(frozen=True)\nclass MatchedPair:\n    pred: Field\n    gold: Field\n\n", "\n\n@dataclasses.dataclass(frozen=True)\nclass FieldMatching:\n    \"\"\"\n    Structure to represent matching between two sets of fields, predictions and annotations.\n\n    Predictions are stored in the original order in `ordered_predictions_with_match` together with\n    the matched annotations (also called gold fields) or None (when they are not matched). If you\n    do not care about the original order of predictions, you can use the following\n    attributes/properties:\n    * matches: i.e., true positives. Sequence of matched pairs.\n    * false_positives: predictions that were not matched.\n    * false_negatives: annotations that were not matched.\n    \"\"\"\n\n    ordered_predictions_with_match: Sequence[Tuple[Field, Optional[Field]]]\n    false_negatives: Sequence[Field]  # not matched annotations\n\n    @property\n    def matches(self) -> Sequence[MatchedPair]:\n        \"\"\"Return matched pairs of predicted and gold fields.\"\"\"\n        return [\n            MatchedPair(pred=pred, gold=gold)\n            for pred, gold in self.ordered_predictions_with_match\n            if gold is not None\n        ]\n\n    @property\n    def false_positives(self) -> Sequence[Field]:\n        return [pred for pred, gold in self.ordered_predictions_with_match if gold is None]\n\n    @property\n    def predictions(self) -> Sequence[Field]:\n        return [pred for pred, _gold in self.ordered_predictions_with_match]\n\n    @property\n    def annotations(self) -> Sequence[Field]:\n        return list(\n            itertools.chain(\n                (gold for _pred, gold in self.ordered_predictions_with_match if gold is not None),\n                self.false_negatives,\n            )\n        )\n\n    @classmethod\n    def empty(cls, predictions: Sequence[Field], annotations: Sequence[Field]) -> \"FieldMatching\":\n        return cls(\n            ordered_predictions_with_match=[(pred, None) for pred in predictions],\n            false_negatives=annotations,\n        )\n\n    def filter(\n        self, same_text: bool = False, fieldtype: str = \"\", exclude_only_for_ap: bool = False\n    ) -> \"FieldMatching\":\n        \"\"\"\n        Filter matching based on the given options.\n\n        Parameters\n        ----------\n        same_text\n            If true, unmatch predictions whose text does not exactly match the ground truth in the\n            matched annotation.\n        fieldtype\n            If nonempty only keep predictions and annotations with the specified fieldtype.\n        exclude_only_for_ap\n            Remove all predictions with the flag `use_only_for_ap`.\n        \"\"\"\n\n        def is_fieldtype_ok(ft: Optional[str]) -> bool:\n            return fieldtype == \"\" or ft == fieldtype\n\n        new_false_negatives = [\n            gold for gold in self.false_negatives if is_fieldtype_ok(gold.fieldtype)\n        ]\n        new_ordered_predictions_with_match: List[Tuple[Field, Optional[Field]]] = []\n        for pred, gold in self.ordered_predictions_with_match:\n            if not is_fieldtype_ok(pred.fieldtype):\n                continue\n            if exclude_only_for_ap and pred.use_only_for_ap:\n                if gold is not None:\n                    new_false_negatives.append(gold)\n                continue\n            if gold is not None and same_text and pred.text != gold.text:\n                new_false_negatives.append(gold)\n                new_ordered_predictions_with_match.append((pred, None))\n            else:\n                new_ordered_predictions_with_match.append((pred, gold))\n        return self.__class__(new_ordered_predictions_with_match, new_false_negatives)\n\n    @classmethod\n    def from_dict(cls, dct: Mapping[str, Any]) -> \"FieldMatching\":\n        dct_decoded_fields = {key: cls._decode_fields(sequence) for key, sequence in dct.items()}\n        return cls(**dct_decoded_fields)  # type: ignore\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            field.name: self._encode_fields(getattr(self, field.name))\n            for field in dataclasses.fields(self)\n        }\n\n    @staticmethod\n    def _decode_fields(\n        collection: Union[Mapping, Sequence, Tuple, None]\n    ) -> Union[Field, List, Tuple, None]:\n        if collection is None:\n            return None\n        if isinstance(collection, dict):\n            return Field.from_dict(collection)\n        if isinstance(collection, tuple):\n            return tuple(FieldMatching._decode_fields(item) for item in collection)\n        if isinstance(collection, list):\n            return [FieldMatching._decode_fields(item) for item in collection]\n        raise ValueError(f\"Unexpected type {type(collection)} while decoding fields\")\n\n    @staticmethod\n    def _encode_fields(\n        collection: Union[Field, Sequence, Tuple, None]\n    ) -> Union[Mapping, List, Tuple, None]:\n        if collection is None:\n            return None\n        if isinstance(collection, Field):\n            return collection.to_dict()\n        if isinstance(collection, tuple):\n            return tuple(FieldMatching._encode_fields(item) for item in collection)\n        if isinstance(collection, list):\n            return [FieldMatching._encode_fields(item) for item in collection]\n        raise ValueError(f\"Unexpected type {type(collection)} while encoding fields\")", "\n\ndef pccs_iou(pcc_set: PCCSet, gold_bbox: BBox, pred_bbox: BBox, page: int) -> float:\n    \"\"\"Calculate IOU over Pseudo Character Centers.\"\"\"\n    if not gold_bbox.intersects(pred_bbox):\n        return 0\n\n    golds = pcc_set.get_covered_pccs(gold_bbox, page)\n    preds = pcc_set.get_covered_pccs(pred_bbox, page)\n\n    if len(golds) == len(preds) == 0:\n        return 1\n\n    return len(golds.intersection(preds)) / len(golds.union(preds))", "\n\ndef get_matches(\n    predictions: Sequence[Field],\n    annotations: Sequence[Field],\n    pcc_set: PCCSet,\n    iou_threshold: float = 1,\n) -> FieldMatching:\n    \"\"\"\n    Find matching between predictions and annotations.\n\n    Parameters\n    ----------\n    predictions\n        Either KILE fields from one page/document or LI fields from one line item. Notice\n        that one line item can span multiple pages. These predictions are being matched in the\n        sorted order by 'score' and in the original order if no score is given (or is equal for\n        several predictions).\n    annotations\n        KILE or LI gold fields for the same page/document.\n    pcc_set\n        Pseudo-Character-Centers (PCCs) covering all pages that have any of the\n        predictions/annotations fields.\n    iou_threshold\n        Necessary 'intersection / union' to accept a pair of fields as a match. The official\n        evaluation uses threshold 1.0 but lower thresholds can be used for debugging.\n    \"\"\"\n    fieldtype_page_to_annotations = defaultdict(lambda: defaultdict(list))\n    for a in annotations:\n        fieldtype_page_to_annotations[a.fieldtype][a.page].append(a)\n\n    ordered_predictions_with_match: List[Tuple[Field, Optional[Field]]] = [\n        (pred, None) for pred in predictions\n    ]\n    for pred_i, pred in sorted(enumerate(predictions), key=_get_sort_key_by_score):\n        gold_candidates = fieldtype_page_to_annotations[pred.fieldtype][pred.page]\n        for gold_i, gold in enumerate(gold_candidates):\n            iou = pccs_iou(\n                pcc_set=pcc_set,\n                gold_bbox=gold.bbox,\n                pred_bbox=pred.bbox,\n                page=pred.page,\n            )\n            # This is equivalent to iou >= iou_threshold but accounts for rounding errors\n            if iou > iou_threshold - EPS:\n                ordered_predictions_with_match[pred_i] = (pred, gold)\n                gold_candidates.pop(gold_i)\n                break\n\n    false_negatives = [\n        field\n        for page_to_fields in fieldtype_page_to_annotations.values()\n        for fields in page_to_fields.values()\n        for field in fields\n    ]\n\n    return FieldMatching(ordered_predictions_with_match, false_negatives)", "\n\ndef _get_sort_key_by_score(pred_with_index: Tuple[int, Field]) -> Tuple[Tuple[bool, float], int]:\n    \"\"\"Sort predictions by score, use original order for equal scores.\"\"\"\n    pred_i, pred = pred_with_index\n    return (pred.score_sort_key, pred_i)\n"]}
{"filename": "docile/evaluation/average_precision.py", "chunked_list": ["from typing import Sequence\n\n\ndef compute_average_precision(\n    sorted_predictions_matched: Sequence[bool],\n    total_annotations: int,\n) -> float:\n    \"\"\"\n    Compute average precision (AP).\n\n    There are some design decisions that influence the result of AP computation. These were done in\n    line with how AP is computed in COCO evaluation for object detection.\n    1.  When the precision-recall curve has a zig-zag pattern (precision increased for higher\n        recall), the \"gaps are filled\".\n    2.  For two consecutive (recall,precision) pairs (r1,p1), (r2,p2) where r2>r1 we use the\n        precision 'p2' in the interval [r1,r2] when computing the Average Precision.\n\n    Points 1. and 2. can be also explained as computing the integral (from 0 to 1) over a function\n    'precision(r)' which is defined as:\n        precision(r) == max{p' | there exists (p',r') with r' >= r}\n\n\n    Parameters\n    ----------\n    sorted_predictions_matched\n        An indicator for each prediction whether it was matched or not. Predictions should be\n        sorted by score (from the highest to the lowest).\n    total_annotations\n        Total number of ground truth annotations, used to calculate the recall.\n\n    Returns\n    -------\n    Average precision metric\n    \"\"\"\n    if total_annotations == 0:\n        return 0.0\n\n    recall_precision_pairs = [[0.0, 1.0]]  # the precision here is not used\n    true_positives = 0\n    observed_predictions = 0\n\n    # Iteratively update precision and recall.\n    for matched in sorted_predictions_matched:\n        true_positives += 1 if matched else 0\n        observed_predictions += 1\n        recall_precision_pairs.append(\n            [(true_positives / total_annotations), (true_positives / observed_predictions)]\n        )\n\n    # Update precision to maximum precision for any larger recall\n    for recall_precision, recall_precision_prev in zip(\n        recall_precision_pairs[:0:-1], recall_precision_pairs[-2::-1]\n    ):\n        recall_precision_prev[1] = max(recall_precision_prev[1], recall_precision[1])\n\n    average_precision = 0.0\n    for recall_precision, recall_precision_next in zip(\n        recall_precision_pairs[:-1], recall_precision_pairs[1:]\n    ):\n        # Notice that if there are multiple (recall,precision) pairs with the same recall, they are\n        # sorted by precision (from highest to lowest). This means that only the first point (with\n        # highest precision) influences the result (for the rest 'recall_diff == 0').\n        recall_diff = recall_precision_next[0] - recall_precision[0]\n        precision = recall_precision_next[1]\n        average_precision += recall_diff * precision\n\n    return average_precision", ""]}
{"filename": "docile/evaluation/__init__.py", "chunked_list": ["from docile.evaluation.evaluate import EvaluationResult, evaluate_dataset\nfrom docile.evaluation.evaluation_subsets import NamedRange, get_evaluation_subsets\nfrom docile.evaluation.pcc import PCC, PCCSet, get_document_pccs\n\n__all__ = [\n    \"EvaluationResult\",\n    \"NamedRange\",\n    \"PCC\",\n    \"PCCSet\",\n    \"evaluate_dataset\",", "    \"PCCSet\",\n    \"evaluate_dataset\",\n    \"get_document_pccs\",\n    \"get_evaluation_subsets\",\n]\n"]}
{"filename": "docile/evaluation/pcc.py", "chunked_list": ["import dataclasses\nimport functools\nimport logging\nfrom bisect import bisect_left, bisect_right\nfrom collections import defaultdict\nfrom typing import List, Sequence, Set\n\nfrom docile.dataset import BBox, Document, Field\n\nlogger = logging.getLogger(__name__)", "\nlogger = logging.getLogger(__name__)\n\n\n@dataclasses.dataclass(frozen=True)\nclass PCC:\n    \"\"\"Class for a single Pseudo-Character Center (PCC), i.e., a position in the document.\"\"\"\n\n    x: float\n    y: float\n    page: int", "\n\nclass PCCSet:\n    def __init__(self, pccs: Sequence[PCC]) -> None:\n        page_to_pccs = defaultdict(list)\n        for pcc in pccs:\n            page_to_pccs[pcc.page].append(pcc)\n\n        self._page_to_sorted_x_pccs = {\n            page: sorted(page_pccs, key=lambda p: p.x) for page, page_pccs in page_to_pccs.items()\n        }\n        self._page_to_sorted_y_pccs = {\n            page: sorted(page_pccs, key=lambda p: p.y) for page, page_pccs in page_to_pccs.items()\n        }\n\n    def get_covered_pccs(self, bbox: BBox, page: int) -> Set[PCC]:\n        \"\"\"Return all pccs on `page` covered by `bbox`.\"\"\"\n\n        # All pccs on the page are sorted by x and y coordinates. Then we find all pccs between\n        # [bbox.left, bbox.right] and all pccs between [bbox.top, bbox.bottom] and return pccs in\n        # the intersection of these two sets.\n        sorted_x_pccs = self._page_to_sorted_x_pccs[page]\n        sorted_x_pccs_x_only = [pcc.x for pcc in sorted_x_pccs]\n        i_l = bisect_left(sorted_x_pccs_x_only, bbox.left)\n        i_r = bisect_right(sorted_x_pccs_x_only, bbox.right)\n        x_subset = set(sorted_x_pccs[i_l:i_r])\n\n        sorted_y_pccs = self._page_to_sorted_y_pccs[page]\n        sorted_y_pccs_y_only = [pcc.y for pcc in sorted_y_pccs]\n        i_t = bisect_left(sorted_y_pccs_y_only, bbox.top)\n        i_b = bisect_right(sorted_y_pccs_y_only, bbox.bottom)\n        y_subset = set(sorted_y_pccs[i_t:i_b])\n\n        return x_subset.intersection(y_subset)", "\n\ndef get_document_pccs(document: Document) -> PCCSet:\n    \"\"\"\n    Get all Pseudo-Character Centers (PCCs) for the whole document.\n\n    PCCs are computed from all OCR words that were snapped to the text.\n    \"\"\"\n    pccs = []\n    for word in _get_snapped_ocr_words(document):\n        if word.text is None or word.text == \"\":\n            logger.debug(f\"Cannot generate PCCs for OCR word with empty text: {word}\")\n            continue\n        pccs.extend(_calculate_pccs(word.bbox, word.text, word.page))\n\n    return PCCSet(pccs)", "\n\ndef _get_snapped_ocr_words(document: Document) -> List[Field]:\n    \"\"\"Get OCR words snapped to the text.\"\"\"\n    words = []\n    with document:\n        for page in range(document.annotation.page_count):\n            words.extend(\n                document.ocr.get_all_words(\n                    page=page,\n                    snapped=True,\n                    use_cached_snapping=True,\n                    get_page_image=functools.partial(document.page_image, page),\n                )\n            )\n    return words", "\n\ndef _calculate_pccs(bbox: BBox, text: str, page: int) -> List[PCC]:\n    if text == \"\":\n        raise ValueError(\"Cannot calculate PCCs from empty text\")\n\n    char_width = (bbox.right - bbox.left) / len(text)\n    y_middle = (bbox.top + bbox.bottom) / 2\n    return [\n        PCC(x=bbox.left + (i + 1 / 2) * char_width, y=y_middle, page=page)\n        for i in range(len(text))\n    ]", ""]}
{"filename": "docile/dataset/document_images.py", "chunked_list": ["import logging\nfrom typing import List\n\nfrom pdf2image import convert_from_bytes\nfrom PIL import Image\n\nfrom docile.dataset.cached_object import CachedObject, CachingConfig\nfrom docile.dataset.paths import DataPaths, PathMaybeInZip\nfrom docile.dataset.types import OptionalImageSize\n", "from docile.dataset.types import OptionalImageSize\n\nlogger = logging.getLogger(__name__)\n\n\nclass DocumentImages(CachedObject[List[Image.Image]]):\n    def __init__(\n        self,\n        path: PathMaybeInZip,\n        pdf_path: PathMaybeInZip,\n        page_count: int,\n        size: OptionalImageSize = (None, None),\n        cache: CachingConfig = CachingConfig.OFF,\n    ):\n        \"\"\"\n        Convert PDF Document to images for its pages.\n\n        Parameters\n        ----------\n        path\n            Path to directory where images of individual pages are stored.\n        pdf_path\n            Path to the input pdf document.\n        page_count\n            Number of pages of the document.\n        size\n            Check https://pdf2image.readthedocs.io/en/latest/reference.html for documentation of\n            this parameter.\n        cache\n            Whether to cache images generated from pdfs to disk and/or to memory.\n        \"\"\"\n        super().__init__(path=path, cache=cache)\n        self.pdf_path = pdf_path\n        self.page_count = page_count\n        self.size = size\n\n    def from_disk(self) -> List[Image.Image]:\n        images = []\n        for page_i in range(self.page_count):\n            page_path = DataPaths.cache_page_image_path(self.path, page_i)\n            with Image.open(str(page_path)) as page_img:\n                try:\n                    page_img.load()\n                except Exception as e:\n                    logger.error(\n                        f\"Error while loading image {page_path}, consider removing directory \"\n                        f\"{self.path} from cache\"\n                    )\n                    raise e\n                images.append(page_img)\n        return images\n\n    def to_disk(self, content: List[Image.Image]) -> None:\n        self.path.full_path.mkdir(parents=True, exist_ok=True)\n        for page_i in range(self.page_count):\n            page_path = DataPaths.cache_page_image_path(self.path, page_i)\n            content[page_i].save(str(page_path.full_path))\n\n    def predict(self) -> List[Image.Image]:\n        images = convert_from_bytes(self.pdf_path.read_bytes(), size=self.size)\n        if len(images) != self.page_count:\n            raise RuntimeError(\n                f\"Generated unexpected number of images: {len(images)} (expected: {self.page_count}\"\n            )\n        return images", ""]}
{"filename": "docile/dataset/types.py", "chunked_list": ["from typing import Optional, Tuple, Union\n\nOptionalImageSize = Union[int, Tuple[Optional[int], Optional[int]]]\n"]}
{"filename": "docile/dataset/bbox.py", "chunked_list": ["import dataclasses\nfrom functools import reduce\nfrom typing import Generic, Tuple, TypeVar, no_type_check\n\nT = TypeVar(\"T\", int, float)\n\n\n@dataclasses.dataclass(frozen=True)\nclass BBox(Generic[T]):\n    left: T\n    top: T\n    right: T\n    bottom: T\n\n    def to_absolute_coords(self, width: float, height: float) -> \"BBox[int]\":\n        return BBox(\n            round(self.left * width),\n            round(self.top * height),\n            round(self.right * width),\n            round(self.bottom * height),\n        )\n\n    def to_relative_coords(self, width: float, height: float) -> \"BBox[float]\":\n        return BBox(\n            self.left / width,\n            self.top / height,\n            self.right / width,\n            self.bottom / height,\n        )\n\n    def has_valid_relative_coords(self) -> bool:\n        return 0 <= self.left <= self.right <= 1 and 0 <= self.top <= self.bottom <= 1\n\n    def to_tuple(self) -> Tuple[T, T, T, T]:\n        return self.left, self.top, self.right, self.bottom\n\n    def intersects(self, other: \"BBox\") -> bool:\n        if self.left > other.right or other.left > self.right:\n            return False\n        if self.top > other.bottom or other.top > self.bottom:\n            return False\n        return True\n\n    def union(self, *others: \"BBox[T]\") -> \"BBox[T]\":\n        if not others:\n            return self\n        return self.__class__(\n            min(self, *others, key=lambda bbox: bbox.left).left,\n            min(self, *others, key=lambda bbox: bbox.top).top,\n            max(self, *others, key=lambda bbox: bbox.right).right,\n            max(self, *others, key=lambda bbox: bbox.bottom).bottom,\n        )\n\n    def __and__(self, other: \"BBox[T]\") -> \"BBox[T]\":\n        l1, t1, r1, b1 = self.to_tuple()\n        l2, t2, r2, b2 = other.to_tuple()\n        l, t, r, b = max(l1, l2), max(t1, t2), min(r1, r2), min(b1, b2)\n        return self.__class__(l, t, r, b) if l < r and t < b else self.zero_bbox()\n\n    def __or__(self, other: \"BBox[T]\") -> \"BBox[T]\":\n        return self.union(other)\n\n    @classmethod\n    def zero_bbox(cls) -> \"BBox[T]\":\n        return cls(0, 0, 0, 0)\n\n    @property\n    def width(self) -> T:\n        return self.right - self.left\n\n    @property\n    def height(self) -> T:\n        return self.bottom - self.top\n\n    @property\n    def size(self) -> Tuple[T, T]:\n        return self.width, self.height\n\n    @property\n    def area(self) -> T:\n        return self.width * self.height\n\n    @property\n    def centroid(self) -> Tuple[T, T]:\n        ctr = ((self.left + self.right) / 2), ((self.top + self.bottom) / 2)\n        return (round(ctr[0]), round(ctr[1])) if isinstance(self.left, int) else ctr\n\n    @no_type_check\n    def intersection(self, *others: \"BBox[T]\") -> \"BBox[T]\":\n        return reduce(self.__class__.__and__, others, self)", "class BBox(Generic[T]):\n    left: T\n    top: T\n    right: T\n    bottom: T\n\n    def to_absolute_coords(self, width: float, height: float) -> \"BBox[int]\":\n        return BBox(\n            round(self.left * width),\n            round(self.top * height),\n            round(self.right * width),\n            round(self.bottom * height),\n        )\n\n    def to_relative_coords(self, width: float, height: float) -> \"BBox[float]\":\n        return BBox(\n            self.left / width,\n            self.top / height,\n            self.right / width,\n            self.bottom / height,\n        )\n\n    def has_valid_relative_coords(self) -> bool:\n        return 0 <= self.left <= self.right <= 1 and 0 <= self.top <= self.bottom <= 1\n\n    def to_tuple(self) -> Tuple[T, T, T, T]:\n        return self.left, self.top, self.right, self.bottom\n\n    def intersects(self, other: \"BBox\") -> bool:\n        if self.left > other.right or other.left > self.right:\n            return False\n        if self.top > other.bottom or other.top > self.bottom:\n            return False\n        return True\n\n    def union(self, *others: \"BBox[T]\") -> \"BBox[T]\":\n        if not others:\n            return self\n        return self.__class__(\n            min(self, *others, key=lambda bbox: bbox.left).left,\n            min(self, *others, key=lambda bbox: bbox.top).top,\n            max(self, *others, key=lambda bbox: bbox.right).right,\n            max(self, *others, key=lambda bbox: bbox.bottom).bottom,\n        )\n\n    def __and__(self, other: \"BBox[T]\") -> \"BBox[T]\":\n        l1, t1, r1, b1 = self.to_tuple()\n        l2, t2, r2, b2 = other.to_tuple()\n        l, t, r, b = max(l1, l2), max(t1, t2), min(r1, r2), min(b1, b2)\n        return self.__class__(l, t, r, b) if l < r and t < b else self.zero_bbox()\n\n    def __or__(self, other: \"BBox[T]\") -> \"BBox[T]\":\n        return self.union(other)\n\n    @classmethod\n    def zero_bbox(cls) -> \"BBox[T]\":\n        return cls(0, 0, 0, 0)\n\n    @property\n    def width(self) -> T:\n        return self.right - self.left\n\n    @property\n    def height(self) -> T:\n        return self.bottom - self.top\n\n    @property\n    def size(self) -> Tuple[T, T]:\n        return self.width, self.height\n\n    @property\n    def area(self) -> T:\n        return self.width * self.height\n\n    @property\n    def centroid(self) -> Tuple[T, T]:\n        ctr = ((self.left + self.right) / 2), ((self.top + self.bottom) / 2)\n        return (round(ctr[0]), round(ctr[1])) if isinstance(self.left, int) else ctr\n\n    @no_type_check\n    def intersection(self, *others: \"BBox[T]\") -> \"BBox[T]\":\n        return reduce(self.__class__.__and__, others, self)", ""]}
{"filename": "docile/dataset/field.py", "chunked_list": ["import dataclasses\nimport json\nimport warnings\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Mapping, Optional, Sequence, Tuple\n\nfrom docile.dataset.bbox import BBox\n\n\n@dataclasses.dataclass(frozen=True)\nclass Field:\n    bbox: BBox\n    page: int\n    score: Optional[float] = None\n    text: Optional[str] = None\n    fieldtype: Optional[str] = None\n    line_item_id: Optional[int] = None\n\n    # The flag `use_only_for_ap` can be set for some predictions in which case these will be only\n    # used for Average Precision (AP) computation but they will not be used for:\n    # * f1, precision, recall, true positives, false positives, false negatives computation.\n    # * Matching of line items. Line items matching is computed only once for each document (not\n    #   iteratively as in AP matching) from all predictions that have use_only_for_ap=False.\n    #\n    # Notice that for AP it can never hurt to add additional predictions if their score is lower\n    # than the score of all other predictions (across all documents) so this flag can be used for\n    # predictions that would be otherwise discarded as noise.\n    #\n    # Warning: All predictions with this flag set to True are considered to have lower score than\n    # all predictions with this flag set to False, even if provided `score` suggests otherwise.\n    # This does not affect the usual use case which is to set this flag to True for all predictions\n    # with score under some threshold.\n    use_only_for_ap: bool = False\n\n    @classmethod\n    def from_dict(cls, dct: Mapping[str, Any]) -> \"Field\":\n        dct_copy = dict(dct)\n        bbox = BBox(*(dct_copy.pop(\"bbox\")))\n\n        # do not fail on extra keys (if participants save extra information), only warn.\n        expected_keys = {f.name for f in dataclasses.fields(cls)}\n        for k in list(dct_copy.keys()):\n            if k not in expected_keys:\n                warnings.warn(f\"Ignoring unexpected key {k}\", stacklevel=1)\n                dct_copy.pop(k)\n\n        return cls(bbox=bbox, **dct_copy)\n\n    def to_dict(self) -> Dict[str, Any]:\n        dct = dataclasses.asdict(self)\n        dct[\"bbox\"] = dataclasses.astuple(self.bbox)\n        return dct\n\n    @property\n    def score_sort_key(self) -> Tuple[bool, float]:\n        \"\"\"\n        Sort key used to sort predictions by score from highest to lowest.\n\n        This function makes sure that all predictions with `use_only_for_ap=True` come after the\n        remaining predictions.\n        \"\"\"\n        score = self.score if self.score is not None else 0\n\n        return (self.use_only_for_ap, -score)\n\n    def __repr__(self) -> str:\n        dataclass_fields_str = \", \".join(\n            f\"{dataclass_field.name}={getattr(self, dataclass_field.name)!r}\"\n            for dataclass_field in dataclasses.fields(self)\n            if getattr(self, dataclass_field.name) != getattr(dataclass_field, \"default\", None)\n        )\n        return f\"Field({dataclass_fields_str})\"", "\n@dataclasses.dataclass(frozen=True)\nclass Field:\n    bbox: BBox\n    page: int\n    score: Optional[float] = None\n    text: Optional[str] = None\n    fieldtype: Optional[str] = None\n    line_item_id: Optional[int] = None\n\n    # The flag `use_only_for_ap` can be set for some predictions in which case these will be only\n    # used for Average Precision (AP) computation but they will not be used for:\n    # * f1, precision, recall, true positives, false positives, false negatives computation.\n    # * Matching of line items. Line items matching is computed only once for each document (not\n    #   iteratively as in AP matching) from all predictions that have use_only_for_ap=False.\n    #\n    # Notice that for AP it can never hurt to add additional predictions if their score is lower\n    # than the score of all other predictions (across all documents) so this flag can be used for\n    # predictions that would be otherwise discarded as noise.\n    #\n    # Warning: All predictions with this flag set to True are considered to have lower score than\n    # all predictions with this flag set to False, even if provided `score` suggests otherwise.\n    # This does not affect the usual use case which is to set this flag to True for all predictions\n    # with score under some threshold.\n    use_only_for_ap: bool = False\n\n    @classmethod\n    def from_dict(cls, dct: Mapping[str, Any]) -> \"Field\":\n        dct_copy = dict(dct)\n        bbox = BBox(*(dct_copy.pop(\"bbox\")))\n\n        # do not fail on extra keys (if participants save extra information), only warn.\n        expected_keys = {f.name for f in dataclasses.fields(cls)}\n        for k in list(dct_copy.keys()):\n            if k not in expected_keys:\n                warnings.warn(f\"Ignoring unexpected key {k}\", stacklevel=1)\n                dct_copy.pop(k)\n\n        return cls(bbox=bbox, **dct_copy)\n\n    def to_dict(self) -> Dict[str, Any]:\n        dct = dataclasses.asdict(self)\n        dct[\"bbox\"] = dataclasses.astuple(self.bbox)\n        return dct\n\n    @property\n    def score_sort_key(self) -> Tuple[bool, float]:\n        \"\"\"\n        Sort key used to sort predictions by score from highest to lowest.\n\n        This function makes sure that all predictions with `use_only_for_ap=True` come after the\n        remaining predictions.\n        \"\"\"\n        score = self.score if self.score is not None else 0\n\n        return (self.use_only_for_ap, -score)\n\n    def __repr__(self) -> str:\n        dataclass_fields_str = \", \".join(\n            f\"{dataclass_field.name}={getattr(self, dataclass_field.name)!r}\"\n            for dataclass_field in dataclasses.fields(self)\n            if getattr(self, dataclass_field.name) != getattr(dataclass_field, \"default\", None)\n        )\n        return f\"Field({dataclass_fields_str})\"", "\n\ndef store_predictions(path: Path, docid_to_predictions: Mapping[str, Sequence[Field]]) -> None:\n    path.write_text(\n        json.dumps(\n            {\n                docid: [prediction.to_dict() for prediction in predictions]\n                for docid, predictions in docid_to_predictions.items()\n            },\n            indent=2,\n        )\n    )", "\n\ndef load_predictions(path: Path) -> Dict[str, List[Field]]:\n    docid_to_raw_predictions = json.loads(path.read_text())\n    return {\n        docid: [Field.from_dict(prediction) for prediction in predictions]\n        for docid, predictions in docid_to_raw_predictions.items()\n    }\n", ""]}
{"filename": "docile/dataset/dataset.py", "chunked_list": ["import json\nimport logging\nimport os\nfrom pathlib import Path\nfrom random import Random\nfrom typing import Iterator, List, Optional, Sequence, Union, overload\n\nfrom tqdm import tqdm\n\nfrom docile.dataset.cached_object import CachingConfig", "\nfrom docile.dataset.cached_object import CachingConfig\nfrom docile.dataset.document import Document\nfrom docile.dataset.paths import DataPaths\n\nlogger = logging.getLogger(__name__)\n\n\nclass Dataset:\n    \"\"\"Structure representing a dataset, i.e., a collection of documents.\"\"\"\n\n    def __init__(\n        self,\n        split_name: str,\n        dataset_path: Union[Path, str, DataPaths],\n        load_annotations: bool = True,\n        load_ocr: bool = True,\n        cache_images: CachingConfig = CachingConfig.DISK,\n        docids: Optional[Sequence[str]] = None,\n    ):\n        \"\"\"\n        Load dataset from index file or from a custom list of document ids.\n\n        By default, annotations and OCR are loaded from disk into memory. This is useful for\n        smaller datasets -- for 10000 pages it takes 1-2 minutes to load these resources and ~3 GB\n        of memory. Note: The 'train' split has 6759 pages.\n\n        When annotations and OCR are not loaded into memory, you can still temporarily cache\n        document resources in memory by using the document as a context manager:\n        ```\n        with dataset[5] as document:\n            # Now document.annotation, document.ocr and images generated with document.page_image()\n            # are cached in memory.\n        ```\n\n        Parameters\n        ----------\n        split_name\n            Name of the dataset split. If there is an index file stored in the dataset folder (such\n            as for `train`, `val`, `test` or `trainval`), it will be used to load the document ids.\n        dataset_path\n            Path to the root directory with the unzipped dataset or a path to the ZIP file with the\n            dataset.\n        load_annotations\n            If true, annotations for all documents are loaded immediately to memory.\n        load_ocr\n            If true, ocr for all documents are loaded immediately to memory.\n        cache_images\n            Whether to cache images generated from pdfs to disk and/or to memory. Use\n            CachingConfig.OFF if you do not have enough disk space to store all images (e.g., for\n            the unlabeled dataset).\n        docids\n            Custom list of document ids that are part of the dataset split.\n        \"\"\"\n        self.split_name = split_name\n        self.data_paths = DataPaths(dataset_path)\n\n        docids_from_file = self._load_docids_from_index(split_name)\n        if docids is None and docids_from_file is None:\n            raise ValueError(\n                f\"Index file for split {split_name} does not exist and no docids were passed.\"\n            )\n        if docids is not None and docids_from_file is not None and docids != docids_from_file:\n            raise ValueError(\n                f\"Passed docids do not match the content of the index file for split {split_name}.\"\n            )\n        docids = docids if docids is not None else docids_from_file\n        assert docids is not None  # this is guaranteed thanks to the checks above\n\n        documents = [\n            Document(\n                docid=docid,\n                dataset_path=self.data_paths,\n                load_annotations=False,\n                load_ocr=False,\n                cache_images=cache_images,\n            )\n            for docid in tqdm(\n                docids,\n                desc=f\"Initializing documents for {self.name}\",\n                disable=len(docids) <= 10000,\n            )\n        ]\n        self._set_documents(documents)\n        self.load(load_annotations, load_ocr)\n\n    def load(self, annotations: bool = True, ocr: bool = True) -> \"Dataset\":\n        \"\"\"\n        Load document resources to memory.\n\n        It can be useful to delay loading of document resources, e.g., when working with just a\n        sample of a big dataset.\n        ```\n        dataset_sample = (\n            Dataset(\"unlabeled\", DATASET_PATH, load_annotations=False, load_ocr=False)\n            .sample(10)\n            .load()\n        )\n        ```\n\n        Parameters\n        ----------\n        annotations\n            If true, load annotations for all documents to memory.\n        ocr\n            If true, load ocr for all documents to memory.\n\n        Returns\n        -------\n        Dataset (self) with loaded document resources.\n        \"\"\"\n        if not annotations and not ocr:\n            return self\n        if ocr and len(self) > 10000:\n            logger.warning(\n                f\"Loading OCR for {len(self)} documents will have a big memory footprint.\"\n            )\n        for doc in tqdm(self.documents, desc=f\"Loading documents for {self.name}\"):\n            doc.load(annotations, ocr)\n        return self\n\n    def release(self, annotations: bool = True, ocr: bool = True) -> \"Dataset\":\n        \"\"\"Free up document resources from memory.\"\"\"\n        for doc in self.documents:\n            doc.release(annotations, ocr)\n        return self\n\n    @property\n    def name(self) -> str:\n        return f\"{self.data_paths.name}:{self.split_name}\"\n\n    @property\n    def docids(self) -> List[str]:\n        return [doc.docid for doc in self.documents]\n\n    def get_cluster(self, cluster_id: int) -> \"Dataset\":\n        return self.from_documents(\n            split_name=f\"{self.split_name}[cluster_id={cluster_id}]\",\n            documents=[doc for doc in self.documents if doc.annotation.cluster_id == cluster_id],\n        )\n\n    @overload\n    def __getitem__(self, id_or_pos_or_slice: Union[str, int]) -> Document:\n        pass\n\n    @overload\n    def __getitem__(self, id_or_pos_or_slice: slice) -> \"Dataset\":\n        pass\n\n    def __getitem__(\n        self, id_or_pos_or_slice: Union[str, int, slice]\n    ) -> Union[Document, \"Dataset\"]:\n        \"\"\"\n        Get a single document or a sliced dataset.\n\n        The function has three possible behaviours based on the parameter type:\n        * If the parameter is string, return the document with this docid\n        * If the parameter is int, return the document with this index\n        * If the parameter is slice, return a new Dataset representing the corresponding subset of\n          documents.\n        \"\"\"\n        if isinstance(id_or_pos_or_slice, slice):\n            str_start = \"\" if id_or_pos_or_slice.start is None else str(id_or_pos_or_slice.start)\n            str_stop = \"\" if id_or_pos_or_slice.stop is None else str(id_or_pos_or_slice.stop)\n            if id_or_pos_or_slice.step is None:\n                str_slice = f\"{str_start}:{str_stop}\"\n            else:\n                str_slice = f\"{str_start}:{str_stop}:{id_or_pos_or_slice.step}\"\n            return self.from_documents(\n                split_name=f\"{self.split_name}[{str_slice}]\",\n                documents=self.documents[id_or_pos_or_slice],\n            )\n        if isinstance(id_or_pos_or_slice, str):\n            return self.documents[self.docid_to_index[id_or_pos_or_slice]]\n        elif isinstance(id_or_pos_or_slice, int):\n            return self.documents[id_or_pos_or_slice]\n        raise KeyError(f\"Unknown document ID or index {id_or_pos_or_slice}.\")\n\n    def sample(self, sample_size: int, seed: Optional[int] = None) -> \"Dataset\":\n        \"\"\"\n        Return a dataset with a random subsample of the current documents.\n\n        Parameters\n        ----------\n        sample_size\n            Number of documents in the sample\n        seed\n            Random seed to be used for the subsample. If None, it will be chosen randomly.\n        \"\"\"\n        if seed is None:\n            seed = int.from_bytes(os.urandom(8), \"big\")\n        rng = Random(seed)\n        sample_documents = rng.sample(self.documents, sample_size)\n        split_name = f\"{self.split_name}[sample({sample_size},seed={seed})]\"\n        return self.from_documents(split_name, sample_documents)\n\n    def __iter__(self) -> Iterator[Document]:\n        \"\"\"Iterate over documents in the dataset, temporarily turning on memory caching.\"\"\"\n        for document in self.documents:\n            with document:\n                yield document\n\n    def __len__(self) -> int:\n        return len(self.documents)\n\n    def total_page_count(self) -> int:\n        return sum(doc.page_count for doc in self.documents)\n\n    def __str__(self) -> str:\n        return f\"Dataset({self.name})\"\n\n    def __repr__(self) -> str:\n        return (\n            f\"Dataset(split_name={self.split_name!r}, \"\n            f\"dataset_path={self.data_paths.dataset_path.root_path!r})\"\n        )\n\n    @classmethod\n    def from_documents(\n        cls,\n        split_name: str,\n        documents: Sequence[Document],\n    ) -> \"Dataset\":\n        \"\"\"\n        Create a dataset directly from documents, rather than from docids.\n\n        This is useful when the documents were already loaded once, e.g., when creating a dataset\n        with just a sample of the current documents.\n        \"\"\"\n        if len(documents) == 0:\n            raise ValueError(\"Cannot create a dataset with no documents\")\n\n        data_paths = documents[0].data_paths\n        dataset = cls(\n            split_name=split_name,\n            dataset_path=data_paths,\n            # Do not load annotations and OCR since it might be already loaded once in `documents`.\n            load_annotations=False,\n            load_ocr=False,\n            cache_images=CachingConfig.OFF,\n            docids=[doc.docid for doc in documents],\n        )\n        dataset._set_documents(documents)\n        return dataset\n\n    def store_index(self) -> None:\n        \"\"\"Store dataset index to disk.\"\"\"\n        index_path = self.data_paths.dataset_index_path(self.split_name)\n        if index_path.exists():\n            raise RuntimeError(\n                f\"Index file for {self} already exists at path {index_path}. Delete it first if \"\n                \"you want to overwrite it.\"\n            )\n\n        index_path.write_text(json.dumps(self.docids, indent=2))\n        logger.info(f\"Stored index for {self} to file {index_path}\")\n\n    def _load_docids_from_index(self, split_name: str) -> Optional[Sequence[str]]:\n        \"\"\"\n        Load docids from the index file on disk.\n\n        Returns\n        -------\n        Docids loaded from the index file or None if the file does not exist.\n        \"\"\"\n        index_path = self.data_paths.dataset_index_path(split_name)\n        if index_path.exists():\n            return json.loads(index_path.read_bytes())\n        return None\n\n    def _set_documents(self, documents: Sequence[Document]) -> None:\n        \"\"\"Set dataset documents to the provided documents.\"\"\"\n        self.documents = documents\n        self.docid_to_index = {doc.docid: i for i, doc in enumerate(self.documents)}", "class Dataset:\n    \"\"\"Structure representing a dataset, i.e., a collection of documents.\"\"\"\n\n    def __init__(\n        self,\n        split_name: str,\n        dataset_path: Union[Path, str, DataPaths],\n        load_annotations: bool = True,\n        load_ocr: bool = True,\n        cache_images: CachingConfig = CachingConfig.DISK,\n        docids: Optional[Sequence[str]] = None,\n    ):\n        \"\"\"\n        Load dataset from index file or from a custom list of document ids.\n\n        By default, annotations and OCR are loaded from disk into memory. This is useful for\n        smaller datasets -- for 10000 pages it takes 1-2 minutes to load these resources and ~3 GB\n        of memory. Note: The 'train' split has 6759 pages.\n\n        When annotations and OCR are not loaded into memory, you can still temporarily cache\n        document resources in memory by using the document as a context manager:\n        ```\n        with dataset[5] as document:\n            # Now document.annotation, document.ocr and images generated with document.page_image()\n            # are cached in memory.\n        ```\n\n        Parameters\n        ----------\n        split_name\n            Name of the dataset split. If there is an index file stored in the dataset folder (such\n            as for `train`, `val`, `test` or `trainval`), it will be used to load the document ids.\n        dataset_path\n            Path to the root directory with the unzipped dataset or a path to the ZIP file with the\n            dataset.\n        load_annotations\n            If true, annotations for all documents are loaded immediately to memory.\n        load_ocr\n            If true, ocr for all documents are loaded immediately to memory.\n        cache_images\n            Whether to cache images generated from pdfs to disk and/or to memory. Use\n            CachingConfig.OFF if you do not have enough disk space to store all images (e.g., for\n            the unlabeled dataset).\n        docids\n            Custom list of document ids that are part of the dataset split.\n        \"\"\"\n        self.split_name = split_name\n        self.data_paths = DataPaths(dataset_path)\n\n        docids_from_file = self._load_docids_from_index(split_name)\n        if docids is None and docids_from_file is None:\n            raise ValueError(\n                f\"Index file for split {split_name} does not exist and no docids were passed.\"\n            )\n        if docids is not None and docids_from_file is not None and docids != docids_from_file:\n            raise ValueError(\n                f\"Passed docids do not match the content of the index file for split {split_name}.\"\n            )\n        docids = docids if docids is not None else docids_from_file\n        assert docids is not None  # this is guaranteed thanks to the checks above\n\n        documents = [\n            Document(\n                docid=docid,\n                dataset_path=self.data_paths,\n                load_annotations=False,\n                load_ocr=False,\n                cache_images=cache_images,\n            )\n            for docid in tqdm(\n                docids,\n                desc=f\"Initializing documents for {self.name}\",\n                disable=len(docids) <= 10000,\n            )\n        ]\n        self._set_documents(documents)\n        self.load(load_annotations, load_ocr)\n\n    def load(self, annotations: bool = True, ocr: bool = True) -> \"Dataset\":\n        \"\"\"\n        Load document resources to memory.\n\n        It can be useful to delay loading of document resources, e.g., when working with just a\n        sample of a big dataset.\n        ```\n        dataset_sample = (\n            Dataset(\"unlabeled\", DATASET_PATH, load_annotations=False, load_ocr=False)\n            .sample(10)\n            .load()\n        )\n        ```\n\n        Parameters\n        ----------\n        annotations\n            If true, load annotations for all documents to memory.\n        ocr\n            If true, load ocr for all documents to memory.\n\n        Returns\n        -------\n        Dataset (self) with loaded document resources.\n        \"\"\"\n        if not annotations and not ocr:\n            return self\n        if ocr and len(self) > 10000:\n            logger.warning(\n                f\"Loading OCR for {len(self)} documents will have a big memory footprint.\"\n            )\n        for doc in tqdm(self.documents, desc=f\"Loading documents for {self.name}\"):\n            doc.load(annotations, ocr)\n        return self\n\n    def release(self, annotations: bool = True, ocr: bool = True) -> \"Dataset\":\n        \"\"\"Free up document resources from memory.\"\"\"\n        for doc in self.documents:\n            doc.release(annotations, ocr)\n        return self\n\n    @property\n    def name(self) -> str:\n        return f\"{self.data_paths.name}:{self.split_name}\"\n\n    @property\n    def docids(self) -> List[str]:\n        return [doc.docid for doc in self.documents]\n\n    def get_cluster(self, cluster_id: int) -> \"Dataset\":\n        return self.from_documents(\n            split_name=f\"{self.split_name}[cluster_id={cluster_id}]\",\n            documents=[doc for doc in self.documents if doc.annotation.cluster_id == cluster_id],\n        )\n\n    @overload\n    def __getitem__(self, id_or_pos_or_slice: Union[str, int]) -> Document:\n        pass\n\n    @overload\n    def __getitem__(self, id_or_pos_or_slice: slice) -> \"Dataset\":\n        pass\n\n    def __getitem__(\n        self, id_or_pos_or_slice: Union[str, int, slice]\n    ) -> Union[Document, \"Dataset\"]:\n        \"\"\"\n        Get a single document or a sliced dataset.\n\n        The function has three possible behaviours based on the parameter type:\n        * If the parameter is string, return the document with this docid\n        * If the parameter is int, return the document with this index\n        * If the parameter is slice, return a new Dataset representing the corresponding subset of\n          documents.\n        \"\"\"\n        if isinstance(id_or_pos_or_slice, slice):\n            str_start = \"\" if id_or_pos_or_slice.start is None else str(id_or_pos_or_slice.start)\n            str_stop = \"\" if id_or_pos_or_slice.stop is None else str(id_or_pos_or_slice.stop)\n            if id_or_pos_or_slice.step is None:\n                str_slice = f\"{str_start}:{str_stop}\"\n            else:\n                str_slice = f\"{str_start}:{str_stop}:{id_or_pos_or_slice.step}\"\n            return self.from_documents(\n                split_name=f\"{self.split_name}[{str_slice}]\",\n                documents=self.documents[id_or_pos_or_slice],\n            )\n        if isinstance(id_or_pos_or_slice, str):\n            return self.documents[self.docid_to_index[id_or_pos_or_slice]]\n        elif isinstance(id_or_pos_or_slice, int):\n            return self.documents[id_or_pos_or_slice]\n        raise KeyError(f\"Unknown document ID or index {id_or_pos_or_slice}.\")\n\n    def sample(self, sample_size: int, seed: Optional[int] = None) -> \"Dataset\":\n        \"\"\"\n        Return a dataset with a random subsample of the current documents.\n\n        Parameters\n        ----------\n        sample_size\n            Number of documents in the sample\n        seed\n            Random seed to be used for the subsample. If None, it will be chosen randomly.\n        \"\"\"\n        if seed is None:\n            seed = int.from_bytes(os.urandom(8), \"big\")\n        rng = Random(seed)\n        sample_documents = rng.sample(self.documents, sample_size)\n        split_name = f\"{self.split_name}[sample({sample_size},seed={seed})]\"\n        return self.from_documents(split_name, sample_documents)\n\n    def __iter__(self) -> Iterator[Document]:\n        \"\"\"Iterate over documents in the dataset, temporarily turning on memory caching.\"\"\"\n        for document in self.documents:\n            with document:\n                yield document\n\n    def __len__(self) -> int:\n        return len(self.documents)\n\n    def total_page_count(self) -> int:\n        return sum(doc.page_count for doc in self.documents)\n\n    def __str__(self) -> str:\n        return f\"Dataset({self.name})\"\n\n    def __repr__(self) -> str:\n        return (\n            f\"Dataset(split_name={self.split_name!r}, \"\n            f\"dataset_path={self.data_paths.dataset_path.root_path!r})\"\n        )\n\n    @classmethod\n    def from_documents(\n        cls,\n        split_name: str,\n        documents: Sequence[Document],\n    ) -> \"Dataset\":\n        \"\"\"\n        Create a dataset directly from documents, rather than from docids.\n\n        This is useful when the documents were already loaded once, e.g., when creating a dataset\n        with just a sample of the current documents.\n        \"\"\"\n        if len(documents) == 0:\n            raise ValueError(\"Cannot create a dataset with no documents\")\n\n        data_paths = documents[0].data_paths\n        dataset = cls(\n            split_name=split_name,\n            dataset_path=data_paths,\n            # Do not load annotations and OCR since it might be already loaded once in `documents`.\n            load_annotations=False,\n            load_ocr=False,\n            cache_images=CachingConfig.OFF,\n            docids=[doc.docid for doc in documents],\n        )\n        dataset._set_documents(documents)\n        return dataset\n\n    def store_index(self) -> None:\n        \"\"\"Store dataset index to disk.\"\"\"\n        index_path = self.data_paths.dataset_index_path(self.split_name)\n        if index_path.exists():\n            raise RuntimeError(\n                f\"Index file for {self} already exists at path {index_path}. Delete it first if \"\n                \"you want to overwrite it.\"\n            )\n\n        index_path.write_text(json.dumps(self.docids, indent=2))\n        logger.info(f\"Stored index for {self} to file {index_path}\")\n\n    def _load_docids_from_index(self, split_name: str) -> Optional[Sequence[str]]:\n        \"\"\"\n        Load docids from the index file on disk.\n\n        Returns\n        -------\n        Docids loaded from the index file or None if the file does not exist.\n        \"\"\"\n        index_path = self.data_paths.dataset_index_path(split_name)\n        if index_path.exists():\n            return json.loads(index_path.read_bytes())\n        return None\n\n    def _set_documents(self, documents: Sequence[Document]) -> None:\n        \"\"\"Set dataset documents to the provided documents.\"\"\"\n        self.documents = documents\n        self.docid_to_index = {doc.docid: i for i, doc in enumerate(self.documents)}", ""]}
{"filename": "docile/dataset/table_grid.py", "chunked_list": ["import dataclasses\nfrom typing import Any, Mapping, Sequence, Tuple\n\nfrom docile.dataset.bbox import BBox\n\n\n@dataclasses.dataclass(frozen=True)\nclass TableGrid:\n    \"\"\"Class describing a structure of a single table.\n\n    Parameters\n    ----------\n    bbox: BBox\n        Bounding box of the table. Notice that this is not equal to the union of bboxes of all line\n        item fields of the page as the table can have some header/footer/gaps on sides etc.\n    rows_bbox_with_type: Sequence[Tuple[BBox, str]]\n        List of rows (in top-down order), each with a bbox (covering the whole table in the\n        horizontal direction) and a row type. Possible row types are: `data`, `header`,\n        `subsection-header`, `footer`, `gap`, `down-merge`, `up-merge` and `unknown`.\n    columns_bbox_with_type: Sequence[Tuple[BBox, str]]\n        List of columns (in left-right order), each with a bbox (covering the whole table in the\n        vertical direction) and a column type. Column type is either one of the\n        docile.dataset.LIR_FIELDTYPES or an empty string if the column does not contain data with\n        one of the recognized field types. Sometimes not all columns are annotated, see the flag\n        `missing_columns` below for details.\n    missing_columns: bool\n        Flag filled by annotators to indicate that the table contains more than 5 \"other\" columns.\n        I.e., columns that do not contain data with one of the recognized field types. In this\n        case, only 5 such columns are part of `columns_bbox_with_type`. Notice that this flag is\n        global for the whole document, so it might be true for only one of the pages.\n    missing_second_table_on_page: bool\n        Flag filled by annotators to indicate that a second table exists on the page which was not\n        annotated. Notice that this flag is global for the whole document, so it might be true for\n        only one of the pages.\n    table_border_type: str\n        Filled by annotators to indicate whether the table has borders or not. Possible values are\n        `no_borders`, `column_borders`, `row_borders`, `all_borders` and `unknown` (only for\n        synthetic data). Notice that this was annotated globally for the whole document and not for\n        the tables individually but in practice the table borders are the same on all pages.\n    table_structure: str\n        Filled by annotators to indicate whether the table has a basic or a more complicated\n        structure. Possible values are `normal`, `merged_cells` and `structured_rows`. Notice that\n        this was annotated globally for the whole document and not for the tables individually but\n        in practice the table structure is the same on all pages.\n    \"\"\"\n\n    bbox: BBox\n    rows_bbox_with_type: Sequence[Tuple[BBox, str]]\n    columns_bbox_with_type: Sequence[Tuple[BBox, str]]\n    missing_columns: bool\n    missing_second_table_on_page: bool\n    table_border_type: str\n    table_structure: str\n\n    @classmethod\n    def from_dict(cls, dct: Mapping[str, Any]) -> \"TableGrid\":\n        dct_copy = dict(dct)\n        table_bbox = BBox(*(dct_copy.pop(\"bbox\")))\n        rows = [\n            (\n                BBox(table_bbox.left, row[\"top\"], table_bbox.right, row[\"bottom\"]),\n                row[\"row_type\"],\n            )\n            for row in dct_copy.pop(\"rows\")\n        ]\n        columns = [\n            (\n                BBox(column[\"left\"], table_bbox.top, column[\"right\"], table_bbox.bottom),\n                column[\"column_type\"],\n            )\n            for column in dct_copy.pop(\"columns\")\n        ]\n        return cls(\n            bbox=table_bbox, rows_bbox_with_type=rows, columns_bbox_with_type=columns, **dct_copy\n        )", ""]}
{"filename": "docile/dataset/document.py", "chunked_list": ["from pathlib import Path\nfrom types import TracebackType\nfrom typing import Optional, Tuple, Type, Union\n\nfrom PIL import Image\n\nfrom docile.dataset.cached_object import CachingConfig\nfrom docile.dataset.document_annotation import DocumentAnnotation\nfrom docile.dataset.document_images import DocumentImages\nfrom docile.dataset.document_ocr import DocumentOCR", "from docile.dataset.document_images import DocumentImages\nfrom docile.dataset.document_ocr import DocumentOCR\nfrom docile.dataset.paths import DataPaths\nfrom docile.dataset.types import OptionalImageSize\n\n\nclass Document:\n    \"\"\"\n    Structure representing a single document, with or without annotations.\n\n    You can enter the document using the `with` statement to temporarily cache its annoations, ocr\n    and generated images in memory.\n    \"\"\"\n\n    def __init__(\n        self,\n        docid: str,\n        dataset_path: Union[Path, str, DataPaths],\n        load_annotations: bool = True,\n        load_ocr: bool = True,\n        cache_images: CachingConfig = CachingConfig.DISK,\n    ):\n        \"\"\"\n        Load document from the dataset path.\n\n        You can temporarily cache document resources in memory (even when they were not loaded\n        during initialization) by using it as a context manager:\n        ```\n        with Document(\"docid\", \"dataset_path\", cache_images=CachingConfig.OFF) as document:\n            for i in range(5):\n                # Image is only generated once\n                img = document.page_image(page=0)\n        ```\n\n        Parameters\n        ----------\n        docid\n            Id of the document.\n        dataset_path\n            Path to the root directory with the unzipped dataset or a path to the ZIP file with the\n            dataset.\n        load_annotations\n            If true, annotations are loaded to memory.\n        load_ocr\n            If true, ocr is loaded to memory.\n        cache_images\n            Whether to cache images generated from the pdf to disk and/or to memory.\n        \"\"\"\n        self.docid = docid\n        self.data_paths = DataPaths(dataset_path)\n\n        if self.data_paths.is_in_zip() and cache_images.disk_cache:\n            raise ValueError(\"Cannot use disk cache for images when reading dataset from ZIP file\")\n\n        cache_annotation = (\n            CachingConfig.DISK_AND_MEMORY if load_annotations else CachingConfig.DISK\n        )\n        annotation_path = self.data_paths.annotation_path(docid)\n        self.annotation = DocumentAnnotation(annotation_path, cache=cache_annotation)\n\n        cache_ocr = CachingConfig.DISK_AND_MEMORY if load_ocr else CachingConfig.DISK\n        ocr_path = self.data_paths.ocr_path(docid)\n        pdf_path = self.data_paths.pdf_path(docid)\n        self.ocr = DocumentOCR(ocr_path, pdf_path, cache=cache_ocr)\n\n        self.load(load_annotations, load_ocr)\n\n        self.images = {}\n        self.cache_images = cache_images\n\n        # Page count is always cached, even when otherwise caching is turned off.\n        self._page_count: Optional[int] = None\n\n        self._open = 0\n\n    def load(self, annotations: bool = True, ocr: bool = True) -> \"Document\":\n        \"\"\"Load the annotations and/or OCR content to memory.\"\"\"\n        if annotations:\n            self.annotation.load()\n        if ocr:\n            self.ocr.load()\n        return self\n\n    def release(self, annotations: bool = True, ocr: bool = True) -> \"Document\":\n        \"\"\"Free up document resources from memory.\"\"\"\n        if annotations:\n            self.annotation.release()\n        if ocr:\n            self.ocr.release()\n        return self\n\n    @property\n    def page_count(self) -> int:\n        if self._page_count is None:\n            self._page_count = self.annotation.page_count\n        return self._page_count\n\n    def page_image(self, page: int, image_size: OptionalImageSize = (None, None)) -> Image.Image:\n        \"\"\"\n        Get image of the requested page.\n\n        The image size with default parameters is equal to `self.page_image_size(page)`. It is an\n        image rendered from the PDF at 200 DPI. To render images at lower DPI, you can use:\n        ```\n        image_size = document.page_image_size(page, dpi=72)\n        image = document.page_image(page, image_size)\n        ```\n\n        Parameters\n        ----------\n        page\n            Number of the page (from 0 to page_count - 1)\n        image_size\n            Size of the requested image as (width, height) tuple. If both dimensions are given,\n            aspect ratio is not preserved. If one dimension is None, aspect ratio is preserved with\n            the second dimension determining the image size. If both dimensions are None (default),\n            aspect ratio is preserved and the image is rendered at 200 DPI. The parameter can be\n            also a single integer in which case the result is a square image. Check\n            https://pdf2image.readthedocs.io/en/latest/reference.html for more details.\n        \"\"\"\n        if image_size not in self.images:\n            self.images[image_size] = DocumentImages(\n                path=self.data_paths.cache_images_path(self.docid, image_size),\n                pdf_path=self.data_paths.pdf_path(self.docid),\n                page_count=self.page_count,\n                size=image_size,\n                cache=self.cache_images,\n            )\n            if self._open:\n                self.images[image_size].__enter__()\n\n        return self.images[image_size].content[page]\n\n    def page_image_size(self, page: int, dpi: int = 200) -> Tuple[int, int]:\n        \"\"\"\n        Get (width, height) of the page when rendered with `self.page_image(page)` at `dpi`.\n\n        In a very few cases in the unlabeled set, the rendering fails (due to the pdf pages being\n        too big) and the rendered image has size (1,1). You can skip these documents or convert the\n        pdfs to images in a different way.\n        \"\"\"\n        width_200dpi, height_200dpi = self.annotation.page_image_size_at_200dpi(page)\n        image_size = (\n            max(1, round(dpi / 200 * width_200dpi)),\n            max(1, round(dpi / 200 * height_200dpi)),\n        )\n        return image_size\n\n    def __enter__(self) -> \"Document\":\n        self._open += 1\n        if self._open == 1:\n            for ctx in (self.ocr, self.annotation, *self.images.values()):\n                ctx.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        traceback: Optional[TracebackType],\n    ) -> None:\n        self._open -= 1\n        if self._open == 0:\n            for ctx in (self.ocr, self.annotation, *self.images.values()):\n                ctx.__exit__(exc_type, exc, traceback)\n\n    def __str__(self) -> str:\n        return f\"Document({self.data_paths.name}:{self.docid})\"\n\n    def __repr__(self) -> str:\n        return (\n            f\"Document(docid={self.docid!r}, \"\n            f\"dataset_path={self.data_paths.dataset_path.root_path!r})\"\n        )", ""]}
{"filename": "docile/dataset/__init__.py", "chunked_list": ["from docile.dataset.bbox import BBox\nfrom docile.dataset.cached_object import CachingConfig\nfrom docile.dataset.dataset import Dataset\nfrom docile.dataset.document import Document\nfrom docile.dataset.field import Field, load_predictions, store_predictions\nfrom docile.dataset.table_grid import TableGrid\n\nKILE_FIELDTYPES = [\n    \"account_num\",\n    \"amount_due\",", "    \"account_num\",\n    \"amount_due\",\n    \"amount_paid\",\n    \"amount_total_gross\",\n    \"amount_total_net\",\n    \"amount_total_tax\",\n    \"bank_num\",\n    \"bic\",\n    \"currency_code_amount_due\",\n    \"customer_billing_address\",", "    \"currency_code_amount_due\",\n    \"customer_billing_address\",\n    \"customer_billing_name\",\n    \"customer_delivery_address\",\n    \"customer_delivery_name\",\n    \"customer_id\",\n    \"customer_order_id\",\n    \"customer_other_address\",\n    \"customer_other_name\",\n    \"customer_registration_id\",", "    \"customer_other_name\",\n    \"customer_registration_id\",\n    \"customer_tax_id\",\n    \"date_due\",\n    \"date_issue\",\n    \"document_id\",\n    \"iban\",\n    \"order_id\",\n    \"payment_reference\",\n    \"payment_terms\",", "    \"payment_reference\",\n    \"payment_terms\",\n    \"tax_detail_gross\",\n    \"tax_detail_net\",\n    \"tax_detail_rate\",\n    \"tax_detail_tax\",\n    \"vendor_address\",\n    \"vendor_email\",\n    \"vendor_name\",\n    \"vendor_order_id\",", "    \"vendor_name\",\n    \"vendor_order_id\",\n    \"vendor_registration_id\",\n    \"vendor_tax_id\",\n]\n\nLIR_FIELDTYPES = [\n    \"line_item_amount_gross\",\n    \"line_item_amount_net\",\n    \"line_item_code\",", "    \"line_item_amount_net\",\n    \"line_item_code\",\n    \"line_item_currency\",\n    \"line_item_date\",\n    \"line_item_description\",\n    \"line_item_discount_amount\",\n    \"line_item_discount_rate\",\n    \"line_item_hts_number\",\n    \"line_item_order_id\",\n    \"line_item_person_name\",", "    \"line_item_order_id\",\n    \"line_item_person_name\",\n    \"line_item_position\",\n    \"line_item_quantity\",\n    \"line_item_tax\",\n    \"line_item_tax_rate\",\n    \"line_item_unit_price_gross\",\n    \"line_item_unit_price_net\",\n    \"line_item_units_of_measure\",\n    \"line_item_weight\",", "    \"line_item_units_of_measure\",\n    \"line_item_weight\",\n]\n\n__all__ = [\n    \"BBox\",\n    \"CachingConfig\",\n    \"Dataset\",\n    \"Document\",\n    \"Field\",", "    \"Document\",\n    \"Field\",\n    \"KILE_FIELDTYPES\",\n    \"LIR_FIELDTYPES\",\n    \"TableGrid\",\n    \"load_predictions\",\n    \"store_predictions\",\n]\n", ""]}
{"filename": "docile/dataset/paths.py", "chunked_list": ["from pathlib import Path, PurePosixPath\nfrom typing import Optional, Union\nfrom zipfile import ZipFile\n\nfrom docile.dataset.types import OptionalImageSize\n\n\nclass PathMaybeInZip:\n    \"\"\"Path that can point to the file system or to a path inside of a ZIP file.\"\"\"\n\n    def __init__(\n        self,\n        root_path: Path,\n        relative_path: Union[PurePosixPath, str] = \"\",\n        open_zip_file: Optional[ZipFile] = None,\n    ):\n        self.root_path = root_path\n        self.relative_path = PurePosixPath(relative_path)\n\n        if self.is_in_zip() and open_zip_file is None:\n            self._zip_file: Optional[ZipFile] = ZipFile(self.root_path, \"r\")\n            self._zip_file_owner = True\n        else:\n            self._zip_file = open_zip_file\n            self._zip_file_owner = False\n\n    def __del__(self) -> None:\n        if self._zip_file_owner:\n            self.zip_file.close()\n\n    @property\n    def zip_file(self) -> ZipFile:\n        assert self._zip_file is not None\n        return self._zip_file\n\n    def exists(self) -> bool:\n        if self.is_in_zip():\n            return str(self.relative_path) in self.zip_file.namelist()\n        return self.full_path.exists()\n\n    def read_bytes(self) -> bytes:\n        if self.is_in_zip():\n            return self.zip_file.read(str(self.relative_path))\n        return self.full_path.read_bytes()\n\n    def write_text(self, text: str) -> int:\n        if self.is_in_zip():\n            raise RuntimeError(\n                f\"Trying to write to ZIP file {self.root_path} (path {self.relative_path}) which \"\n                \"is not allowed, you need to unzip the dataset first.\",\n            )\n        return self.full_path.write_text(text)\n\n    def with_suffix(self, suffix: str) -> \"PathMaybeInZip\":\n        return PathMaybeInZip(\n            self.root_path, self.relative_path.with_suffix(suffix), self._zip_file\n        )\n\n    def __truediv__(self, key: str) -> \"PathMaybeInZip\":\n        return PathMaybeInZip(self.root_path, self.relative_path / key, self._zip_file)\n\n    def is_in_zip(self) -> bool:\n        return self.root_path.suffix == \".zip\"\n\n    @property\n    def full_path(self) -> Path:\n        assert not self.is_in_zip(), \"Path object can only be created if path is not in ZIP\"\n        return self.root_path / self.relative_path\n\n    def __str__(self) -> str:\n        if self.is_in_zip():\n            if self.relative_path == PurePosixPath():\n                return str(self.root_path)\n            return f\"{self.root_path!s}[{self.relative_path!s}]\"\n        return str(self.full_path)\n\n    def __repr__(self) -> str:\n        return (\n            f\"PathMaybeInZip(root_path={self.root_path!r}, relative_path={self.relative_path!r})\"\n        )", "\n\nclass DataPaths:\n    def __init__(self, dataset_path: Union[Path, str, \"DataPaths\"]):\n        if not isinstance(dataset_path, DataPaths):\n            self.dataset_path = PathMaybeInZip(Path(dataset_path))\n        else:\n            self.dataset_path = dataset_path.dataset_path\n\n    @property\n    def name(self) -> str:\n        return self.dataset_path.root_path.name\n\n    def is_in_zip(self) -> bool:\n        return self.dataset_path.is_in_zip()\n\n    def dataset_index_path(self, split_name: str) -> PathMaybeInZip:\n        return (self.dataset_path / split_name).with_suffix(\".json\")\n\n    def pdf_path(self, docid: str) -> PathMaybeInZip:\n        return self.dataset_path / \"pdfs\" / f\"{docid}.pdf\"\n\n    def ocr_path(self, docid: str) -> PathMaybeInZip:\n        return self.dataset_path / \"ocr\" / f\"{docid}.json\"\n\n    def annotation_path(self, docid: str) -> PathMaybeInZip:\n        return self.dataset_path / \"annotations\" / f\"{docid}.json\"\n\n    def cache_images_path(self, docid: str, size: OptionalImageSize) -> PathMaybeInZip:\n        \"\"\"Path to directory with cached images for the individual pages.\"\"\"\n        directory_name = docid\n        size_tag = self._size_tag(size)\n        if size_tag != \"\":\n            directory_name += f\"__{self._size_tag(size)}\"\n        return self.dataset_path / \"cached_images\" / directory_name\n\n    @staticmethod\n    def cache_page_image_path(cache_images_path: PathMaybeInZip, page_i: int) -> PathMaybeInZip:\n        return cache_images_path / f\"{page_i}.png\"\n\n    @staticmethod\n    def _size_tag(size: OptionalImageSize) -> str:\n        \"\"\"Convert size param to string. This string is used as part of the cache path for images.\"\"\"\n        if size == (None, None):\n            return \"\"\n        if isinstance(size, int):\n            return str(size)\n        return f\"{size[0]}x{size[1]}\"", ""]}
{"filename": "docile/dataset/cached_object.py", "chunked_list": ["from enum import Enum, auto\nfrom types import TracebackType\nfrom typing import Generic, Optional, Type, TypeVar\n\nfrom docile.dataset.paths import PathMaybeInZip\n\nCT = TypeVar(\"CT\")\n\n\nclass CachingConfig(Enum):\n    OFF = auto()\n    DISK = auto()\n    MEMORY = auto()\n    DISK_AND_MEMORY = auto()\n\n    @property\n    def disk_cache(self) -> bool:\n        return self in [self.DISK, self.DISK_AND_MEMORY]\n\n    @property\n    def memory_cache(self) -> bool:\n        return self in [self.MEMORY, self.DISK_AND_MEMORY]", "\nclass CachingConfig(Enum):\n    OFF = auto()\n    DISK = auto()\n    MEMORY = auto()\n    DISK_AND_MEMORY = auto()\n\n    @property\n    def disk_cache(self) -> bool:\n        return self in [self.DISK, self.DISK_AND_MEMORY]\n\n    @property\n    def memory_cache(self) -> bool:\n        return self in [self.MEMORY, self.DISK_AND_MEMORY]", "\n\nclass CachedObject(Generic[CT]):\n    \"\"\"\n    Base class for objects that can be cached to disk and memory.\n\n    To use disk caching, you have to implement `from_disk` and `to_disk` method. You can also\n    implement `predict` which will be used if the object does not exist in cache (resp. on disk).\n\n    You can temporarily turn on memory caching by entering the object as a context manager.\n    \"\"\"\n\n    def __init__(self, path: PathMaybeInZip, cache: CachingConfig):\n        # initialize in-memory cache\n        self._content: Optional[CT] = None\n\n        self.path = path\n        self.memory_cache_permanent = cache.memory_cache\n        self.memory_cache = cache.memory_cache\n        self.disk_cache = cache.disk_cache\n\n    def load(self) -> None:\n        self.memory_cache_permanent = True\n        self.memory_cache = True\n        if self._content is None:\n            self.content\n\n    def release(self) -> None:\n        self.memory_cache_permanent = False\n        self.memory_cache = False\n        self._content = None\n\n    def __enter__(self) -> \"CachedObject\":\n        self.memory_cache = True\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc: Optional[BaseException],\n        traceback: Optional[TracebackType],\n    ) -> None:\n        if not self.memory_cache_permanent:\n            self.memory_cache = False\n            self._content = None\n\n    def from_disk(self) -> CT:\n        raise NotImplementedError\n\n    def to_disk(self, content: CT) -> None:  # noqa: U100\n        raise NotImplementedError\n\n    def predict(self) -> CT:\n        raise NotImplementedError\n\n    def overwrite(self, content: CT) -> None:\n        if self.memory_cache:\n            self._content = content\n        if self.disk_cache:\n            if self.path.is_in_zip():\n                raise RuntimeError(f\"Cannot write to disk cache since path {self.path} is in ZIP\")\n            self.to_disk(content)\n\n    def predict_and_overwrite(self) -> CT:\n        content = self.predict()\n        self.overwrite(content)\n        return content\n\n    @property\n    def content(self) -> CT:\n        \"\"\"Try to load the content from cache.\"\"\"\n        if self.memory_cache and self._content is not None:\n            return self._content\n        if self.disk_cache and self.path.exists():\n            content = self.from_disk()\n            if self.memory_cache:\n                self._content = content\n            return content\n\n        # Object not found in cache, we need to predict it.\n        try:\n            return self.predict_and_overwrite()\n        except NotImplementedError:\n            raise ValueError(\n                f\"Object {self.path} not found in memory, on disk and cannot be created\"\n            )", ""]}
{"filename": "docile/dataset/document_annotation.py", "chunked_list": ["import json\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom docile.dataset.cached_object import CachedObject, CachingConfig\nfrom docile.dataset.field import Field\nfrom docile.dataset.paths import PathMaybeInZip\nfrom docile.dataset.table_grid import TableGrid\n\n\nclass DocumentAnnotation(CachedObject[Dict]):\n    \"\"\"\n    All annotations available for the document.\n\n    Unlabeled documents still have some annotations available, namely: page_count, cluster_id,\n    page_image_size_at_200dpi, source, original_filename. Notice that pre-computed OCR, which is\n    also available for unlabeled documents, is provided separately through `document.ocr`\n    (DocumentOCR class).\n\n    This is also true for the test set with the exception of cluster_id which is not shared.\n\n    Otherwise, all annotations are available for both annotated and synthetic documents, with the\n    exception of `template_document_id` which is only defined for the synthetic documents.\n\n    For synthetic documents, the values for document_type, source and original_filename are taken\n    from the template document.\n    \"\"\"\n\n    def __init__(self, path: PathMaybeInZip, cache: CachingConfig = CachingConfig.DISK) -> None:\n        super().__init__(path=path, cache=cache)\n\n    def from_disk(self) -> Dict[str, Any]:\n        return json.loads(self.path.read_bytes())\n\n    @property\n    def page_count(self) -> int:\n        return self.content[\"metadata\"][\"page_count\"]\n\n    @property\n    def fields(self) -> List[Field]:\n        \"\"\"All KILE fields on the document.\"\"\"\n        return [Field.from_dict(a) for a in self.content[\"field_extractions\"]]\n\n    def page_fields(self, page: int) -> List[Field]:\n        \"\"\"KILE fields on the given page of the document.\"\"\"\n        return [f for f in self.fields if f.page == page]\n\n    @property\n    def li_fields(self) -> List[Field]:\n        \"\"\"All LI fields on the document.\"\"\"\n        return [Field.from_dict(a) for a in self.content[\"line_item_extractions\"]]\n\n    def page_li_fields(self, page: int) -> List[Field]:\n        \"\"\"LI fields on the given page of the document.\"\"\"\n        return [f for f in self.li_fields if f.page == page]\n\n    @property\n    def li_headers(self) -> List[Field]:\n        \"\"\"\n        Fields corresponding to column headers in tables.\n\n        Predicting these fields is not part of the LIR task.\n        \"\"\"\n        return [Field.from_dict(a) for a in self.content[\"line_item_headers\"]]\n\n    def page_li_headers(self, page: int) -> List[Field]:\n        \"\"\"Fields corresponding to column headers in tables on the given page.\"\"\"\n        return [f for f in self.li_headers if f.page == page]\n\n    @property\n    def cluster_id(self) -> int:\n        \"\"\"\n        Id of the cluster the document belongs to.\n\n        Cluster represents a group of documents with the same layout.\n        \"\"\"\n        return self.content[\"metadata\"][\"cluster_id\"]\n\n    def page_image_size_at_200dpi(self, page: int) -> Tuple[int, int]:\n        \"\"\"\n        Page image size at 200 DPI.\n\n        This is exactly equal to the generated image size when using `document.page_image(page)`\n        with the default parameters.\n        \"\"\"\n        return self.content[\"metadata\"][\"page_sizes_at_200dpi\"][page]\n\n    @property\n    def document_type(self) -> str:\n        return self.content[\"metadata\"][\"document_type\"]\n\n    @property\n    def currency(self) -> str:\n        \"\"\"Document currency or 'other' if no currency is specified on the document.\"\"\"\n        return self.content[\"metadata\"][\"currency\"]\n\n    @property\n    def language(self) -> str:\n        return self.content[\"metadata\"][\"language\"]\n\n    def get_table_grid(self, page: int) -> Optional[TableGrid]:\n        \"\"\"\n        Get table structure on a given page.\n\n        While Line Items do not necessarily follow a table structure, most documents also contain\n        annotation of the table structure -- bounding boxes of the table, rows (with row types) and\n        columns (with column types, corresponding to line item fieldtypes). See TableGrid class for\n        more info.\n\n        Each page has at most one table. In some rare cases the table annotation might be missing\n        even though the page has some line items annotated.\n\n        Some documents have a second table present, for which the table grid is not available and\n        from which line items were not extracted. Info about this is present in\n        `table_grid.missing_second_table_on_page` attribute.\n        \"\"\"\n        page_str = str(page)\n        table_grid_dict = self.content[\"metadata\"][\"page_to_table_grid\"].get(page_str, None)\n        if table_grid_dict is None:\n            return None\n        return TableGrid.from_dict(table_grid_dict)\n\n    @property\n    def source(self) -> str:\n        \"\"\"Source of the document, either 'ucsf' or 'pif'.\"\"\"\n        return self.content[\"metadata\"][\"source\"]\n\n    @property\n    def original_filename(self) -> str:\n        \"\"\"\n        Id/filename of the document in the original source.\n\n        Several documents can have the same original_filename if the original pdf was composed of\n        several self-contained documents.\n        \"\"\"\n        return self.content[\"metadata\"][\"original_filename\"]\n\n    @property\n    def template_document_id(self) -> str:\n        \"\"\"\n        Id of the annotated document that was used as a template for the document generation.\n\n        Only available for synthetic documents.\n        \"\"\"\n        return self.content[\"metadata\"][\"template_document_id\"]", "\nclass DocumentAnnotation(CachedObject[Dict]):\n    \"\"\"\n    All annotations available for the document.\n\n    Unlabeled documents still have some annotations available, namely: page_count, cluster_id,\n    page_image_size_at_200dpi, source, original_filename. Notice that pre-computed OCR, which is\n    also available for unlabeled documents, is provided separately through `document.ocr`\n    (DocumentOCR class).\n\n    This is also true for the test set with the exception of cluster_id which is not shared.\n\n    Otherwise, all annotations are available for both annotated and synthetic documents, with the\n    exception of `template_document_id` which is only defined for the synthetic documents.\n\n    For synthetic documents, the values for document_type, source and original_filename are taken\n    from the template document.\n    \"\"\"\n\n    def __init__(self, path: PathMaybeInZip, cache: CachingConfig = CachingConfig.DISK) -> None:\n        super().__init__(path=path, cache=cache)\n\n    def from_disk(self) -> Dict[str, Any]:\n        return json.loads(self.path.read_bytes())\n\n    @property\n    def page_count(self) -> int:\n        return self.content[\"metadata\"][\"page_count\"]\n\n    @property\n    def fields(self) -> List[Field]:\n        \"\"\"All KILE fields on the document.\"\"\"\n        return [Field.from_dict(a) for a in self.content[\"field_extractions\"]]\n\n    def page_fields(self, page: int) -> List[Field]:\n        \"\"\"KILE fields on the given page of the document.\"\"\"\n        return [f for f in self.fields if f.page == page]\n\n    @property\n    def li_fields(self) -> List[Field]:\n        \"\"\"All LI fields on the document.\"\"\"\n        return [Field.from_dict(a) for a in self.content[\"line_item_extractions\"]]\n\n    def page_li_fields(self, page: int) -> List[Field]:\n        \"\"\"LI fields on the given page of the document.\"\"\"\n        return [f for f in self.li_fields if f.page == page]\n\n    @property\n    def li_headers(self) -> List[Field]:\n        \"\"\"\n        Fields corresponding to column headers in tables.\n\n        Predicting these fields is not part of the LIR task.\n        \"\"\"\n        return [Field.from_dict(a) for a in self.content[\"line_item_headers\"]]\n\n    def page_li_headers(self, page: int) -> List[Field]:\n        \"\"\"Fields corresponding to column headers in tables on the given page.\"\"\"\n        return [f for f in self.li_headers if f.page == page]\n\n    @property\n    def cluster_id(self) -> int:\n        \"\"\"\n        Id of the cluster the document belongs to.\n\n        Cluster represents a group of documents with the same layout.\n        \"\"\"\n        return self.content[\"metadata\"][\"cluster_id\"]\n\n    def page_image_size_at_200dpi(self, page: int) -> Tuple[int, int]:\n        \"\"\"\n        Page image size at 200 DPI.\n\n        This is exactly equal to the generated image size when using `document.page_image(page)`\n        with the default parameters.\n        \"\"\"\n        return self.content[\"metadata\"][\"page_sizes_at_200dpi\"][page]\n\n    @property\n    def document_type(self) -> str:\n        return self.content[\"metadata\"][\"document_type\"]\n\n    @property\n    def currency(self) -> str:\n        \"\"\"Document currency or 'other' if no currency is specified on the document.\"\"\"\n        return self.content[\"metadata\"][\"currency\"]\n\n    @property\n    def language(self) -> str:\n        return self.content[\"metadata\"][\"language\"]\n\n    def get_table_grid(self, page: int) -> Optional[TableGrid]:\n        \"\"\"\n        Get table structure on a given page.\n\n        While Line Items do not necessarily follow a table structure, most documents also contain\n        annotation of the table structure -- bounding boxes of the table, rows (with row types) and\n        columns (with column types, corresponding to line item fieldtypes). See TableGrid class for\n        more info.\n\n        Each page has at most one table. In some rare cases the table annotation might be missing\n        even though the page has some line items annotated.\n\n        Some documents have a second table present, for which the table grid is not available and\n        from which line items were not extracted. Info about this is present in\n        `table_grid.missing_second_table_on_page` attribute.\n        \"\"\"\n        page_str = str(page)\n        table_grid_dict = self.content[\"metadata\"][\"page_to_table_grid\"].get(page_str, None)\n        if table_grid_dict is None:\n            return None\n        return TableGrid.from_dict(table_grid_dict)\n\n    @property\n    def source(self) -> str:\n        \"\"\"Source of the document, either 'ucsf' or 'pif'.\"\"\"\n        return self.content[\"metadata\"][\"source\"]\n\n    @property\n    def original_filename(self) -> str:\n        \"\"\"\n        Id/filename of the document in the original source.\n\n        Several documents can have the same original_filename if the original pdf was composed of\n        several self-contained documents.\n        \"\"\"\n        return self.content[\"metadata\"][\"original_filename\"]\n\n    @property\n    def template_document_id(self) -> str:\n        \"\"\"\n        Id of the annotated document that was used as a template for the document generation.\n\n        Only available for synthetic documents.\n        \"\"\"\n        return self.content[\"metadata\"][\"template_document_id\"]", ""]}
{"filename": "docile/dataset/document_ocr.py", "chunked_list": ["import copy\nimport json\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional\n\nimport numpy as np\nfrom PIL import Image, ImageOps\n\nfrom docile.dataset.bbox import BBox\nfrom docile.dataset.cached_object import CachedObject, CachingConfig", "from docile.dataset.bbox import BBox\nfrom docile.dataset.cached_object import CachedObject, CachingConfig\nfrom docile.dataset.field import Field\nfrom docile.dataset.paths import PathMaybeInZip\n\nlogger = logging.getLogger(__name__)\n\n\nclass DocumentOCR(CachedObject[Dict]):\n    _model = None\n\n    def __init__(\n        self,\n        path: PathMaybeInZip,\n        pdf_path: PathMaybeInZip,\n        cache: CachingConfig = CachingConfig.DISK,\n    ) -> None:\n        super().__init__(path=path, cache=cache)\n        self.pdf_path = pdf_path\n\n    def from_disk(self) -> Dict:\n        return json.loads(self.path.read_bytes())\n\n    def to_disk(self, content: Any) -> None:\n        self.path.full_path.parent.mkdir(parents=True, exist_ok=True)\n        self.path.full_path.write_text(json.dumps(content))\n\n    def predict(self) -> Dict:\n        \"\"\"Predict the OCR.\"\"\"\n        # Load dependencies inside so that they are not needed when the pre-computed OCR is used.\n        from doctr.io import DocumentFile\n\n        pdf_doc = DocumentFile.from_pdf(self.pdf_path.read_bytes())\n\n        ocr_pred = self.get_model()(pdf_doc)\n        return ocr_pred.export()\n\n    def get_all_words(\n        self,\n        page: int,\n        snapped: bool = False,\n        use_cached_snapping: bool = True,\n        get_page_image: Optional[Callable[[], Image.Image]] = None,\n    ) -> List[Field]:\n        \"\"\"\n        Get all OCR words on a given page.\n\n        There are two possible settings, `snapped=False` returns the original word boxes as\n        returned by the OCR predictions, while `snapped=True` performs additional heuristics to\n        make the bounding boxes tight around text. This is used in evaluation to make sure correct\n        predictions are not penalized if they differ from ground-truth only by the amount of\n        background on any side of the bounding box.\n\n        Parameters\n        ----------\n        page\n            Page number (0-based) for which to get all OCR words.\n        snapped\n            If False, use original detections. If True, use bounding boxes snapped to the text.\n        use_cached_snapping\n            Only used if `snapped=True`. If True, the OCR cache (including the files on disk) is\n            used to load/store the snapped bounding boxes.\n        get_page_image\n            Only used if `snapped=True`. Not needed if `use_cached_snapping=True` and the snapped\n            bounding boxes are pre-computed. This should be a function that returns the image of\n            the page. It is needed to perform the snapping. Tip: make sure the image is stored in\n            memory. E.g., use `lambda page_img=page_img: page_img` or open the document first\n            (`with document:`) to turn on memory caching and then use\n            `functools.partial(document.page_image, page)`.\n        \"\"\"\n        # When both snapped and use_cached_snapping options are used but the OCR predictions do not\n        # yet contain the snapped geometry, the dictionary with OCR predictions is extended with\n        # the snapped geometry (in `_get_bbox_from_ocr_word`).\n        load_or_store_snapped_bboxes = snapped and use_cached_snapping\n\n        # Make a copy of the OCR dictionary in case it is extended with the snapped geometry below.\n        ocr_dict_original = self.content\n        ocr_dict = (\n            copy.deepcopy(ocr_dict_original) if load_or_store_snapped_bboxes else ocr_dict_original\n        )\n\n        words = []\n\n        ocr_page = ocr_dict[\"pages\"][page]\n        for block in ocr_page[\"blocks\"]:\n            for line in block[\"lines\"]:\n                for word in line[\"words\"]:\n                    if float(word[\"confidence\"])>=0.7:\n                        bbox = DocumentOCR._get_bbox_from_ocr_word(\n                            word, snapped, use_cached_snapping, get_page_image\n                        )\n                        words.append(Field(text=word[\"value\"], bbox=bbox, page=page))\n\n        # If cached snapping is used and the snapping was not pre-computed, store it in the file.\n        if load_or_store_snapped_bboxes and ocr_dict != ocr_dict_original:\n            self.overwrite(ocr_dict)\n\n        return words\n\n    @classmethod\n    def get_model(cls) -> Callable:\n        if cls._model:\n            return cls._model\n\n        import torch\n        from doctr.models import ocr_predictor\n\n        logger.info(\"Initializing OCR predictor model.\")\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(\"DocTR using device:\", device)\n        cls._model = ocr_predictor(pretrained=True).to(device=device)\n\n        return cls._model\n\n    @staticmethod\n    def _get_bbox_from_ocr_word(\n        word: Dict[str, Any],\n        snapped: bool,\n        use_cached_snapping: bool,\n        get_page_image: Optional[Callable[[], Image.Image]] = None,\n    ) -> BBox:\n        \"\"\"Get BBox for an OCR word and perform snapping if required.\"\"\"\n        bbox = DocumentOCR._get_bbox_from_ocr_geometry(word[\"geometry\"])\n        if not snapped:\n            return bbox\n        if use_cached_snapping and \"snapped_geometry\" in word:\n            return DocumentOCR._get_bbox_from_ocr_geometry(word[\"snapped_geometry\"])\n\n        if get_page_image is None:\n            raise ValueError(\n                \"Function to get page image not provided but it is needed to perform snapping\"\n            )\n        snapped_bbox = _snap_bbox_to_text(bbox, get_page_image())\n\n        if use_cached_snapping:\n            word[\"snapped_geometry\"] = DocumentOCR._get_ocr_geometry_from_bbox(snapped_bbox)\n\n        return snapped_bbox\n\n    @staticmethod\n    def _get_bbox_from_ocr_geometry(geometry: List[List[float]]) -> BBox:\n        \"\"\"Convert the OCR geometry to the BBox structure.\"\"\"\n        left_top, right_bottom = geometry\n        return BBox(\n            left=left_top[0], top=left_top[1], right=right_bottom[0], bottom=right_bottom[1]\n        )\n\n    @staticmethod\n    def _get_ocr_geometry_from_bbox(bbox: BBox) -> List[List[float]]:\n        \"\"\"Convert the bounding box into OCR geometry format.\"\"\"\n        return [[bbox.left, bbox.top], [bbox.right, bbox.bottom]]", "class DocumentOCR(CachedObject[Dict]):\n    _model = None\n\n    def __init__(\n        self,\n        path: PathMaybeInZip,\n        pdf_path: PathMaybeInZip,\n        cache: CachingConfig = CachingConfig.DISK,\n    ) -> None:\n        super().__init__(path=path, cache=cache)\n        self.pdf_path = pdf_path\n\n    def from_disk(self) -> Dict:\n        return json.loads(self.path.read_bytes())\n\n    def to_disk(self, content: Any) -> None:\n        self.path.full_path.parent.mkdir(parents=True, exist_ok=True)\n        self.path.full_path.write_text(json.dumps(content))\n\n    def predict(self) -> Dict:\n        \"\"\"Predict the OCR.\"\"\"\n        # Load dependencies inside so that they are not needed when the pre-computed OCR is used.\n        from doctr.io import DocumentFile\n\n        pdf_doc = DocumentFile.from_pdf(self.pdf_path.read_bytes())\n\n        ocr_pred = self.get_model()(pdf_doc)\n        return ocr_pred.export()\n\n    def get_all_words(\n        self,\n        page: int,\n        snapped: bool = False,\n        use_cached_snapping: bool = True,\n        get_page_image: Optional[Callable[[], Image.Image]] = None,\n    ) -> List[Field]:\n        \"\"\"\n        Get all OCR words on a given page.\n\n        There are two possible settings, `snapped=False` returns the original word boxes as\n        returned by the OCR predictions, while `snapped=True` performs additional heuristics to\n        make the bounding boxes tight around text. This is used in evaluation to make sure correct\n        predictions are not penalized if they differ from ground-truth only by the amount of\n        background on any side of the bounding box.\n\n        Parameters\n        ----------\n        page\n            Page number (0-based) for which to get all OCR words.\n        snapped\n            If False, use original detections. If True, use bounding boxes snapped to the text.\n        use_cached_snapping\n            Only used if `snapped=True`. If True, the OCR cache (including the files on disk) is\n            used to load/store the snapped bounding boxes.\n        get_page_image\n            Only used if `snapped=True`. Not needed if `use_cached_snapping=True` and the snapped\n            bounding boxes are pre-computed. This should be a function that returns the image of\n            the page. It is needed to perform the snapping. Tip: make sure the image is stored in\n            memory. E.g., use `lambda page_img=page_img: page_img` or open the document first\n            (`with document:`) to turn on memory caching and then use\n            `functools.partial(document.page_image, page)`.\n        \"\"\"\n        # When both snapped and use_cached_snapping options are used but the OCR predictions do not\n        # yet contain the snapped geometry, the dictionary with OCR predictions is extended with\n        # the snapped geometry (in `_get_bbox_from_ocr_word`).\n        load_or_store_snapped_bboxes = snapped and use_cached_snapping\n\n        # Make a copy of the OCR dictionary in case it is extended with the snapped geometry below.\n        ocr_dict_original = self.content\n        ocr_dict = (\n            copy.deepcopy(ocr_dict_original) if load_or_store_snapped_bboxes else ocr_dict_original\n        )\n\n        words = []\n\n        ocr_page = ocr_dict[\"pages\"][page]\n        for block in ocr_page[\"blocks\"]:\n            for line in block[\"lines\"]:\n                for word in line[\"words\"]:\n                    if float(word[\"confidence\"])>=0.7:\n                        bbox = DocumentOCR._get_bbox_from_ocr_word(\n                            word, snapped, use_cached_snapping, get_page_image\n                        )\n                        words.append(Field(text=word[\"value\"], bbox=bbox, page=page))\n\n        # If cached snapping is used and the snapping was not pre-computed, store it in the file.\n        if load_or_store_snapped_bboxes and ocr_dict != ocr_dict_original:\n            self.overwrite(ocr_dict)\n\n        return words\n\n    @classmethod\n    def get_model(cls) -> Callable:\n        if cls._model:\n            return cls._model\n\n        import torch\n        from doctr.models import ocr_predictor\n\n        logger.info(\"Initializing OCR predictor model.\")\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(\"DocTR using device:\", device)\n        cls._model = ocr_predictor(pretrained=True).to(device=device)\n\n        return cls._model\n\n    @staticmethod\n    def _get_bbox_from_ocr_word(\n        word: Dict[str, Any],\n        snapped: bool,\n        use_cached_snapping: bool,\n        get_page_image: Optional[Callable[[], Image.Image]] = None,\n    ) -> BBox:\n        \"\"\"Get BBox for an OCR word and perform snapping if required.\"\"\"\n        bbox = DocumentOCR._get_bbox_from_ocr_geometry(word[\"geometry\"])\n        if not snapped:\n            return bbox\n        if use_cached_snapping and \"snapped_geometry\" in word:\n            return DocumentOCR._get_bbox_from_ocr_geometry(word[\"snapped_geometry\"])\n\n        if get_page_image is None:\n            raise ValueError(\n                \"Function to get page image not provided but it is needed to perform snapping\"\n            )\n        snapped_bbox = _snap_bbox_to_text(bbox, get_page_image())\n\n        if use_cached_snapping:\n            word[\"snapped_geometry\"] = DocumentOCR._get_ocr_geometry_from_bbox(snapped_bbox)\n\n        return snapped_bbox\n\n    @staticmethod\n    def _get_bbox_from_ocr_geometry(geometry: List[List[float]]) -> BBox:\n        \"\"\"Convert the OCR geometry to the BBox structure.\"\"\"\n        left_top, right_bottom = geometry\n        return BBox(\n            left=left_top[0], top=left_top[1], right=right_bottom[0], bottom=right_bottom[1]\n        )\n\n    @staticmethod\n    def _get_ocr_geometry_from_bbox(bbox: BBox) -> List[List[float]]:\n        \"\"\"Convert the bounding box into OCR geometry format.\"\"\"\n        return [[bbox.left, bbox.top], [bbox.right, bbox.bottom]]", "\n\ndef _snap_bbox_to_text(bbox: BBox, page_image: Image.Image) -> BBox:\n    \"\"\"\n    Return a new bbox that is tight around the contained text.\n\n    This is done by binarizing the cropped part of the image and removing rows/columns from\n    top/bottom/left/right that are empty or probably do not contain the required text (this is done\n    by heuristics explained in detail in `_foreground_text_bbox` function).\n    \"\"\"\n    # Load dependencies inside so that they are not needed when the pre-computed OCR is used.\n    import cv2\n\n    scaled_bbox = bbox.to_absolute_coords(page_image.width, page_image.height)\n    bbox_image = page_image.crop(scaled_bbox.to_tuple())\n    bbox_image = ImageOps.grayscale(bbox_image)\n    bbox_image_array = np.array(bbox_image)\n    threshold, bbox_image_array = cv2.threshold(\n        src=bbox_image_array, thresh=0, maxval=255, type=cv2.THRESH_BINARY | cv2.THRESH_OTSU\n    )\n    # We assume the more frequent color corresponds to the background.\n    foreground_mask = bbox_image_array != np.median(bbox_image_array)\n\n    # Snap to foreground or use the original bounding box if snapping was unsuccessful.\n    snapped_bbox_crop = _foreground_text_bbox(foreground_mask)\n    if snapped_bbox_crop is None:\n        return bbox\n\n    snapped_bbox_page = BBox(\n        left=snapped_bbox_crop.left + scaled_bbox.left,\n        top=snapped_bbox_crop.top + scaled_bbox.top,\n        right=snapped_bbox_crop.right + scaled_bbox.left,\n        bottom=snapped_bbox_crop.bottom + scaled_bbox.top,\n    ).to_relative_coords(page_image.width, page_image.height)\n\n    return snapped_bbox_page", "\n\ndef _foreground_text_bbox(\n    foreground_mask: np.ndarray,\n    margin_size: int = 5,\n    min_char_width_margin: int = 6,\n    min_line_height_margin: int = 10,\n    min_char_width_inside: int = 2,\n    min_line_height_inside: int = 5,\n) -> Optional[BBox]:\n    \"\"\"\n    Locate text inside of an array representing which pixels are in the foreground.\n\n    The location is initialized as a bbox covering the whole array and then shranked in two phases:\n    * In the first phase, more aggressive shrinking is done around the margins. In this phase the\n      bbox is shranked by at most `margin_size` in each direction.\n    * In the second phase, shrinking can proceed without limits but is less aggressive.\n\n    If at any point the bbox would be shranked to an empty bbox, `None` is returned.\n\n    Shrinking is controlled by two parameters for each phase (`min_char_width_*` and\n    `min_line_height_*`). They define how many consecutive non-empty columns/rows are needed to\n    stop further shrinking.\n\n    Motivation:\n    The implementation and parameters are fine-tuned to the pre-computed OCR provided with the\n    DocILE dataset, which in practice is not tight on the text and often contains some visual\n    elements around the margins of the detected text box (such as table borders or small portions\n    of the surrounding text).\n\n    Failure scenarios:\n    The shrinking can be overly aggressive in the cases when very low or narrow character (such as\n    'i', 'l', ':', ',', '.') is very close to the margins or when punctuation can be separated from\n    the rest of the letters (such as in word 'mini'). This is not a problem for the purposes of\n    DocILE evaluation where the Pseudo-Character Centers will be shifted only marginally in these\n    cases.\n\n    Parameters\n    ----------\n    foreground_mask\n        Two dimensional numpy array containing booleans, representing which pixels are part of the\n        foreground.\n    margin_size\n        Shrink the bounding box aggressively in the margins of the bounding box.\n    min_char_width_margin\n        Number of consecutive non-empty (i.e., containing at least one foreground pixel) columns\n        needed to not shrink past the current column. This applies to the first and last\n        `margin_size` columns.\n    min_line_height_margin\n        Number of consecutive non-empty rows needed to not shrink past the current row. This\n        applies to the first and last `margin_size` rows.\n    min_char_width_inside\n        As `min_char_width_margin` but applies to columns that are further than `margin_size` from\n        the edges.\n    min_line_height_inside\n        As `min_line_height_margin` but applies to rows that are further than `margin_size` from\n        the edges.\n\n    Returns\n    -------\n    BBox around the text located in the `foreground_mask` or `None` if the localization is\n    unsuccessful.\n    \"\"\"\n\n    width = foreground_mask.shape[1]\n    height = foreground_mask.shape[0]\n    left: Optional[int] = 0\n    top: Optional[int] = 0\n    right: Optional[int] = width\n    bottom: Optional[int] = height\n\n    # Notice that the second phase is done twice as shrinking the bbox can mark further\n    # rows/columns as empty. This could be true even after the second iteration but in practice two\n    # iterations are enough. In the first phase one iteration is sufficient because the\n    # rows/columns in the margin are already ignored and the bbox cannot be shranked past them.\n    for stop_at, min_char_width, min_line_height in [\n        (margin_size, min_char_width_margin, min_line_height_margin),\n        (None, min_char_width_inside, min_line_height_inside),\n        (None, min_char_width_inside, min_line_height_inside),\n    ]:\n        # Find non-empty rows and columns\n        if stop_at is not None:\n            # In the first phase, when locating non-empty rows (resp. columns), consider columns\n            # (resp. rows) within the margin as background as margins often contain noise. This is\n            # not done in the second phase because if some side of the bbox did not move beyond the\n            # margin in the first phase, text (not noise) is probably located within this margin.\n            foreground_rows = foreground_mask[:, margin_size : (width - margin_size)].any(axis=1)\n            foreground_columns = foreground_mask[margin_size : (height - margin_size), :].any(\n                axis=0\n            )\n        else:\n            # In the secnod phase, consider everything outside of (left, top, right, bottom) as\n            # background (as if the image was already cropped).\n            foreground_rows = foreground_mask[:, left:right].any(axis=1)\n            foreground_rows[:top] = 0\n            foreground_rows[bottom:] = 0\n\n            foreground_columns = foreground_mask[top:bottom, :].any(axis=0)\n            foreground_columns[:left] = 0\n            foreground_columns[right:] = 0\n\n        # Finally, update the (left, top, right, bottom) bbox such that there are the prescribed\n        # number of non-empty consecutive rows/columns at each side of the bbox (but stop if\n        # `stop_at` rows/columns were already thrown away).\n        top = _find_nonzero_sequence(\n            sequence=foreground_rows,\n            stop_at=stop_at,\n            min_consecutive_nonzero=min_line_height,\n            from_start=True,\n        )\n        bottom = _find_nonzero_sequence(\n            sequence=foreground_rows,\n            stop_at=stop_at,\n            min_consecutive_nonzero=min_line_height,\n            from_start=False,\n        )\n        left = _find_nonzero_sequence(\n            sequence=foreground_columns,\n            stop_at=stop_at,\n            min_consecutive_nonzero=min_char_width,\n            from_start=True,\n        )\n        right = _find_nonzero_sequence(\n            sequence=foreground_columns,\n            stop_at=stop_at,\n            min_consecutive_nonzero=min_char_width,\n            from_start=False,\n        )\n        if any(coord is None for coord in [left, right, top, bottom]):\n            return None\n\n    return BBox(left=left, top=top, right=right, bottom=bottom)", "\n\ndef _find_nonzero_sequence(\n    sequence: np.ndarray, stop_at: Optional[int], min_consecutive_nonzero: int, from_start: bool\n) -> Optional[int]:\n    \"\"\"\n    Find the first (or last) subsequence of consecutive non-zero values.\n\n    Parameters\n    ----------\n    sequence\n        One dimensional sequence of values.\n    stop_at\n        Only search among the first (resp. last) `stop_at` positions.\n    min_consecutive_nonzero\n        Search for a subsequence of non-zero items of this length.\n    from_start\n        Whether to search from the start or end of the sequence.\n\n    Returns\n    -------\n    Returns the first index (resp. last if `from_start` is `False`) such that\n    `min_consecutive_nonzero` items starting at (resp. ending before) this position are all\n    nonzero. Return `stop_at` (resp. `len(sequence) - stop_at`) if the index was not among the\n    first (resp. last) `stop_at` items. Return `None` if the subsequence of prescribed length does\n    not exist and `stop_at` is `None`.\n    \"\"\"\n    if not from_start:\n        pos_from_right = _find_nonzero_sequence(\n            np.flip(sequence), stop_at, min_consecutive_nonzero, from_start=True\n        )\n        if pos_from_right is None:\n            return None\n        return len(sequence) - pos_from_right\n\n    for idx in sequence.nonzero()[0]:  # iterate over the nonzero indices\n        if stop_at is not None and idx > stop_at:\n            break\n        if (\n            idx + min_consecutive_nonzero <= len(sequence)\n            and sequence[idx : (idx + min_consecutive_nonzero)].all()\n        ):\n            return idx\n\n    # Return the maximum allowed value or `None` if `stop_at` is not set.\n    return stop_at", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/conftest.py", "chunked_list": ["from pathlib import Path\n\nimport pytest\n\nfrom docile.dataset import Dataset\n\n\n@pytest.fixture\ndef sample_dataset_docid() -> str:\n    return \"516f2d61ea404b30a9192a72\"", "def sample_dataset_docid() -> str:\n    return \"516f2d61ea404b30a9192a72\"\n\n\n@pytest.fixture\ndef sample_dataset_path() -> Path:\n    return Path(\"tests/data/sample-dataset\")\n\n\n@pytest.fixture\ndef sample_dataset(sample_dataset_path: Path) -> Dataset:\n    return Dataset(\"dev\", sample_dataset_path)", "\n@pytest.fixture\ndef sample_dataset(sample_dataset_path: Path) -> Dataset:\n    return Dataset(\"dev\", sample_dataset_path)\n"]}
{"filename": "tests/evaluation/test_average_precision.py", "chunked_list": ["import pytest\n\nfrom docile.evaluation.average_precision import compute_average_precision\n\n\ndef test_compute_average_precision() -> None:\n    # Recall 0.5 is achieved with perfect precision, recall 0.75 is achieved with precision 0.75\n    # and higher recall cannot be achieved.\n    predictions = [True, True, False, True, False, False]\n    total_annotations = 4\n    assert compute_average_precision(predictions, total_annotations) == pytest.approx(\n        0.5 * 1.0 + 0.25 * 0.75\n    )\n    # Throwing out false predictions with lowest score does not influence the result.\n    assert compute_average_precision(\n        [True, True, False, True], total_annotations\n    ) == pytest.approx(0.5 * 1.0 + 0.25 * 0.75)\n    # Throwing out false prediction if the same (or lower) score contains also true predictions\n    # improves the result.\n    assert compute_average_precision(\n        [True, True, True, False, False], total_annotations\n    ) == pytest.approx(0.5 * 1.0 + 0.25 * 1.0)", "\n\ndef test_compute_average_precision_fill_gaps() -> None:\n    \"\"\"Test the influence of \"filling gaps\".\"\"\"\n\n    predictions = [True, False, False, True, True]\n    # precision recall pairs:\n    # recall -> precision\n    #    0.1    1/1\n    #    0.2    2/4, adjusted to 3/5 achievable for recall 0.3\n    #    0.3    3/5\n    total_annotations = 10\n    expected_average_precision = 0.1 * 1 / 1 + 0.1 * 3 / 5 + 0.1 * 3 / 5  # 0.22\n    assert compute_average_precision(predictions, total_annotations) == pytest.approx(\n        expected_average_precision\n    )\n\n    predictions_sorted_false_first = sorted(predictions, key=lambda sm: sm)\n    # precision recall pairs:\n    # recall -> precision\n    #    0.1    1/3, adjusted to 3/5 achievable for recall 0.3\n    #    0.2    2/4, adjusted to 3/5 achievable for recall 0.3\n    #    0.3    3/5\n    expected_average_precision = 0.3 * 3 / 5  # 0.18\n    assert compute_average_precision(\n        predictions_sorted_false_first, total_annotations\n    ) == pytest.approx(expected_average_precision)\n\n    predictions_sorted_true_first = sorted(predictions, key=lambda sm: not sm)\n    # precision recall pairs:\n    # recall -> precision\n    #    0.1    1\n    #    0.2    1\n    #    0.3    1\n    assert compute_average_precision(\n        predictions_sorted_true_first, total_annotations\n    ) == pytest.approx(0.3)", ""]}
{"filename": "tests/evaluation/test_pcc_field_matching.py", "chunked_list": ["import pytest\n\nfrom docile.dataset import BBox, Field\nfrom docile.evaluation.pcc import PCC, PCCSet\nfrom docile.evaluation.pcc_field_matching import FieldMatching, get_matches, pccs_iou\n\n\ndef test_filter_field_matching() -> None:\n    bbox = BBox(0, 0, 1, 1)\n    field_f1_no_text = Field(bbox, page=0, fieldtype=\"f1\")\n    field_f1_text_a = Field(bbox, page=0, fieldtype=\"f1\", text=\"a\")\n    field_f1_text_b = Field(bbox, page=0, fieldtype=\"f1\", text=\"b\", use_only_for_ap=True)\n    field_f2 = Field(bbox, page=0, fieldtype=\"f2\", text=\"x\")\n    field_matching = FieldMatching(\n        ordered_predictions_with_match=[\n            (field_f1_no_text, None),\n            (field_f1_no_text, field_f1_text_a),\n            (field_f1_text_a, field_f1_text_a),\n            (field_f1_text_b, field_f1_text_a),\n            (field_f1_text_b, None),\n            (field_f2, field_f2),\n        ],\n        false_negatives=[field_f2, field_f1_text_a],\n    )\n\n    assert field_matching.filter() == field_matching\n    assert field_matching.filter(same_text=True) == FieldMatching(\n        ordered_predictions_with_match=[\n            (field_f1_no_text, None),\n            (field_f1_no_text, None),\n            (field_f1_text_a, field_f1_text_a),\n            (field_f1_text_b, None),\n            (field_f1_text_b, None),\n            (field_f2, field_f2),\n        ],\n        false_negatives=[field_f2, field_f1_text_a, field_f1_text_a, field_f1_text_a],\n    )\n    assert field_matching.filter(fieldtype=\"f1\") == FieldMatching(\n        ordered_predictions_with_match=[\n            (field_f1_no_text, None),\n            (field_f1_no_text, field_f1_text_a),\n            (field_f1_text_a, field_f1_text_a),\n            (field_f1_text_b, field_f1_text_a),\n            (field_f1_text_b, None),\n        ],\n        false_negatives=[field_f1_text_a],\n    )\n    assert field_matching.filter(same_text=True, fieldtype=\"f1\") == FieldMatching(\n        ordered_predictions_with_match=[\n            (field_f1_no_text, None),\n            (field_f1_no_text, None),\n            (field_f1_text_a, field_f1_text_a),\n            (field_f1_text_b, None),\n            (field_f1_text_b, None),\n        ],\n        false_negatives=[field_f1_text_a, field_f1_text_a, field_f1_text_a],\n    )\n    assert field_matching.filter(exclude_only_for_ap=True) == FieldMatching(\n        ordered_predictions_with_match=[\n            (field_f1_no_text, None),\n            (field_f1_no_text, field_f1_text_a),\n            (field_f1_text_a, field_f1_text_a),\n            (field_f2, field_f2),\n        ],\n        false_negatives=[field_f2, field_f1_text_a, field_f1_text_a],\n    )", "\n\ndef test_pccs_iou() -> None:\n    pcc_set = PCCSet([PCC(0, 0.5, 1), PCC(0.5, 0.5, 1), PCC(1, 1, 1)])\n    gold_bbox = BBox(0.0, 0.0, 1.0, 1.0)\n    pred_bbox = BBox(0.25, 0.25, 0.75, 0.75)\n    assert pccs_iou(pcc_set, gold_bbox, pred_bbox, page=1) == pytest.approx(1 / 3)\n\n\ndef test_pccs_iou_empty() -> None:\n    pcc_set = PCCSet([PCC(1, 1, 1)])\n    gold_bbox_empty = BBox(0.25, 0.25, 0.75, 0.75)\n    pred_bbox_empty = BBox(0.0, 0.0, 0.75, 0.75)\n    pred_bbox_nonempty = BBox(0.0, 0.0, 1.25, 1.25)\n\n    assert pccs_iou(pcc_set, gold_bbox_empty, pred_bbox_empty, page=1) == 1.0\n    assert pccs_iou(pcc_set, gold_bbox_empty, pred_bbox_nonempty, page=1) == 0.0", "\ndef test_pccs_iou_empty() -> None:\n    pcc_set = PCCSet([PCC(1, 1, 1)])\n    gold_bbox_empty = BBox(0.25, 0.25, 0.75, 0.75)\n    pred_bbox_empty = BBox(0.0, 0.0, 0.75, 0.75)\n    pred_bbox_nonempty = BBox(0.0, 0.0, 1.25, 1.25)\n\n    assert pccs_iou(pcc_set, gold_bbox_empty, pred_bbox_empty, page=1) == 1.0\n    assert pccs_iou(pcc_set, gold_bbox_empty, pred_bbox_nonempty, page=1) == 0.0\n", "\n\ndef test_get_matches() -> None:\n    pcc_set = PCCSet(\n        [\n            PCC(0, 0, 0),\n            PCC(0.1, 0.1, 0),\n            PCC(0.2, 0.1, 0),\n            PCC(0.5, 0.4, 0),\n            PCC(0.5, 0.6, 0),\n            PCC(1, 1, 0),\n            PCC(0.5, 0.5, 1),\n        ]\n    )\n\n    annotations = [\n        Field(fieldtype=\"full_match\", text=\"ab\", bbox=BBox(0.4, 0.4, 0.7, 0.7), page=0),\n        Field(fieldtype=\"no_match\", text=\"ab\", bbox=BBox(0, 0, 0.3, 0.2), page=0),\n        Field(fieldtype=\"full_match\", text=\"ab\", bbox=BBox(0.4, 0.4, 0.7, 0.7), page=1),\n        Field(fieldtype=\"partial_match\", text=\"ab\", bbox=BBox(0.05, 0.05, 0.3, 0.2), page=0),\n        Field(fieldtype=\"no_match\", text=\"ab\", bbox=BBox(0, 0, 1.0, 1.0), page=0),\n    ]\n    predictions = [\n        Field(fieldtype=\"full_match\", bbox=BBox(0.4, 0.4, 0.7, 0.7), page=0),\n        Field(fieldtype=\"full_match\", bbox=BBox(0.4, 0.4, 0.7, 0.7), page=1),\n        Field(fieldtype=\"partial_match\", bbox=BBox(0.15, 0.05, 0.3, 0.2), page=0),\n        Field(fieldtype=\"no_match\", bbox=BBox(0.9, 0.9, 1, 1), page=0),\n        Field(fieldtype=\"no_match\", bbox=BBox(0.7, 0.7, 0.8, 0.8), page=0),\n        Field(fieldtype=\"no_match\", bbox=BBox(0, 0, 1.0, 1.0), page=1),  # mismatching page\n    ]\n\n    matching = get_matches(predictions=predictions, annotations=annotations, pcc_set=pcc_set)\n    assert len(matching.matches) == 2\n    assert all(\n        match.pred.fieldtype == match.gold.fieldtype == \"full_match\" for match in matching.matches\n    )\n    assert len(matching.false_positives) == 4\n    assert len(matching.false_negatives) == 3\n    assert matching.ordered_predictions_with_match == [\n        (predictions[0], annotations[0]),\n        (predictions[1], annotations[2]),\n        (predictions[2], None),\n        (predictions[3], None),\n        (predictions[4], None),\n        (predictions[5], None),\n    ]\n\n    matching_iou05 = get_matches(\n        predictions=predictions, annotations=annotations, pcc_set=pcc_set, iou_threshold=0.5\n    )\n    assert len(matching_iou05.matches) == 3\n    assert all(\n        match.pred.fieldtype == match.gold.fieldtype\n        and match.pred.fieldtype in [\"full_match\", \"partial_match\"]\n        for match in matching_iou05.matches\n    )\n    assert len(matching_iou05.false_positives) == 3\n    assert len(matching_iou05.false_negatives) == 2\n    assert matching_iou05.ordered_predictions_with_match == [\n        (predictions[0], annotations[0]),\n        (predictions[1], annotations[2]),\n        (predictions[2], annotations[3]),\n        (predictions[3], None),\n        (predictions[4], None),\n        (predictions[5], None),\n    ]", ""]}
{"filename": "tests/evaluation/__init__.py", "chunked_list": [""]}
{"filename": "tests/evaluation/test_evaluate.py", "chunked_list": ["import random\nfrom copy import deepcopy\nfrom dataclasses import replace\nfrom typing import Tuple\n\nimport pytest\n\nfrom docile.dataset import BBox, Dataset, Field\nfrom docile.evaluation.evaluate import (\n    EvaluationResult,", "from docile.evaluation.evaluate import (\n    EvaluationResult,\n    PredictionsValidationError,\n    _get_prediction_sort_key,\n    _sort_predictions,\n    _validate_predictions,\n    compute_metrics,\n    evaluate_dataset,\n)\nfrom docile.evaluation.pcc_field_matching import FieldMatching", ")\nfrom docile.evaluation.pcc_field_matching import FieldMatching\n\n\n@pytest.fixture\ndef mock_evaluation_result() -> EvaluationResult:\n    field1 = Field(BBox(0, 0, 1, 1), page=0, score=1, fieldtype=\"f1\")\n    field05 = Field(BBox(0, 0, 1, 1), page=0, score=0.5, fieldtype=\"f05\")\n    field_ap_only = Field(BBox(0, 0, 1, 1), page=0, score=1, fieldtype=\"f1\", use_only_for_ap=True)\n    evaluation_result = EvaluationResult(\n        task_to_docid_to_matching={\n            \"kile\": {\n                \"a\": FieldMatching(\n                    ordered_predictions_with_match=[(field1, field1), (field1, None)],\n                    false_negatives=[field1],\n                ),\n                \"b\": FieldMatching.empty([field05, field05], [field1]),\n            },\n            \"lir\": {\n                \"a\": FieldMatching.empty([], [field1]),\n                \"b\": FieldMatching(\n                    ordered_predictions_with_match=[\n                        (field1, field1),\n                        (field05, None),\n                        (field05, None),\n                        (field05, field05),\n                        (field_ap_only, None),\n                        (field_ap_only, None),\n                        (field_ap_only, None),\n                        (field_ap_only, field1),\n                    ],\n                    false_negatives=[],\n                ),\n            },\n        },\n        dataset_name=\"mock-dataset\",\n        iou_threshold=1.0,\n    )\n    return evaluation_result", "\n\ndef test_evaluation_result_get_primary_metric(mock_evaluation_result: EvaluationResult) -> None:\n    # AP = 0.5 for KILE because recall is 1/3 and the first prediction is the correct one.\n    assert mock_evaluation_result.get_primary_metric(\"kile\") == 1 / 3\n    # f1 = 1/2 for LIR because both precision and recall are 0.5 (2/4)\n    assert mock_evaluation_result.get_primary_metric(\"lir\") == 1 / 2\n\n\ndef test_evaluation_result_get_metrics(mock_evaluation_result: EvaluationResult) -> None:\n    assert mock_evaluation_result.get_metrics(\"kile\") == {\n        \"TP\": 1,\n        \"FP\": 3,\n        \"FN\": 2,\n        \"precision\": 1 / 4,\n        \"recall\": 1 / 3,\n        \"f1\": pytest.approx(2 / 7),\n        \"AP\": 1 / 3,\n    }\n    assert mock_evaluation_result.get_metrics(\"lir\") == {\n        \"TP\": 2,\n        \"FP\": 2,\n        \"FN\": 2,\n        \"precision\": 1 / 2,\n        \"recall\": 1 / 2,\n        \"f1\": 1 / 2,\n        \"AP\": (1 / 4) * (1 / 1) + (1 / 4) * (2 / 4) + (1 / 4) * (3 / 8),\n    }\n    assert mock_evaluation_result.get_metrics(\"kile\", fieldtype=\"f1\") == {\n        \"TP\": 1,\n        \"FP\": 1,\n        \"FN\": 2,\n        \"precision\": 1 / 2,\n        \"recall\": 1 / 3,\n        \"f1\": pytest.approx(2 / 5),\n        \"AP\": 1 / 3,\n    }\n    assert mock_evaluation_result.get_metrics(\"lir\", fieldtype=\"f05\", docids=[\"b\"]) == {\n        \"TP\": 1,\n        \"FP\": 2,\n        \"FN\": 0,\n        \"precision\": pytest.approx(1 / 3),\n        \"recall\": 1,\n        \"f1\": 1 / 2,\n        \"AP\": pytest.approx(1 / 3),\n    }", "\ndef test_evaluation_result_get_metrics(mock_evaluation_result: EvaluationResult) -> None:\n    assert mock_evaluation_result.get_metrics(\"kile\") == {\n        \"TP\": 1,\n        \"FP\": 3,\n        \"FN\": 2,\n        \"precision\": 1 / 4,\n        \"recall\": 1 / 3,\n        \"f1\": pytest.approx(2 / 7),\n        \"AP\": 1 / 3,\n    }\n    assert mock_evaluation_result.get_metrics(\"lir\") == {\n        \"TP\": 2,\n        \"FP\": 2,\n        \"FN\": 2,\n        \"precision\": 1 / 2,\n        \"recall\": 1 / 2,\n        \"f1\": 1 / 2,\n        \"AP\": (1 / 4) * (1 / 1) + (1 / 4) * (2 / 4) + (1 / 4) * (3 / 8),\n    }\n    assert mock_evaluation_result.get_metrics(\"kile\", fieldtype=\"f1\") == {\n        \"TP\": 1,\n        \"FP\": 1,\n        \"FN\": 2,\n        \"precision\": 1 / 2,\n        \"recall\": 1 / 3,\n        \"f1\": pytest.approx(2 / 5),\n        \"AP\": 1 / 3,\n    }\n    assert mock_evaluation_result.get_metrics(\"lir\", fieldtype=\"f05\", docids=[\"b\"]) == {\n        \"TP\": 1,\n        \"FP\": 2,\n        \"FN\": 0,\n        \"precision\": pytest.approx(1 / 3),\n        \"recall\": 1,\n        \"f1\": 1 / 2,\n        \"AP\": pytest.approx(1 / 3),\n    }", "\n\ndef test_evaluation_result_print_report(mock_evaluation_result: EvaluationResult) -> None:\n    assert (\n        mock_evaluation_result.print_report(include_fieldtypes=False, floatfmt=\".2f\")\n        == \"\"\"\\\nEvaluation report for mock-dataset\n==================================\nKILE\n----\nPrimary metric (AP): 0.3333333333333333\n\n| fieldtype            |   AP |   f1 |   precision |   recall |   TP |   FP |   FN |\n|----------------------|------|------|-------------|----------|------|------|------|\n| **-> micro average** | 0.33 | 0.29 |        0.25 |     0.33 |    1 |    3 |    2 |\n\nLIR\n---\nPrimary metric (f1): 0.5\n\n| fieldtype            |   AP |   f1 |   precision |   recall |   TP |   FP |   FN |\n|----------------------|------|------|-------------|----------|------|------|------|\n| **-> micro average** | 0.47 | 0.50 |        0.50 |     0.50 |    2 |    2 |    2 |\n\nNotes:\n* For AP all predictions are used. For f1, precision, recall, TP, FP and FN predictions explicitly marked with flag `use_only_for_ap=True` are excluded.\n\"\"\"\n    )", "\n\ndef _assert_metrics_at_least(\n    evaluation_result: EvaluationResult,\n    minimum_value: float,\n    tasks: Tuple[str, ...] = (\"kile\", \"lir\"),\n    eval_same_text: Tuple[bool, ...] = (False, True),\n    check_metric_names: Tuple[str, ...] = (\"AP\", \"f1\", \"precision\", \"recall\"),\n) -> None:\n    for task in tasks:\n        for same_text in eval_same_text:\n            metrics = evaluation_result.get_metrics(task=task, same_text=same_text)\n            for metric_name in check_metric_names:\n                assert metrics[metric_name] == minimum_value", "\n\ndef test_evaluate_dataset_perfect_predictions(\n    sample_dataset: Dataset, sample_dataset_docid: str\n) -> None:\n    kile_predictions = {\n        sample_dataset_docid: sample_dataset[sample_dataset_docid].annotation.fields\n    }\n    lir_predictions = {\n        sample_dataset_docid: sample_dataset[sample_dataset_docid].annotation.li_fields\n    }\n    evaluation_result = evaluate_dataset(sample_dataset, kile_predictions, lir_predictions)\n    _assert_metrics_at_least(evaluation_result, 1.0)", "\n\ndef test_evaluate_dataset_perfect_predictions_with_perturbations(\n    sample_dataset: Dataset, sample_dataset_docid: str, random_seed: int = 402269889108107772\n) -> None:\n    \"\"\"Test that changing the bboxes by 0.005% of the width/height does not influence the metric.\"\"\"\n\n    def _field_perturbation(field: Field, rng: random.Random, max_change: float) -> Field:\n        \"\"\"Change each bbox coordinate by -1 or 1 times 'max_change'.\"\"\"\n        new_bbox = BBox(*(x + rng.randrange(-1, 2, 2) * max_change for x in field.bbox.to_tuple()))\n        return replace(field, bbox=new_bbox)\n\n    rng = random.Random(random_seed)\n    max_change = 0.005 / 100\n    kile_predictions = {\n        sample_dataset_docid: [\n            _field_perturbation(field, rng, max_change)\n            for field in sample_dataset[sample_dataset_docid].annotation.fields\n        ]\n    }\n    lir_predictions = {\n        sample_dataset_docid: [\n            _field_perturbation(field, rng, max_change)\n            for field in sample_dataset[sample_dataset_docid].annotation.li_fields\n        ]\n    }\n    evaluation_result = evaluate_dataset(\n        sample_dataset, kile_predictions, lir_predictions, iou_threshold=0.9\n    )\n    _assert_metrics_at_least(evaluation_result, 1.0)", "\n\ndef test_evaluate_dataset_kile_missing_and_wrong_predictions(\n    sample_dataset: Dataset,\n    sample_dataset_docid: str,\n) -> None:\n    kile_predictions = {\n        sample_dataset_docid: [\n            replace(f, score=1) for f in sample_dataset[sample_dataset_docid].annotation.fields\n        ]\n    }\n    # 1 missing and 3 wrong predictions\n    kile_predictions[sample_dataset_docid].pop()\n    kile_predictions[sample_dataset_docid][0] = replace(\n        kile_predictions[sample_dataset_docid][0],\n        page=1000,\n        score=0.8,  # all other fields have default score 1.0\n    )\n    kile_predictions[sample_dataset_docid][1] = replace(\n        kile_predictions[sample_dataset_docid][1],\n        bbox=BBox(0, 0, 1, 1),\n    )\n    kile_predictions[sample_dataset_docid][2] = replace(\n        kile_predictions[sample_dataset_docid][2],\n        fieldtype=\"mock_fieldtype\",\n    )\n\n    fields = len(sample_dataset[sample_dataset_docid].annotation.fields)\n    false_negatives = 4\n    true_positives = fields - false_negatives\n    false_positives = 3\n    recall = true_positives / fields\n    precision = true_positives / (true_positives + false_positives)\n    f1 = 2 * precision * recall / (precision + recall)\n\n    # AP computation:\n    # After sorting the predictions by score and prediction id, we get the following list:\n    #       False, False, True, True, True, ..., True, False\n    # The best precision is achieved for the highest recall which means it will be used also for\n    # the smaller recall values (check average_precison.py for details).\n\n    # the false prediction with the lowest score does not affect AP value\n    ap_precision = true_positives / (true_positives + false_positives - 1)\n    ap = recall * ap_precision\n\n    evaluation_result = evaluate_dataset(sample_dataset, kile_predictions, {})\n    assert evaluation_result.get_metrics(\"kile\") == {\n        \"AP\": pytest.approx(ap),\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"TP\": true_positives,\n        \"FP\": false_positives,\n        \"FN\": false_negatives,\n    }", "\n\ndef test_evaluate_dataset_lir_missing_and_wrong_predictions(\n    sample_dataset: Dataset,\n    sample_dataset_docid: str,\n) -> None:\n    lir_predictions = {\n        sample_dataset_docid: [\n            replace(f, score=1)\n            for f in deepcopy(sample_dataset[sample_dataset_docid].annotation.li_fields)\n        ]\n    }\n    # missing prediction\n    lir_predictions[sample_dataset_docid].pop()\n    # wrong page\n    lir_predictions[sample_dataset_docid][0] = replace(\n        lir_predictions[sample_dataset_docid][0],\n        page=1000,\n    )\n    # duplicated predictions\n    lir_predictions[sample_dataset_docid].append(\n        lir_predictions[sample_dataset_docid][1],\n    )\n    lir_predictions[sample_dataset_docid].append(\n        lir_predictions[sample_dataset_docid][1],\n    )\n    # wrong fieldtype\n    lir_predictions[sample_dataset_docid][2] = replace(\n        lir_predictions[sample_dataset_docid][2],\n        fieldtype=\"mock_fieldtype\",\n    )\n    # assigned to wrong line item\n    lir_predictions[sample_dataset_docid][3] = replace(\n        lir_predictions[sample_dataset_docid][3],\n        line_item_id=1000,\n    )\n\n    # change line item ids, does not influence results\n    for i in range(len(lir_predictions[sample_dataset_docid])):\n        if lir_predictions[sample_dataset_docid][i].line_item_id in [5, 6, 7, 8]:\n            # swap line item ids 5<->8 & 6<->7\n            new_line_item_id = 13 - lir_predictions[sample_dataset_docid][i].line_item_id\n            lir_predictions[sample_dataset_docid][i] = replace(\n                lir_predictions[sample_dataset_docid][i],\n                line_item_id=new_line_item_id,\n            )\n\n    fields = len(sample_dataset[sample_dataset_docid].annotation.li_fields)\n    false_negatives = 4\n    true_positives = fields - false_negatives\n    false_positives = 5\n    recall = true_positives / fields\n    precision = true_positives / (true_positives + false_positives)\n    f1 = 2 * precision * recall / (precision + recall)\n\n    # AP computation:\n    # After sorting the predictions by score and prediction id, we get the following list:\n    #       False, True, False, False, True, True, True, ..., True, False, False\n    # The best precision is achieved for the highest recall which means it will be used also for\n    # the smaller recall values (check average_precison.py for details).\n\n    # The 2 extra predictions are ignored as they are last.\n    ap_precision = true_positives / (true_positives + false_positives - 2)\n    ap = recall * ap_precision\n\n    evaluation_result = evaluate_dataset(sample_dataset, {}, lir_predictions)\n    assert evaluation_result.get_metrics(\"lir\") == {\n        \"AP\": pytest.approx(ap),\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"TP\": true_positives,\n        \"FP\": false_positives,\n        \"FN\": false_negatives,\n    }", "\n\ndef test_compute_metrics() -> None:\n    field1 = Field(BBox(0, 0, 1, 1), page=0, score=1)\n    field05 = Field(BBox(0, 0, 1, 1), page=0, score=0.5)\n    field_ap_only = Field(BBox(0, 0, 1, 1), page=0, score=0.25, use_only_for_ap=True)\n    docid_to_field_matching = {\n        \"a\": FieldMatching(\n            ordered_predictions_with_match=[\n                (field_ap_only, field1),\n                (field_ap_only, None),\n                (field05, None),\n                (field1, field1),\n                (field05, field05),\n                (field1, None),\n            ],\n            false_negatives=[],\n        ),\n        \"b\": FieldMatching(\n            ordered_predictions_with_match=[(field1, field1)],\n            false_negatives=[],\n        ),\n    }\n    assert compute_metrics(docid_to_field_matching) == {\n        \"TP\": 3,\n        \"FP\": 2,\n        \"FN\": 1,\n        \"precision\": 3 / 5,\n        \"recall\": 3 / 4,\n        \"f1\": pytest.approx(2 / 3),\n        # sorted predictions matched: [True, True, False, False, True, True, False]\n        # (recall, precision) pairs: [(0.25, 1), (0.5, 1), (0.75, 0.6), (1, 4/6)]\n        # after \"filling gaps\": [(0.5, 1), (1, 4/6)]\n        \"AP\": pytest.approx(0.5 * 1 + 0.5 * 4 / 6),\n    }", "\n\ndef test_validate_predictions(sample_dataset: Dataset, sample_dataset_docid: str) -> None:\n    bbox = BBox(0, 0, 1, 1)\n    with pytest.raises(\n        PredictionsValidationError,\n        match=\"You need to provide at least one prediction for at least one of the tasks.\",\n    ):\n        _validate_predictions(sample_dataset, {})\n\n    too_many_predictions = {\"task\": {sample_dataset_docid: [Field(bbox=bbox, page=2)] * 1001}}\n    with pytest.raises(\n        PredictionsValidationError,\n        match=f\"TASK: Exceeded limit of 1000 predictions per page for doc: {sample_dataset_docid}\",\n    ):\n        _validate_predictions(sample_dataset, too_many_predictions)\n\n    missing_fieldtype = {\"task\": {sample_dataset_docid: [Field(bbox=bbox, page=0)]}}\n    with pytest.raises(\n        PredictionsValidationError, match=\"TASK: Prediction is missing 'fieldtype'.\"\n    ):\n        _validate_predictions(sample_dataset, missing_fieldtype)\n\n    with_line_item_id = {\n        sample_dataset_docid: [Field(bbox=bbox, page=0, fieldtype=\"f\", line_item_id=8)]\n    }\n    _validate_predictions(sample_dataset, {\"lir\": with_line_item_id})  # ok\n    with pytest.raises(\n        PredictionsValidationError, match=\"KILE: Prediction has extra 'line_item_id'.\"\n    ):\n        _validate_predictions(sample_dataset, {\"kile\": with_line_item_id})\n\n    without_line_item_id = {sample_dataset_docid: [Field(bbox=bbox, page=0, fieldtype=\"f\")]}\n    _validate_predictions(sample_dataset, {\"kile\": without_line_item_id})\n    with pytest.raises(\n        PredictionsValidationError, match=\"LIR: Prediction is missing 'line_item_id'.\"\n    ):\n        _validate_predictions(sample_dataset, {\"lir\": without_line_item_id})\n\n    only_part_with_scores = {\n        sample_dataset_docid: [\n            Field(bbox=bbox, page=0, fieldtype=\"f1\"),\n            Field(bbox=bbox, page=0, fieldtype=\"f2\", score=1.0),\n        ]\n    }\n    with pytest.raises(\n        PredictionsValidationError,\n        match=\"TASK: Either all or no predictions should have 'score' defined\",\n    ):\n        _validate_predictions(sample_dataset, {\"task\": only_part_with_scores})\n\n    extra_doc = {sample_dataset_docid: [], \"mock-docid\": []}\n    with pytest.raises(\n        PredictionsValidationError,\n        match=\"TASK: Predictions provided for 1 documents not in the dataset sample-dataset:dev\",\n    ):\n        _validate_predictions(sample_dataset, {\"task\": extra_doc})\n\n    missing_doc = {}\n    with pytest.raises(\n        PredictionsValidationError,\n        match=(\n            \"TASK: Predictions not provided for 1/1 documents. Pass an empty list of predictions \"\n            \"for these documents if this was intended.\"\n        ),\n    ):\n        _validate_predictions(sample_dataset, {\"task\": missing_doc})", "\n\ndef test_sort_predictions() -> None:\n    bbox = BBox(0, 0, 1, 1)\n    f_gold = Field(bbox=bbox, page=0)\n    predictions_doc_a = [\n        Field(bbox=bbox, text=\"a\", page=0, score=0.4),\n        Field(bbox=bbox, text=\"a\", page=0, score=1),\n        Field(bbox=bbox, text=\"a\", page=0, score=0.8),\n        Field(bbox=bbox, text=\"a\", page=0, score=0.4),\n    ]\n    predictions_doc_b = [\n        Field(bbox=bbox, text=\"b\", page=0, score=0.8),\n        Field(bbox=bbox, text=\"b\", page=0, score=0.4),\n        Field(bbox=bbox, text=\"b\", page=0, score=0.5),\n    ]\n    docid_to_matching = {\n        \"a\": FieldMatching(\n            ordered_predictions_with_match=[\n                (predictions_doc_a[0], f_gold),\n                (predictions_doc_a[1], f_gold),\n                (predictions_doc_a[2], f_gold),\n                (predictions_doc_a[3], f_gold),\n            ],\n            false_negatives=[f_gold],\n        ),\n        \"b\": FieldMatching(\n            ordered_predictions_with_match=[\n                (predictions_doc_b[0], None),\n                (predictions_doc_b[1], None),\n                (predictions_doc_b[2], None),\n            ],\n            false_negatives=[],\n        ),\n    }\n\n    actual_sorted_predictions = _sort_predictions(docid_to_matching)\n    expected_sorted_predictions = [\n        True,  # a[1], score=1\n        False,  # b[0], score=0.8, pred_i=0\n        True,  # a[2], score=0.8, pred_i=2\n        False,  # b[2], score=0.5\n        True,  # a[0], score=0.4, pred_i=0\n        False,  # b[1], score=0.4, pred_i=1\n        True,  # a[3], score=0.4, pred_i=3\n    ]\n    assert actual_sorted_predictions == expected_sorted_predictions", "\n\ndef test_sort_predictions_with_ap_only() -> None:\n    bbox = BBox(0, 0, 1, 1)\n    f_gold = Field(bbox=bbox, page=0)\n    predictions_doc_a = [\n        Field(bbox=bbox, text=\"a\", page=0, score=0.4),\n        Field(bbox=bbox, text=\"a\", page=0, score=1),\n        Field(bbox=bbox, text=\"a\", page=0, score=0.8, use_only_for_ap=True),\n        Field(bbox=bbox, text=\"a\", page=0, score=0.4),\n    ]\n    predictions_doc_b = [\n        Field(bbox=bbox, text=\"b\", page=0, score=0.8),\n        Field(bbox=bbox, text=\"b\", page=0, score=0.4, use_only_for_ap=True),\n        Field(bbox=bbox, text=\"b\", page=0, score=0.5),\n    ]\n    docid_to_matching = {\n        \"a\": FieldMatching(\n            ordered_predictions_with_match=[\n                (predictions_doc_a[0], f_gold),\n                (predictions_doc_a[1], f_gold),\n                (predictions_doc_a[2], f_gold),\n                (predictions_doc_a[3], f_gold),\n            ],\n            false_negatives=[f_gold],\n        ),\n        \"b\": FieldMatching(\n            ordered_predictions_with_match=[\n                (predictions_doc_b[0], None),\n                (predictions_doc_b[1], None),\n                (predictions_doc_b[2], None),\n            ],\n            false_negatives=[],\n        ),\n    }\n\n    # fields with use_only_for_ap=True will be last\n    actual_sorted_predictions = _sort_predictions(docid_to_matching)\n    expected_sorted_predictions = [\n        True,  # a[1], score=1\n        False,  # b[0], score=0.8, pred_i=0\n        False,  # b[2], score=0.5\n        True,  # a[0], score=0.4, pred_i=0\n        True,  # a[3], score 0.4, pred_i=3\n        True,  # a[2], use_only_for_ap=True, score=0.8, pred_i=2\n        False,  # b[1], use_only_for_ap=True, score=0.4, pred_i=1\n    ]\n    assert actual_sorted_predictions == expected_sorted_predictions", "\n\ndef test_get_prediction_sort_key() -> None:\n    a0 = _get_prediction_sort_key((False, -1.0), 0, \"a\")\n    b0 = _get_prediction_sort_key((False, -1.0), 0, \"b\")\n    a2 = _get_prediction_sort_key((True, -1.5), 2, \"a\")\n    b2 = _get_prediction_sort_key((True, -1.5), 2, \"b\")\n    assert a0 == ((False, -1.0), 0, \"d95520d967275249\")\n    assert b0 == ((False, -1.0), 0, \"0407a70fca4cc072\")\n    assert a2 == ((True, -1.5), 2, \"55d1989cd656edd2\")\n    assert b2 == ((True, -1.5), 2, \"77b295898f2dbda5\")\n    # The two pairs (a0,b0) and (a2,b2) have the same score and prediction_i but are in different\n    # order for the two docs. This is thanks to the hashing of docid with prediction_i.\n    assert b0 < a0\n    assert a2 < b2", ""]}
{"filename": "tests/evaluation/test_pcc.py", "chunked_list": ["import pytest\n\nfrom docile.dataset import BBox, Dataset\nfrom docile.evaluation.pcc import PCC, PCCSet, _calculate_pccs, _get_snapped_ocr_words\n\n\ndef test_get_covered_pccs() -> None:\n    pccs = [PCC(0, 0.5, 0), PCC(0.5, 0.5, 0), PCC(1, 1, 0), PCC(0.5, 0.5, 1), PCC(0.4, 0.6, 1)]\n    pcc_set = PCCSet(pccs)\n    bbox = BBox(0.25, 0.25, 0.75, 0.75)\n    assert pcc_set.get_covered_pccs(bbox, 0) == {PCC(0.5, 0.5, 0)}\n    assert pcc_set.get_covered_pccs(bbox, 1) == {PCC(0.5, 0.5, 1), PCC(0.4, 0.6, 1)}\n    assert pcc_set.get_covered_pccs(BBox(0.39, 0.59, 0.41, 0.61), 1) == {PCC(0.4, 0.6, 1)}", "\n\ndef test_get_snapped_ocr_words(sample_dataset: Dataset, sample_dataset_docid: str) -> None:\n    document = sample_dataset[sample_dataset_docid]\n    words = _get_snapped_ocr_words(document)\n    original_words = []\n    for page in range(document.page_count):\n        original_words.extend(document.ocr.get_all_words(page))\n\n    for word in words:\n        assert _bbox_area(word.bbox) > 0\n\n    words_total_area = sum(_bbox_area(word.bbox) for word in words)\n    original_words_total_area = sum(_bbox_area(word.bbox) for word in original_words)\n\n    # On average, area of snapped bounding boxes decreases to 50-60% of the original area (for the\n    # default OCR method).\n    assert words_total_area < 0.9 * original_words_total_area", "\n\ndef test_calculate_pccs() -> None:\n    bbox = BBox(0.2, 0.1, 0.8, 0.3)\n    with pytest.raises(ValueError):\n        assert _calculate_pccs(bbox, \"\", 0)\n\n    y = pytest.approx(0.2)\n    assert _calculate_pccs(bbox, \"x\", 0) == [PCC(pytest.approx(0.5), y, 0)]  # type: ignore\n    assert _calculate_pccs(bbox, \"xx\", 1) == [\n        PCC(pytest.approx(0.35), y, 1),  # type: ignore\n        PCC(pytest.approx(0.65), y, 1),  # type: ignore\n    ]\n    assert _calculate_pccs(bbox, \"xxx\", 2) == [\n        PCC(pytest.approx(0.3), y, 2),  # type: ignore\n        PCC(pytest.approx(0.5), y, 2),  # type: ignore\n        PCC(pytest.approx(0.7), y, 2),  # type: ignore\n    ]", "\n\ndef _bbox_area(bbox: BBox) -> float:\n    return (bbox.right - bbox.left) * (bbox.bottom - bbox.top)\n"]}
{"filename": "tests/evaluation/test_line_item_matching.py", "chunked_list": ["import pytest\n\nfrom docile.dataset import BBox, Field\nfrom docile.evaluation import PCC, PCCSet\nfrom docile.evaluation.line_item_matching import (\n    _get_covering_bbox,\n    _get_line_item_id,\n    _place_bbox_in_document,\n    get_lir_matches,\n)", "    get_lir_matches,\n)\nfrom docile.evaluation.pcc_field_matching import MatchedPair\n\n\ndef test_get_line_item_id() -> None:\n    assert _get_line_item_id(Field(bbox=BBox(0, 0, 1, 1), page=0, line_item_id=3)) == 3\n    with pytest.raises(ValueError):\n        _get_line_item_id(Field(bbox=BBox(0, 0, 1, 1), page=0))\n", "\n\ndef test_place_bbox_in_document() -> None:\n    bbox = BBox(0.2, 0.3, 0.4, 0.5)\n    assert _place_bbox_in_document(bbox, 0) == bbox\n    assert _place_bbox_in_document(bbox, 1) == BBox(0.2, 1.3, 0.4, 1.5)\n    assert _place_bbox_in_document(bbox, 2) == BBox(0.2, 2.3, 0.4, 2.5)\n\n\ndef test_get_covering_bbox() -> None:\n    with pytest.raises(ValueError):\n        _get_covering_bbox([])\n\n    bboxes = [\n        BBox(0.2, 0.3, 0.4, 0.5),\n        BBox(0.2, 0.3, 0.4, 0.5),\n        BBox(0.3, 0.2, 0.35, 0.35),\n        BBox(0.3, 1.1, 0.35, 1.2),\n    ]\n    assert _get_covering_bbox(bboxes[:1]) == bboxes[0]\n    assert _get_covering_bbox(bboxes[:2]) == bboxes[0]\n    assert _get_covering_bbox(bboxes[:3]) == BBox(0.2, 0.2, 0.4, 0.5)\n    assert _get_covering_bbox(iter(bboxes)) == BBox(0.2, 0.2, 0.4, 1.2)", "\ndef test_get_covering_bbox() -> None:\n    with pytest.raises(ValueError):\n        _get_covering_bbox([])\n\n    bboxes = [\n        BBox(0.2, 0.3, 0.4, 0.5),\n        BBox(0.2, 0.3, 0.4, 0.5),\n        BBox(0.3, 0.2, 0.35, 0.35),\n        BBox(0.3, 1.1, 0.35, 1.2),\n    ]\n    assert _get_covering_bbox(bboxes[:1]) == bboxes[0]\n    assert _get_covering_bbox(bboxes[:2]) == bboxes[0]\n    assert _get_covering_bbox(bboxes[:3]) == BBox(0.2, 0.2, 0.4, 0.5)\n    assert _get_covering_bbox(iter(bboxes)) == BBox(0.2, 0.2, 0.4, 1.2)", "\n\ndef test_get_lir_matches() -> None:\n    pcc_set = PCCSet(\n        [\n            PCC(0, 0, 0),\n            PCC(0.1, 0.1, 0),\n            PCC(0.2, 0.1, 0),\n            PCC(0.5, 0.4, 0),\n            PCC(0.5, 0.6, 0),\n            PCC(1, 1, 0),\n            PCC(0.1, 0.1, 1),\n        ]\n    )\n\n    annotations = [\n        Field(fieldtype=\"a\", text=\"ab\", line_item_id=0, bbox=BBox(0.4, 0.4, 0.7, 0.7), page=0),\n        Field(fieldtype=\"b\", text=\"ab\", line_item_id=0, bbox=BBox(0.4, 0.4, 0.7, 0.7), page=0),\n        Field(fieldtype=\"a\", text=\"ab\", line_item_id=1, bbox=BBox(0.05, 0.05, 0.3, 0.2), page=0),\n        Field(fieldtype=\"c\", text=\"ab\", line_item_id=1, bbox=BBox(0, 0, 0.3, 0.2), page=0),\n        Field(fieldtype=\"a\", text=\"ab\", line_item_id=2, bbox=BBox(0.4, 0.5, 1.0, 1.0), page=0),\n        Field(fieldtype=\"b\", text=\"ab\", line_item_id=2, bbox=BBox(0, 0, 0.2, 0.2), page=1),\n    ]\n    predictions = [\n        # pred LI 0: 1 match with gold LI 0\n        Field(fieldtype=\"a\", line_item_id=4, bbox=BBox(0.4, 0.4, 0.7, 0.7), page=0),  # match in 0\n        Field(fieldtype=\"b\", line_item_id=4, bbox=BBox(0.4, 0.4, 0.7, 0.55), page=0),\n        # pred LI 1: 1 matches with gold LI 1, 2 matches with gold LI 2\n        Field(fieldtype=\"a\", line_item_id=1, bbox=BBox(0.4, 0.5, 1.0, 1.0), page=0),  # match in 2\n        Field(fieldtype=\"c\", line_item_id=1, bbox=BBox(0, 0, 0.3, 0.2), page=0),  # match in 1\n        Field(fieldtype=\"b\", line_item_id=1, bbox=BBox(0, 0, 0.2, 0.2), page=1),  # match in 2\n        # pred LI 2: 2 matches with gold LI 2 + 2 extra matches with gold LI 1 but with predictions\n        # marked as `use_only_for_ap=True` that do not affect line item matching.\n        Field(\n            fieldtype=\"a\", line_item_id=2, bbox=BBox(0.25, 0.59, 1.0, 1.0), page=0\n        ),  # match in 2\n        Field(\n            fieldtype=\"b\", line_item_id=2, bbox=BBox(0.05, 0.05, 0.15, 0.15), page=1\n        ),  # match in 2\n        Field(\n            fieldtype=\"a\",\n            line_item_id=2,\n            bbox=BBox(0.05, 0.05, 0.3, 0.2),\n            page=0,\n            use_only_for_ap=True,\n        ),  # match in 1\n        Field(\n            fieldtype=\"c\", line_item_id=2, bbox=BBox(0, 0, 0.3, 0.2), page=0, use_only_for_ap=True\n        ),  # match in 1\n    ]\n\n    # While greedy matching might assign pred line item (LI) 1 to gold LI 2, maximum matching\n    # will assign it to gold LI 1 (so that pred LI 2 can be assigned to gold LI 2). Notice that the\n    # predictions marked with `use_only_for_ap=True` are ignored for matching of LIs.\n\n    field_matching, li_matching = get_lir_matches(\n        predictions=predictions, annotations=annotations, pcc_set=pcc_set, iou_threshold=1\n    )\n    assert li_matching == {4: 0, 1: 1, 2: 2}\n    assert set(field_matching.matches) == {\n        MatchedPair(pred=predictions[0], gold=annotations[0]),\n        MatchedPair(pred=predictions[3], gold=annotations[3]),\n        MatchedPair(pred=predictions[5], gold=annotations[4]),\n        MatchedPair(pred=predictions[6], gold=annotations[5]),\n    }\n    assert set(field_matching.false_positives) == {\n        predictions[1],\n        predictions[2],\n        predictions[4],\n        predictions[7],\n        predictions[8],\n    }\n    assert set(field_matching.false_negatives) == {annotations[1], annotations[2]}\n\n    assert field_matching.ordered_predictions_with_match == [\n        (predictions[0], annotations[0]),\n        (predictions[1], None),\n        (predictions[2], None),\n        (predictions[3], annotations[3]),\n        (predictions[4], None),\n        (predictions[5], annotations[4]),\n        (predictions[6], annotations[5]),\n        (predictions[7], None),\n        (predictions[8], None),\n    ]", ""]}
{"filename": "tests/dataset/__init__.py", "chunked_list": [""]}
{"filename": "tests/dataset/test_document.py", "chunked_list": ["from pathlib import Path\n\nfrom docile.dataset import Document\n\n\ndef test_document_load(sample_dataset_docid: str, sample_dataset_path: Path) -> None:\n    document = Document(sample_dataset_docid, sample_dataset_path)\n\n    assert document.docid == sample_dataset_docid\n    assert document.page_count == 1\n    assert document.annotation is not None", ""]}
{"filename": "tests/dataset/test_bbox.py", "chunked_list": ["from docile.dataset.bbox import BBox\n\n\ndef test_bbox_to_absolute_coords() -> None:\n    b_float = BBox(left=0.1, top=0.2, right=0.3, bottom=0.4)\n    b_abs_w10_h100 = BBox(left=1, top=20, right=3, bottom=40)\n    assert b_float.to_absolute_coords(width=10, height=100) == b_abs_w10_h100\n\n\ndef test_bbox_intersects() -> None:\n    b0022 = BBox(0, 0, 2, 2)\n    b1122 = BBox(1, 1, 2, 2)\n    b2233 = BBox(2, 2, 3, 3)\n    b3344 = BBox(3, 3, 4, 4)\n    assert b0022.intersects(b1122)\n    assert b0022.intersects(b2233)\n    assert not b0022.intersects(b3344)\n    assert b1122.intersects(b2233)\n    assert not b1122.intersects(b3344)\n    assert b2233.intersects(b3344)", "\ndef test_bbox_intersects() -> None:\n    b0022 = BBox(0, 0, 2, 2)\n    b1122 = BBox(1, 1, 2, 2)\n    b2233 = BBox(2, 2, 3, 3)\n    b3344 = BBox(3, 3, 4, 4)\n    assert b0022.intersects(b1122)\n    assert b0022.intersects(b2233)\n    assert not b0022.intersects(b3344)\n    assert b1122.intersects(b2233)\n    assert not b1122.intersects(b3344)\n    assert b2233.intersects(b3344)", ""]}
{"filename": "tests/dataset/test_document_annotation.py", "chunked_list": ["from docile.dataset.bbox import BBox\nfrom docile.dataset.dataset import Dataset\n\n\ndef test_document_fields_getters(sample_dataset: Dataset, sample_dataset_docid: str) -> None:\n    doc = sample_dataset[sample_dataset_docid]\n    assert len(doc.annotation.fields) == 11\n    assert {f.fieldtype for f in doc.annotation.fields} == {\n        \"amount_due\",\n        \"amount_total_gross\",\n        \"customer_billing_address\",\n        \"customer_billing_name\",\n        \"customer_id\",\n        \"date_due\",\n        \"date_issue\",\n        \"document_id\",\n        \"payment_reference\",\n        \"payment_terms\",\n        \"vendor_name\",\n    }\n    assert len(doc.annotation.page_fields(0)) == 11\n    assert doc.annotation.page_fields(1) == []\n\n    assert len(doc.annotation.li_fields) == 40\n    assert {f.line_item_id for f in doc.annotation.li_fields} == set(range(1, 9))\n    assert len(doc.annotation.page_li_fields(0)) == 40\n\n    assert len(doc.annotation.li_headers) == 7\n    assert {f.line_item_id for f in doc.annotation.li_headers} == {0}\n    assert len(doc.annotation.page_li_headers(0)) == 7", "\n\ndef test_document_metadata_getters(sample_dataset: Dataset, sample_dataset_docid: str) -> None:\n    doc = sample_dataset[sample_dataset_docid]\n    assert doc.annotation.page_count == 1\n    assert doc.annotation.cluster_id == 554\n    assert doc.annotation.page_image_size_at_200dpi(0) == [1692, 2245]\n    assert doc.annotation.document_type == \"tax_invoice\"\n    assert doc.annotation.currency == \"other\"\n    assert doc.annotation.language == \"eng\"\n    assert doc.annotation.source == \"ucsf\"\n    assert doc.annotation.original_filename == \"nkvc0055\"", "\n\ndef test_document_annotation_get_table_grid(\n    sample_dataset: Dataset, sample_dataset_docid: str\n) -> None:\n    doc = sample_dataset[sample_dataset_docid]\n    grid = doc.annotation.get_table_grid(page=0)\n    assert grid is not None\n    assert grid.bbox == BBox(\n        left=133 / 1240, top=579 / 1645, right=1132 / 1240, bottom=1423 / 1645\n    )\n\n    assert len(grid.rows_bbox_with_type) == 17\n    assert all(\n        row[0].left == grid.bbox.left and row[0].right == grid.bbox.right\n        for row in grid.rows_bbox_with_type\n    )\n    assert {row[1] for row in grid.rows_bbox_with_type} == {\n        \"header\",\n        \"gap\",\n        \"data\",\n        \"gap-with-text\",\n        \"footer\",\n    }\n\n    assert len(grid.columns_bbox_with_type) == 8\n    assert all(\n        column[0].top == grid.bbox.top and column[0].bottom == grid.bbox.bottom\n        for column in grid.columns_bbox_with_type\n    )\n    assert [column[1] for column in grid.columns_bbox_with_type] == [\n        \"line_item_quantity\",\n        \"line_item_code\",\n        \"line_item_description\",\n        \"line_item_quantity\",\n        \"\",\n        \"line_item_quantity\",\n        \"line_item_unit_price_gross\",\n        \"line_item_amount_gross\",\n    ]\n\n    assert not grid.missing_columns\n    assert not grid.missing_second_table_on_page\n\n    assert grid.table_border_type == \"column_borders\"\n    assert grid.table_structure == \"normal\"\n\n    assert doc.annotation.get_table_grid(page=1) is None", ""]}
{"filename": "tests/dataset/test_dataset.py", "chunked_list": ["from pathlib import Path\n\nimport pytest\n\nfrom docile.dataset import Dataset, Document\n\n\ndef test_dataset_init(sample_dataset_docid: str, sample_dataset_path: Path) -> None:\n    dataset = Dataset(\"dev\", sample_dataset_path)\n    assert len(dataset) == len(list(dataset)) == 1  # type: ignore\n    assert isinstance(dataset[0], Document)\n    assert isinstance(dataset[sample_dataset_docid], Document)\n\n    custom_dataset = Dataset(\n        \"non-existent-split\", sample_dataset_path, docids=[sample_dataset_docid]\n    )\n    assert custom_dataset.docids == [sample_dataset_docid]\n    # It is possible to give both the index and list of docids if they agree with each other\n    assert (\n        dataset.docids == Dataset(\"dev\", sample_dataset_path, docids=[sample_dataset_docid]).docids\n    )\n\n    with pytest.raises(ValueError):\n        Dataset(\"dev\", sample_dataset_path, docids=[\"different-docid\"])\n    with pytest.raises(ValueError):\n        Dataset(\"non-existent-split\", sample_dataset_path)", ""]}
{"filename": "baselines/__init__.py", "chunked_list": [""]}
{"filename": "baselines/NER/my_lilt_multilabel.py", "chunked_list": ["from typing import Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.bert.modeling_bert import _CONFIG_FOR_DOC\n# from transformers.models.xlm_roberta import XLMRobertaModel\n# from transformers.models.xlm_roberta.modeling_xlm_roberta import (", "# from transformers.models.xlm_roberta import XLMRobertaModel\n# from transformers.models.xlm_roberta.modeling_xlm_roberta import (\n#     XLM_ROBERTA_INPUTS_DOCSTRING,\n#     XLM_ROBERTA_START_DOCSTRING,\n#     XLMRobertaPreTrainedModel,\n# )\nfrom transformers.models.lilt.modeling_lilt import (\n  LILT_INPUTS_DOCSTRING,\n  LILT_START_DOCSTRING,\n  LILT_PRETRAINED_MODEL_ARCHIVE_LIST,", "  LILT_START_DOCSTRING,\n  LILT_PRETRAINED_MODEL_ARCHIVE_LIST,\n  LiltPreTrainedModel,\n)\nfrom transformers.models.lilt import LiltModel\nfrom transformers.utils import (\n    add_code_sample_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n)", "    add_start_docstrings_to_model_forward,\n)\n\n\nclass MyLiltClassificationHead(nn.Module):\n    \"\"\"Head for sentence-level classification tasks.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        classifier_dropout = (\n            config.classifier_dropout\n            if config.classifier_dropout is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        # x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x", "\n\n@add_start_docstrings(\n    \"\"\"\n    XLM-RoBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.\n    for Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    LILT_START_DOCSTRING,\n)\n# Copied from transformers.models.roberta.modeling_roberta.RobertaForTokenClassification with Roberta->XLMRoberta, ROBERTA->XLM_ROBERTA\nclass MyLiltForTokenClassification(LiltPreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.lilt = LiltModel(config, add_pooling_layer=False)\n        classifier_dropout = (\n            config.classifier_dropout\n            if config.classifier_dropout is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n\n        self.use_2d_positional_embeddings = config.use_2d_positional_embeddings\n        self.use_1d_positional_embeddings = config.use_1d_positional_embeddings\n        self.use_2d_concat = config.use_2d_concat\n        self.use_new_2D_pos_emb = config.use_new_2D_pos_emb\n\n        self.quant_step_size = config.quant_step_size\n\n        if config.use_2d_positional_embeddings and config.use_2d_concat:\n            bb_emb_dim = config.bb_emb_dim\n            self.bb_left_emb = nn.Embedding(bb_emb_dim, config.hidden_size // 4)\n            self.bb_top_emb = nn.Embedding(bb_emb_dim, config.hidden_size // 4)\n            self.bb_right_emb = nn.Embedding(bb_emb_dim, config.hidden_size // 4)\n            self.bb_bottom_emb = nn.Embedding(bb_emb_dim, config.hidden_size // 4)\n        elif config.use_2d_positional_embeddings:\n            bb_emb_dim = config.bb_emb_dim\n            self.bb_left_emb = nn.Embedding(bb_emb_dim, config.hidden_size)\n            self.bb_top_emb = nn.Embedding(bb_emb_dim, config.hidden_size)\n            self.bb_right_emb = nn.Embedding(bb_emb_dim, config.hidden_size)\n            self.bb_bottom_emb = nn.Embedding(bb_emb_dim, config.hidden_size)\n        elif config.use_new_2D_pos_emb:\n            pos_emb_dim = int(config.pos_emb_dim / config.quant_step_size)\n            self.pos2_cx_emb = nn.Embedding(pos_emb_dim, config.hidden_size)\n            self.pos2_cy_emb = nn.Embedding(pos_emb_dim, config.hidden_size)\n            self.pos2_w_emb = nn.Embedding(pos_emb_dim, config.hidden_size)\n            self.pos2_h_emb = nn.Embedding(pos_emb_dim, config.hidden_size)\n\n        if config.use_1d_positional_embeddings:\n            self.pos_emb = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n\n        try:\n            self.use_classification_head = config.use_classification_head\n        except Exception:\n            self.use_classification_head = False\n\n        if self.use_classification_head:\n            self.classifier = MyLiltClassificationHead(config)\n        else:\n            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(\n        LILT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\")\n    )\n    @add_code_sample_docstrings(\n        checkpoint=\"nielsr/lilt-xlm-roberta-base\",\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"['O', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'LOC']\",\n        expected_loss=0.01,\n    )\n    def forward(\n        self,\n        bboxes: Optional[torch.Tensor] = None,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.lilt(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if self.use_1d_positional_embeddings:\n            input_shape = input_ids.size()\n            seq_length = input_shape[1]\n            if position_ids is None:\n                position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n                position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n            position_emb = self.pos_emb(position_ids)\n\n        if self.use_2d_positional_embeddings:\n            # create embeddings (each coordinate separate)\n            # bb_left_emb = self.bb_left_emb(bboxes[:, 0])\n            bb_left_emb = self.bb_left_emb(bboxes[:, :, 0])\n            # bb_top_emb = self.bb_top_emb(bboxes[:, 1])\n            bb_top_emb = self.bb_top_emb(bboxes[:, :, 1])\n            # bb_right_emb = self.bb_right_emb(bboxes[:, 2])\n            bb_right_emb = self.bb_right_emb(bboxes[:, :, 2])\n            # bb_bottom_emb = self.bb_bottom_emb(bboxes[:, 3])\n            bb_bottom_emb = self.bb_bottom_emb(bboxes[:, :, 3])\n            # n_rep = outputs[0].shape[1]\n\n            # final bbox embedding is a sum of all coordinate embeddings\n            # bbox_embedding = bb_top_emb.unsqueeze(1).expand(-1, n_rep, -1) + bb_left_emb.unsqueeze(1).expand(-1, n_rep, -1) + bb_bottom_emb.unsqueeze(1).expand(-1, n_rep, -1) + bb_right_emb.unsqueeze(1).expand(-1, n_rep, -1)\n            # bbox_embedding = bb_top_emb + bb_left_emb + bb_bottom_emb + bb_right_emb\n            if self.use_2d_concat:\n                bbox_embedding = torch.cat(\n                    [bb_top_emb, bb_left_emb, bb_bottom_emb, bb_right_emb], dim=-1\n                )\n            else:\n                bbox_embedding = bb_top_emb + bb_left_emb + bb_bottom_emb + bb_right_emb\n\n        if self.use_new_2D_pos_emb:\n            l = bboxes[:, :, 0]  # noqa: E741\n            t = bboxes[:, :, 1]\n            r = bboxes[:, :, 2]\n            b = bboxes[:, :, 3]\n            cx = (l + r) / 2\n            cy = (t + b) / 2\n            w = r - l\n            h = b - t\n            pos2_cx_emb = self.pos2_cx_emb((cx / self.quant_step_size + 0.5).int())\n            pos2_cy_emb = self.pos2_cy_emb((cy / self.quant_step_size + 0.5).int())\n            pos2_w_emb = self.pos2_w_emb((w / self.quant_step_size + 0.5).int())\n            pos2_h_emb = self.pos2_h_emb((h / self.quant_step_size + 0.5).int())\n\n            pos2_emb = pos2_cx_emb + pos2_cy_emb + pos2_w_emb + pos2_h_emb\n\n        sequence_output = outputs[0]\n\n        # add 1D positional embedding\n        if self.use_1d_positional_embeddings:\n            sequence_output += position_emb\n\n        # add 2D positional embedding\n        if self.use_2d_positional_embeddings:\n            sequence_output += bbox_embedding\n\n        if self.use_new_2D_pos_emb:\n            sequence_output += pos2_emb\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels.float())\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ")\n# Copied from transformers.models.roberta.modeling_roberta.RobertaForTokenClassification with Roberta->XLMRoberta, ROBERTA->XLM_ROBERTA\nclass MyLiltForTokenClassification(LiltPreTrainedModel):\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.lilt = LiltModel(config, add_pooling_layer=False)\n        classifier_dropout = (\n            config.classifier_dropout\n            if config.classifier_dropout is not None\n            else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\n\n        self.use_2d_positional_embeddings = config.use_2d_positional_embeddings\n        self.use_1d_positional_embeddings = config.use_1d_positional_embeddings\n        self.use_2d_concat = config.use_2d_concat\n        self.use_new_2D_pos_emb = config.use_new_2D_pos_emb\n\n        self.quant_step_size = config.quant_step_size\n\n        if config.use_2d_positional_embeddings and config.use_2d_concat:\n            bb_emb_dim = config.bb_emb_dim\n            self.bb_left_emb = nn.Embedding(bb_emb_dim, config.hidden_size // 4)\n            self.bb_top_emb = nn.Embedding(bb_emb_dim, config.hidden_size // 4)\n            self.bb_right_emb = nn.Embedding(bb_emb_dim, config.hidden_size // 4)\n            self.bb_bottom_emb = nn.Embedding(bb_emb_dim, config.hidden_size // 4)\n        elif config.use_2d_positional_embeddings:\n            bb_emb_dim = config.bb_emb_dim\n            self.bb_left_emb = nn.Embedding(bb_emb_dim, config.hidden_size)\n            self.bb_top_emb = nn.Embedding(bb_emb_dim, config.hidden_size)\n            self.bb_right_emb = nn.Embedding(bb_emb_dim, config.hidden_size)\n            self.bb_bottom_emb = nn.Embedding(bb_emb_dim, config.hidden_size)\n        elif config.use_new_2D_pos_emb:\n            pos_emb_dim = int(config.pos_emb_dim / config.quant_step_size)\n            self.pos2_cx_emb = nn.Embedding(pos_emb_dim, config.hidden_size)\n            self.pos2_cy_emb = nn.Embedding(pos_emb_dim, config.hidden_size)\n            self.pos2_w_emb = nn.Embedding(pos_emb_dim, config.hidden_size)\n            self.pos2_h_emb = nn.Embedding(pos_emb_dim, config.hidden_size)\n\n        if config.use_1d_positional_embeddings:\n            self.pos_emb = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n\n        try:\n            self.use_classification_head = config.use_classification_head\n        except Exception:\n            self.use_classification_head = False\n\n        if self.use_classification_head:\n            self.classifier = MyLiltClassificationHead(config)\n        else:\n            self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    @add_start_docstrings_to_model_forward(\n        LILT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\")\n    )\n    @add_code_sample_docstrings(\n        checkpoint=\"nielsr/lilt-xlm-roberta-base\",\n        output_type=TokenClassifierOutput,\n        config_class=_CONFIG_FOR_DOC,\n        expected_output=\"['O', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'LOC']\",\n        expected_loss=0.01,\n    )\n    def forward(\n        self,\n        bboxes: Optional[torch.Tensor] = None,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        token_type_ids: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.lilt(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if self.use_1d_positional_embeddings:\n            input_shape = input_ids.size()\n            seq_length = input_shape[1]\n            if position_ids is None:\n                position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n                position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n            position_emb = self.pos_emb(position_ids)\n\n        if self.use_2d_positional_embeddings:\n            # create embeddings (each coordinate separate)\n            # bb_left_emb = self.bb_left_emb(bboxes[:, 0])\n            bb_left_emb = self.bb_left_emb(bboxes[:, :, 0])\n            # bb_top_emb = self.bb_top_emb(bboxes[:, 1])\n            bb_top_emb = self.bb_top_emb(bboxes[:, :, 1])\n            # bb_right_emb = self.bb_right_emb(bboxes[:, 2])\n            bb_right_emb = self.bb_right_emb(bboxes[:, :, 2])\n            # bb_bottom_emb = self.bb_bottom_emb(bboxes[:, 3])\n            bb_bottom_emb = self.bb_bottom_emb(bboxes[:, :, 3])\n            # n_rep = outputs[0].shape[1]\n\n            # final bbox embedding is a sum of all coordinate embeddings\n            # bbox_embedding = bb_top_emb.unsqueeze(1).expand(-1, n_rep, -1) + bb_left_emb.unsqueeze(1).expand(-1, n_rep, -1) + bb_bottom_emb.unsqueeze(1).expand(-1, n_rep, -1) + bb_right_emb.unsqueeze(1).expand(-1, n_rep, -1)\n            # bbox_embedding = bb_top_emb + bb_left_emb + bb_bottom_emb + bb_right_emb\n            if self.use_2d_concat:\n                bbox_embedding = torch.cat(\n                    [bb_top_emb, bb_left_emb, bb_bottom_emb, bb_right_emb], dim=-1\n                )\n            else:\n                bbox_embedding = bb_top_emb + bb_left_emb + bb_bottom_emb + bb_right_emb\n\n        if self.use_new_2D_pos_emb:\n            l = bboxes[:, :, 0]  # noqa: E741\n            t = bboxes[:, :, 1]\n            r = bboxes[:, :, 2]\n            b = bboxes[:, :, 3]\n            cx = (l + r) / 2\n            cy = (t + b) / 2\n            w = r - l\n            h = b - t\n            pos2_cx_emb = self.pos2_cx_emb((cx / self.quant_step_size + 0.5).int())\n            pos2_cy_emb = self.pos2_cy_emb((cy / self.quant_step_size + 0.5).int())\n            pos2_w_emb = self.pos2_w_emb((w / self.quant_step_size + 0.5).int())\n            pos2_h_emb = self.pos2_h_emb((h / self.quant_step_size + 0.5).int())\n\n            pos2_emb = pos2_cx_emb + pos2_cy_emb + pos2_w_emb + pos2_h_emb\n\n        sequence_output = outputs[0]\n\n        # add 1D positional embedding\n        if self.use_1d_positional_embeddings:\n            sequence_output += position_emb\n\n        # add 2D positional embedding\n        if self.use_2d_positional_embeddings:\n            sequence_output += bbox_embedding\n\n        if self.use_new_2D_pos_emb:\n            sequence_output += pos2_emb\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels.float())\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )", ""]}
{"filename": "baselines/NER/docile_train_NER_multilabel.py", "chunked_list": ["import argparse\nimport dataclasses\nimport json\nimport math\nimport os\nfrom bisect import bisect_left, bisect_right\nfrom datetime import datetime\nfrom pathlib import Path\n\nfrom tensorboard import program", "\nfrom tensorboard import program\nimport sys\nsys.path.append('.')\n\nimport numpy as np\nimport torch\nimport torchmetrics\nfrom data_collator import MyMLDataCollatorForTokenClassification\nfrom datasets import Dataset as ArrowDataset", "from data_collator import MyMLDataCollatorForTokenClassification\nfrom datasets import Dataset as ArrowDataset\nfrom datasets import concatenate_datasets\nfrom helpers import FieldWithGroups, show_summary\nfrom my_lilt_multilabel import MyLiltForTokenClassification\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers.models.lilt.configuration_lilt import LiltConfig\nfrom docile.dataset import KILE_FIELDTYPES, LIR_FIELDTYPES, Dataset\nfrom torch.optim.lr_scheduler import StepLR", "from docile.dataset import KILE_FIELDTYPES, LIR_FIELDTYPES, Dataset\nfrom torch.optim.lr_scheduler import StepLR\nimport sys\n\n# import torch.optim.lr_scheduler.StepLR\n# class SAM(torch.optim.Optimizer):\n#     def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n#         assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n#         defaults = dict(rho=rho, adaptive=adaptive, **kwargs)", "\n#         defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n#         super(SAM, self).__init__(params, defaults)\n\n#         self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n#         self.param_groups = self.base_optimizer.param_groups\n#         self.defaults.update(self.base_optimizer.defaults)\n\n#     @torch.no_grad()\n#     def first_step(self, zero_grad=False):", "#     @torch.no_grad()\n#     def first_step(self, zero_grad=False):\n#         grad_norm = self._grad_norm()\n#         for group in self.param_groups:\n#             scale = group[\"rho\"] / (grad_norm + 1e-12)\n\n#             for p in group[\"params\"]:\n#                 if p.grad is None: continue\n#                 self.state[p][\"old_p\"] = p.data.clone()\n#                 e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)", "#                 self.state[p][\"old_p\"] = p.data.clone()\n#                 e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n#                 p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n\n#         if zero_grad: self.zero_grad()\n\n#     @torch.no_grad()\n#     def second_step(self, zero_grad=False):\n#         for group in self.param_groups:\n#             for p in group[\"params\"]:", "#         for group in self.param_groups:\n#             for p in group[\"params\"]:\n#                 if p.grad is None: continue\n#                 p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n\n#         self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n#         if zero_grad: self.zero_grad()\n        \n    ", "        \n    \n#     @torch.no_grad()\n#     def step(self, closure=None):\n#         assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n#         closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n        \n#         self.first_step(zero_grad=True)\n#         closure()\n#         self.second_step()", "#         closure()\n#         self.second_step()\n\n#     def _grad_norm(self):\n#         shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n#         norm = torch.norm(\n#                     torch.stack([\n#                         ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n#                         for group in self.param_groups for p in group[\"params\"]\n#                         if p.grad is not None", "#                         for group in self.param_groups for p in group[\"params\"]\n#                         if p.grad is not None\n#                     ]),\n#                     p=2\n#                )\n#         return norm\n\n#     def load_state_dict(self, state_dict):\n#         super().load_state_dict(state_dict)\n#         self.base_optimizer.param_groups = self.param_groups", "#         super().load_state_dict(state_dict)\n#         self.base_optimizer.param_groups = self.param_groups\n\nclasses = (\n    [\n        # 'background',\n        # special background classes\n        \"KILE_background\",\n        \"LI_background\",\n        \"LIR_background\",", "        \"LI_background\",\n        \"LIR_background\",\n        # LI class\n        # 'LI',  #  NOTE: will be added separately to unique_entities\n    ]\n    + KILE_FIELDTYPES\n    + LIR_FIELDTYPES\n)\n\n\ndef tag_fields_with_entities(fields, unique_entities=[]):  # noqa: B006\n    # assumes that tokens are FieldLabels and already sorted (by text lines, i.e. vertically) and horizontaly (by x-axis)\n    # hash map for determining entity type (B, I)\n    if len(unique_entities) < 1:\n        entity_map = {x: False for x in classes}\n    else:\n        entity_map = {x[2:]: False for x in unique_entities}\n\n    tokens_with_entities = []\n\n    prev_lid = None\n\n    for token, next_token in zip(fields, fields[1:] + [None]):\n        fts = token.fieldtype if isinstance(token.fieldtype, list) else None\n        lid = token.line_item_id\n        next_lid = None if next_token is None else next_token.line_item_id\n\n        other = (token.bbox.to_tuple(),)\n        labels = []\n\n        if fts:\n            for ft in fts:\n                if entity_map[ft]:\n                    labels.append(f\"I-{ft}\")\n                else:\n                    labels.append(f\"B-{ft}\")\n                    entity_map[ft] = True\n        else:\n            labels.extend([\"O-KILE\", \"O-LIR\"])\n\n        if lid is not None:\n            li_labels = []\n            if prev_lid is None or prev_lid != lid:\n                li_labels.append(\"B-LI\")\n            if next_lid is None or next_lid != lid:\n                li_labels.append(\"E-LI\")\n            if li_labels == []:\n                li_labels.append(\"I-LI\")\n            labels.extend(li_labels)\n        else:\n            labels.append(\"O-LI\")\n\n        tokens_with_entities.append((token.text, labels, other))\n\n        # reset line_item labels in entity_map if there is a transition to a different line_item\n        if prev_lid != lid:\n            for k in entity_map.keys():\n                if k.startswith(\"line_item_\"):\n                    entity_map[k] = False\n\n        prev_lid = lid\n\n    return tokens_with_entities", "\n\ndef tag_fields_with_entities(fields, unique_entities=[]):  # noqa: B006\n    # assumes that tokens are FieldLabels and already sorted (by text lines, i.e. vertically) and horizontaly (by x-axis)\n    # hash map for determining entity type (B, I)\n    if len(unique_entities) < 1:\n        entity_map = {x: False for x in classes}\n    else:\n        entity_map = {x[2:]: False for x in unique_entities}\n\n    tokens_with_entities = []\n\n    prev_lid = None\n\n    for token, next_token in zip(fields, fields[1:] + [None]):\n        fts = token.fieldtype if isinstance(token.fieldtype, list) else None\n        lid = token.line_item_id\n        next_lid = None if next_token is None else next_token.line_item_id\n\n        other = (token.bbox.to_tuple(),)\n        labels = []\n\n        if fts:\n            for ft in fts:\n                if entity_map[ft]:\n                    labels.append(f\"I-{ft}\")\n                else:\n                    labels.append(f\"B-{ft}\")\n                    entity_map[ft] = True\n        else:\n            labels.extend([\"O-KILE\", \"O-LIR\"])\n\n        if lid is not None:\n            li_labels = []\n            if prev_lid is None or prev_lid != lid:\n                li_labels.append(\"B-LI\")\n            if next_lid is None or next_lid != lid:\n                li_labels.append(\"E-LI\")\n            if li_labels == []:\n                li_labels.append(\"I-LI\")\n            labels.extend(li_labels)\n        else:\n            labels.append(\"O-LI\")\n\n        tokens_with_entities.append((token.text, labels, other))\n\n        # reset line_item labels in entity_map if there is a transition to a different line_item\n        if prev_lid != lid:\n            for k in entity_map.keys():\n                if k.startswith(\"line_item_\"):\n                    entity_map[k] = False\n\n        prev_lid = lid\n\n    return tokens_with_entities", "\n\nclass NERDataMaker:\n    # NER Data Maker class for processing Full Table\n    def __init__(self, data, metadata, unique_entities=None, use_BIO_format=True) -> None:\n        if not unique_entities:\n            have_unique_entities = False\n            self.unique_entities = []\n        else:\n            have_unique_entities = True\n            self.unique_entities = unique_entities\n        self.processed_tables = []\n\n        temp_processed_tables = []\n        for page_data in tqdm(data, desc=\"Processing tables 1/2\"):\n            tokens_with_entities = tag_fields_with_entities(page_data, self.unique_entities)\n            if tokens_with_entities:\n                if not have_unique_entities:\n                    for _, ent, _ in tokens_with_entities:\n                        if ent not in self.unique_entities:\n                            self.unique_entities.append(ent)\n                temp_processed_tables.append(tokens_with_entities)\n\n        if not have_unique_entities:\n            self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n\n        for tokens_with_entities in tqdm(temp_processed_tables, desc=\"Processing tables 2/2\"):\n            self.processed_tables.append(\n                [\n                    # (t, self.unique_entities.index(ent), info)\n                    (t, [self.unique_entities.index(e) for e in ent], info)\n                    for t, ent, info in tokens_with_entities\n                ]\n            )\n\n    @property\n    def id2label(self):\n        return dict(enumerate(self.unique_entities))\n\n    @property\n    def label2id(self):\n        return {v: k for k, v in self.id2label.items()}\n\n    def __len__(self):\n        return len(self.processed_tables)\n\n    def __getitem__(self, idx):\n        def _process_tokens_for_one_page(id, tokens_with_encoded_entities):\n            ner_tags = []\n            tokens = []\n            infos = []\n            # bboxes = []\n            for t, ent, info in tokens_with_encoded_entities:\n                ner_tags.append(ent)\n                tokens.append(t)\n                infos.append(info)\n\n            return {\n                \"id\": id,\n                \"ner_tags\": ner_tags,\n                \"tokens\": tokens,\n                \"infos\": infos,\n            }\n\n        tokens_with_encoded_entities = self.processed_tables[idx]\n        if isinstance(idx, int):\n            return _process_tokens_for_one_page(idx, tokens_with_encoded_entities)\n        else:\n            return [\n                _process_tokens_for_one_page(i + idx.start, tee)\n                for i, tee in enumerate(tokens_with_encoded_entities)\n            ]\n\n    def as_hf_dataset(self, tokenizer, tag_everything=False, stride=0):\n        from datasets import Array2D\n        from datasets import Dataset as ArrowDataset\n        from datasets import Features, Sequence, Value\n\n        def tokenize_and_align_labels_unbatched(examples):\n            tokenized_inputs = tokenizer(\n                examples[\"tokens\"],\n                is_split_into_words=True,\n                add_special_tokens=True,\n                truncation=True,\n                stride=stride,\n                padding=True,\n                max_length=512,\n                return_overflowing_tokens=True,  # important !!!\n                return_length=True,\n                verbose=True,\n            )\n\n            labels = []\n            bboxes = []\n\n            i = 0\n            label = examples[\"ner_tags\"]\n            bbox = examples[\"bboxes\"]\n            overflowing = tokenized_inputs[i].overflowing\n            for i in range(0, 1 + len(overflowing)):\n                word_ids = tokenized_inputs[i].word_ids\n                previous_word_idx = None\n                label_ids = []\n                bboxes_tmp = []\n                for word_idx in word_ids:  # Set the special tokens to -100.\n                    if word_idx is None:\n                        label_ids.append(np.zeros_like(label[0]))\n                        bboxes_tmp.append(np.array([0, 0, 0, 0], dtype=np.int32))\n                    elif (word_idx != previous_word_idx) or (tag_everything):\n                        label_ids.append(label[word_idx])\n                        bboxes_tmp.append(bbox[word_idx])\n                    else:\n                        label_ids.append(np.zeros_like(label[0]))\n                        bboxes_tmp.append(np.array([0, 0, 0, 0], dtype=np.int32))\n                    previous_word_idx = word_idx\n                labels.append(label_ids)\n                bboxes.append(bboxes_tmp)\n            tokenized_inputs[\"labels\"] = labels\n            tokenized_inputs[\"bboxes\"] = bboxes\n            return tokenized_inputs\n\n        ids, ner_tags, tokens, infos = [], [], [], []\n        bboxes = []\n\n        def make_labels(x, N):\n            tmp = np.zeros(N, dtype=bool)\n            tmp[x] = 1\n            return tmp\n\n        for i, pt in enumerate(self.processed_tables):\n            ids.append(i)\n            pt_tokens, pt_tags, pt_info = list(zip(*pt))\n            ner_tags.append(tuple([make_labels(x, len(self.unique_entities)) for x in pt_tags]))\n            tokens.append(pt_tokens)\n            infos.append(pt_info)\n            bboxes.append(\n                [np.array([d[0][0], d[0][1], d[0][2], d[0][3]], dtype=np.int32) for d in pt_info]\n            )\n        data = {\n            \"id\": ids,\n            \"ner_tags\": ner_tags,\n            \"tokens\": tokens,\n            \"bboxes\": bboxes,\n        }\n        features = Features(\n            {\n                \"tokens\": Sequence(Value(\"string\")),\n                \"ner_tags\": Array2D(shape=(None, len(self.unique_entities)), dtype=\"bool\"),\n                \"id\": Value(\"int32\"),\n                \"bboxes\": Array2D(shape=(None, 4), dtype=\"int32\"),\n            }\n        )\n        ds = ArrowDataset.from_dict(data, features)\n        tokenized_ds = ds.map(tokenize_and_align_labels_unbatched, batched=False)\n        return tokenized_ds", "\n\ndef get_center_line_clusters(line_item):\n    # get centers of text boxes (y-axis only)\n    centers = np.array([x.bbox.centroid[1] for x in line_item])\n    heights = np.array([x.bbox.height for x in line_item])\n\n    n_bins = len(centers)\n    if n_bins < 1:\n        return {}\n\n    hist_h, bin_edges_h = np.histogram(heights, bins=n_bins)\n    bin_centers_h = bin_edges_h[:-1] + np.diff(bin_edges_h) / 2\n    idxs_h = np.where(hist_h)[0]\n    heights_cluster_centers = np.unique(bin_centers_h[idxs_h].astype(np.int32))\n    heights_cluster_centers.sort()\n\n    # group text boxes by heights\n    groups_heights = {}\n    for field in line_item:\n        g = np.array(\n            list(map(lambda height: np.abs(field.bbox.height - height), heights_cluster_centers))\n        ).argmin()\n        gid = heights_cluster_centers[g]\n        if gid not in groups_heights:\n            groups_heights[gid] = [field]\n        else:\n            groups_heights[gid].append(field)\n\n    hist, bin_edges = np.histogram(centers, bins=n_bins)\n    bin_centers = bin_edges[:-1] + np.diff(bin_edges) / 2\n    idxs = np.where(hist)[0]\n    y_center_clusters = bin_centers[idxs]\n    y_center_clusters.sort()\n    line_item_height = y_center_clusters.max() - y_center_clusters.min()\n\n    if line_item_height < heights_cluster_centers[0]:\n        # there is probably just 1 cluster\n        return {0: y_center_clusters.mean()}\n    else:\n        #  estimate the number of lines by looking at the cluster centers\n        clusters = {}\n        cnt = 0\n        yc_prev = y_center_clusters[0]\n        for yc in y_center_clusters:\n            if np.abs(yc_prev - yc) < heights_cluster_centers[0]:\n                flag = True\n            else:\n                flag = False\n            if flag:\n                if cnt not in clusters:\n                    clusters[cnt] = [yc]\n                else:\n                    clusters[cnt].append(yc)\n            else:\n                cnt += 1\n                clusters[cnt] = [yc]\n            yc_prev = yc\n        for k, v in clusters.items():\n            clusters[k] = np.array(v).mean()\n    return clusters", "\n\ndef split_fields_by_text_lines(line_item):\n    clusters = get_center_line_clusters(line_item)\n    new_line_item = []\n    for ft in line_item:\n        g = np.array(\n            # list(map(lambda y: (ft.bbox.centroid[1] - y) ** 2, clusters.values()))\n            list(map(lambda y: np.abs(ft.bbox.to_tuple()[1] - y), clusters.values()))\n        ).argmin()\n        updated_ft = dataclasses.replace(ft, groups=[g])\n        new_line_item.append(updated_ft)\n    return new_line_item, clusters", "\n\ndef get_sorted_field_candidates(original_fields):\n    fields = []\n    # for lid, line_item in original_fields.items():\n\n    # clustering of text boxes in a given line item into individual text lines (stored in fieldlabel.groups)\n    # line_item, clusters = split_fields_by_text_lines(line_item)\n    line_item, clusters = split_fields_by_text_lines(original_fields)\n\n    # sort text boxes by\n    line_item.sort(key=lambda x: x.groups)\n\n    # group by lines:\n    groups = {}\n    for ft in line_item:\n        gid = str(ft.groups)\n        if gid not in groups.keys():\n            groups[gid] = [ft]\n        else:\n            groups[gid].append(ft)\n\n    # lid_str = f\"{lid:04d}\" if lid else \"-001\"\n\n    for gid, fs in groups.items():\n        # sort by x-axis (since we are dealing with a single line)\n        fs.sort(key=lambda x: x.bbox.centroid[0])\n        for f in fs:\n            lid_str = f\"{f.line_item_id:04d}\" if f.line_item_id else \"-001\"\n            updated_f = dataclasses.replace(\n                f,\n                # groups = [f\"{lid:04d}{int(gid.strip('[]')):>04d}\"]\n                groups=[f\"{lid_str}{int(gid.strip('[]')):>04d}\"],\n            )\n            fields.append(updated_f)\n    return fields, clusters", "\n\ndef get_data_from_docile(dataset, overlap_thr=0.5):\n    data = []\n    metadata = []\n\n    for document in tqdm(dataset, desc=f\"Generating data from {dataset}\"):\n        doc_id = document.docid\n        # page_to_table_grids = document.annotation.content[\"metadata\"][\"page_to_table_grids\"]\n\n        kile_fields = [\n            FieldWithGroups.from_dict(field.to_dict()) for field in document.annotation.fields\n        ]\n        li_fields = [\n            FieldWithGroups.from_dict(field.to_dict()) for field in document.annotation.li_fields\n        ]\n        for page in range(document.page_count):\n            img = document.page_image(page)\n            W, H = img.size\n            kile_fields_page = [field for field in kile_fields if field.page == page]\n            li_fields_page = [field for field in li_fields if field.page == page]\n            kile_fields_page = [\n                dataclasses.replace(field, bbox=field.bbox.to_absolute_coords(W, H))\n                for field in kile_fields_page\n            ]\n            li_fields_page = [\n                dataclasses.replace(field, bbox=field.bbox.to_absolute_coords(W, H))\n                for field in li_fields_page\n            ]\n\n            ocr = [\n                FieldWithGroups.from_dict(word.to_dict())\n                for word in document.ocr.get_all_words(page, snapped=True)\n            ]\n            ocr = [\n                dataclasses.replace(\n                    ocr_field, bbox=ocr_field.bbox.to_absolute_coords(W, H), fieldtype=[]\n                )\n                for ocr_field in ocr\n            ]\n\n            # 0. Get table grid\n            table_grid = document.annotation.get_table_grid(page)\n            tables_bbox = table_grid.bbox.to_absolute_coords(W, H) if table_grid else None\n\n            # 1. Tag ocr fields with fieldtypes from kile_fields + li_fields\n            # We sort the kile+lir fields by top coordinate and then for each ocr field we performr\n            # binary search to find only the kile+lir fields overlapping vertically.\n            # Note: original index is kept to preserve original behaviour.\n            kile_li_fields_page = kile_fields_page + li_fields_page\n            kile_li_fields_page_sorted = sorted(\n                enumerate(kile_li_fields_page),\n                key=lambda i_f: i_f[1].bbox.top,\n            )\n            fields_top_coords = [field.bbox.top for _, field in kile_li_fields_page_sorted]\n            # Max bottom coordinate is needed to have a sorted array for binary search. This means\n            # some extra fields will be included in the found range, causing a very minor slowdown.\n            fields_bottom_coords_max = [\n                field.bbox.bottom for _, field in kile_li_fields_page_sorted\n            ]\n            for i in range(1, len(kile_li_fields_page_sorted)):\n                fields_bottom_coords_max[i] = max(\n                    fields_bottom_coords_max[i],\n                    fields_bottom_coords_max[i - 1],\n                )\n            # Indexes to original kile_li_fields_page array\n            fields_idxs = [idx for idx, _ in kile_li_fields_page_sorted]\n\n            updated_ocr = []\n            for ocr_field in ocr:\n                new_ocr_field = dataclasses.replace(ocr_field, groups=\"\")\n                # take only fields with bottom coord after ocr_field.bbox.top\n                i_l = bisect_right(fields_bottom_coords_max, ocr_field.bbox.top)\n                # take only fields with top coord before ocr_field.bbox.bottom\n                i_r = bisect_left(fields_top_coords, ocr_field.bbox.bottom)\n                for idx in sorted(fields_idxs[i_l:i_r]):\n                    field = kile_li_fields_page[idx]\n                    if ocr_field.bbox and field.bbox:\n                        if (\n                            field.bbox.intersection(ocr_field.bbox).area / ocr_field.bbox.area\n                            >= overlap_thr\n                        ):\n                            if field.fieldtype not in ocr_field.fieldtype:\n                                new_ocr_field.fieldtype.append(field.fieldtype)\n                            new_ocr_field = dataclasses.replace(\n                                new_ocr_field, line_item_id=field.line_item_id\n                            )\n                updated_ocr.append(new_ocr_field)\n            ocr = updated_ocr\n\n            # Re-Order OCR boxes\n            sorted_fields, _ = get_sorted_field_candidates(ocr)\n\n            tables_ocr = []\n            if tables_bbox:\n                for i, field in enumerate(sorted_fields):\n                    if tables_bbox.intersection(field.bbox).area / field.bbox.area >= overlap_thr:\n                        tables_ocr.append((i, field))\n\n            # # 2. Split into individual lines, group by line item id\n            # for table_i, table_fields in enumerate(tables_ocr):\n            text_lines = {}\n            # for field in page_fields:\n            for i_field, field in tables_ocr:\n                gid = field.groups[0][4:]\n                if gid not in text_lines:\n                    text_lines[gid] = [(i_field, field)]\n                else:\n                    text_lines[gid].append((i_field, field))\n            # now there should be only 1 line_item_id (or first 04d in groups) per each text_lines\n            # we need to merge text_lines, if there are several of them assigned to the same line_item_id\n            line_items = {}\n            # prev_id = 0 + 1000*table_i\n            prev_id = 0 + 1000 * page\n            for _, fields in text_lines.items():\n                line_item_ids = [f.line_item_id for _i, f in fields if f.line_item_id is not None]\n                prev_id = line_item_ids[0] if line_item_ids else prev_id\n                if prev_id not in line_items:\n                    line_items[prev_id] = fields\n                else:\n                    line_items[prev_id].extend(fields)\n            # 3. Append to data, which will be then used to construct NER Dataset\n            for lid, fields in line_items.items():\n                if lid > 0:\n                    for i_field, field in fields:\n                        gid = field.groups[0]\n                        new_field = dataclasses.replace(\n                            field, line_item_id=lid, groups=[f\"{lid:04d}{gid[4:]}\"]\n                        )\n                        sorted_fields[i_field] = new_field\n\n            # append data and metadata\n            metadata.append(\n                {\n                    \"i\": len(data),\n                    \"doc_id\": doc_id,\n                    \"page_n\": page,\n                    # \"table_n\": table_i,\n                    # \"row_separators\": row_sep[table_i]\n                }\n            )\n            data.append(sorted_fields)\n    return data, metadata", "\n\ndef load_metadata(src: Path):\n    out = []\n    with open(src, \"r\") as json_file:\n        out = json.load(json_file)\n    return out\n\n\ndef store_metadata(dest: Path, metadata):\n    with open(dest, \"w\") as json_file:\n        json.dump(metadata, json_file)", "\ndef store_metadata(dest: Path, metadata):\n    with open(dest, \"w\") as json_file:\n        json.dump(metadata, json_file)\n\n\ndef load_data(src: Path):\n    out = []\n    with open(src, \"r\") as json_file:\n        A = json.load(json_file)\n    for table_data in A:\n        out.append([])\n        for field in table_data:\n            out[-1].append(FieldWithGroups.from_dict(field))\n    return out", "\n\ndef store_data(dest: Path, data):\n    out = []\n    for table_data in data:\n        out.append([])\n        for field in table_data:\n            out[-1].append(\n                {\n                    \"fieldtype\": field.fieldtype if field.fieldtype else \"background\",\n                    \"bbox\": field.bbox.to_tuple(),\n                    \"groups\": field.groups,\n                    \"line_item_id\": field.line_item_id,\n                    \"page\": field.page,\n                    \"score\": field.score,\n                    \"text\": field.text,\n                }\n            )\n    with open(dest, \"w\") as json_file:\n        json.dump(out, json_file)", "\n\ndef _arrow_dataset_path(arrow_format_path, docile_dataset):\n    return arrow_format_path / docile_dataset.split_name\n\n\ndef prepare_hf_dataset(\n    docile_dataset,\n    tokenizer,\n    overlap_thr,\n    arrow_format_path,\n    preprocessed_dataset_path,\n    chunk_size=100000,\n):\n    if len(docile_dataset) > chunk_size:\n        if not arrow_format_path:\n            raise NotImplementedError(\n                f\"You need to set --arrow-format-path because {docile_dataset} has more than 10000 documents\"\n            )\n        num_chunks = math.ceil(len(docile_dataset) / chunk_size)\n        dataset_chunks = []\n        for chunk in range(num_chunks):\n            chunk_dataset = docile_dataset[chunk * chunk_size : (chunk + 1) * chunk_size]\n            chunk_dataset.split_name = f\"{docile_dataset.split_name}_chunk_{chunk}_of_{num_chunks}\"\n            # make sure the chunk is stored to disk\n            prepare_hf_dataset(\n                chunk_dataset,\n                tokenizer,\n                overlap_thr,\n                arrow_format_path,\n                preprocessed_dataset_path,\n                chunk_size,\n            )\n            # load it from disk\n            dataset_chunk = ArrowDataset.load_from_disk(\n                _arrow_dataset_path(arrow_format_path, chunk_dataset)\n            )\n            dataset_chunks.append(dataset_chunk)\n        return concatenate_datasets(dataset_chunks)\n\n    if arrow_format_path:\n        try:\n            load_path = _arrow_dataset_path(arrow_format_path, docile_dataset)\n            print(f\"Loading dataset in arrow format from path {load_path}\")\n            return ArrowDataset.load_from_disk(load_path)\n        except Exception:\n            print(f\"Could not load {docile_dataset.split_name} in arrow format, regenerating.\")\n\n    if preprocessed_dataset_path:\n        dataset_name = docile_dataset.data_paths.name\n        preprocessed_path = preprocessed_dataset_path / dataset_name\n        print(\n            f\"Loading preprocessed {docile_dataset.split_name} data from path {preprocessed_path}\"\n        )\n        try:\n            data = load_data(\n                preprocessed_path / f\"{docile_dataset.split_name}_multilabel_preprocessed.json\"\n            )\n            metadata = load_metadata(\n                preprocessed_path / f\"{docile_dataset.split_name}_multilabel_metadata.json\"\n            )\n        except Exception:\n            print(f\"Could not load preprocessed {docile_dataset.split_name}, regenerating.\")\n            data, metadata = get_data_from_docile(docile_dataset, overlap_thr=overlap_thr)\n            print(\n                f\"Storing preprocessed {docile_dataset.split_name} data to {preprocessed_dataset_path}\"\n            )\n            os.makedirs(preprocessed_dataset_path / dataset_name, exist_ok=True)\n            store_data(\n                preprocessed_path / f\"{docile_dataset.split_name}_multilabel_preprocessed.json\",\n                data,\n            )\n            store_metadata(\n                preprocessed_path / f\"{docile_dataset.split_name}_multilabel_metadata.json\",\n                metadata,\n            )\n    else:\n        data, metadata = get_data_from_docile(docile_dataset, overlap_thr=overlap_thr)\n\n    data_maker = NERDataMaker(data, metadata, unique_entities=unique_entities, use_BIO_format=True)\n    print(\"Converting dataset to HuggingFace format\")\n    dataset = data_maker.as_hf_dataset(\n        tokenizer=tokenizer, stride=args.stride, tag_everything=args.tag_everything\n    )\n    if arrow_format_path:\n        store_path = _arrow_dataset_path(arrow_format_path, docile_dataset)\n        print(\n            f\"Storing HuggingFace Dataset for {docile_dataset.split_name} in arrow format to: {store_path}\"\n        )\n        dataset.save_to_disk(store_path)\n        print(f\"HuggingFace Dataset for {docile_dataset.split_name} in arrow format stored\")\n\n    return dataset", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--docile_path\",\n        type=Path,\n        default=Path(\"/app/data/docile/\"),\n    )\n    parser.add_argument(\n        \"--split\",\n        type=str,\n        default=\"train\",\n        # default=\"synthetic\",\n    )\n    parser.add_argument(\n        \"--overlap_thr\",\n        type=float,\n        default=0.5,\n    )\n    parser.add_argument(\"--output_dir\", type=str, help=\"Where to store the outputs.\")\n    parser.add_argument(\n        \"--train_bs\",\n        type=int,\n        default=8,\n        help=\"Training batch size\",\n    )\n    parser.add_argument(\n        \"--test_bs\",\n        type=int,\n        default=8,\n        help=\"Testing batch size\",\n    )\n    parser.add_argument(\n        \"--model_name\",\n        type=str,\n        default=\"bert-base-multilingual-cased\",\n        help=\"HuggingFace model to fine-tune\",\n    )\n    parser.add_argument(\n        \"--use_bert\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--use_lilt\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--save_total_limit\",\n        default=None,\n        type=int,\n        help=\"If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir\",\n    )\n    parser.add_argument(\n        \"--weight_decay\",\n        default=0.01,\n        type=float,\n        help=\"\",\n    )\n    parser.add_argument(\n        \"--num_epochs\",\n        default=40,\n        type=int,\n        help=\"\",\n    )\n    parser.add_argument(\n        \"--lr\",\n        default=2e-5,\n        type=float,\n        help=\"\",\n    )\n    parser.add_argument(\n        \"--use_BIO_format\",\n        action=\"store_true\",\n        help=\"\",\n    )\n    parser.add_argument(\n        \"--use_2d_positional_embeddings\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--use_1d_positional_embeddings\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--use_2d_concat\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--stride\",\n        default=0,\n        type=int,\n        help=\"Stride for tokenizer\",\n    )\n    parser.add_argument(\"--use_new_2D_pos_emb\", action=\"store_true\")\n    parser.add_argument(\n        \"--quant_step_size\",\n        type=int,\n        default=5,\n    )\n    parser.add_argument(\n        \"--pos_emb_dim\",\n        type=int,\n        default=2500,\n    )\n    parser.add_argument(\n        \"--tag_everything\",\n        action=\"store_true\",\n        help=\"If this is defined, all tokens will be tagged with class, unlike just the beginning of words\",\n    )\n    parser.add_argument(\n        \"--bb_emb_dim\",\n        type=int,\n        default=2500,\n    )\n    parser.add_argument(\n        \"--arrow_format_path\",\n        type=Path,\n        default=None,\n    )\n    parser.add_argument(\n        \"--preprocessed_dataset_path\",\n        type=Path,\n        default=None,\n    )\n    parser.add_argument(\n        \"--report_all_metrics\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"--warmup_ratio\",\n        type=float,\n        default=0.0,\n        help=\"\",\n    )\n    parser.add_argument(\n        \"--warmup_steps\",\n        type=int,\n        default=0,\n        help=\"\",\n    )\n    parser.add_argument(\n        \"--dataloader_num_workers\",\n        type=int,\n        default=0,\n        help=\"\",\n    )\n    parser.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n        help=\"\",\n    )\n    parser.add_argument(\n        \"--use_classification_head\",\n        action=\"store_true\",\n        help=\"Whether to use the more complex classification head instead of just a single dense layer\",\n    )\n    parser.add_argument(\"--resume\", action=\"store_true\")\n    args = parser.parse_args()\n\n    print(f\"{datetime.now()} Started.\")\n\n    show_summary(args, __file__)\n\n    output_dir_exists = os.path.exists(args.output_dir) and os.listdir(args.output_dir) != [\n        \"log_train.txt\"\n    ]\n    if output_dir_exists and not args.resume:\n        raise ValueError(\n            f\"Output dir {args.output_dir} already exists, delete it or use --resume.\"\n        )\n    if not output_dir_exists and args.resume:\n        raise ValueError(\n            f\"--resume was used but output dir {args.output_dir} is empty (apart from log file)\"\n        )\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    # Check GPU availability\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n\n    if \"checkpoint\" in args.model_name or \"so_far_best\" in args.model_name:\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(\n            args.model_name,\n            add_prefix_space=True if (args.model_name == \"lilt-base\") else False,\n        )\n\n    data_collator = MyMLDataCollatorForTokenClassification(\n        tokenizer=tokenizer, max_length=512, padding=\"longest\"\n    )\n\n    if args.use_BIO_format:\n        # add complete background ()\n        # unique_entities = [\"O\"]\n        unique_entities = []\n        # add class specific background tags\n        # unique_entities.extend([f\"O-{x.rstrip('_background')}\" for x in classes[1:] if \"background\" in x])\n        unique_entities.extend(\n            [f\"O-{x.rstrip('_background')}\" for x in classes if \"background\" in x]\n        )\n        # add KILE and LIR class tags\n        unique_entities.extend([f\"B-{x}\" for x in classes if \"background\" not in x])\n        unique_entities.extend([f\"I-{x}\" for x in classes if \"background\" not in x])\n        # add tags for LI\n        unique_entities.extend([\"B-LI\", \"I-LI\", \"E-LI\"])\n        # sort them out\n        unique_entities.sort(key=lambda ent: ent if ent[0] != \"O\" else \"\")\n    else:\n        raise NotImplementedError(\"Not implemented yet.\")\n\n    id2label = dict(enumerate(unique_entities))\n    label2id = {v: k for k, v in id2label.items()}\n\n    val_docile_dataset = Dataset(\"val\", args.docile_path, load_annotations=False, load_ocr=False)\n    val_dataset = prepare_hf_dataset(\n        val_docile_dataset,\n        tokenizer,\n        args.overlap_thr,\n        args.arrow_format_path,\n        args.preprocessed_dataset_path,\n    )\n    train_docile_dataset = Dataset(\n        args.split, args.docile_path, load_annotations=False, load_ocr=False\n    )\n    train_dataset = prepare_hf_dataset(\n        train_docile_dataset,\n        tokenizer,\n        args.overlap_thr,\n        args.arrow_format_path,\n        args.preprocessed_dataset_path,\n    )\n\n    if args.use_lilt:\n        config = LiltConfig.from_pretrained(args.model_name)\n    else:\n        raise Exception(\"Unknown type for NLP backbone selected.\")\n\n    print(\"\\n\\n\\n\")\n\n    # instantiate model\n    if args.use_lilt:\n        config.use_2d_positional_embeddings = args.use_2d_positional_embeddings\n        config.use_1d_positional_embeddings = args.use_1d_positional_embeddings\n        config.use_new_2D_pos_emb = args.use_new_2D_pos_emb\n        config.pos_emb_dim = args.pos_emb_dim\n        config.quant_step_size = args.quant_step_size\n        config.stride = args.stride\n        config.bb_emb_dim = args.bb_emb_dim\n        config.tag_everything = args.tag_everything\n        config.num_labels = len(unique_entities)\n        config.id2label = id2label\n        config.label2id = label2id\n        config.model_name = args.model_name\n        config.use_bert = args.use_bert\n        config.use_lilt = args.use_lilt\n        config.use_BIO_format = args.use_BIO_format\n        config.use_2d_concat = args.use_2d_concat\n        config.use_classification_head = args.use_classification_head\n\n        model = MyLiltForTokenClassification.from_pretrained(\n            args.model_name, config=config\n        )\n\n    # base_optimizer = torch.optim.SGD\n    # optimizer = SAM(model.parameters(), base_optimizer, lr=2e-5, momentum=0.9)\n    # scheduler = StepLR(optimizer, args.lr, args.num_epochs)\n    training_args = TrainingArguments(\n        output_dir=os.path.join(args.output_dir),\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=args.lr,\n        per_device_train_batch_size=args.train_bs,\n        per_device_eval_batch_size=args.test_bs,\n        num_train_epochs=args.num_epochs,\n        weight_decay=args.weight_decay,\n        load_best_model_at_end=True,\n        report_to=\"tensorboard\",\n        save_total_limit=args.save_total_limit,\n        seed=42,\n        data_seed=42,\n        metric_for_best_model=\"OVERALL_f1\",\n        greater_is_better=True,\n        warmup_steps=args.warmup_steps,\n        warmup_ratio=args.warmup_ratio,\n        dataloader_num_workers=args.dataloader_num_workers,\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        resume_from_checkpoint=args.resume,\n    )\n\n    print(f\"INFO: tensorboard stored in {os.path.join(args.output_dir, 'runs')}\")\n\n    label_list = id2label\n\n    def training_step(self, batch, batch_idx):\n        optimizer = self.optimizers()\n\n        # first forward-backward pass\n        loss_1 = self.compute_loss(batch)\n        self.manual_backward(loss_1, optimizer)\n        optimizer.first_step(zero_grad=True)\n\n        # second forward-backward pass\n        loss_2 = self.compute_loss(batch)\n        self.manual_backward(loss_2, optimizer)\n        optimizer.second_step(zero_grad=True)\n\n        return loss_1\n\n    def compute_metrics(eval_pred):\n        predictions, labels = eval_pred\n        pred = torch.tensor(predictions)\n        labs = torch.tensor(labels)\n\n        num_labels = pred.shape[-1]\n\n        labs_final = torch.where(labs == -100, 0, labs)\n        pred_sig = torch.sigmoid(pred)\n        pred_final = torch.where(pred_sig > 0.5, 1, 0)\n\n        # break-down to classes\n        result = torchmetrics.functional.stat_scores(\n            pred_final.view(-1, num_labels),\n            labs_final.view(-1, num_labels),\n            task=\"multilabel\",\n            average=None,\n            num_labels=num_labels,\n        )\n\n        results_breakdown = {}\n        results_breakdown[\"KILE\"] = torch.zeros_like(result[0])\n        results_breakdown[\"LIR\"] = torch.zeros_like(result[0])\n        results_breakdown[\"OVERALL\"] = torch.zeros_like(result[0])\n\n        for i in range(num_labels):\n            key = label_list[i]\n            bio = key[0]\n            key_without_BIO = key[2:]\n            # 1. ignore background classes (i.e. those which start with O-)\n            if bio != \"O\":\n                # 2. group B- and I- results for the same class\n                if key_without_BIO not in results_breakdown:\n                    results_breakdown[key_without_BIO] = result[i]\n                else:\n                    results_breakdown[key_without_BIO] += result[i]\n                results_breakdown[\"OVERALL\"] += result[i]\n                # 3. create summary precision, recall, f1, and accuracy also for KILE and LIR, separately\n                if key_without_BIO != \"LI\" and not key_without_BIO.startswith(\"line_item_\"):\n                    # KILE\n                    results_breakdown[\"KILE\"] += result[i]\n                if key_without_BIO != \"LI\" and key_without_BIO.startswith(\"line_item_\"):\n                    # LIR\n                    results_breakdown[\"LIR\"] += result[i]\n\n        out = {}\n        for key, (tp, fp, tn, fn, sup) in results_breakdown.items():\n            precision = tp / (tp + fp)\n            recall = tp / (tp + fn)\n            f1 = 2 * (precision * recall) / (precision + recall)\n            accuracy = (tp + tn) / (tp + tn + fp + fn)\n            out[f\"{key}_precision\"] = precision if not precision.isnan() else torch.tensor(0.0)\n            out[f\"{key}_recall\"] = recall if not recall.isnan() else torch.tensor(0.0)\n            out[f\"{key}_f1\"] = f1 if not f1.isnan() else torch.tensor(0.0)\n            out[f\"{key}_accuracy\"] = accuracy if not accuracy.isnan() else torch.tensor(0.0)\n            out[f\"{key}_support\"] = sup\n\n        ret = {}\n        for key, value in out.items():\n            if key.startswith(\"KILE\") or key.startswith(\"LI\") or key.startswith(\"OVERALL\"):\n                ret[key] = value\n            elif args.report_all_metrics:\n                ret[key] = value\n\n        return ret\n\n    # instantiate HuggingFace Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics\n        # optimizers = ( optimizer,scheduler)\n    )\n    \n    train_result = trainer.train()\n    metrics = train_result.metrics\n\n    # save the trained model\n    best_model_path = os.path.join(args.output_dir, \"best_model\")\n    os.makedirs(best_model_path, exist_ok=True)\n    print(f\"Saving the best model to {best_model_path}\")\n    trainer.save_model(best_model_path)\n\n    trainer.log_metrics(\"train\", metrics)\n\n    metrics = trainer.evaluate()\n    trainer.log_metrics(\"eval\", metrics)\n\n    # compute stats on train_dataset (skip for synthetic dataset, otherwise it can result in memory issues)\n    if \"synthetic\" not in args.split:\n        trainer.eval_dataset = train_dataset\n        metrics = trainer.evaluate()\n        trainer.log_metrics(\"train-eval\", metrics)\n    \n  \n    print(f\"Tensorboard logs: {os.path.join(args.output_dir, 'runs')}\")\n    print(f\"Best model saved to {best_model_path}\")\n\n    show_summary(args, __file__)\n\n    print(f\"{datetime.now()} Finished.\")", ""]}
{"filename": "baselines/NER/docile_inference_NER_multilabel.py", "chunked_list": ["import argparse\nimport dataclasses\nimport json\nimport os\nimport sys\nsys.path.append(\".\")\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nfrom helpers import FieldWithGroups, show_summary\nfrom my_lilt_multilabel import MyLiltForTokenClassification\nfrom tqdm import tqdm\nfrom transformers import AutoConfig, AutoTokenizer\n\nfrom docile.dataset import KILE_FIELDTYPES, LIR_FIELDTYPES, BBox, Dataset\nfrom docile.evaluation.evaluate import evaluate_dataset", "from docile.dataset import KILE_FIELDTYPES, LIR_FIELDTYPES, BBox, Dataset\nfrom docile.evaluation.evaluate import evaluate_dataset\n\n\ndef dfs(visited, graph, node, out):\n    if node not in visited:\n        out.append(node)\n        visited.add(node)\n        for neighbour in graph[node]:\n            dfs(visited, graph, neighbour, out)", "\n\ndef get_center_line_clusters(line_item):\n    # get centers of text boxes (y-axis only)\n    centers = np.array([x.bbox.centroid[1] for x in line_item])\n    heights = np.array([x.bbox.height for x in line_item])\n\n    n_bins = len(centers)\n    if n_bins < 1:\n        return {}\n\n    hist_h, bin_edges_h = np.histogram(heights, bins=n_bins)\n    bin_centers_h = bin_edges_h[:-1] + np.diff(bin_edges_h) / 2\n    idxs_h = np.where(hist_h)[0]\n    heights_cluster_centers = np.unique(bin_centers_h[idxs_h].astype(np.int32))\n    heights_cluster_centers.sort()\n\n    # group text boxes by heights\n    groups_heights = {}\n    for field in line_item:\n        g = np.array(\n            list(map(lambda height: np.abs(field.bbox.height - height), heights_cluster_centers))\n        ).argmin()\n        gid = heights_cluster_centers[g]\n        if gid not in groups_heights:\n            groups_heights[gid] = [field]\n        else:\n            groups_heights[gid].append(field)\n\n    hist, bin_edges = np.histogram(centers, bins=n_bins)\n    bin_centers = bin_edges[:-1] + np.diff(bin_edges) / 2\n    idxs = np.where(hist)[0]\n    y_center_clusters = bin_centers[idxs]\n    y_center_clusters.sort()\n    line_item_height = y_center_clusters.max() - y_center_clusters.min()\n\n    if line_item_height < heights_cluster_centers[0]:\n        # there is probably just 1 cluster\n        return {0: y_center_clusters.mean()}\n    else:\n        #  estimate the number of lines by looking at the cluster centers\n        clusters = {}\n        cnt = 0\n        yc_prev = y_center_clusters[0]\n        for yc in y_center_clusters:\n            if np.abs(yc_prev - yc) < heights_cluster_centers[0]:\n                flag = True\n            else:\n                flag = False\n            if flag:\n                if cnt not in clusters:\n                    clusters[cnt] = [yc]\n                else:\n                    clusters[cnt].append(yc)\n            else:\n                cnt += 1\n                clusters[cnt] = [yc]\n            yc_prev = yc\n        for k, v in clusters.items():\n            clusters[k] = np.array(v).mean()\n    return clusters", "\n\ndef split_fields_by_text_lines(line_item):\n    clusters = get_center_line_clusters(line_item)\n    new_line_item = []\n    for ft in line_item:\n        g = np.array(\n            list(map(lambda y: np.abs(ft.bbox.to_tuple()[1] - y), clusters.values()))\n        ).argmin()\n        updated_ft = dataclasses.replace(ft, line_item_id=[g, ft.line_item_id])\n        # updated_ft = dataclasses.replace(ft, groups=[g], line_item_id=[g])\n        new_line_item.append(updated_ft)\n    return new_line_item, clusters", "\n\ndef get_sorted_field_candidates(ocr_fields):\n    fields = []\n    ocr_fields, clusters = split_fields_by_text_lines(ocr_fields)\n    # sort by estimated lines\n    ocr_fields.sort(key=lambda x: x.line_item_id)\n    # group by estimated lines\n    groups = {}\n    for field in ocr_fields:\n        # gid = str(field.line_item_id)\n        gid = str(field.line_item_id[0])\n        if gid not in groups:\n            groups[gid] = [field]\n        else:\n            groups[gid].append(field)\n    for gid, fs in groups.items():\n        # sort by x-axis (since we are dealing with a single line)\n        fs.sort(key=lambda x: x.bbox.centroid[0])\n        for f in fs:\n            # lid_str = f\"{f.line_item_id:04d}\" if f.line_item_id else \"-001\"\n            lid_str = f\"{f.line_item_id[1]:04d}\" if f.line_item_id[1] else \"-001\"\n            updated_f = dataclasses.replace(\n                f,\n                line_item_id=[f\"{lid_str}{int(gid.strip('[]')):>04d}\"],\n            )\n            fields.append(updated_f)\n    return fields, clusters", "\n\ndef _join_texts(text1: str, text2: str, separator: str) -> str:\n    return (\n        f\"{text1}{separator}{text2}\"\n        if text1 != \"\" and text2 != \"\"\n        else text1\n        if text1 != \"\"\n        else text2\n    )", "\n\ndef merge_text_boxes(text_boxes, merge_strategy=\"new\"):\n    # group by fieldtype:\n    groups = {}\n    for field in text_boxes:\n        gid = field.fieldtype\n        if gid not in groups.keys():\n            groups[gid] = [field]\n        else:\n            groups[gid].append(field)\n\n    # 1. attempt simply merge all fields of the given detected type\n    final_fields = []\n\n    if merge_strategy == \"naive\":\n        for ft, fs in groups.items():\n            new_field = FieldWithGroups(\n                bbox=fs[0].bbox,\n                page=fs[0].page,\n                line_item_id=fs[0].line_item_id,\n                fieldtype=ft,\n                score=0.0,\n                text=\"\",\n            )\n            last_line = int(fs[0].groups[0][4:])\n            for field in fs:\n                curr_line = int(field.groups[0][4:])\n                if curr_line == last_line:\n                    text = f\"{new_field.text} {field.text}\"\n                else:\n                    text = f\"{new_field.text}\\n{field.text}\"\n                new_field = dataclasses.replace(\n                    new_field,\n                    bbox=new_field.bbox.union(field.bbox),\n                    score=new_field.score + field.score,\n                    text=text,\n                )\n                last_line = curr_line\n            # average final score\n            new_field = dataclasses.replace(new_field, score=new_field.score / len(fs))\n            # resolve line_item_id\n            if ft in KILE_FIELDTYPES:\n                new_field = dataclasses.replace(new_field, line_item_id=None)\n            if ft in LIR_FIELDTYPES and new_field.line_item_id is None:\n                new_field = dataclasses.replace(new_field, line_item_id=0)\n            final_fields.append(new_field)\n\n    # 2. attempt - consider the relative distances when merging horizontally and then vertically\n    if merge_strategy == \"new\":\n        for ft, fs in groups.items():\n            textline_group = {}\n            for field in fs:\n                new_field = field\n                # resolve line_item_id\n                # if ft.startswith(\"line_item_\") and not new_field.line_item_id:\n                if ft in LIR_FIELDTYPES and new_field.line_item_id is None:\n                    new_field = dataclasses.replace(new_field, line_item_id=0)\n                # if not ft.startswith(\"line_item_\") and new_field.line_item_id:\n                if ft in KILE_FIELDTYPES:\n                    new_field = dataclasses.replace(new_field, line_item_id=None)\n\n                gid = int(new_field.groups[0][4:])\n                if gid not in textline_group:\n                    textline_group[gid] = [new_field]\n                else:\n                    textline_group[gid].append(new_field)\n\n            # horizontal merging\n            after_horizontal_merging = []\n            for fields in textline_group.values():\n                fields = sorted(fields, key=lambda f: f.bbox.left)\n                processed = []\n\n                # Iterate over the fields and try if they can be merged with any of the following fields.\n                for field_to_process in fields:\n                    if field_to_process in processed:\n                        continue\n                    processed.append(field_to_process)\n                    new_field = FieldWithGroups.from_dict(field_to_process.to_dict())\n                    glued_count = 1\n                    for field in fields:\n                        if field in processed:\n                            continue\n                        # if (field.bbox.left - new_field.bbox.right) <= (field.bbox.width/len(field.text))*1.5:\n                        if field.bbox.left - new_field.bbox.right <= field.bbox.height * 1.25:\n                            new_field = dataclasses.replace(\n                                new_field,\n                                bbox=new_field.bbox.union(field.bbox),\n                                score=new_field.score + field.score,\n                                text=_join_texts(new_field.text, field.text, \" \"),\n                            )\n                            processed.append(field)\n                            glued_count += 1\n                    after_horizontal_merging.append((new_field, glued_count))\n\n            # vertical merging\n            nodes = {}\n            for i, (field1, _gc1) in enumerate(after_horizontal_merging):\n                nodes[i] = []\n                for j, (field2, _gc2) in enumerate(after_horizontal_merging):\n                    # ignore the same field (diagonal in the adjacency matrix)\n                    if field1 != field2:\n                        y_dist = max(\n                            field2.bbox.top - field1.bbox.bottom,\n                            field1.bbox.top - field2.bbox.bottom,\n                        )\n                        x_dist = max(\n                            field2.bbox.left - field1.bbox.right,\n                            field1.bbox.left - field2.bbox.right,\n                        )\n                        # if (y_dist < field1.bbox.height*1.2):\n                        if (y_dist < field1.bbox.height * 1.2) and (\n                            x_dist <= field1.bbox.height * 1.25\n                        ):\n                            nodes[i].append(j)\n            #\n            visited = set()\n            components = {}\n            for i in range(len(nodes)):\n                dfs_path = []\n                dfs(visited, nodes, i, dfs_path)\n                if dfs_path:\n                    components[i] = dfs_path\n\n            #\n            # Merge found components\n            for idxs in components.values():\n                idxs = sorted(idxs, key=lambda i: after_horizontal_merging[i][0].bbox.top)\n                tmp_field = after_horizontal_merging[idxs[0]][0]\n                new_field = FieldWithGroups(\n                    fieldtype=tmp_field.fieldtype,\n                    bbox=tmp_field.bbox,\n                    text=\"\",\n                    score=0,\n                    page=tmp_field.page,\n                    line_item_id=tmp_field.line_item_id,\n                )\n                glued_count = 0\n                for idx in idxs:\n                    field, gc = after_horizontal_merging[idx]\n                    new_field = dataclasses.replace(\n                        new_field,\n                        bbox=new_field.bbox.union(field.bbox),\n                        score=new_field.score + field.score,\n                        text=_join_texts(new_field.text, field.text, \"\\n\"),\n                    )\n                    glued_count += gc\n                new_field = dataclasses.replace(new_field, score=new_field.score / glued_count)\n                # field.bbox = field.bbox.to_absolute_coords(W, H)\n                # new_field.bbox = new_field.bbox.to_relative_coords(W, H)\n                final_fields.append(new_field)\n\n    return final_fields", "\n\ndef lir_by_table_transformer(line_item_bboxes, sorted_fields, page):\n    tmp_field_labels = []\n    for field in sorted_fields:\n        bbox = field.bbox.to_tuple()\n        # use table-transformer predictions for lir\n        li_id = None\n        max_overlap = 0\n        for lid, line_item_bbox in enumerate(line_item_bboxes):\n            overlap = BBox(*line_item_bbox).intersection(BBox(*bbox)).area / BBox(*bbox).area\n            if overlap > max_overlap:\n                li_id = lid\n                max_overlap = overlap\n\n        if li_id is not None:\n            tmp_field_labels.append(\n                FieldWithGroups(\n                    bbox=BBox(*bbox),\n                    text=field.text,\n                    score=field.score,\n                    page=page,\n                    groups=field.groups,\n                    line_item_id=li_id,\n                    fieldtype=field.fieldtype,\n                )\n            )\n    return tmp_field_labels", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--split\", type=str)\n    parser.add_argument(\"--docile_path\", type=Path, default=Path(\"/app/data/docile/\"))\n    parser.add_argument(\"--overlap_thr\", type=float, default=0.5)\n    parser.add_argument(\"--checkpoint\", type=str)\n    parser.add_argument(\"--output_dir\", type=Path)\n    parser.add_argument(\n        \"--table_transformer_predictions_dir\",\n        type=Path,\n        help=\"Directory with table-transformer predictions jsons (see --crop_bboxes_filename)\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--crop_bboxes_filename\",\n        type=str,\n        help=\"Json file in `table_transformer_predictions_dir` with \"\n        \"table crop bboxes e.g. predicted by table-transformer. If provided, NER \"\n        \"will be run only on texts within the crop.\",\n        default=None,\n    )\n    parser.add_argument(\n        \"--line_item_bboxes_filename\",\n        type=str,\n        help=\"Json file in `table_transformer_predictions_dir` with \"\n        \"line item bboxes e.g. predicted by table-transformer.\",\n        default=None,\n    )\n    parser.add_argument(\"--store_intermediate_results\", action=\"store_true\")\n    parser.add_argument(\"--merge_strategy\", type=str, default=\"new\")\n    args = parser.parse_args()\n\n    print(f\"{datetime.now()} Started.\")\n\n    show_summary(args, __file__)\n\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    # Check GPU availability\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n\n    # DocILE DATASET\n    dataset = Dataset(args.split, args.docile_path)\n\n    docid_to_kile_predictions = {}\n    docid_to_lir_predictions = {}\n\n    intermediate_results = {}\n\n    config = AutoConfig.from_pretrained(args.checkpoint)\n    tokenizer = AutoTokenizer.from_pretrained(args.checkpoint)\n\n    model = MyLiltForTokenClassification.from_pretrained(\n        args.checkpoint, config=config\n    ).to(device)\n\n    # fix the old models (variable_symbol -> payment_reference)\n    try:\n        model.config.id2label[model.config.label2id[\"B-variable_symbol\"]] = \"B-payment_reference\"\n        model.config.id2label[model.config.label2id[\"I-variable_symbol\"]] = \"I-payment_reference\"\n        print(\"INFO: model with an obsolete label set. Updating the label-set...\")\n    except Exception:\n        print(\"INFO: model with up-to-date label set\")\n\n    model.eval()\n\n    crop_bboxes = None\n    if args.crop_bboxes_filename is not None:\n        with open(\n            os.path.join(\n                args.table_transformer_predictions_dir, args.split, args.crop_bboxes_filename\n            )\n        ) as fin:\n            crop_bboxes = json.load(fin)\n    line_item_bboxes = None\n    predict_li_by_NER = True\n    if args.line_item_bboxes_filename is not None:\n        with open(\n            os.path.join(\n                args.table_transformer_predictions_dir, args.split, args.line_item_bboxes_filename\n            )\n        ) as fin:\n            line_item_bboxes = json.load(fin)\n        predict_li_by_NER = False\n\n    for document in tqdm(dataset):\n        doc_id = document.docid\n        pred_kile_fields = []\n        pred_kile_fields_final = []\n        pred_li_fields = []\n        pred_li_fields_final = []\n        all_fields_final = []\n        intermediate_fields = []\n        li_id = -1\n        for page in range(document.page_count):\n            img = document.page_image(page)\n            W, H = img.size\n            ocr = document.ocr.get_all_words(page, snapped=True)\n            ocr = [\n                dataclasses.replace(ocr_field, bbox=ocr_field.bbox.to_absolute_coords(W, H))\n                for ocr_field in ocr\n            ]\n\n            sorted_fields, clusters = get_sorted_field_candidates(ocr)\n\n            # Filter out fields based on the tables region\n            if crop_bboxes is not None:\n                if crop_bboxes[doc_id][str(page)] is not None:\n                    tbb = BBox(*crop_bboxes[doc_id][str(page)])\n                    sorted_fields = [\n                        field\n                        for field in sorted_fields\n                        if tbb.intersection(field.bbox).area / field.bbox.area >= args.overlap_thr\n                    ]\n\n            text_tokens = [x.text for x in sorted_fields]\n            bboxes = np.array([x.bbox.to_tuple() for x in sorted_fields])\n            # groups = [x.groups for x in sorted_fields]\n            groups = [x.line_item_id for x in sorted_fields]\n\n            tokenized_inputs = tokenizer(\n                text_tokens,\n                is_split_into_words=True,\n                add_special_tokens=True,\n                truncation=True,\n                padding=True,\n                max_length=512,\n                return_overflowing_tokens=True,  # important !!!\n                return_length=True,\n                verbose=True,\n                return_tensors=\"pt\",\n                stride=config.stride,\n            ).to(device)\n\n            length = tokenized_inputs.pop(\"length\")\n            overflow_to_sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n\n            i = 0\n            bboxes2 = []\n            overflowing = tokenized_inputs[i].overflowing\n            for i in range(0, 1 + len(overflowing)):\n                word_ids = tokenized_inputs[i].word_ids\n                previous_word_idx = None\n                bboxes_tmp = []\n                for word_idx in word_ids:\n                    if word_idx is None:\n                        bboxes_tmp.append(np.array([0, 0, 0, 0], dtype=np.int32))\n                    elif (word_idx != previous_word_idx) or (model.config.tag_everything):\n                        bboxes_tmp.append(bboxes[word_idx])\n                    else:\n                        bboxes_tmp.append(np.array([0, 0, 0, 0], dtype=np.int32))\n                    previous_word_idx = word_idx\n                bboxes2.append(bboxes_tmp)\n            tokenized_inputs[\"bboxes\"] = (\n                torch.Tensor(np.array(bboxes2, dtype=np.int32)).long().to(device)\n            )\n\n            outputs = model(**tokenized_inputs)\n\n            # multi-label prediction\n            scores = torch.sigmoid(outputs.logits)\n            predictions = torch.where(scores > 0.15, 1, 0)\n            num_batch = outputs.logits.shape[0]\n\n            # LI\n            word_ids = []\n            valid_ids = []\n            tokens = []\n            predictions_flattened = []\n            # confs_flattened = []\n            scores_flattened = []\n            # gather info from batches\n            for b_i in range(num_batch):\n                if b_i > 0:\n                    start_i = model.config.stride + 1\n                    end_i = np.where(\n                        np.array(tokenized_inputs[b_i].tokens) == tokenizer.sep_token\n                    )[0][0]\n                else:\n                    start_i = 1\n                    end_i = -1\n                word_ids.extend(tokenized_inputs.word_ids(b_i)[start_i:end_i])\n                tokens.extend(\n                    tokenizer.convert_ids_to_tokens(tokenized_inputs[\"input_ids\"].tolist()[b_i])[\n                        start_i:end_i\n                    ]\n                )\n                predictions_flattened.extend(predictions[b_i].tolist()[start_i:end_i])\n                # confs_flattened.extend(confs[b_i].tolist()[start_i:end_i])\n                scores_flattened.extend(scores[b_i].tolist()[start_i:end_i])\n            word_ids = np.array(word_ids)\n            valid_ids = np.where(word_ids != None)  # noqa: E711\n            pred_classes_full = []\n            for pred, score in zip(predictions_flattened, scores_flattened):\n                pred_classes_full.append(\n                    [(model.config.id2label[x], score[x]) for x in np.nonzero(pred)[0]]\n                )\n            # post-process predicted classes (namely split into KILE, LIR and LI)\n            pred_classes_groupped = {}\n            kile = []\n            lir = []\n            li = []\n            for pred_class in pred_classes_full:\n                # print(f\"pred_class = {pred_class}\")\n                pred_kile_classes = []\n                pred_lir_classes = []\n                pred_li_classes = []\n                for pc in pred_class:\n                    if pc[0][2:] in KILE_FIELDTYPES or pc[0] == \"O-KILE\":\n                        pred_kile_classes.append(pc)\n                    if pc[0][2:] in LIR_FIELDTYPES or pc[0] == \"O-LIR\":\n                        pred_lir_classes.append(pc)\n                    if pc[0] == \"O-LI\" or pc[0] == \"B-LI\" or pc[0] == \"I-LI\" or pc[0] == \"E-LI\":\n                        pred_li_classes.append(pc)\n                kile.append(pred_kile_classes)\n                lir.append(pred_lir_classes)\n                li.append(pred_li_classes)\n\n            pred_classes_groupped[\"KILE\"] = np.array(kile, dtype=object)\n            pred_classes_groupped[\"LIR\"] = np.array(lir, dtype=object)\n            pred_classes_groupped[\"LI\"] = np.array(li, dtype=object)\n\n            # mark (by index to text_tokens) the beginnings and ends of the LI as predicted\n            LI_beginnings = []\n            LI_ends = []\n            tmp_field_labels = []\n            for word_i, text_token in enumerate(text_tokens):\n                idxs = np.where(word_ids == word_i)[0]\n                if not text_token:\n                    continue\n\n                # KILE class\n                pred_KILE = pred_classes_groupped[\"KILE\"][idxs].tolist()\n                KILE_preds_lengths = [len(x) for x in pred_KILE]\n                N_kp = max(KILE_preds_lengths)\n                if N_kp > 1:\n                    # multiple predictions (thanks to multi-label formulation)\n                    # treat each label separately\n                    # discard prediction, if the other prediction is O-KILE ?\n                    sub_token_classes = []\n                    sub_token_scores = []\n                    for all_pred_for_sub_token in pred_KILE:\n                        sub_token_classes.append([None] * N_kp)\n                        sub_token_scores.append([None] * N_kp)\n                        # for sub_token_pred in all_pred_for_sub_token:\n                        for tmp_i, sub_token_pred in enumerate(all_pred_for_sub_token):\n                            sub_token_classes[-1][tmp_i] = sub_token_pred[0]\n                            sub_token_scores[-1][tmp_i] = sub_token_pred[1]\n                    stc = np.array(sub_token_classes)\n                    sts = np.array(sub_token_scores)\n                    n_tokens, n_preds = stc.shape\n                    kile_pred = []\n                    kile_score = []\n                    for si in range(n_preds):\n                        if np.all(stc[:, si] == stc[:, si][0]):\n                            # consistent prediction for all sub-tokens\n                            tmp = stc[:, si][0]\n                            kile_pred.append(tmp[2:] if tmp[0] != \"O\" else \"background\")\n                            kile_score.append(sts[:, si][0])\n                        else:\n                            # inconsistent prediction - just take the maximum scoring one for the whole word for now\n                            sts[:, si] = np.where(sts[:, si] == None, 0, sts[:, si])  # noqa: E711\n                            maxidx = sts[:, si].argmax()\n                            tmp = stc[:, si][maxidx]\n                            kile_pred.append(tmp[2:] if tmp[0] != \"O\" else \"background\")\n                            kile_score.append(sts[:, si][maxidx])\n                else:\n                    # just 1 class predicted for each sub-token -> we can use the same approach as for single-label\n                    pred_KILE = [item for sublist in pred_KILE for item in sublist]\n                    pred_ents_KILE = np.array(\n                        [x[0][2:] if x[0][0] != \"O\" else \"background\" for x in pred_KILE]\n                    )  # remove the IOB tag\n                    pred_ents_KILE_score = np.array([x[1] for x in pred_KILE])\n                    if len(pred_ents_KILE):\n                        if (pred_ents_KILE == pred_ents_KILE[0]).all():\n                            # consistent prediction for all sub-tokens\n                            kile_pred = pred_ents_KILE[0]\n                            kile_score = pred_ents_KILE_score[0]\n                        else:\n                            # inconsistent prediction - just take the maximum scoring one for the whole word\n                            # for now\n                            kile_pred = pred_ents_KILE[pred_ents_KILE_score.argmax()]\n                            kile_score = pred_ents_KILE_score[pred_ents_KILE_score.argmax()]\n                    else:\n                        kile_pred = \"background\"\n                        kile_score = 0\n\n                # LIR class\n                pred_LIR = pred_classes_groupped[\"LIR\"][idxs].tolist()\n                pred_LIR = [item for sublist in pred_LIR for item in sublist]\n                pred_ents_LIR = np.array(\n                    [x[0][2:] if x[0][0] != \"O\" else \"background\" for x in pred_LIR]\n                )  # remove the IOB tag\n                pred_ents_LIR_score = np.array([x[1] for x in pred_LIR])\n                if len(pred_ents_LIR):\n                    if (pred_ents_LIR == pred_ents_LIR[0]).all():\n                        # consistent prediction for all sub-tokens\n                        lir_pred = pred_ents_LIR[0]\n                        lir_score = pred_ents_LIR_score[0]\n                    else:\n                        # inconsistent prediction - just take the maximum scoring one for the whole word\n                        # for now\n                        lir_pred = pred_ents_LIR[pred_ents_LIR_score.argmax()]\n                        lir_score = pred_ents_LIR_score[pred_ents_LIR_score.argmax()]\n                else:\n                    lir_pred = \"background\"\n                    lir_score = 0\n\n                # LI class\n                pred_LI = pred_classes_groupped[\"LI\"][idxs].tolist()\n                pred_LI = [item for sublist in pred_LI for item in sublist]\n                pred_ents_LI = np.array([x[0] for x in pred_LI])\n                pred_ents_LI_score = np.array([x[1] for x in pred_LI])\n                if len(pred_ents_LI):\n                    if (pred_ents_LI == pred_ents_LI[0]).all():\n                        # consistent prediction for all sub-tokens\n                        li_pred = pred_ents_LI[0]\n                        li_score = pred_ents_LI_score[0]\n                    else:\n                        # inconsistent prediction - just take the maximum scoring one for the whole word\n                        # for now\n                        li_pred = pred_ents_LI[pred_ents_LI_score.argmax()]\n                        li_score = pred_ents_LI_score[pred_ents_LI_score.argmax()]\n                else:\n                    li_pred = \"O-LI\"\n                    li_score = 0\n\n                group = groups[word_i]\n                bbox = bboxes[word_i]\n\n                # just for debugging\n                if li_pred == \"B-LI\":\n                    LI_beginnings.append((word_i, li_score))\n                if li_pred == \"E-LI\":\n                    LI_ends.append((word_i, li_score))\n                # --------\n\n                if li_pred == \"B-LI\":\n                    li_id += 1\n\n                if (\n                    isinstance(kile_pred, str)\n                    and kile_pred == \"background\"\n                    and lir_pred == \"background\"\n                ):\n                    # NOTE: add just one field\n                    tmp_field_labels.append(\n                        FieldWithGroups(\n                            fieldtype=\"background\",\n                            bbox=BBox(*bbox),\n                            text=text_token,\n                            page=page,\n                            groups=group,\n                            line_item_id=li_id if li_id > 0 else None,\n                            score=max(kile_score, lir_score),\n                        )\n                    )\n                elif isinstance(kile_pred, str) and kile_pred == \"background\":\n                    tmp_field_labels.append(\n                        FieldWithGroups(\n                            fieldtype=lir_pred,\n                            bbox=BBox(*bbox),\n                            text=text_token,\n                            page=page,\n                            groups=group,\n                            line_item_id=li_id if li_id > 0 else None,\n                            score=lir_score,\n                        )\n                    )\n                elif lir_pred == \"background\":\n                    if not isinstance(kile_pred, list):\n                        kile_pred = [kile_pred]\n                        kile_score = [kile_score]\n                    for kp, ks in zip(kile_pred, kile_score):\n                        tmp_field_labels.append(\n                            FieldWithGroups(\n                                fieldtype=kp,\n                                bbox=BBox(*bbox),\n                                text=text_token,\n                                page=page,\n                                groups=group,\n                                line_item_id=li_id if li_id > 0 else None,\n                                score=ks,\n                            )\n                        )\n                else:\n                    # NOTE: add the field twice, once for kile and once for lir\n                    if not isinstance(kile_pred, list):\n                        kile_pred = [kile_pred]\n                        kile_score = [kile_score]\n                    for kp, ks in zip(kile_pred, kile_score):\n                        tmp_field_labels.append(\n                            FieldWithGroups(\n                                fieldtype=kp,\n                                bbox=BBox(*bbox),\n                                text=text_token,\n                                page=page,\n                                groups=group,\n                                line_item_id=li_id if li_id > 0 else None,\n                                score=ks,\n                            )\n                        )\n                    tmp_field_labels.append(\n                        FieldWithGroups(\n                            fieldtype=lir_pred,\n                            bbox=BBox(*bbox),\n                            text=text_token,\n                            page=page,\n                            groups=group,\n                            line_item_id=li_id if li_id > 0 else None,\n                            score=lir_score,\n                        )\n                    )\n\n            if not predict_li_by_NER:\n                tmp_field_labels = lir_by_table_transformer(\n                    line_item_bboxes[doc_id][str(page)], tmp_field_labels, page\n                )\n\n            #\n            line_item_groups = {}\n            for field in tmp_field_labels:\n                gid = field.line_item_id\n                if gid not in line_item_groups:\n                    line_item_groups[gid] = [field]\n                else:\n                    line_item_groups[gid].append(field)\n\n            # Merge text boxes to final predictions\n            # out = []\n            for _, fs in line_item_groups.items():\n                # line_items = merge_text_boxes(fs)\n                if args.store_intermediate_results:\n                    for field in fs:\n                        field2 = FieldWithGroups.from_dict(field.to_dict())\n                        field2 = dataclasses.replace(\n                            field2, bbox=field2.bbox.to_relative_coords(W, H)\n                        )\n                        intermediate_fields.append(field2)\n                line_items = merge_text_boxes(\n                    [x for x in fs if x.fieldtype != \"background\"], args.merge_strategy\n                )\n                for field in line_items:\n                    # skip background fields\n                    if field.fieldtype != \"background\":\n                        # transform back to relative coordinates\n                        new_field = dataclasses.replace(\n                            field, bbox=field.bbox.to_relative_coords(W, H)\n                        )\n                        all_fields_final.append(new_field)\n\n        # add final predictions to docid_to_lir_predictions mapping\n        docid_to_kile_predictions[doc_id] = [\n            x for x in all_fields_final if x.fieldtype in KILE_FIELDTYPES\n        ]\n        docid_to_lir_predictions[doc_id] = [\n            x for x in all_fields_final if x.fieldtype in LIR_FIELDTYPES\n        ]\n        if args.store_intermediate_results:\n            intermediate_results[doc_id] = intermediate_fields\n\n    # Store intermediate results\n    if args.store_intermediate_results:\n        predictions_to_store = {}\n        for k, v in intermediate_results.items():\n            predictions_to_store[k] = []\n            for field in v:\n                predictions_to_store[k].append(\n                    {\n                        \"bbox\": field.bbox.to_tuple(),\n                        \"page\": field.page,\n                        \"score\": field.score,\n                        \"text\": field.text,\n                        \"fieldtype\": field.fieldtype,\n                        \"line_item_id\": field.line_item_id,\n                        \"groups\": field.groups,\n                    }\n                )\n        out_path = args.output_dir / f\"{args.split}_intermediate_predictions.json\"\n        with open(out_path, \"w\") as json_file:\n            json.dump(predictions_to_store, json_file)\n\n    # Store predictions\n    predictions_to_store = {}\n    for k, v in docid_to_kile_predictions.items():\n        predictions_to_store[k] = []\n        for field in v:\n            predictions_to_store[k].append(\n                {\n                    \"bbox\": field.bbox.to_tuple(),\n                    \"page\": field.page,\n                    \"score\": field.score,\n                    \"text\": field.text,\n                    \"fieldtype\": field.fieldtype,\n                    \"line_item_id\": field.line_item_id,\n                    \"groups\": field.groups,\n                }\n            )\n    out_path = args.output_dir / f\"{args.split}_predictions_KILE.json\"\n    with open(out_path, \"w\") as json_file:\n        json.dump(predictions_to_store, json_file)\n\n    print(f\"Output stored to {out_path}\")\n\n    predictions_to_store = {}\n    for k, v in docid_to_lir_predictions.items():\n        predictions_to_store[k] = []\n        for field in v:\n            predictions_to_store[k].append(\n                {\n                    \"bbox\": field.bbox.to_tuple(),\n                    \"page\": field.page,\n                    \"score\": field.score,\n                    \"text\": field.text,\n                    \"fieldtype\": field.fieldtype,\n                    \"line_item_id\": field.line_item_id,\n                    \"groups\": field.groups,\n                }\n            )\n    if args.crop_bboxes_filename is not None:\n        crop_info = f\"_cropped_{args.crop_bboxes_filename.split('.')[0]}\"\n    else:\n        crop_info = \"\"\n    if args.line_item_bboxes_filename is not None:\n        lir_info = f\"_lir_{args.line_item_bboxes_filename.split('.')[0]}\"\n    else:\n        lir_info = \"\"\n\n    out_path = args.output_dir / f\"{args.split}_predictions{crop_info}{lir_info}_LIR.json\"\n\n    with open(out_path, \"w\") as json_file:\n        json.dump(predictions_to_store, json_file)\n\n    print(f\"Output stored to {out_path}\")\n\n    # Call DocILE evaluation\n    print(f\"RESULTS for {args.split}\")\n\n    # KILE\n    evaluation_result_KILE = evaluate_dataset(dataset, docid_to_kile_predictions, {})\n    print(evaluation_result_KILE.print_report())\n    evaluation_result_KILE.to_file(args.output_dir / f\"{args.split}_results_KILE.json\")\n\n    # LIR\n    evaluation_result_LIR = evaluate_dataset(dataset, {}, docid_to_lir_predictions)\n    print(evaluation_result_LIR.print_report())\n    evaluation_result_LIR.to_file(args.output_dir / f\"{args.split}_results_LIR.json\")\n\n    print(f\"{datetime.now()} Finished.\")", ""]}
{"filename": "baselines/NER/data_collator.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import Optional, Union\n\nfrom transformers.data.data_collator import DataCollatorMixin\nfrom transformers.models.layoutlmv3.processing_layoutlmv3 import LayoutLMv3Processor\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\nfrom transformers.utils import PaddingStrategy\n\n\n@dataclass\nclass MyMLDataCollatorForTokenClassification(DataCollatorMixin):\n    \"\"\"\n    Data collator that will dynamically pad the inputs received, as well as the labels.\n\n    Args:\n        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n            The tokenizer used for encoding the data.\n        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n\n            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n              is provided).\n            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n              acceptable input length for the model if that argument is not provided.\n            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n              lengths).\n        max_length (`int`, *optional*):\n            Maximum length of the returned list and optionally padding length (see above).\n        pad_to_multiple_of (`int`, *optional*):\n            If set will pad the sequence to a multiple of the provided value.\n\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n        label_pad_token_id (`int`, *optional*, defaults to -100):\n            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n        return_tensors (`str`):\n            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    # label_pad_token_id: int = -100\n    return_tensors: str = \"pt\"\n\n    # Note that only pytorch is supported ATM\n    def torch_call(self, features):\n        import torch\n\n        # custom features\n        features_mod = []\n        for feat in features:\n            for i in range(len(feat[\"input_ids\"])):\n                mod_feat = {}\n                mod_feat[\"input_ids\"] = feat[\"input_ids\"][i]\n                if \"token_type_ids\" in feat:\n                    mod_feat[\"token_type_ids\"] = feat[\"token_type_ids\"][i]\n                mod_feat[\"attention_mask\"] = feat[\"attention_mask\"][i]\n                mod_feat[\"labels\"] = feat[\"labels\"][i]\n                if \"bboxes\" in feat:\n                    mod_feat[\"bboxes\"] = feat[\"bboxes\"][i]\n                features_mod.append(mod_feat)\n\n        # TODO (michal.uricar): cut features_mod, so it has a fixed batch_size ?\n\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n        labels = (\n            [feature[label_name] for feature in features_mod]\n            if label_name in features_mod[0].keys()\n            else None\n        )\n        N_labels = len(labels[0][0]) if labels else None\n        bboxes = (\n            [feature[\"bboxes\"] for feature in features_mod]\n            if \"bboxes\" in features_mod[0].keys()\n            else None\n        )\n        batch = self.tokenizer.pad(\n            features_mod,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n            return_tensors=\"pt\" if labels is None else None,\n        )\n\n        if labels is None:\n            return batch\n\n        sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n        padding_side = self.tokenizer.padding_side\n        if padding_side == \"right\":\n            batch[label_name] = [\n                list(label) + [[False] * N_labels] * (sequence_length - len(label))\n                for label in labels\n            ]\n            if bboxes:\n                batch[\"bboxes\"] = [\n                    list(bbox) + [[0, 0, 0, 0]] * (sequence_length - len(bbox)) for bbox in bboxes\n                ]\n        else:\n            batch[label_name] = [\n                [[False] * N_labels] * (sequence_length - len(label)) + list(label)\n                for label in labels\n            ]\n            if bboxes:\n                batch[\"bboxes\"] = [\n                    [[0, 0, 0, 0]] * (sequence_length - len(bbox)) + list(bbox) for bbox in bboxes\n                ]\n\n        batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n        return batch", "\n@dataclass\nclass MyMLDataCollatorForTokenClassification(DataCollatorMixin):\n    \"\"\"\n    Data collator that will dynamically pad the inputs received, as well as the labels.\n\n    Args:\n        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n            The tokenizer used for encoding the data.\n        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n\n            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n              is provided).\n            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n              acceptable input length for the model if that argument is not provided.\n            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n              lengths).\n        max_length (`int`, *optional*):\n            Maximum length of the returned list and optionally padding length (see above).\n        pad_to_multiple_of (`int`, *optional*):\n            If set will pad the sequence to a multiple of the provided value.\n\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n        label_pad_token_id (`int`, *optional*, defaults to -100):\n            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n        return_tensors (`str`):\n            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    # label_pad_token_id: int = -100\n    return_tensors: str = \"pt\"\n\n    # Note that only pytorch is supported ATM\n    def torch_call(self, features):\n        import torch\n\n        # custom features\n        features_mod = []\n        for feat in features:\n            for i in range(len(feat[\"input_ids\"])):\n                mod_feat = {}\n                mod_feat[\"input_ids\"] = feat[\"input_ids\"][i]\n                if \"token_type_ids\" in feat:\n                    mod_feat[\"token_type_ids\"] = feat[\"token_type_ids\"][i]\n                mod_feat[\"attention_mask\"] = feat[\"attention_mask\"][i]\n                mod_feat[\"labels\"] = feat[\"labels\"][i]\n                if \"bboxes\" in feat:\n                    mod_feat[\"bboxes\"] = feat[\"bboxes\"][i]\n                features_mod.append(mod_feat)\n\n        # TODO (michal.uricar): cut features_mod, so it has a fixed batch_size ?\n\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n        labels = (\n            [feature[label_name] for feature in features_mod]\n            if label_name in features_mod[0].keys()\n            else None\n        )\n        N_labels = len(labels[0][0]) if labels else None\n        bboxes = (\n            [feature[\"bboxes\"] for feature in features_mod]\n            if \"bboxes\" in features_mod[0].keys()\n            else None\n        )\n        batch = self.tokenizer.pad(\n            features_mod,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n            return_tensors=\"pt\" if labels is None else None,\n        )\n\n        if labels is None:\n            return batch\n\n        sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n        padding_side = self.tokenizer.padding_side\n        if padding_side == \"right\":\n            batch[label_name] = [\n                list(label) + [[False] * N_labels] * (sequence_length - len(label))\n                for label in labels\n            ]\n            if bboxes:\n                batch[\"bboxes\"] = [\n                    list(bbox) + [[0, 0, 0, 0]] * (sequence_length - len(bbox)) for bbox in bboxes\n                ]\n        else:\n            batch[label_name] = [\n                [[False] * N_labels] * (sequence_length - len(label)) + list(label)\n                for label in labels\n            ]\n            if bboxes:\n                batch[\"bboxes\"] = [\n                    [[0, 0, 0, 0]] * (sequence_length - len(bbox)) + list(bbox) for bbox in bboxes\n                ]\n\n        batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n        return batch", "\n\n@dataclass\nclass MyLayoutLMv3MLDataCollatorForTokenClassification(DataCollatorMixin):\n    \"\"\"\n    Data collator that will dynamically pad the inputs received, as well as the labels.\n\n    Args:\n        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n            The tokenizer used for encoding the data.\n        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n\n            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n              is provided).\n            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n              acceptable input length for the model if that argument is not provided.\n            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n              lengths).\n        max_length (`int`, *optional*):\n            Maximum length of the returned list and optionally padding length (see above).\n        pad_to_multiple_of (`int`, *optional*):\n            If set will pad the sequence to a multiple of the provided value.\n\n            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n            7.5 (Volta).\n        label_pad_token_id (`int`, *optional*, defaults to -100):\n            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n        return_tensors (`str`):\n            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n    \"\"\"\n\n    # tokenizer: PreTrainedTokenizerBase\n    processor: LayoutLMv3Processor\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    # label_pad_token_id: int = -100\n    return_tensors: str = \"pt\"\n\n    # Note that only pytorch is supported ATM\n    def torch_call(self, features):\n        import torch\n\n        # custom features\n        features_mod = []\n        for feat in features:\n            for i in range(len(feat[\"input_ids\"])):\n                mod_feat = {}\n                mod_feat[\"input_ids\"] = feat[\"input_ids\"][i]\n                if \"token_type_ids\" in feat:\n                    mod_feat[\"token_type_ids\"] = feat[\"token_type_ids\"][i]\n                mod_feat[\"attention_mask\"] = feat[\"attention_mask\"][i]\n                mod_feat[\"labels\"] = feat[\"labels\"][i]\n                if \"bbox\" in feat:\n                    mod_feat[\"bbox\"] = feat[\"bbox\"][i]\n                if \"pixel_values\" in feat:\n                    mod_feat[\"pixel_values\"] = feat[\"pixel_values\"][i]\n                features_mod.append(mod_feat)\n\n        # NOTE (michal.uricar): cut features_mod, so it has a fixed batch_size ?\n\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n        labels = (\n            [feature[label_name] for feature in features_mod]\n            if label_name in features_mod[0].keys()\n            else None\n        )\n        N_labels = len(labels[0][0]) if labels else None\n        batch = self.processor.tokenizer.pad(\n            features_mod,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            # Conversion to tensors will fail if we have labels as they are not of the same length yet.\n            return_tensors=\"pt\" if labels is None else None,\n        )\n\n        if labels is None:\n            return batch\n\n        sequence_length = torch.tensor(batch[\"input_ids\"]).shape[1]\n        padding_side = self.processor.tokenizer.padding_side\n        if padding_side == \"right\":\n            batch[label_name] = [\n                list(label) + [[False] * N_labels] * (sequence_length - len(label))\n                for label in labels\n            ]\n        else:\n            batch[label_name] = [\n                [[False] * N_labels] * (sequence_length - len(label)) + list(label)\n                for label in labels\n            ]\n\n        batch = {\n            k: torch.tensor(v, dtype=torch.int64 if k != \"pixel_values\" else torch.float32)\n            for k, v in batch.items()\n        }\n        return batch", ""]}
{"filename": "baselines/NER/helpers.py", "chunked_list": ["import argparse\nfrom dataclasses import dataclass\nfrom typing import Optional, Sequence\nimport sys\nsys.path.append('.')\nfrom docile.dataset import Field\n\n\n@dataclass(frozen=True)\nclass FieldWithGroups(Field):\n    groups: Optional[Sequence[str]] = None", "@dataclass(frozen=True)\nclass FieldWithGroups(Field):\n    groups: Optional[Sequence[str]] = None\n\n\ndef show_summary(args: argparse.Namespace, filename: str):\n    \"\"\"Helper function showing the summary of surgery experiment instance given by runtime\n    arguments\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        input arguments\n    \"\"\" \"\"\"\"\"\"\n    # Helper function showing the summary of surgery experiment instance given by runtime\n    # arguments\n    # \"\"\"\n    print(\"-\" * 50)\n    print(f\"{filename}\")\n    print(\"-\" * 10)\n    [print(f\"{k.upper()}: {v}\") for k, v in vars(args).items()]\n    print(\"-\" * 50)", "\n\ndef bbox_str(bbox):\n    if bbox:\n        try:\n            return f\"{bbox[0]:>#04.1f}, {bbox[1]:>#04.1f}, {bbox[2]:>#04.1f}, {bbox[3]:>#04.1f}\"\n        except Exception:\n            bbox = bbox.to_tuple()\n            return f\"{bbox[0]:>#04.1f}, {bbox[1]:>#04.1f}, {bbox[2]:>#04.1f}, {bbox[3]:>#04.1f}\"\n    else:\n        return \"<NONE>\"", "\n\ndef print_docile_fields(fields, fieldtype=None, ft_width=65):\n    for i, ft in enumerate(fields):\n        if ft:\n            if (fieldtype and ft.fieldtype == fieldtype) or fieldtype is None:\n                if ft.text:\n                    text = repr(ft.text) if isinstance(ft.text, str) else ft.text\n                else:\n                    text = \"<NONE>\"\n                if isinstance(ft.fieldtype, list):\n                    fieldtype_1 = \";\".join(ft.fieldtype) if ft.fieldtype else \"None\"\n                else:\n                    fieldtype_1 = ft.fieldtype if ft.fieldtype else \"None\"\n                # NOTE (michal.uricar): add page\n                score = f\"{ft.score:.2f}\" if ft.score else \"None\"\n                print(\n                    f\"{i:05d}: \",\n                    f\"ft='{fieldtype_1:<{ft_width}}' |\"\n                    f\"page='{ft.page:<3}' |\"\n                    f\"'{text:<30}' |\"\n                    f\"{bbox_str(ft.bbox):<30} |\"\n                    f\"{ft.groups} |\"\n                    f\"{ft.line_item_id} |\"\n                    f\"score={score:<5} | \",\n                )\n        else:\n            print(\"None\")", ""]}
