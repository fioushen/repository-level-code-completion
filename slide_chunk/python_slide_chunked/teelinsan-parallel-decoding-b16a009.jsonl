{"filename": "main.py", "chunked_list": ["import hydra\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    MBartForConditionalGeneration,\n)\n\nfrom src.bench import MTBenchmarker\nfrom src.dataset.flores_dataset import Flores\nfrom src.dataset.ittb_dataset import Ittb", "from src.dataset.flores_dataset import Flores\nfrom src.dataset.ittb_dataset import Ittb\nfrom src.dataset.iwslt_dataset import Iwslt\nfrom src.dataset.wmt_dataset import Wmt\nfrom src.ipi.decoders.autoregressive import AutoregressiveDecoder\nfrom src.ipi.decoders.beam_search import BeamSearchDecoder\nfrom src.ipi.decoders.gs_jacobi import GSJacobiDecoder\nfrom src.ipi.decoders.hybrid_jacobi import HybridJacobiDecoder\nfrom src.ipi.decoders.jacobi import JacobiDecoder\nfrom src.ipi.initializer import Initializer", "from src.ipi.decoders.jacobi import JacobiDecoder\nfrom src.ipi.initializer import Initializer\nfrom src.ipi.decoders.mt_decoding import MTDecoder\nfrom src.utils.beam_search import BeamSearcher\nfrom src.utils.utils import retrieve_samples\n\n\ndef load_tokenizer(cfg):\n    # MBart\n    mapping_dict = {\n        \"ar\": \"ar_AR\",\n        \"cs\": \"cs_CZ\",\n        \"de\": \"de_DE\",\n        \"en\": \"en_XX\",\n        \"es\": \"es_XX\",\n        \"et\": \"et_EE\",\n        \"fi\": \"fi_FI\",\n        \"fr\": \"fr_XX\",\n        \"gu\": \"gu_IN\",\n        \"hi\": \"hi_IN\",\n        \"it\": \"it_IT\",\n        \"ja\": \"ja_XX\",\n        \"kk\": \"kk_KZ\",\n        \"ko\": \"ko_KR\",\n        \"lt\": \"lt_LT\",\n        \"lv\": \"lv_LV\",\n        \"my\": \"my_MM\",\n        \"ne\": \"ne_NP\",\n        \"nl\": \"nl_XX\",\n        \"ro\": \"ro_RO\",\n        \"ru\": \"ru_RU\",\n        \"si\": \"si_LK\",\n        \"tr\": \"tr_TR\",\n        \"vi\": \"vi_VN\",\n        \"zh\": \"zh_CN\",\n        \"af\": \"af_ZA\",\n        \"az\": \"az_AZ\",\n        \"bn\": \"bn_IN\",\n        \"fa\": \"fa_IR\",\n        \"he\": \"he_IL\",\n        \"hr\": \"hr_HR\",\n        \"id\": \"id_ID\",\n        \"ka\": \"ka_GE\",\n        \"km\": \"km_KH\",\n        \"mk\": \"mk_MK\",\n        \"ml\": \"ml_IN\",\n        \"mn\": \"mn_MN\",\n        \"mr\": \"mr_IN\",\n        \"pl\": \"pl_PL\",\n        \"ps\": \"ps_AF\",\n        \"pt\": \"pt_XX\",\n        \"sv\": \"sv_SE\",\n        \"sw\": \"sw_KE\",\n        \"ta\": \"ta_IN\",\n        \"te\": \"te_IN\",\n        \"th\": \"th_TH\",\n        \"tl\": \"tl_XX\",\n        \"uk\": \"uk_UA\",\n        \"ur\": \"ur_PK\",\n        \"xh\": \"xh_ZA\",\n        \"gl\": \"gl_ES\",\n        \"sl\": \"sl_SI\",\n    }\n\n    if \"mbart\" in cfg.model_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            cfg.model_name,\n            src_lang=mapping_dict[cfg.src_lang],\n            tgt_lang=mapping_dict[cfg.tgt_lang],\n            use_fast=False,\n        )\n    else:\n        print(cfg.model_name)\n        tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n\n    return tokenizer", "\n\ndef load_model(cfg):\n    if \"mbart\" in cfg.model_name:\n        model = MBartForConditionalGeneration.from_pretrained(cfg.model_name).to(\n            cfg.device\n        )\n    else:\n        model = AutoModelForSeq2SeqLM.from_pretrained(cfg.model_name).to(cfg.device)\n\n    return model", "\n\ndef load_dataset(tokenizer, cfg):\n    # Wmt-xx-xx-xx\n    if cfg.name == \"wmt\":\n        split = cfg.split\n        if cfg.subset.use_subset:\n            split = f\"{cfg.split}[{cfg.subset.start}:{cfg.subset.end + 1}]\"\n\n        dataset = Wmt(\n            version=cfg.version,\n            src_lan=cfg.src_lang,\n            tgt_lan=cfg.tgt_lang,\n            hugginface_tokenizer=tokenizer,\n            split=split,\n        )\n    # Iwsltxx-xx-xx\n    elif cfg.name == \"iwslt\":\n        dataset = Iwslt(\n            data_dir=cfg.data_dir,\n            version=str(cfg.version),\n            src_lan=cfg.src_lang,\n            tgt_lan=cfg.tgt_lang,\n            hugginface_tokenizer=tokenizer,\n            split=cfg.split,\n        )\n    elif cfg.name == \"ittb\":\n        dataset = Ittb(\n            src_lan=cfg.src_lang,\n            tgt_lan=cfg.tgt_lang,\n            hugginface_tokenizer=tokenizer,\n            split=cfg.split,\n        )\n    elif cfg.name == \"flores\":\n        dataset = Flores(\n            src_lan=cfg.src_lang,\n            tgt_lan=cfg.tgt_lang,\n            hugginface_tokenizer=tokenizer,\n            split=cfg.split,\n        )\n    else:\n        raise ValueError(f\"{cfg.dataset.name} is not yet implemented\")\n\n    return dataset", "\n\ndef load_initializer(tokenizer, cfg):\n    if cfg.use_initializer:\n        initializer = Initializer(\n            src_len=cfg.src_lang,\n            tgt_len=cfg.tgt_lang,\n            hugginface_tokenizer=tokenizer,\n            use_init=cfg.use_init,\n            device=cfg.device,\n        )\n\n    else:\n        initializer = None\n\n    return initializer", "\n\ndef compute_beam_search(cfg, model, dataset):\n    initializer = load_initializer(dataset.tokenizer, cfg.initializer)\n\n    decoder = MTDecoder(\n        tokenizer=dataset.tokenizer,\n        model=model,\n        use_cache=cfg.decoder.use_cache,\n        gs_jaco_blocks=cfg.decoder.gs_jaco_blocks,\n        device=cfg.device,\n        initializer=initializer\n    )\n\n    beam_searcher = BeamSearcher(\n        model=model,\n        dataset=dataset,\n        initializer=initializer,\n        decoder=decoder,\n        batch_size=cfg.beam_search.batch_size,\n        num_beams=cfg.beam_search.num_beams,\n        device=cfg.beam_search.device,\n        no_repeat_ngram_size=2,\n        early_stopping=True,\n        result_dir=cfg.beam_search.result_dir,\n    )\n\n    beam_searcher.compute_beam_search(cfg)", "\n\ndef load_decoders(cfg, tokenizer, model, initializer):\n    decoders = []\n    for decoder in cfg.decoder.decoders:\n        if decoder == \"autoregressive\":\n            dec = AutoregressiveDecoder(\n                tokenizer=tokenizer,\n                model=model,\n                initializer=initializer,\n                use_cache=cfg.decoder.use_cache,\n                device=cfg.decoder.device\n            )\n        elif decoder == \"jacobi\":\n            dec = JacobiDecoder(\n                tokenizer=tokenizer,\n                model=model,\n                initializer=initializer,\n                use_cache=cfg.decoder.use_cache,\n                device=cfg.decoder.device\n            )\n        elif decoder == \"gs_jacobi\":\n            dec = GSJacobiDecoder(\n                tokenizer=tokenizer,\n                model=model,\n                initializer=initializer,\n                gs_jaco_blocks=cfg.decoder.gs_jaco_blocks,\n                init_mode=\"\",\n                use_cache=cfg.decoder.use_cache,\n                device=cfg.decoder.device\n            )\n        elif decoder == \"h_jacobi\":\n            dec = HybridJacobiDecoder(\n                tokenizer=tokenizer,\n                model=model,\n                initializer=initializer,\n                init_mode=\"fixed\",\n                gs_jaco_blocks=cfg.decoder.gs_jaco_blocks,\n                use_cache=cfg.decoder.use_cache,\n                device=cfg.decoder.device\n            )\n        elif decoder == \"beam_search\":\n            dec = BeamSearchDecoder(\n                tokenizer=tokenizer,\n                model=model,\n                initializer=initializer,\n                num_beams=cfg.beam_search.num_beams,\n                early_stopping=True,\n            )\n        else:\n            raise ValueError(f\"{decoder} decoder have not been implemented yet.\")\n\n        decoders.append(dec)\n\n    return decoders", "\n\ndef compute_benchmark(cfg, tokenizer, dataset, model):\n    initializer = load_initializer(tokenizer, cfg.initializer)\n    decoders = load_decoders(cfg, tokenizer, model, initializer)\n\n    benchmarker = MTBenchmarker(\n        dataset=dataset,\n        decoders=decoders,\n        src_lang=cfg.model.src_lang,\n        tgt_lang=cfg.model.tgt_lang,\n        result_dir=cfg.bench.result_dir,\n        device=cfg.bench.device,\n        debug=True,\n    )\n    benchmarker.compute_total_time()", "\n\n@hydra.main(config_path=\"conf\", config_name=\"config\", version_base=\"1.1\")\ndef main(cfg):\n    tokenizer = load_tokenizer(cfg.model)\n\n    model = load_model(cfg.model)\n\n    dataset = load_dataset(tokenizer, cfg.dataset)\n\n    if \"benchmark\" in cfg.task:\n        compute_benchmark(cfg, tokenizer, dataset, model)\n    elif \"beam_search\" in cfg.task:\n        compute_beam_search(cfg, model, dataset)\n    elif \"sample\" in cfg.task:\n        retrieve_samples(cfg.sample.path, dataset)\n    else:\n        raise ValueError(f\"{cfg.task} is not yet implemented\")", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "exp/flops_calculator.py", "chunked_list": ["\"\"\"Computes the flops needed for training/running transformer networks. Adapted from https://github.com/google-research/electra/blob/master/flops_computation.py \"\"\"\n\nimport collections\n\n# We checked this code with TensorFlow\"s FLOPs counting, although we had to\n# correct for this issue: https://github.com/tensorflow/tensorflow/issues/22071\n# Assumptions going into the FLOPs counting\n#   - An \"operation\" is a mathematical operation, not a machine instruction. So\n#     an \"exp\" takes one opp like and add, even though in practice an exp\n#     might be slower. This is not too bad an assumption because", "#     an \"exp\" takes one opp like and add, even though in practice an exp\n#     might be slower. This is not too bad an assumption because\n#     matrix-multiplies dominate the compute for most models, so minor details\n#     about activation functions don\"t matter too much. Similarly, we count\n#     matrix-multiplies as 2*m*n flops instead of m*n, as one might if\n#     if considering fused multiply-add ops.\n#   - Backward pass takes the same number of FLOPs as forward pass. No exactly\n#     right (e.g., for softmax cross entropy loss the backward pass is faster).\n#     Importantly, it really is the same for matrix-multiplies, which is most of\n#     the compute anyway.", "#     Importantly, it really is the same for matrix-multiplies, which is most of\n#     the compute anyway.\n#   - We assume \"dense\" embedding lookups (i.e., multiplication by a one-hot\n#     vector). On some hardware accelerators, these dense operations are\n#     actually faster than sparse lookups.\n# Please open a github issue if you spot a problem with this code!\n\n# I am not sure if the below constants are 100% right, but they are only applied\n# to O(hidden_size) activations, which is generally a lot less compute than the\n# matrix-multiplies, which are O(hidden_size^2), so they don't affect the total", "# to O(hidden_size) activations, which is generally a lot less compute than the\n# matrix-multiplies, which are O(hidden_size^2), so they don't affect the total\n# number of FLOPs much.\n\n# random number, >=, multiply activations by dropout mask, multiply activations\n# by correction (1 / (1 - dropout_rate))\nDROPOUT_FLOPS = 4\n\n# compute mean activation (sum), computate variance of activation\n# (square and sum), bias (add), scale (multiply)", "# compute mean activation (sum), computate variance of activation\n# (square and sum), bias (add), scale (multiply)\nLAYER_NORM_FLOPS = 5\n\n# GELU: 0.5 * x * (1 + tanh(sqrt(2 / np.pi) * (x + 0.044715 * pow(x, 3))))\nACTIVATION_FLOPS = 8\n\n# max/substract (for stability), exp, sum, divide\nSOFTMAX_FLOPS = 5\n", "SOFTMAX_FLOPS = 5\n\n\nclass TransformerHparams(object):\n  \"\"\"Computes the train/inference FLOPs for transformers.\"\"\"\n\n  def __init__(self, h, l, s=512, v=30522, e=None, i=None, heads=None,\n      head_size=None, output_frac=0.15625, sparse_embed_lookup=False,\n      decoder=False):\n    self.h = h  # hidden size\n    self.l = l  # number of layers\n    self.s = s  # sequence length\n    self.v = v  # vocab size\n    self.e = h if e is None else e  # embedding size\n    self.i = h * 4 if i is None else i  # intermediate size\n    self.kqv = h if head_size is None else head_size * heads  # attn proj sizes\n    self.heads = max(h // 64, 1) if heads is None else heads  # attention heads\n    self.output_frac = output_frac  # percent of tokens using an output softmax\n    self.sparse_embed_lookup = sparse_embed_lookup  # sparse embedding lookups\n    self.decoder = decoder  # decoder has extra attn to encoder states\n\n  def get_block_flops(self):\n    \"\"\"Get the forward-pass FLOPs for a single transformer block.\"\"\"\n    attn_mul = 2 if self.decoder else 1\n    block_flops = dict(\n        kqv=3 * 2 * self.h * self.kqv * attn_mul,\n        kqv_bias=3 * self.kqv * attn_mul,\n        attention_scores=2 * self.kqv * self.s * attn_mul,\n        attn_softmax=SOFTMAX_FLOPS * self.s * self.heads * attn_mul,\n        attention_dropout=DROPOUT_FLOPS * self.s * self.heads * attn_mul,\n        attention_scale=self.s * self.heads * attn_mul,\n        attention_weighted_avg_values=2 * self.h * self.s * attn_mul,\n        attn_output=2 * self.h * self.h * attn_mul,\n        attn_output_bias=self.h * attn_mul,\n        attn_output_dropout=DROPOUT_FLOPS * self.h * attn_mul,\n        attn_output_residual=self.h * attn_mul,\n        attn_output_layer_norm=LAYER_NORM_FLOPS * attn_mul,\n        intermediate=2 * self.h * self.i,\n        intermediate_act=ACTIVATION_FLOPS * self.i,\n        intermediate_bias=self.i,\n        output=2 * self.h * self.i,\n        output_bias=self.h,\n        output_dropout=DROPOUT_FLOPS * self.h,\n        output_residual=self.h,\n        output_layer_norm=LAYER_NORM_FLOPS * self.h,\n    )\n    return sum(block_flops.values()) * self.s\n\n  def get_embedding_flops(self, output=False):\n    \"\"\"Get the forward-pass FLOPs the transformer inputs or output softmax.\"\"\"\n    embedding_flops = {}\n    if output or (not self.sparse_embed_lookup):\n      embedding_flops[\"main_multiply\"] = 2 * self.e * self.v\n    # input embedding post-processing\n    if not output:\n      embedding_flops.update(dict(\n          tok_type_and_position=2 * self.e * (self.s + 2),\n          add_tok_type_and_position=2 * self.e,\n          emb_layer_norm=LAYER_NORM_FLOPS * self.e,\n          emb_dropout=DROPOUT_FLOPS * self.e\n      ))\n    # projection layer if e != h\n    if self.e != self.h or output:\n      embedding_flops.update(dict(\n          hidden_kernel=2 * self.h * self.e,\n          hidden_bias=self.e if output else self.h\n      ))\n      # extra hidden layer and output softmax\n      if output:\n        embedding_flops.update(dict(\n            hidden_activation=ACTIVATION_FLOPS * self.e,\n            hidden_layernorm=LAYER_NORM_FLOPS * self.e,\n            output_softmax=SOFTMAX_FLOPS * self.v,\n            output_target_word=2 * self.v\n        ))\n        return self.output_frac * sum(embedding_flops.values()) * self.s\n    return sum(embedding_flops.values()) * self.s\n\n  def get_binary_classification_flops(self):\n    classification_flops = dict(\n        hidden=2 * self.h * self.h,\n        hidden_bias=self.h,\n        hidden_act=ACTIVATION_FLOPS * self.h,\n        logits=2 * self.h\n    )\n    return sum(classification_flops.values()) * self.s\n\n  def get_train_flops(self, batch_size, train_steps, discriminator=False, use_backprop=True):\n    \"\"\"Get the FLOPs for pre-training the transformer.\"\"\"\n    # 2* for forward/backward pass\n    if use_backprop:\n        mult = 2\n    else:\n        mult = 1\n    return mult * batch_size * train_steps * (\n        (self.l * self.get_block_flops()) +\n        self.get_embedding_flops(output=False) +\n        (self.get_binary_classification_flops() if discriminator else\n         self.get_embedding_flops(output=True))\n    )\n\n  def get_infer_flops(self):\n    \"\"\"Get the FLOPs for running inference with the transformer on a\n    classification task.\"\"\"\n    return ((self.l * self.get_block_flops()) +\n            self.get_embedding_flops(output=False) +\n            self.get_binary_classification_flops())", "\n\ndef get_electra_train_flops(\n    h_d, l_d, h_g, l_g, batch_size, train_steps, tied_embeddings,\n    e=None, s=512, output_frac=0.15625):\n  \"\"\"Get the FLOPs needed for  pre-training ELECTRA.\"\"\"\n  if e is None:\n    e = h_d\n  disc = TransformerHparams(\n      h_d, l_d, s=s, e=e,\n      output_frac=output_frac).get_train_flops(batch_size, train_steps, True)\n  gen = TransformerHparams(\n      h_g, l_g, s=s, e=e if tied_embeddings else None,\n      output_frac=output_frac).get_train_flops(batch_size, train_steps)\n  return disc + gen", "\n\n\ndef calculate_LASSFlops(prior_flop):\n    vq_vaeflops = 0\n\nMODEL_FLOPS = collections.OrderedDict([\n    # These runtimes were computed with tensorflow FLOPs counting instead of the\n    # script, as the neural architectures are quite different.\n    # 768648884 words in LM1b benchmark, 10 epochs with batch size 20,", "    # script, as the neural architectures are quite different.\n    # 768648884 words in LM1b benchmark, 10 epochs with batch size 20,\n    # seq length 128, 568093262680 FLOPs per example.\n    (\"elmo\", 2 * 10 * 768648884 * 568093262680 / (20.0 * 128)),\n    # 15064773691518 is FLOPs for forward pass on 32 examples.\n    # Therefore 2 * steps * batch_size * 15064773691518 / 32 is XLNet compute\n    (\"xlnet\", 2 * 500000 * 8192 * 15064773691518 / 32.0),\n\n    # Runtimes computed with the script\n    (\"gpt\", TransformerHparams(768, 12, v=40000, output_frac=1.0).get_train_flops(", "    # Runtimes computed with the script\n    (\"gpt\", TransformerHparams(768, 12, v=40000, output_frac=1.0).get_train_flops(\n        128, 960800)),\n\n    (\"jukebox\", TransformerHparams(1024, 48, s=8192, v=2048, output_frac=1.0, heads=1).get_infer_flops()),\n\n\n    (\"bert_small\", TransformerHparams(256, 12, e=128, s=128).get_train_flops(128, 1.45e6)),\n    (\"bert_base\", TransformerHparams(768, 12).get_train_flops(256, 1e6)),\n    (\"bert_large\", TransformerHparams(1024, 24).get_train_flops(256, 1e6)),", "    (\"bert_base\", TransformerHparams(768, 12).get_train_flops(256, 1e6)),\n    (\"bert_large\", TransformerHparams(1024, 24).get_train_flops(256, 1e6)),\n    (\"electra_small\", get_electra_train_flops(256, 12, 64, 12, 128, 1e6, True, s=128, e=128)),\n    (\"electra_base\", get_electra_train_flops(768, 12, 256, 12, 256, 766000, True)),\n    (\"electra_400k\", get_electra_train_flops(1024, 24, 256, 24, 2048, 400000, True)),\n    (\"electra_1.75M\", get_electra_train_flops(1024, 24, 256, 24, 2048, 1750000, True)),\n\n    # RoBERTa, ALBERT, and T5 have  minor architectural differences from\n    # BERT/ELECTRA, but I believe they don't significantly effect the runtime,\n    # so we use this script for those models as well.", "    # BERT/ELECTRA, but I believe they don't significantly effect the runtime,\n    # so we use this script for those models as well.\n    (\"roberta\", TransformerHparams(1024, 24, v=50265).get_train_flops(8000, 500000)),\n    (\"albert\", TransformerHparams(4096, 12, v=30000, e=128).get_train_flops(\n        4096, 1.5e6)),\n    (\"t5_11b\", TransformerHparams(\n        1024,  # hidden size\n        24,  # layers\n        v=32000,  # vocab size\n        i=65536,  # ff intermediate hidden size", "        v=32000,  # vocab size\n        i=65536,  # ff intermediate hidden size\n        heads=128, head_size=128,  # heads/head size\n        output_frac=0.0  # encoder has no output softmax\n    ).get_train_flops(2048, 1e6) +  # 1M steps with batch size 2048\n     TransformerHparams(\n         1024,\n         24,\n         v=32000,\n         i=65536,", "         v=32000,\n         i=65536,\n         heads=128, head_size=128,\n         output_frac=1.0,  # decoder has output softmax for all positions\n         decoder=True\n     ).get_train_flops(2048, 1e6)),\n    (\"Shallow\", TransformerHparams(512, 13, i=2048).get_train_flops(250, 300000)),\n    (\"Shallow + KD\", TransformerHparams(512, 13, i=2048).get_train_flops(250, 300000) + TransformerHparams(512, 13, i=2048,\n                                                                                                            heads=16).get_train_flops(\n                250, 300000, use_backprop=False)),", "                                                                                                            heads=16).get_train_flops(\n                250, 300000, use_backprop=False)),\n    (\"DSLP\", TransformerHparams(512, 12, i=2048).get_train_flops(250, 300000) + TransformerHparams(512, 12, i=2048,\n                                                                                                        heads=16).get_train_flops(\n            250, 300000, use_backprop=False)),\n    (\"F-VAE\", TransformerHparams(512, 12, i=2048).get_train_flops(250, 300000) + TransformerHparams(512, 12, i=2048,\n                                                                                                    heads=16).get_train_flops(\n        250, 300000, use_backprop=False)),\n    (\"DisCo\", TransformerHparams(512, 12, i=2048).get_train_flops(250, 300000) + TransformerHparams(1024, 24, i=4096, heads=16).get_train_flops(250, 300000, use_backprop=False)),\n    (\"SUNDAE\", TransformerHparams(512, 12, i=2048).get_train_flops(4096, 10e6)),", "    (\"DisCo\", TransformerHparams(512, 12, i=2048).get_train_flops(250, 300000) + TransformerHparams(1024, 24, i=4096, heads=16).get_train_flops(250, 300000, use_backprop=False)),\n    (\"SUNDAE\", TransformerHparams(512, 12, i=2048).get_train_flops(4096, 10e6)),\n\n])\n\n\ndef main():\n  for k, v in MODEL_FLOPS.items():\n    print(k, v)\n", "\n\nif __name__ == \"__main__\":\n  main()\n"]}
{"filename": "src/bench.py", "chunked_list": ["import csv\nimport logging\nimport os\nimport sys\nimport time\n\nimport torch\nfrom tabulate import tabulate\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm", "from torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\n\nfrom src.utils import utils\nfrom src.utils.bench_scorer import Scorer\nfrom src.utils.utils import retrieve_model_name, check_zero_division\n\nlogging.basicConfig(\n    format=\"%(asctime)s | %(levelname)s | %(name)s |  [%(filename)s:%(lineno)d] %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",", "    format=\"%(asctime)s | %(levelname)s | %(name)s |  [%(filename)s:%(lineno)d] %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\"bench\")\n\n\nclass MTBenchmarker(object):\n    def __init__(\n            self,\n            dataset: Dataset,\n            decoders,\n            src_lang,\n            tgt_lang,\n            compare_to: str = \"autoregressive\",\n            result_dir: str = None,\n            device: str = \"cuda\",\n            debug: bool = False,\n    ):\n        self.dataset = dataset\n        self.decoders = decoders\n        self.device = device\n        self.debug = debug\n\n        self.compare_to = compare_to\n\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.model_name = self.decoders[0].model_name.split(\"/\")[1]\n\n        self.dataloader = DataLoader(\n            dataset, collate_fn=dataset.collate_fn, batch_size=1, shuffle=False\n        )\n\n        self.result_dir = result_dir\n        self.exp_dir = self._retrieve_exp_dir()\n\n    def _synchronize(self):\n        if self.device == \"cuda\":\n            torch.cuda.synchronize()\n\n    def _retrieve_exp_dir(self):\n        file_name = self._retrieve_file_name()\n\n        exp_dir = os.path.join(self.result_dir, \"benchmark\", file_name)\n        utils.makedirs(exp_dir)\n\n        return exp_dir\n\n    def _retrieve_file_name(self):\n        model_name = retrieve_model_name(self.model_name)\n        lang = f\"{self.src_lang}_{self.tgt_lang}\"\n        return f\"{model_name}/{self.device}/{self.dataset.name}/{lang}\"\n\n    @staticmethod\n    def _write_on_file(path, item, i):\n\n        utils.makedirs(path)\n        with open(path, \"a\") as file:\n            writer = csv.writer(file, delimiter=\"\\t\")\n            # write header\n            if os.stat(path).st_size == 0:\n                writer.writerow([\"i\", \"item\"])\n            writer.writerow([i, item])\n\n    def _compute_info_table(self, best_alg, i, grid):\n        # Table for the test info\n        info_table = tabulate([\n                [\n                    self.decoders[0].model_name,\n                    self.dataset.name,\n                    self.device,\n                    best_alg,\n                    i,\n                    f\"{self.src_lang}-{self.tgt_lang}\",\n                ]\n            ],\n            headers=[\n                \"Model\",\n                \"Dataset\",\n                \"Device\",\n                \"Best Algorithm\",\n                \"Sentences\",\n                \"Languages\",\n            ],\n            tablefmt=grid,\n        )\n\n        return info_table\n\n    def _compute_time_table(self, scorers, grid):\n\n        if len(scorers) == 1:\n            name, scorer = list(scorers.items())[0]\n            times_table = tabulate([\n                [\"Time\", scorer.tot_mean_time],\n                [\"Iteration\", scorer.tot_mean_iter],\n            ],\n                headers=[\"Metrics\", name], tablefmt=grid,\n            )\n        else:\n            tests_header = [\"Metrics\"] + [name for name in scorers]\n\n            comp_scorer = scorers.get(self.compare_to)\n            time_speedup = [check_zero_division(comp_scorer.tot_mean_time, scorer.tot_mean_time) for scorer in scorers.values()]\n            iter_speedup = [check_zero_division(comp_scorer.tot_mean_iter, scorer.tot_mean_iter) for scorer in scorers.values()]\n\n            times_table = tabulate(\n                [\n                    [\"Speedup T\"] + time_speedup,\n                    [\"Speedup I\"] + iter_speedup,\n                    [\"Time\"] + [scorer.tot_mean_time for scorer in scorers.values()],\n                    [\"Iter\"] + [scorer.tot_mean_iter for scorer in scorers.values()],\n                ],\n                headers=tests_header, tablefmt=grid,\n            )\n\n        return times_table\n\n    def _compute_bleu_table(self, scorers, grid):\n\n        bleu_scores = [scorer.compute_bleu_score() for scorer in scorers.values()]\n\n        # Table for the Bleu score\n        bleu_table = tabulate([\n            ['Score'] + [score.score for score in bleu_scores],\n            ['Counts'] + [score.counts for score in bleu_scores],\n            ['Totals'] + [score.totals for score in bleu_scores],\n            ['Precisions'] + [score.precisions for score in bleu_scores],\n            ['Bp'] + [score.bp for score in bleu_scores],\n            ['Sys_len'] + [score.sys_len for score in bleu_scores],\n            ['ref_len'] + [score.ref_len for score in bleu_scores],\n        ], headers=[\"Metrics\"] + [name for name in scorers], tablefmt=grid)\n\n        return bleu_table\n\n    def write_report(self, i, scorers, best_algorithm):\n        print(\"Writing report...\")\n\n        # Compute best algorithm\n        best_alg = max(best_algorithm, key=best_algorithm.get)\n        info_table_txt = self._compute_info_table(best_alg, i, grid=\"grid\")\n        info_table_tex = self._compute_info_table(best_alg, i, grid=\"latex\")\n\n        # Table for the benchmark times\n        times_table_txt = self._compute_time_table(scorers, grid=\"rst\")\n        times_table_tex = self._compute_time_table(scorers, grid=\"latex\")\n\n        # Table for the bleu score\n        bleu_table_txt = self._compute_bleu_table(scorers, grid=\"rst\")\n\n        print(self.exp_dir)\n\n        with open(os.path.join(self.exp_dir, \"report.txt\"), mode=\"w\") as report:\n            report.write(f\"Test Info\\n{info_table_txt}\\n\\n\")\n            report.write(f\"Benchmark\\n{times_table_txt}\\n\\n\")\n            report.write(f\"Bleu\\n{bleu_table_txt}\\n\\n\")\n\n        with open(os.path.join(self.exp_dir, \"latex.txt\"), mode=\"w\") as report:\n            report.write(f\"Test Info\\n{info_table_tex}\\n\\n\")\n            report.write(f\"Benchmark\\n{times_table_tex}\\n\\n\")\n\n    def write_inline(self, i: int, scorers, best_alg):\n\n        for name, scorer in scorers.items():\n            # Write times\n            path = os.path.join(self.exp_dir, name, f\"{name}.tsv\")\n            self._write_on_file(path, scorer.current_time, i)\n            # Write iterations\n            path = os.path.join(self.exp_dir, name, f\"iter_{name}.tsv\")\n            self._write_on_file(path, scorer.current_iter, i)\n            # Write translations\n            path = os.path.join(self.exp_dir, name, f\"trans_{name}.tsv\")\n            self._write_on_file(path, scorer.current_transl, i)\n            # Write initializations\n            path = os.path.join(self.exp_dir, name, f\"init_{name}.tsv\")\n            self._write_on_file(path, scorer.current_init, i)\n\n        # Write mean\n        path = os.path.join(self.exp_dir, \"meanvar.tsv\")\n        utils.makedirs(path)\n        with open(path, \"a\") as file:\n            writer = csv.writer(file, delimiter=\"\\t\")\n\n            # Write header\n            if os.stat(path).st_size == 0:\n                header = [\"#sentence\"] + [f\"mean_{name}\" for name in scorers] + [\"best_alg\"]\n                writer.writerow(header)\n\n            row = [i] + [scorer.current_time for scorer in scorers.values()] + [best_alg]\n            writer.writerow(row)\n\n    @staticmethod\n    def _compute_best_algorithm(scorers):\n\n        best = scorers.get(min(scorers, key=lambda x: scorers[x].current_time)).acronym\n\n        return best\n\n    def _compute_postfix(self, scorers, best_algorithms, curr_alg):\n\n        best_alg = max(best_algorithms, key=best_algorithms.get)\n\n        if len(scorers) == 1:\n            _, scorer = list(scorers.items())[0]\n            postfix = {\n                scorer.acronym: (\n                    round(scorer.tot_mean_time, 3),\n                    round(scorer.tot_mean_iter, 3)\n                ),\n                \"ca\": curr_alg,\n                \"ba\": best_alg,\n            }\n\n        else:\n            comp_scorer = scorers.get(self.compare_to)\n\n            postfix = {\n                scorer.acronym: (\n                    check_zero_division(comp_scorer.tot_mean_time, scorer.tot_mean_time),\n                    check_zero_division(comp_scorer.tot_mean_iter, scorer.tot_mean_iter)\n                ) for name, scorer in scorers.items() if name != self.compare_to\n            }\n\n            postfix.update({\"ca\": curr_alg, \"ba\": best_alg})\n\n        return postfix\n\n    def compute_total_time(self):\n\n        i = 0\n        scorers = {decoder.name: Scorer(decoder.name, decoder.acronym) for decoder in self.decoders}\n        best_algorithms = {decoder.acronym: 0 for decoder in self.decoders}\n\n        pbar = tqdm(self.dataloader, desc=\"Computing Benchmark...\")\n        for x in pbar:\n\n            try:\n                input_ids = x[\"source\"][\"input_ids\"].to(self.device)\n                attention_mask = x[\"source\"][\"attention_mask\"].to(self.device)\n\n                tgt_text = x['target']['sentences']\n\n                for decoder, name in zip(self.decoders, scorers):\n                    kwargs = decoder.compute_decode_kwargs(input_ids, attention_mask)\n\n                    start = time.perf_counter()\n                    self._synchronize()\n                    trans, iter = decoder.decode(input_ids, attention_mask, **kwargs)\n                    self._synchronize()\n                    end = time.perf_counter()\n                    sample_time = end - start\n\n                    init_tensor = kwargs[\"init_tensor\"] if \"init_tensor\" in kwargs else \"\"\n\n                    trans = self.dataset.tokenizer.batch_decode(\n                        trans, skip_special_tokens=True\n                    )[0]\n\n                    scorers[name].update_metrics(sample_time, iter, trans, tgt_text[0], init_tensor)\n\n                best_alg = self._compute_best_algorithm(scorers)\n                best_algorithms[best_alg] += 1\n\n                # Update tqdm bar\n                postfix_pbar = self._compute_postfix(scorers, best_algorithms, best_alg)\n                pbar.set_postfix(postfix_pbar)\n\n                self.write_inline(i, scorers, best_alg)\n            except Exception as e:\n                if self.debug:\n                    raise e\n                else:\n                    logger.error(e)\n                return\n\n            i += 1\n\n        self.write_report(i, scorers, best_algorithms)", "class MTBenchmarker(object):\n    def __init__(\n            self,\n            dataset: Dataset,\n            decoders,\n            src_lang,\n            tgt_lang,\n            compare_to: str = \"autoregressive\",\n            result_dir: str = None,\n            device: str = \"cuda\",\n            debug: bool = False,\n    ):\n        self.dataset = dataset\n        self.decoders = decoders\n        self.device = device\n        self.debug = debug\n\n        self.compare_to = compare_to\n\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.model_name = self.decoders[0].model_name.split(\"/\")[1]\n\n        self.dataloader = DataLoader(\n            dataset, collate_fn=dataset.collate_fn, batch_size=1, shuffle=False\n        )\n\n        self.result_dir = result_dir\n        self.exp_dir = self._retrieve_exp_dir()\n\n    def _synchronize(self):\n        if self.device == \"cuda\":\n            torch.cuda.synchronize()\n\n    def _retrieve_exp_dir(self):\n        file_name = self._retrieve_file_name()\n\n        exp_dir = os.path.join(self.result_dir, \"benchmark\", file_name)\n        utils.makedirs(exp_dir)\n\n        return exp_dir\n\n    def _retrieve_file_name(self):\n        model_name = retrieve_model_name(self.model_name)\n        lang = f\"{self.src_lang}_{self.tgt_lang}\"\n        return f\"{model_name}/{self.device}/{self.dataset.name}/{lang}\"\n\n    @staticmethod\n    def _write_on_file(path, item, i):\n\n        utils.makedirs(path)\n        with open(path, \"a\") as file:\n            writer = csv.writer(file, delimiter=\"\\t\")\n            # write header\n            if os.stat(path).st_size == 0:\n                writer.writerow([\"i\", \"item\"])\n            writer.writerow([i, item])\n\n    def _compute_info_table(self, best_alg, i, grid):\n        # Table for the test info\n        info_table = tabulate([\n                [\n                    self.decoders[0].model_name,\n                    self.dataset.name,\n                    self.device,\n                    best_alg,\n                    i,\n                    f\"{self.src_lang}-{self.tgt_lang}\",\n                ]\n            ],\n            headers=[\n                \"Model\",\n                \"Dataset\",\n                \"Device\",\n                \"Best Algorithm\",\n                \"Sentences\",\n                \"Languages\",\n            ],\n            tablefmt=grid,\n        )\n\n        return info_table\n\n    def _compute_time_table(self, scorers, grid):\n\n        if len(scorers) == 1:\n            name, scorer = list(scorers.items())[0]\n            times_table = tabulate([\n                [\"Time\", scorer.tot_mean_time],\n                [\"Iteration\", scorer.tot_mean_iter],\n            ],\n                headers=[\"Metrics\", name], tablefmt=grid,\n            )\n        else:\n            tests_header = [\"Metrics\"] + [name for name in scorers]\n\n            comp_scorer = scorers.get(self.compare_to)\n            time_speedup = [check_zero_division(comp_scorer.tot_mean_time, scorer.tot_mean_time) for scorer in scorers.values()]\n            iter_speedup = [check_zero_division(comp_scorer.tot_mean_iter, scorer.tot_mean_iter) for scorer in scorers.values()]\n\n            times_table = tabulate(\n                [\n                    [\"Speedup T\"] + time_speedup,\n                    [\"Speedup I\"] + iter_speedup,\n                    [\"Time\"] + [scorer.tot_mean_time for scorer in scorers.values()],\n                    [\"Iter\"] + [scorer.tot_mean_iter for scorer in scorers.values()],\n                ],\n                headers=tests_header, tablefmt=grid,\n            )\n\n        return times_table\n\n    def _compute_bleu_table(self, scorers, grid):\n\n        bleu_scores = [scorer.compute_bleu_score() for scorer in scorers.values()]\n\n        # Table for the Bleu score\n        bleu_table = tabulate([\n            ['Score'] + [score.score for score in bleu_scores],\n            ['Counts'] + [score.counts for score in bleu_scores],\n            ['Totals'] + [score.totals for score in bleu_scores],\n            ['Precisions'] + [score.precisions for score in bleu_scores],\n            ['Bp'] + [score.bp for score in bleu_scores],\n            ['Sys_len'] + [score.sys_len for score in bleu_scores],\n            ['ref_len'] + [score.ref_len for score in bleu_scores],\n        ], headers=[\"Metrics\"] + [name for name in scorers], tablefmt=grid)\n\n        return bleu_table\n\n    def write_report(self, i, scorers, best_algorithm):\n        print(\"Writing report...\")\n\n        # Compute best algorithm\n        best_alg = max(best_algorithm, key=best_algorithm.get)\n        info_table_txt = self._compute_info_table(best_alg, i, grid=\"grid\")\n        info_table_tex = self._compute_info_table(best_alg, i, grid=\"latex\")\n\n        # Table for the benchmark times\n        times_table_txt = self._compute_time_table(scorers, grid=\"rst\")\n        times_table_tex = self._compute_time_table(scorers, grid=\"latex\")\n\n        # Table for the bleu score\n        bleu_table_txt = self._compute_bleu_table(scorers, grid=\"rst\")\n\n        print(self.exp_dir)\n\n        with open(os.path.join(self.exp_dir, \"report.txt\"), mode=\"w\") as report:\n            report.write(f\"Test Info\\n{info_table_txt}\\n\\n\")\n            report.write(f\"Benchmark\\n{times_table_txt}\\n\\n\")\n            report.write(f\"Bleu\\n{bleu_table_txt}\\n\\n\")\n\n        with open(os.path.join(self.exp_dir, \"latex.txt\"), mode=\"w\") as report:\n            report.write(f\"Test Info\\n{info_table_tex}\\n\\n\")\n            report.write(f\"Benchmark\\n{times_table_tex}\\n\\n\")\n\n    def write_inline(self, i: int, scorers, best_alg):\n\n        for name, scorer in scorers.items():\n            # Write times\n            path = os.path.join(self.exp_dir, name, f\"{name}.tsv\")\n            self._write_on_file(path, scorer.current_time, i)\n            # Write iterations\n            path = os.path.join(self.exp_dir, name, f\"iter_{name}.tsv\")\n            self._write_on_file(path, scorer.current_iter, i)\n            # Write translations\n            path = os.path.join(self.exp_dir, name, f\"trans_{name}.tsv\")\n            self._write_on_file(path, scorer.current_transl, i)\n            # Write initializations\n            path = os.path.join(self.exp_dir, name, f\"init_{name}.tsv\")\n            self._write_on_file(path, scorer.current_init, i)\n\n        # Write mean\n        path = os.path.join(self.exp_dir, \"meanvar.tsv\")\n        utils.makedirs(path)\n        with open(path, \"a\") as file:\n            writer = csv.writer(file, delimiter=\"\\t\")\n\n            # Write header\n            if os.stat(path).st_size == 0:\n                header = [\"#sentence\"] + [f\"mean_{name}\" for name in scorers] + [\"best_alg\"]\n                writer.writerow(header)\n\n            row = [i] + [scorer.current_time for scorer in scorers.values()] + [best_alg]\n            writer.writerow(row)\n\n    @staticmethod\n    def _compute_best_algorithm(scorers):\n\n        best = scorers.get(min(scorers, key=lambda x: scorers[x].current_time)).acronym\n\n        return best\n\n    def _compute_postfix(self, scorers, best_algorithms, curr_alg):\n\n        best_alg = max(best_algorithms, key=best_algorithms.get)\n\n        if len(scorers) == 1:\n            _, scorer = list(scorers.items())[0]\n            postfix = {\n                scorer.acronym: (\n                    round(scorer.tot_mean_time, 3),\n                    round(scorer.tot_mean_iter, 3)\n                ),\n                \"ca\": curr_alg,\n                \"ba\": best_alg,\n            }\n\n        else:\n            comp_scorer = scorers.get(self.compare_to)\n\n            postfix = {\n                scorer.acronym: (\n                    check_zero_division(comp_scorer.tot_mean_time, scorer.tot_mean_time),\n                    check_zero_division(comp_scorer.tot_mean_iter, scorer.tot_mean_iter)\n                ) for name, scorer in scorers.items() if name != self.compare_to\n            }\n\n            postfix.update({\"ca\": curr_alg, \"ba\": best_alg})\n\n        return postfix\n\n    def compute_total_time(self):\n\n        i = 0\n        scorers = {decoder.name: Scorer(decoder.name, decoder.acronym) for decoder in self.decoders}\n        best_algorithms = {decoder.acronym: 0 for decoder in self.decoders}\n\n        pbar = tqdm(self.dataloader, desc=\"Computing Benchmark...\")\n        for x in pbar:\n\n            try:\n                input_ids = x[\"source\"][\"input_ids\"].to(self.device)\n                attention_mask = x[\"source\"][\"attention_mask\"].to(self.device)\n\n                tgt_text = x['target']['sentences']\n\n                for decoder, name in zip(self.decoders, scorers):\n                    kwargs = decoder.compute_decode_kwargs(input_ids, attention_mask)\n\n                    start = time.perf_counter()\n                    self._synchronize()\n                    trans, iter = decoder.decode(input_ids, attention_mask, **kwargs)\n                    self._synchronize()\n                    end = time.perf_counter()\n                    sample_time = end - start\n\n                    init_tensor = kwargs[\"init_tensor\"] if \"init_tensor\" in kwargs else \"\"\n\n                    trans = self.dataset.tokenizer.batch_decode(\n                        trans, skip_special_tokens=True\n                    )[0]\n\n                    scorers[name].update_metrics(sample_time, iter, trans, tgt_text[0], init_tensor)\n\n                best_alg = self._compute_best_algorithm(scorers)\n                best_algorithms[best_alg] += 1\n\n                # Update tqdm bar\n                postfix_pbar = self._compute_postfix(scorers, best_algorithms, best_alg)\n                pbar.set_postfix(postfix_pbar)\n\n                self.write_inline(i, scorers, best_alg)\n            except Exception as e:\n                if self.debug:\n                    raise e\n                else:\n                    logger.error(e)\n                return\n\n            i += 1\n\n        self.write_report(i, scorers, best_algorithms)", ""]}
{"filename": "src/__init__.py", "chunked_list": ["import logging\nimport os\nfrom pathlib import Path\nfrom typing import Optional\n\nimport dotenv\nimport git\n\npylogger = logging.getLogger(__name__)\n", "pylogger = logging.getLogger(__name__)\n\n\ndef get_env(env_name: str, default: Optional[str] = None) -> str:\n    \"\"\"Safely read an environment variable.\n    Raises errors if it is not defined or it is empty.\n    :param env_name: the name of the environment variable\n    :param default: the default (optional) value for the environment variable\n    :return: the value of the environment variable\n    \"\"\"\n    if env_name not in os.environ:\n        if default is None:\n            message = f\"{env_name} not defined and no default value is present!\"\n            pylogger.error(message)\n            raise KeyError(message)\n        return default\n\n    env_value: str = os.environ[env_name]\n    if not env_value:\n        if default is None:\n            message = (\n                f\"{env_name} has yet to be configured and no default value is present!\"\n            )\n            pylogger.error(message)\n            raise ValueError(message)\n        return default\n\n    return env_value", "\n\ndef load_envs(env_file: Optional[str] = None) -> None:\n    \"\"\"Load all the environment variables defined in the `env_file`.\n    This is equivalent to `. env_file` in bash.\n    It is possible to define all the system specific variables in the `env_file`.\n    :param env_file: the file that defines the environment variables to use. If None\n                     it searches for a `.env` file in the project.\n    \"\"\"\n    dotenv.load_dotenv(dotenv_path=env_file, override=True)", "\n\n# Load environment variables\nload_envs()\n\n\nif \"PROJECT_ROOT\" not in os.environ:\n    try:\n        PROJECT_ROOT = Path(\n            git.Repo(Path.cwd(), search_parent_directories=True).working_dir\n        )\n    except git.exc.InvalidGitRepositoryError:\n        PROJECT_ROOT = Path.cwd()\n\n    pylogger.debug(f\"Inferred project root: {PROJECT_ROOT}\")\n    os.environ[\"PROJECT_ROOT\"] = str(PROJECT_ROOT)\nelse:\n    PROJECT_ROOT: Path = Path(os.environ[\"PROJECT_ROOT\"])", "\n__all__ = [\"__version__\", \"PROJECT_ROOT\"]\n"]}
{"filename": "src/viz/dependecy_graph.py", "chunked_list": ["from typing import Tuple\n\nimport numpy as np\nimport plotly.express as px\nimport torch\nfrom transformers import MBart50Tokenizer\n\n\nclass DecodingDependencyGraph(object):\n    def __init__(self, model, tokenizer, gold_target: torch.Tensor):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.init_matrix = []\n        self.output_probs = []\n        self.gold_target = gold_target\n\n        if isinstance(tokenizer, MBart50Tokenizer):\n            self.is_mbart = True\n            self.starting_index = 2\n        else:\n            self.is_mbart = False\n            self.starting_index = 1\n\n    def insert_one_element(\n        self, current_tensor, gold_tensor, index=None, output_probs=None\n    ):\n        self.init_matrix.append(\n            (\n                current_tensor[:, self.starting_index :]\n                == gold_tensor[:, self.starting_index :]\n            )\n        )\n        if output_probs is not None:\n            self.output_probs.append(output_probs)\n\n    def finalize_matrices(self) -> Tuple[torch.Tensor, torch.Tensor]:\n        return (\n            torch.stack(self.init_matrix).permute(1, 0, 2).cpu(),\n            torch.stack(self.output_probs).permute(1, 0, 2).cpu(),\n        )\n\n    def _create_labels(self, sample_index: int, method):\n        sample_target = self.gold_target[sample_index, :].squeeze(0)\n\n        if method == \"decoded_ids\":\n            labels = self.tokenizer.convert_ids_to_tokens(sample_target)\n            labels = [\n                f\"{i}:{id}\"\n                for i, id in zip(\n                    labels[self.starting_index - 1 :],\n                    sample_target[self.starting_index - 1 :],\n                )\n            ]\n        elif method == \"basic\":\n            labels = [f\"{i}\" for i in sample_target[self.starting_index - 1 :].tolist()]\n\n        return labels\n\n    def pretty_print(\n        self, sample_index: int, sentence_id: str, method=\"basic\", x_remap=\"text\"\n    ):\n        labels = self._create_labels(sample_index=sample_index, method=method)\n        iteration_matrix, probability_matrix = self.finalize_matrices()\n        iteration_matrix = iteration_matrix[sample_index, :, :].int().numpy()\n        probability_matrix = probability_matrix[sample_index, :, 1:].numpy()\n\n        mask: np.ndarray = np.zeros_like(iteration_matrix)\n        i, j = 0, 0\n        while i < iteration_matrix.shape[0] and j < iteration_matrix.shape[1]:\n            if iteration_matrix[i, j]:\n                mask[i, j] += 1\n                j += 1\n            else:\n                mask[i, j:] = iteration_matrix[i, j:]\n                i += 1\n\n        probability_matrix = mask * probability_matrix\n\n        fig = px.imshow(\n            iteration_matrix + mask,\n            binary_compression_level=0,\n            title=f\"Decoding Dependency Graph for sentence {sentence_id}\",\n            color_continuous_scale=\"Viridis\",\n        )\n\n        fig.update_xaxes(\n            tickmode=\"array\",\n            tickvals=list(range(len(labels[1:]))),\n            ticktext=[\n                self.tokenizer.convert_ids_to_tokens([x])[0] if x_remap else str(x)\n                for x in labels[1:]\n            ],\n            tickangle=45,\n        )\n\n        fig.update_traces(\n            text=[\n                [f\"{xy:.2f}\" if xy > 0 else \"\" for xy in x] for x in probability_matrix\n            ],\n            texttemplate=\"%{text}\",\n        )\n\n        fig.update_layout(\n            font=dict(family=\"Courier New, monospace\", size=22, color=\"Black\"),\n            showlegend=False,\n            coloraxis_showscale=False,\n        )\n\n        fig.show()\n\n    def plot_confusion_matrix(\n        self, cm, target_names, title=\"Confusion matrix\", cmap=None, normalize=True\n    ):\n        \"\"\"\n        given a sklearn confusion matrix (cm), make a nice plot\n\n        Arguments\n        ---------\n        cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n        target_names: given classification classes such as [0, 1, 2]\n                                  the class names, for example: ['high', 'medium', 'low']\n\n        title:        the text to display at the top of the matrix\n\n        cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                                  see http://matplotlib.org/examples/color/colormaps_reference.html\n                                  plt.get_cmap('jet') or plt.cm.Blues\n\n        normalize:    If False, plot the raw numbers\n                                  If True, plot the proportions\n\n        Usage\n        -----\n        plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                                                                                          # sklearn.metrics.confusion_matrix\n                                                  normalize    = True,                # show proportions\n                                                  target_names = y_labels_vals,       # list of names of the classes\n                                                  title        = best_estimator_name) # title of graph\n\n        Citiation\n        ---------\n        http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n        \"\"\"\n        import itertools\n\n        import matplotlib.pyplot as plt\n        import numpy as np\n\n        accuracy = np.trace(cm) / np.sum(cm).astype(\"float\")\n        misclass = 1 - accuracy\n\n        if cmap is None:\n            cmap = plt.get_cmap(\"Blues\")\n\n        plt.figure(figsize=(8, 6))\n        plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n        plt.title(title)\n        plt.colorbar()\n\n        if target_names is not None:\n            tick_marks = np.arange(len(target_names))\n            plt.xticks(tick_marks, target_names, rotation=45)\n            plt.yticks(tick_marks, target_names)\n\n        if normalize:\n            cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n\n        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            if normalize:\n                plt.text(\n                    j,\n                    i,\n                    \"{:0.4f}\".format(cm[i, j]),\n                    horizontalalignment=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\",\n                )\n            else:\n                plt.text(\n                    j,\n                    i,\n                    \"{:,}\".format(cm[i, j]),\n                    horizontalalignment=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\",\n                )\n\n        plt.tight_layout()\n        plt.ylabel(\"True label\")\n        plt.xlabel(\n            \"Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}\".format(\n                accuracy, misclass\n            )\n        )\n        plt.show()", "class DecodingDependencyGraph(object):\n    def __init__(self, model, tokenizer, gold_target: torch.Tensor):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.init_matrix = []\n        self.output_probs = []\n        self.gold_target = gold_target\n\n        if isinstance(tokenizer, MBart50Tokenizer):\n            self.is_mbart = True\n            self.starting_index = 2\n        else:\n            self.is_mbart = False\n            self.starting_index = 1\n\n    def insert_one_element(\n        self, current_tensor, gold_tensor, index=None, output_probs=None\n    ):\n        self.init_matrix.append(\n            (\n                current_tensor[:, self.starting_index :]\n                == gold_tensor[:, self.starting_index :]\n            )\n        )\n        if output_probs is not None:\n            self.output_probs.append(output_probs)\n\n    def finalize_matrices(self) -> Tuple[torch.Tensor, torch.Tensor]:\n        return (\n            torch.stack(self.init_matrix).permute(1, 0, 2).cpu(),\n            torch.stack(self.output_probs).permute(1, 0, 2).cpu(),\n        )\n\n    def _create_labels(self, sample_index: int, method):\n        sample_target = self.gold_target[sample_index, :].squeeze(0)\n\n        if method == \"decoded_ids\":\n            labels = self.tokenizer.convert_ids_to_tokens(sample_target)\n            labels = [\n                f\"{i}:{id}\"\n                for i, id in zip(\n                    labels[self.starting_index - 1 :],\n                    sample_target[self.starting_index - 1 :],\n                )\n            ]\n        elif method == \"basic\":\n            labels = [f\"{i}\" for i in sample_target[self.starting_index - 1 :].tolist()]\n\n        return labels\n\n    def pretty_print(\n        self, sample_index: int, sentence_id: str, method=\"basic\", x_remap=\"text\"\n    ):\n        labels = self._create_labels(sample_index=sample_index, method=method)\n        iteration_matrix, probability_matrix = self.finalize_matrices()\n        iteration_matrix = iteration_matrix[sample_index, :, :].int().numpy()\n        probability_matrix = probability_matrix[sample_index, :, 1:].numpy()\n\n        mask: np.ndarray = np.zeros_like(iteration_matrix)\n        i, j = 0, 0\n        while i < iteration_matrix.shape[0] and j < iteration_matrix.shape[1]:\n            if iteration_matrix[i, j]:\n                mask[i, j] += 1\n                j += 1\n            else:\n                mask[i, j:] = iteration_matrix[i, j:]\n                i += 1\n\n        probability_matrix = mask * probability_matrix\n\n        fig = px.imshow(\n            iteration_matrix + mask,\n            binary_compression_level=0,\n            title=f\"Decoding Dependency Graph for sentence {sentence_id}\",\n            color_continuous_scale=\"Viridis\",\n        )\n\n        fig.update_xaxes(\n            tickmode=\"array\",\n            tickvals=list(range(len(labels[1:]))),\n            ticktext=[\n                self.tokenizer.convert_ids_to_tokens([x])[0] if x_remap else str(x)\n                for x in labels[1:]\n            ],\n            tickangle=45,\n        )\n\n        fig.update_traces(\n            text=[\n                [f\"{xy:.2f}\" if xy > 0 else \"\" for xy in x] for x in probability_matrix\n            ],\n            texttemplate=\"%{text}\",\n        )\n\n        fig.update_layout(\n            font=dict(family=\"Courier New, monospace\", size=22, color=\"Black\"),\n            showlegend=False,\n            coloraxis_showscale=False,\n        )\n\n        fig.show()\n\n    def plot_confusion_matrix(\n        self, cm, target_names, title=\"Confusion matrix\", cmap=None, normalize=True\n    ):\n        \"\"\"\n        given a sklearn confusion matrix (cm), make a nice plot\n\n        Arguments\n        ---------\n        cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n        target_names: given classification classes such as [0, 1, 2]\n                                  the class names, for example: ['high', 'medium', 'low']\n\n        title:        the text to display at the top of the matrix\n\n        cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                                  see http://matplotlib.org/examples/color/colormaps_reference.html\n                                  plt.get_cmap('jet') or plt.cm.Blues\n\n        normalize:    If False, plot the raw numbers\n                                  If True, plot the proportions\n\n        Usage\n        -----\n        plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                                                                                          # sklearn.metrics.confusion_matrix\n                                                  normalize    = True,                # show proportions\n                                                  target_names = y_labels_vals,       # list of names of the classes\n                                                  title        = best_estimator_name) # title of graph\n\n        Citiation\n        ---------\n        http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n        \"\"\"\n        import itertools\n\n        import matplotlib.pyplot as plt\n        import numpy as np\n\n        accuracy = np.trace(cm) / np.sum(cm).astype(\"float\")\n        misclass = 1 - accuracy\n\n        if cmap is None:\n            cmap = plt.get_cmap(\"Blues\")\n\n        plt.figure(figsize=(8, 6))\n        plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n        plt.title(title)\n        plt.colorbar()\n\n        if target_names is not None:\n            tick_marks = np.arange(len(target_names))\n            plt.xticks(tick_marks, target_names, rotation=45)\n            plt.yticks(tick_marks, target_names)\n\n        if normalize:\n            cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n\n        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            if normalize:\n                plt.text(\n                    j,\n                    i,\n                    \"{:0.4f}\".format(cm[i, j]),\n                    horizontalalignment=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\",\n                )\n            else:\n                plt.text(\n                    j,\n                    i,\n                    \"{:,}\".format(cm[i, j]),\n                    horizontalalignment=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\",\n                )\n\n        plt.tight_layout()\n        plt.ylabel(\"True label\")\n        plt.xlabel(\n            \"Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}\".format(\n                accuracy, misclass\n            )\n        )\n        plt.show()", ""]}
{"filename": "src/viz/__init__.py", "chunked_list": [""]}
{"filename": "src/viz/visualize.py", "chunked_list": ["import functools\nimport hashlib\nimport json\nfrom pathlib import Path\n\nimport argparse\nimport datasets\nimport numpy as np\nimport plotly.express as px\nimport torch.utils.data", "import plotly.express as px\nimport torch.utils.data\nfrom datasets import load_from_disk, Dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    PreTrainedTokenizer,\n    PreTrainedModel,\n)\n", ")\n\nfrom src import PROJECT_ROOT\nfrom src.ipi.decoders.jacobi import JacobiDecoder\nfrom src.ipi.initializer import Initializer\nfrom src.ipi.decoders.mt_decoding import MTDecoder, generate_target\n\n\ndef add_iteration_matrix(\n    sample: dict, src_lang: str, tgt_lang: str, model, tokenizer, initializer, decoder\n):\n    source = sample[\"translation\"][src_lang]\n    target = sample[\"translation\"][tgt_lang]\n\n    encoded_source = tokenizer(\n        source,\n        return_tensors=\"pt\",\n    )\n    encoded_target = tokenizer(\n        target,\n        return_tensors=\"pt\",\n    )\n\n    x = {\n        \"source\": {\n            \"input_ids\": encoded_source[\"input_ids\"],\n            \"attention_mask\": encoded_source[\"attention_mask\"],\n            \"sentences\": source,\n        },\n        \"target\": {\n            \"input_ids\": encoded_target[\"input_ids\"],\n            \"attention_mask\": encoded_target[\"attention_mask\"],\n            \"sentences\": target,\n        },\n    }\n\n    input_ids = encoded_source[\"input_ids\"].to(device)\n    attention_mask = encoded_source[\"attention_mask\"].to(device)\n    gold_output = generate_target(\n        model=model,\n        tokenizer=tokenizer,\n        input_ids=input_ids.to(device),\n        attention_mask=attention_mask.to(device),\n        is_mbart=False,\n    )\n\n    init_tensor, _ = initializer.init_translation(gold_output.shape[-1])\n\n    return_tensor, index, ddg = decoder.decode(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        init_tensor=init_tensor,\n        compute_ddg=True,\n    )\n\n    iteration_matrix, probability_matrix = ddg.finalize_matrices()\n    parallel_steps = iteration_matrix.shape[1]\n    gold_steps = gold_output.shape[1]\n\n    return dict(\n        gold_output=gold_output[0],\n        iteration_matrix=iteration_matrix[0],\n        probability_matrix=probability_matrix[0],\n        parallel_steps=parallel_steps,\n        gold_steps=gold_steps,\n        score=gold_steps - parallel_steps,\n    )", "def add_iteration_matrix(\n    sample: dict, src_lang: str, tgt_lang: str, model, tokenizer, initializer, decoder\n):\n    source = sample[\"translation\"][src_lang]\n    target = sample[\"translation\"][tgt_lang]\n\n    encoded_source = tokenizer(\n        source,\n        return_tensors=\"pt\",\n    )\n    encoded_target = tokenizer(\n        target,\n        return_tensors=\"pt\",\n    )\n\n    x = {\n        \"source\": {\n            \"input_ids\": encoded_source[\"input_ids\"],\n            \"attention_mask\": encoded_source[\"attention_mask\"],\n            \"sentences\": source,\n        },\n        \"target\": {\n            \"input_ids\": encoded_target[\"input_ids\"],\n            \"attention_mask\": encoded_target[\"attention_mask\"],\n            \"sentences\": target,\n        },\n    }\n\n    input_ids = encoded_source[\"input_ids\"].to(device)\n    attention_mask = encoded_source[\"attention_mask\"].to(device)\n    gold_output = generate_target(\n        model=model,\n        tokenizer=tokenizer,\n        input_ids=input_ids.to(device),\n        attention_mask=attention_mask.to(device),\n        is_mbart=False,\n    )\n\n    init_tensor, _ = initializer.init_translation(gold_output.shape[-1])\n\n    return_tensor, index, ddg = decoder.decode(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        init_tensor=init_tensor,\n        compute_ddg=True,\n    )\n\n    iteration_matrix, probability_matrix = ddg.finalize_matrices()\n    parallel_steps = iteration_matrix.shape[1]\n    gold_steps = gold_output.shape[1]\n\n    return dict(\n        gold_output=gold_output[0],\n        iteration_matrix=iteration_matrix[0],\n        probability_matrix=probability_matrix[0],\n        parallel_steps=parallel_steps,\n        gold_steps=gold_steps,\n        score=gold_steps - parallel_steps,\n    )", "\n\ndef enrich_dataset(\n    run_info: dict,\n    device: str,\n    src_lang: str,\n    tgt_lang: str,\n    dataset_name: str,\n    dataset_key: str,\n    tokenizer: PreTrainedTokenizer,\n    model: PreTrainedModel,\n    force_recompute: bool = False,\n) -> Dataset:\n    # MarianMT\n    run_dir: Path = run_info[\"run_dir\"]\n\n    dataset_path: Path = run_dir / \"dataset\"\n\n    if run_dir.exists() and not force_recompute:\n        dataset = load_from_disk(str(run_dir / \"dataset\"))\n    else:\n        initializer = Initializer(src_lang, tgt_lang, tokenizer, use_init=False)\n        model.eval()\n\n        # decoder = MTDecoder(\n        #     tokenizer=tokenizer,\n        #     model=model,\n        #     use_cache=False,\n        #     gs_jaco_blocks=5,\n        #     device=device,\n        #     initializer=initializer\n        # )\n\n        decoder = JacobiDecoder(\n            tokenizer=tokenizer,\n            model=model,\n            initializer=initializer,\n            use_cache=False,\n            device=device\n        )\n\n        dataset = datasets.load_dataset(\n            dataset_name,\n            dataset_key,\n            split=\"test[0:10]\",\n            data_dir=str(PROJECT_ROOT / \"hf_data\"),\n            cache_dir=str(PROJECT_ROOT / \"hf_cache\"),\n        ).map(\n            function=functools.partial(\n                add_iteration_matrix,\n                src_lang=src_lang,\n                tgt_lang=tgt_lang,\n                model=model,\n                initializer=initializer,\n                decoder=decoder,\n                tokenizer=tokenizer,\n            )\n        )\n        dataset.save_to_disk(str(dataset_path))\n\n        json.dump(\n            run_info,\n            fp=(run_dir / \"run_info.json\").open(\"w\", encoding=\"utf-8\"),\n            indent=4,\n            default=lambda x: str(x)\n        )\n\n    return dataset", "\n\ndef draw(sample: dict, tokenizer: PreTrainedTokenizer, starting_index: int):\n    labels = [f\"{i}\" for i in sample[\"gold_output\"][starting_index - 1 :]]\n    iteration_matrix = torch.as_tensor(sample[\"iteration_matrix\"])\n    probability_matrix = torch.as_tensor(sample[\"probability_matrix\"])\n    iteration_matrix = iteration_matrix[:, :].int().numpy()\n    probability_matrix = probability_matrix[:, 1:].numpy()\n\n    mask: np.ndarray = np.zeros_like(iteration_matrix)\n    i, j = 0, 0\n    while i < iteration_matrix.shape[0] and j < iteration_matrix.shape[1]:\n        if iteration_matrix[i, j]:\n            mask[i, j] += 1\n            j += 1\n        else:\n            mask[i, j:] = iteration_matrix[i, j:]\n            i += 1\n\n    probability_matrix = mask * probability_matrix\n\n    fig = px.imshow(\n        iteration_matrix + mask,\n        binary_compression_level=0,\n        # title=f\"Decoding Dependency Graph for sentence {sentence_id}\",\n        color_continuous_scale=\"Viridis\",\n    )\n\n    fig.update_xaxes(\n        tickmode=\"array\",\n        tickvals=list(range(len(labels[1:]))),\n        ticktext=[tokenizer.convert_ids_to_tokens([x])[0] for x in labels[1:]],\n        tickangle=45,\n    )\n\n    fig.update_traces(\n        text=[[f\"{xy:.2f}\" if xy > 0 else \"\" for xy in x] for x in probability_matrix],\n        texttemplate=\"%{text}\",\n    )\n\n    fig.update_layout(\n        font=dict(family=\"Courier New, monospace\", size=22, color=\"Black\"),\n        showlegend=False,\n        coloraxis_showscale=False,\n        margin=dict(l=0, r=0, b=0, t=0),\n        paper_bgcolor=\"rgba(0,0,0,0)\",\n        plot_bgcolor=\"rgba(0,0,0,0)\",\n    )\n\n    return fig", "\n\nif __name__ == \"__main__\":\n\n\n    parser = argparse.ArgumentParser(description='Dependency Graph Visualizer (DDGviz)')\n\n    parser.add_argument('--src', default=\"ro\", help='src language')\n    parser.add_argument('--tgt', default=\"en\", help='target language')\n    parser.add_argument('--dataset', default=\"wmt16\", help='Dataset name')\n    # parser.add_argument('--examples', default=[1566, 960], help='Examples to print with DDGviz')\n    parser.add_argument('--examples', default=[1, 4], help='Examples to print with DDGviz')\n\n    args = parser.parse_args()\n\n    device = \"cpu\"\n    src_lang = args.src\n    tgt_lang = args.tgt\n    dataset_name: str = args.dataset\n    dataset_key: str = f\"{src_lang}-{tgt_lang}\"\n    langs = {src_lang, tgt_lang}\n    examples_to_print = args.examples\n\n\n    if \"en\" in langs:\n        dataset_key = (\n            f\"{src_lang}-{tgt_lang}\" if \"en\" == tgt_lang else f\"{tgt_lang}-{src_lang}\"\n        )\n\n    model_src_lang = src_lang\n    if model_src_lang == \"ro\":\n        model_src_lang: str = \"roa\"\n\n    dataset_split: str = \"test\"\n    model_name: str = f\"Helsinki-NLP/opus-mt-{model_src_lang}-{tgt_lang}\"\n    # model_name: str = \"zannabethl/opus-mt-en-ro-finetuned-en-to-ro\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n\n    run_info = dict(\n        model_name=model_name,\n        source_lang=src_lang,\n        target_lang=tgt_lang,\n        dataset_name=dataset_name,\n        dataset_key=dataset_key,\n        split=dataset_split,\n    )\n\n    run_info_hash: str = hashlib.md5(\n        json.dumps(run_info).encode(encoding=\"utf-8\")\n    ).hexdigest()\n    run_dir: Path = PROJECT_ROOT / \"iteration_matrix\" / run_info_hash\n\n    run_info[\"run_dir\"] = run_dir\n\n    dataset = (\n        enrich_dataset(\n            run_info=run_info,\n            device=device,\n            src_lang=src_lang,\n            tgt_lang=tgt_lang,\n            dataset_name=dataset_name,\n            dataset_key=dataset_key,\n            tokenizer=tokenizer,\n            model=model,\n        )\n        .map(function=lambda x, i: {\"index\": i}, with_indices=True)\n        .select(examples_to_print)\n    )\n\n    starting_index: int = 1\n    images_dir: Path = run_dir / \"images\"\n    images_dir.mkdir(exist_ok=True)\n    for sample in dataset:\n        fig = draw(sample=sample, tokenizer=tokenizer, starting_index=1)\n        # fig.show()\n        print(f\"Index: {sample['index']}\")\n        print(f\"Translations: {sample['translation']}\")\n        print(\n            f\"Gold output: {tokenizer.decode(sample['gold_output'], skip_special_tokens=True)}\"\n        )\n        print()\n        # input()\n        # continue\n        fig.write_image(images_dir / f\"{sample['index']}.png\", width=1800, height=1500)\n        x = {x: sample[x] for x in (\"translation\", \"gold_output\", \"index\", \"score\")}\n        x[\"gold_output\"] = tokenizer.decode(sample[\"gold_output\"])\n\n        (images_dir / f\"{sample['index']}.json\").write_text(\n            json.dumps(x, indent=4, sort_keys=True), encoding=\"utf-8\"\n        )", ""]}
{"filename": "src/ipi/stopping_condition.py", "chunked_list": ["import torch\n\n\ndef limit_past_key_values(past_key_values, limit):\n    new_list = []\n    for elem in past_key_values:\n        new_elem = list(elem)\n        new_elem[0] = elem[0][:, :, :limit, :]\n        new_elem[1] = elem[1][:, :, :limit, :]\n        new_list.append(tuple(new_elem))\n    return tuple(new_list)", "\n\ndef stopping_criterion(past_tensor, current_tensor, eos=None):\n    assert past_tensor.shape == current_tensor.shape\n    if torch.equal(past_tensor, current_tensor):\n        tensor = current_tensor\n        if eos is not None:\n            if eos in current_tensor[0]:\n                pos = (current_tensor[0] == eos).nonzero(as_tuple=True)[0]\n                if pos.shape[0] > 1:\n                    pos = pos[0].item()\n                else:\n                    pos = pos.item()\n                return True, tensor, pos\n            else:\n                return True, tensor, -1\n        return True, tensor\n    else:\n        if eos is not None:\n            return False, current_tensor, False\n        else:\n            return False, current_tensor", "\n\ndef check_stop_cond(tensor, eos):\n    if eos in tensor[0]:\n        pos = (tensor[0] == eos).nonzero(as_tuple=True)[0]\n        if pos.shape[0] > 1:\n            pos = pos[0].item()\n        else:\n            pos = pos.item()\n        return pos\n    else:\n        return -1", ""]}
{"filename": "src/ipi/initializer.py", "chunked_list": ["import torch\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom transformers import (\n    M2M100Tokenizer,\n    MarianTokenizer,\n    MBart50Tokenizer,\n    MBartTokenizer,\n)\n", ")\n\n\nclass Initializer(object):\n    def __init__(\n        self,\n        src_len,\n        tgt_len,\n        hugginface_tokenizer,\n        use_init=True,\n        device=\"cpu\",\n    ):\n\n        self.src_len = src_len\n        self.tgt_len = tgt_len\n        self.tokenizer = hugginface_tokenizer\n\n        self.pad_token_id = self.tokenizer.pad_token_id\n        self.tokenizer_nltk = ToktokTokenizer()\n        self.detokenizer_nltk = TreebankWordDetokenizer()\n        self.use_init = use_init\n        self.device = device\n\n    def init_translation(self, tgt_len=None):\n        final_translation = \"\"\n        with self.tokenizer.as_target_tokenizer():\n            if isinstance(self.tokenizer, MBartTokenizer):\n                tgt_tensor = self.tokenizer(\n                    final_translation, return_tensors=\"pt\", padding=True\n                ).data[\"input_ids\"]\n                if tgt_tensor.shape[-1] == 2:\n                    tgt_tensor = tgt_tensor[:, :1]\n            elif isinstance(self.tokenizer, MarianTokenizer):\n                bos = torch.tensor([self.pad_token_id]).unsqueeze(0)\n                tgt_tensor = bos\n            elif isinstance(self.tokenizer, MBart50Tokenizer) or isinstance(\n                self.tokenizer, M2M100Tokenizer\n            ):\n                bos = torch.tensor([self.tokenizer.eos_token_id]).unsqueeze(0)\n                tgt_tensor = self.tokenizer(\n                    final_translation, return_tensors=\"pt\", padding=True\n                ).data[\"input_ids\"]\n                tgt_tensor = torch.cat([bos, tgt_tensor], dim=-1)\n            else:\n                bos = torch.tensor([self.tokenizer.bos_token_id]).unsqueeze(0)\n                tgt_tensor = self.tokenizer(\n                    final_translation, return_tensors=\"pt\", padding=True\n                ).data[\"input_ids\"]\n                tgt_tensor = torch.cat([bos, tgt_tensor], dim=-1)\n        if tgt_len is not None:\n            tgt_tensor = self.trim_length(tgt_tensor, tgt_len)\n        return tgt_tensor.to(self.device), final_translation\n\n    def trim_length(self, tgt_tensor, tgt_len):\n        last_elem = tgt_tensor[:, -1].unsqueeze(0)\n        if tgt_tensor.shape[-1] > tgt_len:\n            return torch.cat([tgt_tensor[..., : tgt_len - 1], last_elem], dim=-1)\n        elif tgt_tensor.shape[-1] < tgt_len:\n            delta = tgt_len - tgt_tensor.shape[-1] - 1\n            init_tensor = torch.tensor(\n                [self.pad_token_id] * delta, dtype=tgt_tensor.dtype\n            ).unsqueeze(0)\n            return_tensor = torch.cat([tgt_tensor, init_tensor, last_elem], dim=-1)\n            return return_tensor\n        else:\n            return tgt_tensor", ""]}
{"filename": "src/ipi/__init__.py", "chunked_list": [""]}
{"filename": "src/ipi/decoders/autoregressive.py", "chunked_list": ["import torch\n\nfrom src.ipi.decoders.mt_decoding import MTDecoder\n\n\nclass AutoregressiveDecoder(MTDecoder):\n    def __init__(self, tokenizer, model, initializer, **kwargs):\n        super().__init__(tokenizer, model, initializer, **kwargs)\n\n        self.name = \"autoregressive\"\n        self.acronym = \"a\"\n\n    @torch.no_grad()\n    def decode(self, input_ids, attention_mask, init_tensor=None, logits_preprocessor=None, *args, **kwargs):\n\n        index = 0\n\n        if init_tensor is None:\n            init_tensor = torch.tensor(\n                [self.pad_token_id], device=self.device\n            ).unsqueeze(0)\n        elif self.is_mbart:\n            output = self.model(\n                input_ids,\n                attention_mask,\n                decoder_input_ids=init_tensor[:, 0].unsqueeze(0),\n                use_cache=True,\n            )\n            encoder_last_hidden_state = output.encoder_last_hidden_state\n            past_key_values = output.past_key_values\n            index += 1\n            total_res = torch.tensor(\n                [[init_tensor[:, 0], init_tensor[:, 1]]], device=self.device\n            )\n            init_tensor = init_tensor[:, 1].unsqueeze(0)\n        else:\n            init_tensor = init_tensor[:, 0].unsqueeze(0)\n\n        total_res = init_tensor.clone()\n        while True:\n            if self.use_cache and index > 0:\n                if index == 1024:\n                    print(total_res)\n                output = self.model(\n                    None,\n                    attention_mask,\n                    decoder_input_ids=init_tensor,\n                    encoder_outputs=(encoder_last_hidden_state, None, None),\n                    use_cache=True,\n                    past_key_values=past_key_values,\n                )\n            else:\n                output = self.model(\n                    input_ids,\n                    attention_mask,\n                    decoder_input_ids=init_tensor,\n                    use_cache=True,\n                )\n            encoder_last_hidden_state = output.encoder_last_hidden_state\n            past_key_values = output.past_key_values\n            logits = output.logits\n            if logits_preprocessor is not None:\n                logits = logits_preprocessor(total_res, logits[:,-1,:])\n            else:\n                logits = logits[:,-1,:]\n            max_value = torch.argmax(logits, dim=-1)\n            last = max_value\n            init_tensor = last.unsqueeze(0)\n            total_res = torch.cat((total_res, init_tensor), dim=1)\n\n            index += 1\n            if last[0].item() == self.eos_token_id or index == self.model.config.max_length - 1:\n                break\n        return total_res, index\n\n    def initialize(self):\n        if self.initializer is not None:\n            init_tensor, _ = self.initializer.init_translation()\n        else:\n            init_tensor = None\n\n        return init_tensor\n\n    def compute_decode_kwargs(self, input_ids, *args, **kwargs):\n        init_tensor = self.initialize()\n        logits_preprocessor = self.generate_logits_preprocessor(input_ids)\n\n        return {\n            \"init_tensor\": init_tensor.clone(),\n            \"logits_preprocessor\": logits_preprocessor\n        }", "\n"]}
{"filename": "src/ipi/decoders/jacobi.py", "chunked_list": ["import torch\n\nfrom src.ipi.decoders.mt_decoding import MTDecoder\nfrom src.viz.dependecy_graph import DecodingDependencyGraph\n\n\nclass JacobiDecoder(MTDecoder):\n    def __init__(self, tokenizer, model, initializer, **kwargs):\n        super().__init__(tokenizer, model, initializer, **kwargs)\n\n        self.name = \"jacobi\"\n        self.acronym = \"j\"\n\n    def generate_target(\n            self,\n            input_ids: torch.Tensor,\n            attention_mask: torch.Tensor,\n            is_mbart: bool,\n            decoding_method: str = \"greedy\",\n            remove_padding: bool = False,\n    ):\n        if decoding_method == \"greedy\":\n            if is_mbart:\n                with self.tokenizer.as_target_tokenizer():\n                    gold_output = self.model.generate(\n                        **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                        num_beams=1,\n                        do_sample=False,\n                        forced_bos_token_id=self.tokenizer.cur_lang_code_id,\n                    )\n            else:\n                gold_output = self.model.generate(\n                    **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                    num_beams=1,\n                    do_sample=False,\n                )\n        else:\n            raise NotImplementedError()\n\n        if remove_padding:\n            sample_lengths = (gold_output != self.tokenizer.pad_token_id).sum(dim=1)\n            gold_output = [\n                sample[:length] for sample, length in zip(gold_output, sample_lengths)\n            ]\n\n        return gold_output\n\n\n    @torch.no_grad()\n    def decode(\n        self,\n        input_ids,\n        attention_mask,\n        target_len=None,\n        gold_target=None,\n        init_tensor=None,\n        compute_ddg: bool = False,\n        logits_preprocessor=None,\n        *args,\n        **kwargs\n    ):\n        max_length = target_len\n        str_index = 0\n        key_cache = 0\n        if compute_ddg:\n            if gold_target is None:\n                gold_target = self.generate_target(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    is_mbart=self.is_mbart,\n                )\n            ddg = DecodingDependencyGraph(\n                model=self.model, tokenizer=self.tokenizer, gold_target=gold_target\n            )\n\n            max_length = gold_target.shape[-1]\n\n        if init_tensor is None:\n            init_tensor = torch.tensor(\n                [self.pad_token_id] * input_ids.size(0) * max_length,\n                device=self.device,\n            ).reshape(input_ids.size(0), max_length)\n        elif self.is_mbart:\n            if init_tensor.shape[0] == 1:\n                decoder_input_ids = init_tensor[:, 0].unsqueeze(0)\n            else:\n                decoder_input_ids = init_tensor[:, 0].unsqueeze(-1)\n            output = self.model(\n                input_ids,\n                attention_mask,\n                decoder_input_ids=decoder_input_ids,\n                use_cache=True,\n            )\n            encoder_last_hidden_state = output.encoder_last_hidden_state\n            past_key_values = output.past_key_values\n            str_index = 1\n            total_res = init_tensor\n            init_tensor = init_tensor[:, 1:]\n            key_cache = 1\n\n        output_probs = init_tensor.clone().float()\n\n        for index in range(str_index, max_length):\n            if self.use_cache and index > 0:\n                old_init_tensor = total_res.detach().clone()\n                init_tensor = total_res[:, index:]\n                output = self.model(\n                    input_ids,\n                    attention_mask,\n                    decoder_input_ids=init_tensor,\n                    encoder_outputs=(encoder_last_hidden_state, None, None),\n                    use_cache=True,\n                    past_key_values=self.limit_past_key_values(past_key_values, index + key_cache),\n                )\n            else:\n                old_init_tensor = init_tensor.detach().clone()\n                output = self.model(\n                    input_ids,\n                    attention_mask,\n                    decoder_input_ids=init_tensor,\n                    use_cache=True,\n                )\n            encoder_last_hidden_state = output.encoder_last_hidden_state\n            past_key_values = output.past_key_values\n            logits = output.logits\n            max_index = torch.argmax(logits, dim=-1)\n            max_value, max_i = torch.max(torch.softmax(logits, dim=-1), dim=-1)\n            if index > 0 and logits_preprocessor is not None:\n                logits_new = logits_preprocessor(total_res[:, : index + 1], logits[:, 0, :])\n                max_value_new = torch.argmax(logits_new, dim=-1)\n                max_index[:, 0] = max_value_new\n            if self.use_cache and index > 0:\n                init_tensor = max_index\n                total_res = torch.cat(\n                    (total_res[:, : index + 1], init_tensor[:, :-1]), dim=1\n                )\n            else:\n                init_tensor[:, index + 1 :] = max_index[:, index:-1]\n                total_res = init_tensor\n\n                output_probs[:, index + 1 :] = max_value[:, index:-1]\n\n            stop_condition, return_tensor = self.stopping_criterion(\n                old_init_tensor, total_res\n            )\n\n            if compute_ddg:\n                ddg.insert_one_element(\n                    total_res, gold_target, output_probs=output_probs\n                )\n\n            if stop_condition:\n                break\n\n        if compute_ddg:\n            return return_tensor, index, ddg\n\n        return return_tensor, index\n\n\n    def initialize(self, init_transl):\n        if self.initializer is not None:\n            init_tensor, _ = self.initializer.init_translation(init_transl.shape[-1])\n        else:\n            init_tensor = None\n\n        return init_tensor\n\n    def compute_decode_kwargs(self, input_ids, attention_mask, **kwargs):\n\n        gold_autoregressive = self.generate_gold_autoregressive(input_ids, attention_mask)\n        init_tensor = self.initialize(init_transl=gold_autoregressive)\n        logits_preprocessor = self.generate_logits_preprocessor(input_ids)\n\n        return {\n            \"init_tensor\": init_tensor.clone(),\n            \"target_len\": gold_autoregressive.shape[-1],\n            \"gold_target\": gold_autoregressive,\n            \"logits_preprocessor\": logits_preprocessor\n        }", ""]}
{"filename": "src/ipi/decoders/__init__.py", "chunked_list": [""]}
{"filename": "src/ipi/decoders/hybrid_jacobi.py", "chunked_list": ["import torch\nfrom more_itertools import sliced\n\nfrom src.ipi.decoders.mt_decoding import MTDecoder\n\n\nclass HybridJacobiDecoder(MTDecoder):\n    def __init__(self, tokenizer, model, gs_jaco_blocks, init_mode, initializer, percent=300, **kwargs):\n        super().__init__(tokenizer, model, initializer, **kwargs)\n\n        self.name = \"hybrid_jacobi\"\n        self.acronym = \"h\"\n\n        self.gs_jaco_blocks = gs_jaco_blocks\n\n        self.init_mode = init_mode\n        self.percent = percent\n\n    @torch.no_grad()\n    def decode(\n        self, input_ids, attention_mask, target_len, gold_target, init_tensor=None, logits_preprocessor=None, *args, **kwargs\n    ):\n        key_cache = 1\n        if init_tensor is None:\n            init_tensor = torch.tensor(\n                [self.pad_token_id] * target_len, device=self.device\n            )\n            blocks = list(sliced(init_tensor, self.gs_jaco_blocks))\n            init_tensor = init_tensor.unsqueeze(0)\n            total_past_key_values = None\n        elif self.is_mbart:\n            output = self.model(\n                input_ids,\n                attention_mask,\n                decoder_input_ids=init_tensor[:, 0].unsqueeze(0),\n                use_cache=True,\n            )\n            encoder_last_hidden_state = output.encoder_last_hidden_state\n            total_past_key_values = output.past_key_values\n            init_tensor = init_tensor[:, 1:]\n            blocks = list(sliced(init_tensor.squeeze(0), self.gs_jaco_blocks))\n            key_cache = 2\n        else:\n            init_tensor = init_tensor\n            blocks = list(sliced(init_tensor.squeeze(0), self.gs_jaco_blocks))\n            total_past_key_values = None\n\n        iteration_saved = 0\n        base_value = 0\n\n        for blocco in blocks:\n            max_len = blocco.shape[-1]\n            blocco_usr = init_tensor[:, base_value : base_value + max_len]\n            for index in range(max_len):\n                old_blocco = blocco_usr.detach().clone()\n                trig = self.trig_eos(\n                    old_blocco, self.eos_token_id, init_tensor, base_value\n                )\n                if trig is not None:\n                    return trig, (gold_target.shape[-1] - 1) - iteration_saved\n                blocco_usr_new = blocco_usr[:, index:]\n                if base_value == 0 and index == 0 and not self.is_mbart:\n                    output = self.model(\n                        input_ids,\n                        attention_mask,\n                        decoder_input_ids=blocco_usr_new,\n                        use_cache=True,\n                        past_key_values=total_past_key_values,\n                    )\n                    encoder_last_hidden_state = output.encoder_last_hidden_state\n                else:\n                    output = self.model(\n                        input_ids,\n                        attention_mask,\n                        decoder_input_ids=blocco_usr_new,\n                        encoder_outputs=(encoder_last_hidden_state, None, None),\n                        use_cache=True,\n                        past_key_values=total_past_key_values,\n                    )\n\n                total_past_key_values = self.limit_past_key_values(\n                    output.past_key_values,\n                    base_value + index + key_cache,\n                )\n\n                logits = output.logits\n                max_value = torch.argmax(logits, dim=-1)\n\n                if logits_preprocessor is not None:\n                    logits_new = logits_preprocessor(init_tensor[:, :base_value + index + 1], logits[:, 0, :])\n                    max_value_new = torch.argmax(logits_new, dim=-1)\n                    max_value[:,0] = max_value_new\n\n                if (\n                    max_value.shape[-1]\n                    == init_tensor[\n                        :, base_value + index + 1 : base_value + max_len + 1\n                    ].shape[-1]\n                ):\n                    init_tensor[\n                        :, base_value + index + 1 : base_value + max_len + 1\n                    ] = max_value[:, :]\n                else:\n                    # If last block remove the last token after EOS\n                    init_tensor[\n                        :, base_value + index + 1 : base_value + max_len + 1\n                    ] = max_value[:, :-1]\n\n                stop_condition, _, eos_cond = self.stopping_criterion(\n                    old_blocco, blocco_usr, eos=self.eos_token_id\n                )\n\n                if stop_condition:\n                    if eos_cond >= 0:\n                        return (\n                            init_tensor[:, : base_value + eos_cond + 1],\n                            (gold_target.shape[-1] - 1) - iteration_saved,\n                        )\n                    if index + 1 != max_len:\n                        iteration_saved += max_len - index - 1\n                        total_past_key_values = self.limit_past_key_values(\n                            output.past_key_values,\n                            base_value + max_len + 1,\n                        )\n                        break\n\n            base_value += max_len\n\n        total_res, total_iter = (\n            init_tensor,\n            (gold_target.shape[-1] - 1) - iteration_saved,\n        )\n\n        init_tensor = init_tensor[:, -1].clone().unsqueeze(0)\n\n        #Go autoregressive until [EOS]\n        while True and base_value != self.model.config.max_length - 1:\n            index = 0\n            output = self.model(\n                input_ids,\n                attention_mask,\n                decoder_input_ids=init_tensor,\n                encoder_outputs=(encoder_last_hidden_state, None, None),\n                use_cache=True,\n                past_key_values=total_past_key_values,\n            )\n            encoder_last_hidden_state = output.encoder_last_hidden_state\n            total_past_key_values = output.past_key_values\n            logits = output.logits\n            max_value = torch.argmax(logits, dim=-1)\n            last = max_value[:, -1]\n            if self.use_cache:\n                init_tensor = last.unsqueeze(0)\n                total_res = torch.cat((total_res, init_tensor), dim=1)\n\n            index += 1\n            if last[0].item() == self.eos_token_id:\n                break\n        return total_res, index + total_iter\n\n    def initialize(self, input_ids, gold_autoregressive):\n        if self.initializer is not None:\n            if self.init_mode == \"under\":\n                len = max(3, input_ids.shape[-1] - self.percent / 100 * input_ids.shape[-1])\n                m = int(len)\n            elif self.init_mode == \"over\":\n                len = input_ids.shape[-1] + self.percent / 100 * input_ids.shape[-1]\n                m = int(len)\n            elif self.init_mode == \"fixed\":\n                m = 511\n            else:\n                m = gold_autoregressive.shape[-1]\n            init_tensor, _ = self.initializer.init_translation(m)\n        else:\n            init_tensor = None\n\n        return init_tensor\n\n    def compute_decode_kwargs(self, input_ids, attention_mask, **kwargs):\n        gold_autoregressive = self.generate_gold_autoregressive(input_ids, attention_mask)\n        init_tensor = self.initialize(input_ids, gold_autoregressive)\n        logits_preprocessor = self.generate_logits_preprocessor(input_ids)\n\n        return{\n            \"init_tensor\": init_tensor.clone(),\n            \"gold_target\": gold_autoregressive,\n            \"target_len\": gold_autoregressive.shape[-1],\n            \"logits_preprocessor\": logits_preprocessor\n        }"]}
{"filename": "src/ipi/decoders/gs_jacobi.py", "chunked_list": ["import torch\n\nfrom src.ipi.decoders.mt_decoding import MTDecoder\nfrom more_itertools import sliced\n\n\nclass GSJacobiDecoder(MTDecoder):\n    def __init__(self, tokenizer, model, initializer, gs_jaco_blocks, init_mode, **kwargs):\n        super().__init__(tokenizer, model, initializer, **kwargs)\n\n        self.name = \"gs_jacobi\"\n        self.acronym = \"g\"\n\n        self.gs_jaco_blocks = gs_jaco_blocks\n        self.init_mode = init_mode\n\n    @torch.no_grad()\n    def decode(\n        self, input_ids, attention_mask, target_len, gold_target, init_tensor=None, logits_preprocessor=None, *args, **kwargs\n    ):\n        key_cache = 1\n        if init_tensor is None:\n            init_tensor = torch.tensor(\n                [self.pad_token_id] * target_len, device=self.device\n            )\n            blocks = list(sliced(init_tensor, self.gs_jaco_blocks))\n            init_tensor = init_tensor.unsqueeze(0)\n            total_past_key_values = None\n        elif self.is_mbart:\n            output = self.model(\n                input_ids,\n                attention_mask,\n                decoder_input_ids=init_tensor[:, 0].unsqueeze(0),\n                use_cache=True,\n            )\n            encoder_last_hidden_state = output.encoder_last_hidden_state\n            total_past_key_values = output.past_key_values\n            delta = 1\n            # total_res = init_tensor.to(self.device)\n            init_tensor = init_tensor[:, 1:]\n            blocks = list(sliced(init_tensor.squeeze(0), self.gs_jaco_blocks))\n            key_cache = 2\n        else:\n            init_tensor = init_tensor\n            blocks = list(sliced(init_tensor.squeeze(0), self.gs_jaco_blocks))\n            total_past_key_values = None\n\n        iteration_saved = 0\n        base_value = 0\n\n        for blocco in blocks:\n            max_len = blocco.shape[-1]\n            blocco_usr = init_tensor[:, base_value : base_value + max_len]\n\n            for index in range(max_len):\n                old_blocco = blocco_usr.detach().clone()\n                blocco_usr_new = blocco_usr[:, index:]\n                if base_value == 0 and index == 0 and not self.is_mbart:\n                    output = self.model(\n                        input_ids,\n                        attention_mask,\n                        decoder_input_ids=blocco_usr_new,\n                        use_cache=True,\n                        past_key_values=total_past_key_values,\n                    )\n                    encoder_last_hidden_state = output.encoder_last_hidden_state\n                else:\n                    output = self.model(\n                        input_ids,\n                        attention_mask,\n                        decoder_input_ids=blocco_usr_new,\n                        encoder_outputs=(encoder_last_hidden_state, None, None),\n                        use_cache=True,\n                        past_key_values=total_past_key_values,\n                    )\n\n                total_past_key_values = self.limit_past_key_values(\n                    output.past_key_values,\n                    base_value + index + key_cache,\n                )\n                logits = output.logits\n                max_value = torch.argmax(logits, dim=-1)\n\n                if logits_preprocessor is not None:\n                    logits_new = logits_preprocessor(init_tensor[:, :base_value + index +1], logits[:, 0, :])\n                    max_value_new = torch.argmax(logits_new, dim=-1)\n                    max_value[:, 0] = max_value_new\n\n\n                if (\n                    max_value.shape[-1]\n                    == init_tensor[\n                        :, base_value + index + 1 : base_value + max_len + 1\n                    ].shape[-1]\n                ):\n                    init_tensor[\n                        :, base_value + index + 1 : base_value + max_len + 1\n                    ] = max_value[:, :]\n                else:\n                    # If last block remove the last token after EOS\n                    init_tensor[\n                        :, base_value + index + 1 : base_value + max_len + 1\n                    ] = max_value[:, :-1]\n\n                stop_condition, _ = self.stopping_criterion(old_blocco, blocco_usr)\n\n                if stop_condition and index + 1 != max_len:\n                    total_past_key_values = self.limit_past_key_values(\n                        output.past_key_values,\n                        base_value + max_len + 1,\n                    )\n                    iteration_saved += max_len - index - 1\n                    break\n            base_value += max_len\n        return init_tensor, (gold_target.shape[-1] - 1) - iteration_saved\n\n    def initialize(self, input_ids, gold_autoregressive):\n        if self.initializer is not None:\n            if self.init_mode == \"overprov\":\n                m = int(input_ids.shape[-1] + 10 / 100 * input_ids.shape[-1])\n            else:\n                m = gold_autoregressive.shape[-1]\n            init_tensor, _ = self.initializer.init_translation(m)\n        else:\n            init_tensor = None\n\n        return init_tensor\n\n    def compute_decode_kwargs(self, input_ids, attention_mask, **kwargs):\n        gold_autoregressive = self.generate_gold_autoregressive(input_ids, attention_mask)\n        init_tensor = self.initialize(input_ids, gold_autoregressive)\n        logits_preprocessor = self.generate_logits_preprocessor(input_ids)\n\n        return{\n            \"init_tensor\": init_tensor.clone(),\n            \"gold_target\": gold_autoregressive,\n            \"target_len\": gold_autoregressive.shape[-1],\n            \"logits_preprocessor\": logits_preprocessor\n        }", ""]}
{"filename": "src/ipi/decoders/mt_decoding.py", "chunked_list": ["from typing import Dict, Optional\n\nimport torch\nfrom transformers import MBartForConditionalGeneration\n\nfrom src.ipi import stopping_condition as sc\nfrom src.utils.utils import get_logits_preprocessor\n\nPREC_GOLD_AUTOREGRESSIVE: Dict[str, Optional[torch.Tensor]] = {\"input_ids\": None, \"gold\": None}\nPREC_LOGISTS_PREPROCESSOR: Dict[str, Optional[torch.Tensor]] = {\"input_ids\": None, \"logists\": None}", "PREC_GOLD_AUTOREGRESSIVE: Dict[str, Optional[torch.Tensor]] = {\"input_ids\": None, \"gold\": None}\nPREC_LOGISTS_PREPROCESSOR: Dict[str, Optional[torch.Tensor]] = {\"input_ids\": None, \"logists\": None}\n\nclass MTDecoder:\n    def __init__(\n            self,\n            tokenizer,\n            model,\n            initializer,\n            use_cache: bool = True,\n            use_logits_preprocessor: bool = True,\n            device: str = \"cuda\",\n            **kwargs\n    ):\n        self.tokenizer = tokenizer\n\n        self.initializer = initializer\n\n        self.pad_token_id = self.tokenizer.pad_token_id\n        self.eos_token_id = self.tokenizer.eos_token_id\n\n        self.use_cache = use_cache\n        self.device = device\n\n        with torch.no_grad():\n            self.model = model\n            self.model_name = self.model.name_or_path\n            self.model.eval()\n\n        self.max_length = min(self.tokenizer.model_max_length, 511)\n\n        self.use_logits_preprocessor = use_logits_preprocessor\n\n        self.is_mbart = isinstance(self.model, MBartForConditionalGeneration)\n\n    def decode(self, input_ids, attention_mask, *args, **kwargs):\n        pass\n\n    def compute_decode_kwargs(self, *args, **kwargs):\n        pass\n\n    def initialize(self, **kwargs):\n        pass\n\n    def generate_gold_autoregressive(self, input_ids, attention_mask):\n\n        global PREC_GOLD_AUTOREGRESSIVE\n\n        if PREC_GOLD_AUTOREGRESSIVE['input_ids'] is None or not torch.equal(input_ids, PREC_GOLD_AUTOREGRESSIVE['input_ids']):\n            if self.is_mbart:\n                with self.tokenizer.as_target_tokenizer():\n                    try:\n                        lang_id = self.tokenizer.cur_lang_code_id\n                    except:\n                        lang_id = self.tokenizer.cur_lang_id\n                gold_autoregressive = self.model.generate(\n                    **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                    num_beams=1,\n                    do_sample=False,\n                    use_cache=False,\n                    forced_bos_token_id=lang_id,\n                )\n            else:\n                gold_autoregressive = self.model.generate(\n                    **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                    num_beams=1,\n                    do_sample=False,\n                    use_cache=False,\n                )\n            gold_autoregressive = gold_autoregressive[:, : self.max_length]\n\n            PREC_GOLD_AUTOREGRESSIVE['input_ids'] = input_ids\n            PREC_GOLD_AUTOREGRESSIVE['gold'] = gold_autoregressive\n\n        return PREC_GOLD_AUTOREGRESSIVE['gold']\n\n    def generate_logits_preprocessor(self, input_ids):\n\n        global PREC_LOGISTS_PREPROCESSOR\n\n        if self.use_logits_preprocessor:\n            if PREC_LOGISTS_PREPROCESSOR['input_ids'] is None or not torch.equal(input_ids, PREC_LOGISTS_PREPROCESSOR['input_ids']):\n                logits_preprocessor = get_logits_preprocessor(\n                    model=self.model,\n                    input_ids=input_ids,\n                    eos_token_id=self.eos_token_id\n                )\n\n                PREC_LOGISTS_PREPROCESSOR['input_ids'] = input_ids\n                PREC_LOGISTS_PREPROCESSOR['logists'] = logits_preprocessor\n        else:\n            return None\n\n        return PREC_LOGISTS_PREPROCESSOR['logists']\n\n    @staticmethod\n    def stopping_criterion(past_tensor, current_tensor, eos=None):\n        return sc.stopping_criterion(past_tensor, current_tensor, eos)\n\n    @staticmethod\n    def limit_past_key_values(past_key_values, limit):\n        return sc.limit_past_key_values(past_key_values, limit)\n\n    @staticmethod\n    def trig_eos(tensor, eos_token_id, init_tensor, base_value):\n        if tensor[:, 0].item() == eos_token_id:\n            return init_tensor[:, : base_value + 1]\n        else:\n            return None", "\n\ndef generate_target(\n    tokenizer,\n    model,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    is_mbart: bool,\n    decoding_method: str = \"greedy\",\n    remove_padding: bool = False,\n):\n    if decoding_method == \"greedy\":\n        if is_mbart:\n            with tokenizer.as_target_tokenizer():\n                gold_output = model.generate(\n                    **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                    num_beams=1,\n                    do_sample=False,\n                    forced_bos_token_id=tokenizer.cur_lang_code_id,\n                )\n        else:\n            gold_output = model.generate(\n                **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                num_beams=1,\n                do_sample=False,\n            )\n    else:\n        raise NotImplementedError()\n\n    if remove_padding:\n        sample_lengths = (gold_output != tokenizer.pad_token_id).sum(dim=1)\n        gold_output = [\n            sample[:length] for sample, length in zip(gold_output, sample_lengths)\n        ]\n\n    return gold_output", "\n"]}
{"filename": "src/ipi/decoders/beam_search.py", "chunked_list": ["from src.ipi.decoders.mt_decoding import MTDecoder\n\n\nclass BeamSearchDecoder(MTDecoder):\n    def __init__(self, tokenizer, model, initializer, num_beams, early_stopping, **kwargs):\n        super().__init__(tokenizer, model, initializer, **kwargs)\n\n        self.name = \"beam_search\"\n        self.acronym = \"b\"\n\n        self.num_beams = num_beams\n        self.early_stopping = early_stopping\n\n    def decode(self, input_ids, attention_mask, *args, **kwargs):\n        if self.is_mbart:\n            with self.tokenizer.as_target_tokenizer():\n                try:\n                    lang_id = self.tokenizer.cur_lang_code_id\n                except:\n                    lang_id = self.tokenizer.cur_lang_id\n            beam_output = self.model.generate(\n                **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                num_beams=self.num_beams,\n                early_stopping=self.early_stopping,\n                forced_bos_token_id=lang_id,\n            )\n        else:\n            beam_output = self.model.generate(\n                **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n                num_beams=self.num_beams,\n                early_stopping=self.early_stopping,\n            )\n\n        return beam_output, 0\n\n    def compute_decode_kwargs(self, *args, **kwargs):\n        return {}"]}
{"filename": "src/utils/bench_scorer.py", "chunked_list": ["from src.utils.bleu_calculator import BleuEvaluator, BleuValues\n\n\nclass Scorer(object):\n    def __init__(self, name, acronym):\n        self.name = name\n        self.acronym = acronym\n\n        self.bleu_scorer = BleuEvaluator()\n\n        # number of sentences\n        self.i = 0\n\n        # param bleu score\n        self.predictions = []\n        self.references = []\n\n        # benchmark values\n        self.tot_mean_time = 0\n        self.tot_mean_iter = 0\n\n        # inline values\n        self.current_init = None\n        self.current_transl = None\n        self.current_time = None\n        self.current_iter = None\n\n    def update_metrics(self, time, iter, translation, gold, init):\n        self.tot_mean_time += (time - self.tot_mean_time) / (self.i + 1)\n        self.tot_mean_iter += (iter - self.tot_mean_iter) / (self.i + 1)\n\n        self.predictions.append(translation)\n        self.references.append([gold])\n\n        self.current_init = init\n        self.current_transl = translation\n        self.current_time = time\n        self.current_iter = iter\n\n        self.i += 1\n\n    def compute_bleu_score(self):\n        bleu_score = self.bleu_scorer.final_score(\n            model_predictions=self.predictions,\n            gold_references=self.references\n        )\n\n        bleu_dict = {\n            'score': bleu_score['score'],\n            'counts': str(bleu_score['counts']),\n            'totals': str(bleu_score['totals']),\n            'precisions': str(['%.2f' % prec for prec in bleu_score['precisions']]),\n            'bp': bleu_score['bp'],\n            'sys_len': bleu_score['sys_len'],\n            'ref_len': bleu_score['ref_len']\n        }\n\n        return BleuValues(**bleu_dict)"]}
{"filename": "src/utils/__init__.py", "chunked_list": [""]}
{"filename": "src/utils/utils.py", "chunked_list": ["import csv\nimport os\nimport random\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.nn.modules import linear\nimport datasets", "from torch.nn.modules import linear\nimport datasets\n\ndef makedirs(path):\n    if path.endswith((\".tsv\", \".csv\", \".txt\")):\n        path = \"/\".join(path.split(\"/\")[:-1])\n\n    if not os.path.exists(path):\n        os.makedirs(path)\n\ndef check_zero_division(a, b):\n    return \"na\" if b == 0 else round(a / b, 3)", "\ndef check_zero_division(a, b):\n    return \"na\" if b == 0 else round(a / b, 3)\n\ndef get_logits_preprocessor(model, input_ids, eos_token_id):\n    logits_preprocessor = model._get_logits_processor(\n        repetition_penalty=None,\n        no_repeat_ngram_size=None,\n        encoder_no_repeat_ngram_size=None,\n        input_ids_seq_length=1,\n        encoder_input_ids=input_ids,\n        bad_words_ids=None,\n        min_length=0,\n        max_length=model.config.max_length,\n        eos_token_id=eos_token_id,\n        forced_bos_token_id=None,\n        forced_eos_token_id=None,\n        prefix_allowed_tokens_fn=None,\n        num_beams=1,\n        num_beam_groups=1,\n        diversity_penalty=None,\n        remove_invalid_values=None,\n        exponential_decay_length_penalty=None,\n        logits_processor=[],\n        renormalize_logits=None\n    )\n    return logits_preprocessor", "\n\n\ndef retrieve_model_name(model_name):\n    if \"opus\" in model_name:\n        return \"opus\"\n    if \"mbart\" in model_name:\n        if \"50\" in model_name:\n            return \"mbart_50\"\n        return \"mbart\"", "\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True", "\n\ndef retrieve_map_languages_flores(lan):\n    lang_map = {\n        \"ab\": \"Abkhaz\",\n        \"aa\": \"Afar\",\n        \"af\": \"Afrikaans\",\n        \"ak\": \"Akan\",\n        \"sq\": \"Albanian\",\n        \"am\": \"Amharic\",\n        \"ar\": \"Arabic\",\n        \"an\": \"Aragonese\",\n        \"hy\": \"Armenian\",\n        \"as\": \"Assamese\",\n        \"av\": \"Avaric\",\n        \"ae\": \"Avestan\",\n        \"ay\": \"Aymara\",\n        \"az\": \"Azerbaijani\",\n        \"bm\": \"Bambara\",\n        \"ba\": \"Bashkir\",\n        \"eu\": \"Basque\",\n        \"be\": \"Belarusian\",\n        \"bn\": \"Bengali\",\n        \"bh\": \"Bihari\",\n        \"bi\": \"Bislama\",\n        \"bs\": \"Bosnian\",\n        \"br\": \"Breton\",\n        \"bg\": \"Bulgarian\",\n        \"my\": \"Burmese\",\n        \"ca\": \"Catalan\",\n        \"ch\": \"Chamorro\",\n        \"ce\": \"Chechen\",\n        \"ny\": \"Chichewa\",\n        \"zh\": \"Chinese\",\n        \"cv\": \"Chuvash\",\n        \"kw\": \"Cornish\",\n        \"co\": \"Corsican\",\n        \"cr\": \"Cree\",\n        \"hr\": \"Croatian\",\n        \"cs\": \"Czech\",\n        \"da\": \"Danish\",\n        \"dv\": \"Divehi\",\n        \"nl\": \"Dutch\",\n        \"dz\": \"Dzongkha\",\n        \"en\": \"English\",\n        \"eo\": \"Esperanto\",\n        \"et\": \"Estonian\",\n        \"ee\": \"Ewe\",\n        \"fo\": \"Faroese\",\n        \"fj\": \"Fijian\",\n        \"fi\": \"Finnish\",\n        \"fr\": \"Franch\",\n        \"ff\": \"Fula\",\n        \"gl\": \"Galician\",\n        \"ka\": \"Georgian\",\n        \"de\": \"German\",\n        \"el\": \"Greek\",\n        \"gn\": \"Guaran\u00ed\",\n        \"gu\": \"Gujarati\",\n        \"ht\": \"Haitian\",\n        \"ha\": \"Hausa\",\n        \"he\": \"Hebrew\",\n        \"hz\": \"Herero\",\n        \"hi\": \"Hindi\",\n        \"ho\": \"Hiri Motu\",\n        \"hu\": \"Hungarian\",\n        \"ia\": \"Interlingua\",\n        \"id\": \"Indonesian\",\n        \"ie\": \"Interlingue\",\n        \"ga\": \"Irish\",\n        \"ig\": \"Igbo\",\n        \"ik\": \"Inupiaq\",\n        \"io\": \"Ido\",\n        \"is\": \"Icelandic\",\n        \"it\": \"Italian\",\n        \"iu\": \"Inuktitut\",\n        \"ja\": \"Japanese\",\n        \"jv\": \"Javanese\",\n        \"kl\": \"Kalaallisut\",\n        \"kn\": \"Kannada\",\n        \"kr\": \"Kanuri\",\n        \"ks\": \"Kashmiri\",\n        \"kk\": \"Kazakh\",\n        \"km\": \"Khmer\",\n        \"ki\": \"Kikuyu\",\n        \"rw\": \"Kinyarwanda\",\n        \"ky\": \"Kirghiz\",\n        \"kv\": \"Komi\",\n        \"kg\": \"Kongo\",\n        \"ko\": \"Korean\",\n        \"ku\": \"Kurdish\",\n        \"kj\": \"Kwanyama\",\n        \"la\": \"Latin\",\n        \"lb\": \"Luxembourgish\",\n        \"lg\": \"Luganda\",\n        \"li\": \"Limburgish\",\n        \"ln\": \"Lingala\",\n        \"lo\": \"Lao\",\n        \"lt\": \"Lithuanian\",\n        \"lu\": \"Luba-Katanga\",\n        \"lv\": \"Latvian\",\n        \"gv\": \"Manx\",\n        \"mk\": \"Macedonian\",\n        \"mg\": \"Malagasy\",\n        \"ms\": \"Malay\",\n        \"ml\": \"Malayalam\",\n        \"mt\": \"Maltese\",\n        \"mi\": \"M\u0101ori\",\n        \"mr\": \"Marathi\",\n        \"mh\": \"Marshallese\",\n        \"mn\": \"Mongolian\",\n        \"na\": \"Nauru\",\n        \"nv\": \"Navajo\",\n        \"nb\": \"Norwegian Bokm\u00e5l\",\n        \"nd\": \"North Ndebele\",\n        \"ne\": \"Nepali\",\n        \"ng\": \"Ndonga\",\n        \"nn\": \"Norwegian Nynorsk\",\n        \"no\": \"Norwegian\",\n        \"ii\": \"Nuosu\",\n        \"nr\": \"South Ndebele\",\n        \"oc\": \"Occitan\",\n        \"oj\": \"Ojibwe\",\n        \"cu\": \"Old Church Slavonic\",\n        \"om\": \"Oromo\",\n        \"or\": \"Oriya\",\n        \"os\": \"Ossetian\",\n        \"pa\": \"Panjabi\",\n        \"pi\": \"P\u0101li\",\n        \"fa\": \"Persian\",\n        \"pl\": \"Polish\",\n        \"ps\": \"Pashto\",\n        \"pt\": \"Portuguese\",\n        \"qu\": \"Quechua\",\n        \"rm\": \"Romansh\",\n        \"rn\": \"Kirundi\",\n        \"ro\": \"Romanian\",\n        \"ru\": \"Russian\",\n        \"sa\": \"Sanskrit\",\n        \"sc\": \"Sardinian\",\n        \"sd\": \"Sindhi\",\n        \"se\": \"Northern Sami\",\n        \"sm\": \"Samoan\",\n        \"sg\": \"Sango\",\n        \"sr\": \"Serbian\",\n        \"gd\": \"Scottish Gaelic\",\n        \"sn\": \"Shona\",\n        \"si\": \"Sinhala\",\n        \"sk\": \"Slovak\",\n        \"sl\": \"Slovene\",\n        \"so\": \"Somali\",\n        \"st\": \"Southern Sotho\",\n        \"es\": \"Spanish\",\n        \"su\": \"Sundanese\",\n        \"sw\": \"Swahili\",\n        \"ss\": \"Swati\",\n        \"sv\": \"Swedish\",\n        \"ta\": \"Tamil\",\n        \"te\": \"Telugu\",\n        \"tg\": \"Tajik\",\n        \"th\": \"Thai\",\n        \"ti\": \"Tigrinya\",\n        \"bo\": \"Tibetan\",\n        \"tk\": \"Turkmen\",\n        \"tl\": \"Tagalog\",\n        \"tn\": \"Tswana\",\n        \"to\": \"Tonga\",\n        \"tr\": \"Turkish\",\n        \"ts\": \"Tsonga\",\n        \"tt\": \"Tatar\",\n        \"tw\": \"Twi\",\n        \"ty\": \"Tahitian\",\n        \"ug\": \"Uighur\",\n        \"uk\": \"Ukrainian\",\n        \"ur\": \"Urdu\",\n        \"uz\": \"Uzbek\",\n        \"ve\": \"Venda\",\n        \"vi\": \"Vietnamese\",\n        \"vo\": \"Volap\u00fck\",\n        \"wa\": \"Walloon\",\n        \"cy\": \"Welsh\",\n        \"wo\": \"Wolof\",\n        \"fy\": \"Western Frisian\",\n        \"xh\": \"Xhosa\",\n        \"yi\": \"Yiddish\",\n        \"yo\": \"Yoruba\",\n        \"za\": \"Zhuang\",\n        \"zu\": \"Zulu\",\n    }\n\n    return lang_map[lan]", "\ndef read_csv(path_csv):\n    csv_reader = pd.read_csv(path_csv, sep=\"\\t\", header=0)\n    return csv_reader[\"times\"].tolist()\n\n\ndef retrieve_samples(path, dataset):\n    sacrebleu = datasets.load_metric(\"sacrebleu\")\n    decoders = [\"autoregressive\", \"jacobi\", \"gs_jacobi\", \"aw_jacobi\"]\n\n    decoder2file = {\n        \"autoregressive\": {\"time\": [], \"trans\": []},\n        \"jacobi\": {\"time\": [], \"trans\": []},\n        \"gs_jacobi\": {\"time\": [], \"trans\": []},\n        \"aw_jacobi\": {\"time\": [], \"trans\": []},\n    }\n\n    for folder in decoders:\n        subf = os.path.join(path, folder)\n\n        for root, dirs, files in os.walk(subf):\n            for filename in files:\n                if \"time\" in filename:\n                    times = read_csv(os.path.join(root, filename))\n                    decoder2file[folder]['time'].extend(times)\n                if 'trans' in filename:\n                    trans = read_csv(os.path.join(root, filename))\n                    decoder2file[folder]['trans'].extend(trans)\n\n    diff_times = np.array([auto-aw for auto, aw in zip(decoder2file['autoregressive']['time'], decoder2file['aw_jacobi']['time'])])\n    indices = diff_times.argsort()[::-1]\n\n    for i in indices:\n        target = dataset[int(i)][1]\n        source = dataset[int(i)][0]\n        print(\"gold\", \"nd\", source)\n        print(\"gold\", \"nd\", target)\n        for d in decoders:\n            prediction = decoder2file[d]['trans'][i]\n            results = sacrebleu.compute(predictions=[prediction], references=[[target]])\n            print(d, decoder2file[d]['time'][i], round(results['score'], 3), prediction)\n\n        input()\n        continue", "\ndef clean_text(text):\n    return re.sub(\"[!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\u00a3\u2212\u20ac\u00bf]+\", \" \", text)\n\n\ndef read_wrong_beam_translations(path):\n    csv_reader = pd.read_csv(path, sep=\"\\t\", header=0)\n    idx = csv_reader['i'].tolist()\n    bleus = csv_reader['bleu'].tolist()\n    beams = csv_reader['beam'].tolist()\n    autos = csv_reader['auto'].tolist()\n    tgts = csv_reader['tgt'].tolist()\n\n    for x in zip(idx, bleus, beams, autos, tgts):\n        print(f\"{x[0]} {x[1]}\\nBeam: {x[2]}\\nAuto: {x[3]}\\nTgt: {x[4]}\")\n        input()\n        continue", "\n\ndef write_sentences(path, data):\n\n    with open(path, 'w') as out_file:\n        output = csv.writer(out_file, delimiter=\"\\n\")\n        output.writerow(data)\n"]}
{"filename": "src/utils/bleu_calculator.py", "chunked_list": ["import os\n\nimport datasets\nimport pandas as pd\nfrom tabulate import tabulate\n\n\nclass BleuValues:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)", "\n\nclass BleuEvaluator(object):\n    def __init__(self):\n        self.metric = datasets.load_metric(\"sacrebleu\")\n\n    def add_element(self, model_predictions, gold_references):\n        self.metric.add(predictions=model_predictions, references=gold_references)\n\n    def add_batch(self, predictions, references):\n        self.metric.add_batch(predictions=predictions, references=references)\n\n    def final_score(self, model_predictions, gold_references):\n        return self.metric.compute(predictions=model_predictions, references=gold_references)", "\n\nclass BleuCalculator:\n    def __init__(\n        self,\n        dataset,\n        result_dir,\n    ):\n        self.dataset = dataset\n        self.result_dir = result_dir\n\n    @staticmethod\n    def read_csv(path_csv):\n        csv_reader = pd.read_csv(path_csv, sep=\"\\t\", header=0)\n        return {\n            k: v\n            for k, v in zip(\n                csv_reader[\"#sentence\"].tolist(), csv_reader[\"times\"].tolist()\n            )\n        }\n\n    def _retrieve_files(self):\n        file2data = dict()\n        for root, dirs, files in os.walk(self.result_dir):\n            if any(map(lambda x: \"trans\" in x, files)) and \"initrans\" not in root:\n                trans_files_name = list(filter(lambda x: (\"trans\" in x), files))[0]\n                data = self.read_csv(path_csv=os.path.join(root, trans_files_name))\n                file2data.update({trans_files_name.split(\".\")[-2]: data})\n        return file2data\n\n    def _load_dataset(self):\n        return {i: x[1] for i, x in enumerate(self.dataset)}\n\n    @staticmethod\n    def _match_indices(method, gold):\n        new_gold = dict()\n        for k in gold:\n            if k in method:\n                new_gold.update({k: gold[k]})\n\n        return new_gold\n\n    @staticmethod\n    def _bleu_score_formatter(bleu_score):\n\n        bleu_dict = {\n            \"score\": bleu_score[\"score\"],\n            \"counts\": str(bleu_score[\"counts\"]),\n            \"totals\": str(bleu_score[\"totals\"]),\n            \"precisions\": str([\"%.2f\" % prec for prec in bleu_score[\"precisions\"]]),\n            \"bp\": bleu_score[\"bp\"],\n            \"sys_len\": bleu_score[\"sys_len\"],\n            \"ref_len\": bleu_score[\"ref_len\"],\n        }\n\n        return BleuValues(**bleu_dict)\n\n    def write_report(self, file2score):\n        print(\"Writing report...\")\n\n        # Table for the Bleu score\n        header = [\"Metrics\"] + [m[0] for m in file2score]\n\n        bleu_table = tabulate(\n            [\n                [\"Score\"] + [b[1].score for b in file2score],\n                [\"Counts\"] + [b[1].counts for b in file2score],\n                [\"Totals\"] + [b[1].totals for b in file2score],\n                [\"Precisions\"] + [b[1].precisions for b in file2score],\n                [\"Bp\"] + [b[1].bp for b in file2score],\n                [\"Sys_len\"] + [b[1].sys_len for b in file2score],\n                [\"ref_len\"] + [b[1].ref_len for b in file2score],\n            ],\n            headers=header,\n            tablefmt=\"rst\",\n        )\n\n        with open(os.path.join(self.result_dir, \"bleu_report.txt\"), mode=\"w\") as report:\n            report.write(f\"Bleu Score\\n{bleu_table}\\n\\n\")\n\n    def _compute_bleu_score(self, name, translations, gold):\n        scorer = BleuEvaluator()\n\n        translations = list(translations.values())\n        gold = list(gold.values())\n\n        gold = [[g] for g in gold]\n\n        # for t, g in zip(translations, gold):\n        #     scorer.add_element(t, [g])\n\n        score_value = scorer.final_score(translations, gold)\n        return name, self._bleu_score_formatter(score_value)\n\n    def compute_bleu_score(self):\n        file2data = self._retrieve_files()\n        gold = self._load_dataset()\n\n        if \"trans_beam\" in file2data:\n            beam_search = self._match_indices(\n                file2data[\"trans_gs_jacobi\"], file2data[\"trans_beam\"]\n            )\n            file2data[\"trans_beam\"] = beam_search\n        gold = self._match_indices(file2data[\"trans_gs_jacobi\"], gold)\n\n        file2score = [\n            self._compute_bleu_score(file, file2data[file], gold) for file in file2data\n        ]\n\n        self.write_report(file2score)", ""]}
{"filename": "src/utils/beam_search.py", "chunked_list": ["import csv\nimport os\n\nimport torch\nimport time\nfrom tabulate import tabulate\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import MBartForConditionalGeneration, M2M100ForConditionalGeneration\nimport numpy as np", "from transformers import MBartForConditionalGeneration, M2M100ForConditionalGeneration\nimport numpy as np\n\nfrom src.utils.bench_scorer import Scorer\nfrom src.utils.utils import makedirs, retrieve_model_name, write_sentences, get_logits_preprocessor\n\n\nclass BeamSearcher(object):\n    def __init__(\n            self,\n            dataset,\n            model,\n            initializer,\n            decoder,\n            num_beams,\n            no_repeat_ngram_size,\n            early_stopping,\n            batch_size,\n            device,\n            result_dir,\n    ):\n        self.num_beams = num_beams\n        self.no_repeat_ngram_size = no_repeat_ngram_size\n        self.early_stopping = early_stopping\n        self.device = device\n        self.result_dir = result_dir\n\n        self.dataset = dataset\n        self.initializer = initializer\n        self.decoder = decoder\n\n        self.tokenizer = dataset.tokenizer\n        self.model = model\n        model.eval().to(self.device)\n\n        self.model_name = self.model.name_or_path\n        print(\"model name in beam_search\", self.model_name)\n\n        self.dataloader = DataLoader(\n            dataset, collate_fn=dataset.collate_fn, batch_size=1, shuffle=False\n        )\n\n        if isinstance(\n                self.model, MBartForConditionalGeneration\n        ) or isinstance(\n            self.model, M2M100ForConditionalGeneration\n        ):\n            self.is_mbart = True\n        else:\n            self.is_mbart = False\n\n        self.exp_dir = self._retrieve_exp_dir()\n\n    def _synchronize(self):\n        if self.device == \"cuda\":\n            torch.cuda.synchronize()\n\n    def _retrieve_exp_dir(self):\n        file_name = self._retrieve_file_name()\n\n        exp_dir = os.path.join(self.result_dir, 'beam_search', file_name)\n        makedirs(exp_dir)\n\n        return exp_dir\n\n    def _retrieve_file_name(self):\n        model_name = retrieve_model_name(self.model_name.split(\"/\")[1])\n        lang = f\"{self.dataset.src_lan}_{self.dataset.tgt_lan}\"\n        return f\"{model_name}/{self.dataset.name}/{lang}\"\n\n    def _beam_search(self, input_ids, attention_mask):\n      if self.is_mbart:\n        with self.tokenizer.as_target_tokenizer():\n            try:\n                lang_id = self.tokenizer.cur_lang_code_id\n            except:\n                lang_id = self.tokenizer.cur_lang_id\n        beam_output = self.model.generate(\n            **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n            num_beams=self.num_beams,\n            early_stopping=self.early_stopping,\n            # no_repeat_ngram_size=self.no_repeat_ngram_size,\n            forced_bos_token_id=lang_id,\n            # do_sample=False,\n            # use_cache=True,\n        )\n      else:\n          beam_output = self.model.generate(\n              **{\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n              num_beams=self.num_beams,\n              early_stopping=self.early_stopping,\n              # no_repeat_ngram_size=self.no_repeat_ngram_size,\n              # do_sample=False,\n              # use_cache=True,\n          )\n\n      return beam_output\n\n\n    def _bench_time(self, input_ids, attention_mask):\n        sample_time = []\n        for _ in range(1):\n          start = time.perf_counter()\n          self._synchronize()\n          beam_output = self._beam_search(input_ids, attention_mask)\n          self._synchronize()\n          end = time.perf_counter()\n          sample_time.append(end - start)\n\n        sample_mean = np.average(sample_time)\n        sample_variance = np.var(sample_time)\n\n        return sample_mean, sample_variance, beam_output\n\n    def _auto_time(self, input_ids, attention_mask, logits_preprocessor=None):\n\n        if self.initializer is not None:\n            init_tensor, _ = self.initializer.init_translation()\n        else:\n            init_tensor = None\n\n        sample_time = []\n        for _ in range(1):\n            init_new = init_tensor.clone()\n            start = time.perf_counter()\n            self._synchronize()\n            auto_output, _ = self.decoder.autoregressive(\n                input_ids, attention_mask, init_tensor=init_new, logits_preprocessor=logits_preprocessor\n            )\n            self._synchronize()\n            end = time.perf_counter()\n            sample_time.append(end - start)\n\n        sample_mean = np.average(sample_time)\n        sample_variance = np.var(sample_time)\n\n        return sample_mean, sample_variance, auto_output\n\n    def compute_beam_search(self, cfg):\n\n        beam_scorer, auto_scorer = Scorer(), Scorer()\n        worst_beam_translations = []\n\n        pbar = tqdm(self.dataloader, desc=\"Computing Beam Search...\")\n        for x in pbar:\n            input_ids = x[\"source\"][\"input_ids\"].to(self.device)\n            attention_mask = x[\"source\"][\"attention_mask\"].to(self.device)\n\n            tgt_text = x['target']['sentences']\n\n            if cfg.model.use_logits_preprocessor:\n                logits_preprocessor = get_logits_preprocessor(self.decoder, input_ids)\n            else:\n                logits_preprocessor = None\n\n            mean_beam, var_beam, beam_output = self._bench_time(input_ids, attention_mask)\n            mean_auto, var_auto, auto_output = self._auto_time(input_ids, attention_mask, logits_preprocessor)\n\n            translation_beam = self.tokenizer.batch_decode(\n                beam_output, skip_special_tokens=True\n            )\n            translation_auto = self.tokenizer.batch_decode(\n                auto_output, skip_special_tokens=True\n            )\n\n            beam_scorer.update_metrics(mean_beam, var_beam, 0, translation_beam[0], tgt_text[0])\n            auto_scorer.update_metrics(mean_auto, var_auto, 0, translation_auto[0], tgt_text[0])\n\n            worst_beam_translations.extend(\n                self._compute_tmp_bleu(\n                    translation_beam[0],\n                    translation_auto[0],\n                    tgt_text[0],\n                    beam_scorer.i\n                )\n            )\n\n        self.write_report(beam_scorer, auto_scorer, worst_beam_translations)\n\n    def write_report(self, beam_scorer, auto_scorer, worst_beam_translations):\n        print(\"Writing report...\")\n\n        beam_score = beam_scorer.compute_bleu_score()\n        auto_score = auto_scorer.compute_bleu_score()\n\n        worst_beam_translations.sort(key=lambda x: x[1])\n\n        # Table for the test info\n        info_table = tabulate([\n            ['Model', self.model_name],\n            ['Dataset', self.dataset.name],\n            ['Languages', f\"{self.dataset.src_lan}-{self.dataset.tgt_lan}\"],\n            ['Device', self.device],\n            ['Sentences', beam_scorer.i],\n            ['Num Beams', self.num_beams],\n            ['No Rep Ngram Size', self.no_repeat_ngram_size],\n            ['Early Stopping', self.early_stopping],\n            ['Do Sample', False],\n            ['Use Cache', True],\n        ], headers=['Info', 'Value'], tablefmt='grid')\n\n        # Table for the Time\n        time_table = tabulate([\n            ['Time', beam_scorer.tot_mean_time, auto_scorer.tot_mean_time, (auto_scorer.tot_mean_time / beam_scorer.tot_mean_time)],\n            ['Iter', beam_scorer.tot_mean_iter, auto_scorer.tot_mean_iter, 1],\n            ['Var', beam_scorer.tot_var_time, auto_scorer.tot_var_time, 1],\n        ], headers=['Metrics', 'Beam', 'Auto', 'Speedup'], tablefmt='grid')\n\n        # Table for the Bleu score\n        bleu_table = tabulate([\n            ['Score', beam_score.score, auto_score.score],\n            ['Counts', beam_score.counts, auto_score.counts],\n            ['Totals', beam_score.totals, auto_score.totals],\n            ['Precisions', beam_score.precisions, auto_score.precisions],\n            ['Bp', beam_score.bp, auto_score.bp],\n            ['Sys_len', beam_score.sys_len, auto_score.sys_len],\n            ['ref_len', beam_score.ref_len, auto_score.ref_len],\n        ], headers=['Metrics', 'Beam Search', 'Auto'], tablefmt='rst')\n\n        with open(os.path.join(self.exp_dir, \"report.txt\"), mode='w') as report:\n            report.write(f\"Test Info\\n{info_table}\\n\\n\")\n            report.write(f\"Time\\n{time_table}\\n\\n\")\n            report.write(f\"Bleu Score\\n{bleu_table}\\n\\n\")\n\n        with open(os.path.join(self.exp_dir, \"worst_beam_translation.csv\"), 'w') as csvfile:\n            writer = csv.writer(csvfile, delimiter='\\t')\n            writer.writerow(['i', 'bleu', 'beam', 'auto', 'tgt'])\n            for sample in worst_beam_translations:\n                writer.writerow(list(sample))\n\n        write_sentences(os.path.join(self.exp_dir, \"beam.txt\"), beam_scorer.predictions)\n        write_sentences(os.path.join(self.exp_dir, \"auto.txt\"), auto_scorer.predictions)\n        write_sentences(os.path.join(self.exp_dir, \"reference.txt\"), sum(auto_scorer.references, []))\n\n    def _compute_tmp_bleu(self, translation_beam, translation_auto, tgt_text, i):\n\n        beam_tmp_scorer, auto_tmp_scorer = Scorer(), Scorer()\n\n        beam_tmp_scorer.update_metrics(0, 0, 0, translation_beam, tgt_text)\n        auto_tmp_scorer.update_metrics(0, 0, 0, translation_auto, tgt_text)\n\n        beam_score = beam_tmp_scorer.compute_bleu_score()\n        auto_score = auto_tmp_scorer.compute_bleu_score()\n\n        if beam_score.score < auto_score.score:\n            return [(i, beam_score.score, translation_beam, translation_auto, tgt_text)]\n        else:\n            return []", "\n\n\n"]}
{"filename": "src/dataset/iwslt_dataset.py", "chunked_list": ["import os\nimport typing as t\n\nimport datasets\nimport torch\nfrom datasets.utils.download_manager import DownloadManager\nfrom torch.utils.data.dataset import Dataset\n\nfrom src.utils.utils import clean_text\n", "from src.utils.utils import clean_text\n\n\nclass Iwslt(Dataset):\n    def __init__(\n        self,\n        version: str = \"17\",\n        src_lan: str = \"en\",\n        tgt_lan: str = \"ro\",\n        data_dir: str = None,\n        hugginface_tokenizer=None,\n        split: str = None,\n    ):\n        self.version = version\n        self.src_lan = src_lan\n        self.tgt_lan = tgt_lan\n        self.max_length = 511\n\n        self.dl = DownloadManager()\n\n        self.name = f\"iwslt{self.version}\"\n\n        self.version2folder = {\n            \"15\": os.path.join(data_dir, \"2015-01/texts\"),\n            \"17\": os.path.join(data_dir, \"2017-01-trnted/texts\"),\n        }\n        self.version2years = {\n            \"15\": {\"train_and_test\": [2010, 2011, 2012, 2013], \"dev\": [2010]},\n            \"17\": {\n                \"train_and_test\": [2010, 2011, 2012, 2013, 2014, 2015],\n                \"dev\": [2010],\n            },\n        }\n\n        data_file = f\"{self.version2folder[version]}/{src_lan}/{tgt_lan}/{src_lan}-{tgt_lan}.tgz\"\n\n        splitted_generators = self._split_generators(data_file)\n        self.translation_dataset = self.load_dataset(splitted_generators, split=split)\n\n        with torch.no_grad():\n            self.tokenizer = hugginface_tokenizer\n\n    def load_dataset(\n        self,\n        splitted_generators: t.List[datasets.SplitGenerator],\n        split: str,\n    ) -> t.List[t.Dict]:\n        splitted_generators = self.concat_dataset(splitted_generators, split)\n\n        return list(\n            self._generate_examples(\n                source_files=splitted_generators.gen_kwargs[\"source_files\"],\n                target_files=splitted_generators.gen_kwargs[\"target_files\"],\n            )\n        )\n\n    @staticmethod\n    def concat_dataset(\n        splitted_generators: t.List[datasets.SplitGenerator],\n        split: str,\n    ) -> datasets.SplitGenerator:\n        split2ix = {\"train\": 0, \"test\": 1, \"validation\": 2}\n        assert (\n            split in split2ix\n        ), \"Iwslt: split must be either train or test on validation\"\n        if split is not None:\n            return splitted_generators[split2ix[split]]\n\n    def _split_generators(self, data_file: str) -> t.List[datasets.SplitGenerator]:\n        \"\"\"Returns SplitGenerators.\"\"\"\n        pair = f\"{self.src_lan}-{self.tgt_lan}\"\n        dl_dir = self.dl.extract(data_file)\n        data_dir = os.path.join(dl_dir, f\"{self.src_lan}-{self.tgt_lan}\")\n\n        years = self.version2years[self.version][\"train_and_test\"]\n        dev = self.version2years[self.version][\"dev\"]\n\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    \"source_files\": [\n                        os.path.join(\n                            data_dir,\n                            f\"train.tags.{pair}.{self.src_lan}\",\n                        )\n                    ],\n                    \"target_files\": [\n                        os.path.join(\n                            data_dir,\n                            f\"train.tags.{pair}.{self.tgt_lan}\",\n                        )\n                    ],\n                    \"split\": \"train\",\n                },\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    \"source_files\": [\n                        os.path.join(\n                            data_dir,\n                            f\"IWSLT{self.version}.TED.tst{year}.{pair}.{self.src_lan}.xml\",\n                        )\n                        for year in years\n                    ],\n                    \"target_files\": [\n                        os.path.join(\n                            data_dir,\n                            f\"IWSLT{self.version}.TED.tst{year}.{pair}.{self.tgt_lan}.xml\",\n                        )\n                        for year in years\n                    ],\n                    \"split\": \"test\",\n                },\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.VALIDATION,\n                # These kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    \"source_files\": [\n                        os.path.join(\n                            data_dir,\n                            f\"IWSLT{self.version}.TED.dev{year}.{pair}.{self.src_lan}.xml\",\n                        )\n                        for year in dev\n                    ],\n                    \"target_files\": [\n                        os.path.join(\n                            data_dir,\n                            f\"IWSLT{self.version}.TED.dev{year}.{pair}.{self.tgt_lan}.xml\",\n                        )\n                        for year in dev\n                    ],\n                    \"split\": \"validation\",\n                },\n            ),\n        ]\n\n    def _generate_examples(\n        self, source_files: t.List[str], target_files: t.List[str]\n    ) -> t.List[t.Dict]:\n        \"\"\"Yields examples.\"\"\"\n        for source_file, target_file in zip(source_files, target_files):\n            with open(source_file, \"r\", encoding=\"utf-8\") as sf:\n                with open(target_file, \"r\", encoding=\"utf-8\") as tf:\n                    for source_row, target_row in zip(sf, tf):\n                        source_row = source_row.strip()\n                        target_row = target_row.strip()\n\n                        if source_row.startswith(\"<\"):\n                            if source_row.startswith(\"<seg\"):\n                                # Remove <seg id=\"1\">.....</seg>\n                                # Very simple code instead of regex or xml parsing\n                                part1 = source_row.split(\">\")[1]\n                                source_row = part1.split(\"<\")[0]\n                                part1 = target_row.split(\">\")[1]\n                                target_row = part1.split(\"<\")[0]\n\n                                source_row = source_row.strip()\n                                target_row = target_row.strip()\n                            else:\n                                continue\n\n                        yield {\n                            \"translation\": {\n                                self.src_lan: source_row,\n                                self.tgt_lan: target_row,\n                            }\n                        }\n\n    def collate_fn(self, batch):\n\n        batch_source = [b[0] for b in batch]\n        batch_target = [b[1] for b in batch]\n\n        encoded_source = self.tokenizer(\n            batch_source,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        encoded_target = self.tokenizer(\n            batch_target,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"source\": {\n                \"input_ids\": encoded_source[\"input_ids\"],\n                \"attention_mask\": encoded_source[\"attention_mask\"],\n                \"sentences\": batch_source,\n            },\n            \"target\": {\n                \"input_ids\": encoded_target[\"input_ids\"],\n                \"attention_mask\": encoded_target[\"attention_mask\"],\n                \"sentences\": batch_target,\n            },\n        }\n\n    def __len__(self) -> int:\n        return len(self.translation_dataset)\n\n    def __getitem__(self, idx: int) -> t.Tuple[str, str]:\n        sample = self.translation_dataset[idx]\n        source = sample[\"translation\"][self.src_lan]\n        target = sample[\"translation\"][self.tgt_lan]\n\n        return source, target", ""]}
{"filename": "src/dataset/wmt_dataset.py", "chunked_list": ["import typing as t\n\nimport datasets\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom src.utils.utils import clean_text\n\n\nclass Wmt(Dataset):\n    \"\"\"\n    Wmt machine translation dataset reader\n\n    Input:\n        - version -> the dataset version dataset, by default '16' (dataset-16)\n        - src_lan -> the source language, by default 'ro' (Romanian)\n        - tgt_lan -> the target language, by default 'en' (English)\n        - tokenizer_model -> the tokenizer model\n        - split -> if not None, allows to split the dataset in following set: ['train', 'test', 'validation']\n        - concat -> if not None, make possible the concatenation of the specified set.\n                    Note: It works only if split is None\n                    It can be: ['train', 'test', 'validation']\n    \"\"\"\n\n    def __init__(\n        self,\n        version: str = \"16\",\n        src_lan: str = \"ro\",\n        tgt_lan: str = \"en\",\n        hugginface_tokenizer=None,\n        split: str = None,\n    ):\n        self.src_lan = src_lan\n        self.tgt_lan = tgt_lan\n        self.tokenizer_model = hugginface_tokenizer\n        self.max_length = 511\n\n        if src_lan == \"en\":\n            source2target = \"{}-{}\".format(self.tgt_lan, self.src_lan)\n        else:\n            source2target = \"{}-{}\".format(self.src_lan, self.tgt_lan)\n\n        if version == \"19\" and \"test\" in split:\n            split = \"validation\"\n\n        version = f\"wmt{version}\"\n\n        self.name = version\n\n        try:\n            self.translation_dataset = datasets.load_dataset(\n                version, source2target, split=split\n            )\n        except:\n            raise ValueError(\n                f\"{version} can read only the pairs cs-en, en-cs, de-en, en-de,\"\n                f\" fi-en, en-fi, ro-en, en-ro, ru-en, en-ru, tr-en, en-tr\"\n            )\n\n        with torch.no_grad():\n            self.tokenizer = hugginface_tokenizer\n\n    def collate_fn(self, batch):\n\n        batch_source = [b[0] for b in batch]\n        batch_target = [b[1] for b in batch]\n\n        encoded_source = self.tokenizer(\n            batch_source,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        encoded_target = self.tokenizer(\n            batch_target,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"source\": {\n                \"input_ids\": encoded_source[\"input_ids\"],\n                \"attention_mask\": encoded_source[\"attention_mask\"],\n                \"sentences\": batch_source,\n            },\n            \"target\": {\n                \"input_ids\": encoded_target[\"input_ids\"],\n                \"attention_mask\": encoded_target[\"attention_mask\"],\n                \"sentences\": batch_target,\n            },\n        }\n\n    def __len__(self) -> int:\n        return len(self.translation_dataset)\n\n    def __getitem__(self, idx: int) -> t.Tuple[str, str]:\n        sample = self.translation_dataset[idx]\n        source = sample[\"translation\"][self.src_lan]\n        target = sample[\"translation\"][self.tgt_lan]\n\n        return source, target", "\nclass Wmt(Dataset):\n    \"\"\"\n    Wmt machine translation dataset reader\n\n    Input:\n        - version -> the dataset version dataset, by default '16' (dataset-16)\n        - src_lan -> the source language, by default 'ro' (Romanian)\n        - tgt_lan -> the target language, by default 'en' (English)\n        - tokenizer_model -> the tokenizer model\n        - split -> if not None, allows to split the dataset in following set: ['train', 'test', 'validation']\n        - concat -> if not None, make possible the concatenation of the specified set.\n                    Note: It works only if split is None\n                    It can be: ['train', 'test', 'validation']\n    \"\"\"\n\n    def __init__(\n        self,\n        version: str = \"16\",\n        src_lan: str = \"ro\",\n        tgt_lan: str = \"en\",\n        hugginface_tokenizer=None,\n        split: str = None,\n    ):\n        self.src_lan = src_lan\n        self.tgt_lan = tgt_lan\n        self.tokenizer_model = hugginface_tokenizer\n        self.max_length = 511\n\n        if src_lan == \"en\":\n            source2target = \"{}-{}\".format(self.tgt_lan, self.src_lan)\n        else:\n            source2target = \"{}-{}\".format(self.src_lan, self.tgt_lan)\n\n        if version == \"19\" and \"test\" in split:\n            split = \"validation\"\n\n        version = f\"wmt{version}\"\n\n        self.name = version\n\n        try:\n            self.translation_dataset = datasets.load_dataset(\n                version, source2target, split=split\n            )\n        except:\n            raise ValueError(\n                f\"{version} can read only the pairs cs-en, en-cs, de-en, en-de,\"\n                f\" fi-en, en-fi, ro-en, en-ro, ru-en, en-ru, tr-en, en-tr\"\n            )\n\n        with torch.no_grad():\n            self.tokenizer = hugginface_tokenizer\n\n    def collate_fn(self, batch):\n\n        batch_source = [b[0] for b in batch]\n        batch_target = [b[1] for b in batch]\n\n        encoded_source = self.tokenizer(\n            batch_source,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        encoded_target = self.tokenizer(\n            batch_target,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"source\": {\n                \"input_ids\": encoded_source[\"input_ids\"],\n                \"attention_mask\": encoded_source[\"attention_mask\"],\n                \"sentences\": batch_source,\n            },\n            \"target\": {\n                \"input_ids\": encoded_target[\"input_ids\"],\n                \"attention_mask\": encoded_target[\"attention_mask\"],\n                \"sentences\": batch_target,\n            },\n        }\n\n    def __len__(self) -> int:\n        return len(self.translation_dataset)\n\n    def __getitem__(self, idx: int) -> t.Tuple[str, str]:\n        sample = self.translation_dataset[idx]\n        source = sample[\"translation\"][self.src_lan]\n        target = sample[\"translation\"][self.tgt_lan]\n\n        return source, target", ""]}
{"filename": "src/dataset/__init__.py", "chunked_list": [""]}
{"filename": "src/dataset/ittb_dataset.py", "chunked_list": ["import torch\nfrom datasets import load_dataset\nfrom torch.utils.data.dataset import Dataset\n\nfrom src.utils.utils import clean_text\n\n\nclass Ittb(Dataset):\n    def __init__(\n        self,\n        src_lan,\n        tgt_lan,\n        hugginface_tokenizer=None,\n        split: str = None,\n    ):\n        self.src_lan = src_lan\n        self.tgt_lan = tgt_lan\n        self.name = \"ittb\"\n        self.max_length = 511\n\n        assert (\n            src_lan == \"en\" or src_lan == \"hi\"\n        ), \"Ittb: src_lan must be either en or hi\"\n        assert (\n            tgt_lan == \"en\" or tgt_lan == \"hi\"\n        ), \"Ittb: tgt_lan must be either en or hi\"\n        assert src_lan != tgt_lan, \"Ittb: src_lan and tgt_lan cannot be the same\"\n\n        self.translation_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=split)\n\n        with torch.no_grad():\n            self.tokenizer = hugginface_tokenizer\n\n    def collate_fn(self, batch):\n\n        batch_source = [b[0] for b in batch]\n        batch_target = [b[1] for b in batch]\n\n        encoded_source = self.tokenizer(\n            batch_source,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        encoded_target = self.tokenizer(\n            batch_target,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"source\": {\n                \"input_ids\": encoded_source[\"input_ids\"],\n                \"attention_mask\": encoded_source[\"attention_mask\"],\n                \"sentences\": batch_source,\n            },\n            \"target\": {\n                \"input_ids\": encoded_target[\"input_ids\"],\n                \"attention_mask\": encoded_target[\"attention_mask\"],\n                \"sentences\": batch_target,\n            },\n        }\n\n    def __len__(self):\n        return len(self.translation_dataset)\n\n    def __getitem__(self, idx: int):\n        source = self.translation_dataset[\"translation\"][idx][self.src_lan]\n        target = self.translation_dataset[\"translation\"][idx][self.tgt_lan]\n\n        return source, target", ""]}
{"filename": "src/dataset/flores_dataset.py", "chunked_list": ["import torch\nfrom torch.utils.data import Dataset\nimport datasets\n\nfrom src.utils.utils import retrieve_map_languages_flores, clean_text\nimport typing as t\n\n\nclass Flores(Dataset):\n    def __init__(\n        self,\n        src_lan: str = \"ro\",\n        tgt_lan: str = \"en\",\n        hugginface_tokenizer=None,\n        split: str = None,\n    ):\n        self.name = \"flores\"\n        self.max_length = 511\n        self.src_lan = retrieve_map_languages_flores(src_lan).lower()[:3]\n        self.tgt_lan = retrieve_map_languages_flores(tgt_lan).lower()[:3]\n\n        if \"test\" in split:\n            split = \"dev\" + split\n\n        self.translation_dataset_src = datasets.load_dataset(\n            \"gsarti/flores_101\", self.src_lan, split=split\n        )\n        self.translation_dataset_tgt = datasets.load_dataset(\n            \"gsarti/flores_101\", self.tgt_lan, split=split\n        )\n\n        with torch.no_grad():\n            self.tokenizer = hugginface_tokenizer\n\n    def collate_fn(self, batch):\n\n        batch_source = [b[0] for b in batch]\n        batch_target = [b[1] for b in batch]\n\n        encoded_source = self.tokenizer(\n            batch_source,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        encoded_target = self.tokenizer(\n            batch_target,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"source\": {\n                \"input_ids\": encoded_source[\"input_ids\"],\n                \"attention_mask\": encoded_source[\"attention_mask\"],\n                \"sentences\": batch_source,\n            },\n            \"target\": {\n                \"input_ids\": encoded_target[\"input_ids\"],\n                \"attention_mask\": encoded_target[\"attention_mask\"],\n                \"sentences\": batch_target,\n            },\n        }\n\n    def __len__(self) -> int:\n        return self.translation_dataset_src.num_rows\n\n    def __getitem__(self, idx: int) -> t.Tuple[str, str]:\n        source = str(self.translation_dataset_src.data.column(6)[idx])\n        target = str(self.translation_dataset_tgt.data.column(6)[idx])\n\n        return source, target", "class Flores(Dataset):\n    def __init__(\n        self,\n        src_lan: str = \"ro\",\n        tgt_lan: str = \"en\",\n        hugginface_tokenizer=None,\n        split: str = None,\n    ):\n        self.name = \"flores\"\n        self.max_length = 511\n        self.src_lan = retrieve_map_languages_flores(src_lan).lower()[:3]\n        self.tgt_lan = retrieve_map_languages_flores(tgt_lan).lower()[:3]\n\n        if \"test\" in split:\n            split = \"dev\" + split\n\n        self.translation_dataset_src = datasets.load_dataset(\n            \"gsarti/flores_101\", self.src_lan, split=split\n        )\n        self.translation_dataset_tgt = datasets.load_dataset(\n            \"gsarti/flores_101\", self.tgt_lan, split=split\n        )\n\n        with torch.no_grad():\n            self.tokenizer = hugginface_tokenizer\n\n    def collate_fn(self, batch):\n\n        batch_source = [b[0] for b in batch]\n        batch_target = [b[1] for b in batch]\n\n        encoded_source = self.tokenizer(\n            batch_source,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        encoded_target = self.tokenizer(\n            batch_target,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"source\": {\n                \"input_ids\": encoded_source[\"input_ids\"],\n                \"attention_mask\": encoded_source[\"attention_mask\"],\n                \"sentences\": batch_source,\n            },\n            \"target\": {\n                \"input_ids\": encoded_target[\"input_ids\"],\n                \"attention_mask\": encoded_target[\"attention_mask\"],\n                \"sentences\": batch_target,\n            },\n        }\n\n    def __len__(self) -> int:\n        return self.translation_dataset_src.num_rows\n\n    def __getitem__(self, idx: int) -> t.Tuple[str, str]:\n        source = str(self.translation_dataset_src.data.column(6)[idx])\n        target = str(self.translation_dataset_tgt.data.column(6)[idx])\n\n        return source, target", ""]}
