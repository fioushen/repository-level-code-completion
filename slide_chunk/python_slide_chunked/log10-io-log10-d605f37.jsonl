{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\nfrom distutils.core import setup\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"Log10\",\n    version=\"0.2.8\",\n    description=\"Log10 LLM data management\",\n    author=\"Log10 team\",", "    description=\"Log10 LLM data management\",\n    author=\"Log10 team\",\n    author_email=\"team@log10.io\",\n    url=\"\",\n    packages=find_packages(),\n    install_requires=[\n        \"openai\",\n        \"python-dotenv\",\n    ],\n    extras_require={", "    ],\n    extras_require={\n        \"bigquery\": [\"google-cloud-bigquery\"],\n        \"dev\": [\"chromadb\"],\n    },\n)\n"]}
{"filename": "log10/evals.py", "chunked_list": ["import subprocess\nimport tempfile\nimport os\nimport csv\nimport json\nimport logging\nfrom log10.llm import Messages\nfrom log10.utils import fuzzy_match, parse_field\n\n\ndef run_metric(metric, ideal, model_output):\n    ideal = parse_field(ideal)\n    for ideal_candidate in ideal:\n        # Ref: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/utils.py\n        if metric == \"match\":\n            if model_output.startswith(ideal_candidate):\n                return True\n        elif metric == \"includes\":  # case-insensitive\n            # Ref: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/basic/includes.py\n            if ideal_candidate.lower() in model_output.lower():\n                return True\n        elif metric == \"fuzzy_match\":\n            if fuzzy_match(ideal_candidate, model_output):\n                return True\n    return False", "\n\ndef run_metric(metric, ideal, model_output):\n    ideal = parse_field(ideal)\n    for ideal_candidate in ideal:\n        # Ref: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/utils.py\n        if metric == \"match\":\n            if model_output.startswith(ideal_candidate):\n                return True\n        elif metric == \"includes\":  # case-insensitive\n            # Ref: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/basic/includes.py\n            if ideal_candidate.lower() in model_output.lower():\n                return True\n        elif metric == \"fuzzy_match\":\n            if fuzzy_match(ideal_candidate, model_output):\n                return True\n    return False", "\n\ndef write_to_csv(file_name, row_data):\n    with open(file_name, \"a\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerow(row_data)\n\n\ndef eval(llm, dataset, metric, out_file_path):\n    csv_file_name, mapping = dataset\n    with open(csv_file_name, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        examples = []\n        for example in reader:\n            mapped_example = {}\n            for key, value in mapping.items():\n                mapped_example[key] = example[value]\n            examples.append(mapped_example)\n\n        # todo: each example could be launched as separate threads or separate api calls to job runners\n        write_to_csv(out_file_path, [\"input\", \"ideal\", \"model_completion\", \"metric\"])\n        for example in examples:\n            messages = Messages.from_dict(json.loads(example[\"input\"]))\n            model_completion = llm.chat(messages)\n\n            example_metric = run_metric(metric, example[\"ideal\"], model_completion.content)\n            write_to_csv(\n                out_file_path,\n                [example[\"input\"], example[\"ideal\"], model_completion, example_metric],\n            )\n            print(f\"\\n\\nIdeal:{example['ideal']}\\nModel output:{model_completion.content}\")\n            print(f\"Correct:{example_metric}\")\n    return", "def eval(llm, dataset, metric, out_file_path):\n    csv_file_name, mapping = dataset\n    with open(csv_file_name, \"r\") as csv_file:\n        reader = csv.DictReader(csv_file)\n        examples = []\n        for example in reader:\n            mapped_example = {}\n            for key, value in mapping.items():\n                mapped_example[key] = example[value]\n            examples.append(mapped_example)\n\n        # todo: each example could be launched as separate threads or separate api calls to job runners\n        write_to_csv(out_file_path, [\"input\", \"ideal\", \"model_completion\", \"metric\"])\n        for example in examples:\n            messages = Messages.from_dict(json.loads(example[\"input\"]))\n            model_completion = llm.chat(messages)\n\n            example_metric = run_metric(metric, example[\"ideal\"], model_completion.content)\n            write_to_csv(\n                out_file_path,\n                [example[\"input\"], example[\"ideal\"], model_completion, example_metric],\n            )\n            print(f\"\\n\\nIdeal:{example['ideal']}\\nModel output:{model_completion.content}\")\n            print(f\"Correct:{example_metric}\")\n    return", "\n\ndef compile(code):\n    with tempfile.NamedTemporaryFile(suffix=\".c\", delete=False) as temp:\n        temp.write(code.encode())\n        temp.close()\n        process = subprocess.run(\n            [\"gcc\", temp.name], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        os.remove(temp.name)  # remove the temp file\n        if process.returncode == 0:\n            return True\n        else:\n            return False, process.stderr.decode()", ""]}
{"filename": "log10/langchain.py", "chunked_list": ["import time\nimport uuid\n\nfrom langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseMessage\nfrom uuid import UUID\n\n\"\"\"Callback Handler that prints to std out.\"\"\"\nfrom typing import Any, Dict, List, Optional, Union\n\nfrom langchain.callbacks.base import BaseCallbackHandler", "\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.schema import AgentAction, AgentFinish, LLMResult\nfrom log10.llm import LLM, Kind, Message\n\nimport logging\nlogging.basicConfig()\nlogger = logging.getLogger(\"log10\")\n\ndef kwargs_to_hparams(kwargs: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Convert kwargs to hparams.\"\"\"\n    hparams = {}\n    if \"temperature\" in kwargs:\n        hparams[\"temperature\"] = kwargs[\"temperature\"]\n    if \"top_p\" in kwargs:\n        hparams[\"top_p\"] = kwargs[\"top_p\"]\n    if \"top_k\" in kwargs:\n        hparams[\"top_k\"] = kwargs[\"top_k\"]\n    if \"max_tokens\" in kwargs:\n        hparams[\"max_tokens\"] = kwargs[\"max_tokens\"]\n    if \"max_tokens_to_sample\" in kwargs:\n        hparams[\"max_tokens\"] = kwargs[\"max_tokens_to_sample\"]\n    if \"frequency_penalty\" in kwargs:\n        hparams[\"frequency_penalty\"] = kwargs[\"frequency_penalty\"]\n    if \"presence_penalty\" in kwargs:\n        hparams[\"presence_penalty\"] = kwargs[\"presence_penalty\"]\n    return hparams", "\ndef kwargs_to_hparams(kwargs: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Convert kwargs to hparams.\"\"\"\n    hparams = {}\n    if \"temperature\" in kwargs:\n        hparams[\"temperature\"] = kwargs[\"temperature\"]\n    if \"top_p\" in kwargs:\n        hparams[\"top_p\"] = kwargs[\"top_p\"]\n    if \"top_k\" in kwargs:\n        hparams[\"top_k\"] = kwargs[\"top_k\"]\n    if \"max_tokens\" in kwargs:\n        hparams[\"max_tokens\"] = kwargs[\"max_tokens\"]\n    if \"max_tokens_to_sample\" in kwargs:\n        hparams[\"max_tokens\"] = kwargs[\"max_tokens_to_sample\"]\n    if \"frequency_penalty\" in kwargs:\n        hparams[\"frequency_penalty\"] = kwargs[\"frequency_penalty\"]\n    if \"presence_penalty\" in kwargs:\n        hparams[\"presence_penalty\"] = kwargs[\"presence_penalty\"]\n    return hparams", "\n\nclass Log10Callback(BaseCallbackHandler, LLM):\n    \"\"\"Callback Handler that prints to std out.\"\"\"\n\n    def __init__(self, log10_config: Optional[dict] = None) -> None:\n        \"\"\"Initialize callback handler.\"\"\"\n        super().__init__(log10_config=log10_config, hparams=None)\n        self.runs = {}\n\n        if log10_config.DEBUG:\n            logger.setLevel(logging.DEBUG)\n        else:\n            logger.setLevel(logging.INFO)\n\n    def on_llm_start(\n        self,\n        serialized: Dict[str, Any],\n        prompts: List[str],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Print out the prompts.\"\"\"\n        logger.debug(\n            f\"**\\n**on_llm_start**\\n**\\n: serialized:\\n {serialized} \\n\\n prompts:\\n {prompts} \\n\\n rest: {kwargs}\"\n        )\n        kwargs = serialized.get(\"kwargs\", {})\n        hparams = kwargs_to_hparams(kwargs)\n\n        model = kwargs.get(\"model_name\", None)\n        if model is None:\n            model = kwargs.get(\"model\", None)\n        if model is None:\n            raise BaseException(\"No model found in serialized or kwargs\")\n\n        if len(prompts) != 1:\n            raise BaseException(\"Only support one prompt at a time\")\n\n        request = {\"model\": model, \"prompt\": prompts[0], **hparams}\n\n        logger.debug(f\"request: {request}\")\n\n        completion_id = self.log_start(request, Kind.text, tags)\n\n        self.runs[run_id] = {\n            \"kind\": Kind.text,\n            \"completion_id\": completion_id,\n            \"start_time\": time.perf_counter(),\n            \"model\": model,\n        }\n\n    def on_chat_model_start(\n        self,\n        serialized: Dict[str, Any],\n        messages: List[List[BaseMessage]],\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        tags: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> None:\n        logger.debug(\n            f\"**\\n**on_chat_model_start**\\n**\\n: run_id:{run_id}\\nserialized:\\n{serialized}\\n\\nmessages:\\n{messages}\\n\\nkwargs: {kwargs}\"\n        )\n\n        #\n        # Find model string\n        #\n        kwargs = serialized.get(\"kwargs\", {})\n        model = kwargs.get(\"model_name\", None)\n        if model is None:\n            model = kwargs.get(\"model\", None)\n        if model is None:\n            raise BaseException(\"No model found in serialized or kwargs\")\n\n        hparams = kwargs_to_hparams(kwargs)\n        hparams[\"model\"] = model\n\n        logger.debug(f\"hparams: {hparams}\")\n\n        if len(messages) != 1:\n            raise BaseException(\"Only support one message at a time\")\n\n        # Convert messages to log10 format\n        log10_messages = []\n        for message in messages[0]:\n            logger.debug(f\"message: {message}\")\n            if isinstance(message, HumanMessage):\n                log10_messages.append(Message(role=\"user\", content=message.content))\n            elif isinstance(message, AIMessage):\n                log10_messages.append(\n                    Message(role=\"assistant\", content=message.content)\n                )\n            elif isinstance(message, SystemMessage):\n                log10_messages.append(Message(role=\"system\", content=message.content))\n            else:\n                raise BaseException(f\"Unknown message type {type(message)}\")\n\n        request = {\n            \"messages\": [message.to_dict() for message in log10_messages],\n            **hparams,\n        }\n        logger.debug(f\"request: {request}\")\n\n        completion_id = self.log_start(\n            request,\n            Kind.chat,\n            tags,\n        )\n\n        self.runs[run_id] = {\n            \"kind\": Kind.chat,\n            \"completion_id\": completion_id,\n            \"start_time\": time.perf_counter(),\n            \"model\": model,\n        }\n\n        logger.debug(f\"logged start with completion_id: {completion_id}\")\n\n    def on_llm_end(\n        self,\n        response: LLMResult,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Do nothing.\"\"\"\n        # Find run in runs.\n        run = self.runs.get(run_id, None)\n        if run is None:\n            raise BaseException(\"Could not find run in runs\")\n\n        if run[\"kind\"] != Kind.chat and run[\"kind\"] != Kind.text:\n            raise BaseException(\"Only support chat kind\")\n\n        duration = time.perf_counter() - run[\"start_time\"]\n\n        # Log end\n        if len(response.generations) != 1:\n            raise BaseException(\"Only support one message at a time\")\n        if len(response.generations[0]) != 1:\n            raise BaseException(\"Only support one message at a time\")\n\n        content = response.generations[0][0].text\n\n        log10response = {}\n        if run[\"kind\"] == Kind.chat:\n            log10response = {\n                \"id\": str(uuid.uuid4()),\n                \"object\": \"chat.completion\",\n                \"model\": run[\"model\"],\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"message\": {\"role\": \"assistant\", \"content\": content},\n                        \"finish_reason\": \"stop\",\n                    }\n                ],\n            }\n        elif run[\"kind\"] == Kind.text:\n            log10response = {\n                \"id\": str(uuid.uuid4()),\n                \"object\": \"text_completion\",\n                \"model\": run[\"model\"],\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"text\": content,\n                        \"logprobs\": None,\n                        \"finish_reason\": \"stop\",\n                    }\n                ],\n            }\n\n        # Determine if we can provide usage metrics (token count).\n        logger.debug(f\"**** response: {response}\")\n        if response.llm_output is not None:\n            token_usage = response.llm_output.get(\"token_usage\")\n            if token_usage is not None:\n                log10response[\"usage\"] = token_usage\n                logger.debug(f\"usage: {log10response['usage']}\")\n\n        logger.debug(\n            f\"**\\n**on_llm_end**\\n**\\n: response:\\n {log10response} \\n\\n rest: {kwargs}\"\n        )\n        self.log_end(run[\"completion_id\"], log10response, duration)\n\n    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n        \"\"\"Do nothing.\"\"\"\n        logger.debug(f\"token:\\n {token} \\n\\n rest: {kwargs}\")\n\n    def on_llm_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> None:\n        \"\"\"Do nothing.\"\"\"\n        logger.debug(f\"error:\\n {error} \\n\\n rest: {kwargs}\")\n\n    def on_chain_start(\n        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n    ) -> None:\n        \"\"\"Print out that we are entering a chain.\"\"\"\n        pass\n\n    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n        \"\"\"Print out that we finished a chain.\"\"\"\n        pass\n\n    def on_chain_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> None:\n        \"\"\"Do nothing.\"\"\"\n        pass\n\n    def on_tool_start(\n        self,\n        serialized: Dict[str, Any],\n        input_str: str,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Do nothing.\"\"\"\n        pass\n\n    def on_agent_action(\n        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n    ) -> Any:\n        \"\"\"Run on agent action.\"\"\"\n        pass\n\n    def on_tool_end(\n        self,\n        output: str,\n        color: Optional[str] = None,\n        observation_prefix: Optional[str] = None,\n        llm_prefix: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"If not the final action, print out observation.\"\"\"\n        pass\n\n    def on_tool_error(\n        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n    ) -> None:\n        \"\"\"Do nothing.\"\"\"\n        pass\n\n    def on_text(\n        self,\n        text: str,\n        color: Optional[str] = None,\n        end: str = \"\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Run when agent ends.\"\"\"\n        pass\n\n    def on_agent_finish(\n        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n    ) -> None:\n        \"\"\"Run on agent end.\"\"\"\n        pass", ""]}
{"filename": "log10/openai.py", "chunked_list": ["from copy import deepcopy\nimport time\nfrom typing import List\nimport openai\nfrom log10.llm import LLM, ChatCompletion, Message, TextCompletion\n\nimport logging\n\n# for exponential backoff\nimport backoff", "# for exponential backoff\nimport backoff\nfrom openai.error import RateLimitError, APIConnectionError\nimport openai\n\n\nclass OpenAI(LLM):\n    def __init__(self, hparams: dict = None, log10_config=None):\n        super().__init__(hparams, log10_config)\n\n    @backoff.on_exception(backoff.expo, (RateLimitError, APIConnectionError))\n    def chat(self, messages: List[Message], hparams: dict = None) -> ChatCompletion:\n        completion = openai.ChatCompletion.create(\n            **self.chat_request(messages, hparams)\n        )\n\n        return ChatCompletion(\n            role=completion.choices[0][\"message\"][\"role\"],\n            content=completion.choices[0][\"message\"][\"content\"],\n            response=completion,\n        )\n\n    def chat_request(self, messages: List[Message], hparams: dict = None) -> dict:\n        merged_hparams = deepcopy(self.hparams)\n        if hparams:\n            for hparam in hparams:\n                merged_hparams[hparam] = hparams[hparam]\n\n        return {\n            \"messages\": [message.to_dict() for message in messages],\n            **merged_hparams,\n        }\n\n    @backoff.on_exception(backoff.expo, (RateLimitError, APIConnectionError))\n    def text(self, prompt: str, hparams: dict = None) -> TextCompletion:\n        request = self.text_request(prompt, hparams)\n        completion = openai.Completion.create(**request)\n        return TextCompletion(text=completion.choices[0].text, response=completion)\n\n    def text_request(self, prompt: str, hparams: dict = None) -> dict:\n        merged_hparams = deepcopy(self.hparams)\n        if hparams:\n            for hparam in hparams:\n                merged_hparams[hparam] = hparams[hparam]\n        output = {\"prompt\": prompt, **merged_hparams}\n        return output", ""]}
{"filename": "log10/anthropic.py", "chunked_list": ["import os\nfrom copy import deepcopy\nfrom typing import List\nfrom log10.llm import LLM, ChatCompletion, Message, TextCompletion\n\n\nfrom anthropic import HUMAN_PROMPT, AI_PROMPT\nimport anthropic\n\nimport uuid", "\nimport uuid\n\nimport logging\n\n\nclass Anthropic(LLM):\n    def __init__(\n        self, hparams: dict = None, skip_initialization: bool = False, log10_config=None\n    ):\n        super().__init__(hparams, log10_config)\n\n        if not skip_initialization:\n            self.client = anthropic.Anthropic()\n        self.hparams = hparams\n\n        if \"max_tokens_to_sample\" not in self.hparams:\n            self.hparams[\"max_tokens_to_sample\"] = 1024\n\n    def chat(self, messages: List[Message], hparams: dict = None) -> ChatCompletion:\n        chat_request = self.chat_request(messages, hparams)\n        completion = self.client.completions.create(\n            **chat_request\n        )\n        content = completion.completion\n\n        reason = \"stop\"\n        if completion.stop_reason == \"stop_sequence\":\n            reason = \"stop\"\n        elif completion.stop_reason == \"max_tokens\":\n            reason = \"length\"\n\n        tokens_usage = self.create_tokens_usage(chat_request[\"prompt\"], content)\n\n        # Imitate OpenAI reponse format.\n        response = {\n            \"id\": str(uuid.uuid4()),\n            \"object\": \"chat.completion\",\n            \"model\": completion.model,\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"message\": {\"role\": \"assistant\", \"content\": content},\n                    \"finish_reason\": reason,\n                }\n            ],\n            \"usage\": tokens_usage\n        }\n\n        return ChatCompletion(role=\"assistant\", content=content, response=response)\n\n    def chat_request(self, messages: List[Message], hparams: dict = None) -> dict:\n        merged_hparams = deepcopy(self.hparams)\n        if hparams:\n            for hparam in hparams:\n                merged_hparams[hparam] = hparams[hparam]\n\n        # NOTE: That we may have to convert this to openai messages, if we want\n        #       to use the same log viewer for all chat based models.\n        prompt = Anthropic.convert_history_to_claude(messages)\n        return {\"prompt\": prompt, \"stop_sequences\": [HUMAN_PROMPT], **merged_hparams}\n\n    def text(self, prompt: str, hparams: dict = None) -> TextCompletion:\n        text_request = self.text_request(prompt, hparams)\n        completion = self.client.completions.create(\n            **text_request\n        )\n        text = completion.completion\n\n        # Imitate OpenAI reponse format.\n        reason = \"stop\"\n        if completion.stop_reason == \"stop_sequence\":\n            reason = \"stop\"\n        elif completion.stop_reason == \"max_tokens\":\n            reason = \"length\"\n\n        tokens_usage = self.create_tokens_usage(text_request[\"prompt\"], text)\n\n        # Imitate OpenAI reponse format.\n        response = {\n            \"id\": str(uuid.uuid4()),\n            \"object\": \"text_completion\",\n            \"model\": completion.model,\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"text\": text,\n                    \"logprobs\": None,\n                    \"finish_reason\": reason,\n                }\n            ],\n            \"usage\": tokens_usage\n        }\n        logging.info(\"Returning text completion\")\n        return TextCompletion(text=text, response=response)\n\n    def text_request(self, prompt: str, hparams: dict = None) -> TextCompletion:\n        merged_hparams = deepcopy(self.hparams)\n        if hparams:\n            for hparam in hparams:\n                merged_hparams[hparam] = hparams[hparam]\n        return {\n            \"prompt\": HUMAN_PROMPT + prompt + \"\\n\" + AI_PROMPT,\n            \"stop_sequences\": [HUMAN_PROMPT],\n            **merged_hparams,\n        }\n\n    def convert_history_to_claude(messages: List[Message]):\n        text = \"\"\n        for message in messages:\n            # Anthropic doesn't support a system prompt OOB\n            if message.role == \"user\" or message.role == \"system\":\n                text += HUMAN_PROMPT\n            elif message.role == \"assistant\":\n                text += AI_PROMPT\n            text += f\"{message.content}\"\n        text += AI_PROMPT\n        return text\n\n    def convert_claude_to_messages(prompt: str):\n        pass\n\n    def create_tokens_usage(self, prompt: str, completion: str):\n        prompt_tokens = self.client.count_tokens(prompt)\n        completion_tokens = self.client.count_tokens(completion)\n        total_tokens = prompt_tokens + completion_tokens\n\n        # Imitate OpenAI usage format.\n        return {\n            \"prompt_tokens\": prompt_tokens,\n            \"completion_tokens\": completion_tokens,\n            \"total_tokens\": total_tokens\n        }", ""]}
{"filename": "log10/llm.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom copy import deepcopy\nfrom enum import Enum\nimport os\nimport sys\nimport traceback\n\nRole = Enum(\"Role\", [\"system\", \"assistant\", \"user\"])\nKind = Enum(\"Kind\", [\"chat\", \"text\"])\n", "Kind = Enum(\"Kind\", [\"chat\", \"text\"])\n\nfrom typing import List, Optional\n\nimport json\n\nimport logging\nimport requests\n\n\nclass Log10Config:\n    def __init__(\n        self,\n        url: str = None,\n        token: str = None,\n        org_id: str = None,\n        tags: List[str] = None,\n        DEBUG: bool = False,\n    ):\n        self.url = url if url else os.getenv(\"LOG10_URL\")\n        self.token = token if token else os.getenv(\"LOG10_TOKEN\")\n        self.org_id = org_id if org_id else os.getenv(\"LOG10_ORG_ID\")\n        self.DEBUG = DEBUG\n\n        # Get tags from env, if not set, use empty list\n        if tags:\n            self.tags = tags\n        elif os.getenv(\"LOG10_TAGS\") is not None:\n            self.tags = os.getenv(\"LOG10_TAGS\").split(\",\")\n        else:\n            self.tags = []", "\n\nclass Log10Config:\n    def __init__(\n        self,\n        url: str = None,\n        token: str = None,\n        org_id: str = None,\n        tags: List[str] = None,\n        DEBUG: bool = False,\n    ):\n        self.url = url if url else os.getenv(\"LOG10_URL\")\n        self.token = token if token else os.getenv(\"LOG10_TOKEN\")\n        self.org_id = org_id if org_id else os.getenv(\"LOG10_ORG_ID\")\n        self.DEBUG = DEBUG\n\n        # Get tags from env, if not set, use empty list\n        if tags:\n            self.tags = tags\n        elif os.getenv(\"LOG10_TAGS\") is not None:\n            self.tags = os.getenv(\"LOG10_TAGS\").split(\",\")\n        else:\n            self.tags = []", "\n\nclass Message(ABC):\n    def __init__(\n        self, role: Role, content: str, id: str = None, completion: str = None\n    ):\n        self.id = id\n        self.role = role\n        self.content = content\n        self.completion = completion\n\n    def to_dict(self):\n        return {\n            \"role\": self.role,\n            \"content\": self.content,\n        }\n\n    def from_dict(message: dict):\n        return Message(\n            role=message[\"role\"],\n            content=message[\"content\"],\n            id=message.get(\"id\"),\n            completion=message.get(\"completion\"),\n        )", "\n\nclass Messages(ABC):\n    def from_dict(messages: dict):\n        return [Message.from_dict(message) for message in messages]\n\n\nclass Completion(ABC):\n    pass\n", "\n\nclass ChatCompletion(Completion):\n    def __init__(\n        self, role: str, content: str, response: dict = None, completion_id: str = None\n    ):\n        self.role = role\n        self.content = content\n        self.response = response\n        self.completion_id = completion_id\n\n    def to_dict(self) -> dict:\n        return {\n            \"role\": self.role,\n            \"content\": self.content,\n        }\n\n    def __str__(self) -> str:\n        return json.dumps(self.to_dict())", "\n\nclass TextCompletion(Completion):\n    def __init__(self, text: str, response: dict = None, completion_id=None):\n        self._text = text\n        self.response = response\n        self.completion_id = completion_id\n\n    def text(self) -> str:\n        return self._text", "\n\nclass LLM(ABC):\n    last_completion_response = None\n\n    def __init__(self, hparams: dict = None, log10_config: Log10Config = None):\n        self.log10_config = log10_config\n        self.hparams = hparams\n\n        # Start session\n        if self.log10_config:\n            session_url = self.log10_config.url + \"/api/sessions\"\n            try:\n                res = requests.request(\n                    \"POST\",\n                    session_url,\n                    headers={\n                        \"x-log10-token\": self.log10_config.token,\n                        \"Content-Type\": \"application/json\",\n                    },\n                    json={\"organization_id\": self.log10_config.org_id},\n                )\n                response = res.json()\n                self.session_id = response[\"sessionID\"]\n            except Exception as e:\n                logging.warning(\n                    f\"Failed to start session with {session_url} using token {self.log10_config.token}. Won't be able to log. {e}\"\n                )\n                self.log10_config = None\n\n    def last_completion_url(self):\n        if self.last_completion_response is None:\n            return None\n\n        return self.log10_config.url + '/app/' + self.last_completion_response['organizationSlug'] + '/completions/' + self.last_completion_response['completionID']\n\n    def text(self, prompt: str, hparams: dict = None) -> TextCompletion:\n        raise Exception(\"Not implemented\")\n\n    def text_request(self, prompt: str, hparams: dict = None) -> dict:\n        raise Exception(\"Not implemented\")\n\n    def chat(self, messages: List[Message], hparams: dict = None) -> ChatCompletion:\n        raise Exception(\"Not implemented\")\n\n    def chat_request(self, messages: List[Message], hparams: dict = None) -> dict:\n        raise Exception(\"Not implemented\")\n\n    def api_request(self, rel_url: str, method: str, request: dict):\n        return requests.request(\n            method,\n            f\"{self.log10_config.url}{rel_url}\",\n            headers={\n                \"x-log10-token\": self.log10_config.token,\n                \"Content-Type\": \"application/json\",\n            },\n            json=request,\n        )\n\n    # Save the start of a completion in **openai request format**.\n    def log_start(self, request, kind: Kind, tags: Optional[List[str]] = None):\n        if not self.log10_config:\n            return None\n\n        res = self.api_request(\n            \"/api/completions\", \"POST\", {\"organization_id\": self.log10_config.org_id}\n        )\n        self.last_completion_response = res.json()\n        completion_id = res.json()[\"completionID\"]\n\n        # merge tags\n        if tags:\n            tags = list(set(tags + self.log10_config.tags))\n        else:\n            tags = self.log10_config.tags\n\n        res = self.api_request(\n            f\"/api/completions/{completion_id}\",\n            \"POST\",\n            {\n                \"kind\": kind == Kind.text and \"completion\" or \"chat\",\n                \"organization_id\": self.log10_config.org_id,\n                \"session_id\": self.session_id,\n                \"orig_module\": \"openai.api_resources.completion\"\n                if kind == Kind.text\n                else \"openai.api_resources.chat_completion\",\n                \"orig_qualname\": \"Completion.create\"\n                if kind == Kind.text\n                else \"ChatCompletion.create\",\n                \"status\": \"started\",\n                \"tags\": tags,\n                \"request\": json.dumps(request),\n            },\n        )\n\n        return completion_id\n\n    # Save the end of a completion in **openai request format**.\n    def log_end(self, completion_id: str, response: dict, duration: int):\n        if not self.log10_config:\n            return None\n\n        current_stack_frame = traceback.extract_stack()\n        stacktrace = [\n            {\n                \"file\": frame.filename,\n                \"line\": frame.line,\n                \"lineno\": frame.lineno,\n                \"name\": frame.name,\n            }\n            for frame in current_stack_frame\n        ]\n\n        self.api_request(\n            f\"/api/completions/{completion_id}\",\n            \"POST\",\n            {\n                \"response\": json.dumps(response),\n                \"status\": \"finished\",\n                \"duration\": int(duration * 1000),\n                \"stacktrace\": json.dumps(stacktrace),\n            },\n        )", "\n\nclass NoopLLM(LLM):\n    def __init__(self, hparams: dict = None, log10_config=None):\n        pass\n\n    def chat(self, messages: List[Message], hparams: dict = None) -> ChatCompletion:\n        logging.info(\"Received chat completion requst: \" + str(messages))\n        return ChatCompletion(role=\"assistant\", content=\"I'm not a real LLM\")\n\n    def text(self, prompt: str, hparams: dict = None) -> TextCompletion:\n        logging.info(\"Received text completion requst: \" + prompt)\n        return TextCompletion(text=\"I'm not a real LLM\")", "\n\ndef merge_hparams(override, base):\n    merged = deepcopy(base)\n    if override:\n        for hparam in override:\n            merged[hparam] = override[hparam]\n\n    return merged\n", ""]}
{"filename": "log10/__init__.py", "chunked_list": [""]}
{"filename": "log10/utils.py", "chunked_list": ["import re\nimport string\nfrom anthropic import HUMAN_PROMPT, AI_PROMPT\nimport json\n\n# Ref: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/utils.py\ndef normalize(s: str) -> str:\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    s = s.lower()\n    exclude = set(string.punctuation)\n    s = \"\".join(char for char in s if char not in exclude)\n    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n    s = \" \".join(s.split())\n    return s", "\n\ndef fuzzy_match(s1: str, s2: str) -> bool:\n    s1 = normalize(s1)\n    s2 = normalize(s2)\n\n    if s1 == \"\" or s2 == \"\":\n        return s1 == s2\n\n    return s1 in s2 or s2 in s1", "\n\ndef parse_field(value):\n    try:\n        # Try to parse the value as JSON (list)\n        parsed = json.loads(value)\n        if isinstance(parsed, list):\n            return parsed\n        else:\n            return [value]\n    except json.JSONDecodeError:\n        # If it's not valid JSON, return the original string value as a list with singleton element\n        return [value]", ""]}
{"filename": "log10/tools.py", "chunked_list": ["from langchain.document_loaders import WebBaseLoader\n\nfrom log10.llm import Message\n\n\n# Browser agent / tool\ndef browser(URL: str) -> str:\n    \"\"\"useful when you need to scrape a website.\n    Input is the URL of the website to be scraped.\n    Output is the text of the website\n    \"\"\"\n    # Example code if we want to follow links\n    # explored_sites = set()\n    # queue = [URL]\n\n    # def parse_site(base_url):\n    #   if base_url in explored_sites:\n    #     return\n\n    #   explored_sites.add(base_url)\n\n    #   if debug:\n    #     print(\"Scraping: \", base_url)\n\n    #   page = requests.get(URL)\n    #   soup = BeautifulSoup(page.content, \"html.parser\")\n    #   for a_href in soup.find_all(\"a\", href=True):\n    #       link = a_href[\"href\"]\n    #       if link[0] == \"/\":\n    #         link = URL + link\n\n    #       if link.startswith(URL):\n    #         to_explore = link.split(\"#\")[0]\n    #         queue.append(to_explore)\n\n    # while len(queue) != 0:\n    #   url = queue.pop()\n    #   parse_site(url)\n\n    # return list(explored_sites)\n\n    loader = WebBaseLoader(URL)\n    data = loader.load()\n    data[0].page_content = \" \".join(data[0].page_content.split())\n\n    # hack: crop to 5000 char to fit within context length\n    # Will need better strategy eventually\n    #   maxlen = 5000 if len(data[0].page_content) > 5000 else len(data[0].page_content)\n    #   return data[0].page_content[:maxlen]\n    return data[0].page_content", "\n\ndef code_extractor(full_response, language, extraction_model, llm):\n    \"\"\"useful when you need to extract just the code from a detailed LLM response\"\"\"\n    messages = [\n        Message(\n            role=\"system\",\n            content=f\"Extract just the {language} code from the following snippet. Do not include ``` or markdown syntax. Format your reply so I can directly copy your entire response, save it as a file and compile and run the code.\",\n        ),\n        Message(role=\"user\", content=\"Here's the snippet:\\n\" + full_response),\n    ]\n    completion = llm.chat(messages, {\"model\": extraction_model, \"temperature\": 0.2})\n    return completion.content", ""]}
{"filename": "log10/load.py", "chunked_list": ["import types\nimport functools\nimport inspect\nimport requests\nimport os\nimport json\nimport time\nimport traceback\nfrom aiohttp import ClientSession\nimport asyncio", "from aiohttp import ClientSession\nimport asyncio\nimport threading\nimport queue\nfrom contextlib import contextmanager\nimport logging\nfrom dotenv import load_dotenv\nimport backoff  # for exponential backoff\nfrom openai.error import RateLimitError, APIConnectionError\n", "from openai.error import RateLimitError, APIConnectionError\n\nload_dotenv()\n\nurl = os.environ.get(\"LOG10_URL\")\ntoken = os.environ.get(\"LOG10_TOKEN\")\norg_id = os.environ.get(\"LOG10_ORG_ID\")\n\n# log10, bigquery\ntarget_service = os.environ.get(\"LOG10_DATA_STORE\", \"log10\")", "# log10, bigquery\ntarget_service = os.environ.get(\"LOG10_DATA_STORE\", \"log10\")\n\nif target_service == \"bigquery\":\n    from log10.bigquery import initialize_bigquery\n    bigquery_client, bigquery_table = initialize_bigquery()\n    import uuid\n    from datetime import datetime, timezone\nelif target_service is None:\n    target_service = \"log10\"  # default to log10", "\n\n@backoff.on_exception(backoff.expo, (RateLimitError, APIConnectionError))\ndef func_with_backoff(func, *args, **kwargs):\n    return func(*args, **kwargs)\n\n\ndef get_session_id():\n    if target_service == \"bigquery\":\n        return str(uuid.uuid4())\n\n    try:\n        session_url = url + \"/api/sessions\"\n        res = requests.request(\"POST\",\n                               session_url, headers={\"x-log10-token\": token, \"Content-Type\": \"application/json\"}, json={\n                                   \"organization_id\": org_id\n                               })\n\n        return res.json()['sessionID']\n    except Exception as e:\n        raise Exception(\"Failed to create LOG10 session: \" + str(e) + \"\\nLikely cause: LOG10 env vars missing or not picked up correctly!\" +\n                        \"\\nSee https://github.com/log10-io/log10#%EF%B8%8F-setup for details\")", "\n\n# Global variable to store the current sessionID.\nsessionID = get_session_id()\nlast_completion_response = None\nglobal_tags = []\n\nclass log10_session:\n    def __init__(self, tags=None):\n         self.tags = tags\n\n         if tags is not None:\n            global global_tags\n            global_tags = tags\n\n    def __enter__(self):\n        global sessionID\n        global last_completion_response\n        sessionID = get_session_id()\n        last_completion_response = None\n        return self\n\n    def last_completion_url(self):\n        if last_completion_response is None:\n            return None\n\n        return url + '/app/' + last_completion_response['organizationSlug'] + '/completions/' + last_completion_response['completionID']\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.tags is not None:\n            global global_tags\n            global_tags = None\n        return", "\n\n@contextmanager\ndef timed_block(block_name):\n    if DEBUG:\n        start_time = time.perf_counter()\n        try:\n            yield\n        finally:\n            elapsed_time = time.perf_counter() - start_time\n            logging.debug(\n                f\"TIMED BLOCK - {block_name} took {elapsed_time:.6f} seconds to execute.\")\n    else:\n        yield", "\n\ndef log_url(res, completionID):\n    output = res.json()\n    organizationSlug = output['organizationSlug']\n    full_url = url + '/app/' + organizationSlug + '/completions/' + completionID\n    logging.debug(f\"LOG10: Completion URL: {full_url}\")\n\n\nasync def log_async(completion_url, func, **kwargs):", "\nasync def log_async(completion_url, func, **kwargs):\n    async with ClientSession() as session:\n        global last_completion_response\n\n        res = requests.request(\"POST\",\n                               completion_url, headers={\"x-log10-token\": token, \"Content-Type\": \"application/json\"}, json={\n                                   \"organization_id\": org_id\n                               })\n        # todo: handle session id for bigquery scenario", "                               })\n        # todo: handle session id for bigquery scenario\n        last_completion_response = res.json()\n        completionID = res.json()['completionID']\n\n        if DEBUG:\n            log_url(res, completionID)\n        log_row = {\n            # do we want to also store args?\n            \"status\": \"started\",", "            # do we want to also store args?\n            \"status\": \"started\",\n            \"orig_module\": func.__module__,\n            \"orig_qualname\": func.__qualname__,\n            \"request\": json.dumps(kwargs),\n            \"session_id\": sessionID,\n            \"organization_id\": org_id,\n            \"tags\": global_tags\n        }\n        if target_service == \"log10\":\n            res = requests.request(\"POST\",\n                                   completion_url + \"/\" + completionID,\n                                   headers={\"x-log10-token\": token,\n                                            \"Content-Type\": \"application/json\"},\n                                   json=log_row)\n        elif target_service == \"bigquery\":\n            pass", "        }\n        if target_service == \"log10\":\n            res = requests.request(\"POST\",\n                                   completion_url + \"/\" + completionID,\n                                   headers={\"x-log10-token\": token,\n                                            \"Content-Type\": \"application/json\"},\n                                   json=log_row)\n        elif target_service == \"bigquery\":\n            pass\n            # NOTE: We only save on request finalization.", "            # NOTE: We only save on request finalization.\n\n        return completionID\n\n\ndef run_async_in_thread(completion_url, func, result_queue, **kwargs):\n    result = asyncio.run(\n        log_async(completion_url=completion_url, func=func, **kwargs))\n    result_queue.put(result)\n", "\n\ndef log_sync(completion_url, func, **kwargs):\n    global last_completion_response\n    res = requests.request(\"POST\",\n                           completion_url, headers={\"x-log10-token\": token, \"Content-Type\": \"application/json\"}, json={\n                               \"organization_id\": org_id\n                           })\n\n    last_completion_response = res.json()\n    completionID = res.json()['completionID']\n    if DEBUG:\n        log_url(res, completionID)\n    res = requests.request(\"POST\",\n                           completion_url + \"/\" + completionID,\n                           headers={\"x-log10-token\": token,\n                                    \"Content-Type\": \"application/json\"},\n                           json={\n                               # do we want to also store args?\n                               \"status\": \"started\",\n                               \"orig_module\": func.__module__,\n                               \"orig_qualname\": func.__qualname__,\n                               \"request\": json.dumps(kwargs),\n                               \"session_id\": sessionID,\n                               \"organization_id\": org_id,\n                               \"tags\": global_tags\n                           })\n    return completionID", "\n\ndef intercepting_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        completion_url = url + \"/api/completions\"\n        output = None\n        result_queue = queue.Queue()\n\n        try:\n            with timed_block(sync_log_text + \" call duration\"):\n                if USE_ASYNC:\n                    threading.Thread(target=run_async_in_thread, kwargs={\n                        \"completion_url\": completion_url, \"func\": func, \"result_queue\": result_queue, **kwargs}).start()\n                else:\n                    completionID = log_sync(\n                        completion_url=completion_url, func=func, **kwargs)\n\n            current_stack_frame = traceback.extract_stack()\n            stacktrace = ([{\"file\": frame.filename,\n                          \"line\": frame.line,\n                           \"lineno\": frame.lineno,\n                            \"name\": frame.name} for frame in current_stack_frame])\n\n            start_time = time.perf_counter()\n            output = func_with_backoff(func, *args, **kwargs)\n            duration = time.perf_counter() - start_time\n            logging.debug(\n                f\"LOG10: TIMED BLOCK - LLM call duration: {duration}\")\n\n            if USE_ASYNC:\n                with timed_block(\"extra time spent waiting for log10 call\"):\n                    while result_queue.empty():\n                        pass\n                    completionID = result_queue.get()\n\n            with timed_block(\"result call duration (sync)\"):\n                # Adjust the Anthropic output to match OAI completion output\n                if func.__qualname__ == \"Client.completion\":\n                    output['choices'] = [{\n                        'text': output['completion'],\n                        'index': 0,\n                    }]\n                    log_row = {\n                        \"response\": json.dumps(output),\n                        \"status\": \"finished\",\n                        \"duration\": int(duration*1000),\n                        \"stacktrace\": json.dumps(stacktrace),\n                        \"kind\": \"completion\"\n                    }\n                else:\n                    log_row = {\n                        \"response\": json.dumps(output),\n                        \"status\": \"finished\",\n                        \"duration\": int(duration*1000),\n                        \"stacktrace\": json.dumps(stacktrace)\n                    }\n\n                if target_service == \"log10\":\n                    res = requests.request(\"POST\",\n                                           completion_url + \"/\" + completionID,\n                                           headers={\n                                               \"x-log10-token\": token, \"Content-Type\": \"application/json\"},\n                                           json=log_row)\n                elif target_service == \"bigquery\":\n                    try:\n                        log_row[\"id\"] = str(uuid.uuid4())\n                        log_row[\"created_at\"] = datetime.now(\n                            timezone.utc).isoformat()\n                        log_row[\"request\"] = json.dumps(kwargs)\n\n                        if func.__qualname__ == \"Completion.create\":\n                            log_row[\"kind\"] = \"completion\"\n                        elif func.__qualname__ == \"ChatCompletion.create\":\n                            log_row[\"kind\"] = \"chat\"\n\n                        log_row[\"orig_module\"] = func.__module__\n                        log_row[\"orig_qualname\"] = func.__qualname__\n                        log_row[\"session_id\"] = sessionID\n\n                        bigquery_client.insert_rows_json(\n                            bigquery_table, [log_row])\n\n                    except Exception as e:\n                        logging.error(\n                            f\"LOG10: failed to insert in Bigquery: {log_row} with error {e}\")\n        except Exception as e:\n            logging.error(\"LOG10: failed\", e)\n\n        return output\n\n    return wrapper", "\n\ndef set_sync_log_text(USE_ASYNC=True):\n    return \"async\" if USE_ASYNC else \"sync\"\n\n\ndef log10(module, DEBUG_=False, USE_ASYNC_=True):\n    \"\"\"Intercept and overload module for logging purposes\n\n    Keyword arguments:\n    module -- the module to be intercepted (e.g. openai)\n    DEBUG_ -- whether to show log10 related debug statements via python logging (default False)\n    USE_ASYNC_ -- whether to run in async mode (default True)\n    \"\"\"\n    global DEBUG, USE_ASYNC, sync_log_text\n    DEBUG = DEBUG_\n    USE_ASYNC = USE_ASYNC_\n    sync_log_text = set_sync_log_text(USE_ASYNC=USE_ASYNC)\n    logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO,\n                        format='%(asctime)s - %(levelname)s - LOG10 - %(message)s')\n\n    # def intercept_nested_functions(obj):\n    #     for name, attr in vars(obj).items():\n    #         if callable(attr) and isinstance(attr, types.FunctionType):\n    #             setattr(obj, name, intercepting_decorator(attr))\n    #         elif inspect.isclass(attr):\n    #             intercept_class_methods(attr)\n\n    # def intercept_class_methods(cls):\n    #     for method_name, method in vars(cls).items():\n    #         if isinstance(method, classmethod):\n    #             original_method = method.__func__\n    #             decorated_method = intercepting_decorator(original_method)\n    #             setattr(cls, method_name, classmethod(decorated_method))\n    #         elif isinstance(method, (types.FunctionType, types.MethodType)):\n    #             print(f\"method:{method}\")\n    #             setattr(cls, method_name, intercepting_decorator(method))\n    #         elif inspect.isclass(method):  # Handle nested classes\n    #             intercept_class_methods(method)\n\n    for name, attr in vars(module).items():\n        if inspect.isclass(attr):\n            # OpenAI\n            if module.__name__ == \"openai\" and name in [\"ChatCompletion\", \"Completion\"]:\n                for method_name, method in vars(attr).items():\n                    if isinstance(method, classmethod):\n                        original_method = method.__func__\n                        if original_method.__qualname__ in [\"ChatCompletion.create\", \"Completion.create\"]:\n                            decorated_method = intercepting_decorator(\n                                original_method)\n                            setattr(attr, method_name,\n                                    classmethod(decorated_method))\n            # Anthropic\n            elif module.__name__ == \"anthropic\" and name == \"Client\":\n                for method_name, method in vars(attr).items():\n                    if isinstance(method, (types.FunctionType, types.MethodType)) and method_name == \"completion\":\n                        setattr(attr, method_name,\n                                intercepting_decorator(method))", "\n            # For future reference:\n            # if callable(attr) and isinstance(attr, types.FunctionType):\n            #     print(f\"attr:{attr}\")\n            #     setattr(module, name, intercepting_decorator(attr))\n            # elif inspect.isclass(attr):  # Check if attribute is a class\n            #     intercept_class_methods(attr)\n            # # else: # uncomment if we want to include nested function support\n            # #     intercept_nested_functions(attr)\n", "            # #     intercept_nested_functions(attr)\n"]}
{"filename": "log10/bigquery.py", "chunked_list": ["from google.cloud import bigquery\nfrom google.api_core.exceptions import NotFound\nimport os\n\n\n# todo: add requirements.txt file\n# todo: add instructions for bigquery integration\n\ndef initialize_bigquery(debug=False):\n\n    # Configure the BigQuery client\n    project_id = os.environ.get(\"LOG10_BQ_PROJECT_ID\")\n    dataset_id = os.environ.get(\"LOG10_BQ_DATASET_ID\")\n    completions_table_id = os.environ.get(\"LOG10_BQ_COMPLETIONS_TABLE_ID\")\n\n    client = bigquery.Client(project=project_id)\n\n    def dataset_exists(dataset_id):\n        try:\n            client.get_dataset(dataset_id)  # API request\n            return True\n        except NotFound:\n            return False\n\n    def table_exists(dataset_id, completions_table_id):\n        try:\n            table_ref = client.dataset(dataset_id).table(completions_table_id)\n            client.get_table(table_ref)  # API request\n            return True\n        except NotFound:\n            return False\n\n    # Check if dataset exists\n    if dataset_exists(dataset_id):\n        if debug:\n            print(f\"Dataset {dataset_id} exists.\")\n    else:\n        if debug:\n            print(f\"Dataset {dataset_id} does not exist. Creating...\")\n        # Create the dataset\n        dataset_ref = client.dataset(dataset_id)\n        dataset = bigquery.Dataset(dataset_ref)\n        # Set the location, e.g., \"US\", \"EU\", \"asia-northeast1\", etc.\n        dataset.location = \"US\"\n        created_dataset = client.create_dataset(dataset)  # API request\n\n        if debug:\n            print(f\"Dataset {created_dataset.dataset_id} created.\")\n\n    client_dataset = client.dataset(dataset_id)\n\n    # Check if table exists\n    if table_exists(dataset_id, completions_table_id):\n        if debug:\n            print(f\"Table {completions_table_id} exists in dataset {dataset_id}.\")\n    else:\n        if debug:\n            print(\n                f\"Table {completions_table_id} does not exist in dataset {dataset_id}. Creating...\")\n        # Create the table\n        table_ref = client_dataset.table(completions_table_id)\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        schema_path = os.path.join(script_dir, 'schemas', 'bigquery.json')\n        schema = client.schema_from_json(schema_path)\n        table = bigquery.Table(table_ref, schema=schema)\n        created_table = client.create_table(table)  # API request\n\n        if debug:\n            print(f\"Table {created_table.table_id} created.\")\n\n    # load dataset and table\n    table_ref = client_dataset.table(completions_table_id)\n    table = client.get_table(table_ref)\n\n    return client, table", "def initialize_bigquery(debug=False):\n\n    # Configure the BigQuery client\n    project_id = os.environ.get(\"LOG10_BQ_PROJECT_ID\")\n    dataset_id = os.environ.get(\"LOG10_BQ_DATASET_ID\")\n    completions_table_id = os.environ.get(\"LOG10_BQ_COMPLETIONS_TABLE_ID\")\n\n    client = bigquery.Client(project=project_id)\n\n    def dataset_exists(dataset_id):\n        try:\n            client.get_dataset(dataset_id)  # API request\n            return True\n        except NotFound:\n            return False\n\n    def table_exists(dataset_id, completions_table_id):\n        try:\n            table_ref = client.dataset(dataset_id).table(completions_table_id)\n            client.get_table(table_ref)  # API request\n            return True\n        except NotFound:\n            return False\n\n    # Check if dataset exists\n    if dataset_exists(dataset_id):\n        if debug:\n            print(f\"Dataset {dataset_id} exists.\")\n    else:\n        if debug:\n            print(f\"Dataset {dataset_id} does not exist. Creating...\")\n        # Create the dataset\n        dataset_ref = client.dataset(dataset_id)\n        dataset = bigquery.Dataset(dataset_ref)\n        # Set the location, e.g., \"US\", \"EU\", \"asia-northeast1\", etc.\n        dataset.location = \"US\"\n        created_dataset = client.create_dataset(dataset)  # API request\n\n        if debug:\n            print(f\"Dataset {created_dataset.dataset_id} created.\")\n\n    client_dataset = client.dataset(dataset_id)\n\n    # Check if table exists\n    if table_exists(dataset_id, completions_table_id):\n        if debug:\n            print(f\"Table {completions_table_id} exists in dataset {dataset_id}.\")\n    else:\n        if debug:\n            print(\n                f\"Table {completions_table_id} does not exist in dataset {dataset_id}. Creating...\")\n        # Create the table\n        table_ref = client_dataset.table(completions_table_id)\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        schema_path = os.path.join(script_dir, 'schemas', 'bigquery.json')\n        schema = client.schema_from_json(schema_path)\n        table = bigquery.Table(table_ref, schema=schema)\n        created_table = client.create_table(table)  # API request\n\n        if debug:\n            print(f\"Table {created_table.table_id} created.\")\n\n    # load dataset and table\n    table_ref = client_dataset.table(completions_table_id)\n    table = client.get_table(table_ref)\n\n    return client, table", ""]}
{"filename": "log10/agents/camel.py", "chunked_list": ["import logging\nfrom dotenv import load_dotenv\n\nfrom log10.llm import LLM, Message\n\nfrom typing import List, Tuple\n\nload_dotenv()\n\nlogging.basicConfig(", "\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format=\"%(asctime)s - %(levelname)s - LOG10 - CAMEL - %(message)s\",\n)\n\n# Repeat word termination conditions\n# https://github.com/lightaime/camel/blob/master/examples/ai_society/role_playing_multiprocess.py#L63\nrepeat_word_threshold = 4\nrepeat_word_list = [\"goodbye\", \"good bye\", \"thank\", \"bye\", \"welcome\", \"language model\"]", "repeat_word_threshold = 4\nrepeat_word_list = [\"goodbye\", \"good bye\", \"thank\", \"bye\", \"welcome\", \"language model\"]\n\n\ndef camel_agent(\n    user_role: str,\n    assistant_role: str,\n    task_prompt: str,\n    max_turns: int,\n    user_prompt: str = None,\n    assistant_prompt: str = None,\n    summary_model: str = None,\n    llm: LLM = None,\n) -> Tuple[List[Message], List[Message]]:\n    generator = camel_agent_generator(\n        user_role,\n        assistant_role,\n        task_prompt,\n        max_turns,\n        user_prompt,\n        assistant_prompt,\n        summary_model,\n        llm,\n    )\n    *_, last = generator\n    return last", "\n\ndef camel_agent_generator(\n    user_role: str,\n    assistant_role: str,\n    task_prompt: str,\n    max_turns: int,\n    user_prompt: str,\n    assistant_prompt: str,\n    summary_model: str,\n    llm: LLM,\n):\n    try:\n        assistant_inception_prompt = f\"\"\"Never forget you are a {assistant_role} and I am a {user_role}. Never flip roles! Never instruct me!\nWe share a common interest in collaborating to successfully complete a task.\nYou must help me to complete the task.\nHere is the task: {task_prompt}. Never forget our task!\nI must instruct you based on your expertise and my needs to complete the task.\n\nI must give you one instruction at a time.\nYou must write a specific solution that appropriately completes the requested instruction.\nYou must decline my instruction honestly if you cannot perform the instruction due to physical, moral, legal reasons or your capability and explain the reasons.\nDo not add anything else other than your solution to my instruction.\nYou are never supposed to ask me any questions you only answer questions.\nYou are never supposed to reply with a flake solution. Explain your solutions.\nYour solution must be declarative sentences and simple present tense.\nUnless I say the task is completed, you should always start with:\n\nSolution: <YOUR_SOLUTION>\n\n<YOUR_SOLUTION> should be specific and provide preferable implementations and examples for task-solving.\nAlways end <YOUR_SOLUTION> with: Next request.\"\"\"\n\n        if assistant_prompt is not None:\n            assistant_inception_prompt = assistant_prompt\n\n        user_inception_prompt = f\"\"\"Never forget you are a {user_role} and I am a {assistant_role}. Never flip roles! You will always instruct me.\nWe share a common interest in collaborating to successfully complete a task.\nI must help you to complete the task.\nHere is the task: {task_prompt}. Never forget our task!\nYou must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n\n1. Instruct with a necessary input:\nInstruction: <YOUR_INSTRUCTION>\nInput: <YOUR_INPUT>\n\n2. Instruct without any input:\nInstruction: <YOUR_INSTRUCTION>\nInput: None\n\nThe \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n\nYou must give me one instruction at a time.\nI must write a response that appropriately completes the requested instruction.\nI must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\nYou should instruct me not ask me questions.\nNow you must start to instruct me using the two ways described above.\nDo not add anything else other than your instruction and the optional corresponding input!\nKeep giving me instructions and necessary inputs until you think the task is completed.\nWhen the task is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\nNever say <CAMEL_TASK_DONE> unless my responses have solved your task.\"\"\"\n\n        if user_prompt is not None:\n            user_inception_prompt = user_prompt\n\n        assistant_prompt = f\"ASSISTANT PROMPT: \\n {assistant_inception_prompt}\"\n        user_prompt = f\"USER PROMPT: \\n {user_inception_prompt}\"\n\n        assistant_messages = [\n            Message(role=\"system\", content=assistant_prompt),\n            Message(role=\"user\", content=assistant_prompt),\n        ]\n        user_messages = [\n            Message(role=\"system\", content=user_prompt),\n            Message(\n                role=\"user\",\n                content=user_prompt\n                + \" Now start to give me instructions one by one. Only reply with Instruction and Input.\",\n            ),\n        ]\n        repeat_word_counter = 0\n        repeat_word_threshold_exceeded = False\n\n        for i in range(max_turns):\n            repeated_word_current_turn = False\n            #\n            # User turn\n            #\n            user_message = llm.chat(user_messages)\n            user_messages.append(user_message)\n            assistant_messages.append(\n                Message(role=\"user\", content=user_message.content)\n            )\n            logging.info(f\"User turn {i}: {user_message}\")\n\n            #\n            # Assistant turn\n            #\n            assistant_message = llm.chat(assistant_messages)\n            assistant_messages.append(assistant_message)\n            user_messages.append(\n                Message(role=\"user\", content=assistant_message.content)\n            )\n            logging.info(f\"Assistant turn {i}: {assistant_message}\")\n\n            yield (user_messages, assistant_messages)\n\n            #\n            # Termination conditions.\n            #\n            for repeat_word in repeat_word_list:\n                if (\n                    repeat_word in assistant_message.content.lower()\n                    or repeat_word in user_message.content.lower()\n                ):\n                    repeat_word_counter += 1\n                    repeated_word_current_turn = True\n                    logging.info(f\"Repeat word counter = {repeat_word_counter}\")\n                    if repeat_word_counter == repeat_word_threshold:\n                        repeat_word_threshold_exceeded = True\n                    break\n\n            if not repeated_word_current_turn:\n                repeat_word_counter = 0\n\n            if (\n                (\"CAMEL_TASK_DONE\" in user_messages[-2].content)\n                or (i == max_turns - 1)\n                or repeat_word_threshold_exceeded\n            ):\n                #\n                # Summary turn\n                #\n                summary_context = \"\\n\".join(\n                    [\n                        f\"{turn.role.capitalize()} ({user_role if turn.role == 'user' else assistant_role}): {turn.content}\"\n                        for turn in assistant_messages\n                        if turn.role in [\"assistant\", \"user\"]\n                    ]\n                )\n\n                logging.info(f\"summary context: {summary_context}\")\n                summary_system_prompt = \"\"\"You are an experienced solution extracting agent.\nYour task is to extract full and complete solutions by looking at the conversation between a user and an assistant with particular specializations.\nYou should present me with a final and detailed solution purely based on the conversation.\nYou should present the solution as if its yours.\nUse present tense and as if you are the one presenting the solution.\nYou should not miss any necessary details or examples.\nKeep all provided explanations and codes provided throughout the conversation.\nRemember your task is not to summarize rather to extract the full solution.\"\"\"\n\n                summary_closing_prompt = f\"\"\"\\n\\nAs a reminder, the above context is to help you provide a complete and concise solution to the following task: {task_prompt}\nDo not attempt to describe the solution, but try to answer in a way such that your answer will directly solve the task - not just describe the steps required to solve the task.\nOnly use the provided context above and no other sources.\nEven if I told you that the task is completed in the context above you should still reply with a complete solution. Never tell me the task is completed or ask for the next request, but instead replay the final solution back to me.\"\"\"\n                summary_prompt = (\n                    f\"Task:{task_prompt}\\n\" + summary_context + summary_closing_prompt\n                )\n                summary_messages = [\n                    Message(role=\"system\", content=summary_system_prompt),\n                    Message(\n                        role=\"user\",\n                        content=\"Here is the conversation: \" + summary_prompt,\n                    ),\n                ]\n\n                hparams = {\"model\": summary_model}\n                message = llm.chat(summary_messages, hparams)\n                assistant_messages.append(message)\n                user_messages.append(Message(role=\"user\", content=message.content))\n                logging.info(message.content)\n\n                # End of conversation\n                yield (user_messages, assistant_messages)\n                break\n\n    except Exception as e:\n        logging.error(\"Error in CAMEL agent: \", e)", ""]}
{"filename": "log10/agents/scrape_summarizer.py", "chunked_list": ["from log10.llm import LLM, Message\nfrom anthropic import HUMAN_PROMPT\nfrom log10.tools import browser\n\n\n# Set up Summarizer agent\nsystem_prompt = \"You are an expert at extracting the main points from a website. Only look at the provided website content by the user to extract the main points.\"\nsummarize_prompt = (\n    \"Extract the main points from the following website:\\n {website_text}\"\n)", "    \"Extract the main points from the following website:\\n {website_text}\"\n)\n\n\ndef scrape_summarizer(url, llm: LLM):\n    website_text = browser(url)\n    prompt = summarize_prompt.format(website_text=website_text)\n    messages = [\n        Message(role=\"system\", content=system_prompt),\n        Message(role=\"user\", content=prompt),\n    ]\n\n    hparams = {\"temperature\": 0.2}\n\n    completion = llm.chat(messages, hparams)\n\n    return completion.content", ""]}
{"filename": "examples/agents/code_optimizer.py", "chunked_list": ["import os\nfrom log10.anthropic import Anthropic\nfrom log10.llm import NoopLLM\nfrom log10.load import log10\nfrom log10.evals import compile\nfrom log10.agents.camel import camel_agent\nfrom log10.openai import OpenAI\nfrom log10.tools import code_extractor\n\n# Select one of OpenAI or Anthropic models", "\n# Select one of OpenAI or Anthropic models\nmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\nmax_turns = 10\n\nllm = None\nsummary_model = None\nextraction_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    extraction_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    extraction_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    extraction_model = \"gpt-4\"\n    llm = OpenAI({\"model\": model})", "if \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    extraction_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    extraction_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    extraction_model = \"gpt-4\"\n    llm = OpenAI({\"model\": model})", "\n\n# example calls from playground (select 1)\nuser_messages, assistant_messages = camel_agent(\n    user_role=\"C developer\",\n    assistant_role=\"Cybersecurity expert\",\n    task_prompt='Correct the following code.\\n\\n#include <stdio.h>\\n#include <string.h>\\n\\nint main() {\\n    char password[8];\\n    int granted = 0;\\n\\n    printf(\"Enter password: \");\\n    scanf(\"%s\", password);\\n\\n    if (strcmp(password, \"password\") == 0) {\\n        granted = 1;\\n    }\\n\\n    if (granted) {\\n        printf(\"Access granted.\\\\n\");\\n    } else {\\n        printf(\"Access denied.\\\\n\");\\n    }\\n\\n    return 0;\\n}',\n    summary_model=summary_model,\n    max_turns=max_turns,\n    llm=llm,", "    max_turns=max_turns,\n    llm=llm,\n)\n\nfull_response = assistant_messages[-1].content\n\n# Next extract just the C code\ncode = code_extractor(full_response, \"C\", extraction_model, llm=llm)\nprint(f\"Extracted code\\n###\\n{code}\")\n", "print(f\"Extracted code\\n###\\n{code}\")\n\n# Evaluate if the code compiles\nresult = compile(code)\nif result is True:\n    print(\"Compilation successful\")\nelse:\n    print(\"Compilation failed with error:\")\n    print(result[1])\n", ""]}
{"filename": "examples/agents/coder.py", "chunked_list": ["import os\nfrom log10.anthropic import Anthropic\nfrom log10.llm import NoopLLM\nfrom log10.load import log10\nfrom log10.agents.camel import camel_agent\nfrom dotenv import load_dotenv\n\nfrom log10.openai import OpenAI\n\nload_dotenv()", "\nload_dotenv()\n\n# Select one of OpenAI or Anthropic models\nmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\nmax_turns = 30\n\nllm = None\nsummary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "summary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "\n# example calls from playground (select 1)\ncamel_agent(\n    user_role=\"Stock Trader\",\n    assistant_role=\"Python Programmer\",\n    task_prompt=\"Develop a trading bot for the stock market\",\n    summary_model=summary_model,\n    max_turns=max_turns,\n    llm=llm,\n)", "    llm=llm,\n)\n"]}
{"filename": "examples/agents/biochemist.py", "chunked_list": ["import os\nfrom log10.anthropic import Anthropic\nfrom log10.llm import NoopLLM\nfrom log10.agents.camel import camel_agent\nfrom dotenv import load_dotenv\n\nfrom log10.openai import OpenAI\nfrom log10.load import log10\n\nload_dotenv()", "\nload_dotenv()\n\n# Select one of OpenAI or Anthropic models\nmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\nmax_turns = 30\n\nllm = None\nsummary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "summary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "\n# example calls from playground (select 1)\ncamel_agent(\n    user_role=\"Poor PhD Student\",\n    assistant_role=\"Experienced Computational Chemist\",\n    task_prompt=\"Perform a molecular dynamics solution of a molecule: CN1CCC[C@H]1c2cccnc2. Design and conduct a 100 ns molecular dynamics simulation of the molecule CN1CCC[C@H]1c2cccnc2 in an explicit solvent environment using the CHARMM force field and analyze the conformational changes and hydrogen bonding patterns over time\",\n    summary_model=summary_model,\n    max_turns=max_turns,\n    llm=llm,\n)", "    llm=llm,\n)\n"]}
{"filename": "examples/agents/email_generator.py", "chunked_list": ["import os\nfrom log10.anthropic import Anthropic\nfrom log10.llm import NoopLLM\nfrom log10.load import log10\nfrom log10.agents.camel import camel_agent\nfrom dotenv import load_dotenv\n\nfrom log10.openai import OpenAI\n\nload_dotenv()", "\nload_dotenv()\n\n# Select one of OpenAI or Anthropic models\nmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\nmax_turns = 30\n\nllm = None\nsummary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "summary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "\n# example calls from playground (select 1)\ncamel_agent(\n    user_role=\"Sales email copyeditor\",\n    assistant_role=\"Sales email copywriter\",\n    task_prompt=\"Write a sales email to Pfizer about a new healthcare CRM\",\n    summary_model=summary_model,\n    max_turns=max_turns,\n    llm=llm,\n)", "    llm=llm,\n)\n"]}
{"filename": "examples/agents/scrape_summarizer.py", "chunked_list": ["import os\nfrom log10.agents.scrape_summarizer import scrape_summarizer\nfrom log10.anthropic import Anthropic\nfrom log10.llm import NoopLLM\nfrom log10.load import log10\nfrom log10.openai import OpenAI\n\n\n# Select one of OpenAI or Anthropic models\nmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")", "# Select one of OpenAI or Anthropic models\nmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n\nllm = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    llm = OpenAI({\"model\": model})", "\nurl = \"https://nytimes.com\"\nprint(scrape_summarizer(url, llm))\n"]}
{"filename": "examples/agents/translator.py", "chunked_list": ["import os\nfrom log10.anthropic import Anthropic\nfrom log10.llm import NoopLLM\nfrom log10.load import log10\nfrom log10.agents.camel import camel_agent\nfrom dotenv import load_dotenv\n\nfrom log10.openai import OpenAI\n\nload_dotenv()", "\nload_dotenv()\n\n# Select one of OpenAI or Anthropic models\nmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\n\nmax_turns = 30\n\nllm = None\nsummary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "llm = None\nsummary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "\n# example calls from playground (select 1)\ncamel_agent(\n    user_role=\"Web3 guru\",\n    assistant_role=\"Hindi translator\",\n    task_prompt=\"Write a blog post about web3 in Hindi\",\n    summary_model=summary_model,\n    max_turns=max_turns,\n    llm=llm,\n)", "    llm=llm,\n)\n"]}
{"filename": "examples/agents/cybersecurity_expert.py", "chunked_list": ["import os\nfrom log10.anthropic import Anthropic\nfrom log10.llm import NoopLLM\nfrom log10.load import log10\nfrom log10.agents.camel import camel_agent\nfrom dotenv import load_dotenv\n\nfrom log10.openai import OpenAI\n\nload_dotenv()", "\nload_dotenv()\n\n# Select one of OpenAI or Anthropic models\nmodel = os.environ.get(\"LOG10_EXAMPLES_MODEL\", \"gpt-3.5-turbo-16k\")\nmax_turns = 30\n\nllm = None\nsummary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "summary_model = None\nif \"claude\" in model:\n    import anthropic\n    log10(anthropic)\n    summary_model = \"claude-1-100k\"\n    llm = Anthropic({\"model\": model})\nelif model == \"noop\":\n    summary_model = model\n    llm = NoopLLM()\nelse:\n    import openai\n    log10(openai)\n    summary_model = \"gpt-3.5-turbo-16k\"\n    llm = OpenAI({\"model\": model})", "\n# example calls from playground (select 1)\ncamel_agent(\n    user_role=\"C developer\",\n    assistant_role=\"Cybersecurity expert\",\n    task_prompt='Correct the following code.\\n\\n#include <stdio.h>\\n#include <string.h>\\n\\nint main() {\\n    char password[8];\\n    int granted = 0;\\n\\n    printf(\"Enter password: \");\\n    scanf(\"%s\", password);\\n\\n    if (strcmp(password, \"password\") == 0) {\\n        granted = 1;\\n    }\\n\\n    if (granted) {\\n        printf(\"Access granted.\\\\n\");\\n    } else {\\n        printf(\"Access denied.\\\\n\");\\n    }\\n\\n    return 0;\\n}',\n    summary_model=summary_model,\n    max_turns=max_turns,\n    llm=llm,\n)", "    llm=llm,\n)\n"]}
{"filename": "examples/logging/langchain_model_logger_url.py", "chunked_list": ["from langchain import OpenAI\nfrom langchain.chat_models import ChatAnthropic\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom log10.langchain import Log10Callback\nfrom log10.llm import Log10Config\n\n\nlog10_callback = Log10Callback(log10_config=Log10Config())", "\nlog10_callback = Log10Callback(log10_config=Log10Config())\n\n\nmessages = [\n    HumanMessage(content=\"You are a ping pong machine\"),\n    HumanMessage(content=\"Ping?\"),\n]\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])", "\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])\ncompletion = llm.predict_messages(messages, tags=[\"foobar\"])\nprint(completion)\n\nprint(log10_callback.last_completion_url())\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])\nmessages.append(HumanMessage(content=\"Pong!\"))\ncompletion = llm.predict_messages(messages, tags=[\"foobar\"])", "messages.append(HumanMessage(content=\"Pong!\"))\ncompletion = llm.predict_messages(messages, tags=[\"foobar\"])\nprint(completion)\n\nprint(log10_callback.last_completion_url())"]}
{"filename": "examples/logging/anthropic_completion.py", "chunked_list": ["import os\nfrom log10.load import log10\nimport anthropic\nimport os\n\nlog10(anthropic, DEBUG_=False)\nanthropicClient = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n\nresponse = anthropicClient.completions.create(\n    model=\"claude-1\",", "response = anthropicClient.completions.create(\n    model=\"claude-1\",\n    prompt=f\"\\n\\nHuman:Write the names of all Star Wars movies and spinoffs along with the time periods in which they were set?{anthropic.AI_PROMPT}\",\n    temperature=0,\n    max_tokens_to_sample=1024,\n    top_p=1,\n    top_k=0\n)\n\nprint(response)", "\nprint(response)\n"]}
{"filename": "examples/logging/langchain_multiple_tools.py", "chunked_list": ["import os\nfrom log10.load import log10\nimport openai\n\nlog10(openai)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\nMAX_TOKENS = 512\nTOOLS_DEFAULT_LIST =  ['llm-math', 'wikipedia']\n", "TOOLS_DEFAULT_LIST =  ['llm-math', 'wikipedia']\n\nfrom langchain.llms import OpenAI\nfrom langchain.agents import load_tools, initialize_agent\nimport wikipedia\n\nllm = OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=MAX_TOKENS)\n\n# Set up Langchain\ntools = load_tools(TOOLS_DEFAULT_LIST, llm=llm)", "# Set up Langchain\ntools = load_tools(TOOLS_DEFAULT_LIST, llm=llm)\nchain = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n\ninp = \"How many years elapsed between the founding of Apple and Google?\"\nprint(chain.run(inp))"]}
{"filename": "examples/logging/langchain_sqlagent.py", "chunked_list": ["import faker\nimport sqlalchemy\nfrom langchain.sql_database import SQLDatabase\nfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\nfrom langchain.agents import create_sql_agent\nfrom langchain.llms import OpenAI\nfrom faker import Faker\nimport random\nimport datetime\nfrom sqlalchemy.orm import sessionmaker", "import datetime\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy import create_engine, Column, Integer, String, DateTime\nimport os\nfrom log10.load import log10\nimport openai\n\nlog10(openai)\n", "log10(openai)\n\n\n# Set up a dummy database\nfake = Faker()\n\n# Create a SQLite database and connect to it\nengine = create_engine('sqlite:///users.db', echo=True)\nBase = declarative_base()\n", "Base = declarative_base()\n\n# Define the User class with standard fields and the created_at field\nclass User(Base):\n    __tablename__ = 'users'\n\n    id = Column(Integer, primary_key=True)\n    username = Column(String, unique=True, nullable=False)\n    email = Column(String, unique=True, nullable=False)\n    first_name = Column(String, nullable=False)\n    last_name = Column(String, nullable=False)\n    age = Column(Integer, nullable=False)\n    created_at = Column(DateTime, default=datetime.datetime.utcnow)\n\n    def __repr__(self):\n        return f\"<User(id={self.id}, username='{self.username}', email='{self.email}', first_name='{self.first_name}', last_name='{self.last_name}', age={self.age}, created_at={self.created_at})>\"", "\n# Helper function to generate a random user using Faker\ndef generate_random_user():\n    username = fake.user_name()\n    email = fake.email()\n    first_name = fake.first_name()\n    last_name = fake.last_name()\n    age = random.randint(18, 100)\n    return User(username=username, email=email, first_name=first_name, last_name=last_name, age=age)\n", "\n\n# Create the 'users' table\nBase.metadata.create_all(engine)\n\n# Create a session factory and a session\nSession = sessionmaker(bind=engine)\nsession = Session()\n\n# Add some example users\nfor n_users in range(10):\n    user = generate_random_user()\n    session.add(user)", "\n# Add some example users\nfor n_users in range(10):\n    user = generate_random_user()\n    session.add(user)\n\nsession.commit()\n\n# Query the users and print the results\nall_users = session.query(User).all()", "# Query the users and print the results\nall_users = session.query(User).all()\nprint(all_users)\n\nsession.close()\n\n# Setup vars for Langchain\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Setup Langchain SQL agent", "\n# Setup Langchain SQL agent\ndb = SQLDatabase.from_uri(\"sqlite:///users.db\")\ntoolkit = SQLDatabaseToolkit(db=db)\n\nagent_executor = create_sql_agent(\n    llm=OpenAI(temperature=0, model_name=\"text-davinci-003\"),\n    toolkit=toolkit,\n    verbose=True\n)", "    verbose=True\n)\n\nprint(agent_executor.run(\"Who is the least recent user?\"))\n"]}
{"filename": "examples/logging/chatcompletion_async_vs_sync.py", "chunked_list": ["import sys\n\nif 'init_modules' in globals():\n    # second or subsequent run: remove all but initially loaded modules\n    for m in list(sys.modules.keys()):\n        if m not in init_modules:\n            del (sys.modules[m])\nelse:\n    # first run: find out which modules were initially loaded\n    init_modules = list(sys.modules.keys())", "\nimport os\nfrom log10.load import log10, log10_session\nimport openai\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Launch an async run\nwith log10_session():\n    log10(openai, DEBUG_=True, USE_ASYNC_=True)\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {'role': \"system\", \"content\": \"You are the most knowledgable Star Wars guru on the planet\"},\n            {\"role\": \"user\", \"content\": \"Write the time period of all the Star Wars movies and spinoffs?\"}\n        ]\n    )\n    print(completion.choices[0].message)", "with log10_session():\n    log10(openai, DEBUG_=True, USE_ASYNC_=True)\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {'role': \"system\", \"content\": \"You are the most knowledgable Star Wars guru on the planet\"},\n            {\"role\": \"user\", \"content\": \"Write the time period of all the Star Wars movies and spinoffs?\"}\n        ]\n    )\n    print(completion.choices[0].message)", "\n# reload modules to prevent double calling openAI\nif 'init_modules' in globals():\n    # second or subsequent run: remove all but initially loaded modules\n    for m in list(sys.modules.keys()):\n        if m not in init_modules:\n            del (sys.modules[m])\nelse:\n    # first run: find out which modules were initially loaded\n    init_modules = list(sys.modules.keys())", "\n\nimport openai  # noqa\n\n# Compare to sync run - note there can be variability in the OpenAI calls\nwith log10_session():\n    log10(openai, DEBUG_=True, USE_ASYNC_=False)\n\n    completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {'role': \"system\", \"content\": \"You are the most knowledgable Star Wars guru on the planet\"},\n            {\"role\": \"user\", \"content\": \"Write the time period of all the Star Wars movies and spinoffs?\"}\n        ]\n    )\n    print(completion.choices[0].message)", ""]}
{"filename": "examples/logging/langchain_model_logger.py", "chunked_list": ["from langchain import OpenAI\nfrom langchain.chat_models import ChatAnthropic\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage\n\nfrom log10.langchain import Log10Callback\nfrom log10.llm import Log10Config\n\n\nlog10_callback = Log10Callback(log10_config=Log10Config())", "\nlog10_callback = Log10Callback(log10_config=Log10Config())\n\n\nmessages = [\n    HumanMessage(content=\"You are a ping pong machine\"),\n    HumanMessage(content=\"Ping?\"),\n]\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])", "\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", callbacks=[log10_callback], temperature=0.5, tags=[\"test\"])\ncompletion = llm.predict_messages(messages, tags=[\"foobar\"])\nprint(completion)\n\nllm = ChatAnthropic(model=\"claude-2\", callbacks=[log10_callback], temperature=0.7, tags=[\"baz\"])\nllm.predict_messages(messages)\nprint(completion)\n\nllm = OpenAI(model_name=\"text-davinci-003\", callbacks=[log10_callback], temperature=0.5)", "\nllm = OpenAI(model_name=\"text-davinci-003\", callbacks=[log10_callback], temperature=0.5)\ncompletion = llm.predict(\"You are a ping pong machine.\\nPing?\\n\")\nprint(completion)\n"]}
{"filename": "examples/logging/langchain_simple_sequential.py", "chunked_list": ["import os\nfrom log10.load import log10\nimport openai\n\nlog10(openai)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate", "from langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain, SimpleSequentialChain\n\nllm = OpenAI(temperature=0.9, model_name=\"text-babbage-001\")\nprompt = PromptTemplate(\n    input_variables=[\"product\"],\n    template=\"What is a good name for a company that makes {product}?\",\n)\n", ")\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\nsecond_prompt = PromptTemplate(\n    input_variables=[\"company_name\"],\n    template=\"Write a catchphrase for the following company: {company_name}\",\n)\nchain_two = LLMChain(llm=llm, prompt=second_prompt)\n", "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n\noverall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n\n# Run the chain specifying only the input variable for the first chain.\ncatchphrase = overall_chain.run(\"colorful socks\")\nprint(catchphrase)"]}
{"filename": "examples/logging/completion_ada.py", "chunked_list": ["import os\nfrom log10.load import log10\nimport openai\n\nlog10(openai, DEBUG_=True, USE_ASYNC_=False)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nresponse = openai.Completion.create(\n    model=\"text-ada-001\",", "response = openai.Completion.create(\n    model=\"text-ada-001\",\n    prompt=\"What is 2+2?\",\n    temperature=0,\n    max_tokens=1024,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0\n)\n", ")\n\nprint(response)\n"]}
{"filename": "examples/logging/tags_openai.py", "chunked_list": ["import os\nfrom log10.load import log10, log10_session\nimport openai\n\nlog10(openai)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\nresponse = openai.Completion.create(", "\nresponse = openai.Completion.create(\n    model=\"text-ada-001\",\n    prompt=\"Where are the pyramids?\",\n    temperature=0,\n    max_tokens=1024,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n)", "    presence_penalty=0,\n)\nprint(response)\n\nwith log10_session(tags=[\"foo\", \"bar\"]):\n    response = openai.Completion.create(\n        model=\"text-ada-001\",\n        prompt=\"Where is the Eiffel Tower?\",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    print(response)", "\nwith log10_session(tags=[\"bar\", \"baz\"]):\n    response = openai.Completion.create(\n        model=\"text-ada-001\",\n        prompt=\"Where is the statue of liberty?\",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    print(response)", "\nresponse = openai.Completion.create(\n    model=\"text-ada-001\",\n    prompt=\"Where is machu picchu?\",\n    temperature=0,\n    max_tokens=1024,\n    top_p=1,\n    frequency_penalty=0,\n    presence_penalty=0,\n)", "    presence_penalty=0,\n)\nprint(response)"]}
{"filename": "examples/logging/completion.py", "chunked_list": ["import os\nfrom log10.load import log10\nimport openai\n\nlog10(openai)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nresponse = openai.Completion.create(\n  model=\"text-davinci-003\",", "response = openai.Completion.create(\n  model=\"text-davinci-003\",\n  prompt=\"Write the names of all Star Wars movies and spinoffs along with the time periods in which they were set?\",\n  temperature=0,\n  max_tokens=1024,\n  top_p=1,\n  frequency_penalty=0,\n  presence_penalty=0\n)\n", ")\n\nprint(response)\n"]}
{"filename": "examples/logging/langchain_babyagi.py", "chunked_list": ["import os\nfrom collections import deque\nfrom typing import Dict, List, Optional, Any\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain import LLMChain, PromptTemplate\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.llms import BaseLLM\nfrom langchain.vectorstores.base import VectorStore\nfrom pydantic import BaseModel, Field", "from langchain.vectorstores.base import VectorStore\nfrom pydantic import BaseModel, Field\nfrom langchain.chains.base import Chain\n\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore import InMemoryDocstore\n\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, SerpAPIWrapper, LLMChain\n", "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n\nimport openai\nimport log10\nfrom log10.load import log10\n\nlog10(openai)\n\n# Adapted from: https://python.langchain.com/en/latest/use_cases/agents/baby_agi_with_agent.html\n", "# Adapted from: https://python.langchain.com/en/latest/use_cases/agents/baby_agi_with_agent.html\n\n# Define your embedding model\nembeddings_model = OpenAIEmbeddings()\n# Initialize the vectorstore as empty\nimport faiss\nembedding_size = 1536\nindex = faiss.IndexFlatL2(embedding_size)\nvectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n", "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})\n\n\nclass TaskCreationChain(LLMChain):\n    \"\"\"Chain to generates tasks.\"\"\"\n\n    @classmethod\n    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n        \"\"\"Get the response parser.\"\"\"\n        task_creation_template = (\n            \"You are an task creation AI that uses the result of an execution agent\"\n            \" to create new tasks with the following objective: {objective},\"\n            \" The last completed task has the result: {result}.\"\n            \" This result was based on this task description: {task_description}.\"\n            \" These are incomplete tasks: {incomplete_tasks}.\"\n            \" Based on the result, create new tasks to be completed\"\n            \" by the AI system that do not overlap with incomplete tasks.\"\n            \" Return the tasks as an array.\"\n        )\n        prompt = PromptTemplate(\n            template=task_creation_template,\n            input_variables=[\"result\", \"task_description\", \"incomplete_tasks\", \"objective\"],\n        )\n        return cls(prompt=prompt, llm=llm, verbose=verbose)", "    \nclass TaskPrioritizationChain(LLMChain):\n    \"\"\"Chain to prioritize tasks.\"\"\"\n\n    @classmethod\n    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n        \"\"\"Get the response parser.\"\"\"\n        task_prioritization_template = (\n            \"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\n            \" the following tasks: {task_names}.\"\n            \" Consider the ultimate objective of your team: {objective}.\"\n            \" Do not remove any tasks. Return the result as a numbered list, like:\"\n            \" #. First task\"\n            \" #. Second task\"\n            \" Start the task list with number {next_task_id}.\"\n        )\n        prompt = PromptTemplate(\n            template=task_prioritization_template,\n            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n        )\n        return cls(prompt=prompt, llm=llm, verbose=verbose)", "\ntodo_prompt = PromptTemplate.from_template(\"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\")\ntodo_chain = LLMChain(llm=ChatOpenAI(temperature=0), prompt=todo_prompt)\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name = \"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\"\n    ),", "        description=\"useful for when you need to answer questions about current events\"\n    ),\n    Tool(\n        name = \"TODO\",\n        func=todo_chain.run,\n        description=\"useful for when you need to come up with todo lists. Input: an objective to create a todo list for. Output: a todo list for that objective. Please be very clear what the objective is!\"\n    )\n]\n\n", "\n\nprefix = \"\"\"You are an AI who performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.\"\"\"\nsuffix = \"\"\"Question: {task}\n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools, \n    prefix=prefix, \n    suffix=suffix, \n    input_variables=[\"objective\", \"task\", \"context\",\"agent_scratchpad\"]", "    suffix=suffix, \n    input_variables=[\"objective\", \"task\", \"context\",\"agent_scratchpad\"]\n)\n    \ndef get_next_task(task_creation_chain: LLMChain, result: Dict, task_description: str, task_list: List[str], objective: str) -> List[Dict]:\n    \"\"\"Get the next task.\"\"\"\n    incomplete_tasks = \", \".join(task_list)\n    response = task_creation_chain.run(result=result, task_description=task_description, incomplete_tasks=incomplete_tasks, objective=objective)\n    new_tasks = response.split('\\n')\n    return [{\"task_name\": task_name} for task_name in new_tasks if task_name.strip()]", "\ndef prioritize_tasks(task_prioritization_chain: LLMChain, this_task_id: int, task_list: List[Dict], objective: str) -> List[Dict]:\n    \"\"\"Prioritize tasks.\"\"\"\n    task_names = [t[\"task_name\"] for t in task_list]\n    next_task_id = int(this_task_id) + 1\n    response = task_prioritization_chain.run(task_names=task_names, next_task_id=next_task_id, objective=objective)\n    new_tasks = response.split('\\n')\n    prioritized_task_list = []\n    for task_string in new_tasks:\n        if not task_string.strip():\n            continue\n        task_parts = task_string.strip().split(\".\", 1)\n        if len(task_parts) == 2:\n            task_id = task_parts[0].strip()\n            task_name = task_parts[1].strip()\n            prioritized_task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n    return prioritized_task_list", "\ndef _get_top_tasks(vectorstore, query: str, k: int) -> List[str]:\n    \"\"\"Get the top k tasks based on the query.\"\"\"\n    results = vectorstore.similarity_search_with_score(query, k=k)\n    if not results:\n        return []\n    sorted_results, _ = zip(*sorted(results, key=lambda x: x[1], reverse=True))\n    return [str(item.metadata['task']) for item in sorted_results]\n\ndef execute_task(vectorstore, execution_chain: LLMChain, objective: str, task: str, k: int = 5) -> str:\n    \"\"\"Execute a task.\"\"\"\n    context = _get_top_tasks(vectorstore, query=objective, k=k)\n    return execution_chain.run(objective=objective, context=context, task=task)", "\ndef execute_task(vectorstore, execution_chain: LLMChain, objective: str, task: str, k: int = 5) -> str:\n    \"\"\"Execute a task.\"\"\"\n    context = _get_top_tasks(vectorstore, query=objective, k=k)\n    return execution_chain.run(objective=objective, context=context, task=task)\n\nclass BabyAGI(Chain, BaseModel):\n    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n\n    task_list: deque = Field(default_factory=deque)\n    task_creation_chain: TaskCreationChain = Field(...)\n    task_prioritization_chain: TaskPrioritizationChain = Field(...)\n    execution_chain: AgentExecutor = Field(...)\n    task_id_counter: int = Field(1)\n    vectorstore: VectorStore = Field(init=False)\n    max_iterations: Optional[int] = None\n        \n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n        arbitrary_types_allowed = True\n\n    def add_task(self, task: Dict):\n        self.task_list.append(task)\n\n    def print_task_list(self):\n        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n        for t in self.task_list:\n            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n\n    def print_next_task(self, task: Dict):\n        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n\n    def print_task_result(self, result: str):\n        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n        print(result)\n        \n    @property\n    def input_keys(self) -> List[str]:\n        return [\"objective\"]\n    \n    @property\n    def output_keys(self) -> List[str]:\n        return []\n\n    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Run the agent.\"\"\"\n        objective = inputs['objective']\n        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n        num_iters = 0\n        while True:\n            if self.task_list:\n                self.print_task_list()\n\n                # Step 1: Pull the first task\n                task = self.task_list.popleft()\n                self.print_next_task(task)\n\n                # Step 2: Execute the task\n                result = execute_task(\n                    self.vectorstore, self.execution_chain, objective, task[\"task_name\"]\n                )\n                this_task_id = int(task[\"task_id\"])\n                self.print_task_result(result)\n\n                # Step 3: Store the result in Pinecone\n                result_id = f\"result_{task['task_id']}\"\n                self.vectorstore.add_texts(\n                    texts=[result],\n                    metadatas=[{\"task\": task[\"task_name\"]}],\n                    ids=[result_id],\n                )\n\n                # Step 4: Create new tasks and reprioritize task list\n                new_tasks = get_next_task(\n                    self.task_creation_chain, result, task[\"task_name\"], [t[\"task_name\"] for t in self.task_list], objective\n                )\n                for new_task in new_tasks:\n                    self.task_id_counter += 1\n                    new_task.update({\"task_id\": self.task_id_counter})\n                    self.add_task(new_task)\n                self.task_list = deque(\n                    prioritize_tasks(\n                        self.task_prioritization_chain, this_task_id, list(self.task_list), objective\n                    )\n                )\n            num_iters += 1\n            if self.max_iterations is not None and num_iters == self.max_iterations:\n                print(\"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\")\n                break\n        return {}\n\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLLM,\n        vectorstore: VectorStore,\n        verbose: bool = False,\n        **kwargs\n    ) -> \"BabyAGI\":\n        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n        task_creation_chain = TaskCreationChain.from_llm(\n            llm, verbose=verbose\n        )\n        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n            llm, verbose=verbose\n        )\n        llm_chain = LLMChain(llm=llm, prompt=prompt)\n        tool_names = [tool.name for tool in tools]\n        agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\n        agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n        return cls(\n            task_creation_chain=task_creation_chain,\n            task_prioritization_chain=task_prioritization_chain,\n            execution_chain=agent_executor,\n            vectorstore=vectorstore,\n            **kwargs\n        )", "\nOBJECTIVE = \"Invent a new drug to cure cancer.\"\n\nllm = ChatOpenAI(temperature=0)\n\n# Logging of LLMChains\nverbose=False\n# If None, will keep on going forever\nmax_iterations: Optional[int] = 3\nbaby_agi = BabyAGI.from_llm(", "max_iterations: Optional[int] = 3\nbaby_agi = BabyAGI.from_llm(\n    llm=llm,\n    vectorstore=vectorstore,\n    verbose=verbose,\n    max_iterations=max_iterations\n)\n\nbaby_agi({\"objective\": OBJECTIVE})\n", "baby_agi({\"objective\": OBJECTIVE})\n\n"]}
{"filename": "examples/logging/multiple_sessions.py", "chunked_list": ["from langchain.chains import LLMChain, SimpleSequentialChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\nimport os\nfrom log10.load import log10, log10_session\nimport openai\n\nlog10(openai)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")", "\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\nllm = OpenAI(temperature=0.9, model_name=\"text-curie-001\")\n\nwith log10_session():\n    prompt = PromptTemplate(\n        input_variables=[\"product\"],\n        template=\"What is a good name for a company that makes {product}?\",\n    )\n    chain = LLMChain(llm=llm, prompt=prompt)\n    second_prompt = PromptTemplate(\n        input_variables=[\"company_name\"],\n        template=\"Write a catchphrase for the following company: {company_name}\",\n    )\n    chain_two = LLMChain(llm=llm, prompt=second_prompt)\n    overall_chain = SimpleSequentialChain(\n        chains=[chain, chain_two], verbose=True)\n    # Run the chain specifying only the input variable for the first chain.\n    catchphrase = overall_chain.run(\"colorful socks\")\n    print(catchphrase)", "\nwith log10_session():\n    third_prompt = PromptTemplate(\n        input_variables=[\"month\"],\n        template=\"What is a good country to travel during {month}?\",\n    )\n\n    chain_three = LLMChain(llm=llm, prompt=third_prompt)\n\n    fourth_prompt = PromptTemplate(\n        input_variables=[\"country_name\"],\n        template=\"Write a 1 day itinerary to {country_name}\",\n    )\n    chain_four = LLMChain(llm=llm, prompt=fourth_prompt)\n\n    overall_chain_two = SimpleSequentialChain(\n        chains=[chain_three, chain_four], verbose=True)\n\n    # Run the chain specifying only the input variable for the first chain.\n    catchphrase_two = overall_chain_two.run(\"April\")\n    print(catchphrase_two)", ""]}
{"filename": "examples/logging/langchain_qa.py", "chunked_list": ["import os\nfrom log10.load import log10\nimport openai\n\nlog10(openai)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Example from: https://python.langchain.com/en/latest/use_cases/question_answering.html\n# Download the state_of_the_union.txt here: https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt", "# Example from: https://python.langchain.com/en/latest/use_cases/question_answering.html\n# Download the state_of_the_union.txt here: https://raw.githubusercontent.com/hwchase17/langchain/master/docs/modules/state_of_the_union.txt\n# This example requires: pip install chromadb\n\n# Load Your Documents\nfrom langchain.document_loaders import TextLoader\nloader = TextLoader('./examples/logging/state_of_the_union.txt')\n\n# Create Your Index\nfrom langchain.indexes import VectorstoreIndexCreator", "# Create Your Index\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chat_models import ChatOpenAI\n\nindex = VectorstoreIndexCreator(\n    vectorstore_cls=Chroma, \n    embedding=OpenAIEmbeddings(),", "    vectorstore_cls=Chroma, \n    embedding=OpenAIEmbeddings(),\n    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n).from_loaders([loader])\n\n# Query Your Index\nquery = \"What did the president say about Ketanji Brown Jackson\"\nprint(index.query_with_sources(query, llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")))\n", ""]}
{"filename": "examples/logging/chatcompletion.py", "chunked_list": ["import os\nfrom log10.load import log10\nimport openai\n\nlog10(openai)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ncompletion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",", "completion = openai.ChatCompletion.create(\n  model=\"gpt-3.5-turbo\",\n  messages=[\n    {'role': \"system\", \"content\": \"You are the most knowledgable Star Wars guru on the planet\"},\n    {\"role\": \"user\", \"content\": \"Write the time period of all the Star Wars movies and spinoffs?\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n", "print(completion.choices[0].message)\n"]}
{"filename": "examples/logging/get_url.py", "chunked_list": ["from langchain.chains import LLMChain, SimpleSequentialChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.llms import OpenAI\nimport os\nfrom log10.load import log10, log10_session\nimport openai\n\nlog10(openai)\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")", "\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nllm = OpenAI(temperature=0.9, model_name=\"text-curie-001\")\n\nwith log10_session() as session:\n    print(session.last_completion_url())\n\n    response = openai.Completion.create(\n        model=\"text-ada-001\",\n        prompt=\"Why did the chicken cross the road?\",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n\n    print(session.last_completion_url())\n\n    response = openai.Completion.create(\n        model=\"text-ada-001\",\n        prompt=\"Why did the cow cross the road?\",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n\n    print(session.last_completion_url())", "\nwith log10_session() as session:\n    print(session.last_completion_url())\n\n    response = openai.Completion.create(\n        model=\"text-ada-001\",\n        prompt=\"Why did the frog cross the road?\",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n\n    print(session.last_completion_url())\n\n    response = openai.Completion.create(\n        model=\"text-ada-001\",\n        prompt=\"Why did the scorpion cross the road?\",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n\n    print(session.last_completion_url())", ""]}
{"filename": "examples/logging/tags_mixed.py", "chunked_list": ["import os\nfrom log10.load import log10, log10_session\nimport openai\nfrom langchain import OpenAI\n\nlog10(openai)\n\nwith log10_session(tags=[\"foo\", \"bar\"]):\n    response = openai.Completion.create(\n        model=\"text-ada-001\",\n        prompt=\"Where is the Eiffel Tower?\",\n        temperature=0,\n        max_tokens=1024,\n        top_p=1,\n        frequency_penalty=0,\n        presence_penalty=0,\n    )\n    print(response)\n\n    llm = OpenAI(model_name=\"text-ada-001\", temperature=0.5)\n    response = llm.predict(\"You are a ping pong machine.\\nPing?\\n\")\n    print(response)", ""]}
{"filename": "examples/evals/fuzzy.py", "chunked_list": ["import os\nfrom log10.openai import OpenAI\nfrom log10.anthropic import Anthropic\nfrom log10.evals import eval\n\n# Choose provider\nprovider = \"anthropic\"  # \"anthropic\"\nllm = None\nif provider == \"openai\":\n    llm = OpenAI(\n        {\n            \"model\": \"gpt-3.5-turbo\",\n            \"temperature\": 0,\n            \"max_tokens\": 1024,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n        }\n    )\nelif provider == \"anthropic\":\n    llm = Anthropic(\n        {\n            \"model\": \"claude-1\",\n            \"temperature\": 0,\n            \"max_tokens_to_sample\": 1024,\n        }\n    )\nelse:\n    print(\n        f\"Unsupported provider option: {provider}. Supported providers are 'openai' or 'anthropic'.\"\n    )", "if provider == \"openai\":\n    llm = OpenAI(\n        {\n            \"model\": \"gpt-3.5-turbo\",\n            \"temperature\": 0,\n            \"max_tokens\": 1024,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n        }\n    )\nelif provider == \"anthropic\":\n    llm = Anthropic(\n        {\n            \"model\": \"claude-1\",\n            \"temperature\": 0,\n            \"max_tokens_to_sample\": 1024,\n        }\n    )\nelse:\n    print(\n        f\"Unsupported provider option: {provider}. Supported providers are 'openai' or 'anthropic'.\"\n    )", "\n# Ground truth dataset to use for evaluation\neval_dataset = (\n    \"fuzzy_data.csv\",\n    {\"input\": \"my_input_column\", \"ideal\": \"my_output_column\"},\n)\n\n# Specify which metrics to use. Options are:\n# 'match': model_output.startswith(ideal)\n# 'includes': ideal.lower() in model_output.lower()", "# 'match': model_output.startswith(ideal)\n# 'includes': ideal.lower() in model_output.lower()\n# 'fuzzy_match': similar to includes but remove punctuation, articles and extra whitespace and compare both ways\neval_metric = \"fuzzy_match\"\n\n# Path to output file to store the metrics\n# Example from: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/registry/evals/test-basic.yaml#L7\nout_file_path = \"fuzzy_output.csv\"\n\n# Get back and id and url for the summary of results and status", "\n# Get back and id and url for the summary of results and status\n# todo: get back path to logfile; eval_id, eval_url =\neval(llm, eval_dataset, eval_metric, out_file_path)\n"]}
{"filename": "examples/evals/compile.py", "chunked_list": ["from log10.anthropic import Anthropic\nfrom log10.llm import Message, NoopLLM\nfrom log10.load import log10\nfrom log10.evals import compile\nfrom log10.openai import OpenAI\nfrom log10.tools import code_extractor\n\n# Select one of OpenAI or Anthropic models\nmodel = \"gpt-3.5-turbo\"\n# model = \"claude-1\"", "model = \"gpt-3.5-turbo\"\n# model = \"claude-1\"\n\n\nllm = None\nif \"claude\" in model:\n    llm = Anthropic({\"model\": model})\n    extraction_model = \"claude-1-100k\"\nelif model == \"noop\":\n    llm = NoopLLM()\n    extraction_model = \"noop\"\nelse:\n    llm = OpenAI({\"model\": model})\n    extraction_model = \"gpt-4\"", "\n\n# First, write a hello world program\nmessages = [\n    Message(role=\"system\", content=\"You are an expert C programmer.\"),\n    Message(\n        role=\"user\",\n        content=\"Write a hello world program. Insert a null character after the hello world\",\n    ),\n]", "    ),\n]\n\ncompletion = llm.chat(messages, {\"temperature\": 0.2})\nfull_response = completion.content\n\nprint(f\"Full response\\n###\\n{full_response}\")\n\n# Next extract just the C code\ncode = code_extractor(full_response, \"C\", extraction_model, llm)", "# Next extract just the C code\ncode = code_extractor(full_response, \"C\", extraction_model, llm)\nprint(f\"Extracted code\\n###\\n{code}\")\n\n# Evaluate if the code compiles\nresult = compile(code)\nif result is True:\n    print(\"Compilation successful\")\nelse:\n    print(\"Compilation failed with error:\")\n    print(result[1])", ""]}
{"filename": "examples/evals/basic_eval.py", "chunked_list": ["import os\nfrom log10.anthropic import Anthropic\nfrom log10.evals import eval\nfrom log10.openai import OpenAI\n\n# Choose provider\nprovider = \"openai\"  # \"anthropic\"\n\n# TODO: Replace with LLM abstraction.\nllm = None\nif provider == \"openai\":\n    llm = OpenAI(\n        {\n            \"model\": \"gpt-3.5-turbo\",\n            \"temperature\": 0,\n            \"max_tokens\": 1024,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n        }\n    )\nelif provider == \"anthropic\":\n    llm = Anthropic(\n        {\n            \"model\": \"claude-1\",\n            \"temperature\": 0,\n            \"max_tokens_to_sample\": 1024,\n        }\n    )\nelse:\n    print(\n        f\"Unsupported provider option: {provider}. Supported providers are 'openai' or 'anthropic'.\"\n    )", "# TODO: Replace with LLM abstraction.\nllm = None\nif provider == \"openai\":\n    llm = OpenAI(\n        {\n            \"model\": \"gpt-3.5-turbo\",\n            \"temperature\": 0,\n            \"max_tokens\": 1024,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n        }\n    )\nelif provider == \"anthropic\":\n    llm = Anthropic(\n        {\n            \"model\": \"claude-1\",\n            \"temperature\": 0,\n            \"max_tokens_to_sample\": 1024,\n        }\n    )\nelse:\n    print(\n        f\"Unsupported provider option: {provider}. Supported providers are 'openai' or 'anthropic'.\"\n    )", "\n# Ground truth dataset to use for evaluation\neval_dataset = (\n    \"match_data.csv\",\n    {\"input\": \"my_input_column\", \"ideal\": \"my_output_column\"},\n)\n\n# Specify which metrics to use. Options are:\n# 'match': model_output.startswith(ideal)\n# 'includes': ideal.lower() in model_output.lower()", "# 'match': model_output.startswith(ideal)\n# 'includes': ideal.lower() in model_output.lower()\n# 'fuzzy_match': similar to includes but remove punctuation, articles and extra whitespace and compare both ways\neval_metric = \"includes\"\n\n# Path to output file to store the metrics\n# Example from: https://github.com/openai/evals/blob/a24f20a357ecb3cc5eec8323097aeade9585796c/evals/elsuite/test/match.py\nout_file_path = \"match_output.csv\"\n\n# Get back and id and url for the summary of results and status", "\n# Get back and id and url for the summary of results and status\n# todo: get back path to logfile; eval_id, eval_url =\neval(llm, eval_dataset, eval_metric, out_file_path)\n"]}
