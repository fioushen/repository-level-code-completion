{"filename": "langchain/indexify_langchain/__init__.py", "chunked_list": ["from indexify_langchain.memory.indexify import IndexifyMemory\nfrom indexify import DEFAULT_INDEXIFY_URL\n\n__all__ = [\"IndexifyMemory\", \"DEFAULT_INDEXIFY_URL\"]"]}
{"filename": "langchain/indexify_langchain/memory/indexify.py", "chunked_list": ["from typing import Any, Dict, List\nfrom langchain.memory.chat_memory import BaseChatMemory\n\nfrom indexify.data_containers import Message\nfrom indexify.memory import Memory\n\n'''\nThis class will initialize the Indexify class with the indexify_url your installation\n\nExample:", "\nExample:\nmemory = IndexifyMemory(indexify_url=\"http://10.0.0.1:8900/\")\n'''\n\n\nclass IndexifyMemory(BaseChatMemory):\n    human_prefix: str = \"Human\"\n    ai_prefix: str = \"AI\"\n    memory_key: str = \"history\"\n    indexify_url: str = \"http://localhost:8900\"\n    indexify_index_name: str = \"default\"\n    memory: Memory = None\n    init: bool = False\n\n    def __init__(self,\n                 human_prefix: str = \"Human\",\n                 ai_prefix: str = \"AI\",\n                 memory_key: str = \"history\",\n                 indexify_url: str = \"http://localhost:8900\",\n                 indexify_index_name: str = \"default\",\n                 **kwargs: Any):\n        super().__init__(**kwargs)\n        self.human_prefix = human_prefix\n        self.ai_prefix = ai_prefix\n        self.memory_key = memory_key\n        self.indexify_url = indexify_url\n        self.indexify_index_name = indexify_index_name\n        self.memory: Memory = Memory(self.indexify_url, self.indexify_index_name)\n\n    @property\n    def memory_variables(self) -> List[str]:\n        return [self.memory_key]\n\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n        self.may_init()\n        input_str, output_str = self._get_input_output(inputs, outputs)\n        self.memory.add(Message(self.human_prefix, input_str))\n        self.memory.add(Message(self.ai_prefix, output_str))\n\n    def load_memory_variables(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n        self.may_init()\n        all_messages = \"\"\n        for message in self.memory.all():\n            all_messages += f\"{message.role}: {message.text}\\n\"\n        return {self.memory_key: all_messages}\n\n    def clear(self) -> None:\n        # Recreate the memory\n        self.memory.create()\n\n    def may_init(self) -> None:\n        if not self.init:\n            self.memory.create()\n            self.init = True", ""]}
{"filename": "langchain/indexify_langchain/memory/__init__.py", "chunked_list": [""]}
{"filename": "langchain/tests/__init__.py", "chunked_list": [""]}
{"filename": "langchain/examples/agent_memory.py", "chunked_list": ["from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, LLMChain\nfrom langchain.utilities import GoogleSearchAPIWrapper\n\nfrom indexify_langchain import IndexifyMemory, DEFAULT_INDEXIFY_URL\n\n'''\nFor this script to work we need to set the following environment settings.\nexport GOOGLE_API_KEY=\"\"\nexport GOOGLE_CSE_ID=\"\"", "export GOOGLE_API_KEY=\"\"\nexport GOOGLE_CSE_ID=\"\"\nexport OPENAI_API_KEY=\"\"\n\nYou can get the Google's CSE and API key from following URL's:\nhttps://programmablesearchengine.google.com/controlpanel/create\nhttps://console.cloud.google.com/projectselector2/google/maps-apis/credentials\n\nOpenAI API Key from:\nhttps://platform.openai.com/account/api-keys", "OpenAI API Key from:\nhttps://platform.openai.com/account/api-keys\n\nNote: This example is a modification from langchain documentation here:\nhttps://python.langchain.com/docs/modules/memory/how_to/agent_with_memory\n'''\n\n# Single line Magic to add vectorized memory!\nmemory = IndexifyMemory(memory_key=\"chat_history\", indexify_url=DEFAULT_INDEXIFY_URL)\n", "memory = IndexifyMemory(memory_key=\"chat_history\", indexify_url=DEFAULT_INDEXIFY_URL)\n\n# Initialize Google search for the langchain.\nsearch = GoogleSearchAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    )", "        description=\"useful for when you need to answer questions about current events\",\n    )\n]\n\nprefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\nsuffix = \"\"\"Begin!\"\n\n{chat_history}\nQuestion: {input}\n{agent_scratchpad}\"\"\"", "Question: {input}\n{agent_scratchpad}\"\"\"\n\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n)\n", ")\n\nllm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)\nagent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\nagent_chain = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True, memory=memory\n)\n\n\ndef ask(question):\n    print(f\"User question: {question}\")\n    agent_chain.run(input=question)", "\ndef ask(question):\n    print(f\"User question: {question}\")\n    agent_chain.run(input=question)\n\n\n# Ask your questions off. Helper function just prints the question before starting the chain!\nask(\"How many people live in USA?\")\nask(\"what is their national anthem called?\")\nask(\"How many per state?\")", "ask(\"what is their national anthem called?\")\nask(\"How many per state?\")\nask(\"How many lines is the song?\")\n"]}
{"filename": "sdk-py/indexify/data_containers.py", "chunked_list": ["from enum import Enum\nfrom typing import List\nfrom dataclasses import dataclass, field\n\n\nclass TextSplitter(str, Enum):\n    NEWLINE = \"new_line\"\n    REGEX = \"regex\"\n    NOOP = \"noop\"\n\n    def __str__(self) -> str:\n        return self.value.lower()", "\n\n@dataclass\nclass TextChunk:\n    text: str\n    metadata: dict[str, any] = field(default_factory=dict)\n\n    def to_dict(self):\n        return {\"text\": self.text, \"metadata\": self.metadata}\n", "\n\n@dataclass\nclass Message:\n    role: str\n    text: str\n    metadata: dict[str, any] = field(default_factory=dict)\n\n    def to_dict(self):\n        return {\"role\": self.role, \"text\": self.text, \"metadata\": self.metadata}", "\n\n@dataclass\nclass SearchChunk:\n    index: str\n    query: str\n    k: int\n\n    def to_dict(self):\n        return {\"index\": self.index, \"query\": self.query, \"k\": self.k}", "\n\n@dataclass\nclass SearchResult:\n    results: List[TextChunk]\n"]}
{"filename": "sdk-py/indexify/__init__.py", "chunked_list": ["from .index import Index, AIndex\nfrom .memory import Memory, AMemory\nfrom .repository import Repository, ARepository\nfrom .data_containers import TextChunk, Message\nfrom .utils import wait_until\n\nDEFAULT_INDEXIFY_URL = \"http://localhost:8900\"\n\n__all__ = [\"Index\", \"Memory\", \"Repository\", \"AIndex\", \"AMemory\", \"ARepository\",\n           \"Message\", \"TextChunk\", \"DEFAULT_INDEXIFY_URL\", \"wait_until\", \"IndexifyMemory\"]", "__all__ = [\"Index\", \"Memory\", \"Repository\", \"AIndex\", \"AMemory\", \"ARepository\",\n           \"Message\", \"TextChunk\", \"DEFAULT_INDEXIFY_URL\", \"wait_until\", \"IndexifyMemory\"]\n"]}
{"filename": "sdk-py/indexify/utils.py", "chunked_list": ["import asyncio\nfrom enum import Enum\nimport json\n\n\nclass ApiException(Exception):\n    def __init__(self, message: str) -> None:\n        super().__init__(message)\n\n\nclass Metric(str, Enum):\n    COSINE = \"cosine\"\n    DOT = \"dot\"\n    EUCLIDEAN = \"euclidean\"\n\n    def __str__(self) -> str:\n        return self.name.lower()", "\n\nclass Metric(str, Enum):\n    COSINE = \"cosine\"\n    DOT = \"dot\"\n    EUCLIDEAN = \"euclidean\"\n\n    def __str__(self) -> str:\n        return self.name.lower()\n", "\n\nasync def _get_payload(response):\n    try:\n        resp = await response.text()\n        return json.loads(resp)\n    except:\n        raise ApiException(resp)\n\n\ndef wait_until(functions):\n    single_result = False\n    if not isinstance(functions, list):\n        single_result = True\n        functions = [functions]\n    holder = []\n\n    async def run_and_capture_result():\n        holder.append(await asyncio.gather(*functions))\n\n    asyncio.run(run_and_capture_result())\n    if single_result:\n        return holder[0][0]  # single result\n    else:\n        return holder[0]  # list of results", "\n\ndef wait_until(functions):\n    single_result = False\n    if not isinstance(functions, list):\n        single_result = True\n        functions = [functions]\n    holder = []\n\n    async def run_and_capture_result():\n        holder.append(await asyncio.gather(*functions))\n\n    asyncio.run(run_and_capture_result())\n    if single_result:\n        return holder[0][0]  # single result\n    else:\n        return holder[0]  # list of results", ""]}
{"filename": "sdk-py/indexify/memory.py", "chunked_list": ["import aiohttp\n\nfrom .data_containers import *\nfrom .utils import _get_payload, wait_until\n\n\nclass AMemory:\n\n    def __init__(self, url, repository=\"default\"):\n        self._session_id = None\n        self._url = url\n        self._repo = repository\n\n    async def create(self) -> str:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(f\"{self._url}/memory/create\", json={\"repository\": self._repo}) as resp:\n                resp = await _get_payload(resp)\n                self._session_id = resp[\"session_id\"]\n        return self._session_id\n\n    async def add(self, *messages: Message) -> None:\n        parsed_messages = []\n        for message in messages:\n            parsed_messages.append(message.to_dict())\n\n        req = {\"session_id\": self._session_id, \"repository\": self._repo, \"messages\": parsed_messages}\n        async with aiohttp.ClientSession() as session:\n            async with session.post(f\"{self._url}/memory/add\", json=req) as resp:\n                return await _get_payload(resp)\n\n    async def all(self) -> list[Message]:\n        req = {\"session_id\": self._session_id, \"repository\": self._repo}\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f\"{self._url}/memory/get\", json=req) as resp:\n                payload = await _get_payload(resp)\n                messages = []\n                for raw_message in payload[\"messages\"]:\n                    messages.append(Message(raw_message[\"role\"], raw_message[\"text\"], raw_message[\"metadata\"]))\n                return messages", "\n\nclass Memory(AMemory):\n    def __init__(self, url, repository=\"default\"):\n        AMemory.__init__(self, url, repository)\n\n    def create(self) -> str:\n        return wait_until(AMemory.create(self))\n\n    def add(self, *messages: Message) -> None:\n        wait_until(AMemory.add(self, *messages))\n\n    def all(self) -> list[Message]:\n        return wait_until(AMemory.all(self))", ""]}
{"filename": "sdk-py/indexify/repository.py", "chunked_list": ["import aiohttp\n\nfrom .data_containers import *\nfrom .utils import _get_payload, wait_until\n\n\nclass ARepository:\n\n    def __init__(self, url: str, name: str = \"default\"):\n        self._url = url\n        self._name = name\n\n    async def run_extractors(self, repository: str = \"default\") -> dict:\n        req = {\"repository\": repository}\n        async with aiohttp.ClientSession() as session:\n            async with session.post(f\"{self._url}/repository/run_extractors\", json=req) as resp:\n                return await _get_payload(resp)\n\n    async def add(self, *chunks: TextChunk) -> None:\n        parsed_chunks = []\n        for chunk in chunks:\n            parsed_chunks.append(chunk.to_dict())\n        req = {\"documents\": parsed_chunks, \"repository\": self._name}\n        async with aiohttp.ClientSession() as session:\n            async with session.post(f\"{self._url}/repository/add_texts\", json=req) as resp:\n                return await _get_payload(resp)", "\n\nclass Repository(ARepository):\n\n    def __init__(self, url, name):\n        ARepository.__init__(self, url, name)\n\n    def add(self, *chunks: TextChunk) -> None:\n        return wait_until(ARepository.add(self, *chunks))\n    \n    def run_extractors(self, repository: str = \"default\") -> dict:\n        return wait_until(ARepository.run_extractors(self, repository))", ""]}
{"filename": "sdk-py/indexify/index.py", "chunked_list": ["import aiohttp\n\nfrom .data_containers import *\nfrom .utils import _get_payload, wait_until\n\n\nclass AIndex:\n\n    def __init__(self, url: str, index: str = \"default/default\"):\n        self._url = url\n        self._index = index\n\n    async def search(self, query: str, top_k: int) -> list[TextChunk]:\n        req = SearchChunk(index=self._index, query=query, k=top_k)\n        async with aiohttp.ClientSession() as session:\n            async with session.get(f\"{self._url}/index/search\", json=req.to_dict()) as resp:\n                payload = await _get_payload(resp)\n                result = []\n                for res in payload[\"results\"]:\n                    result.append(TextChunk(text=res[\"text\"], metadata=res[\"metadata\"]))\n                return result", "\n\nclass Index(AIndex):\n\n    def __init__(self, url, index):\n        AIndex.__init__(self, url, index)\n\n    def search(self, query: str, top_k: int) -> list[TextChunk]:\n        wait_until(AIndex.search(self, query, top_k))\n", "\n"]}
{"filename": "sdk-py/examples/memory_example.py", "chunked_list": ["from indexify import Memory, Message, DEFAULT_INDEXIFY_URL\n\n\nclass DemoMemoryExample:\n    def __init__(self):\n        self._memory = Memory(DEFAULT_INDEXIFY_URL, \"default\")\n\n    def execute(self):\n        print(\"Running memory example...\")\n        # Create a memory session\n        session = self._memory.create()\n        # Add to the vector and persistence memory\n        self._memory.add(Message(\"human\", \"Indexify is amazing!\"),\n                         Message(\"assistant\", \"How are you planning on using Indexify?!\"))\n        # Get all the memory events for a given session.\n        response = self._memory.all()\n        print([message.to_dict() for message in response])", "\n\nif __name__ == '__main__':\n    demo = DemoMemoryExample()\n    demo.execute()\n"]}
{"filename": "sdk-py/examples/question_answer_demo.py", "chunked_list": ["from indexify import AIndex, ARepository, TextChunk, DEFAULT_INDEXIFY_URL, wait_until\nfrom datasets import load_dataset\n\n\nclass DemoQA:\n\n    def __init__(self):\n        self.repository = ARepository(DEFAULT_INDEXIFY_URL, \"default\")\n        self.idx = AIndex(DEFAULT_INDEXIFY_URL, \"default/default\")\n\n    def execute(self):\n        # Add All Wikipedia articles\n        datasets = load_dataset('squad', split='train')\n        q_a_all = []\n        futures = []\n        print(\"Running QA example...\")\n        print(\"Adding all Wikipedia articles to the index...\")\n        for i in range(0, 10):\n            context: str = datasets[i][\"context\"]\n            question = datasets[i][\"question\"]\n            answers = datasets[i][\"answers\"]\n            futures.append(self.repository.add(TextChunk(context)))\n            q_a_all.append((question, answers))\n        wait_until(futures)\n        print(\"Running extractors now... workaround for concurrency issues\")\n        resp = wait_until(self.repository.run_extractors(\"default\"))\n        print(f\"number of extracted entities: {resp}\")\n        print(\"Starting search now...\")\n        for q_a in q_a_all:\n            question = q_a[0]\n            values = wait_until(self.idx.search(question, 1))\n            print(f\"Question: {question}, \\nContext / Answer can be found in: {values[0].text}\")", "\n\nif __name__ == '__main__':\n    demo = DemoQA()\n    demo.execute()\n"]}
{"filename": "sdk-py/examples/benchmark.py", "chunked_list": ["import argparse\nimport time\n\nfrom indexify import AIndex, ARepository, AMemory, TextChunk, DEFAULT_INDEXIFY_URL, wait_until, Message\nfrom datasets import load_dataset\n\n\nclass BulkUploadRepository:\n\n    def __init__(self, url):\n        self.url = url\n        self.repository = ARepository(url)\n        self.idx = AIndex(url)\n\n    def benchmark_repository(self, concurrency: int, loop: int):\n        datasets = load_dataset('squad', split='train')\n        for j in range(0, loop):\n            start_time = time.time()\n            futures = []\n            for i in range(0, concurrency):\n                context: str = datasets[i][\"context\"]\n                futures.append(self.repository.add(TextChunk(context)))\n            wait_until(futures)\n            print(f\"repository.add seconds: {(time.time() - start_time)}\")\n\n        print(\"Running extractors now... workaround for concurrency issues\")\n        resp = wait_until(self.repository.run_extractors(\"default\"))\n        print(f\"number of extracted entities: {resp}\")\n\n        for j in range(0, loop):\n            start_time = time.time()\n            futures = []\n            for i in range(0, concurrency):\n                question = datasets[i][\"question\"]\n                futures.append(self.idx.search(question, 1))\n            wait_until(futures)\n            print(f\"repository.search seconds: {(time.time() - start_time)}\")\n\n    def benchmark_memory(self, concurrency: int, loop: int):\n        for j in range(0, loop):\n            start_time = time.time()\n            memory_list = []\n            for i in range(0, concurrency):\n                memory_list.append(AMemory(self.url))\n            futures_create = []\n            for i in range(0, concurrency):\n                futures_create.append(memory_list[i].create())\n            wait_until(futures_create)\n            print(f\"memory.create seconds: {(time.time() - start_time)}\")\n            futures_add = []\n            for i in range(0, concurrency):\n                futures_add.append(memory_list[i].add(Message(\"human\", \"Indexify is amazing!\"),\n                                                      Message(\"assistant\", \"How are you planning on using Indexify?!\")))\n            wait_until(futures_add)\n            print(f\"memory.add seconds: {(time.time() - start_time)}\")\n            futures_all = []\n            for i in range(0, concurrency):\n                futures_all.append(self._memory.all())\n            wait_until(futures_all)\n            print(f\"memory.all seconds: {(time.time() - start_time)}\")", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-u\", \"--url\", default=DEFAULT_INDEXIFY_URL, help=\"indexify url to connect to\")\n    parser.add_argument(\"-c\", \"--concurrency\", default=10,\n                        help=\"We deal with Async functions its how often we wait\")\n    parser.add_argument(\"-l\", \"--loop\", default=100,\n                        help=\"Number of loops with concurrency set as per -c\")\n    args = parser.parse_args()\n    print(f\"looping for {args.loop}, with concurrency is set to be: {args.concurrency}, \"\n          f\"will run {args.loop * args.concurrency} requests\")\n    demo = BulkUploadRepository(args.url)\n    demo.benchmark_repository(args.concurrency, args.loop)\n    demo.benchmark_memory(args.concurrency, args.loop)", ""]}
{"filename": "src_py/tests/test_st_embeddings.py", "chunked_list": ["import unittest\nfrom indexify_py.sentence_transformer import SentenceTransformersEmbedding\nfrom indexify_py.dpr import DPREmbeddings\n\n\nclass TestSTEmbeddings(unittest.TestCase):\n\n    def __init__(self, *args, **kwargs):\n        super(TestSTEmbeddings, self).__init__(*args, **kwargs)\n\n\n    def test_query_embedding(self):\n        st = SentenceTransformersEmbedding(\"all-MiniLM-L6-v2\")\n        embeddings = st.embed_query(\"hello world\")\n        self.assertEqual(len(embeddings), 384)\n\n    def test_ctx_embeddings(self):\n        st = SentenceTransformersEmbedding(\"all-MiniLM-L6-v2\")\n        embeddings = st.embed_ctx([\"hello\", \"world\"])\n        self.assertEqual(len(embeddings), 2)\n        self.assertEqual(len(embeddings[0]), 384)\n\n    def test_tokenize(self):\n        st = SentenceTransformersEmbedding(\"all-MiniLM-L6-v2\")\n        chunks = st.tokenize([\"hello\", \"world hi\"])\n        self.assertEqual(len(chunks), 2)\n        #self.assertEqual(len(chunks[0]), 1)\n        self.assertEqual(chunks[1], \"world hi\")\n\n    def test_token_decode_encode(self):\n        st = SentenceTransformersEmbedding(\"all-MiniLM-L6-v2\")\n        tokens = st.tokenizer_encode([\"hello\", \"world hi\"])\n        texts = st.tokenizer_decode(tokens)\n        self.assertEqual(len(texts), 2)\n        self.assertAlmostEqual(texts[0], \"hello\")\n        self.assertAlmostEqual(texts[1], \"world hi\")", " \n\n\nclass TestDPR(unittest.TestCase):\n\n    def __init__(self, *args, **kwargs):\n        super(TestDPR, self).__init__(*args, **kwargs)\n\n    def test_query_embedding(self):\n        st = DPREmbeddings(\"facebook-dpr-ctx_encoder-multiset-base\", \"facebook-dpr-question_encoder-multiset-base\")\n        embeddings = st.embed_query(\"hello world\")\n        self.assertEqual(len(embeddings), 768)\n\n    def test_ctx_embeddings(self):\n        st = DPREmbeddings(\"facebook-dpr-ctx_encoder-multiset-base\", \"facebook-dpr-question_encoder-multiset-base\")\n        embeddings = st.embed_ctx([\"hello\", \"world\"])\n        self.assertEqual(len(embeddings), 2)\n        self.assertEqual(len(embeddings[0]), 768)\n\n    def test_token_decode_encode(self):\n        st = DPREmbeddings(\"facebook-dpr-ctx_encoder-multiset-base\", \"facebook-dpr-question_encoder-multiset-base\")\n        tokens = st.tokenizer_encode([\"hello\", \"world hi\"])\n        texts = st.tokenizer_decode(tokens)\n        self.assertEqual(len(texts), 2)\n        self.assertAlmostEqual(texts[0], \"hello\")\n        self.assertAlmostEqual(texts[1], \"world hi\")\n\n    def test_tokenize(self):\n        st = DPREmbeddings(\"facebook-dpr-ctx_encoder-multiset-base\", \"facebook-dpr-question_encoder-multiset-base\")\n        chunks = st.tokenize([\"hello\", \"world hi\"])\n        self.assertEqual(len(chunks), 2)\n        #self.assertEqual(len(chunks[0]), 1)\n        self.assertEqual(chunks[1], \"world hi\")", "\nif __name__ == \"__main__\":\n    unittest.main()"]}
{"filename": "src_py/tests/test_dpr.py", "chunked_list": ["import unittest\nfrom indexify_py.dpr_onnx import OnnxDPR\n\n\nclass TestDPREmbeddings(unittest.TestCase):\n\n    def __init__(self, *args, **kwargs):\n        super(TestDPREmbeddings, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def setUpClass(cls):\n        cls._dpr = OnnxDPR()\n\n    def test_embedding_query(self):\n        query = \"What is the capital of France?\"\n        embeddings = self._dpr.generate_embeddings_query(query)\n        self.assertEqual(len(embeddings), 768)\n\n    def test_embedding_context(self):\n        context = [\"Paris is the capital of France.\", \"London is the capital of England.\"]\n        embeddings = self._dpr.generate_embeddings_ctx(context)\n        self.assertEqual(len(embeddings), 2)\n        self.assertEqual(len(embeddings[0]), 768)", "\n\nif __name__ == \"__main__\":\n    unittest.main()\n"]}
{"filename": "src_py/tests/test_entity_extractor.py", "chunked_list": ["import unittest\nfrom src_py.indexify_py.entity_extractor import EntityExtractor\n\n\nclass TestEntityExtractor(unittest.TestCase):\n\n    def __init__(self, *args, **kwargs):\n        super(TestEntityExtractor, self).__init__(*args, **kwargs)\n\n    @classmethod\n    def setUpClass(cls):\n        cls._entityextractor = EntityExtractor()\n        cls._otherextractor = EntityExtractor(model_name=\"dslim/bert-large-NER\")\n\n    def test_extractor(self):\n        input = \"My name is Wolfgang and I live in Berlin\"\n        entities = self._entityextractor.extract(input)\n        print(entities)\n        self.assertEqual(len(entities), 2)\n\n    def test_other_extractor(self):\n        input = \"My name is Wolfgang and I live in Berlin\"\n        entities = self._otherextractor.extract(input)\n        print(entities)\n        self.assertEqual(len(entities), 2)", "\nif __name__ == \"__main__\":\n    unittest.main()"]}
{"filename": "src_py/tests/__init__.py", "chunked_list": [""]}
{"filename": "src_py/indexify_py/sentence_transformer.py", "chunked_list": ["from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\nfrom typing import List\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)", "\n\nclass SentenceTransformersEmbedding:\n\n    def __init__(self, model_name) -> None:\n        self._model_name = model_name\n        self._tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{model_name}')\n        self._model = AutoModel.from_pretrained(f'sentence-transformers/{model_name}')\n\n    def embed_ctx(self, inputs: List[str])  -> List[List[float]]:\n        result = self._embed(inputs)\n        return result.tolist()\n\n    def embed_query(self, query: str) -> List[float]:\n       result = self._embed([query])\n       return result[0].tolist()\n    \n    def _embed(self, inputs: List[str]) -> torch.Tensor:\n        encoded_input = self._tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            model_output = self._model(**encoded_input)\n        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n        return F.normalize(sentence_embeddings, p=2, dim=1)\n    \n    def tokenizer_encode(self, inputs: List[str]) -> List[List[int]]:\n        return self._tokenizer.batch_encode_plus(inputs)['input_ids']\n\n    def tokenizer_decode(self, tokens: List[List[int]]) -> List[str]:\n        return self._tokenizer.batch_decode(tokens, skip_special_tokens=True)\n\n    def tokenize(self, inputs: List[str]) -> List[List[str]]:\n        result = []\n        for input in inputs:\n            result.extend(self._tokenize(input))\n        return result\n\n    def _tokenize(self, input: str) -> List[str]:\n        max_length = self._tokenizer.model_max_length\n        chunks = []\n        chunk = []\n        chunk_length = 0\n        words = input.split(' ')\n\n        for word in words:\n            tokens = self._tokenizer.tokenize(word)\n            token_length = len(tokens)\n\n            if chunk_length + token_length <= max_length:\n                chunk.append(word)\n                chunk_length += token_length\n            else:\n                chunks.append(' '.join(chunk))\n                chunk = [word]\n                chunk_length = token_length\n\n        chunks.append(' '.join(chunk))\n        return chunks"]}
{"filename": "src_py/indexify_py/dpr_onnx.py", "chunked_list": ["import torch\nfrom optimum.onnxruntime import ORTModelForFeatureExtraction\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, Pipeline\nfrom pathlib import Path\nfrom typing import List\n\n\ndef cls_pooling(model_output, attention_mask):\n    return model_output[0][:, 0]", "def cls_pooling(model_output, attention_mask):\n    return model_output[0][:, 0]\n\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[\n        0\n    ]  # First element of model_output contains all token embeddings\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask", "\nclass SentenceEmbeddingPipeline(Pipeline):\n    def _sanitize_parameters(self, **kwargs):\n        # we don't have any hyperameters to sanitize\n        preprocess_kwargs = {}\n        return preprocess_kwargs, {}, {}\n\n    def preprocess(self, inputs):\n        encoded_inputs = self.tokenizer(\n            inputs, padding=True, truncation=True, return_tensors=\"pt\"\n        )\n        return encoded_inputs\n\n    def _forward(self, model_inputs):\n        outputs = self.model(**model_inputs)\n        return {\"outputs\": outputs, \"attention_mask\": model_inputs[\"attention_mask\"]}\n\n    def postprocess(self, model_outputs):\n        # Perform pooling\n        sentence_embeddings = mean_pooling(\n            model_outputs[\"outputs\"], model_outputs[\"attention_mask\"]\n        )\n        # Normalize embeddings\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n        return sentence_embeddings.squeeze().tolist()", "\n\n\nclass OnnxDPR:\n    def __init__(\n        self,\n        ctx_model: str = \"sentence-transformers/facebook-dpr-ctx_encoder-multiset-base\",\n        ques_model: str = \"sentence-transformers/facebook-dpr-question_encoder-multiset-base\",\n    ):\n        self._ctx_model = ORTModelForFeatureExtraction.from_pretrained(ctx_model, export=True)\n        self._ques_model = ORTModelForFeatureExtraction.from_pretrained(ques_model, export=True)\n        self._tokenizer_ctx_model = AutoTokenizer.from_pretrained(ctx_model)\n        self._tokenizer_ques_model = AutoTokenizer.from_pretrained(ques_model)\n        self._ctx_pipeline = SentenceEmbeddingPipeline(self._ctx_model, self._tokenizer_ctx_model)\n        self._ques_pipeline = SentenceEmbeddingPipeline(self._ques_model, self._tokenizer_ques_model)\n\n    def generate_embeddings_ctx(self, inputs: List[str]):\n        embeddings = self._ctx_pipeline(inputs)\n        return embeddings\n\n    def generate_embeddings_query(self, query: str):\n        embeddings = self._ques_pipeline(query)\n        return embeddings", "    \n\n\n## TODO Move this to proper unit tests later\nif __name__ == \"__main__\":\n    dpr = OnnxDPR()\n    print(dpr.generate_embeddings_ctx([\"What is the capital of England?\"]))\n    print(dpr.generate_embeddings_query(\"What is the capital of England?\"))\n\n", "\n\n"]}
{"filename": "src_py/indexify_py/entity_extractor.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import List\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\nfrom decimal import Decimal \nfrom enum import Enum\nfrom typing import Optional\n\n\n@dataclass\nclass EntityAttributes:\n    score: Decimal ", "\n@dataclass\nclass EntityAttributes:\n    score: Decimal \n\n@dataclass\nclass Entity:\n   name: str\n   value: str\n   attributes: EntityAttributes", "\n\nclass EntityType(Enum): \n    def get_entity_type(type: str) -> str:\n        entity_type_map = {\n            \"B-PER\": \"Person\",\n            \"B-ORG\": \"Organization\",\n            \"I-ORG\": \"Organization\",\n            \"B-LOC\": \"Location\",\n            \"I-LOC\": \"Location\"\n         }   \n        return entity_type_map.get(type, \"INVALID\")", "    \n\nclass EntityExtractor:\n   \n    def __init__(self, model_name: str = \"dslim/bert-base-NER\"):\n        self.model_name = model_name\n        # Load model - https://huggingface.co/dslim/bert-base-NER\n        self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self._model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n        self._ctx_pipeline = pipeline(\"ner\", model=self._model, tokenizer=self._tokenizer)\n\n        \n    def extract(self, text: str) -> List[Entity]:\n         ner_list = self._ctx_pipeline(text)\n         entities = []\n\n         for ner in ner_list:\n             name = EntityType.get_entity_type(ner[\"entity\"])\n             value = ner[\"word\"]\n             score = ner[\"score\"]\n             entity = Entity(name=name, value=value, attributes=EntityAttributes(score=score))\n             entities.append(entity)\n         return entities", "\n    "]}
{"filename": "src_py/indexify_py/__init__.py", "chunked_list": [""]}
{"filename": "src_py/indexify_py/dpr.py", "chunked_list": ["from transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\n\nfrom typing import List\n\ndef cls_pooling(model_output, attention_mask):\n    return model_output[0][:,0]\n\nclass DPREmbeddings:\n\n    def __init__(self, ctx_model_name=\"facebook-dpr-ctx_encoder-multiset-base\", query_model_name=\"facebook-dpr-question_encoder-multiset-base\") -> None:\n        self._ctx_tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{ctx_model_name}')\n        self._ctx_model = AutoModel.from_pretrained(f'sentence-transformers/{ctx_model_name}')\n        self._query_tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{query_model_name}')\n        self._query_model = AutoModel.from_pretrained(f'sentence-transformers/{query_model_name}')\n\n    def embed_ctx(self, inputs: List[str])  -> List[List[float]]:\n        encoded_input = self._ctx_tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            model_output = self._ctx_model(**encoded_input)\n        sentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n        return F.normalize(sentence_embeddings, p=2, dim=1).tolist()\n\n    def embed_query(self, input: str) -> torch.Tensor:\n        encoded_input = self._query_tokenizer(input, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            model_output = self._query_model(**encoded_input)\n        sentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n        return F.normalize(sentence_embeddings, p=2, dim=1)[0].tolist()\n    \n    def tokenizer_encode(self, inputs: List[str]) -> List[List[int]]:\n        return self._ctx_tokenizer.batch_encode_plus(inputs)['input_ids']\n\n    def tokenizer_decode(self, tokens: List[List[int]]) -> List[str]:\n        return self._ctx_tokenizer.batch_decode(tokens, skip_special_tokens=True)\n\n    def tokenize(self, inputs: List[str]) -> List[List[str]]:\n        result = []\n        for input in inputs:\n            result.extend(self._tokenize(input))\n        return result\n\n    def _tokenize(self, input: str) -> List[str]:\n        max_length = self._ctx_tokenizer.model_max_length\n        chunks = []\n        chunk = []\n        chunk_length = 0\n        words = input.split(' ')\n\n        for word in words:\n            tokens = self._ctx_tokenizer.tokenize(word)\n            token_length = len(tokens)\n\n            if chunk_length + token_length <= max_length:\n                chunk.append(word)\n                chunk_length += token_length\n            else:\n                chunks.append(' '.join(chunk))\n                chunk = [word]\n                chunk_length = token_length\n\n        chunks.append(' '.join(chunk))\n        return chunks", "\nclass DPREmbeddings:\n\n    def __init__(self, ctx_model_name=\"facebook-dpr-ctx_encoder-multiset-base\", query_model_name=\"facebook-dpr-question_encoder-multiset-base\") -> None:\n        self._ctx_tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{ctx_model_name}')\n        self._ctx_model = AutoModel.from_pretrained(f'sentence-transformers/{ctx_model_name}')\n        self._query_tokenizer = AutoTokenizer.from_pretrained(f'sentence-transformers/{query_model_name}')\n        self._query_model = AutoModel.from_pretrained(f'sentence-transformers/{query_model_name}')\n\n    def embed_ctx(self, inputs: List[str])  -> List[List[float]]:\n        encoded_input = self._ctx_tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            model_output = self._ctx_model(**encoded_input)\n        sentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n        return F.normalize(sentence_embeddings, p=2, dim=1).tolist()\n\n    def embed_query(self, input: str) -> torch.Tensor:\n        encoded_input = self._query_tokenizer(input, padding=True, truncation=True, return_tensors='pt')\n        with torch.no_grad():\n            model_output = self._query_model(**encoded_input)\n        sentence_embeddings = cls_pooling(model_output, encoded_input['attention_mask'])\n        return F.normalize(sentence_embeddings, p=2, dim=1)[0].tolist()\n    \n    def tokenizer_encode(self, inputs: List[str]) -> List[List[int]]:\n        return self._ctx_tokenizer.batch_encode_plus(inputs)['input_ids']\n\n    def tokenizer_decode(self, tokens: List[List[int]]) -> List[str]:\n        return self._ctx_tokenizer.batch_decode(tokens, skip_special_tokens=True)\n\n    def tokenize(self, inputs: List[str]) -> List[List[str]]:\n        result = []\n        for input in inputs:\n            result.extend(self._tokenize(input))\n        return result\n\n    def _tokenize(self, input: str) -> List[str]:\n        max_length = self._ctx_tokenizer.model_max_length\n        chunks = []\n        chunk = []\n        chunk_length = 0\n        words = input.split(' ')\n\n        for word in words:\n            tokens = self._ctx_tokenizer.tokenize(word)\n            token_length = len(tokens)\n\n            if chunk_length + token_length <= max_length:\n                chunk.append(word)\n                chunk_length += token_length\n            else:\n                chunks.append(' '.join(chunk))\n                chunk = [word]\n                chunk_length = token_length\n\n        chunks.append(' '.join(chunk))\n        return chunks"]}
{"filename": "src_py/indexify_py/extractor.py", "chunked_list": ["from abc import ABC, abstractmethod\n\nfrom typing import Any\n\nclass Extractor(ABC):\n\n    @abstractmethod\n    def extract(self, content: Any, attrs: dict[str, Any]) -> Any:\n        \"\"\"\n        Extracts information from the content.\n        \"\"\"\n        pass\n\n\n    @abstractmethod\n    def get_name(self) -> str:\n        \"\"\"\n        Returns the name of the extractor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_schema(self) -> str:\n        \"\"\"\n        Returns the schema of the extractor in JSON schema format.\n        It should have the following fields:\n        {\n            \"type\": \"extraction object\",\n            \"description\": \"string\",\n            \"properties\": {\n                \"property_name_1\": \"property_type\",\n                \"property_name_2\": \"property_type\",\n            }\n        }\n        \"\"\"\n        pass"]}
