{"filename": "train.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport os\nimport socket", "import os\nimport socket\nimport time\n\nimport pytorch_lightning as pl\nimport torch\nfrom powerbev.config import get_cfg, get_parser\nfrom powerbev.data import prepare_powerbev_dataloaders\nfrom powerbev.trainer import TrainingModule\nfrom pytorch_lightning.callbacks import ModelCheckpoint", "from powerbev.trainer import TrainingModule\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.plugins import DDPPlugin\n\n\ndef main():\n    args = get_parser().parse_args()\n    cfg = get_cfg(args)\n\n    trainloader, valloader = prepare_powerbev_dataloaders(cfg)\n    model = TrainingModule(cfg.convert_to_dict())\n\n    if cfg.PRETRAINED.LOAD_WEIGHTS:\n        # Load single-image instance segmentation model.\n        pretrained_model_weights = torch.load(\n            cfg.PRETRAINED.PATH , map_location='cpu'\n        )['state_dict']\n\n        model.load_state_dict(pretrained_model_weights, strict=False)\n        print(f'Loaded single-image model weights from {cfg.PRETRAINED.PATH}')\n\n    save_dir = os.path.join(\n        cfg.LOG_DIR, time.strftime('%d%B%Yat%H:%M:%S%Z') + '_' + socket.gethostname() + '_' + cfg.TAG\n    ) \n    tb_logger = pl.loggers.TensorBoardLogger(save_dir=save_dir)\n    checkpoint_callback = ModelCheckpoint(monitor='vpq', save_top_k=5, mode='max')\n    trainer = pl.Trainer(\n        gpus=cfg.GPUS,\n        accelerator='ddp',\n        precision=cfg.PRECISION,\n        sync_batchnorm=True,\n        gradient_clip_val=cfg.GRAD_NORM_CLIP,\n        max_epochs=cfg.EPOCHS,\n        weights_summary='full',\n        logger=tb_logger,\n        log_every_n_steps=cfg.LOGGING_INTERVAL,\n        plugins=DDPPlugin(find_unused_parameters=True),\n        profiler='simple',\n        callbacks=[checkpoint_callback],\n    )\n    trainer.fit(model, trainloader, valloader)", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "visualise.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport os\nfrom argparse import ArgumentParser", "import os\nfrom argparse import ArgumentParser\nfrom glob import glob\n\nimport cv2\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision", "import torch\nimport torchvision\nfrom PIL import Image\nfrom powerbev.config import get_cfg, get_parser\nfrom powerbev.data import prepare_powerbev_dataloaders\nfrom powerbev.trainer import TrainingModule\nfrom powerbev.utils.instance import predict_instance_segmentation\nfrom powerbev.utils.network import NormalizeInverse\nfrom powerbev.utils.visualisation import (convert_figure_numpy,\n                                          generate_instance_colours,", "from powerbev.utils.visualisation import (convert_figure_numpy,\n                                          generate_instance_colours,\n                                          make_contour, plot_instance_map)\n\n\ndef plot_prediction(image, output, cfg):\n    # Process predictions\n    consistent_instance_seg, matched_centers = predict_instance_segmentation(\n        output, compute_matched_centers=True, spatial_extent=(cfg.LIFT.X_BOUND[1], cfg.LIFT.Y_BOUND[1])\n    )\n    first_instance_seg = consistent_instance_seg[0, 1]\n\n    # Plot future trajectories\n    unique_ids = torch.unique(first_instance_seg).cpu().long().numpy()[1:]\n    instance_map = dict(zip(unique_ids, unique_ids))\n    instance_colours = generate_instance_colours(instance_map)\n    vis_image = plot_instance_map(first_instance_seg.cpu().numpy(), instance_map)\n    trajectory_img = np.zeros(vis_image.shape, dtype=np.uint8)\n    for instance_id in unique_ids:\n        path = matched_centers[instance_id]\n        for t in range(len(path) - 1):\n            color = instance_colours[instance_id].tolist()\n            cv2.line(trajectory_img, tuple(path[t]), tuple(path[t + 1]),\n                     color, 4)\n\n    # Overlay arrows\n    temp_img = cv2.addWeighted(vis_image, 0.7, trajectory_img, 0.3, 1.0)\n    mask = ~ np.all(trajectory_img == 0, axis=2)\n    vis_image[mask] = temp_img[mask]\n\n    # Plot present RGB frames and predictions\n    val_w = 2.99\n    cameras = cfg.IMAGE.NAMES\n    image_ratio = cfg.IMAGE.FINAL_DIM[0] / cfg.IMAGE.FINAL_DIM[1]\n    val_h = val_w * image_ratio\n    fig = plt.figure(figsize=(4 * val_w, 2 * val_h))\n    width_ratios = (val_w, val_w, val_w, val_w)\n    gs = mpl.gridspec.GridSpec(2, 4, width_ratios=width_ratios)\n    gs.update(wspace=0.0, hspace=0.0, left=0.0, right=1.0, top=1.0, bottom=0.0)\n\n    denormalise_img = torchvision.transforms.Compose(\n        (NormalizeInverse(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n         torchvision.transforms.ToPILImage(),)\n    )\n    for imgi, img in enumerate(image[0, -1]):\n        ax = plt.subplot(gs[imgi // 3, imgi % 3])\n        showimg = denormalise_img(img.cpu())\n        if imgi > 2:\n            showimg = showimg.transpose(Image.FLIP_LEFT_RIGHT)\n\n        plt.annotate(cameras[imgi].replace('_', ' ').replace('CAM ', ''), (0.01, 0.87), c='white',\n                     xycoords='axes fraction', fontsize=14)\n        plt.imshow(showimg)\n        plt.axis('off')\n\n    ax = plt.subplot(gs[:, 3])\n    plt.imshow(make_contour(vis_image[::-1, ::-1]))\n    plt.axis('off')\n\n    plt.draw()\n    figure_numpy = convert_figure_numpy(fig)\n    plt.close()\n    return figure_numpy", "    \n\ndef visualise():\n    args = get_parser().parse_args()\n    cfg = get_cfg(args)\n\n    _, valloader = prepare_powerbev_dataloaders(cfg)\n\n    trainer = TrainingModule(cfg.convert_to_dict())\n\n    if cfg.PRETRAINED.LOAD_WEIGHTS:\n        # Load single-image instance segmentation model.\n        weights_path = cfg.PRETRAINED.PATH\n        pretrained_model_weights = torch.load(\n            weights_path , map_location='cpu'\n        )['state_dict']\n\n        trainer.load_state_dict(pretrained_model_weights, strict=False)\n        print(f'Loaded single-image model weights from {weights_path}')\n\n    device = torch.device('cuda:0')\n    trainer = trainer.to(device)\n    trainer.eval()\n\n    for i, batch in enumerate(valloader):\n        image = batch['image'].to(device)\n        intrinsics = batch['intrinsics'].to(device)\n        extrinsics = batch['extrinsics'].to(device)\n        future_egomotions = batch['future_egomotion'].to(device)\n\n        # Forward pass\n        with torch.no_grad():\n            output = trainer.model(image, intrinsics, extrinsics, future_egomotions)\n\n        figure_numpy = plot_prediction(image, output, trainer.cfg)\n        os.makedirs(os.path.join(cfg.VISUALIZATION.OUTPUT_PATH), exist_ok=True)\n        output_filename = os.path.join(cfg.VISUALIZATION.OUTPUT_PATH, 'sample_'+str(i)) + '.png'\n        Image.fromarray(figure_numpy).save(output_filename)\n        print(f'Saved output in {output_filename}')\n\n        if i >= cfg.VISUALIZATION.SAMPLE_NUMBER-1:\n            return", "\n\nif __name__ == '__main__':\n    visualise()"]}
{"filename": "test.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport os\nimport socket", "import os\nimport socket\nimport time\n\nimport pytorch_lightning as pl\nimport torch\nfrom powerbev.config import get_cfg, get_parser\nfrom powerbev.data import prepare_powerbev_dataloaders\nfrom powerbev.trainer import TrainingModule\nfrom pytorch_lightning.plugins import DDPPlugin", "from powerbev.trainer import TrainingModule\nfrom pytorch_lightning.plugins import DDPPlugin\n\n\ndef main():\n    args = get_parser().parse_args()\n    cfg = get_cfg(args)\n\n    _, valloader = prepare_powerbev_dataloaders(cfg)\n    model = TrainingModule(cfg.convert_to_dict())\n\n    if cfg.PRETRAINED.LOAD_WEIGHTS:\n        # Load single-image instance segmentation model.\n        pretrained_model_weights = torch.load(\n            cfg.PRETRAINED.PATH , map_location='cpu'\n        )['state_dict']\n\n        model.load_state_dict(pretrained_model_weights, strict=False)\n        print(f'Loaded single-image model weights from {cfg.PRETRAINED.PATH}')\n\n    save_dir = os.path.join(\n        cfg.LOG_DIR, time.strftime('%d%B%Yat%H:%M:%S%Z') + '_' + socket.gethostname() + '_' + cfg.TAG\n    ) \n    tb_logger = pl.loggers.TensorBoardLogger(save_dir=save_dir)\n    trainer = pl.Trainer(\n        gpus=cfg.GPUS,\n        accelerator='ddp',\n        precision=cfg.PRECISION,\n        sync_batchnorm=True,\n        gradient_clip_val=cfg.GRAD_NORM_CLIP,\n        max_epochs=cfg.EPOCHS,\n        weights_summary='full',\n        logger=tb_logger,\n        log_every_n_steps=cfg.LOGGING_INTERVAL,\n        plugins=DDPPlugin(find_unused_parameters=True),\n        profiler='simple',\n    )\n    trainer.test(model, valloader)", "\n\nif __name__ == \"__main__\":\n    main()"]}
{"filename": "powerbev/losses.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SpatialRegressionLoss(nn.Module):\n    def __init__(self, norm, ignore_index=255, future_discount=1.0):\n        super(SpatialRegressionLoss, self).__init__()\n        self.norm = norm\n        self.ignore_index = ignore_index\n        self.future_discount = future_discount\n\n        if norm == 1:\n            self.loss_fn = F.l1_loss\n        elif norm == 2:\n            self.loss_fn = F.mse_loss\n        elif norm == 1.5:\n            self.loss_fn = F.smooth_l1_loss\n        else:\n            raise ValueError(f'Expected norm 1 or 2, but got norm={norm}')\n    \n    def forward(self, prediction, target):       \n        assert len(prediction.shape) == 5, 'Must be a 5D tensor'\n        # ignore_index is the same across all channels\n        mask = target[:, :, :1] != self.ignore_index\n        if mask.sum() == 0:\n            return prediction.new_zeros(1)[0].float()\n\n        loss = self.loss_fn(prediction, target, reduction='none')\n\n        # Sum channel dimension\n        loss = torch.sum(loss, dim=-3, keepdims=True)\n\n        seq_len = loss.shape[1]\n        future_discounts = self.future_discount ** torch.arange(seq_len, device=loss.device, dtype=loss.dtype)\n        future_discounts = future_discounts.view(1, seq_len, 1, 1, 1)\n        loss = loss * future_discounts\n\n        return loss[mask].mean()", "\n\nclass SegmentationLoss(nn.Module):\n    def __init__(self, class_weights, ignore_index=255, use_top_k=False, top_k_ratio=1.0, future_discount=1.0):\n        super().__init__()\n        self.class_weights = class_weights\n        self.ignore_index = ignore_index\n        self.use_top_k = use_top_k\n        self.top_k_ratio = top_k_ratio\n        self.future_discount = future_discount\n        \n    def forward(self, prediction, target):\n        if target.shape[-3] != 1:\n            raise ValueError('segmentation label must be an index-label with channel dimension = 1.')\n        b, s, c, h, w = prediction.shape\n\n        prediction = prediction.view(b * s, c, h, w)\n        target = target.view(b * s, h, w)\n        loss = F.cross_entropy(\n            prediction,\n            target,\n            ignore_index=self.ignore_index,\n            reduction='none',\n            weight=self.class_weights.to(target.device),\n        )\n        \n        loss = loss.view(b, s, h, w)\n\n        future_discounts = self.future_discount ** torch.arange(s, device=loss.device, dtype=loss.dtype)\n        future_discounts = future_discounts.view(1, s, 1, 1)\n        loss = loss * future_discounts\n\n        loss = loss.view(b, s, -1)\n        if self.use_top_k:\n            # Penalises the top-k hardest pixels\n            k = int(self.top_k_ratio * loss.shape[2])\n            loss, _ = torch.sort(loss, dim=2, descending=True)\n            loss = loss[:, :, :k]\n\n        return torch.mean(loss)"]}
{"filename": "powerbev/config.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport argparse\n", "import argparse\n\nfrom fvcore.common.config import CfgNode as _CfgNode\n\n\ndef convert_to_dict(cfg_node, key_list=[]):\n    \"\"\"Convert a config node to dictionary.\"\"\"\n    _VALID_TYPES = {tuple, list, str, int, float, bool}\n    if not isinstance(cfg_node, _CfgNode):\n        if type(cfg_node) not in _VALID_TYPES:\n            print(\n                'Key {} with value {} is not a valid type; valid types: {}'.format(\n                    '.'.join(key_list), type(cfg_node), _VALID_TYPES\n                ),\n            )\n        return cfg_node\n    else:\n        cfg_dict = dict(cfg_node)\n        for k, v in cfg_dict.items():\n            cfg_dict[k] = convert_to_dict(v, key_list + [k])\n        return cfg_dict", "\n\nclass CfgNode(_CfgNode):\n    \"\"\"Remove once https://github.com/rbgirshick/yacs/issues/19 is merged.\"\"\"\n\n    def convert_to_dict(self):\n        return convert_to_dict(self)\n\n\nCN = CfgNode", "\nCN = CfgNode\n\n_C = CN()\n_C.LOG_DIR = 'tensorboard_logs'\n_C.TAG = 'default'\n\n_C.GPUS = [0]  # which gpus to use\n_C.PRECISION = 32  # 16bit or 32bit\n_C.BATCHSIZE = 2", "_C.PRECISION = 32  # 16bit or 32bit\n_C.BATCHSIZE = 2\n_C.EPOCHS = 20\n\n_C.N_WORKERS = 5\n_C.VIS_INTERVAL = 5000\n_C.LOGGING_INTERVAL = 500\n\n_C.PRETRAINED = CN()\n_C.PRETRAINED.LOAD_WEIGHTS = False", "_C.PRETRAINED = CN()\n_C.PRETRAINED.LOAD_WEIGHTS = False\n_C.PRETRAINED.PATH = ''\n\n_C.DATASET = CN()\n_C.DATASET.DATAROOT = './nuscenes/'\n_C.DATASET.VERSION = 'trainval'\n_C.DATASET.NAME = 'nuscenes'\n_C.DATASET.IGNORE_INDEX = 255  # Ignore index when creating flow/offset labels\n_C.DATASET.FILTER_INVISIBLE_VEHICLES = True  # Filter vehicles that are not visible from the cameras", "_C.DATASET.IGNORE_INDEX = 255  # Ignore index when creating flow/offset labels\n_C.DATASET.FILTER_INVISIBLE_VEHICLES = True  # Filter vehicles that are not visible from the cameras\n\n_C.TIME_RECEPTIVE_FIELD = 3  # how many frames of temporal context (1 for single timeframe)\n_C.N_FUTURE_FRAMES = 4  # how many time steps into the future to predict\n\n_C.IMAGE = CN()\n_C.IMAGE.FINAL_DIM = (224, 480)\n_C.IMAGE.RESIZE_SCALE = 0.3\n_C.IMAGE.TOP_CROP = 46", "_C.IMAGE.RESIZE_SCALE = 0.3\n_C.IMAGE.TOP_CROP = 46\n_C.IMAGE.ORIGINAL_HEIGHT = 900  # Original input RGB camera height\n_C.IMAGE.ORIGINAL_WIDTH = 1600  # Original input RGB camera width\n_C.IMAGE.NAMES = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']\n\n_C.LIFT = CN()  # image to BEV lifting\n_C.LIFT.X_BOUND = [-50.0, 50.0, 0.5]  #\u00a0Forward\n_C.LIFT.Y_BOUND = [-50.0, 50.0, 0.5]  # Sides\n_C.LIFT.Z_BOUND = [-10.0, 10.0, 20.0]  # Height", "_C.LIFT.Y_BOUND = [-50.0, 50.0, 0.5]  # Sides\n_C.LIFT.Z_BOUND = [-10.0, 10.0, 20.0]  # Height\n_C.LIFT.D_BOUND = [2.0, 50.0, 1.0]\n\n_C.MODEL = CN()\n\n_C.MODEL.ENCODER = CN()\n_C.MODEL.ENCODER.DOWNSAMPLE = 8\n_C.MODEL.ENCODER.NAME = 'efficientnet-b4'\n_C.MODEL.ENCODER.OUT_CHANNELS = 64", "_C.MODEL.ENCODER.NAME = 'efficientnet-b4'\n_C.MODEL.ENCODER.OUT_CHANNELS = 64\n_C.MODEL.ENCODER.USE_DEPTH_DISTRIBUTION = True\n\n_C.MODEL.STCONV = CN()\n_C.MODEL.STCONV.LATENT_DIM = 16\n_C.MODEL.STCONV.NUM_FEATURES = [16, 24, 32, 48, 64]\n_C.MODEL.STCONV.NUM_BLOCKS = 3\n_C.MODEL.STCONV.INPUT_EGOPOSE = True\n", "_C.MODEL.STCONV.INPUT_EGOPOSE = True\n\n_C.MODEL.TEMPORAL_MODEL = CN()\n_C.MODEL.TEMPORAL_MODEL.NAME = 'temporal_block'  # type of temporal model\n_C.MODEL.TEMPORAL_MODEL.START_OUT_CHANNELS = 64\n_C.MODEL.TEMPORAL_MODEL.EXTRA_IN_CHANNELS = 0\n_C.MODEL.TEMPORAL_MODEL.INBETWEEN_LAYERS = 0\n_C.MODEL.TEMPORAL_MODEL.PYRAMID_POOLING = True\n_C.MODEL.TEMPORAL_MODEL.INPUT_EGOPOSE = True\n", "_C.MODEL.TEMPORAL_MODEL.INPUT_EGOPOSE = True\n\n_C.MODEL.DISTRIBUTION = CN()\n_C.MODEL.DISTRIBUTION.LATENT_DIM = 32\n_C.MODEL.DISTRIBUTION.MIN_LOG_SIGMA = -5.0\n_C.MODEL.DISTRIBUTION.MAX_LOG_SIGMA = 5.0\n\n_C.MODEL.FUTURE_PRED = CN()\n_C.MODEL.FUTURE_PRED.N_GRU_BLOCKS = 3\n_C.MODEL.FUTURE_PRED.N_RES_LAYERS = 3", "_C.MODEL.FUTURE_PRED.N_GRU_BLOCKS = 3\n_C.MODEL.FUTURE_PRED.N_RES_LAYERS = 3\n\n_C.MODEL.DECODER = CN()\n\n_C.MODEL.BN_MOMENTUM = 0.1\n_C.MODEL.SUBSAMPLE = False  # Subsample frames for Lyft\n\n_C.SEMANTIC_SEG = CN()\n_C.SEMANTIC_SEG.WEIGHTS = [1.0, 2.0]  # per class cross entropy weights (bg, dynamic, drivable, lane)", "_C.SEMANTIC_SEG = CN()\n_C.SEMANTIC_SEG.WEIGHTS = [1.0, 2.0]  # per class cross entropy weights (bg, dynamic, drivable, lane)\n_C.SEMANTIC_SEG.USE_TOP_K = True  # backprop only top-k hardest pixels\n_C.SEMANTIC_SEG.TOP_K_RATIO = 0.25\n\n_C.INSTANCE_SEG = CN()\n\n_C.INSTANCE_FLOW = CN()\n_C.INSTANCE_FLOW.ENABLED = True\n", "_C.INSTANCE_FLOW.ENABLED = True\n\n_C.PROBABILISTIC = CN()\n_C.PROBABILISTIC.ENABLED = False  # learn a distribution over futures\n_C.PROBABILISTIC.WEIGHT = 100.0\n_C.PROBABILISTIC.FUTURE_DIM = 6  # number of dimension added (future flow, future centerness, offset, seg)\n\n_C.FUTURE_DISCOUNT = 0.95\n\n_C.OPTIMIZER = CN()", "\n_C.OPTIMIZER = CN()\n_C.OPTIMIZER.LR = 3e-4\n_C.OPTIMIZER.WEIGHT_DECAY = 1e-7\n_C.GRAD_NORM_CLIP = 5\n\n_C.VISUALIZATION = CN()\n_C.VISUALIZATION.OUTPUT_PATH = './visualization_outputs'\n_C.VISUALIZATION.SAMPLE_NUMBER = 1000\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description='PowerBEV training')\n    parser.add_argument('--config-file', default='', metavar='FILE', help='path to config file')\n    parser.add_argument(\n        'opts', help='Modify config options using the command-line', default=None, nargs=argparse.REMAINDER,\n    )\n    return parser", "_C.VISUALIZATION.SAMPLE_NUMBER = 1000\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description='PowerBEV training')\n    parser.add_argument('--config-file', default='', metavar='FILE', help='path to config file')\n    parser.add_argument(\n        'opts', help='Modify config options using the command-line', default=None, nargs=argparse.REMAINDER,\n    )\n    return parser\n", "\n\ndef get_cfg(args=None, cfg_dict=None):\n    \"\"\" First get default config. Then merge cfg_dict. Then merge according to args. \"\"\"\n\n    cfg = _C.clone()\n\n    if cfg_dict is not None:\n        cfg.merge_from_other_cfg(CfgNode(cfg_dict))\n\n    if args is not None:\n        if args.config_file:\n            cfg.merge_from_file(args.config_file)\n        cfg.merge_from_list(args.opts)\n        cfg.freeze()\n    return cfg", ""]}
{"filename": "powerbev/metrics.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nfrom typing import Optional\n", "from typing import Optional\n\nimport torch\nfrom pytorch_lightning.metrics.functional.classification import \\\n    stat_scores_multiple_classes\nfrom pytorch_lightning.metrics.functional.reduction import reduce\nfrom pytorch_lightning.metrics.metric import Metric\n\n\nclass IntersectionOverUnion(Metric):\n    \"\"\"Computes intersection-over-union.\"\"\"\n    def __init__(\n        self,\n        n_classes: int,\n        ignore_index: Optional[int] = None,\n        absent_score: float = 0.0,\n        reduction: str = 'none',\n        compute_on_step: bool = False,\n    ):\n        super().__init__(compute_on_step=compute_on_step)\n\n        self.n_classes = n_classes\n        self.ignore_index = ignore_index\n        self.absent_score = absent_score\n        self.reduction = reduction\n\n        self.add_state('true_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('false_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('false_negative', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('support', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\n    def update(self, prediction: torch.Tensor, target: torch.Tensor):\n        tps, fps, _, fns, sups = stat_scores_multiple_classes(prediction, target, self.n_classes)\n\n        self.true_positive += tps\n        self.false_positive += fps\n        self.false_negative += fns\n        self.support += sups\n\n    def compute(self):\n        scores = torch.zeros(self.n_classes, device=self.true_positive.device, dtype=torch.float32)\n\n        for class_idx in range(self.n_classes):\n            if class_idx == self.ignore_index:\n                continue\n\n            tp = self.true_positive[class_idx]\n            fp = self.false_positive[class_idx]\n            fn = self.false_negative[class_idx]\n            sup = self.support[class_idx]\n\n            # If this class is absent in the target (no support) AND absent in the pred (no true or false\n            # positives), then use the absent_score for this class.\n            if sup + tp + fp == 0:\n                scores[class_idx] = self.absent_score\n                continue\n\n            denominator = tp + fp + fn\n            score = tp.to(torch.float) / denominator\n            scores[class_idx] = score\n\n        # Remove the ignored class index from the scores.\n        if (self.ignore_index is not None) and (0 <= self.ignore_index < self.n_classes):\n            scores = torch.cat([scores[:self.ignore_index], scores[self.ignore_index+1:]])\n\n        return reduce(scores, reduction=self.reduction)", "\nclass IntersectionOverUnion(Metric):\n    \"\"\"Computes intersection-over-union.\"\"\"\n    def __init__(\n        self,\n        n_classes: int,\n        ignore_index: Optional[int] = None,\n        absent_score: float = 0.0,\n        reduction: str = 'none',\n        compute_on_step: bool = False,\n    ):\n        super().__init__(compute_on_step=compute_on_step)\n\n        self.n_classes = n_classes\n        self.ignore_index = ignore_index\n        self.absent_score = absent_score\n        self.reduction = reduction\n\n        self.add_state('true_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('false_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('false_negative', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('support', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\n    def update(self, prediction: torch.Tensor, target: torch.Tensor):\n        tps, fps, _, fns, sups = stat_scores_multiple_classes(prediction, target, self.n_classes)\n\n        self.true_positive += tps\n        self.false_positive += fps\n        self.false_negative += fns\n        self.support += sups\n\n    def compute(self):\n        scores = torch.zeros(self.n_classes, device=self.true_positive.device, dtype=torch.float32)\n\n        for class_idx in range(self.n_classes):\n            if class_idx == self.ignore_index:\n                continue\n\n            tp = self.true_positive[class_idx]\n            fp = self.false_positive[class_idx]\n            fn = self.false_negative[class_idx]\n            sup = self.support[class_idx]\n\n            # If this class is absent in the target (no support) AND absent in the pred (no true or false\n            # positives), then use the absent_score for this class.\n            if sup + tp + fp == 0:\n                scores[class_idx] = self.absent_score\n                continue\n\n            denominator = tp + fp + fn\n            score = tp.to(torch.float) / denominator\n            scores[class_idx] = score\n\n        # Remove the ignored class index from the scores.\n        if (self.ignore_index is not None) and (0 <= self.ignore_index < self.n_classes):\n            scores = torch.cat([scores[:self.ignore_index], scores[self.ignore_index+1:]])\n\n        return reduce(scores, reduction=self.reduction)", "\n\nclass PanopticMetric(Metric):\n    def __init__(\n        self,\n        n_classes: int,\n        temporally_consistent: bool = True,\n        vehicles_id: int = 1,\n        compute_on_step: bool = False,\n    ):\n        super().__init__(compute_on_step=compute_on_step)\n\n        self.n_classes = n_classes\n        self.temporally_consistent = temporally_consistent\n        self.vehicles_id = vehicles_id\n        self.keys = ['iou', 'true_positive', 'false_positive', 'false_negative']\n\n        self.add_state('iou', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('true_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('false_positive', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n        self.add_state('false_negative', default=torch.zeros(n_classes), dist_reduce_fx='sum')\n\n    def update(self, pred_instance, gt_instance):\n        \"\"\"\n        Update state with predictions and targets.\n\n        Parameters\n        ----------\n            pred_instance: (b, s, h, w)\n                Temporally consistent instance segmentation prediction.\n            gt_instance: (b, s, h, w)\n                Ground truth instance segmentation.\n        \"\"\"\n        batch_size, sequence_length = gt_instance.shape[:2]\n        # Process labels\n        assert gt_instance.min() == 0, 'ID 0 of gt_instance must be background'\n        pred_segmentation = (pred_instance > 0).long()\n        gt_segmentation = (gt_instance > 0).long()\n\n        for b in range(batch_size):\n            unique_id_mapping = {}\n            for t in range(sequence_length):\n                result = self.panoptic_metrics(\n                    pred_segmentation[b, t].detach(),\n                    pred_instance[b, t].detach(),\n                    gt_segmentation[b, t],\n                    gt_instance[b, t],\n                    unique_id_mapping,\n                )\n\n                self.iou += result['iou']\n                self.true_positive += result['true_positive']\n                self.false_positive += result['false_positive']\n                self.false_negative += result['false_negative']\n\n    def compute(self):\n        denominator = torch.maximum(\n            (self.true_positive + self.false_positive / 2 + self.false_negative / 2),\n            torch.ones_like(self.true_positive)\n        )\n        pq = self.iou / denominator\n        sq = self.iou / torch.maximum(self.true_positive, torch.ones_like(self.true_positive))\n        rq = self.true_positive / denominator\n\n        return {'pq': pq,\n                'sq': sq,\n                'rq': rq,\n                #\u00a0If 0, it means there wasn't any detection.\n                'denominator': (self.true_positive + self.false_positive / 2 + self.false_negative / 2),\n                }\n\n    def panoptic_metrics(self, pred_segmentation, pred_instance, gt_segmentation, gt_instance, unique_id_mapping):\n        \"\"\"\n        Computes panoptic quality metric components.\n\n        Parameters\n        ----------\n            pred_segmentation: [H, W] range {0, ..., n_classes-1} (>= n_classes is void)\n            pred_instance: [H, W] range {0, ..., n_instances} (zero means background)\n            gt_segmentation: [H, W] range {0, ..., n_classes-1} (>= n_classes is void)\n            gt_instance: [H, W] range {0, ..., n_instances} (zero means background)\n            unique_id_mapping: instance id mapping to check consistency\n        \"\"\"\n        n_classes = self.n_classes\n\n        result = {key: torch.zeros(n_classes, dtype=torch.float32, device=gt_instance.device) for key in self.keys}\n\n        assert pred_segmentation.dim() == 2\n        assert pred_segmentation.shape == pred_instance.shape == gt_segmentation.shape == gt_instance.shape\n\n        n_instances = int(torch.cat([pred_instance, gt_instance]).max().item())\n        n_all_things = n_instances + n_classes  # Classes + instances.\n        n_things_and_void = n_all_things + 1\n\n        #\u00a0Now 1 is background; 0 is void (not used). 2 is vehicle semantic class but since it overlaps with\n        # instances, it is not present.\n        # and the rest are instance ids starting from 3\n        prediction, pred_to_cls = self.combine_mask(pred_segmentation, pred_instance, n_classes, n_all_things)\n        target, target_to_cls = self.combine_mask(gt_segmentation, gt_instance, n_classes, n_all_things)\n\n        # Compute ious between all stuff and things\n        # hack for bincounting 2 arrays together\n        x = prediction + n_things_and_void * target\n        bincount_2d = torch.bincount(x.long(), minlength=n_things_and_void ** 2)\n        if bincount_2d.shape[0] != n_things_and_void ** 2:\n            raise ValueError('Incorrect bincount size.')\n        conf = bincount_2d.reshape((n_things_and_void, n_things_and_void))\n        # Drop void class\n        conf = conf[1:, 1:]\n\n        # Confusion matrix contains intersections between all combinations of classes\n        union = conf.sum(0).unsqueeze(0) + conf.sum(1).unsqueeze(1) - conf\n        iou = torch.where(union > 0, (conf.float() + 1e-9) / (union.float() + 1e-9), torch.zeros_like(union).float())\n\n        # In the iou matrix, first dimension is target idx, second dimension is pred idx.\n        # Mapping will contain a tuple that maps prediction idx to target idx for segments matched by iou.\n        mapping = (iou > 0.5).nonzero(as_tuple=False)\n\n        # Check that classes match.\n        is_matching = pred_to_cls[mapping[:, 1]] == target_to_cls[mapping[:, 0]]\n        mapping = mapping[is_matching]\n        tp_mask = torch.zeros_like(conf, dtype=torch.bool)\n        tp_mask[mapping[:, 0], mapping[:, 1]] = True\n\n        # First ids correspond to \"stuff\" i.e. semantic seg.\n        # Instance ids are offset accordingly\n        for target_id, pred_id in mapping:\n            cls_id = pred_to_cls[pred_id]\n\n            if self.temporally_consistent and cls_id == self.vehicles_id:\n                if target_id.item() in unique_id_mapping and unique_id_mapping[target_id.item()] != pred_id.item():\n                    # Not temporally consistent\n                    result['false_negative'][target_to_cls[target_id]] += 1\n                    result['false_positive'][pred_to_cls[pred_id]] += 1\n                    unique_id_mapping[target_id.item()] = pred_id.item()\n                    continue\n\n            result['true_positive'][cls_id] += 1\n            result['iou'][cls_id] += iou[target_id][pred_id]\n            unique_id_mapping[target_id.item()] = pred_id.item()\n\n        for target_id in range(n_classes, n_all_things):\n            # If this is a true positive do nothing.\n            if tp_mask[target_id, n_classes:].any():\n                continue\n            # If this target instance didn't match with any predictions and was present set it as false negative.\n            if target_to_cls[target_id] != -1:\n                result['false_negative'][target_to_cls[target_id]] += 1\n\n        for pred_id in range(n_classes, n_all_things):\n            # If this is a true positive do nothing.\n            if tp_mask[n_classes:, pred_id].any():\n                continue\n            # If this predicted instance didn't match with any prediction, set that predictions as false positive.\n            if pred_to_cls[pred_id] != -1 and (conf[:, pred_id] > 0).any():\n                result['false_positive'][pred_to_cls[pred_id]] += 1\n\n        return result\n\n    def combine_mask(self, segmentation: torch.Tensor, instance: torch.Tensor, n_classes: int, n_all_things: int):\n        \"\"\"Shifts all things ids by num_classes and combines things and stuff into a single mask\n\n        Returns a combined mask + a mapping from id to segmentation class.\n        \"\"\"\n        instance = instance.view(-1)\n        instance_mask = instance > 0\n        instance = instance - 1 + n_classes\n\n        segmentation = segmentation.clone().view(-1)\n        segmentation_mask = segmentation < n_classes  # Remove void pixels.\n\n        # Build an index from instance id to class id.\n        instance_id_to_class_tuples = torch.cat(\n            (\n                instance[instance_mask & segmentation_mask].unsqueeze(1),\n                segmentation[instance_mask & segmentation_mask].unsqueeze(1),\n            ),\n            dim=1,\n        )\n        instance_id_to_class = -instance_id_to_class_tuples.new_ones((n_all_things,))\n        instance_id_to_class[instance_id_to_class_tuples[:, 0]] = instance_id_to_class_tuples[:, 1]\n        instance_id_to_class[torch.arange(n_classes, device=segmentation.device)] = torch.arange(\n            n_classes, device=segmentation.device\n        )\n\n        segmentation[instance_mask] = instance[instance_mask]\n        segmentation += 1  # Shift all legit classes by 1.\n        segmentation[~segmentation_mask] = 0  # Shift void class to zero.\n\n        return segmentation, instance_id_to_class", ""]}
{"filename": "powerbev/data.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport os\nfrom operator import rshift", "import os\nfrom operator import rshift\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nfrom cv2 import fastNlMeansDenoising\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset", "from cv2 import fastNlMeansDenoising\nfrom lyft_dataset_sdk.lyftdataset import LyftDataset\nfrom nuscenes.nuscenes import NuScenes\nfrom nuscenes.utils.data_classes import Box\nfrom nuscenes.utils.splits import create_splits_scenes\nfrom PIL import Image\nfrom powerbev.utils.geometry import (calculate_birds_eye_view_parameters,\n                                     convert_egopose_to_matrix_numpy,\n                                     invert_matrix_egopose_numpy, mat2pose_vec,\n                                     pose_vec2mat, resize_and_crop_image,", "                                     invert_matrix_egopose_numpy, mat2pose_vec,\n                                     pose_vec2mat, resize_and_crop_image,\n                                     update_intrinsics)\nfrom powerbev.utils.instance import \\\n    convert_instance_mask_to_center_and_offset_label\nfrom powerbev.utils.lyft_splits import TRAIN_LYFT_INDICES, VAL_LYFT_INDICES\nfrom pyquaternion import Quaternion\nfrom scipy.spatial.transform import Rotation as R\n\n\nclass FuturePredictionDataset(torch.utils.data.Dataset):\n    def __init__(self, nusc, is_train, cfg):\n        self.nusc = nusc\n        self.is_train = is_train\n        self.cfg = cfg\n\n        self.is_lyft = isinstance(nusc, LyftDataset)\n\n        if self.is_lyft:\n            self.dataroot = self.nusc.data_path\n        else:\n            self.dataroot = self.nusc.dataroot\n\n        self.mode = 'train' if self.is_train else 'val'\n\n        self.sequence_length = cfg.TIME_RECEPTIVE_FIELD + cfg.N_FUTURE_FRAMES\n\n        self.scenes = self.get_scenes()\n        self.ixes = self.get_samples()\n        self.indices = self.get_indices()\n\n        # Image resizing and cropping\n        self.augmentation_parameters = self.get_resizing_and_cropping_parameters()\n\n        # Normalising input images\n        self.normalise_image = torchvision.transforms.Compose(\n            [torchvision.transforms.ToTensor(),\n             torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ]\n        )\n\n        # Bird's-eye view parameters\n        bev_resolution, bev_start_position, bev_dimension = calculate_birds_eye_view_parameters(\n            cfg.LIFT.X_BOUND, cfg.LIFT.Y_BOUND, cfg.LIFT.Z_BOUND\n        )\n        self.bev_resolution, self.bev_start_position, self.bev_dimension = (\n            bev_resolution.numpy(), bev_start_position.numpy(), bev_dimension.numpy()\n        )\n\n        # Spatial extent in bird's-eye view, in meters\n        self.spatial_extent = (self.cfg.LIFT.X_BOUND[1], self.cfg.LIFT.Y_BOUND[1])\n\n    def get_scenes(self):\n        \"\"\"\n        Obtain the list of scenes names in the given split.\n        \"\"\"\n        if self.is_lyft:\n            scenes = [row['name'] for row in self.nusc.scene]\n\n            # Split in train/val\n            indices = TRAIN_LYFT_INDICES if self.is_train else VAL_LYFT_INDICES\n            scenes = [scenes[i] for i in indices]\n        else:\n            # filter by scene split\n            split = {'v1.0-trainval': {True: 'train', False: 'val'},\n                     'v1.0-mini': {True: 'mini_train', False: 'mini_val'},}[\n                self.nusc.version\n            ][self.is_train]\n\n            scenes = create_splits_scenes()[split]\n\n        return scenes\n\n    def get_samples(self):\n        \"\"\"\n        Find and sort the samples in the given split by scene.\n        \"\"\"\n        samples = [sample for sample in self.nusc.sample]\n\n        # remove samples that aren't in this split\n        samples = [sample for sample in samples if self.nusc.get('scene', sample['scene_token'])['name'] in self.scenes]\n\n        # sort by scene, timestamp (only to make chronological viz easier)\n        samples.sort(key=lambda x: (x['scene_token'], x['timestamp']))\n\n        return samples\n\n    def get_indices(self):\n        \"\"\"\n        Group the sample indices by sequence length.\n        \"\"\"\n        indices = []\n        for index in range(len(self.ixes)):\n            is_valid_data = True\n            previous_rec = None\n            current_indices = []\n            for t in range(self.sequence_length):\n                index_t = index + t\n                # Going over the dataset size limit.\n                if index_t >= len(self.ixes):\n                    is_valid_data = False\n                    break\n                rec = self.ixes[index_t]\n                # Check if scene is the same\n                if (previous_rec is not None) and (rec['scene_token'] != previous_rec['scene_token']):\n                    is_valid_data = False\n                    break\n\n                current_indices.append(index_t)\n                previous_rec = rec\n\n            if is_valid_data:\n                indices.append(current_indices)\n\n        return np.asarray(indices)\n\n    def get_resizing_and_cropping_parameters(self):\n        \"\"\"\n        Determine the parameters for preprocessing of the input image, e.g. image size after scaling, cropping area.\n        \"\"\"\n        original_height, original_width = self.cfg.IMAGE.ORIGINAL_HEIGHT, self.cfg.IMAGE.ORIGINAL_WIDTH\n        final_height, final_width = self.cfg.IMAGE.FINAL_DIM\n\n        resize_scale = self.cfg.IMAGE.RESIZE_SCALE\n        resize_dims = (int(original_width * resize_scale), int(original_height * resize_scale))\n        resized_width, resized_height = resize_dims\n\n        crop_h = self.cfg.IMAGE.TOP_CROP\n        crop_w = int(max(0, (resized_width - final_width) / 2))\n        # Left, top, right, bottom crops.\n        crop = (crop_w, crop_h, crop_w + final_width, crop_h + final_height)\n\n        if resized_width != final_width:\n            print('Zero padding left and right parts of the image.')\n        if crop_h + final_height != resized_height:\n            print('Zero padding bottom part of the image.')\n\n        return {'scale_width': resize_scale,\n                'scale_height': resize_scale,\n                'resize_dims': resize_dims,\n                'crop': crop,\n                }\n\n    def get_input_data(self, rec):\n        \"\"\"\n        Obtain the input image as well as the intrinsics and extrinsics parameters of the corresponding camera.\n\n        Parameters\n        ----------\n            rec: nuscenes identifier for a given timestamp\n\n        Returns\n        -------\n            images: torch.Tensor<float> (N, 3, H, W)\n            intrinsics: torch.Tensor<float> (3, 3)\n            extrinsics: torch.Tensor(N, 4, 4)\n        \"\"\"\n        images = []\n        intrinsics = []\n        extrinsics = []\n        cameras = self.cfg.IMAGE.NAMES\n\n        #\u00a0The extrinsics we want are from the camera sensor to \"flat egopose\" as defined\n        # https://github.com/nutonomy/nuscenes-devkit/blob/9b492f76df22943daf1dc991358d3d606314af27/python-sdk/nuscenes/nuscenes.py#L279\n        # which corresponds to the position of the lidar.\n        # This is because the labels are generated by projecting the 3D bounding box in this lidar's reference frame.\n\n        # From lidar egopose to world.\n        lidar_sample = self.nusc.get('sample_data', rec['data']['LIDAR_TOP'])\n        lidar_pose = self.nusc.get('ego_pose', lidar_sample['ego_pose_token'])\n        yaw = Quaternion(lidar_pose['rotation']).yaw_pitch_roll[0]\n        lidar_rotation = Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)])\n        lidar_translation = np.array(lidar_pose['translation'])[:, None]\n        lidar_to_world = np.vstack([\n            np.hstack((lidar_rotation.rotation_matrix, lidar_translation)),\n            np.array([0, 0, 0, 1])\n        ])\n\n        for cam in cameras:\n            camera_sample = self.nusc.get('sample_data', rec['data'][cam])\n\n            # Transformation from world to egopose\n            car_egopose = self.nusc.get('ego_pose', camera_sample['ego_pose_token'])\n            egopose_rotation = Quaternion(car_egopose['rotation']).inverse\n            egopose_translation = -np.array(car_egopose['translation'])[:, None]\n            world_to_car_egopose = np.vstack([\n                np.hstack((egopose_rotation.rotation_matrix, egopose_rotation.rotation_matrix @ egopose_translation)),\n                np.array([0, 0, 0, 1])\n            ])\n\n            # From egopose to sensor\n            sensor_sample = self.nusc.get('calibrated_sensor', camera_sample['calibrated_sensor_token'])\n            intrinsic = torch.Tensor(sensor_sample['camera_intrinsic'])\n            sensor_rotation = Quaternion(sensor_sample['rotation'])\n            sensor_translation = np.array(sensor_sample['translation'])[:, None]\n            car_egopose_to_sensor = np.vstack([\n                np.hstack((sensor_rotation.rotation_matrix, sensor_translation)),\n                np.array([0, 0, 0, 1])\n            ])\n            car_egopose_to_sensor = np.linalg.inv(car_egopose_to_sensor)\n\n            # Combine all the transformation.\n            # From sensor to lidar.\n            lidar_to_sensor = car_egopose_to_sensor @ world_to_car_egopose @ lidar_to_world\n            sensor_to_lidar = torch.from_numpy(np.linalg.inv(lidar_to_sensor)).float()\n\n            # Load image\n            image_filename = os.path.join(self.dataroot, camera_sample['filename'])\n            img = Image.open(image_filename)\n            # Resize and crop\n            img = resize_and_crop_image(\n                img, resize_dims=self.augmentation_parameters['resize_dims'], crop=self.augmentation_parameters['crop']\n            )\n            # Normalise image\n            normalised_img = self.normalise_image(img)\n\n            # Combine resize/cropping in the intrinsics\n            top_crop = self.augmentation_parameters['crop'][1]\n            left_crop = self.augmentation_parameters['crop'][0]\n            intrinsic = update_intrinsics(\n                intrinsic, top_crop, left_crop,\n                scale_width=self.augmentation_parameters['scale_width'],\n                scale_height=self.augmentation_parameters['scale_height']\n            )\n\n            images.append(normalised_img.unsqueeze(0).unsqueeze(0))\n            intrinsics.append(intrinsic.unsqueeze(0).unsqueeze(0))\n            extrinsics.append(sensor_to_lidar.unsqueeze(0).unsqueeze(0))\n\n        images, intrinsics, extrinsics = (torch.cat(images, dim=1),\n                                          torch.cat(intrinsics, dim=1),\n                                          torch.cat(extrinsics, dim=1)\n                                          )\n\n        return images, intrinsics, extrinsics\n\n    def _get_top_lidar_pose(self, rec):\n        \"\"\"\n        Obtain the vehicle attitude at the current moment.\n        \"\"\"\n        egopose = self.nusc.get('ego_pose', self.nusc.get('sample_data', rec['data']['LIDAR_TOP'])['ego_pose_token'])\n        trans = -np.array(egopose['translation'])\n        yaw = Quaternion(egopose['rotation']).yaw_pitch_roll[0]\n        rot = Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)]).inverse\n        return trans, rot\n\n    def record_instance(self, rec, instance_map):\n        \"\"\"\n        Record information about each visible instance in the sequence and assign a unique ID to it.\n        \"\"\"\n        translation, rotation = self._get_top_lidar_pose(rec)\n        self.egopose_list.append([translation, rotation])\n\n        for annotation_token in rec['anns']:\n            \n            annotation = self.nusc.get('sample_annotation', annotation_token)\n\n            if not self.is_lyft:\n                # NuScenes filter\n                # Filter out all non vehicle instances\n                if 'vehicle' not in annotation['category_name']:\n                    continue\n                # Filter out invisible vehicles\n                if self.cfg.DATASET.FILTER_INVISIBLE_VEHICLES and int(annotation['visibility_token']) == 1 and annotation['instance_token'] not in self.visible_instance_set:\n                    continue\n                # Filter out vehicles that have not been seen in the past\n                if self.counter >= self.cfg.TIME_RECEPTIVE_FIELD and annotation['instance_token'] not in self.visible_instance_set:\n                    continue\n                self.visible_instance_set.add(annotation['instance_token'])\n            else:\n                # Lyft filter\n                # Filter out all non vehicle instances\n                if annotation['category_name'] not in ['bus', 'car', 'construction_vehicle', 'trailer', 'truck']:\n                    continue\n\n            if annotation['instance_token'] not in instance_map:\n                instance_map[annotation['instance_token']] = len(instance_map) + 1\n            instance_id = instance_map[annotation['instance_token']]\n\n            if not self.is_lyft:\n                instance_attribute = int(annotation['visibility_token'])\n            else:\n                instance_attribute = 0\n            \n            if annotation['instance_token'] not in self.instance_dict:\n                # For the first occurrence of an instance\n                self.instance_dict[annotation['instance_token']] = {\n                    'timestep': [self.counter],\n                    'translation': [annotation['translation']],\n                    'rotation': [annotation['rotation']],\n                    'size': annotation['size'],\n                    'instance_id': instance_id,\n                    'attribute_label': [instance_attribute],\n                }\n            else:\n                # For the instance that have appeared before\n                self.instance_dict[annotation['instance_token']]['timestep'].append(self.counter)\n                self.instance_dict[annotation['instance_token']]['translation'].append(annotation['translation'])\n                self.instance_dict[annotation['instance_token']]['rotation'].append(annotation['rotation'])\n                self.instance_dict[annotation['instance_token']]['attribute_label'].append(instance_attribute)\n\n        return instance_map\n    \n    @staticmethod\n    def _check_consistency(translation, prev_translation, threshold=1.0):\n        \"\"\"\n        Check for significant displacement of the instance adjacent moments.\n        \"\"\"\n        x, y = translation[:2]\n        prev_x, prev_y = prev_translation[:2]\n\n        if abs(x - prev_x) > threshold or abs(y - prev_y) > threshold:\n            return False\n        return True\n\n    def refine_instance_poly(self, instance):\n        \"\"\"\n        Fix the missing frames and disturbances of ground truth caused by noise.\n        \"\"\"\n        pointer = 1\n        for i in range(instance['timestep'][0] + 1, self.sequence_length):\n            # Fill in the missing frames\n            if i not in instance['timestep']:\n                instance['timestep'].insert(pointer, i)\n                instance['translation'].insert(pointer, instance['translation'][pointer-1])\n                instance['rotation'].insert(pointer, instance['rotation'][pointer-1])\n                instance['attribute_label'].insert(pointer, instance['attribute_label'][pointer-1])\n                pointer += 1\n                continue\n            \n            # Eliminate observation disturbances\n            if self._check_consistency(instance['translation'][pointer], instance['translation'][pointer-1]):\n                instance['translation'][pointer] = instance['translation'][pointer-1]\n                instance['rotation'][pointer] = instance['rotation'][pointer-1]\n                instance['attribute_label'][pointer] = instance['attribute_label'][pointer-1]\n            pointer += 1\n        \n        return instance\n\n    @staticmethod\n    def generate_flow(flow, instance_img, instance, instance_id):\n        \"\"\"\n        Generate ground truth for the flow of each instance based on instance segmentation.\n        \"\"\"\n        _, h, w = instance_img.shape\n        x, y = torch.meshgrid(torch.arange(h, dtype=torch.float), torch.arange(w, dtype=torch.float))\n        grid = torch.stack((x, y), dim=0)\n\n        # Set the first frame\n        instance_mask = (instance_img[0] == instance_id)\n        flow[0, 1, instance_mask] = grid[0, instance_mask].mean(dim=0, keepdim=True).round() - grid[0, instance_mask]\n        flow[0, 0, instance_mask] = grid[1, instance_mask].mean(dim=0, keepdim=True).round() - grid[1, instance_mask]\n\n        for i, timestep in enumerate(instance['timestep']):\n            if i == 0:\n                continue\n\n            instance_mask = (instance_img[timestep] == instance_id)\n            prev_instance_mask = (instance_img[timestep-1] == instance_id)\n            if instance_mask.sum() == 0 or prev_instance_mask.sum() == 0:\n                continue\n\n            # Centripetal backward flow is defined as displacement vector from each foreground pixel at time t to the object center of the associated instance identity at time t\u22121\n            flow[timestep, 1, instance_mask] = grid[0, prev_instance_mask].mean(dim=0, keepdim=True).round() - grid[0, instance_mask]\n            flow[timestep, 0, instance_mask] = grid[1, prev_instance_mask].mean(dim=0, keepdim=True).round() - grid[1, instance_mask]\n\n        return flow\n    \n    def get_flow_label(self, instance_img, instance_map, ignore_index=255):\n        \"\"\"\n        Generate the global map of the flow ground truth.\n        \"\"\"\n        seq_len, h, w = instance_img.shape\n        flow = ignore_index * torch.ones(seq_len, 2, h, w)\n\n        for token, instance in self.instance_dict.items():\n            flow = self.generate_flow(flow, instance_img, instance, instance_map[token])\n        return flow\n\n    def _get_poly_region_in_image(self, instance_annotation, present_egopose):\n        \"\"\"\n        Obtain the bounding box polygon of the instance.\n        \"\"\"\n        present_ego_translation, present_ego_rotation = present_egopose\n\n        box = Box(\n            instance_annotation['translation'], instance_annotation['size'], Quaternion(instance_annotation['rotation'])\n        )\n        box.translate(present_ego_translation)\n        box.rotate(present_ego_rotation)\n        pts = box.bottom_corners()[:2].T\n\n        if self.cfg.LIFT.X_BOUND[0] <= pts.min(axis=0)[0] and pts.max(axis=0)[0] <= self.cfg.LIFT.X_BOUND[1] and self.cfg.LIFT.Y_BOUND[0] <= pts.min(axis=0)[1] and pts.max(axis=0)[1] <= self.cfg.LIFT.Y_BOUND[1]:\n            pts = np.round((pts - self.bev_start_position[:2] + self.bev_resolution[:2] / 2.0) / self.bev_resolution[:2]).astype(np.int32)\n            pts[:, [1, 0]] = pts[:, [0, 1]]\n            z = box.bottom_corners()[2, 0]\n            return pts, z\n        else:\n            return None, None\n\n    def get_label(self):\n        \"\"\"\n        Generate labels for semantic segmentation, instance segmentation, z position, attribute from the raw data of nuScenes.\n        \"\"\"\n        timestep = self.counter\n        segmentation = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n        # Background is ID 0\n        instance = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n        z_position = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n        attribute_label = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n        \n        for instance_token, instance_annotation in self.instance_dict.items():\n            if timestep not in instance_annotation['timestep']:\n                continue\n            pointer = instance_annotation['timestep'].index(timestep)\n            annotation = {\n                'translation': instance_annotation['translation'][pointer],\n                'rotation': instance_annotation['rotation'][pointer],\n                'size': instance_annotation['size'],\n            }\n            poly_region, z = self._get_poly_region_in_image(annotation, self.egopose_list[self.cfg.TIME_RECEPTIVE_FIELD - 1]) \n            if isinstance(poly_region, np.ndarray):\n                if self.counter >= self.cfg.TIME_RECEPTIVE_FIELD and instance_token not in self.visible_instance_set:\n                    continue\n                self.visible_instance_set.add(instance_token)\n\n                cv2.fillPoly(instance, [poly_region], instance_annotation['instance_id'])\n                cv2.fillPoly(segmentation, [poly_region], 1.0)\n                cv2.fillPoly(z_position, [poly_region], z)\n                cv2.fillPoly(attribute_label, [poly_region], instance_annotation['attribute_label'][pointer]) \n        \n        segmentation = torch.from_numpy(segmentation).long().unsqueeze(0).unsqueeze(0)\n        instance = torch.from_numpy(instance).long().unsqueeze(0)\n        z_position = torch.from_numpy(z_position).float().unsqueeze(0).unsqueeze(0)\n        attribute_label = torch.from_numpy(attribute_label).long().unsqueeze(0).unsqueeze(0)\n\n        return segmentation, instance, z_position, attribute_label\n\n    def get_future_egomotion(self, rec, index):\n        \"\"\"\n        Obtain the egomotion in the corresponding sequence from the raw data of nuScenes.\n        \"\"\"\n        rec_t0 = rec\n\n        # Identity\n        future_egomotion = np.eye(4, dtype=np.float32)\n\n        if index < len(self.ixes) - 1:\n            rec_t1 = self.ixes[index + 1]\n\n            if rec_t0['scene_token'] == rec_t1['scene_token']:\n                egopose_t0 = self.nusc.get(\n                    'ego_pose', self.nusc.get('sample_data', rec_t0['data']['LIDAR_TOP'])['ego_pose_token']\n                )\n                egopose_t1 = self.nusc.get(\n                    'ego_pose', self.nusc.get('sample_data', rec_t1['data']['LIDAR_TOP'])['ego_pose_token']\n                )\n\n                egopose_t0 = convert_egopose_to_matrix_numpy(egopose_t0)\n                egopose_t1 = convert_egopose_to_matrix_numpy(egopose_t1)\n\n                future_egomotion = invert_matrix_egopose_numpy(egopose_t1).dot(egopose_t0)\n                future_egomotion[3, :3] = 0.0\n                future_egomotion[3, 3] = 1.0\n\n        future_egomotion = torch.Tensor(future_egomotion).float()\n\n        # Convert to 6DoF vector\n        future_egomotion = mat2pose_vec(future_egomotion)\n        return future_egomotion.unsqueeze(0)\n    \n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns\n        -------\n            data: dict with the following keys:\n                image: torch.Tensor<float> (T, N, 3, H, W)\n                    normalised cameras images with T the sequence length, and N the number of cameras.\n                intrinsics: torch.Tensor<float> (T, N, 3, 3)\n                    intrinsics containing resizing and cropping parameters.\n                extrinsics: torch.Tensor<float> (T, N, 4, 4)\n                    6 DoF pose from world coordinates to camera coordinates.\n                segmentation: torch.Tensor<int64> (T, 1, H_bev, W_bev)\n                    (H_bev, W_bev) are the pixel dimensions in bird's-eye view.\n                instance: torch.Tensor<int64> (T, 1, H_bev, W_bev)\n                centerness: torch.Tensor<float> (T, 1, H_bev, W_bev)\n                offset: torch.Tensor<float> (T, 2, H_bev, W_bev)\n                flow: torch.Tensor<float> (T, 2, H_bev, W_bev)\n                future_egomotion: torch.Tensor<float> (T, 6)\n                    6 DoF egomotion t -> t+1\n                sample_token: List<str> (T,)\n                'z_position': list_z_position,\n                'attribute': list_attribute_label,\n\n        \"\"\"\n        data = {}\n        keys = ['image', 'intrinsics', 'extrinsics',\n                'segmentation', 'instance', 'centerness', 'offset', 'flow', 'future_egomotion',\n                'sample_token',\n                'z_position', 'attribute'\n                ]\n        for key in keys:\n            data[key] = []\n\n        instance_map = {}\n        # The visible instance must have high visibility in the time receptive field\n        self.visible_instance_set = set()\n        # Record all valid instance\n        self.instance_dict = {}\n        # Record translation and rotation of the ego-vehicle for the entire time period\n        self.egopose_list = []\n        # Generate input data\n        for self.counter, index_t in enumerate(self.indices[index]):\n            rec = self.ixes[index_t]\n\n            images, intrinsics, extrinsics = self.get_input_data(rec)\n            instance_map = self.record_instance(rec, instance_map)\n            future_egomotion = self.get_future_egomotion(rec, index_t)\n\n            data['image'].append(images)\n            data['intrinsics'].append(intrinsics)\n            data['extrinsics'].append(extrinsics)\n            data['future_egomotion'].append(future_egomotion)\n            data['sample_token'].append(rec['token'])\n\n        # Refine the generated instance polygons    \n        for token in self.instance_dict.keys():\n            self.instance_dict[token] = self.refine_instance_poly(self.instance_dict[token])\n\n        # The visible instance must have high visibility in the time receptive field\n        self.visible_instance_set = set()\n        # Generate instance ground truth\n        for self.counter in range(self.sequence_length):\n            segmentation, instance, z_position, attribute_label = self.get_label()\n            data['segmentation'].append(segmentation)\n            data['instance'].append(instance)\n            data['z_position'].append(z_position)\n            data['attribute'].append(attribute_label)\n\n        for key, value in data.items():\n            if key in ['sample_token', 'centerness', 'offset', 'flow']:\n                continue\n            data[key] = torch.cat(value, dim=0)\n\n        # Generate centripetal backward flow ground truth from the instance ground truth\n        data['flow'] = self.get_flow_label(data['instance'], instance_map, ignore_index=self.cfg.DATASET.IGNORE_INDEX)\n\n        #\u00a0If lyft need to subsample, and update future_egomotions\n        if self.cfg.MODEL.SUBSAMPLE:\n            for key, value in data.items():\n                if key in ['future_egomotion', 'sample_token', 'centerness', 'offset', 'flow']:\n                    continue\n                data[key] = data[key][::2].clone()\n            data['sample_token'] = data['sample_token'][::2]\n\n            # Update future egomotions\n            future_egomotions_matrix = pose_vec2mat(data['future_egomotion'])\n            future_egomotion_accum = torch.zeros_like(future_egomotions_matrix)\n            future_egomotion_accum[:-1] = future_egomotions_matrix[:-1] @ future_egomotions_matrix[1:]\n            future_egomotion_accum = mat2pose_vec(future_egomotion_accum)\n            data['future_egomotion'] = future_egomotion_accum[::2].clone()\n\n        # Generate the ground truth of centerness and offset\n        instance_centerness, instance_offset = convert_instance_mask_to_center_and_offset_label(\n            data['instance'], num_instances=len(instance_map), ignore_index=self.cfg.DATASET.IGNORE_INDEX,\n        )\n        data['centerness'] = instance_centerness\n        data['offset'] = instance_offset\n        data['num_instances'] = torch.tensor([len(instance_map)])\n\n        return data", "\n\nclass FuturePredictionDataset(torch.utils.data.Dataset):\n    def __init__(self, nusc, is_train, cfg):\n        self.nusc = nusc\n        self.is_train = is_train\n        self.cfg = cfg\n\n        self.is_lyft = isinstance(nusc, LyftDataset)\n\n        if self.is_lyft:\n            self.dataroot = self.nusc.data_path\n        else:\n            self.dataroot = self.nusc.dataroot\n\n        self.mode = 'train' if self.is_train else 'val'\n\n        self.sequence_length = cfg.TIME_RECEPTIVE_FIELD + cfg.N_FUTURE_FRAMES\n\n        self.scenes = self.get_scenes()\n        self.ixes = self.get_samples()\n        self.indices = self.get_indices()\n\n        # Image resizing and cropping\n        self.augmentation_parameters = self.get_resizing_and_cropping_parameters()\n\n        # Normalising input images\n        self.normalise_image = torchvision.transforms.Compose(\n            [torchvision.transforms.ToTensor(),\n             torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ]\n        )\n\n        # Bird's-eye view parameters\n        bev_resolution, bev_start_position, bev_dimension = calculate_birds_eye_view_parameters(\n            cfg.LIFT.X_BOUND, cfg.LIFT.Y_BOUND, cfg.LIFT.Z_BOUND\n        )\n        self.bev_resolution, self.bev_start_position, self.bev_dimension = (\n            bev_resolution.numpy(), bev_start_position.numpy(), bev_dimension.numpy()\n        )\n\n        # Spatial extent in bird's-eye view, in meters\n        self.spatial_extent = (self.cfg.LIFT.X_BOUND[1], self.cfg.LIFT.Y_BOUND[1])\n\n    def get_scenes(self):\n        \"\"\"\n        Obtain the list of scenes names in the given split.\n        \"\"\"\n        if self.is_lyft:\n            scenes = [row['name'] for row in self.nusc.scene]\n\n            # Split in train/val\n            indices = TRAIN_LYFT_INDICES if self.is_train else VAL_LYFT_INDICES\n            scenes = [scenes[i] for i in indices]\n        else:\n            # filter by scene split\n            split = {'v1.0-trainval': {True: 'train', False: 'val'},\n                     'v1.0-mini': {True: 'mini_train', False: 'mini_val'},}[\n                self.nusc.version\n            ][self.is_train]\n\n            scenes = create_splits_scenes()[split]\n\n        return scenes\n\n    def get_samples(self):\n        \"\"\"\n        Find and sort the samples in the given split by scene.\n        \"\"\"\n        samples = [sample for sample in self.nusc.sample]\n\n        # remove samples that aren't in this split\n        samples = [sample for sample in samples if self.nusc.get('scene', sample['scene_token'])['name'] in self.scenes]\n\n        # sort by scene, timestamp (only to make chronological viz easier)\n        samples.sort(key=lambda x: (x['scene_token'], x['timestamp']))\n\n        return samples\n\n    def get_indices(self):\n        \"\"\"\n        Group the sample indices by sequence length.\n        \"\"\"\n        indices = []\n        for index in range(len(self.ixes)):\n            is_valid_data = True\n            previous_rec = None\n            current_indices = []\n            for t in range(self.sequence_length):\n                index_t = index + t\n                # Going over the dataset size limit.\n                if index_t >= len(self.ixes):\n                    is_valid_data = False\n                    break\n                rec = self.ixes[index_t]\n                # Check if scene is the same\n                if (previous_rec is not None) and (rec['scene_token'] != previous_rec['scene_token']):\n                    is_valid_data = False\n                    break\n\n                current_indices.append(index_t)\n                previous_rec = rec\n\n            if is_valid_data:\n                indices.append(current_indices)\n\n        return np.asarray(indices)\n\n    def get_resizing_and_cropping_parameters(self):\n        \"\"\"\n        Determine the parameters for preprocessing of the input image, e.g. image size after scaling, cropping area.\n        \"\"\"\n        original_height, original_width = self.cfg.IMAGE.ORIGINAL_HEIGHT, self.cfg.IMAGE.ORIGINAL_WIDTH\n        final_height, final_width = self.cfg.IMAGE.FINAL_DIM\n\n        resize_scale = self.cfg.IMAGE.RESIZE_SCALE\n        resize_dims = (int(original_width * resize_scale), int(original_height * resize_scale))\n        resized_width, resized_height = resize_dims\n\n        crop_h = self.cfg.IMAGE.TOP_CROP\n        crop_w = int(max(0, (resized_width - final_width) / 2))\n        # Left, top, right, bottom crops.\n        crop = (crop_w, crop_h, crop_w + final_width, crop_h + final_height)\n\n        if resized_width != final_width:\n            print('Zero padding left and right parts of the image.')\n        if crop_h + final_height != resized_height:\n            print('Zero padding bottom part of the image.')\n\n        return {'scale_width': resize_scale,\n                'scale_height': resize_scale,\n                'resize_dims': resize_dims,\n                'crop': crop,\n                }\n\n    def get_input_data(self, rec):\n        \"\"\"\n        Obtain the input image as well as the intrinsics and extrinsics parameters of the corresponding camera.\n\n        Parameters\n        ----------\n            rec: nuscenes identifier for a given timestamp\n\n        Returns\n        -------\n            images: torch.Tensor<float> (N, 3, H, W)\n            intrinsics: torch.Tensor<float> (3, 3)\n            extrinsics: torch.Tensor(N, 4, 4)\n        \"\"\"\n        images = []\n        intrinsics = []\n        extrinsics = []\n        cameras = self.cfg.IMAGE.NAMES\n\n        #\u00a0The extrinsics we want are from the camera sensor to \"flat egopose\" as defined\n        # https://github.com/nutonomy/nuscenes-devkit/blob/9b492f76df22943daf1dc991358d3d606314af27/python-sdk/nuscenes/nuscenes.py#L279\n        # which corresponds to the position of the lidar.\n        # This is because the labels are generated by projecting the 3D bounding box in this lidar's reference frame.\n\n        # From lidar egopose to world.\n        lidar_sample = self.nusc.get('sample_data', rec['data']['LIDAR_TOP'])\n        lidar_pose = self.nusc.get('ego_pose', lidar_sample['ego_pose_token'])\n        yaw = Quaternion(lidar_pose['rotation']).yaw_pitch_roll[0]\n        lidar_rotation = Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)])\n        lidar_translation = np.array(lidar_pose['translation'])[:, None]\n        lidar_to_world = np.vstack([\n            np.hstack((lidar_rotation.rotation_matrix, lidar_translation)),\n            np.array([0, 0, 0, 1])\n        ])\n\n        for cam in cameras:\n            camera_sample = self.nusc.get('sample_data', rec['data'][cam])\n\n            # Transformation from world to egopose\n            car_egopose = self.nusc.get('ego_pose', camera_sample['ego_pose_token'])\n            egopose_rotation = Quaternion(car_egopose['rotation']).inverse\n            egopose_translation = -np.array(car_egopose['translation'])[:, None]\n            world_to_car_egopose = np.vstack([\n                np.hstack((egopose_rotation.rotation_matrix, egopose_rotation.rotation_matrix @ egopose_translation)),\n                np.array([0, 0, 0, 1])\n            ])\n\n            # From egopose to sensor\n            sensor_sample = self.nusc.get('calibrated_sensor', camera_sample['calibrated_sensor_token'])\n            intrinsic = torch.Tensor(sensor_sample['camera_intrinsic'])\n            sensor_rotation = Quaternion(sensor_sample['rotation'])\n            sensor_translation = np.array(sensor_sample['translation'])[:, None]\n            car_egopose_to_sensor = np.vstack([\n                np.hstack((sensor_rotation.rotation_matrix, sensor_translation)),\n                np.array([0, 0, 0, 1])\n            ])\n            car_egopose_to_sensor = np.linalg.inv(car_egopose_to_sensor)\n\n            # Combine all the transformation.\n            # From sensor to lidar.\n            lidar_to_sensor = car_egopose_to_sensor @ world_to_car_egopose @ lidar_to_world\n            sensor_to_lidar = torch.from_numpy(np.linalg.inv(lidar_to_sensor)).float()\n\n            # Load image\n            image_filename = os.path.join(self.dataroot, camera_sample['filename'])\n            img = Image.open(image_filename)\n            # Resize and crop\n            img = resize_and_crop_image(\n                img, resize_dims=self.augmentation_parameters['resize_dims'], crop=self.augmentation_parameters['crop']\n            )\n            # Normalise image\n            normalised_img = self.normalise_image(img)\n\n            # Combine resize/cropping in the intrinsics\n            top_crop = self.augmentation_parameters['crop'][1]\n            left_crop = self.augmentation_parameters['crop'][0]\n            intrinsic = update_intrinsics(\n                intrinsic, top_crop, left_crop,\n                scale_width=self.augmentation_parameters['scale_width'],\n                scale_height=self.augmentation_parameters['scale_height']\n            )\n\n            images.append(normalised_img.unsqueeze(0).unsqueeze(0))\n            intrinsics.append(intrinsic.unsqueeze(0).unsqueeze(0))\n            extrinsics.append(sensor_to_lidar.unsqueeze(0).unsqueeze(0))\n\n        images, intrinsics, extrinsics = (torch.cat(images, dim=1),\n                                          torch.cat(intrinsics, dim=1),\n                                          torch.cat(extrinsics, dim=1)\n                                          )\n\n        return images, intrinsics, extrinsics\n\n    def _get_top_lidar_pose(self, rec):\n        \"\"\"\n        Obtain the vehicle attitude at the current moment.\n        \"\"\"\n        egopose = self.nusc.get('ego_pose', self.nusc.get('sample_data', rec['data']['LIDAR_TOP'])['ego_pose_token'])\n        trans = -np.array(egopose['translation'])\n        yaw = Quaternion(egopose['rotation']).yaw_pitch_roll[0]\n        rot = Quaternion(scalar=np.cos(yaw / 2), vector=[0, 0, np.sin(yaw / 2)]).inverse\n        return trans, rot\n\n    def record_instance(self, rec, instance_map):\n        \"\"\"\n        Record information about each visible instance in the sequence and assign a unique ID to it.\n        \"\"\"\n        translation, rotation = self._get_top_lidar_pose(rec)\n        self.egopose_list.append([translation, rotation])\n\n        for annotation_token in rec['anns']:\n            \n            annotation = self.nusc.get('sample_annotation', annotation_token)\n\n            if not self.is_lyft:\n                # NuScenes filter\n                # Filter out all non vehicle instances\n                if 'vehicle' not in annotation['category_name']:\n                    continue\n                # Filter out invisible vehicles\n                if self.cfg.DATASET.FILTER_INVISIBLE_VEHICLES and int(annotation['visibility_token']) == 1 and annotation['instance_token'] not in self.visible_instance_set:\n                    continue\n                # Filter out vehicles that have not been seen in the past\n                if self.counter >= self.cfg.TIME_RECEPTIVE_FIELD and annotation['instance_token'] not in self.visible_instance_set:\n                    continue\n                self.visible_instance_set.add(annotation['instance_token'])\n            else:\n                # Lyft filter\n                # Filter out all non vehicle instances\n                if annotation['category_name'] not in ['bus', 'car', 'construction_vehicle', 'trailer', 'truck']:\n                    continue\n\n            if annotation['instance_token'] not in instance_map:\n                instance_map[annotation['instance_token']] = len(instance_map) + 1\n            instance_id = instance_map[annotation['instance_token']]\n\n            if not self.is_lyft:\n                instance_attribute = int(annotation['visibility_token'])\n            else:\n                instance_attribute = 0\n            \n            if annotation['instance_token'] not in self.instance_dict:\n                # For the first occurrence of an instance\n                self.instance_dict[annotation['instance_token']] = {\n                    'timestep': [self.counter],\n                    'translation': [annotation['translation']],\n                    'rotation': [annotation['rotation']],\n                    'size': annotation['size'],\n                    'instance_id': instance_id,\n                    'attribute_label': [instance_attribute],\n                }\n            else:\n                # For the instance that have appeared before\n                self.instance_dict[annotation['instance_token']]['timestep'].append(self.counter)\n                self.instance_dict[annotation['instance_token']]['translation'].append(annotation['translation'])\n                self.instance_dict[annotation['instance_token']]['rotation'].append(annotation['rotation'])\n                self.instance_dict[annotation['instance_token']]['attribute_label'].append(instance_attribute)\n\n        return instance_map\n    \n    @staticmethod\n    def _check_consistency(translation, prev_translation, threshold=1.0):\n        \"\"\"\n        Check for significant displacement of the instance adjacent moments.\n        \"\"\"\n        x, y = translation[:2]\n        prev_x, prev_y = prev_translation[:2]\n\n        if abs(x - prev_x) > threshold or abs(y - prev_y) > threshold:\n            return False\n        return True\n\n    def refine_instance_poly(self, instance):\n        \"\"\"\n        Fix the missing frames and disturbances of ground truth caused by noise.\n        \"\"\"\n        pointer = 1\n        for i in range(instance['timestep'][0] + 1, self.sequence_length):\n            # Fill in the missing frames\n            if i not in instance['timestep']:\n                instance['timestep'].insert(pointer, i)\n                instance['translation'].insert(pointer, instance['translation'][pointer-1])\n                instance['rotation'].insert(pointer, instance['rotation'][pointer-1])\n                instance['attribute_label'].insert(pointer, instance['attribute_label'][pointer-1])\n                pointer += 1\n                continue\n            \n            # Eliminate observation disturbances\n            if self._check_consistency(instance['translation'][pointer], instance['translation'][pointer-1]):\n                instance['translation'][pointer] = instance['translation'][pointer-1]\n                instance['rotation'][pointer] = instance['rotation'][pointer-1]\n                instance['attribute_label'][pointer] = instance['attribute_label'][pointer-1]\n            pointer += 1\n        \n        return instance\n\n    @staticmethod\n    def generate_flow(flow, instance_img, instance, instance_id):\n        \"\"\"\n        Generate ground truth for the flow of each instance based on instance segmentation.\n        \"\"\"\n        _, h, w = instance_img.shape\n        x, y = torch.meshgrid(torch.arange(h, dtype=torch.float), torch.arange(w, dtype=torch.float))\n        grid = torch.stack((x, y), dim=0)\n\n        # Set the first frame\n        instance_mask = (instance_img[0] == instance_id)\n        flow[0, 1, instance_mask] = grid[0, instance_mask].mean(dim=0, keepdim=True).round() - grid[0, instance_mask]\n        flow[0, 0, instance_mask] = grid[1, instance_mask].mean(dim=0, keepdim=True).round() - grid[1, instance_mask]\n\n        for i, timestep in enumerate(instance['timestep']):\n            if i == 0:\n                continue\n\n            instance_mask = (instance_img[timestep] == instance_id)\n            prev_instance_mask = (instance_img[timestep-1] == instance_id)\n            if instance_mask.sum() == 0 or prev_instance_mask.sum() == 0:\n                continue\n\n            # Centripetal backward flow is defined as displacement vector from each foreground pixel at time t to the object center of the associated instance identity at time t\u22121\n            flow[timestep, 1, instance_mask] = grid[0, prev_instance_mask].mean(dim=0, keepdim=True).round() - grid[0, instance_mask]\n            flow[timestep, 0, instance_mask] = grid[1, prev_instance_mask].mean(dim=0, keepdim=True).round() - grid[1, instance_mask]\n\n        return flow\n    \n    def get_flow_label(self, instance_img, instance_map, ignore_index=255):\n        \"\"\"\n        Generate the global map of the flow ground truth.\n        \"\"\"\n        seq_len, h, w = instance_img.shape\n        flow = ignore_index * torch.ones(seq_len, 2, h, w)\n\n        for token, instance in self.instance_dict.items():\n            flow = self.generate_flow(flow, instance_img, instance, instance_map[token])\n        return flow\n\n    def _get_poly_region_in_image(self, instance_annotation, present_egopose):\n        \"\"\"\n        Obtain the bounding box polygon of the instance.\n        \"\"\"\n        present_ego_translation, present_ego_rotation = present_egopose\n\n        box = Box(\n            instance_annotation['translation'], instance_annotation['size'], Quaternion(instance_annotation['rotation'])\n        )\n        box.translate(present_ego_translation)\n        box.rotate(present_ego_rotation)\n        pts = box.bottom_corners()[:2].T\n\n        if self.cfg.LIFT.X_BOUND[0] <= pts.min(axis=0)[0] and pts.max(axis=0)[0] <= self.cfg.LIFT.X_BOUND[1] and self.cfg.LIFT.Y_BOUND[0] <= pts.min(axis=0)[1] and pts.max(axis=0)[1] <= self.cfg.LIFT.Y_BOUND[1]:\n            pts = np.round((pts - self.bev_start_position[:2] + self.bev_resolution[:2] / 2.0) / self.bev_resolution[:2]).astype(np.int32)\n            pts[:, [1, 0]] = pts[:, [0, 1]]\n            z = box.bottom_corners()[2, 0]\n            return pts, z\n        else:\n            return None, None\n\n    def get_label(self):\n        \"\"\"\n        Generate labels for semantic segmentation, instance segmentation, z position, attribute from the raw data of nuScenes.\n        \"\"\"\n        timestep = self.counter\n        segmentation = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n        # Background is ID 0\n        instance = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n        z_position = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n        attribute_label = np.zeros((self.bev_dimension[0], self.bev_dimension[1]))\n        \n        for instance_token, instance_annotation in self.instance_dict.items():\n            if timestep not in instance_annotation['timestep']:\n                continue\n            pointer = instance_annotation['timestep'].index(timestep)\n            annotation = {\n                'translation': instance_annotation['translation'][pointer],\n                'rotation': instance_annotation['rotation'][pointer],\n                'size': instance_annotation['size'],\n            }\n            poly_region, z = self._get_poly_region_in_image(annotation, self.egopose_list[self.cfg.TIME_RECEPTIVE_FIELD - 1]) \n            if isinstance(poly_region, np.ndarray):\n                if self.counter >= self.cfg.TIME_RECEPTIVE_FIELD and instance_token not in self.visible_instance_set:\n                    continue\n                self.visible_instance_set.add(instance_token)\n\n                cv2.fillPoly(instance, [poly_region], instance_annotation['instance_id'])\n                cv2.fillPoly(segmentation, [poly_region], 1.0)\n                cv2.fillPoly(z_position, [poly_region], z)\n                cv2.fillPoly(attribute_label, [poly_region], instance_annotation['attribute_label'][pointer]) \n        \n        segmentation = torch.from_numpy(segmentation).long().unsqueeze(0).unsqueeze(0)\n        instance = torch.from_numpy(instance).long().unsqueeze(0)\n        z_position = torch.from_numpy(z_position).float().unsqueeze(0).unsqueeze(0)\n        attribute_label = torch.from_numpy(attribute_label).long().unsqueeze(0).unsqueeze(0)\n\n        return segmentation, instance, z_position, attribute_label\n\n    def get_future_egomotion(self, rec, index):\n        \"\"\"\n        Obtain the egomotion in the corresponding sequence from the raw data of nuScenes.\n        \"\"\"\n        rec_t0 = rec\n\n        # Identity\n        future_egomotion = np.eye(4, dtype=np.float32)\n\n        if index < len(self.ixes) - 1:\n            rec_t1 = self.ixes[index + 1]\n\n            if rec_t0['scene_token'] == rec_t1['scene_token']:\n                egopose_t0 = self.nusc.get(\n                    'ego_pose', self.nusc.get('sample_data', rec_t0['data']['LIDAR_TOP'])['ego_pose_token']\n                )\n                egopose_t1 = self.nusc.get(\n                    'ego_pose', self.nusc.get('sample_data', rec_t1['data']['LIDAR_TOP'])['ego_pose_token']\n                )\n\n                egopose_t0 = convert_egopose_to_matrix_numpy(egopose_t0)\n                egopose_t1 = convert_egopose_to_matrix_numpy(egopose_t1)\n\n                future_egomotion = invert_matrix_egopose_numpy(egopose_t1).dot(egopose_t0)\n                future_egomotion[3, :3] = 0.0\n                future_egomotion[3, 3] = 1.0\n\n        future_egomotion = torch.Tensor(future_egomotion).float()\n\n        # Convert to 6DoF vector\n        future_egomotion = mat2pose_vec(future_egomotion)\n        return future_egomotion.unsqueeze(0)\n    \n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns\n        -------\n            data: dict with the following keys:\n                image: torch.Tensor<float> (T, N, 3, H, W)\n                    normalised cameras images with T the sequence length, and N the number of cameras.\n                intrinsics: torch.Tensor<float> (T, N, 3, 3)\n                    intrinsics containing resizing and cropping parameters.\n                extrinsics: torch.Tensor<float> (T, N, 4, 4)\n                    6 DoF pose from world coordinates to camera coordinates.\n                segmentation: torch.Tensor<int64> (T, 1, H_bev, W_bev)\n                    (H_bev, W_bev) are the pixel dimensions in bird's-eye view.\n                instance: torch.Tensor<int64> (T, 1, H_bev, W_bev)\n                centerness: torch.Tensor<float> (T, 1, H_bev, W_bev)\n                offset: torch.Tensor<float> (T, 2, H_bev, W_bev)\n                flow: torch.Tensor<float> (T, 2, H_bev, W_bev)\n                future_egomotion: torch.Tensor<float> (T, 6)\n                    6 DoF egomotion t -> t+1\n                sample_token: List<str> (T,)\n                'z_position': list_z_position,\n                'attribute': list_attribute_label,\n\n        \"\"\"\n        data = {}\n        keys = ['image', 'intrinsics', 'extrinsics',\n                'segmentation', 'instance', 'centerness', 'offset', 'flow', 'future_egomotion',\n                'sample_token',\n                'z_position', 'attribute'\n                ]\n        for key in keys:\n            data[key] = []\n\n        instance_map = {}\n        # The visible instance must have high visibility in the time receptive field\n        self.visible_instance_set = set()\n        # Record all valid instance\n        self.instance_dict = {}\n        # Record translation and rotation of the ego-vehicle for the entire time period\n        self.egopose_list = []\n        # Generate input data\n        for self.counter, index_t in enumerate(self.indices[index]):\n            rec = self.ixes[index_t]\n\n            images, intrinsics, extrinsics = self.get_input_data(rec)\n            instance_map = self.record_instance(rec, instance_map)\n            future_egomotion = self.get_future_egomotion(rec, index_t)\n\n            data['image'].append(images)\n            data['intrinsics'].append(intrinsics)\n            data['extrinsics'].append(extrinsics)\n            data['future_egomotion'].append(future_egomotion)\n            data['sample_token'].append(rec['token'])\n\n        # Refine the generated instance polygons    \n        for token in self.instance_dict.keys():\n            self.instance_dict[token] = self.refine_instance_poly(self.instance_dict[token])\n\n        # The visible instance must have high visibility in the time receptive field\n        self.visible_instance_set = set()\n        # Generate instance ground truth\n        for self.counter in range(self.sequence_length):\n            segmentation, instance, z_position, attribute_label = self.get_label()\n            data['segmentation'].append(segmentation)\n            data['instance'].append(instance)\n            data['z_position'].append(z_position)\n            data['attribute'].append(attribute_label)\n\n        for key, value in data.items():\n            if key in ['sample_token', 'centerness', 'offset', 'flow']:\n                continue\n            data[key] = torch.cat(value, dim=0)\n\n        # Generate centripetal backward flow ground truth from the instance ground truth\n        data['flow'] = self.get_flow_label(data['instance'], instance_map, ignore_index=self.cfg.DATASET.IGNORE_INDEX)\n\n        #\u00a0If lyft need to subsample, and update future_egomotions\n        if self.cfg.MODEL.SUBSAMPLE:\n            for key, value in data.items():\n                if key in ['future_egomotion', 'sample_token', 'centerness', 'offset', 'flow']:\n                    continue\n                data[key] = data[key][::2].clone()\n            data['sample_token'] = data['sample_token'][::2]\n\n            # Update future egomotions\n            future_egomotions_matrix = pose_vec2mat(data['future_egomotion'])\n            future_egomotion_accum = torch.zeros_like(future_egomotions_matrix)\n            future_egomotion_accum[:-1] = future_egomotions_matrix[:-1] @ future_egomotions_matrix[1:]\n            future_egomotion_accum = mat2pose_vec(future_egomotion_accum)\n            data['future_egomotion'] = future_egomotion_accum[::2].clone()\n\n        # Generate the ground truth of centerness and offset\n        instance_centerness, instance_offset = convert_instance_mask_to_center_and_offset_label(\n            data['instance'], num_instances=len(instance_map), ignore_index=self.cfg.DATASET.IGNORE_INDEX,\n        )\n        data['centerness'] = instance_centerness\n        data['offset'] = instance_offset\n        data['num_instances'] = torch.tensor([len(instance_map)])\n\n        return data", "\n\ndef prepare_powerbev_dataloaders(cfg, return_dataset=False):\n    \"\"\"\n    Prepare the dataloader of PowerBEV.\n    \"\"\"\n    version = cfg.DATASET.VERSION\n    train_on_training_data = True\n\n    if cfg.DATASET.NAME == 'nuscenes':\n        # 28130 train and 6019 val\n        dataroot = os.path.join(cfg.DATASET.DATAROOT, version)\n        nusc = NuScenes(version='v1.0-{}'.format(cfg.DATASET.VERSION), dataroot=dataroot, verbose=False)\n    elif cfg.DATASET.NAME == 'lyft':\n        # train contains 22680 samples\n        # we split in 16506 6174\n        dataroot = os.path.join(cfg.DATASET.DATAROOT, 'trainval')\n        nusc = LyftDataset(data_path=dataroot,\n                           json_path=os.path.join(dataroot, 'train_data'),\n                           verbose=True)\n\n    train_data = FuturePredictionDataset(nusc, train_on_training_data, cfg)\n    val_data = FuturePredictionDataset(nusc, False, cfg)\n\n    if cfg.DATASET.VERSION == 'mini':\n        train_data.indices = train_data.indices[:10]\n        val_data.indices = val_data.indices[:10]\n\n    nworkers = cfg.N_WORKERS\n    train_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=cfg.BATCHSIZE, shuffle=True, num_workers=nworkers, pin_memory=True, drop_last=True\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_data, batch_size=1, shuffle=False, num_workers=nworkers, pin_memory=True, drop_last=False)\n\n    if return_dataset:\n        return train_loader, val_loader, train_data, val_data\n    else:\n        return train_loader, val_loader", ""]}
{"filename": "powerbev/trainer.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport time\n", "import time\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nfrom powerbev.config import get_cfg\nfrom powerbev.losses import SegmentationLoss, SpatialRegressionLoss\nfrom powerbev.metrics import IntersectionOverUnion, PanopticMetric\nfrom powerbev.models.powerbev import PowerBEV\nfrom powerbev.utils.instance import predict_instance_segmentation", "from powerbev.models.powerbev import PowerBEV\nfrom powerbev.utils.instance import predict_instance_segmentation\nfrom powerbev.utils.visualisation import visualise_output\nfrom thop import profile\n\n\nclass TrainingModule(pl.LightningModule):\n    def __init__(self, hparams):\n        super().__init__()\n        \n        # see config.py for details\n        self.hparams = hparams\n        # pytorch lightning does not support saving YACS CfgNone\n        cfg = get_cfg(cfg_dict=self.hparams)\n        self.cfg = cfg\n        self.n_classes = len(self.cfg.SEMANTIC_SEG.WEIGHTS)\n\n        # Bird's-eye view extent in meters\n        assert self.cfg.LIFT.X_BOUND[1] > 0 and self.cfg.LIFT.Y_BOUND[1] > 0\n        self.spatial_extent = (self.cfg.LIFT.X_BOUND[1], self.cfg.LIFT.Y_BOUND[1])\n\n        # Model\n        self.model = PowerBEV(cfg)\n        self.calculate_flops = True\n\n        # Losses\n        self.losses_fn = nn.ModuleDict()\n        self.losses_fn['segmentation'] = SegmentationLoss(\n            class_weights=torch.Tensor(self.cfg.SEMANTIC_SEG.WEIGHTS),\n            use_top_k=self.cfg.SEMANTIC_SEG.USE_TOP_K,\n            top_k_ratio=self.cfg.SEMANTIC_SEG.TOP_K_RATIO,\n            future_discount=self.cfg.FUTURE_DISCOUNT,\n        )\n        self.losses_fn['instance_flow'] = SpatialRegressionLoss(\n            norm=1.5, \n            future_discount=self.cfg.FUTURE_DISCOUNT, \n            ignore_index=self.cfg.DATASET.IGNORE_INDEX,\n        )\n\n        # Uncertainty weighting\n        self.model.segmentation_weight = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n        self.model.flow_weight = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n\n        # Metrics\n        self.metric_iou_val = IntersectionOverUnion(self.n_classes)\n        self.metric_panoptic_val = PanopticMetric(n_classes=self.n_classes)\n\n        self.training_step_count = 0\n\n        # Run time\n        self.perception_time, self.prediction_time, self.postprocessing_time = [], [], []\n\n    def shared_step(self, batch, is_train):\n        image = batch['image']\n        intrinsics = batch['intrinsics']\n        extrinsics = batch['extrinsics']\n        future_egomotion = batch['future_egomotion']\n\n        # Warp labels\n        labels, future_distribution_inputs = self.prepare_future_labels(batch)\n\n        # Calculate FLOPs\n        if self.calculate_flops:\n            flops, _ = profile(self.model, inputs=(image, intrinsics, extrinsics, future_egomotion, future_distribution_inputs))\n            print('{:.2f} G \\tTotal FLOPs'.format(flops/1000**3))\n            self.calculate_flops = False\n\n        # Forward pass\n        output = self.model(image, intrinsics, extrinsics, future_egomotion, future_distribution_inputs)\n\n        # Calculate loss\n        loss = self.calculate_loss(output, labels)\n\n        if not is_train:\n            # Perform warping-based pixel-level association\n            start_time = time.time()\n            pred_consistent_instance_seg = predict_instance_segmentation(output, spatial_extent=self.spatial_extent)\n            end_time = time.time()\n\n            # Calculate metrics\n            self.metric_iou_val(torch.argmax(output['segmentation'].detach(), dim=2, keepdims=True)[:, 1:], labels['segmentation'][:, 1:])\n            self.metric_panoptic_val(pred_consistent_instance_seg[:, 1:], labels['instance'][:, 1:])\n        \n            # Record run time\n            self.perception_time.append(output['perception_time'])\n            self.prediction_time.append(output['prediction_time'])\n            self.postprocessing_time.append(end_time-start_time)\n\n        return output, labels, loss\n\n    def calculate_loss(self, output, labels):\n        loss = {}\n\n        segmentation_factor = 100 / torch.exp(self.model.segmentation_weight)\n        loss['segmentation'] = segmentation_factor * self.losses_fn['segmentation'](\n            output['segmentation'], \n            labels['segmentation'], \n        )\n        loss[f'segmentation_uncertainty'] = 0.5 * self.model.segmentation_weight\n\n        flow_factor = 0.1 / (2*torch.exp(self.model.flow_weight))\n        loss['instance_flow'] = flow_factor * self.losses_fn['instance_flow'](\n            output['instance_flow'], \n            labels['flow']\n        )\n        loss['flow_uncertainty'] = 0.5 * self.model.flow_weight\n    \n        return loss\n\n    def prepare_future_labels(self, batch):\n        labels = {}\n        future_distribution_inputs = []\n\n        segmentation_labels = batch['segmentation']\n        instance_center_labels = batch['centerness']\n        instance_offset_labels = batch['offset']\n        instance_flow_labels = batch['flow']\n        gt_instance = batch['instance']\n\n        label_time_range = self.model.receptive_field - 2  # See section 3.4 in paper for details.\n\n        segmentation_labels = segmentation_labels[:, label_time_range:].long().contiguous()\n        labels['segmentation'] = segmentation_labels\n        future_distribution_inputs.append(segmentation_labels)\n\n        gt_instance = gt_instance[:, label_time_range:].long().contiguous()\n        labels['instance'] = gt_instance\n\n        instance_center_labels = instance_center_labels[:, label_time_range:].contiguous()\n        labels['centerness'] = instance_center_labels\n        future_distribution_inputs.append(instance_center_labels)\n\n        instance_offset_labels = instance_offset_labels[:, label_time_range:].contiguous()\n        labels['offset'] = instance_offset_labels\n        future_distribution_inputs.append(instance_offset_labels)\n\n        instance_flow_labels = instance_flow_labels[:, label_time_range:]\n        labels['flow'] = instance_flow_labels\n        future_distribution_inputs.append(instance_flow_labels)\n\n        if len(future_distribution_inputs) > 0:\n            future_distribution_inputs = torch.cat(future_distribution_inputs, dim=2)\n        \n        labels['future_egomotion'] = batch['future_egomotion']\n\n        return labels, future_distribution_inputs\n\n    def visualise(self, labels, output, batch_idx, prefix='train'):\n        visualisation_video = visualise_output(labels, output, self.cfg)\n        name = f'{prefix}_outputs'\n        if prefix == 'val':\n            name = name + f'_{batch_idx}'\n        self.logger.experiment.add_video(name, visualisation_video, global_step=self.training_step_count, fps=2)\n\n    def training_step(self, batch, batch_idx):\n        output, labels, loss = self.shared_step(batch, True)\n        self.training_step_count += 1\n        for key, value in loss.items():\n            self.logger.experiment.add_scalar('train_loss/' + key, value, global_step=self.training_step_count)\n        \n        if self.training_step_count % self.cfg.VIS_INTERVAL == 0:\n            self.visualise(labels, output, batch_idx, prefix='train')\n        return sum(loss.values())\n\n    def validation_step(self, batch, batch_idx):\n        output, labels, loss = self.shared_step(batch, False)\n        for key, value in loss.items():\n            self.log('val_loss/' + key, value)\n\n        if batch_idx == 0:\n            self.visualise(labels, output, batch_idx, prefix='val')\n    \n    def test_step(self, batch, batch_idx):\n        output, labels, loss = self.shared_step(batch, False)\n        for key, value in loss.items():\n            self.log('test_loss/' + key, value)\n\n        if batch_idx == 0:\n            self.visualise(labels, output, batch_idx, prefix='test')\n\n    def shared_epoch_end(self, step_outputs, is_train):\n        # Log per class iou metrics\n        class_names = ['background', 'dynamic']\n        if not is_train:\n            print(\"========================== Metrics ==========================\")\n            scores = self.metric_iou_val.compute()\n            \n            for key, value in zip(class_names, scores):\n                self.logger.experiment.add_scalar('metrics/val_iou_' + key, value, global_step=self.training_step_count)\n                print(f\"val_iou_{key}: {value}\")\n            self.metric_iou_val.reset()\n\n            scores = self.metric_panoptic_val.compute()\n\n            for key, value in scores.items():\n                for instance_name, score in zip(class_names, value):\n                    if instance_name != 'background':\n                        self.logger.experiment.add_scalar(f'metrics/val_{key}_{instance_name}', score.item(),\n                                                          global_step=self.training_step_count)\n                        print(f\"val_{key}_{instance_name}: {score.item()}\")\n                    # Log VPQ metric for the model checkpoint monitor \n                    if key == 'pq' and instance_name == 'dynamic':\n                        self.log('vpq', score.item())\n            self.metric_panoptic_val.reset()\n\n            print(\"========================== Runtime ==========================\")\n            perception_time = sum(self.perception_time) / (len(self.perception_time) + 1e-8)\n            prediction_time = sum(self.prediction_time) / (len(self.prediction_time) + 1e-8)\n            postprocessing_time = sum(self.postprocessing_time) / (len(self.postprocessing_time) + 1e-8)\n            print(f\"perception_time: {perception_time}\")\n            print(f\"prediction_time: {prediction_time}\")\n            print(f\"postprocessing_time: {postprocessing_time}\")\n            print(f\"total_time: {perception_time + prediction_time + postprocessing_time}\")\n            print(\"=============================================================\")\n            self.perception_time, self.prediction_time, self.postprocessing_time = [], [], []\n\n        self.logger.experiment.add_scalar('weights/segmentation_weight', 1 / (torch.exp(self.model.segmentation_weight)),\n                                          global_step=self.training_step_count)\n        self.logger.experiment.add_scalar('weights/flow_weight', 1 / (2 * torch.exp(self.model.flow_weight)),\n                                          global_step=self.training_step_count)\n\n    def training_epoch_end(self, step_outputs):\n        self.shared_epoch_end(step_outputs, True)\n\n    def validation_epoch_end(self, step_outputs):\n        self.shared_epoch_end(step_outputs, False)\n\n    def test_epoch_end(self, step_outputs):\n        self.shared_epoch_end(step_outputs, False)\n\n    def configure_optimizers(self):\n        params = self.model.parameters()\n        optimizer = torch.optim.Adam(\n            params, lr=self.cfg.OPTIMIZER.LR, weight_decay=self.cfg.OPTIMIZER.WEIGHT_DECAY\n        )\n\n        return optimizer"]}
{"filename": "powerbev/utils/instance.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nfrom typing import Tuple\n", "from typing import Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom powerbev.utils.geometry import flow_warp\nfrom scipy.optimize import linear_sum_assignment\n\n\n# set ignore index to 0 for vis\ndef convert_instance_mask_to_center_and_offset_label(instance_img, num_instances, ignore_index=255, sigma=3):\n    seq_len, h, w = instance_img.shape\n    center_label = torch.zeros(seq_len, 1, h, w)\n    offset_label = ignore_index * torch.ones(seq_len, 2, h, w)\n    # x is vertical displacement, y is horizontal displacement\n    x, y = torch.meshgrid(torch.arange(h, dtype=torch.float), torch.arange(w, dtype=torch.float))\n\n    # Ignore id 0 which is the background\n    for instance_id in range(1, num_instances+1):\n        for t in range(seq_len):\n            instance_mask = (instance_img[t] == instance_id)\n\n            xc = x[instance_mask].mean().round().long()\n            yc = y[instance_mask].mean().round().long()\n\n            off_x = xc - x\n            off_y = yc - y\n            g = torch.exp(-(off_x ** 2 + off_y ** 2) / sigma ** 2)\n            center_label[t, 0] = torch.maximum(center_label[t, 0], g)\n            offset_label[t, 0, instance_mask] = off_x[instance_mask]\n            offset_label[t, 1, instance_mask] = off_y[instance_mask]\n\n    return center_label, offset_label", "\n# set ignore index to 0 for vis\ndef convert_instance_mask_to_center_and_offset_label(instance_img, num_instances, ignore_index=255, sigma=3):\n    seq_len, h, w = instance_img.shape\n    center_label = torch.zeros(seq_len, 1, h, w)\n    offset_label = ignore_index * torch.ones(seq_len, 2, h, w)\n    # x is vertical displacement, y is horizontal displacement\n    x, y = torch.meshgrid(torch.arange(h, dtype=torch.float), torch.arange(w, dtype=torch.float))\n\n    # Ignore id 0 which is the background\n    for instance_id in range(1, num_instances+1):\n        for t in range(seq_len):\n            instance_mask = (instance_img[t] == instance_id)\n\n            xc = x[instance_mask].mean().round().long()\n            yc = y[instance_mask].mean().round().long()\n\n            off_x = xc - x\n            off_y = yc - y\n            g = torch.exp(-(off_x ** 2 + off_y ** 2) / sigma ** 2)\n            center_label[t, 0] = torch.maximum(center_label[t, 0], g)\n            offset_label[t, 0, instance_mask] = off_x[instance_mask]\n            offset_label[t, 1, instance_mask] = off_y[instance_mask]\n\n    return center_label, offset_label", "\n\ndef find_instance_centers(center_prediction: torch.Tensor, conf_threshold: float = 0.1, nms_kernel_size: float = 3):\n    assert len(center_prediction.shape) == 3\n    center_prediction = F.threshold(center_prediction, threshold=conf_threshold, value=-1)\n\n    nms_padding = (nms_kernel_size - 1) // 2\n    maxpooled_center_prediction = F.max_pool2d(\n        center_prediction, kernel_size=nms_kernel_size, stride=1, padding=nms_padding\n    )\n\n    # Filter all elements that are not the maximum (i.e. the center of the heatmap instance)\n    center_prediction[center_prediction != maxpooled_center_prediction] = -1\n    return torch.nonzero(center_prediction > 0)[:, 1:]", "\n\ndef group_pixels(centers: torch.Tensor, offset_predictions: torch.Tensor) -> torch.Tensor:\n    width, height = offset_predictions.shape[-2:]\n    x_grid = (\n        torch.arange(width, dtype=offset_predictions.dtype, device=offset_predictions.device)\n        .view(1, width, 1)\n        .repeat(1, 1, height)\n    )\n    y_grid = (\n        torch.arange(height, dtype=offset_predictions.dtype, device=offset_predictions.device)\n        .view(1, 1, height)\n        .repeat(1, width, 1)\n    )\n    pixel_grid = torch.cat((x_grid, y_grid), dim=0)\n    offset = torch.stack([offset_predictions[1], offset_predictions[0]], dim=0)\n    center_locations = (pixel_grid + offset).view(2, width * height, 1).permute(2, 1, 0)\n    centers = centers.view(-1, 1, 2)\n\n    distances = torch.norm(centers - center_locations, dim=-1)\n\n    instance_id = torch.argmin(distances, dim=0).reshape(1, width, height) + 1\n    return instance_id", "\n\ndef get_instance_segmentation_and_centers(\n    center_predictions: torch.Tensor,\n    offset_predictions: torch.Tensor,\n    foreground_mask: torch.Tensor,\n    conf_threshold: float = 0.1,\n    nms_kernel_size: float = 5,\n    max_n_instance_centers: int = 100,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    width, height = offset_predictions.shape[-2:]\n    center_predictions = center_predictions.view(1, width, height)\n    offset_predictions = offset_predictions.view(2, width, height)\n    foreground_mask = foreground_mask.view(1, width, height)\n\n    centers = find_instance_centers(center_predictions, conf_threshold=conf_threshold, nms_kernel_size=nms_kernel_size)\n    if not len(centers):\n        return torch.zeros(center_predictions.shape, dtype=torch.int64, device=center_predictions.device)\n\n    if len(centers) > max_n_instance_centers:\n        # print(f'There are a lot of detected instance centers: {centers.shape}')\n        centers = centers[:max_n_instance_centers].clone()\n\n    instance_ids = group_pixels(centers, offset_predictions * foreground_mask.float())\n    instance_seg = (instance_ids * foreground_mask.float()).long()\n\n    # Make the indices of instance_seg consecutive\n    instance_seg = make_instance_seg_consecutive(instance_seg)\n\n    return instance_seg.long()", "\n\ndef update_instance_ids(instance_seg, old_ids, new_ids):\n    \"\"\"\n    Parameters\n    ----------\n        instance_seg: torch.Tensor arbitrary shape\n        old_ids: 1D tensor containing the list of old ids, must be all present in instance_seg.\n        new_ids: 1D tensor with the new ids, aligned with old_ids\n\n    Returns\n        new_instance_seg: torch.Tensor same shape as instance_seg with new ids\n    \"\"\"\n    indices = torch.arange(old_ids.max() + 1, device=instance_seg.device)\n    for old_id, new_id in zip(old_ids, new_ids):\n        indices[old_id] = new_id\n\n    return indices[instance_seg].long()", "\n\ndef make_instance_seg_consecutive(instance_seg):\n    # Make the indices of instance_seg consecutive\n    unique_ids = torch.unique(instance_seg)\n    new_ids = torch.arange(len(unique_ids), device=instance_seg.device)\n    instance_seg = update_instance_ids(instance_seg, unique_ids, new_ids)\n    return instance_seg\n\n\ndef make_instance_id_temporally_consecutive(pred_inst, preds, backward_flow, ignore_index=255.0):\n    assert pred_inst.shape[0] == 1, 'Assumes batch size = 1'\n\n    # Initialise instance segmentations with prediction corresponding to the present\n    consistent_instance_seg = [pred_inst[:, 0:1]]\n    backward_flow = backward_flow.clone().detach()\n    backward_flow[backward_flow == ignore_index] = 0.0\n    _, seq_len, _, h, w = preds.shape\n\n    for t in range(1, seq_len):\n        init_warped_instance_seg = flow_warp(consistent_instance_seg[-1].unsqueeze(2).float(), backward_flow[:, t:t+1]).squeeze(2).int()\n\n        warped_instance_seg = init_warped_instance_seg * preds[:, t:t+1, 0]\n    \n        consistent_instance_seg.append(warped_instance_seg)\n    \n    consistent_instance_seg = torch.cat(consistent_instance_seg, dim=1)\n    return consistent_instance_seg", "\n\ndef make_instance_id_temporally_consecutive(pred_inst, preds, backward_flow, ignore_index=255.0):\n    assert pred_inst.shape[0] == 1, 'Assumes batch size = 1'\n\n    # Initialise instance segmentations with prediction corresponding to the present\n    consistent_instance_seg = [pred_inst[:, 0:1]]\n    backward_flow = backward_flow.clone().detach()\n    backward_flow[backward_flow == ignore_index] = 0.0\n    _, seq_len, _, h, w = preds.shape\n\n    for t in range(1, seq_len):\n        init_warped_instance_seg = flow_warp(consistent_instance_seg[-1].unsqueeze(2).float(), backward_flow[:, t:t+1]).squeeze(2).int()\n\n        warped_instance_seg = init_warped_instance_seg * preds[:, t:t+1, 0]\n    \n        consistent_instance_seg.append(warped_instance_seg)\n    \n    consistent_instance_seg = torch.cat(consistent_instance_seg, dim=1)\n    return consistent_instance_seg", "\n\ndef predict_instance_segmentation(output, compute_matched_centers=False,  vehicles_id=1, spatial_extent=[50, 50]):\n    preds = output['segmentation'].detach()\n    preds = torch.argmax(preds, dim=2, keepdims=True)\n    foreground_masks = preds.squeeze(2) == vehicles_id\n\n    batch_size, seq_len = preds.shape[:2]\n    pred_inst = []\n    for b in range(batch_size):\n        pred_inst_batch = get_instance_segmentation_and_centers(\n            torch.softmax(output['segmentation'], dim=2)[b, 0:1, vehicles_id].detach(),\n            output['instance_flow'][b, 1:2].detach(),\n            foreground_masks[b, 1:2].detach(),\n            nms_kernel_size=round(350/spatial_extent[0]),\n        )\n        pred_inst.append(pred_inst_batch)\n    pred_inst = torch.stack(pred_inst).squeeze(2)\n\n    if output['instance_flow'] is None:\n        print('Using zero flow because instance_future_output is None')\n        output['instance_flow'] = torch.zeros_like(output['instance_flow'])\n    consistent_instance_seg = []\n    for b in range(batch_size):\n        consistent_instance_seg.append(\n            make_instance_id_temporally_consecutive(\n                pred_inst[b:b+1],\n                preds[b:b+1, 1:],\n                output['instance_flow'][b:b+1, 1:].detach(),\n                )\n        )\n    consistent_instance_seg = torch.cat(consistent_instance_seg, dim=0)\n    consistent_instance_seg = torch.cat([torch.zeros_like(pred_inst), consistent_instance_seg], dim=1)\n\n    if compute_matched_centers:\n        assert batch_size == 1\n        # Generate trajectories\n        matched_centers = {}\n        _, seq_len, h, w = consistent_instance_seg.shape\n        grid = torch.stack(torch.meshgrid(\n            torch.arange(h, dtype=torch.float, device=preds.device),\n            torch.arange(w, dtype=torch.float, device=preds.device)\n        ))\n\n        for instance_id in torch.unique(consistent_instance_seg[0, 1])[1:].cpu().numpy():\n            for t in range(seq_len):\n                instance_mask = consistent_instance_seg[0, t] == instance_id\n                if instance_mask.sum() > 0:\n                    matched_centers[instance_id] = matched_centers.get(instance_id, []) + [\n                        grid[:, instance_mask].mean(dim=-1)]\n\n        for key, value in matched_centers.items():\n            matched_centers[key] = torch.stack(value).cpu().numpy()[:, ::-1]\n\n        return consistent_instance_seg, matched_centers\n\n    return consistent_instance_seg.long()", ""]}
{"filename": "powerbev/utils/network.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nimport torchvision\n\n\ndef pack_sequence_dim(x):\n    b, s = x.shape[:2]\n    return x.view(b * s, *x.shape[2:])\n\n\ndef unpack_sequence_dim(x, b, s):\n    return x.view(b, s, *x.shape[1:])", "\n\ndef unpack_sequence_dim(x, b, s):\n    return x.view(b, s, *x.shape[1:])\n\n\ndef preprocess_batch(batch, device, unsqueeze=False):\n    for key, value in batch.items():\n        if key != 'sample_token':\n            batch[key] = value.to(device)\n            if unsqueeze:\n                batch[key] = batch[key].unsqueeze(0)", "\n\ndef set_module_grad(module, requires_grad=False):\n    for p in module.parameters():\n        p.requires_grad = requires_grad\n\n\ndef set_bn_momentum(model, momentum=0.1):\n    for m in model.modules():\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n            m.momentum = momentum", "\n\nclass NormalizeInverse(torchvision.transforms.Normalize):\n    #  https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/8\n    def __init__(self, mean, std):\n        mean = torch.as_tensor(mean)\n        std = torch.as_tensor(std)\n        std_inv = 1 / (std + 1e-7)\n        mean_inv = -mean * std_inv\n        super().__init__(mean=mean_inv, std=std_inv)\n\n    def __call__(self, tensor):\n        return super().__call__(tensor.clone())", ""]}
{"filename": "powerbev/utils/visualisation.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport matplotlib.pylab\nimport numpy as np", "import matplotlib.pylab\nimport numpy as np\nimport torch\nfrom powerbev.utils.instance import predict_instance_segmentation\n\nDEFAULT_COLORMAP = matplotlib.pylab.cm.jet\n\n\ndef flow_to_image(flow: np.ndarray, autoscale: bool = False) -> np.ndarray:\n    \"\"\"\n    Applies colour map to flow which should be a 2 channel image tensor HxWx2. Returns a HxWx3 numpy image\n    Code adapted from: https://github.com/liruoteng/FlowNet/blob/master/models/flownet/scripts/flowlib.py\n    \"\"\"\n    u = flow[0, :, :]\n    v = flow[1, :, :]\n\n    # Convert to polar coordinates\n    rad = np.sqrt(u ** 2 + v ** 2)\n    maxrad = np.max(rad)\n\n    # Normalise flow maps\n    if autoscale:\n        u /= maxrad + np.finfo(float).eps\n        v /= maxrad + np.finfo(float).eps\n\n    # visualise flow with cmap\n    return np.uint8(compute_color(u, v) * 255)", "def flow_to_image(flow: np.ndarray, autoscale: bool = False) -> np.ndarray:\n    \"\"\"\n    Applies colour map to flow which should be a 2 channel image tensor HxWx2. Returns a HxWx3 numpy image\n    Code adapted from: https://github.com/liruoteng/FlowNet/blob/master/models/flownet/scripts/flowlib.py\n    \"\"\"\n    u = flow[0, :, :]\n    v = flow[1, :, :]\n\n    # Convert to polar coordinates\n    rad = np.sqrt(u ** 2 + v ** 2)\n    maxrad = np.max(rad)\n\n    # Normalise flow maps\n    if autoscale:\n        u /= maxrad + np.finfo(float).eps\n        v /= maxrad + np.finfo(float).eps\n\n    # visualise flow with cmap\n    return np.uint8(compute_color(u, v) * 255)", "\n\ndef _normalise(image: np.ndarray) -> np.ndarray:\n    lower = np.min(image)\n    delta = np.max(image) - lower\n    if delta == 0:\n        delta = 1\n    image = (image.astype(np.float32) - lower) / delta\n    return image\n", "\n\ndef apply_colour_map(\n    image: np.ndarray, cmap: matplotlib.colors.LinearSegmentedColormap = DEFAULT_COLORMAP, autoscale: bool = False\n) -> np.ndarray:\n    \"\"\"\n    Applies a colour map to the given 1 or 2 channel numpy image. if 2 channel, must be 2xHxW.\n    Returns a HxWx3 numpy image\n    \"\"\"\n    if image.ndim == 2 or (image.ndim == 3 and image.shape[0] == 1):\n        if image.ndim == 3:\n            image = image[0]\n        # grayscale scalar image\n        if autoscale:\n            image = _normalise(image)\n        return cmap(image)[:, :, :3]\n    if image.shape[0] == 2:\n        # 2 dimensional UV\n        return flow_to_image(image, autoscale=autoscale)\n    if image.shape[0] == 3:\n        # normalise rgb channels\n        if autoscale:\n            image = _normalise(image)\n        return np.transpose(image, axes=[1, 2, 0])\n    raise Exception('Image must be 1, 2 or 3 channel to convert to colour_map (CxHxW)')", "\n\ndef heatmap_image(\n    image: np.ndarray, cmap: matplotlib.colors.LinearSegmentedColormap = DEFAULT_COLORMAP, autoscale: bool = True\n) -> np.ndarray:\n    \"\"\"Colorize an 1 or 2 channel image with a colourmap.\"\"\"\n    if not issubclass(image.dtype.type, np.floating):\n        raise ValueError(f\"Expected a ndarray of float type, but got dtype {image.dtype}\")\n    if not (image.ndim == 2 or (image.ndim == 3 and image.shape[0] in [1, 2])):\n        raise ValueError(f\"Expected a ndarray of shape [H, W] or [1, H, W] or [2, H, W], but got shape {image.shape}\")\n    heatmap_np = apply_colour_map(image, cmap=cmap, autoscale=autoscale)\n    heatmap_np = np.uint8(heatmap_np * 255)\n    return heatmap_np", "\n\ndef compute_color(u: np.ndarray, v: np.ndarray) -> np.ndarray:\n    assert u.shape == v.shape\n    [h, w] = u.shape\n    img = np.zeros([h, w, 3])\n    nan_mask = np.isnan(u) | np.isnan(v)\n    u[nan_mask] = 0\n    v[nan_mask] = 0\n\n    colorwheel = make_color_wheel()\n    ncols = np.size(colorwheel, 0)\n\n    rad = np.sqrt(u ** 2 + v ** 2)\n    a = np.arctan2(-v, -u) / np.pi\n    f_k = (a + 1) / 2 * (ncols - 1) + 1\n    k_0 = np.floor(f_k).astype(int)\n    k_1 = k_0 + 1\n    k_1[k_1 == ncols + 1] = 1\n    f = f_k - k_0\n\n    for i in range(0, np.size(colorwheel, 1)):\n        tmp = colorwheel[:, i]\n        col0 = tmp[k_0 - 1] / 255\n        col1 = tmp[k_1 - 1] / 255\n        col = (1 - f) * col0 + f * col1\n\n        idx = rad <= 1\n        col[idx] = 1 - rad[idx] * (1 - col[idx])\n        notidx = np.logical_not(idx)\n\n        col[notidx] *= 0.75\n        img[:, :, i] = col * (1 - nan_mask)\n\n    return img", "\n\ndef make_color_wheel() -> np.ndarray:\n    \"\"\"\n    Create colour wheel.\n    Code adapted from https://github.com/liruoteng/FlowNet/blob/master/models/flownet/scripts/flowlib.py\n    \"\"\"\n    red_yellow = 15\n    yellow_green = 6\n    green_cyan = 4\n    cyan_blue = 11\n    blue_magenta = 13\n    magenta_red = 6\n\n    ncols = red_yellow + yellow_green + green_cyan + cyan_blue + blue_magenta + magenta_red\n    colorwheel = np.zeros([ncols, 3])\n\n    col = 0\n\n    # red_yellow\n    colorwheel[0:red_yellow, 0] = 255\n    colorwheel[0:red_yellow, 1] = np.transpose(np.floor(255 * np.arange(0, red_yellow) / red_yellow))\n    col += red_yellow\n\n    # yellow_green\n    colorwheel[col : col + yellow_green, 0] = 255 - np.transpose(\n        np.floor(255 * np.arange(0, yellow_green) / yellow_green)\n    )\n    colorwheel[col : col + yellow_green, 1] = 255\n    col += yellow_green\n\n    # green_cyan\n    colorwheel[col : col + green_cyan, 1] = 255\n    colorwheel[col : col + green_cyan, 2] = np.transpose(np.floor(255 * np.arange(0, green_cyan) / green_cyan))\n    col += green_cyan\n\n    # cyan_blue\n    colorwheel[col : col + cyan_blue, 1] = 255 - np.transpose(np.floor(255 * np.arange(0, cyan_blue) / cyan_blue))\n    colorwheel[col : col + cyan_blue, 2] = 255\n    col += cyan_blue\n\n    # blue_magenta\n    colorwheel[col : col + blue_magenta, 2] = 255\n    colorwheel[col : col + blue_magenta, 0] = np.transpose(np.floor(255 * np.arange(0, blue_magenta) / blue_magenta))\n    col += +blue_magenta\n\n    # magenta_red\n    colorwheel[col : col + magenta_red, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, magenta_red) / magenta_red))\n    colorwheel[col : col + magenta_red, 0] = 255\n\n    return colorwheel", "\n\ndef make_contour(img, colour=[0, 0, 0], double_line=False):\n    h, w = img.shape[:2]\n    out = img.copy()\n    # Vertical lines\n    out[np.arange(h), np.repeat(0, h)] = colour\n    out[np.arange(h), np.repeat(w - 1, h)] = colour\n\n    # Horizontal lines\n    out[np.repeat(0, w), np.arange(w)] = colour\n    out[np.repeat(h - 1, w), np.arange(w)] = colour\n\n    if double_line:\n        out[np.arange(h), np.repeat(1, h)] = colour\n        out[np.arange(h), np.repeat(w - 2, h)] = colour\n\n        # Horizontal lines\n        out[np.repeat(1, w), np.arange(w)] = colour\n        out[np.repeat(h - 2, w), np.arange(w)] = colour\n    return out", "\n\ndef plot_instance_map(instance_image, instance_map, instance_colours=None, bg_image=None):\n    if isinstance(instance_image, torch.Tensor):\n        instance_image = instance_image.cpu().numpy()\n    assert isinstance(instance_image, np.ndarray)\n    if instance_colours is None:\n        instance_colours = generate_instance_colours(instance_map)\n    if len(instance_image.shape) > 2:\n        instance_image = instance_image.reshape((instance_image.shape[-2], instance_image.shape[-1]))\n\n    if bg_image is None:\n        plot_image = 255 * np.ones((instance_image.shape[0], instance_image.shape[1], 3), dtype=np.uint8)\n    else:\n        plot_image = bg_image\n\n    for key, value in instance_colours.items():\n        plot_image[instance_image == key] = value\n\n    return plot_image", "\n\ndef visualise_output(labels, output, cfg):\n    semantic_colours = np.array([[255, 255, 255], [0, 0, 0]], dtype=np.uint8)\n    consistent_instance_seg = predict_instance_segmentation(output, spatial_extent=(cfg.LIFT.X_BOUND[1], cfg.LIFT.Y_BOUND[1]))\n\n    sequence_length = consistent_instance_seg.shape[1]\n    b = 0\n    video = []\n    for t in range(1, sequence_length):\n        out_t = []\n        # Ground truth\n        unique_ids = torch.unique(labels['instance'][b, t]).cpu().numpy()[1:]\n        instance_map = dict(zip(unique_ids, unique_ids))\n        instance_plot = plot_instance_map(labels['instance'][b, t].cpu(), instance_map)[::-1, ::-1]\n        instance_plot = make_contour(instance_plot)\n\n        semantic_seg = labels['segmentation'].squeeze(2).cpu().numpy()\n        semantic_plot = semantic_colours[semantic_seg[b, t][::-1, ::-1]]\n        semantic_plot = make_contour(semantic_plot)\n\n        future_flow_plot = labels['flow'][b, t].cpu().numpy()\n        future_flow_plot[:, semantic_seg[b, t] != 1] = 0\n        future_flow_plot = flow_to_image(future_flow_plot)[::-1, ::-1]\n        future_flow_plot = make_contour(future_flow_plot)\n\n        out_t.append(np.concatenate([instance_plot, future_flow_plot, semantic_plot], axis=0))\n\n        # Predictions\n        unique_ids = torch.unique(consistent_instance_seg[b, t]).cpu().numpy()[1:]\n        instance_map = dict(zip(unique_ids, unique_ids))\n        instance_plot = plot_instance_map(consistent_instance_seg[b, t].cpu(), instance_map)[::-1, ::-1]\n        instance_plot = make_contour(instance_plot)\n\n        semantic_seg = torch.argmax(output['segmentation'], dim=2, keepdims=True)\n        semantic_seg = semantic_seg.squeeze(2).detach().cpu().numpy()\n        semantic_plot = semantic_colours[semantic_seg[b, t][::-1, ::-1]]\n        semantic_plot = make_contour(semantic_plot)\n\n        future_flow_plot = output['instance_flow'][b, t].detach().cpu().numpy()\n        future_flow_plot[:, semantic_seg[b, t] != 1] = 0\n        future_flow_plot = flow_to_image(future_flow_plot)[::-1, ::-1]\n        future_flow_plot = make_contour(future_flow_plot)\n\n        out_t.append(np.concatenate([instance_plot, future_flow_plot, semantic_plot], axis=0))\n        out_t = np.concatenate(out_t, axis=1)\n        # Shape (C, H, W)\n        out_t = out_t.transpose((2, 0, 1))\n\n        video.append(out_t)\n\n    # Shape (B, T, C, H, W)\n    video = np.stack(video)[None]\n    return video", "\n\ndef convert_figure_numpy(figure):\n    \"\"\" Convert figure to numpy image \"\"\"\n    figure_np = np.frombuffer(figure.canvas.tostring_rgb(), dtype=np.uint8)\n    figure_np = figure_np.reshape(figure.canvas.get_width_height()[::-1] + (3,))\n    return figure_np\n\n\ndef generate_instance_colours(instance_map):\n    # Most distinct 22 colors (kelly colors from https://stackoverflow.com/questions/470690/how-to-automatically-generate\n    # -n-distinct-colors)\n    # plus some colours from AD40k\n    INSTANCE_COLOURS = np.asarray([\n        [0, 0, 0],\n        [255, 179, 0],\n        [128, 62, 117],\n        [255, 104, 0],\n        [166, 189, 215],\n        [193, 0, 32],\n        [206, 162, 98],\n        [129, 112, 102],\n        [0, 125, 52],\n        [246, 118, 142],\n        [0, 83, 138],\n        [255, 122, 92],\n        [83, 55, 122],\n        [255, 142, 0],\n        [179, 40, 81],\n        [244, 200, 0],\n        [127, 24, 13],\n        [147, 170, 0],\n        [89, 51, 21],\n        [241, 58, 19],\n        [35, 44, 22],\n        [112, 224, 255],\n        [70, 184, 160],\n        [153, 0, 255],\n        [71, 255, 0],\n        [255, 0, 163],\n        [255, 204, 0],\n        [0, 255, 235],\n        [255, 0, 235],\n        [255, 0, 122],\n        [255, 245, 0],\n        [10, 190, 212],\n        [214, 255, 0],\n        [0, 204, 255],\n        [20, 0, 255],\n        [255, 255, 0],\n        [0, 153, 255],\n        [0, 255, 204],\n        [41, 255, 0],\n        [173, 0, 255],\n        [0, 245, 255],\n        [71, 0, 255],\n        [0, 255, 184],\n        [0, 92, 255],\n        [184, 255, 0],\n        [255, 214, 0],\n        [25, 194, 194],\n        [92, 0, 255],\n        [220, 220, 220],\n        [255, 9, 92],\n        [112, 9, 255],\n        [8, 255, 214],\n        [255, 184, 6],\n        [10, 255, 71],\n        [255, 41, 10],\n        [7, 255, 255],\n        [224, 255, 8],\n        [102, 8, 255],\n        [255, 61, 6],\n        [255, 194, 7],\n        [0, 255, 20],\n        [255, 8, 41],\n        [255, 5, 153],\n        [6, 51, 255],\n        [235, 12, 255],\n        [160, 150, 20],\n        [0, 163, 255],\n        [140, 140, 140],\n        [250, 10, 15],\n        [20, 255, 0],\n    ])\n\n    return {instance_id: INSTANCE_COLOURS[global_instance_id % len(INSTANCE_COLOURS)] for\n            instance_id, global_instance_id in instance_map.items()\n            }", "\ndef generate_instance_colours(instance_map):\n    # Most distinct 22 colors (kelly colors from https://stackoverflow.com/questions/470690/how-to-automatically-generate\n    # -n-distinct-colors)\n    # plus some colours from AD40k\n    INSTANCE_COLOURS = np.asarray([\n        [0, 0, 0],\n        [255, 179, 0],\n        [128, 62, 117],\n        [255, 104, 0],\n        [166, 189, 215],\n        [193, 0, 32],\n        [206, 162, 98],\n        [129, 112, 102],\n        [0, 125, 52],\n        [246, 118, 142],\n        [0, 83, 138],\n        [255, 122, 92],\n        [83, 55, 122],\n        [255, 142, 0],\n        [179, 40, 81],\n        [244, 200, 0],\n        [127, 24, 13],\n        [147, 170, 0],\n        [89, 51, 21],\n        [241, 58, 19],\n        [35, 44, 22],\n        [112, 224, 255],\n        [70, 184, 160],\n        [153, 0, 255],\n        [71, 255, 0],\n        [255, 0, 163],\n        [255, 204, 0],\n        [0, 255, 235],\n        [255, 0, 235],\n        [255, 0, 122],\n        [255, 245, 0],\n        [10, 190, 212],\n        [214, 255, 0],\n        [0, 204, 255],\n        [20, 0, 255],\n        [255, 255, 0],\n        [0, 153, 255],\n        [0, 255, 204],\n        [41, 255, 0],\n        [173, 0, 255],\n        [0, 245, 255],\n        [71, 0, 255],\n        [0, 255, 184],\n        [0, 92, 255],\n        [184, 255, 0],\n        [255, 214, 0],\n        [25, 194, 194],\n        [92, 0, 255],\n        [220, 220, 220],\n        [255, 9, 92],\n        [112, 9, 255],\n        [8, 255, 214],\n        [255, 184, 6],\n        [10, 255, 71],\n        [255, 41, 10],\n        [7, 255, 255],\n        [224, 255, 8],\n        [102, 8, 255],\n        [255, 61, 6],\n        [255, 194, 7],\n        [0, 255, 20],\n        [255, 8, 41],\n        [255, 5, 153],\n        [6, 51, 255],\n        [235, 12, 255],\n        [160, 150, 20],\n        [0, 163, 255],\n        [140, 140, 140],\n        [250, 10, 15],\n        [20, 255, 0],\n    ])\n\n    return {instance_id: INSTANCE_COLOURS[global_instance_id % len(INSTANCE_COLOURS)] for\n            instance_id, global_instance_id in instance_map.items()\n            }", ""]}
{"filename": "powerbev/utils/lyft_splits.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\n\nTRAIN_LYFT_INDICES = [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16,", "\nTRAIN_LYFT_INDICES = [1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16,\n                      17, 18, 19, 20, 21, 23, 24, 27, 28, 29, 30, 31, 32,\n                      33, 35, 36, 37, 39, 41, 43, 44, 45, 46, 47, 48, 49,\n                      50, 51, 52, 53, 55, 56, 59, 60, 62, 63, 65, 68, 69,\n                      70, 71, 72, 73, 74, 75, 76, 78, 79, 81, 82, 83, 84,\n                      86, 87, 88, 89, 93, 95, 97, 98, 99, 103, 104, 107, 108,\n                      109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 121, 122, 124,\n                      127, 128, 130, 131, 132, 134, 135, 136, 137, 138, 139, 143, 144,\n                      146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159,", "                      127, 128, 130, 131, 132, 134, 135, 136, 137, 138, 139, 143, 144,\n                      146, 147, 148, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159,\n                      161, 162, 165, 166, 167, 171, 172, 173, 174, 175, 176, 177, 178,\n                      179]\n\nVAL_LYFT_INDICES = [0, 2, 4, 13, 22, 25, 26, 34, 38, 40, 42, 54, 57,\n                    58, 61, 64, 66, 67, 77, 80, 85, 90, 91, 92, 94, 96,\n                    100, 101, 102, 105, 106, 112, 120, 123, 125, 126, 129, 133, 140,\n                    141, 142, 145, 155, 160, 163, 164, 168, 169, 170]\n", "                    141, 142, 145, 155, 160, 163, 164, 168, 169, 170]\n"]}
{"filename": "powerbev/utils/geometry.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport numpy as np\nimport PIL", "import numpy as np\nimport PIL\nimport torch\nimport torch.nn.functional as F\nfrom pyquaternion import Quaternion\n\n\ndef resize_and_crop_image(img, resize_dims, crop):\n    # Bilinear resizing followed by cropping\n    img = img.resize(resize_dims, resample=PIL.Image.BILINEAR)\n    img = img.crop(crop)\n    return img", "\n\ndef update_intrinsics(intrinsics, top_crop=0.0, left_crop=0.0, scale_width=1.0, scale_height=1.0):\n    \"\"\"\n    Parameters\n    ----------\n        intrinsics: torch.Tensor (3, 3)\n        top_crop: float\n        left_crop: float\n        scale_width: float\n        scale_height: float\n    \"\"\"\n    updated_intrinsics = intrinsics.clone()\n    # Adjust intrinsics scale due to resizing\n    updated_intrinsics[0, 0] *= scale_width\n    updated_intrinsics[0, 2] *= scale_width\n    updated_intrinsics[1, 1] *= scale_height\n    updated_intrinsics[1, 2] *= scale_height\n\n    # Adjust principal point due to cropping\n    updated_intrinsics[0, 2] -= left_crop\n    updated_intrinsics[1, 2] -= top_crop\n\n    return updated_intrinsics", "\n\ndef calculate_birds_eye_view_parameters(x_bounds, y_bounds, z_bounds):\n    \"\"\"\n    Parameters\n    ----------\n        x_bounds: Forward direction in the ego-car.\n        y_bounds: Sides\n        z_bounds: Height\n\n    Returns\n    -------\n        bev_resolution: Bird's-eye view bev_resolution\n        bev_start_position Bird's-eye view first element\n        bev_dimension Bird's-eye view tensor spatial dimension\n    \"\"\"\n    bev_resolution = torch.tensor([row[2] for row in [x_bounds, y_bounds, z_bounds]])\n    bev_start_position = torch.tensor([row[0] + row[2] / 2.0 for row in [x_bounds, y_bounds, z_bounds]])\n    bev_dimension = torch.tensor([(row[1] - row[0]) / row[2] for row in [x_bounds, y_bounds, z_bounds]],\n                                 dtype=torch.long)\n\n    return bev_resolution, bev_start_position, bev_dimension", "\n\ndef convert_egopose_to_matrix_numpy(egopose):\n    transformation_matrix = np.zeros((4, 4), dtype=np.float32)\n    rotation = Quaternion(egopose['rotation']).rotation_matrix\n    translation = np.array(egopose['translation'])\n    transformation_matrix[:3, :3] = rotation\n    transformation_matrix[:3, 3] = translation\n    transformation_matrix[3, 3] = 1.0\n    return transformation_matrix", "\n\ndef invert_matrix_egopose_numpy(egopose):\n    \"\"\" Compute the inverse transformation of a 4x4 egopose numpy matrix.\"\"\"\n    inverse_matrix = np.zeros((4, 4), dtype=np.float32)\n    rotation = egopose[:3, :3]\n    translation = egopose[:3, 3]\n    inverse_matrix[:3, :3] = rotation.T\n    inverse_matrix[:3, 3] = -np.dot(rotation.T, translation)\n    inverse_matrix[3, 3] = 1.0\n    return inverse_matrix", "\n\ndef mat2pose_vec(matrix: torch.Tensor):\n    \"\"\"\n    Converts a 4x4 pose matrix into a 6-dof pose vector\n    Args:\n        matrix (ndarray): 4x4 pose matrix\n    Returns:\n        vector (ndarray): 6-dof pose vector comprising translation components (tx, ty, tz) and\n        rotation components (rx, ry, rz)\n    \"\"\"\n\n    # M[1, 2] = -sinx*cosy, M[2, 2] = +cosx*cosy\n    rotx = torch.atan2(-matrix[..., 1, 2], matrix[..., 2, 2])\n\n    # M[0, 2] = +siny, M[1, 2] = -sinx*cosy, M[2, 2] = +cosx*cosy\n    cosy = torch.sqrt(matrix[..., 1, 2] ** 2 + matrix[..., 2, 2] ** 2)\n    roty = torch.atan2(matrix[..., 0, 2], cosy)\n\n    # M[0, 0] = +cosy*cosz, M[0, 1] = -cosy*sinz\n    rotz = torch.atan2(-matrix[..., 0, 1], matrix[..., 0, 0])\n\n    rotation = torch.stack((rotx, roty, rotz), dim=-1)\n\n    # Extract translation params\n    translation = matrix[..., :3, 3]\n    return torch.cat((translation, rotation), dim=-1)", "\n\ndef euler2mat(angle: torch.Tensor):\n    \"\"\"Convert euler angles to rotation matrix.\n    Reference: https://github.com/pulkitag/pycaffe-utils/blob/master/rot_utils.py#L174\n    Args:\n        angle: rotation angle along 3 axis (in radians) [Bx3]\n    Returns:\n        Rotation matrix corresponding to the euler angles [Bx3x3]\n    \"\"\"\n    shape = angle.shape\n    angle = angle.view(-1, 3)\n    x, y, z = angle[:, 0], angle[:, 1], angle[:, 2]\n\n    cosz = torch.cos(z)\n    sinz = torch.sin(z)\n\n    zeros = torch.zeros_like(z)\n    ones = torch.ones_like(z)\n    zmat = torch.stack([cosz, -sinz, zeros, sinz, cosz, zeros, zeros, zeros, ones], dim=1).view(-1, 3, 3)\n\n    cosy = torch.cos(y)\n    siny = torch.sin(y)\n\n    ymat = torch.stack([cosy, zeros, siny, zeros, ones, zeros, -siny, zeros, cosy], dim=1).view(-1, 3, 3)\n\n    cosx = torch.cos(x)\n    sinx = torch.sin(x)\n\n    xmat = torch.stack([ones, zeros, zeros, zeros, cosx, -sinx, zeros, sinx, cosx], dim=1).view(-1, 3, 3)\n\n    rot_mat = xmat.bmm(ymat).bmm(zmat)\n    rot_mat = rot_mat.view(*shape[:-1], 3, 3)\n    return rot_mat", "\n\ndef pose_vec2mat(vec: torch.Tensor):\n    \"\"\"\n    Convert 6DoF parameters to transformation matrix.\n    Args:\n        vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz [B,6]\n    Returns:\n        A transformation matrix [B,4,4]\n    \"\"\"\n    translation = vec[..., :3].unsqueeze(-1)  # [...x3x1]\n    rot = vec[..., 3:].contiguous()  # [...x3]\n    rot_mat = euler2mat(rot)  # [...,3,3]\n    transform_mat = torch.cat([rot_mat, translation], dim=-1)  # [...,3,4]\n    transform_mat = torch.nn.functional.pad(transform_mat, [0, 0, 0, 1], value=0)  # [...,4,4]\n    transform_mat[..., 3, 3] = 1.0\n    return transform_mat", "\n\ndef invert_pose_matrix(x):\n    \"\"\"\n    Parameters\n    ----------\n        x: [B, 4, 4] batch of pose matrices\n\n    Returns\n    -------\n        out: [B, 4, 4] batch of inverse pose matrices\n    \"\"\"\n    assert len(x.shape) == 3 and x.shape[1:] == (4, 4), 'Only works for batch of pose matrices.'\n\n    transposed_rotation = torch.transpose(x[:, :3, :3], 1, 2)\n    translation = x[:, :3, 3:]\n\n    inverse_mat = torch.cat([transposed_rotation, -torch.bmm(transposed_rotation, translation)], dim=-1) # [B,3,4]\n    inverse_mat = torch.nn.functional.pad(inverse_mat, [0, 0, 0, 1], value=0)  # [B,4,4]\n    inverse_mat[..., 3, 3] = 1.0\n    return inverse_mat", "\n\ndef warp_features(x, flow, mode='nearest', spatial_extent=None):\n    \"\"\" Applies a rotation and translation to feature map x.\n        Args:\n            x: (b, c, h, w) feature map\n            flow: (b, 6) 6DoF vector (only uses the xy poriton)\n            mode: use 'nearest' when dealing with categorical inputs\n        Returns:\n            in plane transformed feature map\n        \"\"\"\n    if flow is None:\n        return x\n    b, c, h, w = x.shape\n    # z-rotation\n    angle = flow[:, 5].clone()  # torch.atan2(flow[:, 1, 0], flow[:, 0, 0])\n    # x-y translation\n    translation = flow[:, :2].clone()  # flow[:, :2, 3]\n\n    # Normalise translation. Need to divide by how many meters is half of the image.\n    # because translation of 1.0 correspond to translation of half of the image.\n    translation[:, 0] /= spatial_extent[0]\n    translation[:, 1] /= spatial_extent[1]\n    # forward axis is inverted\n    translation[:, 0] *= -1\n\n    cos_theta = torch.cos(angle)\n    sin_theta = torch.sin(angle)\n\n    # output = Rot.input + translation\n    # tx and ty are inverted as is the case when going from real coordinates to numpy coordinates\n    # translation_pos_0 -> positive value makes the image move to the left\n    # translation_pos_1 -> positive value makes the image move to the top\n    # Angle -> positive value in rad makes the image move in the trigonometric way\n    transformation = torch.stack([cos_theta, -sin_theta, translation[:, 1],\n                                  sin_theta, cos_theta, translation[:, 0]], dim=-1).view(b, 2, 3)\n\n    # Note that a rotation will preserve distances only if height = width. Otherwise there's\n    # resizing going on. e.g. rotation of pi/2 of a 100x200 image will make what's in the center of the image\n    # elongated.\n    grid = torch.nn.functional.affine_grid(transformation, size=x.shape, align_corners=False)\n    warped_x = torch.nn.functional.grid_sample(x, grid.float(), mode=mode, padding_mode='zeros', align_corners=False)\n\n    return warped_x", "\n\ndef cumulative_warp_features(x, flow, mode='nearest', spatial_extent=None):\n    \"\"\" Warps a sequence of feature maps by accumulating incremental 2d flow.\n\n    x[:, -1] remains unchanged\n    x[:, -2] is warped using flow[:, -2]\n    x[:, -3] is warped using flow[:, -3] @ flow[:, -2]\n    ...\n    x[:, 0] is warped using flow[:, 0] @ ... @ flow[:, -3] @ flow[:, -2]\n\n    Args:\n        x: (b, t, c, h, w) sequence of feature maps\n        flow: (b, t, 6) sequence of 6 DoF pose\n            from t to t+1 (only uses the xy poriton)\n\n    \"\"\"\n    sequence_length = x.shape[1]\n    if sequence_length == 1:\n        return x\n\n    flow = pose_vec2mat(flow)\n\n    out = [x[:, -1]]\n    cum_flow = flow[:, -2]\n    for t in reversed(range(sequence_length - 1)):\n        out.append(warp_features(x[:, t], mat2pose_vec(cum_flow), mode=mode, spatial_extent=spatial_extent))\n        # @ is the equivalent of torch.bmm\n        cum_flow = flow[:, t - 1] @ cum_flow\n\n    return torch.stack(out[::-1], 1)", "\n\ndef cumulative_warp_features_reverse(x, flow, mode='nearest', spatial_extent=None):\n    \"\"\" Warps a sequence of feature maps by accumulating incremental 2d flow.\n\n    x[:, 0] remains unchanged\n    x[:, 1] is warped using flow[:, 0].inverse()\n    x[:, 2] is warped using flow[:, 0].inverse() @ flow[:, 1].inverse()\n    ...\n\n    Args:\n        x: (b, t, c, h, w) sequence of feature maps\n        flow: (b, t, 6) sequence of 6 DoF pose\n            from t to t+1 (only uses the xy poriton)\n\n    \"\"\"\n    flow = pose_vec2mat(flow)\n\n    out = [x[:,0]]\n    \n    for i in range(1, x.shape[1]):\n        if i==1:\n            cum_flow = invert_pose_matrix(flow[:, 0])\n        else:\n            cum_flow = cum_flow @ invert_pose_matrix(flow[:,i-1])\n        out.append( warp_features(x[:,i], mat2pose_vec(cum_flow), mode, spatial_extent=spatial_extent))\n    return torch.stack(out, 1)", "\n\ndef flow_warp(occupancy, flow, mode='nearest', padding_mode='zeros'):\n    \"\"\"Warps ground-truth flow-origin occupancies according to predicted flows.\n\n    Performs bilinear interpolation and samples from 4 pixels for each flow\n    vector.\n\n    Args:\n      occupancy: occupancy as float32 tensors with the shape of BxTx1xHxW\n      flow: flow as float32 tensors with the shape of BxTx2xHxW\n      mode: mode of grid sample\n\n    Returns:\n      warped_occupancy: occupancy grids for vehicles as float32 tensors with the shape of BxTx1xHxW.\n\n    Note: the flow must always be 1 timestep ahead of the corresponding occupancy\n    \"\"\"\n    _, num_waypoints, _, grid_height_cells, grid_width_cells = occupancy.size()\n\n    h = torch.linspace(-1, 1, steps=grid_height_cells)\n    w = torch.linspace(-1, 1, steps=grid_width_cells)\n    h_idx, w_idx = torch.meshgrid(h, w)\n    # These indices map each (x, y) location to the pixel (x, y).\n    identity_indices = torch.stack((w_idx, h_idx), dim=0).to(device=occupancy.device)  # 2xHxW, storing x, y coordinates.\n\n    warped_occupancy = []\n    for k in range(num_waypoints):\n        flow_origin_occupancy = occupancy[:, k]  # BxTx1xHxW -> Bx1xHxW\n        pred_flow = flow[:, k]  # BxTx2xHxW -> Bx2xHxW\n        # Normalize along the width and height direction\n        normalize_pred_flow = torch.stack(\n            (2.0 * pred_flow[:, 0] / (grid_width_cells - 1),  \n            2.0 * pred_flow[:, 1] / (grid_height_cells - 1)),\n            dim=1,\n        )\n        # Shift the identity grid indices according to predicted flow tells us\n        # the source (origin) grid cell for each flow vector. We simply sample\n        # occupancy values from these locations.\n        warped_indices = identity_indices + normalize_pred_flow  # Bx2xHxW\n        warped_indices = warped_indices.permute(0, 2, 3, 1)  # Bx2xHxW -> BxHxWx2\n        sampled_occupancy = F.grid_sample(\n            input=flow_origin_occupancy,\n            grid=warped_indices,\n            mode=mode,\n            padding_mode='zeros',\n            align_corners=True,\n        )\n        warped_occupancy.append(sampled_occupancy)\n    warped_occupancy = torch.stack(warped_occupancy, dim=1)\n    return warped_occupancy", "\nclass VoxelsSumming(torch.autograd.Function):\n    \"\"\"Adapted from https://github.com/nv-tlabs/lift-splat-shoot/blob/master/src/tools.py#L193\"\"\"\n    @staticmethod\n    def forward(ctx, x, geometry, ranks):\n        \"\"\"The features `x` and `geometry` are ranked by voxel positions.\"\"\"\n        # Cumulative sum of all features.\n        x = x.cumsum(0)\n\n        # Indicates the change of voxel.\n        mask = torch.ones(x.shape[0], device=x.device, dtype=torch.bool)\n        mask[:-1] = ranks[1:] != ranks[:-1]\n\n        x, geometry = x[mask], geometry[mask]\n        # Calculate sum of features within a voxel.\n        x = torch.cat((x[:1], x[1:] - x[:-1]))\n\n        ctx.save_for_backward(mask)\n        ctx.mark_non_differentiable(geometry)\n\n        return x, geometry\n\n    @staticmethod\n    def backward(ctx, grad_x, grad_geometry):\n        (mask,) = ctx.saved_tensors\n        # Since the operation is summing, we simply need to send gradient\n        # to all elements that were part of the summation process.\n        indices = torch.cumsum(mask, 0)\n        indices[mask] -= 1\n\n        output_grad = grad_x[indices]\n\n        return output_grad, None, None", ""]}
{"filename": "powerbev/layers/temporal.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nfrom collections import OrderedDict\n", "from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nfrom powerbev.layers.convolutions import ConvBlock\nfrom powerbev.utils.geometry import warp_features\n\n\nclass SpatialGRU(nn.Module):\n    \"\"\"A GRU cell that takes an input tensor [BxTxCxHxW] and an optional previous state and passes a\n    convolutional gated recurrent unit over the data\"\"\"\n\n    def __init__(self, input_size, hidden_size, gru_bias_init=0.0, norm='bn', activation='relu'):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.gru_bias_init = gru_bias_init\n\n        self.conv_update = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, bias=True, padding=1)\n        self.conv_reset = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, bias=True, padding=1)\n\n        self.conv_state_tilde = ConvBlock(\n            input_size + hidden_size, hidden_size, kernel_size=3, bias=False, norm=norm, activation=activation\n        )\n\n    def forward(self, x, state=None, flow=None, mode='bilinear'):\n        # pylint: disable=unused-argument, arguments-differ\n        # Check size\n        assert len(x.size()) == 5, 'Input tensor must be BxTxCxHxW.'\n        b, timesteps, c, h, w = x.size()\n        assert c == self.input_size, f'feature sizes must match, got input {c} for layer with size {self.input_size}'\n\n        # recurrent layers\n        rnn_output = []\n        rnn_state = torch.zeros(b, self.hidden_size, h, w, device=x.device) if state is None else state\n        for t in range(timesteps):\n            x_t = x[:, t]\n            if flow is not None:\n                rnn_state = warp_features(rnn_state, flow[:, t], mode=mode)\n\n            # propagate rnn state\n            rnn_state = self.gru_cell(x_t, rnn_state)\n            rnn_output.append(rnn_state)\n\n        # reshape rnn output to batch tensor\n        return torch.stack(rnn_output, dim=1)\n\n    def gru_cell(self, x, state):\n        # Compute gates\n        x_and_state = torch.cat([x, state], dim=1)\n        update_gate = self.conv_update(x_and_state)\n        reset_gate = self.conv_reset(x_and_state)\n        # Add bias to initialise gate as close to identity function\n        update_gate = torch.sigmoid(update_gate + self.gru_bias_init)\n        reset_gate = torch.sigmoid(reset_gate + self.gru_bias_init)\n\n        # Compute proposal state, activation is defined in norm_act_config (can be tanh, ReLU etc)\n        state_tilde = self.conv_state_tilde(torch.cat([x, (1.0 - reset_gate) * state], dim=1))\n\n        output = (1.0 - update_gate) * state + update_gate * state_tilde\n        return output", "class SpatialGRU(nn.Module):\n    \"\"\"A GRU cell that takes an input tensor [BxTxCxHxW] and an optional previous state and passes a\n    convolutional gated recurrent unit over the data\"\"\"\n\n    def __init__(self, input_size, hidden_size, gru_bias_init=0.0, norm='bn', activation='relu'):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.gru_bias_init = gru_bias_init\n\n        self.conv_update = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, bias=True, padding=1)\n        self.conv_reset = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size=3, bias=True, padding=1)\n\n        self.conv_state_tilde = ConvBlock(\n            input_size + hidden_size, hidden_size, kernel_size=3, bias=False, norm=norm, activation=activation\n        )\n\n    def forward(self, x, state=None, flow=None, mode='bilinear'):\n        # pylint: disable=unused-argument, arguments-differ\n        # Check size\n        assert len(x.size()) == 5, 'Input tensor must be BxTxCxHxW.'\n        b, timesteps, c, h, w = x.size()\n        assert c == self.input_size, f'feature sizes must match, got input {c} for layer with size {self.input_size}'\n\n        # recurrent layers\n        rnn_output = []\n        rnn_state = torch.zeros(b, self.hidden_size, h, w, device=x.device) if state is None else state\n        for t in range(timesteps):\n            x_t = x[:, t]\n            if flow is not None:\n                rnn_state = warp_features(rnn_state, flow[:, t], mode=mode)\n\n            # propagate rnn state\n            rnn_state = self.gru_cell(x_t, rnn_state)\n            rnn_output.append(rnn_state)\n\n        # reshape rnn output to batch tensor\n        return torch.stack(rnn_output, dim=1)\n\n    def gru_cell(self, x, state):\n        # Compute gates\n        x_and_state = torch.cat([x, state], dim=1)\n        update_gate = self.conv_update(x_and_state)\n        reset_gate = self.conv_reset(x_and_state)\n        # Add bias to initialise gate as close to identity function\n        update_gate = torch.sigmoid(update_gate + self.gru_bias_init)\n        reset_gate = torch.sigmoid(reset_gate + self.gru_bias_init)\n\n        # Compute proposal state, activation is defined in norm_act_config (can be tanh, ReLU etc)\n        state_tilde = self.conv_state_tilde(torch.cat([x, (1.0 - reset_gate) * state], dim=1))\n\n        output = (1.0 - update_gate) * state + update_gate * state_tilde\n        return output", "\n\nclass CausalConv3d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=(2, 3, 3), dilation=(1, 1, 1), bias=False):\n        super().__init__()\n        assert len(kernel_size) == 3, 'kernel_size must be a 3-tuple.'\n        time_pad = (kernel_size[0] - 1) * dilation[0]\n        height_pad = ((kernel_size[1] - 1) * dilation[1]) // 2\n        width_pad = ((kernel_size[2] - 1) * dilation[2]) // 2\n\n        # Pad temporally on the left\n        self.pad = nn.ConstantPad3d(padding=(width_pad, width_pad, height_pad, height_pad, time_pad, 0), value=0)\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, dilation=dilation, stride=1, padding=0, bias=bias)\n        self.norm = nn.BatchNorm3d(out_channels)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, *inputs):\n        (x,) = inputs\n        x = self.pad(x)\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.activation(x)\n        return x", "\n\nclass CausalMaxPool3d(nn.Module):\n    def __init__(self, kernel_size=(2, 3, 3)):\n        super().__init__()\n        assert len(kernel_size) == 3, 'kernel_size must be a 3-tuple.'\n        time_pad = kernel_size[0] - 1\n        height_pad = (kernel_size[1] - 1) // 2\n        width_pad = (kernel_size[2] - 1) // 2\n\n        # Pad temporally on the left\n        self.pad = nn.ConstantPad3d(padding=(width_pad, width_pad, height_pad, height_pad, time_pad, 0), value=0)\n        self.max_pool = nn.MaxPool3d(kernel_size, stride=1)\n\n    def forward(self, *inputs):\n        (x,) = inputs\n        x = self.pad(x)\n        x = self.max_pool(x)\n        return x", "\n\ndef conv_1x1x1_norm_activated(in_channels, out_channels):\n    \"\"\"1x1x1 3D convolution, normalization and activation layer.\"\"\"\n    return nn.Sequential(\n        OrderedDict(\n            [\n                ('conv', nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False)),\n                ('norm', nn.BatchNorm3d(out_channels)),\n                ('activation', nn.ReLU(inplace=True)),\n            ]\n        )\n    )", "\n\nclass Bottleneck3D(nn.Module):\n    \"\"\"\n    Defines a bottleneck module with a residual connection\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels=None, kernel_size=(2, 3, 3), dilation=(1, 1, 1)):\n        super().__init__()\n        bottleneck_channels = in_channels // 2\n        out_channels = out_channels or in_channels\n\n        self.layers = nn.Sequential(\n            OrderedDict(\n                [\n                    # First projection with 1x1 kernel\n                    ('conv_down_project', conv_1x1x1_norm_activated(in_channels, bottleneck_channels)),\n                    # Second conv block\n                    (\n                        'conv',\n                        CausalConv3d(\n                            bottleneck_channels,\n                            bottleneck_channels,\n                            kernel_size=kernel_size,\n                            dilation=dilation,\n                            bias=False,\n                        ),\n                    ),\n                    # Final projection with 1x1 kernel\n                    ('conv_up_project', conv_1x1x1_norm_activated(bottleneck_channels, out_channels)),\n                ]\n            )\n        )\n\n        if out_channels != in_channels:\n            self.projection = nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n                nn.BatchNorm3d(out_channels),\n            )\n        else:\n            self.projection = None\n\n    def forward(self, *args):\n        (x,) = args\n        x_residual = self.layers(x)\n        x_features = self.projection(x) if self.projection is not None else x\n        return x_residual + x_features", "\n\nclass PyramidSpatioTemporalPooling(nn.Module):\n    \"\"\" Spatio-temporal pyramid pooling.\n        Performs 3D average pooling followed by 1x1x1 convolution to reduce the number of channels and upsampling.\n        Setting contains a list of kernel_size: usually it is [(2, h, w), (2, h//2, w//2), (2, h//4, w//4)]\n    \"\"\"\n\n    def __init__(self, in_channels, reduction_channels, pool_sizes):\n        super().__init__()\n        self.features = []\n        for pool_size in pool_sizes:\n            assert pool_size[0] == 2, (\n                \"Time kernel should be 2 as PyTorch raises an error when\" \"padding with more than half the kernel size\"\n            )\n            stride = (1, *pool_size[1:])\n            padding = (pool_size[0] - 1, 0, 0)\n            self.features.append(\n                nn.Sequential(\n                    OrderedDict(\n                        [\n                            # Pad the input tensor but do not take into account zero padding into the average.\n                            (\n                                'avgpool',\n                                torch.nn.AvgPool3d(\n                                    kernel_size=pool_size, stride=stride, padding=padding, count_include_pad=False\n                                ),\n                            ),\n                            ('conv_bn_relu', conv_1x1x1_norm_activated(in_channels, reduction_channels)),\n                        ]\n                    )\n                )\n            )\n        self.features = nn.ModuleList(self.features)\n\n    def forward(self, *inputs):\n        (x,) = inputs\n        b, _, t, h, w = x.shape\n        # Do not include current tensor when concatenating\n        out = []\n        for f in self.features:\n            # Remove unnecessary padded values (time dimension) on the right\n            x_pool = f(x)[:, :, :-1].contiguous()\n            c = x_pool.shape[1]\n            x_pool = nn.functional.interpolate(\n                x_pool.view(b * t, c, *x_pool.shape[-2:]), (h, w), mode='bilinear', align_corners=False\n            )\n            x_pool = x_pool.view(b, c, t, h, w)\n            out.append(x_pool)\n        out = torch.cat(out, 1)\n        return out", "\n\nclass TemporalBlock(nn.Module):\n    \"\"\" Temporal block with the following layers:\n        - 2x3x3, 1x3x3, spatio-temporal pyramid pooling\n        - dropout\n        - skip connection.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels=None, use_pyramid_pooling=False, pool_sizes=None):\n        super().__init__()\n        self.in_channels = in_channels\n        self.half_channels = in_channels // 2\n        self.out_channels = out_channels or self.in_channels\n        self.kernels = [(2, 3, 3), (1, 3, 3)]\n\n        # Flag for spatio-temporal pyramid pooling\n        self.use_pyramid_pooling = use_pyramid_pooling\n\n        # 3 convolution paths: 2x3x3, 1x3x3, 1x1x1\n        self.convolution_paths = []\n        for kernel_size in self.kernels:\n            self.convolution_paths.append(\n                nn.Sequential(\n                    conv_1x1x1_norm_activated(self.in_channels, self.half_channels),\n                    CausalConv3d(self.half_channels, self.half_channels, kernel_size=kernel_size),\n                )\n            )\n        self.convolution_paths.append(conv_1x1x1_norm_activated(self.in_channels, self.half_channels))\n        self.convolution_paths = nn.ModuleList(self.convolution_paths)\n\n        agg_in_channels = len(self.convolution_paths) * self.half_channels\n\n        if self.use_pyramid_pooling:\n            assert pool_sizes is not None, \"setting must contain the list of kernel_size, but is None.\"\n            reduction_channels = self.in_channels // 3\n            self.pyramid_pooling = PyramidSpatioTemporalPooling(self.in_channels, reduction_channels, pool_sizes)\n            agg_in_channels += len(pool_sizes) * reduction_channels\n\n        # Feature aggregation\n        self.aggregation = nn.Sequential(\n            conv_1x1x1_norm_activated(agg_in_channels, self.out_channels),)\n\n        if self.out_channels != self.in_channels:\n            self.projection = nn.Sequential(\n                nn.Conv3d(self.in_channels, self.out_channels, kernel_size=1, bias=False),\n                nn.BatchNorm3d(self.out_channels),\n            )\n        else:\n            self.projection = None\n\n    def forward(self, *inputs):\n        (x,) = inputs\n        x_paths = []\n        for conv in self.convolution_paths:\n            x_paths.append(conv(x))\n        x_residual = torch.cat(x_paths, dim=1)\n        if self.use_pyramid_pooling:\n            x_pool = self.pyramid_pooling(x)\n            x_residual = torch.cat([x_residual, x_pool], dim=1)\n        x_residual = self.aggregation(x_residual)\n\n        if self.out_channels != self.in_channels:\n            x = self.projection(x)\n        x = x + x_residual\n        return x", ""]}
{"filename": "powerbev/layers/convolutions.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nfrom collections import OrderedDict\nfrom functools import partial", "from collections import OrderedDict\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ConvBlock(nn.Module):\n    \"\"\"2D convolution followed by\n         - an optional normalisation (batch norm or instance norm)\n         - an optional activation (ReLU, LeakyReLU, or tanh)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        kernel_size=3,\n        stride=1,\n        norm='bn',\n        activation='relu',\n        bias=False,\n        transpose=False,\n    ):\n        super().__init__()\n        out_channels = out_channels or in_channels\n        padding = int((kernel_size - 1) / 2)\n        self.conv = nn.Conv2d if not transpose else partial(nn.ConvTranspose2d, output_padding=1)\n        self.conv = self.conv(in_channels, out_channels, kernel_size, stride, padding=padding, bias=bias)\n\n        if norm == 'bn':\n            self.norm = nn.BatchNorm2d(out_channels)\n        elif norm == 'in':\n            self.norm = nn.InstanceNorm2d(out_channels)\n        elif norm == 'none':\n            self.norm = None\n        else:\n            raise ValueError('Invalid norm {}'.format(norm))\n\n        if activation == 'relu':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == 'lrelu':\n            self.activation = nn.LeakyReLU(0.1, inplace=True)\n        elif activation == 'elu':\n            self.activation = nn.ELU(inplace=True)\n        elif activation == 'tanh':\n            self.activation = nn.Tanh(inplace=True)\n        elif activation == 'none':\n            self.activation = None\n        else:\n            raise ValueError('Invalid activation {}'.format(activation))\n\n    def forward(self, x):\n        x = self.conv(x)\n\n        if self.norm:\n            x = self.norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x", "class ConvBlock(nn.Module):\n    \"\"\"2D convolution followed by\n         - an optional normalisation (batch norm or instance norm)\n         - an optional activation (ReLU, LeakyReLU, or tanh)\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        kernel_size=3,\n        stride=1,\n        norm='bn',\n        activation='relu',\n        bias=False,\n        transpose=False,\n    ):\n        super().__init__()\n        out_channels = out_channels or in_channels\n        padding = int((kernel_size - 1) / 2)\n        self.conv = nn.Conv2d if not transpose else partial(nn.ConvTranspose2d, output_padding=1)\n        self.conv = self.conv(in_channels, out_channels, kernel_size, stride, padding=padding, bias=bias)\n\n        if norm == 'bn':\n            self.norm = nn.BatchNorm2d(out_channels)\n        elif norm == 'in':\n            self.norm = nn.InstanceNorm2d(out_channels)\n        elif norm == 'none':\n            self.norm = None\n        else:\n            raise ValueError('Invalid norm {}'.format(norm))\n\n        if activation == 'relu':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == 'lrelu':\n            self.activation = nn.LeakyReLU(0.1, inplace=True)\n        elif activation == 'elu':\n            self.activation = nn.ELU(inplace=True)\n        elif activation == 'tanh':\n            self.activation = nn.Tanh(inplace=True)\n        elif activation == 'none':\n            self.activation = None\n        else:\n            raise ValueError('Invalid activation {}'.format(activation))\n\n    def forward(self, x):\n        x = self.conv(x)\n\n        if self.norm:\n            x = self.norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x", "\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Defines a bottleneck module with a residual connection\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        kernel_size=3,\n        dilation=1,\n        groups=1,\n        upsample=False,\n        downsample=False,\n        dropout=0.0,\n    ):\n        super().__init__()\n        self._downsample = downsample\n        bottleneck_channels = int(in_channels / 2)\n        out_channels = out_channels or in_channels\n        padding_size = ((kernel_size - 1) * dilation + 1) // 2\n\n        # Define the main conv operation\n        assert dilation == 1\n        if upsample:\n            assert not downsample, 'downsample and upsample not possible simultaneously.'\n            bottleneck_conv = nn.ConvTranspose2d(\n                bottleneck_channels,\n                bottleneck_channels,\n                kernel_size=kernel_size,\n                bias=False,\n                dilation=1,\n                stride=2,\n                output_padding=padding_size,\n                padding=padding_size,\n                groups=groups,\n            )\n        elif downsample:\n            bottleneck_conv = nn.Conv2d(\n                bottleneck_channels,\n                bottleneck_channels,\n                kernel_size=kernel_size,\n                bias=False,\n                dilation=dilation,\n                stride=2,\n                padding=padding_size,\n                groups=groups,\n            )\n        else:\n            bottleneck_conv = nn.Conv2d(\n                bottleneck_channels,\n                bottleneck_channels,\n                kernel_size=kernel_size,\n                bias=False,\n                dilation=dilation,\n                padding=padding_size,\n                groups=groups,\n            )\n\n        self.layers = nn.Sequential(\n            OrderedDict(\n                [\n                    # First projection with 1x1 kernel\n                    ('conv_down_project', nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, bias=False)),\n                    ('abn_down_project', nn.Sequential(nn.BatchNorm2d(bottleneck_channels),\n                                                       nn.ReLU(inplace=True))),\n                    # Second conv block\n                    ('conv', bottleneck_conv),\n                    ('abn', nn.Sequential(nn.BatchNorm2d(bottleneck_channels), nn.ReLU(inplace=True))),\n                    # Final projection with 1x1 kernel\n                    ('conv_up_project', nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, bias=False)),\n                    ('abn_up_project', nn.Sequential(nn.BatchNorm2d(out_channels),\n                                                     nn.ReLU(inplace=True))),\n                    # Regulariser\n                    ('dropout', nn.Dropout2d(p=dropout)),\n                ]\n            )\n        )\n\n        if out_channels == in_channels and not downsample and not upsample:\n            self.projection = None\n        else:\n            projection = OrderedDict()\n            if upsample:\n                projection.update({'upsample_skip_proj': Interpolate(scale_factor=2)})\n            elif downsample:\n                projection.update({'upsample_skip_proj': nn.MaxPool2d(kernel_size=2, stride=2)})\n            projection.update(\n                {\n                    'conv_skip_proj': nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                    'bn_skip_proj': nn.BatchNorm2d(out_channels),\n                }\n            )\n            self.projection = nn.Sequential(projection)\n\n    # pylint: disable=arguments-differ\n    def forward(self, *args):\n        (x,) = args\n        x_residual = self.layers(x)\n        if self.projection is not None:\n            if self._downsample:\n                # pad h/w dimensions if they are odd to prevent shape mismatch with residual layer\n                x = nn.functional.pad(x, (0, x.shape[-1] % 2, 0, x.shape[-2] % 2), value=0)\n            return x_residual + self.projection(x)\n        return x_residual + x", "\n\nclass Interpolate(nn.Module):\n    def __init__(self, scale_factor: int = 2):\n        super().__init__()\n        self._interpolate = nn.functional.interpolate\n        self._scale_factor = scale_factor\n\n    # pylint: disable=arguments-differ\n    def forward(self, x):\n        return self._interpolate(x, scale_factor=self._scale_factor, mode='bilinear', align_corners=False)", "\n\nclass UpsamplingConcat(nn.Module):\n    def __init__(self, in_channels, out_channels, scale_factor=2):\n        super().__init__()\n\n        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x_to_upsample, x):\n        x_to_upsample = self.upsample(x_to_upsample)\n        x_to_upsample = torch.cat([x, x_to_upsample], dim=1)\n        return self.conv(x_to_upsample)", "\n\nclass UpsamplingAdd(nn.Module):\n    def __init__(self, in_channels, out_channels, scale_factor=2):\n        super().__init__()\n        self.upsample_layer = nn.Sequential(\n            nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n\n    def forward(self, x, x_skip):\n        x = self.upsample_layer(x)\n        return x + x_skip", ""]}
{"filename": "powerbev/layers/spatial_temporal.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nfrom collections import OrderedDict\n", "from collections import OrderedDict\n\nfrom torch import nn\n\n\nclass Residual(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        kernel_size=3,\n        dilation=1,\n        upsample=False,\n        downsample=False,\n    ):\n        super().__init__()\n        self._downsample = downsample\n        out_channels = out_channels or in_channels\n        padding_size = ((kernel_size - 1) * dilation + 1) // 2\n\n        if upsample:\n            assert not downsample, 'downsample and upsample not possible simultaneously.'\n            conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, bias=False, dilation=1, stride=2, output_padding=padding_size, padding=padding_size)\n        elif downsample:\n            conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=False, dilation=dilation, stride=2, padding=padding_size)\n        else:\n            conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=False, dilation=dilation, padding=padding_size)\n        \n        self.layers = nn.Sequential(conv, nn.BatchNorm2d(out_channels), nn.LeakyReLU(inplace=True))\n\n        if out_channels == in_channels and not downsample and not upsample:\n            self.projection = None\n        else:\n            projection = OrderedDict()\n            if upsample:\n                projection.update({'upsample_skip_proj': nn.Upsample(scale_factor=2, mode='bilinear')})\n            elif downsample:\n                projection.update({'upsample_skip_proj': nn.MaxPool2d(kernel_size=2, stride=2)})\n            projection.update(\n                {\n                    'conv_skip_proj': nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                    'bn_skip_proj': nn.BatchNorm2d(out_channels),\n                }\n            )\n            self.projection = nn.Sequential(projection)\n    \n    def forward(self, *args):\n        (x,) = args\n        x_residual = self.layers(x)\n        if self.projection is not None:\n            if self._downsample:\n                # pad h/w dimensions if they are odd to prevent shape mismatch with residual layer\n                x = nn.functional.pad(x, (0, x.shape[-1] % 2, 0, x.shape[-2] % 2), value=0)\n            return x_residual + self.projection(x)\n        return x_residual + x", "\n\nclass Bottleneck(nn.Module):\n    \"\"\"expand + depthwise + pointwise\"\"\"\n    def __init__(\n        self,\n        in_channels,\n        out_channels=None,\n        kernel_size=3,\n        dilation=1,\n        upsample=False,\n        downsample=False,\n        expand_ratio=2,\n        dropout=0.0,\n    ):\n        super().__init__()\n        self._downsample = downsample\n        expand_channels = round(in_channels * expand_ratio)\n        out_channels = out_channels or in_channels\n        padding_size = ((kernel_size - 1) * dilation + 1) // 2\n\n        if upsample:\n            assert not downsample, 'downsample and upsample not possible simultaneously.'\n            conv = nn.ConvTranspose2d(expand_channels, expand_channels, kernel_size=kernel_size, bias=False, dilation=1, stride=2, output_padding=padding_size, padding=padding_size, groups=expand_channels)\n        elif downsample:\n            conv = nn.Conv2d(expand_channels, expand_channels, kernel_size=kernel_size, bias=False, dilation=dilation, stride=2, padding=padding_size, groups=expand_channels)\n        else:\n            conv = nn.Conv2d(expand_channels, expand_channels, kernel_size=kernel_size, bias=False, dilation=dilation, padding=padding_size, groups=expand_channels)\n        \n        self.layers = nn.Sequential(\n            # pw\n            nn.Conv2d(in_channels, expand_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(expand_channels),\n            nn.Hardswish(inplace=True),\n            # dw\n            conv,\n            nn.BatchNorm2d(expand_channels),\n            nn.Hardswish(inplace=True),\n            # pw-linear\n            nn.Conv2d(expand_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.Dropout2d(p=dropout),\n            # SeModule(out_channels),\n        )\n\n        if out_channels == in_channels and not downsample and not upsample:\n            self.projection = None\n        else:\n            projection = OrderedDict()\n            if upsample:\n                projection.update({'upsample_skip_proj': nn.Upsample(scale_factor=2, mode='bilinear')})\n            elif downsample:\n                projection.update({'upsample_skip_proj': nn.MaxPool2d(kernel_size=2, stride=2)})\n            projection.update(\n                {\n                    'conv_skip_proj': nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n                    'bn_skip_proj': nn.BatchNorm2d(out_channels),\n                }\n            )\n            self.projection = nn.Sequential(projection)\n\n    def forward(self, *args):\n        (x,) = args\n        x_residual = self.layers(x)\n        if self.projection is not None:\n            if self._downsample:\n                # pad h/w dimensions if they are odd to prevent shape mismatch with residual layer\n                x = nn.functional.pad(x, (0, x.shape[-1] % 2, 0, x.shape[-2] % 2), value=0)\n            return x_residual + self.projection(x)\n        return x_residual + x"]}
{"filename": "powerbev/models/decoder.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport torch.nn as nn\nfrom powerbev.layers.convolutions import UpsamplingAdd", "import torch.nn as nn\nfrom powerbev.layers.convolutions import UpsamplingAdd\nfrom torchvision.models.resnet import resnet18\n\n\nclass Decoder(nn.Module):\n    def __init__(self, in_channels, n_classes, predict_future_flow):\n        super().__init__()\n        backbone = resnet18(pretrained=False, zero_init_residual=True)\n        self.first_conv = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = backbone.bn1\n        self.relu = backbone.relu\n\n        self.layer1 = backbone.layer1\n        self.layer2 = backbone.layer2\n        self.layer3 = backbone.layer3\n        self.predict_future_flow = predict_future_flow\n\n        shared_out_channels = in_channels\n        self.up3_skip = UpsamplingAdd(256, 128, scale_factor=2)\n        self.up2_skip = UpsamplingAdd(128, 64, scale_factor=2)\n        self.up1_skip = UpsamplingAdd(64, shared_out_channels, scale_factor=2)\n\n        self.segmentation_head = nn.Sequential(\n            nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(shared_out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(shared_out_channels, n_classes, kernel_size=1, padding=0),\n        )\n        self.instance_offset_head = nn.Sequential(\n            nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(shared_out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(shared_out_channels, 2, kernel_size=1, padding=0),\n        )\n        self.instance_center_head = nn.Sequential(\n            nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(shared_out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(shared_out_channels, 1, kernel_size=1, padding=0),\n            nn.Sigmoid(),\n        )\n\n        if self.predict_future_flow:\n            self.instance_future_head = nn.Sequential(\n                nn.Conv2d(shared_out_channels, shared_out_channels, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(shared_out_channels),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(shared_out_channels, 2, kernel_size=1, padding=0),\n            )\n\n    def forward(self, x):\n        b, s, c, h, w = x.shape\n        x = x.view(b * s, c, h, w)\n\n        # (H, W)\n        skip_x = {'1': x}\n        x = self.first_conv(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        # (H/4, W/4)\n        x = self.layer1(x)\n        skip_x['2'] = x\n        x = self.layer2(x)\n        skip_x['3'] = x\n\n        # (H/8, W/8)\n        x = self.layer3(x)\n\n        #\u00a0First upsample to (H/4, W/4)\n        x = self.up3_skip(x, skip_x['3'])\n\n        # Second upsample to (H/2, W/2)\n        x = self.up2_skip(x, skip_x['2'])\n\n        # Third upsample to (H, W)\n        x = self.up1_skip(x, skip_x['1'])\n\n        segmentation_output = self.segmentation_head(x)\n        instance_center_output = self.instance_center_head(x)\n        instance_offset_output = self.instance_offset_head(x)\n        instance_future_output = self.instance_future_head(x) if self.predict_future_flow else None\n        return {\n            'segmentation': segmentation_output.view(b, s, *segmentation_output.shape[1:]),\n            'instance_center': instance_center_output.view(b, s, *instance_center_output.shape[1:]),\n            'instance_offset': instance_offset_output.view(b, s, *instance_offset_output.shape[1:]),\n            'instance_flow': instance_future_output.view(b, s, *instance_future_output.shape[1:])\n            if instance_future_output is not None else None,\n        }", ""]}
{"filename": "powerbev/models/temporal_model.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport torch.nn as nn\n", "import torch.nn as nn\n\n\nclass TemporalModelIdentity(nn.Module):\n    def __init__(self, in_channels, receptive_field):\n        super().__init__()\n        self.receptive_field = receptive_field\n        self.out_channels = in_channels\n\n    def forward(self, x):\n        return x[:, (self.receptive_field - 1):]", ""]}
{"filename": "powerbev/models/stconv.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nfrom powerbev.layers.spatial_temporal import Bottleneck, Residual\nfrom torch import nn\n\nconv_block = Residual # [Residual, Bottleneck]\n\nclass MultiBranchSTconv(torch.nn.Module):\n    \"\"\"\"\"\"\n    def __init__(\n        self,\n        cfg,\n        in_channels,\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.latent_dim = self.cfg.MODEL.STCONV.LATENT_DIM\n\n        self.segmentation_branch = STconv(cfg, in_channels, self.latent_dim)\n        self.segmentation_head = Head(self.latent_dim, len(self.cfg.SEMANTIC_SEG.WEIGHTS))\n\n        self.flow_branch = STconv(cfg, in_channels, self.latent_dim)\n        self.flow_head = Head(self.latent_dim, 2)\n\n        self.parameter_init()\n        \n    def parameter_init(self):\n        if isinstance(self.segmentation_head.last_conv, nn.Conv2d):\n            self.segmentation_head.last_conv.bias = nn.parameter.Parameter(torch.tensor([4.6, 0.0], requires_grad=True))\n        if isinstance(self.flow_head.last_conv, nn.Conv2d):\n            self.flow_head.last_conv.bias = nn.parameter.Parameter(torch.tensor([0.0, 0.0], requires_grad=True))\n\n    def forward(self, x):\n        output = {}\n\n        segmentation_branch_output = self.branch_forward(x, self.segmentation_branch)\n        output['segmentation'] = self.head_forward(segmentation_branch_output, self.segmentation_head)\n        \n        flow_branch_output = self.branch_forward(x, self.flow_branch)\n        output['instance_flow'] = self.head_forward(flow_branch_output, self.flow_head)\n        \n        return output\n    \n    @staticmethod\n    def branch_forward(x, branch):\n        return branch(x)\n    \n    @staticmethod\n    def head_forward(x, head):\n        return head(x)", "        \n        \nclass STconv(torch.nn.Module):\n    def __init__(\n        self,\n        cfg,\n        in_channels,\n        out_channels,\n    ):\n        super().__init__()\n        # Data configs\n        self.cfg = cfg\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_past_curr_steps = self.cfg.TIME_RECEPTIVE_FIELD\n        self.num_waypoints = self.cfg.N_FUTURE_FRAMES + 2 # See section 3.4 in paper for details.\n\n        # Model configs\n        self.num_features = [f for f in self.cfg.MODEL.STCONV.NUM_FEATURES]\n        self.num_blocks = self.cfg.MODEL.STCONV.NUM_BLOCKS\n\n        # BEV Encoder\n        self.BEV_encoder = STEncoder(\n            num_features=[self.in_channels] + self.num_features,\n            num_timesteps=self.num_past_curr_steps,\n            num_blocks=self.num_blocks,\n        )\n\n        # BEV Predictor\n        self.BEV_predictor = STPredictor(\n            num_features=self.num_features,\n            in_timesteps=self.num_past_curr_steps,\n            out_timesteps=self.num_waypoints,\n        )\n\n        # BEV Decoder\n        self.BEV_decoder = STDecoder(\n            num_features=self.num_features[::-1] + [self.out_channels],\n            num_timesteps=self.num_waypoints,\n            num_blocks=self.num_blocks,\n        )\n\n    def forward(self, f_in):      \n        # BEV Encoder\n        f_enc = self.BEV_encoder(f_in)  # list of N x T_in x C_i x H_i x W_i\n        \n        # BEV Predictor\n        f_dec = self.BEV_predictor(f_enc)  # list of N x T_out x C_i x H_i x W_i\n\n        # BEV Decoder\n        f_out = self.BEV_decoder(f_dec)  # N x T_out x C x H x W\n\n        # Output\n        return f_out", "\n\nclass STEncoder(nn.Module):\n    def __init__(\n        self,\n        num_features,\n        num_timesteps,\n        num_blocks=3,\n    ):\n        super().__init__()\n        self.num_features = num_features\n        self.num_timesteps = num_timesteps\n\n        self.conv = nn.ModuleList()\n        for i in range(1, len(self.num_features)):\n            stage = nn.Sequential()\n            for j in range(1, num_blocks+1):\n                stage.add_module(\n                    f'downconv_{i}_{j}', conv_block(\n                        in_channels=self.num_features[i - 1] * self.num_timesteps if j == 1 else self.num_features[i] * self.num_timesteps,\n                        out_channels=self.num_features[i] * self.num_timesteps,\n                        downsample=True if j == num_blocks else False\n                    )\n                )\n            self.conv.append(stage)\n\n    def forward(self, x):\n        b, t, _, _, _ = x.shape\n        x = x.reshape(b, -1, *x.shape[3:])\n\n        feature_pyramid = []\n        for _, stage in enumerate(self.conv):\n            x = stage(x)\n            feature_pyramid.append(x.reshape(b, t, -1, *x.shape[2:]))\n            \n        return feature_pyramid", "\n\nclass STPredictor(nn.Module):\n    def __init__(\n        self,\n        num_features,\n        in_timesteps,\n        out_timesteps,\n    ):\n        super().__init__()\n        self.predictor = nn.ModuleList()\n        for nf in num_features:\n            self.predictor.append(nn.Sequential(\n                conv_block(nf * in_timesteps, nf * in_timesteps),\n                conv_block(nf * in_timesteps, nf * in_timesteps),\n                conv_block(nf * in_timesteps, nf * out_timesteps),\n                conv_block(nf * out_timesteps, nf * out_timesteps),\n                conv_block(nf * out_timesteps, nf * out_timesteps),\n            ))\n\n    def forward(self, x):\n        assert len(x) == len(self.predictor), f'The number of input feature tensors ({len(x)}) must be the same as the number of STPredictor blocks {len(self.predictor)}.'\n        \n        y = []\n        for i in range(len(x)):\n            b, _, c, _, _ = x[i].shape\n            x_temp = x[i].reshape(b, -1, *x[i].shape[3:])\n            y.append(self.predictor[i](x_temp).reshape(b, -1, c, *x_temp.shape[2:]))\n        \n        return y", "\n\nclass STDecoder(nn.Module):\n    def __init__(\n        self,\n        num_features,\n        num_timesteps,\n        num_blocks=3,\n    ):\n        super().__init__()\n        self.num_features = num_features\n        self.num_timesteps = num_timesteps\n\n        self.conv = nn.ModuleList()\n        for i in range(1, len(self.num_features)):\n            stage = nn.Sequential()\n            for j in range(1, num_blocks+1):\n                stage.add_module(\n                    f'upconv_{i}_{j}', conv_block(\n                        in_channels=self.num_features[i - 1] * 2 * self.num_timesteps if j == 1 else self.num_features[i] * self.num_timesteps,\n                        out_channels=self.num_features[i] * self.num_timesteps,\n                        upsample=True if j == 1 else False\n                    )\n                )\n            self.conv.append(stage)\n\n    def forward(self, x):\n        assert isinstance(x, list)\n        for i, stage in enumerate(self.conv):\n            b, t, _, _, _ = x[-1-i].shape\n            x_temp = x[-1-i]\n            x_temp = x_temp.reshape(b, -1, *x_temp.shape[3:])\n\n            if i == 0:\n                y = x_temp.repeat(1, 2, 1, 1)\n            else:\n                if y.shape != x_temp.shape:\n                    y = F.interpolate(y, size=x_temp.shape[2:], mode='bilinear', align_corners=True)\n                y = torch.cat((y, x_temp), dim=1)\n            y = stage(y)\n\n        y = y.reshape((b, t, -1, *y.shape[2:]))\n        return y", "\n\nclass Head(nn.Module):\n    def __init__(self, in_channels, out_channels, sigmoid=False):\n        super().__init__()\n        self.sigmoid = sigmoid\n\n        self.head = nn.Sequential(\n            conv_block(in_channels, in_channels//2),\n            conv_block(in_channels//2, in_channels//2),\n            conv_block(in_channels//2, in_channels//4),\n            conv_block(in_channels//4, in_channels//4),\n        )\n\n        self.last_conv = nn.Conv2d(in_channels//4, out_channels, kernel_size=3, padding=1, bias=True)\n        if self.sigmoid:\n            self.last_sigmoid = nn.Sigmoid()\n    \n    def forward(self, x):\n        y = x.clone()\n        b, t, _, _, _ = y.shape\n        y = y.reshape(-1, *y.shape[2:])\n        y = self.head(y)\n        y = self.last_conv(y)\n        if self.sigmoid:\n            y = self.last_sigmoid(y)\n        return y.reshape(b, t, *y.shape[1:])"]}
{"filename": "powerbev/models/encoder.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport torch.nn as nn\nfrom efficientnet_pytorch import EfficientNet", "import torch.nn as nn\nfrom efficientnet_pytorch import EfficientNet\nfrom powerbev.layers.convolutions import UpsamplingConcat\n\n\nclass Encoder(nn.Module):\n    def __init__(self, cfg, D):\n        super().__init__()\n        self.D = D\n        self.C = cfg.OUT_CHANNELS\n        self.use_depth_distribution = cfg.USE_DEPTH_DISTRIBUTION\n        self.downsample = cfg.DOWNSAMPLE\n        self.version = cfg.NAME.split('-')[1]\n\n        self.backbone = EfficientNet.from_pretrained(cfg.NAME)\n        self.delete_unused_layers()\n\n        if self.downsample == 16:\n            if self.version == 'b0':\n                upsampling_in_channels = 320 + 112\n            elif self.version == 'b4':\n                upsampling_in_channels = 448 + 160\n            upsampling_out_channels = 512\n        elif self.downsample == 8:\n            if self.version == 'b0':\n                upsampling_in_channels = 112 + 40\n            elif self.version == 'b4':\n                upsampling_in_channels = 160 + 56\n            upsampling_out_channels = 128\n        else:\n            raise ValueError(f'Downsample factor {self.downsample} not handled.')\n\n        self.upsampling_layer = UpsamplingConcat(upsampling_in_channels, upsampling_out_channels)\n        if self.use_depth_distribution:\n            self.depth_layer = nn.Conv2d(upsampling_out_channels, self.C + self.D, kernel_size=1, padding=0)\n        else:\n            self.depth_layer = nn.Conv2d(upsampling_out_channels, self.C, kernel_size=1, padding=0)\n\n    def delete_unused_layers(self):\n        indices_to_delete = []\n        for idx in range(len(self.backbone._blocks)):\n            if self.downsample == 8:\n                if self.version == 'b0' and idx > 10:\n                    indices_to_delete.append(idx)\n                if self.version == 'b4' and idx > 21:\n                    indices_to_delete.append(idx)\n\n        for idx in reversed(indices_to_delete):\n            del self.backbone._blocks[idx]\n\n        del self.backbone._conv_head\n        del self.backbone._bn1\n        del self.backbone._avg_pooling\n        del self.backbone._dropout\n        del self.backbone._fc\n\n    def get_features(self, x):\n        # Adapted from https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/model.py#L231\n        endpoints = dict()\n\n        # Stem\n        x = self.backbone._swish(self.backbone._bn0(self.backbone._conv_stem(x)))\n        prev_x = x\n\n        # Blocks\n        for idx, block in enumerate(self.backbone._blocks):\n            drop_connect_rate = self.backbone._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self.backbone._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n            if prev_x.size(2) > x.size(2):\n                endpoints['reduction_{}'.format(len(endpoints) + 1)] = prev_x\n            prev_x = x\n\n            if self.downsample == 8:\n                if self.version == 'b0' and idx == 10:\n                    break\n                if self.version == 'b4' and idx == 21:\n                    break\n\n        # Head\n        endpoints['reduction_{}'.format(len(endpoints) + 1)] = x\n\n        if self.downsample == 16:\n            input_1, input_2 = endpoints['reduction_5'], endpoints['reduction_4']\n        elif self.downsample == 8:\n            input_1, input_2 = endpoints['reduction_4'], endpoints['reduction_3']\n\n        x = self.upsampling_layer(input_1, input_2)\n        return x\n\n    def forward(self, x):\n        x = self.get_features(x)  # get feature vector\n\n        x = self.depth_layer(x)  # feature and depth head\n\n        if self.use_depth_distribution:\n            depth = x[:, : self.D].softmax(dim=1)\n            x = depth.unsqueeze(1) * x[:, self.D : (self.D + self.C)].unsqueeze(2)  # outer product depth and features\n        else:\n            x = x.unsqueeze(2).repeat(1, 1, self.D, 1, 1)\n\n        return x", ""]}
{"filename": "powerbev/models/powerbev.py", "chunked_list": ["# ------------------------------------------------------------------------\n# PowerBEV\n# Copyright (c) 2023 Peizheng Li. All Rights Reserved.\n# ------------------------------------------------------------------------\n# Modified from FIERY (https://github.com/wayveai/fiery)\n# Copyright (c) 2021 Wayve Technologies Limited. All Rights Reserved.\n# ------------------------------------------------------------------------\n\nimport time\n", "import time\n\nimport torch\nimport torch.nn as nn\nfrom powerbev.models.decoder import Decoder\nfrom powerbev.models.encoder import Encoder\nfrom powerbev.models.stconv import MultiBranchSTconv\nfrom powerbev.models.temporal_model import TemporalModelIdentity\nfrom powerbev.utils.geometry import (VoxelsSumming,\n                                     calculate_birds_eye_view_parameters,", "from powerbev.utils.geometry import (VoxelsSumming,\n                                     calculate_birds_eye_view_parameters,\n                                     cumulative_warp_features)\nfrom powerbev.utils.network import (pack_sequence_dim, set_bn_momentum,\n                                    unpack_sequence_dim)\n\n\nclass PowerBEV(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n\n        bev_resolution, bev_start_position, bev_dimension = calculate_birds_eye_view_parameters(\n            self.cfg.LIFT.X_BOUND, self.cfg.LIFT.Y_BOUND, self.cfg.LIFT.Z_BOUND\n        )\n        self.bev_resolution = nn.Parameter(bev_resolution, requires_grad=False)\n        self.bev_start_position = nn.Parameter(bev_start_position, requires_grad=False)\n        self.bev_dimension = nn.Parameter(bev_dimension, requires_grad=False)\n\n        self.encoder_downsample = self.cfg.MODEL.ENCODER.DOWNSAMPLE\n        self.encoder_out_channels = self.cfg.MODEL.ENCODER.OUT_CHANNELS\n\n        self.frustum = self.create_frustum()\n        self.depth_channels, _, _, _ = self.frustum.shape\n\n        if self.cfg.TIME_RECEPTIVE_FIELD == 1:\n            assert self.cfg.MODEL.TEMPORAL_MODEL.NAME == 'identity'\n\n        # temporal block\n        self.receptive_field = self.cfg.TIME_RECEPTIVE_FIELD\n        self.n_future = self.cfg.N_FUTURE_FRAMES\n\n        if self.cfg.MODEL.SUBSAMPLE:\n            assert self.cfg.DATASET.NAME == 'lyft'\n            self.receptive_field = 3\n            self.n_future = 5\n\n        # Spatial extent in bird's-eye view, in meters\n        self.spatial_extent = (self.cfg.LIFT.X_BOUND[1], self.cfg.LIFT.Y_BOUND[1])\n        self.bev_size = (self.bev_dimension[0].item(), self.bev_dimension[1].item())\n\n        # Encoder\n        self.encoder = Encoder(cfg=self.cfg.MODEL.ENCODER, D=self.depth_channels)\n\n        # Temporal model\n        temporal_in_channels = self.encoder_out_channels\n        temporal_out_channels = temporal_in_channels    \n        if self.n_future == 0:\n            self.temporal_model = TemporalModelIdentity(temporal_in_channels, self.receptive_field)\n            # Decoder\n            self.future_pred_in_channels = temporal_out_channels\n            self.decoder = Decoder(\n                in_channels=self.future_pred_in_channels,\n                n_classes=len(self.cfg.SEMANTIC_SEG.WEIGHTS),\n                predict_future_flow=self.cfg.INSTANCE_FLOW.ENABLED,\n            )\n        elif self.n_future > 0:\n            if self.cfg.MODEL.STCONV.INPUT_EGOPOSE:\n                temporal_in_channels += 6\n            # Multi-branch STConv\n            self.stconv = MultiBranchSTconv(cfg=self.cfg, in_channels = temporal_in_channels)\n        else:\n            raise NotImplementedError(f'Number of future frames {self.n_future}.')\n\n        set_bn_momentum(self, self.cfg.MODEL.BN_MOMENTUM)\n\n    def create_frustum(self):\n        # Create grid in image plane\n        h, w = self.cfg.IMAGE.FINAL_DIM\n        downsampled_h, downsampled_w = h // self.encoder_downsample, w // self.encoder_downsample\n\n        # Depth grid\n        depth_grid = torch.arange(*self.cfg.LIFT.D_BOUND, dtype=torch.float)\n        depth_grid = depth_grid.view(-1, 1, 1).expand(-1, downsampled_h, downsampled_w)\n        n_depth_slices = depth_grid.shape[0]\n\n        # x and y grids\n        x_grid = torch.linspace(0, w - 1, downsampled_w, dtype=torch.float)\n        x_grid = x_grid.view(1, 1, downsampled_w).expand(n_depth_slices, downsampled_h, downsampled_w)\n        y_grid = torch.linspace(0, h - 1, downsampled_h, dtype=torch.float)\n        y_grid = y_grid.view(1, downsampled_h, 1).expand(n_depth_slices, downsampled_h, downsampled_w)\n\n        # Dimension (n_depth_slices, downsampled_h, downsampled_w, 3)\n        #\u00a0containing data points in the image: left-right, top-bottom, depth\n        frustum = torch.stack((x_grid, y_grid, depth_grid), -1)\n        return nn.Parameter(frustum, requires_grad=False)\n\n    def forward(self, image, intrinsics, extrinsics, future_egomotion, future_distribution_inputs=None, noise=None):\n        output = {}\n        start_time = time.time()\n\n        # Only process features from the past and present\n        image = image[:, :self.receptive_field].contiguous()\n        intrinsics = intrinsics[:, :self.receptive_field].contiguous()\n        extrinsics = extrinsics[:, :self.receptive_field].contiguous()\n        future_egomotion = future_egomotion[:, :self.receptive_field].contiguous()\n\n        # Lifting features and project to bird's-eye view\n        x = self.calculate_birds_eye_view_features(image, intrinsics, extrinsics)\n\n        # Warp past features to the present's reference frame\n        x = cumulative_warp_features(\n            x.clone(), future_egomotion,\n            mode='bilinear', spatial_extent=self.spatial_extent,\n        )\n\n        perception_time = time.time()\n       \n        # Temporal model\n        if self.n_future == 0:\n            states = self.temporal_model(x)\n            bev_output = self.decoder(states[:, -1:])\n        elif self.n_future > 0:\n            if self.cfg.MODEL.STCONV.INPUT_EGOPOSE:\n                b, s, c = future_egomotion.shape\n                h, w = x.shape[-2:]\n                future_egomotions_spatial = future_egomotion.view(b, s, c, 1, 1).expand(b, s, c, h, w)\n                # at time 0, no egomotion so feed zero vector\n                future_egomotions_spatial = torch.cat([torch.zeros_like(future_egomotions_spatial[:, :1]),\n                                                    future_egomotions_spatial[:, :(self.receptive_field-1)]], dim=1)\n                x = torch.cat([x, future_egomotions_spatial], dim=-3)\n\n            bev_output = self.stconv(x)\n        else:\n            raise NotImplementedError(f'Number of future frames {self.n_future}.')\n\n        prediction_time = time.time()\n\n        output['perception_time'] = perception_time - start_time\n        output['prediction_time'] = prediction_time - perception_time\n        output['total_time'] = output['perception_time'] + output['prediction_time']\n        \n        output = {**output, **bev_output}\n\n        return output\n\n    def get_geometry(self, intrinsics, extrinsics):\n        \"\"\"Calculate the (x, y, z) 3D position of the features.\n        \"\"\"\n        rotation, translation = extrinsics[..., :3, :3], extrinsics[..., :3, 3]\n        B, N, _ = translation.shape\n        # Add batch, camera dimension, and a dummy dimension at the end\n        points = self.frustum.unsqueeze(0).unsqueeze(0).unsqueeze(-1)\n\n        # Camera to ego reference frame\n        points = torch.cat((points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3], points[:, :, :, :, :, 2:3]), 5)\n        combined_transformation = rotation.matmul(torch.inverse(intrinsics))\n        points = combined_transformation.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)\n        points += translation.view(B, N, 1, 1, 1, 3)\n\n        # The 3 dimensions in the ego reference frame are: (forward, sides, height)\n        return points\n\n    def encoder_forward(self, x):\n        # batch, n_cameras, channels, height, width\n        b, n, c, h, w = x.shape\n\n        x = x.view(b * n, c, h, w)\n        x = self.encoder(x)\n        x = x.view(b, n, *x.shape[1:])\n        x = x.permute(0, 1, 3, 4, 5, 2)\n\n        return x\n\n    def projection_to_birds_eye_view(self, x, geometry):\n        \"\"\" Adapted from https://github.com/nv-tlabs/lift-splat-shoot/blob/master/src/models.py#L200\"\"\"\n        # batch, n_cameras, depth, height, width, channels\n        batch, n, d, h, w, c = x.shape\n        output = torch.zeros(\n            (batch, c, self.bev_dimension[0], self.bev_dimension[1]), dtype=torch.float, device=x.device\n        )\n\n        # Number of 3D points\n        N = n * d * h * w\n        for b in range(batch):\n            # flatten x\n            x_b = x[b].reshape(N, c)\n\n            # Convert positions to integer indices\n            geometry_b = ((geometry[b] - (self.bev_start_position - self.bev_resolution / 2.0)) / self.bev_resolution)\n            geometry_b = geometry_b.view(N, 3).long()\n\n            # Mask out points that are outside the considered spatial extent.\n            mask = (\n                    (geometry_b[:, 0] >= 0)\n                    & (geometry_b[:, 0] < self.bev_dimension[0])\n                    & (geometry_b[:, 1] >= 0)\n                    & (geometry_b[:, 1] < self.bev_dimension[1])\n                    & (geometry_b[:, 2] >= 0)\n                    & (geometry_b[:, 2] < self.bev_dimension[2])\n            )\n            x_b = x_b[mask]\n            geometry_b = geometry_b[mask]\n\n            # Sort tensors so that those within the same voxel are consecutives.\n            ranks = (\n                    geometry_b[:, 0] * (self.bev_dimension[1] * self.bev_dimension[2])\n                    + geometry_b[:, 1] * (self.bev_dimension[2])\n                    + geometry_b[:, 2]\n            )\n            ranks_indices = ranks.argsort()\n            x_b, geometry_b, ranks = x_b[ranks_indices], geometry_b[ranks_indices], ranks[ranks_indices]\n\n            # Project to bird's-eye view by summing voxels.\n            x_b, geometry_b = VoxelsSumming.apply(x_b, geometry_b, ranks)\n\n            bev_feature = torch.zeros((self.bev_dimension[2], self.bev_dimension[0], self.bev_dimension[1], c),\n                                      device=x_b.device)\n            bev_feature[geometry_b[:, 2], geometry_b[:, 0], geometry_b[:, 1]] = x_b\n\n            # Put channel in second position and remove z dimension\n            bev_feature = bev_feature.permute((0, 3, 1, 2))\n            bev_feature = bev_feature.squeeze(0)\n\n            output[b] = bev_feature\n\n        return output\n\n    def calculate_birds_eye_view_features(self, x, intrinsics, extrinsics):\n        b, s, n, c, h, w = x.shape\n        # Reshape\n        x = pack_sequence_dim(x)\n        intrinsics = pack_sequence_dim(intrinsics)\n        extrinsics = pack_sequence_dim(extrinsics)\n\n        geometry = self.get_geometry(intrinsics, extrinsics)\n        x = self.encoder_forward(x)\n        x = self.projection_to_birds_eye_view(x, geometry)\n        x = unpack_sequence_dim(x, b, s)\n        return x", ""]}
