{"filename": "vectordb_orm/attributes.py", "chunked_list": ["from enum import Enum\nfrom typing import TYPE_CHECKING, Any\n\nif TYPE_CHECKING:\n    from vectordb_orm.base import MilvusBase\n\n\nclass OperationType(Enum):\n    \"\"\"\n    Types fo comparison supported by our querying language\n\n    \"\"\"\n    EQUALS = 'EQUALS'\n    GREATER_THAN = 'GREATER_THAN'\n    LESS_THAN = 'LESS_THAN'\n    LESS_THAN_EQUAL = 'LESS_THAN_EQUAL'\n    GREATER_THAN_EQUAL = 'GREATER_THAN_EQUAL'\n    NOT_EQUAL = 'NOT_EQUAL'", "\n\nclass AttributeCompare:\n    \"\"\"\n    Attributes accessed on the class level are used for query construction like filtering or retrieval. This\n    class allows for Python-native comparison definition like `MyObj.text == \"value\"`. The operation type is\n    used to construct the query expression at execution time.\n\n    \"\"\"\n    def __init__(self, base_cls: \"MilvusBase\", attr: str, value: Any = None, op: OperationType | None = None):\n        self.base_cls = base_cls\n        self.attr = attr\n        self.value = value\n        self.op = op\n\n    def __eq__(self, other):\n        return AttributeCompare(self.base_cls, self.attr, other, OperationType.EQUALS)\n\n    def __gt__(self, other):\n        return AttributeCompare(self.base_cls, self.attr, other, OperationType.GREATER_THAN)\n\n    def __lt__(self, other):\n        return AttributeCompare(self.base_cls, self.attr, other, OperationType.LESS_THAN)\n\n    def __le__(self, other):\n        return AttributeCompare(self.base_cls, self.attr, other, OperationType.LESS_THAN_EQUAL)\n\n    def __ge__(self, other):\n        return AttributeCompare(self.base_cls, self.attr, other, OperationType.GREATER_THAN_EQUAL)", ""]}
{"filename": "vectordb_orm/base.py", "chunked_list": ["from typing import Any\n\nfrom vectordb_orm.attributes import AttributeCompare\nfrom vectordb_orm.enums import ConsistencyType\nfrom vectordb_orm.fields import BaseField, PrimaryKeyField\n\n\nclass VectorSchemaBaseMeta(type):\n    def __getattr__(cls, name):\n        if name in set(cls.__annotations__.keys()):\n            return AttributeCompare(cls, name)\n        raise AttributeError(f\"'{cls.__name__}' object has no attribute '{name}'\")", "\n\nclass VectorSchemaBase(metaclass=VectorSchemaBaseMeta):\n    _type_configuration: dict[str, BaseField]\n\n    def __init__(self, *values: Any, **data: Any) -> None:\n        \"\"\"\n        Initialize a new data object using keyword arguments. This class is both used for creating new objects\n        and for querying existing objects. The keyword arguments are used to specify the attributes of the object\n        to be created.\n\n        :param data: Keyword arguments with attribute names as keys.\n        :raises ValueError: If positional arguments are provided or an unexpected keyword argument is encountered.\n        \"\"\"\n        if values:\n            raise ValueError(\"Use keyword arguments to initialize a Milvus object.\")\n\n        allowed_keys = set(self.__class__.__annotations__.keys())\n\n        for key, value in data.items():\n            if key not in allowed_keys:\n                raise ValueError(f\"Unexpected keyword argument '{key}'\")\n\n        for key in allowed_keys:\n            if key in data:\n                setattr(self, key, data[key])\n            else:\n                field_configuration = self._type_configuration.get(key)\n                if field_configuration:\n                    setattr(self, key, field_configuration.default)\n                else:\n                    raise ValueError(f\"Missing required argument '{key}'\")\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__()\n\n        # Cache the type configurations and then remove the values. We want accessing the property of the\n        # class to return the value of the field (or default to AttributeCompare via __getattr__ in the case of\n        # a direct class attribute), not the field configuration itself.\n        cls._type_configuration = {}\n        for key in cls.__annotations__.keys():\n            attribute_value = getattr(cls, key)\n            cls._type_configuration[key] = attribute_value if isinstance(attribute_value, BaseField) else None\n            # All attributes will result in some value because we return a AttributeCompare by default\n            # for non-configured objects. Checking that the field is actually equal to a base\n            # value allows us to only delete legitimate configuration objects. Otherwise attempting to delete\n            # the synthetic AttributeCompare object will throw an error.\n            if isinstance(attribute_value, BaseField):\n                delattr(cls, key)\n\n    @classmethod\n    def collection_name(self) -> str:\n        if not hasattr(self, '__collection_name__'):\n            raise ValueError(f\"Class {self.__name__} does not have a collection name, specify `__collection_name__` on the class definition.\")\n        return self.__collection_name__\n\n    @classmethod\n    def consistency_type(self) -> ConsistencyType | None:\n        if not hasattr(self, '__consistency_type__'):\n            return None\n        return self.__consistency_type__\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        \"\"\"\n        Create a MilvusBase object from a dictionary.\n\n        :param data: A dictionary containing the attribute names and values.\n        :returns: A MilvusBase object.\n        :raises ValueError: If an unexpected attribute name is encountered in the dictionary.\n        \"\"\"\n        init_payload = {}\n        allowed_keys = list(cls.__annotations__.keys())\n        for attribute_name, value in data.items():\n            if attribute_name not in allowed_keys:\n                raise ValueError(f\"Key `{attribute_name}` is not allowed on {cls.__name__}\")\n            init_payload[attribute_name] = value\n        return cls(**init_payload)", ""]}
{"filename": "vectordb_orm/query.py", "chunked_list": ["from typing import Any\n\nfrom vectordb_orm.attributes import AttributeCompare\nfrom vectordb_orm.backends.base import BackendBase\nfrom vectordb_orm.base import VectorSchemaBase\nfrom vectordb_orm.fields import EmbeddingField\n\n\nclass VectorQueryBuilder:\n    \"\"\"\n    Recursive query builder to allow for chaining of queries.\n\n    Example:\n        >>> query(MyObject).filter(MyObject.text == \"hello\").limit(10).all()\n        >>> query(MyObject.text).filter(MyObject.text == \"hello\").limit(10).all()\n        >>> query(MyObject.text, MyObject.embedding).filter(MyObject.text == \"hello\").limit(10).all()\n\n    \"\"\"\n    def __init__(\n        self,\n        backend: BackendBase,\n        *query_objects: VectorSchemaBase | AttributeCompare\n    ):\n        if len(query_objects) == 0:\n            raise ValueError(\"Must specify at least one query object.\")\n\n        self.backend = backend\n        if isinstance(query_objects[0], AttributeCompare):\n            self.cls = query_objects[0].base_cls\n        elif issubclass(query_objects[0], VectorSchemaBase):\n            self.cls = query_objects[0]\n        else:\n            raise ValueError(f\"Query object must be of type `VectorSchemaBase` or `AttributeCompare`, got {type(query_objects[0])}\")\n\n        # Queries created over time\n        self._query_objects = list(query_objects)\n        self._filters: list[str] = []\n        self._limit : int | None = None\n        self._offset : int | None = None\n\n        # Set if similarity ranking is used\n        self._similarity_attribute : AttributeCompare | None = None\n        self._similarity_value = None\n\n    def filter(self, *filters: AttributeCompare):\n        for f in filters:\n            self._filters.append(f)\n        return self\n\n    def offset(self, offset: int):\n        self._offset = offset\n        return self\n\n    def limit(self, limit: int | None):\n        self._limit = limit\n        return self\n\n    def order_by_similarity(self, embedding_attr: AttributeCompare, query_embedding):\n        if self._similarity_attribute is not None:\n            raise ValueError(\"Only one similarity ordering can be used per query.\")\n        self._similarity_attribute = embedding_attr\n        self._similarity_value = query_embedding\n        return self\n\n    def all(self):\n        # Global configuration\n        output_fields = self._get_output_fields()\n        offset = self._offset if self._offset is not None else 0\n        # Sum of limit and offset should be less than MAX_MILVUS_INT\n        #limit = self._limit if self._limit is not None else (MAX_MILVUS_INT - offset)\n        # TODO: offset is only relevant really for Milvus backends\n        limit = self._limit if self._limit is not None else (self.backend.max_fetch_size - offset)\n\n        return self.backend.search(\n            schema=self.cls,\n            output_fields=output_fields,\n            filters=self._filters,\n            search_embedding=self._similarity_value,\n            search_embedding_key=self._similarity_attribute.attr if self._similarity_attribute is not None else None,\n            limit=limit,\n            offset=offset,\n        )\n\n    def _get_output_fields(self) -> list[str]:\n        \"\"\"\n        Milvus requires us to specify the fields we're looking for explicitly. We by default ignore\n        the embedding field since it's a lot of data to send over the wire and therefore require\n        users to ask for this explicitly in the query definition.\n\n        \"\"\"\n        fields : set[str] = set()\n        for query_object in self._query_objects:\n            if isinstance(query_object, AttributeCompare):\n                field_configuration = self.cls._type_configuration.get(query_object.attr)\n                if field_configuration and isinstance(field_configuration, EmbeddingField):\n                    raise ValueError(\"Cannot query for embedding field. Use `order_by_similarity` instead.\")\n                fields.add(query_object.attr)\n            elif issubclass(query_object, VectorSchemaBase):\n                for attribute_name in self.cls.__annotations__.keys():\n                    field_configuration = self.cls._type_configuration.get(attribute_name)\n                    if field_configuration and isinstance(field_configuration, EmbeddingField):\n                        continue\n                    fields.add(attribute_name)\n        return list(fields)", "class VectorQueryBuilder:\n    \"\"\"\n    Recursive query builder to allow for chaining of queries.\n\n    Example:\n        >>> query(MyObject).filter(MyObject.text == \"hello\").limit(10).all()\n        >>> query(MyObject.text).filter(MyObject.text == \"hello\").limit(10).all()\n        >>> query(MyObject.text, MyObject.embedding).filter(MyObject.text == \"hello\").limit(10).all()\n\n    \"\"\"\n    def __init__(\n        self,\n        backend: BackendBase,\n        *query_objects: VectorSchemaBase | AttributeCompare\n    ):\n        if len(query_objects) == 0:\n            raise ValueError(\"Must specify at least one query object.\")\n\n        self.backend = backend\n        if isinstance(query_objects[0], AttributeCompare):\n            self.cls = query_objects[0].base_cls\n        elif issubclass(query_objects[0], VectorSchemaBase):\n            self.cls = query_objects[0]\n        else:\n            raise ValueError(f\"Query object must be of type `VectorSchemaBase` or `AttributeCompare`, got {type(query_objects[0])}\")\n\n        # Queries created over time\n        self._query_objects = list(query_objects)\n        self._filters: list[str] = []\n        self._limit : int | None = None\n        self._offset : int | None = None\n\n        # Set if similarity ranking is used\n        self._similarity_attribute : AttributeCompare | None = None\n        self._similarity_value = None\n\n    def filter(self, *filters: AttributeCompare):\n        for f in filters:\n            self._filters.append(f)\n        return self\n\n    def offset(self, offset: int):\n        self._offset = offset\n        return self\n\n    def limit(self, limit: int | None):\n        self._limit = limit\n        return self\n\n    def order_by_similarity(self, embedding_attr: AttributeCompare, query_embedding):\n        if self._similarity_attribute is not None:\n            raise ValueError(\"Only one similarity ordering can be used per query.\")\n        self._similarity_attribute = embedding_attr\n        self._similarity_value = query_embedding\n        return self\n\n    def all(self):\n        # Global configuration\n        output_fields = self._get_output_fields()\n        offset = self._offset if self._offset is not None else 0\n        # Sum of limit and offset should be less than MAX_MILVUS_INT\n        #limit = self._limit if self._limit is not None else (MAX_MILVUS_INT - offset)\n        # TODO: offset is only relevant really for Milvus backends\n        limit = self._limit if self._limit is not None else (self.backend.max_fetch_size - offset)\n\n        return self.backend.search(\n            schema=self.cls,\n            output_fields=output_fields,\n            filters=self._filters,\n            search_embedding=self._similarity_value,\n            search_embedding_key=self._similarity_attribute.attr if self._similarity_attribute is not None else None,\n            limit=limit,\n            offset=offset,\n        )\n\n    def _get_output_fields(self) -> list[str]:\n        \"\"\"\n        Milvus requires us to specify the fields we're looking for explicitly. We by default ignore\n        the embedding field since it's a lot of data to send over the wire and therefore require\n        users to ask for this explicitly in the query definition.\n\n        \"\"\"\n        fields : set[str] = set()\n        for query_object in self._query_objects:\n            if isinstance(query_object, AttributeCompare):\n                field_configuration = self.cls._type_configuration.get(query_object.attr)\n                if field_configuration and isinstance(field_configuration, EmbeddingField):\n                    raise ValueError(\"Cannot query for embedding field. Use `order_by_similarity` instead.\")\n                fields.add(query_object.attr)\n            elif issubclass(query_object, VectorSchemaBase):\n                for attribute_name in self.cls.__annotations__.keys():\n                    field_configuration = self.cls._type_configuration.get(attribute_name)\n                    if field_configuration and isinstance(field_configuration, EmbeddingField):\n                        continue\n                    fields.add(attribute_name)\n        return list(fields)", ""]}
{"filename": "vectordb_orm/enums.py", "chunked_list": ["from enum import Enum\n\nfrom pymilvus.orm.types import (CONSISTENCY_BOUNDED, CONSISTENCY_EVENTUALLY,\n                                CONSISTENCY_SESSION, CONSISTENCY_STRONG)\n\n\nclass ConsistencyType(Enum):\n    \"\"\"\n    Define the strength of the consistency within the distributed DB:\n    https://milvus.io/docs/consistency.md\n\n    \"\"\"\n    STRONG = CONSISTENCY_STRONG\n    BOUNDED = CONSISTENCY_BOUNDED\n    SESSION = CONSISTENCY_SESSION\n    EVENTUALLY = CONSISTENCY_EVENTUALLY", ""]}
{"filename": "vectordb_orm/__init__.py", "chunked_list": ["from vectordb_orm.backends.milvus.indexes import (Milvus_BIN_FLAT,\n                                                  Milvus_BIN_IVF_FLAT,\n                                                  Milvus_FLAT, Milvus_HNSW,\n                                                  Milvus_IVF_FLAT,\n                                                  Milvus_IVF_PQ,\n                                                  Milvus_IVF_SQ8)\nfrom vectordb_orm.backends.milvus.milvus import MilvusBackend\nfrom vectordb_orm.backends.pinecone.indexes import (PineconeIndex,\n                                                    PineconeSimilarityMetric)\nfrom vectordb_orm.backends.pinecone.pinecone import PineconeBackend", "                                                    PineconeSimilarityMetric)\nfrom vectordb_orm.backends.pinecone.pinecone import PineconeBackend\nfrom vectordb_orm.base import VectorSchemaBase\nfrom vectordb_orm.enums import ConsistencyType\nfrom vectordb_orm.fields import EmbeddingField, PrimaryKeyField, VarCharField\nfrom vectordb_orm.session import VectorSession\n"]}
{"filename": "vectordb_orm/session.py", "chunked_list": ["from typing import Type\n\nfrom pymilvus import Milvus\n\nfrom vectordb_orm.backends.base import BackendBase\nfrom vectordb_orm.base import VectorSchemaBase\nfrom vectordb_orm.query import VectorQueryBuilder\n\n\nclass VectorSession:\n    \"\"\"\n    Core session object used to interact with a vector database backend.\n\n    \"\"\"\n    def __init__(self, backend: BackendBase):\n        self.backend = backend\n\n    def query(self, *query_objects: VectorSchemaBase | VectorQueryBuilder):\n        return VectorQueryBuilder(self.backend, *query_objects)\n\n    def create_collection(self, schema: Type[VectorSchemaBase]):\n        return self.backend.create_collection(schema)\n\n    def clear_collection(self, schema: Type[VectorSchemaBase]):\n        return self.backend.clear_collection(schema)\n\n    def delete_collection(self, schema: Type[VectorSchemaBase]):\n        return self.backend.delete_collection(schema)\n\n    def insert(self, obj: VectorSchemaBase) -> VectorSchemaBase:\n        new_id = self.backend.insert(obj)\n        obj.id = new_id\n        return obj\n\n    def insert_batch(self, objs: list[VectorSchemaBase], show_progress: bool = False) -> list[VectorSchemaBase]:\n        new_ids = self.backend.insert_batch(objs, show_progress=show_progress)\n        for new_id, obj in zip(new_ids, objs):\n            obj.id = new_id\n        return objs\n\n    def delete(self, obj: VectorSchemaBase) -> None:\n        if not obj.id:\n            raise ValueError(\"Cannot delete object that hasn't been inserted into the database\")\n\n        self.backend.delete(obj)\n        obj.id = None\n\n    def flush(self, schema: Type[VectorSchemaBase]):\n        self.backend.flush(schema)\n\n    def load(self, schema: Type[VectorSchemaBase]):\n        self.backend.load(schema)", "\nclass VectorSession:\n    \"\"\"\n    Core session object used to interact with a vector database backend.\n\n    \"\"\"\n    def __init__(self, backend: BackendBase):\n        self.backend = backend\n\n    def query(self, *query_objects: VectorSchemaBase | VectorQueryBuilder):\n        return VectorQueryBuilder(self.backend, *query_objects)\n\n    def create_collection(self, schema: Type[VectorSchemaBase]):\n        return self.backend.create_collection(schema)\n\n    def clear_collection(self, schema: Type[VectorSchemaBase]):\n        return self.backend.clear_collection(schema)\n\n    def delete_collection(self, schema: Type[VectorSchemaBase]):\n        return self.backend.delete_collection(schema)\n\n    def insert(self, obj: VectorSchemaBase) -> VectorSchemaBase:\n        new_id = self.backend.insert(obj)\n        obj.id = new_id\n        return obj\n\n    def insert_batch(self, objs: list[VectorSchemaBase], show_progress: bool = False) -> list[VectorSchemaBase]:\n        new_ids = self.backend.insert_batch(objs, show_progress=show_progress)\n        for new_id, obj in zip(new_ids, objs):\n            obj.id = new_id\n        return objs\n\n    def delete(self, obj: VectorSchemaBase) -> None:\n        if not obj.id:\n            raise ValueError(\"Cannot delete object that hasn't been inserted into the database\")\n\n        self.backend.delete(obj)\n        obj.id = None\n\n    def flush(self, schema: Type[VectorSchemaBase]):\n        self.backend.flush(schema)\n\n    def load(self, schema: Type[VectorSchemaBase]):\n        self.backend.load(schema)", ""]}
{"filename": "vectordb_orm/fields.py", "chunked_list": ["from abc import ABC\nfrom typing import Any\n\nfrom vectordb_orm.index import IndexBase\n\n\nclass BaseField(ABC):\n    \"\"\"\n    BaseField is the superclass to all fields that require additional customization behavior. They\n    are specified as values for the typehints that otherwise populate the class, like:\n\n    ```\n    class MyModel:\n        embedding: np.array = EmbeddingField(dim=16)\n    ```\n\n    \"\"\"\n    def __init__(self, default: Any):\n        self.default = default", "\n\nclass PrimaryKeyField(BaseField):\n    def __init__(self):\n        super().__init__(default=None)\n\n\nclass EmbeddingField(BaseField):\n    def __init__(\n        self,\n        dim: int,\n        index: IndexBase,\n        default: Any = None,\n    ):\n        super().__init__(default=default)\n\n        self.dim = dim\n        self.index = index", "\n\nclass VarCharField(BaseField):\n    def __init__(self, max_length: int, default: Any = None):\n        super().__init__(default=default)\n\n        self.max_length = max_length\n"]}
{"filename": "vectordb_orm/results.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Optional\n\nif TYPE_CHECKING:\n    from vectordb_orm.base import VectorSchemaBase\n\n@dataclass\nclass QueryResult:\n    result: \"VectorSchemaBase\"\n\n    # Score and distance is only returned for queries requesting vector-similarity\n    score: Optional[float] = None\n    distance: Optional[float] = None", ""]}
{"filename": "vectordb_orm/index.py", "chunked_list": ["class IndexBase:\n    pass\n"]}
{"filename": "vectordb_orm/tests/test_query.py", "chunked_list": ["from typing import Type\n\nimport numpy as np\nimport pytest\n\nfrom vectordb_orm import VectorSchemaBase, VectorSession\nfrom vectordb_orm.tests.conftest import SESSION_MODEL_PAIRS\n\n\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_query(session: str, model: Type[VectorSchemaBase], request):\n    \"\"\"\n    General test of querying and query chaining\n    \"\"\"\n    session : VectorSession = request.getfixturevalue(session)\n\n    # Create some MyObject instances\n    obj1 = model(text=\"foo\", embedding=np.array([1.0] * 128))\n    obj2 = model(text=\"bar\", embedding=np.array([4.0] * 128))\n    obj3 = model(text=\"baz\", embedding=np.array([7.0] * 128))\n\n    # Insert the objects into Milvus\n    session.insert(obj1)\n    session.insert(obj2)\n    session.insert(obj3)\n\n    session.flush(model)\n    session.load(model)\n\n    # Test a simple filter and similarity ranking\n    results = session.query(model).filter(model.text == 'bar').order_by_similarity(model.embedding, np.array([1.0]*128)).limit(2).all()\n    assert len(results) == 1\n    assert results[0].result.id == obj2.id\n\n    # Test combined filtering\n    results = session.query(model).filter(model.text == 'baz', model.id > 1).order_by_similarity(model.embedding, np.array([8.0]*128)).limit(2).all()\n    assert len(results) == 1\n    assert results[0].result.id == obj3.id", "\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_query(session: str, model: Type[VectorSchemaBase], request):\n    \"\"\"\n    General test of querying and query chaining\n    \"\"\"\n    session : VectorSession = request.getfixturevalue(session)\n\n    # Create some MyObject instances\n    obj1 = model(text=\"foo\", embedding=np.array([1.0] * 128))\n    obj2 = model(text=\"bar\", embedding=np.array([4.0] * 128))\n    obj3 = model(text=\"baz\", embedding=np.array([7.0] * 128))\n\n    # Insert the objects into Milvus\n    session.insert(obj1)\n    session.insert(obj2)\n    session.insert(obj3)\n\n    session.flush(model)\n    session.load(model)\n\n    # Test a simple filter and similarity ranking\n    results = session.query(model).filter(model.text == 'bar').order_by_similarity(model.embedding, np.array([1.0]*128)).limit(2).all()\n    assert len(results) == 1\n    assert results[0].result.id == obj2.id\n\n    # Test combined filtering\n    results = session.query(model).filter(model.text == 'baz', model.id > 1).order_by_similarity(model.embedding, np.array([8.0]*128)).limit(2).all()\n    assert len(results) == 1\n    assert results[0].result.id == obj3.id", "\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_query_default_ignores_embeddings(session: str, model: Type[VectorSchemaBase], request):\n    \"\"\"\n    Ensure that querying on the class by default ignores embeddings that are included\n    within the type definition.\n    \"\"\"\n    session : VectorSession = request.getfixturevalue(session)\n\n    obj1 = model(text=\"foo\", embedding=np.array([1.0] * 128))\n    session.insert(obj1)\n\n    session.flush(model)\n    session.load(model)\n\n    # Test a simple filter and similarity ranking\n    results = session.query(model).filter(model.text == 'foo').limit(2).all()\n    assert len(results) == 1\n\n    result : model = results[0].result\n    assert result.embedding is None", "\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_query_with_fields(session: str, model: str, request):\n    \"\"\"\n    Test querying with specific fields\n    \"\"\"\n    session : VectorSession = request.getfixturevalue(session)\n\n    obj1 = model(text=\"foo\", embedding=np.array([1.0] * 128))\n    session.insert(obj1)\n\n    session.flush(model)\n    session.load(model)\n\n    # We can query for regular attributes\n    results = session.query(model.text).filter(model.text == 'foo').all()\n    assert len(results) == 1\n\n    # We shouldn't be able to query for embeddings\n    with pytest.raises(ValueError):\n        session.query(model.embedding).filter(model.text == 'foo').all()", ""]}
{"filename": "vectordb_orm/tests/models.py", "chunked_list": ["import numpy as np\n\nfrom vectordb_orm import (ConsistencyType, EmbeddingField, Milvus_BIN_FLAT,\n                          Milvus_IVF_FLAT, PineconeIndex,\n                          PineconeSimilarityMetric, PrimaryKeyField,\n                          VarCharField, VectorSchemaBase)\n\n\nclass MilvusMyObject(VectorSchemaBase):\n    __collection_name__ = 'my_collection'\n    __consistency_type__ = ConsistencyType.STRONG\n\n    id: int = PrimaryKeyField()\n    text: str = VarCharField(max_length=128)\n    embedding: np.ndarray = EmbeddingField(dim=128, index=Milvus_IVF_FLAT(cluster_units=128))", "class MilvusMyObject(VectorSchemaBase):\n    __collection_name__ = 'my_collection'\n    __consistency_type__ = ConsistencyType.STRONG\n\n    id: int = PrimaryKeyField()\n    text: str = VarCharField(max_length=128)\n    embedding: np.ndarray = EmbeddingField(dim=128, index=Milvus_IVF_FLAT(cluster_units=128))\n\n\nclass MilvusBinaryEmbeddingObject(VectorSchemaBase):\n    __collection_name__ = 'binary_collection'\n    __consistency_type__ = ConsistencyType.STRONG\n\n    id: int = PrimaryKeyField()\n    embedding: np.ndarray[np.bool_] = EmbeddingField(dim=128, index=Milvus_BIN_FLAT())", "\nclass MilvusBinaryEmbeddingObject(VectorSchemaBase):\n    __collection_name__ = 'binary_collection'\n    __consistency_type__ = ConsistencyType.STRONG\n\n    id: int = PrimaryKeyField()\n    embedding: np.ndarray[np.bool_] = EmbeddingField(dim=128, index=Milvus_BIN_FLAT())\n\n\nclass PineconeMyObject(VectorSchemaBase):\n    __collection_name__ = 'my_collection'\n\n    id: int = PrimaryKeyField()\n    text: str = VarCharField(max_length=128)\n    embedding: np.ndarray = EmbeddingField(dim=128, index=PineconeIndex(metric_type=PineconeSimilarityMetric.COSINE))", "\nclass PineconeMyObject(VectorSchemaBase):\n    __collection_name__ = 'my_collection'\n\n    id: int = PrimaryKeyField()\n    text: str = VarCharField(max_length=128)\n    embedding: np.ndarray = EmbeddingField(dim=128, index=PineconeIndex(metric_type=PineconeSimilarityMetric.COSINE))\n"]}
{"filename": "vectordb_orm/tests/__init__.py", "chunked_list": [""]}
{"filename": "vectordb_orm/tests/test_base.py", "chunked_list": ["from typing import Type\n\nimport numpy as np\nimport pytest\n\nfrom vectordb_orm import (EmbeddingField, PrimaryKeyField, VectorSchemaBase,\n                          VectorSession)\nfrom vectordb_orm.backends.milvus.indexes import Milvus_IVF_FLAT\nfrom vectordb_orm.tests.conftest import SESSION_MODEL_PAIRS\n", "from vectordb_orm.tests.conftest import SESSION_MODEL_PAIRS\n\n\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_create_object(session: str, model: Type[VectorSchemaBase]):\n    my_object = model(text='example', embedding=np.array([1.0] * 128))\n    assert my_object.text == 'example'\n    assert np.array_equal(my_object.embedding, np.array([1.0] * 128))\n    assert my_object.id is None\n", "\n\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_from_dict(session: str, model: Type[VectorSchemaBase]):\n    my_object = model.from_dict(\n        {\n            \"text\": 'example',\n            \"embedding\": np.array([1.0] * 128)\n        }\n    )\n    assert my_object.text == 'example'\n    assert np.array_equal(my_object.embedding, np.array([1.0] * 128))\n    assert my_object.id is None", "\n\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_insert_object(session: str, model: Type[VectorSchemaBase], request):\n    session : VectorSession = request.getfixturevalue(session)\n\n    my_object = model(text='example', embedding=np.array([1.0] * 128))\n    session.insert(my_object)\n    assert my_object.id is not None\n\n    session.flush(model)\n    session.load(model)\n\n    # Retrieve the object and ensure the values are equivalent\n    results = session.query(model).filter(model.id == my_object.id).all()\n    assert len(results) == 1\n\n    result : model = results[0].result\n    assert result.text == my_object.text", "\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_insert_batch(session: str, model: Type[VectorSchemaBase], request):\n    session : VectorSession = request.getfixturevalue(session)\n\n    obj1 = model(text='example1', embedding=np.array([1.0] * 128))\n    obj2 = model(text='example2', embedding=np.array([2.0] * 128))\n    obj3 = model(text='example3', embedding=np.array([3.0] * 128))\n\n    session.insert_batch([obj1, obj2, obj3])\n\n    for obj in [obj1, obj2, obj3]:\n        assert obj.id is not None", "\n@pytest.mark.parametrize(\"session,model\", SESSION_MODEL_PAIRS)\ndef test_delete_object(session: str, model: Type[VectorSchemaBase],request):\n    session : VectorSession = request.getfixturevalue(session)\n\n    my_object = model(text='example', embedding=np.array([1.0] * 128))\n    session.insert(my_object)\n\n    session.flush(model)\n    session.load(model)\n\n    results = session.query(model).filter(model.text == \"example\").all()\n    assert len(results) == 1\n\n    session.delete(my_object)\n\n    session.flush(model)\n    session.load(model)\n    # Allow enough time to become consistent\n    #sleep(1)\n\n    results = session.query(model).filter(model.text == \"example\").all()\n    assert len(results) == 0", "\n\ndef test_milvus_invalid_typesignatures(milvus_session: VectorSession):\n    class TestInvalidObject(VectorSchemaBase):\n        \"\"\"\n        An IVF_FLAT can't be used with a boolean embedding type\n        \"\"\"\n        __collection_name__ = 'invalid_collection'\n\n        id: int = PrimaryKeyField()\n        embedding: np.ndarray[np.bool_] = EmbeddingField(dim=128, index=Milvus_IVF_FLAT(cluster_units=128))\n\n    with pytest.raises(ValueError, match=\"not compatible with binary vectors\"):\n        milvus_session.create_collection(TestInvalidObject)", "\n\ndef test_mixed_configuration_fields(milvus_session: VectorSession):\n    \"\"\"\n    Confirm that schema definitions can mix typehints that have configuration values\n    and those that only have vanilla markup.\n\n    \"\"\"\n    class TestMixedConfigurationObject(VectorSchemaBase):\n        __collection_name__ = 'mixed_configuration_collection'\n\n        id: int = PrimaryKeyField()\n        is_valid: bool\n        embedding: np.ndarray = EmbeddingField(dim=128, index=Milvus_IVF_FLAT(cluster_units=128))\n\n    milvus_session.create_collection(TestMixedConfigurationObject)", ""]}
{"filename": "vectordb_orm/tests/conftest.py", "chunked_list": ["from os import getenv\nfrom time import sleep\n\nimport pytest\nfrom dotenv import load_dotenv\nfrom pymilvus import Milvus, connections\n\nfrom vectordb_orm import MilvusBackend, PineconeBackend, VectorSession\nfrom vectordb_orm.tests.models import (MilvusBinaryEmbeddingObject,\n                                       MilvusMyObject, PineconeMyObject)", "from vectordb_orm.tests.models import (MilvusBinaryEmbeddingObject,\n                                       MilvusMyObject, PineconeMyObject)\n\n\n@pytest.fixture()\ndef milvus_session():\n    session = VectorSession(MilvusBackend(Milvus()))\n    connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n\n    # Wipe the previous collections\n    session.delete_collection(MilvusMyObject)\n    session.delete_collection(MilvusBinaryEmbeddingObject)\n\n    # Flush\n    sleep(1)\n\n    session.create_collection(MilvusMyObject)\n    session.create_collection(MilvusBinaryEmbeddingObject)\n\n    return session", "\n@pytest.fixture()\ndef pinecone_session():\n    load_dotenv()\n    session = VectorSession(\n        PineconeBackend(\n            api_key=getenv(\"PINECONE_API_KEY\"),\n            environment=getenv(\"PINECONE_ENVIRONMENT\"),\n        )\n    )\n\n    # Wipe the previous collections\n    # Pinecone doesn't have the notion of binary objects like Milvus does, so we\n    # only create one object. Their free tier also doesn't support more than 1\n    # collection, so that's another limitation that encourages a single index here.\n    session.create_collection(PineconeMyObject)\n    session.clear_collection(PineconeMyObject)\n\n    return session", "\nSESSION_FIXTURE_KEYS = [\"milvus_session\", \"pinecone_session\"]\nSESSION_MODEL_PAIRS = [\n    (\n        \"milvus_session\",\n        MilvusMyObject,\n    ),\n    (\n        \"pinecone_session\",\n        PineconeMyObject,", "        \"pinecone_session\",\n        PineconeMyObject,\n    ),\n]"]}
{"filename": "vectordb_orm/tests/milvus/test_milvus.py", "chunked_list": ["import numpy as np\nimport pytest\n\nfrom vectordb_orm import VectorSession\nfrom vectordb_orm.tests.models import MilvusBinaryEmbeddingObject\n\n\n# @pierce 04-21- 2023: Currently flaky\n# https://github.com/piercefreeman/vectordb-orm/pull/5\n@pytest.mark.xfail(strict=False)\ndef test_binary_collection_query(session: VectorSession):\n    # Create some MyObject instances\n    obj1 = MilvusBinaryEmbeddingObject(embedding=np.array([True] * 128))\n    obj2 = MilvusBinaryEmbeddingObject(embedding=np.array([False] * 128))\n\n    # Insert the objects into Milvus\n    session.insert(obj1)\n    session.insert(obj2)\n\n    session.flush(MilvusBinaryEmbeddingObject)\n    session.load(MilvusBinaryEmbeddingObject)  \n\n    # Test our ability to recall 1:1 the input content\n    results = session.query(MilvusBinaryEmbeddingObject).order_by_similarity(MilvusBinaryEmbeddingObject.embedding, np.array([True]*128)).limit(2).all()\n    assert len(results) == 2\n    assert results[0].result.id == obj1.id\n\n    results = session.query(MilvusBinaryEmbeddingObject).order_by_similarity(BinaryEmbeddingObject.embedding, np.array([False]*128)).limit(2).all()\n    assert len(results) == 2\n    assert results[0].result.id == obj2.id", "# https://github.com/piercefreeman/vectordb-orm/pull/5\n@pytest.mark.xfail(strict=False)\ndef test_binary_collection_query(session: VectorSession):\n    # Create some MyObject instances\n    obj1 = MilvusBinaryEmbeddingObject(embedding=np.array([True] * 128))\n    obj2 = MilvusBinaryEmbeddingObject(embedding=np.array([False] * 128))\n\n    # Insert the objects into Milvus\n    session.insert(obj1)\n    session.insert(obj2)\n\n    session.flush(MilvusBinaryEmbeddingObject)\n    session.load(MilvusBinaryEmbeddingObject)  \n\n    # Test our ability to recall 1:1 the input content\n    results = session.query(MilvusBinaryEmbeddingObject).order_by_similarity(MilvusBinaryEmbeddingObject.embedding, np.array([True]*128)).limit(2).all()\n    assert len(results) == 2\n    assert results[0].result.id == obj1.id\n\n    results = session.query(MilvusBinaryEmbeddingObject).order_by_similarity(BinaryEmbeddingObject.embedding, np.array([False]*128)).limit(2).all()\n    assert len(results) == 2\n    assert results[0].result.id == obj2.id", ""]}
{"filename": "vectordb_orm/tests/milvus/test_indexes.py", "chunked_list": ["from itertools import product\n\nimport numpy as np\nimport pytest\nfrom pymilvus import Milvus\n\nfrom vectordb_orm import (EmbeddingField, PrimaryKeyField, VectorSchemaBase,\n                          VectorSession)\nfrom vectordb_orm.backends.milvus.indexes import (BINARY_INDEXES,\n                                                  FLOATING_INDEXES,", "from vectordb_orm.backends.milvus.indexes import (BINARY_INDEXES,\n                                                  FLOATING_INDEXES,\n                                                  MilvusIndexBase)\nfrom vectordb_orm.backends.milvus.similarity import (\n    MilvusBinarySimilarityMetric, MilvusFloatSimilarityMetric)\n\n# Different index definitions require different kwarg arguments; we centralize\n# them here for ease of accessing them during different test runs\nINDEX_DEFAULTS = {\n    \"IVF_FLAT\": dict(", "INDEX_DEFAULTS = {\n    \"IVF_FLAT\": dict(\n        cluster_units=128,\n    ),\n    \"IVF_SQ8\": dict(\n        cluster_units=128,\n    ),\n    \"IVF_PQ\": dict(\n        cluster_units=128,\n        product_quantization=16,", "        cluster_units=128,\n        product_quantization=16,\n        low_dimension_bits=16,\n    ),\n    \"HNSW\": dict(\n        max_degree=4,\n        search_scope_index=16,\n        search_scope_inference=128,\n    ),\n    \"BIN_IVF_FLAT\": dict(", "    ),\n    \"BIN_IVF_FLAT\": dict(\n        cluster_units=128,\n    )\n}\n\n@pytest.mark.parametrize(\n    \"index_cls,metric_type\",\n    product(\n        FLOATING_INDEXES,", "    product(\n        FLOATING_INDEXES,\n        [item for item in MilvusFloatSimilarityMetric],\n    )\n)\ndef test_floating_index(\n    milvus_session: VectorSession,\n    index_cls: MilvusIndexBase,\n    metric_type: MilvusFloatSimilarityMetric,\n):\n    class IndexSubclassObject(VectorSchemaBase):\n        __collection_name__ = 'index_collection'\n\n        id: int = PrimaryKeyField()\n        embedding: np.ndarray = EmbeddingField(\n            dim=128,\n            index=index_cls(\n                metric_type=metric_type,\n                **INDEX_DEFAULTS.get(index_cls.index_type, {})\n            )\n        )\n\n    milvus_session.delete_collection(IndexSubclassObject)\n    milvus_session.create_collection(IndexSubclassObject)\n\n    # Insert an object\n    my_object = IndexSubclassObject(embedding=np.array([1.0] * 128))\n    milvus_session.insert(my_object)\n\n    # Flush and load the collection\n    milvus_session.flush(IndexSubclassObject)\n    milvus_session.load(IndexSubclassObject)\n\n    # Perform a query\n    results = milvus_session.query(IndexSubclassObject).order_by_similarity(IndexSubclassObject.embedding, np.array([1.0] * 128)).all()\n    assert len(results) == 1\n    assert results[0].result.id == my_object.id", "\n\n@pytest.mark.parametrize(\n    \"index_cls,metric_type\",\n    product(\n        BINARY_INDEXES,\n        [item for item in MilvusBinarySimilarityMetric],\n    )\n)\ndef test_binary_index(\n    milvus_session: VectorSession,\n    index_cls: MilvusIndexBase,\n    metric_type: MilvusBinarySimilarityMetric,\n):\n    class IndexSubclassObject(VectorSchemaBase):\n        __collection_name__ = 'index_collection'\n\n        id: int = PrimaryKeyField()\n        embedding: np.ndarray[np.bool_] = EmbeddingField(\n            dim=128,\n            index=index_cls(\n                metric_type=metric_type,\n                **INDEX_DEFAULTS.get(index_cls.index_type, {})\n            )\n        )\n\n    milvus_session.delete_collection(IndexSubclassObject)\n    milvus_session.create_collection(IndexSubclassObject)\n\n    # Insert an object\n    my_object = IndexSubclassObject(embedding=np.array([True] * 128))\n    milvus_session.insert(my_object)\n\n    # Flush and load the collection\n    milvus_session.flush(IndexSubclassObject)\n    milvus_session.load(IndexSubclassObject)\n\n    # Perform a query\n    results = milvus_session.query(IndexSubclassObject).order_by_similarity(IndexSubclassObject.embedding, np.array([True] * 128)).all()\n    assert len(results) == 1\n    assert results[0].result.id == my_object.id", ")\ndef test_binary_index(\n    milvus_session: VectorSession,\n    index_cls: MilvusIndexBase,\n    metric_type: MilvusBinarySimilarityMetric,\n):\n    class IndexSubclassObject(VectorSchemaBase):\n        __collection_name__ = 'index_collection'\n\n        id: int = PrimaryKeyField()\n        embedding: np.ndarray[np.bool_] = EmbeddingField(\n            dim=128,\n            index=index_cls(\n                metric_type=metric_type,\n                **INDEX_DEFAULTS.get(index_cls.index_type, {})\n            )\n        )\n\n    milvus_session.delete_collection(IndexSubclassObject)\n    milvus_session.create_collection(IndexSubclassObject)\n\n    # Insert an object\n    my_object = IndexSubclassObject(embedding=np.array([True] * 128))\n    milvus_session.insert(my_object)\n\n    # Flush and load the collection\n    milvus_session.flush(IndexSubclassObject)\n    milvus_session.load(IndexSubclassObject)\n\n    # Perform a query\n    results = milvus_session.query(IndexSubclassObject).order_by_similarity(IndexSubclassObject.embedding, np.array([True] * 128)).all()\n    assert len(results) == 1\n    assert results[0].result.id == my_object.id", ""]}
{"filename": "vectordb_orm/tests/milvus/__init__.py", "chunked_list": [""]}
{"filename": "vectordb_orm/backends/base.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import Type\n\nimport numpy as np\n\nfrom vectordb_orm.attributes import AttributeCompare\nfrom vectordb_orm.base import VectorSchemaBase\nfrom vectordb_orm.fields import PrimaryKeyField\nfrom vectordb_orm.results import QueryResult\n", "from vectordb_orm.results import QueryResult\n\n\nclass BackendBase(ABC):\n    max_fetch_size: int\n\n    @abstractmethod\n    def create_collection(self, schema: Type[VectorSchemaBase]):\n        pass\n\n    @abstractmethod\n    def clear_collection(self, schema: Type[VectorSchemaBase]):\n        pass\n\n    @abstractmethod\n    def delete_collection(self, schema: Type[VectorSchemaBase]):\n        pass\n\n    @abstractmethod\n    def insert(self, entity: VectorSchemaBase) -> int:\n        pass\n\n    @abstractmethod\n    def insert_batch(self, entities: list[VectorSchemaBase], show_progress: bool) -> list[int]:\n        pass\n\n    @abstractmethod\n    def delete(self, entity: VectorSchemaBase):\n        pass\n\n    @abstractmethod\n    def search(\n        self,\n        schema: Type[VectorSchemaBase],\n        output_fields: list[str],\n        filters: list[AttributeCompare] | None,\n        search_embedding: np.ndarray | None,\n        search_embedding_key: str | None,\n        limit: int,\n        offset: int,\n    ) -> list[QueryResult]:\n        pass\n\n    @abstractmethod\n    def flush(self, schema: Type[VectorSchemaBase]):\n        pass\n\n    @abstractmethod\n    def load(self, schema: Type[VectorSchemaBase]):\n        pass\n\n    def _get_primary(self, schema: Type[VectorSchemaBase]):\n        \"\"\"\n        If the class has a primary key, return it, otherwise return None\n        \"\"\"\n        for attribute_name in schema.__annotations__.keys():\n            if isinstance(schema._type_configuration.get(attribute_name), PrimaryKeyField):\n                return attribute_name\n        return None\n\n    def _assert_has_primary(self, schema: Type[VectorSchemaBase]):\n        \"\"\"\n        Ensure we have a primary key, this is the only field that's fully required\n        \"\"\"\n        if self._get_primary(schema) is None:\n            raise ValueError(f\"Class {schema.__name__} does not have a primary key, specify `PrimaryKeyField` on the class definition.\")", ""]}
{"filename": "vectordb_orm/backends/__init__.py", "chunked_list": [""]}
{"filename": "vectordb_orm/backends/pinecone/indexes.py", "chunked_list": ["from enum import Enum\n\nfrom vectordb_orm.index import IndexBase\n\n\nclass PineconeSimilarityMetric(Enum):\n    COSINE = \"cosine\"\n    EUCLIDEAN = \"euclidean\"\n    DOT_PRODUCT = \"dotproduct\"\n", "\n\nclass PineconeIndex(IndexBase):\n    \"\"\"\n    Pinecone only supports one type of index\n    \"\"\"\n    def __init__(self, metric_type: PineconeSimilarityMetric):\n        self.metric_type = metric_type\n        self._assert_metric_type(metric_type)\n\n    def get_index_parameters(self):\n        return {}\n\n    def get_inference_parameters(self):\n        return {\"metric_type\": self.metric_type.name}\n\n    def _assert_metric_type(self, metric_type: PineconeSimilarityMetric):\n        # Only support valid combinations of metric type and index\n        if not isinstance(metric_type, PineconeSimilarityMetric):\n            raise ValueError(f\"Index type {self} is not supported for metric type {metric_type}\")", ""]}
{"filename": "vectordb_orm/backends/pinecone/__init__.py", "chunked_list": ["from vectordb_orm.backends.pinecone.indexes import *\nfrom vectordb_orm.backends.pinecone.pinecone import *\n"]}
{"filename": "vectordb_orm/backends/pinecone/pinecone.py", "chunked_list": ["from collections import defaultdict\nfrom logging import info\nfrom re import match as re_match\nfrom typing import Type\nfrom uuid import uuid4\n\nimport numpy as np\nimport pinecone\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom vectordb_orm.attributes import AttributeCompare, OperationType\nfrom vectordb_orm.backends.base import BackendBase\nfrom vectordb_orm.backends.pinecone.indexes import PineconeIndex\nfrom vectordb_orm.base import VectorSchemaBase\nfrom vectordb_orm.fields import EmbeddingField\nfrom vectordb_orm.results import QueryResult\n\n\nclass PineconeBackend(BackendBase):\n    max_fetch_size = 1000\n\n    def __init__(\n        self,\n        api_key: str,\n        environment: str\n    ):\n        # Pinecone can't accept quotes in either parameters, and these are sometimes\n        # unintentionally included in env variables\n        # We quit before initializing with a more informative error message\n        if \"'\" in api_key or '\"' in api_key:\n            raise ValueError(\"Pinecone `api_key` contains single or double quotes, which isn't allowed.\")\n        if \"'\" in environment or '\"' in environment:\n            raise ValueError(\"Pinecone `environment` contains single or double quotes, which isn't allowed.\")\n\n        pinecone.init(\n            api_key=api_key,\n            environment=environment,\n        )\n\n    def create_collection(self, schema: Type[VectorSchemaBase]):\n        collection_name = self.transform_collection_name(schema.collection_name())\n        current_indexes = pinecone.list_indexes()\n        if collection_name in current_indexes:\n            info(\"Collection already created...\")\n            return\n\n        info(\"Creating collection, this could take 30s-5min...\")\n\n        self._assert_valid_collection_name(collection_name)\n        self._assert_has_primary(schema)\n        self._assert_valid_embedding_field(schema)\n\n        # Pinecone allows for dynamic keys on each object\n        # However we need to pre-provide the keys we want to search on\n        # This does increase memory size so we can consider adding an explict index flag\n        # for the metadata fields that we want to allow search on\n        # https://docs.pinecone.io/docs/manage-indexes#selective-metadata-indexing\n        metadata_config = {\n            \"indexed\": [\n                key\n                for key in schema.__annotations__.keys()\n            ]\n        }\n\n        _, embedding_field = self._get_embedding_field(schema)\n        index : PineconeIndex = embedding_field.index\n\n        pinecone.create_index(\n            name=collection_name,\n            dimension=embedding_field.dim,\n            metric=index.metric_type.value,\n            metadata_config=metadata_config\n        )\n        return pinecone.Index(index_name=collection_name)\n\n    def clear_collection(self, schema: Type[VectorSchemaBase]):\n        collection_name = self.transform_collection_name(schema.collection_name())\n\n        current_indexes = pinecone.list_indexes()\n        if collection_name not in current_indexes:\n            return\n\n        index = pinecone.Index(index_name=collection_name)\n        delete_response = index.delete(delete_all=True)\n        if delete_response:\n            # Success should have an empty dict\n            raise ValueError(f\"Failed to clear collection {collection_name}: {delete_response}\")\n\n    def delete_collection(self, schema: Type[VectorSchemaBase]):\n        collection_name = self.transform_collection_name(schema.collection_name())\n    \n        # Pinecone throws a 404 if the index doesn't exist\n        current_indexes = pinecone.list_indexes()\n        if collection_name not in current_indexes:\n            return\n\n        pinecone.delete_index(collection_name)\n\n    def insert(self, entity: VectorSchemaBase) -> list:\n        schema = entity.__class__\n        collection_name = self.transform_collection_name(schema.collection_name())\n\n        embedding_field_key, _ = self._get_embedding_field(schema)\n        primary_key = self._get_primary(schema)\n\n        identifier = uuid4().int & (1<<64)-1\n\n        index = pinecone.Index(index_name=collection_name)\n        index.upsert([\n            self._prepare_upsert_tuple(\n                entity,\n                identifier,\n                embedding_field_key=embedding_field_key,\n                primary_key=primary_key,\n            )\n        ])\n\n        return identifier\n\n    def insert_batch(\n        self,\n        entities: list[VectorSchemaBase],\n        show_progress: bool,\n        batch_size: int = 100\n    ) -> list[int]:\n        identifiers = [\n            uuid4().int & (1<<64)-1\n            for _ in range(len(entities))\n        ]\n\n        # Group by the objects\n        schema_to_original_index = defaultdict(list)\n        schema_to_class = {}\n\n        for i, entity in enumerate(entities):\n            schema = entity.__class__\n            schema_name = schema.collection_name()\n\n            schema_to_original_index[schema_name].append(i)\n            schema_to_class[schema_name] = schema\n\n        for schema_name, original_indexes in schema_to_original_index.items():\n            schema = schema_to_class[schema_name]\n            collection_name = self.transform_collection_name(schema.collection_name())\n\n            index = pinecone.Index(index_name=collection_name)\n\n            embedding_field_key, _ = self._get_embedding_field(schema)\n            primary_key = self._get_primary(schema)\n\n            for i in tqdm(range(0, len(original_indexes), batch_size)):\n                batch_indexes = original_indexes[i:i+batch_size]\n                batch_entities = [entities[index] for index in batch_indexes]\n                batch_identifiers = [identifiers[index] for index in batch_indexes]\n\n                index.upsert(\n                    [\n                        self._prepare_upsert_tuple(\n                            entity,\n                            identifier,\n                            embedding_field_key=embedding_field_key,\n                            primary_key=primary_key,\n                        )\n                        for entity, identifier in zip(batch_entities, batch_identifiers)\n                    ]\n                )\n\n        return identifiers\n\n    def delete(self, entity: VectorSchemaBase):\n        schema = entity.__class__\n        collection_name = self.transform_collection_name(schema.collection_name())\n        index = pinecone.Index(index_name=collection_name)\n        delete_response = index.delete(ids=[str(entity.id)])\n        if delete_response:\n            # Success should have an empty dict\n            raise ValueError(f\"Failed to clear collection {collection_name}: {delete_response}\")\n\n    def search(\n        self,\n        schema: Type[VectorSchemaBase],\n        output_fields: list[str],\n        filters: list[AttributeCompare] | None,\n        search_embedding: np.ndarray | None,\n        search_embedding_key: str | None,\n        limit: int,\n        offset: int,\n    ):\n        # For performance reasons, do not return vector metadata when top_k>1000\n        # A ORM search query automatically returns some fields, so we need to\n        # throw an error under these conditions\n        if limit > 1000:\n            raise ValueError(\"Pinecone only supports retrieving element values with limit <= 1000\")\n\n        if offset > 0:\n            raise ValueError(\"Pinecone doesn't currently support query offsets\")\n\n        collection_name = self.transform_collection_name(schema.collection_name())\n        primary_key = self._get_primary(schema)\n        index = pinecone.Index(index_name=collection_name) \n\n        # Unlike some other backends, Pinecone requires us to search with some vector as the input\n        # We therefore\n        missing_vector = search_embedding is None\n        if missing_vector:\n            info(\"No vector provided for Pinecone search, using a zero vector to still retrieve content...\")\n            search_embedding_key, embedding_configuration = self._get_embedding_field(schema)\n            search_embedding = np.zeros((embedding_configuration.dim,))\n\n        filters =  {\n            attribute.attr: self._attribute_to_value_payload(schema, attribute)\n            for attribute in filters\n        }\n        query_response = index.query(\n            filter=filters,\n            top_k=limit,\n            include_values=False,\n            include_metadata=True,\n            vector=search_embedding.tolist(),\n        )\n\n        objects = []\n        for item in query_response.to_dict()[\"matches\"]:\n            objects.append(\n                QueryResult(\n                    result=schema.from_dict(\n                        {\n                            **item[\"metadata\"],\n                            primary_key: int(item[\"id\"]),\n                        }\n                    ),\n                    score=item[\"score\"] if not missing_vector else None,\n                )\n            )\n        return objects\n\n    def flush(self, schema: Type[VectorSchemaBase]):\n        # No local caching is involved in Pinecone\n        pass\n\n    def load(self, schema: Type[VectorSchemaBase]):\n        # No local caching is involved in Pinecone\n        pass\n\n    def transform_collection_name(self, collection_name: str):\n        return collection_name.replace(\"_\", \"-\")\n\n    def _assert_valid_collection_name(self, collection_name: str):\n        is_valid = all(\n            [\n                collection_name,\n                re_match(r'^[a-z0-9]+(-[a-z0-9]+)*$', collection_name),\n                collection_name[0].isalnum(),\n                collection_name[-1].isalnum()\n            ]\n        )\n\n        if not is_valid:\n            raise ValueError(f\"Invalid collection name: {collection_name}; must be lowercase, alphanumeric, and hyphenated.\")\n\n    def _get_embedding_field(self, schema: Type[VectorSchemaBase]):\n        embedding_fields = {\n            key: value\n            for key, value in schema._type_configuration.items()\n            if isinstance(value, EmbeddingField)\n        }\n\n        if len(embedding_fields) != 1:\n            raise ValueError(f\"Pinecone only supports one embedding field per collection. {schema} has {len(embedding_fields)} defined: {list(embedding_fields.keys())}.\")\n\n        return list(embedding_fields.items())[0]\n\n    def _assert_valid_embedding_field(self, schema: Type[VectorSchemaBase]):\n        _, embedding_field = self._get_embedding_field(schema)\n\n        # Ensure that we are using a supported index\n        if not isinstance(embedding_field.index, PineconeIndex):\n            raise ValueError(\"Pinecone only supports a basic `PineconeIndex`.\")\n\n    def _attribute_to_value_payload(self, schema: Type[VectorSchemaBase], attribute: AttributeCompare):\n        \"\"\"\n        Converts an attribute to a filter value payload for Pinecone, using Mongo's filtering syntax\n\n        \"\"\"\n        operation_type_maps = {\n            OperationType.EQUALS: \"$eq\",\n            OperationType.GREATER_THAN: \"$gt\",\n            OperationType.GREATER_THAN_EQUAL: \"$gte\",\n            OperationType.LESS_THAN: \"$lt\",\n            OperationType.LESS_THAN_EQUAL: \"$lte\",\n            OperationType.NOT_EQUAL: \"$ne\",\n            # TODO: Support $in and $nin\n        }\n\n        return {\n            operation_type_maps[attribute.op]: attribute.value\n        }\n\n    def _prepare_upsert_tuple(\n        self,\n        entity: VectorSchemaBase,\n        identifier: int,\n        embedding_field_key: str,\n        primary_key: str,\n    ):\n        \"\"\"\n        Formats a tuple for upsert\n        \"\"\"\n        schema = entity.__class__\n\n        embedding_value : np.ndarray = getattr(entity, embedding_field_key)\n        metadata_fields = {\n            key: getattr(entity, key)\n            for key in schema.__annotations__.keys()\n            if key not in {embedding_field_key, primary_key}\n        }\n\n        return (\n            str(identifier),\n            embedding_value.tolist(),\n            {\n                **metadata_fields,\n                primary_key: identifier,\n            }\n        )", "\n\nclass PineconeBackend(BackendBase):\n    max_fetch_size = 1000\n\n    def __init__(\n        self,\n        api_key: str,\n        environment: str\n    ):\n        # Pinecone can't accept quotes in either parameters, and these are sometimes\n        # unintentionally included in env variables\n        # We quit before initializing with a more informative error message\n        if \"'\" in api_key or '\"' in api_key:\n            raise ValueError(\"Pinecone `api_key` contains single or double quotes, which isn't allowed.\")\n        if \"'\" in environment or '\"' in environment:\n            raise ValueError(\"Pinecone `environment` contains single or double quotes, which isn't allowed.\")\n\n        pinecone.init(\n            api_key=api_key,\n            environment=environment,\n        )\n\n    def create_collection(self, schema: Type[VectorSchemaBase]):\n        collection_name = self.transform_collection_name(schema.collection_name())\n        current_indexes = pinecone.list_indexes()\n        if collection_name in current_indexes:\n            info(\"Collection already created...\")\n            return\n\n        info(\"Creating collection, this could take 30s-5min...\")\n\n        self._assert_valid_collection_name(collection_name)\n        self._assert_has_primary(schema)\n        self._assert_valid_embedding_field(schema)\n\n        # Pinecone allows for dynamic keys on each object\n        # However we need to pre-provide the keys we want to search on\n        # This does increase memory size so we can consider adding an explict index flag\n        # for the metadata fields that we want to allow search on\n        # https://docs.pinecone.io/docs/manage-indexes#selective-metadata-indexing\n        metadata_config = {\n            \"indexed\": [\n                key\n                for key in schema.__annotations__.keys()\n            ]\n        }\n\n        _, embedding_field = self._get_embedding_field(schema)\n        index : PineconeIndex = embedding_field.index\n\n        pinecone.create_index(\n            name=collection_name,\n            dimension=embedding_field.dim,\n            metric=index.metric_type.value,\n            metadata_config=metadata_config\n        )\n        return pinecone.Index(index_name=collection_name)\n\n    def clear_collection(self, schema: Type[VectorSchemaBase]):\n        collection_name = self.transform_collection_name(schema.collection_name())\n\n        current_indexes = pinecone.list_indexes()\n        if collection_name not in current_indexes:\n            return\n\n        index = pinecone.Index(index_name=collection_name)\n        delete_response = index.delete(delete_all=True)\n        if delete_response:\n            # Success should have an empty dict\n            raise ValueError(f\"Failed to clear collection {collection_name}: {delete_response}\")\n\n    def delete_collection(self, schema: Type[VectorSchemaBase]):\n        collection_name = self.transform_collection_name(schema.collection_name())\n    \n        # Pinecone throws a 404 if the index doesn't exist\n        current_indexes = pinecone.list_indexes()\n        if collection_name not in current_indexes:\n            return\n\n        pinecone.delete_index(collection_name)\n\n    def insert(self, entity: VectorSchemaBase) -> list:\n        schema = entity.__class__\n        collection_name = self.transform_collection_name(schema.collection_name())\n\n        embedding_field_key, _ = self._get_embedding_field(schema)\n        primary_key = self._get_primary(schema)\n\n        identifier = uuid4().int & (1<<64)-1\n\n        index = pinecone.Index(index_name=collection_name)\n        index.upsert([\n            self._prepare_upsert_tuple(\n                entity,\n                identifier,\n                embedding_field_key=embedding_field_key,\n                primary_key=primary_key,\n            )\n        ])\n\n        return identifier\n\n    def insert_batch(\n        self,\n        entities: list[VectorSchemaBase],\n        show_progress: bool,\n        batch_size: int = 100\n    ) -> list[int]:\n        identifiers = [\n            uuid4().int & (1<<64)-1\n            for _ in range(len(entities))\n        ]\n\n        # Group by the objects\n        schema_to_original_index = defaultdict(list)\n        schema_to_class = {}\n\n        for i, entity in enumerate(entities):\n            schema = entity.__class__\n            schema_name = schema.collection_name()\n\n            schema_to_original_index[schema_name].append(i)\n            schema_to_class[schema_name] = schema\n\n        for schema_name, original_indexes in schema_to_original_index.items():\n            schema = schema_to_class[schema_name]\n            collection_name = self.transform_collection_name(schema.collection_name())\n\n            index = pinecone.Index(index_name=collection_name)\n\n            embedding_field_key, _ = self._get_embedding_field(schema)\n            primary_key = self._get_primary(schema)\n\n            for i in tqdm(range(0, len(original_indexes), batch_size)):\n                batch_indexes = original_indexes[i:i+batch_size]\n                batch_entities = [entities[index] for index in batch_indexes]\n                batch_identifiers = [identifiers[index] for index in batch_indexes]\n\n                index.upsert(\n                    [\n                        self._prepare_upsert_tuple(\n                            entity,\n                            identifier,\n                            embedding_field_key=embedding_field_key,\n                            primary_key=primary_key,\n                        )\n                        for entity, identifier in zip(batch_entities, batch_identifiers)\n                    ]\n                )\n\n        return identifiers\n\n    def delete(self, entity: VectorSchemaBase):\n        schema = entity.__class__\n        collection_name = self.transform_collection_name(schema.collection_name())\n        index = pinecone.Index(index_name=collection_name)\n        delete_response = index.delete(ids=[str(entity.id)])\n        if delete_response:\n            # Success should have an empty dict\n            raise ValueError(f\"Failed to clear collection {collection_name}: {delete_response}\")\n\n    def search(\n        self,\n        schema: Type[VectorSchemaBase],\n        output_fields: list[str],\n        filters: list[AttributeCompare] | None,\n        search_embedding: np.ndarray | None,\n        search_embedding_key: str | None,\n        limit: int,\n        offset: int,\n    ):\n        # For performance reasons, do not return vector metadata when top_k>1000\n        # A ORM search query automatically returns some fields, so we need to\n        # throw an error under these conditions\n        if limit > 1000:\n            raise ValueError(\"Pinecone only supports retrieving element values with limit <= 1000\")\n\n        if offset > 0:\n            raise ValueError(\"Pinecone doesn't currently support query offsets\")\n\n        collection_name = self.transform_collection_name(schema.collection_name())\n        primary_key = self._get_primary(schema)\n        index = pinecone.Index(index_name=collection_name) \n\n        # Unlike some other backends, Pinecone requires us to search with some vector as the input\n        # We therefore\n        missing_vector = search_embedding is None\n        if missing_vector:\n            info(\"No vector provided for Pinecone search, using a zero vector to still retrieve content...\")\n            search_embedding_key, embedding_configuration = self._get_embedding_field(schema)\n            search_embedding = np.zeros((embedding_configuration.dim,))\n\n        filters =  {\n            attribute.attr: self._attribute_to_value_payload(schema, attribute)\n            for attribute in filters\n        }\n        query_response = index.query(\n            filter=filters,\n            top_k=limit,\n            include_values=False,\n            include_metadata=True,\n            vector=search_embedding.tolist(),\n        )\n\n        objects = []\n        for item in query_response.to_dict()[\"matches\"]:\n            objects.append(\n                QueryResult(\n                    result=schema.from_dict(\n                        {\n                            **item[\"metadata\"],\n                            primary_key: int(item[\"id\"]),\n                        }\n                    ),\n                    score=item[\"score\"] if not missing_vector else None,\n                )\n            )\n        return objects\n\n    def flush(self, schema: Type[VectorSchemaBase]):\n        # No local caching is involved in Pinecone\n        pass\n\n    def load(self, schema: Type[VectorSchemaBase]):\n        # No local caching is involved in Pinecone\n        pass\n\n    def transform_collection_name(self, collection_name: str):\n        return collection_name.replace(\"_\", \"-\")\n\n    def _assert_valid_collection_name(self, collection_name: str):\n        is_valid = all(\n            [\n                collection_name,\n                re_match(r'^[a-z0-9]+(-[a-z0-9]+)*$', collection_name),\n                collection_name[0].isalnum(),\n                collection_name[-1].isalnum()\n            ]\n        )\n\n        if not is_valid:\n            raise ValueError(f\"Invalid collection name: {collection_name}; must be lowercase, alphanumeric, and hyphenated.\")\n\n    def _get_embedding_field(self, schema: Type[VectorSchemaBase]):\n        embedding_fields = {\n            key: value\n            for key, value in schema._type_configuration.items()\n            if isinstance(value, EmbeddingField)\n        }\n\n        if len(embedding_fields) != 1:\n            raise ValueError(f\"Pinecone only supports one embedding field per collection. {schema} has {len(embedding_fields)} defined: {list(embedding_fields.keys())}.\")\n\n        return list(embedding_fields.items())[0]\n\n    def _assert_valid_embedding_field(self, schema: Type[VectorSchemaBase]):\n        _, embedding_field = self._get_embedding_field(schema)\n\n        # Ensure that we are using a supported index\n        if not isinstance(embedding_field.index, PineconeIndex):\n            raise ValueError(\"Pinecone only supports a basic `PineconeIndex`.\")\n\n    def _attribute_to_value_payload(self, schema: Type[VectorSchemaBase], attribute: AttributeCompare):\n        \"\"\"\n        Converts an attribute to a filter value payload for Pinecone, using Mongo's filtering syntax\n\n        \"\"\"\n        operation_type_maps = {\n            OperationType.EQUALS: \"$eq\",\n            OperationType.GREATER_THAN: \"$gt\",\n            OperationType.GREATER_THAN_EQUAL: \"$gte\",\n            OperationType.LESS_THAN: \"$lt\",\n            OperationType.LESS_THAN_EQUAL: \"$lte\",\n            OperationType.NOT_EQUAL: \"$ne\",\n            # TODO: Support $in and $nin\n        }\n\n        return {\n            operation_type_maps[attribute.op]: attribute.value\n        }\n\n    def _prepare_upsert_tuple(\n        self,\n        entity: VectorSchemaBase,\n        identifier: int,\n        embedding_field_key: str,\n        primary_key: str,\n    ):\n        \"\"\"\n        Formats a tuple for upsert\n        \"\"\"\n        schema = entity.__class__\n\n        embedding_value : np.ndarray = getattr(entity, embedding_field_key)\n        metadata_fields = {\n            key: getattr(entity, key)\n            for key in schema.__annotations__.keys()\n            if key not in {embedding_field_key, primary_key}\n        }\n\n        return (\n            str(identifier),\n            embedding_value.tolist(),\n            {\n                **metadata_fields,\n                primary_key: identifier,\n            }\n        )", ""]}
{"filename": "vectordb_orm/backends/milvus/similarity.py", "chunked_list": ["from enum import Enum\n\nfrom pymilvus.client.types import MetricType\n\n\nclass MilvusFloatSimilarityMetric(Enum):\n    \"\"\"\n    Specify the metric used for floating-point search. At inference time a query vector is broadcast to the vectors in the database\n    using this approach. The string values of this enums are directly used by Milvus, see here for more info: https://milvus.io/docs/metric.md\n\n    \"\"\"\n    # Euclidean\n    L2 = MetricType.L2.name\n    # Inner Product\n    IP = MetricType.IP.name", "\nclass MilvusBinarySimilarityMetric(Enum):\n    \"\"\"\n    Specify the metric used for binary search. These are distance metrics.\n\n    \"\"\"\n    JACCARD = MetricType.JACCARD.name\n    TANIMOTO = MetricType.TANIMOTO.name\n    HAMMING = MetricType.HAMMING.name\n", ""]}
{"filename": "vectordb_orm/backends/milvus/indexes.py", "chunked_list": ["from abc import ABC, abstractmethod\n\nfrom vectordb_orm.backends.milvus.similarity import (\n    MilvusBinarySimilarityMetric, MilvusFloatSimilarityMetric)\nfrom vectordb_orm.index import IndexBase\n\n\nclass MilvusIndexBase(IndexBase):\n    \"\"\"\n    Specify indexes used for embedding creation: https://milvus.io/docs/index.md\n    Individual docstrings for the index types are taken from this page of ideal scenarios.\n\n    \"\"\"\n    index_type: str\n\n    def __init__(\n        self,\n        metric_type: MilvusFloatSimilarityMetric | MilvusBinarySimilarityMetric | None = None,\n    ):\n        # Choose a reasonable default if metric_type is null, depending on the type of index\n        if metric_type is None:\n            if isinstance(self, tuple(FLOATING_INDEXES)):\n                metric_type = MilvusFloatSimilarityMetric.L2\n            elif isinstance(self, tuple(BINARY_INDEXES)):\n                metric_type = MilvusBinarySimilarityMetric.JACCARD\n\n        self._assert_metric_type(metric_type)\n        self.metric_type = metric_type\n\n    @abstractmethod\n    def get_index_parameters(self):\n        pass\n\n    @abstractmethod\n    def get_inference_parameters(self):\n        \"\"\"\n        NOTE: For simplicity, each index type will have the same inference parameters as index parameters. Milvus does allow\n        for the customization of these per-query, but we will see how this use-case develops.\n        \"\"\"\n        pass\n\n    def _assert_metric_type(self, metric_type: MilvusFloatSimilarityMetric | MilvusBinarySimilarityMetric):\n        \"\"\"\n        Binary indexes only support binary metrics, and floating indexes only support floating metrics. Assert\n        that the combination of metric type and index is valid.\n\n        \"\"\"\n        # Only support valid combinations of metric type and index\n        if isinstance(metric_type, MilvusFloatSimilarityMetric):\n            if not isinstance(self, tuple(FLOATING_INDEXES)):\n                raise ValueError(f\"Index type {self} is not supported for metric type {metric_type}\")\n        elif isinstance(metric_type, MilvusBinarySimilarityMetric):\n            if not isinstance (self, tuple(BINARY_INDEXES)):\n                raise ValueError(f\"Index type {self} is not supported for metric type {metric_type}\")\n\n    def _assert_cluster_units_and_inference_comparison(self, cluster_units: int, inference_comparison: int | None) -> tuple[int, int]:\n        if not (cluster_units >= 1 and cluster_units <= 65536):\n            raise ValueError(\"cluster_units must be between 1 and 65536\")\n        if inference_comparison is not None and not (inference_comparison >= 1 and inference_comparison <= cluster_units):\n            raise ValueError(\"inference_comparison must be between 1 and cluster_units\")", "\nclass Milvus_FLAT(MilvusIndexBase):\n    \"\"\"\n    - Relatively small dataset\n    - Requires a 100% recall rate\n    \"\"\"\n    index_type = \"FLAT\"\n\n    def get_index_parameters(self):\n        return {}\n\n    def get_inference_parameters(self):\n        return {\"metric_type\": self.metric_type.name}", "\n\nclass Milvus_IVF_FLAT(MilvusIndexBase):\n    \"\"\"\n    - High-speed query\n    - Requires a recall rate as high as possible\n    \"\"\"\n    index_type = \"IVF_FLAT\"\n\n    def __init__(\n        self,\n        cluster_units: int,\n        inference_comparison: int | None = None,\n        metric_type: MilvusFloatSimilarityMetric | MilvusBinarySimilarityMetric | None = None,\n    ):\n        \"\"\"\n        :param cluster_units: Number of clusters (nlist in the docs)\n        :param inference_comparison: Number of cluster centroids to compare during inference (nprobe in the docs)\n            By default if this is not specified, it will be set to the same value as cluster_units.\n        \"\"\"\n        super().__init__(metric_type=metric_type)\n\n        self._assert_cluster_units_and_inference_comparison(cluster_units, inference_comparison)\n\n        self.nlist = cluster_units\n        self.nprobe = inference_comparison or cluster_units\n\n    def get_index_parameters(self):\n        return {\"nlist\": self.nlist}\n\n    def get_inference_parameters(self):\n        return {\"nprobe\": self.nprobe}", "\n\nclass Milvus_IVF_SQ8(MilvusIndexBase):\n    \"\"\"\n    - High-speed query\n    - Limited memory resources\n    - Accepts minor compromise in recall rate\n    \"\"\"\n    index_type = \"IVF_SQ8\"\n\n    def __init__(\n        self,\n        cluster_units: int,\n        inference_comparison: int | None = None,\n        metric_type: MilvusFloatSimilarityMetric | MilvusBinarySimilarityMetric | None = None,\n    ):\n        \"\"\"\n        :param cluster_units: Number of clusters (nlist in the docs)\n        :param inference_comparison: Number of cluster centroids to compare during inference (nprobe in the docs)\n            By default if this is not specified, it will be set to the same value as cluster_units.\n        \"\"\"\n        super().__init__(metric_type=metric_type)\n\n        self._assert_cluster_units_and_inference_comparison(cluster_units, inference_comparison)\n\n        self.nlist = cluster_units\n        self.nprobe = inference_comparison or cluster_units\n\n    def get_index_parameters(self):\n        return {\"nlist\": self.nlist}\n\n    def get_inference_parameters(self):\n        return {\"nprobe\": self.nprobe}", "\n\nclass Milvus_IVF_PQ(MilvusIndexBase):\n    \"\"\"\n    - Very high-speed query\n    - Limited memory resources\n    - Accepts substantial compromise in recall rate\n    \"\"\"\n    index_type = \"IVF_PQ\"\n\n    def __init__(\n        self,\n        cluster_units: int,\n        product_quantization: int | None = None,\n        inference_comparison: int | None = None,\n        low_dimension_bits: int | None = None,\n        metric_type: MilvusFloatSimilarityMetric | MilvusBinarySimilarityMetric | None = None,\n    ):\n        \"\"\"\n        :param cluster_units: Number of clusters (nlist in the docs)\n        :param inference_comparison: Number of cluster centroids to compare during inference (nprobe in the docs)\n            By default if this is not specified, it will be set to the same value as cluster_units.\n        \"\"\"\n        super().__init__(metric_type=metric_type)\n\n        self._assert_cluster_units_and_inference_comparison(cluster_units, inference_comparison)\n        self._assert_low_dimension_bits(low_dimension_bits)\n\n        self.m = product_quantization\n        self.nbits = low_dimension_bits or 8\n        self.nlist = cluster_units\n        self.nprobe = inference_comparison or cluster_units\n\n    def get_index_parameters(self):\n        return {\"nlist\": self.nlist, \"m\": self.m, \"nbits\": self.nbits}\n\n    def get_inference_parameters(self):\n        return {\"nprobe\": self.nprobe}\n\n    def _assert_low_dimension_bits(self, low_dimension_bits: int | None):\n        if low_dimension_bits is not None and not (low_dimension_bits >= 1 and low_dimension_bits <= 16):\n            raise ValueError(\"low_dimension_bits must be between 1 and 16\")", "\nclass Milvus_HNSW(MilvusIndexBase):\n    \"\"\"\n    - High-speed query\n    - Requires a recall rate as high as possible\n    - Large memory resources\n    \"\"\"\n    index_type = \"HNSW\"\n\n    def __init__(\n        self,\n        max_degree: int,\n        search_scope_index: int,\n        search_scope_inference: int,\n        metric_type: MilvusFloatSimilarityMetric | MilvusBinarySimilarityMetric | None = None,\n    ):\n        \"\"\"\n        :param max_degree: Maximum degree of the node\n        :param search_scope: Search scope\n        \"\"\"\n        super().__init__(metric_type=metric_type)\n\n        self._assert_max_degree(max_degree)\n        self._assert_search_scope_index(search_scope_index)\n        self._assert_search_scope_inference(search_scope_inference)\n\n        self.m = max_degree\n        self.efConstruction = search_scope_index\n        self.ef = search_scope_inference\n\n    def get_index_parameters(self):\n        return {\"M\": self.m, \"efConstruction\": self.efConstruction}\n\n    def get_inference_parameters(self):\n        return {\"ef\": self.ef}\n\n    def _assert_max_degree(self, max_degree: int):\n        if not (max_degree >= 4 and max_degree <= 64):\n            raise ValueError(\"max_degree must be between 4 and 64\")\n\n    def _assert_search_scope_index(self, search_scope_index: int):\n        if not (search_scope_index >= 8 and search_scope_index <= 512):\n            raise ValueError(\"search_scope must be between 1 and 512\")\n\n    def _assert_search_scope_inference(self, search_scope_inference: int):\n        if not (search_scope_inference >= 1 and search_scope_inference <= 32768):\n            # NOTE: Technically this needs to be between [top_k, 32768], but we don't know what top_k is\n            # at index time\n            raise ValueError(\"search_scope must be between 1 and 32768\")", "\n\nclass Milvus_BIN_FLAT(MilvusIndexBase):\n    \"\"\"\n    - Relatively small dataset\n    - Requires a 100% recall rate\n    \"\"\"\n    index_type = \"BIN_FLAT\"\n\n    def get_index_parameters(self):\n        return {}\n\n    def get_inference_parameters(self):\n        return {\"metric_type\": self.metric_type.name}", "\n\nclass Milvus_BIN_IVF_FLAT(MilvusIndexBase):\n    \"\"\"\n    - High-speed query\n    - Requires a recall rate as high as possible\n    \"\"\"\n    index_type = \"BIN_IVF_FLAT\"\n\n    def __init__(\n        self,\n        cluster_units: int,\n        inference_comparison: int | None = None,\n        metric_type: MilvusFloatSimilarityMetric | MilvusBinarySimilarityMetric | None = None,\n    ):\n        \"\"\"\n        :param cluster_units: Number of clusters (nlist in the docs)\n        :param inference_comparison: Number of cluster centroids to compare during inference (nprobe in the docs)\n            By default if this is not specified, it will be set to the same value as cluster_units.\n        \"\"\"\n        super().__init__(metric_type=metric_type)\n\n        self._assert_cluster_units_and_inference_comparison(cluster_units, inference_comparison)\n\n        self.nlist = cluster_units\n        self.nprobe = inference_comparison or cluster_units\n\n    def get_index_parameters(self):\n        return {\"nlist\": self.nlist}\n\n    def get_inference_parameters(self):\n        return {\"nprobe\": self.nprobe, \"metric_type\": self.metric_type.name}", "\n\nFLOATING_INDEXES : set[MilvusIndexBase] = {Milvus_FLAT, Milvus_IVF_FLAT, Milvus_IVF_SQ8, Milvus_IVF_PQ, Milvus_HNSW}\nBINARY_INDEXES : set[MilvusIndexBase] = {Milvus_BIN_FLAT, Milvus_BIN_IVF_FLAT}\n"]}
{"filename": "vectordb_orm/backends/milvus/__init__.py", "chunked_list": ["from vectordb_orm.backends.milvus.indexes import *\nfrom vectordb_orm.backends.milvus.milvus import *\nfrom vectordb_orm.backends.milvus.similarity import *\n"]}
{"filename": "vectordb_orm/backends/milvus/milvus.py", "chunked_list": ["from collections import defaultdict\nfrom logging import info\nfrom typing import Any, Type, get_args, get_origin\n\nimport numpy as np\nfrom pymilvus import Collection, Milvus\nfrom pymilvus.client.abstract import ChunkedQueryResult\nfrom pymilvus.client.types import DataType\nfrom pymilvus.orm.schema import CollectionSchema, FieldSchema\n", "from pymilvus.orm.schema import CollectionSchema, FieldSchema\n\nfrom vectordb_orm.attributes import AttributeCompare, OperationType\nfrom vectordb_orm.backends.base import BackendBase\nfrom vectordb_orm.backends.milvus.indexes import (BINARY_INDEXES,\n                                                  FLOATING_INDEXES)\nfrom vectordb_orm.base import VectorSchemaBase\nfrom vectordb_orm.fields import (BaseField, EmbeddingField, PrimaryKeyField,\n                                 VarCharField)\nfrom vectordb_orm.results import QueryResult", "                                 VarCharField)\nfrom vectordb_orm.results import QueryResult\n\n\nclass MilvusBackend(BackendBase):\n    # https://milvus.io/docs/search.md\n    max_fetch_size = 16384\n\n    def __init__(self, milvus_client: Milvus):\n        self.client = milvus_client\n\n    def create_collection(self, schema: Type[VectorSchemaBase]):\n        self._assert_embedding_validity(schema)\n        self._assert_has_primary(schema)\n\n        fields: list[FieldSchema] = []\n\n        for attribute_name, type_hint in schema.__annotations__.items():\n            fields.append(\n                self._field_schema_from_typehint(\n                    attribute_name,\n                    type_hint,\n                    schema._type_configuration.get(attribute_name)\n                )\n            )\n\n        print(f\"Creating collection {schema.collection_name()} with schema {fields}\")\n\n        field_schema = CollectionSchema(fields=fields, description=f\"{schema.__name__} vectordb-generated collection\")\n        collection = Collection(schema.collection_name(), field_schema)\n\n        # For all embeddings, create an associated index\n        for attribute_name, field_configuration in schema._type_configuration.items():\n            if isinstance(field_configuration, EmbeddingField):\n                index = {\n                    \"index_type\": field_configuration.index.index_type,\n                    \"metric_type\": field_configuration.index.metric_type.value,\n                    \"params\": field_configuration.index.get_index_parameters(),\n                }\n                print(f\"Creating index {index} for field {attribute_name}\")\n                collection.create_index(\"embedding\", index)\n\n        return collection\n\n    def clear_collection(self, schema: Type[VectorSchemaBase]):\n        # Since Milvus can only delete entities by listing explicit primary keys,\n        # the most efficient way to clear the collection is to fully delete and recreate it\n        self.delete_collection(schema)\n        self.create_collection(schema)\n\n    def delete_collection(self, schema: Type[VectorSchemaBase]):\n        self.client.drop_collection(schema.collection_name())\n\n    def insert(self, entity: VectorSchemaBase) -> int:\n        entities = self._dict_representation(entity)\n        print(\"Entity insertion\", entities)\n        mutation_result = self.client.insert(collection_name=entity.__class__.collection_name(), entities=entities)\n        return mutation_result.primary_keys[0]\n\n    def insert_batch(\n        self,\n        entities: list[VectorSchemaBase],\n        show_progress: bool,\n    ) -> list[int]:\n        if show_progress:\n            raise ValueError(\"Milvus backend does not support batch insertion progress logging because it is done in one operation.\")\n\n        # Group by the schema type since we allow for the insertion of multiple different schema\n        # `schema_to_entities` - input entities grouped by the schema name\n        # `schema_to_original_index` - since we switch to a per-schema representation, keep track of a mapping\n        #    from the schema to the original index in `entities`\n        # `schema_to_ids` - map of schema to the resulting primary keys\n        schema_to_original_index = defaultdict(list)\n        schema_to_class = {}\n        schema_to_ids = {}\n\n        for i, entity in enumerate(entities):\n            schema = entity.__class__\n            schema_name = schema.collection_name()\n\n            schema_to_original_index[schema_name].append(i)\n            schema_to_class[schema_name] = schema\n\n        for schema_name, schema_indexes in schema_to_original_index.items():\n            schema = schema_to_class[schema_name]\n            schema_entities = [entities[index] for index in schema_indexes]\n\n            # The primary key should be null at this stage of things, so we ignore it\n            # during the insertion\n            ignore_keys = {\n                self._get_primary(schema)\n            }\n\n            # Group this schema's objects by their keys\n            by_key_values = defaultdict(list)\n            by_key_type = {}\n\n            for entity in schema_entities:\n                for attribute_name, type_hint in entity.__annotations__.items():\n                    value = getattr(entity, attribute_name)\n                    db_type, value = self._type_to_value(type_hint, value)\n                    by_key_values[attribute_name].append(value)\n                    by_key_type[attribute_name] = db_type\n\n            # Ensure each key in `schema_to_objects` matches the quantity of objects\n            # that should have been created. This *shouldn't* happen but it's possible\n            # some combination of programatically deleting attributes or annotations will\n            # lead to this case. We proactively raise an error because this could result in\n            # data corruption.\n            all_lengths = {len(values) for values in by_key_values.values()}\n            if len(all_lengths) > 1:\n                raise ValueError(f\"Inserted objects don't align for schema `{schema_name}`\")\n\n            payload = [\n                {\n                    \"name\": attribute_name,\n                    \"type\": by_key_type[attribute_name],\n                    \"values\": values,\n                }\n                for attribute_name, values in by_key_values.items()\n                if attribute_name not in ignore_keys\n            ]\n\n            mutation_result = self.client.insert(collection_name=schema_name, entities=payload)\n            schema_to_ids[schema_name] = mutation_result.primary_keys\n\n        # Reorder ids to match the input entities\n        ordered_ids = [None] * len(entities)\n        for schema_name, primary_keys in schema_to_ids.items():\n            for i, original_index in enumerate(schema_to_original_index[schema_name]):\n                ordered_ids[original_index] = primary_keys[i]\n\n        return ordered_ids\n\n    def delete(self, entity: VectorSchemaBase):\n        schema = entity.__class__\n        identifier_key = self._get_primary(schema)\n        # Milvus only supports deleting entities with the `in` conditional; equality doesn't work\n        delete_expression = f\"{identifier_key} in [{entity.id}]\"\n        self.client.delete(collection_name=schema.collection_name(), expr=delete_expression)\n\n    def search(\n        self,\n        schema: Type[VectorSchemaBase],\n        output_fields: list[str],\n        filters: list[AttributeCompare] | None,\n        search_embedding: np.ndarray | None,\n        search_embedding_key: str | None,\n        limit: int,\n        offset: int,\n    ):\n        filters = \" and \".join(\n            [\n                self._attribute_to_expression(f)\n                for f in filters\n            ]\n        ) if filters else None\n\n        optional_args = dict()\n        if schema.consistency_type() is not None:\n            optional_args[\"consistency_level\"] = schema.consistency_type().value\n\n        # Milvus supports to different quering behaviors depending on whether or not we are looking\n        # for vector similarities\n        # A `search` is used when we are looking for vector similarities, and a `query` is used more akin\n        # to a traditional relational database when we're just looking to filter on metadata\n        if search_embedding_key is not None:\n            embedding_configuration : EmbeddingField = schema._type_configuration.get(search_embedding_key)\n\n            # Go through the same type conversion as the embedding field during insert time\n            _, similarity_value = self._type_to_value(\n                schema.__annotations__[search_embedding_key],\n                search_embedding,\n            )\n            query_records = [similarity_value]\n\n            search_result = self.client.search(\n                data=query_records,\n                anns_field=search_embedding_key,\n                param=embedding_configuration.index.get_inference_parameters(),\n                limit=limit,\n                offset=offset,\n                collection_name=schema.collection_name(),\n                expression=filters,\n                output_fields=output_fields,\n                **optional_args,\n            )\n        else:\n            search_result = self.client.query(\n                expr=filters,\n                offset=offset,\n                limit=limit,\n                output_fields=output_fields,\n                collection_name=schema.collection_name(),\n                **optional_args,\n            )\n        return self._result_to_objects(schema, search_result)\n\n    def flush(self, schema: Type[VectorSchemaBase]):\n        self.client.flush([schema.collection_name()])\n\n    def load(self, schema: Type[VectorSchemaBase]):\n        self.client.load_collection(schema.collection_name())\n\n    def _field_schema_from_typehint(cls, name: str, type_hint: Any, field_customization: BaseField | None = None):\n        \"\"\"\n        Create a FieldSchema based on the provided type hint and field customization.\n\n        This function is called internally during the creation of a MilvusBase instance.\n\n        :param name: The name of the field.\n        :param type_hint: The type hint for the field.\n        :param field_customization: An optional customization object for the field.\n        :returns: A FieldSchema instance.\n        :raises ValueError: If an unsupported type hint is provided or the type hint configuration is incorrect.\n        \"\"\"\n        if issubclass(extract_base_type(type_hint), np.ndarray):\n            if not isinstance(field_customization, EmbeddingField):\n                raise ValueError(\"Embedding typehints should be configured with vectordb.EmbeddingField\")\n\n            type_arguments = get_args(type_hint)\n            vector_type = (\n                DataType.BINARY_VECTOR\n                if type_arguments and type_arguments[0] == np.bool_\n                else DataType.FLOAT_VECTOR\n            )\n\n            return FieldSchema(name=name, dtype=vector_type, dim=field_customization.dim)\n        elif issubclass(type_hint, str):\n            if not isinstance(field_customization, VarCharField):\n                raise ValueError(\"String typehints should be configured with vectordb.VarCharField\")\n            return FieldSchema(name=name, dtype=DataType.VARCHAR, max_length=field_customization.max_length)\n        elif issubclass(type_hint, int):\n            # We currently only support ints as the primary key\n            is_primary = isinstance(field_customization, PrimaryKeyField)\n\n            # Mirror the python int sizing\n            return FieldSchema(name=name, dtype=DataType.INT64, is_primary=is_primary, auto_id=is_primary)\n        elif issubclass(type_hint, float):\n            # Mirror the python float sizing, equal to a double\n            return FieldSchema(name=name, dtype=DataType.DOUBLE)\n        else:\n            raise ValueError(f\"{cls.__name__}: Typehint {name}:{type_hint} is not supported\")\n\n    def _type_to_value(self, type_hint, value: Any | None = None):\n        \"\"\"\n        Use the typehint signatures to convert into milvus DataType. Also convert the values\n        into the correct format for milvus insertion.\n\n        \"\"\"\n        if issubclass(extract_base_type(type_hint), np.ndarray):\n            type_arguments = get_args(type_hint)\n            vector_type = (\n                DataType.BINARY_VECTOR\n                if type_arguments and type_arguments[0] == np.bool_\n                else DataType.FLOAT_VECTOR\n            )\n            if vector_type == DataType.BINARY_VECTOR:\n                if value is not None:\n                    packed_uint8_array = np.packbits(value)\n                    #value = packed_uint8_array.tobytes()\n                    value = bytes(packed_uint8_array.tolist())\n                return vector_type, value\n            else:\n                if value is not None:\n                    value = value.tolist()\n                return vector_type, value\n        elif issubclass(type_hint, str):\n            return DataType.VARCHAR, value\n        elif issubclass(type_hint, int):\n            return DataType.INT64, value\n        elif issubclass(type_hint, float):\n            return DataType.DOUBLE, value\n        else:\n            raise ValueError(f\"Typehint {type_hint} is not supported\")\n\n    def _dict_representation(self, entity: VectorSchemaBase):\n        \"\"\"\n        Convert the MilvusBase object to a dictionary representation for storage.\n\n        This function is called internally during the insertion of a MilvusBase object.\n\n        :returns: A list of dictionaries containing the name, type, and values of the attributes.\n        \"\"\"\n        payload = []\n\n        for attribute_name, type_hint in entity.__annotations__.items():\n            value = getattr(entity, attribute_name)\n            if value is not None:\n                db_type, value = self._type_to_value(type_hint, value)\n                payload.append(\n                    {\n                        \"name\": attribute_name,\n                        \"type\": db_type,\n                        \"values\": [value]\n                    }\n                )\n        return payload\n\n    def _assert_embedding_validity(self, schema: Type[VectorSchemaBase]):\n        \"\"\"\n        Ensure that the embedding configurations align with the typehinting\n        \"\"\"\n        # For all embeddings, create an associated index\n        for attribute_name, field_configuration in schema._type_configuration.items():\n            if not isinstance(field_configuration, EmbeddingField):\n                continue\n\n            field_index = field_configuration.index\n            vector_type, _ = self._type_to_value(schema.__annotations__[attribute_name], None)\n            # Ensure that the index type is compatible with these fields\n            if vector_type == DataType.BINARY_VECTOR and not isinstance(field_index, tuple(BINARY_INDEXES)):\n                raise ValueError(f\"Index type {field_index} is not compatible with binary vectors.\")\n            elif vector_type == DataType.FLOAT_VECTOR and not isinstance(field_index, tuple(FLOATING_INDEXES)):\n                raise ValueError(f\"Index type {field_index} is not compatible with float vectors.\")\n\n            # Milvus max size\n            # https://milvus.io/docs/limitations.md\n            if field_configuration.dim > 32768:\n                raise ValueError(f\"Milvus only supports vectors with dimensions under 32768. {attribute_name} is too large: {field_configuration.dim}.\")\n\n    def _result_to_objects(\n        self,\n        schema: Type[VectorSchemaBase],\n        search_result: ChunkedQueryResult | list[dict[str, Any]]\n    ):\n        query_results : list[QueryResult] = []\n\n        if isinstance(search_result, ChunkedQueryResult):\n            for hit in search_result:\n                for result in hit:\n                    entity = {\n                        key: result.entity.get(key)\n                        for key in result.entity.fields\n                    }\n                    obj = schema.from_dict(entity)\n                    query_results.append(QueryResult(obj, score=result.score, distance=result.distance))\n        else:\n            for result in search_result:\n                obj = schema.from_dict(result)\n                query_results.append(QueryResult(obj))\n\n        return query_results\n\n    def _attribute_to_expression(self, attribute: AttributeCompare):\n        value = attribute.value\n        if isinstance(value, str):\n            value = f\"\\\"{attribute.value}\\\"\"\n\n        operation_type_maps = {\n            OperationType.EQUALS: '==',\n            OperationType.GREATER_THAN: '>',\n            OperationType.LESS_THAN: '<',\n            OperationType.LESS_THAN_EQUAL: '<=',\n            OperationType.GREATER_THAN_EQUAL: '>=',\n            OperationType.NOT_EQUAL: '!='\n        }\n\n        return f\"{attribute.attr} {operation_type_maps[attribute.op]} {value}\"", "\ndef extract_base_type(type):\n    \"\"\"\n    Given a type like `np.ndarray[np.float32]`, return the root type `np.ndarray`.\n    Also works if passed a non-generic type like `np.ndarray`.\n    \"\"\"\n    return get_origin(type) or type\n"]}
