{"filename": "sdk/setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nsetup(\n    name='havenpy',\n    version='0.2.0',\n    author='Haven Technologies Inc.',\n    author_email='hello@havenllm.com',\n    description='Haven SDK',\n    packages=find_packages(),\n    install_requires=[", "    packages=find_packages(),\n    install_requires=[\n        'grpcio==1.54.2',\n        'protobuf==4.23.3'\n    ],\n)\n"]}
{"filename": "sdk/test.py", "chunked_list": ["from havenpy import Haven\n\nclient = Haven(\"localhost:50051\", \"insecure\")\n\n#with open(\"./key.json\", \"r\") as f:\n\t#client.setup(f.read())\n\n\"\"\"arr = client.list_workers()\nfor a in arr.workers:\n\tprint(\"DELETE \", a.worker_name)", "for a in arr.workers:\n\tprint(\"DELETE \", a.worker_name)\n\tclient.delete_inference_worker(a.worker_name)\"\"\"\n\n\n    \n# print(client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-chat\", quantization=\"float16\", gpu_type=\"A100\", gpu_count=1))\n\"\"\"\nprint(client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-chat\", quantization=\"float16\", gpu_type=\"T4\", gpu_count=1))\n", "print(client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-chat\", quantization=\"float16\", gpu_type=\"T4\", gpu_count=1))\n\nprint(client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-instruct\", quantization=\"float16\", gpu_type=\"A100\", gpu_count=1))\n\nprint(client.create_inference_worker(model_name=\"@huggingface/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2\", quantization=\"int8\", gpu_type=\"T4\", gpu_count=1))\nprint(client.create_inference_worker(model_name=\"@huggingface/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2\", quantization=\"float16\", gpu_type=\"T4\", gpu_count=1))\nprint(client.create_inference_worker(model_name=\"@huggingface/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2\", quantization=\"float16\", gpu_type=\"A100\", gpu_count=1))\n\n\"\"\"\n", "\"\"\"\n\n\"\"\"client.chat_completion(\"haven-example-base-lj7ieyhi\", messages=[{\n    \"content\": \"Give me a recipe for cake!\",\n    \"role\": \"USER\"\n}], stream=True)\"\"\"\n\n# print(client.list_models())\n\nprint(client.list_workers())", "\nprint(client.list_workers())\n\n# print(client.create_inference_worker(\"@huggingface/lmsys/vicuna-7b-v1.3\", gpu_type=\"A100\", gpu_count=1, quantization=\"float16\"))\n# print(client.create_inference_worker(\"@huggingface/lmsys/vicuna-7b-v1.3\", gpu_type=\"T4\", gpu_count=1, quantization=\"float16\"))\n\n\"\"\"res = client.chat_completion(\"haven-w-vicuna-7b-v1-3-ljm41836\", messages=[{\n    \"content\": \"Write a newspaper article about Marc Zuckerberg.\",\n    \"role\": \"USER\"\n}], stream=True)", "    \"role\": \"USER\"\n}], stream=True)\n\nfor r in res:\n    print(r.text, end=\"\", flush=True)\"\"\"\n\n# print(client.create_inference_worker(model_name=\"@huggingface/togethercomputer/RedPajama-INCITE-Chat-3B-v1\", quantization=\"float16\", gpu_type=\"T4\", gpu_count=1))\n\n\"\"\"history = []\n", "\"\"\"history = []\n\nhistory.append({\n\t\"content\": my_description,\n\t\"role\": \"USER\"\n})\n\nwhile True:\n\tres = client.chat_completion(\"haven-redpajama-incite-chat-3b-v1-ljg7qwku\", messages=history, stream=True)\n\tmessage = \"\"", "\tres = client.chat_completion(\"haven-redpajama-incite-chat-3b-v1-ljg7qwku\", messages=history, stream=True)\n\tmessage = \"\"\n\tfor r in res:\n\t\tmessage += r.text\n\t\tprint(r.text, end=\"\", flush=True)\n\t\n\tprint()\n\n\thistory.append({\n\t\t\"content\": message,", "\thistory.append({\n\t\t\"content\": message,\n\t\t\"role\": \"ASSISTANT\"\n\t})\n\n\tuser_input = input(\"Your response: \")\n\thistory.append({\n\t\t\"content\": user_input,\n\t\t\"role\": \"USER\"\n\t})\"\"\"", "\t\t\"role\": \"USER\"\n\t})\"\"\"\n\n#key_file = open(\"key.txt\", \"r\")\n#client.setup(key_file=key_file.read())\n\n# client.create_inference_worker(model_name=\"@huggingface/mosaicml/mpt-7b-chat\", quantization=\"float16\", gpu_type=\"A100\", gpu_count=1)\n#res = client.generate(\"haven-example-base-lj7ieyhi\", \"Give me a recipe for cake!\", stream=True)\n\n#for r in res:", "\n#for r in res:\n\t#print(r.text)\n\n# client.pause_inference_worker(\"haven-example-base-lj7ieyhi\")\n#client.resume_inference_worker(\"haven-example-base-lj7ieyhi\")\n\n# client.delete_inference_worker(\"haven-example-base-lj7ieyhi\")\n\n# res = client.generate(\"test2\", \"Schreibe einen Nachrichtenartikel \u00fcber die aktuelle Lage in Deutschland.\", stream=False)", "\n# res = client.generate(\"test2\", \"Schreibe einen Nachrichtenartikel \u00fcber die aktuelle Lage in Deutschland.\", stream=False)\n# print(res)\"\"\"\n"]}
{"filename": "sdk/havenpy/run.py", "chunked_list": ["import grpc\n\nfrom .interceptor import add_header\nfrom .pb import manager_pb2_grpc, manager_pb2\n\nfrom typing import List\n\ndef stream_to_string(stream: manager_pb2.CompletionResponse) -> str:\n\tres: str = \"\"\n\tfor response in stream:\n\t\tres += response.text\n\n\treturn res", "\nclass Haven:\n\tclient: manager_pb2_grpc.HavenStub\n\n\tdef __init__(self, url: str, token: str):\n\t\tchannel = grpc.insecure_channel(url)\n\t\tinterceptor = add_header('authorization', f'Bearer {token}')\n\t\tchannel = grpc.intercept_channel(channel, interceptor)\n\t\tself.client = manager_pb2_grpc.HavenStub(channel)\n\n\t\tself.setup()\n\n\tdef setup(self, key_file: str = None) -> None:\n\t\trequest = manager_pb2.SetupRequest(key_file=key_file)\n\t\tresponse: manager_pb2.SetupResponse = self.client.Setup(request)\n\n\t\tif hasattr(response, \"message\") and response.message != \"\":\n\t\t\tprint(response.message)\n\n\tdef chat_completion(self, worker_name: str, messages: List[manager_pb2.Message], stream: bool = False, max_tokens: int = -1, top_p: float = -1, top_k: int = -1, temperature: float = -1) -> manager_pb2.CompletionResponse or str:\n\t\trequest = manager_pb2.ChatCompletionRequest(worker_name=worker_name, messages=messages, max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t\tresponseStream: manager_pb2.CompletionResponse = self.client.ChatCompletion(request)\n\n\t\tif stream:\n\t\t\treturn responseStream\n\t\t\n\t\treturn stream_to_string(responseStream)\n\t\n\tdef completion(self, worker_name: str, prompt: str, stream: bool = False, max_tokens: int = -1, top_p: float = -1, top_k: int = -1, temperature: float = -1) -> manager_pb2.CompletionResponse or str:\n\t\t# TODO: we're currently not using stop_tokens\n\t\trequest = manager_pb2.CompletionRequest(worker_name=worker_name, prompt=prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\t\tresponseStream: manager_pb2.CompletionResponse = self.client.Completion(request)\n\n\t\tif stream:\n\t\t\treturn responseStream\n\t\t\n\t\treturn stream_to_string(responseStream)\n\t\n\tdef list_models(self) -> manager_pb2.ListModelsResponse:\n\t\trequest = manager_pb2.Empty()\n\t\treturn self.client.ListModels(request)\n\t\n\tdef add_model(self, architecture: str, name: str, tokenizer: str, system_prompt: str = None, instruction_prefix: str = None, instruction_postfix: str = None, output_prefix: str = None, output_postfix: str = None) -> manager_pb2.Empty:\n\t\trequest = manager_pb2.Model(architecture=architecture, name=name, tokenizer=tokenizer, system_prompt=system_prompt, instruction_prefix=instruction_prefix, instruction_postfix=instruction_postfix, output_prefix=output_prefix, output_postfix=output_postfix)\n\t\treturn self.client.AddModel(request)\n\t\n\tdef delete_model(self, name: str) -> manager_pb2.Empty:\n\t\trequest = manager_pb2.ModelName(name=name)\n\t\treturn self.client.DeleteModel(request)\n\t\n\tdef list_workers(self) -> manager_pb2.ListWorkersResponse:\n\t\trequest = manager_pb2.Empty()\n\t\tresponse = self.client.ListWorkers(request)\n\n\t\t# Response is of a weird GRPC type, so we transform it to a list of dicts\n\t\t# with worker_name and status as string attributes\n\n\t\tworkers = []\n\t\tfor worker in response.workers:\n\t\t\t# Convert enum to string representation\n\t\t\tstatus = manager_pb2.Status.Name(worker.status)\n\n\t\t\tworkers.append({\n\t\t\t\t\"worker_name\": worker.worker_name,\n\t\t\t\t\"status\": status\n\t\t\t})\n\n\t\treturn workers\n\t\n\tdef create_inference_worker(self, model_name: str, quantization: str, worker_name: str = None, gpu_type: manager_pb2.GpuType = None, gpu_count: int = None, zone: str = None) -> manager_pb2.InferenceWorker:\n\t\trequest = manager_pb2.CreateInferenceWorkerRequest(model_name=model_name, quantization=quantization, worker_name=worker_name, gpu_type=gpu_type, gpu_count=gpu_count, zone=zone)\n\t\tresponse = self.client.CreateInferenceWorker(request)\n\t\treturn response.worker_name\n\t\n\tdef pause_inference_worker(self, worker_name: str) -> manager_pb2.InferenceWorker:\n\t\trequest = manager_pb2.InferenceWorker(worker_name=worker_name)\n\t\treturn self.client.PauseInferenceWorker(request)\n\t\n\tdef resume_inference_worker(self, worker_name: str) -> manager_pb2.InferenceWorker:\n\t\trequest = manager_pb2.InferenceWorker(worker_name=worker_name)\n\t\treturn self.client.ResumeInferenceWorker(request)\n\t\n\tdef delete_inference_worker(self, worker_name: str) -> manager_pb2.InferenceWorker:\n\t\trequest = manager_pb2.InferenceWorker(worker_name=worker_name)\n\t\treturn self.client.DeleteInferenceWorker(request)", ""]}
{"filename": "sdk/havenpy/interceptor.py", "chunked_list": ["#\n# See https://github.com/grpc/grpc/blob/master/examples/python/interceptors/headers\n#\n\nimport grpc\nimport collections\n\nclass _GenericClientInterceptor(grpc.UnaryUnaryClientInterceptor,\n                                grpc.UnaryStreamClientInterceptor,\n                                grpc.StreamUnaryClientInterceptor,\n                                grpc.StreamStreamClientInterceptor):\n\n    def __init__(self, interceptor_function):\n        self._fn = interceptor_function\n\n    def intercept_unary_unary(self, continuation, client_call_details, request):\n        new_details, new_request_iterator, postprocess = self._fn(\n            client_call_details, iter((request,)), False, False)\n        response = continuation(new_details, next(new_request_iterator))\n        return postprocess(response) if postprocess else response\n\n    def intercept_unary_stream(self, continuation, client_call_details,\n                               request):\n        new_details, new_request_iterator, postprocess = self._fn(\n            client_call_details, iter((request,)), False, True)\n        response_it = continuation(new_details, next(new_request_iterator))\n        return postprocess(response_it) if postprocess else response_it\n\n    def intercept_stream_unary(self, continuation, client_call_details,\n                               request_iterator):\n        new_details, new_request_iterator, postprocess = self._fn(\n            client_call_details, request_iterator, True, False)\n        response = continuation(new_details, new_request_iterator)\n        return postprocess(response) if postprocess else response\n\n    def intercept_stream_stream(self, continuation, client_call_details,\n                                request_iterator):\n        new_details, new_request_iterator, postprocess = self._fn(\n            client_call_details, request_iterator, True, True)\n        response_it = continuation(new_details, new_request_iterator)\n        return postprocess(response_it) if postprocess else response_it", "\nclass _ClientCallDetails(\n        collections.namedtuple(\n            '_ClientCallDetails',\n            ('method', 'timeout', 'metadata', 'credentials')),\n        grpc.ClientCallDetails):\n    pass\n\ndef add_header(key, value):\n\tdef intercept_call(client_call_details, request_iterator, request_streaming, response_streaming):\n\t\tmetadata = []\n\t\tif client_call_details.metadata != None:\n\t\t\tmetadata = list(client_call_details.metadata)\n\t\tmetadata.append((key, value))\n\n\t\tclient_call_details = _ClientCallDetails(\n\t\t\tclient_call_details.method, client_call_details.timeout, metadata, \n\t\t\tclient_call_details.credentials)\n\t\t\n\t\treturn client_call_details, request_iterator, None\n\t\n\treturn _GenericClientInterceptor(intercept_call)", "def add_header(key, value):\n\tdef intercept_call(client_call_details, request_iterator, request_streaming, response_streaming):\n\t\tmetadata = []\n\t\tif client_call_details.metadata != None:\n\t\t\tmetadata = list(client_call_details.metadata)\n\t\tmetadata.append((key, value))\n\n\t\tclient_call_details = _ClientCallDetails(\n\t\t\tclient_call_details.method, client_call_details.timeout, metadata, \n\t\t\tclient_call_details.credentials)\n\t\t\n\t\treturn client_call_details, request_iterator, None\n\t\n\treturn _GenericClientInterceptor(intercept_call)", ""]}
{"filename": "sdk/havenpy/__init__.py", "chunked_list": ["from .run import Haven"]}
{"filename": "sdk/havenpy/pb/manager_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: manager.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf.internal import builder as _builder\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n", "# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\rmanager.proto\\x12\\x05haven\\\"\\x07\\n\\x05\\x45mpty\\\"2\\n\\x0cSetupRequest\\x12\\x15\\n\\x08key_file\\x18\\x01 \\x01(\\tH\\x00\\x88\\x01\\x01\\x42\\x0b\\n\\t_key_file\\\"1\\n\\rSetupResponse\\x12\\x14\\n\\x07message\\x18\\x01 \\x01(\\tH\\x00\\x88\\x01\\x01\\x42\\n\\n\\x08_message\\\"5\\n\\x07Message\\x12\\x19\\n\\x04role\\x18\\x01 \\x01(\\x0e\\x32\\x0b.haven.Role\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\t\\\"\\xdc\\x01\\n\\x15\\x43hatCompletionRequest\\x12\\x13\\n\\x0bworker_name\\x18\\x01 \\x01(\\t\\x12 \\n\\x08messages\\x18\\x02 \\x03(\\x0b\\x32\\x0e.haven.Message\\x12\\x17\\n\\nmax_tokens\\x18\\x03 \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12\\x12\\n\\x05top_p\\x18\\x04 \\x01(\\x02H\\x01\\x88\\x01\\x01\\x12\\x12\\n\\x05top_k\\x18\\x05 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x18\\n\\x0btemperature\\x18\\x06 \\x01(\\x02H\\x03\\x88\\x01\\x01\\x42\\r\\n\\x0b_max_tokensB\\x08\\n\\x06_top_pB\\x08\\n\\x06_top_kB\\x0e\\n\\x0c_temperature\\\"\\xdb\\x01\\n\\x11\\x43ompletionRequest\\x12\\x13\\n\\x0bworker_name\\x18\\x01 \\x01(\\t\\x12\\x0e\\n\\x06prompt\\x18\\x02 \\x01(\\t\\x12\\x13\\n\\x0bstop_tokens\\x18\\x07 \\x03(\\t\\x12\\x17\\n\\nmax_tokens\\x18\\x03 \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12\\x12\\n\\x05top_p\\x18\\x04 \\x01(\\x02H\\x01\\x88\\x01\\x01\\x12\\x12\\n\\x05top_k\\x18\\x05 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x18\\n\\x0btemperature\\x18\\x06 \\x01(\\x02H\\x03\\x88\\x01\\x01\\x42\\r\\n\\x0b_max_tokensB\\x08\\n\\x06_top_pB\\x08\\n\\x06_top_kB\\x0e\\n\\x0c_temperature\\\"\\\"\\n\\x12\\x43ompletionResponse\\x12\\x0c\\n\\x04text\\x18\\x01 \\x01(\\t\\\"\\xbc\\x02\\n\\x05Model\\x12\\x14\\n\\x0c\\x61rchitecture\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\x11\\n\\ttokenizer\\x18\\x03 \\x01(\\t\\x12\\x1a\\n\\rsystem_prompt\\x18\\x04 \\x01(\\tH\\x00\\x88\\x01\\x01\\x12\\x1f\\n\\x12instruction_prefix\\x18\\x05 \\x01(\\tH\\x01\\x88\\x01\\x01\\x12 \\n\\x13instruction_postfix\\x18\\x06 \\x01(\\tH\\x02\\x88\\x01\\x01\\x12\\x1a\\n\\routput_prefix\\x18\\x07 \\x01(\\tH\\x03\\x88\\x01\\x01\\x12\\x1b\\n\\x0eoutput_postfix\\x18\\x08 \\x01(\\tH\\x04\\x88\\x01\\x01\\x42\\x10\\n\\x0e_system_promptB\\x15\\n\\x13_instruction_prefixB\\x16\\n\\x14_instruction_postfixB\\x10\\n\\x0e_output_prefixB\\x11\\n\\x0f_output_postfix\\\"\\x19\\n\\tModelName\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\\"2\\n\\x12ListModelsResponse\\x12\\x1c\\n\\x06models\\x18\\x01 \\x03(\\x0b\\x32\\x0c.haven.Model\\\"<\\n\\x06Worker\\x12\\x13\\n\\x0bworker_name\\x18\\x01 \\x01(\\t\\x12\\x1d\\n\\x06status\\x18\\x02 \\x01(\\x0e\\x32\\r.haven.Status\\\"5\\n\\x13ListWorkersResponse\\x12\\x1e\\n\\x07workers\\x18\\x01 \\x03(\\x0b\\x32\\r.haven.Worker\\\"\\xe8\\x01\\n\\x1c\\x43reateInferenceWorkerRequest\\x12\\x12\\n\\nmodel_name\\x18\\x01 \\x01(\\t\\x12\\x14\\n\\x0cquantization\\x18\\x02 \\x01(\\t\\x12\\x18\\n\\x0bworker_name\\x18\\x03 \\x01(\\tH\\x00\\x88\\x01\\x01\\x12%\\n\\x08gpu_type\\x18\\x04 \\x01(\\x0e\\x32\\x0e.haven.GpuTypeH\\x01\\x88\\x01\\x01\\x12\\x16\\n\\tgpu_count\\x18\\x06 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x11\\n\\x04zone\\x18\\x07 \\x01(\\tH\\x03\\x88\\x01\\x01\\x42\\x0e\\n\\x0c_worker_nameB\\x0b\\n\\t_gpu_typeB\\x0c\\n\\n_gpu_countB\\x07\\n\\x05_zone\\\"&\\n\\x0fInferenceWorker\\x12\\x13\\n\\x0bworker_name\\x18\\x01 \\x01(\\t*\\x1f\\n\\x04Role\\x12\\r\\n\\tASSISTANT\\x10\\x00\\x12\\x08\\n\\x04USER\\x10\\x01*8\\n\\x06Status\\x12\\n\\n\\x06ONLINE\\x10\\x00\\x12\\x0b\\n\\x07LOADING\\x10\\x01\\x12\\n\\n\\x06PAUSED\\x10\\x02\\x12\\t\\n\\x05\\x45RROR\\x10\\x03**\\n\\x07GpuType\\x12\\x08\\n\\x04\\x41\\x31\\x30\\x30\\x10\\x00\\x12\\r\\n\\tA100_80GB\\x10\\x01\\x12\\x06\\n\\x02T4\\x10\\x02\\x32\\xda\\x05\\n\\x05Haven\\x12\\x34\\n\\x05Setup\\x12\\x13.haven.SetupRequest\\x1a\\x14.haven.SetupResponse\\\"\\x00\\x12M\\n\\x0e\\x43hatCompletion\\x12\\x1c.haven.ChatCompletionRequest\\x1a\\x19.haven.CompletionResponse\\\"\\x00\\x30\\x01\\x12\\x45\\n\\nCompletion\\x12\\x18.haven.CompletionRequest\\x1a\\x19.haven.CompletionResponse\\\"\\x00\\x30\\x01\\x12\\x37\\n\\nListModels\\x12\\x0c.haven.Empty\\x1a\\x19.haven.ListModelsResponse\\\"\\x00\\x12(\\n\\x08\\x41\\x64\\x64Model\\x12\\x0c.haven.Model\\x1a\\x0c.haven.Empty\\\"\\x00\\x12/\\n\\x0b\\x44\\x65leteModel\\x12\\x10.haven.ModelName\\x1a\\x0c.haven.Empty\\\"\\x00\\x12\\x39\\n\\x0bListWorkers\\x12\\x0c.haven.Empty\\x1a\\x1a.haven.ListWorkersResponse\\\"\\x00\\x12V\\n\\x15\\x43reateInferenceWorker\\x12#.haven.CreateInferenceWorkerRequest\\x1a\\x16.haven.InferenceWorker\\\"\\x00\\x12H\\n\\x14PauseInferenceWorker\\x12\\x16.haven.InferenceWorker\\x1a\\x16.haven.InferenceWorker\\\"\\x00\\x12I\\n\\x15ResumeInferenceWorker\\x12\\x16.haven.InferenceWorker\\x1a\\x16.haven.InferenceWorker\\\"\\x00\\x12I\\n\\x15\\x44\\x65leteInferenceWorker\\x12\\x16.haven.InferenceWorker\\x1a\\x16.haven.InferenceWorker\\\"\\x00\\x62\\x06proto3')\n\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())", "\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'manager_pb2', globals())\nif _descriptor._USE_C_DESCRIPTORS == False:\n\n  DESCRIPTOR._options = None\n  _ROLE._serialized_start=1462\n  _ROLE._serialized_end=1493\n  _STATUS._serialized_start=1495\n  _STATUS._serialized_end=1551\n  _GPUTYPE._serialized_start=1553\n  _GPUTYPE._serialized_end=1595\n  _EMPTY._serialized_start=24\n  _EMPTY._serialized_end=31\n  _SETUPREQUEST._serialized_start=33\n  _SETUPREQUEST._serialized_end=83\n  _SETUPRESPONSE._serialized_start=85\n  _SETUPRESPONSE._serialized_end=134\n  _MESSAGE._serialized_start=136\n  _MESSAGE._serialized_end=189\n  _CHATCOMPLETIONREQUEST._serialized_start=192\n  _CHATCOMPLETIONREQUEST._serialized_end=412\n  _COMPLETIONREQUEST._serialized_start=415\n  _COMPLETIONREQUEST._serialized_end=634\n  _COMPLETIONRESPONSE._serialized_start=636\n  _COMPLETIONRESPONSE._serialized_end=670\n  _MODEL._serialized_start=673\n  _MODEL._serialized_end=989\n  _MODELNAME._serialized_start=991\n  _MODELNAME._serialized_end=1016\n  _LISTMODELSRESPONSE._serialized_start=1018\n  _LISTMODELSRESPONSE._serialized_end=1068\n  _WORKER._serialized_start=1070\n  _WORKER._serialized_end=1130\n  _LISTWORKERSRESPONSE._serialized_start=1132\n  _LISTWORKERSRESPONSE._serialized_end=1185\n  _CREATEINFERENCEWORKERREQUEST._serialized_start=1188\n  _CREATEINFERENCEWORKERREQUEST._serialized_end=1420\n  _INFERENCEWORKER._serialized_start=1422\n  _INFERENCEWORKER._serialized_end=1460\n  _HAVEN._serialized_start=1598\n  _HAVEN._serialized_end=2328", "# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "sdk/havenpy/pb/__init__.py", "chunked_list": [""]}
{"filename": "sdk/havenpy/pb/manager_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport grpc\n\nfrom . import manager_pb2 as manager__pb2\n\n\nclass HavenStub(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    def __init__(self, channel):\n        \"\"\"Constructor.\n\n        Args:\n            channel: A grpc.Channel.\n        \"\"\"\n        self.Setup = channel.unary_unary(\n                '/haven.Haven/Setup',\n                request_serializer=manager__pb2.SetupRequest.SerializeToString,\n                response_deserializer=manager__pb2.SetupResponse.FromString,\n                )\n        self.ChatCompletion = channel.unary_stream(\n                '/haven.Haven/ChatCompletion',\n                request_serializer=manager__pb2.ChatCompletionRequest.SerializeToString,\n                response_deserializer=manager__pb2.CompletionResponse.FromString,\n                )\n        self.Completion = channel.unary_stream(\n                '/haven.Haven/Completion',\n                request_serializer=manager__pb2.CompletionRequest.SerializeToString,\n                response_deserializer=manager__pb2.CompletionResponse.FromString,\n                )\n        self.ListModels = channel.unary_unary(\n                '/haven.Haven/ListModels',\n                request_serializer=manager__pb2.Empty.SerializeToString,\n                response_deserializer=manager__pb2.ListModelsResponse.FromString,\n                )\n        self.AddModel = channel.unary_unary(\n                '/haven.Haven/AddModel',\n                request_serializer=manager__pb2.Model.SerializeToString,\n                response_deserializer=manager__pb2.Empty.FromString,\n                )\n        self.DeleteModel = channel.unary_unary(\n                '/haven.Haven/DeleteModel',\n                request_serializer=manager__pb2.ModelName.SerializeToString,\n                response_deserializer=manager__pb2.Empty.FromString,\n                )\n        self.ListWorkers = channel.unary_unary(\n                '/haven.Haven/ListWorkers',\n                request_serializer=manager__pb2.Empty.SerializeToString,\n                response_deserializer=manager__pb2.ListWorkersResponse.FromString,\n                )\n        self.CreateInferenceWorker = channel.unary_unary(\n                '/haven.Haven/CreateInferenceWorker',\n                request_serializer=manager__pb2.CreateInferenceWorkerRequest.SerializeToString,\n                response_deserializer=manager__pb2.InferenceWorker.FromString,\n                )\n        self.PauseInferenceWorker = channel.unary_unary(\n                '/haven.Haven/PauseInferenceWorker',\n                request_serializer=manager__pb2.InferenceWorker.SerializeToString,\n                response_deserializer=manager__pb2.InferenceWorker.FromString,\n                )\n        self.ResumeInferenceWorker = channel.unary_unary(\n                '/haven.Haven/ResumeInferenceWorker',\n                request_serializer=manager__pb2.InferenceWorker.SerializeToString,\n                response_deserializer=manager__pb2.InferenceWorker.FromString,\n                )\n        self.DeleteInferenceWorker = channel.unary_unary(\n                '/haven.Haven/DeleteInferenceWorker',\n                request_serializer=manager__pb2.InferenceWorker.SerializeToString,\n                response_deserializer=manager__pb2.InferenceWorker.FromString,\n                )", "\n\nclass HavenServicer(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    def Setup(self, request, context):\n        \"\"\"Setup (first time starting the manager)\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def ChatCompletion(self, request, context):\n        \"\"\"Generate text.\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def Completion(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def ListModels(self, request, context):\n        \"\"\"Get the list of models and their descriptions.\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def AddModel(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def DeleteModel(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def ListWorkers(self, request, context):\n        \"\"\"Get the list of workers and their statuses.\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def CreateInferenceWorker(self, request, context):\n        \"\"\"Inference worker management.\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def PauseInferenceWorker(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def ResumeInferenceWorker(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def DeleteInferenceWorker(self, request, context):\n        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')", "\n\ndef add_HavenServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n            'Setup': grpc.unary_unary_rpc_method_handler(\n                    servicer.Setup,\n                    request_deserializer=manager__pb2.SetupRequest.FromString,\n                    response_serializer=manager__pb2.SetupResponse.SerializeToString,\n            ),\n            'ChatCompletion': grpc.unary_stream_rpc_method_handler(\n                    servicer.ChatCompletion,\n                    request_deserializer=manager__pb2.ChatCompletionRequest.FromString,\n                    response_serializer=manager__pb2.CompletionResponse.SerializeToString,\n            ),\n            'Completion': grpc.unary_stream_rpc_method_handler(\n                    servicer.Completion,\n                    request_deserializer=manager__pb2.CompletionRequest.FromString,\n                    response_serializer=manager__pb2.CompletionResponse.SerializeToString,\n            ),\n            'ListModels': grpc.unary_unary_rpc_method_handler(\n                    servicer.ListModels,\n                    request_deserializer=manager__pb2.Empty.FromString,\n                    response_serializer=manager__pb2.ListModelsResponse.SerializeToString,\n            ),\n            'AddModel': grpc.unary_unary_rpc_method_handler(\n                    servicer.AddModel,\n                    request_deserializer=manager__pb2.Model.FromString,\n                    response_serializer=manager__pb2.Empty.SerializeToString,\n            ),\n            'DeleteModel': grpc.unary_unary_rpc_method_handler(\n                    servicer.DeleteModel,\n                    request_deserializer=manager__pb2.ModelName.FromString,\n                    response_serializer=manager__pb2.Empty.SerializeToString,\n            ),\n            'ListWorkers': grpc.unary_unary_rpc_method_handler(\n                    servicer.ListWorkers,\n                    request_deserializer=manager__pb2.Empty.FromString,\n                    response_serializer=manager__pb2.ListWorkersResponse.SerializeToString,\n            ),\n            'CreateInferenceWorker': grpc.unary_unary_rpc_method_handler(\n                    servicer.CreateInferenceWorker,\n                    request_deserializer=manager__pb2.CreateInferenceWorkerRequest.FromString,\n                    response_serializer=manager__pb2.InferenceWorker.SerializeToString,\n            ),\n            'PauseInferenceWorker': grpc.unary_unary_rpc_method_handler(\n                    servicer.PauseInferenceWorker,\n                    request_deserializer=manager__pb2.InferenceWorker.FromString,\n                    response_serializer=manager__pb2.InferenceWorker.SerializeToString,\n            ),\n            'ResumeInferenceWorker': grpc.unary_unary_rpc_method_handler(\n                    servicer.ResumeInferenceWorker,\n                    request_deserializer=manager__pb2.InferenceWorker.FromString,\n                    response_serializer=manager__pb2.InferenceWorker.SerializeToString,\n            ),\n            'DeleteInferenceWorker': grpc.unary_unary_rpc_method_handler(\n                    servicer.DeleteInferenceWorker,\n                    request_deserializer=manager__pb2.InferenceWorker.FromString,\n                    response_serializer=manager__pb2.InferenceWorker.SerializeToString,\n            ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n            'haven.Haven', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))", "\n\n # This class is part of an EXPERIMENTAL API.\nclass Haven(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    @staticmethod\n    def Setup(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/Setup',\n            manager__pb2.SetupRequest.SerializeToString,\n            manager__pb2.SetupResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def ChatCompletion(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_stream(request, target, '/haven.Haven/ChatCompletion',\n            manager__pb2.ChatCompletionRequest.SerializeToString,\n            manager__pb2.CompletionResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def Completion(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_stream(request, target, '/haven.Haven/Completion',\n            manager__pb2.CompletionRequest.SerializeToString,\n            manager__pb2.CompletionResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def ListModels(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/ListModels',\n            manager__pb2.Empty.SerializeToString,\n            manager__pb2.ListModelsResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def AddModel(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/AddModel',\n            manager__pb2.Model.SerializeToString,\n            manager__pb2.Empty.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def DeleteModel(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/DeleteModel',\n            manager__pb2.ModelName.SerializeToString,\n            manager__pb2.Empty.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def ListWorkers(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/ListWorkers',\n            manager__pb2.Empty.SerializeToString,\n            manager__pb2.ListWorkersResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def CreateInferenceWorker(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/CreateInferenceWorker',\n            manager__pb2.CreateInferenceWorkerRequest.SerializeToString,\n            manager__pb2.InferenceWorker.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def PauseInferenceWorker(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/PauseInferenceWorker',\n            manager__pb2.InferenceWorker.SerializeToString,\n            manager__pb2.InferenceWorker.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def ResumeInferenceWorker(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/ResumeInferenceWorker',\n            manager__pb2.InferenceWorker.SerializeToString,\n            manager__pb2.InferenceWorker.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def DeleteInferenceWorker(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/haven.Haven/DeleteInferenceWorker',\n            manager__pb2.InferenceWorker.SerializeToString,\n            manager__pb2.InferenceWorker.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)", ""]}
{"filename": "worker/app/__main__.py", "chunked_list": ["import os\nimport json\n\nimport asyncio\nimport grpc\n\nfrom transformers import TextIteratorStreamer\nfrom app.pb import worker_pb2, worker_pb2_grpc\nfrom app.worker.inference_worker import InferenceClient\nfrom app.worker.models.inference_utils.parameter_passing import get_inference_parameter_dict", "from app.worker.inference_worker import InferenceClient\nfrom app.worker.models.inference_utils.parameter_passing import get_inference_parameter_dict\n\nABSOLUTE_PATH = os.path.dirname(__file__)\npath_to_config =os.path.join(ABSOLUTE_PATH, \"../config.json\")\nconfig = json.load(open(path_to_config, \"r\"))\n\ninference_client = InferenceClient(config=config)\nrunning = True\n\nclass WorkerService(worker_pb2_grpc.WorkerServiceServicer):\n\n    def Health(self, request, context):\n        global running\n        status = worker_pb2.WorkerStatus.OK if running else worker_pb2.WorkerStatus.STOPPING\n        return worker_pb2.HealthResponse(status=status)\n\n    def Shutdown(self, request, context):\n        global running\n        running = False\n        return worker_pb2.ShutdownResponse()\n\n    async def ChatCompletion(self, request: worker_pb2.ChatCompletionRequest, context):\n        \"\"\"\n            Haven supports chat-models and non-chat models. We can distinguish between the two \n            by checking if the instructionPrefix is part of the config that is passed to the worker.\n        \"\"\"\n        if \"instructionPrefix\" not in config:\n            await context.abort(grpc.StatusCode.FAILED_PRECONDITION, \"This worker only supports non-chat completion requests. Refer to the documentation if you are unsure what this means.\")\n            return\n\n        # Now we can handle the request\n        messages = list(request.messages)\n\n        inference_params = get_inference_parameter_dict(dict(max_tokens=request.max_tokens, top_p=request.top_p, top_k=request.top_k, temperature=request.temperature))\n        streamer = inference_client.complete_chat(messages=messages, inference_params=inference_params)\n\n        if isinstance(streamer, TextIteratorStreamer):\n            for text in streamer:\n                if inference_client.model_engine.model_config[\"instructionPrefix\"] in text:\n                    break\n\n                yield worker_pb2.ChatCompletionResponse(text=text)\n        else:\n            potential_stop_string = \"\"\n            async for text in streamer:\n                if potential_stop_string+text in inference_client.model_engine.model_config[\"instructionPrefix\"]:\n                            potential_stop_string += text\n                            continue\n                \n                yield worker_pb2.CompletionResponse(text=potential_stop_string+text)\n                potential_stop_string = \"\"\n\n    async def Completion(self, request: worker_pb2.CompletionRequest, context):\n        prompt = request.prompt\n        stop_tokens = list(request.stop_tokens)\n        \n        inference_params = get_inference_parameter_dict(dict(max_tokens=request.max_tokens, top_p=request.top_p, top_k=request.top_k, temperature=request.temperature))\n        streamer = inference_client.complete(prompt=prompt, stop_tokens=stop_tokens, inference_params=inference_params)\n\n        if isinstance(streamer, TextIteratorStreamer):\n            for text in streamer:\n                yield worker_pb2.CompletionResponse(text=text)\n\n        else:\n            async for text in streamer:                \n                yield worker_pb2.CompletionResponse(text=text)", "running = True\n\nclass WorkerService(worker_pb2_grpc.WorkerServiceServicer):\n\n    def Health(self, request, context):\n        global running\n        status = worker_pb2.WorkerStatus.OK if running else worker_pb2.WorkerStatus.STOPPING\n        return worker_pb2.HealthResponse(status=status)\n\n    def Shutdown(self, request, context):\n        global running\n        running = False\n        return worker_pb2.ShutdownResponse()\n\n    async def ChatCompletion(self, request: worker_pb2.ChatCompletionRequest, context):\n        \"\"\"\n            Haven supports chat-models and non-chat models. We can distinguish between the two \n            by checking if the instructionPrefix is part of the config that is passed to the worker.\n        \"\"\"\n        if \"instructionPrefix\" not in config:\n            await context.abort(grpc.StatusCode.FAILED_PRECONDITION, \"This worker only supports non-chat completion requests. Refer to the documentation if you are unsure what this means.\")\n            return\n\n        # Now we can handle the request\n        messages = list(request.messages)\n\n        inference_params = get_inference_parameter_dict(dict(max_tokens=request.max_tokens, top_p=request.top_p, top_k=request.top_k, temperature=request.temperature))\n        streamer = inference_client.complete_chat(messages=messages, inference_params=inference_params)\n\n        if isinstance(streamer, TextIteratorStreamer):\n            for text in streamer:\n                if inference_client.model_engine.model_config[\"instructionPrefix\"] in text:\n                    break\n\n                yield worker_pb2.ChatCompletionResponse(text=text)\n        else:\n            potential_stop_string = \"\"\n            async for text in streamer:\n                if potential_stop_string+text in inference_client.model_engine.model_config[\"instructionPrefix\"]:\n                            potential_stop_string += text\n                            continue\n                \n                yield worker_pb2.CompletionResponse(text=potential_stop_string+text)\n                potential_stop_string = \"\"\n\n    async def Completion(self, request: worker_pb2.CompletionRequest, context):\n        prompt = request.prompt\n        stop_tokens = list(request.stop_tokens)\n        \n        inference_params = get_inference_parameter_dict(dict(max_tokens=request.max_tokens, top_p=request.top_p, top_k=request.top_k, temperature=request.temperature))\n        streamer = inference_client.complete(prompt=prompt, stop_tokens=stop_tokens, inference_params=inference_params)\n\n        if isinstance(streamer, TextIteratorStreamer):\n            for text in streamer:\n                yield worker_pb2.CompletionResponse(text=text)\n\n        else:\n            async for text in streamer:                \n                yield worker_pb2.CompletionResponse(text=text)", "\nasync def serve():\n    server = grpc.aio.server()\n    worker_pb2_grpc.add_WorkerServiceServicer_to_server(\n        WorkerService(), server)\n    listen_address = '[::]:50051'  # Set your desired listening address\n    server.add_insecure_port(listen_address)\n    await server.start()\n    await server.wait_for_termination()\n\nif __name__ == '__main__':\n    print(\"starting server...\")\n    asyncio.run(serve())", "    await server.wait_for_termination()\n\nif __name__ == '__main__':\n    print(\"starting server...\")\n    asyncio.run(serve())"]}
{"filename": "worker/app/__init__.py", "chunked_list": [""]}
{"filename": "worker/app/pb/worker_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\nimport grpc\n\nfrom . import worker_pb2 as worker__pb2\n\n\nclass WorkerServiceStub(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    def __init__(self, channel):\n        \"\"\"Constructor.\n\n        Args:\n            channel: A grpc.Channel.\n        \"\"\"\n        self.Health = channel.unary_unary(\n                '/worker.WorkerService/Health',\n                request_serializer=worker__pb2.HealthRequest.SerializeToString,\n                response_deserializer=worker__pb2.HealthResponse.FromString,\n                )\n        self.Shutdown = channel.unary_unary(\n                '/worker.WorkerService/Shutdown',\n                request_serializer=worker__pb2.ShutdownRequest.SerializeToString,\n                response_deserializer=worker__pb2.ShutdownResponse.FromString,\n                )\n        self.ChatCompletion = channel.unary_stream(\n                '/worker.WorkerService/ChatCompletion',\n                request_serializer=worker__pb2.ChatCompletionRequest.SerializeToString,\n                response_deserializer=worker__pb2.CompletionResponse.FromString,\n                )\n        self.Completion = channel.unary_stream(\n                '/worker.WorkerService/Completion',\n                request_serializer=worker__pb2.CompletionRequest.SerializeToString,\n                response_deserializer=worker__pb2.CompletionResponse.FromString,\n                )", "\n\nclass WorkerServiceServicer(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    def Health(self, request, context):\n        \"\"\"Check health of the worker.\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def Shutdown(self, request, context):\n        \"\"\"Shutdown the worker.\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def ChatCompletion(self, request, context):\n        \"\"\"Generate text from chat history.\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')\n\n    def Completion(self, request, context):\n        \"\"\"Generate text from a prompt.\n        \"\"\"\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details('Method not implemented!')\n        raise NotImplementedError('Method not implemented!')", "\n\ndef add_WorkerServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n            'Health': grpc.unary_unary_rpc_method_handler(\n                    servicer.Health,\n                    request_deserializer=worker__pb2.HealthRequest.FromString,\n                    response_serializer=worker__pb2.HealthResponse.SerializeToString,\n            ),\n            'Shutdown': grpc.unary_unary_rpc_method_handler(\n                    servicer.Shutdown,\n                    request_deserializer=worker__pb2.ShutdownRequest.FromString,\n                    response_serializer=worker__pb2.ShutdownResponse.SerializeToString,\n            ),\n            'ChatCompletion': grpc.unary_stream_rpc_method_handler(\n                    servicer.ChatCompletion,\n                    request_deserializer=worker__pb2.ChatCompletionRequest.FromString,\n                    response_serializer=worker__pb2.CompletionResponse.SerializeToString,\n            ),\n            'Completion': grpc.unary_stream_rpc_method_handler(\n                    servicer.Completion,\n                    request_deserializer=worker__pb2.CompletionRequest.FromString,\n                    response_serializer=worker__pb2.CompletionResponse.SerializeToString,\n            ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n            'worker.WorkerService', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))", "\n\n # This class is part of an EXPERIMENTAL API.\nclass WorkerService(object):\n    \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\n    @staticmethod\n    def Health(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/worker.WorkerService/Health',\n            worker__pb2.HealthRequest.SerializeToString,\n            worker__pb2.HealthResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def Shutdown(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/worker.WorkerService/Shutdown',\n            worker__pb2.ShutdownRequest.SerializeToString,\n            worker__pb2.ShutdownResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def ChatCompletion(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_stream(request, target, '/worker.WorkerService/ChatCompletion',\n            worker__pb2.ChatCompletionRequest.SerializeToString,\n            worker__pb2.CompletionResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)\n\n    @staticmethod\n    def Completion(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_stream(request, target, '/worker.WorkerService/Completion',\n            worker__pb2.CompletionRequest.SerializeToString,\n            worker__pb2.CompletionResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)", ""]}
{"filename": "worker/app/pb/__init__.py", "chunked_list": [""]}
{"filename": "worker/app/pb/worker_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: worker.proto\n\"\"\"Generated protocol buffer code.\"\"\"\nfrom google.protobuf.internal import builder as _builder\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import descriptor_pool as _descriptor_pool\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n", "# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\\n\\x0cworker.proto\\x12\\x06worker\\\"\\x0f\\n\\rHealthRequest\\\"6\\n\\x0eHealthResponse\\x12$\\n\\x06status\\x18\\x01 \\x01(\\x0e\\x32\\x14.worker.WorkerStatus\\\"\\x11\\n\\x0fShutdownRequest\\\"\\x12\\n\\x10ShutdownResponse\\\"6\\n\\x07Message\\x12\\x1a\\n\\x04role\\x18\\x01 \\x01(\\x0e\\x32\\x0c.worker.Role\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x02 \\x01(\\t\\\"\\xc8\\x01\\n\\x15\\x43hatCompletionRequest\\x12!\\n\\x08messages\\x18\\x01 \\x03(\\x0b\\x32\\x0f.worker.Message\\x12\\x17\\n\\nmax_tokens\\x18\\x02 \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12\\x12\\n\\x05top_p\\x18\\x03 \\x01(\\x02H\\x01\\x88\\x01\\x01\\x12\\x12\\n\\x05top_k\\x18\\x04 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x18\\n\\x0btemperature\\x18\\x05 \\x01(\\x02H\\x03\\x88\\x01\\x01\\x42\\r\\n\\x0b_max_tokensB\\x08\\n\\x06_top_pB\\x08\\n\\x06_top_kB\\x0e\\n\\x0c_temperature\\\"\\xc6\\x01\\n\\x11\\x43ompletionRequest\\x12\\x0e\\n\\x06prompt\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0bstop_tokens\\x18\\x06 \\x03(\\t\\x12\\x17\\n\\nmax_tokens\\x18\\x02 \\x01(\\x05H\\x00\\x88\\x01\\x01\\x12\\x12\\n\\x05top_p\\x18\\x03 \\x01(\\x02H\\x01\\x88\\x01\\x01\\x12\\x12\\n\\x05top_k\\x18\\x04 \\x01(\\x05H\\x02\\x88\\x01\\x01\\x12\\x18\\n\\x0btemperature\\x18\\x05 \\x01(\\x02H\\x03\\x88\\x01\\x01\\x42\\r\\n\\x0b_max_tokensB\\x08\\n\\x06_top_pB\\x08\\n\\x06_top_kB\\x0e\\n\\x0c_temperature\\\"\\\"\\n\\x12\\x43ompletionResponse\\x12\\x0c\\n\\x04text\\x18\\x01 \\x01(\\t*1\\n\\x0cWorkerStatus\\x12\\x06\\n\\x02OK\\x10\\x00\\x12\\x0c\\n\\x08STOPPING\\x10\\x01\\x12\\x0b\\n\\x07OFFLINE\\x10\\x63*\\x1f\\n\\x04Role\\x12\\r\\n\\tASSISTANT\\x10\\x00\\x12\\x08\\n\\x04USER\\x10\\x01\\x32\\xa5\\x02\\n\\rWorkerService\\x12\\x39\\n\\x06Health\\x12\\x15.worker.HealthRequest\\x1a\\x16.worker.HealthResponse\\\"\\x00\\x12?\\n\\x08Shutdown\\x12\\x17.worker.ShutdownRequest\\x1a\\x18.worker.ShutdownResponse\\\"\\x00\\x12O\\n\\x0e\\x43hatCompletion\\x12\\x1d.worker.ChatCompletionRequest\\x1a\\x1a.worker.CompletionResponse\\\"\\x00\\x30\\x01\\x12G\\n\\nCompletion\\x12\\x19.worker.CompletionRequest\\x1a\\x1a.worker.CompletionResponse\\\"\\x00\\x30\\x01\\x62\\x06proto3')\n\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())", "\n_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'worker_pb2', globals())\nif _descriptor._USE_C_DESCRIPTORS == False:\n\n  DESCRIPTOR._options = None\n  _WORKERSTATUS._serialized_start=632\n  _WORKERSTATUS._serialized_end=681\n  _ROLE._serialized_start=683\n  _ROLE._serialized_end=714\n  _HEALTHREQUEST._serialized_start=24\n  _HEALTHREQUEST._serialized_end=39\n  _HEALTHRESPONSE._serialized_start=41\n  _HEALTHRESPONSE._serialized_end=95\n  _SHUTDOWNREQUEST._serialized_start=97\n  _SHUTDOWNREQUEST._serialized_end=114\n  _SHUTDOWNRESPONSE._serialized_start=116\n  _SHUTDOWNRESPONSE._serialized_end=134\n  _MESSAGE._serialized_start=136\n  _MESSAGE._serialized_end=190\n  _CHATCOMPLETIONREQUEST._serialized_start=193\n  _CHATCOMPLETIONREQUEST._serialized_end=393\n  _COMPLETIONREQUEST._serialized_start=396\n  _COMPLETIONREQUEST._serialized_end=594\n  _COMPLETIONRESPONSE._serialized_start=596\n  _COMPLETIONRESPONSE._serialized_end=630\n  _WORKERSERVICE._serialized_start=717\n  _WORKERSERVICE._serialized_end=1010", "# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "worker/app/worker/training_worker.py", "chunked_list": ["import json\n\nfrom models.model_registry import ModelRegistry\n\n\n\nclass TrainingClient:\n    def __init__(self, config):\n\n        with open(config, 'r') as f:\n            self.config = json.load(f)\n\n        self.model_engine = ModelRegistry.REGISTRY[self.config[\"base_class\"]](self.config)\n        self.model_engine.prepare_model_for_training()\n        self.model_engine.prepare_data_for_training()\n\n\n    def train(self):\n        self.model_engine.train()", "\n"]}
{"filename": "worker/app/worker/__init__.py", "chunked_list": [""]}
{"filename": "worker/app/worker/inference_worker.py", "chunked_list": ["from .models.model_registry import ModelRegistry\n\nclass InferenceClient:\n    def __init__(self, config):\n        self.config = config\n        self.model_engine = ModelRegistry.REGISTRY[self.config[\"architecture\"]](self.config)\n        self.model_engine.prepare_for_inference()\n\n    def complete_chat(self, messages, inference_params):\n        prompt = self.model_engine.create_prompt_from_messages(messages)\n        return self.model_engine.generate_stream(prompt, **inference_params)\n    \n    def complete(self, prompt, stop_tokens, inference_params):\n        return self.model_engine.generate_stream(prompt, stop_tokens=stop_tokens, **inference_params)", "    "]}
{"filename": "worker/app/worker/models/model_registry.py", "chunked_list": ["class ModelRegistry(type):\n\n    REGISTRY = {}\n\n    def __new__(cls, name, bases, attrs):\n        # instantiate a new type corresponding to the type of class being defined\n        # this is currently RegisterBase but in child classes will be the child class\n\n        new_cls = type.__new__(cls, name, bases, attrs)\n        cls.REGISTRY[new_cls.architecture_name] = new_cls\n        return new_cls\n\n    @classmethod\n    def get_registry(cls):\n        return dict(cls.REGISTRY)", "    \n\nclass RegisteredModel(metaclass=ModelRegistry):\n\n    architecture_name = \"model\"\n\n\n# REGISTER ALL MODELS\n\nfrom .base_causal import AutoCausalModel", "\nfrom .base_causal import AutoCausalModel\nfrom .bigcode_15b import BigCode15B\nfrom .falcon_7b import Falcon7BModel\nfrom .gpt_neox_3b import GPTNeoX3B\nfrom .gpt_neox_7b import GPTNeoX7B\nfrom .gpt_neox_12b import GPTNeoX12B\nfrom .llama_7b import Llama7B\nfrom .llama_13b import Llama13B\nfrom .mpt_7b import Mpt7B", "from .llama_13b import Llama13B\nfrom .mpt_7b import Mpt7B\nfrom .mpt_30b import Mpt30B\nfrom .vllm_causal import VllmCausalModel\n"]}
{"filename": "worker/app/worker/models/bigcode_15b.py", "chunked_list": ["from typing import List\nfrom .vllm_causal import VllmCausalModel\n\n\nclass BigCode15B(VllmCausalModel):\n\n    architecture_name = \"bigcode_15b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        super().prepare_for_inference()\n\n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 8000, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=stop_tokens, max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text", "\n"]}
{"filename": "worker/app/worker/models/gpt_neox_12b.py", "chunked_list": ["from typing import List\nfrom .vllm_causal import VllmCausalModel\n\n\nclass GPTNeoX12B(VllmCausalModel):\n\n    architecture_name = \"gpt_neox_12b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        super().prepare_for_inference()\n\n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text", ""]}
{"filename": "worker/app/worker/models/mpt_30b.py", "chunked_list": ["from typing import List\nfrom .vllm_causal import VllmCausalModel\n\n\nclass Mpt30B(VllmCausalModel):\n\n    architecture_name = \"mpt_30b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        super().prepare_for_inference()\n\n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text", ""]}
{"filename": "worker/app/worker/models/llama_7b.py", "chunked_list": ["import os\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom typing import List\nfrom .vllm_causal import VllmCausalModel\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\n\n\nclass Llama7B(VllmCausalModel):\n\n    architecture_name = \"llama_7b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n\n        if self.model_config[\"quantization\"] == \"int8\":\n            raise NotImplementedError(\"VLLM Models do not yet support 8bit-quantization.\")\n\n\n        elif self.model_config[\"quantization\"] == \"float16\":\n            \"\"\"\n                The GPU types are\n                0: A100\n                1: A100 80GB\n                2: T4\n\n                TODO: We're not covering A100 80GB yet\n            \"\"\"\n            if self.model_config[\"gpuType\"] == 2:\n                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"], dtype=\"float16\")\n\n            elif self.model_config[\"gpuType\"] == 0:\n                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"])\n            \n            self.model_vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n        else:\n            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text", "\nclass Llama7B(VllmCausalModel):\n\n    architecture_name = \"llama_7b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n\n        if self.model_config[\"quantization\"] == \"int8\":\n            raise NotImplementedError(\"VLLM Models do not yet support 8bit-quantization.\")\n\n\n        elif self.model_config[\"quantization\"] == \"float16\":\n            \"\"\"\n                The GPU types are\n                0: A100\n                1: A100 80GB\n                2: T4\n\n                TODO: We're not covering A100 80GB yet\n            \"\"\"\n            if self.model_config[\"gpuType\"] == 2:\n                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"], dtype=\"float16\")\n\n            elif self.model_config[\"gpuType\"] == 0:\n                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"])\n            \n            self.model_vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n        else:\n            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text"]}
{"filename": "worker/app/worker/models/vllm_causal.py", "chunked_list": ["import transformers\nfrom transformers import TextIteratorStreamer, StoppingCriteriaList, Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\nfrom threading import Thread\nfrom typing import List\nfrom peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\nimport re\nimport os\n\nfrom app.pb import worker_pb2, worker_pb2_grpc\n", "from app.pb import worker_pb2, worker_pb2_grpc\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.utils import random_uuid\n\nfrom .model_registry import RegisteredModel\nfrom .inference_utils.stopping_criteria import StopOnTokens\nfrom .training_utils.tokenizer_resize import resize_tokenizer_and_embeddings", "from .inference_utils.stopping_criteria import StopOnTokens\nfrom .training_utils.tokenizer_resize import resize_tokenizer_and_embeddings\nfrom .training_utils.data_processing import make_supervised_data_module\n\n\nclass VllmCausalModel(RegisteredModel):\n\n    architecture_name = \"causal_vllm_model\"\n\n    def __init__(self, config):\n        self.model_config = config\n\n        if config[\"name\"].startswith(\"@huggingface\"):\n            self.model_config[\"huggingface_name\"] = \"/\".join(config[\"name\"].split(\"/\")[1:])\n\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n\n        if self.model_config[\"quantization\"] == \"int8\":\n            raise NotImplementedError(\"VLLM Models do not yet support 8bit-quantization.\")\n\n\n        elif self.model_config[\"quantization\"] == \"float16\":\n            \"\"\"\n                The GPU types are\n                0: A100\n                1: A100 80GB\n                2: T4\n\n                TODO: We're not covering A100 80GB yet\n            \"\"\"\n\n            if self.model_config[\"gpuType\"] == 2:\n                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tensor_parallel_size=self.model_config[\"gpuCount\"], dtype=\"float16\", swap_space=1)\n\n            elif self.model_config[\"gpuType\"] == 0:\n                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tensor_parallel_size=self.model_config[\"gpuCount\"])\n            \n            self.model_vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n        else:\n            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        sampling_params = SamplingParams(\n                max_tokens=max_tokens if max_tokens < self.model_config[\"contextSize\"] else self.model_config[\"contextSize\"],\n                top_p=1 if temperature==0 else top_p, \n                temperature=temperature, \n                stop=self.get_stopword_list() + stop_tokens\n            )\n        \n        id = random_uuid()\n        results_generator = self.model_vllm_engine.generate(prompt, sampling_params, id)\n        \n        prev_text = \"\"\n        async for request_output in results_generator:\n            text = request_output.outputs[0].text\n            text = text.replace(prev_text, \"\")\n            if not text in prev_text:\n                yield text\n                prev_text = request_output.outputs[0].text\n            else:\n                yield \"\"\n\n\n    def create_prompt_from_messages(self, messages):\n        if messages[-1].role == worker_pb2.ASSISTANT:\n            raise Exception(\"Last message should be from user, not assistant\")\n                \n        prompt = self.model_config[\"systemPrompt\"]\n        for message_obj in messages:\n            if message_obj.role == worker_pb2.USER:\n                prompt += self.model_config[\"instructionPrefix\"] + message_obj.content + self.model_config[\"instructionPostfix\"]\n\n            elif message_obj.role == worker_pb2.ASSISTANT:\n                prompt += self.model_config[\"outputPrefix\"] + message_obj.content + self.model_config[\"outputPostfix\"]\n\n        prompt += self.model_config[\"outputPrefix\"]\n\n        return prompt\n    \n\n    def get_stopword_list(self):\n        if not \"outputPostfix\" in self.model_config:\n            return [\"<|endoftext|>\"]\n        if all(char.isspace() for char in self.model_config[\"outputPostfix\"]):\n            return [self.model_config[\"instructionPrefix\"].strip(), \"<|endoftext|>\"]\n        else:\n            return [self.model_config[\"outputPostfix\"].strip(), \"<|endoftext|>\"]\n            \n\n    ##############################\n    ### FINETUNING   #############\n    ##############################\n    def prepare_model_for_training(self):\n\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=self.model_config[\"lora\"])\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n\n        if self.tokenizer.pad_token is None:\n            resize_tokenizer_and_embeddings(\n                tokenizer=self.tokenizer,\n                model=self.model,\n            )\n\n        if self.model_config[\"lora\"]:\n            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=self.model_config[\"lora_params\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n            self.model = prepare_model_for_int8_training(self.model)\n            self.model = get_peft_model(self.model, lora_config)\n\n\n\n\n    def prepare_data_for_training(self):\n        self.train_dataset, self.collator = make_supervised_data_module(tokenizer=self.tokenizer, data_path=self.model_config[\"train_data_path\"], instruction_prefix=self.model_config[\"instructionPrefix\"], output_prefix=self.model_config[\"outputPrefix\"])\n        self.eval_dataset, _ = make_supervised_data_module(tokenizer=self.tokenizer, data_path=self.model_config[\"eval_data_path\"], instruction_prefix=self.model_config[\"instructionPrefix\"], output_prefix=self.model_config[\"outputPrefix\"])\n\n\n\n\n    def train(self):\n\n        training_args = TrainingArguments(learning_rate=self.model_config[\"learning_rate\"], per_device_train_batch_size=self.model_config[\"batch_size\"], per_device_eval_batch_size=self.model_config[\"batch_size\"], output_dir=\"out\")\n\n        trainer = Trainer(\n            model=self.model, \n            tokenizer=self.tokenizer,\n            train_dataset=self.train_dataset, \n            eval_dataset=self.eval_dataset,\n            data_collator=self.collator,\n            args=training_args\n        )\n\n        trainer.train()\n\n        self.model.save_pretrained(self.model_config[\"trained_model_path\"])\n        self.tokenizer.save_pretrained(self.model_config[\"trained_model_path\"])"]}
{"filename": "worker/app/worker/models/__init__.py", "chunked_list": [""]}
{"filename": "worker/app/worker/models/base_causal.py", "chunked_list": ["import transformers\nfrom transformers import TextIteratorStreamer, StoppingCriteriaList, Trainer, TrainingArguments\nfrom threading import Thread\nfrom typing import List\nfrom peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\n\nfrom app.pb import worker_pb2, worker_pb2_grpc\n\nfrom .model_registry import RegisteredModel\nfrom .inference_utils.stopping_criteria import StopOnTokens", "from .model_registry import RegisteredModel\nfrom .inference_utils.stopping_criteria import StopOnTokens\nfrom .training_utils.tokenizer_resize import resize_tokenizer_and_embeddings\nfrom .training_utils.data_processing import make_supervised_data_module\n\n\nclass AutoCausalModel(RegisteredModel):\n\n    architecture_name = \"causal_model\"\n        \n    def __init__(self, config):\n        self.model_config = config\n\n        if config[\"name\"].startswith(\"@huggingface\"):\n            self.model_config[\"huggingface_name\"] = \"/\".join(config[\"name\"].split(\"/\")[1:])\n\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        if self.model_config[\"quantization\"] == \"int8\":\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=True)\n        \n        elif self.model_config[\"quantization\"] == \"float16\":\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\")\n\n        else:\n            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\n\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n        self.stopping_criteria = StoppingCriteriaList([StopOnTokens(self.tokenizer, [self.model_config[\"instructionPrefix\"]]+[self.tokenizer.eos_token])])\n\n\n    def generate_stream(self, prompt: str, stop_tokens=[], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        input_tokenized = self.tokenizer([prompt], return_tensors='pt').input_ids.to('cuda')\n\n        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n        \n        if temperature == 0:\n            sample = False\n            temperature = 1\n        else:\n            sample = True\n\n\n        generation_kwargs=dict(\n            inputs=input_tokenized,\n            streamer=streamer,\n            do_sample=sample,\n            top_p=top_p, \n            top_k=top_k, \n            temperature=temperature, \n            stopping_criteria=self.stopping_criteria,\n            max_length=max_tokens if max_tokens < self.model_config[\"contextSize\"] else self.model_config[\"contextSize\"],\n        )\n\n        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n        thread.start()\n\n        return streamer\n    \n\n    def create_prompt_from_messages(self, messages):\n        if messages[-1].role == worker_pb2.ASSISTANT:\n            raise Exception(\"Last message should be from user, not assistant\")\n                \n        prompt = self.model_config[\"systemPrompt\"]\n        for message_obj in messages:\n            if message_obj.role == worker_pb2.USER:\n                prompt += self.model_config[\"instructionPrefix\"] + message_obj.content + self.model_config[\"instructionPostfix\"]\n\n            elif message_obj.role == worker_pb2.ASSISTANT:\n                prompt += self.model_config[\"outputPrefix\"] + message_obj.content + self.model_config[\"outputPostfix\"]\n\n        prompt += self.model_config[\"outputPrefix\"]\n\n        return prompt\n\n      \n      \n\n    ##############################\n    ### FINETUNING   #############\n    ##############################\n    def prepare_model_for_training(self):\n        \n        self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=self.model_config[\"lora\"])\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n\n        if self.tokenizer.pad_token is None:\n            resize_tokenizer_and_embeddings(\n                tokenizer=self.tokenizer,\n                model=self.model,\n            )\n\n        if self.model_config[\"lora\"]:\n            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=self.model_config[\"lora_params\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n            self.model = prepare_model_for_int8_training(self.model)\n            self.model = get_peft_model(self.model, lora_config)\n\n\n\n\n    def prepare_data_for_training(self):\n        self.train_dataset, self.collator = make_supervised_data_module(tokenizer=self.tokenizer, data_path=self.model_config[\"train_data_path\"], instruction_prefix=self.model_config[\"instructionPrefix\"], output_prefix=self.model_config[\"outputPrefix\"])\n        self.eval_dataset, _ = make_supervised_data_module(tokenizer=self.tokenizer, data_path=self.model_config[\"eval_data_path\"], instruction_prefix=self.model_config[\"instructionPrefix\"], output_prefix=self.model_config[\"outputPrefix\"])\n\n\n\n\n    def train(self):\n\n\n        training_args = TrainingArguments(learning_rate=self.model_config[\"learning_rate\"], per_device_train_batch_size=self.model_config[\"batch_size\"], per_device_eval_batch_size=self.model_config[\"batch_size\"], output_dir=\"out\")\n\n        trainer = Trainer(\n            model=self.model, \n            tokenizer=self.tokenizer,\n            train_dataset=self.train_dataset, \n            eval_dataset=self.eval_dataset,\n            data_collator=self.collator,\n            args=training_args\n        )\n        \n        trainer.train()\n\n        self.model.save_pretrained(self.model_config[\"trained_model_path\"])\n        self.tokenizer.save_pretrained(self.model_config[\"trained_model_path\"])", "\n\n        \n\n    \n"]}
{"filename": "worker/app/worker/models/falcon_7b.py", "chunked_list": ["import transformers\nfrom transformers import TextIteratorStreamer, StoppingCriteriaList\nfrom threading import Thread\nimport torch\nfrom peft import LoraConfig, prepare_model_for_int8_training, get_peft_model\nfrom typing import List\n\nfrom .base_causal import AutoCausalModel\nfrom .training_utils.tokenizer_resize import resize_tokenizer_and_embeddings\nfrom .training_utils.data_processing import make_supervised_data_module", "from .training_utils.tokenizer_resize import resize_tokenizer_and_embeddings\nfrom .training_utils.data_processing import make_supervised_data_module\nfrom .inference_utils.stopping_criteria import StopOnTokens\n\nclass Falcon7BModel(AutoCausalModel):\n\n    architecture_name = \"falcon_7b\"\n        \n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        if self.model_config[\"quantization\"] == \"int8\":\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=True, trust_remote_code=True, torch_dtype=torch.bfloat16)\n\n        elif self.model_config[\"quantization\"] == \"float16\":\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n\n        else:\n            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\n\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n        self.stopping_criteria = StoppingCriteriaList([StopOnTokens(self.tokenizer, [self.model_config[\"instructionPrefix\"]]+[self.tokenizer.eos_token])])\n\n        \n    def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        return super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_model_for_training(self):\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(self.model_config[\"huggingface_name\"], device_map=\"auto\", load_in_8bit=self.model_config[\"lora\"], trust_remote_code=True, torch_dtype=torch.bfloat16)\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_config[\"huggingface_name\"])\n\n        if self.tokenizer.pad_token is None:\n            resize_tokenizer_and_embeddings(\n                tokenizer=self.tokenizer,\n                model=self.model,\n            )\n\n        if self.model_config[\"lora\"]:\n            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=self.model_config[\"lora_params\"], lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n            self.model = prepare_model_for_int8_training(self.model)\n            self.model = get_peft_model(self.model, lora_config)\n\n\n    def prepare_data_for_training(self):\n        super().prepare_data_for_training()\n\n    def train(self):\n        super().train()", ""]}
{"filename": "worker/app/worker/models/gpt_neox_7b.py", "chunked_list": ["import os\nfrom typing import List\nfrom .vllm_causal import VllmCausalModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\n\n\nclass GPTNeoX7B(VllmCausalModel):\n\n    architecture_name = \"gpt_neox_7b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        super().prepare_for_inference()\n        \n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text", "\nclass GPTNeoX7B(VllmCausalModel):\n\n    architecture_name = \"gpt_neox_7b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        super().prepare_for_inference()\n        \n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text", ""]}
{"filename": "worker/app/worker/models/llama_13b.py", "chunked_list": ["import os\nfrom typing import List\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom .vllm_causal import VllmCausalModel\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.utils import random_uuid\n", "from vllm.utils import random_uuid\n\n\nclass Llama13B(VllmCausalModel):\n\n    architecture_name = \"llama_13b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n\n        if self.model_config[\"quantization\"] == \"int8\":\n            raise NotImplementedError(\"VLLM Models do not yet support 8bit-quantization.\")\n\n\n        elif self.model_config[\"quantization\"] == \"float16\":\n            \"\"\"\n                The GPU types are\n                0: A100\n                1: A100 80GB\n                2: T4\n\n                TODO: We're not covering A100 80GB yet\n            \"\"\"\n            if self.model_config[\"gpuType\"] == 2:\n                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"], dtype=\"float16\")\n\n            elif self.model_config[\"gpuType\"] == 0:\n                engine_args = AsyncEngineArgs(model=self.model_config[\"huggingface_name\"], trust_remote_code=True, tokenizer_mode=\"slow\", tensor_parallel_size=self.model_config[\"gpuCount\"])\n            \n            self.model_vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)\n\n        else:\n            raise NotImplementedError(f\"{self.model_config['quantization']} is not a valid quantization config\")\n\n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text"]}
{"filename": "worker/app/worker/models/mpt_7b.py", "chunked_list": ["from typing import List\nfrom .vllm_causal import VllmCausalModel\n\n\nclass Mpt7B(VllmCausalModel):\n\n    architecture_name = \"mpt_7b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        super().prepare_for_inference()\n\n\n    async def generate_stream(self, prompt: str, stop_tokens = [], max_tokens: int = 28000, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, stop_tokens=[], max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text", ""]}
{"filename": "worker/app/worker/models/gpt_neox_3b.py", "chunked_list": ["from typing import List\nfrom .vllm_causal import VllmCausalModel\n\n\nclass GPTNeoX3B(VllmCausalModel):\n\n    architecture_name = \"gpt_neox_3b\"\n\n    def __init__(self, config):\n        super().__init__(config)\n\n\n    ##############################\n    ### INFERENCE    #############\n    ##############################\n    def prepare_for_inference(self):\n        super().prepare_for_inference()\n\n\n    async def generate_stream(self, prompt: str, stop_tokens= [], max_tokens: int = 2048, top_p=0.8, top_k=500, temperature=0.9):\n        stream = super().generate_stream(prompt, max_tokens=max_tokens, top_p=top_p, top_k=top_k, temperature=temperature)\n        async for text in stream:\n            yield text", ""]}
{"filename": "worker/app/worker/models/training_utils/tokenizer_resize.py", "chunked_list": ["from typing import Dict\nimport transformers\n\n\ndef resize_tokenizer_and_embeddings(\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n\n    special_tokens_dict = dict(pad_token=\"[PAD]\")\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg", ""]}
{"filename": "worker/app/worker/models/training_utils/data_processing.py", "chunked_list": ["import copy\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Dict, Sequence\nimport torch\nimport transformers\nfrom torch.utils.data import Dataset\nimport io\nimport json\n", "import json\n\n\ndef _make_r_io_base(f, mode: str):\n    if not isinstance(f, io.IOBase):\n        f = open(f, mode=mode)\n    return f\n\n\ndef jload(f, mode=\"r\"):\n    \"\"\"Load a .json file into a dictionary.\"\"\"\n    f = _make_r_io_base(f, mode)\n    jdict = json.load(f)\n    f.close()\n    return jdict", "\ndef jload(f, mode=\"r\"):\n    \"\"\"Load a .json file into a dictionary.\"\"\"\n    f = _make_r_io_base(f, mode)\n    jdict = json.load(f)\n    f.close()\n    return jdict\n\n\n", "\n\n\n# SUPERVISED FINETUNING\n\nclass SupervisedDataset(Dataset):\n    \"\"\"\n    Dataset for supervised fine-tuning.\n    \"\"\"\n\n    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, instruction_prefix, output_prefix):\n        super(SupervisedDataset, self).__init__()\n        logging.warning(\"Loading data...\")\n        list_data_dict = jload(data_path)\n\n        logging.warning(\"Formatting inputs...\")\n        sources = [example[\"prompt\"] for example in list_data_dict]\n        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n\n        logging.warning(\"Tokenizing inputs... This may take some time...\")\n        data_dict = preprocess(sources, targets, tokenizer, instruction_prefix, output_prefix)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(input_ids=self.input_ids[i], labels=self.labels[i])", "\n\n@dataclass\nclass DataCollatorForSupervisedDataset(object):\n    \"\"\"\n    Collate examples for supervised fine-tuning.\n    \"\"\"\n\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n        )\n        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n        \n        return dict(\n            input_ids=input_ids,\n            labels=labels,\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n        )", "    \n\ndef preprocess(sources: Sequence[str], targets: Sequence[str], tokenizer: transformers.PreTrainedTokenizer, instruction_prefix, output_prefix) -> Dict:\n    \"\"\"\n    Preprocess the data by tokenizing.\n    \"\"\"\n    \n    examples = [instruction_prefix + s + output_prefix + t + tokenizer.eos_token for s, t in zip(sources, targets)]\n    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n    input_ids = examples_tokenized[\"input_ids\"]\n    labels = copy.deepcopy(input_ids)\n    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n        label[:source_len] = label[:source_len]\n\n    return dict(input_ids=input_ids, labels=labels)", "\n\n\n\ndef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_path, instruction_prefix, output_prefix) -> Dict:\n    \"\"\"\n    Make dataset and collator for supervised fine-tuning.\n    \"\"\"\n    dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_path, instruction_prefix=instruction_prefix, output_prefix=output_prefix)\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n    \n    return dataset, data_collator", "\n\n\n\ndef _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n    \"\"\"\n    Tokenize a list of strings.\n    \"\"\"\n    tokenized_list = [\n        tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        )\n        for text in strings\n    ]\n    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n    input_ids_lens = labels_lens = [\n        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n    ]\n    return dict(\n        input_ids=input_ids,\n        labels=labels,\n        input_ids_lens=input_ids_lens,\n        labels_lens=labels_lens,\n    )"]}
{"filename": "worker/app/worker/models/inference_utils/parameter_passing.py", "chunked_list": ["from typing import Dict\n\ndef get_inference_parameter_dict(params: Dict):\n    inference_params = dict()\n\n    for p in params.keys():\n        if params[p] != -1:\n            inference_params[p] = params[p]\n\n    return inference_params", ""]}
{"filename": "worker/app/worker/models/inference_utils/stopping_criteria.py", "chunked_list": ["from transformers import StoppingCriteria\nimport torch\n\nclass StopOnTokens(StoppingCriteria):\n    def __init__(self, tokenizer, stop_token_list):\n        super(StopOnTokens, self).__init__()\n        self.stopping_ids = tokenizer.convert_tokens_to_ids(stop_token_list)\n\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        for stop_id in self.stopping_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False", ""]}
{"filename": "worker/app/worker/models/inference_utils/__init__.py", "chunked_list": [""]}
{"filename": "llamatune/setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nsetup(\n    name='llamatune',\n    version='0.1.1',\n    author='Haven Technologies Inc.',\n    author_email='hello@havenllm.com',\n    description=\"Haven\\'s Tuning Library for LLM finetuning\",\n    packages=find_packages(),\n    install_requires=[", "    packages=find_packages(),\n    install_requires=[\n        'torch==2.0.1',\n        'bitsandbytes==0.40.0',\n        'einops==0.6.1',\n        'evaluate==0.4.0',\n        'scikit-learn==1.2.2',\n        'sentencepiece==0.1.99',\n        'wandb==0.15.3',\n        'accelerate==0.21.0',", "        'wandb==0.15.3',\n        'accelerate==0.21.0',\n        'posthog==2.5.0'\n    ],\n)\n\n"]}
{"filename": "llamatune/llamatune/train.py", "chunked_list": ["from llamatune import ChatTrainer\nfrom transformers import HfArgumentParser\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport torch\n\n\n@dataclass\nclass TrainingConfig:\n    model_name: str = field(default=\"meta-llama/Llama-2-7b-hf\", metadata={\"help\": 'Huggingface Name of the model you want to train'})\n    data_path: str = field(default=\"data.json\", metadata={\"help\": 'Path towards your training data'})\n    output_dir: str = field(default='./trained_model', metadata={\"help\": 'The output dir for logs and checkpoints'})\n    training_recipe: str = field(default=\"lora\", metadata={\"help\": \"Lora Training or Full Training\"})\n    optim: str = field(default='paged_adamw_8bit', metadata={\"help\": 'The optimizer to be used'})\n    batch_size: int = field(default=1, metadata={\"help\": 'The training batch size per GPU. Increase for better speed.'})\n    gradient_accumulation_steps: int = field(default=16, metadata={\"help\": 'How many gradients to accumulate before to perform an optimizer step'})\n    n_epochs: int = field(default=3, metadata={\"help\": 'How many optimizer update steps to take'})\n    weight_decay: float = field(default=0.0, metadata={\"help\": 'The L2 weight decay rate of AdamW'}) \n    learning_rate: float = field(default=1e-4, metadata={\"help\": 'The learning rate'})\n    max_grad_norm: float = field(default=0.3, metadata={\"help\": 'Gradient clipping max norm. This is tuned and works well for all models tested.'})\n    gradient_checkpointing: bool = field(default=True, metadata={\"help\": 'Use gradient checkpointing. You want to use this.'})\n    do_train: bool = field(default=True, metadata={\"help\": 'To train or not to train, that is the question?'})\n    lr_scheduler_type: str = field(default='cosine', metadata={\"help\": 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'})\n    warmup_ratio: float = field(default=0.03, metadata={\"help\": 'Fraction of steps to do a warmup for'})\n    logging_steps: int = field(default=1, metadata={\"help\": 'The frequency of update steps after which to log the loss'})\n    group_by_length: bool = field(default=True, metadata={\"help\": 'Group sequences into batches with same length. Saves memory and speeds up training considerably.'})\n    save_strategy: str = field(default='epoch', metadata={\"help\": 'When to save checkpoints'})\n    save_total_limit: int = field(default=3, metadata={\"help\": 'How many checkpoints to save before the oldest is overwritten'})\n    fp16: bool = field(default=True, metadata={\"help\": 'Whether to use fp16 mixed precision training'})\n    tokenizer_type: str = field(default=\"llama\", metadata={\"help\": \"Tokenizer type. Should be \\\"llama\\\" for llama models to address tokenizer issue\"})\n    trust_remote_code: str = field(default=False, metadata={\"help\": \"Whether to trust remote code.\"})\n    compute_dtype: torch.dtype = field(default=torch.float16, metadata={\"help\":\"Compute Datatype for models, either float16 or float32.\"})\n    max_tokens: int = field(default=4096, metadata={\"help\":\"Max tokens\"})\n    do_eval: bool = field(default=True, metadata={\"help\": \"Whether to evaluate or not\"})\n    evaluation_strategy: str = field(default=\"epoch\", metadata={\"help\": \"When to evaluate, after certain number of steps or each epoch\"})\n    use_auth_token: str = field(default=None, metadata={\"help\": \"auth token\"})\n    use_fast: bool = field(default=False, metadata={\"help\": \"Whether to use fast tokenizer\"})\n    bits: Optional[int] = field(default=4, metadata={\"help\": \"Number of bits to quantize the model to\"})\n    double_quant: bool = field(default=True, metadata={\"help\": \"Compress the quantization statistics through double quantization.\"})\n    quant_type: str = field(default=\"nf4\", metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"})\n    lora_r: int = field(default=64, metadata={\"help\": \"Lora R dimension.\"})\n    lora_alpha: float = field(default=16, metadata={\"help\": \" Lora alpha.\"})\n    lora_dropout: float = field(default=0.0, metadata={\"help\":\"Lora dropout.\"})", "class TrainingConfig:\n    model_name: str = field(default=\"meta-llama/Llama-2-7b-hf\", metadata={\"help\": 'Huggingface Name of the model you want to train'})\n    data_path: str = field(default=\"data.json\", metadata={\"help\": 'Path towards your training data'})\n    output_dir: str = field(default='./trained_model', metadata={\"help\": 'The output dir for logs and checkpoints'})\n    training_recipe: str = field(default=\"lora\", metadata={\"help\": \"Lora Training or Full Training\"})\n    optim: str = field(default='paged_adamw_8bit', metadata={\"help\": 'The optimizer to be used'})\n    batch_size: int = field(default=1, metadata={\"help\": 'The training batch size per GPU. Increase for better speed.'})\n    gradient_accumulation_steps: int = field(default=16, metadata={\"help\": 'How many gradients to accumulate before to perform an optimizer step'})\n    n_epochs: int = field(default=3, metadata={\"help\": 'How many optimizer update steps to take'})\n    weight_decay: float = field(default=0.0, metadata={\"help\": 'The L2 weight decay rate of AdamW'}) \n    learning_rate: float = field(default=1e-4, metadata={\"help\": 'The learning rate'})\n    max_grad_norm: float = field(default=0.3, metadata={\"help\": 'Gradient clipping max norm. This is tuned and works well for all models tested.'})\n    gradient_checkpointing: bool = field(default=True, metadata={\"help\": 'Use gradient checkpointing. You want to use this.'})\n    do_train: bool = field(default=True, metadata={\"help\": 'To train or not to train, that is the question?'})\n    lr_scheduler_type: str = field(default='cosine', metadata={\"help\": 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'})\n    warmup_ratio: float = field(default=0.03, metadata={\"help\": 'Fraction of steps to do a warmup for'})\n    logging_steps: int = field(default=1, metadata={\"help\": 'The frequency of update steps after which to log the loss'})\n    group_by_length: bool = field(default=True, metadata={\"help\": 'Group sequences into batches with same length. Saves memory and speeds up training considerably.'})\n    save_strategy: str = field(default='epoch', metadata={\"help\": 'When to save checkpoints'})\n    save_total_limit: int = field(default=3, metadata={\"help\": 'How many checkpoints to save before the oldest is overwritten'})\n    fp16: bool = field(default=True, metadata={\"help\": 'Whether to use fp16 mixed precision training'})\n    tokenizer_type: str = field(default=\"llama\", metadata={\"help\": \"Tokenizer type. Should be \\\"llama\\\" for llama models to address tokenizer issue\"})\n    trust_remote_code: str = field(default=False, metadata={\"help\": \"Whether to trust remote code.\"})\n    compute_dtype: torch.dtype = field(default=torch.float16, metadata={\"help\":\"Compute Datatype for models, either float16 or float32.\"})\n    max_tokens: int = field(default=4096, metadata={\"help\":\"Max tokens\"})\n    do_eval: bool = field(default=True, metadata={\"help\": \"Whether to evaluate or not\"})\n    evaluation_strategy: str = field(default=\"epoch\", metadata={\"help\": \"When to evaluate, after certain number of steps or each epoch\"})\n    use_auth_token: str = field(default=None, metadata={\"help\": \"auth token\"})\n    use_fast: bool = field(default=False, metadata={\"help\": \"Whether to use fast tokenizer\"})\n    bits: Optional[int] = field(default=4, metadata={\"help\": \"Number of bits to quantize the model to\"})\n    double_quant: bool = field(default=True, metadata={\"help\": \"Compress the quantization statistics through double quantization.\"})\n    quant_type: str = field(default=\"nf4\", metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"})\n    lora_r: int = field(default=64, metadata={\"help\": \"Lora R dimension.\"})\n    lora_alpha: float = field(default=16, metadata={\"help\": \" Lora alpha.\"})\n    lora_dropout: float = field(default=0.0, metadata={\"help\":\"Lora dropout.\"})", "\n\nhfparser = HfArgumentParser((TrainingConfig))\nargs = hfparser.parse_args_into_dataclasses(return_remaining_strings=True)[0]\n\ntrainer = ChatTrainer(training_config=args)\ntrainer.train()"]}
{"filename": "llamatune/llamatune/__init__.py", "chunked_list": ["from .trainer import ChatTrainer"]}
{"filename": "llamatune/llamatune/utils.py", "chunked_list": ["import torch\n\ndef _get_compute_dtype():\n    major, minor = torch.cuda.get_device_capability()\n    if major >= 8:\n        return torch.bfloat16\n    else:\n        return torch.float16\n\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || \"\n        f\"all params: {all_param} || \"\n        f\"trainable: {100 * trainable_params / all_param}\"\n    )", "\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || \"\n        f\"all params: {all_param} || \"\n        f\"trainable: {100 * trainable_params / all_param}\"\n    )", ""]}
{"filename": "llamatune/llamatune/trainer.py", "chunked_list": ["import transformers\nfrom typing import Dict\nfrom dataclasses import dataclass, field\nimport torch\n\nfrom llamatune.model_engines.llama_model_engine import LlamaEngine\nfrom llamatune.data.chat_data_module import ChatDataModule\n\n\nclass ChatTrainer:\n    def __init__(self, training_config: dataclass):\n        self.training_config = training_config\n        self.model_engine : LlamaEngine = LlamaEngine(training_config.model_name, training_config=training_config)\n        self.data_path = training_config.data_path\n\n    def train(self):\n        self.model_engine.prepare_model_for_training()\n\n        self.data_module = ChatDataModule(\n            tokenizer=self.model_engine.tokenizer,\n            data_path_train=self.data_path,\n            max_tokens=self.model_engine.config.max_tokens\n        )\n\n        self.model_engine.train(data_module=self.data_module)", "\nclass ChatTrainer:\n    def __init__(self, training_config: dataclass):\n        self.training_config = training_config\n        self.model_engine : LlamaEngine = LlamaEngine(training_config.model_name, training_config=training_config)\n        self.data_path = training_config.data_path\n\n    def train(self):\n        self.model_engine.prepare_model_for_training()\n\n        self.data_module = ChatDataModule(\n            tokenizer=self.model_engine.tokenizer,\n            data_path_train=self.data_path,\n            max_tokens=self.model_engine.config.max_tokens\n        )\n\n        self.model_engine.train(data_module=self.data_module)", "        \n"]}
{"filename": "llamatune/llamatune/data/__init__.py", "chunked_list": ["\n"]}
{"filename": "llamatune/llamatune/data/chat_data_module.py", "chunked_list": ["import torch\nimport transformers\nimport io\nimport json\nimport logging\n\nfrom dataclasses import dataclass\nfrom typing import Dict, Sequence\nfrom torch.utils.data import Dataset\nfrom llamatune.data.data_module import DataModule", "from torch.utils.data import Dataset\nfrom llamatune.data.data_module import DataModule\n\n\nSYS_PREFIX = \"<<SYS>>\\n\"\nSYS_POSTFIX = \"\\n<</SYS>>\\n\\n\"\nINST_PREFIX = \"<s>[INST] \"\nINST_POSTFIX = \" \"\nOUTPUT_PREFIX = \"[/INST] \"\nOUTPUT_POSTFIX = \"</s>\"", "OUTPUT_PREFIX = \"[/INST] \"\nOUTPUT_POSTFIX = \"</s>\"\n\n\ndef _make_r_io_base(f, mode: str):\n    if not isinstance(f, io.IOBase):\n        f = open(f, mode=mode)\n    return f\n\n\ndef jload(f, mode=\"r\"):\n    \"\"\"Load a .json file into a dictionary.\"\"\"\n    f = _make_r_io_base(f, mode)\n    jdict = json.load(f)\n    f.close()\n    return jdict", "\n\ndef jload(f, mode=\"r\"):\n    \"\"\"Load a .json file into a dictionary.\"\"\"\n    f = _make_r_io_base(f, mode)\n    jdict = json.load(f)\n    f.close()\n    return jdict\n\n\nclass ChatDataset(Dataset):\n    def __init__(self, data_path: str, tokenizer: transformers.AutoTokenizer, max_tokens=None):\n        super(ChatDataset, self).__init__()\n        logging.warning(\"Loading data...\")\n        conversations = jload(data_path)\n\n        logging.warning(\"Tokenizing inputs... This may take some time...\")\n        data_dict = preprocess(conversations, tokenizer, max_tokens)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(input_ids=self.input_ids[i], labels=self.labels[i])", "\n\nclass ChatDataset(Dataset):\n    def __init__(self, data_path: str, tokenizer: transformers.AutoTokenizer, max_tokens=None):\n        super(ChatDataset, self).__init__()\n        logging.warning(\"Loading data...\")\n        conversations = jload(data_path)\n\n        logging.warning(\"Tokenizing inputs... This may take some time...\")\n        data_dict = preprocess(conversations, tokenizer, max_tokens)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(input_ids=self.input_ids[i], labels=self.labels[i])", "\n\n@dataclass\nclass DataCollatorForChatDataset(object):\n    \"\"\"\n    Collate examples for supervised fine-tuning.\n    \"\"\"\n\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n\n        return dict(\n            input_ids=input_ids,\n            labels=labels,\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n        )", "    \n\nclass ChatDataModule(DataModule):\n    def __init__(self, tokenizer: transformers.PreTrainedTokenizer, data_path_train: str, max_tokens = None):\n\n        self.train_dataset = ChatDataset(tokenizer=tokenizer, data_path=data_path_train, max_tokens=max_tokens)\n        self.data_collator = DataCollatorForChatDataset(tokenizer=tokenizer)\n        \n\ndef preprocess(conversations: Sequence[Sequence[dict]], tokenizer: transformers.PreTrainedTokenizer, max_tokens=None) -> Dict:\n    \"\"\"\n    Preprocess the data by tokenizing.\n    \"\"\"\n\n    all_input_ids = []\n    all_labels = []\n\n    for conv in conversations:\n        roles = [msg[\"role\"] for msg in conv]\n        messages = [msg[\"content\"] for msg in conv]\n\n        assert roles[0] != \"ASSISTANT\"\n        assert roles[-1] == \"ASSISTANT\"\n\n        input_messages = []\n        if roles[0] == \"SYSTEM\":\n            input_messages.append(SYS_PREFIX+messages[0]+SYS_POSTFIX)\n\n        for role, msg in zip(roles, messages):\n            if role == \"ASSISTANT\":\n                input_messages.append(msg + OUTPUT_POSTFIX)\n            elif role == \"USER\":\n                input_messages.append(INST_PREFIX + msg + INST_POSTFIX + OUTPUT_PREFIX)\n\n        tokenized_input = tokenizer(input_messages, add_special_tokens=False)\n\n        input_ids = []\n        labels = []\n\n        if roles[0] == \"SYSTEM\":\n            input_ids.extend(tokenized_input.input_ids[0])\n            labels.extend([-100]*len(tokenized_input.input_ids[0]))\n\n        for role, msg in zip(roles, tokenized_input.input_ids):\n\n            if role == \"USER\":\n                labels.extend([-100]*len(msg))\n                input_ids.extend(msg)\n            \n            elif role == \"ASSISTANT\":\n                labels.extend(msg)\n                input_ids.extend(msg)\n\n        if max_tokens is None:\n            max_tokens = tokenizer.model_max_length\n\n        input_ids = torch.LongTensor(input_ids)[:max_tokens]\n        labels = torch.LongTensor(labels)[:max_tokens]\n\n        assert input_ids.shape == labels.shape   \n\n        all_input_ids.append(input_ids)  \n        all_labels.append(labels)\n\n\n    return dict(input_ids=all_input_ids, labels=all_labels)", "\ndef preprocess(conversations: Sequence[Sequence[dict]], tokenizer: transformers.PreTrainedTokenizer, max_tokens=None) -> Dict:\n    \"\"\"\n    Preprocess the data by tokenizing.\n    \"\"\"\n\n    all_input_ids = []\n    all_labels = []\n\n    for conv in conversations:\n        roles = [msg[\"role\"] for msg in conv]\n        messages = [msg[\"content\"] for msg in conv]\n\n        assert roles[0] != \"ASSISTANT\"\n        assert roles[-1] == \"ASSISTANT\"\n\n        input_messages = []\n        if roles[0] == \"SYSTEM\":\n            input_messages.append(SYS_PREFIX+messages[0]+SYS_POSTFIX)\n\n        for role, msg in zip(roles, messages):\n            if role == \"ASSISTANT\":\n                input_messages.append(msg + OUTPUT_POSTFIX)\n            elif role == \"USER\":\n                input_messages.append(INST_PREFIX + msg + INST_POSTFIX + OUTPUT_PREFIX)\n\n        tokenized_input = tokenizer(input_messages, add_special_tokens=False)\n\n        input_ids = []\n        labels = []\n\n        if roles[0] == \"SYSTEM\":\n            input_ids.extend(tokenized_input.input_ids[0])\n            labels.extend([-100]*len(tokenized_input.input_ids[0]))\n\n        for role, msg in zip(roles, tokenized_input.input_ids):\n\n            if role == \"USER\":\n                labels.extend([-100]*len(msg))\n                input_ids.extend(msg)\n            \n            elif role == \"ASSISTANT\":\n                labels.extend(msg)\n                input_ids.extend(msg)\n\n        if max_tokens is None:\n            max_tokens = tokenizer.model_max_length\n\n        input_ids = torch.LongTensor(input_ids)[:max_tokens]\n        labels = torch.LongTensor(labels)[:max_tokens]\n\n        assert input_ids.shape == labels.shape   \n\n        all_input_ids.append(input_ids)  \n        all_labels.append(labels)\n\n\n    return dict(input_ids=all_input_ids, labels=all_labels)", "\n"]}
{"filename": "llamatune/llamatune/data/data_module.py", "chunked_list": ["import torch\n\nclass DataModule:\n    def __init__():\n        pass\n"]}
{"filename": "llamatune/llamatune/model_engines/llama_model_engine.py", "chunked_list": ["import os\nimport torch\nimport transformers\nimport bitsandbytes as bnb\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Dict\nfrom transformers import BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom transformers import Trainer", "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom transformers import Trainer\n\nfrom llamatune.utils import _get_compute_dtype, print_trainable_parameters\nfrom llamatune.telemetry.send_event import capture_event\n\n\n\nclass LlamaEngine:\n\n    def __init__(self, model_name,  training_config):\n        if not training_config.training_recipe in [\"lora\", \"full_training\"]:\n            raise Exception(f\"{training_config.training_recipe} is not a valid training recipe. Please choose either \\\"lora\\\" or \\\"full_training\\\"\")\n        \n        self.config = training_config\n        self.training_recipe = training_config.training_recipe\n        self.model_name = model_name\n\n    \n    def train(self, data_module):\n        print(\"config\", self.config)\n\n        training_args = self.construct_training_arguments()\n\n        trainer = Trainer(\n            model=self.model,\n            tokenizer=self.tokenizer,\n            train_dataset=data_module.train_dataset,\n            data_collator=data_module.data_collator,\n            args=training_args\n        )\n\n        trainer.train()\n        capture_event(\"end-training\", {})\n\n\n    def prepare_model_for_training(self):\n        capture_event(\"start-training\", {})\n\n        if self.config.training_recipe == \"lora\":\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                load_in_4bit=self.config.bits == 4,\n                load_in_8bit=self.config.bits == 8,\n                device_map=self._get_device_map(),\n                torch_dtype=self.config.compute_dtype,\n                trust_remote_code=self.config.trust_remote_code,\n                use_auth_token=self.config.use_auth_token,\n                quantization_config=BitsAndBytesConfig(\n                    load_in_4bit=self.config.bits == 4,\n                    load_in_8bit=self.config.bits == 8,\n                    llm_int8_threshold=6.0,\n                    llm_int8_has_fp16_weight=False,\n                    bnb_4bit_compute_dtype=self.config.compute_dtype,\n                    bnb_4bit_use_double_quant=self.config.double_quant,\n                    bnb_4bit_quant_type=self.config.quant_type,\n                    trust_remote_code=self.config.trust_remote_code\n                )\n            )\n\n        elif self.config.training_recipe == \"full_training\":\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                device_map=self._get_device_map(),\n                torch_dtype=self.config.compute_dtype,\n                trust_remote_code=self.config.trust_remote_code,\n                use_auth_token=self.config.use_auth_token\n            )\n\n\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name, padding_side=\"right\", use_fast=self.config.use_fast, tokenizer_type=self.config.tokenizer_type, trust_remote_code=self.config.trust_remote_code, use_auth_token=self.config.use_auth_token)\n\n        self._smart_tokenizer_and_embedding_resize()\n\n        if self.training_recipe == \"lora\":\n            self.model = prepare_model_for_kbit_training(self.model, use_gradient_checkpointing=self.config.gradient_checkpointing)\n\n        if self.config.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n\n        if self.training_recipe == \"lora\":\n            modules = self._find_all_linear_names()\n\n            lora_config = LoraConfig(\n                r=self.config.lora_r,\n                lora_alpha=self.config.lora_alpha,\n                target_modules=modules,\n                lora_dropout=self.config.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n            \n            self.model = get_peft_model(self.model, lora_config)\n\n        print(\"Model ready for training!\")\n        print_trainable_parameters(self.model)\n\n\n    def _get_device_map(self):\n        device_map = \"auto\"\n\n        if os.environ.get(\"LOCAL_RANK\") is not None:\n            device_map = {'': int(os.environ.get('LOCAL_RANK', '0'))}\n        \n        return device_map\n    \n    def construct_training_arguments(self):\n\n        args=transformers.TrainingArguments(\n                output_dir = self.config.output_dir,\n                optim = self.config.optim,\n                per_device_train_batch_size = self.config.batch_size,\n                gradient_accumulation_steps = self.config.gradient_accumulation_steps,\n                num_train_epochs = self.config.n_epochs,\n                weight_decay = self.config.weight_decay,\n                learning_rate = self.config.learning_rate,\n                max_grad_norm = self.config.max_grad_norm,\n                gradient_checkpointing = self.config.gradient_checkpointing,\n                do_train = self.config.do_train,\n                lr_scheduler_type = self.config.lr_scheduler_type,\n                warmup_ratio = self.config.warmup_ratio,\n                logging_steps = self.config.logging_steps,\n                group_by_length = self.config.group_by_length,\n                save_strategy = self.config.save_strategy,\n            )\n\n        return args\n\n\n    def _find_all_linear_names(self):\n        cls = bnb.nn.Linear4bit if self.config.bits == 4 else (bnb.nn.Linear8bitLt if self.config.bits == 8 else torch.nn.Linear)\n        lora_module_names = set()\n        for name, module in self.model.named_modules():\n            if isinstance(module, cls):\n                names = name.split('.')\n                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n        if \"lm_head\" in lora_module_names: # needed for 16-bit\n            lora_module_names.remove(\"lm_head\")\n\n        return list(lora_module_names)\n    \n\n    def _smart_tokenizer_and_embedding_resize(self):\n        if self.tokenizer.pad_token is None:\n            num_new_tokens = self.tokenizer.add_special_tokens(dict(pad_token=\"[PAD]\"))\n            self.model.resize_token_embeddings(len(self.tokenizer))\n\n            if num_new_tokens > 0:\n                input_embeddings = self.model.get_input_embeddings().weight.data\n                output_embeddings = self.model.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg", "class LlamaEngine:\n\n    def __init__(self, model_name,  training_config):\n        if not training_config.training_recipe in [\"lora\", \"full_training\"]:\n            raise Exception(f\"{training_config.training_recipe} is not a valid training recipe. Please choose either \\\"lora\\\" or \\\"full_training\\\"\")\n        \n        self.config = training_config\n        self.training_recipe = training_config.training_recipe\n        self.model_name = model_name\n\n    \n    def train(self, data_module):\n        print(\"config\", self.config)\n\n        training_args = self.construct_training_arguments()\n\n        trainer = Trainer(\n            model=self.model,\n            tokenizer=self.tokenizer,\n            train_dataset=data_module.train_dataset,\n            data_collator=data_module.data_collator,\n            args=training_args\n        )\n\n        trainer.train()\n        capture_event(\"end-training\", {})\n\n\n    def prepare_model_for_training(self):\n        capture_event(\"start-training\", {})\n\n        if self.config.training_recipe == \"lora\":\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                load_in_4bit=self.config.bits == 4,\n                load_in_8bit=self.config.bits == 8,\n                device_map=self._get_device_map(),\n                torch_dtype=self.config.compute_dtype,\n                trust_remote_code=self.config.trust_remote_code,\n                use_auth_token=self.config.use_auth_token,\n                quantization_config=BitsAndBytesConfig(\n                    load_in_4bit=self.config.bits == 4,\n                    load_in_8bit=self.config.bits == 8,\n                    llm_int8_threshold=6.0,\n                    llm_int8_has_fp16_weight=False,\n                    bnb_4bit_compute_dtype=self.config.compute_dtype,\n                    bnb_4bit_use_double_quant=self.config.double_quant,\n                    bnb_4bit_quant_type=self.config.quant_type,\n                    trust_remote_code=self.config.trust_remote_code\n                )\n            )\n\n        elif self.config.training_recipe == \"full_training\":\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                self.model_name,\n                device_map=self._get_device_map(),\n                torch_dtype=self.config.compute_dtype,\n                trust_remote_code=self.config.trust_remote_code,\n                use_auth_token=self.config.use_auth_token\n            )\n\n\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.model_name, padding_side=\"right\", use_fast=self.config.use_fast, tokenizer_type=self.config.tokenizer_type, trust_remote_code=self.config.trust_remote_code, use_auth_token=self.config.use_auth_token)\n\n        self._smart_tokenizer_and_embedding_resize()\n\n        if self.training_recipe == \"lora\":\n            self.model = prepare_model_for_kbit_training(self.model, use_gradient_checkpointing=self.config.gradient_checkpointing)\n\n        if self.config.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n\n        if self.training_recipe == \"lora\":\n            modules = self._find_all_linear_names()\n\n            lora_config = LoraConfig(\n                r=self.config.lora_r,\n                lora_alpha=self.config.lora_alpha,\n                target_modules=modules,\n                lora_dropout=self.config.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n            \n            self.model = get_peft_model(self.model, lora_config)\n\n        print(\"Model ready for training!\")\n        print_trainable_parameters(self.model)\n\n\n    def _get_device_map(self):\n        device_map = \"auto\"\n\n        if os.environ.get(\"LOCAL_RANK\") is not None:\n            device_map = {'': int(os.environ.get('LOCAL_RANK', '0'))}\n        \n        return device_map\n    \n    def construct_training_arguments(self):\n\n        args=transformers.TrainingArguments(\n                output_dir = self.config.output_dir,\n                optim = self.config.optim,\n                per_device_train_batch_size = self.config.batch_size,\n                gradient_accumulation_steps = self.config.gradient_accumulation_steps,\n                num_train_epochs = self.config.n_epochs,\n                weight_decay = self.config.weight_decay,\n                learning_rate = self.config.learning_rate,\n                max_grad_norm = self.config.max_grad_norm,\n                gradient_checkpointing = self.config.gradient_checkpointing,\n                do_train = self.config.do_train,\n                lr_scheduler_type = self.config.lr_scheduler_type,\n                warmup_ratio = self.config.warmup_ratio,\n                logging_steps = self.config.logging_steps,\n                group_by_length = self.config.group_by_length,\n                save_strategy = self.config.save_strategy,\n            )\n\n        return args\n\n\n    def _find_all_linear_names(self):\n        cls = bnb.nn.Linear4bit if self.config.bits == 4 else (bnb.nn.Linear8bitLt if self.config.bits == 8 else torch.nn.Linear)\n        lora_module_names = set()\n        for name, module in self.model.named_modules():\n            if isinstance(module, cls):\n                names = name.split('.')\n                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n        if \"lm_head\" in lora_module_names: # needed for 16-bit\n            lora_module_names.remove(\"lm_head\")\n\n        return list(lora_module_names)\n    \n\n    def _smart_tokenizer_and_embedding_resize(self):\n        if self.tokenizer.pad_token is None:\n            num_new_tokens = self.tokenizer.add_special_tokens(dict(pad_token=\"[PAD]\"))\n            self.model.resize_token_embeddings(len(self.tokenizer))\n\n            if num_new_tokens > 0:\n                input_embeddings = self.model.get_input_embeddings().weight.data\n                output_embeddings = self.model.get_output_embeddings().weight.data\n\n                input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n                output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n                input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                output_embeddings[-num_new_tokens:] = output_embeddings_avg", "        \n"]}
{"filename": "llamatune/llamatune/model_engines/__init__.py", "chunked_list": [""]}
{"filename": "llamatune/llamatune/telemetry/send_event.py", "chunked_list": ["from posthog import Posthog\nimport os\n\nposthog = Posthog('phc_YpKoFD7smPe4SXRtVyMW766uP9AjUwnuRJ8hh2EJcVv',\n                  host='https://eu.posthog.com')\n\n\ndef capture_event(event_name, event_properties):\n    if not os.environ.get('EVENT_CAPTURE') == \"disable\":\n        posthog.capture(\"not_distinct\", event_name, event_properties)"]}
{"filename": "llamatune/llamatune/telemetry/__init__.py", "chunked_list": [""]}
