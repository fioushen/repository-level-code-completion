{"filename": "tools/render.py", "chunked_list": ["\"\"\"Convert the results to an ingredient for LaTeX table.\n\"\"\"\n\nimport argparse\nimport json\nimport os\n\nimport numpy as np\nfrom termcolor import cprint\n", "from termcolor import cprint\n\nfrom evalplus.eval import estimate_pass_at_k\n\nTEMPS = [0.2, 0.4, 0.6, 0.8]\n\n\ndef analyze_resfile(resfile):\n    before_summary = {}\n    after_summary = {}\n\n    res = json.load(open(resfile))[\"eval\"]\n    total = []\n    before_pass = []\n    after_pass = []\n    for v in res.values():\n        total.append(v[\"nfiles\"])\n        bc = sum([r[0] == SUCCESS for r in v[\"base\"]])\n        before_pass.append(bc)\n        if v[\"plus\"]:\n            after_pass.append(\n                sum(\n                    [\n                        v[\"plus\"][i][0] == v[\"base\"][i][0] == SUCCESS\n                        for i in range(len(v[\"plus\"]))\n                    ]\n                )\n            )\n\n    total = np.array(total)\n    before_pass = np.array(before_pass)\n    after_pass = np.array(after_pass)\n    for k in [1, 10, 100]:\n        if total.min() >= k:\n            pass_at_k = estimate_pass_at_k(total, before_pass, k).mean()\n            before_summary[f\"pass@{k}\"] = pass_at_k * 100  # scale to %\n    for k in [1, 10, 100]:\n        if total.min() >= k:\n            pass_at_k = estimate_pass_at_k(total, after_pass, k).mean()\n            after_summary[f\"pass@{k}\"] = pass_at_k * 100\n\n    return before_summary, after_summary", "\n\ndef align_ampersands(str1, str2):\n    \"\"\"\n    This function takes two strings containing various \"&\" characters and transforms them so that the indices of \"&\"\n    are aligned. This is useful for formatting LaTeX tables.\n\n    Args:\n        str1 (str): First input string containing \"&\" characters.\n        str2 (str): Second input string containing \"&\" characters.\n\n    Returns:\n        Tuple[str, str]: Two transformed strings with aligned \"&\" indices.\n    \"\"\"\n    # Find indices of \"&\" characters in both strings\n    amp_idx1 = [i for i, char in enumerate(str1) if char == \"&\"]\n    amp_idx2 = [i for i, char in enumerate(str2) if char == \"&\"]\n\n    assert len(amp_idx1) == len(amp_idx2)\n\n    # Replace \"&\" characters in the strings with \"\\&\" at the aligned indices\n    acc1, acc2 = 0, 0\n    for i, j in zip(amp_idx1, amp_idx2):\n        diff = (j + acc2) - (i + acc1)\n        if diff > 0:\n            str1 = str1[: i + acc1] + \" \" * diff + str1[i + acc1 :]\n            acc1 += diff\n        elif diff < 0:\n            str2 = str2[: j + acc2] + \" \" * (-diff) + str2[j + acc2 :]\n            acc2 -= diff\n\n    return str1, str2", "\n\ndef texprint(before_summary, after_summary, bfgreedy, afgreedy):\n    TEXTTEMPS = [r\"\\temptwo{}\", r\"\\tempfour{}\", r\"\\tempsix{}\", r\"\\tempeight{}\"]\n\n    def aplus(s) -> str:\n        return r\"\\aplus{\" + s + r\"}\"\n\n    def make_line(summary, amax, ap=False):\n        pkvals = [f\"{v[amax[i]]:.1f}\" for i, v in enumerate(summary.values())]\n        if ap:\n            pkvals = [aplus(v) for v in pkvals]\n        return (\n            \" & \".join(pkvals)\n            + \" & \"\n            + \" & \".join([TEXTTEMPS[i] for i in amax])\n            + r\" \\\\\"\n        )\n\n    print(\"======== LaTeX Table Ingredent ========\")\n    argmax = [np.argmax(v) for v in before_summary.values()]\n    text1 = \"before & \"\n    if bfgreedy is not None:\n        text1 += f\"{bfgreedy:.1f} & \"\n    argmax = [np.argmax(v) for v in after_summary.values()]\n    text1 += make_line(before_summary, argmax)\n\n    text2 = \"after & \"\n    if afgreedy is not None:\n        text2 += aplus(f\"{afgreedy:.1f}\") + \" & \"\n    text2 += make_line(after_summary, argmax, ap=True)\n\n    text1, text2 = align_ampersands(text1, text2)\n    cprint(text1, \"green\")\n    cprint(text2, \"green\")", "\n\ndef rich_print(before_summary, after_summary, bfgreedy, afgreedy):\n    from rich.console import Console\n    from rich.table import Table\n\n    console = Console()\n    table = Table(show_header=True, header_style=\"bold magenta\", title=\"pass@k results\")\n\n    before_row = []\n    after_row = []\n\n    table.add_column(\" \", style=\"dim\", no_wrap=True)\n\n    if bfgreedy is not None:\n        assert afgreedy is not None\n        table.add_column(\"Greedy pass@1\", justify=\"right\", style=\"bold green\")\n        before_row.append(f\"{bfgreedy:.1f}\")\n        after_row.append(f\"{afgreedy:.1f}\")\n\n    for k in before_summary:\n        table.add_column(k, justify=\"right\", style=\"bold magenta\")\n        table.add_column(\"Tbest.\", justify=\"right\")\n        amax_before = np.argmax(before_summary[k])\n        amax_after = np.argmax(after_summary[k])\n        before_row.append(f\"{before_summary[k][amax_before]:.1f}\")\n        before_row.append(f\"{TEMPS[amax_before]}\")\n        after_row.append(f\"{after_summary[k][amax_after]:.1f}\")\n        after_row.append(f\"{TEMPS[amax_after]}\")\n\n    table.add_row(\"Before\", *before_row)\n    table.add_row(\"After\", *after_row)\n\n    console.print(table)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--type\", type=str, required=True)\n    args = parser.parse_args()\n\n    # Analyszing 0.2~0.8 temperatures.\n    resfiles = []\n    # check existance\n    for t in TEMPS:\n        f = os.path.join(f\"{args.type}_temp_{t}\", f\"eval_results.json\")\n        assert os.path.exists(f), f\"{f} not found\"\n        resfiles.append(f)\n\n    before_summary = {}\n    after_summary = {}\n\n    SUCCESS = \"success\"\n\n    for resfile in resfiles:\n        # load the results\n        before, after = analyze_resfile(resfile)\n        for k, v in before.items():\n            before_summary.setdefault(k, []).append(v)\n        for k, v in after.items():\n            after_summary.setdefault(k, []).append(v)\n\n    # print pass@1~100, and corresponding max temperature\n\n    # Analyszing greedy decoding (temperature=0.0)\n    gf = os.path.join(f\"{args.type}_temp_0.0\", f\"eval_results.json\")\n    bfgreedy, afgreedy = None, None\n    if os.path.exists(gf):\n        bfgreedy, afgreedy = analyze_resfile(gf)\n        bfgreedy = bfgreedy[\"pass@1\"]\n        afgreedy = afgreedy[\"pass@1\"]\n\n    # Rich printing\n    rich_print(before_summary, after_summary, bfgreedy, afgreedy)\n\n    # LaTeX printing\n    texprint(before_summary, after_summary, bfgreedy, afgreedy)", ""]}
{"filename": "tools/filter_inputs.py", "chunked_list": ["import json\nimport os\n\nfrom evalplus.data import get_human_eval_plus\nfrom evalplus.gen.util import trusted_exec\n\n\ndef execute(code, input_list) -> bool:\n    try:\n        trusted_exec(code, [input_list], entry_point)\n    except Exception as e:\n        assert str(e) == \"invalid inputs\"\n        return False\n    return True", "\n\ndef write(new_input_dict):\n    with open(new_input_path, \"a\") as f:\n        f.write(json.dumps(new_input_dict) + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input\", type=str, default=\"HumanEvalPlusInputs.jsonl\")\n    args = parser.parse_args()\n\n    new_input_path = args.input.replace(\".jsonl\", \"_sanitized.jsonl\")\n    assert not os.path.exists(new_input_path)\n\n    task_inputs = {}\n    for line in open(args.input, \"r\").read().split(\"\\n\"):\n        if not line:\n            continue\n        plus = json.loads(line)\n        task_inputs[plus[\"task_id\"]] = plus[\"inputs\"]\n\n    for p in get_human_eval_plus().values():\n        entry_point = p[\"entry_point\"]\n        code = p[\"prompt\"] + p[\"canonical_solution\"]\n        task_id = p[\"task_id\"]\n        new_inputs = task_inputs[task_id]\n        count = 0\n        new_input_dict = {\"task_id\": task_id, \"inputs\": []}\n        for input_list in new_inputs:\n            res = execute(code, input_list)\n            if res:\n                new_input_dict[\"inputs\"].append(input_list)\n            else:\n                count += 1\n        write(new_input_dict)\n        if count != 0:\n            print(f\"Task {task_id}: {count}/{len(new_inputs)} tests filtered\")", ""]}
{"filename": "tools/init_ground_truth.py", "chunked_list": ["import os\nimport pathlib\n\nfrom evalplus.data import get_human_eval\n\nif __name__ == \"__main__\":\n    # check existance of ground truth folder\n    GT_DIR = pathlib.Path(__file__).parent.parent / \"groundtruth\" / \"humaneval\"\n\n    assert not os.path.exists(\n        GT_DIR\n    ), \"Ground truth folder already exists! If you want to reinitialize, delete the folder first.\"\n\n    os.mkdir(GT_DIR)\n\n    human_eval = get_human_eval()\n    for i, task in enumerate(human_eval):\n        incomplete = (\n            task[\"prompt\"]\n            + task[\"canonical_solution\"]\n            + \"\\n\\n\"\n            + task[\"test\"]\n            + \"\\n\"\n            + f\"check({task['entry_point']})\"\n        )\n        with open(\n            os.path.join(GT_DIR, f\"{str(i).zfill(3)}_{task['entry_point']}.py\"),\n            \"w\",\n        ) as f:\n            f.write(incomplete)", ""]}
{"filename": "tools/viz_passrate.py", "chunked_list": ["import json\nimport os\nimport pickle\nfrom os import PathLike\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom evalplus.eval import estimate_pass_at_k\n\nSMALL_SIZE = 10\nMEDIUM_SIZE = 14\nBIGGER_SIZE = 18\n\nplt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\nplt.rc(\"axes\", titlesize=MEDIUM_SIZE)  # fontsize of the axes title", "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\nplt.rc(\"axes\", titlesize=MEDIUM_SIZE)  # fontsize of the axes title\nplt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\nplt.rc(\"xtick\", labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\nplt.rc(\"ytick\", labelsize=MEDIUM_SIZE)  # fontsize of the tick labels\nplt.rc(\"legend\", fontsize=MEDIUM_SIZE - 1)  # legend fontsize\nplt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n\nplt.rc(\"text\", usetex=True)\n", "plt.rc(\"text\", usetex=True)\n\nSUCCESS = \"success\"\n\n\ndef passk_rel_drop(task2bvs_old, task2bvs_new):\n    # old_rate:\n    # dim0: problems\n    # dim1: each experiment (model@temperature)\n    # dim2: pass/fail booleans for each sample\n    # this fn computes the relative drop in pass@k averaged over experiments\n\n    passk_old = {}\n    passk_new = {}\n    # sample size => k => List[pass@k]\n\n    for exp_i in range(len(task2bvs_old[0])):\n        ntotal = []\n        npass_old = []\n        npass_new = []\n        nsamples = None\n        for task_i in range(len(task2bvs_old)):\n            bv_old = task2bvs_old[task_i][exp_i]\n            bv_new = task2bvs_new[task_i][exp_i]\n            ntotal.append(len(bv_old))\n            npass_old.append(bv_old.sum())\n            npass_new.append(bv_new.sum())\n            if nsamples is None:\n                nsamples = len(bv_old)\n            assert len(bv_old) == len(bv_new) == nsamples\n\n        d_old = passk_old.setdefault(nsamples, {})\n        d_new = passk_new.setdefault(nsamples, {})\n        for k in [1, 10, 100]:\n            if nsamples >= k:\n                pass_at_k_old = estimate_pass_at_k(ntotal, npass_old, k).mean() * 100\n                pass_at_k_new = estimate_pass_at_k(ntotal, npass_new, k).mean() * 100\n                d_old.setdefault(k, []).append(pass_at_k_old)\n                d_new.setdefault(k, []).append(pass_at_k_new)\n\n    for nsamples in passk_old:\n        print(\"=====================================\")\n        print(f\"{nsamples = }\")\n        do = passk_old[nsamples]\n        dn = passk_new[nsamples]\n        drops = []\n        for k in [1, 10, 100]:\n            if k in do:\n                pko = np.array(do[k]).mean()\n                pkn = np.array(dn[k]).mean()\n                drop = 100 * (pko - pkn) / pko\n                drops.append(drop)\n                print(f\"pass@{k}: \\t{pko:.1f}% -> {pkn:.1f}% (drop {drop:.1f}%)\")\n        drops = np.array(drops)\n        print(f\"+++ {drops.mean() = :.1f}%\")\n        print(\"=====================================\")", "\n\ndef get_data(paths: List[PathLike]):\n    task2bvs_old = None\n    task2bvs_new = None\n\n    for path in tqdm(paths):  # each experiment\n        res = json.load(open(path, \"r\"))[\"eval\"]\n        ntask = len(res)\n\n        assert ntask == 164\n\n        if task2bvs_old is None and task2bvs_new is None:\n            task2bvs_old = [[] for _ in range(ntask)]\n            task2bvs_new = [[] for _ in range(ntask)]\n            # i-th => task-i pass rate for an experiment\n\n        for i, v in enumerate(res.values()):  # each task\n            base = v[\"base\"]\n            plus = v[\"plus\"]\n            bbv = np.array([s == SUCCESS for s, _ in base])\n            pbv = np.array([s == SUCCESS for s, _ in plus]) & bbv\n            assert bbv.mean() >= pbv.mean()\n\n            task2bvs_old[i].append(bbv)\n            task2bvs_new[i].append(pbv)\n\n    assert len(task2bvs_old) == len(task2bvs_new)\n    return task2bvs_old, task2bvs_new", "\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--root\", type=str, default=\"/JawTitan/EvalPlus/humaneval\")\n    args = parser.parse_args()\n\n    paths = []\n    for path in os.listdir(args.root):\n        eval_json_path = os.path.join(args.root, path, \"eval_results.json\")\n        if not os.path.isfile(eval_json_path) or not path[-1].isdigit():\n            print(f\"skip {path}\")\n            continue\n        paths.append(eval_json_path)\n\n    CACHE_PATH = \"passrate.pkl\"\n    if os.path.isfile(CACHE_PATH):  # use cache\n        task2bvs_old, task2bvs_new = pickle.load(open(CACHE_PATH, \"rb\"))\n    else:\n        task2bvs_old, task2bvs_new = get_data(paths)\n        pickle.dump((task2bvs_old, task2bvs_new), open(CACHE_PATH, \"wb\"))\n\n    passk_rel_drop(task2bvs_old, task2bvs_new)\n\n    rate_old = [[bv.mean() for bv in task] for task in task2bvs_old]\n    rate_new = [[bv.mean() for bv in task] for task in task2bvs_new]\n\n    rate_old = 100 * np.array(rate_old).mean(axis=1)\n    rate_new = 100 * np.array(rate_new).mean(axis=1)\n\n    ntask = len(rate_old)\n\n    # sort by old pass rate\n    # numpy argsort\n    indices = np.array(rate_old).argsort()\n    # find unsolved tasks. i.e., indices where rate_old == 0\n    unsolved = np.where(np.array(rate_old) == 0)[0]\n    print(\"Unsolvable: \", unsolved)\n    # sort indices according to the differences between rate_old and rate_new\n    diff = np.array(rate_old) - np.array(rate_new)\n    diff_indices = diff.argsort()\n    for i in reversed(diff_indices[-10:]):\n        print(\n            f\"#{i} drops {diff[i] :.1f} ~ {100 * diff[i] / rate_old[i]:.1f}%:\"\n            f\" {rate_old[i]:.1f} -> {rate_new[i]:.1f}\"\n        )\n\n    rate_old = np.array(rate_old)[indices]\n    rate_new = np.array(rate_new)[indices]\n    # rate_old, rate_new = zip(*sorted(zip(rate_old, rate_new), key=lambda x: x[0]))\n\n    # making a barplot\n    x = np.arange(ntask)\n    width = 1  # the width of the bars\n\n    fig, ax = plt.subplots(figsize=(9, 3))\n    HUMANEVAL = r\"\\textsc{HumanEval}\"\n    HUMANEVAL_PLUS = r\"\\textsc{HumanEval\\textsuperscript{+}}\"\n    rects1 = ax.bar(x, rate_old, color=\"coral\", label=HUMANEVAL, alpha=0.5)\n    rects2 = ax.bar(x, rate_new, color=\"firebrick\", label=HUMANEVAL_PLUS, alpha=0.6)\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax.set_ylabel(\"Average Pass Rate (\\%)\")\n    ax.set_xlabel(f\"Problems (Sorted by {HUMANEVAL} pass rate)\")\n\n    # turn off xticks\n    ax.set_xticks([])\n    ax.set_xticklabels([])\n    # tight x ranges\n    ax.set_xlim(-0.5, ntask - 0.5)\n\n    # x grid\n    ax.grid(linestyle=\"--\", linewidth=1, alpha=0.8)\n\n    # log scale\n    ax.set_yscale(\"log\")\n\n    ax.legend()\n    fig.tight_layout()\n\n    plt.savefig(\"passrate.pdf\", bbox_inches=\"tight\")\n    plt.savefig(\"passrate.png\", bbox_inches=\"tight\")", ""]}
{"filename": "tools/sanitize.py", "chunked_list": ["\"\"\"Purpose of this file: Sanitize the code produced by LLMs for the following reasons.\n1. Vicuna generated code could miss one white space. We fix the white space to make Vicuna more capable.\n2. {Our fault lol.} We find more EOFs tokens afterwards and truncate some messy code afterwards.\n\"\"\"\n\nimport os\n\nfrom tqdm import tqdm\n\nfrom evalplus.data import get_human_eval", "\nfrom evalplus.data import get_human_eval\n\nINCODER_EXTRA = [\"</code>\", \"<|\", \"</CODE>\"]\nPOLYCODER_EXTRA = [\"\\n//\", \"\\n/*\"]\nNON_CODE_EOFS = [\"<|endoftext|>\", \"\\n```\", \"\\n</s>\", \"\\n#\"]\n\n\ndef get_all_python_files(folder):\n    # return a list of full-path python files\n    py_files = []\n    for root, _, files in os.walk(folder):\n        for file in files:\n            if file.endswith(\".py\"):\n                py_files.append(os.path.join(root, file))\n    return py_files", "def get_all_python_files(folder):\n    # return a list of full-path python files\n    py_files = []\n    for root, _, files in os.walk(folder):\n        for file in files:\n            if file.endswith(\".py\"):\n                py_files.append(os.path.join(root, file))\n    return py_files\n\n\ndef remove_unindented_lines(code, ok_starts):\n    new_code = \"\"\n    for line in code.splitlines():\n        if any([line.startswith(t) for t in ok_starts]) or line.strip() == \"\":\n            new_code += line + \"\\n\"\n            continue\n\n        lspace = len(line) - len(line.lstrip())\n        if lspace == 0:\n            continue\n\n        new_code += line + \"\\n\"\n\n    return new_code", "\n\ndef remove_unindented_lines(code, ok_starts):\n    new_code = \"\"\n    for line in code.splitlines():\n        if any([line.startswith(t) for t in ok_starts]) or line.strip() == \"\":\n            new_code += line + \"\\n\"\n            continue\n\n        lspace = len(line) - len(line.lstrip())\n        if lspace == 0:\n            continue\n\n        new_code += line + \"\\n\"\n\n    return new_code", "\n\nif __name__ == \"__main__\":\n    import argparse\n    import pathlib\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--folder\", type=str, required=True)\n    parser.add_argument(\"--eof\", action=\"store_true\")\n\n    args = parser.parse_args()\n\n    # task_id -> entry_point\n    entry_point = {}\n    for task_id, problem in get_human_eval().items():\n        entry_point[task_id] = problem[\"entry_point\"]\n\n    # make a new folder with \"-sanitized\" suffix\n    old_folder = pathlib.Path(args.folder)\n    new_folder = old_folder.parent / (old_folder.name + \"-sanitized\")\n\n    nsan = 0\n    ntotal = 0\n    for pyf in tqdm(get_all_python_files(args.folder)):\n        # Get [?] from \"[prefix]/HumanEval_[?]/[number].py\":\n        task_id = pyf.split(\"/\")[-2].replace(\"HumanEval_\", \"HumanEval/\")\n\n        ntotal += 1\n        old_code = open(pyf).read()\n\n        fndef = \"def \" + entry_point[task_id] + \"(\"\n        new_code = old_code\n        chunks = new_code.split(fndef)\n        # prefix\n        # impl\n        if len(chunks) == 2:\n            new_code = fndef + chunks[-1]  # fn + impl\n\n        if \"chatgpt\" in args.folder:\n            tmp = \"\"\n            for line in new_code.splitlines():\n                if line.strip() == \"python\":\n                    continue\n                tmp += line + \"\\n\"\n            new_code = tmp\n\n        if \"vicuna\" in args.folder:\n            tmp = \"\"\n            for line in new_code.splitlines():\n                lspace = len(line) - len(line.lstrip())\n                if lspace == 3:\n                    tmp += \" \"\n                tmp += line + \"\\n\"\n            new_code = tmp\n\n        if args.eof:\n            eof_strs = NON_CODE_EOFS\n            if \"incoder\" in args.folder:\n                eof_strs = eof_strs + INCODER_EXTRA\n            if \"polycoder\" in args.folder:\n                eof_strs = eof_strs + POLYCODER_EXTRA\n            for eof in eof_strs:\n                new_code = new_code.split(eof)[0]\n\n        # remove lines that are not indented\n        new_code = remove_unindented_lines(new_code, ok_starts=[fndef])\n\n        if len(chunks) == 2:\n            new_code = chunks[0] + new_code\n\n        # write to new folder\n        new_pyf = pyf.replace(str(old_folder), str(new_folder))\n\n        if new_code.strip() != old_code.strip():\n            print(\"Sanitized: \", pyf, \"->\", new_pyf)\n            nsan += 1\n\n        pathlib.Path(new_pyf).parent.mkdir(parents=True, exist_ok=True)\n        with open(new_pyf, \"w\") as f:\n            f.write(new_code)\n\n    print(f\"Sanitized {nsan} out of {ntotal} files.\")", ""]}
{"filename": "tools/merge_dataset.py", "chunked_list": ["if __name__ == \"__main__\":\n    import argparse\n    import json\n    import os\n\n    from tempdir import TempDir\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset\", default=\"humaneval\", type=str)\n    parser.add_argument(\"--plus-input\", required=True, type=str)\n    parser.add_argument(\"--output\", required=True, type=str)\n    args = parser.parse_args()\n\n    assert args.dataset == \"humaneval\"\n    assert not os.path.exists(args.output), f\"{args.output} already exists!\"\n\n    with TempDir() as tempdir:\n        # Generate inputs\n        plus_input = {}\n        with open(args.plus_input) as file:\n            for line in file:\n                problem = json.loads(line)\n                plus_input[problem[\"task_id\"]] = problem[\"inputs\"]\n\n        tempf = None\n        if args.dataset == \"humaneval\":\n            from evalplus.data import get_human_eval_plus\n\n            # Allow it to be incomplete\n            problems = get_human_eval_plus(err_incomplete=False)\n            tempf = os.path.join(tempdir, \"HumanEvalPlus.jsonl\")\n            with open(tempf, \"w\") as file:\n                for problem in problems:\n                    problem[\"plus_input\"] = plus_input[problem[\"task_id\"]]\n                    file.write(json.dumps(problem) + \"\\n\")\n\n        # Move to the right place\n        os.rename(tempf, args.output)", ""]}
{"filename": "tools/stat_plus.py", "chunked_list": ["import numpy as np\n\nfrom evalplus.data import get_human_eval_plus\n\nif __name__ == \"__main__\":\n    sizes = [\n        [len(inp[\"base_input\"]), len(inp[\"plus_input\"])]\n        for inp in get_human_eval_plus().values()\n    ]\n    size_base = sizes[:, 0]\n    print(f\"{size_base.min() = }\", f\"{size_base.argmin() = }\")\n    print(f\"{size_base.max() = }\", f\"{size_base.argmax() = }\")\n    print(f\"{np.percentile(size_base, 50) = :.1f}\")\n    print(f\"{size_base.mean() = :.1f}\")\n\n    size_plus = sizes[:, 1]\n    size_plus += size_base\n    print(f\"{size_plus.min() = }\", f\"{size_plus.argmin() = }\")\n    print(f\"{size_plus.max() = }\", f\"{size_plus.argmax() = }\")\n    print(f\"{np.percentile(size_plus, 50) = :.1f}\")\n    print(f\"{size_plus.mean() = :.1f}\")", ""]}
{"filename": "tools/checker.py", "chunked_list": ["\"\"\"This file checks two things:\n1. Is the LLMs codegen completed for each benchmark?\n2. Warn the code that are not compilable (it could be some impl issues).\n\"\"\"\n\nimport ast\nimport os\nimport traceback\n\nfrom termcolor import colored", "\nfrom termcolor import colored\n\n\ndef get_all_python_files(folder):\n    # return a list of full-path python files\n    py_files = []\n    for root, _, files in os.walk(folder):\n        for file in files:\n            if file.endswith(\".py\"):\n                py_files.append(os.path.join(root, file))\n    return py_files", "\n\ndef syntax_check(code, verbose=False):\n    try:\n        ast.parse(code)\n        return True\n    except (SyntaxError, MemoryError):\n        if verbose:\n            traceback.print_exc()\n        return False", "\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--folder\", type=str, required=True)\n    parser.add_argument(\"--dataset\", type=str, default=\"humaneval\")\n    parser.add_argument(\"--nsample\", type=int)\n    parser.add_argument(\"--verbose\", action=\"store_true\")\n\n    args = parser.parse_args()\n\n    if args.nsample is None:\n        if \"temp_0.0\" in args.folder:\n            print(colored(\"Setting nsample = 1 for 0 temp.\", \"yellow\"))\n            args.nsample = 1\n        else:\n            print(colored(\"Setting nsample = 200 for non-0 temp.\", \"yellow\"))\n            args.nsample = 200\n\n    if args.dataset == \"humaneval\":\n        ntask = 164\n        print(colored(\"==============================\", \"blue\"))\n        print(colored(\" ::: Checking completeness... \", \"blue\"))\n        print(colored(\" ::::: All tasks complete?    \", \"blue\"))\n        ndone = 0\n        for i in range(ntask):\n            task_folder = os.path.join(args.folder, f\"HumanEval_{i}\")\n            if not os.path.exists(task_folder):\n                print(colored(f\" \u26a0\ufe0f HumanEval_{i} is missing!\", \"red\"))\n                continue\n            # get the # of .py files under task_folder\n            nfiles = len(get_all_python_files(task_folder))\n            if nfiles != args.nsample:\n                print(\n                    colored(\n                        f\" \u26a0\ufe0f HumanEval_{i} only has {nfiles} samples! But {args.nsample} are expected.\",\n                        \"red\",\n                    )\n                )\n                continue\n            ndone += 1\n        if ntask != ndone:\n            ntbd = ntask - ndone\n            print(colored(f\" ::::: \u26a0\ufe0f {ntbd}/{ntask} tasks incomplete!\", \"red\"))\n        else:\n            print(colored(f\" ::::: All {ntask} tasks complete!\", \"green\"))\n\n        print(colored(\"==============================\", \"blue\"))\n        print(colored(\" ::: Checking compilation...  \", \"blue\"))\n        print(colored(\" ::::: All code compilable?   \", \"blue\"))\n        ncode = 0\n        npass = 0\n        for i in range(ntask):\n            task_folder = os.path.join(args.folder, f\"HumanEval_{i}\")\n            # folder must exist\n            if not os.path.exists(task_folder):\n                continue\n\n            for pyf in get_all_python_files(task_folder):\n                ncode += 1\n                if not syntax_check(open(pyf).read(), args.verbose):\n                    print(colored(f\" \u26a0\ufe0f {pyf} is not compilable!\", \"red\"))\n                    npass += 1\n        if ncode != npass:\n            print(colored(f\" ::::: \u26a0\ufe0f {npass}/{ncode} code are not compilable!\", \"red\"))\n    else:\n        raise NotImplementedError", ""]}
{"filename": "tools/humaneval/init_plus.py", "chunked_list": ["\"\"\"\nThis script aims at quickly initialize a sketch for HumanEvalPlus. It's not going to be\nperfect, but we will either manually or automatically fix/complete it later.\n+ CHANGE 1: Adds \"contract\", \"base_input\", \"atol\" in addition to HumanEval.\n\"\"\"\n\nimport json\nimport os\nimport pathlib\nfrom importlib import import_module", "import pathlib\nfrom importlib import import_module\nfrom inspect import getsource\nfrom typing import Tuple\n\nfrom tempdir import TempDir\n\nfrom evalplus.data import get_human_eval\n\nHUMANEVAL_PLUS_PATH = (", "\nHUMANEVAL_PLUS_PATH = (\n    pathlib.Path(__file__).parent.parent.parent / \"HumanEvalPlus.jsonl\"\n)\n\n\ndef _ret(entry_point) -> str:\n    \"\"\"This is a hacky function to return some garbages so that we can\n    successfully run the function .\n    \"\"\"\n    if entry_point == \"sort_third\" or entry_point == \"sort_even\":\n        return [1, 2, 3]\n    elif entry_point == \"bf\":\n        return ()\n    return \"1\"", "\n\ndef instrument_inputs(entry_point, prompt, test) -> str:\n    globals()[\"_inputs\"] = []\n    fn_text = f\"\"\"{prompt.split(f\"def {entry_point}\")[0]}\n\ndef {entry_point}(*args):\n    _inputs.append(args)\n    return {_ret(entry_point)}\n\"\"\"\n    exec(fn_text, globals())\n    exec(test.replace(\"assert \", \"\"), globals())\n    exec(f\"check({entry_point})\", globals())\n    exec(fn_text, globals())\n    return globals()[\"_inputs\"]", "\n\ndef get_contract_and_ref(task_id: int, entry_point) -> Tuple[str, str]:\n    mod = import_module(f\"groundtruth.humaneval.{str(task_id).zfill(3)}_{entry_point}\")\n    fn = getattr(mod, entry_point)\n\n    doc = fn.__doc__\n    if task_id == 51:\n        doc = doc.replace(\"bcdf\\nghjklm\", r\"bcdf\\nghjklm\").replace(\n            \"abcdef\\nghijklm\", r\"abcdef\\nghijklm\"\n        )\n\n    code = (\n        getsource(fn).replace(doc, \"\").replace(\"''''''\", '\"\"\"\"\"\"').split('\"\"\"\"\"\"\\n')[-1]\n    )\n\n    assert code, f\"Something wrong with {task_id}!\"\n    assert code[:3] != \"def\", f\"Something wrong with the {task_id}!\"\n\n    # split code to contract and impl\n    contract = \"\"\n    impl = \"\"\n\n    reading_contract = True\n    for line in code.strip(\"\\n\").split(\"\\n\"):\n        if reading_contract and \"$_CONTRACT_$\" in line:\n            contract += line + \"\\n\"\n        else:\n            reading_contract = False\n            impl += line + \"\\n\"\n\n    if contract:\n        contract = \"\\n\" + contract\n\n    return contract, \"\\n\" + impl + \"\\n\"", "\n\ndef get_atol(task_id: int) -> float:\n    if task_id == 2 or task_id == 4:\n        return 1e-6\n    elif task_id == 32:\n        return 1e-4\n    return 0\n\n\nif __name__ == \"__main__\":\n    assert not HUMANEVAL_PLUS_PATH.exists(), f\"{HUMANEVAL_PLUS_PATH} already exists!\"\n\n    human_eval = get_human_eval()\n    with TempDir() as temp_dir:\n        tmp_file = os.path.join(temp_dir, HUMANEVAL_PLUS_PATH)\n        with open(tmp_file, \"w\") as writer:\n            for task in human_eval:\n                task_id = int(task[\"task_id\"].split(\"/\")[-1])\n                task[\"contract\"], task[\"canonical_solution\"] = get_contract_and_ref(\n                    task_id, task[\"entry_point\"]\n                )\n                task[\"base_input\"] = instrument_inputs(\n                    task[\"entry_point\"], task[\"prompt\"], task[\"test\"]\n                )\n                task[\"atol\"] = get_atol(task_id)\n                task[\"task_id\"] = task[\"task_id\"]\n\n                writer.write(json.dumps(task) + \"\\n\")\n        # move tmp_file to HUMANEVAL_PLUS_PATH\n        os.rename(tmp_file, HUMANEVAL_PLUS_PATH)", "\n\nif __name__ == \"__main__\":\n    assert not HUMANEVAL_PLUS_PATH.exists(), f\"{HUMANEVAL_PLUS_PATH} already exists!\"\n\n    human_eval = get_human_eval()\n    with TempDir() as temp_dir:\n        tmp_file = os.path.join(temp_dir, HUMANEVAL_PLUS_PATH)\n        with open(tmp_file, \"w\") as writer:\n            for task in human_eval:\n                task_id = int(task[\"task_id\"].split(\"/\")[-1])\n                task[\"contract\"], task[\"canonical_solution\"] = get_contract_and_ref(\n                    task_id, task[\"entry_point\"]\n                )\n                task[\"base_input\"] = instrument_inputs(\n                    task[\"entry_point\"], task[\"prompt\"], task[\"test\"]\n                )\n                task[\"atol\"] = get_atol(task_id)\n                task[\"task_id\"] = task[\"task_id\"]\n\n                writer.write(json.dumps(task) + \"\\n\")\n        # move tmp_file to HUMANEVAL_PLUS_PATH\n        os.rename(tmp_file, HUMANEVAL_PLUS_PATH)", ""]}
{"filename": "tools/humaneval/fix_v011.py", "chunked_list": ["def fix(data):\n    # fix 140 https://github.com/evalplus/evalplus/issues/3\n    assert data[140][\"task_id\"] == \"HumanEval/140\"\n    data[140][\"canonical_solution\"] = data[140][\"canonical_solution\"].replace(\n        \"range(len(text)-1, 2, -1)\", \"range(len(text), 2, -1)\"\n    )\n\n    # fix 75 https://github.com/evalplus/evalplus/issues/4\n    assert data[75][\"task_id\"] == \"HumanEval/75\"\n    org_contract = '\\n    assert type(a) == int, \"invalid inputs\" # $_CONTRACT_$\\n'\n    assert org_contract in data[75][\"contract\"]\n    data[75][\"contract\"] = (\n        org_contract + '        assert a < 100, \"invalid inputs\" # $_CONTRACT_$\\n'\n    )\n    data[75][\"base_input\"] = [x for x in data[75][\"base_input\"] if x[0] < 100]\n    data[75][\"plus_input\"] = [x for x in data[75][\"plus_input\"] if x[0] < 100]\n\n    # fix 129 https://github.com/evalplus/evalplus/issues/4\n    assert data[129][\"task_id\"] == \"HumanEval/129\"\n    data[129][\n        \"contract\"\n    ] = R\"\"\"\n    assert type(k) == int, \"invalid inputs\" # $_CONTRACT_$\n    assert k > 0, \"invalid inputs\" # $_CONTRACT_$\n    assert len(grid) >= 2, \"invalid inputs\" # $_CONTRACT_$\n    assert all(len(l) == len(grid) for l in grid), \"invalid inputs\" # $_CONTRACT_$\n    assert {x for l in grid for x in l} == set(range(1, len(grid) ** 2 + 1)), \"invalid inputs\" # $_CONTRACT_$\n\"\"\"\n\n    def check_unique(grid):\n        return {x for l in grid for x in l} == set(range(1, len(grid) ** 2 + 1))\n\n    data[129][\"base_input\"] = [x for x in data[129][\"base_input\"] if check_unique(x[0])]\n    data[129][\"plus_input\"] = [x for x in data[129][\"plus_input\"] if check_unique(x[0])]\n\n    return data", "\n\nif __name__ == \"__main__\":\n    import json\n\n    with open(\"HumanEvalPlus-v0.1.1.jsonl\") as f:\n        data = [json.loads(line) for line in f.readlines() if line]\n\n    data = fix(data)\n    with open(\"HumanEvalPlus-v0.1.2.jsonl\", \"wb\") as f:\n        for x in data:\n            f.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))\n\n    with open(\"HumanEvalPlus-Mini-v0.1.1.jsonl\") as f:\n        data = [json.loads(line) for line in f.readlines() if line]\n\n    data = fix(data)\n    with open(\"HumanEvalPlus-Mini-v0.1.2.jsonl\", \"wb\") as f:\n        for x in data:\n            f.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))", ""]}
{"filename": "tools/humaneval/check_ground_truth.py", "chunked_list": ["\"\"\"This script checks:\n1. Independence of \"contract\" and \"canonical_solution\" in groundtruth. (i.e., it should work without the \"contract\" part)\n\"\"\"\n\nimport multiprocessing as mp\nimport pathlib\n\nfrom rich.progress import track\n\nfrom evalplus.data import get_human_eval_plus", "\nfrom evalplus.data import get_human_eval_plus\n\nif __name__ == \"__main__\":\n    human_eval_plus = get_human_eval_plus().values()\n\n    for i, task in track(enumerate(human_eval_plus)):\n        fname = (\n            pathlib.Path(__file__).parent.parent\n            / \"groundtruth\"\n            / \"humaneval\"\n            / (str(i).zfill(3) + \"_\" + task[\"entry_point\"] + \".py\")\n        )\n        print(fname)\n        code = open(fname, \"r\").read()\n        if task[\"contract\"]:\n            assert task[\"contract\"] in code\n            code = code.replace(task[\"contract\"], \"\\n\")\n\n        # run the code in a subprocess\n        p = mp.Process(target=exec, args=(code, globals()))\n        p.start()\n        p.join(timeout=2)\n        assert not p.is_alive(), f\"Timeout for {fname}!\"\n        p.terminate()\n        p.join()\n        assert p.exitcode == 0, f\"Error for {fname}! {code}\"", ""]}
{"filename": "tools/humaneval/fix_v013.py", "chunked_list": ["def check_id(data, task_id):\n    assert data[task_id][\"task_id\"] == f\"HumanEval/{task_id}\"\n\n\ndef fix(data):\n    check_id(data, 116)\n    data[116][\"contract\"] = (\n        '\\n    assert isinstance(arr, list), \"invalid inputs\" # $_CONTRACT_$'\n        + '\\n    assert all(isinstance(x, int) and x >= 0 for x in arr), \"invalid inputs\" # $_CONTRACT_$\\n'\n    )\n    data[116][\"plus_input\"] = [\n        l\n        for l in data[116][\"plus_input\"]\n        if isinstance(l[0], list) and all(isinstance(x, int) and x >= 0 for x in l[0])\n    ]\n\n    return data", "\n\nif __name__ == \"__main__\":\n    import json\n\n    with open(\"HumanEvalPlus-v0.1.3.jsonl\") as f:\n        data = [json.loads(line) for line in f.readlines() if line]\n\n    data = fix(data)\n    with open(\"HumanEvalPlus-v0.1.4.jsonl\", \"wb\") as f:\n        for x in data:\n            f.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))\n\n    with open(\"HumanEvalPlus-Mini-v0.1.3.jsonl\") as f:\n        data = [json.loads(line) for line in f.readlines() if line]\n\n    data = fix(data)\n    with open(\"HumanEvalPlus-Mini-v0.1.4.jsonl\", \"wb\") as f:\n        for x in data:\n            f.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))", ""]}
{"filename": "tools/humaneval/fix_v014.py", "chunked_list": ["import math\n\n\ndef check_id(data, task_id):\n    assert data[task_id][\"task_id\"] == f\"HumanEval/{task_id}\"\n\n\ndef poly(xs, x):\n    return sum([coeff * math.pow(x, i) for i, coeff in enumerate(xs)])\n", "\n\ndef check_valid(xs):\n    if not (isinstance(xs, list) and len(xs) > 0 and len(xs) % 2 == 0):\n        return False\n    if not all(type(x) == int for x in xs):\n        return False\n    dxs = [xs[i] * i for i in range(1, len(xs))]\n\n    def func(x):\n        return poly(xs, x)\n\n    def derivative(x):\n        return poly(dxs, x)\n\n    x, tol = 0, 1e-5\n    for _ in range(1000):\n        fx = func(x)\n        dfx = derivative(x)\n        if abs(fx) < tol:\n            break\n        x = x - fx / dfx\n    if abs(poly(xs, x)) >= tol:\n        return False\n    return True", "\n\ndef fix(data):\n    check_id(data, 32)\n    data[32][\"contract\"] = (\n        '\\n    assert isinstance(xs, list) and len(xs) > 0 and len(xs) % 2 == 0, \"invalid inputs\" # $_CONTRACT_$'\n        + '\\n    assert all(type(x) == int for x in xs), \"invalid inputs\" # $_CONTRACT_$'\n        + \"\\n    dxs = [xs[i] * i for i in range(1, len(xs))] # $_CONTRACT_$\"\n        + \"\\n    def func(x): # $_CONTRACT_$\"\n        + \"\\n        return poly(xs, x) # $_CONTRACT_$\"\n        + \"\\n    def derivative(x): # $_CONTRACT_$\"\n        + \"\\n        return poly(dxs, x) # $_CONTRACT_$\"\n        + \"\\n    x, tol = 0, 1e-5 # $_CONTRACT_$\"\n        + \"\\n    for _ in range(1000): # $_CONTRACT_$\"\n        + \"\\n        fx = func(x) # $_CONTRACT_$\"\n        + \"\\n        dfx = derivative(x) # $_CONTRACT_$\"\n        + \"\\n        if abs(fx) < tol: break # $_CONTRACT_$\"\n        + \"\\n        x = x - fx / dfx # $_CONTRACT_$\"\n        + '\\n    assert abs(poly(xs, x)) < tol, \"invalid inputs\" # $_CONTRACT_$\\n'\n    )\n    data[32][\"plus_input\"] = [l for l in data[32][\"plus_input\"] if check_valid(l[0])]\n\n    return data", "\n\nif __name__ == \"__main__\":\n    import json\n\n    with open(\"HumanEvalPlus-v0.1.4.jsonl\") as f:\n        data = [json.loads(line) for line in f.readlines() if line]\n\n    data = fix(data)\n\n    with open(\"HumanEvalPlus-v0.1.5.jsonl\", \"wb\") as f:\n        for x in data:\n            f.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))\n\n    with open(\"HumanEvalPlus-Mini-v0.1.4.jsonl\") as f:\n        data = [json.loads(line) for line in f.readlines() if line]\n\n    data = fix(data)\n    with open(\"HumanEvalPlus-Mini-v0.1.5.jsonl\", \"wb\") as f:\n        for x in data:\n            f.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))", ""]}
{"filename": "tools/humaneval/fix_v012.py", "chunked_list": ["def check_id(data, task_id):\n    assert data[task_id][\"task_id\"] == f\"HumanEval/{task_id}\"\n\n\ndef fix(data):\n    # fix 53 https://github.com/evalplus/evalplus/issues/8\n    check_id(data, 53)\n    data[53][\"contract\"] = (\n        '\\n    assert isinstance(x, int), \"invalid inputs\" # $_CONTRACT_$'\n        + '\\n    assert isinstance(y, int), \"invalid inputs\" # $_CONTRACT_$\\n'\n    )\n    data[53][\"plus_input\"] = [\n        x\n        for x in data[53][\"plus_input\"]\n        if isinstance(x[0], int) and isinstance(x[1], int)\n    ]\n\n    # fix 0\n    check_id(data, 0)\n    data[0][\"contract\"] = (\n        '\\n    assert isinstance(threshold, float) and threshold > 0, \"invalid inputs\" # $_CONTRACT_$'\n        + '\\n    assert isinstance(numbers, list), \"invalid inputs\" # $_CONTRACT_$'\n        + '\\n    assert all([isinstance(v, (int, float)) for v in numbers]), \"invalid inputs\" # $_CONTRACT_$\\n'\n    )\n    data[0][\"plus_input\"] = [\n        x\n        for x in data[0][\"base_input\"]\n        if isinstance(x[1], float) and x[1] > 0 and isinstance(x[0], list)\n    ]\n\n    # fix 3\n    check_id(data, 3)\n    data[3][\"contract\"] = (\n        '\\n    assert type(operations) == list, \"invalid inputs\" # $_CONTRACT_$'\n        + '\\n    assert all([isinstance(v, int) for v in operations]), \"invalid inputs\" # $_CONTRACT_$\\n'\n    )\n    data[3][\"plus_input\"] = [x for x in data[3][\"base_input\"] if isinstance(x[0], list)]\n\n    # fix 9\n    check_id(data, 9)\n    data[9][\"contract\"] = (\n        '\\n    assert isinstance(numbers, list), \"invalid inputs\" # $_CONTRACT_$'\n        + '\\n    assert all([isinstance(v, int) for v in numbers]), \"invalid inputs\" # $_CONTRACT_$\\n'\n    )\n    data[9][\"plus_input\"] = [x for x in data[9][\"base_input\"] if isinstance(x[0], list)]\n\n    # fix 148\n    check_id(data, 148)\n    data[148][\n        \"contract\"\n    ] = '\\n    assert isinstance(planet1, str) and isinstance(planet2, str), \"invalid inputs\" # $_CONTRACT_$\\n'\n    data[148][\"plus_input\"] = [\n        x\n        for x in data[148][\"base_input\"]\n        if isinstance(x[0], str) and isinstance(x[1], str)\n    ]\n\n    # minor format fix 75\n    check_id(data, 75)\n    data[75][\"contract\"] = (\n        '\\n    assert type(a) == int, \"invalid inputs\" # $_CONTRACT_$'\n        + '\\n    assert a < 100, \"invalid inputs\" # $_CONTRACT_$\\n'\n    )\n\n    return data", "\n\nif __name__ == \"__main__\":\n    import json\n\n    with open(\"HumanEvalPlus-v0.1.2.jsonl\") as f:\n        data = [json.loads(line) for line in f.readlines() if line]\n\n    data = fix(data)\n    with open(\"HumanEvalPlus-v0.1.3.jsonl\", \"wb\") as f:\n        for x in data:\n            f.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))\n\n    with open(\"HumanEvalPlus-Mini-v0.1.2.jsonl\") as f:\n        data = [json.loads(line) for line in f.readlines() if line]\n\n    data = fix(data)\n    with open(\"HumanEvalPlus-Mini-v0.1.3.jsonl\", \"wb\") as f:\n        for x in data:\n            f.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))", ""]}
{"filename": "tools/_experimental/topset_distill.py", "chunked_list": ["import json\nimport os\n\nimport numpy as np\n\nfrom evalplus.data import get_human_eval_plus, get_human_eval_plus_inputs\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--root\", type=str, default=\"/JawTitan/EvalPlus/humaneval\")\n    args = parser.parse_args()\n\n    plus_inputs = get_human_eval_plus_inputs()\n    problems = get_human_eval_plus().values()\n\n    base_bvs = {}\n    plus_bvs = {}\n    id2idx = {}\n\n    for i, problem in enumerate(problems):\n        task_id = problem[\"task_id\"]\n        id2idx[task_id] = i\n        base_bvs[task_id] = np.zeros(len(problem[\"base_input\"]), dtype=bool)\n        plus_bvs[task_id] = np.zeros(len(plus_inputs[task_id]), dtype=bool)\n\n    for path in os.listdir(args.root):\n        eval_json_path = os.path.join(args.root, path, \"eval_results.json\")\n        if not os.path.isfile(eval_json_path) or not path[-1].isdigit():\n            print(f\"skip {path}\")\n            continue\n        res = json.load(open(eval_json_path, \"r\"))[\"eval\"]\n\n        for task_id, v in res.items():\n            for status, details in v[\"base\"]:\n                if details is None:  # all fail => skip\n                    continue\n                fails = np.logical_not(details)\n                base_bvs[task_id][: len(details)] = np.logical_xor(\n                    base_bvs[task_id][: len(details)], fails\n                )\n            for status, details in v[\"plus\"]:\n                if details is None:\n                    continue\n                fails = np.logical_not(details)\n                plus_bvs[task_id][: len(details)] = np.logical_xor(\n                    plus_bvs[task_id][: len(details)], fails\n                )\n\n    testsuite = []\n\n    new_sizes = []\n    for task_id, bbv in base_bvs.items():\n        new_inputs = []\n        idx = id2idx[task_id]\n        for i in np.nonzero(bbv)[0]:\n            new_inputs.append(problems[idx][\"base_input\"][i])\n        pbv = plus_bvs[task_id]\n        for i in np.nonzero(pbv)[0]:\n            new_inputs.append(plus_inputs[task_id][i])\n        testsuite.append({\"task_id\": task_id, \"inputs\": new_inputs})\n        print(\n            task_id, f\" org base {len(bbv)}; org plus {len(pbv)}; new {len(new_inputs)}\"\n        )\n        new_sizes.append(len(new_inputs))\n\n    new_sizes = np.array(new_sizes)\n    print(f\"{new_sizes.mean() = }, {new_sizes.min() = }, {new_sizes.max() = }\")", ""]}
{"filename": "tools/_experimental/set_cover.py", "chunked_list": ["import json\nimport os\n\nfrom rich.progress import track\n\nfrom evalplus.data import get_human_eval_plus, get_human_eval_plus_inputs\n\nLLM_HOME_PATH = \"/JawTitan/EvalPlus/humaneval\"\nmodel_paths = os.listdir(LLM_HOME_PATH)\n", "model_paths = os.listdir(LLM_HOME_PATH)\n\nproblems = get_human_eval_plus().values()\nnew_inputs = get_human_eval_plus_inputs()\ncover_info = {f\"HumanEval_{i}\": {} for i in range(164)}\n\n\n# One dict is super huge, so split them into separate JSON files\ndef get_cover_info():\n    for model_path in track(model_paths, description=\"Collecting sets...\"):\n        if not model_path[-1].isdigit():\n            continue\n        eval_json_path = os.path.join(LLM_HOME_PATH, model_path, \"eval_results.json\")\n        if not os.path.exists(eval_json_path):\n            continue\n        with open(eval_json_path, \"r\") as f:\n            res = json.load(f)[\"eval\"]\n            for task_id, v in res.items():\n                for i_code, (status, res_list) in enumerate(v[\"base\"]):\n                    if status == \"success\":\n                        continue\n                    code_id = hash(v[\"files\"][i_code])\n                    for i_test, res in enumerate(res_list):\n                        test_id = f\"base_{i_test}\"\n                        if res == False:\n                            cover_info[task_id].setdefault(test_id, []).append(code_id)\n                for i_code, (status, res_list) in enumerate(v[\"plus\"]):\n                    if status == \"success\":\n                        continue\n                    code_id = hash(v[\"files\"][i_code])\n                    for i_test, res in enumerate(res_list):\n                        test_id = f\"plus_{i_test}\"\n                        if res == False:\n                            cover_info[task_id].setdefault(test_id, []).append(code_id)", "def get_cover_info():\n    for model_path in track(model_paths, description=\"Collecting sets...\"):\n        if not model_path[-1].isdigit():\n            continue\n        eval_json_path = os.path.join(LLM_HOME_PATH, model_path, \"eval_results.json\")\n        if not os.path.exists(eval_json_path):\n            continue\n        with open(eval_json_path, \"r\") as f:\n            res = json.load(f)[\"eval\"]\n            for task_id, v in res.items():\n                for i_code, (status, res_list) in enumerate(v[\"base\"]):\n                    if status == \"success\":\n                        continue\n                    code_id = hash(v[\"files\"][i_code])\n                    for i_test, res in enumerate(res_list):\n                        test_id = f\"base_{i_test}\"\n                        if res == False:\n                            cover_info[task_id].setdefault(test_id, []).append(code_id)\n                for i_code, (status, res_list) in enumerate(v[\"plus\"]):\n                    if status == \"success\":\n                        continue\n                    code_id = hash(v[\"files\"][i_code])\n                    for i_test, res in enumerate(res_list):\n                        test_id = f\"plus_{i_test}\"\n                        if res == False:\n                            cover_info[task_id].setdefault(test_id, []).append(code_id)", "\n\nif __name__ == \"__main__\":\n    get_cover_info()\n    for i in track(range(164), description=\"Solving set covering...\"):\n        task_id = f\"HumanEval_{i}\"\n        tests = cover_info[task_id]\n        q, U = [], set()\n        for test_name, test_cover in tests.items():\n            cover_set = set(test_cover)\n            q.append((test_name, cover_set))\n            U = U.union(cover_set)\n        # Greedy\n        min_cover = []\n        while len(U) > 0:\n            max_uncover_set, max_test_name = {}, \"\"\n            for test_name, cover_set in q:\n                if len(cover_set) > len(max_uncover_set):\n                    max_uncover_set = cover_set\n                    max_test_name = test_name\n            min_cover.append(max_test_name)\n            U = U - max_uncover_set\n            qq = []\n            for test_name, cover_set in q:\n                new_cover_set = U.intersection(cover_set)\n                if len(new_cover_set) != 0:\n                    qq.append((test_name, new_cover_set))\n            q = qq\n\n        d = {\"task_id\": task_id, \"inputs\": []}\n        for test in min_cover:\n            tmp = test.split(\"_\")\n            t, n = tmp[0], int(tmp[1])\n            if t == \"base\":\n                d[\"inputs\"].append(problems[i][\"base_input\"][n])\n            else:\n                print(task_id, n)\n                d[\"inputs\"].append(new_inputs[task_id][n])\n        with open(\"HumanEvalPlusInputsMin.jsonl\", \"a\") as f:\n            f.write(json.dumps(d) + \"\\n\")", ""]}
{"filename": "codegen/model.py", "chunked_list": ["import os\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom warnings import warn\n\n# Communism\nos.environ[\"HF_HOME\"] = os.environ.get(\"HF_HOME\", \"/JawTitan/huggingface/\")\n\nimport openai\n", "import openai\n\n# ==============================================================\n# # The vicuna-7b weights are at /ColossalTitan/vicuna/vicuna-7b\n# Made by running:\n# ```\n# python3 -m fastchat.model.apply_delta \\\n#     --base /ColossalTitan/llama/converted_hf_7B \\\n#     --target /ColossalTitan/vicuna/vicuna-7b \\\n#     --delta lmsys/vicuna-7b-delta-v1.1", "#     --target /ColossalTitan/vicuna/vicuna-7b \\\n#     --delta lmsys/vicuna-7b-delta-v1.1\n# ```\n# ==============================================================\n# The vicuna-13b weights are at /ColossalTitan/vicuna/vicuna-13b\n# Made by running:\n# ```\n# python3 -m fastchat.model.apply_delta \\\n#     --base /ColossalTitan/llama/converted_hf_13B \\\n#     --target /ColossalTitan/vicuna/vicuna-13b \\", "#     --base /ColossalTitan/llama/converted_hf_13B \\\n#     --target /ColossalTitan/vicuna/vicuna-13b \\\n#     --delta lmsys/vicuna-13b-delta-v1.1\n# ```\n# ==============================================================\n# Acknoledgement:\n# Modified from https://github.com/lm-sys/FastChat/blob/main/fastchat/serve/huggingface_api.py\nimport torch\nfrom fastchat.serve.inference import load_model\nfrom transformers import (", "from fastchat.serve.inference import load_model\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    StoppingCriteria,\n    StoppingCriteriaList,\n)\n\nfrom evalplus.gen.util.api_request import create_chatgpt_config, request_chatgpt_engine\n", "from evalplus.gen.util.api_request import create_chatgpt_config, request_chatgpt_engine\n\nHUMANEVAL_EOS = [\"\\nclass\", \"\\ndef\", \"\\n#\", \"\\n@\", \"\\nprint\", \"\\nif\"]\nNON_CODE_EOS = [\"<|endoftext|>\", \"\\n```\", \"\\n</s>\", \"<|endofmask|>\"]\nEOS = HUMANEVAL_EOS + NON_CODE_EOS\n\n\n# Adopted from https://github.com/huggingface/transformers/pull/14897\nclass EndOfFunctionCriteria(StoppingCriteria):\n    def __init__(self, start_length, eos, tokenizer, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_length = start_length\n        self.eos = eos\n        self.tokenizer = tokenizer\n        self.end_length = {}\n\n    def __call__(self, input_ids, scores, **kwargs):\n        \"\"\"Returns true if all generated sequences contain any of the end-of-function strings.\"\"\"\n        decoded_generations = self.tokenizer.batch_decode(\n            input_ids[:, self.start_length :]\n        )\n        done = []\n        for index, decoded_generation in enumerate(decoded_generations):\n            finished = any(\n                [stop_string in decoded_generation for stop_string in self.eos]\n            )\n            if (\n                finished and index not in self.end_length\n            ):  # ensures first time we see it\n                for stop_string in self.eos:\n                    if stop_string in decoded_generation:\n                        self.end_length[index] = len(\n                            input_ids[\n                                index,  # get length of actual generation\n                                self.start_length : -len(\n                                    self.tokenizer.encode(\n                                        stop_string,\n                                        add_special_tokens=False,\n                                        return_tensors=\"pt\",\n                                    )[0]\n                                ),\n                            ]\n                        )\n            done.append(finished)\n        return all(done)", "class EndOfFunctionCriteria(StoppingCriteria):\n    def __init__(self, start_length, eos, tokenizer, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.start_length = start_length\n        self.eos = eos\n        self.tokenizer = tokenizer\n        self.end_length = {}\n\n    def __call__(self, input_ids, scores, **kwargs):\n        \"\"\"Returns true if all generated sequences contain any of the end-of-function strings.\"\"\"\n        decoded_generations = self.tokenizer.batch_decode(\n            input_ids[:, self.start_length :]\n        )\n        done = []\n        for index, decoded_generation in enumerate(decoded_generations):\n            finished = any(\n                [stop_string in decoded_generation for stop_string in self.eos]\n            )\n            if (\n                finished and index not in self.end_length\n            ):  # ensures first time we see it\n                for stop_string in self.eos:\n                    if stop_string in decoded_generation:\n                        self.end_length[index] = len(\n                            input_ids[\n                                index,  # get length of actual generation\n                                self.start_length : -len(\n                                    self.tokenizer.encode(\n                                        stop_string,\n                                        add_special_tokens=False,\n                                        return_tensors=\"pt\",\n                                    )[0]\n                                ),\n                            ]\n                        )\n            done.append(finished)\n        return all(done)", "\n\nclass DecoderBase(ABC):\n    def __init__(\n        self,\n        name: str,\n        batch_size: int = 1,\n        temperature: float = 0.8,\n        max_new_tokens: int = 512,\n    ) -> None:\n        print(\"Initializing a decoder model: {} ...\".format(name))\n        self.name = name\n        self.batch_size = batch_size\n        self.temperature = temperature\n        self.eos = EOS\n        self.skip_special_tokens = False\n        self.max_new_tokens = max_new_tokens\n\n    @abstractmethod\n    def codegen(\n        self, prompt: str, do_sample: bool = True, num_samples: int = 200\n    ) -> List[str]:\n        pass\n\n    def __repr__(self) -> str:\n        return self.name\n\n    def __str__(self) -> str:\n        return self.name", "\n\nclass OpenAIDecoder(DecoderBase):\n    def __init__(\n        self, name: str, batch_size: int = 1, temperature: float = 0.8\n    ) -> None:\n        super().__init__(name, batch_size, temperature)\n        openai.api_key = os.environ.get(\"OPENAI_API_KEY\", \"dummy\")\n        FAUXIPILOT_ADDR = None\n        if name == \"codegen-16b\":\n            FAUXIPILOT_ADDR = \"http://127.0.0.1:5000/v1\"\n        elif name == \"codegen-6b\":\n            FAUXIPILOT_ADDR = \"http://127.0.0.1:5010/v1\"\n        openai.api_base = os.environ.get(\"OPENAI_API_BASE\", FAUXIPILOT_ADDR)\n\n    def codegen(\n        self, prompt: str, do_sample: bool = True, num_samples: int = 200\n    ) -> List[str]:\n        if do_sample:\n            assert self.temperature > 0, \"Temperature must be greater than 0!\"\n        batch_size = min(self.batch_size, num_samples)\n\n        ret = openai.Completion.create(\n            model=\"fastertransformer\",\n            prompt=prompt,\n            max_tokens=self.max_new_tokens,\n            temperature=self.temperature,\n            n=batch_size,\n            top_p=0.95,\n            stop=EOS,\n        )\n\n        # assert the ret are not empty\n        assert len(ret[\"choices\"]) > 0, \"OpenAI API returns empty results!\"\n\n        # process the output and return\n        return [x[\"text\"] for x in ret[\"choices\"]]", "\n\nclass HFTorchDecoder(DecoderBase):\n    def __init__(self, name: str, batch_size: int = 1, temperature: float = 0.8):\n        super().__init__(name=name, batch_size=batch_size, temperature=temperature)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        kwargs = {\n            \"trust_remote_code\": name\n            in {\n                \"bigcode/santacoder\",\n                \"Salesforce/codegen2-1B\",\n                \"Salesforce/codegen2-3_7B\",\n                \"Salesforce/codegen2-7B\",\n                \"Salesforce/codegen2-16B\",\n            }\n        }\n        if \"codegen-\" in name:  # use fp16 for codegen models\n            kwargs[\"torch_dtype\"] = torch.float16\n        if \"codegen2-\" in name:  # avoid warning of trust remote code\n            kwargs[\"revision\"] = \"main\"\n            kwargs[\"torch_dtype\"] = torch.float16\n            if \"16b\" in name.lower():\n                kwargs[\"device_map\"] = \"auto\"\n                # Not working... # int8 acceleration\n                # kwargs[\"load_in_8bit\"] = True\n        if \"starcoder\" in name:\n            kwargs[\"torch_dtype\"] = torch.bfloat16\n\n        self.tokenizer = AutoTokenizer.from_pretrained(name)\n        self.model = AutoModelForCausalLM.from_pretrained(name, **kwargs)\n        if name in {\"StabilityAI/stablelm-base-alpha-7b\"}:\n            print(\"Switching to float16 ...\")\n            self.model = self.model.half()\n            self.skip_special_tokens = True\n        self.model = self.model.to(self.device)\n\n    # Assumption is that all inputs should probably fit under maximum context. but can add a checking function\n    # just in case. TODO: think about\n    @torch.inference_mode()\n    def codegen(\n        self, prompt: str, do_sample: bool = True, num_samples: int = 200\n    ) -> List[str]:\n        input_tokens = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(\n            self.device\n        )\n        scores = StoppingCriteriaList(\n            [\n                EndOfFunctionCriteria(\n                    start_length=len(input_tokens[0]),\n                    eos=self.eos,\n                    tokenizer=self.tokenizer,\n                )\n            ]\n        )\n        raw_outputs = self.model.generate(\n            input_tokens,\n            max_new_tokens=self.max_new_tokens,\n            stopping_criteria=scores,\n            do_sample=do_sample,\n            top_p=0.95,\n            top_k=None,\n            temperature=self.temperature,\n            output_scores=True,\n            return_dict_in_generate=True,\n            num_return_sequences=min(self.batch_size, num_samples),\n            pad_token_id=self.tokenizer.eos_token_id,\n        )  # remove warning\n        gen_seqs = raw_outputs.sequences[:, len(input_tokens[0]) :]\n        gen_strs = self.tokenizer.batch_decode(\n            gen_seqs, skip_special_tokens=self.skip_special_tokens\n        )\n        outputs = []\n        # removes eos tokens.\n        for output in gen_strs:\n            min_index = 10000\n            for eos in self.eos:\n                if eos in output:\n                    # could be multiple eos in outputs, better pick minimum one\n                    min_index = min(min_index, output.index(eos))\n            outputs.append(output[:min_index])\n        return outputs", "\n\nclass FsChatDecoder(HFTorchDecoder):\n    def __init__(self, name: str, batch_size: int = 1, temperature: float = 0.8):\n        DecoderBase.__init__(\n            self, name=name, batch_size=batch_size, temperature=temperature\n        )\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model, self.tokenizer = load_model(\n            f\"/ColossalTitan/vicuna/{name}\",\n            device=\"cuda\",\n            num_gpus=1,\n            load_8bit=False,\n            debug=False,\n        )", "\n\nclass ChatGPTDecoder(DecoderBase):\n    def __init__(\n        self,\n        name: str,\n        batch_size: int = 1,\n        temperature: float = 0.8,\n        model_name: str = \"gpt-3.5-turbo\",\n    ) -> None:\n        super().__init__(name, batch_size, temperature)\n        self.model_name = model_name\n        openai.api_key = os.environ.get(\"OPENAI_API_KEY\", \"dummy\")\n\n    @staticmethod\n    def _find_gen_func_sig(prompt):\n        func_sig = \"\"\n        for x in prompt.splitlines():\n            if x.startswith(\"def \") and x.endswith(\":\"):\n                # always pick the last one, since there could pre-defined functions.\n                func_sig = x\n        return func_sig\n\n    @staticmethod\n    def _remove_eos(gen):\n        min_index = 1000\n        for eos in EOS:\n            if eos in gen:\n                min_index = min(min_index, gen.index(eos))\n        return gen[:min_index]\n\n    def _chatgpt_parse(self, ret, prompt):\n        outputs = []\n        for returns in ret[\"choices\"]:\n            raw_o = returns[\"message\"][\"content\"]\n            if \"```\" in raw_o:\n                gen = raw_o.split(\"```\")[1].strip()\n                if gen.startswith(\"python\"):\n                    gen = gen[len(\"python\") :].strip()\n                if gen.startswith(prompt.strip()):\n                    suf = gen.split(prompt.strip())[-1]\n                    suf = self._remove_eos(suf)\n                    gen = prompt.strip() + suf\n                elif self._find_gen_func_sig(prompt) in gen:\n                    # same function sign is in the prompt\n                    sig = self._find_gen_func_sig(prompt)\n                    pre, suf = gen.split(sig)[0], gen.split(sig)[-1]\n                    suf = self._remove_eos(suf)\n                    gen = pre + sig + suf\n                else:\n                    gen = f\"# CANNOT PARSE CODE SNIPPET\\n{gen}\"\n            else:\n                # cannot really handle parse just dump to file and maybe process later.\n                gen = f\"# CANNOT PARSE\\n{raw_o}\"\n            outputs.append(gen)\n        return outputs\n\n    def codegen(\n        self, prompt: str, do_sample: bool = True, num_samples: int = 200\n    ) -> List[str]:\n        if do_sample:\n            assert self.temperature > 0, \"Temperature must be positive for sampling\"\n\n        batch_size = min(self.batch_size, num_samples)\n        assert batch_size <= 20, \"Use larger batch size could blow up the memory!\"\n\n        # construct prompt\n        message = (\n            f\"Please complete the following code snippet.\\n```\\n{prompt.strip()}\\n```\"\n        )\n        config = create_chatgpt_config(\n            message=message,\n            # max_tokens = 512, # for regular generation\n            max_tokens=1024,\n            temperature=self.temperature,\n            batch_size=batch_size,\n            model=self.model_name,\n        )\n        ret = request_chatgpt_engine(config)\n        return self._chatgpt_parse(ret, prompt.strip())", "\n\nclass IncoderDecoder(HFTorchDecoder):\n    def __init__(\n        self, name: str, batch_size: int = 1, temperature: float = 0.8\n    ) -> None:\n        super().__init__(name, batch_size, temperature)\n        self.infill_ph = \"<|mask:0|>\"\n        self.extra_end = \"<|mask:1|><|mask:0|>\"\n        self.extra_eos = [\n            \"<|endofmask|>\",\n            \"<|/ file\",\n            \"</cell>\",\n            \"</text>\",\n            \"</code>\",\n            \"<|\",\n            \"</CODE>\",\n        ]\n        self.eos = self.eos + self.extra_eos\n\n    def codegen(\n        self, prompt: str, do_sample: bool = True, num_samples: int = 200\n    ) -> List[str]:\n        input = prompt + self.infill_ph + self.extra_end\n        input_tokens = self.tokenizer.encode(input, return_tensors=\"pt\").to(self.device)\n        scores = StoppingCriteriaList(\n            [\n                EndOfFunctionCriteria(\n                    start_length=len(input_tokens[0]),\n                    eos=self.eos,\n                    tokenizer=self.tokenizer,\n                )\n            ]\n        )\n        raw_outputs = self.model.generate(\n            input_tokens,\n            max_new_tokens=self.max_new_tokens,\n            stopping_criteria=scores,\n            do_sample=do_sample,\n            top_p=0.95,\n            top_k=None,\n            temperature=self.temperature,\n            num_return_sequences=min(self.batch_size, num_samples),\n            output_scores=True,\n            return_dict_in_generate=True,\n        )\n        gen_seqs = raw_outputs.sequences[:, len(input_tokens[0]) :]\n        gen_strs = self.tokenizer.batch_decode(\n            gen_seqs, skip_special_tokens=self.skip_special_tokens\n        )\n        outputs = []\n        # removes eos tokens.\n        for output in gen_strs:\n            min_index = 10000\n            for eos in self.eos:\n                if eos in output:\n                    min_index = min(min_index, output.index(eos))\n            outputs.append(output[:min_index])\n        return outputs", "\n\nclass Codegen2Decoder(HFTorchDecoder):\n    def __init__(\n        self, name: str, batch_size: int = 1, temperature: float = 0.8\n    ) -> None:\n        super().__init__(name, batch_size, temperature)\n        self.infill_ph = \"<mask_1>\"\n        # taken from: https://huggingface.co/Salesforce/codegen2-16B\n        self.extra_end = \"<|endoftext|><sep><mask_1>\"\n        self.extra_eos = [\"<eom>\"]\n        self.eos = self.eos + self.extra_eos\n\n    @torch.inference_mode()\n    def codegen(\n        self, prompt: str, do_sample: bool = True, num_samples: int = 200\n    ) -> List[str]:\n        input = prompt + self.infill_ph + self.extra_end\n        input_tokens = self.tokenizer.encode(input, return_tensors=\"pt\").to(self.device)\n        scores = StoppingCriteriaList(\n            [\n                EndOfFunctionCriteria(\n                    start_length=len(input_tokens[0]),\n                    eos=self.eos,\n                    tokenizer=self.tokenizer,\n                )\n            ]\n        )\n        raw_outputs = self.model.generate(\n            input_tokens,\n            max_new_tokens=self.max_new_tokens,\n            stopping_criteria=scores,\n            do_sample=do_sample,\n            top_p=0.95,\n            top_k=None,\n            temperature=self.temperature,\n            output_scores=True,\n            return_dict_in_generate=True,\n            num_return_sequences=min(self.batch_size, num_samples),\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n        gen_seqs = raw_outputs.sequences[:, len(input_tokens[0]) :]\n        gen_strs = self.tokenizer.batch_decode(\n            gen_seqs, skip_special_tokens=self.skip_special_tokens\n        )\n        outputs = []\n        # removes eos tokens.\n        for output in gen_strs:\n            min_index = 10000\n            for eos in self.eos:\n                if eos in output:\n                    min_index = min(min_index, output.index(eos))\n            outputs.append(output[:min_index])\n        return outputs", "\n\nclass SantaCoder(HFTorchDecoder):\n    def __init__(\n        self, name: str, batch_size: int = 1, temperature: float = 0.8\n    ) -> None:\n        super().__init__(name, batch_size, temperature)\n        self.prefix_token = \"<fim-prefix>\"\n        self.suffix_token = \"<fim-suffix>\\n<fim-middle>\"\n        self.extra_eos = [\"<|endofmask|>\"]\n        self.eos = self.eos + self.extra_eos\n\n    def codegen(\n        self, prompt: str, do_sample: bool = True, num_samples: int = 200\n    ) -> List[str]:\n        input = self.prefix_token + prompt + self.suffix_token\n        input_tokens = self.tokenizer.encode(input, return_tensors=\"pt\").to(self.device)\n        scores = StoppingCriteriaList(\n            [\n                EndOfFunctionCriteria(\n                    start_length=len(input_tokens[0]),\n                    eos=self.eos,\n                    tokenizer=self.tokenizer,\n                )\n            ]\n        )\n        raw_outputs = self.model.generate(\n            input_tokens,\n            max_new_tokens=self.max_new_tokens,\n            stopping_criteria=scores,\n            do_sample=do_sample,\n            top_p=0.95,\n            top_k=None,\n            temperature=self.temperature,\n            num_return_sequences=min(self.batch_size, num_samples),\n            output_scores=True,\n            return_dict_in_generate=True,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n        gen_seqs = raw_outputs.sequences[:, len(input_tokens[0]) :]\n        gen_strs = self.tokenizer.batch_decode(\n            gen_seqs,\n            skip_special_tokens=self.skip_special_tokens,\n            truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"],\n        )\n        outputs = []\n        # removes eos tokens.\n        for output in gen_strs:\n            min_index = 10000\n            for eos in self.eos:\n                if eos in output:\n                    min_index = min(min_index, output.index(eos))\n            outputs.append(output[:min_index])\n        return outputs", "\n\nclass StarCoder(HFTorchDecoder):\n    def __init__(\n        self, name: str, batch_size: int = 1, temperature: float = 0.8\n    ) -> None:\n        super().__init__(name, batch_size, temperature)\n        self.prefix_token = \"<fim_prefix>\"\n        self.suffix_token = \"<fim_suffix><fim_middle>\"\n\n    def codegen(\n        self, prompt: str, do_sample: bool = True, num_samples: int = 200\n    ) -> List[str]:\n        input = self.prefix_token + prompt + self.suffix_token\n        input_tokens = self.tokenizer.encode(input, return_tensors=\"pt\").to(self.device)\n        scores = StoppingCriteriaList(\n            [\n                EndOfFunctionCriteria(\n                    start_length=len(input_tokens[0]),\n                    eos=self.eos,\n                    tokenizer=self.tokenizer,\n                )\n            ]\n        )\n        temperature = max(self.temperature, 1e-2)\n        raw_outputs = self.model.generate(\n            input_tokens,\n            max_new_tokens=self.max_new_tokens,\n            stopping_criteria=scores,\n            do_sample=do_sample,\n            top_p=0.95,\n            top_k=None,\n            temperature=temperature,\n            num_return_sequences=min(self.batch_size, num_samples),\n            output_scores=True,\n            return_dict_in_generate=True,\n            repetition_penalty=1.0,\n            pad_token_id=self.tokenizer.eos_token_id,\n        )\n        gen_seqs = raw_outputs.sequences[:, len(input_tokens[0]) :]\n        gen_strs = self.tokenizer.batch_decode(\n            gen_seqs, skip_special_tokens=self.skip_special_tokens\n        )\n        outputs = []\n        # removes eos tokens.\n        for output in gen_strs:\n            min_index = 10000\n            for eos in self.eos:\n                if eos in output:\n                    min_index = min(min_index, output.index(eos))\n            outputs.append(output[:min_index])\n        return outputs", "\n\ndef make_model(name: str, batch_size: int = 1, temperature: float = 0.8):\n    if name == \"codegen-2b\":\n        return HFTorchDecoder(\n            batch_size=batch_size,\n            name=\"Salesforce/codegen-2B-mono\",\n            temperature=temperature,\n        )\n    elif name == \"codegen-6b\":\n        warn(\n            \"Using fauxipilot backend for codegen-6b by default. \"\n            \"If you wish to use huggingface backend go `codegen-6b-hf`\"\n        )\n        return OpenAIDecoder(\n            batch_size=batch_size, name=\"codegen-6b\", temperature=temperature\n        )\n    elif name == \"codegen-6b-hf\":\n        return HFTorchDecoder(\n            batch_size=batch_size,\n            name=\"Salesforce/codegen-6B-mono\",\n            temperature=temperature,\n        )\n    elif name == \"codegen-16b\":\n        return OpenAIDecoder(\n            batch_size=batch_size, name=\"codegen-16b\", temperature=temperature\n        )\n    elif name == \"codegen2-1b\":\n        return Codegen2Decoder(\n            batch_size=batch_size,\n            name=\"Salesforce/codegen2-1B\",\n            temperature=temperature,\n        )\n    elif name == \"codegen2-3b\":\n        return Codegen2Decoder(\n            batch_size=batch_size,\n            name=\"Salesforce/codegen2-3_7B\",\n            temperature=temperature,\n        )\n    elif name == \"codegen2-7b\":\n        return Codegen2Decoder(\n            batch_size=batch_size,\n            name=\"Salesforce/codegen2-7B\",\n            temperature=temperature,\n        )\n    elif name == \"codegen2-16b\":\n        warn(\n            \"codegen2-16b checkpoint is `unfinished` at this point (05/11/2023) according to their paper. \"\n            \"So it might not make sense to use it.\"\n        )\n        return Codegen2Decoder(\n            batch_size=batch_size,\n            name=\"Salesforce/codegen2-16B\",\n            temperature=temperature,\n        )\n    elif name == \"polycoder\":\n        return HFTorchDecoder(\n            batch_size=batch_size,\n            name=\"NinedayWang/PolyCoder-2.7B\",\n            temperature=temperature,\n        )\n    elif name == \"vicuna-7b\" or name == \"vicuna-13b\":\n        return FsChatDecoder(batch_size=batch_size, name=name, temperature=temperature)\n    elif name == \"santacoder\":\n        return SantaCoder(\n            batch_size=batch_size, name=\"bigcode/santacoder\", temperature=temperature\n        )\n    elif name == \"incoder-1b\":\n        return IncoderDecoder(\n            batch_size=batch_size, name=\"facebook/incoder-1B\", temperature=temperature\n        )\n    elif name == \"incoder-6b\":\n        return IncoderDecoder(\n            batch_size=batch_size, name=\"facebook/incoder-6B\", temperature=temperature\n        )\n    elif name == \"stablelm-7b\":\n        return HFTorchDecoder(\n            batch_size=batch_size,\n            name=\"StabilityAI/stablelm-base-alpha-7b\",\n            temperature=temperature,\n        )\n    elif name == \"chatgpt\":\n        return ChatGPTDecoder(\n            batch_size=batch_size,\n            name=\"ChatGPT\",\n            temperature=temperature,\n            model_name=\"gpt-3.5-turbo\",\n        )\n    elif name == \"gpt-4\":\n        return ChatGPTDecoder(\n            batch_size=batch_size,\n            name=\"GPT4\",\n            temperature=temperature,\n            model_name=\"gpt-4\",\n        )\n    elif name == \"gptneo-2b\":\n        return HFTorchDecoder(\n            batch_size=batch_size,\n            name=\"EleutherAI/gpt-neo-2.7B\",\n            temperature=temperature,\n        )\n    elif name == \"gpt-j\":\n        return HFTorchDecoder(\n            batch_size=batch_size, name=\"EleutherAI/gpt-j-6B\", temperature=temperature\n        )\n    elif name == \"starcoder\":\n        return StarCoder(\n            batch_size=batch_size, name=\"bigcode/starcoder\", temperature=temperature\n        )\n    raise ValueError(f\"Invalid model name: {name}\")", ""]}
{"filename": "codegen/generate.py", "chunked_list": ["import argparse\nimport os\nfrom os import PathLike\n\nfrom model import DecoderBase, make_model\nfrom rich.progress import (\n    BarColumn,\n    MofNCompleteColumn,\n    Progress,\n    TextColumn,", "    Progress,\n    TextColumn,\n    TimeElapsedColumn,\n)\n\nfrom evalplus.data import get_human_eval_plus\n\n\ndef construct_contract_prompt(prompt: str, contract_type: str, contract: str) -> str:\n    if contract_type == \"no\":\n        return prompt\n    elif contract_type == \"docstring\":\n        # embed within the docstring\n        sep = \"\"\n        if '\"\"\"' in prompt:\n            sep = '\"\"\"'\n        elif \"'''\" in prompt:\n            sep = \"'''\"\n        assert sep != \"\"\n        l = prompt.split(sep)\n        contract = \"\\n\".join([x.split(\"#\")[0] for x in contract.splitlines()])\n        l[1] = (\n            l[1] + contract + \"\\n\" + \" \" * (len(contract) - len(contract.lstrip()) - 1)\n        )\n        return sep.join(l)\n    elif contract_type == \"code\":\n        # at the beginning of the function\n        contract = \"\\n\".join([x.split(\"#\")[0] for x in contract.splitlines()])\n        return prompt + contract", "def construct_contract_prompt(prompt: str, contract_type: str, contract: str) -> str:\n    if contract_type == \"no\":\n        return prompt\n    elif contract_type == \"docstring\":\n        # embed within the docstring\n        sep = \"\"\n        if '\"\"\"' in prompt:\n            sep = '\"\"\"'\n        elif \"'''\" in prompt:\n            sep = \"'''\"\n        assert sep != \"\"\n        l = prompt.split(sep)\n        contract = \"\\n\".join([x.split(\"#\")[0] for x in contract.splitlines()])\n        l[1] = (\n            l[1] + contract + \"\\n\" + \" \" * (len(contract) - len(contract.lstrip()) - 1)\n        )\n        return sep.join(l)\n    elif contract_type == \"code\":\n        # at the beginning of the function\n        contract = \"\\n\".join([x.split(\"#\")[0] for x in contract.splitlines()])\n        return prompt + contract", "\n\ndef code_generate(args, workdir: PathLike, model: DecoderBase):\n    with Progress(\n        TextColumn(\n            f\"{args.dataset} \u2022\" + \"[progress.percentage]{task.percentage:>3.0f}%\"\n        ),\n        BarColumn(),\n        MofNCompleteColumn(),\n        TextColumn(\"\u2022\"),\n        TimeElapsedColumn(),\n    ) as p:\n        for task_id, task in p.track(get_human_eval_plus().items()):\n            p_name = task_id.replace(\"/\", \"_\")\n            if args.use_contracts != \"no\" and task[\"contract\"] == \"\":\n                continue\n            os.makedirs(os.path.join(workdir, p_name), exist_ok=True)\n            log = f\"Codegen: {p_name} @ {model}\"\n            n_existing = 0\n            if args.resume:\n                # count existing .py files\n                n_existing = len(\n                    [\n                        f\n                        for f in os.listdir(os.path.join(workdir, p_name))\n                        if f.endswith(\".py\")\n                    ]\n                )\n                if n_existing > 0:\n                    log += f\" (resuming from {n_existing})\"\n\n            nsamples = args.n_samples - n_existing\n            p.console.print(log)\n\n            sidx = args.n_samples - nsamples\n            while sidx < args.n_samples:\n                outputs = model.codegen(\n                    construct_contract_prompt(\n                        task[\"prompt\"], args.use_contracts, task[\"contract\"]\n                    ),\n                    do_sample=not args.greedy,\n                    num_samples=args.n_samples - sidx,\n                )\n                for impl in outputs:\n                    try:\n                        with open(\n                            os.path.join(workdir, p_name, f\"{sidx}.py\"),\n                            \"w\",\n                            encoding=\"utf-8\",\n                        ) as f:\n                            if args.model in {\"chatgpt\", \"gpt-4\"}:\n                                f.write(impl)\n                            else:\n                                f.write(task[\"prompt\"] + impl)\n                    except UnicodeEncodeError:\n                        continue\n                    sidx += 1", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", required=True, type=str)\n    parser.add_argument(\"--bs\", required=True, type=int)\n    parser.add_argument(\"--temperature\", required=True, type=float)\n    parser.add_argument(\"--dataset\", default=\"humaneval\", type=str)\n    parser.add_argument(\"--root\", default=\"/JawTitan/EvalPlus\", type=str)\n    parser.add_argument(\"--n_samples\", default=200, type=int)\n    parser.add_argument(\"--resume\", action=\"store_true\")\n    parser.add_argument(\"--use_contracts\", default=\"no\", type=str)\n    parser.add_argument(\"--greedy\", action=\"store_true\")\n    args = parser.parse_args()\n\n    if args.dataset not in [\"humaneval\"]:\n        raise NotImplementedError(\"Unsupported dataset: {}\".format(args.dataset))\n\n    if args.use_contracts not in [\"no\", \"code\", \"docstring\"]:\n        raise NotImplementedError(\n            \"Unsupported contract usage: {}\".format(args.use_contracts)\n        )\n    if args.greedy and (args.temperature != 0 or args.bs != 1 or args.n_samples != 1):\n        raise ValueError(\n            f\"Greedy decoding is only supported with temperature({args.temperature}) = 0, batch_size({args.bs}) = 1\"\n            f\" and n_samples({args.n_samples}) = 1\"\n        )\n\n    # Make project dir\n    os.makedirs(args.root, exist_ok=True)\n    # Make dataset dir\n    os.makedirs(os.path.join(args.root, args.dataset), exist_ok=True)\n    # Make dir for codes generated by each model\n    args.model = args.model.lower()\n    model = make_model(\n        name=args.model, batch_size=args.bs, temperature=args.temperature\n    )\n    workdir = os.path.join(\n        args.root,\n        args.dataset,\n        args.model\n        + f\"_temp_{args.temperature}\"\n        + (\"\" if args.use_contracts == \"no\" else f\"-contract-{args.use_contracts}\"),\n    )\n    os.makedirs(workdir, exist_ok=True)\n\n    with open(os.path.join(workdir, \"args.txt\"), \"w\") as f:\n        f.write(str(args))\n\n    code_generate(args, workdir=workdir, model=model)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "evalplus/inputgen.py", "chunked_list": ["\"\"\"Generate a .jsonl file where each line is a json object\nrepresenting a programming problem with a task ID (\"task_id\")\nand a list of enhanced inputs (\"inputs\") for that task.\n\"\"\"\n\nimport argparse\nimport json\nimport os\n\nfrom evalplus.gen.chatgpt_gen import ChatGPTGen", "\nfrom evalplus.gen.chatgpt_gen import ChatGPTGen\nfrom evalplus.gen.type_mut import TypedMutGen\n\n\nclass SetEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, set):\n            return list(obj)\n        return json.JSONEncoder.default(self, obj)", "\n\ndef input_generation(args, problems):\n    with open(args.output, \"w\") as file:\n        for problem in problems:\n            new_input = {}\n            task_id = problem[\"task_id\"]\n            print(f\"generating inputs for {task_id} ...\")\n            # by default we do not include constraints in the prompt\n            code = problem[\"prompt\"] + problem[\"canonical_solution\"]\n            c_code = (\n                problem[\"prompt\"] + problem[\"contract\"] + problem[\"canonical_solution\"]\n            )\n            # first generate chatgpt\n            input_gen = ChatGPTGen(\n                problem[\"base_input\"], problem[\"entry_point\"], c_code, code\n            ).generate(args.chatgpt_len)\n            # generate mutation next\n            input_gen.extend(\n                TypedMutGen(input_gen, problem[\"entry_point\"], c_code).generate(\n                    args.mut_len\n                )\n            )\n            print(f\"generated {len(input_gen)} inputs\")\n            new_input[\"task_id\"] = task_id\n            new_input[\"inputs\"] = input_gen\n            file.write(json.dumps(new_input, cls=SetEncoder) + \"\\n\")", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset\", required=True, type=str)\n    parser.add_argument(\"--chatgpt_len\", required=True, type=int)\n    parser.add_argument(\"--mut_len\", required=True, type=int)\n    parser.add_argument(\n        \"--output\", default=None, type=int, help=\"Output .jsonl file name.\"\n    )\n    args = parser.parse_args()\n\n    problems = None\n    if args.dataset == \"humaneval\":\n        from evalplus.data import get_human_eval_plus\n\n        # Allow it to be incomplete\n        problems = get_human_eval_plus(err_incomplete=False)\n        if args.output is None:\n            args.output = \"HumanEvalPlusInputs.jsonl\"\n\n    if problems is None:\n        raise NotImplementedError(f\"Unsupported dataset: {args.dataset}\")\n\n    assert os.path.isfile(args.output), f\"{args.output} already exists!\"\n    input_generation(args, problems)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "evalplus/evaluate.py", "chunked_list": ["import argparse\nimport json\nimport multiprocessing\nimport os\nimport pickle\nimport threading\nimport time\nfrom collections import Counter, defaultdict\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom datetime import datetime", "from concurrent.futures import ProcessPoolExecutor, as_completed\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom evalplus.data import (\n    CACHE_DIR,\n    get_human_eval_plus,", "    CACHE_DIR,\n    get_human_eval_plus,\n    get_human_eval_plus_hash,\n    load_solutions,\n)\nfrom evalplus.eval import (\n    SUCCESS,\n    compatible_eval_result,\n    estimate_pass_at_k,\n    untrusted_check,", "    estimate_pass_at_k,\n    untrusted_check,\n)\nfrom evalplus.gen.util import trusted_exec\n\n# 1st item: the status\n# 2nd item (optional): the detailed pass/fail boolean for each input\nResult = Tuple[str, List[bool]]\n\n\ndef get_groundtruth(problems, hashcode):\n    cache_file = os.path.join(CACHE_DIR, f\"{hashcode}.pkl\")\n    if os.path.exists(cache_file):\n        print(f\"Load from ground-truth from {cache_file}\")\n        with open(cache_file, \"rb\") as f:\n            return pickle.load(f)\n\n    print(\"Computing expected output...\")\n    tbegin = time.time()\n    expected_output = {}\n    for task_id, problem in problems.items():\n        oracle = {}\n        oracle[\"base\"], oracle[\"base_time\"] = trusted_exec(\n            problem[\"prompt\"] + problem[\"canonical_solution\"],\n            problem[\"base_input\"],\n            problem[\"entry_point\"],\n            record_time=True,\n        )\n\n        oracle[\"plus\"], oracle[\"plus_time\"] = trusted_exec(\n            problem[\"prompt\"] + problem[\"canonical_solution\"],\n            problem[\"plus_input\"],\n            problem[\"entry_point\"],\n            record_time=True,\n        )\n        expected_output[task_id] = oracle\n    print(f\"Expected outputs computed in {time.time() - tbegin:.2f}s\")\n\n    with open(cache_file, \"wb\") as f:\n        pickle.dump(expected_output, f)\n\n    return expected_output", "\n\ndef get_groundtruth(problems, hashcode):\n    cache_file = os.path.join(CACHE_DIR, f\"{hashcode}.pkl\")\n    if os.path.exists(cache_file):\n        print(f\"Load from ground-truth from {cache_file}\")\n        with open(cache_file, \"rb\") as f:\n            return pickle.load(f)\n\n    print(\"Computing expected output...\")\n    tbegin = time.time()\n    expected_output = {}\n    for task_id, problem in problems.items():\n        oracle = {}\n        oracle[\"base\"], oracle[\"base_time\"] = trusted_exec(\n            problem[\"prompt\"] + problem[\"canonical_solution\"],\n            problem[\"base_input\"],\n            problem[\"entry_point\"],\n            record_time=True,\n        )\n\n        oracle[\"plus\"], oracle[\"plus_time\"] = trusted_exec(\n            problem[\"prompt\"] + problem[\"canonical_solution\"],\n            problem[\"plus_input\"],\n            problem[\"entry_point\"],\n            record_time=True,\n        )\n        expected_output[task_id] = oracle\n    print(f\"Expected outputs computed in {time.time() - tbegin:.2f}s\")\n\n    with open(cache_file, \"wb\") as f:\n        pickle.dump(expected_output, f)\n\n    return expected_output", "\n\ndef check_correctness(\n    completion_id: int,\n    problem: Dict[str, Any],\n    solution: str,\n    expected_output: Dict[str, List],\n    base_only=False,\n    fast_check=False,\n    identifier=None,\n    min_time_limit: float = 0.1,\n    gt_time_limit_factor: float = 2.0,\n) -> Dict[str, Union[int, Optional[Result]]]:\n    ret = {\n        \"completion_id\": completion_id,\n        \"task_id\": problem[\"task_id\"],\n        \"_identifier\": identifier,\n    }\n    ret[\"base\"] = untrusted_check(\n        solution,\n        problem[\"base_input\"],\n        problem[\"entry_point\"],\n        expected=expected_output[\"base\"],\n        atol=problem[\"atol\"],\n        ref_time=expected_output[\"base_time\"],\n        fast_check=fast_check,\n        min_time_limit=min_time_limit,\n        gt_time_limit_factor=gt_time_limit_factor,\n    )\n\n    if not base_only:\n        ret[\"plus\"] = untrusted_check(\n            solution,\n            problem[\"plus_input\"],\n            problem[\"entry_point\"],\n            expected=expected_output[\"plus\"],\n            atol=problem[\"atol\"],\n            ref_time=expected_output[\"plus_time\"],\n            fast_check=fast_check,\n            min_time_limit=min_time_limit,\n            gt_time_limit_factor=gt_time_limit_factor,\n        )\n\n    return ret", "\n\ndef evaluate_humaneval(flags):\n    if flags.parallel is None:\n        n_workers = max(1, multiprocessing.cpu_count() // 2)\n    else:\n        n_workers = flags.parallel\n\n    if os.path.isdir(flags.samples):\n        result_path = os.path.join(flags.samples, \"eval_results.json\")\n    else:\n        assert flags.samples.endswith(\".jsonl\")\n        result_path = flags.samples.replace(\".jsonl\", \"_eval_results.json\")\n\n    if os.path.isfile(result_path) and not flags.i_just_wanna_run:\n        print(f\"Load from previous results from {result_path}\")\n        with open(result_path, \"r\") as f:\n            results = json.load(f)\n\n        results = compatible_eval_result(results)\n    else:\n        problems = get_human_eval_plus(mini=flags.mini)\n\n        dataset_hash = get_human_eval_plus_hash()\n        expected_output = get_groundtruth(problems, dataset_hash)\n\n        results = {\n            \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n            \"hash\": dataset_hash,\n            \"eval\": {},\n        }\n\n        with ProcessPoolExecutor(max_workers=n_workers) as executor:\n            futures = []\n            completion_id = Counter()\n            n_samples = 0\n            eval_results = defaultdict(list)\n            remainings = set()\n\n            print(\"Reading samples...\")\n            for sample in tqdm(load_solutions(flags.samples)):\n                task_id = sample[\"task_id\"]\n                solution = (\n                    sample[\"solution\"]\n                    if \"solution\" in sample\n                    else problems[task_id][\"prompt\"] + sample[\"completion\"]\n                )\n                remainings.add(sample[\"_identifier\"])\n                args = (\n                    completion_id[task_id],\n                    problems[task_id],\n                    solution,\n                    expected_output[task_id],\n                    flags.base_only,\n                    not flags.test_details,  # fast_check\n                    sample[\"_identifier\"],\n                    flags.min_time_limit,\n                    flags.gt_time_limit_factor,\n                )\n                futures.append(executor.submit(check_correctness, *args))\n                completion_id[task_id] += 1\n                n_samples += 1\n\n            assert n_samples == len(remainings), \"Missing problems in unfinished\"\n            assert len(completion_id) == len(problems), \"Missing problems in samples\"\n\n            def stucking_checker():\n                while remainings:\n                    last_size = len(remainings)\n                    time.sleep(10)\n                    if last_size == len(remainings) and len(remainings) > 0:\n                        print(f\"Stucking for 10 seconds... {len(remainings)} left\")\n                        for remaining in remainings:\n                            print(remaining)\n\n            threading.Thread(target=stucking_checker).start()\n\n            for future in tqdm(as_completed(futures), total=n_samples):\n                result = future.result()\n                remainings.remove(result[\"_identifier\"])\n                eval_results[result[\"task_id\"]].append(result)\n\n        # sort the results for each problem by completion_id\n        for task_id, task_results in eval_results.items():\n            task_results.sort(key=lambda x: x[\"completion_id\"])\n            results[\"eval\"][task_id] = {\n                \"nfiles\": len(task_results),\n                \"base\": [x[\"base\"] for x in task_results],\n                \"plus\": [x[\"plus\"] for x in task_results]\n                if not flags.base_only\n                else [],\n            }\n\n    if os.path.isfile(result_path) and flags.i_just_wanna_run:\n        decision = \"\"\n        while decision.lower() not in [\"y\", \"n\"]:\n            print(f\"{result_path} already exists. Press [Y/N] to overwrite or exit...\")\n            decision = input()\n\n        if decision.lower() == \"y\":\n            # mv the file to a backup\n            new_path = result_path + \".bak\"\n            while os.path.isfile(new_path):\n                new_path += \".bak\"\n            os.rename(result_path, new_path)\n            print(f\"Backup {result_path} to {new_path}\")\n\n    if not os.path.isfile(result_path):\n        with open(result_path, \"w\") as f:\n            json.dump(results, f)\n\n    # Calculate pass@k.\n    total = np.array([r[\"nfiles\"] for r in results[\"eval\"].values()])\n    base_correct = []\n    new_correct = []\n\n    for res in results[\"eval\"].values():\n        bc = sum([r[0] == SUCCESS for r in res[\"base\"]])\n        base_correct.append(bc)\n        if res[\"plus\"]:\n            new_correct.append(\n                sum(\n                    [\n                        res[\"plus\"][i][0] == res[\"base\"][i][0] == SUCCESS\n                        for i in range(len(res[\"plus\"]))\n                    ]\n                )\n            )\n    base_correct = np.array(base_correct)\n\n    pass_at_k = {\n        f\"pass@{k}\": estimate_pass_at_k(total, base_correct, k).mean()\n        for k in [1, 10, 100]\n        if total.min() >= k\n    }\n    print(\"Base\")\n    print(pass_at_k)\n\n    if new_correct:\n        print(\"Base + Extra\")\n        pass_at_k = {\n            f\"pass@{k}\": estimate_pass_at_k(total, np.array(new_correct), k).mean()\n            for k in [1, 10, 100]\n            if (total >= k).all()\n        }\n        print(pass_at_k)", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset\", required=True, type=str)\n    parser.add_argument(\"--samples\", required=True, type=str)\n    parser.add_argument(\"--base-only\", action=\"store_true\")\n    parser.add_argument(\"--parallel\", default=None, type=int)\n    parser.add_argument(\"--i-just-wanna-run\", action=\"store_true\")\n    parser.add_argument(\"--test-details\", action=\"store_true\")\n    parser.add_argument(\"--min-time-limit\", default=0.2, type=float)\n    parser.add_argument(\"--gt-time-limit-factor\", default=4.0, type=float)\n    parser.add_argument(\"--mini\", action=\"store_true\")\n    args = parser.parse_args()\n\n    if args.dataset == \"humaneval\":\n        evaluate_humaneval(args)\n    else:\n        raise NotImplementedError(\"Unsupported dataset: {}\".format(args.dataset))", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "evalplus/__init__.py", "chunked_list": ["try:\n    from evalplus._version import __version__, __version_tuple__\nexcept ImportError:\n    __version__ = \"local-dev\"\n"]}
{"filename": "evalplus/tsr/run.py", "chunked_list": ["import os\n\nfrom evalplus.tsr.utils import execute_cmd\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, help=\"Model for testing\")\n    parser.add_argument(\n        \"--report_dir\",\n        type=str,\n        help=\"Path to JSON report and cache files\",\n        default=\"./tsr_info\",\n    )\n    parser.add_argument(\n        \"--sample_eval_dir\",\n        type=str,\n        required=True,\n        help=\"Path to sample evaluation files\",\n    )\n    parser.add_argument(\n        \"--mini_path\", type=str, default=\"./tsr_info\", help=\"Path to Mini Dataset\"\n    )\n    parser.add_argument(\"--mutation_only\", action=\"store_true\", default=False)\n    args = parser.parse_args()\n\n    os.makedirs(\"tsr_info\", exist_ok=True)\n    if args.mutation_only:\n        execute_cmd(\n            [\n                \"python3\",\n                \"evalplus/tsr/mutation_init.py\",\n                \"--report_dir\",\n                args.report_dir,\n            ]\n        )\n    else:\n        execute_cmd(\n            [\n                \"python3\",\n                \"evalplus/tsr/coverage_init.py\",\n                \"--report_dir\",\n                args.report_dir,\n            ]\n        )\n        execute_cmd(\n            [\n                \"python3\",\n                \"evalplus/tsr/sample_init.py\",\n                \"--report_dir\",\n                args.report_dir,\n                \"--sample_eval_dir\",\n                args.sample_eval_dir,\n            ]\n        )\n        execute_cmd(\n            [\n                \"python3\",\n                \"evalplus/tsr/minimization.py\",\n                \"--model\",\n                args.model,\n                \"--report_dir\",\n                args.report_dir,\n                \"--sample_eval_dir\",\n                args.sample_eval_dir,\n                \"--mini_path\",\n                args.mini_path,\n            ]\n        )", ""]}
{"filename": "evalplus/tsr/minimization.py", "chunked_list": ["import argparse\nimport json\nimport os\nimport pickle\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom copy import deepcopy\nfrom typing import Any, Dict, List, Optional, Tuple\n\nfrom rich.progress import track\n", "from rich.progress import track\n\nfrom evalplus.data import write_jsonl\nfrom evalplus.tsr.coverage_init import collect_coverage_info\nfrom evalplus.tsr.mutation_init import collect_mutation_info\nfrom evalplus.tsr.sample_init import collect_sample_info\nfrom evalplus.tsr.utils import HUMANEVAL_COUNT, problems, task_ids, to_path\n\n###########################\n# Greedy Min Set Covering #", "###########################\n# Greedy Min Set Covering #\n###########################\n\n\ndef merge_set_cover(*args) -> Dict[str, List[str]]:\n    merged_set_cover = {task_id: [] for task_id in task_ids}\n    for set_cover_dict in args:\n        for task_id, plus_tests in set_cover_dict.items():\n            for plus_test in plus_tests:\n                if plus_test not in merged_set_cover[task_id]:\n                    merged_set_cover[task_id].append(plus_test)\n    return merged_set_cover", "\n\ndef greedy_cover(\n    task_id: str, tests: Dict[str, List[Any]], exclude_model: str\n) -> Tuple[str, List[str]]:\n    q, U = [], set()\n    for test_name, test_cover in tests.items():\n        cover_set = set()\n        for model_path, i_code in test_cover:\n            if exclude_model not in model_path:\n                cover_set.add((model_path, i_code))\n        q.append((test_name, cover_set))\n        U = U.union(cover_set)\n    # Greedy algorithm for min set cover\n    min_cover = []\n    while len(U) > 0:\n        max_uncover_set, max_test_name = {}, \"\"\n        for test_name, cover_set in q:\n            if len(cover_set) > len(max_uncover_set):\n                max_uncover_set = cover_set\n                max_test_name = test_name\n        min_cover.append(max_test_name)\n        U = U - max_uncover_set\n        qq = []\n        for test_name, cover_set in q:\n            new_cover_set = U.intersection(cover_set)\n            if len(new_cover_set) != 0:\n                qq.append((test_name, new_cover_set))\n        q = qq\n    return task_id, min_cover", "\n\ndef parallel_greedy_cover(\n    info_dict: Optional[Dict[str, Dict[str, List[Any]]]],\n    exclude_model: str,\n    type: str,\n    **kwargs,\n) -> Dict[str, List[str]]:\n    plus_tests = {task_id: [] for task_id in task_ids}\n\n    with ProcessPoolExecutor(max_workers=32) as executor:\n        futures = []\n        for task_id in task_ids:\n            if type == \"sample\":\n                path_task_id = to_path(task_id)\n                sample_dir = kwargs[\"sample_dir\"]\n                with open(os.path.join(sample_dir, f\"{path_task_id}.pkl\"), \"rb\") as f:\n                    td = pickle.load(f)\n                args = (task_id, td, exclude_model)\n            else:\n                args = (task_id, info_dict[task_id], exclude_model)\n            futures.append(executor.submit(greedy_cover, *args))\n        for future in track(as_completed(futures), f\"min set cover :: {type}\"):\n            task_id, min_cover = future.result()\n            plus_tests[task_id] = min_cover\n\n    return plus_tests", "\n\n#####################\n# Collect Set Cover #\n#####################\n\n\ndef get_coverage_set_cover(\n    coverage_dir: str, exclude_model: str\n) -> Dict[str, List[str]]:\n    coverage_info_dict = collect_coverage_info(coverage_dir)\n    return parallel_greedy_cover(coverage_info_dict, exclude_model, \"coverage\")", "\n\ndef get_mutation_set_cover(\n    mutation_dir: str, exclude_model: str\n) -> Dict[str, List[str]]:\n    mutation_info_dict = collect_mutation_info(\n        os.path.join(mutation_dir, \"eval_results.json\")\n    )\n    return parallel_greedy_cover(mutation_info_dict, exclude_model, \"mutation\")\n", "\n\ndef get_sample_set_cover(\n    sample_dir: str, sample_eval_dir: str, exclude_model: str\n) -> Dict[str, List[str]]:\n    collect_sample_info(sample_dir, sample_eval_dir)\n    return parallel_greedy_cover(None, exclude_model, \"sample\", sample_dir=sample_dir)\n\n\n#################", "\n#################\n# pass@1 greedy #\n#################\n\n\ndef compute_avg_test(set_cover_info: Dict[str, List[str]]) -> float:\n    sum_tests = sum(\n        len(problems[task_id][\"base_input\"]) + len(set_cover_info[task_id])\n        for task_id in task_ids\n    )\n    return sum_tests / HUMANEVAL_COUNT", "\n\ndef gen_report(set_cover_info: Dict[str, List[str]], sample_eval_dir: str, model: str):\n    tsr_dict = {\"ntests\": compute_avg_test(set_cover_info), \"pass@1\": 0}\n    model_path = os.path.join(sample_eval_dir, f\"{model}_temp_0.0\", \"eval_results.json\")\n    with open(model_path, \"r\") as f:\n        mdict = json.load(f)\n    correct_cnt = 0\n    for task_id in task_ids:\n        legacy_task_id = task_id\n        if legacy_task_id not in mdict[\"eval\"]:\n            legacy_task_id = legacy_task_id.replace(\"/\", \"_\")\n        if mdict[\"eval\"][legacy_task_id][\"base\"][0][0] != \"success\":\n            continue\n        correct = True\n        for plus_id in set_cover_info[task_id]:\n            index = int(plus_id.split(\"_\")[-1])\n            if mdict[\"eval\"][legacy_task_id][\"plus\"][0][1][index] == False:\n                correct = False\n                break\n        if correct:\n            correct_cnt += 1\n    tsr_dict[\"pass@1\"] = correct_cnt / HUMANEVAL_COUNT\n    return tsr_dict", "\n\ndef dump_humaneval_plus_mini(set_cover_info: Dict[str, List[str]], mini_path: str):\n    new_problems = []\n    for task_id in task_ids:\n        otask = problems[task_id]\n        task = {\n            \"task_id\": task_id,\n            \"prompt\": otask[\"prompt\"],\n            \"contract\": otask[\"contract\"],\n            \"canonical_solution\": otask[\"canonical_solution\"],\n            \"entry_point\": otask[\"entry_point\"],\n            \"base_input\": otask[\"base_input\"],\n            \"plus_input\": [],\n            \"atol\": otask[\"atol\"],\n        }\n        for plus_test in set_cover_info[task_id]:\n            index = int(plus_test.split(\"_\")[-1])\n            task[\"plus_input\"].append(otask[\"plus_input\"][index])\n        new_problems.append(deepcopy(task))\n    write_jsonl(os.path.join(mini_path, \"HumanEvalPlus-Mini.jsonl\"), new_problems)", "\n\ndef main(flags):\n    coverage_dir = os.path.join(flags.report_dir, \"coverage_cache\")\n    mutation_dir = os.path.join(flags.report_dir, \"mutation_cache\")\n    sample_dir = os.path.join(flags.report_dir, \"sample_cache\")\n    os.makedirs(flags.report_dir, exist_ok=True)\n\n    coverage_set_cover = get_coverage_set_cover(coverage_dir, flags.model)  # ~25min\n    mutation_set_cover = get_mutation_set_cover(mutation_dir, flags.model)\n    sample_set_cover = get_sample_set_cover(\n        sample_dir, flags.sample_eval_dir, flags.model\n    )\n    merged_set_cover = merge_set_cover(\n        coverage_set_cover, mutation_set_cover, sample_set_cover\n    )\n\n    if flags.model != \"ALL\":\n        final_report = dict()\n        # Stage 1: Coverage min set cover\n        final_report[\"coverage\"] = gen_report(\n            coverage_set_cover, flags.sample_eval_dir, flags.model\n        )\n        # Stage 2: Mutation min set cover\n        final_report[\"mutation\"] = gen_report(\n            mutation_set_cover, flags.sample_eval_dir, flags.model\n        )\n        # Stage 3: Sampling min set cover\n        final_report[\"sample\"] = gen_report(\n            sample_set_cover, flags.sample_eval_dir, flags.model\n        )\n        # Stage 4: All\n        final_report[\"full\"] = gen_report(\n            merged_set_cover, flags.sample_eval_dir, flags.model\n        )\n        with open(\n            os.path.join(flags.report_dir, f\"report_{flags.model}.json\"), \"w\"\n        ) as f:\n            json.dump(final_report, f, indent=4)\n    else:\n        dump_humaneval_plus_mini(merged_set_cover, flags.mini_path)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", required=True, type=str, help=\"Model for testing\")\n    parser.add_argument(\n        \"--report_dir\", type=str, help=\"Path to JSON report and cache files\"\n    )\n    parser.add_argument(\n        \"--sample_eval_dir\", type=str, help=\"Path to sample evaluation files\"\n    )\n    parser.add_argument(\"--mini_path\", type=str, help=\"Path to Mini Dataset\")\n    args = parser.parse_args()\n\n    main(args)", ""]}
{"filename": "evalplus/tsr/coverage_init.py", "chunked_list": ["import os\nimport pickle\nimport sys\nfrom importlib import import_module\nfrom io import StringIO\nfrom typing import Any, Dict, List\n\nimport coverage\nfrom rich.progress import track\n", "from rich.progress import track\n\nfrom evalplus.eval.utils import swallow_io\nfrom evalplus.tsr.utils import problems, task_ids, to_path\n\n\nclass Capturing(list):\n    def __enter__(self):\n        self._stdout = sys.stdout\n        sys.stdout = self._stringio = StringIO()\n        return self\n\n    def __exit__(self, *args):\n        self.extend(self._stringio.getvalue().splitlines())\n        del self._stringio\n        sys.stdout = self._stdout", "\n\ndef parse_lcov(outputs: List[str]):\n    switch, extracted_outputs = False, []\n    for line in outputs:\n        if switch == False and \"tmp_src\" in line:\n            switch = True\n        if switch == True and \"end_of_record\" in line:\n            switch = False\n        if switch:\n            extracted_outputs.append(line)\n\n    branch, branch_covered = [], []\n    for line in extracted_outputs:\n        if line.startswith(\"BRDA\"):\n            # BRDA format: BR:<lineno>,<blockno>,<branchno>,<taken>\n            lineno, blockno, branchno, taken = line[5:].split(\",\")\n            branch_sig = f\"BR:{lineno},{blockno},{branchno}\"\n            branch.append(branch_sig)\n            if taken not in [\"0\", \"-\"]:\n                branch_covered.append(branch_sig)\n    per = 1.0 if len(branch) == 0 else len(branch_covered) / len(branch)\n    return per, branch, branch_covered", "\n\ndef test_code_coverage(\n    identifier: str, code: str, inputs: List[List[Any]], entry_point: str\n):\n    module_name = f\"tmp_src_{identifier}\"\n    with open(f\"{module_name}.py\", \"w\") as f:\n        f.write(code)\n\n    mod = import_module(module_name)\n    func = getattr(mod, entry_point, None)\n    assert func != None, f\"entry_point = {entry_point} not exist, code: {code}\"\n\n    cov = coverage.Coverage(branch=True)\n    cov.start()\n    with swallow_io():\n        for input_list in inputs:\n            func(*input_list)\n    cov.stop()\n    with Capturing() as outputs:\n        cov.lcov_report(outfile=\"-\")\n\n    ret = parse_lcov(outputs)\n\n    os.remove(f\"{module_name}.py\")\n    return ret", "\n\ndef collect_coverage_info(coverage_dir: str) -> Dict[str, Dict[str, Any]]:\n    os.makedirs(coverage_dir, exist_ok=True)\n    coverage_info = {task_id: {} for task_id in task_ids}\n    for task_id in track(task_ids, description=\"Testing gt coverage...\"):\n        coverage_cache_path = os.path.join(coverage_dir, f\"{to_path(task_id)}.pkl\")\n        if os.path.isfile(coverage_cache_path):\n            with open(coverage_cache_path, \"rb\") as f:\n                coverage_info[task_id] = pickle.load(f)\n            continue\n        groundtruth_code = (\n            problems[task_id][\"prompt\"] + problems[task_id][\"canonical_solution\"]\n        )\n        plus_tests = problems[task_id][\"plus_input\"]\n        entry_point = problems[task_id][\"entry_point\"]\n        for i, plus_test in enumerate(plus_tests):\n            per, branch, branch_covered = test_code_coverage(\n                to_path(task_id), groundtruth_code, [plus_test], entry_point\n            )\n            test_id = f\"plus_{i}\"\n            coverage_info[task_id].setdefault(test_id, []).extend(\n                [(br, \"gt\") for br in branch_covered]\n            )\n        with open(coverage_cache_path, \"wb\") as f:\n            pickle.dump(coverage_info[task_id], f)\n\n    return coverage_info", "\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--report_dir\", required=True, type=str)\n    args = parser.parse_args()\n\n    coverage_dir = os.path.join(args.report_dir, \"coverage_cache\")\n    collect_coverage_info(coverage_dir)", ""]}
{"filename": "evalplus/tsr/mutation_init.py", "chunked_list": ["import argparse\nimport json\nimport os\nfrom typing import Any, Dict, List\n\nfrom rich.progress import track\n\nfrom evalplus.eval.utils import swallow_io\nfrom evalplus.evaluate import evaluate_humaneval\nfrom evalplus.tsr.utils import (", "from evalplus.evaluate import evaluate_humaneval\nfrom evalplus.tsr.utils import (\n    clean,\n    execute_cmd,\n    get_cmd_output,\n    problems,\n    task_ids,\n    to_path,\n)\n", ")\n\n\ndef prepare_mutants(mutation_dir: str):\n    pwd = os.getcwd()\n    os.makedirs(mutation_dir, exist_ok=True)\n    for task_id in track(task_ids, \"Generating mutants\"):\n        task_dir = os.path.join(mutation_dir, to_path(task_id))\n        os.makedirs(task_dir, exist_ok=True)\n        if any(map(lambda filename: filename.startswith(\"m\"), os.listdir(task_dir))):\n            # already have mutants\n            continue\n        # Make groundtruth\n        groundtruth_code = (\n            problems[task_id][\"prompt\"] + problems[task_id][\"canonical_solution\"]\n        )\n        with open(os.path.join(task_dir, \"gt.py\"), \"w\") as f:\n            f.write(groundtruth_code)\n        # Make dummy pytest\n        with open(os.path.join(task_dir, \"test_dummy.py\"), \"w\") as f:\n            f.write(\"def test_dummy():\\n    pass\")\n        # Use mutmut to generate mutants\n        try:\n            os.chdir(task_dir)\n            clean(\".mutmut-cache\")\n            execute_cmd([\"mutmut run\", \"--paths-to-mutate=gt.py\", \"1>/dev/null\"])\n            # Collect metainfo\n            try:\n                total_mutants = int(\n                    get_cmd_output([\"mutmut\", \"results\"]).split(\"\\n\")[-2].split(\"-\")[-1]\n                )\n            except:\n                total_mutants = 0\n            # Dump mutants\n            for i in range(1, total_mutants + 1):\n                execute_cmd([\"cp\", \"gt.py\", \"gt_copy.py\"])\n                execute_cmd([\"mutmut\", \"apply\", str(i)])\n                execute_cmd([\"mv\", \"gt.py\", f\"m{i}.py\"])\n                execute_cmd([\"mv\", \"gt_copy.py\", \"gt.py\"])\n            # Remove gt and dummy pytest\n            execute_cmd([\"rm\", \"gt.py\"])\n            execute_cmd([\"rm\", \"test_dummy.py\"])\n        except:\n            assert 0\n\n    os.chdir(pwd)", "\n\ndef mutants_eval(mutation_dir: str):\n    args = argparse.Namespace(\n        dataset=\"humaneval\",\n        samples=mutation_dir,\n        base_only=False,\n        parallel=None,\n        full=True,\n        i_just_wanna_run=False,\n    )\n    print(\"Evaluating mutants... \", end=\"\")\n    with swallow_io():\n        evaluate_humaneval(args)\n    print(\"Done\")", "\n\ndef collect_mutation_info(eval_path: str) -> Dict[str, Dict[str, List[Any]]]:\n    mutation_info = {task_id: {} for task_id in task_ids}\n    assert os.path.isfile(\n        eval_path\n    ), f\"mutation testing result file {eval_path} missing!\"\n    eval_res = json.load(open(eval_path, \"r\"))[\"eval\"]\n    for task_id, v in eval_res.items():\n        for i_code, (status, res_list) in enumerate(v[\"plus\"]):\n            if status == \"success\":\n                continue\n            for i_test, res in enumerate(res_list):\n                test_id = f\"plus_{i_test}\"\n                if res == False:\n                    mutation_info[task_id].setdefault(test_id, []).append(\n                        (\"mutant\", i_code)\n                    )\n    return mutation_info", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--report_dir\", required=True, type=str)\n    args = parser.parse_args()\n\n    mutation_dir = os.path.join(args.report_dir, \"mutation_cache\")\n    prepare_mutants(mutation_dir)\n    mutants_eval(mutation_dir)\n    collect_mutation_info(os.path.join(mutation_dir, \"eval_results.json\"))", ""]}
{"filename": "evalplus/tsr/__init__.py", "chunked_list": [""]}
{"filename": "evalplus/tsr/utils.py", "chunked_list": ["import os\nimport subprocess\n\nfrom evalplus.data import get_human_eval_plus\n\nHUMANEVAL_COUNT = 164\nproblems = get_human_eval_plus()\n\ntask_ids = [f\"HumanEval/{i}\" for i in range(HUMANEVAL_COUNT)]\n", "task_ids = [f\"HumanEval/{i}\" for i in range(HUMANEVAL_COUNT)]\n\n\ndef to_path(task_id: str) -> str:\n    assert task_id in task_ids, f\"invalid task_id = {task_id}\"\n    return task_id.replace(\"/\", \"_\")\n\n\ndef clean(file_path: str):\n    if os.path.exists(file_path):\n        os.remove(file_path)", "def clean(file_path: str):\n    if os.path.exists(file_path):\n        os.remove(file_path)\n\n\ndef execute_cmd(cmd: list):\n    os.system(\" \".join(cmd))\n\n\ndef get_cmd_output(cmd_list: list) -> str:\n    return subprocess.run(cmd_list, stdout=subprocess.PIPE, check=True).stdout.decode()", "\ndef get_cmd_output(cmd_list: list) -> str:\n    return subprocess.run(cmd_list, stdout=subprocess.PIPE, check=True).stdout.decode()\n"]}
{"filename": "evalplus/tsr/sample_init.py", "chunked_list": ["import json\nimport os\nimport pickle\n\nfrom rich.progress import track\n\nfrom evalplus.tsr.utils import task_ids, to_path\n\n\ndef collect_sample_info(sample_dir: str, sample_eval_dir: str):\n    if os.path.exists(sample_dir) and len(os.listdir(sample_dir)) > 0:\n        # cache file exists\n        return\n\n    assert os.path.exists(sample_eval_dir), \"sample evaluation files missing\"\n    os.makedirs(sample_dir, exist_ok=True)\n    kill_info = {task_id: {} for task_id in task_ids}\n    model_paths = os.listdir(sample_eval_dir)\n    for model_path in track(model_paths, description=\"Collecting sets...\"):\n        if not model_path[-1].isdigit():\n            continue\n        eval_json_path = os.path.join(sample_eval_dir, model_path, \"eval_results.json\")\n        if not os.path.exists(eval_json_path):\n            continue\n        with open(eval_json_path, \"r\") as f:\n            res = json.load(f)[\"eval\"]\n            for task_id, v in res.items():\n                if task_id not in task_ids:\n                    continue\n                for i_code, (status, res_list) in enumerate(v[\"plus\"]):\n                    if status == \"success\":\n                        continue\n                    for i_test, res in enumerate(res_list):\n                        test_id = f\"plus_{i_test}\"\n                        if res == False:\n                            if \"_\" in task_id:\n                                task_id = task_id.replace(\"_\", \"/\")\n                            kill_info[task_id].setdefault(test_id, []).append(\n                                (model_path, i_code)\n                            )\n    for task_id in task_ids:\n        path_task_id = to_path(task_id)\n        with open(os.path.join(sample_dir, f\"{path_task_id}.pkl\"), \"wb\") as f:\n            pickle.dump(kill_info[task_id], f)", "\ndef collect_sample_info(sample_dir: str, sample_eval_dir: str):\n    if os.path.exists(sample_dir) and len(os.listdir(sample_dir)) > 0:\n        # cache file exists\n        return\n\n    assert os.path.exists(sample_eval_dir), \"sample evaluation files missing\"\n    os.makedirs(sample_dir, exist_ok=True)\n    kill_info = {task_id: {} for task_id in task_ids}\n    model_paths = os.listdir(sample_eval_dir)\n    for model_path in track(model_paths, description=\"Collecting sets...\"):\n        if not model_path[-1].isdigit():\n            continue\n        eval_json_path = os.path.join(sample_eval_dir, model_path, \"eval_results.json\")\n        if not os.path.exists(eval_json_path):\n            continue\n        with open(eval_json_path, \"r\") as f:\n            res = json.load(f)[\"eval\"]\n            for task_id, v in res.items():\n                if task_id not in task_ids:\n                    continue\n                for i_code, (status, res_list) in enumerate(v[\"plus\"]):\n                    if status == \"success\":\n                        continue\n                    for i_test, res in enumerate(res_list):\n                        test_id = f\"plus_{i_test}\"\n                        if res == False:\n                            if \"_\" in task_id:\n                                task_id = task_id.replace(\"_\", \"/\")\n                            kill_info[task_id].setdefault(test_id, []).append(\n                                (model_path, i_code)\n                            )\n    for task_id in task_ids:\n        path_task_id = to_path(task_id)\n        with open(os.path.join(sample_dir, f\"{path_task_id}.pkl\"), \"wb\") as f:\n            pickle.dump(kill_info[task_id], f)", "\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--report_dir\", required=True, type=str)\n    parser.add_argument(\"--sample_eval_dir\", required=True, type=str)\n    args = parser.parse_args()\n\n    sample_dir = os.path.join(args.report_dir, \"sample_cache\")\n    collect_sample_info(sample_dir, args.sample_eval_dir)", ""]}
{"filename": "evalplus/data/__init__.py", "chunked_list": ["import gzip\nimport hashlib\nimport json\nimport os\nfrom os import PathLike\nfrom typing import Dict, Iterable\n\nimport tempdir\nimport wget\nfrom appdirs import user_cache_dir", "import wget\nfrom appdirs import user_cache_dir\n\nCACHE_DIR = user_cache_dir(\"evalplus\")\n\n\nHUMANEVAL_URL = (\n    \"https://github.com/openai/human-eval/raw/master/data/HumanEval.jsonl.gz\"\n)\nHUMANEVAL_PLUS_VERSION = \"v0.1.5\"", ")\nHUMANEVAL_PLUS_VERSION = \"v0.1.5\"\n\n\ndef get_dataset_metadata(name, version, mini):\n    assert name in [\"HumanEvalPlus\"], f\"Unknown/unsupported dataset: {name}\"\n    extra = \"-Mini\" if mini else \"\"\n    url = f\"https://github.com/ganler/release/releases/download/humanevalplus/{name}{extra}-{version}.jsonl.gz\"\n    cache_path = os.path.join(CACHE_DIR, f\"{name}{extra}-{version}.jsonl\")\n    return url, cache_path", "\n\n# hacky way to handle \\n\\r, etc in strings\ndef to_raw(string):\n    return string.encode(\"unicode-escape\").decode().replace(\"\\\\\\\\\", \"\\\\\")\n\n\ndef write_jsonl(filename: str, data: Iterable[Dict], append: bool = False):\n    \"\"\"\n    Writes an iterable of dictionaries to jsonl\n    \"\"\"\n    if append:\n        mode = \"ab\"\n    else:\n        mode = \"wb\"\n    filename = os.path.expanduser(filename)\n    if filename.endswith(\".gz\"):\n        with open(filename, mode) as fp:\n            with gzip.GzipFile(fileobj=fp, mode=\"wb\") as gzfp:\n                for x in data:\n                    gzfp.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))\n    else:\n        with open(filename, mode) as fp:\n            for x in data:\n                fp.write((json.dumps(x) + \"\\n\").encode(\"utf-8\"))", "\n\ndef stream_jsonl(filename: str) -> Iterable[Dict]:\n    \"\"\"\n    Parses each jsonl line and yields it as a dictionary\n    \"\"\"\n    if filename.endswith(\".gz\"):\n        with open(filename, \"rb\") as gzfp:\n            with gzip.open(gzfp, \"rt\") as fp:\n                for line in fp:\n                    if any(not x.isspace() for x in line):\n                        yield json.loads(line)\n    else:\n        with open(filename, \"r\") as fp:\n            for line in fp:\n                if any(not x.isspace() for x in line):\n                    yield json.loads(line)", "\n\ndef load_solutions(sample_path: PathLike) -> Iterable[Dict]:\n    \"\"\"We accept two formats of inputs.\n    + `sample.jsonl` which is the format from HumanEval, i.e., {task_id, completion}.\n    + A folder which contains sub-folders named after the task_id. Each sub-folder\n    contains samples named in `[?].py` where `?` is the solution id starting with 0.\n    Different from `sample.jsonl`, the solutions must be complete (with prompt prefix).\n    \"\"\"\n\n    # if it is a file\n    if os.path.isfile(sample_path):\n        for i, sample in enumerate(stream_jsonl(sample_path)):\n            sample[\"_identifier\"] = sample[\"task_id\"] + \"_\" + str(i)\n            yield sample\n    else:\n        # if it is a folder\n        for task_id in os.listdir(sample_path):\n            task_path = os.path.join(sample_path, task_id)\n            if os.path.isdir(task_path):\n                for solution_id in os.listdir(task_path):\n                    solution_path = os.path.join(task_path, solution_id)\n                    if os.path.isfile(solution_path) and solution_path.endswith(\".py\"):\n                        with open(solution_path, \"r\") as f:\n                            completion = f.read()\n                        yield {\n                            \"_identifier\": solution_path,\n                            \"task_id\": task_id.replace(\"HumanEval_\", \"HumanEval/\"),\n                            \"solution\": completion,\n                        }", "\n\ndef _ready_human_eval_plus_path(mini=False) -> str:\n    url, plus_path = get_dataset_metadata(\"HumanEvalPlus\", HUMANEVAL_PLUS_VERSION, mini)\n\n    # Check if human eval file exists in CACHE_DIR\n    if not os.path.exists(plus_path):\n        # Install HumanEval dataset and parse as jsonl\n        # https://github.com/openai/human-eval/blob/master/data/HumanEval.jsonl.gz\n        print(\"Downloading HumanEvalPlus dataset...\")\n        with tempdir.TempDir() as tmpdir:\n            plus_gz_path = os.path.join(tmpdir, f\"data.jsonl.gz\")\n            wget.download(url, plus_gz_path)\n\n            with gzip.open(plus_gz_path, \"rb\") as f:\n                plus = f.read().decode(\"utf-8\")\n\n        # create CACHE_DIR if not exists\n        if not os.path.exists(CACHE_DIR):\n            os.makedirs(CACHE_DIR)\n\n        # Write the original human eval file to CACHE_DIR\n        with open(plus_path, \"w\") as f:\n            f.write(plus)\n\n    return plus_path", "\n\ndef get_human_eval_plus_hash() -> str:\n    \"\"\"Get the hash of HumanEvalPlus.\n    Returns:\n        str: The hash of HumanEvalPlus\n    \"\"\"\n    plus_path = _ready_human_eval_plus_path()\n    with open(plus_path, \"rb\") as f:\n        plus = f.read()\n    return hashlib.md5(plus).hexdigest()", "\n\ndef get_human_eval_plus(err_incomplete=True, mini=False) -> Dict[str, Dict]:\n    \"\"\"Get HumanEvalPlus locally.\n    Args:\n        err_incomplete (bool, optional): Whether to raise error if HumanEvalPlus is not complete. Defaults to True.\n        mini (bool, optional): Whether to use the mini version of HumanEvalPlus. Defaults to False.\n    Returns:\n        List[Dict[str, str]]: List of dicts with keys \"task_id\", \"prompt\", \"contract\", \"canonical_solution\", \"base_input\"\n    Notes:\n        \"task_id\" is the identifier string for the task\n        \"prompt\" is the function signature with docstring\n        \"contract\" is the assertions for the function's input (validity)\n        \"canonical_solution\" is the ground-truth implementation for diff-testing\n        \"base_input\" is the test inputs from original HumanEval\n        \"plus_input\" is the test inputs brought by EvalPlus\n        \"atol\" is the absolute tolerance for diff-testing\n    \"\"\"\n    plus_path = _ready_human_eval_plus_path(mini=mini)\n    plus = {task[\"task_id\"]: task for task in stream_jsonl(plus_path)}\n    if err_incomplete:\n        for task_id, task in plus.items():\n            for key in [\n                \"prompt\",\n                \"contract\",\n                \"canonical_solution\",\n                \"base_input\",\n                \"plus_input\",\n                \"atol\",\n            ]:\n                assert key in task, f\"{key} not found in HumanEvalPlus #{task_id}!\"\n    return plus", "\n\ndef get_human_eval() -> Dict[str, Dict]:\n    \"\"\"Get HumanEval from OpenAI's github repo and return as a list of parsed dicts.\n\n    Returns:\n        List[Dict[str, str]]: List of dicts with keys \"prompt\", \"test\", \"entry_point\"\n\n    Notes:\n        \"task_id\" is the identifier string for the task.\n        \"prompt\" is the prompt to be used for the task (function signature with docstrings).\n        \"test\" is test-cases wrapped in a `check` function.\n        \"entry_point\" is the name of the function.\n    \"\"\"\n    # Check if human eval file exists in CACHE_DIR\n    human_eval_path = os.path.join(CACHE_DIR, \"HumanEval.jsonl\")\n\n    human_eval = None\n    if not os.path.exists(human_eval_path):\n        # Install HumanEval dataset and parse as jsonl\n        # https://github.com/openai/human-eval/blob/master/data/HumanEval.jsonl.gz\n        print(\"Downloading HumanEval dataset...\")\n        with tempdir.TempDir() as tmpdir:\n            human_eval_gz_path = os.path.join(tmpdir, \"HumanEval.jsonl.gz\")\n            wget.download(HUMANEVAL_URL, human_eval_gz_path)\n\n            with gzip.open(human_eval_gz_path, \"rb\") as f:\n                human_eval = f.read().decode(\"utf-8\")\n\n        # create CACHE_DIR if not exists\n        if not os.path.exists(CACHE_DIR):\n            os.makedirs(CACHE_DIR)\n\n        # Write the original human eval file to CACHE_DIR\n        with open(human_eval_path, \"w\") as f:\n            f.write(human_eval)\n\n    human_eval = open(human_eval_path, \"r\").read() if not human_eval else human_eval\n    human_eval = human_eval.split(\"\\n\")\n    human_eval = [json.loads(line) for line in human_eval if line]\n\n    # Handle 115_max_fill.py to make its docstring well-formed\n    human_eval[114][\"prompt\"] = \"import math\\n\" + human_eval[114][\"prompt\"].replace(\n        \"import math\\n\", \"\"\n    )\n\n    return {task[\"task_id\"]: task for task in human_eval}", ""]}
{"filename": "evalplus/gen/type_mut.py", "chunked_list": ["import copy\nimport random\nimport string\nimport time\nfrom typing import Any, Dict, List, Set, Tuple\n\nfrom multipledispatch import dispatch\n\nfrom evalplus.gen.mut_gen import MutateGen\nfrom evalplus.gen.util import trusted_check_exec", "from evalplus.gen.mut_gen import MutateGen\nfrom evalplus.gen.util import trusted_check_exec\n\nMAX_MULTI_STEP_SIZE = 5\nMUTATE_BOUND_SIZE = 8\n\nNoneType = type(None)\n\n\n# decorator to use ingredients\nclass use_ingredient:\n    def __init__(self, prob: float):\n        assert 0 <= prob <= 0.95\n        self.prob = prob\n\n    def __call__(obj, func):\n        def wrapper(self, seed_input):\n            if random.random() < obj.prob and self.ingredients[type(seed_input)]:\n                return random.choice(list(self.ingredients[type(seed_input)]))\n            else:\n                return func(self, seed_input)\n\n        return wrapper", "\n# decorator to use ingredients\nclass use_ingredient:\n    def __init__(self, prob: float):\n        assert 0 <= prob <= 0.95\n        self.prob = prob\n\n    def __call__(obj, func):\n        def wrapper(self, seed_input):\n            if random.random() < obj.prob and self.ingredients[type(seed_input)]:\n                return random.choice(list(self.ingredients[type(seed_input)]))\n            else:\n                return func(self, seed_input)\n\n        return wrapper", "\n\nclass TypedMutGen(MutateGen):\n    def __init__(self, inputs: List, signature: str, contract_code: str):\n        super().__init__(inputs, signature, contract_code)\n        self.timeout = 60 * 60  # 1 hour\n        self.ingredients = {\n            int: set(),\n            float: set(),\n            str: set(),\n        }\n        for x in inputs:\n            self.fetch_ingredient(x)\n\n    def seed_selection(self):\n        # random for now.\n        return random.choice(self.seed_pool)\n\n    def mutate(self, seed_input: Any) -> List:\n        new_input = copy.deepcopy(seed_input)\n\n        patience = MUTATE_BOUND_SIZE\n        while new_input == seed_input or patience == 0:\n            new_input = self.typed_mutate(new_input)\n            patience -= 1\n\n        return new_input\n\n    #########################\n    # Type-aware generation #\n    #########################\n    @dispatch(NoneType)\n    def typed_gen(self, _):\n        return None\n\n    @dispatch(int)\n    def typed_gen(self, _):\n        @use_ingredient(0.5)\n        def _impl(*_):\n            return random.randint(-100, 100)\n\n        return _impl(self, _)\n\n    @dispatch(float)\n    def typed_gen(self, _):\n        @use_ingredient(0.5)\n        def _impl(*_):\n            return random.uniform(-100, 100)\n\n        return _impl(self, _)\n\n    @dispatch(bool)\n    def typed_gen(self, _):\n        return random.choice([True, False])\n\n    @dispatch(str)\n    def typed_gen(self, _):\n        @use_ingredient(0.5)\n        def _impl(*_):\n            return \"\".join(\n                random.choice(string.ascii_letters)\n                for _ in range(random.randint(0, 10))\n            )\n\n        return _impl(self, _)\n\n    def any_gen(self):\n        # weighted choose\n        choice = random.choices(\n            [\n                True,\n                1,\n                1.1,\n                \"str\",\n                [],  # list\n                tuple(),  # tuple\n                dict(),  # dict\n                None,  # None\n            ],\n            [0.2, 0.2, 0.2, 0.2, 0.05, 0.05, 0.05, 0.05],\n        )[0]\n        return self.typed_gen(choice)\n\n    @dispatch(list)\n    def typed_gen(self, _):\n        ret = []\n        size = random.randint(0, 10)\n        if random.randint(0, 4) == 0:  # heterogeneous\n            for _ in range(size):\n                ret.append(self.any_gen())\n        else:  # homogeneous\n            t = random.choice([bool(), int(), float(), str()])\n            for _ in range(size):\n                ret.append(self.typed_gen(t))\n        return ret\n\n    @dispatch(tuple)\n    def typed_gen(self, _):\n        return tuple(self.typed_gen([]))\n\n    # NOTE: disable set for now as Steven is too weak in Python (/s)\n    # @dispatch(set)\n    # def typed_gen(self, _):\n    #     return set(self.typed_gen([]))\n\n    @dispatch(dict)\n    def typed_gen(self, _):\n        ret = dict()\n        values = self.typed_gen([])\n        # NOTE: Assumption: nobody uses dict with heterogeneous keys\n        # NOTE: Assumption: nobody uses dict with boolean keys\n        key_type = random.choice([int(), float(), str()])\n        for v in values:\n            ret[self.typed_gen(key_type)] = self.typed_gen(v)\n        return ret\n\n    ########################\n    # Type-aware mutation  #\n    ########################\n    # Simple primitives\n    @dispatch(int)\n    def typed_mutate(self, seed_input: int):\n        @use_ingredient(0.5)\n        def _impl(_, seed_input: int):\n            return seed_input + random.randint(-1, 1)\n\n        return _impl(self, seed_input)\n\n    @dispatch(float)\n    def typed_mutate(self, seed_input: float):\n        @use_ingredient(0.5)\n        def _impl(_, seed_input: float):\n            if random.randint(0, 1):\n                return seed_input + random.uniform(-1, 1)\n            return seed_input * (1 + random.uniform(-0.5, 0.5))\n\n        return _impl(self, seed_input)\n\n    @dispatch(bool)\n    def typed_mutate(self, seed_input: bool):\n        return random.choice([True, False])\n\n    @dispatch(NoneType)\n    def typed_mutate(self, seed_input: NoneType):\n        return None\n\n    # List-like\n    @dispatch(list)\n    def typed_mutate(self, seed_input: List):\n        if len(seed_input) == 0:\n            return self.typed_gen([])\n\n        choice = random.randint(0, 3)\n        idx = random.randint(0, len(seed_input) - 1)\n        if choice == 0:  # remove one element\n            seed_input.pop(random.randint(0, len(seed_input) - 1))\n        elif choice == 1 and len(seed_input) > 0:  # add one mutated element\n            seed_input.insert(\n                random.randint(0, len(seed_input) - 1),\n                self.typed_mutate(seed_input[idx]),\n            )\n        elif choice == 2 and len(seed_input) > 0:  # repeat one element\n            seed_input.append(seed_input[idx])\n        else:  # inplace element change\n            seed_input[idx] = self.typed_mutate(seed_input[idx])\n        return seed_input\n\n    @dispatch(tuple)\n    def typed_mutate(self, seed_input: Tuple):\n        return tuple(self.typed_mutate(list(seed_input)))\n\n    # String\n    @dispatch(str)\n    def typed_mutate(self, seed_input: str):\n        @use_ingredient(0.4)\n        def _impl(_, seed_input: str):\n            choice = random.randint(0, 2) if seed_input else 0\n            if choice == 0 and self.ingredients[str]:  # insert an ingredient\n                idx = random.randint(0, len(seed_input))\n                return (\n                    seed_input[:idx]\n                    + random.choice(list(self.ingredients[str]))\n                    + seed_input[idx:]\n                )\n            # other choices assume len(seed_input) > 0\n            elif choice == 1:  # replace a substring with empty or mutated string\n                start = random.randint(0, len(seed_input) - 1)\n                end = random.randint(start + 1, len(seed_input))\n                mid = (\n                    \"\"\n                    if random.randint(0, 1)\n                    else self.typed_mutate(seed_input[start:end])\n                )\n                return seed_input[:start] + mid + seed_input[end:]\n            elif choice == 2:  # repeat one element\n                idx = random.randint(0, len(seed_input) - 1)\n                return (\n                    seed_input[:idx]\n                    + seed_input[random.randint(0, len(seed_input) - 1)]\n                    + seed_input[idx:]\n                )\n\n            # random char\n            return self.typed_gen(str())\n\n        return _impl(self, seed_input)\n\n    # Set\n    @dispatch(set)\n    def typed_mutate(self, seed_input: Set):\n        return set(self.typed_mutate(list(seed_input)))\n\n    # Dict\n    @dispatch(dict)\n    def typed_mutate(self, seed_input: Dict):\n        if len(seed_input) == 0:\n            return self.typed_gen(dict())\n\n        choice = random.randint(0, 2)\n        if choice == 0:  # remove a kv\n            del seed_input[random.choice(list(seed_input.keys()))]\n        elif choice == 1:  # add a kv\n            k = self.typed_mutate(random.choice(list(seed_input.keys())))\n            v = self.typed_mutate(random.choice(list(seed_input.values())))\n            seed_input[k] = v\n        elif choice == 2:  # inplace value change\n            k0, v0 = random.choice(list(seed_input.items()))\n            seed_input[k0] = self.typed_mutate(v0)\n        return seed_input\n\n    ############################################\n    # Fetching ingredients to self.ingredients #\n    ############################################\n    def fetch_ingredient(self, seed_input):\n        self.typed_fetch(seed_input)\n\n    @dispatch(int)\n    def typed_fetch(self, seed_input: int):\n        self.ingredients[int].add(seed_input)\n\n    @dispatch(float)\n    def typed_fetch(self, seed_input: float):\n        self.ingredients[float].add(seed_input)\n\n    @dispatch(str)\n    def typed_fetch(self, seed_input: str):\n        self.ingredients[str].add(seed_input)\n        for token in seed_input.strip().split():\n            self.ingredients[str].add(token)\n\n    # List-like\n    def _fetch_list_like(self, seed_input):\n        for x in seed_input:\n            if self.typed_fetch.dispatch(type(x)):\n                self.fetch_ingredient(x)\n\n    @dispatch(list)\n    def typed_fetch(self, seed_input: List):\n        self._fetch_list_like(seed_input)\n\n    @dispatch(tuple)\n    def typed_fetch(self, seed_input: Tuple):\n        self._fetch_list_like(seed_input)\n\n    # NOTE: disable set for now as Steven is too weak in Python (/s)\n    # @dispatch(set)\n    # def typed_fetch(self, seed_input: Set):\n    #     self._fetch_list_like(seed_input)\n\n    # Dict\n    @dispatch(dict)\n    def typed_fetch(self, seed_input: Dict):\n        self._fetch_list_like(seed_input.keys())\n        self._fetch_list_like(seed_input.values())\n\n    def generate(self, num: int):\n        start = time.time()\n        num_generated = 1\n        while len(self.new_inputs) < num and time.time() - start < self.timeout:\n            if num_generated % 1000 == 0:\n                print(\n                    f\"generated {num_generated} already with {len(self.new_inputs)} new inputs ... \"\n                )\n            new_input = self.seed_selection()\n            # Multi-step instead of single-step\n            for _ in range(random.randint(1, MAX_MULTI_STEP_SIZE)):\n                new_input = self.mutate(new_input)\n            num_generated += 1\n            if hash(str(new_input)) not in self.seed_hash:\n                if trusted_check_exec(self.contract, [new_input], self.entry_point):\n                    self.typed_fetch(new_input)\n                    self.seed_pool.append(new_input)\n                    self.new_inputs.append(new_input)\n                self.seed_hash.add(hash(str(new_input)))\n        return self.new_inputs[:num]", ""]}
{"filename": "evalplus/gen/__init__.py", "chunked_list": ["import copy\nfrom typing import Any, List\n\n\nclass BaseGen(object):\n    def __init__(self, inputs: List[Any], entry_point: str, contract: str):\n        \"\"\"Initializing a input mutator.\n\n        Args:\n            inputs (List[Any]): The set of initial inputs (i.e., seeds)\n            entry_point (str): The function name to invoke with the input\n            contract (str): The contract to verify input validity\n        \"\"\"\n        self.contract = contract\n        self.entry_point = entry_point\n        self.seed_pool: List[Any] = copy.deepcopy(inputs)\n        self.new_inputs = []\n        self.seed_hash = set([hash(str(x)) for x in self.seed_pool])\n\n    def generate(self, num: int) -> List[Any]:\n        raise NotImplementedError", ""]}
{"filename": "evalplus/gen/mut_gen.py", "chunked_list": ["import random\nfrom abc import abstractmethod\nfrom typing import Any, List\n\nfrom evalplus.gen import BaseGen\nfrom evalplus.gen.util import trusted_check_exec\n\n\nclass MutateGen(BaseGen):\n    def __init__(self, inputs: List, signature: str, contract_code: str):\n        super().__init__(inputs, signature, contract_code)\n\n    def seed_selection(self):\n        # random for now.\n        return random.choice(self.seed_pool)\n\n    @abstractmethod\n    def mutate(self, seed_input: Any) -> Any:\n        pass\n\n    def generate(self, num: int) -> List[Any]:\n        while len(self.new_inputs) < num:\n            seed = self.seed_selection()\n            new_input = self.mutate(seed)\n            if hash(str(new_input)) not in self.seed_hash:\n                if trusted_check_exec(self.contract, [new_input], self.entry_point):\n                    self.seed_pool.append(new_input)\n                    self.seed_hash.add(hash(str(new_input)))\n                    self.new_inputs.append(new_input)\n        return self.new_inputs[:num]", "class MutateGen(BaseGen):\n    def __init__(self, inputs: List, signature: str, contract_code: str):\n        super().__init__(inputs, signature, contract_code)\n\n    def seed_selection(self):\n        # random for now.\n        return random.choice(self.seed_pool)\n\n    @abstractmethod\n    def mutate(self, seed_input: Any) -> Any:\n        pass\n\n    def generate(self, num: int) -> List[Any]:\n        while len(self.new_inputs) < num:\n            seed = self.seed_selection()\n            new_input = self.mutate(seed)\n            if hash(str(new_input)) not in self.seed_hash:\n                if trusted_check_exec(self.contract, [new_input], self.entry_point):\n                    self.seed_pool.append(new_input)\n                    self.seed_hash.add(hash(str(new_input)))\n                    self.new_inputs.append(new_input)\n        return self.new_inputs[:num]", ""]}
{"filename": "evalplus/gen/chatgpt_gen.py", "chunked_list": ["import ast\nimport os\nimport random\nfrom typing import Dict, List\n\nimport openai\n\nfrom evalplus.data import to_raw\nfrom evalplus.gen import BaseGen\nfrom evalplus.gen.util import trusted_check_exec", "from evalplus.gen import BaseGen\nfrom evalplus.gen.util import trusted_check_exec\nfrom evalplus.gen.util.api_request import create_chatgpt_config, request_chatgpt_engine\n\n\nclass ChatGPTGen(BaseGen):\n    def __init__(self, inputs: List, signature: str, contract_code: str, gd_code: str):\n        super().__init__(inputs, signature, contract_code)\n        self.gd_code = gd_code\n        self.prompt_messages = [\n            \"Please generate complex inputs to test the function.\",\n            \"Please generate corner case inputs to test the function.\",\n            \"Please generate difficult inputs to test the function.\",\n        ]\n        self.iteration = 20\n        openai.api_key = os.environ.get(\"OPENAI_API_KEY\", \"dummy\")\n\n    def seed_selection(self) -> List:\n        # get 5 for now.\n        return random.sample(self.seed_pool, k=min(len(self.seed_pool), 5))\n\n    @staticmethod\n    def _parse_ret(ret: Dict) -> List:\n        rets = []\n        output = ret[\"choices\"][0][\"message\"][\"content\"]\n        if \"```\" in output:\n            for x in output.split(\"```\")[1].splitlines():\n                if x.strip() == \"\":\n                    continue\n                try:\n                    # remove comments\n                    input = ast.literal_eval(f\"[{x.split('#')[0].strip()}]\")\n                except:  # something wrong.\n                    continue\n                rets.append(input)\n        return rets\n\n    def chatgpt_generate(self, selected_inputs: List) -> List:\n        # append the groundtruth function\n        # actually it can be any function (maybe we can generate inputs for each llm generated code individually)\n        message = f\"Here is a function that we want to test:\\n```\\n{self.gd_code}\\n```\"\n        str_inputs = \"\\n\".join(\n            [\n                \", \".join([f\"'{to_raw(i)}'\" if type(i) == str else str(i) for i in x])\n                for x in selected_inputs\n            ]\n        )\n        message += f\"\\nThese are some example inputs used to test the function:\\n```\\n{str_inputs}\\n```\"\n        message += f\"\\n{random.choice(self.prompt_messages)}\"\n        config = create_chatgpt_config(message, 256)\n        ret = request_chatgpt_engine(config)\n        return self._parse_ret(ret)\n\n    def generate(self, num: int):\n        while len(self.new_inputs) < num and self.iteration >= 0:\n            seeds = self.seed_selection()\n            new_inputs = self.chatgpt_generate(seeds)\n            for new_input in new_inputs:\n                if hash(str(new_input)) not in self.seed_hash:\n                    if trusted_check_exec(self.contract, [new_input], self.entry_point):\n                        self.seed_pool.append(new_input)\n                        self.seed_hash.add(hash(str(new_input)))\n                        self.new_inputs.append(new_input)\n            self.iteration -= 1\n        return self.new_inputs[:num]", ""]}
{"filename": "evalplus/gen/util/__init__.py", "chunked_list": ["import time\n\n\ndef trusted_exec(code, inputs, entry_point, record_time=False):\n    \"\"\"Execute trusted code in place.\"\"\"\n    exec_globals = {}\n    exec(code, exec_globals)\n    fn = exec_globals[entry_point]\n\n    rtime = []\n    ret = []\n    for inp in inputs:\n        if record_time:\n            start = time.time()\n            ret.append(fn(*inp))\n            rtime.append(time.time() - start)\n        else:\n            ret.append(fn(*inp))\n\n    if record_time:\n        return ret, rtime\n    else:\n        return ret", "\n\ndef trusted_check_exec(code, inputs, entry_point):\n    \"\"\"Check trusted_exec success.\"\"\"\n    try:\n        trusted_exec(code, inputs, entry_point)\n    except Exception:\n        return False\n    return True\n", ""]}
{"filename": "evalplus/gen/util/api_request.py", "chunked_list": ["import signal\nimport time\nfrom typing import Dict\n\nimport openai\n\n# TODO Codex request if we need it.\n\n\ndef create_chatgpt_config(\n    message: str,\n    max_tokens: int,\n    temperature: float = 1,\n    batch_size: int = 1,\n    system_message: str = \"You are a helpful assistant.\",\n    model: str = \"gpt-3.5-turbo\",\n) -> Dict:\n    config = {\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"n\": batch_size,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": message},\n        ],\n    }\n    return config", "\ndef create_chatgpt_config(\n    message: str,\n    max_tokens: int,\n    temperature: float = 1,\n    batch_size: int = 1,\n    system_message: str = \"You are a helpful assistant.\",\n    model: str = \"gpt-3.5-turbo\",\n) -> Dict:\n    config = {\n        \"model\": model,\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature,\n        \"n\": batch_size,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": message},\n        ],\n    }\n    return config", "\n\ndef handler(signum, frame):\n    # swallow signum and frame\n    raise Exception(\"end of time\")\n\n\ndef request_chatgpt_engine(config) -> Dict:\n    ret = None\n    while ret is None:\n        try:\n            signal.signal(signal.SIGALRM, handler)\n            signal.alarm(100)\n            ret = openai.ChatCompletion.create(**config)\n            signal.alarm(0)\n        except openai.error.InvalidRequestError as e:\n            print(e)\n            signal.alarm(0)\n        except openai.error.RateLimitError as e:\n            print(\"Rate limit exceeded. Waiting...\")\n            signal.alarm(0)\n            time.sleep(5)\n        except openai.error.APIConnectionError as e:\n            print(\"API connection error. Waiting...\")\n            signal.alarm(0)\n            time.sleep(5)\n        except Exception as e:\n            print(\"Unknown error. Waiting...\")\n            print(e)\n            signal.alarm(0)\n            time.sleep(1)\n    return ret", ""]}
{"filename": "evalplus/_experimental/evaluate_runtime.py", "chunked_list": ["import math\nimport multiprocessing\nimport time\nfrom typing import Any, List, Union\n\nfrom evalplus.data import get_human_eval_plus\nfrom evalplus.eval.utils import (\n    TimeoutException,\n    create_tempdir,\n    reliability_guard,", "    create_tempdir,\n    reliability_guard,\n    swallow_io,\n    time_limit,\n)\n\nMAX_WARMUP_LIMIT = 5\nRUN_REPEAT = 25\n\n\ndef execute_for_runtime(\n    code: str, inputs: List, warmups: List, entry_point: str\n) -> Union[str, float]:\n    def unsafe_execute():\n        with create_tempdir():\n            # These system calls are needed when cleaning up tempdir.\n            import os\n            import shutil\n\n            rmtree = shutil.rmtree\n            rmdir = os.rmdir\n            chdir = os.chdir\n            # Disable functionalities that can make destructive changes to the test.\n            reliability_guard()\n            # load functions\n            exec_globals = {}\n            exec(code, exec_globals)\n            fn = exec_globals[entry_point]\n            try:\n                # warmup calls\n                for warmup in warmups:\n                    with swallow_io():\n                        fn(*warmup)\n\n                start_time = time.time()\n                # real call\n                with swallow_io():\n                    with time_limit(3):\n                        fn(*inputs)\n                duration = time.time() - start_time\n\n                result.append(duration)\n            except TimeoutException:\n                result.append(\"timed out\")\n            except BaseException as e:\n                result.append(\"thrown exception\")\n            # Needed for cleaning up.\n            shutil.rmtree = rmtree\n            os.rmdir = rmdir\n            os.chdir = chdir\n\n    manager = multiprocessing.Manager()\n    result = manager.list()\n    p = multiprocessing.Process(target=unsafe_execute)\n    p.start()\n    p.join(timeout=3 + 1)\n    if p.is_alive():\n        p.kill()\n    return result[0]", "\n\ndef execute_for_runtime(\n    code: str, inputs: List, warmups: List, entry_point: str\n) -> Union[str, float]:\n    def unsafe_execute():\n        with create_tempdir():\n            # These system calls are needed when cleaning up tempdir.\n            import os\n            import shutil\n\n            rmtree = shutil.rmtree\n            rmdir = os.rmdir\n            chdir = os.chdir\n            # Disable functionalities that can make destructive changes to the test.\n            reliability_guard()\n            # load functions\n            exec_globals = {}\n            exec(code, exec_globals)\n            fn = exec_globals[entry_point]\n            try:\n                # warmup calls\n                for warmup in warmups:\n                    with swallow_io():\n                        fn(*warmup)\n\n                start_time = time.time()\n                # real call\n                with swallow_io():\n                    with time_limit(3):\n                        fn(*inputs)\n                duration = time.time() - start_time\n\n                result.append(duration)\n            except TimeoutException:\n                result.append(\"timed out\")\n            except BaseException as e:\n                result.append(\"thrown exception\")\n            # Needed for cleaning up.\n            shutil.rmtree = rmtree\n            os.rmdir = rmdir\n            os.chdir = chdir\n\n    manager = multiprocessing.Manager()\n    result = manager.list()\n    p = multiprocessing.Process(target=unsafe_execute)\n    p.start()\n    p.join(timeout=3 + 1)\n    if p.is_alive():\n        p.kill()\n    return result[0]", "\n\ndef test_solution_runtime(\n    dataset: str = \"humaneval\",\n    task_id: str = \"HumanEval/0\",\n    impl: str = \"canonical\",\n    inputs: Union[str, List[List[Any]]] = \"base_input\",\n):\n    if \"humaneval\" in dataset:\n        problems, problem = get_human_eval_plus(), None\n        for p in problems:\n            if p[\"task_id\"] == task_id:\n                problem = p\n        assert problem != None, f\"invalid {task_id = }\"\n        entry_point = problem[\"entry_point\"]\n        impl = problem[\"prompt\"] + (\n            impl if impl != \"canonical\" else problem[\"canonical_solution\"]\n        )\n        if inputs == \"base_input\":\n            inputs = problem[\"base_input\"]\n\n        results = [1000, 1000]\n        for input_list in inputs:\n            # choose warmup input\n            warmups = []\n            for base_input_list in problem[\"base_input\"]:\n                if (\n                    hash(str(base_input_list)) != hash(str(input_list))\n                    and len(warmups) < MAX_WARMUP_LIMIT\n                ):\n                    warmups.append(base_input_list)\n            runtime_list = [\n                execute_for_runtime(impl, input_list, warmups, entry_point)\n                for _ in range(RUN_REPEAT)\n            ]\n            if any(type(x) != float for x in runtime_list):\n                print(f\"{task_id = } incorrect\")\n                return None, None\n\n            avg_runtime = sum(runtime_list) / len(runtime_list)\n            sd = math.sqrt(\n                sum((runtime - avg_runtime) ** 2 for runtime in runtime_list)\n                / (RUN_REPEAT - 1)\n            )\n            if sd < results[1]:\n                results[0] = avg_runtime\n                results[1] = sd\n\n        return results", ""]}
{"filename": "evalplus/_experimental/generate_big_input.py", "chunked_list": ["import json\nimport multiprocessing\nimport os\n\nfrom evalplus._experimental.type_mut_for_eff import TypedMutEffGen\nfrom evalplus.data import HUMANEVAL_PLUS_INPUTS_PATH, get_human_eval_plus\n\nHUMANEVAL_PLUS_BIG_INPUTS_PATH = \"/home/yuyao/eval-plus/HumanEvalPlusBigInputs\"\n\n\ndef main():\n    problems = get_human_eval_plus()\n    for p in problems:\n        print(f\"{p['task_id']}...\")\n        filename = p[\"task_id\"].replace(\"/\", \"_\")\n        big_input_path = os.path.join(\n            HUMANEVAL_PLUS_BIG_INPUTS_PATH, f\"{filename}.json\"\n        )\n\n        if os.path.exists(big_input_path):\n            continue\n        inputs = p[\"base_input\"]\n        signature = p[\"entry_point\"]\n        contract_code = p[\"prompt\"] + p[\"contract\"] + p[\"canonical_solution\"]\n\n        def input_generation(inputs, signature, contract_code):\n            try:\n                gen = TypedMutEffGen(inputs, signature, contract_code)\n                new_inputs = gen.generate()\n                results.append(new_inputs)\n            except:\n                with open(\"fail.txt\", \"a\") as f:\n                    f.write(f\"{signature} failed\")\n                results.append(\"fail\")\n\n        manager = multiprocessing.Manager()\n        results = manager.list()\n        proc = multiprocessing.Process(\n            target=input_generation, args=(inputs, signature, contract_code)\n        )\n        proc.start()\n        proc.join(timeout=300)\n        if proc.is_alive():\n            proc.terminate()\n            proc.kill()\n            continue\n        if len(results) == 0 or type(results[0]) == str:\n            continue\n        new_inputs = results[0]\n\n        new_input_dict = dict()\n        new_input_dict[\"task_id\"] = p[\"task_id\"]\n        new_input_dict[\"inputs\"] = []\n        new_input_dict[\"sd\"] = []\n        for item in new_inputs:\n            new_input_dict[\"inputs\"].append(item.inputs)\n            new_input_dict[\"sd\"].append(item.fluctuate_ratio)\n        with open(\n            os.path.join(HUMANEVAL_PLUS_BIG_INPUTS_PATH, f\"{filename}.json\"), \"w\"\n        ) as f:\n            json.dump(new_input_dict, f)", "\n\ndef main():\n    problems = get_human_eval_plus()\n    for p in problems:\n        print(f\"{p['task_id']}...\")\n        filename = p[\"task_id\"].replace(\"/\", \"_\")\n        big_input_path = os.path.join(\n            HUMANEVAL_PLUS_BIG_INPUTS_PATH, f\"{filename}.json\"\n        )\n\n        if os.path.exists(big_input_path):\n            continue\n        inputs = p[\"base_input\"]\n        signature = p[\"entry_point\"]\n        contract_code = p[\"prompt\"] + p[\"contract\"] + p[\"canonical_solution\"]\n\n        def input_generation(inputs, signature, contract_code):\n            try:\n                gen = TypedMutEffGen(inputs, signature, contract_code)\n                new_inputs = gen.generate()\n                results.append(new_inputs)\n            except:\n                with open(\"fail.txt\", \"a\") as f:\n                    f.write(f\"{signature} failed\")\n                results.append(\"fail\")\n\n        manager = multiprocessing.Manager()\n        results = manager.list()\n        proc = multiprocessing.Process(\n            target=input_generation, args=(inputs, signature, contract_code)\n        )\n        proc.start()\n        proc.join(timeout=300)\n        if proc.is_alive():\n            proc.terminate()\n            proc.kill()\n            continue\n        if len(results) == 0 or type(results[0]) == str:\n            continue\n        new_inputs = results[0]\n\n        new_input_dict = dict()\n        new_input_dict[\"task_id\"] = p[\"task_id\"]\n        new_input_dict[\"inputs\"] = []\n        new_input_dict[\"sd\"] = []\n        for item in new_inputs:\n            new_input_dict[\"inputs\"].append(item.inputs)\n            new_input_dict[\"sd\"].append(item.fluctuate_ratio)\n        with open(\n            os.path.join(HUMANEVAL_PLUS_BIG_INPUTS_PATH, f\"{filename}.json\"), \"w\"\n        ) as f:\n            json.dump(new_input_dict, f)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "evalplus/_experimental/type_mut_for_eff.py", "chunked_list": ["import copy\nimport math\nimport random\nimport string\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\nfrom multipledispatch import dispatch\nfrom rich.progress import track\n\nfrom evalplus._experimental.evaluate_runtime import (", "\nfrom evalplus._experimental.evaluate_runtime import (\n    MAX_WARMUP_LIMIT,\n    RUN_REPEAT,\n    execute_for_runtime,\n)\nfrom evalplus.gen.mut_gen import MutateGen\n\nMUTATE_BOUND_SIZE = 5\nMAX_MULTI_STEP_SIZE = 1000", "MUTATE_BOUND_SIZE = 5\nMAX_MULTI_STEP_SIZE = 1000\nMAX_SEED_POOL = 10\n\nNoneType = type(None)\nMAX_SIZE = 80000\nVALUE_MAX = 1000000\n\n\n# decorator to use ingredients\nclass use_ingredient:\n    def __init__(self, prob: float):\n        assert 0 <= prob <= 0.95\n        self.prob = prob\n\n    def __call__(obj, func):\n        def wrapper(self, seed_input):\n            if random.random() < obj.prob and self.ingredients[type(seed_input)]:\n                return random.choice(list(self.ingredients[type(seed_input)]))\n            else:\n                return func(self, seed_input)\n\n        return wrapper", "\n# decorator to use ingredients\nclass use_ingredient:\n    def __init__(self, prob: float):\n        assert 0 <= prob <= 0.95\n        self.prob = prob\n\n    def __call__(obj, func):\n        def wrapper(self, seed_input):\n            if random.random() < obj.prob and self.ingredients[type(seed_input)]:\n                return random.choice(list(self.ingredients[type(seed_input)]))\n            else:\n                return func(self, seed_input)\n\n        return wrapper", "\n\nclass TestInput:\n    def __init__(self, inputs: List, runtime: float, sd: float):\n        self.inputs = inputs\n        self.sz = self.typed_size(inputs)\n        self.runtime = runtime\n        self.sd = sd\n        self.rank_sd = self.rank_sz = 1\n\n    def __str__(self):\n        return str(self.inputs)\n\n    @property\n    def fluctuate_ratio(self) -> float:\n        return self.sd / self.runtime * 100\n\n    @property\n    def rank(self) -> float:\n        return self.rank_sd * (self.rank_sz**0.8) if self.sz <= 2000 else self.rank_sd\n\n    @dispatch(NoneType)\n    def typed_size(self, _) -> int:\n        return 1\n\n    @dispatch(int)\n    def typed_size(self, _) -> int:\n        return 1\n\n    @dispatch(float)\n    def typed_size(self, _) -> int:\n        return 1\n\n    @dispatch(bool)\n    def typed_size(self, _) -> int:\n        return 1\n\n    @dispatch(str)\n    def typed_size(self, s: str) -> int:\n        return len(s)\n\n    @dispatch(list)\n    def typed_size(self, l: list) -> int:\n        return sum(self.typed_size(x) for x in l)\n\n    @dispatch(tuple)\n    def typed_size(self, t: tuple) -> int:\n        return sum(self.typed_size(x) for x in t)\n\n    @dispatch(set)\n    def typed_size(self, s: set) -> int:\n        return sum(self.typed_size(x) for x in s)\n\n    @dispatch(dict)\n    def typed_size(self, d: dict) -> int:\n        return sum(self.typed_size(x) for x in d.items())", "\n\nclass TypedMutEffGen(MutateGen):\n    def __init__(self, inputs: List, signature: str, contract_code: str):\n        super().__init__(inputs, signature, contract_code)\n\n        self.base_inputs = copy.deepcopy(inputs)\n        self.seed_pool: List[TestInput] = []\n        self.seed_hash: Set[str] = set()\n        for base_input in self.base_inputs:\n            avg, sd = self.test_efficiency(base_input)\n            assert avg != None and sd != None, \"base inputs not correct\"\n            self.insert_input(TestInput(base_input, avg, sd))\n            self.seed_hash.add(hash(str(base_input)))\n\n        self.ingredients = {\n            int: set(),\n            float: set(),\n            str: set(),\n        }\n        for x in inputs:\n            self.fetch_ingredient(x)\n\n    def insert_input(self, new_input: TestInput):\n        new_input_hash = hash(str(new_input))\n        if new_input_hash in self.seed_hash:\n            return\n        self.seed_pool.append(new_input)\n        self.seed_pool.sort(key=lambda x: x.fluctuate_ratio)\n        self.seed_hash.add(new_input_hash)\n\n        if len(self.seed_pool) > MAX_SEED_POOL:\n            self.seed_pool.sort(key=lambda x: x.fluctuate_ratio)\n            for i in range(len(self.seed_pool)):\n                self.seed_pool[i].rank_sd = i + 1\n            self.seed_pool.sort(key=lambda x: -x.sz)\n            for i in range(len(self.seed_pool)):\n                self.seed_pool[i].rank_sz = i + 1\n            self.seed_pool.sort(key=lambda x: x.rank)\n            seed_deleted = self.seed_pool[-1]\n            self.seed_hash.remove(hash(str(seed_deleted)))\n            self.seed_pool = self.seed_pool[:-1]\n\n    def test_efficiency(self, new_input: List) -> Tuple[Optional[float]]:\n        warmups = []\n        new_input_hash = hash(str(new_input))\n        for input_list in self.base_inputs:\n            if (\n                len(warmups) < MAX_WARMUP_LIMIT\n                and hash(str(input_list)) != new_input_hash\n            ):\n                warmups.append(input_list)\n        runtime_list = [\n            execute_for_runtime(self.contract_code, new_input, warmups, self.signature)\n            for _ in range(RUN_REPEAT)\n        ]\n        if any(type(x) != float for x in runtime_list):\n            return None, None\n        avg = sum(runtime_list) / RUN_REPEAT\n        sd = math.sqrt(sum((t - avg) ** 2 for t in runtime_list) / (RUN_REPEAT - 1))\n        return avg, sd\n\n    #########################\n    # Type-aware generation #\n    #########################\n    @dispatch(NoneType)\n    def typed_gen(self, _):\n        return None\n\n    @dispatch(int)\n    def typed_gen(self, _):\n        @use_ingredient(0.5)\n        def _impl(*_):\n            return random.randint(-VALUE_MAX, VALUE_MAX)\n\n        return _impl(self, _)\n\n    @dispatch(float)\n    def typed_gen(self, _):\n        @use_ingredient(0.5)\n        def _impl(*_):\n            return random.uniform(-VALUE_MAX, VALUE_MAX)\n\n        return _impl(self, _)\n\n    @dispatch(bool)\n    def typed_gen(self, _):\n        return random.choice([True, False])\n\n    @dispatch(str)\n    def typed_gen(self, _):\n        @use_ingredient(0.5)\n        def _impl(*_):\n            return \"\".join(\n                random.choice(string.ascii_letters)\n                for _ in range(random.randint(0, 10))\n            )\n\n        return _impl(self, _)\n\n    def any_gen(self):\n        # weighted choose\n        choice = random.choices(\n            [\n                True,\n                1,\n                1.1,\n                \"str\",\n                [],  # list\n                tuple(),  # tuple\n                dict(),  # dict\n                None,  # None\n            ],\n            [0.2, 0.2, 0.2, 0.2, 0.05, 0.05, 0.05, 0.05],\n        )[0]\n        return self.typed_gen(choice)\n\n    @dispatch(list)\n    def typed_gen(self, _):\n        ret = []\n        size = random.randint(0, 10)\n        if random.randint(0, 4) == 0:  # heterogeneous\n            for _ in range(size):\n                ret.append(self.any_gen())\n        else:  # homogeneous\n            t = random.choice([bool(), int(), float(), str()])\n            for _ in range(size):\n                ret.append(self.typed_gen(t))\n        return ret\n\n    @dispatch(tuple)\n    def typed_gen(self, _):\n        return tuple(self.typed_gen([]))\n\n    # NOTE: disable set for now as Steven is too weak in Python (/s)\n    # @dispatch(set)\n    # def typed_gen(self, _):\n    #     return set(self.typed_gen([]))\n\n    @dispatch(dict)\n    def typed_gen(self, _):\n        ret = dict()\n        values = self.typed_gen([])\n        # NOTE: Assumption: nobody uses dict with heterogeneous keys\n        # NOTE: Assumption: nobody uses dict with boolean keys\n        key_type = random.choice([int(), float(), str()])\n        for v in values:\n            ret[self.typed_gen(key_type)] = self.typed_gen(v)\n        return ret\n\n    ########################\n    # Type-aware mutation  #\n    ########################\n    # Simple primitives\n    @dispatch(int)\n    def typed_mutate(self, seed_input: int):\n        @use_ingredient(0.1)\n        def _impl(_, seed_input: int):\n            prob = random.uniform(0, 1)\n            if 0 <= prob < 0.2:\n                return seed_input * 2\n            elif 0.2 <= prob < 0.9:\n                return random.randint(-VALUE_MAX, VALUE_MAX)\n            else:\n                return seed_input + 5\n\n        return _impl(self, seed_input)\n\n    @dispatch(float)\n    def typed_mutate(self, seed_input: float):\n        @use_ingredient(0.1)\n        def _impl(_, seed_input: float):\n            prob = random.uniform(0, 1)\n            if 0 <= prob < 0.2:\n                return seed_input * (2 + random.uniform(-0.5, 0.5))\n            elif 0.2 <= prob < 0.9:\n                return random.uniform(-VALUE_MAX, VALUE_MAX)\n            else:\n                return seed_input + 5.0\n\n        return _impl(self, seed_input)\n\n    @dispatch(bool)\n    def typed_mutate(self, seed_input: bool):\n        return random.choice([True, False])\n\n    @dispatch(NoneType)\n    def typed_mutate(self, seed_input: NoneType):\n        return None\n\n    # List-like\n    @dispatch(list)\n    def typed_mutate(self, seed_input: List):\n        if len(seed_input) == 0:\n            return self.typed_gen([])\n\n        choice = random.randint(1, 3)\n        idx = random.randint(0, len(seed_input) - 1)\n        if choice == 1 and 0 < len(seed_input) < MAX_SIZE:  # length *= 1.1\n            old_length = len(seed_input)\n            new_length = math.ceil(old_length * 1.1)\n            for _ in range(new_length - old_length):\n                seed_input.insert(\n                    random.randint(0, len(seed_input) - 1),\n                    self.typed_mutate(seed_input[idx]),\n                )\n        elif choice == 2 and 0 < len(seed_input) < MAX_SIZE:  # repeat, length *= 1.1\n            old_length = len(seed_input)\n            new_length = math.ceil(old_length * 1.1)\n            for _ in range(new_length - old_length):\n                seed_input.append(seed_input[idx])\n        else:  # inplace element change, large_scale\n            for idx in range(len(seed_input)):\n                if random.uniform(0, 1) > 0.7:\n                    seed_input[idx] = self.typed_mutate(seed_input[idx])\n        return seed_input\n\n    @dispatch(tuple)\n    def typed_mutate(self, seed_input: Tuple):\n        return tuple(self.typed_mutate(list(seed_input)))\n\n    # String\n    @dispatch(str)\n    def typed_mutate(self, seed_input: str):\n        @use_ingredient(0.1)\n        def _impl(_, seed_input: str):\n            choice = random.randint(0, 2) if seed_input else 0\n            if (\n                choice <= 1 and self.ingredients[str]\n            ):  # insert ingredients, length *= 1.1\n                new_length = math.ceil(len(seed_input) * 1.1)\n                while len(seed_input) < new_length:\n                    idx = random.randint(0, len(seed_input))\n                    seed_input = (\n                        seed_input[:idx]\n                        + random.choice(list(self.ingredients[str]))\n                        + seed_input[idx:]\n                    )\n                return seed_input\n            # other choices assume len(seed_input) > 0\n            elif choice == 2:  # inplace mutation, large_scale\n                ch_list = []\n                for i in range(len(seed_input)):\n                    if random.uniform(0, 1) > 0.7:\n                        ch_list.append(random.choice(string.ascii_letters))\n                    else:\n                        ch_list.append(seed_input[i])\n                return \"\".join(ch_list)\n\n            # random char\n            return self.typed_gen(str())\n\n        return _impl(self, seed_input)\n\n    # Set\n    @dispatch(set)\n    def typed_mutate(self, seed_input: Set):\n        return set(self.typed_mutate(list(seed_input)))\n\n    # Dict\n    @dispatch(dict)\n    def typed_mutate(self, seed_input: Dict):\n        if len(seed_input) == 0:\n            return self.typed_gen(dict())\n\n        choice = random.randint(1, 2)\n        if choice == 1:  # add a kv\n            k = self.typed_mutate(random.choice(list(seed_input.keys())))\n            v = self.typed_mutate(random.choice(list(seed_input.values())))\n            seed_input[k] = v\n        elif choice == 2:  # inplace value change\n            k0, v0 = random.choice(list(seed_input.items()))\n            seed_input[k0] = self.typed_mutate(v0)\n        return seed_input\n\n    ############################################\n    # Fetching ingredients to self.ingredients #\n    ############################################\n    def fetch_ingredient(self, seed_input):\n        self.typed_fetch(seed_input)\n\n    @dispatch(int)\n    def typed_fetch(self, seed_input: int):\n        self.ingredients[int].add(seed_input)\n\n    @dispatch(float)\n    def typed_fetch(self, seed_input: float):\n        self.ingredients[float].add(seed_input)\n\n    @dispatch(str)\n    def typed_fetch(self, seed_input: str):\n        self.ingredients[str].add(seed_input)\n        for token in seed_input.strip().split():\n            self.ingredients[str].add(token)\n\n    # List-like\n    def _fetch_list_like(self, seed_input):\n        for x in seed_input:\n            if self.typed_fetch.dispatch(type(x)):\n                self.fetch_ingredient(x)\n\n    @dispatch(list)\n    def typed_fetch(self, seed_input: List):\n        self._fetch_list_like(seed_input)\n\n    @dispatch(tuple)\n    def typed_fetch(self, seed_input: Tuple):\n        self._fetch_list_like(seed_input)\n\n    # NOTE: disable set for now as Steven is too weak in Python (/s)\n    # @dispatch(set)\n    # def typed_fetch(self, seed_input: Set):\n    #     self._fetch_list_like(seed_input)\n\n    # Dict\n    @dispatch(dict)\n    def typed_fetch(self, seed_input: Dict):\n        self._fetch_list_like(seed_input.keys())\n        self._fetch_list_like(seed_input.values())\n\n    # Type-aware concatenation\n\n    @dispatch(int, int)\n    def concat(x: int, y: int):\n        return x + y\n\n    @dispatch(float, float)\n    def concat(x: float, y: float):\n        return x + y\n\n    @dispatch(bool, bool)\n    def concat(x: bool, y: bool):\n        return random.choice([x, y])\n\n    @dispatch(NoneType, NoneType)\n    def concat(x: NoneType, y: NoneType):\n        return None\n\n    @dispatch(list, list)\n    def concat(x: list, y: list):\n        choice = random.randint(0, 1)\n        return (\n            copy.deepcopy(x) + copy.deepcopy(y)\n            if choice == 0\n            else copy.deepcopy(y) + copy.deepcopy(x)\n        )\n\n    @dispatch(str, str)\n    def concat(x: str, y: str):\n        choice = random.randint(0, 1)\n        return x + y if choice == 0 else y + x\n\n    @dispatch(set, set)\n    def concat(x: set, y: set):\n        return x.union(y)\n\n    @dispatch(dict, dict)\n    def concat(x: dict, y: dict):\n        return x.update(y)\n\n    def mutate(self, seed: TestInput) -> List[Any]:\n        new_input = copy.deepcopy(seed.inputs)\n\n        for _ in range(20):\n            prob = random.uniform(0, 1)\n            if 0 <= prob < 0.1 and seed.sz <= MAX_SIZE:\n                another_seed = random.choice(self.seed_pool).inputs\n                new_input = [\n                    self.concat(new_input[i], another_seed[i])\n                    for i in range(len(new_input))\n                ]\n            else:\n                for i in range(len(new_input)):\n                    new_input[i] = self.typed_mutate(new_input[i])\n\n        return new_input\n\n    def generate(self) -> List[TestInput]:\n        for _ in track(range(40)):\n            seed = self.seed_selection()\n            new_input = self.mutate(seed)\n            # print(len(new_input[0]))\n            avg, sd = self.test_efficiency(new_input)\n            if avg != None and sd != None:\n                self.insert_input(TestInput(new_input, avg, sd))\n        return self.seed_pool", "\n\nif __name__ == \"__main__\":\n    from evalplus.data import get_human_eval_plus\n\n    problems = get_human_eval_plus()\n    for p in problems[43:44]:\n        inputs = p[\"base_input\"]\n        entry_point = p[\"entry_point\"]\n        contract = p[\"prompt\"] + p[\"contract\"] + p[\"canonical_solution\"]\n        gen = TypedMutEffGen(inputs, entry_point, contract)\n        new_inputs = gen.generate()\n        for i, new_input in enumerate(new_inputs):\n            print(f\"New input {i}: sz: {new_input.sz}\")\n            if new_input.sz <= 10:\n                print(new_input.inputs)\n            print(\n                f\"- Runtime: {new_input.runtime}, Sd: {new_input.sd}, Per: {new_input.fluctuate_ratio}\"\n            )", ""]}
{"filename": "evalplus/_experimental/evaluate_coverage.py", "chunked_list": ["import argparse\nimport importlib\nimport inspect\nimport multiprocessing\nimport os\nimport sys\nfrom io import StringIO\nfrom typing import Any, Callable, List, Union\n\nimport coverage", "\nimport coverage\n\nfrom evalplus.data import get_human_eval_plus\nfrom evalplus.eval import construct_inputs_sig\nfrom evalplus.eval.utils import reliability_guard, swallow_io, time_limit\n\n\nclass Capturing(list):\n    def __enter__(self):\n        self._stdout = sys.stdout\n        sys.stdout = self._stringio = StringIO()\n        return self\n\n    def __exit__(self, *args):\n        self.extend(self._stringio.getvalue().splitlines())\n        del self._stringio\n        sys.stdout = self._stdout", "class Capturing(list):\n    def __enter__(self):\n        self._stdout = sys.stdout\n        sys.stdout = self._stringio = StringIO()\n        return self\n\n    def __exit__(self, *args):\n        self.extend(self._stringio.getvalue().splitlines())\n        del self._stringio\n        sys.stdout = self._stdout", "\n\ndef parse_lcov(outputs: List[str], func: Callable, mode: str = \"branch\"):\n    switch, extracted_outputs = False, []\n    for line in outputs:\n        if switch == False and \"tmp_src\" in line:\n            switch = True\n        if switch == True and \"end_of_record\" in line:\n            switch = False\n        if switch:\n            extracted_outputs.append(line)\n\n    src, start_lineno = inspect.getsourcelines(func)\n    end_lineno = start_lineno + len(src) - 1\n\n    if mode == \"branch\":\n        branch, branch_covered = [], []\n        for line in extracted_outputs:\n            if line.startswith(\"BRDA\"):\n                # BRDA format: BR:<lineno>,<blockno>,<branchno>,<taken>\n                lineno, blockno, branchno, taken = line[5:].split(\",\")\n                branch_sig = f\"BR:{lineno},{blockno},{branchno}\"\n                branch.append(branch_sig)\n                if taken not in [\"0\", \"-\"]:\n                    branch_covered.append(branch_sig)\n        per = 1.0 if len(branch) == 0 else len(branch_covered) / len(branch)\n        return per, branch, branch_covered\n    else:\n        not_covered_lines = []\n        for line in extracted_outputs:\n            if line.startswith(\"DA\"):\n                # DA format: DA:<lineno>,<exec_count>[,...]\n                lineno, exec_count = line[3:].split(\",\")[:2]\n                if start_lineno <= int(lineno) <= end_lineno:\n                    if exec_count == \"0\":\n                        not_covered_lines.append(int(lineno))\n        for lineno in not_covered_lines:\n            line = src[lineno - start_lineno]\n            if line.strip() != \"\" and \"def\" not in line:\n                src[lineno - start_lineno] = line[:-1] + \"  # Not executed\\n\"\n        return \"\".join(src)", "\n\ndef test_code_coverage(\n    code: str, inputs: List[List[Any]], entry_point: str, mode=\"branch\"\n):\n    def safety_test(code: str, inputs: List[List[Any]], entry_point: str):\n        for input_list in inputs:\n            code += f\"{entry_point}({construct_inputs_sig(input_list)})\\n\"\n        reliability_guard()\n        try:\n            with swallow_io():\n                with time_limit(1):\n                    exec(code, {})\n        except:\n            sys.exit(1)\n\n    p = multiprocessing.Process(target=safety_test, args=(code, inputs, entry_point))\n    p.start()\n    p.join()\n    safe = p.exitcode == 0\n    if p.is_alive():\n        p.terminate()\n        p.kill()\n    if not safe:\n        print(\"Potentially dangerous code, refuse coverage test.\")\n        return None\n\n    with open(\"tmp_src.py\", \"w\") as f:\n        f.write(code)\n    import tmp_src\n\n    importlib.reload(tmp_src)\n    func = getattr(tmp_src, f\"{entry_point}\", None)\n    assert func != None, f\"{entry_point = } not exist\"\n\n    cov = coverage.Coverage(branch=True)\n    cov.start()\n    with swallow_io():\n        for input_list in inputs:\n            func(*input_list)\n    cov.stop()\n    with Capturing() as outputs:\n        cov.lcov_report(outfile=\"-\")\n\n    ret = parse_lcov(outputs, func, mode)\n\n    os.remove(\"tmp_src.py\")\n    return ret", "\n\ndef test_solution_coverage(\n    dataset: str = \"HumanEvalPlus\",\n    task_id: str = \"HumanEval/0\",\n    impl: str = \"canonical\",\n    inputs: Union[str, List[List[Any]]] = \"base_input\",\n    mode: str = \"branch\",\n):\n    \"\"\"\n    Parameters:\n    * dataset: {None, \"HumanEval\", \"HumanEvalPlus\"}\n    * task_id: ralated to dataset\n    * impl: {\"canonical\", source code}\n    * inputs: {\"base_inputs\", list}\n    * mode: {\"branch\"}, will support \"line\" for coverage-guided LLM test generation\n    \"\"\"\n    if \"HumanEval\" in dataset:\n        problems, problem = get_human_eval_plus(), None\n        for p in problems:\n            if p[\"task_id\"] == task_id:\n                problem = p\n        assert problem != None, f\"invalid {task_id = }\"\n        entry_point = problem[\"entry_point\"]\n        code = problem[\"prompt\"] + (\n            impl if impl != \"canonical\" else problem[\"canonical_solution\"]\n        )\n        if inputs == \"base_input\":\n            inputs = problem[\"base_input\"]\n    else:\n        raise NotImplementedError\n\n    return test_code_coverage(code, inputs, entry_point, mode)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--mode\", type=str, default=\"branch\", choices=[\"line\", \"branch\"]\n    )\n    args = parser.parse_args()\n\n    if args.mode == \"branch\":\n        for i in range(0, 164):\n            task_id = f\"HumanEval/{i}\"\n            branch, branch_covered = test_solution_coverage(\n                dataset=\"HumanEval\", task_id=task_id, mode=\"branch\"\n            )\n            per = 1.0 if len(branch) == 0 else len(branch_covered) / len(branch)\n            if per != 1.0:\n                print(i, per, len(branch_covered), len(branch))\n    else:\n        for i in range(0, 164):\n            task_id = f\"HumanEval/{i}\"\n            annotated_code = test_solution_coverage(\n                dataset=\"HumanEval\", task_id=task_id, mode=\"line\"\n            )\n            if \"Not executed\" in annotated_code:\n                print(f\"{task_id = }\")\n                print(annotated_code)", ""]}
{"filename": "evalplus/eval/__init__.py", "chunked_list": ["import itertools\nimport math\nimport multiprocessing\nimport time\nfrom multiprocessing import Array, Value\nfrom typing import Any, Dict, List, Tuple, Union\n\nimport numpy as np\n\nfrom evalplus.data import to_raw", "\nfrom evalplus.data import to_raw\nfrom evalplus.eval.utils import (\n    create_tempdir,\n    reliability_guard,\n    swallow_io,\n    time_limit,\n)\n\n\ndef compatible_eval_result(results: Dict) -> Dict:\n    # compatibility\n    for task_results in results[\"eval\"].values():\n        # update the \"files\" field to \"nfiles\"\n        if \"files\" in task_results and \"nfiles\" not in task_results:\n            task_results[\"nfiles\"] = len(task_results.pop(\"files\"))\n    return results", "\n\ndef compatible_eval_result(results: Dict) -> Dict:\n    # compatibility\n    for task_results in results[\"eval\"].values():\n        # update the \"files\" field to \"nfiles\"\n        if \"files\" in task_results and \"nfiles\" not in task_results:\n            task_results[\"nfiles\"] = len(task_results.pop(\"files\"))\n    return results\n", "\n\n# unbiased estimator from https://github.com/openai/human-eval\ndef estimate_pass_at_k(\n    num_samples: Union[int, List[int], np.ndarray],\n    num_correct: Union[List[int], np.ndarray],\n    k: int,\n) -> np.ndarray:\n    \"\"\"\n    Estimates pass@k of each problem and returns them in an array.\n    \"\"\"\n\n    def estimator(n: int, c: int, k: int) -> float:\n        \"\"\"\n        Calculates 1 - comb(n - c, k) / comb(n, k).\n        \"\"\"\n        if n - c < k:\n            return 1.0\n        return 1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1))\n\n    if isinstance(num_samples, int):\n        num_samples_it = itertools.repeat(num_samples, len(num_correct))\n    else:\n        assert len(num_samples) == len(num_correct)\n        num_samples_it = iter(num_samples)\n\n    return np.array(\n        [estimator(int(n), int(c), k) for n, c in zip(num_samples_it, num_correct)]\n    )", "\n\ndef construct_inputs_sig(inputs: list) -> str:\n    str_builder = \"\"\n    for x in inputs:\n        if type(x) == str:\n            str_builder += f\"'{to_raw(x)}',\"\n        else:\n            str_builder += f\"{x},\"\n    return str_builder[:-1]", "\n\n# oracle for 032\ndef _poly(xs: list, x: float):\n    \"\"\"\n    Evaluates polynomial with coefficients xs at point x.\n    return xs[0] + xs[1] * x + xs[1] * x^2 + .... xs[n] * x^n\n    \"\"\"\n    return sum([coeff * math.pow(x, i) for i, coeff in enumerate(xs)])\n", "\n\nSUCCESS = \"success\"\nFAILED = \"failed\"\nTIMEOUT = \"timed out\"\n\n_SUCCESS = 0\n_FAILED = 1\n_TIMEOUT = 2\n_UNKNOWN = 3", "_TIMEOUT = 2\n_UNKNOWN = 3\n\n_mapping = {_SUCCESS: SUCCESS, _FAILED: FAILED, _TIMEOUT: TIMEOUT, _UNKNOWN: None}\n\n\ndef is_floats(x) -> bool:\n    # check if it is float; List[float]; Tuple[float]\n    if isinstance(x, float):\n        return True\n    if isinstance(x, (list, tuple)):\n        return all(isinstance(i, float) for i in x)\n    if isinstance(x, np.ndarray):\n        return x.dtype == np.float64 or x.dtype == np.float32\n    return False", "\n\ndef unsafe_execute(\n    entry_point: str,\n    code: str,\n    inputs,\n    expected: List,\n    time_limits,\n    atol,\n    fast_check,\n    stat: Value,\n    details: Array,\n    progress: Value,\n):\n    with create_tempdir():\n        # These system calls are needed when cleaning up tempdir.\n        import os\n        import shutil\n\n        rmtree = shutil.rmtree\n        rmdir = os.rmdir\n        chdir = os.chdir\n        # Disable functionalities that can make destructive changes to the test.\n        # allow only 4GB memory usage\n        maximum_memory_bytes = 4 * 1024 * 1024 * 1024\n        reliability_guard(maximum_memory_bytes=maximum_memory_bytes)\n        exec_globals = {}\n        try:\n            with swallow_io():\n                exec(code, exec_globals)\n                fn = exec_globals[entry_point]\n                for i, inp in enumerate(inputs):\n                    try:\n                        with time_limit(time_limits[i]):\n                            out = fn(*inp)\n\n                        exp = expected[i]\n                        exact_match = out == exp\n\n                        if \"find_zero\" == entry_point:\n                            assert _poly(*out, inp) <= atol\n\n                        if atol == 0 and is_floats(exp):\n                            atol = 1e-6  # enforce atol for float comparison\n                        if not exact_match and atol != 0:\n                            np.testing.assert_allclose(out, exp, atol=atol)\n                        else:\n                            assert exact_match\n                    except BaseException:\n                        if fast_check:\n                            raise\n\n                        details[i] = False\n                        progress.value += 1\n                        continue\n\n                    details[i] = True\n                    progress.value += 1\n            stat.value = _SUCCESS\n        except BaseException:\n            stat.value = _FAILED\n        # Needed for cleaning up.\n        shutil.rmtree = rmtree\n        os.rmdir = rmdir\n        os.chdir = chdir", "\n\ndef untrusted_check(\n    code: str,\n    inputs: List[Any],\n    entry_point: str,\n    expected,\n    atol,\n    ref_time: List[float],\n    fast_check: bool = False,\n    min_time_limit: float = 0.1,\n    gt_time_limit_factor: float = 2.0,\n) -> Tuple[str, np.ndarray]:\n    time_limits = [max(min_time_limit, gt_time_limit_factor * t) for t in ref_time]\n    timeout = sum(time_limits) + 1\n    if not fast_check:\n        timeout += 1  # extra time for data collection\n\n    # shared memory objects\n    progress = Value(\"i\", 0)\n    stat = Value(\"i\", _UNKNOWN)\n    details = Array(\"b\", [False for _ in range(len(inputs))])\n\n    p = multiprocessing.Process(\n        target=unsafe_execute,\n        args=(\n            entry_point,\n            code,\n            inputs,\n            expected,\n            time_limits,\n            atol,\n            fast_check,\n            # return values\n            stat,\n            details,\n            progress,\n        ),\n    )\n    p.start()\n    p.join(timeout=timeout + 1)\n    if p.is_alive():\n        p.terminate()\n        time.sleep(0.1)\n    if p.is_alive():\n        p.kill()\n        time.sleep(0.1)\n\n    stat = _mapping[stat.value]\n    details = details[: progress.value]\n\n    if not stat:\n        stat = TIMEOUT\n\n    if stat == SUCCESS:\n        if len(details) != len(inputs) or not all(details):\n            stat = FAILED\n\n    return stat, details", "\n\ndef evaluate_files(\n    files: List[str],\n    inputs: List,\n    expected: List,\n    entry_point: str,\n    atol: float,\n    ref_time: List[float],\n    fast_check: bool = False,\n    min_time_limit: float = 0.1,\n    gt_time_limit_factor: float = 2.0,\n) -> List[Tuple[str, List[bool]]]:\n    ret = []\n    # sort files by the id in name (i.e., \"../n.py\")\n    files = sorted(files, key=lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n    for file in files:\n        code = open(file, \"r\").read()\n        stat, det = untrusted_check(\n            code,\n            inputs,\n            entry_point,\n            expected=expected,\n            atol=atol,\n            ref_time=ref_time,\n            fast_check=fast_check,\n            min_time_limit=min_time_limit,\n            gt_time_limit_factor=gt_time_limit_factor,\n        )\n        ret.append((stat, det.tolist()))\n    return ret", ""]}
{"filename": "evalplus/eval/utils.py", "chunked_list": ["# Adopted from https://github.com/openai/human-eval\n\nimport contextlib\nimport faulthandler\nimport io\nimport os\nimport platform\nimport signal\nimport tempfile\nfrom typing import Optional", "import tempfile\nfrom typing import Optional\n\n\n@contextlib.contextmanager\ndef swallow_io():\n    stream = WriteOnlyStringIO()\n    with contextlib.redirect_stdout(stream):\n        with contextlib.redirect_stderr(stream):\n            with redirect_stdin(stream):\n                yield", "\n\n@contextlib.contextmanager\ndef time_limit(seconds: float):\n    def signal_handler(signum, frame):\n        raise TimeoutException(\"Timed out!\")\n\n    signal.setitimer(signal.ITIMER_REAL, seconds)\n    signal.signal(signal.SIGALRM, signal_handler)\n    try:\n        yield\n    finally:\n        signal.setitimer(signal.ITIMER_REAL, 0)", "\n\n@contextlib.contextmanager\ndef create_tempdir():\n    with tempfile.TemporaryDirectory() as dirname:\n        with chdir(dirname):\n            yield dirname\n\n\n@contextlib.contextmanager\ndef chdir(root):\n    if root == \".\":\n        yield\n        return\n    cwd = os.getcwd()\n    os.chdir(root)\n    try:\n        yield\n    except BaseException as exc:\n        raise exc\n    finally:\n        os.chdir(cwd)", "\n@contextlib.contextmanager\ndef chdir(root):\n    if root == \".\":\n        yield\n        return\n    cwd = os.getcwd()\n    os.chdir(root)\n    try:\n        yield\n    except BaseException as exc:\n        raise exc\n    finally:\n        os.chdir(cwd)", "\n\nclass TimeoutException(Exception):\n    pass\n\n\nclass WriteOnlyStringIO(io.StringIO):\n    \"\"\"StringIO that throws an exception when it's read from\"\"\"\n\n    def read(self, *args, **kwargs):\n        raise IOError\n\n    def readline(self, *args, **kwargs):\n        raise IOError\n\n    def readlines(self, *args, **kwargs):\n        raise IOError\n\n    def readable(self, *args, **kwargs):\n        \"\"\"Returns True if the IO object can be read.\"\"\"\n        return False", "\n\nclass redirect_stdin(contextlib._RedirectStream):  # type: ignore\n    _stream = \"stdin\"\n\n\ndef reliability_guard(maximum_memory_bytes: Optional[int] = None):\n    \"\"\"\n    This disables various destructive functions and prevents the generated code\n    from interfering with the test (e.g. fork bomb, killing other processes,\n    removing filesystem files, etc.)\n\n    WARNING\n    This function is NOT a security sandbox. Untrusted code, including, model-\n    generated code, should not be blindly executed outside of one. See the\n    Codex paper for more information about OpenAI's code sandbox, and proceed\n    with caution.\n    \"\"\"\n\n    if maximum_memory_bytes is not None:\n        import resource\n\n        resource.setrlimit(\n            resource.RLIMIT_AS, (maximum_memory_bytes, maximum_memory_bytes)\n        )\n        resource.setrlimit(\n            resource.RLIMIT_DATA, (maximum_memory_bytes, maximum_memory_bytes)\n        )\n        if not platform.uname().system == \"Darwin\":\n            resource.setrlimit(\n                resource.RLIMIT_STACK, (maximum_memory_bytes, maximum_memory_bytes)\n            )\n\n    faulthandler.disable()\n\n    import builtins\n\n    builtins.exit = None\n    builtins.quit = None\n\n    import os\n\n    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n\n    os.kill = None\n    os.system = None\n    os.putenv = None\n    os.remove = None\n    os.removedirs = None\n    os.rmdir = None\n    os.fchdir = None\n    os.setuid = None\n    os.fork = None\n    os.forkpty = None\n    os.killpg = None\n    os.rename = None\n    os.renames = None\n    os.truncate = None\n    os.replace = None\n    os.unlink = None\n    os.fchmod = None\n    os.fchown = None\n    os.chmod = None\n    os.chown = None\n    os.chroot = None\n    os.fchdir = None\n    os.lchflags = None\n    os.lchmod = None\n    os.lchown = None\n    os.getcwd = None\n    os.chdir = None\n    builtins.open = None\n\n    import shutil\n\n    shutil.rmtree = None\n    shutil.move = None\n    shutil.chown = None\n\n    import subprocess\n\n    subprocess.Popen = None  # type: ignore\n\n    __builtins__[\"help\"] = None\n\n    import sys\n\n    sys.modules[\"ipdb\"] = None\n    sys.modules[\"joblib\"] = None\n    sys.modules[\"resource\"] = None\n    sys.modules[\"psutil\"] = None\n    sys.modules[\"tkinter\"] = None", ""]}
