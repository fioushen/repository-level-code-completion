{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"autoresearcher\",\n    version=\"0.0.6\",\n    author=\"Eimen Hamedat\",\n    author_email=\"eimen.hamedat@gmail.com\",", "    author=\"Eimen Hamedat\",\n    author_email=\"eimen.hamedat@gmail.com\",\n    description=\"Automating scientic workflows with AI\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/eimenhmdt/autoresearcher\",\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",", "        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    packages=find_packages(),\n    python_requires=\">=3.8\",\n    include_package_data=True,\n    install_requires=[\n        \"openai==0.27.0\",\n        \"python-dotenv==1.0.0\",\n        \"requests==2.26.0\",", "        \"python-dotenv==1.0.0\",\n        \"requests==2.26.0\",\n        \"termcolor==1.1.0\",\n        \"jellyfish==0.11.2\",\n        \"tiktoken==0.3.3\",\n        \"setuptools>=42\",\n        \"wheel\"\n    ],\n)\n", ")\n"]}
{"filename": "autoresearcher/__init__.py", "chunked_list": ["from .workflows.literature_review.literature_review import literature_review\n"]}
{"filename": "autoresearcher/utils/generate_keyword_combinations.py", "chunked_list": ["from autoresearcher.llms.openai import openai_call\nfrom autoresearcher.utils.prompts import keyword_combination_prompt\n\n\n# Generate keyword combinations for a given research question\ndef generate_keyword_combinations(research_question):\n    \"\"\"\n    Generates keyword combinations for a given research question.\n    Args:\n      research_question (str): The research question to generate keyword combinations for.\n    Returns:\n      list: A list of keyword combinations for the given research question.\n    Examples:\n      >>> generate_keyword_combinations(\"What is the impact of AI on healthcare?\")\n      [\"AI healthcare\", \"impact AI healthcare\", \"AI healthcare impact\"]\n    \"\"\"\n    prompt = keyword_combination_prompt.format(research_question=research_question)\n    response = openai_call(prompt, use_gpt4=False, temperature=0, max_tokens=200)\n    combinations = response.split(\"\\n\")\n    return [\n        combination.split(\": \")[1]\n        for combination in combinations\n        if \": \" in combination\n    ]", ""]}
{"filename": "autoresearcher/utils/get_citations.py", "chunked_list": ["import os\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nEMAIL = os.getenv(\"EMAIL\", \"\")\nassert EMAIL, \"EMAIL environment variable is missing from .env\"\n\n\ndef get_citation_by_doi(doi):\n    \"\"\"\n    Retrieves a citation for a given DOI.\n    Args:\n      doi (str): The DOI of the citation to retrieve.\n    Returns:\n      str: The citation for the given DOI.\n    Raises:\n      ValueError: If the response is not valid JSON.\n    Notes:\n      Requires an email address to be set in the EMAIL environment variable.\n    Examples:\n      >>> get_citation_by_doi(\"10.1038/s41586-020-2003-7\")\n      \"Liu, Y., Chen, X., Han, M., Li, Y., Li, L., Zhang, J., ... & Zhang, Y. (2020). A SARS-CoV-2 protein interaction map reveals targets for drug repurposing. Nature, 581(7809), 561-570.\"\n    \"\"\"\n    url = f\"https://api.citeas.org/product/{doi}?email={EMAIL}\"\n    response = requests.get(url)\n    try:\n        data = response.json()\n        return data[\"citations\"][0][\"citation\"]\n    except ValueError:\n        return response.text", "\n\ndef get_citation_by_doi(doi):\n    \"\"\"\n    Retrieves a citation for a given DOI.\n    Args:\n      doi (str): The DOI of the citation to retrieve.\n    Returns:\n      str: The citation for the given DOI.\n    Raises:\n      ValueError: If the response is not valid JSON.\n    Notes:\n      Requires an email address to be set in the EMAIL environment variable.\n    Examples:\n      >>> get_citation_by_doi(\"10.1038/s41586-020-2003-7\")\n      \"Liu, Y., Chen, X., Han, M., Li, Y., Li, L., Zhang, J., ... & Zhang, Y. (2020). A SARS-CoV-2 protein interaction map reveals targets for drug repurposing. Nature, 581(7809), 561-570.\"\n    \"\"\"\n    url = f\"https://api.citeas.org/product/{doi}?email={EMAIL}\"\n    response = requests.get(url)\n    try:\n        data = response.json()\n        return data[\"citations\"][0][\"citation\"]\n    except ValueError:\n        return response.text", ""]}
{"filename": "autoresearcher/utils/__init__.py", "chunked_list": [""]}
{"filename": "autoresearcher/utils/prompts.py", "chunked_list": ["literature_review_prompt =  \"\"\"\"\n`reset`\n`no quotes`\n`no explanations`\n`no prompt`\n`no self-reference`\n`no apologies`\n`no filler`\n`just answer`\n", "`just answer`\n\nI will give you a list of research findings and a research question.\n\nSynthesize the list of research findings to generate a scientific literature review. Also, identify knowledge gaps and future research directions.\n\nMake sure to always reference every research finding you use with in-text citations in APA format using the source provided. \n\nOnly use the research findings I provide you with to create your literature review. Only give me the output and nothing else.\n", "Only use the research findings I provide you with to create your literature review. Only give me the output and nothing else.\n\nNow, using the concepts above, create a literature review for this research question '{research_question}' using the following research findings:\n\n{answer_list}\n\"\"\"\n\n\nextract_answer_prompt = \"\"\"\n`reset`", "extract_answer_prompt = \"\"\"\n`reset`\n`no quotes`\n`no explanations`\n`no prompt`\n`no self-reference`\n`no apologies`\n`no filler`\n`just answer`\n", "`just answer`\n\nI will give you the abstract of an academic paper. Extract the answer to this research question: {research_question} from the abstract.\n\nIf the answer is not in the abstract, then you are only allowed to respond with 'No answer found'.\n\nThis is the abstract: {abstract}\n\"\"\"\n\nkeyword_combination_prompt = \"\"\"", "\nkeyword_combination_prompt = \"\"\"\n`reset`\n`no quotes`\n`no explanations`\n`no prompt`\n`no self-reference`\n`no apologies`\n`no filler`\n`just answer`", "`no filler`\n`just answer`\n\nGenerate several keyword combinations based on the following research question: {research_question}. \nDon't generate more than 5 keyword combinations.\n\nThe output should be structured like this:\nWrite \"KeywordCombination:\" and then list the keywords like so \"Keyword,Keyword,Keyword\"\n\n\"\"\"", "\n\"\"\""]}
{"filename": "autoresearcher/utils/count_tokens.py", "chunked_list": ["import tiktoken\n\n\ndef count_tokens(text):\n    \"\"\"\n    Counts the number of tokens in a given text.\n    Args:\n      text (str): The text to tokenize.\n    Returns:\n      int: The number of tokens in `text`.\n    Examples:\n      >>> count_tokens(\"This is a sentence.\")\n      6\n    Notes:\n      The encoding used is determined by the `tiktoken.encoding_for_model` function.\n    \"\"\"\n    # encoding = tiktoken.get_encoding(\"cl100k_base\")\n    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n\n    tokens = encoding.encode(text)\n    return len(tokens)", ""]}
{"filename": "autoresearcher/llms/openai.py", "chunked_list": ["import openai\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\nassert OPENAI_API_KEY, \"OPENAI_API_KEY environment variable is missing from .env\"\n\n# Configure OpenAI", "\n# Configure OpenAI\nopenai.api_key = OPENAI_API_KEY\n\n\ndef openai_call(\n    prompt: str, use_gpt4: bool = False, temperature: float = 0.5, max_tokens: int = 100\n):\n    \"\"\"\n    Calls OpenAI API to generate a response to a given prompt.\n    Args:\n      prompt (str): The prompt to generate a response to.\n      use_gpt4 (bool, optional): Whether to use GPT-4 or GPT-3.5. Defaults to False.\n      temperature (float, optional): The temperature of the response. Defaults to 0.5.\n      max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 100.\n    Returns:\n      str: The generated response.\n    Examples:\n      >>> openai_call(\"Hello, how are you?\")\n      \"I'm doing great, thanks for asking!\"\n    Notes:\n      The OpenAI API key must be set in the environment variable OPENAI_API_KEY.\n    \"\"\"\n    if not use_gpt4:\n        # Call GPT-3.5 turbo model\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n        )\n        return response.choices[0].message.content.strip()\n    else:\n        # Call GPT-4 chat model\n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        response = openai.ChatCompletion.create(\n            model=\"gpt-4\",\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            n=1,\n            stop=None,\n        )\n        return response.choices[0].message.content.strip()", ""]}
{"filename": "autoresearcher/llms/__init__.py", "chunked_list": [""]}
{"filename": "autoresearcher/workflows/__init__.py", "chunked_list": [""]}
{"filename": "autoresearcher/workflows/literature_review/combine_answers.py", "chunked_list": ["from autoresearcher.llms.openai import openai_call\nfrom autoresearcher.utils.prompts import literature_review_prompt\nfrom autoresearcher.utils.count_tokens import count_tokens\n\n\n# Combine answers into a concise literature review using OpenAI API\ndef combine_answers(answers, research_question, use_gpt4=False, temperature=0.1):\n    \"\"\"\n    Combines a list of answers into a concise literature review using OpenAI API.\n    Args:\n      answers (list): A list of answers to combine.\n      research_question (str): The research question to use in the literature review.\n      use_gpt4 (bool, optional): Whether to use GPT-4 for the literature review. Defaults to False.\n      temperature (float, optional): The temperature to use for the OpenAI API. Defaults to 0.1.\n    Returns:\n      str: The literature review.\n    Examples:\n      >>> answers = [\"Answer 1\", \"Answer 2\"]\n      >>> research_question = \"What is the impact of AI on society?\"\n      >>> combine_answers(answers, research_question)\n      \"The impact of AI on society is significant. Answer 1...Answer 2...\"\n    \"\"\"\n    answer_list = \"\\n\\n\".join(answers)\n    prompt = literature_review_prompt.format(\n        research_question=research_question, answer_list=answer_list\n    )\n\n    # Calculate the tokens in the input\n    input_tokens = count_tokens(prompt)\n\n    # Calculate the remaining tokens for the response\n    remaining_tokens = 4080 - input_tokens\n    max_tokens = max(remaining_tokens, 0)\n    literature_review = openai_call(\n        prompt, use_gpt4=use_gpt4, temperature=temperature, max_tokens=max_tokens\n    )\n\n    return literature_review", ""]}
{"filename": "autoresearcher/workflows/literature_review/literature_review.py", "chunked_list": ["#!/usr/bin/env python3\nfrom termcolor import colored\nfrom autoresearcher.llms.openai import openai_call\nfrom autoresearcher.workflows.literature_review.extract_citations import (\n    extract_citations,\n)\nfrom autoresearcher.utils.generate_keyword_combinations import (\n    generate_keyword_combinations,\n)\nfrom autoresearcher.workflows.literature_review.combine_answers import combine_answers", ")\nfrom autoresearcher.workflows.literature_review.combine_answers import combine_answers\nfrom autoresearcher.workflows.literature_review.extract_answers_from_papers import (\n    extract_answers_from_papers,\n)\nfrom autoresearcher.data_sources.web_apis.semantic_scholar_loader import (\n    SemanticScholarLoader,\n)\n\n\ndef literature_review(research_question, output_file=None):\n    \"\"\"\n    Generates an academic literature review for a given research question.\n    Args:\n      research_question (str): The research question to generate a literature review for.\n      output_file (str, optional): The file path to save the literature review to.\n    Returns:\n      str: The generated literature review.\n    Examples:\n      >>> literature_review('What is the impact of AI on healthcare?')\n      Research question: What is the impact of AI on healthcare?\n      Auto Researcher initiated!\n      Generating keyword combinations...\n      Keyword combinations generated!\n      Fetching top 20 papers...\n      Top 20 papers fetched!\n      Extracting research findings from papers...\n      Research findings extracted!\n      Synthesizing answers...\n      Literature review generated!\n      Academic Literature Review: ...\n      References:\n      1. ...\n      Keyword combinations used to search for papers: 1. AI healthcare, 2. impact AI healthcare\n    \"\"\"\n    SemanticScholar = SemanticScholarLoader()\n\n    print(\n        colored(\n            f\"Research question: {research_question}\", \"yellow\", attrs=[\"bold\", \"blink\"]\n        )\n    )\n    print(colored(\"Auto Researcher initiated!\", \"yellow\"))\n\n    # Generate keyword combinations\n    print(colored(\"Generating keyword combinations...\", \"yellow\"))\n    keyword_combinations = generate_keyword_combinations(research_question)\n    print(colored(\"Keyword combinations generated!\", \"green\"))\n\n    # Fetch the top 20 papers for the research question\n    search_query = research_question\n    print(colored(\"Fetching top 20 papers...\", \"yellow\"))\n    top_papers = SemanticScholar.fetch_and_sort_papers(\n        search_query, keyword_combinations=keyword_combinations, year_range=\"2000-2023\"\n    )\n    print(colored(\"Top 20 papers fetched!\", \"green\"))\n\n    # Extract answers and from the top 20 papers\n    print(colored(\"Extracting research findings from papers...\", \"yellow\"))\n    answers = extract_answers_from_papers(top_papers, research_question)\n    print(colored(\"Research findings extracted!\", \"green\"))\n\n    # Combine answers into a concise academic literature review\n    print(colored(\"Synthesizing answers...\", \"yellow\"))\n    literature_review = combine_answers(answers, research_question)\n    print(colored(\"Literature review generated!\", \"green\"))\n\n    # Extract citations from answers and append a references list to the literature review\n    citations = extract_citations(answers)\n    references_list = \"\\n\".join(\n        [f\"{idx + 1}. {citation}\" for idx, citation in enumerate(citations)]\n    )\n    literature_review += \"\\n\\nReferences:\\n\" + references_list\n\n    # Append the keyword combinations to the literature review\n    literature_review += \"\\n\\nKeyword combinations used to search for papers: \"\n    literature_review += \", \".join(\n        [f\"{i+1}. {combination}\" for i, combination in enumerate(keyword_combinations)]\n    )\n\n    # Print the academic literature review\n    print(colored(\"Academic Literature Review:\", \"cyan\"), literature_review, \"\\n\")\n\n    # Save the literature review to a file if the output_file argument is provided\n    if output_file:\n        with open(output_file, \"w\") as f:\n            f.write(literature_review)\n        print(colored(f\"Literature review saved to {output_file}\", \"green\"))\n\n    return literature_review", "\n\ndef literature_review(research_question, output_file=None):\n    \"\"\"\n    Generates an academic literature review for a given research question.\n    Args:\n      research_question (str): The research question to generate a literature review for.\n      output_file (str, optional): The file path to save the literature review to.\n    Returns:\n      str: The generated literature review.\n    Examples:\n      >>> literature_review('What is the impact of AI on healthcare?')\n      Research question: What is the impact of AI on healthcare?\n      Auto Researcher initiated!\n      Generating keyword combinations...\n      Keyword combinations generated!\n      Fetching top 20 papers...\n      Top 20 papers fetched!\n      Extracting research findings from papers...\n      Research findings extracted!\n      Synthesizing answers...\n      Literature review generated!\n      Academic Literature Review: ...\n      References:\n      1. ...\n      Keyword combinations used to search for papers: 1. AI healthcare, 2. impact AI healthcare\n    \"\"\"\n    SemanticScholar = SemanticScholarLoader()\n\n    print(\n        colored(\n            f\"Research question: {research_question}\", \"yellow\", attrs=[\"bold\", \"blink\"]\n        )\n    )\n    print(colored(\"Auto Researcher initiated!\", \"yellow\"))\n\n    # Generate keyword combinations\n    print(colored(\"Generating keyword combinations...\", \"yellow\"))\n    keyword_combinations = generate_keyword_combinations(research_question)\n    print(colored(\"Keyword combinations generated!\", \"green\"))\n\n    # Fetch the top 20 papers for the research question\n    search_query = research_question\n    print(colored(\"Fetching top 20 papers...\", \"yellow\"))\n    top_papers = SemanticScholar.fetch_and_sort_papers(\n        search_query, keyword_combinations=keyword_combinations, year_range=\"2000-2023\"\n    )\n    print(colored(\"Top 20 papers fetched!\", \"green\"))\n\n    # Extract answers and from the top 20 papers\n    print(colored(\"Extracting research findings from papers...\", \"yellow\"))\n    answers = extract_answers_from_papers(top_papers, research_question)\n    print(colored(\"Research findings extracted!\", \"green\"))\n\n    # Combine answers into a concise academic literature review\n    print(colored(\"Synthesizing answers...\", \"yellow\"))\n    literature_review = combine_answers(answers, research_question)\n    print(colored(\"Literature review generated!\", \"green\"))\n\n    # Extract citations from answers and append a references list to the literature review\n    citations = extract_citations(answers)\n    references_list = \"\\n\".join(\n        [f\"{idx + 1}. {citation}\" for idx, citation in enumerate(citations)]\n    )\n    literature_review += \"\\n\\nReferences:\\n\" + references_list\n\n    # Append the keyword combinations to the literature review\n    literature_review += \"\\n\\nKeyword combinations used to search for papers: \"\n    literature_review += \", \".join(\n        [f\"{i+1}. {combination}\" for i, combination in enumerate(keyword_combinations)]\n    )\n\n    # Print the academic literature review\n    print(colored(\"Academic Literature Review:\", \"cyan\"), literature_review, \"\\n\")\n\n    # Save the literature review to a file if the output_file argument is provided\n    if output_file:\n        with open(output_file, \"w\") as f:\n            f.write(literature_review)\n        print(colored(f\"Literature review saved to {output_file}\", \"green\"))\n\n    return literature_review", "\n\nif __name__ == \"__main__\":\n    import sys\n\n    if len(sys.argv) > 2:\n        research_question = sys.argv[1]\n        output_file = sys.argv[2]\n    elif len(sys.argv) > 1:\n        research_question = sys.argv[1]\n        output_file = None\n    else:\n        raise ValueError(\n            \"No research question provided. Usage: python literature_review.py 'My research question' 'optional_output_file.txt'\"\n        )\n\n    literature_review(research_question, output_file)", ""]}
{"filename": "autoresearcher/workflows/literature_review/extract_citations.py", "chunked_list": ["# Extract bibliographical citations from answers\ndef extract_citations(answers):\n    \"\"\"\n    Extracts bibliographical citations from a list of answers.\n    Args:\n      answers (list): A list of strings containing answers.\n    Returns:\n      list: A list of strings containing bibliographical citations.\n    Examples:\n      >>> answers = [\"This is an answer. SOURCE: Smith, J. (2020).\",\n      ...            \"This is another answer. SOURCE: Jones, A. (2021).\"]\n      >>> extract_citations(answers)\n      [\"Smith, J. (2020)\", \"Jones, A. (2021)\"]\n    \"\"\"\n    citations = []\n    for answer in answers:\n        citation_start = answer.rfind(\"SOURCE: \")\n        if citation_start != -1:\n            citation = answer[citation_start + len(\"SOURCE: \") :]\n            citations.append(citation)\n    return citations", ""]}
{"filename": "autoresearcher/workflows/literature_review/__init__.py", "chunked_list": [""]}
{"filename": "autoresearcher/workflows/literature_review/extract_answers_from_papers.py", "chunked_list": ["#!/usr/bin/env python3\nfrom autoresearcher.utils.get_citations import get_citation_by_doi\nfrom termcolor import colored\nfrom autoresearcher.utils.prompts import extract_answer_prompt\nfrom autoresearcher.llms.openai import openai_call\n\n\n# Extract answers from paper abstracts\ndef extract_answers_from_papers(\n    papers, research_question, use_gpt4=False, temperature=0, max_tokens=150\n):\n    \"\"\"\n    Extracts answers from paper abstracts.\n    Args:\n      papers (list): A list of papers.\n      research_question (str): The research question to answer.\n      use_gpt4 (bool, optional): Whether to use GPT-4 for answer extraction. Defaults to False.\n      temperature (float, optional): The temperature for GPT-4 answer extraction. Defaults to 0.\n      max_tokens (int, optional): The maximum number of tokens for GPT-4 answer extraction. Defaults to 150.\n    Returns:\n      list: A list of answers extracted from the paper abstracts.\n    Examples:\n      >>> extract_answers_from_papers(papers, research_question)\n      ['Answer 1 SOURCE: Citation 1', 'Answer 2 SOURCE: Citation 2']\n    \"\"\"\n    answers = []\n    default_answer = \"No answer found.\"\n\n    for paper in papers:\n        abstract = paper.get(\"abstract\", \"\")\n        title = colored(paper.get(\"title\", \"\"), \"magenta\", attrs=[\"bold\"])\n        if \"externalIds\" in paper and \"DOI\" in paper[\"externalIds\"]:\n            citation = get_citation_by_doi(paper[\"externalIds\"][\"DOI\"])\n        else:\n            citation = paper[\"url\"]\n        prompt = extract_answer_prompt.format(\n            research_question=research_question, abstract=abstract\n        )\n        answer = openai_call(\n            prompt, use_gpt4=use_gpt4, temperature=temperature, max_tokens=max_tokens\n        )\n\n        print(f\"Processing paper: {title}\")\n\n        answer_with_citation = f\"{answer}\\n{citation}\"\n        if answer != default_answer:\n            answer_with_citation = f\"{answer} SOURCE: {citation}\"\n            answers.append(answer_with_citation)\n            print(colored(f\"Answer found!\", \"green\"))\n            print(colored(f\"{answer_with_citation}\", \"cyan\"))\n\n    return answers", "def extract_answers_from_papers(\n    papers, research_question, use_gpt4=False, temperature=0, max_tokens=150\n):\n    \"\"\"\n    Extracts answers from paper abstracts.\n    Args:\n      papers (list): A list of papers.\n      research_question (str): The research question to answer.\n      use_gpt4 (bool, optional): Whether to use GPT-4 for answer extraction. Defaults to False.\n      temperature (float, optional): The temperature for GPT-4 answer extraction. Defaults to 0.\n      max_tokens (int, optional): The maximum number of tokens for GPT-4 answer extraction. Defaults to 150.\n    Returns:\n      list: A list of answers extracted from the paper abstracts.\n    Examples:\n      >>> extract_answers_from_papers(papers, research_question)\n      ['Answer 1 SOURCE: Citation 1', 'Answer 2 SOURCE: Citation 2']\n    \"\"\"\n    answers = []\n    default_answer = \"No answer found.\"\n\n    for paper in papers:\n        abstract = paper.get(\"abstract\", \"\")\n        title = colored(paper.get(\"title\", \"\"), \"magenta\", attrs=[\"bold\"])\n        if \"externalIds\" in paper and \"DOI\" in paper[\"externalIds\"]:\n            citation = get_citation_by_doi(paper[\"externalIds\"][\"DOI\"])\n        else:\n            citation = paper[\"url\"]\n        prompt = extract_answer_prompt.format(\n            research_question=research_question, abstract=abstract\n        )\n        answer = openai_call(\n            prompt, use_gpt4=use_gpt4, temperature=temperature, max_tokens=max_tokens\n        )\n\n        print(f\"Processing paper: {title}\")\n\n        answer_with_citation = f\"{answer}\\n{citation}\"\n        if answer != default_answer:\n            answer_with_citation = f\"{answer} SOURCE: {citation}\"\n            answers.append(answer_with_citation)\n            print(colored(f\"Answer found!\", \"green\"))\n            print(colored(f\"{answer_with_citation}\", \"cyan\"))\n\n    return answers", ""]}
{"filename": "autoresearcher/data_sources/__init__.py", "chunked_list": [""]}
{"filename": "autoresearcher/data_sources/web_apis/__init__.py", "chunked_list": [""]}
{"filename": "autoresearcher/data_sources/web_apis/wikipedia_loader.py", "chunked_list": ["import wikipedia\n\nfrom autoresearcher.data_sources.web_apis.base_web_api_data_loader import (\n    BaseWebAPIDataLoader,\n)\n\n\nclass WikipediaLoader(BaseWebAPIDataLoader):\n    def __init__(self):\n        super().__init__(\"https://en.wikipedia.org/w/api.php\")\n\n    def fetch_data(self, search_query, results=10, language=\"en\"):\n        \"\"\"\n        Fetches data from the Wikipedia API.\n        Args:\n          search_query (str): The query to search for.\n          results (int, optional): The maximum number of results to return. Defaults to 10.\n          language (str, optional): The language to search in. Defaults to \"en\".\n        Returns:\n          list: A list of dictionaries containing the data for each result.\n        Raises:\n          wikipedia.exceptions.DisambiguationError: If the search query returns a disambiguation page.\n        Examples:\n          >>> loader = WikipediaLoader()\n          >>> loader.fetch_data(\"Python\")\n          [\n            {\n              \"title\": \"Python (programming language)\",\n              \"url\": \"https://en.wikipedia.org/wiki/Python_(programming_language)\",\n              \"summary\": \"Python is an interpreted, high-level, general-purpose programming language.\",\n              \"content\": \"Python is an interpreted, high-level, general-purpose programming language...\",\n              \"categories\": [\"Programming languages\"],\n              \"references\": [\"https://www.python.org/\"]\n            }\n          ]\n        \"\"\"\n        wikipedia.set_lang(language)\n        wikipedia.set_rate_limiting(True)\n\n        search_results = wikipedia.search(search_query, results=results)\n        data = []\n\n        for result in search_results:\n            try:\n                page = wikipedia.page(result)\n                data.append(\n                    {\n                        \"title\": page.title,\n                        \"url\": page.url,\n                        \"summary\": page.summary,\n                        \"content\": page.content,\n                        \"categories\": page.categories,\n                        \"references\": page.references,\n                    }\n                )\n            except wikipedia.exceptions.DisambiguationError as e:\n                # Handle disambiguation pages by selecting the first option\n                if e.options:\n                    page = wikipedia.page(e.options[0])\n                    data.append(\n                        {\n                            \"title\": page.title,\n                            \"url\": page.url,\n                            \"summary\": page.summary,\n                            \"content\": page.content,\n                            \"categories\": page.categories,\n                            \"references\": page.references,\n                        }\n                    )\n            except wikipedia.exceptions.PageError:\n                # Skip pages that cannot be found\n                continue\n\n        return data", ""]}
{"filename": "autoresearcher/data_sources/web_apis/semantic_scholar_loader.py", "chunked_list": ["from autoresearcher.data_sources.web_apis.base_web_api_data_loader import (\n    BaseWebAPIDataLoader,\n)\nimport jellyfish\n\n\nclass SemanticScholarLoader(BaseWebAPIDataLoader):\n    def __init__(self):\n        \"\"\"\n        Initializes the SemanticScholarLoader class.\n        Args:\n          None\n        Returns:\n          None\n        Notes:\n          Calls the superclass constructor with the SemanticScholar API URL.\n        \"\"\"\n        super().__init__(\"https://api.semanticscholar.org/graph/v1/paper/search\")\n\n    def fetch_data(self, search_query, limit=100, year_range=None):\n        \"\"\"\n        Fetches data from the SemanticScholar API.\n        Args:\n          search_query (str): The query to search for.\n          limit (int, optional): The maximum number of results to return. Defaults to 100.\n          year_range (tuple, optional): A tuple of two integers representing the start and end year of the search. Defaults to None.\n        Returns:\n          list: A list of paper objects.\n        Examples:\n          >>> fetch_data(\"machine learning\", limit=50, year_range=(2010, 2020))\n          [{...}, {...}, ...]\n        \"\"\"\n        params = {\n            \"query\": search_query,\n            \"limit\": limit,\n            \"fields\": \"title,url,abstract,authors,citationStyles,journal,citationCount,year,externalIds\",\n        }\n\n        if year_range is not None:\n            params[\"year\"] = year_range\n\n        data = self.make_request(\"\", params=params)\n        return data.get(\"data\", [])\n\n    def fetch_and_sort_papers(\n        self,\n        search_query,\n        limit=100,\n        top_n=20,\n        year_range=None,\n        keyword_combinations=None,\n        weight_similarity=0.5,\n    ):\n        \"\"\"\n        Fetches and sorts papers from the SemanticScholar API.\n        Args:\n          search_query (str): The query to search for.\n          limit (int, optional): The maximum number of results to return. Defaults to 100.\n          top_n (int, optional): The maximum number of results to return after sorting. Defaults to 20.\n          year_range (tuple, optional): A tuple of two integers representing the start and end year of the search. Defaults to None.\n          keyword_combinations (list, optional): A list of keyword combinations to search for. Defaults to None.\n          weight_similarity (float, optional): The weight to give to the similarity score when sorting. Defaults to 0.5.\n        Returns:\n          list: A list of the top `top_n` paper objects sorted by combined score.\n        Examples:\n          >>> fetch_and_sort_papers(\"machine learning\", limit=50, top_n=10, year_range=(2010, 2020))\n          [{...}, {...}, ...]\n        \"\"\"\n        papers = []\n        if keyword_combinations is None:\n            keyword_combinations = [search_query]\n\n        for combination in keyword_combinations:\n            papers.extend(self.fetch_data(combination, limit, year_range))\n\n        max_citations = max(papers, key=lambda x: x[\"citationCount\"])[\"citationCount\"]\n\n        for paper in papers:\n            similarity = jellyfish.jaro_similarity(search_query, paper[\"title\"])\n            normalized_citation_count = paper[\"citationCount\"] / max_citations\n            paper[\"combined_score\"] = (weight_similarity * similarity) + (\n                (1 - weight_similarity) * normalized_citation_count\n            )\n\n        sorted_papers = sorted(papers, key=lambda x: x[\"combined_score\"], reverse=True)\n\n        # deduplicate paper entries prior to taking top n results\n        sorted_dedup_papers = list(\n            {each_paper[\"paperId\"]: each_paper for each_paper in sorted_papers}.values()\n        )\n\n        return sorted_dedup_papers[:top_n]", ""]}
{"filename": "autoresearcher/data_sources/web_apis/base_web_api_data_loader.py", "chunked_list": ["import requests\nfrom abc import ABC, abstractmethod\n\n\nclass BaseWebAPIDataLoader(ABC):\n    def __init__(self, base_url):\n        self.base_url = base_url\n\n    @abstractmethod\n    def fetch_data(self, search_query, **kwargs):\n        \"\"\"\n        Fetches data from the API.\n        Args:\n          search_query (str): The search query to use.\n          **kwargs: Additional keyword arguments to pass to the API.\n        Returns:\n          dict: The response from the API.\n        Raises:\n          NotImplementedError: If the method is not implemented.\n        \"\"\"\n        pass\n\n    def make_request(self, endpoint, params=None):\n        \"\"\"\n        Makes a request to the API.\n        Args:\n          endpoint (str): The API endpoint to make the request to.\n          params (dict, optional): Additional parameters to pass to the API. Defaults to None.\n        Returns:\n          dict: The response from the API.\n        Raises:\n          Exception: If the request fails.\n        \"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        response = requests.get(url, params=params)\n\n        if response.status_code == 200:\n            data = response.json()\n            return data\n        else:\n            raise Exception(f\"Failed to fetch data from API: {response.status_code}\")", ""]}
{"filename": "api/main.py", "chunked_list": ["import asyncio\nfrom fastapi import FastAPI, Response, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\n\nfrom autoresearcher import literature_review\n# Placeholder function for literature_review for fast testing\n# async def literature_review(q: str):\n#     return \"answer to: \" + q\n", "#     return \"answer to: \" + q\n\napp = FastAPI()\n\n# List of allowed origins (you can replace these with your own domain names)\nallowed_origins = [\n    \"*\",\n    \"https://restfox.dev\",  # Testing\n    \"http://localhost:3000\",  # Local development\n    \"https://example.com\",    # Production domain", "    \"http://localhost:3000\",  # Local development\n    \"https://example.com\",    # Production domain\n]\n\n# Add CORS middleware to the FastAPI application\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=allowed_origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"],  # Allow all HTTP methods (GET, POST, PUT, DELETE, etc.)", "    allow_credentials=True,\n    allow_methods=[\"*\"],  # Allow all HTTP methods (GET, POST, PUT, DELETE, etc.)\n    allow_headers=[\"*\"],  # Allow all headers\n)\n\n# Placeholder class for BrowserError (replace with your implementation)\nclass BrowserError(Exception):\n    pass\n\n# Define a Pydantic model for the POST request body", "\n# Define a Pydantic model for the POST request body\n# this should be for ALL tools + workflows\nclass QuestionModel(BaseModel):\n    research_question: str\n\n# this is purely for testing and memes\n# return a text response @ get\n# @app.get(\"/literature-review\")\n@app.get(\"/q/{q}\")", "# @app.get(\"/literature-review\")\n@app.get(\"/q/{q}\")\nasync def get_literature_review(q: str):\n    print('[GET] New Question:', q)\n    try:\n        if q is None:\n           return \"type a question after /q/type your question here \"\n        researcher = literature_review(q)\n        return researcher\n    except BrowserError as e:\n        return {\"error\": str(e)}", "\n# return a JSON response @ POST\n# optionally return a streamed response\n# Define the POST endpoint and accept the JSON request body\n@app.post(\"/\")\nasync def get_literature_review(request: QuestionModel):\n    q = request.research_question\n    print('[POST] New Question:', q)\n    try:\n        researcher = literature_review(q)\n        return {\"researcher\": researcher}\n    except BrowserError as e:\n        return {\"error\": str(e)}", "    try:\n        researcher = literature_review(q)\n        return {\"researcher\": researcher}\n    except BrowserError as e:\n        return {\"error\": str(e)}\n\n# to support plugins\n@app.get(\"/.well-known/ai-plugin.json\")\nasync def load_plugin(request: Request):\n    host = request.headers[\"host\"]\n    try:\n        with open(\"./.well-known/ai-plugin.json\") as file:\n            text = file.read()\n    except FileNotFoundError:\n        return Response(status_code=404, content=\"Not found\")", "async def load_plugin(request: Request):\n    host = request.headers[\"host\"]\n    try:\n        with open(\"./.well-known/ai-plugin.json\") as file:\n            text = file.read()\n    except FileNotFoundError:\n        return Response(status_code=404, content=\"Not found\")\n    text = text.replace(\"PLUGIN_HOSTNAME\", f\"http://{host}\")\n    return Response(content=text, media_type=\"text/json\")\n", "    return Response(content=text, media_type=\"text/json\")\n\n@app.get(\"/openapi.yaml\")\nasync def load_openapi(request: Request):\n    host = request.headers[\"host\"]\n    try:\n        with open(\"openapi.yaml\") as file:\n            text = file.read()\n    except FileNotFoundError:\n        return Response(status_code=404, content=\"Not found\")", "    text = text.replace(\"PLUGIN_HOSTNAME\", f\"http://{host}\")\n    return Response(content=text, media_type=\"text/yaml\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n\n\n", ""]}
