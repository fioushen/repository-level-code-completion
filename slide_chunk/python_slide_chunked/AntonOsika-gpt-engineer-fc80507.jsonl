{"filename": "scripts/clean_benchmarks.py", "chunked_list": ["# list all folders in benchmark folder\n# for each folder, run the benchmark\n\nimport os\nimport shutil\n\nfrom pathlib import Path\n\nfrom typer import run\n", "from typer import run\n\n\ndef main():\n    benchmarks = Path(\"benchmark\")\n\n    for benchmark in benchmarks.iterdir():\n        if benchmark.is_dir():\n            print(f\"Cleaning {benchmark}\")\n            for path in benchmark.iterdir():\n                if path.name in [\"prompt\", \"main_prompt\"]:\n                    continue\n\n                # Get filename of Path object\n                if path.is_dir():\n                    # delete the entire directory\n                    shutil.rmtree(path)\n                else:\n                    # delete the file\n                    os.remove(path)", "\n\nif __name__ == \"__main__\":\n    run(main)\n"]}
{"filename": "scripts/print_chat.py", "chunked_list": ["import json\n\nimport typer\n\nfrom termcolor import colored\n\napp = typer.Typer()\n\n\ndef pretty_print_conversation(messages):\n    role_to_color = {\n        \"system\": \"red\",\n        \"user\": \"green\",\n        \"assistant\": \"blue\",\n        \"function\": \"magenta\",\n    }\n    formatted_messages = []\n    for message in messages:\n        if message[\"role\"] == \"function\":\n            formatted_messages.append(\n                f\"function ({message['name']}): {message['content']}\\n\"\n            )\n        else:\n            assistant_content = (\n                message[\"function_call\"]\n                if message.get(\"function_call\")\n                else message[\"content\"]\n            )\n            role_to_message = {\n                \"system\": f\"system: {message['content']}\\n\",\n                \"user\": f\"user: {message['content']}\\n\",\n                \"assistant\": f\"assistant: {assistant_content}\\n\",\n            }\n            formatted_messages.append(role_to_message[message[\"role\"]])\n\n    for formatted_message in formatted_messages:\n        role = messages[formatted_messages.index(formatted_message)][\"role\"]\n        color = role_to_color[role]\n        print(colored(formatted_message, color))", "\ndef pretty_print_conversation(messages):\n    role_to_color = {\n        \"system\": \"red\",\n        \"user\": \"green\",\n        \"assistant\": \"blue\",\n        \"function\": \"magenta\",\n    }\n    formatted_messages = []\n    for message in messages:\n        if message[\"role\"] == \"function\":\n            formatted_messages.append(\n                f\"function ({message['name']}): {message['content']}\\n\"\n            )\n        else:\n            assistant_content = (\n                message[\"function_call\"]\n                if message.get(\"function_call\")\n                else message[\"content\"]\n            )\n            role_to_message = {\n                \"system\": f\"system: {message['content']}\\n\",\n                \"user\": f\"user: {message['content']}\\n\",\n                \"assistant\": f\"assistant: {assistant_content}\\n\",\n            }\n            formatted_messages.append(role_to_message[message[\"role\"]])\n\n    for formatted_message in formatted_messages:\n        role = messages[formatted_messages.index(formatted_message)][\"role\"]\n        color = role_to_color[role]\n        print(colored(formatted_message, color))", "\n\n@app.command()\ndef main(\n    messages_path: str,\n):\n    with open(messages_path) as f:\n        messages = json.load(f)\n\n    pretty_print_conversation(messages)", "\n\nif __name__ == \"__main__\":\n    app()\n"]}
{"filename": "scripts/rerun_edited_message_logs.py", "chunked_list": ["import json\nimport pathlib\n\nfrom typing import Union\n\nimport typer\n\nfrom gpt_engineer.ai import AI\nfrom gpt_engineer.chat_to_files import to_files\n", "from gpt_engineer.chat_to_files import to_files\n\napp = typer.Typer()\n\n\n@app.command()\ndef main(\n    messages_path: str,\n    out_path: Union[str, None] = None,\n    model: str = \"gpt-4\",\n    temperature: float = 0.1,\n):\n    ai = AI(\n        model_name=model,\n        temperature=temperature,\n    )\n\n    with open(messages_path) as f:\n        messages = json.load(f)\n\n    messages = ai.next(messages, step_name=\"rerun\")\n\n    if out_path:\n        to_files(messages[-1][\"content\"], out_path)\n        with open(pathlib.Path(out_path) / \"all_output.txt\", \"w\") as f:\n            json.dump(messages[-1][\"content\"], f)", "\n\nif __name__ == \"__main__\":\n    app()\n"]}
{"filename": "scripts/benchmark.py", "chunked_list": ["# list all folders in benchmark folder\n# for each folder, run the benchmark\nimport contextlib\nimport json\nimport os\nimport subprocess\n\nfrom datetime import datetime\nfrom itertools import islice\nfrom pathlib import Path", "from itertools import islice\nfrom pathlib import Path\nfrom typing import Iterable, Union\n\nfrom tabulate import tabulate\nfrom typer import run\n\n\ndef main(\n    n_benchmarks: Union[int, None] = None,\n):\n    path = Path(\"benchmark\")\n\n    folders: Iterable[Path] = path.iterdir()\n\n    if n_benchmarks:\n        folders = islice(folders, n_benchmarks)\n\n    benchmarks = []\n    for bench_folder in folders:\n        if os.path.isdir(bench_folder):\n            print(f\"Running benchmark for {bench_folder}\")\n\n            log_path = bench_folder / \"log.txt\"\n            log_file = open(log_path, \"w\")\n            process = subprocess.Popen(\n                [\n                    \"python\",\n                    \"-u\",  # Unbuffered output\n                    \"-m\",\n                    \"gpt_engineer.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"benchmark\",\n                ],\n                stdout=log_file,\n                stderr=log_file,\n                bufsize=0,\n            )\n            benchmarks.append((bench_folder, process, log_file))\n\n            print(\"You can stream the log file by running:\")\n            print(f\"tail -f {log_path}\")\n            print()\n\n    for bench_folder, process, file in benchmarks:\n        process.wait()\n        file.close()\n\n        print(\"process\", bench_folder.name, \"finished with code\", process.returncode)\n        print(\"Running it. Original benchmark prompt:\")\n        print()\n        with open(bench_folder / \"prompt\") as f:\n            print(f.read())\n        print()\n\n        with contextlib.suppress(KeyboardInterrupt):\n            subprocess.run(\n                [\n                    \"python\",\n                    \"-m\",\n                    \"gpt_engineer.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"evaluate\",\n                ],\n            )\n\n    generate_report(benchmarks, path)", "def main(\n    n_benchmarks: Union[int, None] = None,\n):\n    path = Path(\"benchmark\")\n\n    folders: Iterable[Path] = path.iterdir()\n\n    if n_benchmarks:\n        folders = islice(folders, n_benchmarks)\n\n    benchmarks = []\n    for bench_folder in folders:\n        if os.path.isdir(bench_folder):\n            print(f\"Running benchmark for {bench_folder}\")\n\n            log_path = bench_folder / \"log.txt\"\n            log_file = open(log_path, \"w\")\n            process = subprocess.Popen(\n                [\n                    \"python\",\n                    \"-u\",  # Unbuffered output\n                    \"-m\",\n                    \"gpt_engineer.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"benchmark\",\n                ],\n                stdout=log_file,\n                stderr=log_file,\n                bufsize=0,\n            )\n            benchmarks.append((bench_folder, process, log_file))\n\n            print(\"You can stream the log file by running:\")\n            print(f\"tail -f {log_path}\")\n            print()\n\n    for bench_folder, process, file in benchmarks:\n        process.wait()\n        file.close()\n\n        print(\"process\", bench_folder.name, \"finished with code\", process.returncode)\n        print(\"Running it. Original benchmark prompt:\")\n        print()\n        with open(bench_folder / \"prompt\") as f:\n            print(f.read())\n        print()\n\n        with contextlib.suppress(KeyboardInterrupt):\n            subprocess.run(\n                [\n                    \"python\",\n                    \"-m\",\n                    \"gpt_engineer.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"evaluate\",\n                ],\n            )\n\n    generate_report(benchmarks, path)", "\n\ndef generate_report(benchmarks, benchmark_path):\n    headers = [\"Benchmark\", \"Ran\", \"Works\", \"Perfect\", \"Notes\"]\n    rows = []\n    for bench_folder, _, _ in benchmarks:\n        memory = bench_folder / \"memory\"\n        with open(memory / \"review\") as f:\n            review = json.loads(f.read())\n            rows.append(\n                [\n                    bench_folder.name,\n                    to_emoji(review.get(\"ran\", None)),\n                    to_emoji(review.get(\"works\", None)),\n                    to_emoji(review.get(\"perfect\", None)),\n                    review.get(\"comments\", None),\n                ]\n            )\n    table: str = tabulate(rows, headers, tablefmt=\"pipe\")\n    print(\"\\nBenchmark report:\\n\")\n    print(table)\n    print()\n    append_to_results = ask_yes_no(\"Append report to the results file?\")\n    if append_to_results:\n        results_path = benchmark_path / \"RESULTS.md\"\n        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n        insert_markdown_section(results_path, current_date, table, 2)", "\n\ndef to_emoji(value: bool) -> str:\n    return \"\\U00002705\" if value else \"\\U0000274C\"\n\n\ndef insert_markdown_section(file_path, section_title, section_text, level):\n    with open(file_path, \"r\") as file:\n        lines = file.readlines()\n\n    header_prefix = \"#\" * level\n    new_section = f\"{header_prefix} {section_title}\\n\\n{section_text}\\n\\n\"\n\n    # Find the first section with the specified level\n    line_number = -1\n    for i, line in enumerate(lines):\n        if line.startswith(header_prefix):\n            line_number = i\n            break\n\n    if line_number != -1:\n        lines.insert(line_number, new_section)\n    else:\n        print(\n            f\"Markdown file was of unexpected format. No section of level {level} found. \"\n            \"Did not write results.\"\n        )\n        return\n\n    # Write the file\n    with open(file_path, \"w\") as file:\n        file.writelines(lines)", "\n\ndef ask_yes_no(question: str) -> bool:\n    while True:\n        response = input(question + \" (y/n): \").lower().strip()\n        if response == \"y\":\n            return True\n        elif response == \"n\":\n            return False\n        else:\n            print(\"Please enter either 'y' or 'n'.\")", "\n\nif __name__ == \"__main__\":\n    run(main)\n"]}
{"filename": "tests/test_ai.py", "chunked_list": ["import pytest\n\nfrom gpt_engineer.ai import AI\n\n\n@pytest.mark.xfail(reason=\"Constructor assumes API access\")\ndef test_ai():\n    AI()\n    # TODO Assert that methods behave and not only constructor.\n", "    # TODO Assert that methods behave and not only constructor.\n"]}
{"filename": "tests/test_db.py", "chunked_list": ["import pytest\n\nfrom gpt_engineer.db import DB, DBs\n\n\ndef test_DB_operations(tmp_path):\n    # Test initialization\n    db = DB(tmp_path)\n\n    # Test __setitem__\n    db[\"test_key\"] = \"test_value\"\n\n    assert (tmp_path / \"test_key\").is_file()\n\n    # Test __getitem__\n    val = db[\"test_key\"]\n\n    assert val == \"test_value\"", "\n\ndef test_DBs_initialization(tmp_path):\n    dir_names = [\"memory\", \"logs\", \"preprompts\", \"input\", \"workspace\", \"archive\"]\n    directories = [tmp_path / name for name in dir_names]\n\n    # Create DB objects\n    dbs = [DB(dir) for dir in directories]\n\n    # Create DB instance\n    dbs_instance = DBs(*dbs)\n\n    assert isinstance(dbs_instance.memory, DB)\n    assert isinstance(dbs_instance.logs, DB)\n    assert isinstance(dbs_instance.preprompts, DB)\n    assert isinstance(dbs_instance.input, DB)\n    assert isinstance(dbs_instance.workspace, DB)\n    assert isinstance(dbs_instance.archive, DB)", "\n\ndef test_invalid_path():\n    with pytest.raises((PermissionError, OSError)):\n        # Test with a path that will raise a permission error\n        DB(\"/root/test\")\n\n\ndef test_large_files(tmp_path):\n    db = DB(tmp_path)\n    large_content = \"a\" * (10**6)  # 1MB of data\n\n    # Test write large files\n    db[\"large_file\"] = large_content\n\n    # Test read large files\n    assert db[\"large_file\"] == large_content", "def test_large_files(tmp_path):\n    db = DB(tmp_path)\n    large_content = \"a\" * (10**6)  # 1MB of data\n\n    # Test write large files\n    db[\"large_file\"] = large_content\n\n    # Test read large files\n    assert db[\"large_file\"] == large_content\n", "\n\ndef test_concurrent_access(tmp_path):\n    import threading\n\n    db = DB(tmp_path)\n\n    num_threads = 10\n    num_writes = 1000\n\n    def write_to_db(thread_id):\n        for i in range(num_writes):\n            key = f\"thread{thread_id}_write{i}\"\n            db[key] = str(i)\n\n    threads = []\n    for thread_id in range(num_threads):\n        t = threading.Thread(target=write_to_db, args=(thread_id,))\n        t.start()\n        threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    # Verify that all expected data was written\n    for thread_id in range(num_threads):\n        for i in range(num_writes):\n            key = f\"thread{thread_id}_write{i}\"\n            assert key in db  # using __contains__ now\n            assert db[key] == str(i)", "\n\ndef test_error_messages(tmp_path):\n    db = DB(tmp_path)\n\n    # Test error on getting non-existent key\n    with pytest.raises(KeyError):\n        db[\"non_existent\"]\n\n    with pytest.raises(AssertionError) as e:\n        db[\"key\"] = [\"Invalid\", \"value\"]\n\n    assert str(e.value) == \"val must be str\"", "\n\ndef test_DBs_dataclass_attributes(tmp_path):\n    dir_names = [\"memory\", \"logs\", \"preprompts\", \"input\", \"workspace\", \"archive\"]\n    directories = [tmp_path / name for name in dir_names]\n\n    # Create DB objects\n    dbs = [DB(dir) for dir in directories]\n\n    # Create DBs instance\n    dbs_instance = DBs(*dbs)\n\n    assert dbs_instance.memory == dbs[0]\n    assert dbs_instance.logs == dbs[1]\n    assert dbs_instance.preprompts == dbs[2]\n    assert dbs_instance.input == dbs[3]\n    assert dbs_instance.workspace == dbs[4]", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_chat_to_files.py", "chunked_list": ["import textwrap\n\nfrom gpt_engineer.chat_to_files import to_files\n\n\ndef test_to_files():\n    chat = textwrap.dedent(\n        \"\"\"\n    This is a sample program.\n\n    file1.py\n    ```python\n    print(\"Hello, World!\")\n    ```\n\n    file2.py\n    ```python\n    def add(a, b):\n        return a + b\n    ```\n    \"\"\"\n    )\n\n    workspace = {}\n    to_files(chat, workspace)\n\n    assert workspace[\"all_output.txt\"] == chat\n\n    expected_files = {\n        \"file1.py\": 'print(\"Hello, World!\")\\n',\n        \"file2.py\": \"def add(a, b):\\n    return a + b\\n\",\n        \"README.md\": \"\\nThis is a sample program.\\n\\nfile1.py\\n\",\n    }\n\n    for file_name, file_content in expected_files.items():\n        assert workspace[file_name] == file_content", "\n\ndef test_to_files_with_square_brackets():\n    chat = textwrap.dedent(\n        \"\"\"\n    This is a sample program.\n\n    [file1.py]\n    ```python\n    print(\"Hello, World!\")\n    ```\n\n    [file2.py]\n    ```python\n    def add(a, b):\n        return a + b\n    ```\n    \"\"\"\n    )\n    workspace = {}\n    to_files(chat, workspace)\n\n    assert workspace[\"all_output.txt\"] == chat\n\n    expected_files = {\n        \"file1.py\": 'print(\"Hello, World!\")\\n',\n        \"file2.py\": \"def add(a, b):\\n    return a + b\\n\",\n        \"README.md\": \"\\nThis is a sample program.\\n\\n[file1.py]\\n\",\n    }\n\n    for file_name, file_content in expected_files.items():\n        assert workspace[file_name] == file_content", "\n\ndef test_files_with_brackets_in_name():\n    chat = textwrap.dedent(\n        \"\"\"\n    This is a sample program.\n\n    [id].jsx\n    ```javascript\n    console.log(\"Hello, World!\")\n    ```\n    \"\"\"\n    )\n\n    workspace = {}\n    to_files(chat, workspace)\n\n    assert workspace[\"all_output.txt\"] == chat\n\n    expected_files = {\n        \"[id].jsx\": 'console.log(\"Hello, World!\")\\n',\n        \"README.md\": \"\\nThis is a sample program.\\n\\n[id].jsx\\n\",\n    }\n\n    for file_name, file_content in expected_files.items():\n        assert workspace[file_name] == file_content", "\n\ndef test_files_with_file_colon():\n    chat = textwrap.dedent(\n        \"\"\"\n    This is a sample program.\n\n    [FILE: file1.py]\n    ```python\n    print(\"Hello, World!\")\n    ```\n    \"\"\"\n    )\n\n    workspace = {}\n    to_files(chat, workspace)\n\n    assert workspace[\"all_output.txt\"] == chat\n\n    expected_files = {\n        \"file1.py\": 'print(\"Hello, World!\")\\n',\n        \"README.md\": \"\\nThis is a sample program.\\n\\n[FILE: file1.py]\\n\",\n    }\n\n    for file_name, file_content in expected_files.items():\n        assert workspace[file_name] == file_content", "\n\ndef test_files_with_back_tick():\n    chat = textwrap.dedent(\n        \"\"\"\n    This is a sample program.\n\n    `file1.py`\n    ```python\n    print(\"Hello, World!\")\n    ```\n    \"\"\"\n    )\n\n    workspace = {}\n    to_files(chat, workspace)\n\n    assert workspace[\"all_output.txt\"] == chat\n\n    expected_files = {\n        \"file1.py\": 'print(\"Hello, World!\")\\n',\n        \"README.md\": \"\\nThis is a sample program.\\n\\n`file1.py`\\n\",\n    }\n\n    for file_name, file_content in expected_files.items():\n        assert workspace[file_name] == file_content", "\n\ndef test_files_with_newline_between():\n    chat = textwrap.dedent(\n        \"\"\"\n    This is a sample program.\n\n    file1.py\n\n    ```python\n    print(\"Hello, World!\")\n    ```\n    \"\"\"\n    )\n\n    workspace = {}\n    to_files(chat, workspace)\n\n    assert workspace[\"all_output.txt\"] == chat\n\n    expected_files = {\n        \"file1.py\": 'print(\"Hello, World!\")\\n',\n        \"README.md\": \"\\nThis is a sample program.\\n\\nfile1.py\\n\\n\",\n    }\n\n    for file_name, file_content in expected_files.items():\n        assert workspace[file_name] == file_content", "\n\ndef test_files_with_newline_between_header():\n    chat = textwrap.dedent(\n        \"\"\"\n    This is a sample program.\n\n    ## file1.py\n\n    ```python\n    print(\"Hello, World!\")\n    ```\n    \"\"\"\n    )\n\n    workspace = {}\n    to_files(chat, workspace)\n\n    assert workspace[\"all_output.txt\"] == chat\n\n    expected_files = {\n        \"file1.py\": 'print(\"Hello, World!\")\\n',\n        \"README.md\": \"\\nThis is a sample program.\\n\\n## file1.py\\n\\n\",\n    }\n\n    for file_name, file_content in expected_files.items():\n        assert workspace[file_name] == file_content", ""]}
{"filename": "tests/test_collect.py", "chunked_list": ["import json\nimport os\n\nfrom unittest.mock import MagicMock\n\nimport pytest\nimport rudderstack.analytics as rudder_analytics\n\nfrom gpt_engineer.collect import collect_learnings, steps_file_hash\nfrom gpt_engineer.db import DB, DBs", "from gpt_engineer.collect import collect_learnings, steps_file_hash\nfrom gpt_engineer.db import DB, DBs\nfrom gpt_engineer.learning import extract_learning\nfrom gpt_engineer.steps import gen_code_after_unit_tests\n\n\ndef test_collect_learnings(monkeypatch):\n    monkeypatch.setattr(os, \"environ\", {\"COLLECT_LEARNINGS_OPT_IN\": \"true\"})\n    monkeypatch.setattr(rudder_analytics, \"track\", MagicMock())\n\n    model = \"test_model\"\n    temperature = 0.5\n    steps = [gen_code_after_unit_tests]\n    dbs = DBs(DB(\"/tmp\"), DB(\"/tmp\"), DB(\"/tmp\"), DB(\"/tmp\"), DB(\"/tmp\"), DB(\"/tmp\"))\n    dbs.input = {\n        \"prompt\": \"test prompt\\n with newlines\",\n        \"feedback\": \"test feedback\",\n    }\n    code = \"this is output\\n\\nit contains code\"\n    dbs.logs = {\n        gen_code_after_unit_tests.__name__: json.dumps(\n            [{\"role\": \"system\", \"content\": code}]\n        )\n    }\n    dbs.workspace = {\"all_output.txt\": \"test workspace\\n\" + code}\n\n    collect_learnings(model, temperature, steps, dbs)\n\n    learnings = extract_learning(\n        model, temperature, steps, dbs, steps_file_hash=steps_file_hash()\n    )\n    assert rudder_analytics.track.call_count == 1\n    assert rudder_analytics.track.call_args[1][\"event\"] == \"learning\"\n    a = {\n        k: v\n        for k, v in rudder_analytics.track.call_args[1][\"properties\"].items()\n        if k != \"timestamp\"\n    }\n    b = {k: v for k, v in learnings.to_dict().items() if k != \"timestamp\"}\n    assert a == b\n\n    assert json.dumps(code) in learnings.logs\n    assert code in learnings.workspace", "\n\nif __name__ == \"__main__\":\n    pytest.main([\"-v\"])\n"]}
{"filename": "tests/steps/__init__.py", "chunked_list": [""]}
{"filename": "tests/steps/test_archive.py", "chunked_list": ["import datetime\nimport os\n\nfrom unittest.mock import MagicMock\n\nfrom gpt_engineer.db import DB, DBs, archive\n\n\ndef freeze_at(monkeypatch, time):\n    datetime_mock = MagicMock(wraps=datetime.datetime)\n    datetime_mock.now.return_value = time\n    monkeypatch.setattr(datetime, \"datetime\", datetime_mock)", "def freeze_at(monkeypatch, time):\n    datetime_mock = MagicMock(wraps=datetime.datetime)\n    datetime_mock.now.return_value = time\n    monkeypatch.setattr(datetime, \"datetime\", datetime_mock)\n\n\ndef setup_dbs(tmp_path, dir_names):\n    directories = [tmp_path / name for name in dir_names]\n\n    # Create DB objects\n    dbs = [DB(dir) for dir in directories]\n\n    # Create DBs instance\n    return DBs(*dbs)", "\n\ndef test_archive(tmp_path, monkeypatch):\n    dbs = setup_dbs(\n        tmp_path, [\"memory\", \"logs\", \"preprompts\", \"input\", \"workspace\", \"archive\"]\n    )\n    freeze_at(monkeypatch, datetime.datetime(2020, 12, 25, 17, 5, 55))\n    archive(dbs)\n    assert not os.path.exists(tmp_path / \"memory\")\n    assert not os.path.exists(tmp_path / \"workspace\")\n    assert os.path.isdir(tmp_path / \"archive\" / \"20201225_170555\")\n\n    dbs = setup_dbs(\n        tmp_path, [\"memory\", \"logs\", \"preprompts\", \"input\", \"workspace\", \"archive\"]\n    )\n    freeze_at(monkeypatch, datetime.datetime(2022, 8, 14, 8, 5, 12))\n    archive(dbs)\n    assert not os.path.exists(tmp_path / \"memory\")\n    assert not os.path.exists(tmp_path / \"workspace\")\n    assert os.path.isdir(tmp_path / \"archive\" / \"20201225_170555\")\n    assert os.path.isdir(tmp_path / \"archive\" / \"20220814_080512\")", ""]}
{"filename": "evals/evals_existing_code.py", "chunked_list": ["import os\nimport subprocess\n\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport yaml\n\nfrom eval_tools import check_evaluation_component\nfrom tabulate import tabulate", "from eval_tools import check_evaluation_component\nfrom tabulate import tabulate\n\nfrom gpt_engineer.chat_to_files import parse_chat\nfrom gpt_engineer.db import DB\n\nEVAL_LIST_NAME = \"evaluations\"  # the top level list in the YAML file\n\n\ndef single_evaluate(eval_ob: dict) -> list[bool]:\n    \"\"\"Evaluates a single prompt.\"\"\"\n    print(f\"running evaluation: {eval_ob['name']}\")\n\n    # Step 1. Setup known project\n    # load the known files into the project\n    # the files can be anywhere in the projects folder\n\n    workspace = DB(eval_ob[\"project_root\"])\n    file_list_string = \"\"\n    code_base_abs = Path(os.getcwd()) / eval_ob[\"project_root\"]\n\n    files = parse_chat(open(eval_ob[\"code_blob\"]).read())\n    for file_name, file_content in files:\n        absolute_path = code_base_abs / file_name\n        print(\"creating: \", absolute_path)\n        workspace[absolute_path] = file_content\n        file_list_string += str(absolute_path) + \"\\n\"\n\n    # create file_list.txt (should be full paths)\n    workspace[\"file_list.txt\"] = file_list_string\n\n    # create the prompt\n    workspace[\"prompt\"] = eval_ob[\"improve_code_prompt\"]\n\n    # Step 2.  run the project in improve code mode,\n    # make sure the flag -sf is set to skip feedback\n\n    print(f\"Modifying code for {eval_ob['project_root']}\")\n\n    log_path = code_base_abs / \"log.txt\"\n    log_file = open(log_path, \"w\")\n    process = subprocess.Popen(\n        [\n            \"python\",\n            \"-u\",  # Unbuffered output\n            \"-m\",\n            \"gpt_engineer.main\",\n            eval_ob[\"project_root\"],\n            \"--steps\",\n            \"eval_improve_code\",\n        ],\n        stdout=log_file,\n        stderr=log_file,\n        bufsize=0,\n    )\n    print(f\"waiting for {eval_ob['name']} to finish.\")\n    process.wait()  # we want to wait until it finishes.\n\n    # Step 3. Run test of modified code, tests\n    print(\"running tests on modified code\")\n    evaluation_results = []\n    for test_case in eval_ob[\"expected_results\"]:\n        print(f\"checking: {test_case['type']}\")\n        test_case[\"project_root\"] = Path(eval_ob[\"project_root\"])\n        evaluation_results.append(check_evaluation_component(test_case))\n\n    return evaluation_results", "\ndef single_evaluate(eval_ob: dict) -> list[bool]:\n    \"\"\"Evaluates a single prompt.\"\"\"\n    print(f\"running evaluation: {eval_ob['name']}\")\n\n    # Step 1. Setup known project\n    # load the known files into the project\n    # the files can be anywhere in the projects folder\n\n    workspace = DB(eval_ob[\"project_root\"])\n    file_list_string = \"\"\n    code_base_abs = Path(os.getcwd()) / eval_ob[\"project_root\"]\n\n    files = parse_chat(open(eval_ob[\"code_blob\"]).read())\n    for file_name, file_content in files:\n        absolute_path = code_base_abs / file_name\n        print(\"creating: \", absolute_path)\n        workspace[absolute_path] = file_content\n        file_list_string += str(absolute_path) + \"\\n\"\n\n    # create file_list.txt (should be full paths)\n    workspace[\"file_list.txt\"] = file_list_string\n\n    # create the prompt\n    workspace[\"prompt\"] = eval_ob[\"improve_code_prompt\"]\n\n    # Step 2.  run the project in improve code mode,\n    # make sure the flag -sf is set to skip feedback\n\n    print(f\"Modifying code for {eval_ob['project_root']}\")\n\n    log_path = code_base_abs / \"log.txt\"\n    log_file = open(log_path, \"w\")\n    process = subprocess.Popen(\n        [\n            \"python\",\n            \"-u\",  # Unbuffered output\n            \"-m\",\n            \"gpt_engineer.main\",\n            eval_ob[\"project_root\"],\n            \"--steps\",\n            \"eval_improve_code\",\n        ],\n        stdout=log_file,\n        stderr=log_file,\n        bufsize=0,\n    )\n    print(f\"waiting for {eval_ob['name']} to finish.\")\n    process.wait()  # we want to wait until it finishes.\n\n    # Step 3. Run test of modified code, tests\n    print(\"running tests on modified code\")\n    evaluation_results = []\n    for test_case in eval_ob[\"expected_results\"]:\n        print(f\"checking: {test_case['type']}\")\n        test_case[\"project_root\"] = Path(eval_ob[\"project_root\"])\n        evaluation_results.append(check_evaluation_component(test_case))\n\n    return evaluation_results", "\n\ndef to_emoji(value: bool) -> str:\n    return \"\\U00002705\" if value else \"\\U0000274C\"\n\n\ndef generate_report(evals: list[dict], res: list[list[bool]]) -> None:\n    # High level shows if all the expected_results passed\n    # Detailed shows all the test cases and a pass/fail for each\n    output_lines = []\n    current_date = datetime.now().strftime(\"%Y-%m-%d\")\n    output_lines.append(f\"## {current_date}\\n\\n\")\n\n    # Create a summary table\n    headers = [\"Project\", \"Evaluation\", \"All Tests Pass\"]\n    rows = []\n    for i, eval_ob in enumerate(evals):\n        rows.append(\n            [eval_ob[\"project_root\"], eval_ob[\"name\"], to_emoji(all(res[i]))]\n        )  # logical AND of all tests\n    table: str = tabulate(rows, headers, tablefmt=\"pipe\")\n    title = \"Existing Code Evaluation Summary:\"\n    print(f\"\\n{title}\\n\")\n    print(table)\n    print()\n    output_lines.append(f\"### {title}\\n\\n{table}\\n\\n\")\n\n    # Create a detailed table\n    headers = [\"Project\", \"Evaluation\", \"Test\", \"Pass\"]\n    rows = []\n    for i, eval_ob in enumerate(evals):\n        for j, test in enumerate(eval_ob[\"expected_results\"]):\n            rows.append(\n                [\n                    eval_ob[\"project_root\"],\n                    eval_ob[\"name\"],\n                    eval_ob[\"expected_results\"][j][\"type\"],\n                    to_emoji(res[i][j]),\n                ]\n            )\n    detail_table: str = tabulate(rows, headers, tablefmt=\"pipe\")\n    title = \"Detailed Test Results:\"\n    print(f\"\\n{title} \\n\")\n    print(detail_table)\n    print()\n\n    output_lines.append(f\"### {title}\\n\\n{detail_table}\\n\\n\")\n    with open(\"evals/IMPROVE_CODE_RESULTS.md\", \"a\") as file:\n        file.writelines(output_lines)", "\n\ndef load_evaluations_from_file(file_path):\n    \"\"\"Loads the evaluations from a YAML file.\"\"\"\n    try:\n        with open(file_path, \"r\") as file:\n            data = yaml.safe_load(file)\n            if EVAL_LIST_NAME in data:\n                return data[EVAL_LIST_NAME]\n            else:\n                print(f\"'{EVAL_LIST_NAME}' not found in {file_path}\")\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")", "\n\ndef run_all_evaluations(eval_list: list[dict]) -> None:\n    results = []\n    for eval_ob in eval_list:\n        results.append(single_evaluate(eval_ob))\n\n    # Step 4. Generate Report\n    generate_report(eval_list, results)\n", "\n\nif __name__ == \"__main__\":\n    eval_list = load_evaluations_from_file(\"evals/existing_code_eval.yaml\")\n    run_all_evaluations(eval_list)\n"]}
{"filename": "evals/eval_tools.py", "chunked_list": ["\"\"\"\nThis library is used for the evaluation of gpt-engineer's performance, on\nediting and creating code.  This is very low level in that it looks at the\ncode written.  It is possible that the AI could solve the problem in ways\nthat we cannot forsee, with this in mind higher level tests are always\nbetter than lower.\n\nThe scope will bre relatively limited to a few languages but this could\nbe expanded.\n\"\"\"", "be expanded.\n\"\"\"\n\n\ndef check_language(eval_d: dict) -> None:\n    if eval_d[\"language\"] != \"python\":\n        raise Exception(f\"Language: {eval_d['language']} is not supported.\")\n\n\ndef assert_exists_in_source_code(eval_d: dict) -> bool:\n    \"\"\"Checks of some text exists in the source code.\"\"\"\n    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n    return source_body.find(eval_d[\"existing_string\"]) > -1", "\ndef assert_exists_in_source_code(eval_d: dict) -> bool:\n    \"\"\"Checks of some text exists in the source code.\"\"\"\n    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n    return source_body.find(eval_d[\"existing_string\"]) > -1\n\n\ndef run_code_class_has_property(eval_d: dict) -> bool:\n    \"\"\"Will execute code, then check if the code has the desired proprty.\"\"\"\n    check_language(eval_d)\n    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n    exec(source_body)\n\n    class_ref = locals().get(eval_d[\"class_name\"])\n    ob = class_ref()\n    return hasattr(ob, eval_d[\"property_name\"])", "\n\ndef run_code_class_has_property_w_value(eval_d: dict) -> bool:\n    \"\"\"Will execute code, then check if the code has the desired proprty.\"\"\"\n    check_language(eval_d)\n    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n    exec(source_body)\n\n    class_ref = locals().get(eval_d[\"class_name\"])\n    ob = class_ref()\n\n    assert hasattr(ob, eval_d[\"property_name\"])\n\n    return getattr(ob, eval_d[\"property_name\"]) == eval_d[\"expected_value\"]", "\n\ndef run_code_eval_function(eval_d) -> bool:\n    \"\"\"Similar to run_code_class_has_property() except is evaluates a function call.\"\"\"\n    check_language(eval_d)\n    source_body = open(eval_d[\"project_root\"] / eval_d[\"source_file\"]).read()\n    exec(source_body)\n    function_ref = globals().get(eval_d[\"function_name\"])\n\n    # TODO: add the ability to have function arguments\n    return function_ref() == eval_d[\"expected_value\"]", "\n\ndef check_evaluation_component(eval_d: dict) -> bool:\n    \"\"\"Switch on evaluation components\"\"\"\n    test_type = eval_d.get(\"type\")\n    if test_type == \"assert_exists_in_source_code\":\n        return assert_exists_in_source_code(eval_d)\n    elif test_type == \"run_code_class_has_property\":\n        return run_code_class_has_property(eval_d)\n    elif test_type == \"run_code_class_has_property_w_value\":\n        return run_code_class_has_property_w_value(eval_d)\n    elif test_type == \"run_code_eval_function\":\n        return run_code_eval_function(eval_d)\n    else:\n        raise Exception(f\"Test type '{test_type}' is not recognized.\")", ""]}
{"filename": "evals/__init__.py", "chunked_list": [""]}
{"filename": "gpt_engineer/main.py", "chunked_list": ["import logging\nimport os\n\nfrom pathlib import Path\n\nimport openai\nimport typer\n\nfrom dotenv import load_dotenv\n", "from dotenv import load_dotenv\n\nfrom gpt_engineer.ai import AI\nfrom gpt_engineer.collect import collect_learnings\nfrom gpt_engineer.db import DB, DBs, archive\nfrom gpt_engineer.learning import collect_consent\nfrom gpt_engineer.steps import STEPS, Config as StepsConfig\n\napp = typer.Typer()  # creates a CLI app\n", "app = typer.Typer()  # creates a CLI app\n\n\ndef load_env_if_needed():\n    if os.getenv(\"OPENAI_API_KEY\") is None:\n        load_dotenv()\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\n@app.command()\ndef main(\n    project_path: str = typer.Argument(\"projects/example\", help=\"path\"),\n    model: str = typer.Argument(\"gpt-4\", help=\"model id string\"),\n    temperature: float = 0.1,\n    steps_config: StepsConfig = typer.Option(\n        StepsConfig.DEFAULT, \"--steps\", \"-s\", help=\"decide which steps to run\"\n    ),\n    improve_option: bool = typer.Option(\n        False,\n        \"--improve\",\n        \"-i\",\n        help=\"Improve code from existing project.\",\n    ),\n    azure_endpoint: str = typer.Option(\n        \"\",\n        \"--azure\",\n        \"-a\",\n        help=\"\"\"Endpoint for your Azure OpenAI Service (https://xx.openai.azure.com).\n            In that case, the given model is the deployment name chosen in the Azure AI Studio.\"\"\",\n    ),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\"),\n):\n    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n\n    # For the improve option take current project as path and add .gpteng folder\n    if improve_option:\n        # The default option for the --improve is the IMPROVE_CODE, not DEFAULT\n        if steps_config == StepsConfig.DEFAULT:\n            steps_config = StepsConfig.IMPROVE_CODE\n\n    load_env_if_needed()\n\n    ai = AI(\n        model_name=model,\n        temperature=temperature,\n        azure_endpoint=azure_endpoint,\n    )\n\n    input_path = Path(project_path).absolute()\n    memory_path = input_path / \"memory\"\n    workspace_path = input_path / \"workspace\"\n    archive_path = input_path / \"archive\"\n\n    dbs = DBs(\n        memory=DB(memory_path),\n        logs=DB(memory_path / \"logs\"),\n        input=DB(input_path),\n        workspace=DB(workspace_path),\n        preprompts=DB(\n            Path(__file__).parent / \"preprompts\"\n        ),  # Loads preprompts from the preprompts directory\n        archive=DB(archive_path),\n    )\n\n    if steps_config not in [\n        StepsConfig.EXECUTE_ONLY,\n        StepsConfig.USE_FEEDBACK,\n        StepsConfig.EVALUATE,\n        StepsConfig.IMPROVE_CODE,\n    ]:\n        archive(dbs)\n\n        if not dbs.input.get(\"prompt\"):\n            dbs.input[\"prompt\"] = input(\n                \"\\nWhat application do you want gpt-engineer to generate?\\n\"\n            )\n\n    steps = STEPS[steps_config]\n    for step in steps:\n        messages = step(ai, dbs)\n        dbs.logs[step.__name__] = AI.serialize_messages(messages)\n\n    if collect_consent():\n        collect_learnings(model, temperature, steps, dbs)\n\n    dbs.logs[\"token_usage\"] = ai.format_token_usage_log()", "\n@app.command()\ndef main(\n    project_path: str = typer.Argument(\"projects/example\", help=\"path\"),\n    model: str = typer.Argument(\"gpt-4\", help=\"model id string\"),\n    temperature: float = 0.1,\n    steps_config: StepsConfig = typer.Option(\n        StepsConfig.DEFAULT, \"--steps\", \"-s\", help=\"decide which steps to run\"\n    ),\n    improve_option: bool = typer.Option(\n        False,\n        \"--improve\",\n        \"-i\",\n        help=\"Improve code from existing project.\",\n    ),\n    azure_endpoint: str = typer.Option(\n        \"\",\n        \"--azure\",\n        \"-a\",\n        help=\"\"\"Endpoint for your Azure OpenAI Service (https://xx.openai.azure.com).\n            In that case, the given model is the deployment name chosen in the Azure AI Studio.\"\"\",\n    ),\n    verbose: bool = typer.Option(False, \"--verbose\", \"-v\"),\n):\n    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n\n    # For the improve option take current project as path and add .gpteng folder\n    if improve_option:\n        # The default option for the --improve is the IMPROVE_CODE, not DEFAULT\n        if steps_config == StepsConfig.DEFAULT:\n            steps_config = StepsConfig.IMPROVE_CODE\n\n    load_env_if_needed()\n\n    ai = AI(\n        model_name=model,\n        temperature=temperature,\n        azure_endpoint=azure_endpoint,\n    )\n\n    input_path = Path(project_path).absolute()\n    memory_path = input_path / \"memory\"\n    workspace_path = input_path / \"workspace\"\n    archive_path = input_path / \"archive\"\n\n    dbs = DBs(\n        memory=DB(memory_path),\n        logs=DB(memory_path / \"logs\"),\n        input=DB(input_path),\n        workspace=DB(workspace_path),\n        preprompts=DB(\n            Path(__file__).parent / \"preprompts\"\n        ),  # Loads preprompts from the preprompts directory\n        archive=DB(archive_path),\n    )\n\n    if steps_config not in [\n        StepsConfig.EXECUTE_ONLY,\n        StepsConfig.USE_FEEDBACK,\n        StepsConfig.EVALUATE,\n        StepsConfig.IMPROVE_CODE,\n    ]:\n        archive(dbs)\n\n        if not dbs.input.get(\"prompt\"):\n            dbs.input[\"prompt\"] = input(\n                \"\\nWhat application do you want gpt-engineer to generate?\\n\"\n            )\n\n    steps = STEPS[steps_config]\n    for step in steps:\n        messages = step(ai, dbs)\n        dbs.logs[step.__name__] = AI.serialize_messages(messages)\n\n    if collect_consent():\n        collect_learnings(model, temperature, steps, dbs)\n\n    dbs.logs[\"token_usage\"] = ai.format_token_usage_log()", "\n\nif __name__ == \"__main__\":\n    app()\n"]}
{"filename": "gpt_engineer/chat_to_files.py", "chunked_list": ["import os\nimport re\n\nfrom typing import List, Tuple\n\nfrom gpt_engineer.db import DB\n\n\ndef parse_chat(chat) -> List[Tuple[str, str]]:\n    \"\"\"\n    Extracts all code blocks from a chat and returns them\n    as a list of (filename, codeblock) tuples.\n\n    Parameters\n    ----------\n    chat : str\n        The chat to extract code blocks from.\n\n    Returns\n    -------\n    List[Tuple[str, str]]\n        A list of tuples, where each tuple contains a filename and a code block.\n    \"\"\"\n    # Get all ``` blocks and preceding filenames\n    regex = r\"(\\S+)\\n\\s*```[^\\n]*\\n(.+?)```\"\n    matches = re.finditer(regex, chat, re.DOTALL)\n\n    files = []\n    for match in matches:\n        # Strip the filename of any non-allowed characters and convert / to \\\n        path = re.sub(r'[\\:<>\"|?*]', \"\", match.group(1))\n\n        # Remove leading and trailing brackets\n        path = re.sub(r\"^\\[(.*)\\]$\", r\"\\1\", path)\n\n        # Remove leading and trailing backticks\n        path = re.sub(r\"^`(.*)`$\", r\"\\1\", path)\n\n        # Remove trailing ]\n        path = re.sub(r\"[\\]\\:]$\", \"\", path)\n\n        # Get the code\n        code = match.group(2)\n\n        # Add the file to the list\n        files.append((path, code))\n\n    # Get all the text before the first ``` block\n    readme = chat.split(\"```\")[0]\n    files.append((\"README.md\", readme))\n\n    # Return the files\n    return files", "def parse_chat(chat) -> List[Tuple[str, str]]:\n    \"\"\"\n    Extracts all code blocks from a chat and returns them\n    as a list of (filename, codeblock) tuples.\n\n    Parameters\n    ----------\n    chat : str\n        The chat to extract code blocks from.\n\n    Returns\n    -------\n    List[Tuple[str, str]]\n        A list of tuples, where each tuple contains a filename and a code block.\n    \"\"\"\n    # Get all ``` blocks and preceding filenames\n    regex = r\"(\\S+)\\n\\s*```[^\\n]*\\n(.+?)```\"\n    matches = re.finditer(regex, chat, re.DOTALL)\n\n    files = []\n    for match in matches:\n        # Strip the filename of any non-allowed characters and convert / to \\\n        path = re.sub(r'[\\:<>\"|?*]', \"\", match.group(1))\n\n        # Remove leading and trailing brackets\n        path = re.sub(r\"^\\[(.*)\\]$\", r\"\\1\", path)\n\n        # Remove leading and trailing backticks\n        path = re.sub(r\"^`(.*)`$\", r\"\\1\", path)\n\n        # Remove trailing ]\n        path = re.sub(r\"[\\]\\:]$\", \"\", path)\n\n        # Get the code\n        code = match.group(2)\n\n        # Add the file to the list\n        files.append((path, code))\n\n    # Get all the text before the first ``` block\n    readme = chat.split(\"```\")[0]\n    files.append((\"README.md\", readme))\n\n    # Return the files\n    return files", "\n\ndef to_files(chat: str, workspace: DB):\n    \"\"\"\n    Parse the chat and add all extracted files to the workspace.\n\n    Parameters\n    ----------\n    chat : str\n        The chat to parse.\n    workspace : DB\n        The workspace to add the files to.\n    \"\"\"\n    workspace[\"all_output.txt\"] = chat  # TODO store this in memory db instead\n\n    files = parse_chat(chat)\n    for file_name, file_content in files:\n        workspace[file_name] = file_content", "\n\ndef overwrite_files(chat, dbs):\n    \"\"\"\n    Replace the AI files with the older local files.\n\n    Parameters\n    ----------\n    chat : str\n        The chat containing the AI files.\n    dbs : DBs\n        The database containing the workspace.\n    \"\"\"\n    dbs.workspace[\"all_output.txt\"] = chat  # TODO store this in memory db instead\n\n    files = parse_chat(chat)\n    for file_name, file_content in files:\n        if file_name == \"README.md\":\n            dbs.workspace[\n                \"LAST_MODIFICATION_README.md\"\n            ] = file_content  # TODO store this in memory db instead\n        else:\n            dbs.workspace[file_name] = file_content", "\n\ndef get_code_strings(input: DB) -> dict[str, str]:\n    \"\"\"\n    Read file_list.txt and return file names and their content.\n\n    Parameters\n    ----------\n    input : dict\n        A dictionary containing the file_list.txt.\n\n    Returns\n    -------\n    dict[str, str]\n        A dictionary mapping file names to their content.\n    \"\"\"\n    files_paths = input[\"file_list.txt\"].strip().split(\"\\n\")\n    files_dict = {}\n    for full_file_path in files_paths:\n        with open(full_file_path, \"r\") as file:\n            file_data = file.read()\n        if file_data:\n            file_name = os.path.relpath(full_file_path, input.path)\n            files_dict[file_name] = file_data\n    return files_dict", "\n\ndef format_file_to_input(file_name: str, file_content: str) -> str:\n    \"\"\"\n    Format a file string to use as input to the AI agent.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the file.\n    file_content : str\n        The content of the file.\n\n    Returns\n    -------\n    str\n        The formatted file string.\n    \"\"\"\n    file_str = f\"\"\"\n    {file_name}\n    ```\n    {file_content}\n    ```\n    \"\"\"\n    return file_str", ""]}
{"filename": "gpt_engineer/domain.py", "chunked_list": ["from typing import Callable, List, TypeVar\n\nfrom gpt_engineer.ai import AI\nfrom gpt_engineer.db import DBs\n\nStep = TypeVar(\"Step\", bound=Callable[[AI, DBs], List[dict]])\n"]}
{"filename": "gpt_engineer/db.py", "chunked_list": ["import datetime\nimport shutil\n\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Optional, Union\n\n\n# This class represents a simple database that stores its data as files in a directory.\nclass DB:\n    \"\"\"A simple key-value store, where keys are filenames and values are file contents.\"\"\"\n\n    def __init__(self, path: Union[str, Path]):\n        \"\"\"\n        Initialize the DB class.\n\n        Parameters\n        ----------\n        path : Union[str, Path]\n            The path to the directory where the database files are stored.\n        \"\"\"\n        self.path: Path = Path(path).absolute()\n\n        self.path.mkdir(parents=True, exist_ok=True)\n\n    def __contains__(self, key: str) -> bool:\n        \"\"\"\n        Check if a file with the specified name exists in the database.\n\n        Parameters\n        ----------\n        key : str\n            The name of the file to check.\n\n        Returns\n        -------\n        bool\n            True if the file exists, False otherwise.\n        \"\"\"\n        return (self.path / key).is_file()\n\n    def __getitem__(self, key: str) -> str:\n        \"\"\"\n        Get the content of a file in the database.\n\n        Parameters\n        ----------\n        key : str\n            The name of the file to get the content of.\n\n        Returns\n        -------\n        str\n            The content of the file.\n\n        Raises\n        ------\n        KeyError\n            If the file does not exist in the database.\n        \"\"\"\n        full_path = self.path / key\n\n        if not full_path.is_file():\n            raise KeyError(f\"File '{key}' could not be found in '{self.path}'\")\n        with full_path.open(\"r\", encoding=\"utf-8\") as f:\n            return f.read()\n\n    def get(self, key: str, default: Optional[Any] = None) -> Any:\n        \"\"\"\n        Get the content of a file in the database, or a default value if the file does not exist.\n\n        Parameters\n        ----------\n        key : str\n            The name of the file to get the content of.\n        default : any, optional\n            The default value to return if the file does not exist, by default None.\n\n        Returns\n        -------\n        any\n            The content of the file, or the default value if the file does not exist.\n        \"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __setitem__(self, key: Union[str, Path], val: str) -> None:\n        \"\"\"\n        Set the content of a file in the database.\n\n        Parameters\n        ----------\n        key : Union[str, Path]\n            The name of the file to set the content of.\n        val : str\n            The content to set.\n\n        Raises\n        ------\n        TypeError\n            If val is not string.\n        \"\"\"\n        if str(key).startswith(\"../\"):\n            raise ValueError(f\"File name {key} attempted to access parent path.\")\n\n        assert isinstance(val, str), \"val must be str\"\n\n        full_path = self.path / key\n        full_path.parent.mkdir(parents=True, exist_ok=True)\n\n        full_path.write_text(val, encoding=\"utf-8\")", "# This class represents a simple database that stores its data as files in a directory.\nclass DB:\n    \"\"\"A simple key-value store, where keys are filenames and values are file contents.\"\"\"\n\n    def __init__(self, path: Union[str, Path]):\n        \"\"\"\n        Initialize the DB class.\n\n        Parameters\n        ----------\n        path : Union[str, Path]\n            The path to the directory where the database files are stored.\n        \"\"\"\n        self.path: Path = Path(path).absolute()\n\n        self.path.mkdir(parents=True, exist_ok=True)\n\n    def __contains__(self, key: str) -> bool:\n        \"\"\"\n        Check if a file with the specified name exists in the database.\n\n        Parameters\n        ----------\n        key : str\n            The name of the file to check.\n\n        Returns\n        -------\n        bool\n            True if the file exists, False otherwise.\n        \"\"\"\n        return (self.path / key).is_file()\n\n    def __getitem__(self, key: str) -> str:\n        \"\"\"\n        Get the content of a file in the database.\n\n        Parameters\n        ----------\n        key : str\n            The name of the file to get the content of.\n\n        Returns\n        -------\n        str\n            The content of the file.\n\n        Raises\n        ------\n        KeyError\n            If the file does not exist in the database.\n        \"\"\"\n        full_path = self.path / key\n\n        if not full_path.is_file():\n            raise KeyError(f\"File '{key}' could not be found in '{self.path}'\")\n        with full_path.open(\"r\", encoding=\"utf-8\") as f:\n            return f.read()\n\n    def get(self, key: str, default: Optional[Any] = None) -> Any:\n        \"\"\"\n        Get the content of a file in the database, or a default value if the file does not exist.\n\n        Parameters\n        ----------\n        key : str\n            The name of the file to get the content of.\n        default : any, optional\n            The default value to return if the file does not exist, by default None.\n\n        Returns\n        -------\n        any\n            The content of the file, or the default value if the file does not exist.\n        \"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __setitem__(self, key: Union[str, Path], val: str) -> None:\n        \"\"\"\n        Set the content of a file in the database.\n\n        Parameters\n        ----------\n        key : Union[str, Path]\n            The name of the file to set the content of.\n        val : str\n            The content to set.\n\n        Raises\n        ------\n        TypeError\n            If val is not string.\n        \"\"\"\n        if str(key).startswith(\"../\"):\n            raise ValueError(f\"File name {key} attempted to access parent path.\")\n\n        assert isinstance(val, str), \"val must be str\"\n\n        full_path = self.path / key\n        full_path.parent.mkdir(parents=True, exist_ok=True)\n\n        full_path.write_text(val, encoding=\"utf-8\")", "\n\n# dataclass for all dbs:\n@dataclass\nclass DBs:\n    memory: DB\n    logs: DB\n    preprompts: DB\n    input: DB\n    workspace: DB\n    archive: DB", "\n\ndef archive(dbs: DBs) -> None:\n    \"\"\"\n    Archive the memory and workspace databases.\n\n    Parameters\n    ----------\n    dbs : DBs\n        The databases to archive.\n    \"\"\"\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    shutil.move(\n        str(dbs.memory.path), str(dbs.archive.path / timestamp / dbs.memory.path.name)\n    )\n    shutil.move(\n        str(dbs.workspace.path),\n        str(dbs.archive.path / timestamp / dbs.workspace.path.name),\n    )\n    return []", ""]}
{"filename": "gpt_engineer/steps.py", "chunked_list": ["import inspect\nimport re\nimport subprocess\n\nfrom enum import Enum\nfrom typing import List, Union\n\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nfrom termcolor import colored\n", "from termcolor import colored\n\nfrom gpt_engineer.ai import AI\nfrom gpt_engineer.chat_to_files import (\n    format_file_to_input,\n    get_code_strings,\n    overwrite_files,\n    to_files,\n)\nfrom gpt_engineer.db import DBs", ")\nfrom gpt_engineer.db import DBs\nfrom gpt_engineer.file_selector import FILE_LIST_NAME, ask_for_files\nfrom gpt_engineer.learning import human_review_input\n\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\n\ndef setup_sys_prompt(dbs: DBs) -> str:\n    \"\"\"\n    Primes the AI with instructions as to how it should\n    generate code and the philosophy to follow\n    \"\"\"\n    return (\n        dbs.preprompts[\"roadmap\"]\n        + dbs.preprompts[\"generate\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"])\n        + \"\\nUseful to know:\\n\"\n        + dbs.preprompts[\"philosophy\"]\n    )", "def setup_sys_prompt(dbs: DBs) -> str:\n    \"\"\"\n    Primes the AI with instructions as to how it should\n    generate code and the philosophy to follow\n    \"\"\"\n    return (\n        dbs.preprompts[\"roadmap\"]\n        + dbs.preprompts[\"generate\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"])\n        + \"\\nUseful to know:\\n\"\n        + dbs.preprompts[\"philosophy\"]\n    )", "\n\ndef setup_sys_prompt_existing_code(dbs: DBs) -> str:\n    \"\"\"\n    Similar to code generation, but using an existing code base.\n    \"\"\"\n    return (\n        dbs.preprompts[\"improve\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"])\n        + \"\\nUseful to know:\\n\"\n        + dbs.preprompts[\"philosophy\"]\n    )", "\n\ndef curr_fn() -> str:\n    \"\"\"\n    Get the name of the current function\n    NOTE: This will be the name of the function that called this function,\n    so it serves to ensure we don't hardcode the function name in the step,\n    but allow the step names to be refactored\n    \"\"\"\n    return inspect.stack()[1].function", "\n\n# All steps below have the Step signature\n\n\ndef simple_gen(ai: AI, dbs: DBs) -> List[Message]:\n    \"\"\"Run the AI on the main prompt and save the results\"\"\"\n    messages = ai.start(setup_sys_prompt(dbs), dbs.input[\"prompt\"], step_name=curr_fn())\n    to_files(messages[-1].content.strip(), dbs.workspace)\n    return messages", "\n\ndef clarify(ai: AI, dbs: DBs) -> List[Message]:\n    \"\"\"\n    Ask the user if they want to clarify anything and save the results to the workspace\n    \"\"\"\n    messages: List[Message] = [ai.fsystem(dbs.preprompts[\"clarify\"])]\n    user_input = dbs.input[\"prompt\"]\n    while True:\n        messages = ai.next(messages, user_input, step_name=curr_fn())\n        msg = messages[-1].content.strip()\n\n        if msg == \"Nothing more to clarify.\":\n            break\n\n        if msg.lower().startswith(\"no\"):\n            print(\"Nothing more to clarify.\")\n            break\n\n        print()\n        user_input = input('(answer in text, or \"c\" to move on)\\n')\n        print()\n\n        if not user_input or user_input == \"c\":\n            print(\"(letting gpt-engineer make its own assumptions)\")\n            print()\n            messages = ai.next(\n                messages,\n                \"Make your own assumptions and state them explicitly before starting\",\n                step_name=curr_fn(),\n            )\n            print()\n            return messages\n\n        user_input += (\n            \"\\n\\n\"\n            \"Is anything else unclear? If yes, only answer in the form:\\n\"\n            \"{remaining unclear areas} remaining questions.\\n\"\n            \"{Next question}\\n\"\n            'If everything is sufficiently clear, only answer \"Nothing more to clarify.\".'\n        )\n\n    print()\n    return messages", "\n\ndef gen_spec(ai: AI, dbs: DBs) -> List[Message]:\n    \"\"\"\n    Generate a spec from the main prompt + clarifications and save the results to\n    the workspace\n    \"\"\"\n    messages = [\n        ai.fsystem(setup_sys_prompt(dbs)),\n        ai.fsystem(f\"Instructions: {dbs.input['prompt']}\"),\n    ]\n\n    messages = ai.next(messages, dbs.preprompts[\"spec\"], step_name=curr_fn())\n\n    dbs.memory[\"specification\"] = messages[-1].content.strip()\n\n    return messages", "\n\ndef respec(ai: AI, dbs: DBs) -> List[Message]:\n    \"\"\"Asks the LLM to review the specs so far and reiterate them if necessary\"\"\"\n    messages = AI.deserialize_messages(dbs.logs[gen_spec.__name__])\n    messages += [ai.fsystem(dbs.preprompts[\"respec\"])]\n\n    messages = ai.next(messages, step_name=curr_fn())\n    messages = ai.next(\n        messages,\n        (\n            \"Based on the conversation so far, please reiterate the specification for \"\n            \"the program. \"\n            \"If there are things that can be improved, please incorporate the \"\n            \"improvements. \"\n            \"If you are satisfied with the specification, just write out the \"\n            \"specification word by word again.\"\n        ),\n        step_name=curr_fn(),\n    )\n\n    dbs.memory[\"specification\"] = messages[-1].content.strip()\n    return messages", "\n\ndef gen_unit_tests(ai: AI, dbs: DBs) -> List[dict]:\n    \"\"\"\n    Generate unit tests based on the specification, that should work.\n    \"\"\"\n    messages = [\n        ai.fsystem(setup_sys_prompt(dbs)),\n        ai.fuser(f\"Instructions: {dbs.input['prompt']}\"),\n        ai.fuser(f\"Specification:\\n\\n{dbs.memory['specification']}\"),\n    ]\n\n    messages = ai.next(messages, dbs.preprompts[\"unit_tests\"], step_name=curr_fn())\n\n    dbs.memory[\"unit_tests\"] = messages[-1].content.strip()\n    to_files(dbs.memory[\"unit_tests\"], dbs.workspace)\n\n    return messages", "\n\ndef gen_clarified_code(ai: AI, dbs: DBs) -> List[dict]:\n    \"\"\"Takes clarification and generates code\"\"\"\n    messages = AI.deserialize_messages(dbs.logs[clarify.__name__])\n\n    messages = [\n        ai.fsystem(setup_sys_prompt(dbs)),\n    ] + messages[\n        1:\n    ]  # skip the first clarify message, which was the original clarify priming prompt\n    messages = ai.next(\n        messages,\n        dbs.preprompts[\"generate\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"]),\n        step_name=curr_fn(),\n    )\n\n    to_files(messages[-1].content.strip(), dbs.workspace)\n    return messages", "\n\ndef gen_code_after_unit_tests(ai: AI, dbs: DBs) -> List[dict]:\n    \"\"\"Generates project code after unit tests have been produced\"\"\"\n    messages = [\n        ai.fsystem(setup_sys_prompt(dbs)),\n        ai.fuser(f\"Instructions: {dbs.input['prompt']}\"),\n        ai.fuser(f\"Specification:\\n\\n{dbs.memory['specification']}\"),\n        ai.fuser(f\"Unit tests:\\n\\n{dbs.memory['unit_tests']}\"),\n    ]\n    messages = ai.next(\n        messages,\n        dbs.preprompts[\"generate\"].replace(\"FILE_FORMAT\", dbs.preprompts[\"file_format\"]),\n        step_name=curr_fn(),\n    )\n    to_files(messages[-1].content.strip(), dbs.workspace)\n    return messages", "\n\ndef execute_entrypoint(ai: AI, dbs: DBs) -> List[dict]:\n    command = dbs.workspace[\"run.sh\"]\n\n    print()\n    print(\n        colored(\n            \"Do you want to execute this code? (y/n)\",\n            \"red\",\n        )\n    )\n    print()\n    print(command)\n    print()\n    print(\"To execute, you can also press enter.\")\n    print()\n    if input() not in [\"\", \"y\", \"yes\"]:\n        print(\"Ok, not executing the code.\")\n        return []\n    print(\"Executing the code...\")\n    print()\n    print(\n        colored(\n            \"Note: If it does not work as expected, consider running the code\"\n            + \" in another way than above.\",\n            \"green\",\n        )\n    )\n    print()\n    print(\"You can press ctrl+c *once* to stop the execution.\")\n    print()\n\n    p = subprocess.Popen(\"bash run.sh\", shell=True, cwd=dbs.workspace.path)\n    try:\n        p.wait()\n    except KeyboardInterrupt:\n        print()\n        print(\"Stopping execution.\")\n        print(\"Execution stopped.\")\n        p.kill()\n        print()\n\n    return []", "\n\ndef gen_entrypoint(ai: AI, dbs: DBs) -> List[dict]:\n    messages = ai.start(\n        system=(\n            \"You will get information about a codebase that is currently on disk in \"\n            \"the current folder.\\n\"\n            \"From this you will answer with code blocks that includes all the necessary \"\n            \"unix terminal commands to \"\n            \"a) install dependencies \"\n            \"b) run all necessary parts of the codebase (in parallel if necessary).\\n\"\n            \"Do not install globally. Do not use sudo.\\n\"\n            \"Do not explain the code, just give the commands.\\n\"\n            \"Do not use placeholders, use example values (like . for a folder argument) \"\n            \"if necessary.\\n\"\n        ),\n        user=\"Information about the codebase:\\n\\n\" + dbs.workspace[\"all_output.txt\"],\n        step_name=curr_fn(),\n    )\n    print()\n\n    regex = r\"```\\S*\\n(.+?)```\"\n    matches = re.finditer(regex, messages[-1].content.strip(), re.DOTALL)\n    dbs.workspace[\"run.sh\"] = \"\\n\".join(match.group(1) for match in matches)\n    return messages", "\n\ndef use_feedback(ai: AI, dbs: DBs):\n    messages = [\n        ai.fsystem(setup_sys_prompt(dbs)),\n        ai.fuser(f\"Instructions: {dbs.input['prompt']}\"),\n        ai.fassistant(\n            dbs.workspace[\"all_output.txt\"]\n        ),  # reload previously generated code\n    ]\n    if dbs.input[\"feedback\"]:\n        messages = ai.next(messages, dbs.input[\"feedback\"], step_name=curr_fn())\n        to_files(messages[-1].content.strip(), dbs.workspace)\n        return messages\n    else:\n        print(\n            \"No feedback was found in the input folder. Please create a file \"\n            + \"called 'feedback' in the same folder as the prompt file.\"\n        )\n        exit(1)", "\n\ndef set_improve_filelist(ai: AI, dbs: DBs):\n    \"\"\"Sets the file list for files to work with in existing code mode.\"\"\"\n    ask_for_files(dbs.input)  # stores files as full paths.\n    return []\n\n\ndef assert_files_ready(ai: AI, dbs: DBs):\n    \"\"\"Checks that the required files are present for headless\n    improve code execution.\"\"\"\n    assert (\n        \"file_list.txt\" in dbs.input\n    ), \"For auto_mode file_list.txt need to be in your project folder.\"\n    assert \"prompt\" in dbs.input, \"For auto_mode a prompt file must exist.\"\n    return []", "def assert_files_ready(ai: AI, dbs: DBs):\n    \"\"\"Checks that the required files are present for headless\n    improve code execution.\"\"\"\n    assert (\n        \"file_list.txt\" in dbs.input\n    ), \"For auto_mode file_list.txt need to be in your project folder.\"\n    assert \"prompt\" in dbs.input, \"For auto_mode a prompt file must exist.\"\n    return []\n\n\ndef get_improve_prompt(ai: AI, dbs: DBs):\n    \"\"\"\n    Asks the user what they would like to fix.\n    \"\"\"\n\n    if not dbs.input.get(\"prompt\"):\n        dbs.input[\"prompt\"] = input(\n            \"\\nWhat do you need to improve with the selected files?\\n\"\n        )\n\n    confirm_str = \"\\n\".join(\n        [\n            \"-----------------------------\",\n            \"The following files will be used in the improvement process:\",\n            f\"{FILE_LIST_NAME}:\",\n            str(dbs.input[\"file_list.txt\"]),\n            \"\",\n            \"The inserted prompt is the following:\",\n            f\"'{dbs.input['prompt']}'\",\n            \"-----------------------------\",\n            \"\",\n            \"You can change these files in your project before proceeding.\",\n            \"\",\n            \"Press enter to proceed with modifications.\",\n            \"\",\n        ]\n    )\n    input(confirm_str)\n    return []", "\n\ndef get_improve_prompt(ai: AI, dbs: DBs):\n    \"\"\"\n    Asks the user what they would like to fix.\n    \"\"\"\n\n    if not dbs.input.get(\"prompt\"):\n        dbs.input[\"prompt\"] = input(\n            \"\\nWhat do you need to improve with the selected files?\\n\"\n        )\n\n    confirm_str = \"\\n\".join(\n        [\n            \"-----------------------------\",\n            \"The following files will be used in the improvement process:\",\n            f\"{FILE_LIST_NAME}:\",\n            str(dbs.input[\"file_list.txt\"]),\n            \"\",\n            \"The inserted prompt is the following:\",\n            f\"'{dbs.input['prompt']}'\",\n            \"-----------------------------\",\n            \"\",\n            \"You can change these files in your project before proceeding.\",\n            \"\",\n            \"Press enter to proceed with modifications.\",\n            \"\",\n        ]\n    )\n    input(confirm_str)\n    return []", "\n\ndef improve_existing_code(ai: AI, dbs: DBs):\n    \"\"\"\n    After the file list and prompt have been aquired, this function is called\n    to sent the formatted prompt to the LLM.\n    \"\"\"\n\n    files_info = get_code_strings(dbs.input)  # this only has file names not paths\n\n    messages = [\n        ai.fsystem(setup_sys_prompt_existing_code(dbs)),\n    ]\n    # Add files as input\n    for file_name, file_str in files_info.items():\n        code_input = format_file_to_input(file_name, file_str)\n        messages.append(ai.fuser(f\"{code_input}\"))\n\n    messages.append(ai.fuser(f\"Request: {dbs.input['prompt']}\"))\n\n    messages = ai.next(messages, step_name=curr_fn())\n\n    overwrite_files(messages[-1].content.strip(), dbs)\n    return messages", "\n\ndef fix_code(ai: AI, dbs: DBs):\n    messages = AI.deserialize_messages(dbs.logs[gen_code_after_unit_tests.__name__])\n    code_output = messages[-1].content.strip()\n    messages = [\n        ai.fsystem(setup_sys_prompt(dbs)),\n        ai.fuser(f\"Instructions: {dbs.input['prompt']}\"),\n        ai.fuser(code_output),\n        ai.fsystem(dbs.preprompts[\"fix_code\"]),\n    ]\n    messages = ai.next(\n        messages, \"Please fix any errors in the code above.\", step_name=curr_fn()\n    )\n    to_files(messages[-1].content.strip(), dbs.workspace)\n    return messages", "\n\ndef human_review(ai: AI, dbs: DBs):\n    \"\"\"Collects and stores human review of the code\"\"\"\n    review = human_review_input()\n    if review is not None:\n        dbs.memory[\"review\"] = review.to_json()  # type: ignore\n    return []\n\n\nclass Config(str, Enum):\n    DEFAULT = \"default\"\n    BENCHMARK = \"benchmark\"\n    SIMPLE = \"simple\"\n    TDD = \"tdd\"\n    TDD_PLUS = \"tdd+\"\n    CLARIFY = \"clarify\"\n    RESPEC = \"respec\"\n    EXECUTE_ONLY = \"execute_only\"\n    EVALUATE = \"evaluate\"\n    USE_FEEDBACK = \"use_feedback\"\n    IMPROVE_CODE = \"improve_code\"\n    EVAL_IMPROVE_CODE = \"eval_improve_code\"", "\n\nclass Config(str, Enum):\n    DEFAULT = \"default\"\n    BENCHMARK = \"benchmark\"\n    SIMPLE = \"simple\"\n    TDD = \"tdd\"\n    TDD_PLUS = \"tdd+\"\n    CLARIFY = \"clarify\"\n    RESPEC = \"respec\"\n    EXECUTE_ONLY = \"execute_only\"\n    EVALUATE = \"evaluate\"\n    USE_FEEDBACK = \"use_feedback\"\n    IMPROVE_CODE = \"improve_code\"\n    EVAL_IMPROVE_CODE = \"eval_improve_code\"", "\n\n# Define the steps to run for different configs\nSTEPS = {\n    Config.DEFAULT: [\n        clarify,\n        gen_clarified_code,\n        gen_entrypoint,\n        execute_entrypoint,\n        human_review,", "        execute_entrypoint,\n        human_review,\n    ],\n    Config.BENCHMARK: [\n        simple_gen,\n        gen_entrypoint,\n    ],\n    Config.SIMPLE: [\n        simple_gen,\n        gen_entrypoint,", "        simple_gen,\n        gen_entrypoint,\n        execute_entrypoint,\n    ],\n    Config.TDD: [\n        gen_spec,\n        gen_unit_tests,\n        gen_code_after_unit_tests,\n        gen_entrypoint,\n        execute_entrypoint,", "        gen_entrypoint,\n        execute_entrypoint,\n        human_review,\n    ],\n    Config.TDD_PLUS: [\n        gen_spec,\n        gen_unit_tests,\n        gen_code_after_unit_tests,\n        fix_code,\n        gen_entrypoint,", "        fix_code,\n        gen_entrypoint,\n        execute_entrypoint,\n        human_review,\n    ],\n    Config.CLARIFY: [\n        clarify,\n        gen_clarified_code,\n        gen_entrypoint,\n        execute_entrypoint,", "        gen_entrypoint,\n        execute_entrypoint,\n        human_review,\n    ],\n    Config.RESPEC: [\n        gen_spec,\n        respec,\n        gen_unit_tests,\n        gen_code_after_unit_tests,\n        fix_code,", "        gen_code_after_unit_tests,\n        fix_code,\n        gen_entrypoint,\n        execute_entrypoint,\n        human_review,\n    ],\n    Config.USE_FEEDBACK: [use_feedback, gen_entrypoint, execute_entrypoint, human_review],\n    Config.EXECUTE_ONLY: [execute_entrypoint],\n    Config.EVALUATE: [execute_entrypoint, human_review],\n    Config.IMPROVE_CODE: [", "    Config.EVALUATE: [execute_entrypoint, human_review],\n    Config.IMPROVE_CODE: [\n        set_improve_filelist,\n        get_improve_prompt,\n        improve_existing_code,\n    ],\n    Config.EVAL_IMPROVE_CODE: [assert_files_ready, improve_existing_code],\n}\n\n# Future steps that can be added:", "\n# Future steps that can be added:\n# run_tests_and_fix_files\n# execute_entrypoint_and_fix_files_if_it_results_in_error\n"]}
{"filename": "gpt_engineer/__init__.py", "chunked_list": [""]}
{"filename": "gpt_engineer/file_selector.py", "chunked_list": ["import os\nimport re\nimport sys\nimport tkinter as tk\nimport tkinter.filedialog as fd\n\nfrom pathlib import Path\nfrom typing import List, Union\n\nIGNORE_FOLDERS = {\"site-packages\", \"node_modules\", \"venv\"}", "\nIGNORE_FOLDERS = {\"site-packages\", \"node_modules\", \"venv\"}\nFILE_LIST_NAME = \"file_list.txt\"\n\n\nclass DisplayablePath(object):\n    \"\"\"\n    A class representing a displayable path in a file explorer.\n    \"\"\"\n\n    display_filename_prefix_middle = \"\u251c\u2500\u2500 \"\n    display_filename_prefix_last = \"\u2514\u2500\u2500 \"\n    display_parent_prefix_middle = \"    \"\n    display_parent_prefix_last = \"\u2502   \"\n\n    def __init__(\n        self, path: Union[str, Path], parent_path: \"DisplayablePath\", is_last: bool\n    ):\n        \"\"\"\n        Initialize a DisplayablePath object.\n\n        Args:\n            path (Union[str, Path]): The path of the file or directory.\n            parent_path (DisplayablePath): The parent path of the file or directory.\n            is_last (bool): Whether the file or directory is the last child of its parent.\n        \"\"\"\n        self.depth: int = 0\n        self.path = Path(str(path))\n        self.parent = parent_path\n        self.is_last = is_last\n        if self.parent:\n            self.depth = self.parent.depth + 1\n\n    @property\n    def display_name(self) -> str:\n        \"\"\"\n        Get the display name of the file or directory.\n\n        Returns:\n            str: The display name.\n        \"\"\"\n        if self.path.is_dir():\n            return self.path.name + \"/\"\n        return self.path.name\n\n    @classmethod\n    def make_tree(cls, root: Union[str, Path], parent=None, is_last=False, criteria=None):\n        \"\"\"\n        Generate a tree of DisplayablePath objects.\n\n        Args:\n            root: The root path of the tree.\n            parent: The parent path of the root path. Defaults to None.\n            is_last: Whether the root path is the last child of its parent.\n            criteria: The criteria function to filter the paths. Defaults to None.\n\n        Yields:\n            DisplayablePath: The DisplayablePath objects in the tree.\n        \"\"\"\n        root = Path(str(root))\n        criteria = criteria or cls._default_criteria\n\n        displayable_root = cls(root, parent, is_last)\n        yield displayable_root\n\n        children = sorted(\n            list(path for path in root.iterdir() if criteria(path)),\n            key=lambda s: str(s).lower(),\n        )\n        count = 1\n        for path in children:\n            is_last = count == len(children)\n            if path.is_dir() and path.name not in IGNORE_FOLDERS:\n                yield from cls.make_tree(\n                    path, parent=displayable_root, is_last=is_last, criteria=criteria\n                )\n            else:\n                yield cls(path, displayable_root, is_last)\n            count += 1\n\n    @classmethod\n    def _default_criteria(cls, path: Path) -> bool:\n        \"\"\"\n        The default criteria function to filter the paths.\n\n        Args:\n            path: The path to check.\n\n        Returns:\n            bool: True if the path should be included, False otherwise.\n        \"\"\"\n        return True\n\n    def displayable(self) -> str:\n        \"\"\"\n        Get the displayable string representation of the file or directory.\n\n        Returns:\n            str: The displayable string representation.\n        \"\"\"\n        if self.parent is None:\n            return self.display_name\n\n        _filename_prefix = (\n            self.display_filename_prefix_last\n            if self.is_last\n            else self.display_filename_prefix_middle\n        )\n\n        parts = [\"{!s} {!s}\".format(_filename_prefix, self.display_name)]\n\n        parent = self.parent\n        while parent and parent.parent is not None:\n            parts.append(\n                self.display_parent_prefix_middle\n                if parent.is_last\n                else self.display_parent_prefix_last\n            )\n            parent = parent.parent\n\n        return \"\".join(reversed(parts))", "\n\nclass TerminalFileSelector:\n    def __init__(self, root_folder_path: Path) -> None:\n        self.number_of_selectable_items = 0\n        self.selectable_file_paths: dict[int, str] = {}\n        self.file_path_list: list = []\n        self.db_paths = DisplayablePath.make_tree(\n            root_folder_path, parent=None, criteria=is_in_ignoring_extensions\n        )\n\n    def display(self):\n        \"\"\"\n        Select files from a directory and display the selected files.\n        \"\"\"\n        count = 0\n        file_path_enumeration = {}\n        file_path_list = []\n        for path in self.db_paths:\n            n_digits = len(str(count))\n            n_spaces = 3 - n_digits\n            if n_spaces < 0:\n                # We can only print 1000 aligned files. I think it is decent enough\n                n_spaces = 0\n            spaces_str = \" \" * n_spaces\n            if not path.path.is_dir():\n                print(f\"{count}. {spaces_str}{path.displayable()}\")\n                file_path_enumeration[count] = path.path\n                file_path_list.append(path.path)\n                count += 1\n            else:\n                # By now we do not accept selecting entire dirs.\n                # But could add that in the future. Just need to add more functions\n                # and remove this else block...\n                number_space = \" \" * n_digits\n                print(f\"{number_space}  {spaces_str}{path.displayable()}\")\n\n        self.number_of_selectable_items = count\n        self.file_path_list = file_path_list\n        self.selectable_file_paths = file_path_enumeration\n\n    def ask_for_selection(self) -> List[str]:\n        \"\"\"\n        Ask user to select files from the terminal after displaying it\n\n        Returns:\n            List[str]: list of selected paths\n        \"\"\"\n        user_input = input(\n            \"\\n\".join(\n                [\n                    \"Select files by entering the numbers separated by commas/spaces or\",\n                    \"specify range with a dash. \",\n                    \"Example: 1,2,3-5,7,9,13-15,18,20 (enter 'all' to select everything)\",\n                    \"\\n\\nSelect files:\",\n                ]\n            )\n        )\n        selected_paths = []\n        regex = r\"\\d+(-\\d+)?([, ]\\d+(-\\d+)?)*\"\n\n        if user_input.lower() == \"all\":\n            selected_paths = self.file_path_list\n        elif re.match(regex, user_input):\n            try:\n                user_input = (\n                    user_input.replace(\" \", \",\") if \" \" in user_input else user_input\n                )\n                selected_files = user_input.split(\",\")\n                for file_number_str in selected_files:\n                    if \"-\" in file_number_str:\n                        start_str, end_str = file_number_str.split(\"-\")\n                        start = int(start_str)\n                        end = int(end_str)\n                        for num in range(start, end + 1):\n                            selected_paths.append(str(self.selectable_file_paths[num]))\n                    else:\n                        num = int(file_number_str)\n                        selected_paths.append(str(self.selectable_file_paths[num]))\n\n            except ValueError:\n                pass\n        else:\n            print(\"Please use a valid number/series of numbers.\\n\")\n            sys.exit(1)\n\n        return selected_paths", "\n\ndef is_in_ignoring_extensions(path: Path) -> bool:\n    \"\"\"\n    Check if a path is not hidden or in the __pycache__ directory.\n\n    Args:\n        path: The path to check.\n\n    Returns:\n        bool: True if the path is not in ignored rules. False otherwise.\n    \"\"\"\n    is_hidden = not path.name.startswith(\".\")\n    is_pycache = \"__pycache__\" not in path.name\n    return is_hidden and is_pycache", "\n\ndef ask_for_files(db_input) -> None:\n    \"\"\"\n    Ask user to select files to improve.\n    It can be done by terminal, gui, or using the old selection.\n\n    Returns:\n        dict[str, str]: Dictionary where key = file name and value = file path\n    \"\"\"\n    use_last_string = \"\"\n    if \"file_list.txt\" in db_input:\n        use_last_string = (\n            \"3. Use previous file list (available at \"\n            + f\"{os.path.join(db_input.path, 'file_list.txt')})\\n\"\n        )\n        selection_number = 3\n    else:\n        selection_number = 1\n    selection_str = \"\\n\".join(\n        [\n            \"How do you want to select the files?\",\n            \"\",\n            \"1. Use File explorer.\",\n            \"2. Use Command-Line.\",\n            use_last_string if len(use_last_string) > 1 else \"\",\n            f\"Select option and press Enter (default={selection_number}): \",\n        ]\n    )\n\n    file_path_list = []\n    selected_number_str = input(selection_str)\n    if selected_number_str:\n        try:\n            selection_number = int(selected_number_str)\n        except ValueError:\n            print(\"Invalid number. Select a number from the list above.\\n\")\n            sys.exit(1)\n\n    if selection_number == 1:\n        # Open GUI selection\n        file_path_list = gui_file_selector()\n    elif selection_number == 2:\n        # Open terminal selection\n        file_path_list = terminal_file_selector()\n    if (\n        selection_number <= 0\n        or selection_number > 3\n        or (selection_number == 3 and not use_last_string)\n    ):\n        print(\"Invalid number. Select a number from the list above.\\n\")\n        sys.exit(1)\n\n    if not selection_number == 3:\n        db_input[\"file_list.txt\"] = \"\\n\".join(file_path_list)", "\n\ndef gui_file_selector() -> List[str]:\n    \"\"\"\n    Display a tkinter file selection window to select context files.\n    \"\"\"\n    root = tk.Tk()\n    root.withdraw()\n    root.call(\"wm\", \"attributes\", \".\", \"-topmost\", True)\n    file_list = list(\n        fd.askopenfilenames(\n            parent=root,\n            initialdir=os.getcwd(),\n            title=\"Select files to improve (or give context):\",\n        )\n    )\n    return file_list", "\n\ndef terminal_file_selector() -> List[str]:\n    \"\"\"\n    Display a terminal file selection to select context files.\n    \"\"\"\n    file_selector = TerminalFileSelector(Path(os.getcwd()))\n    file_selector.display()\n    selected_list = file_selector.ask_for_selection()\n    return selected_list", ""]}
{"filename": "gpt_engineer/collect.py", "chunked_list": ["import hashlib\n\nfrom typing import List\n\nfrom gpt_engineer import steps\nfrom gpt_engineer.db import DBs\nfrom gpt_engineer.domain import Step\nfrom gpt_engineer.learning import Learning, extract_learning\n\n\ndef send_learning(learning: Learning):\n    \"\"\"\n    Send the learning data to RudderStack for analysis.\n\n    Note:\n    This function is only called if consent is given to share data.\n    Data is not shared to a third party. It is used with the sole purpose of\n    improving gpt-engineer, and letting it handle more use cases.\n    Consent logic is in gpt_engineer/learning.py\n\n    Parameters\n    ----------\n    learning : Learning\n        The learning data to send.\n    \"\"\"\n    import rudderstack.analytics as rudder_analytics\n\n    rudder_analytics.write_key = \"2Re4kqwL61GDp7S8ewe6K5dbogG\"\n    rudder_analytics.dataPlaneUrl = \"https://gptengineerezm.dataplane.rudderstack.com\"\n\n    rudder_analytics.track(\n        user_id=learning.session,\n        event=\"learning\",\n        properties=learning.to_dict(),  # type: ignore\n    )", "\n\ndef send_learning(learning: Learning):\n    \"\"\"\n    Send the learning data to RudderStack for analysis.\n\n    Note:\n    This function is only called if consent is given to share data.\n    Data is not shared to a third party. It is used with the sole purpose of\n    improving gpt-engineer, and letting it handle more use cases.\n    Consent logic is in gpt_engineer/learning.py\n\n    Parameters\n    ----------\n    learning : Learning\n        The learning data to send.\n    \"\"\"\n    import rudderstack.analytics as rudder_analytics\n\n    rudder_analytics.write_key = \"2Re4kqwL61GDp7S8ewe6K5dbogG\"\n    rudder_analytics.dataPlaneUrl = \"https://gptengineerezm.dataplane.rudderstack.com\"\n\n    rudder_analytics.track(\n        user_id=learning.session,\n        event=\"learning\",\n        properties=learning.to_dict(),  # type: ignore\n    )", "\n\ndef collect_learnings(model: str, temperature: float, steps: List[Step], dbs: DBs):\n    \"\"\"\n    Collect the learning data and send it to RudderStack for analysis.\n\n    Parameters\n    ----------\n    model : str\n        The name of the model used.\n    temperature : float\n        The temperature used.\n    steps : List[Step]\n        The list of steps.\n    dbs : DBs\n        The database containing the workspace.\n    \"\"\"\n    learnings = extract_learning(\n        model, temperature, steps, dbs, steps_file_hash=steps_file_hash()\n    )\n    try:\n        send_learning(learnings)\n    except RuntimeError as e:\n        # try to remove some parts of learning that might be too big\n        # rudderstack max event size is 32kb\n        overflow = len(learnings.to_json()) - (32 << 10)  # type: ignore\n        assert overflow > 0, f\"encountered error {e} but overflow is {overflow}\"\n\n        learnings.logs = (\n            learnings.logs[: -overflow - 200] + f\"\\n\\n[REMOVED {overflow} CHARACTERS]\"\n        )\n        print(\n            \"WARNING: learning too big, removing some parts. \"\n            \"Please report if this results in a crash.\"\n        )\n        send_learning(learnings)", "\n\ndef steps_file_hash():\n    \"\"\"\n    Compute the SHA-256 hash of the steps file.\n\n    Returns\n    -------\n    str\n        The SHA-256 hash of the steps file.\n    \"\"\"\n    with open(steps.__file__, \"r\") as f:\n        content = f.read()\n        return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()", ""]}
{"filename": "gpt_engineer/learning.py", "chunked_list": ["import json\nimport random\nimport tempfile\n\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import List, Optional\n\nfrom dataclasses_json import dataclass_json", "\nfrom dataclasses_json import dataclass_json\nfrom termcolor import colored\n\nfrom gpt_engineer.db import DB, DBs\nfrom gpt_engineer.domain import Step\n\n\n@dataclass_json\n@dataclass\nclass Review:\n    ran: Optional[bool]\n    perfect: Optional[bool]\n    works: Optional[bool]\n    comments: str\n    raw: str", "@dataclass_json\n@dataclass\nclass Review:\n    ran: Optional[bool]\n    perfect: Optional[bool]\n    works: Optional[bool]\n    comments: str\n    raw: str\n\n", "\n\n@dataclass_json\n@dataclass\nclass Learning:\n    model: str\n    temperature: float\n    steps: str\n    steps_file_hash: str\n    prompt: str\n    logs: str\n    workspace: str\n    feedback: Optional[str]\n    session: str\n    review: Optional[Review]\n    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n    version: str = \"0.3\"", "\n\nTERM_CHOICES = (\n    colored(\"y\", \"green\")\n    + \"/\"\n    + colored(\"n\", \"red\")\n    + \"/\"\n    + colored(\"u\", \"yellow\")\n    + \"(ncertain): \"\n)", "    + \"(ncertain): \"\n)\n\n\ndef human_review_input() -> Review:\n    \"\"\"\n    Ask the user to review the generated code and return their review.\n\n    Returns\n    -------\n    Review\n        The user's review of the generated code.\n    \"\"\"\n    print()\n    if not check_consent():\n        return None\n    print()\n    print(\n        colored(\"To help gpt-engineer learn, please answer 3 questions:\", \"light_green\")\n    )\n    print()\n\n    ran = input(\"Did the generated code run at all? \" + TERM_CHOICES)\n    while ran not in (\"y\", \"n\", \"u\"):\n        ran = input(\"Invalid input. Please enter y, n, or u: \")\n\n    perfect = \"\"\n    useful = \"\"\n\n    if ran == \"y\":\n        perfect = input(\n            \"Did the generated code do everything you wanted? \" + TERM_CHOICES\n        )\n        while perfect not in (\"y\", \"n\", \"u\"):\n            perfect = input(\"Invalid input. Please enter y, n, or u: \")\n\n        if perfect != \"y\":\n            useful = input(\"Did the generated code do anything useful? \" + TERM_CHOICES)\n            while useful not in (\"y\", \"n\", \"u\"):\n                useful = input(\"Invalid input. Please enter y, n, or u: \")\n\n    comments = \"\"\n    if perfect != \"y\":\n        comments = input(\n            \"If you have time, please explain what was not working \"\n            + colored(\"(ok to leave blank)\\n\", \"light_green\")\n        )\n\n    return Review(\n        raw=\", \".join([ran, perfect, useful]),\n        ran={\"y\": True, \"n\": False, \"u\": None, \"\": None}[ran],\n        works={\"y\": True, \"n\": False, \"u\": None, \"\": None}[useful],\n        perfect={\"y\": True, \"n\": False, \"u\": None, \"\": None}[perfect],\n        comments=comments,\n    )", "\n\ndef check_consent() -> bool:\n    \"\"\"\n    Check if the user has given consent to store their data.\n    If not, ask for their consent.\n    \"\"\"\n    path = Path(\".gpte_consent\")\n    if path.exists() and path.read_text() == \"true\":\n        return True\n    answer = input(\"Is it ok if we store your prompts to learn? (y/n)\")\n    while answer.lower() not in (\"y\", \"n\"):\n        answer = input(\"Invalid input. Please enter y or n: \")\n\n    if answer.lower() == \"y\":\n        path.write_text(\"true\")\n        print(colored(\"Thank you\ufe0f\", \"light_green\"))\n        print()\n        print(\"(If you change your mind, delete the file .gpte_consent)\")\n        return True\n    else:\n        print(colored(\"We understand \u2764\ufe0f\", \"light_green\"))\n        return False", "\n\ndef collect_consent() -> bool:\n    \"\"\"\n    Check if the user has given consent to store their data.\n    If not, ask for their consent.\n\n    Returns\n    -------\n    bool\n        True if the user has given consent, False otherwise.\n    \"\"\"\n    consent_flag = Path(\".gpte_consent\")\n    has_given_consent = consent_flag.exists() and consent_flag.read_text() == \"true\"\n\n    if has_given_consent:\n        return True\n\n    if ask_if_can_store():\n        consent_flag.write_text(\"true\")\n        print()\n        print(\"(If you change your mind, delete the file .gpte_consent)\")\n        return True\n    return False", "\n\ndef ask_if_can_store() -> bool:\n    \"\"\"\n    Ask the user if their data can be stored.\n\n    Returns\n    -------\n    bool\n        True if the user agrees to have their data stored, False otherwise.\n    \"\"\"\n    print()\n    can_store = input(\n        \"Have you understood and agree to that \"\n        + colored(\"OpenAI \", \"light_green\")\n        + \"and \"\n        + colored(\"gpt-engineer \", \"light_green\")\n        + \"store anonymous learnings about how gpt-engineer is used \"\n        + \"(with the sole purpose of improving it)?\\n(y/n)\"\n    ).lower()\n    while can_store not in (\"y\", \"n\"):\n        can_store = input(\"Invalid input. Please enter y or n: \").lower()\n\n    if can_store == \"n\":\n        print(colored(\"Ok we understand\", \"light_green\"))\n\n    return can_store == \"y\"", "\n\ndef logs_to_string(steps: List[Step], logs: DB) -> str:\n    \"\"\"\n    Convert the logs of the steps to a string.\n\n    Parameters\n    ----------\n    steps : List[Step]\n        The list of steps.\n    logs : DB\n        The database containing the logs.\n\n    Returns\n    -------\n    str\n        The logs of the steps as a string.\n    \"\"\"\n    chunks = []\n    for step in steps:\n        chunks.append(f\"--- {step.__name__} ---\\n\")\n        chunks.append(logs[step.__name__])\n    return \"\\n\".join(chunks)", "\n\ndef extract_learning(\n    model: str, temperature: float, steps: List[Step], dbs: DBs, steps_file_hash\n) -> Learning:\n    \"\"\"\n    Extract the learning data from the steps and databases.\n\n    Parameters\n    ----------\n    model : str\n        The name of the model used.\n    temperature : float\n        The temperature used.\n    steps : List[Step]\n        The list of steps.\n    dbs : DBs\n        The databases containing the input, logs, memory, and workspace.\n    steps_file_hash : str\n        The hash of the steps file.\n\n    Returns\n    -------\n    Learning\n        The extracted learning data.\n    \"\"\"\n    review = None\n    if \"review\" in dbs.memory:\n        review = Review.from_json(dbs.memory[\"review\"])  # type: ignore\n    learning = Learning(\n        prompt=dbs.input[\"prompt\"],\n        model=model,\n        temperature=temperature,\n        steps=json.dumps([step.__name__ for step in steps]),\n        steps_file_hash=steps_file_hash,\n        feedback=dbs.input.get(\"feedback\"),\n        session=get_session(),\n        logs=logs_to_string(steps, dbs.logs),\n        workspace=dbs.workspace[\"all_output.txt\"],\n        review=review,\n    )\n    return learning", "\n\ndef get_session() -> str:\n    \"\"\"\n    Returns a unique user id for the current user project (session).\n\n    Returns\n    -------\n    str\n        The unique user id.\n    \"\"\"\n    path = Path(tempfile.gettempdir()) / \"gpt_engineer_user_id.txt\"\n\n    try:\n        if path.exists():\n            user_id = path.read_text()\n        else:\n            # random uuid:\n            user_id = str(random.randint(0, 2**32))\n            path.write_text(user_id)\n        return user_id\n    except IOError:\n        return \"ephemeral_\" + str(random.randint(0, 2**32))", ""]}
{"filename": "gpt_engineer/ai.py", "chunked_list": ["from __future__ import annotations\n\nimport json\nimport logging\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Union\n\nimport openai\nimport tiktoken", "import openai\nimport tiktoken\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models import AzureChatOpenAI, ChatOpenAI\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,", "    HumanMessage,\n    SystemMessage,\n    messages_from_dict,\n    messages_to_dict,\n)\n\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\nlogger = logging.getLogger(__name__)\n", "logger = logging.getLogger(__name__)\n\n\n@dataclass\nclass TokenUsage:\n    step_name: str\n    in_step_prompt_tokens: int\n    in_step_completion_tokens: int\n    in_step_total_tokens: int\n    total_prompt_tokens: int\n    total_completion_tokens: int\n    total_tokens: int", "\n\nclass AI:\n    def __init__(self, model_name=\"gpt-4\", temperature=0.1, azure_endpoint=\"\"):\n        \"\"\"\n        Initialize the AI class.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The name of the model to use, by default \"gpt-4\".\n        temperature : float, optional\n            The temperature to use for the model, by default 0.1.\n        \"\"\"\n        self.temperature = temperature\n        self.azure_endpoint = azure_endpoint\n        self.model_name = (\n            fallback_model(model_name) if azure_endpoint == \"\" else model_name\n        )\n        self.llm = create_chat_model(self, self.model_name, self.temperature)\n        self.tokenizer = get_tokenizer(self.model_name)\n        logger.debug(f\"Using model {self.model_name} with llm {self.llm}\")\n\n        # initialize token usage log\n        self.cumulative_prompt_tokens = 0\n        self.cumulative_completion_tokens = 0\n        self.cumulative_total_tokens = 0\n        self.token_usage_log = []\n\n    def start(self, system: str, user: str, step_name: str) -> List[Message]:\n        \"\"\"\n        Start the conversation with a system message and a user message.\n\n        Parameters\n        ----------\n        system : str\n            The content of the system message.\n        user : str\n            The content of the user message.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages in the conversation.\n        \"\"\"\n        messages: List[Message] = [\n            SystemMessage(content=system),\n            HumanMessage(content=user),\n        ]\n        return self.next(messages, step_name=step_name)\n\n    def fsystem(self, msg: str) -> SystemMessage:\n        \"\"\"\n        Create a system message.\n\n        Parameters\n        ----------\n        msg : str\n            The content of the message.\n\n        Returns\n        -------\n        SystemMessage\n            The created system message.\n        \"\"\"\n        return SystemMessage(content=msg)\n\n    def fuser(self, msg: str) -> HumanMessage:\n        \"\"\"\n        Create a user message.\n\n        Parameters\n        ----------\n        msg : str\n            The content of the message.\n\n        Returns\n        -------\n        HumanMessage\n            The created user message.\n        \"\"\"\n        return HumanMessage(content=msg)\n\n    def fassistant(self, msg: str) -> AIMessage:\n        \"\"\"\n        Create an AI message.\n\n        Parameters\n        ----------\n        msg : str\n            The content of the message.\n\n        Returns\n        -------\n        AIMessage\n            The created AI message.\n        \"\"\"\n        return AIMessage(content=msg)\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Advances the conversation by sending message history\n        to LLM and updating with the response.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        prompt : Optional[str], optional\n            The prompt to use, by default None.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The updated list of messages in the conversation.\n        \"\"\"\n        \"\"\"\n        Advances the conversation by sending message history\n        to LLM and updating with the response.\n        \"\"\"\n        if prompt:\n            messages.append(self.fuser(prompt))\n\n        logger.debug(f\"Creating a new chat completion: {messages}\")\n\n        callsbacks = [StreamingStdOutCallbackHandler()]\n        response = self.llm(messages, callbacks=callsbacks)  # type: ignore\n        messages.append(response)\n\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        self.update_token_usage_log(\n            messages=messages, answer=response.content, step_name=step_name\n        )\n\n        return messages\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        \"\"\"\n        Serialize a list of messages to a JSON string.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to serialize.\n\n        Returns\n        -------\n        str\n            The serialized messages as a JSON string.\n        \"\"\"\n        return json.dumps(messages_to_dict(messages))\n\n    @staticmethod\n    def deserialize_messages(jsondictstr: str) -> List[Message]:\n        \"\"\"\n        Deserialize a JSON string to a list of messages.\n\n        Parameters\n        ----------\n        jsondictstr : str\n            The JSON string to deserialize.\n\n        Returns\n        -------\n        List[Message]\n            The deserialized list of messages.\n        \"\"\"\n        return list(messages_from_dict(json.loads(jsondictstr)))  # type: ignore\n\n    def update_token_usage_log(\n        self, messages: List[Message], answer: str, step_name: str\n    ) -> None:\n        \"\"\"\n        Update the token usage log with the number of tokens used in the current step.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        answer : str\n            The answer from the AI.\n        step_name : str\n            The name of the step.\n        \"\"\"\n        prompt_tokens = self.num_tokens_from_messages(messages)\n        completion_tokens = self.num_tokens(answer)\n        total_tokens = prompt_tokens + completion_tokens\n\n        self.cumulative_prompt_tokens += prompt_tokens\n        self.cumulative_completion_tokens += completion_tokens\n        self.cumulative_total_tokens += total_tokens\n\n        self.token_usage_log.append(\n            TokenUsage(\n                step_name=step_name,\n                in_step_prompt_tokens=prompt_tokens,\n                in_step_completion_tokens=completion_tokens,\n                in_step_total_tokens=total_tokens,\n                total_prompt_tokens=self.cumulative_prompt_tokens,\n                total_completion_tokens=self.cumulative_completion_tokens,\n                total_tokens=self.cumulative_total_tokens,\n            )\n        )\n\n    def format_token_usage_log(self) -> str:\n        \"\"\"\n        Format the token usage log as a CSV string.\n\n        Returns\n        -------\n        str\n            The token usage log formatted as a CSV string.\n        \"\"\"\n        result = \"step_name,\"\n        result += \"prompt_tokens_in_step,completion_tokens_in_step,total_tokens_in_step\"\n        result += \",total_prompt_tokens,total_completion_tokens,total_tokens\\n\"\n        for log in self.token_usage_log:\n            result += log.step_name + \",\"\n            result += str(log.in_step_prompt_tokens) + \",\"\n            result += str(log.in_step_completion_tokens) + \",\"\n            result += str(log.in_step_total_tokens) + \",\"\n            result += str(log.total_prompt_tokens) + \",\"\n            result += str(log.total_completion_tokens) + \",\"\n            result += str(log.total_tokens) + \"\\n\"\n        return result\n\n    def num_tokens(self, txt: str) -> int:\n        \"\"\"\n        Get the number of tokens in a text.\n\n        Parameters\n        ----------\n        txt : str\n            The text to count the tokens in.\n\n        Returns\n        -------\n        int\n            The number of tokens in the text.\n        \"\"\"\n        return len(self.tokenizer.encode(txt))\n\n    def num_tokens_from_messages(self, messages: List[Message]) -> int:\n        \"\"\"\n        Get the total number of tokens used by a list of messages.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to count the tokens in.\n\n        Returns\n        -------\n        int\n            The total number of tokens used by the messages.\n        \"\"\"\n        \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n        n_tokens = 0\n        for message in messages:\n            n_tokens += (\n                4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            )\n            n_tokens += self.num_tokens(message.content)\n        n_tokens += 2  # every reply is primed with <im_start>assistant\n        return n_tokens", "\n\ndef fallback_model(model: str) -> str:\n    \"\"\"\n    Retrieve the specified model, or fallback to \"gpt-3.5-turbo\" if the model is not available.\n\n    Parameters\n    ----------\n    model : str\n        The name of the model to retrieve.\n\n    Returns\n    -------\n    str\n        The name of the retrieved model, or \"gpt-3.5-turbo\" if the specified model is not available.\n    \"\"\"\n    try:\n        openai.Model.retrieve(model)\n        return model\n    except openai.InvalidRequestError:\n        print(\n            f\"Model {model} not available for provided API key. Reverting \"\n            \"to gpt-3.5-turbo. Sign up for the GPT-4 wait list here: \"\n            \"https://openai.com/waitlist/gpt-4-api\\n\"\n        )\n        return \"gpt-3.5-turbo\"", "\n\ndef create_chat_model(self, model: str, temperature) -> BaseChatModel:\n    \"\"\"\n    Create a chat model with the specified model name and temperature.\n\n    Parameters\n    ----------\n    model : str\n        The name of the model to create.\n    temperature : float\n        The temperature to use for the model.\n\n    Returns\n    -------\n    BaseChatModel\n        The created chat model.\n    \"\"\"\n    if self.azure_endpoint:\n        return AzureChatOpenAI(\n            openai_api_base=self.azure_endpoint,\n            openai_api_version=\"2023-05-15\",  # might need to be flexible in the future\n            deployment_name=model,\n            openai_api_type=\"azure\",\n            streaming=True,\n        )\n    # Fetch available models from OpenAI API\n    supported = [model[\"id\"] for model in openai.Model.list()[\"data\"]]\n    if model not in supported:\n        raise ValueError(\n            f\"Model {model} is not supported, supported models are: {supported}\"\n        )\n    return ChatOpenAI(\n        model=model,\n        temperature=temperature,\n        streaming=True,\n        client=openai.ChatCompletion,\n    )", "\n\ndef get_tokenizer(model: str):\n    \"\"\"\n    Get the tokenizer for the specified model.\n\n    Parameters\n    ----------\n    model : str\n        The name of the model to get the tokenizer for.\n\n    Returns\n    -------\n    Tokenizer\n        The tokenizer for the specified model.\n    \"\"\"\n    if \"gpt-4\" in model or \"gpt-3.5\" in model:\n        return tiktoken.encoding_for_model(model)\n\n    logger.debug(\n        f\"No encoder implemented for model {model}.\"\n        \"Defaulting to tiktoken cl100k_base encoder.\"\n        \"Use results only as estimates.\"\n    )\n    return tiktoken.get_encoding(\"cl100k_base\")", "\n\ndef serialize_messages(messages: List[Message]) -> str:\n    return AI.serialize_messages(messages)\n"]}
{"filename": "docs/conf.py", "chunked_list": ["#!/usr/bin/env python\n#\n# file_processor documentation build configuration file, created by\n# sphinx-quickstart on Fri Jun  9 13:47:02 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.", "# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another\n# directory, add these directories to sys.path here. If the directory is\n# relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.", "# relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.\n#\nimport os\nimport sys\n\nfrom pathlib import Path\n\nimport toml\n", "import toml\n\nsys.path.insert(0, os.path.abspath(\"..\"))\n\nROOT_DIR = Path(__file__).parents[1].absolute()\n\nwith open(\"../pyproject.toml\") as f:\n    data = toml.load(f)\n\n", "\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = data[\"project\"][\"name\"]\ncopyright = \"2023 Anton Osika\"\nauthor = \" Anton Osika & Contributors\"\n", "author = \" Anton Osika & Contributors\"\n\n# The version info for the project you're documenting, acts as replacement\n# for |version| and |release|, also used in various other places throughout\n# the built documents.\n#\n# The short X.Y version.\nversion = data[\"project\"][\"version\"]\n# The full version, including alpha/beta/rc tags.\nrelease = data[\"project\"][\"version\"]", "# The full version, including alpha/beta/rc tags.\nrelease = data[\"project\"][\"version\"]\n\n\n# -- General configuration ---------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n", "# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autodoc.typehints\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.viewcode\",", "    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx_copybutton\",\n    \"sphinx_panels\",\n    \"myst_parser\",\n    \"IPython.sphinxext.ipython_console_highlighting\",\n]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:", "# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n\nsource_suffix = [\".rst\", \".md\"]\n\nautodoc_pydantic_model_show_json = False\nautodoc_pydantic_field_list_validators = False\nautodoc_pydantic_config_members = False\nautodoc_pydantic_model_show_config_summary = False\nautodoc_pydantic_model_show_validator_members = False", "autodoc_pydantic_model_show_config_summary = False\nautodoc_pydantic_model_show_validator_members = False\nautodoc_pydantic_model_show_validator_summary = False\nautodoc_pydantic_model_signature_prefix = \"class\"\nautodoc_pydantic_field_signature_prefix = \"param\"\nautodoc_member_order = \"groupwise\"\nautoclass_content = \"both\"\nautodoc_typehints_format = \"short\"\n\nautodoc_default_options = {", "\nautodoc_default_options = {\n    \"members\": True,\n    \"show-inheritance\": True,\n    \"inherited-members\": \"BaseModel\",\n    \"undoc-members\": True,\n    \"special-members\": \"__call__\",\n}\n\n# Add any paths that contain templates here, relative to this directory.", "\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n\n# source_suffix = '.rst'\n\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.", "# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path", "# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n", "todo_include_todos = False\n\n\n# -- Options for HTML output -------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = 'alabaster'\nhtml_theme = \"sphinx_rtd_theme\"", "# html_theme = 'alabaster'\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a\n# theme further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,", "\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n\n# -- Options for HTMLHelp output ---------------------------------------\n\n# Output file base name for HTML help builder.", "\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"gpt_engineerdoc\"\n\n\n# -- Options for LaTeX output ------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #", "    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n    # Latex figure (float) alignment", "    # 'preamble': '',\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass\n# [howto, manual, or own class]).\nlatex_documents = [", "# [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \"gpt_engineer.tex\", \"GPT-ENgineer Documentation\", \"manual\"),\n]\n\n\n# -- Options for manual page output ------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).", "# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \"gpt_engineer\", \"GPT-Engineer Documentation\", [author], 1)]\n\n\n# -- Options for Texinfo output ----------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)", "# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        \"gpt_engineer\",\n        \"GPT-Engineer Documentation\",\n        author,\n        \"gpt_engineer\",\n        \"One line description of project.\",", "        \"gpt_engineer\",\n        \"One line description of project.\",\n        \"Miscellaneous\",\n    ),\n]\n\n# generate autosummary even if no references\nautosummary_generate = True\n\nmyst_enable_extensions = [", "\nmyst_enable_extensions = [\n    \"colon_fence\",\n]\n"]}
{"filename": "docs/create_api_rst.py", "chunked_list": ["\"\"\"Script for auto-generating api_reference.rst\"\"\"\nimport glob\nimport re\n\nfrom pathlib import Path\n\nROOT_DIR = Path(__file__).parents[1].absolute()\nprint(ROOT_DIR)\nPKG_DIR = ROOT_DIR / \"gpt_engineer\"\nWRITE_FILE = Path(__file__).parent / \"api_reference.rst\"", "PKG_DIR = ROOT_DIR / \"gpt_engineer\"\nWRITE_FILE = Path(__file__).parent / \"api_reference.rst\"\n\n\ndef load_members() -> dict:\n    members: dict = {}\n    for py in glob.glob(str(PKG_DIR) + \"/**/*.py\", recursive=True):\n        module = py[len(str(PKG_DIR)) + 1 :].replace(\".py\", \"\").replace(\"/\", \".\")\n        top_level = module.split(\".\")[0]\n        if top_level not in members:\n            members[top_level] = {\"classes\": [], \"functions\": []}\n        with open(py, \"r\") as f:\n            for line in f.readlines():\n                cls = re.findall(r\"^class ([^_].*)\\(\", line)\n                members[top_level][\"classes\"].extend([module + \".\" + c for c in cls])\n                func = re.findall(r\"^def ([^_].*)\\(\", line)\n                afunc = re.findall(r\"^async def ([^_].*)\\(\", line)\n                func_strings = [module + \".\" + f for f in func + afunc]\n                members[top_level][\"functions\"].extend(func_strings)\n    return members", "\n\ndef construct_doc(members: dict) -> str:\n    full_doc = \"\"\"\\\n.. _api_reference:\n\n=============\nAPI Reference\n=============\n\n\"\"\"\n    for module, _members in sorted(members.items(), key=lambda kv: kv[0]):\n        classes = _members[\"classes\"]\n        functions = _members[\"functions\"]\n        if not (classes or functions):\n            continue\n\n        module_title = module.replace(\"_\", \" \").title()\n        if module_title == \"Llms\":\n            module_title = \"LLMs\"\n        section = f\":mod:`gpt_engineer.{module}`: {module_title}\"\n        full_doc += f\"\"\"\\\n{section}\n{'=' * (len(section) + 1)}\n\n.. automodule:: gpt_engineer.{module}\n    :no-members:\n    :no-inherited-members:\n\n\"\"\"\n\n        if classes:\n            cstring = \"\\n    \".join(sorted(classes))\n            full_doc += f\"\"\"\\\nClasses\n--------------\n.. currentmodule:: gpt_engineer\n\n.. autosummary::\n    :toctree: {module}\n    :template: class.rst\n\n    {cstring}\n\n\"\"\"\n        if functions:\n            fstring = \"\\n    \".join(sorted(functions))\n            full_doc += f\"\"\"\\\nFunctions\n--------------\n.. currentmodule:: gpt_engineer\n\n.. autosummary::\n    :toctree: {module}\n\n    {fstring}\n\n\"\"\"\n    return full_doc", "\n\ndef main() -> None:\n    members = load_members()\n    full_doc = construct_doc(members)\n    with open(WRITE_FILE, \"w\") as f:\n        f.write(full_doc)\n\n\nif __name__ == \"__main__\":\n    main()", "\nif __name__ == \"__main__\":\n    main()\n"]}
