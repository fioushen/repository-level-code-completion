{"filename": "tensorrt-conda/recipe/run_test.py", "chunked_list": ["from vapoursynth import core\n\nprint(core.cycmunet.CycMuNetVersion())\n"]}
{"filename": "mindspore/inference.py", "chunked_list": ["import time\nfrom collections import namedtuple\n\nimport numpy as np\nimport mindspore as ms\n\nfrom model import CycMuNet\nfrom util.normalize import Normalizer\nfrom dataset.video import VideoFrameDataset\n", "from dataset.video import VideoFrameDataset\n\ndummyArg = namedtuple('dummyArg', (\n    'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n    'stop_at_conf'))\n\nsize = 128\nargs = dummyArg(nf=64, groups=8, upscale_factor=2, format='yuv420', layers=4, cycle_count=3, batch_mode='batch',\n                all_frames=True, stop_at_conf=False)\n", "                all_frames=True, stop_at_conf=False)\n\nds_path = r\"/home/ma-user/work/cctv-scaled/\"\n# ds_path = r\"./test-files/\"\n\nif __name__ == '__main__':\n    ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n    # ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"Ascend\")\n    # ms.set_context(mode=ms.GRAPH_MODE, device_target=\"CPU\")\n    # ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"CPU\")\n\n    print('Init done')\n    model = CycMuNet(args)\n    ms.load_checkpoint(\"model-files/2x_yuv420_cycle3_layer4.ckpt\", model)\n\n    # model = model.to_float(ms.float16)\n    print('Load done')\n\n    nm = Normalizer()\n    ds_test = VideoFrameDataset(ds_path + \"index-test.txt\", size, args.upscale_factor, True, nm)\n    ds_test = ds_test.batch(1)\n\n    start = time.time()\n    for n, data in enumerate(ds_test.create_tuple_iterator()):\n        start = time.time()\n        # data = [t.astype(ms.float16) for t in data]\n        inputs = [data[6:8], data[10:12]]\n        model(*inputs)\n        print(f\"#{n:0>3} inference in {time.time() - start}s\")", ""]}
{"filename": "mindspore/train.py", "chunked_list": ["import os\nimport time\nfrom collections import namedtuple\nimport argparse\n\nimport numpy as np\nimport tqdm\nimport mindspore as ms\nfrom mindspore import nn, ops\n", "from mindspore import nn, ops\n\nfrom model import CycMuNet, TrainModel\nfrom util.rmse import RMSELoss\nfrom util.normalize import Normalizer\nfrom dataset.video import VideoFrameDataset\n\nprint(\"Initialized.\")\n\ndummyArg = namedtuple('dummyArg', (", "\ndummyArg = namedtuple('dummyArg', (\n    'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames', 'stop_at_conf'))\n\nsize = 128\nargs = dummyArg(nf=64, groups=8, upscale_factor=2, format='yuv420', layers=4, cycle_count=3, batch_mode='batch',\n                all_frames=True, stop_at_conf=False)\n\nepochs = 1\nbatch_size = 1", "epochs = 1\nbatch_size = 1\nlearning_rate = 0.001\nsave_prefix = 'monitor'\n\nds_path = r\"/home/ma-user/work/cctv-scaled/\"\npretrained = \"/home/ma-user/work/cycmunet-ms/model-files/2x_yuv420_cycle3_layer4.ckpt\"\nsave_path = \"/home/ma-user/work/cycmunet-ms/checkpoints/\"\n\n# ds_path = r\"./test-files/index-train.txt\"", "\n# ds_path = r\"./test-files/index-train.txt\"\n# pretrained = \"./model-files/2x_yuv420_cycle3_layer4.ckpt\"\n# save_path = \"./checkpoints/\"\n\n# ds_path = r\"D:\\Python\\cycmunet-ms\\test-files/\"\n# pretrained = \"\"\n\n# parser = argparse.ArgumentParser(\n#     prog='CycMuNet+ MindSpore Training')", "# parser = argparse.ArgumentParser(\n#     prog='CycMuNet+ MindSpore Training')\n#\n# parser.add_argument('--dataset')\n# parser.add_argument('--pretrained')\n# parser.add_argument('--save')\n#\n# cmd_args = parser.parse_args()\n#\n# ds_path = cmd_args.dataset", "#\n# ds_path = cmd_args.dataset\n# pretrained = cmd_args.pretrained\n# save_path = cmd_args.save\n\n\nsave_prefix = f'{save_prefix}_{args.upscale_factor}x_l{args.layers}_c{args.cycle_count}'\n\nnetwork = CycMuNet(args)\nif pretrained:\n    ms.load_checkpoint(pretrained, network)", "network = CycMuNet(args)\nif pretrained:\n    ms.load_checkpoint(pretrained, network)\n\n\nms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n\ninp = (ms.Tensor(np.zeros((batch_size, 1, size, size), dtype=np.float32)),\n       ms.Tensor(np.zeros((batch_size, 2, size // 2, size // 2), dtype=np.float32)))\nnetwork.compile(inp, inp)", "       ms.Tensor(np.zeros((batch_size, 2, size // 2, size // 2), dtype=np.float32)))\nnetwork.compile(inp, inp)\ninp = None\n\n\nloss_fn = RMSELoss()\n\nnm = Normalizer()\nds_train = VideoFrameDataset(ds_path + \"index-train.txt\", size, args.upscale_factor, True, nm)\nds_test = VideoFrameDataset(ds_path + \"index-test.txt\", size, args.upscale_factor, True, nm)", "ds_train = VideoFrameDataset(ds_path + \"index-train.txt\", size, args.upscale_factor, True, nm)\nds_test = VideoFrameDataset(ds_path + \"index-test.txt\", size, args.upscale_factor, True, nm)\n\nds_train = ds_train.batch(batch_size)\nds_test = ds_test.batch(batch_size)\n\nscheduler = nn.CosineDecayLR(min_lr=1e-7, max_lr=learning_rate, decay_steps=640000)\noptimizer = nn.AdaMax(network.trainable_params(), learning_rate=learning_rate)\n\n", "\n\nmodel = TrainModel(network, loss_fn)\nmodel = ms.Model(model, optimizer=optimizer, eval_network=model, boost_level=\"O1\")\n\n\ndef save_model(epoch):\n    if epoch == -1:\n        name = \"snapshot\"\n    else:\n        name = f\"epoch_{epoch}\"\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    output_path = save_path + f\"{save_prefix}_{name}.ckpt\"\n    ms.save_checkpoint(network, str(output_path))\n    print(f\"Checkpoint saved to {output_path}\")", "\n\nprint(\"Start train.\")\n\nprofiler = ms.Profiler(output_path='./profiler_data')\n\nfor t in range(1, epochs + 1):\n    try:\n        print(f\"Epoch {t}\\n-------------------------------\")\n        model.train(t, ds_train, dataset_sink_mode=True)\n        save_model(t)\n    except KeyboardInterrupt:\n        save_model(-1)", "\nprofiler.analyse()\n\nprint(\"Done.\")\n"]}
{"filename": "mindspore/build_mindir.py", "chunked_list": ["from collections import namedtuple\nimport time\n\nimport numpy as np\nimport mindspore as ms\n\nfrom model import CycMuNet\n\n\ndef export_rgb(checkpoint, size):\n    dummyArg = namedtuple('dummyArg', (\n        'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n        'stop_at_conf'))\n\n    args = dummyArg(nf=64, groups=8, upscale_factor=4, format='rgb', layers=3, cycle_count=5, batch_mode='sequence',\n                    all_frames=False, stop_at_conf=False)\n\n    ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n\n    print('Init done')\n    build_start = time.time()\n    model = CycMuNet(args)\n    ms.load_checkpoint(checkpoint, model)\n\n    inp = ms.Tensor(np.ones((2, 3, *size), dtype=np.float32))\n    inp = inp.astype(ms.float16)\n    model = model.to_float(ms.float16)\n    model.compile(inp)\n    print(f'Load done in {time.time() - build_start}s')\n    # verify\n    model(inp)\n\n    ms.export(model, inp, file_name=model, file_format='MINDIR')\n    print('Export done')", "\ndef export_rgb(checkpoint, size):\n    dummyArg = namedtuple('dummyArg', (\n        'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n        'stop_at_conf'))\n\n    args = dummyArg(nf=64, groups=8, upscale_factor=4, format='rgb', layers=3, cycle_count=5, batch_mode='sequence',\n                    all_frames=False, stop_at_conf=False)\n\n    ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n\n    print('Init done')\n    build_start = time.time()\n    model = CycMuNet(args)\n    ms.load_checkpoint(checkpoint, model)\n\n    inp = ms.Tensor(np.ones((2, 3, *size), dtype=np.float32))\n    inp = inp.astype(ms.float16)\n    model = model.to_float(ms.float16)\n    model.compile(inp)\n    print(f'Load done in {time.time() - build_start}s')\n    # verify\n    model(inp)\n\n    ms.export(model, inp, file_name=model, file_format='MINDIR')\n    print('Export done')", "\n\ndef export_yuv(checkpoint, size):\n    dummyArg = namedtuple('dummyArg', (\n        'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n        'stop_at_conf'))\n\n    args = dummyArg(nf=64, groups=8, upscale_factor=2, format='yuv420', layers=4, cycle_count=3, batch_mode='sequence',\n                    all_frames=False, stop_at_conf=False)\n\n    ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n\n    print('Init done')\n    build_start = time.time()\n    model = CycMuNet(args)\n    ms.load_checkpoint(checkpoint, model)\n\n    inp_y = ms.Tensor(np.zeros((2, 1, *size), dtype=np.float16))\n    inp_uv = ms.Tensor(np.zeros((2, 2, size[0] // 2, size[1] // 2), dtype=np.float16))\n    model = model.to_float(ms.float16)\n    model.compile(inp_y, inp_uv)\n    print(f'Load done in {time.time() - build_start}s')\n    # verify\n    model(inp_y, inp_uv)\n\n    ms.export(model, inp_y, inp_uv, file_name=model, file_format='MINDIR')\n    print('Export done')", "\n\nif __name__ == '__main__':\n    # export_rgb('model-files/2x_rgb_base.ckpt', 'model-files/cycmunet_2x_rgb', (64, 64))\n    export_yuv('model-files/2x_yuv420_cycle3_layer4.ckpt', 'model-files/cycmunet_2x_yuv420_cycle3_layer4', (1920, 1088))\n"]}
{"filename": "mindspore/convert_weight.py", "chunked_list": ["import re\nfrom collections import namedtuple\n\nimport mindspore as ms\nfrom mindspore import ops\nimport torch\n\nimport model\n\n\ndef transform_dcnpack(weights):\n    result = {\n        'dcn_weight': weights['dcn.weight'],\n        'dcn_bias': weights['dcn.bias'],\n        'conv_mask.weight': weights['conv_mask.weight'],\n        'conv_mask.bias': weights['conv_mask.bias'],\n    }\n\n    w = weights['conv_offset.weight'].reshape(72, 2, 64, 3, 3)\n    b = weights['conv_offset.bias'].reshape(72, 2)\n    w = w[:, ::-1, ...].transpose(1, 0, 2, 3, 4).reshape(144, 64, 3, 3)\n    b = b[:, ::-1, ...].transpose(1, 0).reshape(144)\n\n    result['conv_offset.weight'] = w\n    result['conv_offset.bias'] = b\n    return result", "\n\ndef transform_dcnpack(weights):\n    result = {\n        'dcn_weight': weights['dcn.weight'],\n        'dcn_bias': weights['dcn.bias'],\n        'conv_mask.weight': weights['conv_mask.weight'],\n        'conv_mask.bias': weights['conv_mask.bias'],\n    }\n\n    w = weights['conv_offset.weight'].reshape(72, 2, 64, 3, 3)\n    b = weights['conv_offset.bias'].reshape(72, 2)\n    w = w[:, ::-1, ...].transpose(1, 0, 2, 3, 4).reshape(144, 64, 3, 3)\n    b = b[:, ::-1, ...].transpose(1, 0).reshape(144)\n\n    result['conv_offset.weight'] = w\n    result['conv_offset.bias'] = b\n    return result", "\n\nif __name__ == '__main__':\n    torch_source = 'checkpoints/2x_cycle3_yuv420_sparsity_epoch_20.pth'\n    ms_normal = 'checkpoints/2x_yuv420_cycle3_layer4.ckpt'\n    ms_sep = 'checkpoints/2x_sep_yuv420_cycle3_layer4.ckpt'\n\n    dummyArg = namedtuple('dummyArg', (\n        'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n        'stop_at_conf'))\n\n    size = (64, 64)\n    args = dummyArg(nf=64, groups=8, upscale_factor=2, format='yuv420', layers=4, cycle_count=3, batch_mode='sequence',\n                    all_frames=False, stop_at_conf=False)\n\n    print('Init done')\n\n    rewrite_names = {\n        \".Pro_align.conv1x1.\": 3,\n        \".Pro_align.conv1_3x3.\": 2,\n        \".offset_conv1.\": 2,\n        \".offset_conv2.\": 2,\n        \".fea_conv.\": 2,\n        \"ff.fusion.\": 2,\n        \"mu.conv.\": 2 * args.cycle_count + 1\n    }\n\n    rewrite_names_re = {\n        r\"(merge1?\\.(\\d+)\\.)\": lambda match: int(match[2]) + 1,\n    }\n\n    # normal model\n    model_normal = model.CycMuNet(args)\n\n    source = torch.load(torch_source, map_location=torch.device('cpu'))\n    source = {k: v for k, v in source.items() if '__weight_mma_mask' not in k}\n    template = model_normal.parameters_dict()\n\n    dest = dict()\n    pending_dcn = dict()\n    for k, v in source.items():\n        if '.dcnpack.' in k:\n            module, name = k.split('.dcnpack.')\n            if module in pending_dcn:\n                pending_dcn[module][name] = v.numpy()\n            else:\n                pending_dcn[module] = {name: v.numpy()}\n            continue\n\n        for name in rewrite_names:\n            k = k.replace(name, name + 'conv.')\n        for re_name in rewrite_names_re:\n            k = re.sub(re_name, \"\\\\1conv.\", k)\n        if k in template:\n            dest[k] = ms.Parameter(v.numpy())\n        else:\n            print(f\"Unknown parameter {k} ignored.\")\n\n    for m, ws in pending_dcn.items():\n        for name, w in transform_dcnpack(ws).items():\n            dest[f'{m}.dcnpack.{name}'] = ms.Parameter(w)\n\n    print(ms.load_param_into_net(model_normal, dest, strict_load=True))\n    ms.save_checkpoint(model_normal, ms_normal)\n\n    print('Done normal model')\n\n    # sep model: concat + conv is separated to multiple conv + add, to reduce memory footprint\n    model_separate = model.cycmunet_sep(args)\n    template = model_separate.parameters_dict()\n\n    dest = dict()\n    pending_dcn = dict()\n\n    def filter_catconv(k, tensor):\n        for name, n in rewrite_names.items():\n            if name in k:\n                t = ms.Parameter(tensor.numpy())\n                if k.endswith('.weight'):\n                    dest.update({k.replace(name, f'{name}convs.{i}.'): ms.Parameter(v) for i, v in\n                                 enumerate(ops.split(t, axis=1, output_num=n))})\n                elif k.endswith('.bias'):\n                    dest[k.replace(name, name + 'convs.0.')] = t\n                return True\n        for name, get_n in rewrite_names_re.items():\n            search_result = re.search(name, k)\n            if not search_result:\n                continue\n            n = get_n(search_result)\n            t = ms.Parameter(tensor.numpy())\n            if k.endswith('.weight'):\n                dest.update({re.sub(name, f'\\\\1convs.{i}.', k): ms.Parameter(v) for i, v in\n                             enumerate(ops.split(t, axis=1, output_num=n))})\n            elif k.endswith('.bias'):\n                dest[re.sub(name, f'\\\\1convs.0.', k)] = t\n            return True\n        return False\n\n    for k, v in source.items():\n        if '.dcnpack.' in k:\n            module, name = k.split('.dcnpack.')\n            if module in pending_dcn:\n                pending_dcn[module][name] = v.numpy()\n            else:\n                pending_dcn[module] = {name: v.numpy()}\n            continue\n\n        if filter_catconv(k, v):\n            continue\n        if k in template:\n            dest[k] = ms.Parameter(v.numpy())\n        else:\n            print(f\"Unknown parameter {k} ignored.\")\n\n    for m, ws in pending_dcn.items():\n        for name, w in transform_dcnpack(ws).items():\n            dest[f'{m}.dcnpack.{name}'] = ms.Parameter(w)\n\n    print(ms.load_param_into_net(model_separate, dest, strict_load=True))\n    ms.save_checkpoint(model_separate, ms_sep)\n\n    print('Done separate model')", ""]}
{"filename": "mindspore/model/model.py", "chunked_list": ["from collections.abc import Iterable\n\nimport mindspore as ms\nfrom mindspore import nn, ops\n\nfrom .deform_conv import DCN_sep_compat\n\n# half_pixel not supported on cpu now. use align_corners for test temporarily.\n# coordinate_transformation_mode = 'half_pixel'\ncoordinate_transformation_mode = 'align_corners'", "# coordinate_transformation_mode = 'half_pixel'\ncoordinate_transformation_mode = 'align_corners'\n\n\ndef Conv2d(in_channels,\n           out_channels,\n           kernel_size,\n           stride=1,\n           padding=0,\n           dilation=1,\n           has_bias=True):\n    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                     pad_mode='pad', padding=padding, dilation=dilation, has_bias=has_bias)", "\n\ndef Conv2dTranspose(in_channels,\n                    out_channels,\n                    kernel_size,\n                    stride=1,\n                    dilation=1):\n    return nn.Conv2dTranspose(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                              stride=stride, pad_mode='same', dilation=dilation, has_bias=True)\n", "\n\ndef float_tuple(*tuple_):\n    tuple_ = tuple_[0] if isinstance(tuple_[0], Iterable) else tuple_\n    return tuple(float(i) for i in tuple_)\n\n\ndef transpose(mat):\n    count = len(mat[0])\n    ret = [[] for _ in range(count)]\n    for i in mat:\n        for j in range(count):\n            ret[j].append(i[j])\n    return ret", "\n\ndef cat_simp(elements, axis=1):\n    if len(elements) == 1:\n        return elements[0]\n    else:\n        return ms.ops.concat(elements, axis=axis)\n\n\nclass CatConv(nn.Cell):\n    def __init__(self, in_channels, in_count, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n        super().__init__()\n        self.arg_pack = {\n            'in_channels': in_channels,\n            'in_count': in_count,\n            'out_channels': out_channels,\n            'kernel_size': kernel_size,\n            'stride': stride,\n            'padding': padding,\n            'dilation': dilation\n        }\n        self.in_count = in_count\n        self.conv = Conv2d(in_channels * in_count, out_channels, kernel_size, stride, padding, dilation)\n\n    def construct(self, *inputs):\n        return self.conv(ops.concat(inputs, axis=1))", "\nclass CatConv(nn.Cell):\n    def __init__(self, in_channels, in_count, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n        super().__init__()\n        self.arg_pack = {\n            'in_channels': in_channels,\n            'in_count': in_count,\n            'out_channels': out_channels,\n            'kernel_size': kernel_size,\n            'stride': stride,\n            'padding': padding,\n            'dilation': dilation\n        }\n        self.in_count = in_count\n        self.conv = Conv2d(in_channels * in_count, out_channels, kernel_size, stride, padding, dilation)\n\n    def construct(self, *inputs):\n        return self.conv(ops.concat(inputs, axis=1))", "\n\nclass Pro_align(nn.Cell):\n    def __init__(self, args):\n        super(Pro_align, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n\n        self.conv1x1 = CatConv(self.nf, 3, self.nf, 1, 1, 0)\n        self.conv3x3 = Conv2d(self.nf, self.nf, 3, 1, 1)\n        self.conv1_3x3 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def construct(self, l1, l2, l3):\n        r1 = self.lrelu(self.conv3x3(l1))\n        r2 = self.lrelu(self.conv3x3(l2))\n        r3 = self.lrelu(self.conv3x3(l3))\n        fuse = self.lrelu(self.conv1x1(r1, r2, r3))\n        r1 = self.lrelu(self.conv1_3x3(r1, fuse))\n        r2 = self.lrelu(self.conv1_3x3(r2, fuse))\n        r3 = self.lrelu(self.conv1_3x3(r3, fuse))\n        return l1 + r1, l2 + r2, l3 + r3", "\n\nclass SR(nn.Cell):\n    def __init__(self, args):\n        super(SR, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.factor = (1, 1, self.args.upscale_factor, self.args.upscale_factor)\n        self.Pro_align = Pro_align(args)\n        self.conv1x1 = Conv2d(self.nf, self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def upsample(self, x):\n        x = ops.interpolate(x, scales=float_tuple(self.factor), mode='bilinear',\n                            coordinate_transformation_mode=coordinate_transformation_mode)\n        return self.lrelu(self.conv1x1(x))\n\n    def construct(self, l1, l2, l3):\n        l1, l2, l3 = self.Pro_align(l1, l2, l3)\n        return tuple(self.upsample(i) for i in (l1, l2, l3))", "\n\nclass DR(nn.Cell):\n    def __init__(self, args):\n        super(DR, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.factor = (1, 1, 1 / self.args.upscale_factor, 1 / self.args.upscale_factor)\n        self.Pro_align = Pro_align(args)\n        self.conv = Conv2d(self.nf, self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def downsample(self, x):\n        x = ops.interpolate(x, scales=float_tuple(self.factor), mode='bilinear',\n                            coordinate_transformation_mode=coordinate_transformation_mode)\n        return self.lrelu(self.conv(x))\n\n    def construct(self, l1, l2, l3):\n        l1 = self.downsample(l1)\n        l2 = self.downsample(l2)\n        l3 = self.downsample(l3)\n        return self.Pro_align(l1, l2, l3)", "\n\nclass Up_projection(nn.Cell):\n    def __init__(self, args):\n        super(Up_projection, self).__init__()\n        self.args = args\n        self.SR = SR(args)\n        self.DR = DR(args)\n        self.SR1 = SR(args)\n\n    def construct(self, l1, l2, l3):\n        h1, h2, h3 = self.SR(l1, l2, l3)\n        d1, d2, d3 = self.DR(h1, h2, h3)\n        r1, r2, r3 = d1 - l1, d2 - l2, d3 - l3\n        s1, s2, s3 = self.SR1(r1, r2, r3)\n        return h1 + s1, h2 + s3, h3 + s3", "\n\nclass Down_projection(nn.Cell):\n    def __init__(self, args):\n        super(Down_projection, self).__init__()\n        self.args = args\n        self.SR = SR(args)\n        self.DR = DR(args)\n        self.DR1 = DR(args)\n\n    def construct(self, h1, h2, h3):\n        l1, l2, l3 = self.DR(h1, h2, h3)\n        s1, s2, s3 = self.SR(l1, l2, l3)\n        r1, r2, r3 = s1 - h1, s2 - h2, s3 - h3\n        d1, d2, d3 = self.DR1(r1, r2, r3)\n        return l1 + d1, l2 + d2, l3 + d3", "\n\nclass FusionPyramidLayer(nn.Cell):\n    def __init__(self, args, first_layer: bool):\n        super(FusionPyramidLayer, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n\n        self.offset_conv1 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n        self.offset_conv3 = Conv2d(self.nf, self.nf, 3, 1, 1)\n        self.dcnpack = DCN_sep_compat(self.nf, self.nf, self.nf, 3, stride=1, padding=1, dilation=1,\n                                      deformable_groups=self.groups)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n        if not first_layer:\n            self.offset_conv2 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n            self.fea_conv = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\n    def construct(self, current_sources, last_offset, last_feature):\n        offset = self.lrelu(self.offset_conv1(*current_sources))\n        if last_offset is not None:\n            last_offset = ops.interpolate(last_offset, scales=float_tuple(1, 1, 2, 2), mode='bilinear',\n                                          coordinate_transformation_mode=coordinate_transformation_mode)\n            offset = self.lrelu(self.offset_conv2(offset, last_offset * 2))\n        offset = self.lrelu(self.offset_conv3(offset))\n        feature = self.dcnpack(current_sources[0], offset)\n        if last_feature is not None:\n            last_feature = ops.interpolate(last_feature, scales=float_tuple(1, 1, 2, 2), mode='bilinear',\n                                           coordinate_transformation_mode=coordinate_transformation_mode)\n            feature = self.fea_conv(feature, last_feature)\n        feature = self.lrelu(feature)\n        return offset, feature", "\n\nclass ResidualBlock_noBN(nn.Cell):\n    '''Residual block w/o BN\n    ---Conv-ReLU-Conv-+-\n     |________________|\n    '''\n\n    def __init__(self, nf=64):\n        super(ResidualBlock_noBN, self).__init__()\n        self.conv1 = Conv2d(nf, nf, 3, 1, 1)\n        self.conv2 = Conv2d(nf, nf, 3, 1, 1)\n        self.relu = ops.ReLU()\n\n        # TODO: initialization\n        # initialize_weights([self.conv1, self.conv2], 0.1)\n\n    def construct(self, x):\n        identity = x\n        out = self.relu(self.conv1(x))\n        out = self.conv2(out)\n        return identity + out", "\n\nclass cycmunet_head(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_head, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n        if self.args.format == 'rgb':\n            self.conv_first = Conv2d(3, self.nf, 3, 1, 1)\n            self._construct = self.construct_rgb\n        elif self.args.format == 'yuv444':\n            self.conv_first = Conv2d(3, self.nf, 3, 1, 1)\n            self._construct = self.construct_yuv444\n        elif self.args.format == 'yuv422':\n            self.conv_first_y = Conv2d(1, self.nf, 3, 1, 1)\n            self.conv_up = Conv2dTranspose(2, self.nf, (1, 3), (1, 2))\n            self._construct = self.construct_yuv42x\n        elif self.args.format == 'yuv420':\n            self.conv_first_y = Conv2d(1, self.nf, 3, 1, 1)\n            self.conv_up = Conv2dTranspose(2, self.nf, 3, 2)\n            self._construct = self.construct_yuv42x\n        else:\n            raise ValueError(f'unknown input pixel format: {self.args.format}')\n\n    def construct_rgb(self, x):\n        x = self.lrelu(self.conv_first(x))\n        return x\n\n    def construct_yuv444(self, yuv):\n        x = ms.ops.concat(yuv, axis=1)\n        x = self.lrelu(self.conv_first(x))\n        return x\n\n    def construct_yuv42x(self, yuv):\n        y, uv = yuv\n        y = self.conv_first_y(y)\n        uv = self.conv_up(uv)\n        x = self.lrelu(y + uv)\n        return x\n\n    def construct(self, *args):\n        return self._construct(*args)", "\n\nclass cycmunet_feature_extract(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_feature_extract, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n        self.layers = self.args.layers\n        self.front_RBs = 5\n\n        self.feature_extraction = nn.SequentialCell(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.front_RBs)))\n        self.fea_conv1s = nn.CellList([Conv2d(self.nf, self.nf, 3, 2, 1) for _ in range(self.layers - 1)])\n        self.fea_conv2s = nn.CellList([Conv2d(self.nf, self.nf, 3, 1, 1) for _ in range(self.layers - 1)])\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def construct(self, x):\n        features = [self.feature_extraction(x)]\n        for i in range(self.layers - 1):\n            feature = features[-1]\n            feature = self.lrelu(self.fea_conv1s[i](feature))\n            feature = self.lrelu(self.fea_conv2s[i](feature))\n            features.append(feature)\n        return tuple(features[::-1])  # lowest dimension layer at first", "\n\nclass cycmunet_feature_fusion(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_feature_fusion, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n        self.layers = self.args.layers\n\n        # from small to big.\n        self.modules12 = nn.CellList([FusionPyramidLayer(args, i == 0) for i in range(self.layers)])\n        self.modules21 = nn.CellList([FusionPyramidLayer(args, i == 0) for i in range(self.layers)])\n\n        self.fusion = CatConv(self.nf, 2, self.nf, 1, 1)\n\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    @staticmethod\n    def fuse_features(modules, f1, f2):\n        offset, feature = None, None\n        for idx, sources in enumerate(zip(f1, f2)):\n            offset, feature = modules[idx](sources, offset, feature)\n        return feature\n\n    def construct(self, f1, f2):\n        feature1 = self.fuse_features(self.modules12, f1, f2)\n        feature2 = self.fuse_features(self.modules21, f2, f1)\n        fused_feature = self.fusion(feature1, feature2)\n        return fused_feature, None  # TODO detect scene change here", "\n\nclass cycmunet_mutual_cycle(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_mutual_cycle, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.cycle_count = self.args.cycle_count\n        self.all_frames = self.args.all_frames\n\n        self.merge = nn.CellList([CatConv(64, i + 1, 64, 1, 1, 0) for i in range(self.cycle_count)])\n        self.merge1 = nn.CellList([CatConv(64, i + 1, 64, 1, 1, 0) for i in range(self.cycle_count)])\n\n        self.down = nn.CellList([Down_projection(args) for _ in range(self.cycle_count)])\n        self.up = nn.CellList([Up_projection(args) for _ in range(self.cycle_count + 1)])\n\n        self.conv = CatConv(self.nf, 2 * self.cycle_count + 1, self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def construct(self, lf0, lf1, lf2):\n        l_out, h_out = [(lf0, lf1, lf2)], []\n        l_feats = []\n        for j1 in range(self.cycle_count):\n            l_feats = tuple(self.lrelu(self.merge[j1](*frame_outs)) for frame_outs in transpose(l_out))\n            h_feat = self.up[j1](*l_feats)\n            h_out.append(h_feat)\n            h_feats = tuple(self.lrelu(self.merge1[j1](*frame_outs)) for frame_outs in transpose(h_out))\n            l_feat = self.down[j1](*h_feats)\n            l_out.append(l_feat)\n\n        lf_out, hf_out = [l_out[-1]], []\n        for j2 in range(self.cycle_count):\n            l_feats = tuple(self.lrelu(self.merge[j2](*frame_outs)) for frame_outs in transpose(lf_out))\n            h_feat = self.up[j2](*l_feats)\n            hf_out.append(h_feat)\n            l_feat = self.down[j2](*h_feat)\n            lf_out.append(l_feat)\n        hf_out.append(self.up[self.cycle_count](*l_feats))\n\n        if self.all_frames:\n            h_outs = transpose(h_out + hf_out)  # packed 3 frames\n            _, l1_out, _ = transpose(l_out + lf_out[1:])\n\n            h_outs = list(self.lrelu(self.conv(*h_frame)) for h_frame in h_outs)\n            l1_out = self.lrelu(self.conv(*l1_out))\n            return tuple(h_outs + [l1_out])\n        else:\n            h1_out, h2_out, _ = transpose(h_out + hf_out)\n            h1_out = self.lrelu(self.conv(*h1_out))\n            h2_out = self.lrelu(self.conv(*h2_out))\n\n            return h1_out, h2_out", "\n\nclass cycmunet_feature_recon(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_feature_recon, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.back_RBs = 40\n\n        self.recon_trunk = nn.SequentialCell(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.back_RBs)))\n\n    def construct(self, x):\n        out = self.recon_trunk(x)\n        return out", "\n\nclass cycmunet_tail(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_tail, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        if self.args.format == 'rgb':\n            self.conv_last2 = Conv2d(self.nf, 3, 3, 1, 1)\n            self._construct = self.construct_rgb\n        elif self.args.format == 'yuv444':\n            self.conv_last2 = Conv2d(self.nf, 3, 3, 1, 1)\n            self._construct = self.construct_yuv444\n        elif self.args.format == 'yuv422':\n            self.conv_last_y = Conv2d(self.nf, 1, 3, 1, 1)\n            self.conv_last_uv = Conv2d(self.nf, 2, (1, 3), (1, 2), (0, 0, 1, 1))\n            self._construct = self.construct_yuv42x\n        elif self.args.format == 'yuv420':\n            self.conv_last_y = Conv2d(self.nf, 1, 3, 1, 1)\n            self.conv_last_uv = Conv2d(self.nf, 2, 3, 2, 1)\n            self._construct = self.construct_yuv42x\n        else:\n            raise ValueError(f'unknown input pixel format: {self.args.format}')\n\n    def construct_rgb(self, x):\n        out = self.conv_last2(x)\n        return out,\n\n    def construct_yuv444(self, x):\n        out = self.conv_last2(x)\n        return out[:, :1, ...], out[:, 1:, ...]\n\n    def construct_yuv42x(self, x):\n        y = self.conv_last_y(x)\n        uv = self.conv_last_uv(x)\n        return y, uv\n\n    def construct(self, *args):\n        return self._construct(*args)", "\n\nclass CycMuNet(nn.Cell):\n    def __init__(self, args):\n        super(CycMuNet, self).__init__()\n        self.args = args\n        self.factor = (1, 1, self.args.upscale_factor, self.args.upscale_factor)\n        self.upsample_mode = 'bilinear'\n        self.batch_mode = self.args.batch_mode\n        self.stop_at_conf = self.args.stop_at_conf\n        self.all_frames = self.args.all_frames\n        self.layers = self.args.layers\n\n        self.head = cycmunet_head(args)\n        self.fe = cycmunet_feature_extract(args)\n        self.ff = cycmunet_feature_fusion(args)\n        self.mu = cycmunet_mutual_cycle(args)\n        self.fr = cycmunet_feature_recon(args)\n        self.tail = cycmunet_tail(args)\n\n    def merge_hf(self, lf, hf):\n        return ops.interpolate(lf, scales=float_tuple(self.factor), mode='bilinear',\n                               coordinate_transformation_mode=coordinate_transformation_mode) + hf\n\n    def mu_fr(self, *lf):\n        mu_out = self.mu(*lf)\n        if self.all_frames:\n            # *hf, lf1 = mu_out\n            hf, lf1 = mu_out[:-1], mu_out[-1]\n            hf = tuple(self.merge_hf(l, self.fr(h)) for l, h in zip(lf, hf))\n            outs = list(hf) + [lf1]\n            outs = tuple(self.tail(i) for i in outs)\n        else:\n            outs = tuple(self.tail(self.merge_hf(l, self.fr(h))) for l, h in zip(lf, mu_out))\n        return outs\n\n    def construct_batch(self, f0, f2):\n        lf0s = self.fe(self.head(f0))\n        lf2s = self.fe(self.head(f2))\n        lf0, lf2 = lf0s[self.layers - 1], lf2s[self.layers - 1]\n        lf1, _ = self.ff(lf0s, lf2s)\n        if self.stop_at_conf:  # TODO detect frame difference and exit if too big\n            return\n        return self.mu_fr(lf0, lf1, lf2)\n\n    def construct_sequence(self, x_or_yuv):\n        x = self.head(x_or_yuv)\n        ls = self.fe(x)\n        lf0, lf2 = ls[2][:-1], ls[2][1:]\n        lf1, _ = self.ff([layer[:-1] for layer in ls], [layer[1:] for layer in ls])\n        return self.mu_fr(lf0, lf1, lf2)\n\n    def construct(self, f0, f2=None):\n        if self.batch_mode == 'batch':\n            outs = self.construct_batch(f0, f2)\n        elif self.batch_mode == 'sequence':\n            outs = self.construct_sequence(f0)\n        else:\n            raise ValueError(f\"Invalid batch_mode: {self.args.batch_mode}\")\n        return outs", ""]}
{"filename": "mindspore/model/model_sep.py", "chunked_list": ["from collections.abc import Iterable\n\nimport mindspore as ms\nfrom mindspore import nn, ops\n\nfrom .deform_conv import DCN_sep_compat\n\n# half_pixel not supported on cpu now. use align_corners for test temporarily.\ncoordinate_transformation_mode = 'half_pixel'\n# coordinate_transformation_mode = 'align_corners'", "coordinate_transformation_mode = 'half_pixel'\n# coordinate_transformation_mode = 'align_corners'\n\n\ndef Conv2d(in_channels,\n           out_channels,\n           kernel_size,\n           stride=1,\n           padding=0,\n           dilation=1,\n           has_bias=True):\n    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                     pad_mode='pad', padding=padding, dilation=dilation, has_bias=has_bias)", "\n\ndef Conv2dTranspose(in_channels,\n                    out_channels,\n                    kernel_size,\n                    stride=1,\n                    dilation=1):\n    return nn.Conv2dTranspose(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                              stride=stride, pad_mode='same', dilation=dilation, has_bias=True)\n", "\n\ndef float_tuple(*tuple_):\n    tuple_ = tuple_[0] if isinstance(tuple_[0], Iterable) else tuple_\n    return tuple(float(i) for i in tuple_)\n\n\ndef transpose(mat):\n    count = len(mat[0])\n    ret = [[] for _ in range(count)]\n    for i in mat:\n        for j in range(count):\n            ret[j].append(i[j])\n    return ret", "\n\ndef cat_simp(elements, axis=1):\n    if len(elements) == 1:\n        return elements[0]\n    else:\n        return ms.ops.concat(elements, axis=axis)\n\n\nclass CatConv(nn.Cell):\n    def __init__(self, in_channels, in_count, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n        super().__init__()\n        self.convs = nn.CellList(\n            [Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, i == 0) for i in\n             range(in_count)])\n\n    def construct(self, *inputs):\n        output = None\n        for idx, inp in enumerate(inputs):\n            if output is None:\n                output = self.convs[idx](inp)\n            else:\n                output += self.convs[idx](inp)\n        return output", "\nclass CatConv(nn.Cell):\n    def __init__(self, in_channels, in_count, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n        super().__init__()\n        self.convs = nn.CellList(\n            [Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, i == 0) for i in\n             range(in_count)])\n\n    def construct(self, *inputs):\n        output = None\n        for idx, inp in enumerate(inputs):\n            if output is None:\n                output = self.convs[idx](inp)\n            else:\n                output += self.convs[idx](inp)\n        return output", "\n\nclass Pro_align(nn.Cell):\n    def __init__(self, args):\n        super(Pro_align, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n\n        self.conv1x1 = CatConv(self.nf, 3, self.nf, 1, 1, 0)\n        self.conv3x3 = Conv2d(self.nf, self.nf, 3, 1, 1)\n        self.conv1_3x3 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def construct(self, l1, l2, l3):\n        r1 = self.lrelu(self.conv3x3(l1))\n        r2 = self.lrelu(self.conv3x3(l2))\n        r3 = self.lrelu(self.conv3x3(l3))\n        fuse = self.lrelu(self.conv1x1(r1, r2, r3))\n        r1 = self.lrelu(self.conv1_3x3(r1, fuse))\n        r2 = self.lrelu(self.conv1_3x3(r2, fuse))\n        r3 = self.lrelu(self.conv1_3x3(r3, fuse))\n        return l1 + r1, l2 + r2, l3 + r3", "\n\nclass SR(nn.Cell):\n    def __init__(self, args):\n        super(SR, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.factor = (1, 1, self.args.upscale_factor, self.args.upscale_factor)\n        self.Pro_align = Pro_align(args)\n        self.conv1x1 = Conv2d(self.nf, self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def upsample(self, x):\n        x = ops.interpolate(x, scales=float_tuple(self.factor), mode='bilinear',\n                            coordinate_transformation_mode=coordinate_transformation_mode)\n        return self.lrelu(self.conv1x1(x))\n\n    def construct(self, l1, l2, l3):\n        l1, l2, l3 = self.Pro_align(l1, l2, l3)\n        return tuple(self.upsample(i) for i in (l1, l2, l3))", "\n\nclass DR(nn.Cell):\n    def __init__(self, args):\n        super(DR, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.factor = (1, 1, 1 / self.args.upscale_factor, 1 / self.args.upscale_factor)\n        self.Pro_align = Pro_align(args)\n        self.conv = Conv2d(self.nf, self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def downsample(self, x):\n        x = ops.interpolate(x, scales=float_tuple(self.factor), mode='bilinear',\n                            coordinate_transformation_mode=coordinate_transformation_mode)\n        return self.lrelu(self.conv(x))\n\n    def construct(self, l1, l2, l3):\n        l1 = self.downsample(l1)\n        l2 = self.downsample(l2)\n        l3 = self.downsample(l3)\n        return self.Pro_align(l1, l2, l3)", "\n\nclass Up_projection(nn.Cell):\n    def __init__(self, args):\n        super(Up_projection, self).__init__()\n        self.args = args\n        self.SR = SR(args)\n        self.DR = DR(args)\n        self.SR1 = SR(args)\n\n    def construct(self, l1, l2, l3):\n        h1, h2, h3 = self.SR(l1, l2, l3)\n        d1, d2, d3 = self.DR(h1, h2, h3)\n        r1, r2, r3 = d1 - l1, d2 - l2, d3 - l3\n        s1, s2, s3 = self.SR1(r1, r2, r3)\n        return h1 + s1, h2 + s3, h3 + s3", "\n\nclass Down_projection(nn.Cell):\n    def __init__(self, args):\n        super(Down_projection, self).__init__()\n        self.args = args\n        self.SR = SR(args)\n        self.DR = DR(args)\n        self.DR1 = DR(args)\n\n    def construct(self, h1, h2, h3):\n        l1, l2, l3 = self.DR(h1, h2, h3)\n        s1, s2, s3 = self.SR(l1, l2, l3)\n        r1, r2, r3 = s1 - h1, s2 - h2, s3 - h3\n        d1, d2, d3 = self.DR1(r1, r2, r3)\n        return l1 + d1, l2 + d2, l3 + d3", "\n\nclass FusionPyramidLayer(nn.Cell):\n    def __init__(self, args, first_layer: bool):\n        super(FusionPyramidLayer, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n\n        self.offset_conv1 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n        self.offset_conv3 = Conv2d(self.nf, self.nf, 3, 1, 1)\n        self.dcnpack = DCN_sep_compat(self.nf, self.nf, self.nf, 3, stride=1, padding=1, dilation=1,\n                                      deformable_groups=self.groups)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n        if not first_layer:\n            self.offset_conv2 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n            self.fea_conv = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\n    def construct(self, current_sources, last_offset, last_feature):\n        offset = self.lrelu(self.offset_conv1(*current_sources))\n        if last_offset is not None:\n            last_offset = ops.interpolate(last_offset, scales=float_tuple(1, 1, 2, 2), mode='bilinear',\n                                          coordinate_transformation_mode=coordinate_transformation_mode)\n            offset = self.lrelu(self.offset_conv2(offset, last_offset * 2))\n        offset = self.lrelu(self.offset_conv3(offset))\n        feature = self.dcnpack(current_sources[0], offset)\n        if last_feature is not None:\n            last_feature = ops.interpolate(last_feature, scales=float_tuple(1, 1, 2, 2), mode='bilinear',\n                                           coordinate_transformation_mode=coordinate_transformation_mode)\n            feature = self.fea_conv(feature, last_feature)\n        feature = self.lrelu(feature)\n        return offset, feature", "\n\nclass ResidualBlock_noBN(nn.Cell):\n    '''Residual block w/o BN\n    ---Conv-ReLU-Conv-+-\n     |________________|\n    '''\n\n    def __init__(self, nf=64):\n        super(ResidualBlock_noBN, self).__init__()\n        self.conv1 = Conv2d(nf, nf, 3, 1, 1)\n        self.conv2 = Conv2d(nf, nf, 3, 1, 1)\n        self.relu = ops.ReLU()\n\n        # TODO: initialization\n        # initialize_weights([self.conv1, self.conv2], 0.1)\n\n    def construct(self, x):\n        identity = x\n        out = self.relu(self.conv1(x))\n        out = self.conv2(out)\n        return identity + out", "\n\nclass cycmunet_head(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_head, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n        if self.args.format == 'rgb':\n            self.conv_first = Conv2d(3, self.nf, 3, 1, 1)\n            self._construct = self.construct_rgb\n        elif self.args.format == 'yuv444':\n            self.conv_first = Conv2d(3, self.nf, 3, 1, 1)\n            self._construct = self.construct_yuv444\n        elif self.args.format == 'yuv422':\n            self.conv_first_y = Conv2d(1, self.nf, 3, 1, 1)\n            self.conv_up = Conv2dTranspose(2, self.nf, (1, 3), (1, 2))\n            self._construct = self.construct_yuv42x\n        elif self.args.format == 'yuv420':\n            self.conv_first_y = Conv2d(1, self.nf, 3, 1, 1)\n            self.conv_up = Conv2dTranspose(2, self.nf, 3, 2)\n            self._construct = self.construct_yuv42x\n        else:\n            raise ValueError(f'unknown input pixel format: {self.args.format}')\n\n    def construct_rgb(self, x):\n        x = self.lrelu(self.conv_first(x))\n        return x\n\n    def construct_yuv444(self, yuv):\n        x = ms.ops.concat(yuv, axis=1)\n        x = self.lrelu(self.conv_first(x))\n        return x\n\n    def construct_yuv42x(self, yuv):\n        y, uv = yuv\n        y = self.conv_first_y(y)\n        uv = self.conv_up(uv)\n        x = self.lrelu(y + uv)\n        return x\n\n    def construct(self, *args):\n        return self._construct(*args)", "\n\nclass cycmunet_feature_extract(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_feature_extract, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n        self.layers = self.args.layers\n        self.front_RBs = 5\n\n        self.feature_extraction = nn.SequentialCell(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.front_RBs)))\n        self.fea_conv1s = nn.CellList([Conv2d(self.nf, self.nf, 3, 2, 1) for _ in range(self.layers - 1)])\n        self.fea_conv2s = nn.CellList([Conv2d(self.nf, self.nf, 3, 1, 1) for _ in range(self.layers - 1)])\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def construct(self, x):\n        features = [self.feature_extraction(x)]\n        for i in range(self.layers - 1):\n            feature = features[-1]\n            feature = self.lrelu(self.fea_conv1s[i](feature))\n            feature = self.lrelu(self.fea_conv2s[i](feature))\n            features.append(feature)\n        return tuple(features[::-1])  # lowest dimension layer at first", "\n\nclass cycmunet_feature_fusion(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_feature_fusion, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n        self.layers = self.args.layers\n\n        # from small to big.\n        self.modules12 = nn.CellList([FusionPyramidLayer(args, i == 0) for i in range(self.layers)])\n        self.modules21 = nn.CellList([FusionPyramidLayer(args, i == 0) for i in range(self.layers)])\n\n        self.fusion = CatConv(self.nf, 2, self.nf, 1, 1)\n\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    @staticmethod\n    def fuse_features(modules, f1, f2):\n        offset, feature = None, None\n        for idx, sources in enumerate(zip(f1, f2)):\n            offset, feature = modules[idx](sources, offset, feature)\n        return feature\n\n    def construct(self, f1, f2):\n        feature1 = self.fuse_features(self.modules12, f1, f2)\n        feature2 = self.fuse_features(self.modules21, f2, f1)\n        fused_feature = self.fusion(feature1, feature2)\n        return fused_feature, None  # TODO detect scene change here", "\n\nclass cycmunet_mutual_cycle(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_mutual_cycle, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.cycle_count = self.args.cycle_count\n        self.all_frames = self.args.all_frames\n\n        self.merge = nn.CellList([CatConv(64, i + 1, 64, 1, 1, 0) for i in range(self.cycle_count)])\n        self.merge1 = nn.CellList([CatConv(64, i + 1, 64, 1, 1, 0) for i in range(self.cycle_count)])\n\n        self.down = nn.CellList([Down_projection(args) for _ in range(self.cycle_count)])\n        self.up = nn.CellList([Up_projection(args) for _ in range(self.cycle_count + 1)])\n\n        self.conv = CatConv(self.nf, 2 * self.cycle_count + 1, self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\n    def construct(self, lf0, lf1, lf2):\n        l_out, h_out = [(lf0, lf1, lf2)], []\n        l_feats = []\n        for j1 in range(self.cycle_count):\n            l_feats = tuple(self.lrelu(self.merge[j1](*frame_outs)) for frame_outs in transpose(l_out))\n            h_feat = self.up[j1](*l_feats)\n            h_out.append(h_feat)\n            h_feats = tuple(self.lrelu(self.merge1[j1](*frame_outs)) for frame_outs in transpose(h_out))\n            l_feat = self.down[j1](*h_feats)\n            l_out.append(l_feat)\n\n        lf_out, hf_out = [l_out[-1]], []\n        for j2 in range(self.cycle_count):\n            l_feats = tuple(self.lrelu(self.merge[j2](*frame_outs)) for frame_outs in transpose(lf_out))\n            h_feat = self.up[j2](*l_feats)\n            hf_out.append(h_feat)\n            l_feat = self.down[j2](*h_feat)\n            lf_out.append(l_feat)\n        hf_out.append(self.up[self.cycle_count](*l_feats))\n\n        if self.all_frames:\n            h_outs = transpose(h_out + hf_out)  # packed 3 frames\n            _, l1_out, _ = transpose(l_out + lf_out[1:])\n\n            h_outs = list(self.lrelu(self.conv(*h_frame)) for h_frame in h_outs)\n            l1_out = self.lrelu(self.conv(*l1_out))\n            return tuple(h_outs + [l1_out])\n        else:\n            h1_out, h2_out, _ = transpose(h_out + hf_out)\n            h1_out = self.lrelu(self.conv(*h1_out))\n            h2_out = self.lrelu(self.conv(*h2_out))\n\n            return h1_out, h2_out", "\n\nclass cycmunet_feature_recon(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_feature_recon, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.back_RBs = 40\n\n        self.recon_trunk = nn.SequentialCell(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.back_RBs)))\n\n    def construct(self, x):\n        out = self.recon_trunk(x)\n        return out", "\n\nclass cycmunet_tail(nn.Cell):\n    def __init__(self, args):\n        super(cycmunet_tail, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        if self.args.format == 'rgb':\n            self.conv_last2 = Conv2d(self.nf, 3, 3, 1, 1)\n            self._construct = self.construct_rgb\n        elif self.args.format == 'yuv444':\n            self.conv_last2 = Conv2d(self.nf, 3, 3, 1, 1)\n            self._construct = self.construct_yuv444\n        elif self.args.format == 'yuv422':\n            self.conv_last_y = Conv2d(self.nf, 1, 3, 1, 1)\n            self.conv_last_uv = Conv2d(self.nf, 2, (1, 3), (1, 2), (0, 0, 1, 1))\n            self._construct = self.construct_yuv42x\n        elif self.args.format == 'yuv420':\n            self.conv_last_y = Conv2d(self.nf, 1, 3, 1, 1)\n            self.conv_last_uv = Conv2d(self.nf, 2, 3, 2, 1)\n            self._construct = self.construct_yuv42x\n        else:\n            raise ValueError(f'unknown input pixel format: {self.args.format}')\n\n    def construct_rgb(self, x):\n        out = self.conv_last2(x)\n        return out,\n\n    def construct_yuv444(self, x):\n        out = self.conv_last2(x)\n        return out[:, :1, ...], out[:, 1:, ...]\n\n    def construct_yuv42x(self, x):\n        y = self.conv_last_y(x)\n        uv = self.conv_last_uv(x)\n        return y, uv\n\n    def construct(self, *args):\n        return self._construct(*args)", "\n\nclass CycMuNet(nn.Cell):\n    def __init__(self, args):\n        super(CycMuNet, self).__init__()\n        self.args = args\n        self.factor = (1, 1, self.args.upscale_factor, self.args.upscale_factor)\n        self.upsample_mode = 'bilinear'\n        self.batch_mode = self.args.batch_mode\n        self.stop_at_conf = self.args.stop_at_conf\n        self.all_frames = self.args.all_frames\n\n        self.head = cycmunet_head(args)\n        self.fe = cycmunet_feature_extract(args)\n        self.ff = cycmunet_feature_fusion(args)\n        self.mu = cycmunet_mutual_cycle(args)\n        self.fr = cycmunet_feature_recon(args)\n        self.tail = cycmunet_tail(args)\n\n    def merge_hf(self, lf, hf):\n        return ops.interpolate(lf, scales=float_tuple(self.factor), mode='bilinear',\n                               coordinate_transformation_mode=coordinate_transformation_mode) + hf\n\n    def mu_fr(self, *lf):\n        mu_out = self.mu(*lf)\n        if self.all_frames:\n            # *hf, lf1 = mu_out\n            hf, lf1 = mu_out[:-1], mu_out[-1]\n            hf = tuple(self.merge_hf(l, self.fr(h)) for l, h in zip(lf, hf))\n            outs = list(hf) + [lf1]\n            outs = tuple(self.tail(i) for i in outs)\n        else:\n            outs = tuple(self.tail(self.merge_hf(l, self.fr(h))) for l, h in zip(lf, mu_out))\n        return outs\n\n    def construct_batch(self, f0, f2):\n        lf0s = self.fe(self.head(f0))\n        lf2s = self.fe(self.head(f2))\n        lf0, lf2 = lf0s[self.layers - 1], lf2s[self.layers - 1]\n        lf1, _ = self.ff(lf0s, lf2s)\n        if self.stop_at_conf:  # TODO detect frame difference and exit if too big\n            return\n        return self.mu_fr(lf0, lf1, lf2)\n\n    def construct_sequence(self, x_or_yuv):\n        x = self.head(x_or_yuv)\n        ls = self.fe(x)\n        lf0, lf2 = ls[2][:-1], ls[2][1:]\n        lf1, _ = self.ff([layer[:-1] for layer in ls], [layer[1:] for layer in ls])\n        return self.mu_fr(lf0, lf1, lf2)\n\n    def construct(self, f0, f2=None):\n        if self.batch_mode == 'batch':\n            outs = self.construct_batch(f0, f2)\n        elif self.batch_mode == 'sequence':\n            outs = self.construct_sequence(f0)\n        else:\n            raise ValueError(f\"Invalid batch_mode: {self.args.batch_mode}\")\n        return outs", ""]}
{"filename": "mindspore/model/train.py", "chunked_list": ["from mindspore import nn\n\n\nclass TrainModel(nn.Cell):\n    def __init__(self, network, loss):\n        super().__init__()\n        self.net = network\n        self.loss = loss\n\n    def loss_frame(self, expected, actual):\n        return self.loss(expected[0], actual[0]) + self.loss(expected[1], actual[1])\n\n    def construct(self, *data):\n        inputs = [data[6:8], data[10:12]]\n        expected = [data[0:2], data[2:4], data[4:6], data[8:10]]\n        actual = self.net(*inputs)\n        loss = self.loss_frame(expected[0], actual[0]) * 0.5 + \\\n               self.loss_frame(expected[1], actual[1]) + \\\n               self.loss_frame(expected[2], actual[2]) * 0.5 + \\\n               self.loss_frame(expected[3], actual[3])\n        return loss", ""]}
{"filename": "mindspore/model/__init__.py", "chunked_list": ["from .model import CycMuNet\nfrom .model_sep import CycMuNet as CycMuNetSep\nfrom .train import TrainModel\n"]}
{"filename": "mindspore/model/deform_conv.py", "chunked_list": ["import math\n\nimport mindspore as ms\nfrom mindspore import nn, ops\nfrom mindspore.common import initializer as init\nfrom mindspore._checkparam import twice\n\n\ndef _Conv2d(in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1):\n    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                     pad_mode='pad', padding=padding, dilation=dilation, has_bias=True)", "def _Conv2d(in_channels,\n            out_channels,\n            kernel_size,\n            stride=1,\n            padding=0,\n            dilation=1):\n    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n                     pad_mode='pad', padding=padding, dilation=dilation, has_bias=True)\n\n\nclass DCN_sep(nn.Cell):\n    def __init__(self,\n                 in_channels,\n                 in_channels_features,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True,\n                 mask=True):\n        super(DCN_sep, self).__init__()\n\n        kernel_size_ = twice(kernel_size)\n\n        self.dcn_weight = ms.Parameter(\n            ms.Tensor(\n                shape=(out_channels, in_channels // groups, kernel_size_[0], kernel_size_[1]),\n                dtype=ms.float32,\n                init=init.HeUniform(negative_slope=math.sqrt(5))\n            )\n        )\n\n        fan_in = in_channels // groups * kernel_size_[0] * kernel_size_[1]\n        bound = 1 / math.sqrt(fan_in)\n\n        self.dcn_bias = ms.Parameter(\n            ms.Tensor(\n                shape=(out_channels,),\n                dtype=ms.float32,\n                init=init.Uniform(bound)\n            )\n        ) if bias else None\n\n        self.dcn_kwargs = {\n            'kernel_size': kernel_size_,\n            'strides': (1, 1, *twice(stride)),\n            'padding': (padding,) * 4 if isinstance(padding, int) else padding,\n            'dilations': (1, 1, *twice(dilation)),\n            'groups': groups,\n            'deformable_groups': deformable_groups,\n            'modulated': mask\n        }\n\n        offset_channels = deformable_groups * kernel_size_[0] * kernel_size_[1]\n\n        self.conv_offset = _Conv2d(in_channels_features, offset_channels * 2, kernel_size=kernel_size,\n                                   stride=stride, padding=padding, dilation=dilation)\n        if mask:\n            self.conv_mask = _Conv2d(in_channels_features, offset_channels, kernel_size=kernel_size,\n                                     stride=stride, padding=padding, dilation=dilation)\n        else:\n            raise NotImplementedError()\n        self.relu = nn.ReLU()\n\n    def construct(self, input, feature):\n        offset = self.conv_offset(feature)\n        mask = ops.sigmoid(self.conv_mask(feature))\n        offsets = ops.concat([offset, mask], axis=1)\n\n        return ops.deformable_conv2d(input, self.dcn_weight, offsets, bias=self.dcn_bias, **self.dcn_kwargs)", "\n\nclass DCN_sep(nn.Cell):\n    def __init__(self,\n                 in_channels,\n                 in_channels_features,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True,\n                 mask=True):\n        super(DCN_sep, self).__init__()\n\n        kernel_size_ = twice(kernel_size)\n\n        self.dcn_weight = ms.Parameter(\n            ms.Tensor(\n                shape=(out_channels, in_channels // groups, kernel_size_[0], kernel_size_[1]),\n                dtype=ms.float32,\n                init=init.HeUniform(negative_slope=math.sqrt(5))\n            )\n        )\n\n        fan_in = in_channels // groups * kernel_size_[0] * kernel_size_[1]\n        bound = 1 / math.sqrt(fan_in)\n\n        self.dcn_bias = ms.Parameter(\n            ms.Tensor(\n                shape=(out_channels,),\n                dtype=ms.float32,\n                init=init.Uniform(bound)\n            )\n        ) if bias else None\n\n        self.dcn_kwargs = {\n            'kernel_size': kernel_size_,\n            'strides': (1, 1, *twice(stride)),\n            'padding': (padding,) * 4 if isinstance(padding, int) else padding,\n            'dilations': (1, 1, *twice(dilation)),\n            'groups': groups,\n            'deformable_groups': deformable_groups,\n            'modulated': mask\n        }\n\n        offset_channels = deformable_groups * kernel_size_[0] * kernel_size_[1]\n\n        self.conv_offset = _Conv2d(in_channels_features, offset_channels * 2, kernel_size=kernel_size,\n                                   stride=stride, padding=padding, dilation=dilation)\n        if mask:\n            self.conv_mask = _Conv2d(in_channels_features, offset_channels, kernel_size=kernel_size,\n                                     stride=stride, padding=padding, dilation=dilation)\n        else:\n            raise NotImplementedError()\n        self.relu = nn.ReLU()\n\n    def construct(self, input, feature):\n        offset = self.conv_offset(feature)\n        mask = ops.sigmoid(self.conv_mask(feature))\n        offsets = ops.concat([offset, mask], axis=1)\n\n        return ops.deformable_conv2d(input, self.dcn_weight, offsets, bias=self.dcn_bias, **self.dcn_kwargs)", "\n\n# Same as DCN_sep but compatible with Ascend.\n# Can be removed once deformable_groups can be values other than 1 on Ascend.\nclass DCN_sep_compat(nn.Cell):\n    def __init__(self,\n                 in_channels,\n                 in_channels_features,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,\n                 bias=True,\n                 mask=True):\n        super(DCN_sep_compat, self).__init__()\n        if deformable_groups == 1:\n            raise ValueError(\"Use DCN_sep\")\n\n        if groups != 1:\n            raise NotImplementedError()\n\n        self.separated = groups != 1\n\n        kernel_size_ = twice(kernel_size)\n\n        self.deformable_groups = deformable_groups\n        self.dcn_weight = ms.Parameter(\n            ms.Tensor(\n                shape=(out_channels, in_channels // groups, kernel_size_[0], kernel_size_[1]),\n                dtype=ms.float32,\n                init=init.HeUniform(negative_slope=math.sqrt(5))\n            )\n        )\n\n        fan_in = in_channels // groups * kernel_size_[0] * kernel_size_[1]\n        bound = 1 / math.sqrt(fan_in)\n\n        self.dcn_bias = ms.Parameter(\n            ms.Tensor(\n                shape=(out_channels,),\n                dtype=ms.float32,\n                init=init.Uniform(bound)\n            )\n        ) if bias else None\n\n        self.dcn_kwargs = {\n            'kernel_size': kernel_size_,\n            'strides': (1, 1, *twice(stride)),\n            'padding': (padding,) * 4 if isinstance(padding, int) else padding,\n            'dilations': (1, 1, *twice(dilation)),\n            'groups': 1,\n            'deformable_groups': 1,\n            'modulated': mask is not None\n        }\n\n        offset_channels = deformable_groups * kernel_size_[0] * kernel_size_[1]\n\n        self.conv_offset = _Conv2d(in_channels_features, offset_channels * 2, kernel_size=kernel_size,\n                                   stride=stride, padding=padding, dilation=dilation)\n        if mask:\n            self.conv_mask = _Conv2d(in_channels_features, offset_channels, kernel_size=kernel_size,\n                                     stride=stride, padding=padding, dilation=dilation)\n        else:\n            raise NotImplementedError()\n        self.relu = nn.ReLU()\n\n    def construct(self, input, feature):\n        offset = self.conv_offset(feature)\n        mask = ops.sigmoid(self.conv_mask(feature))\n        offset_y, offset_x = ops.split(offset, axis=1, output_num=2)\n\n        inputs = ops.split(input, axis=1, output_num=self.deformable_groups)\n        dcn_weights = ops.split(self.dcn_weight, axis=1, output_num=self.deformable_groups)\n        offset_ys = ops.split(offset_y, axis=1, output_num=self.deformable_groups)\n        offset_xs = ops.split(offset_y, axis=1, output_num=self.deformable_groups)\n        masks = ops.split(mask, axis=1, output_num=self.deformable_groups)\n\n        output = None\n        for i in range(self.deformable_groups):\n            offsets = ops.concat([offset_ys[i], offset_xs[i], masks[i]], axis=1)\n            if output is None:\n                output = ops.deformable_conv2d(inputs[i], dcn_weights[i], offsets, bias=self.dcn_bias,\n                                               **self.dcn_kwargs)\n            else:\n                output += ops.deformable_conv2d(inputs[i], dcn_weights[i], offsets, bias=None, **self.dcn_kwargs)\n\n        return output"]}
{"filename": "mindspore/dataset/triplet.py", "chunked_list": ["import pathlib\nimport random\nfrom typing import List\n\nimport cv2\nimport mindspore as ms\nfrom mindspore import dataset\nfrom mindspore.dataset import vision, transforms\nimport numpy as np\nfrom PIL import Image, ImageFilter", "import numpy as np\nfrom PIL import Image, ImageFilter\n\n\nclass ImageTripletGenerator:\n    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n        self.dataset_base = pathlib.Path(index_file).parent\n        self.triplets = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n                         if i if not i.startswith('#')]\n        self.patch_size = patch_size\n        self.scale_factor = scale_factor\n        self.augment = augment\n        self.rand = random.Random(seed)\n\n    def _load_triplet(self, path):\n        path = self.dataset_base / \"sequences\" / path\n        images = [Image.open(path / f\"im{i + 1}.png\") for i in range(3)]\n        if not (images[0].size == images[1].size and images[0].size == images[2].size):\n            raise ValueError(\"triplet has different dimensions\")\n        return images\n\n    def _prepare_images(self, images: List[Image.Image]):\n        w, h = images[0].size\n        f = self.scale_factor\n        s = self.patch_size * f\n        dh, dw = self.rand.randrange(0, h - s, 2) * f, self.rand.randrange(0, w - s, 2) * f\n        images = [i.crop((dw, dh, dw + s, dh + s)) for i in images]\n        return images\n\n    trans_groups = {\n        'none': [None],\n        'rotate': [None, Image.ROTATE_90, Image.ROTATE_180, Image.ROTATE_270],\n        'mirror': [None, Image.FLIP_LEFT_RIGHT],\n        'flip': [None, Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM, Image.ROTATE_180],\n        'all': [None] + [e.value for e in Image.Transpose],\n    }\n\n    trans_names = [e.name for e in Image.Transpose]\n\n    def _augment_images(self, images: List[Image.Image], trans_mode='all'):\n        trans_action = 'none'\n        trans_op = self.rand.choice(self.trans_groups[trans_mode])\n        if trans_op is not None:\n            images = [i.transpose(trans_op) for i in images]\n            trans_action = self.trans_names[trans_op]\n        return images, trans_action\n\n    scale_filters = [Image.BILINEAR, Image.BICUBIC, Image.LANCZOS]\n\n    def _scale_images(self, images: List[Image.Image]):\n        f = self.scale_factor\n        return [i.resize((i.width // f, i.height // f), self.rand.choice(self.scale_filters)) for i in images]\n\n    def _degrade_images(self, images: List[Image.Image]):\n        degrade_action = None\n        decision = self.rand.randrange(4)\n        if decision == 1:\n            degrade_action = 'box'\n            arr = [np.array(Image.blend(j, j.copy().filter(ImageFilter.BoxBlur(1)), 0.5)) for j in images]\n        elif decision == 2:\n            degrade_action = 'gaussian'\n            radius = self.rand.random() * 2\n            arr = [np.array(j.filter(ImageFilter.GaussianBlur(radius))) for j in images]\n        elif decision == 3:\n            degrade_action = 'halo'\n            radius = 1 + self.rand.random() * 2\n            modulation = 0.1 + radius * 0.3\n            contour = [np.array(i.copy().filter(ImageFilter.CONTOUR).filter(ImageFilter.GaussianBlur(radius)))\n                       for i in images]\n            arr = [cv2.addWeighted(np.array(i), 1, j, modulation, 0) for i, j in zip(images, contour)]\n        else:\n            arr = [np.array(i) for i in images]\n\n        return arr, degrade_action\n\n    def __len__(self):\n        return len(self.triplets)\n\n    def __getitem__(self, idx):\n        triplet = self._load_triplet(self.triplets[idx])\n        triplet = self._prepare_images(triplet)  # crop to requested size\n        original, _ = self._augment_images(triplet)  # flip and rotates\n        lf1 = original[1]\n        lf1 = np.array(lf1.resize((lf1.width // self.scale_factor, lf1.height // self.scale_factor), Image.LANCZOS))\n        degraded, _ = self._degrade_images(self._scale_images([original[0], original[2]]))\n        degraded.insert(1, lf1)\n        return (*original, *degraded)", "\n\ndef ImageTripletDataset(index_file, patch_size, scale_factor, augment, normalizer):\n    ds = dataset.GeneratorDataset(\n        ImageTripletGenerator(index_file, patch_size, scale_factor, augment),\n        column_names=[f'{s}{i}' for s in ('h', 'l') for i in range(3)],\n    )\n\n    mean, std = normalizer.rgb_dist()\n\n    for col in [f'{s}{i}' for s in ('h', 'l') for i in range(3)]:\n        ds = ds.map([\n            transforms.TypeCast(ms.float32),\n            vision.Rescale(1.0 / 255.0, 0),\n            vision.Normalize(mean, std, is_hwc=True),\n            vision.HWC2CHW()\n        ], col)\n\n    return ds", ""]}
{"filename": "mindspore/dataset/video.py", "chunked_list": ["import bisect\nimport collections\nimport functools\nimport itertools\nimport pathlib\nimport random\nimport time\nfrom typing import List, Tuple\n\nimport av", "\nimport av\nimport av.logging\nimport cv2\nimport numpy as np\nimport mindspore as ms\nfrom mindspore import dataset\nfrom mindspore.dataset import vision, transforms\n\n", "\n\nav.logging.set_level(av.logging.FATAL)\n\nperf_debug = False\n\n\nclass Video:\n    def __init__(self, file, kf):\n        self.container = av.open(file)\n        self.stream = self.container.streams.video[0]\n        self.stream.thread_type = \"AUTO\"\n        self.at = 0\n        self.kf = kf\n\n    def get_frames(self, pts, n=1):\n        frames = []\n        if bisect.bisect_left(self.kf, pts) != bisect.bisect_left(self.kf, self.at) or pts <= self.at:\n            self.container.seek(pts, stream=self.stream)\n        found = False\n        first = True\n        if perf_debug:\n            print(f'Seek {pts} done at {time.perf_counter()}')\n        for frame in self.container.decode(video=0):\n            if first:\n                if perf_debug:\n                    print(f'Search {pts} from {frame.pts} at {time.perf_counter()}')\n                first = False\n            if not found and frame.pts != pts:\n                continue\n            found = True\n            if perf_debug:\n                print(f'Found {frame.pts} at {time.perf_counter()}')\n            self.at = frame.pts\n            yuv = frame.to_ndarray()\n            h, w = frame.height, frame.width\n            y, uv = yuv[:h, :].reshape(1, h, w), yuv[h:, :].reshape(2, h // 2, w // 2)\n            frames.append((y, uv))\n            if len(frames) == n:\n                return frames\n        raise ValueError(\"unexpected end\")\n\n    def __del__(self):\n        self.container.close()", "\n\nvideo_info = collections.namedtuple('video_info', [\n    'org',\n    'deg',\n    'frames',\n    'pts_org',\n    'pts_deg',\n    'key_org',\n    'key_deg'", "    'key_org',\n    'key_deg'\n])\n\n\ndef flatten_once(it):\n    return itertools.chain.from_iterable(it)\n\n\nclass VideoFrameGenerator:\n    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n        self.dataset_base = pathlib.PurePath(index_file).parent\n        index_lines = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n                       if i if not i.startswith('#')]\n        files = [tuple(i.split(',')) for i in index_lines]\n        self.files = []\n        self.indexes = []\n        for org, deg, frames, pts_org, pts_deg, key_org, key_deg in files:\n            info = video_info(\n                org,\n                deg,\n                int(frames),\n                tuple(int(i) for i in pts_org.split(' ')),\n                tuple(int(i) for i in pts_deg.split(' ')),\n                tuple(int(i) for i in key_org.split(' ')),\n                tuple(int(i) for i in key_deg.split(' ')),\n            )\n            self.files.append(info)\n            self.indexes.append(info.frames)\n        self.indexes = list(itertools.accumulate(self.indexes))\n        self.patch_size = (patch_size, patch_size) if isinstance(patch_size, int) else patch_size\n        self.scale_factor = scale_factor\n        self.augment = augment\n        self.rand = random.Random(seed)\n\n    @functools.lru_cache(2)\n    def get_video(self, v_idx):\n        info = self.files[v_idx]\n        return Video(str(self.dataset_base / info.org), info.key_org), \\\n            Video(str(self.dataset_base / info.deg), info.key_deg), info.pts_org, info.pts_deg\n\n    def _augment_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n        if self.rand.random() > 0.5:\n            org = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in org]\n            deg = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in deg]\n        return org, deg\n\n    def _prepare_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n        _, h, w = deg[0][0].shape\n        sw, sh = self.patch_size\n        sh_uv, sw_uv = sh // 2, sw // 2\n        dh, dw = self.rand.randrange(0, h - sh, 2), self.rand.randrange(0, w - sw, 2)\n        dh_uv, dw_uv = dh // 2, dw // 2\n        deg = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in deg]\n        f = self.scale_factor\n        size, size_uv = (sw, sh), (sw_uv, sh_uv)\n        sh, sw, sh_uv, sw_uv = sh * f, sw * f, sh_uv * f, sw_uv * f\n        dh, dw, dh_uv, dw_uv = dh * f, dw * f, dh_uv * f, dw_uv * f\n        org = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in org]\n\n        deg1_y = cv2.resize(org[1][0][0], size, interpolation=cv2.INTER_LANCZOS4)\n        deg1_u = cv2.resize(org[1][1][0], size_uv, interpolation=cv2.INTER_LANCZOS4)\n        deg1_v = cv2.resize(org[1][1][1], size_uv, interpolation=cv2.INTER_LANCZOS4)\n        deg.insert(1, (deg1_y.reshape((1, *size[::-1])), np.stack((deg1_u, deg1_v)).reshape((2, *size_uv[::-1]))))\n        return org, deg\n\n    def __len__(self):\n        return self.indexes[-1]\n\n    def __getitem__(self, idx):\n        start = time.perf_counter()\n        v_idx = bisect.bisect_right(self.indexes, idx)\n        f_idx = idx if v_idx == 0 else idx - self.indexes[v_idx - 1]\n        org, deg, pts_org, pts_deg = self.get_video(v_idx)\n        org_frames = org.get_frames(pts_org[f_idx], 3)\n        deg_frames = deg.get_frames(pts_deg[f_idx], 3)\n        deg_frames.pop(1)\n        org_frames, deg_frames = self._prepare_frame(org_frames, deg_frames)\n        if self.augment:\n            org_frames, deg_frames = self._augment_frame(org_frames, deg_frames)\n        ret = (*flatten_once(org_frames), *flatten_once(deg_frames))\n        if perf_debug:\n            print(f'Prepared data {idx}, in {time.perf_counter() - start}')\n        return ret", "\nclass VideoFrameGenerator:\n    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n        self.dataset_base = pathlib.PurePath(index_file).parent\n        index_lines = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n                       if i if not i.startswith('#')]\n        files = [tuple(i.split(',')) for i in index_lines]\n        self.files = []\n        self.indexes = []\n        for org, deg, frames, pts_org, pts_deg, key_org, key_deg in files:\n            info = video_info(\n                org,\n                deg,\n                int(frames),\n                tuple(int(i) for i in pts_org.split(' ')),\n                tuple(int(i) for i in pts_deg.split(' ')),\n                tuple(int(i) for i in key_org.split(' ')),\n                tuple(int(i) for i in key_deg.split(' ')),\n            )\n            self.files.append(info)\n            self.indexes.append(info.frames)\n        self.indexes = list(itertools.accumulate(self.indexes))\n        self.patch_size = (patch_size, patch_size) if isinstance(patch_size, int) else patch_size\n        self.scale_factor = scale_factor\n        self.augment = augment\n        self.rand = random.Random(seed)\n\n    @functools.lru_cache(2)\n    def get_video(self, v_idx):\n        info = self.files[v_idx]\n        return Video(str(self.dataset_base / info.org), info.key_org), \\\n            Video(str(self.dataset_base / info.deg), info.key_deg), info.pts_org, info.pts_deg\n\n    def _augment_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n        if self.rand.random() > 0.5:\n            org = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in org]\n            deg = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in deg]\n        return org, deg\n\n    def _prepare_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n        _, h, w = deg[0][0].shape\n        sw, sh = self.patch_size\n        sh_uv, sw_uv = sh // 2, sw // 2\n        dh, dw = self.rand.randrange(0, h - sh, 2), self.rand.randrange(0, w - sw, 2)\n        dh_uv, dw_uv = dh // 2, dw // 2\n        deg = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in deg]\n        f = self.scale_factor\n        size, size_uv = (sw, sh), (sw_uv, sh_uv)\n        sh, sw, sh_uv, sw_uv = sh * f, sw * f, sh_uv * f, sw_uv * f\n        dh, dw, dh_uv, dw_uv = dh * f, dw * f, dh_uv * f, dw_uv * f\n        org = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in org]\n\n        deg1_y = cv2.resize(org[1][0][0], size, interpolation=cv2.INTER_LANCZOS4)\n        deg1_u = cv2.resize(org[1][1][0], size_uv, interpolation=cv2.INTER_LANCZOS4)\n        deg1_v = cv2.resize(org[1][1][1], size_uv, interpolation=cv2.INTER_LANCZOS4)\n        deg.insert(1, (deg1_y.reshape((1, *size[::-1])), np.stack((deg1_u, deg1_v)).reshape((2, *size_uv[::-1]))))\n        return org, deg\n\n    def __len__(self):\n        return self.indexes[-1]\n\n    def __getitem__(self, idx):\n        start = time.perf_counter()\n        v_idx = bisect.bisect_right(self.indexes, idx)\n        f_idx = idx if v_idx == 0 else idx - self.indexes[v_idx - 1]\n        org, deg, pts_org, pts_deg = self.get_video(v_idx)\n        org_frames = org.get_frames(pts_org[f_idx], 3)\n        deg_frames = deg.get_frames(pts_deg[f_idx], 3)\n        deg_frames.pop(1)\n        org_frames, deg_frames = self._prepare_frame(org_frames, deg_frames)\n        if self.augment:\n            org_frames, deg_frames = self._augment_frame(org_frames, deg_frames)\n        ret = (*flatten_once(org_frames), *flatten_once(deg_frames))\n        if perf_debug:\n            print(f'Prepared data {idx}, in {time.perf_counter() - start}')\n        return ret", "\n\ndef VideoFrameDataset(index_file, patch_size, scale_factor, augment, normalizer):\n    ds = dataset.GeneratorDataset(\n        VideoFrameGenerator(index_file, patch_size, scale_factor, augment),\n        column_names=[f'{s}{i}_{p}' for s in ('h', 'l') for i in range(3) for p in ('y', 'uv')],\n        shuffle=False,\n        python_multiprocessing=True,\n        num_parallel_workers=3,\n    )\n\n    mean, std = normalizer.yuv_dist()\n\n    for col in [f'{s}{i}' for s in ('h', 'l') for i in range(3)]:\n        ds = ds.map([\n            transforms.TypeCast(ms.float32),\n            vision.Rescale(1.0 / 255.0, 0),\n            vision.Normalize(mean[:1], std[:1], is_hwc=False)\n        ], col + '_y')\n        ds = ds.map([\n            transforms.TypeCast(ms.float32),\n            vision.Rescale(1.0 / 255.0, 0),\n            vision.Normalize(mean[1:], std[1:], is_hwc=False)\n        ], col + '_uv')\n\n    return ds", ""]}
{"filename": "mindspore/util/normalize.py", "chunked_list": ["import math\n\n\nclass Normalizer:\n    sqrt1_2 = 1 / math.sqrt(2)\n\n    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), kr=0.2126, kb=0.0722, depth=8):\n        self.mean = mean\n        self.std = std\n        self.krgb = (kr, 1 - kr - kb, kb)\n        self.depth = depth\n        self.uv_bias = (1 << (depth - 1)) / ((1 << depth) - 1)\n\n    @staticmethod\n    def _inv(mean, std):\n        inv_std = tuple(1 / i for i in std)\n        inv_mean = tuple(-j * i for i, j in zip(inv_std, mean))\n        return inv_mean, inv_std\n\n    def rgb_dist(self):\n        return self.mean, self.std\n\n    def _yuv_dist(self):\n        rm, gm, bm = self.mean\n        rs, gs, bs = self.std\n        kr, kg, kb = self.krgb\n\n        ym = rm * kr + gm * kg + bm * kb\n        ys = math.sqrt((rs * kr) ** 2 + (gs * kg) ** 2 + (bs * kb) ** 2)\n        um = (bm - ym) / (1 - kb) / 2 + self.uv_bias\n        us = math.sqrt(bs * bs + ys * ys) / (1 - kb) / 2\n        vm = (rm - ym) / (1 - kr) / 2 + self.uv_bias\n        vs = math.sqrt(rs * rs + ys * ys) / (1 - kr) / 2\n        return [ym, um, vm], [ys, us, vs]\n\n    def yuv_dist(self, mode='yuv420'):\n        mean, std = self._yuv_dist()\n        if mode == 'yuv422':\n            std[1], std[2] = std[1] * self.sqrt1_2, std[2] * self.sqrt1_2\n        elif mode == 'yuv420':\n            std[1], std[2] = std[1] * 0.5, std[2] * 0.5\n        return mean, std", ""]}
{"filename": "mindspore/util/rmse.py", "chunked_list": ["from mindspore import nn\nfrom mindspore.ops import functional as F\n\n\nclass RMSELoss(nn.LossBase):\n    def __init__(self, epsilon=0.001):\n        super(RMSELoss, self).__init__()\n        self.epsilon = epsilon * epsilon\n        self.MSELoss = nn.MSELoss()\n\n    def construct(self, logits, label):\n        rmse_loss = F.sqrt(self.MSELoss(logits, label) + self.epsilon)\n        return rmse_loss", ""]}
{"filename": "mindspore/util/converter.py", "chunked_list": [""]}
{"filename": "torch/cycmunet_test.py", "chunked_list": ["import math\nimport logging\nimport os\nimport time\n\nimport tqdm\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.backends.cuda", "from torch.utils.data import DataLoader\nimport torch.backends.cuda\nimport torch.backends.cudnn\nimport torchvision.utils\nfrom pytorch_msssim import SSIM\n\nfrom model import CycMuNet\nfrom model.util import converter, normalizer\nimport dataset\nfrom cycmunet.model import model_arg", "import dataset\nfrom cycmunet.model import model_arg\nfrom cycmunet.run import test_arg\n\nmodel_args = model_arg(nf=64,\n                       groups=8,\n                       upscale_factor=2,\n                       format='yuv420',\n                       layers=4,\n                       cycle_count=3", "                       layers=4,\n                       cycle_count=3\n                       )\n\ntest_args = test_arg(\n    size=(256, 256),\n    checkpoint='checkpoints/monitor-ugly-sparsity_2x_l4_c3_epoch_2.pth',\n    dataset_indexes=[\n        \"/root/videos/cctv-scaled/index-test-good.txt\",\n        \"/root/videos/cctv-scaled/index-test-ugly.txt\",", "        \"/root/videos/cctv-scaled/index-test-good.txt\",\n        \"/root/videos/cctv-scaled/index-test-ugly.txt\",\n        \"/root/videos/cctv-scaled/index-test-smooth.txt\",\n        \"/root/videos/cctv-scaled/index-test-sharp.txt\",\n    ],\n    preview_interval=100,\n    seed=0,\n    batch_size=4,\n    fp16=True,\n)", "    fp16=True,\n)\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\nforce_data_dtype = torch.float16 if test_args.fp16 else None", "\nforce_data_dtype = torch.float16 if test_args.fp16 else None\n\n# --------------------------------------\n# Start of code\n\npreview_interval = 100 \\\n    if (len(test_args.dataset_indexes) == 1 or math.gcd(100, len(test_args.dataset_indexes)) == 1) \\\n    else 101\n", "    else 101\n\nnrow = 1 if test_args.size[0] * 9 > test_args.size[1] * 16 else 3\n\ntorch.manual_seed(test_args.seed)\ntorch.cuda.manual_seed(test_args.seed)\n\nformatter = logging.Formatter('%(asctime)s %(levelname)s [%(name)s]: %(message)s')\n\nch = logging.StreamHandler()", "\nch = logging.StreamHandler()\nch.setFormatter(formatter)\nch.setLevel(logging.DEBUG)\n\nlogger = logging.getLogger('test_progress')\nlogger.addHandler(ch)\nlogger.setLevel(logging.DEBUG)\n\nlogger_init = logging.getLogger('initialization')", "\nlogger_init = logging.getLogger('initialization')\nlogger_init.addHandler(ch)\nlogger_init.setLevel(logging.DEBUG)\n\ncvt = converter()\nnorm = normalizer()\n\ndataset_types = {\n    'triplet': dataset.ImageSequenceDataset,", "dataset_types = {\n    'triplet': dataset.ImageSequenceDataset,\n    'video': dataset.VideoFrameDataset\n}\nDataset = dataset_types[test_args.dataset_type]\nif len(test_args.dataset_indexes) == 1:\n    ds_test = Dataset(test_args.dataset_indexes[0],\n                      test_args.size,\n                      model_args.upscale_factor,\n                      augment=True,\n                      seed=test_args.seed)\nelse:\n    ds_test = dataset.InterleavedDataset(*[\n        Dataset(dataset_index,\n                test_args.size,\n                model_args.upscale_factor,\n                augment=True,\n                seed=test_args.seed + i)\n        for i, dataset_index in enumerate(test_args.dataset_indexes)])", "ds_test = DataLoader(ds_test,\n                     num_workers=1,\n                     batch_size=test_args.batch_size,\n                     shuffle=Dataset.want_shuffle,  # Video dataset friendly\n                     drop_last=True)\n\nmodel = CycMuNet(model_args)\nmodel.eval()\nnum_params = 0\nfor param in model.parameters():\n    num_params += param.numel()", "num_params = 0\nfor param in model.parameters():\n    num_params += param.numel()\nlogger_init.info(f\"Model has {num_params} parameters.\")\n\nif not os.path.exists(test_args.checkpoint):\n    logger_init.error(f\"Checkpoint weight {test_args.checkpoint} not exist.\")\n    exit(1)\nstate_dict = torch.load(test_args.checkpoint, map_location=lambda storage, loc: storage)\nload_result = model.load_state_dict(state_dict, strict=False)\nif load_result.unexpected_keys:\n    logger_init.warning(f\"Unknown parameters ignored: {load_result.unexpected_keys}\")", "state_dict = torch.load(test_args.checkpoint, map_location=lambda storage, loc: storage)\nload_result = model.load_state_dict(state_dict, strict=False)\nif load_result.unexpected_keys:\n    logger_init.warning(f\"Unknown parameters ignored: {load_result.unexpected_keys}\")\nif load_result.missing_keys:\n    logger_init.warning(f\"Missing parameters not initialized: {load_result.missing_keys}\")\nlogger_init.info(\"Checkpoint loaded.\")\n\nmodel = model.cuda()\nif force_data_dtype:\n    model = model.to(force_data_dtype)", "model = model.cuda()\nif force_data_dtype:\n    model = model.to(force_data_dtype)\n\nepsilon = (1 / 255) ** 2\n\n\ndef rmse(a, b):\n    return torch.mean(torch.sqrt((a - b) ** 2 + epsilon))\n", "\n\nssim_module = SSIM(data_range=1.0, nonnegative_ssim=True).cuda()\n\n\ndef ssim(a, b):\n    return 1 - ssim_module(a, b)\n\n\ndef recursive_cuda(li, force_data_dtype):\n    if isinstance(li, (list, tuple)):\n        return tuple(recursive_cuda(i, force_data_dtype) for i in li)\n    else:\n        if force_data_dtype is not None:\n            return li.cuda().to(force_data_dtype)\n        else:\n            return li.cuda()", "\ndef recursive_cuda(li, force_data_dtype):\n    if isinstance(li, (list, tuple)):\n        return tuple(recursive_cuda(i, force_data_dtype) for i in li)\n    else:\n        if force_data_dtype is not None:\n            return li.cuda().to(force_data_dtype)\n        else:\n            return li.cuda()\n", "\n\nif __name__ == '__main__':\n    with torch.no_grad():\n        total_loss = [0.0] * 4\n        total_iter = len(ds_test)\n        with tqdm.tqdm(total=total_iter, desc=f\"Test\") as progress:\n            for it, data in enumerate(ds_test):\n                (hf0, hf1, hf2), (lf0, lf1, lf2) = recursive_cuda(data, force_data_dtype)\n                if Dataset.pix_type == 'yuv':\n                    target = [cvt.yuv2rgb(*inp) for inp in (hf0, hf1, hf2, lf1)]\n                else:\n                    target = [hf0, hf1, hf2, lf1]\n\n                if it % preview_interval == 0:\n                    if Dataset.pix_type == 'yuv':\n                        org = [F.interpolate(cvt.yuv2rgb(y[0:1], uv[0:1]),\n                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n                                             mode='nearest').detach().float().cpu()\n                               for y, uv in (lf0, lf1, lf2)]\n                    else:\n                        org = [F.interpolate(lf[0:1],\n                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n                                             mode='nearest').detach().float().cpu()\n                               for lf in (lf0, lf1, lf2)]\n\n                if Dataset.pix_type == 'rgb':\n                    lf0, lf2 = cvt.rgb2yuv(lf0), cvt.rgb2yuv(lf2)\n\n                t0 = time.perf_counter()\n                lf0, lf2 = norm.normalize_yuv_420(*lf0), norm.normalize_yuv_420(*lf2)\n                outs = model(lf0, lf2, batch_mode='batch')\n\n                t1 = time.perf_counter()\n                t_forward = t1 - t0\n                actual = [cvt.yuv2rgb(*norm.denormalize_yuv_420(*out)).float() for out in outs]\n\n                if it % preview_interval == 0:\n                    out = [i[0:1].detach().float().cpu() for i in actual[:3]]\n                    ref = [i[0:1].detach().float().cpu() for i in target[:3]]\n\n                    for idx, ts in enumerate(zip(org, out, ref)):\n                        torchvision.utils.save_image(torch.concat(ts), f\"./result/out{idx}.png\",\n                                                     value_range=(0, 1), nrow=nrow, padding=0)\n\n                rmse_loss = [rmse(a, t).item() for a, t in zip(actual, target)]\n                ssim_loss = [ssim(a, t).item() for a, t in zip(actual, target)]\n\n                t2 = time.perf_counter()\n                t_loss = t2 - t1\n\n                rmse_h = sum(rmse_loss[:3]) / 3\n                rmse_l = rmse_loss[3]\n                ssim_h = sum(ssim_loss[:3]) / 3\n                ssim_l = ssim_loss[3]\n\n                total_loss[0] += rmse_h\n                total_loss[1] += rmse_l\n                total_loss[2] += ssim_h\n                total_loss[3] += ssim_l\n\n                progress.set_postfix(ordered_dict={\n                    \"rmse_h\": f\"{rmse_h:.4f}\",\n                    \"rmse_l\": f\"{rmse_l:.4f}\",\n                    \"ssim_h\": f\"{ssim_h:.4f}\",\n                    \"ssim_l\": f\"{ssim_l:.4f}\",\n                    \"f\": f\"{t_forward:.4f}s\",\n                    \"l\": f\"{t_loss:.4f}s\",\n                })\n                progress.update()\n\n        logger.info(f\"Test Complete: \"\n                    f\"RMSE HQ: {total_loss[0] / total_iter:.4f} \"\n                    f\"RMSE LQ: {total_loss[1] / total_iter:.4f} \"\n                    f\"SSIM HQ: {total_loss[2] / total_iter:.4f} \"\n                    f\"SSIM LQ: {total_loss[3] / total_iter:.4f}\")", ""]}
{"filename": "torch/cycmunet_train.py", "chunked_list": ["import math\nimport logging\nimport os\nimport pathlib\nimport time\n\nimport tqdm\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader", "import torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.backends.cuda\nimport torch.backends.cudnn\nimport torchvision.utils\nfrom pytorch_msssim import SSIM\n\nfrom model import CycMuNet\nfrom model.util import converter, normalizer", "from model import CycMuNet\nfrom model.util import converter, normalizer\nimport dataset\nfrom cycmunet.model import model_arg\nfrom cycmunet.run import train_arg\n\n# ------------------------------------------\n# Configs\n\nmodel_args = model_arg(nf=64,", "\nmodel_args = model_arg(nf=64,\n                       groups=8,\n                       upscale_factor=2,\n                       format='yuv420',\n                       layers=4,\n                       cycle_count=3\n                       )\n\ntrain_args = train_arg(", "\ntrain_args = train_arg(\n    size=(128, 128),\n    pretrained=\"/root/cycmunet-new/checkpoints/monitor-ugly_2x_l4_c3_epoch_19.pth\",\n    # dataset_type=\"video\",\n    # dataset_indexes=[\n    #     \"/root/videos/cctv-scaled/index-train-good.txt\",\n    #     \"/root/videos/cctv-scaled/index-train-ugly.txt\",\n    #     \"/root/videos/cctv-scaled/index-train-smooth.txt\",\n    #     \"/root/videos/cctv-scaled/index-train-sharp.txt\",", "    #     \"/root/videos/cctv-scaled/index-train-smooth.txt\",\n    #     \"/root/videos/cctv-scaled/index-train-sharp.txt\",\n    # ],\n    dataset_type=\"triplet\",\n    dataset_indexes=[\n        \"/root/dataset/vimeo_triplet/tri_trainlist.txt\"\n    ],\n    preview_interval=100,\n    seed=0,\n    lr=0.001,", "    seed=0,\n    lr=0.001,\n    start_epoch=1,\n    end_epoch=11,\n    sparsity=True,\n    batch_size=2,\n    autocast=False,\n    loss_type='rmse',\n    save_path='checkpoints',\n    save_prefix='triplet',", "    save_path='checkpoints',\n    save_prefix='triplet',\n)\n\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cudnn.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n", "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\n# --------------------------------------\n# Start of code\n\npreview_interval = 100 \\\n    if (len(train_args.dataset_indexes) == 1 or math.gcd(100, len(train_args.dataset_indexes)) == 1) \\\n    else 101\n\nsave_prefix = f'{train_args.save_prefix}_{model_args.upscale_factor}x_l{model_args.layers}_c{model_args.cycle_count}'", "\nsave_prefix = f'{train_args.save_prefix}_{model_args.upscale_factor}x_l{model_args.layers}_c{model_args.cycle_count}'\n\nsave_path = pathlib.Path(train_args.save_path)\n\nnrow = 1 if train_args.size[0] * 9 > train_args.size[1] * 16 else 3\n\ntorch.manual_seed(train_args.seed)\ntorch.cuda.manual_seed(train_args.seed)\n", "torch.cuda.manual_seed(train_args.seed)\n\nformatter = logging.Formatter('%(asctime)s %(levelname)s [%(name)s]: %(message)s')\n\nch = logging.StreamHandler()\nch.setFormatter(formatter)\nch.setLevel(logging.DEBUG)\n\nlogger = logging.getLogger('train_progress')\nlogger.addHandler(ch)", "logger = logging.getLogger('train_progress')\nlogger.addHandler(ch)\nlogger.setLevel(logging.DEBUG)\n\nlogger_init = logging.getLogger('initialization')\nlogger_init.addHandler(ch)\nlogger_init.setLevel(logging.DEBUG)\n\ncvt = converter()\nnorm = normalizer()", "cvt = converter()\nnorm = normalizer()\n\ndataset_types = {\n    'triplet': dataset.ImageSequenceDataset,\n    'video': dataset.VideoFrameDataset\n}\nDataset = dataset_types[train_args.dataset_type]\nif len(train_args.dataset_indexes) == 1:\n    ds_train = Dataset(train_args.dataset_indexes[0],\n                       train_args.size,\n                       model_args.upscale_factor,\n                       augment=True,\n                       seed=train_args.seed)\nelse:\n    ds_train = dataset.InterleavedDataset(*[\n        Dataset(dataset_index,\n                train_args.size,\n                model_args.upscale_factor,\n                augment=True,\n                seed=train_args.seed + i)\n        for i, dataset_index in enumerate(train_args.dataset_indexes)])", "if len(train_args.dataset_indexes) == 1:\n    ds_train = Dataset(train_args.dataset_indexes[0],\n                       train_args.size,\n                       model_args.upscale_factor,\n                       augment=True,\n                       seed=train_args.seed)\nelse:\n    ds_train = dataset.InterleavedDataset(*[\n        Dataset(dataset_index,\n                train_args.size,\n                model_args.upscale_factor,\n                augment=True,\n                seed=train_args.seed + i)\n        for i, dataset_index in enumerate(train_args.dataset_indexes)])", "ds_train = DataLoader(ds_train,\n                      num_workers=1,\n                      batch_size=train_args.batch_size,\n                      shuffle=Dataset.want_shuffle,  # Video dataset friendly\n                      drop_last=True)\n\nmodel = CycMuNet(model_args)\nmodel.train()\nmodel_updated = False\nnum_params = 0\nfor param in model.parameters():\n    num_params += param.numel()", "model_updated = False\nnum_params = 0\nfor param in model.parameters():\n    num_params += param.numel()\nlogger_init.info(f\"Model has {num_params} parameters.\")\n\nif train_args.pretrained:\n    if not os.path.exists(train_args.pretrained):\n        logger_init.warning(f\"Pretrained weight {train_args.pretrained} not exist.\")\n    state_dict = torch.load(train_args.pretrained, map_location=lambda storage, loc: storage)\n    load_result = model.load_state_dict(state_dict, strict=False)\n    if load_result.unexpected_keys:\n        logger_init.warning(f\"Unknown parameters ignored: {load_result.unexpected_keys}\")\n    if load_result.missing_keys:\n        logger_init.warning(f\"Missing parameters not initialized: {load_result.missing_keys}\")\n    logger_init.info(\"Pretrained weights loaded.\")", "\nmodel = model.cuda()\noptimizer = optim.Adamax(model.parameters(), lr=train_args.lr, betas=(0.9, 0.999), eps=1e-8)\n# Or, train only some parts\n# optimizer = optim.Adamax(itertools.chain(\n#         model.head.parameters(),\n#         model.fe.parameters(),\n#         model.fr.parameters(),\n#         model.tail.parameters()\n# ), lr=args.lr, betas=(0.9, 0.999), eps=1e-8)", "#         model.tail.parameters()\n# ), lr=args.lr, betas=(0.9, 0.999), eps=1e-8)\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=40000, eta_min=1e-7)\n\nnum_params_train = 0\nfor group in optimizer.param_groups:\n    for params in group.get('params', []):\n        num_params_train += params.numel()\nlogger_init.info(f\"Model has {num_params} parameters to train.\")\n\nif train_args.sparsity:\n    from apex.contrib.sparsity import ASP\n\n    target_layers = []\n    target_layers.extend('mu.' + name for name, _ in model.mu.named_modules())\n    target_layers.extend('fr.' + name for name, _ in model.fr.named_modules())\n\n    ASP.init_model_for_pruning(model,\n                               mask_calculator=\"m4n2_1d\",\n                               allowed_layer_names=target_layers,\n                               verbosity=2,\n                               whitelist=[torch.nn.Linear, torch.nn.Conv2d],\n                               allow_recompute_mask=False,\n                               allow_permutation=False,\n                               )\n    ASP.init_optimizer_for_pruning(optimizer)\n\n    # import torch.fx\n    # original_symbolic_trace = torch.fx.symbolic_trace\n    # torch.fx.symbolic_trace = functools.partial(original_symbolic_trace, concrete_args={\n    #     'batch_mode': '_no_use_sparsity_pseudo',\n    #     'stop_at_conf': False,\n    #     'all_frames': True\n    # })\n\n    ASP.compute_sparse_masks()\n    # torch.fx.symbolic_trace = original_symbolic_trace\n    logger.info('Training with sparsity.')", "logger_init.info(f\"Model has {num_params} parameters to train.\")\n\nif train_args.sparsity:\n    from apex.contrib.sparsity import ASP\n\n    target_layers = []\n    target_layers.extend('mu.' + name for name, _ in model.mu.named_modules())\n    target_layers.extend('fr.' + name for name, _ in model.fr.named_modules())\n\n    ASP.init_model_for_pruning(model,\n                               mask_calculator=\"m4n2_1d\",\n                               allowed_layer_names=target_layers,\n                               verbosity=2,\n                               whitelist=[torch.nn.Linear, torch.nn.Conv2d],\n                               allow_recompute_mask=False,\n                               allow_permutation=False,\n                               )\n    ASP.init_optimizer_for_pruning(optimizer)\n\n    # import torch.fx\n    # original_symbolic_trace = torch.fx.symbolic_trace\n    # torch.fx.symbolic_trace = functools.partial(original_symbolic_trace, concrete_args={\n    #     'batch_mode': '_no_use_sparsity_pseudo',\n    #     'stop_at_conf': False,\n    #     'all_frames': True\n    # })\n\n    ASP.compute_sparse_masks()\n    # torch.fx.symbolic_trace = original_symbolic_trace\n    logger.info('Training with sparsity.')", "\n\nepsilon = (1 / 255) ** 2\n\n\ndef rmse(a, b):\n    return torch.mean(torch.sqrt((a - b) ** 2 + epsilon))\n\n\nssim_module = SSIM(data_range=1.0, nonnegative_ssim=True).cuda()", "\nssim_module = SSIM(data_range=1.0, nonnegative_ssim=True).cuda()\n\n\ndef ssim(a, b):\n    return 1 - ssim_module(a, b)\n\n\ndef recursive_cuda(li, force_data_dtype):\n    if isinstance(li, (list, tuple)):\n        return tuple(recursive_cuda(i, force_data_dtype) for i in li)\n    else:\n        if force_data_dtype is not None:\n            return li.cuda().to(force_data_dtype)\n        else:\n            return li.cuda()", "def recursive_cuda(li, force_data_dtype):\n    if isinstance(li, (list, tuple)):\n        return tuple(recursive_cuda(i, force_data_dtype) for i in li)\n    else:\n        if force_data_dtype is not None:\n            return li.cuda().to(force_data_dtype)\n        else:\n            return li.cuda()\n\n\ndef train(epoch):\n    epoch_loss = 0\n    total_iter = len(ds_train)\n    loss_coeff = [1, 0.5, 1, 0.5]\n    with tqdm.tqdm(total=total_iter, desc=f\"Epoch {epoch}\") as progress:\n        for it, data in enumerate(ds_train):\n            optimizer.zero_grad()\n\n            def compute_loss(force_data_dtype=None):\n                (hf0, hf1, hf2), (lf0, lf1, lf2) = recursive_cuda(data, force_data_dtype)\n                if Dataset.pix_type == 'yuv':\n                    target = [cvt.yuv2rgb(*inp) for inp in (hf0, hf1, hf2, lf1)]\n                else:\n                    target = [hf0, hf1, hf2, lf1]\n\n                if it % preview_interval == 0:\n                    if Dataset.pix_type == 'yuv':\n                        org = [F.interpolate(cvt.yuv2rgb(y[0:1], uv[0:1]),\n                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n                                             mode='nearest').detach().float().cpu()\n                               for y, uv in (lf0, lf1, lf2)]\n                    else:\n                        org = [F.interpolate(lf[0:1],\n                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n                                             mode='nearest').detach().float().cpu()\n                               for lf in (lf0, lf1, lf2)]\n\n                if Dataset.pix_type == 'rgb':\n                    lf0, lf2 = cvt.rgb2yuv(lf0), cvt.rgb2yuv(lf2)\n\n                t0 = time.perf_counter()\n                lf0, lf2 = norm.normalize_yuv_420(*lf0), norm.normalize_yuv_420(*lf2)\n                outs = model(lf0, lf2, batch_mode='batch')\n\n                t1 = time.perf_counter()\n                actual = [cvt.yuv2rgb(*norm.denormalize_yuv_420(*out)) for out in outs]\n\n                if train_args.loss_type == 'rmse':\n                    loss = [rmse(a, t) * c for a, t, c in zip(actual, target, loss_coeff)]\n                elif train_args.loss_type == 'ssim':\n                    loss = [ssim(a, t) * c for a, t, c in zip(actual, target, loss_coeff)]\n                else:\n                    raise ValueError(\"Unknown loss type: \" + train_args.loss_type)\n\n                assert not any(torch.any(torch.isnan(i)).item() for i in loss)\n\n                t2 = time.perf_counter()\n\n                if it % preview_interval == 0:\n                    out = [i[0:1].detach().float().cpu() for i in actual[:3]]\n                    ref = [i[0:1].detach().float().cpu() for i in target[:3]]\n\n                    for idx, ts in enumerate(zip(org, out, ref)):\n                        torchvision.utils.save_image(torch.concat(ts), f\"./result/out{idx}.png\",\n                                                     value_range=(0, 1), nrow=nrow, padding=0)\n\n                return loss, t1 - t0, t2 - t1\n\n            if train_args.autocast:\n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    loss, t_forward, t_loss = compute_loss(torch.float16)\n            else:\n                loss, t_forward, t_loss = compute_loss()\n\n            total_loss = sum(loss)\n            epoch_loss += total_loss.item()\n\n            t3 = time.perf_counter()\n            total_loss.backward()\n            optimizer.step()\n            scheduler.step()\n            t_backward = time.perf_counter() - t3\n\n            global model_updated\n            model_updated = True\n\n            progress.set_postfix(ordered_dict={\n                \"loss\": f\"{total_loss.item():.4f}\",\n                \"lr\": f\"{optimizer.param_groups[0]['lr']:.6e}\",\n                \"f\": f\"{t_forward:.4f}s\",\n                \"l\": f\"{t_loss:.4f}s\",\n                \"b\": f\"{t_backward:.4f}s\",\n            })\n            progress.update()\n\n    logger.info(f\"Epoch {epoch} Complete: Avg. Loss: {epoch_loss / total_iter:.4f}\")", "\n\ndef train(epoch):\n    epoch_loss = 0\n    total_iter = len(ds_train)\n    loss_coeff = [1, 0.5, 1, 0.5]\n    with tqdm.tqdm(total=total_iter, desc=f\"Epoch {epoch}\") as progress:\n        for it, data in enumerate(ds_train):\n            optimizer.zero_grad()\n\n            def compute_loss(force_data_dtype=None):\n                (hf0, hf1, hf2), (lf0, lf1, lf2) = recursive_cuda(data, force_data_dtype)\n                if Dataset.pix_type == 'yuv':\n                    target = [cvt.yuv2rgb(*inp) for inp in (hf0, hf1, hf2, lf1)]\n                else:\n                    target = [hf0, hf1, hf2, lf1]\n\n                if it % preview_interval == 0:\n                    if Dataset.pix_type == 'yuv':\n                        org = [F.interpolate(cvt.yuv2rgb(y[0:1], uv[0:1]),\n                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n                                             mode='nearest').detach().float().cpu()\n                               for y, uv in (lf0, lf1, lf2)]\n                    else:\n                        org = [F.interpolate(lf[0:1],\n                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n                                             mode='nearest').detach().float().cpu()\n                               for lf in (lf0, lf1, lf2)]\n\n                if Dataset.pix_type == 'rgb':\n                    lf0, lf2 = cvt.rgb2yuv(lf0), cvt.rgb2yuv(lf2)\n\n                t0 = time.perf_counter()\n                lf0, lf2 = norm.normalize_yuv_420(*lf0), norm.normalize_yuv_420(*lf2)\n                outs = model(lf0, lf2, batch_mode='batch')\n\n                t1 = time.perf_counter()\n                actual = [cvt.yuv2rgb(*norm.denormalize_yuv_420(*out)) for out in outs]\n\n                if train_args.loss_type == 'rmse':\n                    loss = [rmse(a, t) * c for a, t, c in zip(actual, target, loss_coeff)]\n                elif train_args.loss_type == 'ssim':\n                    loss = [ssim(a, t) * c for a, t, c in zip(actual, target, loss_coeff)]\n                else:\n                    raise ValueError(\"Unknown loss type: \" + train_args.loss_type)\n\n                assert not any(torch.any(torch.isnan(i)).item() for i in loss)\n\n                t2 = time.perf_counter()\n\n                if it % preview_interval == 0:\n                    out = [i[0:1].detach().float().cpu() for i in actual[:3]]\n                    ref = [i[0:1].detach().float().cpu() for i in target[:3]]\n\n                    for idx, ts in enumerate(zip(org, out, ref)):\n                        torchvision.utils.save_image(torch.concat(ts), f\"./result/out{idx}.png\",\n                                                     value_range=(0, 1), nrow=nrow, padding=0)\n\n                return loss, t1 - t0, t2 - t1\n\n            if train_args.autocast:\n                with torch.autocast(device_type='cuda', dtype=torch.float16):\n                    loss, t_forward, t_loss = compute_loss(torch.float16)\n            else:\n                loss, t_forward, t_loss = compute_loss()\n\n            total_loss = sum(loss)\n            epoch_loss += total_loss.item()\n\n            t3 = time.perf_counter()\n            total_loss.backward()\n            optimizer.step()\n            scheduler.step()\n            t_backward = time.perf_counter() - t3\n\n            global model_updated\n            model_updated = True\n\n            progress.set_postfix(ordered_dict={\n                \"loss\": f\"{total_loss.item():.4f}\",\n                \"lr\": f\"{optimizer.param_groups[0]['lr']:.6e}\",\n                \"f\": f\"{t_forward:.4f}s\",\n                \"l\": f\"{t_loss:.4f}s\",\n                \"b\": f\"{t_backward:.4f}s\",\n            })\n            progress.update()\n\n    logger.info(f\"Epoch {epoch} Complete: Avg. Loss: {epoch_loss / total_iter:.4f}\")", "\n\ndef save_model(epoch):\n    if epoch == -1:\n        name = \"snapshot\"\n    else:\n        name = f\"epoch_{epoch}\"\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    output_path = save_path / f\"{save_prefix}_{name}.pth\"\n    torch.save(model.state_dict(), output_path)\n    logger.info(f\"Checkpoint saved to {output_path}\")", "\n\nif __name__ == '__main__':\n    try:\n        for epoch in range(train_args.start_epoch, train_args.end_epoch):\n            # with torch.autograd.detect_anomaly():\n            #     train(epoch)\n            train(epoch)\n            save_model(epoch)\n    except KeyboardInterrupt:\n        if model_updated:\n            save_model(-1)", ""]}
{"filename": "torch/cycmunet_export_onnx.py", "chunked_list": ["import os\nimport pathlib\n\nimport torch\nfrom torch.onnx import symbolic_helper\nimport onnx\nimport onnx.shape_inference\nimport onnxsim\nimport onnx_graphsurgeon as gs\n", "import onnx_graphsurgeon as gs\n\nfrom model import CycMuNet, use_fold_catconv\nfrom cycmunet.model import model_arg\n\nmodel_args = model_arg(nf=64,\n                       groups=8,\n                       upscale_factor=2,\n                       format='yuv420',\n                       layers=4,", "                       format='yuv420',\n                       layers=4,\n                       cycle_count=3\n                       )\n\ncheckpoint_file = 'checkpoints/triplet_s2_2x_l4_c3_snapshot.pth'\noutput_path = 'onnx/triplet_new'\n\n\nsize = 1 << model_args.layers", "\nsize = 1 << model_args.layers\nsize_in = (size, size)\nsize_out = tuple(i * model_args.upscale_factor for i in size_in)\noutput_path = pathlib.Path(output_path)\nconfig_string = f\"_{model_args.upscale_factor}x_l{model_args.layers}\"\nif model_args.format == 'yuv420':\n    size_uv_in = tuple(i // 2 for i in size_in)\n    config_string += '_yuv1-1'\n", "\nfe_onnx = str(output_path / f'fe{config_string}.onnx')\nff_onnx = str(output_path / f'ff{config_string}.onnx')\nos.makedirs(output_path, exist_ok=True)\n\n\n# Placeholder to export DeformConv\n@symbolic_helper.parse_args(\"v\", \"v\", \"v\", \"v\", \"v\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"b\")\ndef symbolic_deform_conv2d_forward(g,\n                                   input,\n                                   weight,\n                                   offset,\n                                   mask,\n                                   bias,\n                                   stride_h,\n                                   stride_w,\n                                   pad_h,\n                                   pad_w,\n                                   dil_h,\n                                   dil_w,\n                                   n_weight_grps,\n                                   n_offset_grps,\n                                   use_mask):\n    if n_weight_grps != 1 or not use_mask:\n        raise NotImplementedError()\n    return g.op(\"custom::DeformConv2d\", input, offset, mask, weight, bias, stride_i=[stride_h, stride_w],\n                padding_i=[pad_h, pad_w], dilation_i=[dil_h, dil_w], deformable_groups_i=n_offset_grps)", "def symbolic_deform_conv2d_forward(g,\n                                   input,\n                                   weight,\n                                   offset,\n                                   mask,\n                                   bias,\n                                   stride_h,\n                                   stride_w,\n                                   pad_h,\n                                   pad_w,\n                                   dil_h,\n                                   dil_w,\n                                   n_weight_grps,\n                                   n_offset_grps,\n                                   use_mask):\n    if n_weight_grps != 1 or not use_mask:\n        raise NotImplementedError()\n    return g.op(\"custom::DeformConv2d\", input, offset, mask, weight, bias, stride_i=[stride_h, stride_w],\n                padding_i=[pad_h, pad_w], dilation_i=[dil_h, dil_w], deformable_groups_i=n_offset_grps)", "\n\n# Register custom symbolic function\ntorch.onnx.register_custom_op_symbolic(\"torchvision::deform_conv2d\", symbolic_deform_conv2d_forward, 13)\n\n\ndef clean_fp16_subnormal(t: torch.Tensor):\n    threshold = 0.00006103515625\n    mask = torch.logical_and(t > -threshold, t < threshold)\n    t[mask] = 0\n    return t", "\n\ndef as_module(func):\n    class mod(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, *x):\n            return func(*x)\n\n    return mod().eval()", "\n\ndef simplify(name):\n    model, other = onnxsim.simplify(name)\n    graph = gs.import_onnx(model)\n    graph.fold_constants().cleanup()\n\n    for n in graph.nodes:\n        if n.op == 'DeformConv2d':\n            if n.outputs[0].outputs[0].op == 'LeakyRelu':\n                lrelu = n.outputs[0].outputs[0]\n                n.attrs['activation_type'] = 3\n                n.attrs['alpha'] = lrelu.attrs['alpha']\n                n.attrs['beta'] = 0.0\n                n.outputs = lrelu.outputs\n                lrelu.inputs = []\n                lrelu.outputs = []\n            else:\n                n.attrs['activation_type'] = -1\n                n.attrs['alpha'] = 0.0\n                n.attrs['beta'] = 0.0\n\n    graph.cleanup().toposort()\n\n    model = gs.export_onnx(graph)\n    onnx.save_model(model, name)\n    print(f'Simplify {name} done')", "\n\nuse_fold_catconv()\nmodel = CycMuNet(model_args)\nstate_dict = torch.load(checkpoint_file, map_location='cpu')\nstate_dict = {k: clean_fp16_subnormal(v) for k, v in state_dict.items() if '__weight_mma_mask' not in k}\nmodel.load_state_dict(state_dict)\nmodel = model.eval()\nfor v in model.parameters(recurse=True):\n    v.requires_grad = False", "for v in model.parameters(recurse=True):\n    v.requires_grad = False\n\n\nif __name__ == '__main__':\n    with torch.no_grad():\n        print(\"Exporting fe...\")\n\n        if model_args.format == 'rgb':\n            fe_i = torch.zeros((2, 3, *size))\n            dynamic_axes = {\n                \"x\": {\n                    0: \"batch_size\",\n                    2: \"input_height\",\n                    3: \"input_width\"\n                },\n            }\n        elif model_args.format == 'yuv420':\n            fe_i = tuple([torch.zeros((2, 1, *size_in)), torch.zeros((2, 2, *size_uv_in))])\n            dynamic_axes = {\n                \"y\": {\n                    0: \"batch_size\",\n                    2: \"input_height\",\n                    3: \"input_width\"\n                },\n                \"uv\": {\n                    0: \"batch_size\",\n                    2: \"input_height_uv\",\n                    3: \"input_width_uv\"\n                },\n            }\n        else:\n            raise NotImplementedError()\n        input_names = list(dynamic_axes.keys())\n        output_names = [f'l{i}' for i in range(model_args.layers)[::-1]]\n        dynamic_axes.update({f'l{i}': {\n            0: \"batch_size\",\n            2: f\"feature_height_{i}\",\n            3: f\"feature_width_{i}\"\n        } for i in range(model_args.layers)[::-1]})\n\n        @as_module\n        def fe(*x_or_y_uv: torch.Tensor):\n            if model_args.format == 'rgb':\n                return model.head_fe(x_or_y_uv[0])\n            else:\n                return model.head_fe(x_or_y_uv)\n\n\n        torch.onnx.export(fe, fe_i, fe_onnx, opset_version=13,\n                          export_params=True,\n                          input_names=input_names, output_names=output_names,\n                          dynamic_axes=dynamic_axes)\n\n        print(\"Exporting ff...\")\n\n        ff_i = []\n        input_axes = dict()\n        cur_size = size_in\n        for i in range(model_args.layers)[::-1]:\n            ff_i.insert(0, torch.zeros(1, model_args.nf, *cur_size))\n            cur_size = tuple((i + 1) // 2 for i in cur_size)\n\n        for i in range(model_args.layers):\n            axes = {\n                0: \"batch_size\",\n                2: f\"feature_height_{i}\",\n                3: f\"feature_width_{i}\"\n            }\n            input_axes[f'f0l{i}'] = axes\n            input_axes[f'f2l{i}'] = axes\n        input_names = [f'f0l{i}' for i in range(model_args.layers)[::-1]] + \\\n                      [f'f2l{i}' for i in range(model_args.layers)[::-1]]\n        output_names = ['f1']\n        dynamic_axes = dict(input_axes)\n        dynamic_axes[f'f1'] = {\n            0: \"batch_size\",\n            2: f\"feature_height_{model_args.layers - 1}\",\n            3: f\"feature_width_{model_args.layers - 1}\"\n        }\n\n        if model_args.format == 'rgb':\n            output_axes = {\n                \"h0\": {\n                    0: \"batch_size\",\n                    2: \"output_height\",\n                    3: \"output_width\"\n                },\n                \"h1\": {\n                    0: \"batch_size\",\n                    2: \"output_height\",\n                    3: \"output_width\"\n                },\n            }\n        elif model_args.format == 'yuv420':\n            output_axes = {\n                \"h0_y\": {\n                    0: \"batch_size\",\n                    2: \"output_height\",\n                    3: \"output_width\"\n                },\n                \"h0_uv\": {\n                    0: \"batch_size\",\n                    2: \"output_height_uv\",\n                    3: \"output_width_uv\"\n                },\n                \"h1_y\": {\n                    0: \"batch_size\",\n                    2: \"output_height\",\n                    3: \"output_width\"\n                },\n                \"h1_uv\": {\n                    0: \"batch_size\",\n                    2: \"output_height_uv\",\n                    3: \"output_width_uv\"\n                },\n            }\n        else:\n            raise NotImplementedError()\n        output_names = list(output_axes.keys())\n        dynamic_axes = dict(input_axes)\n        dynamic_axes.update(output_axes)\n\n        @as_module\n        def ff(input1, input2):\n            fea = [input1[-1], model.ff(input1, input2)[0], input2[-1]]\n            outs = model.mu_fr_tail(fea, all_frames=False)\n            return outs\n\n\n        torch.onnx.export(ff, (ff_i, ff_i),\n                          str(output_path / f'ff{config_string}.onnx'), opset_version=13,\n                          export_params=True,\n                          input_names=input_names, output_names=output_names,\n                          dynamic_axes=dynamic_axes)\n\n    simplify(fe_onnx)\n    simplify(ff_onnx)", ""]}
{"filename": "torch/cycmunet/model.py", "chunked_list": ["from collections import namedtuple\n\nmodel_arg = namedtuple('model_arg', ('nf',  # number of feature channel\n                                     'groups',  # number of deformable convolution group\n                                     'upscale_factor',  # model upscale factor\n                                     'format',  # model I/O format (rgb, yuv420)\n                                     'layers',  # feature fusion pyramid layers\n                                     'cycle_count'  # mutual cycle count\n                                     ))\n", "                                     ))\n"]}
{"filename": "torch/cycmunet/run.py", "chunked_list": ["from collections import namedtuple\n\n_share_args = ('size',  # input size\n               'dataset_type',  # type of dataset\n               'dataset_indexes',  # index files for dataset\n               'preview_interval',  # interval to save network output for previewing\n               'batch_size',  # process batch size\n               'seed',  # seed for random number generators\n               )\n", "               )\n\n_train_args = ('lr',  # init learning rate\n               'pretrained',  # pretrained checkpoint\n               'start_epoch',  # start epoch index\n               'end_epoch',  # end epoch index (exclusive)\n               'sparsity',  # train network with sparsity\n               'autocast',  # train with auto mixed precision\n               'loss_type',  # loss type for optimization\n               'save_path',  # checkpoint save path", "               'loss_type',  # loss type for optimization\n               'save_path',  # checkpoint save path\n               'save_prefix',  # prefix of checkpoint file name\n               )\n\n_test_args = ('checkpoints',  # checkpoint to test\n              'fp16',  # use fp16 to run network forward\n              )\n\ntrain_arg = namedtuple('train_arg', (*_share_args, *_train_args))", "\ntrain_arg = namedtuple('train_arg', (*_share_args, *_train_args))\ntest_arg = namedtuple('test_arg', (*_share_args, *_test_args))\n"]}
{"filename": "torch/model/part.py", "chunked_list": ["from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nfrom torch.nn.common_types import _size_2_t\nfrom torch.nn.modules.utils import _pair\nimport torchvision.ops\n", "import torchvision.ops\n\nfrom .util import cat_conv\n\n\nclass ResidualBlock_noBN(nn.Module):\n    '''Residual block w/o BN\n    ---Conv-ReLU-Conv-+-\n     |________________|\n    '''\n\n    def __init__(self, nf=64):\n        super(ResidualBlock_noBN, self).__init__()\n        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\n        # initialization\n        self.init_weights(self.conv1)\n        self.init_weights(self.conv2)\n\n    @staticmethod\n    def init_weights(conv):\n        init.kaiming_normal_(conv.weight, a=0, mode='fan_in')\n        conv.weight.data *= 0.1  # for residual block\n        if conv.bias is not None:\n            conv.bias.data.zero_()\n\n    def forward(self, x):\n        identity = x\n        out = F.relu(self.conv1(x), inplace=True)\n        out = self.conv2(out)\n        return identity + out", "\n\nclass DCN_sep(nn.Module):\n    def __init__(self,\n                 in_channels: int,\n                 in_channels_features: int,\n                 out_channels: int,\n                 kernel_size: _size_2_t,\n                 stride: _size_2_t = 1,\n                 padding: _size_2_t = 0,\n                 dilation: _size_2_t = 1,\n                 groups: int = 1,\n                 deformable_groups: int = 1,\n                 bias: bool = True,\n                 mask: bool = True):\n        super(DCN_sep, self).__init__()\n\n        self.dcn = torchvision.ops.DeformConv2d(in_channels, out_channels, kernel_size, stride, padding, dilation,\n                                                groups, bias)\n\n        kernel_size_ = _pair(kernel_size)\n        offset_channels = deformable_groups * kernel_size_[0] * kernel_size_[1]\n\n        self.conv_offset = nn.Conv2d(in_channels_features, offset_channels * 2, kernel_size=kernel_size,\n                                     stride=stride, padding=padding, dilation=dilation, bias=True)\n        self.conv_mask = nn.Conv2d(in_channels_features, offset_channels, kernel_size=kernel_size,\n                                   stride=stride, padding=padding, dilation=dilation, bias=True) if mask else None\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, input: torch.Tensor, feature: torch.Tensor):\n        offset = self.conv_offset(feature)\n        mask = torch.sigmoid(self.conv_mask(feature)) if self.conv_mask else None\n\n        return self.dcn(input, offset, mask)", "\n\nclass PCDLayer(nn.Module):\n    \"\"\" Alignment module using Pyramid, Cascading and Deformable convolution\"\"\"\n    def __init__(self, args, first_layer: bool):\n        super(PCDLayer, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n\n        self.offset_conv1 = nn.Conv2d(2 * self.nf, self.nf, 3, 1, 1)\n        self.offset_conv3 = nn.Conv2d(self.nf, self.nf, 3, 1, 1)\n        self.dcnpack = DCN_sep(self.nf, self.nf, self.nf, 3, stride=1, padding=1, dilation=1,\n                               deformable_groups=self.groups)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n        if not first_layer:\n            self.offset_conv2 = nn.Conv2d(2 * self.nf, self.nf, 3, 1, 1)\n            self.fea_conv = nn.Conv2d(2 * self.nf, self.nf, 3, 1, 1)\n\n    def forward(self, current_sources: Tuple[torch.Tensor, torch.Tensor],\n                last_offset: torch.Tensor, last_feature: torch.Tensor):\n        offset = self.lrelu(cat_conv(self.offset_conv1, current_sources))\n        if last_offset is not None:\n            last_offset = F.interpolate(last_offset, scale_factor=2, mode='bilinear', align_corners=False)\n            _, _, h, w = offset.shape\n            last_offset = last_offset[..., :h, :w]\n            offset = self.lrelu(cat_conv(self.offset_conv2, (offset, last_offset * 2)))\n        offset = self.lrelu(self.offset_conv3(offset))\n        feature = self.dcnpack(current_sources[0], offset)\n        if last_feature is not None:\n            last_feature = F.interpolate(last_feature, scale_factor=2, mode='bilinear', align_corners=False)\n            _, _, h, w = feature.shape\n            last_feature = last_feature[..., :h, :w]\n            feature = cat_conv(self.fea_conv, (feature, last_feature))\n        feature = self.lrelu(feature)\n        return offset, feature", ""]}
{"filename": "torch/model/__init__.py", "chunked_list": ["from .util import use_fold_catconv\n\nfrom .cycmunet import CycMuNet\n"]}
{"filename": "torch/model/util.py", "chunked_list": ["import functools\nimport itertools\nimport math\nfrom typing import Tuple, Union\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision", "import torch.nn.functional as F\nimport torchvision\n\nRGBOrYUV = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n\n_use_fold_catconv = False\n\n\ndef use_fold_catconv(value=True):\n    global _use_fold_catconv", "def use_fold_catconv(value=True):\n    global _use_fold_catconv\n    _use_fold_catconv = value\n\n\ndef cat_simp(ts, *args, **kwargs):\n    \"\"\"auto eliminate cat if there's only one input\"\"\"\n    if len(ts) == 1:\n        return ts[0]\n    return torch.cat(ts, *args, **kwargs)", "        return ts[0]\n    return torch.cat(ts, *args, **kwargs)\n\n\ndef cat_conv(conv: nn.Conv2d, tensors, scale=None):\n    \"\"\"separate cat+conv into multiple conv to reduce memory footprint\"\"\"\n    if _use_fold_catconv:\n        w = conv.weight.detach()\n        b = conv.bias.detach()\n        if scale is not None:", "        b = conv.bias.detach()\n        if scale is not None:\n            w *= scale\n            b *= scale\n        output = None\n        channels = [0]\n        channels.extend(itertools.accumulate(int(tensor.shape[1]) for tensor in tensors))\n        for ti, cb, ce in zip(tensors, channels, channels[1:]):\n            c = ti.shape[1]\n            convi = nn.Conv2d(c, conv.out_channels, conv.kernel_size, conv.stride, conv.padding,", "            c = ti.shape[1]\n            convi = nn.Conv2d(c, conv.out_channels, conv.kernel_size, conv.stride, conv.padding,\n                              conv.dilation, bias=output is None).eval()\n            convi.weight = nn.Parameter(w[:, cb:ce, :, :], requires_grad=False)\n            if output is None:\n                convi.bias = nn.Parameter(b, requires_grad=False)\n            outputi = convi(ti)\n            output = outputi if output is None else output + outputi\n        return output\n    else:", "        return output\n    else:\n        return conv(torch.cat(tensors, dim=1))\n\n\n# mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\nclass normalizer:\n    nm = torchvision.transforms.Normalize\n    sqrt2 = math.sqrt(2)\n", "    sqrt2 = math.sqrt(2)\n\n    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), kr=0.2126, kb=0.0722, depth=8):\n        self.mean = mean\n        self.std = std\n        self.krgb = (kr, 1 - kr - kb, kb)\n        self.depth = depth\n        self.uv_bias = (1 << (depth - 1)) / ((1 << depth) - 1)\n\n    @staticmethod", "\n    @staticmethod\n    def _inv(mean, std):\n        inv_std = tuple(1 / i for i in std)\n        inv_mean = tuple(-j * i for i, j in zip(inv_std, mean))\n        return inv_mean, inv_std\n\n    def _yuv_dist(self):\n        rm, gm, bm = self.mean\n        rs, gs, bs = self.std", "        rm, gm, bm = self.mean\n        rs, gs, bs = self.std\n        kr, kg, kb = self.krgb\n\n        ym = rm * kr + gm * kg + bm * kb\n        ys = math.sqrt((rs * kr) ** 2 + (gs * kg) ** 2 + (bs * kb) ** 2)\n        um = (bm - ym) / (1 - kb) / 2 + self.uv_bias\n        us = math.sqrt(bs * bs + ys * ys) / (1 - kb) / 2\n        vm = (rm - ym) / (1 - kr) / 2 + self.uv_bias\n        vs = math.sqrt(rs * rs + ys * ys) / (1 - kr) / 2", "        vm = (rm - ym) / (1 - kr) / 2 + self.uv_bias\n        vs = math.sqrt(rs * rs + ys * ys) / (1 - kr) / 2\n        return [ym, um, vm], [ys, us, vs]\n\n    def normalize_rgb(self, rgb: torch.Tensor):\n        return self.nm(self.mean, self.std)(rgb)\n\n    def denormalize_rgb(self, rgb: torch.Tensor):\n        return self.nm(*self._inv(self.mean, self.std))(rgb)\n", "        return self.nm(*self._inv(self.mean, self.std))(rgb)\n\n    def normalize_yuv_444(self, yuv: torch.Tensor):\n        return self.nm(*self._yuv_dist())(yuv)\n\n    def denormalize_yuv_444(self, yuv: torch.Tensor):\n        return self.nm(*self._inv(*self._yuv_dist()))(yuv)\n\n    def _normalize_yuv_42x(self, y: torch.Tensor, uv: torch.Tensor, scale):\n        mean, std = self._yuv_dist()", "    def _normalize_yuv_42x(self, y: torch.Tensor, uv: torch.Tensor, scale):\n        mean, std = self._yuv_dist()\n        std[1], std[2] = std[1] * scale, std[2] * scale\n        y = self.nm(mean[0], std[0])(y)\n        uv = self.nm(mean[1:], std[1:])(uv)\n        return y, uv\n\n    def _denormalize_yuv_42x(self, y: torch.Tensor, uv: torch.Tensor, scale):\n        mean, std = self._yuv_dist()\n        std[1], std[2] = std[1] * scale, std[2] * scale", "        mean, std = self._yuv_dist()\n        std[1], std[2] = std[1] * scale, std[2] * scale\n        mean, std = self._inv(mean, std)\n        y = self.nm(mean[0], std[0])(y)\n        uv = self.nm(mean[1:], std[1:])(uv)\n        return y, uv\n\n    def normalize_yuv_422(self, y: torch.Tensor, uv: torch.Tensor):\n        return self._normalize_yuv_42x(y, uv, 1 / self.sqrt2)\n", "        return self._normalize_yuv_42x(y, uv, 1 / self.sqrt2)\n\n    def denormalize_yuv_422(self, y: torch.Tensor, uv: torch.Tensor):\n        return self._denormalize_yuv_42x(y, uv, 1 / self.sqrt2)\n\n    def normalize_yuv_420(self, y: torch.Tensor, uv: torch.Tensor):\n        return self._normalize_yuv_42x(y, uv, 1 / 2)\n\n    def denormalize_yuv_420(self, y: torch.Tensor, uv: torch.Tensor):\n        return self._denormalize_yuv_42x(y, uv, 1 / 2)", "    def denormalize_yuv_420(self, y: torch.Tensor, uv: torch.Tensor):\n        return self._denormalize_yuv_42x(y, uv, 1 / 2)\n\n\nclass converter:\n    def __init__(self, kr=0.2126, kb=0.0722, depth=8, format='yuv420', upsample_mode='bilinear'):\n        self.krgb = (kr, 1 - kr - kb, kb)\n        self.depth = depth\n        self.uv_bias = (1 << (depth - 1)) / ((1 << depth) - 1)\n        match format:", "        self.uv_bias = (1 << (depth - 1)) / ((1 << depth) - 1)\n        match format:\n            case 'yuv444':\n                self.downsample = lambda x: x\n                self.upsample = lambda x: x\n            case 'yuv422':\n                self.downsample = functools.partial(F.interpolate, scale_factor=(1, 1 / 2), mode='bilinear',\n                                                    align_corners=False)\n                self.upsample = functools.partial(F.interpolate, scale_factor=(1, 2), mode=upsample_mode,\n                                                  align_corners=False)", "                self.upsample = functools.partial(F.interpolate, scale_factor=(1, 2), mode=upsample_mode,\n                                                  align_corners=False)\n            case 'yuv420':\n                self.downsample = functools.partial(F.interpolate, scale_factor=(1 / 2, 1 / 2), mode='bilinear',\n                                                    align_corners=False)\n                self.upsample = functools.partial(F.interpolate, scale_factor=(2, 2), mode=upsample_mode,\n                                                  align_corners=False)\n\n    def rgb2yuv(self, x: torch.Tensor):\n        kr, kg, kb = self.krgb", "    def rgb2yuv(self, x: torch.Tensor):\n        kr, kg, kb = self.krgb\n\n        r, g, b = torch.chunk(x, 3, 1)\n        y = kr * r + kg * g + kb * b\n        u = (y - b) / (kb - 1) / 2 + self.uv_bias\n        v = (y - r) / (kr - 1) / 2 + self.uv_bias\n        uv = torch.cat((u, v), dim=1)\n        return y, self.downsample(uv)\n", "        return y, self.downsample(uv)\n\n    def yuv2rgb(self, y: torch.Tensor, uv: torch.Tensor):\n        kr, kg, kb = self.krgb\n\n        uv = self.upsample(uv - self.uv_bias)\n        u, v = torch.chunk(uv, 2, 1)\n        r = y + 2 * (1 - kr) * v\n        b = y + 2 * (1 - kb) * u\n        g = y - 2 * (1 - kr) * kr * v - 2 * (1 - kb) * kb * u", "        b = y + 2 * (1 - kb) * u\n        g = y - 2 * (1 - kr) * kr * v - 2 * (1 - kb) * kb * u\n        return torch.cat((r, g, b), dim=1)\n"]}
{"filename": "torch/model/cycmunet/part.py", "chunked_list": ["import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..util import cat_conv\n\n\"\"\"CycMuNet Private Network Build Block\"\"\"\n\n\nclass Pro_align(nn.Module):\n    def __init__(self, args):\n        super(Pro_align, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.conv1x1 = nn.Conv2d(self.nf * 3, self.nf, 1, 1, 0)\n        self.conv3x3 = nn.Conv2d(self.nf, self.nf, 3, 1, 1)\n        self.conv1_3x3 = nn.Conv2d(self.nf * 2, self.nf, 3, 1, 1)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def forward(self, l1, l2, l3):\n        r1 = self.lrelu(self.conv3x3(l1))\n        r2 = self.lrelu(self.conv3x3(l2))\n        r3 = self.lrelu(self.conv3x3(l3))\n        fuse = self.lrelu(cat_conv(self.conv1x1, [r1, r2, r3]))\n        r1 = self.lrelu(cat_conv(self.conv1_3x3, [r1, fuse]))\n        r2 = self.lrelu(cat_conv(self.conv1_3x3, [r2, fuse]))\n        r3 = self.lrelu(cat_conv(self.conv1_3x3, [r3, fuse]))\n        return l1 + r1, l2 + r2, l3 + r3", "class Pro_align(nn.Module):\n    def __init__(self, args):\n        super(Pro_align, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.conv1x1 = nn.Conv2d(self.nf * 3, self.nf, 1, 1, 0)\n        self.conv3x3 = nn.Conv2d(self.nf, self.nf, 3, 1, 1)\n        self.conv1_3x3 = nn.Conv2d(self.nf * 2, self.nf, 3, 1, 1)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def forward(self, l1, l2, l3):\n        r1 = self.lrelu(self.conv3x3(l1))\n        r2 = self.lrelu(self.conv3x3(l2))\n        r3 = self.lrelu(self.conv3x3(l3))\n        fuse = self.lrelu(cat_conv(self.conv1x1, [r1, r2, r3]))\n        r1 = self.lrelu(cat_conv(self.conv1_3x3, [r1, fuse]))\n        r2 = self.lrelu(cat_conv(self.conv1_3x3, [r2, fuse]))\n        r3 = self.lrelu(cat_conv(self.conv1_3x3, [r3, fuse]))\n        return l1 + r1, l2 + r2, l3 + r3", "\n\nclass SR(nn.Module):\n    def __init__(self, args):\n        super(SR, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.factor = (self.args.upscale_factor, self.args.upscale_factor)\n        self.Pro_align = Pro_align(args)\n        self.conv1x1 = nn.Conv2d(self.nf, self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def upsample(self, x):\n        x = F.interpolate(x, scale_factor=self.factor, mode='bilinear', align_corners=False)\n        return self.lrelu(self.conv1x1(x))\n\n    def forward(self, l1, l2, l3):\n        l1, l2, l3 = self.Pro_align(l1, l2, l3)\n        return tuple(self.upsample(i) for i in (l1, l2, l3))", "\n\nclass DR(nn.Module):\n    def __init__(self, args):\n        super(DR, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.factor = (1 / self.args.upscale_factor, 1 / self.args.upscale_factor)\n        self.Pro_align = Pro_align(args)\n        self.conv = nn.Conv2d(self.nf, self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def downsample(self, x):\n        x = F.interpolate(x, scale_factor=self.factor, mode='bilinear', align_corners=False)\n        return self.lrelu(self.conv(x))\n\n    def forward(self, l1, l2, l3):\n        l1 = self.downsample(l1)\n        l2 = self.downsample(l2)\n        l3 = self.downsample(l3)\n        return self.Pro_align(l1, l2, l3)", "\n\nclass Up_projection(nn.Module):\n    def __init__(self, args):\n        super(Up_projection, self).__init__()\n        self.args = args\n        self.SR = SR(args)\n        self.DR = DR(args)\n        self.SR1 = SR(args)\n\n    def forward(self, l1, l2, l3):\n        h1, h2, h3 = self.SR(l1, l2, l3)\n        d1, d2, d3 = self.DR(h1, h2, h3)\n        r1, r2, r3 = d1 - l1, d2 - l2, d3 - l3\n        s1, s2, s3 = self.SR1(r1, r2, r3)\n        return h1 + s1, h2 + s3, h3 + s3", "\n\nclass Down_projection(nn.Module):\n    def __init__(self, args):\n        super(Down_projection, self).__init__()\n        self.args = args\n        self.SR = SR(args)\n        self.DR = DR(args)\n        self.DR1 = DR(args)\n\n    def forward(self, h1, h2, h3):\n        l1, l2, l3 = self.DR(h1, h2, h3)\n        s1, s2, s3 = self.SR(l1, l2, l3)\n        r1, r2, r3 = s1 - h1, s2 - h2, s3 - h3\n        d1, d2, d3 = self.DR1(r1, r2, r3)\n        return l1 + d1, l2 + d2, l3 + d3", ""]}
{"filename": "torch/model/cycmunet/__init__.py", "chunked_list": ["from typing import Union\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..util import RGBOrYUV\n\nfrom .module import head, feature_extract, feature_fusion, mutual_cycle, feature_recon, tail\n\n\nclass CycMuNet(nn.Module):\n    def __init__(self, args):\n        super(CycMuNet, self).__init__()\n        self.args = args\n        self.factor = (self.args.upscale_factor, self.args.upscale_factor)\n        self.upsample_mode = 'bilinear'\n\n        self.head = head(args)\n        self.fe = feature_extract(args)\n        self.ff = feature_fusion(args)\n        self.mu = mutual_cycle(args)\n        self.fr = feature_recon(args)\n        self.tail = tail(args)\n\n    def merge_hf(self, lf, hf):\n        return F.interpolate(lf, scale_factor=self.factor, mode='bilinear', align_corners=False) + hf\n\n    def head_fe(self, x_or_yuv: RGBOrYUV):\n        x = self.head(x_or_yuv)\n        return self.fe(x)\n\n    def mu_fr_tail(self, lf, all_frames):\n        mu_out = self.mu(*lf, all_frames=all_frames)\n        if all_frames:\n            *hf, lf1 = mu_out\n            lf1 = self.fr(lf1)\n            hf = tuple(self.merge_hf(l, self.fr(h)) for l, h in zip(lf, hf))\n            outs = tuple(self.tail(i) for i in (*hf, lf1))\n        else:\n            outs = tuple(self.tail(self.merge_hf(l, self.fr(h))) for l, h in zip(lf, mu_out))\n        return outs\n\n    def forward_batch(self, lf0: RGBOrYUV, lf2: RGBOrYUV, all_frames=True, stop_at_conf=False):\n        lf0s, lf2s = self.head_fe(lf0), self.head_fe(lf2)\n        lf1, _ = self.ff(lf0s, lf2s)\n        if stop_at_conf:  # TODO detect frame difference and exit if too big\n            return\n        lf = (lf0s[-1], lf1, lf2s[-1])\n        return self.mu_fr_tail(lf, all_frames)\n\n    def forward_sequence(self, x_or_yuv: RGBOrYUV, all_frames=False):\n        ls = self.head_fe(x_or_yuv)\n        n = ls[0].shape[0]\n        lf1, _ = self.ff([layer[:n - 1] for layer in ls], [layer[1:] for layer in ls])\n        lf = (ls[-1][:n - 1], lf1, ls[-1][1:])\n        return self.mu_fr_tail(lf, all_frames)\n\n    # This is for symbolic tracing for sparsity\n    def pseudo_forward_sparsity(self, lf0, lf1, lf2):\n        hf0, *_ = self.mu(lf0, lf1, lf2, all_frames=True)\n        return self.fr(hf0)\n\n    def forward(self, lf0: RGBOrYUV, lf2: Union[RGBOrYUV, None] = None, sparsity_ex=None, /, batch_mode='batch',\n                **kwargs):\n        if batch_mode == '_no_use_sparsity_pseudo':\n            return self.pseudo_forward_sparsity(lf0, lf2, sparsity_ex)\n        if batch_mode == 'batch':\n            outs = self.forward_batch(lf0, lf2, **kwargs)\n        elif batch_mode == 'sequence':\n            outs = self.forward_sequence(lf0, **kwargs)\n        else:\n            raise ValueError(f\"Invalid batch_mode: {batch_mode}\")\n        return tuple(outs)", "\n\nclass CycMuNet(nn.Module):\n    def __init__(self, args):\n        super(CycMuNet, self).__init__()\n        self.args = args\n        self.factor = (self.args.upscale_factor, self.args.upscale_factor)\n        self.upsample_mode = 'bilinear'\n\n        self.head = head(args)\n        self.fe = feature_extract(args)\n        self.ff = feature_fusion(args)\n        self.mu = mutual_cycle(args)\n        self.fr = feature_recon(args)\n        self.tail = tail(args)\n\n    def merge_hf(self, lf, hf):\n        return F.interpolate(lf, scale_factor=self.factor, mode='bilinear', align_corners=False) + hf\n\n    def head_fe(self, x_or_yuv: RGBOrYUV):\n        x = self.head(x_or_yuv)\n        return self.fe(x)\n\n    def mu_fr_tail(self, lf, all_frames):\n        mu_out = self.mu(*lf, all_frames=all_frames)\n        if all_frames:\n            *hf, lf1 = mu_out\n            lf1 = self.fr(lf1)\n            hf = tuple(self.merge_hf(l, self.fr(h)) for l, h in zip(lf, hf))\n            outs = tuple(self.tail(i) for i in (*hf, lf1))\n        else:\n            outs = tuple(self.tail(self.merge_hf(l, self.fr(h))) for l, h in zip(lf, mu_out))\n        return outs\n\n    def forward_batch(self, lf0: RGBOrYUV, lf2: RGBOrYUV, all_frames=True, stop_at_conf=False):\n        lf0s, lf2s = self.head_fe(lf0), self.head_fe(lf2)\n        lf1, _ = self.ff(lf0s, lf2s)\n        if stop_at_conf:  # TODO detect frame difference and exit if too big\n            return\n        lf = (lf0s[-1], lf1, lf2s[-1])\n        return self.mu_fr_tail(lf, all_frames)\n\n    def forward_sequence(self, x_or_yuv: RGBOrYUV, all_frames=False):\n        ls = self.head_fe(x_or_yuv)\n        n = ls[0].shape[0]\n        lf1, _ = self.ff([layer[:n - 1] for layer in ls], [layer[1:] for layer in ls])\n        lf = (ls[-1][:n - 1], lf1, ls[-1][1:])\n        return self.mu_fr_tail(lf, all_frames)\n\n    # This is for symbolic tracing for sparsity\n    def pseudo_forward_sparsity(self, lf0, lf1, lf2):\n        hf0, *_ = self.mu(lf0, lf1, lf2, all_frames=True)\n        return self.fr(hf0)\n\n    def forward(self, lf0: RGBOrYUV, lf2: Union[RGBOrYUV, None] = None, sparsity_ex=None, /, batch_mode='batch',\n                **kwargs):\n        if batch_mode == '_no_use_sparsity_pseudo':\n            return self.pseudo_forward_sparsity(lf0, lf2, sparsity_ex)\n        if batch_mode == 'batch':\n            outs = self.forward_batch(lf0, lf2, **kwargs)\n        elif batch_mode == 'sequence':\n            outs = self.forward_sequence(lf0, **kwargs)\n        else:\n            raise ValueError(f\"Invalid batch_mode: {batch_mode}\")\n        return tuple(outs)", "\n\n__all__ = ['CycMuNet']\n"]}
{"filename": "torch/model/cycmunet/module.py", "chunked_list": ["from typing import Tuple, List\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..util import cat_conv\nfrom ..part import ResidualBlock_noBN, PCDLayer\n\nfrom .part import Down_projection, Up_projection\n", "from .part import Down_projection, Up_projection\n\n\"\"\"CycMuNet model partitions\"\"\"\n\n\nclass head(nn.Module):\n    def __init__(self, args):\n        super(head, self).__init__()\n        self.args = args\n        self.nf = self.args.nf", "        self.args = args\n        self.nf = self.args.nf\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n        match self.args.format:\n            case 'rgb':\n                self.conv_first = nn.Conv2d(3, self.nf, 3, 1, 1)\n                self.forward = self.forward_rgb\n            case 'yuv444':\n                self.conv_first = nn.Conv2d(3, self.nf, 3, 1, 1)", "            case 'yuv444':\n                self.conv_first = nn.Conv2d(3, self.nf, 3, 1, 1)\n                self.forward = self.forward_yuv444\n            case 'yuv422':\n                self.conv_first_y = nn.Conv2d(1, self.nf, 3, 1, 1)\n                self.conv_up = nn.ConvTranspose2d(2, self.nf, (1, 3), (1, 2), (0, 1), (0, 1))\n                self.forward = self.forward_yuv42x\n            case 'yuv420':\n                self.conv_first_y = nn.Conv2d(1, self.nf, 3, 1, 1)\n                self.conv_up = nn.ConvTranspose2d(2, self.nf, 3, 2, 1, 1)", "                self.conv_first_y = nn.Conv2d(1, self.nf, 3, 1, 1)\n                self.conv_up = nn.ConvTranspose2d(2, self.nf, 3, 2, 1, 1)\n                self.forward = self.forward_yuv42x\n            case unk:\n                raise ValueError(f'unknown input pixel format: {unk}')\n\n    def forward_rgb(self, x: torch.Tensor):\n        x = self.lrelu(self.conv_first(x))\n        return x\n", "        return x\n\n    def forward_yuv444(self, yuv: Tuple[torch.Tensor, torch.Tensor]):\n        x = torch.cat(yuv, dim=1)\n        x = self.lrelu(self.conv_first(x))\n        return x\n\n    def forward_yuv42x(self, yuv: Tuple[torch.Tensor, torch.Tensor]):\n        y, uv = yuv\n        y = self.conv_first_y(y)", "        y, uv = yuv\n        y = self.conv_first_y(y)\n        uv = self.conv_up(uv)\n        x = self.lrelu(y + uv)\n        return x\n\n\nclass feature_extract(nn.Module):\n    def __init__(self, args):\n        super(feature_extract, self).__init__()", "    def __init__(self, args):\n        super(feature_extract, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n        self.layers = self.args.layers\n        self.front_RBs = 5\n\n        self.feature_extraction = nn.Sequential(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.front_RBs)))\n        self.fea_conv1s = nn.ModuleList(nn.Conv2d(self.nf, self.nf, 3, 2, 1, bias=True) for _ in range(self.layers - 1))", "        self.feature_extraction = nn.Sequential(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.front_RBs)))\n        self.fea_conv1s = nn.ModuleList(nn.Conv2d(self.nf, self.nf, 3, 2, 1, bias=True) for _ in range(self.layers - 1))\n        self.fea_conv2s = nn.ModuleList(nn.Conv2d(self.nf, self.nf, 3, 1, 1, bias=True) for _ in range(self.layers - 1))\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def forward(self, x: torch.Tensor):\n        features: List[torch.Tensor] = [self.feature_extraction(x)]\n        for i in range(self.layers - 1):\n            feature = features[-1]\n            _, _, h, w = feature.shape", "            feature = features[-1]\n            _, _, h, w = feature.shape\n            h = torch.div(h + 1, 2, rounding_mode=\"trunc\") * 2 - h\n            w = torch.div(w + 1, 2, rounding_mode=\"trunc\") * 2 - w\n            feature = F.pad(feature, (0, w, 0, h), mode=\"replicate\")\n            feature = self.lrelu(self.fea_conv1s[i](feature))\n            feature = self.lrelu(self.fea_conv2s[i](feature))\n            features.append(feature)\n        return tuple(features[::-1])  # lowest dimension layer at first\n", "        return tuple(features[::-1])  # lowest dimension layer at first\n\n\nclass feature_fusion(nn.Module):\n    def __init__(self, args):\n        super(feature_fusion, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.groups = self.args.groups\n        self.layers = self.args.layers", "        self.groups = self.args.groups\n        self.layers = self.args.layers\n\n        # from small to big.\n        self.modules12 = nn.ModuleList(PCDLayer(args, i == 0) for i in range(self.layers))\n        self.modules21 = nn.ModuleList(PCDLayer(args, i == 0) for i in range(self.layers))\n\n        self.fusion = nn.Conv2d(2 * self.nf, self.nf, 1, 1)\n\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)", "\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    @staticmethod\n    def fuse_features(modules, f1, f2):\n        offset, feature = None, None\n        for idx, sources in enumerate(zip(f1, f2)):\n            offset, feature = modules[idx](sources, offset, feature)\n        return feature\n", "        return feature\n\n    def forward(self, f1, f2):\n        feature1 = self.fuse_features(self.modules12, f1, f2)\n        feature2 = self.fuse_features(self.modules21, f2, f1)\n        fused_feature = cat_conv(self.fusion, (feature1, feature2))\n        return fused_feature, None\n\n\nclass mutual_cycle(nn.Module):", "\nclass mutual_cycle(nn.Module):\n    def __init__(self, args):\n        super(mutual_cycle, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        self.cycle_count = self.args.cycle_count\n\n        self.merge = nn.ModuleList(nn.Conv2d(64 * (i + 1), 64, 1, 1, 0) for i in range(self.cycle_count))\n        self.merge1 = nn.ModuleList(nn.Conv2d(64 * (i + 1), 64, 1, 1, 0) for i in range(self.cycle_count))", "        self.merge = nn.ModuleList(nn.Conv2d(64 * (i + 1), 64, 1, 1, 0) for i in range(self.cycle_count))\n        self.merge1 = nn.ModuleList(nn.Conv2d(64 * (i + 1), 64, 1, 1, 0) for i in range(self.cycle_count))\n\n        self.down = nn.ModuleList(Down_projection(args) for _ in range(self.cycle_count))\n        self.up = nn.ModuleList(Up_projection(args) for _ in range(self.cycle_count + 1))\n\n        self.conv = nn.Conv2d(self.nf * (2 * self.cycle_count + 1), self.nf, 1, 1, 0)\n        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\n    def forward(self, lf0, lf1, lf2, all_frames=False):", "\n    def forward(self, lf0, lf1, lf2, all_frames=False):\n        assert self.cycle_count > 0\n\n        l_out, h_out = [(lf0, lf1, lf2)], []\n        for j in range(self.cycle_count):\n            l_feats = tuple(self.lrelu(cat_conv(self.merge[j], frame_outs)) for frame_outs in zip(*l_out))\n            h_feat = self.up[j](*l_feats)\n            h_out.append(h_feat)\n            h_feats = tuple(self.lrelu(cat_conv(self.merge1[j], frame_outs)) for frame_outs in zip(*h_out))", "            h_out.append(h_feat)\n            h_feats = tuple(self.lrelu(cat_conv(self.merge1[j], frame_outs)) for frame_outs in zip(*h_out))\n            l_feat = self.down[j](*h_feats)\n            l_out.append(l_feat)\n\n        lf_out, hf_out = [l_out[-1]], []\n        for j in range(self.cycle_count):\n            l_feats = tuple(self.lrelu(cat_conv(self.merge[j], frame_outs)) for frame_outs in zip(*lf_out))\n            h_feat = self.up[j](*l_feats)\n            hf_out.append(h_feat)", "            h_feat = self.up[j](*l_feats)\n            hf_out.append(h_feat)\n            l_feat = self.down[j](*h_feat)\n            lf_out.append(l_feat)\n        hf_out.append(self.up[self.cycle_count](*l_feats))\n\n        if all_frames:\n            h_outs = zip(*h_out, *hf_out)  # packed 3 frames\n            _, l1_out, _ = zip(*l_out, *lf_out[1:])\n", "            _, l1_out, _ = zip(*l_out, *lf_out[1:])\n\n            h_outs = tuple(self.lrelu(cat_conv(self.conv, h_frame)) for h_frame in h_outs)\n            l1_out = self.lrelu(cat_conv(self.conv, l1_out))\n            return *h_outs, l1_out\n        else:\n            h1_out, h2_out, _ = zip(*h_out, *hf_out)\n            h1_out = self.lrelu(cat_conv(self.conv, h1_out))\n            h2_out = self.lrelu(cat_conv(self.conv, h2_out))\n", "            h2_out = self.lrelu(cat_conv(self.conv, h2_out))\n\n            return h1_out, h2_out\n\n\nclass feature_recon(nn.Module):\n    def __init__(self, args):\n        super(feature_recon, self).__init__()\n        self.args = args\n        self.nf = self.args.nf", "        self.args = args\n        self.nf = self.args.nf\n        self.back_RBs = 40\n        self.factor = (self.args.upscale_factor, self.args.upscale_factor)\n\n        self.recon_trunk = nn.Sequential(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.back_RBs)))\n\n    def forward(self, x):\n        out = self.recon_trunk(x)\n        return out", "        out = self.recon_trunk(x)\n        return out\n\n\nclass tail(nn.Module):\n    def __init__(self, args):\n        super(tail, self).__init__()\n        self.args = args\n        self.nf = self.args.nf\n        match self.args.format:", "        self.nf = self.args.nf\n        match self.args.format:\n            case 'rgb':\n                self.conv_last2 = nn.Conv2d(self.nf, 3, 3, 1, 1)\n                self.forward = self.forward_rgb\n            case 'yuv444':\n                self.conv_last2 = nn.Conv2d(self.nf, 3, 3, 1, 1)\n                self.forward = self.forward_yuv444\n            case 'yuv422':\n                self.conv_last_y = nn.Conv2d(self.nf, 1, 3, 1, 1)", "            case 'yuv422':\n                self.conv_last_y = nn.Conv2d(self.nf, 1, 3, 1, 1)\n                self.conv_last_uv = nn.Conv2d(self.nf, 2, (1, 3), (1, 2), (0, 1))\n                self.forward = self.forward_yuv42x\n            case 'yuv420':\n                self.conv_last_y = nn.Conv2d(self.nf, 1, 3, 1, 1)\n                self.conv_last_uv = nn.Conv2d(self.nf, 2, 3, 2, 1)\n                self.forward = self.forward_yuv42x\n            case unk:\n                raise ValueError(f'unknown input pixel format: {unk}')", "            case unk:\n                raise ValueError(f'unknown input pixel format: {unk}')\n\n    def forward_rgb(self, x):\n        out = self.conv_last2(x)\n        return out,\n\n    def forward_yuv444(self, x):\n        out = self.conv_last2(x)\n        return out[:, :1, ...], out[:, 1:, ...]", "        out = self.conv_last2(x)\n        return out[:, :1, ...], out[:, 1:, ...]\n\n    def forward_yuv42x(self, x):\n        y = self.conv_last_y(x)\n        uv = self.conv_last_uv(x)\n        return y, uv\n"]}
{"filename": "torch/dataset/sequence.py", "chunked_list": ["import glob\nimport itertools\nimport pathlib\nimport random\nfrom typing import List\n\nimport torch.utils.data as data\nimport numpy as np\nimport torchvision.transforms\nfrom PIL import Image, ImageFilter", "import torchvision.transforms\nfrom PIL import Image, ImageFilter\n\n\nclass ImageSequenceDataset(data.Dataset):\n    want_shuffle = True\n    pix_type = 'rgb'\n\n    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n        self.dataset_base = pathlib.Path(index_file).parent\n        self.sequences = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n                          if i if not i.startswith('#')]\n        self.patch_size = patch_size\n        self.scale_factor = scale_factor\n        self.augment = augment\n        self.rand = random.Random(seed)\n        self.transform = torchvision.transforms.ToTensor()\n\n    def _load_sequence(self, path):\n        path = self.dataset_base / \"sequences\" / path\n        files = glob.glob(\"*.png\", root_dir=path)\n        assert len(files) > 1\n        images = [Image.open(file) for file in files]\n        if not all(i.size != images[0].size for i in images[1:]):\n            raise ValueError(\"sequence has different dimensions\")\n        return images\n\n    def _prepare_images(self, images: List[Image.Image]):\n        w, h = images[0].size\n        f = self.scale_factor\n        sw, sh = self.patch_size\n        sw, sh = sw * f, sh * f\n        assert h >= sh and w >= sw\n        dh, dw = self.rand.randint(0, h - sh), self.rand.randint(0, w - sw)\n        images = [i.crop((dw, dh, dw + sw, dh + sh)) for i in images]\n        return images\n\n    trans_groups = {\n        'none': [None],\n        'rotate': [None, Image.ROTATE_90, Image.ROTATE_180, Image.ROTATE_270],\n        'mirror': [None, Image.FLIP_LEFT_RIGHT],\n        'flip': [None, Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM, Image.ROTATE_180],\n        'all': [None] + [e.value for e in Image.Transpose],\n    }\n\n    trans_names = [e.name for e in Image.Transpose]\n\n    def _augment_images(self, images: List[Image.Image], trans_mode='all'):\n        trans_action = 'none'\n        trans_op = self.rand.choice(self.trans_groups[trans_mode])\n        if trans_op is not None:\n            images = [i.transpose(trans_op) for i in images]\n            trans_action = self.trans_names[trans_op]\n        return images, trans_action\n\n    scale_filters = [Image.BILINEAR, Image.BICUBIC, Image.LANCZOS]\n\n    def _scale_images(self, images: List[Image.Image]):\n        f = self.scale_factor\n        return [i.resize((i.width // f, i.height // f), self.rand.choice(self.scale_filters)) for i in images]\n\n    def _degrade_images(self, images: List[Image.Image]):\n        degrade_action = None\n        decision = self.rand.randrange(4)\n        if decision == 1:\n            degrade_action = 'box'\n            percent = 0.5 + 0.5 * self.rand.random()\n            images = [Image.blend(j, j.copy().filter(ImageFilter.BoxBlur(1)), percent) for j in images]\n        elif decision == 2:\n            degrade_action = 'gaussian'\n            radius = self.rand.random()\n            images = [j.filter(ImageFilter.GaussianBlur(radius)) for j in images]\n        elif decision == 3:\n            degrade_action = 'halo'\n            percent = 0.5 + 0.5 * self.rand.random()\n            images = [Image.blend(i,\n                                  i.resize((i.width // 2, i.height // 2), resample=Image.LANCZOS)\n                                  .resize(i.size, resample=Image.BILINEAR), percent)\n                      for i in images]\n\n        return images, degrade_action\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence = self._load_sequence(self.sequences[idx])\n        sequence = self._prepare_images(sequence)  # crop to requested size\n        original, _ = self._augment_images(sequence)  # flip and rotates\n        lfs_pred = [np.array(lf.resize((lf.width // self.scale_factor, lf.height // self.scale_factor), Image.LANCZOS))\n                    for lf in original[1::2]]\n        lfs_deg = self._scale_images(original[::2])\n        # lfs_deg, _ = self._degrade_images(lfs_deg)\n        degraded = [i for i in itertools.zip_longest(lfs_deg, lfs_pred) if i is not None]\n        original = [self.transform(i) for i in original]\n        degraded = [self.transform(i) for i in degraded]\n        return original, degraded", ""]}
{"filename": "torch/dataset/__init__.py", "chunked_list": ["from .sequence import ImageSequenceDataset\nfrom .video import VideoFrameDataset\nfrom .util import InterleavedDataset\n"]}
{"filename": "torch/dataset/util.py", "chunked_list": ["import bisect\n\nimport torch.utils.data as data\n\n\nclass InterleavedDataset(data.Dataset):\n    def __init__(self, *datasets: data.Dataset):\n        self.datasets = datasets\n        if not all(hasattr(i, '__len__') for i in datasets):\n            raise AttributeError('need datasets with known length')\n        sizes = [len(i) for i in datasets]\n        self.total = sum(sizes)\n        self.sizes = [0] + sorted(set(sizes))\n        self.index = {\n            0: datasets\n        }\n        total, last_n = 0, len(datasets)\n        for last_size, size in zip(self.sizes, self.sizes[1:]):\n            total += (size - last_size) * last_n\n            this_datasets = [ds for ds in datasets if len(ds) > size]\n            self.index[total] = this_datasets\n            last_n = len(this_datasets)\n        self.index.popitem()\n        self.index_keys = list(self.index.keys())\n\n    def __len__(self):\n        return self.total\n\n    def __getitem__(self, idx):\n        stage = bisect.bisect_right(self.index_keys, idx) - 1\n        offset = self.sizes[stage]\n        begin = self.index_keys[stage]\n        idx -= begin\n        datasets = self.index[begin]\n        n, i = idx % len(datasets), idx // len(datasets)\n        return datasets[n][offset + i]", ""]}
{"filename": "torch/dataset/video.py", "chunked_list": ["import bisect\nimport collections\nimport functools\nimport itertools\nimport pathlib\nimport random\nfrom typing import List, Tuple\n\nimport av\nimport av.logging", "import av\nimport av.logging\nimport numpy as np\nimport cv2\nimport torch\nimport torch.utils.data as data\n\n\nav.logging.set_level(av.logging.FATAL)\n", "av.logging.set_level(av.logging.FATAL)\n\n\nclass Video:\n    def __init__(self, file, kf):\n        self.container = av.open(file)\n        self.stream = self.container.streams.video[0]\n        self.stream.thread_type = \"AUTO\"\n        self.at = 0\n        self.kf = kf\n\n    def get_frames(self, pts, n=1):\n        frames = []\n        if bisect.bisect_left(self.kf, pts) != bisect.bisect_left(self.kf, self.at) or pts <= self.at:\n            self.container.seek(pts, stream=self.stream)\n        found = False\n        for frame in self.container.decode(video=0):\n            if not found and frame.pts != pts:\n                continue\n            found = True\n            self.at = frame.pts\n            yuv = frame.to_ndarray()\n            h, w = frame.height, frame.width\n            y, uv = yuv[:h, :].reshape(1, h, w), yuv[h:, :].reshape(2, h // 2, w // 2)\n            frames.append((y, uv))\n            if len(frames) == n:\n                return frames\n        raise ValueError(\"unexpected end\")\n\n    def __del__(self):\n        self.container.close()", "\n\nvideo_info = collections.namedtuple('video_info', [\n    'org',\n    'deg',\n    'frames',\n    'pts_org',\n    'pts_deg',\n    'key_org',\n    'key_deg'", "    'key_org',\n    'key_deg'\n])\n\n\nclass VideoFrameDataset(data.Dataset):\n    want_shuffle = False\n    pix_type = 'yuv'\n\n    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n        self.dataset_base = pathlib.PurePath(index_file).parent\n        index_lines = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n                       if i if not i.startswith('#')]\n        files = [tuple(i.split(',')) for i in index_lines]\n        self.files = []\n        self.indexes = []\n        for org, deg, frames, pts_org, pts_deg, key_org, key_deg in files:\n            info = video_info(\n                org,\n                deg,\n                int(frames),\n                tuple(int(i) for i in pts_org.split(' ')),\n                tuple(int(i) for i in pts_deg.split(' ')),\n                tuple(int(i) for i in key_org.split(' ')),\n                tuple(int(i) for i in key_deg.split(' ')),\n            )\n            self.files.append(info)\n            self.indexes.append(info.frames)\n        self.indexes = list(itertools.accumulate(self.indexes))\n        self.patch_size = (patch_size, patch_size) if isinstance(patch_size, int) else patch_size\n        self.scale_factor = scale_factor\n        self.augment = augment\n        self.rand = random.Random(seed)\n\n    @staticmethod\n    def transform(yuv):\n        return tuple(torch.from_numpy(i).contiguous().to(dtype=torch.float32).div(255) for i in yuv)\n\n    @functools.lru_cache(2)\n    def get_video(self, v_idx):\n        info = self.files[v_idx]\n        return Video(str(self.dataset_base / info.org), info.key_org), \\\n            Video(str(self.dataset_base / info.deg), info.key_deg), info.pts_org, info.pts_deg\n\n    def _augment_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n        if self.rand.random() > 0.5:\n            org = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in org]\n            deg = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in deg]\n        return org, deg\n\n    def _prepare_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n        _, h, w = deg[0][0].shape\n        sw, sh = self.patch_size\n        sh_uv, sw_uv = sh // 2, sw // 2\n        assert h >= sh and w >= sw\n        dh, dw = self.rand.randrange(0, h - sh + 2, 2), self.rand.randrange(0, w - sw + 2, 2)\n        dh_uv, dw_uv = dh // 2, dw // 2\n        deg = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in deg]\n        f = self.scale_factor\n        size, size_uv = (sw, sh), (sw_uv, sh_uv)\n        sh, sw, sh_uv, sw_uv = sh * f, sw * f, sh_uv * f, sw_uv * f\n        dh, dw, dh_uv, dw_uv = dh * f, dw * f, dh_uv * f, dw_uv * f\n        org = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in org]\n\n        deg1_y = cv2.resize(org[1][0][0], size, interpolation=cv2.INTER_LANCZOS4)\n        deg1_u = cv2.resize(org[1][1][0], size_uv, interpolation=cv2.INTER_LANCZOS4)\n        deg1_v = cv2.resize(org[1][1][1], size_uv, interpolation=cv2.INTER_LANCZOS4)\n        deg.insert(1, (deg1_y.reshape((1, *size[::-1])), np.stack((deg1_u, deg1_v)).reshape((2, *size_uv[::-1]))))\n        return org, deg\n\n    def __len__(self):\n        return self.indexes[-1]\n\n    def __getitem__(self, idx):\n        v_idx = bisect.bisect_right(self.indexes, idx)\n        f_idx = idx if v_idx == 0 else idx - self.indexes[v_idx - 1]\n        org, deg, pts_org, pts_deg = self.get_video(v_idx)\n        org_frames = org.get_frames(pts_org[f_idx], 3)\n        deg_frames = deg.get_frames(pts_deg[f_idx], 3)\n        deg_frames.pop(1)\n        org_frames, deg_frames = self._prepare_frame(org_frames, deg_frames)\n        if self.augment:\n            org_frames, deg_frames = self._augment_frame(org_frames, deg_frames)\n        org_frames = [self.transform(i) for i in org_frames]\n        deg_frames = [self.transform(i) for i in deg_frames]\n        return org_frames, deg_frames", ""]}
