{"filename": "src/tracking_model.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'''\n1. encode image1 and image2 to get 2D features\n2. sample features at seg_point1 and seg_point2", "1. encode image1 and image2 to get 2D features\n2. sample features at seg_point1 and seg_point2\n3. cross attention between two feature vectors\n4. output normalized correspondence id\n'''\n\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n", "import cv2\n\nfrom src.tracking_utils import generate_pos_emb, get_adj_ind, abs2rel, bilinear_sampler_1d, cnt2poly, cost_volume_at_contour_points, normalize_1d_features\n\n\nclass PointSetTracker(tf.Module):\n    def __init__(self, num_iter=5, input_image_size=(None, None, 3)):\n        super(PointSetTracker, self).__init__()\n        self.num_iter = num_iter\n        self.local_alignment = LocalAlignment(num_iter=num_iter)\n\n    def initialize(self, cnt):\n        B, N = cnt.shape[:2]\n\n        pos_emb = generate_pos_emb(N)\n        pos_emb = tf.expand_dims(pos_emb, axis=0)\n        pos_emb = tf.broadcast_to(pos_emb, [B, N, 2])  # copy the same pos_emb and paste to all batch indices\n\n        return pos_emb\n\n    def propagate(self, x0, x1, orig_seg_point1, orig_seg_point2, pos_emb):\n        '''\n\n        :param x0: First Image\n        :param x1: Second Image\n        :param pos_emb: positional embedding from the number of contour points\n        :return:\n        '''\n\n        size = x0.shape[-2:-4:-1]  # Width, Height\n        seg_point1 = abs2rel(orig_seg_point1, size)\n        seg_point2 = abs2rel(orig_seg_point2, size)\n\n        seg_point_mask1 = orig_seg_point1[:, :, 2:]\n        seg_point_mask2 = orig_seg_point2[:, :, 2:]\n\n        forward_spatial_offset, backward_spatial_offset, saved_offset = self.local_alignment(x0, x1, seg_point1, seg_point2, pos_emb, seg_point_mask1, seg_point_mask2)\n\n        return forward_spatial_offset, backward_spatial_offset, saved_offset\n\n    def __call__(self, *args, **kwargs):\n        kwargs.pop('training', None)  # because kwargs has training Keyword\n        in_len = len(args) + len(kwargs)\n        if in_len == 1:\n            return self.initialize(*args, **kwargs)\n        elif in_len == 5:\n            return self.propagate(*args, **kwargs)\n        else:\n            raise ValueError(f'input length of {in_len} is not supported')", "\n\nclass LocalAlignment(tf.keras.Model):\n    def __init__(self, num_iter=5, adj_num=4):\n        super(LocalAlignment, self).__init__()\n        self.num_iter = num_iter\n        self.adj_num = adj_num\n\n        # ------------------------ Post Implementation ----------------------------\n        # self.encoder = PostEncoder(out_dim=128)\n        # self.align_module = LAM(feature_dim=128 + 4, state_dim=128, output_num=2)\n        # -------------------------------------------------------------------------\n        self.encoder = LocalEncoder(out_dim=128)\n        \n        # self.spatial_offset_module = LAM(feature_dim=128 + 4, state_dim=128, output_num=2)\n\n        # Circular convoltuion\n        # self.spatial_offset_module = tf.keras.Sequential([\n        #     CircConv(256),\n        #     tf.keras.layers.ReLU(),\n        #     CircConv(64),\n        #     tf.keras.layers.ReLU(),\n        #     CircConv(2)\n        # ])\n\n        # 1D convolution\n        # self.spatial_offset_module = tf.keras.Sequential([\n        #     LinearConv(256),\n        #     tf.keras.layers.ReLU(),\n        #     LinearConv(64),\n        #     tf.keras.layers.ReLU(),\n        #     LinearConv(2)\n        # ])\n\n        # Dense layers\n        self.spatial_offset_module = tf.keras.Sequential([\n            tf.keras.layers.Dense(256),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.Dense(64),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.Dense(2)\n        ])\n\n        # tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=64, value_dim=64)\n        self.cross_attn_layer_forward = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=128, value_dim=128)\n        self.cross_attn_layer_backward = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=128, value_dim=128)\n\n    def call(self, x0, x1, seg_point1, seg_point2, pos_emb, seg_point_mask1, seg_point_mask2):\n        '''\n\n        :param x0: original first feature (B, H, W, C)\n        :param x1: orignal second feature (B, H, W, C)\n        :param cnt0: contour of the first image (B, N, C) Points are in Width and Height order, unlike the image\n        :param pos_emb: (B, N, 2)\n\n        :param seg_point1_limit and seg_point2_limit\n        During inference, they are used to remove all negative values that are padded in the data loading procedure to obtain consistent shape\n        During training, they remove most of the negative values that are padded\n\n        x1_h : transformed first image to second image\n        cnt1_h: transformed first contour to second contour\n\n        :return:\n        '''\n        saved_offset = {}  # key for layer number\n\n        # ----------------------------------------------\n        # single level, PoST\n        # features = self.encoder(x0, x1)\n        # a_feature = features[-1]\n        \n        # forward_sampled_cost_volume = cost_volume_at_contour_points(a_feature, a_feature, seg_point1, seg_point2, max_displacement=1)\n        # backward_sampled_cost_volume = cost_volume_at_contour_points(a_feature, a_feature, seg_point2, seg_point1, max_displacement=1)\n        \n        # poly1 = cnt2poly(seg_point1)  # It represents the contour points' coordinates on +image space. shape is B, N, 2\n        # m_in1 = tf.concat((forward_sampled_cost_volume, pos_emb, poly1), axis=-1)  # m_in shape is (B, N, C+4)\n        # poly2 = cnt2poly(seg_point2)  # It represents the contour points' coordinates on +image space. shape is B, N, 2\n        # m_in2 = tf.concat((backward_sampled_cost_volume, pos_emb, poly2), axis=-1)  # m_in shape is (B, N, C+4)\n        \n        # N = seg_point1.shape[1]\n        # adj = get_adj_ind(self.adj_num, N)\n        # forward_spatial_offset = self.align_module(m_in1) # adj\n        # backward_spatial_offset = self.align_module(m_in2)\n        \n        # saved_offset[0] = forward_spatial_offset\n\n        # ----------------------------------------------\n        # single level, OURS\n\n        features1 = self.encoder(x0)\n        features2 = self.encoder(x1)\n        a_feature1 = features1[-1]\n        a_feature2 = features2[-1]\n\n        # TODO: check if bilinear_sampler_1d is necessary since it's sampling at exact 2d coordinates, not floating point\n        sampled_feature1 = bilinear_sampler_1d(a_feature1, seg_point1[:, :, 0], seg_point1[:, :, 1])  # B x N x c\n        normalized_sampled_feature1 = normalize_1d_features(sampled_feature1)\n        poly1 = cnt2poly(seg_point1)  # It represents the contour points' coordinates on +image space. shape is B, N, 2\n        concat_sampled_features1 = tf.concat((normalized_sampled_feature1, pos_emb, poly1),\n                                             axis=-1)  # m_in shape is (B, N, C+4)\n\n        sampled_feature2 = bilinear_sampler_1d(a_feature2, seg_point2[:, :, 0], seg_point2[:, :, 1])  # B x N x c\n        normalized_sampled_feature2 = normalize_1d_features(sampled_feature2)\n        poly2 = cnt2poly(seg_point2)  # It represents the contour points' coordinates on +image space. shape is B, N, 2\n        concat_sampled_features2 = tf.concat((normalized_sampled_feature2, pos_emb, poly2),\n                                             axis=-1)  # m_in shape is (B, N, C+4)\n\n        # cross_attn_tensor shape is ([batch_size, target's num_points, 132])\n        # cross_attn_scores shape is ([batch_size, num_heads, target's num_points, source's num_points])\n        # implement forward and backward attention\n        # t_seg_point_mask1 = tf.transpose(seg_point_mask1, perm=[0,2,1])  # For each target index i, [0,i,:] gives 40 indices with 1 mask\n        # t_seg_point_mask2 = tf.transpose(seg_point_mask2, perm=[0,2,1])\n\n        # forward and backward cross attentions\n        forward_cross_attn, forward_cross_attn_scores = self.cross_attn_layer_forward(concat_sampled_features1, concat_sampled_features2, return_attention_scores=True)  # target (query), source(key)\n        backward_cross_attn, backward_cross_attn_scores = self.cross_attn_layer_backward(concat_sampled_features2, concat_sampled_features1, return_attention_scores=True)  # target, source\n        \n        # single cross attention\n        # forward_cross_attn, forward_cross_attn_scores = self.cross_attn_layer_forward(concat_sampled_features1, concat_sampled_features2, return_attention_scores=True)  # target (query), source(key)\n        # backward_cross_attn, backward_cross_attn_scores = self.cross_attn_layer_forward(concat_sampled_features2, concat_sampled_features1, return_attention_scores=True)  # target (query), source(key)\n        \n        # no cross attention is used\n        # forward_cross_attn = concat_sampled_features1 + concat_sampled_features2\n        # backward_cross_attn = forward_cross_attn\n\n        forward_spatial_offset = self.spatial_offset_module(forward_cross_attn)\n        backward_spatial_offset = self.spatial_offset_module(backward_cross_attn)  # shape=(1, 1150, 2), dtype=float32\n        saved_offset[0] = forward_spatial_offset\n\n        return forward_spatial_offset, backward_spatial_offset, saved_offset", "\n\nclass LAM(tf.keras.layers.Layer):\n    def __init__(self, feature_dim, state_dim, output_num):\n        super(LAM, self).__init__()\n        self.head = BasicBlock(feature_dim, state_dim)\n\n        self.dilation_list = [1, 1, 1]  # [1,1,1] # [1, 1, 1, 2, 2, 4, 4]\n        self.block_list = []\n        for i in range(len(self.dilation_list)):\n            a_basic_block = BasicBlock(state_dim, state_dim, dilation=self.dilation_list[i])\n            self.block_list.append(a_basic_block)\n\n        fusion_state_dim = 256\n        self.fusion = tf.keras.layers.Conv1D(filters=fusion_state_dim, kernel_size=1)\n\n        # conv1d with kernel=1 is equivalent to linear layer\n        self.prediction = tf.keras.Sequential([\n            tf.keras.layers.Conv1D(filters=256, kernel_size=1),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.Conv1D(filters=64, kernel_size=1),\n            tf.keras.layers.ReLU(),\n            tf.keras.layers.Conv1D(filters=output_num, kernel_size=1)\n        ])\n\n    def call(self, input_x):\n        '''\n        Architecture Overview:\n            Circular Block: 8 circular conv, bn, relu\n            Fusion: 1x1 conv, max pooling\n            LSTM\n            Prediction: 1x1 conv relu, 1x1 conv relu, 1x1 conv\n\n        :param x: interpolated image feature at points + position embeddings of points (N, # of points, C)\n        :param adj:\n        :return:\n        '''\n        states = []\n        x = self.head(input_x)  # B, N, C\n\n        states.append(x)\n        for i in range(len(self.dilation_list)):\n            # x = self.__getattribute__('res' + str(i))(x, adj) + x\n            x = self.block_list[i](x) + x\n            states.append(x)\n        state = tf.concat(states, axis=-1)\n\n        global_state = tf.math.reduce_max(self.fusion(state), axis=1, keepdims=True)\n        global_state = tf.broadcast_to(global_state, [global_state.shape[0], tf.shape(state)[1], global_state.shape[2]])  # B, N, C\n        state = tf.concat([global_state, state], axis=-1)  # state output: (1, 128, 1024), global_state: (1, 128, 256)\n\n        x = self.prediction(state)  # B, N, 1\n\n        return x", "\nclass BasicBlock(tf.keras.layers.Layer):\n    def __init__(self, state_dim, out_state_dim, n_adj=4, dilation=1):\n        super(BasicBlock, self).__init__()\n        self.dense_layer = tf.keras.layers.Dense(out_state_dim)\n        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=state_dim)  # key_dim defines the hiddne_dim/output_dim of dense layer\n        self.circ_conv = CircConv(state_dim, out_state_dim, n_adj, dilation)\n        self.relu = tf.keras.layers.ReLU()\n\n    def call(self, x):\n        x = self.mha(query=x, value=x, key=x)\n        x = self.circ_conv(x)\n        x = self.relu(x)\n\n        return x", "\n\nclass LinearConv(tf.keras.layers.Layer):\n    '''\n    Refered to https://www.tensorflow.org/tutorials/customization/custom_layers\n    https://www.tensorflow.org/guide/keras/custom_layers_and_models\n    https://www.tensorflow.org/guide/intro_to_modules\n\n    convert first channel order in Pytorch to last channel order in tensorflow\n    '''\n\n    def __init__(self, state_dim, out_state_dim=None, n_adj=4, dilation=1):\n        super(LinearConv, self).__init__()\n\n        self.n_adj = n_adj\n        self.dilation = dilation\n        out_state_dim = state_dim if out_state_dim is None else out_state_dim\n        self.fc = tf.keras.layers.Conv1D(filters=out_state_dim, strides=1, kernel_size=self.n_adj * 2 + 1, dilation_rate=self.dilation)\n\n    def call(self, input):\n        # adj is not used\n        if self.n_adj != 0:\n            pad_zeros = tf.zeros_like(input[:, -self.n_adj * self.dilation:, :])\n            pad_zeros_right = tf.zeros_like(input[:, :self.n_adj * self.dilation, :])\n\n            input = tf.concat([pad_zeros, input, pad_zeros_right], axis=1)\n\n        return self.fc(input)", "\n\n\nclass CircConv(tf.keras.layers.Layer):\n    '''\n    Refered to https://www.tensorflow.org/tutorials/customization/custom_layers\n    https://www.tensorflow.org/guide/keras/custom_layers_and_models\n    https://www.tensorflow.org/guide/intro_to_modules\n\n    convert first channel order in Pytorch to last channel order in tensorflow\n    '''\n\n    def __init__(self, state_dim, out_state_dim=None, n_adj=4, dilation=1):\n        super(CircConv, self).__init__()\n\n        self.n_adj = n_adj\n        self.dilation = dilation\n        out_state_dim = state_dim if out_state_dim is None else out_state_dim\n        self.fc = tf.keras.layers.Conv1D(filters=out_state_dim, strides=1, kernel_size=self.n_adj * 2 + 1, dilation_rate=self.dilation)\n\n    def call(self, input):\n        # adj is not used\n        if self.n_adj != 0:\n            input = tf.concat([input[:, -self.n_adj * self.dilation:, :], input, input[:, :self.n_adj * self.dilation, :]], axis=1)\n\n        return self.fc(input)", "\n\nclass VGG16(tf.keras.layers.Layer):\n    def __init__(self):\n        super(VGG16, self).__init__()\n        a_model = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=[None, None, 3], pooling=None)\n\n        self.conv1_pad = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv1_pad')\n\n        init_kernel = a_model.get_layer('block1_conv1').weights[0]\n        init_bias = a_model.get_layer('block1_conv1').bias\n        kernel_initializer = tf.keras.initializers.constant(init_kernel)\n        bias_initializer = tf.keras.initializers.constant(init_bias)\n\n        self.conv_layer_x0 = tf.keras.layers.Conv2D(64, 3,\n                                                    input_shape=(None, None, 3),\n                                                    kernel_initializer=kernel_initializer,\n                                                    bias_initializer=bias_initializer)\n\n        self.conv_layer_x1 = tf.keras.layers.Conv2D(64, 3,\n                                                    input_shape=(None, None, 3),\n                                                    kernel_initializer=kernel_initializer,\n                                                    bias_initializer=bias_initializer)\n        self.conv_layer_m1_h = tf.keras.layers.Conv2D(64, 3, input_shape=(None, None, 1))\n\n        # Methods to create new models by sequential model or function API from keras.applications\n        # refer to https://stackoverflow.com/questions/67176547/how-to-remove-first-n-layers-from-a-keras-model\n        # https://stackoverflow.com/questions/49546922/keras-replacing-input-layer\n\n        # remove two intermediate layers, create a new input layer, and set 5 outputs\n        model_config = a_model.get_config()\n\n        del model_config['layers'][1]\n        model_config['layers'][0] = {\n            'name': 'new_input',\n            'class_name': 'InputLayer',\n            'config': {\n                'batch_input_shape': a_model.layers[2].input_shape,\n                'dtype': 'float32',\n                'sparse': False,\n                'name': 'new_input'\n            },\n            'inbound_nodes': []\n        }\n        model_config['layers'][1]['inbound_nodes'] = [[['new_input', 0, 0, {}]]]\n        model_config['input_layers'] = [['new_input', 0, 0]]\n        model_config['output_layers'] = [['block1_conv2', 0, 0],\n                                         ['block2_conv2', 0, 0],\n                                         ['block3_conv3', 0, 0],\n                                         ['block4_conv3', 0, 0],\n                                         ['block5_conv3', 0, 0]]\n\n        self.modified_model = a_model.__class__.from_config(model_config, custom_objects={})\n\n        weights = [layer.get_weights() for layer in a_model.layers[2:]]\n        for layer, weight in zip(self.modified_model.layers[1:], weights):\n            layer.set_weights(weight)\n\n    def call(self, x0):\n        # Combine features from the first image, second image, and transformed mask\n        x0 = self.conv1_pad(x0)\n        conv_x0 = self.conv_layer_x0(x0)\n\n        c1, c2, c3, c4, c5 = self.modified_model(conv_x0)\n\n        # the order of features are from coarse to fine !!!\n        return c5, c4, c3, c2, c1", "\n\nclass ResNet50(tf.keras.layers.Layer):\n    def __init__(self):\n        super(ResNet50, self).__init__()\n        resnet = tf.keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=[None, None, 3], pooling=None)\n\n        self.conv1_pad = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv1_pad')\n        self.conv_layer_x0 = tf.keras.layers.Conv2D(64, 3,\n                                                    activation='relu',\n                                                    input_shape=(None, None, 3))\n        self.conv_layer_x1 = tf.keras.layers.Conv2D(64, 3,\n                                                    activation='relu',\n                                                    input_shape=(None, None, 3))\n\n        # Methods to create new models by sequential model or function API from keras.applications Model are hard to use because ResNet has layers with multiple input or outputs\n        # refer to https://stackoverflow.com/questions/67176547/how-to-remove-first-n-layers-from-a-keras-model\n        # https://stackoverflow.com/questions/49546922/keras-replacing-input-layer\n        # remove two intermidate layers, create a new input layer, and set 5 outputs\n        model_config = resnet.get_config()\n        del model_config['layers'][1:3]\n        model_config['layers'][0] = {\n            'name': 'new_input',\n            'class_name': 'InputLayer',\n            'config': {\n                'batch_input_shape': resnet.layers[3].input_shape,\n                'dtype': 'float32',\n                'sparse': False,\n                'name': 'new_input'\n            },\n            'inbound_nodes': []\n        }\n        model_config['layers'][1]['inbound_nodes'] = [[['new_input', 0, 0, {}]]]\n        model_config['input_layers'] = [['new_input', 0, 0]]\n        model_config['output_layers'] = [['conv1_relu', 0, 0],\n                                         ['conv2_block3_out', 0, 0],\n                                         ['conv3_block4_out', 0, 0],\n                                         ['conv4_block6_out', 0, 0],\n                                         ['conv5_block3_out', 0, 0]]\n        self.modified_resnet = resnet.__class__.from_config(model_config, custom_objects={})\n        weights = [layer.get_weights() for layer in resnet.layers[3:]]\n        for layer, weight in zip(self.modified_resnet.layers[1:], weights):\n            layer.set_weights(weight)\n\n\n\n    def call(self, x0, x1):\n        # Combine extracted features from the first image and the second image\n        x0 = self.conv1_pad(x0)\n        x1 = self.conv1_pad(x1)\n        conv_x0 = self.conv_layer_x0(x0)\n        conv_x1 = self.conv_layer_x1(x1)\n\n        x = (conv_x0 + conv_x1) / 2\n\n        c1, c2, c3, c4, c5 = self.modified_resnet(x)\n\n        return c5, c4, c3, c2, c1", "\n\nclass FeaturePyramid(tf.keras.layers.Layer):\n    \"\"\"\n    Referenced https://keras.io/examples/vision/retinanet/\n    Builds the Feature Pyramid with the feature maps from the backbone.\n\n    Attributes:\n      num_classes: Number of classes in the dataset.\n      backbone: The backbone to build the feature pyramid from.\n        Currently supports ResNet50 only.\n    \"\"\"\n\n    def __init__(self, out_dim, **kwargs):\n        super(FeaturePyramid, self).__init__(name=\"FeaturePyramid\", **kwargs)\n        self.conv_c1_1x1 = tf.keras.layers.Conv2D(out_dim, 1, 1, \"same\")\n        self.conv_c2_1x1 = tf.keras.layers.Conv2D(out_dim, 1, 1, \"same\")\n        self.conv_c3_1x1 = tf.keras.layers.Conv2D(out_dim, 1, 1, \"same\")\n        self.conv_c4_1x1 = tf.keras.layers.Conv2D(out_dim, 1, 1, \"same\")\n        self.conv_c5_1x1 = tf.keras.layers.Conv2D(out_dim, 1, 1, \"same\")\n\n        self.conv_c1_3x3 = tf.keras.layers.Conv2D(out_dim, 3, 1, \"same\")\n        self.conv_c2_3x3 = tf.keras.layers.Conv2D(out_dim, 3, 1, \"same\")\n        self.conv_c3_3x3 = tf.keras.layers.Conv2D(out_dim, 3, 1, \"same\")\n        self.conv_c4_3x3 = tf.keras.layers.Conv2D(out_dim, 3, 1, \"same\")\n        self.conv_c5_3x3 = tf.keras.layers.Conv2D(out_dim, 3, 1, \"same\")\n        self.upsample_2x = tf.keras.layers.UpSampling2D(2)\n\n    def call(self, c5_output, c4_output, c3_output, c2_output, c1_output):\n        # are these 1x1 convolutions helpful, except for reducing the computation?\n        # What about keeping the channel # the same?\n        p1_output = self.conv_c1_1x1(c1_output)\n        p2_output = self.conv_c2_1x1(c2_output)\n        p3_output = self.conv_c3_1x1(c3_output)\n        p4_output = self.conv_c4_1x1(c4_output)\n        p5_output = self.conv_c5_1x1(c5_output)\n\n        p4_output = p4_output + self.upsample_2x(p5_output)\n        p3_output = p3_output + self.upsample_2x(p4_output)\n        p2_output = p2_output + self.upsample_2x(p3_output)\n        p1_output = p1_output + self.upsample_2x(p2_output)\n\n        p1_output = self.conv_c1_3x3(p1_output)\n        p2_output = self.conv_c2_3x3(p2_output)\n        p3_output = self.conv_c3_3x3(p3_output)\n        p4_output = self.conv_c4_3x3(p4_output)\n        p5_output = self.conv_c5_3x3(p5_output)\n\n        # the order of features are from coarse to fine\n        return p5_output, p4_output, p3_output, p2_output, p1_output", "\n\nclass PostEncoder(tf.keras.layers.Layer):\n    '''\n    outdim corresponds to the number of tracking points\n    '''\n\n    def __init__(self, out_dim):\n        super(PostEncoder, self).__init__()\n        self.encoder = ResNet50()  # VGG16() # VGG19_dropout('uflow/assets/model_weights/vgg19_dropout.hdf5') # VGG19_dropout('')\n        self.fpn = FeaturePyramid(out_dim)  # FPN([2048, 1024, 512, 256, 64], out_dim)\n\n    def call(self, x0, x1):\n        # x0: original first image\n        # x1: original second image\n        # p: transformed first image to second image\n        # m: Mask from (transformed first contour to second contour)\n        rs = self.encoder(x0, x1)\n        outs = self.fpn(*rs)\n\n        return outs", "\n\nclass LocalEncoder(tf.keras.layers.Layer):\n    '''\n    outdim corresponds to the number of tracking points\n    '''\n\n    def __init__(self, out_dim):\n        super(LocalEncoder, self).__init__()\n        self.encoder = VGG16() # VGG19_dropout('uflow/assets/model_weights/vgg19_dropout.hdf5') # VGG19_dropout('')\n        self.fpn = FeaturePyramid(out_dim)  # FPN([2048, 1024, 512, 256, 64], out_dim)\n\n    def call(self, x0):\n        rs = self.encoder(x0)\n        outs = self.fpn(*rs)\n\n        return outs", ""]}
{"filename": "src/tracking_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'''\nTensorflow implementation of PoST/utils/proc.py\nAlso includes some of my custom loss functions", "Tensorflow implementation of PoST/utils/proc.py\nAlso includes some of my custom loss functions\n'''\n\nfrom PIL import Image, ImageStat\nimport cv2\nimport numpy as np\nimport math\nimport tensorflow as tf\nimport collections", "import tensorflow as tf\nimport collections\n\n\n# ------------------------------ Loss ------------------------------------\n\ndef robust_l2(x):\n    return (x ** 2 + 0.001 ** 2)\n\n\ndef robust_l1(x):\n    return (x ** 2 + 0.001 ** 2) ** 0.5", "\n\ndef robust_l1(x):\n    return (x ** 2 + 0.001 ** 2) ** 0.5\n\n\ndef corr_2d_loss( gt_prev_id_assignments, gt_cur_id_assignments, corr_2d_matrix):\n    # classification loss only for tracked points\n    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n\n    sampled_corr_2d_matrix = sample_data_by_index(gt_prev_id_assignments, corr_2d_matrix)\n\n    a_loss = scce(gt_cur_id_assignments[:,:,0], sampled_corr_2d_matrix)\n\n    return a_loss", "\ndef corr_cycle_consistency(forward_corr_2d_matrix, backward_corr_2d_matrix):\n    '''\n\n    :param forward_corr_2d_matrix: shape (B, 1150, 1150), dtype float32\n    :param backward_corr_2d_matrix: shape (B, 1150, 1150), dtype float32\n    :return:\n    '''\n    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n\n    trans_backward_corr_2d_matrix = tf.transpose(backward_corr_2d_matrix, perm=[0, 2, 1])\n\n    forawrd_loss = scce(tf.math.argmax(forward_corr_2d_matrix, axis=-1), trans_backward_corr_2d_matrix)\n    backward_loss = scce(tf.math.argmax(trans_backward_corr_2d_matrix, axis=-1), forward_corr_2d_matrix)\n\n    batch_size = tf.shape(forward_corr_2d_matrix)[0]\n    num_seg_points = tf.shape(forward_corr_2d_matrix)[1]\n    diagnoal_one_increasing_tensor = tf.repeat( tf.expand_dims( tf.range( num_seg_points ), axis=0), repeats=batch_size, axis=0)\n    forward_diagnoal_loss = scce(diagnoal_one_increasing_tensor, forward_corr_2d_matrix)\n    backward_diagnoal_loss = scce(diagnoal_one_increasing_tensor, backward_corr_2d_matrix)\n\n    return forawrd_loss + backward_loss + forward_diagnoal_loss + backward_diagnoal_loss", "\n# -------------------------------------------------------------------------------------------------------\n\ndef sample_data_by_index(index_tensor, data_tensor):\n    '''\n    sample from data_tensor by the indices inside index_tensor\n\n    :param index_tensor: tensor of shape (B, num_tracked_points, 1), dtype int32\n    :param data_tensor: tensor of shape (B, num_seg_points, channels), dtype float32\n\n    e.g)\n    index_tensor = tf.constant( [[[-1]]] , dtype='int32')\n    sample_data_by_index(index_tensor, data_tensor)\n\n    If the index in index_tensor is outside the bound of data_tensor as above,\n    the output is tf.Tensor: shape=(1, 1, 1), numpy=array([[[0.]]]\n\n\n    :return: tensor of shape (B, num_tracked_points, 1), dtype float32\n    '''\n\n    batch_size = tf.shape(index_tensor)[0]\n    num_tracked_points = tf.shape(index_tensor)[1]\n    gt_prev_id_assignment_vector = tf.reshape(index_tensor, shape=[-1])\n    batch_index_list = tf.repeat(tf.range(batch_size), repeats=num_tracked_points)\n    indices = tf.stack([batch_index_list, gt_prev_id_assignment_vector], axis=1)  # shape is (number of tracked points, 2)\n\n    # sample from data_tensor by the index values inside index_tensor\n    sampled_data_tensor = tf.gather_nd(data_tensor, indices)\n    num_channels = tf.shape(data_tensor)[2]\n    reshaped_sampled_data_tensor = tf.reshape(sampled_data_tensor, shape=[batch_size, num_tracked_points, num_channels])\n\n    return reshaped_sampled_data_tensor", "\n\ndef matching_contour_points_loss(gt_prev_id_assignments, gt_cur_id_assignments, pred_id_assignments):\n    \"\"\"\n\n    :param gt_prev_id_assignments: tensor of shape (B, num_tracked_points, 1), dtype int32\n    :param gt_cur_id_assignments: tensor of shape (B, num_tracked_points, 1), dtype int32\n    :param pred_id_assignments: tensor of shape (B, num_seg_points, 1), dtype float32\n\n    :return: loss\n    \"\"\"\n    # total_loss = 0\n    # for batch_index in range(gt_prev_id_assignments.shape[0]):\n    #     for gt_point_index in range(gt_prev_id_assignments.shape[1]):\n    #         prev_seg_point_index = gt_prev_id_assignments[batch_index, gt_point_index, 0]\n    #         pred_cur_seg_point_index = pred_id_assignments[batch_index, prev_seg_point_index, 0]\n    #\n    #         a_diff = tf.cast(gt_cur_id_assignments[batch_index, gt_point_index, 0] - pred_cur_seg_point_index, tf.float32)\n    #         a_loss = robust_l1(a_diff)\n    #         total_loss = total_loss + a_loss\n    #\n    # # average\n    # a_loss = total_loss / (gt_prev_id_assignments.shape[0]*gt_prev_id_assignments.shape[1])\n\n    # for debugging\n    # index_tensor = tf.constant( [[[-1]],[[-1]],[[-1]],[[-1]],[[-1]],[[-1]],[[-1]],[[-1]]] , dtype='int32')\n    # sample_data_by_index(index_tensor, pred_id_assignments)\n\n    reshaped_sampled_pred_id_assignments = sample_data_by_index(gt_prev_id_assignments, pred_id_assignments)\n    error = robust_l1( reshaped_sampled_pred_id_assignments - tf.cast( gt_cur_id_assignments, tf.float32) )\n    a_loss = tf.reduce_mean(error)\n\n    return a_loss", "\n\ndef matching_contour_points_spatial_metric(cur_seg_points, gt_prev_id_assignments, gt_cur_id_assignments, pred_id_assignments):\n    \"\"\"\n    :param cur_seg_points: tensor of shape (B, num_seg_points, 2), dtype float32\n    :param gt_prev_id_assignments: tensor of shape (B, num_tracked_points, 1), dtype int32\n    :param gt_cur_id_assignments: tensor of shape (B, num_tracked_points, 1), dtype int32\n    :param pred_id_assignments: tensor of shape (B, num_seg_points, 1), dtype float32\n\n    :return: loss\n    \"\"\"\n    sampled_pred_id_assignments = sample_data_by_index(gt_prev_id_assignments, pred_id_assignments)\n    rounded_sampled_pred_id_assignments = tf.cast(tf.round(sampled_pred_id_assignments), dtype=tf.int32)\n\n    a_loss = matching_spatial_points_loss(rounded_sampled_pred_id_assignments, gt_cur_id_assignments, cur_seg_points, cur_seg_points)\n\n    return a_loss", "\n\ndef matching_spatial_points_loss(gt_prev_id_assignments, gt_cur_id_assignments, cur_seg_points, pred_points):\n\n    sampled_gt_points = sample_data_by_index(gt_cur_id_assignments, cur_seg_points)\n    sampled_pred_points = sample_data_by_index(gt_prev_id_assignments, pred_points)\n\n    error = robust_l2(sampled_gt_points - sampled_pred_points)\n    a_loss = tf.reduce_mean(error)\n\n    return a_loss", "\n# -------------------------------- Unsupervised Learning Losses -----------------------------------------------------------\ndef get_closest_contour_id(seg_points_xy, seg_points_mask, pred_tracking_points):\n    '''\n\n    :param seg_points_xy: shape [batch_size, num_seg_points, 2], dtype tf.float32\n    :param seg_points_mask:  shape [batch_size, num_seg_points, 1], dtype tf.float32\n    :param pred_tracking_points: shape [batch_size, num_seg_points, 2], dtype tf.float32\n    :return: shape [batch_size, num_seg_points, 1], dtype tf.int32\n    '''\n    closest_contour_id_list = []\n\n    for a_point_index in range(pred_tracking_points.shape[1]):\n        temp_pred_tracking_points = tf.expand_dims(pred_tracking_points[:, a_point_index, :], axis=1)\n        diff_dist = seg_points_xy - tf.repeat(temp_pred_tracking_points, repeats=pred_tracking_points.shape[1], axis=1)\n        l2_dist = tf.math.reduce_euclidean_norm(diff_dist, axis=-1)\n        cur_id_assign = tf.math.argmin(l2_dist, axis=1)  # shape (Batch_size) and dtype int64\n        closest_contour_id_list.append(cur_id_assign)\n\n    # (1150, 8) --> (8, 1150) --> (8, 1150, 1)\n    closest_contour_id_tensor = tf.expand_dims( tf.transpose( tf.convert_to_tensor(closest_contour_id_list), perm=[1,0]), axis=-1)\n    closest_contour_id_tensor = tf.cast(closest_contour_id_tensor, dtype=tf.int32)\n\n    return closest_contour_id_tensor", "\n\ndef cycle_consistency_spatial_loss(prev_seg_points_xy, prev_seg_points_mask, cur_seg_points_xy, cur_seg_points_mask, forward_spatial_offset, backward_spatial_offset):\n    prev_seg_points_mask = tf.expand_dims(prev_seg_points_mask, axis=-1)\n    cur_seg_points_mask = tf.expand_dims(cur_seg_points_mask, axis=-1)\n\n    pred_cur_seg_points = prev_seg_points_xy + forward_spatial_offset\n    pred_prev_seg_points = cur_seg_points_xy + backward_spatial_offset\n\n    # get the closest contour point id in the second contour\n    closest_cur_seg_points_id = get_closest_contour_id(cur_seg_points_xy, prev_seg_points_mask, pred_cur_seg_points)\n\n    # use the closest contour id above to get the pred_tracking_points\n    pred_prev_tracking_points = sample_data_by_index(closest_cur_seg_points_id, pred_prev_seg_points)\n\n    return_to_first_contour_error = robust_l2( prev_seg_points_xy - pred_prev_tracking_points ) * prev_seg_points_mask\n\n    # to make the gradient flow in both directions,\n    # get the closest contour point id in the first contour\n    closest_prev_seg_points_id = get_closest_contour_id(prev_seg_points_xy, cur_seg_points_mask, pred_prev_seg_points)\n\n    # use the closest contour id above to get the pred_tracking_points\n    pred_cur_tracking_points = sample_data_by_index(closest_prev_seg_points_id, pred_cur_seg_points)\n\n    return_to_second_contour_error = robust_l2( cur_seg_points_xy - pred_cur_tracking_points ) * cur_seg_points_mask\n\n    # get_first_occurrence_indices(prev_seg_points_mask[:,:,0], 0.1)\n    # get_first_occurrence_indices(cur_seg_points_mask[:,:,0], 0.1)\n    a_loss = tf.reduce_mean(return_to_first_contour_error + return_to_second_contour_error)\n\n    return a_loss", "\n\ndef cycle_consistency_assign_spatial_loss(prev_seg_points, cur_seg_points, forward_id_assignments, forward_spatial_offset):\n    sampled_cur_seg_points = sample_data_by_index(tf.cast(tf.round(forward_id_assignments), dtype=tf.int32), cur_seg_points)\n\n    pred_seg_points = prev_seg_points + forward_spatial_offset\n    sampled_pred_seg_points = sample_data_by_index(tf.cast(tf.round(forward_id_assignments), dtype=tf.int32), pred_seg_points)\n\n    error = robust_l1(sampled_cur_seg_points - sampled_pred_seg_points)\n    a_loss = tf.reduce_mean(error)\n\n    return a_loss", "\n\ndef cycle_consistency_assign_loss(forward_id_assignments, backward_id_assignments, prev_seg_mask, cur_seg_mask):\n    \"\"\"\n    :param forward_id_assignments: tensor of shape (B, num_seg_points, 1), dtype float32\n    :param backward_id_assignments: tensor of shape (B, num_seg_points, 1), dtype float32\n    :param prev_seg_mask: tensor of shape (B, num_seg_points, 1), dtype float32\n    :param cur_seg_mask: tensor of shape (B, num_seg_points, 1), dtype float32\n    :return:\n    \"\"\"\n    epsilon = 0.000000001\n    contour_indices = make_contour_order_indices_from_offsets(forward_id_assignments)\n\n    forward_id_offsets = ( forward_id_assignments - contour_indices ) * prev_seg_mask\n    backward_id_offsets = ( backward_id_assignments - contour_indices ) * cur_seg_mask\n\n    # Compute both ways so that gradients flow for both forward and backward assignments (gradient doesn't flow through tf.round)\n    # index backward_id_offsets by forward_id_assignment\n    sampled_backward_id_offsets = sample_data_by_index( tf.cast(tf.round(forward_id_assignments), dtype=tf.int32), backward_id_offsets)  # shape is (B, num_seg_points1, 1)\n    # index forword_offsets by backward_id_assignments\n    sampled_forward_id_offsets = sample_data_by_index(tf.cast(tf.round(backward_id_assignments), dtype=tf.int32), forward_id_offsets)  # shape is (B, num_seg_points2, 1)\n    # normalize this loss by their magnitude to prevent the magnitude going to 0\n    error_one = robust_l1( (forward_id_offsets + sampled_backward_id_offsets) / ( tf.math.abs(forward_id_offsets) + tf.math.abs(sampled_backward_id_offsets) + epsilon ) )\n    error_two = robust_l1( (backward_id_offsets + sampled_forward_id_offsets) / ( tf.math.abs(backward_id_offsets) + tf.math.abs(sampled_forward_id_offsets) + epsilon) )\n\n    a_loss = tf.reduce_mean(error_one + error_two)\n    \n    return a_loss", "\n\ndef contour_points_order_loss(pred_id_assignments):\n    contour_order_indices = tf.range(tf.shape(pred_id_assignments)[1], delta=1, dtype=pred_id_assignments.dtype, name='range')\n    contour_order_indices = tf.expand_dims( tf.expand_dims( contour_order_indices, axis=0), axis=-1)\n    repeated_contour_order_indices = tf.repeat(contour_order_indices, repeats=tf.shape(pred_id_assignments)[0], axis=0)\n\n    error = robust_l1(repeated_contour_order_indices - pred_id_assignments)\n    a_loss = tf.reduce_mean(error)\n\n    return a_loss", "\n# ---------\ndef tracker_photometric_loss(cur_image, gt_cur_tracking_points, pred_cur_tracking_points):\n    sampled_gt_image = exact_point_sampler(cur_image, gt_cur_tracking_points[:, :, 0], gt_cur_tracking_points[:, :, 1], abs_scale=True)  # (B, N, 3)\n    sampled_pred_image = bilinear_sampler_1d(cur_image, pred_cur_tracking_points[:, :, 0], pred_cur_tracking_points[:, :, 1], abs_scale=True)  # (B, N, 3)\n\n    error = robust_l1( (sampled_gt_image - sampled_pred_image) )\n    a_loss = tf.reduce_mean(error)\n\n    return a_loss", "\n\ndef tracker_unsupervised_photometric_loss(prev_image, cur_image, gt_prev_tracking_points, gt_prev_tracking_points_mask, pred_cur_tracking_points):\n    # since the number of valid gt_prev_tracking_points and number of valid pred_cur_tracking_points are different, we need mask\n    gt_prev_tracking_points_mask = tf.repeat( tf.expand_dims(gt_prev_tracking_points_mask, axis=-1), repeats=3, axis=-1)\n\n    sampled_gt_image = exact_point_sampler(prev_image, gt_prev_tracking_points[:, :, 0], gt_prev_tracking_points[:, :, 1], abs_scale=True)  # (B, N, 3)\n    sampled_pred_image = bilinear_sampler_1d(cur_image, pred_cur_tracking_points[:, :, 0], pred_cur_tracking_points[:, :, 1], abs_scale=True)  # (B, N, 3)\n\n    error = robust_l1( (sampled_gt_image - sampled_pred_image) ) * gt_prev_tracking_points_mask\n    a_loss = tf.reduce_mean(error)\n\n    return a_loss", "\n\n# -----------------------------------------------------------------------\ndef abs_robust_loss(diff, eps=0.01, q=0.4):\n    \"\"\"The so-called robust loss used by DDFlow.\"\"\"\n    return tf.pow((tf.abs(diff) + eps), q)\n\ndef census_transform(image, patch_size):\n    \"\"\"The census transform as described by DDFlow.\n\n    See the paper at https://arxiv.org/abs/1902.09145\n\n    Args:\n      image: tensor of shape (b, h, w, c)\n      patch_size: int\n    Returns:\n      image with census transform applied\n    \"\"\"\n    intensities = tf.image.rgb_to_grayscale(image) * 255\n\n    # 49x49 identity matrix to kernel [filter_height, filter_width, in_channels, out_channels]\n    kernel = tf.reshape(\n        tf.eye(patch_size * patch_size),\n        (patch_size, patch_size, 1, patch_size * patch_size))\n\n    neighbors = tf.nn.conv2d(\n        input=intensities, filters=kernel, strides=[1, 1, 1, 1], padding='SAME')\n    diff = neighbors - intensities\n    # Coefficients adopted from DDFlow.\n    diff_norm = diff / tf.sqrt(.81 + tf.square(diff))\n\n    return diff_norm", "\ndef soft_hamming(a_bhwk, b_bhwk, thresh=.1):\n    \"\"\"A soft hamming distance between tensor a_bhwk and tensor b_bhwk.\n\n    Args:\n      a_bhwk: tf.Tensor of shape (batch, N, features)\n      b_bhwk: tf.Tensor of shape (batch, N, features)\n      thresh: float threshold\n\n    Returns:\n      a tensor with approx. 1 in location that are significantly\n      more different than thresh and approx. 0 if significantly less\n      different than thresh.\n      (batch, N, 1)\n    \"\"\"\n    sq_dist_bhwk = tf.square(a_bhwk - b_bhwk)\n    soft_thresh_dist_bhwk = sq_dist_bhwk / (thresh + sq_dist_bhwk)\n    return tf.reduce_sum(input_tensor=soft_thresh_dist_bhwk, axis=2, keepdims=True)", "# -----------------------------------------------------------------------\n\ndef tracker_census_loss(cur_image, gt_cur_tracking_points, pred_cur_tracking_points, patch_size=5):\n    \"\"\"Compares the similarity of the census transform of two points\"\"\"\n\n    census_cur_image = census_transform(cur_image, patch_size)  # B x H x W x patch_size^2\n\n    sampled_census_gt_points = exact_point_sampler(census_cur_image, gt_cur_tracking_points[:, :, 0], gt_cur_tracking_points[:, :, 1], abs_scale=True)  # (B, N, patch_size^2)\n    sampled_census_pred_points = bilinear_sampler_1d(census_cur_image, pred_cur_tracking_points[:, :, 0], pred_cur_tracking_points[:, :, 1], abs_scale=True)  # (B, N, patch_size^2)\n\n    # soft hamming between two points\n    hamming_bhw = soft_hamming(sampled_census_gt_points, sampled_census_pred_points)\n\n    diff = abs_robust_loss(hamming_bhw)\n    a_loss = tf.reduce_mean(input_tensor=diff)\n\n    return a_loss", "\n\ndef tracker_unsupervised_census_loss(prev_image, cur_image, gt_prev_tracking_points, gt_prev_tracking_points_mask, pred_cur_tracking_points, patch_size=5):\n    \"\"\"Compares the similarity of the census transform of two points\"\"\"\n    census_prev_image = census_transform(prev_image, patch_size)  # B x H x W x patch_size^2\n    census_cur_image = census_transform(cur_image, patch_size)  # B x H x W x patch_size^2\n\n    sampled_census_gt_points = exact_point_sampler(census_prev_image, gt_prev_tracking_points[:, :, 0],\n                                                   gt_prev_tracking_points[:, :, 1],\n                                                   abs_scale=True)  # (B, N, patch_size^2)\n    sampled_census_pred_points = bilinear_sampler_1d(census_cur_image, pred_cur_tracking_points[:, :, 0],\n                                                     pred_cur_tracking_points[:, :, 1],\n                                                     abs_scale=True)  # (B, N, patch_size^2)\n\n    # soft hamming between two points\n    # since the number of valid gt_prev_tracking_points and number of valid pred_cur_tracking_points are different, we need mask\n    hamming_bhw = soft_hamming(sampled_census_gt_points, sampled_census_pred_points)\n\n    diff = abs_robust_loss(hamming_bhw) * tf.expand_dims(gt_prev_tracking_points_mask, axis=-1)\n    a_loss = tf.reduce_mean(diff)\n\n    return a_loss", "\n\n# -------------------------------------------------------------------------------------------------------\n\ndef snake_loss_from_offsets(gt_prev_points, gt_prev_points_mask, pred_offsets):\n    '''\n    :param gt_prev_points: (B, N, 2), float32\n    :param gt_prev_points_mask: (B,N), float32\n    :param pred_offsets: (B, N, 2), float32\n    :return:\n    '''\n    pred_points = gt_prev_points + pred_offsets\n\n    left_pred_points = tf.roll(pred_points, shift=-1, axis=1)  # 0th index element goes to the last index\n    point_diff_left = left_pred_points - pred_points  # x2 - x1, x3 - x2, ..., xn - x(n-1)\n    point_diff_left = point_diff_left[:, :-1, :]  # trim shifted point\n    point_diff_left_norm = tf.math.reduce_sum(point_diff_left ** 2, axis=-1)  # tf.maximum to prevent nan due to tf.math.sqrt\n\n    valid_point_diff_left_norm = point_diff_left_norm * gt_prev_points_mask[:, :-1]  # to remove padded points at the end, ignore last one index due to shift above\n\n    # this is to remove the large negative value padded points in the middle of the index\n    last_valid_index_tensor = tf.cast( tf.argmax(tf.math.abs(valid_point_diff_left_norm), axis=-1) )\n    batch_size = valid_point_diff_left_norm.shape[0]\n\n    total_tension = 0\n    total_stiffness = 0\n    for batch_index in range(batch_size):\n        if tf.reduce_sum(gt_prev_points_mask[batch_index]) < gt_prev_points_mask.shape[1]:\n            last_valid_index = last_valid_index_tensor[batch_index]\n        else:\n            last_valid_index = gt_prev_points_mask.shape[1] - 1\n\n        total_tension = total_tension + tf.reduce_mean(valid_point_diff_left_norm[batch_index, :last_valid_index])\n        a_stiffness = tf.reduce_mean( tf.math.reduce_sum(( point_diff_left[batch_index, 1:last_valid_index] - point_diff_left[batch_index, :(last_valid_index-1)]) ** 2, axis=-1) )\n        total_stiffness = total_stiffness + a_stiffness\n\n    tension_loss = total_tension / batch_size  # continuity/tension term\n    total_stiffness = total_stiffness / batch_size  # smoothness/stiffness term\n\n    return tension_loss, total_stiffness", "\n\ndef mechanical_loss_from_offsets(gt_prev_points, gt_prev_points_mask, pred_offsets):\n    '''\n\n    :param gt_prev_points: (B, N, 2), float32\n    :param gt_prev_points_mask: (B,N), float32\n    :param pred_offsets: (B, N, 2), float32\n    :return:\n    '''\n    # ------------ Linear Spring Force ------------\n    pred_points = gt_prev_points + pred_offsets\n\n    left_pred_points = tf.roll(pred_points, shift=-1, axis=1)\n    point_diff = robust_l1(pred_points - left_pred_points)\n    point_diff = point_diff[:, :-1,:] # trim shifted point\n    point_diff_norm = tf.math.sqrt( tf.maximum(tf.math.reduce_sum(point_diff ** 2, axis=-1), 1e-9) )  # tf.maximum to prevent nan due to tf.math.sqrt\n\n    average_length = tf.math.reduce_mean(point_diff_norm, axis=-1)  # shape (batch_size), dtype float32\n\n    valid_point_diff_norm = point_diff_norm * gt_prev_points_mask[:,:-1]  # to remove padded points at the end, ignore last one index due to shift above\n\n    # this is to remove the large negative value padded points in the middle of the index\n    last_valid_index_tensor = tf.cast( tf.argmax(tf.math.abs(valid_point_diff_norm), axis=-1), dtype=tf.int32)\n\n    total_linear_spring_force = 0\n    batch_size = valid_point_diff_norm.shape[0]\n    for batch_index in range(batch_size):\n        if tf.reduce_sum(gt_prev_points_mask[batch_index]) < gt_prev_points_mask.shape[1]:\n            last_valid_index = last_valid_index_tensor[batch_index]\n        else:\n            last_valid_index = gt_prev_points_mask.shape[1]\n        total_linear_spring_force = total_linear_spring_force + tf.reduce_mean( robust_l1( valid_point_diff_norm[batch_index, :last_valid_index] - average_length[batch_index] ) )\n\n    linear_spring_loss = total_linear_spring_force / batch_size\n    # ------------ Normal Torsion Force -----------\n    # Compute tangent by central differences. You can use a closed form tangent if you have it.\n    # referred https://stackoverflow.com/questions/66676502/how-can-i-move-points-along-the-normal-vector-to-a-curve-in-python\n    two_left_gt_prev_points = tf.roll(gt_prev_points, shift=-2, axis=1)\n    tangent_vectors = (two_left_gt_prev_points - gt_prev_points) / 2\n\n    normal_vectors = tf.stack([-tangent_vectors[:, :-2, 1], tangent_vectors[:, :-2, 0]], axis=-1)  # ignore last two indices due to shift above\n    # repeated_normal_vectors = tf.repeat( normal_vectors, pred_offsets.shape[0], axis=0)\n    unit_normal_vectors = normal_vectors / ( tf.expand_dims(tf.norm(normal_vectors, axis=-1), -1) + 0.0000001)\n\n    unit_pred_offsets = pred_offsets[:, 1:-1, :] / ( tf.expand_dims(tf.norm(pred_offsets[:, 1:-1, :], axis=-1), -1) + 0.0000001)\n\n    error = robust_l1( unit_normal_vectors - unit_pred_offsets ) * tf.expand_dims(gt_prev_points_mask[:,:-2], axis=-1) # to remove padded points at the end, ignore last two indices due to shift above\n    normal_tendancy_loss = tf.reduce_mean( error )\n\n    return linear_spring_loss, normal_tendancy_loss", "\n\n# def mechanical_loss(gt_prev_points, pred_points):\n#     '''\n#\n#     :param gt_prev_points: (B, N, 2)\n#     :param pred_points: (B, N, 2)\n#     :return:\n#     '''\n#     # ------------ Side Spring Force ------------", "#     '''\n#     # ------------ Side Spring Force ------------\n#     left_pred_points = tf.roll(pred_points, shift=1, axis=1)\n#     trimed_left_pred_points = left_pred_points[0, 1:, :]  # trim left boundary point after roll\n#\n#     point_diff = tf.math.abs(pred_points[0, 1:, :] - trimed_left_pred_points)\n#     distance_diff = tf.math.sqrt(tf.math.reduce_sum(point_diff ** 2, axis=1))\n#     average_length = tf.math.reduce_mean(distance_diff)\n#\n#     linear_spring_force = (distance_diff - average_length)", "#\n#     linear_spring_force = (distance_diff - average_length)\n#     left_linear_spring_force = linear_spring_force[1:]\n#     right_linear_spring_force = linear_spring_force[:-1]\n#\n#     linear_spring_loss = tf.math.abs(tf.reduce_mean(left_linear_spring_force - right_linear_spring_force))\n#\n#     # ------------ Normal Tendancy Force -----------\n#     # unit_w_vector = tf.linalg.normalize(gt_prev_points - pred_points, axis=1)[0]\n#     w_vector = gt_prev_points - pred_points", "#     # unit_w_vector = tf.linalg.normalize(gt_prev_points - pred_points, axis=1)[0]\n#     w_vector = gt_prev_points - pred_points\n#     w_vector = w_vector[0, 1:-1, :]\n#\n#     # Compute tangent by central differences. You can use a closed form tangent if you have it.\n#     # referred https://stackoverflow.com/questions/66676502/how-can-i-move-points-along-the-normal-vector-to-a-curve-in-python\n#     two_left_gt_prev_points = tf.roll(gt_prev_points, shift=2, axis=1)\n#     tangent_vectors = (two_left_gt_prev_points - gt_prev_points) / 2\n#\n#     normal_vectors = tf.stack([-tangent_vectors[0, 2:, 1], tangent_vectors[0, 2:, 0]], axis=1)", "#\n#     normal_vectors = tf.stack([-tangent_vectors[0, 2:, 1], tangent_vectors[0, 2:, 0]], axis=1)\n#     # unit_normal_vectors = tf.linalg.normalize(normal_vectors, axis=1)[0]\n#\n#     normal_tendancy_loss = tf.math.abs(tf.reduce_mean(normal_vectors - w_vector))\n#\n#     return linear_spring_loss, normal_tendancy_loss\n\n# ------------------------------------------------------------------------\n\ndef bayesian_points_loss(gt_points, pred_points, uncertainty):\n    error = robust_l1(gt_points - pred_points) * tf.math.exp(-1*uncertainty) + uncertainty\n    a_loss = tf.reduce_mean(error)\n\n    return a_loss", "# ------------------------------------------------------------------------\n\ndef bayesian_points_loss(gt_points, pred_points, uncertainty):\n    error = robust_l1(gt_points - pred_points) * tf.math.exp(-1*uncertainty) + uncertainty\n    a_loss = tf.reduce_mean(error)\n\n    return a_loss\n\n\ndef multi_level_points_loss(gt_points, saved_offset):\n    total_loss = 0\n    gamma = 0.5\n    total_level = len(saved_offset)\n    for cur_level, a_offset in saved_offset.items():\n        pred_points = a_offset[:, :, :2]\n        a_uncertainty = a_offset[:, :, -1]\n        a_uncertainty = tf.expand_dims(a_uncertainty, axis=-1)\n\n        total_loss = total_loss + bayesian_points_loss(gt_points, pred_points, a_uncertainty) * gamma**(total_level-cur_level)\n\n        # total_loss = total_loss + matching_points_loss(gt_points, pred_points) * gamma**(total_level-cur_level)\n\n    return total_loss", "\ndef multi_level_points_loss(gt_points, saved_offset):\n    total_loss = 0\n    gamma = 0.5\n    total_level = len(saved_offset)\n    for cur_level, a_offset in saved_offset.items():\n        pred_points = a_offset[:, :, :2]\n        a_uncertainty = a_offset[:, :, -1]\n        a_uncertainty = tf.expand_dims(a_uncertainty, axis=-1)\n\n        total_loss = total_loss + bayesian_points_loss(gt_points, pred_points, a_uncertainty) * gamma**(total_level-cur_level)\n\n        # total_loss = total_loss + matching_points_loss(gt_points, pred_points) * gamma**(total_level-cur_level)\n\n    return total_loss", "\n\ndef pixel_matching_loss(gt_image, pred_image, global_transformed_points):\n    # for global alignment in PoST\n    image_size = gt_image.shape[-2:-4:-1]\n    rel_global_transformed_points = abs2rel(global_transformed_points, image_size)\n    a_mask = cnt2mask(rel_global_transformed_points, image_size)\n    a_mask = tf.cast(a_mask > 0, tf.float32)\n    total_area_of_mask = tf.reduce_sum(a_mask)\n\n    a_loss_map = tf.reduce_sum(tf.math.multiply(robust_l2(gt_image - pred_image), a_mask) / total_area_of_mask)\n    a_loss = tf.reduce_sum(a_loss_map) * 255\n\n    return a_loss", "\n\ndef offset_regularization_loss(a_offset):\n    # penalize offset all pointing to the same direction\n    offset_loss = tf.math.reduce_sum(a_offset, axis=1)\n    offset_loss = tf.math.abs(offset_loss)\n    offset_loss = tf.math.reduce_sum(offset_loss)\n\n    return offset_loss\n", "\n# -----------------------------------------------------------------------\n# ------------------------ Utility functions ----------------------------\n# -----------------------------------------------------------------------\n\ndef make_contour_order_indices_from_offsets(id_assign_offsets):\n    contour_order_indices = tf.range(tf.shape(id_assign_offsets)[1], delta=1, dtype=id_assign_offsets.dtype)\n    contour_order_indices = tf.expand_dims(tf.expand_dims(contour_order_indices, axis=0), axis=-1)\n    repeated_contour_order_indices = tf.repeat(contour_order_indices, repeats=tf.shape(id_assign_offsets)[0], axis=0)\n\n    return repeated_contour_order_indices", "\n\ndef linear_seq_increase(iteration, max_seq):\n    '''\n    Rewrite below more succintly\n\n    # cur_seq = 2\n    # if iteration > 400:\n    #     cur_seq = 3\n    # elif iteration > 800:\n    #     cur_seq = 4\n    # elif iteration > 1200:\n    #     cur_seq = 5\n\n    :param iteration:\n    :param max_seq:\n    :return:\n    '''\n\n    min_seq = 2\n    lowest_iter_bound = 5\n    cur_seq = tf.math.minimum(iteration, lowest_iter_bound*(max_seq-min_seq)) // lowest_iter_bound + min_seq\n    # cur_seq = min(iteration, lowest_iter_bound*(max_seq-min_seq)) // lowest_iter_bound + min_seq\n\n    return cur_seq", "\ndef get_first_occurrence_indices(sequence, eos_idx):\n    '''\n    https://stackoverflow.com/questions/42184663/how-to-find-an-index-of-the-first-matching-element-in-tensorflow\n    args:\n      sequence: [batch, length]\n      eos_idx: scalar\n    return:\n     tf.tensor with shape (batch), dtype tf.int32\n    '''\n    batch_size, maxlen = sequence.get_shape().as_list()\n    eos_idx = tf.convert_to_tensor(eos_idx)\n    tensor = tf.concat( [sequence, tf.tile(eos_idx[None, None], [batch_size, 1])], axis=-1)\n    index_all_occurrences = tf.where(tf.math.less(tensor, eos_idx))\n    index_all_occurrences = tf.cast(index_all_occurrences, tf.int32)\n    index_first_occurrences = tf.math.segment_min(index_all_occurrences[:, 1],\n                                           index_all_occurrences[:, 0])\n    index_first_occurrences.set_shape([batch_size])\n    index_first_occurrences = tf.minimum(index_first_occurrences, maxlen)\n\n    return index_first_occurrences", "\n\ndef normalize_each_row(a_tensor):\n    '''\n    :param a_tensor: shape (batch_size, seg_point_num, channels)\n    :return: normalized_tensor: shape (batch_size, seg_point_num, channels)\n    '''\n    epsilon = 0.0000001\n\n    min_tensor = tf.reduce_min(a_tensor, axis=-1)\n    min_tensor = tf.expand_dims( min_tensor, axis=-1)  # batch_size, seg_point_num, 1\n    max_tensor = tf.reduce_max(a_tensor, axis=-1)\n    max_tensor = tf.expand_dims( max_tensor, axis=-1)  # batch_size, seg_point_num, 1\n    normalized_tensor = tf.math.divide( tf.subtract( a_tensor, min_tensor), tf.subtract( max_tensor, min_tensor ) + epsilon )\n\n    return normalized_tensor", "\n\ndef fit_id_assignments_to_next_contour(id_assignments, prev_seg_mask, cur_seg_max_index):\n    '''\n    :param id_assignments: shape (batch_size, seg_point_num, 1)\n    :param cur_seg_max_index:  TensorShape(), tf.int32\n    :return:\n    '''\n\n    cur_seg_max_index = tf.cast(cur_seg_max_index-1, id_assignments.dtype)\n    total_num_seg_point = tf.shape(id_assignments)[1]\n    if cur_seg_max_index.shape == []:\n        cur_seg_max_index = tf.expand_dims(cur_seg_max_index, axis=-1)\n    cur_seg_max_index = tf.expand_dims(cur_seg_max_index, axis=-1)\n    cur_seg_max_index = tf.expand_dims(cur_seg_max_index, axis=-1)\n    cur_seg_max_index = tf.repeat( cur_seg_max_index, repeats=total_num_seg_point, axis=1)  # batch_size, total_num_seg_point, 1\n\n    masked_id_assignments = id_assignments * prev_seg_mask\n    max_id = tf.reduce_max(masked_id_assignments, axis=1) # batch_size, 1\n    max_id = tf.expand_dims( max_id, axis=1)  # batch_size, 1, 1\n    max_id = tf.repeat( max_id, repeats=total_num_seg_point, axis=1)  # batch_size, total_num_seg_point, 1\n\n    fit_id_assignments = (masked_id_assignments / max_id ) * cur_seg_max_index  # batch_size, total_num_seg_point, 1\n\n    fit_id_assignments = fit_id_assignments + (prev_seg_mask - 1)  # so that masked id_assignments are negative\n\n    return fit_id_assignments", "\n\ndef resize_seg_points(orig_height, orig_width, new_height, new_width, tracking_points):\n    '''\n    Find new coordinate of GT tracking points on the resized image\n\n    :param orig_height:\n    :param orig_width:\n    :param new_height:\n    :param new_width:\n    :param tracking_points: are in (x, y) order\n    :return:\n    '''\n\n    if tf.is_tensor(orig_height):\n        orig_height = tf.cast(orig_height, dtype='float32')\n    else:\n        orig_height = tf.constant(orig_height, dtype='float32')\n    if tf.is_tensor(orig_width):\n        orig_width = tf.cast(orig_width, dtype='float32')\n    else:\n        orig_width = tf.constant(orig_width, dtype='float32')\n\n    if not tf.is_tensor(new_height):\n        new_height = tf.constant(new_height, dtype='float32')\n    if not tf.is_tensor(new_width):\n        new_width = tf.constant(new_width, dtype='float32')\n\n    Ry = new_height / orig_height\n    Rx = new_width / orig_width\n    tracking_points = tf.cast(tracking_points, dtype='float32')\n    \n    x_points = tracking_points[:, :, 0] * Rx  # x-axis (width) column\n    y_points = tracking_points[:, :, 1] * Ry  # y-axis (height) row\n    points_tensor = tf.round(tf.stack([x_points, y_points], axis=-1))\n    # tf.print(Ry, Rx, orig_height, orig_width, new_height, new_width)\n\n    masking_tensor = tracking_points[:, :, 2:]\n    \n    return tf.concat([points_tensor, masking_tensor], axis=-1)", "\n\ndef rel2abs(cnt, size):\n    cnt_intermediate = cnt / 2 + 0.5\n    cnt_x = cnt_intermediate[..., 0] * size[0]\n    cnt_y = cnt_intermediate[..., 1] * size[1]\n    cnt_out = tf.stack([cnt_x, cnt_y], axis=-1)\n\n    return cnt_out\n", "\n\ndef abs2rel(cnt, size):\n    cnt = tf.cast(cnt, dtype='float32')\n\n    cnt_x = tf.divide(cnt[..., 0], size[0])\n    cnt_y = tf.divide(cnt[..., 1], size[1])\n    cnt_xy = tf.stack([cnt_x, cnt_y], axis=-1)\n    cnt_out = (cnt_xy - 0.5) * 2  # [-1 1]\n\n    return cnt_out", "\n\ndef cnt2mask(cnt, size):\n    '''\n    Range of the intensity of the mask is 0 ~ 1\n    :param cnt:\n    :param size: Width, Height\n    :return:\n    '''\n    B, N, _ = cnt.shape\n\n    abs_cnt = tf.math.round(rel2abs(cnt, size))\n    masks = []\n    for i in range(B):\n        mask = np.zeros((size[1], size[0], 3))\n        abs_cnt_np = abs_cnt[i].numpy().astype('int32')\n        mask = cv2.drawContours(mask, [abs_cnt_np], -1, (1, 1, 1), cv2.FILLED)\n        mask = tf.convert_to_tensor(np.mean(mask, axis=-1), dtype='float32')  # H, W\n        masks.append(tf.expand_dims(mask, axis=-1))  # H, W, 1\n    masks = tf.stack(masks, axis=0)  # B, H, W, 1\n\n    return masks", "\n\ndef transform(img, cnt, theta):\n    img_align = transform_img(img, theta)\n    cnt_align = transform_cnt(cnt, theta)\n    return img_align, cnt_align\n\n\ndef transform_img(img, theta):\n    # inv_theta = invert(theta)\n    grid = affine_grid_generator(*img.shape[:3], theta)\n    img_align = bilinear_sampler_2d(*img.shape[:3], img, grid[:,:,:,0], grid[:,:,:,1])\n\n    return img_align", "def transform_img(img, theta):\n    # inv_theta = invert(theta)\n    grid = affine_grid_generator(*img.shape[:3], theta)\n    img_align = bilinear_sampler_2d(*img.shape[:3], img, grid[:,:,:,0], grid[:,:,:,1])\n\n    return img_align\n\n\ndef transform_cnt(cnt, theta):\n    # cnt: B, N, 2\n    # theta: B, 2, 3\n\n    B, N, _ = cnt.shape\n    cnt_align = tf.concat((cnt, tf.ones_like(cnt)[..., :1]), axis=2)  # B, N, 3\n    theta = tf.transpose(theta, perm=[0, 2, 1])  # B, 3, 2\n    theta = tf.cast(theta, 'float32')\n    cnt_align = tf.linalg.matmul(cnt_align, theta)  # B, N, 2\n\n    return cnt_align", "def transform_cnt(cnt, theta):\n    # cnt: B, N, 2\n    # theta: B, 2, 3\n\n    B, N, _ = cnt.shape\n    cnt_align = tf.concat((cnt, tf.ones_like(cnt)[..., :1]), axis=2)  # B, N, 3\n    theta = tf.transpose(theta, perm=[0, 2, 1])  # B, 3, 2\n    theta = tf.cast(theta, 'float32')\n    cnt_align = tf.linalg.matmul(cnt_align, theta)  # B, N, 2\n\n    return cnt_align", "\n\ndef cnt2poly(cnt):\n    x_min = tf.math.reduce_min(cnt[..., 0], axis=1, keepdims=True)\n    y_min = tf.math.reduce_min(cnt[..., 1], axis=1, keepdims=True)\n    xy_min = tf.stack( (x_min, y_min), axis=-1)\n    expanded_xy_min = tf.broadcast_to(xy_min, tf.shape(cnt))  # B, N, C\n\n    poly = cnt - expanded_xy_min\n    poly = tf.cast(poly, 'float32')\n\n    return poly", "\n\ndef get_adj_ind(n_adj, n_nodes):\n    '''\n    :param n_adj: number of neighboring points\n    :param n_nodes: total number of tracking points\n    :return: (n_nodes, n_adj) indices\n    e.g.\n    array([[5, 6, 1, 2],\n       [6, 0, 2, 3],\n       [0, 1, 3, 4],\n       [1, 2, 4, 5],\n       [2, 3, 5, 6],\n       [3, 4, 6, 0],\n       [4, 5, 0, 1]], dtype=int32)>\n    '''\n    ind = tf.convert_to_tensor([i for i in range(-n_adj // 2, n_adj // 2 + 1) if i != 0], dtype='int32')\n    ind = (tf.range(n_nodes)[:, None] + ind[None]) % n_nodes\n\n    return ind", "\ndef erode(mask, it=10):\n    kernel = np.ones((10, 10), np.uint8)\n    mask = cv2.erode(mask, kernel, iterations=it)\n    return mask\n\n\ndef dilate(mask, it=10):\n    if it > 0:\n        kernel = np.ones((10, 10), np.uint8)\n        mask = cv2.dilate(mask, kernel, iterations=it)\n    return mask", "\n\ndef sample_cnt(cnt, num_cp):\n    # cnt: N_all, 2\n    cnt = cnt.clone()\n    N_all = tf.shape(cnt)[0]\n\n    select = np.linspace(0, N_all - (N_all // num_cp), num_cp, dtype=np.int64)\n    sample = cnt[select]  # N, 2\n    return sample", "\n\ndef sample_cnt_with_idx(cnt, idx, num_cp):\n    total_num = cnt.shape[0]\n    select = np.linspace(0, total_num - 1, num_cp).astype(np.int64)\n    sampled_idx = ((idx / total_num) * num_cp).astype(np.int64)\n    select[sampled_idx] = idx\n    sampled_cnt = cnt[select]  # N, 2\n    sampled_cnt = sampled_cnt.reshape(-1, 1, 2)\n\n    return sampled_cnt, sampled_idx", "\n\ndef update_cnt(cnt_out, cnt_smpl, start):\n    B = cnt_out.shape[0]\n    N = cnt_out.shape[1]\n    N_smpl = cnt_smpl.shape[1]\n\n    skip = N // N_smpl\n\n    cnt_out[:, start::skip] = cnt_smpl\n\n    return cnt_out", "\n\n# def upsample_offset(offset, stride=2):\n#     B, N, _, _ = offset.shape\n#\n#     offset_up = offset.clone()\n#     offset_up = offset_up.expand(B, N, stride, 2).contiguous().view(B, N * stride, 1, 2)  # B, N*stride, 1, 2\n#     offset_up = torch.cat([offset_up.roll(i, dims=1) for i in range(stride)], dim=2)  # B, N*stride, stride, 2\n#     offset_up = torch.mean(offset_up, dim=2, keepdim=True)  # B, N*stride, 1, 2)\n#     return offset_up", "#     offset_up = torch.mean(offset_up, dim=2, keepdim=True)  # B, N*stride, 1, 2)\n#     return offset_up\n\n\ndef invert(theta):\n    det = theta[:, 0, 0] * theta[:, 1, 1] - theta[:, 0, 1] * theta[:, 1, 0]\n    adj_x = -theta[:, 1, 1] * theta[:, 0, 2] + theta[:, 0, 1] * theta[:, 1, 2]\n    adj_y = theta[:, 1, 0] * theta[:, 0, 2] - theta[:, 0, 0] * theta[:, 1, 2]\n\n    inv_theta = tf.convert_to_tensor( [ [theta[:, 1, 1], -theta[:, 0, 1], adj_x], [-theta[:, 1, 0], theta[:, 0, 0], adj_y] ] , dtype=theta.dtype)\n    # inv_theta[:, 0, 0] = theta[:, 1, 1]\n    # inv_theta[:, 1, 1] = theta[:, 0, 0]\n    # inv_theta[:, 0, 1] = -theta[:, 0, 1]\n    # inv_theta[:, 1, 0] = -theta[:, 1, 0]\n    # inv_theta[:, 0, 2] = adj_x\n    # inv_theta[:, 1, 2] = adj_y\n\n    inv_theta = inv_theta / tf.reshape(det, shape=(-1, 1, 1))\n\n    return inv_theta", "\n\ndef generate_pos_emb(num_pos):\n\n    emb = np.arange(0, num_pos, 1, dtype=np.float) * 2 * math.pi / num_pos\n    sin_p = np.sin(emb)  # N\n    cos_p = np.cos(emb)  # N\n    emb = np.stack([sin_p, cos_p], axis=1)  # N, 2\n    emb = tf.convert_to_tensor(emb, dtype='float32') # torch.FloatTensor(emb)\n\n    return emb", "\n\ndef crop_and_resize(img, cbox, context, size,\n                    margin=0, correct=False, resample=Image.BILINEAR):\n    if isinstance(img, np.ndarray):\n        img = Image.fromarray(img)\n    assert (isinstance(img, Image.Image))\n    assert (isinstance(cbox, np.ndarray))\n    min_sz = 10\n\n    W, H = img.size\n\n    if img.mode != 'P':\n        img = img.convert('RGB')\n\n    cbox = cbox.copy()\n\n    cbox[0] = cbox[0] + 0.5 * (cbox[2] - 1)\n    cbox[1] = cbox[1] + 0.5 * (cbox[3] - 1)\n\n    # define output format\n    out_size = size + margin\n\n    if img.mode != 'P':\n        avg_color = ImageStat.Stat(img).mean\n        avg_color = tuple(int(round(c)) for c in avg_color)\n        patch = Image.new(img.mode, (out_size, out_size), color=avg_color)\n    else:\n        patch = Image.new(img.mode, (out_size, out_size))\n\n    m = (cbox[2] + cbox[3]) * 0.5\n    search_range = math.sqrt((cbox[2] + m) * (cbox[3] + m)) * context\n\n    # crop\n    crop_sz = int(round(search_range * out_size / size))\n    crop_sz = max(5, crop_sz)\n    crop_ctr = cbox[:2]\n\n    dldt = crop_ctr - 0.5 * (crop_sz - 1)\n    drdb = dldt + crop_sz\n    plpt = np.maximum(0, -dldt)\n    dldt = np.maximum(0, dldt)\n    drdb = np.minimum((W, H), drdb)\n\n    dltrb = np.concatenate((dldt, drdb))\n    dltrb = np.round(dltrb).astype('int')\n    cp_img = img.crop(dltrb)\n\n    # resize\n    cW, cH = cp_img.size\n    tW = max(cW * out_size / crop_sz, min_sz)\n    tH = max(cH * out_size / crop_sz, min_sz)\n    tW = int(round(tW))\n    tH = int(round(tH))\n    rz_img = cp_img.resize((tW, tH), resample)\n\n    # calculate padding to paste to patch\n    plpt = plpt * out_size / crop_sz\n    plpt_ = np.round(plpt).astype('int')\n    pltrb = np.concatenate((plpt_, plpt_ + (tW, tH)))\n\n    # paste\n    patch.paste(rz_img, pltrb)\n\n    # if flag 'correct' is Ture, return information about turning back.\n    if correct:\n        scale = crop_sz / out_size\n        leftmost = crop_ctr - (crop_sz - 1) / 2\n        return patch, leftmost, scale\n\n    return patch", "\n\ndef affine_grid_generator(B, H, W, theta):\n    '''\n    https://pyimagesearch.com/2022/05/23/spatial-transformer-networks-using-tensorflow/\n    \n    :param B: \n    :param H: \n    :param W: \n    :param theta: \n    :return: \n    '''\n    # create normalized 2D grid\n    x = tf.linspace(-1.0, 1.0, W)\n    y = tf.linspace(-1.0, 1.0, H)\n    (xT, yT) = tf.meshgrid(x, y)\n    # flatten the meshgrid\n    xTFlat = tf.reshape(xT, [-1])\n    yTFlat = tf.reshape(yT, [-1])\n    # reshape the meshgrid and concatenate ones to convert it to homogeneous form\n    ones = tf.ones_like(xTFlat)\n    samplingGrid = tf.stack([xTFlat, yTFlat, ones])\n    # repeat grid batch size times\n    samplingGrid = tf.broadcast_to(samplingGrid, (B, 3, H * W))\n    # cast the affine parameters and sampling grid to float32\n    # required for matmul\n    theta = tf.cast(theta, \"float32\")\n    samplingGrid = tf.cast(samplingGrid, \"float32\")\n    # transform the sampling grid with the affine parameter\n    batchGrids = tf.matmul(theta, samplingGrid)  # 2x3 and 3x####\n    # reshape the sampling grid to (B, H, W, 2)\n    batchGrids = tf.reshape(batchGrids, [B, 2, H, W])\n    transposed_batchGrids = tf.transpose(batchGrids, perm=[0, 2, 3, 1])\n\n    return transposed_batchGrids", "\n\ndef bilinear_sampler_2d(B, H, W, featureMap, x, y):\n    '''\n    From https://pyimagesearch.com/2022/05/23/spatial-transformer-networks-using-tensorflow/\n\n    :param B:\n    :param H:\n    :param W:\n    :param featureMap:\n    :param x:\n    :param y:\n    :return:\n    '''\n\n    def get_pixel_value_2d(B, H, W, featureMap, x, y):\n\n        # create batch indices and reshape it\n        batchIdx = tf.range(0, B)\n        batchIdx = tf.reshape(batchIdx, (B, 1, 1))\n        # create the indices matrix which will be used to sample the feature map\n        b = tf.tile(batchIdx, (1, H, W))\n        indices = tf.stack([b, y, x], 3)\n        # gather the feature map values for the corresponding indices\n        gatheredPixelValue = tf.gather_nd(featureMap, indices)\n        # return the gather pixel values\n        return gatheredPixelValue\n\n    # define the bounds of the image\n    maxY = tf.cast(H - 1, \"int32\")\n    maxX = tf.cast(W - 1, \"int32\")\n    zero = tf.zeros([], dtype=\"int32\")\n    # rescale x and y to feature spatial dimensions\n    x = tf.cast(x, \"float32\")\n    y = tf.cast(y, \"float32\")\n    x = 0.5 * ((x + 1.0) * tf.cast(maxX - 1, \"float32\"))\n    y = 0.5 * ((y + 1.0) * tf.cast(maxY - 1, \"float32\"))\n    # grab 4 nearest corner points for each (x, y)\n    x0 = tf.cast(tf.floor(x), \"int32\")\n    x1 = x0 + 1\n    y0 = tf.cast(tf.floor(y), \"int32\")\n    y1 = y0 + 1\n    # clip to range to not violate feature map boundaries\n    x0 = tf.clip_by_value(x0, zero, maxX)\n    x1 = tf.clip_by_value(x1, zero, maxX)\n    y0 = tf.clip_by_value(y0, zero, maxY)\n    y1 = tf.clip_by_value(y1, zero, maxY)\n\n    # get pixel value at corner coords\n    Ia = get_pixel_value_2d(B, H, W, featureMap, x0, y0)\n    Ib = get_pixel_value_2d(B, H, W, featureMap, x0, y1)\n    Ic = get_pixel_value_2d(B, H, W, featureMap, x1, y0)\n    Id = get_pixel_value_2d(B, H, W, featureMap, x1, y1)\n\n    # recast as float for delta calculation\n    x0 = tf.cast(x0, \"float32\")\n    x1 = tf.cast(x1, \"float32\")\n    y0 = tf.cast(y0, \"float32\")\n    y1 = tf.cast(y1, \"float32\")\n    # calculate deltas\n    wa = (x1 - x) * (y1 - y)\n    wb = (x1 - x) * (y - y0)\n    wc = (x - x0) * (y1 - y)\n    wd = (x - x0) * (y - y0)\n    # add dimension for addition\n    wa = tf.expand_dims(wa, axis=3)\n    wb = tf.expand_dims(wb, axis=3)\n    wc = tf.expand_dims(wc, axis=3)\n    wd = tf.expand_dims(wd, axis=3)\n    # compute transformed feature map\n    transformedFeatureMap = tf.add_n(\n        [wa * Ia, wb * Ib, wc * Ic, wd * Id])\n    # transformedFeatureMap = wb * Ib + wd * Id # wa * Ia + wc * Ic\n\n    return transformedFeatureMap", "\n\ndef get_pixel_value(a_feature, x_coord_list, y_coord_list):\n    \"\"\"\n    Utility function to get pixel value for coordinate\n    vectors x and y from a 4D tensor image.\n    Input\n    -----\n    - a_feature: tensor of shape (B, H, W, C)\n    - x_list: tensor of shape (B, x_coord of Points)\n    - y_list: tensor of shape (B, y_coord of Points)\n    Returns\n    -------\n    - output: tensor of shape (B, num_points, C)\n    \"\"\"\n    # assert x_coord_list.shape == y_coord_list.shape\n    batch_size = x_coord_list.shape[0]\n    number_of_points = tf.shape(x_coord_list)[1]  # returns the dynamic shape whereas Tensor.shape returns the static shape of the tensor\n\n    x_coord_list = tf.reshape(x_coord_list, shape=[-1])\n    y_coord_list = tf.reshape(y_coord_list, shape=[-1])\n    batch_index_list = tf.repeat(tf.range(batch_size), repeats=number_of_points)\n    indices = tf.stack([batch_index_list, y_coord_list, x_coord_list], axis=1)  # shape is (number of points, 3)\n\n    features_at_points = tf.gather_nd(a_feature, indices)\n    number_of_channels = a_feature.shape[3]\n    features_at_points = tf.reshape(features_at_points, shape=[batch_size, number_of_points, number_of_channels])\n\n    return features_at_points", "\n\ndef bilinear_sampler_1d(input_feature, x, y, abs_scale=False):\n    \"\"\"\n    From https://github.com/kevinzakka/spatial-transformer-network/blob/master/stn/transformer.py#L159\n    https://stackoverflow.com/questions/52888146/what-is-the-equivalent-of-torch-nn-functional-grid-sample-in-tensorflow-numpy\n\n    Performs bilinear sampling of the input images according to the\n    normalized coordinates provided by the sampling grid. Note that\n    the sampling is done identically for each channel of the input.\n    To test if the function works properly, output image should be\n    identical to input image when theta is initialized to identity\n    transform.\n    Input\n    -----\n    - input_feature: batch of images in (B, H, W, C) layout.\n    - grid: x, y which is the output of affine_grid_generator.\n    Returns\n    -------\n    - out: interpolated images according to grids. Same size as grid.\n\n    \"\"\"\n\n    input_feature = tf.cast(input_feature, dtype='float32')\n    H = tf.shape(input_feature)[1]\n    W = tf.shape(input_feature)[2]\n\n    max_y = tf.cast(H - 1, 'int32')\n    max_x = tf.cast(W - 1, 'int32')\n    zero = tf.zeros([], dtype='int32')\n\n    # rescale x and y to [0, W-1/H-1]\n    if not abs_scale:\n        x = tf.cast(x, 'float32')\n        y = tf.cast(y, 'float32')\n        x = 0.5 * ((x + 1.0) * tf.cast(max_x, 'float32'))\n        y = 0.5 * ((y + 1.0) * tf.cast(max_y, 'float32'))\n    # grab 4 nearest corner points for each (x_i, y_i)\n    x0 = tf.cast(tf.floor(x), 'int32')\n    x1 = x0 + 1\n    y0 = tf.cast(tf.floor(y), 'int32')\n    y1 = y0 + 1\n\n    # clip to range [0, H-1/W-1] to not violate input_feature boundaries\n    x0 = tf.clip_by_value(x0, zero, max_x)\n    x1 = tf.clip_by_value(x1, zero, max_x)\n    y0 = tf.clip_by_value(y0, zero, max_y)\n    y1 = tf.clip_by_value(y1, zero, max_y)\n\n    # get pixel value at corner coords\n    Ia = get_pixel_value(input_feature, x0, y0)\n    Ib = get_pixel_value(input_feature, x0, y1)\n    Ic = get_pixel_value(input_feature, x1, y0)\n    Id = get_pixel_value(input_feature, x1, y1)\n\n    # recast as float for delta calculation\n    x0 = tf.cast(x0, 'float32')\n    x1 = tf.cast(x1, 'float32')\n    y0 = tf.cast(y0, 'float32')\n    y1 = tf.cast(y1, 'float32')\n\n    # calculate deltas\n    wa = (x1-x) * (y1-y)\n    wb = (x1-x) * (y-y0)\n    wc = (x-x0) * (y1-y)\n    wd = (x-x0) * (y-y0)\n\n    # add dimension for addition\n    wa = tf.expand_dims(wa, axis=2)\n    wb = tf.expand_dims(wb, axis=2)\n    wc = tf.expand_dims(wc, axis=2)\n    wd = tf.expand_dims(wd, axis=2)\n\n    # compute output\n    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n\n    return out", "\n\ndef exact_point_sampler(input_feature, x, y, abs_scale=False):\n    \"\"\"\n\n    Performs sampling of the input images according to the\n    normalized coordinates provided by the sampling grid. Note that\n    the sampling is done identically for each channel of the input.\n    Input\n    -----\n    - input_feature: batch of images in (B, H, W, C) layout.\n    - x, y\n\n    \"\"\"\n\n    input_feature = tf.cast(input_feature, dtype='float32')\n    H = tf.shape(input_feature)[1]\n    W = tf.shape(input_feature)[2]\n\n    max_y = tf.cast(H - 1, 'int32')\n    max_x = tf.cast(W - 1, 'int32')\n    zero = tf.zeros([], dtype='int32')\n\n    # rescale x and y to [0, W-1/H-1]\n    if not abs_scale:\n        x = tf.cast(x, 'float32')\n        y = tf.cast(y, 'float32')\n        x = 0.5 * ((x + 1.0) * tf.cast(max_x, 'float32'))\n        y = 0.5 * ((y + 1.0) * tf.cast(max_y, 'float32'))\n\n    # grab 4 nearest corner points for each (x_i, y_i)\n    x0 = tf.cast(tf.floor(x), 'int32')\n    y0 = tf.cast(tf.floor(y), 'int32')\n\n    # clip to range [0, H-1/W-1] to not violate input_feature boundaries\n    x0 = tf.clip_by_value(x0, zero, max_x)\n    y0 = tf.clip_by_value(y0, zero, max_y)\n\n    return get_pixel_value(input_feature, x0, y0)", "\n\ndef sample_2d_features(input_feature, x, y, max_disp):\n    \"\"\"\n    Sample input features at the normalized coordinates and around them with tolearance max_disp\n    The sampling is done identically for each channel of the input.\n\n    Mainly for cost_volume_at_contour_points\n\n    Input\n    -----\n    - input_feature: batch of images in (B, H, W, C) layout.\n    - grid: x, y which is the output of affine_grid_generator.\n\n    Output\n    -----\n    [b, N, C, (max_disp * 2 + 1)^2]\n    \"\"\"\n\n    input_feature = tf.cast(input_feature, dtype='float32')\n    H = tf.shape(input_feature)[1] - max_disp*2\n    W = tf.shape(input_feature)[2] - max_disp*2\n\n    max_y = tf.cast(H - 1, 'int32')\n    max_x = tf.cast(W - 1, 'int32')\n    zero = tf.zeros([], dtype='int32')\n\n    # rescale x and y to [0, W-1/H-1]\n    x = tf.cast(x, 'float32')\n    y = tf.cast(y, 'float32')\n    rescaled_x = 0.5 * ((x + 1.0) * tf.cast(max_x, 'float32'))\n    rescaled_y = 0.5 * ((y + 1.0) * tf.cast(max_y, 'float32'))\n    # add max_disp to account for the padding\n    rescaled_x = rescaled_x + max_disp\n    rescaled_y = rescaled_y + max_disp\n\n    # grab closest corner point to x and y\n    x0 = tf.cast(tf.math.round(rescaled_x), 'int32')\n    y0 = tf.cast(tf.math.round(rescaled_y), 'int32')\n\n    values_within_shift = []  # max_disp * 2 + 1, (B, N, C)\n    for i_offset in range(-max_disp, max_disp+1):\n      for j_offset in range(-max_disp, max_disp+1):\n        # clip to range [0, H-1/W-1] to not violate input_feature boundaries\n        shifted_x0 = tf.clip_by_value(x0+i_offset, zero, max_x)\n        shifted_y0 = tf.clip_by_value(y0+j_offset, zero, max_y)\n        values_within_shift.append( get_pixel_value(input_feature, shifted_x0, shifted_y0) )\n\n    stacked_values_within_shift = tf.stack(values_within_shift, axis=-1)\n\n    return stacked_values_within_shift", "\n\ndef normalize_1d_features(a_feature):\n    \"\"\"Normalizes feature tensor (e.g., before computing the cost volume).\n\n    Args:\n      a_feature: tf.tensor with dimensions [b, N, c, d^2]\n\n    Returns:\n      normalized_feature: tf.tensor with dimensions [b, N, c, d^2] normalized across the channel (c) axis\n    \"\"\"\n\n    mean_tensor, variance_tensor = tf.nn.moments(x=a_feature, axes=[-2], keepdims=True)\n\n    centered_feature = a_feature - mean_tensor\n    std_tensor = tf.sqrt(variance_tensor + 1e-16)\n    normalized_feature = centered_feature / std_tensor\n\n    return normalized_feature", "\n\ndef cost_volume_at_contour_points(a_feature1, a_feature2, cnt0, cnt1, max_displacement):\n    \"\"\"\n    Compute the cost volume (correlation) between 1D contour's feature and 2D image's feature at contour points only\n\n    Displace features2 up to max_displacement in any direction and compute the\n    per pixel cost of features1 and the displaced features2.\n\n    Args:\n    features1: tf.tensor of shape [b, h, w, c]  which means batch, height, width, channels\n    features2: tf.tensor of shape [b, h, w, c]  which means batch, height, width, channels\n    cnt0: tf.tensor of shape [b, N, 2]  which means batch, number of tracking points, x_y position. of the first frame\n    cnt1: tf.tensor of shape [b, N, 2]  which means batch, number of tracking points, x_y position. of the second frame\n    max_displacement: int, maximum displacement for cost volume computation.\n\n    Returns:\n    tf.tensor of shape [b, N, (2 * max_displacement + 1) ** 2] of costs for all displacements.\n    \"\"\"\n\n    sampled_feature1 = bilinear_sampler_1d(a_feature1, cnt0[:, :, 0], cnt0[:, :, 1])  # B x N x c\n    normalized_sampled_feature1 = normalize_1d_features(tf.expand_dims(sampled_feature1,axis=-1))\n\n    # Set maximum displacement and compute the number of image shifts.\n    _, height, width, _ = a_feature2.shape.as_list()\n    if max_displacement <= 0 or max_displacement >= height or max_displacement >= width:\n        raise ValueError(f'Max displacement of {max_displacement} is too large.')\n\n    max_disp = max_displacement\n\n    # Pad features2 such that shifts do not go out of bounds.\n    features2_padded = tf.pad(\n        tensor=a_feature2,\n        paddings=[[0, 0], [max_disp, max_disp], [max_disp, max_disp], [0, 0]],\n        mode='CONSTANT')\n    \n    # sample 2D features from features2\n    sampled_feature2 = sample_2d_features(features2_padded, cnt1[:,:,0], cnt1[:,:,1], max_disp)  # B x N x c x d^2\n    normalized_sampled_feature2 = normalize_1d_features(sampled_feature2)\n\n    # compute the cost volume [b, N, d^2] <-- [b, N, c] * [b, N, c, d^2]\n    cost_volume = tf.reduce_mean( normalized_sampled_feature1 * normalized_sampled_feature2, axis=2)\n\n    # for debugging, check the following\n    # tf.nn.moments(x=normalized_sampled_feature1, axes=[-2], keepdims=True)\n    # tf.nn.moments(x=normalized_sampled_feature2, axes=[-2], keepdims=True)\n\n    return cost_volume", ""]}
{"filename": "src/contour_flow_net.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nSimple interface for training and inference.\n\"\"\"", "Simple interface for training and inference.\n\"\"\"\n\nimport math\nimport sys\nimport time\nimport cv2\n\nimport gin\nimport tensorflow as tf", "import gin\nimport tensorflow as tf\n\nfrom src import contour_flow_model\nfrom src import tracking_model\nfrom src import uflow_utils\nfrom src import tracking_utils\n\n\n@gin.configurable\nclass ContourFlow(object):\n  \"\"\"Simple interface with infer and train methods.\"\"\"\n\n  def __init__(\n      self,\n      checkpoint_dir='',\n      summary_dir='',\n      optimizer='adam',\n      learning_rate=0.0001,  # Origianl 0.0002\n      only_forward=False,\n      level1_num_layers=3,\n      level1_num_filters=32,\n      level1_num_1x1=0,\n      dropout_rate=.25,\n      build_selfsup_transformations=None,\n      fb_sigma_teacher=0.003,\n      fb_sigma_student=0.03,\n      train_with_supervision=False,\n      train_with_gt_occlusions=False,\n      train_with_segmentations=False,\n      train_with_seg_points=False,\n      train_with_tracking_points=False,\n      smoothness_edge_weighting='gaussian',\n      teacher_image_version='original',\n      stop_gradient_mask=True,\n      selfsup_mask='gaussian',\n      normalize_before_cost_volume=True,\n      original_layer_sizes=False,\n      shared_flow_decoder=False,\n      channel_multiplier=1,\n      use_cost_volume=True,\n      use_feature_warp=True,\n      num_levels=5,\n      accumulate_flow=True,\n      occlusion_estimation='wang',\n      occ_weights=None,\n      occ_thresholds=None,\n      occ_clip_max=None,\n      smoothness_at_level=2,\n      use_bfloat16=False,\n      input_image_size=None\n  ):\n    \"\"\"Instantiate a UFlow model.\n\n    Args:\n      checkpoint_dir: str, location to checkpoint model\n      summary_dir: str, location to write tensorboard summary\n      optimizer: str, identifier of which optimizer to use\n      learning_rate: float, learning rate to use for training\n      only_forward: bool, if True, only infer flow in one direction\n      level1_num_layers: int, pwc architecture property\n      level1_num_filters: int, pwc architecture property\n      level1_num_1x1: int, pwc architecture property\n      dropout_rate: float, how much dropout to use with pwc net\n      build_selfsup_transformations: list of functions which transform the flow\n        predicted from the raw images to be in the frame of images transformed\n        by geometric_augmentation_fn\n      fb_sigma_teacher: float, controls how much forward-backward flow\n        consistency is needed by the teacher model in order to supervise the\n        student\n      fb_sigma_student: float, controls how much forward-backward consistency is\n        needed by the student model in order to not receive supervision from the\n        teacher model\n      train_with_supervision: bool, Whether to train with ground truth flow,\n        currently not supported\n      train_with_gt_occlusions: bool, if True, use ground truth occlusions\n        instead of predicted occlusions during training. Only works with Sintel\n        which has dense ground truth occlusions.\n      smoothness_edge_weighting: str, controls how smoothness penalty is\n        determined\n      teacher_image_version: str, which image to give to teacher model\n      stop_gradient_mask: bool, whether to stop the gradient of photometric loss\n        through the occlusion mask.\n      selfsup_mask: str, type of selfsupervision mask to use\n      normalize_before_cost_volume: bool, toggles pwc architecture property\n      original_layer_sizes: bool, toggles pwc architecture property\n      shared_flow_decoder: bool, toogles pwc architecutre property\n      channel_multiplier: int, channel factor to use in pwc\n      use_cost_volume: bool, toggles pwc architecture property\n      use_feature_warp: bool, toggles pwc architecture property\n      num_levels: int, how many pwc pyramid layers to use\n      accumulate_flow: bool, toggles pwc architecture property\n      occlusion_estimation: which type of occlusion estimation to use\n      occ_weights: dict of string -> float indicating how to weight occlusions\n      occ_thresholds: dict of str -> float indicating thresholds to apply for\n        occlusions\n      occ_clip_max: dict of string -> float indicating how to clip occlusion\n      smoothness_at_level: int, which level to compute smoothness on\n      use_bfloat16: bool, whether to run in bfloat16 mode.\n\n    Returns:\n      Uflow object instance.\n    \"\"\"\n    self._only_forward = only_forward\n    self._build_selfsup_transformations = build_selfsup_transformations\n    self._fb_sigma_teacher = fb_sigma_teacher\n    self._fb_sigma_student = fb_sigma_student\n    self._train_with_supervision = train_with_supervision\n    self._train_with_gt_occlusions = train_with_gt_occlusions\n    self._train_with_segmentations = train_with_segmentations\n    self._train_with_seg_points = train_with_seg_points\n    self._train_with_tracking_points = train_with_tracking_points\n    self._smoothness_edge_weighting = smoothness_edge_weighting\n    self._smoothness_at_level = smoothness_at_level\n    self._teacher_flow_model = None\n    self._teacher_feature_model = None\n    self._teacher_image_version = teacher_image_version\n    self._stop_gradient_mask = stop_gradient_mask\n    self._selfsup_mask = selfsup_mask\n    self._num_levels = num_levels\n\n    # self._feature_model = contour_flow_model.PWCFeaturePyramid(\n    #     level1_num_layers=level1_num_layers,\n    #     level1_num_filters=level1_num_filters,\n    #     level1_num_1x1=level1_num_1x1,\n    #     original_layer_sizes=original_layer_sizes,\n    #     num_levels=num_levels,\n    #     channel_multiplier=channel_multiplier,\n    #     pyramid_resolution='half',\n    #     use_bfloat16=use_bfloat16)\n    #\n    # self._flow_model = contour_flow_model.ContourFlowModel(\n    #     dropout_rate=dropout_rate,\n    #     normalize_before_cost_volume=normalize_before_cost_volume,\n    #     num_levels=num_levels,\n    #     use_feature_warp=use_feature_warp,\n    #     use_cost_volume=use_cost_volume,\n    #     channel_multiplier=channel_multiplier,\n    #     accumulate_flow=accumulate_flow,\n    #     use_bfloat16=use_bfloat16,\n    #     shared_flow_decoder=shared_flow_decoder)\n\n    self._tracking_model = tracking_model.PointSetTracker(num_iter=5, input_image_size=input_image_size)\n\n    # By default, the teacher flow and feature models are the same as\n    # the student flow and feature models.\n    # self._teacher_flow_model = self._flow_model\n    # self._teacher_feature_model = self._feature_model\n\n    self._learning_rate = learning_rate\n    self._optimizer_type = optimizer\n    self._make_or_reset_optimizer()\n\n    # Set up checkpointing.\n    self._make_or_reset_checkpoint()\n    self.update_checkpoint_dir(checkpoint_dir)\n\n    # Set up tensorboard log files.\n    self.summary_dir = summary_dir\n    if self.summary_dir:\n      self.writer = tf.compat.v1.summary.create_file_writer(summary_dir)\n      self.writer.set_as_default()\n\n    self._occlusion_estimation = occlusion_estimation\n\n    if occ_weights is None:\n      occ_weights = {\n          'fb_abs': 1.0,\n          'forward_collision': 1.0,\n          'backward_zero': 10.0\n      }\n    self._occ_weights = occ_weights\n\n    if occ_thresholds is None:\n      occ_thresholds = {\n          'fb_abs': 1.5,\n          'forward_collision': 0.4,\n          'backward_zero': 0.25\n      }\n    self._occ_thresholds = occ_thresholds\n\n    if occ_clip_max is None:\n      occ_clip_max = {'fb_abs': 10.0, 'forward_collision': 5.0}\n    self._occ_clip_max = occ_clip_max\n\n  def set_teacher_models(self, teacher_feature_model, teacher_flow_model):\n    self._teacher_feature_model = teacher_feature_model\n    self._teacher_flow_model = teacher_flow_model\n\n  @property\n  def feature_model(self):\n    return self._feature_model\n\n  @property\n  def flow_model(self):\n    return self._flow_model\n\n  def update_checkpoint_dir(self, checkpoint_dir):\n    \"\"\"Changes the checkpoint directory for saving and restoring.\"\"\"\n    self._manager = tf.train.CheckpointManager(\n        self._checkpoint, directory=checkpoint_dir, max_to_keep=1)\n\n  def restore(self, reset_optimizer=False, reset_global_step=False):\n    \"\"\"Restores a saved model from a checkpoint.\"\"\"\n    status = self._checkpoint.restore(self._manager.latest_checkpoint).expect_partial()  # expect_partial to silence warning\n    try:\n      status.assert_existing_objects_matched()\n    except AssertionError as e:\n      print('Error while attempting to restore UFlow models:', e)\n    if reset_optimizer:\n      self._make_or_reset_optimizer()\n      self._make_or_reset_checkpoint()\n    if reset_global_step:\n      tf.compat.v1.train.get_or_create_global_step().assign(0)\n\n  def save(self):\n    \"\"\"Saves a model checkpoint.\"\"\"\n    self._manager.save()\n\n  def _make_or_reset_optimizer(self):\n    if self._optimizer_type == 'adam':\n      self._optimizer = tf.compat.v1.train.AdamOptimizer(\n          self._learning_rate, name='Optimizer')\n    elif self._optimizer_type == 'sgd':\n      self._optimizer = tf.compat.v1.train.GradientDescentOptimizer(\n          self._learning_rate, name='Optimizer')\n    else:\n      raise ValueError('Optimizer \"{}\" not yet implemented.'.format(\n          self._optimizer_type))\n\n  @property\n  def optimizer(self):\n    return self._optimizer\n\n  def _make_or_reset_checkpoint(self):\n    self._checkpoint = tf.train.Checkpoint(\n        optimizer=self._optimizer,\n        # feature_model=self._feature_model,\n        # flow_model=self._flow_model,\n        tracking_model=self._tracking_model,\n        optimizer_step=tf.compat.v1.train.get_or_create_global_step())\n\n  # Use of tf.function breaks exporting the model, see b/138864493\n  def infer_no_tf_function(self,\n                           image1,\n                           image2,\n                           segmentation1,\n                           segmentation2,\n                           seg_point1=None,\n                           seg_point2=None,\n                           tracking_point1=None,\n                           tracking_point2=None,\n                           tracking_pos_emb=None,\n                           input_height=None,\n                           input_width=None,\n                           resize_flow_to_img_res=True,\n                           infer_occlusion=False,\n                           frame_index=None):\n    \"\"\"Infer flow for two images.\n\n    Args:\n      image1: tf.tensor of shape [height, width, 3].\n      image2: tf.tensor of shape [height, width, 3].\n      input_height: height at which the model should be applied if different\n        from image height.\n      input_width: width at which the model should be applied if different from\n        image width\n      resize_flow_to_img_res: bool, if True, return the flow resized to the same\n        resolution as (image1, image2). If False, return flow at the whatever\n        resolution the model natively predicts it.\n      infer_occlusion: bool, if True, return both flow and a soft occlusion\n        mask, else return just flow.\n\n    Returns:\n      Optical flow for each pixel in image1 pointing to image2.\n    \"\"\"\n\n    results = self.batch_infer_no_tf_function(\n        tf.stack([image1, image2])[None],\n        tf.stack([segmentation1, segmentation2])[None],\n        tf.stack([seg_point1, seg_point2])[None],\n        tf.stack([tracking_point1, tracking_point2])[None],\n        input_height=input_height,\n        input_width=input_width,\n        resize_flow_to_img_res=resize_flow_to_img_res,\n        infer_occlusion=infer_occlusion)\n\n    # Remove batch dimension from all results.\n    if isinstance(results, (tuple, list)):\n      return [x[0] for x in results]\n    else:\n      return results[0]\n\n\n  def infer_tracking_function(self, images,\n                                 ground_truth_segmentations,\n                                 seg_point1,\n                                 seg_point2,\n                                 prev_tracking_points,\n                                 tracking_pos_emb,\n                                 input_height=None,\n                                 input_width=None):\n\n    \"\"\"\n    :param images: tf.tensor of shape [batchsize, 2, height, width, 3].\n    :param ground_truth_segmentations:\n    :param prev_tracking_points:\n    :param frame_index:\n    :param input_height: height at which the model should be applied if different from image height.\n    :param input_width: width at which the model should be applied if different from image width\n    :return: tracking points with shape [batch, Number of points, 2]\n    \"\"\"\n\n    \"\"\"Infers flow from two images.\n\n        Returns:\n          Optical flow for each pixel in image1 pointing to image2.\n        \"\"\"\n\n    batch_size, seq_len, orig_height, orig_width, image_channels = images.shape.as_list()\n\n    if input_height is None:\n        input_height = orig_height\n    if input_width is None:\n        input_width = orig_width\n\n    # predict the location of tracking points in the next frame\n    prev_patch = images[:,0,:,:,:]\n    cur_patch = images[:,1,:,:,:]\n\n    prev_seg_points = seg_point1\n    cur_seg_points = seg_point2\n    prev_seg_points = tracking_utils.resize_seg_points(orig_height, orig_width, new_height=input_height, new_width=input_width, tracking_points=prev_seg_points)\n    cur_seg_points = tracking_utils.resize_seg_points(orig_height, orig_width, new_height=input_height, new_width=input_width, tracking_points=cur_seg_points)\n\n    # Get position embedding of the segmentation points\n    tracking_pos_emb = self._tracking_model(prev_seg_points)\n\n    # debug whether images are loaded correctly for prediction\n    # cv2.imwrite(f'debug_prev_patch.png', prev_patch[0].numpy()*255)\n    # cv2.imwrite(f'debug_cur_patch.png', cur_patch[0].numpy()*255)\n    # import pdb;pdb.set_trace()\n\n    # -------------------- To remove negative padding -------------------\n    # find the index in seg_points from where negative values start\n    # prev_seg_points_limit = tracking_utils.get_first_occurrence_indices(prev_seg_points[:,:,0], -0.1)[0]  # index 0 is fine since batch size is 1\n    # cur_seg_points_limit = tracking_utils.get_first_occurrence_indices(cur_seg_points[:,:,0], -0.1)[0]\n\n    forward_spatial_offset, backward_spatial_offset, saved_offset = self._tracking_model(prev_patch, cur_patch, prev_seg_points, cur_seg_points, tracking_pos_emb)\n\n    return forward_spatial_offset, backward_spatial_offset, tracking_pos_emb, input_height, input_width\n\n\n  def batch_infer_no_tf_function(self,\n                                 images,\n                                 segmentations,\n                                 seg_points=None,\n                                 tracking_points=None,\n                                 input_height=None,\n                                 input_width=None,\n                                 resize_flow_to_img_res=True,\n                                 infer_occlusion=False):\n    \"\"\"Infers flow from two images.\n\n    Args:\n      images: tf.tensor of shape [batchsize, 2, height, width, 3].\n      input_height: height at which the model should be applied if different\n        from image height.\n      input_width: width at which the model should be applied if different from\n        image width\n      resize_flow_to_img_res: bool, if True, return the flow resized to the same\n        resolution as (image1, image2). If False, return flow at the whatever\n        resolution the model natively predicts it.\n      infer_occlusion: bool, if True, return both flow and a soft occlusion\n        mask, else return just flow.\n\n    Returns:\n      Optical flow for each pixel in image1 pointing to image2.\n    \"\"\"\n\n    batch_size, seq_len, orig_height, orig_width, image_channels = images.shape.as_list(\n    )\n\n    if input_height is None:\n      input_height = orig_height\n    if input_width is None:\n      input_width = orig_width\n\n    # Ensure a feasible computation resolution. If specified size is not\n    # feasible with the model, change it to a slightly higher resolution.\n    divisible_by_num = pow(2.0, self._num_levels)\n    if (input_height % divisible_by_num != 0 or\n        input_width % divisible_by_num != 0):\n      print('Cannot process images at a resolution of ' + str(input_height) +\n            'x' + str(input_width) + ', since the height and/or width is not a '\n            'multiple of ' + str(divisible_by_num) + '.')\n      # compute a feasible resolution\n      input_height = int(\n          math.ceil(float(input_height) / divisible_by_num) * divisible_by_num)\n      input_width = int(\n          math.ceil(float(input_width) / divisible_by_num) * divisible_by_num)\n      print('Inference will be run at a resolution of ' + str(input_height) +\n            'x' + str(input_width) + '.')\n\n    # Resize images to desired input height and width.\n    if input_height != orig_height or input_width != orig_width:\n      images = uflow_utils.resize(\n          images, input_height, input_width, is_flow=False)\n\n    # Flatten images by folding sequence length into the batch dimension, apply\n    # the feature network and undo the flattening.\n    images_flattened = tf.reshape(\n        images,\n        [batch_size * seq_len, input_height, input_width, image_channels])\n    # noinspection PyCallingNonCallable\n\n    features_flattened = self._feature_model(\n        images_flattened, split_features_by_sample=False)\n    features = [\n        tf.reshape(f, [batch_size, seq_len] + f.shape.as_list()[1:])\n        for f in features_flattened\n    ]\n\n    features1, features2 = [[f[:, i] for f in features] for i in range(2)]\n\n    # split segmentations\n    segmentation1 = segmentations[:, 0, :, :, :]\n    segmentation2 = segmentations[:, 1, :, :, :]\n\n    # Compute flow in frame of image1.\n    # noinspection PyCallingNonCallable\n    flow = self._flow_model(features1, features2, segmentation1, segmentation2, training=False)[0]\n\n    if infer_occlusion:\n      # noinspection PyCallingNonCallable\n      flow_backward = self._flow_model(features2, features1, segmentation1, segmentation2, training=False)[0]\n      warps, valid_warp_masks, range_map, occlusion_mask = self.infer_occlusion(flow, flow_backward)\n      # originally, the shape is [1, 160, 160, 1] before the resize\n\n      warps = uflow_utils.resize(\n          warps, orig_height, orig_width, is_flow=False)\n\n      valid_warp_masks = uflow_utils.resize(\n          valid_warp_masks, orig_height, orig_width, is_flow=False)\n\n      occlusion_mask = uflow_utils.resize(\n          occlusion_mask, orig_height, orig_width, is_flow=False)\n\n      range_map = uflow_utils.resize(\n          range_map, orig_height, orig_width, is_flow=False)\n\n    # Resize and rescale flow to original resolution. This always needs to be\n    # done because flow is generated at a lower resolution.\n    if resize_flow_to_img_res:\n      flow = uflow_utils.resize(flow, orig_height, orig_width, is_flow=True)\n\n    if infer_occlusion:\n      return flow, warps, valid_warp_masks, range_map, occlusion_mask\n\n    return flow\n\n  @tf.function\n  def infer(self,\n            image1,\n            image2,\n            segmentation1,\n            segmentation2,\n            seg_point1,\n            seg_point2,\n            tracking_point1=None,\n            tracking_pos_emb=None,\n            input_height=None,\n            input_width=None,\n            resize_flow_to_img_res=True,\n            infer_occlusion=False):\n\n    return self.infer_tracking_function(tf.stack([image1, image2])[None],\n                                        tf.stack([segmentation1, segmentation2])[None],\n                                        seg_point1,\n                                        seg_point2,\n                                        tracking_point1,\n                                        tracking_pos_emb,\n                                        input_height=input_height,\n                                        input_width=input_width)\n\n    # return self.infer_no_tf_function(image1, image2, segmentation1, segmentation2, tracking_point1, tracking_point2, tracking_pos_emb,\n    #                                 input_height, input_width, resize_flow_to_img_res, infer_occlusion, frame_index)\n\n  # @tf.function\n  # def batch_infer(self,\n  #                 images,\n  #                 segmentations,\n  #                 input_height=None,\n  #                 input_width=None,\n  #                 resize_flow_to_img_res=True,\n  #                 infer_occlusion=False):\n  #\n  #   return self.batch_infer_no_tf_function(images, segmentations, input_height, input_width,\n  #                                          resize_flow_to_img_res,\n  #                                          infer_occlusion)\n\n  def infer_occlusion(self, flow_forward, flow_backward):\n    \"\"\"Gets a 'soft' occlusion mask from the forward and backward flow.\"\"\"\n\n    flows = {\n        (0, 1, 'inference'): [flow_forward],\n        (1, 0, 'inference'): [flow_backward],\n    }\n    warps, valid_warp_masks, range_maps_low_res, occlusion_masks, _, _ = uflow_utils.compute_warps_and_occlusion(\n        flows,\n        self._occlusion_estimation,\n        self._occ_weights,\n        self._occ_thresholds,\n        self._occ_clip_max,\n        occlusions_are_zeros=False)\n\n\n    warps = warps[(0, 1, 'inference')][0]\n    valid_warp_masks = valid_warp_masks[(0, 1, 'inference')][0]\n    occlusion_mask_forward = occlusion_masks[(0, 1, 'inference')][0]\n    range_maps_low_res = range_maps_low_res[(0, 1, 'inference')][0]\n\n    return warps, valid_warp_masks, range_maps_low_res, occlusion_mask_forward\n\n  def features_no_tf_function(self, image1, image2):\n    \"\"\"Runs the feature extractor portion of the model on image1 and image2.\"\"\"\n    images = tf.stack([image1, image2])\n    # noinspection PyCallingNonCallable\n    return self._feature_model(images, split_features_by_sample=True)\n\n  @tf.function\n  def features(self, image1, image2):\n    \"\"\"Runs the feature extractor portion of the model on image1 and image2.\"\"\"\n    return self.features_no_tf_function(image1, image2)\n\n  # -----------------------------------------------------------------------------------------------\n\n  def train_step_no_tf_function(self,\n                                batch,\n                                current_epoch,\n                                weights=None,\n                                plot_dir=None,\n                                distance_metrics=None,\n                                ground_truth_flow=None,\n                                ground_truth_valid=None,\n                                ground_truth_occlusions=None,\n                                ground_truth_segmentations=None,\n                                ground_truth_seg_points=None,\n                                ground_truth_tracking_points=None,\n                                images_without_photo_aug=None,\n                                occ_active=None):\n    \"\"\"Perform single gradient step.\"\"\"\n    if weights is None:\n      weights = {\n          'smooth2': 2.0,\n          'edge_constant': 100.0,\n          'census': 1.0,\n      }\n    else:\n      # Support values and callables (e.g. to compute weights from global step).\n      weights = {k: v() if callable(v) else v for k, v in weights.items()}\n\n    losses, gradients, variables, saved_offset_dict = self._loss_and_grad(\n        batch,\n        current_epoch,\n        weights,\n        plot_dir,\n        distance_metrics=distance_metrics,\n        ground_truth_flow=ground_truth_flow,\n        ground_truth_valid=ground_truth_valid,\n        ground_truth_occlusions=ground_truth_occlusions,\n        ground_truth_segmentations=ground_truth_segmentations,\n        ground_truth_seg_points=ground_truth_seg_points,\n        ground_truth_tracking_points=ground_truth_tracking_points,\n        images_without_photo_aug=images_without_photo_aug,\n        occ_active=occ_active)\n\n    self._optimizer.apply_gradients(\n        list(zip(gradients, variables)),\n        global_step=tf.compat.v1.train.get_or_create_global_step())\n\n    return losses, saved_offset_dict\n\n  @tf.function\n  def train_step(self,\n                 batch,\n                 current_epoch,\n                 weights=None,\n                 distance_metrics=None,\n                 ground_truth_flow=None,\n                 ground_truth_valid=None,\n                 ground_truth_occlusions=None,\n                 ground_truth_segmentations=None,\n                 ground_truth_seg_points=None,\n                 ground_truth_tracking_points=None,\n                 images_without_photo_aug=None,\n                 occ_active=None):\n    \"\"\"Performs a train step on the batch.\"\"\"\n\n    return self.train_step_no_tf_function(\n        batch,\n        current_epoch,\n        weights,\n        distance_metrics=distance_metrics,\n        ground_truth_flow=ground_truth_flow,\n        ground_truth_valid=ground_truth_valid,\n        ground_truth_occlusions=ground_truth_occlusions,\n        ground_truth_segmentations=ground_truth_segmentations,\n        ground_truth_seg_points=ground_truth_seg_points,\n        ground_truth_tracking_points=ground_truth_tracking_points,\n        images_without_photo_aug=images_without_photo_aug,\n        occ_active=occ_active)\n\n  def train(self,\n            data_it,\n            current_epoch,\n            num_steps,\n            weights=None,\n            progress_bar=True,\n            plot_dir=None,\n            distance_metrics=None,\n            occ_active=None):\n    \"\"\"Trains flow from a data iterator for a number of gradient steps.\n\n    Args:\n      data_it: tf.data.Iterator that produces tensors of shape [b,3,h,w,3].\n      num_steps: int, number of gradient steps to train for.\n      weights: dictionary with weight for each loss.\n      progress_bar: boolean flag for continuous printing of a progress bar.\n      plot_dir: location to plot results or None\n      distance_metrics: dictionary of which type of distance metric to use for\n        photometric losses\n      occ_active: dictionary of which occlusion types are active\n\n    Returns:\n      a dict that contains all losses.\n    \"\"\"\n\n    # Log dictionary for storing losses of this epoch.\n    log = dict()\n    # Support constant lr values and callables (for learning rate schedules).\n    if callable(self._learning_rate):\n      log['learning-rate'] = self._learning_rate()\n    else:\n      log['learning-rate'] = self._learning_rate\n\n    start_time_data = time.time()\n    for _, batch in zip(range(num_steps), data_it):\n      stop_time_data = time.time()\n\n      if progress_bar:\n        sys.stdout.write('.')\n        sys.stdout.flush()\n      # Split batch into images, occlusion masks, and ground truth flow.\n      images, labels = batch\n      ground_truth_flow = labels.get('flow_uv', None)\n      ground_truth_valid = labels.get('flow_valid', None)\n      ground_truth_occlusions = labels.get('occlusions', None)\n      images_without_photo_aug = labels.get('images_without_photo_aug', None)\n      ground_truth_segmentations = labels.get('segmentations', None)\n      ground_truth_seg_points = labels.get('segmentation_points', None)\n      ground_truth_tracking_points = labels.get('tracking_points', None)\n\n      # thresholding is necessary since segmentation is loaded blurrily\n      if ground_truth_segmentations is not None:\n          seg_threshold = 0\n          ground_truth_segmentations = tf.cast((ground_truth_segmentations > seg_threshold), dtype='float32') * 255\n\n      # use code below if thresholding is not used\n      # ground_truth_segmentations = tf.cast(ground_truth_segmentations, dtype='float32')\n\n      # -----------------------------------\n      # debug whether images are loaded correctly for prediction\n      # cv2.imwrite(f'uflow/debug_generated/debug_prev_patch.png', images[0,0,:,:,:].numpy()*255)\n      # cv2.imwrite(f'uflow/debug_generated/debug_cur_patch.png', images[0,1,:,:,:].numpy()*255)\n      # import pdb; pdb.set_trace()\n\n      # Debug whether segmentation was loaded correctly for training\n      # cv2.imwrite(f'test_segmentation_{seg_threshold}.png', ground_truth_segmentations.numpy()[0,0,:,:,0])\n      #\n      # print( tf.unique_with_counts(tf.reshape(ground_truth_segmentations, -1)) )\n      # import pdb; pdb.set_trace()\n      # -----------------------------------------\n\n      start_time_train_step = time.time()\n      # Use tf.function unless intermediate results have to be plotted.\n      if plot_dir is None:\n        # Perform a gradient step (optimized by tf.function).\n        losses, saved_offset_dict = self.train_step(\n            images,\n            current_epoch,\n            weights,\n            distance_metrics=distance_metrics,\n            ground_truth_flow=ground_truth_flow,\n            ground_truth_valid=ground_truth_valid,\n            ground_truth_occlusions=ground_truth_occlusions,\n            ground_truth_segmentations=ground_truth_segmentations,\n            ground_truth_seg_points=ground_truth_seg_points,\n            ground_truth_tracking_points=ground_truth_tracking_points,\n            images_without_photo_aug=images_without_photo_aug,\n            occ_active=occ_active)\n      else:\n        # Perform a gradient step without tf.function to allow plotting.\n        losses, saved_offset_dict = self.train_step_no_tf_function(\n            images,\n            current_epoch,\n            weights,\n            plot_dir,\n            distance_metrics=distance_metrics,\n            ground_truth_flow=ground_truth_flow,\n            ground_truth_valid=ground_truth_valid,\n            ground_truth_occlusions=ground_truth_occlusions,\n            ground_truth_seg_points=ground_truth_seg_points,\n            ground_truth_tracking_points=ground_truth_tracking_points,\n            images_without_photo_aug=images_without_photo_aug,\n            occ_active=occ_active)\n\n      stop_time_train_step = time.time()\n\n      log_update = losses\n      # Compute time in ms.\n      log_update['data-time'] = (stop_time_data - start_time_data) * 1000\n      log_update['train-time'] = (stop_time_train_step -\n                                  start_time_train_step) * 1000\n\n      # Log losses and times.\n      for key in log_update:\n        if key in log:\n          log[key].append(log_update[key])\n        else:\n          log[key] = [log_update[key]]\n        if self.summary_dir:\n          tf.summary.scalar(key, log[key])\n\n      # Set start time for data gathering to measure data pipeline efficiency.\n      start_time_data = time.time()\n\n    for key in log:\n      log[key] = tf.reduce_mean(input_tensor=log[key])\n\n    if progress_bar:\n      sys.stdout.write('\\n')\n      sys.stdout.flush()\n\n    return log, saved_offset_dict\n\n  def _loss_and_grad(self,\n                     batch,\n                     current_epoch,\n                     weights,\n                     plot_dir=None,\n                     distance_metrics=None,\n                     ground_truth_flow=None,\n                     ground_truth_valid=None,\n                     ground_truth_occlusions=None,\n                     ground_truth_segmentations=None,\n                     ground_truth_seg_points=None,\n                     ground_truth_tracking_points=None,\n                     images_without_photo_aug=None,\n                     occ_active=None):\n    \"\"\"Apply the model on the data in batch and compute the loss.\n\n    Args:\n      batch: tf.tensor of shape [b, seq, h, w, c] that holds a batch of image\n        sequences.\n      weights: dictionary with float entries per loss.\n      plot_dir: str, directory to plot images\n      distance_metrics: dict, which distance metrics to use,\n      ground_truth_flow: Tensor, optional ground truth flow for first image\n      ground_truth_valid: Tensor, indicates locations where gt flow is valid\n      ground_truth_occlusions: Tensor, optional ground truth occlusions for\n        computing loss. If None, predicted occlusions will be used.\n      images_without_photo_aug: optional images without any photometric\n        augmentation applied. Will be used for computing photometric losses if\n        provided.\n      occ_active: optional dict indicating which occlusion methods are active\n\n    Returns:\n      A tuple consisting of a tf.scalar that represents the total loss for the\n      current batch, a list of gradients, and a list of the respective\n      variables.\n    \"\"\"\n\n    # with tf.GradientTape() as tape:\n    #   losses = self.compute_loss(\n    #       batch,\n    #       weights,\n    #       plot_dir,\n    #       distance_metrics=distance_metrics,\n    #       ground_truth_flow=ground_truth_flow,\n    #       ground_truth_valid=ground_truth_valid,\n    #       ground_truth_occlusions=ground_truth_occlusions,\n    #       ground_truth_segmentations=ground_truth_segmentations,\n    #       ground_truth_tracking_points=ground_truth_tracking_points,\n    #       images_without_photo_aug=images_without_photo_aug,\n    #       occ_active=occ_active)\n    #\n    # variables = (\n    #     self._feature_model.trainable_variables +\n    #     self._flow_model.trainable_variables)\n    # grads = tape.gradient(losses['total-loss'], variables)\n\n    # This is where to add the tracking model\n    with tf.GradientTape() as tape:\n      losses, saved_offset_dict = self.compute_loss_tracking(batch,\n                                                              current_epoch,\n                                                              ground_truth_seg_points=ground_truth_seg_points,\n                                                              ground_truth_tracking_points=ground_truth_tracking_points)\n\n    # print(self._tracking_model.local_alignment.summary())\n    variables = ( self._tracking_model.trainable_variables)\n    grads = tape.gradient(losses['total-loss'], variables)\n\n    return losses, grads, variables, saved_offset_dict\n\n  def compute_loss_tracking(self,\n                           batch,\n                           current_epoch,\n                           ground_truth_seg_points=None,\n                           ground_truth_tracking_points=None):\n\n      # determine seq length\n      max_seq_len = int(batch.shape[1])\n      # seq_len = tracking_utils.linear_seq_increase(current_epoch, max_seq_len)\n\n      # Get position embedding of the segmentation points\n      prev_seg_points = ground_truth_seg_points[:, 0, :, :]\n      cur_seg_points = ground_truth_seg_points[:, 1, :, :]\n      pos_emb = self._tracking_model(prev_seg_points)\n\n      gt_prev_id_assignments = ground_truth_tracking_points[:, 0, :, :]\n      gt_cur_id_assignments = ground_truth_tracking_points[:, 1, :, :]\n\n      # predict the location of tracking points in the next frame\n      prev_patch = batch[:, 0, :, :, :]\n      cur_patch = batch[:, 1, :, :, :]\n\n      # -------------------- To remove negative padding -------------------\n      # prev_seg_points_limit = tracking_utils.get_first_occurrence_indices(prev_seg_points[:, :, 0], -0.1)\n      # cur_seg_points_limit = tracking_utils.get_first_occurrence_indices(cur_seg_points[:, :, 0], -0.1)\n\n      # get the biggest value in the tensor\n      # prev_seg_points_limit = tf.math.reduce_max(prev_seg_points_limit)\n      # cur_seg_points_limit = tf.math.reduce_max(cur_seg_points_limit)\n      # -------------------------------------------------------------------\n      # find dense correspondence of all segmentation points, whereas gt id assignments are for a subset of points\n      forward_spatial_offset, backward_spatial_offset, saved_offset = self._tracking_model(prev_patch, cur_patch, prev_seg_points, cur_seg_points, pos_emb)\n\n      # forward_corr_2d_loss = tracking_utils.corr_2d_loss( gt_prev_id_assignments, gt_cur_id_assignments, forward_corr_2d_matrix)\n      # backward_corr_2d_loss = tracking_utils.corr_2d_loss( gt_cur_id_assignments, gt_prev_id_assignments, backward_corr_2d_matrix)\n      # corr_cycle_consistency_loss = tracking_utils.corr_cycle_consistency(forward_corr_2d_matrix, backward_corr_2d_matrix)\n\n      # process forward_id_assign by contour length\n      # prev_seg_points_limit = tracking_utils.get_first_occurrence_indices(prev_seg_points[:, :, 0], -0.1)\n      # cur_seg_points_limit = tracking_utils.get_first_occurrence_indices(cur_seg_points[:, :, 0], -0.1)\n      # forward_id_assign = tracking_utils.fit_id_assignments_to_next_contour(forward_id_assign, tf.expand_dims(prev_seg_points[:,:,-1], axis=-1), cur_seg_points_limit)\n      # backward_id_assign = tracking_utils.fit_id_assignments_to_next_contour(backward_id_assign, tf.expand_dims(cur_seg_points[:,:,-1], axis=-1), prev_seg_points_limit)\n\n      # total_forward_matching_points_loss = tracking_utils.matching_contour_points_loss(gt_prev_id_assignments, gt_cur_id_assignments, forward_id_assign)\n      # total_backward_matching_points_loss = tracking_utils.matching_contour_points_loss(gt_cur_id_assignments, gt_prev_id_assignments, backward_id_assign)\n      # total_cycle_consistency_assign_loss = tracking_utils.cycle_consistency_assign_loss(forward_id_assign, backward_id_assign, tf.expand_dims(prev_seg_points[:,:,-1], axis=-1),  tf.expand_dims(cur_seg_points[:,:,-1], axis=-1))\n      # total_points_order_loss = tracking_utils.contour_points_order_loss(forward_id_assign)\n\n      # total_forward_matching_points_spatial_metric = tracking_utils.matching_contour_points_spatial_metric(cur_seg_points[:,:,:2], gt_prev_id_assignments, gt_cur_id_assignments, forward_id_assign)\n      pred_cur_tracking_points = prev_seg_points[:,:,:2] + forward_spatial_offset\n      forward_spatial_points_loss = tracking_utils.matching_spatial_points_loss(gt_prev_id_assignments, gt_cur_id_assignments,\n                                                                                      tf.cast(cur_seg_points[:,:,:2], dtype=pred_cur_tracking_points.dtype), pred_cur_tracking_points)\n\n      cycle_consistency_spatial_loss = tracking_utils.cycle_consistency_spatial_loss(prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], cur_seg_points[:,:,:2], cur_seg_points[:,:,-1], forward_spatial_offset, backward_spatial_offset)\n\n      forward_tracker_photometric_loss = tracking_utils.tracker_unsupervised_photometric_loss(prev_patch, cur_patch, prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], pred_cur_tracking_points)\n\n      backward_tracker_photometric_loss = tracking_utils.tracker_unsupervised_photometric_loss(cur_patch, prev_patch, cur_seg_points[:,:,:2], cur_seg_points[:,:,-1], cur_seg_points[:,:,:2] + backward_spatial_offset)\n\n      # forward_tracker_census_loss = tracking_utils.tracker_unsupervised_census_loss(prev_patch, cur_patch, prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], pred_cur_tracking_points)\n\n      # backward_tracker_census_loss = tracking_utils.tracker_unsupervised_census_loss(cur_patch, prev_patch, cur_seg_points[:,:,:2], cur_seg_points[:,:,-1], cur_seg_points[:,:,:2] + backward_spatial_offset)\n\n      forward_linear_spring_loss, forward_normal_force_loss = tracking_utils.mechanical_loss_from_offsets(prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], forward_spatial_offset)\n      backward_linear_spring_loss, backward_normal_force_loss = tracking_utils.mechanical_loss_from_offsets(cur_seg_points[:,:,:2], cur_seg_points[:,:,-1], backward_spatial_offset)\n\n      # snake_tension_loss, snake_stiffness_loss = tracking_utils.snake_loss_from_offsets(prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], forward_spatial_offset)\n\n      saved_offset_dict = {0: saved_offset}  # key for sequence number, only one key 0 if seq_num=2\n\n      total_loss = cycle_consistency_spatial_loss + forward_normal_force_loss\n\n      losses = {'total-loss' : total_loss,\n                'forward_spatial_points_loss': forward_spatial_points_loss,\n                'forward_tracker_photometric_loss': forward_tracker_photometric_loss, 'backward_tracker_photometric_loss': backward_tracker_photometric_loss,\n                # 'forward_tracker_census_loss': forward_tracker_census_loss, 'backward_tracker_census_loss': backward_tracker_census_loss,\n                'cycle_consistency_spatial_loss': cycle_consistency_spatial_loss,\n                # 'snake_tension_loss': snake_tension_loss, 'snake_stiffness_loss': snake_stiffness_loss,\n                'forward_linear_spring_loss': forward_linear_spring_loss, 'forward_normal_force_loss': forward_normal_force_loss,\n                'backward_linear_spring_loss': backward_linear_spring_loss, 'backward_normal_force_loss': backward_normal_force_loss}\n\n      return losses, saved_offset_dict\n\n  def compute_loss(self,\n                   batch,\n                   weights,\n                   plot_dir=None,\n                   distance_metrics=None,\n                   ground_truth_flow=None,\n                   ground_truth_valid=None,\n                   ground_truth_occlusions=None,\n                   ground_truth_segmentations=None,\n                   ground_truth_tracking_points=None,\n                   images_without_photo_aug=None,\n                   occ_active=None):\n    \"\"\"Applies the model and computes losses for a batch of image sequences.\"\"\"\n    # Compute only a supervised loss.\n    if self._train_with_supervision:\n      if ground_truth_flow is None:\n        raise ValueError('Need ground truth flow to compute supervised loss.')\n      flows = uflow_utils.compute_flow_for_supervised_loss(\n          self._feature_model, self._flow_model, batch=batch, training=True)\n      losses = uflow_utils.supervised_loss(weights, ground_truth_flow,\n                                           ground_truth_valid, flows)\n      losses = {key + '-loss': losses[key] for key in losses}\n      return losses\n\n    # Use possibly augmented images if non augmented version is not provided.\n    if images_without_photo_aug is None:\n      images_without_photo_aug = batch\n\n    flows, selfsup_transform_fns = uflow_utils.compute_features_and_flow(\n        self._feature_model,\n        self._flow_model,\n        batch=batch,\n        batch_without_aug=images_without_photo_aug,\n        training=True,\n        build_selfsup_transformations=self._build_selfsup_transformations,\n        teacher_feature_model=self._teacher_feature_model,\n        teacher_flow_model=self._teacher_flow_model,\n        teacher_image_version=self._teacher_image_version,\n        ground_truth_segmentations=ground_truth_segmentations,\n        ground_truth_tracking_points=ground_truth_tracking_points\n    )\n\n    # Prepare images and contours for unsupervised loss (prefer unaugmented images).\n    seq_len = int(batch.shape[1])\n    images = {i: images_without_photo_aug[:, i] for i in range(seq_len)}\n    contours = {i: ground_truth_segmentations[:, i] for i in range(seq_len)}\n\n    # Warp stuff and compute occlusion.\n    warps, valid_warp_masks, _, not_occluded_masks, fb_sq_diff, fb_sum_sq = uflow_utils.compute_warps_and_occlusion(\n                                                                            flows,\n                                                                            occlusion_estimation=self._occlusion_estimation,\n                                                                            occ_weights=self._occ_weights,\n                                                                            occ_thresholds=self._occ_thresholds,\n                                                                            occ_clip_max=self._occ_clip_max,\n                                                                            occlusions_are_zeros=True,\n                                                                            occ_active=occ_active)\n\n    # Warp images and features.\n    warped_images = uflow_utils.apply_warps_stop_grad(images, warps, level=0)\n\n    # Warp contours\n    assert len(images) == len(contours)\n\n    # dilate contours and get image at the contour regions\n    img_contours = {}\n    for dict_i in range(len(images)):\n        # img_contours[dict_i] = images[dict_i] * contours[dict_i]  # contour image without dilation\n\n        a_depth = contours[dict_i].shape[-1]\n        dilation_filter = tf.zeros( [1, 1, a_depth], tf.float32)  # why zeros? https://stackoverflow.com/questions/54686895/tensorflow-dilation-behave-differently-than-morphological-dilation\n        dilated_contour = tf.nn.dilation2d(input=contours[dict_i], filters=dilation_filter, strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=[1, 1, 1, 1])\n        img_contours[dict_i] = images[dict_i] * dilated_contour\n\n        # debug whether images are loaded correctly for training\n        # cv2.imwrite(f'debug_dilated_contour.png', dilated_contour.numpy()[0, :, :, :])\n        # cv2.imwrite(f'debug_img_{dict_i}.png', images[dict_i].numpy()[0, :, :, :] * 255)\n        # cv2.imwrite(f'debug_contour_{dict_i}.png', contours[dict_i].numpy()[0,:,:,:])\n        # cv2.imwrite(f'debug_contour_img_{dict_i}.png', img_contours[dict_i].numpy()[0, :, :, :])\n        # print( tf.unique_with_counts(tf.reshape(dilated_contour, -1)) )\n\n    warped_contours = uflow_utils.apply_warps_stop_grad(img_contours, warps, level=0)\n\n    # cv2.imwrite(f'debug_warped_contour_0.png', warped_contours[(0, 1, 'original-teacher')].numpy()[0, :, :, :])\n    # cv2.imwrite(f'debug_warped_contour_1.png', warped_contours[(1, 0, 'original-teacher')].numpy()[0, :, :, :])\n    # import pdb; pdb.set_trace()\n\n    # Compute losses.\n    losses = uflow_utils.compute_loss(\n        weights=weights,\n        images=images,\n        contours=img_contours,\n        flows=flows,\n        warps=warps,\n        valid_warp_masks=valid_warp_masks,\n        not_occluded_masks=not_occluded_masks,\n        fb_sq_diff=fb_sq_diff,\n        fb_sum_sq=fb_sum_sq,\n        warped_images=warped_images,\n        warped_contours=warped_contours,\n        only_forward=self._only_forward,\n        selfsup_transform_fns=selfsup_transform_fns,\n        fb_sigma_teacher=self._fb_sigma_teacher,\n        fb_sigma_student=self._fb_sigma_student,\n        plot_dir=plot_dir,\n        distance_metrics=distance_metrics,\n        smoothness_edge_weighting=self._smoothness_edge_weighting,\n        stop_gradient_mask=self._stop_gradient_mask,\n        selfsup_mask=self._selfsup_mask,\n        ground_truth_occlusions=ground_truth_occlusions,\n        smoothness_at_level=self._smoothness_at_level)\n    losses = {key + '-loss': losses[key] for key in losses}\n\n    return losses", "\n@gin.configurable\nclass ContourFlow(object):\n  \"\"\"Simple interface with infer and train methods.\"\"\"\n\n  def __init__(\n      self,\n      checkpoint_dir='',\n      summary_dir='',\n      optimizer='adam',\n      learning_rate=0.0001,  # Origianl 0.0002\n      only_forward=False,\n      level1_num_layers=3,\n      level1_num_filters=32,\n      level1_num_1x1=0,\n      dropout_rate=.25,\n      build_selfsup_transformations=None,\n      fb_sigma_teacher=0.003,\n      fb_sigma_student=0.03,\n      train_with_supervision=False,\n      train_with_gt_occlusions=False,\n      train_with_segmentations=False,\n      train_with_seg_points=False,\n      train_with_tracking_points=False,\n      smoothness_edge_weighting='gaussian',\n      teacher_image_version='original',\n      stop_gradient_mask=True,\n      selfsup_mask='gaussian',\n      normalize_before_cost_volume=True,\n      original_layer_sizes=False,\n      shared_flow_decoder=False,\n      channel_multiplier=1,\n      use_cost_volume=True,\n      use_feature_warp=True,\n      num_levels=5,\n      accumulate_flow=True,\n      occlusion_estimation='wang',\n      occ_weights=None,\n      occ_thresholds=None,\n      occ_clip_max=None,\n      smoothness_at_level=2,\n      use_bfloat16=False,\n      input_image_size=None\n  ):\n    \"\"\"Instantiate a UFlow model.\n\n    Args:\n      checkpoint_dir: str, location to checkpoint model\n      summary_dir: str, location to write tensorboard summary\n      optimizer: str, identifier of which optimizer to use\n      learning_rate: float, learning rate to use for training\n      only_forward: bool, if True, only infer flow in one direction\n      level1_num_layers: int, pwc architecture property\n      level1_num_filters: int, pwc architecture property\n      level1_num_1x1: int, pwc architecture property\n      dropout_rate: float, how much dropout to use with pwc net\n      build_selfsup_transformations: list of functions which transform the flow\n        predicted from the raw images to be in the frame of images transformed\n        by geometric_augmentation_fn\n      fb_sigma_teacher: float, controls how much forward-backward flow\n        consistency is needed by the teacher model in order to supervise the\n        student\n      fb_sigma_student: float, controls how much forward-backward consistency is\n        needed by the student model in order to not receive supervision from the\n        teacher model\n      train_with_supervision: bool, Whether to train with ground truth flow,\n        currently not supported\n      train_with_gt_occlusions: bool, if True, use ground truth occlusions\n        instead of predicted occlusions during training. Only works with Sintel\n        which has dense ground truth occlusions.\n      smoothness_edge_weighting: str, controls how smoothness penalty is\n        determined\n      teacher_image_version: str, which image to give to teacher model\n      stop_gradient_mask: bool, whether to stop the gradient of photometric loss\n        through the occlusion mask.\n      selfsup_mask: str, type of selfsupervision mask to use\n      normalize_before_cost_volume: bool, toggles pwc architecture property\n      original_layer_sizes: bool, toggles pwc architecture property\n      shared_flow_decoder: bool, toogles pwc architecutre property\n      channel_multiplier: int, channel factor to use in pwc\n      use_cost_volume: bool, toggles pwc architecture property\n      use_feature_warp: bool, toggles pwc architecture property\n      num_levels: int, how many pwc pyramid layers to use\n      accumulate_flow: bool, toggles pwc architecture property\n      occlusion_estimation: which type of occlusion estimation to use\n      occ_weights: dict of string -> float indicating how to weight occlusions\n      occ_thresholds: dict of str -> float indicating thresholds to apply for\n        occlusions\n      occ_clip_max: dict of string -> float indicating how to clip occlusion\n      smoothness_at_level: int, which level to compute smoothness on\n      use_bfloat16: bool, whether to run in bfloat16 mode.\n\n    Returns:\n      Uflow object instance.\n    \"\"\"\n    self._only_forward = only_forward\n    self._build_selfsup_transformations = build_selfsup_transformations\n    self._fb_sigma_teacher = fb_sigma_teacher\n    self._fb_sigma_student = fb_sigma_student\n    self._train_with_supervision = train_with_supervision\n    self._train_with_gt_occlusions = train_with_gt_occlusions\n    self._train_with_segmentations = train_with_segmentations\n    self._train_with_seg_points = train_with_seg_points\n    self._train_with_tracking_points = train_with_tracking_points\n    self._smoothness_edge_weighting = smoothness_edge_weighting\n    self._smoothness_at_level = smoothness_at_level\n    self._teacher_flow_model = None\n    self._teacher_feature_model = None\n    self._teacher_image_version = teacher_image_version\n    self._stop_gradient_mask = stop_gradient_mask\n    self._selfsup_mask = selfsup_mask\n    self._num_levels = num_levels\n\n    # self._feature_model = contour_flow_model.PWCFeaturePyramid(\n    #     level1_num_layers=level1_num_layers,\n    #     level1_num_filters=level1_num_filters,\n    #     level1_num_1x1=level1_num_1x1,\n    #     original_layer_sizes=original_layer_sizes,\n    #     num_levels=num_levels,\n    #     channel_multiplier=channel_multiplier,\n    #     pyramid_resolution='half',\n    #     use_bfloat16=use_bfloat16)\n    #\n    # self._flow_model = contour_flow_model.ContourFlowModel(\n    #     dropout_rate=dropout_rate,\n    #     normalize_before_cost_volume=normalize_before_cost_volume,\n    #     num_levels=num_levels,\n    #     use_feature_warp=use_feature_warp,\n    #     use_cost_volume=use_cost_volume,\n    #     channel_multiplier=channel_multiplier,\n    #     accumulate_flow=accumulate_flow,\n    #     use_bfloat16=use_bfloat16,\n    #     shared_flow_decoder=shared_flow_decoder)\n\n    self._tracking_model = tracking_model.PointSetTracker(num_iter=5, input_image_size=input_image_size)\n\n    # By default, the teacher flow and feature models are the same as\n    # the student flow and feature models.\n    # self._teacher_flow_model = self._flow_model\n    # self._teacher_feature_model = self._feature_model\n\n    self._learning_rate = learning_rate\n    self._optimizer_type = optimizer\n    self._make_or_reset_optimizer()\n\n    # Set up checkpointing.\n    self._make_or_reset_checkpoint()\n    self.update_checkpoint_dir(checkpoint_dir)\n\n    # Set up tensorboard log files.\n    self.summary_dir = summary_dir\n    if self.summary_dir:\n      self.writer = tf.compat.v1.summary.create_file_writer(summary_dir)\n      self.writer.set_as_default()\n\n    self._occlusion_estimation = occlusion_estimation\n\n    if occ_weights is None:\n      occ_weights = {\n          'fb_abs': 1.0,\n          'forward_collision': 1.0,\n          'backward_zero': 10.0\n      }\n    self._occ_weights = occ_weights\n\n    if occ_thresholds is None:\n      occ_thresholds = {\n          'fb_abs': 1.5,\n          'forward_collision': 0.4,\n          'backward_zero': 0.25\n      }\n    self._occ_thresholds = occ_thresholds\n\n    if occ_clip_max is None:\n      occ_clip_max = {'fb_abs': 10.0, 'forward_collision': 5.0}\n    self._occ_clip_max = occ_clip_max\n\n  def set_teacher_models(self, teacher_feature_model, teacher_flow_model):\n    self._teacher_feature_model = teacher_feature_model\n    self._teacher_flow_model = teacher_flow_model\n\n  @property\n  def feature_model(self):\n    return self._feature_model\n\n  @property\n  def flow_model(self):\n    return self._flow_model\n\n  def update_checkpoint_dir(self, checkpoint_dir):\n    \"\"\"Changes the checkpoint directory for saving and restoring.\"\"\"\n    self._manager = tf.train.CheckpointManager(\n        self._checkpoint, directory=checkpoint_dir, max_to_keep=1)\n\n  def restore(self, reset_optimizer=False, reset_global_step=False):\n    \"\"\"Restores a saved model from a checkpoint.\"\"\"\n    status = self._checkpoint.restore(self._manager.latest_checkpoint).expect_partial()  # expect_partial to silence warning\n    try:\n      status.assert_existing_objects_matched()\n    except AssertionError as e:\n      print('Error while attempting to restore UFlow models:', e)\n    if reset_optimizer:\n      self._make_or_reset_optimizer()\n      self._make_or_reset_checkpoint()\n    if reset_global_step:\n      tf.compat.v1.train.get_or_create_global_step().assign(0)\n\n  def save(self):\n    \"\"\"Saves a model checkpoint.\"\"\"\n    self._manager.save()\n\n  def _make_or_reset_optimizer(self):\n    if self._optimizer_type == 'adam':\n      self._optimizer = tf.compat.v1.train.AdamOptimizer(\n          self._learning_rate, name='Optimizer')\n    elif self._optimizer_type == 'sgd':\n      self._optimizer = tf.compat.v1.train.GradientDescentOptimizer(\n          self._learning_rate, name='Optimizer')\n    else:\n      raise ValueError('Optimizer \"{}\" not yet implemented.'.format(\n          self._optimizer_type))\n\n  @property\n  def optimizer(self):\n    return self._optimizer\n\n  def _make_or_reset_checkpoint(self):\n    self._checkpoint = tf.train.Checkpoint(\n        optimizer=self._optimizer,\n        # feature_model=self._feature_model,\n        # flow_model=self._flow_model,\n        tracking_model=self._tracking_model,\n        optimizer_step=tf.compat.v1.train.get_or_create_global_step())\n\n  # Use of tf.function breaks exporting the model, see b/138864493\n  def infer_no_tf_function(self,\n                           image1,\n                           image2,\n                           segmentation1,\n                           segmentation2,\n                           seg_point1=None,\n                           seg_point2=None,\n                           tracking_point1=None,\n                           tracking_point2=None,\n                           tracking_pos_emb=None,\n                           input_height=None,\n                           input_width=None,\n                           resize_flow_to_img_res=True,\n                           infer_occlusion=False,\n                           frame_index=None):\n    \"\"\"Infer flow for two images.\n\n    Args:\n      image1: tf.tensor of shape [height, width, 3].\n      image2: tf.tensor of shape [height, width, 3].\n      input_height: height at which the model should be applied if different\n        from image height.\n      input_width: width at which the model should be applied if different from\n        image width\n      resize_flow_to_img_res: bool, if True, return the flow resized to the same\n        resolution as (image1, image2). If False, return flow at the whatever\n        resolution the model natively predicts it.\n      infer_occlusion: bool, if True, return both flow and a soft occlusion\n        mask, else return just flow.\n\n    Returns:\n      Optical flow for each pixel in image1 pointing to image2.\n    \"\"\"\n\n    results = self.batch_infer_no_tf_function(\n        tf.stack([image1, image2])[None],\n        tf.stack([segmentation1, segmentation2])[None],\n        tf.stack([seg_point1, seg_point2])[None],\n        tf.stack([tracking_point1, tracking_point2])[None],\n        input_height=input_height,\n        input_width=input_width,\n        resize_flow_to_img_res=resize_flow_to_img_res,\n        infer_occlusion=infer_occlusion)\n\n    # Remove batch dimension from all results.\n    if isinstance(results, (tuple, list)):\n      return [x[0] for x in results]\n    else:\n      return results[0]\n\n\n  def infer_tracking_function(self, images,\n                                 ground_truth_segmentations,\n                                 seg_point1,\n                                 seg_point2,\n                                 prev_tracking_points,\n                                 tracking_pos_emb,\n                                 input_height=None,\n                                 input_width=None):\n\n    \"\"\"\n    :param images: tf.tensor of shape [batchsize, 2, height, width, 3].\n    :param ground_truth_segmentations:\n    :param prev_tracking_points:\n    :param frame_index:\n    :param input_height: height at which the model should be applied if different from image height.\n    :param input_width: width at which the model should be applied if different from image width\n    :return: tracking points with shape [batch, Number of points, 2]\n    \"\"\"\n\n    \"\"\"Infers flow from two images.\n\n        Returns:\n          Optical flow for each pixel in image1 pointing to image2.\n        \"\"\"\n\n    batch_size, seq_len, orig_height, orig_width, image_channels = images.shape.as_list()\n\n    if input_height is None:\n        input_height = orig_height\n    if input_width is None:\n        input_width = orig_width\n\n    # predict the location of tracking points in the next frame\n    prev_patch = images[:,0,:,:,:]\n    cur_patch = images[:,1,:,:,:]\n\n    prev_seg_points = seg_point1\n    cur_seg_points = seg_point2\n    prev_seg_points = tracking_utils.resize_seg_points(orig_height, orig_width, new_height=input_height, new_width=input_width, tracking_points=prev_seg_points)\n    cur_seg_points = tracking_utils.resize_seg_points(orig_height, orig_width, new_height=input_height, new_width=input_width, tracking_points=cur_seg_points)\n\n    # Get position embedding of the segmentation points\n    tracking_pos_emb = self._tracking_model(prev_seg_points)\n\n    # debug whether images are loaded correctly for prediction\n    # cv2.imwrite(f'debug_prev_patch.png', prev_patch[0].numpy()*255)\n    # cv2.imwrite(f'debug_cur_patch.png', cur_patch[0].numpy()*255)\n    # import pdb;pdb.set_trace()\n\n    # -------------------- To remove negative padding -------------------\n    # find the index in seg_points from where negative values start\n    # prev_seg_points_limit = tracking_utils.get_first_occurrence_indices(prev_seg_points[:,:,0], -0.1)[0]  # index 0 is fine since batch size is 1\n    # cur_seg_points_limit = tracking_utils.get_first_occurrence_indices(cur_seg_points[:,:,0], -0.1)[0]\n\n    forward_spatial_offset, backward_spatial_offset, saved_offset = self._tracking_model(prev_patch, cur_patch, prev_seg_points, cur_seg_points, tracking_pos_emb)\n\n    return forward_spatial_offset, backward_spatial_offset, tracking_pos_emb, input_height, input_width\n\n\n  def batch_infer_no_tf_function(self,\n                                 images,\n                                 segmentations,\n                                 seg_points=None,\n                                 tracking_points=None,\n                                 input_height=None,\n                                 input_width=None,\n                                 resize_flow_to_img_res=True,\n                                 infer_occlusion=False):\n    \"\"\"Infers flow from two images.\n\n    Args:\n      images: tf.tensor of shape [batchsize, 2, height, width, 3].\n      input_height: height at which the model should be applied if different\n        from image height.\n      input_width: width at which the model should be applied if different from\n        image width\n      resize_flow_to_img_res: bool, if True, return the flow resized to the same\n        resolution as (image1, image2). If False, return flow at the whatever\n        resolution the model natively predicts it.\n      infer_occlusion: bool, if True, return both flow and a soft occlusion\n        mask, else return just flow.\n\n    Returns:\n      Optical flow for each pixel in image1 pointing to image2.\n    \"\"\"\n\n    batch_size, seq_len, orig_height, orig_width, image_channels = images.shape.as_list(\n    )\n\n    if input_height is None:\n      input_height = orig_height\n    if input_width is None:\n      input_width = orig_width\n\n    # Ensure a feasible computation resolution. If specified size is not\n    # feasible with the model, change it to a slightly higher resolution.\n    divisible_by_num = pow(2.0, self._num_levels)\n    if (input_height % divisible_by_num != 0 or\n        input_width % divisible_by_num != 0):\n      print('Cannot process images at a resolution of ' + str(input_height) +\n            'x' + str(input_width) + ', since the height and/or width is not a '\n            'multiple of ' + str(divisible_by_num) + '.')\n      # compute a feasible resolution\n      input_height = int(\n          math.ceil(float(input_height) / divisible_by_num) * divisible_by_num)\n      input_width = int(\n          math.ceil(float(input_width) / divisible_by_num) * divisible_by_num)\n      print('Inference will be run at a resolution of ' + str(input_height) +\n            'x' + str(input_width) + '.')\n\n    # Resize images to desired input height and width.\n    if input_height != orig_height or input_width != orig_width:\n      images = uflow_utils.resize(\n          images, input_height, input_width, is_flow=False)\n\n    # Flatten images by folding sequence length into the batch dimension, apply\n    # the feature network and undo the flattening.\n    images_flattened = tf.reshape(\n        images,\n        [batch_size * seq_len, input_height, input_width, image_channels])\n    # noinspection PyCallingNonCallable\n\n    features_flattened = self._feature_model(\n        images_flattened, split_features_by_sample=False)\n    features = [\n        tf.reshape(f, [batch_size, seq_len] + f.shape.as_list()[1:])\n        for f in features_flattened\n    ]\n\n    features1, features2 = [[f[:, i] for f in features] for i in range(2)]\n\n    # split segmentations\n    segmentation1 = segmentations[:, 0, :, :, :]\n    segmentation2 = segmentations[:, 1, :, :, :]\n\n    # Compute flow in frame of image1.\n    # noinspection PyCallingNonCallable\n    flow = self._flow_model(features1, features2, segmentation1, segmentation2, training=False)[0]\n\n    if infer_occlusion:\n      # noinspection PyCallingNonCallable\n      flow_backward = self._flow_model(features2, features1, segmentation1, segmentation2, training=False)[0]\n      warps, valid_warp_masks, range_map, occlusion_mask = self.infer_occlusion(flow, flow_backward)\n      # originally, the shape is [1, 160, 160, 1] before the resize\n\n      warps = uflow_utils.resize(\n          warps, orig_height, orig_width, is_flow=False)\n\n      valid_warp_masks = uflow_utils.resize(\n          valid_warp_masks, orig_height, orig_width, is_flow=False)\n\n      occlusion_mask = uflow_utils.resize(\n          occlusion_mask, orig_height, orig_width, is_flow=False)\n\n      range_map = uflow_utils.resize(\n          range_map, orig_height, orig_width, is_flow=False)\n\n    # Resize and rescale flow to original resolution. This always needs to be\n    # done because flow is generated at a lower resolution.\n    if resize_flow_to_img_res:\n      flow = uflow_utils.resize(flow, orig_height, orig_width, is_flow=True)\n\n    if infer_occlusion:\n      return flow, warps, valid_warp_masks, range_map, occlusion_mask\n\n    return flow\n\n  @tf.function\n  def infer(self,\n            image1,\n            image2,\n            segmentation1,\n            segmentation2,\n            seg_point1,\n            seg_point2,\n            tracking_point1=None,\n            tracking_pos_emb=None,\n            input_height=None,\n            input_width=None,\n            resize_flow_to_img_res=True,\n            infer_occlusion=False):\n\n    return self.infer_tracking_function(tf.stack([image1, image2])[None],\n                                        tf.stack([segmentation1, segmentation2])[None],\n                                        seg_point1,\n                                        seg_point2,\n                                        tracking_point1,\n                                        tracking_pos_emb,\n                                        input_height=input_height,\n                                        input_width=input_width)\n\n    # return self.infer_no_tf_function(image1, image2, segmentation1, segmentation2, tracking_point1, tracking_point2, tracking_pos_emb,\n    #                                 input_height, input_width, resize_flow_to_img_res, infer_occlusion, frame_index)\n\n  # @tf.function\n  # def batch_infer(self,\n  #                 images,\n  #                 segmentations,\n  #                 input_height=None,\n  #                 input_width=None,\n  #                 resize_flow_to_img_res=True,\n  #                 infer_occlusion=False):\n  #\n  #   return self.batch_infer_no_tf_function(images, segmentations, input_height, input_width,\n  #                                          resize_flow_to_img_res,\n  #                                          infer_occlusion)\n\n  def infer_occlusion(self, flow_forward, flow_backward):\n    \"\"\"Gets a 'soft' occlusion mask from the forward and backward flow.\"\"\"\n\n    flows = {\n        (0, 1, 'inference'): [flow_forward],\n        (1, 0, 'inference'): [flow_backward],\n    }\n    warps, valid_warp_masks, range_maps_low_res, occlusion_masks, _, _ = uflow_utils.compute_warps_and_occlusion(\n        flows,\n        self._occlusion_estimation,\n        self._occ_weights,\n        self._occ_thresholds,\n        self._occ_clip_max,\n        occlusions_are_zeros=False)\n\n\n    warps = warps[(0, 1, 'inference')][0]\n    valid_warp_masks = valid_warp_masks[(0, 1, 'inference')][0]\n    occlusion_mask_forward = occlusion_masks[(0, 1, 'inference')][0]\n    range_maps_low_res = range_maps_low_res[(0, 1, 'inference')][0]\n\n    return warps, valid_warp_masks, range_maps_low_res, occlusion_mask_forward\n\n  def features_no_tf_function(self, image1, image2):\n    \"\"\"Runs the feature extractor portion of the model on image1 and image2.\"\"\"\n    images = tf.stack([image1, image2])\n    # noinspection PyCallingNonCallable\n    return self._feature_model(images, split_features_by_sample=True)\n\n  @tf.function\n  def features(self, image1, image2):\n    \"\"\"Runs the feature extractor portion of the model on image1 and image2.\"\"\"\n    return self.features_no_tf_function(image1, image2)\n\n  # -----------------------------------------------------------------------------------------------\n\n  def train_step_no_tf_function(self,\n                                batch,\n                                current_epoch,\n                                weights=None,\n                                plot_dir=None,\n                                distance_metrics=None,\n                                ground_truth_flow=None,\n                                ground_truth_valid=None,\n                                ground_truth_occlusions=None,\n                                ground_truth_segmentations=None,\n                                ground_truth_seg_points=None,\n                                ground_truth_tracking_points=None,\n                                images_without_photo_aug=None,\n                                occ_active=None):\n    \"\"\"Perform single gradient step.\"\"\"\n    if weights is None:\n      weights = {\n          'smooth2': 2.0,\n          'edge_constant': 100.0,\n          'census': 1.0,\n      }\n    else:\n      # Support values and callables (e.g. to compute weights from global step).\n      weights = {k: v() if callable(v) else v for k, v in weights.items()}\n\n    losses, gradients, variables, saved_offset_dict = self._loss_and_grad(\n        batch,\n        current_epoch,\n        weights,\n        plot_dir,\n        distance_metrics=distance_metrics,\n        ground_truth_flow=ground_truth_flow,\n        ground_truth_valid=ground_truth_valid,\n        ground_truth_occlusions=ground_truth_occlusions,\n        ground_truth_segmentations=ground_truth_segmentations,\n        ground_truth_seg_points=ground_truth_seg_points,\n        ground_truth_tracking_points=ground_truth_tracking_points,\n        images_without_photo_aug=images_without_photo_aug,\n        occ_active=occ_active)\n\n    self._optimizer.apply_gradients(\n        list(zip(gradients, variables)),\n        global_step=tf.compat.v1.train.get_or_create_global_step())\n\n    return losses, saved_offset_dict\n\n  @tf.function\n  def train_step(self,\n                 batch,\n                 current_epoch,\n                 weights=None,\n                 distance_metrics=None,\n                 ground_truth_flow=None,\n                 ground_truth_valid=None,\n                 ground_truth_occlusions=None,\n                 ground_truth_segmentations=None,\n                 ground_truth_seg_points=None,\n                 ground_truth_tracking_points=None,\n                 images_without_photo_aug=None,\n                 occ_active=None):\n    \"\"\"Performs a train step on the batch.\"\"\"\n\n    return self.train_step_no_tf_function(\n        batch,\n        current_epoch,\n        weights,\n        distance_metrics=distance_metrics,\n        ground_truth_flow=ground_truth_flow,\n        ground_truth_valid=ground_truth_valid,\n        ground_truth_occlusions=ground_truth_occlusions,\n        ground_truth_segmentations=ground_truth_segmentations,\n        ground_truth_seg_points=ground_truth_seg_points,\n        ground_truth_tracking_points=ground_truth_tracking_points,\n        images_without_photo_aug=images_without_photo_aug,\n        occ_active=occ_active)\n\n  def train(self,\n            data_it,\n            current_epoch,\n            num_steps,\n            weights=None,\n            progress_bar=True,\n            plot_dir=None,\n            distance_metrics=None,\n            occ_active=None):\n    \"\"\"Trains flow from a data iterator for a number of gradient steps.\n\n    Args:\n      data_it: tf.data.Iterator that produces tensors of shape [b,3,h,w,3].\n      num_steps: int, number of gradient steps to train for.\n      weights: dictionary with weight for each loss.\n      progress_bar: boolean flag for continuous printing of a progress bar.\n      plot_dir: location to plot results or None\n      distance_metrics: dictionary of which type of distance metric to use for\n        photometric losses\n      occ_active: dictionary of which occlusion types are active\n\n    Returns:\n      a dict that contains all losses.\n    \"\"\"\n\n    # Log dictionary for storing losses of this epoch.\n    log = dict()\n    # Support constant lr values and callables (for learning rate schedules).\n    if callable(self._learning_rate):\n      log['learning-rate'] = self._learning_rate()\n    else:\n      log['learning-rate'] = self._learning_rate\n\n    start_time_data = time.time()\n    for _, batch in zip(range(num_steps), data_it):\n      stop_time_data = time.time()\n\n      if progress_bar:\n        sys.stdout.write('.')\n        sys.stdout.flush()\n      # Split batch into images, occlusion masks, and ground truth flow.\n      images, labels = batch\n      ground_truth_flow = labels.get('flow_uv', None)\n      ground_truth_valid = labels.get('flow_valid', None)\n      ground_truth_occlusions = labels.get('occlusions', None)\n      images_without_photo_aug = labels.get('images_without_photo_aug', None)\n      ground_truth_segmentations = labels.get('segmentations', None)\n      ground_truth_seg_points = labels.get('segmentation_points', None)\n      ground_truth_tracking_points = labels.get('tracking_points', None)\n\n      # thresholding is necessary since segmentation is loaded blurrily\n      if ground_truth_segmentations is not None:\n          seg_threshold = 0\n          ground_truth_segmentations = tf.cast((ground_truth_segmentations > seg_threshold), dtype='float32') * 255\n\n      # use code below if thresholding is not used\n      # ground_truth_segmentations = tf.cast(ground_truth_segmentations, dtype='float32')\n\n      # -----------------------------------\n      # debug whether images are loaded correctly for prediction\n      # cv2.imwrite(f'uflow/debug_generated/debug_prev_patch.png', images[0,0,:,:,:].numpy()*255)\n      # cv2.imwrite(f'uflow/debug_generated/debug_cur_patch.png', images[0,1,:,:,:].numpy()*255)\n      # import pdb; pdb.set_trace()\n\n      # Debug whether segmentation was loaded correctly for training\n      # cv2.imwrite(f'test_segmentation_{seg_threshold}.png', ground_truth_segmentations.numpy()[0,0,:,:,0])\n      #\n      # print( tf.unique_with_counts(tf.reshape(ground_truth_segmentations, -1)) )\n      # import pdb; pdb.set_trace()\n      # -----------------------------------------\n\n      start_time_train_step = time.time()\n      # Use tf.function unless intermediate results have to be plotted.\n      if plot_dir is None:\n        # Perform a gradient step (optimized by tf.function).\n        losses, saved_offset_dict = self.train_step(\n            images,\n            current_epoch,\n            weights,\n            distance_metrics=distance_metrics,\n            ground_truth_flow=ground_truth_flow,\n            ground_truth_valid=ground_truth_valid,\n            ground_truth_occlusions=ground_truth_occlusions,\n            ground_truth_segmentations=ground_truth_segmentations,\n            ground_truth_seg_points=ground_truth_seg_points,\n            ground_truth_tracking_points=ground_truth_tracking_points,\n            images_without_photo_aug=images_without_photo_aug,\n            occ_active=occ_active)\n      else:\n        # Perform a gradient step without tf.function to allow plotting.\n        losses, saved_offset_dict = self.train_step_no_tf_function(\n            images,\n            current_epoch,\n            weights,\n            plot_dir,\n            distance_metrics=distance_metrics,\n            ground_truth_flow=ground_truth_flow,\n            ground_truth_valid=ground_truth_valid,\n            ground_truth_occlusions=ground_truth_occlusions,\n            ground_truth_seg_points=ground_truth_seg_points,\n            ground_truth_tracking_points=ground_truth_tracking_points,\n            images_without_photo_aug=images_without_photo_aug,\n            occ_active=occ_active)\n\n      stop_time_train_step = time.time()\n\n      log_update = losses\n      # Compute time in ms.\n      log_update['data-time'] = (stop_time_data - start_time_data) * 1000\n      log_update['train-time'] = (stop_time_train_step -\n                                  start_time_train_step) * 1000\n\n      # Log losses and times.\n      for key in log_update:\n        if key in log:\n          log[key].append(log_update[key])\n        else:\n          log[key] = [log_update[key]]\n        if self.summary_dir:\n          tf.summary.scalar(key, log[key])\n\n      # Set start time for data gathering to measure data pipeline efficiency.\n      start_time_data = time.time()\n\n    for key in log:\n      log[key] = tf.reduce_mean(input_tensor=log[key])\n\n    if progress_bar:\n      sys.stdout.write('\\n')\n      sys.stdout.flush()\n\n    return log, saved_offset_dict\n\n  def _loss_and_grad(self,\n                     batch,\n                     current_epoch,\n                     weights,\n                     plot_dir=None,\n                     distance_metrics=None,\n                     ground_truth_flow=None,\n                     ground_truth_valid=None,\n                     ground_truth_occlusions=None,\n                     ground_truth_segmentations=None,\n                     ground_truth_seg_points=None,\n                     ground_truth_tracking_points=None,\n                     images_without_photo_aug=None,\n                     occ_active=None):\n    \"\"\"Apply the model on the data in batch and compute the loss.\n\n    Args:\n      batch: tf.tensor of shape [b, seq, h, w, c] that holds a batch of image\n        sequences.\n      weights: dictionary with float entries per loss.\n      plot_dir: str, directory to plot images\n      distance_metrics: dict, which distance metrics to use,\n      ground_truth_flow: Tensor, optional ground truth flow for first image\n      ground_truth_valid: Tensor, indicates locations where gt flow is valid\n      ground_truth_occlusions: Tensor, optional ground truth occlusions for\n        computing loss. If None, predicted occlusions will be used.\n      images_without_photo_aug: optional images without any photometric\n        augmentation applied. Will be used for computing photometric losses if\n        provided.\n      occ_active: optional dict indicating which occlusion methods are active\n\n    Returns:\n      A tuple consisting of a tf.scalar that represents the total loss for the\n      current batch, a list of gradients, and a list of the respective\n      variables.\n    \"\"\"\n\n    # with tf.GradientTape() as tape:\n    #   losses = self.compute_loss(\n    #       batch,\n    #       weights,\n    #       plot_dir,\n    #       distance_metrics=distance_metrics,\n    #       ground_truth_flow=ground_truth_flow,\n    #       ground_truth_valid=ground_truth_valid,\n    #       ground_truth_occlusions=ground_truth_occlusions,\n    #       ground_truth_segmentations=ground_truth_segmentations,\n    #       ground_truth_tracking_points=ground_truth_tracking_points,\n    #       images_without_photo_aug=images_without_photo_aug,\n    #       occ_active=occ_active)\n    #\n    # variables = (\n    #     self._feature_model.trainable_variables +\n    #     self._flow_model.trainable_variables)\n    # grads = tape.gradient(losses['total-loss'], variables)\n\n    # This is where to add the tracking model\n    with tf.GradientTape() as tape:\n      losses, saved_offset_dict = self.compute_loss_tracking(batch,\n                                                              current_epoch,\n                                                              ground_truth_seg_points=ground_truth_seg_points,\n                                                              ground_truth_tracking_points=ground_truth_tracking_points)\n\n    # print(self._tracking_model.local_alignment.summary())\n    variables = ( self._tracking_model.trainable_variables)\n    grads = tape.gradient(losses['total-loss'], variables)\n\n    return losses, grads, variables, saved_offset_dict\n\n  def compute_loss_tracking(self,\n                           batch,\n                           current_epoch,\n                           ground_truth_seg_points=None,\n                           ground_truth_tracking_points=None):\n\n      # determine seq length\n      max_seq_len = int(batch.shape[1])\n      # seq_len = tracking_utils.linear_seq_increase(current_epoch, max_seq_len)\n\n      # Get position embedding of the segmentation points\n      prev_seg_points = ground_truth_seg_points[:, 0, :, :]\n      cur_seg_points = ground_truth_seg_points[:, 1, :, :]\n      pos_emb = self._tracking_model(prev_seg_points)\n\n      gt_prev_id_assignments = ground_truth_tracking_points[:, 0, :, :]\n      gt_cur_id_assignments = ground_truth_tracking_points[:, 1, :, :]\n\n      # predict the location of tracking points in the next frame\n      prev_patch = batch[:, 0, :, :, :]\n      cur_patch = batch[:, 1, :, :, :]\n\n      # -------------------- To remove negative padding -------------------\n      # prev_seg_points_limit = tracking_utils.get_first_occurrence_indices(prev_seg_points[:, :, 0], -0.1)\n      # cur_seg_points_limit = tracking_utils.get_first_occurrence_indices(cur_seg_points[:, :, 0], -0.1)\n\n      # get the biggest value in the tensor\n      # prev_seg_points_limit = tf.math.reduce_max(prev_seg_points_limit)\n      # cur_seg_points_limit = tf.math.reduce_max(cur_seg_points_limit)\n      # -------------------------------------------------------------------\n      # find dense correspondence of all segmentation points, whereas gt id assignments are for a subset of points\n      forward_spatial_offset, backward_spatial_offset, saved_offset = self._tracking_model(prev_patch, cur_patch, prev_seg_points, cur_seg_points, pos_emb)\n\n      # forward_corr_2d_loss = tracking_utils.corr_2d_loss( gt_prev_id_assignments, gt_cur_id_assignments, forward_corr_2d_matrix)\n      # backward_corr_2d_loss = tracking_utils.corr_2d_loss( gt_cur_id_assignments, gt_prev_id_assignments, backward_corr_2d_matrix)\n      # corr_cycle_consistency_loss = tracking_utils.corr_cycle_consistency(forward_corr_2d_matrix, backward_corr_2d_matrix)\n\n      # process forward_id_assign by contour length\n      # prev_seg_points_limit = tracking_utils.get_first_occurrence_indices(prev_seg_points[:, :, 0], -0.1)\n      # cur_seg_points_limit = tracking_utils.get_first_occurrence_indices(cur_seg_points[:, :, 0], -0.1)\n      # forward_id_assign = tracking_utils.fit_id_assignments_to_next_contour(forward_id_assign, tf.expand_dims(prev_seg_points[:,:,-1], axis=-1), cur_seg_points_limit)\n      # backward_id_assign = tracking_utils.fit_id_assignments_to_next_contour(backward_id_assign, tf.expand_dims(cur_seg_points[:,:,-1], axis=-1), prev_seg_points_limit)\n\n      # total_forward_matching_points_loss = tracking_utils.matching_contour_points_loss(gt_prev_id_assignments, gt_cur_id_assignments, forward_id_assign)\n      # total_backward_matching_points_loss = tracking_utils.matching_contour_points_loss(gt_cur_id_assignments, gt_prev_id_assignments, backward_id_assign)\n      # total_cycle_consistency_assign_loss = tracking_utils.cycle_consistency_assign_loss(forward_id_assign, backward_id_assign, tf.expand_dims(prev_seg_points[:,:,-1], axis=-1),  tf.expand_dims(cur_seg_points[:,:,-1], axis=-1))\n      # total_points_order_loss = tracking_utils.contour_points_order_loss(forward_id_assign)\n\n      # total_forward_matching_points_spatial_metric = tracking_utils.matching_contour_points_spatial_metric(cur_seg_points[:,:,:2], gt_prev_id_assignments, gt_cur_id_assignments, forward_id_assign)\n      pred_cur_tracking_points = prev_seg_points[:,:,:2] + forward_spatial_offset\n      forward_spatial_points_loss = tracking_utils.matching_spatial_points_loss(gt_prev_id_assignments, gt_cur_id_assignments,\n                                                                                      tf.cast(cur_seg_points[:,:,:2], dtype=pred_cur_tracking_points.dtype), pred_cur_tracking_points)\n\n      cycle_consistency_spatial_loss = tracking_utils.cycle_consistency_spatial_loss(prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], cur_seg_points[:,:,:2], cur_seg_points[:,:,-1], forward_spatial_offset, backward_spatial_offset)\n\n      forward_tracker_photometric_loss = tracking_utils.tracker_unsupervised_photometric_loss(prev_patch, cur_patch, prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], pred_cur_tracking_points)\n\n      backward_tracker_photometric_loss = tracking_utils.tracker_unsupervised_photometric_loss(cur_patch, prev_patch, cur_seg_points[:,:,:2], cur_seg_points[:,:,-1], cur_seg_points[:,:,:2] + backward_spatial_offset)\n\n      # forward_tracker_census_loss = tracking_utils.tracker_unsupervised_census_loss(prev_patch, cur_patch, prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], pred_cur_tracking_points)\n\n      # backward_tracker_census_loss = tracking_utils.tracker_unsupervised_census_loss(cur_patch, prev_patch, cur_seg_points[:,:,:2], cur_seg_points[:,:,-1], cur_seg_points[:,:,:2] + backward_spatial_offset)\n\n      forward_linear_spring_loss, forward_normal_force_loss = tracking_utils.mechanical_loss_from_offsets(prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], forward_spatial_offset)\n      backward_linear_spring_loss, backward_normal_force_loss = tracking_utils.mechanical_loss_from_offsets(cur_seg_points[:,:,:2], cur_seg_points[:,:,-1], backward_spatial_offset)\n\n      # snake_tension_loss, snake_stiffness_loss = tracking_utils.snake_loss_from_offsets(prev_seg_points[:,:,:2], prev_seg_points[:,:,-1], forward_spatial_offset)\n\n      saved_offset_dict = {0: saved_offset}  # key for sequence number, only one key 0 if seq_num=2\n\n      total_loss = cycle_consistency_spatial_loss + forward_normal_force_loss\n\n      losses = {'total-loss' : total_loss,\n                'forward_spatial_points_loss': forward_spatial_points_loss,\n                'forward_tracker_photometric_loss': forward_tracker_photometric_loss, 'backward_tracker_photometric_loss': backward_tracker_photometric_loss,\n                # 'forward_tracker_census_loss': forward_tracker_census_loss, 'backward_tracker_census_loss': backward_tracker_census_loss,\n                'cycle_consistency_spatial_loss': cycle_consistency_spatial_loss,\n                # 'snake_tension_loss': snake_tension_loss, 'snake_stiffness_loss': snake_stiffness_loss,\n                'forward_linear_spring_loss': forward_linear_spring_loss, 'forward_normal_force_loss': forward_normal_force_loss,\n                'backward_linear_spring_loss': backward_linear_spring_loss, 'backward_normal_force_loss': backward_normal_force_loss}\n\n      return losses, saved_offset_dict\n\n  def compute_loss(self,\n                   batch,\n                   weights,\n                   plot_dir=None,\n                   distance_metrics=None,\n                   ground_truth_flow=None,\n                   ground_truth_valid=None,\n                   ground_truth_occlusions=None,\n                   ground_truth_segmentations=None,\n                   ground_truth_tracking_points=None,\n                   images_without_photo_aug=None,\n                   occ_active=None):\n    \"\"\"Applies the model and computes losses for a batch of image sequences.\"\"\"\n    # Compute only a supervised loss.\n    if self._train_with_supervision:\n      if ground_truth_flow is None:\n        raise ValueError('Need ground truth flow to compute supervised loss.')\n      flows = uflow_utils.compute_flow_for_supervised_loss(\n          self._feature_model, self._flow_model, batch=batch, training=True)\n      losses = uflow_utils.supervised_loss(weights, ground_truth_flow,\n                                           ground_truth_valid, flows)\n      losses = {key + '-loss': losses[key] for key in losses}\n      return losses\n\n    # Use possibly augmented images if non augmented version is not provided.\n    if images_without_photo_aug is None:\n      images_without_photo_aug = batch\n\n    flows, selfsup_transform_fns = uflow_utils.compute_features_and_flow(\n        self._feature_model,\n        self._flow_model,\n        batch=batch,\n        batch_without_aug=images_without_photo_aug,\n        training=True,\n        build_selfsup_transformations=self._build_selfsup_transformations,\n        teacher_feature_model=self._teacher_feature_model,\n        teacher_flow_model=self._teacher_flow_model,\n        teacher_image_version=self._teacher_image_version,\n        ground_truth_segmentations=ground_truth_segmentations,\n        ground_truth_tracking_points=ground_truth_tracking_points\n    )\n\n    # Prepare images and contours for unsupervised loss (prefer unaugmented images).\n    seq_len = int(batch.shape[1])\n    images = {i: images_without_photo_aug[:, i] for i in range(seq_len)}\n    contours = {i: ground_truth_segmentations[:, i] for i in range(seq_len)}\n\n    # Warp stuff and compute occlusion.\n    warps, valid_warp_masks, _, not_occluded_masks, fb_sq_diff, fb_sum_sq = uflow_utils.compute_warps_and_occlusion(\n                                                                            flows,\n                                                                            occlusion_estimation=self._occlusion_estimation,\n                                                                            occ_weights=self._occ_weights,\n                                                                            occ_thresholds=self._occ_thresholds,\n                                                                            occ_clip_max=self._occ_clip_max,\n                                                                            occlusions_are_zeros=True,\n                                                                            occ_active=occ_active)\n\n    # Warp images and features.\n    warped_images = uflow_utils.apply_warps_stop_grad(images, warps, level=0)\n\n    # Warp contours\n    assert len(images) == len(contours)\n\n    # dilate contours and get image at the contour regions\n    img_contours = {}\n    for dict_i in range(len(images)):\n        # img_contours[dict_i] = images[dict_i] * contours[dict_i]  # contour image without dilation\n\n        a_depth = contours[dict_i].shape[-1]\n        dilation_filter = tf.zeros( [1, 1, a_depth], tf.float32)  # why zeros? https://stackoverflow.com/questions/54686895/tensorflow-dilation-behave-differently-than-morphological-dilation\n        dilated_contour = tf.nn.dilation2d(input=contours[dict_i], filters=dilation_filter, strides=[1,1,1,1], padding='SAME', data_format='NHWC', dilations=[1, 1, 1, 1])\n        img_contours[dict_i] = images[dict_i] * dilated_contour\n\n        # debug whether images are loaded correctly for training\n        # cv2.imwrite(f'debug_dilated_contour.png', dilated_contour.numpy()[0, :, :, :])\n        # cv2.imwrite(f'debug_img_{dict_i}.png', images[dict_i].numpy()[0, :, :, :] * 255)\n        # cv2.imwrite(f'debug_contour_{dict_i}.png', contours[dict_i].numpy()[0,:,:,:])\n        # cv2.imwrite(f'debug_contour_img_{dict_i}.png', img_contours[dict_i].numpy()[0, :, :, :])\n        # print( tf.unique_with_counts(tf.reshape(dilated_contour, -1)) )\n\n    warped_contours = uflow_utils.apply_warps_stop_grad(img_contours, warps, level=0)\n\n    # cv2.imwrite(f'debug_warped_contour_0.png', warped_contours[(0, 1, 'original-teacher')].numpy()[0, :, :, :])\n    # cv2.imwrite(f'debug_warped_contour_1.png', warped_contours[(1, 0, 'original-teacher')].numpy()[0, :, :, :])\n    # import pdb; pdb.set_trace()\n\n    # Compute losses.\n    losses = uflow_utils.compute_loss(\n        weights=weights,\n        images=images,\n        contours=img_contours,\n        flows=flows,\n        warps=warps,\n        valid_warp_masks=valid_warp_masks,\n        not_occluded_masks=not_occluded_masks,\n        fb_sq_diff=fb_sq_diff,\n        fb_sum_sq=fb_sum_sq,\n        warped_images=warped_images,\n        warped_contours=warped_contours,\n        only_forward=self._only_forward,\n        selfsup_transform_fns=selfsup_transform_fns,\n        fb_sigma_teacher=self._fb_sigma_teacher,\n        fb_sigma_student=self._fb_sigma_student,\n        plot_dir=plot_dir,\n        distance_metrics=distance_metrics,\n        smoothness_edge_weighting=self._smoothness_edge_weighting,\n        stop_gradient_mask=self._stop_gradient_mask,\n        selfsup_mask=self._selfsup_mask,\n        ground_truth_occlusions=ground_truth_occlusions,\n        smoothness_at_level=self._smoothness_at_level)\n    losses = {key + '-loss': losses[key] for key in losses}\n\n    return losses", ""]}
{"filename": "src/uflow_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2021 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"UFlow utils.\n\nThis library contains the various util functions used in UFlow.", "\nThis library contains the various util functions used in UFlow.\n\"\"\"\n\nimport time\nimport sys\n\nimport tensorflow as tf\nfrom src import uflow_plotting\nfrom src.uflow_resampler import resampler", "from src import uflow_plotting\nfrom src.uflow_resampler import resampler\nimport cv2\nimport numpy as np\n\ndef flow_to_warp(flow):\n  \"\"\"Compute the warp from the flow field.\n\n  Args:\n    flow: tf.tensor representing optical flow.\n\n  Returns:\n    The warp, i.e. the endpoints of the estimated flow.\n  \"\"\"\n\n  # Construct a grid of the image coordinates.\n  height, width = flow.shape.as_list()[-3:-1]\n  i_grid, j_grid = tf.meshgrid(\n      tf.linspace(0.0, height - 1.0, int(height)),\n      tf.linspace(0.0, width - 1.0, int(width)),\n      indexing='ij')\n  grid = tf.stack([i_grid, j_grid], axis=2)\n\n  # Potentially add batch dimension to match the shape of flow.\n  if len(flow.shape) == 4:\n    grid = grid[None]\n\n  # Add the flow field to the image grid.\n  if flow.dtype != grid.dtype:\n    grid = tf.cast(grid, flow.dtype)\n  warp = grid + flow\n  return warp", "\n\ndef flow_to_warp_np(flow):\n  \"\"\"Compute the warp from the flow field.\n\n  Args:\n    flow: tf.tensor representing optical flow.\n\n  Returns:\n    The warp, i.e. the endpoints of the estimated flow.\n  \"\"\"\n\n  # Construct a grid of the image coordinates.\n  height, width = flow.shape[-3:-1]\n  i_grid, j_grid = np.meshgrid(\n      tf.linspace(0.0, height - 1.0, int(height)),\n      tf.linspace(0.0, width - 1.0, int(width)),\n      indexing='ij')\n  grid = np.stack([i_grid, j_grid], axis=2)\n\n  # Potentially add batch dimension to match the shape of flow.\n  if len(flow.shape) == 4:\n    grid = grid[None]\n\n  # Add the flow field to the image grid.\n  if flow.dtype != grid.dtype:\n    grid = grid.astype(flow.dtype)\n  warp = grid + flow\n  return warp", "\n\ndef mask_invalid(coords):\n  \"\"\"Mask coordinates outside of the image.\n\n  Valid = 1, invalid = 0.\n\n  Args:\n    coords: a 4D float tensor of image coordinates.\n\n  Returns:\n    The mask showing which coordinates are valid.\n  \"\"\"\n  coords_rank = len(coords.shape)\n  if coords_rank != 4:\n    raise NotImplementedError()\n  max_height = float(coords.shape[-3] - 1)\n  max_width = float(coords.shape[-2] - 1)\n  mask = tf.logical_and(\n      tf.logical_and(coords[:, :, :, 0] >= 0.0,\n                     coords[:, :, :, 0] <= max_height),\n      tf.logical_and(coords[:, :, :, 1] >= 0.0,\n                     coords[:, :, :, 1] <= max_width))\n  mask = tf.cast(mask, dtype=tf.float32)[:, :, :, None]\n  return mask", "\n\ndef resample(source, coords):\n  \"\"\"Resample the source image at the passed coordinates.\n\n  Args:\n    source: tf.tensor, batch of images to be resampled.\n    coords: tf.tensor, batch of coordinates in the image.\n\n  Returns:\n    The resampled image.\n\n  Coordinates should be between 0 and size-1. Coordinates outside of this range\n  are handled by interpolating with a background image filled with zeros in the\n  same way that SAME size convolution works.\n  \"\"\"\n\n  # Wrap this function because it uses a different order of height/width dims.\n  orig_source_dtype = source.dtype\n  if source.dtype != tf.float32:\n    source = tf.cast(source, tf.float32)\n  if coords.dtype != tf.float32:\n    coords = tf.cast(coords, tf.float32)\n  coords_rank = len(coords.shape)\n  if coords_rank == 4:\n    output = resampler(source, coords[:, :, :, ::-1])\n    if orig_source_dtype != source.dtype:\n      return tf.cast(output, orig_source_dtype)\n    return output\n  else:\n    raise NotImplementedError()", "\n\ndef resample_np(source_img, coords):\n  \"\"\"Resample the source_img image at the passed coordinates.\n\n  Args:\n    source_img: np.ndarray, batch of images to be resampled.\n    coords: np.ndarray, batch of coordinates in the image.\n\n  Returns:\n    The resampled image in tf.tensor foramt\n\n  Coordinates should be between 0 and size-1. Coordinates outside of this range\n  are handled by interpolating with a background image filled with zeros in the\n  same way that SAME size convolution works.\n  \"\"\"\n\n  # import pdb; pdb.set_trace()\n  # Wrap this function because it uses a different order of height/width dims.\n  orig_source_dtype = source_img.dtype\n  if orig_source_dtype != np.float32:\n    source_img = source_img.astype(np.float32)\n  if coords.dtype != np.float32:\n    coords = coords.astype(np.float32)\n  coords_rank = len(coords.shape)\n\n  if coords_rank == 3:\n    source_img = np.expand_dims(source_img, axis=0)\n    coords = np.expand_dims(coords, axis=0)\n    output = resampler(source_img, coords[:, :, :, ::-1])\n    if orig_source_dtype != source_img.dtype:\n      return tf.cast(output, orig_source_dtype)\n    return output\n\n  if coords_rank == 4:\n    output = resampler(source_img, coords[:, :, :, ::-1])\n    if orig_source_dtype != source_img.dtype:\n      return tf.cast(output, orig_source_dtype)\n    return output\n  else:\n    raise NotImplementedError()", "\n\ndef compute_range_map(flow,\n                      downsampling_factor=1,\n                      reduce_downsampling_bias=True,\n                      resize_output=True):\n  \"\"\"Count how often each coordinate is sampled.\n\n  Counts are assigned to the integer coordinates around the sampled coordinates\n  using weights from bilinear interpolation.\n\n  Args:\n    flow: A float tensor of shape (batch size x height x width x 2) that\n      represents a dense flow field.\n    downsampling_factor: An integer, by which factor to downsample the output\n      resolution relative to the input resolution. Downsampling increases the\n      bin size but decreases the resolution of the output. The output is\n      normalized such that zero flow input will produce a constant ones output.\n    reduce_downsampling_bias: A boolean, whether to reduce the downsampling bias\n      near the image boundaries by padding the flow field.\n    resize_output: A boolean, whether to resize the output ot the input\n      resolution.\n\n  Returns:\n    A float tensor of shape [batch_size, height, width, 1] that denotes how\n    often each pixel is sampled.\n  \"\"\"\n\n  # Get input shape.\n  input_shape = flow.shape.as_list()\n  if len(input_shape) != 4:\n    raise NotImplementedError()\n  batch_size, input_height, input_width, _ = input_shape\n\n  flow_height = input_height\n  flow_width = input_width\n\n  # Apply downsampling (and move the coordinate frame appropriately).\n  output_height = input_height // downsampling_factor\n  output_width = input_width // downsampling_factor\n  if downsampling_factor > 1:\n    # Reduce the bias that comes from downsampling, where pixels at the edge\n    # will get lower counts that pixels in the middle of the image, by padding\n    # the flow field.\n    if reduce_downsampling_bias:\n      p = downsampling_factor // 2\n      flow_height += 2 * p\n      flow_width += 2 * p\n      # Apply padding in multiple steps to padd with the values on the edge.\n      for _ in range(p):\n        flow = tf.pad(\n            tensor=flow,\n            paddings=[[0, 0], [1, 1], [1, 1], [0, 0]],\n            mode='SYMMETRIC')\n      coords = flow_to_warp(flow) - p\n    # Update the coordinate frame to the downsampled one.\n    coords = (coords + (1 - downsampling_factor) * 0.5) / downsampling_factor\n  elif downsampling_factor == 1:\n    coords = flow_to_warp(flow)\n  else:\n    raise ValueError('downsampling_factor must be an integer >= 1.')\n\n  # Split coordinates into an integer part and a float offset for interpolation.\n  coords_floor = tf.floor(coords)\n  coords_offset = coords - coords_floor\n  coords_floor = tf.cast(coords_floor, 'int32')\n\n  # Define a batch offset for flattened indexes into all pixels.\n  batch_range = tf.reshape(tf.range(batch_size), [batch_size, 1, 1])\n  idx_batch_offset = tf.tile(\n      batch_range, [1, flow_height, flow_width]) * output_height * output_width\n\n  # Flatten everything.\n  coords_floor_flattened = tf.reshape(coords_floor, [-1, 2])\n  coords_offset_flattened = tf.reshape(coords_offset, [-1, 2])\n  idx_batch_offset_flattened = tf.reshape(idx_batch_offset, [-1])\n\n  # Initialize results.\n  idxs_list = []\n  weights_list = []\n\n  # Loop over differences di and dj to the four neighboring pixels.\n  for di in range(2):\n    for dj in range(2):\n\n      # Compute the neighboring pixel coordinates.\n      idxs_i = coords_floor_flattened[:, 0] + di\n      idxs_j = coords_floor_flattened[:, 1] + dj\n      # Compute the flat index into all pixels.\n      idxs = idx_batch_offset_flattened + idxs_i * output_width + idxs_j\n\n      # Only count valid pixels.\n      mask = tf.reshape(\n          tf.compat.v1.where(\n              tf.logical_and(\n                  tf.logical_and(idxs_i >= 0, idxs_i < output_height),\n                  tf.logical_and(idxs_j >= 0, idxs_j < output_width))), [-1])\n      valid_idxs = tf.gather(idxs, mask)\n      valid_offsets = tf.gather(coords_offset_flattened, mask)\n\n      # Compute weights according to bilinear interpolation.\n      weights_i = (1. - di) - (-1)**di * valid_offsets[:, 0]\n      weights_j = (1. - dj) - (-1)**dj * valid_offsets[:, 1]\n      weights = weights_i * weights_j\n\n      # Append indices and weights to the corresponding list.\n      idxs_list.append(valid_idxs)\n      weights_list.append(weights)\n\n  # Concatenate everything.\n  idxs = tf.concat(idxs_list, axis=0)\n  weights = tf.concat(weights_list, axis=0)\n\n  # Sum up weights for each pixel and reshape the result.\n  counts = tf.math.unsorted_segment_sum(\n      weights, idxs, batch_size * output_height * output_width)\n  count_image = tf.reshape(counts, [batch_size, output_height, output_width, 1])\n\n  if downsampling_factor > 1:\n    # Normalize the count image so that downsampling does not affect the counts.\n    count_image /= downsampling_factor**2\n    if resize_output:\n      count_image = resize(\n          count_image, input_height, input_width, is_flow=False)\n\n  return count_image", "\n\ndef compute_warps_and_occlusion(flows,\n                                occlusion_estimation,\n                                occ_weights=None,\n                                occ_thresholds=None,\n                                occ_clip_max=None,\n                                occlusions_are_zeros=True,\n                                occ_active=None):\n  \"\"\"Compute warps, valid warp masks, advection maps, and occlusion masks.\"\"\"\n\n  if occ_clip_max is not None:\n    for key in occ_clip_max:\n      if key not in ['forward_collision', 'fb_abs']:\n        raise ValueError('occ_clip_max for this key is not supported')\n\n  warps = dict()\n  range_maps_high_res = dict()\n  range_maps_low_res = dict()\n  occlusion_logits = dict()\n  occlusion_scores = dict()\n  occlusion_masks = dict()\n  valid_warp_masks = dict()\n  fb_sq_diff = dict()\n  fb_sum_sq = dict()\n\n  # import pdb; pdb.set_trace()\n  for key in flows:\n\n    i, j, t = key  # ([(0, 1, 'original-teacher'), (0, 1, 'augmented-student'), (0, 1, 'transformed-student'), (1, 0, 'original-teacher'), (1, 0, 'augmented-student'), (1, 0, 'transformed-student')])\n    rev_key = (j, i, t)\n\n    warps[key] = []\n    range_maps_high_res[key] = []\n    range_maps_low_res[rev_key] = []\n    occlusion_masks[key] = []\n    valid_warp_masks[key] = []\n    fb_sq_diff[key] = []\n    fb_sum_sq[key] = []\n\n    for level in range(min(3, len(flows[key]))):\n\n      flow_ij = flows[key][level]\n      flow_ji = flows[rev_key][level]\n\n      # Compute warps (coordinates) and a mask for which coordinates are valid.\n      warps[key].append(flow_to_warp(flow_ij))\n      valid_warp_masks[key].append(mask_invalid(warps[key][level]))\n\n      # Compare forward and backward flow.\n      flow_ji_in_i = resample(flow_ji, warps[key][level])\n      fb_sq_diff[key].append(\n          tf.reduce_sum(\n              input_tensor=(flow_ij + flow_ji_in_i)**2, axis=-1, keepdims=True))\n      fb_sum_sq[key].append(\n          tf.reduce_sum(\n              input_tensor=(flow_ij**2 + flow_ji_in_i**2),\n              axis=-1,\n              keepdims=True))\n\n      if level != 0:\n        continue\n\n      # This initializations avoids problems in tensorflow (likely AutoGraph)\n      occlusion_mask = tf.zeros_like(flow_ij[Ellipsis, :1], dtype=tf.float32)\n      occlusion_scores['forward_collision'] = tf.zeros_like(\n          flow_ij[Ellipsis, :1], dtype=tf.float32)\n      occlusion_scores['backward_zero'] = tf.zeros_like(\n          flow_ij[Ellipsis, :1], dtype=tf.float32)\n      occlusion_scores['fb_abs'] = tf.zeros_like(\n          flow_ij[Ellipsis, :1], dtype=tf.float32)\n\n      if occlusion_estimation == 'none' or (\n          occ_active is not None and not occ_active[occlusion_estimation]):\n        occlusion_mask = tf.zeros_like(flow_ij[Ellipsis, :1], dtype=tf.float32)\n\n      elif occlusion_estimation == 'brox':\n        occlusion_mask = tf.cast(\n            fb_sq_diff[key][level] > 0.01 * fb_sum_sq[key][level] + 0.5,\n            tf.float32)\n\n      elif occlusion_estimation == 'fb_abs':\n        occlusion_mask = tf.cast(fb_sq_diff[key][level]**0.5 > 1.5, tf.float32)\n\n      elif occlusion_estimation == 'wang':\n        range_maps_low_res[rev_key].append(\n            compute_range_map(\n                flow_ji,\n                downsampling_factor=1,\n                reduce_downsampling_bias=False,\n                resize_output=False))\n        # Invert so that low values correspond to probable occlusions,\n        # range [0, 1].\n        occlusion_mask = (\n            1. - tf.clip_by_value(range_maps_low_res[rev_key][level], 0., 1.))\n\n        # import pdb; pdb.set_trace()\n        # ---------------------------------------------------\n        # TODO change it\n        # when occlusions_are_zeros=True, there is no occlusion when occlusion_mask = 0 here\n        # if occlusions_are_zeros:\n        #     occlusion_mask = tf.zeros_like(occlusion_mask, dtype='float32')\n        # else:\n        #     occlusion_mask = tf.ones_like(occlusion_mask, dtype='float32')\n        #----------------------------------------------------\n\n      elif occlusion_estimation == 'wang4':\n        range_maps_low_res[rev_key].append(\n            compute_range_map(\n                flow_ji,\n                downsampling_factor=4,\n                reduce_downsampling_bias=True,\n                resize_output=True))\n        # Invert so that low values correspond to probable occlusions,\n        # range [0, 1].\n        occlusion_mask = (\n            1. - tf.clip_by_value(range_maps_low_res[rev_key][level], 0., 1.))\n\n      elif occlusion_estimation == 'wangthres':\n        range_maps_low_res[rev_key].append(\n            compute_range_map(\n                flow_ji,\n                downsampling_factor=1,\n                reduce_downsampling_bias=True,\n                resize_output=True))\n        # Invert so that low values correspond to probable occlusions,\n        # range [0, 1].\n        occlusion_mask = tf.cast(range_maps_low_res[rev_key][level] < 0.75,\n                                 tf.float32)\n\n      elif occlusion_estimation == 'wang4thres':\n        range_maps_low_res[rev_key].append(\n            compute_range_map(\n                flow_ji,\n                downsampling_factor=4,\n                reduce_downsampling_bias=True,\n                resize_output=True))\n        # Invert so that low values correspond to probable occlusions,\n        # range [0, 1].\n        occlusion_mask = tf.cast(range_maps_low_res[rev_key][level] < 0.75,\n                                 tf.float32)\n\n      elif occlusion_estimation == 'uflow':\n        # Compute occlusion from the range map of the forward flow, projected\n        # back into the frame of image i. The idea is if many flow vectors point\n        # to the same pixel, those are likely occluded.\n        if 'forward_collision' in occ_weights and (\n            occ_active is None or occ_active['forward_collision']):\n          range_maps_high_res[key].append(\n              compute_range_map(\n                  flow_ij,\n                  downsampling_factor=1,\n                  reduce_downsampling_bias=True,\n                  resize_output=True))\n          fwd_range_map_in_i = resample(range_maps_high_res[key][level],\n                                        warps[key][level])\n          # Rescale to [0, max-1].\n          occlusion_scores['forward_collision'] = tf.clip_by_value(\n              fwd_range_map_in_i, 1., occ_clip_max['forward_collision']) - 1.0\n\n        # Compute occlusion from the range map of the backward flow, which is\n        # already computed in frame i. Pixels that no flow vector points to are\n        # likely occluded.\n        if 'backward_zero' in occ_weights and (occ_active is None or\n                                               occ_active['backward_zero']):\n          range_maps_low_res[rev_key].append(\n              compute_range_map(\n                  flow_ji,\n                  downsampling_factor=4,\n                  reduce_downsampling_bias=True,\n                  resize_output=True))\n          # Invert so that low values correspond to probable occlusions,\n          # range [0, 1].\n          occlusion_scores['backward_zero'] = (\n              1. - tf.clip_by_value(range_maps_low_res[rev_key][level], 0., 1.))\n\n        # Compute occlusion from forward-backward consistency. If the flow\n        # vectors are inconsistent, this means that they are either wrong or\n        # occluded.\n        if 'fb_abs' in occ_weights and (occ_active is None or\n                                        occ_active['fb_abs']):\n          # Clip to [0, max].\n          occlusion_scores['fb_abs'] = tf.clip_by_value(\n              fb_sq_diff[key][level]**0.5, 0.0, occ_clip_max['fb_abs'])\n\n        occlusion_logits = tf.zeros_like(flow_ij[Ellipsis, :1], dtype=tf.float32)\n        for k, v in occlusion_scores.items():\n          occlusion_logits += (v - occ_thresholds[k]) * occ_weights[k]\n        occlusion_mask = tf.sigmoid(occlusion_logits)\n      else:\n        raise ValueError('Unknown value for occlusion_estimation:',\n                         occlusion_estimation)\n\n      occlusion_masks[key].append(\n          1. - occlusion_mask if occlusions_are_zeros else occlusion_mask)\n\n  # import pdb; pdb.set_trace()\n  return warps, valid_warp_masks, range_maps_low_res, occlusion_masks, fb_sq_diff, fb_sum_sq", "\n\ndef apply_warps_stop_grad(sources, warps, level):\n  \"\"\"Apply all warps on the correct sources.\"\"\"\n\n  warped = dict()\n  for (i, j, t) in warps:\n    # Only propagate gradient through the warp, not through the source.\n    warped[(i, j, t)] = resample(\n        tf.stop_gradient(sources[j]), warps[(i, j, t)][level])\n\n  return warped", "\n\ndef upsample(img, is_flow):\n  \"\"\"Double resolution of an image or flow field.\n\n  Args:\n    img: tf.tensor, image or flow field to be resized\n    is_flow: bool, flag for scaling flow accordingly\n\n  Returns:\n    Resized and potentially scaled image or flow field.\n  \"\"\"\n  _, height, width, _ = img.shape.as_list()\n  orig_dtype = img.dtype\n  if orig_dtype != tf.float32:\n    img = tf.cast(img, tf.float32)\n  img_resized = tf.compat.v2.image.resize(img,\n                                          (int(height * 2), int(width * 2)))\n  if is_flow:\n    # Scale flow values to be consistent with the new image size.\n    img_resized *= 2\n  if img_resized.dtype != orig_dtype:\n    return tf.cast(img_resized, orig_dtype)\n  return img_resized", "\n\ndef downsample(img, is_flow):\n  \"\"\"Halve the resolution of an image or flow field.\n\n  Args:\n    img: tf.tensor, image or flow field to be resized\n    is_flow: bool, flag for scaling flow accordingly\n\n  Returns:\n    Resized and potentially scaled image or flow field.\n  \"\"\"\n  _, height, width, _ = img.shape.as_list()\n  img_resized = tf.compat.v2.image.resize(img,\n                                          (int(height / 2), int(width / 2)))\n  if is_flow:\n    # Scale flow values to be consistent with the new image size.\n    img_resized /= 2\n  return img_resized", "\n# --------------------------------------------------------------\n# From get_expanded_whole_frames() function in MARS-Net/models/predict_data_generator.py github repo by Junbong Jang\n\ndef expand_image(self, img, ratio=64.0, expand_more=False):\n    img_row = img.shape[0]\n    img_col = img.shape[1]\n    # expand test set images because U-Net based models only takes the image of size in ratio of 64\n    imgs_row_exp = int(np.ceil(np.divide(img_row, ratio)) * ratio)\n    imgs_col_exp = int(np.ceil(np.divide(img_col, ratio)) * ratio)\n\n    # For the images that need to be expanded more\n    # necessary to prevent boundary effect during prediction\n    if expand_more:\n        if (imgs_row_exp - self.row) < ratio:\n            imgs_row_exp = imgs_row_exp + int(ratio)\n\n        if (imgs_col_exp - self.col) < ratio:\n            imgs_col_exp = imgs_col_exp + int(ratio)\n\n    resized_img = cv2.copyMakeBorder(img, 0, imgs_row_exp - self.row, 0, imgs_col_exp - self.col, cv2.BORDER_REFLECT)\n\n    return resized_img, imgs_row_exp, imgs_col_exp", "\n# --------------------------------------------------------------------------------------------------------------\n\n\n@tf.function\ndef resize(img, height, width, is_flow, mask=None):\n  \"\"\"Resize an image or flow field to a new resolution.\n\n  In case a mask (per pixel {0,1} flag) is passed a weighted resizing is\n  performed to account for missing flow entries in the sparse flow field. The\n  weighting is based on the resized mask, which determines the 'amount of valid\n  flow vectors' that contributed to each individual resized flow vector. Hence,\n  multiplying by the reciprocal cancels out the effect of considering non valid\n  flow vectors.\n\n  Args:\n    img: tf.tensor, image or flow field to be resized of shape [b, h, w, c]\n    height: int, heigh of new resolution\n    width: int, width of new resolution\n    is_flow: bool, flag for scaling flow accordingly\n    mask: tf.tensor, mask (optional) per pixel {0,1} flag\n\n  Returns:\n    Resized and potentially scaled image or flow field (and mask).\n  \"\"\"\n\n  def _resize(img, mask=None):\n    # _, orig_height, orig_width, _ = img.shape.as_list()\n    orig_height = tf.shape(input=img)[1]\n    orig_width = tf.shape(input=img)[2]\n    \n    # if orig_height == height and orig_width == width:  # early return if no resizing is required\n    #   if mask is not None:\n    #     return img, mask\n    #   else:\n    #     return img\n\n    if mask is not None:\n      # multiply with mask, to ensure non-valid locations are zero\n      img = tf.math.multiply(img, mask)\n      # resize image\n      img_resized = tf.compat.v2.image.resize(\n          img, (tf.cast(height,'int32'), tf.cast(width,'int32')), antialias=True)\n      # resize mask (will serve as normalization weights)\n      mask_resized = tf.compat.v2.image.resize(\n          mask, (tf.cast(height,'int32'), tf.cast(width,'int32')), antialias=True)\n      # normalize sparse flow field and mask\n      img_resized = tf.math.multiply(img_resized,\n                                     tf.math.reciprocal_no_nan(mask_resized))\n      mask_resized = tf.math.multiply(mask_resized,\n                                      tf.math.reciprocal_no_nan(mask_resized))\n    else:\n      # normal resize without anti-alaising\n      prev_img_dtype = img.dtype\n      img_resized = tf.compat.v2.image.resize(img, (tf.cast(height,'int32'), tf.cast(width,'int32')))\n      # img_resized = tf.cast(img, prev_img_dtype)\n\n    if is_flow:\n      # If image is a flow image, scale flow values to be consistent with the\n      # new image size.\n      scaling = tf.reshape([\n          tf.cast(height,'float32') / tf.cast(orig_height, tf.float32),\n          tf.cast(width,'float32')  / tf.cast(orig_width, tf.float32)\n      ], [1, 1, 1, 2])\n      img_resized *= scaling\n\n    if mask is not None:\n      return img_resized, mask_resized\n    return img_resized\n\n  # Apply resizing at the right shape.\n  shape = img.shape.as_list()\n  if len(shape) == 3:\n    if mask is not None:\n      img_resized, mask_resized = _resize(img[None], mask[None])\n      return img_resized[0], mask_resized[0]\n    else:\n      return _resize(img[None])[0]\n  elif len(shape) == 4:\n    # Input at the right shape.\n    return _resize(img, mask)\n  elif len(shape) > 4:\n    # Reshape input to [b, h, w, c], resize and reshape back.\n    img_flattened = tf.reshape(img, [-1] + shape[-3:])\n    if mask is not None:\n      mask_flattened = tf.reshape(mask, [-1] + shape[-3:])\n      img_resized, mask_resized = _resize(img_flattened, mask_flattened)\n    else:\n      img_resized = _resize(img_flattened)\n    # There appears to be some bug in tf2 tf.function\n    # that fails to capture the value of height / width inside the closure,\n    # leading the height / width undefined here. Call set_shape to make it\n    # defined again.\n    img_resized.set_shape(\n        (img_resized.shape[0], height, width, img_resized.shape[3]))\n    result_img = tf.reshape(img_resized, shape[:-3] + img_resized.shape[-3:])\n    if mask is not None:\n      mask_resized.set_shape(\n          (mask_resized.shape[0], height, width, mask_resized.shape[3]))\n      result_mask = tf.reshape(mask_resized,\n                               shape[:-3] + mask_resized.shape[-3:])\n      return result_img, result_mask\n    return result_img\n  else:\n    raise ValueError('Cannot resize an image of shape', shape)", "\n@tf.function\ndef resize_uint8(img, height, width):\n  \"\"\"Resize an image or flow field to a new resolution.\n\n  In case a mask (per pixel {0,1} flag) is passed a weighted resizing is\n  performed to account for missing flow entries in the sparse flow field. The\n  weighting is based on the resized mask, which determines the 'amount of valid\n  flow vectors' that contributed to each individual resized flow vector. Hence,\n  multiplying by the reciprocal cancels out the effect of considering non valid\n  flow vectors.\n\n  Args:\n    img: tf.tensor, image or flow field to be resized of shape [b, h, w, c]\n    height: int, heigh of new resolution\n    width: int, width of new resolution\n    is_flow: bool, flag for scaling flow accordingly\n    mask: tf.tensor, mask (optional) per pixel {0,1} flag\n\n  Returns:\n    Resized and potentially scaled image or flow field (and mask).\n  \"\"\"\n\n  def _resize(img):\n      orig_height = tf.shape(input=img)[1]\n      orig_width = tf.shape(input=img)[2]\n      if orig_height == height and orig_width == width:\n          # early return if no resizing is required\n          return img\n\n      # normal resize without anti-alaising\n      prev_img_dtype = img.dtype\n      img_resized = tf.compat.v2.image.resize(img, (int(height), int(width)))\n      img_resized = tf.cast(img_resized, prev_img_dtype)\n\n      return img_resized\n\n  # Apply resizing at the right shape.\n  shape = img.shape.as_list()\n  if len(shape) == 3:\n      return _resize(img[None])[0]\n  elif len(shape) == 4:\n    # Input at the right shape.\n    return _resize(img)\n  elif len(shape) > 4:\n      # Reshape input to [b, h, w, c], resize and reshape back.\n      img_flattened = tf.reshape(img, [-1] + shape[-3:])\n      img_resized = _resize(img_flattened)\n      # There appears to be some bug in tf2 tf.function\n      # that fails to capture the value of height / width inside the closure,\n      # leading the height / width undefined here. Call set_shape to make it\n      # defined again.\n      img_resized.set_shape(\n          (img_resized.shape[0], height, width, img_resized.shape[3]))\n      result_img = tf.reshape(img_resized, shape[:-3] + img_resized.shape[-3:])\n      return result_img\n  else:\n      raise ValueError('Cannot resize an image of shape', shape)", "\n\ndef random_subseq(sequence, subseq_len):\n  \"\"\"Select a random subsequence of a given length.\"\"\"\n  seq_len = tf.shape(input=sequence)[0]\n  start_index = tf.random.uniform([],\n                                  minval=0,\n                                  maxval=seq_len - subseq_len + 1,\n                                  dtype=tf.int32)\n  subseq = sequence[start_index:start_index + subseq_len]\n  return subseq", "\n\ndef normalize_for_feature_metric_loss(features):\n  \"\"\"Normalize features for the feature-metric loss.\"\"\"\n  normalized_features = dict()\n  for key, feature_map in features.items():\n    # Normalize feature channels to have the same absolute activations.\n    norm_feature_map = feature_map / (\n        tf.reduce_sum(\n            input_tensor=abs(feature_map), axis=[0, 1, 2], keepdims=True) +\n        1e-16)\n    # Normalize every pixel feature across all channels to have mean 1.\n    norm_feature_map /= (\n        tf.reduce_sum(\n            input_tensor=abs(norm_feature_map), axis=[-1], keepdims=True) +\n        1e-16)\n    normalized_features[key] = norm_feature_map\n  return normalized_features", "\n\ndef l1(x):\n  return tf.abs(x + 1e-6)\n\n\ndef robust_l1(x):\n  \"\"\"Robust L1 metric.\"\"\"\n  return (x**2 + 0.001**2)**0.5\n", "\n\ndef abs_robust_loss(diff, eps=0.01, q=0.4):\n  \"\"\"The so-called robust loss used by DDFlow.\"\"\"\n  return tf.pow((tf.abs(diff) + eps), q)\n\n\ndef image_grads(image_batch, stride=1):\n  image_batch_gh = image_batch[:, stride:] - image_batch[:, :-stride]\n  image_batch_gw = image_batch[:, :, stride:] - image_batch[:, :, :-stride]\n  return image_batch_gh, image_batch_gw", "\n\ndef image_averages(image_batch):\n  image_batch_ah = (image_batch[:, 1:] + image_batch[:, :-1]) / 2.\n  image_batch_aw = (image_batch[:, :, 1:] + image_batch[:, :, :-1]) / 2\n  return image_batch_ah, image_batch_aw\n\n\ndef get_distance_metric_fns(distance_metrics):\n  \"\"\"Returns a dictionary of distance metrics.\"\"\"\n  output = {}\n  for key, distance_metric in distance_metrics.items():\n    if distance_metric == 'l1':\n      output[key] = l1\n    elif distance_metric == 'robust_l1':\n      output[key] = robust_l1\n    elif distance_metric == 'ddflow':\n      output[key] = abs_robust_loss\n    else:\n      raise ValueError('Unknown loss function')\n  return output", "def get_distance_metric_fns(distance_metrics):\n  \"\"\"Returns a dictionary of distance metrics.\"\"\"\n  output = {}\n  for key, distance_metric in distance_metrics.items():\n    if distance_metric == 'l1':\n      output[key] = l1\n    elif distance_metric == 'robust_l1':\n      output[key] = robust_l1\n    elif distance_metric == 'ddflow':\n      output[key] = abs_robust_loss\n    else:\n      raise ValueError('Unknown loss function')\n  return output", "\n\ndef compute_loss(\n    weights,\n    images,\n    contours,\n    flows,\n    warps,\n    valid_warp_masks,\n    not_occluded_masks,\n    fb_sq_diff,\n    fb_sum_sq,\n    warped_images,\n    warped_contours,\n    only_forward=False,\n    selfsup_transform_fns=None,\n    fb_sigma_teacher=0.003,\n    fb_sigma_student=0.03,\n    plot_dir=None,\n    distance_metrics=None,\n    smoothness_edge_weighting='gaussian',\n    stop_gradient_mask=True,\n    selfsup_mask='gaussian',\n    ground_truth_occlusions=None,\n    smoothness_at_level=2,\n):\n  \"\"\"Compute UFlow losses.\"\"\"\n  if distance_metrics is None:\n    distance_metrics = {\n        'photo': 'robust_l1',\n        'census': 'ddflow',\n    }\n  distance_metric_fns = get_distance_metric_fns(distance_metrics)\n  losses = dict()\n  for key in weights:\n    if key not in ['edge_constant']:\n      losses[key] = 0.0\n\n  compute_loss_for_these_flows = ['augmented-student']\n  # Count number of non self-sup pairs, for which we will apply the losses.\n  num_pairs = sum(\n      [1.0 for (i, j, c) in warps if c in compute_loss_for_these_flows])\n\n  # Iterate over image pairs.\n  for key in warps:\n    i, j, c = key\n\n    if c not in compute_loss_for_these_flows or (only_forward and i > j):\n      continue\n\n    if ground_truth_occlusions is None:\n      if stop_gradient_mask:\n        mask_level0 = tf.stop_gradient(not_occluded_masks[key][0] *\n                                       valid_warp_masks[key][0])\n      else:\n        mask_level0 = not_occluded_masks[key][0] * valid_warp_masks[key][0]\n    else:\n      # For using ground truth mask\n      if i > j:\n        continue\n      ground_truth_occlusions = 1.0 - tf.cast(ground_truth_occlusions,\n                                              tf.float32)\n      mask_level0 = tf.stop_gradient(ground_truth_occlusions *\n                                     valid_warp_masks[key][0])\n\n      height, width = valid_warp_masks[key][1].get_shape().as_list()[-3:-1]\n\n    if 'photo' in weights:\n      error = distance_metric_fns['photo'](images[i] - warped_images[key])\n      losses['photo'] += (\n          weights['photo'] * tf.reduce_sum(input_tensor=mask_level0 * error) /\n          (tf.reduce_sum(input_tensor=mask_level0) + 1e-16) / num_pairs)\n\n    if 'contour' in weights:\n      # import pdb; pdb.set_trace()\n      a_contour = contours[i]  # dtype='float32', values are either 0 or 255\n      a_warped_contour = warped_contours[key]  # dtype='float32', values range from 0 to 255\n\n      # a_warped_contour = tf.zeros_like(warped_contours[key], dtype='float32')  # yields 0.001 loss\n      # a_warped_contour = tf.ones_like(warped_contours[key], dtype='float32')  # yields 1 loss\n\n      # error1 = a_warped_contour - a_contour\n      # error2 = a_contour - a_warped_contour\n      #\n      # # clipping to ignore negative values\n      # cliped_error1 = tf.clip_by_value( error1, 0, 255, name=None )\n      # cliped_error2 = tf.clip_by_value( error2, 0, 255, name=None )\n      #\n      # error = cliped_error1 + cliped_error2\n      #\n      # error = distance_metric_fns['contour'](error)\n\n      error = a_warped_contour - a_contour\n      # error = tf.clip_by_value( error, 0, 255, name=None )\n      error = distance_metric_fns['contour'](error)\n\n      losses['contour'] += ( weights['contour'] * tf.reduce_sum(input_tensor=mask_level0 * error) /\n                              (tf.reduce_sum(input_tensor=mask_level0) + 1e-16) / num_pairs )\n\n    if 'flow_magnitude' in weights:\n        # import pdb; pdb.set_trace()\n        losses['flow_magnitude'] += weights['flow_magnitude'] * tf.reduce_mean(tf.abs(flows[key][0]))\n\n    if 'smooth2' in weights or 'smooth1' in weights or 'contour_smooth2' in weights or 'contour_smooth1' in weights:\n\n      edge_constant = 0.0\n      if 'edge_constant' in weights:\n        edge_constant = weights['edge_constant']\n\n      abs_fn = None\n      if smoothness_edge_weighting == 'gaussian':\n        abs_fn = lambda x: x**2\n      elif smoothness_edge_weighting == 'exponential':\n        abs_fn = abs\n\n      # Compute image gradients and sum them up to match the receptive field\n      # of the flow gradients, which are computed at 1/4 resolution.\n      images_level0 = images[i]\n      height, width = images_level0.shape.as_list()[-3:-1]\n      # Resize two times for a smoother result.\n      images_level1 = resize(\n          images_level0, int(height) // 2, int(width) // 2, is_flow=False)\n      images_level2 = resize(\n          images_level1, int(height) // 4, int(width) // 4, is_flow=False)\n      images_at_level = [images_level0, images_level1, images_level2]\n\n      if 'smooth1' in weights:\n\n        img_gx, img_gy = image_grads(images_at_level[smoothness_at_level])\n        weights_x = tf.exp(-tf.reduce_mean(\n            input_tensor=(abs_fn(edge_constant * img_gx)),\n            axis=-1,\n            keepdims=True))\n        weights_y = tf.exp(-tf.reduce_mean(\n            input_tensor=(abs_fn(edge_constant * img_gy)),\n            axis=-1,\n            keepdims=True))\n\n        # Compute first derivatives of the predicted smoothness.\n        flow_gx, flow_gy = image_grads(flows[key][smoothness_at_level])\n\n        # Compute weighted smoothness\n        losses['smooth1'] += (\n            weights['smooth1'] *\n            (tf.reduce_mean(input_tensor=weights_x * robust_l1(flow_gx)) +\n             tf.reduce_mean(input_tensor=weights_y * robust_l1(flow_gy))) / 2. /\n            num_pairs)\n\n        if plot_dir is not None:\n          uflow_plotting.plot_smoothness(key, images, weights_x, weights_y,\n                                         robust_l1(flow_gx), robust_l1(flow_gy),\n                                         flows, plot_dir)\n\n      if 'smooth2' in weights:\n\n        img_gx, img_gy = image_grads(\n            images_at_level[smoothness_at_level], stride=2)\n        weights_xx = tf.exp(-tf.reduce_mean(\n            input_tensor=(abs_fn(edge_constant * img_gx)),\n            axis=-1,\n            keepdims=True))\n        weights_yy = tf.exp(-tf.reduce_mean(\n            input_tensor=(abs_fn(edge_constant * img_gy)),\n            axis=-1,\n            keepdims=True))\n\n        # Compute second derivatives of the predicted smoothness.\n        flow_gx, flow_gy = image_grads(flows[key][smoothness_at_level])\n        flow_gxx, unused_flow_gxy = image_grads(flow_gx)\n        unused_flow_gyx, flow_gyy = image_grads(flow_gy)\n\n        # Compute weighted smoothness\n        losses['smooth2'] += (\n            weights['smooth2'] *\n            (tf.reduce_mean(input_tensor=weights_xx * robust_l1(flow_gxx)) +\n             tf.reduce_mean(input_tensor=weights_yy * robust_l1(flow_gyy))) /\n            2. / num_pairs)\n\n        if plot_dir is not None:\n          uflow_plotting.plot_smoothness(key, images, weights_xx, weights_yy,\n                                         robust_l1(flow_gxx),\n                                         robust_l1(flow_gyy), flows, plot_dir)\n\n      if 'contour_smooth1' in weights:\n        # import pdb; pdb.set_trace()\n        img_gx, img_gy = image_grads(images_at_level[smoothness_at_level])\n\n        # Compute first derivatives of the predicted smoothness.\n        flow_gx, flow_gy = image_grads(flows[key][smoothness_at_level])\n\n        # get contour\n        a_segmentation = contours[i]\n        a_segmentation = tf.cast(a_segmentation, dtype=img_gx.dtype)\n\n        _, height, width, _ = flow_gx.shape.as_list()\n        a_segmentation_x = tf.image.resize(a_segmentation, [height, width])\n\n        _, height, width, _ = flow_gy.shape.as_list()\n        a_segmentation_y = tf.image.resize(a_segmentation, [height, width])\n\n        # use contour information AND image gradient\n        weights_x = tf.exp(-tf.reduce_mean(\n            input_tensor=(abs_fn(edge_constant * ( a_segmentation_x + img_gx) )),\n            axis=-1,\n            keepdims=True))\n        weights_y = tf.exp(-tf.reduce_mean(\n            input_tensor=(abs_fn(edge_constant * ( a_segmentation_y + img_gy) )),\n            axis=-1,\n            keepdims=True))\n\n        # Compute weighted smoothness\n        losses['contour_smooth1'] += (\n                weights['contour_smooth1'] *\n                (tf.reduce_mean(input_tensor=weights_x * robust_l1(flow_gx)) +\n                 tf.reduce_mean(input_tensor=weights_y * robust_l1(flow_gy))) / 2. /\n                num_pairs)\n\n        if plot_dir is not None:\n            uflow_plotting.plot_smoothness(key, images, weights_x, weights_y,\n                                           robust_l1(flow_gx), robust_l1(flow_gy),\n                                           flows, plot_dir)\n\n\n      if 'contour_smooth2' in weights:\n        # import pdb; pdb.set_trace()\n        img_gx, img_gy = image_grads(\n            images_at_level[smoothness_at_level], stride=2)\n\n        # Compute second derivatives of the predicted smoothness.\n        flow_gx, flow_gy = image_grads(flows[key][smoothness_at_level])\n        flow_gxx, unused_flow_gxy = image_grads(flow_gx)\n        unused_flow_gyx, flow_gyy = image_grads(flow_gy)\n\n        # get contour\n        a_segmentation = contours[i]\n        a_segmentation = tf.cast(a_segmentation, dtype=img_gx.dtype)\n\n        _, height, width, _ = flow_gxx.shape.as_list()\n        a_segmentation_xx = tf.image.resize(a_segmentation, [height, width])\n\n        _, height, width, _ = flow_gyy.shape.as_list()\n        a_segmentation_yy = tf.image.resize(a_segmentation, [height, width])\n\n        # use contour information AND image gradient\n        weights_xx = tf.exp(-tf.reduce_mean(\n            input_tensor=(abs_fn(edge_constant * ( a_segmentation_xx + img_gx) )),\n            axis=-1,\n            keepdims=True))\n        weights_yy = tf.exp(-tf.reduce_mean(\n            input_tensor=(abs_fn(edge_constant * ( a_segmentation_yy + img_gy) )),\n            axis=-1,\n            keepdims=True))\n\n        # Compute weighted smoothness\n        losses['contour_smooth2'] += (\n            weights['contour_smooth2'] *\n            (tf.reduce_mean(input_tensor=weights_xx * robust_l1(flow_gxx)) +\n             tf.reduce_mean(input_tensor=weights_yy * robust_l1(flow_gyy))) /\n            2. / num_pairs)\n\n        if plot_dir is not None:\n          uflow_plotting.plot_smoothness( key, images, weights_xx, weights_yy,\n                                         robust_l1(flow_gxx),\n                                         robust_l1(flow_gyy), flows, plot_dir )\n\n    if 'ssim' in weights:\n      ssim_error, avg_weight = weighted_ssim(warped_images[key], images[i],\n                                             tf.squeeze(mask_level0, axis=-1))\n\n      losses['ssim'] += weights['ssim'] * (\n          tf.reduce_sum(input_tensor=ssim_error * avg_weight) /\n          (tf.reduce_sum(input_tensor=avg_weight) + 1e-16) / num_pairs)\n\n    if 'census' in weights:\n      losses['census'] += weights['census'] * census_loss(\n          images[i],\n          warped_images[key],\n          mask_level0,\n          contours[i],\n          distance_metric_fn=distance_metric_fns['census']) / num_pairs\n\n    if 'selfsup' in weights:\n      assert selfsup_transform_fns is not None\n      _, h, w, _ = flows[key][2].shape.as_list()\n      teacher_flow = flows[(i, j, 'original-teacher')][2]\n      student_flow = flows[(i, j, 'transformed-student')][2]\n      teacher_flow = selfsup_transform_fns[2](\n          teacher_flow, i_or_ij=(i, j), is_flow=True)\n      if selfsup_mask == 'gaussian':\n        student_fb_consistency = tf.exp(\n            -fb_sq_diff[(i, j, 'transformed-student')][2] /\n            (fb_sigma_student**2 * (h**2 + w**2)))\n        teacher_fb_consistency = tf.exp(\n            -fb_sq_diff[(i, j, 'original-teacher')][2] / (fb_sigma_teacher**2 *\n                                                          (h**2 + w**2)))\n      elif selfsup_mask == 'advection':\n        student_fb_consistency = not_occluded_masks[(i, j,\n                                                     'transformed-student')][2]\n        teacher_fb_consistency = not_occluded_masks[(i, j,\n                                                     'original-teacher')][2]\n      elif selfsup_mask == 'ddflow':\n        threshold_student = 0.01 * (fb_sum_sq[\n            (i, j, 'transformed-student')][2]) + 0.5\n        threshold_teacher = 0.01 * (fb_sum_sq[\n            (i, j, 'original-teacher')][2]) + 0.5\n        student_fb_consistency = tf.cast(\n            fb_sq_diff[(i, j, 'transformed-student')][2] < threshold_student,\n            tf.float32)\n        teacher_fb_consistency = tf.cast(\n            fb_sq_diff[(i, j, 'original-teacher')][2] < threshold_teacher,\n            tf.float32)\n      else:\n        raise ValueError('Unknown selfsup_mask', selfsup_mask)\n\n      student_mask = 1. - (\n          student_fb_consistency *\n          valid_warp_masks[(i, j, 'transformed-student')][2])\n      teacher_mask = (\n          teacher_fb_consistency *\n          valid_warp_masks[(i, j, 'original-teacher')][2])\n      teacher_mask = selfsup_transform_fns[2](\n          teacher_mask, i_or_ij=(i, j), is_flow=False)\n      error = robust_l1(tf.stop_gradient(teacher_flow) - student_flow)\n      mask = tf.stop_gradient(teacher_mask * student_mask)\n      losses['selfsup'] += (\n          weights['selfsup'] * tf.reduce_sum(input_tensor=mask * error) /\n          (tf.reduce_sum(input_tensor=tf.ones_like(mask)) + 1e-16) / num_pairs)\n      if plot_dir is not None:\n        uflow_plotting.plot_selfsup(key, images, flows, teacher_flow,\n                                    student_flow, error, teacher_mask,\n                                    student_mask, mask, selfsup_transform_fns,\n                                    plot_dir)\n\n  losses['total'] = sum(losses.values())\n\n  return losses", "\n\ndef supervised_loss(weights, ground_truth_flow, ground_truth_valid,\n                    predicted_flows):\n  \"\"\"Returns a supervised l1 loss when ground-truth flow is provided.\"\"\"\n  losses = {}\n  # ground truth flow is given from image 0 to image 1\n  predicted_flow = predicted_flows[(0, 1, 'augmented')][0]\n  # resize flow to match ground truth (only changes resolution if ground truth\n  # flow was not resized during loading (resize_gt_flow=False)\n  _, height, width, _ = ground_truth_flow.get_shape().as_list()\n  predicted_flow = resize(predicted_flow, height, width, is_flow=True)\n  # compute error/loss metric\n  error = robust_l1(ground_truth_flow - predicted_flow)\n  if ground_truth_valid is None:\n    b, h, w, _ = ground_truth_flow.shape.as_list()\n    ground_truth_valid = tf.ones((b, h, w, 1), tf.float32)\n  losses['supervision'] = (\n      weights['supervision'] *\n      tf.reduce_sum(input_tensor=ground_truth_valid * error) /\n      (tf.reduce_sum(input_tensor=ground_truth_valid) + 1e-16))\n  losses['total'] = losses['supervision']\n\n  return losses", "\n\ndef compute_features_and_flow(\n    feature_model,\n    flow_model,\n    batch,\n    batch_without_aug,\n    training,\n    build_selfsup_transformations=None,\n    teacher_feature_model=None,\n    teacher_flow_model=None,\n    teacher_image_version='original',\n    ground_truth_segmentations=None,\n    ground_truth_tracking_points=None\n):\n  \"\"\"Compute features and flow for an image batch.\n\n  Args:\n    feature_model: A model to compute features for flow.\n    flow_model: A model to compute flow.\n    batch: A tf.tensor of shape [b, seq, h, w, c] holding a batch of triplets.\n    batch_without_aug: Batch without photometric augmentation\n    training: bool that tells the model to use training or inference code.\n    build_selfsup_transformations: A function which, when called with images\n      and flows, populates the images and flows dictionary with student images\n      and modified teacher flows corresponding to the student images.\n    teacher_feature_model: None or instance of of feature model. If None, will\n      not compute features and images for teacher distillation.\n    teacher_flow_model: None or instance of flow model. If None, will not\n      compute features and images for teacher distillation.\n    teacher_image_version: str, either 'original' or 'augmented'\n\n  Returns:\n    A tuple consisting of the images, the extracted features, the estimated\n    flows, and the upsampled refined flows.\n  \"\"\"\n\n  images = dict()\n  flows = dict()\n  features = dict()\n\n  seq_len = int(batch.shape[1])\n\n  perform_selfsup = (\n      training and teacher_feature_model is not None and\n      teacher_flow_model is not None and\n      build_selfsup_transformations is not None)\n  if perform_selfsup:\n    selfsup_transform_fns = build_selfsup_transformations()\n  else:\n    selfsup_transform_fns = None\n\n  for i in range(seq_len):\n    # Populate teacher images with native, unmodified images.\n    images[(i, 'original')] = batch_without_aug[:, i]\n    images[(i, 'augmented')] = batch[:, i]\n    if perform_selfsup:\n      images[(i, 'transformed')] = selfsup_transform_fns[0](\n          images[(i, 'augmented')], i_or_ij=i, is_flow=False)\n\n\n\n  for key, image in images.items():\n    i, image_version = key\n    # if perform_selfsup and image_version == 'original':\n    if perform_selfsup and image_version == teacher_image_version:\n      features[(i, 'original-teacher')] = teacher_feature_model(\n          image, split_features_by_sample=False, training=False)\n\n    features[(i, image_version + '-student')] = feature_model(\n        image, split_features_by_sample=False, training=training)\n\n  # Only use original images and features computed on those for computing\n  # photometric losses down the road.\n  images = {i: images[(i, 'original')] for i in range(seq_len)}\n\n\n  # --------------------------------------------\n  ground_truth_segmentation1 = ground_truth_segmentations[:,0,:,:,:]\n  ground_truth_segmentation2 = ground_truth_segmentations[:,1,:,:,:]\n  # import pdb; pdb.set_trace()\n  # ----------------------------------------------\n\n\n  # Compute flow for all pairs of consecutive images that have the same (or no)\n  # transformation applied to them, i.e. that have the same t.\n  # pylint:disable=dict-iter-missing-items\n  for (i, ti) in features:\n    for (j, tj) in features:\n      if (i + 1 == j or i - 1 == j) and ti == tj:\n        t = ti\n        key = (i, j, t)\n        # No need to compute the flow for student applied to the original\n        # image. We just need the features from that for the photometric loss.\n        \n        if t in ['augmented-student', 'transformed-student']:\n          # Compute flow from i to j, defined in image i.\n          flow = flow_model(\n              features[(i, t)], features[(j, t)], ground_truth_segmentation1, ground_truth_segmentation2, training=training)\n\n        elif t in ['original-teacher']:\n          flow = teacher_flow_model(\n              features[(i, t)], features[(j, t)], ground_truth_segmentation1, ground_truth_segmentation2, training=False)\n        else:\n          continue\n\n        # Keep flows at levels 0-2.\n        flow_level2 = flow[0]\n        flow_level1 = upsample(flow_level2, is_flow=True)\n        flow_level0 = upsample(flow_level1, is_flow=True)\n        flows[key] = [flow_level0, flow_level1, flow_level2]\n\n  return flows, selfsup_transform_fns", "\n\ndef compute_flow_for_supervised_loss(feature_model, flow_model, batch,\n                                     training):\n  \"\"\"Compute features and flow for an image batch.\n\n  Args:\n    feature_model: A model to compute features for flow.\n    flow_model: A model to compute flow.\n    batch: A tf.tensor of shape [b, seq, h, w, c] holding a batch of triplets.\n    training: bool that tells the model to use training or inference code.\n\n  Returns:\n    A tuple consisting of the images, the extracted features, the estimated\n    flows, and the upsampled refined flows.\n  \"\"\"\n\n  flows = dict()\n\n  image_0 = batch[:, 0]\n  image_1 = batch[:, 1]\n\n  features_0 = feature_model(\n      image_0, split_features_by_sample=False, training=training)\n  features_1 = feature_model(\n      image_1, split_features_by_sample=False, training=training)\n\n  # -------------------------------------------\n\n  # ------------------------------------------\n\n\n  flow = flow_model(features_0, features_1, training=training)\n  flow_level2 = flow[0]\n  flow_level1 = upsample(flow_level2, is_flow=True)\n  flow_level0 = upsample(flow_level1, is_flow=True)\n  flows[(0, 1, 'augmented')] = [flow_level0, flow_level1, flow_level2]\n\n  return flows", "\n\ndef random_crop(batch, max_offset_height=32, max_offset_width=32):\n  \"\"\"Randomly crop a batch of images.\n\n  Args:\n    batch: a 4-D tensor of shape [batch_size, height, width, num_channels].\n    max_offset_height: an int, the maximum vertical coordinate of the top left\n      corner of the cropped result.\n    max_offset_width: an int, the maximum horizontal coordinate of the top left\n      corner of the cropped result.\n\n  Returns:\n    a pair of 1) the cropped images in form of a tensor of shape\n    [batch_size, height-max_offset, width-max_offset, num_channels],\n    2) an offset tensor of shape [batch_size, 2] for height and width offsets.\n  \"\"\"\n\n  # Compute current shapes and target shapes of the crop.\n  batch_size, height, width, num_channels = batch.shape\n  target_height = height - max_offset_height\n  target_width = width - max_offset_width\n\n  # Randomly sample offsets.\n  offsets_height = tf.random.uniform([batch_size],\n                                     maxval=max_offset_height + 1,\n                                     dtype=tf.int32)\n  offsets_width = tf.random.uniform([batch_size],\n                                    maxval=max_offset_width + 1,\n                                    dtype=tf.int32)\n  offsets = tf.stack([offsets_height, offsets_width], axis=-1)\n\n  # Loop over the batch and perform cropping.\n  cropped_images = []\n  for image, offset_height, offset_width in zip(batch, offsets_height,\n                                                offsets_width):\n    cropped_images.append(\n        tf.slice(\n            image,\n            begin=[offset_height, offset_width, 0],\n            size=[target_height, target_width, num_channels]))\n  cropped_batch = tf.stack(cropped_images)\n\n  return cropped_batch, offsets", "\n\ndef random_shift(batch, max_shift_height=32, max_shift_width=32):\n  \"\"\"Randomly shift a batch of images (with wrap around).\n\n  Args:\n    batch: a 4-D tensor of shape [batch_size, height, width, num_channels].\n    max_shift_height: an int, the maximum shift along the height dimension in\n      either direction.\n    max_shift_width: an int, the maximum shift along the width dimension in\n      either direction\n\n  Returns:\n    a pair of 1) the shifted images in form of a tensor of shape\n    [batch_size, height, width, num_channels] and 2) the random shifts of shape\n    [batch_size, 2], where positive numbers mean the image was shifted\n    down / right and negative numbers mean it was shifted up / left.\n  \"\"\"\n\n  # Randomly sample by how much the images are being shifted.\n  batch_size, _, _, _ = batch.shape\n  shifts_height = tf.random.uniform([batch_size],\n                                    minval=-max_shift_height,\n                                    maxval=max_shift_height + 1,\n                                    dtype=tf.int32)\n  shifts_width = tf.random.uniform([batch_size],\n                                   minval=-max_shift_width,\n                                   maxval=max_shift_width + 1,\n                                   dtype=tf.int32)\n  shifts = tf.stack([shifts_height, shifts_width], axis=-1)\n\n  # Loop over the batch and shift the images\n  shifted_images = []\n  for image, shift_height, shift_width in zip(batch, shifts_height,\n                                              shifts_width):\n    shifted_images.append(\n        tf.roll(image, shift=[shift_height, shift_width], axis=[0, 1]))\n  shifted_images = tf.stack(shifted_images)\n\n  return shifted_images, shifts", "\n\ndef randomly_shift_features(feature_pyramid,\n                            max_shift_height=64,\n                            max_shift_width=64):\n  \"\"\"Randomly shift a batch of images (with wrap around).\n\n  Args:\n    feature_pyramid: a list of 4-D tensors of shape [batch_size, height, width,\n      num_channels], where the first entry is at level 1 (image size / 2).\n    max_shift_height: an int, the maximum shift along the height dimension in\n      either direction.\n    max_shift_width: an int, the maximum shift along the width dimension in\n      either direction\n\n  Returns:\n    a pair of 1) a list of shifted feature images as tensors of shape\n    [batch_size, height, width, num_channels] and 2) the random shifts of shape\n    [batch_size, 2], where positive numbers mean the image was shifted\n    down / right and negative numbers mean it was shifted up / left.\n  \"\"\"\n  batch_size, height, width = feature_pyramid[0].shape[:3]\n  # Image size is double the size of the features at level1 (index 0).\n  height *= 2\n  width *= 2\n\n  # Transform the shift range to the size of the top level of the pyramid.\n  top_level_scale = 2**len(feature_pyramid)\n  max_shift_height_top_level = max_shift_height // top_level_scale\n  max_shift_width_top_level = max_shift_width // top_level_scale\n\n  # Randomly sample by how much the images are being shifted at the top level\n  # and scale the shift back to level 0 (original image resolution).\n  shifts_height = top_level_scale * tf.random.uniform(\n      [batch_size],\n      minval=-max_shift_height_top_level,\n      maxval=max_shift_height_top_level + 1,\n      dtype=tf.int32)\n  shifts_width = top_level_scale * tf.random.uniform(\n      [batch_size],\n      minval=-max_shift_width_top_level,\n      maxval=max_shift_width_top_level + 1,\n      dtype=tf.int32)\n  shifts = tf.stack([shifts_height, shifts_width], axis=-1)\n\n  # Iterate over pyramid levels.\n  shifted_features = []\n  for level, feature_image_batch in enumerate(feature_pyramid, start=1):\n    shifts_at_this_level = shifts // 2**level\n    # pylint:disable=g-complex-comprehension\n    shifted_features.append(\n        tf.stack([\n            tf.roll(\n                feature_image_batch[i],\n                shift=shifts_at_this_level[i],\n                axis=[0, 1]) for i in range(batch_size)\n        ],\n                 axis=0))\n\n  return shifted_features, tf.cast(shifts, dtype=tf.float32)", "\n\ndef zero_mask_border(mask_bhw3, patch_size):\n  \"\"\"Used to ignore border effects from census_transform.\"\"\"\n  mask_padding = patch_size // 2\n  mask = mask_bhw3[:, mask_padding:-mask_padding, mask_padding:-mask_padding, :]\n  return tf.pad(\n      tensor=mask,\n      paddings=[[0, 0], [mask_padding, mask_padding],\n                [mask_padding, mask_padding], [0, 0]])", "\n\ndef census_transform(image, patch_size):\n  \"\"\"The census transform as described by DDFlow.\n\n  See the paper at https://arxiv.org/abs/1902.09145\n\n  Args:\n    image: tensor of shape (b, h, w, c)\n    patch_size: int\n  Returns:\n    image with census transform applied\n  \"\"\"\n  intensities = tf.image.rgb_to_grayscale(image) * 255\n\n  # identity matrix of 49x49 to kernel of shape [filter_height, filter_width, in_channels, out_channels\n  kernel = tf.reshape(\n      tf.eye(patch_size * patch_size),\n      (patch_size, patch_size, 1, patch_size * patch_size))\n  neighbors = tf.nn.conv2d(\n      input=intensities, filters=kernel, strides=[1, 1, 1, 1], padding='SAME')\n  diff = neighbors - intensities\n  # Coefficients adopted from DDFlow.\n  diff_norm = diff / tf.sqrt(.81 + tf.square(diff))\n\n  return diff_norm", "\n\ndef soft_hamming(a_bhwk, b_bhwk, thresh=.1):\n  \"\"\"A soft hamming distance between tensor a_bhwk and tensor b_bhwk.\n\n  Args:\n    a_bhwk: tf.Tensor of shape (batch, height, width, features)\n    b_bhwk: tf.Tensor of shape (batch, height, width, features)\n    thresh: float threshold\n\n  Returns:\n    a tensor with approx. 1 in (h, w) locations that are significantly\n    more different than thresh and approx. 0 if significantly less\n    different than thresh.\n  \"\"\"\n  sq_dist_bhwk = tf.square(a_bhwk - b_bhwk)\n  soft_thresh_dist_bhwk = sq_dist_bhwk / (thresh + sq_dist_bhwk)\n  return tf.reduce_sum( input_tensor=soft_thresh_dist_bhwk, axis=3, keepdims=True )", "\n\ndef census_loss(image_a_bhw3,\n                image_b_bhw3,\n                mask_bhw3,\n                ground_truth_segmentation,\n                patch_size=7,\n                distance_metric_fn=abs_robust_loss):\n  \"\"\"Compares the similarity of the census transform of two images.\"\"\"\n\n  census_image_a_bhwk = census_transform(image_a_bhw3, patch_size)\n  census_image_b_bhwk = census_transform(image_b_bhw3, patch_size)\n\n  # ------------------------ Contour Loss --------------------\n  # if ground_truth_segmentation is not None:\n    # import pdb; pdb.set_trace()\n    # ground_truth_segmentation = tf.cast(ground_truth_segmentation, dtype=image_a_bhw3.dtype)\n    # census_image_a_bhwk = tf.multiply(census_image_a_bhwk, ground_truth_segmentation)\n    # census_image_b_bhwk = tf.multiply(census_image_b_bhwk, ground_truth_segmentation)\n\n  # ----------------------------------------------------------\n\n  hamming_bhw1 = soft_hamming(census_image_a_bhwk, census_image_b_bhwk)\n\n  # Set borders of mask to zero to ignore edge effects.\n  padded_mask_bhw3 = zero_mask_border(mask_bhw3, patch_size)\n  diff = distance_metric_fn(hamming_bhw1)\n  diff *= padded_mask_bhw3\n  diff_sum = tf.reduce_sum(input_tensor=diff)\n  loss_mean = diff_sum / (\n      tf.reduce_sum(input_tensor=tf.stop_gradient(padded_mask_bhw3) + 1e-6))\n  return loss_mean", "\n\ndef time_it(f, num_reps=1, execute_once_before=False):\n  \"\"\"Times a tensorflow function in eager mode.\n\n  Args:\n    f: function with no arguments that should be timed.\n    num_reps: int, number of repetitions for timing.\n    execute_once_before: boolean, whether to execute the function once before\n      timing in order to not count the tf.function compile time.\n\n  Returns:\n    tuple of the average time in ms and the functions output.\n  \"\"\"\n  assert num_reps >= 1\n  # Execute f once before timing it to allow tf.function to compile the graph.\n  if execute_once_before:\n    x = f()\n  # Make sure that there is nothing still running on the GPU by waiting for the\n  # completion of a bogus command.\n  _ = tf.square(tf.random.uniform([1])).numpy()\n  # Time f for a number of repetitions.\n  start_in_s = time.time()\n  for _ in range(num_reps):\n    x = f()\n    # Make sure that f has finished and was not just enqueued by using another\n    # bogus command. This will overestimate the computing time of f by waiting\n    # until the result has been copied to main memory. Calling reduce_sum\n    # reduces that overestimation.\n    if isinstance(x, tuple) or isinstance(x, list):\n      _ = [tf.reduce_sum(input_tensor=xi).numpy() for xi in x]\n    else:\n      _ = tf.reduce_sum(input_tensor=x).numpy()\n  end_in_s = time.time()\n  # Compute the average time in ms.\n  avg_time = (end_in_s - start_in_s) * 1000. / float(num_reps)\n  return avg_time, x", "\n\ndef _avg_pool3x3(x):\n  return tf.nn.avg_pool(x, [1, 3, 3, 1], [1, 1, 1, 1], 'VALID')\n\n\ndef weighted_ssim(x, y, weight, c1=float('inf'), c2=9e-6, weight_epsilon=0.01):\n  \"\"\"Computes a weighted structured image similarity measure.\n\n  See https://en.wikipedia.org/wiki/Structural_similarity#Algorithm. The only\n  difference here is that not all pixels are weighted equally when calculating\n  the moments - they are weighted by a weight function.\n\n  Args:\n    x: A tf.Tensor representing a batch of images, of shape [B, H, W, C].\n    y: A tf.Tensor representing a batch of images, of shape [B, H, W, C].\n    weight: A tf.Tensor of shape [B, H, W], representing the weight of each\n      pixel in both images when we come to calculate moments (means and\n      correlations).\n    c1: A floating point number, regularizes division by zero of the means.\n    c2: A floating point number, regularizes division by zero of the second\n      moments.\n    weight_epsilon: A floating point number, used to regularize division by the\n      weight.\n\n  Returns:\n    A tuple of two tf.Tensors. First, of shape [B, H-2, W-2, C], is scalar\n    similarity loss oer pixel per channel, and the second, of shape\n    [B, H-2. W-2, 1], is the average pooled `weight`. It is needed so that we\n    know how much to weigh each pixel in the first tensor. For example, if\n    `'weight` was very small in some area of the images, the first tensor will\n    still assign a loss to these pixels, but we shouldn't take the result too\n    seriously.\n  \"\"\"\n  if c1 == float('inf') and c2 == float('inf'):\n    raise ValueError('Both c1 and c2 are infinite, SSIM loss is zero. This is '\n                     'likely unintended.')\n  weight = tf.expand_dims(weight, -1)\n  average_pooled_weight = _avg_pool3x3(weight)\n  weight_plus_epsilon = weight + weight_epsilon\n  inverse_average_pooled_weight = 1.0 / (average_pooled_weight + weight_epsilon)\n\n  def weighted_avg_pool3x3(z):\n    wighted_avg = _avg_pool3x3(z * weight_plus_epsilon)\n    return wighted_avg * inverse_average_pooled_weight\n\n  mu_x = weighted_avg_pool3x3(x)\n  mu_y = weighted_avg_pool3x3(y)\n  sigma_x = weighted_avg_pool3x3(x**2) - mu_x**2\n  sigma_y = weighted_avg_pool3x3(y**2) - mu_y**2\n  sigma_xy = weighted_avg_pool3x3(x * y) - mu_x * mu_y\n  if c1 == float('inf'):\n    ssim_n = (2 * sigma_xy + c2)\n    ssim_d = (sigma_x + sigma_y + c2)\n  elif c2 == float('inf'):\n    ssim_n = 2 * mu_x * mu_y + c1\n    ssim_d = mu_x**2 + mu_y**2 + c1\n  else:\n    ssim_n = (2 * mu_x * mu_y + c1) * (2 * sigma_xy + c2)\n    ssim_d = (mu_x**2 + mu_y**2 + c1) * (sigma_x + sigma_y + c2)\n  result = ssim_n / ssim_d\n  return tf.clip_by_value((1 - result) / 2, 0, 1), average_pooled_weight", ""]}
{"filename": "src/uflow_augmentation.py", "chunked_list": ["# coding=utf-8\n# Copyright 2021 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"UFlow augmentation.\n\nThis library contains various augmentation functions.", "\nThis library contains various augmentation functions.\n\"\"\"\n\n# pylint:disable=g-importing-member\nfrom functools import partial\nfrom math import pi\n\nimport gin\nimport gin.tf", "import gin\nimport gin.tf\nimport tensorflow as tf\nfrom tensorflow_addons import image as tfa_image\n\nfrom src import uflow_utils\n\n\ndef apply_augmentation(images, flow=None, mask=None,\n                       crop_height=640, crop_width=640):\n  \"\"\"Applies photometric and geometric augmentations to images and flow.\"\"\"\n  # ensure sequence length of two, to be able to unstack images\n  images = tf.ensure_shape(images, (2, None, None, None))\n  # apply geometric augmentation functions\n  images, flow, mask = geometric_augmentation(\n      images, flow, mask, crop_height, crop_width)\n  # apply photometric augmentation functions\n  images_aug = photometric_augmentation(images)\n\n  # return flow and mask if available\n  if flow is not None:\n    return images_aug, images, flow, mask\n  return images_aug, images", "def apply_augmentation(images, flow=None, mask=None,\n                       crop_height=640, crop_width=640):\n  \"\"\"Applies photometric and geometric augmentations to images and flow.\"\"\"\n  # ensure sequence length of two, to be able to unstack images\n  images = tf.ensure_shape(images, (2, None, None, None))\n  # apply geometric augmentation functions\n  images, flow, mask = geometric_augmentation(\n      images, flow, mask, crop_height, crop_width)\n  # apply photometric augmentation functions\n  images_aug = photometric_augmentation(images)\n\n  # return flow and mask if available\n  if flow is not None:\n    return images_aug, images, flow, mask\n  return images_aug, images", "\n\n@gin.configurable\ndef photometric_augmentation(images,\n                             augment_color_swap=True,\n                             augment_hue_shift=True,\n                             augment_saturation=False,\n                             augment_brightness=False,\n                             augment_contrast=False,\n                             augment_gaussian_noise=False,\n                             augment_brightness_individual=False,\n                             augment_contrast_individual=False,\n                             max_delta_hue=0.5,\n                             min_bound_saturation=0.8,\n                             max_bound_saturation=1.2,\n                             max_delta_brightness=0.1,\n                             min_bound_contrast=0.8,\n                             max_bound_contrast=1.2,\n                             min_bound_gaussian_noise=0.0,\n                             max_bound_gaussian_noise=0.02,\n                             max_delta_brightness_individual=0.02,\n                             min_bound_contrast_individual=0.95,\n                             max_bound_contrast_individual=1.05):\n  \"\"\"Applies photometric augmentations to an image pair.\"\"\"\n  # Randomly permute colors by rolling and reversing.\n  # This covers all permutations.\n  \n  if augment_color_swap:\n    r = tf.random.uniform([], maxval=3, dtype=tf.int32)\n    images = tf.roll(images, r, axis=-1)\n    r = tf.equal(tf.random.uniform([], maxval=2, dtype=tf.int32), 1)\n    images = tf.cond(pred=r,\n                     true_fn=lambda: tf.reverse(images, axis=[-1]),\n                     false_fn=lambda: images)\n\n  if augment_hue_shift:\n    images = tf.image.random_hue(images, max_delta_hue)\n\n  if augment_saturation:\n    images = tf.image.random_saturation(\n        images, min_bound_saturation, max_bound_saturation)\n\n  if augment_brightness:\n    images = tf.image.random_brightness(images, max_delta_brightness)\n\n  if augment_contrast:\n    images = tf.image.random_contrast(\n        images, min_bound_contrast, max_bound_contrast)\n\n  if augment_gaussian_noise:\n    sigma = tf.random.uniform([],\n                              minval=min_bound_gaussian_noise,\n                              maxval=max_bound_gaussian_noise,\n                              dtype=tf.float32)\n    noise = tf.random.normal(\n        tf.shape(input=images), stddev=sigma, dtype=tf.float32)\n    images = images + noise\n\n  # perform relative photometric augmentation (individually per image)\n  image_1, image_2 = tf.unstack(images)\n  if augment_brightness_individual:\n    image_1 = tf.image.random_contrast(\n        image_1, min_bound_contrast_individual, max_bound_contrast_individual)\n    image_2 = tf.image.random_contrast(\n        image_2, min_bound_contrast_individual, max_bound_contrast_individual)\n\n  if augment_contrast_individual:\n    image_1 = tf.image.random_brightness(\n        image_1, max_delta_brightness_individual)\n    image_2 = tf.image.random_brightness(\n        image_2, max_delta_brightness_individual)\n\n  # crop values to ensure values in [0,1] (some augmentations can violate this)\n  image_1 = tf.clip_by_value(image_1, 0.0, 1.0)\n  image_2 = tf.clip_by_value(image_2, 0.0, 1.0)\n  return tf.stack([image_1, image_2])", "\n\n@gin.configurable\ndef geometric_augmentation(images,\n                           flow=None,\n                           mask=None,\n                           crop_height=640,\n                           crop_width=640,\n                           augment_flip_left_right=False,\n                           augment_flip_up_down=False,\n                           augment_scale=False,\n                           augment_relative_scale=False,\n                           augment_rotation=False,\n                           augment_relative_rotation=False,\n                           augment_crop_offset=False,\n                           min_bound_scale=0.9,\n                           max_bound_scale=1.5,\n                           min_bound_relative_scale=0.95,\n                           max_bound_relative_scale=1.05,\n                           max_rotation_deg=15,\n                           max_relative_rotation_deg=3,\n                           max_relative_crop_offset=5):\n  \"\"\"Apply geometric augmentations to an image pair and corresponding flow.\"\"\"\n  \n  # apply geometric augmentation\n  if augment_flip_left_right:\n    images, flow, mask = random_flip_left_right(images, flow, mask)\n\n  if augment_flip_up_down:\n    images, flow, mask = random_flip_up_down(images, flow, mask)\n\n  if augment_scale:\n    images, flow, mask = random_scale(\n        images,\n        flow,\n        mask,\n        min_scale=min_bound_scale,\n        max_scale=max_bound_scale)\n\n  if augment_relative_scale:\n    images, flow, mask = random_scale_second(\n        images, flow, mask,\n        min_scale=min_bound_relative_scale, max_scale=max_bound_relative_scale)\n\n  if augment_rotation:\n    images, flow, mask = random_rotation(\n        images, flow, mask,\n        max_rotation=max_rotation_deg, not_empty_crop=True)\n\n  if augment_relative_rotation:\n    images, flow, mask = random_rotation_second(\n        images, flow, mask,\n        max_rotation=max_relative_rotation_deg, not_empty_crop=True)\n\n  # always perform random cropping\n  if not augment_crop_offset:\n    max_relative_crop_offset = 0\n  images, flow, mask = random_crop(\n      images, flow, mask, crop_height, crop_width,\n      relative_offset=max_relative_crop_offset)\n\n  # return flow and mask if available\n  return images, flow, mask", "\n\ndef _center_crop(images, height, width):\n  \"\"\"Performs a center crop with the given heights and width.\"\"\"\n  # ensure height, width to be int\n  height = tf.cast(height, tf.int32)\n  width = tf.cast(width, tf.int32)\n  # get current size\n  images_shape = tf.shape(images)\n  current_height = images_shape[-3]\n  current_width = images_shape[-2]\n  # compute required offset\n  offset_height = tf.cast((current_height - height) / 2, tf.int32)\n  offset_width = tf.cast((current_width - width) / 2, tf.int32)\n  # perform the crop\n  images = tf.image.crop_to_bounding_box(\n      images, offset_height, offset_width, height, width)\n  return images", "\n\ndef _positions_center_origin(height, width):\n  \"\"\"Returns image coordinates where the origin at the image center.\"\"\"\n  h = tf.range(0.0, height, 1)\n  w = tf.range(0.0, width, 1)\n  center_h = tf.cast(height, tf.float32) / 2.0 - 0.5\n  center_w = tf.cast(width, tf.float32) / 2.0 - 0.5\n  return tf.stack(tf.meshgrid(h - center_h, w - center_w, indexing='ij'), -1)\n", "\n\ndef rotate(img, angle_radian, is_flow, mask=None):\n  \"\"\"Rotate an image or flow field.\"\"\"\n  def _rotate(img, mask=None):\n    if angle_radian == 0.0:\n      # early return if no resizing is required\n      if mask is not None:\n        return img, mask\n      else:\n        return img\n\n    if mask is not None:\n      # multiply with mask, to ensure non-valid locations are zero\n      img = tf.math.multiply(img, mask)\n      # rotate img\n      img_rotated = tfa_image.rotate(\n          img, angle_radian, interpolation='BILINEAR')\n      # rotate mask (will serve as normalization weights)\n      mask_rotated = tfa_image.rotate(\n          mask, angle_radian, interpolation='BILINEAR')\n      # normalize sparse flow field and mask\n      img_rotated = tf.math.multiply(\n          img_rotated, tf.math.reciprocal_no_nan(mask_rotated))\n      mask_rotated = tf.math.multiply(\n          mask_rotated, tf.math.reciprocal_no_nan(mask_rotated))\n    else:\n      img_rotated = tfa_image.rotate(\n          img, angle_radian, interpolation='BILINEAR')\n\n    if is_flow:\n      # If image is a flow image, scale flow values to be consistent with the\n      # rotation.\n      cos = tf.math.cos(angle_radian)\n      sin = tf.math.sin(angle_radian)\n      rotation_matrix = tf.reshape([cos, sin, -sin, cos], [2, 2])\n      img_rotated = tf.linalg.matmul(img_rotated, rotation_matrix)\n\n    if mask is not None:\n      return img_rotated, mask_rotated\n    return img_rotated\n\n  # Apply resizing at the right shape.\n  shape = img.shape.as_list()\n  if len(shape) == 3:\n    if mask is not None:\n      img_rotated, mask_rotated = _rotate(img[None], mask[None])\n      return img_rotated[0], mask_rotated[0]\n    else:\n      return _rotate(img[None])[0]\n  elif len(shape) == 4:\n    # Input at the right shape.\n    return _rotate(img, mask)\n  else:\n    raise ValueError('Cannot rotate an image of shape', shape)", "\n\ndef random_flip_left_right(images, flow=None, mask=None):\n  \"\"\"Performs a random left/right flip.\"\"\"\n  # 50/50 chance\n  perform_flip = tf.equal(tf.random.uniform([], maxval=2, dtype=tf.int32), 1)\n  # apply flip\n  images = tf.cond(pred=perform_flip,\n                   true_fn=lambda: tf.reverse(images, axis=[-2]),\n                   false_fn=lambda: images)\n  if flow is not None:\n    flow = tf.cond(pred=perform_flip,\n                   true_fn=lambda: tf.reverse(flow, axis=[-2]),\n                   false_fn=lambda: flow)\n    mask = tf.cond(pred=perform_flip,\n                   true_fn=lambda: tf.reverse(mask, axis=[-2]),\n                   false_fn=lambda: mask)\n    # correct sign of flow\n    sign_correction = tf.reshape([1.0, -1.0], [1, 1, 2])\n    flow = tf.cond(pred=perform_flip,\n                   true_fn=lambda: flow * sign_correction,\n                   false_fn=lambda: flow)\n  return images, flow, mask", "\n\ndef random_flip_up_down(images, flow=None, mask=None):\n  \"\"\"Performs a random up/down flip.\"\"\"\n  # 50/50 chance\n  perform_flip = tf.equal(tf.random.uniform([], maxval=2, dtype=tf.int32), 1)\n  # apply flip\n  images = tf.cond(pred=perform_flip,\n                   true_fn=lambda: tf.reverse(images, axis=[-3]),\n                   false_fn=lambda: images)\n  if flow is not None:\n    flow = tf.cond(pred=perform_flip,\n                   true_fn=lambda: tf.reverse(flow, axis=[-3]),\n                   false_fn=lambda: flow)\n    mask = tf.cond(pred=perform_flip,\n                   true_fn=lambda: tf.reverse(mask, axis=[-3]),\n                   false_fn=lambda: mask)\n    # correct sign of flow\n    sign_correction = tf.reshape([-1.0, 1.0], [1, 1, 2])\n    flow = tf.cond(pred=perform_flip,\n                   true_fn=lambda: flow * sign_correction,\n                   false_fn=lambda: flow)\n  return images, flow, mask", "\n\ndef random_scale(images, flow=None, mask=None, min_scale=1.0, max_scale=1.0):\n  \"\"\"Performs a random scaling in the given range.\"\"\"\n  # choose a random scale factor and compute new resolution\n  orig_height = tf.shape(images)[-3]\n  orig_width = tf.shape(images)[-2]\n  scale = tf.random.uniform([],\n                            minval=min_scale,\n                            maxval=max_scale,\n                            dtype=tf.float32)\n  new_height = tf.cast(\n      tf.math.ceil(tf.cast(orig_height, tf.float32) * scale), tf.int32)\n  new_width = tf.cast(\n      tf.math.ceil(tf.cast(orig_width, tf.float32) * scale), tf.int32)\n\n  # rescale the images (and flow)\n  images = uflow_utils.resize(images, new_height, new_width, is_flow=False)\n  if flow is not None:\n    flow, mask = uflow_utils.resize(\n        flow, new_height, new_width, is_flow=True, mask=mask)\n  return images, flow, mask", "\n\ndef random_scale_second(\n    images, flow=None, mask=None, min_scale=1.0, max_scale=1.0):\n  \"\"\"Performs a random scaling on the second image in the given range.\"\"\"\n  # choose a random scale factor and compute new resolution\n  orig_height = tf.shape(images)[-3]\n  orig_width = tf.shape(images)[-2]\n  scale = tf.random.uniform(\n      [], minval=min_scale, maxval=max_scale, dtype=tf.float32)\n  new_height = tf.cast(\n      tf.math.ceil(tf.cast(orig_height, tf.float32) * scale), tf.int32)\n  new_width = tf.cast(\n      tf.math.ceil(tf.cast(orig_width, tf.float32) * scale), tf.int32)\n\n  # rescale only the second image\n  image_1, image_2 = tf.unstack(images)\n  image_2 = uflow_utils.resize(image_2, new_height, new_width, is_flow=False)\n  # crop either first or second image to have matching dimensions\n  if scale < 1.0:\n    image_1 = _center_crop(image_1, new_height, new_width)\n  else:\n    image_2 = _center_crop(image_2, orig_height, orig_width)\n  images = tf.stack([image_1, image_2])\n\n  if flow is not None:\n    # get current locations (with the origin in the image center)\n    positions = _positions_center_origin(orig_height, orig_width)\n\n    # compute scale factor of the actual new image resolution\n    scale_flow_h = tf.cast(new_height, tf.float32) / tf.cast(\n        orig_height, tf.float32)\n    scale_flow_w = tf.cast(new_width, tf.float32) / tf.cast(\n        orig_width, tf.float32)\n    scale_flow = tf.stack([scale_flow_h, scale_flow_w])\n\n    # compute augmented flow (multiply by mask to zero invalid flow locations)\n    flow = ((positions + flow) * scale_flow - positions) * mask\n\n    if scale < 1.0:\n      # in case we downsample the image we crop the reference image to keep the\n      # same shape\n      flow = _center_crop(flow, new_height, new_width)\n      mask = _center_crop(mask, new_height, new_width)\n  return images, flow, mask", "\n\ndef random_crop(images, flow=None, mask=None, crop_height=None, crop_width=None,\n                relative_offset=0):\n  \"\"\"Performs a random crop with the given height and width.\"\"\"\n  # early return if crop_height or crop_width is not specified\n  if crop_height is None or crop_width is None:\n    return images, flow, mask\n\n  orig_height = tf.shape(images)[-3]\n  orig_width = tf.shape(images)[-2]\n\n  # check if crop size fits the image size\n  scale = 1.0\n  ratio = tf.cast(crop_height, tf.float32) / tf.cast(orig_height, tf.float32)\n  scale = tf.math.maximum(scale, ratio)\n  ratio = tf.cast(crop_width, tf.float32) / tf.cast(orig_width, tf.float32)\n  scale = tf.math.maximum(scale, ratio)\n  # compute minimum required hight\n  new_height = tf.cast(\n      tf.math.ceil(tf.cast(orig_height, tf.float32) * scale), tf.int32)\n  new_width = tf.cast(\n      tf.math.ceil(tf.cast(orig_width, tf.float32) * scale), tf.int32)\n  # perform resize (scales with 1 if not required)\n  images = uflow_utils.resize(images, new_height, new_width, is_flow=False)\n\n  # compute joint offset\n  max_offset_h = new_height - tf.cast(crop_height, dtype=tf.int32)\n  max_offset_w = new_width - tf.cast(crop_width, dtype=tf.int32)\n  joint_offset_h = tf.random.uniform([], maxval=max_offset_h+1, dtype=tf.int32)\n  joint_offset_w = tf.random.uniform([], maxval=max_offset_w+1, dtype=tf.int32)\n\n  # compute relative offset\n  min_relative_offset_h = tf.math.maximum(\n      joint_offset_h - relative_offset, 0)\n  max_relative_offset_h = tf.math.minimum(\n      joint_offset_h + relative_offset, max_offset_h)\n  min_relative_offset_w = tf.math.maximum(\n      joint_offset_w - relative_offset, 0)\n  max_relative_offset_w = tf.math.minimum(\n      joint_offset_w + relative_offset, max_offset_w)\n  relative_offset_h = tf.random.uniform(\n      [], minval=min_relative_offset_h, maxval=max_relative_offset_h+1,\n      dtype=tf.int32)\n  relative_offset_w = tf.random.uniform(\n      [], minval=min_relative_offset_w, maxval=max_relative_offset_w+1,\n      dtype=tf.int32)\n\n  # crop both images\n  image_1, image_2 = tf.unstack(images)\n  image_1 = tf.image.crop_to_bounding_box(\n      image_1, offset_height=joint_offset_h, offset_width=joint_offset_w,\n      target_height=crop_height, target_width=crop_width)\n  image_2 = tf.image.crop_to_bounding_box(\n      image_2, offset_height=relative_offset_h, offset_width=relative_offset_w,\n      target_height=crop_height, target_width=crop_width)\n  images = tf.stack([image_1, image_2])\n\n  if flow is not None:\n    # perform resize (scales with 1 if not required)\n    flow, mask = uflow_utils.resize(\n        flow, new_height, new_width, is_flow=True, mask=mask)\n\n    # crop flow and mask\n    flow = tf.image.crop_to_bounding_box(\n        flow,\n        offset_height=joint_offset_h,\n        offset_width=joint_offset_w,\n        target_height=crop_height,\n        target_width=crop_width)\n    mask = tf.image.crop_to_bounding_box(\n        mask,\n        offset_height=joint_offset_h,\n        offset_width=joint_offset_w,\n        target_height=crop_height,\n        target_width=crop_width)\n\n    # correct flow for relative shift (/crop)\n    flow_delta = tf.stack(\n        [tf.cast(relative_offset_h - joint_offset_h, tf.float32),\n         tf.cast(relative_offset_w - joint_offset_w, tf.float32)])\n    flow = (flow - flow_delta) * mask\n  return images, flow, mask", "\n\ndef random_rotation(\n    images, flow=None, mask=None, max_rotation=10, not_empty_crop=True):\n  \"\"\"Performs a random rotation with the specified maximum rotation.\"\"\"\n\n  angle_radian = tf.random.uniform(\n      [], minval=-max_rotation, maxval=max_rotation,\n      dtype=tf.float32) * pi / 180.0\n  images = rotate(images, angle_radian, is_flow=False, mask=None)\n\n  if not_empty_crop:\n    orig_height = tf.shape(images)[-3]\n    orig_width = tf.shape(images)[-2]\n    # introduce abbreviations for shorter notation\n    cos = tf.math.cos(angle_radian % pi)\n    sin = tf.math.sin(angle_radian % pi)\n    h = tf.cast(orig_height, tf.float32)\n    w = tf.cast(orig_width, tf.float32)\n\n    # compute required scale factor\n    scale = tf.cond(tf.math.less(angle_radian % pi, pi/2.0),\n                    lambda: tf.math.maximum((w/h)*sin+cos, (h/w)*sin+cos),\n                    lambda: tf.math.maximum((w/h)*sin-cos, (h/w)*sin-cos))\n    new_height = tf.math.floor(h / scale)\n    new_width = tf.math.floor(w / scale)\n\n    # crop image again to original size\n    offset_height = tf.cast((h - new_height) / 2, tf.int32)\n    offset_width = tf.cast((w - new_width) / 2, tf.int32)\n    images = tf.image.crop_to_bounding_box(\n        images,\n        offset_height=offset_height,\n        offset_width=offset_width,\n        target_height=tf.cast(new_height, tf.int32),\n        target_width=tf.cast(new_width, tf.int32))\n\n  if flow is not None:\n    flow, mask = rotate(flow, angle_radian, is_flow=True, mask=mask)\n\n    if not_empty_crop:\n      # crop flow and mask again to original size\n      flow = tf.image.crop_to_bounding_box(\n          flow,\n          offset_height=offset_height,\n          offset_width=offset_width,\n          target_height=tf.cast(new_height, tf.int32),\n          target_width=tf.cast(new_width, tf.int32))\n      mask = tf.image.crop_to_bounding_box(\n          mask,\n          offset_height=offset_height,\n          offset_width=offset_width,\n          target_height=tf.cast(new_height, tf.int32),\n          target_width=tf.cast(new_width, tf.int32))\n  return images, flow, mask", "\n\ndef random_rotation_second(\n    images, flow=None, mask=None, max_rotation=10, not_empty_crop=True):\n  \"\"\"Performs a random rotation on only the second image.\"\"\"\n\n  angle_radian = tf.random.uniform(\n      [], minval=-max_rotation, maxval=max_rotation, dtype=tf.float32)*pi/180.0\n\n  image_1, image_2 = tf.unstack(images)\n  image_2 = rotate(image_2, angle_radian, is_flow=False, mask=None)\n  images = tf.stack([image_1, image_2])\n\n  if not_empty_crop:\n    orig_height = tf.shape(images)[-3]\n    orig_width = tf.shape(images)[-2]\n    # introduce abbreviations for shorter notation\n    cos = tf.math.cos(angle_radian % pi)\n    sin = tf.math.sin(angle_radian % pi)\n    h = tf.cast(orig_height, tf.float32)\n    w = tf.cast(orig_width, tf.float32)\n\n    # compute required scale factor\n    scale = tf.cond(tf.math.less(angle_radian % pi, pi/2.0),\n                    lambda: tf.math.maximum((w/h)*sin+cos, (h/w)*sin+cos),\n                    lambda: tf.math.maximum((w/h)*sin-cos, (h/w)*sin-cos))\n    new_height = tf.math.floor(h / scale)\n    new_width = tf.math.floor(w / scale)\n\n    # crop image again to original size\n    offset_height = tf.cast((h-new_height)/2, tf.int32)\n    offset_width = tf.cast((w-new_width)/2, tf.int32)\n    images = tf.image.crop_to_bounding_box(\n        images,\n        offset_height=offset_height,\n        offset_width=offset_width,\n        target_height=tf.cast(new_height, tf.int32),\n        target_width=tf.cast(new_width, tf.int32))\n\n  if flow is not None:\n    # get current locations (with the origin in the image center)\n    positions = _positions_center_origin(orig_height, orig_width)\n\n    # compute augmented flow (multiply by mask to zero invalid flow locations)\n    cos = tf.math.cos(angle_radian)\n    sin = tf.math.sin(angle_radian)\n    rotation_matrix = tf.reshape([cos, sin, -sin, cos], [2, 2])\n    flow = (tf.linalg.matmul((positions+flow), rotation_matrix)-positions)*mask\n\n    if not_empty_crop:\n      # crop flow and mask again to original size\n      flow = tf.image.crop_to_bounding_box(\n          flow,\n          offset_height=offset_height,\n          offset_width=offset_width,\n          target_height=tf.cast(new_height, tf.int32),\n          target_width=tf.cast(new_width, tf.int32))\n      mask = tf.image.crop_to_bounding_box(\n          mask,\n          offset_height=offset_height,\n          offset_width=offset_width,\n          target_height=tf.cast(new_height, tf.int32),\n          target_width=tf.cast(new_width, tf.int32))\n  return images, flow, mask", "\n\ndef build_selfsup_transformations(num_flow_levels=3,\n                                  seq_len=2,\n                                  crop_height=0,\n                                  crop_width=0,\n                                  max_shift_height=0,\n                                  max_shift_width=0,\n                                  resize=True):\n  \"\"\"Apply augmentations to a list of student images.\"\"\"\n\n  def transform(images, i_or_ij, is_flow, crop_height, crop_width,\n                shift_heights, shift_widths, resize):\n    # Expect (i, j) for flows and masks and i for images.\n    if isinstance(i_or_ij, int):\n      i = i_or_ij\n      # Flow needs i and j.\n      assert not is_flow\n    else:\n      i, j = i_or_ij\n\n    if is_flow:\n      shifts = tf.stack([shift_heights, shift_widths], axis=-1)\n      flow_offset = shifts[i] - shifts[j]\n      images = images + tf.cast(flow_offset, tf.float32)\n\n    shift_height = shift_heights[i]\n    shift_width = shift_widths[i]\n    height = images.shape[-3]\n    width = images.shape[-2]\n\n    # Assert that the cropped bounding box does not go out of the image frame.\n    op1 = tf.compat.v1.assert_greater_equal(crop_height + shift_height, 0)\n    op2 = tf.compat.v1.assert_greater_equal(crop_width + shift_width, 0)\n    op3 = tf.compat.v1.assert_less_equal(height - crop_height + shift_height,\n                                         height)\n    op4 = tf.compat.v1.assert_less_equal(width - crop_width + shift_width,\n                                         width)\n    op5 = tf.compat.v1.assert_greater(\n        height,\n        2 * crop_height,\n        message='Image height is too small for cropping.')\n    op6 = tf.compat.v1.assert_greater(\n        width, 2 * crop_width, message='Image width is too small for cropping.')\n    with tf.control_dependencies([op1, op2, op3, op4, op5, op6]):\n      images = images[:, crop_height + shift_height:height - crop_height +\n                      shift_height, crop_width + shift_width:width -\n                      crop_width + shift_width, :]\n    if resize:\n      images = uflow_utils.resize(images, height, width, is_flow=is_flow)\n      images.set_shape((images.shape[0], height, width, images.shape[3]))\n    else:\n      images.set_shape((images.shape[0], height - 2 * crop_height,\n                        width - 2 * crop_width, images.shape[3]))\n    return images\n\n  max_divisor = 2**(num_flow_levels - 1)\n  assert crop_height % max_divisor == 0\n  assert crop_width % max_divisor == 0\n  assert max_shift_height <= crop_height\n  assert max_shift_width <= crop_width\n  # Compute random shifts for different images in a sequence.\n  if max_shift_height > 0 or max_shift_width > 0:\n    max_rand = max_shift_height // max_divisor\n    shift_height_at_highest_level = tf.random.uniform([seq_len],\n                                                      minval=-max_rand,\n                                                      maxval=max_rand + 1,\n                                                      dtype=tf.int32)\n    shift_heights = shift_height_at_highest_level * max_divisor\n\n    max_rand = max_shift_height // max_divisor\n    shift_width_at_highest_level = tf.random.uniform([seq_len],\n                                                     minval=-max_rand,\n                                                     maxval=max_rand + 1,\n                                                     dtype=tf.int32)\n    shift_widths = shift_width_at_highest_level * max_divisor\n  transform_fns = []\n  for level in range(num_flow_levels):\n\n    if max_shift_height == 0 and max_shift_width == 0:\n      shift_heights = [0, 0]\n      shift_widths = [0, 0]\n    else:\n      shift_heights = shift_heights // (2**level)\n      shift_widths = shift_widths // (2**level)\n\n    fn = partial(\n        transform,\n        crop_height=crop_height // (2**level),\n        crop_width=crop_width // (2**level),\n        shift_heights=shift_heights,\n        shift_widths=shift_widths,\n        resize=resize)\n    transform_fns.append(fn)\n  assert len(transform_fns) == num_flow_levels\n  return transform_fns", ""]}
{"filename": "src/uflow_flags.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\n\"\"\"Flags used by uflow training and evaluation.\"\"\"", "\n\"\"\"Flags used by uflow training and evaluation.\"\"\"\n\nfrom absl import flags\n\nFLAGS = flags.FLAGS\n\n# General flags.\nflags.DEFINE_bool(\n    'no_tf_function', False, 'If True, run without'", "flags.DEFINE_bool(\n    'no_tf_function', False, 'If True, run without'\n    ' tf functions. This incurs a performance hit, but can'\n    ' make debugging easier.')\nflags.DEFINE_string('train_on', '',\n                    '\"format0:path0;format1:path1\", e.g. \"kitti:/usr/...\"')\nflags.DEFINE_string('valid_on', '',\n                    '\"format0:path0;format1:path1\", e.g. \"kitti:/usr/...\"')\nflags.DEFINE_string('predict_on', '',\n                    '\"format0:path0;format1:path1\", e.g. \"kitti:/usr/...\"')", "flags.DEFINE_string('predict_on', '',\n                    '\"format0:path0;format1:path1\", e.g. \"kitti:/usr/...\"')\nflags.DEFINE_string('generated_dir', '', 'Path to directory where everything is saved.')\nflags.DEFINE_string('plot_dir', '', 'Path to directory where plots are saved.')\nflags.DEFINE_string('checkpoint_dir', '',\n                    'Path to directory for saving and restoring checkpoints.')\nflags.DEFINE_string('init_checkpoint_dir', '',\n                    'Path to directory for initializing from a checkpoint.')\nflags.DEFINE_bool(\n    'plot_debug_info', False,", "flags.DEFINE_bool(\n    'plot_debug_info', False,\n    'Flag to indicate whether to plot debug info during training.')\nflags.DEFINE_bool(\n    'use_tensorboard', False, 'Toggles logging to tensorboard.')\nflags.DEFINE_string(\n    'tensorboard_logdir', '', 'Where to log tensorboard summaries.')\nflags.DEFINE_bool(\n    'frozen_teacher', False, 'Whether or not to freeze the '\n    'teacher model during distillation.')", "    'frozen_teacher', False, 'Whether or not to freeze the '\n    'teacher model during distillation.')\nflags.DEFINE_bool(\n    'reset_global_step', True, 'Reset global step to 0 after '\n    'loading from init_checkpoint')\nflags.DEFINE_bool(\n    'reset_optimizer', True, 'Reset optimizer internals after '\n    'loading from init_checkpoint')\n\n# Training flags.", "\n# Training flags.\nflags.DEFINE_bool('from_scratch', False,\n                  'Train from scratch. Do not restore the last checkpoint.')\nflags.DEFINE_bool('no_checkpointing', False,\n                  'Do not save model checkpoints during training.')\nflags.DEFINE_integer('epoch_length', 1000,\n                     'Number of gradient steps per epoch.')\nflags.DEFINE_integer('num_train_steps', int(5e4),\n                     'Number of gradient steps to train for.')", "flags.DEFINE_integer('num_train_steps', int(5e4),\n                     'Number of gradient steps to train for.')\nflags.DEFINE_integer('selfsup_after_num_steps', int(5e5),\n                     'Number of gradient steps before self-supervision.')\nflags.DEFINE_integer('selfsup_ramp_up_steps', int(1e5),\n                     'Number of gradient steps for ramping up self-sup.')\nflags.DEFINE_integer(\n    'selfsup_step_cycle', int(1e10),\n    'Number steps until the step counter for self-supervsion is reset.')\nflags.DEFINE_integer('shuffle_buffer_size', 1024,", "    'Number steps until the step counter for self-supervsion is reset.')\nflags.DEFINE_integer('shuffle_buffer_size', 1024,\n                     'Shuffle buffer size for training.')\nflags.DEFINE_integer('height', None, 'Image height for training and evaluation.')\nflags.DEFINE_integer('width', None, 'Image width for training and evaluation.')\nflags.DEFINE_bool('crop_instead_of_resize', False, 'Crops images for training '\n                  'instead of resizing the images.')\nflags.DEFINE_integer('seq_len', 2, 'Sequence length for training flow.')\nflags.DEFINE_integer('batch_size', 4, 'Batch size for training flow on '\n                     'gpu.')", "flags.DEFINE_integer('batch_size', 4, 'Batch size for training flow on '\n                     'gpu.')\nflags.DEFINE_string('optimizer', 'adam', 'One of \"adam\", \"sgd\"')\nflags.DEFINE_float('gpu_learning_rate', 10**-4, 'Learning rate for training '\n                   'UFlow on GPU.')\nflags.DEFINE_integer('lr_decay_after_num_steps', 10000, '')\nflags.DEFINE_integer('lr_decay_steps', 50000, '')\nflags.DEFINE_string('lr_decay_type', 'linear',\n                    'One of [\"none\", \"exponential\", \"linear\", \"gaussian\"]')\nflags.DEFINE_bool(", "                    'One of [\"none\", \"exponential\", \"linear\", \"gaussian\"]')\nflags.DEFINE_bool(\n    'stop_gradient_mask', True, 'Whether or not to stop the '\n    'gradient propagation through the occlusion mask.')\nflags.DEFINE_integer('num_occlusion_iterations', 1,\n                     'If occlusion estimation is \"iterative\"')\nflags.DEFINE_bool('only_forward', False, '')\n# Data augmentation (-> now gin configurable)\nflags.DEFINE_string('teacher_image_version', 'original',\n                    'one of original, augmented')", "flags.DEFINE_string('teacher_image_version', 'original',\n                    'one of original, augmented')\nflags.DEFINE_float(\n    'channel_multiplier', 1.,\n    'Globally multiply the number of model convolution channels'\n    'by this factor.')\nflags.DEFINE_integer('num_levels', 5, 'The number of feature pyramid levels to '\n                     'use.')\nflags.DEFINE_bool('use_cost_volume', True, 'Whether or not to compute the '\n                  'cost volume.')", "flags.DEFINE_bool('use_cost_volume', True, 'Whether or not to compute the '\n                  'cost volume.')\nflags.DEFINE_bool(\n    'use_feature_warp', True, 'Whether or not to warp the '\n    'model features when computing flow.')\nflags.DEFINE_bool(\n    'accumulate_flow', True, 'Whether or not to predict a flow '\n    'adjustment on each feature pyramid level.')\nflags.DEFINE_integer('level1_num_layers', 3, '')\nflags.DEFINE_integer('level1_num_filters', 32, '')", "flags.DEFINE_integer('level1_num_layers', 3, '')\nflags.DEFINE_integer('level1_num_filters', 32, '')\nflags.DEFINE_integer('level1_num_1x1', 0, '')\nflags.DEFINE_float('dropout_rate', 0.1, 'Amount of level dropout.')\nflags.DEFINE_bool('normalize_before_cost_volume', True, '')\nflags.DEFINE_bool('original_layer_sizes', False, '')\nflags.DEFINE_bool('shared_flow_decoder', False, '')\nflags.DEFINE_bool('resize_selfsup', True, '')\nflags.DEFINE_integer(\n    'selfsup_crop_height', 64,", "flags.DEFINE_integer(\n    'selfsup_crop_height', 64,\n    'Number of pixels removed from the image at top and bottom'\n    'for self-supervision.')\nflags.DEFINE_integer(\n    'selfsup_crop_width', 64,\n    'Number of pixels removed from the image left and right'\n    'for self-supervision.')\nflags.DEFINE_integer(\n    'selfsup_max_shift', 0,", "flags.DEFINE_integer(\n    'selfsup_max_shift', 0,\n    'Number of pixels removed from the image at top and bottom, left and right'\n    'for self-supervision.')\nflags.DEFINE_float(\n    'fb_sigma_teacher', 0.003,\n    'Forward-backward consistency scaling constant used for self-supervision.')\nflags.DEFINE_float(\n    'fb_sigma_student', 0.03,\n    'Forward-backward consistency scaling constant used for self-supervision.')", "    'fb_sigma_student', 0.03,\n    'Forward-backward consistency scaling constant used for self-supervision.')\nflags.DEFINE_string('selfsup_mask', 'gaussian',\n                    'One of [gaussian, ddflow, advection]')\nflags.DEFINE_float('weight_photo', 0.0, 'Weight for photometric loss.')\nflags.DEFINE_float('weight_ssim', 0.0, 'Weight for SSIM loss.')\nflags.DEFINE_float('weight_census', 1.0, 'Weight for census loss.')\nflags.DEFINE_float('weight_smooth1', 0.0, 'Weight for smoothness loss.')\nflags.DEFINE_float('weight_smooth2', 2.0, 'Weight for smoothness loss.')\nflags.DEFINE_float('weight_contour_smooth1', 0.0, 'Weight for contour + smoothness loss.')", "flags.DEFINE_float('weight_smooth2', 2.0, 'Weight for smoothness loss.')\nflags.DEFINE_float('weight_contour_smooth1', 0.0, 'Weight for contour + smoothness loss.')\nflags.DEFINE_float('weight_contour_smooth2', 0.0, 'Weight for contour + smoothness loss.')\nflags.DEFINE_float('weight_contour', 1.0, 'Weight for contour reconstruction loss.')\nflags.DEFINE_float('weight_flow_magnitude', 0.1, 'Weight for flow magnitude regularization loss.')\nflags.DEFINE_float('smoothness_edge_constant', 150.,\n                   'Edge constant for smoothness loss.')\nflags.DEFINE_string('smoothness_edge_weighting', 'exponential',\n                    'One of: gaussian, exponential')\nflags.DEFINE_integer('smoothness_at_level', 2, '')", "                    'One of: gaussian, exponential')\nflags.DEFINE_integer('smoothness_at_level', 2, '')\nflags.DEFINE_integer('num_plot', 40, '')\n\nflags.DEFINE_float('weight_selfsup', 0.6, 'Weight for self-supervision loss.')\nflags.DEFINE_float('weight_transl_consist', 0.0,\n                   'Weight for loss enforcing uniform source usage.')\n\n# Occlusion estimation parameters\nflags.DEFINE_string('occlusion_estimation', 'wang',", "# Occlusion estimation parameters\nflags.DEFINE_string('occlusion_estimation', 'wang',\n                    'One of: none, brox, wang, uflow')\nflags.DEFINE_integer('occ_after_num_steps_brox', 0, '')\nflags.DEFINE_integer('occ_after_num_steps_wang', 0, '')\nflags.DEFINE_integer('occ_after_num_steps_fb_abs', 0, '')\nflags.DEFINE_integer('occ_after_num_steps_forward_collision', 0, '')\nflags.DEFINE_integer('occ_after_num_steps_backward_zero', 0, '')\nflags.DEFINE_float('occ_weights_fb_abs', 1000.0, '')\nflags.DEFINE_float('occ_weights_forward_collision', 1000.0, '')", "flags.DEFINE_float('occ_weights_fb_abs', 1000.0, '')\nflags.DEFINE_float('occ_weights_forward_collision', 1000.0, '')\nflags.DEFINE_float('occ_weights_backward_zero', 1000.0, '')\nflags.DEFINE_float('occ_thresholds_fb_abs', 1.5, '')\nflags.DEFINE_float('occ_thresholds_forward_collision', 0.4, '')\nflags.DEFINE_float('occ_thresholds_backward_zero', 0.25, '')\nflags.DEFINE_float('occ_clip_max_fb_abs', 10.0, '')\nflags.DEFINE_float('occ_clip_max_forward_collision', 5.0, '')\n\nflags.DEFINE_string(", "\nflags.DEFINE_string(\n    'distance_census', 'ddflow', 'Which type of distance '\n    'metric to use when computing loss.')\nflags.DEFINE_string(\n    'distance_photo', 'robust_l1', 'Which type of distance '\n    'metric to use when computing loss.')\nflags.DEFINE_string(\n    'distance_contour', 'robust_l1', 'Which type of distance '\n    'metric to use when computing loss.')", "    'distance_contour', 'robust_l1', 'Which type of distance '\n    'metric to use when computing loss.')\nflags.DEFINE_bool('use_supervision', False, 'Whether or not to train with '\n                  'a supervised loss.')\nflags.DEFINE_bool('resize_gt_flow_supervision', True, 'Whether or not to '\n                  'resize ground truth flow for the supervised loss.')\nflags.DEFINE_bool('use_gt_occlusions', False, 'Whether or not to train with '\n                  'a ground truth occlusion')\n\nflags.DEFINE_bool('use_segmentations', False, 'Whether or not to train with '", "\nflags.DEFINE_bool('use_segmentations', False, 'Whether or not to train with '\n                  'ground truth segmentaions corresponding to images')\nflags.DEFINE_bool('use_tracking_points', False, 'Whether or not to train with '\n                  'ground truth tracking points')\nflags.DEFINE_bool('use_seg_points', False, 'Whether or not to train with '\n                  'ground truth segmentation points')\n# Gin params are used to specify which augmentations to perform.\nflags.DEFINE_multi_string(\n    'config_file', None,", "flags.DEFINE_multi_string(\n    'config_file', None,\n    'Path to a Gin config file. Can be specified multiple times. '\n    'Order matters, later config files override former ones.')\n\nflags.DEFINE_multi_string(\n    'gin_bindings', None,\n    'Newline separated list of Gin parameter bindings. Can be specified '\n    'multiple times. Overrides config from --config_file.')\n", "    'multiple times. Overrides config from --config_file.')\n"]}
{"filename": "src/uflow_data.py", "chunked_list": ["# coding=utf-8\n# Copyright Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nLibrary for loading train and eval data.\n", "Library for loading train and eval data.\n\nThis libary contains two functions, make_train_iterator for generating a\ntraining data iterator from multiple sources of different formats and\nmake_eval_function for creating an evaluation function that evaluates on\ndata from multiple sources of different formats.\n\"\"\"\n\n# pylint:disable=g-importing-member\nfrom functools import partial", "# pylint:disable=g-importing-member\nfrom functools import partial\n\nimport tensorflow as tf\n\nfrom src import uflow_augmentation\nfrom src.data import generic_flow_dataset as flow_dataset\n\n# pylint:disable=g-long-lambda\n", "# pylint:disable=g-long-lambda\n\n\ndef make_train_iterator(\n    train_on,\n    height,\n    width,\n    shuffle_buffer_size,\n    batch_size,\n    seq_len,\n    crop_instead_of_resize=False,\n    apply_augmentation=True,\n    include_ground_truth=False,\n    resize_gt_flow=True,\n    include_occlusions=False,\n    include_segmentations=False,\n    include_seg_points=False,\n    include_tracking_points=False,\n    seed=41,\n    mode='train',\n):\n  \"\"\"Build joint training iterator for all data in train_on.\n\n  Args:\n    train_on: string of the format 'format0:path0;format1:path1', e.g.\n       'kitti:/usr/local/home/...'.\n    height: int, height to which the images will be resized or cropped.\n    width: int, width to which the images will be resized or cropped.\n    shuffle_buffer_size: int, size that will be used for the shuffle buffer.\n    batch_size: int, batch size for the iterator.\n    seq_len: int, number of frames per sequences (at the moment this should\n      always be 2)\n    crop_instead_of_resize: bool, indicates if cropping should be used instead\n      of resizing\n    apply_augmentation: bool, indicates if geometric and photometric data\n      augmentation shall be activated (paramaters are gin configurable)\n    include_ground_truth: bool, if True, return ground truth optical flow with\n      the training images. This only exists for some datasets.\n    resize_gt_flow: bool, indicates if ground truth flow should be resized (only\n      important if resizing and supervised training is used)\n    include_occlusions: bool, indicates if ground truth occlusions should be\n      loaded (currently not supported in combination with augmentation)\n    seed: A seed for a random number generator, controls shuffling of data.\n    mode: str, will be passed on to the data iterator class. Can be used to\n      specify different settings within the data iterator.\n\n  Returns:\n    A tf.data.Iterator that produces batches of images of shape [batch\n    size, sequence length=3, height, width, channels=3]\n  \"\"\"\n  train_datasets = []\n  # Split strings according to pattern \"format0:path0;format1:path1\".\n  for format_and_path in train_on.split(';'):\n\n    data_format, path = format_and_path.split(':')\n\n    if include_occlusions:\n      mode += '-include-occlusions'\n\n    if include_segmentations:\n      mode += '-include-segmentations'\n\n    if include_seg_points:\n      mode += '-include-segmentation_points'\n\n    if include_tracking_points:\n      mode += '-include-tracking_points'\n\n    if include_ground_truth:\n      mode += '-supervised'\n\n    if include_occlusions:\n      raise ValueError('The parameter include_occlusions is not supported')\n\n    if include_ground_truth and ('chairs' not in data_format):\n      raise NotImplementedError('The parameter include_ground_truth is only'\n                                'supported for flying_chairs and'\n                                'wod data at the moment.')\n\n    # Add a dataset based on format and path.\n    if 'chairs' in data_format:\n      dataset = flow_dataset.make_dataset(\n          path,\n          mode=mode,\n          shuffle_buffer_size=shuffle_buffer_size,\n          height=None if crop_instead_of_resize else height,\n          width=None if crop_instead_of_resize else width,\n          resize_gt_flow=resize_gt_flow,\n          gt_flow_shape=[384, 512, 2],\n          seed=seed,\n      )\n    else:  # custom dataset\n      print('Unknown data format \"{}\"'.format(data_format))\n      dataset = flow_dataset.make_dataset(\n          path,\n          mode=mode,\n          shuffle_buffer_size=shuffle_buffer_size,\n          height=None if crop_instead_of_resize else height,\n          width=None if crop_instead_of_resize else width,\n          resize_gt_flow=resize_gt_flow,\n          seed=seed,\n      )\n\n    train_datasets.append(dataset)  # <PrefetchDataset shapes: ((None, None, None, 3), (None, None, None, 1)), types: (tf.float32, tf.uint8)>\n\n  # prepare augmentation function\n  # in case no crop is desired set it to the size images have been resized to\n  # This will fail if none or both are specified.\n  augmentation_fn = partial(\n      uflow_augmentation.apply_augmentation,\n      crop_height=height,\n      crop_width=width)\n\n  # returns a function to apply ensure_shape on all the available data\n  def _ensure_shapes():\n    # shape of the data\n    imgs_shape = (batch_size, seq_len, height, width, 3)\n    if resize_gt_flow:\n      flow_shape = (batch_size, height, width, 2)\n      valid_shape = (batch_size, height, width, 1)\n    else:\n      flow_shape = (batch_size, None, None, 2)\n      valid_shape = (batch_size, None, None, 1)\n    occ_shape = (batch_size, height, width, 1)\n    seg_shape = (batch_size, seq_len, height, width, 1)\n    seg_points_shape = (batch_size, seq_len, None, 3)\n    tracking_points_shape = (batch_size, seq_len, None, 1)\n\n    # different cases of data combinations\n    if include_ground_truth and apply_augmentation:\n      return lambda imgs, imgs_na, flow, valid: (tf.ensure_shape(\n          imgs, imgs_shape), {\n              'images_without_photo_aug': tf.ensure_shape(imgs_na, imgs_shape),\n              'flow_uv': tf.ensure_shape(flow, flow_shape),\n              'flow_valid': tf.ensure_shape(valid, valid_shape)\n          })\n    elif include_segmentations and apply_augmentation:\n      return lambda imgs, imgs_na, seg: (tf.ensure_shape(\n          imgs, imgs_shape), {\n              'images_without_photo_aug': tf.ensure_shape(imgs_na, imgs_shape),\n              'segmentations': tf.ensure_shape(seg, seg_shape)\n          })\n    elif include_ground_truth and include_occlusions:\n      return lambda imgs, flow, valid, occ: (tf.ensure_shape(\n          imgs, imgs_shape), {\n              'flow_uv': tf.ensure_shape(flow, flow_shape),\n              'flow_valid': tf.ensure_shape(valid, valid_shape),\n              'occlusions': tf.ensure_shape(occ, occ_shape)\n          })\n    elif include_ground_truth:\n      return lambda imgs, flow, valid: (tf.ensure_shape(imgs, imgs_shape), {\n          'flow_uv': tf.ensure_shape(flow, flow_shape),\n          'flow_valid': tf.ensure_shape(valid, valid_shape)\n      })\n    elif include_occlusions:\n      return lambda imgs, occ: (tf.ensure_shape(imgs, imgs_shape), {\n          'occlusions': tf.ensure_shape(occ, occ_shape)\n      })\n    elif include_seg_points and include_tracking_points:\n      # def hi(imgs, seg_points, tracking_points):\n      #   import pdb;pdb.set_trace()\n      #   return tf.ensure_shape(imgs, imgs_shape), { 'segmentation_points': tf.ensure_shape(seg_points, seg_points_shape), 'tracking_points': tf.ensure_shape(tracking_points, tracking_points_shape) }\n      #\n      # return hi\n      return lambda imgs, seg_points, tracking_points: (tf.ensure_shape(imgs, imgs_shape), {\n          'segmentation_points': tf.ensure_shape(seg_points, seg_points_shape),\n          'tracking_points': tf.ensure_shape(tracking_points, tracking_points_shape)\n      })\n    elif include_segmentations and include_tracking_points:\n      return lambda imgs, seg, tracking_points: (tf.ensure_shape(imgs, imgs_shape), {\n          'segmentations': tf.ensure_shape(seg, seg_shape),\n          'tracking_points': tf.ensure_shape(tracking_points, tracking_points_shape)\n      })\n    elif include_tracking_points:\n      return lambda imgs, tracking_points: (tf.ensure_shape(imgs, imgs_shape), {\n          'tracking_points': tf.ensure_shape(tracking_points, tracking_points_shape)\n      })\n    elif include_segmentations:\n      return lambda imgs, seg: (tf.ensure_shape(imgs, imgs_shape), {\n          'segmentations': tf.ensure_shape(seg, seg_shape)\n      })\n    elif apply_augmentation:\n      return lambda imgs, imgs_na: (tf.ensure_shape(imgs, imgs_shape), {\n          'images_without_photo_aug': tf.ensure_shape(imgs_na, imgs_shape)\n      })\n    else:\n      return lambda imgs: (tf.ensure_shape(imgs, imgs_shape), {})\n\n  train_ds = train_datasets[0]\n  # Perform data augmentation\n  # This cannot handle occlusions at the moment.\n  if apply_augmentation:\n    train_ds = train_ds.map(augmentation_fn)\n\n  train_ds = train_ds.batch(batch_size)\n  train_ds = train_ds.prefetch(4)\n  train_ds = train_ds.map(_ensure_shapes())\n  # train_it = tf.compat.v1.data.make_one_shot_iterator(train_ds)\n  train_it = train_ds\n\n  return train_it", "\n\ndef make_predict_function(predict_on, height, width, progress_bar, plot_dir,\n                       num_plots, include_segmentations, include_seg_points, include_tracking_points, evaluate_bool):\n  \"\"\"Build predict function for uflow.\n\n  Args:\n    predict_on: string of the format 'format0:path0;format1:path1',\n    height: int, the height to which the images should be resized for inference.\n    width: int, the width to which the images should be resized for inference.\n    progress_bar: boolean, flag to indicate whether the function should print a\n      progress_bar during evaluaton.\n    plot_dir: string, optional path to a directory in which plots are saved (if\n      num_plots > 0).\n    num_plots: int, maximum number of qualitative results to plot for the\n      evaluation.\n  Returns:\n    A pair consisting of an evaluation function and a list of strings\n      that holds the keys of the evaluation result.\n  \"\"\"\n\n  mode = 'test'\n  if include_segmentations:\n      mode += '-segmentations'\n\n  if include_seg_points:\n      mode += '-segmentation_points'\n\n  if include_tracking_points:\n      mode += '-tracking_points'\n\n  predict_functions_and_datasets = []\n  # Split strings according to pattern \"format0:path0;format1:path1\".\n\n  for format_and_path in predict_on.split(';'):\n    data_format, path = format_and_path.split(':')\n\n    # Add a dataset based on format and path.\n    if 'chairs' in data_format or 'custom' in data_format:\n      dataset = flow_dataset.make_dataset(path, mode=mode, height=height, width=width)\n\n      predict_fn = partial(\n          flow_dataset.predict,\n          evaluate_bool=evaluate_bool)\n\n    else:\n      print('Unknown data format \"{}\"'.format(data_format))\n      continue\n\n    dataset = dataset.prefetch(4)\n    predict_functions_and_datasets.append((predict_fn, dataset))\n\n    # Make an eval function that aggregates all evaluations.\n    def predict_function(uflow):\n        for predict_fn, ds in predict_functions_and_datasets:\n            results = predict_fn(\n                uflow.infer, ds, height,\n                width, progress_bar, plot_dir, num_plots, include_segmentations, include_seg_points, include_tracking_points)\n\n            return results\n\n  return predict_function"]}
{"filename": "src/uflow_resampler.py", "chunked_list": ["# coding=utf-8\n# Copyright 2021 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Functions for resampling images.\"\"\"\n\nimport tensorflow as tf", "\nimport tensorflow as tf\n\n\ndef safe_gather_nd(params, indices):\n  \"\"\"Gather slices from params into a Tensor with shape specified by indices.\n\n  Similar functionality to tf.gather_nd with difference: when index is out of\n  bound, always return 0.\n\n  Args:\n    params: A Tensor. The tensor from which to gather values.\n    indices: A Tensor. Must be one of the following types: int32, int64. Index\n      tensor.\n\n  Returns:\n    A Tensor. Has the same type as params. Values from params gathered from\n    specified indices (if they exist) otherwise zeros, with shape\n    indices.shape[:-1] + params.shape[indices.shape[-1]:].\n  \"\"\"\n  params_shape = tf.shape(params)\n  indices_shape = tf.shape(indices)\n  slice_dimensions = indices_shape[-1]\n\n  max_index = params_shape[:slice_dimensions] - 1\n  min_index = tf.zeros_like(max_index, dtype=tf.int32)\n\n  clipped_indices = tf.clip_by_value(indices, min_index, max_index)\n\n  # Check whether each component of each index is in range [min, max], and\n  # allow an index only if all components are in range:\n  mask = tf.reduce_all(\n      tf.logical_and(indices >= min_index, indices <= max_index), -1)\n  mask = tf.expand_dims(mask, -1)\n\n  return (tf.cast(mask, dtype=params.dtype) *\n          tf.gather_nd(params, clipped_indices))", "\n\ndef resampler(data, warp, name='resampler'):\n  \"\"\"Resamples input data at user defined coordinates.\n\n  Args:\n    data: Tensor of shape `[batch_size, data_height, data_width,\n      data_num_channels]` containing 2D data that will be resampled.\n    warp: Tensor shape `[batch_size, dim_0, ... , dim_n, 2]` containing the\n      coordinates at which resampling will be performed.\n    name: Optional name of the op.\n\n  Returns:\n    Tensor of resampled values from `data`. The output tensor shape is\n    `[batch_size, dim_0, ... , dim_n, data_num_channels]`.\n  \"\"\"\n  data = tf.convert_to_tensor(data)\n  warp = tf.convert_to_tensor(warp)\n  with tf.name_scope(name + '/unstack_warp'):\n    warp_x, warp_y = tf.unstack(warp, axis=-1)\n  return resampler_with_unstacked_warp(data, warp_x, warp_y, name=name)", "\n\ndef resampler_with_unstacked_warp(data,\n                                  warp_x,\n                                  warp_y,\n                                  safe=True,\n                                  name='resampler'):\n  \"\"\"Resamples input data at user defined coordinates.\n\n  The resampler functions in the same way as `resampler` above, with the\n  following differences:\n  1. The warp coordinates for x and y are given as separate tensors.\n  2. If warp_x and warp_y are known to be within their allowed bounds, (that is,\n     0 <= warp_x <= width_of_data - 1, 0 <= warp_y <= height_of_data - 1) we\n     can disable the `safe` flag.\n\n  Args:\n    data: Tensor of shape `[batch_size, data_height, data_width,\n      data_num_channels]` containing 2D data that will be resampled.\n    warp_x: Tensor of shape `[batch_size, dim_0, ... , dim_n]` containing the x\n      coordinates at which resampling will be performed.\n    warp_y: Tensor of the same shape as warp_x containing the y coordinates at\n      which resampling will be performed.\n    safe: A boolean, if True, warp_x and warp_y will be clamped to their bounds.\n      Disable only if you know they are within bounds, otherwise a runtime\n      exception will be thrown.\n    name: Optional name of the op.\n\n  Returns:\n     Tensor of resampled values from `data`. The output tensor shape is\n    `[batch_size, dim_0, ... , dim_n, data_num_channels]`.\n\n  Raises:\n    ValueError: If warp_x, warp_y and data have incompatible shapes.\n  \"\"\"\n\n  with tf.name_scope(name):\n    warp_x = tf.convert_to_tensor(warp_x)\n    warp_y = tf.convert_to_tensor(warp_y)\n    data = tf.convert_to_tensor(data)\n    if not warp_x.shape.is_compatible_with(warp_y.shape):\n      raise ValueError(\n          'warp_x and warp_y are of incompatible shapes: %s vs %s ' %\n          (str(warp_x.shape), str(warp_y.shape)))\n    warp_shape = tf.shape(warp_x)\n    if warp_x.shape[0] != data.shape[0]:\n      raise ValueError(\n          '\\'warp_x\\' and \\'data\\' must have compatible first '\n          'dimension (batch size), but their shapes are %s and %s ' %\n          (str(warp_x.shape[0]), str(data.shape[0])))\n    # Compute the four points closest to warp with integer value.\n    warp_floor_x = tf.math.floor(warp_x)\n    warp_floor_y = tf.math.floor(warp_y)\n    # Compute the weight for each point.\n    right_warp_weight = warp_x - warp_floor_x\n    down_warp_weight = warp_y - warp_floor_y\n\n    warp_floor_x = tf.cast(warp_floor_x, tf.int32)\n    warp_floor_y = tf.cast(warp_floor_y, tf.int32)\n    warp_ceil_x = tf.cast(tf.math.ceil(warp_x), tf.int32)\n    warp_ceil_y = tf.cast(tf.math.ceil(warp_y), tf.int32)\n\n    left_warp_weight = tf.subtract(\n        tf.convert_to_tensor(1.0, right_warp_weight.dtype), right_warp_weight)\n    up_warp_weight = tf.subtract(\n        tf.convert_to_tensor(1.0, down_warp_weight.dtype), down_warp_weight)\n\n    # Extend warps from [batch_size, dim_0, ... , dim_n, 2] to\n    # [batch_size, dim_0, ... , dim_n, 3] with the first element in last\n    # dimension being the batch index.\n\n    # A shape like warp_shape but with all sizes except the first set to 1:\n    warp_batch_shape = tf.concat(\n        [warp_shape[0:1], tf.ones_like(warp_shape[1:])], 0)\n\n    warp_batch = tf.reshape(\n        tf.range(warp_shape[0], dtype=tf.int32), warp_batch_shape)\n\n    # Broadcast to match shape:\n    warp_batch += tf.zeros_like(warp_y, dtype=tf.int32)\n    left_warp_weight = tf.expand_dims(left_warp_weight, axis=-1)\n    down_warp_weight = tf.expand_dims(down_warp_weight, axis=-1)\n    up_warp_weight = tf.expand_dims(up_warp_weight, axis=-1)\n    right_warp_weight = tf.expand_dims(right_warp_weight, axis=-1)\n\n    up_left_warp = tf.stack([warp_batch, warp_floor_y, warp_floor_x], axis=-1)\n    up_right_warp = tf.stack([warp_batch, warp_floor_y, warp_ceil_x], axis=-1)\n    down_left_warp = tf.stack([warp_batch, warp_ceil_y, warp_floor_x], axis=-1)\n    down_right_warp = tf.stack([warp_batch, warp_ceil_y, warp_ceil_x], axis=-1)\n\n    def gather_nd(params, indices):\n      return (safe_gather_nd if safe else tf.gather_nd)(params, indices)\n\n    # gather data then take weighted average to get resample result.\n    result = (\n        (gather_nd(data, up_left_warp) * left_warp_weight +\n         gather_nd(data, up_right_warp) * right_warp_weight) * up_warp_weight +\n        (gather_nd(data, down_left_warp) * left_warp_weight +\n         gather_nd(data, down_right_warp) * right_warp_weight) *\n        down_warp_weight)\n    result_shape = (\n        warp_x.get_shape().as_list() + data.get_shape().as_list()[-1:])\n    result.set_shape(result_shape)\n\n    return result", ""]}
{"filename": "src/metrics.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'''\nCollection of metrics to evaluate performance of contour tracking models\n", "Collection of metrics to evaluate performance of contour tracking models\n\nA few of them are the Point Tracking Metrics implemented in Polygonal Point Set Tracking\nThese measure the distance between the exact corresponding points\nfrom https://github.com/ghnam-ken/PoST/blob/main/eval/metric.py\n'''\n\nimport numpy as np\nimport math\nimport cv2", "import math\nimport cv2\nfrom skimage.morphology import disk\n\n\ndef normalize_points(pts, image_width, image_height):\n    pts = pts.copy()\n    pts[...,0] /= image_width\n    pts[...,1] /= image_height\n    return pts", "\n\ndef point_wise_spatial_accuracy(ps1, ps2, image_width, image_height, thresh):\n    '''\n        Args) ps1, ps2 : normalized point sets\n        Retern) acc: spatial accuracy\n    '''\n    assert len(ps1) == len(ps2), \\\n        f\"length of given point sets are differenct: len(ps1)={len(ps1)}, len(ps2)={len(ps2)}\"\n\n    ps1 = normalize_points(ps1, image_width, image_height)\n    ps2 = normalize_points(ps2, image_width, image_height)\n\n    dists = (ps1 - ps2) ** 2\n    dists = np.sum(dists, axis=-1)\n    dists = np.sqrt(dists)\n\n    return (dists <= thresh)*1", "\n\ndef spatial_accuracy(ps1, ps2, image_width, image_height, thresh):\n    '''\n        Args) ps1, ps2 : normalized point sets\n        Retern) acc: spatial accuracy\n    '''\n    assert len(ps1) == len(ps2), \\\n        f\"length of given point sets are differenct: len(ps1)={len(ps1)}, len(ps2)={len(ps2)}\"\n\n    ps1 = normalize_points(ps1, image_width, image_height)\n    ps2 = normalize_points(ps2, image_width, image_height)\n\n    dists = (ps1 - ps2) ** 2\n    dists = np.sum(dists, axis=-1)\n    dists = np.sqrt(dists)\n\n    acc = np.mean(dists <= thresh)\n    return acc", "\ndef relative_spatial_accuracy(ps1, ps2, prev_ps1, prev_ps2, image_width, image_height, thresh):\n    '''\n        Args) ps1, ps2 : normalized point sets\n        Retern) acc: temporal accuracy\n\n        ps1: shape (num_points, 2), dtype: np.float32\n        ps2: shape (num_points, 2), dtype: np.float32\n    '''\n    assert len(ps1) == len(ps2), \\\n            f\"length of given point sets are differenct: len(ps1)={len(ps1)}, len(ps2)={len(ps2)}\"\n    assert len(prev_ps1) == len(prev_ps2), \\\n            f\"length of given point sets are differenct: len(prev_ps1)={len(prev_ps1)}, len(prev_ps2)={len(prev_ps2)}\"\n    assert len(ps1) == len(prev_ps1)\n\n    ps1 = normalize_points(ps1, image_width, image_height)\n    ps2 = normalize_points(ps2, image_width, image_height)\n    prev_ps1 = normalize_points(prev_ps1, image_width, image_height)\n    prev_ps2 = normalize_points(prev_ps2, image_width, image_height)\n\n    gt_dists = ps1 - prev_ps1\n    pred_dists = ps2 - prev_ps2\n\n    diffs = (gt_dists - pred_dists) ** 2\n    diffs = np.sum(diffs, axis=-1)\n    diffs = np.sqrt(diffs)\n\n    acc = np.mean(diffs <= thresh, axis=-1)\n    acc = np.mean(acc)\n    return acc", "\n\ndef temporal_accuracy(ps1, ps2, prev_ps1, prev_ps2, image_width, image_height, thresh):\n    '''\n        Args) ps1, ps2 : normalized point sets\n        Retern) acc: temporal accuracy\n    '''\n    assert len(ps1) == len(ps2), \\\n            f\"length of given point sets are differenct: len(ps1)={len(ps1)}, len(ps2)={len(ps2)}\"\n    assert len(prev_ps1) == len(prev_ps2), \\\n            f\"length of given point sets are differenct: len(prev_ps1)={len(prev_ps1)}, len(prev_ps2)={len(prev_ps2)}\"\n    assert len(ps1) == len(prev_ps1)\n\n    ps1 = normalize_points(ps1, image_width, image_height)\n    ps2 = normalize_points(ps2, image_width, image_height)\n    prev_ps1 = normalize_points(prev_ps1, image_width, image_height)\n    prev_ps2 = normalize_points(prev_ps2, image_width, image_height)\n\n    dists_prev = ps1 - ps2\n    dists_next = prev_ps1 - prev_ps2\n\n    diffs = (dists_prev - dists_next) ** 2\n    diffs = np.sum(diffs, axis=-1)\n    diffs = np.sqrt(diffs)\n\n    acc = np.mean(diffs <= thresh, axis=-1)\n    acc = np.mean(acc)\n    return acc", "\n\ndef contour_accuracy(contour_indices1, contour_indices2, total_contour_length, thresh):\n    '''\n        Args) contour_indices1, contour_indices2\n        Retern) acc: contour accuracy\n    '''\n    assert len(contour_indices1) == len(contour_indices2), \\\n        f\"length of given point sets are differenct: len(ps1)={len(contour_indices1)}, len(ps2)={len(contour_indices2)}\"\n\n    dists = np.abs(contour_indices1 - contour_indices2) / total_contour_length\n    acc = np.mean(dists <= thresh )\n\n    return acc", "\n'''\nRegion similarity J For the evaluation on video object segmentation datasets (CPC and DAVIS2016)\n\nfrom https://github.com/davisvideochallenge/davis2017-evaluation/blob/master/davis2017/metrics.py\n'''\n\ndef db_eval_iou(annotation, segmentation, void_pixels=None):\n    \"\"\"  Compute region similarity as the Jaccard Index (J).\n\n    Jaccard Index = (the number in both sets) / (the number in either set) * 100\n\n    Arguments:\n        annotation   (ndarray): binary annotation   map.\n        segmentation (ndarray): binary segmentation map.\n        void_pixels  (ndarray): optional mask with void pixels\n    Return:\n        jaccard (float): region similarity\n    \"\"\"\n    assert annotation.shape == segmentation.shape, \\\n        f'Annotation({annotation.shape}) and segmentation:{segmentation.shape} dimensions do not match.'\n    annotation = annotation.astype(np.bool)\n    segmentation = segmentation.astype(np.bool)\n\n    if void_pixels is not None:\n        assert annotation.shape == void_pixels.shape, \\\n            f'Annotation({annotation.shape}) and void pixels:{void_pixels.shape} dimensions do not match.'\n        void_pixels = void_pixels.astype(np.bool)\n    else:\n        void_pixels = np.zeros_like(segmentation)\n\n    # Intersection between all sets\n    inters = np.sum((segmentation & annotation) & np.logical_not(void_pixels), axis=(-2, -1))\n    union = np.sum((segmentation | annotation) & np.logical_not(void_pixels), axis=(-2, -1))\n\n    j = inters / union\n    if j.ndim == 0:\n        j = 1 if np.isclose(union, 0) else j\n    else:\n        j[np.isclose(union, 0)] = 1\n    return j", "\n\ndef db_eval_boundary(annotation, segmentation, void_pixels=None, bound_th=0.008):\n    assert annotation.shape == segmentation.shape\n    if void_pixels is not None:\n        assert annotation.shape == void_pixels.shape\n    if annotation.ndim != 2:\n        raise ValueError(f'db_eval_boundary does not support tensors with {annotation.ndim} dimensions')\n\n    f_res = f_measure(segmentation, annotation, void_pixels, bound_th=bound_th)\n\n    return f_res", "\n'''\nBoundary Accuracy F for the evaluation on video object segmentation datasets (CPC and DAVIS2016)\n\nfrom https://github.com/davisvideochallenge/davis2017-evaluation/blob/master/davis2017/metrics.py\n'''\n\ndef f_measure(fg_boundary, gt_boundary, void_pixels=None, bound_pix=1):\n    \"\"\"\n    Compute mean,recall and decay from per-frame evaluation.\n    Calculates precision/recall for boundaries between foreground_mask and\n    gt_mask using morphological operators to speed it up.\n    Arguments:\n        fg_boundary (ndarray): binary predicted contour image.\n        gt_boundary (ndarray): binary annotated contour image.\n        void_pixels (ndarray): optional mask with void pixels\n    Returns:\n        F (float): boundaries F-measure\n    \"\"\"\n\n    # TODO Fix Precision value too big problem due to fg_match being too large\n    assert np.atleast_3d(fg_boundary).shape[2] == 1\n    if void_pixels is not None:\n        void_pixels = void_pixels.astype(np.bool)\n    else:\n        void_pixels = np.zeros_like(fg_boundary).astype(np.bool)\n\n    # bound_th = 0.008\n    # bound_pix = bound_th if bound_th >= 1 else \\\n    #     np.ceil(bound_th * np.linalg.norm(foreground_mask.shape))  # frobenius norm\n    intensity_threshold = 0  # slightly better than 127\n    # import pdb;pdb.set_trace()\n    fg_boundary = (fg_boundary > intensity_threshold).astype(np.uint8)\n    gt_boundary = (gt_boundary > intensity_threshold).astype(np.uint8)\n\n    # Get the pixel boundaries of both masks\n    fg_boundary = fg_boundary * np.logical_not(void_pixels)\n    gt_boundary = gt_boundary * np.logical_not(void_pixels)\n\n    a_disk = disk(bound_pix).astype(np.uint8)\n    fg_dil = cv2.dilate(fg_boundary, a_disk )\n    gt_dil = cv2.dilate(gt_boundary, a_disk )\n\n    # Get the intersection\n    gt_match = gt_boundary * fg_dil\n    fg_match = fg_boundary * gt_dil\n\n    # ---------------- save intermediate outputs for debugging -----------------\n    # cv2.imwrite('fg_dil.png', fg_dil*255)\n    # cv2.imwrite('gt_dil.png', gt_dil*255)\n    # cv2.imwrite('gt_match.png', gt_match*255)\n    # cv2.imwrite('fg_match.png', fg_match*255)\n    # ---------------------------------------------------------\n\n    # Area of the intersection\n    n_fg = np.sum(fg_boundary)\n    n_gt = np.sum(gt_boundary)\n\n    # % Compute precision and recall\n    if n_fg == 0 and n_gt > 0:\n        precision = 1\n        recall = 0\n    elif n_fg > 0 and n_gt == 0:\n        precision = 0\n        recall = 1\n    elif n_fg == 0 and n_gt == 0:\n        precision = 1\n        recall = 1\n    else:\n        precision = np.sum(fg_match) / float(n_fg)\n        recall = np.sum(gt_match) / float(n_gt)\n\n    # Compute F measure\n    if precision + recall == 0:\n        F = 0\n    else:\n        F = 2 * precision * recall / (precision + recall)\n\n    return F, precision, recall", "\n\ndef _seg2bmap(seg, width=None, height=None):\n    \"\"\"\n    From a segmentation, compute a binary boundary map with 1 pixel wide\n    boundaries.  The boundary pixels are offset by 1/2 pixel towards the\n    origin from the actual segment boundary.\n    Arguments:\n        seg     : Segments labeled from 1..k.\n        width\t  :\tWidth of desired bmap  <= seg.shape[1]\n        height  :\tHeight of desired bmap <= seg.shape[0]\n    Returns:\n        bmap (ndarray):\tBinary boundary map.\n     David Martin <dmartin@eecs.berkeley.edu>\n     January 2003\n    \"\"\"\n\n    seg = seg.astype(np.bool)\n    seg[seg > 0] = 1\n\n    assert np.atleast_3d(seg).shape[2] == 1\n\n    width = seg.shape[1] if width is None else width\n    height = seg.shape[0] if height is None else height\n\n    h, w = seg.shape[:2]\n\n    ar1 = float(width) / float(height)\n    ar2 = float(w) / float(h)\n\n    assert not (\n            width > w | height > h | abs(ar1 - ar2) > 0.01\n    ), \"Can\" \"t convert %dx%d seg to %dx%d bmap.\" % (w, h, width, height)\n\n    e = np.zeros_like(seg)\n    s = np.zeros_like(seg)\n    se = np.zeros_like(seg)\n\n    e[:, :-1] = seg[:, 1:]\n    s[:-1, :] = seg[1:, :]\n    se[:-1, :-1] = seg[1:, 1:]\n\n    b = seg ^ e | seg ^ s | seg ^ se\n    b[-1, :] = seg[-1, :] ^ e[-1, :]\n    b[:, -1] = seg[:, -1] ^ s[:, -1]\n    b[-1, -1] = 0\n\n    if w == width and h == height:\n        bmap = b\n    else:\n        bmap = np.zeros((height, width))\n        for x in range(w):\n            for y in range(h):\n                if b[y, x]:\n                    j = 1 + math.floor((y - 1) + height / h)\n                    i = 1 + math.floor((x - 1) + width / h)\n                    bmap[j, i] = 1\n\n    return bmap", "\n\ndef get_warp_error(a_warped_contour, a_contour):\n    def l1(x):\n        return tf.abs(x + 1e-6)\n\n    def robust_l1(x):\n        \"\"\"Robust L1 metric.\"\"\"\n        return (x ** 2 + 0.001 ** 2) ** 0.5\n\n    error = a_warped_contour.astype(np.int32) - a_contour.astype(np.int32)  # e.g. 127 - 255  or 127 - 0\n\n    cliped_error = np.clip(error, 0, 255)  # clipping to ignore negative values\n\n    final_error = robust_l1(cliped_error)\n\n    return final_error", ""]}
{"filename": "src/contour_flow_main.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nMain script to train and evaluate our contour tracker.\n\"\"\"", "Main script to train and evaluate our contour tracker.\n\"\"\"\n\n# pylint:disable=g-importing-member\nfrom functools import partial\nfrom absl import app\nfrom absl import flags\nimport datetime\n\nimport gin", "\nimport gin\nimport numpy as np\nimport tensorflow as tf\n\nfrom src import uflow_augmentation\nfrom src import uflow_data\n# pylint:disable=unused-import\nfrom src import uflow_flags\nfrom src import uflow_plotting", "from src import uflow_flags\nfrom src import uflow_plotting\nfrom src.contour_flow_net import ContourFlow\n\nFLAGS = flags.FLAGS\n\ndef create_contour_flow():\n  \"\"\"Build the contour flow model.\"\"\"\n\n  build_selfsup_transformations = partial(\n      uflow_augmentation.build_selfsup_transformations,\n      crop_height=FLAGS.selfsup_crop_height,\n      crop_width=FLAGS.selfsup_crop_width,\n      max_shift_height=FLAGS.selfsup_max_shift,\n      max_shift_width=FLAGS.selfsup_max_shift,\n      resize=FLAGS.resize_selfsup)\n\n  # Define learning rate schedules [none, cosine, linear, expoential].\n  def learning_rate_fn():\n    step = tf.compat.v1.train.get_or_create_global_step()\n    effective_step = tf.maximum(step - FLAGS.lr_decay_after_num_steps + 1, 0)\n    lr_step_ratio = tf.cast(effective_step, 'float32') / float( FLAGS.lr_decay_steps)\n\n    if FLAGS.lr_decay_type == 'none' or FLAGS.lr_decay_steps <= 0:\n      return FLAGS.gpu_learning_rate\n    elif FLAGS.lr_decay_type == 'cosine':\n      x = np.pi * tf.minimum(lr_step_ratio, 1.0)\n      return FLAGS.gpu_learning_rate * (tf.cos(x) + 1.0) / 2.0\n    elif FLAGS.lr_decay_type == 'linear':\n      return FLAGS.gpu_learning_rate * tf.maximum(1.0 - lr_step_ratio, 0.0)\n    elif FLAGS.lr_decay_type == 'exponential':\n      return FLAGS.gpu_learning_rate * 0.5**lr_step_ratio\n    else:\n      raise ValueError('Unknown lr_decay_type', FLAGS.lr_decay_type)\n\n  occ_weights = {\n      'fb_abs': FLAGS.occ_weights_fb_abs,\n      'forward_collision': FLAGS.occ_weights_forward_collision,\n      'backward_zero': FLAGS.occ_weights_backward_zero,\n  }\n  # Switch off loss-terms that have weights < 1e-2.\n  occ_weights = {k: v for (k, v) in occ_weights.items() if v > 1e-2}\n\n  occ_thresholds = {\n      'fb_abs': FLAGS.occ_thresholds_fb_abs,\n      'forward_collision': FLAGS.occ_thresholds_forward_collision,\n      'backward_zero': FLAGS.occ_thresholds_backward_zero,\n  }\n  occ_clip_max = {\n      'fb_abs': FLAGS.occ_clip_max_fb_abs,\n      'forward_collision': FLAGS.occ_clip_max_forward_collision,\n  }\n\n  contour_flow = ContourFlow(\n      checkpoint_dir=FLAGS.checkpoint_dir,\n      optimizer=FLAGS.optimizer,\n      learning_rate=learning_rate_fn,\n      only_forward=FLAGS.only_forward,\n      level1_num_layers=FLAGS.level1_num_layers,\n      level1_num_filters=FLAGS.level1_num_filters,\n      level1_num_1x1=FLAGS.level1_num_1x1,\n      dropout_rate=FLAGS.dropout_rate,\n      build_selfsup_transformations=build_selfsup_transformations,\n      fb_sigma_teacher=FLAGS.fb_sigma_teacher,\n      fb_sigma_student=FLAGS.fb_sigma_student,\n      train_with_supervision=FLAGS.use_supervision,\n      train_with_gt_occlusions=FLAGS.use_gt_occlusions,\n      train_with_segmentations=FLAGS.use_segmentations,\n      train_with_seg_points=FLAGS.use_seg_points,\n      train_with_tracking_points=FLAGS.use_tracking_points,\n      smoothness_edge_weighting=FLAGS.smoothness_edge_weighting,\n      teacher_image_version=FLAGS.teacher_image_version,\n      stop_gradient_mask=FLAGS.stop_gradient_mask,\n      selfsup_mask=FLAGS.selfsup_mask,\n      normalize_before_cost_volume=FLAGS.normalize_before_cost_volume,\n      original_layer_sizes=FLAGS.original_layer_sizes,\n      shared_flow_decoder=FLAGS.shared_flow_decoder,\n      channel_multiplier=FLAGS.channel_multiplier,\n      num_levels=FLAGS.num_levels,\n      use_cost_volume=FLAGS.use_cost_volume,\n      use_feature_warp=FLAGS.use_feature_warp,\n      accumulate_flow=FLAGS.accumulate_flow,\n      occlusion_estimation=FLAGS.occlusion_estimation,\n      occ_weights=occ_weights,\n      occ_thresholds=occ_thresholds,\n      occ_clip_max=occ_clip_max,\n      smoothness_at_level=FLAGS.smoothness_at_level,\n      input_image_size=(FLAGS.height, FLAGS.width, 3)\n  )\n  return contour_flow", "\n\ndef check_model_frozen(feature_model, flow_model, prev_flow_output=None):\n  \"\"\"Check that a frozen model isn't somehow changing over time.\"\"\"\n  state = np.random.RandomState(40)\n  input1 = state.randn(FLAGS.batch_size, FLAGS.height, FLAGS.width,\n                       3).astype(np.float32)\n  input2 = state.randn(FLAGS.batch_size, FLAGS.height, FLAGS.width,\n                       3).astype(np.float32)\n  feature_output1 = feature_model(input1, split_features_by_sample=False)\n  feature_output2 = feature_model(input2, split_features_by_sample=False)\n  flow_output = flow_model(feature_output1, feature_output2, training=False)\n  if prev_flow_output is None:\n    return flow_output\n  for f1, f2 in zip(prev_flow_output, flow_output):\n    assert np.max(f1.numpy() - f2.numpy()) < .01", "\n\ndef create_frozen_teacher_models(uflow):\n  \"\"\"Create a frozen copy of the current uflow model.\"\"\"\n  uflow_copy = create_contour_flow()\n  teacher_feature_model = uflow_copy.feature_model\n  teacher_flow_model = uflow_copy.flow_model\n  # need to create weights in teacher models by calling them\n  bogus_input1 = np.random.randn(FLAGS.batch_size, FLAGS.height,\n                                 FLAGS.width, 3).astype(np.float32)\n  bogus_input2 = np.random.randn(FLAGS.batch_size, FLAGS.height,\n                                 FLAGS.width, 3).astype(np.float32)\n  existing_model_output = uflow.feature_model(\n      bogus_input1, split_features_by_sample=False)\n  _ = teacher_feature_model(bogus_input1, split_features_by_sample=False)\n  teacher_feature_model.set_weights(uflow.feature_model.get_weights())\n  teacher_output1 = teacher_feature_model(\n      bogus_input1, split_features_by_sample=False)\n  teacher_output2 = teacher_feature_model(\n      bogus_input2, split_features_by_sample=False)\n\n  # check that both feature models have the same output\n  assert np.max(existing_model_output[-1].numpy() -\n                teacher_output1[-1].numpy()) < .01\n  existing_model_flow = uflow.flow_model(\n      teacher_output1, teacher_output2, training=False)\n  _ = teacher_flow_model(teacher_output1, teacher_output2, training=False)\n  teacher_flow_model.set_weights(uflow.flow_model.get_weights())\n  teacher_flow = teacher_flow_model(\n      teacher_output1, teacher_output2, training=False)\n  # check that both flow models have the same output\n  assert np.max(existing_model_flow[-1].numpy() -\n                teacher_flow[-1].numpy()) < .01\n  # Freeze the teacher models.\n  for layer in teacher_feature_model.layers:\n    layer.trainable = False\n  for layer in teacher_flow_model.layers:\n    layer.trainable = False\n\n  return teacher_feature_model, teacher_flow_model", "\n\ndef main(unused_argv):\n  # ---------------------------\n  FLAGS.checkpoint_dir = FLAGS.generated_dir + '/checkpoints/'\n  FLAGS.plot_dir = FLAGS.generated_dir + '/plot/'\n  # ---------------------------\n\n  if FLAGS.no_tf_function:\n    tf.config.run_functions_eagerly(True)\n    print('TFFUNCTION DISABLED')\n\n  gin.parse_config_files_and_bindings(FLAGS.config_file, FLAGS.gin_bindings)\n  # Make directories if they do not exist yet.\n  if FLAGS.checkpoint_dir and not tf.io.gfile.exists(FLAGS.checkpoint_dir):\n    print('Making new checkpoint directory', FLAGS.checkpoint_dir)\n    tf.io.gfile.makedirs(FLAGS.checkpoint_dir)\n  if FLAGS.plot_dir and not tf.io.gfile.exists(FLAGS.plot_dir):\n    print('Making new plot directory', FLAGS.plot_dir)\n    tf.io.gfile.makedirs(FLAGS.plot_dir)\n\n  # -------------------- Prepare the dataset loader --------------------\n\n  if FLAGS.train_on:\n    print('Making training iterator.')\n    print('batch size is', FLAGS.batch_size)\n    train_it = uflow_data.make_train_iterator(\n        FLAGS.train_on,\n        FLAGS.height,\n        FLAGS.width,\n        FLAGS.shuffle_buffer_size,\n        FLAGS.batch_size,\n        FLAGS.seq_len,\n        crop_instead_of_resize=FLAGS.crop_instead_of_resize,\n        apply_augmentation=False, # TODO: set it to true\n        include_ground_truth=FLAGS.use_supervision,\n        resize_gt_flow=FLAGS.resize_gt_flow_supervision,\n        include_occlusions=FLAGS.use_gt_occlusions,\n        include_segmentations=FLAGS.use_segmentations,\n        include_seg_points=FLAGS.use_seg_points,\n        include_tracking_points=FLAGS.use_tracking_points)\n\n    if FLAGS.valid_on:\n        print('Making eval datasets and eval functions.')\n        evaluate = uflow_data.make_predict_function(\n            FLAGS.valid_on,\n            FLAGS.height,\n            FLAGS.width,\n            progress_bar=True,\n            plot_dir=FLAGS.plot_dir,\n            num_plots=FLAGS.num_plot,\n            include_segmentations=FLAGS.use_segmentations,\n            include_seg_points=FLAGS.use_seg_points,\n            include_tracking_points=FLAGS.use_tracking_points,\n            evaluate_bool=True)\n\n  elif FLAGS.predict_on:\n    print('Making predict function.')\n    predict = uflow_data.make_predict_function(\n      FLAGS.predict_on,\n      FLAGS.height,\n      FLAGS.width,\n      progress_bar=True,\n      plot_dir=FLAGS.plot_dir,\n      num_plots=FLAGS.num_plot,\n      include_segmentations=FLAGS.use_segmentations,\n      include_seg_points=FLAGS.use_seg_points,\n      include_tracking_points=FLAGS.use_tracking_points,\n      evaluate_bool=False)\n\n  # -------------------- Build the model --------------------\n  uflow = create_contour_flow()\n\n  if not FLAGS.from_scratch:\n      # First restore from init_checkpoint_dir, which is only restored from but\n      # not saved to, and then restore from checkpoint_dir if there is already\n      # a model there (e.g. if the run was stopped and restarted).\n      if FLAGS.init_checkpoint_dir:\n          print('Initializing model from checkpoint {}.'.format(\n              FLAGS.init_checkpoint_dir))\n          uflow.update_checkpoint_dir(FLAGS.init_checkpoint_dir)\n          uflow.restore(\n              reset_optimizer=FLAGS.reset_optimizer,\n              reset_global_step=FLAGS.reset_global_step)\n          uflow.update_checkpoint_dir(FLAGS.checkpoint_dir)\n\n      if FLAGS.checkpoint_dir:\n          print('Restoring model from checkpoint {}.'.format(FLAGS.checkpoint_dir))\n          uflow.restore()\n\n  else:\n      print('Starting from scratch')\n\n  print('@@@uflow is ready@@@')\n\n  # --------------------- Train or predict or eval ---------------------\n\n  if FLAGS.train_on:\n\n    if FLAGS.use_supervision:\n      # Since this is the only loss in this setting, and the Adam optimizer\n      # is scale invariant, the actual weight here does not matter for now.\n      weights = {'supervision': 1.}\n    else:\n      # Note that self-supervision loss is added during training.\n      weights = {\n          'photo': FLAGS.weight_photo,\n          'ssim': FLAGS.weight_ssim,\n          'census': FLAGS.weight_census,\n          'smooth1': FLAGS.weight_smooth1,\n          'smooth2': FLAGS.weight_smooth2,\n          'contour_smooth1': FLAGS.weight_contour_smooth1,\n          'contour_smooth2': FLAGS.weight_contour_smooth2,\n          'contour': FLAGS.weight_contour,\n          'flow_magnitude': FLAGS.weight_flow_magnitude,\n          'edge_constant': FLAGS.smoothness_edge_constant,\n      }\n\n      # import pdb; pdb.set_trace()\n      # weights {'census': 1.0, 'smooth2': 2.0, 'edge_constant': 150.0}\n      # Switch off loss-terms that have weights < 1e-7.\n      weights = {\n          k: v for (k, v) in weights.items() if v > 1e-7 or k == 'edge_constant'\n      }\n\n    def weight_selfsup_fn():\n      step = tf.compat.v1.train.get_or_create_global_step(\n      ) % FLAGS.selfsup_step_cycle\n      # Start self-supervision only after a certain number of steps.\n      # Linearly increase self-supervision weight for a number of steps.\n      ramp_up_factor = tf.clip_by_value(\n          float(step - (FLAGS.selfsup_after_num_steps - 1)) /\n          float(max(FLAGS.selfsup_ramp_up_steps, 1)), 0., 1.)\n      return FLAGS.weight_selfsup * ramp_up_factor\n\n    distance_metrics = {\n        'photo': FLAGS.distance_photo,\n        'contour': FLAGS.distance_contour,\n        'census': FLAGS.distance_census,\n    }\n\n    print('Starting training loop.')\n    log = dict()\n    epoch = 0\n\n    teacher_feature_model = None\n    teacher_flow_model = None\n    test_frozen_flow = None\n\n    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    train_log_dir = f\"{FLAGS.checkpoint_dir}/training_log_{current_time}\"\n    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n\n    while True:\n      current_step = tf.compat.v1.train.get_or_create_global_step().numpy()\n\n      # Set which occlusion estimation methods could be active at this point.\n      # (They will only be used if occlusion_estimation is set accordingly.)\n      occ_active = {\n          'uflow':\n              FLAGS.occlusion_estimation == 'uflow',\n          'brox':\n              current_step > FLAGS.occ_after_num_steps_brox,\n          'wang':\n              current_step > FLAGS.occ_after_num_steps_wang,\n          'wang4':\n              current_step > FLAGS.occ_after_num_steps_wang,\n          'wangthres':\n              current_step > FLAGS.occ_after_num_steps_wang,\n          'wang4thres':\n              current_step > FLAGS.occ_after_num_steps_wang,\n          'fb_abs':\n              current_step > FLAGS.occ_after_num_steps_fb_abs,\n          'forward_collision':\n              current_step > FLAGS.occ_after_num_steps_forward_collision,\n          'backward_zero':\n              current_step > FLAGS.occ_after_num_steps_backward_zero,\n      }\n\n      current_weights = {k: v for k, v in weights.items()}\n\n      # Prepare self-supervision if it will be used in the next epoch.\n      if FLAGS.weight_selfsup > 1e-7 and (\n          current_step % FLAGS.selfsup_step_cycle\n      ) + FLAGS.epoch_length > FLAGS.selfsup_after_num_steps:\n\n        # Add selfsup weight with a ramp-up schedule. This will cause a\n        # recompilation of the training graph defined in uflow.train(...).\n        current_weights['selfsup'] = weight_selfsup_fn\n\n        # Freeze model for teacher distillation.\n        if teacher_feature_model is None and FLAGS.frozen_teacher:\n          # Create a copy of the existing models and freeze them as a teacher.\n          # Tell uflow about the new, frozen teacher model.\n          teacher_feature_model, teacher_flow_model = create_frozen_teacher_models(\n              uflow)\n          uflow.set_teacher_models(\n              teacher_feature_model=teacher_feature_model,\n              teacher_flow_model=teacher_flow_model)\n          test_frozen_flow = check_model_frozen(\n              teacher_feature_model, teacher_flow_model, prev_flow_output=None)\n\n          # Check that the model actually is frozen.\n          if FLAGS.frozen_teacher and test_frozen_flow is not None:\n            check_model_frozen(\n                teacher_feature_model,\n                teacher_flow_model,\n                prev_flow_output=test_frozen_flow)\n                  \n      # Train for an epoch and save the results.\n      log_update, saved_offset_dict = uflow.train(\n          train_it,\n          current_epoch=int(current_step//FLAGS.epoch_length),\n          num_steps=FLAGS.epoch_length,\n          weights=current_weights,\n          progress_bar=True,\n          plot_dir=FLAGS.plot_dir if FLAGS.plot_debug_info else None,\n          distance_metrics=distance_metrics,\n          occ_active=occ_active)\n\n      # ------ Validation after training epoch ------\n      if FLAGS.valid_on:\n        eval_log_update = evaluate(uflow)\n        log_update = {**log_update, **eval_log_update} # merge two dict\n\n      # ------ Log training and validation results ------\n\n      for key in log_update:\n        if key in log:\n          log[key].append(log_update[key])\n        else:\n          log[key] = [log_update[key]]\n\n      if FLAGS.checkpoint_dir and not FLAGS.no_checkpointing:\n        uflow.save()\n\n      # Print losses from last epoch.\n      uflow_plotting.print_log(log, epoch)\n      # log losses and offsets from last epoch to tensorboard\n      uflow_plotting.log_to_tensorboard(train_summary_writer, log, saved_offset_dict, epoch)\n\n      if current_step >= FLAGS.num_train_steps:\n        break\n\n      epoch += 1\n\n  elif FLAGS.predict_on:\n    predict(uflow)\n    print('Prediction Complete.')", "\n\nif __name__ == '__main__':\n  app.run(main)"]}
{"filename": "src/contour_flow_model.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n'''\nImplement contour flow model takes features along the contour\nEstimates the optical flow along the contour", "Implement contour flow model takes features along the contour\nEstimates the optical flow along the contour\n'''\n\nimport collections\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Conv2D", "from tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Conv2DTranspose\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.models import Sequential\n\nfrom src import uflow_utils\n\n\ndef normalize_features(feature_list, normalize, center, moments_across_channels,\n                       moments_across_images):\n  \"\"\"Normalizes feature tensors (e.g., before computing the cost volume).\n\n  Args:\n    feature_list: list of tf.tensors, each with dimensions [b, h, w, c]\n    normalize: bool flag, divide features by their standard deviation\n    center: bool flag, subtract feature mean\n    moments_across_channels: bool flag, compute mean and std across channels\n    moments_across_images: bool flag, compute mean and std across images\n\n  Returns:\n    list, normalized feature_list\n  \"\"\"\n\n  # Compute feature statistics.\n\n  statistics = collections.defaultdict(list)\n  axes = [-3, -2, -1] if moments_across_channels else [-3, -2]\n  for feature_image in feature_list:\n    mean, variance = tf.nn.moments(x=feature_image, axes=axes, keepdims=True)\n    statistics['mean'].append(mean)\n    statistics['var'].append(variance)\n\n  if moments_across_images:\n    statistics['mean'] = ([tf.reduce_mean(input_tensor=statistics['mean'])] *\n                          len(feature_list))\n    statistics['var'] = [tf.reduce_mean(input_tensor=statistics['var'])\n                        ] * len(feature_list)\n\n  statistics['std'] = [tf.sqrt(v + 1e-16) for v in statistics['var']]\n\n  # Center and normalize features.\n\n  if center:\n    feature_list = [\n        f - mean for f, mean in zip(feature_list, statistics['mean'])\n    ]\n  if normalize:\n    feature_list = [f / std for f, std in zip(feature_list, statistics['std'])]\n\n  return feature_list", "\ndef normalize_features(feature_list, normalize, center, moments_across_channels,\n                       moments_across_images):\n  \"\"\"Normalizes feature tensors (e.g., before computing the cost volume).\n\n  Args:\n    feature_list: list of tf.tensors, each with dimensions [b, h, w, c]\n    normalize: bool flag, divide features by their standard deviation\n    center: bool flag, subtract feature mean\n    moments_across_channels: bool flag, compute mean and std across channels\n    moments_across_images: bool flag, compute mean and std across images\n\n  Returns:\n    list, normalized feature_list\n  \"\"\"\n\n  # Compute feature statistics.\n\n  statistics = collections.defaultdict(list)\n  axes = [-3, -2, -1] if moments_across_channels else [-3, -2]\n  for feature_image in feature_list:\n    mean, variance = tf.nn.moments(x=feature_image, axes=axes, keepdims=True)\n    statistics['mean'].append(mean)\n    statistics['var'].append(variance)\n\n  if moments_across_images:\n    statistics['mean'] = ([tf.reduce_mean(input_tensor=statistics['mean'])] *\n                          len(feature_list))\n    statistics['var'] = [tf.reduce_mean(input_tensor=statistics['var'])\n                        ] * len(feature_list)\n\n  statistics['std'] = [tf.sqrt(v + 1e-16) for v in statistics['var']]\n\n  # Center and normalize features.\n\n  if center:\n    feature_list = [\n        f - mean for f, mean in zip(feature_list, statistics['mean'])\n    ]\n  if normalize:\n    feature_list = [f / std for f, std in zip(feature_list, statistics['std'])]\n\n  return feature_list", "\n\ndef compute_cost_volume(features1, features2, max_displacement):\n  \"\"\"Compute the cost volume between features1 and features2.\n\n  Displace features2 up to max_displacement in any direction and compute the\n  per pixel cost of features1 and the displaced features2.\n\n  Args:\n    features1: tf.tensor of shape [b, h, w, c]  which means batch, height, width, channels\n    features2: tf.tensor of shape [b, h, w, c]\n    max_displacement: int, maximum displacement for cost volume computation.\n\n  Returns:\n    tf.tensor of shape [b, h, w, (2 * max_displacement + 1) ** 2] of costs for\n    all displacements.\n  \"\"\"\n\n  # Set maximum displacement and compute the number of image shifts.\n  _, height, width, _ = features1.shape.as_list()\n  if max_displacement <= 0 or max_displacement >= height:  # why not check width also?\n    raise ValueError(f'Max displacement of {max_displacement} is too large.')\n\n  max_disp = max_displacement\n  num_shifts = 2 * max_disp + 1\n\n\n  # Pad features2 such that shifts do not go out of bounds.\n  features2_padded = tf.pad(\n      tensor=features2,\n      paddings=[[0, 0], [max_disp, max_disp], [max_disp, max_disp], [0, 0]],\n      mode='CONSTANT')\n\n  # Shift features2 while keeping features1 fixed to compute the cost volume (correlation).\n  cost_list = []\n  for i in range(num_shifts):\n    for j in range(num_shifts):\n      corr = tf.reduce_mean(\n          input_tensor=features1 *\n          features2_padded[:, i:(height + i), j:(width + j), :],\n          axis=-1,\n          keepdims=True)\n      cost_list.append(corr)\n  cost_volume = tf.concat(cost_list, axis=-1)\n\n  return cost_volume", "\n\nclass ContourFlowModel(Model):\n  \"\"\"Model for estimating flow based on the feature pyramids of two images.\"\"\"\n\n  def __init__(self,\n               leaky_relu_alpha=0.1,\n               dropout_rate=0.25,\n               num_channels_upsampled_context=32,\n               num_levels=5,\n               normalize_before_cost_volume=True,\n               channel_multiplier=1.,\n               use_cost_volume=True,\n               use_feature_warp=True,\n               accumulate_flow=True,\n               use_bfloat16=False,\n               shared_flow_decoder=False):\n\n    super(ContourFlowModel, self).__init__()\n    self._use_bfloat16 = use_bfloat16\n    if use_bfloat16:\n      self._dtype_policy = tf.keras.mixed_precision.experimental.Policy(\n          'mixed_bfloat16')\n    else:\n      self._dtype_policy = tf.keras.mixed_precision.experimental.Policy(\n          'float32')\n    self._leaky_relu_alpha = leaky_relu_alpha\n    self._drop_out_rate = dropout_rate\n    self._num_context_up_channels = num_channels_upsampled_context\n    self._num_levels = num_levels\n    self._normalize_before_cost_volume = normalize_before_cost_volume\n    self._channel_multiplier = channel_multiplier\n    self._use_cost_volume = use_cost_volume\n    self._use_feature_warp = use_feature_warp\n    self._accumulate_flow = accumulate_flow\n    self._shared_flow_decoder = shared_flow_decoder\n\n    self._refine_model = self._build_refinement_model()\n    # print('self._refine_model', self._refine_model.summary())\n    self._flow_layers = self._build_flow_layers()\n    # print('self._flow_layers', self._flow_layers.summary())\n    \n    if not self._use_cost_volume:\n      self._cost_volume_surrogate_convs = self._build_cost_volume_surrogate_convs(\n      )\n    if num_channels_upsampled_context:\n      self._context_up_layers = self._build_upsample_layers(\n          num_channels=int(num_channels_upsampled_context * channel_multiplier))\n    if self._shared_flow_decoder:\n      # pylint:disable=invalid-name\n      self._1x1_shared_decoder = self._build_1x1_shared_decoder()\n\n  def call(self, feature_pyramid1, feature_pyramid2, segmentation1, segmentation2, training=False):\n    \"\"\"Run the model.\"\"\"\n    context = None\n    flow = None\n    flow_up = None\n    context_up = None\n    flows = []\n\n    # Go top down through the levels to the second to last one to estimate flow.\n    for level, (features1, features2) in reversed(\n        list(enumerate(zip(feature_pyramid1, feature_pyramid2)))[1:]):\n\n      batch_size, height, width, _ = features1.shape.as_list()\n      # ----------------------------------------------------------\n      # Feature attention to contour region only\n\n      # import pdb; pdb.set_trace()\n      # if training:\n      #   # resize\n      #   segmentation1 = tf.image.resize(-1*segmentation1+1, [height, width])\n      #   segmentation2 = tf.image.resize(-1*segmentation2+1, [height, width])\n      #\n      #   # sample features at segmentation portion only\n      #   features1 = tf.math.multiply(features1, segmentation1)\n      #   features2 = tf.math.multiply(features2, segmentation2)\n\n      # -------------------------------------------------------\n\n      # init flows with zeros for coarsest level if needed\n      if self._shared_flow_decoder and flow_up is None:\n\n        flow_up = tf.zeros(\n            [batch_size, height, width, 2],\n            dtype=tf.bfloat16 if self._use_bfloat16 else tf.float32)\n        if self._num_context_up_channels:\n          num_channels = int(self._num_context_up_channels *\n                             self._channel_multiplier)\n          context_up = tf.zeros(\n              [batch_size, height, width, num_channels],\n              dtype=tf.bfloat16 if self._use_bfloat16 else tf.float32)\n\n      # --------------- Warp features2 with upsampled flow from higher level.\n\n      if flow_up is None or not self._use_feature_warp:\n        warped2 = features2\n      else:\n        warp_up = uflow_utils.flow_to_warp(flow_up)\n        warped2 = uflow_utils.resample(features2, warp_up)\n\n      # --------------- Compute cost volume by comparing features1 and warped features2.\n      features1_normalized, warped2_normalized = normalize_features(\n          [features1, warped2],\n          normalize=self._normalize_before_cost_volume,\n          center=self._normalize_before_cost_volume,\n          moments_across_channels=True,\n          moments_across_images=True)\n\n      if self._use_cost_volume:\n        cost_volume = compute_cost_volume(\n            features1_normalized, warped2_normalized, max_displacement=4)\n      else:\n        concat_features = Concatenate(axis=-1)(\n            [features1_normalized, warped2_normalized])\n        cost_volume = self._cost_volume_surrogate_convs[level](concat_features)\n\n      cost_volume = LeakyReLU(\n          alpha=self._leaky_relu_alpha, dtype=self._dtype_policy)(\n              cost_volume)\n\n      # ----------------------------------------------\n\n      if self._shared_flow_decoder:\n        # This will ensure to work for arbitrary feature sizes per level.\n        conv_1x1 = self._1x1_shared_decoder[level]\n        features1 = conv_1x1(features1)\n\n      # ------------------- Compute context and flow from previous flow, cost volume, and features1\n      if flow_up is None:\n        x_in = Concatenate(axis=-1)([cost_volume, features1])\n      else:\n        if context_up is None:\n          x_in = Concatenate(axis=-1)([flow_up, cost_volume, features1])\n        else:\n          x_in = Concatenate(axis=-1)(\n              [context_up, flow_up, cost_volume, features1])\n\n\n      # Use dense-net connections to get context\n      x_out = None\n      if self._shared_flow_decoder:\n        # reuse the same flow decoder on all levels\n        flow_layers = self._flow_layers\n      else:\n        flow_layers = self._flow_layers[level]\n      for layer in flow_layers[:-1]:\n        x_out = layer(x_in)\n        x_in = Concatenate(axis=-1)([x_in, x_out])\n      context = x_out\n      # ----------------- Compute Flow -----------------------------\n      flow = flow_layers[-1](context)\n\n      if (training and self._drop_out_rate):\n        maybe_dropout = tf.cast(\n            tf.math.greater(tf.random.uniform([]), self._drop_out_rate),\n            tf.bfloat16 if self._use_bfloat16 else tf.float32)\n        context *= maybe_dropout\n        flow *= maybe_dropout\n\n      if flow_up is not None and self._accumulate_flow:\n        flow += flow_up\n\n      # Upsample flow for the next lower level.\n      flow_up = uflow_utils.upsample(flow, is_flow=True)\n      if self._num_context_up_channels:\n        context_up = self._context_up_layers[level](context)\n\n      # Append results to list.\n      flows.insert(0, flow)\n\n    # Refine flow at level 1.\n    refinement = self._refine_model([context, flow])\n\n    if (training and self._drop_out_rate):\n      refinement *= tf.cast(\n          tf.math.greater(tf.random.uniform([]), self._drop_out_rate),\n          tf.bfloat16 if self._use_bfloat16 else tf.float32)\n    refined_flow = flow + refinement\n    flows[0] = refined_flow\n    return [tf.cast(flow, tf.float32) for flow in flows]\n\n  def _build_cost_volume_surrogate_convs(self):\n    layers = []\n    for _ in range(self._num_levels):\n      layers.append(\n          Conv2D(\n              int(64 * self._channel_multiplier),\n              kernel_size=(4, 4),\n              padding='same',\n              dtype=self._dtype_policy))\n    return layers\n\n  def _build_upsample_layers(self, num_channels):\n    \"\"\"Build layers for upsampling via deconvolution.\"\"\"\n    layers = []\n    for unused_level in range(self._num_levels):\n      layers.append(\n          Conv2DTranspose(\n              num_channels,\n              kernel_size=(4, 4),\n              strides=2,\n              padding='same',\n              dtype=self._dtype_policy))\n    return layers\n\n  def _build_flow_layers(self):\n    \"\"\"Build layers for flow estimation.\"\"\"\n    # Empty list of layers level 0 because flow is only estimated at levels > 0.\n    result = [[]]\n    for _ in range(1, self._num_levels):\n      layers = []\n      for c in [128, 128, 96, 64, 32]:\n        layers.append(\n            Sequential([\n                Conv2D(\n                    int(c * self._channel_multiplier),\n                    kernel_size=(3, 3),\n                    strides=1,\n                    padding='same',\n                    dtype=self._dtype_policy),\n                LeakyReLU(\n                    alpha=self._leaky_relu_alpha, dtype=self._dtype_policy)\n            ]))\n      layers.append(\n          Conv2D(\n              2,\n              kernel_size=(3, 3),\n              strides=1,\n              padding='same',\n              dtype=self._dtype_policy))\n      if self._shared_flow_decoder:\n        return layers\n      result.append(layers)\n    return result\n\n  def _build_refinement_model(self):\n    \"\"\"Build model for flow refinement using dilated convolutions.\"\"\"\n    layers = []\n    layers.append(Concatenate(axis=-1))\n    for c, d in [(128, 1), (128, 2), (128, 4), (96, 8), (64, 16), (32, 1)]:\n      layers.append(\n          Conv2D(\n              int(c * self._channel_multiplier),\n              kernel_size=(3, 3),\n              strides=1,\n              padding='same',\n              dilation_rate=d,\n              dtype=self._dtype_policy))\n      layers.append(\n          LeakyReLU(alpha=self._leaky_relu_alpha, dtype=self._dtype_policy))\n    layers.append(\n        Conv2D(\n            2,\n            kernel_size=(3, 3),\n            strides=1,\n            padding='same',\n            dtype=self._dtype_policy))\n    return Sequential(layers)\n\n  def _build_1x1_shared_decoder(self):\n    \"\"\"Build layers for flow estimation.\"\"\"\n    # Empty list of layers level 0 because flow is only estimated at levels > 0.\n    result = [[]]\n    for _ in range(1, self._num_levels):\n      result.append(\n          Conv2D(\n              32,\n              kernel_size=(1, 1),\n              strides=1,\n              padding='same',\n              dtype=self._dtype_policy))\n    return result", "\n\nclass PWCFeaturePyramid(Model):\n  \"\"\"Model for computing a feature pyramid from an image.\"\"\"\n\n  def __init__(self,\n               leaky_relu_alpha=0.1,\n               filters=None,\n               level1_num_layers=3,\n               level1_num_filters=16,\n               level1_num_1x1=0,\n               original_layer_sizes=False,\n               num_levels=5,\n               channel_multiplier=1.,\n               pyramid_resolution='half',\n               use_bfloat16=False):\n    \"\"\"Constructor.\n\n    Args:\n      leaky_relu_alpha: Float. Alpha for leaky ReLU.\n      filters: Tuple of tuples. Used to construct feature pyramid. Each tuple is\n        of form (num_convs_per_group, num_filters_per_conv).\n      level1_num_layers: How many layers and filters to use on the first\n        pyramid. Only relevant if filters is None and original_layer_sizes\n        is False.\n      level1_num_filters: int, how many filters to include on pyramid layer 1.\n        Only relevant if filters is None and original_layer_sizes if False.\n      level1_num_1x1: How many 1x1 convolutions to use on the first pyramid\n        level.\n      original_layer_sizes: bool, if True, use the original PWC net number\n        of layers and filters.\n      num_levels: int, How many feature pyramid levels to construct.\n      channel_multiplier: float, used to scale up or down the amount of\n        computation by increasing or decreasing the number of channels\n        by this factor.\n      pyramid_resolution: str, specifies the resolution of the lowest (closest\n        to input pyramid resolution)\n      use_bfloat16: bool, whether or not to run in bfloat16 mode.\n    \"\"\"\n\n    super(PWCFeaturePyramid, self).__init__()\n    self._use_bfloat16 = use_bfloat16\n    if use_bfloat16:\n      self._dtype_policy = tf.keras.mixed_precision.experimental.Policy(\n          'mixed_bfloat16')\n    else:\n      self._dtype_policy = tf.keras.mixed_precision.experimental.Policy(\n          'float32')\n    self._channel_multiplier = channel_multiplier\n    if num_levels > 6:\n      raise NotImplementedError('Max number of pyramid levels is 6')\n    if filters is None:\n      if original_layer_sizes:\n        # Orig - last layer\n        filters = ((3, 16), (3, 32), (3, 64), (3, 96), (3, 128),\n                   (3, 196))[:num_levels]\n      else:\n        filters = ((level1_num_layers, level1_num_filters), (3, 32), (3, 32),\n                   (3, 32), (3, 32), (3, 32))[:num_levels]\n    assert filters\n    assert all(len(t) == 2 for t in filters)\n    assert all(t[0] > 0 for t in filters)\n\n    self._leaky_relu_alpha = leaky_relu_alpha\n    self._convs = []\n    self._level1_num_1x1 = level1_num_1x1\n\n    for level, (num_layers, num_filters) in enumerate(filters):\n      group = []\n      for i in range(num_layers):\n        stride = 1\n        if i == 0 or (i == 1 and level == 0 and\n                      pyramid_resolution == 'quarter'):\n          stride = 2\n        conv = Conv2D(\n            int(num_filters * self._channel_multiplier),\n            kernel_size=(3,\n                         3) if level > 0 or i < num_layers - level1_num_1x1 else\n            (1, 1),\n            strides=stride,\n            padding='valid',\n            dtype=self._dtype_policy)\n        group.append(conv)\n      self._convs.append(group)\n          \n  def call(self, x, split_features_by_sample=False):\n    if self._use_bfloat16:\n      x = tf.cast(x, tf.bfloat16)\n    x = x * 2. - 1.  # Rescale input from [0,1] to [-1, 1]\n    features = []\n    for level, conv_tuple in enumerate(self._convs):\n      for i, conv in enumerate(conv_tuple):\n        if level > 0 or i < len(conv_tuple) - self._level1_num_1x1:\n          x = tf.pad(\n              tensor=x,\n              paddings=[[0, 0], [1, 1], [1, 1], [0, 0]],\n              mode='CONSTANT')\n        x = conv(x)\n        x = LeakyReLU(alpha=self._leaky_relu_alpha, dtype=self._dtype_policy)(x)\n      features.append(x)\n\n    if split_features_by_sample:\n\n      # Split the list of features per level (for all samples) into a nested\n      # list that can be indexed by [sample][level].\n\n      n = len(features[0])\n      features = [[f[i:i + 1] for f in features] for i in range(n)]  # pylint: disable=g-complex-comprehension\n\n    return features", "\n"]}
{"filename": "src/uflow_plotting.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nSome plotting functionality for contour tracking inference.\n\"\"\"", "Some plotting functionality for contour tracking inference.\n\"\"\"\n\nimport io\nimport os\nimport time\n\nimport matplotlib\nmatplotlib.use('Agg')  # None-interactive plots do not need tk\nimport matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top", "matplotlib.use('Agg')  # None-interactive plots do not need tk\nimport matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nimport pylab\nfrom tqdm import tqdm\n\nfrom src import uflow_utils  # for resampling the warped image\n", "from src import uflow_utils  # for resampling the warped image\n\n# How much to scale motion magnitude in visualization.\n_FLOW_SCALING_FACTOR = 50.0\n\n# pylint:disable=g-long-lambda\n\n\ndef print_log(log, epoch=None, mean_over_num_steps=1):\n  \"\"\"Print log returned by UFlow.train(...).\"\"\"\n\n  if epoch is None:\n    status = ''\n  else:\n    status = '{} -- '.format(epoch)\n      \n  status += 'total-loss: {:.6f}'.format(\n      np.mean(log['total-loss'][-mean_over_num_steps:]))\n\n  for key in sorted(log):\n    if key not in ['total-loss']:\n      loss_mean = np.mean(log[key][-mean_over_num_steps:])\n      status += ', {}: {:.6f}'.format(key, loss_mean)\n  print(status)", "def print_log(log, epoch=None, mean_over_num_steps=1):\n  \"\"\"Print log returned by UFlow.train(...).\"\"\"\n\n  if epoch is None:\n    status = ''\n  else:\n    status = '{} -- '.format(epoch)\n      \n  status += 'total-loss: {:.6f}'.format(\n      np.mean(log['total-loss'][-mean_over_num_steps:]))\n\n  for key in sorted(log):\n    if key not in ['total-loss']:\n      loss_mean = np.mean(log[key][-mean_over_num_steps:])\n      status += ', {}: {:.6f}'.format(key, loss_mean)\n  print(status)", "\ndef print_eval(eval_dict):\n  \"\"\"Prints eval_dict to console.\"\"\"\n\n  status = ''.join(\n      ['{}: {:.6f}, '.format(key, eval_dict[key]) for key in sorted(eval_dict)])\n  print(status[:-2])\n\n\ndef plot_log(log, plot_dir):\n  plt.figure(1)\n  plt.clf()\n\n  keys = ['total-loss'\n         ] + [key for key in sorted(log) if key not in ['total-loss']]\n  for key in keys:\n    plt.plot(log[key], '--' if key == 'total-loss' else '-', label=key)\n  plt.legend()\n  save_and_close(os.path.join(plot_dir, 'log.png'))", "\ndef plot_log(log, plot_dir):\n  plt.figure(1)\n  plt.clf()\n\n  keys = ['total-loss'\n         ] + [key for key in sorted(log) if key not in ['total-loss']]\n  for key in keys:\n    plt.plot(log[key], '--' if key == 'total-loss' else '-', label=key)\n  plt.legend()\n  save_and_close(os.path.join(plot_dir, 'log.png'))", "\n\ndef log_to_tensorboard(summary_writer, log, saved_offset_dict, epoch=None, mean_over_num_steps=1):\n    with summary_writer.as_default():\n        for key in sorted(log):\n            tf.summary.scalar(key, np.mean(log[key][-mean_over_num_steps:]), step=epoch)\n\n        with tf.name_scope(\"saved_offset_dict\"):\n            for seq_key in saved_offset_dict.keys():\n                for layer_key in saved_offset_dict[seq_key].keys():\n                    a_string_tensor = tf.strings.as_string(saved_offset_dict[seq_key][layer_key])\n                    tf.summary.text(f\"seq_pair {seq_key}, layer {layer_key}:\", a_string_tensor, step=epoch)", "\ndef save_and_close(filename):\n  \"\"\"Save figures.\"\"\"\n\n  # Create a python byte stream into which to write the plot image.\n  buf = io.BytesIO()\n\n  # Save the image into the buffer.\n  plt.savefig(buf, format='png')\n\n  # Seek the buffer back to the beginning, then either write to file or stdout.\n  buf.seek(0)\n  with tf.io.gfile.GFile(filename, 'w') as f:\n    f.write(buf.read(-1))\n  plt.close('all')", "\n\ndef time_data_it(data_it, simulated_train_time_ms=100.0):\n  print('Timing training iterator with simulated train time of {:.2f}ms'.format(\n      simulated_train_time_ms))\n  for i in range(100):\n    start = time.time()\n    _ = data_it.get_next()\n    end = time.time()\n    print(i, 'Time to get one batch (ms):', (end - start) * 1000)\n    if simulated_train_time_ms > 0.0:\n      plt.pause(simulated_train_time_ms / 1000.)", "\n\ndef save_image_as_png(image, filename):\n  image_uint8 = tf.image.convert_image_dtype(image, tf.uint8, saturate=True)\n  image_png = tf.image.encode_png(image_uint8)\n  tf.io.write_file(filename, image_png)\n\n\ndef plot_data(data_it, plot_dir, num_plots):\n  print('Saving images from the dataset to', plot_dir)\n  for i, (image_batch, _) in enumerate(data_it):\n    if i >= num_plots:\n      break\n    for j, image_sequence in enumerate(image_batch):\n      for k, image in enumerate(image_sequence):\n        save_image_as_png(\n            image, os.path.join(plot_dir, '{}_{}_{}.png'.format(i, j, k)))", "def plot_data(data_it, plot_dir, num_plots):\n  print('Saving images from the dataset to', plot_dir)\n  for i, (image_batch, _) in enumerate(data_it):\n    if i >= num_plots:\n      break\n    for j, image_sequence in enumerate(image_batch):\n      for k, image in enumerate(image_sequence):\n        save_image_as_png(\n            image, os.path.join(plot_dir, '{}_{}_{}.png'.format(i, j, k)))\n", "\n\ndef flow_to_rgb(flow):\n  \"\"\"Computes an RGB visualization of a flow field.\"\"\"\n  shape = flow.shape\n  is_graph_mode = False\n  if not isinstance(shape[0], int):  # In graph mode, this is a Dimension object\n    is_graph_mode = True\n    shape = [s.value for s in shape]\n  height, width = [float(s) for s in shape[-3:-1]]\n  scaling = _FLOW_SCALING_FACTOR / (height**2 + width**2)**0.5\n\n  # Compute angles and lengths of motion vectors.\n  if is_graph_mode:\n    motion_angle = tf.atan2(flow[Ellipsis, 1], flow[Ellipsis, 0])\n  else:\n    motion_angle = np.arctan2(flow[Ellipsis, 1], flow[Ellipsis, 0])\n  motion_magnitude = (flow[Ellipsis, 1]**2 + flow[Ellipsis, 0]**2)**0.5\n  # print('motion_angle', motion_angle * (180 / np.pi))\n  # print('motion_magnitude', motion_magnitude)\n\n  # Visualize flow using the HSV color space, where angles are represented by\n  # hue and magnitudes are represented by saturation.\n  if is_graph_mode:\n    flow_hsv = tf.stack([((motion_angle / np.math.pi) + 1.) / 2.,\n                         tf.clip_by_value(motion_magnitude * scaling, 0.0, 1.0),\n                         tf.ones_like(motion_magnitude)],\n                        axis=-1)\n  else:\n    flow_hsv = np.stack([((motion_angle / np.math.pi) + 1.) / 2.,\n                         np.clip(motion_magnitude * scaling, 0.0, 1.0),\n                         np.ones_like(motion_magnitude)],\n                        axis=-1)\n  # Transform colors from HSV to RGB color space for plotting.\n  if is_graph_mode:\n    return tf.image.hsv_to_rgb(flow_hsv)\n  return matplotlib.colors.hsv_to_rgb(flow_hsv)", "\n\n# --------------------------------------------------------------------------------------------\n\ndef save_tracked_contour_indices(plot_dir, cur_index, num_plots, np_all_cur_id_assign):\n    file_path = f\"{plot_dir}/tracked_contour_points.npy\"\n\n    # save previous predicted tracking points\n    if cur_index == 1:\n        saved_tracking_points = np.zeros(shape=(num_plots, np_all_cur_id_assign.shape[0]), dtype=np.int32)\n    else:\n        if os.path.exists(file_path):\n            saved_tracking_points = np.load(file_path)\n\n    saved_tracking_points[cur_index - 1, :] = np_all_cur_id_assign\n    np.save(file_path, saved_tracking_points)", "\n\ndef predict_tracking_plot_manuscript(plot_dir,\n                                      cur_index,\n                                      image1,\n                                      image2,\n                                      seg_point1,\n                                      seg_point2,\n                                      pred_tracking_points,\n                                      num_plots):\n    '''\n    Plot tracking points on the corresponding frame\n    '''\n\n    a_image = image2\n    cm = pylab.get_cmap('gist_rainbow')\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/\"):\n        os.makedirs(f\"{plot_dir}/\")\n    file_path = f\"{plot_dir}/saved_pred_offset_points.npy\"\n\n    # save previous predicted tracking points\n    if cur_index == 1:\n        saved_tracking_points = np.zeros(shape=(num_plots, pred_tracking_points.shape[0], pred_tracking_points.shape[1]), dtype=np.float32)\n    else:\n        if os.path.exists(file_path):\n            saved_tracking_points = np.load(file_path)\n    saved_tracking_points[cur_index - 1, :, :] = pred_tracking_points\n    np.save(file_path, saved_tracking_points)", "\n\ndef predict_tracking_plot(plot_dir,\n                         cur_index,\n                         image1,\n                         image2,\n                         segmentation1,\n                         segmentation2,\n                         seg_point1,\n                         seg_point2,\n                         gt_tracking_points,\n                         pred_tracking_points,\n                         num_plots,\n                         frame_skip=None):\n    '''\n    Plot tracking points on the corresponding frame\n    '''\n    \n    # --------------- Plot Original Image -------------------------\n    # save_fig(plot_dir, cur_index, frame_skip, name='image_rgb', cv2_imwrite_data=image1)\n    # ---------------- Plot Segmentation ------------------\n    # save_fig(plot_dir, cur_index, frame_skip, name='segmentation1', cv2_imwrite_data=segmentation1)\n\n    # ---------------- Plot 2D correlation matrix ----------------\n    # prepare save folder\n    # save_folder_name = 'corr_2d_matrix'\n    # if not os.path.exists(f\"{plot_dir}/{save_folder_name}/\"):\n    #     os.makedirs(f\"{plot_dir}/{save_folder_name}/\")\n    #\n    # cv2.imwrite(f\"{plot_dir}/{save_folder_name}/pred_{cur_index}.png\", np.expand_dims(corr_2d_matrix[0], axis=-1) * 255)\n\n    # --------------- Plot Tracking Points ---------------\n    def plot_tracking_points(plot_dir, num_plots, cur_index, gt_tracking_points, pred_tracking_points, a_image):\n        '''\n        visualize and save moved tracking points\n\n        :return:\n        '''\n        cm = pylab.get_cmap('gist_rainbow')\n\n        # prepare save folder\n        if not os.path.exists(f\"{plot_dir}/\"):\n            os.makedirs(f\"{plot_dir}/\")\n        file_path = f\"{plot_dir}/saved_tracking_points.npy\"\n\n        # save previous predicted tracking points\n        if cur_index == 1:\n            saved_tracking_points = np.zeros(shape=(num_plots, pred_tracking_points.shape[0], pred_tracking_points.shape[1]), dtype=np.int32 )\n        else:\n            if os.path.exists(file_path):\n                saved_tracking_points = np.load(file_path)\n\n        saved_tracking_points[cur_index-1,:,:] = np.round(pred_tracking_points)\n        np.save(file_path, saved_tracking_points)\n\n        MAX_NUM_POINTS = pred_tracking_points.shape[0]\n        # -------------- draw predicted points on an image --------------\n        for point_index, (pred_tracking_point, gt_tracking_point) in enumerate(zip(pred_tracking_points, gt_tracking_points)):\n            pred_col, pred_row = pred_tracking_point\n\n            # TODO: is it ok to ignore these points going outside the image?\n            if pred_col >= 0 and pred_row >= 0 and pred_col < a_image.shape[1] and pred_row < a_image.shape[0]:\n                plt.scatter(x=pred_col, y=pred_row, c=np.array([cm(1. * point_index / MAX_NUM_POINTS)]), s=5)\n\n            gt_col, gt_row = gt_tracking_point\n            plt.scatter(x=gt_col, y=gt_row, s=5, facecolors='none', linewidths=0.5, edgecolors=np.array([cm(1. * point_index / MAX_NUM_POINTS)]))\n            \n        plt.imshow(a_image, cmap='gray')\n        plt.axis('off')\n        plt.savefig(f\"{plot_dir}/pred_{cur_index}.png\", bbox_inches=\"tight\", pad_inches=0)\n        plt.close()\n\n        # -------------- draw GT points on an image --------------\n        # for point_index, gt_tracking_point in enumerate(gt_tracking_points):\n        #     gt_col, gt_row = gt_tracking_point\n        #     plt.scatter(x=gt_col, y=gt_row, s=5, facecolors='none', linewidths=0.5, edgecolors=np.array([cm(1. * point_index / MAX_NUM_POINTS)]))\n        #\n        # plt.imshow(a_image, cmap='gray')\n        # plt.axis('off')\n        # plt.savefig(f\"{plot_dir}/gt_{cur_index}.png\", bbox_inches=\"tight\", pad_inches=0)\n        # plt.close()\n        \n        if cur_index == num_plots:  # e.g) final frame's index is 50\n            # -------------- draw trajectory of tracking points in the last image --------------------\n            plt.imshow(a_image, cmap='gray')\n\n            # draw points from each frame on an image\n            # saved_tracking_points shape is [#frames, #points, 2]\n            for a_point_index in range(saved_tracking_points.shape[1]):\n                col_list = saved_tracking_points[:, a_point_index, 0]\n                row_list = saved_tracking_points[:, a_point_index, 1]\n                plt.plot(col_list, row_list, c=np.array(cm(1. * a_point_index / MAX_NUM_POINTS)), marker='o', linewidth=2, markersize=2)\n\n            plt.savefig(f\"{plot_dir}/trajectory.png\", bbox_inches=\"tight\", pad_inches=0)\n            plt.close()\n\n    # to reduce the number of tracking points\n    if gt_tracking_points.shape[0] > 20:\n        gt_tracking_points = gt_tracking_points[::3, :]\n        pred_tracking_points = pred_tracking_points[::3, :]\n\n    plot_tracking_points(plot_dir, num_plots=num_plots, cur_index=cur_index, gt_tracking_points=gt_tracking_points, pred_tracking_points=pred_tracking_points, a_image=image2)", "\n\ndef predict_plot(plot_dir,\n                index,\n                image1,\n                image2,\n                segmentation1,\n                segmentation2,\n                tracking_point1,\n                tracking_point2,\n                flow_uv,\n                predicted_occlusion,\n                predicted_range_map,\n                forward_warp,\n                forward_valid_warp_mask,\n                num_plots,\n                frame_skip=None):\n  '''\n  Plots rgb image, flow, occlusions, all as separate images.\n  '''\n\n  # --------------- Plot Original Image -------------------------\n  save_fig(plot_dir, index, frame_skip, name='image_rgb', cv2_imwrite_data=image1)\n\n  # --------------- Convert flow map to arrow map --------------\n  flow_uv = -flow_uv[:, :, ::-1]\n  quiver_U = flow_uv[Ellipsis, 0]\n  quiver_V = flow_uv[Ellipsis, 1]\n\n  # maxpooling to make arrows more visible\n  resize_factor = 8\n  reshaped_quiver_U = tf.reshape(quiver_U, (1, quiver_U.shape[0], quiver_U.shape[1], 1))\n  quiver_U = tf.nn.max_pool(reshaped_quiver_U, ksize=resize_factor, strides=resize_factor, padding='VALID')\n  quiver_U = quiver_U[0,:,:,0]\n  reshaped_quiver_V = tf.reshape(quiver_V, (1, quiver_V.shape[0], quiver_V.shape[1], 1))\n  quiver_V = tf.nn.max_pool(reshaped_quiver_V, ksize=resize_factor, strides=resize_factor, padding='VALID')\n  quiver_V = quiver_V[0,:,:,0]\n\n  # set up the figure\n  fig = plt.figure()\n  ax = fig.add_subplot(111)\n  ax.set_aspect('equal')\n  # ax.set_xlim(left=0, right=flow_uv.shape[1] / resize_factor)\n  # ax.set_ylim(bottom=0, top=flow_uv.shape[0] / resize_factor)\n\n  # draw arrows\n  ax.quiver(-1*np.flip(quiver_U,0), np.flip(quiver_V,0), units=\"width\", alpha=0.8)\n  save_fig(plot_dir, index, frame_skip, name='predicted_flow_arrow', fig=fig)\n\n  # --------------------------------------------------------------------------------------\n\n  # --- this is only works if image1 is the segmented image because the canny edge works well on segmented images ---\n  # blurred_image1 = cv2.blur(np.uint8(image1), (7, 7))\n  # edge_image = cv2.Canny(blurred_image1, 0.5, 1, 3, L2gradient=True)  # because the image1's intensity is in range [0, 1]\n  # flow_rgb = flow_to_rgb(flow_uv)\n  # flow_rgb[edge_image>0] = 0\n\n  # ----------------- Plot Optical Flow ------------------\n  flow_rgb = flow_to_rgb(flow_uv)\n  save_fig(plot_dir, index, frame_skip, name='predicted_flow', cv2_imwrite_data=flow_rgb)\n  print(\"unique flow values: \", np.unique(flow_uv))\n\n  # ---------------- Plot Occlusion ---------------------\n  # occluded region is white\n  # save_fig(plot_dir, index, frame_skip, name='predicted_occlusion', cv2_imwrite_data=predicted_occlusion[:, :, 0])\n\n  # ---------------- Plot Range Map ---------------------\n  # save_fig(plot_dir, index, frame_skip, name='predicted_range_map', cv2_imwrite_data=predicted_range_map[:, :, 0])\n\n  # ---------------- Plot Warped image and contour ------------------\n  a_warp = uflow_utils.flow_to_warp_np(flow_uv)\n\n  def save_result_to_npy(a_result, save_path):\n      if index == 1:\n        list_of_result = np.expand_dims(a_result, axis=0)\n      else:\n        list_of_result = np.load(save_path, allow_pickle=True)\n        list_of_result = np.append([a_result], list_of_result, axis=0)\n      np.save(save_path, list_of_result, allow_pickle=True)\n\n  # save warp to plot dense correspondence later\n  warp_path = f\"{plot_dir}/warp_list.npy\"\n  save_result_to_npy(a_warp, warp_path)\n  \n  # Warp Image\n  warped_image1 = uflow_utils.resample_np(image2, a_warp)\n  save_fig(plot_dir, index, frame_skip, name='predicted_warped_image', cv2_imwrite_data=warped_image1.numpy()[0])\n\n  # Warp contour\n  # tf.unique_with_counts(tf.reshape(warped_contour1, -1))\n  warped_contour1 = uflow_utils.resample_np(segmentation2, a_warp)   # uint8 [ 474, 392, 1] -> uint8 [474, 392, 1]\n  warped_contour1 = warped_contour1.numpy()[0]\n  save_fig(plot_dir, index, frame_skip, name='predicted_warped_contour', cv2_imwrite_data=warped_contour1)\n\n  error1 = warped_contour1.astype(np.int32) - segmentation1.astype(np.int32)   # e.g. 127 - 255  or 127 - 0\n  error2 = segmentation1.astype(np.int32) - warped_contour1.astype(np.int32)   # e.g. 255 - 127  or 0 - 127\n\n  # clipping to ignore negative values\n  cliped_error1 = np.clip(error1, 0, 255)\n  cliped_error2 = np.clip(error2, 0, 255)\n  error = cliped_error1 + cliped_error2\n\n  save_fig(plot_dir, index, frame_skip, name='contour_warp_error', cv2_imwrite_data=error.astype(np.uint8))\n\n  # ---------------- Plot Segmentation ------------------\n  save_fig(plot_dir, index, frame_skip, name='segmentation1', cv2_imwrite_data=segmentation1)\n\n  # --------------- Plot Tracking Points ---------------\n  if index>1:\n      tracking_point1=None\n  tracking_points = plot_tracking_points_from_optical_flow(plot_dir, save_name='optical_flow_tracking_points', num_plots=num_plots, cur_index=index, a_optical_flow=flow_uv, a_tracking_point=tracking_point1, a_image=image1)\n\n  # if index == 50:\n    # plot_contour_correspondence_with_color(plot_dir, final_frame_index=index, save_name='correspondence_warped_contour', final_frame_contour=segmentation2[:, :, 0]) # segmentation2 only has one channel\n\n  return warped_contour1, tracking_points", "\n\ndef plot_tracking_points_from_optical_flow(plot_dir, save_name, num_plots, cur_index, a_optical_flow, a_tracking_point, a_image):\n    '''\n    get tracking point from the previous frame\n    move tracking points using the current frame's optical flow\n    save moved tracking point\n\n    :param plot_dir:\n    :param save_name:\n    :param image:\n    :return:\n    '''\n    cm = pylab.get_cmap('gist_rainbow')\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n      os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    # get tracking points from the previous frame\n    if a_tracking_point is None:\n        file_path = f\"{plot_dir}/{save_name}/saved_tracking_points.npy\"\n        if os.path.exists(file_path):\n            loaded_tracking_points = np.load(file_path)\n    else:\n        loaded_tracking_points = np.zeros(shape=(num_plots+1, a_tracking_point.shape[0], a_tracking_point.shape[1]), dtype=np.int32 )\n        loaded_tracking_points[0,:,:] = a_tracking_point\n\n    MAX_NUM_POINTS = loaded_tracking_points.shape[1]\n\n    # move tracking points using the optical flow\n    for point_index, a_tracking_point in enumerate(loaded_tracking_points[cur_index-1]):\n\n        col, row = a_tracking_point\n        col_dif = round(a_optical_flow[row, col, 0])\n        row_dif = round(a_optical_flow[row, col, 1])\n        new_col = col + col_dif\n        new_row = row + row_dif\n\n        # TODO: is it ok to ignore these points going outside the image?\n        # draw points on an image\n        if new_col >= 0 and new_row >= 0 and new_col < a_image.shape[1] and new_row < a_image.shape[0]:\n            plt.scatter(x=new_col, y=new_row, c=np.array([cm(1. * point_index / MAX_NUM_POINTS)]), s=10)\n            loaded_tracking_points[cur_index, point_index, 0] = new_col\n            loaded_tracking_points[cur_index, point_index, 1] = new_row\n\n    plt.imshow(a_image, cmap='gray')\n    plt.axis('off')\n    plt.savefig(f\"{plot_dir}/{save_name}/{cur_index}.png\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close()\n\n    # save moved tracking point\n    np.save(f\"{plot_dir}/{save_name}/saved_tracking_points.npy\", loaded_tracking_points)\n\n    if cur_index == num_plots:  # e.g) final frame's index is 50\n        # -------------- draw trajectory of tracking points in the last image --------------------\n        plt.imshow(a_image, cmap='gray')\n\n        # draw points from each frame on an image\n        for a_frame in loaded_tracking_points:\n            for tracking_point_index, a_tracking_point in enumerate(a_frame):\n                col, row = a_tracking_point\n                plt.plot(col, row, c=np.array(cm(1. * tracking_point_index / MAX_NUM_POINTS)), marker='o', linewidth=2, markersize=2)\n\n        plt.savefig(f\"{plot_dir}/{save_name}/trajectory.png\", bbox_inches=\"tight\", pad_inches=0)\n        plt.close()\n\n    return loaded_tracking_points", "\n\ndef plot_contour_correspondence_with_color(plot_dir, final_frame_index, save_name, final_frame_contour):\n    '''\n    Visualize correspondence between contours in the video by color\n    Using the saved optical flow at each time step, warp the 200th frame contour back to 1st frame contour\n\n    For colormap inspiration, Refer to https://github.com/microsoft/DIF-Net/blob/main/generate.py\n\n    :return:\n    '''\n    # ------------------- Color the contour -----------------------------\n    cm = pylab.get_cmap('gist_rainbow')  # get color map\n\n    y_coords, x_coords = np.where(final_frame_contour > 0)  # get coords of the contour points\n\n    # assign color to each contour point in order\n    NUM_COLORS = len(x_coords)\n    print('Total # of edge pixels', NUM_COLORS)\n\n    color_edge = np.zeros((final_frame_contour.shape[0], final_frame_contour.shape[1], 3), dtype=np.float32)\n    for a_index, (x_coord, y_coord) in enumerate(zip(x_coords, y_coords)):\n        color_edge[y_coord, x_coord, :] = cm(1. * a_index / NUM_COLORS)[:3]\n\n    save_fig(plot_dir, final_frame_index+1, frame_skip=None, name=save_name, cv2_imwrite_data=color_edge)\n\n    # ----------------- Warp the colored contour continuously -------\n    # get list of warps saved in the computer\n    warp_path = f\"{plot_dir}/warp_list.npy\"\n    list_of_warps = np.load(warp_path, allow_pickle=True)\n\n    print('Original Contour: ', np.sum(np.sum(color_edge, axis=-1) > 0), '   Unique Colors:', np.unique(np.reshape(color_edge, (-1, 3)), axis=0).shape[0] - 1 )\n    # i.e. Warps 200th frame to 199th frame and then from 199th frame to 198th frame and so on...\n    for a_index, a_warp in enumerate(list_of_warps):\n        if a_index == 0:\n            warped_contour = uflow_utils.resample_np(color_edge, a_warp)\n        else:\n            warped_contour = uflow_utils.resample_np(warped_contour, a_warp)\n\n        warped_contour = warped_contour.numpy()[0]\n        print('Warped Contour: ', np.sum(np.sum(warped_contour, axis=-1) > 0), '  Unique Colors:', np.unique(np.reshape(warped_contour, (-1, 3)), axis=0).shape[0] - 1 )\n        \n        save_fig(plot_dir, final_frame_index-a_index, frame_skip=None, name=save_name, cv2_imwrite_data=warped_contour)", "\n\ndef save_fig(plot_dir, index, frame_skip, name, fig=None, cv2_imwrite_data=None):\n\n    plot_dir = f\"{plot_dir}/{name}/\"\n    if not os.path.exists(plot_dir):\n      os.makedirs(plot_dir)\n\n    if frame_skip is not None:\n        filename = f\"{str(index)}_{str(frame_skip)}_{name}.png\"\n    else:\n        filename = f\"{str(index)}_{name}.png\"\n    full_path = os.path.join(plot_dir, filename)\n\n    if cv2_imwrite_data is not None:\n        if cv2_imwrite_data.dtype != np.uint8:\n            cv2_imwrite_data = (cv2_imwrite_data*255).astype(np.uint8)\n        cv2_imwrite_data = cv2.cvtColor(cv2_imwrite_data, cv2.COLOR_BGR2RGB)\n        cv2.imwrite(full_path, cv2_imwrite_data)\n\n    else:\n        plt.xticks([])\n        plt.yticks([])\n        if fig is None:\n          fig = plt\n        fig.savefig(full_path, bbox_inches='tight')\n        plt.close('all')", "\n\ndef plot_cum_accuracy_from_first_to_last_frame(plot_dir, list_of_sa_ours, list_of_ca_ours):\n    num_frames = len(list_of_sa_ours)\n    x_range = range(num_frames)\n    np_frames = np.linspace(1, num_frames, num=num_frames)\n\n    # ---------------------------------------------------------------------------------\n    # rebuttal - on phase contrast videos\n    list_of_sa_ours1 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.4, 0.4, 0.8, 0.8, 0.8, 0.8, 0.4, 0.4, 0.4, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.4, 0.4, 0.6]\n    list_of_sa_ours2 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.8, 0.6]\n    list_of_sa_ours3 = [1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 0.6, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2]\n    list_of_sa_ours4 = [1.0, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 0.5714285714285714, 0.7142857142857143, 0.5714285714285714, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.8571428571428571, 1.0, 0.8571428571428571, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.2857142857142857, 0.2857142857142857]\n    list_of_sa_ours = ( np.array(list_of_sa_ours1) + np.array(list_of_sa_ours2) + np.array(list_of_sa_ours3) + np.array(list_of_sa_ours4) ) / 4\n    \n    list_of_ca_ours1 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8, 0.8, 0.8, 0.4, 0.4, 0.6, 0.8, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6]\n    list_of_ca_ours2 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8]\n    list_of_ca_ours3 = [1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.4, 0.4, 0.4, 0.6, 0.4, 0.4, 0.4, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2]\n    list_of_ca_ours4 = [1.0, 1.0, 1.0, 0.8571428571428571, 0.8571428571428571, 0.7142857142857143, 0.7142857142857143, 0.5714285714285714, 0.7142857142857143, 0.5714285714285714, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.8571428571428571, 1.0, 0.8571428571428571, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.5714285714285714, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855, 0.42857142857142855]\n    list_of_ca_ours = ( np.array(list_of_ca_ours1) + np.array(list_of_ca_ours2) + np.array(list_of_ca_ours3) + np.array(list_of_ca_ours4) ) / 4\n\n    list_of_sa_mechanical1 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.2, 0.4, 0.2, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n    list_of_sa_mechanical2 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.6, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.8, 0.8]\n    list_of_sa_mechanical3 = [1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.8, 0.6, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n    list_of_sa_mechanical4 = [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 0.7142857142857143, 0.8571428571428571, 0.8571428571428571, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.8571428571428571, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.5714285714285714, 0.5714285714285714, 0.42857142857142855, 0.42857142857142855]\n    list_of_sa_mechanical = ( np.array(list_of_sa_mechanical1) + np.array(list_of_sa_mechanical2) + np.array(list_of_sa_mechanical3) + np.array(list_of_sa_mechanical4) ) / 4\n    \n    list_of_ca_mechanical1 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.6, 0.2, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.2, 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n    list_of_ca_mechanical2 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]\n    list_of_ca_mechanical3 = [1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.4, 0.4, 0.4, 0.6, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n    list_of_ca_mechanical4 = [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8571428571428571, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.7142857142857143, 0.7142857142857143, 0.8571428571428571, 0.8571428571428571, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.5714285714285714, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.7142857142857143, 0.5714285714285714, 0.7142857142857143, 0.5714285714285714, 0.5714285714285714, 0.7142857142857143, 0.7142857142857143, 0.42857142857142855]\n    list_of_ca_mechanical = ( np.array(list_of_ca_mechanical1) + np.array(list_of_ca_mechanical2) + np.array(list_of_ca_mechanical3) + np.array(list_of_ca_mechanical4) ) / 4\n\n    # list_of_sa_mechanical = [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.8, 0.6, 0.8, 0.6, 0.6, 0.8, 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]\n    # list_of_ca_mechanical = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6, 0.8, 0.6, 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]\n    # ---------------------------------------------------------------------------------\n    # get cumulative accuracy\n    np_cum_sa = np.cumsum(list_of_sa_ours)\n    np_cum_mean_sa = np_cum_sa/np_frames\n\n    np_cum_ca = np.cumsum(list_of_ca_ours)\n    np_cum_mean_ca = np_cum_ca/np_frames\n\n    np_cum_sa_mechanical = np.cumsum(list_of_sa_mechanical)\n    np_cum_mean_sa_mechanical = np_cum_sa_mechanical/np_frames\n\n    np_cum_ca_mechanical = np.cumsum(list_of_ca_mechanical)\n    np_cum_mean_ca_mechanical = np_cum_ca_mechanical/np_frames\n\n    plt.figure(figsize=(6, 2), dpi=300)\n    plt.plot(x_range, np_cum_mean_sa, label='Ours (SA$_{.02}$)', color='lawngreen')\n    plt.plot(x_range, np_cum_mean_ca, label='Ours (CA$_{.01}$)', color='lawngreen', linestyle='dashed')\n    plt.plot(x_range, np_cum_mean_sa_mechanical, label='Mechanical (SA$_{.02}$)', color='darkorange')\n    plt.plot(x_range, np_cum_mean_ca_mechanical, label='Mechanical (CA$_{.01}$)', color='darkorange', linestyle='dashed')\n    plt.xlabel('Frames')\n    plt.ylabel('CMA')\n    plt.legend(loc='lower left')\n    plt.tight_layout()\n    plt.savefig(f'{plot_dir}/cumulative_accuracy.svg')", ""]}
{"filename": "src/data/generic_flow_dataset.py", "chunked_list": ["# coding=utf-8\n# Copyright 2021 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Data loader for \"generic\" optical flow datasets.\"\"\"\n\nimport os", "\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom src.data import data_utils\n# pylint:disable=unused-import\nfrom src.data.data_utils import predict, list_eval_keys\n", "from src.data.data_utils import predict, list_eval_keys\n\n\ndef make_dataset(path,\n                 mode,\n                 shuffle_buffer_size=0,\n                 height=None,\n                 width=None,\n                 resize_gt_flow=True,\n                 gt_flow_shape=None,\n                 seed=41):\n  \"\"\"Make a dataset for training or evaluating Uflow in uflow_main.\n\n  Args:\n    path: string, in the format of 'some/path/dir1,dir2,dir3' to load all files\n      in some/path/dir1, some/path/dir2, and some/path/dir3.\n    mode: string, one of ['train', 'eval', 'test'] to switch between loading\n      training data, evaluation data, and test data, which right now all return\n      the same data.\n    seq_len: int length of sequence to return. Currently only 2 is supported.\n    shuffle_buffer_size: int, size of the shuffle buffer; no shuffling if 0.\n    height: int, height for reshaping the images (only if mode==train)\n    width: int, width for reshaping the images (only if mode==train)\n    resize_gt_flow: bool, indicates if ground truth flow should be resized\n      during traing or not (only relevant for supervised training)\n    gt_flow_shape: list, if not None sets a fixed size for ground truth flow\n      tensor, e.g. [384,512,2]\n    seed: int, controls the shuffling of the data shards.\n\n  Returns:\n    A tf.dataset of image sequences and ground truth flow for training\n    (see parse functions above). The dataset still requires batching\n    and prefetching before using it to make an iterator.\n  \"\"\"\n  if ',' in path:\n    paths = []\n    l = path.split(',')\n    paths.append(l[0])\n    for subpath in l[1:]:\n      subpath_length = len(subpath.split('/'))\n      basedir = '/'.join(l[0].split('/')[:-subpath_length])\n      paths.append(os.path.join(basedir, subpath))\n  else:\n    paths = [path]\n      \n  # Generate list of filenames.\n  # pylint:disable=g-complex-comprehension\n  files = [\n      os.path.join(d, f)\n      for d in paths\n      for f in tf.io.gfile.listdir(d)\n  ]\n\n  if 'train' in mode:\n    rgen = np.random.RandomState(seed=seed)\n    rgen.shuffle(files)\n\n  num_files = len(files)\n\n  ds = tf.data.Dataset.from_tensor_slices(files)\n  if shuffle_buffer_size:\n    ds = ds.shuffle(num_files)\n  # Create a nested dataset.\n  ds = ds.map(tf.data.TFRecordDataset)\n  # Parse each element of the subsequences and unbatch the result\n  # Do interleave rather than flat_map because it is much faster.\n  include_flow = 'optical_flow_sup' in mode\n  include_segmentations = 'segmentations' in mode\n  include_seg_points = 'segmentation_points' in mode\n  include_tracking_points = 'tracking_points' in mode\n  \n  # pylint:disable=g-long-lambda\n  ds = ds.interleave(\n      lambda x: x.map(\n          lambda y: data_utils.parse_data(\n              y,\n              include_flow=include_flow,\n              height=height,\n              width=width,\n              include_segmentations=include_segmentations,\n              include_seg_points=include_seg_points,\n              include_tracking_points=include_tracking_points,\n              resize_gt_flow=resize_gt_flow,\n              gt_flow_shape=gt_flow_shape),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE),  # number of parallel calls is set dynamically based on available CPU\n      cycle_length=min(10, num_files),\n      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n  \n  if shuffle_buffer_size:\n    # Shuffle image pairs.\n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n  # Put repeat after shuffle.\n\n  if 'train' in mode:\n    ds = ds.repeat()  # repeat dataset indefinitely until training ends\n  # Prefetch a number of batches because reading new ones can take much longer\n  # when they are from new files.\n  ds = ds.prefetch(10)\n\n  return ds", ""]}
{"filename": "src/data/data_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\n\"\"\"", "\n\"\"\"\nData loading and evaluation utilities shared across multiple datasets.\n\nSome datasets are very similar, so to prevent code duplication, shared utilities\nare put into this class.\n\nrefer to https://www.tensorflow.org/guide/data for data pipeline\n\"\"\"\n# pylint:disable=g-importing-member", "\"\"\"\n# pylint:disable=g-importing-member\nfrom collections import defaultdict\nimport sys\nimport time\nfrom statistics import mean\n\nimport numpy as np\nimport tensorflow as tf\n", "import tensorflow as tf\n\nfrom src import tracking_utils\nfrom src import uflow_plotting\nfrom src import uflow_utils\nfrom src import metrics\nfrom datetime import datetime\n\n\ndef parse_data(proto,\n               include_flow,\n               height=None,\n               width=None,\n               include_occlusion=False,\n               include_invalid=False,\n               include_segmentations=False,\n               include_seg_points=False,\n               include_tracking_points=False,\n               resize_gt_flow=True,\n               gt_flow_shape=None):\n  \"\"\"\n  Parses data proto and\n  resizes images and tracking points\n\n  Args:\n    proto: path to data proto file\n    include_flow: bool, whether or not to include flow in the output\n    height: int or None height to resize image to\n    width: int or None width to resize image to\n    include_occlusion: bool, whether or not to also return occluded pixels (will\n      throw error if occluded pixels are not present)\n    include_invalid: bool, whether or not to also return invalid pixels (will\n      throw error if invalid pixels are not present)\n    resize_gt_flow: bool, wether or not to resize flow ground truth as the image\n    gt_flow_shape: list, shape of the original ground truth flow (only required\n      to set a fixed ground truth flow shape for tensorflow estimator in case of\n      supervised training at full resolution resize_gt_flow=False)\n\n  # 11/3/2021 modified by Junbong Jang\n  Returns:\n    images, flow: A tuple of (image1, image2), flow\n  \"\"\"\n\n  # Parse context and image sequence from protobuffer.\n  context_features = {\n      'height': tf.io.FixedLenFeature([], tf.int64),\n      'width': tf.io.FixedLenFeature([], tf.int64),\n  }\n  sequence_features = {\n      'images': tf.io.FixedLenSequenceFeature([], tf.string),\n  }\n\n  if include_invalid:\n    sequence_features['invalid_masks'] = tf.io.FixedLenSequenceFeature([], tf.string)\n\n  if include_flow:\n    context_features['flow_uv'] = tf.io.FixedLenFeature([], tf.string)\n\n  if include_occlusion:\n    context_features['occlusion_mask'] = tf.io.FixedLenFeature([], tf.string)\n\n  if include_segmentations:\n    sequence_features['segmentations'] = tf.io.FixedLenSequenceFeature([], tf.string)\n\n  if include_seg_points:\n    sequence_features['segmentation_points'] = tf.io.FixedLenSequenceFeature([], tf.string)\n\n  if include_tracking_points:\n    sequence_features['tracking_points'] = tf.io.FixedLenSequenceFeature([], tf.string)\n\n  context_parsed, sequence_parsed = tf.io.parse_single_sequence_example(\n      proto,\n      context_features=context_features,\n      sequence_features=sequence_features,\n  )\n\n  def deserialize(s, dtype, dims):\n    return tf.reshape(tf.io.decode_raw(s, dtype), [context_parsed['height'], context_parsed['width'], dims])\n\n  def deserialize_points(s, dtype, dims):\n      return tf.reshape(tf.io.decode_raw(s, dtype), [-1, dims])\n\n  images = tf.map_fn(\n      lambda s: deserialize(s, tf.uint8, 3),\n      sequence_parsed['images'],\n      dtype=tf.uint8)\n\n  images = tf.image.convert_image_dtype(images, tf.float32)\n  orig_height = tf.shape(input=images)[1]\n  orig_width = tf.shape(input=images)[2]\n\n  # check if pre-specified size is given\n  if height is None or width is None:\n      # If specified size is not feasible with the model, change it to a feasible resolution (used to be ceil, instead of round)\n      _num_levels = 5\n      divisible_by_num = int(pow(2.0, _num_levels))\n      height = tf.math.round(orig_height / divisible_by_num) * divisible_by_num\n      width = tf.math.round(orig_width / divisible_by_num) * divisible_by_num\n      height = tf.cast(height, 'float32')\n      width = tf.cast(width, 'float32')\n\n  images = uflow_utils.resize(images, height, width, is_flow=False)\n  output = [images]\n\n  if include_flow:\n    flow_uv = deserialize(context_parsed['flow_uv'], tf.float32, 2)\n    flow_uv = flow_uv[Ellipsis, ::-1]\n    if height is not None and width is not None and resize_gt_flow:\n      flow_uv = uflow_utils.resize(flow_uv, height, width, is_flow=True)\n    else:\n      if gt_flow_shape is not None:\n        flow_uv.set_shape(gt_flow_shape)\n    # To be consistent with uflow internals, we flip the ordering of flow.\n    output.append(flow_uv)\n    # create valid mask\n    flow_valid = tf.ones_like(flow_uv[Ellipsis, :1], dtype=tf.float32)\n    output.append(flow_valid)\n\n  if include_occlusion:\n    occlusion_mask = deserialize(context_parsed['occlusion_mask'], tf.uint8, 1)\n    if height is not None and width is not None:\n      occlusion_mask = uflow_utils.resize_uint8(\n          occlusion_mask, height, width)\n    output.append(occlusion_mask)\n\n  if include_invalid:\n    invalid_masks = tf.map_fn(\n        lambda s: deserialize(s, tf.uint8, 1),\n        sequence_parsed['invalid_masks'],\n        dtype=tf.uint8)\n    if height is not None and width is not None:\n      invalid_masks = uflow_utils.resize_uint8(\n          invalid_masks, height, width)\n    output.append(invalid_masks)\n\n  if include_segmentations:\n    segmentations = tf.map_fn(\n        lambda s: deserialize(s, tf.uint8, 1),\n        sequence_parsed['segmentations'],\n        dtype=tf.uint8)\n\n    if height is not None and width is not None:\n      segmentations = uflow_utils.resize_uint8(segmentations, height, width)\n    output.append(segmentations)\n\n  if include_seg_points:\n    seg_points = tf.map_fn(\n        lambda s: deserialize_points(s, tf.int32, 3),\n        sequence_parsed['segmentation_points'],\n        dtype=tf.int32)\n\n    # move segmentation points based on the resized images and segmentation\n    resized_seg_points = tracking_utils.resize_seg_points(orig_height, orig_width, height, width, seg_points)\n    output.append(resized_seg_points)\n\n  if include_tracking_points:\n    tracking_points = tf.map_fn(\n        lambda s: deserialize_points(s, tf.int32, 1),\n        sequence_parsed['tracking_points'],\n        dtype=tf.int32)\n\n    output.append(tracking_points)\n\n  # Only put the output in a list if there are more than one items in there.\n  if len(output) == 1:\n    output = output[0]\n\n  return output", "\ndef parse_data(proto,\n               include_flow,\n               height=None,\n               width=None,\n               include_occlusion=False,\n               include_invalid=False,\n               include_segmentations=False,\n               include_seg_points=False,\n               include_tracking_points=False,\n               resize_gt_flow=True,\n               gt_flow_shape=None):\n  \"\"\"\n  Parses data proto and\n  resizes images and tracking points\n\n  Args:\n    proto: path to data proto file\n    include_flow: bool, whether or not to include flow in the output\n    height: int or None height to resize image to\n    width: int or None width to resize image to\n    include_occlusion: bool, whether or not to also return occluded pixels (will\n      throw error if occluded pixels are not present)\n    include_invalid: bool, whether or not to also return invalid pixels (will\n      throw error if invalid pixels are not present)\n    resize_gt_flow: bool, wether or not to resize flow ground truth as the image\n    gt_flow_shape: list, shape of the original ground truth flow (only required\n      to set a fixed ground truth flow shape for tensorflow estimator in case of\n      supervised training at full resolution resize_gt_flow=False)\n\n  # 11/3/2021 modified by Junbong Jang\n  Returns:\n    images, flow: A tuple of (image1, image2), flow\n  \"\"\"\n\n  # Parse context and image sequence from protobuffer.\n  context_features = {\n      'height': tf.io.FixedLenFeature([], tf.int64),\n      'width': tf.io.FixedLenFeature([], tf.int64),\n  }\n  sequence_features = {\n      'images': tf.io.FixedLenSequenceFeature([], tf.string),\n  }\n\n  if include_invalid:\n    sequence_features['invalid_masks'] = tf.io.FixedLenSequenceFeature([], tf.string)\n\n  if include_flow:\n    context_features['flow_uv'] = tf.io.FixedLenFeature([], tf.string)\n\n  if include_occlusion:\n    context_features['occlusion_mask'] = tf.io.FixedLenFeature([], tf.string)\n\n  if include_segmentations:\n    sequence_features['segmentations'] = tf.io.FixedLenSequenceFeature([], tf.string)\n\n  if include_seg_points:\n    sequence_features['segmentation_points'] = tf.io.FixedLenSequenceFeature([], tf.string)\n\n  if include_tracking_points:\n    sequence_features['tracking_points'] = tf.io.FixedLenSequenceFeature([], tf.string)\n\n  context_parsed, sequence_parsed = tf.io.parse_single_sequence_example(\n      proto,\n      context_features=context_features,\n      sequence_features=sequence_features,\n  )\n\n  def deserialize(s, dtype, dims):\n    return tf.reshape(tf.io.decode_raw(s, dtype), [context_parsed['height'], context_parsed['width'], dims])\n\n  def deserialize_points(s, dtype, dims):\n      return tf.reshape(tf.io.decode_raw(s, dtype), [-1, dims])\n\n  images = tf.map_fn(\n      lambda s: deserialize(s, tf.uint8, 3),\n      sequence_parsed['images'],\n      dtype=tf.uint8)\n\n  images = tf.image.convert_image_dtype(images, tf.float32)\n  orig_height = tf.shape(input=images)[1]\n  orig_width = tf.shape(input=images)[2]\n\n  # check if pre-specified size is given\n  if height is None or width is None:\n      # If specified size is not feasible with the model, change it to a feasible resolution (used to be ceil, instead of round)\n      _num_levels = 5\n      divisible_by_num = int(pow(2.0, _num_levels))\n      height = tf.math.round(orig_height / divisible_by_num) * divisible_by_num\n      width = tf.math.round(orig_width / divisible_by_num) * divisible_by_num\n      height = tf.cast(height, 'float32')\n      width = tf.cast(width, 'float32')\n\n  images = uflow_utils.resize(images, height, width, is_flow=False)\n  output = [images]\n\n  if include_flow:\n    flow_uv = deserialize(context_parsed['flow_uv'], tf.float32, 2)\n    flow_uv = flow_uv[Ellipsis, ::-1]\n    if height is not None and width is not None and resize_gt_flow:\n      flow_uv = uflow_utils.resize(flow_uv, height, width, is_flow=True)\n    else:\n      if gt_flow_shape is not None:\n        flow_uv.set_shape(gt_flow_shape)\n    # To be consistent with uflow internals, we flip the ordering of flow.\n    output.append(flow_uv)\n    # create valid mask\n    flow_valid = tf.ones_like(flow_uv[Ellipsis, :1], dtype=tf.float32)\n    output.append(flow_valid)\n\n  if include_occlusion:\n    occlusion_mask = deserialize(context_parsed['occlusion_mask'], tf.uint8, 1)\n    if height is not None and width is not None:\n      occlusion_mask = uflow_utils.resize_uint8(\n          occlusion_mask, height, width)\n    output.append(occlusion_mask)\n\n  if include_invalid:\n    invalid_masks = tf.map_fn(\n        lambda s: deserialize(s, tf.uint8, 1),\n        sequence_parsed['invalid_masks'],\n        dtype=tf.uint8)\n    if height is not None and width is not None:\n      invalid_masks = uflow_utils.resize_uint8(\n          invalid_masks, height, width)\n    output.append(invalid_masks)\n\n  if include_segmentations:\n    segmentations = tf.map_fn(\n        lambda s: deserialize(s, tf.uint8, 1),\n        sequence_parsed['segmentations'],\n        dtype=tf.uint8)\n\n    if height is not None and width is not None:\n      segmentations = uflow_utils.resize_uint8(segmentations, height, width)\n    output.append(segmentations)\n\n  if include_seg_points:\n    seg_points = tf.map_fn(\n        lambda s: deserialize_points(s, tf.int32, 3),\n        sequence_parsed['segmentation_points'],\n        dtype=tf.int32)\n\n    # move segmentation points based on the resized images and segmentation\n    resized_seg_points = tracking_utils.resize_seg_points(orig_height, orig_width, height, width, seg_points)\n    output.append(resized_seg_points)\n\n  if include_tracking_points:\n    tracking_points = tf.map_fn(\n        lambda s: deserialize_points(s, tf.int32, 1),\n        sequence_parsed['tracking_points'],\n        dtype=tf.int32)\n\n    output.append(tracking_points)\n\n  # Only put the output in a list if there are more than one items in there.\n  if len(output) == 1:\n    output = output[0]\n\n  return output", "\n\ndef compute_f_metrics(mask_prediction, mask_gt, num_thresholds=40):\n  \"\"\"Return a dictionary of the true positives, etc. for two binary masks.\"\"\"\n  results = defaultdict(dict)\n  mask_prediction = tf.cast(mask_prediction, tf.float32)\n  mask_gt = tf.cast(mask_gt, tf.float32)\n  for threshold in np.linspace(0, 1, num_thresholds):\n    mask_thresh = tf.cast(\n        tf.math.greater(mask_prediction, threshold), tf.float32)\n    true_pos = tf.cast(tf.math.count_nonzero(mask_thresh * mask_gt), tf.float32)\n    true_neg = tf.math.count_nonzero((mask_thresh - 1) * (mask_gt - 1))\n    false_pos = tf.cast(\n        tf.math.count_nonzero(mask_thresh * (mask_gt - 1)), tf.float32)\n    false_neg = tf.cast(\n        tf.math.count_nonzero((mask_thresh - 1) * mask_gt), tf.float32)\n    results[threshold]['tp'] = true_pos\n    results[threshold]['fp'] = false_pos\n    results[threshold]['fn'] = false_neg\n    results[threshold]['tn'] = true_neg\n  return results", "\n\ndef get_fmax_and_best_thresh(results):\n  \"\"\"Select which threshold produces the best f1 score.\"\"\"\n  fmax = -1.\n  best_thresh = -1.\n  for thresh, metrics in results.items():\n    precision = metrics['tp'] / (metrics['tp'] + metrics['fp'] + 1e-6)\n    recall = metrics['tp'] / (metrics['tp'] + metrics['fn'] + 1e-6)\n    f1 = 2 * precision * recall / (precision + recall + 1e-6)\n    if f1 > fmax:\n      fmax = f1\n      best_thresh = thresh\n  return fmax, best_thresh", "\n\ndef predict(\n    inference_fn,\n    dataset,\n    height,\n    width,\n    progress_bar=False,\n    plot_dir='',\n    num_plots=0,\n    include_segmentations=True,\n    include_seg_points=True,\n    include_tracking_points=True,\n    evaluate_bool=False\n):\n  \"\"\"inference function for flow.\n\n  Args:\n    inference_fn: An inference function that produces a flow_field from two\n      images, e.g. the infer method of UFlow. \n      please look at infer() function in uflow_net.py\n    dataset: A dataset produced by the method above with for_eval=True.\n    height: int, the height to which the images should be resized for inference.\n    width: int, the width to which the images should be resized for inference.\n    progress_bar: boolean, flag to indicate whether the function should print a\n      progress_bar during evaluaton.\n    plot_dir: string, optional path to a directory in which plots are saved (if\n      num_plots > 0).\n    num_plots: int, maximum number of qualitative results to plot for the\n      evaluation.\n\n  Returns:\n    None\n  \"\"\"\n  eval_start_in_s = time.time()\n  \n  it = tf.compat.v1.data.make_one_shot_iterator(dataset)\n  inference_times = []\n  list_of_sa_at_thresholds = {0.02:[], 0.04:[], 0.06:[]}\n  list_of_ca_at_thresholds = {0.01:[], 0.02:[], 0.03:[]}\n  list_of_rsa = []\n  list_of_ta = []\n  list_of_baseline_f_measure = []\n\n  list_of_points_spatial_accuracy = []\n  list_of_delta_along_normal_vector = []\n\n  list_of_corr_2d_loss = []\n  list_of_matching_points_loss = []\n  list_of_cycle_consistency_assign_loss = []\n  list_of_spatial_points_loss = []\n  list_of_cycle_consistency_spatial_loss = []\n  list_of_forward_matching_points_spatial_metric = []\n  list_of_forward_tracker_photometric_loss = []\n  list_of_backward_tracker_photometric_loss = []\n  list_of_linear_spring_force_loss = []\n  list_of_normal_tendancy_force_loss = []\n\n  plot_count = 0\n  eval_count = -1\n\n  # these three variables are iteratively updated\n  pred_tracking_points = None\n  prev_id_assign = None\n  tracking_pos_emb = None\n\n  for cur_index, test_batch in enumerate(it):\n\n    if progress_bar:\n      sys.stdout.write(':')\n      sys.stdout.flush()\n\n    image_batch = test_batch[0]\n\n    # if len(test_batch) == 1:\n    #     segmentation_batch = test_batch[0]\n    #     tracking_points_batch = test_batch[0]\n    # if len(test_batch) == 2:\n    #     if include_segmentations:\n    #         segmentation_batch = test_batch[1]\n    #         tracking_points_batch = test_batch[1]  # to prevent error in inference_fn\n    #     elif include_tracking_points:\n    #         segmentation_batch = test_batch[0]  # to prevent error in inference_fn\n    #         tracking_points_batch = test_batch[1]\n    # elif len(test_batch) == 3:\n    #     segmentation_batch = test_batch[1]\n    #     tracking_points_batch = test_batch[2]\n\n    if len(test_batch) == 3:\n        segmentation_batch = test_batch[0]\n        seg_points_batch = test_batch[1]\n        tracking_points_batch = test_batch[2]\n    else:\n        raise ValueError('test_batch size is not 3', len(test_batch))\n\n    if evaluate_bool:\n        f = lambda: inference_fn(\n            image_batch[0],\n            image_batch[1],\n            segmentation_batch[0],\n            segmentation_batch[1],\n            tf.expand_dims(seg_points_batch[0], axis=0),\n            tf.expand_dims(seg_points_batch[1], axis=0),\n            tracking_point1= tf.expand_dims(tracking_points_batch[0], axis=0),\n            tracking_pos_emb=tracking_pos_emb,\n            input_height=height,\n            input_width=width,\n            infer_occlusion=True)\n\n    else:\n        # for the first frame, pass the initial tracking points\n        if pred_tracking_points is None:\n            # for debugging inference on tracking points\n            # pred_tracking_points = tf.constant([[[100.0, 100.0], [110.0, 110.0], [120.0, 120.0], [130.0, 130.0], [140.0, 140.0], [150.0, 150.0], [160.0, 160.0]]])\n            pred_tracking_points = tf.expand_dims(tracking_points_batch[0], axis=0)\n\n        f = lambda: inference_fn(\n            image_batch[0],\n            image_batch[1],\n            segmentation_batch[0],\n            segmentation_batch[1],\n            tf.expand_dims(seg_points_batch[0], axis=0),\n            tf.expand_dims(seg_points_batch[1], axis=0),\n            tracking_point1=pred_tracking_points,\n            tracking_pos_emb=tracking_pos_emb,\n            input_height=height,\n            input_width=width,\n            infer_occlusion=True)\n\n    # for contour tracking point\n    inference_time_in_ms, (forward_spatial_offset, backward_spatial_offset, tracking_pos_emb, resized_height, resized_width) = uflow_utils.time_it(f, execute_once_before=eval_count == 1)\n    inference_times.append(inference_time_in_ms)\n\n    # ---------------------- process forward_id_assign by contour length ----------------------\n    prev_seg_points = tf.expand_dims(seg_points_batch[0,:,:2], axis=0)\n    cur_seg_points = tf.expand_dims(seg_points_batch[1,:,:2], axis=0)\n    prev_seg_points_mask = tf.expand_dims(seg_points_batch[0, :, -1], axis=0)\n    cur_seg_points_mask = tf.expand_dims(seg_points_batch[1, :, -1], axis=0)\n\n    # prev_seg_points_limit = tracking_utils.get_first_occurrence_indices(prev_seg_points[:,:,0], -0.1)[0]  # index 0 is fine since batch size is 1\n    # cur_seg_points_limit = tracking_utils.get_first_occurrence_indices(cur_seg_points[:,:,0], -0.1)[0]\n    # forward_id_assign = tracking_utils.fit_id_assignments_to_next_contour(forward_id_assign, tf.expand_dims(seg_points_batch[:,:,-1], axis=-1), cur_seg_points_limit)\n    # backward_id_assign = tracking_utils.fit_id_assignments_to_next_contour(backward_id_assign, tf.expand_dims(seg_points_batch[:,:,-1], axis=-1), prev_seg_points_limit)\n    # ----------------------\n\n    pred_back_spatial_tracking_points = cur_seg_points + backward_spatial_offset\n    pred_spatial_tracking_points = prev_seg_points + forward_spatial_offset\n    # ----------------------- Metrics -----------------------\n    if evaluate_bool:  # evaluation on validation set during training\n        # perform validation for every epoch\n        gt_prev_id_assignments = tf.expand_dims( tracking_points_batch[0], axis=0)\n        gt_cur_id_assignments = tf.expand_dims( tracking_points_batch[1], axis=0)\n\n        # corr_2d_loss = tracking_utils.corr_2d_loss(gt_prev_id_assignments, gt_cur_id_assignments, corr_2d_matrix)\n\n        # matching_points_loss = tracking_utils.matching_contour_points_loss(gt_prev_id_assignments, gt_cur_id_assignments, forward_id_assign)\n\n        # cycle_consistency_assign_loss = tracking_utils.cycle_consistency_assign_loss(forward_id_assign, backward_id_assign, tf.expand_dims(prev_seg_points[:,:,-1], axis=-1),  tf.expand_dims(cur_seg_points[:,:,-1], axis=-1))\n\n        spatial_points_loss = tracking_utils.matching_spatial_points_loss( gt_prev_id_assignments, gt_cur_id_assignments, cur_seg_points, pred_spatial_tracking_points)\n\n        cycle_consistency_spatial_loss = tracking_utils.cycle_consistency_spatial_loss(prev_seg_points, prev_seg_points_mask, cur_seg_points, cur_seg_points_mask, forward_spatial_offset, backward_spatial_offset)\n\n        # forward_matching_points_spatial_metric = tracking_utils.matching_contour_points_spatial_metric(cur_seg_points,\n        #                                                                                                  gt_prev_id_assignments,\n        #                                                                                                  gt_cur_id_assignments, forward_id_assign)\n\n        forward_tracker_photometric_loss = tracking_utils.tracker_unsupervised_photometric_loss(tf.expand_dims(image_batch[0], axis=0), tf.expand_dims(image_batch[1], axis=0), prev_seg_points, prev_seg_points_mask, pred_spatial_tracking_points)\n\n        backward_tracker_photometric_loss = tracking_utils.tracker_unsupervised_photometric_loss(tf.expand_dims(image_batch[1], axis=0), tf.expand_dims(image_batch[0], axis=0), cur_seg_points, cur_seg_points_mask, cur_seg_points + backward_spatial_offset)\n\n        linear_spring_force_loss, normal_tendancy_force_loss = tracking_utils.mechanical_loss_from_offsets(prev_seg_points, prev_seg_points_mask, forward_spatial_offset)\n\n\n        # list_of_corr_2d_loss.append(corr_2d_loss)\n        # list_of_matching_points_loss.append(matching_points_loss)\n        # list_of_cycle_consistency_assign_loss.append(cycle_consistency_assign_loss)\n        list_of_spatial_points_loss.append(spatial_points_loss)\n        list_of_cycle_consistency_spatial_loss.append(cycle_consistency_spatial_loss)\n        # list_of_forward_matching_points_spatial_metric.append(forward_matching_points_spatial_metric)\n        list_of_forward_tracker_photometric_loss.append(forward_tracker_photometric_loss)\n        list_of_backward_tracker_photometric_loss.append(backward_tracker_photometric_loss)\n        list_of_linear_spring_force_loss.append(linear_spring_force_loss)\n        list_of_normal_tendancy_force_loss.append(normal_tendancy_force_loss)\n\n    else:  # prediction on test set after training\n        # convert id_assign to x,y points in image space\n        id_assign1 = tracking_points_batch[0].numpy()\n        id_assign2 = tracking_points_batch[1].numpy()\n        seg_point1 = seg_points_batch[0].numpy()\n        seg_point2 = seg_points_batch[1].numpy()\n        gt_tracking_points = seg_point2[id_assign2[:, 0], :2]  # index 0 to remove the last dimension\n        gt_prev_tracking_points = seg_point1[id_assign1[:, 0], :2]  # index 0 to remove the last dimension\n\n        # Case1: use previous pred_id_assign predicted by the model as the initial points\n        # ------------------------------------------------------------------\n        # old method that get pred_id_assign by ID regression\n        # pred_id_assign = forward_id_assign[0].numpy()\n        # pred_id_assign = np.expand_dims(pred_id_assign, axis=0)\n        # if prev_id_assign is None:  # If this is the first frame, use the GT points in id_assign1\n        #     sampled_pred_id_assign = tracking_utils.sample_data_by_index(np.expand_dims(id_assign1, axis=0), pred_id_assign)\n        # else:\n        #     sampled_pred_id_assign = tracking_utils.sample_data_by_index(np.expand_dims(prev_id_assign, axis=0), pred_id_assign)\n        # ------------------------------------------------------------------\n        # new method that get pred_id_assign from corr_2d_matrix\n        # if prev_id_assign is None:  # If this is the first frame, use the GT points in id_assign1\n        #     sampled_corr_2d_matrix = tracking_utils.sample_data_by_index(np.expand_dims(id_assign1, axis=0), corr_2d_matrix) # (B, num_track_points, num_seg_points2)\n        # else:\n        #     sampled_corr_2d_matrix = tracking_utils.sample_data_by_index(np.expand_dims(prev_id_assign, axis=0), corr_2d_matrix)  # (B, num_track_points, num_seg_points2)\n        # sampled_pred_id_assign = tf.math.argmax(sampled_corr_2d_matrix, axis=-1)  # (B, num_track_points)\n        # sampled_pred_id_assign = np.expand_dims(sampled_pred_id_assign, axis=-1)  # (B, num_track_points, 1)\n        # # ---------------------------------------------\n        # sampled_pred_id_assign = np.round(sampled_pred_id_assign[0]).astype(np.int32)\n        # prev_id_assign = sampled_pred_id_assign\n        #\n        # # remove any big id\n        # if (sampled_pred_id_assign >= seg_point2.shape[0]).any():\n        #     sampled_pred_id_assign[sampled_pred_id_assign >= seg_point2.shape[0]] = seg_point2.shape[0] - 1\n        # pred_tracking_points = seg_point2[sampled_pred_id_assign[:, 0], :2]\n\n        #---------------------------------Case 1 Ends ---------------------------------------------\n        # Case2: use x,y coordinate of tracked point\n\n        # For backward contour tracking\n        # gt_tracking_points = seg_point1[id_assign1[:, 0], :2]  # index 0 to remove the last dimension\n        # gt_prev_tracking_points = seg_point2[id_assign2[:, 0], :2]  # index 0 to remove the last dimension\n        #\n        # if prev_id_assign is None:  # If this is the first frame, use the GT points in id_assign1\n        #     my_pred_tracking_points = tracking_utils.sample_data_by_index(tf.expand_dims(id_assign2[:, 0], axis=0), pred_back_spatial_tracking_points)\n        #     my_overfit_pred_tracking_points = my_pred_tracking_points\n        # else:\n        #     my_pred_tracking_points = tracking_utils.sample_data_by_index(tf.expand_dims(prev_id_assign[:, 0], axis=0), pred_back_spatial_tracking_points)\n        #     my_overfit_pred_tracking_points = tracking_utils.sample_data_by_index( tf.expand_dims(id_assign1[:, 0], axis=0), pred_back_spatial_tracking_points)\n        #\n        # def get_closest_contour_id(my_pred_tracking_points, seg_points):\n        #     # This operation is referred to as phi in the manuscript\n        #     cur_id_assign_list = []\n        #     for a_point_index in range(my_pred_tracking_points.shape[1]):\n        #         temp_contour_points = tf.expand_dims(seg_points[:,:2], axis=0)\n        #         temp_pred_tracking_points = tf.expand_dims(my_pred_tracking_points[:,a_point_index,:], axis=1)\n        #         diff_dist = temp_contour_points - temp_pred_tracking_points\n        #         l2_dist = tf.math.reduce_euclidean_norm(diff_dist, axis=-1)\n        #         cur_id_assign = tf.math.argmin( l2_dist, axis=1)  # shape (num_gt_points, 1) and dtype int32\n        #         cur_id_assign_list.append(cur_id_assign.numpy()[0])\n        #     np_cur_id_assign = np.expand_dims(np.asarray(cur_id_assign_list, dtype=np.int32), axis=-1)\n        #\n        #     return np_cur_id_assign\n        #\n        # # use the closest contour id to get the pred_tracking_points\n        # np_cur_id_assign = get_closest_contour_id(my_pred_tracking_points, seg_point1)\n        # pred_tracking_points = seg_point1[np_cur_id_assign[:, 0], :2]\n        # prev_id_assign = np_cur_id_assign\n        #\n        # np_overfit_cur_id_assign = get_closest_contour_id(my_overfit_pred_tracking_points, seg_point1)\n        # overfit_pred_tracking_points = seg_point1[np_overfit_cur_id_assign[:, 0], :2]\n        #\n        # # Dense offset figure for manuscript\n        # np_all_cur_id_assign = get_closest_contour_id(pred_back_spatial_tracking_points, seg_point1) # shape is (1640, 1)\n\n        # -----------------------------------------------------------------------------------------\n        # For Forward contour tracking\n        if prev_id_assign is None:  # If this is the first frame, use the GT points in id_assign1\n            my_pred_tracking_points = tracking_utils.sample_data_by_index(tf.expand_dims(id_assign1[:, 0], axis=0), pred_spatial_tracking_points)\n            my_overfit_pred_tracking_points = my_pred_tracking_points\n        else:\n            my_pred_tracking_points = tracking_utils.sample_data_by_index(tf.expand_dims(prev_id_assign[:, 0], axis=0), pred_spatial_tracking_points)\n            my_overfit_pred_tracking_points = tracking_utils.sample_data_by_index( tf.expand_dims(id_assign1[:, 0], axis=0), pred_spatial_tracking_points)\n\n        def get_closest_contour_id(my_pred_tracking_points, seg_points):\n            # This operation is referred to as phi in the manuscript\n            cur_id_assign_list = []\n            for a_point_index in range(my_pred_tracking_points.shape[1]):\n                temp_contour_points = tf.expand_dims(seg_points[:,:2], axis=0)\n                temp_pred_tracking_points = tf.expand_dims(my_pred_tracking_points[:,a_point_index,:], axis=1)\n                diff_dist = temp_contour_points - temp_pred_tracking_points\n                l2_dist = tf.math.reduce_euclidean_norm(diff_dist, axis=-1)\n                cur_id_assign = tf.math.argmin( l2_dist, axis=1)  # shape (num_gt_points, 1) and dtype int32\n                cur_id_assign_list.append(cur_id_assign.numpy()[0])\n            np_cur_id_assign = np.expand_dims(np.asarray(cur_id_assign_list, dtype=np.int32), axis=-1)\n\n            return np_cur_id_assign\n\n        # use the closest contour id to get the pred_tracking_points\n        np_cur_id_assign = get_closest_contour_id(my_pred_tracking_points, seg_point2)\n        pred_tracking_points = seg_point2[np_cur_id_assign[:, 0], :2]\n        prev_id_assign = np_cur_id_assign\n\n        np_overfit_cur_id_assign = get_closest_contour_id(my_overfit_pred_tracking_points, seg_point2)\n        overfit_pred_tracking_points = seg_point2[np_overfit_cur_id_assign[:, 0], :2]\n\n        # Dense offset figure for manuscript\n        np_all_cur_id_assign = get_closest_contour_id(pred_spatial_tracking_points, seg_point2) # shape is (1640, 1)\n\n        # ------------------------- get normal vectors -------------------------\n        def get_protrusion_along_normal_vector(prev_contour_points, cur_contour_points, id_assign1, np_cur_id_assign):\n            two_left_contour_points = np.roll(prev_contour_points, shift=-2, axis=0)\n            tangent_vectors = (two_left_contour_points - prev_contour_points) / 2\n\n            normal_vectors = np.stack([-tangent_vectors[:-2, 1], tangent_vectors[:-2, 0]], axis=-1)  # ignore last two indices due to shift above\n            # repeated_normal_vectors = tf.repeat( normal_vectors, pred_offsets.shape[0], axis=0)\n            unit_normal_vectors = normal_vectors / (np.expand_dims(np.linalg.norm(normal_vectors, axis=-1), -1) + 0.0000001)\n            # put back first and last index that were shifted\n            unit_normal_vectors = np.append(unit_normal_vectors, [[0, 0]], axis=0)\n            unit_normal_vectors = np.insert(unit_normal_vectors, 0, [0, 0], axis=0)\n\n            delta_along_normal_vector_list = []\n            for prev_contour_point_index, cur_contour_point_index in zip(id_assign1, np_cur_id_assign):\n                prev_contour_point_index = prev_contour_point_index[0]\n                cur_contour_point_index = cur_contour_point_index[0]\n                cur_contour_point = cur_contour_points[cur_contour_point_index]\n                cur_x, cur_y, cur_mask = cur_contour_point[0], cur_contour_point[1], cur_contour_point[2]\n                prev_contour_point = prev_contour_points[prev_contour_point_index]\n                prev_x, prev_y, prev_mask = prev_contour_point[0], prev_contour_point[1], prev_contour_point[2]\n\n                assert cur_mask == 1\n                assert prev_mask == 1\n\n                delta_x = cur_x - prev_x\n                delta_y = cur_y - prev_y\n\n                delta_along_normal_vector = np.dot([delta_x, delta_y], unit_normal_vectors[prev_contour_point_index])\n                delta_along_normal_vector = np.round(delta_along_normal_vector, decimals=3)\n                delta_along_normal_vector_list.append(delta_along_normal_vector)\n\n            # for each point, get delta\n            return delta_along_normal_vector_list\n\n        delta_along_normal_vector = get_protrusion_along_normal_vector(seg_point1, seg_point2, id_assign1, np_cur_id_assign)\n        if cur_index == 0:\n            list_of_delta_along_normal_vector = delta_along_normal_vector\n        else:\n            list_of_delta_along_normal_vector = list_of_delta_along_normal_vector + delta_along_normal_vector\n        # --------------------------- Case2 Ends -----------------------------------\n        # if (cur_index+2) % 5 == 0:\n        if plot_dir and plot_count < num_plots:\n          plot_count += 1\n          save_dir = f'{plot_dir}/pred_tracking_points'\n\n          uflow_plotting.predict_tracking_plot(save_dir,\n                                               plot_count,\n                                               image_batch[0].numpy(),\n                                               image_batch[1].numpy(),\n                                               segmentation_batch[0].numpy(),\n                                               segmentation_batch[1].numpy(),\n                                               seg_points_batch[0].numpy(),\n                                               seg_points_batch[1].numpy(),\n                                               gt_tracking_points,\n                                               pred_tracking_points,\n                                               num_plots,\n                                               frame_skip=None)\n\n          # Dense correspondence figure for manuscript\n          uflow_plotting.save_tracked_contour_indices(save_dir, plot_count, num_plots, np_all_cur_id_assign[:,0])\n          uflow_plotting.predict_tracking_plot_manuscript(save_dir,\n                                                          plot_count,\n                                                          image_batch[0].numpy(),\n                                                          image_batch[1].numpy(),\n                                                          seg_points_batch[0].numpy(),\n                                                          seg_points_batch[1].numpy(),\n                                                          pred_spatial_tracking_points[0],\n                                                          num_plots)\n\n        # ---------------- Evaluate F Metric, warp error, and spatial accuracy ------------------\n        image_height, image_width = image_batch.shape[1:3]\n\n        for sa_thershold in list_of_sa_at_thresholds.keys():\n          a_spatial_accuracy = metrics.spatial_accuracy(gt_tracking_points, pred_tracking_points, image_width, image_height, thresh=sa_thershold)\n          list_of_sa_at_thresholds[sa_thershold].append(a_spatial_accuracy)\n\n\n        # SA depending on the contour expansion and contraction\n        points_spatial_accuracy = metrics.point_wise_spatial_accuracy(gt_tracking_points, pred_tracking_points, image_width, image_height, thresh=0.02)\n        if cur_index == 0:\n            list_of_points_spatial_accuracy = points_spatial_accuracy.tolist()\n        else:\n            list_of_points_spatial_accuracy = list_of_points_spatial_accuracy + points_spatial_accuracy.tolist()\n\n        if seg_point2[-1,2] == 1:  # entire vector is filled with valid contour points\n            total_contour_length = seg_point2.shape[0]\n        else:\n            total_contour_length = np.where(seg_point2[:,2] == 0)[0][0]\n        \n        for ca_thershold in list_of_ca_at_thresholds.keys():\n          a_contour_accuracy = metrics.contour_accuracy(id_assign2[:, 0], np_cur_id_assign[:, 0], total_contour_length, thresh=ca_thershold)\n          list_of_ca_at_thresholds[ca_thershold].append(a_contour_accuracy)\n\n\n        # rsa_threshold = 0.01\n        # pred_prev_tracking_points = seg_point1[prev_id_assign[:, 0], :2]\n        # a_relative_spatial_accuracy = metrics.relative_spatial_accuracy(gt_tracking_points, overfit_pred_tracking_points, gt_prev_tracking_points, gt_prev_tracking_points, image_width, image_height, rsa_threshold)\n        # list_of_rsa.append(a_relative_spatial_accuracy)\n\n        # a_temporal_accuracy = metrics.temporal_accuracy(gt_tracking_points, pred_tracking_points,\n        #                                                                 gt_prev_tracking_points,\n        #                                                                 pred_prev_tracking_points, image_width,\n        #                                                                 image_height, spatial_accuracy_threshold)\n        # list_of_ta.append(a_temporal_accuracy)\n\n\n        # ------------------------------------- for optical flow -------------------------------------\n        # inference_time_in_ms, (flow, warps, valid_warp_masks, soft_occlusion_mask, range_map) = uflow_utils.time_it(f, execute_once_before=eval_count == 1)\n        # inference_times.append(inference_time_in_ms)\n        #\n        # best_thresh = .5\n        # if plot_dir and plot_count < num_plots:\n        #   plot_count += 1\n        #   mask_thresh = tf.cast(\n        #       tf.math.greater(soft_occlusion_mask, best_thresh), tf.float32)\n        #   warped_contour1, tracking_points = uflow_plotting.predict_plot(plot_dir,\n        #                            plot_count,\n        #                            image_batch[0].numpy(),\n        #                            image_batch[1].numpy(),\n        #                            segmentation_batch[0].numpy(),\n        #                            segmentation_batch[1].numpy(),\n        #                            tracking_points_batch[0].numpy(),\n        #                            tracking_points_batch[1].numpy(),\n        #                            flow.numpy(),\n        #                            1. - mask_thresh.numpy(),\n        #                            range_map.numpy(),\n        #                            warps.numpy(),\n        #                            valid_warp_masks.numpy(),\n        #                            num_plots,\n        #                            frame_skip=None)\n        #\n        #   # ---------------- Evaluate F Metric, warp error, and spatial accuracy ------------------\n        #   f_value, precision, recall = metrics.f_measure(warped_contour1[:,:,0], segmentation_batch[0].numpy()[:,:,0])\n        #   a_warp_error = metrics.get_warp_error(warped_contour1[:,:,0], segmentation_batch[0].numpy()[:,:,0])\n        #   a_spatial_accuracy = metrics.spatial_accuracy(tracking_points_batch[0].numpy(), tracking_points[plot_count], thresh=1)\n        #   list_of_results.append((f_value, precision, recall, np.sum(a_warp_error), a_spatial_accuracy))\n        #\n        #   # ---------------- Evaluate for baseline --------------\n        #   baseline_f_value, baseline_precision, baseline_recall = metrics.f_measure(segmentation_batch[0].numpy()[:,:,0], segmentation_batch[1].numpy()[:,:,0])\n        #   list_of_baseline_f_measure.append((baseline_f_value, baseline_precision, baseline_recall))\n\n  if progress_bar:\n    sys.stdout.write('\\n')\n    sys.stdout.flush()\n\n  if evaluate_bool:\n      # For Training, evaluate on validation\n      eval_stop_in_s = time.time()\n      results = {\n          # 'val_corr_2d_loss': my_tf_round(tf.math.reduce_mean(list_of_corr_2d_loss), 4).numpy(),\n          # 'val_matching_points_loss': my_tf_round(tf.math.reduce_mean(list_of_matching_points_loss), 4).numpy(),\n          # 'val_cycle_consistency_assign_loss': my_tf_round(tf.math.reduce_mean(list_of_cycle_consistency_assign_loss), 4).numpy(),\n          'val_spatial_points_loss': my_tf_round(tf.math.reduce_mean(list_of_spatial_points_loss), 4).numpy(),\n          'val_cycle_consistency_spatial_loss': my_tf_round(tf.math.reduce_mean(list_of_cycle_consistency_spatial_loss), 4).numpy(),\n          # 'val_matching_points_spatial_metric': my_tf_round(tf.math.reduce_mean(list_of_forward_matching_points_spatial_metric), 4).numpy(),\n          'val_forward_tracker_photometric_loss': my_tf_round(tf.math.reduce_mean(list_of_forward_tracker_photometric_loss), 4).numpy(),\n          'val_backward_tracker_photometric_loss': my_tf_round(tf.math.reduce_mean(list_of_backward_tracker_photometric_loss), 4).numpy(),\n          'val_linear_spring_force_loss': my_tf_round(tf.math.reduce_mean(list_of_linear_spring_force_loss), 4).numpy(),\n          'val_normal_tendancy_force_loss': my_tf_round(tf.math.reduce_mean(list_of_normal_tendancy_force_loss), 4).numpy(),\n          # 'val-inf-time(ms)': np.mean(inference_times),\n          # 'val-eval-time(s)': eval_stop_in_s - eval_start_in_s\n      }\n      return results\n\n  else:\n      # For Prediction: Save Evaluation\n      # uflow_plotting.plot_cum_accuracy_from_first_to_last_frame(plot_dir, list_of_sa, list_of_ca)\n\n      save_filename = 'eval_results'\n      print('-----------', save_filename, '-----------')\n      \n      for sa_thershold in list_of_sa_at_thresholds.keys():\n       print(f'Spatial Accuracy_{sa_thershold}: ', round(mean(list_of_sa_at_thresholds[sa_thershold]), 4))\n      # print('Relative Spatial Accuracy: ', round(mean(list_of_rsa), 4))\n      # print('Temporal Accuracy: ', round(mean(list_of_ta), 4))\n      for ca_thershold in list_of_ca_at_thresholds.keys():\n       print(f'Contour Accuracy_{ca_thershold}: ', round(mean(list_of_ca_at_thresholds[ca_thershold]), 4))\n      save_dict = {'spatial_accuracy_02': list_of_sa_at_thresholds[0.02],\n                   'spatial_accuracy_04': list_of_sa_at_thresholds[0.04],\n                   'spatial_accuracy_06': list_of_sa_at_thresholds[0.06],\n                  #  'relative_spatial_accuracy': list_of_rsa,\n                  #  'temporal accuracy': list_of_ta,\n                   'contour_accuracy_01': list_of_ca_at_thresholds[0.01],\n                   'contour_accuracy_02': list_of_ca_at_thresholds[0.02],\n                   'contour_accuracy_03': list_of_ca_at_thresholds[0.03],\n                   'list_of_delta_along_normal_vector': list_of_delta_along_normal_vector,\n                   'list_of_points_spatial_accuracy':list_of_points_spatial_accuracy}\n\n      with open(f'{plot_dir}/{save_filename}.txt', 'w') as f:\n          print(save_dict, file=f)\n\n      # save_eval_results(list_of_results, plot_dir, 'eval_results')\n      # save_eval_results(list_of_baseline_f_measure, plot_dir, 'baseline_eval_results')\n\n      eval_stop_in_s = time.time()\n      print('---------------------------------------')\n      print('inf-time(ms): ', np.mean(inference_times))\n      print('eval-time(s): ', eval_stop_in_s - eval_start_in_s)", "\n\ndef my_tf_round(x, decimals=0):\n  multiplier = tf.constant(10 ** decimals, dtype=x.dtype)\n  return tf.round(x * multiplier) / multiplier\n\n\ndef save_eval_results(list_of_results, save_dir, save_filename):\n    list_of_f = [a_tuple[0] for a_tuple in list_of_results]\n    list_of_precision = [a_tuple[1] for a_tuple in list_of_results]\n    list_of_recall = [a_tuple[2] for a_tuple in list_of_results]\n\n    print('-----------', save_filename, '-----------')\n    print('F: ', round(mean(list_of_f), 4))\n    print('Precision: ', round(mean(list_of_precision), 4))\n    print('Recall: ', round(mean(list_of_recall), 4))\n    save_dict = {'f': list_of_f, 'precision': list_of_precision, 'recall': list_of_recall}\n\n    if len(list_of_results[0]) >= 4:\n        list_of_warp_error = [a_tuple[3] for a_tuple in list_of_results]\n        print('Warp Error: ', round(mean(list_of_warp_error), 0))\n        save_dict['warp_error'] = list_of_warp_error\n\n        if len(list_of_results[0]) >= 5:\n            list_of_spatial_accuracy = [a_tuple[4] for a_tuple in list_of_results]\n            print('Spatial Accuracy: ', round(mean(list_of_spatial_accuracy), 4))\n            save_dict['spatial_accuracy'] = list_of_spatial_accuracy\n\n    with open(f'{save_dir}/{save_filename}.txt', 'w') as f:\n        print(save_dict, file=f)", "\n\ndef list_eval_keys(prefix=''):\n  \"\"\"List the keys of the dictionary returned by the evaluate function.\"\"\"\n  keys = [\n      'EPE', 'ER', 'inf-time(ms)', 'eval-time(s)', 'occl-f-max',\n      'best-occl-thresh'\n  ]\n  if prefix:\n    return [prefix + '-' + k for k in keys]\n  return keys", ""]}
{"filename": "src/preprocessing/MATLAB_tracking_points.py", "chunked_list": ["'''\nAuthor Junbong Jang\nDate: 4/18/2022\n\nLoad protrusion results from MATLAB windowing & Protrusion package\nOverlay protrusion results on the raw image\nCreate the edge correspondence dataset for our AI tracking model\n'''\n\nimport scipy.io", "\nimport scipy.io\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pylab\nfrom tqdm import tqdm\nfrom glob import glob\nimport ast\nimport os\nfrom visualization_utils import visualize_points, visualize_points_in_frame", "import os\nfrom visualization_utils import visualize_points, visualize_points_in_frame\n\nnp.random.seed(10)\n\nGLOBAL_left_offset = 10\nGLOBAL_right_offset = 10\nGLOBAL_total_offset = GLOBAL_left_offset + GLOBAL_right_offset\n\ndef get_image_name(image_path, image_format):\n    return image_path.split('\\\\')[-1].replace(image_format, '')", "\ndef get_image_name(image_path, image_format):\n    return image_path.split('\\\\')[-1].replace(image_format, '')\n\n\ndef show_figure(cur_image, movie_index, title_string):\n    plt.imshow(cur_image, cmap='gray')\n    # plt.axis('off')\n    # plt.gca().invert_yaxis()\n    plt.title(f'frame {movie_index}. {title_string}')\n    plt.show()\n    # plt.savefig(f\"{save_path}/{image_name}.png\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close()", "\n\ndef save_figure(cur_image, movie_index, title_string, save_path):\n    plt.imshow(cur_image, cmap='gray')\n    plt.title(f'frame {movie_index}. {title_string}')\n    plt.savefig(f\"{save_path}/{movie_index}_{title_string}.png\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close()\n\n\ndef move_prev_tracked_point_and_align_to_smoothEdge(cur_smoothedEdge, next_smoothedEdge, cur_protrusion, cur_smoothedEdge_index):\n    # move the previous tracked points\n    x, y = cur_smoothedEdge[cur_smoothedEdge_index, :]\n    u, v = cur_protrusion[cur_smoothedEdge_index, :]\n    new_x = x + u\n    new_y = y + v\n\n    # find each moved point with the closest smoothedEdge point in the current frame\n    min_dist = 185808  # cur_image.shape[0] * cur_image.shape[1]\n    min_index = None\n    for next_smoothedEdge_i, (x, y) in enumerate(next_smoothedEdge):\n        dist = (new_x - x) ** 2 + (new_y - y) ** 2\n        if dist < min_dist:\n            min_index = next_smoothedEdge_i\n            min_dist = dist\n\n    return min_index", "\ndef move_prev_tracked_point_and_align_to_smoothEdge(cur_smoothedEdge, next_smoothedEdge, cur_protrusion, cur_smoothedEdge_index):\n    # move the previous tracked points\n    x, y = cur_smoothedEdge[cur_smoothedEdge_index, :]\n    u, v = cur_protrusion[cur_smoothedEdge_index, :]\n    new_x = x + u\n    new_y = y + v\n\n    # find each moved point with the closest smoothedEdge point in the current frame\n    min_dist = 185808  # cur_image.shape[0] * cur_image.shape[1]\n    min_index = None\n    for next_smoothedEdge_i, (x, y) in enumerate(next_smoothedEdge):\n        dist = (new_x - x) ** 2 + (new_y - y) ** 2\n        if dist < min_dist:\n            min_index = next_smoothedEdge_i\n            min_dist = dist\n\n    return min_index", "\n\ndef visualize_MATLAB_protrusion_vectors(cur_image, movie_index, rounded_cur_smoothedEdge, cur_protrusion, cur_normal):\n    '''\n    visualize the smoothed edge, protrusion, normal vectors\n\n    :param rounded_cur_smoothedEdge:\n    :param cur_protrusion:\n    :param cur_normal:\n    :return:\n    '''\n    MAX_NUM_POINTS = rounded_cur_smoothedEdge.shape[0]\n\n    for a_index, (x,y) in enumerate(rounded_cur_smoothedEdge):\n        plt.scatter(x=x, y=y, c=np.array([cm(1. * a_index / MAX_NUM_POINTS)]), s=1)\n    show_figure(cur_image, movie_index, 'cur_boundary')\n\n    for a_index, (x,y) in enumerate(rounded_cur_smoothedEdge):\n        u,v = cur_protrusion[a_index, :]\n        plt.quiver(x, y, u, v, angles='xy', scale=15, units=\"width\", width=0.003, color=np.array([cm(1. * a_index / MAX_NUM_POINTS)]))\n    show_figure(cur_image, movie_index, 'protrusion')\n\n    # visualize the normal vector\n    for a_index, (x,y) in enumerate(rounded_cur_smoothedEdge):\n        u,v = cur_normal[a_index, :]\n        plt.quiver(x, y, u, v, angles='xy', scale=15, units=\"width\", width=0.003, color=np.array([cm(1. * a_index / MAX_NUM_POINTS)]))\n    show_figure(cur_image, movie_index, 'normal')\n\n    # move each edge pixel in the smoothedEdge according the protrusion vector\n    for a_index, (x,y) in enumerate(rounded_cur_smoothedEdge):\n        u,v = cur_protrusion[a_index, :]\n        new_x = x + u\n        new_y = y + v\n        new_x = round(new_x)\n        new_y = round(new_y)\n        plt.scatter(x=new_x, y=new_y, c=np.array([cm(1. * a_index / MAX_NUM_POINTS)]), s=1)\n    show_figure(cur_image, movie_index, 'next_boundary')", "\n\n# def visualize_tracked_points(track_id_dict_list, image_path_list, movie_protrusion, movie_smoothedEdge, save_path):\n#     '''\n#     show how each point's coordinate changes throughout the movie\n#     To deal with the expansion, simply start visualization at the frame where expansion happens\n#\n#\n#     :param track_id_dict_list:\n#     :param image_path_list:", "#     :param track_id_dict_list:\n#     :param image_path_list:\n#     :param movie_protrusion:\n#     :param movie_smoothedEdge:\n#     :param save_path:\n#     :return:\n#     '''\n#\n#     # Choose which tracking points to visualize\n#     point_inteval = 10", "#     # Choose which tracking points to visualize\n#     point_inteval = 10\n#     TOTAL_NUM_POINTS = len(track_id_dict_list[0]) // point_inteval\n#     cur_track_id_list = []\n#     for i in range(1, TOTAL_NUM_POINTS+1):\n#         cur_track_id_list.append(i*point_inteval)\n#     next_track_id_list = cur_track_id_list\n#\n#     # Draw each point and its protrusion velocity\n#     tracking_point_dict_trajectory = {}  # {cur_track_id: [x_list, y_list]}", "#     # Draw each point and its protrusion velocity\n#     tracking_point_dict_trajectory = {}  # {cur_track_id: [x_list, y_list]}\n#     for movie_index, a_track_id_dict_list in enumerate(tqdm(track_id_dict_list)):\n#         if movie_index < len(track_id_dict_list)-1:\n#             # At each frame of the movie, get associated rounded coordinate for each smoothEdge index\n#             cur_image = plt.imread(image_path_list[movie_index])\n#             cur_smoothedEdge = movie_smoothedEdge[movie_index][0]\n#             cur_protrusion = movie_protrusion[movie_index][0]\n#\n#             for cur_track_index, cur_track_id in enumerate(cur_track_id_list):", "#\n#             for cur_track_index, cur_track_id in enumerate(cur_track_id_list):\n#                 if movie_index > 0:\n#                     cur_track_id = next_track_id_list[cur_track_index]\n#\n#                 x, y = cur_smoothedEdge[cur_track_id, :]\n#                 u,v = cur_protrusion[cur_track_id, :]\n#                 # plt.quiver(x, y, u, v, angles='xy', scale=15, units=\"width\", width=0.005, color=np.array([cm(1. * cur_track_index / len(cur_track_id_list))]))\n#                 plt.scatter(x=x, y=y, c=np.array([cm(1. * cur_track_index / len(cur_track_id_list) )]), s=2)\n#                 next_track_id_list[cur_track_index] = a_track_id_dict_list[cur_track_id]", "#                 plt.scatter(x=x, y=y, c=np.array([cm(1. * cur_track_index / len(cur_track_id_list) )]), s=2)\n#                 next_track_id_list[cur_track_index] = a_track_id_dict_list[cur_track_id]\n#\n#                 if movie_index == 0:\n#                     tracking_point_dict_trajectory[cur_track_index] = [[x], [y]]\n#                 else:\n#                     tracking_point_dict_trajectory[cur_track_index][0].append(x)\n#                     tracking_point_dict_trajectory[cur_track_index][1].append(y)\n#\n#             save_figure(cur_image, movie_index, 'tracked point', save_path)", "#\n#             save_figure(cur_image, movie_index, 'tracked point', save_path)\n#\n#     # Draw trajectory of all points\n#     for cur_track_index, cur_track_id in enumerate(cur_track_id_list):\n#         plt.plot(tracking_point_dict_trajectory[cur_track_index][0], tracking_point_dict_trajectory[cur_track_index][1],\n#                  c=np.array([cm(1. * cur_track_index / len(cur_track_id_list) )]), marker='o', linestyle='solid', linewidth=1, markersize=1)\n#\n#     save_figure(cur_image, movie_index, 'tracked trajectory', save_path)\n", "#     save_figure(cur_image, movie_index, 'tracked trajectory', save_path)\n\n\ndef save_correspondence_as_tracking_points_per_frame(track_id_dict_list, image_path_list, movie_smoothedEdge, NUM_SAMPLE_POINTS, save_path, visualize_save_path=''):\n    '''\n    show how each point's coordinate changes throughout the movie\n    Side note: to deal with the expansion, simply start visualization at the frame where expansion happens\n\n    :param track_id_dict_list:\n    :param image_path_list:\n    :param movie_smoothedEdge:\n    :param save_path:\n    :return:\n    '''\n\n    if os.path.exists(save_path) is not True:\n        os.makedirs(save_path)\n\n    if visualize_save_path != '':\n        if os.path.exists(visualize_save_path) is not True:\n            os.makedirs(visualize_save_path)\n\n    TOTAL_NUM_POINTS = len(track_id_dict_list[0])\n\n    # for every interval, reset to new tracking points to prevent point convergence\n    MOVIE_INDEX_INTERVAL = 100000   # if you don't want reset, set it to 100000\n\n    for movie_index, a_track_id_dict_list in enumerate(tqdm(track_id_dict_list)):\n        cur_smoothedEdge = movie_smoothedEdge[movie_index][0]\n        contour_points = []  # shape = (Number of Points, 2)\n        if movie_index % MOVIE_INDEX_INTERVAL == 0:\n        # if movie_index == 0:\n            next_track_id_list = []\n            TOTAL_NUM_POINTS = len(track_id_dict_list[movie_index])\n\n            for point_index in range(TOTAL_NUM_POINTS):\n                next_track_id_list.append(track_id_dict_list[movie_index][point_index])\n                x, y = cur_smoothedEdge[point_index, :]\n                contour_points.append([round(x),round(y)])\n\n        else:\n            # At each frame of the movie, get corresponding coordinate for each smoothEdge index\n            for point_index in range(TOTAL_NUM_POINTS):\n                cur_track_id = next_track_id_list[point_index]\n                x, y = cur_smoothedEdge[cur_track_id, :]\n                contour_points.append([round(x),round(y)])\n                next_track_id_list[point_index] = a_track_id_dict_list[cur_track_id]\n\n        contour_points = sample_few_tracking_points(contour_points, NUM_SAMPLE_POINTS)\n        a_image_name = get_image_name(image_path_list[movie_index], image_format)\n        save_coordinates_to_textfile(contour_points, a_image_name, save_path)\n        if visualize_save_path != '':\n            visualize_points_in_frame(image_path_list[movie_index], ordered_contour_points=contour_points, save_path=visualize_save_path, save_name=a_image_name)\n\n\n    # for the last frame\n    contour_points = []\n    movie_index = movie_index + 1\n    cur_smoothedEdge = movie_smoothedEdge[movie_index][0]\n    for point_index in range(TOTAL_NUM_POINTS):\n        cur_track_id = next_track_id_list[point_index]\n        x, y = cur_smoothedEdge[cur_track_id, :]\n        contour_points.append([round(x), round(y)])\n\n    # convert coordinates to text file\n    contour_points = sample_few_tracking_points(contour_points, NUM_SAMPLE_POINTS)\n    a_image_name = get_image_name(image_path_list[movie_index], image_format)\n    save_coordinates_to_textfile(contour_points, a_image_name, save_path)\n    if visualize_save_path != '':\n        visualize_points_in_frame(image_path_list[movie_index], ordered_contour_points=contour_points, save_path=visualize_save_path, save_name=a_image_name)", "\n\ndef sample_few_tracking_points(contour_points, NUM_SAMPLE_POINTS):\n\n    # # sample evenly spaced contour points\n    # total_contour_points = len(contour_points)\n    # point_interval = total_contour_points // (NUM_SAMPLE_POINTS + GLOBAL_total_offset)\n    # sampled_contour_points = []\n    # for a_index, contour_point in enumerate(contour_points):\n    #     if a_index % point_interval == 0:\n    #         sampled_contour_points.append(contour_point)\n    #\n    # # remove first and last contour points\n    # sampled_contour_points = sampled_contour_points[left_offset:-right_offset]\n    sampled_contour_points = contour_points[GLOBAL_left_offset:-GLOBAL_right_offset]\n\n    return sampled_contour_points", "\n    # randomly select points that exceed the number_of_points_to_sample\n    # num_exceed = len(sampled_contour_points) - NUM_SAMPLE_POINTS\n    # if num_exceed > 0:\n    #     exceed_interval = len(sampled_contour_points) // num_exceed\n    #\n    #     reduced_sampled_contour_points = []\n    #     for a_index, contour_point in enumerate(sampled_contour_points):\n    #         if a_index % exceed_interval == (exceed_interval-1) and a_index < (exceed_interval * num_exceed):\n    #             continue", "    #         if a_index % exceed_interval == (exceed_interval-1) and a_index < (exceed_interval * num_exceed):\n    #             continue\n    #         else:\n    #             reduced_sampled_contour_points.append(contour_point)\n    #\n    #     assert len(reduced_sampled_contour_points) == NUM_SAMPLE_POINTS\n    # else:\n    #     reduced_sampled_contour_points = sampled_contour_points\n    #\n    # return reduced_sampled_contour_points", "    #\n    # return reduced_sampled_contour_points\n\n\ndef save_coordinates_to_textfile(contour_points, a_image_name, save_path):\n    save_string = \"\"\n    for a_coordinate in contour_points:\n        # saved as x & y coordinates\n        save_string += str(a_coordinate[0]) + ' ' + str(a_coordinate[1]) + '\\n'\n\n    with open(f'{save_path}/{a_image_name}.txt', 'w') as f:\n        f.write(save_string)", "\n\n\nif __name__ == \"__main__\":\n    # for MARS-Net dataset\n    # root_folder = 'Tracking'\n    # root_path = f'../../../assets/Computer Vision/{root_folder}/'\n    # for dense frames\n    # image_folder = 'img_all'\n    # dataset_folders =  ['040119_PtK1_S01_01_phase', '040119_PtK1_S01_01_phase_ROI2', '040119_PtK1_S01_01_phase_2_DMSO_nd_01', '040119_PtK1_S01_01_phase_3_DMSO_nd_03']\n    # dataset_folders = ['matlab_040119_PtK1_S01_01_phase', 'matlab_040119_PtK1_S01_01_phase_ROI2', 'matlab_040119_PtK1_S01_01_phase_2_DMSO_nd_01','matlab_040119_PtK1_S01_01_phase_2_DMSO_nd_02','matlab_040119_PtK1_S01_01_phase_3_DMSO_nd_03']\n    # ----\n    # for sparse frames\n    # image_folder = 'img'\n    # dataset_folders = ['040119_PtK1_S01_01_phase_sparse', '040119_PtK1_S01_01_phase_ROI2_sparse', '040119_PtK1_S01_01_phase_3_DMSO_nd_03_sparse']\n\n    # -------------------------\n    # for SDC dataset\n    # root_folder = 'HACKS_live'\n    # image_folder = 'images_png'\n    # root_path = f'../../../assets/Computer Vision/{root_folder}/'\n    # dataset_path_list = glob(f\"{root_path}/*\")\n    # convert to folder_list\n    # dataset_folders = []\n    # for dataset_path in dataset_path_list:\n    #     dataset_folders.append( dataset_path.split('\\\\')[-1] )\n    # only GT labeled folders\n    # dataset_folders = ['1_050818_DMSO_09_sparse', '2_052818_S02_none_08_sparse', '3_120217_S02_CK689_50uM_08_sparse',\n    #                    '4_122217_S02_DMSO_04_sparse', '5_120217_S02_CK689_50uM_07_sparse', '6_052818_S02_none_02_sparse',\n    #                    '7_120217_S02_CK689_50uM_13_sparse', '8_TFM-08122012-5_sparse', '9_052818_S02_none_12_sparse']\n    # -------------------------\n    # for jelly dataset\n    root_folder = 'Jellyfish'\n    image_folder = 'cropped'\n    root_path = f'../../../generated/Computer Vision/{root_folder}/'\n    dataset_folders = ['First']\n\n    # -------------------------\n\n    cm = pylab.get_cmap('gist_rainbow')\n\n    for a_folder in dataset_folders:\n        print(a_folder)\n        image_format = '.png'\n        image_path_list = glob(f'{root_path}{a_folder}/{image_folder}/*{image_format}')\n\n        loaded_matlab_data = scipy.io.loadmat(f'{root_path}{a_folder}/WindowingPackage/protrusion/protrusion_vectors.mat')\n\n        # first dimension is column, x\n        # second dimension is row, y\n        movie_protrusion = loaded_matlab_data['protrusion']\n        movie_normals = loaded_matlab_data['normals']\n        movie_smoothedEdge = loaded_matlab_data['smoothedEdge']\n\n        # ------------------------------------- Convert MATLAB into correspondence format --------------------------------------------------------\n        # for every pixels along the current frame's contour (smoothedEdge), find its next position in the next frame's contour\n        # keys: ids of all points\n        # values: [(x1,y1), (x2,y2), ...] which are the (row, column) coordinates of a point for 200 frames\n\n        track_id_dict_list = [{}]\n\n        for movie_index in tqdm(range(movie_smoothedEdge.shape[0] - 1)):\n            cur_image_name = get_image_name(image_path_list[movie_index], image_format)\n            cur_image = plt.imread(image_path_list[movie_index])\n\n            # automatically parse each string array into ndarray\n            cur_protrusion = movie_protrusion[movie_index][0]\n            cur_normal = movie_normals[movie_index][0]\n            cur_smoothedEdge = movie_smoothedEdge[movie_index][0]\n\n            assert cur_smoothedEdge.shape[0] == cur_normal.shape[0] == cur_protrusion.shape[0]\n\n            # -------------------------------------------------------\n            # rounded_cur_smoothedEdge = np.around(cur_smoothedEdge, decimals=0).astype('uint16')\n            # visualize_MATLAB_protrusion_vectors(cur_image, movie_index, rounded_cur_smoothedEdge, cur_protrusion, cur_normal)\n            # ---------------------------------------------------------------\n\n            # initialize contour id to each pixel in the first frame's smoothedEdge\n            next_smoothedEdge = movie_smoothedEdge[movie_index + 1][0]\n            # print('next smoothed edge points: ', next_smoothedEdge.shape[0])\n            if movie_index == 0:\n                for track_id, (x,y) in enumerate(cur_smoothedEdge):\n                    next_track_id = move_prev_tracked_point_and_align_to_smoothEdge(cur_smoothedEdge, next_smoothedEdge, cur_protrusion, track_id)\n                    track_id_dict_list[movie_index][track_id] = next_track_id\n                print('Total tracking contour points: ', len(track_id_dict_list[movie_index]))\n\n            # track the same point no matter where it goes based on protrusion\n            # For each iteration, move the tracking points\n            # find each moved point with the closest smoothedEdge point in the current frame\n            # assign the index of the smoothedEdge to track_id_dict_list such that one tracking id has a list of smoothedEdge index\n            # expansion case: find the smoothedEdge index that does not belong to current track_id_dict_list\n\n            else:\n                track_id_dict_list.append({})\n                for track_id in track_id_dict_list[movie_index-1].values():\n                    next_track_id = move_prev_tracked_point_and_align_to_smoothEdge(cur_smoothedEdge, next_smoothedEdge, cur_protrusion, track_id)\n                    track_id_dict_list[movie_index][track_id] = next_track_id\n\n                # expansion case: find cur_smoothedEdge index that does not belong to current track_id_dict_list\n                tracked_id_list = track_id_dict_list[movie_index].keys()\n                for track_id, (x,y) in enumerate(cur_smoothedEdge):\n                    if track_id not in tracked_id_list:\n                        next_track_id = move_prev_tracked_point_and_align_to_smoothEdge(cur_smoothedEdge, next_smoothedEdge, cur_protrusion,\n                                                                                    track_id)\n                        track_id_dict_list[movie_index][track_id] = next_track_id\n\n                assert len(track_id_dict_list[movie_index]) == cur_smoothedEdge.shape[0]\n\n        # save to create the edge correspondence dataset\n        if os.path.exists(f'{root_path}{a_folder}/matlab_correspondence/') is not True:\n            os.makedirs(f'{root_path}{a_folder}/matlab_correspondence/')\n        for a_dict_index in range(len(track_id_dict_list)):\n            with open(f'{root_path}{a_folder}/matlab_correspondence/{a_dict_index}.txt', 'w') as f:\n                print(track_id_dict_list[a_dict_index], file=f)\n\n        # ----------------------------------------- Save MATLAB tracked points in x,y coordinate and visualize them ---------------------------------------------------------\n\n        # read matlab_correspondence dict\n        track_id_dict_list = []\n        for a_dict_index in range((movie_smoothedEdge.shape[0] - 1)):\n            with open(f'{root_path}{a_folder}/matlab_correspondence/{a_dict_index}.txt', \"r\") as f:\n                contents = f.read()\n                track_id_dict_list.append(ast.literal_eval(contents))\n\n        initial_NUM_SAMPLE_POINTS = movie_smoothedEdge[0][0].shape[0] - GLOBAL_total_offset\n        print('initial_NUM_SAMPLE_POINTS', initial_NUM_SAMPLE_POINTS)\n        save_correspondence_as_tracking_points_per_frame(track_id_dict_list, image_path_list, movie_smoothedEdge,\n                                                         NUM_SAMPLE_POINTS=initial_NUM_SAMPLE_POINTS,\n                                                         save_path=f'../../../generated/Computer Vision/{root_folder}/{a_folder}/MATLAB_tracked_points/',\n                                                         visualize_save_path=f'../../../generated/Computer Vision/{root_folder}/{a_folder}/MATLAB_tracked_points_visualize/')", ""]}
{"filename": "src/preprocessing/main_process_tracking_points.py", "chunked_list": ["'''\nAuthor: Junbong Jang\nDate: 2/7/2022\n\nIt loads PC, HACKS, and Jellyfish videos, GT labels, pseudo-labels from Mechanical model, predictions from various contour tracking algorithms. \nThen, it draws manuscript figures and evaluate models' performance by spatial and contour accuracy.\n'''\n\nimport os\nimport cv2", "import os\nimport cv2\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport pylab\nimport numpy as np\nfrom tqdm import tqdm\nimport shutil\nfrom statistics import mean\nimport matplotlib as mpl", "from statistics import mean\nimport matplotlib as mpl\nimport scipy.io as sio\nimport ast\nfrom matplotlib.colors import LinearSegmentedColormap\n\nfrom visualization_utils import plot_tracking_points, display_image_in_actual_size, get_image_name\nfrom contour_tracking_manuscript_figures import rainbow_contour_pred_only, manuscript_figure1_trajectory, manuscript_figure1, manuscript_figure4_for_jelly, manuscript_figure4, manuscript_figure4_no_GT, manuscript_figure5\n\n\ndef get_ordered_contour_points_from_mask(a_mask):\n    # find contours without approx\n    cnts = cv2.findContours(a_mask * 255, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[-2]\n    # get the max-area contour\n    cnt = sorted(cnts, key=cv2.contourArea)[-1]\n    ordered_contour_points = cnt[:, 0, :]\n\n    return ordered_contour_points", "\n\ndef get_ordered_contour_points_from_mask(a_mask):\n    # find contours without approx\n    cnts = cv2.findContours(a_mask * 255, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[-2]\n    # get the max-area contour\n    cnt = sorted(cnts, key=cv2.contourArea)[-1]\n    ordered_contour_points = cnt[:, 0, :]\n\n    return ordered_contour_points", "\n\ndef remove_points_touching_image_boundary(ordered_contour_points, a_image):\n    remove_point_indices = []\n\n    def is_point_touching_boundary(a_point, a_image):\n        return a_point[0] == 0 or a_point[0] == a_image.shape[1] - 1 or \\\n                a_point[1] == 0 or a_point[1] == a_image.shape[0] - 1\n\n    for point_index, a_point in enumerate(ordered_contour_points):\n        # a_point is x and y coordinates or column and row\n        if is_point_touching_boundary(a_point, a_image):\n            remove_point_indices.append(point_index)\n        elif point_index > 0 and point_index < ordered_contour_points.shape[0]-1:\n            # special case where the point is not but left and right points are touching the image boundary\n            left_point = ordered_contour_points[point_index - 1, :]\n            right_point = ordered_contour_points[point_index + 1, :]\n            if is_point_touching_boundary(left_point, a_image) and is_point_touching_boundary(right_point, a_image):\n                remove_point_indices.append(point_index)\n    processed_ordered_contour_points = np.delete(ordered_contour_points, remove_point_indices, axis=0)\n\n    return processed_ordered_contour_points", "\n\ndef reorder_contour_points(processed_ordered_contour_points, height, width):\n    # get left_anchor_bool\n    leftmost_x = np.amin(processed_ordered_contour_points[:,0])\n    rightmost_x = np.amax(processed_ordered_contour_points[:,0])\n    bottom_y = np.amax(processed_ordered_contour_points[:,1])\n    top_y = np.amin(processed_ordered_contour_points[:,1])\n\n    if leftmost_x == 0:\n        # print('left anchor')\n        # find the index with the least x coordinate (left most point)\n        least_x_index = np.argmin(processed_ordered_contour_points[:,0])\n        # reorder by the x coordinate in increasing/decreasing order\n        processed_ordered_contour_points = np.roll(processed_ordered_contour_points, -least_x_index, axis=0)\n    elif rightmost_x == width-1:\n        # print('right_anchor')\n        max_x_index = np.argmax(processed_ordered_contour_points[:,0])\n        processed_ordered_contour_points = np.roll(processed_ordered_contour_points, -max_x_index, axis=0)\n    elif top_y == 0:\n        # print('top anchor')\n        min_y_index = np.argmin(processed_ordered_contour_points[:,1])\n        processed_ordered_contour_points = np.roll(processed_ordered_contour_points, -min_y_index, axis=0)\n\n    elif bottom_y == height-1:\n        # print('bottom anchor')\n        max_y_index = np.argmax(processed_ordered_contour_points[:,1])\n        processed_ordered_contour_points = np.roll(processed_ordered_contour_points, -max_y_index, axis=0)\n\n    return processed_ordered_contour_points", "\n\ndef save_sampled_tracking_points(contour_points, a_image_name, dataset_folder, save_folder):\n    if not os.path.exists(f\"{root_generated_path}{dataset_folder}/{save_folder}\"):\n        os.mkdir(f\"{root_generated_path}{dataset_folder}/{save_folder}\")\n\n    save_string = \"\"\n    for a_coordinate in contour_points:\n        # saved as x & y coordinates\n        save_string += str(a_coordinate[0]) + ' ' + str(a_coordinate[1]) + '\\n'\n\n    with open(f'{root_generated_path}{dataset_folder}/{save_folder}/{a_image_name}.txt', 'w') as f:\n        f.write(save_string)", "\n\ndef sample_contour_points(root_assets_path, dataset_folder, image_folder, processed_mask_folder, image_format):\n    '''\n    Given a mask, get ordered contour points along the boundary of the segmentation mask, \n    '''\n\n    def plot_points(a_img, ordered_contour_points, unit_normal_list, dataset_folder, save_folder, filename):\n\n        if len(a_img.shape) == 2:\n            three_channel_img = np.repeat(a_img[:, :, np.newaxis], 3, axis=2)\n        fig, ax = display_image_in_actual_size(three_channel_img, cm, blank=False)\n        NUM_COLORS = len(ordered_contour_points)\n        for a_index, a_coord in enumerate(ordered_contour_points):\n            # TODO: is it ok to just ignore it?\n            if a_coord is not None:\n                ax.scatter(x=a_coord[0], y=a_coord[1], c=np.array([cm(1. * a_index / NUM_COLORS)]), s=1)\n        if unit_normal_list is not None:\n            for a_index, (a_coord, a_normal) in enumerate(zip(ordered_contour_points, unit_normal_list)):\n                ax.scatter(x=a_coord[0], y=a_coord[1], c=np.array([cm(1. * a_index / NUM_COLORS)]), s=1)\n                ax.quiver(a_coord[0], a_coord[1], a_normal[0], a_normal[1], angles='xy', scale=15, units=\"width\", width=0.005, color=np.array([cm(1. * a_index / NUM_COLORS)]))\n\n        if not os.path.exists(f\"{root_generated_path}{dataset_folder}/{save_folder}\"):\n            os.mkdir(f\"{root_generated_path}{dataset_folder}/{save_folder}\")\n\n        fig.savefig(f\"{root_generated_path}{dataset_folder}/{save_folder}/{filename}.png\", bbox_inches=\"tight\",\n                    pad_inches=0)\n        plt.close()\n\n    if not os.path.exists(root_generated_path + dataset_folder):\n        os.mkdir(root_generated_path + dataset_folder)\n\n    mask_path_list = sorted(glob(f\"{root_assets_path}/{dataset_folder}/{processed_mask_folder}/*{image_format}\"))\n    img_path_list = sorted(glob(f\"{root_assets_path}/{dataset_folder}/{image_folder}/*{image_format}\"))\n\n    cm = pylab.get_cmap('gist_rainbow')\n    number_of_edge_pixels_list = []\n    total_num_points_list = []\n    for img_index, (mask_path, img_path) in tqdm(enumerate(zip(mask_path_list, img_path_list))):\n        a_image_name = get_image_name(mask_path, image_format )\n        a_image_name = a_image_name.replace('refined_', '')  # to make the name of mask the same as the name of image\n        # a_image_name = a_image_name[-3:]\n\n        assert a_image_name == get_image_name(img_path, image_format )\n\n        a_img = plt.imread(img_path)\n        a_mask = plt.imread(mask_path).astype('uint8')\n        # sample all points along the contour with order\n        ordered_contour_points = get_ordered_contour_points_from_mask(a_mask)\n        ordered_contour_points = reorder_contour_points(ordered_contour_points, height=a_mask.shape[0], width=a_mask.shape[1])\n        processed_ordered_contour_points = remove_points_touching_image_boundary(ordered_contour_points, a_mask)\n        total_num_points_list.append(processed_ordered_contour_points.shape[0])\n        plot_points(a_img, processed_ordered_contour_points, None, dataset_folder, save_folder='contour_points_visualize', filename=f\"{a_image_name}\")\n        save_sampled_tracking_points(processed_ordered_contour_points, a_image_name, dataset_folder, save_folder='contour_points')\n\n    total_num_points_array = np.asarray(total_num_points_list)\n    total_num_point_diff_array = total_num_points_array[1:] - total_num_points_array[:-1]\n    print('max contour points:', np.amax(total_num_points_array))\n    print('num_point_diff max', np.amax(total_num_point_diff_array), 'min', np.amin(total_num_point_diff_array))", "\n\ndef convert_GT_tracking_points_to_contour_indices(root_generated_path, dataset_folder):\n    '''\n    Convert the ground truth tracking points in x and y coordinates to contour indices along the boundary of the segmentation mask\n    '''\n    \n    contour_points_path_list = sorted(glob(f\"{root_generated_path}{dataset_folder}/contour_points/*.txt\"))\n\n    # ------------------------------------------------------------------------\n    # GT_tracking_points_path_list = glob(f\"{root_generated_path}{dataset_folder}/MATLAB_tracked_points/*.txt\")  # MATLAB pseudo-labeled GT points\n    GT_tracking_points_path_list = sorted(glob(f\"{root_generated_path}{dataset_folder}/points/*.txt\"))  # my manual GT points\n\n    # if there is no MATLAB pseudo-labels or manual GT labels, create dummy labels\n    if len(GT_tracking_points_path_list) == 0:\n        print('No MATLAB pseudo-labels or manual GT labels. Creating dummy labels.')\n        os.makedirs(f\"{root_generated_path}{dataset_folder}/points/\", exist_ok=True)\n\n        save_string = \"\"\n        for i in range(4):\n            # reorder row & column coordinate to x & y coordinate\n            save_string += '1 7\\n'\n\n        for a_contour_point_path in contour_points_path_list:\n            a_image_name = get_image_name(a_contour_point_path, '.txt')\n            GT_tracking_points_path = f'{root_generated_path}{dataset_folder}/points/{a_image_name}.txt'\n            with open(GT_tracking_points_path, 'w') as f:\n                f.write(save_string)\n\n            GT_tracking_points_path_list.append(GT_tracking_points_path)\n\n    # Make dense frames sparse\n    # GT_tracking_points_path_list = [GT_tracking_points_path_list[0]] + GT_tracking_points_path_list[4::5]\n    # contour_points_path_list = [contour_points_path_list[0]] + contour_points_path_list[4::5]\n    # ------------------------------------------------------------------------\n\n    assert len(GT_tracking_points_path_list) == len(contour_points_path_list)\n\n    for a_index, (a_tracking_points_path, a_contour_points_path) in enumerate(tqdm(zip(GT_tracking_points_path_list, contour_points_path_list))):\n        a_image_name = get_image_name(a_tracking_points_path, '.txt')\n\n        gt_tracking_points = np.loadtxt(a_tracking_points_path)\n        gt_tracking_points = gt_tracking_points.astype('int32')\n\n        contour_points = np.loadtxt(a_contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        # find index of matching contour point for every gt points\n        contour_indices = []\n        for a_gt_point in gt_tracking_points:\n            min_dist = None\n            match_index = None\n            for a_contour_index, a_contour_point in enumerate(contour_points):\n                a_dist = np.linalg.norm(a_gt_point - a_contour_point)\n                if min_dist is None or min_dist > a_dist:\n                    min_dist = a_dist\n                    match_index = a_contour_index\n            contour_indices.append(match_index)\n\n        assert gt_tracking_points.shape[0] == len(contour_indices)\n        sorted_contour_indices = sorted(contour_indices)  # !!! sort indices in ascending order\n\n        save_path = f\"{root_generated_path}{dataset_folder}/tracked_points_in_contour_indices\"\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        save_string = \"\"\n        for a_contour_index in sorted_contour_indices:\n            save_string += str(a_contour_index) + '\\n'\n\n        with open(f'{save_path}/{a_image_name}.txt', 'w') as f:\n            f.write(save_string)", "\n\ndef copy_paste_images_to_generated_path(root_assets_path, root_generated_path, image_folder, image_format, dataset_folder):\n    img_path_list = glob(f\"{root_assets_path}/{dataset_folder}/{image_folder}/*{image_format}\")\n    dst_root_path = f\"{root_generated_path}/{dataset_folder}/images\"\n    os.makedirs(dst_root_path, exist_ok=True)\n\n    for src_img_path in img_path_list:\n        src_img_name = os.path.basename(src_img_path)\n        shutil.copy(src_img_path, f\"{root_generated_path}/{dataset_folder}/images/{src_img_name}\")", "    \n\ndef evaluate_Matlab_tracking_points_on_my_GT_tracking_points(dataset_folder, image_folder, image_format):\n    '''\n    load GT points\n    load contour points\n    load Matlab prediction protrusion\n\n    :param dataset_folder:\n    :return:\n    '''\n\n    Matlab_GT_tracking_points_path_list = sorted(glob(f\"{root_generated_path}{dataset_folder}/MATLAB_tracked_points/*.txt\"))\n    my_GT_tracking_points_path_list = sorted(glob(f\"{root_generated_path}{dataset_folder}/points/*.txt\"))  # my manual GT points\n    contour_points_path_list = sorted(glob(f\"{root_generated_path}{dataset_folder}/contour_points/*.txt\"))\n    img_path_list = sorted(glob(f\"{root_assets_path}/{dataset_folder}/{image_folder}/*{image_format}\"))\n\n    # Matlab_GT_tracking_points_path_list = Matlab_GT_tracking_points_path_list[:41]\n    # my_GT_tracking_points_path_list = my_GT_tracking_points_path_list[:41]\n    # contour_points_path_list = contour_points_path_list[:41]\n    # img_path_list = img_path_list[:41]\n\n    if len(Matlab_GT_tracking_points_path_list) == 41 and len(contour_points_path_list) == 200:\n        contour_points_path_list = [contour_points_path_list[0]] + contour_points_path_list[4::5]\n    assert len(Matlab_GT_tracking_points_path_list) == len(contour_points_path_list)\n    assert len(my_GT_tracking_points_path_list) == 41\n\n    sa_list = []\n    rsa_list = []\n    ta_list = []\n    ca_list = []\n    selected_matlab_GT_tracking_points_indices = []\n    for a_index, (matlab_GT_tracking_points_path, contour_points_path, image_path) in enumerate(zip(Matlab_GT_tracking_points_path_list, contour_points_path_list, img_path_list)):\n        if len(Matlab_GT_tracking_points_path_list) == 200:\n            my_GT_tracking_points_path = my_GT_tracking_points_path_list[(a_index+1)//5]\n        else:\n            my_GT_tracking_points_path = my_GT_tracking_points_path_list[a_index]\n\n        # if len(Matlab_GT_tracking_points_path_list) == 200 and (a_index == 0 or (a_index+1) % 5 == 0):\n        matlab_GT_tracking_points = np.loadtxt(matlab_GT_tracking_points_path)\n        matlab_GT_tracking_points = matlab_GT_tracking_points.astype('int32')\n\n        my_gt_tracking_points = np.loadtxt(my_GT_tracking_points_path)\n        my_gt_tracking_points = my_gt_tracking_points.astype('int32')\n\n        contour_points = np.loadtxt(contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        # -------------------------------------------------------------------------------\n        # Put my_gt_tracking_points along the contour points\n        gt_tracking_points_on_contour_indices = []\n        for my_gt_point in my_gt_tracking_points:\n            min_dist = None\n            match_index = None\n            for index, contour_point in enumerate(contour_points):\n                a_dist = np.linalg.norm(contour_point - my_gt_point)\n                if min_dist is None or min_dist > a_dist:\n                    min_dist = a_dist\n                    match_index = index\n\n            gt_tracking_points_on_contour_indices.append(match_index)\n        assert len(gt_tracking_points_on_contour_indices) == len(my_gt_tracking_points)\n        gt_tracking_points_on_contour = contour_points[gt_tracking_points_on_contour_indices]\n\n        # -------------------------------------------------------------------------------\n        if a_index == 0:\n            # find corresponding index of Matlab tracking points for every GT tracking points\n            for gt_point in gt_tracking_points_on_contour:\n                min_dist = None\n                match_index = None\n                for matlab_index, matlab_gt_point in enumerate(matlab_GT_tracking_points):\n                    a_dist = np.linalg.norm(matlab_gt_point - gt_point)\n                    if min_dist is None or min_dist > a_dist:\n                        min_dist = a_dist\n                        match_index = matlab_index\n\n                selected_matlab_GT_tracking_points_indices.append(match_index)\n            assert len(selected_matlab_GT_tracking_points_indices) == len(gt_tracking_points_on_contour)\n\n        elif a_index > 0:\n            a_image = plt.imread(image_path)\n            image_height = a_image.shape[0]\n            image_width = a_image.shape[1]\n            spatial_accuracy_threshold = 0.02\n            rsa_threshold = 0.01\n            ca_threshold = 0.01\n\n            selected_matlab_GT_tracking_points = matlab_GT_tracking_points[selected_matlab_GT_tracking_points_indices]\n            absolute_sa = metrics.spatial_accuracy(gt_tracking_points_on_contour, selected_matlab_GT_tracking_points, image_width, image_height, spatial_accuracy_threshold)\n            sa_list.append( absolute_sa )\n\n            # get contour points closest to the selected_matlab_GT_tracking_points\n            selected_matlab_contour_indices = []\n            for a_matlab_point in selected_matlab_GT_tracking_points:\n                min_dist = None\n                match_index = None\n                for contour_index, contour_point in enumerate(contour_points):\n                    a_dist = np.linalg.norm(contour_point - a_matlab_point)\n                    if min_dist is None or min_dist > a_dist:\n                        min_dist = a_dist\n                        match_index = contour_index\n                selected_matlab_contour_indices.append(match_index)\n\n            ca_list.append( metrics.contour_accuracy(gt_tracking_points_on_contour_indices, selected_matlab_contour_indices, matlab_GT_tracking_points.shape[0], ca_threshold) )\n\n            # ------------------------------------------------------------------------------\n            # get corresponding matlab indices for prev_gt_tracking_points\n            prev_overfit_selected_matlab_GT_tracking_points_indices = []\n            # find corresponding index of Matlab tracking points for every GT tracking points\n            for gt_point in prev_gt_tracking_points:\n                min_dist = None\n                match_index = None\n                for matlab_index, matlab_gt_point in enumerate(prev_matlab_GT_tracking_points):\n                    a_dist = np.linalg.norm(matlab_gt_point - gt_point)\n                    if min_dist is None or min_dist > a_dist:\n                        min_dist = a_dist\n                        match_index = matlab_index\n\n                prev_overfit_selected_matlab_GT_tracking_points_indices.append(match_index)\n            assert len(prev_overfit_selected_matlab_GT_tracking_points_indices) == len(gt_tracking_points_on_contour)\n            overfit_selected_matlab_GT_tracking_points = matlab_GT_tracking_points[prev_overfit_selected_matlab_GT_tracking_points_indices]\n\n            relative_sa = metrics.relative_spatial_accuracy(gt_tracking_points_on_contour, overfit_selected_matlab_GT_tracking_points, prev_gt_tracking_points, prev_gt_tracking_points, image_width, image_height, rsa_threshold)\n            rsa_list.append( relative_sa )\n\n            prev_selected_matlab_GT_tracking_points = prev_matlab_GT_tracking_points[selected_matlab_GT_tracking_points_indices]\n            ta = metrics.temporal_accuracy(gt_tracking_points_on_contour, selected_matlab_GT_tracking_points, prev_gt_tracking_points, prev_selected_matlab_GT_tracking_points, image_width, image_height, spatial_accuracy_threshold)\n            ta_list.append( ta )\n\n            #----------------------------------------------------------------------------------\n\n            # visualize points\n            plot_tracking_points(f\"{root_generated_path}{dataset_folder}/\", 'matlab_gt_my_gt_compare', a_index, gt_tracking_points_on_contour, selected_matlab_GT_tracking_points, a_image)\n\n        # save previous results for relative spatial accuracy\n        prev_matlab_GT_tracking_points = matlab_GT_tracking_points\n        prev_gt_tracking_points = gt_tracking_points_on_contour\n\n    print('SA: ', round(mean(sa_list), 4))\n    # print('Relative: ', round(mean(rsa_list), 4))\n    # print('Temporal: ', round(mean(ta_list), 4))\n    print('CA: ', round(mean(ca_list), 4))\n\n    print('sa_list', sa_list)\n    print('ca_list', ca_list)\n\n    return sa_list, rsa_list, ta_list, ca_list", "\n\ndef plot_colar_bar(root_generated_path, dataset_folder, image_path, cmap):\n    plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n    save_name = 'colorbar'\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n        os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    gt_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"lightcoral\", \"red\"])\n    pred_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"greenyellow\", \"green\"])\n    matlab_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"skyblue\", \"blue\"])\n    contour_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"dimgrey\", \"snow\"])\n    tracking_point_cmap = pylab.get_cmap('gist_rainbow')\n\n    a_image = plt.imread(image_path)\n    plt.imshow(a_image, cmap=cmap)\n    cbar = plt.colorbar()\n    cbar.set_ticks([])\n\n    plt.savefig(f\"{plot_dir}/{save_name}/hi.svg\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n    plt.close()", "\n\nif __name__ == \"__main__\":\n    dataset_name = 'PC' #PC # HACKS # JELLY \n    image_format = '.png'\n        \n    # for processing SDC dataset\n    if dataset_name == 'HACKS':\n        root_assets_path = \"assets/Computer Vision/HACKS_live/\"\n        root_generated_path = \"generated/Computer Vision/HACKS_live/\"\n        processed_mask_folder = 'masks_png'\n        image_folder = 'images_png'\n\n        # All folders\n        # dataset_path_list = glob(f\"{root_assets_path}/*\")\n        # dataset_folders = []\n        # for dataset_path in dataset_path_list:\n        #     dataset_folders.append( dataset_path.split('\\\\')[-1] )\n\n        # folder container top or bottom anchored images\n        # dataset_folders = ['3_122217_S02_DMSO_09', '3_122217_S02_DMSO_14', '3_122217_S02_DMSO_19', '4_Cell_5', '5_120217_S02_CK689_50uM_03',\n        #                    '7_120217_S02_CK689_50uM_14', '8_120217_S02_CK689_50uM_09', '9_TFM-08122012-3']\n\n        # Dense frames dataset\n        # dataset_folders = ['1_050818_DMSO_09', '2_052818_S02_none_08', '3_120217_S02_CK689_50uM_08',\n        #                    '4_122217_S02_DMSO_04', '5_120217_S02_CK689_50uM_07', '6_052818_S02_none_02',\n        #                    '7_120217_S02_CK689_50uM_13', '8_TFM-08122012-5', '9_052818_S02_none_12']\n\n        # Sparse frames dataset (used this for manuscript)\n        dataset_folders = ['1_050818_DMSO_09_sparse', '2_052818_S02_none_08_sparse', '3_120217_S02_CK689_50uM_08_sparse',\n                           '4_122217_S02_DMSO_04_sparse', '5_120217_S02_CK689_50uM_07_sparse', '6_052818_S02_none_02_sparse',\n                           '7_120217_S02_CK689_50uM_13_sparse', '8_TFM-08122012-5_sparse', '9_052818_S02_none_12_sparse']\n\n        # current setting\n        # dataset_folders = ['2_052818_S02_none_08_sparse']\n    # ---------------------------------------------------------------\n    # for processing MARS-Net phase contrast dataset\n    elif dataset_name == 'PC':\n        root_assets_path = '/data/junbong/optical_flow/assets/data_processing_pc/'\n        root_generated_path = \"/data/junbong/optical_flow/assets/data_processed_pc/\"\n        processed_mask_folder = 'refined_masks/refined_masks_for_channel_1'\n        image_folder = 'img'\n        # dataset_folders =  ['040119_PtK1_S01_01_phase', '040119_PtK1_S01_01_phase_ROI2', '040119_PtK1_S01_01_phase_2_DMSO_nd_01', \"040119_PtK1_S01_01_phase_3_DMSO_nd_03\"]\n        # image_folder = 'img_all'\n\n        # for MATLAB pseudo-label dataset\n        # dataset_folders = ['matlab_040119_PtK1_S01_01_phase', 'matlab_040119_PtK1_S01_01_phase_ROI2', 'matlab_040119_PtK1_S01_01_phase_2_DMSO_nd_01','matlab_040119_PtK1_S01_01_phase_2_DMSO_nd_02','matlab_040119_PtK1_S01_01_phase_3_DMSO_nd_03']\n        # image_folder = 'images'\n\n        # for Sparse frames dataset (used this for manuscript)\n        # dataset_folders = ['040119_PtK1_S01_01_phase_sparse', '040119_PtK1_S01_01_phase_ROI2_sparse', '040119_PtK1_S01_01_phase_2_DMSO_nd_01_sparse', '040119_PtK1_S01_01_phase_3_DMSO_nd_03_sparse']\n        # image_folder = 'img'\n\n        # current setting\n        dataset_folders = [\"040119_PtK1_S01_01_phase_sparse\"]\n    # -----------------------------------------------\n    elif dataset_name == 'JELLY':\n        root_assets_path = 'generated/Computer Vision/Jellyfish/'\n        root_generated_path = \"generated/Computer Vision/Jellyfish/\"\n        image_folder = 'cropped'\n        image_folder = 'cropped_color'\n        processed_mask_folder = 'refined_masks/refined_masks_for_channel_1'\n        \n        dataset_folders = [\"First\"]\n\n    else:\n        raise ValueError('Unknown dataset_name', dataset_name)\n\n    #------------------------------------------------\n    # for sampling contour points and converting GT points to contour indices\n\n    for dataset_folder in dataset_folders:\n        print('dataset', dataset_folder)\n        # --------------------------- Data preprocessing --------------------------------------\n        copy_paste_images_to_generated_path(root_assets_path, root_generated_path, image_folder, image_format, dataset_folder)\n        sample_contour_points(root_assets_path, dataset_folder, image_folder, processed_mask_folder, image_format)\n        convert_GT_tracking_points_to_contour_indices(root_generated_path, dataset_folder)", "\n\n\n\n\n        # --------------------------- Data Loading for manuscript drawing --------------------------------------------\n        # Matlab_GT_tracking_points_path_list = glob(f\"{root_generated_path}{dataset_folder}/MATLAB_tracked_points/*.txt\")\n        # my_GT_tracking_points_path_list = glob(f\"{root_generated_path}{dataset_folder}/points/*.txt\")  # my manual GT points\n        \n        # pred_tracking_points_np = np.load(f\"{root_generated_path}{dataset_folder}/saved_tracking_points.npy\", allow_pickle=True)", "        \n        # pred_tracking_points_np = np.load(f\"{root_generated_path}{dataset_folder}/saved_tracking_points.npy\", allow_pickle=True)\n        # pred_tracking_points_contour_indices = np.load(f\"{root_generated_path}{dataset_folder}/tracked_contour_points.npy\", allow_pickle=True)\n        \n        # contour_points_path_list = glob(f\"{root_generated_path}{dataset_folder}/contour_points/*.txt\")\n        # img_path_list = glob(f\"{root_assets_path}/{dataset_folder}/{image_folder}/*{image_format}\")\n        \n        # if len(contour_points_path_list) == 200:\n        #     contour_points_path_list = [contour_points_path_list[0]] + contour_points_path_list[4::5]\n        #     assert pred_tracking_points_contour_indices.shape[0] == 40", "        #     contour_points_path_list = [contour_points_path_list[0]] + contour_points_path_list[4::5]\n        #     assert pred_tracking_points_contour_indices.shape[0] == 40\n        #     assert pred_tracking_points_np.shape[0] == 40\n        #     assert len(Matlab_GT_tracking_points_path_list) == len(contour_points_path_list)\n        #     assert len(Matlab_GT_tracking_points_path_list) == len(my_GT_tracking_points_path_list)\n        #     assert len(my_GT_tracking_points_path_list) == 41\n        # if  len(contour_points_path_list) == 199:\n        #     contour_points_path_list = [contour_points_path_list[0]] + contour_points_path_list[4::5]\n        # if len(contour_points_path_list) == 40 and pred_tracking_points_contour_indices.shape[0] == 40:\n        #     pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[:-1]", "        # if len(contour_points_path_list) == 40 and pred_tracking_points_contour_indices.shape[0] == 40:\n        #     pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[:-1]\n        # assert len(img_path_list) == len(contour_points_path_list)\n        # assert len(img_path_list) == pred_tracking_points_contour_indices.shape[0] + 1\n\n        # ---------------------------- MATLAB ---------------------\n        # first dimension is column, x\n        # second dimension is row, y\n        # loaded_matlab_data = sio.loadmat(f'{root_generated_path}{dataset_folder}/WindowingPackage/protrusion/protrusion_vectors.mat')\n        # movie_smoothedEdge = loaded_matlab_data['smoothedEdge']", "        # loaded_matlab_data = sio.loadmat(f'{root_generated_path}{dataset_folder}/WindowingPackage/protrusion/protrusion_vectors.mat')\n        # movie_smoothedEdge = loaded_matlab_data['smoothedEdge']\n        \n        # # read matlab_correspondence dict\n        # track_id_dict_list = []\n        # for a_dict_index in range((movie_smoothedEdge.shape[0] - 1)):\n        #     with open(f'{root_generated_path}{dataset_folder}/matlab_correspondence/{a_dict_index}.txt', \"r\") as f:\n        #         contents = f.read()\n        #         track_id_dict_list.append(ast.literal_eval(contents))\n", "        #         track_id_dict_list.append(ast.literal_eval(contents))\n\n        # --------------------------- Data Loading Ends --------------------------------------------\n\n        # --------------------------- Draw Manuscript Figures --------------------------------------------\n        # plot_colar_bar(img_path_list[0])\n        # manuscript_figure4_for_jelly(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices,\n        #                    Matlab_GT_tracking_points_path_list, track_id_dict_list, movie_smoothedEdge, my_GT_tracking_points_path_list, arrow_plot=False )\n        # manuscript_figure4(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices, Matlab_GT_tracking_points_path_list, my_GT_tracking_points_path_list, arrow_plot=False )\n        # manuscript_figure4_no_GT(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices, Matlab_GT_tracking_points_path_list)", "        # manuscript_figure4(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices, Matlab_GT_tracking_points_path_list, my_GT_tracking_points_path_list, arrow_plot=False )\n        # manuscript_figure4_no_GT(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices, Matlab_GT_tracking_points_path_list)\n        # manuscript_figure1(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices)\n        # manuscript_figure1_trajectory(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices)\n        # manuscript_figure5(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices)\n\n        # rainbow_contour_pred_only(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices)\n\n        # --------------------------- For Rebuttal ---------------------------\n        # rebuttal_error_study(root_generated_path)", "        # --------------------------- For Rebuttal ---------------------------\n        # rebuttal_error_study(root_generated_path)\n        # cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [(173/255,255/255,47/255), (0, 50/255,0)], N=6)\n        # plot_colar_bar(root_generated_path, dataset_folder, img_path_list[0], cmap)\n        # rebuttal_labeling_figure(root_generated_path, dataset_folder, img_path_list, contour_points_path_list)\n\n    # for evaluation of Mechanical model on my GT points (used this for manuscript comparison table)\n    # all_sa = []\n    # all_rsa = []\n    # all_ta = []", "    # all_rsa = []\n    # all_ta = []\n    # all_ca = []\n    # for dataset_folder in dataset_folders:\n    #     print('dataset', dataset_folder)\n    #     sa_list, rsa_list, ta_list, ca_list = evaluate_Matlab_tracking_points_on_my_GT_tracking_points(dataset_folder, image_folder, image_format)\n    #     all_sa = all_sa + sa_list\n    #     all_rsa = all_rsa + rsa_list\n    #     all_ta = all_ta + ta_list\n    #     all_ca = all_ca + ca_list", "    #     all_ta = all_ta + ta_list\n    #     all_ca = all_ca + ca_list\n    # print('Average SA: ', round(mean(all_sa), 4))\n    # print('Average RSA: ', round(mean(all_rsa), 4))\n    # print('Average TA: ', round(mean(all_ta), 4))\n    # print('Average CA: ', round(mean(all_ca), 4))\n\n"]}
{"filename": "src/preprocessing/tracking_points_labeling_GUI.py", "chunked_list": ["'''\nAuthor Junbong Jang\nDate: 2/8/2022\n\nRefered to https://realpython.com/python-gui-tkinter/\nThis GUI is for creating the ground truth tracking points, similar to Polygonal Point Set Tracking\n\nA user can click on the image to create the contour points\nand those points are tracked across frames in the movie\n", "and those points are tracked across frames in the movie\n\n'''\nimport tkinter as tk\nfrom PIL import Image, ImageTk, ImageGrab\nimport numpy as np\nfrom glob import glob\nfrom os import path\nimport pyautogui  # essential for getting a correct mouse x,y coordinates\nfrom datetime import datetime", "import pyautogui  # essential for getting a correct mouse x,y coordinates\nfrom datetime import datetime\nimport collections\nimport pylab\nimport os\n\ndef get_image_name(image_path, image_format):\n    return image_path.split('\\\\')[-1].replace(image_format, '')\n\n\nclass LoadMovie:\n    # refered to https://stackoverflow.com/questions/63128011/magnifying-glass-with-resizeable-image-for-tkinter\n    def __init__(self, root_window, left_frame, right_frame, movie_path, save_path):\n        self.save_path = save_path\n        # make a save folder\n        if not os.path.exists(self.save_path):\n            os.mkdir(self.save_path)\n\n        # get images in the folder\n        self.image_format = '.png'\n        self.image_path_list = glob(f\"{movie_path}/*{self.image_format}\")\n        self.movie_frame_num = 1\n\n\n        # set up zoom canvas\n        zoom_canvas_width = 300\n        zoom_canvas_height = 200\n\n        zoom_label = tk.Label(left_frame, text=\"Zoom In\", width=20, height=2, fg=\"red\", font=(\"Helvetica\", 14))\n        zoom_label.pack()\n\n        self.zoom_canvas = tk.Canvas(left_frame, width=zoom_canvas_width, height=zoom_canvas_height, relief=\"solid\", bd=2)\n        self.zoom_canvas.pack()\n\n\n        # set up label\n        self.image_label = tk.Label(right_frame, text=\"Frame Number: 1\", width=20, height=2, fg=\"red\", font=(\"Helvetica\", 14))\n        self.image_label.pack(side='top')\n        # color map for each loaded point\n        self.color_map = pylab.get_cmap('gist_rainbow')\n\n        # ------------------------------- set up main canvas -------------------------------\n        orig_img = Image.open(self.image_path_list[0]).convert('RGB')\n        img_width, img_height = orig_img.width, orig_img.height\n\n        self.canvas_image_id = None\n        canvas_frame = tk.Frame(right_frame, relief=\"solid\", bd=2, width=500, height=400)\n        canvas_frame.pack()\n\n        self.canvas = tk.Canvas(canvas_frame, relief=\"solid\", bd=2, width=700, height=400, scrollregion=(0,0,img_width,img_height))\n        hbar = tk.Scrollbar(canvas_frame, orient=tk.HORIZONTAL)\n        hbar.pack(side='bottom', fill=tk.X)\n        hbar.config(command=self.canvas.xview)\n        vbar = tk.Scrollbar(canvas_frame, orient=tk.VERTICAL)\n        vbar.pack(side='right', fill=tk.Y)\n        vbar.config(command=self.canvas.yview)\n        self.canvas.config(xscrollcommand=hbar.set, yscrollcommand=vbar.set)\n        self.canvas.pack( expand=True, fill=tk.BOTH)\n\n        self.display_image_on_canvas()\n        self.canvas_oval_coords = np.zeros(shape=(img_height, img_width), dtype='uint16') # row, column\n        self.tracking_id_from_oval_id = {}   # new oval id : unique tracking id\n\n        # Bind user action to features\n        # if scrolled, zoom in the image\n        self.zoomcycle = 0\n        self.zimg_id = None\n        self.zoval_id = None\n        self.prev_cursor_x = None\n        self.prev_cursor_y = None\n        self.canvas.bind(\"<MouseWheel>\", self.zoomer)\n        self.canvas.bind(\"<Motion>\", self.crop)\n        self.pix_tolerance = 20  # mouse cursor to the point proximity in pixels\n\n        # If left-clicked, save x,y cursor coordinates\n        self.canvas.bind('<ButtonRelease-1>', self.draw_circle)\n\n        # if right-clicked, remove previously clicked coordinate\n        # self.canvas.bind('<Button-3>', self.remove_circle)\n\n        # if left-clicked moving, a closest tracking point is dragged\n        self.canvas.bind('<B1-Motion>', self.drag_tracking_point)\n\n        # ---------- Setup Label ------------\n        self.tracking_label = tk.Label(right_frame, text=f\"\", width=40, height=2, fg=\"red\", font=(\"Helvetica\", 14))\n        self.tracking_label.pack()\n        self.update_tracking_label()\n        self.load_tracking_points()\n\n        # ---------- Setup buttons ----------\n        paned_window_buttons = tk.PanedWindow(right_frame, relief=\"raised\", bd=2)\n        paned_window_buttons.pack()\n\n        prev_button = tk.Button(paned_window_buttons, text='Prev', width=25, command=self.get_prev_frame, font=(\"Helvetica\", 12))\n        save_button = tk.Button(paned_window_buttons, text='Save', width=25, command=self.save_tracking_points, font=(\"Helvetica\", 12))\n        next_button = tk.Button(paned_window_buttons, text='Next', width=25, command=self.get_next_frame, font=(\"Helvetica\", 12))\n\n        paned_window_buttons.add(prev_button)\n        paned_window_buttons.add(save_button)\n        paned_window_buttons.add(next_button)\n\n        clear_button = tk.Button(right_frame, text='Clear', width=25, command=self.clear_ovals_in_canvas, font=(\"Helvetica\", 12))\n        clear_button.pack()\n\n        root_window.bind('<Left>', self.get_5_prev_frame)\n        root_window.bind('<Right>', self.get_5_next_frame)\n        root_window.bind('<KeyPress>', self.onKeyPress)\n\n\n    def onKeyPress(self, event):\n        print('onKeyPress:', event.char)\n        if event.char == ',':\n            self.get_prev_frame()\n        elif event.char == '.':\n            self.get_next_frame()\n        elif event.char == 'm':\n            self.save_tracking_points()\n\n    def get_canvas_as_image(self, cur_x, cur_y, tol_x, tol_y):\n        x = self.canvas.winfo_rootx() + cur_x\n        y = self.canvas.winfo_rooty() + cur_y\n        self.prev_cursor_x = cur_x\n        self.prev_cursor_y = cur_y\n\n        return ImageGrab.grab().crop((x-tol_x, y-tol_y, x+tol_x, y+tol_y))\n\n    def zoomer(self, event):\n        if (event.delta > 0):\n            if self.zoomcycle != 5: self.zoomcycle += 1\n        elif (event.delta < 0):\n            if self.zoomcycle != 0: self.zoomcycle -= 1\n        self.crop_x_y(event.x, event.y)\n\n    def crop(self, event):\n        if event is not None:\n            self.crop_x_y(event.x, event.y)\n\n    def crop_x_y(self, x, y):\n        if self.zimg_id: self.zoom_canvas.delete(self.zimg_id)\n        if self.zoval_id: self.zoom_canvas.delete(self.zoval_id)\n        if (self.zoomcycle) != 0:\n            if self.zoomcycle == 1:\n                tmp = self.get_canvas_as_image(x,y, 90, 67.5)\n            if self.zoomcycle == 2:\n                tmp = self.get_canvas_as_image(x,y, 60, 45)\n            elif self.zoomcycle == 3:\n                tmp = self.get_canvas_as_image(x,y, 45, 30)\n            elif self.zoomcycle == 4:\n                tmp = self.get_canvas_as_image(x,y, 30, 20)\n            elif self.zoomcycle == 5:\n                tmp = self.get_canvas_as_image(x,y, 15, 10)\n            size = self.zoom_canvas.winfo_width(), self.zoom_canvas.winfo_height()\n\n            self.zimg = ImageTk.PhotoImage(tmp.resize(size))\n            self.zimg_id = self.zoom_canvas.create_image(0, 0, image=self.zimg, anchor='nw')\n            self.zoval_id = self.zoom_canvas.create_oval(self.zoom_canvas.winfo_width()/2-5, self.zoom_canvas.winfo_height()/2-5,\n                                         self.zoom_canvas.winfo_width()/2+5, self.zoom_canvas.winfo_height()/2+5, outline =\"red\", width=1)\n\n    def get_x_y_from_event(self, event):\n        canvas = event.widget\n        x = canvas.canvasx(event.x)  # event.x\n        y = canvas.canvasy(event.y)  # event.y\n\n        return int(x), int(y)\n\n    def draw_circle(self, event):\n        x, y = self.get_x_y_from_event(event)\n\n        closest_oval_location = self.get_closest_oval_location(x, y, self.pix_tolerance)\n\n        if closest_oval_location is None:\n            print('closest oval is not found')\n            new_oval_id = self.draw_circle_x_y(x, y, event)\n            self.tracking_id_from_oval_id[new_oval_id] = new_oval_id\n\n    def draw_circle_x_y(self, x, y, event=None, zoomin=True, outline_color=\"blue\"):\n        print('draw_circle: {}, {}'.format(x, y))\n        if self.canvas_oval_coords[y, x] == 0:\n            x1 = x-5\n            y1 = y-5\n            x2 = x+5\n            y2 = y+5\n            oval_id = self.canvas.create_oval(x1, y1, x2, y2, outline=outline_color, width=2)\n            self.canvas_oval_coords[y, x] = oval_id\n            # update zoom canvas\n            if zoomin:\n                self.zoom_canvas.after(1, self.crop, event)\n\n            return oval_id\n        else:\n            print('Circle is already drawn at {}, {}'.format(x, y))\n            return self.canvas_oval_coords[y, x]  # prev oval_id\n\n    def clip_val(self, val, max):\n        if val < 0:\n            val = 0\n        if val > max:\n            val = max\n        return val\n\n    def get_closest_oval_location(self, x, y, pix_tolerance):\n        # from the current mouse cursor position\n        neighboring_oval_locations = (self.canvas_oval_coords\n                                      [self.clip_val(y - pix_tolerance, self.canvas_oval_coords.shape[0]):\n                                       self.clip_val(y + pix_tolerance, self.canvas_oval_coords.shape[0]),\n                                      self.clip_val(x - pix_tolerance, self.canvas_oval_coords.shape[1]):\n                                      self.clip_val(x + pix_tolerance, self.canvas_oval_coords.shape[1])] > 0)\n        neighboring_oval_locations = np.transpose(neighboring_oval_locations.nonzero())\n\n        min_dist = (pix_tolerance ** 2) * 2 + 1\n        closest_oval_location = None\n        for a_oval_location in neighboring_oval_locations:\n            new_dist = (a_oval_location[0] - pix_tolerance)**2 + (a_oval_location[1] - pix_tolerance)**2\n            if min_dist > new_dist:\n                closest_oval_location = a_oval_location\n                min_dist = new_dist\n\n        return closest_oval_location\n\n    def remove_circle(self, event):\n        x, y = self.get_x_y_from_event(event)\n\n        self.remove_circle_x_y(x, y, event)\n\n    def remove_circle_x_y(self, x, y, event=None):\n        print('remove_circle_x_y: {}, {}'.format(x, y))\n        closest_oval_location = self.get_closest_oval_location(x, y, self.pix_tolerance)\n\n        if closest_oval_location is None:\n            print('closest oval is not found')\n            closest_oval_id = None\n\n        else:\n            pix_tolerance_y = self.pix_tolerance\n            pix_tolerance_x = self.pix_tolerance\n            if y < self.pix_tolerance:\n                pix_tolerance_y = y\n            if x < self.pix_tolerance:\n                pix_tolerance_x = x\n\n            closest_oval_id = self.canvas_oval_coords[y+closest_oval_location[0]-pix_tolerance_y,\n                                                      x+closest_oval_location[1]-pix_tolerance_x ]\n\n            # remove from the canvas\n            self.canvas.delete(closest_oval_id)\n            # remove from the record\n            self.canvas_oval_coords[\n                y + closest_oval_location[0] - pix_tolerance_y, x + closest_oval_location[1] - pix_tolerance_x] = 0\n            # update zoom canvas\n            self.zoom_canvas.after(1, self.crop, event)\n\n        return closest_oval_id\n\n    def clear_ovals_in_canvas(self):\n        neighboring_oval_locations = np.transpose((self.canvas_oval_coords > 0).nonzero())\n\n        for a_oval_location in neighboring_oval_locations:\n            oval_id = self.canvas_oval_coords[a_oval_location[0], a_oval_location[1] ]\n            # remove from the canvas\n            self.canvas.delete(oval_id)\n            # remove from the record\n            self.canvas_oval_coords[ a_oval_location[0], a_oval_location[1]] = 0\n\n    def canvas_oval_coords_to_string(self):\n        '''\n        Save the coordinates to the text file with the following format\n        column_coord row_coord\\n\n\n        :return:\n        '''\n        a_string = \"\"\n        oval_location_dict = {}\n        oval_locations = (self.canvas_oval_coords > 0 ).nonzero()  # np.transpose\n        rows = oval_locations[0]\n        columns = oval_locations[1]\n\n        unique_tracking_ids_visible_in_canvas = []  # for handling removed points\n        for a_coordinate in zip(rows, columns):\n            oval_id = self.canvas_oval_coords[a_coordinate]\n            unique_tracking_id = self.tracking_id_from_oval_id[oval_id]\n            unique_tracking_ids_visible_in_canvas.append(unique_tracking_id)\n            oval_location_dict[unique_tracking_id] = a_coordinate\n\n        # Handle the removed points\n        for oval_id, unique_tracking_id in self.tracking_id_from_oval_id.items():\n            if unique_tracking_id not in unique_tracking_ids_visible_in_canvas:\n                oval_location_dict[unique_tracking_id] = (-1, -1)\n\n        # sort dict by keys\n        od = collections.OrderedDict(sorted(oval_location_dict.items()))\n\n        for k, a_coordinate in od.items():\n            # reorder row & column coordinate to x & y coordinate\n            a_string += str(a_coordinate[1]) + ' ' + str(a_coordinate[0]) + '\\n'\n\n        return a_string\n\n    def drag_tracking_point(self, event):\n        x, y = self.get_x_y_from_event(event)\n        prev_oval_id = self.remove_circle_x_y(x, y, event)\n        if prev_oval_id is not None:\n            unique_tracking_id = self.tracking_id_from_oval_id[prev_oval_id]\n            del self.tracking_id_from_oval_id[prev_oval_id]\n\n            new_oval_id = self.draw_circle_x_y(x, y, event)\n            self.tracking_id_from_oval_id[new_oval_id] = unique_tracking_id\n\n\n    def save_tracking_points(self):\n        a_image_name = get_image_name(self.image_path_list[ self.movie_frame_num - 1 ], self.image_format )\n        save_string = self.canvas_oval_coords_to_string()\n        with open(f'{self.save_path}/{a_image_name}.txt', 'w') as f:\n            f.write(save_string)\n\n        self.update_tracking_label()\n\n    def load_tracking_points(self):\n        def _load_tracking_points(load_path):\n            self.tracking_id_from_oval_id = {}\n            with open(load_path, 'r') as f:\n                canvas_oval_coords = f.readlines()\n\n            MAX_NUM_POINTS = len(canvas_oval_coords)\n\n            for a_index, canvas_oval_coord in enumerate(canvas_oval_coords):\n                a_coord = canvas_oval_coord.split()\n                a_color = np.array([self.color_map(1. * a_index / MAX_NUM_POINTS)])*255\n                a_color = a_color.astype('uint8')\n                a_color_hex = \"#%02x%02x%02x\" % (a_color[0,0], a_color[0,1], a_color[0,2])\n\n                if int(a_coord[0]) == -1:  # removed coordinate\n                    new_oval_id = self.draw_circle_x_y( 0, 0, zoomin=False)\n                    self.remove_circle_x_y( 0, 0)\n                    self.tracking_id_from_oval_id[new_oval_id] = new_oval_id\n                else:\n                    new_oval_id = self.draw_circle_x_y(int(a_coord[0]), int(a_coord[1]), zoomin=False, outline_color=a_color_hex)\n                self.tracking_id_from_oval_id[new_oval_id] = new_oval_id\n\n        a_image_name = get_image_name(self.image_path_list[ self.movie_frame_num - 1 ], self.image_format )\n        load_path = f'{self.save_path}/{a_image_name}.txt'\n        if path.exists(load_path):\n            _load_tracking_points(load_path)\n            self.update_tracking_label(saved=True)\n\n        else:\n            # in a new frame, load previous frame's GT points\n            prev_movie_frame_num = self.movie_frame_num - 2\n            while prev_movie_frame_num >= 0:\n                a_image_name = get_image_name(self.image_path_list[prev_movie_frame_num], self.image_format)\n                load_path = f'{self.save_path}/{a_image_name}.txt'\n                if path.exists(load_path):\n                    _load_tracking_points(load_path)\n                    self.update_tracking_label(saved=False)\n                    break\n                else:\n                    prev_movie_frame_num = prev_movie_frame_num - 1\n\n    def display_image_on_canvas(self):\n        if self.canvas_image_id is not None:\n            self.canvas.delete(self.canvas_image_id)  # delete previous image\n            self.clear_ovals_in_canvas()\n        orig_img = Image.open(self.image_path_list[self.movie_frame_num - 1]).convert('RGB')\n        self.canvas_img = ImageTk.PhotoImage(orig_img)\n        self.canvas_image_id = self.canvas.create_image(0, 0, image=self.canvas_img, anchor=\"nw\")\n\n    # -----------------------------------------\n    def get_prev_frame(self):\n        if self.movie_frame_num > 1:\n            self.movie_frame_num = self.movie_frame_num - 1\n            self.update_new_frame()\n\n    def get_5_prev_frame(self, event):\n        if self.movie_frame_num < 2:\n            print()\n        else:\n            self.save_tracking_points()\n            if self.movie_frame_num > 5:\n                # If the frame_num is not in the multiple of 5\n                a_remainder = (self.movie_frame_num - 5) % 5\n                if a_remainder == 0:\n                    self.movie_frame_num = self.movie_frame_num - 5\n                else:\n                    self.movie_frame_num = self.movie_frame_num - a_remainder\n            elif self.movie_frame_num <= 5:\n                self.movie_frame_num = 1\n            self.update_new_frame()\n\n    def get_next_frame(self):\n        if self.movie_frame_num < len(self.image_path_list):\n            # self.save_tracking_points()\n            self.movie_frame_num = self.movie_frame_num + 1\n            self.update_new_frame()\n\n    def get_5_next_frame(self, event):\n        if self.movie_frame_num >= len(self.image_path_list)-5:\n            print()\n        else:\n            self.save_tracking_points()\n            if self.movie_frame_num < 5:\n                self.movie_frame_num = 5\n            else:\n                # If the frame_num is not in the multiple of 5\n                a_remainder = (self.movie_frame_num + 5) % 5\n                self.movie_frame_num = self.movie_frame_num + 5 - a_remainder\n            self.update_new_frame()\n\n    # -----------------------------------------\n\n    def update_new_frame(self):\n        print(f'----------- {self.movie_frame_num} ------------')\n        self.display_image_on_canvas()\n        self.load_tracking_points()\n        self.zoom_canvas.after(1, self.crop_x_y, self.prev_cursor_x, self.prev_cursor_y)\n        self.update_frame_label()\n\n    def update_frame_label(self):\n        self.image_label.config(text=f'Frame Number: {self.movie_frame_num}')\n\n    def update_tracking_label(self, saved=True):\n        now = datetime.now()\n        current_time = now.strftime(\"%H:%M:%S\")\n\n        if saved:\n            self.tracking_label.config(text=f'Tracking Points: { np.count_nonzero(self.canvas_oval_coords>0) }; '\n                                        f'Saved time: {current_time}')\n        else:\n            self.tracking_label.config(text=f'Tracking Points: { np.count_nonzero(self.canvas_oval_coords>0) }; '\n                                        f'Not Saved')", "\n\nclass LoadMovie:\n    # refered to https://stackoverflow.com/questions/63128011/magnifying-glass-with-resizeable-image-for-tkinter\n    def __init__(self, root_window, left_frame, right_frame, movie_path, save_path):\n        self.save_path = save_path\n        # make a save folder\n        if not os.path.exists(self.save_path):\n            os.mkdir(self.save_path)\n\n        # get images in the folder\n        self.image_format = '.png'\n        self.image_path_list = glob(f\"{movie_path}/*{self.image_format}\")\n        self.movie_frame_num = 1\n\n\n        # set up zoom canvas\n        zoom_canvas_width = 300\n        zoom_canvas_height = 200\n\n        zoom_label = tk.Label(left_frame, text=\"Zoom In\", width=20, height=2, fg=\"red\", font=(\"Helvetica\", 14))\n        zoom_label.pack()\n\n        self.zoom_canvas = tk.Canvas(left_frame, width=zoom_canvas_width, height=zoom_canvas_height, relief=\"solid\", bd=2)\n        self.zoom_canvas.pack()\n\n\n        # set up label\n        self.image_label = tk.Label(right_frame, text=\"Frame Number: 1\", width=20, height=2, fg=\"red\", font=(\"Helvetica\", 14))\n        self.image_label.pack(side='top')\n        # color map for each loaded point\n        self.color_map = pylab.get_cmap('gist_rainbow')\n\n        # ------------------------------- set up main canvas -------------------------------\n        orig_img = Image.open(self.image_path_list[0]).convert('RGB')\n        img_width, img_height = orig_img.width, orig_img.height\n\n        self.canvas_image_id = None\n        canvas_frame = tk.Frame(right_frame, relief=\"solid\", bd=2, width=500, height=400)\n        canvas_frame.pack()\n\n        self.canvas = tk.Canvas(canvas_frame, relief=\"solid\", bd=2, width=700, height=400, scrollregion=(0,0,img_width,img_height))\n        hbar = tk.Scrollbar(canvas_frame, orient=tk.HORIZONTAL)\n        hbar.pack(side='bottom', fill=tk.X)\n        hbar.config(command=self.canvas.xview)\n        vbar = tk.Scrollbar(canvas_frame, orient=tk.VERTICAL)\n        vbar.pack(side='right', fill=tk.Y)\n        vbar.config(command=self.canvas.yview)\n        self.canvas.config(xscrollcommand=hbar.set, yscrollcommand=vbar.set)\n        self.canvas.pack( expand=True, fill=tk.BOTH)\n\n        self.display_image_on_canvas()\n        self.canvas_oval_coords = np.zeros(shape=(img_height, img_width), dtype='uint16') # row, column\n        self.tracking_id_from_oval_id = {}   # new oval id : unique tracking id\n\n        # Bind user action to features\n        # if scrolled, zoom in the image\n        self.zoomcycle = 0\n        self.zimg_id = None\n        self.zoval_id = None\n        self.prev_cursor_x = None\n        self.prev_cursor_y = None\n        self.canvas.bind(\"<MouseWheel>\", self.zoomer)\n        self.canvas.bind(\"<Motion>\", self.crop)\n        self.pix_tolerance = 20  # mouse cursor to the point proximity in pixels\n\n        # If left-clicked, save x,y cursor coordinates\n        self.canvas.bind('<ButtonRelease-1>', self.draw_circle)\n\n        # if right-clicked, remove previously clicked coordinate\n        # self.canvas.bind('<Button-3>', self.remove_circle)\n\n        # if left-clicked moving, a closest tracking point is dragged\n        self.canvas.bind('<B1-Motion>', self.drag_tracking_point)\n\n        # ---------- Setup Label ------------\n        self.tracking_label = tk.Label(right_frame, text=f\"\", width=40, height=2, fg=\"red\", font=(\"Helvetica\", 14))\n        self.tracking_label.pack()\n        self.update_tracking_label()\n        self.load_tracking_points()\n\n        # ---------- Setup buttons ----------\n        paned_window_buttons = tk.PanedWindow(right_frame, relief=\"raised\", bd=2)\n        paned_window_buttons.pack()\n\n        prev_button = tk.Button(paned_window_buttons, text='Prev', width=25, command=self.get_prev_frame, font=(\"Helvetica\", 12))\n        save_button = tk.Button(paned_window_buttons, text='Save', width=25, command=self.save_tracking_points, font=(\"Helvetica\", 12))\n        next_button = tk.Button(paned_window_buttons, text='Next', width=25, command=self.get_next_frame, font=(\"Helvetica\", 12))\n\n        paned_window_buttons.add(prev_button)\n        paned_window_buttons.add(save_button)\n        paned_window_buttons.add(next_button)\n\n        clear_button = tk.Button(right_frame, text='Clear', width=25, command=self.clear_ovals_in_canvas, font=(\"Helvetica\", 12))\n        clear_button.pack()\n\n        root_window.bind('<Left>', self.get_5_prev_frame)\n        root_window.bind('<Right>', self.get_5_next_frame)\n        root_window.bind('<KeyPress>', self.onKeyPress)\n\n\n    def onKeyPress(self, event):\n        print('onKeyPress:', event.char)\n        if event.char == ',':\n            self.get_prev_frame()\n        elif event.char == '.':\n            self.get_next_frame()\n        elif event.char == 'm':\n            self.save_tracking_points()\n\n    def get_canvas_as_image(self, cur_x, cur_y, tol_x, tol_y):\n        x = self.canvas.winfo_rootx() + cur_x\n        y = self.canvas.winfo_rooty() + cur_y\n        self.prev_cursor_x = cur_x\n        self.prev_cursor_y = cur_y\n\n        return ImageGrab.grab().crop((x-tol_x, y-tol_y, x+tol_x, y+tol_y))\n\n    def zoomer(self, event):\n        if (event.delta > 0):\n            if self.zoomcycle != 5: self.zoomcycle += 1\n        elif (event.delta < 0):\n            if self.zoomcycle != 0: self.zoomcycle -= 1\n        self.crop_x_y(event.x, event.y)\n\n    def crop(self, event):\n        if event is not None:\n            self.crop_x_y(event.x, event.y)\n\n    def crop_x_y(self, x, y):\n        if self.zimg_id: self.zoom_canvas.delete(self.zimg_id)\n        if self.zoval_id: self.zoom_canvas.delete(self.zoval_id)\n        if (self.zoomcycle) != 0:\n            if self.zoomcycle == 1:\n                tmp = self.get_canvas_as_image(x,y, 90, 67.5)\n            if self.zoomcycle == 2:\n                tmp = self.get_canvas_as_image(x,y, 60, 45)\n            elif self.zoomcycle == 3:\n                tmp = self.get_canvas_as_image(x,y, 45, 30)\n            elif self.zoomcycle == 4:\n                tmp = self.get_canvas_as_image(x,y, 30, 20)\n            elif self.zoomcycle == 5:\n                tmp = self.get_canvas_as_image(x,y, 15, 10)\n            size = self.zoom_canvas.winfo_width(), self.zoom_canvas.winfo_height()\n\n            self.zimg = ImageTk.PhotoImage(tmp.resize(size))\n            self.zimg_id = self.zoom_canvas.create_image(0, 0, image=self.zimg, anchor='nw')\n            self.zoval_id = self.zoom_canvas.create_oval(self.zoom_canvas.winfo_width()/2-5, self.zoom_canvas.winfo_height()/2-5,\n                                         self.zoom_canvas.winfo_width()/2+5, self.zoom_canvas.winfo_height()/2+5, outline =\"red\", width=1)\n\n    def get_x_y_from_event(self, event):\n        canvas = event.widget\n        x = canvas.canvasx(event.x)  # event.x\n        y = canvas.canvasy(event.y)  # event.y\n\n        return int(x), int(y)\n\n    def draw_circle(self, event):\n        x, y = self.get_x_y_from_event(event)\n\n        closest_oval_location = self.get_closest_oval_location(x, y, self.pix_tolerance)\n\n        if closest_oval_location is None:\n            print('closest oval is not found')\n            new_oval_id = self.draw_circle_x_y(x, y, event)\n            self.tracking_id_from_oval_id[new_oval_id] = new_oval_id\n\n    def draw_circle_x_y(self, x, y, event=None, zoomin=True, outline_color=\"blue\"):\n        print('draw_circle: {}, {}'.format(x, y))\n        if self.canvas_oval_coords[y, x] == 0:\n            x1 = x-5\n            y1 = y-5\n            x2 = x+5\n            y2 = y+5\n            oval_id = self.canvas.create_oval(x1, y1, x2, y2, outline=outline_color, width=2)\n            self.canvas_oval_coords[y, x] = oval_id\n            # update zoom canvas\n            if zoomin:\n                self.zoom_canvas.after(1, self.crop, event)\n\n            return oval_id\n        else:\n            print('Circle is already drawn at {}, {}'.format(x, y))\n            return self.canvas_oval_coords[y, x]  # prev oval_id\n\n    def clip_val(self, val, max):\n        if val < 0:\n            val = 0\n        if val > max:\n            val = max\n        return val\n\n    def get_closest_oval_location(self, x, y, pix_tolerance):\n        # from the current mouse cursor position\n        neighboring_oval_locations = (self.canvas_oval_coords\n                                      [self.clip_val(y - pix_tolerance, self.canvas_oval_coords.shape[0]):\n                                       self.clip_val(y + pix_tolerance, self.canvas_oval_coords.shape[0]),\n                                      self.clip_val(x - pix_tolerance, self.canvas_oval_coords.shape[1]):\n                                      self.clip_val(x + pix_tolerance, self.canvas_oval_coords.shape[1])] > 0)\n        neighboring_oval_locations = np.transpose(neighboring_oval_locations.nonzero())\n\n        min_dist = (pix_tolerance ** 2) * 2 + 1\n        closest_oval_location = None\n        for a_oval_location in neighboring_oval_locations:\n            new_dist = (a_oval_location[0] - pix_tolerance)**2 + (a_oval_location[1] - pix_tolerance)**2\n            if min_dist > new_dist:\n                closest_oval_location = a_oval_location\n                min_dist = new_dist\n\n        return closest_oval_location\n\n    def remove_circle(self, event):\n        x, y = self.get_x_y_from_event(event)\n\n        self.remove_circle_x_y(x, y, event)\n\n    def remove_circle_x_y(self, x, y, event=None):\n        print('remove_circle_x_y: {}, {}'.format(x, y))\n        closest_oval_location = self.get_closest_oval_location(x, y, self.pix_tolerance)\n\n        if closest_oval_location is None:\n            print('closest oval is not found')\n            closest_oval_id = None\n\n        else:\n            pix_tolerance_y = self.pix_tolerance\n            pix_tolerance_x = self.pix_tolerance\n            if y < self.pix_tolerance:\n                pix_tolerance_y = y\n            if x < self.pix_tolerance:\n                pix_tolerance_x = x\n\n            closest_oval_id = self.canvas_oval_coords[y+closest_oval_location[0]-pix_tolerance_y,\n                                                      x+closest_oval_location[1]-pix_tolerance_x ]\n\n            # remove from the canvas\n            self.canvas.delete(closest_oval_id)\n            # remove from the record\n            self.canvas_oval_coords[\n                y + closest_oval_location[0] - pix_tolerance_y, x + closest_oval_location[1] - pix_tolerance_x] = 0\n            # update zoom canvas\n            self.zoom_canvas.after(1, self.crop, event)\n\n        return closest_oval_id\n\n    def clear_ovals_in_canvas(self):\n        neighboring_oval_locations = np.transpose((self.canvas_oval_coords > 0).nonzero())\n\n        for a_oval_location in neighboring_oval_locations:\n            oval_id = self.canvas_oval_coords[a_oval_location[0], a_oval_location[1] ]\n            # remove from the canvas\n            self.canvas.delete(oval_id)\n            # remove from the record\n            self.canvas_oval_coords[ a_oval_location[0], a_oval_location[1]] = 0\n\n    def canvas_oval_coords_to_string(self):\n        '''\n        Save the coordinates to the text file with the following format\n        column_coord row_coord\\n\n\n        :return:\n        '''\n        a_string = \"\"\n        oval_location_dict = {}\n        oval_locations = (self.canvas_oval_coords > 0 ).nonzero()  # np.transpose\n        rows = oval_locations[0]\n        columns = oval_locations[1]\n\n        unique_tracking_ids_visible_in_canvas = []  # for handling removed points\n        for a_coordinate in zip(rows, columns):\n            oval_id = self.canvas_oval_coords[a_coordinate]\n            unique_tracking_id = self.tracking_id_from_oval_id[oval_id]\n            unique_tracking_ids_visible_in_canvas.append(unique_tracking_id)\n            oval_location_dict[unique_tracking_id] = a_coordinate\n\n        # Handle the removed points\n        for oval_id, unique_tracking_id in self.tracking_id_from_oval_id.items():\n            if unique_tracking_id not in unique_tracking_ids_visible_in_canvas:\n                oval_location_dict[unique_tracking_id] = (-1, -1)\n\n        # sort dict by keys\n        od = collections.OrderedDict(sorted(oval_location_dict.items()))\n\n        for k, a_coordinate in od.items():\n            # reorder row & column coordinate to x & y coordinate\n            a_string += str(a_coordinate[1]) + ' ' + str(a_coordinate[0]) + '\\n'\n\n        return a_string\n\n    def drag_tracking_point(self, event):\n        x, y = self.get_x_y_from_event(event)\n        prev_oval_id = self.remove_circle_x_y(x, y, event)\n        if prev_oval_id is not None:\n            unique_tracking_id = self.tracking_id_from_oval_id[prev_oval_id]\n            del self.tracking_id_from_oval_id[prev_oval_id]\n\n            new_oval_id = self.draw_circle_x_y(x, y, event)\n            self.tracking_id_from_oval_id[new_oval_id] = unique_tracking_id\n\n\n    def save_tracking_points(self):\n        a_image_name = get_image_name(self.image_path_list[ self.movie_frame_num - 1 ], self.image_format )\n        save_string = self.canvas_oval_coords_to_string()\n        with open(f'{self.save_path}/{a_image_name}.txt', 'w') as f:\n            f.write(save_string)\n\n        self.update_tracking_label()\n\n    def load_tracking_points(self):\n        def _load_tracking_points(load_path):\n            self.tracking_id_from_oval_id = {}\n            with open(load_path, 'r') as f:\n                canvas_oval_coords = f.readlines()\n\n            MAX_NUM_POINTS = len(canvas_oval_coords)\n\n            for a_index, canvas_oval_coord in enumerate(canvas_oval_coords):\n                a_coord = canvas_oval_coord.split()\n                a_color = np.array([self.color_map(1. * a_index / MAX_NUM_POINTS)])*255\n                a_color = a_color.astype('uint8')\n                a_color_hex = \"#%02x%02x%02x\" % (a_color[0,0], a_color[0,1], a_color[0,2])\n\n                if int(a_coord[0]) == -1:  # removed coordinate\n                    new_oval_id = self.draw_circle_x_y( 0, 0, zoomin=False)\n                    self.remove_circle_x_y( 0, 0)\n                    self.tracking_id_from_oval_id[new_oval_id] = new_oval_id\n                else:\n                    new_oval_id = self.draw_circle_x_y(int(a_coord[0]), int(a_coord[1]), zoomin=False, outline_color=a_color_hex)\n                self.tracking_id_from_oval_id[new_oval_id] = new_oval_id\n\n        a_image_name = get_image_name(self.image_path_list[ self.movie_frame_num - 1 ], self.image_format )\n        load_path = f'{self.save_path}/{a_image_name}.txt'\n        if path.exists(load_path):\n            _load_tracking_points(load_path)\n            self.update_tracking_label(saved=True)\n\n        else:\n            # in a new frame, load previous frame's GT points\n            prev_movie_frame_num = self.movie_frame_num - 2\n            while prev_movie_frame_num >= 0:\n                a_image_name = get_image_name(self.image_path_list[prev_movie_frame_num], self.image_format)\n                load_path = f'{self.save_path}/{a_image_name}.txt'\n                if path.exists(load_path):\n                    _load_tracking_points(load_path)\n                    self.update_tracking_label(saved=False)\n                    break\n                else:\n                    prev_movie_frame_num = prev_movie_frame_num - 1\n\n    def display_image_on_canvas(self):\n        if self.canvas_image_id is not None:\n            self.canvas.delete(self.canvas_image_id)  # delete previous image\n            self.clear_ovals_in_canvas()\n        orig_img = Image.open(self.image_path_list[self.movie_frame_num - 1]).convert('RGB')\n        self.canvas_img = ImageTk.PhotoImage(orig_img)\n        self.canvas_image_id = self.canvas.create_image(0, 0, image=self.canvas_img, anchor=\"nw\")\n\n    # -----------------------------------------\n    def get_prev_frame(self):\n        if self.movie_frame_num > 1:\n            self.movie_frame_num = self.movie_frame_num - 1\n            self.update_new_frame()\n\n    def get_5_prev_frame(self, event):\n        if self.movie_frame_num < 2:\n            print()\n        else:\n            self.save_tracking_points()\n            if self.movie_frame_num > 5:\n                # If the frame_num is not in the multiple of 5\n                a_remainder = (self.movie_frame_num - 5) % 5\n                if a_remainder == 0:\n                    self.movie_frame_num = self.movie_frame_num - 5\n                else:\n                    self.movie_frame_num = self.movie_frame_num - a_remainder\n            elif self.movie_frame_num <= 5:\n                self.movie_frame_num = 1\n            self.update_new_frame()\n\n    def get_next_frame(self):\n        if self.movie_frame_num < len(self.image_path_list):\n            # self.save_tracking_points()\n            self.movie_frame_num = self.movie_frame_num + 1\n            self.update_new_frame()\n\n    def get_5_next_frame(self, event):\n        if self.movie_frame_num >= len(self.image_path_list)-5:\n            print()\n        else:\n            self.save_tracking_points()\n            if self.movie_frame_num < 5:\n                self.movie_frame_num = 5\n            else:\n                # If the frame_num is not in the multiple of 5\n                a_remainder = (self.movie_frame_num + 5) % 5\n                self.movie_frame_num = self.movie_frame_num + 5 - a_remainder\n            self.update_new_frame()\n\n    # -----------------------------------------\n\n    def update_new_frame(self):\n        print(f'----------- {self.movie_frame_num} ------------')\n        self.display_image_on_canvas()\n        self.load_tracking_points()\n        self.zoom_canvas.after(1, self.crop_x_y, self.prev_cursor_x, self.prev_cursor_y)\n        self.update_frame_label()\n\n    def update_frame_label(self):\n        self.image_label.config(text=f'Frame Number: {self.movie_frame_num}')\n\n    def update_tracking_label(self, saved=True):\n        now = datetime.now()\n        current_time = now.strftime(\"%H:%M:%S\")\n\n        if saved:\n            self.tracking_label.config(text=f'Tracking Points: { np.count_nonzero(self.canvas_oval_coords>0) }; '\n                                        f'Saved time: {current_time}')\n        else:\n            self.tracking_label.config(text=f'Tracking Points: { np.count_nonzero(self.canvas_oval_coords>0) }; '\n                                        f'Not Saved')", "\n\nif __name__ == \"__main__\":\n    root_window = tk.Tk()\n    root_window.title('Tracking Points Label tool')\n    root_window.geometry(\"1100x725\")\n\n    # set up label\n    left_frame = tk.Frame(root_window, relief=\"solid\", bd=2)\n    left_frame.pack(side=\"left\", fill=\"both\", expand=True)\n\n    # heading_label = tk.Label(left_frame, text=\"How to use: \\n Scroll to zoom. \\n Finish labeling one point \\n before adding one more point\", font=(\"Helvetica\", 12))\n    # heading_label.pack(side=\"left\", expand=True)\n\n    right_frame = tk.Frame(root_window, relief=\"solid\", bd=2)\n    right_frame.pack(side=\"right\", fill=\"both\", expand=True)\n\n    # for Phase contrast\n    # 040119_PtK1_S01_01_phase, 040119_PtK1_S01_01_phase_ROI2, 040119_PtK1_S01_01_phase_2_DMSO_nd_01, 040119_PtK1_S01_01_phase_3_DMSO_nd_03\n    LoadMovie(root_window, left_frame, right_frame,\n              movie_path=\"generated/Computer Vision/PC_live/040119_PtK1_S01_01_phase/contour_points_visualize/\",\n              save_path='generated/Computer Vision/PC_live/040119_PtK1_S01_01_phase/points/')\n\n    # for SDC dataset\n    # 1_050818_DMSO_09, 2_052818_S02_none_08, 3_120217_S02_CK689_50uM_08, 4_122217_S02_DMSO_04, 5_120217_S02_CK689_50uM_07, 6_052818_S02_none_02, 7_120217_S02_CK689_50uM_13, 8_TFM-08122012-5, 9_052818_S02_none_12\n    # LoadMovie(root_window, right_frame,\n    #           movie_path=\"generated/Computer Vision/HACKS_live/5_120217_S02_CK689_50uM_07/contour_points_visualize/\",\n    #           save_path='generated/Computer Vision/HACKS_live/5_120217_S02_CK689_50uM_07/points/')\n\n    # for jellyfish\n    # LoadMovie(root_window, right_frame,\n    #           movie_path=\"generated/Computer Vision/Jellyfish/First/contour_points_visualize/\",\n    #           save_path='generated/Computer Vision/Jellyfish/First/points/')\n\n    root_window.eval('tk::PlaceWindow . center')\n    root_window.mainloop()"]}
{"filename": "src/preprocessing/visualization_utils.py", "chunked_list": ["'''\nAuthor: Junbong Jang\nDate: 8/8/2022\n\nGiven an input image and tracking points of that image,\nDraw tracking points over the image.\n'''\n\nimport os\nimport cv2", "import os\nimport cv2\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport pylab\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef get_image_name(image_path, image_format):\n    return os.path.basename(image_path).replace(image_format, '')", "\ndef get_image_name(image_path, image_format):\n    return os.path.basename(image_path).replace(image_format, '')\n\n\ndef display_image_in_actual_size(img, cmap, dpi = 100, blank=False):\n\n    if len(img.shape) == 2:\n        height, width = img.shape\n    elif len(img.shape) == 3:\n        height, width, depth = img.shape\n    else:\n        raise ValueError('incorrect im_data shape')\n\n    # What size does the figure need to be in inches to fit the image?\n    figsize = width / float(dpi), height / float(dpi)\n\n    # Create a figure of the right size with one axes that takes up the full figure\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_axes([0, 0, 1, 1])\n\n    # Hide spines, ticks, etc.\n    ax.axis('off')\n\n    if blank:\n        img[:, :, :] = 0\n        # img[:, :, 0] =` 222/255\n        # img[:, :, 1] = 235/255\n        # img[:, :, 2] = 247/255\n    ax.imshow(img, cmap=cmap)\n\n    return fig, ax", "\n\ndef visualize_points(root_path, points_path, cmap, image_format, a_folder, save_path):\n    if os.path.exists(save_path) is not True:\n        os.makedirs(save_path)\n\n    # images = glob(f\"{root_path}/{a_folder}/images/*{image_format}\")\n    points_textfiles = glob(f\"{root_path}/{points_path}/*.txt\")\n\n    # keep image paths that have corresponding textfiles (for visualizing PoST)\n    image_paths = []\n    for textfile_path in points_textfiles:\n        image_name = os.path.basename(textfile_path).replace('.txt', image_format)\n        corresponding_image_path = f\"assets/Computer Vision/PoST_gt/{a_folder}/images/{image_name}\"\n        image_paths.append(corresponding_image_path)\n\n    assert len(image_paths) == len(points_textfiles)\n    image_paths = sorted(image_paths)\n    points_textfiles = sorted(points_textfiles)\n\n    tracking_points = {}\n    last_image_path = \"\"\n\n    # get the max number of points among all point set files to have the same color map for points\n    MAX_NUM_POINTS = 0\n    for image_path in image_paths:\n        image_name = get_image_name(image_path, image_format)\n        textfile_path = f\"{root_path}/{points_path}/{image_name}.txt\"\n        if os.path.exists(textfile_path):\n            ndarray_points = np.loadtxt(textfile_path)\n\n            if MAX_NUM_POINTS < ndarray_points.shape[0]:\n                MAX_NUM_POINTS = ndarray_points.shape[0]\n\n    # find the image that has the corresponding points_textfile\n    for image_index, (image_path, textfile_path)  in enumerate(zip(image_paths, points_textfiles)):\n        image_name = get_image_name(image_path, image_format)\n        if os.path.exists(textfile_path):\n            print('image_name', image_name)\n            ndarray_points = np.loadtxt(textfile_path)\n\n            # set up image\n            last_image_path = image_path\n            im = plt.imread(image_path)\n            plt.imshow(im, cmap=cmap)\n            cm = pylab.get_cmap('gist_rainbow')\n\n            for a_index, a_point in enumerate(ndarray_points):\n                # initialize tracking_points dict\n                if not tracking_points:\n                    tracking_points[a_index] = [[], []]\n                elif a_index not in tracking_points:\n                    tracking_points[a_index] = [[], []]\n\n                # draw all points\n                x, y = a_point\n                # TODO: is it ok to ignore these points going outside the image?\n                if x >= 0 and y >= 0 and x < im.shape[1] and y < im.shape[0]:\n                    plt.scatter(x=x, y=y, c=np.array([cm(1. * a_index / MAX_NUM_POINTS)]), s=10)\n                tracking_points[a_index][0].append(x)\n                tracking_points[a_index][1].append(y)\n\n            plt.axis('off')\n            plt.savefig(f\"{save_path}/{image_name}.png\", bbox_inches=\"tight\", pad_inches=0)\n            plt.close()\n\n        # --------- Visualize contour points as mask -----------------\n        tracking_points_np = np.zeros((len(tracking_points), 2))\n        for a_key in tracking_points:\n            tracking_points_np[a_key, 0] = tracking_points[a_key][0][image_index]\n            tracking_points_np[a_key, 1] = tracking_points[a_key][1][image_index]\n\n        # a_mask = cnt2mask(tracking_points_np, im.shape)\n        # cv2.imwrite(f\"{save_path}/cnt2mask_{image_name}.png\", a_mask*255)\n\n    # -------------- draw trajectory of tracking points in the last image --------------------\n    last_image_name = get_image_name(last_image_path, image_format)\n    ndarray_points = np.loadtxt(f\"{root_path}/{points_path}/{last_image_name}.txt\")\n\n    # set up image\n    a_img = plt.imread(last_image_path) # .astype('uint8')\n    fig, ax = display_image_in_actual_size(a_img, cmap)\n    NUM_COLORS = len(ndarray_points)\n    cm = pylab.get_cmap('gist_rainbow')\n\n    for a_key in tracking_points:\n        # draw all points\n        x, y = tracking_points[a_key]\n        ax.plot(x, y, c=np.array([cm(1. * a_key / NUM_COLORS)]), marker='o', linewidth=1, markersize=1)\n\n    fig.savefig(f\"{save_path}/trajectory_{last_image_name}.png\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close()", "\ndef visualize_points_in_frame(a_image_path, ordered_contour_points, save_path, save_name):\n    cur_image = plt.imread(a_image_path)\n    plt.imshow(cur_image, cmap='gray')\n\n    cmap = pylab.get_cmap('gist_rainbow')\n    NUM_COLORS = len(ordered_contour_points)\n    for a_index, a_coord in enumerate(ordered_contour_points):\n        if a_coord is not None:\n            plt.scatter(x=a_coord[0], y=a_coord[1], c=np.array([cmap(1. * a_index / NUM_COLORS)]), s=1)\n\n    if not os.path.exists(f\"{save_path}\"):\n        os.mkdir(f\"{save_path}\")\n\n    # save figure\n    plt.savefig(f\"{save_path}/{save_name}.png\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close()", "\n\ndef plot_tracking_points(plot_dir, save_name, cur_index, gt_tracking_points, pred_tracking_points, a_image):\n    '''\n    visualize and save moved tracking points\n\n    :param plot_dir:\n    :param save_name:\n    :param image:\n    :return:\n    '''\n    cm = pylab.get_cmap('gist_rainbow')\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n        os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    MAX_NUM_POINTS = pred_tracking_points.shape[0]\n    # -------------- draw predicted points on an image --------------\n    for point_index, (pred_tracking_point, gt_tracking_point) in enumerate(zip(pred_tracking_points, gt_tracking_points)):\n        pred_col, pred_row = pred_tracking_point\n        plt.scatter(x=pred_col, y=pred_row, c=np.array([cm(1. * point_index / MAX_NUM_POINTS)]), s=5)\n\n        gt_col, gt_row = gt_tracking_point\n        plt.scatter(x=gt_col, y=gt_row, s=5, facecolors='none', linewidths=0.5, edgecolors=np.array([cm(1. * point_index / MAX_NUM_POINTS)]))\n\n    plt.imshow(a_image, cmap='gray')\n    plt.axis('off')\n    plt.savefig(f\"{plot_dir}/{save_name}/matlab_gt_vs_my_gt{cur_index}.png\", bbox_inches=\"tight\", pad_inches=0)\n    plt.close()", "\n# --------------------------------------------------------------------------------\n# for PoST\ndef visualize_points_in_POST(root_assets_path, root_generated_path):\n    '''\n    Date: 2/7/2022\n\n    Given an input image and the label about the location of contour points from PoST dataset,\n    Draw the contour points over the image.\n    '''\n\n    image_format = '.jpg'\n    all_folders = glob(f\"{root_assets_path}/PoST_gt/*\")\n    # all_folders = ['floating_polygon'] # bear', 'blackswan', 'freeway', 'helicopter\n    cmap = None\n\n    for a_folder in all_folders:\n        a_folder = os.path.basename(a_folder)\n        print('-----------------------', a_folder, '-----------------------')\n\n        save_path = f\"{root_generated_path}/PoST_gt/{a_folder}/\"\n        gt_points_path = f\"{a_folder}/points/\"\n        visualize_points(f\"{root_assets_path}/PoST_gt\", gt_points_path, cmap, image_format, a_folder, save_path)", "\n        # save_path = f\"{root_generated_path}/PoST_predicted/{a_folder}/\"\n        # predicted_points_path = f\"{a_folder}/\"\n        # visualize_points(f\"{root_assets_path}/PoST_predicted\", predicted_points_path, cmap, image_format, a_folder, save_path)\n\n\ndef visualize_points_in_live_cell(root_assets_path, root_generated_path):\n    '''\n    Given an input image and the label about the location of contour points from PoST dataset,\n    Draw the contour points over the image.\n    '''\n\n    image_format = '.png'\n    all_folders = ['040119_PtK1_S01_01_phase_3_DMSO_nd_03']\n    cmap = 'gray'\n\n    for a_folder in all_folders:\n        print('-----------------------', a_folder, '-----------------------')\n        a_folder = os.path.basename(a_folder)\n\n        save_path = f\"{root_generated_path}/{a_folder}/gt/\"\n        points_path = f\"{a_folder}/points/\"\n        visualize_points(root_assets_path, points_path, cmap, image_format, a_folder, save_path)", "\n        # save_path = f\"{root_generated_path}/{a_folder}/generated/\"\n        # points_path = f\"{root_path}/{a_folder}/generated_points/\"\n        # visualize_points(root_path, points_path, cmap, image_format, a_folder, save_path)\n\n\n\nif __name__ == \"__main__\":\n    visualize_points_in_POST(root_assets_path='assets/Computer Vision/', root_generated_path='generated/Computer Vision/')\n    ", "    \n    # root_assets_path = 'assets/Computer Vision/PC_live/'\n    # root_generated_path = \"generated/Computer Vision/PC_live/\"\n    # visualize_points_in_live_cell(root_assets_path, root_generated_path)"]}
{"filename": "src/preprocessing/contour_tracking_manuscript_figures.py", "chunked_list": ["'''\nAuthor: Junbong Jang\nDate: 1/29/2023\n\nGenerate figures for contour tracking manuscript\n\n'''\n\nimport os\nimport matplotlib.pyplot as plt", "import os\nimport matplotlib.pyplot as plt\nimport pylab\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom statistics import mean\nimport matplotlib as mpl\nimport scipy.io as sio\nimport seaborn as sns", "import scipy.io as sio\nimport seaborn as sns\n\nfrom matplotlib.colors import LinearSegmentedColormap, ListedColormap\n\n# ----------------------------------- Utility functions -----------------------------------\n\n\ndef get_closest_indices_from_gt_and_pred(init_gt_points, pred_points):\n    closest_pred_point_indices = []\n    for gt_point in init_gt_points:\n        min_dist = None\n        match_index = None\n        for index, pred_point in enumerate(pred_points):\n            a_dist = np.linalg.norm(pred_point - gt_point)\n            if min_dist is None or min_dist > a_dist:\n                min_dist = a_dist\n                match_index = index\n\n        closest_pred_point_indices.append(match_index)\n    return closest_pred_point_indices", "def get_closest_indices_from_gt_and_pred(init_gt_points, pred_points):\n    closest_pred_point_indices = []\n    for gt_point in init_gt_points:\n        min_dist = None\n        match_index = None\n        for index, pred_point in enumerate(pred_points):\n            a_dist = np.linalg.norm(pred_point - gt_point)\n            if min_dist is None or min_dist > a_dist:\n                min_dist = a_dist\n                match_index = index\n\n        closest_pred_point_indices.append(match_index)\n    return closest_pred_point_indices", "\n\n\ndef get_red_to_green_cmap():\n    # https://stackoverflow.com/questions/38246559/how-to-create-a-heat-map-in-python-that-ranges-from-green-to-red\n    # c = [\"red\", \"tomato\", \"green\", \"darkgreen\"]\n    # v = [0, .33, 0.66, 1.]\n    # l = list(zip(v, c))\n    # cmap = LinearSegmentedColormap.from_list('rg', l, N=256)\n    cmap = ListedColormap([\"red\", \"tomato\", \"green\", \"darkgreen\"])\n\n    return cmap", "\ndef draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color='w'):\n    delta_x = cur_x - prev_x\n    delta_y = cur_y - prev_y\n    # subtract delta by epsilon since arrows go outside the circle\n    # epsilon = 2\n    # if abs(delta_y) > epsilon:\n    #     if delta_y > 0:\n    #         delta_y = delta_y - epsilon\n    #     else:\n    #         delta_y = delta_y + epsilon\n    # if abs(delta_x) > epsilon:\n    #     if delta_x > 0:\n    #         delta_x = delta_x - epsilon\n    #     else:\n    #         delta_x = delta_x + epsilon\n\n    plt.arrow(prev_x, prev_y, delta_x, delta_y, linewidth=0.1, head_width=2, edgecolor=arrow_color, facecolor=arrow_color, antialiased=True)", "\n\ndef prevent_trajectory_cross(pred_tracking_points_contour_indices):\n    # order preservation constraint\n    # TODO: improve it by refering to the contour correspondence via ant colony optimization paper\n\n    max_contour_index = np.amax(pred_tracking_points_contour_indices)\n\n    # go the other way also\n    # for for_index, contour_index in enumerate(pred_tracking_points_contour_indices[::-1]):\n    #     # descending case\n    #     if for_index > 0:\n    #         prev_contour_index = pred_tracking_points_contour_indices[pred_tracking_points_contour_indices.shape[0]-for_index]\n    #         if max_contour_index//4 > (contour_index - prev_contour_index) and prev_contour_index < contour_index:\n    #             # current point at i <-- prev point at i + 1 index\n    #             pred_tracking_points_contour_indices[pred_tracking_points_contour_indices.shape[0] - for_index - 1] = pred_tracking_points_contour_indices[pred_tracking_points_contour_indices.shape[0] - for_index]\n\n    for for_index, contour_index in enumerate(pred_tracking_points_contour_indices):\n        # ascending case\n        if for_index > 0:\n            prev_contour_index = pred_tracking_points_contour_indices[for_index - 1]\n            if max_contour_index//4 > (prev_contour_index - contour_index) and prev_contour_index > contour_index:\n                # current point at i <-- prev point at i - 1 index\n                pred_tracking_points_contour_indices[for_index] = prev_contour_index\n\n    return pred_tracking_points_contour_indices", "\n\ndef load_protrusion_colormap():\n    matlab_file = sio.loadmat('protrusion_map_colormap.mat')\n\n    return matlab_file['velocity_cmap']\n\n# ----------------------------------- Draw Manuscript Figures -----------------------------------\n\ndef manuscript_figure1_trajectory(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices):\n\n    plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n    save_name = 'manuscript_figure1'\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n        os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    # -------------------------- Combine many predictions ------------------------------------------------\n    tracking_point_cmap = pylab.get_cmap('gist_rainbow')\n    contour_cmap = get_red_to_green_cmap()\n\n    initial_frame = 31\n    final_frame = 35\n    a_image = plt.imread(img_path_list[initial_frame])\n    plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n    plt.axis('off')\n\n    # initial_tracked_contour_point_indices_dict = {455:[],460:[],465:[],470:[],480:[],485:[],486:[],487:[],488:[],495:[],500:[],505:[],520:[],530:[],535:[],540:[],545:[]}\n    # initial_tracked_contour_point_indices_dict = {740:[],746:[],750:[],755:[],760:[],765:[],770:[],775:[],780:[],785:[], 790:[],795:[],796:[]}\n    initial_tracked_contour_point_indices_dict = {}\n    for a_index in range(0, 483, 7):\n        initial_tracked_contour_point_indices_dict[a_index] = []\n    for cur_frame in tqdm(range(initial_frame, final_frame+1)):\n        # Draw contours of multiple frames in one image\n        a_contour_points_path = contour_points_path_list[cur_frame]\n        contour_points = np.loadtxt(a_contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[cur_frame - 1, :]\n\n        for for_index, a_tracked_contour_point_index in enumerate(initial_tracked_contour_point_indices_dict.keys()):\n\n            if cur_frame > initial_frame:\n                prev_tracked_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n            else:\n                prev_tracked_contour_point_index = a_tracked_contour_point_index\n\n            next_tracked_contour_point_index = frame_pred_tracking_points_contour_indices[prev_tracked_contour_point_index]\n            initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index].append(next_tracked_contour_point_index)\n\n            # plot arrow\n            if cur_frame > initial_frame:\n                cur_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n                prev_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-2]\n                cur_x, cur_y = contour_points[cur_contour_point_index]\n                prev_x, prev_y = prev_contour_points[prev_contour_point_index]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array([tracking_point_cmap(1. * for_index / len(initial_tracked_contour_point_indices_dict.keys()) ) ]) )\n\n        prev_contour_points = contour_points\n\n\n    plt.savefig(f\"{plot_dir}/{save_name}/trajectory{initial_frame}_{final_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n    plt.close()", "\ndef manuscript_figure1_trajectory(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices):\n\n    plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n    save_name = 'manuscript_figure1'\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n        os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    # -------------------------- Combine many predictions ------------------------------------------------\n    tracking_point_cmap = pylab.get_cmap('gist_rainbow')\n    contour_cmap = get_red_to_green_cmap()\n\n    initial_frame = 31\n    final_frame = 35\n    a_image = plt.imread(img_path_list[initial_frame])\n    plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n    plt.axis('off')\n\n    # initial_tracked_contour_point_indices_dict = {455:[],460:[],465:[],470:[],480:[],485:[],486:[],487:[],488:[],495:[],500:[],505:[],520:[],530:[],535:[],540:[],545:[]}\n    # initial_tracked_contour_point_indices_dict = {740:[],746:[],750:[],755:[],760:[],765:[],770:[],775:[],780:[],785:[], 790:[],795:[],796:[]}\n    initial_tracked_contour_point_indices_dict = {}\n    for a_index in range(0, 483, 7):\n        initial_tracked_contour_point_indices_dict[a_index] = []\n    for cur_frame in tqdm(range(initial_frame, final_frame+1)):\n        # Draw contours of multiple frames in one image\n        a_contour_points_path = contour_points_path_list[cur_frame]\n        contour_points = np.loadtxt(a_contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[cur_frame - 1, :]\n\n        for for_index, a_tracked_contour_point_index in enumerate(initial_tracked_contour_point_indices_dict.keys()):\n\n            if cur_frame > initial_frame:\n                prev_tracked_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n            else:\n                prev_tracked_contour_point_index = a_tracked_contour_point_index\n\n            next_tracked_contour_point_index = frame_pred_tracking_points_contour_indices[prev_tracked_contour_point_index]\n            initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index].append(next_tracked_contour_point_index)\n\n            # plot arrow\n            if cur_frame > initial_frame:\n                cur_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n                prev_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-2]\n                cur_x, cur_y = contour_points[cur_contour_point_index]\n                prev_x, prev_y = prev_contour_points[prev_contour_point_index]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array([tracking_point_cmap(1. * for_index / len(initial_tracked_contour_point_indices_dict.keys()) ) ]) )\n\n        prev_contour_points = contour_points\n\n\n    plt.savefig(f\"{plot_dir}/{save_name}/trajectory{initial_frame}_{final_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n    plt.close()", "\n\n\ndef manuscript_figure1(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices):\n    '''\n    load my deep learning model's predicted points\n    load contour points\n    load images\n\n    Draw arrows from the contour\n\n    :param dataset_folder:\n    :return:\n    '''\n\n    plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n    save_name = 'manuscript_figure1'\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n        os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    # -------------------------- Combine many predictions ------------------------------------------------\n    # contour_cmap = get_red_to_green_cmap()\n    #\n    # initial_frame = 31\n    # final_frame = 35\n    # a_image = plt.imread(img_path_list[initial_frame])\n    # plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n    # plt.axis('off')\n    # for cur_frame in tqdm(range(initial_frame, final_frame+1)):\n    #     # Draw contours of multiple frames in one image\n    #     a_contour_points_path = contour_points_path_list[cur_frame]\n    #     contour_points = np.loadtxt(a_contour_points_path)\n    #     contour_points = contour_points.astype('int32')\n    #     a_color = np.array([contour_cmap(1. * (final_frame-cur_frame) / (final_frame-initial_frame) )])\n    #     for point_index, a_point in enumerate(contour_points):\n    #         if point_index % 2 == 0:\n    #             a_col, a_row = a_point\n    #             plt.scatter(x=a_col, y=a_row, c=a_color, s=0.1, marker='o', antialiased=True)\n    #\n    #     if cur_frame > initial_frame:\n    #         # draw points from each frame on an image\n    #         frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[cur_frame-1, :]\n    #         if prev_contour_points.shape[0] > frame_pred_tracking_points_contour_indices.shape[0]:\n    #             min_valid_index = frame_pred_tracking_points_contour_indices.shape[0]\n    #         else:\n    #             min_valid_index = np.argmax(frame_pred_tracking_points_contour_indices >= frame_pred_tracking_points_contour_indices[-1])\n    #             # assert prev_contour_points.shape[0] == min_valid_index\n    #         for point_index in range(min_valid_index):\n    #             if point_index % 2 == 0:\n    #                 contour_point_index = frame_pred_tracking_points_contour_indices[point_index]\n    #                 cur_x, cur_y = contour_points[contour_point_index]\n    #                 prev_x, prev_y = prev_contour_points[point_index]\n    #                 draw_arrow_plot(cur_x, cur_y, prev_x, prev_y)\n    #     prev_contour_points = contour_points\n    #\n    # plt.savefig(f\"{plot_dir}/{save_name}/combine{initial_frame}_{final_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n    # plt.close()\n\n    # -------------------------- Every two frames ------------------------------------------------\n\n    contour_cmap = pylab.get_cmap('gist_rainbow')\n    for cur_frame in tqdm(range(0, len(contour_points_path_list)-1)):\n\n        a_image = plt.imread(img_path_list[cur_frame])\n        img_height, img_width = a_image.shape[:2]\n        plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n        plt.axis('off')\n\n        two_list = [contour_points_path_list[cur_frame], contour_points_path_list[cur_frame+1]]\n        # Draw contours of two frames in one image\n        for two_frame, a_contour_points_path in enumerate(two_list):\n            contour_points = np.loadtxt(a_contour_points_path)\n            contour_points = contour_points.astype('int32')\n\n            # for point_index, a_point in enumerate(contour_points):\n            #     if point_index % 2 == 0:\n            #         a_col, a_row = a_point\n            #         if two_frame == 0:\n            #             plt.scatter(x=a_col, y=a_row, c='g', s=0.1, marker='o', antialiased=True)\n            #         else:\n            #             plt.scatter(x=a_col, y=a_row, c='r', s=0.1, marker='o', antialiased=True)\n            if two_frame == 1:\n                # draw points from each frame on an image\n                frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[cur_frame, :]\n                frame_pred_tracking_points_contour_indices = prevent_trajectory_cross(frame_pred_tracking_points_contour_indices)\n                if prev_contour_points.shape[0] > frame_pred_tracking_points_contour_indices.shape[0]:\n                    min_valid_index = frame_pred_tracking_points_contour_indices.shape[0]\n                else:\n                    min_valid_index = np.argmax(frame_pred_tracking_points_contour_indices >= frame_pred_tracking_points_contour_indices[-1])\n                    # assert prev_contour_points.shape[0] == min_valid_index\n                for point_index in range(min_valid_index):\n                    a_color = np.array([contour_cmap(1. * point_index / min_valid_index)])\n                    if point_index % 2 == 0:\n                        contour_point_index = frame_pred_tracking_points_contour_indices[point_index]\n                        cur_x, cur_y = contour_points[contour_point_index]\n                        prev_x, prev_y = prev_contour_points[point_index]\n                        # to prevent white padding due to arrows going outside the image\n                        if cur_x >= 3 and cur_x < (img_width-3) and cur_y >= 3 and cur_y < (img_height-3) and \\\n                            prev_x >= 3 and prev_x < (img_width-3) and prev_y >= 3 and prev_y < (img_height-3):\n                            draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, a_color)\n\n            prev_contour_points = contour_points\n\n        fig = plt.gcf()\n        fig.set_size_inches(10, 10)\n        plt.savefig(f\"{plot_dir}/{save_name}/{cur_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n        plt.close()", "\n\ndef manuscript_figure4_for_jelly(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices,\n                                 Matlab_GT_tracking_points_path_list, track_id_dict_list, movie_smoothedEdge, my_GT_tracking_points_path_list, arrow_plot=False):\n    '''\n    load my deep learning model's predicted points\n    load contour points\n    load images\n\n    :param dataset_folder:\n    :return:\n    '''\n\n    # --------------------------------------------- load GT and MATLAB prediction data ---------------------------------------------\n    Matlab_GT_tracking_points_all_frames = []\n    gt_tracking_points_all_frames = []\n    selected_matlab_GT_tracking_points_indices = []\n    first_gt_tracking_points_on_contour_indices = []\n    for a_index, (matlab_GT_tracking_points_path, contour_points_path, image_path) in enumerate( zip(Matlab_GT_tracking_points_path_list, contour_points_path_list, img_path_list)):\n        if len(Matlab_GT_tracking_points_path_list) == 200:\n            my_GT_tracking_points_path = my_GT_tracking_points_path_list[(a_index + 1) // 5]\n        else:\n            my_GT_tracking_points_path = my_GT_tracking_points_path_list[a_index]\n\n        # if len(Matlab_GT_tracking_points_path_list) == 200 and (a_index == 0 or (a_index+1) % 5 == 0):\n        matlab_GT_tracking_points = np.loadtxt(matlab_GT_tracking_points_path)\n        matlab_GT_tracking_points = matlab_GT_tracking_points.astype('int32')\n\n        my_gt_tracking_points = np.loadtxt(my_GT_tracking_points_path)\n        my_gt_tracking_points = my_gt_tracking_points.astype('int32')\n\n        contour_points = np.loadtxt(contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        # -------------------------------------------------------------------------------\n        # Put my_gt_tracking_points along the contour points\n        gt_tracking_points_on_contour_indices = get_closest_indices_from_gt_and_pred(my_gt_tracking_points, contour_points)\n        assert len(gt_tracking_points_on_contour_indices) == len(my_gt_tracking_points)\n        if a_index == 0:\n            first_gt_tracking_points_on_contour_indices = gt_tracking_points_on_contour_indices\n        gt_tracking_points_on_contour = contour_points[gt_tracking_points_on_contour_indices]\n        if a_index == 0:\n            init_gt_tracking_points_on_contour = gt_tracking_points_on_contour\n\n        gt_tracking_points_all_frames.append(gt_tracking_points_on_contour)\n        # -------------------------------------------------------------------------------\n        # get Matlab points along the contour points\n        if a_index == 0:\n            # find corresponding index of Matlab tracking points for every GT tracking points\n            selected_matlab_GT_tracking_points_indices = get_closest_indices_from_gt_and_pred(gt_tracking_points_on_contour, matlab_GT_tracking_points)\n            assert len(selected_matlab_GT_tracking_points_indices) == len(gt_tracking_points_on_contour)\n\n        elif a_index > 0:\n            # a_image = plt.imread(image_path)\n            selected_matlab_GT_tracking_points = matlab_GT_tracking_points[selected_matlab_GT_tracking_points_indices]\n            Matlab_GT_tracking_points_all_frames.append(selected_matlab_GT_tracking_points)\n\n    # ----------------------------- From save_correspondence_as_tracking_points_per_frame, load MATLAB points differently --------------------------------------------\n    TOTAL_NUM_POINTS = len(track_id_dict_list[0])\n    # for every interval, reset to new tracking points to prevent point convergence\n    All_MATLAB_contour_points = []\n    for movie_index, a_track_id_dict_list in enumerate(tqdm(track_id_dict_list)):\n        cur_smoothedEdge = movie_smoothedEdge[movie_index][0]\n        MATLAB_contour_points = []  # shape = (Number of Points, 2)\n        if movie_index == 0:\n            next_track_id_list = []\n            TOTAL_NUM_POINTS = len(track_id_dict_list[movie_index])\n\n            for point_index in range(TOTAL_NUM_POINTS):\n                next_track_id_list.append(track_id_dict_list[movie_index][point_index])\n                x, y = cur_smoothedEdge[point_index, :]\n                MATLAB_contour_points.append([round(x), round(y)])\n\n        else:\n            # At each frame of the movie, get corresponding coordinate for each smoothEdge index\n            for point_index in range(TOTAL_NUM_POINTS):\n                cur_track_id = next_track_id_list[point_index]\n                x, y = cur_smoothedEdge[cur_track_id, :]\n                MATLAB_contour_points.append([round(x), round(y)])\n                next_track_id_list[point_index] = a_track_id_dict_list[cur_track_id]\n        All_MATLAB_contour_points.append(MATLAB_contour_points)\n\n    # for the last frame\n    MATLAB_contour_points = []\n    movie_index = movie_index + 1\n    cur_smoothedEdge = movie_smoothedEdge[movie_index][0]\n    for point_index in range(TOTAL_NUM_POINTS):\n        cur_track_id = next_track_id_list[point_index]\n        x, y = cur_smoothedEdge[cur_track_id, :]\n        MATLAB_contour_points.append([round(x), round(y)])\n    All_MATLAB_contour_points.append(MATLAB_contour_points)\n\n    closest_matlab_point_indices = get_closest_indices_from_gt_and_pred(init_gt_tracking_points_on_contour, All_MATLAB_contour_points[0])\n    closest_matlab_point_indices = np.asarray(closest_matlab_point_indices)\n    All_MATLAB_contour_points = All_MATLAB_contour_points[4::5]\n\n    # ----------------------------------- Process prediction -----------------------------------------\n\n    # create a dict of points to track\n    initial_tracked_contour_point_indices_dict = {}\n    for a_contour_index in first_gt_tracking_points_on_contour_indices:\n        initial_tracked_contour_point_indices_dict[a_contour_index] = []\n\n    pred_tracking_points_np = np.zeros(shape=(39, len(initial_tracked_contour_point_indices_dict.keys()), 2))\n\n    # create prediction np from dense correspondences\n    for cur_frame in tqdm(range(39)):\n        a_contour_points_path = contour_points_path_list[cur_frame +1]\n        contour_points = np.loadtxt(a_contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[cur_frame, :]\n\n        for for_index, a_tracked_contour_point_index in enumerate(initial_tracked_contour_point_indices_dict.keys()):\n            if cur_frame > 0:\n                prev_tracked_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n            else:\n                prev_tracked_contour_point_index = a_tracked_contour_point_index\n\n            next_tracked_contour_point_index = frame_pred_tracking_points_contour_indices[prev_tracked_contour_point_index]\n            initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index].append(next_tracked_contour_point_index)\n            pred_tracking_points_np[cur_frame, for_index, :] = contour_points[next_tracked_contour_point_index]\n\n    # -----------------------------------------------------------------------------------\n    # --------------------------- Visualize points --------------------------------------\n    # -----------------------------------------------------------------------------------\n\n    Matlab_GT_tracking_points_np = np.asarray(Matlab_GT_tracking_points_all_frames)\n    gt_tracking_points_np = np.asarray(gt_tracking_points_all_frames[1:])\n    first_gt_tracking_points_np = np.asarray(gt_tracking_points_all_frames[0])\n\n    # sort points\n    sort_index = np.argsort(first_gt_tracking_points_np[: ,1], axis=0)\n    first_gt_tracking_points_np = first_gt_tracking_points_np[sort_index]\n    gt_tracking_points_np = gt_tracking_points_np[: ,sort_index]\n    Matlab_GT_tracking_points_np = Matlab_GT_tracking_points_np[: ,sort_index]\n\n    sort_index = np.argsort(pred_tracking_points_np[0 ,:, 1], axis=0)\n    pred_tracking_points_np = pred_tracking_points_np[:, sort_index]\n\n    assert pred_tracking_points_np.shape == Matlab_GT_tracking_points_np.shape\n    assert pred_tracking_points_np.shape == gt_tracking_points_np.shape\n\n    gt_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"lightcoral\", \"red\"])\n    pred_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"greenyellow\", \"green\"])\n    matlab_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"skyblue\", \"blue\"])\n    contour_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"dimgrey\", \"snow\"])\n    initial_frame = 0\n    final_frame = 39 # pred_tracking_points_np.shape[0]\n\n    sa_list = []\n    ca_list = []\n\n    first_contour_index = 0\n    last_contour_index = 1000\n    total_frames = final_frame - initial_frame\n\n    a_image = plt.imread(img_path_list[initial_frame])\n    image_height, image_width = a_image.shape[:2]\n    plt.imshow(a_image, cmap='gray')\n    plt.axis('off')\n\n    plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n    save_name = 'manuscript_figure4'\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n        os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    for cur_frame in tqdm(range(initial_frame, final_frame)):\n        # Draw contours of multiple frames in one image\n        # if cur_frame == initial_frame:\n        #     a_contour_points_path = contour_points_path_list[cur_frame]\n        #     contour_points = np.loadtxt(a_contour_points_path)\n        #     contour_points = contour_points.astype('int32')\n        #     contour_points = contour_points[first_contour_index:last_contour_index, :]\n        #     a_color = np.array([contour_cmap(1. * (cur_frame - initial_frame) / total_frames)])\n        #     for point_index, a_point in enumerate(contour_points):\n        #         a_col, a_row = a_point\n        #         plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s', linewidths=1, antialiased=True)\n        #\n        a_contour_points_path = contour_points_path_list[cur_frame +1]\n        contour_points = np.loadtxt(a_contour_points_path)\n        contour_points = contour_points.astype('int32')\n        # contour_points = contour_points[first_contour_index:last_contour_index,:]\n        # a_color = np.array([contour_cmap(1. * (cur_frame-initial_frame) / total_frames)])\n        # for point_index, a_point in enumerate(contour_points):\n        #     a_col, a_row = a_point\n        #     plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s',linewidths=1, antialiased=True)\n\n        if not arrow_plot:\n            a_image = plt.imread(img_path_list[cur_frame +1])\n            plt.imshow(a_image, cmap='gray')\n            plt.axis('off')\n\n        # -----------------------------------------------------------------------------------\n        for a_point_index in range(pred_tracking_points_np.shape[1]):\n            if arrow_plot:\n                cur_x, cur_y = gt_tracking_points_np[cur_frame, a_point_index, :]\n                if cur_frame == 0:\n                    prev_x, prev_y = first_gt_tracking_points_np[a_point_index, :]\n                else:\n                    prev_x, prev_y = gt_tracking_points_np[cur_frame -1, a_point_index, :]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array(gt_cmap(1. * (cur_frame -initial_frame) / total_frames)))\n\n                cur_x, cur_y = Matlab_GT_tracking_points_np[cur_frame, a_point_index, :]\n                if cur_frame > 0:\n                    prev_x, prev_y = Matlab_GT_tracking_points_np[cur_frame -1, a_point_index, :]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color= np.array(matlab_cmap(1. * (cur_frame -initial_frame) / total_frames)))\n\n                cur_x, cur_y = pred_tracking_points_np[cur_frame, a_point_index, :]\n                if cur_frame > 0:\n                    prev_x, prev_y = pred_tracking_points_np[cur_frame -1, a_point_index, :]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array(pred_cmap(1. * (cur_frame -initial_frame) / total_frames)) )\n            else:\n                # np.asarray(All_MATLAB_contour_points[cur_frame])[closest_matlab_point_indices][a_point_index]\n                cur_x, cur_y = Matlab_GT_tracking_points_np[cur_frame, a_point_index, :]\n                plt.scatter(x=cur_x, y=cur_y, c='b', s=1, marker='o', antialiased=True)\n                cur_x, cur_y = pred_tracking_points_np[cur_frame, a_point_index, :]\n                plt.scatter(x=cur_x, y=cur_y, c='g', s=1, marker='o', antialiased=True)\n                cur_x, cur_y = gt_tracking_points_np[cur_frame, a_point_index, :]\n                plt.scatter(x=cur_x, y=cur_y, c='r', s=1, marker='o', antialiased=True)\n\n        # if not arrow_plot:\n        #     fig = plt.gcf()\n        #     # fig.set_size_inches(10, 10)\n        #     plt.savefig(f\"{plot_dir}/{save_name}/gt_mech_ours_{cur_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=300)\n        #     plt.close()\n\n        # ---------------------------- Evaluate ----------------------------------\n        # spatial_accuracy_threshold = 0.25\n        # ca_threshold = 50\n        #\n        # selected_matlab_GT_tracking_points = np.asarray(All_MATLAB_contour_points[cur_frame])[closest_matlab_point_indices]  # pred_tracking_points_np[cur_frame]\n        # absolute_sa = metrics.spatial_accuracy(gt_tracking_points_on_contour, selected_matlab_GT_tracking_points, image_width, image_height, spatial_accuracy_threshold)\n        # sa_list.append(absolute_sa)\n        #\n        # # get contour points closest to the selected_matlab_GT_tracking_points\n        # selected_matlab_contour_indices = get_closest_indices_from_gt_and_pred(selected_matlab_GT_tracking_points, contour_points)\n        # ca_list.append(metrics.contour_accuracy(gt_tracking_points_on_contour_indices, selected_matlab_contour_indices, ca_threshold))\n\n\n    # -------------- Draw line plot of a few predicted points on an image --------------\n    # tracking_point_cmap = pylab.get_cmap('gist_rainbow')\n    # MAX_NUM_POINTS = pred_tracking_points_np.shape[1]  # shape is [#frames, #points, 2]\n    #\n    # for a_point_index in tqdm(range(pred_tracking_points_np.shape[1])):\n    #     col_list = pred_tracking_points_np[:, a_point_index, 0]\n    #     row_list = pred_tracking_points_np[:, a_point_index, 1]\n    #     plt.plot(col_list, row_list, c='g', marker='o', linewidth=1, markersize=1, antialiased=True) # np.array(tracking_point_cmap(1. * a_point_index / MAX_NUM_POINTS))\n    #\n    #     col_list = Matlab_GT_tracking_points_np[:, a_point_index, 0]\n    #     row_list = Matlab_GT_tracking_points_np[:, a_point_index, 1]\n    #     plt.plot(col_list, row_list, c='b', marker='o', linewidth=1, markersize=1, antialiased=True)\n    #\n    #     col_list = gt_tracking_points_np[:, a_point_index, 0]\n    #     row_list = gt_tracking_points_np[:, a_point_index, 1]\n    #     plt.plot(col_list, row_list, c='r', marker='o', linewidth=1, markersize=1, antialiased=True)\n\n    if arrow_plot:\n        fig = plt.gcf()\n        fig.set_size_inches(10, 10)\n        plt.savefig(f\"{plot_dir}/{save_name}/all_frames_trajectory_{initial_frame}_{final_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n        plt.close()\n\n\n    print('SA: ', round(mean(sa_list), 4))\n    # print('Relative: ', round(mean(rsa_list), 4))\n    # print('Temporal: ', round(mean(ta_list), 4))\n    print('CA: ', round(mean(ca_list), 4))", "\n\n\ndef manuscript_figure4(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices, Matlab_GT_tracking_points_path_list, my_GT_tracking_points_path_list, arrow_plot=False):\n    '''\n    load my deep learning model's predicted points\n    load contour points\n    load images\n\n    Can draw contours of multiple frames on the first frame\n    Draw GT points, our contour tracker's predicted points, and mechanical model's predicted points for comparison\n\n    :param dataset_folder:\n    :return:\n    '''\n\n    # --------------------------------------------- load GT and MATLAB prediction data ---------------------------------------------\n    Matlab_GT_tracking_points_all_frames = []\n    gt_tracking_points_all_frames = []\n    selected_matlab_GT_tracking_points_indices = []\n    first_gt_tracking_points_on_contour_indices = []\n    for a_index, (matlab_GT_tracking_points_path, contour_points_path, image_path) in enumerate( zip(Matlab_GT_tracking_points_path_list, contour_points_path_list, img_path_list)):\n        if len(Matlab_GT_tracking_points_path_list) == 200:\n            my_GT_tracking_points_path = my_GT_tracking_points_path_list[(a_index + 1) // 5]\n        else:\n            my_GT_tracking_points_path = my_GT_tracking_points_path_list[a_index]\n\n        # if len(Matlab_GT_tracking_points_path_list) == 200 and (a_index == 0 or (a_index+1) % 5 == 0):\n        matlab_GT_tracking_points = np.loadtxt(matlab_GT_tracking_points_path)\n        matlab_GT_tracking_points = matlab_GT_tracking_points.astype('int32')\n\n        my_gt_tracking_points = np.loadtxt(my_GT_tracking_points_path)\n        my_gt_tracking_points = my_gt_tracking_points.astype('int32')\n\n        contour_points = np.loadtxt(contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        # -------------------------------------------------------------------------------\n        # Put my_gt_tracking_points along the contour points\n        gt_tracking_points_on_contour_indices = get_closest_indices_from_gt_and_pred(my_gt_tracking_points, contour_points)\n        assert len(gt_tracking_points_on_contour_indices) == len(my_gt_tracking_points)\n        if a_index == 0:\n            first_gt_tracking_points_on_contour_indices = gt_tracking_points_on_contour_indices\n        gt_tracking_points_on_contour = contour_points[gt_tracking_points_on_contour_indices]\n        if a_index == 0:\n            init_gt_tracking_points_on_contour = gt_tracking_points_on_contour\n\n        gt_tracking_points_all_frames.append(gt_tracking_points_on_contour)\n        # -------------------------------------------------------------------------------\n        # get Matlab points along the contour points\n        if a_index == 0:\n            # find corresponding index of Matlab tracking points for every GT tracking points\n            selected_matlab_GT_tracking_points_indices = get_closest_indices_from_gt_and_pred(gt_tracking_points_on_contour, matlab_GT_tracking_points)\n            assert len(selected_matlab_GT_tracking_points_indices) == len(gt_tracking_points_on_contour)\n\n        elif a_index > 0:\n            selected_matlab_GT_tracking_points = matlab_GT_tracking_points[selected_matlab_GT_tracking_points_indices]\n            Matlab_GT_tracking_points_all_frames.append(selected_matlab_GT_tracking_points)\n\n\n    # ----------------------------------- Process prediction -----------------------------------------\n\n    # create a dict of points to track\n    initial_tracked_contour_point_indices_dict = {}\n    for a_contour_index in first_gt_tracking_points_on_contour_indices:\n        initial_tracked_contour_point_indices_dict[a_contour_index] = []\n\n    pred_tracking_points_np = np.zeros(shape=(40, len(initial_tracked_contour_point_indices_dict.keys()), 2))\n\n    # create prediction np from dense correspondences\n    for cur_frame in tqdm(range(40)):\n        a_contour_points_path = contour_points_path_list[cur_frame +1]\n        contour_points = np.loadtxt(a_contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[cur_frame, :]\n\n        for for_index, a_tracked_contour_point_index in enumerate(initial_tracked_contour_point_indices_dict.keys()):\n            if cur_frame > 0:\n                prev_tracked_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n            else:\n                prev_tracked_contour_point_index = a_tracked_contour_point_index\n\n            next_tracked_contour_point_index = frame_pred_tracking_points_contour_indices[prev_tracked_contour_point_index]\n            initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index].append(next_tracked_contour_point_index)\n            pred_tracking_points_np[cur_frame, for_index, :] = contour_points[next_tracked_contour_point_index]\n\n    # -----------------------------------------------------------------------------------\n    # --------------------------- Visualize points --------------------------------------\n    # -----------------------------------------------------------------------------------\n\n    Matlab_GT_tracking_points_np = np.asarray(Matlab_GT_tracking_points_all_frames)\n    gt_tracking_points_np = np.asarray(gt_tracking_points_all_frames[1:])\n    first_gt_tracking_points_np = np.asarray(gt_tracking_points_all_frames[0])\n\n    # sort points\n    sort_index = np.argsort(first_gt_tracking_points_np[: ,1], axis=0)\n    first_gt_tracking_points_np = first_gt_tracking_points_np[sort_index]\n    gt_tracking_points_np = gt_tracking_points_np[: ,sort_index]\n    Matlab_GT_tracking_points_np = Matlab_GT_tracking_points_np[: ,sort_index]\n\n    sort_index = np.argsort(pred_tracking_points_np[0 ,:, 1], axis=0)\n    pred_tracking_points_np = pred_tracking_points_np[:, sort_index]\n\n    assert pred_tracking_points_np.shape == Matlab_GT_tracking_points_np.shape\n    assert pred_tracking_points_np.shape == gt_tracking_points_np.shape\n\n    gt_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"lightcoral\", \"red\"])\n    pred_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"greenyellow\", \"green\"])\n    matlab_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"skyblue\", \"blue\"])\n    contour_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"dimgrey\", \"snow\"])\n    initial_frame = 0\n    final_frame = 40 # pred_tracking_points_np.shape[0]\n\n    first_contour_index = 0\n    last_contour_index = 1000\n    total_frames = final_frame - initial_frame\n\n    a_image = plt.imread(img_path_list[initial_frame])\n    plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n    plt.axis('off')\n\n    plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n    save_name = 'manuscript_figure4'\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n        os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    for cur_frame in tqdm(range(initial_frame, final_frame)):\n        # Draw contours of multiple frames in one image\n        # if cur_frame == initial_frame:\n        #     a_contour_points_path = contour_points_path_list[cur_frame]\n        #     contour_points = np.loadtxt(a_contour_points_path)\n        #     contour_points = contour_points.astype('int32')\n        #     contour_points = contour_points[first_contour_index:last_contour_index, :]\n        #     a_color = np.array([contour_cmap(1. * (cur_frame - initial_frame) / total_frames)])\n        #     for point_index, a_point in enumerate(contour_points):\n        #         a_col, a_row = a_point\n        #         plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s', linewidths=1, antialiased=True)\n        #\n        # a_contour_points_path = contour_points_path_list[cur_frame+1]\n        # contour_points = np.loadtxt(a_contour_points_path)\n        # contour_points = contour_points.astype('int32')\n        # contour_points = contour_points[first_contour_index:last_contour_index,:]\n        # a_color = np.array([contour_cmap(1. * (cur_frame-initial_frame) / total_frames)])\n        # for point_index, a_point in enumerate(contour_points):\n        #     a_col, a_row = a_point\n        #     plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s',linewidths=1, antialiased=True)\n\n        if not arrow_plot:\n            a_image = plt.imread(img_path_list[cur_frame +1])\n            plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n            plt.axis('off')\n\n        # -----------------------------------------------------------------------------------\n        for a_point_index in range(pred_tracking_points_np.shape[1]):\n            if arrow_plot:\n                cur_x, cur_y = gt_tracking_points_np[cur_frame, a_point_index, :]\n                if cur_frame == 0:\n                    prev_x, prev_y = first_gt_tracking_points_np[a_point_index, :]\n                else:\n                    prev_x, prev_y = gt_tracking_points_np[cur_frame -1, a_point_index, :]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array(gt_cmap(1. * (cur_frame -initial_frame) / total_frames)))\n\n                cur_x, cur_y = Matlab_GT_tracking_points_np[cur_frame, a_point_index, :]\n                if cur_frame > 0:\n                    prev_x, prev_y = Matlab_GT_tracking_points_np[cur_frame -1, a_point_index, :]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color= np.array(matlab_cmap(1. * (cur_frame -initial_frame) / total_frames)))\n\n                cur_x, cur_y = pred_tracking_points_np[cur_frame, a_point_index, :]\n                if cur_frame > 0:\n                    prev_x, prev_y = pred_tracking_points_np[cur_frame -1, a_point_index, :]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array(pred_cmap(1. * (cur_frame -initial_frame) / total_frames)) )\n            else:\n                cur_x, cur_y = Matlab_GT_tracking_points_np[cur_frame, a_point_index, :]\n                plt.scatter(x=cur_x, y=cur_y, c='b', s=1, marker='o', antialiased=True)\n                cur_x, cur_y = pred_tracking_points_np[cur_frame, a_point_index, :]\n                plt.scatter(x=cur_x, y=cur_y, c='g', s=1, marker='o', antialiased=True)\n                cur_x, cur_y = gt_tracking_points_np[cur_frame, a_point_index, :]\n                plt.scatter(x=cur_x, y=cur_y, c='r', s=1, marker='o', antialiased=True)\n\n        if not arrow_plot:\n            fig = plt.gcf()\n            # fig.set_size_inches(10, 10)\n            plt.savefig(f\"{plot_dir}/{save_name}/gt_mech_ours_{cur_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=300)\n            plt.close()\n\n    if arrow_plot:\n        fig = plt.gcf()\n        fig.set_size_inches(10, 10)\n        plt.savefig(f\"{plot_dir}/{save_name}/all_frames_trajectory_{initial_frame}_{final_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n        plt.close()", "\n\ndef manuscript_figure4_no_GT(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices, Matlab_GT_tracking_points_path_list):\n    '''\n    load my deep learning model's predicted points\n    load contour points\n    load images\n\n    :param dataset_folder:\n    :return:\n    '''\n\n    # ----------------------------------- Process my prediction -----------------------------------------\n\n    pred_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"greenyellow\", \"green\"])\n    matlab_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"skyblue\", \"blue\"])\n    contour_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"dimgrey\", \"snow\"])\n\n    total_frames = 5\n    for initial_frame in tqdm(range(40 - total_frames)):\n        final_frame = total_frames + initial_frame\n\n        init_contour_points = np.loadtxt(contour_points_path_list[initial_frame])\n        init_contour_points = init_contour_points.astype('int32')\n\n        # create a dict of points to track\n        initial_tracked_contour_point_indices_dict = {}\n        max_contour_points = init_contour_points.shape[0]\n        if init_contour_points.shape[0] > 1640:\n            max_contour_points = 1640\n        for a_contour_index in range(0, max_contour_points, 3):\n            initial_tracked_contour_point_indices_dict[a_contour_index] = []\n\n        pred_tracking_points_np = np.zeros(shape=(total_frames, len(initial_tracked_contour_point_indices_dict.keys()), 2))\n\n        # create prediction np from dense correspondences\n        for cur_frame in range(total_frames):\n            a_contour_points_path = contour_points_path_list[initial_frame + cur_frame +1]\n            contour_points = np.loadtxt(a_contour_points_path)\n            contour_points = contour_points.astype('int32')\n\n            frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[initial_frame + cur_frame, :]\n\n            for for_index, a_tracked_contour_point_index in enumerate(initial_tracked_contour_point_indices_dict.keys()):\n                if cur_frame > 0:\n                    prev_tracked_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n                else:\n                    prev_tracked_contour_point_index = a_tracked_contour_point_index\n\n                next_tracked_contour_point_index = frame_pred_tracking_points_contour_indices[prev_tracked_contour_point_index]\n                initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index].append(next_tracked_contour_point_index)\n                pred_tracking_points_np[cur_frame, for_index, :] = contour_points[next_tracked_contour_point_index]\n\n        # --------------------------------------------- load MATLAB prediction data ---------------------------------------------\n        # Matlab_GT_tracking_points_all_frames = []\n        # selected_matlab_GT_tracking_points_indices = []\n        # for a_index, (matlab_GT_tracking_points_path, contour_points_path, image_path) in enumerate( zip(Matlab_GT_tracking_points_path_list, contour_points_path_list, img_path_list)):\n        #\n        #     matlab_GT_tracking_points = np.loadtxt(matlab_GT_tracking_points_path)\n        #     matlab_GT_tracking_points = matlab_GT_tracking_points.astype('int32')\n        #\n        #     contour_points = np.loadtxt(contour_points_path)\n        #     contour_points = contour_points.astype('int32')\n        #\n        #     # -------------------------------------------------------------------------------\n        #     if a_index == 0:\n        #\n        #         for a_tracked_contour_point_index in initial_tracked_contour_point_indices_dict.keys():\n        #             a_contour_point = contour_points[a_tracked_contour_point_index]\n        #\n        #             # find corresponding index of Matlab tracking points for every tracked contour points\n        #             min_dist = None\n        #             match_index = None\n        #             for matlab_index, matlab_gt_point in enumerate(matlab_GT_tracking_points):\n        #                 a_dist = np.linalg.norm(matlab_gt_point - a_contour_point)\n        #                 if min_dist is None or min_dist > a_dist:\n        #                     min_dist = a_dist\n        #                     match_index = matlab_index\n        #\n        #             selected_matlab_GT_tracking_points_indices.append(match_index)\n        #         assert len(selected_matlab_GT_tracking_points_indices) == len(initial_tracked_contour_point_indices_dict.keys())\n        #\n        #     elif a_index > 0:\n        #\n        #         selected_matlab_GT_tracking_points = matlab_GT_tracking_points[selected_matlab_GT_tracking_points_indices]\n        #         Matlab_GT_tracking_points_all_frames.append(selected_matlab_GT_tracking_points)\n        #\n        # Matlab_GT_tracking_points_np = np.asarray(Matlab_GT_tracking_points_all_frames)\n\n        # -----------------------------------------------------------------------------------\n        # --------------------------- Visualize points --------------------------------------\n        # -----------------------------------------------------------------------------------\n\n        # sort points\n        # sort_index = np.argsort(Matlab_GT_tracking_points_np[0,:,0], axis=0)\n        # Matlab_GT_tracking_points_np = Matlab_GT_tracking_points_np[:,sort_index]\n\n        sort_index = np.argsort(pred_tracking_points_np[0 ,: ,0], axis=0)\n        pred_tracking_points_np = pred_tracking_points_np[:, sort_index]\n\n        # sort_index = np.argsort(init_contour_points[:,0], axis=0)\n        contour_point_indices = list(initial_tracked_contour_point_indices_dict.keys())\n        init_contour_points_np = init_contour_points[contour_point_indices, :]\n        init_contour_points_np = init_contour_points_np[sort_index]\n\n        # assert pred_tracking_points_np.shape == Matlab_GT_tracking_points_np.shape\n\n        first_contour_index = 0\n        last_contour_index = 1000\n\n        a_image = plt.imread(img_path_list[final_frame])\n        plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n        plt.axis('off')\n\n        plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n        save_name = 'manuscript_figure4_no_GT'\n\n        # prepare save folder\n        if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n            os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n        for cur_frame in range(total_frames):\n            # Draw contours of multiple frames in one image\n            # if cur_frame == initial_frame:\n            #     a_contour_points_path = contour_points_path_list[cur_frame]\n            #     contour_points = np.loadtxt(a_contour_points_path)\n            #     contour_points = contour_points.astype('int32')\n            #     contour_points = contour_points[first_contour_index:last_contour_index, :]\n            #     a_color = np.array([contour_cmap(1. * (cur_frame - initial_frame) / total_frames)])\n            #     for point_index, a_point in enumerate(contour_points):\n            #         a_col, a_row = a_point\n            #         plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s', linewidths=1, antialiased=True)\n\n            # a_contour_points_path = contour_points_path_list[initial_frame+cur_frame+1]\n            # contour_points = np.loadtxt(a_contour_points_path)\n            # contour_points = contour_points.astype('int32')\n            # contour_points = contour_points[first_contour_index:last_contour_index,:]\n            # a_color = np.array([contour_cmap(1. * (cur_frame-initial_frame) / total_frames)])\n            # for point_index, a_point in enumerate(contour_points):\n            #     a_col, a_row = a_point\n            #     plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s',linewidths=1, antialiased=True)\n\n            # -----------------------------------------------------------------------------------\n            for a_point_index in range(pred_tracking_points_np.shape[1]):\n                cur_x, cur_y = pred_tracking_points_np[cur_frame, a_point_index, :]\n                if cur_frame == 0:\n                    prev_x, prev_y = init_contour_points_np[a_point_index, :]\n                else:\n                    prev_x, prev_y = pred_tracking_points_np[cur_frame -1, a_point_index, :]\n                draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array(pred_cmap(1. * (cur_frame) / total_frames)))\n\n                # cur_x, cur_y = Matlab_GT_tracking_points_np[cur_frame, a_point_index, :]\n                # if cur_frame > 0:\n                #     prev_x, prev_y = Matlab_GT_tracking_points_np[cur_frame-1, a_point_index, :]\n                # draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color= np.array(matlab_cmap(1. * (cur_frame) / total_frames)))\n\n        fig = plt.gcf()\n        fig.set_size_inches(10, 10)\n        plt.savefig(f\"{plot_dir}/{save_name}/trajectory_{initial_frame}_{final_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n        plt.close()", "\n\n# --------------------------------\n\ndef rainbow_contour_pred_only(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices):\n    '''\n    load my deep learning model's predicted points\n    load contour points\n    load images\n\n    :param dataset_folder:\n    :return:\n    '''\n\n    # ----------------------------------- Process my prediction -----------------------------------------\n\n    pred_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"greenyellow\", \"green\"])\n    matlab_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"skyblue\", \"blue\"])\n    contour_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\", [\"dimgrey\", \"snow\"])\n\n    total_frames = 1\n    for initial_frame in tqdm(range(40 - total_frames)):\n        final_frame = total_frames + initial_frame\n\n        init_contour_points = np.loadtxt(contour_points_path_list[initial_frame])\n        init_contour_points = init_contour_points.astype('int32')\n\n        # create a dict of points to track\n        initial_tracked_contour_point_indices_dict = {}\n        max_contour_points = init_contour_points.shape[0]\n        if init_contour_points.shape[0] > 1640:\n            max_contour_points = 1640\n        for a_contour_index in range(0, max_contour_points, 3):\n            initial_tracked_contour_point_indices_dict[a_contour_index] = []\n\n        pred_tracking_points_np = np.zeros(shape=(total_frames, len(initial_tracked_contour_point_indices_dict.keys()), 2))\n\n        # create prediction np from dense correspondences\n        for cur_frame in range(total_frames):\n            a_contour_points_path = contour_points_path_list[initial_frame + cur_frame +1]\n            contour_points = np.loadtxt(a_contour_points_path)\n            contour_points = contour_points.astype('int32')\n\n            frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[initial_frame + cur_frame, :]\n\n            for for_index, a_tracked_contour_point_index in enumerate(initial_tracked_contour_point_indices_dict.keys()):\n                if cur_frame > 0:\n                    prev_tracked_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n                else:\n                    prev_tracked_contour_point_index = a_tracked_contour_point_index\n\n                next_tracked_contour_point_index = frame_pred_tracking_points_contour_indices[prev_tracked_contour_point_index]\n                initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index].append(next_tracked_contour_point_index)\n                pred_tracking_points_np[cur_frame, for_index, :] = contour_points[next_tracked_contour_point_index]\n\n      \n        # -----------------------------------------------------------------------------------\n        # --------------------------- Visualize points --------------------------------------\n        # -----------------------------------------------------------------------------------\n\n        # sort points\n        sort_index = np.argsort(pred_tracking_points_np[0 ,: ,0], axis=0)\n        pred_tracking_points_np = pred_tracking_points_np[:, sort_index]\n\n        # sort_index = np.argsort(init_contour_points[:,0], axis=0)\n        contour_point_indices = list(initial_tracked_contour_point_indices_dict.keys())\n        init_contour_points_np = init_contour_points[contour_point_indices, :]\n        init_contour_points_np = init_contour_points_np[sort_index]\n\n        a_image = plt.imread(img_path_list[final_frame])\n        plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n        plt.axis('off')\n\n        plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n        save_name = 'rainbow_contour_pred_only_figure'\n        cm = pylab.get_cmap('gist_rainbow')\n\n        # prepare save folder\n        if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n            os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n        first_contour_index = 0\n        last_contour_index = 1000\n\n        for cur_frame in range(total_frames):\n            # Draw contours of multiple frames in one image\n            if cur_frame == 0:\n                a_contour_points_path = contour_points_path_list[initial_frame+cur_frame]\n                contour_points = np.loadtxt(a_contour_points_path)\n                contour_points = contour_points.astype('int32')\n                contour_points = contour_points[first_contour_index:last_contour_index, :]\n                a_color = np.array([contour_cmap(1. * (cur_frame) / total_frames)])\n                for point_index, a_point in enumerate(contour_points):\n                    a_col, a_row = a_point\n                    plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s', linewidths=1, antialiased=True)\n\n            # a_contour_points_path = contour_points_path_list[initial_frame+cur_frame+1]\n            # contour_points = np.loadtxt(a_contour_points_path)\n            # contour_points = contour_points.astype('int32')\n            # contour_points = contour_points[first_contour_index:last_contour_index,:]\n            # a_color = np.array([contour_cmap(1. * (cur_frame+1) / total_frames)])\n            # for point_index, a_point in enumerate(contour_points):\n            #     a_col, a_row = a_point\n            #     plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s',linewidths=1, antialiased=True)\n\n            # -----------------------------------------------------------------------------------\n            if cur_frame == (total_frames-1):\n                for a_point_index in range(pred_tracking_points_np.shape[1]):\n                    cur_x, cur_y = pred_tracking_points_np[cur_frame, a_point_index, :]\n                    if cur_frame == 0:\n                        prev_x, prev_y = init_contour_points_np[a_point_index, :]\n                    else:\n                        prev_x, prev_y = pred_tracking_points_np[cur_frame -1, a_point_index, :]\n\n                    # -----\n                    # a_color = np.array([cm(1. * a_point_index / pred_tracking_points_np.shape[1])])\n                    # plt.scatter(x=cur_x, y=cur_y, c=a_color, s=1, marker='s',linewidths=1, antialiased=True)\n                    # plt.scatter(x=prev_x, y=prev_y, c=a_color, s=1, marker='s',linewidths=1, antialiased=True)\n                    # ------\n                    draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array(pred_cmap(1. * (cur_frame) / total_frames)))\n\n        fig = plt.gcf()\n        fig.set_size_inches(10, 10)\n        plt.savefig(f\"{plot_dir}/{save_name}/trajectory_{initial_frame}_{final_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n        plt.close()", "\n\n\n# -----------------------------------------------------------------------------------------------------\n\ndef manuscript_figure5(root_generated_path, dataset_folder, img_path_list, contour_points_path_list, pred_tracking_points_contour_indices):\n    '''\n\n    Draw trajectory arrows for all contour points between two end-points\n    Draw morphodynamics protrusion map\n\n    :param dataset_folder:\n    :param contour_points_path_list:\n    :param pred_tracking_points_contour_indices:\n    :return:\n    '''\n    plot_dir = f\"{root_generated_path}{dataset_folder}/\"\n    save_name = 'manuscript_figure5'\n\n    # prepare save folder\n    if not os.path.exists(f\"{plot_dir}/{save_name}/\"):\n        os.makedirs(f\"{plot_dir}/{save_name}/\")\n\n    # -------------------------- Combine many predictions -----------------------\n    tracking_point_cmap = pylab.get_cmap('gist_rainbow')\n    initial_frame = 0\n    final_frame = 4\n    total_frames = final_frame - initial_frame\n\n    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n    a_image = plt.imread(img_path_list[initial_frame])\n    plt.imshow(a_image, cmap='gray', vmin=0, vmax=1)\n    plt.axis('off')\n    # plt.xticks(np.arange(0,total_frames + 1, 5), fontsize=16)\n    # plt.yticks(fontsize=16)\n    # plt.xlabel('Number of Frame')\n    # plt.ylabel('Index of Tracked Point')\n\n    # ----------------------- initialize -----------------------\n    contour_points = np.loadtxt(contour_points_path_list[0])\n    contour_points = contour_points.astype('int32')\n    initial_tracked_contour_point_indices_dict = {}\n    # choose the range of contour points to track here\n    for a_index in range(contour_points.shape[0]):\n        if a_index > 375 and a_index < 635:\n            initial_tracked_contour_point_indices_dict[a_index] = []\n    # initial_tracked_contour_point_indices_dict = {740:[],746:[],750:[],755:[],760:[],765:[],770:[],775:[],780:[],785:[], 790:[],795:[],796:[]}\n    heatmap_np = np.zeros(shape=(len(initial_tracked_contour_point_indices_dict), total_frames) )\n\n    for cur_frame in tqdm(range(initial_frame, final_frame+1)):\n        # Draw contours of multiple frames in one image\n        a_contour_points_path = contour_points_path_list[cur_frame]\n        contour_points = np.loadtxt(a_contour_points_path)\n        contour_points = contour_points.astype('int32')\n\n        if cur_frame > initial_frame:\n            # calculate normal vector of contour points\n            # Compute tangent by central differences. You can use a closed form tangent if you have it.\n            # referred https://stackoverflow.com/questions/66676502/how-can-i-move-points-along-the-normal-vector-to-a-curve-in-python\n            two_left_contour_points = np.roll(prev_contour_points, shift=-2, axis=0)\n            tangent_vectors = (two_left_contour_points - prev_contour_points) / 2\n\n            normal_vectors = np.stack([-tangent_vectors[:-2, 1], tangent_vectors[:-2, 0]], axis=-1)  # ignore last two indices due to shift above\n            # repeated_normal_vectors = tf.repeat( normal_vectors, pred_offsets.shape[0], axis=0)\n            unit_normal_vectors = normal_vectors / (np.expand_dims(np.linalg.norm(normal_vectors, axis=-1), -1) + 0.0000001)\n            # put back first and last index that were shifted\n            unit_normal_vectors = np.append(unit_normal_vectors, [[0,0]], axis=0)\n            unit_normal_vectors = np.insert(unit_normal_vectors, 0, [0,0], axis=0)\n\n            # plot normal vectors for verification\n            # if cur_frame == initial_frame:\n            #     a_contour_points_path = contour_points_path_list[cur_frame]\n            #     contour_points = np.loadtxt(a_contour_points_path)\n            #     contour_points = contour_points.astype('int32')\n            #     for point_index, a_point in enumerate(contour_points):\n            #         a_color = np.array([tracking_point_cmap(1. * point_index / len(contour_points))])\n            #         a_col, a_row = a_point\n            #         # plt.scatter(x=a_col, y=a_row, c=a_color, s=1, marker='s', linewidths=1, antialiased=True)\n            #         delta_x, delta_y = unit_normal_vectors[point_index,:]\n            #         draw_arrow_plot(a_col+delta_x, a_row+delta_y, a_col, a_row, arrow_color=a_color )\n\n        frame_pred_tracking_points_contour_indices = pred_tracking_points_contour_indices[cur_frame - 1, :]\n\n        for for_index, a_tracked_contour_point_index in enumerate(initial_tracked_contour_point_indices_dict.keys()):\n\n            if cur_frame > initial_frame:\n                prev_tracked_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n            else:\n                prev_tracked_contour_point_index = a_tracked_contour_point_index\n\n            next_tracked_contour_point_index = frame_pred_tracking_points_contour_indices[prev_tracked_contour_point_index]\n            initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index].append(next_tracked_contour_point_index)\n\n            # calculate velocity with respect to the normal vector\n            if cur_frame > initial_frame:\n                cur_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-1]\n                prev_contour_point_index = initial_tracked_contour_point_indices_dict[a_tracked_contour_point_index][-2]\n                cur_x, cur_y = contour_points[cur_contour_point_index]\n                prev_x, prev_y = prev_contour_points[prev_contour_point_index]\n\n                delta_x = cur_x - prev_x\n                delta_y = cur_y - prev_y\n\n                if for_index % 4 == 0:\n                    draw_arrow_plot(cur_x, cur_y, prev_x, prev_y, arrow_color=np.array([tracking_point_cmap(1. * for_index / len(initial_tracked_contour_point_indices_dict.keys()))]))\n\n                # normalize delta\n                # a_norm = np.linalg.norm([delta_x, delta_y])\n                # norm_delta_x = delta_x / (a_norm  + 0.0000001)\n                # norm_delta_y = delta_y / (a_norm  + 0.0000001)\n\n                delta_along_normal_vector = np.dot([delta_x, delta_y] , unit_normal_vectors[prev_contour_point_index])\n\n                cutoff = 5\n                delta_along_normal_vector = np.clip(delta_along_normal_vector, -cutoff, cutoff)\n\n                heatmap_np[for_index, cur_frame-1] = delta_along_normal_vector # protrusion/retraction velocity\n\n        prev_contour_points = contour_points\n\n\n    # protrusion_map_np = load_protrusion_colormap()\n    protrusion_map_cmap = sns.color_palette(\"coolwarm\", as_cmap=True) # ListedColormap(protrusion_map_np)\n    # plt.imshow(heatmap_np, cmap=protrusion_map_cmap, interpolation='nearest', aspect='auto')\n\n    # cbar = plt.colorbar()\n    # cbar.ax.tick_params(labelsize=16)\n\n    fig = plt.gcf()\n    fig.set_size_inches(20, 15)\n    plt.savefig(f\"{plot_dir}/{save_name}/morphodynamics_{initial_frame}_{final_frame}.png\", bbox_inches=\"tight\", pad_inches=0, dpi=400)\n    plt.close()", ""]}
{"filename": "src/data_conversion_scripts/conversion_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2021 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\"Shared utils for converting datasets.\"\"\"\n", "\"\"\"Shared utils for converting datasets.\"\"\"\n\nimport numpy as np\nimport tensorflow as tf\n\nMIDDLEBURY_MAGIC = 202021.25\n\n\ndef generate_sharded_filenames(filename):\n  name, num_shards = filename.split('@')\n  filenames = []\n  for num in range(int(num_shards)):\n    filenames.append(name + f'@{num}.tfrecord')\n  return filenames", "def generate_sharded_filenames(filename):\n  name, num_shards = filename.split('@')\n  filenames = []\n  for num in range(int(num_shards)):\n    filenames.append(name + f'@{num}.tfrecord')\n  return filenames\n\n\ndef bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))", "def bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef read_flow(filename):\n  \"\"\"Read .flo file in Middlebury format.\n\n  See http://vision.middlebury.edu/flow/code/flow-code/README.txt for more info.\n\n  Args:\n    filename: str\n  Returns:\n    numpy array with flow\n  \"\"\"\n  with open(filename, 'rb') as f:\n    magic_no = np.fromfile(f, np.float32, count=1)\n    if magic_no != MIDDLEBURY_MAGIC:\n      raise ValueError('Magic no. incorrect. {} is invalid.'.format(filename))\n    else:\n      width = np.fromfile(f, np.int32, count=1)\n      height = np.fromfile(f, np.int32, count=1)\n      data = np.fromfile(f, np.float32, count=2 * int(width) * int(height))\n      return np.resize(data, (int(height), int(width), 2))", "def read_flow(filename):\n  \"\"\"Read .flo file in Middlebury format.\n\n  See http://vision.middlebury.edu/flow/code/flow-code/README.txt for more info.\n\n  Args:\n    filename: str\n  Returns:\n    numpy array with flow\n  \"\"\"\n  with open(filename, 'rb') as f:\n    magic_no = np.fromfile(f, np.float32, count=1)\n    if magic_no != MIDDLEBURY_MAGIC:\n      raise ValueError('Magic no. incorrect. {} is invalid.'.format(filename))\n    else:\n      width = np.fromfile(f, np.int32, count=1)\n      height = np.fromfile(f, np.int32, count=1)\n      data = np.fromfile(f, np.float32, count=2 * int(width) * int(height))\n      return np.resize(data, (int(height), int(width), 2))", "\n\ndef write_flow(filename, uv):\n  \"\"\"Write optical flow to file using .flo Middlebury format.\n\n  Args:\n    filename: str, where to write .flo file\n    uv: np.array, predicted flow in u, v coordinates (horizontal, vertical)\n  \"\"\"\n  n_bands = 2\n  tag_char = np.array([MIDDLEBURY_MAGIC], np.float32)\n  assert uv.ndim == 3\n  assert uv.shape[2] == 2\n  u = uv[:, :, 0]\n  v = uv[:, :, 1]\n  assert u.shape == v.shape\n  height, width = u.shape\n  with open(filename, 'wb') as f:\n    f.write(tag_char)\n    np.array(width).astype(np.int32).tofile(f)\n    np.array(height).astype(np.int32).tofile(f)\n    flow = np.zeros((height, width * n_bands))\n    flow[:, np.arange(width) * 2] = u\n    flow[:, np.arange(width) * 2 + 1] = v\n    flow.astype(np.float32).tofile(f)", ""]}
{"filename": "src/data_conversion_scripts/convert_custom_to_tfrecords.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023 Junbong Jang.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis script converts Custom Data ( images, segmentation mask, tracking points ) into the TFRecords format.\n\"\"\"", "This script converts Custom Data ( images, segmentation mask, tracking points ) into the TFRecords format.\n\"\"\"\n\nimport os\nfrom absl import app\nfrom absl import flags\nimport imageio\nimport numpy as np\nimport tensorflow as tf\nfrom src.data_conversion_scripts import conversion_utils", "import tensorflow as tf\nfrom src.data_conversion_scripts import conversion_utils\nimport cv2\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string('data_dir', None, 'Dataset folder.')\nflags.DEFINE_string('output_dir', '', 'Location to export to.')\nflags.DEFINE_integer('shard', 0, 'Which shard this is.')\nflags.DEFINE_integer('num_shards', 100, 'How many total shards there are.')\nflags.DEFINE_integer('seq_len', 2, 'How long is the sequence?')", "flags.DEFINE_integer('num_shards', 100, 'How many total shards there are.')\nflags.DEFINE_integer('seq_len', 2, 'How long is the sequence?')\nflags.DEFINE_string('img_format', 'jpg', 'image format')\nflags.DEFINE_string('mode', 'sparse', 'sparse or dense video frames')\nflags.DEFINE_string('data_split', None, 'training or test video frames')\n\n\ndef get_image_name(image_path, file_format):\n    return os.path.basename(image_path).replace(f\".{file_format}\", '')\n", "\n\ndef find_least_tracking_points(tracking_points_folders):\n    least_tracking_points = None\n    for a_folder_path in tracking_points_folders:\n        tracking_points_path_list = tf.io.gfile.glob(a_folder_path + '/*.txt')\n        a_tracking_point = np.loadtxt(tracking_points_path_list[0], dtype=np.int32)\n        if least_tracking_points is None or a_tracking_point.shape[0] < least_tracking_points:\n            least_tracking_points = a_tracking_point.shape[0]\n\n    assert least_tracking_points is not None\n\n    return least_tracking_points", "\n\ndef create_list_of_mask_vector(CUSTOM_MAX_TOTAL_SEG_POINTS_NUM, cur_seg_points, next_seg_points):\n    '''\n    For masking cross-attention\n\n    :param CUSTOM_MAX_TOTAL_SEG_POINTS_NUM:\n    :param cur_seg_points:\n    :return:\n    '''\n    if cur_seg_points.shape[0] > CUSTOM_MAX_TOTAL_SEG_POINTS_NUM:\n        mask_vector = np.ones(CUSTOM_MAX_TOTAL_SEG_POINTS_NUM)\n    else:\n        # Create a mask vector with 0 for zero padded seg_points\n        mask_vector = np.ones(cur_seg_points.shape[0])\n        mask_vector = np.pad(mask_vector, (0, CUSTOM_MAX_TOTAL_SEG_POINTS_NUM - cur_seg_points.shape[0]), 'constant', constant_values=(0))\n    mask_vector = np.expand_dims(mask_vector, axis=-1)\n    mask_vector = mask_vector.astype(np.int32)\n    list_of_mask_vector = [mask_vector]\n\n    # -------------------------------------------------------------------------\n    # total_offset = 40\n    # assert cur_seg_points.shape[0] > total_offset\n    # list_of_mask_vector = []\n    # # set mask to 1 for left 20 pixels and right 20 pixels\n    # for point_index in range(CUSTOM_MAX_TOTAL_SEG_POINTS_NUM):\n    #     # print(point_index, cur_seg_points.shape, next_seg_points.shape)\n    #     if point_index < total_offset // 2:\n    #         # print('case 1@@')\n    #         one_vector = np.ones(total_offset // 2 + point_index)\n    #         mask_vector = np.pad(one_vector, (0, CUSTOM_MAX_TOTAL_SEG_POINTS_NUM - one_vector.shape[0]), 'constant', constant_values=(0))\n    #\n    #     elif total_offset // 2 <= point_index < cur_seg_points.shape[0] :\n    #         # print('case 2@@')\n    #         cur_point_index = point_index - total_offset // 2\n    #         one_vector = np.ones(total_offset)\n    #         mask_vector = np.pad(one_vector, (0, CUSTOM_MAX_TOTAL_SEG_POINTS_NUM - one_vector.shape[0]), 'constant', constant_values=(0))\n    #         mask_vector = np.roll(mask_vector, cur_point_index)\n    #\n    #     elif point_index >= cur_seg_points.shape[0]:\n    #         # print('case 3@@')\n    #         mask_vector = np.zeros(CUSTOM_MAX_TOTAL_SEG_POINTS_NUM)\n    #\n    #     mask_vector[next_seg_points.shape[0]:] = 0\n    #     mask_vector = np.expand_dims(mask_vector, axis=-1)\n    #     mask_vector = mask_vector.astype(np.int32)\n    #     list_of_mask_vector.append(mask_vector)\n\n    return list_of_mask_vector", "\n\ndef create_seq_from_list(tracking_points_path_list, seq_len, data_split, mode):\n    total_tracking_points_path_num = len(tracking_points_path_list)\n\n    if data_split == 'training' and mode == 'dense':\n        interval_len = 5\n        tracking_points_seq = [[], []]\n        for start_index in range(interval_len):\n            temp_path_list = tracking_points_path_list[start_index:total_tracking_points_path_num - interval_len + start_index + 1:interval_len]\n            temp_path_num = len(temp_path_list)\n            temp_seq = [temp_path_list[seq_index:temp_path_num + seq_index + 1 - seq_len] for seq_index in\n                        range(seq_len)]  # e.g) [[1,6,...,191], [6,11,16,...,196]]\n            tracking_points_seq[0] = tracking_points_seq[0] + temp_seq[0]\n            tracking_points_seq[1] = tracking_points_seq[1] + temp_seq[1]\n        # returns [[1,6,...,191, 2,7,12,...,192, 3,8,13,...,193, 4,...], [6,11,16,...,196, 7,12,17,...,197, 8,13,...,198, 9,...]]\n    else:\n        # tracking_points_seq = zip(tracking_points_path_list[:-1], tracking_points_path_list[1:])\n        tracking_points_seq = [tracking_points_path_list[seq_index:total_tracking_points_path_num + seq_index + 1 - seq_len] for seq_index in\n                               range(seq_len)]  # i.e) if seq_len == 2, [ [1,2,3,4,...198,199], [2,3,4,5,...,199,200] ]\n\n    return tracking_points_seq", "\n\ndef convert_dataset(seq_len, mode, data_split):\n    \"\"\"\n    Convert the data to the TFRecord format.\n    dataset is images and tracking points label, but no segmentation label\n\n    \"\"\"\n\n    def write_records(data_path_list, output_folder, num_least_tracking_points, CUSTOM_MAX_TOTAL_SEG_POINTS_NUM):\n        \"\"\"\n        Takes in list: 200 x [((im1_path, ..., imn_path), (seg1_path, ..., seqn_path), (track1_path, ..., trackn_path))]\n        and writes records.\n        \"\"\"\n\n        # Reading ppm and flo can fail on network filesystem, so copy to tmpdir first.\n        tmpdir = '/tmp/convert_custom_to_tfrecords'\n        if not os.path.exists(tmpdir):\n            os.mkdir(tmpdir)\n\n        filenames = conversion_utils.generate_sharded_filenames(\n            os.path.join(output_folder, 'custom@{}'.format(FLAGS.num_shards)))\n        with tf.io.TFRecordWriter(filenames[FLAGS.shard]) as record_writer:\n            total = len(data_path_list)\n            images_per_shard = total // FLAGS.num_shards\n            start = images_per_shard * FLAGS.shard\n            end = start + images_per_shard\n            # Account for num images not being divisible by num shards.\n            if FLAGS.shard == FLAGS.num_shards - 1:\n                data_path_list = data_path_list[start:]\n            else:\n                data_path_list = data_path_list[start:end]\n\n            tf.compat.v1.logging.info('Writing %d images per shard', images_per_shard)\n            tf.compat.v1.logging.info('Writing range %d to %d of %d total.', start, end, total)\n\n            # create temp path\n            seq_len = len(data_path_list[0][0])\n            temp_img_path_list = []\n            temp_seg_points_path_list = []\n            temp_tracking_point_path_list = []\n            for seq_index in range(seq_len):\n                temp_img_path_list.append( os.path.join(tmpdir, f'img{seq_index}.{FLAGS.img_format}') )\n                temp_seg_points_path_list.append( os.path.join(tmpdir, f'segmentation_points{seq_index}.txt') )\n                temp_tracking_point_path_list.append( os.path.join(tmpdir, f'tracking_point{seq_index}.txt') )\n\n            max_total_seg_points_num = 0\n            # each element of data_path_list consists of seq_len of images and tracking_points\n\n            for i, (images, seg_points, tracking_points) in enumerate(data_path_list):\n                img_data_list = []\n                seg_points_data_list = []\n                tracking_point_data_list = []\n\n                for seq_index in range(seq_len):\n                    # --------------- Images --------------\n                    if os.path.exists(temp_img_path_list[seq_index]):\n                        os.remove(temp_img_path_list[seq_index])\n                    tf.io.gfile.copy(images[seq_index], temp_img_path_list[seq_index])\n                    a_image = imageio.imread( temp_img_path_list[seq_index] )\n                    if a_image.dtype == np.uint16:\n                        # convert to uint8\n                        a_image = cv2.convertScaleAbs(a_image, alpha=(255.0/65535.0))\n\n                    img_data_list.append( a_image )\n\n                    if len(img_data_list[seq_index].shape) == 2:\n                        img_data_list[seq_index] = np.expand_dims(img_data_list[seq_index] , axis=-1)\n                        img_data_list[seq_index] = np.repeat(img_data_list[seq_index], 3, axis=-1)\n                    elif img_data_list[seq_index].shape[-1] == 4:\n                        img_data_list[seq_index] = img_data_list[seq_index][:, :, 0:3]\n\n                    # --------------- Segmentation points -----------------\n                    if os.path.exists(temp_seg_points_path_list[seq_index]):\n                        os.remove(temp_seg_points_path_list[seq_index])\n\n                    tf.io.gfile.copy(seg_points[seq_index], temp_seg_points_path_list[seq_index])\n                    cur_seg_points = np.loadtxt(temp_seg_points_path_list[seq_index], dtype=np.int32)  # convert to uint8 ~~~~~~~~~\n\n                    # zero padding to make every tensor's length the same\n                    if cur_seg_points.shape[0] > CUSTOM_MAX_TOTAL_SEG_POINTS_NUM:\n                        print('number of seg points is greater than CUSTOM_MAX_TOTAL_SEG_POINTS_NUM', cur_seg_points.shape[0])\n                        padded_seg_points = cur_seg_points[:CUSTOM_MAX_TOTAL_SEG_POINTS_NUM,:]\n                    else:\n                        padded_seg_points = np.pad(cur_seg_points, ((0, CUSTOM_MAX_TOTAL_SEG_POINTS_NUM - cur_seg_points.shape[0]), (0,0)), 'constant', constant_values=(-100))\n\n                    # create masking\n                    assert seq_len == 2\n                    if seq_index == 0:\n                        next_seq_index = 1\n\n                    elif seq_index == 1:\n                        next_seq_index = 0\n                    else:\n                        raise ValueError('@@@ seg_points')\n                    if os.path.exists(temp_seg_points_path_list[next_seq_index]):\n                        os.remove(temp_seg_points_path_list[next_seq_index])\n\n                    tf.io.gfile.copy(seg_points[next_seq_index], temp_seg_points_path_list[next_seq_index])\n                    next_seg_points = np.loadtxt(temp_seg_points_path_list[next_seq_index], dtype=np.int32)\n\n                    list_of_mask_vector = create_list_of_mask_vector(CUSTOM_MAX_TOTAL_SEG_POINTS_NUM, cur_seg_points, next_seg_points)\n                    concat_seg_points = np.concatenate((padded_seg_points, *list_of_mask_vector), axis=1)\n                    seg_points_data_list.append( concat_seg_points )\n\n                    assert np.int32 == concat_seg_points.dtype\n                    assert concat_seg_points.shape == (CUSTOM_MAX_TOTAL_SEG_POINTS_NUM, 3)\n                    \n                    # --------------- Tracking points ----------------\n                    if os.path.exists(temp_tracking_point_path_list[seq_index]):\n                        os.remove(temp_tracking_point_path_list[seq_index])\n\n                    tf.io.gfile.copy(tracking_points[seq_index], temp_tracking_point_path_list[seq_index])\n                    a_tracking_point = np.loadtxt(temp_tracking_point_path_list[seq_index], dtype=np.int32)\n                    a_tracking_point = np.expand_dims(a_tracking_point, axis=-1)\n                    tracking_point_data_list.append( a_tracking_point )\n\n                    assert a_tracking_point.shape[1] == 1\n                    \n                    # --------------------------------\n                    # check data shapes\n                    if seq_index > 0:\n                        assert height == img_data_list[seq_index].shape[0]\n                        assert width == img_data_list[seq_index].shape[1]\n                        assert total_tracking_point_num == len(tracking_point_data_list[seq_index])\n\n                    height = img_data_list[seq_index].shape[0]\n                    width = img_data_list[seq_index].shape[1]\n                    total_tracking_point_num = len(tracking_point_data_list[seq_index])\n                    total_seg_points_num = len(seg_points_data_list[seq_index])\n                    if max_total_seg_points_num < total_seg_points_num:\n                        max_total_seg_points_num = total_seg_points_num\n\n                # ------------------------------\n                # partition tracking points such that each partition has num_least_tracking_points\n                partitioned_tracking_point_data_list = []\n                num_all_tracking_points = tracking_point_data_list[0].shape[0]\n                if num_all_tracking_points > num_least_tracking_points:\n                    # print('partition!')\n                    for index in range(0, num_all_tracking_points, num_least_tracking_points):  # iterate floor(num_all / num_least) + 1 times\n                        one_partition_tracking_point_data_list = []\n                        for a_tracking_point_data in tracking_point_data_list:  # iterate seq_length time\n                            if num_all_tracking_points - index < num_least_tracking_points:\n                                indexed_tracking_point_data = a_tracking_point_data[-num_least_tracking_points:]\n                            else:\n                                indexed_tracking_point_data = a_tracking_point_data[index:index + num_least_tracking_points]\n\n                            assert indexed_tracking_point_data.shape[0] == num_least_tracking_points\n                            one_partition_tracking_point_data_list.append(indexed_tracking_point_data)\n\n                        partitioned_tracking_point_data_list.append(one_partition_tracking_point_data_list)\n\n                else:\n                    partitioned_tracking_point_data_list = [tracking_point_data_list]\n\n                # ------------------------------\n                # Case 1: Use only one partition of tracked points since supervision is not done\n                a_partitioned_tracking_point_data_list = partitioned_tracking_point_data_list[0]\n\n                # Case 2: uncomment below and then tab below? if more tracking points need to be used\n                # for a_partitioned_tracking_point_data_list in partitioned_tracking_point_data_list:\n\n                # save data\n                feature = {\n                    'height': conversion_utils.int64_feature(height),\n                    'width': conversion_utils.int64_feature(width),\n                }\n\n                for seq_index in range(seq_len):\n                    feature.update({\n                        f'image{seq_index}_path': conversion_utils.bytes_feature(str.encode(images[seq_index])),\n                        f'segmentation_points{seq_index}_path': conversion_utils.bytes_feature(str.encode(seg_points[seq_index])),\n                        f'tracking_points{seq_index}_path': conversion_utils.bytes_feature(str.encode(tracking_points[seq_index])),\n                    })\n\n                example = tf.train.SequenceExample(\n                    context=tf.train.Features(feature=feature),\n                    feature_lists=tf.train.FeatureLists(\n                        feature_list={\n                            'images':\n                                tf.train.FeatureList(feature=[conversion_utils.bytes_feature(image_data.tobytes()) for image_data in img_data_list ]),\n                            'segmentation_points':\n                                tf.train.FeatureList(feature=[conversion_utils.bytes_feature(seg_points_data.tobytes()) for seg_points_data in seg_points_data_list ]),\n                            'tracking_points':\n                                tf.train.FeatureList(feature=[conversion_utils.bytes_feature(tracking_point_data.tobytes()) for tracking_point_data in a_partitioned_tracking_point_data_list ])\n                        }))\n\n\n                # log saving\n                if i % 10 == 0:\n                    tf.compat.v1.logging.info('Writing %d out of %d total.', i, len(data_path_list))\n                record_writer.write(example.SerializeToString())\n\n            print('max_total_seg_points_num', max_total_seg_points_num)\n\n        tf.compat.v1.logging.info('Saved results to %s', FLAGS.output_dir)\n\n        # ------------------------------------------- Write Records End -------------------------------------------\n\n    if not tf.io.gfile.exists(FLAGS.output_dir):\n        tf.io.gfile.mkdir(FLAGS.output_dir)\n\n    split_folder = os.path.join(FLAGS.output_dir, data_split)\n    if not tf.io.gfile.exists(split_folder):\n        tf.io.gfile.mkdir(split_folder)\n\n    output_folder = os.path.join(FLAGS.output_dir, data_split)\n    if not tf.io.gfile.exists(output_folder):\n        tf.io.gfile.mkdir(output_folder)\n\n    data_folder_path = os.path.join(FLAGS.data_dir, data_split)\n\n    image_folders = sorted(tf.io.gfile.glob( data_folder_path + f'/*/images'))\n    seg_points_folders = sorted(tf.io.gfile.glob(data_folder_path + f'/*/contour_points'))\n    tracking_points_folders = sorted(tf.io.gfile.glob(data_folder_path + f'/*/tracked_points_in_contour_indices'))\n\n    assert len(image_folders) == len(tracking_points_folders)\n    assert len(image_folders) == len(seg_points_folders)\n\n    data_list = []\n    for image_folder_path, seg_points_folder_path, tracking_points_folder_path in zip(image_folders, seg_points_folders, tracking_points_folders):\n        # get image path\n        image_path_list = tf.io.gfile.glob(image_folder_path + f\"/*.{FLAGS.img_format}\")\n        sort_by_frame_index = lambda x: int( os.path.basename(x).split('_')[-1].split('.')[0])  # MARS-Net, Jellyfish\n        # sort_by_frame_index = lambda x: int( os.path.basename(x)[-8:].split('.')[0] )  # HACKS\n        image_path_list = sorted(image_path_list, key=sort_by_frame_index)\n\n        # get segmentation points path\n        seg_points_path_list = tf.io.gfile.glob(seg_points_folder_path + f\"/*.txt\")\n        seg_points_path_list = sorted(seg_points_path_list, key=sort_by_frame_index)\n\n        # get their names\n        image_name_list = []\n        seg_name_list = []\n        for a_image_path, cur_seg_points_path in zip(image_path_list, seg_points_path_list):\n            image_name_list.append(get_image_name(a_image_path, FLAGS.img_format))\n            seg_name_list.append(get_image_name(cur_seg_points_path, 'txt'))\n\n        # get tracking points path\n        tracking_points_path_list = tf.io.gfile.glob(tracking_points_folder_path + '/*.txt')\n        tracking_points_path_list = sorted(tracking_points_path_list, key=sort_by_frame_index)\n\n        # --------------------------------------------------------------------------------------\n        expanded_tracking_points_path_list = []\n        # assert len(tracking_points_path_list) == 41\n        if data_split == 'test_dense' and len(tracking_points_path_list) == 41:\n            print('collect a consecutive frames for dense prediction')\n            for path_index, a_path in enumerate(tracking_points_path_list):\n                if path_index == 0:\n                    expanded_tracking_points_path_list.append(a_path)\n                    expanded_tracking_points_path_list.append(a_path)\n                    expanded_tracking_points_path_list.append(a_path)\n                    expanded_tracking_points_path_list.append(a_path)\n                elif path_index >= len(tracking_points_path_list)-1:\n                    expanded_tracking_points_path_list.append(a_path)\n                else:\n                    expanded_tracking_points_path_list.append(a_path)\n                    expanded_tracking_points_path_list.append(a_path)\n                    expanded_tracking_points_path_list.append(a_path)\n                    expanded_tracking_points_path_list.append(a_path)\n                    expanded_tracking_points_path_list.append(a_path)\n            tracking_points_path_list = expanded_tracking_points_path_list\n            assert len(tracking_points_path_list) == 200\n\n            selected_image_path_list = image_path_list\n            selected_seg_points_path_list = seg_points_path_list\n        else:\n            # to predict on sparse frames given 41 tracking points\n            # only select images and segmentations that has the corresponding labeled tracking points\n            selected_image_path_list = []\n            selected_seg_points_path_list = []\n            for a_contour_point_path, a_tracking_point_path in zip(seg_points_path_list, tracking_points_path_list):\n                # get filenames of tracking points\n                tracking_point_filename = get_image_name(a_tracking_point_path, 'txt')\n                # query_image_name = 'img' + tracking_point_filename[-4:]  # HACKS\n                \n                # get images with the same filenames\n                # a_index = image_name_list.index(query_image_name)  # HACKS\n                a_index = image_name_list.index(tracking_point_filename)  # MARS-Net\n                selected_image_path_list.append(image_path_list[a_index])\n\n                a_index = seg_name_list.index(tracking_point_filename)\n                selected_seg_points_path_list.append(seg_points_path_list[a_index])\n\n        # -------------------------------------------------------------------------------\n        new_tracking_points_seq = create_seq_from_list(tracking_points_path_list, seq_len, data_split, mode)\n        new_image_seq = create_seq_from_list(selected_image_path_list, seq_len, data_split, mode)\n        new_seg_seq = create_seq_from_list(selected_seg_points_path_list, seq_len, data_split, mode)\n        # for training with shuffled sequence\n        # assert 'training' == data_split\n\n        # @@@@@@@@@@ for backward tracking test set, reverse sequence\n        # assert seq_len == 2\n        # assert ('test' in data_split) == True\n        # new_tracking_points_seq[0].reverse()\n        # new_tracking_points_seq[1].reverse()\n        # new_image_seq[0].reverse()\n        # new_image_seq[1].reverse()\n        # new_seg_seq[0].reverse()\n        # new_seg_seq[1].reverse()\n\n        # -------------------------------------------------------------------------------\n        zipped_tracking_points_seq = zip(*new_tracking_points_seq)\n        zipped_image_seq = zip(*new_image_seq)\n        zipped_seg_seq = zip(*new_seg_seq)\n\n        assert len(tracking_points_path_list) == len(selected_image_path_list)\n        assert len(tracking_points_path_list) == len(selected_seg_points_path_list)\n        assert len(new_tracking_points_seq) == seq_len\n        assert len(new_tracking_points_seq) == len(new_image_seq)\n        assert len(new_tracking_points_seq) == len(new_seg_seq)\n        assert len(new_tracking_points_seq[0]) == len(new_tracking_points_seq[1])\n        assert len(new_tracking_points_seq[0]) == len(new_image_seq[0])\n        assert len(new_tracking_points_seq[0]) == len(new_seg_seq[0])\n        assert len(new_tracking_points_seq[0]) == len(new_image_seq[1])\n        assert len(new_tracking_points_seq[0]) == len(new_seg_seq[1])\n\n        data_list.extend(zip(zipped_image_seq, zipped_seg_seq, zipped_tracking_points_seq))\n\n    num_least_tracking_points = find_least_tracking_points(tracking_points_folders)\n    print('num_least_tracking_points', num_least_tracking_points)\n\n    CUSTOM_MAX_TOTAL_SEG_POINTS_NUM = 1640 # 1640  # 1150 # TODO\n    write_records(data_list, output_folder, num_least_tracking_points, CUSTOM_MAX_TOTAL_SEG_POINTS_NUM)", "\n# ------------------------------------------------------------------------------------\n\n@tf.function\ndef resize(img, height, width, is_flow, mask=None):\n  \"\"\"Resize an image or flow field to a new resolution.\n\n  In case a mask (per pixel {0,1} flag) is passed a weighted resizing is\n  performed to account for missing flow entries in the sparse flow field. The\n  weighting is based on the resized mask, which determines the 'amount of valid\n  flow vectors' that contributed to each individual resized flow vector. Hence,\n  multiplying by the reciprocal cancels out the effect of considering non valid\n  flow vectors.\n\n  Args:\n    img: tf.tensor, image or flow field to be resized of shape [b, h, w, c]\n    height: int, heigh of new resolution\n    width: int, width of new resolution\n    is_flow: bool, flag for scaling flow accordingly\n    mask: tf.tensor, mask (optional) per pixel {0,1} flag\n\n  Returns:\n    Resized and potentially scaled image or flow field (and mask).\n  \"\"\"\n\n  def _resize(img, mask=None):\n    # _, orig_height, orig_width, _ = img.shape.as_list()\n    orig_height = tf.shape(input=img)[1]\n    orig_width = tf.shape(input=img)[2]\n    if orig_height == height and orig_width == width:\n      # early return if no resizing is required\n      if mask is not None:\n        return img, mask\n      else:\n        return img\n\n    if mask is not None:\n      # multiply with mask, to ensure non-valid locations are zero\n      img = tf.math.multiply(img, mask)\n      # resize image\n      img_resized = tf.compat.v2.image.resize(\n          img, (int(height), int(width)), antialias=True)\n      # resize mask (will serve as normalization weights)\n      mask_resized = tf.compat.v2.image.resize(\n          mask, (int(height), int(width)), antialias=True)\n      # normalize sparse flow field and mask\n      img_resized = tf.math.multiply(img_resized,\n                                     tf.math.reciprocal_no_nan(mask_resized))\n      mask_resized = tf.math.multiply(mask_resized,\n                                      tf.math.reciprocal_no_nan(mask_resized))\n    else:\n      # normal resize without anti-alaising\n      prev_img_dtype = img.dtype\n      img_resized = tf.compat.v2.image.resize(img, (int(height), int(width)))\n      # img_resized = tf.cast(img, prev_img_dtype)\n\n    if is_flow:\n      # If image is a flow image, scale flow values to be consistent with the\n      # new image size.\n      scaling = tf.reshape([\n          float(height) / tf.cast(orig_height, tf.float32),\n          float(width) / tf.cast(orig_width, tf.float32)\n      ], [1, 1, 1, 2])\n      img_resized *= scaling\n\n    if mask is not None:\n      return img_resized, mask_resized\n    return img_resized\n\n  # Apply resizing at the right shape.\n  shape = img.shape.as_list()\n  if len(shape) == 3:\n    if mask is not None:\n      img_resized, mask_resized = _resize(img[None], mask[None])\n      return img_resized[0], mask_resized[0]\n    else:\n      return _resize(img[None])[0]\n  elif len(shape) == 4:\n    # Input at the right shape.\n    return _resize(img, mask)\n  elif len(shape) > 4:\n    # Reshape input to [b, h, w, c], resize and reshape back.\n    img_flattened = tf.reshape(img, [-1] + shape[-3:])\n    if mask is not None:\n      mask_flattened = tf.reshape(mask, [-1] + shape[-3:])\n      img_resized, mask_resized = _resize(img_flattened, mask_flattened)\n    else:\n      img_resized = _resize(img_flattened)\n    # There appears to be some bug in tf2 tf.function\n    # that fails to capture the value of height / width inside the closure,\n    # leading the height / width undefined here. Call set_shape to make it\n    # defined again.\n    img_resized.set_shape(\n        (img_resized.shape[0], height, width, img_resized.shape[3]))\n    result_img = tf.reshape(img_resized, shape[:-3] + img_resized.shape[-3:])\n    if mask is not None:\n      mask_resized.set_shape(\n          (mask_resized.shape[0], height, width, mask_resized.shape[3]))\n      result_mask = tf.reshape(mask_resized,\n                               shape[:-3] + mask_resized.shape[-3:])\n      return result_img, result_mask\n    return result_img\n  else:\n    raise ValueError('Cannot resize an image of shape', shape)", "\n\n@tf.function\ndef resize_uint8(img, height, width):\n  \"\"\"Resize an image or flow field to a new resolution.\n\n  In case a mask (per pixel {0,1} flag) is passed a weighted resizing is\n  performed to account for missing flow entries in the sparse flow field. The\n  weighting is based on the resized mask, which determines the 'amount of valid\n  flow vectors' that contributed to each individual resized flow vector. Hence,\n  multiplying by the reciprocal cancels out the effect of considering non valid\n  flow vectors.\n\n  Args:\n    img: tf.tensor, image or flow field to be resized of shape [b, h, w, c]\n    height: int, heigh of new resolution\n    width: int, width of new resolution\n    is_flow: bool, flag for scaling flow accordingly\n    mask: tf.tensor, mask (optional) per pixel {0,1} flag\n\n  Returns:\n    Resized and potentially scaled image or flow field (and mask).\n  \"\"\"\n\n  def _resize(img):\n      orig_height = tf.shape(input=img)[1]\n      orig_width = tf.shape(input=img)[2]\n      if orig_height == height and orig_width == width:\n          # early return if no resizing is required\n          return img\n\n      # normal resize without anti-alaising\n      prev_img_dtype = img.dtype\n      img_resized = tf.compat.v2.image.resize(img, (int(height), int(width)))\n      img_resized = tf.cast(img_resized, prev_img_dtype)\n\n      return img_resized\n\n\n  # Apply resizing at the right shape.\n  shape = img.shape.as_list()\n  if len(shape) == 3:\n      return _resize(img[None])[0]\n  elif len(shape) == 4:\n    # Input at the right shape.\n    return _resize(img)\n  elif len(shape) > 4:\n      # Reshape input to [b, h, w, c], resize and reshape back.\n      img_flattened = tf.reshape(img, [-1] + shape[-3:])\n      img_resized = _resize(img_flattened)\n      # There appears to be some bug in tf2 tf.function\n      # that fails to capture the value of height / width inside the closure,\n      # leading the height / width undefined here. Call set_shape to make it\n      # defined again.\n      img_resized.set_shape(\n          (img_resized.shape[0], height, width, img_resized.shape[3]))\n      result_img = tf.reshape(img_resized, shape[:-3] + img_resized.shape[-3:])\n      return result_img\n  else:\n      raise ValueError('Cannot resize an image of shape', shape)", "\n\ndef debug_file():\n    raw_dataset = tf.data.TFRecordDataset(\"uflow/assets/pc_celltrack/tfrecord/training/img/custom@0.tfrecord\")\n    for raw_record in raw_dataset.take(1):\n      record = tf.train.SequenceExample.FromString(raw_record.numpy())\n      with open('record.txt', 'w') as f:\n          f.write(str(record))\n\n\ndef main(_):\n    FLAGS.output_dir = FLAGS.data_dir + \"tfrecord/\"\n    \n    convert_dataset(FLAGS.seq_len, FLAGS.mode, FLAGS.data_split)\n    # convert_dataset_with_segmentation()\n\n    print('@@@@@@@@@@@')\n    print('seq_len', FLAGS.seq_len)\n    print('mode', FLAGS.mode)\n    print('@@@@@@@@@@@')", "\n\ndef main(_):\n    FLAGS.output_dir = FLAGS.data_dir + \"tfrecord/\"\n    \n    convert_dataset(FLAGS.seq_len, FLAGS.mode, FLAGS.data_split)\n    # convert_dataset_with_segmentation()\n\n    print('@@@@@@@@@@@')\n    print('seq_len', FLAGS.seq_len)\n    print('mode', FLAGS.mode)\n    print('@@@@@@@@@@@')", "\n\nif __name__ == '__main__':\n  # debug_file()\n  app.run(main)\n"]}
