{"filename": "backend/app_test.py", "chunked_list": ["from app import main\n\n\ndef test_main():\n    assert main() == \"<p>GRAPEVNE flask server is running</p>\"\n"]}
{"filename": "backend/app.py", "chunked_list": ["import base64\nimport json\n\nimport filesystem\nfrom flask import Flask\nfrom flask import request\nfrom flask import Response\nfrom flask_cors import CORS\nfrom flask_cors import cross_origin\n", "from flask_cors import cross_origin\n\nimport builder\nimport runner\n\napp = Flask(__name__)\nCORS(app)\n\n\n@app.route(\"/api/post\", methods=[\"POST\"])", "\n@app.route(\"/api/post\", methods=[\"POST\"])\n@cross_origin()\ndef post():\n    \"\"\"Handles POST requests from the frontend.\"\"\"\n    try:\n        app.logger.debug(f\"POST request: {request.json}\")\n\n        # File system queries\n        if request.json[\"query\"] == \"display/folderinfo\":\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": json.dumps(filesystem.GetFolderItems(request.json[\"data\"])),\n            }\n\n        # Builder queries\n        elif request.json[\"query\"] == \"builder/compile-to-json\":\n            data = request.json[\"data\"]\n            js = json.loads(data[\"content\"])\n            with open(\"workflow.json\", \"w\") as f:  # dump config file to disk for debug\n                json.dump(js, f, indent=4)\n            memory_zip, _ = builder.BuildFromJSON(js)\n            return Response(\n                base64.b64encode(memory_zip),\n                mimetype=\"application/zip\",\n                headers={\"Content-Disposition\": \"attachment;filename=workflow.zip\"},\n            )\n        elif request.json[\"query\"] == \"builder/get-remote-modules\":\n            data = request.json[\"data\"]\n            js = json.loads(data[\"content\"])\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": builder.GetModulesList(js[\"url\"]),\n            }\n\n        # Runner queries\n        elif request.json[\"query\"] == \"runner/build\":\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": runner.Build(request.json[\"data\"]),\n            }\n        elif request.json[\"query\"] == \"runner/deleteresults\":\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": json.dumps(runner.DeleteAllOutput(request.json[\"data\"])),\n            }\n        elif request.json[\"query\"] == \"runner/lint\":\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": json.dumps(runner.LintContents(request.json[\"data\"])),\n            }\n        elif request.json[\"query\"] == \"runner/loadworkflow\":\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": json.dumps(runner.LoadWorkflow(request.json[\"data\"])),\n            }\n        elif request.json[\"query\"] == \"runner/tokenize\":\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": json.dumps(runner.Tokenize(request.json[\"data\"])),\n            }\n        elif request.json[\"query\"] == \"runner/tokenize_load\":\n            try:\n                # Try full-tokenize (may fail if dependencies not present)\n                body = runner.FullTokenizeFromFile(request.json[\"data\"])\n            except BaseException:\n                # ...then try in-situ tokenization\n                body = runner.TokenizeFromFile(request.json[\"data\"])\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": json.dumps(body),\n            }\n        elif request.json[\"query\"] == \"runner/jobstatus\":\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": json.dumps(runner.TokenizeFromFile(request.json[\"data\"])),\n            }\n        elif request.json[\"query\"] == \"runner/launch\":\n            app.logger.debug(f\"Data: {request.json['data']}\")\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": json.dumps(runner.Launch(request.json[\"data\"])),\n            }\n        elif request.json[\"query\"] == \"runner/check-node-dependencies\":\n            data = {\n                \"query\": request.json[\"query\"],\n                \"body\": runner.CheckNodeDependencies(request.json[\"data\"]),\n            }\n\n        else:\n            return Response(f\"Query not supported: {request.json['query']}\", status=400)\n    except ValueError as err:\n        # can be raised by runner module (provided with invalid file format)\n        app.logger.error(err)\n        return Response(err, status=400)\n    except TypeError as err:\n        app.logger.error(err)\n        return Response(\"Server error\", status=500)\n    return json.dumps(data)", "\n\n@app.route(\"/\")\ndef main() -> str:\n    \"\"\"Returns a simple message to indicate that the server is running.\"\"\"\n    return \"<p>GRAPEVNE flask server is running</p>\"\n"]}
{"filename": "backend/filesystem.py", "chunked_list": ["import os\nimport shutil\n\n\ndef DeleteResults(data) -> dict:\n    \"\"\"Remove the local 'results' folder\"\"\"\n    dirname = data[\"content\"]\n    dirname = f\"{dirname}/results\"\n    shutil.rmtree(dirname)\n    return {}", "\n\ndef GetFolderItems(data) -> dict:\n    \"\"\"Get the contents of a folder\"\"\"\n    dirname = data[\"content\"]\n\n    dirlist = [\n        filename\n        for filename in os.listdir(dirname)\n        if os.path.isdir(os.path.join(dirname, filename))\n    ]\n    dirlist = [name for name in dirlist if name[0] != \".\"]\n    dirlist.sort()\n    isdir = len(dirlist) * [True]\n\n    filelist = [\n        filename\n        for filename in os.listdir(dirname)\n        if not os.path.isdir(os.path.join(dirname, filename))\n    ]\n    filelist = [name for name in filelist if name[0] != \".\"]\n    filelist.sort()\n    isdir.extend(len(filelist) * [False])\n\n    contents = [*dirlist, *filelist]\n\n    contents = [{\"name\": name, \"isdir\": isdir} for name, isdir in zip(contents, isdir)]\n\n    js = {\"foldername\": dirname, \"contents\": contents}\n    return js", ""]}
{"filename": "builder/setup.py", "chunked_list": ["#!/usr/bin/env python\nfrom setuptools import setup\n\n\nsetup(\n    name=\"builder\",\n    version=\"0.1\",\n    description=\"GRAPEVNE builder\",\n    packages=[\"builder\"],\n    zip_safe=False,", "    packages=[\"builder\"],\n    zip_safe=False,\n)\n"]}
{"filename": "builder/builder/__init__.py", "chunked_list": ["from .builder import BuildFromFile  # noqa: F401\nfrom .builder import BuildFromJSON  # noqa: F401\nfrom .builder import CleanBuildFolder  # noqa: F401\nfrom .builder_web import GetModulesList  # noqa: F401\nfrom .ImportWorkflow import ImportWorkflowDir  # noqa: F401\n"]}
{"filename": "builder/builder/ImportWorkflow.py", "chunked_list": ["import re\n\nimport yaml\n\nfrom builder.builder import Model\nfrom builder.builder_web import GetWorkflowFiles\n\n\ndef ImportWorkflowDir(\n    workflow_dir: str,\n) -> Model:\n    \"\"\"Import linked modules snakemake workflow as Model object\n\n    Args:\n        filename: Path to the workflow file\n        levels: Number of levels to import (default -1 for all)\n    \"\"\"\n    workflow_str, config_str = GetWorkflowFiles(f\"'{workflow_dir}'\")\n    workflow_config = yaml.safe_load(config_str)\n    modules = re.findall(\"^module (.*):\", workflow_str, re.MULTILINE)\n\n    # Build model\n    m = Model()\n    for rulename in modules:\n        # Namespaces are not ordinary parameters\n        config = workflow_config[rulename][\"config\"]\n        c = config.copy()\n        c.pop(\"input_namespace\", None)\n        c.pop(\"output_namespace\", None)\n        node = m.AddModule(rulename, {\"config\": c})\n        # Retain namespace mapping\n        node.input_namespace = config.get(\"input_namespace\", node.input_namespace)\n        node.output_namespace = config.get(\"output_namespace\", node.output_namespace)\n        node.snakefile = workflow_config[rulename].get(\"snakefile\", node.snakefile)\n\n    # Expand modules\n    m.ExpandAllModules()\n\n    return m", "def ImportWorkflowDir(\n    workflow_dir: str,\n) -> Model:\n    \"\"\"Import linked modules snakemake workflow as Model object\n\n    Args:\n        filename: Path to the workflow file\n        levels: Number of levels to import (default -1 for all)\n    \"\"\"\n    workflow_str, config_str = GetWorkflowFiles(f\"'{workflow_dir}'\")\n    workflow_config = yaml.safe_load(config_str)\n    modules = re.findall(\"^module (.*):\", workflow_str, re.MULTILINE)\n\n    # Build model\n    m = Model()\n    for rulename in modules:\n        # Namespaces are not ordinary parameters\n        config = workflow_config[rulename][\"config\"]\n        c = config.copy()\n        c.pop(\"input_namespace\", None)\n        c.pop(\"output_namespace\", None)\n        node = m.AddModule(rulename, {\"config\": c})\n        # Retain namespace mapping\n        node.input_namespace = config.get(\"input_namespace\", node.input_namespace)\n        node.output_namespace = config.get(\"output_namespace\", node.output_namespace)\n        node.snakefile = workflow_config[rulename].get(\"snakefile\", node.snakefile)\n\n    # Expand modules\n    m.ExpandAllModules()\n\n    return m", "\n\ndef ParseFunctionSignature(signature: str):\n    def f(*args, **kwargs):\n        return args, kwargs\n\n    name, args = signature.split(\"(\", 1)\n    return name, *eval(\"f(\" + args)\n", ""]}
{"filename": "builder/builder/builder.py", "chunked_list": ["import argparse\nimport copy\nimport json\nimport logging\nimport os\nimport pathlib\nimport re\nimport shutil\nimport tempfile\nfrom typing import List", "import tempfile\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport requests\nimport yaml\n\n", "\n\nNamespace = Union[str, None, dict]\n\n# Set up logging\nlogfile = os.path.expanduser(\"~\") + \"/GRAPEVNE.log\"\nlogging.basicConfig(\n    filename=logfile,\n    encoding=\"utf-8\",\n    level=logging.DEBUG,", "    encoding=\"utf-8\",\n    level=logging.DEBUG,\n)\nlogging.info(\"Working directory: %s\", os.getcwd())\n\n\nclass Node:\n    \"\"\"Node class for use with the workflow Model\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        rulename: str,\n        nodetype: str,\n        snakefile: Union[\n            str, dict\n        ] = \"\",  # str | {function: str, args: List, kwargs: dict}\n        config={},\n        input_namespace: Namespace = \"\",\n        output_namespace: str = \"\",\n    ):\n        \"\"\"Initialise a Node object, the parent class for Modules\n\n        Args:\n            name (str): Name of the node\n            rulename (str): Name of the rule\n            nodetype (str): Type of node (module, connector, etc.)\n            snakefile (str|dict): str location or dict representing function call\n            config (dict): Configuration (parameters) for the Snakefile\n            input_namespace (str): Input namespace\n            output_namespace (str): Output namespace\n        \"\"\"\n\n        self.name = name\n        self.rulename = rulename\n        self.nodetype = nodetype\n        self.snakefile = snakefile\n        self.config = config\n        self.input_namespace = input_namespace\n        self.output_namespace = output_namespace\n\n    def GetOutputNamespace(self) -> str:\n        \"\"\"Returns the output namespace\"\"\"\n        return self.output_namespace\n\n    def GetInputNamespace(self) -> Namespace:\n        \"\"\"Returns the input namespace, which can be a string or dictionary\"\"\"\n        return self.input_namespace", "\n\nclass Module(Node):\n    \"\"\"Module class for use with the workflow Model\"\"\"\n\n    def __init__(self, name: str, kwargs: dict):\n        \"\"\"Initialise a Module object\n\n        Args:\n            name (str): Name of the module\n            kwargs: See Node class for kwargs\n        \"\"\"\n        kwargs[\"nodetype\"] = kwargs.get(\"nodetype\", \"module\")\n        kwargs[\"config\"] = kwargs.get(\"config\", {})\n        kwargs[\"input_namespace\"] = kwargs[\"config\"].get(\"input_namespace\", None)\n        super().__init__(name, **kwargs)\n\n    def _GetConfigFileinfo(self) -> Union[str, dict]:\n        \"\"\"Returns the config filename, or an equivalent dict for remote files\"\"\"\n        workflow_filename = \"workflow/Snakefile\"\n        config_filename = \"config/config.yaml\"\n        if isinstance(self.snakefile, str):\n            # Local file\n            filename = self.snakefile\n            filename = filename.replace(workflow_filename, config_filename)\n            return filename\n        if isinstance(self.snakefile, dict):\n            # Remote file\n            c = copy.deepcopy(self.snakefile)\n            c[\"kwargs\"][\"path\"] = c[\"kwargs\"][\"path\"].replace(\n                workflow_filename, config_filename\n            )\n            return c\n        raise ValueError(\"Invalid snakefile type\")\n\n    def _ReadFile(self, fileinfo: Union[str, dict]) -> str:\n        \"\"\"Helper function that reads a file, either local or remote\"\"\"\n        if isinstance(fileinfo, str):\n            # Local file\n            with open(fileinfo, \"r\") as file:\n                contents = file.read()\n            return contents\n        if isinstance(fileinfo, dict):\n            # Remote file\n            if fileinfo.get(\"function\", None) not in [\"github\"]:\n                raise ValueError(\n                    \"Only github function is currently supported for remote files\"\n                )\n            url_github: str = \"https://raw.githubusercontent.com\"\n            repo = fileinfo[\"args\"][0]\n            branch = fileinfo.get(\"kwargs\", {}).get(\"branch\", \"main\")\n            path = fileinfo.get(\"kwargs\", {}).get(\"path\", \"\")\n            url: str = f\"{url_github}/{repo}/{branch}/{path}\"\n            workflow_file = requests.get(url).text\n            return workflow_file\n        raise ValueError(\"Invalid snakefile type\")\n\n    def ReadWorkflowFile(self):\n        return self._ReadFile(self.snakefile)\n\n    def ReadConfigFile(self):\n        return yaml.safe_load(self._ReadFile(self._GetConfigFileinfo()))", "\n\nclass Model:\n    \"\"\"Model class for the workflow\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialise the model\"\"\"\n        self.nodes: List[Node] = []  # List of Node objects\n        self.partial_build: bool = False\n\n    def SetPartialBuild(self, partial_build: bool) -> None:\n        \"\"\"Sets the partial build flag (does not throw if a node is missing)\"\"\"\n        self.partial_build = partial_build\n\n    def BuildSnakefile(\n        self,\n        configfile: str = \"config/config.yaml\",\n    ) -> str:\n        \"\"\"Builds the workflow Snakefile (links modules)\"\"\"\n        s = \"\"\n        if configfile:\n            s = f'configfile: \"{configfile}\"\\n'\n        # Build Snakefile\n        for node in self.nodes:\n            s += \"\\n\"\n            s += f\"module {node.rulename}:\\n\"\n            s += \"    snakefile:\\n\"\n            if isinstance(node.snakefile, str):\n                # String denoted a local file\n                s += f'        config[\"{node.rulename}\"][\"snakefile\"]\\n'\n            else:\n                # Dynamic evaluation of function specified in config file\n                s += \"        eval(\\n\"\n                s += f'            f\\'{{config[\"{node.rulename}\"][\"snakefile\"][\"function\"]}}\\'\\n'\n                s += f'            \\'(*config[\"{node.rulename}\"][\"snakefile\"][\"args\"],\\'\\n'\n                s += f'            \\'**config[\"{node.rulename}\"][\"snakefile\"][\"kwargs\"])\\'\\n'\n                s += \"        )\\n\"\n            s += \"    config:\\n\"\n            s += f'        config[\"{node.rulename}\"][\"config\"]\\n'\n            s += f\"use rule * from {node.rulename} as {node.rulename}_*\\n\"\n        return s\n\n    def BuildSnakefileConfig(self) -> str:\n        \"\"\"Builds the workflow configuration as YAML\"\"\"\n        c = self.ConstructSnakefileConfig()\n        return yaml.dump(c)\n\n    def ConstructSnakefileConfig(self) -> dict:\n        \"\"\"Builds the workflow configuration as a dictionary\"\"\"\n        c: dict = {}\n        c[\"input_namespace\"] = self.ExposeOrphanInputs()\n        module_output_namespaces = self.ExposeOrphanOutputs()\n        # only a single output_namespace is currently supported\n        if len(module_output_namespaces) == 0:\n            # Model has no orphan outputs, so will form a Terminal module\n            # This will most likely need marking in the config somewhere.\n            ...\n        if len(module_output_namespaces) == 1:\n            c[\"output_namespace\"] = module_output_namespaces[0]\n        else:\n            # Could support multiple output namespaces by automatically adding\n            # a convergence module, but this might be better left to the user\n            # as a deliberate action.\n            # raise ValueError(\"Multiple output namespaces not currently supported. \"\n            #                 \"Requested: \", module_output_namespaces)\n            print(\n                \"Multiple output namespaces not currently supported. Request: \",\n                module_output_namespaces,\n            )\n            print(\"Continuing for debug purposes...\")\n        # Add configurations for each module\n        for node in self.nodes:\n            cnode = node.config.copy()\n\n            # Input namespace\n            if node.input_namespace:\n                cnode[\"input_namespace\"] = cnode.get(\n                    \"input_namespace\", node.input_namespace\n                )\n                if isinstance(cnode[\"input_namespace\"], dict):\n                    if not isinstance(node.input_namespace, dict):\n                        node.input_namespace = {}\n                    for k, v in cnode[\"input_namespace\"].items():\n                        if node.input_namespace.get(k, None):\n                            cnode[\"input_namespace\"][k] = node.input_namespace[k]\n                        # Don't use 'null' for input namespaces\n                        if not cnode[\"input_namespace\"][k]:\n                            cnode[\"input_namespace\"][k] = k\n                if isinstance(node.input_namespace, str):\n                    cnode[\"input_namespace\"] = node.input_namespace\n            else:\n                cnode[\"input_namespace\"] = None\n\n            # Output namespace\n            cnode[\"output_namespace\"] = node.output_namespace\n\n            # Save\n            c[node.rulename] = {\n                \"name\": node.name,\n                \"type\": node.nodetype,\n                \"snakefile\": node.snakefile,\n                \"config\": cnode,\n            }\n        return c\n\n    def SaveWorkflow(\n        self,\n        build_path: str = \"build\",\n        clean_build: bool = True,\n    ) -> str:\n        \"\"\"Saves the workflow to the build directory\"\"\"\n        if clean_build:  # Delete build directory before rebuilding\n            shutil.rmtree(build_path, ignore_errors=True)\n        pathlib.Path(f\"{build_path}/config\").mkdir(parents=True, exist_ok=True)\n        pathlib.Path(f\"{build_path}/workflow\").mkdir(parents=True, exist_ok=True)\n        with open(f\"{build_path}/workflow/Snakefile\", \"w\") as file:\n            file.write(self.BuildSnakefile())\n        with open(f\"{build_path}/config/config.yaml\", \"w\") as file:\n            file.write(self.BuildSnakefileConfig())\n        return build_path\n\n    def WrangleName(self, basename: str, subname: str = \"\") -> str:\n        \"\"\"Wrangles a valid and unique rule name\"\"\"\n        rulename = self.WrangleRuleName(basename)\n        name = f\"{rulename}\"\n        if subname:\n            name = f\"{name}_{subname}\"\n        offset = 1\n        wrangledName = name\n        while wrangledName in self.WrangledNameList():\n            # NOTE: hash is not deterministic across runs\n            # wrangledName = f\"{name}_{hash(name + str(offset)) % (2**31)}\"\n            wrangledName = f\"{name}_{str(offset)}\"\n            offset += 1\n        return wrangledName\n\n    def WrangledNameList(self) -> List[str]:\n        \"\"\"Returns a list of all wrangled names\"\"\"\n        return [n.rulename for n in self.nodes]\n\n    def WrangleRuleName(self, name: str) -> str:\n        \"\"\"Wrangles a valid rulename (separate from the human readable name)\"\"\"\n        return (\n            name.replace(\" \", \"_\")\n            .replace(\"/\", \"_\")\n            .replace(\".\", \"_\")\n            .replace(\"(\", \"\")\n            .replace(\")\", \"\")\n            .lower()\n        )\n\n    def AddModule(self, name: str, module: dict) -> Module:\n        print(\"=== Add module\")\n        print(name)\n        print(module)\n        \"\"\"Adds a module to the workflow\"\"\"\n        kwargs = module.copy()\n        if \"rulename\" not in kwargs:\n            kwargs[\"rulename\"] = self.WrangleName(name)\n        node = Module(name, kwargs)\n        self.nodes.append(node)\n        node.output_namespace = node.rulename\n        return node\n\n    def AddConnector(self, name, connector) -> None:\n        \"\"\"Adds a connection between modules\n\n        Connectors map all inputs to a module. If the first element of\n        connector is a string then only a single input_namespace need be\n        considered. If the first element is a dictionary then key-value pairs\n        represent the input_namespaces / ports, and their associated input\n        modules.\n\n        Example: Connect the output of module2 to the single input on module1\n            connector = [ \"module2\", \"module1\" ]\n\n        Example: Connect the output of module2 to the named input on module1\n            connector = [ {\"input1\": \"module2\"}, \"module1\" ]\n\n        Error behaviour depends on the partial_build flag. If False then an\n        error is thrown if a node is not found. If True then the connector is\n        ignored and None returned.\n\n        Args:\n            name (str): Name of the connector\n            connector (list): Connector definition\n        \"\"\"\n        mapping = connector.get(\"map\", None)\n        node_to = self.GetNodeByName(mapping[1])\n        if not node_to:\n            if self.partial_build:\n                return\n            raise ValueError(\n                \"No matching node found for connector source: \"\n                \"Requested '\" + mapping[1] + \"'\"\n            )\n        if isinstance(mapping[0], dict):\n            assert isinstance(\n                node_to.input_namespace, dict\n            ), \"Connector mapping is a dictionary but the destination node does not have a dictionary input namespace\"\n            for k, v in mapping[0].items():\n                incoming_node = self.GetNodeByName(v)\n                if not incoming_node:\n                    if self.partial_build:\n                        return\n                    raise ValueError(\n                        \"No matching node found for connector source: \" + v\n                    )\n                node_to.input_namespace[k] = incoming_node.output_namespace\n        else:\n            node_from = self.GetNodeByName(mapping[0])\n            if not node_from:\n                if self.partial_build:\n                    return\n                raise ValueError(\n                    \"No matching node found for connector destination: \" + mapping[0]\n                )\n            node_to.input_namespace = node_from.output_namespace\n        return None\n\n    def GetNodeByName(self, name: str) -> Optional[Node]:\n        \"\"\"Returns a node object by name\"\"\"\n        name = name.casefold()\n        for node in self.nodes:\n            if node.name.casefold() == name:\n                return node\n        return None\n\n    def GetNodeByRuleName(self, rulename: str) -> Optional[Node]:\n        \"\"\"Returns a node object by name\"\"\"\n        rulename = rulename.casefold()\n        for node in self.nodes:\n            if node.rulename == rulename:\n                return node\n        return None\n\n    def NodeIsTerminus(self, node: Node) -> bool:\n        \"\"\"Returns true if the given node is a terminus\"\"\"\n        # Check for onward connections from the given node\n        for n in self.nodes:\n            nodes_in = n.input_namespace\n            if isinstance(nodes_in, str):\n                nodes_in = {\"in\": nodes_in}\n            if isinstance(nodes_in, dict):\n                if node.rulename in nodes_in.values():\n                    return False\n        return True\n\n    def ExposeOrphanInputs(self) -> Namespace:\n        \"\"\"Find orphan inputs and return as a valid input_namespace\"\"\"\n        module_input_namespace: dict = {}\n        all_output_namespaces = self.GetRuleNames()\n        for node in self.nodes:\n            ref_rulename = node.rulename + \"$\"\n            if isinstance(node.input_namespace, str):\n                if node.input_namespace not in all_output_namespaces:\n                    module_input_namespace[ref_rulename] = node.input_namespace\n            elif isinstance(node.input_namespace, dict):\n                module_input_namespace[ref_rulename] = {}\n                for k, v in node.input_namespace.items():\n                    if v not in all_output_namespaces:\n                        # namespace should be unique to avoid clashes\n                        module_input_namespace[ref_rulename + k] = self.WrangleName(v)\n                if not module_input_namespace[ref_rulename]:\n                    del module_input_namespace[ref_rulename]\n            elif node.input_namespace is None:\n                pass\n            else:\n                raise ValueError(\"Invalid input_namespace type\")\n        if len(module_input_namespace) == 0:\n            return None\n        return module_input_namespace\n\n    def ExposeOrphanInputsList(self) -> List[str]:\n        \"\"\"Find orphan inputs and return as a valid input_namespace\"\"\"\n        orphans: List[str] = []\n        all_output_namespaces = self.GetRuleNames()\n        for node in self.nodes:\n            if isinstance(node.input_namespace, str):\n                if node.input_namespace not in all_output_namespaces:\n                    orphans.append(node.rulename)\n            elif isinstance(node.input_namespace, dict):\n                for k, v in node.input_namespace.items():\n                    if v not in all_output_namespaces:\n                        # namespace should be unique to avoid clashes\n                        orphans.append(v)\n            elif node.input_namespace is None:\n                # No input_namespace - source node/module\n                pass\n            else:\n                raise ValueError(\"Invalid input_namespace type\")\n        return orphans\n\n    def ExposeOrphanOutputs(self) -> List[str]:\n        \"\"\"Find orphan output and return as a valid output_namespace\"\"\"\n        module_output_namespaces: List[str] = []\n        all_input_namespaces = self.GetInputNamespaces()\n        for node in self.nodes:\n            if node.output_namespace not in all_input_namespaces:\n                module_output_namespaces.append(node.rulename)\n        return module_output_namespaces\n\n    def ExpandAllModules(self) -> None:\n        \"\"\"Expand all modules recursively\"\"\"\n        module_list: List[str] = []\n        while (modules := self.GetModuleNames()) != module_list:\n            module_list = modules\n            for rulename in modules:\n                self.ExpandModule(rulename)\n\n    def ExpandModule(self, rulename: str):\n        \"\"\"Expands a module into its constituent part\"\"\"\n        # Identify node\n        node = self.GetNodeByRuleName(rulename)\n        if not node:\n            raise ValueError(\"No matching node found for rulename: \" + rulename)\n        if not isinstance(node, Module):\n            raise ValueError(\"Node is not a module: \" + rulename)\n        # Read module spec (Snakefile, configfile) from source\n        workflow_contents = node.ReadWorkflowFile()\n        modules_list = re.findall(\"^module (.*):\", workflow_contents, re.MULTILINE)\n        config = node.ReadConfigFile()\n        # Narrow list of modules to those with valid GRAPEVNE entries in config\n        modules_list = [\n            m\n            for m in modules_list\n            if (m in config)  # GRAPEVNE config entry requirements here\n        ]\n        if not modules_list:\n            # No valid modules found, return original node\n            return node\n\n        # Keep record of orphan namespaces before expansion\n        orphan_inputs_prior = self.ExposeOrphanInputsList()\n        orphan_outputs_prior = self.ExposeOrphanOutputs()\n\n        # Add new nodes\n        rulemapping = {}\n        new_nodes: List[Node] = []\n        for n in modules_list:\n            new_node = self.AddModule(n, {\"config\": config[n].get(\"config\", {})})\n            # Retain namespace mapping\n            new_node.input_namespace = config[n][\"config\"].get(\n                \"input_namespace\", new_node.input_namespace\n            )\n            new_node.output_namespace = config[n][\"config\"].get(\n                \"output_namespace\", new_node.output_namespace\n            )\n            new_node.snakefile = config[n].get(\"snakefile\", new_node.snakefile)\n            # Record new node and rulename mapping, if different\n            new_nodes.append(new_node)\n            if n != new_node.rulename:\n                rulemapping[n] = new_node.rulename\n\n        print(\n            \"Attempting to expand module\",\n            node.rulename,\n            \" from \",\n            modules_list,\n            \" into \",\n            [n.rulename for n in new_nodes],\n        )\n\n        # Ensure namespace consistency between new nodes after rename\n        for n in new_nodes:\n            # output_namespace\n            if n.output_namespace in rulemapping.keys():\n                n.output_namespace = rulemapping[n.output_namespace]\n            # input_namespace\n            if isinstance(n.input_namespace, str):\n                if n.input_namespace in rulemapping.keys():\n                    n.input_namespace = rulemapping[n.input_namespace]\n            elif isinstance(n.input_namespace, dict):\n                for k, v in n.input_namespace.items():\n                    if k in rulemapping.keys():\n                        n.input_namespace[k] = rulemapping[v]\n            elif n.input_namespace is None:\n                pass\n            else:\n                raise ValueError(\"Namespace type not recognised\")\n\n        # Find orphan inputs and outputs from new node network\n        # Sort to prevent reording of nodes that afffect rule name wrangling\n        #  (important for testing)\n        new_orphan_inputs = sorted(\n            list(set(self.ExposeOrphanInputsList()) - set(orphan_inputs_prior))\n        )\n        new_orphan_outputs = sorted(\n            list(set(self.ExposeOrphanOutputs()) - set(orphan_outputs_prior))\n        )\n        assert (\n            len(new_orphan_outputs) <= 1\n        ), \"More than one new orphan output found: \" + str(new_orphan_outputs)\n\n        # Preserve incoming connections to parent node\n        if len(new_orphan_inputs) == 0:\n            # Now orphan inputs - source module\n            node.input_namespace = None\n        elif isinstance(node.input_namespace, str):\n            orphan_node = self.GetNodeByRuleName(list(new_orphan_inputs)[0])\n            if orphan_node:\n                orphan_node.input_namespace = node.input_namespace\n            else:\n                raise ValueError(\n                    \"No matching node found for name: \" + list(new_orphan_inputs)[0]\n                )\n        elif isinstance(node.input_namespace, dict):\n            # raise ValueError(\"Input dictionary namespaces not supported yet\")\n            print(\"Input dictionary namespaces not supported yet\")\n            print(\"Continuing for debug purposes...\")\n        elif node.input_namespace is None:\n            # Module is a Source and (no incoming connections)\n            pass\n        else:\n            raise ValueError(\"Namespace type not recognised\")\n\n        # Preserve outgoing connections from parent node\n        for n in self.nodes:\n            if isinstance(n.input_namespace, str):\n                if n.input_namespace == node.output_namespace:\n                    assert len(new_orphan_outputs) == 1, (\n                        \"Expanding node has one input, but \"\n                        + str(len(new_orphan_outputs))\n                        + \" new orphan outputs found\"\n                    )\n                    n.input_namespace = list(new_orphan_outputs)[0]\n            elif isinstance(n.input_namespace, dict):\n                for k, v in n.input_namespace.items():\n                    if v == node.output_namespace:\n                        n.input_namespace[k] = list(new_orphan_outputs)[0]\n            elif n.input_namespace is None:\n                pass\n            else:\n                raise ValueError(\"Namespace type not recognised\")\n\n        # Remove expanded node from model\n        self.nodes.remove(node)\n\n        # Return new nodes\n        return new_nodes\n\n    def GetModuleNames(self) -> List[str]:\n        return [n.rulename for n in self.nodes if isinstance(n, Module)]\n\n    def GetInputNamespaces(self) -> List[str]:\n        input_namespaces: List[str] = []\n        for n in self.nodes:\n            if isinstance(n.input_namespace, str):\n                input_namespaces.append(n.input_namespace)\n            elif isinstance(n.input_namespace, dict):\n                for k, v in n.input_namespace.items():\n                    input_namespaces.append(v)\n            elif n.input_namespace is None:\n                continue\n            else:\n                raise ValueError(\"Namespace type not recognised\")\n        return [name for name in input_namespaces if name]\n\n    def GetRuleNames(self) -> List[str]:\n        return [n.rulename for n in self.nodes]\n\n    def LookupRuleName(self, name: str) -> Optional[str]:\n        for n in self.nodes:\n            if name == n.name:\n                return n.rulename\n        return None\n\n    def LookupRuleNames(self, names: List[str]) -> List[Optional[str]]:\n        \"\"\"Lookup rule name given full name, can take a single string or list\"\"\"\n        return [self.LookupRuleName(name) for name in names]", "\n\ndef YAMLToConfig(content: str) -> str:\n    \"\"\"Transcribes YAML to a (Snakemake readable) dictionary config syntax\"\"\"\n    yl = yaml.safe_load(content)\n\n    def parse_struct(yl: dict):\n        \"\"\"Recursively parses a YAML structure\"\"\"\n        c = \"\"\n        for key, value in yl.items():\n            if isinstance(value, dict):\n                vv = parse_struct(value)\n                vv = [v for v in vv.split(\"\\n\") if v]\n                c += f'[\"{key}\"]={{}}\\n'  # Create empty dict\n                c += \"\\n\".join([f'[\"{key}\"]{v}' for v in vv]) + \"\\n\"\n            elif isinstance(value, list):\n                c += f'[\"{key}\"]=[]\\n'  # Create empty list\n                for item in value:\n                    c += f'[\"{key}\"].append(\"{item}\")\\n'\n                # raise Exception(\"Lists not supported in config\")\n            elif not value:\n                # Null\n                c += f'[\"{key}\"]=\"None\"\\n'\n            else:\n                # Some primitive type\n                c += f'[\"{key}\"]=\"{value}\"\\n'\n        return c\n\n    c = parse_struct(yl)\n    c = [\"config\" + s for s in c.split(\"\\n\") if s]\n    c = \"\\n\".join(c) + \"\\n\"\n    c = \"config={}\\n\" + c\n    return c", "\n\ndef BuildFromFile(\n    filename: str, **kwargs\n) -> Tuple[Union[Tuple[str, str], bytes], Model, str]:\n    \"\"\"Builds a workflow from a JSON specification file\"\"\"\n    try:\n        with open(filename, \"r\") as file:\n            config = json.load(file)\n    except FileNotFoundError:\n        print(f\"Configuration file not found: {filename}\")\n        exit(1)\n    except json.decoder.JSONDecodeError:\n        print(f\"Invalid JSON file: {filename}\")\n        exit(1)\n    return BuildFromJSON(config, **kwargs)", "\n\ndef CleanBuildFolder(build_path: str = \"\") -> None:\n    \"\"\"Deletes the build folder, if it exists\"\"\"\n    if build_path:\n        shutil.rmtree(build_path, ignore_errors=True)\n\n\ndef BuildFromJSON(\n    config: dict,\n    singlefile: bool = False,\n    expand: bool = True,\n    build_path: str = \"build\",\n    clean_build: bool = True,\n    partial_build: bool = False,  # Don't throw an error if node is missing\n) -> Tuple[Union[Tuple[str, str], bytes], Model, str]:\n    \"\"\"Builds a workflow from a JSON specification\n\n    Returns a tuple of the workflow and the workflow model object.\n    With singlefile=True the workflow is a tuple of (config, snakefile) strings\n    With singlefile=False the workflow is a (zipped) directory structure.\n    \"\"\"\n    logging.debug(\"BuildFromJSON\")\n    logging.debug(\n        f\"{config=}, {singlefile=}, {expand=}, {build_path=}, \"\n        f\"{clean_build=}, {partial_build=}\"\n    )\n    m = Model()\n    m.SetPartialBuild(partial_build)\n    # Add modules first to ensure all namespaces are defined before connectors\n    for item in config:\n        if item[\"type\"].casefold() in [\"module\", \"source\", \"terminal\"]:\n            logging.debug(\"=== Add module (call)\")\n            logging.debug(item)\n            m.AddModule(\n                item[\"name\"],\n                item[\"config\"],\n            )\n    # Add connectors\n    for item in config:\n        if item[\"type\"].casefold() in [\"connector\"]:\n            m.AddConnector(\n                item[\"name\"],\n                item[\"config\"],\n            )\n    if expand:\n        logging.debug(\"Expanding modules...\")\n        m.ExpandAllModules()\n    if singlefile:\n        # Return composite string\n        logging.debug(\"Returning single file build...\")\n        logging.debug(f\"{m.BuildSnakefileConfig()}, {m.BuildSnakefile()}\")\n        return ((m.BuildSnakefileConfig(), m.BuildSnakefile())), m, \"\"\n    else:\n        # Create (zipped) workflow and return as binary object\n        logging.debug(\"Creating zip file...\")\n        build_path = m.SaveWorkflow(build_path, clean_build)\n        zipfilename = tempfile.gettempdir() + \"/build\"\n        shutil.make_archive(zipfilename, \"zip\", build_path)\n        with open(f\"{zipfilename}.zip\", \"rb\") as file:\n            contents = file.read()\n        logging.debug(f\"Returning zip file: {zipfilename}.zip\")\n        return contents, m, zipfilename + \".zip\"", "def BuildFromJSON(\n    config: dict,\n    singlefile: bool = False,\n    expand: bool = True,\n    build_path: str = \"build\",\n    clean_build: bool = True,\n    partial_build: bool = False,  # Don't throw an error if node is missing\n) -> Tuple[Union[Tuple[str, str], bytes], Model, str]:\n    \"\"\"Builds a workflow from a JSON specification\n\n    Returns a tuple of the workflow and the workflow model object.\n    With singlefile=True the workflow is a tuple of (config, snakefile) strings\n    With singlefile=False the workflow is a (zipped) directory structure.\n    \"\"\"\n    logging.debug(\"BuildFromJSON\")\n    logging.debug(\n        f\"{config=}, {singlefile=}, {expand=}, {build_path=}, \"\n        f\"{clean_build=}, {partial_build=}\"\n    )\n    m = Model()\n    m.SetPartialBuild(partial_build)\n    # Add modules first to ensure all namespaces are defined before connectors\n    for item in config:\n        if item[\"type\"].casefold() in [\"module\", \"source\", \"terminal\"]:\n            logging.debug(\"=== Add module (call)\")\n            logging.debug(item)\n            m.AddModule(\n                item[\"name\"],\n                item[\"config\"],\n            )\n    # Add connectors\n    for item in config:\n        if item[\"type\"].casefold() in [\"connector\"]:\n            m.AddConnector(\n                item[\"name\"],\n                item[\"config\"],\n            )\n    if expand:\n        logging.debug(\"Expanding modules...\")\n        m.ExpandAllModules()\n    if singlefile:\n        # Return composite string\n        logging.debug(\"Returning single file build...\")\n        logging.debug(f\"{m.BuildSnakefileConfig()}, {m.BuildSnakefile()}\")\n        return ((m.BuildSnakefileConfig(), m.BuildSnakefile())), m, \"\"\n    else:\n        # Create (zipped) workflow and return as binary object\n        logging.debug(\"Creating zip file...\")\n        build_path = m.SaveWorkflow(build_path, clean_build)\n        zipfilename = tempfile.gettempdir() + \"/build\"\n        shutil.make_archive(zipfilename, \"zip\", build_path)\n        with open(f\"{zipfilename}.zip\", \"rb\") as file:\n            contents = file.read()\n        logging.debug(f\"Returning zip file: {zipfilename}.zip\")\n        return contents, m, zipfilename + \".zip\"", "\n\nif __name__ == \"__main__\":\n    \"\"\"Builds a workflow given a JSON specification file\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"filename\", help=\"Filename of json configuration\")\n    args = parser.parse_args()\n    BuildFromFile(args.filename)\n", ""]}
{"filename": "builder/builder/builder_web.py", "chunked_list": ["from os import listdir\nfrom os.path import abspath\nfrom os.path import isdir\nfrom os.path import join\nfrom typing import List\nfrom typing import Tuple\n\nimport requests\nimport yaml\n", "import yaml\n\n\ndef GetModulesList(url: dict) -> List[dict]:\n    \"\"\"Get modules list from url\"\"\"\n    if url[\"type\"] == \"github\":\n        return GetRemoteModulesGithub(url[\"repo\"], url[\"listing_type\"])\n    elif url[\"type\"] == \"local\":\n        return GetLocalModules(url[\"repo\"])\n    else:\n        raise Exception(\"Invalid url type.\")", "\n\ndef GetLocalModules(path: str) -> List[dict]:\n    \"\"\"Get local modules from path\"\"\"\n    path_base: str = f\"{path}/workflows\"\n\n    # Get list of local filesystem directories in path\n    orgs = sorted([f for f in listdir(path_base) if isdir(join(path_base, f))])\n    modules = []\n\n    # First-level (organisation) listing\n    for org in orgs:\n        org_path = f\"{path_base}/{org}\"\n        module_types = reversed(\n            sorted([f for f in listdir(org_path) if isdir(join(org_path, f))])\n        )\n\n        # Second-level (module type) listing\n        for module_type in module_types:\n            module_type_path = f\"{org_path}/{module_type}\"\n            workflows = sorted(\n                [\n                    f\n                    for f in listdir(module_type_path)\n                    if isdir(join(module_type_path, f))\n                ]\n            )\n\n            # Third-level (module/workflow) listing\n            for workflow in workflows:\n                url_workflow = (\n                    f\"{path_base}/{org}/{module_type}/{workflow}/workflow/Snakefile\"\n                )\n                config_file = (\n                    f\"{path_base}/{org}/{module_type}/{workflow}/config/config.yaml\"\n                )\n                config = {}\n                try:\n                    with open(config_file, \"r\") as file:\n                        config = yaml.safe_load(file)\n                except FileNotFoundError:\n                    print(f\"Config file not found - assuming blank: {file}\")\n                module_classification = GetModuleClassification(config)\n                modules.append(\n                    {\n                        \"name\": f\"({org}) {FormatName(workflow)}\",\n                        # \"type\": module_type[:-1],  # remove plural\n                        \"type\": module_classification,\n                        \"config\": {\n                            \"snakefile\": abspath(url_workflow),\n                            \"config\": config,\n                        },\n                    }\n                )\n    return modules", "\n\ndef GetRemoteModulesGithub(\n    repo: str, listing_type: str = \"BranchListing\"\n) -> List[dict]:\n    \"\"\"Get remote modules from url\n\n    Function currently supports workflows by directory listing, but could also\n    support listings by branches in the future.\n\n    Args:\n        repo: repository string in the format: \"owner/repository\"\n        listing_type: type of listing to use (default: \"BranchListing\").\n                      Options: \"BranchListing\", \"DirectoryListing\"\n\n    Returns:\n        List of modules\n    \"\"\"\n\n    if listing_type == \"DirectoryListing\":\n        return GetRemoteModulesGithubDirectoryListing(repo)\n    elif listing_type == \"BranchListing\":\n        return GetRemoteModulesGithubBranchListing(repo)\n    else:\n        raise Exception(\"Invalid Github listing type.\")", "\n\ndef GetRemoteModulesGithubDirectoryListing(repo: str) -> List[dict]:\n    \"\"\"Get remote modules from url (by directory listing)\n\n    Args:\n        repo: repository string in the format: \"owner/repository\"\n\n    Returns:\n        List of modules\n    \"\"\"\n\n    url_github: str = \"https://api.github.com/repos\"\n    url_base: str = f\"{url_github}/{repo}/contents/workflows\"\n    r_base = requests.get(url_base)\n    if r_base.status_code != 200:\n        raise Exception(\"Github API request failed (getting organisation listing).\")\n    modules = []\n    branch = \"main\"\n\n    # First-level (organisation) listing\n    for org in r_base.json():\n        if org[\"type\"] != \"dir\":\n            continue\n        url_org = f\"{url_base}/{org['name']}\"\n        print(url_org)\n        r_org = requests.get(url_org)\n        if r_org.status_code != 200:\n            raise Exception(\"Github API request failed (getting module type listing).\")\n\n        # Second-level (module type) listing\n        for module_type in reversed(r_org.json()):\n            if module_type[\"type\"] != \"dir\":\n                continue\n            url_workflow = f\"{url_org}/{module_type['name']}\"\n            r_workflow = requests.get(url_workflow)\n            if r_workflow.status_code != 200:\n                raise Exception(\n                    \"Github API request failed (getting module/workflow listing).\"\n                )\n\n            # Third-level (module/workflow) listing\n            for workflow in r_workflow.json():\n                if workflow[\"type\"] != \"dir\":\n                    continue\n                url_workflow = (\n                    f\"{repo}/workflows/\"\n                    f\"{org['name']}/\"\n                    f\"{module_type['name']}/\"\n                    f\"{workflow['name']}/workflow/Snakefile\"\n                )\n                url_config = (\n                    f\"https://raw.githubusercontent.com/{repo}/\"\n                    f\"{branch}/workflows/\"\n                    f\"{org['name']}/\"\n                    f\"{module_type['name']}/\"\n                    f\"{workflow['name']}/config/config.yaml\"\n                )\n                r_config = requests.get(url_config)\n                if r_config.status_code != 200:\n                    raise Exception(\n                        \"Github API request failed (getting workflow config file).\"\n                    )\n                config = yaml.safe_load(r_config.text)\n                # Determine module type by config file, rather than directory name\n                module_classification = GetModuleClassification(config)\n                modules.append(\n                    {\n                        \"name\": f\"({org['name']}) {FormatName(workflow['name'])}\",\n                        # \"type\": module_type[\"name\"][:-1],  # remove plural\n                        \"type\": module_classification,\n                        \"config\": {\n                            \"snakefile\": {\n                                \"function\": \"github\",\n                                \"args\": [repo],\n                                \"kwargs\": {\n                                    \"path\": f\"workflows/{org['name']}/\"\n                                    f\"{module_type['name']}/\"\n                                    f\"{workflow['name']}/\"\n                                    \"workflow/Snakefile\",\n                                    \"branch\": branch,\n                                },\n                            },\n                            \"config\": config,\n                        },\n                    }\n                )\n\n    return modules", "\n\ndef GetRemoteModulesGithubBranchListing(repo: str) -> List[dict]:\n    \"\"\"Get remote modules from url (by branches)\n\n    Args:\n        repo: repository string in the format: \"owner/repository\"\n\n    Returns:\n        List of modules\n    \"\"\"\n\n    url_github: str = \"https://api.github.com/repos\"\n    url_base: str = f\"{url_github}/{repo}/branches\"\n    r_base = requests.get(url_base)\n    if r_base.status_code != 200:\n        raise Exception(\"Github API request failed (getting GitHub branches).\")\n    modules = []\n\n    # Branch listing\n    module_types = {\"m\": \"module\", \"c\": \"connector\", \"s\": \"source\", \"t\": \"terminal\"}\n    for branch in r_base.json():\n        if branch[\"name\"] == \"main\":\n            continue\n        [module_type, module_org_and_name] = branch[\"name\"].split(\"/\", 1)\n        [module_org, module_name] = module_org_and_name.split(\"/\", 1)\n        if module_type not in module_types:\n            raise Exception(\n                f\"Invalid module type '{module_type}' in branch '{branch['name']}'.\"\n            )\n\n        url_config = (\n            f\"https://raw.githubusercontent.com/{repo}/\"\n            f\"{branch['name']}/config/config.yaml\"\n        )\n        r_config = requests.get(url_config)\n        if r_config.status_code != 200:\n            raise Exception(\"Github API request failed (getting workflow config file).\")\n        config = yaml.safe_load(r_config.text)\n        module_classification = GetModuleClassification(config)\n        modules.append(\n            {\n                \"name\": branch[\"name\"],\n                # \"type\": module_types[module_type],\n                \"type\": module_classification,\n                \"config\": {\n                    \"snakefile\": {\n                        \"function\": \"github\",\n                        \"args\": [repo],\n                        \"kwargs\": {\n                            \"path\": \"/workflow/Snakefile\",\n                            \"branch\": branch[\"name\"],\n                        },\n                    },\n                    \"config\": config,\n                },\n            }\n        )\n\n    return modules", "\n\ndef GetModuleClassification(config: dict) -> str:\n    \"\"\"Determine the module classification from the config file\n\n    Args:\n        config: module config file\n    \"\"\"\n    # If config is None, then default to module\n    if config is None:\n        return \"module\"\n    # If the input namespace exists and is anything other than None, then it is\n    # a module\n    if config.get(\"input_namespace\", \"blank\") is None:\n        return \"source\"\n    return \"module\"", "\n\ndef GetWorkflowFiles(\n    load_command: str,\n) -> Tuple[str, str]:\n    \"\"\"Read workflow and config files\n\n    Args:\n        load_command: command used to load the module in the Snakemake file\n    \"\"\"\n    # Determine if workflow is local or remote\n    if load_command[0] in [\"'\", '\"']:\n        return GetWorkflowFilesLocal(load_command)\n    else:\n        return GetWorkflowFilesRemote(load_command)", "\n\ndef GetWorkflowFilesLocal(\n    load_command: str,\n) -> Tuple[str, str]:\n    \"\"\"Read workflow and config files from local directory\n\n    Args:\n        load_command: command used to load the module in the Snakemake file\n    \"\"\"\n    # Isolate string containing workflow directory\n    workflow_dir = eval(load_command)\n    if isinstance(workflow_dir, tuple):  # account for trailing comma\n        workflow_dir = workflow_dir[0]\n    workflow_file = abspath(join(workflow_dir, \"workflow/Snakefile\"))\n    with open(workflow_file, \"r\") as file:\n        workflow_str = file.read()\n    # Parse workflow for configfile directive\n\n    # Import config file (if specified)\n    configfile = \"config/config.yaml\"\n    config_file = abspath(join(workflow_dir, configfile))\n    with open(config_file, \"r\") as file:\n        config_str = file.read()\n\n    return workflow_str, config_str", "\n\ndef GetWorkflowFilesRemote(\n    load_command: str,\n) -> Tuple[str, str]:\n    \"\"\"Read workflow and config files from remote source\n\n    Args:\n        load_command: command used to load the module in the Snakemake file\n    \"\"\"\n    raise Exception(\n        \"Only loading from local sources (load_command contains a string) \"\n        \"is currently supported.\"\n    )", "\n\ndef FormatName(name: str) -> str:\n    \"\"\"Formats name to (human readable) title case\"\"\"\n    return \" \".join([n[0].upper() + n[1:] for n in name.replace(\"_\", \" \").split(\" \")])\n\n\nif __name__ == \"__main__\":\n    \"\"\"Test function using default repository\"\"\"\n    print(\n        GetModulesList(\n            {\n                \"type\": \"local\",\n                \"listing_type\": \"DirectoryListing\",\n                \"path\": \"../../snakeshack\",\n            }\n        )\n    )", ""]}
{"filename": "builder/builder/tests/builder_web_test.py", "chunked_list": ["import unittest.mock\n\nfrom builder.builder_web import GetRemoteModulesGithub\n\n# from builder.builder_web import GetLocalModules\n# from builder.builder_web import GetModulesList\n\n# from builder.builder_web import GetRemoteModulesGithubDirectoryListing\n\n\nclass TestBuilderWeb(unittest.TestCase):\n    def test_GetModulesList(self) -> None:\n        # url: dict = {}\n        # assert GetModulesList(url) == ...\n        ...\n\n    def test_GetLocalModules(self) -> None:\n        # path: str = \"\"\n        # assert GetLocalModules(path) == ...\n        ...\n\n    def test_GetRemoteModulesGithub(self) -> None:\n        repo: str = \"owner/repo\"\n        with self.assertRaises(Exception):\n            GetRemoteModulesGithub(repo, \"Not a valid listing type\")\n            GetRemoteModulesGithub(repo, \"BranchListing\")\n        listing_type = \"DirectoryListing\"\n        with unittest.mock.patch(\n            \"builder.builder_web.GetRemoteModulesGithubDirectoryListing\",\n            return_value=[{\"name\": \"test\"}],\n        ) as mock:\n            assert GetRemoteModulesGithub(repo, listing_type) == mock.return_value\n\n    def test_GetRemoteModulesGithubDirectoryListing(self) -> None:\n        # repo: str = \"\"\n        # assert GetRemoteModulesGithubDirectoryListing(repo) == ...\n        ...\n\n    def test_GetRemoteModulesGithubBranches(self) -> None:\n        # GetRemoteModulesGithubBranches(repo)\n        ...", "\n\nclass TestBuilderWeb(unittest.TestCase):\n    def test_GetModulesList(self) -> None:\n        # url: dict = {}\n        # assert GetModulesList(url) == ...\n        ...\n\n    def test_GetLocalModules(self) -> None:\n        # path: str = \"\"\n        # assert GetLocalModules(path) == ...\n        ...\n\n    def test_GetRemoteModulesGithub(self) -> None:\n        repo: str = \"owner/repo\"\n        with self.assertRaises(Exception):\n            GetRemoteModulesGithub(repo, \"Not a valid listing type\")\n            GetRemoteModulesGithub(repo, \"BranchListing\")\n        listing_type = \"DirectoryListing\"\n        with unittest.mock.patch(\n            \"builder.builder_web.GetRemoteModulesGithubDirectoryListing\",\n            return_value=[{\"name\": \"test\"}],\n        ) as mock:\n            assert GetRemoteModulesGithub(repo, listing_type) == mock.return_value\n\n    def test_GetRemoteModulesGithubDirectoryListing(self) -> None:\n        # repo: str = \"\"\n        # assert GetRemoteModulesGithubDirectoryListing(repo) == ...\n        ...\n\n    def test_GetRemoteModulesGithubBranches(self) -> None:\n        # GetRemoteModulesGithubBranches(repo)\n        ...", ""]}
{"filename": "builder/builder/tests/test_local.py", "chunked_list": ["# import builder\n# filename = \"test_local.json\"\n# builder.BuildFromFile(filename)\n"]}
{"filename": "builder/builder/tests/builder_workflow_test.py", "chunked_list": ["from typing import List\nfrom typing import Tuple\n\nimport pytest\nimport yaml\n\nfrom builder import builder\n\nworkflow_folder = \"builder/tests/workflow_import_test\"\n", "workflow_folder = \"builder/tests/workflow_import_test\"\n\n\n@pytest.mark.parametrize(\n    \"name\",\n    [\n        (\"connect_source_sleep1input\", \"srcsleep1in\"),\n        (\"connect_sleep1input_sleep1input\", \"c2sleep1in\"),\n        (\"connect_c2sleep1in_c2sleep1in\", \"c2c2sleep1in\"),\n        (\"connect_source_c2sleep1in\", \"srcc2sleep1in\"),", "        (\"connect_c2sleep1in_c2sleep1in\", \"c2c2sleep1in\"),\n        (\"connect_source_c2sleep1in\", \"srcc2sleep1in\"),\n        (\"connect_2source_sleep2inputs\", \"2srcsleep2in\"),\n        (\"connect_1source_sleep2inputs\", \"1srcsleep2in\"),\n        (\"connect_2source_2sleep2inputs\", \"2src2sleep2in\"),\n    ],\n)\ndef test_Workflow_Imports(name: Tuple[List[str], str]):\n    modules, module_out = name\n    build, m, _ = builder.BuildFromFile(\n        f\"{workflow_folder}/workflow_imports/{modules}.json\",\n        singlefile=True,\n        expand=True,\n    )\n    configfile: str = str(build[0])\n    snakefile: str = str(build[1])\n    # Remove last newline (test files stripped by syntax formatter)\n    # snakefile = snakefile[:-1]\n    configfileYAML = yaml.safe_load(configfile)\n    with open(f\"{workflow_folder}/modules/{module_out}/config/config.yaml\") as file:\n        configfileYAML_expected = yaml.safe_load(file.read())\n    for key in [\"input_namespace\", \"output_namespace\"]:\n        assert configfileYAML[key] == configfileYAML_expected[key]\n    with open(f\"{workflow_folder}/modules/{module_out}/workflow/Snakefile\") as file:\n        assert str(snakefile) == file.read()", ""]}
{"filename": "builder/builder/tests/builder_test.py", "chunked_list": ["import pytest\n\nfrom builder.builder import Model\nfrom builder.builder import YAMLToConfig\n\n\ndef test_BuildSnakefile():\n    m = Model()\n    # Add modules\n    m.AddModule(\"module1\", {\"snakefile\": \"snakefile1\"})\n    m.AddModule(\"module2\", {\"snakefile\": \"snakefile2\"})\n    m.AddModule(\"module3\", {\"snakefile\": \"snakefile3\"})\n    m.AddConnector(\"conn23\", {\"map\": [\"module2\", \"module3\"]})\n    # Verify Snakefile\n    target_snakefile = \"\"\"configfile: \"config/config.yaml\"\n\nmodule module1:\n    snakefile:\n        config[\"module1\"][\"snakefile\"]\n    config:\n        config[\"module1\"][\"config\"]\nuse rule * from module1 as module1_*\n\nmodule module2:\n    snakefile:\n        config[\"module2\"][\"snakefile\"]\n    config:\n        config[\"module2\"][\"config\"]\nuse rule * from module2 as module2_*\n\nmodule module3:\n    snakefile:\n        config[\"module3\"][\"snakefile\"]\n    config:\n        config[\"module3\"][\"config\"]\nuse rule * from module3 as module3_*\n\"\"\"\n    assert m.BuildSnakefile() == target_snakefile", "\n\ndef test_ConstructSnakefileConfig():\n    m = Model()\n    m.AddModule(\"module1\", {\"snakefile\": \"snakefile1\", \"config\": {\"param1\": \"value1\"}})\n    m.AddModule(\n        \"module2\", {\"snakefile\": \"snakefile2\", \"input_namespace\": \"in3\", \"config\": {}}\n    )\n    m.AddModule(\n        \"module3\", {\"snakefile\": \"snakefile2\", \"input_namespace\": \"in3\", \"config\": {}}\n    )\n    # Namespace connector\n    m.AddConnector(\"conn12\", {\"map\": [\"module1\", \"module2\"]})\n    m.AddConnector(\"conn23\", {\"map\": [\"module2\", \"module3\"]})\n    c = m.ConstructSnakefileConfig()\n    # Verify config\n    assert c[\"module1\"][\"config\"].get(\"param1\", None) == \"value1\"\n    assert (\n        c[\"module2\"][\"config\"][\"input_namespace\"]\n        == c[\"module1\"][\"config\"][\"output_namespace\"]\n    )\n    assert (\n        c[\"module3\"][\"config\"][\"input_namespace\"]\n        == c[\"module2\"][\"config\"][\"output_namespace\"]\n    )", "\n\n@pytest.mark.skip(reason=\"Not implemented\")\ndef test_BuildSnakefileConfig():\n    # m = Model()\n    # m.BuildSnakefileConfig()\n    ...\n\n\n@pytest.mark.skip(reason=\"Not implemented\")\ndef test_SaveWorkflow():\n    # m = Model()\n    # m.SaveWorkflow()\n    ...", "\n@pytest.mark.skip(reason=\"Not implemented\")\ndef test_SaveWorkflow():\n    # m = Model()\n    # m.SaveWorkflow()\n    ...\n\n\ndef test_WrangleName():\n    # WrangleName should produce a unique name each time\n    m = Model()\n    basename = \"basename\"\n    subname = None\n    names = []\n    for _ in range(3):\n        newname = m.WrangleName(basename, subname)\n        assert newname not in names\n        m.AddModule(newname, {})\n    subname = \"subname\"\n    for _ in range(3):\n        newname = m.WrangleName(basename, subname)\n        assert newname not in names\n        m.AddModule(newname, {})", "def test_WrangleName():\n    # WrangleName should produce a unique name each time\n    m = Model()\n    basename = \"basename\"\n    subname = None\n    names = []\n    for _ in range(3):\n        newname = m.WrangleName(basename, subname)\n        assert newname not in names\n        m.AddModule(newname, {})\n    subname = \"subname\"\n    for _ in range(3):\n        newname = m.WrangleName(basename, subname)\n        assert newname not in names\n        m.AddModule(newname, {})", "\n\ndef test_WrangledNameList():\n    # Produces a list of all namespaces\n    m = Model()\n    m.AddModule(\"module1\", {})\n    m.AddModule(\"module2\", {})\n    m.AddModule(\"module3\", {})\n    wrangledNameList = m.WrangledNameList()\n    assert len(set(wrangledNameList)) == 3", "\n\ndef test_WrangleRuleName():\n    m = Model()\n    rulename_in = \"replace/special.and.(remove).brackets/but_not_underscores\"\n    rulename_out = \"replace_special_and_remove_brackets_but_not_underscores\"\n    assert m.WrangleRuleName(rulename_in) == rulename_out\n\n\ndef test_AddModule_SingleInputNamespace():\n    m = Model()\n    name = \"module1\"\n    module = {\n        \"rulename\": \"module1\",\n        \"nodetype\": \"moduletype1\",\n        \"snakefile\": \"snakefile1\",\n        \"config\": {\n            \"input_namespace\": \"input_namespace1\",\n            \"output_namespace\": \"output_namespace1\",\n            \"param1\": \"value1\",\n        },\n    }\n    m.AddModule(name, module)\n    # Verify module assigned correctly\n    assert m.nodes[0].name == name\n    assert m.nodes[0].nodetype == \"moduletype1\"\n    # Verify module attributes assigned correctly\n    for key in module:\n        # output_namespace is wrangled\n        if key not in [\"output_namespace\"]:\n            assert getattr(m.nodes[0], key) == module[key]", "\ndef test_AddModule_SingleInputNamespace():\n    m = Model()\n    name = \"module1\"\n    module = {\n        \"rulename\": \"module1\",\n        \"nodetype\": \"moduletype1\",\n        \"snakefile\": \"snakefile1\",\n        \"config\": {\n            \"input_namespace\": \"input_namespace1\",\n            \"output_namespace\": \"output_namespace1\",\n            \"param1\": \"value1\",\n        },\n    }\n    m.AddModule(name, module)\n    # Verify module assigned correctly\n    assert m.nodes[0].name == name\n    assert m.nodes[0].nodetype == \"moduletype1\"\n    # Verify module attributes assigned correctly\n    for key in module:\n        # output_namespace is wrangled\n        if key not in [\"output_namespace\"]:\n            assert getattr(m.nodes[0], key) == module[key]", "\n\ndef test_AddModule_MultipleInputNamespaces():\n    m = Model()\n    name = \"module2\"\n    module = {\n        \"rulename\": \"module2\",\n        \"nodetype\": \"moduletype2\",\n        \"snakefile\": \"snakefile2\",\n        \"config\": {\n            \"param2\": \"value2\",\n            \"input_namespace\": {\n                \"in2a\": \"input_namespace2a\",\n                \"in2b\": \"input_namespace2b\",\n            },\n            \"output_namespace\": \"output_namespace2\",\n        },\n    }\n    m.AddModule(name, module)\n    # Verify module assigned correctly\n    assert m.nodes[0].name == name\n    assert m.nodes[0].nodetype == \"moduletype2\"\n    # Verify module attributes assigned correctly\n    for key in module:\n        # output_namespace is wrangled\n        if key not in [\"output_namespace\"]:\n            assert getattr(m.nodes[0], key) == module[key]", "\n\ndef test_AddModule_DuplicateName():\n    m = Model()\n    m.AddModule(\"module\", {})\n    m.AddModule(\"module\", {})\n    m.AddModule(\"module\", {})\n    assert len(m.nodes) == 3\n    assert len(set([n.rulename for n in m.nodes])) == 3\n", "\n\ndef test_AddConnector_SingleInput():\n    # Namespace connector\n    m = Model()\n    module1 = m.AddModule(\"module1\", {})\n    module2 = m.AddModule(\"module2\", {})\n    m.AddConnector(\"conn12\", {\"map\": [\"module1\", \"module2\"]})\n    # Verify module namespaces connect appropriately\n    assert module1.output_namespace == module2.input_namespace", "\n\ndef test_AddConnector_MultiInput():\n    # Namespace connector\n    m = Model()\n    module1 = m.AddModule(\n        \"module1\",\n        {\n            \"snakefile\": \"snakefile1\",\n            \"config\": {\n                \"input_namespace\": \"in1\",\n                \"output_namespace\": \"out1\",\n            },\n        },\n    )\n    module2 = m.AddModule(\n        \"module2\",\n        {\n            \"snakefile\": \"snakefile2\",\n            \"config\": {\n                \"input_namespace\": {\n                    \"in2a\": \"input2_A\",\n                    \"in2b\": \"input2_B\",\n                },\n                \"output_namespace\": \"out2\",\n            },\n        },\n    )\n    # Connect the single output from module1 to the first input of module2\n    m.AddConnector(\"conn12\", {\"map\": [{\"in2a\": \"module1\"}, \"module2\"]})\n    # Verify module namespaces connect appropriately\n    assert module1.output_namespace == module2.input_namespace[\"in2a\"]", "\n\ndef test_GetNodeByName():\n    m = Model()\n    node = m.AddModule(\"module1\", {})\n    assert m.GetNodeByName(\"module1\") == node\n\n\ndef test_NodeIsTerminus():\n    # Terminus nodes have no forward connections\n    m = Model()\n    node1 = m.AddModule(\"module1\", {})  # Link to node2\n    node2 = m.AddModule(\"module2\", {})  # Link to node3\n    node3 = m.AddModule(\"module3\", {})  # No links\n    node4 = m.AddModule(\"module4\", {})  # Isolated\n    m.AddConnector(\"conn12\", {\"map\": [\"module1\", \"module2\"]})\n    m.AddConnector(\"conn23\", {\"map\": [\"module2\", \"module3\"]})\n    assert m.NodeIsTerminus(node1) is False\n    assert m.NodeIsTerminus(node2) is False\n    assert m.NodeIsTerminus(node3)\n    assert m.NodeIsTerminus(node4)", "def test_NodeIsTerminus():\n    # Terminus nodes have no forward connections\n    m = Model()\n    node1 = m.AddModule(\"module1\", {})  # Link to node2\n    node2 = m.AddModule(\"module2\", {})  # Link to node3\n    node3 = m.AddModule(\"module3\", {})  # No links\n    node4 = m.AddModule(\"module4\", {})  # Isolated\n    m.AddConnector(\"conn12\", {\"map\": [\"module1\", \"module2\"]})\n    m.AddConnector(\"conn23\", {\"map\": [\"module2\", \"module3\"]})\n    assert m.NodeIsTerminus(node1) is False\n    assert m.NodeIsTerminus(node2) is False\n    assert m.NodeIsTerminus(node3)\n    assert m.NodeIsTerminus(node4)", "\n\ndef test_YAMLToConfig():\n    content = \"\"\"singleton: alone\nmodules:\n    name1: first\n    name2: second\n\"\"\"\n    target = \"\"\"config={}\nconfig[\"singleton\"]=\"alone\"\nconfig[\"modules\"]={}\nconfig[\"modules\"][\"name1\"]=\"first\"\nconfig[\"modules\"][\"name2\"]=\"second\"\n\"\"\"\n    assert YAMLToConfig(content) == target", ""]}
{"filename": "builder/builder/tests/ImportWorkflow_test.py", "chunked_list": ["from builder.ImportWorkflow import ImportWorkflowDir\nfrom builder.ImportWorkflow import ParseFunctionSignature\n\n\"\"\" Check for github (or other) snakefiles ---- CURRENTLY UNUSED\nassert isinstance(m.nodes[1], dict)\nassert m.nodes[1].snakefile.get(\"function\", None) == \"calling_fcn\"\nassert m.nodes[1].snakefile.get(\"args\", []) == [\"arg1\", \"arg2\"]\nassert m.nodes[1].snakefile.get(\"kwargs\", {}) == {\n    \"kwarg1\": \"kwval1\",\n    \"kwarg2\": \"kwval2\",", "    \"kwarg1\": \"kwval1\",\n    \"kwarg2\": \"kwval2\",\n}\n\"\"\"\n\n\ndef test_ImportWorkflow_ImportWorkflowDir() -> None:\n    m = ImportWorkflowDir(\n        \"builder/tests/workflow_import_test/modules/skeleton\",\n    )\n    assert len(m.nodes) == 3\n\n    assert m.nodes[0].name == \"module1\"\n    assert (\n        m.nodes[0].snakefile\n        == \"builder/tests/workflow_import_test/modules/source/workflow/Snakefile\"\n    )\n    assert m.nodes[0].output_namespace == \"module1\"\n\n    assert m.nodes[1].name == \"module2\"\n    assert (\n        m.nodes[1].snakefile\n        == \"builder/tests/workflow_import_test/modules/sleep1input/workflow/Snakefile\"\n    )\n\n    assert m.nodes[1].input_namespace == \"module1\"\n    assert m.nodes[1].output_namespace == \"module2\"\n\n    assert m.nodes[2].name == \"module3\"\n    assert (\n        m.nodes[2].snakefile\n        == \"builder/tests/workflow_import_test/modules/sleep2inputs/workflow/Snakefile\"\n    )\n    assert m.nodes[2].input_namespace == {\"in1\": \"module1\", \"in2\": \"module2\"}", "\n\ndef test_ParesFunctionSignature():\n    c = 'fcn(\"pos1\", 2, kw1=\"val3\", kw2=4)'\n    name, args, kwargs = ParseFunctionSignature(c)\n    assert name == \"fcn\"\n    assert args == (\"pos1\", 2)\n    assert kwargs == {\"kw1\": \"val3\", \"kw2\": 4}\n", ""]}
{"filename": "runner/setup.py", "chunked_list": ["#!/usr/bin/env python\nfrom setuptools import setup\n\n\nsetup(\n    name=\"runner\",\n    version=\"0.1\",\n    description=\"GRAPEVNE runner\",\n    packages=[\"runner\", \"runner.snakemake_runner\"],\n    zip_safe=False,", "    packages=[\"runner\", \"runner.snakemake_runner\"],\n    zip_safe=False,\n)\n"]}
{"filename": "runner/runner/runner.py", "chunked_list": ["import json\n\nfrom .snakemake_runner import snakefile\n\n\ndef Build(data: dict) -> str:\n    \"\"\"Builds a workflow from a JSON object.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        build_data: str = snakefile.Build(json.loads(data[\"content\"]))\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return build_data", "\n\ndef DeleteAllOutput(data: dict) -> dict:\n    \"\"\"Deletes all output files from a workflow.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        del_data: dict = snakefile.DeleteAllOutput(data[\"content\"])\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return del_data\n", "\n\ndef Lint(data: dict) -> dict:\n    \"\"\"Lints a workflow given a directory location.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        lint_response: dict = snakefile.LintContents(data[\"content\"])\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return lint_response\n", "\n\ndef LintContents(data: dict) -> dict:\n    \"\"\"Lints a workflow given a Snakefile as string.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        build_data: str = snakefile.Build(json.loads(data[\"content\"]))\n        lint_response: dict = snakefile.LintContents(build_data)\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return lint_response", "\n\ndef Launch(data: dict, **kwargs) -> dict:\n    \"\"\"Launches a workflow in a given location.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        launch_response: dict = snakefile.Launch(data[\"content\"], **kwargs)\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return launch_response\n", "\n\ndef Launch_cmd(data: dict, *args, **kwargs) -> dict:\n    \"\"\"Returns the launch command for a workflow\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        snakemake_args = data.get(\"args\", \"\").split(\" \")\n        args = *args, *data.get(\"targets\", [])\n        cmd, workdir = snakefile.Launch_cmd(\n            data[\"content\"],\n            *snakemake_args,\n            *args,\n            **kwargs,\n        )\n        launch_response: dict = {\"command\": cmd, \"workdir\": workdir}\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return launch_response", "\n\ndef Tokenize(data: dict) -> dict:\n    \"\"\"Tokenizes a workflow given a Snakefile as string.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        tokenized_data: dict = snakefile.SplitByRulesFileContent(data[\"content\"])\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return tokenized_data\n", "\n\ndef LoadWorkflow(data: dict) -> dict:\n    \"\"\"Tokenizes a workflow and returns rule/module nodes.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        tokenized_data: dict = snakefile.LoadWorkflow(data[\"content\"])\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return tokenized_data\n", "\n\ndef TokenizeFromFile(data: dict) -> dict:\n    \"\"\"Tokenizes a workflow given a workflow location.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        tokenized_data: dict = snakefile.LoadWorkflow(data[\"content\"])\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return tokenized_data\n", "\n\ndef FullTokenizeFromFile(data: dict) -> dict:\n    \"\"\"Creates a working copy of the Snakefile and tokenizes.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        tokenized_data: dict = snakefile.FullTokenizeFromFile(data[\"content\"])\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return tokenized_data\n", "\n\ndef CheckNodeDependencies(data: dict) -> dict:\n    \"\"\"Checks the dependencies of a node, given the node and first-order inputs.\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        js = json.loads(data[\"content\"])\n        snakemake_launcher = data.get(\"backend\", \"\")\n        response: dict = snakefile.CheckNodeDependencies(\n            js,\n            snakemake_launcher,\n        )\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return response", "\n\ndef SnakemakeRun(data: dict) -> dict:\n    \"\"\"Run snakemake using a customised snakemake package import\"\"\"\n    if data[\"format\"] == \"Snakefile\":\n        capture_output = data[\"content\"].get(\"capture_output\", False)\n        stdout, stderr = snakefile.snakemake_run(\n            data[\"content\"][\"command\"].split(\" \"),\n            data[\"content\"][\"workdir\"],\n            capture_output=capture_output,\n            snakemake_launcher=data[\"content\"].get(\"backend\", \"\"),\n        )\n        response = {\n            \"stdout\": stdout,\n            \"stderr\": stderr,\n        }\n    else:\n        raise ValueError(f\"Format not supported: {data['format']}\")\n    return response", ""]}
{"filename": "runner/runner/TokenizeFile.py", "chunked_list": ["import io\nimport tokenize\nfrom typing import List\nfrom typing import Tuple\n\n\nclass TokenizeFile:\n    \"\"\"Tokenize file and calculate indentation levels\"\"\"\n\n    def __init__(self, content: str):\n        \"\"\"Tokenize file and calculate indentation levels\"\"\"\n        self.content: str = \"\"\n        self.tokens: List[Tuple] = []\n        self.indent_levels: List[int] = []\n        self.blocks: List[int] = []\n        self.rootblock: List[int] = []\n\n        self.content = content\n        file = io.BytesIO(bytes(content, \"utf-8\"))\n        self.tokens = list(tokenize.tokenize(file.readline))\n        self.CalcIndentLevels()\n        self.CalcBlocks()\n\n    def PrintTokens(self, log_fn=print, filter_token=None):\n        \"\"\"Pretty print tokens, with optional filter (token number or string)\"\"\"\n        for ix, token in enumerate(self.tokens):\n            toknum, tokval, start, end, line = token\n            match = False\n            if not filter_token:\n                match = True\n            if isinstance(filter_token, int) and toknum == filter_token:\n                match = True\n            if isinstance(filter_token, str) and tokval == filter_token:\n                match = True\n            if match:\n                log_fn(ix, token)\n\n    def PrintLineInfo(self):\n        \"\"\"Print line number, indent level and line\"\"\"\n        lineno = 0\n        print(\"Block Indent Line\")\n        with io.StringIO(self.content) as file:\n            for line in file:\n                blockno = self.rootblock[lineno + 1]\n                indentlevel = self.GetIndentLevelFromLinenumber(lineno + 1)\n                print(f\"{blockno} {indentlevel} {line}\", end=\"\")\n                lineno = lineno + 1\n\n    def FindTokenSequence(self, search_seq) -> List[int]:\n        \"\"\"Find token sequence in longer sequence and return indices\"\"\"\n        if not len(search_seq) or not len(self.tokens):\n            return []\n        searchpos = 0\n        foundlist = []\n        for ix, token in enumerate(self.tokens):\n            toknum, tokval, _, _, _ = token\n            searchpos = (\n                searchpos + 1 if (toknum, tokval) == search_seq[searchpos] else 0\n            )\n            # Search term found\n            if searchpos == len(search_seq):\n                foundlist.append(ix - searchpos + 1)\n                searchpos = 0\n        return foundlist\n\n    def GetLineNumberFromTokenIndex(self, pos) -> int:\n        \"\"\"Return line number from token index\"\"\"\n        _, _, start, _, _ = self.tokens[pos]\n        row, _ = start\n        return row\n\n    def GetIndentLevelFromLinenumber(self, linenumber):\n        \"\"\"Return indentation level from line number\"\"\"\n        return self.indent_levels[linenumber]\n\n    def GetIndentLevelFromTokenIndex(self, pos):\n        \"\"\"Return indentation level from token index\"\"\"\n        indent = 0\n        for token in self.tokens[: pos - 1]:\n            toknum, _, _, _, _ = token\n            if toknum == 5:  # 5 = indent\n                indent += 1\n            if toknum == 6:  # 6 = dedent\n                indent -= 1\n        return indent\n\n    def GetFirstTokenIndexOfLine(self, linenumber):\n        \"\"\"Return first token index of line\"\"\"\n        for ix, token in enumerate(self.tokens):\n            _, _, start, _, _ = token\n            row, col = start\n            if row == linenumber:\n                return ix\n        return None\n\n    def GetContentBetweenTokenIndices(self, pos_from, pos_to):\n        \"\"\"Return content between token indices\"\"\"\n        tokens = []\n        startrow = self.tokens[pos_from][2][0] - 1\n        endrow = self.tokens[pos_from][3][0] - 1\n        for token in self.tokens[pos_from : pos_to + 1]:\n            toknum, tokval, start, end, line = token\n            tokens.append(\n                tokenize.TokenInfo(\n                    toknum,\n                    tokval,\n                    (start[0] - startrow, start[1]),\n                    (end[0] - endrow, end[1]),\n                    line,\n                )\n            )\n        return tokenize.untokenize(tokens)\n\n    def GetBlockFromIndex(self, blockno: int):\n        \"\"\"Return content of block from block number\"\"\"\n        return self.GetContentBetweenLines(\n            self.rootblock.index(blockno),\n            len(self.rootblock) - self.rootblock[::-1].index(blockno) - 1,\n        )\n\n    def GetContentBetweenLines(self, line_from, line_to):\n        \"\"\"Return content between line numbers\"\"\"\n        content = \"\"\n        for line in range(line_from, line_to + 1):\n            token = self.tokens[self.GetFirstTokenIndexOfLine(line)]\n            toknum, tokval, start, end, line = token\n            content += line\n        return content\n\n    def GetContentOfIndentBlock(self, linenumber, indentlevel):\n        \"\"\"Return content of indent block\"\"\"\n        block_list = self.GetBlockList(indentlevel)\n        blockno = block_list[linenumber]\n        line_from = block_list.index(blockno)\n        line_to = len(block_list) - block_list[::-1].index(blockno) - 1\n        return self.GetContentBetweenLines(line_from, line_to)\n\n    def GetBlock(self, search_seq, ignore_tokens):\n        \"\"\"Return content of block containing search sequence\"\"\"\n        pos = self.FindTokenSequence(search_seq)\n        if not pos:\n            return None\n        linenumber = self.GetLineNumberFromTokenIndex(pos[0]) + 1\n        indentlevel = self.GetIndentLevelFromLinenumber(linenumber)\n        return self.GetContentOfIndentBlock(linenumber, indentlevel)\n\n    def CalcIndentLevels(self):\n        \"\"\"Return indentation levels for each line in the file\"\"\"\n        blockno = 0\n        lastlinenumber = self.tokens[-1][2][0]\n        indent_list = [None] * (lastlinenumber + 1)\n        rootblock = [None] * (lastlinenumber + 1)\n        indent = 0\n        lastindent = 1\n        for token in self.tokens:\n            toknum, _, start, _, _ = token\n            row, _ = start\n            if toknum == 5:  # 5 = indent\n                indent += 1\n            if toknum == 6:  # 6 = dedent\n                indent -= 1\n            # Update indent for line\n            indent_list[row] = indent\n            if lastindent == 0 and indent > 0:\n                blockno = blockno + 1\n                rootblock[row - 1] = blockno\n            rootblock[row] = blockno\n            lastindent = indent\n        self.indent_levels = indent_list\n        self.rootblock = rootblock\n\n    def GetIndentLevels(self):\n        \"\"\"Return indentation levels for each line in the file\"\"\"\n        return self.indent_levels\n\n    def CalcBlocks(self):\n        \"\"\"Return block membership by number\"\"\"\n        self.blocks = self.GetMembershipFromList(self.indent_levels)\n\n    def GetBlockList(self, level=1):\n        \"\"\"Return block membership by number, separated at the specified level\"\"\"\n        if level:\n            blocks = [\n                block if block <= level else level for block in self.indent_levels\n            ]\n        else:\n            blocks = self.blocks\n        return self.GetMembershipFromList(blocks)\n\n    def GetMembershipFromList(self, blocks):\n        \"\"\"Return membership indices indicating which block each line belongs to\"\"\"\n        index = 0\n        lastblock = 0\n        block_membership = [None] * len(blocks)\n        for ix, block in enumerate(blocks):\n            if block != lastblock:\n                index += 1\n                lastblock = block\n            block_membership[ix] = index\n        return block_membership", ""]}
{"filename": "runner/runner/__init__.py", "chunked_list": ["from .runner import Build  # noqa: F401\nfrom .runner import CheckNodeDependencies  # noqa: F401\nfrom .runner import DeleteAllOutput  # noqa: F401\nfrom .runner import FullTokenizeFromFile  # noqa: F401\nfrom .runner import Launch  # noqa: F401\nfrom .runner import Launch_cmd  # noqa: F401\nfrom .runner import Lint  # noqa: F401\nfrom .runner import LintContents  # noqa: F401\nfrom .runner import LoadWorkflow  # noqa: F401\nfrom .runner import SnakemakeRun  # noqa: F401", "from .runner import LoadWorkflow  # noqa: F401\nfrom .runner import SnakemakeRun  # noqa: F401\nfrom .runner import Tokenize  # noqa: F401\nfrom .runner import TokenizeFromFile  # noqa: F401\n"]}
{"filename": "runner/runner/TokenizeFile_test.py", "chunked_list": ["import pytest\n\nfrom runner.TokenizeFile import TokenizeFile\n\n\ncontent = \"\"\"\\\nrule:\n    rule_item\nlevel2:\n    level3:", "level2:\n    level3:\n        level3_item\ninput:\n    input_item\noutput:\n    output_item\n\"\"\"\n\n\ndef test_FindTokenSequence():\n    tf = TokenizeFile(content)\n    search_seq = [(1, \"rule\"), (54, \":\")]\n    assert tf.FindTokenSequence(search_seq) == [1]\n    search_seq = [(5, \"    \")]  # only finds single indents\n    assert tf.FindTokenSequence(search_seq) == [4, 11, 23, 30]", "\n\ndef test_FindTokenSequence():\n    tf = TokenizeFile(content)\n    search_seq = [(1, \"rule\"), (54, \":\")]\n    assert tf.FindTokenSequence(search_seq) == [1]\n    search_seq = [(5, \"    \")]  # only finds single indents\n    assert tf.FindTokenSequence(search_seq) == [4, 11, 23, 30]\n\n\ndef test_GetLineNumberFromTokenIndex():\n    tf = TokenizeFile(content)\n    for ix, token in enumerate(tf.tokens):\n        _, _, start, _, _ = token\n        row, _ = start\n        assert row == tf.GetLineNumberFromTokenIndex(ix)", "\n\ndef test_GetLineNumberFromTokenIndex():\n    tf = TokenizeFile(content)\n    for ix, token in enumerate(tf.tokens):\n        _, _, start, _, _ = token\n        row, _ = start\n        assert row == tf.GetLineNumberFromTokenIndex(ix)\n\n\ndef test_GetIndentLevelFromLinenumber():\n    tf = TokenizeFile(content)\n    expected = [0, 0, 1, 0, 1, 2, 0, 1, 0, 1]\n    for linenumber, _ in enumerate(expected):\n        assert tf.GetIndentLevelFromLinenumber(linenumber) == expected[linenumber]", "\n\ndef test_GetIndentLevelFromLinenumber():\n    tf = TokenizeFile(content)\n    expected = [0, 0, 1, 0, 1, 2, 0, 1, 0, 1]\n    for linenumber, _ in enumerate(expected):\n        assert tf.GetIndentLevelFromLinenumber(linenumber) == expected[linenumber]\n\n\ndef test_GetIndentLevelFromTokenIndex():\n    tf = TokenizeFile(content)\n    pos = [1]\n    expected = [0]\n    for ix in range(len(pos)):\n        assert tf.GetIndentLevelFromTokenIndex(pos[ix]) == expected[ix]", "\ndef test_GetIndentLevelFromTokenIndex():\n    tf = TokenizeFile(content)\n    pos = [1]\n    expected = [0]\n    for ix in range(len(pos)):\n        assert tf.GetIndentLevelFromTokenIndex(pos[ix]) == expected[ix]\n\n\ndef test_GetFirstTokenIndexOfLine():\n    tf = TokenizeFile(content)\n    expected_list = [0, 1, 4, 7, 11, 15, 18, 23, 26, 30, 33]\n    for linenumber, expected in enumerate(expected_list):\n        assert tf.GetFirstTokenIndexOfLine(linenumber) == expected", "\ndef test_GetFirstTokenIndexOfLine():\n    tf = TokenizeFile(content)\n    expected_list = [0, 1, 4, 7, 11, 15, 18, 23, 26, 30, 33]\n    for linenumber, expected in enumerate(expected_list):\n        assert tf.GetFirstTokenIndexOfLine(linenumber) == expected\n\n\ndef test_GetContentBetweenTokensIndices():\n    tf = TokenizeFile(content)\n    indices = [\n        [1, 2],\n    ]\n    expected = [\n        \"rule:\",\n    ]\n    for ix in range(len(indices)):\n        assert (\n            tf.GetContentBetweenTokenIndices(indices[ix][0], indices[ix][1])\n            == expected[ix]\n        )", "def test_GetContentBetweenTokensIndices():\n    tf = TokenizeFile(content)\n    indices = [\n        [1, 2],\n    ]\n    expected = [\n        \"rule:\",\n    ]\n    for ix in range(len(indices)):\n        assert (\n            tf.GetContentBetweenTokenIndices(indices[ix][0], indices[ix][1])\n            == expected[ix]\n        )", "\n\ndef test_GetContentBetweenLines():\n    tf = TokenizeFile(content)\n    lines = [\n        [1, 2],\n    ]\n    expected = [\n        \"rule:\\n    rule_item\\n\",\n    ]\n    for ix in range(len(lines)):\n        assert tf.GetContentBetweenLines(lines[ix][0], lines[ix][1]) == expected[ix]", "\n\ndef test_GetContentOfIndentBlock():\n    tf = TokenizeFile(content)\n    linenumber = [4]\n    indent = [1]\n    expected_list = [\"    level3:\\n        level3_item\\n\"]\n    # can introduce zip(..., strict=True) once python 3.10 is the min version\n    for line, indent, expected in zip(linenumber, indent, expected_list):\n        assert tf.GetContentOfIndentBlock(line, indent) == expected", "\n\n@pytest.mark.parametrize(\n    \"blockno,expected\",\n    [\n        (1, \"rule:\\n    rule_item\\n\"),\n        (2, \"level2:\\n    level3:\\n        level3_item\\n\"),\n        (3, \"input:\\n    input_item\\n\"),\n        (4, \"output:\\n    output_item\\n\"),\n    ],", "        (4, \"output:\\n    output_item\\n\"),\n    ],\n)\ndef test_GetBlockFromNumber(blockno, expected):\n    tf = TokenizeFile(content)\n    assert tf.GetBlockFromIndex(blockno) == expected\n\n\ndef test_GetBlock():\n    tf = TokenizeFile(content)\n    search_seq = [(1, \"level2\"), (54, \":\")]\n    ignore_tokens = []\n    block = tf.GetBlock(search_seq, ignore_tokens)\n    expected = \"    level3:\\n        level3_item\\n\"\n    assert block == expected", "def test_GetBlock():\n    tf = TokenizeFile(content)\n    search_seq = [(1, \"level2\"), (54, \":\")]\n    ignore_tokens = []\n    block = tf.GetBlock(search_seq, ignore_tokens)\n    expected = \"    level3:\\n        level3_item\\n\"\n    assert block == expected\n\n\ndef test_GetIndentLevels():\n    tf = TokenizeFile(content)\n    indent_level = tf.GetIndentLevels()\n    # index[0]=encoding; index[-1]=dedent-to-start\n    expected_list = [0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0]\n    assert indent_level == expected_list", "\ndef test_GetIndentLevels():\n    tf = TokenizeFile(content)\n    indent_level = tf.GetIndentLevels()\n    # index[0]=encoding; index[-1]=dedent-to-start\n    expected_list = [0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0]\n    assert indent_level == expected_list\n\n\ndef test_GetBlockList():\n    tf = TokenizeFile(content)\n    block_list = tf.GetBlockList(level=1)\n    # index[0]=encoding; index[-1]=dedent-to-start\n    expected_list = [0, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8]\n    assert block_list == expected_list", "\ndef test_GetBlockList():\n    tf = TokenizeFile(content)\n    block_list = tf.GetBlockList(level=1)\n    # index[0]=encoding; index[-1]=dedent-to-start\n    expected_list = [0, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8]\n    assert block_list == expected_list\n\n\ndef test_GetMembershipFromList():\n    tf = TokenizeFile(content)\n    indent_levels = [1, 1, 1, 2, 2, 2, 3, 2, 3]\n    expected_list = [1, 1, 1, 2, 2, 2, 3, 4, 5]\n    member_list = tf.GetMembershipFromList(indent_levels)\n    assert member_list == expected_list", "\ndef test_GetMembershipFromList():\n    tf = TokenizeFile(content)\n    indent_levels = [1, 1, 1, 2, 2, 2, 3, 2, 3]\n    expected_list = [1, 1, 1, 2, 2, 2, 3, 4, 5]\n    member_list = tf.GetMembershipFromList(indent_levels)\n    assert member_list == expected_list\n"]}
{"filename": "runner/runner/snakemake_runner_test/snakefile_test.py", "chunked_list": ["import json\nimport os\nimport shutil\nfrom unittest import mock\n\nimport pytest\n\nfrom runner.snakemake_runner.snakefile import Build\nfrom runner.snakemake_runner.snakefile import FullTokenizeFromFile\nfrom runner.snakemake_runner.snakefile import GetMissingFileDependencies_FromContents", "from runner.snakemake_runner.snakefile import FullTokenizeFromFile\nfrom runner.snakemake_runner.snakefile import GetMissingFileDependencies_FromContents\nfrom runner.snakemake_runner.snakefile import IsolatedTempFile\nfrom runner.snakemake_runner.snakefile import LintContents\nfrom runner.snakemake_runner.snakefile import SplitByRulesFileContent\nfrom runner.snakemake_runner.snakefile import SplitByRulesFromFile\n\nsnakemakerunner = shutil.which(\"snakemake\")\n\n\ndef test_Snakefile_Build():\n    rules = {\n        \"block\": [\n            {\"name\": \"name1\", \"type\": \"type1\", \"content\": \"code1a\\ncode1b\"},\n            {\"name\": \"name2\", \"type\": \"type2\", \"content\": \"code2\"},\n            {\"name\": \"name3\", \"type\": \"type3\", \"content\": \"\\ncode3\\n\"},\n        ]\n    }\n    expected = \"code1a\\ncode1b\\ncode2\\n\\ncode3\\n\\n\"\n    contents = Build(rules)\n    assert contents == expected", "\n\ndef test_Snakefile_Build():\n    rules = {\n        \"block\": [\n            {\"name\": \"name1\", \"type\": \"type1\", \"content\": \"code1a\\ncode1b\"},\n            {\"name\": \"name2\", \"type\": \"type2\", \"content\": \"code2\"},\n            {\"name\": \"name3\", \"type\": \"type3\", \"content\": \"\\ncode3\\n\"},\n        ]\n    }\n    expected = \"code1a\\ncode1b\\ncode2\\n\\ncode3\\n\\n\"\n    contents = Build(rules)\n    assert contents == expected", "\n\ndef test_Snakefile_Lint():\n    # Load test case\n    with open(\"runner/snakemake_runner_test/Snakefile\", \"r\") as file:\n        contents = file.read()\n    lint_return = LintContents(contents)\n    with open(\"runner/snakemake_runner_test/Snakefile_lint\", \"r\") as file:\n        expected = json.load(file)\n    # Linters will differ by their Snakefile filenames:\n    for rule in lint_return[\"rules\"]:\n        rule[\"for\"][\"snakefile\"] = \"runner/Snakefile\"\n    assert lint_return == expected", "\n\ndef test_Snakefile_SplitByRules():\n    # Load test case\n    with open(\"runner/snakemake_runner_test/Snakefile\", \"r\") as file:\n        contents = file.read()\n    # Tokenise and split by rule\n    with mock.patch(\n        \"runner.snakemake_runner.snakefile.DagLocal\",\n        return_value={\"nodes\": [], \"links\": []},\n    ):\n        result = SplitByRulesFileContent(contents)\n    assert isinstance(result, dict)\n    assert isinstance(result[\"block\"], list)\n    assert len(result[\"block\"]) == 6\n    for block in result[\"block\"]:\n        assert \"name\" in block\n        assert \"content\" in block", "\n\ndef test_FullTokenizeFromFile():\n    filename = os.path.abspath(\"../examples/snakemake-short-tutorial/Snakefile\")\n    blocks = FullTokenizeFromFile(filename)\n    expected_links = [\n        [\"call_variants\", \"all\"],\n        [\"plot_quals\", \"all\"],\n        [\"in_genome\", \"call_variants\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"in_genome\", \"map_reads\"],\n        [\"in_samples\", \"map_reads\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"in_genome\", \"map_reads\"],\n        [\"in_samples\", \"map_reads\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"in_genome\", \"map_reads\"],\n        [\"in_samples\", \"map_reads\"],\n        [\"call_variants\", \"plot_quals\"],\n    ]\n    assert blocks[\"links\"][\"content\"] == expected_links", "\n\ndef test_SplitByRulesFromFile():\n    filename = os.path.abspath(\"../examples/snakemake-short-tutorial/Snakefile\")\n    blocks = SplitByRulesFromFile(filename)\n    # Some datafiles are present, so those links are not included in the DAG\n    expected_links = [\n        [\"call_variants\", \"all\"],\n        [\"plot_quals\", \"all\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"call_variants\", \"plot_quals\"],\n    ]\n    assert blocks[\"links\"][\"content\"] == expected_links", "\n\ndef test_SplitByRulesFromFile_submodules():\n    filename = os.path.abspath(\"../examples/submodules/Snakefile\")\n    blocks = SplitByRulesFromFile(filename)\n    # Some datafiles are present, so those links are not included in the DAG\n    expected_links = [\n        [\"rule_6\", \"all\"],\n        [\"other_rule_5\", \"rule_6\"],\n        [\"other_rule_4\", \"other_rule_5\"],\n        [\"other_rule_3\", \"other_rule_4\"],\n        [\"other_rule_2\", \"other_rule_3\"],\n        [\"rule_1\", \"other_rule_2\"],\n    ]\n    assert blocks[\"links\"][\"content\"] == expected_links", "\n\ndef test_SplitByRulesFileContent():\n    filename = os.path.abspath(\"../examples/snakemake-short-tutorial/Snakefile\")\n    with open(filename, \"r\") as file:\n        blocks = SplitByRulesFileContent(file.read())\n    expected_links = [\n        [\"call_variants\", \"all\"],\n        [\"plot_quals\", \"all\"],\n        [\"in_genome\", \"call_variants\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"sort_alignments\", \"call_variants\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"in_genome\", \"map_reads\"],\n        [\"in_samples\", \"map_reads\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"in_genome\", \"map_reads\"],\n        [\"in_samples\", \"map_reads\"],\n        [\"map_reads\", \"sort_alignments\"],\n        [\"in_genome\", \"map_reads\"],\n        [\"in_samples\", \"map_reads\"],\n        [\"call_variants\", \"plot_quals\"],\n    ]\n    assert blocks[\"links\"][\"content\"] == expected_links", "\n\ndef test_IsolatedTempFile():\n    contents = \"Sample contents\"\n    with IsolatedTempFile(contents) as temp_filename:\n        filename = temp_filename\n        assert os.path.exists(filename)\n        with open(filename, \"r\") as infile:\n            assert infile.read() == contents\n    assert not os.path.exists(filename)", "\n\n@pytest.mark.skipif(not snakemakerunner, reason=\"Snakemake not installed\")\ndef test_GetMissingFileDependencies() -> None:\n    # Provide a list of files with no associated inputs, then mark them as\n    # outputs of a later rule one at a time; lists should correspond\n    target = [\"a.txt\", \"b.txt\", \"c.txt\"]\n    test_snakefile: str = \"\"\"\nrule all:\n    input:\n\"\"\"\n    for trgt in target:\n        test_snakefile += f'        \"{trgt}\",\\n'\n    deps = GetMissingFileDependencies_FromContents(test_snakefile)\n    assert len(deps) == len(target)\n    assert set(deps) == set(target)\n    test_snakefile += \"\"\"\nrule missing_targets:\n    output:\n\"\"\"\n    # Recurse through, emptying list\n    for _ in range(len(target)):\n        test_snakefile += f'        \"{target[0]}\",\\n'\n        target.pop(0)\n        deps = GetMissingFileDependencies_FromContents(test_snakefile)\n        assert len(deps) == len(target)\n        assert set(deps) == set(target)\n    assert len(target) == 0", "\n\ndef test_GetMissingFileDependencies_FullReturn() -> None:\n    # Returns all dependencies\n    test_snakefile: str = \"\"\n    with mock.patch(\n        \"runner.snakemake_runner.snakefile.GetMissingFileDependencies_FromFile\",\n        side_effect=[[\"a.txt\"], [\"b.txt\"], [\"c.txt\"], []],\n    ):\n        depsAll = GetMissingFileDependencies_FromContents(test_snakefile)\n        assert depsAll == [\"a.txt\", \"b.txt\", \"c.txt\"]", "\n\ndef test_GetMissingFileDependencies_TruncatedReturn() -> None:\n    # Returns only dependencies up to target namespace\n    test_snakefile: str = \"\"\n    with mock.patch(\n        \"runner.snakemake_runner.snakefile.GetMissingFileDependencies_FromFile\",\n        side_effect=[[\"a.txt\"], [\"b.txt\"], [\"c.txt\"], []],\n    ):\n        depsTruncated = GetMissingFileDependencies_FromContents(\n            test_snakefile, target_namespaces=[\"a.txt\"]\n        )\n        assert depsTruncated == [\"a.txt\"]", ""]}
{"filename": "runner/runner/snakemake_runner/snakefile.py", "chunked_list": ["import contextlib\nimport io\nimport json\nimport logging\nimport os\nimport platform\nimport shutil\nimport subprocess\nimport tempfile\nfrom contextlib import redirect_stderr", "import tempfile\nfrom contextlib import redirect_stderr\nfrom contextlib import redirect_stdout\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n", "from typing import Union\n\nimport snakemake.remote.HTTP\nimport snakemake.remote.S3\nfrom snakemake import main as snakemake_main\nfrom snakemake.remote import AUTO  # noqa: F401\nfrom snakemake.remote import AutoRemoteProvider\n\nfrom builder.builder import BuildFromJSON\nfrom builder.builder import YAMLToConfig", "from builder.builder import BuildFromJSON\nfrom builder.builder import YAMLToConfig\nfrom runner.TokenizeFile import TokenizeFile\n\n# ##############################################################################\n# Log file\n# ##############################################################################\n\nlogfile = os.path.expanduser(\"~\") + \"/GRAPEVNE.log\"\n", "logfile = os.path.expanduser(\"~\") + \"/GRAPEVNE.log\"\n\n# Set up logging\nlogging.basicConfig(\n    filename=logfile,\n    encoding=\"utf-8\",\n    level=logging.DEBUG,\n)\nlogging.info(\"Working directory: %s\", os.getcwd())\n", "logging.info(\"Working directory: %s\", os.getcwd())\n\n# ##############################################################################\n# Snakemake customizations\n# ##############################################################################\n\n# Override snakemake's 'AUTO' protocol mapper (this replaces the normal dynamic\n# import behaviour which is not supported when building PyInstaller binaries)\n\n", "\n\n@property  # type: ignore\ndef protocol_mapping(self):\n    provider_list = [\n        snakemake.remote.HTTP.RemoteProvider,\n        snakemake.remote.S3.RemoteProvider,\n    ]\n    # assemble scheme mapping\n    protocol_dict = {}\n    for Provider in provider_list:\n        try:\n            for protocol in Provider().available_protocols:\n                protocol_short = protocol[:-3]  # remove \"://\" suffix\n                protocol_dict[protocol_short] = Provider\n        except Exception as e:\n            # If for any reason Provider() fails (e.g. missing python\n            # packages or config env vars), skip this provider.\n            print(f\"Instantiating {Provider.__class__.__name__} failed: {e}\")\n    return protocol_dict", "\n\n# Method replacement in snakemake.remote package\nAutoRemoteProvider.protocol_mapping = protocol_mapping\n\n\n# ##############################################################################\n# Queries\n# ##############################################################################\n", "# ##############################################################################\n\n\ndef Build(data: dict) -> str:\n    \"\"\"Build Snakefile from a dictionary of 'block' elements\"\"\"\n    contents: str = \"\"\n    for block in data[\"block\"]:\n        contents += block[\"content\"] + \"\\n\"\n    return contents\n", "\n\ndef DeleteAllOutput(filename: str) -> dict:\n    \"\"\"Ask snakemake to remove all output files\"\"\"\n    filename, workdir = GetFileAndWorkingDirectory(filename)\n    stdout, stderr = snakemake_launch(\n        filename,\n        \"--delete-all-output\",\n        workdir=workdir,\n    )\n    return {\n        \"status\": \"ok\" if not stderr else \"error\",\n        \"content\": {\"stdout\": stdout, \"stderr\": stderr},\n    }", "\n\ndef Launch_cmd(filename: str, *args, **kwargs) -> Tuple[List[str], str]:\n    \"\"\"Return the snakemake launch command and working directory\"\"\"\n    filename, workdir = GetFileAndWorkingDirectory(filename)\n    return snakemake_cmd(\n        filename,\n        workdir=workdir,\n        *args,\n        **kwargs,\n    )", "\n\ndef Launch(filename: str, *args, **kwargs) -> dict:\n    \"\"\"Launch snakemake workflow given a [locally accessible] location\"\"\"\n    cmd, workdir = Launch_cmd(filename, *args, **kwargs)\n    stdout, stderr = snakemake_run(cmd, workdir)\n    return {\n        \"status\": \"ok\" if not stderr else \"error\",\n        \"content\": {\"stdout\": stdout, \"stderr\": stderr},\n    }", "\n\ndef Lint(snakefile: str) -> dict:\n    \"\"\"Lint a Snakefile using the snakemake library, returns JSON\"\"\"\n    try:\n        stdout, stderr = snakemake_launch(snakefile, \"--lint\", \"json\")\n    except BaseException as e:\n        return {\"error\": str(e)}\n    # strip first and last lines as needed (snakemake returns)\n    sl = stdout.split(\"\\n\")\n    if sl[0] in {\"True\", \"False\"}:\n        sl = sl[1:]\n    if sl[-1] in {\"True\", \"False\"}:\n        sl = sl[:-1]\n    return json.loads(\"\\n\".join(sl))", "\n\ndef LintContents(content: str, tempdir: Optional[str] = None) -> dict:\n    \"\"\"Lint Snakefile contents using the snakemake library, returns JSON\"\"\"\n    with IsolatedTempFile(content) as snakefile:\n        lint_return = Lint(snakefile)\n    return lint_return\n\n\ndef LoadWorkflow(filename: str) -> dict:\n    \"\"\"Load workflow from provided folder\n\n    Valid Snakefile locations:\n        ./Snakefile\n        ./workflow/Snakefile\n    \"\"\"\n    filename, workdir = GetFileAndWorkingDirectory(filename)\n    return SplitByDagFromFile(filename, workdir)", "\ndef LoadWorkflow(filename: str) -> dict:\n    \"\"\"Load workflow from provided folder\n\n    Valid Snakefile locations:\n        ./Snakefile\n        ./workflow/Snakefile\n    \"\"\"\n    filename, workdir = GetFileAndWorkingDirectory(filename)\n    return SplitByDagFromFile(filename, workdir)", "\n\ndef FullTokenizeFromFile(filename: str) -> dict:\n    \"\"\"Copy Snakefile contents to a new temporary file and analyze in isolation\"\"\"\n    with open(filename, \"r\") as infile:\n        with IsolatedTempFile(infile.read()) as tempfile:\n            return SplitByRulesFromFile(tempfile)\n\n\ndef SplitByRulesFromFile(filename: str, workdir: str = \"\") -> dict:\n    \"\"\"\n    Tokenize snakefile, split by 'rule' segments\n\n    This function is conceptually similar to SplitByRulesFile, but takes the\n    full filename of the Snakefile, which requires the file to be accessible by\n    the local filesystem, allowing the runner to interrogate the originating\n    folder for data files. This permits construction of directed acyclic graphs\n    (DAGs)\n    \"\"\"\n    fullfilename = os.path.abspath(filename)\n    return SplitByIndent(fullfilename, workdir, get_dag=True)", "\ndef SplitByRulesFromFile(filename: str, workdir: str = \"\") -> dict:\n    \"\"\"\n    Tokenize snakefile, split by 'rule' segments\n\n    This function is conceptually similar to SplitByRulesFile, but takes the\n    full filename of the Snakefile, which requires the file to be accessible by\n    the local filesystem, allowing the runner to interrogate the originating\n    folder for data files. This permits construction of directed acyclic graphs\n    (DAGs)\n    \"\"\"\n    fullfilename = os.path.abspath(filename)\n    return SplitByIndent(fullfilename, workdir, get_dag=True)", "\n\ndef SplitByDagFromFile(filename: str, workdir: str = \"\") -> dict:\n    \"\"\"Tokenize input using rules derived from dag graph\"\"\"\n    print(\"here\")\n\n    # Query snakemake for DAG of graph (used for connections)\n    dag = DagLocal(os.path.abspath(filename), workdir)\n\n    # Get rules text from origin Snakefile\n    blocks = SplitByIndent(filename, workdir, get_dag=False)\n\n    # Build JSON representation\n    rules: Dict = {\"block\": []}\n    for node in dag[\"nodes\"]:\n        # construct dictionary for block and add to list\n        block = {\n            \"id\": node[\"id\"],\n            \"name\": node[\"value\"][\"rule\"],\n            \"type\": \"rule\",\n            \"content\": \"(module)\",\n        }\n        # cross-reference with block runner\n        for b in blocks[\"block\"]:\n            if b[\"name\"] == block[\"name\"]:\n                block[\"content\"] = b[\"content\"]\n        rules[\"block\"].append(block)\n    # include config nodes\n    for b in blocks[\"block\"]:\n        if b[\"type\"] == \"config\" or b[\"type\"] == \"module\":\n            rules[\"block\"].append(\n                {\n                    \"id\": len(rules[\"block\"]),\n                    \"name\": b[\"name\"],\n                    \"type\": b[\"type\"],\n                    \"content\": b[\"content\"],\n                }\n            )\n\n    # Return links, as determined by snakemake DAG\n    links = []\n    for link in dag[\"links\"]:\n        try:\n            links.append(\n                [\n                    GetRuleFromID(rules[\"block\"], link[\"u\"]),\n                    GetRuleFromID(rules[\"block\"], link[\"v\"]),\n                ]\n            )\n        except KeyError:\n            pass\n\n    rules[\"links\"] = {\n        \"id\": -1,\n        \"name\": \"links\",\n        \"type\": \"links\",\n        \"content\": links,\n    }\n    return rules", "\n\ndef GetRuleFromID(blocks: List[dict], id: int) -> str:\n    \"\"\"Get rule name from block ID\"\"\"\n    for block in blocks:\n        if block[\"id\"] == id:\n            return block[\"name\"]\n    return \"\"\n\n\ndef SplitByRulesFileContent(content: str) -> dict:\n    \"\"\"Tokenize Snakefile, split into 'rules', return as dict list of rules\"\"\"\n    with IsolatedTempFile(content) as snakefile:\n        rules = SplitByRulesFromFile(snakefile)\n    return rules", "\n\ndef SplitByRulesFileContent(content: str) -> dict:\n    \"\"\"Tokenize Snakefile, split into 'rules', return as dict list of rules\"\"\"\n    with IsolatedTempFile(content) as snakefile:\n        rules = SplitByRulesFromFile(snakefile)\n    return rules\n\n\ndef SplitByIndent(filename: str, workdir: str = \"\", get_dag: bool = False):\n    \"\"\"Tokenize input, splitting chunks by indentation level\"\"\"\n\n    # Tokenize into blocks by indentation level\n    with open(filename, \"r\") as file:\n        contents = file.read()\n    tf = TokenizeFile(contents)\n    blockcount = max(tf.rootblock)\n\n    # Query snakemake for DAG of graph (used for connections)\n    if get_dag:\n        dag = DagLocal(os.path.abspath(file.name), workdir)\n    else:\n        dag = {\"nodes\": [], \"links\": []}\n\n    # Build JSON representation\n    rules: Dict = {\"block\": []}\n    for block_index in range(blockcount + 1):\n        content = tf.GetBlockFromIndex(block_index)\n        words = content.split()\n        if not words:\n            continue\n        if words[0] == \"rule\":\n            blocktype = \"rule\"\n            name = words[1].replace(\":\", \"\")\n        elif words[0] == \"module\":\n            blocktype = \"module\"\n            name = words[1].replace(\":\", \"\")\n        else:\n            blocktype = \"config\"\n            name = \"config\"\n        # construct dictionary for block and add to list\n        block = {\n            \"id\": block_index,\n            \"name\": name,\n            \"type\": blocktype,\n            \"content\": content,\n        }\n        rules[\"block\"].append(block)\n\n    # Return links, as determined by snakemake DAG\n    links = []\n    dagnodes = [\n        {\"id\": d[\"value\"][\"jobid\"], \"name\": d[\"value\"][\"rule\"]} for d in dag[\"nodes\"]\n    ]\n    for link in dag[\"links\"]:\n        try:\n            links.append(\n                [GetRuleFromID(dagnodes, link[\"u\"]), GetRuleFromID(dagnodes, link[\"v\"])]\n            )\n        except KeyError:\n            pass\n\n    rules[\"links\"] = {\n        \"id\": -1,\n        \"name\": \"links\",\n        \"type\": \"links\",\n        \"content\": links,\n    }\n\n    return rules", "\ndef SplitByIndent(filename: str, workdir: str = \"\", get_dag: bool = False):\n    \"\"\"Tokenize input, splitting chunks by indentation level\"\"\"\n\n    # Tokenize into blocks by indentation level\n    with open(filename, \"r\") as file:\n        contents = file.read()\n    tf = TokenizeFile(contents)\n    blockcount = max(tf.rootblock)\n\n    # Query snakemake for DAG of graph (used for connections)\n    if get_dag:\n        dag = DagLocal(os.path.abspath(file.name), workdir)\n    else:\n        dag = {\"nodes\": [], \"links\": []}\n\n    # Build JSON representation\n    rules: Dict = {\"block\": []}\n    for block_index in range(blockcount + 1):\n        content = tf.GetBlockFromIndex(block_index)\n        words = content.split()\n        if not words:\n            continue\n        if words[0] == \"rule\":\n            blocktype = \"rule\"\n            name = words[1].replace(\":\", \"\")\n        elif words[0] == \"module\":\n            blocktype = \"module\"\n            name = words[1].replace(\":\", \"\")\n        else:\n            blocktype = \"config\"\n            name = \"config\"\n        # construct dictionary for block and add to list\n        block = {\n            \"id\": block_index,\n            \"name\": name,\n            \"type\": blocktype,\n            \"content\": content,\n        }\n        rules[\"block\"].append(block)\n\n    # Return links, as determined by snakemake DAG\n    links = []\n    dagnodes = [\n        {\"id\": d[\"value\"][\"jobid\"], \"name\": d[\"value\"][\"rule\"]} for d in dag[\"nodes\"]\n    ]\n    for link in dag[\"links\"]:\n        try:\n            links.append(\n                [GetRuleFromID(dagnodes, link[\"u\"]), GetRuleFromID(dagnodes, link[\"v\"])]\n            )\n        except KeyError:\n            pass\n\n    rules[\"links\"] = {\n        \"id\": -1,\n        \"name\": \"links\",\n        \"type\": \"links\",\n        \"content\": links,\n    }\n\n    return rules", "\n\ndef CheckNodeDependencies(jsDeps: dict, snakemake_launcher: str = \"\") -> dict:\n    \"\"\"Check if all dependencies are resolved for a given node\"\"\"\n\n    # Build model from JSON (for dependency analysis)\n    build, model, _ = BuildFromJSON(jsDeps, singlefile=True, partial_build=True)\n\n    # Determine input namespaces for target node\n    input_namespaces = model.ConstructSnakefileConfig()[\n        model.GetNodeByName(model.nodes[0].name).rulename  # first (target) node\n    ][\"config\"].get(\"input_namespace\", {})\n    if isinstance(input_namespaces, str):\n        input_namespaces = {\"In\": input_namespaces}\n    if not input_namespaces:\n        input_namespaces = {\"In\": \"In\"}\n    input_namespaces = set(input_namespaces.values())\n\n    # Determine unresolved dependencies (and their source namespaces)\n    target_namespaces = set([f\"results/{n}\" for n in input_namespaces])\n    missing_deps = set(\n        GetMissingFileDependencies_FromContents(\n            build,\n            list(target_namespaces),\n            snakemake_launcher,\n        )\n    )\n    unresolved_dep_sources = set(\n        s.split(\"/\")[1] for s in missing_deps if s.startswith(\"results/\")\n    )\n\n    # Target dependencies are resolved if there is no overlap between missing\n    # dependencies and the target node's input namespaces\n    unresolved_deps = unresolved_dep_sources.intersection(input_namespaces)\n    if unresolved_deps:\n        return {\n            \"status\": \"missing\",\n            \"unresolved\": list(unresolved_deps),\n        }\n    return {\"status\": \"ok\"}", "\n\n# ##############################################################################\n# Utility functions\n# ##############################################################################\n\n\ndef DagFileContent(content: str) -> dict:\n    \"\"\"Returns DAG as JSON from Snakefile content\"\"\"\n    with IsolatedTempFile(content) as snakefile:\n        dag = DagLocal(snakefile)\n    return dag", "\n\ndef DagLocal(filename: str, workdir: str = \"\") -> dict:\n    \"\"\"Returns DAG as JSON from Snakefile\"\"\"\n    kwargs = {\"workdir\": workdir} if workdir else {}\n    stdout, stderr = snakemake_launch(filename, \"--d3dag\", **kwargs)\n    # strip first and last lines as needed (snakemake returns True/False)\n    sl = stdout.split(\"\\n\")\n    if sl[0] in {\"True\", \"False\"}:\n        sl = sl[1:]\n    if sl[-1] in {\"True\", \"False\"}:\n        sl = sl[:-1]\n    return json.loads(\"\\n\".join(sl))", "\n\ndef GetFileAndWorkingDirectory(filename: str) -> Tuple[str, str]:\n    \"\"\"Get file and working directory from filename\"\"\"\n    if os.path.isdir(filename):\n        workdir = os.path.abspath(filename)\n        filelist = [\n            f\"{workdir}/Snakefile\",\n            f\"{workdir}/workflow/Snakefile\",\n        ]\n        for file in filelist:\n            if os.path.exists(file):\n                filename = file\n                break\n    else:\n        workdir = \"\"\n    return filename, workdir", "\n\ndef GetMissingFileDependencies_FromContents(\n    content: Union[Tuple[dict, str], str],\n    target_namespaces: List[str] = [],\n    snakemake_launcher: str = \"\",\n) -> List[str]:\n    \"\"\"Get missing file dependencies from snakemake\n\n    Recursively find missing dependencies for rulesets. Once missing\n    dependencies are found, touch those file (in an isolated environment) and\n    repeat the process to capture all missing dependencies.\n\n    If target_namespaces is provided, return as soon as any target dependencies\n    are found.\n    \"\"\"\n    if isinstance(content, tuple):\n        # Flattern config and Snakemake files together\n        workflow_lines = content[1].split(\"\\n\")\n        workflow_lines = [\n            line for line in workflow_lines if not line.startswith(\"configfile:\")\n        ]\n        content = (content[0], \"\\n\".join(workflow_lines))\n        content_str: str = YAMLToConfig(content[0]) + \"\\n\" + content[1]\n    else:\n        content_str = content\n    deps = []\n    with IsolatedTempFile(content_str) as snakefile:\n        path = os.path.dirname(os.path.abspath(snakefile))\n        while file_list := GetMissingFileDependencies_FromFile(\n            snakefile, snakemake_launcher\n        ):\n            deps.extend(file_list)\n\n            # Return early if target dependencies are not resolved\n            if set(deps).intersection(target_namespaces):\n                return deps\n\n            # Touch missing files\n            for dep in deps:\n                target = os.path.abspath(f\"{path}/{dep}\")\n                Path(os.path.dirname(target)).mkdir(parents=True, exist_ok=True)\n                Path(target).touch()\n    return deps", "\n\ndef GetMissingFileDependencies_FromFile(\n    filename: str,\n    *args,\n    snakemake_launcher: str = \"\",\n    **kwargs,\n) -> List[str]:\n    \"\"\"Get missing file dependencies from snakemake (single file)\n\n    Find missing dependencies for one run of a ruleset; for recursive /\n    multiple runs use the corresponding _FromContents function.\n    \"\"\"\n    if \"--d3dag\" not in args:\n        args = args + (\"--d3dag\",)\n\n    stdout, stderr = snakemake_launch(\n        filename,\n        *args,\n        snakemake_launcher=snakemake_launcher,\n        **kwargs,\n    )\n    stderr = \"\\n\".join(stderr.split(\"\\n\"))\n    if stdout:\n        # build succeeded\n        return []\n    if not stderr:\n        return []\n    exceptions = set(\n        [\n            ex\n            for ex in [line.split(\" \")[0] for line in stderr.split(\"\\n\")[0:2]]\n            if ex.endswith(\"Exception\")\n        ]\n    )\n    permitted_exceptions = set(\n        [\n            \"MissingInputException\",\n        ]\n    )\n    if len(exceptions - permitted_exceptions) > 0:\n        raise Exception(\n            f\"A non-expected error has been detected: \\nstdout={stdout}\\nstderr={stderr}\"\n        )\n    fileslist = list(filter(None, map(str.strip, stderr.split(\"\\n\"))))\n    try:\n        ix = fileslist.index(\"affected files:\")\n    except ValueError:\n        raise Exception(\"No affected files found: \" + stdout + \"\\n\" + stderr)\n    return fileslist[(ix + 1) :]", "\n\n@contextlib.contextmanager\ndef IsolatedTempFile(content: str, tempdir=None):\n    \"\"\"Create isolated file with passed content placed into a new blank folder\"\"\"\n    snakefile_dir = tempfile.TemporaryDirectory(dir=tempdir)\n    snakefile_file = tempfile.NamedTemporaryFile(\n        dir=snakefile_dir.name, mode=\"w\", encoding=\"utf-8\", delete=False\n    )\n    snakefile: str = snakefile_file.name\n    snakefile_file.write(content)\n    snakefile_file.seek(0)\n    snakefile_file.close\n    # Yield filename as context\n    yield snakefile\n    # Cleanup\n    os.remove(snakefile)\n    shutil.rmtree(snakefile_dir.name)", "\n\ndef WrapCommandForTerminal(cmd: List[str], workdir: str) -> List[str]:\n    \"\"\"Wrap command for terminal execution\n\n    This function takes a command and wraps it in a terminal execution command\n    for the current platform.\n    \"\"\"\n    cmdstr = \" \".join(cmd)\n    if platform.system() == \"Windows\":\n        # Launch terminal process on Windows\n        cmd = [\n            \"cmd.exe\",\n            \"/k\",  # Keep terminal open after execution\n            f'\"cd {workdir}\\n{cmdstr}\"',\n        ]\n    elif platform.system() == \"Darwin\":\n        # Launch terminal process on macOS\n        cmd = [\n            \"osascript\",\n            \"-e\",\n            f'tell app \"Terminal\" to do script \"cd {workdir}\\n{cmdstr}\"',\n        ]\n    elif platform.system() == \"Linux\":\n        # Launch terminal process on Linux\n        cmd = [\"x-terminal-emulator\", \"-e\", f'\"cd {workdir}\\n{cmdstr}\"']\n    else:\n        raise Exception(f\"Unsupported platform: {platform.system()}.\")\n    return cmd", "\n\ndef snakemake_cmd(filename: str, *args, **kwargs) -> Tuple[List[str], str]:\n    \"\"\"Determine snakemake command to launch as subprocess\n\n    This function takes optional arguments that are passed through to the\n    snakemake executable, with the exception of:\n        workdir: sets the working directory for job execution\n        terminal: if True, run snakemake in a terminal window\n    \"\"\"\n    # Get Snakefile path\n    snakefile = os.path.abspath(filename)\n    # Check for work directory, otherwise default to Snakefile directory\n    workdir = kwargs.get(\"workdir\", \"\")\n    if not workdir:\n        workdir = os.path.dirname(snakefile)\n    try:\n        del kwargs[\"workdir\"]\n    except KeyError:\n        pass\n    # Check for terminal flag, then omit from kwargs\n    terminal = kwargs.get(\"terminal\", None)\n    try:\n        del kwargs[\"terminal\"]\n    except KeyError:\n        pass\n    # Collate arguments list\n    arglist = list(args)\n    for k, v in kwargs.items():\n        arglist.extend([k, v])\n    # Launch process and wait for return\n    cmd = [\n        \"snakemake\",\n        \"--snakefile\",\n        snakefile,\n        *arglist,\n    ]\n    if terminal:\n        cmd = WrapCommandForTerminal(cmd, workdir)\n    print(cmd)\n    return cmd, workdir", "\n\ndef snakemake_run(\n    cmd: List[str],\n    workdir: str,\n    capture_output: bool = True,\n    snakemake_launcher: str = \"\",\n) -> Tuple[str, str]:\n    \"\"\"Run the snakemake command by the selected launch method\"\"\"\n    logging.info(\"Launching snakemake [%s]: %s\", snakemake_launcher, \" \".join(cmd))\n    snakemake_launcher = \"builtin\" if not snakemake_launcher else snakemake_launcher\n    if snakemake_launcher == \"system\":\n        p = subprocess.run(\n            cmd,\n            cwd=workdir,\n            capture_output=capture_output,\n        )\n        return p.stdout.decode(\"utf-8\"), p.stderr.decode(\"utf-8\")\n    elif snakemake_launcher == \"builtin\":\n        cmd_str = \" \".join(cmd[1:])  # strip snakemake executable\n        if capture_output:\n            with redirect_stdout(io.StringIO()) as f_stdout:\n                with redirect_stderr(io.StringIO()) as f_stderr:\n                    try:\n                        snakemake_main(\n                            cmd_str + \" -d \" + workdir,\n                        )\n                    except SystemExit:\n                        # SystemExit is raised by snakemake upon exit\n                        pass\n            return f_stdout.getvalue(), f_stderr.getvalue()\n        else:\n            try:\n                snakemake_main(\n                    cmd_str + \" -d \" + workdir,\n                )\n            except SystemExit:\n                # SystemExit is raised by snakemake upon exit\n                pass\n            return \"\", \"\"\n    else:\n        raise Exception(f\"Unsupported launcher: {snakemake_launcher}.\")", "\n\ndef snakemake_launch(\n    filename: str,\n    *args,\n    snakemake_launcher: str = \"\",\n    **kwargs,\n) -> Tuple[str, str]:\n    \"\"\"Construct the snakemake command and then launch\n\n    See snakemake_cmd for details on arguments.\n    \"\"\"\n    cmd, workdir = snakemake_cmd(filename, *args, **kwargs)\n    return snakemake_run(cmd, workdir, snakemake_launcher=snakemake_launcher)", ""]}
{"filename": "electron-app/src/python/backend.py", "chunked_list": ["import contextlib\nimport json\nimport logging\nimport os\nimport sys\nimport tempfile\n\nimport filesystem\n\nimport builder", "\nimport builder\nimport runner\n\n\ndefault_build_path = tempfile.gettempdir() + \"/workflows/build\"\ndefault_testbuild_path = tempfile.gettempdir() + \"/workflows/testbuild\"\nlogfile = os.path.expanduser(\"~\") + \"/GRAPEVNE.log\"\n\n# Set up logging", "\n# Set up logging\nlogging.basicConfig(\n    filename=logfile,\n    encoding=\"utf-8\",\n    level=logging.DEBUG,\n)\nlogging.info(\"Working directory: %s\", os.getcwd())\n\n\ndef post(request):\n    \"\"\"Handles POST requests from the frontend.\"\"\"\n    request_js = json.loads(request)\n    query = request_js[\"query\"]\n    data = request_js[\"data\"]\n\n    logging.info(\"Received query: %s\", query)\n    logging.info(\"Received data: %s\", data)\n\n    if query == \"runner/snakemake-run\":\n        # Special query - does not capture stdout, stderr\n        runner.SnakemakeRun(data)\n        return None\n\n    # Suppress stdout as we only want to control the return data from this\n    # PyInstaller executable which is called from electron.\n    with contextlib.redirect_stdout(None):\n        # File system queries\n        if query == \"display/folderinfo\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(filesystem.GetFolderItems(data)),\n            }\n\n        # Builder queries\n        elif query == \"builder/compile-to-json\":\n            js = data[\"content\"]\n            # with open(default_build_path + \"/workflow.json\", \"w\") as f:  # dump config file to disk for debug\n            #     json.dump(js, f, indent=4)\n            memory_zip, _, zipfilename = builder.BuildFromJSON(\n                js,\n                build_path=default_build_path,\n            )\n            # Binary return is not used when passing the information over stdout.\n            # Instead, the zip file is read back off the disk and forwarded by\n            # electron / nodejs.\n            data = {\n                \"query\": query,\n                \"body\": {\n                    \"zipfile\": zipfilename,\n                },\n            }\n        elif query == \"builder/build-and-run\":\n            # First, build the workflow\n            logging.info(\"Building workflow\")\n            js = data[\"content\"]\n            # with open(\"workflow.json\", \"w\") as f:  # dump config file to disk for debug\n            #     json.dump(js, f, indent=4)\n            build_path = default_testbuild_path\n            logging.info(\"BuildFromJSON\")\n            memory_zip, m, _ = builder.BuildFromJSON(\n                js,\n                build_path=build_path,\n                clean_build=False,  # Do not overwrite existing build\n            )\n            targets = data.get(\"targets\", [])\n            target_modules = m.LookupRuleNames(targets)\n\n            # Get list of snakemake rules, cross-reference with target_modules\n            # and select 'target' or all rules, then pass on to command\n            data_list = runner.Launch_cmd(\n                {\n                    \"format\": data[\"format\"],\n                    \"content\": build_path,\n                    \"args\": \"--list\",\n                },\n                terminal=False,\n            )\n            # Stringify command\n            data_list[\"command\"] = \" \".join(data_list[\"command\"])\n            logging.info(\"List command: %s\", data_list[\"command\"])\n            response = runner.SnakemakeRun(\n                {\n                    \"format\": data[\"format\"],\n                    \"content\": {\n                        \"command\": data_list[\"command\"],\n                        \"workdir\": data_list[\"workdir\"],\n                        \"capture_output\": True,\n                        \"backend\": data.get(\"backend\", \"\"),\n                    },\n                }\n            )\n            snakemake_list = response[\"stdout\"].split(\"\\n\")\n            logging.debug(\"snakemake --list output: %s\", snakemake_list)\n            target_rules = []\n            for target in target_modules:\n                target_rule = f\"{target}_target\"\n                if target_rule in snakemake_list:\n                    target_rules.append(target_rule)\n                else:\n                    target_rules.extend(\n                        [\n                            rulename\n                            for rulename in snakemake_list\n                            if rulename.startswith(target)\n                        ]\n                    )\n\n            # Second, return the launch command\n            logging.info(\"Generating launch command\")\n            data = runner.Launch_cmd(\n                {\n                    \"format\": data[\"format\"],\n                    \"content\": build_path,\n                    \"targets\": target_rules,\n                    \"args\": data.get(\"args\", \"\"),\n                },\n                terminal=False,\n            )\n            # Stringify command\n            data[\"command\"] = \" \".join(data[\"command\"])\n            logging.info(\"Launch command: %s\", data[\"command\"])\n            # Return the launch command\n            data = {\n                \"query\": query,\n                \"body\": data,\n            }\n        elif query == \"builder/clean-build-folder\":\n            data = {\n                \"query\": query,\n                \"body\": builder.CleanBuildFolder(default_testbuild_path),\n            }\n        elif query == \"builder/get-remote-modules\":\n            js = data[\"content\"]\n            data = {\n                \"query\": query,\n                \"body\": builder.GetModulesList(js[\"url\"]),\n            }\n\n        # Runner queries\n        elif query == \"runner/build\":\n            data = {\n                \"query\": query,\n                \"body\": runner.Build(data),\n            }\n        elif query == \"runner/deleteresults\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.DeleteAllOutput(data)),\n            }\n        elif query == \"runner/lint\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.LintContents(data)),\n            }\n        elif query == \"runner/loadworkflow\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.LoadWorkflow(data)),\n            }\n        elif query == \"runner/tokenize\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.Tokenize(data)),\n            }\n        elif query == \"runner/tokenize_load\":\n            try:\n                # Try full-tokenize (may fail if dependencies not present)\n                body = runner.FullTokenizeFromFile(data)\n            except BaseException:\n                # ...then try in-situ tokenization\n                body = runner.TokenizeFromFile(data)\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(body),\n            }\n        elif query == \"runner/jobstatus\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.TokenizeFromFile(data)),\n            }\n        elif query == \"runner/launch\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.Launch(data)),\n            }\n        elif query == \"runner/check-node-dependencies\":\n            data = {\n                \"query\": query,\n                \"body\": runner.CheckNodeDependencies(data),\n            }\n\n        else:\n            raise NotImplementedError(f\"Unknown query: {query}\")\n\n    # Return via stdout\n    rtn = json.dumps(data)\n    print(rtn)\n    logging.info(\"Query response: %s\", rtn)", "\n\ndef post(request):\n    \"\"\"Handles POST requests from the frontend.\"\"\"\n    request_js = json.loads(request)\n    query = request_js[\"query\"]\n    data = request_js[\"data\"]\n\n    logging.info(\"Received query: %s\", query)\n    logging.info(\"Received data: %s\", data)\n\n    if query == \"runner/snakemake-run\":\n        # Special query - does not capture stdout, stderr\n        runner.SnakemakeRun(data)\n        return None\n\n    # Suppress stdout as we only want to control the return data from this\n    # PyInstaller executable which is called from electron.\n    with contextlib.redirect_stdout(None):\n        # File system queries\n        if query == \"display/folderinfo\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(filesystem.GetFolderItems(data)),\n            }\n\n        # Builder queries\n        elif query == \"builder/compile-to-json\":\n            js = data[\"content\"]\n            # with open(default_build_path + \"/workflow.json\", \"w\") as f:  # dump config file to disk for debug\n            #     json.dump(js, f, indent=4)\n            memory_zip, _, zipfilename = builder.BuildFromJSON(\n                js,\n                build_path=default_build_path,\n            )\n            # Binary return is not used when passing the information over stdout.\n            # Instead, the zip file is read back off the disk and forwarded by\n            # electron / nodejs.\n            data = {\n                \"query\": query,\n                \"body\": {\n                    \"zipfile\": zipfilename,\n                },\n            }\n        elif query == \"builder/build-and-run\":\n            # First, build the workflow\n            logging.info(\"Building workflow\")\n            js = data[\"content\"]\n            # with open(\"workflow.json\", \"w\") as f:  # dump config file to disk for debug\n            #     json.dump(js, f, indent=4)\n            build_path = default_testbuild_path\n            logging.info(\"BuildFromJSON\")\n            memory_zip, m, _ = builder.BuildFromJSON(\n                js,\n                build_path=build_path,\n                clean_build=False,  # Do not overwrite existing build\n            )\n            targets = data.get(\"targets\", [])\n            target_modules = m.LookupRuleNames(targets)\n\n            # Get list of snakemake rules, cross-reference with target_modules\n            # and select 'target' or all rules, then pass on to command\n            data_list = runner.Launch_cmd(\n                {\n                    \"format\": data[\"format\"],\n                    \"content\": build_path,\n                    \"args\": \"--list\",\n                },\n                terminal=False,\n            )\n            # Stringify command\n            data_list[\"command\"] = \" \".join(data_list[\"command\"])\n            logging.info(\"List command: %s\", data_list[\"command\"])\n            response = runner.SnakemakeRun(\n                {\n                    \"format\": data[\"format\"],\n                    \"content\": {\n                        \"command\": data_list[\"command\"],\n                        \"workdir\": data_list[\"workdir\"],\n                        \"capture_output\": True,\n                        \"backend\": data.get(\"backend\", \"\"),\n                    },\n                }\n            )\n            snakemake_list = response[\"stdout\"].split(\"\\n\")\n            logging.debug(\"snakemake --list output: %s\", snakemake_list)\n            target_rules = []\n            for target in target_modules:\n                target_rule = f\"{target}_target\"\n                if target_rule in snakemake_list:\n                    target_rules.append(target_rule)\n                else:\n                    target_rules.extend(\n                        [\n                            rulename\n                            for rulename in snakemake_list\n                            if rulename.startswith(target)\n                        ]\n                    )\n\n            # Second, return the launch command\n            logging.info(\"Generating launch command\")\n            data = runner.Launch_cmd(\n                {\n                    \"format\": data[\"format\"],\n                    \"content\": build_path,\n                    \"targets\": target_rules,\n                    \"args\": data.get(\"args\", \"\"),\n                },\n                terminal=False,\n            )\n            # Stringify command\n            data[\"command\"] = \" \".join(data[\"command\"])\n            logging.info(\"Launch command: %s\", data[\"command\"])\n            # Return the launch command\n            data = {\n                \"query\": query,\n                \"body\": data,\n            }\n        elif query == \"builder/clean-build-folder\":\n            data = {\n                \"query\": query,\n                \"body\": builder.CleanBuildFolder(default_testbuild_path),\n            }\n        elif query == \"builder/get-remote-modules\":\n            js = data[\"content\"]\n            data = {\n                \"query\": query,\n                \"body\": builder.GetModulesList(js[\"url\"]),\n            }\n\n        # Runner queries\n        elif query == \"runner/build\":\n            data = {\n                \"query\": query,\n                \"body\": runner.Build(data),\n            }\n        elif query == \"runner/deleteresults\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.DeleteAllOutput(data)),\n            }\n        elif query == \"runner/lint\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.LintContents(data)),\n            }\n        elif query == \"runner/loadworkflow\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.LoadWorkflow(data)),\n            }\n        elif query == \"runner/tokenize\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.Tokenize(data)),\n            }\n        elif query == \"runner/tokenize_load\":\n            try:\n                # Try full-tokenize (may fail if dependencies not present)\n                body = runner.FullTokenizeFromFile(data)\n            except BaseException:\n                # ...then try in-situ tokenization\n                body = runner.TokenizeFromFile(data)\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(body),\n            }\n        elif query == \"runner/jobstatus\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.TokenizeFromFile(data)),\n            }\n        elif query == \"runner/launch\":\n            data = {\n                \"query\": query,\n                \"body\": json.dumps(runner.Launch(data)),\n            }\n        elif query == \"runner/check-node-dependencies\":\n            data = {\n                \"query\": query,\n                \"body\": runner.CheckNodeDependencies(data),\n            }\n\n        else:\n            raise NotImplementedError(f\"Unknown query: {query}\")\n\n    # Return via stdout\n    rtn = json.dumps(data)\n    print(rtn)\n    logging.info(\"Query response: %s\", rtn)", "\n\npost(sys.argv[1])\n"]}
{"filename": "electron-app/src/python/filesystem.py", "chunked_list": ["import os\nimport shutil\n\n\ndef DeleteResults(data) -> dict:\n    \"\"\"Remove the local 'results' folder\"\"\"\n    dirname = data[\"content\"]\n    dirname = f\"{dirname}/results\"\n    shutil.rmtree(dirname)\n    return {}", "\n\ndef GetFolderItems(data) -> dict:\n    \"\"\"Get the contents of a folder\"\"\"\n    dirname = data[\"content\"]\n\n    dirlist = [\n        filename\n        for filename in os.listdir(dirname)\n        if os.path.isdir(os.path.join(dirname, filename))\n    ]\n    dirlist = [name for name in dirlist if name[0] != \".\"]\n    dirlist.sort()\n    isdir = len(dirlist) * [True]\n\n    filelist = [\n        filename\n        for filename in os.listdir(dirname)\n        if not os.path.isdir(os.path.join(dirname, filename))\n    ]\n    filelist = [name for name in filelist if name[0] != \".\"]\n    filelist.sort()\n    isdir.extend(len(filelist) * [False])\n\n    contents = [*dirlist, *filelist]\n\n    contents = [{\"name\": name, \"isdir\": isdir} for name, isdir in zip(contents, isdir)]\n\n    js = {\"foldername\": dirname, \"contents\": contents}\n    return js", ""]}
{"filename": "docs/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = \"GRAPEVNE\"\ncopyright = \"2023, kraemer-lab\"\nauthor = \"kraemer-lab\"", "copyright = \"2023, kraemer-lab\"\nauthor = \"kraemer-lab\"\n\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.coverage\",", "    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.coverage\",\n    \"sphinx.ext.graphviz\",\n    \"myst_parser\",\n    \"sphinx_rtd_theme\",\n]\n\ntemplates_path = [\n    \"_templates\",\n]", "    \"_templates\",\n]\nexclude_patterns = [\n    \"_build\",\n    \"Thumbs.db\",\n    \".DS_Store\",\n    \"venv\",\n    \"README.md\",\n]\n", "]\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = \"sphinx_rtd_theme\"\nhtml_static_path = [\n    \"_static\",\n]", "    \"_static\",\n]\n\ngraphviz_output_format = \"svg\"\n"]}
