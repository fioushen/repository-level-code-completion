{"filename": "tests/test_wikidata.py", "chunked_list": ["import pytest\n\nfrom srtk.knowledge_graph import Wikidata\n\nWIKIDATA_ENDPOINT = \"https://query.wikidata.org/sparql\"\n\n@pytest.fixture\ndef wikidata():\n    return Wikidata(WIKIDATA_ENDPOINT, prepend_prefixes=False, exclude_qualifiers=True)\n\ndef test_get_label(wikidata: Wikidata):\n    label = wikidata.get_label(\"Q157808\")\n    assert label == \"Technical University of Munich\"", "\ndef test_get_label(wikidata: Wikidata):\n    label = wikidata.get_label(\"Q157808\")\n    assert label == \"Technical University of Munich\"\n\ndef test_search_one_hop_relations(wikidata: Wikidata):\n    src = \"Q157808\"\n    dst = \"Q183\"\n    relations = wikidata.search_one_hop_relations(src, dst)\n    assert [\"P17\"] in relations, \"(TUM --country--> Germany)\"", "\ndef test_search_two_hop_relations(wikidata: Wikidata):\n    src = \"Q157808\"\n    dst = \"Q458\"\n    relations = wikidata.search_two_hop_relations(src, dst)\n    assert [\"P17\", \"P463\"] in relations, \"(TUM --country--> Germany --member of--> European Union)\"\n\ndef test_deduce_leaves(wikidata: Wikidata):\n    src = \"Q157808\"\n\n    # Test one-hop leaves\n    path = (\"P17\",)\n    leaves = wikidata.deduce_leaves(src, path, limit=10)\n    assert \"Q183\"  in leaves, \"(TUM --country--> Germany )\"\n\n    # Test two-hop leaves\n    path = (\"P17\", \"P463\")\n    leaves = wikidata.deduce_leaves(src, path, limit=100)\n    assert \"Q458\" in leaves, \"(TUM --country--> Germany --member of--> European Union)\"", "\ndef test_get_neighbor_relations():\n    src = \"Q157808\"\n    wikidata = Wikidata(WIKIDATA_ENDPOINT, exclude_qualifiers=True)\n\n    # Test 1-hop relations AND exclude_qualifiers=True\n    relations = wikidata.get_neighbor_relations(src, hop=1, limit=200)\n    assert \"P17\" in relations and \"P112\" in relations,  \"TUM has 1-hop relations country and founded by\"\n    assert \"P571\" not in relations, \"inception should be filtered out as it is a quantifier\"\n\n    # Test 2-hop relations\n    relations = wikidata.get_neighbor_relations(src, hop=2)\n    assert (\"P17\", \"P361\") in relations, \"TUM --country--> Germany --part of--> European Union\"\n\n    # The wikidata endpoint is re-initialized to avoid caching\n    wikidata = Wikidata(WIKIDATA_ENDPOINT, prepend_prefixes=False, exclude_qualifiers=False)\n    # Test exclude_qualifiers=False\n    relations = wikidata.get_neighbor_relations(src, hop=1, limit=200)\n    assert \"P571\" in relations, \"inception should be present when qualifiers are not excluded\"", "\ndef test_deduce_leaves_from_multiple_srcs(wikidata: Wikidata):\n    srcs = [\"Q157808\", \"Q55044\"]  # TUM and LMU\n\n    # Test searching one-hop common leaves\n    path = (\"P17\",)\n    leaves = wikidata.deduce_leaves_from_multiple_srcs(srcs, path, limit=10)\n    assert \"Q183\" in leaves, \"(TUM | LMU --country--> Germany )\"\n", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_encoder.py", "chunked_list": ["import pytest\nimport torch\n\nfrom srtk.scorer import LitSentenceEncoder\n\n\n@pytest.fixture\ndef model():\n    return LitSentenceEncoder('smallbenchnlp/roberta-small')\n", "\n\ndef examine_single_pooler(pooler):\n    # Test pooler without batch size\n    hidden_states = torch.randn(10, 256)\n    attention_mask = torch.ones(10)\n    pooled = pooler(hidden_states, attention_mask)\n    assert pooled.shape == (256,)\n    # Test pooler with batch size\n    hidden_states = torch.randn(2, 10, 256)\n    attention_mask = torch.ones(2, 10)\n    pooled = pooler(hidden_states, attention_mask)\n    assert pooled.shape == (2, 256)", "\ndef test_avg_pooler(model):\n    if hasattr(model, 'avg_pool'):\n        examine_single_pooler(model.avg_pool)\n    else:\n        pytest.skip('No avg pooler found')\n\ndef test_cls_pooler(model):\n    if hasattr(model, 'cls_pool'):\n        examine_single_pooler(model.cls_pool)\n    else:\n        pytest.skip('No cls pooler found')", "\ndef test_compute_sentence_similarity(model):\n    # Test batched similarity\n    query = torch.randn(2, 1, 10, 256)\n    samples = torch.randn(2, 3, 10, 256)\n    similarity = model.compute_sentence_similarity(query, samples)\n    assert similarity.shape == (2, 3)\n    # Test single pair similarity\n    query = torch.randn(1, 10, 256)\n    sample = torch.randn(1, 10, 256)\n    similarity = model.compute_sentence_similarity(query, sample)\n    assert similarity.shape == (1,)", "\ndef test_training_step(model):\n    # Test training step\n    batch = {\n        'input_ids': torch.randint(0, 100, (2, 4, 10)),\n        'attention_mask': torch.ones(2, 4, 10),\n    }\n    loss = model.training_step(batch, 0)\n    assert loss.shape == ()\n", ""]}
{"filename": "tests/test_scorer.py", "chunked_list": ["import pytest\nfrom srtk.scorer import Scorer\n\n\n@pytest.fixture\ndef scorer():\n    \"\"\"Whether the scorer can be initialized\"\"\"\n    return Scorer('smallbenchnlp/roberta-small')\n\ndef test_score(scorer):\n    \"\"\"Whether the scorer can run and return a digit, without crashing.\"\"\"\n    query = 'query: how old is the natural satellite of the earch?'\n    prev_relations = ('satellite of',)\n    next_relation = 'relation: age'\n    scorer.score(query, prev_relations, next_relation)", "\ndef test_score(scorer):\n    \"\"\"Whether the scorer can run and return a digit, without crashing.\"\"\"\n    query = 'query: how old is the natural satellite of the earch?'\n    prev_relations = ('satellite of',)\n    next_relation = 'relation: age'\n    scorer.score(query, prev_relations, next_relation)\n\ndef test_batch_score(scorer):\n    \"\"\"Whether the batch scorer is capable of batch scoring.\"\"\"\n    query = 'query: how old is the natural satellite of the earch?'\n    prev_relations = ('satellite of',)\n    next_relations = ('relation: age', 'relation: age')\n    scores = scorer.batch_score(query, prev_relations, next_relations)\n    assert isinstance(scores, list)\n    assert len(scores) == len(next_relations)\n    assert scores[0] == scores[1], 'The scores should be the same for the same samples.'", "def test_batch_score(scorer):\n    \"\"\"Whether the batch scorer is capable of batch scoring.\"\"\"\n    query = 'query: how old is the natural satellite of the earch?'\n    prev_relations = ('satellite of',)\n    next_relations = ('relation: age', 'relation: age')\n    scores = scorer.batch_score(query, prev_relations, next_relations)\n    assert isinstance(scores, list)\n    assert len(scores) == len(next_relations)\n    assert scores[0] == scores[1], 'The scores should be the same for the same samples.'\n", ""]}
{"filename": "tests/test_link.py", "chunked_list": ["import json\nimport requests\nfrom argparse import Namespace\n\nimport pytest\nfrom srtk.link import link\n\ndef check_url_availability(url):\n    try:\n        response = requests.head(url)\n        return response.status_code == 200\n    except requests.ConnectionError:\n        return False", "\ndef test_dbpedia_linker():\n    question = {\"id\": \"berlin\", \"question\": \"Berlin is the capital of Germany.\"}\n    with open(\"question.jsonl\", \"w\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(question) + \"\\n\")\n    dbpedia_endpoint = \"https://api.dbpedia-spotlight.org/en/annotate\"\n    if not check_url_availability(dbpedia_endpoint):\n        pytest.skip(\"DBpedia endpoint is not available.\")\n    args = Namespace(\n        input=\"question.jsonl\",\n        output=\"linked.jsonl\",\n        knowledge_graph=\"dbpedia\",\n        ground_on=\"question\",\n        el_endpoint=dbpedia_endpoint,\n        service=\"spotlight\",\n        token=None,\n    )\n    link(args)\n    with open(\"linked.jsonl\", \"r\", encoding=\"utf-8\") as f:\n        linked = json.loads(f.readline())\n    assert \"Berlin\" in linked[\"question_entities\"]\n    assert \"Germany\" in linked[\"question_entities\"]", ""]}
{"filename": "tests/test_freebase.py", "chunked_list": ["import requests\n\nimport pytest\n\nfrom srtk.knowledge_graph import Freebase\n\nFREEBASE_ENDPOINT = \"http://localhost:3001/sparql\"\n\n\ndef endpoint_reachable():\n    try:\n        response = requests.get(FREEBASE_ENDPOINT)\n        if response.status_code == 200:\n            return True\n        else:\n            return False\n    except requests.exceptions.RequestException:\n        return False", "\ndef endpoint_reachable():\n    try:\n        response = requests.get(FREEBASE_ENDPOINT)\n        if response.status_code == 200:\n            return True\n        else:\n            return False\n    except requests.exceptions.RequestException:\n        return False", "\nskip_if_unreachable = pytest.mark.skipif(\n    not endpoint_reachable(), reason=\"Freebase endpoint not reachable\"\n)\n\n\n@pytest.fixture\ndef freebase():\n    return Freebase(FREEBASE_ENDPOINT, prepend_prefixes=True)\n", "\n@skip_if_unreachable\ndef test_get_label(freebase: Freebase):\n    # Skip the test if the fixture is skipped\n    label = freebase.get_label(\"m.03_r3\")\n    assert label == \"Jamaica\"\n\n@skip_if_unreachable\ndef test_search_one_hop_relations(freebase: Freebase):\n    src = \"m.03_r3\"\n    dst = \"m.01428y\"\n    relations = freebase.search_one_hop_relations(src, dst)\n    assert [\"location.country.languages_spoken\"] in relations, \"(Jamaica --speak--> Jamaican English)\"", "def test_search_one_hop_relations(freebase: Freebase):\n    src = \"m.03_r3\"\n    dst = \"m.01428y\"\n    relations = freebase.search_one_hop_relations(src, dst)\n    assert [\"location.country.languages_spoken\"] in relations, \"(Jamaica --speak--> Jamaican English)\"\n\n@skip_if_unreachable\ndef test_search_two_hop_relations(freebase: Freebase):\n    src = \"m.0157m\"\n    dst = \"m.0crpbj\"\n    relations = freebase.search_two_hop_relations(src, dst)\n    assert [\"people.person.education\", \"education.education.institution\"] in relations, \\\n        \"Bill Clinton --education-->  ?? --institution--> Hot Springs High School\"", "\n@skip_if_unreachable\ndef test_deduce_leaves(freebase: Freebase):\n    src = \"m.0157m\"\n\n    # Test one-hop leaves\n    path = (\"people.person.education\",)\n    leaves = freebase.deduce_leaves(src, path, limit=10)\n    assert \"m.0125cddf\"  in leaves, \"(Bill Clinton --education-->  )\"\n    \n    # Test two-hop leaves\n    path = (\"people.person.education\", \"education.education.institution\")\n    leaves = freebase.deduce_leaves(src, path, limit=10)\n    assert \"m.0crpbj\" in leaves, \"(Bill Clinton --education-->  --insittution--> Hot Springs High School\"", "\n@skip_if_unreachable\ndef test_get_neighbor_relations(freebase):\n    src = \"m.0157m\"\n    # Test 1-hop relations\n    relations = freebase.get_neighbor_relations(src, hop=1, limit=200)\n    assert \"government.us_president.vice_president\" in relations and \"government.politician.party\" in relations,\\\n        \"Billclinton has 1-hop relations vice_president and party\"\n\n    # Test 2-hop relations\n    src = \"m.0157m\"\n    relations = freebase.get_neighbor_relations(src, hop=2, limit=200)\n    assert ('government.us_president.vice_president', 'people.person.gender') in relations,\\\n        \"Bill Clinton --vice president--> --gender--> ...\"", "\n@skip_if_unreachable\ndef test_deduce_leaves_from_multiple_srcs(freebase: Freebase):\n    srcs = [\"m.0157m\", \"m.02mjmr\"]  # Bill and Barack\n\n    # Test searching one-hop common leaves\n    path = (\"government.us_president.vice_president\",)\n    leaves = freebase.deduce_leaves_from_multiple_srcs(srcs, path, limit=10)\n    assert \"m.0d05fv\" in leaves, \"(Bill | Barack --vice president--> Joe Biden )\"\n", ""]}
{"filename": "tests/test_dbpedia.py", "chunked_list": ["import pytest\n\nfrom srtk.knowledge_graph.dbpedia import DBpedia\n\nDBPEDIA_ENDPOINT = \"https://dbpedia.org/sparql\"\n\n@pytest.fixture\ndef dbpedia():\n    return DBpedia(DBPEDIA_ENDPOINT, prepend_prefixes=False)\n\ndef test_get_label(dbpedia: DBpedia):\n    label = dbpedia.get_label(\"Elizabeth_II\")\n    assert label == \"Elizabeth II\"", "\ndef test_get_label(dbpedia: DBpedia):\n    label = dbpedia.get_label(\"Elizabeth_II\")\n    assert label == \"Elizabeth II\"\n\ndef test_search_one_hop_relations(dbpedia: DBpedia):\n    src = \"Elizabeth_II\"\n    dst = \"Charles_III\"\n    relations = dbpedia.search_one_hop_relations(src, dst)\n    assert [\"child\"] in relations and [\"successor\"] in relations", "\ndef test_search_two_hop_relations(dbpedia: DBpedia):\n    src = \"Technical_University_of_Munich\"\n    dst = \"Bavaria\"\n    relations = dbpedia.search_two_hop_relations(src, dst)\n    assert [\"city\", \"federalState\"] in relations\n\ndef test_deduce_leaves(dbpedia: DBpedia):\n    src = \"Technical_University_of_Munich\"\n\n    # Test one-hop leaves\n    path = (\"city\",)\n    leaves = dbpedia.deduce_leaves(src, path, limit=10)\n    assert \"Munich\"  in leaves, \"(TUM --city--> Munich )\"\n    \n    # Test two-hop leaves\n    path = (\"city\", \"federalState\")\n    leaves = dbpedia.deduce_leaves(src, path, limit=10)\n    assert \"Bavaria\" in leaves", "\ndef test_get_neighbor_relations(dbpedia):\n    src = \"Charles_III\"\n\n    # Test 1-hop relations\n    relations = dbpedia.get_neighbor_relations(src, hop=1, limit=200)\n    assert  all(r in relations for r in [\"predecessor\", \"parent\", \"successor\"]),\\\n        \"'predecessor', 'parent', 'successor' should present in the relations\"\n\n    # Test 2-hop relations\n    relations = dbpedia.get_neighbor_relations(src, hop=2)\n    assert (\"parent\", \"title\") in relations, \"Charles III --parent--> Elizabeth II --title--> Queen\"", "\ndef test_deduce_leaves_from_multiple_srcs(dbpedia):\n    srcs = [\"Charles_III\", \"Elizabeth_II\"]\n    path = (\"successor\",)\n    leaves = dbpedia.deduce_leaves_from_multiple_srcs(srcs, path, limit=10)\n    assert \"Charles_III\" in leaves and \"William,_Prince_of_Wales\" in leaves\n"]}
{"filename": "docs/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../src'))\n\n# -- Project information -----------------------------------------------------", "\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = 'srtk'\ncopyright = '2023, Yuanchun Shen'\nauthor = 'Yuanchun Shen'\nrelease = '0.0.5'\n\n# -- General configuration ---------------------------------------------------", "\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nautodoc_mock_imports = ['beautifulsoup4', 'lightning', 'pyvis', 'SPARQLWrapper',\n                        'srsly', 'transformers', 'tqdm', 'torch', 'datasets',\n                        'bs4', 'wikimapper']\n\nextensions = ['sphinx.ext.autodoc', 'myst_parser', 'sphinx.ext.napoleon']\n", "extensions = ['sphinx.ext.autodoc', 'myst_parser', 'sphinx.ext.napoleon']\n\ntemplates_path = ['_templates']\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = 'sphinx_rtd_theme'\nhtml_static_path = ['_static']\n"]}
{"filename": "src/srtk/train.py", "chunked_list": ["\"\"\"The script to train the scorer model.\n\ne.g.\npython train.py --data-file data/train.jsonl --model-name-or-path intfloat/e5-small --save-model-path artifacts/scorer\n\"\"\"\nimport argparse\nimport datetime\nfrom collections import defaultdict\nfrom dataclasses import dataclass\n", "from dataclasses import dataclass\n\nimport lightning.pytorch as pl\nimport torch\nfrom datasets import load_dataset\nfrom lightning.pytorch.loggers import WandbLogger\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, PreTrainedTokenizerBase\n\nfrom .scorer import LitSentenceEncoder", "\nfrom .scorer import LitSentenceEncoder\n\n\ndef concate_all(example):\n    \"\"\"Concatenate all columns into one column for input.\n    The resulted 'input_text' column is a list of strings.\n    \"\"\"\n    query = 'query: ' + example['query']\n    rels = [example['positive']] + example['negatives']\n    rels = ['relation: ' + rel for rel in rels]\n    example['input_text'] = [query] + rels\n    return example", "\n\n@dataclass\nclass Collator:\n    \"\"\"Collate a list of examples into a batch.\"\"\"\n    tokenizer: PreTrainedTokenizerBase\n\n    def __call__(self, features):\n        batched = defaultdict(list)\n        for item in features:\n            for key, value in item.items():\n                value = torch.tensor(value)\n                if key == 'attention_mask':\n                    value = value.bool()\n                batched[key].append(value)\n        for key, value in batched.items():\n            batched[key] = torch.stack(value, dim=0)\n        return batched", "\n\ndef prepare_dataloaders(train_data, validation_data, tokenizer, batch_size):\n    \"\"\"Prepare dataloaders for training and validation.\n\n    If validation dataset is not provided, 5 percent of the training data will be used as validation data.\n    \"\"\"\n    def tokenize(example):\n        tokenized = tokenizer(example['input_text'], padding='max_length', truncation=True, return_tensors='pt', max_length=32)\n        return tokenized\n\n    train_split = 'train[:95%]' if validation_data is None else 'train'\n    validation_split = 'train[95%:]' if validation_data is None else 'train'\n    if validation_data is None:\n        validation_data = train_data\n\n    train_dataset = load_dataset('json', data_files=train_data, split=train_split)\n    train_dataset = train_dataset.map(concate_all, remove_columns=train_dataset.column_names)\n    train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n    validation_dataset = load_dataset('json', data_files=validation_data, split=validation_split)\n    validation_dataset = validation_dataset.map(concate_all, remove_columns=validation_dataset.column_names)\n    validation_dataset = validation_dataset.map(tokenize, remove_columns=validation_dataset.column_names)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=Collator(tokenizer), num_workers=8)\n    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=Collator(tokenizer), num_workers=8)\n    return train_loader, validation_loader", "\n\ndef train(args):\n    \"\"\"Train the scorer model.\n\n    The model compares the similarity between [question; previous relation] and the next relation.\n    \"\"\"\n    torch.set_float32_matmul_precision('medium')\n    model = LitSentenceEncoder(args.model_name_or_path, lr=args.learning_rate, loss=args.loss)\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n    train_loader, validation_loader = prepare_dataloaders(args.train_dataset, args.validation_dataset, tokenizer, args.batch_size)\n    day_hour = datetime.datetime.now().strftime('%m%d%H%M')\n    wandb_logger = WandbLogger(project=args.wandb_project, name=day_hour , group=args.wandb_group, save_dir=args.wandb_savedir)\n    trainer = pl.Trainer(accelerator=args.accelerator, default_root_dir=args.output_dir,\n                         fast_dev_run=args.fast_dev_run, max_epochs=args.max_epochs, logger=wandb_logger)\n    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=validation_loader)\n    model.save_huggingface_model(args.output_dir)\n    tokenizer.save_pretrained(args.output_dir)", "\n\ndef _add_arguments(parser):\n    \"\"\"Add train arguments to a parser in place.\"\"\"\n    parser.add_argument('-t', '--train-dataset', required=True,\n                        help='path to the training dataset. It should be a JSONL file with fields: query, positive, negatives')\n    parser.add_argument('-v', '--validation-dataset',\n                        help='path to the validation dataset. If not provided, 5 percent of the training data will be used as validation data.\\\n                        (default: None)')\n    parser.add_argument('-o', '--output-dir', default='artifacts/scorer',\n                        help='output model path. the model will be saved in the format of huggingface models,\\\n                        which can be uploaded to the huggingface hub and shared with the community.\\\n                        (default: artifacts/scorer)')\n    parser.add_argument('-m', '--model-name-or-path', default='intfloat/e5-small',\n                        help='pretrained model name or path. It is fully compatible with HuggingFace models.\\\n                        You can specify either a local path where a model is saved, or an encoder model identifier\\\n                        from huggingface hub. (default: intfloat/e5-small)')\n    parser.add_argument('-lr', '--learning-rate', default=5e-5, type=float, help='learning rate (default: 5e-5)')\n    parser.add_argument('--batch-size', default=16, type=int, help='batch size (default: 16)')\n    parser.add_argument('--loss', default='cross_entropy', choices=['cross_entropy', 'contrastive'],\n                        help='loss function, can be cross_entropy or contrastive (default: cross_entropy)')\n    parser.add_argument('--max-epochs', default=10, type=int, help='max epochs (default: 10)')\n    parser.add_argument('--accelerator', default='gpu', help='accelerator, can be cpu, gpu, or tpu (default: gpu)')\n    parser.add_argument('--fast-dev-run', action='store_true',\n                        help='fast dev run for debugging, only use 1 batch for training and validation')\n    parser.add_argument('--wandb-project', default='retrieval', help='wandb project name (default: retrieval)')\n    parser.add_argument('--wandb-group', default='contrastive', help='wandb group name (default: contrastive)')\n    parser.add_argument('--wandb-savedir', default='artifacts', help='wandb save directory (default: artifacts)')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    _add_arguments(parser)\n    args = parser.parse_args()\n\n    train(args)\n", ""]}
{"filename": "src/srtk/link.py", "chunked_list": ["\"\"\"Entity linking\nThis step links the entity mentions in the question to the entities in the Wikidata knowledge graph.\nIt inference on the REL endpoint.\n\"\"\"\nimport argparse\nfrom pathlib import Path\n\nimport srsly\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nfrom .entity_linking import WikidataLinker, DBpediaLinker\nfrom .utils import socket_reachable\n\n\ndef link(args):\n    \"\"\"Link the entities in the questions to the Wikidata knowledge graph\"\"\"\n    if not socket_reachable(args.el_endpoint):\n        raise RuntimeError(f\"Can't reach the endpoint {args.el_endpoint}\")\n\n    if args.knowledge_graph == 'wikidata':\n        linker = WikidataLinker(args.el_endpoint, args.wikimapper_db, service=args.service)\n    elif args.knowledge_graph == 'dbpedia':\n        linker = DBpediaLinker(args.el_endpoint)\n    else:\n        raise NotImplementedError(f\"Knowledge graph {args.knowledge_graph} not implemented\")\n\n    extra_kwargs = {}\n    if args.token:\n        extra_kwargs['token'] = args.token\n    with open(args.input, 'r', encoding='utf-8') as f:\n        total_lines = len(f.readlines())\n    questions = srsly.read_jsonl(args.input)\n    cnt_id_not_found = 0\n    cnt_id_found = 0\n    all_linked = []\n    for question in tqdm(questions, total=total_lines, desc=f\"Entity linking {args.input}\"):\n        linked = linker.annotate(question[args.ground_on], **extra_kwargs)\n        cnt_id_found += len(linked[\"question_entities\"])\n        if 'not_converted_entities' in linked:\n            cnt_id_not_found += len(linked['not_converted_entities'])\n        if 'id' in question:\n            linked['id'] = question['id']\n        all_linked.append(linked)\n    if cnt_id_not_found > 0:\n        print(f\"{cnt_id_not_found} / {cnt_id_found + cnt_id_not_found} grounded entities not converted to ids\")\n    # check whether the folder exists\n    folder_path = Path(args.output).parent\n    if not folder_path.exists():\n        folder_path.mkdir(parents=True)\n        print(f\"Folder {folder_path} created\")\n    Path(args.output).parent.mkdir(parents=True, exist_ok=True)\n    srsly.write_jsonl(args.output, all_linked)\n    print(f\"Entity linking result saved to {args.output}\")", "\n\ndef _add_arguments(parser):\n    \"\"\"Add entity linking arguments to the parser\"\"\"\n    parser.description = '''Entity linking on Wikidata.\n    The input is a jsonl file. The field of interest is specified by the argument --ground-on.\n    The output is a jsonl file, each line is a dict with keys: id, question_entities, spans, entity_names.\n    '''\n    parser.add_argument('-i', '--input', type=str, help='Input file path, in which the question is stored')\n    parser.add_argument('-o', '--output', type=str, help='Output file path, in which the entity linking result is stored')\n    parser.add_argument('-e', '--el-endpoint', type=str, default='http://127.0.0.1:1235', help='Entity linking endpoint \\\n                        (default: http://127.0.0.1:1235 <local REL endpoint>)')\n    parser.add_argument('-kg', '--knowledge-graph', type=str, default='wikidata', choices=['wikidata', 'dbpedia'],\n                        help='Knowledge graph to link to, only wikidata is supported now')\n    parser.add_argument('--wikimapper-db', type=str, default='resources/wikimapper/index_enwiki.db', help='Wikimapper database path')\n    parser.add_argument('--ground-on', type=str, default='question',\n                        help='The key to ground on, the corresponding text will be sent to the REL endpoint for entity linking')\n    parser.add_argument('--service', type=str, choices=['tagme', 'wat', 'rel', 'spotlight'],\n                        help='Entity linking service to use. Currently only tagme, wat, rel, spotlight are supported.')\n    parser.add_argument('--token', type=str, default=None, help='Token for the entity linking endpoint')", "\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    _add_arguments(parser)\n    args = parser.parse_args()\n    link(args)\n"]}
{"filename": "src/srtk/__init__.py", "chunked_list": [""]}
{"filename": "src/srtk/utils.py", "chunked_list": ["import socket\nfrom urllib.parse import urlparse\n\n\ndef get_host_port(url):\n    \"\"\"Get the host and port from a URL\"\"\"\n    parsed_url = urlparse(url)\n    host = parsed_url.hostname\n    port = parsed_url.port\n    if port is None:\n        if parsed_url.scheme == 'http':\n            port = 80\n        elif parsed_url.scheme == 'https':\n            port = 443\n    return host, port", "\n\ndef socket_reachable(url):\n    \"\"\"Check if a socket is reachable\n    \"\"\"\n    host, port = get_host_port(url)\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.settimeout(2) # set a timeout value for the socket\n        s.connect((host, port))\n        s.close()\n        return True\n    except Exception as err:\n        print(err)\n        return False", ""]}
{"filename": "src/srtk/visualize.py", "chunked_list": ["\"\"\"Visualize the graph (represented as a set of triplets) using pyvis.\nThe visualized subgraphs are html files.\n\"\"\"\nimport os\nimport argparse\nimport json\nfrom pathlib import Path\n\nimport srsly\nfrom pyvis.network import Network", "import srsly\nfrom pyvis.network import Network\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup as Soup\n\nfrom .knowledge_graph import KnowledgeGraphBase, get_knowledge_graph\n\n\n__DESCRIPTION__ = \"\"\"Visualize the graph (represented as a set of triplets) using pyvis.\nIt expects the input to be a JSONL file, where each line contains fields id, triplets, ", "__DESCRIPTION__ = \"\"\"Visualize the graph (represented as a set of triplets) using pyvis.\nIt expects the input to be a JSONL file, where each line contains fields id, triplets, \nquestion, answer, question_entities, answer_entities. Their meanings are as follows:\n- id: the id of the sample. the output will be named as [id].html\n- triplets (list[list[str]]): a list of triplets, where each triplet is a list of three strings: subject, relation, object\n- question (str): the question. it will be shown in backgraound\n- answer (str, optional): the answer. it will be shown in backgraound\n- question_entities (list[str], optional): a list of entity identifiers. they will be highlighted in blue.\n- answer_entities (list[str], optional): a list of entity identifiers. they will be highlighted in green.\n\"\"\"", "- answer_entities (list[str], optional): a list of entity identifiers. they will be highlighted in green.\n\"\"\"\n\ndef visualize_subgraph(sample, kg: KnowledgeGraphBase):\n    \"\"\"Visualize the subgraph. It returns an html string.\n    \"\"\"\n    net = Network(directed=True, font_color='#000000')\n    net.barnes_hut()\n    question_entities = sample['question_entities'] if 'question_entities' in sample else []\n    answer_entities = sample['answer_entities'] if 'answer_entities' in sample else []\n    # Add question entities even if they are not in the triplets\n    for entity in question_entities:\n        net.add_node(entity, label=kg.get_label(entity), color='#114B7A')\n    for triplet in sample['triplets']:\n        subject, relation, obj = triplet\n        subject_label = kg.get_label(subject)\n        subject_options = {'color':'#114B7A'} if subject in question_entities else {}\n        obj_label = kg.get_label(obj)\n        obj_options = {'color':'#1B5E20'} if obj in answer_entities else {}\n        relation_label = kg.get_label(relation)\n        net.add_node(subject, label=subject_label, **subject_options)\n        net.add_node(obj, label=obj_label, **obj_options)\n        net.add_edge(subject, obj, label=relation_label)\n\n    net_options = {\n        'shape': 'dot',\n        'font': {\n            'size': '1em',\n            'face': 'fontFace',\n            'strokeColor': '#fff',\n            'strokeWidth': 5\n        },\n        'size': '1.5em',\n    }\n    net.set_options(json.dumps(net_options))\n    return net.generate_html(notebook=False)", "\n\ndef add_text_to_html(html, text):\n    soup = Soup(html, 'html.parser')\n    style_tag = soup.new_tag('style')\n    style_tag.string = '''\n        .background-text {\n            position: absolute;\n            z-index: 1;\n            top: 0;\n            left: 0;\n            font-size: 2em;\n            color: #ccc;\n        }\n    '''\n    soup.head.append(style_tag)\n    p_tag = soup.new_tag('p', attrs={'class': 'background-text'}, style=\"white-space:pre-wrap\")\n    p_tag.string = text\n    soup.body.append(p_tag)\n    return soup.prettify()", "    \n\ndef visualize(args):\n    \"\"\"Main entry for subgraph visualization.\n\n    Args:\n        args (Namespace): arguments for subgraph visualization.\n    \"\"\"\n    knowledge_graph = get_knowledge_graph(args.knowledge_graph, args.sparql_endpoint)\n    samples = srsly.read_jsonl(args.input)\n    total = sum(1 for _ in srsly.read_jsonl(args.input))\n    total = min(total, args.max_output)\n    if not os.path.exists(args.output_dir):\n        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n        print(f'Created output directory: {args.output_dir}')\n    for i, sample in enumerate(tqdm(samples, desc='Visualizing graphs', total=total)):\n        if i >= args.max_output:\n            break\n        html = visualize_subgraph(sample, knowledge_graph)\n        text_to_append = f\"Question: {sample['question']}\"\n        if 'answer' in sample:\n            text_to_append += f\"\\n    Answer: {sample['answer']}\"\n        html = add_text_to_html(html, text_to_append)\n        output_path = os.path.join(args.output_dir, sample['id'] + '.html')\n        with open(output_path, 'w', encoding='utf-8') as fout:\n            fout.write(html)\n    print(f'Visualized graphs outputted to {args.output_dir}.')", "\n\ndef _add_arguments(parser):\n    parser.description = __DESCRIPTION__\n    parser.add_argument('-i', '--input', required=True, help='The input subgraph file path.')\n    parser.add_argument('-o', '--output-dir', required=True, help='The output directory path.')\n    parser.add_argument('-e', '--sparql-endpoint', type=str, default='http://localhost:1234/api/endpoint/sparql',\n                        help='SPARQL endpoint for Wikidata or Freebase services. In this step, it is used to get the labels of entities.\\\n                        (Default: http://localhost:1234/api/endpoint/sparql)')\n    parser.add_argument('-kg', '--knowledge-graph', type=str, choices=('wikidata', 'freebase', 'dbpedia'), default='wikidata',\n                        help='The knowledge graph type to use. (Default: wikidata)')\n    parser.add_argument('--max-output', type=int, default=1000,\n                        help='The maximum number of graphs to output. This is useful for debugging. (Default: 1000)')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    _add_arguments(parser)\n    args = parser.parse_args()\n    visualize(args)\n"]}
{"filename": "src/srtk/preprocess.py", "chunked_list": ["\"\"\"This script creates the training data from the grounded questions.\n\nInputs should be a jsonl file, with each line representing a grounded question.\nThe format of each line should be like this example:\n\n.. code-block:: json\n\n    {\n        \"id\": \"sample-id\",\n        \"question\": \"Which universities did Barack Obama graduate from?\",", "        \"id\": \"sample-id\",\n        \"question\": \"Which universities did Barack Obama graduate from?\",\n        \"question_entities\": [\n            \"Q76\"\n        ],\n        \"answer_entities\": [\n            \"Q49122\",\n            \"Q1346110\",\n            \"Q4569677\"\n        ]", "            \"Q4569677\"\n        ]\n    }\n\"\"\"\n\nimport os\nimport argparse\nfrom argparse import Namespace\nfrom pathlib import Path\n", "from pathlib import Path\n\nfrom .preprocessing.search_path import main as search_path\nfrom .preprocessing.score_path import main as score_path\nfrom .preprocessing.negative_sampling import main as negative_sampling\n\n\ndef preprocess(args):\n    output_path = args.output\n    # Create parent dir for output if not exists.\n    Path(os.path.dirname(output_path)).mkdir(parents=True, exist_ok=True)\n    if args.search_path:\n        intermediate_dir = args.intermediate_dir\n        if intermediate_dir is None:\n            intermediate_dir = os.path.dirname(output_path)\n        Path(intermediate_dir).mkdir(parents=True, exist_ok=True)\n        paths_file = os.path.join(intermediate_dir, 'paths.jsonl')\n        scores_file = os.path.join(intermediate_dir, 'scores.jsonl')\n        search_path_args = Namespace(sparql_endpoint=args.sparql_endpoint,\n                                    knowledge_graph=args.knowledge_graph,\n                                    ground_path=args.input,\n                                    output_path=paths_file,\n                                    remove_sample_without_path=True)\n        score_path_args = Namespace(sparql_endpoint=args.sparql_endpoint,\n                                    knowledge_graph=args.knowledge_graph,\n                                    paths_file=paths_file,\n                                    output_path=scores_file,\n                                    metric=args.metric,)\n        negative_sampling_args = Namespace(sparql_endpoint=args.sparql_endpoint,\n                                           knowledge_graph=args.knowledge_graph,\n                                           scored_path_file=scores_file,\n                                           num_negative=args.num_negative,\n                                           positive_threshold=args.positive_threshold,\n                                           output_path=output_path,)\n        search_path(search_path_args)\n        score_path(score_path_args)\n    else:\n        negative_sampling_args = Namespace(sparql_endpoint=args.sparql_endpoint,\n                                           knowledge_graph=args.knowledge_graph,\n                                           scored_path_file=args.input,\n                                           num_negative=args.num_negative,\n                                           positive_threshold=args.positive_threshold,\n                                           output_path=output_path,)\n    negative_sampling(negative_sampling_args)", "\n\ndef _add_arguments(parser):\n    \"\"\"Add preprocess arguments to a parser in place.\"\"\"\n    parser.description = 'Create the training data from the grounded questions.'\n    parser.add_argument('-i', '--input', type=str, required=True,\n                        help='The grounded questions file with question, question & answer entities')\n    parser.add_argument('-o', '--output', type=str, required=True,\n                        help='The output path where the final training data will be saved.')\n    parser.add_argument('--intermediate-dir', type=str, help=\"The directory to save intermediate files. If not specified, the intermediate \\\n                        files will be saved in the same directory as the output file, with the name paths.jsonl and scores.jsonl\")\n    parser.add_argument('-e', '--sparql-endpoint', type=str, required=True,\n                        help=\"SPARQL endpoint URL for either Wikidata or Freebase\\\n                        (e.g., 'http://localhost:1234/api/endpoint/sparql' for default local qEndpoint)\")\n    parser.add_argument('-kg', '--knowledge-graph', type=str, required=True, choices=('wikidata', 'freebase', 'dbpedia'),\n                        help='knowledge graph name, either wikidata or freebase')\n    parser.add_argument('--search-path', action='store_true',\n                        help='Whether to search paths between question and answer entities. If not specified, paths and scores fields\\\n                        must present in the input file. You **have to** specify this for weakly supervised learning. (default: False)')\n    parser.add_argument('--metric', choices=('jaccard', 'recall'), default='jaccard',\n                        help='The metric used to score the paths. recall will usually result in a lager size of training dataset.\\\n                        (default: jaccard))')\n    parser.add_argument('--num-negative', type=int, default=15,\n                        help='The number of negative relations to sample for each positive relation. (default: 15)')\n    parser.add_argument('--positive-threshold', type=float, default=0.5,\n                        help='The threshold to determine whether a path is positive or negative. If you want to use \\\n                        a larger training dataset, you can set this value to a smaller value. (default: 0.5)')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    _add_arguments(parser)\n    args = parser.parse_args()\n    preprocess(args)\n"]}
{"filename": "src/srtk/retrieve.py", "chunked_list": ["\"\"\"This script retrieves subgraphs from a knowledge graph according to a natural\nlanguage query (usually a question). This command can also be used to evaluate\na trained retriever when the answer entities are known.\n\nThe expected fields of one sample are:\n- question: question text\n- question_entities: list of grounded question entities (ids)\n\nFor evaluation, the following field is also required:\n- answer_entities: list of grounded answer entities (ids)", "For evaluation, the following field is also required:\n- answer_entities: list of grounded answer entities (ids)\n\"\"\"\nimport argparse\nimport heapq\nimport os\nimport pathlib\nfrom collections import namedtuple\nfrom typing import List, Any, Dict\n", "from typing import List, Any, Dict\n\nimport srsly\nimport torch\nfrom tqdm import tqdm\n\nfrom .knowledge_graph import KnowledgeGraphBase, get_knowledge_graph\nfrom .scorer import Scorer\n\n", "\n\nEND_REL = 'END OF HOP'\n\n\n# Path collects the information at each traversal step\n# - prev_relations stores the relations that have been traversed\n# - score stores the score of the relation path\nPath = namedtuple('Path', ['prev_relations', 'score'], defaults=[(), 0])\n", "Path = namedtuple('Path', ['prev_relations', 'score'], defaults=[(), 0])\n\n\nclass KnowledgeGraphTraverser:\n    '''KnowledgeGraphTraverser is a helper class that traverses a knowledge graph'''\n    def __init__(self, kg: KnowledgeGraphBase):\n        self.kg = kg\n\n    def retrive_subgraph(self, entity, path):\n        '''Retrive subgraph entities and triplets by traversing from an entity following\n        a relation path hop by hop.\n\n        Args:\n            entity (str): the identifier of the source node\n            path (list[str]): a list of relation identifiers\n\n        Returns:\n            entities: a list of entity identifiers\n            triplets: a list of triplets\n        '''\n        entities, triplets = set(), set()\n        tracked_entities = set((entity,))\n        for relation in path:\n            next_hops = set()\n            if relation == END_REL:\n                continue\n            for e in tracked_entities:\n                leaves = set(self.kg.deduce_leaves(e, (relation,)))\n                next_hops |= leaves\n                triplets |= {(e, relation, leaf) for leaf in leaves}\n            entities |= next_hops\n            tracked_entities = next_hops\n        return list(entities), list(triplets)\n\n    def deduce_leaves(self, entity, path):\n        \"\"\"Deduce leaves from an entity following a path hop by hop.\n\n        Args:\n            entity (str): the identifier of the source node\n            path (list[str]): a list of relation identifiers\n\n        Returns:\n            set[str]: a set of leave identifiers that are n-hop away from the source node,\n                where n is the length of the path\n        \"\"\"\n        leaves = set((entity,))\n        for relation in path:\n            if relation == END_REL:\n                continue\n            leaves = set().union(*(self.kg.deduce_leaves(leaf, (relation,)) for leaf in leaves))\n        return leaves\n\n    def deduce_leaf_relations(self, entity, path):\n        \"\"\"Deduce leaf relations from an entity following a path hop by hop.\n\n        Args:\n            entity (str): the identifier of the source node\n            path (list[str]): a list of relation identifiers\n\n        Returns:\n            tuple[str]: a tuple of relations that are n-hop away from the source node,\n                where n is the length of the path\n        \"\"\"\n        leaves = self.deduce_leaves(entity, path)\n        relations = set().union(*(self.kg.get_neighbor_relations(leaf) for leaf in leaves))\n        # Special filter relation for freebase\n        if self.kg.name == 'freebase':\n            relations = [r for r in relations if r.split('.')[0] not in ['kg', 'common']]\n        return tuple(relations)\n\n    def get_relation_label(self, identifier):\n        \"\"\"Get the relation label of an entity or a relation.\n\n        It serves as a proxy to the knowledge graph's get_label function. For freebase,\n        we directly use the identifier as the label. For others, we return the retrieved\n        label if it exists, otherwise return the identifier.\n\n        Args:\n            identifier (str): the identifier of an entity or a relation\n\n        Returns:\n            str: the label of the entity or the relation\n        \"\"\"\n        # For freebase, the relation identifier contains enough information\n        if self.kg.name == 'freebase' or self.kg.name == 'dbpedia':\n            return identifier\n        if identifier == END_REL:\n            return END_REL\n        label =  self.kg.get_label(identifier)\n        if label is None:\n            return identifier\n        return label", "\n\nclass Retriever:\n    '''Retriever retrieves subgraphs from a knowledge graph with a question and its\n    linked entities. The retrieval process takes the semantic information of the question\n    and the expanding path into consideration.\n    '''\n    def __init__(self, kg: KnowledgeGraphBase, scorer: Scorer, beam_width: int, max_depth: int):\n        self.kgh = KnowledgeGraphTraverser(kg)\n        self.scorer = scorer\n        self.beam_width = beam_width\n        self.max_depth = max_depth\n        self.num_entity_threshold = 1000\n\n    def retrieve_subgraph_triplets(self, sample: Dict[str, Any]):\n        \"\"\"Retrieve triplets as subgraphs from paths.\n\n        Args:\n            sample (dict): a sample from the dataset, which contains at least the following fields:\n                question: a string\n                question_entities: a list of entities\n\n        Returns:\n            list(tuple): a list of triplets\n        \"\"\"\n        question = sample['question']\n        triplets = []\n        for question_entity in sample['question_entities']:\n            path_score_list = self.beam_search_path(question, question_entity)\n            n_nodes = 0\n            for relations, _ in path_score_list:\n                partial_nodes, partial_triples = self.kgh.retrive_subgraph(question_entity, relations)\n                if len(partial_nodes) > self.num_entity_threshold:\n                    continue\n                n_nodes += len(partial_nodes)\n                triplets.extend(partial_triples)\n\n                if n_nodes > self.num_entity_threshold:\n                    break\n        triplets = list(set(triplets))\n        return triplets\n\n    def beam_search_path(self, question: str, question_entity: str):\n        '''This function reimplement RUC's paper's solution. In the search process, only the history\n        paths are recorded; each new relation is looked up via looking up the end relations from the\n        question entities following a history path.\n\n        Args:\n            question (str): a natural language question\n            question_entity (str): a grounded question entity\n\n        Returns:\n            list[Path]: path score list, a list of (path, score) tuples\n        '''\n        candidate_paths = [Path()]  # path and its score\n        result_paths = []\n        depth = 0\n\n        while candidate_paths and len(result_paths) < self.beam_width and depth < self.max_depth:\n            next_relations_batched = []\n            for path in candidate_paths:\n                prev_relations = path.prev_relations\n                next_relations = self.kgh.deduce_leaf_relations(question_entity, prev_relations)\n                next_relations = next_relations + (END_REL,)\n                next_relations_batched.append(next_relations)\n\n            tracked_paths = self.expand_and_score_paths(question, candidate_paths, next_relations_batched)\n            tracked_paths = heapq.nlargest(self.beam_width, tracked_paths, key=lambda x: x.score)\n            depth += 1\n            # Update candidate_paths\n            candidate_paths = []\n            for path in tracked_paths:\n                if  path.prev_relations and path.prev_relations[-1] == END_REL:\n                    result_paths.append(path)\n                else:\n                    candidate_paths.append(path)\n        # Merge not-yet-ended paths into the result paths \n        candidate_paths = [Path(prev_relations + (END_REL,), score) for prev_relations, score in candidate_paths]\n        result_paths = heapq.nlargest(self.beam_width, result_paths + candidate_paths, key=lambda x: x.score)\n        return result_paths\n\n    def expand_and_score_paths(self, question: str, paths: List[Path], relations_batched: List[List[str]]) -> List[Path]:\n        '''Expand the paths by one hop and score them by comparing the embedding similarity between\n        the query (question + prev_relations) and the next relation.\n\n        Args:\n            question (str)\n            paths (list[Path]): a list of current paths\n            relations_batched (list[list[str]]): a list of next relations for each path\n\n        Returns:\n            list[Path]: scored_paths, a list of newly expanded and scored paths\n        '''\n        scored_paths = []\n        score_matrix = []\n        for path, next_relations in zip(paths, relations_batched):\n            prev_relation_labels = tuple(self.kgh.get_relation_label(r) for r in path.prev_relations)\n            next_relation_labels = tuple(self.kgh.get_relation_label(r) for r in next_relations)\n            scores = self.scorer.batch_score(question, prev_relation_labels, next_relation_labels)\n            score_matrix.append(scores)\n\n        for i, (path, next_relations) in enumerate(zip(paths, relations_batched)):\n            for j, relation in enumerate(next_relations):\n                new_prev_relations = path.prev_relations + (relation,)\n                score = float(score_matrix[i][j]) + path.score\n                scored_paths.append(Path(new_prev_relations, score))\n        return scored_paths", "\n\ndef calculate_hit_and_miss(retrieved_path):\n    \"\"\"Calculate the recall of answer entities in retrieved triplets,\n    if answer_entities exists in each sample.\n    \n    Args:\n        retrieved_path (str): path to the retrieved triplets\n    \n    Returns:\n        tuple(int, int): number of samples that have at least one answer entity in retrieved triplets,\n            and number of samples that have no answer entity in retrieved triplets\n        \n    \"\"\"\n    retrieval = srsly.read_jsonl(retrieved_path)\n    hit = 0\n    miss = 0\n    for sample in retrieval:\n        if 'answer_entities' not in sample:\n            continue\n        answers = sample['answer_entities']\n        entities = set().union(*((triplet[0], triplet[-1]) for triplet in sample['triplets']))\n        if any([entity in answers for entity in entities]):\n            hit += 1\n        else:\n            miss += 1\n    return hit, miss", "\n\ndef calculate_subgraph_size(retrieved_path):\n    \"\"\"Calculate the average number of triplets, entities and relations in retrieved subgraphs.\n\n    Args:\n        retrieved_path (str): path to the retrieved triplets\n\n    Returns:\n        tuple: average number of triplets, entities and relations in retrieved subgraphs\n    \"\"\"\n    retrieval = srsly.read_jsonl(retrieved_path)\n    n_triplets = []\n    n_entities = []\n    n_relations = []\n    for sample in retrieval:\n        entities = set().union(*((triplet[0], triplet[-1]) for triplet in sample['triplets']))\n        relations = set().union(*((triplet[1],) for triplet in sample['triplets']))\n        n_triplets.append(len(sample['triplets']))\n        n_entities.append(len(entities))\n        n_relations.append(len(relations))\n    avg_n_triplets = sum(n_triplets) / len(n_triplets) if len(n_triplets) > 0 else 0\n    avg_n_entities = sum(n_entities) / len(n_entities) if len(n_entities) > 0 else 0\n    avg_n_relations = sum(n_relations) / len(n_relations) if len(n_relations) > 0 else 0\n    return avg_n_triplets, avg_n_entities, avg_n_relations", "\n\ndef print_and_save_recall(retrieved_path):\n    \"\"\"Calculate and print the recall of answer entities in retrieved triplets,\n    If any answer from the answer entities is in the retrieved entities, the sample\n    counts as a hit.\n    \"\"\"\n    hit, miss = calculate_hit_and_miss(retrieved_path)\n    avg_triplets, avg_entities, avg_relations = calculate_subgraph_size(retrieved_path)\n    print(f\"Answer coverage rate: {hit / (hit + miss)} ({hit} / {hit + miss})\")\n    print(f\"Average number of triplets: {avg_triplets}, entities: {avg_entities}, relations: {avg_relations}\")\n    info = {}\n    if hit + miss != 0:\n        info = {\n            'hit': hit,\n            'miss': miss,\n            'recall': hit / (hit + miss)\n        }\n    # path/to/subgraph.jsonl -> path/to/subgraph.metric\n    recall_path = os.path.splitext(retrieved_path)[0] + '.metric'\n    srsly.write_json(recall_path, info)", "\n\ndef retrieve(args):\n    \"\"\"Retrieve subgraphs from a knowledge graph.\n\n    Args:\n        args (Namespace): arguments for subgraph retrieval\n    \"\"\"\n    pathlib.Path(os.path.dirname(args.output)).mkdir(parents=True, exist_ok=True)\n    kg = get_knowledge_graph(args.knowledge_graph, args.sparql_endpoint,\n                             prepend_prefixes=not args.omit_prefixes,\n                             exclude_qualifiers=not args.include_qualifiers)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    scorer = Scorer(args.scorer_model_path, device)\n    retriever = Retriever(kg, scorer, args.beam_width, args.max_depth)\n    samples = list(srsly.read_jsonl(args.input))\n    total = sum(1 for _ in srsly.read_jsonl(args.input))\n    for sample in tqdm(samples, desc='Retrieving subgraphs', total=total):\n        triplets = retriever.retrieve_subgraph_triplets(sample)\n        sample['triplets'] = triplets\n    srsly.write_jsonl(args.output, samples)\n    print(f'Retrieved subgraphs saved to to {args.output}')\n    if args.evaluate:\n        print_and_save_recall(args.output)", "\n\ndef _add_arguments(parser):\n    \"\"\"Add retrieve arguments to the parser in place.\"\"\"\n    parser.description = '''Retrieve subgraphs with a trained model on a dataset that entities are linked.\n    This command can also be used to evaluate a trained retriever when the answer entities are known.\n\n    Provide a JSON file as input, where each JSON object must contain at least the 'question' and 'question_entities' fields.\n    When ``--evaluate`` is set, the input JSON file must also contain the 'answer_entities' field.\n\n    The output JSONL file will include an added 'triplets' field, based on the input JSONL file. This field consists of a list of triplets,\n    with each triplet representing a (head, relation, tail) tuple.\n    When ``--evaluate`` is set, a metric file will also be saved to the same directory as the output JSONL file.\n    '''\n    parser.add_argument('-i', '--input', type=str, required=True, help='path to input jsonl file. it should contain at least \\\n                        ``question`` and ``question_entities`` fields.')\n    parser.add_argument('-o', '--output', type=str, required=True, help='output file path for storing retrieved triplets.')\n    parser.add_argument('-e', '--sparql-endpoint', type=str, help='SPARQL endpoint for Wikidata or Freebase services.')\n    parser.add_argument('-kg', '--knowledge-graph', type=str, required=True, choices=('freebase', 'wikidata', 'dbpedia'),\n                        help='choose the knowledge graph: currently supports ``freebase``, ``wikidata`` and ``dbpedia``.')\n    parser.add_argument('-m', '--scorer-model-path', type=str, required=True, help='Path to the scorer model, containing \\\n                        both the saved model and its tokenizer in the Huggingface models format.\\\n                        Such a model is saved automatically when using the ``srtk train`` command.\\\n                        Alternatively, provide a pre-trained model name from the Hugging Face model hub.\\\n                        In practice it supports any Huggingface transformers encoder model, though models that do not use [CLS] \\\n                        tokens may require modifications on similarity function.')\n    parser.add_argument('--beam-width', type=int, default=10, help='beam width for beam search (default: 10).')\n    parser.add_argument('--max-depth', type=int, default=2, help='maximum depth for beam search (default: 2).')\n    parser.add_argument('--evaluate', action='store_true', help='Evaluate the retriever model. When the answer \\\n                        entities are known, the recall can be evluated as the number of samples that any of the \\\n                        answer entities are retrieved in the subgraph by the number of all samples. This equires \\\n                        `answer_entities` field in the input jsonl.')\n    parser.add_argument('--include-qualifiers', action='store_true', help='Include qualifiers from the retrieved triplets. \\\n                        Qualifiers are informations represented in non-entity form, like date, count etc.\\\n                        This is only relevant for Wikidata.')\n    parser.add_argument('--omit-prefixes', action='store_true', help='Whether to omit prefixes when passing SPARQLs \\\n                        to the endpoints. This can potentially save some bandwidth, but may cause errors when the \\\n                        prefixes are not defined in the endpoint.')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    _add_arguments(parser)\n    args = parser.parse_args()\n    if not args.sparql_endpoint:\n        if args.knowledge_graph == 'freebase':\n            args.sparql_endpoint = 'http://localhost:3001/sparql'\n        elif args.knowledge_graph == 'wikidata':\n            args.sparql_endpoint = 'http://localhost:1234/api/endpoint/sparql'\n        else:\n            args.sparql_endpoint = 'https://dbpedia.org/sparql'\n        print(f'Using default sparql endpoint for {args.knowledge_graph}: {args.sparql_endpoint}')\n    retrieve(args)", ""]}
{"filename": "src/srtk/cli.py", "chunked_list": ["\"\"\"Command-line interface for SRTK.\"\"\"\nimport argparse\n\nfrom .link import _add_arguments as add_link_arguments\nfrom .link import link\nfrom .preprocess import _add_arguments as add_preprocess_arguments\nfrom .preprocess import preprocess\nfrom .retrieve import _add_arguments as add_retrieve_arguments\nfrom .retrieve import retrieve\nfrom .train import _add_arguments as add_train_arguments", "from .retrieve import retrieve\nfrom .train import _add_arguments as add_train_arguments\nfrom .train import train\nfrom .visualize import _add_arguments as add_visualize_arguments\nfrom .visualize import visualize\n\n\ndef main():\n    \"\"\"Main entry to the command line interface.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='SRTK: A toolkit for smantic-relevant subgraph retrieval')\n    subparsers = parser.add_subparsers(help='sub-command help')\n\n    parser_link = subparsers.add_parser('link', help='link entities to a knowledge graph')\n    add_link_arguments(parser_link)\n    parser_link.set_defaults(func=link)\n\n    parser_preprocess = subparsers.add_parser('preprocess', help='preprocess the data')\n    add_preprocess_arguments(parser_preprocess)\n    parser_preprocess.set_defaults(func=preprocess)\n\n    parser_train = subparsers.add_parser('train', help='train a subgraph retriever')\n    add_train_arguments(parser_train)\n    parser_train.set_defaults(func=train)\n\n    parser_retrieve = subparsers.add_parser('retrieve', help='retrieve a subgraph with a natural query')\n    add_retrieve_arguments(parser_retrieve)\n    parser_retrieve.set_defaults(func=retrieve)\n\n    parser_visualize = subparsers.add_parser('visualize', help='visualize the retrieved subgraph')\n    add_visualize_arguments(parser_visualize)\n    parser_visualize.set_defaults(func=visualize)\n\n    args = parser.parse_args()\n    if hasattr(args, 'func'):\n        args.func(args)\n    else:\n        parser.print_help()", "\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "src/srtk/preprocessing/merge_ground.py", "chunked_list": ["\"\"\"2. This script merges the grounded data into one training data.\n\ne.g.\npython preprocess/merge_ground.py  --output-path data/preprocess/merged-ground.jsonl --ground-files\\\n    data/preprocess/mintaka-ground.jsonl data/preprocess/mkqa-ground.jsonl\n\"\"\"\nimport argparse\nimport srsly\n\ndef main(args):\n    merged_samples = []\n    for ground_file in args.ground_files:\n        samples = srsly.read_jsonl(ground_file)\n        merged_samples.extend(samples)\n    srsly.write_jsonl(args.output_path, merged_samples)", "\ndef main(args):\n    merged_samples = []\n    for ground_file in args.ground_files:\n        samples = srsly.read_jsonl(ground_file)\n        merged_samples.extend(samples)\n    srsly.write_jsonl(args.output_path, merged_samples)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--output-path', type=str, required=True)\n    parser.add_argument('--ground-files', nargs='+', required=True)\n    args = parser.parse_args()\n    main(args)", "if __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--output-path', type=str, required=True)\n    parser.add_argument('--ground-files', nargs='+', required=True)\n    args = parser.parse_args()\n    main(args)\n"]}
{"filename": "src/srtk/preprocessing/negative_sampling.py", "chunked_list": ["\"\"\"5. Negative Sampling\n\nRegarding negative sampling method, the author states in the paper:\n> We replace the observed relation at each time step with other sampled relations as the\nnegative instances to optimize the probability of the observed ones.\n\ne.g.\npython preprocess/negative_sampling.py \\\n    --scored-path-file data/preprocess/paths_scored.jsonl \\\n    --output-file data/preprocess/train_.jsonl\\", "    --scored-path-file data/preprocess/paths_scored.jsonl \\\n    --output-file data/preprocess/train_.jsonl\\\n    --positive-threshold 0.3\n\"\"\"\nimport os\nimport sys\nimport argparse\nimport random\nfrom collections import defaultdict\nfrom functools import lru_cache", "from collections import defaultdict\nfrom functools import lru_cache\n\nimport srsly\nfrom tqdm import tqdm\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', )))\nfrom knowledge_graph import KnowledgeGraphBase, get_knowledge_graph\n\n", "\n\nEND_REL = \"END OF HOP\"\n\n\ndef sample_negative_relations(soruce_entities, prev_path, positive_connections,\n                              num_negative, kg: KnowledgeGraphBase):\n    \"\"\"A helper function to sample negative relations.\n    \n    Args:\n        soruce_entities (list[str]): list of source entities\n        prev_path (list[str]): previous path / relations\n        positive_connections (dict): a dictionary of positive connections\n        num_negative (int): number of negative relations to sample\n        kg (KnowledgeGraphBase): a knoledge graph instance\n        \n    Returns:\n        list[str]: list of negative relations\n    \"\"\"\n    negative_relations = set()\n    for src in soruce_entities:\n        # get all relations connected to current tracked entities (question or intermediate entities)\n        negative_relations |= set(kg.get_neighbor_relations(src, limit=50))\n        if len(negative_relations) > 100:  # yet another magic number :(\n            break\n    negative_relations = negative_relations - positive_connections[tuple(prev_path)]\n    if len(negative_relations) == 0:\n        return []\n    negative_relations = random.choices(list(negative_relations), k=num_negative)\n    return negative_relations", "\n\ndef is_candidate_space_too_large(path, question_entities, kg: KnowledgeGraphBase, candidate_depth_multiplier=5):\n    \"\"\"Check whether the number of the candidate entities along the path is too large.\n    \n    Args:\n        path (list[str]): path from source entity to destination entity\n        question_entities (list[str]): list of question entities\n        kg (KnowledgeGraphBase): a knowledge graph instance\n        candidate_depth_multiplier (int, optional): a multiplier to control the number of candidate entities\n            at each depth. Defaults to 10.\n    \"\"\"\n    flag_too_large = False\n    for i in range(1, len(path)):\n        prev_path = tuple(path[:i])\n        # The further the path is from the question, the greater the search space becomes.\n        limit = candidate_depth_multiplier ** i\n        candidate_entities = set()\n        for src in question_entities:\n            candidate_entities |= set(kg.deduce_leaves(src, prev_path, limit=limit))\n            # Stop early if the number of candidate entities is already very large\n            if len(candidate_entities) > limit:\n                break\n        # Check the entity space at each depth, if it is too large at any depth, we flag the path as too large.\n        if len(candidate_entities) > limit:\n            flag_too_large = True\n            break\n    return flag_too_large", "\n\ndef sample_records_from_path(path, question, question_entities, positive_connections,\n                             kg: KnowledgeGraphBase, num_negative):\n    \"\"\"Sample training records from a path.\n    \n    Returns:\n        list[dict]: list of training records, each record has the following fields:\n            - question (str): the question\n            - prev_path (list): previous relations up to a relation (positive_relation) in the path\n            - positive_relation (str): the next relation of the prev_path is regarded as the positive relation\n            - negative_relations (list): a list of negative relations, the number is specified by num_negative\n    \"\"\"\n    # My interpretation: If the number of candidate entities is too large, we simply discard this path.\n    # But isn't it weird? The author checks the number of connected entities to the question entities\n    # with each relation along the path, and simply discard those paths with too many connected entities.\n    if is_candidate_space_too_large(path, question_entities, kg, candidate_depth_multiplier=5):\n        return []\n\n    path = path + [END_REL]\n\n    records = []\n    tracked_entities = question_entities\n    for i, current_relation in enumerate(path):\n        prev_path = path[:i]\n        negative_relations = sample_negative_relations(tracked_entities, prev_path, positive_connections,\n                                                       num_negative, kg)\n        if len(negative_relations) == 0:\n            continue\n        record = {\n            'question': question,\n            'prev_path': prev_path,\n            'positive_relation': current_relation,\n            'negative_relations': negative_relations\n        }\n        records.append(record)\n        if current_relation != END_REL:\n            # update tracked entities\n            tracked_entities = kg.deduce_leaves_from_multiple_srcs(tracked_entities, [current_relation], limit=100)\n    return records", "\n\ndef get_positive_connections_along_paths(paths):\n    \"\"\"Collect positive connections along paths. A positive connection is defined as\n    {prev_relations: next_relation}. END_REL is added to the end of each path.\n    \n    Returns:\n        dict: a dictionary of positive connections\n    \"\"\"\n    positive_connections = defaultdict(set)\n    for path in paths:\n        path = path + [END_REL]\n        for i, rel in enumerate(path):\n            positive_connections[tuple(path[:i])].add(rel)\n    return positive_connections", "\n\ndef convert_records_relation_id_to_lable(records, kg):\n    \"\"\"Convert relation ids to relation labels in each record.\n    \"\"\"\n    processed_records = []\n\n    @lru_cache\n    def get_label(rel):\n        if kg.name == 'dbpedia' or kg.name == 'freebase':\n            return rel\n        if rel == END_REL:\n            return END_REL\n        return kg.get_relation_label(rel) or rel\n\n    for record in tqdm(records, desc='Converting relation ids to labels'):\n        record['prev_path'] = [get_label(rel) for rel in record['prev_path']]\n        record['positive_relation'] = get_label(record['positive_relation'])\n        record['negative_relations'] = [get_label(rel) for rel in record['negative_relations']]\n        processed_records.append(record)\n    return processed_records", "\n\ndef create_jsonl_dataset(records):\n    \"\"\"It combines the question and prev_path to query. Each train sample is a dict with the following fields:\n    - query (str): question + prev_path\n    - positive (str): the next relation of the prev_path is regarded as the positive relation\n    - negatives (list): a list of negative relations\n    \n    Args:\n        records (list[dict]): list of records\n        \n    Returns:\n        list[dict]: list of train samples\n    \"\"\"\n    samples = []\n    for record in records:\n        sample = {\n            'query': record['question'] + ' [SEP] ' + ' # '.join(record['prev_path']),\n            'positive': record['positive_relation'],\n            'negatives': record['negative_relations']\n        }\n        samples.append(sample)\n    return samples", "\n\ndef main(args):\n    kg = get_knowledge_graph(args.knowledge_graph, args.sparql_endpoint)\n    positive_threshold = args.positive_threshold\n    # Each sample has the following fields:\n    # - id: sample id\n    # - question: question text\n    # - question_entities: list of question entities (ids)\n    # - answer_entities: list of answer entities (ids)\n    # - question: question text\n    # - paths: list of paths\n    # - path_scores: list of path scores\n    samples = srsly.read_jsonl(args.scored_path_file)\n    total = sum(1 for _ in srsly.read_jsonl(args.scored_path_file))\n    train_records = []\n    for sample in tqdm(samples, total=total, desc='Negative sampling'):\n        paths = sample['paths']\n        path_scores = sample['path_scores']\n        question = sample['question']\n        question_entities = sample['question_entities']\n\n        # Filter out paths with low scores.\n        paths = [path for path, score in zip(paths, path_scores) if score >= positive_threshold]\n        path_scores = [score for score in path_scores if score >= positive_threshold]\n        if len(paths) != len(path_scores):\n            raise ValueError(f'The number of paths and path scores are not equal. {len(paths)} != {len(path_scores)}')\n\n        # A dictionary of {prev_rels: {next_rel, next_rel, ...}, ...},\n        # where prev_rels is a tuple of previous relations.\n        positive_connections = get_positive_connections_along_paths(paths)\n\n        for path in paths:\n            train_records.extend(sample_records_from_path(path, question, question_entities,\n                                                          positive_connections, kg, args.num_negative))\n    print(f\"Number of training records: {len(train_records)}\")\n    train_records = convert_records_relation_id_to_lable(train_records, kg)\n    train_records = create_jsonl_dataset(train_records)\n    srsly.write_jsonl(args.output_path, train_records)\n    print(f\"Training samples are saved to {args.output_path}\")", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-e', '--sparql-endpoint', default='http://localhost:1234/api/endpoint/sparql',\n                        help='knowledge graph endpoint (default: http://localhost:1234/api/endpoint/sparql)')\n    parser.add_argument('-kg', '--knowledge-graph', default='wikidata', choices=['wikidata', 'freebase', 'dbpedia'],\n                        help='knowledge graph name')\n    parser.add_argument('--scored-path-file', help='The file containing scored paths')\n    parser.add_argument('--output-path', help='The path to the output file')\n    parser.add_argument('--positive-threshold', type=float, default=0.5, help='The threshold to determine whether a path is positive or negative')\n    parser.add_argument('--num-negative', type=int, default=15, help='The number of negative relations to sample for each positive relation')\n    args = parser.parse_args()\n\n    main(args)", ""]}
{"filename": "src/srtk/preprocessing/score_path.py", "chunked_list": ["\"\"\"4. Score path\n\nThe score of a relation path is defined as the HIT rate of the prediction\nwith the ground truth entities. The *prediction* refers to the search results\nfrom the question entities following the relation path.\n\nPersonal notes:\nWhy this is necessary? Isn't the relation path already the path from the question\nentities to the ground truth entities?\nIn my understanding, this is similar to TF-IDF, the path is more precise if the results is", "entities to the ground truth entities?\nIn my understanding, this is similar to TF-IDF, the path is more precise if the results is\na smaller set of entities but have a higher intersection with the ground truth entities.\n\ne.g.\npython preprocess/score_path.py --paths-file data/preprocess/paths.jsonl --output-path data/preprocess/paths_scored.jsonl\n\"\"\"\nimport os\nimport sys\nimport argparse", "import sys\nimport argparse\n\nimport srsly\nfrom tqdm import tqdm\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', )))\nfrom knowledge_graph import KnowledgeGraphBase, get_knowledge_graph\n\n\ndef score_path(kg: KnowledgeGraphBase, src, path, answers, metric='jaccard'):\n    \"\"\"Calculate the HIT score of a given path.\n    \n    Args:\n        kg (KnowledgeGraphBase): knowledge graph instance\n        src (str): the source entity\n        path (list): the path\n        answers (list): the ground truth entities\n        metric (str): how the paths are scored. 'jaccard' or 'recall'\n            Default: 'jaccard', per the original implementation\n    \"\"\"\n    if metric not in ('jaccard', 'recall'):\n        raise ValueError(f'Unknown metric: {metric}')\n    leaves = kg.deduce_leaves(src, path)\n    leaves = set(leaves)\n    answers = set(answers)\n    hit = leaves.intersection(answers)\n    if not leaves:\n        # In the original implementation, they return 1 if the leaves is empty\n        # I think it's more meaningful to set it to 0, as this path leads to no results\n        return 0\n    if metric == 'jaccard':\n        score = len(hit) / len(leaves)\n    else: # metric == 'recall':\n        score = len(hit) / len(answers)\n    return score", "\n\ndef score_path(kg: KnowledgeGraphBase, src, path, answers, metric='jaccard'):\n    \"\"\"Calculate the HIT score of a given path.\n    \n    Args:\n        kg (KnowledgeGraphBase): knowledge graph instance\n        src (str): the source entity\n        path (list): the path\n        answers (list): the ground truth entities\n        metric (str): how the paths are scored. 'jaccard' or 'recall'\n            Default: 'jaccard', per the original implementation\n    \"\"\"\n    if metric not in ('jaccard', 'recall'):\n        raise ValueError(f'Unknown metric: {metric}')\n    leaves = kg.deduce_leaves(src, path)\n    leaves = set(leaves)\n    answers = set(answers)\n    hit = leaves.intersection(answers)\n    if not leaves:\n        # In the original implementation, they return 1 if the leaves is empty\n        # I think it's more meaningful to set it to 0, as this path leads to no results\n        return 0\n    if metric == 'jaccard':\n        score = len(hit) / len(leaves)\n    else: # metric == 'recall':\n        score = len(hit) / len(answers)\n    return score", "\n\ndef main(args):\n    kg = get_knowledge_graph(args.knowledge_graph_type, args.sparql_endpoint)\n    samples = srsly.read_jsonl(args.paths_file)\n    total_lines = sum(1 for _ in srsly.read_jsonl(args.paths_file))\n    processed_samples = []  # adds path_scores to each sample\n    # Each sample is a dict with the following fields:\n    # - id: sample id\n    # - question: question text\n    # - question_entities: list of question entities\n    # - answer_entities: list of answer entities\n    # - paths: list of paths\n    for sample in tqdm(samples, total=total_lines, desc='Scoring paths'):\n        question_entities = sample['question_entities']\n        answer_entities = sample['answer_entities']\n        paths = sample['paths']\n        path_scores = []\n        for path in paths:\n            # path score is the max score of all possible source entities following the path\n            # Personal note: this is weird, why don't you start from the question entity where the\n            # path was originally found?\n            path = tuple(path)  # this makes it hashable\n            score = max(score_path(kg, src, path, answer_entities, metric=args.metric) for src in question_entities)\n            path_scores.append(score)\n        sample['path_scores'] = path_scores\n        processed_samples.append(sample)\n    srsly.write_jsonl(args.output_path, processed_samples)\n    print(f'Scored paths saved to {args.output_path}')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-e', '--sparql-endpoint', default='http://localhost:1234/api/endpoint/sparql', help='knowledge graph endpoint')\n    parser.add_argument('-kg', '--knowledge-graph', default='wikidata', choices=('wikidata', 'freebase', 'dbpedia'),\n                        help='knowledge graph name')\n    parser.add_argument('--paths-file', help='the file where the paths are stored')\n    parser.add_argument('--output-path', help='the file where the scores are stored')\n    parser.add_argument('--metric', default='jaccard', choices=('jaccard', 'recall'), help='the metric used to score the paths')\n    args = parser.parse_args()\n\n    main(args)", ""]}
{"filename": "src/srtk/preprocessing/__init__.py", "chunked_list": [""]}
{"filename": "src/srtk/preprocessing/search_path.py", "chunked_list": ["\"\"\"3. Search Path\n\nThis corresponds to search_to_get_path.py in the RUC's code.\nIt enumerates all paths from the question entities to answer entities.\n\npython preprocess/search_path.py --ground-path data/preprocess/merged-ground.jsonl --output-path data/preprocess/paths.jsonl --remove-sample-without-path\n\"\"\"\nimport sys\nimport os\nimport argparse", "import os\nimport argparse\n\nimport srsly\nfrom tqdm import tqdm\n\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', )))\nfrom knowledge_graph import KnowledgeGraphBase, get_knowledge_graph\n\n\ndef generate_paths(src_entities, dst_entities, kg: KnowledgeGraphBase, max_path=50):\n    \"\"\"Generate paths from question entities to answer entities.\n    \"\"\"\n    paths = []\n    for src in src_entities:\n        for dst in dst_entities:\n            if len(paths) >= max_path:\n                break\n            one_hop_paths = kg.search_one_hop_relations(src, dst)\n            paths.extend(one_hop_paths)\n            # If there are alreay one-hop paths between the two entities, \n            # we don't need to look further.\n            if len(one_hop_paths) == 0:\n                paths.extend(kg.search_two_hop_relations(src, dst))\n    paths = [tuple(path) for path in paths]\n    paths = list(set(paths))\n    return paths[:max_path]", "\n\ndef generate_paths(src_entities, dst_entities, kg: KnowledgeGraphBase, max_path=50):\n    \"\"\"Generate paths from question entities to answer entities.\n    \"\"\"\n    paths = []\n    for src in src_entities:\n        for dst in dst_entities:\n            if len(paths) >= max_path:\n                break\n            one_hop_paths = kg.search_one_hop_relations(src, dst)\n            paths.extend(one_hop_paths)\n            # If there are alreay one-hop paths between the two entities, \n            # we don't need to look further.\n            if len(one_hop_paths) == 0:\n                paths.extend(kg.search_two_hop_relations(src, dst))\n    paths = [tuple(path) for path in paths]\n    paths = list(set(paths))\n    return paths[:max_path]", "\n\ndef has_type_relation(path):\n    \"\"\"A utility function to check whether the path contain certain relations.\"\"\"\n    for rel in path:\n        if rel in ('type.object.type', 'type.type.instance'):\n            return False\n    return True\n\n\ndef main(args):\n    # Each ground sample has the following fields:\n    # - id: sample id\n    # - question: question text\n    # - question_entities: list of question entities\n    # - answer_entities: list of answer entities\n    ground_samples = srsly.read_jsonl(args.ground_path)\n    total_samples = sum(1 for _ in srsly.read_jsonl(args.ground_path))\n    kg = get_knowledge_graph(args.knowledge_graph, args.sparql_endpoint)\n    processed_samples = []\n    skipped = 0\n    for sample in tqdm(ground_samples, total=total_samples, desc='Searching paths'):\n        question_entities = sample['question_entities']\n        answer_entities = sample['answer_entities']\n        try:\n            paths = generate_paths(question_entities, answer_entities, kg)\n        except Exception as e:\n            skipped += 1\n            print(e)\n            continue\n        if args.remove_sample_without_path and not paths:\n            skipped += 1\n            continue\n        # Special filter for Freebase\n        if args.knowledge_graph == 'freebase':\n            paths = list(filter(has_type_relation, paths))\n        sample['paths'] = paths\n        processed_samples.append(sample)\n    print(f'Processed {len(processed_samples)} samples; skipped {skipped} samples without any paths between question entities and answer entities; total {total_samples} samples')\n    srsly.write_jsonl(args.output_path, processed_samples)\n    print(f'Retrieved paths saved to {args.output_path}')", "\n\ndef main(args):\n    # Each ground sample has the following fields:\n    # - id: sample id\n    # - question: question text\n    # - question_entities: list of question entities\n    # - answer_entities: list of answer entities\n    ground_samples = srsly.read_jsonl(args.ground_path)\n    total_samples = sum(1 for _ in srsly.read_jsonl(args.ground_path))\n    kg = get_knowledge_graph(args.knowledge_graph, args.sparql_endpoint)\n    processed_samples = []\n    skipped = 0\n    for sample in tqdm(ground_samples, total=total_samples, desc='Searching paths'):\n        question_entities = sample['question_entities']\n        answer_entities = sample['answer_entities']\n        try:\n            paths = generate_paths(question_entities, answer_entities, kg)\n        except Exception as e:\n            skipped += 1\n            print(e)\n            continue\n        if args.remove_sample_without_path and not paths:\n            skipped += 1\n            continue\n        # Special filter for Freebase\n        if args.knowledge_graph == 'freebase':\n            paths = list(filter(has_type_relation, paths))\n        sample['paths'] = paths\n        processed_samples.append(sample)\n    print(f'Processed {len(processed_samples)} samples; skipped {skipped} samples without any paths between question entities and answer entities; total {total_samples} samples')\n    srsly.write_jsonl(args.output_path, processed_samples)\n    print(f'Retrieved paths saved to {args.output_path}')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--sparql-endpoint', default='http://localhost:1234/api/endpoint/sparql', help='knowledge graph SPARQL endpoint')\n    parser.add_argument('--knowledge-graph', type=str, default='wikidata', choices=('wikidata', 'freebase', 'dbpedia'), help='knowledge graph name (default: wikidata)')\n    parser.add_argument('--ground-path', type=str, required=True, help='grounded file where the question and answer entities are stored')\n    parser.add_argument('--output-path', type=str, required=True, help='path file where several paths for each sample stored')\n    parser.add_argument('--remove-sample-without-path', action='store_true', help='remove samples without paths')\n    args = parser.parse_args()\n\n    main(args)", ""]}
{"filename": "src/srtk/preprocessing/load_dataset.py", "chunked_list": ["\"\"\"1. Load the dataset\nThis scripts provides an example of how to prepare the dataset. It filters the grounded samples,\nremoving those without any answer or question entity.\n\nThe example input contains the following fields:\n- id: the sample id\n- sent: the question\n- qc: the question entities\n- ac: the answer entities\n", "- ac: the answer entities\n\nExample usage:\n\n- For Mintaka dataset, those without any answer entity are removed.\n$ python preprocess/load_dataset.py --dataset mintaka --ground-path data/preprocess/mintaka-ground-raw.jsonl --output-path data/preprocess/mintaka-ground.jsonl\nTrain + Validation + Test: Processed 12188 samples, skipped 7812 samples, total 20000 samples\n\n- For MKQA dataset, those without any question entity are removed.\n$ python preprocess/load_dataset.py --dataset mkqa --ground-path data/preprocess/mkqa-ground-raw.jsonl --output-path data/preprocess/mkqa-ground.jsonl", "- For MKQA dataset, those without any question entity are removed.\n$ python preprocess/load_dataset.py --dataset mkqa --ground-path data/preprocess/mkqa-ground-raw.jsonl --output-path data/preprocess/mkqa-ground.jsonl\nProcessed 2112 samples, skipped 7888 samples, total 10000 samples\n\"\"\"\nimport argparse\nfrom pathlib import Path\n\nimport srsly\nfrom tqdm import tqdm\n\ndef main(args):\n    samples= srsly.read_jsonl(args.ground_path)\n    total_lines = sum(1 for _ in srsly.read_jsonl(args.ground_path))\n    skipped = 0\n    processed_samples = []\n    for sample in tqdm(samples, total=total_lines):\n        if len(sample['qc']) == 0 or len(sample['ac']) == 0 or None in sample['ac']:\n            skipped += 1\n            continue\n        processed_sample = {\n            'id': args.dataset + '_' + str(sample['id']),\n            'question': sample['sent'],\n            'question_entities': sample['qc'],\n            'answer_entities': sample['ac'],\n        }\n        processed_samples.append(processed_sample)\n\n    output_path = Path(args.output_path)\n    if not output_path.parent.exists():\n        output_path.parent.mkdir(parents=True)\n    srsly.write_jsonl(output_path, processed_samples)\n    print(f'Processed {len(processed_samples)} samples, skipped {skipped} samples, total {total_lines} samples')\n    print(f'Output saved to {output_path}')", "from tqdm import tqdm\n\ndef main(args):\n    samples= srsly.read_jsonl(args.ground_path)\n    total_lines = sum(1 for _ in srsly.read_jsonl(args.ground_path))\n    skipped = 0\n    processed_samples = []\n    for sample in tqdm(samples, total=total_lines):\n        if len(sample['qc']) == 0 or len(sample['ac']) == 0 or None in sample['ac']:\n            skipped += 1\n            continue\n        processed_sample = {\n            'id': args.dataset + '_' + str(sample['id']),\n            'question': sample['sent'],\n            'question_entities': sample['qc'],\n            'answer_entities': sample['ac'],\n        }\n        processed_samples.append(processed_sample)\n\n    output_path = Path(args.output_path)\n    if not output_path.parent.exists():\n        output_path.parent.mkdir(parents=True)\n    srsly.write_jsonl(output_path, processed_samples)\n    print(f'Processed {len(processed_samples)} samples, skipped {skipped} samples, total {total_lines} samples')\n    print(f'Output saved to {output_path}')", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default='', help='dataset name, which will be prepend to sample ids')\n    parser.add_argument('--ground-path', type=str, required=True)\n    parser.add_argument('--output-path', type=str, required=True)\n    args = parser.parse_args()\n    main(args)\n", ""]}
{"filename": "src/srtk/entity_linking/linker_base.py", "chunked_list": ["from abc import abstractmethod\n\n\nclass LinkerBase:\n    \"\"\"Base class for entity linking\"\"\"\n\n    @abstractmethod\n    def annotate(self, text: str, **kwargs):\n        \"\"\"Annotate a text with the entities in the knowledge graph\n\n        The returned dictionary should at least have the following fields:\n        - question: The input text\n        - question_entities: The entity ids of the entities in the text\n\n        Args:\n            text (str): The text to annotate\n            kwargs: Extra arguments\n\n        Returns:\n            dictionary: The annotated text with linked entities\n        \"\"\"", ""]}
{"filename": "src/srtk/entity_linking/freebase.py", "chunked_list": [""]}
{"filename": "src/srtk/entity_linking/dbpedia.py", "chunked_list": ["\"\"\"\nLink entitiy mentions to DBpedia entities with the DBpedia Spotlight endpoint.\n\nKnow more about DBpedia Spotlight at https://www.dbpedia-spotlight.org/.\n\"\"\"\nimport requests\n\nfrom .linker_base import LinkerBase\n\nclass DBpediaLinker(LinkerBase):\n    \"\"\"Link entitiy mentions to DBpedia entities with the DBpedia Spotlight endpoint\"\"\"\n    def __init__(self, endpoint):\n        \"\"\"Initialize the linker\n        \n        Args:\n            endpoint (str): The endpoint of the DBpedia Spotlight service\n                e.g. https://api.dbpedia-spotlight.org/en/annotate\n        \"\"\"\n        self.endpoint = endpoint\n\n    def annotate(self, text, **kwargs):\n        \"\"\"Annotate a text with the entities in the DBpedia knowledge graph\n\n        Args:\n            text (str): The text to annotate\n\n        Returns:\n            dict: A dictionary with the following keys:\n                question: The input text\n                question_entities: The DBpedia entities in the text\n                spans: The spans of the entities in the text\n                similarity_scores: The similarity scores of the entities in the text\n        \"\"\"\n        params = {'text': text}\n        headers = {'Accept': 'application/json'}\n        response = requests.get(self.endpoint, params=params, headers=headers,\n                                timeout=60).json()\n        resources = response['Resources']\n        question_entities = []\n        spans = []\n        similarity_scores = []\n        for resource in resources:\n            uri = resource['@URI']\n            if not uri.startswith('http://dbpedia.org/resource/'):\n                continue\n            entity = uri[len('http://dbpedia.org/resource/'):]\n            offset = int(resource['@offset'])\n            surface_form = resource['@surfaceForm']\n            similarity_score = float(resource['@similarityScore'])\n            span = (offset, offset + len(surface_form))\n            question_entities.append(entity)\n            spans.append(span)\n            similarity_scores.append(similarity_score)\n        linked = {\n            \"question\": text,\n            \"question_entities\": question_entities,\n            \"spans\": spans,\n            \"similarity_scores\": similarity_scores,\n        }\n        return linked", "\nclass DBpediaLinker(LinkerBase):\n    \"\"\"Link entitiy mentions to DBpedia entities with the DBpedia Spotlight endpoint\"\"\"\n    def __init__(self, endpoint):\n        \"\"\"Initialize the linker\n        \n        Args:\n            endpoint (str): The endpoint of the DBpedia Spotlight service\n                e.g. https://api.dbpedia-spotlight.org/en/annotate\n        \"\"\"\n        self.endpoint = endpoint\n\n    def annotate(self, text, **kwargs):\n        \"\"\"Annotate a text with the entities in the DBpedia knowledge graph\n\n        Args:\n            text (str): The text to annotate\n\n        Returns:\n            dict: A dictionary with the following keys:\n                question: The input text\n                question_entities: The DBpedia entities in the text\n                spans: The spans of the entities in the text\n                similarity_scores: The similarity scores of the entities in the text\n        \"\"\"\n        params = {'text': text}\n        headers = {'Accept': 'application/json'}\n        response = requests.get(self.endpoint, params=params, headers=headers,\n                                timeout=60).json()\n        resources = response['Resources']\n        question_entities = []\n        spans = []\n        similarity_scores = []\n        for resource in resources:\n            uri = resource['@URI']\n            if not uri.startswith('http://dbpedia.org/resource/'):\n                continue\n            entity = uri[len('http://dbpedia.org/resource/'):]\n            offset = int(resource['@offset'])\n            surface_form = resource['@surfaceForm']\n            similarity_score = float(resource['@similarityScore'])\n            span = (offset, offset + len(surface_form))\n            question_entities.append(entity)\n            spans.append(span)\n            similarity_scores.append(similarity_score)\n        linked = {\n            \"question\": text,\n            \"question_entities\": question_entities,\n            \"spans\": spans,\n            \"similarity_scores\": similarity_scores,\n        }\n        return linked", ""]}
{"filename": "src/srtk/entity_linking/__init__.py", "chunked_list": ["from .wikidata import WikidataLinker\nfrom .dbpedia import DBpediaLinker\n"]}
{"filename": "src/srtk/entity_linking/wikidata.py", "chunked_list": ["import json\nimport logging\nimport requests\n\nfrom wikimapper import WikiMapper\n\nfrom .linker_base import LinkerBase\n\n\nclass WikidataLinker(LinkerBase):\n    \"\"\"Link entitiy mentions to Wikidata entities using the REL endpoint\"\"\"\n    def __init__(self, endpoint, wikimapper_db, service='rel'):\n        \"\"\"Initialize the linker\n\n        Args:\n            endpoint (str): The endpoint of the REL service\n            wikimapper_db (str): The path to the Wikimapper database\n        \"\"\"\n        self.endpoint = endpoint\n        self.mapper = WikiMapper(wikimapper_db)\n        self.service = service\n\n    def annotate_rel(self, text, token=None):\n        \"\"\"Annotate using a local REL service.\n        Check https://rel.readthedocs.io/en/latest/tutorials/e2e_entity_linking for setup instructions.\n\n        Args:\n            text (str): The text to annotate\n\n        Returns:\n            dict: annotation results\n        \"\"\"\n        document = {\n            'text': text,\n        }\n        headers = {\n            # 'Content-Type': 'application/json'\n        }\n        if token is not None:\n            headers['gcube-token'] = token\n        api_results = requests.post(self.endpoint, json=document, timeout=60,\n                                    headers=headers)\n        if api_results.status_code != 200:\n            logging.error(f\"Error in REL service: {api_results.text}\")\n            decoded = []\n        else:\n            decoded = api_results.json()\n\n        qids = []\n        spans = []\n        entities = []\n        not_converted_entities = []\n        for result in decoded:\n            start_pos, mention_length, mention, entity, disambiguation_cofidence, mention_detection_confidence, tag = result\n            qid = self.mapper.title_to_id(entity)\n            span = (start_pos, start_pos + mention_length)\n            if qid is None:\n                not_converted_entities.append(entity)\n            else:\n                qids.append(qid)\n                entities.append(entity)\n                spans.append(span)\n        linked = {\n            \"question\": text,\n            \"question_entities\": qids,\n            \"spans\": spans,\n            \"entity_names\": entities,\n            \"not_converted_entities\": not_converted_entities,\n        }\n        return linked\n\n    def annotate_tagme_wat(self, text, token):\n        \"\"\"Annotate using WAT or REL online services\n\n        Args:\n            text (str): The text to annotate\n            token (str): The token to access the service\n\n        Returns:\n            dict: annotation results\n        \"\"\"\n        params = {\n            'gcube-token': token,\n            'text': text\n        }\n        response = requests.get(self.endpoint, params=params, timeout=60)\n        # Parse the JSON response\n        data = json.loads(response.text)\n        qids = []\n        spans = []\n        entity_names = []\n        not_converted_entities = []\n        if \"annotations\" in data:\n            for annotation in data[\"annotations\"]:\n                title = annotation[\"title\"]\n                qid = self.mapper.title_to_id(title)\n                if qid is None:\n                    not_converted_entities.append(title)\n                else:\n                    qids.append(qid)\n                    spans.append((annotation[\"start\"], annotation[\"end\"]))\n                    entity_names.append(title)\n        linked = {\n            \"question\": text,\n            \"question_entities\": qids,\n            \"spans\": spans,\n            \"entity_names\": entity_names,\n            \"not_converted_entities\": not_converted_entities,\n        }\n        return linked\n\n    def annotate(self, text, **kwargs):\n        \"\"\"Annotate a text with the entities in the Wikidata knowledge graph\n\n        Args:\n            text (str): The text to annotate\n\n        Returns:\n            dict: A dictionary with the following keys:\n                question: The input text\n                question_entities: The Wikidata ids of the entities in the text\n                spans: The spans of the entities in the text\n                entity_names: The names of the entities in the text\n                not_converted_entities: The entities that are not converted to Wikidata ids\n        \"\"\"\n        token = kwargs.get(\"token\", None)\n        if self.service in ['tagme', 'wat']:\n            if token is None:\n                raise ValueError(f\"The {self.service} service requires a token\")\n            return self.annotate_tagme_wat(text, token)\n\n        if self.service == 'rel':\n            return self.annotate_rel(text, token)\n\n        raise NotImplementedError(f\"Service {self.service} is not implemented\")", "\nclass WikidataLinker(LinkerBase):\n    \"\"\"Link entitiy mentions to Wikidata entities using the REL endpoint\"\"\"\n    def __init__(self, endpoint, wikimapper_db, service='rel'):\n        \"\"\"Initialize the linker\n\n        Args:\n            endpoint (str): The endpoint of the REL service\n            wikimapper_db (str): The path to the Wikimapper database\n        \"\"\"\n        self.endpoint = endpoint\n        self.mapper = WikiMapper(wikimapper_db)\n        self.service = service\n\n    def annotate_rel(self, text, token=None):\n        \"\"\"Annotate using a local REL service.\n        Check https://rel.readthedocs.io/en/latest/tutorials/e2e_entity_linking for setup instructions.\n\n        Args:\n            text (str): The text to annotate\n\n        Returns:\n            dict: annotation results\n        \"\"\"\n        document = {\n            'text': text,\n        }\n        headers = {\n            # 'Content-Type': 'application/json'\n        }\n        if token is not None:\n            headers['gcube-token'] = token\n        api_results = requests.post(self.endpoint, json=document, timeout=60,\n                                    headers=headers)\n        if api_results.status_code != 200:\n            logging.error(f\"Error in REL service: {api_results.text}\")\n            decoded = []\n        else:\n            decoded = api_results.json()\n\n        qids = []\n        spans = []\n        entities = []\n        not_converted_entities = []\n        for result in decoded:\n            start_pos, mention_length, mention, entity, disambiguation_cofidence, mention_detection_confidence, tag = result\n            qid = self.mapper.title_to_id(entity)\n            span = (start_pos, start_pos + mention_length)\n            if qid is None:\n                not_converted_entities.append(entity)\n            else:\n                qids.append(qid)\n                entities.append(entity)\n                spans.append(span)\n        linked = {\n            \"question\": text,\n            \"question_entities\": qids,\n            \"spans\": spans,\n            \"entity_names\": entities,\n            \"not_converted_entities\": not_converted_entities,\n        }\n        return linked\n\n    def annotate_tagme_wat(self, text, token):\n        \"\"\"Annotate using WAT or REL online services\n\n        Args:\n            text (str): The text to annotate\n            token (str): The token to access the service\n\n        Returns:\n            dict: annotation results\n        \"\"\"\n        params = {\n            'gcube-token': token,\n            'text': text\n        }\n        response = requests.get(self.endpoint, params=params, timeout=60)\n        # Parse the JSON response\n        data = json.loads(response.text)\n        qids = []\n        spans = []\n        entity_names = []\n        not_converted_entities = []\n        if \"annotations\" in data:\n            for annotation in data[\"annotations\"]:\n                title = annotation[\"title\"]\n                qid = self.mapper.title_to_id(title)\n                if qid is None:\n                    not_converted_entities.append(title)\n                else:\n                    qids.append(qid)\n                    spans.append((annotation[\"start\"], annotation[\"end\"]))\n                    entity_names.append(title)\n        linked = {\n            \"question\": text,\n            \"question_entities\": qids,\n            \"spans\": spans,\n            \"entity_names\": entity_names,\n            \"not_converted_entities\": not_converted_entities,\n        }\n        return linked\n\n    def annotate(self, text, **kwargs):\n        \"\"\"Annotate a text with the entities in the Wikidata knowledge graph\n\n        Args:\n            text (str): The text to annotate\n\n        Returns:\n            dict: A dictionary with the following keys:\n                question: The input text\n                question_entities: The Wikidata ids of the entities in the text\n                spans: The spans of the entities in the text\n                entity_names: The names of the entities in the text\n                not_converted_entities: The entities that are not converted to Wikidata ids\n        \"\"\"\n        token = kwargs.get(\"token\", None)\n        if self.service in ['tagme', 'wat']:\n            if token is None:\n                raise ValueError(f\"The {self.service} service requires a token\")\n            return self.annotate_tagme_wat(text, token)\n\n        if self.service == 'rel':\n            return self.annotate_rel(text, token)\n\n        raise NotImplementedError(f\"Service {self.service} is not implemented\")", "\n\n# if __name__ == '__main__':\n#     linker = WikidataLinker('http://localhost:5000/rel', 'data/index_enwiki-latest-uncased.db')\n#     text = \"The city of [[Amsterdam]] is the capital of [[Netherlands]].\"\n#     print(linker.annotate(text))"]}
{"filename": "src/srtk/knowledge_graph/freebase.py", "chunked_list": ["from functools import lru_cache\nfrom SPARQLWrapper import SPARQLWrapper, JSON\n\nfrom .graph_base import KnowledgeGraphBase\n\n\nclass Freebase(KnowledgeGraphBase):\n    PREFIXES: str = \"\"\"\n        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n        PREFIX ns: <http://rdf.freebase.com/ns/>\n        \"\"\"\n\n    def __init__(self, endpoint, prepend_prefixes=True) -> None:\n        self.sparql = SPARQLWrapper(endpoint)\n        self.sparql.setReturnFormat(JSON)\n        self.prepend_prefixes = prepend_prefixes\n        self.name = 'freebase'\n\n    def queryFreebase(self, query):\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n\n        self.sparql.setQuery(query)\n        try:\n            ret = self.sparql.queryAndConvert()\n            result = ret['results']['bindings']\n        except Exception as exeption:\n            print(exeption)\n            print(f'Failed executing query: {query}')\n            result = []\n        return result\n\n    @staticmethod\n    def get_id_from_uri(uri):\n        \"\"\"Get id from uri.\"\"\"\n        return uri.split('/')[-1]\n\n    @lru_cache\n    def search_one_hop_relations(self, src, dst):\n        \"\"\"Search one hop relation between src and dst.\n        \n        Args:\n            src (str): source entity\n            dst (str): destination entity\n        \n        Returns:\n            list[list[str]]: list of paths, each path is a list of PIDs\n        \"\"\"\n        query = f\"\"\"\n            SELECT distinct ?r1 where {{\n                ns:{src} ?r1_ ns:{dst} . \n                FILTER REGEX(?r1_, \"http://rdf.freebase.com/ns/\")\n                BIND(STRAFTER(STR(?r1_),str(ns:)) AS ?r1)\n            }}\n        \"\"\"\n        paths = self.queryFreebase(query)\n        paths = [[path['r1']['value']] for path in paths]\n        return paths\n\n    @lru_cache\n    def search_two_hop_relations(self, src, dst):\n        query = f\"\"\"\n            SELECT distinct ?r1 ?r2 where {{\n                ns:{src} ?r1_ ?e1 . \n                ?e1 ?r2_ ns:{dst} .\n                FILTER REGEX(?e1, \"http://rdf.freebase.com/ns/\")\n                FILTER REGEX(?r1_, \"http://rdf.freebase.com/ns/\")\n                FILTER REGEX(?r2_, \"http://rdf.freebase.com/ns/\")\n                FILTER (?r1_ != <http://rdf.freebase.com/ns/type.object.type>)\n                FILTER (?r2_ != <http://rdf.freebase.com/ns/type.object.type>)\n                BIND(STRAFTER(STR(?r1_),str(ns:)) AS ?r1)\n                BIND(STRAFTER(STR(?r2_),str(ns:)) AS ?r2)\n            }}\n        \"\"\"\n        paths = self.queryFreebase(query)\n        paths = [[path['r1']['value'], path['r2']['value']] for path in paths]\n        return paths\n\n    @lru_cache\n    def deduce_leaves(self, src, path, limit=2000):\n        \"\"\"Deduce leave entities from source entity following the path.\n        \n        Args:\n            src_entity (str): source entity\n            path (tuple[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves. Defaults to 2000.\n        \n        Returns:\n            list[str]: list of leaves. Each leaf is a QID.\n        \"\"\"\n        if len(path) >= 3:\n            raise NotImplementedError(f'Currenly only support paths with length less than 3, got {len(path)}')\n        if len(path) == 0:\n            return [src]\n        if len(path) == 1:\n            query = f\"\"\"\n                SELECT DISTINCT ?leaf WHERE {{\n                    ns:{src} ns:{path[0]} ?t0_ .\n                    FILTER REGEX(?t0_, \"http://rdf.freebase.com/ns/\")\n                    BIND(STRAFTER(STR(?t0_),STR(ns:)) AS ?leaf)\n                }} LIMIT {limit}\n                \"\"\"\n        else: # len(path) == 2:\n            query = f\"\"\"\n                SELECT DISTINCT ?leaf WHERE {{\n                    ns:{src} ns:{path[0]} ?e1_ . \n                    ?e1_ ns:{path[1]} ?e2_ .\n                    FILTER REGEX(?e1_, \"http://rdf.freebase.com/ns/\")\n                    FILTER REGEX(?e2_, \"http://rdf.freebase.com/ns/\")\n                    BIND(STRAFTER(STR(?e2_),str(ns:)) AS ?leaf)\n                }} LIMIT {limit}\n                \"\"\"\n        results = self.queryFreebase(query)\n        return [i['leaf']['value'] for i in results]\n\n    def deduce_leaves_from_multiple_srcs(self, srcs, path, limit=2000):\n        \"\"\"Deuce leave entities from multiple source entities following the path.\n\n        Args:\n            srcs (list[str]): list of source entities\n            path (list[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves. Defaults to 200.\n\n        Returns:\n            list[str]: list of leaves. Each leaf is a QID.\n        \"\"\"\n        if len(path) >= 2:\n            raise NotImplementedError(f'Currenly only support paths with length less than 2, got {len(path)}')\n        if len(path) == 0:\n            return srcs\n        query = f\"\"\"\n            SELECT DISTINCT ?leaf WHERE {{\n                VALUES ?src {{ns:{' ns:'.join(srcs)}}}\n                ?src ns:{path[0]} ?t0_ .\n                FILTER REGEX(?t0_, \"http://rdf.freebase.com/ns/\")\n                BIND(STRAFTER(STR(?t0_),STR(ns:)) AS ?leaf)\n            }} LIMIT {limit}\n            \"\"\"\n        results = self.queryFreebase(query)\n        return [i['leaf']['value'] for i in results]\n\n    @lru_cache\n    def get_neighbor_relations(self, src, hop=1, limit=100):\n        \"\"\"Get all relations connected to an entity. The relations are\n        limited to direct relations (those with wdt: prefix).\n\n        Args:\n            src (str): source entity\n            hop (int, optional): hop of the relations. Defaults to 1.\n            limit (int, optional): limit of the number of relations. Defaults to 100.\n\n        Returns:\n            list[str] | list[tuple(str,)]: list of relations. Each relation is a PID or a tuple of PIDs.\n        \"\"\"\n        if hop >= 3:\n            raise NotImplementedError(f'Currenly only support relations with hop less than 3, got {hop}')\n        if hop == 1:\n            query = f\"\"\"\n                SELECT DISTINCT ?rel WHERE {{\n                    ns:{src} ?r0_ ?t0 .\n                    FILTER REGEX(?r0_, \"http://rdf.freebase.com/ns/\")\n                    FILTER REGEX(?t0, \"http://rdf.freebase.com/ns/\")\n                    FILTER (?r0_ != <http://rdf.freebase.com/ns/type.object.type>)\n                    BIND(STRAFTER(STR(?r0_),STR(ns:)) AS ?rel)\n                }} LIMIT {limit}\n                \"\"\"\n        elif hop == 2:\n            query = f\"\"\"\n                SELECT DISTINCT ?rel0, ?rel1 WHERE {{     \n                    {{\n                        SELECT DISTINCT ?t0, ?rel0 WHERE {{\n                        ns:{src} ?r0_ ?t0 .\n                        FILTER REGEX(?r0_, \"http://rdf.freebase.com/ns/\")\n                        FILTER REGEX(?t0, \"http://rdf.freebase.com/ns/\")\n                        FILTER (?r0_ != <http://rdf.freebase.com/ns/type.object.type>)\n                        BIND(STRAFTER(STR(?r0_),STR(ns:)) AS ?rel0)\n                        }} LIMIT 10\n                    }}\n                    ?t0 ?r1_ ?t1 .\n                    FILTER REGEX(?r1_, \"http://rdf.freebase.com/ns/\")\n                    FILTER REGEX(?t1, \"http://rdf.freebase.com/ns/\")\n                    FILTER (?r1_ != <http://rdf.freebase.com/ns/type.object.type>)\n                    BIND(STRAFTER(STR(?r1_),STR(ns:)) AS ?rel1)\n                }} LIMIT {limit}\n                \"\"\"\n        results = self.queryFreebase(query)\n        if hop == 1:\n            paths = [path['rel']['value'] for path in results]\n        else:\n            paths = [(path['rel0']['value'], path['rel1']['value'])\n                     for path in results]\n        return paths\n\n    @lru_cache\n    def get_label(self, identifier):\n        query = f\"\"\"\n            SELECT ?label\n            WHERE {{\n                ns:{identifier} rdfs:label ?label .\n                FILTER (langMatches(lang(?label), \"EN\"))\n            }} LIMIT 1\n            \"\"\"\n        results = self.queryFreebase(query)\n        return results[0]['label']['value'] if results else None\n\n    def get_relation_label(self, relation):\n        \"\"\"For freebase, relation label is the same as the relation identifier.\"\"\"\n        return relation", ""]}
{"filename": "src/srtk/knowledge_graph/graph_base.py", "chunked_list": ["'''Provide protocal for different kinds of knowledge graphs.'''\n\nfrom abc import abstractmethod\nfrom typing import List\n\n\nclass KnowledgeGraphBase:\n    \"\"\"Knowledge graph base class.\"\"\"\n\n    @abstractmethod\n    def search_one_hop_relations(self, src: str, dst: str) -> List[List[str]]:\n        \"\"\"Search one hop relations between src and dst.\n\n        Args:\n            src (str): source entity\n            dst (str): destination entity\n\n        Returns:\n            list[list[str]]: list of paths, each path is a list of PIDs\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def search_two_hop_relations(self, src: str, dst: str) -> List[List[str]]:\n        \"\"\"Search two hop relations between src and dst.\n\n        Args:\n            src (str): source entity\n            dst (str): destination entity\n\n        Returns:\n            list[list[str]]: list of paths, each path is a list of PIDs\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def deduce_leaves(self, src: str, path: List[str], limit: int) -> List[str]:\n        \"\"\"Deduce leave entities from source entity following the path.\n\n        Args:\n            src_entity (str): source entity\n            path (tuple[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves.\n\n        Returns:\n            list[str]: list of leaves. Each leaf is a QID.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_neighbor_relations(self, src: str, hop: int, limit: int) -> List[str]:\n        \"\"\"Get n-hop neighbor relations of src.\n\n        Args:\n            src (str): source entity\n            hop (int, optional): hop of the relations. Defaults to 1.\n            limit (int, optional): limit of the number of relations.\n\n        Returns:\n            list[str] | list[tuple[str]]: list of relations (one-hop)\n                or list of tuples of relations (multi-hop)\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_label(self, identifier: str) -> str:\n        \"\"\"Get label of an entity or a relation.\n\n        Args:\n            identifier (str): entity or relation identifier\n\n        Returns:\n            str: label of the entity or the relation\n        \"\"\"\n        raise NotImplementedError\n\n    def get_relation_label(self, relation: str) -> str:\n        \"\"\"Get the label of a relation. Defaults to get_label.\n\n        Args:\n            relation (str): relation identifier\n\n        Returns:\n            str: label of the relation\n        \"\"\"\n        return self.get_label(relation)\n\n    def get_entity_label(self, entity: str) -> str:\n        \"\"\"Get the label of an entity. Defaults to get_label.\n\n        Args:\n            entity (str): entity identifier\n\n        Returns:\n            str: label of the entity\n        \"\"\"\n        return self.get_label(entity)", ""]}
{"filename": "src/srtk/knowledge_graph/dbpedia.py", "chunked_list": ["from functools import lru_cache\nfrom typing import List\n\nfrom SPARQLWrapper import SPARQLWrapper, JSON\n\nfrom .graph_base import KnowledgeGraphBase\n\n\nclass DBpedia(KnowledgeGraphBase):\n    PREFIXES: str = \"\"\"PREFIX dbo: <http://dbpedia.org/ontology/>\n                       PREFIX dbr: <http://dbpedia.org/resource/>\n                       \"\"\"\n\n    def __init__(self, endpoint, prepend_prefixes=False):\n        self.sparql = SPARQLWrapper(endpoint)\n        self.sparql.setReturnFormat(JSON)\n        self.prepend_prefixes = prepend_prefixes\n        self.name = 'dbpedia'\n\n    def queryDBPedia(self, query):\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n\n        self.sparql.setQuery(query)\n        try:\n            ret = self.sparql.queryAndConvert()\n            result = ret['results']['bindings']\n        except Exception as exeption:\n            print(f'Failed executing query: {query}')\n            print(f'Exception: {exeption}')\n            result = []\n        return result\n\n    def get_id_from_uri(self, uri):\n        return uri.split('/')[-1]\n\n    @lru_cache\n    def search_one_hop_relations(self, src, dst):\n        query = f\"\"\"\n                SELECT DISTINCT ?r WHERE {{\n                    dbr:{src} ?r dbr:{dst}.\n                    FILTER regex(str(?r), \"^http://dbpedia.org/ontology/\")\n                    FILTER (?r != dbo:wikiPageWikiLink)\n                }}\n                \"\"\"\n        paths = self.queryDBPedia(query)\n        # Keep only identifiers in the paths\n        paths = [[self.get_id_from_uri(path['r']['value'])] for path in paths]\n        return paths\n\n    @lru_cache\n    def search_two_hop_relations(self, src: str, dst: str) -> List[List[str]]:\n        \"\"\"Search two hop relations between src and dst.\n\n        Args:\n            src (str): source entity\n            dst (str): destination entity\n\n        Returns:\n            list[list[str]]: list of paths, each path is a list of IDs\n        \"\"\"\n        query = f\"\"\"\n                SELECT DISTINCT ?r1 ?r2 WHERE {{\n                    dbr:{src} ?r1 ?mid.\n                    ?mid ?r2 dbr:{dst}.\n                    FILTER regex(str(?r1), \"^http://dbpedia.org/ontology/\")\n                    FILTER regex(str(?r2), \"^http://dbpedia.org/ontology/\")\n                    FILTER (?r1 != dbo:wikiPageWikiLink)\n                    FILTER (?r2 != dbo:wikiPageWikiLink)\n                }}\n                \"\"\"\n        paths = self.queryDBPedia(query)\n        # Keep only identifiers in the paths\n        paths = [[self.get_id_from_uri(path['r1']['value']), self.get_id_from_uri(path['r2']['value'])] for path in paths]\n        return paths\n\n    def escape_entity(self, entity):\n        \"\"\"Escape brackets in the entity ID.\n\n        Args:\n            entity (str): entity identifier\n\n        Returns:\n            str: the bracket-escaped entity identifier\n        \"\"\"\n        # entity = entity.replace('(', r'\\(').replace(')', r'\\)')\n        entity = entity.replace('\"', r'\\\"')\n        entity = f\"<http://dbpedia.org/resource/{entity}>\"\n        return entity\n\n    def deduce_leaves(self, src: str, path: List[str], limit: int) -> List[str]:\n        \"\"\"Deduce leave entities from source entity following the path.\n\n        Args:\n            src_entity (str): source entity\n            path (tuple[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves.\n\n        Returns:\n            list[str]: list of leaves. Each leaf is a ID.\n        \"\"\"\n        if len(path) > 3:\n            raise NotImplementedError('Deduce leaves for paths longer than 3 is not implemented.')\n        \n        if len(path) == 0:\n            return [src]\n\n        src = self.escape_entity(src)\n        if len(path) == 1:\n            query = f\"\"\"\n                SELECT DISTINCT ?dst WHERE {{\n                    {src} dbo:{path[0]} ?dst.\n                    FILTER(STRSTARTS(str(?dst), \"http://dbpedia.org/resource/\"))\n                }}\n                LIMIT {limit}\n                \"\"\"\n        else:\n            query = f\"\"\"\n                SELECT DISTINCT ?dst WHERE {{\n                    {src} dbo:{path[0]} ?mid.\n                    ?mid dbo:{path[1]} ?dst.\n                    FILTER(STRSTARTS(str(?dst), \"http://dbpedia.org/resource/\"))\n                }}\n                LIMIT {limit}\n                \"\"\"\n        leaves = self.queryDBPedia(query)\n        leaves = [self.get_id_from_uri(leaf['dst']['value']) for leaf in leaves]\n        return leaves\n\n    def deduce_leaves_from_multiple_srcs(self, srcs, path, limit=2000):\n        \"\"\"Deuce leave entities from multiple source entities following the path.\n\n        Args:\n            srcs (list[str]): list of source entities\n            path (list[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves. Defaults to 200.\n\n        Returns:\n            list[str]: list of leaves. Each leaf is a QID.\n        \"\"\"\n        if len(path) >= 2:\n            raise NotImplementedError(f'Currenly only support paths with length less than 2, got {len(path)}')\n        if len(path) == 0:\n            return srcs\n        if len(srcs) == 0:\n            return []\n\n        srcs = [self.escape_entity(src) for src in srcs]\n        query = f\"\"\"\n            SELECT DISTINCT ?x WHERE {{\n                VALUES ?src {{ {' '.join(srcs)} }}\n                ?src dbo:{path[0]} ?x.\n                FILTER(STRSTARTS(str(?x), \"http://dbpedia.org/resource/\"))\n            }}\n            LIMIT {limit}\n            \"\"\"\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n        leaves = self.queryDBPedia(query)\n        # Keep only QIDs in the leaves\n        leaves = [leaf['x']['value'].split('/')[-1] for leaf in leaves]\n        return leaves\n\n    @lru_cache\n    def get_neighbor_relations(self, src, hop=1, limit=100):\n        \"\"\"Get all relations connected to an entity. The relations are\n        limited to direct relations (those with wdt: prefix).\n\n        Args:\n            src (str): source entity\n            hop (int, optional): hop of the relations. Defaults to 1.\n            limit (int, optional): limit of the number of relations. Defaults to 100.\n\n        Returns:\n            list[str] | list[tuple(str,)]: list of relations. Each relation is a PID or a tuple of PIDs.\n        \"\"\"\n        if hop > 2:\n            raise NotImplementedError('Get neighbor relations for hop larger than 2 is not implemented.')\n\n        if hop == 1:\n            query = f\"\"\"\n                SELECT DISTINCT ?r\n                WHERE {{\n                    dbr:Charles_III ?r ?neighbor .\n                    FILTER (STRSTARTS(STR(?r), \"http://dbpedia.org/ontology/\") && !STRSTARTS(STR(?r), \"http://dbpedia.org/ontology/wiki\"))\n                }}\n                LIMIT {limit}\n                \"\"\"\n        else:\n            query = f\"\"\"\n                SELECT DISTINCT ?r1 ?r2 WHERE {{\n                    dbr:{src} ?r1 ?mid.\n                    ?mid ?r2 ?dst.\n                    FILTER (STRSTARTS(STR(?r1), \"http://dbpedia.org/ontology/\") && !STRSTARTS(STR(?r1), \"http://dbpedia.org/ontology/wiki\"))\n                    FILTER (STRSTARTS(STR(?r2), \"http://dbpedia.org/ontology/\") && !STRSTARTS(STR(?r2), \"http://dbpedia.org/ontology/wiki\"))\n                }}\n                LIMIT {limit}\n                \"\"\"\n\n        relations = self.queryDBPedia(query)\n\n        if hop == 1:\n            relations = [self.get_id_from_uri(relation['r']['value'])\n                         for relation in relations]\n        else:\n            relations = [(self.get_id_from_uri(relation['r1']['value']),\n                          self.get_id_from_uri(relation['r2']['value']))\n                         for relation in relations]\n        return relations\n\n    @lru_cache\n    def get_label(self, identifier):\n        \"\"\"Get label of an entity or a relation. If no label is found, return None.\n\n        Args:\n            identifier (str): entity or relation, a QID or a PID\n\n        Returns:\n            str | None: label of the entity or relation\n        \"\"\"\n        query = f\"\"\"\n                SELECT (str(?label) AS ?name)\n                WHERE {{\n                dbr:{identifier} rdfs:label ?label .\n                FILTER (lang(?label) = \"en\")\n                }}\n                LIMIT 1\n                \"\"\"\n        labels = self.queryDBPedia(query)\n        if len(labels) == 0:\n            print(f'No label found for {identifier}')\n            return None\n        label = labels[0]['name']['value']\n        return label", "class DBpedia(KnowledgeGraphBase):\n    PREFIXES: str = \"\"\"PREFIX dbo: <http://dbpedia.org/ontology/>\n                       PREFIX dbr: <http://dbpedia.org/resource/>\n                       \"\"\"\n\n    def __init__(self, endpoint, prepend_prefixes=False):\n        self.sparql = SPARQLWrapper(endpoint)\n        self.sparql.setReturnFormat(JSON)\n        self.prepend_prefixes = prepend_prefixes\n        self.name = 'dbpedia'\n\n    def queryDBPedia(self, query):\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n\n        self.sparql.setQuery(query)\n        try:\n            ret = self.sparql.queryAndConvert()\n            result = ret['results']['bindings']\n        except Exception as exeption:\n            print(f'Failed executing query: {query}')\n            print(f'Exception: {exeption}')\n            result = []\n        return result\n\n    def get_id_from_uri(self, uri):\n        return uri.split('/')[-1]\n\n    @lru_cache\n    def search_one_hop_relations(self, src, dst):\n        query = f\"\"\"\n                SELECT DISTINCT ?r WHERE {{\n                    dbr:{src} ?r dbr:{dst}.\n                    FILTER regex(str(?r), \"^http://dbpedia.org/ontology/\")\n                    FILTER (?r != dbo:wikiPageWikiLink)\n                }}\n                \"\"\"\n        paths = self.queryDBPedia(query)\n        # Keep only identifiers in the paths\n        paths = [[self.get_id_from_uri(path['r']['value'])] for path in paths]\n        return paths\n\n    @lru_cache\n    def search_two_hop_relations(self, src: str, dst: str) -> List[List[str]]:\n        \"\"\"Search two hop relations between src and dst.\n\n        Args:\n            src (str): source entity\n            dst (str): destination entity\n\n        Returns:\n            list[list[str]]: list of paths, each path is a list of IDs\n        \"\"\"\n        query = f\"\"\"\n                SELECT DISTINCT ?r1 ?r2 WHERE {{\n                    dbr:{src} ?r1 ?mid.\n                    ?mid ?r2 dbr:{dst}.\n                    FILTER regex(str(?r1), \"^http://dbpedia.org/ontology/\")\n                    FILTER regex(str(?r2), \"^http://dbpedia.org/ontology/\")\n                    FILTER (?r1 != dbo:wikiPageWikiLink)\n                    FILTER (?r2 != dbo:wikiPageWikiLink)\n                }}\n                \"\"\"\n        paths = self.queryDBPedia(query)\n        # Keep only identifiers in the paths\n        paths = [[self.get_id_from_uri(path['r1']['value']), self.get_id_from_uri(path['r2']['value'])] for path in paths]\n        return paths\n\n    def escape_entity(self, entity):\n        \"\"\"Escape brackets in the entity ID.\n\n        Args:\n            entity (str): entity identifier\n\n        Returns:\n            str: the bracket-escaped entity identifier\n        \"\"\"\n        # entity = entity.replace('(', r'\\(').replace(')', r'\\)')\n        entity = entity.replace('\"', r'\\\"')\n        entity = f\"<http://dbpedia.org/resource/{entity}>\"\n        return entity\n\n    def deduce_leaves(self, src: str, path: List[str], limit: int) -> List[str]:\n        \"\"\"Deduce leave entities from source entity following the path.\n\n        Args:\n            src_entity (str): source entity\n            path (tuple[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves.\n\n        Returns:\n            list[str]: list of leaves. Each leaf is a ID.\n        \"\"\"\n        if len(path) > 3:\n            raise NotImplementedError('Deduce leaves for paths longer than 3 is not implemented.')\n        \n        if len(path) == 0:\n            return [src]\n\n        src = self.escape_entity(src)\n        if len(path) == 1:\n            query = f\"\"\"\n                SELECT DISTINCT ?dst WHERE {{\n                    {src} dbo:{path[0]} ?dst.\n                    FILTER(STRSTARTS(str(?dst), \"http://dbpedia.org/resource/\"))\n                }}\n                LIMIT {limit}\n                \"\"\"\n        else:\n            query = f\"\"\"\n                SELECT DISTINCT ?dst WHERE {{\n                    {src} dbo:{path[0]} ?mid.\n                    ?mid dbo:{path[1]} ?dst.\n                    FILTER(STRSTARTS(str(?dst), \"http://dbpedia.org/resource/\"))\n                }}\n                LIMIT {limit}\n                \"\"\"\n        leaves = self.queryDBPedia(query)\n        leaves = [self.get_id_from_uri(leaf['dst']['value']) for leaf in leaves]\n        return leaves\n\n    def deduce_leaves_from_multiple_srcs(self, srcs, path, limit=2000):\n        \"\"\"Deuce leave entities from multiple source entities following the path.\n\n        Args:\n            srcs (list[str]): list of source entities\n            path (list[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves. Defaults to 200.\n\n        Returns:\n            list[str]: list of leaves. Each leaf is a QID.\n        \"\"\"\n        if len(path) >= 2:\n            raise NotImplementedError(f'Currenly only support paths with length less than 2, got {len(path)}')\n        if len(path) == 0:\n            return srcs\n        if len(srcs) == 0:\n            return []\n\n        srcs = [self.escape_entity(src) for src in srcs]\n        query = f\"\"\"\n            SELECT DISTINCT ?x WHERE {{\n                VALUES ?src {{ {' '.join(srcs)} }}\n                ?src dbo:{path[0]} ?x.\n                FILTER(STRSTARTS(str(?x), \"http://dbpedia.org/resource/\"))\n            }}\n            LIMIT {limit}\n            \"\"\"\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n        leaves = self.queryDBPedia(query)\n        # Keep only QIDs in the leaves\n        leaves = [leaf['x']['value'].split('/')[-1] for leaf in leaves]\n        return leaves\n\n    @lru_cache\n    def get_neighbor_relations(self, src, hop=1, limit=100):\n        \"\"\"Get all relations connected to an entity. The relations are\n        limited to direct relations (those with wdt: prefix).\n\n        Args:\n            src (str): source entity\n            hop (int, optional): hop of the relations. Defaults to 1.\n            limit (int, optional): limit of the number of relations. Defaults to 100.\n\n        Returns:\n            list[str] | list[tuple(str,)]: list of relations. Each relation is a PID or a tuple of PIDs.\n        \"\"\"\n        if hop > 2:\n            raise NotImplementedError('Get neighbor relations for hop larger than 2 is not implemented.')\n\n        if hop == 1:\n            query = f\"\"\"\n                SELECT DISTINCT ?r\n                WHERE {{\n                    dbr:Charles_III ?r ?neighbor .\n                    FILTER (STRSTARTS(STR(?r), \"http://dbpedia.org/ontology/\") && !STRSTARTS(STR(?r), \"http://dbpedia.org/ontology/wiki\"))\n                }}\n                LIMIT {limit}\n                \"\"\"\n        else:\n            query = f\"\"\"\n                SELECT DISTINCT ?r1 ?r2 WHERE {{\n                    dbr:{src} ?r1 ?mid.\n                    ?mid ?r2 ?dst.\n                    FILTER (STRSTARTS(STR(?r1), \"http://dbpedia.org/ontology/\") && !STRSTARTS(STR(?r1), \"http://dbpedia.org/ontology/wiki\"))\n                    FILTER (STRSTARTS(STR(?r2), \"http://dbpedia.org/ontology/\") && !STRSTARTS(STR(?r2), \"http://dbpedia.org/ontology/wiki\"))\n                }}\n                LIMIT {limit}\n                \"\"\"\n\n        relations = self.queryDBPedia(query)\n\n        if hop == 1:\n            relations = [self.get_id_from_uri(relation['r']['value'])\n                         for relation in relations]\n        else:\n            relations = [(self.get_id_from_uri(relation['r1']['value']),\n                          self.get_id_from_uri(relation['r2']['value']))\n                         for relation in relations]\n        return relations\n\n    @lru_cache\n    def get_label(self, identifier):\n        \"\"\"Get label of an entity or a relation. If no label is found, return None.\n\n        Args:\n            identifier (str): entity or relation, a QID or a PID\n\n        Returns:\n            str | None: label of the entity or relation\n        \"\"\"\n        query = f\"\"\"\n                SELECT (str(?label) AS ?name)\n                WHERE {{\n                dbr:{identifier} rdfs:label ?label .\n                FILTER (lang(?label) = \"en\")\n                }}\n                LIMIT 1\n                \"\"\"\n        labels = self.queryDBPedia(query)\n        if len(labels) == 0:\n            print(f'No label found for {identifier}')\n            return None\n        label = labels[0]['name']['value']\n        return label", ""]}
{"filename": "src/srtk/knowledge_graph/__init__.py", "chunked_list": ["from .graph_base import KnowledgeGraphBase\nfrom .wikidata import Wikidata\nfrom .freebase import Freebase\nfrom .dbpedia import DBpedia\nfrom .utils import get_knowledge_graph"]}
{"filename": "src/srtk/knowledge_graph/utils.py", "chunked_list": ["from .wikidata import Wikidata\nfrom .freebase import Freebase\nfrom .dbpedia import DBpedia\n\n\ndef get_knowledge_graph(knowledge_graph_type, sparql_endpoint, prepend_prefixes=False,\n                        exclude_qualifiers=True):\n    \"\"\"Create a knowledge graph object.\n\n    Args:\n        knowledge_graph_type (str): Knowledge graph type. One of 'freebase', 'wikidata', 'dbpedia'.\n        sparql_endpoint (str): The SPARQL endpoint of the knowledge graph.\n        prepend_prefixes (bool): Whether to prepend prefixes to the SPARQL query. Defaults to False.\n        exclude_qualifiers (bool, optional): Whether to exclude qualifiers, only valid for Wikidata. Defaults to True.\n\n    Raises:\n        ValueError: If the knowledge graph type is not supported.\n\n    Returns:\n        KnowledgeGraphBase: A knowledge graph object.\n    \"\"\"\n    if knowledge_graph_type == 'freebase':\n        kg = Freebase(sparql_endpoint, prepend_prefixes=prepend_prefixes)\n    elif knowledge_graph_type == 'wikidata':\n        kg = Wikidata(sparql_endpoint, prepend_prefixes=prepend_prefixes, exclude_qualifiers=exclude_qualifiers)\n    elif knowledge_graph_type == 'dbpedia':\n        kg = DBpedia(sparql_endpoint, prepend_prefixes=prepend_prefixes)\n    else:\n        raise ValueError(f'Unknown knowledge graph type: {knowledge_graph_type}')\n    return kg"]}
{"filename": "src/srtk/knowledge_graph/wikidata.py", "chunked_list": ["from functools import lru_cache\nfrom SPARQLWrapper import SPARQLWrapper, JSON\n\nfrom .graph_base import KnowledgeGraphBase\n\n\nclass Wikidata(KnowledgeGraphBase):\n    PREFIXES: str = \"\"\"PREFIX wd: <http://www.wikidata.org/entity/>\n        PREFIX wds: <http://www.wikidata.org/entity/statement/>\n        PREFIX wdv: <http://www.wikidata.org/value/>\n        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n        PREFIX wikibase: <http://wikiba.se/ontology#>\n        PREFIX p: <http://www.wikidata.org/prop/>\n        PREFIX ps: <http://www.wikidata.org/prop/statement/>\n        PREFIX pq: <http://www.wikidata.org/prop/qualifier/>\n        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n        PREFIX bd: <http://www.bigdata.com/rdf#>\n        PREFIX schema: <http://schema.org/>\n        \"\"\"\n    ENTITY_PREFIX: str = \"http://www.wikidata.org/entity/Q\"\n\n    def __init__(self, endpoint, prepend_prefixes=False, exclude_qualifiers=True):\n        \"\"\"Create a Wikidata query handler.\n\n        Args:\n            endpoint (str): SPARQL endpoint, e.g. https://query.wikidata.org/sparql\n                Note that the protocal part (like https) is necessary.\n            prepend_prefixes (bool, optional): whether to prepend prefixes to the query.\n                Necessary for endpoints without pre-defined prefixes. Defaults to False.\n            exclude_qualifiers (bool, optional): whether to filter out qualifiers in the\n                queried entities. If set to True, only Wikidata entities (QXX) will be\n                considered. Defaults to True.\n        \"\"\"\n        self.sparql = SPARQLWrapper(endpoint)\n        self.sparql.setReturnFormat(JSON)\n        self.prepend_prefixes = prepend_prefixes\n        self.exclude_qualifiers = exclude_qualifiers\n        self.name = 'wikidata'\n\n    def queryWikidata(self, query):\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n\n        self.sparql.setQuery(query)\n        try:\n            ret = self.sparql.queryAndConvert()\n            result = ret['results']['bindings']\n        except Exception as exeption:\n            print(f'Failed executing query: {query}')\n            print(f'Exception: {exeption}')\n            result = []\n        return result\n\n    @staticmethod\n    def get_pid_from_uri(uri):\n        \"\"\"Get property id from uri.\"\"\"\n        return uri.split('/')[-1]\n\n    @staticmethod\n    def is_qid(qid):\n        \"\"\"Check if qid is a valid Wikidata entity id.\"\"\"\n        return qid.startswith('Q') and qid[1:].isdigit()\n\n    @staticmethod\n    def is_pid(pid):\n        \"\"\"Check if pid is a valid Wikidata property id.\"\"\"\n        return pid.startswith('P') and pid[1:].isdigit()\n\n    def get_quantifier_filter(self, var_name):\n        \"\"\"Get quantifier filter string where the var is restricted to be entities.\n        If exclude_qualifiers is set to False, return empty string.\n\n        Note: in Wikidata, entities are prefixed with \"http://www.wikidata.org/entity/Q\",\n            while qualifiers are non-entity (and mostly string) values.\n        \"\"\"\n        return f'FILTER(STRSTARTS(STR(?{var_name}), \"{self.ENTITY_PREFIX}\"))' if self.exclude_qualifiers else ''\n\n    def search_one_hop_relations(self, src, dst):\n        \"\"\"Search one hop relation between src and dst.\n        \n        Args:\n            src (str): source entity\n            dst (str): destination entity\n        \n        Returns:\n            list[list[str]]: list of paths, each path is a list of PIDs\n        \"\"\"\n        if not self.is_qid(src) or not self.is_qid(dst):\n            return []\n\n        query = f\"\"\"\n            SELECT DISTINCT ?r WHERE {{\n                wd:{src} ?r wd:{dst}.\n            }}\n            \"\"\"\n        paths = self.queryWikidata(query)\n        # Keep only PIDs in the paths\n        paths = [[self.get_pid_from_uri(path['r']['value'])] for path in paths]\n        return paths\n\n    def search_two_hop_relations(self, src, dst):\n        \"\"\"Search two hop relation between src and dst.\n        \n        Args:\n            src (str): source entity\n            dst (str): destination entity\n        \n        Returns:\n            list[list[str]]: list of paths, each path is a list of PIDs\n        \"\"\"\n        if not self.is_qid(src) or not self.is_qid(dst):\n            return []\n\n        query = f\"\"\"\n            SELECT DISTINCT ?r1 ?r2 WHERE {{\n                wd:{src} ?r1 ?x.\n                ?x ?r2 wd:{dst}.\n                {self.get_quantifier_filter('x')}\n            }}\n            \"\"\"\n        paths = self.queryWikidata(query)\n        # Keep only PIDs in the paths\n        paths = [[self.get_pid_from_uri(path['r1']['value']),\n                  self.get_pid_from_uri(path['r2']['value'])]\n                 for path in paths]\n        return paths\n\n    @lru_cache\n    def deduce_leaves(self, src, path, limit=2000):\n        \"\"\"Deduce leave entities from source entity following the path.\n        \n        Args:\n            src_entity (str): source entity\n            path (tuple[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves. Defaults to 2000.\n        \n        Returns:\n            list[str]: list of leaves. Each leaf is a QID.\n        \"\"\"\n        if len(path) >= 3:\n            raise NotImplementedError(f'Currenly only support paths with length less than 3, got {len(path)}')\n        if not self.is_qid(src):\n            return []\n\n        if len(path) == 0:\n            return [src]\n        if len(path) == 1:\n            query = f\"\"\"\n                SELECT DISTINCT ?x WHERE {{\n                    wd:{src} wdt:{path[0]} ?x.\n                    {self.get_quantifier_filter('x')}\n                    }}\n                LIMIT {limit}\n            \"\"\"\n        else: # len(path) == 2\n            query = f\"\"\"\n                SELECT DISTINCT ?x WHERE {{\n                    wd:{src} wdt:{path[0]} ?y.\n                    ?y wdt:{path[1]} ?x.\n                    {self.get_quantifier_filter('y')}\n                    {self.get_quantifier_filter('x')}\n                }}\n                LIMIT {limit}\n            \"\"\"\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n        leaves = self.queryWikidata(query)\n        # Keep only QIDs in the leaves\n        leaves = [leaf['x']['value'].split('/')[-1] for leaf in leaves]\n        return leaves\n\n    def deduce_leaves_from_multiple_srcs(self, srcs, path, limit=2000):\n        \"\"\"Deuce leave entities from multiple source entities following the path.\n\n        Args:\n            srcs (list[str]): list of source entities\n            path (list[str]): path from source entity to destination entity\n            limit (int, optional): limit of the number of leaves. Defaults to 200.\n\n        Returns:\n            list[str]: list of leaves. Each leaf is a QID.\n        \"\"\"\n        if len(path) >= 2:\n            raise NotImplementedError(f'Currenly only support paths with length less than 2, got {len(path)}')\n        if len(path) == 0:\n            return srcs\n        srcs = [src for src in srcs if self.is_qid(src)]\n        if len(srcs) == 0:\n            return []\n\n        query = f\"\"\"\n            SELECT DISTINCT ?x WHERE {{\n                VALUES ?src {{wd:{' wd:'.join(srcs)}}}\n                ?src wdt:{path[0]} ?x.\n                {self.get_quantifier_filter('x')}\n            }}\n            LIMIT {limit}\n            \"\"\"\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n        leaves = self.queryWikidata(query)\n        # Keep only QIDs in the leaves\n        leaves = [leaf['x']['value'].split('/')[-1] for leaf in leaves]\n        return leaves\n\n    @lru_cache\n    def get_neighbor_relations(self, src, hop=1, limit=100):\n        \"\"\"Get all relations connected to an entity. The relations are\n        limited to direct relations (those with wdt: prefix).\n\n        Args:\n            src (str): source entity\n            hop (int, optional): hop of the relations. Defaults to 1.\n            limit (int, optional): limit of the number of relations. Defaults to 100.\n\n        Returns:\n            list[str] | list[tuple(str,)]: list of relations. Each relation is a PID or a tuple of PIDs.\n        \"\"\"\n        if hop >= 3:\n            raise NotImplementedError(f'Currenly only support relations with hop less than 3, got {hop}')\n        if not self.is_qid(src):\n            return []\n\n        if hop == 1:\n            query = f\"\"\"SELECT DISTINCT ?rel WHERE {{\n                wd:{src} ?rel ?obj .\n                FILTER(REGEX(STR(?rel), \"^http://www.wikidata.org/prop/direct/\"))\n                {self.get_quantifier_filter('obj')}\n                }}\n                LIMIT {limit}\n                \"\"\"\n        else: # hop == 2\n            query = f\"\"\"SELECT DISTINCT ?rel1 ?rel2 WHERE {{\n                wd:{src} ?rel1 ?obj1 .\n                ?obj1 ?rel2 ?obj2 .\n                FILTER(REGEX(STR(?rel1), \"^http://www.wikidata.org/prop/direct/\"))\n                FILTER(REGEX(STR(?rel2), \"^http://www.wikidata.org/prop/direct/\"))\n                {self.get_quantifier_filter('obj1')}\n                {self.get_quantifier_filter('obj2')}\n                }}\n                LIMIT {limit}\n                \"\"\"\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n\n        relations = self.queryWikidata(query)\n        if hop == 1:\n            relations = [self.get_pid_from_uri(relation['rel']['value'])\n                         for relation in relations]\n        else:\n            relations = [(self.get_pid_from_uri(relation['rel1']['value']),\n                          self.get_pid_from_uri(relation['rel2']['value']))\n                         for relation in relations]\n        return relations\n\n    @lru_cache\n    def get_label(self, identifier):\n        \"\"\"Get label of an entity or a relation. If no label is found, return None.\n\n        Args:\n            identifier (str): entity or relation, a QID or a PID\n\n        Returns:\n            str | None: label of the entity or relation\n        \"\"\"\n        if not self.is_qid(identifier) and not self.is_pid(identifier):\n            return identifier\n\n        query = f\"\"\"\n            SELECT ?label\n                WHERE {{\n                    BIND(wd:{identifier} AS ?identifier)\n                    ?identifier rdfs:label ?label .\n                    FILTER(LANG(?label) = \"en\")\n                    }}\n                LIMIT 1\n            \"\"\"\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n        label = self.queryWikidata(query)\n        if len(label) == 0:\n            print(f'No label for identifier {identifier}.')\n            return None\n        label = label[0]['label']['value']\n        return label\n\n    def get_description(self, identifier):\n        \"\"\"Get description of an entity or a relation. If no description is found, return None.\n\n        Args:\n            identifier (str): entity or relation, a QID or a PID\n\n        Returns:\n            str | None: description of the entity or relation\n        \"\"\"\n        if not self.is_qid(identifier) and not self.is_pid(identifier):\n            return identifier\n\n        query = f\"\"\"\n            SELECT ?description\n                WHERE {{\n                    wd:{identifier} schema:description ?description .\n                    FILTER(LANG(?description) = \"en\")\n                    }}\n                LIMIT 1\n            \"\"\"\n        if self.prepend_prefixes:\n            query = self.PREFIXES + query\n        description = self.queryWikidata(query)\n        if len(description) == 0:\n            print(f'No description for identifier {identifier}.')\n            return None\n        description = description[0]['description']['value']\n        return description"]}
{"filename": "src/srtk/scorer/__init__.py", "chunked_list": ["from .encoder import LitSentenceEncoder\nfrom .scorer import Scorer\n"]}
{"filename": "src/srtk/scorer/scorer.py", "chunked_list": ["from functools import lru_cache\n\nimport torch\nfrom transformers import AutoTokenizer\n\nfrom .encoder import LitSentenceEncoder\n\n\nclass Scorer:\n    \"\"\"Scorer for relation paths.\"\"\"\n\n    def __init__(self, pretrained_name_or_path, device=None):\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_name_or_path)\n        # Set pooling method to average if the model does not have a CLS token.\n        if self.tokenizer.cls_token is None:\n            pool_method = 'avg'\n        else:\n            pool_method = 'cls'\n        self.model = LitSentenceEncoder(pretrained_name_or_path, pool=pool_method)\n        if device:\n            self.model = self.model.to(device)\n\n    @lru_cache\n    def score(self, question, prev_relations, next_relation):\n        \"\"\"Score a relation path.\n\n        Args:\n            question (str): question\n            prev_relations (tuple[str]): tuple of relation **labels** that have been traversed.\n            next_relation (str): next relation *label* to be traversed\n\n        Returns:\n            similarity: similarity between the query and the candidate\n        \"\"\"\n        # Prepending 'query' and 'relation' corresponds to the way the model was trained (check collate_fn)\n        query = f\"query: {question} [SEP] {' # '.join(prev_relations)}\"\n        next_relation = 'relation: ' + next_relation\n        text_pair = [query, next_relation]\n        inputs = self.tokenizer(text_pair, return_tensors='pt', padding=True)\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = self.model(**inputs, return_dict=True)\n            query_embedding = outputs.last_hidden_state[0:1]\n            sample_embedding = outputs.last_hidden_state[1:2]\n            similarity = self.model.compute_sentence_similarity(\n                query_embedding, sample_embedding)\n        return similarity.item()\n\n    @lru_cache\n    def batch_score(self, question, prev_relations, next_relations):\n        \"\"\"Score next relations in batch.\n\n        Args:\n            question (str): question\n            prev_relations (tuple[str]): tuple of relation **labels** that have been traversed.\n            next_relations (tuple[str]): tuple of candidate next relation *labels* that are\n                pertinent to the question and the previous relations.\n\n        Returns:\n            similarities (list[float]): list of similarities between the query and each candidate\n        \"\"\"\n        query = f\"query: {question} [SEP] {' # '.join(prev_relations)}\"\n        next_relations = ['relation: ' + next_relation for next_relation in next_relations]\n        text_pair = [query] + next_relations\n        inputs = self.tokenizer(text_pair, return_tensors='pt', padding=True)\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = self.model(**inputs, return_dict=True)\n            query_embedding = outputs.last_hidden_state[0:1]\n            sample_embeddings = outputs.last_hidden_state[1:]\n            similarities = self.model.compute_sentence_similarity(\n                query_embedding, sample_embeddings)\n            similarities = similarities.view(-1).tolist()\n            if len(similarities) != len(next_relations):\n                raise ValueError(f\"Sanity check failed: {len(similarities)} != {len(next_relations)}\")\n        return similarities", "class Scorer:\n    \"\"\"Scorer for relation paths.\"\"\"\n\n    def __init__(self, pretrained_name_or_path, device=None):\n        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_name_or_path)\n        # Set pooling method to average if the model does not have a CLS token.\n        if self.tokenizer.cls_token is None:\n            pool_method = 'avg'\n        else:\n            pool_method = 'cls'\n        self.model = LitSentenceEncoder(pretrained_name_or_path, pool=pool_method)\n        if device:\n            self.model = self.model.to(device)\n\n    @lru_cache\n    def score(self, question, prev_relations, next_relation):\n        \"\"\"Score a relation path.\n\n        Args:\n            question (str): question\n            prev_relations (tuple[str]): tuple of relation **labels** that have been traversed.\n            next_relation (str): next relation *label* to be traversed\n\n        Returns:\n            similarity: similarity between the query and the candidate\n        \"\"\"\n        # Prepending 'query' and 'relation' corresponds to the way the model was trained (check collate_fn)\n        query = f\"query: {question} [SEP] {' # '.join(prev_relations)}\"\n        next_relation = 'relation: ' + next_relation\n        text_pair = [query, next_relation]\n        inputs = self.tokenizer(text_pair, return_tensors='pt', padding=True)\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = self.model(**inputs, return_dict=True)\n            query_embedding = outputs.last_hidden_state[0:1]\n            sample_embedding = outputs.last_hidden_state[1:2]\n            similarity = self.model.compute_sentence_similarity(\n                query_embedding, sample_embedding)\n        return similarity.item()\n\n    @lru_cache\n    def batch_score(self, question, prev_relations, next_relations):\n        \"\"\"Score next relations in batch.\n\n        Args:\n            question (str): question\n            prev_relations (tuple[str]): tuple of relation **labels** that have been traversed.\n            next_relations (tuple[str]): tuple of candidate next relation *labels* that are\n                pertinent to the question and the previous relations.\n\n        Returns:\n            similarities (list[float]): list of similarities between the query and each candidate\n        \"\"\"\n        query = f\"query: {question} [SEP] {' # '.join(prev_relations)}\"\n        next_relations = ['relation: ' + next_relation for next_relation in next_relations]\n        text_pair = [query] + next_relations\n        inputs = self.tokenizer(text_pair, return_tensors='pt', padding=True)\n        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = self.model(**inputs, return_dict=True)\n            query_embedding = outputs.last_hidden_state[0:1]\n            sample_embeddings = outputs.last_hidden_state[1:]\n            similarities = self.model.compute_sentence_similarity(\n                query_embedding, sample_embeddings)\n            similarities = similarities.view(-1).tolist()\n            if len(similarities) != len(next_relations):\n                raise ValueError(f\"Sanity check failed: {len(similarities)} != {len(next_relations)}\")\n        return similarities", ""]}
{"filename": "src/srtk/scorer/encoder.py", "chunked_list": ["import json\nimport os\nfrom pathlib import Path\n\nimport lightning.pytorch as pl\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoModel\n\ntry:\n    from pytorch_metric_learning.losses import NTXentLoss\nexcept ImportError:\n    pass", "\ntry:\n    from pytorch_metric_learning.losses import NTXentLoss\nexcept ImportError:\n    pass\n\n\nclass LitSentenceEncoder(pl.LightningModule):\n    \"\"\"A lightning module that wraps a sentence encoder.\"\"\"\n    def __init__(self, model_name_or_path, temperature=0.07, lr=5e-5, pool='cls', loss='cross_entropy'):\n        if pool not in ['cls', 'avg']:\n            raise ValueError(f\"pool method must be either cls or avg, got {pool}\")\n        if loss not in ['cross_entropy', 'contrastive']:\n            raise ValueError(f\"loss method must be either cross entropy or contrastive, got {loss}\")\n        if loss == 'contrastive' and 'NTXentLoss' not in globals():\n            raise ImportError(\"pytorch_metric_learning is required for contrastive loss.\\\n                Please install it via `pip install pytorch-metric-learning`.\")\n        super().__init__()\n        self.model = AutoModel.from_pretrained(model_name_or_path)\n        if self.model.config.is_encoder_decoder:\n            self.model = self.model.encoder\n            print(\"The model is an encoder-decoder model, only the encoder will be used.\")\n        self.config = self.model.config\n        self.temperature = temperature\n        self.lr = lr\n        self.pool_method = pool\n        self.loss = loss\n        self._loss_fns = {}\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n\n    @staticmethod\n    def cls_pool(last_hidden_states, attention_mask=None):\n        \"\"\"CLS pool the sentence embedding.\n        This is the pooling method adopted by RUC's SR paper.\n\n        Args:\n            last_hidden_states: [..., seq_len, embedding_dim]\n            attention_mask: [..., seq_len] silently ignored!\n                It exists for compatibility with other pooling methods.\n\n        Returns:\n            torch.Tensor: pooled_embedding [..., embedding_dim]\n        \"\"\"\n        return last_hidden_states[..., 0, :]\n\n    @staticmethod\n    def avg_pool(last_hidden_states, attention_mask=None):\n        \"\"\"Average pool the sentence embedding.\n\n        Args:\n            last_hidden_states (torch.Tensor): [..., seq_len, embedding_dim]\n            attention_mask (torch.Tensor): [..., seq_len]\n\n        Returns:\n            torch.Tensor: pooled_embedding [..., embedding_dim]\n        \"\"\"\n        # Compute the average embedding, ignoring the padding tokens.\n        if attention_mask is None:\n            attention_mask = torch.ones(last_hidden_states.shape[:-1], device=last_hidden_states.device)\n        last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n        return last_hidden.sum(dim=-2) / attention_mask.sum(dim=-1)[..., None]\n\n    def compute_embedding_similarity(self, query, target):\n        \"\"\"Compute the similarity between query and target(s) embeddings.\n\n        Args:\n            query (torch.Tensor): [batch_size, 1, embedding_dim]\n            target (torch.Tensor): [batch_size, k, embedding_dim]\n\n        Returns:\n            torch.Tensor: similarity [batch_size, k]\n        \"\"\"\n        return F.cosine_similarity(query, target, dim=-1) / self.temperature\n\n    def pool_sentence_embedding(self, query, target, query_mask=None, target_mask=None):\n        \"\"\"Pool the query and target(s) sentence embeddings.\n\n        Args:\n            query (torch.Tensor): [..., 1, seq_len, embedding_dim]\n            target (torch.Tensor): [..., k, seq_len, embedding_dim]\n            query_mask (torch.Tensor, optional): [..., 1, seq_len, embedding_dim]. Defaults to None.\n            target_mask (torch.Tensor, optional): [..., k, seq_len, embedding]. Defaults to None.\n\n        Returns:\n            (torch.Tensor, torch.Tensor): pooled query and sentence embeddings\n        \"\"\"\n        embeddings = torch.cat([query, target], dim=-3)  # [..., 1 + k, seq_len, embedding_dim]\n        if self.pool_method == 'cls':\n            embeddings = self.cls_pool(embeddings) # [..., 1 + k, embedding_dim]\n        else:\n            if query_mask is None:\n                query_mask = torch.ones(query.shape[:-1], device=query.device)\n            if target_mask is None:\n                target_mask = torch.ones(target.shape[:-1], device=target.device)\n            attention_mask = torch.cat([query_mask, target_mask], dim=-2)  # [..., 2 + k, seq_len]\n            embeddings = self.avg_pool(embeddings, attention_mask)\n        query_embeddings = embeddings[..., 0:1, :]  # [..., 1, embedding_dim]\n        samples_embeddings = embeddings[..., 1:, :]  # [..., k, embedding_dim]\n        return query_embeddings, samples_embeddings\n\n    def compute_sentence_similarity(self, query, target, query_mask=None, target_mask=None):\n        \"\"\"Compute the similarity between query and target(s) sentence embeddings.\n        The query & target(s) sentence embedding are first pooled. Then the similarity\n        is computed between the pooled query and target(s) embeddings. \n\n        Args:\n            query (torch.Tensor): [..., 1, seq_len, embedding_dim]\n                query sentence embedding\n            target (torch.Tensor): [..., k, seq_len, embedding_dim]\n                target sentence(s) embedding\n\n        Returns:\n            torch.Tensor: similarity [..., k]\n        \"\"\"\n        query_embeddings, samples_embeddings = self.pool_sentence_embedding(query, target, query_mask, target_mask)\n        similarity = self.compute_embedding_similarity(query_embeddings, samples_embeddings)\n        return similarity\n\n    def _get_loss_fn(self):\n        \"\"\"This allows to change the loss function after initialization.\n        Besides, it maintains a single source of truth for the loss function.\n        \"\"\"\n        if self.loss not in self._loss_fns:\n            if self.loss == 'contrastive':\n                self._loss_fns[self.loss] = NTXentLoss(temperature=self.temperature)\n            else:\n                self._loss_fns[self.loss] = F.cross_entropy\n        return self._loss_fns[self.loss]\n\n    def compute_loss(self, pooled_query_embedding, pooled_sample_embeddings):\n        \"\"\"Compute loss using the pooled query and sample embeddings. It supports\n        cross_entropy and contrastive loss.\n\n        Args:\n            pooled_query_embedding (torch.Tensor): [batch_size, 1, embedding_dim]\n            pooled_sample_embeddings (torch.Tensor): [batch_size, k, embedding_dim]\n                In our case, k =  n_positive(1) + n_negatives\n\n        Returns:\n            torch.Tensor: the loss\n        \"\"\"\n        loss_fn = self._get_loss_fn()\n        if self.loss == 'contrastive':\n            # In each sentence group, the first sentence is the query, the second is the positive,\n            # and the rest are negatives. We set the positive to have the same label as the query,\n            # and the negatives to have different labels, so that the query and the positive will\n            # be pulled together, and the query and the negatives will be pushed apart.\n            # Ref: https://github.com/KevinMusgrave/pytorch-metric-learning/issues/179\n            concat_embeddings = torch.cat([pooled_query_embedding, pooled_sample_embeddings], dim=-2)\n            n_total = concat_embeddings.shape[-2]\n            n_neg = n_total - 2\n            # I manurally create positive pairs as (0, 1), and negative pairs as (0, 2), (0, 3), ...\n            # indices_tuple (anchor1, postives, anchor2, negatives)\n            indices_tuple = (torch.zeros((1,), dtype=torch.long), torch.ones((1,), dtype=torch.long),\n                        torch.zeros((n_neg,), dtype=torch.long), torch.arange(2, n_total, dtype=torch.long))\n            indices_tuple = tuple(x.to(self.device) for x in indices_tuple)\n            loss = 0\n            for sentence_group in concat_embeddings:\n                loss = loss + loss_fn(sentence_group, indices_tuple=indices_tuple)\n        else:\n            # similarity: [batch_size, 1 + neg]\n            query_samples_similarity = self.compute_embedding_similarity(pooled_query_embedding, pooled_sample_embeddings)\n            # The zerot-th label, where positive sample locates, is set to 0.\n            labels = torch.zeros(query_samples_similarity.shape[0], dtype=torch.long,\n                                device=query_samples_similarity.device)\n            loss = loss_fn(query_samples_similarity, labels)\n        return loss\n\n    def batch_forward(self, batch):\n        \"\"\"The common forward function for both training and inference.\"\"\"\n        # In each sentence group, the first sentence is the query, the second is the positive,\n        # and the rest are negatives.\n        # batch = {'input_ids': input_ids, 'attention_mask': attention_mask}\n        # input_ids: [batch_size, 1(query) + 1(positive) + neg (negative), seq_len]\n        batch_size, n_samples, seq_len = batch['input_ids'].shape  # n_sentences = 1 + 1 + neg\n        input_ids = batch['input_ids'].view(-1, seq_len)\n        attention_mask = batch['attention_mask'].view(-1, seq_len)\n        # outputs.last_hidden_state: [batch_size * (1 + 1 + neg), seq_len, embedding_dim]\n        outputs = self.model(input_ids, attention_mask=attention_mask, return_dict=True)\n        embeddings = outputs.last_hidden_state.view(batch_size, n_samples, seq_len, -1)\n        query_embedding = embeddings[:, 0:1]  # [batch_size, 1, seq_len, embedding_dim]\n        samples_embedding = embeddings[:, 1:]  # [batch_size, 1 + neg, seq_len, embedding_dim]\n        pooled_query_embedding, pooled_sample_embeddings = self.pool_sentence_embedding(\n            query_embedding, samples_embedding,\n            batch['attention_mask'][:, 0:1], batch['attention_mask'][:, 1:])\n        loss = self.compute_loss(pooled_query_embedding, pooled_sample_embeddings)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        train_loss =  self.batch_forward(batch)\n        self.log('train_loss', train_loss)\n        return train_loss\n\n    def validation_step(self, batch, batch_idx):\n        val_loss =  self.batch_forward(batch)\n        self.log('val_loss', val_loss)\n        return val_loss\n\n    def configure_optimizers(self) :\n        return torch.optim.Adam(self.parameters(), lr=self.lr)\n\n    def save_huggingface_model(self, save_dir):\n        \"\"\"Will save the model, so you can reload it using `from_pretrained()`.\"\"\"\n        save_path = Path(save_dir)\n        if not save_path.exists():\n            save_path.mkdir(parents=True)\n        state_dict = self.model.state_dict()\n        torch.save(state_dict, os.path.join(save_dir, 'pytorch_model.bin'))\n        with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n            json.dump(self.config.to_dict(), f)", ""]}
