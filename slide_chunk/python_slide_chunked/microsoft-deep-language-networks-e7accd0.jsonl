{"filename": "scripts/split_bigbench_date_understanding.py", "chunked_list": ["import os\nimport json\nimport codecs\n\nfrom utils import download_json_from_url\n\n\nbb_raw_json_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/date_understanding/task.json\"\nbbh_file_path = \"data/bbh/date_understanding.json\"\noption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]", "bbh_file_path = \"data/bbh/date_understanding.json\"\noption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]\n\nbbh_sentence, bbh_label = [], []\nbbh_dict = {}\nwith open(bbh_file_path) as fin:\n    data = json.load(fin)\n    data = data[\"examples\"]\n\n    for i in range(len(data)):\n        input, target = data[i][\"input\"], data[i][\"target\"]\n        bbh_sentence.append(input)\n        bbh_label.append(target)\n        q = input.split(\"\\n\")[0]\n        assert q not in bbh_dict\n        bbh_dict[q] = len(bbh_sentence) - 1", "\ndata = download_json_from_url(bb_raw_json_url)\ndata = data[\"examples\"]\nbb_sentence, bb_label = [], []\nbb_len = 0\nin_bbh = 0\nfor i in range(len(data)):\n    bb_len += 1\n    input, target = data[i][\"input\"], data[i][\"target_scores\"]\n    if input in bbh_dict:\n        in_bbh += 1\n        continue\n    res_input = [input]\n    res_target = \"\"\n    for j, key in enumerate(target):\n        if target[key] == 1:\n            res_target = option_list[j]\n        res_input.append(\" \".join([option_list[j], key]))\n    assert len(res_target) > 0\n    res_input = \"\\n\".join(res_input)\n    bb_sentence.append(res_input)\n    bb_label.append(res_target)", "assert in_bbh == len(bbh_dict), \"%s, %s\" % (str(in_bbh), str(len(bbh_dict)))\n\ndata = []\nfor i in range(len(bb_sentence)):\n    data.append({\"input\": bb_sentence[i], \"target\": bb_label[i]})\nprint(\"there are %s data points in big bench\" % str(bb_len))\nprint(\"there are %s data points in big bench hard\" % str(len(bbh_dict)))\nprint(\"collected %s data points from big bench - big bench hard\" % str(len(data)))\n\nbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"", "\nbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"\nprint(\"writing data to \", bb_minus_bbh_file_path)\nif not os.path.exists(bb_minus_bbh_file_path):\n    os.makedirs(bb_minus_bbh_file_path)\nwith codecs.open(bb_minus_bbh_file_path + \"/date_understanding.json\", 'w', encoding='utf-8') as json_file:\n    json.dump({\"examples\": data}, json_file, ensure_ascii=False)\n"]}
{"filename": "scripts/split_bigbench_hyperbaton.py", "chunked_list": ["import os\nimport json\nimport codecs\n\nfrom utils import download_json_from_url\n\n\nbb_raw_json_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/hyperbaton/task.json\"\nbbh_file_path = \"data/bbh/hyperbaton.json\"\noption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]", "bbh_file_path = \"data/bbh/hyperbaton.json\"\noption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]\n\nbbh_sentence, bbh_label = [], []\nbbh_dict = {}\nwith open(bbh_file_path) as fin:\n    data = json.load(fin)\n    data = data[\"examples\"]\n\n    for i in range(len(data)):\n        input, target = data[i][\"input\"], data[i][\"target\"]\n        bbh_sentence.append(input)\n        bbh_label.append(target)\n        assert input not in bbh_dict\n        bbh_dict[input] = len(bbh_sentence) - 1", "\ndata = download_json_from_url(bb_raw_json_url)\ndata = data[\"examples\"]\nbb_sentence, bb_label = [], []\nbb_len = 0\nin_bbh = 0\nfor i in range(len(data)):\n    bb_len += 1\n    input, target = data[i][\"input\"], data[i][\"target_scores\"]\n    candidates = input.split(\"Which sentence has the correct adjective order:\")[-1].strip()\n    input = \"Which sentence has the correct adjective order:\"\n    candidates = [candidates.split(\"\\\"\")[1].strip(), candidates.split(\"\\\"\")[3].strip()]\n    res_input = [input, \"Options:\"]\n    res_target = \"\"\n    for j, key in enumerate(target):\n        if target[key] == 1:\n            res_target = option_list[j]\n        res_input.append(\" \".join([option_list[j], candidates[j]]))\n    assert len(res_target) > 0\n    res_input = \"\\n\".join(res_input)\n    if res_input in bbh_dict:\n        in_bbh += 1\n        continue\n\n    bb_sentence.append(res_input)\n    bb_label.append(res_target)", "assert in_bbh == len(bbh_dict), \"%s, %s\" % (str(in_bbh), str(len(bbh_dict)))\n\ndata = []\nfor i in range(len(bb_sentence)):\n    data.append({\"input\": bb_sentence[i], \"target\": bb_label[i]})\nprint(\"there are %s data points in big bench\" % str(bb_len))\nprint(\"there are %s data points in big bench hard\" % str(len(bbh_dict)))\nprint(\"collected %s data points from big bench - big bench hard\" % str(len(data)))\n\nbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"", "\nbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"\nprint(\"writing data to \", bb_minus_bbh_file_path)\nif not os.path.exists(bb_minus_bbh_file_path):\n    os.makedirs(bb_minus_bbh_file_path)\nwith codecs.open(bb_minus_bbh_file_path + \"/hyperbaton.json\", 'w', encoding='utf-8') as json_file:\n    json.dump({\"examples\": data}, json_file, ensure_ascii=False)\n"]}
{"filename": "scripts/split_bigbench_logical_deduction_seven_objects.py", "chunked_list": ["import os\nimport json\nimport codecs\n\nfrom utils import download_json_from_url\n\n\nbb_raw_json_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/seven_objects/task.json\"\nbbh_file_path = \"data/bbh/logical_deduction_seven_objects.json\"\noption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]", "bbh_file_path = \"data/bbh/logical_deduction_seven_objects.json\"\noption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]\n\nbbh_sentence, bbh_label = [], []\nbbh_dict = {}\nwith open(bbh_file_path) as fin:\n    data = json.load(fin)\n    data = data[\"examples\"]\n\n    for i in range(len(data)):\n        input, target = data[i][\"input\"], data[i][\"target\"]\n        bbh_sentence.append(input)\n        bbh_label.append(target)\n        assert input not in bbh_dict\n        bbh_dict[input] = len(bbh_sentence) - 1", "\ndata = download_json_from_url(bb_raw_json_url)\ndata = data[\"examples\"]\nbb_sentence, bb_label = [], []\nbb_len = 0\nin_bbh = 0\nfor i in range(len(data)):\n    bb_len += 1\n    input, target = data[i][\"input\"], data[i][\"target_scores\"]\n    res_input = [\"The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. \" + input, \"Options:\"]\n    res_target = \"\"\n    for j, key in enumerate(target):\n        if target[key] == 1:\n            res_target = option_list[j]\n        res_input.append(\" \".join([option_list[j], key[:-1]]))  # no periods in bbh\n    assert len(res_target) > 0\n    res_input = \"\\n\".join(res_input)\n    if res_input in bbh_dict:\n        in_bbh += 1\n        continue\n    bb_sentence.append(res_input)\n    bb_label.append(res_target)", "assert in_bbh == len(bbh_dict), \"%s, %s\" % (str(in_bbh), str(len(bbh_dict)))\n\ndata = []\nfor i in range(len(bb_sentence)):\n    data.append({\"input\": bb_sentence[i], \"target\": bb_label[i]})\nprint(\"there are %s data points in big bench\" % str(bb_len))\nprint(\"there are %s data points in big bench hard\" % str(len(bbh_dict)))\nprint(\"collected %s data points from big bench - big bench hard\" % str(len(data)))\n\nbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"", "\nbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"\nprint(\"writing data to \", bb_minus_bbh_file_path)\nif not os.path.exists(bb_minus_bbh_file_path):\n    os.makedirs(bb_minus_bbh_file_path)\nwith codecs.open(bb_minus_bbh_file_path + \"/logical_deduction_seven_objects.json\", 'w', encoding='utf-8') as json_file:\n    json.dump({\"examples\": data}, json_file, ensure_ascii=False)\n"]}
{"filename": "scripts/split_bigbench_navigate.py", "chunked_list": ["import os\nimport json\nimport codecs\n\nfrom utils import download_json_from_url\n\n\nbb_raw_json_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/navigate/task.json\"\nbbh_file_path = \"data/bbh/navigate.json\"\noption_list = [\"Yes\", \"No\"]", "bbh_file_path = \"data/bbh/navigate.json\"\noption_list = [\"Yes\", \"No\"]\noption_map = {\"True\": \"- Yes\", \"False\": \"- No\"}\n\nbbh_sentence, bbh_label = [], []\nbbh_dict = {}\nwith open(bbh_file_path) as fin:\n    data = json.load(fin)\n    data = data[\"examples\"]\n\n    for i in range(len(data)):\n        input, target = data[i][\"input\"], data[i][\"target\"]\n        bbh_sentence.append(input)\n        bbh_label.append(target)\n        assert input not in bbh_dict\n        bbh_dict[input] = len(bbh_sentence) - 1", "\ndata = download_json_from_url(bb_raw_json_url)\ndata = data[\"examples\"]\nbb_sentence, bb_label = [], []\nbb_len = 0\nin_bbh = 0\nfor i in range(len(data)):\n    bb_len += 1\n    input, target = data[i][\"input\"], data[i][\"target_scores\"]\n    res_input = [\"If you follow these instructions, do you return to the starting point? \" + input, \"Options:\"]\n    res_target = \"\"\n    for j, key in enumerate(target):\n        if target[key] == 1:\n            res_target = option_list[j]\n        res_input.append(option_map[key])\n    assert len(res_target) > 0\n    res_input = \"\\n\".join(res_input)\n    if res_input in bbh_dict:\n        in_bbh += 1\n        continue\n\n    bb_sentence.append(res_input)\n    bb_label.append(res_target)", "assert in_bbh == len(bbh_dict), \"%s, %s\" % (str(in_bbh), str(len(bbh_dict)))\n\ndata = []\nfor i in range(len(bb_sentence)):\n    data.append({\"input\": bb_sentence[i], \"target\": bb_label[i]})\nprint(\"there are %s data points in big bench\" % str(bb_len))\nprint(\"there are %s data points in big bench hard\" % str(len(bbh_dict)))\nprint(\"collected %s data points from big bench - big bench hard\" % str(len(data)))\n\nbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"", "\nbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"\nprint(\"writing data to \", bb_minus_bbh_file_path)\nif not os.path.exists(bb_minus_bbh_file_path):\n    os.makedirs(bb_minus_bbh_file_path)\nwith codecs.open(bb_minus_bbh_file_path + \"/navigate.json\", 'w', encoding='utf-8') as json_file:\n    json.dump({\"examples\": data}, json_file, ensure_ascii=False)\n"]}
{"filename": "scripts/__init__.py", "chunked_list": [""]}
{"filename": "scripts/utils.py", "chunked_list": ["import requests\n\n\ndef download_json_from_url(url):\n    response = requests.get(url)\n    response.raise_for_status()\n    json_data = response.json()\n    return json_data\n", ""]}
{"filename": "tests/test_dln_score.py", "chunked_list": ["from unittest.mock import patch\n\nimport numpy as np\nimport tiktoken\n\nfrom dln.score import LogProbsScore, OutputClasses\n\n\ndef test_logprobs_score_with_output_classes(score_requests, top_logprobs):\n    encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n    logprobs_score = LogProbsScore(encoder)\n\n    with patch(\"dln.score.forward_evaluate\", top_logprobs):\n        logprobs = logprobs_score.score_requests(\n            score_requests, output_classes=OutputClasses(protos=[\"a|A\", \"b|B\"])\n        )\n\n    np.testing.assert_almost_equal(logprobs.targets, [-8.6746863, -0.4428973])\n    np.testing.assert_almost_equal(\n        logprobs.contexts,\n        [\n            [9.99829143e-01, 1.70856546e-04],\n            [6.42173164e-01, 3.57826836e-01],\n        ],\n    )", "def test_logprobs_score_with_output_classes(score_requests, top_logprobs):\n    encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n    logprobs_score = LogProbsScore(encoder)\n\n    with patch(\"dln.score.forward_evaluate\", top_logprobs):\n        logprobs = logprobs_score.score_requests(\n            score_requests, output_classes=OutputClasses(protos=[\"a|A\", \"b|B\"])\n        )\n\n    np.testing.assert_almost_equal(logprobs.targets, [-8.6746863, -0.4428973])\n    np.testing.assert_almost_equal(\n        logprobs.contexts,\n        [\n            [9.99829143e-01, 1.70856546e-04],\n            [6.42173164e-01, 3.57826836e-01],\n        ],\n    )", "\n\ndef test_logprobs_score_without_output_classes(score_requests, raw_logprobs):\n    encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n    logprobs_score = LogProbsScore(encoder)\n    with patch(\"dln.score.forward_evaluate\", raw_logprobs):\n        logprobs = logprobs_score.score_requests(score_requests)\n\n    np.testing.assert_almost_equal(logprobs.targets, [-0.7682657, -0.7632834])\n    np.testing.assert_almost_equal(logprobs.contexts, [-2.8217665, -2.73069])", ""]}
{"filename": "tests/test_dln_losses.py", "chunked_list": ["import numpy as np\nfrom dln.loss import ZeroOneLoss\n\n\ndef test_zero_one_loss():\n    y = [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"]\n    y_hat = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"]\n    zero_one_loss = ZeroOneLoss(lambda x: x)\n    losses = zero_one_loss(y, y_hat)\n    np.testing.assert_array_equal(losses, [0.0, 1.0, 1.0, 1.0, 0.0, 0.0])", "\n\ndef test_zero_one_loss_no_postproc():\n    y = [\"A\", \"B\", \"C\", \"a\", \"b\", \"c\"]\n    y_hat = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"]\n    zero_one_loss = ZeroOneLoss()\n    losses = zero_one_loss(y, y_hat)\n    np.testing.assert_array_equal(losses, [1.0, 1.0, 1.0, 1.0, 0.0, 0.0])\n\n\ndef test_zero_one_loss_postproc():\n    y = [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"]\n    y_hat = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"]\n    zero_one_loss = ZeroOneLoss(lambda x: x.lower())\n    losses = zero_one_loss(y, y_hat)\n    np.testing.assert_array_equal(losses, [0.0, 1.0, 1.0, 1.0, 0.0, 0.0])\n    assert y == [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"]  # no side effect", "\n\ndef test_zero_one_loss_postproc():\n    y = [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"]\n    y_hat = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"]\n    zero_one_loss = ZeroOneLoss(lambda x: x.lower())\n    losses = zero_one_loss(y, y_hat)\n    np.testing.assert_array_equal(losses, [0.0, 1.0, 1.0, 1.0, 0.0, 0.0])\n    assert y == [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"]  # no side effect\n", "\n\ndef test_zero_one_loss_postproc_property():\n    zero_one_loss = ZeroOneLoss(lambda x: x.upper())\n    assert zero_one_loss.postproc(\"abc\") == \"ABC\"\n\n    zero_one_loss = ZeroOneLoss()\n    assert zero_one_loss.postproc(\"abc\") == \"abc\"\n", ""]}
{"filename": "tests/test_vi_layers.py", "chunked_list": ["from unittest.mock import patch\n\nimport numpy as np\n\nfrom dln.operator import forward_instantiate\nfrom dln.score import OutputClasses\nfrom dln.vi.layers import PriorLayer, ResidualPriorLayer\n\n\ndef test_apply_residual_without_template():\n    inputs = np.array([\"input1\", \"input2\", \"input3\"])\n    outputs = np.array([\"output1\", \"output2\", \"output3\"])\n    residual_prior_layer = ResidualPriorLayer(\n        forward_template=\"suffix_forward\",\n        init=\"A task description\",\n    )\n    result = residual_prior_layer.apply_residual(outputs, inputs)\n    expected_outputs = np.array(\n        [\n            \"input1\\nYour thoughts were:\\noutput1\",\n            \"input2\\nYour thoughts were:\\noutput2\",\n            \"input3\\nYour thoughts were:\\noutput3\",\n        ]\n    )\n    np.testing.assert_equal(result, expected_outputs)", "\ndef test_apply_residual_without_template():\n    inputs = np.array([\"input1\", \"input2\", \"input3\"])\n    outputs = np.array([\"output1\", \"output2\", \"output3\"])\n    residual_prior_layer = ResidualPriorLayer(\n        forward_template=\"suffix_forward\",\n        init=\"A task description\",\n    )\n    result = residual_prior_layer.apply_residual(outputs, inputs)\n    expected_outputs = np.array(\n        [\n            \"input1\\nYour thoughts were:\\noutput1\",\n            \"input2\\nYour thoughts were:\\noutput2\",\n            \"input3\\nYour thoughts were:\\noutput3\",\n        ]\n    )\n    np.testing.assert_equal(result, expected_outputs)", "\n\ndef test_apply_residual_with_template():\n    inputs = np.array([\"input1\", \"input2\", \"input3\"])\n    outputs = np.array([\"output1\", \"output2\", \"output3\"])\n    residual_prior_layer = ResidualPriorLayer(\n        forward_template=\"suffix_forward\",\n        init=\"A task description\",\n    )\n    result = residual_prior_layer.apply_residual(outputs, inputs, use_template=True)\n    expected_outputs = np.array(\n        [\n            \"input1\\n\\nA task description\\nYour thoughts were:\\noutput1\",\n            \"input2\\n\\nA task description\\nYour thoughts were:\\noutput2\",\n            \"input3\\n\\nA task description\\nYour thoughts were:\\noutput3\",\n        ]\n    )\n    np.testing.assert_equal(result, expected_outputs)", "\n\ndef test_log_p_with_output_classes(top_logprobs):\n    forward_instantiate()\n    inputs = [\"1 + 1\", \"1 * 1\"]\n    outputs = [\"B\", \"A\"]\n    output_classes = OutputClasses(protos=[\"a|A\", \"b|B\"])\n    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n    with patch(\"dln.score.forward_evaluate\", top_logprobs):\n        logp = prior_layer.log_p(\n            inputs, outputs, output_classes=output_classes\n        )\n    np.testing.assert_almost_equal(logp.targets, [-8.67468626, -0.44289729])\n    np.testing.assert_almost_equal(\n        logp.contexts,\n        [\n            [9.99829143e-01, 1.70856546e-04],\n            [6.42173164e-01, 3.57826836e-01],\n        ],\n    )", "\n\ndef test_log_p_without_output_classes(raw_logprobs, score_requests):\n    forward_instantiate()\n    inputs = [s.context for s in score_requests]\n    outputs = [\"B\", \"A\"]\n    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n    with patch(\"dln.score.forward_evaluate\", raw_logprobs):\n        logp = prior_layer.log_p(inputs, outputs)\n    np.testing.assert_almost_equal(logp.targets, [-1.48348267, -1.47351816])", "\n\ndef test_forward_with_output_class(top_logprobs):\n    forward_instantiate()\n    inputs = [\"1 + 1\", \"1 * 1\"]\n    output_classes = OutputClasses(protos=[\"A|a\", \"B|b\"])\n    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n    with patch(\"dln.score.forward_evaluate\", top_logprobs):\n        result = prior_layer.forward(inputs, output_classes)\n    np.testing.assert_equal(result, [\"A\", \"A\"])", "\n\ndef test_forward_without_output_class(text_outputs):\n    forward_instantiate()\n    inputs = [\"1 + 1\", \"1 * 1\"]\n    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n    with patch(\"dln.vi.layers.forward_evaluate\", text_outputs):\n        result = prior_layer.forward(inputs)\n    np.testing.assert_equal(result, [\"A\", \"A\"])\n", "\n\ndef test_forward_strip_double_newlines():\n    forward_instantiate()\n    inputs = [\"1 + 1\"]\n    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n    text_output = lambda *args, **kwargs: [\"A\\n\\n\"]\n    with patch(\"dln.vi.layers.forward_evaluate\", text_output):\n        result = prior_layer.forward(inputs, strip_double_newlines=True)\n    np.testing.assert_equal(result, [\"A\\n\"])", ""]}
{"filename": "tests/test_dln_dataset.py", "chunked_list": ["from unittest.mock import MagicMock, patch\n\nimport pytest\n\nfrom dln.dataset import Dataset, init_dataset\n\n\ndef test_init_dataset_subj():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\"subj\", 42, \"./data\")\n    assert (\n        dataset.instruction\n        == \"Read the following sentence, then choose whether it is subjective or objective.\"\n    )\n    assert dataset.dataset_name == \"subj\"\n    assert output_classes.protos == [\"subjective\", \"objective\"]\n    assert val_examples == -1", "\n\ndef test_init_dataset_mpqa():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\"mpqa\", 42, \"./data\")\n    assert (\n        dataset.instruction\n        == \"Read the following review, then choose whether it is negative or positive.\"\n    )\n    assert dataset.dataset_name == \"mpqa\"\n    assert output_classes.protos == [\"negative\", \"positive\"]\n    assert val_examples == -1", "\n\ndef test_init_dataset_trec():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\"trec\", 42, \"./data\")\n    assert (\n        dataset.instruction\n        == \"Read the following question, then choose whether it is about a description, entity, expression, human, location or number.\"\n    )\n    assert dataset.dataset_name == \"trec\"\n    assert output_classes.protos == [\n        \"description\",\n        \"entity\",\n        \"expression\",\n        \"human\",\n        \"location\",\n        \"number\",\n    ]\n    assert val_examples == -1", "\n\ndef test_init_dataset_disaster():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\"disaster\", 42, \"./data\")\n    assert (\n        dataset.instruction\n        == \"Read the following sentence, then choose whether it is relevant to a disaster.\"\n    )\n    assert dataset.dataset_name == \"disaster\"\n    assert output_classes.protos == [\"no\", \"yes\"]\n    assert val_examples == -1", "\n\ndef test_init_dataset_airline():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\"airline\", 42, \"./data\")\n    assert (\n        dataset.instruction\n        == \"Read the following sentence, then choose whether it is positive, negative, or neutral.\"\n    )\n    assert dataset.dataset_name == \"airline\"\n    assert output_classes.protos == [\"positive\", \"negative\", \"neutral\"]\n    assert val_examples == -1", "\n\ndef test_init_dataset_hyperbaton():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\"hyperbaton\", 42, \"./data\")\n    assert dataset.instruction == \"Which sentence has the correct adjective order:\"\n    assert dataset.dataset_name == \"hyperbaton\"\n    assert output_classes.protos == [\"a|A\", \"b|B\"]\n    assert val_examples == 300\n", "\n\ndef test_init_dataset_navigate():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\"navigate\", 42, \"./data\")\n    assert (\n        dataset.instruction\n        == \"If you follow these instructions, do you return to the starting point?\"\n    )\n    assert dataset.dataset_name == \"navigate\"\n    assert output_classes.protos == [\"yes|Yes\", \"no|No\"]\n    assert val_examples == -1", "\n\ndef test_init_dataset_date_understanding():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\n            \"date_understanding\", 42, \"./data\"\n        )\n    assert dataset.instruction == \"Infer the date from context.\"\n    assert dataset.dataset_name == \"date_understanding\"\n    assert output_classes.protos == [\"a|A\", \"b|B\", \"c|C\", \"d|D\", \"e|E\", \"f|F\"]\n    assert val_examples == -1", "\n\ndef test_init_dataset_logical_deduction_seven_objects():\n    with patch.object(Dataset, \"load_dataset\", MagicMock):\n        dataset, output_classes, val_examples = init_dataset(\n            \"logical_deduction_seven_objects\", 42, \"./data\"\n        )\n    assert (\n        dataset.instruction\n        == \"The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\"\n    )\n    assert dataset.dataset_name == \"logical_deduction_seven_objects\"\n    assert output_classes.protos == [\"a|A\", \"b|B\", \"c|C\", \"d|D\", \"e|E\", \"f|F\", \"g|G\"]\n    assert val_examples == -1", "\n\ndef test_init_dataset_not_found():\n    with pytest.raises(AssertionError, match=r\"Dataset test not found\"):\n        init_dataset(\"test\", 42, \"./data\")\n"]}
{"filename": "tests/test_vi_model.py", "chunked_list": ["from unittest.mock import patch\n\nimport numpy as np\nimport pytest\n\nfrom dln.loss import ZeroOneLoss\nfrom dln.postprocessing import postprocess_prediction\nfrom dln.score import LogProbs, OutputClasses\nfrom dln.vi.layers import PriorLayer, ResidualPriorLayer\nfrom dln.vi.model import VILModel", "from dln.vi.layers import PriorLayer, ResidualPriorLayer\nfrom dln.vi.model import VILModel\nfrom dln.vi.sampler import PosteriorSampler, PromptSampler\n\n\n@pytest.fixture\ndef loss_fn():\n    loss_fn = ZeroOneLoss(postproc=postprocess_prediction)\n    return loss_fn\n", "\n\n@pytest.fixture\ndef log_likes():\n    return np.array(\n        [\n            [[0.2, 0.3], [0.4, 0.1]],\n            [[0.5, 0.5], [0.3, 0.2]],\n        ]\n    )", "\n\n@pytest.fixture\ndef class_weights():\n    return np.array([[0.6, 0.4], [0.8, 0.2]])\n\n\n@pytest.fixture\ndef q_h():\n    return np.array(\n        [\n            [\"test 1.1\", \"test 1.2\"],\n            [\"test 2.1\", \"test 2.2\"],\n            [\"test 3.1\", \"test 3.2\"],\n            [\"test 4.1\", \"test 4.2\"],\n        ]\n    )", "def q_h():\n    return np.array(\n        [\n            [\"test 1.1\", \"test 1.2\"],\n            [\"test 2.1\", \"test 2.2\"],\n            [\"test 3.1\", \"test 3.2\"],\n            [\"test 4.1\", \"test 4.2\"],\n        ]\n    )\n", "\n@pytest.fixture\ndef log_p_fn():\n    def log_p(\n        self,\n        inputs,\n        targets,\n        prompts=None,\n        output_classes=None,\n        agg=\"max\",\n    ):\n        np.random.seed(42)\n        logprobs = LogProbs(\n            np.random.rand(len(inputs)),\n            np.random.rand(len(inputs), len(output_classes))\n            if output_classes else None\n        )\n        return logprobs\n\n    return log_p", "\n\ndef test_memory(loss_fn):\n    model = VILModel(loss_fn, task_description=\"Test task description\", use_memory=2)\n    assert model.get_from_memory(0).size == 0\n    assert model.get_from_memory(1).size == 0\n    with pytest.raises(BaseException):\n        model.get_from_memory(2)\n    model.add_to_memory(\"test1\", \"test2\", 0.5)\n    model.add_to_memory(\"test3\", \"test4\", 0.2)\n    model.add_to_memory(\"test5\", \"test6\", 0.7)\n    np.testing.assert_array_equal(model.get_from_memory(0), [\"test1\", \"test5\"])", "\n\ndef test_compute_elbo_score(loss_fn, log_likes, class_weights):\n    model = VILModel(loss_fn, task_description=\"Test task description\")\n    elbo_score = model.compute_elbo_score(log_likes)\n    expected_output = [[0.35, 0.4], [0.35, 0.15]]\n    assert np.allclose(elbo_score, expected_output)\n\n    elbo_score = model.compute_elbo_score(log_likes, class_weights)\n    assert np.allclose(elbo_score, [0.37, 0.33])", "\n\ndef test_sample_hidden_states(loss_fn, q_h):\n    np.random.seed(42)\n    inputs = np.array([\"test-1\", \"test-2\", \"test-3\", \"test-4\"])\n    y = np.array([\"test_1\", \"test_2\", \"test_3\", \"test_4\"])\n    h = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n    num_h_samples = 2\n    model = VILModel(\n        loss_fn, task_description=\"Test task description\", num_h_samples=num_h_samples\n    )\n    total_h_samples = len(inputs) * num_h_samples\n    mock_l2_log_p = LogProbs(\n        np.random.rand(total_h_samples),\n        np.random.rand(total_h_samples, num_h_samples),\n    )\n    mock_l1_log_p = LogProbs(np.random.rand(total_h_samples), None)\n\n    with patch.object(\n        PosteriorSampler, \"sample_q_h\", return_value=q_h\n    ), patch.object(\n        PriorLayer, \"log_p\", return_value=mock_l2_log_p\n    ), patch.object(\n        ResidualPriorLayer, \"log_p\", return_value=mock_l1_log_p\n    ):\n        hidden_states = model.sample_hidden_states(x=inputs, y=y, h1=h)\n\n    residual_h_tilde_1, h_tilde_1, h_tilde_1_star, weights = hidden_states\n\n    np.testing.assert_equal(\n        residual_h_tilde_1,\n        [\n            [\n                \"test-1\\nYour thoughts were:\\ntest 1.1\",\n                \"test-1\\nYour thoughts were:\\ntest 1.2\",\n            ],\n            [\n                \"test-2\\nYour thoughts were:\\ntest 2.1\",\n                \"test-2\\nYour thoughts were:\\ntest 2.2\",\n            ],\n            [\n                \"test-3\\nYour thoughts were:\\ntest 3.1\",\n                \"test-3\\nYour thoughts were:\\ntest 3.2\",\n            ],\n            [\n                \"test-4\\nYour thoughts were:\\ntest 4.1\",\n                \"test-4\\nYour thoughts were:\\ntest 4.2\",\n            ],\n        ],\n    )\n    np.testing.assert_equal(h_tilde_1, q_h)\n    np.testing.assert_equal(\n        h_tilde_1_star, [\"test 1.2\", \"test 2.2\", \"test 3.1\", \"test 4.2\"]\n    )\n    np.testing.assert_almost_equal(\n        weights,\n        [\n            [0.28796663, 0.71203337],\n            [0.45481729, 0.54518271],\n            [0.63320434, 0.36679566],\n            [0.40828206, 0.59171794],\n        ],\n    )", "\n\ndef test_inference_one_layer(loss_fn, backward_info, log_p_fn):\n    np.random.seed(42)\n    inputs, y, y_hat, losses = backward_info\n    num_h_samples = 2\n    num_p_samples = 2\n    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n    model = VILModel(\n        loss_fn,\n        task_description=\"Test task description\",\n        output_classes=output_classes,\n        num_h_samples=num_h_samples,\n        num_p_samples=num_p_samples,\n        two_layers=False,\n    )\n    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n    with patch.object(\n        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n    ), patch.object(\n        PriorLayer, \"log_p\", log_p_fn\n    ):\n        elbo, _, p2 = model.inference_one_layer(inputs, y, y_hat, losses)\n    np.testing.assert_almost_equal(elbo, 0.64288586)\n    assert p2 == \"prompt 2\"", "\n\n@pytest.mark.parametrize(\n    \"train_p1, train_p2, expec_best_p1_elbo, expec_best_p2_elbo, expec_best_p1, expec_best_p2\",\n    [\n        (True, False, 0.39742681, 0.0, \"prompt 2\", \"Test task description\"),  # Train p1\n        (False, True, 0.0, 0.49596743, \"Test task description\", \"prompt 2\"),  # Train p2\n        (True, True, 0.39742681, 0.49596743, \"prompt 2\", \"prompt 2\"),  # Train e2e\n    ],\n)\ndef test_inference_vi(\n    loss_fn,\n    backward_info,\n    q_h,\n    log_p_fn,\n    train_p1,\n    train_p2,\n    expec_best_p1_elbo,\n    expec_best_p2_elbo,\n    expec_best_p1,\n    expec_best_p2,\n):\n    inputs, y, y_hat, losses = backward_info\n    h1 = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n    num_h_samples = 2\n    num_p_samples = 2\n    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n    model = VILModel(\n        loss_fn,\n        task_description=\"Test task description\",\n        output_classes=output_classes,\n        num_h_samples=num_h_samples,\n        num_p_samples=num_p_samples,\n        train_p1=train_p1,\n        train_p2=train_p2,\n    )\n    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n    with patch.object(\n        PosteriorSampler, \"sample_q_h\", return_value=q_h\n    ), patch.object(\n        PriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        ResidualPriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n    ):\n        r_h1 = model.encoder_l1.apply_residual(h1, inputs)\n        best_p1_elbo, best_p2_elbo, best_p1, best_p2 = model.inference_vi(\n            inputs, h1, r_h1, y, y_hat, losses\n        )\n    np.testing.assert_almost_equal(best_p1_elbo, expec_best_p1_elbo)\n    np.testing.assert_almost_equal(best_p2_elbo, expec_best_p2_elbo)\n    assert best_p1 == expec_best_p1\n    assert best_p2 == expec_best_p2", "    ],\n)\ndef test_inference_vi(\n    loss_fn,\n    backward_info,\n    q_h,\n    log_p_fn,\n    train_p1,\n    train_p2,\n    expec_best_p1_elbo,\n    expec_best_p2_elbo,\n    expec_best_p1,\n    expec_best_p2,\n):\n    inputs, y, y_hat, losses = backward_info\n    h1 = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n    num_h_samples = 2\n    num_p_samples = 2\n    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n    model = VILModel(\n        loss_fn,\n        task_description=\"Test task description\",\n        output_classes=output_classes,\n        num_h_samples=num_h_samples,\n        num_p_samples=num_p_samples,\n        train_p1=train_p1,\n        train_p2=train_p2,\n    )\n    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n    with patch.object(\n        PosteriorSampler, \"sample_q_h\", return_value=q_h\n    ), patch.object(\n        PriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        ResidualPriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n    ):\n        r_h1 = model.encoder_l1.apply_residual(h1, inputs)\n        best_p1_elbo, best_p2_elbo, best_p1, best_p2 = model.inference_vi(\n            inputs, h1, r_h1, y, y_hat, losses\n        )\n    np.testing.assert_almost_equal(best_p1_elbo, expec_best_p1_elbo)\n    np.testing.assert_almost_equal(best_p2_elbo, expec_best_p2_elbo)\n    assert best_p1 == expec_best_p1\n    assert best_p2 == expec_best_p2", "\n@pytest.mark.parametrize(\n    \"train_p1, train_p2, expec_elbo, expec_best_p1, expec_best_p2, expec_loss_mean, expec_elbo1, expec_elbo2\",\n    [\n        (True, False, 0.39742681, 'prompt 2', 'Test task description', 0.5, 0.39742681, 0.0),  # Train p1\n        (False, True, 0.49596743, 'Test task description', 'prompt 2', 0.5, 0.0, 0.49596743),  # Train p2\n        (True, True, 0.89339424, 'prompt 2', 'prompt 2', 0.5, 0.39742681, 0.49596743),  # Train e2e\n    ],\n)\ndef test_forward_two_layers(\n    loss_fn,\n    backward_info,\n    q_h,\n    log_p_fn,\n    train_p1,\n    train_p2,\n    expec_elbo,\n    expec_best_p1,\n    expec_best_p2,\n    expec_loss_mean,\n    expec_elbo1,\n    expec_elbo2,\n):\n    inputs, y, y_hat, _ = backward_info\n    h1 = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n    num_h_samples = 2\n    num_p_samples = 2\n    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n    model = VILModel(\n        loss_fn,\n        task_description=\"Test task description\",\n        output_classes=output_classes,\n        num_h_samples=num_h_samples,\n        num_p_samples=num_p_samples,\n        train_p1=train_p1,\n        train_p2=train_p2,\n        two_layers=True,\n    )\n    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n    with patch.object(\n        PosteriorSampler, \"sample_q_h\", return_value=q_h\n    ), patch.object(\n        PriorLayer, \"forward\", return_value=y_hat\n    ), patch.object(\n        PriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        ResidualPriorLayer, \"forward\", return_value=h1\n    ), patch.object(\n        ResidualPriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n    ):\n        elbo, best_p1, best_p2, loss_mean, elbo1, elbo2 = model.forward(inputs, y)\n    assert best_p1 == expec_best_p1\n    assert best_p2 == expec_best_p2\n    np.testing.assert_almost_equal(elbo, expec_elbo)\n    np.testing.assert_almost_equal(elbo1, expec_elbo1)\n    np.testing.assert_almost_equal(elbo2, expec_elbo2)\n    np.testing.assert_almost_equal(loss_mean, expec_loss_mean)", ")\ndef test_forward_two_layers(\n    loss_fn,\n    backward_info,\n    q_h,\n    log_p_fn,\n    train_p1,\n    train_p2,\n    expec_elbo,\n    expec_best_p1,\n    expec_best_p2,\n    expec_loss_mean,\n    expec_elbo1,\n    expec_elbo2,\n):\n    inputs, y, y_hat, _ = backward_info\n    h1 = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n    num_h_samples = 2\n    num_p_samples = 2\n    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n    model = VILModel(\n        loss_fn,\n        task_description=\"Test task description\",\n        output_classes=output_classes,\n        num_h_samples=num_h_samples,\n        num_p_samples=num_p_samples,\n        train_p1=train_p1,\n        train_p2=train_p2,\n        two_layers=True,\n    )\n    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n    with patch.object(\n        PosteriorSampler, \"sample_q_h\", return_value=q_h\n    ), patch.object(\n        PriorLayer, \"forward\", return_value=y_hat\n    ), patch.object(\n        PriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        ResidualPriorLayer, \"forward\", return_value=h1\n    ), patch.object(\n        ResidualPriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n    ):\n        elbo, best_p1, best_p2, loss_mean, elbo1, elbo2 = model.forward(inputs, y)\n    assert best_p1 == expec_best_p1\n    assert best_p2 == expec_best_p2\n    np.testing.assert_almost_equal(elbo, expec_elbo)\n    np.testing.assert_almost_equal(elbo1, expec_elbo1)\n    np.testing.assert_almost_equal(elbo2, expec_elbo2)\n    np.testing.assert_almost_equal(loss_mean, expec_loss_mean)", "\n\ndef test_forward_one_layer(\n    loss_fn,\n    backward_info,\n    q_h,\n    log_p_fn,\n):\n    inputs, y, y_hat, _ = backward_info\n    num_h_samples = 2\n    num_p_samples = 2\n    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n    model = VILModel(\n        loss_fn,\n        task_description=\"Test task description\",\n        output_classes=output_classes,\n        num_h_samples=num_h_samples,\n        num_p_samples=num_p_samples,\n        train_p1=False,\n        train_p2=False,\n        two_layers=False,\n    )\n    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n    with patch.object(\n        PosteriorSampler, \"sample_q_h\", return_value=q_h\n    ), patch.object(\n        PriorLayer, \"forward\", return_value=y_hat\n    ), patch.object(\n        PriorLayer, \"log_p\", log_p_fn\n    ), patch.object(\n        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n    ):\n        elbo, best_p1, best_p2, loss_mean, elbo1, elbo2 = model.forward(inputs, y)\n    assert best_p1 == None\n    assert best_p2 == \"prompt 2\"\n    np.testing.assert_almost_equal(elbo, 0.64288586)\n    np.testing.assert_almost_equal(elbo1, 0.0)\n    np.testing.assert_almost_equal(elbo2, 0.64288586)\n    np.testing.assert_almost_equal(loss_mean, 0.5)", "\n\n@pytest.mark.parametrize(\n    \"train_p1, train_p2, two_layers, expec_l1_calls\",\n    [\n        (True, False, True, 1),\n        (False, True, True, 1),\n        (True, True, True, 1),\n        (True, False, False, 0),\n        (False, True, False, 0),", "        (True, False, False, 0),\n        (False, True, False, 0),\n        (True, True, False, 0),\n    ],\n)\ndef test_forward_inference(\n    loss_fn,\n    backward_info,\n    train_p1,\n    train_p2,\n    two_layers,\n    expec_l1_calls,\n):\n    inputs, _, y_hat, _ = backward_info\n    h1 = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n    num_h_samples = 2\n    num_p_samples = 2\n    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n    model = VILModel(\n        loss_fn,\n        task_description=\"Test task description\",\n        output_classes=output_classes,\n        num_h_samples=num_h_samples,\n        num_p_samples=num_p_samples,\n        train_p1=train_p1,\n        train_p2=train_p2,\n        two_layers=two_layers,\n    )\n    # should only require forward pass\n    with patch.object(\n        PriorLayer, \"forward\", return_value=y_hat\n    ) as l2, patch.object(\n        ResidualPriorLayer, \"forward\", return_value=h1\n    ) as l1:\n        result = model.forward(inputs)\n    np.testing.assert_equal(result, ['test_', 'test', 'test_', 'test'])\n    assert l1.call_count == expec_l1_calls\n    assert l2.call_count == 1", "\n\ndef test_strip_options(loss_fn):\n    model = VILModel(loss_fn, task_description=\"Test task description\")\n    input_data = np.array(\n        [\n            \"This is a test\\nOptions:\\n(A)\\n(B)\",\n            \"No options here\",\n            \"Another testOptions:(A)(B)\",\n        ]\n    )\n    expected_output = np.array([\"This is a test\", \"No options here\", \"Another test\"])\n    output_data = model.strip_options(input_data)\n    assert np.array_equal(output_data, expected_output)", "\n\ndef test_strip_answer(loss_fn):\n    model = VILModel(loss_fn, task_description=\"Test task description\")\n    input_data = np.array(\n        [\"This is a test\\nAnswer: A\", \"No answer here\", \"Another testAnswer:\"]\n    )\n    expected_output = np.array([\"This is a test\", \"No answer here\", \"Another test\"])\n    output_data = model.strip_answer(input_data)\n    assert np.array_equal(output_data, expected_output)", "\n\ndef test_strip_prefix(loss_fn):\n    model = VILModel(\n        loss_fn, \"Test task description\", strip_prefix_for_hidden=\"PREFIX:\"\n    )\n    input_data = np.array(\n        [\"PREFIX: This is a test\", \"No prefix here\", \"PREFIX: Another test\"]\n    )\n    expected_output = np.array([\"This is a test\", \"No prefix here\", \"Another test\"])\n    output_data = model.strip_prefix(input_data)\n    assert np.array_equal(output_data, expected_output)", ""]}
{"filename": "tests/test_dln_postprocessing.py", "chunked_list": ["import pytest\n\nfrom dln.postprocessing import postprocess_prediction, remove_extra_spaces\n\n\n@pytest.mark.parametrize(\n    \"input,expected\",\n    [\n        (\"foo  bar\", \"foo bar\"),\n        (\"  foo  bar  \", \" foo bar \"),", "        (\"foo  bar\", \"foo bar\"),\n        (\"  foo  bar  \", \" foo bar \"),\n        (\"foo\\n\\nbar\", \"foo\\nbar\"),\n        (\"\\nfoo\\n\\nbar\\n\", \"\\nfoo\\nbar\\n\"),\n    ],\n)\ndef test_remove_extra_spaces(input, expected):\n    assert remove_extra_spaces(input, False) == expected\n\n", "\n\n@pytest.mark.parametrize(\n    \"input,expected\",\n    [\n        (\"foo  bar\", \"foo bar\"),\n        (\"  foo  bar  \", \" foo bar \"),\n        (\"foo\\n\\nbar\", \"foo bar\"),\n        (\"\\nfoo\\n\\nbar\\n\", \" foo bar \"),\n    ],", "        (\"\\nfoo\\n\\nbar\\n\", \" foo bar \"),\n    ],\n)\ndef test_remove_extra_spaces_and_replace_new_lines(input, expected):\n    assert remove_extra_spaces(input, True) == expected\n\n\n@pytest.mark.parametrize(\n    \"input,expected\",\n    [", "    \"input,expected\",\n    [\n        (\"foo@bar\", \"foo\"),\n        (\"foo123bar\", \"foo\"),\n        (\"  Foo  Bar  \", \"foo\"),\n        (\"\", \"\"),\n        (\"Foo\", \"foo\"),\n        (\"Option (A)\", \"a\"),\n        (\"Option (A, B)\", \"a\"),\n        (\"Foo Bar\", \"foo\"),", "        (\"Option (A, B)\", \"a\"),\n        (\"Foo Bar\", \"foo\"),\n    ],\n)\ndef test_postprocess_prediction(input, expected):\n    assert postprocess_prediction(input) == expected\n"]}
{"filename": "tests/test_vi_sampler.py", "chunked_list": ["import re\nfrom unittest.mock import MagicMock\n\nimport numpy as np\nimport pytest\n\nfrom dln.vi.sampler import PosteriorSampler, PromptSampler\n\n\ndef test_sample_q_p(backward_info):\n    inputs, y, y_hat, losses = backward_info\n    sampler = PromptSampler()\n    mock_eval_fn = MagicMock(return_value=[\"new prompt 1\", \"new prompt 2\"])\n    sampler.evaluate_func = mock_eval_fn\n    prompt = \"test prompt\"\n    num_samples = 2\n    held_out_half = False\n    prompts = sampler.sample_q_p(\n        inputs, y, y_hat, losses, prompt, num_samples, held_out_half\n    )\n\n    q_prompt = mock_eval_fn.call_args[0][0][\n        0\n    ]  # rendered template sent to evaluate_func\n\n    success_block = re.findall(r\"# Student successes(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n    assert \"test_1\" in success_block\n    assert \"test_3\" in success_block\n    assert \"test_2\" not in success_block\n    assert \"test_4\" not in success_block\n\n    error_block = re.findall(r\"# Student errors(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n    assert \"test_2\" in error_block\n    assert \"test_4\" in error_block\n    assert \"test_1\" not in error_block\n    assert \"test_3\" not in error_block\n\n    np.testing.assert_array_equal(\n        prompts, [\"test prompt\", \"new prompt 1\", \"new prompt 2\"]\n    )", "\ndef test_sample_q_p(backward_info):\n    inputs, y, y_hat, losses = backward_info\n    sampler = PromptSampler()\n    mock_eval_fn = MagicMock(return_value=[\"new prompt 1\", \"new prompt 2\"])\n    sampler.evaluate_func = mock_eval_fn\n    prompt = \"test prompt\"\n    num_samples = 2\n    held_out_half = False\n    prompts = sampler.sample_q_p(\n        inputs, y, y_hat, losses, prompt, num_samples, held_out_half\n    )\n\n    q_prompt = mock_eval_fn.call_args[0][0][\n        0\n    ]  # rendered template sent to evaluate_func\n\n    success_block = re.findall(r\"# Student successes(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n    assert \"test_1\" in success_block\n    assert \"test_3\" in success_block\n    assert \"test_2\" not in success_block\n    assert \"test_4\" not in success_block\n\n    error_block = re.findall(r\"# Student errors(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n    assert \"test_2\" in error_block\n    assert \"test_4\" in error_block\n    assert \"test_1\" not in error_block\n    assert \"test_3\" not in error_block\n\n    np.testing.assert_array_equal(\n        prompts, [\"test prompt\", \"new prompt 1\", \"new prompt 2\"]\n    )", "\n\ndef test_sample_q_p_hold_out_half(backward_info):\n    inputs, y, y_hat, losses = backward_info\n    sampler = PromptSampler()\n    mock_eval_fn = MagicMock(return_value=[\"new prompt 1\", \"new prompt 2\"])\n    sampler.evaluate_func = mock_eval_fn\n    prompt = \"test prompt\"\n    num_samples = 2\n    held_out_half = True\n    prompts = sampler.sample_q_p(\n        inputs, y, y_hat, losses, prompt, num_samples, held_out_half\n    )\n\n    q_prompt = mock_eval_fn.call_args[0][0][\n        0\n    ]  # rendered template sent to evaluate_func\n\n    success_block = re.findall(r\"# Student successes(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n    error_block = re.findall(r\"# Student errors(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n\n    success_examples = [i for i in y if i in success_block]\n    error_examples = [i for i in y_hat if i in error_block]\n\n    assert len(success_examples + error_examples) == 2\n    assert \"test_2\" not in success_block\n    assert \"test_4\" not in success_block\n    assert \"test_1\" not in error_block\n    assert \"test_3\" not in error_block\n    np.testing.assert_array_equal(\n        prompts, [\"test prompt\", \"new prompt 1\", \"new prompt 2\"]\n    )", "\n\ndef test_sample_q_h(backward_info):\n    inputs, y, _, _ = backward_info\n    h = [\"test 1\", \"test2\", \"test 3\", \"test4\"]\n    num_samples = 2\n    sampler = PosteriorSampler(\"suffix_forward_tbs\")\n    mock_eval_fn = MagicMock(\n        # h * num_samples\n        return_value=[\n            \"test 1.1\",\n            \"test 1.2\",\n            \"test 2.1\",\n            \"test 2.2\",\n            \"test 3.1\",\n            \"test 3.2\",\n            \"test 4.1\",\n            \"test 4.2\",\n        ]\n    )\n    sampler.evaluate_func = mock_eval_fn\n    prompt = \"test prompt\"\n    next_prompt = \"test next prompt\"\n    h_hat = sampler.sample_q_h(\n        inputs,\n        y,\n        h,\n        prompt,\n        next_prompt,\n        num_samples,\n    )\n    np.testing.assert_equal(\n        h_hat,\n        [\n            [\"test 1.1\", \"test 1.2\"],\n            [\"test 2.1\", \"test 2.2\"],\n            [\"test 3.1\", \"test 3.2\"],\n            [\"test 4.1\", \"test 4.2\"],\n        ],\n    )", ""]}
{"filename": "tests/test_dln_templates.py", "chunked_list": ["import pytest\n\nfrom dln.template import DLNTemplate, Templates, load_template\n\n\ndef test_DLNTemplate_render():\n    template = DLNTemplate(template=\"{{ message }}\")\n    rendered = template.render(message=\"Foo bar!\")\n    assert rendered == \"Foo bar!\"\n", "\n\ndef test_DLNTemplate_render_default_message():\n    template = DLNTemplate(template=\"{{ message }}\", message=\"Default foo bar\")\n    rendered = template.render()\n    assert rendered == \"Default foo bar\"\n\n\ndef test_template_get_template():\n    suffix_forward = Templates.get(\"suffix_forward\")\n    assert suffix_forward.template == \"{{ input }}\\n\\n{{ prompt }}\"", "def test_template_get_template():\n    suffix_forward = Templates.get(\"suffix_forward\")\n    assert suffix_forward.template == \"{{ input }}\\n\\n{{ prompt }}\"\n\n\ndef test_template_template_not_found():\n    with pytest.raises(KeyError):\n        Templates.get(\"foo\")\n\n\ndef test_load_template():\n    template = load_template(\"suffix_forward\")\n    rendered = template.render(input=\"input test\", prompt=\"prompt test\")\n    assert rendered == (\"\"\"input test\\n\\nprompt test\"\"\")", "\n\ndef test_load_template():\n    template = load_template(\"suffix_forward\")\n    rendered = template.render(input=\"input test\", prompt=\"prompt test\")\n    assert rendered == (\"\"\"input test\\n\\nprompt test\"\"\")\n"]}
{"filename": "tests/test_dln_operator.py", "chunked_list": ["from unittest.mock import AsyncMock, MagicMock\n\nimport openai\nimport pytest\n\nfrom dln.operator import (\n    GPT,\n    backward_evaluate,\n    backward_instantiate,\n    forward_evaluate,", "    backward_instantiate,\n    forward_evaluate,\n    forward_instantiate,\n)\n\n\n@pytest.fixture\ndef mock_data():\n    chat_completion_data = {\"choices\": [{\"message\": {\"content\": \"Montreal\"}}]}\n    completion_data = {\n        \"choices\": [\n            {\n                \"text\": \"Montreal\",\n                \"logprobs\": {\"token_logprobs\": [0], \"top_logprobs\": [{}], \"tokens\": {}},\n            }\n        ]\n    }\n    return chat_completion_data, completion_data", "\n\n@pytest.fixture\ndef mock_openai_api(monkeypatch, mock_data):\n    chat_completion_data, completion_data = mock_data\n    mock_api = MagicMock()\n    mock_api.ChatCompletion.create.return_value = chat_completion_data\n    mock_api.Completion.create.return_value = completion_data\n    monkeypatch.setattr(openai, \"ChatCompletion\", mock_api.ChatCompletion)\n    monkeypatch.setattr(openai, \"Completion\", mock_api.Completion)", "\n\n@pytest.fixture\ndef mock_openai_api_async(monkeypatch, mock_data):\n    chat_completion_data, completion_data = mock_data\n    mock_api = MagicMock()\n    mock_api.ChatCompletion.acreate = AsyncMock(return_value=chat_completion_data)\n    monkeypatch.setattr(openai, \"ChatCompletion\", mock_api.ChatCompletion)\n\n\ndef test_invalid_model_name():\n    with pytest.raises(ValueError):\n        GPT(\"invalid-model-name\")", "\n\ndef test_invalid_model_name():\n    with pytest.raises(ValueError):\n        GPT(\"invalid-model-name\")\n\n\ndef test_valid_model_name():\n    gpt = GPT(\"text-davinci-003\")\n    assert gpt.engine == \"text-davinci-003\"", "\n\n@pytest.mark.asyncio\nasync def test_aget_chat_completion_response(mock_openai_api_async):\n    gpt = GPT(\"text-davinci-003\")\n    prompt = \"What is the largest city in Quebec?\"\n    response = await gpt.aget_chat_completion_response(prompt)\n    assert \"Montreal\" in response\n\n\ndef test_get_chat_completion_response(mock_openai_api):\n    gpt = GPT(\"text-davinci-003\")\n    prompt = \"What is the largest city in Quebec?\"\n    response = gpt.get_chat_completion_response(prompt)\n    assert \"Montreal\" in response", "\n\ndef test_get_chat_completion_response(mock_openai_api):\n    gpt = GPT(\"text-davinci-003\")\n    prompt = \"What is the largest city in Quebec?\"\n    response = gpt.get_chat_completion_response(prompt)\n    assert \"Montreal\" in response\n\n\ndef test_get_completion_response(mock_openai_api):\n    gpt = GPT(\"text-davinci-003\")\n    prompt = \"What is the largest city in Quebec?\"\n    response = gpt.get_completion_response([prompt])\n    assert \"Montreal\" in response[0]", "\ndef test_get_completion_response(mock_openai_api):\n    gpt = GPT(\"text-davinci-003\")\n    prompt = \"What is the largest city in Quebec?\"\n    response = gpt.get_completion_response([prompt])\n    assert \"Montreal\" in response[0]\n\n\ndef test_forward_evaluate(mock_openai_api):\n    forward_instantiate(\"text-davinci-003\")\n    prompt = \"What is the largest city in Quebec?\"\n    response = forward_evaluate([prompt])\n    assert \"Montreal\" in response[0]", "def test_forward_evaluate(mock_openai_api):\n    forward_instantiate(\"text-davinci-003\")\n    prompt = \"What is the largest city in Quebec?\"\n    response = forward_evaluate([prompt])\n    assert \"Montreal\" in response[0]\n\n\ndef test_backward_evaluate(mock_openai_api):\n    backward_instantiate(\"text-davinci-003\")\n    prompt = \"What is the largest city in Quebec?\"\n    response = backward_evaluate([prompt])\n    assert \"Montreal\" in response[0]", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["import numpy as np\nimport pytest\n\nfrom dln.score import ScoreRequest\n\n\n@pytest.fixture\ndef backward_info():\n    inputs = np.array([\"test-1\", \"test-2\", \"test-3\", \"test-4\"])\n    y = np.array([\"test_1\", \"test_2\", \"test_3\", \"test_4\"])\n    y_hat = np.array([\"test_1\", \"test2\", \"test_3\", \"test4\"])\n    losses = np.array([0.0, 1.0, 0.0, 1.0])\n    return inputs, y, y_hat, losses", "\n\n@pytest.fixture\ndef score_requests():\n    return [\n        ScoreRequest(\n            context=\"1 + 1 is:\\n(A) 1\\n(B) 2\\n\\nAnswer:\",\n            target=\"B\",\n            payload=\"B\",\n        ),\n        ScoreRequest(\n            context=\"1 * 1 is:\\n(A) 1\\n(B) 2\\n\\nAnswer:\",\n            target=\"A\",\n            payload=\"A\",\n        ),\n    ]", "\n\n@pytest.fixture\ndef text_outputs():\n    def logprobs_fn(contexts, *args, **kwargs):\n        #  return logprobs in the same order it was requested (contexts)\n        logprobs = {\n            \"1 + 1\": \"A\",\n            \"1 * 1\": \"A\",\n        }\n        return [logprobs[context[:5]] for context in contexts]\n\n    return logprobs_fn", "\n\n@pytest.fixture\ndef top_logprobs():\n    def logprobs_fn(contexts, *args, **kwargs):\n        #  return logprobs in the same order it was requested (contexts)\n        logprobs = {\n            \"1 + 1\": {\n                \"Option\": -8.876863,\n                \"Result\": -17.299635,\n                \"choice\": -17.710045,\n                \"<\": -17.075796,\n                \"=\": -15.760291,\n                \"correct\": -13.988989,\n                \" A\": -10.678262,\n                \"A\": -3.663905,\n                \"All\": -16.454699,\n                \"B\": -12.343077,\n            },\n            \"1 * 1\": {\n                \"Option\": -8.315238,\n                \"=\": -16.698154,\n                \"A\": -11.863415,\n                \"B\": -12.451943,\n                \"Answer\": -7.4255853,\n                \"answer\": -14.647212,\n                \"Correct\": -8.74908,\n                \"Choice\": -13.000805,\n                \"Yes\": -14.741361,\n                \"b\": -17.22967,\n            },\n        }\n        ordered_log_p = [logprobs[context[:5]] for context in contexts]\n        return [\n            [0, [ordered_log_p[0]], 2],\n            [0, [ordered_log_p[1]], 2],\n        ]\n\n    return logprobs_fn", "\n\n@pytest.fixture\ndef raw_logprobs():\n    def logprobs_fn(contexts, *args, **kwargs):\n        #  return logprobs in the same order it was requested (contexts)\n        logprobs = {\n            \"1 + 1\": [\n                \"1 + 1 is:\\n(A) 1\\n(B) 2\\n\\nAnswer:\\nB\",\n                [\n                    None,\n                    -5.550775,\n                    -3.194002,\n                    -8.062983,\n                    -1.9706848,\n                    -0.9759903,\n                    -11.239477,\n                    -2.745899,\n                    -0.030587194,\n                    -1.4996661,\n                    -0.068833716,\n                    -0.009404114,\n                    -0.0001532674,\n                    -6.5041706e-05,\n                    -0.056048736,\n                    -0.05334273,\n                    -8.41094,\n                    -6.9211907,\n                    -0.001781753,\n                    -0.053041545,\n                    -1.4834975,\n                ],\n                [\n                    \"1\",\n                    \" +\",\n                    \" 1\",\n                    \" is\",\n                    \":\",\n                    \"\\\\n\",\n                    \"(\",\n                    \"A\",\n                    \")\",\n                    \" 1\",\n                    \"\\\\n\",\n                    \"(\",\n                    \"B\",\n                    \")\",\n                    \" 2\",\n                    \"\\\\n\",\n                    \"\\\\n\",\n                    \"Answer\",\n                    \":\",\n                    \"\\\\n\",\n                    \"B\",\n                ],\n            ],\n            \"1 * 1\": [\n                \"1 * 1 is:\\n(A) 1\\n(B) 2\\n\\nAnswer: A\",\n                [\n                    None,\n                    -6.06174,\n                    -4.7931056,\n                    -8.253801,\n                    -2.3915708,\n                    -0.5870681,\n                    -10.741921,\n                    -3.3388677,\n                    -0.011392174,\n                    -0.86958236,\n                    -0.11698982,\n                    -0.48095098,\n                    -0.002377014,\n                    -8.3404535e-05,\n                    -1.417262,\n                    -0.027041545,\n                    -5.510647,\n                    -4.546986,\n                    -0.0010610583,\n                    -0.053041545,\n                    -1.4735329,\n                ],\n                [\n                    \"1\",\n                    \" *\",\n                    \" 1\",\n                    \" is\",\n                    \":\",\n                    \"\\\\n\",\n                    \"(\",\n                    \"A\",\n                    \")\",\n                    \" 1\",\n                    \"\\\\n\",\n                    \"(\",\n                    \"B\",\n                    \")\",\n                    \" 2\",\n                    \"\\\\n\",\n                    \"\\\\n\",\n                    \"Answer\",\n                    \":\",\n                    \"\\\\n\",\n                    \"A\",\n                ],\n            ],\n        }\n        return [logprobs[context[:5]] for context in contexts]\n\n    return logprobs_fn", ""]}
{"filename": "dln/loss.py", "chunked_list": ["from abc import ABC\n\nimport numpy as np\n\n\nclass LLoss(ABC):\n    pass\n\n\nclass ZeroOneLoss(LLoss):\n    def __init__(self, postproc=None):\n        \"\"\"\n        Args:\n            postproc: a function that takes and returns a string to be apply before calculating the loss\n        Returns:\n            ZeroOneLoss as an float32 np.array\n        \"\"\"\n        self._postproc = postproc\n\n    def __call__(self, input, target):\n        \"\"\"\n        Args:\n            input: a list of strings\n            target: a list of strings\n        \"\"\"\n        if self._postproc:\n            input = [self.postproc(i) for i in input]\n            target = [self.postproc(t) for t in target]\n        losses = (np.array(input) != np.array(target)).astype(\"float32\")\n        return losses\n\n    @property\n    def postproc(self):\n        if self._postproc is None:\n            return lambda x: x\n        return self._postproc", "\nclass ZeroOneLoss(LLoss):\n    def __init__(self, postproc=None):\n        \"\"\"\n        Args:\n            postproc: a function that takes and returns a string to be apply before calculating the loss\n        Returns:\n            ZeroOneLoss as an float32 np.array\n        \"\"\"\n        self._postproc = postproc\n\n    def __call__(self, input, target):\n        \"\"\"\n        Args:\n            input: a list of strings\n            target: a list of strings\n        \"\"\"\n        if self._postproc:\n            input = [self.postproc(i) for i in input]\n            target = [self.postproc(t) for t in target]\n        losses = (np.array(input) != np.array(target)).astype(\"float32\")\n        return losses\n\n    @property\n    def postproc(self):\n        if self._postproc is None:\n            return lambda x: x\n        return self._postproc"]}
{"filename": "dln/template.py", "chunked_list": ["from dataclasses import dataclass\nfrom typing import List\nimport os\nimport glob\nimport yaml\nimport logging\nfrom jinja2 import Template\nfrom packaging import version as pkg_version\n\n", "\n\n@dataclass\nclass DLNTemplate:\n    template: str\n    stop_tokens: List[str] = None\n    version: int = \"latest\"\n    description: str = None\n    message: str = None\n    message_alternatives: List[str] = None\n\n    def render(self, **kwargs):\n        if kwargs.get(\"message\") is None:\n            kwargs[\"message\"] = self.message\n\n        return Template(self.template).render(**kwargs)", "\n\nclass Templates:\n    _instance = None\n\n    def __init__(self):\n        self._data = {}\n        template_directory = os.path.join(os.path.dirname(__file__), 'templates/')\n        for filename in glob.glob(f\"{template_directory}/*.yaml\"):\n            template_name = os.path.basename(filename).split(\".\")[0]\n            template = yaml.safe_load(open(filename, \"r\"))\n\n            self._data[template_name] = []\n            for tversion, ttemplate in template.items():\n                if \"v\" not in tversion:\n                    raise ValueError(\"Version must be in the format v1, v1.2, etc.\")\n\n                ttemplate[\"version\"] = pkg_version.parse(tversion.split(\"v\")[-1])\n                if \"stop_tokens\" in ttemplate:\n                    # strip the first \\ of \\\\n from the stop tokens\n                    for i, stop_token in enumerate(ttemplate[\"stop_tokens\"]):\n                        ttemplate[\"stop_tokens\"][i] = ttemplate[\"stop_tokens\"][\n                            i\n                        ].replace(\"\\\\n\", \"\\n\")\n                self._data[template_name].append(DLNTemplate(**ttemplate))\n\n    @staticmethod\n    def get(template_name):\n        template_name, _, version = template_name.partition(\":\")\n        if not version:\n            version = \"latest\"\n\n        if Templates._instance is None:\n            Templates._instance = Templates()\n\n        templates = Templates._instance._data[template_name]\n\n        if version == \"latest\":\n            template = max(templates, key=lambda x: x.version)\n        else:\n            template = [\n                t for t in templates if t.version == pkg_version.parse(version)\n            ][0]\n\n        logging.info(f\"Loaded template {template_name} v{template.version}\")\n        return template", "\n\ndef load_template(template_name):\n    return Templates.get(template_name)\n"]}
{"filename": "dln/postprocessing.py", "chunked_list": ["import re\n\n\ndef remove_extra_spaces(input, remove_new_line=False):\n    assert isinstance(input, str)\n    output = input\n    if remove_new_line:\n        output = output.replace(\"\\n\", \" \")\n    # remove extra spaces\n    while True:\n        if len(output) == 0 or \"  \" not in output:\n            break\n        output = output.replace(\"  \", \" \")\n    # remove extra new lines\n    while True:\n        if len(output) == 0 or \"\\n\\n\" not in output:\n            break\n        output = output.replace(\"\\n\\n\", \"\\n\")\n    return output", "\n\ndef postprocess_prediction(input):\n    assert isinstance(input, str)\n    output = input\n    output = re.sub(r\"\\W+\", \" \", output)  # remove non word\n    output = re.sub(r\"\\d+\", \" \", output)  # remove digits\n    output = remove_extra_spaces(output)\n    output = output.lower()\n\n    output = output.split()\n    if len(output) == 0:\n        return \"\"\n\n    if len(output) == 1:\n        return output[0]\n\n    # More than one word\n\n    # Useful when the model predicts \"Option (A)\" instead of (A).\n    if \"option\" == output[0]:\n        return output[1]\n\n    # Return the first word\n    return output[0]", ""]}
{"filename": "dln/dataset.py", "chunked_list": ["import json\nimport os\nfrom collections import defaultdict\nfrom os.path import join as pjoin\n\nimport numpy as np\nimport yaml\n\nfrom dln.score import OutputClasses\n", "from dln.score import OutputClasses\n\n\nclass Dataset:\n    def __init__(\n        self,\n        dataset_path,\n        dataset,\n        seed,\n        use_label_mapping=True,\n        append_options=True,\n    ):\n        self.dataset_name = dataset\n        self.data_path = dataset_path\n        self.random_seed = seed\n        self.dataset_info = self._load_config(\n            pjoin(os.path.dirname(os.path.abspath(__file__)), \"dataset_info.yaml\")\n        )\n        self.label_mapping = self.dataset_info[self.dataset_name].get(\n            \"label_mapping\", {}\n        )\n        self.use_label_mapping = use_label_mapping and self.label_mapping\n        self.append_options = append_options\n        self.instruction = self.dataset_info[self.dataset_name][\"instruction\"]\n        self.rng = np.random.RandomState(self.random_seed)\n\n        # load dataset from file\n        self.dataset = dict(\n            train_per_class=dict(),\n            train=dict(sentence=[], label=[]),\n            dev=dict(sentence=[], label=[]),\n            test=dict(sentence=[], label=[]),\n        )\n        self.load_dataset()\n\n        print(\"loaded dataset from %s ...\" % self.data_path)\n        print(\n            \"we have %s training, %s dev, and %s test data points.\"\n            % (self.train_size, self.dev_size, self.test_size)\n        )\n        self.reset()\n\n    @staticmethod\n    def _load_config(config_file):\n        assert os.path.exists(config_file), \"Invalid config file\"\n        with open(config_file) as reader:\n            config = yaml.safe_load(reader)\n        return config\n\n    @property\n    def train_size(self):\n        return len(self.dataset[\"train\"][\"label\"])\n\n    @property\n    def dev_size(self):\n        return len(self.dataset[\"dev\"][\"label\"])\n\n    @property\n    def test_size(self):\n        return len(self.dataset[\"test\"][\"label\"])\n\n    def resize(self, split, size):\n        indices = np.random.permutation(np.arange(len(self.dataset[split][\"label\"])))[\n            :size\n        ]\n        self.dataset[split][\"label\"] = [\n            self.dataset[split][\"label\"][i] for i in indices\n        ]\n        self.dataset[split][\"sentence\"] = [\n            self.dataset[split][\"sentence\"][i] for i in indices\n        ]\n\n    def reset(self):\n        self.train_pointer, self.dev_pointer, self.test_pointer = 0, 0, 0\n\n    def load_dataset(self):\n        data_shuffling_rng = np.random.RandomState(42)\n\n        if \"bbh\" in self.data_path:\n            assert self.dataset_name in (\n                \"logical_deduction_seven_objects\",\n                \"hyperbaton\",\n                \"navigate\",\n                \"date_understanding\",\n            ), self.dataset_name\n            train_valid_file_path = os.path.join(\n                self.data_path.replace(\"bbh\", \"bb_minus_bbh\"),\n                self.dataset_name + \".json\",\n            )\n            with open(train_valid_file_path) as fin:\n                data = json.load(fin)\n                data = data[\"examples\"]\n                train_size = len(data) // 2\n                dev_size = len(data) - train_size\n                train_size = min(\n                    train_size, 1000\n                )  # some dataset has too many data, don't want to have a too large train/valid size\n                dev_size = min(dev_size, 1000)\n                assert train_size > 0, train_size\n                assert dev_size > 0, dev_size\n                data_shuffling_rng.shuffle(data)\n                for i in range(len(data)):\n                    if i < train_size:\n                        split = \"train\"\n                    elif i < train_size + dev_size:\n                        split = \"dev\"\n                    else:\n                        break\n                    input, target = data[i][\"input\"], data[i][\"target\"]\n                    if self.dataset_name == \"date_understanding\" and split == \"train\":\n                        # for date understanding, we add training data to dev set, as the dev set is too small\n                        self.dataset[\"dev\"][\"sentence\"].append(input)\n                        self.dataset[\"dev\"][\"label\"].append(target)\n                    self.dataset[split][\"sentence\"].append(input)\n                    self.dataset[split][\"label\"].append(target)\n\n            test_file_path = os.path.join(self.data_path, self.dataset_name + \".json\")\n            with open(test_file_path) as fin:\n                data = json.load(fin)\n                data = data[\"examples\"]\n                for i in range(len(data)):\n                    input, target = data[i][\"input\"], data[i][\"target\"]\n                    self.dataset[\"test\"][\"sentence\"].append(input)\n                    self.dataset[\"test\"][\"label\"].append(target)\n\n        elif \"ordered_prompt\" in self.data_path:\n            assert self.dataset_name in (\"mpqa\", \"trec\", \"subj\"), self.dataset_name\n\n            for split in [\"train\", \"dev\", \"test\"]:\n                _split = \"dev_subsample\" if split == \"dev\" else split\n                file_path = os.path.join(\n                    self.data_path, self.dataset_name, _split + \".jsonl\"\n                )\n                sentence_list, label_list = [], []\n\n                with open(file_path) as fin:\n                    for line in fin:\n                        datapoint = json.loads(line)\n                        sentence, label = datapoint[\"sentence\"], datapoint[\"label\"]\n                        if self.append_options:\n                            sentence = [sentence, \"Options:\"] + [\n                                \"- \" + item\n                                for item in list(self.label_mapping.values())\n                            ]\n                            sentence = \"\\n\".join(sentence)\n                        sentence_list.append(sentence)\n                        label_list.append(label)\n\n                if split == \"train\":\n                    indices = data_shuffling_rng.choice(\n                        len(sentence_list), 1000, replace=False\n                    )\n                    for idx in indices:\n                        self.dataset[split][\"sentence\"].append(sentence_list[idx])\n                        self.dataset[split][\"label\"].append(label_list[idx])\n                elif split == \"dev\":\n                    self.dataset[split][\"sentence\"] = sentence_list\n                    self.dataset[split][\"label\"] = label_list\n                elif split == \"test\":\n                    indices = data_shuffling_rng.choice(\n                        len(sentence_list), 250, replace=False\n                    )\n                    for idx in indices:\n                        self.dataset[split][\"sentence\"].append(sentence_list[idx])\n                        self.dataset[split][\"label\"].append(label_list[idx])\n\n        elif \"leopard\" in self.data_path:\n            assert self.dataset_name in (\"disaster\", \"airline\"), self.dataset_name\n            file_path = os.path.join(\n                self.data_path, self.dataset_name, self.dataset_name + \"_eval.json\"\n            )\n            sentence_list, label_list = [], []\n\n            with open(file_path) as fin:\n                data = json.load(fin)\n                for i in range(len(data)):\n                    sentence, label = data[i][\"sentence1\"], data[i][\"label\"]\n                    if self.append_options:\n                        sentence = [sentence, \"Options:\"] + [\n                            \"- \" + item for item in list(self.label_mapping.values())\n                        ]\n                        sentence = \"\\n\".join(sentence)\n                    sentence_list.append(sentence)\n                    label_list.append(label)\n            indices = data_shuffling_rng.choice(len(sentence_list), 1500, replace=False)\n            for idx in indices[:1000]:\n                self.dataset[\"train\"][\"sentence\"].append(sentence_list[idx])\n                self.dataset[\"train\"][\"label\"].append(label_list[idx])\n            for idx in indices[1000:1250]:\n                self.dataset[\"dev\"][\"sentence\"].append(sentence_list[idx])\n                self.dataset[\"dev\"][\"label\"].append(label_list[idx])\n            for idx in indices[1250:]:\n                self.dataset[\"test\"][\"sentence\"].append(sentence_list[idx])\n                self.dataset[\"test\"][\"label\"].append(label_list[idx])\n        else:\n            raise NotImplementedError\n\n        train_per_class = defaultdict(list)\n        for index, label in enumerate(self.dataset[\"train\"][\"label\"]):\n            train_per_class[label].append(index)\n        self.dataset[\"train_per_class\"] = train_per_class\n\n    def reset_pointer(self, split):\n        if split == \"train\":\n            self.train_pointer = 0\n        elif split == \"dev\":\n            self.dev_pointer = 0\n        elif split == \"test\":\n            self.test_pointer = 0\n\n    def get_batch(self, split, batch_size, random_sample=False, balance=False):\n        assert batch_size > 0\n        assert split in [\"train\", \"dev\", \"test\"]\n        if split == \"train\":\n            pointer = self.train_pointer\n            data_size = self.train_size\n            self.train_pointer += batch_size\n            if self.train_pointer >= data_size:\n                self.train_pointer = 0\n        elif split == \"dev\":\n            pointer = self.dev_pointer\n            data_size = self.dev_size\n            self.dev_pointer += batch_size\n            if self.dev_pointer >= data_size:\n                self.dev_pointer = 0\n        else:\n            pointer = self.test_pointer\n            data_size = self.test_size\n            self.test_pointer += batch_size\n            if self.test_pointer >= data_size:\n                self.test_pointer = 0\n\n        if random_sample is True:\n            if balance is True:\n                indices = []\n                pick_order = self.rng.choice(\n                    list(self.dataset[\"train_per_class\"].keys()),\n                    len(self.dataset[\"train_per_class\"].keys()),\n                    replace=False,\n                )\n\n                i = 0\n                while len(indices) < batch_size:\n                    indices += self.rng.choice(\n                        self.dataset[\"train_per_class\"][\n                            pick_order[i % len(pick_order)]\n                        ],\n                        1,\n                    ).tolist()\n                    i += 1\n            else:\n                indices = self.rng.choice(data_size, batch_size, replace=False)\n        else:\n            start = pointer\n            end = min(start + batch_size, data_size)\n            indices = np.arange(start, end)\n\n        sentence_list, label_list = [], []\n        for idx in indices:\n            sentence_list.append(self.dataset[split][\"sentence\"][idx])\n            if self.use_label_mapping:\n                label_mapping = self.label_mapping\n                label_list.append(label_mapping[self.dataset[split][\"label\"][idx]])\n            else:\n                label_list.append(self.dataset[split][\"label\"][idx])\n\n        return sentence_list, label_list\n\n    def iterate(self, split, batch_size, random_sample=False):\n        if split == \"train\":\n            self.train_pointer = 0\n        elif split == \"dev\":\n            self.dev_pointer = 0\n        else:\n            self.test_pointer = 0\n        while True:\n            yield self.get_batch(split, batch_size, random_sample)\n\n            if not random_sample:\n                if split == \"dev\" and self.dev_pointer == 0:\n                    return\n\n                if split == \"test\" and self.test_pointer == 0:\n                    return\n\n    def get_size(self, split):\n        return len(self.dataset[split][\"label\"])\n\n    def get_data(self, split=\"train\", indices=None):\n        \"\"\"get all data from a split\"\"\"\n        assert split in self.dataset\n        if indices is None:\n            res_sentence = self.dataset[split][\"sentence\"]\n            res_label = self.dataset[split][\"label\"]\n        else:\n            assert isinstance(indices, list) and len(indices) > 0\n            res_sentence, res_label = [], []\n            for idx in indices:\n                res_sentence.append(self.dataset[split][\"sentence\"][idx])\n                res_sentence.append(self.dataset[split][\"label\"][idx])\n        return res_sentence, res_label", "\n\ndef init_dataset(dataset_id, seed, data_dir):\n    ordered_prompt = os.path.join(data_dir, \"ordered_prompt\")\n    leopard = os.path.join(data_dir, \"leopard\")\n    bbh = os.path.join(data_dir, \"bbh\")\n    dataset_location = {\n        \"subj\": ordered_prompt,\n        \"mpqa\": ordered_prompt,\n        \"trec\": ordered_prompt,\n        \"disaster\": leopard,\n        \"airline\": leopard,\n        \"hyperbaton\": bbh,\n        \"navigate\": bbh,\n        \"date_understanding\": bbh,\n        \"logical_deduction_seven_objects\": bbh,\n    }\n\n    assert dataset_id in dataset_location, f\"Dataset {dataset_id} not found\"\n\n    dataset = Dataset(dataset_location[dataset_id], dataset_id, seed)\n    val_examples = {\"hyperbaton\": 300}.get(dataset_id, -1)\n    protos = {\n        \"hyperbaton\": [\"a|A\", \"b|B\"],\n        \"navigate\": [\"yes|Yes\", \"no|No\"],\n        \"date_understanding\": [\"a|A\", \"b|B\", \"c|C\", \"d|D\", \"e|E\", \"f|F\"],\n        \"logical_deduction_seven_objects\": [\n            \"a|A\",\n            \"b|B\",\n            \"c|C\",\n            \"d|D\",\n            \"e|E\",\n            \"f|F\",\n            \"g|G\",\n        ],\n    }.get(dataset_id, list(dataset.label_mapping.values()))\n    output_classes = OutputClasses(protos=protos)\n    return dataset, output_classes, val_examples", ""]}
{"filename": "dln/__init__.py", "chunked_list": [""]}
{"filename": "dln/operator.py", "chunked_list": ["# global interpreter\nfrom typing import List\nimport asyncio\nimport numpy as np\nimport openai\nimport logging\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_exponential,", "    stop_after_attempt,\n    wait_exponential,\n    retry_if_exception_type,\n)\n\nforward_interpreter = None\nbackward_interpreter = None\n\n\nclass GPT:\n    AVAILABLE_MODELS = [\n        \"text-davinci-003\",\n        \"text-davinci-002\",\n        \"code-davinci-002\",\n        \"text-curie-001\",\n        \"text-babbage-001\",\n        \"text-ada-001\",\n        \"gpt-3.5-turbo\",\n        \"gpt-4\",\n        \"gpt-4-32k\",\n    ]\n\n    def __init__(self, model_name=\"text-davinci-003\", **generation_options):\n        if model_name not in self.AVAILABLE_MODELS:\n            raise ValueError(\n                f\"model_name should be one of: {','.join(self.AVAILABLE_MODELS)}\"\n            )\n        self.generation_options = generation_options\n        self.engine = model_name\n\n    @retry(\n        reraise=True,\n        stop=stop_after_attempt(100),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=(\n            retry_if_exception_type(openai.error.Timeout)\n            | retry_if_exception_type(openai.error.APIError)\n            | retry_if_exception_type(openai.error.APIConnectionError)\n            | retry_if_exception_type(openai.error.RateLimitError)\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n        ),\n    )\n    async def aget_chat_completion_response(self, prompt, **kwargs):\n        \"\"\"\n        prompting chatgpt via openai api\n        now batching only works for completion, not on chat\n        \"\"\"\n        if openai.api_type == \"azure\":\n            try:\n                response = await openai.ChatCompletion.acreate(\n                    deployment_id=self.engine,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    **kwargs,\n                )\n            except openai.InvalidRequestError as e:\n                # Most likely a content filtering error from Azure.\n                logging.warn(str(e))\n                return str(e)\n        else:\n            response = await openai.ChatCompletion.acreate(\n                model=self.engine,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                **kwargs,\n            )\n\n        if \"content\" not in response[\"choices\"][0][\"message\"]:\n            return \"\"\n\n        output = response[\"choices\"][0][\"message\"][\"content\"].strip()\n        return output\n\n    @retry(\n        reraise=True,\n        stop=stop_after_attempt(100),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=(\n            retry_if_exception_type(openai.error.Timeout)\n            | retry_if_exception_type(openai.error.APIError)\n            | retry_if_exception_type(openai.error.APIConnectionError)\n            | retry_if_exception_type(openai.error.RateLimitError)\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n        ),\n    )\n    def get_chat_completion_response(self, prompt, **kwargs):\n        \"\"\"\n        prompting chatgpt via openai api\n        now batching only works for completion, not on chat\n        \"\"\"\n        if openai.api_type == \"azure\":\n            try:\n                response = openai.ChatCompletion.create(\n                    deployment_id=self.engine,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    **kwargs,\n                )\n            except openai.InvalidRequestError as e:\n                # Most likely a content filtering error from Azure.\n                logging.warn(str(e))\n                return str(e)\n        else:\n            response = openai.ChatCompletion.create(\n                model=self.engine,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                **kwargs,\n            )\n\n        if \"content\" not in response[\"choices\"][0][\"message\"]:\n            return \"\"\n\n        output = response[\"choices\"][0][\"message\"][\"content\"].strip()\n        return output\n\n    @retry(\n        reraise=True,\n        stop=stop_after_attempt(100),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=(\n            retry_if_exception_type(openai.error.Timeout)\n            | retry_if_exception_type(openai.error.APIError)\n            | retry_if_exception_type(openai.error.APIConnectionError)\n            | retry_if_exception_type(openai.error.RateLimitError)\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n        ),\n    )\n    def get_completion_response(\n        self,\n        prompt_batch,\n        return_logprobs=False,\n        raw_logprobs=False,\n        top_logprobs=False,\n        **kwargs,\n    ):\n        \"\"\"\n        prompting gpt-3 via openai api\n        now batching only works for completion, not on chat\n        \"\"\"\n        logging.debug(kwargs)\n\n        try:\n            response = openai.Completion.create(\n                engine=self.engine,\n                prompt=prompt_batch,\n                logprobs=top_logprobs or 1,\n                **kwargs,\n            )\n        except openai.InvalidRequestError as e:\n            # Most likely a content filtering error from Azure.\n            if \"filtering\" in str(e):\n                logging.warn(str(e))\n                # Process each element in the batch individually.\n                response = {\"choices\": []}\n                for prompt in prompt_batch:\n                    try:\n                        response[\"choices\"].append(\n                            openai.Completion.create(\n                                engine=self.engine,\n                                prompt=prompt,\n                                logprobs=top_logprobs or 1,\n                                **kwargs,\n                            )[\"choices\"][0]\n                        )\n                    except openai.InvalidRequestError as e:\n                        response[\"choices\"].append(\n                            {\n                                \"text\": str(e),\n                                \"logprobs\": {\"token_logprobs\": [0], \"top_logprobs\": [{}], \"tokens\": {}},\n                            }\n                        )\n            else:\n                raise e\n\n        output = []\n        nlls = []\n        lengths = []\n        for response in response[\"choices\"]:\n            output.append(response[\"text\"].strip())\n            if raw_logprobs:\n                nlls.append(response[\"logprobs\"][\"token_logprobs\"])\n                lengths.append(response[\"logprobs\"][\"tokens\"])\n            elif top_logprobs:\n                nlls.append(response[\"logprobs\"][\"top_logprobs\"])\n                lengths.append(response[\"logprobs\"][\"tokens\"])\n            else:\n                if \"token_logprobs\" in response[\"logprobs\"]:\n                    nlls.append(sum(response[\"logprobs\"][\"token_logprobs\"]))\n                    lengths.append(len(response[\"logprobs\"][\"token_logprobs\"]))\n                else:\n                    nlls.append(-np.inf)\n                    lengths.append(1)\n\n        if return_logprobs:\n            output = list(zip(output, nlls, lengths))\n        return output\n\n    async def gather_chat_response(self, inputs, **generation_options):\n        outputs = await asyncio.gather(\n            *[\n                self.aget_chat_completion_response(_input, **generation_options)\n                for _input in inputs\n            ]\n        )\n        return outputs\n\n    def _mini_batch(self, inputs, batch_size=20):\n        input_length = len(inputs)\n        num_batches = input_length // batch_size + (\n            1 if input_length % batch_size > 0 else 0\n        )\n        for i in range(num_batches):\n            input_batch = inputs[batch_size * i : batch_size * (i + 1)]\n            yield input_batch\n\n    def generate(self, inputs, async_generation=True, batch_size=20, **kwargs):\n        if type(inputs) is not list:\n            inputs = [inputs]\n\n        kwargs.pop(\"output_space\", None)\n        generation_options = self.generation_options.copy()\n        generation_options.update(**kwargs)\n\n        if self.engine in (\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-32k\"):\n            if \"return_logprobs\" in generation_options:\n                del generation_options[\"return_logprobs\"]\n\n            if async_generation is True:\n                # async call api, devide to mini batches to avoid call rate limit\n                outputs = []\n                for input_batch in self._mini_batch(inputs, batch_size=10):\n                    outputs_batch = asyncio.run(\n                        self.gather_chat_response(input_batch, **generation_options)\n                    )\n                    outputs = outputs + outputs_batch\n            else:\n                # call api one by one\n                outputs = [\n                    self.get_chat_completion_response(_input, **generation_options)\n                    for _input in inputs\n                ]\n        else:\n            # devide to mini batches (max batch size = 20 according to openai)\n            outputs = []\n            for input_batch in self._mini_batch(inputs, batch_size=batch_size):\n                outputs_batch = self.get_completion_response(\n                    input_batch, **generation_options\n                )\n                outputs = outputs + outputs_batch\n        return outputs", "\nclass GPT:\n    AVAILABLE_MODELS = [\n        \"text-davinci-003\",\n        \"text-davinci-002\",\n        \"code-davinci-002\",\n        \"text-curie-001\",\n        \"text-babbage-001\",\n        \"text-ada-001\",\n        \"gpt-3.5-turbo\",\n        \"gpt-4\",\n        \"gpt-4-32k\",\n    ]\n\n    def __init__(self, model_name=\"text-davinci-003\", **generation_options):\n        if model_name not in self.AVAILABLE_MODELS:\n            raise ValueError(\n                f\"model_name should be one of: {','.join(self.AVAILABLE_MODELS)}\"\n            )\n        self.generation_options = generation_options\n        self.engine = model_name\n\n    @retry(\n        reraise=True,\n        stop=stop_after_attempt(100),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=(\n            retry_if_exception_type(openai.error.Timeout)\n            | retry_if_exception_type(openai.error.APIError)\n            | retry_if_exception_type(openai.error.APIConnectionError)\n            | retry_if_exception_type(openai.error.RateLimitError)\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n        ),\n    )\n    async def aget_chat_completion_response(self, prompt, **kwargs):\n        \"\"\"\n        prompting chatgpt via openai api\n        now batching only works for completion, not on chat\n        \"\"\"\n        if openai.api_type == \"azure\":\n            try:\n                response = await openai.ChatCompletion.acreate(\n                    deployment_id=self.engine,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    **kwargs,\n                )\n            except openai.InvalidRequestError as e:\n                # Most likely a content filtering error from Azure.\n                logging.warn(str(e))\n                return str(e)\n        else:\n            response = await openai.ChatCompletion.acreate(\n                model=self.engine,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                **kwargs,\n            )\n\n        if \"content\" not in response[\"choices\"][0][\"message\"]:\n            return \"\"\n\n        output = response[\"choices\"][0][\"message\"][\"content\"].strip()\n        return output\n\n    @retry(\n        reraise=True,\n        stop=stop_after_attempt(100),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=(\n            retry_if_exception_type(openai.error.Timeout)\n            | retry_if_exception_type(openai.error.APIError)\n            | retry_if_exception_type(openai.error.APIConnectionError)\n            | retry_if_exception_type(openai.error.RateLimitError)\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n        ),\n    )\n    def get_chat_completion_response(self, prompt, **kwargs):\n        \"\"\"\n        prompting chatgpt via openai api\n        now batching only works for completion, not on chat\n        \"\"\"\n        if openai.api_type == \"azure\":\n            try:\n                response = openai.ChatCompletion.create(\n                    deployment_id=self.engine,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    **kwargs,\n                )\n            except openai.InvalidRequestError as e:\n                # Most likely a content filtering error from Azure.\n                logging.warn(str(e))\n                return str(e)\n        else:\n            response = openai.ChatCompletion.create(\n                model=self.engine,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                **kwargs,\n            )\n\n        if \"content\" not in response[\"choices\"][0][\"message\"]:\n            return \"\"\n\n        output = response[\"choices\"][0][\"message\"][\"content\"].strip()\n        return output\n\n    @retry(\n        reraise=True,\n        stop=stop_after_attempt(100),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        retry=(\n            retry_if_exception_type(openai.error.Timeout)\n            | retry_if_exception_type(openai.error.APIError)\n            | retry_if_exception_type(openai.error.APIConnectionError)\n            | retry_if_exception_type(openai.error.RateLimitError)\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n        ),\n    )\n    def get_completion_response(\n        self,\n        prompt_batch,\n        return_logprobs=False,\n        raw_logprobs=False,\n        top_logprobs=False,\n        **kwargs,\n    ):\n        \"\"\"\n        prompting gpt-3 via openai api\n        now batching only works for completion, not on chat\n        \"\"\"\n        logging.debug(kwargs)\n\n        try:\n            response = openai.Completion.create(\n                engine=self.engine,\n                prompt=prompt_batch,\n                logprobs=top_logprobs or 1,\n                **kwargs,\n            )\n        except openai.InvalidRequestError as e:\n            # Most likely a content filtering error from Azure.\n            if \"filtering\" in str(e):\n                logging.warn(str(e))\n                # Process each element in the batch individually.\n                response = {\"choices\": []}\n                for prompt in prompt_batch:\n                    try:\n                        response[\"choices\"].append(\n                            openai.Completion.create(\n                                engine=self.engine,\n                                prompt=prompt,\n                                logprobs=top_logprobs or 1,\n                                **kwargs,\n                            )[\"choices\"][0]\n                        )\n                    except openai.InvalidRequestError as e:\n                        response[\"choices\"].append(\n                            {\n                                \"text\": str(e),\n                                \"logprobs\": {\"token_logprobs\": [0], \"top_logprobs\": [{}], \"tokens\": {}},\n                            }\n                        )\n            else:\n                raise e\n\n        output = []\n        nlls = []\n        lengths = []\n        for response in response[\"choices\"]:\n            output.append(response[\"text\"].strip())\n            if raw_logprobs:\n                nlls.append(response[\"logprobs\"][\"token_logprobs\"])\n                lengths.append(response[\"logprobs\"][\"tokens\"])\n            elif top_logprobs:\n                nlls.append(response[\"logprobs\"][\"top_logprobs\"])\n                lengths.append(response[\"logprobs\"][\"tokens\"])\n            else:\n                if \"token_logprobs\" in response[\"logprobs\"]:\n                    nlls.append(sum(response[\"logprobs\"][\"token_logprobs\"]))\n                    lengths.append(len(response[\"logprobs\"][\"token_logprobs\"]))\n                else:\n                    nlls.append(-np.inf)\n                    lengths.append(1)\n\n        if return_logprobs:\n            output = list(zip(output, nlls, lengths))\n        return output\n\n    async def gather_chat_response(self, inputs, **generation_options):\n        outputs = await asyncio.gather(\n            *[\n                self.aget_chat_completion_response(_input, **generation_options)\n                for _input in inputs\n            ]\n        )\n        return outputs\n\n    def _mini_batch(self, inputs, batch_size=20):\n        input_length = len(inputs)\n        num_batches = input_length // batch_size + (\n            1 if input_length % batch_size > 0 else 0\n        )\n        for i in range(num_batches):\n            input_batch = inputs[batch_size * i : batch_size * (i + 1)]\n            yield input_batch\n\n    def generate(self, inputs, async_generation=True, batch_size=20, **kwargs):\n        if type(inputs) is not list:\n            inputs = [inputs]\n\n        kwargs.pop(\"output_space\", None)\n        generation_options = self.generation_options.copy()\n        generation_options.update(**kwargs)\n\n        if self.engine in (\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-32k\"):\n            if \"return_logprobs\" in generation_options:\n                del generation_options[\"return_logprobs\"]\n\n            if async_generation is True:\n                # async call api, devide to mini batches to avoid call rate limit\n                outputs = []\n                for input_batch in self._mini_batch(inputs, batch_size=10):\n                    outputs_batch = asyncio.run(\n                        self.gather_chat_response(input_batch, **generation_options)\n                    )\n                    outputs = outputs + outputs_batch\n            else:\n                # call api one by one\n                outputs = [\n                    self.get_chat_completion_response(_input, **generation_options)\n                    for _input in inputs\n                ]\n        else:\n            # devide to mini batches (max batch size = 20 according to openai)\n            outputs = []\n            for input_batch in self._mini_batch(inputs, batch_size=batch_size):\n                outputs_batch = self.get_completion_response(\n                    input_batch, **generation_options\n                )\n                outputs = outputs + outputs_batch\n        return outputs", "\n\ndef forward_instantiate(model_name=\"text-davinci-003\", **generation_options):\n    global forward_interpreter\n\n    if forward_interpreter is None:\n        forward_interpreter = GPT(model_name, **generation_options)\n    else:\n        print(\"Forward interpreter already instantiated.\")\n        pass", "\n\ndef backward_instantiate(model_name=\"text-davinci-003\", **generation_options):\n    global backward_interpreter\n\n    if backward_interpreter is None:\n        backward_interpreter = GPT(model_name, **generation_options)\n    else:\n        print(\"Backward interpreter already instantiated.\")\n        pass", "\n\ndef forward_evaluate(input: List[str], **kwargs):\n    return forward_interpreter.generate(input, **kwargs)\n\n\ndef backward_evaluate(input: List[str], **kwargs):\n    return backward_interpreter.generate(input, **kwargs)\n", ""]}
{"filename": "dln/score.py", "chunked_list": ["import logging\nfrom dataclasses import dataclass\nfrom typing import Any, List\n\nimport numpy as np\n\nfrom dln.operator import forward_evaluate\n\n\n@dataclass\nclass ScoreRequest:\n    context: str\n    target: str\n    payload: Any = None", "\n@dataclass\nclass ScoreRequest:\n    context: str\n    target: str\n    payload: Any = None\n\n\n@dataclass\nclass OutputClasses:\n    protos: List[str]\n\n    def __iter__(self):\n        return iter(self.protos)\n\n    def __len__(self):\n        return len(self.protos)\n\n    def verbalizers(self, i):\n        return self.protos[i].split(\"|\")\n\n    def prototype(self, i):\n        return self.protos[i].split(\"|\")[0]", "@dataclass\nclass OutputClasses:\n    protos: List[str]\n\n    def __iter__(self):\n        return iter(self.protos)\n\n    def __len__(self):\n        return len(self.protos)\n\n    def verbalizers(self, i):\n        return self.protos[i].split(\"|\")\n\n    def prototype(self, i):\n        return self.protos[i].split(\"|\")[0]", "\n\n@dataclass\nclass LogProbs:\n    targets: np.ndarray\n    contexts: np.ndarray\n\n\nclass LogProbsScore:\n    def __init__(self, encoder=None):\n        if encoder is None:\n            import tiktoken\n\n            from dln.operator import forward_interpreter\n\n            encoder = tiktoken.encoding_for_model(forward_interpreter.engine)\n        self.encoder = encoder\n\n    def score_requests(self, requests, output_classes=None, agg=\"max\") -> LogProbs:\n        # create the batched inputs for the model\n        if output_classes is not None:\n            return self._forward_logprobs_score_api_with_classes(\n                [b.context for b in requests],\n                [b.target for b in requests],\n                output_classes,\n                agg=agg,\n            )\n        return self._forward_logprobs_score_api(\n            [b.context for b in requests],\n            [b.target for b in requests],\n        )\n\n    def _forward_logprobs_score_api_with_classes(\n        self, contexts, targets, output_classes, agg=\"max\"\n    ) -> LogProbs:\n        eval_kwargs = {\n            \"temperature\": 0.,\n            \"max_tokens\": 1,\n            \"echo\": False,\n            \"return_logprobs\": True,\n            \"raw_logprobs\": False,\n            \"top_logprobs\": 100,\n        }\n\n        unique_contexts = list(set(contexts))\n        context_to_position = {context: i for i, context in enumerate(unique_contexts)}\n        to_eval = [f\"{context}\\n\" for context in unique_contexts]\n\n        print(\"# Scoring requests = {}\".format(len(contexts)))\n        print(\"# Scoring unique requests = {}\".format(len(unique_contexts)))\n        eval_results = forward_evaluate(\n            to_eval,\n            async_generation=True,\n            **eval_kwargs,\n        )\n\n        top_logprobs = []\n        for context in contexts:\n            position = context_to_position[context]\n            context_top_logprobs = eval_results[position][1][0]\n            top_logprobs.append(dict(context_top_logprobs))\n\n        output_logprobs = []\n        output_distribs = []\n        for context, target, context_top_logprobs in zip(contexts, targets, top_logprobs):\n            position = context_to_position[context]\n\n            # make this fixed\n            if context_top_logprobs:\n                min_prob = np.exp(np.min(list(context_top_logprobs.values())))\n            else:\n                min_prob = 1e-6\n\n            output_classes_scores = np.asarray([min_prob for _ in output_classes])\n            # accumulate probability mass for each class verbalizer\n            # the class verbalizer can be either \" a\" or \"a\" (with or without space)\n            for i in range(len(output_classes)):\n                verbalizers = output_classes.verbalizers(i)\n                verbalizers.extend([f\" {v}\" for v in verbalizers])\n                verbalizers = set(verbalizers)\n                verbalizers_scores = [0.]\n                for verbalizer in verbalizers:\n                    if verbalizer in context_top_logprobs:\n                        prob_orig = np.exp(context_top_logprobs[verbalizer])\n                    else:\n                        prob_orig = min_prob\n                    verbalizers_scores.append(prob_orig)\n                if agg == \"max\":\n                    output_classes_scores[i] += np.max(verbalizers_scores)\n                else:\n                    output_classes_scores[i] += np.sum(verbalizers_scores)\n            output_class_index = [i for i, output_class in enumerate(output_classes) if target in output_class.split(\"|\")]\n            assert (\n                len(output_class_index) == 1\n            ), \"The target shouldn't appear in two output classes!\"\n            # accuracy here\n            output_classes_scores = output_classes_scores / output_classes_scores.sum()\n            output_logprobs.append(np.log(output_classes_scores[output_class_index[0]]))\n            output_distribs.append(output_classes_scores)\n        return LogProbs(np.asarray(output_logprobs), np.asarray(output_distribs))\n\n    def _forward_logprobs_score_api(self, contexts, targets) -> LogProbs:\n        logging.info(\"# Scoring requests = {}\".format(len(contexts)))\n\n        eval_kwargs = {\n            \"temperature\": 0,\n            \"max_tokens\": 0,\n            \"echo\": True,\n            \"return_logprobs\": True,\n            \"raw_logprobs\": True,\n        }\n\n        eval_batch = []\n        for context, target in zip(contexts, targets):\n            to_eval = f\"{context}\\n{target}\"\n            eval_batch.append(to_eval)\n\n        # there might be doubles in the eval_batch, so we need to\n        # only perform unique evals\n        unique_keys = list(set(eval_batch))\n        unique_keys_to_positions = {key: i for i, key in enumerate(unique_keys)}\n        unique_eval_results = forward_evaluate(\n            unique_keys,\n            async_generation=True,\n            **eval_kwargs,\n        )\n        # get the results in the same order as the eval_batch\n        eval_results = []\n        for eval_key in eval_batch:\n            eval_results.append(unique_eval_results[unique_keys_to_positions[eval_key]])\n        # get the nll results\n        log_probs = [eval_result[1] for eval_result in eval_results]\n        # get the logprobs results\n        output_logprobs = []\n        context_logprobs = []\n        for context, token_log_probs in zip(contexts, log_probs):\n            num_tokens_prompt = len(self.encoder.encode(context))\n            target_log_probs = token_log_probs[num_tokens_prompt:]\n            context_log_probs = token_log_probs[1:num_tokens_prompt]\n            output_logprobs.append(sum(target_log_probs) / (len(target_log_probs) + 1e-5))\n            context_logprobs.append(sum(context_log_probs) / (len(context_log_probs) + 1e-5))\n        return LogProbs(np.asarray(output_logprobs), np.asarray(context_logprobs))", "class LogProbsScore:\n    def __init__(self, encoder=None):\n        if encoder is None:\n            import tiktoken\n\n            from dln.operator import forward_interpreter\n\n            encoder = tiktoken.encoding_for_model(forward_interpreter.engine)\n        self.encoder = encoder\n\n    def score_requests(self, requests, output_classes=None, agg=\"max\") -> LogProbs:\n        # create the batched inputs for the model\n        if output_classes is not None:\n            return self._forward_logprobs_score_api_with_classes(\n                [b.context for b in requests],\n                [b.target for b in requests],\n                output_classes,\n                agg=agg,\n            )\n        return self._forward_logprobs_score_api(\n            [b.context for b in requests],\n            [b.target for b in requests],\n        )\n\n    def _forward_logprobs_score_api_with_classes(\n        self, contexts, targets, output_classes, agg=\"max\"\n    ) -> LogProbs:\n        eval_kwargs = {\n            \"temperature\": 0.,\n            \"max_tokens\": 1,\n            \"echo\": False,\n            \"return_logprobs\": True,\n            \"raw_logprobs\": False,\n            \"top_logprobs\": 100,\n        }\n\n        unique_contexts = list(set(contexts))\n        context_to_position = {context: i for i, context in enumerate(unique_contexts)}\n        to_eval = [f\"{context}\\n\" for context in unique_contexts]\n\n        print(\"# Scoring requests = {}\".format(len(contexts)))\n        print(\"# Scoring unique requests = {}\".format(len(unique_contexts)))\n        eval_results = forward_evaluate(\n            to_eval,\n            async_generation=True,\n            **eval_kwargs,\n        )\n\n        top_logprobs = []\n        for context in contexts:\n            position = context_to_position[context]\n            context_top_logprobs = eval_results[position][1][0]\n            top_logprobs.append(dict(context_top_logprobs))\n\n        output_logprobs = []\n        output_distribs = []\n        for context, target, context_top_logprobs in zip(contexts, targets, top_logprobs):\n            position = context_to_position[context]\n\n            # make this fixed\n            if context_top_logprobs:\n                min_prob = np.exp(np.min(list(context_top_logprobs.values())))\n            else:\n                min_prob = 1e-6\n\n            output_classes_scores = np.asarray([min_prob for _ in output_classes])\n            # accumulate probability mass for each class verbalizer\n            # the class verbalizer can be either \" a\" or \"a\" (with or without space)\n            for i in range(len(output_classes)):\n                verbalizers = output_classes.verbalizers(i)\n                verbalizers.extend([f\" {v}\" for v in verbalizers])\n                verbalizers = set(verbalizers)\n                verbalizers_scores = [0.]\n                for verbalizer in verbalizers:\n                    if verbalizer in context_top_logprobs:\n                        prob_orig = np.exp(context_top_logprobs[verbalizer])\n                    else:\n                        prob_orig = min_prob\n                    verbalizers_scores.append(prob_orig)\n                if agg == \"max\":\n                    output_classes_scores[i] += np.max(verbalizers_scores)\n                else:\n                    output_classes_scores[i] += np.sum(verbalizers_scores)\n            output_class_index = [i for i, output_class in enumerate(output_classes) if target in output_class.split(\"|\")]\n            assert (\n                len(output_class_index) == 1\n            ), \"The target shouldn't appear in two output classes!\"\n            # accuracy here\n            output_classes_scores = output_classes_scores / output_classes_scores.sum()\n            output_logprobs.append(np.log(output_classes_scores[output_class_index[0]]))\n            output_distribs.append(output_classes_scores)\n        return LogProbs(np.asarray(output_logprobs), np.asarray(output_distribs))\n\n    def _forward_logprobs_score_api(self, contexts, targets) -> LogProbs:\n        logging.info(\"# Scoring requests = {}\".format(len(contexts)))\n\n        eval_kwargs = {\n            \"temperature\": 0,\n            \"max_tokens\": 0,\n            \"echo\": True,\n            \"return_logprobs\": True,\n            \"raw_logprobs\": True,\n        }\n\n        eval_batch = []\n        for context, target in zip(contexts, targets):\n            to_eval = f\"{context}\\n{target}\"\n            eval_batch.append(to_eval)\n\n        # there might be doubles in the eval_batch, so we need to\n        # only perform unique evals\n        unique_keys = list(set(eval_batch))\n        unique_keys_to_positions = {key: i for i, key in enumerate(unique_keys)}\n        unique_eval_results = forward_evaluate(\n            unique_keys,\n            async_generation=True,\n            **eval_kwargs,\n        )\n        # get the results in the same order as the eval_batch\n        eval_results = []\n        for eval_key in eval_batch:\n            eval_results.append(unique_eval_results[unique_keys_to_positions[eval_key]])\n        # get the nll results\n        log_probs = [eval_result[1] for eval_result in eval_results]\n        # get the logprobs results\n        output_logprobs = []\n        context_logprobs = []\n        for context, token_log_probs in zip(contexts, log_probs):\n            num_tokens_prompt = len(self.encoder.encode(context))\n            target_log_probs = token_log_probs[num_tokens_prompt:]\n            context_log_probs = token_log_probs[1:num_tokens_prompt]\n            output_logprobs.append(sum(target_log_probs) / (len(target_log_probs) + 1e-5))\n            context_logprobs.append(sum(context_log_probs) / (len(context_log_probs) + 1e-5))\n        return LogProbs(np.asarray(output_logprobs), np.asarray(context_logprobs))", ""]}
{"filename": "dln/vi/model.py", "chunked_list": ["from collections import Counter\n\nimport numpy as np\nfrom termcolor import colored\n\nfrom dln.loss import LLoss\nfrom dln.score import OutputClasses\nfrom dln.vi.layers import PriorLayer, ResidualPriorLayer\nfrom dln.vi.sampler import PosteriorSampler, PromptSampler\nfrom dln.vi.utils import compute_pairwise_kl, log_message, ResultLogEntry", "from dln.vi.sampler import PosteriorSampler, PromptSampler\nfrom dln.vi.utils import compute_pairwise_kl, log_message, ResultLogEntry\n\n\nclass VILModel:\n    def __init__(\n        self,\n        loss_fn: LLoss,\n        task_description: str,\n        two_layers=True,\n        num_h_samples: int = 3,\n        num_p_samples: int = 5,\n        use_h_argmax: bool = False,\n        init_p1: str = None,\n        init_p2: str = None,\n        q_prompt: str = \"q_action_prompt:latest\",\n        q_hidden: str = \"suffix_forward_tbs:latest\",\n        p_hidden: str = \"suffix_forward_tbs:latest\",\n        p_class: str = \"classify_forward:latest\",\n        output_classes: OutputClasses = None,\n        strip_options_for_hidden: bool = False,\n        strip_answer_for_hidden: bool = False,\n        trust_factor: float = 0.0,\n        forward_use_classes: bool = False,\n        held_out_prompt_ranking: bool = False,\n        use_memory: int = 0,\n        train_p1: bool = True,\n        train_p2: bool = True,\n        logp_penalty: float = 0.0,\n        p1_max_tokens: int = 256,\n        p2_max_tokens: int = 20,\n        posterior_temp: float = 1.0,\n        strip_prefix_for_hidden: str = None,\n    ):\n        \"\"\"\n        Args:\n            loss_fn: loss function to use\n            task_description: task description, required\n            two_layers: whether to use two layers or one layer\n            num_h_samples: number of posterior samples to use for the hidden state\n            num_p_samples: number of posterior samples to use for the prompt\n            use_h_argmax: whether to use the argmax of the posterior distribution when selecting best prompts, if False, then\n                          we compute num_h_samples * num_p_samples scores and select prompts based on the sum of the num_h_samples scores\n            init_p1: initialization for the first prompt, if None, uses \"Decompose the problem to make it simpler:\"\n            init_p2: initialization for the second prompt, if None, uses task description\n            q_prompt: prompt for the posterior over the prompt\n            q_hidden: prompt for the posterior over the hidden state\n            p_hidden: forward template for the forward pass that generates the hidden state\n            p_class: forward template for the classification layer\n            output_classes: if specified, we compute log-likelihood over these classes only\n            strip_options_for_hidden: whether to strip the options from the input when computing the hidden state, don't use it.\n            strip_answer_for_hidden: whether to strip the answer from the input when computing the hidden state, don't use it.\n            trust_factor: trust factor for the KL divergence between the current prompt and the new prompt, it acts *only* at the last layer, a sort of step size.\n            forward_use_classes: whether to use the classes in the forward pass, if True, then we pick the class with the highest probability.\n            held_out_prompt_ranking: when proposing the prompts from the posterior, we only use HALF of the batch, kind of limiting over-fitting, but it decreases batch size\n                                     for posterior distribution.\n            use_memory: whether to use memory, if 0, we don't use memory, if n, we include n best DEV prompts in the list of candidate prompts to select from, etc...\n            train_p1: whether to train the first prompt\n            train_p2: whether to train the second prompt\n        \"\"\"\n        if task_description is None:\n            raise ValueError(\n                \"task_description must be provided when instantiating the VIModel.\"\n            )\n\n        self.encoder_l1 = ResidualPriorLayer(\n            forward_template=p_hidden,\n            init=init_p1 if init_p1 is not None else task_description,\n        )\n        self.encoder_l2 = PriorLayer(\n            forward_template=p_class,\n            init=init_p2 if init_p2 is not None else task_description,\n        )\n        if not two_layers:\n            self.encoder_l1.weight = None\n\n        self.prompt_sampler = PromptSampler(q_prompt)\n        self.q_sampler = PosteriorSampler(q_hidden)\n\n        self.trust_factor = trust_factor\n        self.strip_answer_for_hidden = strip_answer_for_hidden\n        self.strip_options_for_hidden = strip_options_for_hidden\n        self.strip_prefix_for_hidden = strip_prefix_for_hidden\n        self.output_classes = output_classes\n        self.two_layers = two_layers\n        self.loss_fn = loss_fn\n        self.num_h_samples = num_h_samples\n        self.num_p_samples = num_p_samples\n        self.use_h_argmax = use_h_argmax\n        self.task_description = task_description\n        self.forward_use_classes = forward_use_classes\n        self.held_out_prompt_ranking = held_out_prompt_ranking\n        self.use_memory = use_memory\n        self.train_p1 = train_p1\n        self.train_p2 = train_p2\n        self.logp_penalty = logp_penalty\n        self.p1_max_tokens = p1_max_tokens\n        self.p2_max_tokens = p2_max_tokens\n        self.posterior_temp = posterior_temp\n        self.num_p2_steps = 1\n        self.num_p1_steps = 1\n\n        if self.forward_use_classes:\n            assert (\n                self.output_classes is not None\n            ), \"Cannot use classes for forward without output classes\"\n        self.prompt_memory = []\n        self.result_entry = ResultLogEntry()\n\n    def get_from_memory(self, layer_index=0):\n        assert layer_index in [0, 1], \"Layer index out of bounds\"\n        return np.asarray([p[layer_index] for p in self.prompt_memory])\n\n    def add_to_memory(self, p1, p2, score):\n        \"\"\"\n        Max memory size is 2. Add (p1, p2, score) to memory and keep memory sorted.\n        Keep best two prompts in memory.\n        \"\"\"\n        if self.use_memory == 0:\n            raise ValueError(\"Cannot add to memory if use_memory is 0\")\n\n        self.prompt_memory.append((p1, p2, score))\n        self.prompt_memory = sorted(\n            self.prompt_memory, key=lambda x: x[2], reverse=True\n        )\n        if len(self.prompt_memory) > self.use_memory:\n            self.prompt_memory = self.prompt_memory[: self.use_memory][::-1]\n\n    def inference_one_layer(\n        self,\n        x: np.array,\n        y: np.array,\n        y_hat: np.array,\n        losses: np.array,\n    ):\n        batch_size = y.shape[0]\n\n        p_tilde_2: np.array = self.prompt_sampler.sample_q_p(\n            x,\n            y,\n            y_hat,\n            losses,\n            prompt=self.encoder_l2.weight,\n            num_samples=self.num_p_samples,\n            held_out_half=self.held_out_prompt_ranking,\n        )\n        if self.prompt_memory:\n            p_tilde_2 = np.concatenate([p_tilde_2, self.get_from_memory(1)], 0)\n\n        # sum over all samples\n        # build array: (num_samples, num_p_samples)\n        evals = []\n        for i in range(x.shape[0]):\n            for k in range(p_tilde_2.shape[0]):\n                evals.append((x[i], y[i], p_tilde_2[k]))\n\n        # batch_size, num_p_samples\n        ll = self.encoder_l2.log_p(\n            inputs=np.array([eval[0] for eval in evals]),\n            targets=np.array([eval[1] for eval in evals]),\n            prompts=np.array([eval[2] for eval in evals]),\n            output_classes=self.output_classes,\n            agg=\"sum\" if self.forward_use_classes else \"max\",\n        ).targets\n        # batch_size, num_p_samples\n        ll = ll.reshape(batch_size, p_tilde_2.shape[0])\n\n        p2_elbo = ll.mean(axis=0)\n        self.result_entry.log_candidates(p_tilde_2, p2_elbo)\n        best_p2 = p_tilde_2[np.argmax(p2_elbo)]\n        best_p2_elbo = np.max(p2_elbo)\n\n        log_message(\"--- P2 ---\")\n        for i, (p_tilde_2_i, p2_elbo_i) in enumerate(zip(p_tilde_2, p2_elbo)):\n            log_message(\"#\", i, \"ELBO\", p2_elbo_i, \",\", p_tilde_2_i)\n        log_message(\"----------\")\n\n        log_message(\"Best P2 Index: \", np.argmax(p2_elbo))\n        log_message(\"Best P2: \", best_p2)\n        return best_p2_elbo, None, best_p2\n\n    def sample_hidden_states(\n        self,\n        x,\n        y,\n        h1,\n        include_h1=False,\n        add_prior_term_to_score=True,\n        posterior_temp=1.0,\n    ):\n        # samples from the approx. posterior of h_1\n        # (batch_size, num_h_samples)\n        # q(h | x, y, p_1, p_2)\n        batch_size = x.shape[0]\n\n        if not self.num_h_samples and not include_h1:\n            raise ValueError(\"Must sample at least one h or include h1\")\n\n        if self.num_h_samples:\n            h_tilde_1 = self.q_sampler.sample_q_h(\n                x=x,\n                y=y,\n                h=h1,\n                prompt=self.encoder_l1.weight,\n                next_prompt=self.encoder_l2.weight,\n                num_samples=self.num_h_samples,\n            )\n\n        # concatenate the original sample\n        if include_h1:\n            log_message(\n                colored(\"Concatenating original sample to h_tilde_1!\", \"yellow\")\n            )\n            if self.num_h_samples:\n                h_tilde_1 = np.concatenate([h1[:, None], h_tilde_1], axis=1)\n            else:\n                h_tilde_1 = h1[:, None]\n\n        num_h_samples = h_tilde_1.shape[1]\n\n        ## TIGHTEN POSTERIOR APPROXIMATION...\n        ## e.g. compute log p(y | ~h, p_2, x) + log p(~h | x, p_1)\n        # compute log p(y | ~h, p_2) (residual connection added!)\n        x_repeat = x.repeat(num_h_samples, axis=0)\n        residual_h_tilde_1 = self.encoder_l1.apply_residual(\n            h_tilde_1.flatten(), x_repeat\n        ).reshape(batch_size, num_h_samples)\n\n        if num_h_samples > 1:\n            log_message(colored(\"Tightening posterior approximation...\", \"yellow\"))\n            y_repeat = y.repeat(num_h_samples, axis=0)\n            ll = self.encoder_l2.log_p(\n                inputs=residual_h_tilde_1.flatten(),\n                targets=y_repeat.flatten(),\n                output_classes=self.output_classes,\n                agg=\"sum\" if self.forward_use_classes else \"max\",\n            ).targets\n            ll = ll.reshape(batch_size, num_h_samples)\n\n            if add_prior_term_to_score:\n                # now compute the prior log-prob of ~h, log p(~h | x, p_1)\n                log_message(\n                    colored(\n                        \"Scoring posterior samples only with log-likelihood + prior\",\n                        \"yellow\",\n                    )\n                )\n                pr = self.encoder_l1.log_p(\n                    x_repeat, h_tilde_1.flatten()\n                ).targets.reshape(\n                    batch_size, num_h_samples\n                )\n                logits = pr + ll\n            else:\n                log_message(\n                    colored(\n                        \"Scoring posterior samples only with log-likelihood!\", \"yellow\"\n                    )\n                )\n                # we don't need to compute the prior log-prob of ~h\n                logits = ll\n        else:\n            logits = np.zeros((batch_size, num_h_samples))\n\n        # posterior weights for h_tilde_1, (batch_size, num_h_samples,)\n        weights = np.exp(logits / posterior_temp) / np.sum(\n            np.exp(logits / posterior_temp), axis=1, keepdims=True\n        )\n        assert (weights.sum(1).sum(0) - batch_size) < 1e-5\n\n        # get best hidden state\n        best_h_tilde_1_index: np.array = np.argmax(weights, axis=1)\n        residual_h_tilde_1_star = residual_h_tilde_1[\n            np.arange(batch_size), best_h_tilde_1_index\n        ]\n        h_tilde_1_star = h_tilde_1[np.arange(batch_size), best_h_tilde_1_index]\n        num_h_samples = h_tilde_1.shape[1]\n\n        log_message(\"Prior h:\", h1[0])\n        log_message(\"Best Posterior h:\", h_tilde_1_star[0])\n        log_message(\"Best Posterior index:\", best_h_tilde_1_index[0])\n        counter = Counter(best_h_tilde_1_index)\n        log_message(\"Best Posterior indices:\", counter)\n\n        if self.use_h_argmax:\n            h_tilde_1 = h_tilde_1_star[:, None]\n            residual_h_tilde_1 = residual_h_tilde_1_star[:, None]\n            weights = np.ones((batch_size, 1))\n\n        # return both samples and weights associated with them\n        return (\n            residual_h_tilde_1,\n            h_tilde_1,\n            h_tilde_1_star,\n            weights,\n        )\n\n    def compute_elbo_score(self, log_likes, class_weights=None):\n        \"\"\"\n        Args:\n            log_likes: (batch_size, num_h_samples, num_p_samples)\n        \"\"\"\n        if class_weights is None:\n            score = log_likes.mean(0)\n        else:\n            assert log_likes.shape[1] == class_weights.shape[1]\n            score = np.sum(log_likes * class_weights[:, :, None], axis=1).mean(0)\n        return score\n\n    def inference_vi(\n        self,\n        x: np.array,\n        h1: np.array,\n        r_h1: np.array,\n        y: np.array,\n        y_hat: np.array,\n        losses: np.array,\n    ):\n        batch_size = x.shape[0]\n        assert y.shape[0] == batch_size\n\n        # sample hidden states from the proposal distribution\n        (\n            residual_h_tilde_1,\n            h_tilde_1,\n            h_tilde_1_star,\n            weights,\n        ) = self.sample_hidden_states(\n            x=x,\n            y=y,\n            h1=h1,\n            include_h1=True,\n            add_prior_term_to_score=True,\n            posterior_temp=self.posterior_temp,\n        )\n        num_h_samples = h_tilde_1.shape[1]\n\n        log_message(colored(\"Number of h samples: {}\".format(num_h_samples), \"yellow\"))\n        log_message(\n            colored(\n                \"Norm. entropy of posterior q(h): {}\".format(\n                    -(weights * np.log(weights)).sum(-1).mean(0)\n                    / (np.log(weights.shape[1]) + 1e-5)\n                ),\n                \"yellow\",\n            )\n        )\n\n        # marginalize over posterior samples\n        # build array: (num_samples, num_h_samples, num_p_samples)\n        eval_batch_size = batch_size\n        eval_x = x\n        eval_weights = weights\n        eval_r_h_tilde_1 = residual_h_tilde_1\n        eval_y = y\n        eval_h_tilde_1 = h_tilde_1\n\n        if self.train_p2:\n            current_prompt = self.encoder_l2.weight\n            p2_elbos = []\n            for i in range(self.num_p2_steps):\n                # sample from the prompt distribution, (num_prompts,)\n                p_tilde_2: np.array = self.prompt_sampler.sample_q_p(\n                    inputs=r_h1,\n                    y=y,\n                    y_hat=y_hat,\n                    losses=losses,\n                    prompt=current_prompt,\n                    num_samples=self.num_p_samples,\n                    held_out_half=self.held_out_prompt_ranking,\n                )\n                if self.prompt_memory:\n                    p_tilde_2 = np.concatenate([p_tilde_2, self.get_from_memory(1)], 0)\n\n                evals = []\n                for i in range(eval_batch_size):\n                    for j in range(num_h_samples):\n                        for k in range(p_tilde_2.shape[0]):\n                            evals.append(\n                                (\n                                    eval_r_h_tilde_1[i, j],\n                                    eval_y[i],\n                                    p_tilde_2[k],\n                                )\n                            )\n                # batch_size, num_h_samples, num_p_samples\n                log_message(colored(\"Evaluating log likelihoods for p2...\", \"yellow\"))\n                ll = self.encoder_l2.log_p(\n                    inputs=np.array([eval[0] for eval in evals]),\n                    targets=np.array([eval[1] for eval in evals]),\n                    prompts=np.array([eval[2] for eval in evals]),\n                    output_classes=self.output_classes,\n                    agg=\"sum\" if self.forward_use_classes else \"max\",\n                ).targets\n                ll = ll.reshape(eval_batch_size, num_h_samples, p_tilde_2.shape[0])\n\n                if self.trust_factor > 0.0:\n                    evals = []\n                    for i in range(batch_size):\n                        for k in range(p_tilde_2.shape[0]):\n                            evals.append((r_h1[i], y[i], p_tilde_2[k]))\n                    lps = self.encoder_l2.log_p(\n                        inputs=np.array([eval[0] for eval in evals]),\n                        targets=np.array([eval[1] for eval in evals]),\n                        prompts=np.array([eval[2] for eval in evals]),\n                        output_classes=self.output_classes,\n                        agg=\"sum\" if self.forward_use_classes else \"max\",\n                    ).contexts\n                    lps = lps.reshape(batch_size, p_tilde_2.shape[0], -1)\n                    p2_kl = compute_pairwise_kl(lps)\n                else:\n                    p2_kl = np.zeros(p_tilde_2.shape[0])\n\n                p2_elbo = self.compute_elbo_score(ll[:, :, :], eval_weights)\n                p2_reward = p2_elbo - self.trust_factor * p2_kl\n                best_p2 = p_tilde_2[np.argmax(p2_reward)]\n                best_p2_elbo = np.max(p2_reward)\n                best_p2_index = np.argmax(p2_reward)\n                current_prompt = best_p2\n                p2_elbos.append(best_p2_elbo)\n            log_message(\"Optimization of P2... DONE.\", p2_elbos)\n        else:\n            p_tilde_2 = np.asarray([self.encoder_l2.weight])\n            p2_elbo = np.zeros(self.num_p_samples)\n            p2_kl = np.zeros(self.num_p_samples)\n            best_p2 = self.encoder_l2.weight\n            best_p2_elbo = 0.0\n            best_p2_index = 0\n\n        if self.train_p1:\n            # update w.r.t. p2 is done at this point, proceed with p1,\n            # sample proposals for the first layer prompt given the best ~h, h_tilde_1_star\n            current_prompt = self.encoder_l1.weight\n            p1_elbos = []\n\n            for i in range(self.num_p1_steps):\n                p_tilde_1: np.array = self.prompt_sampler.sample_q_p(\n                    inputs=x,\n                    y=h_tilde_1_star,\n                    y_hat=h1,\n                    losses=losses,\n                    prompt=current_prompt,\n                    num_samples=self.num_p_samples,\n                    held_out_half=self.held_out_prompt_ranking,\n                )\n                if self.prompt_memory:\n                    p_tilde_1 = np.concatenate([p_tilde_1, self.get_from_memory(0)], 0)\n\n                # marginalize over all posterior samples\n                # build array: (num_samples, num_h_samples, num_p_samples)\n                evals = []\n                eval_h_tilde_1 = np.concatenate([h1[:, None], eval_h_tilde_1], 1)\n                for i in range(eval_batch_size):\n                    for j in range(num_h_samples + 1):\n                        for k in range(p_tilde_1.shape[0]):\n                            evals.append(\n                                (\n                                    eval_x[i],\n                                    eval_h_tilde_1[i, j], \n                                    p_tilde_1[k],\n                                )\n                            )\n\n                # (batch_size, num_h_samples, num_p_samples)\n                log_message(colored(\"Evaluating log likelihoods for p1...\", \"yellow\"))\n\n                ll = self.encoder_l1.log_p(\n                    inputs=np.array([eval[0] for eval in evals]),\n                    targets=np.array([eval[1] for eval in evals]),\n                    prompts=np.array([eval[2] for eval in evals]),\n                ).targets\n                ll = ll.reshape(\n                    eval_batch_size,\n                    num_h_samples + 1,\n                    p_tilde_1.shape[0],\n                )\n                ll_orig = ll[:, 0, :]\n                p1_elbo = self.compute_elbo_score(ll[:, 1:, :], eval_weights)\n\n                # Compute an exploration like logp penalty.\n                if self.logp_penalty > 0.0:\n                    error_terms = np.where(losses > 0.0)[0]\n\n                    if len(error_terms) > 0:\n                        ll_errors = ll_orig[error_terms]\n                        p1_elbo = p1_elbo - self.logp_penalty * ll_errors.mean(0)\n\n                best_p1 = p_tilde_1[np.argmax(p1_elbo)]\n                best_p1_elbo = np.max(p1_elbo)\n                best_p1_index = np.argmax(p1_elbo)\n                current_prompt = best_p1\n                p1_elbos.append(best_p1_elbo)\n            log_message(\"Optimization of P1... DONE.\", p1_elbos)\n        else:\n            p_tilde_1 = np.asarray([self.encoder_l1.weight])\n            p1_elbo = np.zeros(self.num_p_samples)\n            best_p1 = self.encoder_l1.weight\n            best_p1_elbo = 0.0\n            best_p1_index = 0\n\n        self.result_entry.log_candidates(p_tilde_2, p2_elbo, p_tilde_1, p1_elbo)\n        log_message(\"--- P1 ---\")\n        for i, (p_tilde_1_i, p1_elbo_i) in enumerate(zip(p_tilde_1, p1_elbo)):\n            log_message(\"#\", i, \"ELBO:\", p1_elbo_i, \",\", p_tilde_1_i)\n\n        log_message(\"--- P2 ---\")\n        for i, (p_tilde_2_i, p2_elbo_i, p2_kl_i) in enumerate(\n            zip(p_tilde_2, p2_elbo, p2_kl)\n        ):\n            log_message(\"#\", i, \"ELBO:\", p2_elbo_i, \"XE:\", p2_kl_i, \",\", p_tilde_2_i)\n\n        log_message(\"Best P1 Index: \", best_p1_index)\n        log_message(\"Best P2 Index: \", best_p2_index)\n        log_message(\"Best P1: \", best_p1, best_p1_elbo)\n        log_message(\"Best P2: \", best_p2, best_p2_elbo)\n        return best_p1_elbo, best_p2_elbo, best_p1, best_p2\n\n    def strip_options(self, x):\n        \"\"\"\n        In bbh, there is the lame pre-processing that appends the options\n        to the input. This function removes them, this can be useful for\n        hidden states, where we don't want the model to output the option directly.\n        \"\"\"\n        x_ = []\n        for x_i in x:\n            if \"Options:\" in x_i:\n                x_i = x_i[: x_i.index(\"Options:\")].strip()\n            x_.append(x_i)\n        return np.array(x_)\n\n    def strip_answer(self, x):\n        \"\"\"\n        Strip 'Answer:' from the hidden state if the model generates it.\n        \"\"\"\n        x_ = []\n        for x_i in x:\n            if \"Answer:\" in x_i:\n                x_i = x_i[: x_i.index(\"Answer:\")].strip()\n            x_.append(x_i)\n        return np.array(x_)\n\n    def strip_prefix(self, x):\n        \"\"\"\n        Strip prefix from the hidden state if the model generates it.\n        \"\"\"\n        x_ = []\n        for x_i in x:\n            if self.strip_prefix_for_hidden in x_i:\n                x_i = x_i[\n                    x_i.index(self.strip_prefix_for_hidden)\n                    + len(self.strip_prefix_for_hidden) :\n                ].strip()\n            x_.append(x_i)\n        return np.array(x_)\n\n    def forward(self, x, y=None, temperature=0.0):\n        \"\"\"\n        Args:\n            temperature: temperature to use for the forward pass.\n        \"\"\"\n        # execute first template\n        if self.two_layers:\n            if self.strip_options_for_hidden:\n                x_stripped = self.strip_options(x)\n            else:\n                x_stripped = x\n\n            if self.strip_prefix_for_hidden:\n                x_stripped = self.strip_prefix(x_stripped)\n\n            h_1_out = self.encoder_l1(\n                x_stripped, temperature=temperature, max_tokens=self.p1_max_tokens\n            )\n            if self.strip_answer_for_hidden:\n                h_1_out = self.strip_answer(h_1_out)\n\n            # execute second template\n            h_1 = self.encoder_l1.apply_residual(h_1_out, x)\n            y_hat = self.encoder_l2(\n                h_1,\n                output_classes=self.output_classes\n                if self.forward_use_classes\n                else None,\n                temperature=temperature,\n            )\n        else:\n            h_1_out, h_1 = None, None\n            y_hat = self.encoder_l2(\n                x,\n                output_classes=self.output_classes\n                if self.forward_use_classes\n                else None,\n                temperature=temperature,\n                max_tokens=self.p2_max_tokens,\n            )\n        self.result_entry.log_hiddens(hiddens=h_1_out, size=len(x))\n        self.result_entry.log_outputs(outputs=y_hat)\n\n        y_hat = np.array([self.loss_fn.postproc(y_hat_i) for y_hat_i in y_hat])\n        if y is not None:\n            y = np.array([self.loss_fn.postproc(y_i) for y_i in y])\n            losses = self.loss_fn(y_hat, y)\n            if self.two_layers:\n                elbo1, elbo2, p1, p2 = self.inference_vi(\n                    x_stripped, h_1_out, h_1, y, y_hat, losses\n                )\n                elbo = elbo1 + elbo2\n                return elbo, p1, p2, np.mean(losses), elbo1, elbo2\n            else:\n                elbo, p1, p2 = self.inference_one_layer(x, y, y_hat, losses)\n                return elbo, p1, p2, np.mean(losses), 0.0, elbo\n        else:\n            return y_hat", ""]}
{"filename": "dln/vi/layers.py", "chunked_list": ["from typing import List\n\nimport numpy as np\n\nfrom dln.operator import forward_evaluate\nfrom dln.score import LogProbs, LogProbsScore, OutputClasses, ScoreRequest\nfrom dln.template import load_template\nfrom dln.vi.utils import log_message\n\n\nclass PriorLayer:\n    def __init__(self, forward_template, init=None):\n        self.forward_template = load_template(\n            forward_template\n        )\n        log_message(\"Forward template:\\n\", f\"{repr(self.forward_template.template)}\")\n        self.weight = init\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n    def forward(\n        self,\n        inputs,\n        output_classes: OutputClasses = None,\n        temperature=0.0,\n        strip_double_newlines=True,\n        max_tokens=256,\n    ) -> np.array:\n        \"\"\"Forward pass throught this layer.\n\n        Args:\n            output_classes: if not None, compute the constrained forward pass on the output classes, pick the highest probability amongst\n                            the prototypes.\n            temperature: temperature to use for the forward pass\n            strip_double_newlines: if True, strip any \"\\n\\n\" that might have been added\n            max_tokens: cap the max length for the forward pass\n        \"\"\"\n        if output_classes is None:\n            tpl_inputs = [\n                self.forward_template.render(input=input, prompt=self.weight)\n                for input in inputs\n            ]\n            outputs = forward_evaluate(\n                tpl_inputs,\n                stop=self.forward_template.stop_tokens,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n        else:\n            # compute constrained forward pass on the output classes\n            targets = [output_classes.prototype(0) for _ in inputs]\n            # compute log p of each output class, second return value is the p(class)\n            lp = self.log_p(\n                inputs, targets, output_classes=output_classes, agg=\"sum\"\n            ).contexts\n            # best output class index\n            best_output_class_index = np.argmax(lp, axis=1)\n            # get the best output class token\n            outputs = [output_classes.prototype(idx) for idx in best_output_class_index]\n        # strip any \"\\n\\n\" that might have been added\n        if strip_double_newlines:\n            outputs = [o.replace(\"\\n\\n\", \"\\n\") for o in outputs]\n        return np.asarray(outputs)\n\n    def log_p_request(self, input: str, target: str, prompt: str) -> ScoreRequest:\n        # build up a set of score requests\n        context = self.forward_template.render(input=input, prompt=prompt)\n        return ScoreRequest(context=context, target=target, payload=target)\n\n    def log_p(\n        self,\n        inputs: List[str],\n        targets: List[str],\n        prompts=None,\n        output_classes=None,\n        agg=\"max\",\n    ) -> LogProbs:\n        requests = []\n\n        if prompts is None:\n            prompts = [self.weight for _ in inputs]\n\n        for input, target, prompt in zip(inputs, targets, prompts):\n            requests.append(self.log_p_request(input, target, prompt=prompt))\n\n        # build up a set of score requests\n        logprobs = LogProbsScore().score_requests(requests, output_classes, agg=agg)\n        return logprobs", "\n\nclass PriorLayer:\n    def __init__(self, forward_template, init=None):\n        self.forward_template = load_template(\n            forward_template\n        )\n        log_message(\"Forward template:\\n\", f\"{repr(self.forward_template.template)}\")\n        self.weight = init\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n    def forward(\n        self,\n        inputs,\n        output_classes: OutputClasses = None,\n        temperature=0.0,\n        strip_double_newlines=True,\n        max_tokens=256,\n    ) -> np.array:\n        \"\"\"Forward pass throught this layer.\n\n        Args:\n            output_classes: if not None, compute the constrained forward pass on the output classes, pick the highest probability amongst\n                            the prototypes.\n            temperature: temperature to use for the forward pass\n            strip_double_newlines: if True, strip any \"\\n\\n\" that might have been added\n            max_tokens: cap the max length for the forward pass\n        \"\"\"\n        if output_classes is None:\n            tpl_inputs = [\n                self.forward_template.render(input=input, prompt=self.weight)\n                for input in inputs\n            ]\n            outputs = forward_evaluate(\n                tpl_inputs,\n                stop=self.forward_template.stop_tokens,\n                temperature=temperature,\n                max_tokens=max_tokens,\n            )\n        else:\n            # compute constrained forward pass on the output classes\n            targets = [output_classes.prototype(0) for _ in inputs]\n            # compute log p of each output class, second return value is the p(class)\n            lp = self.log_p(\n                inputs, targets, output_classes=output_classes, agg=\"sum\"\n            ).contexts\n            # best output class index\n            best_output_class_index = np.argmax(lp, axis=1)\n            # get the best output class token\n            outputs = [output_classes.prototype(idx) for idx in best_output_class_index]\n        # strip any \"\\n\\n\" that might have been added\n        if strip_double_newlines:\n            outputs = [o.replace(\"\\n\\n\", \"\\n\") for o in outputs]\n        return np.asarray(outputs)\n\n    def log_p_request(self, input: str, target: str, prompt: str) -> ScoreRequest:\n        # build up a set of score requests\n        context = self.forward_template.render(input=input, prompt=prompt)\n        return ScoreRequest(context=context, target=target, payload=target)\n\n    def log_p(\n        self,\n        inputs: List[str],\n        targets: List[str],\n        prompts=None,\n        output_classes=None,\n        agg=\"max\",\n    ) -> LogProbs:\n        requests = []\n\n        if prompts is None:\n            prompts = [self.weight for _ in inputs]\n\n        for input, target, prompt in zip(inputs, targets, prompts):\n            requests.append(self.log_p_request(input, target, prompt=prompt))\n\n        # build up a set of score requests\n        logprobs = LogProbsScore().score_requests(requests, output_classes, agg=agg)\n        return logprobs", "\n\nclass ResidualPriorLayer(PriorLayer):\n\n    def __init__(self, forward_template, init=None, residual_template=\"classify_residual\"):\n        super().__init__(forward_template, init=init)\n        self.residual_template = load_template(\n            residual_template\n        )\n        log_message(\"Residual template:\\n\", f\"{repr(self.residual_template.template)}\")\n\n    def forward(self, inputs, **kwargs) -> np.array:\n        outputs = super().forward(inputs, **kwargs)\n        return outputs\n\n    def apply_residual(\n        self, outputs: np.array, inputs: np.array, use_template=False\n    ) -> np.array:\n        outputs_ = []\n        if use_template:\n            for output, input in zip(outputs, inputs):\n                tpl_input = self.forward_template.render(\n                    input=input, prompt=self.weight\n                )\n                outputs_.append(\n                    self.residual_template.render(\n                        input=tpl_input, output=output\n                    )\n                )\n        else:\n            for output, input in zip(outputs, inputs):\n                outputs_.append(\n                    self.residual_template.render(\n                        input=input, output=output\n                    )\n                )\n        return np.array(outputs_)", ""]}
{"filename": "dln/vi/sampler.py", "chunked_list": ["import logging\nfrom dataclasses import dataclass\n\nimport numpy as np\nfrom dln.operator import backward_evaluate\nfrom dln.template import load_template\n\nfrom dln.vi.utils import log_message\n\n", "\n\n@dataclass\nclass Info:\n    input: str = None\n    output: str = None\n    target: str = None\n    loss: float = 0.0\n\n\nclass PromptSampler:\n    def __init__(self, p_template=\"q_action_prompt:v3.5\"):\n        self.prompt_template = load_template(p_template)\n        log_message(\"Prompt template:\\n\", f\"{repr(self.prompt_template.template)}\")\n        log_message(\n            \"Message alternatives:\\n\", f\"{self.prompt_template.message_alternatives}\"\n        )\n        self.evaluate_func = backward_evaluate\n\n    def sample_q_p(\n        self,\n        inputs: np.array,\n        y: np.array,\n        y_hat: np.array,\n        losses: np.array,\n        prompt: str,\n        num_samples=1,\n        held_out_half=False,\n    ):\n        \"\"\"\n        Args:\n            inputs: input sequences\n            y: target sequences\n            y_hat: predicted sequences\n            losses: losses for each sequence\n            prompt: prompt to use for sampling\n            num_samples: number of samples to generate\n            held_out_half: if True, only use the first half of the data points for sampling prompts\n        \"\"\"\n        infos = [\n            Info(input=input_i, output=y_hat_i, target=y_i, loss=loss)\n            for input_i, y_i, y_hat_i, loss in zip(inputs, y, y_hat, losses)\n        ]\n        while True:\n            try:\n                tpls = []\n                for i in range(num_samples - 1):\n                    if self.prompt_template.message_alternatives is None:\n                        message = None\n                    else:\n                        message = self.prompt_template.message_alternatives[\n                            i % len(self.prompt_template.message_alternatives)\n                        ]\n                    indices = np.random.permutation(np.arange(len(infos)))\n                    if held_out_half:\n                        infos_ = [infos[i] for i in indices[: len(infos) // 2]]\n                    else:\n                        infos_ = [infos[i] for i in indices]\n                    tpls.append(\n                        self.prompt_template.render(\n                            backward_infos=infos_,\n                            prompt=prompt,\n                            message=message,\n                        )\n                    )\n\n                log_message(\"Prompt Sampler:\", tpls[-1])\n                log_message(\"Generating {} ~p proposals...\".format(num_samples))\n\n                prompts = self.evaluate_func(\n                    tpls, stop=self.prompt_template.stop_tokens, n=1,\n                )\n                log_message(\"DONE...\")\n\n                prompts = np.array([prompt] + list(prompts))\n                return prompts\n            except KeyboardInterrupt:\n                break\n            except:\n                if len(infos) > 1:\n                    infos = infos[1:]\n                    logging.info(\"DROPPING A DATA POINT...\")\n                else:\n                    error_message = \"Still exeeding context length after shrinking backward_infos.\"\n                    logging.info(\n                        error_message\n                    )\n                    raise ValueError(error_message)", "\n\nclass PromptSampler:\n    def __init__(self, p_template=\"q_action_prompt:v3.5\"):\n        self.prompt_template = load_template(p_template)\n        log_message(\"Prompt template:\\n\", f\"{repr(self.prompt_template.template)}\")\n        log_message(\n            \"Message alternatives:\\n\", f\"{self.prompt_template.message_alternatives}\"\n        )\n        self.evaluate_func = backward_evaluate\n\n    def sample_q_p(\n        self,\n        inputs: np.array,\n        y: np.array,\n        y_hat: np.array,\n        losses: np.array,\n        prompt: str,\n        num_samples=1,\n        held_out_half=False,\n    ):\n        \"\"\"\n        Args:\n            inputs: input sequences\n            y: target sequences\n            y_hat: predicted sequences\n            losses: losses for each sequence\n            prompt: prompt to use for sampling\n            num_samples: number of samples to generate\n            held_out_half: if True, only use the first half of the data points for sampling prompts\n        \"\"\"\n        infos = [\n            Info(input=input_i, output=y_hat_i, target=y_i, loss=loss)\n            for input_i, y_i, y_hat_i, loss in zip(inputs, y, y_hat, losses)\n        ]\n        while True:\n            try:\n                tpls = []\n                for i in range(num_samples - 1):\n                    if self.prompt_template.message_alternatives is None:\n                        message = None\n                    else:\n                        message = self.prompt_template.message_alternatives[\n                            i % len(self.prompt_template.message_alternatives)\n                        ]\n                    indices = np.random.permutation(np.arange(len(infos)))\n                    if held_out_half:\n                        infos_ = [infos[i] for i in indices[: len(infos) // 2]]\n                    else:\n                        infos_ = [infos[i] for i in indices]\n                    tpls.append(\n                        self.prompt_template.render(\n                            backward_infos=infos_,\n                            prompt=prompt,\n                            message=message,\n                        )\n                    )\n\n                log_message(\"Prompt Sampler:\", tpls[-1])\n                log_message(\"Generating {} ~p proposals...\".format(num_samples))\n\n                prompts = self.evaluate_func(\n                    tpls, stop=self.prompt_template.stop_tokens, n=1,\n                )\n                log_message(\"DONE...\")\n\n                prompts = np.array([prompt] + list(prompts))\n                return prompts\n            except KeyboardInterrupt:\n                break\n            except:\n                if len(infos) > 1:\n                    infos = infos[1:]\n                    logging.info(\"DROPPING A DATA POINT...\")\n                else:\n                    error_message = \"Still exeeding context length after shrinking backward_infos.\"\n                    logging.info(\n                        error_message\n                    )\n                    raise ValueError(error_message)", "\n\nclass PosteriorSampler:\n    def __init__(self, q_template):\n        self.q_templates = []\n        for q_template in q_template.split(\"|\"):\n            self.q_templates.append(load_template(q_template))\n        for q_template in self.q_templates:\n            log_message(\"Q template:\", f\"{repr(q_template.template)}\")\n        self.stop_tokens = self.q_templates[0].stop_tokens\n        self.evaluate_func = backward_evaluate\n\n    def sample_q_h(\n        self,\n        x: np.array,\n        y: np.array,\n        h: np.array,\n        prompt: str,\n        next_prompt: str,\n        num_samples=1,\n        strip_double_newlines=True,\n    ):\n        \"\"\"\n        Sample a new hidden state from the posterior distribution.\n\n        Args:\n            x: inputs\n            y: labels\n            y_hat: model predictions for the forward pass\n            h: hidden states for the forward pass\n            task_description: task description if any\n            prompt: prompt for the layer that generated h\n            next_prompt: prompt for the layer above h\n            forward_template: template for the forward pass that generated h\n            num_samples: number of samples to generate\n            strip_double_newlines: strip double new lines from the output samples\n        Returns\n            (batch_size, num_samples) array of hidden states\n        \"\"\"\n        tpls = []\n\n        for x_i, h_i, y_i in zip(x, h, y):\n            for j in range(num_samples):\n                # pick a template at random\n                q_template = self.q_templates[\n                    np.random.choice(np.arange(len(self.q_templates)))\n                ]\n                if q_template.message_alternatives is not None:\n                    message = q_template.message_alternatives[\n                        j % len(q_template.message_alternatives)\n                    ]\n                else:\n                    message = None\n                tpl = q_template.render(\n                    input=x_i,\n                    h=h_i,\n                    prompt=prompt,\n                    next_prompt=next_prompt,\n                    y=y_i,\n                    message=message,\n                )\n                tpls.append(tpl)\n\n        # WATCH OUT: we only use max_tokens=128\n        max_tokens = 256\n        assert len(\n            tpls\n        ), \"If we are here, it means that either we resample hidden states, or that there are some errors.\"\n\n        # this might happen when all memories are correct\n        log_message(\"Q proposals: \" + str(len(tpls)) + \", Q template:\" + \"\\n\" + tpls[0])\n        log_message(\n            \"Generating {} ~h proposals... max_tokens={}\".format(\n                num_samples, max_tokens\n            )\n        )\n        sampled = self.evaluate_func(\n            tpls,\n            stop=self.stop_tokens,\n            n=1,\n            max_tokens=max_tokens,\n        )\n\n        # strip any \"\\n\\n\" that might have been added\n        if strip_double_newlines:\n            sampled = [s.replace(\"\\n\\n\", \"\\n\") for s in sampled]\n\n        sampled = np.asarray(sampled).reshape(x.shape[0], num_samples)\n        assert sampled.shape[0] == x.shape[0]\n        return sampled", ""]}
{"filename": "dln/vi/utils.py", "chunked_list": ["import logging\nimport os\nimport copy\nimport json\nfrom typing import Dict\n\nimport numpy as np\n\n\n# Use this function to log messages to the file\ndef log_message(*messages):\n    print(*messages)\n    logging.info(\" \".join(map(str, messages)))", "\n# Use this function to log messages to the file\ndef log_message(*messages):\n    print(*messages)\n    logging.info(\" \".join(map(str, messages)))\n\n\ndef compute_pairwise_kl(lps):\n    \"\"\"We make updates constrained by the KL divergence between the function induced by the current prompt\n    and the function induced by the new prompt. We compute the KL divergence\n    between the functions induced by the prompts.\n\n    The distribution induced by the current prompt is the first element of the second axis in lps.\n    \"\"\"\n    # compute pairwise kl, considers reference always as the first prompt\n    return (\n        (lps[:, :1, :] * (np.log(lps[:, :1, :]) - np.log(lps[:, :, :]))).sum(-1).mean(0)\n    )", "\nclass ResultLogEntry():\n    def __init__(self):\n        self.hiddens = None\n        self.candidates = [[], []]\n        self.metrics = {}\n        self.outputs = []\n\n    def log_metric(self, metric: str, value: float):\n        self.metrics[metric] = value\n\n    def log_outputs(self, outputs):\n        self.outputs = outputs\n\n    def log_hiddens(self, hiddens, size):\n        self.hiddens = [[]] * size if hiddens is None else [[h] for h in hiddens]\n\n    def log_candidates(self, p_tilde_2, p2_elbo, p_tilde_1=None, p1_elbo=None):\n        \"\"\"\n            If one_layer, p_tilde_1 and p1_elbo are None,\n            and we only store the two-layer candidates in the 0th list element. 1st element stays [].\n            If two_layer, we store the first layer candidates in the 0th list element\n            and the second layer candidates in the 1st list element.\n        \"\"\"\n        self.candidates = [[],[]]\n        if p_tilde_1 is not None:\n            for i in range(p_tilde_1.shape[0]):\n                self.candidates[0].append({\n                    \"layer\": p_tilde_1[i],\n                    \"score\": p1_elbo[i],\n                })\n            p2_ind = 1\n        else:\n            p2_ind = 0\n        for i in range(p_tilde_2.shape[0]):\n            self.candidates[p2_ind].append({\n                \"layer\": p_tilde_2[i],\n                \"score\": p2_elbo[i],\n            })", "\n\nclass ResultLogWriter(object):\n    def __init__(self, dataset: str, path: str, name: str = None):\n        \"\"\"\n        Args:\n            dataset: The name of the dataset (used as name if save_name is None)\n            save_name: Dictionary key to save the results under\n        Returns:\n            A ResultLogWriter object\n        \"\"\"\n        self.name = name if name is not None else dataset\n        self.path = path\n        self.result_dict = {}\n        self.result_dict[self.name] = {'training': [], 'examples': []}\n\n    def write_result(self, step, layers, metrics, candidates):\n        self.result_dict[self.name]['training'].append({'step': step})\n        self.result_dict[self.name]['training'][-1]['layers'] = copy.deepcopy(layers)\n        self.result_dict[self.name]['training'][-1]['metrics'] = copy.deepcopy(metrics)\n        self.result_dict[self.name]['training'][-1]['candidates'] = copy.deepcopy(candidates)\n\n    def write_examples(self, step, inputs, labels, outputs, hiddens):\n        \"\"\"\n        Args:\n            step: The iteration number\n            inputs: A list of input strings\n            labels: A list of label strings\n            outputs: A list of output strings\n            hiddens: A list of hidden strings for two-layer-dlns\n        An element of the \"examples\" list in the json file looks like:\n        {\n            \"input\": \"Do cats sit on mats?\",\n            \"label\": \"Yes\",\n            \"trace\": [\n                {\n                    \"step\": 0,\n                    \"hiddens\": [\"Cats are picky.\"],\n                    \"output\": \"No\"\n                },\n                {\n                    \"step\": 1,\n                    \"hiddens\": [\"Cats would sit anywhere.\"],\n                    \"output\": \"Yes\"\n                }\n            ]\n        }\n        \"\"\"\n        for inp, lab, outp, hidden in zip(inputs, labels, outputs, hiddens):\n            # Get the element in the list of examples that matches the input\n            example = next((ex for ex in self.result_dict[self.name]['examples'] if ex['input'] == inp), None)\n            if example is None:\n                self.result_dict[self.name]['examples'].append({\n                    \"input\": inp,\n                    \"label\": lab,\n                    \"trace\": [{\"step\": step, \"hiddens\": hidden, \"output\": outp}],\n                })\n            else:\n                example['trace'].append({\"step\": step, \"hiddens\": hidden, \"output\": outp})\n\n    def save_to_json_file(self):\n        # self.path is a path to a file\n        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n        try:\n            with open(self.path, 'r') as f:\n                print('Loading existing json file %s' % self.path)\n                loaded_dict = json.load(f)\n        except FileNotFoundError:\n            loaded_dict = {}\n        if self.name not in loaded_dict:\n            # Append or add the json dictionary if the result doesn't exist\n            loaded_dict[self.name] = self.result_dict[self.name]\n            with open(self.path, 'w') as f:\n                json.dump(loaded_dict, f, indent=4)\n        else:\n            print(f\"Result named {self.name} already exists in {self.path}!\")"]}
{"filename": "projects/demo/demo.py", "chunked_list": ["import argparse\nimport json\nimport textwrap\n\nimport dash\nimport dash_bootstrap_components as dbc\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom dash import dcc, html", "import plotly.graph_objects as go\nfrom dash import dcc, html\nfrom dash.dependencies import Input, Output\nfrom jinja2 import Template\nfrom plotly.subplots import make_subplots\n\nforward_template_L1 = Template(\n    \"{{ input }}\\n\\n{{ prompt }} Let's think step by step.\"\n)  # Loaded template suffix_forward_tbs v1.0\nforward_template_L2 = Template(", ")  # Loaded template suffix_forward_tbs v1.0\nforward_template_L2 = Template(\n    \"{{ prompt }}\\n\\n{{ input }}\\n\\nAnswer:\"\n)  #  Loaded template classify_forward v3.0\n\nDATASETS = [\n    (\"subj\", \"1 Layer - Subj\"),\n    (\"hyperbaton\", \"1 Layer - Hyperbaton\"),\n    (\"navigate\", \"2 Layers - Navigate\"),\n]", "    (\"navigate\", \"2 Layers - Navigate\"),\n]\n\n\ndef wrap_text(text, width=100):\n    text = text.replace(\"\\n\\n\", \"\\n\")\n    return \"\\n\".join(\"\\n\".join(textwrap.wrap(line, width)) for line in text.split(\"\\n\"))\n\n\ndef load_data(log_file, dataset):\n    with open(log_file) as f:\n        logs = json.load(f)[dataset]\n\n    flattened_data = []\n    flattened_candidates = []\n    for item in logs[\"training\"]:\n        flat_item = {\"step\": item[\"step\"]}\n        flat_item.update(\n            {\n                metric: value if value is not None else np.nan\n                for metric, value in item[\"metrics\"].items()\n            }\n        )\n        flat_item.update(\n            {f\"layer_{i}\": wrap_text(l) for i, l in enumerate(item[\"layers\"], 1)}\n        )\n        flattened_data.append(flat_item)\n        candidates_data = {}\n        for layer, candidates in enumerate(item[\"candidates\"], 1):\n            for idx, candidate in enumerate(candidates):\n                candidate_data = candidates_data.setdefault(idx, {\"step\": item[\"step\"]})\n                candidate_data[f\"layer_{layer}_candidate\"] = candidate[\"layer\"]\n                candidate_data[f\"layer_{layer}_score\"] = candidate[\"score\"]\n        flattened_candidates += list(candidates_data.values())\n\n    flattened_examples = []\n    for i, example in enumerate(logs[\"examples\"]):\n        for item in example[\"trace\"]:\n            flat_item = {\n                \"id\": i + 1,\n                \"input\": wrap_text(example[\"input\"]),\n                \"label\": example[\"label\"],\n                \"step\": item[\"step\"],\n                \"hidden\": wrap_text(item[\"hiddens\"][0]) if item[\"hiddens\"] else \"\",\n                \"output\": wrap_text(item[\"output\"]),\n            }\n            flattened_examples.append(flat_item)\n\n    return (\n        pd.DataFrame(flattened_data),\n        pd.DataFrame(flattened_candidates).dropna(),\n        pd.DataFrame(flattened_examples),\n    )", "\ndef load_data(log_file, dataset):\n    with open(log_file) as f:\n        logs = json.load(f)[dataset]\n\n    flattened_data = []\n    flattened_candidates = []\n    for item in logs[\"training\"]:\n        flat_item = {\"step\": item[\"step\"]}\n        flat_item.update(\n            {\n                metric: value if value is not None else np.nan\n                for metric, value in item[\"metrics\"].items()\n            }\n        )\n        flat_item.update(\n            {f\"layer_{i}\": wrap_text(l) for i, l in enumerate(item[\"layers\"], 1)}\n        )\n        flattened_data.append(flat_item)\n        candidates_data = {}\n        for layer, candidates in enumerate(item[\"candidates\"], 1):\n            for idx, candidate in enumerate(candidates):\n                candidate_data = candidates_data.setdefault(idx, {\"step\": item[\"step\"]})\n                candidate_data[f\"layer_{layer}_candidate\"] = candidate[\"layer\"]\n                candidate_data[f\"layer_{layer}_score\"] = candidate[\"score\"]\n        flattened_candidates += list(candidates_data.values())\n\n    flattened_examples = []\n    for i, example in enumerate(logs[\"examples\"]):\n        for item in example[\"trace\"]:\n            flat_item = {\n                \"id\": i + 1,\n                \"input\": wrap_text(example[\"input\"]),\n                \"label\": example[\"label\"],\n                \"step\": item[\"step\"],\n                \"hidden\": wrap_text(item[\"hiddens\"][0]) if item[\"hiddens\"] else \"\",\n                \"output\": wrap_text(item[\"output\"]),\n            }\n            flattened_examples.append(flat_item)\n\n    return (\n        pd.DataFrame(flattened_data),\n        pd.DataFrame(flattened_candidates).dropna(),\n        pd.DataFrame(flattened_examples),\n    )", "\ndef load_dataset_names(log_file):\n    with open(log_file) as f:\n        logs = json.load(f)\n    return [(x, x) for x in list(logs.keys())]\n\ndef main(args):\n    datasets = load_dataset_names(args.logfile) if args.logfile else DATASETS\n    app = dash.Dash()\n    app.layout = html.Div(\n        [\n            html.H2(\n                \"Deep Language Networks\",\n                style={\n                    \"textAlign\": \"center\",\n                },\n            ),\n            dcc.Dropdown(\n                id=\"dataset_dropdown\",\n                options=[\n                    {\"label\": f\"{title}\", \"value\": id_} for id_, title in datasets\n                ],\n                value=datasets[0][0],\n                multi=False,\n                style={\n                    \"backgroundColor\": \"rgb(229, 236, 246)\",\n                    \"margin\": \"10px 0\",\n                },\n            ),\n            dcc.Dropdown(\n                id=\"example_dropdown\",\n                options=[\n                    {\n                        \"label\": f\"Example {i}\" if i > 0 else \"Show only prompts\",\n                        \"value\": i,\n                    }\n                    for i in range(0, 20 + 1)\n                ],\n                value=0,  # df['id'].iloc[0],\n                multi=False,\n                style={\n                    \"backgroundColor\": \"rgb(229, 236, 246)\",\n                    \"margin\": \"10px 0\",\n                },\n            ),\n            dcc.Graph(id=\"scatter-plot\"),\n            html.Div(\n                id=\"table-container\",\n                style={\n                    \"backgroundColor\": \"rgb(229, 236, 246)\",\n                    \"margin\": \"10px 0\",\n                    \"padding\": \"10px\",\n                },\n            ),\n        ]\n    )\n\n    # Create a callback to update the table-container\n    @app.callback(\n        Output(\"table-container\", \"children\"),\n        [Input(\"scatter-plot\", \"hoverData\"), Input(\"dataset_dropdown\", \"value\")],\n    )  # coulbe be either clickData, hoverData\n    def update_table(callbackData, dataset_dropdown):\n        df, candidates, examples = load_data(\n            args.logfile or \"data.json\", dataset_dropdown\n        )\n\n        # Merge layers and examples\n        df = df.merge(examples, on=\"step\", how=\"left\")\n\n        step = callbackData[\"points\"][0][\"x\"] if callbackData is not None else 0\n        filtered_df = candidates[candidates[\"step\"] == step]\n        table = dbc.Table.from_dataframe(\n            filtered_df, striped=True, bordered=True, hover=True\n        )\n        return table\n\n    @app.callback(\n        Output(\"scatter-plot\", \"figure\"),\n        [Input(\"example_dropdown\", \"value\"), Input(\"dataset_dropdown\", \"value\")],\n    )\n    def update_scatter_plot(example_dropdown, dataset_dropdown):\n        df, candidates, examples = load_data(\n            args.logfile or \"data.json\", dataset_dropdown\n        )\n\n        # Merge layers and examples\n        df = df.merge(examples, on=\"step\", how=\"left\")\n\n        EXAMPLE_ID = example_dropdown or 1\n        dev_df = df\n        dev_df = df[df[\"id\"] == EXAMPLE_ID]\n        dev_df = dev_df[dev_df[\"dev_acc\"] >= 0]\n\n        NB_LAYERS = len([c for c in dev_df.columns if c.startswith(\"layer\")])\n\n        if example_dropdown == 0:\n            if NB_LAYERS == 1:\n                layers_columns = [c for c in dev_df.columns if c.startswith(\"layer\")]\n                for column in layers_columns:\n                    # Wrap text for display in hover\n                    dev_df[column] = dev_df[column].apply(\n                        lambda x: x.replace(\"\\n\", \"<br>\")\n                    )\n\n                hover_template = \"<b> prompt:</b> %{customdata[0]}\"\n\n            elif NB_LAYERS == 2:\n                layers_columns = [c for c in dev_df.columns if c.startswith(\"layer\")]\n                for column in layers_columns:\n                    # Wrap text for display in hover\n                    dev_df[column] = dev_df[column].apply(\n                        lambda x: x.replace(\"\\n\", \"<br>\")\n                    )\n\n                hover_template = (\n                    \"<b>Layer 1 prompt:</b> %{customdata[0]}\"\n                    \"<br><b>Layer 2 prompt:</b> %{customdata[1]}\"\n                )\n        else:\n            if NB_LAYERS == 1:\n                layers_columns = [\n                    c for c in dev_df.columns if c.startswith(\"layer\")\n                ] + [\"input\", \"output\", \"label\"]\n                for column in layers_columns:\n                    # Wrap text for display in hover\n                    dev_df[column] = dev_df[column].apply(\n                        lambda x: x.replace(\"\\n\", \"<br>\")\n                    )\n\n                hover_template = (\n                    \"<b>Input:</b> %{customdata[1]}\"\n                    \"<br><b>Layer 1 prompt:</b> %{customdata[0]}\"\n                    \"<br><b>Output:</b> %{customdata[2]}\"\n                    \"<br><b>Label:</b> %{customdata[3]}\"\n                )\n\n            elif NB_LAYERS == 2:\n                layers_columns = [\n                    c for c in dev_df.columns if c.startswith(\"layer\")\n                ] + [\"input\", \"hidden\", \"output\", \"label\"]\n                for column in layers_columns:\n                    # Wrap text for display in hover\n                    dev_df[column] = dev_df[column].apply(\n                        lambda x: x.replace(\"\\n\", \"<br>\")\n                    )\n\n                hover_template = (\n                    \"<b>Input:</b> %{customdata[2]}\"\n                    \"<br><b>Layer 1 prompt:</b> %{customdata[0]}\"\n                    \"<br><b>Hidden:</b> %{customdata[3]}\"\n                    \"<br><b>Layer 2 prompt:</b> %{customdata[1]}\"\n                    \"<br><b>Output:</b> %{customdata[4]}\"\n                    \"<br><b>Label:</b> %{customdata[5]}\"\n                )\n            else:\n                raise NotImplementedError()\n\n        hover_config = {\n            \"customdata\": dev_df[layers_columns],\n            \"hovertemplate\": hover_template,\n        }\n\n        # Create figure with secondary y-axis\n        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n        # Make Figure taller\n        fig.update_layout(\n            autosize=False,\n            width=1900,\n            height=1000,\n            margin=dict(l=50, r=50, b=100, t=100, pad=4),\n        )\n\n        # Add traces\n        # text = [\"Acc\"] * len(df[\"step\"])\n        fig.add_trace(\n            go.Scatter(\n                x=df[\"step\"], y=df[\"run_acc\"], name=\"Running acc\", hoverinfo=\"none\"\n            ),  # , text=text, **hover_config\n            secondary_y=False,\n        )\n\n        # text = [\"Dev Acc\"] * len(df[\"step\"][df[\"dev_acc\"] >= 0])\n        fig.add_trace(\n            go.Scatter(\n                x=dev_df[\"step\"], y=dev_df[\"dev_acc\"], name=\"Dev acc\", **hover_config\n            ),\n            secondary_y=False,\n        )\n\n        # text = [\"ELBO\"] * len(df[\"step\"])\n        fig.add_trace(\n            go.Scatter(\n                x=df[\"step\"],\n                y=df[\"run_elbo\"],\n                name=\"Running ELBO\",\n                hoverinfo=\"none\",\n                visible=\"legendonly\",\n            ),  # , text=text, **hover_config),\n            secondary_y=True,\n        )\n\n        # Set x-axes title\n        fig.update_xaxes(title_text=\"steps\", nticks=20)\n\n        # Set y-axes titles\n        fig.update_yaxes(title_text=\"<b>Accuracy</b>\", secondary_y=False)\n        fig.update_yaxes(title_text=\"<b>ELBO</b>\", secondary_y=True)\n\n        # Define hover text font and color.\n        fig.update_layout(hovermode=\"x\")\n        fig.update_layout(\n            hoverlabel=dict(\n                bgcolor=\"rgba(255,255,255,0.75)\",\n                font_size=16,\n                font_family=\"Rockwell\",\n            ),\n        )\n        return fig\n\n    app.run_server(debug=args.debug, host=args.dash_host or \"127.0.0.1\")", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"logfile\", nargs=\"?\", help=\"Log file to use (JSON).\")\n    parser.add_argument(\n        \"--dash-host\", help=\"Host for Dash (setting this, implies --dash).\"\n    )\n    parser.add_argument(\n        \"--debug\", action=argparse.BooleanOptionalAction, help=\"Launch in debug mode.\"\n    )\n    args = parser.parse_args()\n\n    main(args)", ""]}
{"filename": "projects/vi_dln/vi_main.py", "chunked_list": ["import datetime\nimport json\nimport logging\nimport os\nfrom collections import Counter\n\nimport click\nimport numpy as np\nimport tqdm\nfrom termcolor import colored", "import tqdm\nfrom termcolor import colored\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom dln.dataset import init_dataset\nfrom dln.loss import ZeroOneLoss\nfrom dln.operator import backward_instantiate, forward_instantiate\nfrom dln.postprocessing import postprocess_prediction\nfrom dln.vi.model import VILModel, log_message\nfrom dln.vi.utils import ResultLogWriter", "from dln.vi.model import VILModel, log_message\nfrom dln.vi.utils import ResultLogWriter\n\ntry:\n    import wandb\n    wandb_installed = True\nexcept ImportError:\n    wandb_installed = False\n\n\ndef init_prompts(dataset, init_p1, init_p2):\n    \"\"\"Initialize the prompts for the two layers of the model.\n    If init_p1 or init_p2 is a json file, load the best weights from the json file.\n    \"\"\"\n\n    if init_p1 and init_p1.endswith(\".json\"):\n        with open(init_p1) as f:\n            best_weights = json.load(f)\n        init_p1 = best_weights[dataset][\"best_weights\"]\n    elif init_p2 and init_p2.endswith(\".json\"):\n        with open(init_p2) as f:\n            best_weights = json.load(f)\n        init_p2 = best_weights[dataset][\"best_weights\"]\n    return init_p1, init_p2", "\n\ndef init_prompts(dataset, init_p1, init_p2):\n    \"\"\"Initialize the prompts for the two layers of the model.\n    If init_p1 or init_p2 is a json file, load the best weights from the json file.\n    \"\"\"\n\n    if init_p1 and init_p1.endswith(\".json\"):\n        with open(init_p1) as f:\n            best_weights = json.load(f)\n        init_p1 = best_weights[dataset][\"best_weights\"]\n    elif init_p2 and init_p2.endswith(\".json\"):\n        with open(init_p2) as f:\n            best_weights = json.load(f)\n        init_p2 = best_weights[dataset][\"best_weights\"]\n    return init_p1, init_p2", "\n\ndef validate(dataset, model, loss_fn, iteration, val_examples, val_scores, writer, result_writer):\n    log_message(colored(\"VALIDATING...\", \"red\"))\n    log_message(\"Current L1 weights:\", model.encoder_l1.weight)\n    log_message(\"Current L2 weights:\", model.encoder_l2.weight)\n\n    val_key = \"{}-{}\".format(model.encoder_l1.weight, model.encoder_l2.weight)\n    if val_key in val_scores:\n        log_message(\"Already evaluated this configuration, skipping...\")\n        dev_acc = val_scores[val_key]\n    else:\n        acc = 0.0\n        tot = 0.0\n        pbar = tqdm.tqdm(\n            total=dataset.get_size(\"dev\") if val_examples < 0 else val_examples,\n            bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\",\n            desc=\"Eval\",\n        )\n        dataset.reset_pointer(\"dev\")\n        num_examples = 0\n\n        for batch in dataset.iterate(\"dev\", batch_size=20):\n            x, y = batch\n            y_hat = model.forward(np.array(x))\n            result_writer.write_examples(iteration, x, y, model.result_entry.outputs, model.result_entry.hiddens)\n            acc += len(y) - np.sum(loss_fn(y_hat, y))\n            tot += len(y)\n            pbar.update(len(y))\n            pbar.set_postfix_str(f\"{acc / tot:.1%}\")\n            num_examples += len(y)\n\n            if num_examples == val_examples:\n                break\n        dev_acc = acc / tot\n        val_scores[val_key] = dev_acc\n\n    if iteration == 0:\n        log_message(colored(\"INIT DEV ACC: {}\".format(dev_acc), \"red\"))\n    log_message(colored(\"DEV ACC: {}\".format(dev_acc), \"red\"))\n    writer.add_scalar(\"dev/acc\", (dev_acc), iteration)\n    return dev_acc", "\n\ndef test(dataset, model, loss_fn, iteration, writer):\n    log_message(colored(\"TESTING...\", \"red\"))\n    acc = 0.0\n    tot = 0.0\n    pbar = tqdm.tqdm(\n        total=dataset.get_size(\"test\"),\n        bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\",\n        desc=\"Eval\",\n    )\n\n    dataset.reset_pointer(\"test\")\n    for batch in dataset.iterate(\"test\", batch_size=20):\n        x, y = batch\n        y_hat = model.forward(np.array(x))\n        acc += len(y) - np.sum(loss_fn(y_hat, y))\n        tot += len(y)\n        pbar.update(len(y))\n        pbar.set_postfix_str(f\"{acc / tot:.1%}\")\n\n    test_acc = acc / tot\n    writer.add_scalar(\"test/acc\", (test_acc), iteration)\n\n    return test_acc", "\n\n@click.command()\n@click.option(\"--seed\", default=42, help=\"Random seed.\")\n@click.option(\"--out_dir\", default=\"log/\")\n@click.option(\"--data_dir\", default=\"../../data\")\n@click.option(\"--val_freq\", default=2)\n@click.option(\"--do_first_eval\", is_flag=True)\n@click.option(\"--do_zero_shot\", is_flag=True)\n@click.option(\"--q_hidden\", default=\"suffix_forward_tbs\")", "@click.option(\"--do_zero_shot\", is_flag=True)\n@click.option(\"--q_hidden\", default=\"suffix_forward_tbs\")\n@click.option(\"--q_prompt\", default=\"q_action_prompt\")\n@click.option(\"--p_hidden\", default=\"suffix_forward_tbs\")\n@click.option(\"--p_class\", default=\"classify_forward\")\n@click.option(\"--balance_batch\", is_flag=True, help=\"Balance batch.\")\n@click.option(\"--batch_size\", type=int, default=20)\n@click.option(\"--one_layer\", is_flag=True)\n@click.option(\"--dataset\", type=str, default=\"subj\")\n@click.option(\"--use_h_argmax\", type=bool, default=False)", "@click.option(\"--dataset\", type=str, default=\"subj\")\n@click.option(\"--use_h_argmax\", type=bool, default=False)\n@click.option(\"--iters\", type=int, default=20)\n@click.option(\"--num_p_samples\", type=int, default=5)\n@click.option(\"--num_h_samples\", type=int, default=3)\n@click.option(\"--tolerance\", type=int, default=-1)\n@click.option(\n    \"--strip_options_for_hidden\",\n    type=bool,\n    default=False,", "    type=bool,\n    default=False,\n    help=\"Remove options from examples for the hidden layer.\",\n)\n@click.option(\n    \"--strip_prefix_for_hidden\",\n    type=bool,\n    default=False,\n    help=\"Strip the prefix from the examples if it exists in some tasks, e.g. BBH.\",\n)", "    help=\"Strip the prefix from the examples if it exists in some tasks, e.g. BBH.\",\n)\n@click.option(\n    \"--strip_answer_for_hidden\",\n    type=bool,\n    default=False,\n    help=\"Strip the 'Answer:' from the hidden state, if the model generates it.\",\n)\n@click.option(\n    \"--trust_factor\",", "@click.option(\n    \"--trust_factor\",\n    default=0.0,\n    help=\"Trust-region factor for prompt update. Ensures KL divergence between the old and new prompt is small.\",\n)\n@click.option(\n    \"--fwd_temp\",\n    default=0.0,\n    help=\"Forward temperature\",\n)", "    help=\"Forward temperature\",\n)\n@click.option(\n    \"--bwd_temp\",\n    default=0.7,\n    help=\"Backward temperature\",\n)\n@click.option(\n    \"--one_batch\", type=float, default=0.0, help=\"Run only one batch, debug mode.\"\n)", "    \"--one_batch\", type=float, default=0.0, help=\"Run only one batch, debug mode.\"\n)\n@click.option(\n    \"--use_memory\",\n    type=int,\n    default=0,\n    help=\"Include evaluation of past prompts that have worked well in the selection list.\",\n)\n@click.option(\n    \"--forward_use_classes\",", "@click.option(\n    \"--forward_use_classes\",\n    type=bool,\n    default=False,\n    help=\"Uses classes in the forward pass, constrains the output space.\",\n)\n@click.option(\n    \"--init_p1\",\n    type=str,\n    default=\"Decompose the problem to make it simpler:\",", "    type=str,\n    default=\"Decompose the problem to make it simpler:\",\n)\n@click.option(\n    \"--init_p2\",\n    type=str,\n    default=None,\n)\n@click.option(\n    \"--held_out_prompt_ranking\",", "@click.option(\n    \"--held_out_prompt_ranking\",\n    type=bool,\n    default=False,\n    help=\"Evaluate prompts to keep for the next iteration on held-out examples in the current batch.\",\n)\n@click.option(\n    \"--train_p1\",\n    type=bool,\n    default=True,", "    type=bool,\n    default=True,\n    help=\"Train 1 layer, if False, keep it fixed.\",\n)\n@click.option(\n    \"--train_p2\",\n    type=bool,\n    default=True,\n    help=\"Train 2 layer, if False, keep it fixed.\",\n)", "    help=\"Train 2 layer, if False, keep it fixed.\",\n)\n@click.option(\n    \"--logp_penalty\",\n    type=float,\n    default=0.0,\n    help=\"Logp penalty for hiddens that haven't worked. Encourages exploration.\",\n)\n@click.option(\n    \"--decay_logp_penalty\",", "@click.option(\n    \"--decay_logp_penalty\",\n    type=bool,\n    default=True,\n    help=\"Decay logp penalty linearly, reaching zero at the last iteration.\",\n)\n@click.option(\n    \"--posterior_temp\",\n    type=float,\n    default=1.0,", "    type=float,\n    default=1.0,\n    help=\"Sharpen (<1.0)/Flatten (>1.0) the posterior distribution over h.\",\n)\n@click.option(\n    \"--model_type\",\n    type=str,\n    default=\"text-davinci-003\",\n)\n@click.option(", ")\n@click.option(\n    \"--fwd_max_tokens\",\n    type=int,\n    default=256,\n    help=\"Forward max tokens.\",\n)\n@click.option(\n    \"--bwd_max_tokens\",\n    type=int,", "    \"--bwd_max_tokens\",\n    type=int,\n    default=512,\n    help=\"Backward max tokens.\",\n)\n@click.option(\n    \"--result_data_path\",\n    type=str,\n    default=\"../demo/result_data.json\",\n    help=\"The path of the file where the result logs json are stored\",", "    default=\"../demo/result_data.json\",\n    help=\"The path of the file where the result logs json are stored\",\n)\n@click.option(\n    \"--result_exp_name\",\n    type=str,\n    default=None,\n    help=\"(Optional) Name of the experiment run to be saved in the result logs json file.\"\n    \"Useful when running multiple experiments with the same dataset name.\",\n)", "    \"Useful when running multiple experiments with the same dataset name.\",\n)\n@click.option(\n    \"--enable_wandb\",\n    is_flag=True,\n    help=\"Enable wandb logging. Requires wandb to be installed.\",\n)\ndef main(\n    seed,\n    out_dir,\n    data_dir,\n    val_freq,\n    do_first_eval,\n    do_zero_shot,\n    q_hidden,\n    q_prompt,\n    p_hidden,\n    p_class,\n    fwd_temp,\n    bwd_temp,\n    balance_batch,\n    batch_size,\n    one_layer,\n    dataset,\n    use_h_argmax,\n    iters,\n    num_p_samples,\n    num_h_samples,\n    strip_options_for_hidden,\n    strip_answer_for_hidden,\n    strip_prefix_for_hidden,\n    trust_factor,\n    one_batch,\n    use_memory,\n    init_p1,\n    init_p2,\n    tolerance,\n    forward_use_classes,\n    held_out_prompt_ranking,\n    train_p1,\n    train_p2,\n    logp_penalty,\n    decay_logp_penalty,\n    posterior_temp,\n    model_type,\n    fwd_max_tokens,\n    bwd_max_tokens,\n    result_data_path,\n    result_exp_name,\n    enable_wandb,\n):\n    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n    out_dir = f\"{out_dir}/{timestamp}\"\n    os.makedirs(out_dir, exist_ok=True)\n\n    logging.basicConfig(\n        filename=f\"{out_dir}/output.log\",\n        level=logging.INFO,\n        format=\"%(asctime)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    log_message(json.dumps(locals()))\n    log_message(\"Logging to... {}\".format(out_dir + \"/output.log\"))\n\n    wandb_enabled = False\n    if enable_wandb:\n        if wandb_installed:\n            wandb_enabled = True\n            wandb.init(config=locals(), project=\"dln\")\n            prompt_table = wandb.Table(columns=[\"epoch\", \"w1\", \"w2\"])\n        else:\n            log_message(colored(\"Wandb is not installed. Please install it to enable wandb logging.\", \"red\"))\n\n    writer = SummaryWriter(f\"{out_dir}\")\n    result_writer = ResultLogWriter(dataset, path=result_data_path, name=result_exp_name)\n\n    init_p1, init_p2 = init_prompts(dataset, init_p1, init_p2)\n    if wandb_enabled:\n        prompt_table.add_data(0, init_p1, init_p2)\n\n    dataset, output_classes, val_examples = init_dataset(dataset, seed, data_dir)\n\n    forward_instantiate(\n        model_type,\n        temperature=0.0,\n        max_tokens=fwd_max_tokens,\n        stop=None,\n    )\n    backward_instantiate(\n        model_type,\n        temperature=bwd_temp,\n        max_tokens=bwd_max_tokens,\n        stop=None,\n    )\n\n    loss_fn = ZeroOneLoss(postproc=postprocess_prediction)\n    model = VILModel(\n        loss_fn,\n        task_description=dataset.instruction,\n        two_layers=not one_layer,\n        num_p_samples=num_p_samples,\n        num_h_samples=num_h_samples,\n        q_hidden=q_hidden,\n        q_prompt=q_prompt,\n        p_hidden=p_hidden,\n        p_class=p_class,\n        init_p1=init_p1,\n        init_p2=init_p2,\n        use_h_argmax=use_h_argmax,\n        output_classes=output_classes,\n        strip_options_for_hidden=strip_options_for_hidden,\n        strip_answer_for_hidden=strip_answer_for_hidden,\n        trust_factor=trust_factor,\n        forward_use_classes=forward_use_classes,\n        held_out_prompt_ranking=held_out_prompt_ranking,\n        use_memory=use_memory,\n        train_p1=train_p1,\n        train_p2=train_p2,\n        logp_penalty=logp_penalty,\n        p1_max_tokens=256,\n        p2_max_tokens=20,\n        posterior_temp=posterior_temp,\n        strip_prefix_for_hidden=dataset.prefix if strip_prefix_for_hidden else None,\n    )\n\n    running_acc = 0.0\n    running_elbo = 0.0\n    best_dev = 0.0\n    best_ps = [model.encoder_l1.weight, model.encoder_l2.weight]\n    train_x, train_y = None, None\n    sample_next_batch = False\n    val_scores = {}\n\n    patience = 0\n    for iteration in range(iters + 1):\n        log_message(\"STARTING EPOCH {} - {}\".format(iteration, out_dir))\n\n        if iteration % val_freq == 0 and (\n            iteration > 0 or do_first_eval or do_zero_shot\n        ):\n            dev_acc = validate(\n                dataset, model, loss_fn, iteration, val_examples, val_scores, writer, result_writer\n            )\n            if wandb_enabled:\n                wandb.log({\"dev/acc\": dev_acc, \"epoch\": iteration})\n\n            model.result_entry.log_metric('dev_acc', dev_acc)\n            if dev_acc > best_dev:\n                best_dev = dev_acc\n                best_ps = (model.encoder_l1.weight, model.encoder_l2.weight)\n\n                if use_memory:\n                    model.add_to_memory(*best_ps, score=best_dev)\n\n                log_message(colored(\"BEST DEV ACC: {}\".format(best_dev), \"red\"))\n                patience = 0\n            else:\n                patience += 1\n\n            if tolerance >= 0 and patience >= tolerance:\n                log_message(\"Loading back the best model...\")\n                model.encoder_l1.weight = best_ps[0]\n                model.encoder_l2.weight = best_ps[1]\n                patience = 0\n        else:\n            model.result_entry.log_metric('dev_acc', None)\n\n        result_writer.write_result(\n            step=iteration,\n            layers=[model.encoder_l2.weight] if one_layer else [model.encoder_l1.weight, model.encoder_l2.weight],\n            metrics=model.result_entry.metrics,\n            candidates=model.result_entry.candidates,\n        )\n\n        # zero shot or allow last iteration for validation\n        if do_zero_shot or iteration == iters:\n            break\n\n        if one_batch > 0.0 and train_x is not None and not sample_next_batch:\n            # use the same batch, just re-shuffle the examples in the batch\n            permutation_indices = np.random.permutation(np.arange(len(train_x)))\n            x, y = np.asarray([train_x[i] for i in permutation_indices]), np.asarray(\n                [train_y[i] for i in permutation_indices]\n            )\n            log_message(colored(\"USING SAME BATCH FOR TRAINING!!!\", \"yellow\"))\n        else:\n            x, y = dataset.get_batch(\n                \"train\", batch_size, random_sample=True, balance=balance_batch\n            )\n            train_x, train_y = x, y\n\n        if decay_logp_penalty:\n            model.logp_penalty = logp_penalty * (1.0 - (iteration / iters))\n\n        log_message(colored(\"Training P2? {}\".format(model.train_p2), \"red\"))\n        log_message(colored(\"LOGPenalty? {}\".format(model.logp_penalty), \"red\"))\n        elbo, p1, p2, loss, elbo1, elbo2 = model.forward(\n            np.array(x), np.array(y), temperature=fwd_temp\n        )\n\n        # Update prompts\n        model.encoder_l1.weight = p1\n        model.encoder_l2.weight = p2\n        log_message(\"Patience: {}\".format(patience))\n\n        if iteration == 0:\n            running_elbo = elbo\n            running_acc = 1.0 - loss\n        else:\n            running_elbo = 0.2 * elbo + 0.8 * running_elbo\n            running_acc = 0.2 * (1.0 - loss) + 0.8 * running_acc\n\n        # get another batch if training accuracy is too good!\n        sample_next_batch = (1.0 - loss) > one_batch\n\n        log_message(\"--------------------\")\n        log_message(colored(\"{} TRAINING EPOCH DONE.\".format(iteration), \"blue\"))\n        log_message(colored(\"ELBO: {}\".format(elbo), \"blue\"))\n        log_message(colored(\"ACC: {}\".format((1.0 - loss)), \"blue\"))\n        log_message(colored(\"RUN ELBO: {}\".format(running_elbo), \"blue\"))\n        log_message(colored(\"RUN ACC: {}\".format(running_acc), \"blue\"))\n        log_message(colored(\"BATCH Y BALANCE: {}\".format(Counter(y)), \"blue\"))\n        log_message(colored(\"BATCH X LEN: {}\".format([len(x_i) for x_i in x]), \"blue\"))\n\n        if wandb_enabled:\n            prompt_table.add_data(iteration + 1, str(p1), str(p2))\n            wandb.log({\"train/prompts\" : prompt_table})\n            wandb.log({\"train/elbo\": elbo, \"train/acc\": (1.0 - loss), \"epoch\": iteration})\n\n        writer.add_scalar(\"elbo\", elbo, iteration)\n        writer.add_scalar(\"elbo1\", elbo1, iteration)\n        writer.add_scalar(\"elbo2\", elbo2, iteration)\n        writer.add_scalar(\"acc\", (1.0 - loss), iteration)\n        model.result_entry.log_metric('elbo', elbo)\n        model.result_entry.log_metric('acc', (1.0 - loss))\n        model.result_entry.log_metric('run_elbo', running_elbo)\n        model.result_entry.log_metric('run_acc', running_acc)\n\n    log_message(\"--------------------\")\n    log_message(\"Loading best model...\")\n\n    model.encoder_l1.weight = best_ps[0]\n    model.encoder_l2.weight = best_ps[1]\n\n    log_message(\"Best L1 weights:\", model.encoder_l1.weight)\n    log_message(\"Best L2 weights:\", model.encoder_l2.weight)\n\n    test_acc = test(dataset, model, loss_fn, iteration, writer)\n\n    if wandb_enabled:\n        wandb.log({\"test/acc\": test_acc, \"epoch\": iteration})\n\n    log_message(colored(\"DEV ACC: {}\".format(best_dev), \"green\"))\n    log_message(colored(\"TEST ACC: {}\".format(test_acc), \"green\"))\n    result_writer.save_to_json_file()\n    writer.close()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "projects/vi_dln/scripts/one_layer/one_layer.py", "chunked_list": ["import json\nimport subprocess\n\nimport click\n\n\n@click.command()\n@click.option(\n    \"--dataset\",\n    type=str,", "    \"--dataset\",\n    type=str,\n    help=\"Dataset name\",\n    required=True,\n)\ndef main(dataset):\n    config_file=\"one-layer-dln-hp-search-result.json\"\n    with open(config_file) as f:\n        config_data = json.load(f)\n\n    config = config_data[dataset]\n    q_prompt_tpl = config[\"hyperparam\"][\"q_prompt_tpl\"]\n    tolerance = config[\"hyperparam\"][\"tolerance\"]\n    use_memory = config[\"hyperparam\"][\"use_memory\"]\n    held_out_prompt_ranking = config[\"hyperparam\"][\"held_out_prompt_ranking\"]\n\n    output_dir = f\"log/one_layer/{dataset}\"\n    for seed in [13, 42, 25]:\n        command = list(map(str, [\n            \"python\",\n            \"vi_main.py\",\n            \"--balance_batch\",\n            \"--num_p_samples\", 20,\n            \"--bwd_temp\", 0.7,\n            \"--iters\", 20,\n            \"--p_class\", \"classify_forward:3.0\",\n            \"--q_prompt\", q_prompt_tpl,\n            \"--out_dir\", output_dir,\n            \"--batch_size\", 20,\n            \"--seed\", seed,\n            \"--dataset\", dataset,\n            \"--tolerance\", tolerance,\n            \"--use_memory\", use_memory,\n            \"--held_out_prompt_ranking\", held_out_prompt_ranking,\n            \"--one_layer\",\n            \"--do_first_eval\",\n        ]))\n        print(' '.join(command))\n        subprocess.run(command)", "\nif __name__ == \"__main__\":\n    main()"]}
