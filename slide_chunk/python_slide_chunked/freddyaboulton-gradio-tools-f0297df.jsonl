{"filename": "gradio_tools/__init__.py", "chunked_list": ["from gradio_tools.tools import (BarkTextToSpeechTool, ClipInterrogatorTool,\n                                DocQueryDocumentAnsweringTool, GradioTool,\n                                ImageCaptioningTool, ImageToMusicTool,\n                                SAMImageSegmentationTool,\n                                StableDiffusionPromptGeneratorTool,\n                                StableDiffusionTool, TextToVideoTool,\n                                WhisperAudioTranscriptionTool)\n\n__all__ = [\n    \"GradioTool\",", "__all__ = [\n    \"GradioTool\",\n    \"StableDiffusionTool\",\n    \"ClipInterrogatorTool\",\n    \"ImageCaptioningTool\",\n    \"ImageToMusicTool\",\n    \"WhisperAudioTranscriptionTool\",\n    \"StableDiffusionPromptGeneratorTool\",\n    \"TextToVideoTool\",\n    \"DocQueryDocumentAnsweringTool\",", "    \"TextToVideoTool\",\n    \"DocQueryDocumentAnsweringTool\",\n    \"BarkTextToSpeechTool\",\n    \"SAMImageSegmentationTool\",\n]\n"]}
{"filename": "gradio_tools/tools/sam_with_clip.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, List\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr", "if TYPE_CHECKING:\n    import gradio as gr\n\n\nclass SAMImageSegmentationTool(GradioTool):\n    \"\"\"Tool for segmenting images based on natural language queries.\"\"\"\n\n    def __init__(\n        self,\n        name=\"SAMImageSegmentation\",\n        description=(\n            \"A tool for identifying objects in images. \"\n            \"Input will be a five strings separated by a |: \"\n            \"the first will be the full path or URL to an image file. \"\n            \"The second will be the string query describing the objects to identify in the image. \"\n            \"The query string should be as detailed as possible. \"\n            \"The third will be the predicted_iou_threshold, if not specified by the user set it to 0.9. \"\n            \"The fourth will be the stability_score_threshold, if not specified by the user set it to 0.8. \"\n            \"The fifth is the clip_threshold, if not specified by the user set it to 0.85. \"\n            \"The output will the a path with an image file with the identified objects overlayed in the image.\"\n        ),\n        src=\"curt-park/segment-anything-with-clip\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        try:\n            (\n                image,\n                query,\n                predicted_iou_threshold,\n                stability_score_threshold,\n                clip_threshold,\n            ) = query.split(\"|\")\n        except ValueError as e:\n            raise ValueError(\n                \"Not enough arguments passed to the SAMImageSegmentationTool! \"\n                \"Expected 5 (image, query, predicted_iou_threshold, stability_score_threshold, clip_threshold)\"\n            ) from e\n        return self.client.submit(\n            float(predicted_iou_threshold),\n            float(stability_score_threshold),\n            float(clip_threshold),\n            image,\n            query.strip(),\n            api_name=\"/predict\",\n        )\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Number(), gr.Number(), gr.Number(), gr.Image(), gr.Textbox()]\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image()]", ""]}
{"filename": "gradio_tools/tools/image_captioning.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, List\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr", "if TYPE_CHECKING:\n    import gradio as gr\n\n\nclass ImageCaptioningTool(GradioTool):\n    \"\"\"Tool for captioning images.\"\"\"\n\n    def __init__(\n        self,\n        name=\"ImageCaptioner\",\n        description=(\n            \"An image captioner. Use this to create a caption for an image. \"\n            \"Input will be a path to an image file. \"\n            \"The output will be a caption of that image.\"\n        ),\n        src=\"gradio-client-demos/BLIP-2\",\n        hf_token=None,\n        duplicate=True,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query.strip(\"'\"), \"Beam Search\", fn_index=0)\n\n    def postprocess(self, output: str) -> str:\n        return output  # type: ignore\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image()]\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Textbox()]", ""]}
{"filename": "gradio_tools/tools/document_qa.py", "chunked_list": ["from typing import TYPE_CHECKING, List\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr\n\n\nclass DocQueryDocumentAnsweringTool(GradioTool):\n    def __init__(\n        self,\n        name=\"DocQuery\",\n        description=(\n            \"A tool for answering questions about a document from the from the image of the document. Input will be a two strings separated by a |: the first will be the path or URL to an image of a document. The second will be your question about the document.\"\n            \"The output will the text answer to your question.\"\n        ),\n        src=\"abidlabs/docquery\",\n        hf_token=None,\n        duplicate=True,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        img, question = query.split(\"|\")\n        return self.client.submit(img.strip(), question.strip(), api_name=\"/predict\")\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image(), gr.Textbox()]", "\n\nclass DocQueryDocumentAnsweringTool(GradioTool):\n    def __init__(\n        self,\n        name=\"DocQuery\",\n        description=(\n            \"A tool for answering questions about a document from the from the image of the document. Input will be a two strings separated by a |: the first will be the path or URL to an image of a document. The second will be your question about the document.\"\n            \"The output will the text answer to your question.\"\n        ),\n        src=\"abidlabs/docquery\",\n        hf_token=None,\n        duplicate=True,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        img, question = query.split(\"|\")\n        return self.client.submit(img.strip(), question.strip(), api_name=\"/predict\")\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image(), gr.Textbox()]", ""]}
{"filename": "gradio_tools/tools/prompt_generator.py", "chunked_list": ["from __future__ import annotations\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\n\nclass StableDiffusionPromptGeneratorTool(GradioTool):\n    def __init__(\n        self,\n        name=\"StableDiffusionPromptGenerator\",\n        description=(\n            \"Use this tool to improve a prompt for stable diffusion and other image and video generators. \"\n            \"This tool will refine your prompt to include key words and phrases that make \"\n            \"stable diffusion and other art generation algorithms perform better. The input is a prompt text string \"\n            \"and the output is a prompt text string\"\n        ),\n        src=\"microsoft/Promptist\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, api_name=\"/predict\")\n\n    def postprocess(self, output: str) -> str:\n        return output", ""]}
{"filename": "gradio_tools/tools/whisper.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, List\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr", "if TYPE_CHECKING:\n    import gradio as gr\n\n\nclass WhisperAudioTranscriptionTool(GradioTool):\n    def __init__(\n        self,\n        name=\"WhisperAudioTranscription\",\n        description=(\n            \"A tool for transcribing audio. Use this tool to transcribe an audio file. \"\n            \"track from an image. Input will be a path to an audio file. \"\n            \"The output will the text transcript of that file.\"\n        ),\n        src=\"abidlabs/whisper\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, api_name=\"/predict\")\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Audio()]", ""]}
{"filename": "gradio_tools/tools/gradio_tool.py", "chunked_list": ["from __future__ import annotations\n\nimport time\nfrom abc import abstractmethod\nfrom typing import Any, List, Tuple, Union\n\nimport gradio_client as grc\nimport huggingface_hub\nfrom gradio_client.client import Job\nfrom gradio_client.utils import QueueError", "from gradio_client.client import Job\nfrom gradio_client.utils import QueueError\n\ntry:\n    import langchain as lc\n\n    LANGCHAIN_INSTALLED = True\nexcept (ModuleNotFoundError, ImportError):\n    LANGCHAIN_INSTALLED = False\n", "\n\nclass GradioTool:\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        src: str,\n        hf_token: str | None = None,\n        duplicate: bool = True,\n    ) -> None:\n        self.name = name\n        self.description = description\n        if hf_token and self._is_space(src) and duplicate:\n            self.client = grc.Client.duplicate(from_id=src, hf_token=hf_token)\n            self.src = self.client.space_id\n        else:\n            self.src = src\n            self.client = grc.Client(self.src, hf_token=hf_token)\n        self.description += (\n            \"If the output is the string QUEUE_FULL you \"\n            \"should try this action again and never stop until you \"\n            \"don't see this message.\"\n        )\n        self._block = None\n\n    @staticmethod\n    def _is_space(src: str) -> bool:\n        try:\n            huggingface_hub.get_space_runtime(src)\n            return True\n        except huggingface_hub.hf_api.RepositoryNotFoundError:\n            return False\n\n    @abstractmethod\n    def create_job(self, query: str) -> Job:\n        pass\n\n    @abstractmethod\n    def postprocess(self, output: Union[Tuple[Any], Any]) -> str:\n        pass\n\n    def run(self, query: str):\n        job = self.create_job(query)\n        while not job.done():\n            status = job.status()\n            print(f\"\\nJob Status: {str(status.code)} eta: {status.eta}\")\n            time.sleep(30)\n        try:\n            output = self.postprocess(job.result())\n        except QueueError:\n            output = \"QUEUE_FULL\"\n        return output\n\n    # Optional gradio functionalities\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Textbox()]\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Textbox()]\n\n    def block_input(self) -> List[\"gr.components.Component\"]:\n        try:\n            import gradio as gr\n\n            GRADIO_INSTALLED = True\n        except (ModuleNotFoundError, ImportError):\n            GRADIO_INSTALLED = False\n        if not GRADIO_INSTALLED:\n            raise ModuleNotFoundError(\"gradio must be installed to call block_input\")\n        else:\n            return self._block_input(gr)\n\n    def block_output(self) -> List[\"gr.components.Component\"]:\n        try:\n            import gradio as gr\n\n            GRADIO_INSTALLED = True\n        except (ModuleNotFoundError, ImportError):\n            GRADIO_INSTALLED = False\n        if not GRADIO_INSTALLED:\n            raise ModuleNotFoundError(\"gradio must be installed to call block_output\")\n        else:\n            return self._block_output(gr)\n\n    def block(self):\n        \"\"\"Get the gradio Blocks of this tool for visualization.\"\"\"\n        try:\n            import gradio as gr\n        except (ModuleNotFoundError, ImportError):\n            raise ModuleNotFoundError(\"gradio must be installed to call block\")\n        if not self._block:\n            self._block = gr.load(name=self.src, src=\"spaces\")\n        return self._block\n\n    # Optional langchain functionalities\n    @property\n    def langchain(self) -> \"langchain.agents.Tool\":  # type: ignore\n        if not LANGCHAIN_INSTALLED:\n            raise ModuleNotFoundError(\n                \"langchain must be installed to access langchain tool\"\n            )\n\n        return lc.agents.Tool(  # type: ignore\n            name=self.name, func=self.run, description=self.description\n        )\n\n    def __repr__(self) -> str:\n        return f\"GradioTool(name={self.name}, src={self.src})\"", ""]}
{"filename": "gradio_tools/tools/text_to_video.py", "chunked_list": ["from typing import TYPE_CHECKING, List\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr\n\n\nclass TextToVideoTool(GradioTool):\n    def __init__(\n        self,\n        name=\"TextToVideo\",\n        description=(\n            \"A tool for creating videos from text.\"\n            \"Use this tool to create videos from text prompts. \"\n            \"Input will be a text prompt describing a video scene. \"\n            \"The output will be a path to a video file.\"\n        ),\n        src=\"damo-vilab/modelscope-text-to-video-synthesis\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, -1, 16, 25, fn_index=1)\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Video()]", "\n\nclass TextToVideoTool(GradioTool):\n    def __init__(\n        self,\n        name=\"TextToVideo\",\n        description=(\n            \"A tool for creating videos from text.\"\n            \"Use this tool to create videos from text prompts. \"\n            \"Input will be a text prompt describing a video scene. \"\n            \"The output will be a path to a video file.\"\n        ),\n        src=\"damo-vilab/modelscope-text-to-video-synthesis\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, -1, 16, 25, fn_index=1)\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Video()]", ""]}
{"filename": "gradio_tools/tools/__init__.py", "chunked_list": ["from gradio_tools.tools.bark import BarkTextToSpeechTool\nfrom gradio_tools.tools.clip_interrogator import ClipInterrogatorTool\nfrom gradio_tools.tools.document_qa import DocQueryDocumentAnsweringTool\nfrom gradio_tools.tools.gradio_tool import GradioTool\nfrom gradio_tools.tools.image_captioning import ImageCaptioningTool\nfrom gradio_tools.tools.image_to_music import ImageToMusicTool\nfrom gradio_tools.tools.prompt_generator import \\\n    StableDiffusionPromptGeneratorTool\nfrom gradio_tools.tools.sam_with_clip import SAMImageSegmentationTool\nfrom gradio_tools.tools.stable_diffusion import StableDiffusionTool", "from gradio_tools.tools.sam_with_clip import SAMImageSegmentationTool\nfrom gradio_tools.tools.stable_diffusion import StableDiffusionTool\nfrom gradio_tools.tools.text_to_video import TextToVideoTool\nfrom gradio_tools.tools.whisper import WhisperAudioTranscriptionTool\n\n__all__ = [\n    \"GradioTool\",\n    \"StableDiffusionTool\",\n    \"ClipInterrogatorTool\",\n    \"ImageCaptioningTool\",", "    \"ClipInterrogatorTool\",\n    \"ImageCaptioningTool\",\n    \"ImageToMusicTool\",\n    \"WhisperAudioTranscriptionTool\",\n    \"StableDiffusionPromptGeneratorTool\",\n    \"TextToVideoTool\",\n    \"DocQueryDocumentAnsweringTool\",\n    \"BarkTextToSpeechTool\",\n    \"SAMImageSegmentationTool\",\n]", "    \"SAMImageSegmentationTool\",\n]\n"]}
{"filename": "gradio_tools/tools/bark.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, List\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr", "if TYPE_CHECKING:\n    import gradio as gr\n\nSUPPORTED_LANGS = [\n    (\"English\", \"en\"),\n    (\"German\", \"de\"),\n    (\"Spanish\", \"es\"),\n    (\"French\", \"fr\"),\n    (\"Hindi\", \"hi\"),\n    (\"Italian\", \"it\"),", "    (\"Hindi\", \"hi\"),\n    (\"Italian\", \"it\"),\n    (\"Japanese\", \"ja\"),\n    (\"Korean\", \"ko\"),\n    (\"Polish\", \"pl\"),\n    (\"Portuguese\", \"pt\"),\n    (\"Russian\", \"ru\"),\n    (\"Turkish\", \"tr\"),\n    (\"Chinese\", \"zh\"),\n]", "    (\"Chinese\", \"zh\"),\n]\n\nSUPPORTED_LANGS = {lang: code for lang, code in SUPPORTED_LANGS}\nVOICES = [\"Unconditional\", \"Announcer\"]\nSUPPORTED_SPEAKERS = VOICES + [p for p in SUPPORTED_LANGS]\n\nNON_SPEECH_TOKENS = [\n    \"[laughter]\",\n    \"[laughs]\",", "    \"[laughter]\",\n    \"[laughs]\",\n    \"[sighs]\",\n    \"[music]\",\n    \"[gasps]\",\n    \"[clears throat]\",\n    \"'\u266a' for song lyrics. Put \u266a on either side of the the text\",\n    \"'\u2026' for hesitations\",\n]\n", "]\n\n\nclass BarkTextToSpeechTool(GradioTool):\n    \"\"\"Tool for calling bark text-to-speech llm.\"\"\"\n\n    def __init__(\n        self,\n        name=\"BarkTextToSpeech\",\n        description=(\n            \"A tool for text-to-speech. Use this tool to convert text \"\n            \"into sounds that sound like a human read it. Input will be a two strings separated by a |: \"\n            \"the first will be the text to read. The second will be the desired speaking language. \"\n            f\"It MUST be one of the following choices {','.join(SUPPORTED_SPEAKERS)}. \"\n            f\"Additionally, you can include the following non speech tokens: {NON_SPEECH_TOKENS}\"\n            \"The output will the text transcript of that file.\"\n        ),\n        src=\"suno/bark\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        try:\n            text, speaker = (\n                query[: query.rindex(\"|\")],\n                query[(query.rindex(\"|\") + 1) :].strip(),\n            )\n        except ValueError:\n            text, speaker = query, \"Unconditional\"\n        if speaker in VOICES:\n            pass\n        elif speaker in SUPPORTED_LANGS:\n            speaker = f\"Speaker 0 ({SUPPORTED_LANGS[speaker]})\"\n        else:\n            speaker = \"Unconditional\"\n        return self.client.submit(text, speaker, fn_index=3)\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Textbox()]\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Audio()]", ""]}
{"filename": "gradio_tools/tools/clip_interrogator.py", "chunked_list": ["from typing import TYPE_CHECKING, List\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr\n\n\nclass ClipInterrogatorTool(GradioTool):\n    def __init__(\n        self,\n        name=\"ClipInterrogator\",\n        description=(\n            \"A tool for reverse engineering a prompt from a source image. \"\n            \"Use this tool to create a prompt for StableDiffusion that matches the \"\n            \"input image. The imput is a path to an image. The output is a text string.\"\n        ),\n        src=\"pharma/CLIP-Interrogator\",\n        hf_token=None,\n        duplicate=True,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(\n            query, \"ViT-L (best for Stable Diffusion 1.*)\", \"best\", fn_index=3\n        )\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image()]", "\n\nclass ClipInterrogatorTool(GradioTool):\n    def __init__(\n        self,\n        name=\"ClipInterrogator\",\n        description=(\n            \"A tool for reverse engineering a prompt from a source image. \"\n            \"Use this tool to create a prompt for StableDiffusion that matches the \"\n            \"input image. The imput is a path to an image. The output is a text string.\"\n        ),\n        src=\"pharma/CLIP-Interrogator\",\n        hf_token=None,\n        duplicate=True,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(\n            query, \"ViT-L (best for Stable Diffusion 1.*)\", \"best\", fn_index=3\n        )\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image()]", ""]}
{"filename": "gradio_tools/tools/stable_diffusion.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import TYPE_CHECKING, List\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr", "if TYPE_CHECKING:\n    import gradio as gr\n\n\nclass StableDiffusionTool(GradioTool):\n    \"\"\"Tool for calling stable diffusion from llm\"\"\"\n\n    def __init__(\n        self,\n        name=\"StableDiffusion\",\n        description=(\n            \"An image generator. Use this to generate images based on \"\n            \"text input. Input should be a description of what the image should \"\n            \"look like. The output will be a path to an image file.\"\n        ),\n        src=\"gradio-client-demos/text-to-image\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(query, api_name=\"/predict\")\n\n    def postprocess(self, output: str) -> str:\n        return output\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Textbox()]\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image()]", ""]}
{"filename": "gradio_tools/tools/image_to_music.py", "chunked_list": ["from typing import TYPE_CHECKING, Any, List, Tuple, Union\n\nfrom gradio_client.client import Job\n\nfrom gradio_tools.tools.gradio_tool import GradioTool\n\nif TYPE_CHECKING:\n    import gradio as gr\n\n\nclass ImageToMusicTool(GradioTool):\n    def __init__(\n        self,\n        name=\"ImagetoMusic\",\n        description=(\n            \"A tool for creating music from images. Use this tool to create a musical \"\n            \"track from an image. Input will be a path to an image file. \"\n            \"The output will be an audio file generated from that image.\"\n        ),\n        src=\"fffiloni/img-to-music\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(\n            query.strip(\"'\"), 15, \"medium\", \"loop\", None, fn_index=0\n        )\n\n    def postprocess(self, output: Union[Tuple[Any], Any]) -> str:\n        return output[1]  # type: ignore\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image()]\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Audio()]", "\n\nclass ImageToMusicTool(GradioTool):\n    def __init__(\n        self,\n        name=\"ImagetoMusic\",\n        description=(\n            \"A tool for creating music from images. Use this tool to create a musical \"\n            \"track from an image. Input will be a path to an image file. \"\n            \"The output will be an audio file generated from that image.\"\n        ),\n        src=\"fffiloni/img-to-music\",\n        hf_token=None,\n        duplicate=False,\n    ) -> None:\n        super().__init__(name, description, src, hf_token, duplicate)\n\n    def create_job(self, query: str) -> Job:\n        return self.client.submit(\n            query.strip(\"'\"), 15, \"medium\", \"loop\", None, fn_index=0\n        )\n\n    def postprocess(self, output: Union[Tuple[Any], Any]) -> str:\n        return output[1]  # type: ignore\n\n    def _block_input(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Image()]\n\n    def _block_output(self, gr) -> List[\"gr.components.Component\"]:\n        return [gr.Audio()]", ""]}
{"filename": "tests/test_tools.py", "chunked_list": ["import pytest\nfrom unittest.mock import patch\n\nimport gradio_tools\nfrom gradio_tools import GradioTool\n\ntry:\n    import gradio as gr\n    GRADIO_INSTALLED = True\nexcept:\n    GRADIO_INSTALLED = False", "\n\n@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\n@patch(\"gradio_client.Client.duplicate\")\ndef test_duplicate(mock_duplicate, tool_class):\n    tool_class(duplicate=True, hf_token=\"dafsdf\")\n    mock_duplicate.assert_called_once()\n\n\n@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())", "\n@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\n@patch(\"gradio_client.Client.duplicate\")\ndef test_dont_duplicate(mock_duplicate, tool_class):\n    tool_class(duplicate=False)\n    mock_duplicate.assert_not_called()\n\n\n@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\ndef test_all_listed_in_init(tool_class):\n    assert tool_class.__name__ in gradio_tools.__all__", "@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\ndef test_all_listed_in_init(tool_class):\n    assert tool_class.__name__ in gradio_tools.__all__\n\n\n@pytest.mark.skipif(not GRADIO_INSTALLED, reason=\"Gradio not installed\")\n@pytest.mark.parametrize(\"tool_class\", GradioTool.__subclasses__())\ndef test_input_output(tool_class):\n    if tool_class.__name__ == \"BarkTextToSpeechTool\":\n        inp = [gr.Textbox()]\n        output = [gr.Audio()]\n    elif tool_class.__name__ == \"ClipInterrogatorTool\":\n        inp = [gr.Image()]\n        output = [gr.Textbox()]\n    elif tool_class.__name__ == \"DocQueryDocumentAnsweringTool\":\n        inp = [gr.Image(), gr.Textbox()]\n        output = [gr.Textbox()]\n    elif tool_class.__name__ == \"ImageCaptioningTool\":\n        inp = [gr.Image()]\n        output = [gr.Textbox()]\n    elif tool_class.__name__ == \"ImageToMusicTool\":\n        inp = [gr.Image()]\n        output = [gr.Audio()]\n    elif tool_class.__name__ == \"StableDiffusionPromptGeneratorTool\":\n        inp = [gr.Textbox()]\n        output = [gr.Textbox()]\n    elif tool_class.__name__ == \"SAMImageSegmentationTool\":\n        inp = [gr.Number(), gr.Number(), gr.Number(), gr.Image(), gr.Textbox()]\n        output = [gr.Image()]\n    elif tool_class.__name__ == \"StableDiffusionTool\":\n        inp = [gr.Textbox()]\n        output = [gr.Image()]\n    elif tool_class.__name__ == \"TextToVideoTool\":\n        inp = [gr.Textbox()]\n        output = [gr.Video()]\n    elif tool_class.__name__ == \"WhisperAudioTranscriptionTool\":\n        inp = [gr.Audio()]\n        output = [gr.Textbox()]\n    else:\n        raise ValueError(f\"Test does not have a case for: {tool_class.__name__}\")\n    \n    tool = tool_class()\n    assert [t.__class__ for t in tool.block_input()] == [t.__class__ for t in inp]\n    assert [t.__class__ for t in tool.block_output()] == [t.__class__ for t in output]", "\n"]}
{"filename": "tests/test_sam.py", "chunked_list": ["import pytest\nfrom unittest.mock import patch\n\nfrom gradio_tools import SAMImageSegmentationTool\n\n\n@patch(\"gradio_client.Client.submit\")\ndef test_input_parsing(mock_submit):\n    tool = SAMImageSegmentationTool()\n    tool.create_job(\"my_image.png| a red horse|0.9|0.8|0.9\")\n    mock_submit.assert_called_with(0.9,0.8,0.9,\"my_image.png\", \"a red horse\", api_name=\"/predict\")", "\n\n@patch(\"gradio_client.Client.submit\")\ndef test_raise_error(mock_submit):\n    tool = SAMImageSegmentationTool()\n    with pytest.raises(ValueError,\n                       match=\"Not enough arguments passed to the SAMImageSegmentationTool!\"):\n        tool.create_job(\"my_image.png| a red horse|\")"]}
{"filename": "examples/langchain/document_qa.py", "chunked_list": ["import os\nimport pathlib\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY must be set\")\n\nfrom langchain.agents import initialize_agent, load_tools\nfrom langchain.llms import OpenAI\nfrom gradio_tools import DocQueryDocumentAnsweringTool\n", "from gradio_tools import DocQueryDocumentAnsweringTool\n\nfrom langchain.memory import ConversationBufferMemory\n\nllm = OpenAI(temperature=0)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\ntools = [DocQueryDocumentAnsweringTool().langchain]\n\nIMG_PATH = pathlib.Path(__file__).parent / \"florida-drivers-license.jpeg\"\n", "IMG_PATH = pathlib.Path(__file__).parent / \"florida-drivers-license.jpeg\"\n\nagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\noutput = agent.run(input=f\"What is the date of birth the driver in {IMG_PATH}?\")\noutput = agent.run(input=f\"What is the current date?\")\noutput = agent.run(input=f\"Using the current date, what is the age of the driver? Explain your reasoning.\")\noutput = agent.run(input=f\"What is the driver's license number?\")"]}
{"filename": "examples/langchain/example.py", "chunked_list": ["import os\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY must be set\")\n\nfrom langchain.agents import initialize_agent\nfrom langchain.llms import OpenAI\nfrom gradio_tools.tools import (StableDiffusionTool, ImageCaptioningTool, StableDiffusionPromptGeneratorTool,\n                                TextToVideoTool)\n", "                                TextToVideoTool)\n\nfrom langchain.memory import ConversationBufferMemory\n\nllm = OpenAI(temperature=0)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\ntools = [StableDiffusionTool().langchain, ImageCaptioningTool().langchain,\n         StableDiffusionPromptGeneratorTool().langchain, TextToVideoTool().langchain]\n\n", "\n\nagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\noutput = agent.run(input=(\"Please create a photo of a dog riding a skateboard \"\n                          \"but improve my prompt prior to using an image generator.\"\n                          \"Please caption the generated image and create a video for it using the improved prompt.\"))\n"]}
{"filename": "examples/langchain/bark_example.py", "chunked_list": ["import os\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY must be set\")\n\nfrom langchain.agents import initialize_agent\nfrom langchain.llms import OpenAI\nfrom gradio_tools.tools import BarkTextToSpeechTool, StableDiffusionTool, StableDiffusionPromptGeneratorTool\n\nfrom langchain.memory import ConversationBufferMemory", "\nfrom langchain.memory import ConversationBufferMemory\n\nllm = OpenAI(temperature=0)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\ntools = [BarkTextToSpeechTool().langchain,\n         StableDiffusionTool().langchain,\n         StableDiffusionPromptGeneratorTool().langchain]\n\n", "\n\nagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\noutput = agent.run(input=(\"Please create a jingle for a spanish company called 'Chipi Chups' that makes lollipops. \"\n                          \"The jingle should be catchy and playful and meant to appeal to all ages.\"))\nprint(output)\noutput = agent.run(input=(\"Now create a logo for this company. Please improve\"))\nprint(output)\n", ""]}
{"filename": "examples/langchain/sam_example.py", "chunked_list": ["import os\nimport pathlib\n\n\nif not os.getenv(\"OPENAI_API_KEY\"):\n    raise ValueError(\"OPENAI_API_KEY must be set\")\n\nfrom langchain.agents import initialize_agent\nfrom langchain.llms import OpenAI\nfrom gradio_tools.tools import SAMImageSegmentationTool", "from langchain.llms import OpenAI\nfrom gradio_tools.tools import SAMImageSegmentationTool\n\nfrom langchain.memory import ConversationBufferMemory\n\nllm = OpenAI(temperature=0)\nmemory = ConversationBufferMemory(memory_key=\"chat_history\")\ntools = [SAMImageSegmentationTool().langchain]\n\nwaldo_1 = pathlib.Path(__file__).parent / \"waldo.jpeg\"", "\nwaldo_1 = pathlib.Path(__file__).parent / \"waldo.jpeg\"\nwaldo_2 = pathlib.Path(__file__).parent / \"waldo_3.webp\"\n\n\n\nagent = initialize_agent(tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True)\noutput = agent.run(input=(f\"Please find Waldo in this image: {waldo_1}. \"\n                          \"Waldo is a man with glasses wearing sweater with red and white stripes\"))\nprint(output)", "                          \"Waldo is a man with glasses wearing sweater with red and white stripes\"))\nprint(output)\noutput = agent.run(input=(f\"Great job! Now find Waldo in this image: {waldo_2}.\"))\nprint(output)\n"]}
{"filename": "examples/minichain/agent.py", "chunked_list": ["from minichain import Id, prompt, OpenAI, show, transform, Mock, Break\nfrom gradio_tools.tools import StableDiffusionTool, ImageCaptioningTool, ImageToMusicTool\n\n\ntools = [StableDiffusionTool(), ImageCaptioningTool(), ImageToMusicTool()]\n\n\n@prompt(OpenAI(stop=[\"Observation:\"]),\n        template_file=\"agent.pmpt.tpl\")\ndef agent(model, query, history):\n    return model(dict(tools=[(str(tool.__class__.__name__), tool.description)\n                             for tool in tools],\n                      input=query,\n                      agent_scratchpad=history\n                      ))", "        template_file=\"agent.pmpt.tpl\")\ndef agent(model, query, history):\n    return model(dict(tools=[(str(tool.__class__.__name__), tool.description)\n                             for tool in tools],\n                      input=query,\n                      agent_scratchpad=history\n                      ))\n@transform()\ndef tool_parse(out):\n    lines = out.split(\"\\n\")\n    if lines[0].split(\"?\")[-1].strip() == \"Yes\":\n        tool = lines[1].split(\":\", 1)[-1].strip()\n        command = lines[2].split(\":\", 1)[-1].strip()\n        return tool, command\n    else:\n        return Break()", "def tool_parse(out):\n    lines = out.split(\"\\n\")\n    if lines[0].split(\"?\")[-1].strip() == \"Yes\":\n        tool = lines[1].split(\":\", 1)[-1].strip()\n        command = lines[2].split(\":\", 1)[-1].strip()\n        return tool, command\n    else:\n        return Break()\n\n@prompt(tools)\ndef tool_use(model, usage):\n    selector, command = usage\n    for i, tool in enumerate(tools):\n        if selector == tool.__class__.__name__:\n            return model(command, tool_num=i)\n    return (\"\",)", "\n@prompt(tools)\ndef tool_use(model, usage):\n    selector, command = usage\n    for i, tool in enumerate(tools):\n        if selector == tool.__class__.__name__:\n            return model(command, tool_num=i)\n    return (\"\",)\n\n@transform()\ndef append(history, new, observation):\n    return history + \"\\n\" + new + \"Observation: \" + observation", "\n@transform()\ndef append(history, new, observation):\n    return history + \"\\n\" + new + \"Observation: \" + observation\n\ndef run(query):\n    history = \"\"\n    observations = []\n    for i in range(3):\n        select_input = agent(query, history)\n        observations.append(tool_use(tool_parse(select_input)))\n        history = append(history, select_input, observations[i])\n\n    return observations[-1]", "\n\ndesc = \"\"\"\n### Agent\n\nChain that executes different tools based on model decisions. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/srush/MiniChain/blob/master/examples/bash.ipynb)\n\n(Adapted from LangChain )\n\"\"\"\n", "\"\"\"\n\ngradio = show(run,\n              subprompts=[agent, tool_use] * 3,\n              examples=[\n                  \"I would please like a photo of a dog riding a skateboard. \"\n                  \"Please caption this image and create a song for it.\",\n                  'Use an image generator tool to draw a cat.',\n                  'Caption the image https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png from the internet'],\n              out_type=\"markdown\",", "                  'Caption the image https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png from the internet'],\n              out_type=\"markdown\",\n              description=desc,\n              show_advanced=False\n              )\nif __name__ == \"__main__\":\n    gradio.queue().launch()\n\n", ""]}
{"filename": "examples/minichain/example.py", "chunked_list": ["\nfrom minichain import show, prompt, OpenAI, GradioConf\nimport gradio as gr\nfrom gradio_tools.tools import StableDiffusionTool, ImageCaptioningTool\n\n@prompt(OpenAI())\ndef picture(model, query):\n    return model(query)\n\n@prompt(StableDiffusionTool(),", "\n@prompt(StableDiffusionTool(),\n        gradio_conf=GradioConf(\n            block_output= lambda: gr.Image(),\n            block_input= lambda: gr.Textbox(show_label=False)))\ndef gen(model, query):\n    return model(query)\n\n@prompt(ImageCaptioningTool(),\n        gradio_conf=GradioConf(", "@prompt(ImageCaptioningTool(),\n        gradio_conf=GradioConf(\n            block_input= lambda: gr.Image(),\n            block_output=lambda: gr.Textbox(show_label=False)))\ndef caption(model, img_src):\n    return model(img_src)\n\ndef gradio_example(query):\n    return caption(gen(picture(query)))\n", "\ndesc = \"\"\"\n### Gradio Tool\n\nExamples using the gradio tool [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/srush/MiniChain/blob/master/examples/gradio_example.ipynb)\n\n\"\"\"\n\ngradio = show(gradio_example,\n              subprompts=[picture, gen, caption],", "gradio = show(gradio_example,\n              subprompts=[picture, gen, caption],\n              examples=['Describe a one-sentence fantasy scene.',\n                        'Describe a one-sentence scene happening on the moon.'],\n              out_type=\"markdown\",\n              description=desc,\n              show_advanced=False\n              )\nif __name__ == \"__main__\":\n    gradio.queue().launch()", "if __name__ == \"__main__\":\n    gradio.queue().launch()\n\n"]}
