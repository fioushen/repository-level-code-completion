{"filename": "main.py", "chunked_list": ["import os\nimport sys\nimport random\nimport shutil\nimport logging\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.optim as optim", "import torch\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\n\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\n\nfrom test import test_all_case\nfrom networks.unet import UNet\nfrom utils.parser import Parser", "from networks.unet import UNet\nfrom utils.parser import Parser\nfrom networks.singlybranchedunet import UNetSingleBranchNetwork\nfrom networks.multiplebranchedunet import UNetMultiBranchNetwork\n\nfrom networks.utils import init_weights\nfrom trainutils.train_branched import train_c2f\nfrom trainutils.train_plain_unet import train_unet\nfrom trainutils.train_cross_pseudo_supervision import train_cps\nfrom trainutils.train_uncertainty_aware_mean_teacher import train_uamt", "from trainutils.train_cross_pseudo_supervision import train_cps\nfrom trainutils.train_uncertainty_aware_mean_teacher import train_uamt\n\nparser = argparse.ArgumentParser()\n# hyper settings\nparser.add_argument('--seed', type=int, default=1234, help='randomization seed')\nparser.add_argument('-g', '--gpu', type=int, default=0, help='gpu on which to train model')\n\n# experiment settings\nparser.add_argument('--bs', type=int, default=24, help='number of batch size')", "# experiment settings\nparser.add_argument('--bs', type=int, default=24, help='number of batch size')\nparser.add_argument('--lr', type=float, default=1e-2, help='base learning rate')\nparser.add_argument('--iter', type=int, default=40000, help='maximum training iterations')\nparser.add_argument('-m', '--mixup', action='store_true', help='whether to use label mixup')\nparser.add_argument('-p', '--pseudo', action='store_true', help='whether to use pseudo labeling')\nparser.add_argument('-s', '--sn', action='store_true', help='whether to use separate batchnorm')\nparser.add_argument('-c', '--pc', action='store_true', help='whether to use priority concatenation')\nparser.add_argument('--restore', action='store_true', help='whether to continue a previous training')\nparser.add_argument('--patch_size', type=list, default=[256, 256], help='size for network input')", "parser.add_argument('--restore', action='store_true', help='whether to continue a previous training')\nparser.add_argument('--patch_size', type=list, default=[256, 256], help='size for network input')\nparser.add_argument('--exp_name', type=str, default='mpsc_anycoarse', help='name of the current model')\nparser.add_argument('--model', choices=['unet', 'cps', 'uamt', 'branched'], default='branched', help='which type of model to train')\nparser.add_argument('--eval', type=str, default='dsc', choices=['dsc', 'hd95', 'precision', 'recall'], help='evaluation metric for saving model')\n\n# path settings\nparser.add_argument('--data_path', type=str, default='/data/dailinrui/dataset/prostate', help='root path for dataset')\nparser.add_argument('--model_path', type=str, default='/nas/dailinrui/SSL4MIS/model_final/prostate', help='root path for training model')\n", "parser.add_argument('--model_path', type=str, default='/nas/dailinrui/SSL4MIS/model_final/prostate', help='root path for training model')\n\n# number of dataset samples for SSL\n# for ACDC or any 3d database with a large interslice spacing and is trained per slice, this is the number of total slices\nparser.add_argument('--labeled_bs', type=int, default=4, help='how many samples are labeled')\nparser.add_argument('--total_num', type=int, default=458, help='how many samples in total')\nparser.add_argument('--labeled_num', type=int, default=77, help='how many samples are labeled')\n\n# network settings\nparser.add_argument('--feature_scale', type=int, default=2, help='feature scale per unet encoder/decoder step')", "# network settings\nparser.add_argument('--feature_scale', type=int, default=2, help='feature scale per unet encoder/decoder step')\nparser.add_argument('--base_feature', type=int, default=16, help='base feature channels for unet layer 0')\nparser.add_argument('--image_scale', type=int, default=2, help='image scale per unet encoder/decoder step')\nparser.add_argument('--is_batchnorm', type=bool, default=True, help='use batchnorm instead of instancenorm')\n\n# irrelevants\nparser.add_argument('--val_bs', type=int, default=1, help='batch size at val time')\nparser.add_argument('--val_step', type=int, default=200, help='do validation per val_step')\nparser.add_argument('--draw_step', type=int, default=50, help='add train graphic result per draw_step')", "parser.add_argument('--val_step', type=int, default=200, help='do validation per val_step')\nparser.add_argument('--draw_step', type=int, default=50, help='add train graphic result per draw_step')\nparser.add_argument('--save_step', type=int, default=5000, help='save model and optimizer state dict per save_step')\nparser.add_argument('--verbose', action='store_true', help='whether to display the loss information per iter')\nparser.add_argument('--init', type=str, choices=['kaiming', 'xavier', 'normal', 'orthogonal'], default='kaiming', help='network weight init type')\nparser.add_argument('--ckpt', type=str, choices=['best', 'last'], default='best', help='which kind of network is evaluated')\n\n# baseline settings\nparser.add_argument('--ema_decay', type=float,  default=0.99, help='ema_decay')\nparser.add_argument('--consistency', type=float, default=0.1, help='consistency')", "parser.add_argument('--ema_decay', type=float,  default=0.99, help='ema_decay')\nparser.add_argument('--consistency', type=float, default=0.1, help='consistency')\nparser.add_argument('--consistency_type', type=str, default=\"mse\", help='consistency_type')\nparser.add_argument('-n', '--nl', action='store_true', help='whether to use negative learning')\nparser.add_argument('--consistency_rampup', type=float, default=200.0, help='consistency_rampup')\n\nargs = parser.parse_args()\nparam = Parser(args).get_param()\n\n\ndef test(model):\n    \n    log = logging.getLogger()\n    for hdlr in log.handlers[:]:\n        log.removeHandler(hdlr)\n    log.addHandler(logging.FileHandler(os.path.join(param.path.path_to_test, \"test_log.txt\"), mode='w'))\n    log.addHandler(logging.StreamHandler(sys.stdout))\n    \n    save_model_path = os.path.join(param.path.path_to_model, f'{param.exp.exp_name}_{args.ckpt}_model.pth')\n    state_dicts = torch.load(save_model_path, map_location='cpu')\n    val_performance = state_dicts['metric']\n    val_performance2 = 0\n    if args.model == 'cps':\n        val_performance2 = state_dicts['metric2']\n    if val_performance2 > val_performance:\n        model.load_state_dict(state_dicts['model_state_dict2'])\n    else:\n        model.load_state_dict(state_dicts['model_state_dict'])\n        \n    logging.info(f\"init weight from {save_model_path},\\\n        performance on validation set is [{args.eval}] {max(val_performance, val_performance2)}\")\n    db_test = param.get_dataset(split='test')\n    testloader = DataLoader(db_test, num_workers=1, batch_size=1)\n    \n    model.eval()\n    test_all_case(model, param, testloader, stride_xy=64, stride_z=64, gpu_id=args.gpu)", "\n\ndef test(model):\n    \n    log = logging.getLogger()\n    for hdlr in log.handlers[:]:\n        log.removeHandler(hdlr)\n    log.addHandler(logging.FileHandler(os.path.join(param.path.path_to_test, \"test_log.txt\"), mode='w'))\n    log.addHandler(logging.StreamHandler(sys.stdout))\n    \n    save_model_path = os.path.join(param.path.path_to_model, f'{param.exp.exp_name}_{args.ckpt}_model.pth')\n    state_dicts = torch.load(save_model_path, map_location='cpu')\n    val_performance = state_dicts['metric']\n    val_performance2 = 0\n    if args.model == 'cps':\n        val_performance2 = state_dicts['metric2']\n    if val_performance2 > val_performance:\n        model.load_state_dict(state_dicts['model_state_dict2'])\n    else:\n        model.load_state_dict(state_dicts['model_state_dict'])\n        \n    logging.info(f\"init weight from {save_model_path},\\\n        performance on validation set is [{args.eval}] {max(val_performance, val_performance2)}\")\n    db_test = param.get_dataset(split='test')\n    testloader = DataLoader(db_test, num_workers=1, batch_size=1)\n    \n    model.eval()\n    test_all_case(model, param, testloader, stride_xy=64, stride_z=64, gpu_id=args.gpu)", " \n    \ndef maybe_restore_model(*models, num_optim_models=1):\n    optimizers = tuple(optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0001) for model in models)[:num_optim_models]\n    if args.restore:\n        \n        save_model_path = os.path.join(param.path.path_to_model, f'{param.exp.exp_name}_{args.ckpt}_model.pth')\n        if not os.path.exists(save_model_path):\n            print(f'the designated model path {save_model_path} does not exist')\n            logging.info(msg=param)\n            return models, optimizers\n        \n        state_dicts = torch.load(save_model_path, map_location='cpu')\n        models[0].load_state_dict(state_dicts['model_state_dict'])\n        optimizers[0].load_state_dict(state_dicts['optimizer_state_dict'])\n        if num_optim_models == 2:\n            models[1].load_state_dict(state_dicts['model2_state_dict'])\n            optimizers[1].load_state_dict(state_dicts['optimizer2_state_dict'])\n        else:\n            RuntimeError(f'not configured for more than 2 models')\n        base_lr = optimizers[0].param_groups[0]['lr']\n        max_iter = param.exp.max_iter - state_dicts['iterations']\n        \n        assert max_iter > 0, f\"restoring from a model trained more than current configured max_iteration {param.exp.max_iter}\"\n        logging.info(f\"restoring from {save_model_path}, base_lr {param.exp.base_lr} -> {base_lr}, max_iter {param.exp.max_iter} -> {max_iter}\")\n        param.exp.base_lr = base_lr\n        param.exp.max_iter = max_iter\n    \n    logging.info(msg=param)\n    return models, optimizers", "\n\ndef main():\n    global param\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n\n    logging.basicConfig(\n        level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s',\n        datefmt='%H:%M:%S'\n    )\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.getLogger().addHandler(logging.FileHandler(os.path.join(param.path.path_to_snapshot, \"log.txt\"), mode='w'))\n    model = None\n\n    if args.model == 'unet':\n        model = UNet(param).cuda(args.gpu)\n        init_weights(model, args.init)\n        train_unet(*maybe_restore_model(model), param, args)\n    \n    elif args.model == 'branched':\n        if param.dataset.n_coarse > 2:\n            model = UNetMultiBranchNetwork(param).cuda(args.gpu)\n        elif param.dataset.n_coarse == 2:\n            model = UNetSingleBranchNetwork(param).cuda(args.gpu)\n        init_weights(model, args.init)\n        train_c2f(*maybe_restore_model(model), param, args)\n        \n    elif args.model == 'cps':\n        param.exp.pseudo_label = False\n        param.exp.mixup_label = False\n        param.exp.separate_norm = False\n        param.exp.priority_cat = False\n        if param.dataset.n_coarse > 2:\n            model = UNetMultiBranchNetwork(param).cuda(args.gpu)\n            model2 = UNetMultiBranchNetwork(param).cuda(args.gpu)\n        elif param.dataset.n_coarse == 2:\n            model = UNetSingleBranchNetwork(param).cuda(args.gpu)\n            model2 = UNetSingleBranchNetwork(param).cuda(args.gpu)\n        init_weights(model, 'kaiming')\n        init_weights(model2, 'xavier')\n        train_cps(*maybe_restore_model(model, model2, num_optim_models=2), param, args)\n        \n    elif args.model == 'uamt':\n        param.exp.pseudo_label = False\n        param.exp.mixup_label = False\n        param.exp.separate_norm = False\n        param.exp.priority_cat = False\n        if param.dataset.n_coarse > 2:\n            model = UNetMultiBranchNetwork(param).cuda(args.gpu)\n            ema_model = UNetMultiBranchNetwork(param).cuda(args.gpu)\n        elif param.dataset.n_coarse == 2:\n            model = UNetSingleBranchNetwork(param).cuda(args.gpu)\n            ema_model = UNetSingleBranchNetwork(param).cuda(args.gpu)\n        init_weights(model, args.init)\n        init_weights(ema_model, args.init)\n        for params in ema_model.parameters():\n            params.detach_()\n        train_uamt(*maybe_restore_model(model, ema_model), param, args)\n        \n    test(model)\n    print(f'train-test over for {param.exp.exp_name}')", "    \n\nif __name__ == '__main__':\n    main()"]}
{"filename": "val.py", "chunked_list": ["import os\nimport math\nimport json\nimport torch\nimport shutil\nimport argparse\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom medpy import metric", "from tqdm import tqdm\nfrom medpy import metric\nimport SimpleITK as sitk\n# from meta import db as param\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict, namedtuple\n\nfrom networks.singlybranchedunet import UNetSingleBranchNetwork\nfrom utils.visualize import visualize", "from networks.singlybranchedunet import UNetSingleBranchNetwork\nfrom utils.visualize import visualize\nfrom dataloaders.base_dataset import BaseDataset\n\nparam = None\neval_metrics = [metric.binary.dc, metric.binary.hd95, metric.binary.precision, metric.binary.recall]\nn_eval = len(eval_metrics)\n\n\ndef test_single_case(net, parameter, sampled_batch, stride_xy, stride_z=0, gpu_id=None, save_pred=False, ids=None):\n    \n    global param\n    param = parameter\n    \n    image, gt_c, gt_f =\\\n        sampled_batch['image'].cuda(gpu_id),\\\n        sampled_batch['coarse'].detach().numpy(),\\\n        sampled_batch['fine'].detach().numpy()\n    \n    if parameter.dataset.n_dim == 3:\n        pred_c, pred_f, feat_map = test_single_case_3d(net, image, stride_xy, stride_z, parameter.exp.patch_size)\n    elif parameter.dataset.n_dim == 2.5:\n        pred_c, pred_f, feat_map = test_single_case_3to2d(net, image, stride_xy, parameter.exp.patch_size)\n    elif parameter.dataset.n_dim == 2: \n        pred_c, pred_f, feat_map = test_single_case_2d(net, image, stride_xy, parameter.exp.patch_size)\n    \n    metric_c, metric_f = np.zeros((parameter.dataset.n_coarse, n_eval)), np.zeros((parameter.dataset.n_fine, n_eval))\n    # only evaluate the performance of foreground segmentation\n    for c in range(1, parameter.dataset.n_coarse): \n        metric_c[c-1] = calculate_metric_percase(pred_c == c, gt_c == c)\n    metric_c[-1] = metric_c[:-1].mean(axis=0)\n    for f in range(1, parameter.dataset.n_fine): \n        metric_f[f-1] = calculate_metric_percase(pred_f == f, gt_f == f)\n    metric_f[-1] = metric_f[:-1].mean(axis=0)\n    \n    image = image.squeeze(0)\n    \n    if save_pred:\n        if ids is None:\n            ids = 'dummy'\n        \n        img_save_path = os.path.join(parameter.path.path_to_test, ids)\n        os.makedirs(img_save_path, exist_ok=True)\n        \n        for mode in range(parameter.dataset.n_mode):\n            img_itk = sitk.GetImageFromArray(image[mode].cpu().detach().numpy())\n            img_itk.SetSpacing((1.0, 1.0, 1.0))\n            sitk.WriteImage(img_itk, img_save_path + f\"/{ids}_img_mode{mode+1}.nii.gz\")\n\n        pred_itk = sitk.GetImageFromArray(pred_f.astype(np.uint8))\n        pred_itk.SetSpacing((1.0, 1.0, 1.0))\n        sitk.WriteImage(pred_itk, img_save_path + f\"/{ids}_pred_fine.nii.gz\")\n\n        lab_itk = sitk.GetImageFromArray(gt_f.astype(np.uint8))\n        lab_itk.SetSpacing((1.0, 1.0, 1.0))\n        sitk.WriteImage(lab_itk, img_save_path + f\"/{ids}_gt_fine.nii.gz\")\n        \n    return metric_c, metric_f, feat_map", "\ndef test_single_case(net, parameter, sampled_batch, stride_xy, stride_z=0, gpu_id=None, save_pred=False, ids=None):\n    \n    global param\n    param = parameter\n    \n    image, gt_c, gt_f =\\\n        sampled_batch['image'].cuda(gpu_id),\\\n        sampled_batch['coarse'].detach().numpy(),\\\n        sampled_batch['fine'].detach().numpy()\n    \n    if parameter.dataset.n_dim == 3:\n        pred_c, pred_f, feat_map = test_single_case_3d(net, image, stride_xy, stride_z, parameter.exp.patch_size)\n    elif parameter.dataset.n_dim == 2.5:\n        pred_c, pred_f, feat_map = test_single_case_3to2d(net, image, stride_xy, parameter.exp.patch_size)\n    elif parameter.dataset.n_dim == 2: \n        pred_c, pred_f, feat_map = test_single_case_2d(net, image, stride_xy, parameter.exp.patch_size)\n    \n    metric_c, metric_f = np.zeros((parameter.dataset.n_coarse, n_eval)), np.zeros((parameter.dataset.n_fine, n_eval))\n    # only evaluate the performance of foreground segmentation\n    for c in range(1, parameter.dataset.n_coarse): \n        metric_c[c-1] = calculate_metric_percase(pred_c == c, gt_c == c)\n    metric_c[-1] = metric_c[:-1].mean(axis=0)\n    for f in range(1, parameter.dataset.n_fine): \n        metric_f[f-1] = calculate_metric_percase(pred_f == f, gt_f == f)\n    metric_f[-1] = metric_f[:-1].mean(axis=0)\n    \n    image = image.squeeze(0)\n    \n    if save_pred:\n        if ids is None:\n            ids = 'dummy'\n        \n        img_save_path = os.path.join(parameter.path.path_to_test, ids)\n        os.makedirs(img_save_path, exist_ok=True)\n        \n        for mode in range(parameter.dataset.n_mode):\n            img_itk = sitk.GetImageFromArray(image[mode].cpu().detach().numpy())\n            img_itk.SetSpacing((1.0, 1.0, 1.0))\n            sitk.WriteImage(img_itk, img_save_path + f\"/{ids}_img_mode{mode+1}.nii.gz\")\n\n        pred_itk = sitk.GetImageFromArray(pred_f.astype(np.uint8))\n        pred_itk.SetSpacing((1.0, 1.0, 1.0))\n        sitk.WriteImage(pred_itk, img_save_path + f\"/{ids}_pred_fine.nii.gz\")\n\n        lab_itk = sitk.GetImageFromArray(gt_f.astype(np.uint8))\n        lab_itk.SetSpacing((1.0, 1.0, 1.0))\n        sitk.WriteImage(lab_itk, img_save_path + f\"/{ids}_gt_fine.nii.gz\")\n        \n    return metric_c, metric_f, feat_map", "\n    \ndef test_single_case_2d(net, image, stride_xy, patch_size):\n    \n    _, _, w, h = image.shape\n    add_pad = False\n    if w < patch_size[0]:\n        w_pad = patch_size[0]-w\n        add_pad = True\n    else:\n        w_pad = 0\n    if h < patch_size[1]:\n        h_pad = patch_size[1]-h\n        add_pad = True\n    else:\n        h_pad = 0\n    wl_pad, wr_pad = w_pad//2, w_pad-w_pad//2\n    hl_pad, hr_pad = h_pad//2, h_pad-h_pad//2\n    if add_pad:\n        image = F.pad(image, (hl_pad, hr_pad, wl_pad, wr_pad), mode='constant')\n    \n    bb, _, ww, hh = image.shape\n\n    sx = math.ceil((ww - patch_size[0]) / stride_xy) + 1\n    sy = math.ceil((hh - patch_size[1]) / stride_xy) + 1\n    score_map_c = np.zeros((bb, param.dataset.n_coarse, ww, hh)).astype(np.float32)\n    score_map_f = np.zeros((bb, param.dataset.n_fine, ww, hh)).astype(np.float32)\n    feat_map_f = np.zeros((bb, param.network.base_feature_num, ww, hh)).astype(np.float32)\n    cnt = np.zeros((bb, ww, hh)).astype(np.float32)\n\n    for x in range(0, sx):\n        xs = min(stride_xy * x, ww-patch_size[0])\n        for y in range(0, sy):\n            ys = min(stride_xy * y, hh-patch_size[1])\n            test_patch = image[:, :, xs:xs+patch_size[0],ys:ys+patch_size[1]]\n\n            with torch.no_grad():\n                out = net(test_patch)\n                try:\n                    y1c, y1f, feat = out['coarse_logit'], out['fine_logit'], out['feature_map']\n                except KeyError:\n                    y1f, feat = out['logit'], out['feature_map']\n                    y1c = torch.cat([y1f[:, 0:1], y1f[:, 1:].sum(dim=1, keepdim=True)], dim=1)\n                yc, yf = torch.softmax(y1c, dim=1), torch.softmax(y1f, dim=1)\n                \n            yc = yc.cpu().data.numpy()\n            yf = yf.cpu().data.numpy()\n            feat = feat.cpu().data.numpy()\n            \n            feat_map_f[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1]] = \\\n                feat_map_f[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1]] + feat\n            \n            score_map_c[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1]] \\\n                = score_map_c[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1]] + yc\n            score_map_f[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1]] \\\n                = score_map_f[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1]] + yf\n            cnt[:, xs:xs+patch_size[0], ys:ys+patch_size[1]] \\\n                = cnt[:, xs:xs+patch_size[0], ys:ys+patch_size[1]] + 1\n                \n    score_map_c = score_map_c/np.expand_dims(cnt, axis=1)\n    score_map_f = score_map_f/np.expand_dims(cnt, axis=1)\n    feat_map_f = feat_map_f/np.expand_dims(cnt, axis=1)\n    label_map_c = np.argmax(score_map_c, axis=1)\n    label_map_f = np.argmax(score_map_f, axis=1)\n    \n    if add_pad:\n        label_map_c = label_map_c[:, wl_pad:wl_pad+w, hl_pad:hl_pad+h]\n        label_map_f = label_map_f[:, wl_pad:wl_pad+w, hl_pad:hl_pad+h]\n        score_map_c = score_map_c[:, :, wl_pad:wl_pad + w, hl_pad:hl_pad+h]\n        score_map_f = score_map_f[:, :, wl_pad:wl_pad + w, hl_pad:hl_pad+h]\n        feat_map_f = feat_map_f[:, :, wl_pad:wl_pad + w, hl_pad:hl_pad+h]\n    \n    feat_map_f = np.mean(feat_map_f, axis=0)\n\n    return label_map_c, label_map_f, feat_map_f", "\n\ndef test_single_case_3to2d(net, image, stride_xy, patch_size):\n    # special case for 3d images that are sliced to 2d inputs\n\n    _, _, _, h, d = image.shape\n    add_pad = False\n    if h < patch_size[0]:\n        h_pad = patch_size[0]-h\n        add_pad = True\n    else:\n        h_pad = 0\n    if d < patch_size[1]:\n        d_pad = patch_size[1]-d\n        add_pad = True\n    else:\n        d_pad = 0\n    hl_pad, hr_pad = h_pad//2, h_pad-h_pad//2\n    dl_pad, dr_pad = d_pad//2, d_pad-d_pad//2\n    if add_pad:\n        image = F.pad(image, [dl_pad, dr_pad, hl_pad, hr_pad, 0, 0], mode='constant')\n    bb, _, ww, hh, dd = image.shape\n\n    sy = math.ceil((hh - patch_size[0]) / stride_xy) + 1\n    sz = math.ceil((dd - patch_size[1]) / stride_xy) + 1\n    score_map_c = np.zeros((bb, param.dataset.n_coarse, ww, hh, dd)).astype(np.float32)\n    score_map_f = np.zeros((bb, param.dataset.n_fine, ww, hh, dd)).astype(np.float32)\n    feat_map_f = np.zeros((bb, param.network.base_feature_num, ww, hh, dd)).astype(np.float32)\n    cnt = np.zeros((bb, ww, hh, dd)).astype(np.float32)\n\n    for xs in range(0, ww):\n        for y in range(0, sy):\n            ys = min(stride_xy * y, hh-patch_size[0])\n            for z in range(0, sz):\n                zs = min(stride_xy * z, dd-patch_size[1])\n                test_patch = image[:, :, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]]\n\n                with torch.no_grad():\n                    out = net(test_patch)\n                    try:\n                        y1c, y1f, feat = out['coarse_logit'], out['fine_logit'], out['feature_map']\n                    except KeyError:\n                        y1f, feat = out['logit'], out['feature_map']\n                        y1c = torch.cat([y1f[:, 0:1], y1f[:, 1:].sum(dim=1, keepdim=True)], dim=1)\n                    yc, yf = torch.softmax(y1c, dim=1), torch.softmax(y1f, dim=1)\n                    \n                yc = yc.cpu().data.numpy()\n                yf = yf.cpu().data.numpy()\n                feat = feat.cpu().data.numpy()\n                \n                feat_map_f[:, :, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]] = \\\n                    feat_map_f[:, :, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]] + feat\n                \n                score_map_c[:, :, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]] \\\n                    = score_map_c[:, :, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]] + yc\n                score_map_f[:, :, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]] \\\n                    = score_map_f[:, :, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]] + yf\n                cnt[:, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]] \\\n                    = cnt[:, xs, ys:ys+patch_size[0], zs:zs+patch_size[1]] + 1\n            \n            # print((xs, (ys, ys+patch_size[0]), (zs, zs+patch_size[1])), (cnt.sum() / np.prod(cnt.shape),))\n                \n    score_map_c = score_map_c/np.expand_dims(cnt, axis=1)\n    score_map_f = score_map_f/np.expand_dims(cnt, axis=1)\n    feat_map_f = feat_map_f/np.expand_dims(cnt, axis=1)\n    label_map_c = np.argmax(score_map_c, axis=1)\n    label_map_f = np.argmax(score_map_f, axis=1)\n    \n    if add_pad:\n        label_map_c = label_map_c[:, :, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n        label_map_f = label_map_f[:, :, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n        score_map_c = score_map_c[:, :, :, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n        score_map_f = score_map_f[:, :, :, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n        feat_map_f = feat_map_f[:, :, :, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n    feat_map_f = np.mean(feat_map_f, axis=0)\n\n    return label_map_c, label_map_f, feat_map_f", "\n\n\ndef test_single_case_3d(net, image, stride_xy, stride_z, patch_size):\n\n    _, _, w, h, d = image.shape\n    add_pad = False\n    if w < patch_size[0]:\n        w_pad = patch_size[0]-w\n        add_pad = True\n    else:\n        w_pad = 0\n    if h < patch_size[1]:\n        h_pad = patch_size[1]-h\n        add_pad = True\n    else:\n        h_pad = 0\n    if d < patch_size[2]:\n        d_pad = patch_size[2]-d\n        add_pad = True\n    else:\n        d_pad = 0\n    wl_pad, wr_pad = w_pad//2, w_pad-w_pad//2\n    hl_pad, hr_pad = h_pad//2, h_pad-h_pad//2\n    dl_pad, dr_pad = d_pad//2, d_pad-d_pad//2\n    if add_pad:\n        image = F.pad(image, [dl_pad, dr_pad, hl_pad, hr_pad, wl_pad, wr_pad], mode='constant')\n    bb, _, ww, hh, dd = image.shape\n\n    sx = math.ceil((ww - patch_size[0]) / stride_xy) + 1\n    sy = math.ceil((hh - patch_size[1]) / stride_xy) + 1\n    sz = math.ceil((dd - patch_size[2]) / stride_z) + 1\n    score_map_c = np.zeros((bb, param.dataset.n_coarse, ww, hh, dd)).astype(np.float32)\n    score_map_f = np.zeros((bb, param.dataset.n_fine, ww, hh, dd)).astype(np.float32)\n    feat_map_f = np.zeros((bb, param.network.base_feature_num, ww, hh, dd)).astype(np.float32)\n    cnt = np.zeros((bb, ww, hh, dd)).astype(np.float32)\n\n    for x in range(0, sx):\n        xs = min(stride_xy * x, ww-patch_size[0])\n        for y in range(0, sy):\n            ys = min(stride_xy * y, hh-patch_size[1])\n            for z in range(0, sz):\n                zs = min(stride_z * z, dd-patch_size[2])\n                test_patch = image[:, :, xs:xs+patch_size[0],ys:ys+patch_size[1], zs:zs+patch_size[2]]\n\n                with torch.no_grad():\n                    out = net(test_patch)\n                    try:\n                        y1c, y1f, feat = out['coarse_logit'], out['fine_logit'], out['feature_map']\n                    except KeyError:\n                        y1f, feat = out['logit'], out['feature_map']\n                        y1c = torch.cat([y1f[:, 0:1], y1f[:, 1:].sum(dim=1, keepdim=True)], dim=1)\n                    yc, yf = torch.softmax(y1c, dim=1), torch.softmax(y1f, dim=1)\n                    \n                yc = yc.cpu().data.numpy()\n                yf = yf.cpu().data.numpy()\n                feat = feat.cpu().data.numpy()\n                \n                feat_map_f[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] = \\\n                    feat_map_f[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + feat\n                \n                score_map_c[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n                    = score_map_c[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + yc\n                score_map_f[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n                    = score_map_f[:, :, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + yf\n                cnt[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] \\\n                    = cnt[:, xs:xs+patch_size[0], ys:ys+patch_size[1], zs:zs+patch_size[2]] + 1\n                \n    score_map_c = score_map_c/np.expand_dims(cnt, axis=1)\n    score_map_f = score_map_f/np.expand_dims(cnt, axis=1)\n    feat_map_f = feat_map_f/np.expand_dims(cnt, axis=1)\n    label_map_c = np.argmax(score_map_c, axis=1)\n    label_map_f = np.argmax(score_map_f, axis=1)\n    \n    if add_pad:\n        label_map_c = label_map_c[:, wl_pad:wl_pad+w, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n        label_map_f = label_map_f[:, wl_pad:wl_pad+w, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n        score_map_c = score_map_c[:, :, wl_pad:wl_pad + w, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n        score_map_f = score_map_f[:, :, wl_pad:wl_pad + w, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n        feat_map_f = feat_map_f[:, :, wl_pad:wl_pad + w, hl_pad:hl_pad+h, dl_pad:dl_pad+d]\n    feat_map_f = np.mean(feat_map_f, axis=0)\n\n    return label_map_c, label_map_f, feat_map_f", "\n\ndef calculate_metric_percase(pred, gt):\n    ret = np.zeros((len(eval_metrics),))\n    if pred.sum() > 0 and gt.sum() > 0:\n        for i, met in enumerate(eval_metrics):\n            ret[i] = met(pred, gt)\n\n    return ret\n", "\n"]}
{"filename": "train_plain_unet.py", "chunked_list": ["#!usr/bin/env python\n\nimport os\nimport sys\nimport math\nimport random\nimport shutil\nimport logging\nimport argparse\nimport numpy as np", "import argparse\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nfrom tensorboardX import SummaryWriter", "\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\nfrom test import test_all_case\nfrom val import test_single_case\nfrom utils import losses\nfrom networks.unet import UNet\nfrom utils.parser import Parser", "from networks.unet import UNet\nfrom utils.parser import Parser\nfrom dataloaders.refuge2020 import Refuge2020\nfrom utils.visualize import make_curve, make_image\n\nparser = argparse.ArgumentParser()\n# hyper settings\nparser.add_argument('-s', '--seed', type=int, default=1234, help='randomization seed')\nparser.add_argument('-g', '--gpu', type=int, default=1, help='gpu on which to train model')\n", "parser.add_argument('-g', '--gpu', type=int, default=1, help='gpu on which to train model')\n\n# experiment settings\nparser.add_argument('--bs', type=int, default=24, help='number of batch size')\nparser.add_argument('--lr', type=float, default=1e-2, help='base learning rate')\nparser.add_argument('--iter', type=int, default=40000, help='maximum training iterations')\nparser.add_argument('--eval', type=str, default='dsc', help='evaluation metric for saving model: [dsc, hd95, precision, recall]')\nparser.add_argument('--mixup', action='store_true', help='whether to use label mixup')\nparser.add_argument('--pseudo', action='store_true', help='whether to use pseudo labeling')\nparser.add_argument('--sn', action='store_true', help='whether to use separate batchnorm')", "parser.add_argument('--pseudo', action='store_true', help='whether to use pseudo labeling')\nparser.add_argument('--sn', action='store_true', help='whether to use separate batchnorm')\nparser.add_argument('--pc', action='store_true', help='whether to use priority concatenation')\nparser.add_argument('--restore', action='store_true', help='whether to continue a previous training')\nparser.add_argument('--patch_size', type=list, default=[256, 256], help='size for network input')\nparser.add_argument('--exp_name', type=str, default='test', help='name of the current model')\n\n# path settings\nparser.add_argument('--data_path', type=str, default='/data/dailinrui/dataset/refuge2020_trainExpand', help='root path for dataset')\nparser.add_argument('--model_path', type=str, default='/nas/dailinrui/SSL4MIS/model_final/REFUGE2020', help='root path for training model')", "parser.add_argument('--data_path', type=str, default='/data/dailinrui/dataset/refuge2020_trainExpand', help='root path for dataset')\nparser.add_argument('--model_path', type=str, default='/nas/dailinrui/SSL4MIS/model_final/REFUGE2020', help='root path for training model')\n\n# number of dataset samples for SSL\n# for ACDC or any 3d database with a large interslice spacing, this is the number of total slices\nparser.add_argument('--total_num', type=int, default=1312, help='how many samples in total')\nparser.add_argument('--labeled_num', type=int, default=1312, help='how many samples are labeled')\n\n# network settings\nparser.add_argument('--feature_scale', type=int, default=2, help='feature scale per unet encoder/decoder step')", "# network settings\nparser.add_argument('--feature_scale', type=int, default=2, help='feature scale per unet encoder/decoder step')\nparser.add_argument('--base_feature', type=int, default=16, help='base feature channels for unet layer 0')\nparser.add_argument('--image_scale', type=int, default=2, help='image scale per unet encoder/decoder step')\nparser.add_argument('--is_batchnorm', type=bool, default=True, help='use batchnorm instead of instancenorm')\n\n# irrelevants\nparser.add_argument('--val_bs', type=int, default=1, help='batch size at val time')\nparser.add_argument('--val_step', type=int, default=200, help='do validation per val_step')\nparser.add_argument('--draw_step', type=int, default=20, help='add train graphic result per draw_step')", "parser.add_argument('--val_step', type=int, default=200, help='do validation per val_step')\nparser.add_argument('--draw_step', type=int, default=20, help='add train graphic result per draw_step')\nparser.add_argument('--verbose', action='store_true', help='whether to display the loss information per iter')\nargs = parser.parse_args()\nparameter = Parser(args).get_param()\n\n\ndef train(model, restore=False):\n    list_trained_iterations = os.listdir(parameter.path.path_to_model)\n    base_lr = parameter.exp.base_lr\n    best_performance = 0.0\n    iter_num = 0\n    loss = {}\n    \n    if len(list_trained_iterations) == 0:\n        restore = False\n        \n    if restore:\n        list_trained_iterations.remove(f'{parameter.exp.exp_name}_best_model.pth')\n        max_trained_iterations = list(map(lambda x: int(x.split('_')[1].rstrip('.pth')), list_trained_iterations))\n        restore_itr = int(max_trained_iterations[-1])\n\n        model_name = f'iter_{restore_itr}'\n        for name in list_trained_iterations:\n            if name.startswith(model_name):\n                model_name = name\n                break\n        else:\n            print('valid model not found')\n            raise ValueError\n        save_model_path = os.path.join(parameter.path.path_to_model, model_name)\n        model.load_state_dict(torch.load(save_model_path, map_location='cpu'))\n        best_performance = max([float(f.split('_')[-1].rstrip('.pth')) for f in list_trained_iterations if 'd' in f])\n        print(\"init weight from {}, current best performance is {}\".format(save_model_path, best_performance))\n        base_lr = base_lr * (restore_itr / parameter.exp.max_iter)\n        iter_num = restore_itr\n        \n    batch_size = parameter.exp.batch_size\n    max_iterations = parameter.exp.max_iter\n\n    db_train = parameter.get_dataset(parameter, split='train')\n    db_val = parameter.get_dataset(parameter, split='val')\n    \n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train,\n                             shuffle=True,\n                             num_workers=4,\n                             pin_memory=True,\n                             batch_size=batch_size,\n                             worker_init_fn=worker_init_fn)\n    \n    valloader = DataLoader(db_val, num_workers=args.val_bs, batch_size=args.val_bs)\n    if args.val_bs > 1:\n        print(f\"setting a validation batch size={args.val_bs} > 1 may provide inaccurate results while saving some time\")\n\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n    \n    ce_loss = CrossEntropyLoss()\n    nce_loss = losses.NegativeCrossEntropyLoss()\n    dice_loss_fine = losses.DiceLoss(parameter.dataset.n_fine) \n\n    writer = SummaryWriter(os.path.join(parameter.path.path_to_snapshot, \"log\"))\n    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n\n    max_epoch = (max_iterations - iter_num) // (len(trainloader)) + 1\n    epoch_iterator = tqdm(range(max_epoch), ncols=100, position=0, leave=True, desc='Training Progress')\n    torch.autograd.set_detect_anomaly(True)\n    \n    for epoch_num in epoch_iterator:\n        for epoch_num, sampled_batch in enumerate(trainloader):\n            q_im, q_lf = sampled_batch['image'], sampled_batch['fine']\n\n            if args.gpu >= 0:\n                q_im, q_lf = q_im.cuda(args.gpu), q_lf.cuda(args.gpu)\n            else:\n                raise RuntimeError(f'Specify a valid gpu id')\n\n            out = model(q_im)\n            out = out['logit']\n            \n            soft = torch.softmax(out, dim=1)\n            pred = torch.argmax(soft, dim=1)\n            \n            loss_ce = ce_loss(out, q_lf)\n            loss_dice = dice_loss_fine(soft, q_lf)\n            loss_fine = 0.5 * (loss_ce + loss_dice)\n            \n            loss['supervise loss fine'] = loss_fine\n            \n            make_curve(writer, pred, q_lf, 'train', parameter.dataset.n_fine, iter_num)\n\n            # loss4 = nce_loss(out_fine[param.exp.labeled_batch_size:], q_lc[param.exp.labeled_batch_size:])\n            # loss['negative learning loss'] = loss4\n\n            loss_sum = sum(loss.values())    \n            optimizer.zero_grad()\n            loss_sum.backward()\n            optimizer.step()\n\n            lr_ = base_lr * (1.0 - iter_num / max_iterations)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar(f'{parameter.exp.exp_name}/lr', lr_, iter_num)\n            writer.add_scalar('loss/total_loss', loss_sum, iter_num)\n            writer.add_scalars('loss/individual_losses', loss, iter_num)\n\n            if args.verbose:\n                loss_names = list(loss.keys())\n                loss_values = list(map(lambda x: str(round(x.item(), 3)), loss.values()))\n                loss_log = ['*'] * (2 * len(loss_names))\n                loss_log[::2] = loss_names\n                loss_log[1::2] = loss_values\n                loss_log = '; '.join(loss_log)\n                logging.info(f\"model {parameter.exp.exp_name} iteration {iter_num} : total loss: {loss_sum.item():.3f}; \\t\" + loss_log)\n\n            if iter_num > 0 and iter_num % args.draw_step == 0:\n                make_image(writer, parameter, q_im, 'image/input_image', iter_num, normalize=True)\n                make_image(writer, parameter, q_lf, 'image/gt', iter_num, parameter.dataset.n_fine)\n                make_image(writer, parameter, pred, 'image/pred', iter_num, parameter.dataset.n_fine)\n\n            if iter_num > 0 and iter_num % args.val_step == 0:\n                model.eval()\n                avg_metric_f = np.zeros((len(valloader), parameter.dataset.n_fine, 4))\n                for case_index, sampled_batch in tqdm(enumerate(valloader), position=1, leave=True, desc='Validation Progress'):\n                    _, batch_metric_f, _ = test_single_case(\n                        model, parameter, sampled_batch, stride_xy=parameter.exp.patch_size[0], stride_z=parameter.exp.patch_size[-1], gpu_id=args.gpu\n                    )\n                    avg_metric_f[case_index] = batch_metric_f\n                \n                if avg_metric_f[:, -1, parameter.exp.eval_metric].mean() > best_performance:\n                    best_performance = avg_metric_f[:, -1, parameter.exp.eval_metric].mean()\n                    save_model_path = os.path.join(parameter.path.path_to_model, 'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance, 4)))\n                    save_best = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n                    torch.save(model.state_dict(), save_model_path)\n                    torch.save(model.state_dict(), save_best)\n                \n                for index, name in enumerate(['dsc', 'hd95', 'precision', 'recall']):\n                    writer.add_scalars(f'val/{name}', {f'fine label={i}': avg_metric_f[:, i-1, index].mean() for i in range(1, parameter.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/{name}', {f'fine avg': avg_metric_f[:, -1, index].mean()}, iter_num)\n\n                logging.info(f'\\riteration {iter_num} : dice_score : {avg_metric_f[:, -1, 0].mean():.5f} hd95 : {avg_metric_f[:, -1, 1].mean():.5f}')\n                model.train()\n\n            if iter_num % 5000 == 0:\n                save_model_path = os.path.join(parameter.path.path_to_model, 'iter_' + str(iter_num) + '.pth')\n                torch.save(model.state_dict(), save_model_path)\n                logging.info(\"save model to {}\".format(save_model_path))\n\n            if iter_num >= max_iterations:\n                break\n            \n        if iter_num >= max_iterations:\n            epoch_iterator.close()\n            break\n    writer.close()\n    return \"Training Finished!\"", "\n\ndef test(model):\n    \n    save_model_path = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n    model.load_state_dict(torch.load(save_model_path))\n    print(\"init weight from {}\".format(save_model_path))\n    \n    db_test = Refuge2020(parameter, split='test')\n    testloader = DataLoader(db_test, num_workers=1, batch_size=1)\n    \n    model.eval()\n    avg_metric_c, avg_metric_f =\\\n        test_all_case(model, parameter, testloader, stride_xy=64, stride_z=64, gpu_id=args.gpu)\n    \n    print(avg_metric_c)\n    print(avg_metric_f)", "\n\nif __name__ == \"__main__\":\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n\n    logging.basicConfig(\n        filename=os.path.join(parameter.path.path_to_snapshot, \"log.txt\"),\n        level=logging.INFO, format='[%(asctime)s.%(msecs)03d] [%(levelname)-5s] %(message)s',\n        datefmt='%H:%M:%S'\n    )\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(msg=parameter)\n    \n    model = UNet(parameter).cuda(args.gpu)\n    \n    train(model, restore=parameter.exp.restore)\n    test(model)\n    print(f'train-test over for {parameter.exp.exp_name}')"]}
{"filename": "train_proposed.py", "chunked_list": ["#!usr/bin/env python\n\nimport os\nimport sys\nimport math\nimport random\nimport shutil\nimport logging\nimport argparse\nimport numpy as np", "import argparse\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nfrom torchvision import transforms", "\nfrom torchvision import transforms\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\nfrom test import test_all_case\nfrom val import test_single_case\nfrom utils import losses\nfrom utils.parser import Parser", "from utils import losses\nfrom utils.parser import Parser\nfrom networks.proposed import UNetBasedNetwork\nfrom dataloaders.refuge2020 import Refuge2020\nfrom utils.visualize import make_curve, make_image\nfrom dataloaders.utils import RandomRotFlip, RandomNoise, TwoStreamBatchSampler\n\nparser = argparse.ArgumentParser()\n# hyper settings\nparser.add_argument('-s', '--seed', type=int, default=1234, help='randomization seed')", "# hyper settings\nparser.add_argument('-s', '--seed', type=int, default=1234, help='randomization seed')\nparser.add_argument('-g', '--gpu', type=int, default=1, help='gpu on which to train model')\n\n# experiment settings\nparser.add_argument('--bs', type=int, default=24, help='number of batch size')\nparser.add_argument('--lr', type=float, default=1e-2, help='base learning rate')\nparser.add_argument('--iter', type=int, default=40000, help='maximum training iterations')\nparser.add_argument('--eval', type=str, default='dsc', choices=['dsc', 'hd95', 'precision', 'recall'], help='evaluation metric for saving model')\nparser.add_argument('--mixup', action='store_true', help='whether to use label mixup')", "parser.add_argument('--eval', type=str, default='dsc', choices=['dsc', 'hd95', 'precision', 'recall'], help='evaluation metric for saving model')\nparser.add_argument('--mixup', action='store_true', help='whether to use label mixup')\nparser.add_argument('--pseudo', action='store_true', help='whether to use pseudo labeling')\nparser.add_argument('--sn', action='store_true', help='whether to use separate batchnorm')\nparser.add_argument('--pc', action='store_true', help='whether to use priority concatenation')\nparser.add_argument('--restore', action='store_true', help='whether to continue a previous training')\nparser.add_argument('--patch_size', type=list, default=[256, 256], help='size for network input')\nparser.add_argument('--exp_name', type=str, default='Proposed_10', help='name of the current model')\n\n# path settings", "\n# path settings\nparser.add_argument('--data_path', type=str, default='/data/dailinrui/dataset/refuge2020_trainExpand', help='root path for dataset')\nparser.add_argument('--model_path', type=str, default='/nas/dailinrui/SSL4MIS/model_final/REFUGE2020', help='root path for training model')\n\n# number of dataset samples for SSL\n# for ACDC or any 3d database with a large interslice spacing and is trained per slice, this is the number of total slices\nparser.add_argument('--labeled_bs', type=int, default=4, help='how many samples are labeled')\nparser.add_argument('--total_num', type=int, default=252, help='how many samples in total')\nparser.add_argument('--labeled_num', type=int, default=10, help='how many samples are labeled')", "parser.add_argument('--total_num', type=int, default=252, help='how many samples in total')\nparser.add_argument('--labeled_num', type=int, default=10, help='how many samples are labeled')\n\n# network settings\nparser.add_argument('--feature_scale', type=int, default=2, help='feature scale per unet encoder/decoder step')\nparser.add_argument('--base_feature', type=int, default=16, help='base feature channels for unet layer 0')\nparser.add_argument('--image_scale', type=int, default=2, help='image scale per unet encoder/decoder step')\nparser.add_argument('--is_batchnorm', type=bool, default=True, help='use batchnorm instead of instancenorm')\n\n# irrelevants", "\n# irrelevants\nparser.add_argument('--val_bs', type=int, default=1, help='batch size at val time')\nparser.add_argument('--val_step', type=int, default=200, help='do validation per val_step')\nparser.add_argument('--draw_step', type=int, default=50, help='add train graphic result per draw_step')\nparser.add_argument('--verbose', action='store_true', help='whether to display the loss information per iter')\nargs = parser.parse_args()\nargs.mixup = args.pseudo = args.sn = args.pc = True\nparameter = Parser(args).get_param()\n", "parameter = Parser(args).get_param()\n\n\ndef train(model, restore=False):\n    list_trained_iterations = os.listdir(parameter.path.path_to_model)\n    base_lr = parameter.exp.base_lr\n    best_performance = 0.0\n    iter_num = 0\n    loss = {}\n    \n    if len(list_trained_iterations) == 0:\n        restore = False\n        \n    if restore:\n        list_trained_iterations.remove(f'{parameter.exp.exp_name}_best_model.pth')\n        max_trained_iterations = list(map(lambda x: int(x.split('_')[1].rstrip('.pth')), list_trained_iterations))\n        restore_itr = int(max_trained_iterations[-1])\n\n        model_name = f'iter_{restore_itr}'\n        for name in list_trained_iterations:\n            if name.startswith(model_name):\n                model_name = name\n                break\n        else:\n            print('valid model not found')\n            raise ValueError\n        save_model_path = os.path.join(parameter.path.path_to_model, model_name)\n        model.load_state_dict(torch.load(save_model_path, map_location='cpu'))\n        best_performance = max([float(f.split('_')[-1].rstrip('.pth')) for f in list_trained_iterations if 'd' in f])\n        print(\"init weight from {}, current best performance is {}\".format(save_model_path, best_performance))\n        base_lr = base_lr * (restore_itr / parameter.exp.max_iter)\n        iter_num = restore_itr\n        \n    batch_size = parameter.exp.batch_size\n    max_iterations = parameter.exp.max_iter\n\n    db_train = parameter.get_dataset(parameter, split='train')\n    db_val = parameter.get_dataset(parameter, split='val')\n    \n    labeled_idxs = db_train.labeled_idxs\n    unlabeled_idxs = db_train.unlabeled_idxs\n    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-parameter.exp.labeled_batch_size)\n    \n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train,\n                             num_workers=4,\n                             pin_memory=True,\n                             batch_sampler=batch_sampler,\n                             worker_init_fn=worker_init_fn)\n    \n    valloader = DataLoader(db_val, num_workers=args.val_bs, batch_size=args.val_bs)\n    if args.val_bs > 1:\n        print(f\"setting a validation batch size={args.val_bs} > 1 may provide inaccurate results while saving some time\")\n\n    model.train()\n    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n    \n    ce_loss = CrossEntropyLoss()\n    nce_loss = losses.NegativeCrossEntropyLoss()\n    dice_loss_coarse = losses.DiceLoss(parameter.dataset.n_coarse)\n    dice_loss_fine = losses.DiceLoss(parameter.dataset.n_fine) \n\n    writer = SummaryWriter(os.path.join(parameter.path.path_to_snapshot, \"log\"))\n    logging.info(\"{} iterations per epoch\".format(parameter.dataset.total_num // len(trainloader)))\n\n    max_epoch = (max_iterations - iter_num) // (len(trainloader)) + 1\n    iterator = tqdm(range(max_epoch), ncols=100, position=0, leave=True, desc='Training Progress')\n    torch.autograd.set_detect_anomaly(True)\n    \n    for _ in iterator:\n        for _, sampled_batch in enumerate(trainloader):\n            q_im, q_lc, q_lf = sampled_batch['image'], sampled_batch['coarse'], sampled_batch['fine']\n\n            if args.gpu >= 0:\n                q_im, q_lc, q_lf = q_im.cuda(args.gpu), q_lc.cuda(args.gpu), q_lf.cuda(args.gpu)\n            else:\n                raise RuntimeError(f'Specify a positive gpu id')\n\n            out = model(q_im)\n            out_coarse, out_fine = out['coarse_logit'], out['fine_logit']\n            \n            soft_coarse = torch.softmax(out_coarse, dim=1)\n            soft_fine = torch.softmax(out_fine, dim=1)\n            \n            pred_coarse = torch.argmax(soft_coarse, dim=1)\n            pred_fine = torch.argmax(soft_fine, dim=1)\n            \n            loss_ce1 = ce_loss(out_coarse, q_lc)\n            loss_dice1 = dice_loss_coarse(soft_coarse, q_lc)\n            loss_coarse = 0.5 * (loss_ce1 + loss_dice1)\n            loss_ce2 = ce_loss(out_fine[:parameter.exp.labeled_batch_size], q_lf[:parameter.exp.labeled_batch_size])\n            loss_dice2 = dice_loss_fine(soft_fine[:parameter.exp.labeled_batch_size], q_lf[:parameter.exp.labeled_batch_size])\n            loss_fine = 0.5 * (loss_ce2 + loss_dice2)\n            \n            loss['supervise loss coarse'] = loss_coarse\n            loss['supervise loss fine'] = loss_fine\n            \n            if parameter.exp.mixup_label:\n                \n                mixed_im, mixed_lf, alpha = sampled_batch['mixed'], sampled_batch['fine'], sampled_batch['alpha']\n                if args.gpu > 0:\n                    mixed_im, mixed_lf, alpha = mixed_im.cuda(args.gpu), mixed_lf.cuda(args.gpu), alpha.cuda(args.gpu)\n                else:\n                    raise RuntimeError(f'Specify a positive gpu id')\n\n                mixed_pred, pseudo_lf = model.gen_mixup_labels(\n                    q_im=q_im[parameter.exp.labeled_batch_size:],\n                    q_lc=q_lc[parameter.exp.labeled_batch_size:],\n                    q_soft=soft_fine[parameter.exp.labeled_batch_size:],\n                    mixed_im=mixed_im[parameter.exp.labeled_batch_size:],\n                    mixed_lf=mixed_lf[parameter.exp.labeled_batch_size:],\n                    alpha=alpha[parameter.exp.labeled_batch_size:],\n                    threshold=max(0.999 ** (iter_num // 10), 0.4),\n                    with_pseudo_label=parameter.exp.pseudo_label\n                )\n                \n                soft_mixed_pred = torch.softmax(mixed_pred, dim=1)\n                loss_ce3 = ce_loss(mixed_pred, pseudo_lf)\n                loss_dice3 = dice_loss_fine(soft_mixed_pred, pseudo_lf, mask=pseudo_lf)\n                loss3 = 0.5 * (loss_dice3 + loss_ce3) / (1 + math.exp(-iter_num // 1000))\n                loss['sematic mixup loss'] = loss3\n                \n            elif parameter.exp.pseudo_label:\n                \n                pseudo_lf = model.gen_pseudo_labels(\n                    q_im=q_im[parameter.exp.labeled_batch_size:],\n                    q_soft=soft_fine[parameter.exp.labeled_batch_size:],\n                    q_lc=q_lc[parameter.exp.labeled_batch_size:],\n                    threshold=max(0.999 ** (iter_num // 10), 0.4)\n                )\n                \n                loss_ce3 = ce_loss(out_fine[parameter.exp.labeled_batch_size:], pseudo_lf)\n                loss_dice3 = dice_loss_fine(\n                    soft_fine[parameter.exp.labeled_batch_size:],\n                    pseudo_lf, mask=pseudo_lf\n                )\n                loss3 = 0.5 * (loss_dice3 + loss_ce3) / (1 + math.exp(-iter_num // 1000))\n                loss['pseudo label loss'] = loss3\n            \n            make_curve(writer, pred_fine, q_lf, 'train', parameter.dataset.n_fine, iter_num)\n            make_curve(writer, pred_coarse, q_lc, 'train', parameter.dataset.n_coarse, iter_num)\n\n            # loss4 = nce_loss(out_fine[param.exp.labeled_batch_size:], q_lc[param.exp.labeled_batch_size:])\n            # loss['negative learning loss'] = loss4\n\n            loss_sum = sum(loss.values())          \n            optimizer.zero_grad()\n            loss_sum.backward()\n            optimizer.step()\n\n            lr_ = base_lr * (1.0 - iter_num / max_iterations)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar(f'{parameter.exp.exp_name}/lr', lr_, iter_num)\n            writer.add_scalar('loss/total_loss', loss_sum, iter_num)\n            writer.add_scalars('loss/individual_losses', loss, iter_num)\n\n            if args.verbose:\n                loss_names = list(loss.keys())\n                loss_values = list(map(lambda x: str(round(x.item(), 3)), loss.values()))\n                loss_log = ['*'] * (2 * len(loss_names))\n                loss_log[::2] = loss_names\n                loss_log[1::2] = loss_values\n                loss_log = '; '.join(loss_log)\n                logging.info(f\"model {parameter.exp.exp_name} iteration {iter_num} : total loss: {loss_sum.item():.3f},\\n\" + loss_log)\n\n            if iter_num % args.draw_step == 0:\n                make_image(writer, parameter, q_im, 'image/input_image', iter_num, normalize=True)\n                make_image(writer, parameter, q_lc, 'image/coarse_gt', iter_num, parameter.dataset.n_coarse - 1)\n                make_image(writer, parameter, q_lf, 'image/fine_gt', iter_num, parameter.dataset.n_fine - 1)\n                make_image(writer, parameter, pred_coarse, 'image/coarse_pred', iter_num, parameter.dataset.n_coarse - 1)\n                make_image(writer, parameter, pred_fine, 'image/fine_pred', iter_num, parameter.dataset.n_fine - 1)\n                \n                if parameter.exp.mixup_label or parameter.exp.pseudo_label:\n                    make_image(writer, parameter, mixed_im, 'pseudo_label/mixup_image', iter_num, normalize=True)\n                    make_image(writer, parameter, mixed_lf, 'pseudo_label/mixup_fine_gt', iter_num, parameter.dataset.n_fine - 1)\n\n            if iter_num > 0 and iter_num % args.val_step == 0:\n                model.eval()\n                avg_metric_f = np.zeros((len(valloader), parameter.dataset.n_fine, 4))\n                for case_index, sampled_batch in enumerate(tqdm(valloader, position=1, leave=True, desc='Validation Progress')):\n                    _, batch_metric_f, _ = test_single_case(model, parameter, sampled_batch, stride_xy=round(parameter.exp.patch_size[0] * 0.7), stride_z=64, gpu_id=args.gpu)\n                    avg_metric_f[case_index] = batch_metric_f\n                \n                if avg_metric_f[:, -1, parameter.exp.eval_metric].mean() > best_performance:\n                    best_performance = avg_metric_f[:, -1, parameter.exp.eval_metric].mean()\n                    save_model_path = os.path.join(parameter.path.path_to_model, 'iter_{}_dice_{}.pth'.format(iter_num, round(best_performance, 4)))\n                    save_best = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n                    torch.save(model.state_dict(), save_model_path)\n                    torch.save(model.state_dict(), save_best)\n                \n                for index, name in enumerate(['dsc', 'hd95', 'precision', 'recall']):\n                    writer.add_scalars(f'val/{name}', {f'fine label={i}': avg_metric_f[:, i-1, index].mean() for i in range(1, parameter.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/{name}', {f'fine avg': avg_metric_f[:, -1, index].mean()}, iter_num)\n\n                logging.info(f'iteration {iter_num} : dice_score : {avg_metric_f[:, -1, 0].mean():.5f} hd95 : {avg_metric_f[:, -1, 1].mean():.5f}')\n                model.train()\n\n            if iter_num % 5000 == 0:\n                save_model_path = os.path.join(parameter.path.path_to_model, 'iter_' + str(iter_num) + '.pth')\n                torch.save(model.state_dict(), save_model_path)\n                logging.info(\"save model to {}\".format(save_model_path))\n\n            if iter_num >= max_iterations:\n                break\n        if iter_num >= max_iterations:\n            iterator.close()\n            break\n    writer.close()\n    return \"Training Finished!\"", "\n\ndef test(model):\n    \n    save_model_path = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n    model.load_state_dict(torch.load(save_model_path))\n    print(\"init weight from {}\".format(save_model_path))\n    \n    db_test = Refuge2020(parameter, split='test')\n    testloader = DataLoader(db_test, num_workers=1, batch_size=1)\n    \n    model.eval()\n    avg_metric_c, avg_metric_f =\\\n        test_all_case(model, parameter, testloader, stride_xy=64, stride_z=64, gpu_id=args.gpu)\n    \n    print(avg_metric_c)\n    print(avg_metric_f)", "\n\nif __name__ == \"__main__\":\n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n\n    logging.basicConfig(\n        filename=os.path.join(parameter.path.path_to_snapshot, \"log.txt\"),\n        level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s',\n        datefmt='%H:%M:%S'\n    )\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.info(msg=parameter)\n    \n    # model = unet_3D(in_channels=4).cuda(args.gpu)\n    model = UNetBasedNetwork(parameter).cuda(args.gpu)\n    \n    train(model, restore=parameter.exp.restore)\n    test(model)\n    print(f'train-test over for {parameter.exp.exp_name}')"]}
{"filename": "test.py", "chunked_list": ["import os\nimport sys\nimport math\nimport json\nimport torch\nimport shutil\nimport logging\nimport argparse\n\nimport numpy as np", "\nimport numpy as np\nfrom tqdm import tqdm\nfrom medpy import metric\nimport torch.nn.functional as F\nfrom collections import namedtuple\nfrom torch.utils.data import DataLoader\n\nfrom networks.unet import UNet\nfrom val import test_single_case", "from networks.unet import UNet\nfrom val import test_single_case\nfrom utils.visualize import visualize\nfrom dataloaders.base_dataset import BaseDataset\n\neval_metrics = [metric.binary.dc, metric.binary.hd95, metric.binary.precision, metric.binary.recall]\nn_eval = len(eval_metrics)\nparam = None\n\n\ndef test_all_case(net, param, testloader, gpu_id, stride_xy=64, stride_z=64, draw_ddm_im=False):\n    \n    logging.info(param)\n    \n    if os.path.exists(os.path.join(param.path.path_to_test, \"test_log.txt\")):\n        os.remove(os.path.join(param.path.path_to_test, \"test_log.txt\"))\n\n    total_metric_c = np.zeros((param.dataset.n_coarse - 1, n_eval))\n    total_metric_f = np.zeros((param.dataset.n_fine, n_eval))\n    all_total_metric_f = np.zeros((len(testloader), param.dataset.n_fine - 1, n_eval))\n    n_images = len(testloader)\n\n    tsne_index = 0 if draw_ddm_im else -1\n    print(\"Testing begin\")\n    with open(os.path.join(param.path.path_to_dataset, 'test.list'), 'r') as fp:\n        image_ids = fp.readlines()\n    \n    logging.info('test metrics:\\t' + '\\t'.join([method.__name__ for method in eval_metrics]) + '\\n')\n    \n    for case_index, sampled_batch in enumerate(tqdm(testloader)):\n        \n        ids = image_ids[case_index].strip()\n        metric_c, metric_f, feat_map = test_single_case(\n            net, param, sampled_batch, stride_xy, stride_z, gpu_id=gpu_id, save_pred=False, ids=ids\n        )\n        \n        if case_index == tsne_index:\n            if not visualize(\n                feat_map, sampled_batch['fine'][0], 0, 'tsne', param,\n                os.path.join(param.path.path_to_test, f'tsne_{param.exp.exp_name}.eps'),\n                legend=param.dataset.legend, n_components=2\n            ):\n                tsne_index += 1\n        \n        for c in range(param.dataset.n_coarse - 1):\n            total_metric_c[c] += metric_c[c]\n            logging.debug(f'{ids}\\t' + '\\t'.join([f\"{metric_c[c, k]:.3f}\" for k in range(n_eval)]))\n            \n        for f in range(param.dataset.n_fine - 1):\n            total_metric_f[f] += metric_f[f]\n            all_total_metric_f[case_index] = metric_f[:-1]\n            logging.debug(f'{ids}\\t' + '\\t'.join([f\"{metric_f[f, k]:.3f}\" for k in range(n_eval)]))\n        logging.debug(f'avg fine for {ids}\\t' + '\\t'.join([f\"{metric_f[-1, k]:.3f}\" for k in range(n_eval)]))\n\n    for i in range(1, param.dataset.n_coarse):\n        log = f'mean of superclass {i}:\\t' + '\\t'.join([f\"{_:.3f}\" for _ in (total_metric_c[i-1] / n_images)])\n        logging.info(log)\n    \n    for i in range(1, param.dataset.n_fine):\n        log = f'mean of subclass {i}:\\t' + '\\t'.join([f\"{_:.3f}\" for _ in total_metric_f[i-1] / n_images])\n        logging.info(log)\n        log = f'std of subclass {i}:\\t' + '\\t'.join([f\"{_:.3f}\" for _ in np.std(all_total_metric_f[:, i-1], axis=0)])\n        logging.info(log)\n    \n    mean_f = [total_metric_f[:, i].sum() / (param.dataset.n_fine - 1) for i in range(len(eval_metrics))]\n    logging.info(f'mean of subclasses:\\t' + '\\t'.join([f'{i / n_images:.3f}' for i in mean_f]))\n    # logging.info(f'std of all subclasses: {np.std(all_total_metric_f[:, i-1]):.5f}')\n\n    total_metric_f[-1] = mean_f\n    return total_metric_c / n_images, total_metric_f / n_images", "\n\ndef test_all_case(net, param, testloader, gpu_id, stride_xy=64, stride_z=64, draw_ddm_im=False):\n    \n    logging.info(param)\n    \n    if os.path.exists(os.path.join(param.path.path_to_test, \"test_log.txt\")):\n        os.remove(os.path.join(param.path.path_to_test, \"test_log.txt\"))\n\n    total_metric_c = np.zeros((param.dataset.n_coarse - 1, n_eval))\n    total_metric_f = np.zeros((param.dataset.n_fine, n_eval))\n    all_total_metric_f = np.zeros((len(testloader), param.dataset.n_fine - 1, n_eval))\n    n_images = len(testloader)\n\n    tsne_index = 0 if draw_ddm_im else -1\n    print(\"Testing begin\")\n    with open(os.path.join(param.path.path_to_dataset, 'test.list'), 'r') as fp:\n        image_ids = fp.readlines()\n    \n    logging.info('test metrics:\\t' + '\\t'.join([method.__name__ for method in eval_metrics]) + '\\n')\n    \n    for case_index, sampled_batch in enumerate(tqdm(testloader)):\n        \n        ids = image_ids[case_index].strip()\n        metric_c, metric_f, feat_map = test_single_case(\n            net, param, sampled_batch, stride_xy, stride_z, gpu_id=gpu_id, save_pred=False, ids=ids\n        )\n        \n        if case_index == tsne_index:\n            if not visualize(\n                feat_map, sampled_batch['fine'][0], 0, 'tsne', param,\n                os.path.join(param.path.path_to_test, f'tsne_{param.exp.exp_name}.eps'),\n                legend=param.dataset.legend, n_components=2\n            ):\n                tsne_index += 1\n        \n        for c in range(param.dataset.n_coarse - 1):\n            total_metric_c[c] += metric_c[c]\n            logging.debug(f'{ids}\\t' + '\\t'.join([f\"{metric_c[c, k]:.3f}\" for k in range(n_eval)]))\n            \n        for f in range(param.dataset.n_fine - 1):\n            total_metric_f[f] += metric_f[f]\n            all_total_metric_f[case_index] = metric_f[:-1]\n            logging.debug(f'{ids}\\t' + '\\t'.join([f\"{metric_f[f, k]:.3f}\" for k in range(n_eval)]))\n        logging.debug(f'avg fine for {ids}\\t' + '\\t'.join([f\"{metric_f[-1, k]:.3f}\" for k in range(n_eval)]))\n\n    for i in range(1, param.dataset.n_coarse):\n        log = f'mean of superclass {i}:\\t' + '\\t'.join([f\"{_:.3f}\" for _ in (total_metric_c[i-1] / n_images)])\n        logging.info(log)\n    \n    for i in range(1, param.dataset.n_fine):\n        log = f'mean of subclass {i}:\\t' + '\\t'.join([f\"{_:.3f}\" for _ in total_metric_f[i-1] / n_images])\n        logging.info(log)\n        log = f'std of subclass {i}:\\t' + '\\t'.join([f\"{_:.3f}\" for _ in np.std(all_total_metric_f[:, i-1], axis=0)])\n        logging.info(log)\n    \n    mean_f = [total_metric_f[:, i].sum() / (param.dataset.n_fine - 1) for i in range(len(eval_metrics))]\n    logging.info(f'mean of subclasses:\\t' + '\\t'.join([f'{i / n_images:.3f}' for i in mean_f]))\n    # logging.info(f'std of all subclasses: {np.std(all_total_metric_f[:, i-1]):.5f}')\n\n    total_metric_f[-1] = mean_f\n    return total_metric_c / n_images, total_metric_f / n_images", "\n\ndef calculate_metric_percase(pred, gt):\n    ret = np.zeros((len(eval_metrics),))\n    if pred.sum() > 0 and gt.sum() > 0:\n        for i, met in enumerate(eval_metrics):\n            ret[i] = met(pred, gt)\n\n    return ret\n", "\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-m', '--model', type=str, choices=['branched', 'unet'], default='unet', help='network type for selected trained model')\n    parser.add_argument('-p', '--path', type=str, default='/nas/dailinrui/SSL4MIS/model_final/prostate/unet24', help='root dir of trained folder')\n    parser.add_argument('-g', '--gpu', type=int, default=5, help='gpu on which to test model')\n    args = parser.parse_args()\n    \n    with open(os.path.join(args.path, 'param.json'), 'r') as fp:\n        d = json.load(fp)\n    \n    d1 = d['dataset']\n    d2 = d['exp']\n    d3 = d['path']\n    d4 = d['network']\n    P = namedtuple('P', ['dataset', 'exp', 'path', 'network'])\n    param = P(dataset=namedtuple('dataset', d1.keys())(*d1.values()),\n              exp=namedtuple('exp', d2.keys())(*d2.values()),\n              path=namedtuple('path', d3.keys())(*d3.values()),\n              network=namedtuple('network', d4.keys())(*d4.values()))\n\n    logging.basicConfig(\n        level=logging.INFO, format='%(asctime)s [%(levelname)-5s] %(message)s',\n        datefmt='%H:%M:%S',\n        handlers=[logging.FileHandler(os.path.join(param.path.path_to_test, \"test_log.txt\"), mode='w'),\n                  logging.StreamHandler(sys.stdout)]\n    )\n    \n    num_classes = (param.dataset.n_coarse, param.dataset.n_fine)\n    test_save_path = param.path.path_to_test\n    \n    if args.model == 'branched':\n        if param.dataset.n_coarse > 2:\n            from networks.multiplebranchedunet import UNetMultiBranchNetwork\n            net = UNetMultiBranchNetwork(param).cuda(args.gpu)\n        elif param.dataset.n_coarse == 2:\n            from networks.singlybranchedunet import UNetSingleBranchNetwork\n            net = UNetSingleBranchNetwork(param).cuda(args.gpu)\n    elif args.model == 'unet':\n        net = UNet(param).cuda(args.gpu)\n    \n    save_mode_path = os.path.join(param.path.path_to_model, '{}_best_model.pth'.format(param.exp.exp_name))\n    state_dicts = torch.load(save_mode_path, map_location='cpu')\n    net.load_state_dict(state_dicts['model_state_dict'])\n    print(\"init weight from {}\".format(save_mode_path))\n    net.eval()\n    \n    db_test = BaseDataset(param, split='test')\n    testloader = DataLoader(db_test, num_workers=1, batch_size=1)\n    \n    test_all_case(net, param, testloader, stride_xy=64, stride_z=64, gpu_id=args.gpu)", ""]}
{"filename": "dataloaders/preprocessing_refuge.py", "chunked_list": ["import os\nimport imageio\nimport h5py\nimport numpy as np\nimport shutil\nfrom functools import wraps\nfrom collections.abc import Iterable\nfrom os.path import *\nfrom tqdm import tqdm\n", "from tqdm import tqdm\n\nimport imgaug as ia\n\nraw_dataset_path = \"/nas/dailinrui/dataset/refuge2020\"\nmod_dataset_path = \"/data/dailinrui/dataset/refuge2020\"\n\n\nfilename_process = lambda fname: fname.split('_')[0] + '.h5'\nfilelist_process = lambda fname: fname.split('_')[0] + '\\n'", "filename_process = lambda fname: fname.split('_')[0] + '.h5'\nfilelist_process = lambda fname: fname.split('_')[0] + '\\n'\nmask_colors = [0, 255, 510]\n\n\ndef string_modify(str_func):\n    def string_mod(func):\n        @wraps(func)\n        def wrapper_string_mod(*args, **kwargs):\n            string_or_string_list = func(*args, **kwargs)\n            if isinstance(string_or_string_list, str):\n                return str_func(string_or_string_list)\n            elif isinstance(string_or_string_list, Iterable) and isinstance(string_or_string_list[0], str):\n                return list(str_func(_) for _ in string_or_string_list)\n        return wrapper_string_mod\n    return string_mod", "\n\n@string_modify(str_func=filelist_process)\ndef mod_listdir(dirname):\n    return [imname for imname in os.listdir(dirname) if not imname.startswith('.')]\n\n\ndef maybe_mkdir(dirname, filename=None):\n    os.makedirs(dirname, exist_ok=True)\n    return join(dirname, filename)", "\n\nwith open(maybe_mkdir(mod_dataset_path, 'train.list'), 'w') as fp:\n    fp.writelines(mod_listdir(join(raw_dataset_path, 'train', 'images')))\nwith open(maybe_mkdir(mod_dataset_path, 'val.list'), 'w') as fp:\n    fp.writelines(mod_listdir(join(raw_dataset_path, 'valid', 'images')))\nwith open(maybe_mkdir(mod_dataset_path, 'test.list'), 'w') as fp:\n    fp.writelines(mod_listdir(join(raw_dataset_path, 'test', 'images')))\n\nfor splits in ['train', 'valid', 'test']:\n    for imname in tqdm(os.listdir(join(raw_dataset_path, splits, 'images'))):\n        im = join(raw_dataset_path, splits, 'images', imname)\n        mask = im.replace('images', 'masks')\n        if imname.startswith('.'):\n            os.remove(im)\n            continue\n        \n        im_ = imageio.imread(im).transpose(2, 0, 1) / 255\n        mask_ = np.sum(imageio.imread(mask), axis=2) / 255\n        \n        h5 = h5py.File(maybe_mkdir(join(mod_dataset_path, 'data'), filename_process(imname)), 'w')\n        h5.create_dataset('image', im_.shape, np.float32, im_, compression='gzip')\n        h5.create_dataset('label', mask_.shape, np.uint8, mask_, compression='gzip')", "\nfor splits in ['train', 'valid', 'test']:\n    for imname in tqdm(os.listdir(join(raw_dataset_path, splits, 'images'))):\n        im = join(raw_dataset_path, splits, 'images', imname)\n        mask = im.replace('images', 'masks')\n        if imname.startswith('.'):\n            os.remove(im)\n            continue\n        \n        im_ = imageio.imread(im).transpose(2, 0, 1) / 255\n        mask_ = np.sum(imageio.imread(mask), axis=2) / 255\n        \n        h5 = h5py.File(maybe_mkdir(join(mod_dataset_path, 'data'), filename_process(imname)), 'w')\n        h5.create_dataset('image', im_.shape, np.float32, im_, compression='gzip')\n        h5.create_dataset('label', mask_.shape, np.uint8, mask_, compression='gzip')", ""]}
{"filename": "dataloaders/brats2021.py", "chunked_list": ["import h5py\nimport torch\nimport random\nimport numpy as np\nfrom PIL import Image\nimport imgaug.augmenters as iaa\nfrom torchvision import transforms\nfrom dataloaders.base_dataset import BaseDataset\nfrom imgaug.augmentables.segmaps import SegmentationMapsOnImage\n", "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n\n\nclass BraTS2021(BaseDataset):\n    def __init__(self, param, split='train'):\n        super(BraTS2021, self).__init__(param, split)\n        \n    def __getitem__(self, idx):\n        # sample is randomly cropped and \"mixup-ed\" in `BaseDataset`\n        sample = super().__getitem__(idx)\n            \n        return sample"]}
{"filename": "dataloaders/utils.py", "chunked_list": ["import h5py\nimport torch\nimport random\nimport itertools\nimport numpy as np\nfrom torch.nn import init\nfrom torch.utils.data import Dataset\nfrom skimage.transform import resize\nfrom torch.utils.data.sampler import Sampler\n    ", "from torch.utils.data.sampler import Sampler\n    \n    \nclass RandomRotFlip(object):\n\n    def __call__(self, sample):\n        image, coarse_label, fine_label = sample['image'], sample['lbl:coarse'], sample['lbl:fine']\n        ndim = coarse_label.ndim\n        assert 1 < ndim < 4\n        \n        k = np.random.randint(0, 4)\n        axis = np.random.randint(1, ndim)\n        \n        # rot\n        image = np.asarray([np.rot90(image[i, ...], k) for i in range(4)])\n        coarse_label = np.rot90(coarse_label, k)\n        fine_label = np.rot90(fine_label, k)\n        \n        # flip\n        image = np.flip(image, axis=axis).copy()\n        coarse_label = np.flip(coarse_label, axis=axis-1).copy()\n        fine_label = np.flip(fine_label, axis=axis-1).copy()\n\n        return {'image': image, 'lbl:coarse': coarse_label, 'lbl:fine': fine_label}", "    \n\nclass RandomNoise(object):\n    def __init__(self, mu=0, sigma=0.1):\n        self.mu = mu\n        self.sigma = sigma\n\n    def __call__(self, sample):\n        image, coarse_label, fine_label = sample['image'], sample['lbl:coarse'], sample['lbl:fine']\n        ndim = coarse_label.ndim\n        assert 1 < ndim < 4\n        \n        if ndim == 3:\n            noise = np.clip(\n                self.sigma * np.random.randn(image.shape[1], image.shape[2], image.shape[3]),\n                -2*self.sigma,\n                2*self.sigma\n            )\n        elif ndim == 2:\n            noise = np.clip(\n                self.sigma * np.random.randn(image.shape[1], image.shape[2]),\n                -2*self.sigma,\n                2*self.sigma\n            )\n        noise = noise + self.mu\n        image = image + noise\n        \n        return {'image': image, 'lbl:coarse': coarse_label, 'lbl:fine': fine_label}", "\n\nclass TwoStreamBatchSampler(Sampler):\n\n    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n        self.primary_indices = primary_indices\n        self.secondary_indices = secondary_indices\n        self.secondary_batch_size = secondary_batch_size\n        self.primary_batch_size = batch_size - secondary_batch_size\n\n        assert len(self.primary_indices) >= self.primary_batch_size > 0,\\\n            f\"condition {len(self.primary_indices)} >= {self.primary_batch_size} > 0 is not satisfied\"\n        assert len(self.secondary_indices) >= self.secondary_batch_size >= 0,\\\n            f\"condition {len(self.secondary_indices)} >= {self.secondary_batch_size} >= 0 is not satisfied\"\n\n    def __iter__(self):\n        primary_iter = iterate_once(self.primary_indices)\n        if self.secondary_batch_size != 0:\n            secondary_iter = iterate_eternally(self.secondary_indices)\n            return (\n                primary_batch + secondary_batch\n                for (primary_batch, secondary_batch)\n                in zip(grouper(primary_iter, self.primary_batch_size),\n                    grouper(secondary_iter, self.secondary_batch_size))\n            )\n        else:\n            return (primary_batch for primary_batch in grouper(primary_iter, self.primary_batch_size))\n\n    def __len__(self):\n        return len(self.primary_indices) // self.primary_batch_size", "\n\ndef iterate_once(iterable):\n    return np.random.permutation(iterable)\n\n\ndef iterate_eternally(indices):\n    def infinite_shuffles():\n        while True:\n            yield np.random.permutation(indices)\n    return itertools.chain.from_iterable(infinite_shuffles())", "\n\ndef grouper(iterable, n):\n    \"Collect data into fixed-length chunks or blocks\"\n    # grouper('ABCDEFG', 3) --> ABC DEF\"\n    args = [iter(iterable)] * n\n    return zip(*args)\n\n", ""]}
{"filename": "dataloaders/base_dataset.py", "chunked_list": ["import h5py\nimport torch\nimport random\nimport numpy as np\nimport imgaug as ia\nfrom torch.utils.data import Dataset\nfrom skimage.transform import resize\nfrom collections.abc import Iterable\n\n\nclass BaseDataset(Dataset):\n    \n    def __init__(self, param, split='train'):\n        \n        self.split = split\n        self.labeled_idxs = []\n        self.unlabeled_idxs = []\n        self.mixup = param.exp.mixup_label\n        self.num = param.dataset.total_num\n        self.patch_size = param.exp.patch_size\n        self.base_dir = param.path.path_to_dataset\n        self.n_labeled_idx = param.exp.labeled_num\n        self.whether_use_3to2d = param.dataset.n_dim == 2.5\n        self.dataset_name = param.__class__.__name__.replace('Parser', '')\n\n        with open(self.base_dir + f'/{split}.list') as f:\n            self.image_list = f.readlines()\n        self.mapping = param.dataset.mapping\n\n        self.image_list = [item.replace('\\n', '').split(\",\")[0] for item in self.image_list][:self.num]\n        print(f\"{self.split}: total {len(self.image_list)} samples\")\n        \n        self.rn_crop = RandomCrop(self.patch_size)\n        self.tensorize = ToTensor()\n        \n        if self.split == 'train': self._find_or_gen_unlabeled_samples()\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, idx):\n        image_name = self.image_list[idx]\n        if self.whether_use_3to2d and self.split == 'train':\n            h5f = h5py.File(self.base_dir + \"/data/slices/{}.h5\".format(image_name), \"r\")\n        else:\n            h5f = h5py.File(self.base_dir + \"/data/{}.h5\".format(image_name), 'r')\n        \n        image = h5f['image'][:]\n        granularity = h5f['granularity'][:][0]\n        if granularity == 0:\n            label_c = h5f['label'][:]\n            label_f = np.full(label_c.shape, fill_value=255, dtype=np.uint8)\n        elif granularity == 1:\n            label_f = h5f['label'][:]\n            label_c = np.zeros(label_f.shape, dtype=np.uint8)\n            for key, value in self.mapping.items():\n                if isinstance(value, list):\n                    for v in value:\n                        label_c[label_f == int(v)] = int(key)\n                else:\n                    print(f\"expect list fine label index(s), got {value}\")\n        else:\n            print(f\"graularity {granularity} is not supported\")\n        \n        ndim = label_c.ndim\n        assert 1 < ndim < 4\n        \n        if image.ndim != ndim + 1:\n            image = image[np.newaxis, ...]\n\n        sample = {'image': image,\n            'coarse': label_c.astype(np.uint8),\n            'fine': label_f.astype(np.uint8),\n        }\n        \n        if self.split == 'train' and self.mixup:\n            if idx not in self.labeled_idxs:\n                if ndim == 3: mixed_im, mixed_lf, alpha = self._mixup_ndarray_3d(sample)\n                else: mixed_im, mixed_lf, alpha = self._mixup_ndarray_2d(sample)\n                \n                sample = self.rn_crop(sample)\n                mixed_sample = {'image': mixed_im, 'coarse': label_c.astype(np.uint8), 'fine': mixed_lf}\n                mixed_sample = self.rn_crop(mixed_sample, keeplast=True)\n                sample['fine'] = mixed_sample['fine']\n                sample['mixed'] = mixed_sample['image']\n                sample['alpha'] = alpha\n            else:\n                sample = self.rn_crop(sample)\n                sample['mixed'] = sample['image']\n                sample['alpha'] = 0\n        \n        elif self.split == 'train':\n            sample = self.rn_crop(sample)\n            sample['mixed'] = sample['image']\n            sample['alpha'] = 0\n        else:\n            sample = sample\n        sample = self.tensorize(sample)\n        \n        return sample\n    \n    def _get_bbox_ndarray_2d(self, label_map):\n        if np.sum(label_map) == 0:\n            return None\n\n        h = np.any(label_map, axis=1)\n        w = np.any(label_map, axis=0)\n\n        hmin, hmax = np.where(h)[0][[0, -1]]\n        wmin, wmax = np.where(w)[0][[0, -1]]\n\n        return (hmin, hmax+1), (wmin, wmax+1)\n    \n    def _get_bbox_ndarray_3d(self, label_map):\n        if np.sum(label_map) == 0:\n            return None\n\n        h = np.any(label_map, axis=(1, 2))\n        w = np.any(label_map, axis=(0, 2))\n        d = np.any(label_map, axis=(0, 1))\n\n        hmin, hmax = np.where(h)[0][[0, -1]]\n        wmin, wmax = np.where(w)[0][[0, -1]]\n        dmin, dmax = np.where(d)[0][[0, -1]]\n\n        return (hmin, hmax+1), (wmin, wmax+1), (dmin, dmax+1)\n    \n    def _mixup_ndarray_2d(self, unlabeled_sample):\n        labeled_idx = random.choice(self.labeled_idxs)\n        q_im, q_lc = unlabeled_sample['image'][:], unlabeled_sample['coarse'][:]\n        if self.whether_use_3to2d:\n            labeled_h5 = h5py.File(self.base_dir + \"/data/slices/{}.h5\".format(self.image_list[labeled_idx]), \"r\")\n        else:\n            labeled_h5 = h5py.File(self.base_dir + \"/data/{}.h5\".format(self.image_list[labeled_idx]), 'r')\n        im = labeled_h5['image'][:]\n        lf = labeled_h5['label'][:]\n        if im.ndim != lf.ndim + 1:\n            im = im[np.newaxis, ...]\n        assert labeled_h5['granularity'][:][0] == 1, 'use a sublabeled sample to generate mixup label'\n        \n        alpha = random.randint(5, 10) / 10\n        mixed_im = q_im.copy()\n        mixed_lf = np.zeros(q_lc.shape, dtype=np.uint8)\n        bbox1 = self._get_bbox_ndarray_2d(lf)\n        bbox2 = self._get_bbox_ndarray_2d(q_lc)\n        \n        if bbox1 is None or bbox2 is None:\n            return mixed_im, mixed_lf, 0\n        \n        cropped_im1 = im[:, slice(*bbox1[0]), slice(*bbox1[1])]\n        cropped_im2 = q_im[:, slice(*bbox2[0]), slice(*bbox2[1])]\n        cropped_lf1 = lf[slice(*bbox1[0]), slice(*bbox1[1])]\n        sz_bbox2 = tuple(x[1] - x[0] for x in bbox2)\n        rsz_im1 = np.concatenate([resize(cropped_im1[channel], output_shape=sz_bbox2, order=1)[np.newaxis] for channel in range(q_im.shape[0])], axis=0)\n        rsz_lf1 = resize(cropped_lf1, output_shape=sz_bbox2, order=0)\n        \n        rsz_lf1 *= (q_lc[slice(*bbox2[0]), slice(*bbox2[1])] > 0).astype(np.uint8)\n        alph = 1 - alpha * (rsz_lf1 > 0).astype(np.uint8)\n        mixed_im2 = alph * cropped_im2 + (1 - alph) * rsz_im1\n            \n        mixed_im[:, slice(*bbox2[0]), slice(*bbox2[1])] = mixed_im2\n        mixed_lf[slice(*bbox2[0]), slice(*bbox2[1])] = rsz_lf1\n        \n        return mixed_im, mixed_lf, alpha\n    \n    def _mixup_ndarray_3d(self, unlabeled_sample):\n        labeled_idx = random.choice(self.labeled_idxs)\n        q_im, q_lc = unlabeled_sample['image'][:], unlabeled_sample['coarse'][:]\n        labeled_h5 = h5py.File(self.base_dir + \"/data/{}.h5\".format(self.image_list[labeled_idx]), 'r')\n        im = labeled_h5['image'][:]\n        lf = labeled_h5['label'][:]\n        if im.ndim != lf.ndim + 1:\n            im = im[np.newaxis, ...]\n        assert labeled_h5['granularity'][:][0] == 1, 'use a sublabeled sample to generate mixup label'\n        \n        alpha = random.random()\n        mixed_im = q_im.copy()\n        mixed_lf = np.zeros(q_lc.shape, dtype=np.uint8)\n        bbox1 = self._get_bbox_ndarray_3d(lf)\n        bbox2 = self._get_bbox_ndarray_3d(q_lc)\n        \n        if bbox1 is None or bbox2 is None:\n            return mixed_im, mixed_lf, 0\n        \n        cropped_im1 = im[:, slice(*bbox1[0]), slice(*bbox1[1]), slice(*bbox1[2])]\n        cropped_im2 = q_im[:, slice(*bbox2[0]), slice(*bbox2[1]), slice(*bbox2[2])]\n        cropped_lf1 = lf[slice(*bbox1[0]), slice(*bbox1[1]), slice(*bbox1[2])]\n        sz_bbox2 = tuple(x[1] - x[0] for x in bbox2)\n        rsz_im1 = np.concatenate([resize(cropped_im1[channel], output_shape=sz_bbox2, order=3)[np.newaxis] for channel in range(q_im.shape[0])], axis=0)\n        rsz_lf1 = resize(cropped_lf1, output_shape=sz_bbox2, order=0)\n        \n        rsz_lf1 *= (q_lc[slice(*bbox2[0]), slice(*bbox2[1]), slice(*bbox2[2])] > 0).astype(np.uint8)\n        alph = 1 - alpha * (rsz_lf1 > 0).astype(np.uint8)\n        mixed_im2 = alph * cropped_im2 + (1 - alph) * rsz_im1\n            \n        mixed_im[:, slice(*bbox2[0]), slice(*bbox2[1]), slice(*bbox2[2])] = mixed_im2\n        mixed_lf[slice(*bbox2[0]), slice(*bbox2[1]), slice(*bbox2[2])] = rsz_lf1\n        \n        return mixed_im, mixed_lf, alpha\n    \n    def _find_or_gen_unlabeled_samples(self):\n        for idx, image_name in enumerate(self.image_list):\n            if self.whether_use_3to2d and self.split == 'train':\n                h5f = h5py.File(self.base_dir + \"/data/slices/{}.h5\".format(image_name), \"r\")\n            else:\n                h5f = h5py.File(self.base_dir + \"/data/{}.h5\".format(image_name), 'r')\n            if h5f['granularity'][:][0] == 1:\n                self.labeled_idxs.append(idx)\n            else:\n                self.unlabeled_idxs.append(idx)\n        \n        if len(self.labeled_idxs) > self.n_labeled_idx:\n            self.unlabeled_idxs.extend(self.labeled_idxs[self.n_labeled_idx:])\n            self.labeled_idxs = self.labeled_idxs[:self.n_labeled_idx]\n        elif len(self.labeled_idxs) < self.n_labeled_idx:\n            self.n_labeled_idx = len(self.labeled_idxs)\n            print(f\"there are only {len(self.labeled_idxs)} labeled samples, using all\")   ", "\n\nclass BaseDataset(Dataset):\n    \n    def __init__(self, param, split='train'):\n        \n        self.split = split\n        self.labeled_idxs = []\n        self.unlabeled_idxs = []\n        self.mixup = param.exp.mixup_label\n        self.num = param.dataset.total_num\n        self.patch_size = param.exp.patch_size\n        self.base_dir = param.path.path_to_dataset\n        self.n_labeled_idx = param.exp.labeled_num\n        self.whether_use_3to2d = param.dataset.n_dim == 2.5\n        self.dataset_name = param.__class__.__name__.replace('Parser', '')\n\n        with open(self.base_dir + f'/{split}.list') as f:\n            self.image_list = f.readlines()\n        self.mapping = param.dataset.mapping\n\n        self.image_list = [item.replace('\\n', '').split(\",\")[0] for item in self.image_list][:self.num]\n        print(f\"{self.split}: total {len(self.image_list)} samples\")\n        \n        self.rn_crop = RandomCrop(self.patch_size)\n        self.tensorize = ToTensor()\n        \n        if self.split == 'train': self._find_or_gen_unlabeled_samples()\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, idx):\n        image_name = self.image_list[idx]\n        if self.whether_use_3to2d and self.split == 'train':\n            h5f = h5py.File(self.base_dir + \"/data/slices/{}.h5\".format(image_name), \"r\")\n        else:\n            h5f = h5py.File(self.base_dir + \"/data/{}.h5\".format(image_name), 'r')\n        \n        image = h5f['image'][:]\n        granularity = h5f['granularity'][:][0]\n        if granularity == 0:\n            label_c = h5f['label'][:]\n            label_f = np.full(label_c.shape, fill_value=255, dtype=np.uint8)\n        elif granularity == 1:\n            label_f = h5f['label'][:]\n            label_c = np.zeros(label_f.shape, dtype=np.uint8)\n            for key, value in self.mapping.items():\n                if isinstance(value, list):\n                    for v in value:\n                        label_c[label_f == int(v)] = int(key)\n                else:\n                    print(f\"expect list fine label index(s), got {value}\")\n        else:\n            print(f\"graularity {granularity} is not supported\")\n        \n        ndim = label_c.ndim\n        assert 1 < ndim < 4\n        \n        if image.ndim != ndim + 1:\n            image = image[np.newaxis, ...]\n\n        sample = {'image': image,\n            'coarse': label_c.astype(np.uint8),\n            'fine': label_f.astype(np.uint8),\n        }\n        \n        if self.split == 'train' and self.mixup:\n            if idx not in self.labeled_idxs:\n                if ndim == 3: mixed_im, mixed_lf, alpha = self._mixup_ndarray_3d(sample)\n                else: mixed_im, mixed_lf, alpha = self._mixup_ndarray_2d(sample)\n                \n                sample = self.rn_crop(sample)\n                mixed_sample = {'image': mixed_im, 'coarse': label_c.astype(np.uint8), 'fine': mixed_lf}\n                mixed_sample = self.rn_crop(mixed_sample, keeplast=True)\n                sample['fine'] = mixed_sample['fine']\n                sample['mixed'] = mixed_sample['image']\n                sample['alpha'] = alpha\n            else:\n                sample = self.rn_crop(sample)\n                sample['mixed'] = sample['image']\n                sample['alpha'] = 0\n        \n        elif self.split == 'train':\n            sample = self.rn_crop(sample)\n            sample['mixed'] = sample['image']\n            sample['alpha'] = 0\n        else:\n            sample = sample\n        sample = self.tensorize(sample)\n        \n        return sample\n    \n    def _get_bbox_ndarray_2d(self, label_map):\n        if np.sum(label_map) == 0:\n            return None\n\n        h = np.any(label_map, axis=1)\n        w = np.any(label_map, axis=0)\n\n        hmin, hmax = np.where(h)[0][[0, -1]]\n        wmin, wmax = np.where(w)[0][[0, -1]]\n\n        return (hmin, hmax+1), (wmin, wmax+1)\n    \n    def _get_bbox_ndarray_3d(self, label_map):\n        if np.sum(label_map) == 0:\n            return None\n\n        h = np.any(label_map, axis=(1, 2))\n        w = np.any(label_map, axis=(0, 2))\n        d = np.any(label_map, axis=(0, 1))\n\n        hmin, hmax = np.where(h)[0][[0, -1]]\n        wmin, wmax = np.where(w)[0][[0, -1]]\n        dmin, dmax = np.where(d)[0][[0, -1]]\n\n        return (hmin, hmax+1), (wmin, wmax+1), (dmin, dmax+1)\n    \n    def _mixup_ndarray_2d(self, unlabeled_sample):\n        labeled_idx = random.choice(self.labeled_idxs)\n        q_im, q_lc = unlabeled_sample['image'][:], unlabeled_sample['coarse'][:]\n        if self.whether_use_3to2d:\n            labeled_h5 = h5py.File(self.base_dir + \"/data/slices/{}.h5\".format(self.image_list[labeled_idx]), \"r\")\n        else:\n            labeled_h5 = h5py.File(self.base_dir + \"/data/{}.h5\".format(self.image_list[labeled_idx]), 'r')\n        im = labeled_h5['image'][:]\n        lf = labeled_h5['label'][:]\n        if im.ndim != lf.ndim + 1:\n            im = im[np.newaxis, ...]\n        assert labeled_h5['granularity'][:][0] == 1, 'use a sublabeled sample to generate mixup label'\n        \n        alpha = random.randint(5, 10) / 10\n        mixed_im = q_im.copy()\n        mixed_lf = np.zeros(q_lc.shape, dtype=np.uint8)\n        bbox1 = self._get_bbox_ndarray_2d(lf)\n        bbox2 = self._get_bbox_ndarray_2d(q_lc)\n        \n        if bbox1 is None or bbox2 is None:\n            return mixed_im, mixed_lf, 0\n        \n        cropped_im1 = im[:, slice(*bbox1[0]), slice(*bbox1[1])]\n        cropped_im2 = q_im[:, slice(*bbox2[0]), slice(*bbox2[1])]\n        cropped_lf1 = lf[slice(*bbox1[0]), slice(*bbox1[1])]\n        sz_bbox2 = tuple(x[1] - x[0] for x in bbox2)\n        rsz_im1 = np.concatenate([resize(cropped_im1[channel], output_shape=sz_bbox2, order=1)[np.newaxis] for channel in range(q_im.shape[0])], axis=0)\n        rsz_lf1 = resize(cropped_lf1, output_shape=sz_bbox2, order=0)\n        \n        rsz_lf1 *= (q_lc[slice(*bbox2[0]), slice(*bbox2[1])] > 0).astype(np.uint8)\n        alph = 1 - alpha * (rsz_lf1 > 0).astype(np.uint8)\n        mixed_im2 = alph * cropped_im2 + (1 - alph) * rsz_im1\n            \n        mixed_im[:, slice(*bbox2[0]), slice(*bbox2[1])] = mixed_im2\n        mixed_lf[slice(*bbox2[0]), slice(*bbox2[1])] = rsz_lf1\n        \n        return mixed_im, mixed_lf, alpha\n    \n    def _mixup_ndarray_3d(self, unlabeled_sample):\n        labeled_idx = random.choice(self.labeled_idxs)\n        q_im, q_lc = unlabeled_sample['image'][:], unlabeled_sample['coarse'][:]\n        labeled_h5 = h5py.File(self.base_dir + \"/data/{}.h5\".format(self.image_list[labeled_idx]), 'r')\n        im = labeled_h5['image'][:]\n        lf = labeled_h5['label'][:]\n        if im.ndim != lf.ndim + 1:\n            im = im[np.newaxis, ...]\n        assert labeled_h5['granularity'][:][0] == 1, 'use a sublabeled sample to generate mixup label'\n        \n        alpha = random.random()\n        mixed_im = q_im.copy()\n        mixed_lf = np.zeros(q_lc.shape, dtype=np.uint8)\n        bbox1 = self._get_bbox_ndarray_3d(lf)\n        bbox2 = self._get_bbox_ndarray_3d(q_lc)\n        \n        if bbox1 is None or bbox2 is None:\n            return mixed_im, mixed_lf, 0\n        \n        cropped_im1 = im[:, slice(*bbox1[0]), slice(*bbox1[1]), slice(*bbox1[2])]\n        cropped_im2 = q_im[:, slice(*bbox2[0]), slice(*bbox2[1]), slice(*bbox2[2])]\n        cropped_lf1 = lf[slice(*bbox1[0]), slice(*bbox1[1]), slice(*bbox1[2])]\n        sz_bbox2 = tuple(x[1] - x[0] for x in bbox2)\n        rsz_im1 = np.concatenate([resize(cropped_im1[channel], output_shape=sz_bbox2, order=3)[np.newaxis] for channel in range(q_im.shape[0])], axis=0)\n        rsz_lf1 = resize(cropped_lf1, output_shape=sz_bbox2, order=0)\n        \n        rsz_lf1 *= (q_lc[slice(*bbox2[0]), slice(*bbox2[1]), slice(*bbox2[2])] > 0).astype(np.uint8)\n        alph = 1 - alpha * (rsz_lf1 > 0).astype(np.uint8)\n        mixed_im2 = alph * cropped_im2 + (1 - alph) * rsz_im1\n            \n        mixed_im[:, slice(*bbox2[0]), slice(*bbox2[1]), slice(*bbox2[2])] = mixed_im2\n        mixed_lf[slice(*bbox2[0]), slice(*bbox2[1]), slice(*bbox2[2])] = rsz_lf1\n        \n        return mixed_im, mixed_lf, alpha\n    \n    def _find_or_gen_unlabeled_samples(self):\n        for idx, image_name in enumerate(self.image_list):\n            if self.whether_use_3to2d and self.split == 'train':\n                h5f = h5py.File(self.base_dir + \"/data/slices/{}.h5\".format(image_name), \"r\")\n            else:\n                h5f = h5py.File(self.base_dir + \"/data/{}.h5\".format(image_name), 'r')\n            if h5f['granularity'][:][0] == 1:\n                self.labeled_idxs.append(idx)\n            else:\n                self.unlabeled_idxs.append(idx)\n        \n        if len(self.labeled_idxs) > self.n_labeled_idx:\n            self.unlabeled_idxs.extend(self.labeled_idxs[self.n_labeled_idx:])\n            self.labeled_idxs = self.labeled_idxs[:self.n_labeled_idx]\n        elif len(self.labeled_idxs) < self.n_labeled_idx:\n            self.n_labeled_idx = len(self.labeled_idxs)\n            print(f\"there are only {len(self.labeled_idxs)} labeled samples, using all\")   ", "\n\nclass RandomCrop(object):\n\n    def __init__(self, output_size):\n        self.output_size = output_size\n        self.w = -1\n        self.h = -1\n        self.d = -1 \n\n    def __call__(self, sample, keeplast=False):\n        image, coarse_label, fine_label = sample['image'], sample['coarse'], sample['fine']\n        ndim = coarse_label.ndim\n        if image.ndim == ndim:\n            image = np.expand_dims(image, axis=0)\n\n        if ndim == 3:\n            if coarse_label.shape[0] <= self.output_size[0] or\\\n                coarse_label.shape[1] <= self.output_size[1] or\\\n                    coarse_label.shape[2] <= self.output_size[2]:\n                pw = max((self.output_size[0] - coarse_label.shape[0]) // 2 + 3, 0)\n                ph = max((self.output_size[1] - coarse_label.shape[1]) // 2 + 3, 0)\n                pd = max((self.output_size[2] - coarse_label.shape[2]) // 2 + 3, 0)\n                image = np.pad(image, [(0, 0), (pw, pw), (ph, ph), (pd, pd)], mode='constant', constant_values=0)\n                coarse_label = np.pad(coarse_label, [(pw, pw), (ph, ph), (pd, pd)], mode='constant', constant_values=0)\n                fine_label = np.pad(fine_label, [(pw, pw), (ph, ph), (pd, pd)], mode='constant', constant_values=0)\n\n            (m, w, h, d) = image.shape\n            if keeplast:\n                w1 = self.w\n                h1 = self.h\n                d1 = self.d\n            else:  \n                self.w = w1 = np.random.randint(0, w - self.output_size[0])\n                self.h = h1 = np.random.randint(0, h - self.output_size[1])\n                self.d = d1 = np.random.randint(0, d - self.output_size[2])\n\n            coarse_label = coarse_label[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n            fine_label = fine_label[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n            image = image[:, w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n        \n        elif ndim == 2:\n            if coarse_label.shape[0] <= self.output_size[0] or\\\n                coarse_label.shape[1] <= self.output_size[1]:\n                pw = max((self.output_size[0] - coarse_label.shape[0]) // 2 + 3, 0)\n                ph = max((self.output_size[1] - coarse_label.shape[1]) // 2 + 3, 0)\n                image = np.pad(image, [(0, 0), (pw, pw), (ph, ph)], mode='constant', constant_values=0)\n                coarse_label = np.pad(coarse_label, [(pw, pw), (ph, ph)], mode='constant', constant_values=0)\n                fine_label = np.pad(fine_label, [(pw, pw), (ph, ph)], mode='constant', constant_values=0)\n\n            (m, w, h) = image.shape\n            if keeplast:\n                w1 = self.w\n                h1 = self.h\n            else:  \n                self.w = w1 = np.random.randint(0, w - self.output_size[0])\n                self.h = h1 = np.random.randint(0, h - self.output_size[1])\n\n            coarse_label = coarse_label[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1]]\n            fine_label = fine_label[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1]]\n            image = image[:, w1:w1 + self.output_size[0], h1:h1 + self.output_size[1]]\n        \n        return {'image': image, 'coarse': coarse_label, 'fine': fine_label}", "    \n    \nclass ToTensor(object):\n    \n    def __call__(self, sample):\n        sample['image'] = torch.from_numpy(sample['image']).float()\n        if sample.__contains__('mixed'):\n            sample['mixed'] = torch.from_numpy(sample['mixed']).float()\n            sample['alpha'] = torch.from_numpy(np.array([sample['alpha'],])).float()\n        sample['coarse'] = torch.from_numpy(sample['coarse']).long()\n        sample['fine'] = torch.from_numpy(sample['fine']).long()\n        \n        return sample"]}
{"filename": "dataloaders/refuge2020.py", "chunked_list": ["import h5py\nimport torch\nimport random\nimport numpy as np\nfrom PIL import Image\nimport imgaug.augmenters as iaa\nfrom torchvision import transforms\nfrom dataloaders.base_dataset import BaseDataset\nfrom imgaug.augmentables.segmaps import SegmentationMapsOnImage\n", "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n\n\nclass Refuge2020(BaseDataset):\n    def __init__(self, param, split='train'):\n        super(Refuge2020, self).__init__(param, split)\n        \n        self.normalize = Normalize()\n        self.grouped_image_aug_func = BaseImageAffineTransformations()\n        self.grouped_tensor_aug_func = BaseImageColorJittering()\n        \n    def __getitem__(self, idx):\n        # sample is randomly cropped and \"mixup-ed\" in `BaseDataset`\n        sample = super().__getitem__(idx)\n        \n        # return sample\n\n        if self.split == 'train':\n            sample = self.grouped_image_aug_func(sample)\n            sample = self.grouped_tensor_aug_func(sample)\n            \n        return sample", "    \n    \nclass Normalize(object):\n    def __call__(self, sample):\n        sample['image'] = sample['image'] / 255\n        if sample.__contains__('mixed'):\n            sample['mixed'] = sample['mixed'] / 255\n        return sample\n            \n            \nclass BaseImageAffineTransformations(object):\n    def __init__(self, does_flip=0.2, does_rot=0.3):\n        self.does_flip = does_flip\n        self.does_rot = does_rot\n    \n    def __call__(self, sample):\n        \n        does_flip = random.random() > self.does_flip\n        does_rot = random.random() > self.does_rot\n        flip_axis = random.randint(0, 1)\n        rot_angle = random.randint(1, 3)\n        \n        if does_flip:\n            sample['image'] = torch.flip(sample['image'], dims=(flip_axis+1,))\n            if sample.__contains__('mixed'):\n                sample['mixed'] = torch.flip(sample['mixed'], dims=(flip_axis+1,))\n            sample['coarse'] = torch.flip(sample['coarse'], dims=(flip_axis,))\n            sample['fine'] = torch.flip(sample['fine'], dims=(flip_axis,))\n            \n        if does_rot:\n            sample['image'] = torch.rot90(sample['image'], dims=(1, 2), k=rot_angle)\n            if sample.__contains__('mixed'):\n                sample['mixed'] = torch.rot90(sample['mixed'], dims=(1, 2), k=rot_angle)\n            sample['coarse'] = torch.rot90(sample['coarse'], dims=(0, 1), k=rot_angle)\n            sample['fine'] = torch.rot90(sample['fine'], dims=(0, 1), k=rot_angle)\n            \n        return sample", "            \n            \nclass BaseImageAffineTransformations(object):\n    def __init__(self, does_flip=0.2, does_rot=0.3):\n        self.does_flip = does_flip\n        self.does_rot = does_rot\n    \n    def __call__(self, sample):\n        \n        does_flip = random.random() > self.does_flip\n        does_rot = random.random() > self.does_rot\n        flip_axis = random.randint(0, 1)\n        rot_angle = random.randint(1, 3)\n        \n        if does_flip:\n            sample['image'] = torch.flip(sample['image'], dims=(flip_axis+1,))\n            if sample.__contains__('mixed'):\n                sample['mixed'] = torch.flip(sample['mixed'], dims=(flip_axis+1,))\n            sample['coarse'] = torch.flip(sample['coarse'], dims=(flip_axis,))\n            sample['fine'] = torch.flip(sample['fine'], dims=(flip_axis,))\n            \n        if does_rot:\n            sample['image'] = torch.rot90(sample['image'], dims=(1, 2), k=rot_angle)\n            if sample.__contains__('mixed'):\n                sample['mixed'] = torch.rot90(sample['mixed'], dims=(1, 2), k=rot_angle)\n            sample['coarse'] = torch.rot90(sample['coarse'], dims=(0, 1), k=rot_angle)\n            sample['fine'] = torch.rot90(sample['fine'], dims=(0, 1), k=rot_angle)\n            \n        return sample", "            \n        \nclass BaseImageColorJittering(object):\n    def __init__(self, does_jitter=0.5):\n        self.does_jitter = does_jitter\n        \n        self.tensor_aug_func = transforms.Compose([\n            transforms.RandomChoice([\n                transforms.ColorJitter(brightness=0.2),\n                transforms.ColorJitter(contrast=0.2), \n                transforms.ColorJitter(saturation=0.2),\n                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0), \n            ]),\n        ])\n        \n    def __call__(self, sample):\n        \n        does_jitter = random.random() > self.does_jitter\n        if does_jitter:\n            sample['image'] = self.tensor_aug_func(sample['image'])\n            if sample.__contains__('mixed'):\n                sample['mixed'] = self.tensor_aug_func(sample['mixed'])\n        \n        return sample", "\n\ndef n_choose_2(start, end):\n    return random.choices(list(range(start, end+1)), k=2)"]}
{"filename": "dataloaders/base_transform.py", "chunked_list": ["import h5py\nimport torch\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nfrom dataloaders.base_dataset import BaseDataset\n\n\nclass BaseImageAffineTransformations(object):\n    def __init__(self, does_flip=0.2, does_rot=0.3):\n        self.does_flip = does_flip\n        self.does_rot = does_rot\n    \n    def __call__(self, sample):\n        \n        does_flip = random.random() > self.does_flip\n        does_rot = random.random() > self.does_rot\n        flip_axis = random.randint(0, 1)\n        rot_angle = random.randint(1, 3)\n        \n        if does_flip:\n            sample['image'] = torch.flip(sample['image'], dims=[flip_axis])\n            if sample.__contains__('mixed'):\n                sample['mixed'] = torch.flip(sample['mixed'], dims=[flip_axis+1])\n            sample['coarse'] = torch.flip(sample['coarse'], dims=[flip_axis])\n            sample['fine'] = torch.flip(sample['fine'], dims=[flip_axis])\n            \n        if does_rot:\n            sample['image'] = torch.rot90(sample['image'], dims=(1, 2), k=rot_angle)\n            if sample.__contains__('mixed'):\n                sample['mixed'] = torch.rot90(sample['mixed'], dims=(1, 2), k=rot_angle)\n            sample['coarse'] = torch.rot90(sample['coarse'], dims=(0, 1), k=rot_angle)\n            sample['fine'] = torch.rot90(sample['fine'], dims=(0, 1), k=rot_angle)\n            \n        return sample", "\nclass BaseImageAffineTransformations(object):\n    def __init__(self, does_flip=0.2, does_rot=0.3):\n        self.does_flip = does_flip\n        self.does_rot = does_rot\n    \n    def __call__(self, sample):\n        \n        does_flip = random.random() > self.does_flip\n        does_rot = random.random() > self.does_rot\n        flip_axis = random.randint(0, 1)\n        rot_angle = random.randint(1, 3)\n        \n        if does_flip:\n            sample['image'] = torch.flip(sample['image'], dims=[flip_axis])\n            if sample.__contains__('mixed'):\n                sample['mixed'] = torch.flip(sample['mixed'], dims=[flip_axis+1])\n            sample['coarse'] = torch.flip(sample['coarse'], dims=[flip_axis])\n            sample['fine'] = torch.flip(sample['fine'], dims=[flip_axis])\n            \n        if does_rot:\n            sample['image'] = torch.rot90(sample['image'], dims=(1, 2), k=rot_angle)\n            if sample.__contains__('mixed'):\n                sample['mixed'] = torch.rot90(sample['mixed'], dims=(1, 2), k=rot_angle)\n            sample['coarse'] = torch.rot90(sample['coarse'], dims=(0, 1), k=rot_angle)\n            sample['fine'] = torch.rot90(sample['fine'], dims=(0, 1), k=rot_angle)\n            \n        return sample", "            \n        \nclass BaseImageColorJittering(object):\n    def __init__(self, does_jitter=0.5):\n        self.does_jitter = does_jitter\n        \n        self.tensor_aug_func = transforms.Compose([\n            transforms.RandomChoice([\n                transforms.ColorJitter(brightness=0.2),\n                transforms.ColorJitter(contrast=0.2), \n                transforms.ColorJitter(saturation=0.2),\n                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0), \n            ]),\n        ])\n        \n    def __call__(self, sample):\n        \n        does_jitter = random.random() > self.does_jitter\n        if does_jitter:\n            sample['image'] = self.tensor_aug_func(sample['image'])\n            if sample.__contains__('mixed'):\n                sample['mixed'] = self.tensor_aug_func(sample['mixed'])\n        \n        return sample"]}
{"filename": "dataloaders/acdc.py", "chunked_list": ["import h5py\nimport torch\nimport random\nimport numpy as np\nfrom PIL import Image\nimport imgaug.augmenters as iaa\nfrom torchvision import transforms\nfrom dataloaders.base_dataset import BaseDataset\nfrom imgaug.augmentables.segmaps import SegmentationMapsOnImage\n", "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n\n\nclass ACDC(BaseDataset):\n    def __init__(self, param, split='train'):\n        super(ACDC, self).__init__(param, split)\n        \n    def __getitem__(self, idx):\n        # sample is randomly cropped and \"mixup-ed\" in `BaseDataset`\n        sample = super().__getitem__(idx)\n            \n        return sample", "    "]}
{"filename": "utils/losses.py", "chunked_list": ["import torch\nfrom torch.nn import functional as F\nimport numpy as np\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\ndef dice_loss(score, target):\n    target = target.float()\n    smooth = 1e-5\n    intersect = torch.sum(score * target)\n    y_sum = torch.sum(target * target)\n    z_sum = torch.sum(score * score)\n    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n    loss = 1 - loss\n    return loss", "\n\ndef dice_loss1(score, target):\n    target = target.float()\n    smooth = 1e-5\n    intersect = torch.sum(score * target)\n    y_sum = torch.sum(target)\n    z_sum = torch.sum(score)\n    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n    loss = 1 - loss\n    return loss", "\n\ndef entropy_loss(p, C=2):\n    # p N*C*W*H*D\n    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1) / \\\n        torch.tensor(np.log(C)).cuda()\n    ent = torch.mean(y1)\n\n    return ent\n", "\n\ndef softmax_dice_loss(input_logits, target_logits):\n    \"\"\"Takes softmax on both sides and returns MSE loss\n\n    Note:\n    - Returns the sum over all examples. Divide by the batch size afterwards\n      if you want the mean.\n    - Sends gradients to inputs but not the targets.\n    \"\"\"\n    assert input_logits.size() == target_logits.size()\n    input_softmax = F.softmax(input_logits, dim=1)\n    target_softmax = F.softmax(target_logits, dim=1)\n    n = input_logits.shape[1]\n    dice = 0\n    for i in range(0, n):\n        dice += dice_loss1(input_softmax[:, i], target_softmax[:, i])\n    mean_dice = dice / n\n\n    return mean_dice", "\n\ndef entropy_loss_map(p, C=2):\n    ent = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n                       keepdim=True)/torch.tensor(np.log(C)).cuda()\n    return ent\n\n\ndef softmax_mse_loss(input_logits, target_logits, sigmoid=False):\n    \"\"\"Takes softmax on both sides and returns MSE loss\n\n    Note:\n    - Returns the sum over all examples. Divide by the batch size afterwards\n      if you want the mean.\n    - Sends gradients to inputs but not the targets.\n    \"\"\"\n    assert input_logits.size() == target_logits.size()\n    if sigmoid:\n        input_softmax = torch.sigmoid(input_logits)\n        target_softmax = torch.sigmoid(target_logits)\n    else:\n        input_softmax = F.softmax(input_logits, dim=1)\n        target_softmax = F.softmax(target_logits, dim=1)\n\n    mse_loss = (input_softmax-target_softmax)**2\n    return mse_loss", "def softmax_mse_loss(input_logits, target_logits, sigmoid=False):\n    \"\"\"Takes softmax on both sides and returns MSE loss\n\n    Note:\n    - Returns the sum over all examples. Divide by the batch size afterwards\n      if you want the mean.\n    - Sends gradients to inputs but not the targets.\n    \"\"\"\n    assert input_logits.size() == target_logits.size()\n    if sigmoid:\n        input_softmax = torch.sigmoid(input_logits)\n        target_softmax = torch.sigmoid(target_logits)\n    else:\n        input_softmax = F.softmax(input_logits, dim=1)\n        target_softmax = F.softmax(target_logits, dim=1)\n\n    mse_loss = (input_softmax-target_softmax)**2\n    return mse_loss", "\n\ndef softmax_kl_loss(input_logits, target_logits, sigmoid=False):\n    \"\"\"Takes softmax on both sides and returns KL divergence\n\n    Note:\n    - Returns the sum over all examples. Divide by the batch size afterwards\n      if you want the mean.\n    - Sends gradients to inputs but not the targets.\n    \"\"\"\n    assert input_logits.size() == target_logits.size()\n    if sigmoid:\n        input_log_softmax = torch.log(torch.sigmoid(input_logits))\n        target_softmax = torch.sigmoid(target_logits)\n    else:\n        input_log_softmax = F.log_softmax(input_logits, dim=1)\n        target_softmax = F.softmax(target_logits, dim=1)\n\n    # return F.kl_div(input_log_softmax, target_softmax)\n    kl_div = F.kl_div(input_log_softmax, target_softmax, reduction='mean')\n    # mean_kl_div = torch.mean(0.2*kl_div[:,0,...]+0.8*kl_div[:,1,...])\n    return kl_div", "\n\ndef symmetric_mse_loss(input1, input2):\n    \"\"\"Like F.mse_loss but sends gradients to both directions\n\n    Note:\n    - Returns the sum over all examples. Divide by the batch size afterwards\n      if you want the mean.\n    - Sends gradients to both input1 and input2.\n    \"\"\"\n    assert input1.size() == input2.size()\n    return torch.mean((input1 - input2)**2)", "\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2, alpha=None, size_average=True):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        if isinstance(alpha, (float, int)):\n            self.alpha = torch.Tensor([alpha, 1-alpha])\n        if isinstance(alpha, list):\n            self.alpha = torch.Tensor(alpha)\n        self.size_average = size_average\n\n    def forward(self, input, target):\n        if input.dim() > 2:\n            # N,C,H,W => N,C,H*W\n            input = input.view(input.size(0), input.size(1), -1)\n            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C\n            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C\n        target = target.view(-1, 1)\n\n        logpt = F.log_softmax(input, dim=1)\n        logpt = logpt.gather(1, target)\n        logpt = logpt.view(-1)\n        pt = Variable(logpt.data.exp())\n\n        if self.alpha is not None:\n            if self.alpha.type() != input.data.type():\n                self.alpha = self.alpha.type_as(input.data)\n            at = self.alpha.gather(0, target.data.view(-1))\n            logpt = logpt * Variable(at)\n\n        loss = -1 * (1-pt)**self.gamma * logpt\n        if self.size_average:\n            return loss.mean()\n        else:\n            return loss.sum()", "\n\nclass DiceLoss(nn.Module):\n    def __init__(self, n_classes=-1):\n        super(DiceLoss, self).__init__()\n        self.n_classes = n_classes\n        \n    def _set_class_num(self, num):\n        self.n_classes = num\n\n    def _one_hot_encoder(self, input_tensor):\n        tensor_list = []\n        for i in range(self.n_classes):\n            temp_prob = input_tensor == i * torch.ones_like(input_tensor)\n            tensor_list.append(temp_prob)\n        output_tensor = torch.cat(tensor_list, dim=1)\n        return output_tensor.float()\n\n    def _dice_loss(self, inputs, target, mask=None):\n        target = target.float()\n        smooth = 1e-5\n        intersect = torch.sum(inputs * target * mask)\n        y_sum = torch.sum(target * target * mask)\n        z_sum = torch.sum(inputs * inputs * mask)\n        loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)\n        loss = 1 - loss\n        return loss\n\n    def forward(self, inputs, target, mask=None, weight=None, softmax=False):\n        if softmax:\n            inputs = torch.softmax(inputs, dim=1)\n        self.n_classes = inputs.size(1)\n        if target.ndim != inputs.ndim:\n            target = target.unsqueeze(1)\n            target = self._one_hot_encoder(target)\n        if weight is None:\n            weight = [1] * self.n_classes\n        assert inputs.size() == target.size(), 'predict & target shape do not match'\n        class_wise_dice = []\n        loss = 0.0\n        if mask is None:\n            mask = torch.ones(inputs.size(), dtype=torch.float32, device=inputs.device)\n        for i in range(0, self.n_classes):\n            dice = self._dice_loss(inputs[:, i], target[:, i], mask[:, i])\n            class_wise_dice.append(1.0 - dice.item())\n            loss += dice * weight[i]\n        return loss / self.n_classes", "    \n    \nclass InfoNCELoss(nn.Module):\n    def __init__(self) -> None:\n        super(InfoNCELoss, self).__init__()\n    \n    def forward(self, logits, labels, temperature=0.07, loss_type='ce'):\n        if loss_type == 'ce':\n            loss = F.cross_entropy(logits / temperature, labels)\n        elif loss_type == 'cosine':\n            logits = F.normalize(logits / temperature, dim=1).detach()\n            loss = -(logits * labels).sum(dim=1).mean()\n        return loss", "\n\nclass CrossEntropyLoss(nn.Module):\n    def __init__(self) -> None:\n        super(CrossEntropyLoss, self).__init__()\n    \n    def forward(self, inputs, targets):\n        valid_targets = torch.sum(targets, dim=1) > 0\n        smooth = 1e-5\n\n        N = torch.sum(valid_targets) + smooth\n        loss = - 1 / N * torch.sum(targets * torch.log_softmax(inputs, dim=1))\n        return loss", "\n\nclass NegativeCrossEntropyLoss(nn.Module):\n\n    def forward(self, fine_inputs, coarse_targets):\n        fine2coarse_logits = torch.zeros((fine_inputs.size(0), 2) + fine_inputs.shape[2:], dtype=torch.float32, device=fine_inputs.device)\n        fine_max_logit = torch.max(fine_inputs[:, 1:], dim=1, keepdim=True).values\n        fine2coarse_logits[:, 0] = fine_inputs[:, 0]\n        fine2coarse_logits[:, 1] = fine_max_logit.squeeze(1) + torch.log(torch.exp(fine_inputs[:, 1:] - fine_max_logit).sum(1))\n\n        fine2coarse_logpred = torch.log_softmax(fine2coarse_logits, dim=1)\n        neg_log_pred_n = fine2coarse_logpred[:, 1]\n        neg_log_pred_p = fine2coarse_logpred[:, 0]\n        \n        smooth = 1e-8\n        '''pred = torch.softmax(pred_fine, dim=1)\n        pred_n = pred[:, 0]\n        neg_log_pred_n = torch.log(1 - pred_n + smooth)\n        neg_log_pred_p = torch.log(pred_n + smooth)'''\n        n = 1 - coarse_targets\n        p = coarse_targets\n\n        n_ = torch.sum(n) + smooth\n        p_ = torch.sum(p) + smooth\n\n        loss = -((1 + smooth) / p_ * torch.sum(neg_log_pred_n * p) + (1 + smooth) / n_ * torch.sum(neg_log_pred_p * n))\n        return loss", "    \n    \nclass LogitNormCrossEntropy(nn.Module):\n    def __init__(self, temperature=0.8, k=4):\n        super(LogitNormCrossEntropy, self).__init__()\n        self.k = k\n        self.tau = temperature\n        \n    @property\n    def lower_bound(self):\n        return torch.log(1 + (self.k - 1)*torch.exp(-2/self.tau))\n    \n    def forward(self, inputs, targets):\n        self.k = torch.max(targets) + 1\n        magnitudes = torch.norm(inputs, dim=1, keepdim=True)\n        normalized_inputs = inputs / (magnitudes * self.tau)\n        loss = F.cross_entropy(normalized_inputs, targets)\n        return loss", "\n\ndef entropy_minmization(p):\n    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1)\n    ent = torch.mean(y1)\n\n    return ent\n\n\ndef entropy_map(p):\n    ent_map = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n                           keepdim=True)\n    return ent_map", "\ndef entropy_map(p):\n    ent_map = -1*torch.sum(p * torch.log(p + 1e-6), dim=1,\n                           keepdim=True)\n    return ent_map\n\n\ndef compute_kl_loss(p, q):\n    p_loss = F.kl_div(F.log_softmax(p, dim=-1),\n                      F.softmax(q, dim=-1), reduction='none')\n    q_loss = F.kl_div(F.log_softmax(q, dim=-1),\n                      F.softmax(p, dim=-1), reduction='none')\n\n    # Using function \"sum\" and \"mean\" are depending on your task\n    p_loss = p_loss.mean()\n    q_loss = q_loss.mean()\n\n    loss = (p_loss + q_loss) / 2\n    return loss", ""]}
{"filename": "utils/parser.py", "chunked_list": ["import os\nimport json\nimport shutil\nimport numpy as np\nfrom os.path import *\nfrom collections import defaultdict\n\nclass DatasetParam:\n    n_dim = 0\n    n_mode = 1\n    n_coarse = 0\n    n_fine = 0\n    total_num = 0\n    legend = None\n    mapping = None\n    dataset_name = None", "    \nclass ExperimentParam:\n    patch_size = None\n    batch_size = 0\n    labeled_batch_size = 0\n    max_iter = 0\n    exp_name = None\n    pseudo_label = False\n    mixup_label = False\n    separate_norm = False\n    priority_cat = False\n    base_lr = 0\n    labeled_num = 0\n    eval_metric = 0\n    restore = False", "    \nclass StaticPaths:\n    path_to_dataset = None\n    path_to_snapshot = None\n    path_to_model = None\n    path_to_test = None\n    path_to_code = None\n    \nclass NetworkParam:\n    base_feature_num = 32\n    feature_scale = 2\n    image_scale = 2\n    is_batchnorm = True\n    network_name = None", "class NetworkParam:\n    base_feature_num = 32\n    feature_scale = 2\n    image_scale = 2\n    is_batchnorm = True\n    network_name = None\n\n\nclass BaseParser:\n    eval_metrics = {'dsc': 0, 'hd95': 1, 'precision': 2, 'recall': 3}\n    \n    def __init__(self, args):\n        self.dataset = DatasetParam()\n        self.exp = ExperimentParam()\n        self.path = StaticPaths()\n        self.network = NetworkParam()\n        self.name = None\n        \n        self.dataset.total_num = args.total_num\n        \n        self.exp.patch_size = args.patch_size\n        self.exp.batch_size = args.bs\n        try:\n            self.exp.labeled_batch_size = args.labeled_bs\n            self.exp.labeled_num = args.labeled_num\n        except AttributeError:\n            # this is a fully supervised setting\n            self.exp.labeled_batch_size = args.bs\n            self.exp.labeled_num = args.total_num\n        assert self.exp.labeled_num <= self.dataset.total_num, 'labeled num must <= total num'\n        assert self.exp.labeled_batch_size <= self.exp.batch_size, 'labeled bs must <= total bs'\n        \n        self.exp.max_iter = args.iter\n        self.exp.exp_name = args.exp_name\n        if args.exp_name == '':\n            self.exp.exp_name = f\"pseudo{args.pseudo}_mixup{args.mixup}_sn{args.sn}_pc{args.pc}\"\n        self.exp.pseudo_label = args.pseudo\n        self.exp.mixup_label = args.mixup\n        self.exp.separate_norm = args.sn\n        self.exp.priority_cat = args.pc\n        self.exp.base_lr = args.lr\n        self.exp.eval_metric = self.eval_metrics[args.eval.lower()]\n        self.exp.restore = args.restore\n        \n        self.path.path_to_dataset = args.data_path\n        self.path.path_to_snapshot = join(args.model_path, args.exp_name)\n        \n        self.network.feature_scale = args.feature_scale\n        self.network.is_batchnorm = args.is_batchnorm\n        self.network.image_scale = args.image_scale\n        self.network.base_feature_num = args.base_feature\n        \n        if not self._checkdir(self.path.path_to_dataset):\n            raise RuntimeError(f\"Dataset folder {self.path.path_to_dataset} is nonexistent\")\n        self._maybe_make_necessary_dirs()\n        self._load_or_get_necessary_data()\n    \n    @staticmethod\n    def _checkdir(path):\n        return exists(path)\n    \n    def get_dataset(self):\n        raise NotImplementedError\n    \n    def _maybe_make_necessary_dirs(self):\n        self.path.path_to_model = join(self.path.path_to_snapshot, 'model')\n        self.path.path_to_test = join(self.path.path_to_snapshot, 'test')\n        self.path.path_to_code = join(self.path.path_to_snapshot, 'code')\n        \n        if exists(self.path.path_to_code):\n            shutil.rmtree(self.path.path_to_code)\n        \n        if not self.exp.restore and exists(self.path.path_to_model) and len(os.listdir(self.path.path_to_model)) > 0:\n            x = input('press y if u want to delete old model files\\n')\n            if x.strip().lower() == 'y':\n                print('deleting old files')\n                shutil.rmtree(self.path.path_to_model)\n                # shutil.rmtree(join(self.path.path_to_snapshot))\n            else:\n                self.path.path_to_model = self.path.path_to_model + '_temp'\n                print(f'preserving old model files, current model path is {self.path.path_to_model}')\n        \n        os.makedirs(self.path.path_to_test, exist_ok=True)\n        os.makedirs(self.path.path_to_model, exist_ok=True)\n            \n        cur_path = abspath('.')\n        shutil.copytree(cur_path, self.path.path_to_code, shutil.ignore_patterns('__pycache__', '.git'))\n        \n    def _load_or_get_necessary_data(self):\n        if exists(join(self.path.path_to_dataset, 'mapping.json')):\n            with open(join(self.path.path_to_dataset, 'mapping.json'), 'r') as fp:\n                self.dataset.mapping = json.load(fp)\n        else:\n            print(f'not valid mapping file under dir {self.path.path_to_dataset}, using default mapping fine:Any -> coarse:1')\n            self.dataset.mapping = {1: list(range(1, self.dataset.n_fine))}\n        self.dataset.dataset_name = self.name\n    \n    def _dump(self):\n        x = lambda : defaultdict(x)\n        d = x()\n        for name, value in self.dataset.__dict__.items():\n            d['dataset'][name] = value\n        for name, value in self.exp.__dict__.items():\n            d['exp'][name] = value\n        for name, value in self.path.__dict__.items():\n            d['path'][name] = value\n        for name, value in self.network.__dict__.items():\n            d['network'][name] = value\n        with open(join(self.path.path_to_snapshot, 'param.json'), 'w') as fp:\n            json.dump(d, fp)\n            \n    def __repr__(self):\n        log = f\"\\n\\n{self.__class__.__name__.replace('Parser', '').upper()} DATASET PARAMETERS\\n\\n\"\n        log += '\\n'.join([f\"{k}: {v}\" for k, v in self.dataset.__dict__.items()])\n        log += '\\n\\nEXPERIMENT PARAMETERS\\n\\n'\n        log += '\\n'.join([f\"{k}: {v}\" for k, v in self.exp.__dict__.items()])\n        log += '\\n\\nNETWORK PARAMETERS\\n\\n'\n        log += '\\n'.join([f\"{k}: {v}\" for k, v in self.network.__dict__.items()])\n        log += '\\n\\nSTATIC PATHS\\n\\n'\n        log += '\\n'.join([f\"{k}: {v}\" for k, v in self.path.__dict__.items()])\n        log += '\\n\\n'\n        return log", "class BaseParser:\n    eval_metrics = {'dsc': 0, 'hd95': 1, 'precision': 2, 'recall': 3}\n    \n    def __init__(self, args):\n        self.dataset = DatasetParam()\n        self.exp = ExperimentParam()\n        self.path = StaticPaths()\n        self.network = NetworkParam()\n        self.name = None\n        \n        self.dataset.total_num = args.total_num\n        \n        self.exp.patch_size = args.patch_size\n        self.exp.batch_size = args.bs\n        try:\n            self.exp.labeled_batch_size = args.labeled_bs\n            self.exp.labeled_num = args.labeled_num\n        except AttributeError:\n            # this is a fully supervised setting\n            self.exp.labeled_batch_size = args.bs\n            self.exp.labeled_num = args.total_num\n        assert self.exp.labeled_num <= self.dataset.total_num, 'labeled num must <= total num'\n        assert self.exp.labeled_batch_size <= self.exp.batch_size, 'labeled bs must <= total bs'\n        \n        self.exp.max_iter = args.iter\n        self.exp.exp_name = args.exp_name\n        if args.exp_name == '':\n            self.exp.exp_name = f\"pseudo{args.pseudo}_mixup{args.mixup}_sn{args.sn}_pc{args.pc}\"\n        self.exp.pseudo_label = args.pseudo\n        self.exp.mixup_label = args.mixup\n        self.exp.separate_norm = args.sn\n        self.exp.priority_cat = args.pc\n        self.exp.base_lr = args.lr\n        self.exp.eval_metric = self.eval_metrics[args.eval.lower()]\n        self.exp.restore = args.restore\n        \n        self.path.path_to_dataset = args.data_path\n        self.path.path_to_snapshot = join(args.model_path, args.exp_name)\n        \n        self.network.feature_scale = args.feature_scale\n        self.network.is_batchnorm = args.is_batchnorm\n        self.network.image_scale = args.image_scale\n        self.network.base_feature_num = args.base_feature\n        \n        if not self._checkdir(self.path.path_to_dataset):\n            raise RuntimeError(f\"Dataset folder {self.path.path_to_dataset} is nonexistent\")\n        self._maybe_make_necessary_dirs()\n        self._load_or_get_necessary_data()\n    \n    @staticmethod\n    def _checkdir(path):\n        return exists(path)\n    \n    def get_dataset(self):\n        raise NotImplementedError\n    \n    def _maybe_make_necessary_dirs(self):\n        self.path.path_to_model = join(self.path.path_to_snapshot, 'model')\n        self.path.path_to_test = join(self.path.path_to_snapshot, 'test')\n        self.path.path_to_code = join(self.path.path_to_snapshot, 'code')\n        \n        if exists(self.path.path_to_code):\n            shutil.rmtree(self.path.path_to_code)\n        \n        if not self.exp.restore and exists(self.path.path_to_model) and len(os.listdir(self.path.path_to_model)) > 0:\n            x = input('press y if u want to delete old model files\\n')\n            if x.strip().lower() == 'y':\n                print('deleting old files')\n                shutil.rmtree(self.path.path_to_model)\n                # shutil.rmtree(join(self.path.path_to_snapshot))\n            else:\n                self.path.path_to_model = self.path.path_to_model + '_temp'\n                print(f'preserving old model files, current model path is {self.path.path_to_model}')\n        \n        os.makedirs(self.path.path_to_test, exist_ok=True)\n        os.makedirs(self.path.path_to_model, exist_ok=True)\n            \n        cur_path = abspath('.')\n        shutil.copytree(cur_path, self.path.path_to_code, shutil.ignore_patterns('__pycache__', '.git'))\n        \n    def _load_or_get_necessary_data(self):\n        if exists(join(self.path.path_to_dataset, 'mapping.json')):\n            with open(join(self.path.path_to_dataset, 'mapping.json'), 'r') as fp:\n                self.dataset.mapping = json.load(fp)\n        else:\n            print(f'not valid mapping file under dir {self.path.path_to_dataset}, using default mapping fine:Any -> coarse:1')\n            self.dataset.mapping = {1: list(range(1, self.dataset.n_fine))}\n        self.dataset.dataset_name = self.name\n    \n    def _dump(self):\n        x = lambda : defaultdict(x)\n        d = x()\n        for name, value in self.dataset.__dict__.items():\n            d['dataset'][name] = value\n        for name, value in self.exp.__dict__.items():\n            d['exp'][name] = value\n        for name, value in self.path.__dict__.items():\n            d['path'][name] = value\n        for name, value in self.network.__dict__.items():\n            d['network'][name] = value\n        with open(join(self.path.path_to_snapshot, 'param.json'), 'w') as fp:\n            json.dump(d, fp)\n            \n    def __repr__(self):\n        log = f\"\\n\\n{self.__class__.__name__.replace('Parser', '').upper()} DATASET PARAMETERS\\n\\n\"\n        log += '\\n'.join([f\"{k}: {v}\" for k, v in self.dataset.__dict__.items()])\n        log += '\\n\\nEXPERIMENT PARAMETERS\\n\\n'\n        log += '\\n'.join([f\"{k}: {v}\" for k, v in self.exp.__dict__.items()])\n        log += '\\n\\nNETWORK PARAMETERS\\n\\n'\n        log += '\\n'.join([f\"{k}: {v}\" for k, v in self.network.__dict__.items()])\n        log += '\\n\\nSTATIC PATHS\\n\\n'\n        log += '\\n'.join([f\"{k}: {v}\" for k, v in self.path.__dict__.items()])\n        log += '\\n\\n'\n        return log", "\n\nclass ACDCParser(BaseParser):\n    name = 'ACDC'\n    def __init__(self, args):\n        super(ACDCParser, self).__init__(args)\n        \n        self.dataset.n_dim = 2.5\n        self.dataset.n_mode = 1\n        self.dataset.n_coarse = 2\n        self.dataset.n_fine = 4\n        self.dataset.total_num = 1312  # total ACDC slices for 140 cases\n        self.dataset.legend = ['ENDO-L', 'EPI-L', 'ENDO-R']\n        \n        self._dump()\n        \n    def get_dataset(self, *args, **kwargs):\n        from dataloaders.acdc import ACDC\n        return ACDC(self, *args, **kwargs)", "        \n        \nclass BraTS2021Parser(BaseParser):\n    name = 'BraTS2021'\n    def __init__(self, args):\n        super(BraTS2021Parser, self).__init__(args)\n        \n        self.dataset.n_dim = 3\n        self.dataset.n_mode = 4\n        self.dataset.n_coarse = 2\n        self.dataset.n_fine = 4\n        self.dataset.total_num = 876\n        self.dataset.legend = ['NTC', 'PET', 'GD-T']\n        \n        self._dump()\n    \n    def get_dataset(self, *args, **kwargs):\n        from dataloaders.brats2021 import BraTS2021\n        return BraTS2021(self, *args, **kwargs)", "        \n\nclass Refuge2020Parser(BaseParser):\n    name = 'REFUGE2020'\n    def __init__(self, args):\n        super(Refuge2020Parser, self).__init__(args)\n        \n        self.dataset.n_dim = 2\n        self.dataset.n_mode = 3\n        self.dataset.n_coarse = 2\n        self.dataset.n_fine = 3\n        self.dataset.total_num = min(self.dataset.total_num, 252)\n        self.dataset.legend = ['optical-disk', 'optical-cup']\n        \n        self._dump()\n    \n    def get_dataset(self, *args, **kwargs):\n        from dataloaders.refuge2020 import Refuge2020\n        return Refuge2020(self, *args, **kwargs)", "    \n    \nclass ProstateParser(BaseParser):\n    name = 'Prostate'\n    def __init__(self, args):\n        super(ProstateParser, self).__init__(args)\n        \n        self.dataset.n_dim = 2.5\n        self.dataset.n_mode = 2\n        self.dataset.n_coarse = 2\n        self.dataset.n_fine = 3\n        self.dataset.total_num = min(self.dataset.total_num, 458)\n        self.dataset.legend = ['central_gland', 'peripheral_zone']\n        \n        self._dump()\n    \n    def get_dataset(self, *args, **kwargs):\n        from dataloaders.prostate import Prostate\n        return Prostate(self, *args, **kwargs)", "        \n        \nclass Parser:\n    def __init__(self, args):\n        self.parser = None\n        \n        if 'acdc' in args.data_path.lower():\n            self.parser = ACDCParser(args)\n        elif 'brats2021' in args.data_path.lower():\n            self.parser = BraTS2021Parser(args)\n        elif 'refuge2020' in args.data_path.lower():\n            self.parser = Refuge2020Parser(args)\n        elif 'prostate' in args.data_path.lower():\n            self.parser = ProstateParser(args)\n        else:\n            raise NotImplementedError\n        \n    def get_param(self):\n        return self.parser\n            \n    def __repr__(self):\n        return self.parser.__repr__()", "    "]}
{"filename": "utils/ramps.py", "chunked_list": ["import numpy as np\n\n\ndef sigmoid_rampup(current, rampup_length):\n    # Exponential rampup from https://arxiv.org/abs/1610.02242\n    if rampup_length == 0:\n        return 1.0\n    else:\n        current = np.clip(current, 0.0, rampup_length)\n        phase = 1.0 - current / rampup_length\n        return float(np.exp(-5.0 * phase * phase))", "\n\ndef linear_rampup(current, rampup_length):\n    # Linear rampup\n    assert current >= 0 and rampup_length >= 0\n    if current >= rampup_length:\n        return 1.0\n    else:\n        return current / rampup_length\n", "\n\ndef cosine_rampdown(current, rampdown_length):\n    # Cosine rampdown from https://arxiv.org/abs/1608.03983\n    assert 0 <= current <= rampdown_length\n    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))"]}
{"filename": "utils/visualize.py", "chunked_list": ["from sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, KernelPCA\n\nfrom torchvision.utils import make_grid\nfrom matplotlib.cm import get_cmap\n# from meta import db as param\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\nimport math\nimport os\n\nbackends = ['GTK3Agg', 'GTK3Cairo', 'MacOSX', 'nbAgg', 'Qt4Agg', 'Qt4Cairo', 'Qt5Agg',\n            'Qt5Cairo', 'TkAgg', 'TkCairo', 'WebAgg', 'WX', 'WXAgg', 'WXCairo']\n\n\ndef visualize(\n    feature_map, label_map, feat_axis, etype, param,\n    impath=None,\n    sampling_ratio=20000,\n    n_components=2,\n    legend=None\n):\n\n    if torch.is_tensor(feature_map):\n        feature_map = feature_map.cpu().detach().numpy()\n    if torch.is_tensor(label_map):\n        label_map = label_map.cpu().detach().numpy()\n    if not isinstance(feature_map, np.ndarray):\n        raise NotImplementedError\n    if not isinstance(label_map, np.ndarray):\n        raise NotImplementedError\n    \n    # permute axes, feature vector is along the last dim\n    if feat_axis != -1 and feat_axis != feature_map.ndim - 1:\n        axes = list(i for i in range(feature_map.ndim))\n        axes.pop(feat_axis)\n        axes.append(feat_axis)\n        feature_map = feature_map.transpose(*axes)\n    assert feature_map.shape[:-1] == label_map.shape\n\n    # ignore bg feature vecs\n    l_feature_vector = feature_map.shape[-1]\n    n_label = len(np.unique(label_map))\n    feature_map = feature_map.reshape((-1, l_feature_vector), order='C')\n    label_map = label_map.flatten(order='C')\n    feature_map = feature_map[label_map > 0]\n    label_map = label_map[label_map > 0]\n\n    # sampling an equal number of all subclasses\n    if sampling_ratio < 1:\n        sampling_ratio = sampling_ratio * len(label_map)\n    if sampling_ratio > len(label_map):\n        sampling_ratio = len(label_map)\n    indices = []\n    for i_label in range(1, n_label):\n        i_label_map = np.nonzero(label_map == i_label)[0]\n        if len(i_label_map) < 2000:\n            print(f'not enough labeled pts at label {i_label}, current num={len(i_label_map)}, thresholded at 2000')\n            return False\n        n_sample = min(sampling_ratio // (n_label - 1), len(i_label_map))\n        indices.extend(np.random.choice(i_label_map, n_sample, replace=False))\n\n    feature_map = feature_map[indices]\n    label_map = label_map[indices]\n\n    # feature->embedding\n    if etype.lower() == 'kpca':\n        embedding = KernelPCA(\n            kernel='rbf',\n            gamma=10,\n            degree=5,\n            n_components=n_components\n        ).fit_transform(feature_map)\n    elif etype.lower() == 'tsne':\n        embedding = TSNE(\n            perplexity=50,\n            learning_rate='auto',\n            init='pca',\n            n_components=n_components\n        ).fit_transform(feature_map)\n    elif etype.lower() == 'pca':\n        embedding = PCA(\n            n_components=n_components\n        ).fit_transform(feature_map)\n    else:\n        raise NotImplementedError(f'This {etype} visualization method is not supported')\n\n    # plot and save plot (if not interactive backend)\n    fig = plt.figure()\n    if embedding.shape[1] == 3:\n        ax = fig.add_subplot(111, projection='3d')\n    else:\n        ax = fig.add_subplot(111)\n    \n    if legend is not None:\n        scatter = ax.scatter(*embedding.transpose(), c=label_map.tolist(), s=.1, label=[legend[x-1] for x in label_map])\n    else:\n        scatter = ax.scatter(*embedding.transpose(), c=label_map.tolist(), s=.1)\n    legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"fine classes\")\n    ax.add_artist(legend)\n    \n    if impath is None:\n        plt.savefig(f'{etype}.png')\n    if matplotlib.get_backend() in backends:\n        plt.show()\n    plt.savefig(os.path.join(param.path.path_to_test, f'{impath}'))\n    plt.close()\n    plt.clf()\n\n    return True", "\ndef visualize(\n    feature_map, label_map, feat_axis, etype, param,\n    impath=None,\n    sampling_ratio=20000,\n    n_components=2,\n    legend=None\n):\n\n    if torch.is_tensor(feature_map):\n        feature_map = feature_map.cpu().detach().numpy()\n    if torch.is_tensor(label_map):\n        label_map = label_map.cpu().detach().numpy()\n    if not isinstance(feature_map, np.ndarray):\n        raise NotImplementedError\n    if not isinstance(label_map, np.ndarray):\n        raise NotImplementedError\n    \n    # permute axes, feature vector is along the last dim\n    if feat_axis != -1 and feat_axis != feature_map.ndim - 1:\n        axes = list(i for i in range(feature_map.ndim))\n        axes.pop(feat_axis)\n        axes.append(feat_axis)\n        feature_map = feature_map.transpose(*axes)\n    assert feature_map.shape[:-1] == label_map.shape\n\n    # ignore bg feature vecs\n    l_feature_vector = feature_map.shape[-1]\n    n_label = len(np.unique(label_map))\n    feature_map = feature_map.reshape((-1, l_feature_vector), order='C')\n    label_map = label_map.flatten(order='C')\n    feature_map = feature_map[label_map > 0]\n    label_map = label_map[label_map > 0]\n\n    # sampling an equal number of all subclasses\n    if sampling_ratio < 1:\n        sampling_ratio = sampling_ratio * len(label_map)\n    if sampling_ratio > len(label_map):\n        sampling_ratio = len(label_map)\n    indices = []\n    for i_label in range(1, n_label):\n        i_label_map = np.nonzero(label_map == i_label)[0]\n        if len(i_label_map) < 2000:\n            print(f'not enough labeled pts at label {i_label}, current num={len(i_label_map)}, thresholded at 2000')\n            return False\n        n_sample = min(sampling_ratio // (n_label - 1), len(i_label_map))\n        indices.extend(np.random.choice(i_label_map, n_sample, replace=False))\n\n    feature_map = feature_map[indices]\n    label_map = label_map[indices]\n\n    # feature->embedding\n    if etype.lower() == 'kpca':\n        embedding = KernelPCA(\n            kernel='rbf',\n            gamma=10,\n            degree=5,\n            n_components=n_components\n        ).fit_transform(feature_map)\n    elif etype.lower() == 'tsne':\n        embedding = TSNE(\n            perplexity=50,\n            learning_rate='auto',\n            init='pca',\n            n_components=n_components\n        ).fit_transform(feature_map)\n    elif etype.lower() == 'pca':\n        embedding = PCA(\n            n_components=n_components\n        ).fit_transform(feature_map)\n    else:\n        raise NotImplementedError(f'This {etype} visualization method is not supported')\n\n    # plot and save plot (if not interactive backend)\n    fig = plt.figure()\n    if embedding.shape[1] == 3:\n        ax = fig.add_subplot(111, projection='3d')\n    else:\n        ax = fig.add_subplot(111)\n    \n    if legend is not None:\n        scatter = ax.scatter(*embedding.transpose(), c=label_map.tolist(), s=.1, label=[legend[x-1] for x in label_map])\n    else:\n        scatter = ax.scatter(*embedding.transpose(), c=label_map.tolist(), s=.1)\n    legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"fine classes\")\n    ax.add_artist(legend)\n    \n    if impath is None:\n        plt.savefig(f'{etype}.png')\n    if matplotlib.get_backend() in backends:\n        plt.show()\n    plt.savefig(os.path.join(param.path.path_to_test, f'{impath}'))\n    plt.close()\n    plt.clf()\n\n    return True", "\n\ndef gen_colors(n):\n    cmap = get_cmap('viridis')\n    rgb = [cmap(i)[:-1] for i in np.arange(0, n) / n]\n    return rgb\n\n\ndef make_image(writer, param, image_or_mask, imname, iter_num, n_labels=0, normalize=False, n_grid_images=5):\n    label_colors = gen_colors(n_labels)\n    dim = math.floor(param.dataset.n_dim)\n    \n    if dim == 3:\n        image_or_mask = image_or_mask[0]  # take first batch\n        if image_or_mask.ndim == 3:\n            image_or_mask = image_or_mask.unsqueeze(0)\n        n, h, w, d = image_or_mask.shape\n        step = int(np.ceil(d / n_grid_images))\n        image_or_mask = image_or_mask[..., 0: d: step].permute(3, 0, 1, 2)\n        \n        if normalize:\n            # a MRI instance, take the first mode\n            if param.dataset.n_mode == 3:\n                # treat as a rgb image\n                write_im = (image_or_mask[:n_grid_images, 0:3] * 255).to(torch.uint8)\n                grid_image = make_grid(write_im, n_grid_images)\n                writer.add_image(f'image/{imname}', grid_image, iter_num)\n            else:\n                write_im = image_or_mask[:n_grid_images, 0:1].repeat(1, 3, 1, 1)\n                grid_image = make_grid(write_im, n_grid_images, normalize=True)\n                writer.add_image(f'image/{imname}', grid_image, iter_num)\n            \n        else:\n            # a label map instance\n            write_im = torch.zeros((n_grid_images, 3, h, w), device=image_or_mask.device)\n            if n == 1:\n                for i_label in range(1, n_labels):\n                    for color_channel in range(3):\n                        write_im[:, color_channel] += (image_or_mask[0] == i_label) * label_colors[i_label - 1][color_channel] / n_labels\n            \n            else:  # one-hot label map\n                for i_label in range(1, n_labels):\n                    for color_channel in range(3):\n                        write_im[:, color_channel] += image_or_mask[:, i_label] * label_colors[i_label - 1][color_channel] / n_labels\n            \n            grid_image = make_grid(write_im, n_grid_images, normalize=False)\n            writer.add_image(f'image/{imname}', grid_image, iter_num)\n    \n    elif dim == 2:\n        if image_or_mask.ndim == 3:\n            image_or_mask = image_or_mask.unsqueeze(1)\n        b, n, h, w = image_or_mask.shape\n        n_grid_images = min(b, n_grid_images)\n        \n        if normalize:\n            # a MRI instance, take the first mode\n            if param.dataset.n_mode == 3:\n                write_im = (image_or_mask[:n_grid_images, 0:3] * 255).to(torch.uint8)\n                grid_image = make_grid(write_im, n_grid_images)\n                writer.add_image(f'image/{imname}', grid_image, iter_num)\n            else:\n                write_im = image_or_mask[:n_grid_images, 0:1].repeat(1, 3, 1, 1)\n                grid_image = make_grid(write_im, n_grid_images, normalize=True)\n                writer.add_image(f'image/{imname}', grid_image, iter_num)\n            \n        else:\n            # a label map instance\n            write_im = torch.zeros((n_grid_images, 3, h, w), device=image_or_mask.device)\n            if n == 1:\n                for i_label in range(1, n_labels):\n                    for color_channel in range(3):\n                        write_im[:, color_channel] += (image_or_mask[:n_grid_images, 0] == i_label) * label_colors[i_label - 1][color_channel]\n            \n            else:  # one-hot label map\n                for i_label in range(1, n_labels):\n                    for color_channel in range(3):\n                        write_im[:, color_channel] += image_or_mask[:n_grid_images, i_label] * label_colors[i_label - 1][color_channel]\n            \n            grid_image = make_grid(write_im, n_grid_images, normalize=False)\n            writer.add_image(f'image/{imname}', grid_image, iter_num)", "def make_image(writer, param, image_or_mask, imname, iter_num, n_labels=0, normalize=False, n_grid_images=5):\n    label_colors = gen_colors(n_labels)\n    dim = math.floor(param.dataset.n_dim)\n    \n    if dim == 3:\n        image_or_mask = image_or_mask[0]  # take first batch\n        if image_or_mask.ndim == 3:\n            image_or_mask = image_or_mask.unsqueeze(0)\n        n, h, w, d = image_or_mask.shape\n        step = int(np.ceil(d / n_grid_images))\n        image_or_mask = image_or_mask[..., 0: d: step].permute(3, 0, 1, 2)\n        \n        if normalize:\n            # a MRI instance, take the first mode\n            if param.dataset.n_mode == 3:\n                # treat as a rgb image\n                write_im = (image_or_mask[:n_grid_images, 0:3] * 255).to(torch.uint8)\n                grid_image = make_grid(write_im, n_grid_images)\n                writer.add_image(f'image/{imname}', grid_image, iter_num)\n            else:\n                write_im = image_or_mask[:n_grid_images, 0:1].repeat(1, 3, 1, 1)\n                grid_image = make_grid(write_im, n_grid_images, normalize=True)\n                writer.add_image(f'image/{imname}', grid_image, iter_num)\n            \n        else:\n            # a label map instance\n            write_im = torch.zeros((n_grid_images, 3, h, w), device=image_or_mask.device)\n            if n == 1:\n                for i_label in range(1, n_labels):\n                    for color_channel in range(3):\n                        write_im[:, color_channel] += (image_or_mask[0] == i_label) * label_colors[i_label - 1][color_channel] / n_labels\n            \n            else:  # one-hot label map\n                for i_label in range(1, n_labels):\n                    for color_channel in range(3):\n                        write_im[:, color_channel] += image_or_mask[:, i_label] * label_colors[i_label - 1][color_channel] / n_labels\n            \n            grid_image = make_grid(write_im, n_grid_images, normalize=False)\n            writer.add_image(f'image/{imname}', grid_image, iter_num)\n    \n    elif dim == 2:\n        if image_or_mask.ndim == 3:\n            image_or_mask = image_or_mask.unsqueeze(1)\n        b, n, h, w = image_or_mask.shape\n        n_grid_images = min(b, n_grid_images)\n        \n        if normalize:\n            # a MRI instance, take the first mode\n            if param.dataset.n_mode == 3:\n                write_im = (image_or_mask[:n_grid_images, 0:3] * 255).to(torch.uint8)\n                grid_image = make_grid(write_im, n_grid_images)\n                writer.add_image(f'image/{imname}', grid_image, iter_num)\n            else:\n                write_im = image_or_mask[:n_grid_images, 0:1].repeat(1, 3, 1, 1)\n                grid_image = make_grid(write_im, n_grid_images, normalize=True)\n                writer.add_image(f'image/{imname}', grid_image, iter_num)\n            \n        else:\n            # a label map instance\n            write_im = torch.zeros((n_grid_images, 3, h, w), device=image_or_mask.device)\n            if n == 1:\n                for i_label in range(1, n_labels):\n                    for color_channel in range(3):\n                        write_im[:, color_channel] += (image_or_mask[:n_grid_images, 0] == i_label) * label_colors[i_label - 1][color_channel]\n            \n            else:  # one-hot label map\n                for i_label in range(1, n_labels):\n                    for color_channel in range(3):\n                        write_im[:, color_channel] += image_or_mask[:n_grid_images, i_label] * label_colors[i_label - 1][color_channel]\n            \n            grid_image = make_grid(write_im, n_grid_images, normalize=False)\n            writer.add_image(f'image/{imname}', grid_image, iter_num)", "        \n        \ndef make_curve(writer, pred_, gt_, curve_name, n_labels, iter_num):\n    assert pred_.shape == gt_.shape\n    write_dict = np.zeros((n_labels-1, 3))\n    \n    for i in range(1, n_labels):\n        pred = pred_ == i\n        gt = gt_ == i\n        if pred.sum() == 0 or gt.sum() == 0:\n            continue\n        \n        tp = torch.bitwise_and(pred, gt).sum()\n        fp = torch.bitwise_and(pred, torch.bitwise_not(gt)).sum()\n        fn = torch.bitwise_and(torch.bitwise_not(pred), gt).sum()\n        \n        write_dict[i-1, 0] = 2 * tp / (2 * tp + fp + fn)  # dice\n        write_dict[i-1, 1] = tp / (tp + fn)  # recall\n        write_dict[i-1, 2] = tp / (tp + fp)  # precision\n        \n    writer.add_scalars(f'train/{curve_name}_dice', {f'label={i}': write_dict[i-1, 0] for i in range(1, n_labels)}, iter_num)\n    writer.add_scalars(f'train/{curve_name}_precision', {f'label={i}': write_dict[i-1, 1] for i in range(1, n_labels)}, iter_num)\n    writer.add_scalars(f'train/{curve_name}_recall', {f'label={i}': write_dict[i-1, 2] for i in range(1, n_labels)}, iter_num)", "\n\nif __name__ == '__main__':\n    test_shape = (1, 211, 16, 102, 145)\n    test_inputs = np.random.random(test_shape)\n    test_labels = np.sum(test_inputs, axis=2)\n    test_labels = (test_labels > test_labels.max() * 0.85) * 1 + (test_labels > test_labels.max() * 0.9) * 1\n\n    visualize(test_inputs, test_labels, 2, 'tsne', n_components=3)\n    ", "    "]}
{"filename": "utils/partition_dataset.py", "chunked_list": ["import os, json, shutil\nimport random\nfrom os.path import *\n\npath = '/data/dailinrui/dataset/refuge2020'\nnew_path = '/data/dailinrui/dataset/refuge2020_trainExpand'\n\nwith open(join(path, 'train.list'), 'r') as fp:\n    all_list = [x for x in fp.readlines() if x.startswith('n')]\n    ", "    \nrandom.shuffle(all_list)\ntrain_list = all_list[:round(0.7 * len(all_list))]\nval_list = all_list[round(0.7 * len(all_list)):round(0.8 * len(all_list))]\ntest_list = all_list[round(0.9 * len(all_list)):]\n\nos.makedirs(join(new_path, 'data'), exist_ok=True)\n\nwith open(join(new_path, 'train.list'), 'w') as fp:\n    fp.writelines(train_list)\nwith open(join(new_path, 'val.list'), 'w') as fp:\n    fp.writelines(val_list)", "with open(join(new_path, 'train.list'), 'w') as fp:\n    fp.writelines(train_list)\nwith open(join(new_path, 'val.list'), 'w') as fp:\n    fp.writelines(val_list)\nwith open(join(new_path, 'test.list'), 'w') as fp:\n    fp.writelines(test_list)\nmapping = {1: [1, 2]}\nwith open(join(path, 'mapping.json'), 'w') as fp:\n    json.dump(mapping, fp)\n    \nfor h5 in os.listdir(join(path, 'data')):\n    if not h5.startswith('n'):\n        continue\n    shutil.copy(join(path, 'data', h5), join(new_path, 'data', h5))\n    print(join(path, 'data', h5), end='\\r')", "    \nfor h5 in os.listdir(join(path, 'data')):\n    if not h5.startswith('n'):\n        continue\n    shutil.copy(join(path, 'data', h5), join(new_path, 'data', h5))\n    print(join(path, 'data', h5), end='\\r')"]}
{"filename": "networks/multi_fg_proposed.py", "chunked_list": ["\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom networks.utils import *\nfrom collections import OrderedDict\n", "from collections import OrderedDict\n\nparam = None\n\n\nclass ForegroundBranch(nn.Module):\n    def __init__(self, in_size, repeat, index):\n        super(ForegroundBranch).__init__()\n        self.foreground_transformer = nn.Sequential(OrderedDict([\n            ('trans', BatchNormNd(in_size)),\n            ('actv', nn.LeakyReLU()),\n            ('ext', ConvBlock(repeat=repeat,\n                              in_channels=in_size, out_channels=in_size,\n                              kernel_size=(3,)*param.dataset.n_dim, padding=(1,)*param.dataset.n_dim))\n        ]))\n        self.coarse_classifier = ConvNd(in_size, 1, 1)\n        self.fine_classifier = ConvNd(in_size + param.exp.priority_cat, len(param.dataset.mapping[index]), 1)\n        \n    def forward(self, inputs):\n        out = self.foreground_transformer(inputs)\n        coarse_logit = self.coarse_classifier(out)\n        if param.exp.priority_cat:\n            fine_logit = self.fine_classifier(torch.cat([coarse_logit, out], dim=1))\n        else:\n            fine_logit = self.fine_classifier(out)\n        return coarse_logit, fine_logit", "    \n\nclass UNetMultiBranchClassifier(nn.Module):\n    def __init__(self, in_size, repeat=1):\n        super(UNetMultiBranchClassifier, self).__init__()\n\n        if param.exp.separate_norm:\n            self.foreground_branches = nn.ModuleDict()\n            for c in range(1, param.dataset.n_coarse):\n                self.foreground_branches[c] = ForegroundBranch(in_size, repeat, c)\n\n            self.background_branch = nn.Sequential(\n                BatchNormNd(in_size),\n                nn.ReLU(),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * param.dataset.n_dim, padding=(1,) * param.dataset.n_dim\n                ),\n                ConvNd(in_size, 1, 1),\n            )\n        \n        elif param.exp.priority_cat:\n            self.conv = nn.Sequential(\n                BatchNormNd(in_size),\n                nn.ReLU(inplace=True),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * param.dataset.n_dim, padding=(1,) * param.dataset.n_dim\n                ),\n            )\n            self.coarse = ConvNd(in_size, param.dataset.n_coarse, 1)\n            self.fine = ConvNd(in_size + param.dataset.n_coarse, param.dataset.n_fine, 1)\n\n        else:\n            self.conv = nn.Sequential(\n                BatchNormNd(in_size),\n                nn.ReLU(inplace=True),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * param.dataset.n_dim, padding=(1,) * param.dataset.n_dim\n                ),\n            )\n            self.coarse = ConvNd(in_size, param.dataset.n_coarse, 1)\n            self.fine = ConvNd(in_size, param.dataset.n_fine, 1)\n        \n    def forward(self, inputs):\n\n        if param.exp.separate_norm:\n            \n            all_fine_logits = torch.zeros((inputs.size(0), param.dataset.n_fine) + inputs.shape[2:], device=inputs.device, dtype=torch.float32)\n            all_coarse_logits = torch.zeros((inputs.size(0), param.dataset.n_coarse) + inputs.shape[2:], device=inputs.device, dtype=torch.float32)\n            \n            for c in range(1, param.dataset.n_coarse):\n                coarse_logit, fine_logit = self.foreground_branches[c](inputs)\n                all_coarse_logits[:, c] = coarse_logit\n                all_fine_logits[:, slice(*param.dataset.mapping[c])] = fine_logit\n            bg_logit = self.background_branch(inputs)\n            all_coarse_logits[:, 0] = all_fine_logits[:, 0] = bg_logit\n\n            return {'coarse_logit': all_coarse_logits, 'fine_logit': all_fine_logits}\n        \n        elif param.exp.priority_cat:\n            inputs = self.conv(inputs)\n            coarse = self.coarse(inputs)\n            fine = torch.cat([inputs, coarse], dim=1)\n            fine = self.fine(fine)\n            return {'coarse_logit': coarse, 'fine_logit': fine}\n        \n        else:\n            inputs = self.conv(inputs)\n            coarse = self.coarse(inputs)\n            fine = self.fine(inputs)\n            return {'coarse_logit': coarse, 'fine_logit': fine}", "        \n\nclass UNetBasedNetwork(nn.Module):\n\n    def __init__(self, parameter):\n        super(UNetBasedNetwork, self).__init__()\n        global param\n        param = parameter\n        set_param(parameter)\n\n        filters = [param.network.base_feature_num * param.network.feature_scale ** x for x in range(5)]\n\n        # encoder\n        self.encoder = UNetEncoder(\n            in_chns=param.dataset.n_mode,\n            feature_num=filters,\n            layer_num=5\n        )\n        # decoder\n        self.decoder = UNetDecoder(\n            feature_num=filters,\n            layer_num=5,\n            output_feature_map=True,\n            trunc_at_feature_map=True\n        )\n        # coarse to fine classification head\n        self.drop = nn.Dropout(0.3)\n        self.classifier = UNetMultiBranchClassifier(in_size=filters[-1], repeat=1)\n\n    def forward(self, inputs):\n        embed = self.encoder(inputs)\n        feat_map = self.decoder(embed)\n        feat_map = self.drop(feat_map)\n        outdict = self.classifier(feat_map)\n        return outdict\n    \n    @torch.no_grad()\n    def gen_pseudo_labels(self, q_im, q_soft, q_lc, threshold=0.4):\n        rot_angle = random.randint(0, 3)\n        flip_axis = random.randint(2, 3)\n        \n        k_im = torch.flip(torch.rot90(q_im, k=rot_angle, dims=(2, 3)), dims=(flip_axis,))\n        \n        k_out = self.forward(k_im)\n        k_soft = torch.softmax(k_out['fine_logit'], dim=1)\n        k_soft = torch.rot90(torch.flip(k_soft, dims=(flip_axis,)), k=-rot_angle, dims=(2, 3))\n        \n        # one-hot label\n        pseudo_label = torch.zeros(q_soft.shape, dtype=torch.float32, device=q_soft.device)\n        for i_label in range(param.dataset.n_fine):\n            i_ = 0 if i_label == 0 else 1\n            pseudo_label[:, i_label] = (k_soft[:, i_label] > threshold) & (q_soft[:, i_label] > threshold) & (q_lc == i_)\n\n        return pseudo_label\n    \n    @torch.no_grad()\n    def gen_mixup_labels(self, q_im, q_lc, q_soft, mixed_im, mixed_lf, alpha, threshold=0.4, with_pseudo_label=True):\n        \n        if param.dataset.n_dim == 3:\n            mixed_lf = F.one_hot(mixed_lf, param.dataset.n_fine).permute(0, 4, 1, 2, 3)\n        elif param.dataset.n_dim == 2:\n            mixed_lf = F.one_hot(mixed_lf, param.dataset.n_fine).permute(0, 3, 1, 2)\n        \n        mixed_label = torch.zeros(mixed_lf.size(), device=mixed_lf.device, dtype=torch.float32)\n        \n        if with_pseudo_label:\n            pseudo_label = self.gen_pseudo_labels(q_im, q_soft, q_lc, threshold)\n            for i_batch in range(pseudo_label.size(0)):\n                mixed_label[i_batch] = pseudo_label[i_batch] * (1 - alpha[i_batch]) + mixed_lf[i_batch] * alpha[i_batch]\n        else:\n            for i_batch in range(pseudo_label.size(0)):\n                mixed_label[i_batch] = mixed_lf[i_batch] * alpha[i_batch]\n            \n        mixed_pred = self.forward(mixed_im)['fine_logit']\n        return mixed_pred, mixed_label", "    \n"]}
{"filename": "networks/unet.py", "chunked_list": ["\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import defaultdict, OrderedDict\nfrom networks.utils import init_weights, set_param, Conv, BatchNorm, MaxPool\n", "from networks.utils import init_weights, set_param, Conv, BatchNorm, MaxPool\n\n\nparam = None\n\n\nclass UnetConv(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm, kernel_size=None, padding_size=None, init_stride=None):\n        super(UnetConv, self).__init__()\n        \n        if kernel_size is None:\n            kernel_size = (3,) * math.floor(param.dataset.n_dim)\n            padding_size = (1,) * math.floor(param.dataset.n_dim)\n            init_stride = 1\n\n        if is_batchnorm:\n            self.conv1 = nn.Sequential(OrderedDict([\n                ('conv', Conv(in_size, out_size, kernel_size, init_stride, padding_size)),\n                ('bn', BatchNorm(out_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n            if param.exp.separate_norm:\n                self.conv2 = Conv(out_size, out_size, kernel_size, init_stride, padding_size)\n            else:\n                self.conv2 = nn.Sequential(OrderedDict([\n                    ('conv', Conv(out_size, out_size, kernel_size, init_stride, padding_size)),\n                    ('bn', BatchNorm(out_size)),\n                    ('nl', nn.ReLU(inplace=True)),\n                ]))\n        else:\n            self.conv1 = nn.Sequential(OrderedDict([\n                ('conv', Conv(in_size, out_size, kernel_size, init_stride, padding_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n            self.conv2 = nn.Sequential(OrderedDict([\n                ('conv', Conv(out_size, out_size, kernel_size, init_stride, padding_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs", "\n\nclass UnetUpConcat(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm):\n        super(UnetUpConcat, self).__init__()\n        self.conv = UnetConv(in_size + out_size, out_size, is_batchnorm)\n        if math.floor(param.dataset.n_dim) == 3:\n            self.up = nn.Upsample(scale_factor=(2, 2, 2), mode='trilinear')\n        elif math.floor(param.dataset.n_dim) == 2:\n            self.up = nn.Upsample(scale_factor=(2, 2), mode='bilinear')\n        else:\n            self.up = None\n\n    def forward(self, inputs1, inputs2):\n        outputs2 = self.up(inputs2)\n        offset = outputs2.size()[2] - inputs1.size()[2]\n        padding = 2 * [offset // 2, offset // 2, 0]\n        outputs1 = F.pad(inputs1, padding)\n        out1 = self.conv(torch.cat([outputs1, outputs2], 1))\n        \n        return out1", "        \n\nclass UNet(nn.Module):\n\n    def __init__(self, parameter):\n        super(UNet, self).__init__()\n        global param\n        param = parameter\n        set_param(parameter)\n        self.in_channels = param.dataset.n_mode\n        self.is_batchnorm = param.network.is_batchnorm\n        self.feature_scale = param.network.feature_scale\n\n        filters = [param.network.base_feature_num * self.feature_scale ** x for x in range(5)]\n\n        # downsampling\n        self.conv1 = UnetConv(self.in_channels, filters[0], self.is_batchnorm)\n        self.maxpool1 = MaxPool(kernel_size=(2,) * math.floor(param.dataset.n_dim))\n        \n        self.conv2 = UnetConv(filters[0], filters[1], self.is_batchnorm)\n        self.maxpool2 = MaxPool(kernel_size=(2,) * math.floor(param.dataset.n_dim))\n        \n        self.conv3 = UnetConv(filters[1], filters[2], self.is_batchnorm)\n        self.maxpool3 = MaxPool(kernel_size=(2,) * math.floor(param.dataset.n_dim))\n        \n        self.conv4 = UnetConv(filters[2], filters[3], self.is_batchnorm)\n        self.maxpool4 = MaxPool(kernel_size=(2,) * math.floor(param.dataset.n_dim))\n\n        self.center = UnetConv(filters[3], filters[4], self.is_batchnorm)\n\n        # upsampling\n        self.up_concat4 = UnetUpConcat(filters[4], filters[3], self.is_batchnorm)\n        self.up_concat3 = UnetUpConcat(filters[3], filters[2], self.is_batchnorm)\n        self.up_concat2 = UnetUpConcat(filters[2], filters[1], self.is_batchnorm)\n        self.up_concat1 = UnetUpConcat(filters[1], filters[0], self.is_batchnorm)\n\n        # final conv (without any concat)\n        self.final = Conv(filters[0], param.dataset.n_fine, 1)\n\n        self.dropout1 = nn.Dropout(p=0.3)\n        self.dropout2 = nn.Dropout(p=0.3)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        maxpool3 = self.maxpool3(conv3)\n        \n        conv4 = self.conv4(maxpool3)\n        maxpool4 = self.maxpool4(conv4)\n       \n        center = self.center(maxpool4)\n        center = self.dropout1(center)\n\n        up4 = self.up_concat4(conv4, center)\n        up3 = self.up_concat3(conv3, up4)\n        up2 = self.up_concat2(conv2, up3)\n        up1 = self.up_concat1(conv1, up2)\n        \n        outdict = dict(logit=self.final(self.dropout2(up1)), feature_map=up1)\n        return outdict", "\n"]}
{"filename": "networks/singlybranchedunet.py", "chunked_list": ["\nimport math\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import OrderedDict\nfrom networks.utils import Conv, BatchNorm, MaxPool, set_param", "from collections import OrderedDict\nfrom networks.utils import Conv, BatchNorm, MaxPool, set_param\n\nparam = None\n\n\nclass UnetConv(nn.Module):\n    def __init__(self, in_size, out_size, is_separate_batchnorm, kernel_size=None, padding_size=None, init_stride=None):\n        super(UnetConv, self).__init__()\n        \n        if kernel_size is None:\n            kernel_size = (3,) * math.floor(param.dataset.n_dim)\n            padding_size = (1,) * math.floor(param.dataset.n_dim)\n            init_stride = 1\n\n        if is_separate_batchnorm:\n            self.conv1 = nn.Sequential(OrderedDict([\n                ('conv', Conv(in_size, out_size, kernel_size, init_stride, padding_size)),\n                ('bn', BatchNorm(out_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n            if param.exp.separate_norm:\n                self.conv2 = Conv(out_size, out_size, kernel_size, init_stride, padding_size)\n            else:\n                self.conv2 = nn.Sequential(OrderedDict([\n                    ('conv', Conv(out_size, out_size, kernel_size, init_stride, padding_size)),\n                    ('bn', BatchNorm(out_size)),\n                    ('nl', nn.ReLU(inplace=True)),\n                ]))\n        else:\n            self.conv1 = nn.Sequential(OrderedDict([\n                ('conv', Conv(in_size, out_size, kernel_size, init_stride, padding_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n            self.conv2 = nn.Sequential(OrderedDict([\n                ('conv', Conv(out_size, out_size, kernel_size, init_stride, padding_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs", "\n\nclass UnetUpConcat(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm=True):\n        super(UnetUpConcat, self).__init__()\n        self.conv = UnetConv(in_size + out_size, out_size, is_batchnorm)\n        if math.floor(param.dataset.n_dim) == 3:\n            self.up = nn.Upsample(scale_factor=(2, 2, 2), mode='trilinear')\n        elif math.floor(param.dataset.n_dim) == 2:\n            self.up = nn.Upsample(scale_factor=(2, 2), mode='bilinear')\n        else:\n            self.up = None\n\n    def forward(self, inputs1, inputs2):\n        outputs2 = self.up(inputs2)\n        offset = outputs2.size()[2] - inputs1.size()[2]\n        padding = 2 * [offset // 2, offset // 2, 0]\n        outputs1 = F.pad(inputs1, padding)\n        out1 = self.conv(torch.cat([outputs1, outputs2], 1))\n        \n        return out1", "\n\nclass ConvBlock(nn.Module):\n    def __init__(self, repeat=2, *args, **kwargs):\n        super(ConvBlock, self).__init__()\n        conv_block = [Conv(*args, **kwargs) for _ in range(repeat)]\n        relu_block = [nn.ReLU(inplace=True) for _ in range(repeat)]\n        conv = [None] * (2 * repeat)\n        conv[::2] = conv_block\n        conv[1::2] = relu_block\n        self.conv = nn.Sequential(*conv)\n\n    def forward(self, inputs):\n        feat = self.conv(inputs)\n        return feat", "    \n\nclass UnetC2FOutput(nn.Module):\n    def __init__(self, in_size, repeat=1):\n        super(UnetC2FOutput, self).__init__()\n\n        if param.exp.separate_norm:\n            self.coarse_foreground = nn.Sequential(\n                BatchNorm(in_size),\n                nn.ReLU(),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * math.floor(param.dataset.n_dim), padding=(1,) * math.floor(param.dataset.n_dim)\n                ),\n            )\n            self.coarse_feat2logit = Conv(in_size, 1, 1)\n            if param.exp.priority_cat:\n                self.coarse_feat2feat = Conv(in_size + 1, param.dataset.n_fine - 1, 1)\n            else:\n                self.coarse_feat2feat = Conv(in_size, param.dataset.n_fine - 1, 1)\n            self.coarse_background = nn.Sequential(\n                BatchNorm(in_size),\n                nn.ReLU(),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * math.floor(param.dataset.n_dim), padding=(1,) * math.floor(param.dataset.n_dim)\n                ),\n                Conv(in_size, 1, 1),\n            )\n        \n        elif param.exp.priority_cat:\n            self.conv = nn.Sequential(\n                BatchNorm(in_size),\n                nn.ReLU(inplace=True),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * math.floor(param.dataset.n_dim), padding=(1,) * math.floor(param.dataset.n_dim)\n                ),\n            )\n            self.coarse = Conv(in_size, param.dataset.n_coarse, 1)\n            self.fine = Conv(in_size + param.dataset.n_coarse, param.dataset.n_fine, 1)\n\n        else:\n            self.conv = nn.Sequential(\n                BatchNorm(in_size),\n                nn.ReLU(inplace=True),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * math.floor(param.dataset.n_dim), padding=(1,) * math.floor(param.dataset.n_dim)\n                ),\n            )\n            self.coarse = Conv(in_size, param.dataset.n_coarse, 1)\n            self.fine = Conv(in_size, param.dataset.n_fine, 1)\n        \n    def forward(self, inputs):\n\n        if param.exp.separate_norm:\n            foreground = self.coarse_foreground(inputs)\n            \n            fg_logit = self.coarse_feat2logit(foreground)\n            bg_logit = self.coarse_background(inputs)\n            \n            fg_concat = torch.cat([foreground, fg_logit], dim=1)\n            if param.exp.priority_cat:\n                fine_split = self.coarse_feat2feat(fg_concat)\n            else:\n                fine_split = self.coarse_feat2feat(foreground)\n            \n            coarse = torch.cat([bg_logit, fg_logit], dim=1)\n            fine = torch.cat([bg_logit, fine_split], dim=1)\n            return {'coarse_logit': coarse, 'fine_logit': fine}\n        \n        elif param.exp.priority_cat:\n            inputs = self.conv(inputs)\n            coarse = self.coarse(inputs)\n            fine = torch.cat([inputs, coarse], dim=1)\n            fine = self.fine(fine)\n            return {'coarse_logit': coarse, 'fine_logit': fine}\n        \n        else:\n            inputs = self.conv(inputs)\n            coarse = self.coarse(inputs)\n            fine = self.fine(inputs)\n            return {'coarse_logit': coarse, 'fine_logit': fine}", "        \n\nclass UNetSingleBranchNetwork(nn.Module):\n\n    def __init__(self, parameter, repeat=1):\n        super(UNetSingleBranchNetwork, self).__init__()\n        global param\n        param = parameter\n        set_param(parameter)\n        self.in_channels = param.dataset.n_mode\n        self.is_batchnorm = param.network.is_batchnorm\n        self.feature_scale = param.network.feature_scale\n\n        filters = [param.network.base_feature_num * self.feature_scale ** x for x in range(5)]\n\n        # downsampling\n        self.conv1 = UnetConv(self.in_channels, filters[0], self.is_batchnorm)\n        self.maxpool1 = MaxPool(kernel_size=(2,) * math.floor(param.dataset.n_dim))\n        \n        self.conv2 = UnetConv(filters[0], filters[1], self.is_batchnorm)\n        self.maxpool2 = MaxPool(kernel_size=(2,) * math.floor(param.dataset.n_dim))\n        \n        self.conv3 = UnetConv(filters[1], filters[2], self.is_batchnorm)\n        self.maxpool3 = MaxPool(kernel_size=(2,) * math.floor(param.dataset.n_dim))\n        \n        self.conv4 = UnetConv(filters[2], filters[3], self.is_batchnorm)\n        self.maxpool4 = MaxPool(kernel_size=(2,) * math.floor(param.dataset.n_dim))\n\n        self.center = UnetConv(filters[3], filters[4], self.is_batchnorm)\n\n        # upsampling\n        self.up_concat4 = UnetUpConcat(filters[4], filters[3], self.is_batchnorm)\n        self.up_concat3 = UnetUpConcat(filters[3], filters[2], self.is_batchnorm)\n        self.up_concat2 = UnetUpConcat(filters[2], filters[1], self.is_batchnorm)\n        self.up_concat1 = UnetUpConcat(filters[1], filters[0], self.is_batchnorm)\n\n        # final conv (without any concat)\n        self.final = UnetC2FOutput(filters[0], repeat=repeat)\n\n        self.dropout1 = nn.Dropout(p=0.3)\n        self.dropout2 = nn.Dropout(p=0.3)\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        maxpool3 = self.maxpool3(conv3)\n        \n        conv4 = self.conv4(maxpool3)\n        maxpool4 = self.maxpool4(conv4)\n       \n        center = self.center(maxpool4)\n        center = self.dropout1(center)\n\n        up4 = self.up_concat4(conv4, center)\n        up3 = self.up_concat3(conv3, up4)\n        up2 = self.up_concat2(conv2, up3)\n        up1 = self.up_concat1(conv1, up2)\n        \n        outdict = self.final(self.dropout2(up1))\n        outdict.update({'feature_map': up1})\n        return outdict\n    \n    @torch.no_grad()\n    def gen_pseudo_labels(self, q_im, q_soft, q_lc, threshold=0.4):\n        rot_angle = random.randint(0, 3)\n        flip_axis = random.randint(2, 3)\n        \n        k_im = torch.flip(torch.rot90(q_im, k=rot_angle, dims=(2, 3)), dims=(flip_axis,))\n        \n        k_out = self.forward(k_im)\n        k_soft = torch.softmax(k_out['fine_logit'], dim=1)\n        k_soft = torch.rot90(torch.flip(k_soft, dims=(flip_axis,)), k=-rot_angle, dims=(2, 3))\n        \n        # one-hot label\n        pseudo_label = torch.zeros(q_soft.shape, dtype=torch.float32, device=q_soft.device)\n        for i_label in range(param.dataset.n_fine):\n            i_ = 0 if i_label == 0 else 1\n            pseudo_label[:, i_label] = (k_soft[:, i_label] > threshold) & (q_soft[:, i_label] > threshold) & (q_lc == i_)\n\n        return pseudo_label\n    \n    @torch.no_grad()\n    def gen_mixup_labels(self, q_im, q_lc, q_soft, mixed_im, mixed_lf, alpha, threshold=0.4, with_pseudo_label=True):\n        \n        if math.floor(param.dataset.n_dim) == 3:\n            mixed_lf = F.one_hot(mixed_lf, param.dataset.n_fine).permute(0, 4, 1, 2, 3)\n        elif math.floor(param.dataset.n_dim) == 2:\n            mixed_lf = F.one_hot(mixed_lf, param.dataset.n_fine).permute(0, 3, 1, 2)\n        \n        mixed_label = torch.zeros(mixed_lf.size(), device=mixed_lf.device, dtype=torch.float32)\n         \n        if with_pseudo_label:\n            pseudo_label = self.gen_pseudo_labels(q_im, q_soft, q_lc, threshold)\n            for i_batch in range(pseudo_label.size(0)):\n                mixed_label[i_batch] = pseudo_label[i_batch] * (1 - alpha[i_batch]) + mixed_lf[i_batch] * alpha[i_batch]\n        else:\n            for i_batch in range(q_im.size(0)):\n                mixed_label[i_batch] = mixed_lf[i_batch] * alpha[i_batch]\n            \n        mixed_pred = self.forward(mixed_im)['fine_logit']\n        return mixed_pred, mixed_label", "    \n"]}
{"filename": "networks/multiplebranchedunet.py", "chunked_list": ["import math\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom networks.utils import *\nfrom collections import OrderedDict\n", "from collections import OrderedDict\n\nparam = None\n\n\nclass ForegroundBranch(nn.Module):\n    def __init__(self, in_size, repeat, index):\n        super(ForegroundBranch).__init__()\n        self.foreground_transformer = nn.Sequential(OrderedDict([\n            ('trans', BatchNorm(in_size)),\n            ('actv', nn.ReLU()),\n            ('ext', ConvBlock(repeat=repeat,\n                              in_channels=in_size, out_channels=in_size,\n                              kernel_size=(3,)*math.floor(param.dataset.n_dim), padding=(1,)*math.floor(param.dataset.n_dim)))\n        ]))\n        self.coarse_classifier = Conv(in_size, 1, 1)\n        self.fine_classifier = Conv(in_size + param.exp.priority_cat, len(param.dataset.mapping[index]), 1)\n        \n    def forward(self, inputs):\n        out = self.foreground_transformer(inputs)\n        coarse_logit = self.coarse_classifier(out)\n        if param.exp.priority_cat:\n            fine_logit = self.fine_classifier(torch.cat([coarse_logit, out], dim=1))\n        else:\n            fine_logit = self.fine_classifier(out)\n        return coarse_logit, fine_logit", "    \n\nclass UNetMultiBranchClassifier(nn.Module):\n    def __init__(self, in_size, repeat=1):\n        super(UNetMultiBranchClassifier, self).__init__()\n        self.foreground_branches = nn.ModuleDict()\n    \n        if param.exp.separate_norm:\n            for c in range(1, param.dataset.n_coarse):\n                self.foreground_branches[c] = ForegroundBranch(in_size, repeat, c)\n\n            self.background_branch = nn.Sequential(OrderedDict([\n                ('trans', BatchNorm(in_size)),\n                ('actv', nn.ReLU()),\n                ('ext', ConvBlock(repeat=repeat,\n                                in_channels=in_size, out_channels=in_size,\n                                kernel_size=(3,)*math.floor(param.dataset.n_dim), padding=(1,)*math.floor(param.dataset.n_dim))),\n                ('classifier', Conv(in_size, 1, 1))\n            ]))\n        \n        else:\n            self.conv = nn.Sequential(OrderedDict([\n                ('trans', BatchNorm(in_size)),\n                ('actv', nn.ReLU()),\n                ('ext', ConvBlock(repeat=repeat,\n                                in_channels=in_size, out_channels=in_size,\n                                kernel_size=(3,)*math.floor(param.dataset.n_dim), padding=(1,)*math.floor(param.dataset.n_dim)))\n            ]))\n            \n            self.coarse = Conv(in_size, param.dataset.n_coarse, 1)\n            if param.exp.priority_cat: self.fine = Conv(in_size + param.dataset.n_coarse, param.dataset.n_fine, 1)\n            else: self.fine = Conv(in_size, param.dataset.n_fine, 1)\n        \n    def forward(self, inputs):\n\n        if param.exp.separate_norm:\n            all_coarse_logits = torch.zeros((inputs.size(0), param.dataset.n_coarse) + inputs.shape[2:],\n                                            device=inputs.device, dtype=torch.float32)\n            all_fine_logits = torch.zeros((inputs.size(0), param.dataset.n_fine) + inputs.shape[2:],\n                                          device=inputs.device, dtype=torch.float32)\n            \n            for c in range(1, param.dataset.n_coarse):\n                coarse_logit, fine_logit = self.foreground_branches[c](inputs)\n                all_coarse_logits[:, c] = coarse_logit\n                all_fine_logits[:, slice(*param.dataset.mapping[c])] = fine_logit\n                \n            bg_logit = self.background_branch(inputs)\n            all_coarse_logits[:, 0] = all_fine_logits[:, 0] = bg_logit\n        \n        else:\n            inputs = self.conv(inputs)\n            all_coarse_logits = self.coarse(inputs)\n            if param.exp.priority_cat: all_fine_logits = self.fine(torch.cat([all_coarse_logits, inputs]), dim=1)\n            else: all_fine_logits = self.fine(inputs)\n                \n        return {'coarse_logit': all_coarse_logits, 'fine_logit': all_fine_logits}", "        \n\nclass UNetMultiBranchNetwork(nn.Module):\n\n    def __init__(self, parameter):\n        super(UNetMultiBranchNetwork, self).__init__()\n        global param\n        param = parameter\n        set_param(parameter)\n\n        filters = [param.network.base_feature_num * param.network.feature_scale ** x for x in range(5)]\n\n        # encoder\n        self.encoder = UNetEncoder(\n            in_chns=param.dataset.n_mode,\n            feature_num=filters,\n            layer_num=5\n        )\n        # decoder\n        self.decoder = UNetDecoder(\n            feature_num=filters,\n            layer_num=5,\n            output_feature_map=True,\n            trunc_at_feature_map=True\n        )\n        # coarse to fine classification head\n        self.drop = nn.Dropout(0.3)\n        self.classifier = UNetMultiBranchClassifier(filters[0], repeat=1)\n\n    def forward(self, inputs):\n        embed, skip = self.encoder(inputs)\n        feat_map = self.decoder(embed, skip)\n        drop_feat_map = self.drop(feat_map)\n        outdict = self.classifier(drop_feat_map)\n        outdict.update({'feature_map': feat_map})\n        return outdict\n    \n    @torch.no_grad()\n    def gen_pseudo_labels(self, q_im, q_soft, q_lc, threshold=0.4):\n        rot_angle = random.randint(0, 3)\n        flip_axis = random.randint(2, 3)\n        \n        k_im = torch.flip(torch.rot90(q_im, k=rot_angle, dims=(2, 3)), dims=(flip_axis,))\n        \n        k_out = self.forward(k_im)\n        k_soft = torch.softmax(k_out['fine_logit'], dim=1)\n        k_soft = torch.rot90(torch.flip(k_soft, dims=(flip_axis,)), k=-rot_angle, dims=(2, 3))\n        \n        # one-hot label\n        pseudo_label = torch.zeros(q_soft.shape, dtype=torch.float32, device=q_soft.device)\n        for i_label in range(param.dataset.n_fine):\n            i_ = 0 if i_label == 0 else 1\n            pseudo_label[:, i_label] = (k_soft[:, i_label] > threshold) & (q_soft[:, i_label] > threshold) & (q_lc == i_)\n\n        return pseudo_label\n    \n    @torch.no_grad()\n    def gen_mixup_labels(self, q_im, q_lc, q_soft, mixed_im, mixed_lf, alpha, threshold=0.4, with_pseudo_label=True):\n        \n        if math.floor(param.dataset.n_dim) == 3:\n            mixed_lf = F.one_hot(mixed_lf, param.dataset.n_fine).permute(0, 4, 1, 2, 3)\n        elif math.floor(param.dataset.n_dim) == 2:\n            mixed_lf = F.one_hot(mixed_lf, param.dataset.n_fine).permute(0, 3, 1, 2)\n        \n        mixed_label = torch.zeros(mixed_lf.size(), device=mixed_lf.device, dtype=torch.float32)\n        \n        if with_pseudo_label:\n            pseudo_label = self.gen_pseudo_labels(q_im, q_soft, q_lc, threshold)\n            for i_batch in range(pseudo_label.size(0)):\n                mixed_label[i_batch] = pseudo_label[i_batch] * (1 - alpha[i_batch]) + mixed_lf[i_batch] * alpha[i_batch]\n        else:\n            for i_batch in range(q_im.size(0)):\n                mixed_label[i_batch] = mixed_lf[i_batch] * alpha[i_batch]\n            \n        mixed_pred = self.forward(mixed_im)['fine_logit']\n        return mixed_pred, mixed_label"]}
{"filename": "networks/utils.py", "chunked_list": ["import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\nfrom torch.nn import init\nfrom collections import OrderedDict\n\nparam = None\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        init.normal(m.weight.data, 0.0, 0.02)\n    elif classname.find('Linear') != -1:\n        init.normal(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        init.normal(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)", "\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        init.normal(m.weight.data, 0.0, 0.02)\n    elif classname.find('Linear') != -1:\n        init.normal(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        init.normal(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)", "\n\ndef weights_init_xavier(m):\n    classname = m.__class__.__name__\n    if classname.find('ConvNd') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.xavier_normal(m.weight.data, gain=1)\n    elif classname.find('BatchNormNd') != -1:\n        init.normal(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)", "\n\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    if classname.find('ConvNd') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('Linear') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n    elif classname.find('BatchNormNd') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)", "\n\ndef weights_init_orthogonal(m):\n    classname = m.__class__.__name__\n    if classname.find('ConvNd') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n    elif classname.find('Linear') != -1:\n        init.orthogonal(m.weight.data, gain=1)\n    elif classname.find('BatchNormNd') != -1:\n        init.normal(m.weight.data, 1.0, 0.02)\n        init.constant(m.bias.data, 0.0)", "\n\ndef init_weights(net, init_type='kaiming'):\n    #print('initialization method [%s]' % init_type)\n    if init_type == 'normal':\n        net.apply(weights_init_normal)\n    elif init_type == 'xavier':\n        net.apply(weights_init_xavier)\n    elif init_type == 'kaiming':\n        net.apply(weights_init_kaiming)\n    elif init_type == 'orthogonal':\n        net.apply(weights_init_orthogonal)\n    else:\n        raise NotImplementedError(f'initialization method {init_type} is not implemented')", "    \n    \nclass Conv(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(Conv, self).__init__()\n        self.conv = None\n        if math.floor(param.dataset.n_dim) == 2:\n            self.conv = nn.Conv2d(*args, **kwargs)\n        elif math.floor(param.dataset.n_dim) == 3:\n            self.conv = nn.Conv3d(*args, **kwargs)\n        self.weight = self.conv.weight\n        self.bias = self.conv.bias\n    \n    def forward(self, inputs):\n        return self.conv(inputs)", "\n\nclass MaxPool(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(MaxPool, self).__init__()\n        self.maxpool = None\n        if math.floor(param.dataset.n_dim) == 2:\n            self.maxpool = nn.MaxPool2d(*args, **kwargs)\n        elif math.floor(param.dataset.n_dim) == 3:\n            self.maxpool = nn.MaxPool3d(*args, **kwargs)\n    \n    def forward(self, inputs):\n        return self.maxpool(inputs)", "\n\nclass BatchNorm(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super(BatchNorm, self).__init__()\n        self.norm = None\n        if math.floor(param.dataset.n_dim) == 2:\n            self.norm = nn.BatchNorm2d(*args, **kwargs)\n        elif math.floor(param.dataset.n_dim) == 3:\n            self.norm = nn.BatchNorm3d(*args, **kwargs)\n        self.weight = self.norm.weight\n        self.bias = self.norm.bias\n    \n    def forward(self, inputs):\n        return self.norm(inputs)", "    \n    \nclass ConvBlock(nn.Module):\n    def __init__(self, repeat=1, **kwargs):\n        super(ConvBlock, self).__init__()\n        self.repeat = repeat\n        self.conv = nn.ModuleList()\n        for _ in range(self.repeat):\n            self.conv.append(nn.Sequential(OrderedDict([\n                ('conv', Conv(**kwargs)),\n                ('actv', nn.ReLU())\n            ])))\n        \n    def forward(self, inputs):\n        for rep in range(self.repeat):\n            inputs = self.conv[rep](inputs)\n        return inputs", "    \n    \nclass UNetEncoderStep(nn.Module):\n    def __init__(self, in_chns, out_chns, kernel_size, stride_size, padding_size, ds=True):\n        super(UNetEncoderStep, self).__init__()\n        if isinstance(kernel_size, int): kernel_size = (kernel_size,) * math.floor(param.dataset.n_dim)\n        if isinstance(padding_size, int): padding_size = (padding_size,) * math.floor(param.dataset.n_dim)\n        self.convs = nn.Sequential(OrderedDict([\n            ('conv1', Conv(in_chns, out_chns, kernel_size, stride_size, padding_size)),\n            ('norm1', BatchNorm(out_chns)),\n            ('actv1', nn.ReLU()),\n            ('conv2', Conv(out_chns, out_chns, kernel_size, stride_size, padding_size)),\n            ('norm2', BatchNorm(out_chns)),\n            ('actv2', nn.ReLU()),\n        ]))\n        self.down = MaxPool(kernel_size=(param.network.image_scale,) * math.floor(param.dataset.n_dim))\n        self.down_sampling = ds\n        \n    def forward(self, inputs):\n        conv = self.convs(inputs)\n        if self.down_sampling:\n            out = self.down(conv)\n        return conv, out", "\n\nclass UnetDecoderStep(nn.Module):\n    def __init__(self, in_chns, out_chns, kernel_size, stride_size, padding_size):\n        super(UnetDecoderStep, self).__init__()\n        if isinstance(kernel_size, int): kernel_size = (kernel_size,) * math.floor(param.dataset.n_dim)\n        if isinstance(padding_size, int): padding_size = (padding_size,) * math.floor(param.dataset.n_dim)\n        self.up = nn.Upsample(\n            scale_factor=(param.network.image_scale,) * math.floor(param.dataset.n_dim),\n            mode='trilinear' if math.floor(param.dataset.n_dim) == 3 else 'bilinear', align_corners=False)\n        self.convs = nn.Sequential(OrderedDict([\n            ('conv1', Conv(in_chns + out_chns, out_chns, kernel_size, stride_size, padding_size)),\n            ('norm1', BatchNorm(out_chns)),\n            ('actv1', nn.ReLU()),\n            ('conv2', Conv(out_chns, out_chns, kernel_size, stride_size, padding_size)),\n            ('norm2', BatchNorm(out_chns)),\n            ('actv2', nn.ReLU()),\n        ]))\n        \n    def forward(self, inputs, skip):\n        inputs = self.up(inputs)\n        offset = inputs.size(2) - skip.size(2)\n        padding = 2 * math.floor(param.dataset.n_dim) * [offset // 2]\n        skip = f.pad(skip, padding)\n        out = torch.cat([skip, inputs], dim=1)\n        out = self.convs(out)\n        return out", "    \n    \nclass UNetEncoder(nn.Module):\n    default_params = dict(\n        in_chns=3,\n        kernel_size=3, padding=1, stride=1,\n        feature_num=(16, 32, 64, 128, 256), layer_num=5\n    )\n    \n    def __init__(self, **kwargs):\n        super(UNetEncoder, self).__init__()\n        self.encoder_steps = nn.ModuleDict()\n        self.stride = kwargs.get('stride', self.default_params['stride'])\n        self.in_chns = kwargs.get('in_chns', self.default_params['in_chns'])\n        self.padding = kwargs.get('padding', self.default_params['padding'])\n        self.layer_num = kwargs.get('layer_num', self.default_params['layer_num'])\n        self.kernel_size = kwargs.get('kernel_size', self.default_params['kernel_size'])\n        self.feature_num = kwargs.get('feature_num', self.default_params['feature_num'])\n        \n        assert len(self.feature_num) == self.layer_num\n        self.feature_num = [self.in_chns] + self.feature_num\n         \n        for layer in range(self.layer_num - 1):\n            self.encoder_steps[f\"{layer}\"] = UNetEncoderStep(\n                self.feature_num[layer],\n                self.feature_num[layer+1], \n                self.kernel_size, self.stride, self.padding\n            )\n        self.center = Conv(self.feature_num[-2], self.feature_num[-1], self.kernel_size, self.stride, self.padding)\n        self.dropout = nn.Dropout(0.3)\n    \n    def forward(self, inputs):\n        skips = []\n        for layer in range(self.layer_num - 1):\n            skip, inputs = self.encoder_steps[f\"{layer}\"](inputs)\n            skips.append(skip)\n        embedding = self.center(inputs)\n        embedding = self.dropout(embedding)\n        return embedding, skips", "    \n    \nclass UNetDecoder(nn.Module):\n    default_params = dict(\n        out_chns=4,\n        kernel_size=3, padding=1, stride=1,\n        feature_num=(16, 32, 64, 128, 256), layer_num=5, \n        output_feature_map = True,\n        trunc_at_feature_map = False\n    )\n    \n    def __init__(self, **kwargs):\n        super(UNetDecoder, self).__init__()\n        self.decoder_steps = nn.ModuleDict()\n        self.stride = kwargs.get('stride', self.default_params['stride'])\n        self.out_chns = kwargs.get('out_chns', self.default_params['out_chns'])\n        self.padding = kwargs.get('padding', self.default_params['padding'])\n        self.layer_num = kwargs.get('layer_num', self.default_params['layer_num'])\n        self.kernel_size = kwargs.get('kernel_size', self.default_params['kernel_size'])\n        self.feature_num = kwargs.get('feature_num', self.default_params['feature_num'])\n        self.output_feature_map = kwargs.get('output_feature_map', self.default_params['output_feature_map'])\n        self.trunc_at_feature_map = kwargs.get('trunc_at_feature_map', self.default_params['trunc_at_feature_map'])\n        \n        assert len(self.feature_num) == self.layer_num\n        self.feature_num = self.feature_num[::-1] + [self.out_chns]\n         \n        for layer in range(self.layer_num-1):\n            self.decoder_steps[f\"{layer}\"] = UnetDecoderStep(\n                self.feature_num[layer],\n                self.feature_num[layer+1],\n                self.kernel_size, self.stride, self.padding\n            )\n        if not self.trunc_at_feature_map:\n            self.classifier = nn.Conv3d(self.feature_num[-2], self.feature_num[-1], 1)\n        self.dropout = nn.Dropout(0.3)\n    \n    def forward(self, inputs, skip):\n        for layer in range(self.layer_num - 1):\n            inputs = self.decoder_steps[f\"{layer}\"](inputs, skip[self.layer_num - 2 - layer])\n        out = self.dropout(inputs)\n        if self.trunc_at_feature_map:\n            return inputs\n\n        out = self.classifier(out)\n        if self.output_feature_map:\n            return out, inputs\n        return out", "\n\ndef set_param(parameter):\n    global param\n    param = parameter"]}
{"filename": "networks/proposed.py", "chunked_list": ["\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom collections import defaultdict, OrderedDict\nfrom networks.utils import init_weights, ConvNd, BatchNormNd, MaxPoolNd, set_param\n", "from networks.utils import init_weights, ConvNd, BatchNormNd, MaxPoolNd, set_param\n\nparam = None\n\n\nclass UnetConv(nn.Module):\n    def __init__(self, in_size, out_size, is_separate_batchnorm, kernel_size=None, padding_size=None, init_stride=None):\n        super(UnetConv, self).__init__()\n        \n        if kernel_size is None:\n            kernel_size = (3,) * param.dataset.n_dim\n            padding_size = (1,) * param.dataset.n_dim\n            init_stride = 1\n\n        if is_separate_batchnorm:\n            self.conv1 = nn.Sequential(OrderedDict([\n                ('conv', ConvNd(in_size, out_size, kernel_size, init_stride, padding_size)),\n                ('bn', BatchNormNd(out_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n            if param.exp.separate_norm:\n                self.conv2 = ConvNd(out_size, out_size, kernel_size, init_stride, padding_size)\n            else:\n                self.conv2 = nn.Sequential(OrderedDict([\n                    ('conv', ConvNd(out_size, out_size, kernel_size, init_stride, padding_size)),\n                    ('bn', BatchNormNd(out_size)),\n                    ('nl', nn.ReLU(inplace=True)),\n                ]))\n        else:\n            self.conv1 = nn.Sequential(OrderedDict([\n                ('conv', ConvNd(in_size, out_size, kernel_size, init_stride, padding_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n            self.conv2 = nn.Sequential(OrderedDict([\n                ('conv', ConvNd(out_size, out_size, kernel_size, init_stride, padding_size)),\n                ('nl', nn.ReLU(inplace=True)),\n            ]))\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs", "\n\nclass UnetUpConcat(nn.Module):\n    def __init__(self, in_size, out_size, is_batchnorm=True):\n        super(UnetUpConcat, self).__init__()\n        self.conv = UnetConv(in_size + out_size, out_size, is_batchnorm)\n        if param.dataset.n_dim == 3:\n            self.up = nn.Upsample(scale_factor=(2, 2, 2), mode='trilinear')\n        elif param.dataset.n_dim == 2:\n            self.up = nn.Upsample(scale_factor=(2, 2), mode='bilinear')\n        else:\n            self.up = None\n\n        # initialise the blocks\n        for m in self.children():\n            if m.__class__.__name__.find('UnetConv') != -1: continue\n            init_weights(m, init_type='kaiming')\n\n    def forward(self, inputs1, inputs2):\n        outputs2 = self.up(inputs2)\n        offset = outputs2.size()[2] - inputs1.size()[2]\n        padding = 2 * [offset // 2, offset // 2, 0]\n        outputs1 = F.pad(inputs1, padding)\n        out1 = self.conv(torch.cat([outputs1, outputs2], 1))\n        \n        return out1", "\n\nclass ConvBlock(nn.Module):\n    def __init__(self, repeat=2, *args, **kwargs):\n        super(ConvBlock, self).__init__()\n        conv_block = [ConvNd(*args, **kwargs) for _ in range(repeat)]\n        relu_block = [nn.ReLU(inplace=True) for _ in range(repeat)]\n        conv = [None] * (2 * repeat)\n        conv[::2] = conv_block\n        conv[1::2] = relu_block\n        self.conv = nn.Sequential(*conv)\n\n    def forward(self, inputs):\n        feat = self.conv(inputs)\n        return feat", "    \n\nclass UnetC2FOutput(nn.Module):\n    def __init__(self, in_size, repeat=1):\n        super(UnetC2FOutput, self).__init__()\n\n        if param.exp.separate_norm:\n            self.coarse_foreground = nn.Sequential(\n                BatchNormNd(in_size),\n                nn.ReLU(),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * param.dataset.n_dim, padding=(1,) * param.dataset.n_dim\n                ),\n            )\n            self.coarse_feat2logit = ConvNd(in_size, 1, 1)\n            if param.exp.priority_cat:\n                self.coarse_feat2feat = ConvNd(in_size + 1, param.dataset.n_fine - 1, 1)\n            else:\n                self.coarse_feat2feat = ConvNd(in_size, param.dataset.n_fine - 1, 1)\n            self.coarse_background = nn.Sequential(\n                BatchNormNd(in_size),\n                nn.ReLU(),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * param.dataset.n_dim, padding=(1,) * param.dataset.n_dim\n                ),\n                ConvNd(in_size, 1, 1),\n            )\n        \n        elif param.exp.priority_cat:\n            self.conv = nn.Sequential(\n                BatchNormNd(in_size),\n                nn.ReLU(inplace=True),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * param.dataset.n_dim, padding=(1,) * param.dataset.n_dim\n                ),\n            )\n            self.coarse = ConvNd(in_size, param.dataset.n_coarse, 1)\n            self.fine = ConvNd(in_size + param.dataset.n_coarse, param.dataset.n_fine, 1)\n\n        else:\n            self.conv = nn.Sequential(\n                BatchNormNd(in_size),\n                nn.ReLU(inplace=True),\n                ConvBlock(\n                    repeat=repeat, in_channels=in_size,\n                    out_channels=in_size, kernel_size=(3,) * param.dataset.n_dim, padding=(1,) * param.dataset.n_dim\n                ),\n            )\n            self.coarse = ConvNd(in_size, param.dataset.n_coarse, 1)\n            self.fine = ConvNd(in_size, param.dataset.n_fine, 1)\n        \n    def forward(self, inputs):\n\n        if param.exp.separate_norm:\n            foreground = self.coarse_foreground(inputs)\n            \n            fg_logit = self.coarse_feat2logit(foreground)\n            bg_logit = self.coarse_background(inputs)\n            \n            fg_concat = torch.cat([foreground, fg_logit], dim=1)\n            if param.exp.priority_cat:\n                fine_split = self.coarse_feat2feat(fg_concat)\n            else:\n                fine_split = self.coarse_feat2feat(foreground)\n            \n            coarse = torch.cat([bg_logit, fg_logit], dim=1)\n            fine = torch.cat([bg_logit, fine_split], dim=1)\n            return {'coarse_logit': coarse, 'fine_logit': fine}\n        \n        elif param.exp.priority_cat:\n            inputs = self.conv(inputs)\n            coarse = self.coarse(inputs)\n            fine = torch.cat([inputs, coarse], dim=1)\n            fine = self.fine(fine)\n            return {'coarse_logit': coarse, 'fine_logit': fine}\n        \n        else:\n            inputs = self.conv(inputs)\n            coarse = self.coarse(inputs)\n            fine = self.fine(inputs)\n            return {'coarse_logit': coarse, 'fine_logit': fine}", "        \n\nclass UNetBasedNetwork(nn.Module):\n\n    def __init__(self, parameter):\n        super(UNetBasedNetwork, self).__init__()\n        global param\n        param = parameter\n        set_param(parameter)\n        self.in_channels = param.dataset.n_mode\n        self.is_batchnorm = param.network.is_batchnorm\n        self.feature_scale = param.network.feature_scale\n\n        filters = [param.network.base_feature_num * self.feature_scale ** x for x in range(5)]\n\n        # downsampling\n        self.conv1 = UnetConv(self.in_channels, filters[0], self.is_batchnorm)\n        self.maxpool1 = MaxPoolNd(kernel_size=(2,) * param.dataset.n_dim)\n        \n        self.conv2 = UnetConv(filters[0], filters[1], self.is_batchnorm)\n        self.maxpool2 = MaxPoolNd(kernel_size=(2,) * param.dataset.n_dim)\n        \n        self.conv3 = UnetConv(filters[1], filters[2], self.is_batchnorm)\n        self.maxpool3 = MaxPoolNd(kernel_size=(2,) * param.dataset.n_dim)\n        \n        self.conv4 = UnetConv(filters[2], filters[3], self.is_batchnorm)\n        self.maxpool4 = MaxPoolNd(kernel_size=(2,) * param.dataset.n_dim)\n\n        self.center = UnetConv(filters[3], filters[4], self.is_batchnorm)\n\n        # upsampling\n        self.up_concat4 = UnetUpConcat(filters[4], filters[3], self.is_batchnorm)\n        self.up_concat3 = UnetUpConcat(filters[3], filters[2], self.is_batchnorm)\n        self.up_concat2 = UnetUpConcat(filters[2], filters[1], self.is_batchnorm)\n        self.up_concat1 = UnetUpConcat(filters[1], filters[0], self.is_batchnorm)\n\n        # final conv (without any concat)\n        self.final = UnetC2FOutput(filters[0], repeat=1)\n\n        self.dropout1 = nn.Dropout(p=0.3)\n        self.dropout2 = nn.Dropout(p=0.3)\n\n        # initialize weights\n        for m in self.modules():\n            if isinstance(m, ConvNd):\n                init_weights(m.conv, init_type='kaiming')\n            elif isinstance(m, BatchNormNd):\n                init_weights(m.norm, init_type='kaiming')\n\n    def forward(self, inputs):\n        conv1 = self.conv1(inputs)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.conv2(maxpool1)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.conv3(maxpool2)\n        maxpool3 = self.maxpool3(conv3)\n        \n        conv4 = self.conv4(maxpool3)\n        maxpool4 = self.maxpool4(conv4)\n       \n        center = self.center(maxpool4)\n        center = self.dropout1(center)\n\n        up4 = self.up_concat4(conv4, center)\n        up3 = self.up_concat3(conv3, up4)\n        up2 = self.up_concat2(conv2, up3)\n        up1 = self.up_concat1(conv1, up2)\n        \n        outdict = self.final(self.dropout2(up1))\n        outdict.update({'feature_map': up1})\n        return outdict\n    \n    @torch.no_grad()\n    def gen_pseudo_labels(self, q_im, q_soft, q_lc, threshold=0.4):\n        rot_angle = random.randint(0, 3)\n        flip_axis = random.randint(2, 3)\n        \n        k_im = torch.flip(torch.rot90(q_im, k=rot_angle, dims=(2, 3)), dims=(flip_axis,))\n        \n        k_out = self.forward(k_im)\n        k_soft = torch.softmax(k_out['fine_logit'], dim=1)\n        k_soft = torch.rot90(torch.flip(k_soft, dims=(flip_axis,)), k=-rot_angle, dims=(2, 3))\n        \n        # one-hot label\n        pseudo_label = torch.zeros(q_soft.shape, dtype=torch.float32, device=q_soft.device)\n        for i_label in range(param.dataset.n_fine):\n            i_ = 0 if i_label == 0 else 1\n            pseudo_label[:, i_label] = (k_soft[:, i_label] > threshold) & (q_soft[:, i_label] > threshold) & (q_lc == i_)\n\n        return pseudo_label\n    \n    @torch.no_grad()\n    def gen_mixup_labels(self, q_im, q_lc, q_soft, mixed_im, mixed_lf, alpha, threshold=0.4, with_pseudo_label=True):\n        \n        if param.dataset.n_dim == 3:\n            mixed_lf = F.one_hot(mixed_lf, param.dataset.n_fine).permute(0, 4, 1, 2, 3)\n        elif param.dataset.n_dim == 2:\n            mixed_lf = F.one_hot(mixed_lf, param.dataset.n_fine).permute(0, 3, 1, 2)\n        \n        mixed_label = torch.zeros(mixed_lf.size(), device=mixed_lf.device, dtype=torch.float32)\n        \n        if with_pseudo_label:\n            pseudo_label = self.gen_pseudo_labels(q_im, q_soft, q_lc, threshold)\n            for i_batch in range(pseudo_label.size(0)):\n                mixed_label[i_batch] = pseudo_label[i_batch] * (1 - alpha[i_batch]) + mixed_lf[i_batch] * alpha[i_batch]\n        else:\n            for i_batch in range(pseudo_label.size(0)):\n                mixed_label[i_batch] = mixed_lf[i_batch] * alpha[i_batch]\n            \n        mixed_pred = self.forward(mixed_im)['fine_logit']\n        return mixed_pred, mixed_label", "    \n"]}
{"filename": "trainutils/train_cross_pseudo_supervision.py", "chunked_list": ["import os\nimport sys\nimport math\nimport random\nimport shutil\nimport logging\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch", "\nimport torch\nfrom torchvision import transforms\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\nfrom val import test_single_case\nfrom utils import losses\nfrom utils.ramps import sigmoid_rampup", "from utils import losses\nfrom utils.ramps import sigmoid_rampup\nfrom utils.visualize import make_curve, make_image\nfrom dataloaders.utils import TwoStreamBatchSampler\n\n\nargs = None\n\n\ndef get_current_consistency_weight(epoch):\n    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n    return args.consistency * sigmoid_rampup(epoch, args.consistency_rampup)", "\ndef get_current_consistency_weight(epoch):\n    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n    return args.consistency * sigmoid_rampup(epoch, args.consistency_rampup)\n\n\ndef update_ema_variables(model, ema_model, alpha, global_step):\n    # Use the true average until the exponential average is more correct\n    alpha = min(1 - 1 / (global_step + 1), alpha)\n    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)", "\n\ndef train_cps(models, optimizers, param, parsed_args):\n    global args\n    args = parsed_args\n    \n    model1, model2 = models\n    optimizer1, optimizer2 = optimizers\n    \n    base_lr = param.exp.base_lr\n    batch_size = param.exp.batch_size\n    max_iterations = param.exp.max_iter\n    \n    best_performance1 = 0.0\n    best_performance2 = 0.0\n    iter_num = 0\n    loss = {}\n    \n    model1.train()\n    model2.train()\n\n    db_train = param.get_dataset(split='train')\n    db_val = param.get_dataset(split='val')\n\n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    labeled_idxs = db_train.labeled_idxs\n    unlabeled_idxs = db_train.unlabeled_idxs\n    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-param.exp.labeled_batch_size)\n    \n    trainloader = DataLoader(db_train,\n                             num_workers=4,\n                             pin_memory=True,\n                             batch_sampler=batch_sampler,\n                             worker_init_fn=worker_init_fn)\n    valloader = DataLoader(db_val, num_workers=args.val_bs, batch_size=args.val_bs)\n\n    ce_loss = CrossEntropyLoss()\n    nce_loss = losses.NegativeCrossEntropyLoss()\n    dice_loss_coarse = losses.DiceLoss(param.dataset.n_coarse)\n    dice_loss_fine = losses.DiceLoss(param.dataset.n_fine) \n\n    writer = SummaryWriter(os.path.join(param.path.path_to_snapshot, \"log\"))\n    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n\n    max_epoch = (max_iterations - iter_num) // (len(trainloader))\n    iterator = tqdm(range(max_epoch), ncols=100, position=0, leave=True, desc='Training Progress')\n    torch.autograd.set_detect_anomaly(True)\n    \n    for epoch_num in iterator:\n        for i_batch, sampled_batch in enumerate(trainloader):\n\n            q_im, q_lc, q_lf = sampled_batch['image'], sampled_batch['coarse'], sampled_batch['fine']\n\n            if args.gpu >= 0:\n                q_im, q_lc, q_lf = q_im.cuda(args.gpu), q_lc.cuda(args.gpu), q_lf.cuda(args.gpu)\n            else: raise RuntimeError(f'Specify a positive gpu id')\n\n            out1 = model1(q_im)\n            out_coarse1, out_fine1 = out1['coarse_logit'], out1['fine_logit']\n            soft_coarse1, soft_fine1 = torch.softmax(out_coarse1, dim=1), torch.softmax(out_fine1, dim=1)\n            pred_fine1 = torch.argmax(soft_fine1, dim=1)\n\n            out2 = model2(q_im)\n            out_coarse2, out_fine2 = out2['coarse_logit'], out2['fine_logit']\n            soft_coarse2, soft_fine2 = torch.softmax(out_coarse2, dim=1), torch.softmax(out_fine2, dim=1)\n            pred_fine2 = torch.argmax(soft_fine2, dim=1)\n            \n            make_curve(writer, pred_fine1, q_lf, 'model1', param.dataset.n_fine, iter_num)\n            make_curve(writer, pred_fine2, q_lf, 'model2', param.dataset.n_fine, iter_num)\n\n            consistency_weight = get_current_consistency_weight(iter_num // 150)\n\n            loss['model1 supervise loss'] = 0.25 * (ce_loss(out_coarse1, q_lc) + dice_loss_coarse(soft_coarse1, q_lc) + \\\n                    ce_loss(out_fine1[:args.labeled_bs], q_lf[:args.labeled_bs]) + dice_loss_fine(soft_fine1[:args.labeled_bs], q_lf[:args.labeled_bs]))\n            \n            loss['model2 supervise loss'] = 0.25 * (ce_loss(out_coarse2, q_lc) + dice_loss_coarse(soft_coarse2, q_lc) + \\\n                    ce_loss(out_fine2[:args.labeled_bs], q_lf[:args.labeled_bs]) + dice_loss_fine(soft_fine2[:args.labeled_bs], q_lf[:args.labeled_bs]))\n\n            pseudo_outputs_f1 = torch.argmax(soft_fine1[args.labeled_bs:].detach(), dim=1, keepdim=False)\n            pseudo_outputs_f2 = torch.argmax(soft_fine2[args.labeled_bs:].detach(), dim=1, keepdim=False)\n\n            pseudo_supervision_f1 = ce_loss(out_fine1[args.labeled_bs:], pseudo_outputs_f2)\n            pseudo_supervision_f2 = ce_loss(out_fine2[args.labeled_bs:], pseudo_outputs_f1)\n\n            loss['model1 supervise loss'] += consistency_weight * (pseudo_supervision_f1)\n            loss['model2 supervise loss'] += consistency_weight * (pseudo_supervision_f2)\n\n            loss_sum = sum(loss.values())\n\n            optimizer1.zero_grad()\n            optimizer2.zero_grad()\n            loss_sum.backward()\n            optimizer1.step()\n            optimizer2.step()\n\n            iter_num = iter_num + 1\n            lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n            for param_group1 in optimizer1.param_groups:\n                param_group1['lr'] = lr_\n            for param_group2 in optimizer2.param_groups:\n                param_group2['lr'] = lr_\n\n            if args.verbose:\n                loss_names = list(loss.keys())\n                loss_values = list(map(lambda x: str(round(x.item(), 3)), loss.values()))\n                loss_log = ['*'] * (2 * len(loss_names))\n                loss_log[::2] = loss_names\n                loss_log[1::2] = loss_values\n                loss_log = '; '.join(loss_log)\n                logging.info(f\"model {param.exp.exp_name} iteration {iter_num} : total loss: {loss_sum.item():.3f},\\n\" + loss_log)\n            \n            if iter_num % args.draw_step == 0:\n                make_image(writer, param, q_im, 'image/input_image', iter_num, normalize=True)\n                make_image(writer, param, q_lf, 'image/fine_gt', iter_num, param.dataset.n_fine - 1)\n                make_image(writer, param, pred_fine1, 'image/model1_fine_pred', iter_num, param.dataset.n_fine - 1)\n                make_image(writer, param, pred_fine2, 'image/model2_fine_pred', iter_num, param.dataset.n_fine - 1)\n\n            if iter_num > 0 and iter_num % args.val_step == 0:\n                model1.eval()\n                model2.eval()\n                avg_metric_f1 = np.zeros((len(valloader), param.dataset.n_fine, 4))\n                avg_metric_f2 = np.zeros((len(valloader), param.dataset.n_fine, 4))\n                for case_index, sampled_batch in enumerate(tqdm(valloader, position=1, leave=True, desc='Validation Progress')):\n                    _, batch_metric_f1, _ = test_single_case(model1, param, sampled_batch, stride_xy=round(param.exp.patch_size[0] * 0.7), stride_z=64, gpu_id=args.gpu)\n                    avg_metric_f1[case_index] = batch_metric_f1\n                for case_index, sampled_batch in enumerate(tqdm(valloader, position=1, leave=True, desc='Validation Progress')):\n                    _, batch_metric_f2, _ = test_single_case(model2, param, sampled_batch, stride_xy=round(param.exp.patch_size[0] * 0.7), stride_z=64, gpu_id=args.gpu)\n                    avg_metric_f2[case_index] = batch_metric_f2\n                \n                save_best_path = os.path.join(param.path.path_to_model, '{}_best_model.pth'.format(param.exp.exp_name))\n                last_best_model1_state_dict = None\n                last_best_model2_state_dict = None\n                if os.path.exists(save_best_path):\n                    state_dicts = torch.load(save_best_path, map_location='cpu')\n                    last_best_model1_state_dict = state_dicts['model_state_dict']\n                    last_best_model2_state_dict = state_dicts['model2_state_dict']\n                    \n                this_performance1 = avg_metric_f1[:, -1, param.exp.eval_metric].mean()\n                this_performance2 = avg_metric_f2[:, -1, param.exp.eval_metric].mean()\n                if this_performance1 > best_performance1 or this_performance2 > best_performance2:\n                    if this_performance1 > best_performance1 or last_best_model1_state_dict is None:\n                        best_model1_state_dict = model1.state_dict()\n                        best_performance1 = avg_metric_f1[:, -1, param.exp.eval_metric].mean()\n                    else:\n                        best_model1_state_dict = last_best_model1_state_dict\n                    if this_performance2 > best_performance2 or last_best_model2_state_dict is None:\n                        best_model2_state_dict = model2.state_dict()\n                        best_performance2 = avg_metric_f2[:, -1, param.exp.eval_metric].mean()\n                    else:\n                        best_model2_state_dict = last_best_model2_state_dict\n                    \n                    torch.save({\"model_state_dict\": best_model1_state_dict,\n                                \"model2_state_dict\": best_model2_state_dict,\n                                \"optimizer_state_dict\": optimizer1.state_dict(),\n                                \"optimizer2_state_dict\": optimizer2.state_dict(),\n                                \"iterations\": iter_num, \"metric\": best_performance1, \"metric2\": best_performance2}, save_best_path)\n                    logging.info(f\"save model to {save_best_path}\")\n                \n                for index, name in enumerate(['dsc', 'hd95', 'precision', 'recall']):\n                    writer.add_scalars(f'val/model1_{name}', {f'fine label={i}': avg_metric_f1[:, i-1, index].mean() for i in range(1, param.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/model1_{name}', {f'fine avg': avg_metric_f1[:, -1, index].mean()}, iter_num)\n                    writer.add_scalars(f'val/model2_{name}', {f'fine label={i}': avg_metric_f2[:, i-1, index].mean() for i in range(1, param.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/model2_{name}', {f'fine avg': avg_metric_f2[:, -1, index].mean()}, iter_num)\n\n                logging.info(f'iteration {iter_num} : [model 1] dice_score : {avg_metric_f1[:, -1, 0].mean():.4f}; hd95 : {avg_metric_f1[:, -1, 1].mean():.4f}')\n                logging.info(f'iteration {iter_num} : [model 2] dice_score : {avg_metric_f2[:, -1, 0].mean():.4f}; hd95 : {avg_metric_f2[:, -1, 1].mean():.4f}')\n                model1.train()\n                model2.train()\n\n            if iter_num > 0 and iter_num % args.save_step == 0:\n                save_model_path = os.path.join(param.path.path_to_model, 'iter_' + str(iter_num) + '.pth')\n                torch.save({\"model_state_dict\": model1.state_dict(),\n                            \"model2_state_dict\": model2.state_dict(),\n                            \"optimizer_state_dict\": optimizer1.state_dict(),\n                            \"optimizer2_state_dict\": optimizer2.state_dict(),\n                            \"iterations\": iter_num, \"loss\": loss_sum.item()}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n\n            if iter_num >= max_iterations:\n                save_model_path = os.path.join(param.path.path_to_model, '{}_last_model.pth'.format(param.exp.exp_name))\n                torch.save({\"model_state_dict\": model1.state_dict(),\n                            \"model2_state_dict\": model2.state_dict(),\n                            \"optimizer_state_dict\": optimizer1.state_dict(),\n                            \"optimizer2_state_dict\": optimizer2.state_dict(),\n                            \"iterations\": iter_num, \"loss\": loss_sum.item()}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n            \n        if iter_num >= max_iterations:\n            iterator.close()\n            break\n        \n    writer.close()\n    return \"Training Finished!\""]}
{"filename": "trainutils/train_plain_unet.py", "chunked_list": ["#!usr/bin/env python\n\nimport os\nimport sys\nimport math\nimport random\nimport shutil\nimport logging\nimport argparse\nimport numpy as np", "import argparse\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nfrom tensorboardX import SummaryWriter", "\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\nfrom test import test_all_case\nfrom val import test_single_case\nfrom utils import losses\nfrom networks.unet import UNet\nfrom utils.parser import Parser", "from networks.unet import UNet\nfrom utils.parser import Parser\nfrom utils.visualize import make_curve, make_image\n\n\nargs = None\n\n\ndef train_unet(model, optimizer, parameter, parsed_args):\n    global args\n    args = parsed_args\n    model = model[0]\n    optimizer = optimizer[0]\n    \n    base_lr = parameter.exp.base_lr\n    best_performance = 0.0\n    iter_num = 0\n    loss = {}\n        \n    batch_size = parameter.exp.batch_size\n    max_iterations = parameter.exp.max_iter\n\n    db_train = parameter.get_dataset(split='train')\n    db_val = parameter.get_dataset(split='val')\n    \n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train,\n                             shuffle=True,\n                             num_workers=4,\n                             pin_memory=True,\n                             batch_size=batch_size,\n                             worker_init_fn=worker_init_fn)\n    \n    valloader = DataLoader(db_val, num_workers=args.val_bs, batch_size=args.val_bs)\n\n    model.train()\n    \n    ce_loss = CrossEntropyLoss()\n    nce_loss = losses.NegativeCrossEntropyLoss()\n    dice_loss_fine = losses.DiceLoss(parameter.dataset.n_fine) \n\n    writer = SummaryWriter(os.path.join(parameter.path.path_to_snapshot, \"log\"))\n    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n\n    max_epoch = (max_iterations - iter_num) // (len(trainloader)) + 1\n    epoch_iterator = tqdm(range(max_epoch), ncols=100, position=0, leave=True, desc='Training Progress')\n    torch.autograd.set_detect_anomaly(True)\n    \n    for epoch_num in epoch_iterator:\n        for epoch_num, sampled_batch in enumerate(trainloader):\n            q_im, q_lc, q_lf = sampled_batch['image'], sampled_batch['coarse'], sampled_batch['fine']\n\n            if args.gpu >= 0:\n                q_im, q_lc, q_lf = q_im.cuda(args.gpu), q_lc.cuda(args.gpu), q_lf.cuda(args.gpu)\n            else:\n                raise RuntimeError(f'Specify a positive gpu id')\n\n            out = model(q_im)\n            out = out['logit']\n            \n            soft = torch.softmax(out, dim=1)\n            pred = torch.argmax(soft, dim=1)\n            \n            loss_ce = ce_loss(out, q_lf)\n            loss_dice = dice_loss_fine(soft, q_lf)\n            loss_fine = 0.5 * (loss_ce + loss_dice)\n            \n            loss['supervise loss fine'] = loss_fine\n            \n            make_curve(writer, pred, q_lf, 'train', parameter.dataset.n_fine, iter_num)\n\n            if args.nl:\n                loss['negative learning loss'] = nce_loss(out[parameter.exp.labeled_batch_size:], q_lc[parameter.exp.labeled_batch_size:])\n\n            loss_sum = sum(loss.values())    \n            optimizer.zero_grad()\n            loss_sum.backward()\n            optimizer.step()\n\n            lr_ = base_lr * (1.0 - iter_num / max_iterations)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar(f'{parameter.exp.exp_name}/lr', lr_, iter_num)\n            writer.add_scalar('loss/total_loss', loss_sum, iter_num)\n            writer.add_scalars('loss/individual_losses', loss, iter_num)\n\n            if args.verbose:\n                loss_names = list(loss.keys())\n                loss_values = list(map(lambda x: str(round(x.item(), 3)), loss.values()))\n                loss_log = ['*'] * (2 * len(loss_names))\n                loss_log[::2] = loss_names\n                loss_log[1::2] = loss_values\n                loss_log = '; '.join(loss_log)\n                logging.info(f\"model {parameter.exp.exp_name} iteration {iter_num} : total loss: {loss_sum.item():.3f}; \\t\" + loss_log)\n\n            if iter_num > 0 and iter_num % args.draw_step == 0:\n                make_image(writer, parameter, q_im, 'image/input_image', iter_num, normalize=True)\n                make_image(writer, parameter, q_lf, 'image/gt', iter_num, parameter.dataset.n_fine)\n                make_image(writer, parameter, pred, 'image/pred', iter_num, parameter.dataset.n_fine)\n\n            if iter_num > 0 and iter_num % args.val_step == 0:\n                model.eval()\n                avg_metric_f = np.zeros((len(valloader), parameter.dataset.n_fine, 4))\n                for case_index, sampled_batch in enumerate(tqdm(valloader, position=1, leave=True, desc='Validation Progress')):\n                    _, batch_metric_f, _ = test_single_case(model, parameter, sampled_batch, stride_xy=round(parameter.exp.patch_size[0] * 0.7), stride_z=64, gpu_id=args.gpu)\n                    avg_metric_f[case_index] = batch_metric_f\n                \n                if avg_metric_f[:, -1, parameter.exp.eval_metric].mean() > best_performance:\n                    best_performance = avg_metric_f[:, -1, parameter.exp.eval_metric].mean()\n                    save_best = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n                    torch.save({\"model_state_dict\": model.state_dict(),\n                                \"optimizer_state_dict\": optimizer.state_dict(),\n                                \"iterations\": iter_num, \"metric\": best_performance}, save_best)\n                    logging.info(f\"save model to {save_best}\")\n                \n                for index, name in enumerate(['dsc', 'hd95', 'precision', 'recall']):\n                    writer.add_scalars(f'val/{name}', {f'fine label={i}': avg_metric_f[:, i-1, index].mean() for i in range(1, parameter.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/{name}', {f'fine avg': avg_metric_f[:, -1, index].mean()}, iter_num)\n\n                logging.info(f'iteration {iter_num} : dice_score : {avg_metric_f[:, -1, 0].mean():.4f} hd95 : {avg_metric_f[:, -1, 1].mean():.4f}')\n                model.train()\n\n            if iter_num > 0 and iter_num % args.save_step == 0:\n                save_model_path = os.path.join(parameter.path.path_to_model, 'iter_' + str(iter_num) + '.pth')\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optimizer_state_dict\": optimizer.state_dict(),\n                            \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n\n            if iter_num == max_iterations:\n                save_model_path = os.path.join(parameter.path.path_to_model, '{}_last_model.pth'.format(parameter.exp.exp_name))\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optimizer_state_dict\": optimizer.state_dict(),\n                            \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n            \n        if iter_num >= max_iterations:\n            epoch_iterator.close()\n            break\n    writer.close()\n    return \"Training Finished!\"", "def train_unet(model, optimizer, parameter, parsed_args):\n    global args\n    args = parsed_args\n    model = model[0]\n    optimizer = optimizer[0]\n    \n    base_lr = parameter.exp.base_lr\n    best_performance = 0.0\n    iter_num = 0\n    loss = {}\n        \n    batch_size = parameter.exp.batch_size\n    max_iterations = parameter.exp.max_iter\n\n    db_train = parameter.get_dataset(split='train')\n    db_val = parameter.get_dataset(split='val')\n    \n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train,\n                             shuffle=True,\n                             num_workers=4,\n                             pin_memory=True,\n                             batch_size=batch_size,\n                             worker_init_fn=worker_init_fn)\n    \n    valloader = DataLoader(db_val, num_workers=args.val_bs, batch_size=args.val_bs)\n\n    model.train()\n    \n    ce_loss = CrossEntropyLoss()\n    nce_loss = losses.NegativeCrossEntropyLoss()\n    dice_loss_fine = losses.DiceLoss(parameter.dataset.n_fine) \n\n    writer = SummaryWriter(os.path.join(parameter.path.path_to_snapshot, \"log\"))\n    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n\n    max_epoch = (max_iterations - iter_num) // (len(trainloader)) + 1\n    epoch_iterator = tqdm(range(max_epoch), ncols=100, position=0, leave=True, desc='Training Progress')\n    torch.autograd.set_detect_anomaly(True)\n    \n    for epoch_num in epoch_iterator:\n        for epoch_num, sampled_batch in enumerate(trainloader):\n            q_im, q_lc, q_lf = sampled_batch['image'], sampled_batch['coarse'], sampled_batch['fine']\n\n            if args.gpu >= 0:\n                q_im, q_lc, q_lf = q_im.cuda(args.gpu), q_lc.cuda(args.gpu), q_lf.cuda(args.gpu)\n            else:\n                raise RuntimeError(f'Specify a positive gpu id')\n\n            out = model(q_im)\n            out = out['logit']\n            \n            soft = torch.softmax(out, dim=1)\n            pred = torch.argmax(soft, dim=1)\n            \n            loss_ce = ce_loss(out, q_lf)\n            loss_dice = dice_loss_fine(soft, q_lf)\n            loss_fine = 0.5 * (loss_ce + loss_dice)\n            \n            loss['supervise loss fine'] = loss_fine\n            \n            make_curve(writer, pred, q_lf, 'train', parameter.dataset.n_fine, iter_num)\n\n            if args.nl:\n                loss['negative learning loss'] = nce_loss(out[parameter.exp.labeled_batch_size:], q_lc[parameter.exp.labeled_batch_size:])\n\n            loss_sum = sum(loss.values())    \n            optimizer.zero_grad()\n            loss_sum.backward()\n            optimizer.step()\n\n            lr_ = base_lr * (1.0 - iter_num / max_iterations)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar(f'{parameter.exp.exp_name}/lr', lr_, iter_num)\n            writer.add_scalar('loss/total_loss', loss_sum, iter_num)\n            writer.add_scalars('loss/individual_losses', loss, iter_num)\n\n            if args.verbose:\n                loss_names = list(loss.keys())\n                loss_values = list(map(lambda x: str(round(x.item(), 3)), loss.values()))\n                loss_log = ['*'] * (2 * len(loss_names))\n                loss_log[::2] = loss_names\n                loss_log[1::2] = loss_values\n                loss_log = '; '.join(loss_log)\n                logging.info(f\"model {parameter.exp.exp_name} iteration {iter_num} : total loss: {loss_sum.item():.3f}; \\t\" + loss_log)\n\n            if iter_num > 0 and iter_num % args.draw_step == 0:\n                make_image(writer, parameter, q_im, 'image/input_image', iter_num, normalize=True)\n                make_image(writer, parameter, q_lf, 'image/gt', iter_num, parameter.dataset.n_fine)\n                make_image(writer, parameter, pred, 'image/pred', iter_num, parameter.dataset.n_fine)\n\n            if iter_num > 0 and iter_num % args.val_step == 0:\n                model.eval()\n                avg_metric_f = np.zeros((len(valloader), parameter.dataset.n_fine, 4))\n                for case_index, sampled_batch in enumerate(tqdm(valloader, position=1, leave=True, desc='Validation Progress')):\n                    _, batch_metric_f, _ = test_single_case(model, parameter, sampled_batch, stride_xy=round(parameter.exp.patch_size[0] * 0.7), stride_z=64, gpu_id=args.gpu)\n                    avg_metric_f[case_index] = batch_metric_f\n                \n                if avg_metric_f[:, -1, parameter.exp.eval_metric].mean() > best_performance:\n                    best_performance = avg_metric_f[:, -1, parameter.exp.eval_metric].mean()\n                    save_best = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n                    torch.save({\"model_state_dict\": model.state_dict(),\n                                \"optimizer_state_dict\": optimizer.state_dict(),\n                                \"iterations\": iter_num, \"metric\": best_performance}, save_best)\n                    logging.info(f\"save model to {save_best}\")\n                \n                for index, name in enumerate(['dsc', 'hd95', 'precision', 'recall']):\n                    writer.add_scalars(f'val/{name}', {f'fine label={i}': avg_metric_f[:, i-1, index].mean() for i in range(1, parameter.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/{name}', {f'fine avg': avg_metric_f[:, -1, index].mean()}, iter_num)\n\n                logging.info(f'iteration {iter_num} : dice_score : {avg_metric_f[:, -1, 0].mean():.4f} hd95 : {avg_metric_f[:, -1, 1].mean():.4f}')\n                model.train()\n\n            if iter_num > 0 and iter_num % args.save_step == 0:\n                save_model_path = os.path.join(parameter.path.path_to_model, 'iter_' + str(iter_num) + '.pth')\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optimizer_state_dict\": optimizer.state_dict(),\n                            \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n\n            if iter_num == max_iterations:\n                save_model_path = os.path.join(parameter.path.path_to_model, '{}_last_model.pth'.format(parameter.exp.exp_name))\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optimizer_state_dict\": optimizer.state_dict(),\n                            \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n            \n        if iter_num >= max_iterations:\n            epoch_iterator.close()\n            break\n    writer.close()\n    return \"Training Finished!\"", "\n\ndef test(model, parameter):\n    \n    save_model_path = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n    model.load_state_dict(torch.load(save_model_path))\n    print(\"init weight from {}\".format(save_model_path))\n    \n    db_test = parameter.get_dataset(split='test')\n    testloader = DataLoader(db_test, num_workers=1, batch_size=1)\n    \n    model.eval()\n    avg_metric_c, avg_metric_f =\\\n        test_all_case(model, parameter, testloader, stride_xy=64, stride_z=64, gpu_id=args.gpu)\n    \n    print(avg_metric_c)\n    print(avg_metric_f)", "\n\nif __name__ == \"__main__\":\n    \n    parser = argparse.ArgumentParser()\n    # hyper settings\n    parser.add_argument('-s', '--seed', type=int, default=1234, help='randomization seed')\n    parser.add_argument('-g', '--gpu', type=int, default=1, help='gpu on which to train model')\n\n    # experiment settings\n    parser.add_argument('--bs', type=int, default=24, help='number of batch size')\n    parser.add_argument('--lr', type=float, default=1e-2, help='base learning rate')\n    parser.add_argument('--iter', type=int, default=40000, help='maximum training iterations')\n    parser.add_argument('--eval', type=str, default='dsc', help='evaluation metric for saving model: [dsc, hd95, precision, recall]')\n    parser.add_argument('--mixup', action='store_true', help='whether to use label mixup')\n    parser.add_argument('--pseudo', action='store_true', help='whether to use pseudo labeling')\n    parser.add_argument('--sn', action='store_true', help='whether to use separate batchnorm')\n    parser.add_argument('--pc', action='store_true', help='whether to use priority concatenation')\n    parser.add_argument('--nl', action='store_true', help='whether to use negative learning')\n    parser.add_argument('--restore', action='store_true', help='whether to continue a previous training')\n    parser.add_argument('--patch_size', type=list, default=[256, 256], help='size for network input')\n    parser.add_argument('--exp_name', type=str, default='test', help='name of the current model')\n\n    # path settings\n    parser.add_argument('--data_path', type=str, default='/data/dailinrui/dataset/refuge2020_trainExpand', help='root path for dataset')\n    parser.add_argument('--model_path', type=str, default='/nas/dailinrui/SSL4MIS/model_final/REFUGE2020', help='root path for training model')\n\n    # number of dataset samples for SSL\n    # for ACDC or any 3d database with a large interslice spacing, this is the number of total slices\n    parser.add_argument('--total_num', type=int, default=1312, help='how many samples in total')\n    parser.add_argument('--labeled_num', type=int, default=1312, help='how many samples are labeled')\n\n    # network settings\n    parser.add_argument('--feature_scale', type=int, default=2, help='feature scale per unet encoder/decoder step')\n    parser.add_argument('--base_feature', type=int, default=16, help='base feature channels for unet layer 0')\n    parser.add_argument('--image_scale', type=int, default=2, help='image scale per unet encoder/decoder step')\n    parser.add_argument('--is_batchnorm', type=bool, default=True, help='use batchnorm instead of instancenorm')\n\n    # irrelevants\n    parser.add_argument('--val_bs', type=int, default=1, help='batch size at val time')\n    parser.add_argument('--val_step', type=int, default=200, help='do validation per val_step')\n    parser.add_argument('--draw_step', type=int, default=20, help='add train graphic result per draw_step')\n    parser.add_argument('--save_step', type=int, default=5000, help='save model and optimizer state dict per save_step')\n    parser.add_argument('--verbose', action='store_true', help='whether to display the loss information per iter')\n    args = parser.parse_args()\n    parameter = Parser(args).get_param()\n    \n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n\n    logging.basicConfig(\n        level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s',\n        datefmt='%H:%M:%S'\n    )\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.getLogger().addHandler(logging.FileHandler(os.path.join(parameter.path.path_to_snapshot, \"log.txt\"), mode='w'))\n    logging.info(msg=parameter)\n    \n    model = UNet(parameter).cuda(args.gpu)\n    \n    train_unet(model, parameter=parameter)\n    test(model, parameter=parameter)\n    print(f'train-test over for {parameter.exp.exp_name}')"]}
{"filename": "trainutils/train_branched.py", "chunked_list": ["#!usr/bin/env python\n\nimport os\nimport sys\nimport math\nimport random\nimport shutil\nimport logging\nimport argparse\nimport numpy as np", "import argparse\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nfrom tensorboardX import SummaryWriter", "\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\nfrom test import test_all_case\nfrom val import test_single_case\nfrom utils import losses\nfrom utils.parser import Parser\nfrom networks.singlybranchedunet import UNetSingleBranchNetwork", "from utils.parser import Parser\nfrom networks.singlybranchedunet import UNetSingleBranchNetwork\nfrom utils.visualize import make_curve, make_image\nfrom dataloaders.utils import TwoStreamBatchSampler\n\n\nargs = None\n\n\ndef train_c2f(model, optimizer, param, parsed_args):\n    global args\n    args = parsed_args\n    model = model[0]\n    optimizer = optimizer[0]\n    \n    base_lr = param.exp.base_lr\n    best_performance = 0.0\n    iter_num = 0\n    loss = {}\n        \n    batch_size = param.exp.batch_size\n    max_iterations = param.exp.max_iter\n\n    db_train = param.get_dataset(split='train')\n    db_val = param.get_dataset(split='val')\n    \n    labeled_idxs = db_train.labeled_idxs\n    unlabeled_idxs = db_train.unlabeled_idxs\n    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-param.exp.labeled_batch_size)\n    \n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train,\n                             num_workers=4,\n                             pin_memory=True,\n                             batch_sampler=batch_sampler,\n                             worker_init_fn=worker_init_fn)\n    \n    valloader = DataLoader(db_val, num_workers=args.val_bs, batch_size=args.val_bs)\n\n    model.train()\n    \n    ce_loss = CrossEntropyLoss()\n    nce_loss = losses.NegativeCrossEntropyLoss()\n    dice_loss_coarse = losses.DiceLoss(param.dataset.n_coarse)\n    dice_loss_fine = losses.DiceLoss(param.dataset.n_fine) \n\n    writer = SummaryWriter(os.path.join(param.path.path_to_snapshot, \"log\"))\n    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n\n    max_epoch = (max_iterations - iter_num) // (len(trainloader))\n    iterator = tqdm(range(max_epoch), ncols=100, position=0, leave=True, desc='Training Progress')\n    torch.autograd.set_detect_anomaly(True)\n    \n    for _ in iterator:\n        for _, sampled_batch in enumerate(trainloader):\n            q_im, q_lc, q_lf = sampled_batch['image'], sampled_batch['coarse'], sampled_batch['fine']\n\n            if args.gpu >= 0:\n                q_im, q_lc, q_lf = q_im.cuda(args.gpu), q_lc.cuda(args.gpu), q_lf.cuda(args.gpu)\n            else: raise RuntimeError(f'Specify a valid gpu id')\n\n            out = model(q_im)\n            out_coarse, out_fine = out['coarse_logit'], out['fine_logit']\n            \n            soft_coarse = torch.softmax(out_coarse, dim=1)\n            soft_fine = torch.softmax(out_fine, dim=1)\n            \n            pred_coarse = torch.argmax(soft_coarse, dim=1)\n            pred_fine = torch.argmax(soft_fine, dim=1)\n            \n            loss_ce1 = ce_loss(out_coarse, q_lc)\n            loss_dice1 = dice_loss_coarse(soft_coarse, q_lc)\n            loss_coarse = 0.5 * (loss_ce1 + loss_dice1)\n            loss_ce2 = ce_loss(out_fine[:param.exp.labeled_batch_size], q_lf[:param.exp.labeled_batch_size])\n            loss_dice2 = dice_loss_fine(soft_fine[:param.exp.labeled_batch_size], q_lf[:param.exp.labeled_batch_size])\n            loss_fine = 0.5 * (loss_ce2 + loss_dice2)\n            \n            loss['supervise loss coarse'] = loss_coarse\n            loss['supervise loss fine'] = loss_fine\n            \n            if args.nl:\n                loss['negative learning loss'] = nce_loss(out[param.exp.labeled_batch_size:], q_lc[param.exp.labeled_batch_size:])\n            \n            if param.exp.mixup_label:\n                \n                mixed_im, mixed_lf, alpha = sampled_batch['mixed'], sampled_batch['fine'], sampled_batch['alpha']\n                if args.gpu >= 0:\n                    mixed_im, mixed_lf, alpha = mixed_im.cuda(args.gpu), mixed_lf.cuda(args.gpu), alpha.cuda(args.gpu)\n                else:\n                    raise RuntimeError(f'Specify a valid gpu id')\n\n                mixed_pred, pseudo_lf = model.gen_mixup_labels(\n                    q_im=q_im[param.exp.labeled_batch_size:],\n                    q_lc=q_lc[param.exp.labeled_batch_size:],\n                    q_soft=soft_fine[param.exp.labeled_batch_size:],\n                    mixed_im=mixed_im[param.exp.labeled_batch_size:],\n                    mixed_lf=mixed_lf[param.exp.labeled_batch_size:],\n                    alpha=alpha[param.exp.labeled_batch_size:],\n                    threshold=max(0.999 ** (iter_num // 10), 0.4),\n                    with_pseudo_label=param.exp.pseudo_label\n                )\n                \n                soft_mixed_pred = torch.softmax(mixed_pred, dim=1)\n                loss_ce3 = ce_loss(mixed_pred, pseudo_lf)\n                loss_dice3 = dice_loss_fine(soft_mixed_pred, pseudo_lf, mask=pseudo_lf)\n                loss3 = 0.5 * (loss_dice3 + loss_ce3) / (1 + math.exp(-iter_num // 1000))\n                loss['sematic mixup loss'] = loss3\n                \n            elif param.exp.pseudo_label:\n                \n                pseudo_lf = model.gen_pseudo_labels(\n                    q_im=q_im[param.exp.labeled_batch_size:],\n                    q_soft=soft_fine[param.exp.labeled_batch_size:],\n                    q_lc=q_lc[param.exp.labeled_batch_size:],\n                    threshold=max(0.999 ** (iter_num // 10), 0.4)\n                )\n                \n                loss_ce3 = ce_loss(out_fine[param.exp.labeled_batch_size:], pseudo_lf)\n                loss_dice3 = dice_loss_fine(\n                    soft_fine[param.exp.labeled_batch_size:],\n                    pseudo_lf, mask=pseudo_lf\n                )\n                loss3 = 0.5 * (loss_dice3 + loss_ce3) / (1 + math.exp(-iter_num // 1000))\n                loss['pseudo label loss'] = loss3\n            \n            make_curve(writer, pred_fine, q_lf, 'train', param.dataset.n_fine, iter_num)\n            make_curve(writer, pred_coarse, q_lc, 'train', param.dataset.n_coarse, iter_num)\n\n            loss_sum = sum(loss.values())          \n            optimizer.zero_grad()\n            loss_sum.backward()\n            optimizer.step()\n\n            lr_ = base_lr * (1.0 - iter_num / max_iterations)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar(f'{param.exp.exp_name}/lr', lr_, iter_num)\n            writer.add_scalar('loss/total_loss', loss_sum, iter_num)\n            writer.add_scalars('loss/individual_losses', loss, iter_num)\n\n            if args.verbose:\n                loss_names = list(loss.keys())\n                loss_values = list(map(lambda x: str(round(x.item(), 3)), loss.values()))\n                loss_log = ['*'] * (2 * len(loss_names))\n                loss_log[::2] = loss_names\n                loss_log[1::2] = loss_values\n                loss_log = '; '.join(loss_log)\n                logging.info(f\"model {param.exp.exp_name} iteration {iter_num} : total loss: {loss_sum.item():.3f},\\n\" + loss_log)\n\n            if iter_num % args.draw_step == 0:\n                make_image(writer, param, q_im, 'image/input_image', iter_num, normalize=True)\n                make_image(writer, param, q_lc, 'image/coarse_gt', iter_num, param.dataset.n_coarse - 1)\n                make_image(writer, param, q_lf, 'image/fine_gt', iter_num, param.dataset.n_fine - 1)\n                make_image(writer, param, pred_coarse, 'image/coarse_pred', iter_num, param.dataset.n_coarse - 1)\n                make_image(writer, param, pred_fine, 'image/fine_pred', iter_num, param.dataset.n_fine - 1)\n                \n                if param.exp.mixup_label:\n                    make_image(writer, param, mixed_im, 'pseudo_label/mixup_image', iter_num, normalize=True)\n                    make_image(writer, param, mixed_lf, 'pseudo_label/mixup_fine_gt', iter_num, param.dataset.n_fine - 1)\n                if param.exp.pseudo_label:\n                    make_image(writer, param, pseudo_lf, 'pseudo_label/pseudo_fine_gt', iter_num, param.dataset.n_fine - 1)\n\n            if iter_num > 0 and iter_num % args.val_step == 0:\n                model.eval()\n                avg_metric_f = np.zeros((len(valloader), param.dataset.n_fine, 4))\n                for case_index, sampled_batch in enumerate(tqdm(valloader, position=1, leave=True, desc='Validation Progress')):\n                    _, batch_metric_f, _ = test_single_case(model, param, sampled_batch, stride_xy=round(param.exp.patch_size[0] * 0.7), stride_z=64, gpu_id=args.gpu)\n                    avg_metric_f[case_index] = batch_metric_f\n                \n                if avg_metric_f[:, -1, param.exp.eval_metric].mean() > best_performance:\n                    best_performance = avg_metric_f[:, -1, param.exp.eval_metric].mean()\n                    save_best = os.path.join(param.path.path_to_model, '{}_best_model.pth'.format(param.exp.exp_name))\n                    torch.save({\"model_state_dict\": model.state_dict(),\n                                \"optimizer_state_dict\": optimizer.state_dict(),\n                                \"iterations\": iter_num, \"metric\": best_performance}, save_best)\n                    logging.info(f\"save model to {save_best}\")\n                \n                for index, name in enumerate(['dsc', 'hd95', 'precision', 'recall']):\n                    writer.add_scalars(f'val/{name}', {f'fine label={i}': avg_metric_f[:, i-1, index].mean() for i in range(1, param.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/{name}', {f'fine avg': avg_metric_f[:, -1, index].mean()}, iter_num)\n\n                logging.info(f'iteration {iter_num} : dice_score : {avg_metric_f[:, -1, 0].mean():.4f} hd95 : {avg_metric_f[:, -1, 1].mean():.4f}')\n                model.train()\n\n            if iter_num > 0 and iter_num % args.save_step == 0:\n                save_model_path = os.path.join(param.path.path_to_model, 'iter_' + str(iter_num) + '.pth')\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optimizer_state_dict\": optimizer.state_dict(),\n                            \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n\n            if iter_num == max_iterations:\n                save_model_path = os.path.join(param.path.path_to_model, '{}_last_model.pth'.format(param.exp.exp_name))\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optimizer_state_dict\": optimizer.state_dict(),\n                            \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n    \n        if iter_num >= max_iterations:\n            iterator.close()\n            break\n            \n    writer.close()\n    return \"Training Finished!\"", "\ndef train_c2f(model, optimizer, param, parsed_args):\n    global args\n    args = parsed_args\n    model = model[0]\n    optimizer = optimizer[0]\n    \n    base_lr = param.exp.base_lr\n    best_performance = 0.0\n    iter_num = 0\n    loss = {}\n        \n    batch_size = param.exp.batch_size\n    max_iterations = param.exp.max_iter\n\n    db_train = param.get_dataset(split='train')\n    db_val = param.get_dataset(split='val')\n    \n    labeled_idxs = db_train.labeled_idxs\n    unlabeled_idxs = db_train.unlabeled_idxs\n    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-param.exp.labeled_batch_size)\n    \n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    trainloader = DataLoader(db_train,\n                             num_workers=4,\n                             pin_memory=True,\n                             batch_sampler=batch_sampler,\n                             worker_init_fn=worker_init_fn)\n    \n    valloader = DataLoader(db_val, num_workers=args.val_bs, batch_size=args.val_bs)\n\n    model.train()\n    \n    ce_loss = CrossEntropyLoss()\n    nce_loss = losses.NegativeCrossEntropyLoss()\n    dice_loss_coarse = losses.DiceLoss(param.dataset.n_coarse)\n    dice_loss_fine = losses.DiceLoss(param.dataset.n_fine) \n\n    writer = SummaryWriter(os.path.join(param.path.path_to_snapshot, \"log\"))\n    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n\n    max_epoch = (max_iterations - iter_num) // (len(trainloader))\n    iterator = tqdm(range(max_epoch), ncols=100, position=0, leave=True, desc='Training Progress')\n    torch.autograd.set_detect_anomaly(True)\n    \n    for _ in iterator:\n        for _, sampled_batch in enumerate(trainloader):\n            q_im, q_lc, q_lf = sampled_batch['image'], sampled_batch['coarse'], sampled_batch['fine']\n\n            if args.gpu >= 0:\n                q_im, q_lc, q_lf = q_im.cuda(args.gpu), q_lc.cuda(args.gpu), q_lf.cuda(args.gpu)\n            else: raise RuntimeError(f'Specify a valid gpu id')\n\n            out = model(q_im)\n            out_coarse, out_fine = out['coarse_logit'], out['fine_logit']\n            \n            soft_coarse = torch.softmax(out_coarse, dim=1)\n            soft_fine = torch.softmax(out_fine, dim=1)\n            \n            pred_coarse = torch.argmax(soft_coarse, dim=1)\n            pred_fine = torch.argmax(soft_fine, dim=1)\n            \n            loss_ce1 = ce_loss(out_coarse, q_lc)\n            loss_dice1 = dice_loss_coarse(soft_coarse, q_lc)\n            loss_coarse = 0.5 * (loss_ce1 + loss_dice1)\n            loss_ce2 = ce_loss(out_fine[:param.exp.labeled_batch_size], q_lf[:param.exp.labeled_batch_size])\n            loss_dice2 = dice_loss_fine(soft_fine[:param.exp.labeled_batch_size], q_lf[:param.exp.labeled_batch_size])\n            loss_fine = 0.5 * (loss_ce2 + loss_dice2)\n            \n            loss['supervise loss coarse'] = loss_coarse\n            loss['supervise loss fine'] = loss_fine\n            \n            if args.nl:\n                loss['negative learning loss'] = nce_loss(out[param.exp.labeled_batch_size:], q_lc[param.exp.labeled_batch_size:])\n            \n            if param.exp.mixup_label:\n                \n                mixed_im, mixed_lf, alpha = sampled_batch['mixed'], sampled_batch['fine'], sampled_batch['alpha']\n                if args.gpu >= 0:\n                    mixed_im, mixed_lf, alpha = mixed_im.cuda(args.gpu), mixed_lf.cuda(args.gpu), alpha.cuda(args.gpu)\n                else:\n                    raise RuntimeError(f'Specify a valid gpu id')\n\n                mixed_pred, pseudo_lf = model.gen_mixup_labels(\n                    q_im=q_im[param.exp.labeled_batch_size:],\n                    q_lc=q_lc[param.exp.labeled_batch_size:],\n                    q_soft=soft_fine[param.exp.labeled_batch_size:],\n                    mixed_im=mixed_im[param.exp.labeled_batch_size:],\n                    mixed_lf=mixed_lf[param.exp.labeled_batch_size:],\n                    alpha=alpha[param.exp.labeled_batch_size:],\n                    threshold=max(0.999 ** (iter_num // 10), 0.4),\n                    with_pseudo_label=param.exp.pseudo_label\n                )\n                \n                soft_mixed_pred = torch.softmax(mixed_pred, dim=1)\n                loss_ce3 = ce_loss(mixed_pred, pseudo_lf)\n                loss_dice3 = dice_loss_fine(soft_mixed_pred, pseudo_lf, mask=pseudo_lf)\n                loss3 = 0.5 * (loss_dice3 + loss_ce3) / (1 + math.exp(-iter_num // 1000))\n                loss['sematic mixup loss'] = loss3\n                \n            elif param.exp.pseudo_label:\n                \n                pseudo_lf = model.gen_pseudo_labels(\n                    q_im=q_im[param.exp.labeled_batch_size:],\n                    q_soft=soft_fine[param.exp.labeled_batch_size:],\n                    q_lc=q_lc[param.exp.labeled_batch_size:],\n                    threshold=max(0.999 ** (iter_num // 10), 0.4)\n                )\n                \n                loss_ce3 = ce_loss(out_fine[param.exp.labeled_batch_size:], pseudo_lf)\n                loss_dice3 = dice_loss_fine(\n                    soft_fine[param.exp.labeled_batch_size:],\n                    pseudo_lf, mask=pseudo_lf\n                )\n                loss3 = 0.5 * (loss_dice3 + loss_ce3) / (1 + math.exp(-iter_num // 1000))\n                loss['pseudo label loss'] = loss3\n            \n            make_curve(writer, pred_fine, q_lf, 'train', param.dataset.n_fine, iter_num)\n            make_curve(writer, pred_coarse, q_lc, 'train', param.dataset.n_coarse, iter_num)\n\n            loss_sum = sum(loss.values())          \n            optimizer.zero_grad()\n            loss_sum.backward()\n            optimizer.step()\n\n            lr_ = base_lr * (1.0 - iter_num / max_iterations)\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            writer.add_scalar(f'{param.exp.exp_name}/lr', lr_, iter_num)\n            writer.add_scalar('loss/total_loss', loss_sum, iter_num)\n            writer.add_scalars('loss/individual_losses', loss, iter_num)\n\n            if args.verbose:\n                loss_names = list(loss.keys())\n                loss_values = list(map(lambda x: str(round(x.item(), 3)), loss.values()))\n                loss_log = ['*'] * (2 * len(loss_names))\n                loss_log[::2] = loss_names\n                loss_log[1::2] = loss_values\n                loss_log = '; '.join(loss_log)\n                logging.info(f\"model {param.exp.exp_name} iteration {iter_num} : total loss: {loss_sum.item():.3f},\\n\" + loss_log)\n\n            if iter_num % args.draw_step == 0:\n                make_image(writer, param, q_im, 'image/input_image', iter_num, normalize=True)\n                make_image(writer, param, q_lc, 'image/coarse_gt', iter_num, param.dataset.n_coarse - 1)\n                make_image(writer, param, q_lf, 'image/fine_gt', iter_num, param.dataset.n_fine - 1)\n                make_image(writer, param, pred_coarse, 'image/coarse_pred', iter_num, param.dataset.n_coarse - 1)\n                make_image(writer, param, pred_fine, 'image/fine_pred', iter_num, param.dataset.n_fine - 1)\n                \n                if param.exp.mixup_label:\n                    make_image(writer, param, mixed_im, 'pseudo_label/mixup_image', iter_num, normalize=True)\n                    make_image(writer, param, mixed_lf, 'pseudo_label/mixup_fine_gt', iter_num, param.dataset.n_fine - 1)\n                if param.exp.pseudo_label:\n                    make_image(writer, param, pseudo_lf, 'pseudo_label/pseudo_fine_gt', iter_num, param.dataset.n_fine - 1)\n\n            if iter_num > 0 and iter_num % args.val_step == 0:\n                model.eval()\n                avg_metric_f = np.zeros((len(valloader), param.dataset.n_fine, 4))\n                for case_index, sampled_batch in enumerate(tqdm(valloader, position=1, leave=True, desc='Validation Progress')):\n                    _, batch_metric_f, _ = test_single_case(model, param, sampled_batch, stride_xy=round(param.exp.patch_size[0] * 0.7), stride_z=64, gpu_id=args.gpu)\n                    avg_metric_f[case_index] = batch_metric_f\n                \n                if avg_metric_f[:, -1, param.exp.eval_metric].mean() > best_performance:\n                    best_performance = avg_metric_f[:, -1, param.exp.eval_metric].mean()\n                    save_best = os.path.join(param.path.path_to_model, '{}_best_model.pth'.format(param.exp.exp_name))\n                    torch.save({\"model_state_dict\": model.state_dict(),\n                                \"optimizer_state_dict\": optimizer.state_dict(),\n                                \"iterations\": iter_num, \"metric\": best_performance}, save_best)\n                    logging.info(f\"save model to {save_best}\")\n                \n                for index, name in enumerate(['dsc', 'hd95', 'precision', 'recall']):\n                    writer.add_scalars(f'val/{name}', {f'fine label={i}': avg_metric_f[:, i-1, index].mean() for i in range(1, param.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/{name}', {f'fine avg': avg_metric_f[:, -1, index].mean()}, iter_num)\n\n                logging.info(f'iteration {iter_num} : dice_score : {avg_metric_f[:, -1, 0].mean():.4f} hd95 : {avg_metric_f[:, -1, 1].mean():.4f}')\n                model.train()\n\n            if iter_num > 0 and iter_num % args.save_step == 0:\n                save_model_path = os.path.join(param.path.path_to_model, 'iter_' + str(iter_num) + '.pth')\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optimizer_state_dict\": optimizer.state_dict(),\n                            \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n\n            if iter_num == max_iterations:\n                save_model_path = os.path.join(param.path.path_to_model, '{}_last_model.pth'.format(param.exp.exp_name))\n                torch.save({\"model_state_dict\": model.state_dict(),\n                            \"optimizer_state_dict\": optimizer.state_dict(),\n                            \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n    \n        if iter_num >= max_iterations:\n            iterator.close()\n            break\n            \n    writer.close()\n    return \"Training Finished!\"", "\n\ndef test(model, parameter):\n    \n    save_model_path = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n    model.load_state_dict(torch.load(save_model_path)['model_state_dict'])\n    print(\"init weight from {}\".format(save_model_path))\n    \n    db_test = parameter.get_dataset(split='test')\n    testloader = DataLoader(db_test, num_workers=1, batch_size=1)\n    \n    model.eval()\n    avg_metric_c, avg_metric_f =\\\n        test_all_case(model, parameter, testloader, stride_xy=64, stride_z=64, gpu_id=args.gpu)\n    \n    print(avg_metric_c)\n    print(avg_metric_f)", "\n\nif __name__ == \"__main__\":\n    \n    parser = argparse.ArgumentParser()\n    # hyper settings\n    parser.add_argument('-s', '--seed', type=int, default=1234, help='randomization seed')\n    parser.add_argument('-g', '--gpu', type=int, default=1, help='gpu on which to train model')\n\n    # experiment settings\n    parser.add_argument('--bs', type=int, default=24, help='number of batch size')\n    parser.add_argument('--lr', type=float, default=1e-2, help='base learning rate')\n    parser.add_argument('--iter', type=int, default=40000, help='maximum training iterations')\n    parser.add_argument('--eval', type=str, default='dsc', choices=['dsc', 'hd95', 'precision', 'recall'], help='evaluation metric for saving model')\n    parser.add_argument('--mixup', action='store_true', help='whether to use label mixup')\n    parser.add_argument('--pseudo', action='store_true', help='whether to use pseudo labeling')\n    parser.add_argument('--sn', action='store_true', help='whether to use separate batchnorm')\n    parser.add_argument('--pc', action='store_true', help='whether to use priority concatenation')\n    parser.add_argument('--nl', action='store_true', help='whether to use negative learning')\n    parser.add_argument('--restore', action='store_true', help='whether to continue a previous training')\n    parser.add_argument('--patch_size', type=list, default=[256, 256], help='size for network input')\n    parser.add_argument('--exp_name', type=str, default='newTest', help='name of the current model')\n\n    # path settings\n    parser.add_argument('--data_path', type=str, default='/data/dailinrui/dataset/refuge2020_trainExpand', help='root path for dataset')\n    parser.add_argument('--model_path', type=str, default='/nas/dailinrui/SSL4MIS/model_final/REFUGE2020', help='root path for training model')\n\n    # number of dataset samples for SSL\n    # for ACDC or any 3d database with a large interslice spacing and is trained per slice, this is the number of total slices\n    parser.add_argument('--labeled_bs', type=int, default=4, help='how many samples are labeled')\n    parser.add_argument('--total_num', type=int, default=252, help='how many samples in total')\n    parser.add_argument('--labeled_num', type=int, default=10, help='how many samples are labeled')\n\n    # network settings\n    parser.add_argument('--feature_scale', type=int, default=2, help='feature scale per unet encoder/decoder step')\n    parser.add_argument('--base_feature', type=int, default=16, help='base feature channels for unet layer 0')\n    parser.add_argument('--image_scale', type=int, default=2, help='image scale per unet encoder/decoder step')\n    parser.add_argument('--is_batchnorm', type=bool, default=True, help='use batchnorm instead of instancenorm')\n\n    # irrelevants\n    parser.add_argument('--val_bs', type=int, default=1, help='batch size at val time')\n    parser.add_argument('--val_step', type=int, default=200, help='do validation per val_step')\n    parser.add_argument('--draw_step', type=int, default=50, help='add train graphic result per draw_step')\n    parser.add_argument('--save_step', type=int, default=5000, help='save model and optimizer state dict per save_step')\n    parser.add_argument('--verbose', action='store_true', help='whether to display the loss information per iter')\n    args = parser.parse_args()\n    parameter = Parser(args).get_param()\n    \n    cudnn.benchmark = False\n    cudnn.deterministic = True\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    \n    logging.basicConfig(\n        level=logging.INFO, format='[%(asctime)s.%(msecs)03d] %(message)s',\n        datefmt='%H:%M:%S'\n    )\n    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n    logging.getLogger().addHandler(logging.FileHandler(os.path.join(parameter.path.path_to_snapshot, \"log.txt\"), mode='w'))\n    logging.info(msg=parameter)\n    \n    # model = unet_3D(in_channels=4).cuda(args.gpu)\n    model = UNetSingleBranchNetwork(parameter).cuda(args.gpu)\n    \n    train_c2f(model, param=parameter)\n    test(model, parameter=parameter)\n    print(f'train-test over for {parameter.exp.exp_name}')"]}
{"filename": "trainutils/train_uncertainty_aware_mean_teacher.py", "chunked_list": ["import os\nimport math\nimport random\nimport shutil\nimport logging\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom tensorboardX import SummaryWriter", "import torch\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\nfrom test import test_all_case\nfrom val import test_single_case\nfrom utils import losses\nfrom utils.ramps import sigmoid_rampup\nfrom utils.visualize import make_curve, make_image", "from utils.ramps import sigmoid_rampup\nfrom utils.visualize import make_curve, make_image\nfrom dataloaders.utils import TwoStreamBatchSampler\n\n\nparam = None\nargs = None\n\n\ndef get_current_consistency_weight(epoch):\n    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n    return args.consistency * sigmoid_rampup(epoch, args.consistency_rampup)", "\ndef get_current_consistency_weight(epoch):\n    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n    return args.consistency * sigmoid_rampup(epoch, args.consistency_rampup)\n\n\ndef update_ema_variables(model, ema_model, alpha, global_step):\n    # Use the true average until the exponential average is more correct\n    alpha = min(1 - 1 / (global_step + 1), alpha)\n    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)", "\n\ndef train_uamt(models, optimizer, parameter, parsed_args):\n    global param, args\n    param = parameter\n    args = parsed_args\n    print(models, optimizer, parameter, parsed_args)\n    \n    model, ema_model = models\n    optimizer = optimizer[0]\n    \n    base_lr = param.exp.base_lr\n    batch_size = param.exp.batch_size\n    max_iterations = param.exp.max_iter\n    \n    best_performance = 0.0\n    iter_num = 0\n    loss = {}\n\n    db_train = param.get_dataset(split='train')\n    db_val = param.get_dataset(split='val')\n\n    def worker_init_fn(worker_id):\n        random.seed(args.seed + worker_id)\n\n    labeled_idxs = db_train.labeled_idxs\n    unlabeled_idxs = db_train.unlabeled_idxs\n    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-parameter.exp.labeled_batch_size)\n\n    trainloader = DataLoader(db_train,\n                             num_workers=4,\n                             pin_memory=True,\n                             batch_sampler=batch_sampler,\n                             worker_init_fn=worker_init_fn)\n    valloader = DataLoader(db_val, num_workers=args.val_bs, batch_size=args.val_bs)\n\n    ce_loss = CrossEntropyLoss()\n    nce_loss = losses.NegativeCrossEntropyLoss()\n    dice_loss_coarse = losses.DiceLoss(parameter.dataset.n_coarse)\n    dice_loss_fine = losses.DiceLoss(parameter.dataset.n_fine) \n\n    writer = SummaryWriter(os.path.join(parameter.path.path_to_snapshot, \"log\"))\n    logging.info(\"{} iterations per epoch\".format(len(trainloader)))\n\n    max_epoch = (max_iterations - iter_num) // (len(trainloader))\n    iterator = tqdm(range(max_epoch), ncols=100, position=0, leave=True, desc='Training Progress')\n    torch.autograd.set_detect_anomaly(True)\n    \n    for epoch_num in iterator:\n        for i_batch, sampled_batch in enumerate(trainloader):\n\n            q_im, q_lc, q_lf = sampled_batch['image'], sampled_batch['coarse'], sampled_batch['fine']\n\n            if args.gpu >= 0:\n                q_im, q_lc, q_lf = q_im.cuda(args.gpu), q_lc.cuda(args.gpu), q_lf.cuda(args.gpu)\n            else: raise RuntimeError(f'Specify a positive gpu id')\n            unlabeled_volume_batch = q_im[args.labeled_bs:]\n\n            noise = torch.clamp(torch.randn_like(unlabeled_volume_batch) * 0.1, -0.2, 0.2)\n            ema_inputs = unlabeled_volume_batch + noise\n\n            out = model(q_im)\n            out_coarse, out_fine = out['coarse_logit'], out['fine_logit']\n            soft_coarse, soft_fine = torch.softmax(out_coarse, dim=1), torch.softmax(out_fine, dim=1)\n            pred_fine = torch.argmax(soft_fine, dim=1)\n            \n            with torch.no_grad():\n                ema_out = ema_model(ema_inputs)\n                ema_out_coarse, ema_out_fine = ema_out['coarse_logit'], ema_out['fine_logit']\n            T = 8\n            \n            if param.dataset.n_dim == 3:\n                _, _, d, w, h = unlabeled_volume_batch.shape\n                volume_batch_r = unlabeled_volume_batch.repeat(2, 1, 1, 1, 1)\n                stride = volume_batch_r.shape[0] // 2\n                preds_f = torch.zeros([stride * T, param.dataset.n_fine, d, w, h], device=q_im.device)\n                \n                for i in range(T // 2):\n                    ema_inputs = volume_batch_r + torch.clamp(torch.randn_like(volume_batch_r) * 0.1, -0.2, 0.2)\n                    with torch.no_grad():\n                        ema_out2 = ema_model(ema_inputs)\n                        preds_f[2 * stride * i: 2 * stride * (i + 1)] = ema_out2['fine_logit']\n                \n                preds_f = torch.softmax(preds_f, dim=1)\n                preds_f = preds_f.reshape(T, stride, param.dataset.n_fine, d, w, h)\n                preds_f = torch.mean(preds_f, dim=0)\n                uncertainty_f = -1.0 * torch.sum(preds_f * torch.log(preds_f + 1e-6), dim=1, keepdim=True)\n                \n            elif math.floor(param.dataset.n_dim) == 2:\n                _, _, w, h = unlabeled_volume_batch.shape\n                volume_batch_r = unlabeled_volume_batch.repeat(2, 1, 1, 1)\n                stride = volume_batch_r.shape[0] // 2\n                preds_f = torch.zeros([stride * T, param.dataset.n_fine, w, h], device=q_im.device)\n                \n                for i in range(T // 2):\n                    ema_inputs = volume_batch_r + torch.clamp(torch.randn_like(volume_batch_r) * 0.1, -0.2, 0.2)\n                    with torch.no_grad():\n                        ema_out2 = ema_model(ema_inputs)\n                        preds_f[2 * stride * i: 2 * stride * (i + 1)] = ema_out2['fine_logit']\n                        \n                preds_f = torch.softmax(preds_f, dim=1)\n                preds_f = preds_f.reshape(T, stride, param.dataset.n_fine, w, h)\n                preds_f = torch.mean(preds_f, dim=0)\n                uncertainty_f = -1.0 * torch.sum(preds_f * torch.log(preds_f + 1e-6), dim=1, keepdim=True)\n\n            loss_ce1 = ce_loss(out_coarse, q_lc)\n            loss_ce2 = ce_loss(out_fine[:args.labeled_bs], q_lf[:args.labeled_bs])\n            loss_dice1 = dice_loss_coarse(soft_coarse, q_lc)\n            loss_dice2 = dice_loss_fine(soft_fine[:args.labeled_bs], q_lf[:args.labeled_bs])\n            loss['supervised loss coarse'] = 0.5 * (loss_dice1 + loss_ce1)\n            loss['supervised loss fine'] = 0.5 * (loss_dice2 + loss_ce2)\n\n            consistency_weight = get_current_consistency_weight(iter_num//150)\n            consistency_dist_f = losses.softmax_mse_loss(out_fine[args.labeled_bs:], ema_out_fine)\n            threshold = (0.75 + 0.25 * sigmoid_rampup(iter_num, max_iterations)) * np.log(2)\n            mask_f = (uncertainty_f < threshold).float()\n            consistency_loss = torch.sum(mask_f * consistency_dist_f) / (2 * torch.sum(mask_f) + 1e-16)\n\n            loss['consistency loss'] = consistency_weight * consistency_loss\n            \n            loss_sum = sum(loss.values())\n            optimizer.zero_grad()\n            loss_sum.backward()\n            optimizer.step()\n            \n            update_ema_variables(model, ema_model, args.ema_decay, iter_num)\n\n            lr_ = base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr_\n\n            iter_num = iter_num + 1\n            if args.verbose:\n                loss_names = list(loss.keys())\n                loss_values = list(map(lambda x: str(round(x.item(), 3)), loss.values()))\n                loss_log = ['*'] * (2 * len(loss_names))\n                loss_log[::2] = loss_names\n                loss_log[1::2] = loss_values\n                loss_log = '; '.join(loss_log)\n                logging.info(f\"model {parameter.exp.exp_name} iteration {iter_num} : total loss: {loss_sum.item():.3f},\\n\" + loss_log)\n            \n            if iter_num % args.draw_step == 0:\n                make_image(writer, parameter, q_im, 'image/input_image', iter_num, normalize=True)\n                make_image(writer, parameter, q_lf, 'image/fine_gt', iter_num, parameter.dataset.n_fine - 1)\n                make_image(writer, parameter, pred_fine, 'image/model1_fine_pred', iter_num, parameter.dataset.n_fine - 1)\n\n            if iter_num > 0 and iter_num % args.val_step == 0:\n                model.eval()\n                avg_metric_f = np.zeros((len(valloader), parameter.dataset.n_fine, 4))\n                for case_index, sampled_batch in enumerate(tqdm(valloader, position=1, leave=True, desc='Validation Progress')):\n                    _, batch_metric_f, _ = test_single_case(model, parameter, sampled_batch, stride_xy=round(parameter.exp.patch_size[0] * 0.7), stride_z=64, gpu_id=args.gpu)\n                    avg_metric_f[case_index] = batch_metric_f\n                \n                if avg_metric_f[:, -1, parameter.exp.eval_metric].mean() > best_performance:\n                    best_performance = avg_metric_f[:, -1, parameter.exp.eval_metric].mean()\n                    save_best = os.path.join(parameter.path.path_to_model, '{}_best_model.pth'.format(parameter.exp.exp_name))\n                    torch.save({\"model_state_dict\": model.state_dict(),\n                                \"optimizer_state_dict\": optimizer.state_dict(),\n                                \"iterations\": iter_num, \"metric\": best_performance}, save_best)\n                    logging.info(f\"save model to {save_best}\")\n                \n                for index, name in enumerate(['dsc', 'hd95', 'precision', 'recall']):\n                    writer.add_scalars(f'val/{name}', {f'fine label={i}': avg_metric_f[:, i-1, index].mean() for i in range(1, parameter.dataset.n_fine)}, iter_num)\n                    writer.add_scalars(f'val/{name}', {f'fine avg': avg_metric_f[:, -1, index].mean()}, iter_num)\n\n                logging.info(f'iteration {iter_num} : dice_score : {avg_metric_f[:, -1, 0].mean():.4f} hd95 : {avg_metric_f[:, -1, 1].mean():.4f}')\n                model.train()\n\n            if iter_num > 0 and iter_num % args.save_step == 0:\n                save_model_path = os.path.join(parameter.path.path_to_model, 'iter_' + str(iter_num) + '.pth')\n                torch.save({\"model_state_dict\": model.state_dict(),\n                                \"optimizer_state_dict\": optimizer.state_dict(),\n                                \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n\n            if iter_num >= max_iterations:\n                save_model_path = os.path.join(parameter.path.path_to_model, '{}_last_model.pth'.format(parameter.exp.exp_name))\n                torch.save({\"model_state_dict\": model.state_dict(),\n                                \"optimizer_state_dict\": optimizer.state_dict(),\n                                \"iterations\": iter_num, \"metric\": best_performance}, save_model_path)\n                logging.info(f\"save model to {save_model_path}\")\n            \n        if iter_num >= max_iterations:\n            iterator.close()\n            break\n\n    writer.close()\n    return \"Training Finished!\""]}
