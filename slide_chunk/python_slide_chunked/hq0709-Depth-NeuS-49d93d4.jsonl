{"filename": "exp_preprocess.py", "chunked_list": ["import os, argparse, logging\nfrom datetime import datetime\nimport numpy as np\n\nimport preprocess.depthneus_data  as depthneus_data\nimport evaluation.EvalScanNet as EvalScanNet\nfrom evaluation.renderer import render_depthmaps_pyrender\n\nimport utils.utils_geometry as GeoUtils\nimport utils.utils_image  as ImageUtils", "import utils.utils_geometry as GeoUtils\nimport utils.utils_image  as ImageUtils\nimport utils.utils_io as IOUtils\nimport utils.utils_normal as NormalUtils\n\nfrom confs.path import lis_name_scenes\n\nif __name__ == '__main__':\n    np.set_printoptions(precision=3)\n    np.set_printoptions(suppress=True)\n\n    FORMAT = \"[%(filename)s:%(lineno)s] %(message)s\"\n    logging.basicConfig(level=logging.INFO, format=FORMAT)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_type', type=str, default='data type')\n    args = parser.parse_args()\n    \n\n    dataset_type = args.data_type\n    \n    if dataset_type == 'scannet':\n        dir_root_scannet = '/media/hp/HKUCS2/Dataset/ScanNet'\n        dir_root_neus = f'{dir_root_scannet}/sample_neus'\n\n        for scene_name in lis_name_scenes:\n            dir_scan = f'{dir_root_scannet}/{scene_name}'\n            dir_neus = f'{dir_root_neus}/{scene_name}'\n            depthneus_data.prepare_depthneus_data_from_scannet(dir_scan, dir_neus, sample_interval=6, \n                                                b_sample = True, \n                                                b_generate_neus_data = True,\n                                                b_pred_normal = True, \n                                                b_detect_planes = False,\n                                                b_unify_labels = False) \n    \n    if dataset_type == 'private':\n        # example of processing iPhone video\n        # put a video under folder tmp_sfm_mvs or put your images under tmp_sfm_mvs/images\n        dir_depthneus = '/home/ethan/Desktop/test_sfm'\n        \n        dir_depthneus = os.path.abspath(dir_depthneus)\n        dir_sfm_mvs = os.path.abspath(f'{dir_depthneus}/tmp_sfm_mvs')\n        \n        crop_image = True\n        original_size_img = (1920, 1080)\n        cropped_size_img = (1360, 1020) # cropped images for normal estimation\n        reso_level = 1          \n        \n        # split video into frames and sample images\n        b_split_images = True\n        path_video = f'{dir_sfm_mvs}/video.MOV'\n        dir_split = f'{dir_sfm_mvs}/images_split'\n        dir_mvs_sample = f'{dir_sfm_mvs}/images' # for mvs reconstruction\n        dir_depthneus_sample = f'{dir_sfm_mvs}/images_calibrated' # remove uncalbrated images\n        dir_depthneus_sample_cropped = f'{dir_depthneus}/image'\n        \n        if b_split_images:\n            ImageUtils.split_video_to_frames(path_video, dir_split)     \n\n        # sample images\n        b_sample = True\n        sample_interval = 10\n        if b_sample:\n            rename_mode = 'order_04d'\n            ext_source = '.png' \n            ext_target = '.png'\n            ImageUtils.convert_images_type(dir_split, dir_mvs_sample, rename_mode, \n                                            target_img_size = None, ext_source = ext_source, ext_target =ext_target, \n                                            sample_interval = sample_interval)\n        \n        # SfM camera calibration\n        b_sfm = True\n        if b_sfm:\n            os.system(f'python ./preprocess/sfm_mvs.py --dir_mvs {dir_sfm_mvs} --image_width {original_size_img[0]} --image_height {original_size_img[1]} --reso_level {reso_level}')\n            \n        b_crop_image = True\n        if crop_image:\n            depthneus_data.crop_images_depthneus(dir_imgs = dir_depthneus_sample, \n                                dir_imgs_crop = dir_depthneus_sample_cropped, \n                                path_intrin = f'{dir_sfm_mvs}/intrinsics.txt', \n                                path_intrin_crop = f'{dir_depthneus}/intrinsics.txt', \n                                crop_size = cropped_size_img)\n\n            # crop depth\n            if IOUtils.checkExistence(f'{dir_sfm_mvs}/depth_calibrated'):\n                ImageUtils.crop_images(dir_images_origin = f'{dir_sfm_mvs}/depth_calibrated',\n                                            dir_images_crop = f'{dir_depthneus}/depth', \n                                            crop_size = cropped_size_img, \n                                            img_ext = '.npy')\n        \n        b_prepare_neus = True\n        if b_prepare_neus:\n            depthneus_data.prepare_depthneus_data_from_private_data(dir_depthneus, cropped_size_img, \n                                                            b_generate_neus_data = True,\n                                                                b_pred_normal = True, \n                                                                b_detect_planes = False)\n            \n    print('Done')", ""]}
{"filename": "exp_runner.py", "chunked_list": ["import cv2 as cv\nimport numpy as np\nimport os, logging, argparse, trimesh, copy\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom models.dataset import Dataset\nfrom models.fields import SDFNetwork, RenderingNetwork, SingleVarianceNetwork, NeRF", "from models.dataset import Dataset\nfrom models.fields import SDFNetwork, RenderingNetwork, SingleVarianceNetwork, NeRF\nfrom models.renderer import NeuSRenderer, extract_fields\nfrom models.nerf_renderer import NeRFRenderer\nfrom models.loss import NeuSLoss\n\nimport models.patch_match_cuda as PatchMatch\nfrom shutil import copyfile\nfrom tqdm import tqdm\nfrom icecream import ic", "from tqdm import tqdm\nfrom icecream import ic\nfrom datetime import datetime\nfrom pyhocon import ConfigFactory, HOCONConverter\n\nimport utils.utils_io as IOUtils\nimport utils.utils_geometry as GeoUtils\nimport utils.utils_image as ImageUtils\nimport utils.utils_training as TrainingUtils\n", "import utils.utils_training as TrainingUtils\n\n\nclass Runner:\n    def __init__(self, conf_path, scene_name = '', mode='train', model_type='', is_continue=False, checkpoint_id = -1):\n        # Initial setting: Genreal\n        self.device = torch.device('cuda')\n        self.conf_path = conf_path\n           \n        self.conf = ConfigFactory.parse_file(conf_path)\n        self.dataset_type = self.conf['general.dataset_type']\n        self.scan_name = self.conf['general.scan_name']\n        if len(scene_name)>0:\n            self.scan_name = scene_name\n            print(f\"**********************Scan name: {self.scan_name}\\n********************** \\n\\n\\n\")\n\n        self.exp_name = self.conf['general.exp_name']\n\n        # parse cmd args\n        self.is_continue = is_continue\n        self.checkpoint_id = checkpoint_id\n        self.mode = mode\n        self.model_type = self.conf['general.model_type']\n        if model_type != '':  # overwrite\n            self.model_type = model_type\n        self.model_list = []\n        self.writer = None\n        self.curr_img_idx = -1\n\n        self.parse_parameters()\n        self.build_dataset()\n        self.build_model()\n\n        if self.is_continue:\n            self.load_pretrained_model()\n                               \n        # recording\n        if self.mode[:5] == 'train':\n            self.file_backup()\n            with open(f'{self.base_exp_dir}/recording/config_modified.conf', 'w') as configfile:\n                configfile.write(HOCONConverter.to_hocon(self.conf))\n\n    def load_pretrained_model(self):\n        # Load checkpoint\n        latest_model_name = None\n\n        model_list_raw = os.listdir(os.path.join(self.base_exp_dir, 'checkpoints'))\n        model_list = []\n        for model_name in model_list_raw:\n            if 'pretrained_sdf' in model_name:\n                continue\n            if model_name[-3:] == 'pth' and int(model_name[5:-4]) <= self.end_iter:\n                model_list.append(model_name)\n        model_list.sort()\n        if self.checkpoint_id == -1: \n            latest_model_name = model_list[-1]\n        else:\n            if f'ckpt_{self.checkpoint_id:06d}.pth' in model_list:\n                latest_model_name = f'ckpt_{self.checkpoint_id:06d}.pth'\n            else:\n                logging.error(f\"path_ckpt is not exist. (f'ckpt_{self.checkpoint_id:06d}.pth')\")\n                exit()\n\n        if latest_model_name is not None:\n            logging.info('Find checkpoint: {}'.format(latest_model_name))\n            self.load_checkpoint(latest_model_name)\n\n    def parse_parameters(self):\n        # patch-match\n        self.thres_robust_ncc = self.conf['dataset.patchmatch_thres_ncc_robust']\n        logging.info(f'Robust ncc threshold: {self.thres_robust_ncc}')\n        self.patchmatch_start = self.conf['dataset.patchmatch_start']\n        self.patchmatch_mode =  self.conf['dataset.patchmatch_mode']\n\n        self.nvs = self.conf.get_bool('train.nvs', default=False)\n        self.save_normamap_npz = self.conf['train.save_normamap_npz']\n\n        self.base_exp_dir = os.path.join(self.conf['general.exp_dir'], self.dataset_type, self.model_type, self.scan_name, str(self.exp_name))\n        os.makedirs(self.base_exp_dir, exist_ok=True)\n        logging.info(f'Exp dir: {self.base_exp_dir}')\n\n        self.iter_step = 0\n        self.radius_norm = 1.0\n\n        # trainning parameters\n        self.end_iter = self.conf.get_int('train.end_iter')\n        self.batch_size = self.conf.get_int('train.batch_size')\n        self.validate_resolution_level = self.conf.get_int('train.validate_resolution_level')\n        self.learning_rate = self.conf.get_float('train.learning_rate')\n        self.learning_rate_milestone = self.conf.get_list('train.learning_rate_milestone')\n        self.learning_rate_factor = self.conf.get_float('train.learning_rate_factor')\n        \n        self.warm_up_end = self.conf.get_float('train.warm_up_end', default=0.0)\n        self.learning_rate_alpha = self.conf.get_float('train.learning_rate_alpha')\n        logging.info(f'Warm up end: {self.warm_up_end}; learning_rate_alpha: {self.learning_rate_alpha}')\n\n        self.use_white_bkgd = self.conf.get_bool('train.use_white_bkgd')\n        self.anneal_start = self.conf.get_float('train.anneal_start', default=0.0)\n        self.anneal_end = self.conf.get_float('train.anneal_end', default=0.0)\n\n        self.n_samples = self.conf.get_int('model.neus_renderer.n_samples')\n\n        # validate parameters\n        for attr in ['save_freq','report_freq', 'val_image_freq', 'val_mesh_freq', 'val_fields_freq', \n                        'freq_valid_points', 'freq_valid_weights',\n                        'freq_save_confidence' ]:\n            setattr(self, attr,  self.conf.get_int(f'train.{attr}'))\n\n    def build_dataset(self):\n        # dataset and directories\n        if self.dataset_type == 'indoor':\n            self.sample_range_indoor =  self.conf['dataset']['sample_range_indoor']\n            self.conf['dataset']['use_normal'] = True if self.conf['model.loss.normal_weight'] > 0 else False\n            self.use_indoor_data = True\n            logging.info(f\"Ray sample range: {self.sample_range_indoor}\")\n        elif self.dataset_type == 'dtu':\n            self.use_indoor_data = False\n            self.conf['model.sdf_network']['reverse_geoinit'] = False\n            self.conf['dataset']['use_normal'] = True if self.conf['model.loss.normal_weight'] > 0 else False\n\n            self.scan_id = int(self.scan_name[4:])\n            logging.info(f\"DTU scan ID: {self.scan_id}\")\n        else:\n            raise NotImplementedError\n        logging.info(f\"Use normal: {self.conf['dataset']['use_normal']}\")\n\n        \n        self.conf['dataset']['use_planes'] = True if self.conf['model.loss.normal_consistency_weight'] > 0 else False\n        self.conf['dataset']['use_plane_offset_loss'] = True if self.conf['model.loss.plane_offset_weight'] > 0 else False\n\n        self.conf['dataset']['data_dir']  = os.path.join(self.conf['general.data_dir'] , self.dataset_type, self.scan_name)\n        self.dataset = Dataset(self.conf['dataset'])\n\n    def build_model(self):\n        # Networks\n        params_to_train = []\n        if self.model_type == 'neus':\n            self.nerf_outside = NeRF(**self.conf['model.tiny_nerf']).to(self.device)\n            self.sdf_network_fine = SDFNetwork(**self.conf['model.sdf_network']).to(self.device)\n            self.variance_network_fine = SingleVarianceNetwork(**self.conf['model.variance_network']).to(self.device)\n            self.color_network_fine = RenderingNetwork(**self.conf['model.rendering_network']).to(self.device)\n            params_to_train += list(self.nerf_outside.parameters())\n            params_to_train += list(self.sdf_network_fine.parameters())\n            params_to_train += list(self.variance_network_fine.parameters())\n            params_to_train += list(self.color_network_fine.parameters())\n\n            self.renderer = NeuSRenderer(self.nerf_outside,\n                                self.sdf_network_fine,\n                                self.variance_network_fine,\n                                self.color_network_fine,\n                                **self.conf['model.neus_renderer'])\n        \n        elif self.model_type == 'nerf':  # NeRF\n            self.nerf_coarse = NeRF(**self.conf['model.nerf']).to(self.device)\n            self.nerf_fine = NeRF(**self.conf['model.nerf']).to(self.device)\n            self.nerf_outside = NeRF(**self.conf['model.tiny_nerf']).to(self.device)\n            params_to_train += list(self.nerf_coarse.parameters())\n            params_to_train += list(self.nerf_fine.parameters())\n            params_to_train += list(self.nerf_outside.parameters())\n\n            self.renderer = NeRFRenderer(self.nerf_coarse,\n                                self.nerf_fine,\n                                self.nerf_outside,\n                                **self.conf['model.nerf_renderer'])\n                                         \n        else:\n            NotImplementedError\n\n        self.optimizer = torch.optim.Adam(params_to_train, lr=self.learning_rate)\n                \n        self.loss_neus = NeuSLoss(self.conf['model.loss'])\n\n    def train(self):\n        if self.model_type == 'neus':\n            self.train_neus()\n        elif self.model_type == 'nerf':\n            self.train_nerf()\n        else:\n            NotImplementedError\n\n    def get_near_far(self, rays_o, rays_d,  image_perm = None, iter_i= None, pixels_x = None, pixels_y= None):\n        log_vox = {}\n        log_vox.clear()\n        batch_size = len(rays_o)\n        if  self.dataset_type == 'dtu':\n            near, far = self.dataset.near_far_from_sphere(rays_o, rays_d)\n        elif self.dataset_type == 'indoor':\n            near, far = torch.zeros(batch_size, 1), self.sample_range_indoor * torch.ones(batch_size, 1)\n        else:\n            NotImplementedError\n\n        logging.debug(f\"[{self.iter_step}] Sample range: max, {torch.max(far -near)}; min, {torch.min(far -near)}\")\n        return near, far, log_vox\n\n    def get_model_input(self, image_perm, iter_i):\n        input_model = {}\n\n        idx_img = image_perm[self.iter_step % self.dataset.n_images]\n        self.curr_img_idx = idx_img\n        data, pixels_x, pixels_y,  normal_sample, planes_sample, subplanes_sample, = self.dataset.random_get_rays_at(idx_img, self.batch_size)\n        \n        rays_o, rays_d, true_rgb, true_mask, pts_target = data[:, :3], data[:, 3: 6], data[:, 6: 9], data[:, 9: 10],data[:,10:13]\n        true_mask =  (true_mask > 0.5).float()\n        mask = true_mask\n\n        if self.conf['model.loss.normal_weight'] > 0:\n            input_model['normals_gt'] = normal_sample\n        \n        if self.conf['model.loss.normal_consistency_weight'] > 0:\n            input_model['planes_gt'] = planes_sample\n\n        if self.conf['model.loss.plane_offset_weight'] > 0:\n            input_model['subplanes_gt'] = subplanes_sample\n\n        near, far, logs_input = self.get_near_far(rays_o, rays_d,  image_perm, iter_i, pixels_x, pixels_y)\n        \n        background_rgb = None\n        if self.use_white_bkgd:\n            background_rgb = torch.ones([1, 3])\n\n        if self.conf['model.loss.mask_weight'] > 0.0:\n            mask = (mask > 0.5).float()\n        else:\n            mask = torch.ones_like(mask)\n\n        mask_sum = mask.sum() + 1e-5\n\n        pixels_uv = torch.stack([pixels_x, pixels_y], dim=-1)\n        pixels_vu = torch.stack([pixels_y, pixels_x], dim=-1)\n        input_model.update({\n            'rays_o': rays_o,\n            'rays_d': rays_d,\n            'near': near,\n            'far': far,\n            'depth_gt': pts_target,\n            'mask': mask,\n            'mask_sum': mask_sum,\n            'true_rgb': true_rgb,\n            'background_rgb': background_rgb,\n            'pixels_x': pixels_x,  # u\n            'pixels_y': pixels_y,   # v,\n            'pixels_uv': pixels_uv,\n            'pixels_vu': pixels_vu\n        })\n        return input_model, logs_input\n\n    def train_neus(self):\n        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))\n        self.update_learning_rate()\n        self.update_iter_step()\n        res_step = self.end_iter - self.iter_step\n\n        if self.dataset.cache_all_data:\n            self.dataset.shuffle()\n\n        # self.validate_mesh() # save mesh at iter 0\n        logs_summary = {}\n        image_perm = torch.randperm(self.dataset.n_images)\n        for iter_i in tqdm(range(res_step)):  \n            logs_summary.clear()\n\n            input_model, logs_input = self.get_model_input(image_perm, iter_i)\n            logs_summary.update(logs_input)\n\n            render_out, logs_render = self.renderer.render(input_model['rays_o'], input_model['rays_d'], \n                                            input_model['near'], input_model['far'],\n                                            background_rgb=input_model['background_rgb'],\n                                            alpha_inter_ratio=self.get_alpha_inter_ratio())\n            logs_summary.update(logs_render)\n\n            patchmatch_out, logs_patchmatch = self.patch_match(input_model, render_out)\n            logs_summary.update(logs_patchmatch)\n\n            loss, logs_loss, mask_keep_gt_normal = self.loss_neus(input_model, render_out, self.sdf_network_fine, patchmatch_out)\n            logs_summary.update(logs_loss)\n\n            self.optimizer.zero_grad()\n\n            loss.backward()\n            self.optimizer.step()\n\n\n            self.iter_step += 1\n\n            logs_val = self.validate(input_model, logs_loss, render_out)\n            logs_summary.update(logs_val)\n            \n            logs_summary.update({'Log/lr': self.optimizer.param_groups[0]['lr']})\n            self.write_summary(logs_summary)\n\n            self.update_learning_rate()\n            self.update_iter_step()\n            self.accumulate_rendered_results(input_model, render_out, patchmatch_out,\n                                                    b_accum_render_difference = False,\n                                                    b_accum_ncc = False,\n                                                    b_accum_normal_pts = False)\n\n            if self.iter_step % self.dataset.n_images == 0:\n                image_perm = torch.randperm(self.dataset.n_images)\n\n            if self.iter_step % 1000 == 0:\n                torch.cuda.empty_cache()\n\n        logging.info(f'Done. [{self.base_exp_dir}]')\n\n    def calculate_ncc_samples(self, input_model, render_out, \n                                    use_peak_value = True, \n                                    use_normal_prior = False):\n        pixels_coords_vu =input_model['pixels_vu'] \n        \n        if use_peak_value:\n            pts_render = render_out['point_peak']\n            normals_render = render_out['normal_peak']\n        else:\n            raise NotImplementedError\n        \n        if use_normal_prior:\n            normals_render = input_model['normals_gt']\n\n        pts_all = pts_render[:, None, :]\n        normals_all = normals_render[:, None, :]\n        coords_all = pixels_coords_vu[:, None, :]\n\n        with torch.no_grad():\n            scores_ncc_all, diff_patch_all, mask_valid_all = self.dataset.score_pixels_ncc(self.curr_img_idx, pts_all.reshape(-1, 3), \n                                                                                                normals_all.reshape(-1, 3), \n                                                                                                coords_all.reshape(-1, 2))\n            num_valid = mask_valid_all.sum()\n            scores_ncc_all = scores_ncc_all.reshape(self.batch_size, -1)\n            mask_valid_all = mask_valid_all.reshape(self.batch_size, -1)\n        return scores_ncc_all, diff_patch_all, mask_valid_all\n                      \n    def patch_match(self, input_model, render_out):\n        patchmatch_out, logs_summary = None, {}\n        if self.iter_step > self.patchmatch_start:\n            # ensure initialization of confidence_map, depth_map and points_map\n            if self.dataset.confidence_accum is None:\n                self.initialize_accumulated_results(mode=self.conf['dataset.mode_init_accum'], \n                                                    iter_step_npz=self.conf['dataset.init_accum_step_npz'],\n                                                    resolution_level=self.conf['dataset.init_accum_reso_level'])\n\n            if self.patchmatch_mode == 'use_geocheck':\n                scores_ncc, diffs_patch, mask_valid = self.calculate_ncc_samples(input_model, render_out)\n\n                # (1) mask of pixels with high confidence\n                mask_high_confidence_curr = scores_ncc < self.thres_robust_ncc\n\n                # (2) check previous ncc confidence\n                pixels_u, pixels_v = input_model['pixels_x'].cpu().numpy(), input_model['pixels_y'].cpu().numpy()\n                ncc_prev = self.dataset.confidence_accum[self.curr_img_idx][pixels_v, pixels_u]\n                mask_high_confidence_prev = ncc_prev < self.thres_robust_ncc\n\n                # (3) remove normals with large difference between rendered normal and gt normal\n                # large difference means large inconsistency of normal between differenct views\n                normal_render = render_out['normal_peak']\n                angles_diff = TrainingUtils.get_angles(normal_render, input_model['normals_gt'])\n                MAX_ANGLE_DIFF = 30.0 / 180.0 * 3.1415926\n                mask_small_normal_diffence = angles_diff < MAX_ANGLE_DIFF\n\n                # (4) merge masks\n                mask_use_gt = mask_high_confidence_curr & torch.from_numpy(mask_high_confidence_prev[:,None]).cuda() & mask_small_normal_diffence\n                \n                # (5) update confidence, discard the normals\n                mask_not_use_gt = (mask_use_gt==False)\n                scores_ncc_all2 = copy.deepcopy(scores_ncc)\n                scores_ncc_all2[mask_not_use_gt] = 1.0\n                self.dataset.confidence_accum[self.curr_img_idx][pixels_v, pixels_u] = scores_ncc_all2.squeeze().cpu().numpy()\n        \n                idx_scores_min = mask_use_gt.long()  # 1 use gt, 0 not\n                patchmatch_out = {\n                    'patchmatch_mode': self.patchmatch_mode,\n                    'scores_ncc_all': scores_ncc,\n                    'idx_scores_min': idx_scores_min,\n                }\n                logs_summary.update({\n                    'Log/pixels_use_volume_rendering_only': (idx_scores_min==0).sum(),\n                    'Log/pixels_use_prior': (idx_scores_min==1).sum(),\n                })\n                \n            else:\n                raise NotImplementedError\n\n        return patchmatch_out, logs_summary\n\n    def accumulate_rendered_results(self, input_model, render_out, patchmatch_out,\n                                        b_accum_render_difference = False,\n                                        b_accum_ncc = False,\n                                        b_accum_normal_pts = False):\n        '''Cache rendererd depth, normal and confidence (if avaliable) in the training process\n        \n        Args:\n            scores: (N,)\n        '''\n        pixels_u, pixels_v = input_model['pixels_x'], input_model['pixels_y']\n        pixels_u_np, pixels_v_np = pixels_u.cpu().numpy(), pixels_v.cpu().numpy()\n        if b_accum_render_difference:\n            color_gt = self.dataset.images_denoise_np[self.curr_img_idx.item()][pixels_v_np, pixels_u_np]\n            color_render = render_out['color_fine'].detach().cpu.numpy()\n            diff = np.abs(color_render - color_gt).sum(axis=-1)\n            self.dataset.render_difference_accum[self.curr_img_idx.item()][pixels_v_np, pixels_u_np] = diff\n\n        if b_accum_ncc:\n            scores_vr = patchmatch_out['scores_samples'] \n            self.dataset.confidence_accum[self.curr_img_idx.item()][pixels_v_np, pixels_u_np] = scores_vr.cpu().numpy()\n   \n        if b_accum_normal_pts:\n            point_peak = render_out['point_peak']\n            self.dataset.points_accum[self.curr_img_idx][pixels_v, pixels_u]  = point_peak.detach().cpu()\n\n            normal_peak = render_out['normal_peak']\n            self.dataset.normals_accum[self.curr_img_idx][pixels_v, pixels_u] = normal_peak.detach().cpu()\n\n        b_accum_all_data = False\n        if b_accum_all_data:\n            self.dataset.depths_accum[self.curr_img_idx][pixels_v, pixels_u]  = render_out['depth_peak'].squeeze()\n            self.dataset.colors_accum[self.curr_img_idx][pixels_v, pixels_u]  = render_out['color_peak']\n\n    def write_summary(self, logs_summary):\n        for key in logs_summary:\n            self.writer.add_scalar(key, logs_summary[key], self.iter_step )\n\n    def update_iter_step(self):\n        self.sdf_network_fine.iter_step = self.iter_step\n        self.sdf_network_fine.end_iter = self.end_iter\n        \n        self.loss_neus.iter_step = self.iter_step\n        self.loss_neus.iter_end = self.end_iter\n\n        self.dataset.iter_step = self.iter_step\n\n    def get_alpha_inter_ratio(self):\n        if self.anneal_end == 0.0:\n            return 1.0\n        elif self.iter_step < self.anneal_start:\n            return 0.0\n        else:\n            return np.min([1.0, (self.iter_step - self.anneal_start) / (self.anneal_end - self.anneal_start)])\n\n    def get_alpha_inter_ratio_decrease(self):\n        anneal_end = 5e4\n        anneal_start = 0\n        \n        if anneal_end == 0.0:\n            return 0.0\n        elif self.iter_step < anneal_start:\n            return 1.0\n        else:\n            return 1.0 - np.min([1.0, (self.iter_step - anneal_start) / (anneal_end - anneal_start)])\n\n    def update_learning_rate(self):\n        if self.iter_step < self.warm_up_end:\n            learning_factor = self.iter_step / self.warm_up_end\n        else:\n            alpha = self.learning_rate_alpha\n            progress = (self.iter_step - self.warm_up_end) / (self.end_iter - self.warm_up_end)\n            learning_factor = (np.cos(np.pi * progress) + 1.0) * 0.5 * (1 - alpha) + alpha\n\n        for g in self.optimizer.param_groups:\n            g['lr'] = self.learning_rate * learning_factor\n\n    def train_nerf(self):\n        self.writer = SummaryWriter(log_dir=os.path.join(self.base_exp_dir, 'logs'))\n        self.update_learning_rate()\n        res_step = self.end_iter - self.iter_step\n        image_perm = self.get_image_perm()\n\n        for iter_i in tqdm(range(res_step)):\n            data, _, _, normal_sample, planes_sample, subplanes_sample, mask_normal_certain_sample = self.dataset.random_get_rays_at(image_perm[self.iter_step % len(image_perm)], self.batch_size)\n\n            rays_o, rays_d, true_rgb, mask = data[:, :3], data[:, 3: 6], data[:, 6: 9], data[:, 9: 10]\n            \n            if self.dataset_type == 'dtu':\n                near, far = self.dataset.near_far_from_sphere(rays_o, rays_d)\n            elif self.dataset_type == 'indoor':\n                near, far = torch.zeros(len(rays_o), 1), self.sample_range_indoor * torch.ones(len(rays_o), 1)\n            else:\n                NotImplementedError\n\n            background_rgb = None\n            if self.use_white_bkgd:\n                background_rgb = torch.ones([1, 3])\n\n            if self.conf['model.loss.mask_weight'] > 0.0:\n                mask = (mask > 0.5).float()\n            else:\n                mask = torch.ones_like(mask)\n\n            mask_sum = mask.sum() + 1e-5\n            render_out = self.renderer.render(rays_o, rays_d, near, far,\n                                              background_rgb=background_rgb)\n\n            color_coarse = render_out['color_coarse']\n            color_fine = render_out['color_fine']\n            weight_sum = render_out['weight_sum']\n\n            # Color loss\n            color_coarse_error = (color_coarse - true_rgb) * mask\n            color_coarse_loss = F.mse_loss(color_coarse_error, torch.zeros_like(color_coarse_error), reduction='sum') / mask_sum\n            color_fine_error = (color_fine - true_rgb) * mask\n            color_fine_loss = F.mse_loss(color_fine_error, torch.zeros_like(color_fine_error), reduction='sum') / mask_sum\n\n            psnr = 20.0 * torch.log10(1.0 / (((color_fine - true_rgb)**2 * mask).sum() / (mask_sum * 3.0)).sqrt())\n\n            # Mask loss, optional\n            mask_loss = F.binary_cross_entropy(weight_sum.clip(1e-3, 1.0 - 1e-3), mask)\n\n            loss = color_coarse_loss + color_fine_loss\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            self.iter_step += 1\n\n            self.writer.add_scalar('Loss/loss', loss, self.iter_step)\n            self.writer.add_scalar('Loss/color_loss', color_fine_loss, self.iter_step)\n            self.writer.add_scalar('Loss/color_coarse_loss', color_coarse_loss, self.iter_step)\n\n            self.writer.add_scalar('Log/psnr', psnr, self.iter_step)\n            self.writer.add_scalar('Log/lr', self.optimizer.param_groups[0]['lr'], self.iter_step)\n\n            if self.iter_step % self.report_freq == 0:\n                print(self.base_exp_dir)\n                logging.info('iter:{:8>d} loss = {} lr={}'.format(self.iter_step, loss, self.optimizer.param_groups[0]['lr']))\n\n            if self.iter_step % self.save_freq == 0:\n                self.save_checkpoint()\n\n            if self.iter_step % self.val_image_freq == 0:\n                self.validate_image_nerf()\n\n            if self.iter_step % self.val_mesh_freq == 0:\n                self.validate_mesh_nerf()\n\n            if self.iter_step % self.freq_eval_mesh == 0:\n                self.validate_mesh_nerf(world_space=False, resolution=512, threshold=25)\n\n            self.update_learning_rate()\n            self.dataset.iter_step = self.iter_step\n\n            if self.iter_step % len(image_perm) == 0:\n                image_perm = self.get_image_perm()\n\n    def get_image_perm(self):\n        if not self.nvs:\n            return torch.randperm(self.dataset.n_images)\n        else:\n            lis = [i for i in range(self.dataset.n_images) if not i in [8, 13, 16, 21, 26, 31, 34, 56]]\n            lis = torch.tensor(lis, dtype=torch.long)\n            return lis[torch.randperm(len(lis))]\n\n    def file_backup(self):\n        # copy python file\n        dir_lis = self.conf['general.recording']\n        os.makedirs(os.path.join(self.base_exp_dir, 'recording'), exist_ok=True)\n        for dir_name in dir_lis:\n            cur_dir = os.path.join(self.base_exp_dir, 'recording', dir_name)\n            os.makedirs(cur_dir, exist_ok=True)\n            files = os.listdir(dir_name)\n            for f_name in files:\n                if f_name[-3:] == '.py':\n                    copyfile(os.path.join(dir_name, f_name), os.path.join(cur_dir, f_name))\n\n        # copy configs\n        copyfile(self.conf_path, os.path.join(self.base_exp_dir, 'recording', 'config.conf'))\n\n    def load_checkpoint(self, checkpoint_name):\n        checkpoint = torch.load(os.path.join(self.base_exp_dir, 'checkpoints', checkpoint_name), map_location=self.device)\n        if self.model_type == 'neus':\n            self.nerf_outside.load_state_dict(checkpoint['nerf'])\n            self.sdf_network_fine.load_state_dict(checkpoint['sdf_network_fine'])\n            self.variance_network_fine.load_state_dict(checkpoint['variance_network_fine'])\n            self.color_network_fine.load_state_dict(checkpoint['color_network_fine'])\n            self.optimizer.load_state_dict(checkpoint['optimizer'])\n            self.iter_step = checkpoint['iter_step']\n\n        elif self.model_type == 'nerf':\n            self.nerf_coarse.load_state_dict(checkpoint['nerf_coarse'])\n            self.nerf_fine.load_state_dict(checkpoint['nerf_fine'])\n            self.nerf_outside.load_state_dict(checkpoint['nerf_outside'])\n            self.optimizer.load_state_dict(checkpoint['optimizer'])\n            self.iter_step = checkpoint['iter_step']\n            \n        else:\n            NotImplementedError\n\n    def save_checkpoint(self):\n        checkpoint = None\n        if self.model_type == 'neus':\n            checkpoint = {\n                'nerf': self.nerf_outside.state_dict(),\n                'sdf_network_fine': self.sdf_network_fine.state_dict(),\n                'variance_network_fine': self.variance_network_fine.state_dict(),\n                'color_network_fine': self.color_network_fine.state_dict(),\n                'optimizer': self.optimizer.state_dict(),\n                'iter_step': self.iter_step,\n            }\n\n        elif self.model_type == 'nerf':\n            checkpoint = {\n                'nerf_coarse': self.nerf_coarse.state_dict(),\n                'nerf_fine': self.nerf_fine.state_dict(),\n                'nerf_outside': self.nerf_outside.state_dict(),\n                'optimizer': self.optimizer.state_dict(),\n                'iter_step': self.iter_step,\n            }\n\n        else:\n            NotImplementedError\n\n        os.makedirs(os.path.join(self.base_exp_dir, 'checkpoints'), exist_ok=True)\n        torch.save(checkpoint, os.path.join(self.base_exp_dir, 'checkpoints', 'ckpt_{:0>6d}.pth'.format(self.iter_step)))\n\n    def validate(self, input_model, loss_out, render_out):\n        mask, rays_o, rays_d, near, far = input_model['mask'], input_model['rays_o'], input_model['rays_d'],  \\\n                                                    input_model['near'], input_model['far']\n        mask_sum = mask.sum() + 1e-5\n        loss, psnr = loss_out['Loss/loss'], loss_out['Log/psnr']\n\n        log_val = {}\n        if self.iter_step % self.report_freq == 0:\n            print('\\n', self.base_exp_dir)\n            logging.info('iter:{:8>d} loss={:.03f} lr={:.06f} var={:.04f}'.format(self.iter_step, loss, self.optimizer.param_groups[0]['lr'], render_out['variance'].mean()))\n            ic((render_out['weight_sum'] * mask).sum() / mask_sum)\n            ic(self.get_alpha_inter_ratio())\n            ic(psnr)\n\n        if self.iter_step % self.save_freq == 0:\n            self.save_checkpoint()\n\n        if self.iter_step % self.val_mesh_freq == 0 or self.iter_step == 1:\n            self.validate_mesh()\n        \n        if self.iter_step % self.val_image_freq == 0:\n            self.validate_image(save_normalmap_npz=self.save_normamap_npz)\n\n        if self.iter_step % self.val_fields_freq == 0:\n            self.validate_fields()\n        \n        if self.iter_step % self.freq_valid_points == 0:\n            self.validate_points(rays_o, rays_d, near, far)\n            \n        if self.iter_step % self.freq_save_confidence == 0:\n            self.save_accumulated_results()\n            self.save_accumulated_results(idx=self.dataset.n_images//2)\n    \n        return log_val\n\n    def validate_image(self, idx=-1, resolution_level=-1, \n                                save_normalmap_npz = False, \n                                save_peak_value = False, \n                                validate_confidence = True,\n                                save_image_render = False):\n        # validate image\n        ic(self.iter_step, idx)\n        logging.info(f'Validate begin: idx {idx}...')\n        if idx < 0:\n            idx = np.random.randint(self.dataset.n_images)\n\n        if resolution_level < 0:\n            resolution_level = self.validate_resolution_level\n        \n        imgs_render = {}\n        for key in ['color_fine', 'confidence', 'normal', 'depth', 'variance_surface', 'confidence_mask']:\n            imgs_render[key] = []\n        \n        if save_peak_value:\n            imgs_render.update({\n                'color_peak': [], \n                'normal_peak': [], \n                'depth_peak':[]\n            })\n            pts_peak_all = []\n\n        # (1) render images\n        rays_o, rays_d = self.dataset.gen_rays_at(idx, resolution_level=resolution_level)\n\n        H, W, _ = rays_o.shape\n        rays_o = rays_o.reshape(-1, 3).split(self.batch_size)\n        rays_d = rays_d.reshape(-1, 3).split(self.batch_size)\n        idx_pixels_vu = torch.tensor([[i, j] for i in range(0, H) for j in range(0, W)]).split(self.batch_size)\n        for rays_o_batch, rays_d_batch, idx_pixels_vu_batch in zip(rays_o, rays_d, idx_pixels_vu):\n            near, far, _ = self.get_near_far(rays_o = rays_o_batch, rays_d = rays_d_batch)\n            background_rgb = torch.ones([1, 3]) if self.use_white_bkgd else None\n\n            render_out, _ = self.renderer.render(rays_o_batch, rays_d_batch, near, far, alpha_inter_ratio=self.get_alpha_inter_ratio(), background_rgb=background_rgb)\n            feasible = lambda key: ((key in render_out) and (render_out[key] is not None))\n\n            for key in imgs_render:\n                if feasible(key):\n                    imgs_render[key].append(render_out[key].detach().cpu().numpy())\n                    \n            if validate_confidence:\n                pts_peak = rays_o_batch + rays_d_batch * render_out['depth_peak']\n                scores_all_mean, diff_patch_all, mask_valid_all = self.dataset.score_pixels_ncc(idx, pts_peak, render_out['normal_peak'], idx_pixels_vu_batch, reso_level=resolution_level)\n                \n                imgs_render['confidence'].append(scores_all_mean.detach().cpu().numpy()[:,None])\n                imgs_render['confidence_mask'].append(mask_valid_all.detach().cpu().numpy()[:,None])\n                if save_peak_value:\n                    pts_peak_all.append(pts_peak.detach().cpu().numpy())\n\n            del render_out\n\n        # (2) reshape rendered images\n        for key in imgs_render:\n            if len(imgs_render[key]) > 0:\n                imgs_render[key] = np.concatenate(imgs_render[key], axis=0)\n                if imgs_render[key].shape[1] == 3: # for color and normal\n                    imgs_render[key] = imgs_render[key].reshape([H, W, 3])\n                elif imgs_render[key].shape[1] == 1:  \n                    imgs_render[key] = imgs_render[key].reshape([H, W])\n\n        # confidence map\n        if save_normalmap_npz:\n            # For each view, save point(.npz), depthmap(.png) point cloud(.ply), normalmap(.png), normal(.npz)\n            # rendered depth in volume rednering and projected depth of points in world are different\n            shape_depthmap = imgs_render['depth'].shape[:2]\n            pts_world = torch.vstack(rays_o).cpu().numpy() +  torch.vstack(rays_d).cpu().numpy() * imgs_render['depth'].reshape(-1,1)\n            os.makedirs(os.path.join(self.base_exp_dir, 'depth'), exist_ok=True)\n            GeoUtils.save_points(os.path.join(self.base_exp_dir, 'depth', f'{self.iter_step:0>8d}_{idx}_reso{resolution_level}.ply'),\n                                    pts_world.reshape(-1,3),\n                                    colors = imgs_render['color_fine'].squeeze().reshape(-1,3),\n                                    normals= imgs_render['normal'].squeeze().reshape(-1,3),\n                                    BRG2RGB=True)\n            \n            # save peak depth and normal\n            if save_peak_value:\n                pts_world_peak = torch.vstack(rays_o).cpu().numpy() +  torch.vstack(rays_d).cpu().numpy() * imgs_render['depth_peak'].reshape(-1,1)\n                os.makedirs(os.path.join(self.base_exp_dir, 'depth'), exist_ok=True)\n                GeoUtils.save_points(os.path.join(self.base_exp_dir, 'depth', f'{self.iter_step:0>8d}_{idx}_reso{resolution_level}_peak.ply'),\n                                        pts_world_peak.reshape(-1,3),\n                                        colors = imgs_render['color_peak'].squeeze().reshape(-1,3),\n                                        normals= imgs_render['normal_peak'].squeeze().reshape(-1,3),\n                                        BRG2RGB=True)\n\n                os.makedirs(os.path.join(self.base_exp_dir, 'normal_peak'), exist_ok=True)\n                np.savez(os.path.join(self.base_exp_dir, 'normal_peak', f'{self.iter_step:08d}_{self.dataset.vec_stem_files[idx]}_reso{resolution_level}.npz'), \n                            imgs_render['normal_peak'].squeeze())\n                \n                os.makedirs(os.path.join(self.base_exp_dir, 'normal_render'), exist_ok=True)\n                np.savez(os.path.join(self.base_exp_dir, 'normal_render', f'{self.iter_step:08d}_{self.dataset.vec_stem_files[idx]}_reso{resolution_level}.npz'), \n                            imgs_render['normal'].squeeze())\n                \n                pts_world2 = pts_world_peak.reshape([H, W, 3])\n                np.savez(os.path.join(self.base_exp_dir, 'depth', f'{self.iter_step:08d}_{idx}_reso{resolution_level}_peak.npz'),\n                            pts_world2)\n            \n        if save_image_render:\n            os.makedirs(os.path.join(self.base_exp_dir, 'image_render'), exist_ok=True)\n            ImageUtils.write_image(os.path.join(self.base_exp_dir, 'image_render', f'{self.iter_step:08d}_{self.dataset.vec_stem_files[idx]}_reso{resolution_level}.png'), \n                        imgs_render['color_fine']*255)\n            psnr_render = 20.0 * torch.log10(1.0 / (((self.dataset.images[idx] - torch.from_numpy(imgs_render['color_fine']))**2).sum() / (imgs_render['color_fine'].size * 3.0)).sqrt())\n            \n            os.makedirs(os.path.join(self.base_exp_dir, 'image_peak'), exist_ok=True)\n            ImageUtils.write_image(os.path.join(self.base_exp_dir, 'image_peak', f'{self.iter_step:08d}_{self.dataset.vec_stem_files[idx]}_reso{resolution_level}.png'), \n                        imgs_render['color_peak']*255)    \n            psnr_peak = 20.0 * torch.log10(1.0 / (((self.dataset.images[idx] - torch.from_numpy(imgs_render['color_peak']))**2).sum() / (imgs_render['color_peak'].size * 3.0)).sqrt())\n            \n            print(f'PSNR (rener, peak): {psnr_render}  {psnr_peak}')\n        \n        # (3) save images\n        lis_imgs = []\n        img_sample = np.zeros_like(imgs_render['color_fine'])\n        img_sample[:,:,1] = 255\n        for key in imgs_render:\n            if len(imgs_render[key]) == 0 or key =='confidence_mask':\n                continue\n\n            img_temp = imgs_render[key]\n            if key in ['normal', 'normal_peak']:\n                img_temp = (((img_temp + 1) * 0.5).clip(0,1) * 255).astype(np.uint8)\n            if key in ['depth', 'depth_peak', 'variance']:\n                img_temp = img_temp / (np.max(img_temp)+1e-6) * 255\n            if key == 'confidence':\n                img_temp2 = ImageUtils.convert_gray_to_cmap(img_temp)\n                img_temp2 = img_temp2 * imgs_render['confidence_mask'][:,:,None]\n                lis_imgs.append(img_temp2)\n\n                img_temp3_mask_use_prior = (img_temp<self.thres_robust_ncc)\n                lis_imgs.append(img_temp3_mask_use_prior*255)\n                img_sample[img_temp==1.0] = (255,0,0)\n                logging.info(f'Pixels: {img_temp3_mask_use_prior.size}; Use prior: {img_temp3_mask_use_prior.sum()}; Invalid patchmatch: {(img_temp==1.0).sum()}')\n                logging.info(f'Ratio: Use prior: {img_temp3_mask_use_prior.sum()/img_temp3_mask_use_prior.size}; Invalid patchmatch: {(img_temp==1.0).sum()/img_temp3_mask_use_prior.size}')\n\n                img_temp = img_temp.clip(0,1) * 255\n            if key in ['color_fine', 'color_peak', 'confidence_mask']:\n                img_temp *= 255\n\n            cv.putText(img_temp, key,  (img_temp.shape[1]-100, img_temp.shape[0]-20), \n                fontFace= cv.FONT_HERSHEY_SIMPLEX, \n                fontScale = 0.5, \n                color = (0, 0, 255), \n                thickness = 2)\n\n            lis_imgs.append(img_temp)\n        \n        dir_images = os.path.join(self.base_exp_dir, 'images')\n        os.makedirs(dir_images, exist_ok=True)\n        img_gt = ImageUtils.resize_image(self.dataset.images[idx].cpu().numpy(), \n                                            (lis_imgs[0].shape[1], lis_imgs[0].shape[0]))\n        img_sample[img_temp3_mask_use_prior] = img_gt[img_temp3_mask_use_prior]*255\n        ImageUtils.write_image_lis(f'{dir_images}/{self.iter_step:08d}_reso{resolution_level}_{idx:08d}.png',\n                                        [img_gt, img_sample] + lis_imgs)\n\n        if save_peak_value:\n            pts_peak_all = np.concatenate(pts_peak_all, axis=0)\n            pts_peak_all = pts_peak_all.reshape([H, W, 3])\n\n            return imgs_render['confidence'], imgs_render['color_peak'], imgs_render['normal_peak'], imgs_render['depth_peak'], pts_peak_all, imgs_render['confidence_mask']\n\n    def compare_ncc_confidence(self, idx=-1, resolution_level=-1):\n        # validate image\n        ic(self.iter_step, idx)\n        logging.info(f'Validate begin: idx {idx}...')\n        if idx < 0:\n            idx = np.random.randint(self.dataset.n_images)\n\n        if resolution_level < 0:\n            resolution_level = self.validate_resolution_level\n\n        # (1) render images\n        rays_o, rays_d = self.dataset.gen_rays_at(idx, resolution_level=resolution_level)\n\n        H, W, _ = rays_o.shape\n        rays_o = rays_o.reshape(-1, 3).split(self.batch_size)\n        rays_d = rays_d.reshape(-1, 3).split(self.batch_size)\n\n        normals_gt = self.dataset.normals[idx].cuda()\n        normals_gt = normals_gt.reshape(-1, 3).split(self.batch_size)\n        scores_render_all, scores_gt_all = [], []\n\n        idx_pixels_vu = torch.tensor([[i, j] for i in range(0, H) for j in range(0, W)]).split(self.batch_size)\n        for rays_o_batch, rays_d_batch, idx_pixels_vu_batch, normals_gt_batch in zip(rays_o, rays_d, idx_pixels_vu, normals_gt):\n            near, far, _ = self.get_near_far(rays_o = rays_o_batch, rays_d = rays_d_batch)\n            background_rgb = torch.ones([1, 3]) if self.use_white_bkgd else None\n\n            render_out, _ = self.renderer.render(rays_o_batch, rays_d_batch, near, far, alpha_inter_ratio=self.get_alpha_inter_ratio(), background_rgb=background_rgb)\n\n\n            pts_peak = rays_o_batch + rays_d_batch * render_out['depth_peak']\n            scores_render, diff_patch_all, mask_valid_all = self.dataset.score_pixels_ncc(idx, pts_peak, render_out['normal_peak'], idx_pixels_vu_batch, reso_level=resolution_level)\n            scores_gt, diff_patch_all_gt, mask_valid_all_gt = self.dataset.score_pixels_ncc(idx, pts_peak, normals_gt_batch, idx_pixels_vu_batch, reso_level=resolution_level)\n            \n            scores_render_all.append(scores_render.cpu().numpy())\n            scores_gt_all.append(scores_gt.cpu().numpy())\n        scores_render_all = np.concatenate(scores_render_all, axis=0).reshape([H, W])\n        scores_gt_all = np.concatenate(scores_gt_all, axis=0).reshape([H, W])\n\n        mask_filter =( (scores_gt_all -0.01) < scores_render_all)\n\n        img_gr=np.zeros((H, W, 3), dtype=np.uint8)\n        img_gr[:,:, 0] = 255 \n        img_gr[mask_filter==False] = (0,0,255)\n\n        img_rgb =np.zeros((H, W, 3), dtype=np.uint8) \n        img_rgb[mask_filter==False] = self.dataset.images_np[idx][mask_filter==False]*255\n\n        ImageUtils.write_image_lis(f'./test/sampling/rgb_{idx:04d}.png', [self.dataset.images_np[idx]*256, mask_filter*255, img_gr, img_rgb])\n\n    def validate_mesh(self, world_space=False, resolution=128, threshold=0.0):\n        bound_min = torch.tensor(self.dataset.bbox_min, dtype=torch.float32) #/ self.sdf_network_fine.scale\n        bound_max = torch.tensor(self.dataset.bbox_max, dtype=torch.float32) # / self.sdf_network_fine.scale\n        vertices, triangles, sdf = self.renderer.extract_geometry(bound_min, bound_max, resolution=resolution, threshold=threshold)\n        os.makedirs(os.path.join(self.base_exp_dir, 'meshes'), exist_ok=True)\n\n        path_mesh = os.path.join(self.base_exp_dir, 'meshes', f'{self.iter_step:0>8d}_reso{resolution}_{self.scan_name}.ply')\n        path_mesh_gt = IOUtils.find_target_file(self.dataset.data_dir, '_vh_clean_2_trans.ply')\n\n        if world_space:\n            path_trans_n2w = f\"{self.conf['dataset']['data_dir']}/trans_n2w.txt\"\n            scale_mat = self.dataset.scale_mats_np[0]\n            if IOUtils.checkExistence(path_trans_n2w):\n                scale_mat = np.loadtxt(path_trans_n2w)\n            vertices = vertices * scale_mat[0, 0] + scale_mat[:3, 3][None]\n            \n            path_mesh = os.path.join(self.base_exp_dir, 'meshes', f'{self.iter_step:0>8d}_reso{resolution}_{self.scan_name}_world.ply')\n            path_mesh_gt = IOUtils.find_target_file(self.dataset.data_dir, '_vh_clean_2.ply')\n\n        mesh = trimesh.Trimesh(vertices, triangles)\n        mesh.export(path_mesh)\n    \n        if path_mesh_gt:\n            GeoUtils.clean_mesh_points_outside_bbox(path_mesh, path_mesh, path_mesh_gt, scale_bbox = 1.1, check_existence=False)\n\n        return path_mesh\n\n    def validate_fields(self, iter_step=-1):\n        if iter_step < 0:\n            iter_step = self.iter_step\n        bound_min = torch.tensor(self.dataset.bbox_min, dtype=torch.float32)\n        bound_max = torch.tensor(self.dataset.bbox_max, dtype=torch.float32)\n        sdf = extract_fields(bound_min, bound_max, 128, lambda pts: self.sdf_network_fine.sdf(pts)[:, 0])\n        os.makedirs(os.path.join(self.base_exp_dir, 'fields'), exist_ok=True)\n        np.save(os.path.join(self.base_exp_dir, 'fields', '{:0>8d}_sdf.npy'.format(iter_step)), sdf)\n\n    def validate_points(self, rays_o, rays_d, near, far):\n        logging.info(f\"[{self.iter_step}]: validate sampled points...\")\n        sample_dist = self.radius_norm / self.n_samples\n\n        z_vals = torch.linspace(0.0, 1.0, self.n_samples)\n        z_vals = near + (far - near) * z_vals[None, :]\n\n        dists = z_vals[..., 1:] - z_vals[..., :-1]\n        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape)], -1)\n\n        mid_z_vals = z_vals + dists * 0.5\n        mid_dists = mid_z_vals[..., 1:] - mid_z_vals[..., :-1]\n\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # n_rays, n_samples, 3\n\n        dir_pts = f\"{self.base_exp_dir}/points\"\n        os.makedirs(dir_pts, exist_ok=True)\n\n        path_pts = f\"{dir_pts}/{self.iter_step:08d}_{self.curr_img_idx}.ply\"\n        pts = pts.reshape(-1, 3)\n        GeoUtils.save_points(path_pts, pts.detach().cpu().numpy())\n\n    def validate_weights(self, weight):\n        dir_weights = os.path.join(self.base_exp_dir, \"weights\")\n        os.makedirs(dir_weights, exist_ok=True)\n\n    def validate_image_nerf(self, idx=-1, resolution_level=-1, save_render = False):\n        print('Validate:')\n        ic(self.iter_step, idx)\n        if idx < 0:\n            idx = np.random.randint(self.dataset.n_images)\n        if resolution_level < 0:\n            resolution_level = self.validate_resolution_level\n        rays_o, rays_d = self.dataset.gen_rays_at(idx, resolution_level=resolution_level)\n        H, W, _ = rays_o.shape\n        rays_o = rays_o.reshape(-1, 3).split(self.batch_size)\n        rays_d = rays_d.reshape(-1, 3).split(self.batch_size)\n\n        out_rgb_fine = []\n\n        for rays_o_batch, rays_d_batch in zip(rays_o, rays_d):\n            near, far = torch.zeros(len(rays_o_batch), 1), 1 * torch.ones(len(rays_o_batch), 1)\n            if self.dataset_type == 'dtu':\n                near, far = self.dataset.near_far_from_sphere(rays_o_batch, rays_d_batch)\n            elif self.dataset_type == 'indoor':\n                near, far = torch.zeros(len(rays_o_batch), 1), 1 * torch.ones(len(rays_o_batch), 1)\n            else:\n                NotImplementedError\n\n            background_rgb = torch.ones([1, 3]) if self.use_white_bkgd else None\n\n            render_out = self.renderer.render(rays_o_batch,\n                                              rays_d_batch,\n                                              near,\n                                              far,\n                                              background_rgb=background_rgb)\n\n            def feasible(key): return ((key in render_out) and (render_out[key] is not None))\n\n            if feasible('color_fine'):\n                out_rgb_fine.append(render_out['color_fine'].detach().cpu().numpy())\n\n            del render_out\n\n        img_fine = None\n        if len(out_rgb_fine) > 0:\n            img_fine = (np.concatenate(out_rgb_fine, axis=0).reshape([H, W, 3, -1]) * 256).clip(0, 255)\n\n        os.makedirs(os.path.join(self.base_exp_dir, 'validations_fine'), exist_ok=True)\n        os.makedirs(os.path.join(self.base_exp_dir, 'image_render'), exist_ok=True)\n\n        for i in range(img_fine.shape[-1]):\n            if len(out_rgb_fine) > 0:\n                cv.imwrite(os.path.join(self.base_exp_dir,\n                                        'validations_fine',\n                                        f'{self.iter_step:08d}_{self.dataset.vec_stem_files[idx]}_reso{resolution_level}.png'), # '{:0>8d}_{}_{}.png'.format(self.iter_step, i, self.dataset.vec_stem_files[idx])),\n                           np.concatenate([img_fine[..., i],\n                                           self.dataset.image_at(idx, resolution_level=resolution_level)]))\n                if save_render:\n                    cv.imwrite(os.path.join(self.base_exp_dir,\n                                            'image_render',\n                                            f'{self.iter_step:08d}_{self.dataset.vec_stem_files[idx]}_reso{resolution_level}.png'), # '{:0>8d}_{}_{}.png'.format(self.iter_step, i, self.dataset.vec_stem_files[idx])),\n                            img_fine[..., i])\n\n    def validate_mesh_nerf(self, world_space=False, resolution=128, threshold=25.0):\n        logging.info(f'Validate mesh (Resolution:{resolution}\uff1b Threhold: {threshold})....')\n        bound_min = torch.tensor(self.dataset.bbox_min, dtype=torch.float32) #/ self.sdf_network_fine.scale\n        bound_max = torch.tensor(self.dataset.bbox_max, dtype=torch.float32) # / self.sdf_network_fine.scale\n\n        vertices, triangles, sdf =\\\n            self.renderer.extract_geometry(bound_min, bound_max, resolution=resolution, threshold=threshold)\n        os.makedirs(os.path.join(self.base_exp_dir, 'meshes'), exist_ok=True)\n        \n        os.makedirs(os.path.join(self.base_exp_dir, 'contour'), exist_ok=True)\n        for ax in ['x', 'y', 'z']:\n            path_contour_img = os.path.join(self.base_exp_dir, 'contour', f'{ax}_{self.iter_step:0>8d}_reso{resolution}.png')\n            if ax == 'x':\n                GeoUtils.visualize_sdf_contour(path_contour_img, -sdf[int(sdf.shape[0]/2), :,:], levels=[i-50 for i in range(0, 100, 10)])\n            if ax == 'y':\n                GeoUtils.visualize_sdf_contour(path_contour_img, -sdf[:, int(sdf.shape[1]/2),:], levels=[i-50 for i in range(0, 100, 10)])\n            if ax == 'z':\n                GeoUtils.visualize_sdf_contour(path_contour_img, -sdf[:,:, int(sdf.shape[2]/2)], levels=[i-50 for i in range(0, 100, 10)])\n\n        path_mesh_gt = IOUtils.find_target_file(self.dataset.data_dir, '_vh_clean_2_trans.ply')\n\n        if world_space:\n            path_trans_n2w = f\"{self.conf['dataset']['data_dir']}/trans_n2w.txt\"\n            print(f'LOad normalization matrix: {path_trans_n2w}')\n            scale_mat = self.dataset.scale_mats_np[0]\n            if IOUtils.checkExistence(path_trans_n2w):\n                scale_mat = np.loadtxt(path_trans_n2w)\n            vertices = vertices * scale_mat[0, 0] + scale_mat[:3, 3][None]\n            \n            path_mesh = os.path.join(self.base_exp_dir, 'meshes', f'{self.iter_step:0>8d}_reso{resolution}_thres{int(threshold):03d}_{self.scan_name}_world.ply')\n            path_mesh_gt = IOUtils.find_target_file(self.dataset.data_dir, '_vh_clean_2.ply')\n\n        mesh = trimesh.Trimesh(vertices, triangles)\n        mesh.export(path_mesh)\n\n        path_mesh_clean = IOUtils.add_file_name_suffix(path_mesh, '_clean')\n        if path_mesh_gt:\n            print(f'Clena points outside bbox')\n            GeoUtils.clean_mesh_points_outside_bbox(path_mesh_clean, path_mesh, path_mesh_gt, scale_bbox = 1.1, check_existence=False)\n\n        mesh = trimesh.Trimesh(vertices, triangles)\n        mesh.export(path_mesh_clean)\n\n    def save_accumulated_results(self, idx=-1):\n        if idx < 0:\n            idx = np.random.randint(self.dataset.n_images)\n        logging.info(f'Save confidence: idx {idx}...')\n              \n        accum_results = {}\n        accum_results['confidence'] = self.dataset.confidence_accum[idx]\n\n        b_accum_all_data = False\n        if b_accum_all_data:\n            accum_results['color'] = self.dataset.colors_accum[idx].cpu().numpy() * 255\n            # accum_results['normal'] = self.dataset.normals_accum[idx].cpu().numpy()\n            accum_results['depth'] = self.dataset.depths_accum[idx].cpu().numpy()\n\n        img_gt = self.dataset.images[idx].cpu().numpy()\n        lis_imgs = []\n        for key in accum_results:\n            if len(accum_results[key]) == 0:\n                continue\n\n            img_temp = accum_results[key].squeeze()\n            if key == 'normal':\n                img_temp = (((img_temp + 1) * 0.5).clip(0,1) * 255).astype(np.uint8)\n            if key in ['depth']:\n                img_temp = img_temp / (np.max(img_temp)+1e-6) * 255\n            if key in ['confidence', 'samples']:\n                if key == 'confidence':\n                    img_temp2 = ImageUtils.convert_gray_to_cmap(img_temp.clip(0,1), vmax=1.0)\n                    lis_imgs.append(img_temp2)\n                    img_temp = img_temp.clip(0,1) * 255\n                if key == 'samples':\n                    img_temp = ImageUtils.convert_gray_to_cmap(img_temp)\n            cv.putText(img_temp, key,  (img_temp.shape[1]-100, img_temp.shape[0]-20), \n                fontFace= cv.FONT_HERSHEY_SIMPLEX, \n                fontScale = 0.5, \n                color = (0, 0, 255), \n                thickness = 2)\n\n            accum_results[key] = img_temp\n            lis_imgs.append(img_temp)\n\n        # save images\n        dir_accum = os.path.join(self.base_exp_dir, 'images_accum')\n        os.makedirs(dir_accum, exist_ok=True)\n        ImageUtils.write_image_lis(f'{dir_accum}/{self.iter_step:0>8d}_{idx}.png', \n                                        [img_gt] + lis_imgs)\n\n        if self.iter_step ==  self.end_iter:\n            dir_accum_npz = f'{self.base_exp_dir}/checkpoints/npz'\n            os.makedirs(dir_accum_npz, exist_ok=True)\n            np.savez(f'{dir_accum_npz}/confidence_accum_{self.iter_step:08d}.npz', self.dataset.confidence_accum)\n            np.savez(f'{dir_accum_npz}/samples_accum_{self.iter_step:08d}.npz', self.dataset.samples_accum.cpu().numpy())\n\n            b_accum_all_data = False\n            if b_accum_all_data:\n                np.savez(f'{dir_accum_npz}/colors_accum_{self.iter_step:08d}.npz', self.dataset.colors_accum.cpu().numpy())\n                np.savez(f'{dir_accum_npz}/depths_accum_{self.iter_step:08d}.npz', self.dataset.depths_accum.cpu().numpy())\n                \n\n    def initialize_accumulated_results(self, mode = 'MODEL', iter_step_npz = None, resolution_level = 2):\n        # For cache rendered depths and normals\n        self.dataset.confidence_accum = np.ones_like(self.dataset.masks_np, dtype=np.float32)\n        \n        b_accum_all_data = False\n        if b_accum_all_data:\n            self.dataset.colors_accum = torch.zeros_like(self.dataset.images, dtype=torch.float32).cuda()\n            self.depths_accum = torch.zeros_like(self.masks, dtype=torch.float32).cpu()\n\n        self.dataset.normals_accum = torch.zeros_like(self.dataset.images, dtype=torch.float32).cpu()  # save gpu memory\n        self.dataset.points_accum = torch.zeros_like(self.dataset.images, dtype=torch.float32).cpu()  # world coordinates\n\n        if mode == 'model':\n            # compute accumulated results from pretrained model\n            logging.info(f'Start initializeing accumulated results....[mode: {mode}; Reso: {resolution_level}]')\n            self.dataset.render_difference_accum = np.ones_like(self.dataset.masks_np, dtype=np.float32)\n\n            t1 = datetime.now()\n            num_neighbors_half = 0\n            for idx in tqdm(range(num_neighbors_half, self.dataset.n_images-num_neighbors_half)):\n                confidence, color_peak, normal_peak, depth_peak, points_peak, _ = self.validate_image(idx, resolution_level=resolution_level, save_peak_value=True)\n                H, W = confidence.shape\n                resize_arr = lambda in_arr : cv.resize(in_arr, (W*resolution_level, H*resolution_level), interpolation=cv.INTER_NEAREST) if resolution_level > 1 else in_arr\n                self.dataset.confidence_accum[idx] = resize_arr(confidence)\n                \n                b_accum_all_data = False\n                if b_accum_all_data:\n                    self.dataset.colors_accum[idx]     = torch.from_numpy(resize_arr(color_peak)).cuda()\n            logging.info(f'Consumed time: {IOUtils.get_consumed_time(t1)/60:.02f} min.') \n            \n            dir_accum_npz = f'{self.base_exp_dir}/checkpoints/npz'\n            os.makedirs(dir_accum_npz, exist_ok=True)\n            np.savez(f'{dir_accum_npz}/confidence_accum_{self.iter_step:08d}.npz', self.dataset.confidence_accum)\n            \n        elif mode == 'npz':\n            # load accumulated results from files\n            logging.info(f'Start initializeing accumulated results....[mode: {mode}; Reso: {resolution_level}]')\n            iter_step_npz = int(iter_step_npz)\n            assert iter_step_npz is not None\n            dir_accum_npz = f'{self.base_exp_dir}/checkpoints/npz'\n            \n            self.dataset.confidence_accum = np.load(f'{dir_accum_npz}/confidence_accum_{int(iter_step_npz):08d}.npz')['arr_0']\n\n            b_accum_all_data = False\n            if b_accum_all_data:\n                self.dataset.colors_accum =     torch.from_numpy(np.load(f'{dir_accum_npz}/colors_accum_{iter_step_npz:08d}.npz')['arr_0']).cuda()\n                self.dataset.depths_accum =     torch.from_numpy(np.load(f'{dir_accum_npz}/depths_accum_{iter_step_npz:08d}.npz')['arr_0']).cuda()\n        else:\n            logging.info('Initialize all accum data to ones.')", "\n            \nif __name__ == '__main__':\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n    FORMAT = \"[%(filename)s:%(lineno)s] %(message)s\"\n    logging.basicConfig(level=logging.INFO, format=FORMAT)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--conf', type=str, default='./confs/base.conf')\n    parser.add_argument('--mode', type=str, default='train')\n    parser.add_argument('--model_type', type=str, default='')\n    parser.add_argument('--threshold', type=float, default=0.0)\n    parser.add_argument('--is_continue', default=False, action=\"store_true\")\n    parser.add_argument('--gpu', type=int, default=0)\n    parser.add_argument('--checkpoint_id', type=int, default=-1)\n    parser.add_argument('--mc_reso', type=int, default=512, help='Marching cube resolution')\n    parser.add_argument('--reset_var', action= 'store_true', help='Reset variance for validate_iamge()' )\n    parser.add_argument('--nvs', action= 'store_true', help='Novel view synthesis' )\n    parser.add_argument('--save_render_peak', action= 'store_true', help='Novel view synthesis' )\n    parser.add_argument('--scene_name', type=str, default='', help='Scene or scan name')\n    args = parser.parse_args()\n\n    torch.cuda.set_device(args.gpu)\n    runner = Runner(args.conf, args.scene_name, args.mode, args.model_type, args.is_continue, args.checkpoint_id)\n\n    if args.mode == 'train':\n        runner.train()\n    elif args.mode == 'validate_mesh':\n        if runner.model_type == 'neus':\n            t1= datetime.now()\n            runner.validate_mesh(world_space=True, resolution=args.mc_reso, threshold=args.threshold)\n            logging.info(f\"[Validate mesh] Consumed time (Step: {runner.iter_step}; MC resolution: {args.mc_reso}): {IOUtils.get_consumed_time(t1):.02f}(s)\")\n\n        elif runner.model_type == 'nerf':\n            thres = args.threshold\n            t1= datetime.now()\n            runner.validate_mesh_nerf(world_space=True, resolution=args.mc_reso, threshold=thres)\n            logging.info(f\"[Validate mesh] Consumed time (MC resolution: {args.mc_reso}\uff1b Threshold: {thres}): {IOUtils.get_consumed_time(t1):.02f}(s)\")\n    \n    elif args.mode.startswith('validate_image'):\n        if runner.model_type == 'neus':\n            for i in range(0, runner.dataset.n_images, 2):\n                t1 = datetime.now()\n                runner.validate_image(i, resolution_level=1, \n                                            save_normalmap_npz=args.save_render_peak, \n                                            save_peak_value=True,\n                                            save_image_render=args.nvs)\n                logging.info(f\"validating image time is : {(datetime.now()-t1).total_seconds()}\")\n        \n        elif runner.model_type == 'nerf':\n            for i in range(0, runner.dataset.n_images, 2):\n                t1 = datetime.now()\n                runner.validate_image_nerf(i, resolution_level=1, save_render=True)\n                logging.info(f\"validating image time is : {(datetime.now()-t1).total_seconds()}\")\n            \n    elif args.mode == 'validate_fields':\n        runner.validate_fields()"]}
{"filename": "exp_evaluation.py", "chunked_list": ["import os, argparse, logging\nfrom datetime import datetime\nimport numpy as np\n\n# import preprocess.depthneus_data  as depthneus_data\nimport evaluation.EvalScanNet as EvalScanNet\nfrom evaluation.renderer import render_depthmaps_pyrender\n\nimport utils.utils_geometry as GeoUtils\nimport utils.utils_image  as ImageUtils", "import utils.utils_geometry as GeoUtils\nimport utils.utils_image  as ImageUtils\nimport utils.utils_io as IOUtils\nimport utils.utils_normal as NormalUtils\n\nfrom confs.path import lis_name_scenes\n\nif __name__ == '__main__':\n    np.set_printoptions(precision=3)\n    np.set_printoptions(suppress=True)\n\n    FORMAT = \"[%(filename)s:%(lineno)s] %(message)s\"\n    logging.basicConfig(level=logging.INFO, format=FORMAT)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--mode', type=str, default='eval_3D_mesh_metrics')\n\n    args = parser.parse_args()\n    \n    if args.mode == 'eval_3D_mesh_metrics':\n        dir_dataset = './dataset/indoor'\n        path_intrin = f'{dir_dataset}/intrinsic_depth.txt'\n        name_baseline = 'neus' # manhattansdf depthneus\n        exp_name = name_baseline\n        eval_threshold = 0.05\n        check_existence = True\n        \n        dir_results_baseline = f'./exps/indoor'\n\n        metrics_eval_all = []\n        for scene_name in lis_name_scenes:\n            logging.info(f'\\n\\nProcess: {scene_name}')\n\n            path_mesh_pred = f'{dir_results_baseline}/{name_baseline}/{scene_name}.ply'\n            metrics_eval =  EvalScanNet.evaluate_3D_mesh(path_mesh_pred, scene_name, dir_dataset = './dataset/indoor',\n                                                                eval_threshold = 0.05, reso_level = 2, \n                                                                check_existence = check_existence)\n    \n            metrics_eval_all.append(metrics_eval)\n        metrics_eval_all = np.array(metrics_eval_all)\n        str_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n        path_log = f'{dir_results_baseline}/eval_{name_baseline}_thres{eval_threshold}_{str_date}.txt'\n        EvalScanNet.save_evaluation_results_to_latex(path_log, \n                                                        header = f'{exp_name}\\n                     Accu.      Comp.      Prec.     Recall     F-score \\n', \n                                                        results = metrics_eval_all, \n                                                        names_item = lis_name_scenes, \n                                                        save_mean = True, \n                                                        mode = 'w')\n     \n    if args.mode == 'eval_mesh_2D_metrices':\n        dir_dataset = 'dataset/indoor'\n        path_intrin = f'{dir_dataset}/intrinsic_depth.txt'\n        \n        name_baseline = 'neus'\n        eval_type_baseline = 'mesh'\n        scale_depth = False\n\n        dir_results_baseline = f'./exps/evaluation/results_baselines/{name_baseline}'\n        results_all =  []\n        for scene_name in lis_name_scenes:\n            # scene_name += '_corner'\n            print(f'Processing {scene_name}...')\n            dir_scan = f'{dir_dataset}/{scene_name}'\n            if eval_type_baseline == 'mesh':\n                # use rendered depth map\n                path_mesh_baseline =  f'{dir_results_baseline}/{scene_name}.ply'\n                pred_depths = render_depthmaps_pyrender(path_mesh_baseline, path_intrin, \n                                                            dir_poses=f'{dir_scan}/pose')\n                img_names = IOUtils.get_files_stem(f'{dir_scan}/depth', '.png')\n            elif eval_type_baseline == 'depth':\n                dir_depth_baseline =  f'{dir_results_baseline}/{scene_name}'\n                pred_depths = GeoUtils.read_depth_maps_np(dir_depth_baseline)\n                img_names = IOUtils.get_files_stem(dir_depth_baseline, '.npy')\n                \n            # evaluation\n            dir_gt_depth = f'{dir_scan}/depth'\n            gt_depths, _ = EvalScanNet.load_gt_depths(img_names, dir_gt_depth)\n            err_gt_depth_scale = EvalScanNet.depth_evaluation(gt_depths, pred_depths, dir_results_baseline, scale_depth=scale_depth)\n            results_all.append(err_gt_depth_scale)\n            \n        results_all = np.array(results_all)\n        print('All results: ', results_all)\n\n        count = 0\n        str_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n        path_log_all = f'./exps/evaluation/results_{name_baseline}-scale_{scale_depth}_{eval_type_baseline}_{str_date}.txt'\n        EvalScanNet.save_evaluation_results_to_latex(path_log_all, header = f'{str_date}\\n\\n',  mode = 'a')\n        \n        precision = 3\n        results_all = np.round(results_all, decimals=precision)\n        EvalScanNet.save_evaluation_results_to_latex(path_log_all, \n                                                        header = f'{name_baseline}', \n                                                        results = results_all, \n                                                        names_item = lis_name_scenes, \n                                                        save_mean = True, \n                                                        mode = 'a',\n                                                        precision = precision)\n\n    if args.mode == 'evaluate_normal':\n        # compute normal errors\n        exp_name = 'exp_depthneus'\n        name_normal_folder = 'normal_render'\n        \n        dir_root_dataset = './dataset/indoor'\n        dir_root_normal_gt = '../TiltedImageSurfaceNormal/datasets/scannet-frames'\n\n        err_neus_all, err_pred_all = [], []\n        num_imgs_eval_all = 0\n        \n        for scene_name in lis_name_scenes:\n            # scene_name = 'scene0085_00'\n            print(f'Process: {scene_name}')\n           \n            dir_normal_neus = f'./exps/indoor/neus/{scene_name}/{exp_name}/{name_normal_folder}'\n            \n            dir_normal_pred = f'{dir_root_dataset}/{scene_name}/pred_normal' \n            dir_poses = f'{dir_root_dataset}/{scene_name}/pose' \n            dir_normal_gt = f'{dir_root_normal_gt}/{scene_name}'\n            error_neus, error_pred, num_imgs_eval = NormalUtils.evauate_normal(dir_normal_neus, dir_normal_pred, dir_normal_gt, dir_poses)\n            err_neus_all.append(error_neus)\n            err_pred_all.append(error_pred)\n            \n            num_imgs_eval_all += num_imgs_eval\n\n        error_neus_all = np.concatenate(err_neus_all).reshape(-1)\n        err_pred_all = np.concatenate(err_pred_all).reshape(-1)\n        metrics_neus = NormalUtils.compute_normal_errors_metrics(error_neus_all)\n        metrics_pred = NormalUtils.compute_normal_errors_metrics(err_pred_all)\n        NormalUtils.log_normal_errors(metrics_neus, first_line='metrics_neus fianl')\n        NormalUtils.log_normal_errors(metrics_pred, first_line='metrics_pred final')\n        print(f'Total evaluation images: {num_imgs_eval_all}')\n\n    if args.mode == 'evaluate_nvs':\n           # compute normal errors\n        name_img_folder = 'image_render'\n        sample_interval = 1\n\n        exp_name_nerf = 'exp_nerf'\n        exp_name_depthneus = 'exp_depthneus'\n        exp_name_neus  = 'exp_neus'\n\n        evals_nvs_all ={\n            'nerf': exp_name_nerf,\n            'neus': exp_name_neus,\n            'depthneus': exp_name_depthneus\n        }\n        psnr_all_methods = {}\n        psnr_imgs = []\n        psnr_imgs_stem = []\n        np.set_printoptions(precision=3)\n        for key in evals_nvs_all:\n            exp_name = evals_nvs_all[key]\n            model_type = 'nerf' if key == 'nerf' else 'neus'\n\n            print(f\"Start to eval: {key}. {exp_name}\")\n            err_neus_all, err_pred_all = [], []\n            num_imgs_eval_all = 0\n            \n            psnr_scenes_all = []\n            psnr_mean_all = []\n            for scene_name in lis_name_scenes:\n                # scene_name = 'scene0085_00'\n                scene_name = scene_name + '_nvs'\n                print(f'Process: {scene_name}')\n                \n                dir_img_gt = f'./dataset/indoor/{scene_name}/image'\n                dir_img_neus = f'./exps/indoor/{model_type}/{scene_name}/{exp_name}/{name_img_folder}'\n                psnr_scene, vec_stem_eval = ImageUtils.eval_imgs_psnr(dir_img_neus, dir_img_gt, sample_interval)\n                print(f'PSNR: {scene_name} {psnr_scene.mean()}  {psnr_scene.shape}')\n                psnr_scenes_all.append(psnr_scene)\n                psnr_imgs.append(psnr_scene)\n                psnr_imgs_stem.append(vec_stem_eval)\n                psnr_mean_all.append(psnr_scene.mean())\n                # input(\"anything to continue\")\n            \n            psnr_scenes_all = np.concatenate(psnr_scenes_all)\n            psnr_mean_all = np.array(psnr_mean_all)\n            print(f'\\n\\n Mean of scnes: {psnr_mean_all.mean()}. PSNR of all scenes: {psnr_mean_all} \\n mean of images:{psnr_scenes_all.mean()} image numbers: {len(psnr_scenes_all)} ')\n\n            psnr_all_methods[key] = (psnr_scenes_all.mean(), psnr_mean_all.mean(),  psnr_mean_all)\n        \n        # psnr_imgs = vec_stem_eval + psnr_imgs\n        path_log_temp = f'./exps/indoor/evaluation/nvs/evaluation_temp_{lis_name_scenes[0]}.txt'\n        flog_temp = open(path_log_temp, 'w')\n        for i in range(len(vec_stem_eval)):\n            try:\n                flog_temp.write(f'{psnr_imgs_stem[0][i][9:13]}  {psnr_imgs_stem[1][i][9:13]}  {psnr_imgs_stem[2][i][9:13]}: {psnr_imgs[0][i]:.1f}:  {psnr_imgs[1][i]:.1f}  {psnr_imgs[2][i]:.1f}\\n')\n            except Exception:\n                print(f'Error: skip {vec_stem_eval[i]}')\n                continue\n        flog_temp.close()\n        input('Continue?')\n\n        print(f'Finish NVS evaluation')\n        # print eval information\n        path_log = f'./exps/indoor/evaluation/nvs/evaluation.txt'\n        flog_nvs = open(path_log, 'a')\n        flog_nvs.write(f'sample interval: {sample_interval}. Scenes number: {len(lis_name_scenes)}. Scene names: {lis_name_scenes}\\n\\n')\n        flog_nvs.write(f'Mean_img  mean_scenes  scenes-> \\n')\n        for key in psnr_all_methods:\n            print(f'[{key}] Mean of all images: {psnr_all_methods[key][0]}. Mean of scenes: {psnr_all_methods[key][1]} \\n Scenes: {psnr_all_methods[key][2]}\\n')\n            flog_nvs.write(f'{key:10s} {psnr_all_methods[key][0]:.03f} {psnr_all_methods[key][1]:.03f}. {psnr_all_methods[key][2]}.\\n')\n        flog_nvs.write(f'Images number: {len(psnr_scenes_all)}\\n')\n        flog_nvs.close()\n    \n    print('Done')", ""]}
{"filename": "evaluation/EvalScanNet.py", "chunked_list": ["# borrowed from nerfingmvs and neuralreon\n\nimport os, cv2,logging\nimport numpy as np\nimport open3d as o3d\n\nimport utils.utils_geometry as GeoUtils\nimport utils.utils_io as IOUtils\n\ndef load_gt_depths(image_list, datadir, H=None, W=None):\n    depths = []\n    masks = []\n\n    for image_name in image_list:\n        frame_id = image_name.split('.')[0]\n        depth_path = os.path.join(datadir, '{:04d}.png'.format(int(frame_id)))\n        depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n        depth = depth.astype(np.float32) / 1000\n        \n        if H is not None:\n            mask = (depth > 0).astype(np.uint8)\n            depth_resize = cv2.resize(depth, (W, H), interpolation=cv2.INTER_NEAREST)\n            mask_resize = cv2.resize(mask, (W, H), interpolation=cv2.INTER_NEAREST)\n            depths.append(depth_resize)\n            masks.append(mask_resize > 0.5)\n        else:\n            depths.append(depth)\n            masks.append(depth > 0)\n    return np.stack(depths), np.stack(masks)", "\ndef load_gt_depths(image_list, datadir, H=None, W=None):\n    depths = []\n    masks = []\n\n    for image_name in image_list:\n        frame_id = image_name.split('.')[0]\n        depth_path = os.path.join(datadir, '{:04d}.png'.format(int(frame_id)))\n        depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)\n        depth = depth.astype(np.float32) / 1000\n        \n        if H is not None:\n            mask = (depth > 0).astype(np.uint8)\n            depth_resize = cv2.resize(depth, (W, H), interpolation=cv2.INTER_NEAREST)\n            mask_resize = cv2.resize(mask, (W, H), interpolation=cv2.INTER_NEAREST)\n            depths.append(depth_resize)\n            masks.append(mask_resize > 0.5)\n        else:\n            depths.append(depth)\n            masks.append(depth > 0)\n    return np.stack(depths), np.stack(masks)", "\ndef load_depths_npy(image_list, datadir, H=None, W=None):\n    depths = []\n\n    for image_name in image_list:\n        frame_id = image_name.split('.')[0]\n        depth_path = os.path.join(datadir, '{}_depth.npy'.format(frame_id))\n        if not os.path.exists(depth_path):\n            depth_path = os.path.join(datadir, '{}.npy'.format(frame_id))\n        depth = np.load(depth_path)\n        \n        if H is not None:\n            depth_resize = cv2.resize(depth, (W, H))\n            depths.append(depth_resize)\n        else:\n            depths.append(depth)\n\n    return np.stack(depths)", "\ndef compute_errors(gt, pred):\n    \"\"\"Computation of error metrics between predicted and ground truth depths\n    \"\"\"\n    thresh = np.maximum((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25     ).mean()\n    a2 = (thresh < 1.25 ** 2).mean()\n    a3 = (thresh < 1.25 ** 3).mean()\n\n    rmse = (gt - pred) ** 2\n    rmse = np.sqrt(rmse.mean())\n\n    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n    rmse_log = np.sqrt(rmse_log.mean())\n\n    abs_rel = np.mean(np.abs(gt - pred) / gt)\n\n    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n\n    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3", "\ndef depth_evaluation(gt_depths, pred_depths, savedir=None, pred_masks=None, min_depth=0.1, max_depth=20, scale_depth = False):\n    assert gt_depths.shape[0] == pred_depths.shape[0]\n\n    gt_depths_valid = []\n    pred_depths_valid = []\n    errors = []\n    num = gt_depths.shape[0]\n    for i in range(num):\n        gt_depth = gt_depths[i]\n        mask = (gt_depth > min_depth) * (gt_depth < max_depth)\n        gt_height, gt_width = gt_depth.shape[:2]\n\n        pred_depth = cv2.resize(pred_depths[i], (gt_width, gt_height))\n\n        if pred_masks is not None:\n            pred_mask = pred_masks[i]\n            pred_mask = cv2.resize(pred_mask.astype(np.uint8), (gt_width, gt_height)) > 0.5\n            mask = mask * pred_mask\n\n        if mask.sum() == 0:\n            continue\n\n        pred_depth = pred_depth[mask]\n        gt_depth = gt_depth[mask]\n        \n        pred_depths_valid.append(pred_depth)\n        gt_depths_valid.append(gt_depth)\n\n    ratio = 1.0\n    if scale_depth:\n        ratio = np.median(np.concatenate(gt_depths_valid)) / \\\n                    np.median(np.concatenate(pred_depths_valid))\n    \n    for i in range(len(pred_depths_valid)):\n        gt_depth = gt_depths_valid[i]\n        pred_depth = pred_depths_valid[i]\n\n        pred_depth *= ratio\n        pred_depth[pred_depth < min_depth] = min_depth\n        pred_depth[pred_depth > max_depth] = max_depth\n\n        errors.append(compute_errors(gt_depth, pred_depth))\n\n    mean_errors = np.array(errors).mean(0)\n\n    # print(\"\\n  \" + (\"{:>8} | \" * 7).format(\"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\"))\n    print((\"&{: 8.3f}  \" * 7).format(*mean_errors.tolist()))\n    # print(\"\\n-> Done!\")\n\n    if savedir is not None:\n        with open(os.path.join(savedir, 'depth_evaluation.txt'), 'a+') as f:\n            if len(f.readlines()) == 0:\n                f.writelines((\"{:>8} | \" * 7).format(\"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\") + '    scale_depth\\n')\n            f.writelines((\"&{: 8.3f}  \" * 7).format(*mean_errors.tolist()) + f\"    {scale_depth}   \\\\\\\\\")\n    \n    return mean_errors", "   \ndef save_evaluation_results(dir_log_eval, errors_mesh, name_exps, step_evaluation):\n    # save evaluation results to latex format\n    mean_errors_mesh = errors_mesh.mean(axis=0)     # 4*7\n\n    names_log = ['err_gt_mesh', 'err_gt_mesh_scale', 'err_gt_depth', 'err_gt_depth_scale']\n    dir_log_eval = f'{dir_log_eval}/{step_evaluation:08d}'\n    IOUtils.ensure_dir_existence(dir_log_eval)\n    for idx_errror_type in range(4):\n        with open(f'{dir_log_eval}/{names_log[idx_errror_type]}.txt', 'w') as f_log:\n            len_name = len(name_exps[0][0])\n            f_log.writelines(('No.' + ' '*np.max([0, len_name-len('   scene id')]) + '     scene id     ' + \"{:>8} | \" * 7).format(\"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\") + '\\n')\n            for idx_scan in range(errors_mesh.shape[0]):\n                f_log.writelines((f'[{idx_scan}] {name_exps[idx_scan][0]} ' + (\"&{: 8.3f}  \" * 7).format(*errors_mesh[idx_scan, idx_errror_type, :].tolist())) + f\" \\\\\\ {name_exps[idx_scan][1]}\\n\")\n            \n            f_log.writelines((' '*len_name + 'Mean' + \" &{: 8.3f} \" * 7).format(*mean_errors_mesh[idx_errror_type, :].tolist()) + \" \\\\\\ \\n\")", "            \ndef evaluate_geometry_neucon(file_pred, file_trgt, threshold=.05, down_sample=.02):\n    \"\"\" Borrowed from NeuralRecon\n    Compute Mesh metrics between prediction and target.\n\n    Opens the Meshs and runs the metrics\n\n    Args:\n        file_pred: file path of prediction\n        file_trgt: file path of target\n        threshold: distance threshold used to compute precision/recal\n        down_sample: use voxel_downsample to uniformly sample mesh points\n\n    Returns:\n        Dict of mesh metrics\n    \"\"\"\n\n    def nn_correspondance(verts1, verts2):\n        \"\"\" for each vertex in verts2 find the nearest vertex in verts1\n\n        Args:\n            nx3 np.array's\n\n        Returns:\n            ([indices], [distances])\n\n        \"\"\"\n\n        indices = []\n        distances = []\n        if len(verts1) == 0 or len(verts2) == 0:\n            return indices, distances\n\n        pcd = o3d.geometry.PointCloud()\n        pcd.points = o3d.utility.Vector3dVector(verts1)\n        kdtree = o3d.geometry.KDTreeFlann(pcd)\n\n        for vert in verts2:\n            _, inds, dist = kdtree.search_knn_vector_3d(vert, 1)\n            indices.append(inds[0])\n            distances.append(np.sqrt(dist[0]))\n\n        return indices, distances\n\n    pcd_pred = GeoUtils.read_point_cloud(file_pred)\n    pcd_trgt = GeoUtils.read_point_cloud(file_trgt)\n    if down_sample:\n        pcd_pred = pcd_pred.voxel_down_sample(down_sample)\n        pcd_trgt = pcd_trgt.voxel_down_sample(down_sample)\n    verts_pred = np.asarray(pcd_pred.points)\n    verts_trgt = np.asarray(pcd_trgt.points)\n\n    _, dist1 = nn_correspondance(verts_pred, verts_trgt)  # para2->para1: dist1 is gt->pred\n    _, dist2 = nn_correspondance(verts_trgt, verts_pred)\n    dist1 = np.array(dist1)\n    dist2 = np.array(dist2)\n\n    precision = np.mean((dist2 < threshold).astype('float'))\n    recal = np.mean((dist1 < threshold).astype('float'))\n    fscore = 2 * precision * recal / (precision + recal)\n    metrics = {'dist1': np.mean(dist2),  # pred->gt\n               'dist2': np.mean(dist1),  # gt -> pred\n               'prec': precision,\n               'recal': recal,\n               'fscore': fscore,\n               }\n    # plot graph\n    # if path_fscore_curve:\n    #     EvalUtils.draw_figure_fscore(path_fscore_curve, threshold, dist2, dist1, plot_stretch=5)\n\n    metrics = np.array([np.mean(dist2), np.mean(dist1), precision, recal, fscore])\n    logging.info(f'{file_pred.split(\"/\")[-1]}: {metrics}')\n    return metrics", "\ndef evaluate_3D_mesh(path_mesh_pred, scene_name, dir_dataset = './dataset/indoor',\n                            eval_threshold = 0.05, reso_level = 2.0, \n                            check_existence = True):\n    '''Evaluate geometry quality of neus using Precison, Recall and F-score.\n    '''\n    path_intrin = f'{dir_dataset}/intrinsic_depth.txt'\n    target_img_size = (640, 480)\n    dir_scan = f'{dir_dataset}/{scene_name}'\n    dir_poses = f'{dir_scan}/pose'\n    # dir_images = f'{dir_scan}/image'\n    \n    path_mesh_gt = f'{dir_dataset}/{scene_name}/{scene_name}_vh_clean_2.ply'\n    path_mesh_gt_clean = IOUtils.add_file_name_suffix(path_mesh_gt, '_clean')\n    path_mesh_gt_2dmask = f'{dir_dataset}/{scene_name}/{scene_name}_vh_clean_2_2dmask.npz'\n    \n    # (1) clean GT mesh\n    GeoUtils.clean_mesh_faces_outside_frustum(path_mesh_gt_clean, path_mesh_gt, \n                                                path_intrin, dir_poses, \n                                                target_img_size, reso_level=reso_level,\n                                                check_existence = check_existence)\n    GeoUtils.generate_mesh_2dmask(path_mesh_gt_2dmask, path_mesh_gt_clean, \n                                                path_intrin, dir_poses, \n                                                target_img_size, reso_level=reso_level,\n                                                check_existence = check_existence)\n    # for fair comparison\n    GeoUtils.clean_mesh_faces_outside_frustum(path_mesh_gt_clean, path_mesh_gt, \n                                                path_intrin, dir_poses, \n                                                target_img_size, reso_level=reso_level,\n                                                path_mask_npz=path_mesh_gt_2dmask,\n                                                check_existence = check_existence)\n\n\n    # (2) clean predicted mesh\n    path_mesh_pred_clean_bbox = IOUtils.add_file_name_suffix(path_mesh_pred, '_clean_bbox')\n    path_mesh_pred_clean_bbox_faces = IOUtils.add_file_name_suffix(path_mesh_pred, '_clean_bbox_faces')\n    path_mesh_pred_clean_bbox_faces_mask = IOUtils.add_file_name_suffix(path_mesh_pred, '_clean_bbox_faces_mask')\n\n    GeoUtils.clean_mesh_points_outside_bbox(path_mesh_pred_clean_bbox, path_mesh_pred, path_mesh_gt,\n                                                scale_bbox=1.1,\n                                                check_existence = check_existence)\n    GeoUtils.clean_mesh_faces_outside_frustum(path_mesh_pred_clean_bbox_faces, path_mesh_pred_clean_bbox, \n                                                    path_intrin, dir_poses, \n                                                    target_img_size, reso_level=reso_level,\n                                                    check_existence = check_existence)\n    GeoUtils.clean_mesh_points_outside_frustum(path_mesh_pred_clean_bbox_faces_mask, path_mesh_pred_clean_bbox_faces, \n                                                    path_intrin, dir_poses, \n                                                    target_img_size, reso_level=reso_level,\n                                                    path_mask_npz=path_mesh_gt_2dmask,\n                                                    check_existence = check_existence)\n    \n    path_eval = path_mesh_pred_clean_bbox_faces_mask \n    metrices_eval = evaluate_geometry_neucon(path_eval, path_mesh_gt_clean, \n                                                        threshold=eval_threshold, down_sample=.02) #f'{dir_eval_fig}/{scene_name}_step{iter_step:06d}_thres{eval_threshold}.png')\n\n    return metrices_eval", "\ndef save_evaluation_results_to_latex(path_log, \n                                        header = '                     Accu.      Comp.      Prec.     Recall     F-score \\n', \n                                        results = None, \n                                        names_item = None, \n                                        save_mean = None, \n                                        mode = 'w',\n                                        precision = 3):\n    '''Save evaluation results to txt in latex mode\n    Args:\n        header:\n            for F-score: '                     Accu.      Comp.      Prec.     Recall     F-score \\n'\n        results:\n            narray, N*M, N lines with M metrics\n        names_item:\n            N*1, item name for each line\n        save_mean: \n            whether calculate the mean value for each metric\n        mode:\n            write mode, default 'w'\n    '''\n    # save evaluation results to latex format\n    with open(path_log, mode) as f_log:\n        if header:\n            f_log.writelines(header)\n        if results is not None:\n            num_lines, num_metrices = results.shape\n            if names_item is None:\n                names_item = np.arange(results.shape[0])\n            for idx in range(num_lines):\n                f_log.writelines((f'{names_item[idx]}    ' + (\"&{: 8.3f}  \" * num_metrices).format(*results[idx, :].tolist())) + \" \\\\\\ \\n\")\n        if save_mean:\n            mean_results = results.mean(axis=0)     # 4*7\n            mean_results = np.round(mean_results, decimals=precision)\n            f_log.writelines(( '       Mean    ' + \" &{: 8.3f} \" * num_metrices).format(*mean_results[:].tolist()) + \" \\\\\\ \\n\")", " \n\n\n"]}
{"filename": "evaluation/renderer.py", "chunked_list": ["\n# Copyright 2020 Magic Leap, Inc.\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#     http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software", "\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#  Originating Author: Zak Murez (zak.murez.com)\n\n# Borrowed from Atlas", "\n# Borrowed from Atlas\n\nimport numpy as np\nimport trimesh, pyrender\n\nfrom tqdm import tqdm\nfrom skimage import measure\nfrom matplotlib.cm import get_cmap as colormap\n", "from matplotlib.cm import get_cmap as colormap\n\nimport utils.utils_geometry as GeoUtils\n\n\n\nclass Renderer():\n    \"\"\"Borrowed from Atlas\n    OpenGL mesh renderer \n    \n    Used to render depthmaps from a mesh for 2d evaluation\n    \"\"\"\n    def __init__(self, height=480, width=640):\n        self.renderer = pyrender.OffscreenRenderer(width, height)\n        self.scene = pyrender.Scene()\n        #self.render_flags = pyrender.RenderFlags.SKIP_CULL_FACES\n\n    def __call__(self, height, width, intrinsics, pose, mesh):\n        self.renderer.viewport_height = height\n        self.renderer.viewport_width = width\n        self.scene.clear()\n        self.scene.add(mesh)\n        cam = pyrender.IntrinsicsCamera(cx=intrinsics[0, 2], cy=intrinsics[1, 2],\n                                        fx=intrinsics[0, 0], fy=intrinsics[1, 1])\n        self.scene.add(cam, pose=self.fix_pose(pose))\n        return self.renderer.render(self.scene)#, self.render_flags) \n\n    def fix_pose(self, pose):\n        # 3D Rotation about the x-axis.\n        t = np.pi\n        c = np.cos(t)\n        s = np.sin(t)\n        R =  np.array([[1, 0, 0],\n                       [0, c, -s],\n                       [0, s, c]])\n        axis_transform = np.eye(4)\n        axis_transform[:3, :3] = R\n        return pose@axis_transform\n\n    def mesh_opengl(self, mesh):\n        return pyrender.Mesh.from_trimesh(mesh)\n\n    def delete(self):\n        self.renderer.delete()", "\n\ndef render_depthmaps_pyrender(path_mesh, path_intrin, dir_poses,\n                path_mask_npz = None):\n\n    # gt depth data loader\n    width, height = 640, 480\n\n    # mesh renderer\n    renderer = Renderer()\n    mesh = trimesh.load(path_mesh, process=False)\n    mesh_opengl = renderer.mesh_opengl(mesh)\n    \n    intrin = GeoUtils.read_cam_matrix(path_intrin)\n    poses = GeoUtils.read_poses(dir_poses)\n    num_images = len(poses)\n    depths_all = []\n    for i in tqdm(range(num_images)):\n        pose_curr = poses[i]\n        _, depth_pred = renderer(height, width, intrin, pose_curr, mesh_opengl)\n        depths_all.append(depth_pred)\n\n    depths_all = np.array(depths_all)\n    return depths_all"]}
{"filename": "utils/utils_training.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\n\ndef get_angles(normals_source, normals_target):\n    '''Get angular error betwee predicted normals and ground truth normals\n    Args:\n        normals_source, normals_target: N*3\n        mask: N*1 (optional, default: None)\n    Return:\n        angular_error: float\n    '''\n    inner = (normals_source * normals_target).sum(dim=-1,keepdim=True)\n    norm_source =  torch.linalg.norm(normals_source, dim=-1, ord=2,keepdim=True)\n    norm_target = torch.linalg.norm(normals_target, dim=-1, ord=2,keepdim=True)\n    angles = torch.arccos(inner/((norm_source*norm_target) + 1e-6)) #.clip(-np.pi, np.pi)\n    assert not torch.isnan(angles).any()\n    return angles", "\ndef get_angular_error(normals_source, normals_target, mask = None, clip_angle_error = -1):\n    '''Get angular error betwee predicted normals and ground truth normals\n    Args:\n        normals_source, normals_target: N*3\n        mask: N*1 (optional, default: None)\n    Return:\n        angular_error: float\n    '''\n    inner = (normals_source * normals_target).sum(dim=-1,keepdim=True)\n    norm_source =  torch.linalg.norm(normals_source, dim=-1, ord=2,keepdim=True)\n    norm_target = torch.linalg.norm(normals_target, dim=-1, ord=2,keepdim=True)\n    angles = torch.arccos(inner/((norm_source*norm_target) + 1e-6)) #.clip(-np.pi, np.pi)\n    assert not torch.isnan(angles).any()\n    if mask is None:\n        mask = torch.ones_like(angles)\n    if mask.ndim == 1:\n        mask =  mask.unsqueeze(-1)\n    assert angles.ndim == mask.ndim\n\n    mask_keep_gt_normal = torch.ones_like(angles).bool()\n    if clip_angle_error>0:\n        mask_keep_gt_normal = angles < clip_angle_error\n        # num_clip = mask_keep_gt_normal.sum()\n    angular_error = F.l1_loss(angles*mask*mask_keep_gt_normal, torch.zeros_like(angles), reduction='sum') / (mask*mask_keep_gt_normal+1e-6).sum()\n    return angular_error, mask_keep_gt_normal", "\n# evaluation\ndef calculate_psnr(img_pred, img_gt, mask=None):\n    psnr = 20.0 * torch.log10(1.0 / (((img_pred - img_gt)**2).mean()).sqrt())\n    return psnr\n\ndef convert_to_homo(pts):\n    pts_homo = torch.cat([pts, torch.ones(pts.shape[:-1] + tuple([1])) ], axis=-1)\n    return pts_homo"]}
{"filename": "utils/utils_geometry.py", "chunked_list": ["import cv2, glob, trimesh, logging\nimport pandas as pd\nimport open3d as o3d\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.nn import functional as F\nfrom scipy.spatial.transform import Rotation as R\nfrom datetime import datetime\nimport copy, os", "from datetime import datetime\nimport copy, os\nfrom tqdm import tqdm\n\nimport utils.utils_image as ImageUtils\nimport utils.utils_io as IOUtils\n\n\ndef modify_intrinsics_of_cropped_images(path_intrin, path_intrin_save, crop_width_half, crop_height_half):\n    intrin = np.loadtxt(path_intrin)\n    intrin[0, 2] = intrin[0, 2] - crop_width_half\n    intrin[1, 2] = intrin[1, 2] - crop_height_half\n    np.savetxt(path_intrin_save, intrin, fmt='%f')\n    return intrin", "def modify_intrinsics_of_cropped_images(path_intrin, path_intrin_save, crop_width_half, crop_height_half):\n    intrin = np.loadtxt(path_intrin)\n    intrin[0, 2] = intrin[0, 2] - crop_width_half\n    intrin[1, 2] = intrin[1, 2] - crop_height_half\n    np.savetxt(path_intrin_save, intrin, fmt='%f')\n    return intrin\n    \ndef read_point_cloud(path):\n    cloud = o3d.io.read_point_cloud(path)\n    return cloud", "\ndef read_triangle_mesh(path):\n    mesh = o3d.io.read_triangle_mesh(path)\n    return mesh\n\ndef write_triangle_mesh(path_save, mesh):\n    # assert IOUtils.ensure_dir_existence(os.path.dirname(path_save))\n    o3d.io.write_triangle_mesh(path_save, mesh)\n    \ndef rot_to_quat(R):\n    batch_size, _,_ = R.shape\n    q = torch.ones((batch_size, 4)).cuda()\n\n    R00 = R[:, 0,0]\n    R01 = R[:, 0, 1]\n    R02 = R[:, 0, 2]\n    R10 = R[:, 1, 0]\n    R11 = R[:, 1, 1]\n    R12 = R[:, 1, 2]\n    R20 = R[:, 2, 0]\n    R21 = R[:, 2, 1]\n    R22 = R[:, 2, 2]\n\n    q[:,0]=torch.sqrt(1.0+R00+R11+R22)/2\n    q[:, 1]=(R21-R12)/(4*q[:,0])\n    q[:, 2] = (R02 - R20) / (4 * q[:, 0])\n    q[:, 3] = (R10 - R01) / (4 * q[:, 0])\n    return q", "    \ndef rot_to_quat(R):\n    batch_size, _,_ = R.shape\n    q = torch.ones((batch_size, 4)).cuda()\n\n    R00 = R[:, 0,0]\n    R01 = R[:, 0, 1]\n    R02 = R[:, 0, 2]\n    R10 = R[:, 1, 0]\n    R11 = R[:, 1, 1]\n    R12 = R[:, 1, 2]\n    R20 = R[:, 2, 0]\n    R21 = R[:, 2, 1]\n    R22 = R[:, 2, 2]\n\n    q[:,0]=torch.sqrt(1.0+R00+R11+R22)/2\n    q[:, 1]=(R21-R12)/(4*q[:,0])\n    q[:, 2] = (R02 - R20) / (4 * q[:, 0])\n    q[:, 3] = (R10 - R01) / (4 * q[:, 0])\n    return q", "\ndef quat_to_rot(q):\n    batch_size, _ = q.shape\n    q = F.normalize(q, dim=1)\n    R = torch.ones((batch_size, 3,3)).cuda()\n    qr=q[:,0]\n    qi = q[:, 1]\n    qj = q[:, 2]\n    qk = q[:, 3]\n    R[:, 0, 0]=1-2 * (qj**2 + qk**2)\n    R[:, 0, 1] = 2 * (qj *qi -qk*qr)\n    R[:, 0, 2] = 2 * (qi * qk + qr * qj)\n    R[:, 1, 0] = 2 * (qj * qi + qk * qr)\n    R[:, 1, 1] = 1-2 * (qi**2 + qk**2)\n    R[:, 1, 2] = 2*(qj*qk - qi*qr)\n    R[:, 2, 0] = 2 * (qk * qi-qj * qr)\n    R[:, 2, 1] = 2 * (qj*qk + qi*qr)\n    R[:, 2, 2] = 1-2 * (qi**2 + qj**2)\n    return R", "\n\n# camera intrin\ndef resize_cam_intrin(intrin, resolution_level):\n    if resolution_level != 1.0:\n        logging.info(f'Resize instrinsics, resolution_level: {resolution_level}')\n        intrin[:2,:3] /= resolution_level\n    return intrin\n\ndef read_cam_matrix(path, data = ''):\n    '''load camera intrinsics or extrinsics\n    '''\n    if path is not None:\n        data = open(path)\n    \n    lines = [[float(w) for w in line.strip().split()] for line in data]\n    assert len(lines) == 4\n    return np.array(lines).astype(np.float32)", "\ndef read_cam_matrix(path, data = ''):\n    '''load camera intrinsics or extrinsics\n    '''\n    if path is not None:\n        data = open(path)\n    \n    lines = [[float(w) for w in line.strip().split()] for line in data]\n    assert len(lines) == 4\n    return np.array(lines).astype(np.float32)", "\n# camera pose related functions\ndef save_poses(dir_pose, poses, stems = None):\n    IOUtils.ensure_dir_existence(dir_pose)\n    num_poses = poses.shape[0]\n    for i in range(num_poses):\n        stem_curr = f'{i:04d}'\n        if stems is not None:\n            stem_curr = stems[i]\n        path_pose = f\"{dir_pose}/{stem_curr}.txt\"\n        np.savetxt(path_pose, poses[i])", "\ndef get_pose_inv(pose):\n    # R = pose[:3, :3]\n    # T = pose[:3, 3]\n    # R_inv = R.transpose()\n    # T_inv = -R_inv @ T\n    # pose_inv = np.identity(4)\n    # pose_inv[:3, :3] = R_inv\n    # pose_inv[:3, 3] = T_inv\n    return np.linalg.inv(pose)", "\ndef get_poses_inverse(poses):   \n    if poses.ndim == 3:\n        poses_inv = []\n        for i in range(poses.shape[0]):\n            pose_i_inv = get_pose_inv(poses[i])\n            poses_inv.append(pose_i_inv)\n        return np.array(poses_inv)\n    elif poses.ndim == 2:\n        return get_pose_inv(poses)\n    else:\n        NotImplementedError", "\ndef get_camera_origins(poses_homo):\n    '''\n    Args:\n        poses_homo: world to camera poses\n    '''\n    if not isinstance(poses_homo, np.ndarray):\n        poses_homo = np.array(poses_homo)\n    cam_centers = []\n    poses_homo = np.array(poses_homo)\n    num_cams = poses_homo.shape[0]\n    for i in range(num_cams):\n        rot = poses_homo[i, :3,:3]\n        trans = poses_homo[i, :3,3]\n        trans = trans.reshape(3,1)\n        cam_center = - np.linalg.inv(rot) @ trans\n        cam_centers.append(cam_center)\n    cam_centers = np.array(cam_centers).squeeze(axis=-1)\n    return cam_centers", "        \ndef get_world_points(depth, intrinsics, extrinsics):\n    '''\n    Args:\n        depthmap: H*W\n        intrinsics: 3*3 or 4*4\n        extrinsics: 4*4, world to camera\n    Return:\n        points: N*3, in world space \n    '''\n    if intrinsics.shape[0] ==4:\n        intrinsics = intrinsics[:3,:3]\n        \n    height, width = depth.shape\n\n    x, y = np.meshgrid(np.arange(0, width), np.arange(0, height))\n\n    # valid_points = np.ma.masked_greater(depth, 0.0).mask\n    # x, y, depth = x[valid_points], y[valid_points], depth[valid_points]\n\n    x = x.reshape((1, height*width))\n    y = y.reshape((1, height*width))\n    depth = depth.reshape((1, height*width))\n\n    xyz_ref = np.matmul(np.linalg.inv(intrinsics),\n                        np.vstack((x, y, np.ones_like(x))) * depth)\n    xyz_world = np.matmul(np.linalg.inv(extrinsics),\n                            np.vstack((xyz_ref, np.ones_like(x))))[:3]\n    xyz_world = xyz_world.transpose((1, 0))\n\n    return xyz_world", "\ndef get_world_normal(normal, extrin):\n    '''\n    Args:\n        normal: N*3\n        extrinsics: 4*4, world to camera\n    Return:\n        normal: N*3, in world space \n    '''\n    extrinsics = copy.deepcopy(extrin)\n    if torch.is_tensor(extrinsics):\n        extrinsics = extrinsics.cpu().numpy()\n        \n    assert extrinsics.shape[0] ==4\n    normal = normal.transpose()\n    extrinsics[:3, 3] = np.zeros(3)  # only rotation, no translation\n\n    normal_world = np.matmul(np.linalg.inv(extrinsics),\n                            np.vstack((normal, np.ones((1, normal.shape[1])))))[:3]\n    normal_world = normal_world.transpose((1, 0))\n\n    return normal_world", "\ndef get_aabb(points, scale=1.0):\n    '''\n    Args:\n        points; 1) numpy array (converted to '2)'; or \n                2) open3d cloud\n    Return:\n        min_bound\n        max_bound\n        center: bary center of geometry coordinates\n    '''\n    if isinstance(points, np.ndarray):\n        point_cloud = o3d.geometry.PointCloud()\n        point_cloud.points = o3d.utility.Vector3dVector(points)\n        points = point_cloud\n    min_max_bounds = o3d.geometry.AxisAlignedBoundingBox.get_axis_aligned_bounding_box(points)\n    min_bound, max_bound = min_max_bounds.min_bound, min_max_bounds.max_bound\n    center = (min_bound+max_bound)/2\n    # center = points.get_center()\n    if scale != 1.0:\n        min_bound = center + scale * (min_bound-center)\n        max_bound = center + scale * (max_bound-center)\n\n    # logging.info(f\"min_bound, max_bound, center: {min_bound, max_bound, center}\")\n    return min_bound, max_bound, center", "\ndef read_poses(dir, lis_stem = None, ext = '.txt'):\n    '''Read camera poses\n    '''\n    vec_path = sorted(glob.glob(f\"{dir}/**{ext}\"))\n    if lis_stem is not None:\n        vec_path = []\n        for stem_curr in lis_stem:\n            vec_path.append(f'{dir}/{stem_curr}{ext}')\n        vec_path = sorted(vec_path)\n\n    poses = []\n    stems_all =  []\n    for i in range(len(vec_path)):\n        pose_i = read_cam_matrix(vec_path[i])\n        poses.append(pose_i)\n\n        _, stem_, _ = IOUtils.get_path_components(vec_path[i])\n        stems_all.append(stem_)\n    if lis_stem is not None:\n        return np.array(poses), stems_all\n    else:\n        return np.array(poses)", "\ndef generate_transform_noise(sigma_rot_angle = 1.0, sigma_trans = 0.01):\n    '''Generate tranform noise matrix\n    Args:\n        sigma_rot_axis: no influence, because of normalization\n        sigma_rot_angle: degrees\n    '''\n    noise_rot_axis = np.random.normal(0, 1.0, 3)\n    noise_rot_axis = noise_rot_axis / (np.linalg.norm(noise_rot_axis, ord=2) + 1e-6)\n    noise_rot_angle = np.random.normal(0, sigma_rot_angle, 1)\n    noise_rot_mat = R.from_rotvec(noise_rot_angle * noise_rot_axis, degrees=True).as_matrix()\n\n    noise_trans = np.random.normal(0, sigma_trans, 3)\n    logging.debug(f'Noise of rotation axis, {noise_rot_axis}; \\n    rotation angle: {noise_rot_angle}; tranlation: {noise_trans}')\n\n    noise_transform_homo = np.identity(4)\n    noise_transform_homo[:3,:3] = noise_rot_mat\n    noise_transform_homo[:3,3] = noise_trans\n\n    return noise_transform_homo", "\n# depthmap related functions\ndef read_depth_maps_np(dir):\n    '''Read depthmaps in dir with .npy format\n    Return:\n        arr_depths: N*W*H\n    '''\n    vec_path_depths = sorted(glob.glob(f\"{dir}/**.npy\"))\n\n    arr_depth_maps = []\n    for i in range(len(vec_path_depths)):\n        depth_map_curr = np.load(vec_path_depths[i])\n        arr_depth_maps.append(depth_map_curr)\n    \n    arr_depth_maps = np.array(arr_depth_maps)\n    return arr_depth_maps", "\ndef fuse_depthmaps(depthmaps, intrinsics, extrinsics,b_normalize = False):\n    '''\n    args:\n        extrinsics: world to camera\n    return:\n        merged depth map points\n    '''\n    points_fuse = None\n    for i in range(depthmaps.shape[0]):\n        cam_ext, depth =  extrinsics[i], depthmaps[i]\n        cam_int = intrinsics if intrinsics.ndim == 2 else intrinsics[i]\n        # if b_normalize:\n        #     depth = scale * depth\n        points = get_world_points(depth, cam_int, cam_ext)\n        if points_fuse is None:\n            points_fuse = points\n        else:\n            points_fuse = np.concatenate((points_fuse, points), axis = 0)\n    return points_fuse", "\ndef calculate_normalmap_from_depthmap(depthmap, intrin, extrin, num_nearest_neighbors=100):\n    '''\n    Args:\n        depthmap: H*W. depth in image plane\n        extrin: word to cam\n    Return:\n        normalmap: H*W*3\n    '''\n    pts = get_world_points(depthmap, intrin, extrin)\n    cam_center = get_camera_origins([extrin])[0]\n    H, W = depthmap.shape\n\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(pts)\n    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamKNN(knn=num_nearest_neighbors))\n    normals = np.array(pcd.normals)\n    \n    # check normal direction: if ray dir and normal angle is smaller than 90, reverse normal\n    ray_dir = pts-cam_center.reshape(1,3)\n    normal_dir_not_correct = (ray_dir*normals).sum(axis=-1) > 0\n    logging.info(f'Normals with wrong direction: {normal_dir_not_correct.sum()}')\n    normals[normal_dir_not_correct] = -normals[normal_dir_not_correct]\n\n    return pts, normals.reshape(H,W,3)", "\ndef save_points(path_save, pts, colors = None, normals = None, BRG2RGB=False):\n    '''save points to point cloud using open3d\n    '''\n    assert len(pts) > 0\n    if colors is not None:\n        assert colors.shape[1] == 3\n    assert pts.shape[1] == 3\n    cloud = o3d.geometry.PointCloud()\n    cloud.points = o3d.utility.Vector3dVector(pts)\n    if colors is not None:\n        # Open3D assumes the color values are of float type and in range [0, 1]\n        if np.max(colors) > 1:\n            colors = colors / np.max(colors)\n        if BRG2RGB:\n            colors = np.stack([colors[:, 2], colors[:, 1], colors[:, 0]], axis=-1)\n        cloud.colors = o3d.utility.Vector3dVector(colors) \n    if normals is not None:\n        cloud.normals = o3d.utility.Vector3dVector(normals) \n\n    o3d.io.write_point_cloud(path_save, cloud)", "\ndef write_point_cloud(path_save, cloud):\n    o3d.io.write_point_cloud(path_save, cloud)\n    \ndef read_point_cloud(path_cloud):\n    assert IOUtils.checkExistence(path_cloud)\n    cloud = o3d.io.read_point_cloud(path_cloud)\n    # o3d.visualization.draw_geometries([cloud])\n    return cloud\n\ndef get_norm_matrix_from_cam_centers(dir_scan, exts, cam_sphere_radius):\n    '''NOrmalize camera centers into a sphere\n    Args:\n        exts: camera poses, from world to camera\n    '''\n    cam_centers = get_camera_origins(exts)\n    save_points(f\"{dir_scan}/cam_centers_origin.ply\", cam_centers)\n    \n    min_bound, max_bound, center = get_aabb(cam_centers)\n    scale = (max_bound-min_bound).max() / cam_sphere_radius # normalize camera centers to a 0.3 bounding box\n\n    scale_n2w = np.diag([scale, scale, scale, 1.0])\n    translate_n2w = np.identity(4)\n    translate_n2w[:3,3] = center\n    \n    trans_n2w = translate_n2w @ scale_n2w \n    return trans_n2w ", "\ndef get_norm_matrix_from_cam_centers(dir_scan, exts, cam_sphere_radius):\n    '''NOrmalize camera centers into a sphere\n    Args:\n        exts: camera poses, from world to camera\n    '''\n    cam_centers = get_camera_origins(exts)\n    save_points(f\"{dir_scan}/cam_centers_origin.ply\", cam_centers)\n    \n    min_bound, max_bound, center = get_aabb(cam_centers)\n    scale = (max_bound-min_bound).max() / cam_sphere_radius # normalize camera centers to a 0.3 bounding box\n\n    scale_n2w = np.diag([scale, scale, scale, 1.0])\n    translate_n2w = np.identity(4)\n    translate_n2w[:3,3] = center\n    \n    trans_n2w = translate_n2w @ scale_n2w \n    return trans_n2w ", "\ndef get_norm_matrix_from_point_cloud(pcd, radius_normalize_sphere = 1.0):\n    '''Normalize point cloud into a sphere\n    '''\n    # if not checkExistence(path_cloud):\n    #     logging.error(f\"Path is not existent. [{path_cloud}]\")\n    #     exit()\n\n    # pcd = read_point_cloud(path_cloud)\n    min_bound, max_bound, pcd_center = get_aabb(pcd)\n    logging.debug(f\"Point cloud center: {pcd_center}\")\n\n    edges_half =  np.linalg.norm(max_bound-min_bound, ord=2) / 2  #np.concatenate((max_bound -pcd_center, pcd_center -min_bound))\n    max_edge_half = np.max(edges_half)\n    scale = max_edge_half / radius_normalize_sphere\n    scale_n2w = np.diag([scale, scale, scale, 1.0])\n    translate_n2w = np.identity(4)\n    translate_n2w[:3,3] = pcd_center\n\n    trans_n2w = translate_n2w @ scale_n2w #@ rx_homo @ rx_homo_2\n\n    return trans_n2w", "\ndef get_projection_matrix(dir_scan, intrin, poses, trans_n2w):\n    '''\n    Args:\n        poses: world to camera\n    '''\n    num_poses = poses.shape[0]\n    \n    projs = []\n    poses_norm = []\n    dir_pose_norm = dir_scan + \"/pose_norm\"\n    IOUtils.ensure_dir_existenceirExistence(dir_pose_norm)\n    for i in range(num_poses):\n        # pose_norm_i = poses[i] @ trans_n2w\n\n        # Method 2\n        pose = np.copy(poses[i])\n        rot = pose[:3,:3]\n        trans = pose[:3,3]\n\n        cam_origin_world = - np.linalg.inv(rot) @ trans.reshape(3,1)\n        cam_origin_world_homo = np.concatenate([cam_origin_world,[[1]]], axis=0)\n        cam_origin_norm = np.linalg.inv(trans_n2w) @ cam_origin_world_homo\n        trans_norm = -rot @ cam_origin_norm[:3]\n\n        pose[:3,3] = np.squeeze(trans_norm)\n        poses_norm.append(pose)\n        proj_norm = intrin @ pose\n        projs.append(proj_norm)\n        \n        np.savetxt(f'{dir_pose_norm}/{i:04d}.txt', pose)   # camera to world\n        np.savetxt(f'{dir_pose_norm}/{i:04d}_inv.txt', get_pose_inv(pose) )  # world to world\n    return np.array(projs), np.array(poses_norm)", "\ndef generate_rays(img_size, intrin, pose = None, normalize_dir = True):\n    '''Generate rays with specified size, intrin and pose.\n    Args:\n        intrin: 4*4\n        pose: 4*4, (default: None, identity), camera to world\n    Return:\n        rays_o, rays_d: H*W*3, numpy array\n    '''\n    if pose is None:\n        pose = np.identity(4)\n    pose = torch.tensor(pose)\n    intrin = torch.tensor(intrin)\n\n    W,H = img_size\n    tu = torch.linspace(0, W - 1, W)\n    tv = torch.linspace(0, H - 1, H)\n    pixels_v, pixels_u = torch.meshgrid(tv, tu)\n    p = torch.stack([pixels_u, pixels_v, torch.ones_like(pixels_v )], dim=-1) # W, H, 3\n    p = torch.matmul(torch.linalg.inv(intrin)[None, None, :3, :3], p[:, :, :, None]).squeeze()  # W, H, 3\n    if normalize_dir:\n        # for volume rendering, depth along ray\n        rays_v = p  / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)\n    else:\n        # for reprojection of depthmap, depth along z axis\n        rays_v = p\n    rays_v = torch.matmul(pose[None, None, :3, :3], rays_v[:, :, :, None]).squeeze()\n    rays_o = pose[None, None, :3, 3].expand(rays_v.shape) \n    return rays_o.cpu().numpy(), rays_v.cpu().numpy()", "\ndef clean_mesh_faces_outside_frustum(path_save_clean, path_mesh, \n                                        path_intrin,  dir_poses,  \n                                        target_img_size,  reso_level = 1.0,\n                                        path_mask_npz = None,\n                                        check_existence = True):\n    '''Remove faces of mesh which cannot be orserved by all cameras\n    '''\n    # if path_mask_npz:\n    #     path_save_clean = IOUtils.add_file_name_suffix(path_save_clean, '_mask')\n\n    if check_existence and IOUtils.checkExistence(path_save_clean):\n        logging.info(f'The source mesh is already cleaned. [{path_save_clean.split(\"/\")[-1]}]')\n        return path_save_clean\n\n    if path_mask_npz:\n        target_2dmask_mesh = np.load(path_mask_npz)['arr_0']\n    mesh = trimesh.load(path_mesh)\n    intersector = trimesh.ray.ray_pyembree.RayMeshIntersector(mesh)\n\n    intrin = read_cam_matrix(path_intrin)\n    intrin = resize_cam_intrin(intrin, resolution_level=reso_level)\n    if reso_level > 1.0:\n        W = target_img_size[0] // reso_level\n        H = target_img_size[1] // reso_level\n        target_img_size = (W,H)\n    \n    vec_path_poses = IOUtils.get_files_path(dir_poses, '.txt')\n    all_indices = []\n    for i in tqdm(range(len(vec_path_poses))):\n        path_pose = vec_path_poses[i]\n        ppath, stem, ext = IOUtils.get_path_components(path_pose)\n        pose = read_cam_matrix(path_pose)\n        rays_o, rays_d = generate_rays(target_img_size, intrin, pose)\n        rays_o = rays_o.reshape(-1,3)\n        rays_d = rays_d.reshape(-1,3)\n\n        idx_faces_hits = intersector.intersects_first(rays_o, rays_d)\n        if path_mask_npz:\n            mask_mesh_i = target_2dmask_mesh[i].reshape(-1)\n            idx_faces_hits[mask_mesh_i==False] = -1\n        all_indices.append(idx_faces_hits)\n    \n    values = np.unique(np.array(all_indices)) \n    mask_faces = np.ones(len(mesh.faces))\n    mask_faces[values[1:]] = 0\n    logging.info(f'Surfaces/Kept: {len(mesh.faces)}/{len(values)}')\n    \n    mesh_o3d = read_triangle_mesh(path_mesh)\n    mesh_o3d.remove_triangles_by_mask(mask_faces)\n    print(f'Before cleaning: {len(mesh_o3d.vertices)}')\n    # mesh_o3d.remove_triangles_by_mask(mask_faces)\n    mesh_o3d.remove_unreferenced_vertices()\n    print(f'After cleaning: {len(mesh_o3d.vertices)}')\n        \n    write_triangle_mesh(path_save_clean, mesh_o3d)", "\ndef get_camera_view_direction(intrin, extrin, size_frustum):\n    '''\n    Return:\n        cam_center: in world coordinates\n        view_dir: in world coordinates\n    '''\n    cam_center = get_camera_origins(np.array([extrin]))[0]\n\n    ixx_center_image = np.array([size_frustum[0]//2, size_frustum[1]//2, 1, 1])\n    view_dir_image = np.linalg.inv(intrin) @ ixx_center_image\n    extrin2 = copy.deepcopy(extrin)\n    extrin2[:3,3] = 0  # waring: remove translation\n    view_dir_world = np.linalg.inv(extrin2) @ view_dir_image\n    return cam_center, view_dir_world", "\ndef clean_mesh_points_outside_frustum(path_save_clean, path_mesh, \n                                        path_intrin,  dir_poses,\n                                        target_img_size, reso_level = 1, enlarge_frustum = 0.,\n                                        path_mask_npz = None,\n                                        check_existence = True):\n    '''Remove points of mesh which cannot be orserved by all cameras\n    Args:\n        enlarge_frustum: pixels\n    '''\n    if check_existence and IOUtils.checkExistence(path_save_clean):\n        logging.info(f'The source mesh is already cleaned. [{path_save_clean.split(\"/\")[-1]}]')\n        return path_save_clean\n\n    if path_mask_npz:\n        target_2dmask_mesh = np.load(path_mask_npz)['arr_0']\n\n    mesh_o3d = read_triangle_mesh(path_mesh)\n    points = np.array(mesh_o3d.vertices)\n    points_homo = np.concatenate( (points, np.ones((len(points), 1))), axis=-1 )\n    mask_inside_all = np.zeros(len(points)).astype(bool)\n    mesh_mask_outside_all = np.zeros(len(points)).astype(bool)\n\n    intrin = read_cam_matrix(path_intrin)    \n    if reso_level > 1:\n        intrin = resize_cam_intrin(intrin, reso_level)\n        target_img_size = (target_img_size[0]//reso_level, target_img_size[1]//reso_level)\n    vec_path_poses = IOUtils.get_files_path(dir_poses, '.txt')\n\n    for i in tqdm(range(len(vec_path_poses))):\n        path_pose = vec_path_poses[i]\n        pose = read_cam_matrix(path_pose)\n        extrin = np.linalg.inv(pose)\n        \n        # remove backside points\n        cam_center, view_dir_cam = get_camera_view_direction(intrin, extrin, target_img_size)\n        view_dirs = points - cam_center\n        angles = calculate_normal_angle(view_dirs, view_dir_cam[:3])\n        mask_front_side =( angles <= np.pi/2.0)\n\n        # reproject points to camera coordinates\n        points_homo_cam = extrin@points_homo.transpose((1, 0))\n        points_homo_image = intrin @ points_homo_cam\n        points_homo_image = (points_homo_image / points_homo_image[2])[:2] # x,y: u,v\n\n        mask_inside_frustum = (points_homo_image[0] < target_img_size[0]+enlarge_frustum) & (points_homo_image[0] >= -enlarge_frustum) &\\\n                    (points_homo_image[1] < target_img_size[1]+enlarge_frustum) & (points_homo_image[1] >= -enlarge_frustum)\n        \n        mask_inside_curr = mask_inside_frustum & mask_front_side\n        if path_mask_npz:\n            points_homo_image_curr = points_homo_image.transpose()[mask_inside_curr]\n            idx_uv = np.floor(points_homo_image_curr).astype(int)\n            mask_mesh_i = target_2dmask_mesh[i]\n\n            inside_gt_mask = mask_mesh_i[idx_uv[:, 1], idx_uv[:, 0]]\n            num = inside_gt_mask.sum()\n            # mask_inside_curr[mask_inside_curr] = inside_gt_mask\n            mesh_mask_outside_all[mask_inside_curr] =  mesh_mask_outside_all[mask_inside_curr] | (inside_gt_mask==False)\n            # mask_inside_curr = mask_inside_curr & mask_mesh_i & \n        mask_inside_all = mask_inside_all | mask_inside_curr\n        # print(mask_inside_all.sum())\n\n    mesh_o3d.remove_vertices_by_mask((mask_inside_all==False) | mesh_mask_outside_all )\n    write_triangle_mesh(path_save_clean, mesh_o3d)", "\ndef clean_mesh_points_outside_bbox(path_clean, path_mesh, path_mesh_gt, scale_bbox = 1.0, check_existence = True):\n    if check_existence and IOUtils.checkExistence(path_clean):\n        logging.info(f'The source mesh is already cleaned. [{path_clean.split(\"/\")[-1]}]')\n        return\n\n    mesh_o3d = read_triangle_mesh(path_mesh)\n    points = np.array(mesh_o3d.vertices)\n    mask_inside_all = np.zeros(len(points)).astype(bool)\n\n    mesh_gt = read_triangle_mesh(path_mesh_gt)\n    min_bound, max_bound, center = get_aabb(mesh_gt, scale_bbox)\n    mask_low = (points - min_bound) >= 0\n    mask_high = (points - max_bound) <= 0\n    mask_inside_all = (mask_low.sum(axis=-1) == 3) & (mask_high.sum(axis=-1) == 3)\n\n    mesh_o3d.remove_vertices_by_mask(mask_inside_all==False)\n    write_triangle_mesh(path_clean, mesh_o3d)", "\ndef calculate_normal_angle(normal1, normal2, use_degree = False):\n    '''Get angle to two vectors\n    Args:\n        normal1: N*3\n        normal2: N*3\n    Return:\n        angles: N*1\n    '''\n    check_dim = lambda normal : np.expand_dims(normal, axis=0) if normal.ndim == 1 else normal\n    normal1 = check_dim(normal1)\n    normal2 = check_dim(normal2)\n\n    inner = (normal1*normal2).sum(axis=-1)\n    norm1 = np.linalg.norm(normal1, axis=1, ord=2)\n    norm2 = np.linalg.norm(normal2, axis=1, ord=2)\n    angles_cos = inner / (norm1*norm2+1e-6)\n    angles = np.arccos(angles_cos)\n    if use_degree:\n        angles = angles/np.pi * 180\n\n    assert not np.isnan(angles).any()\n    return angles", "\ndef generate_mesh_2dmask(path_save_npz, path_mesh, path_intrin, dir_poses, mask_size, reso_level=1, check_existence = True):\n    '''Generate 2D masks of a mesh, given intrinsics, poses and mask size\n    '''\n    '''Remove faces of mesh which cannot be orserved by all cameras\n    '''\n    # if reso_level > 1.0:\n    #     path_save_npz = IOUtils.add_file_name_suffix(path_save_npz, f'_reso{reso_level}')\n\n    if check_existence and IOUtils.checkExistence(path_save_npz):\n        logging.info(f'The 2D mask of mesh is already generated. [{path_save_npz.split(\"/\")[-1]}]')\n        return path_save_npz\n\n    mesh = trimesh.load(path_mesh)\n    intersector = trimesh.ray.ray_pyembree.RayMeshIntersector(mesh)\n\n    intrin = read_cam_matrix(path_intrin)\n    intrin = resize_cam_intrin(intrin, resolution_level=reso_level)\n    if reso_level > 1.0:\n        W = mask_size[0] // reso_level\n        H = mask_size[1] // reso_level\n        mask_size = (W,H)\n    \n    vec_path_poses = IOUtils.get_files_path(dir_poses, '.txt')\n    vec_path_imgs = IOUtils.get_files_path(dir_poses + '/../image', '.png')\n    all_hits = []\n    for i in tqdm(range(len(vec_path_poses))):\n        path_pose = vec_path_poses[i]\n        ppath, stem, ext = IOUtils.get_path_components(path_pose)\n        pose = read_cam_matrix(path_pose)\n        rays_o, rays_d = generate_rays(mask_size, intrin, pose)\n        rays_o = rays_o.reshape(-1,3)\n        rays_d = rays_d.reshape(-1,3)\n\n        hit_faces_ = intersector.intersects_any(rays_o, rays_d)\n        all_hits.append(hit_faces_)\n\n        # img = ImageUtils.read_image(vec_path_imgs[i], target_img_size=mask_size)\n        # img[hit_faces_.reshape(mask_size[::-1])==False] = (0,0,255)\n        # cv2.imwrite(f'./test/{i:04d}.png', img)\n\n    \n    all_hits = np.array(all_hits)\n    mask_2d_mesh = np.array(all_hits).reshape(tuple([len(vec_path_poses)]) + mask_size[::-1])\n    np.savez(path_save_npz, mask_2d_mesh)\n    logging.info(f'Rays, hits, ratio: {mask_2d_mesh.size, mask_2d_mesh.sum(), mask_2d_mesh.sum()/mask_2d_mesh.size}')\n    return path_save_npz", "\n# transformation\ndef transform_mesh(path_mesh, trans, path_save = None):\n    '''Transfrom mesh using the transformation matrix\n    Args:\n        path_mesh\n        trans: 4*4\n    Return:\n        mesh_trans\n    '''\n    mesh = read_triangle_mesh(path_mesh)\n    mesh_trans = copy.deepcopy(mesh)\n    mesh_trans.transform(trans)\n    if path_save is not None:\n        write_triangle_mesh(path_save, mesh_trans)\n    return mesh, mesh_trans", "\n"]}
{"filename": "utils/utils_normal.py", "chunked_list": ["# some snippets are borrowed from https://github.com/baegwangbin/surface_normal_uncertainty\nimport numpy as np\nimport torch\nfrom pathlib import Path\nimport glob\nfrom tqdm import tqdm\nfrom PIL import Image\nimport cv2\n\nimport utils.utils_geometry as GeoUtils", "\nimport utils.utils_geometry as GeoUtils\nimport utils.utils_image as ImageUtils\nimport utils.utils_io as IOUtils\n\ndef compute_normal_errors_metrics(total_normal_errors):\n    metrics = {\n        'mean': np.average(total_normal_errors),\n        'median': np.median(total_normal_errors),\n        'rmse': np.sqrt(np.sum(total_normal_errors * total_normal_errors) / total_normal_errors.shape),\n        'a1': 100.0 * (np.sum(total_normal_errors < 5) / total_normal_errors.shape[0]),\n        'a2': 100.0 * (np.sum(total_normal_errors < 7.5) / total_normal_errors.shape[0]),\n        'a3': 100.0 * (np.sum(total_normal_errors < 11.25) / total_normal_errors.shape[0]),\n        'a4': 100.0 * (np.sum(total_normal_errors < 22.5) / total_normal_errors.shape[0]),\n        'a5': 100.0 * (np.sum(total_normal_errors < 30) / total_normal_errors.shape[0])\n    }\n    return metrics", "\n# log normal errors\ndef log_normal_errors(metrics, where_to_write = None, first_line = ''):\n    print(first_line)\n    print(\"mean   median   rmse   5    7.5   11.25   22.5    30\")\n    print(\"%.3f %.3f %.3f %.3f %.3f %.3f %.3f %.3f \\n\" % (\n        metrics['mean'], metrics['median'], metrics['rmse'],\n        metrics['a1'], metrics['a2'], metrics['a3'], metrics['a4'], metrics['a5']))\n\n    if where_to_write is not None:\n        with open(where_to_write, 'a') as f:\n            f.write('%s\\n' % first_line)\n            f.write(\"mean median rmse 5 7.5 11.25 22.5 30\\n\")\n            f.write(\"%.3f %.3f %.3f %.3f %.3f %.3f %.3f %.3f\\n\\n\" % (\n                metrics['mean'], metrics['median'], metrics['rmse'],\n                metrics['a1'], metrics['a2'], metrics['a3'], metrics['a4'], metrics['a5']))", "\ndef calculate_normal_error(pred_norm, gt_norm, mask = None):\n    if not torch.is_tensor(pred_norm):\n        pred_norm = torch.from_numpy(pred_norm)\n    if not torch.is_tensor(gt_norm):\n        gt_norm = torch.from_numpy(gt_norm)\n    prediction_error = torch.cosine_similarity(pred_norm, gt_norm, dim=1)\n    prediction_error = torch.clamp(prediction_error, min=-1.0, max=1.0)\n    E = torch.acos(prediction_error) * 180.0 / np.pi\n    # mask = None\n    if mask is not None:\n        return E[mask]\n    else:\n        return E", "    \ndef visualiza_normal(path, normal, extrin = None):\n    if extrin is not None:\n        shape = normal.shape\n        normal = GeoUtils.get_world_normal(normal.reshape(-1,3), extrin).reshape(shape)\n    pred_norm_rgb = ((normal + 1) * 0.5) * 255\n    pred_norm_rgb = np.clip(pred_norm_rgb, a_min=0, a_max=255)\n    if path is not None:\n        ImageUtils.write_image(path, pred_norm_rgb, color_space='RGB')\n    return pred_norm_rgb", "     \ndef evauate_normal(dir_normal_neus, dir_normal_pred, dir_normal_gt, dir_poses, interval = 1):\n    vec_path_normal_neus = sorted(glob.glob(f'{dir_normal_neus}/*.npz'))\n    vec_path_normal_pred = sorted(glob.glob(f'{dir_normal_pred}/*.npz'))\n    #assert len(vec_path_normal_neus) == len(vec_path_normal_pred)\n    \n    target_img_size = (640, 480)\n    input_width, input_height = target_img_size\n    \n    num_normals = len(vec_path_normal_neus)\n    num_imgs_eval_gt = 0\n    \n    dir_normal_neus_eval = dir_normal_neus + '_eval'\n    IOUtils.ensure_dir_existence(dir_normal_neus_eval)\n    \n    error_neus_all, error_pred_all, ratio_all = [], [], []\n    for i in tqdm(range(0, num_normals, interval)):\n        stem = Path(vec_path_normal_neus[i]).stem[9:13]\n        idx_img = int(stem)\n        \n        # 2. load GT normal       \n        path_normal_gt = f'{dir_normal_gt}/frame-{idx_img:06d}-normal.png'\n        path_normal_mask_gt = f'{dir_normal_gt}/frame-{idx_img:06d}-orient.png'\n        if not IOUtils.checkExistence(path_normal_gt) or stem in ['0300', '0330']:\n            continue\n        \n        # print(path_normal_neus)\n        normal_gt_cam = Image.open(path_normal_gt).convert(\"RGB\").resize(size=(input_width, input_height), \n                                                                resample=Image.NEAREST)\n        normal_gt_cam = ((np.array(normal_gt_cam).astype(np.float32) / 255.0) * 2.0) - 1.0\n\n\n        # 1. load neus and predicted normal\n        path_normal_neus =vec_path_normal_neus[i] # f'{dir_normal_neus}/00160000_{i:04d}_reso1.npz'\n        normal_neus_world =  np.load(path_normal_neus)['arr_0']\n        path_normal_pred  = f'{dir_normal_pred}/{stem}.npz'\n        normal_pred_camera = -np.load(path_normal_pred)['arr_0']  # flip predicted camera\n        if normal_pred_camera.shape[0] != input_height:\n            normal_pred_camera = cv2.resize(normal_pred_camera, target_img_size, interpolation=cv2.INTER_NEAREST)\n        \n        # 2. normalize neus_world\n        normal_neus_world_norm = np.linalg.norm(normal_neus_world, axis=-1, keepdims=True)\n        # print(f'normal_neus_world_norm shape: {normal_neus_world_norm.shape}  {normal_neus_world.shape}')\n        normal_neus_world = normal_neus_world/normal_neus_world_norm\n        # print(f'Normalized shape: {normal_neus_world.shape}')\n        # input('Continue?')\n\n        # load_GT image\n        path_img_gt  = f'{dir_normal_pred}/../image/{stem}.png'\n        img_rgb  = ImageUtils.read_image(path_img_gt, color_space='RGB')\n\n        \n        # 3. transform normal\n        pose = np.loadtxt(f'{dir_poses}/{idx_img:04d}.txt')\n        normal_pred_world = GeoUtils.get_world_normal(normal_pred_camera.reshape(-1,3), np.linalg.inv(pose))\n        normal_gt_world = GeoUtils.get_world_normal(normal_gt_cam.reshape(-1,3), np.linalg.inv(pose))\n        \n        shape_img = normal_neus_world.shape\n        img_visual_neus = visualiza_normal(None, -normal_neus_world, pose)\n        img_visual_pred = visualiza_normal(None,  -normal_pred_world.reshape(shape_img), pose)\n        img_visual_gt = visualiza_normal(None, -normal_gt_world.reshape(shape_img), pose)\n        ImageUtils.write_image_lis(f'{dir_eval}/{stem}.png', [img_rgb, img_visual_pred, img_visual_neus, img_visual_gt], color_space='RGB')\n\n\n        mask_gt = Image.open(path_normal_mask_gt).convert(\"RGB\").resize(size=(input_width, input_height),  resample=Image.NEAREST)           \n        mask_gt = np.array(mask_gt) \n        mask_gt = np.logical_not(\n                np.logical_and(\n                    np.logical_and(\n                        mask_gt[:, :, 0] == 127, mask_gt[:, :, 1] == 127),\n                    mask_gt[:, :, 2] == 127))\n        norm_valid_mask = mask_gt[:, :, np.newaxis]\n        ratio = norm_valid_mask.sum() /norm_valid_mask.size\n        # cv2.imwrite('./test.png',norm_valid_mask.astype(np.float)*255 )\n        ratio_all.append(ratio)\n        \n        error_neus = calculate_normal_error(normal_neus_world.reshape(-1,3), normal_gt_world, norm_valid_mask.reshape(-1))\n        error_pred = calculate_normal_error(normal_pred_world, normal_gt_world, norm_valid_mask.reshape(-1))\n        \n        error_neus_all.append(error_neus)\n        error_pred_all.append(error_pred)\n        num_imgs_eval_gt += 1\n\n    error_neus_all = torch.cat(error_neus_all).numpy()\n    error_pred_all = torch.cat(error_pred_all).numpy()\n    \n    # error_neus_all = total_normal_errors.data.cpu().numpy()\n    metrics_neus = compute_normal_errors_metrics(error_neus_all)\n    metrics_pred = compute_normal_errors_metrics(error_pred_all)\n    # print(f'Neus error: \\n{metrics_neus}\\nPred error: \\n{metrics_pred}')\n    print(f'Num imgs for evaluation: {num_imgs_eval_gt}')\n    log_normal_errors(metrics_neus, first_line='metrics_neus')\n    log_normal_errors(metrics_pred, first_line='metrics_pred')\n    return error_neus_all, error_pred_all, num_imgs_eval_gt"]}
{"filename": "utils/utils_io.py", "chunked_list": ["from datetime import datetime\nimport os, sys, logging\nimport shutil\nimport subprocess\nfrom pathlib import Path\nimport glob\n\n\n# Path\ndef checkExistence(path):\n    if not os.path.exists(path):\n        return False\n    else:\n        return True", "# Path\ndef checkExistence(path):\n    if not os.path.exists(path):\n        return False\n    else:\n        return True\n\ndef ensure_dir_existence(dir):\n    try:\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n        else:\n            logging.info(f\"Dir is already existent: {dir}\")\n    except Exception:\n        logging.error(f\"Fail to create dir: {dir}\")\n        exit()", "\ndef get_path_components(path):\n    path = Path(path)\n    ppath = str(path.parent)\n    stem = str(path.stem)\n    ext = str(path.suffix)\n    return ppath, stem, ext\n\ndef add_file_name_suffix(path_file, suffix):\n    ppath, stem, ext = get_path_components(path_file)\n    \n    path_name_new = ppath + \"/\" + stem + str(suffix) + ext\n    return path_name_new", "def add_file_name_suffix(path_file, suffix):\n    ppath, stem, ext = get_path_components(path_file)\n    \n    path_name_new = ppath + \"/\" + stem + str(suffix) + ext\n    return path_name_new\n\ndef add_file_name_prefix(path_file, prefix, check_exist = True):\n    '''Add prefix before file name\n    '''\n    ppath, stem, ext = get_path_components(path_file)\n    path_name_new = ppath + \"/\" + str(prefix) + stem  + ext\n\n    if check_exist:\n        ensure_dir_existence(ppath + \"/\" + str(prefix))\n      \n    return path_name_new", "\ndef add_file_name_prefix_and_suffix(path_file, prefix, suffix, check_exist = True):\n    path_file_p = add_file_name_prefix(path_file, prefix, check_exist = True)\n    path_file_p_s = add_file_name_suffix(path_file_p, suffix)\n    return path_file_p_s\n\ndef get_files_stem(dir, ext_file):\n    '''Get stems of all files in directory with target extension\n    Return:\n        vec_stem\n    '''\n    vec_path = sorted(glob.glob(f'{dir}/**{ext_file}'))\n    vec_stem = []\n    for i in range(len(vec_path)):\n        pparent, stem, ext = get_path_components(vec_path[i])\n        vec_stem.append(stem)\n    return vec_stem", "\ndef get_files_path(dir, ext_file):\n    return sorted(glob.glob(f'{dir}/**{ext_file}'))\n\n# IO\ndef readLines(path_txt):\n    fTxt = open(path_txt, \"r\")\n    lines = fTxt.readlines()\n    return lines\n\ndef copy_file(source_path, target_dir):\n    try: \n        ppath, stem, ext = get_path_components(target_dir)\n        ensure_dir_existence(ppath)\n        shutil.copy(source_path, target_dir)\n    except Exception:\n        logging.error(f\"Fail to copy file: {source_path}\")\n        exit(-1)", "\ndef copy_file(source_path, target_dir):\n    try: \n        ppath, stem, ext = get_path_components(target_dir)\n        ensure_dir_existence(ppath)\n        shutil.copy(source_path, target_dir)\n    except Exception:\n        logging.error(f\"Fail to copy file: {source_path}\")\n        exit(-1)\n\ndef remove_dir(dir):\n    try:\n        shutil.rmtree(dir)\n    except Exception as ERROR_MSG:\n        logging.error(f\"{ERROR_MSG}.\\nFail to remove dir: {dir}\")\n        exit(-1)", "\ndef remove_dir(dir):\n    try:\n        shutil.rmtree(dir)\n    except Exception as ERROR_MSG:\n        logging.error(f\"{ERROR_MSG}.\\nFail to remove dir: {dir}\")\n        exit(-1)\n\ndef copy_dir(source_dir, target_dir):\n    try:\n        if not os.path.exists(source_dir):\n            logging.error(f\"source_dir {source_dir} is not exist. Fail to copy directory.\")\n            exit(-1)\n        shutil.copytree(source_dir, target_dir)\n    except Exception as ERROR_MSG:\n        logging.error(f\"{ERROR_MSG}.\\nFail to copy file: {source_dir}\")\n        exit(-1)", "def copy_dir(source_dir, target_dir):\n    try:\n        if not os.path.exists(source_dir):\n            logging.error(f\"source_dir {source_dir} is not exist. Fail to copy directory.\")\n            exit(-1)\n        shutil.copytree(source_dir, target_dir)\n    except Exception as ERROR_MSG:\n        logging.error(f\"{ERROR_MSG}.\\nFail to copy file: {source_dir}\")\n        exit(-1)\n\ndef INFO_MSG(msg):\n    print(msg)\n    sys.stdout.flush()", "\ndef INFO_MSG(msg):\n    print(msg)\n    sys.stdout.flush()\n    \ndef changeWorkingDir(working_dir):\n    try:\n        os.chdir(working_dir)\n        print(f\"Current working directory is { os.getcwd()}.\")\n    except OSError:\n        print(\"Cann't change current working directory.\")\n        sys.stdout.flush()\n        exit(-1)", "\ndef run_subprocess(process_args):\n    pProcess = subprocess.Popen(process_args)\n    pProcess.wait()\n    \ndef find_target_file(dir, file_name):\n    all_files_recur = glob.glob(f'{dir}/**{file_name}*', recursive=True)\n    path_target = None\n    if len(all_files_recur) == 1:\n        path_target = all_files_recur[0]\n\n    assert not len(all_files_recur) > 1\n    return path_target", "\ndef copy_files_in_dir(dir_src, dir_target, ext_file, rename_mode = 'stem'):\n    '''Copy files in dir and rename it if needed\n    '''\n    ensure_dir_existence(dir_target)\n    vec_path_files = sorted(glob.glob(f'{dir_src}/*{ext_file}'))\n    for i in range(len(vec_path_files)):\n        path_src = vec_path_files[i]\n    \n        if rename_mode == 'stem':\n            pp, stem, _ = get_path_components(path_src)\n            path_target = f'{dir_target}/{stem}{ext_file}'\n        elif rename_mode == 'order':\n            path_target = f'{dir_target}/{i}{ext_file}'\n        elif rename_mode == 'order_04d':\n            path_target = f'{dir_target}/{i:04d}{ext_file}'\n        else:\n            NotImplementedError\n\n        copy_file(path_src, path_target)\n    return len(vec_path_files)", "\n# time-related funcs\ndef get_consumed_time(t_start):\n    '''\n    Return:\n        time: seconds\n    '''\n    t_end = datetime.now()\n    return (t_end-t_start).total_seconds()\n\ndef get_time_str(fmt='HMSM'):\n    if fmt == 'YMD-HMS':\n        str_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n    elif fmt == 'HMS':\n        str_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n    elif fmt == 'HMSM':\n        str_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n    return str_time", "\ndef get_time_str(fmt='HMSM'):\n    if fmt == 'YMD-HMS':\n        str_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n    elif fmt == 'HMS':\n        str_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n    elif fmt == 'HMSM':\n        str_time = datetime.now().strftime(\"%H_%M_%S_%f\")\n    return str_time\n\ndef write_list_to_txt(path_list, data_list):\n    num_lines = len(data_list)\n    with open(path_list, 'w') as flis:\n        for i in range(len(data_list)):\n            flis.write(f'{data_list[i]}\\n')", "\ndef write_list_to_txt(path_list, data_list):\n    num_lines = len(data_list)\n    with open(path_list, 'w') as flis:\n        for i in range(len(data_list)):\n            flis.write(f'{data_list[i]}\\n')\n            \nif __name__ == \"__main__\":\n    logging.basicConfig(\n        format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S',\n        level=logging.INFO,\n        stream=sys.stdout,\n        filename='example.log'\n    )"]}
{"filename": "utils/utils_image.py", "chunked_list": ["import matplotlib.pyplot as plt\nimport torch\nimport glob, os\nimport cv2\nimport logging\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport copy\n", "import copy\n\nfrom skimage.color import rgb2gray\nfrom skimage.filters import sobel\nfrom skimage.segmentation import felzenszwalb\nfrom skimage.util import img_as_float\nfrom tqdm import tqdm\n\nimport utils.utils_io as IOUtils\n", "import utils.utils_io as IOUtils\n\nDIR_FILE = os.path.abspath(os.path.dirname(__file__))\n\ndef calculate_normal_angle(normal1, normal2, use_degree = False):\n    '''Get angle to two vectors\n    Args:\n        normal1: N*3\n        normal2: N*3\n    Return:\n        angles: N*1\n    '''\n    check_dim = lambda normal : np.expand_dims(normal, axis=0) if normal.ndim == 1 else normal\n    normal1 = check_dim(normal1)\n    normal2 = check_dim(normal2)\n\n    inner = (normal1*normal2).sum(axis=-1)\n    norm1 = np.linalg.norm(normal1, axis=1, ord=2)\n    norm2 = np.linalg.norm(normal2, axis=1, ord=2)\n    angles_cos = inner / (norm1*norm2 + 1e-6)\n    angles = np.arccos(angles_cos)\n    if use_degree:\n        angles = angles/np.pi * 180\n\n    assert not np.isnan(angles).any()\n    return angles", "\ndef read_image(path, target_img_size = None, interpolation=cv2.INTER_LINEAR, color_space = 'BGR'):\n    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    if target_img_size is not None:\n        img= resize_image(img, target_img_size, interpolation=interpolation)\n    if color_space == 'RGB':\n        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n    return img\n\ndef write_image(path, img, color_space = None, \n                    target_img_size = None, interpolation = cv2.INTER_LINEAR):\n    '''If color space is defined, convert colors to RGB mode\n    Args:\n        target_img_size: resize image if defined\n    \n    '''\n    if color_space == 'RGB':\n        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n    if target_img_size is not None:\n        img = resize_image(img, target_size=target_img_size, interpolation=interpolation)\n    cv2.imwrite(path, img)", "\ndef write_image(path, img, color_space = None, \n                    target_img_size = None, interpolation = cv2.INTER_LINEAR):\n    '''If color space is defined, convert colors to RGB mode\n    Args:\n        target_img_size: resize image if defined\n    \n    '''\n    if color_space == 'RGB':\n        img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n    if target_img_size is not None:\n        img = resize_image(img, target_size=target_img_size, interpolation=interpolation)\n    cv2.imwrite(path, img)", "\ndef write_images(dir, imgs, stems = None, ext_img = '.png', color_space = None):\n    IOUtils.ensure_dir_existence(dir)\n    for i in range(len(imgs)):\n        if stems is None:\n            path = f'{dir}/{i:04d}{ext_img}'\n        else:\n            path = f'{dir}/{stems[i]}{ext_img}'\n        write_image(path, imgs[i], color_space)\n\ndef read_images(dir, target_img_size = None, interpolation=cv2.INTER_LINEAR, img_ext = '.png', use_rgb_mode = False, vec_stems = None):\n    f'''Read images in directory with extrension {img_ext}\n    Args:\n        dir: directory of images\n        target_img_size: if not none, resize read images to target size\n        img_ext: defaut {img_ext}\n        use_rgb_mode: convert brg to rgb if true\n    Return:\n        imgs: N*W*H\n        img_stems\n    '''\n    if img_ext == '.npy':\n        read_img = lambda path : np.load(path)\n    elif img_ext == '.npz':\n        read_img = lambda path : np.load(path)['arr_0']\n    elif img_ext in ['.png', '.jpg']:\n        read_img = lambda path : read_image(path)\n    else:\n        raise NotImplementedError\n\n    vec_path = sorted(glob.glob(f\"{dir}/**{img_ext}\"))\n    if vec_stems is not None:\n        vec_path = []\n        for stem_curr in vec_stems:\n            vec_path.append(f'{dir}/{stem_curr}{img_ext}')\n        vec_path = sorted(vec_path)\n\n    rgbs = []\n    img_stems = []\n    for i in range(len(vec_path)):\n        img = read_img(vec_path[i])\n        if (target_img_size != None) and (target_img_size[0] != img.shape[1]):\n            img= cv2.resize(img, target_img_size, interpolation=cv2.INTER_LINEAR)\n        if use_rgb_mode:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        rgbs.append(img)\n\n        _, stem, _ = IOUtils.get_path_components(vec_path[i])\n        img_stems.append(stem)\n    return np.array(rgbs), img_stems", "\ndef read_images(dir, target_img_size = None, interpolation=cv2.INTER_LINEAR, img_ext = '.png', use_rgb_mode = False, vec_stems = None):\n    f'''Read images in directory with extrension {img_ext}\n    Args:\n        dir: directory of images\n        target_img_size: if not none, resize read images to target size\n        img_ext: defaut {img_ext}\n        use_rgb_mode: convert brg to rgb if true\n    Return:\n        imgs: N*W*H\n        img_stems\n    '''\n    if img_ext == '.npy':\n        read_img = lambda path : np.load(path)\n    elif img_ext == '.npz':\n        read_img = lambda path : np.load(path)['arr_0']\n    elif img_ext in ['.png', '.jpg']:\n        read_img = lambda path : read_image(path)\n    else:\n        raise NotImplementedError\n\n    vec_path = sorted(glob.glob(f\"{dir}/**{img_ext}\"))\n    if vec_stems is not None:\n        vec_path = []\n        for stem_curr in vec_stems:\n            vec_path.append(f'{dir}/{stem_curr}{img_ext}')\n        vec_path = sorted(vec_path)\n\n    rgbs = []\n    img_stems = []\n    for i in range(len(vec_path)):\n        img = read_img(vec_path[i])\n        if (target_img_size != None) and (target_img_size[0] != img.shape[1]):\n            img= cv2.resize(img, target_img_size, interpolation=cv2.INTER_LINEAR)\n        if use_rgb_mode:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        rgbs.append(img)\n\n        _, stem, _ = IOUtils.get_path_components(vec_path[i])\n        img_stems.append(stem)\n    return np.array(rgbs), img_stems", "\ndef get_planes_from_normalmap(dir_pred, thres_uncertain = 0):\n    '''Get normals from normal map for indoor dataset (ScanNet), which was got by the following paper:\n    Surface normal estimation with uncertainty\n    '''\n    dir_normals = os.path.join(dir_pred, 'pred_normal')\n    vec_path_imgs = sorted(glob.glob(f'{dir_normals}/**.png'))\n    num_images = len(vec_path_imgs)\n    assert num_images > 0\n    logging.info(f'Found images: {num_images}')\n\n    dir_mask_labels = os.path.join(dir_normals, '../pred_normal_planes')\n    dir_mask_labels_rgb = os.path.join(dir_normals, '../pred_normal_planes_rgb')\n    os.makedirs(dir_mask_labels, exist_ok=True)\n    os.makedirs(dir_mask_labels_rgb, exist_ok=True)\n\n    vec_path_kappa =  sorted(glob.glob(f'{dir_normals}/../pred_kappa/**.png'))\n    dir_normals_certain = f'{dir_normals}/../pred_normal_certain'\n    os.makedirs(dir_normals_certain, exist_ok=True)\n\n    channel_threshold = 200\n    channel_threshold_curr = channel_threshold\n\n    for j in tqdm(range(num_images)):\n        path = vec_path_imgs[j]\n        _, stem, ext = IOUtils.get_path_components(path)\n\n        img = read_image(path)\n        \n        if thres_uncertain > 0:\n            img_kappa = read_image(vec_path_kappa[j])\n            mask_uncertain = img_kappa < thres_uncertain\n            img[mask_uncertain] = 0\n            write_image(f'{dir_normals_certain}/{stem}{ext}', img)\n        \n        img_masks = []\n        imgs_lables = np.zeros((img.shape[0], img.shape[1]))\n        for i in range(3):\n            ch = img[:,:, i]\n\n            ch_mask = ch > channel_threshold\n            test = ch_mask.sum()\n            while ch_mask.sum() == 0:\n                channel_threshold_curr -= 10\n                ch_mask = ch > channel_threshold_curr\n            channel_threshold_curr = channel_threshold\n            \n            if i==2:\n                ch_mask = img[:,:, 0] > 0\n                if ch_mask.sum() ==0:\n                    ch_mask = img[:,:, 1] > 0\n\n            ch_arr = ch[ch_mask]\n            count_arr = np.bincount(ch[ch_mask])\n            ch_value_most = np.argmax(count_arr)\n            logging.info(f\"[{j}-{i}] Channel value most: {ch_value_most}\")\n\n            # ch_value_most = np.max(ch)\n            sample_range_half = 5\n            range_min = ch_value_most - sample_range_half if ch_value_most > sample_range_half else 0\n            range_max = ch_value_most + sample_range_half if ch_value_most < (255-sample_range_half) else 255\n            logging.debug(f'Sample range: [{range_min}, {range_max}]')\n\n            # ToDO: check other channels\n\n            ch_mask = (ch > range_min) & (ch < range_max)\n            ch = ch * ch_mask\n            \n            img_masks.append(ch_mask)\n            imgs_lables[ch_mask] = i + 1\n\n            img[ch_mask] = 0\n\n        img = np.stack(img_masks, axis=-1).astype(np.float) * 255\n        write_image(f'{dir_mask_labels_rgb}/{stem}{ext}', img) \n        write_image(f'{dir_mask_labels}/{stem}{ext}', imgs_lables) ", "\ndef cluster_normals_kmeans(path_normal, n_clusters=12, thres_uncertain = -1, folder_name_planes = 'pred_normal_planes'):\n    '''Use k-means to cluster normals of images.\n    Extract the maximum 6 planes, where the second largest 3 planes will remove the uncertain pixels by thres_uncertain\n    '''\n    PROP_PLANE = 0.03\n    PROP_DOMINANT_PLANE = 0.05\n    ANGLE_DIFF_DOMINANT_PLANE_MIN = 75\n    ANGLE_DIFF_DOMINANT_PLANE_MAX = 105\n    MAGNITUDE_PLANES_MASK = 1\n    \n    \n    path_img_normal = path_normal[:-4]+'.png'\n    img = np.load(path_normal)['arr_0']\n\n    shape = img.shape\n    num_pixels_img = shape[0] * shape [1]\n    MIN_SIZE_PIXELS_PLANE_AREA = num_pixels_img*0.02\n\n    if thres_uncertain > 0:\n        path_alpha = IOUtils.add_file_name_prefix(path_normal, '../pred_alpha/')\n        img_alpha = np.load(path_alpha)['arr_0']\n        mask_uncertain = img_alpha > thres_uncertain\n\n    path_rgb = IOUtils.add_file_name_prefix(path_img_normal, '../image/')\n    img_rgb = read_image(path_rgb)[:,:,:3]\n    img_rgb = resize_image(img_rgb, target_size=(shape[1], shape[0], 3))\n        # img[mask_uncertain] = 0\n        \n        # path_uncertain = os.path.abspath( IOUtils.add_file_name_prefix(path_img_normal, '../pred_uncertain/') )\n        # os.makedirs(os.path.dirname(path_uncertain),exist_ok=True)\n        # write_image(path_uncertain, img)\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(img.reshape(-1, 3))\n    pred = kmeans.labels_\n    centers = kmeans.cluster_centers_\n\n    num_max_planes = 7\n    count_values = np.bincount(pred)\n    max5 = np.argpartition(count_values,-num_max_planes)[-num_max_planes:]\n    prop_planes = np.sort(count_values[max5] / num_pixels_img)[::-1]\n    # logging.info(f'Proportion of planes: {prop_planes}; Proportion of the 3 largest planes: {np.sum(prop_planes[:3]):.04f}; Image name: {path_img_normal.split(\"/\")[-1]}')\n    centers_max5 = centers[max5]\n    sorted_idx_max5 = np.argsort(count_values[max5] / num_pixels_img)\n    sorted_max5 = max5[sorted_idx_max5][::-1]\n    \n    sorted_centers_max5  = centers_max5[sorted_idx_max5][::-1]\n    # idx_center_uncertain_area = -1\n    # for i in range(len(sorted_centers_max5)):\n    #     # print(sorted_centers_max5[i].sum())\n    #     if sorted_centers_max5[i].sum() < 1e-6:\n    #         idx_center_uncertain_area = i\n    # if idx_center_uncertain_area >= 0:\n    #     sorted_max5 = np.delete(sorted_max5, idx_center_uncertain_area)\n\n    \n    colors_planes =[(255,0,0), (0,255,0), (0,0,255), (255,255,0), (0,255,255), (255,0,255)]  # r,g,b, yellow, Cyan, Magenta\n    planes_rgb = np.zeros(shape).reshape(-1,3)\n    img_labels = np.zeros([*shape[:2]]).reshape(-1,1)\n    count_planes = 0\n    angles_diff = np.zeros(3)\n    for i in range(6):\n        curr_plane = (pred==sorted_max5[count_planes])\n\n        # remove small isolated areas\n        mask_clean = remove_small_isolated_areas((curr_plane>0).reshape(*shape[:2])*255, min_size=MIN_SIZE_PIXELS_PLANE_AREA).reshape(-1)\n        curr_plane[mask_clean==0] = 0\n    \n        ratio_curr_plane = curr_plane.sum() / num_pixels_img\n\n        check_sim = True\n        if check_sim:\n            # (1) check plane size\n            if ratio_curr_plane < PROP_PLANE: # small planes: ratio > 2%\n                continue\n            if i < 3 and ratio_curr_plane < PROP_DOMINANT_PLANE: # dominant planes: ratio > 10%\n                continue\n            \n            # (2) check normal similarity\n            eval_metric = 'angle'\n            if eval_metric == 'distance':\n                curr_normal = sorted_centers_max5[count_planes]\n                thres_diff = ANGLE_DIFF_DOMINANT_PLANE\n                if i ==1: \n                    dist1 = np.linalg.norm(sorted_centers_max5[i-1]-curr_normal).sum()\n                    angles_diff[0] = dist1\n                    if dist1 < thres_diff:\n                        continue\n                if i == 2:\n                    dist1 = np.linalg.norm(sorted_centers_max5[count_planes-1]-curr_normal).sum()\n                    dist2 = np.linalg.norm(sorted_centers_max5[0]-curr_normal).sum()\n                    angles_diff[1] = dist1\n                    angles_diff[2] = dist2\n                    if dist1 < thres_diff or dist2 < thres_diff:\n                        continue\n                        print(f'Dist1, dist2: {dist1, dist2}')\n            if eval_metric == 'angle':\n                curr_normal = sorted_centers_max5[count_planes]\n                thres_diff_min = ANGLE_DIFF_DOMINANT_PLANE_MIN\n                thres_diff_max = ANGLE_DIFF_DOMINANT_PLANE_MAX\n                if i ==1: \n                    angle1 = calculate_normal_angle(sorted_centers_max5[i-1], curr_normal, use_degree=True)\n                    angles_diff[0] = angle1\n                    if angle1 < thres_diff_min or angle1 > thres_diff_max:\n                        continue\n                if i == 2:\n                    angle1 = calculate_normal_angle(sorted_centers_max5[count_planes-1], curr_normal, use_degree=True)\n                    angle2 = calculate_normal_angle(sorted_centers_max5[0], curr_normal, use_degree=True)\n                    angles_diff[1] = angle1\n                    angles_diff[2] = angle2\n                    if angle1 < thres_diff_min or angle2 < thres_diff_min or \\\n                        angle1 >  thres_diff_max or angle2 >  thres_diff_max :\n                        continue\n                        print(f'Dist1, dist2: {dist1, dist2}')                \n        count_planes += 1\n        img_labels[curr_plane] = (i+1)*MAGNITUDE_PLANES_MASK\n\n        if thres_uncertain > -1:\n            # remove the influence of uncertainty pixels on small planes\n            curr_plane[mask_uncertain.reshape(-1)] = 0\n        planes_rgb[curr_plane] = colors_planes[i]\n\n        # save current plane\n        if img_rgb is not None:\n            path_planes_visual_compose = IOUtils.add_file_name_prefix(path_img_normal, f\"../{folder_name_planes}_visual_compose/{i+1}/\",check_exist=True)\n            mask_non_plane = (curr_plane < 1).reshape(shape[:2])\n            img_compose = copy.deepcopy(img_rgb)\n            img_compose[mask_non_plane] = 0\n            write_image(path_planes_visual_compose, img_compose)\n\n\n\n        # else:\n        #     center0, center1, center2, center3 = centers[sorted_max5[0]], centers[sorted_max5[1]],centers[sorted_max5[2]], centers[sorted_max5[3]]\n        #     diff = center1 - center2\n        #     dist12 = np.linalg.norm(diff)\n        #     if dist12 < 50:\n        #         img_labels[pred==sorted_max5[3]]= i+1\n        #         curr_channel = (pred==sorted_max5[3])\n        #     else:\n        #         img_labels[pred==sorted_max5[2]]= i+1\n        #         curr_channel = (pred==sorted_max5[2])\n        \n    img_labels = img_labels.reshape(*shape[:2])\n    path_labels = IOUtils.add_file_name_prefix(path_img_normal, f\"../{folder_name_planes}/\")\n    write_image(path_labels, img_labels)\n    msg_log = f'{path_img_normal.split(\"/\")[-1]}: {prop_planes} {np.sum(prop_planes[:3]):.04f} {1.0 - (img_labels==0).sum() / num_pixels_img : .04f}. Angle differences (degrees): {angles_diff}'\n    logging.info(msg_log)\n\n    # planes_rgb = np.stack(planes_rgb, axis=-1).astype(np.float32)*255\n    # visualization\n    planes_rgb = planes_rgb.reshape(shape)\n    path_planes_visual = IOUtils.add_file_name_prefix(path_img_normal, f\"../{folder_name_planes}_visual/\", check_exist=True)\n    planes_rgb = cv2.cvtColor(planes_rgb.astype(np.uint8), cv2.COLOR_BGR2RGB)\n    \n    mask_planes = planes_rgb.sum(axis=-1)>0\n    # mask_planes = np.stack([mask_planes]*3, axis=-1)\n    curre_img_ = copy.deepcopy(img_rgb)\n    curre_img_[mask_planes==False] = (255,255,255)\n    # planes_rgb_cat = np.concatenate((planes_rgb, curre_img_, img_rgb))\n    # write_image(path_planes_visual, planes_rgb_cat)\n\n    # visualize plane error\n    img_normal_error = np.zeros(img.shape[:2])\n    MAX_ANGLE_ERROR = 15\n    for i in range(int(np.max(img_labels))):\n        id_label = i+1\n        mask_plane_curr = (img_labels==id_label)\n        if mask_plane_curr.sum() ==0:\n            continue\n        normal_curr_plane = img[mask_plane_curr]\n\n        mean_normal_curr = normal_curr_plane.mean(axis=0)\n        angle_error = calculate_normal_angle(normal_curr_plane, mean_normal_curr, use_degree=True)\n        img_normal_error[mask_plane_curr] = np.abs(angle_error)\n\n    if thres_uncertain > -1:\n        # remove the influence of uncertainty pixels on small planes\n        img_normal_error[mask_uncertain] = 0\n    \n    path_planes_visual_error = IOUtils.add_file_name_prefix(path_img_normal, f\"../{folder_name_planes}_visual_error/\", check_exist=True)\n    path_planes_visual_error2 = IOUtils.add_file_name_suffix(path_planes_visual_error, \"_jet\")\n    \n    img_normal_error_cmap =  convert_gray_to_cmap(img_normal_error.clip(0, MAX_ANGLE_ERROR))\n    # img_normal_error_cmap = convert_color_BRG2RGB(img_normal_error_cmap)\n    img_normal_error_cmap[mask_planes==False] = (255,255,255)\n    \n    img_normal_error_stack = np.stack([img_normal_error.clip(0, MAX_ANGLE_ERROR)]*3, axis=-1)/MAX_ANGLE_ERROR*255\n    img_normal_error_stack[mask_planes==False] = (255,255,255)\n\n    # img_gap = 255 * np.ones((img_normal_error.shape[0], 20, 3)).astype('uint8')\n    # write_image(path_planes_visual_error, np.concatenate([img_rgb, img_gap, planes_rgb, img_gap, img_normal_error_cmap, img_gap, img_normal_error_stack], axis=1))\n    write_image_lis(path_planes_visual, [img_rgb, planes_rgb, curre_img_, img_normal_error_cmap, img_normal_error_stack])\n    # plt.imsave(path_planes_visual_error2, img_normal_error, vmin=0, vmax=MAX_ANGLE_ERROR, cmap = 'jet')\n    # plt.imsave(path_planes_visual_error, img_normal_error, vmin=0, vmax=MAX_ANGLE_ERROR, cmap = 'gray')\n    \n    # img_normal_error = img_normal_error/np.max(img_normal_error) * 255\n    # write_image(path_planes_visual_error, img_normal_error)  \n\n    # if planes_rgb.shape[2] > 3:\n    #     path_planes_visual2 = IOUtils.add_file_name_suffix(path_planes_visual, '_2')\n    #     write_image(path_planes_visual2, planes_rgb[:,:, 3:])\n\n    # img2 = copy.deepcopy((img_rgb))\n    # mask_planes2_reverse = (planes_rgb[:,:, 3:].sum(axis=-1) == 0)\n    # img2[mask_planes2_reverse] = 0\n    # path_planes2_color = IOUtils.add_file_name_suffix(path_planes_visual, f\"_color2\")\n    # write_image(path_planes2_color, img2)\n\n    # img3 = copy.deepcopy((img_rgb))\n    # img3[planes_rgb[:,:, 3:].sum(axis=-1) > 0] = 255\n    # path_color2_reverse = IOUtils.add_file_name_suffix(path_planes_visual, '_color2_reverse')\n    # write_image(path_color2_reverse, img3)\n\n    # path_default = IOUtils.add_file_name_suffix(path_planes_visual, '_default')\n    # write_image(path_default, img_rgb)\n    return msg_log", "\ndef remove_image_background(path_png, path_mask, path_merge = None, reso_level=0):\n    '''Remove image background using mask and resize image if reso_level > 0\n    Args:\n        path_png: path of source image\n    Return:\n        img_with_mask: image without background\n    '''\n    img = cv2.imread(path_png)\n    mask = cv2.imread(path_mask)[:,:,-1]\n    res = cv2.bitwise_and(img, img, mask=mask)\n    mask = np.expand_dims(mask, -1)\n    img_with_mask = np.concatenate((res, mask), axis=2)\n\n    if reso_level > 0:\n        shape_target = img_with_mask.shape[:2] // np.power(2, reso_level)\n        shape_target = (shape_target[1], shape_target[0])\n        img_with_mask = cv2.resize(img_with_mask, shape_target, interpolation=cv2.INTER_LINEAR)\n\n    if path_merge != None:\n        cv2.imwrite(path_merge, img_with_mask)\n    logging.debug(f\"Remove background of img: {path_png}\")\n    return img_with_mask", "\ndef resize_image(img, target_size, interpolation=cv2.INTER_LINEAR):\n    '''Resize image to target size\n    Args:\n        target_size: (W,H)\n    Return img_resize\n    \n    '''\n    W,H = target_size[0], target_size[1]\n    if img.shape[0] != H or img.shape[1] != W:\n        img_resize = cv2.resize(img, (W,H), interpolation=interpolation)\n        return img_resize\n    else:\n        return img", "\ndef convert_images_type(dir_imgs, dir_target, \n                        rename_mode = 'stem',\n                        target_img_size = None, ext_source = '.png', ext_target = '.png', \n                        sample_interval = -1):\n    '''Convert image type in directory from ext_imgs to ext_target\n    '''\n    IOUtils.ensure_dir_existence(dir_target)\n    vec_path_files = sorted(glob.glob(f'{dir_imgs}/**{ext_source}'))\n    stems = []\n    for i in range(len(vec_path_files)):\n        pp, stem, ext = IOUtils.get_path_components(vec_path_files[i])\n        # id_img = int(stem)\n        id_img = i\n        if sample_interval > 1:\n            # sample images\n            if id_img % sample_interval !=0:\n                continue\n            \n        if rename_mode == 'stem':\n            path_target = f'{dir_target}/{stem}{ext_target}'\n        elif rename_mode == 'order':\n            path_target = f'{dir_target}/{i}{ext_target}'\n        elif rename_mode == 'order_04d':\n            path_target = f'{dir_target}/{i:04d}{ext_target}'\n        else:\n            NotImplementedError\n        \n        if ext_source[-4:] == ext_target:\n            IOUtils.copy_file(vec_path_files[i], path_target)\n        else:\n            img = read_image(vec_path_files[i])\n            write_image(path_target, img, target_img_size=target_img_size)\n        \n        stems.append(stem)\n    return len(vec_path_files), stems", "\n# image noise\ndef add_image_noise(image, noise_type='gauss', noise_std = 10):\n    '''Parameters\n    ----------\n    image : ndarray\n        Input image data. Will be converted to float.\n    mode : str\n        One of the following strings, selecting the type of noise to add:\n        'gauss'     Gaussian-distributed additive noise.\n        'poisson'   Poisson-distributed noise generated from the data.\n        's&p'       Replaces random pixels with 0 or 1.\n        'speckle'   Multiplicative noise using out = image + n*image,where\n                    n is uniform noise with specified mean & variance.\n    Ref: https://stackoverflow.com/questions/22937589/how-to-add-noise-gaussian-salt-and-pepper-etc-to-image-in-python-with-opencv\n    '''\n    if noise_type == \"gauss\":\n        row,col,ch= image.shape\n        mean = 0\n        sigma = noise_std\n        logging.debug(f'Gauss noise (mean, sigma): {mean,sigma}')\n        gauss = np.random.normal(mean,sigma,(row,col, ch))\n        gauss = gauss.reshape(row,col, ch)\n        noisy = image + gauss\n        return noisy\n    elif noise_type == \"s&p\":\n        row,col,ch = image.shape\n        s_vs_p = 0.5\n        amount = 0.004\n        out = np.copy(image)\n        # Salt mode\n        num_salt = np.ceil(amount * image.size * s_vs_p)\n        coords = [np.random.randint(0, i - 1, int(num_salt))\n                for i in image.shape]\n        out[coords] = 1\n\n        # Pepper mode\n        num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n        coords = [np.random.randint(0, i - 1, int(num_pepper))\n                for i in image.shape]\n        out[coords] = 0\n        return out\n    elif noise_type == \"poisson\":\n        vals = len(np.unique(image))\n        vals = 2 ** np.ceil(np.log2(vals))\n        noisy = np.random.poisson(image * vals) / float(vals)\n        return noisy\n    elif noise_type ==\"speckle\":\n        row,col,ch = image.shape\n        gauss = np.random.randn(row,col,ch)\n        gauss = gauss.reshape(row,col,ch)        \n        noisy = image + image * gauss\n        return noisy", "\n# lines\ndef extract_lines(dir_image, img_size, focal, ext_img = '.png'):\n    dir_lines = dir_image + '_lines'\n    IOUtils.ensure_dir_existence(dir_lines)\n\n    dir_only_lines = dir_image + '_only_lines'\n    IOUtils.ensure_dir_existence(dir_only_lines)\n\n    vec_path_imgs = glob.glob(f'{dir_image}/**{ext_img}')\n    for i in tqdm(range(len(vec_path_imgs))):\n        path = vec_path_imgs[i]\n        _, stem, ext = IOUtils.get_path_components(path)\n        path_lines_img = f'{dir_lines}/{stem}.png'\n        path_lines_txt = f'{dir_lines}/{stem}.txt'\n        path_log =  f'{dir_lines}/num_lines.log'\n       \n        width = img_size[0]\n        height = img_size[1]\n        \n        path_only_lines =  f'{dir_only_lines}/{stem}.png'\n        os.system(f'{DIR_FILE}/VanishingPoint {path} {path_lines_img} {path_lines_txt} {width} {height} {focal} {path_log} {path_only_lines}')", "\n# image segmentation\ndef extract_superpixel(path_img, CROP = 16):\n    scales, markers = [1], [400]\n\n    img = cv2.imread(path_img)\n    image = copy.deepcopy(img)\n    image = image[CROP:-CROP, CROP:-CROP, :]\n\n    segments = []\n    for s, m in zip(scales, markers):\n        image = cv2.resize(image, (384//s, 288//s), interpolation=cv2.INTER_LINEAR)\n        image = img_as_float(image)\n\n        gradient = sobel(rgb2gray(image))\n        # segment = watershed(gradient, markers=m, compactness=0.001)\n        segment = felzenszwalb(image, scale=100, sigma=0.5, min_size=50)\n        segments.append(segment)\n    img_seg = segments[0].astype(np.int16)\n    img_seg = resize_image(img_seg, target_size=img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n    return img, img_seg", "\ndef find_labels_max_clusters(pred, num_max_clusters):\n    '''Find labels of the maximum n clusters with descending order\n    Args:\n        pred: N*1\n        num_max_clusters: how many clusters to find\n    '''\n    count_values = np.bincount(pred)\n    num_max_clusters = np.min([len(count_values), num_max_clusters])\n    max_planes = np.argpartition(count_values,-num_max_clusters)[-num_max_clusters:]\n    \n    sorted_idx_max_planes = np.argsort(count_values[max_planes] / len(pred))\n    sorted_max_planes = max_planes[sorted_idx_max_planes][::-1]\n    prop_planes = np.sort(count_values[sorted_max_planes] / len(pred))[::-1]\n    return sorted_max_planes, prop_planes, num_max_clusters", "\ndef visualize_semantic_label():\n    # scannet\n    img = read_image('/media/hp/HKUCS2/Dataset/ScanNet/scannet_whole/scene0079_00/label-filt/1.png')\n\n    img_mask = np.zeros(img.shape + tuple([3]))\n    for i in range(int(img.max()+1)):\n        mask_curr = (img == i)\n        img_mask[mask_curr] = np.random.randint(0, 255, 3)\n    write_image('./test.png', img_mask)", "\ndef remove_small_isolated_areas(img, min_size = 3000):\n    f'''Remove the small isolated areas with size smaller than defined {min_size}\n    '''\n    if img.ndim ==3:\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    else:\n        gray = copy.deepcopy(img).astype(np.uint8)\n    ret, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n\n    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(binary, connectivity=8)\n    sizes = stats[1:, -1]; nb_components = nb_components - 1\n\n    img_clean = np.zeros((output.shape))\n    for i in range(0, nb_components):\n        if sizes[i] >= min_size:\n            img_clean[output == i + 1] = 255\n\n    return img_clean", "\ndef convert_gray_to_cmap(img_gray, map_mode = 'jet', revert = True, vmax = None):\n    '''Visualize point distances with 'hot_r' color map\n    Args:\n        cloud_source\n        dists_to_target\n    Return:\n        cloud_visual: use color map, max\n    '''\n    img_gray = copy.deepcopy(img_gray)\n    shape = img_gray.shape\n\n    cmap = plt.get_cmap(map_mode)\n    if vmax is not None:\n        img_gray = img_gray / vmax\n    else:\n        img_gray = img_gray / (np.max(img_gray)+1e-6)\n    if revert:\n        img_gray = 1- img_gray      \n    colors = cmap(img_gray.reshape(-1))[:, :3]\n\n    # visualization\n    colors = colors.reshape(shape+tuple([3]))*255\n    return colors", "\n#image editting\ndef crop_images(dir_images_origin, dir_images_crop, crop_size, img_ext = '.png'):\n    IOUtils.ensure_dir_existence(dir_images_crop)\n    imgs, stems_img = read_images(dir_images_origin, img_ext = img_ext)\n    W_target, H_target = crop_size\n    for i in range(len(imgs)):\n        img_curr = imgs[i]\n        H_origin, W_origin = img_curr.shape[:2]\n        \n        crop_width_half = (W_origin-W_target)//2\n        crop_height_half = (H_origin-H_target) //2\n        assert (W_origin-W_target)%2 ==0 and (H_origin- H_target) %2 == 0\n        \n        img_crop = img_curr[crop_height_half:H_origin-crop_height_half, crop_width_half:W_origin-crop_width_half]\n        if img_ext == '.png':\n            write_image(f'{dir_images_crop}/{stems_img[i]}.png', img_crop)\n        elif img_ext == '.npy':\n            np.save(f'{dir_images_crop}/{stems_img[i]}.npy', img_crop)\n        else:\n            raise NotImplementedError\n        \n    return crop_width_half, crop_height_half", "\ndef split_video_to_frames(path_video, dir_images):\n    IOUtils.ensure_dir_existence(dir_images)\n    vidcap = cv2.VideoCapture(path_video)\n    success,image = vidcap.read()\n    count = 0\n    while success:\n        cv2.imwrite(f'{dir_images}/{count:04d}.png', image)     # save frame as JPEG file      \n        success,image = vidcap.read()\n        logging.info(f'Read a new frame: {count}, {success}.')\n        count += 1\n    logging.info(f'End. Frames: {count}')", "\n# concatenate images\ndef write_image_lis(path_img_cat, lis_imgs, use_cmap = False, interval_img = 20, cat_mode = 'horizontal', color_space = 'BGR'):\n    '''Concatenate an image list to a single image and save it to the target path\n    Args:\n        cat_mode: horizonal/vertical\n    '''\n    img_cat = []\n    for i in range(len(lis_imgs)):\n        img = lis_imgs[i]\n        H, W = img.shape[:2]\n        \n        if use_cmap:\n            img = convert_gray_to_cmap(img) if img.ndim==2 else img\n        else:\n            img = np.stack([img]*3, axis=-1) if img.ndim==2 else img\n        \n        if img.max() <= 1.0:\n            img *= 255\n\n        img_cat.append(img)\n        if cat_mode == 'horizontal':\n            img_cat.append(255 * np.ones((H, interval_img, 3)).astype('uint8'))\n        elif cat_mode == 'vertical':\n            img_cat.append(255 * np.ones((interval_img, W, 3)).astype('uint8'))\n    \n    if cat_mode == 'horizontal':\n        img_cat = np.concatenate(img_cat[:-1], axis=1)\n    elif cat_mode == 'vertical':\n        img_cat = np.concatenate(img_cat[:-1], axis=0)\n    else:\n        raise NotImplementedError\n    if color_space == 'RGB':\n        img_cat = cv2.cvtColor(img_cat.astype(np.uint8), cv2.COLOR_BGR2RGB)\n    cv2.imwrite(path_img_cat, img_cat)", "\n\ndef calculate_psnr_nerf(path_img_src, path_img_gt, mask = None):\n    img_src = read_image(path_img_src) / 255.0 #[:480]\n    img_gt = read_image(path_img_gt) / 255.0\n    # print(img_gt.shape)\n    \n    img_src = torch.from_numpy(img_src)\n    img_gt = torch.from_numpy(img_gt)\n    \n    img2mse = lambda x, y : torch.mean((x - y) ** 2)\n    mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\n\n    err = img2mse(img_src, img_gt)\n    # print(f'Error shape: {err.shape} {err}')\n    psnr = mse2psnr(err)\n    return float(psnr)", "    \n    \ndef eval_imgs_psnr(dir_img_src, dir_img_gt, sample_interval):\n    vec_stems_imgs = IOUtils.get_files_stem(dir_img_src, '.png')   \n    psnr_all = []\n    for i in tqdm(range(0, len(vec_stems_imgs), sample_interval)):\n        stem_img = vec_stems_imgs[i]\n        path_img_src = f'{dir_img_src}/{stem_img}.png'  \n        path_img_gt   = f'{dir_img_gt}/{stem_img[9:13]}.png' \n        # print(path_img_src, path_img_gt)\n        psnr = calculate_psnr_nerf(path_img_src, path_img_gt)\n        # print(f'PSNR: {psnr} {stem_img}')\n        psnr_all.append(psnr)\n    return np.array(psnr_all), vec_stems_imgs", "        \n"]}
{"filename": "preprocess/depthneus_data.py", "chunked_list": ["\nimport os, glob\nimport logging, copy, pickle\n\nimport numpy as np\nfrom cv2 import  cv2\nfrom tqdm import tqdm\n\nfrom preprocess.scannet_data import ScannetData\nfrom utils.utils_geometry import read_cam_matrix, resize_cam_intrin", "from preprocess.scannet_data import ScannetData\nfrom utils.utils_geometry import read_cam_matrix, resize_cam_intrin\nfrom utils.utils_image import cluster_normals_kmeans, find_labels_max_clusters, remove_small_isolated_areas\nfrom utils.utils_io import add_file_name_suffix, checkExistence\nfrom utils.utils_image import extract_superpixel, read_image, read_images, write_image, \\\n                                find_labels_max_clusters, read_image, read_images, write_image\n\nimport utils.utils_geometry as GeoUtils\nimport utils.utils_image  as ImageUtils\nimport utils.utils_io as IOUtils", "import utils.utils_image  as ImageUtils\nimport utils.utils_io as IOUtils\n\nfrom confs.path import path_tiltedsn_pth_pfpn, path_tiltedsn_pth_sr, dir_tiltedsn_code, \\\n                            path_snu_pth, dir_snu_code, \\\n                            lis_name_scenes_remove\n\n\ndef crop_images_depthneus(dir_imgs, dir_imgs_crop, path_intrin, path_intrin_crop, crop_size):\n    assert checkExistence(dir_imgs)\n    crop_width_half, crop_height_half = ImageUtils.crop_images(dir_imgs, dir_imgs_crop, crop_size)\n    GeoUtils.modify_intrinsics_of_cropped_images(path_intrin, path_intrin_crop,  crop_width_half, crop_height_half)", "def crop_images_depthneus(dir_imgs, dir_imgs_crop, path_intrin, path_intrin_crop, crop_size):\n    assert checkExistence(dir_imgs)\n    crop_width_half, crop_height_half = ImageUtils.crop_images(dir_imgs, dir_imgs_crop, crop_size)\n    GeoUtils.modify_intrinsics_of_cropped_images(path_intrin, path_intrin_crop,  crop_width_half, crop_height_half)\n    \ndef extract_planes_from_normals(dir_normal, thres_uncertain = 70, folder_name_planes = 'pred_normal_planes', n_clusters = 12):\n    vec_path_files = sorted(glob.glob(f'{dir_normal}/**.npz'))\n    path_log = f'{dir_normal}/../log_extract_planes.txt'\n    f_log = open(path_log, 'w')\n    for i in tqdm(range(len(vec_path_files))):\n        path = vec_path_files[i]\n        msg_log = cluster_normals_kmeans(path, n_clusters= n_clusters, thres_uncertain = thres_uncertain, folder_name_planes = folder_name_planes)\n        f_log.write(msg_log + '\\n')\n    f_log.close()", "\ndef segment_one_image_superpixels(path_img, folder_visual):\n    # refer to: https://github.com/SJTU-ViSYS/StructDepth for more details\n    MAGNITUDE_PLANE_LABEL = 1\n    image, pred = extract_superpixel(path_img)\n    shape = image.shape\n\n    num_max_clusters= 12\n    pred = pred.reshape(-1)\n    sorted_max5, prop_planes, num_max_clusters = find_labels_max_clusters(pred, num_max_clusters=num_max_clusters)\n\n    # visulize labels\n    colors_planes =[(255,0,0), (0,255,0), (0,0,255), (255,255,0), (0,255,255), (255,0,255)]  # r,g,b, yellow, Cyan, Magenta\n    planes_rgb = np.zeros(shape).reshape(-1,3)\n    img_labels = np.zeros([*shape[:2]]).reshape(-1,1)\n    for i in range(num_max_clusters):\n        curr_plane = (pred==sorted_max5[i])\n        ratio = curr_plane.sum() / shape[0]*shape[1]\n        if ratio < 0.05:\n            continue\n\n        img_labels[curr_plane] = (i+1) * MAGNITUDE_PLANE_LABEL\n        if i < 6:\n            planes_rgb[curr_plane] = colors_planes[i]\n        else:\n            planes_rgb[curr_plane] = np.random.randint(low=0, high=255, size=3)\n\n        \n    img_labels = img_labels.reshape(*shape[:2])\n    write_image(IOUtils.add_file_name_prefix(path_img, f'../{folder_visual}/'), img_labels)\n\n    planes_rgb = planes_rgb.reshape(shape)\n    mask_planes = planes_rgb.sum(axis=-1 )>0\n    mask_planes = np.stack([mask_planes]*3, axis=-1)\n    curre_img_compose = copy.deepcopy(image )\n    curre_img_compose[mask_planes==False] = 0\n    planes_rgb_cat = np.concatenate([planes_rgb, curre_img_compose, image], axis=0)\n    write_image(IOUtils.add_file_name_prefix(path_img, f'../{folder_visual}_visual/'), planes_rgb_cat)\n\n    msg_log = f'{prop_planes} {np.sum(prop_planes[:6]):.04f} {path_img.split(\"/\")[-1]}'\n\n    return msg_log", "\ndef segment_images_superpixels(dir_images):\n    '''Segment images in directory and save segmented images in the same parent folder\n    Args:\n        dir_images\n    Return:\n        None\n    '''\n    vec_path_imgs = IOUtils.get_files_path(dir_images, '.png')\n    path_log = f'{dir_images}/log_seg_images.txt'\n    f_log = open(path_log, 'w')\n    for i in tqdm(range(len(vec_path_imgs))):\n        path = vec_path_imgs[i]\n        msg_log = segment_one_image_superpixels(path, folder_visual='image_planes')\n        f_log.write(msg_log + '\\n')\n    f_log.close()", "\ndef compose_normal_img_planes(dir):\n    MAGNITUDE_COMPOSED_PLANER_LABEL = 1\n    PROPORTION_PLANES = 0.03  # 5%\n\n    dir_img_planes = f'{dir}/image_planes'\n    dir_normal_planes = f'{dir}/pred_normal_planes'\n    \n    dir_planes_compose = f'{dir}/pred_normal_subplanes'\n    dir_planes_compose_visual = f'{dir}/pred_normal_subplanes_visual'\n    IOUtils.ensure_dir_existence(dir_planes_compose)\n    IOUtils.ensure_dir_existence(dir_planes_compose_visual)\n    \n    planes_normal_all, stem_normals = read_images(f'{dir}/pred_normal_planes')\n    planes_color_all, stem_imgs = read_images( f'{dir}/image_planes', target_img_size=planes_normal_all[0].shape[::-1], interpolation=cv2.INTER_NEAREST)\n    imgs_rgb,_ = read_images(f'{dir}/image', target_img_size=planes_normal_all[0].shape[::-1])\n\n    # 0. all planes\n    planes_compose_all = []\n    planes_compose_rgb_all = []\n    f_log = open(f'{dir}/log_num_subplanes.txt', 'w')\n    for i in tqdm(range(len(planes_color_all))):\n        curr_planes_normal = planes_normal_all[i]\n        curr_planes_color = planes_color_all[i]\n        \n        # 1. nornal planes\n        curr_planes_compose = np.zeros_like(curr_planes_normal)\n        curr_planes_compose_rgb = np.zeros((curr_planes_normal.shape[0], curr_planes_normal.shape[1], 3))\n        count_planes = 0\n        for j in range(int(curr_planes_normal.max())):\n            mask_curr_plane_normal = (curr_planes_normal==j+1)\n\n            # make other planes' label be zero\n            curr_planes_color_2 = copy.deepcopy(curr_planes_color)\n            curr_planes_color_2[mask_curr_plane_normal==False] = 0\n\n            sorted_max_planes, prop_planes, num_max_clusters2 = find_labels_max_clusters(curr_planes_color_2.reshape(-1), num_max_clusters=6)\n            \n            # 2. image planes\n            ratios = np.zeros(6)\n            for k in range(len(sorted_max_planes)):\n                # check proportion and remove background\n                if sorted_max_planes[k] == 0:\n                    continue\n                if prop_planes[k] < PROPORTION_PLANES:\n                    continue\n                ratios[k] = prop_planes[k]\n\n                mask_curr_plane_img = (curr_planes_color_2 == sorted_max_planes[k])\n                mask_curr_plane_img = remove_small_isolated_areas(mask_curr_plane_img, min_size=3000) > 0\n                curr_planes_compose_rgb[mask_curr_plane_img] = np.random.randint(low=0, high=255, size=3).astype(np.uint8)\n                # curr_planes_compose_rgb = curr_planes_compose_rgb.astype(np.uint8)\n                \n                count_planes += 1\n                curr_planes_compose[mask_curr_plane_img] = count_planes * MAGNITUDE_COMPOSED_PLANER_LABEL\n        assert count_planes < 255 // MAGNITUDE_COMPOSED_PLANER_LABEL\n        f_log.write(f'{stem_imgs[i]}: {count_planes} \\n')\n        write_image(f'{dir_planes_compose}/{stem_imgs[i]}.png', curr_planes_compose)\n\n        mask_planes = curr_planes_compose_rgb.sum(axis=-1 )>0\n        mask_planes = np.stack([mask_planes]*3, axis=-1)\n        curre_img = copy.deepcopy(imgs_rgb[i])\n        curre_img[mask_planes==False] = 0\n        curr_planes_compose_rgb2 = np.concatenate([curr_planes_compose_rgb, curre_img, imgs_rgb[i]], axis=0)\n        write_image(f'{dir_planes_compose_visual}/{stem_imgs[i]}.png', curr_planes_compose_rgb2)\n\n        planes_compose_rgb_all.append(curr_planes_compose_rgb)\n    f_log.close()", "\n# scannet     \ndef prepare_depthneus_data_from_scannet(dir_scan, dir_neus, sample_interval=6, \n                                    b_sample = True, \n                                    b_crop_images = True,\n                                    b_generate_neus_data = True,\n                                    b_pred_normal = True, normal_method = 'snu',\n                                    b_detect_planes = False):\n    '''Sample iamges (1296,968)\n    '''   \n    \n    H,W, _ = read_image(f'{dir_scan}/rgb/0.jpg').shape\n    path_intrin_color = f'{dir_scan}/../intrinsic_color.txt'\n    \n    if W == 1296: \n        path_intrin_color_crop_resize = f'{dir_scan}/../intrinsic_color_crop1248_resize640.txt'\n        origin_size = (1296, 968)\n        cropped_size = (1248, 936)\n        reso_level = 1.95\n    elif W == 640:\n        path_intrin_color_640 = add_file_name_suffix(path_intrin_color, '_640')\n        if not checkExistence(path_intrin_color_640):\n            intrin_temp = read_cam_matrix(path_intrin_color)\n            intrin_640 = resize_cam_intrin(intrin_temp, resolution_level=1296/640)\n            np.savetxt(path_intrin_color_640, intrin_640, fmt='%f')\n            \n        path_intrin_color = path_intrin_color_640\n        path_intrin_color_crop_resize = f'{dir_scan}/../intrinsic_color_crop640_resize640.txt'\n        origin_size = (640, 480)\n        cropped_size = (624, 468)\n        reso_level = 0.975   \n    else:\n        raise NotImplementedError\n    \n    if not IOUtils.checkExistence(path_intrin_color_crop_resize):\n        crop_width_half = (origin_size[0]-cropped_size[0])//2\n        crop_height_half =(origin_size[1]-cropped_size[1]) //2\n        \n        \n        intrin = np.loadtxt(path_intrin_color)\n        intrin[0,2] -= crop_width_half\n        intrin[1,2] -= crop_height_half\n        intrin_resize = resize_cam_intrin(intrin, reso_level)\n        np.savetxt(path_intrin_color_crop_resize, intrin_resize, fmt = '%f')\n    \n    if b_sample:\n        start_id = 0\n        num_images = len(glob.glob(f\"{dir_scan}/rgb/**.jpg\"))\n        end_id = num_images*2\n        ScannetData.select_data_by_range(dir_scan, dir_neus, start_id, end_id, sample_interval, \n                                            b_crop_images, cropped_size)\n    # prepare neus data\n    if b_generate_neus_data:\n        dir_neus = dir_neus\n        height, width =  480, 640\n\n        msg = input('Remove pose (nan)...[y/n]') # observe pose file size\n        if msg != 'y':\n            exit()\n\n        path_intrin_color = f'{dir_scan}/../../intrinsic_color.txt'\n        if b_crop_images:\n            path_intrin_color = path_intrin_color_crop_resize\n        path_intrin_depth = f'{dir_scan}/../../intrinsic_depth.txt'\n        dataset = ScannetData(dir_neus, height, width, \n                                    use_normal = False,\n                                    path_intrin_color = path_intrin_color,\n                                    path_intrin_depth = path_intrin_depth)\n        dataset.generate_neus_data()\n\n        if b_pred_normal:\n            predict_normal(dir_neus, normal_method)\n\n        # detect planes\n        if b_detect_planes == True:\n            extract_planes_from_normals(dir_neus + '/pred_normal', thres_uncertain=-1)\n            segment_images_superpixels(dir_neus + '/image')\n            compose_normal_img_planes(dir_neus)", "        \n# privivate data\ndef prepare_depthneus_data_from_private_data(dir_neus, size_img = (6016, 4016),\n                                            b_generate_neus_data = True,\n                                            b_pred_normal = True, normal_method = 'snu',\n                                            b_detect_planes = False):   \n    '''Run sfm and prepare neus data\n    '''\n    W, H = size_img\n    \n    path_intrin_color = f'{dir_neus}/intrinsics.txt'\n    if b_generate_neus_data:\n        dataset = ScannetData(dir_neus, H, W, use_normal=False,\n                                    path_intrin_color = path_intrin_color,\n                                    path_intrin_depth = path_intrin_color, \n                                    path_cloud_sfm=f'{dir_neus}/point_cloud_openMVS.ply')\n        dataset.generate_neus_data()\n    \n    if b_pred_normal:\n        predict_normal(dir_neus, normal_method)\n\n        \n    # detect planes\n    if b_detect_planes == True:\n        extract_planes_from_normals(dir_neus + '/pred_normal', thres_uncertain=-1)\n        segment_images_superpixels(dir_neus + '/image')\n        compose_normal_img_planes(dir_neus)", "\ndef predict_normal(dir_neus, normal_method = 'snu'):\n    # For scannet data, retraining of normal network is required to guarantee the test scenes are in the test set of normal network.\n    # For your own data, the officially provided pretrained model of the normal network can be utilized directly.\n    if normal_method == 'snu':\n        # ICCV2021, https://github.com/baegwangbin/surface_normal_uncertainty\n        logging.info('Predict normal')\n        IOUtils.changeWorkingDir(dir_snu_code)\n        os.system(f'python test.py --pretrained scannet --path_ckpt {path_snu_pth} --architecture BN --imgs_dir {dir_neus}/image/')\n\n    # Tilted-SN\n    if normal_method == 'tiltedsn':\n        # ECCV2020, https://github.com/MARSLab-UMN/TiltedImageSurfaceNormal\n        IOUtils.changeWorkingDir(dir_tiltedsn_code)\n        os.system(f'python inference_surface_normal.py --checkpoint_path {path_tiltedsn_pth_pfpn} \\\n                                --sr_checkpoint_path {path_tiltedsn_pth_sr} \\\n                                --log_folder {dir_neus} \\\n                                --operation inference \\\n                                --batch_size 8 \\\n                                --net_architecture sr_dfpn \\\n                                --test_dataset {dir_neus}/image')", " \n# normal prior\ndef update_pickle_TiltedSN_rectified_2dofa(path_pickle, path_update, scenes_remove):\n    '''Remove part of training data\n    '''\n    data_split = pickle.load(open(path_pickle, 'rb'))\n    keys_splits = ['train'] #, 'test'\n    keys_subsplicts = ['e2', '-e2']\n\n    masks_all = []\n    remove_all = {}\n    for key_split in keys_splits:\n        for key_subsplit in keys_subsplicts:\n            data_curr = data_split[key_split][key_subsplit]\n            len_data_curr = len(data_curr)\n            mask_curr = np.ones(len_data_curr).astype(bool)\n            remove_all[key_split+'_'+key_subsplit] =  []\n            for i in range(len_data_curr):\n                path_i = data_curr[i].split('/')\n                scene_i = path_i[0]\n                if scene_i[:-3] in scenes_remove:\n                    mask_curr[i] = False\n                    if scene_i not in remove_all[key_split+'_'+key_subsplit]:\n                        remove_all[key_split+'_'+key_subsplit].append(scene_i)\n            \n            remove_all[key_split+'_'+key_subsplit] = sorted(remove_all[key_split+'_'+key_subsplit])\n            logging.info(f'{key_split} {key_subsplit}: {int(mask_curr.sum())}/{len_data_curr}. Ratio: {int(mask_curr.sum())/len_data_curr:.03f}')\n            masks_all.append(mask_curr)\n            # remove_all.append(remove_curr)\n\n            # update original data\n            data_split[key_split][key_subsplit] = np.array(data_split[key_split][key_subsplit])[mask_curr].tolist()\n            len_update = len(data_split[key_split][key_subsplit])\n            logging.info(f'[{key_split} {key_subsplit}]:{len_data_curr}->{len_update}')\n            \n    # save data\n    with open(path_update, 'wb') as handle:\n        pickle.dump(data_split, handle, protocol=pickle.HIGHEST_PROTOCOL)", "\n    # debug\n    # data_split2 = pickle.load(open(path_update, 'rb'))\n    # print('done')\n\ndef update_pickle_TiltedSN_full_2dofa(path_pickle, path_update, scenes_remove):\n    '''Remove part of training data\n    '''\n    data_split = pickle.load(open(path_pickle, 'rb'))\n    keys_splits = ['train']\n    keys_subsplicts_l1 = ['with_ga', 'no_ga']\n    keys_subsplicts_l2 = ['e2', '-e2']\n\n    masks_all = []\n    remove_all = {}\n    for key_split in keys_splits:\n        for key_l1 in keys_subsplicts_l1:   \n            for key_l2 in keys_subsplicts_l2:\n                if key_l1 == 'with_ga':\n                    data_curr = data_split[key_split][key_l1][key_l2]\n                elif key_l1 == 'no_ga':\n                    data_curr = data_split[key_split][key_l1]\n                else:\n                    raise NotImplementedError\n                \n                len_data_curr = len(data_curr)\n                mask_curr = np.ones(len_data_curr).astype(bool)\n                remove_all[key_split + '_'+ key_l1 + '_'  + key_l2] =  []\n                for i in range(len_data_curr):\n                    path_i = data_curr[i].split('/')\n                    scene_i = path_i[0]\n                    if scene_i[:-3] in scenes_remove:\n                        mask_curr[i] = False\n                        if scene_i not in remove_all[key_split + '_'+ key_l1 + '_'  + key_l2]:\n                            remove_all[key_split + '_'+ key_l1 + '_'  + key_l2].append(scene_i)\n            \n                logging.info(f'[{key_split} {key_l1} {key_l2}]: {int(mask_curr.sum())}/{len_data_curr}. Ratio: {int(mask_curr.sum())/len_data_curr:.03f}')\n                masks_all.append(mask_curr)\n                remove_all[key_split + '_'+ key_l1 + '_'  + key_l2] = sorted(remove_all[key_split + '_'+ key_l1 + '_'  + key_l2])\n                # remove_all.append(remove_curr)\n\n                # update original data\n                if key_l1 == 'with_ga':\n                    data_split[key_split][key_l1][key_l2] = np.array(data_split[key_split][key_l1][key_l2])[mask_curr].tolist()\n                    len_update = len(data_split[key_split][key_l1][key_l2])\n                elif key_l1 == 'no_ga':\n                    data_split[key_split][key_l1] = np.array(data_split[key_split][key_l1])[mask_curr].tolist()\n                    len_update = len(data_split[key_split][key_l1])\n                else:\n                    raise NotImplementedError\n\n                logging.info(f'[{key_split} {key_l1} {key_l2}]:{len_data_curr}->{len_update}')\n            \n    # save data\n    with open(path_update, 'wb') as handle:\n        pickle.dump(data_split, handle, protocol=pickle.HIGHEST_PROTOCOL)", "\n    # # debug\n    # data_split2 = pickle.load(open(path_update, 'rb'))\n    # print('done')\n\ndef update_pickle_file_TiltedSN():\n    '''Remove part of training data\n    '''\n    path_pkl = '../TiltedImageSurfaceNormal/data/rectified_2dofa_framenet.pkl' \n    path_update = IOUtils.add_file_name_suffix(path_pkl, \"_update\")\n    update_pickle_TiltedSN_rectified_2dofa(path_pkl, path_update, lis_name_scenes_remove)\n\n    path_pkl = '../TiltedImageSurfaceNormal/data/full_2dofa_framenet.pkl'  \n    path_update = IOUtils.add_file_name_suffix(path_pkl, \"_update\")\n    update_pickle_TiltedSN_full_2dofa(path_pkl, path_update, lis_name_scenes_remove)", "\ndef update_pickle_framenet(path_pickle, path_update):\n    '''Remove part of training data\n    '''\n    scenes_remove = lis_name_scenes_remove\n    \n    data_split = pickle.load(open(path_pickle, 'rb'))\n    keys_splits = ['train'] #, 'test'\n    keys_subsplicts = [0, 1, 2]\n\n    masks_all = []\n    remove_all = {}\n    data_split_update = {'train':[],\n                         'test':[]}\n    scenes_all_used = []\n    scenes_all = {}                     \n    for key_split in keys_splits:\n        for key_subsplit in keys_subsplicts:\n            data_curr = data_split[key_split][key_subsplit]  \n            key_subsplit2 = str(key_subsplit)          \n            len_data_curr = len(data_curr)\n            mask_curr = np.ones(len_data_curr).astype(bool)\n            remove_all[key_split+'_'+key_subsplit2] =  []\n            \n            scenes_all[key_split]  = []\n            for i in range(len_data_curr):\n                path_i = data_curr[i].split('/')\n                scene_i = path_i[4]\n                if scene_i[:-3] in scenes_remove:\n                    mask_curr[i] = False\n                    if scene_i not in remove_all[key_split+'_'+key_subsplit2]:\n                        remove_all[key_split+'_'+key_subsplit2].append(scene_i)\n                # if scene_i[:-3] not in scenes_all[key_split]:\n                scenes_all[key_split].append(scene_i)\n            remove_all[key_split+'_'+key_subsplit2] = sorted(remove_all[key_split+'_'+key_subsplit2])\n            logging.info(f'{key_split} {key_subsplit}: {int(mask_curr.sum())}/{len_data_curr}. Ratio: {int(mask_curr.sum())/len_data_curr:.03f}')\n            masks_all.append(mask_curr)\n            # remove_all.append(remove_curr)\n            scenes_all[key_split] = np.unique(np.array(scenes_all[key_split]))\n\n            # update original data\n            data_update_ = np.array(data_split[key_split][key_subsplit])[mask_curr].tolist()\n            data_split_update[key_split].append(data_update_)\n            len_update = len(data_update_)\n            logging.info(f'[{key_split} {key_subsplit}]:{len_data_curr}->{len_update}')\n            \n            num_remove = len(remove_all[key_split+'_'+key_subsplit2])\n            logging.info(f'[{key_split}] scenes_all: {len(scenes_all[key_split])}, scenes-remove: {num_remove}')\n            scenes_all_used.append(scenes_all) \n            \n    # save data\n    data_split['train'] = ( data_split_update['train'][0],data_split_update['train'][1], data_split_update['train'][2])\n    with open(path_update, 'wb') as handle:\n        pickle.dump(data_split, handle, protocol=pickle.HIGHEST_PROTOCOL)", "\n    # # debug\n    # data_split2 = pickle.load(open(path_update, 'rb'))\n    # print('done')"]}
{"filename": "preprocess/sfm_pipeline.py", "chunked_list": ["#!/usr/bin/python\n#! -*- encoding: utf-8 -*-\n\n# This file is part of OpenMVG (Open Multiple View Geometry) C++ library.\n\n# Python implementation of the bash script written by Romuald Perrot\n# Created by @vins31\n# Modified by Pierre Moulon\n#\n# this script is for easy use of OpenMVG", "#\n# this script is for easy use of OpenMVG\n#\n# usage : python openmvg.py image_dir output_dir\n#\n# image_dir is the input directory where images are located\n# output_dir is where the project must be saved\n#\n# if output_dir is not present script will create it\n# borrowd from openMVG", "# if output_dir is not present script will create it\n# borrowd from openMVG\n\n\n\n\nimport os\nimport subprocess\nimport sys\n\nif len(sys.argv) < 6:\n    print (\"Please input sufficient paras!\")\n    sys.exit(1)", "import sys\n\nif len(sys.argv) < 6:\n    print (\"Please input sufficient paras!\")\n    sys.exit(1)\n\ninput_dir = sys.argv[1]\noutput_dir = sys.argv[2]\nfocal_length = sys.argv[3]\nnThreads = sys.argv[4]", "focal_length = sys.argv[3]\nnThreads = sys.argv[4]\ndir_root_mvg = sys.argv[5]\n\n# Indicate the openMVG binary directory\nOPENMVG_SFM_BIN = f\"{dir_root_mvg}/Linux-x86_64-RELEASE\"\n\n# Indicate the openMVG camera sensor width directory\nCAMERA_SENSOR_WIDTH_DIRECTORY = f\"{dir_root_mvg}/../openMVG/src/software/SfM\" + \"/../../openMVG/exif/sensor_width_database\"\n", "CAMERA_SENSOR_WIDTH_DIRECTORY = f\"{dir_root_mvg}/../openMVG/src/software/SfM\" + \"/../../openMVG/exif/sensor_width_database\"\n\nmatches_dir = os.path.join(output_dir, \"matches\")\nreconstruction_dir = os.path.join(output_dir, \"reconstruction_sequential\")\ncamera_file_params = os.path.join(CAMERA_SENSOR_WIDTH_DIRECTORY, \"sensor_width_camera_database.txt\")\n\nprint (\"Using input dir  : \", input_dir)\nprint (\"      output_dir : \", output_dir)\nprint (\"    focal_length : \", focal_length)\n", "print (\"    focal_length : \", focal_length)\n\n# Create the ouput/matches folder if not present\nif not os.path.exists(output_dir):\n  os.mkdir(output_dir)\nif not os.path.exists(matches_dir):\n  os.mkdir(matches_dir)\n\nprint (\"1. Intrinsics analysis\")\npIntrisics = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_SfMInit_ImageListing\"),  \"-i\", input_dir, \"-o\", matches_dir, \"-d\", camera_file_params, \"-f\", focal_length] )", "print (\"1. Intrinsics analysis\")\npIntrisics = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_SfMInit_ImageListing\"),  \"-i\", input_dir, \"-o\", matches_dir, \"-d\", camera_file_params, \"-f\", focal_length] )\npIntrisics.wait()\n\nprint (\"2. Compute features\")\npFeatures = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_ComputeFeatures\"), \"-p\", \"HIGH\",  \"-f\", \"1\", \"-i\", matches_dir+\"/sfm_data.json\", \"-o\", matches_dir, \"-m\", \"SIFT\",\"-n\",nThreads] )\npFeatures.wait()\n\nprint (\"3. Compute matches\")\npMatches = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_ComputeMatches\"),  \"-f\", \"1\", \"-i\", matches_dir+\"/sfm_data.json\", \"-o\", matches_dir] )", "print (\"3. Compute matches\")\npMatches = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_ComputeMatches\"),  \"-f\", \"1\", \"-i\", matches_dir+\"/sfm_data.json\", \"-o\", matches_dir] )\npMatches.wait()\n\n# Create the reconstruction if not present\nif not os.path.exists(reconstruction_dir):\n    os.mkdir(reconstruction_dir)\n\nprint (\"4. Do Sequential/Incremental reconstruction\")\npRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_IncrementalSfM\"),  \"-i\", matches_dir+\"/sfm_data.json\", \"-m\", matches_dir, \"-o\", reconstruction_dir] )", "print (\"4. Do Sequential/Incremental reconstruction\")\npRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_IncrementalSfM\"),  \"-i\", matches_dir+\"/sfm_data.json\", \"-m\", matches_dir, \"-o\", reconstruction_dir] )\npRecons.wait()\n\nprint (\"5. Colorize Structure\")\npRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_ComputeSfM_DataColor\"),  \"-i\", reconstruction_dir+\"/sfm_data.bin\", \"-o\", os.path.join(reconstruction_dir,\"colorized.ply\")] )\npRecons.wait()\n\n# optional, compute final valid structure from the known camera poses\nprint (\"6. Structure from Known Poses (robust triangulation)\")", "# optional, compute final valid structure from the known camera poses\nprint (\"6. Structure from Known Poses (robust triangulation)\")\npRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_ComputeStructureFromKnownPoses\"),  \"-i\", reconstruction_dir+\"/sfm_data.bin\", \"-m\", matches_dir, \"-f\", os.path.join(matches_dir, \"matches.f.bin\"), \"-o\", os.path.join(reconstruction_dir,\"robust.bin\")] )\npRecons.wait()\n\npRecons = subprocess.Popen( [os.path.join(OPENMVG_SFM_BIN, \"openMVG_main_ComputeSfM_DataColor\"),  \"-i\", reconstruction_dir+\"/robust.bin\", \"-o\", os.path.join(reconstruction_dir,\"robust_colorized.ply\")] )\npRecons.wait()\n\n", ""]}
{"filename": "preprocess/scannet_data.py", "chunked_list": ["import os, sys\nimport utils.utils_io as IOUtils\nimport utils.utils_geometry as GeometryUtils\nimport utils.utils_image as Imageutils\nimport shutil, glob, os\nimport cv2\nimport numpy as np\nimport logging, glob\n\nimport open3d as o3d", "\nimport open3d as o3d\nfrom datetime import datetime\n\nclass ScannetData:\n    def __init__(self, dir_scan, height, width, use_normal = False, \n                        cam_sphere_radius=-1,\n                        dir_extrinsics = None, \n                        path_intrin_color = None,\n                        path_intrin_depth = None,\n                        path_cloud_sfm = None):\n        '''\n        ScanNet Dataset:\n            default pose: camera to world\n            \n        Args:\n            use_normal: if true, use scanned depth to calculate world normals\n                            and upsample(resize) depth map to image shape\n        '''\n        self.dir_scan = dir_scan\n        self.use_normal = use_normal\n        self.height = height\n        self.width = width\n        self.cam_sphere_radius = cam_sphere_radius\n        logging.info(f'cam_sphere_radius: {self.cam_sphere_radius}. Image height: {self.height}. width: {self.width}')\n\n        # intrinsics_rgb = '''1169.621094 0.000000 646.295044 0.000000\n        #     0.000000 1167.105103 489.927032 0.000000\n        #     0.000000 0.000000 1.000000 0.000000\n        #     0.000000 0.000000 0.000000 1.000000'''\n        # intrinsics_depth = '''577.590698 0.000000 318.905426 0.000000\n        #     0.000000 578.729797 242.683609 0.000000\n        #     0.000000 0.000000 1.000000 0.000000\n        #     0.000000 0.000000 0.000000 1.000000'''\n        self.intrinsics = GeometryUtils.read_cam_matrix(path_intrin_color)\n        self.intrinsics_depth = GeometryUtils.read_cam_matrix(path_intrin_depth)\n        \n        self.dir_depthmap = os.path.join(self.dir_scan, 'depth')\n        self.dir_image = os.path.join(self.dir_scan, 'image')\n        if dir_extrinsics is not None: \n            self.poses_w2c = GeometryUtils.read_poses(dir_extrinsics)   # default pose: camera to world\n            self.poses_c2w = np.linalg.inv(self.poses_w2c)\n            # print( self.poses_w2c @ self.poses_c2w )\n        else:\n            self.dir_pose = os.path.join(self.dir_scan, 'pose')\n            self.poses_c2w = GeometryUtils.read_poses(self.dir_pose)   # default pose: camera to world\n            self.poses_w2c = GeometryUtils.get_poses_inverse(self.poses_c2w)  # extrinsics: world to camera\n        \n        self.dir_normal = os.path.join(self.dir_scan, 'normal')\n        self.path_cloud_sfm = path_cloud_sfm if path_cloud_sfm is not None else None\n\n    @staticmethod\n    def select_data_by_range(dir_scan, dir_scan_select, start_id, end_id, interval, b_crop_images, cropped_size = (1248, 936)):  \n        '''\n        Args:\n            b_crop_images: crop images to cropped size if true, and resize cropped images to (640,480)\n            cropped_size: (640,480)*1.95\n        '''    \n        IOUtils.ensure_dir_existence(dir_scan_select)\n        for i in ['image', 'depth', 'pose']:\n            IOUtils.ensure_dir_existence(f\"{dir_scan_select}/{i}/\")\n        \n        crop_height_half, crop_width_half = 0, 0\n        for idx in range(start_id, end_id, interval):\n            # rgb\n            path_src = f\"{dir_scan}/rgb/{idx}.jpg\"\n            img = cv2.imread(path_src, cv2.IMREAD_UNCHANGED)\n            height, width, _ = img.shape\n            if b_crop_images:\n                W_target, H_target = cropped_size\n                # if width == 640:\n                #     raise NotImplementedError\n                # crop\n                crop_width_half = (width-W_target)//2\n                crop_height_half = (height-H_target) //2\n                assert (width-W_target)%2 ==0 and (height- H_target) %2 == 0\n                # resize\n                img_crop = img[crop_height_half:height-crop_height_half, crop_width_half:width-crop_width_half, :]\n                assert img_crop.shape[0] == cropped_size[1]\n                img = cv2.resize(img_crop, (640, 480), interpolation=cv2.INTER_LINEAR)\n            path_target = f\"{dir_scan_select}/image/{idx:04d}.png\"\n            cv2.imwrite(path_target, img)\n            \n            # pose\n            path_src = f\"{dir_scan}/pose/{idx}.txt\"\n            path_target = f\"{dir_scan_select}/pose/{idx:04d}.txt\"\n            shutil.copyfile(path_src, path_target)\n        \n            # depth map\n            path_src = f\"{dir_scan}/depth/{idx}.png\"\n            path_target = f\"{dir_scan_select}/depth/{idx:04d}.png\"\n            shutil.copyfile(path_src, path_target)\n            \n        # GT mesh\n        path_gt_mesh = IOUtils.find_target_file(dir_scan, '_vh_clean_2.ply')\n        assert path_gt_mesh\n        _, _stem, _ext = IOUtils.get_path_components(path_gt_mesh)\n        path_target = f\"{dir_scan_select}/{_stem}{_ext}\"\n        shutil.copyfile(path_gt_mesh, path_target)\n            \n        return crop_height_half, crop_width_half\n    \n    def load_and_merge_depth_maps(self):\n        self.depthmaps = self.read_depthmaps(self.dir_depthmap)\n        self.num_images = len(glob.glob(f\"{self.dir_image}/**.png\"))\n \n        if self.depthmaps.shape[0]> 200:\n            logging.info(\"Sample 200 depth maps to get merged points...\")\n            idx_imgs = np.random.randint(low=0, high=self.depthmaps.shape[0], size=200)\n            depthmaps_fuse = self.depthmaps[idx_imgs]\n            points = GeometryUtils.fuse_depthmaps(depthmaps_fuse, self.intrinsics_depth, self.poses_w2c[idx_imgs])\n\n            self.arr_imgs = (self.read_rgbs(self.dir_image, (640,480))[idx_imgs])\n            arr_imgs = self.arr_imgs.reshape(-1,3)\n        else:\n            points = GeometryUtils.fuse_depthmaps(self.depthmaps, self.intrinsics_depth, self.poses_w2c)\n            self.arr_imgs = self.read_rgbs(self.dir_image, (640,480))\n            arr_imgs = self.arr_imgs.reshape(-1,3)\n        idx_pts = np.random.randint(low=0, high=points.shape[0], size=int(1e6))\n        self.pts_sample = points[idx_pts]\n        self.colors_sample = arr_imgs[idx_pts]\n\n    def read_one_img(self, path):\n        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n        return img\n\n    def read_depthmaps(self, dir):\n        vec_path = sorted(glob.glob(f\"{dir}/**.png\"))\n        depth_maps = []\n        for i in range(len(vec_path)):\n            img = self.read_one_img(vec_path[i]).astype(np.int32) / 1000  # unit: m\n            depth_maps.append(img)\n        return np.array(depth_maps)\n    \n    def read_normals(self, dir):\n        vec_path = sorted(glob.glob(f\"{dir}/**.png\"))\n        normals = []\n        for i in range(len(vec_path)):\n            img = self.read_one_img(vec_path[i])\n            normals.append(img)\n        return np.array(normals)\n\n    def read_rgbs(self, dir, target_img_size = None):\n        vec_path = sorted(glob.glob(f\"{dir}/**.png\"))\n        rgbs = []\n        for i in range(len(vec_path)):\n            img = self.read_one_img(vec_path[i])\n            if target_img_size != None:\n                img = cv2.resize(img, target_img_size, interpolation=cv2.INTER_LINEAR)\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            rgbs.append(img)\n        return np.array(rgbs)\n    \n    def read_poses(self, dir):\n        vec_path = sorted(glob.glob(f\"{dir}/**.txt\"))\n        poses = []\n        for i in range(len(vec_path)):\n            img = GeometryUtils.read_cam_matrix(vec_path[i])\n            poses.append(img)\n        return np.array(poses)\n\n    def get_projection_matrix(self, intrin, poses, trans_n2w):\n        '''\n        Args:\n            poses: world to camera\n        '''\n        num_poses = poses.shape[0]\n        \n        projs = []\n        poses_norm = []\n        dir_pose_norm = self.dir_scan + \"/extrin_norm\"\n        IOUtils.ensure_dir_existence(dir_pose_norm)\n        for i in range(num_poses):\n            # pose_norm_i = poses[i] @ trans_n2w\n\n            # Method 2\n            pose = poses[i]\n            rot = pose[:3,:3]\n            trans = pose[:3,3]\n\n            cam_origin_world = - np.linalg.inv(rot) @ trans.reshape(3,1)\n            cam_origin_world_homo = np.concatenate([cam_origin_world,[[1]]], axis=0)\n            cam_origin_norm = np.linalg.inv(trans_n2w) @ cam_origin_world_homo\n            trans_norm = -rot @ cam_origin_norm[:3]\n\n            pose[:3,3] = np.squeeze(trans_norm)\n            poses_norm.append(pose)\n            proj_norm = intrin @ pose\n            projs.append(proj_norm)\n            \n            np.savetxt(f'{dir_pose_norm}/{i:04d}.txt', pose, fmt='%f') # world to camera\n            np.savetxt(f'{dir_pose_norm}/{i:04d}_inv.txt', GeometryUtils.get_pose_inv(pose) , fmt='%f') # inv: camera to world\n        return np.array(projs), np.array(poses_norm)\n    \n    def calculate_normals(self):\n        # visualize normal\n        IOUtils.ensure_dir_existence(self.dir_normal)\n        for i in range(self.num_images):\n            logging.info(f\"Caluclate normal of image: {i}/{self.num_images}\")\n            pts_i, normal_map_i = GeometryUtils.calculate_normalmap_from_depthmap(self.depthmaps[i], self.intrinsics_depth, self.poses_w2c[i])\n\n            if self.height != 480:\n                logging.info(f\"{i} Upsample normal map to size: (1296, 968).\")\n                normal_map_i = cv2.resize(normal_map_i, (1296, 968), interpolation=cv2.INTER_LINEAR)\n            np.savez(f\"{self.dir_normal}/{i:04d}.npz\", normal=normal_map_i)\n            cv2.imwrite(f\"{self.dir_normal}/{i:04d}.png\", normal_map_i*255)\n                \n    def generate_neus_data(self, radius_normalize_sphere=1.0):\n        if self.path_cloud_sfm:\n            msg = input('Check bounding box of openMVS point cloud (Manually remove floating outliers)...[y/n]')\n            if msg != 'y':\n                exit()\n            cloud_clean = GeometryUtils.read_point_cloud(self.path_cloud_sfm)\n        else:\n            self.load_and_merge_depth_maps()\n            path_point_cloud_scan = f'{self.dir_scan}/point_cloud_scan.ply'\n            GeometryUtils.save_points(path_point_cloud_scan,  self.pts_sample, self.colors_sample)\n            msg = input('Check bounding box of merged point cloud (Manually remove floating outliers)...[y/n]')\n            if msg != 'y':\n                exit()\n\n            if self.use_normal:\n                t1 = datetime.now()\n                self.calculate_normals()\n                logging.info(f\"Calculate normal: {(datetime.now()-t1).total_seconds():.0f} seconds\")\n                    \n            cloud_clean = GeometryUtils.read_point_cloud(path_point_cloud_scan)\n            \n        trans_n2w = GeometryUtils.get_norm_matrix_from_point_cloud(cloud_clean, radius_normalize_sphere=radius_normalize_sphere)\n        projs, poses_norm = self.get_projection_matrix(self.intrinsics, self.poses_w2c, trans_n2w)\n        path_trans_n2w = f'{self.dir_scan}/trans_n2w.txt'\n        np.savetxt(path_trans_n2w, trans_n2w, fmt = '%.04f')\n\n        cloud_clean_trans = cloud_clean.transform(np.linalg.inv(trans_n2w))\n        o3d.io.write_point_cloud(f'{self.dir_scan}/point_cloud_scan_norm.ply', cloud_clean_trans)\n\n        pts_cam_norm = GeometryUtils.get_camera_origins(poses_norm)\n        GeometryUtils.save_points(f'{self.dir_scan}/cam_norm.ply', pts_cam_norm)\n        \n        pts_cam = (trans_n2w[None, :3,:3] @ pts_cam_norm[:, :, None]).squeeze()  + trans_n2w[None, :3, 3]\n        GeometryUtils.save_points(f'{self.dir_scan}/cam_origin.ply', pts_cam)\n\n        scale_mat = np.identity(4)\n        num_cams = projs.shape[0]\n        cams_neus = {}\n        for i in range(num_cams):\n            cams_neus[f\"scale_mat_{i}\"] = scale_mat\n            cams_neus[f'world_mat_{i}'] = projs[i]\n        \n        np.savez(f'{self.dir_scan}/cameras_sphere.npz', **cams_neus)\n        \n        # transform gt mesh\n        path_gt_mesh = IOUtils.find_target_file(self.dir_scan, '_vh_clean_2.ply')\n        if path_gt_mesh is None:\n            return\n        \n        path_save = IOUtils.add_file_name_suffix(path_gt_mesh, \"_trans\")\n        trans = np.linalg.inv(np.loadtxt(path_trans_n2w))\n        GeometryUtils.transform_mesh(path_gt_mesh, trans, path_save) ", "\nif __name__ == \"__main__\":\n    print(\"Nothing\")\n    "]}
{"filename": "preprocess/sfm_mvs.py", "chunked_list": ["import sys, os\nimport argparse\n\nDIR_FILE = os.path.abspath(os.path.dirname(__file__))\nsys.path.append(os.path.join(DIR_FILE, '..'))\n\nimport utils.utils_io as IOUtils\nimport os, sys, glob, subprocess\nfrom pathlib import Path\nimport numpy as np", "from pathlib import Path\nimport numpy as np\nimport cv2\nimport re\n\n# SfM and MVS paras\nnNumThreads = 6 \nnNumViews = 5\nnMaxResolution = 7000\nfDepthDiffThreshold = 0.005", "nMaxResolution = 7000\nfDepthDiffThreshold = 0.005\nfNormalDiffThreshold = 25\nbRemoveDepthMaps = True\nverbosity = 2\nnRamdomIters = 4\nnEstiIters = 2\n\nfrom confs.path import DIR_MVS_BUILD, DIR_MVG_BUILD\n", "from confs.path import DIR_MVS_BUILD, DIR_MVG_BUILD\n\n\n\ndef perform_sfm(dir_images, dir_output, nImageWidth):\n    IOUtils.ensure_dir_existence(dir_output)\n    \n    dir_undistorted_images = dir_output + \"/undistorted_images\" \n    IOUtils.changeWorkingDir(dir_output) \n\n    fFocalLength_pixel = 1.2 * nImageWidth\n    IOUtils.INFO_MSG(\"Use sequential pipeline\")\n    args_sfm = [\"python3\",  DIR_FILE + \"/sfm_pipeline.py\", \\\n                            dir_images, dir_output, str(fFocalLength_pixel), str(nNumThreads), DIR_MVG_BUILD] \n    IOUtils.run_subprocess(args_sfm)\n\n\n    dir_input_sfm2mvs = dir_output + \"/reconstruction_sequential/sfm_data.bin\"\n    args_sfm2mvs = [DIR_MVG_BUILD +\"/Linux-x86_64-RELEASE/openMVG_main_openMVG2openMVS\", \n                        \"-i\", dir_input_sfm2mvs, \n                        \"-o\", dir_output + \"/scene.mvs\",\n                        \"-d\", dir_undistorted_images]\n    IOUtils.run_subprocess(args_sfm2mvs)", "\ndef removeFiles(path_files, file_ext):\n    \"\"\"\n    Remove files in \"path_files\" with extension \"file_ext\"\n    \"\"\"\n    paths = Path(path_files).glob(\"**/*\" + file_ext)\n    for path in paths:\n        path_str = str(path)  # because path is object not string\n\n        if os.path.exists(path_str):\t# Remove file\n            print(f\"Remove file {path_str}.\")\n            os.remove(path_str)", "        # else:\n        #     print(f\"File {path_str} doesn't exist.\"\n\ndef perform_mvs(dir_output, nResolutionLevel, bRemoveDepthMaps=True):\n    os.chdir(dir_output)\n    if bRemoveDepthMaps:\n        removeFiles(os.getcwd(), \".dmap\")\n\n    DENSE_RECONSTRUCTION = DIR_MVS_BUILD + \"/bin/DensifyPointCloud\"\n    args_dense_reconstruction = [DENSE_RECONSTRUCTION, \"scene.mvs\",  \n                                    \"--resolution-level\", str(nResolutionLevel), \n                                    \"--max-resolution\", str(nMaxResolution),\n                                    \"--depth-diff-threshold\", str(fDepthDiffThreshold),\n                                    \"--normal-diff-threshold\", str(fNormalDiffThreshold),\n                                    \"--random-iters\", str(nRamdomIters),\n                                    \"--estimation-iters\",str(nEstiIters),\n                                    \"--verbosity\", str(verbosity),\n                                    \"--number-views\", str(nNumViews)\n                                    ]\n    # reconstruction\n    IOUtils.run_subprocess(args_dense_reconstruction)\n    \n    # remove depth maps\n    if bRemoveDepthMaps:\n        removeFiles(os.getcwd(), \".dmap\")", "\ndef extract_intrinsics_from_KRC(path_KRC, path_intrin, path_imgs_cal):\n    fKRC = open(path_KRC, 'r').readlines()\n    lines_K = fKRC[3:6]\n    intrin = np.identity(4)\n    idx_row = 0\n    \n    # get calibrated images\n    stems_img_cal = []\n    count_lines = 0\n    for line in fKRC:\n        line = re.split('/|\\n', line)\n        # print(line)   \n        if count_lines%13==1:\n            stems_img_cal.append(Path(line[-2]).stem)    \n        count_lines+=1\n    \n    IOUtils.write_list_to_txt(path_imgs_cal, stems_img_cal)\n    # get camera instrisics\n    for line in lines_K:\n        line = re.split('[ \\] \\[ , ; \\n]', line)\n        idx_col = 0\n        for elem in line:\n            if len(elem) > 0:\n                intrin[idx_row, idx_col] = float(elem)\n                \n                idx_col +=1\n            \n        idx_row +=1\n    np.savetxt(path_intrin, intrin, fmt='%f')         \n    return intrin, stems_img_cal", "\ndef export_cameras(dir_mvs_output):\n    BIN_EXPORT_DATA = DIR_MVS_BUILD + \"/bin/ExportData\"\n    IOUtils.changeWorkingDir(dir_mvs_output)\n\n    # export cameras\n    dir_export_cameras = dir_mvs_output + \"/cameras\"\n    IOUtils.ensure_dir_existence(dir_export_cameras)\n    pExportData = subprocess.Popen([BIN_EXPORT_DATA, \"scene.mvs\", \"--export-path\", dir_export_cameras])\n    pExportData.wait()", "\ndef select_extrin_to_poses(dir_source, dir_target, lis_stem_sample,\n                        target_img_size = None, \n                        ext_source = '.txt', ext_target = '.txt'):\n    '''Convert image type in directory from ext_imgs to ext_target\n    '''\n    IOUtils.ensure_dir_existence(dir_target)\n    stems = []\n    for i in range(len(lis_stem_sample)):\n        # id_img = int(stem)\n        stem_curr = lis_stem_sample[i]        \n        path_source = f'{dir_source}/{stem_curr}{ext_source}'       \n        path_target = f'{dir_target}/{stem_curr}{ext_target}'       \n\n        extrin = np.loadtxt(path_source)\n        pose = np.linalg.inv(extrin)\n        np.savetxt(path_target, pose)", "\ndef select_images(dir_source, dir_target, lis_stem_sample,\n                        target_img_size = None, \n                        ext_source = '.png', ext_target = '.png'):\n    '''Convert image type in directory from ext_imgs to ext_target\n    '''\n    IOUtils.ensure_dir_existence(dir_target)\n    stems = []\n    for i in range(len(lis_stem_sample)):\n        # id_img = int(stem)\n        stem_curr = lis_stem_sample[i]        \n        path_source = f'{dir_source}/{stem_curr}{ext_source}'       \n        path_target = f'{dir_target}/{stem_curr}{ext_target}'       \n        if ext_source[-4:] == ext_target and (target_img_size is not None):\n            IOUtils.copy_file(path_source, path_target)\n        else:\n            img = cv2.imread(path_source, cv2.IMREAD_UNCHANGED)\n            if target_img_size is not None:\n                img = cv2.resize(img, target_img_size)\n            cv2.imwrite(path_target, img)", "\ndef select_depths(dir_source, dir_target, lis_stem_sample, size_image, \n                        target_img_size = None, \n                        ext_source = '.npy', ext_target = '.npy',\n                        stem_prefix_src = 'depth'):\n    '''Convert image type in directory from ext_imgs to ext_target\n    '''\n    IOUtils.ensure_dir_existence(dir_target)\n    \n    path_depth0 = f'{dir_source}/{stem_prefix_src}{lis_stem_sample[0]}{ext_source}'  \n    depth0 = np.load(path_depth0).reshape(size_image[1], size_image[0])\n    \n    for i in range(len(lis_stem_sample)):\n        # id_img = int(stem)\n        stem_curr = lis_stem_sample[i] \n        path_source = f'{dir_source}/{stem_curr}{ext_source}'       \n        if stem_prefix_src is not None:       \n            path_source = f'{dir_source}/{stem_prefix_src}{stem_curr}{ext_source}'       \n        path_target = f'{dir_target}/{stem_curr}{ext_target}'\n        if not IOUtils.checkExistence(path_source):\n            print(f'Depth not existed. Create one with all zeros... {path_target}')\n            depth_tmp = np.zeros_like(depth0)\n            np.save(path_target, depth_tmp)  \n            continue\n        \n        if ext_source[-4:] == ext_target and (target_img_size is None):\n            depth_curr = np.load(path_source).reshape(size_image[1], size_image[0])\n            np.save(path_target, depth_curr)\n            # IOUtils.copy_file(path_source, path_target)\n        else:\n            raise NotImplementedError", "     \ndef prepare_neus_data(dir_sfm_output, dir_neus, imgs_cal_stem, args):\n    IOUtils.ensure_dir_existence(dir_neus)\n    \n    # images\n    dir_src = f'{dir_sfm_output}/undistorted_images'\n    dir_target = f'{dir_sfm_output}/images_calibrated'\n    select_images(dir_src, dir_target, imgs_cal_stem)\n\n    dir_src = f'{dir_sfm_output}'\n    dir_target = f'{dir_sfm_output}/depth_calibrated'\n    assert args.reso_level == 1\n    select_depths(dir_src, dir_target, imgs_cal_stem, (args.image_width, args.image_height))\n    \n    # cameras\n    dir_src = f'{dir_sfm_output}/cameras/extrinsics'\n    dir_target = f'{dir_neus}/pose'\n    select_extrin_to_poses(dir_src, dir_target, imgs_cal_stem)\n\n    # intrinsics\n    path_src = f'{dir_sfm_output}/cameras/intrinsics.txt'\n    path_target = f'{dir_sfm_output}/intrinsics.txt'\n    IOUtils.copy_file(path_src, path_target)\n    \n    # point cloud\n    path_src = f'{dir_sfm_output}/scene_dense.ply'\n    scene_name = path_src.split('/')[-3]\n    path_target = f'{dir_neus}/point_cloud_openMVS.ply'\n    IOUtils.copy_file(path_src, path_target)\n    \n    # neighbors\n    path_src = f'{dir_sfm_output}/neighbors.txt'\n    path_target = f'{dir_neus}/neighbors.txt'\n    IOUtils.copy_file(path_src, path_target)", "    \n    \ndef perform_sfm_mvs_pipeline(dir_mvs, args):\n    dir_images = os.path.join(dir_mvs, 'images')\n    dir_output = dir_mvs\n    dir_neus = f'{dir_output}/..'\n    \n    # reconstruction\n    perform_sfm(dir_images, dir_output, args.image_width)\n    perform_mvs(dir_output, nResolutionLevel = args.reso_level)\n    export_cameras(dir_output)\n    \n    # Prepare neus data: image, intrin, extrins\n    dir_KRC = f'{dir_output}/cameras/KRC.txt'\n    path_intrin = f'{dir_output}/cameras/intrinsics.txt'\n    path_imgs_cal = f'{dir_output}/cameras/stems_calibrated_images.txt'\n    intrin, stems_img_cal = extract_intrinsics_from_KRC(dir_KRC, path_intrin, path_imgs_cal)\n    prepare_neus_data(dir_output, dir_neus, stems_img_cal, args)", " \ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dir_mvs', type=str, default='/home/ethan/Desktop/test_sfm/tmp_sfm_mvs' )\n    parser.add_argument('--image_width', type=int, default = 1024)\n    parser.add_argument('--image_height', type=int, default = 768)\n    parser.add_argument('--reso_level', type=int, default = 1)\n    \n    args = parser.parse_args()\n    return args", "\nif __name__ == \"__main__\":\n    \n    args = parse_args()    \n    perform_sfm_mvs_pipeline(args.dir_mvs, args)\n"]}
{"filename": "models/embedder.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nimport numpy as np\n\n\"\"\" Positional encoding embedding. Code was taken from https://github.com/bmild/nerf. \"\"\"\n\nclass Embedder:\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n        self.create_embedding_fn()\n\n    def create_embedding_fn(self):\n        embed_fns = []\n        d = self.kwargs['input_dims']\n        out_dim = 0\n        if self.kwargs['include_input']:\n            embed_fns.append(lambda x: x)\n            out_dim += d\n\n        max_freq = self.kwargs['max_freq_log2']\n        N_freqs = self.kwargs['num_freqs']\n\n        if self.kwargs['log_sampling']:\n            freq_bands = 2. ** torch.linspace(0., max_freq, N_freqs)\n        else:\n            freq_bands = torch.linspace(2.**0., 2.**max_freq, N_freqs)\n\n        for freq in freq_bands:\n            for p_fn in self.kwargs['periodic_fns']:\n                if self.kwargs['normalize']:\n                    embed_fns.append(lambda x, p_fn=p_fn,\n                                     freq=freq: p_fn(x * freq) / freq)\n                else:\n                    embed_fns.append(lambda x, p_fn=p_fn,\n                                            freq=freq: p_fn(x * freq))\n                out_dim += d\n\n        self.embed_fns = embed_fns\n        self.out_dim = out_dim\n\n    def embed(self, inputs):\n        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)", "\ndef get_embedder(multires, normalize=False, input_dims=3):\n    embed_kwargs = {\n        'include_input': True,\n        'input_dims': input_dims,\n        'max_freq_log2': multires-1,\n        'num_freqs': multires,\n        'normalize': normalize,\n        'log_sampling': True,\n        'periodic_fns': [torch.sin, torch.cos],\n    }\n\n    embedder_obj = Embedder(**embed_kwargs)\n    def embed(x, eo=embedder_obj): return eo.embed(x)\n    return embed, embedder_obj.out_dim", "\n\ndef positional_encoding(input,L): # [B,...,N]\n    shape = input.shape\n    freq = 2**torch.arange(L,dtype=torch.float32)*np.pi # [L]\n    spectrum = input[...,None]*freq # [B,...,N,L]\n    sin,cos = spectrum.sin(),spectrum.cos() # [B,...,N,L]\n    input_enc = torch.stack([sin,cos],dim=-2) # [B,...,N,2,L]\n    input_enc = input_enc.view(*shape[:-1],-1) # [B,...,2NL]\n    return input_enc", "\n\ndef positional_encoding_c2f(input,L, emb_c2f, alpha_ratio = -1): # [B,...,N]\n    input_enc = positional_encoding(input,L=L) # [B,...,2NL]\n    # coarse-to-fine: smoothly mask positional encoding for BARF\n    weight = None\n    if emb_c2f is not None:\n        # set weights for different frequency bands\n        start,end = emb_c2f\n        alpha = (alpha_ratio-start)/(end-start)*L\n        k = torch.arange(L,dtype=torch.float32) + 1\n        weight = (1-(alpha-k).clamp_(min=0,max=1).mul_(np.pi).cos_())/2\n        # apply weights\n        shape = input_enc.shape\n        input_enc = (input_enc.view(-1,L)*weight).view(*shape)\n    # input_enc = torch.zeros_like(input_enc)\n    input_enc = torch.cat([input,input_enc],dim=-1) # [B,...,6L+3]\n    return input_enc, weight"]}
{"filename": "models/loss.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\nfrom itertools import combinations\n\nimport utils.utils_training as TrainingUtils\n\nMIN_PIXELS_PLANE = 20", "\nMIN_PIXELS_PLANE = 20\n\ndef get_normal_consistency_loss(normals, mask_curr_plane, error_mode = 'angle_error'):\n    '''Return normal loss of pixels on the same plane\n\n    Return:\n        normal_consistency_loss: float, on each pixels\n        num_pixels_curr_plane: int\n        normal_mean_curr, 3*1\n    '''\n    num_pixels_curr_plane = mask_curr_plane.sum()\n    if num_pixels_curr_plane < MIN_PIXELS_PLANE:\n        return 0.0,  num_pixels_curr_plane, torch.zeros(3)\n\n    normals_fine_curr_plane = normals * mask_curr_plane\n    normal_mean_curr = normals_fine_curr_plane.sum(dim=0) / num_pixels_curr_plane\n\n    if error_mode == 'angle_error':\n        inner = (normals * normal_mean_curr).sum(dim=-1,keepdim=True)\n        norm_all =  torch.linalg.norm(normals, dim=-1, ord=2,keepdim=True)\n        norm_mean_curr = torch.linalg.norm(normal_mean_curr, dim=-1, ord=2,keepdim=True)\n        angles = torch.arccos(inner/((norm_all*norm_mean_curr) + 1e-6)) #.clip(-np.pi, np.pi)\n        angles = angles*mask_curr_plane\n        normal_consistency_loss = F.l1_loss(angles, torch.zeros_like(angles), reduction='sum')\n    \n    return normal_consistency_loss, num_pixels_curr_plane, normal_mean_curr", "\ndef get_plane_offset_loss(pts, ave_normal, mask_curr_plane, mask_subplanes):\n    '''\n    Args:\n        pts: pts in world coordinates\n        normals: normals of pts in world coordinates\n        mask_plane: mask of pts which belong to the same plane\n    '''\n    mask_subplanes_curr = copy.deepcopy(mask_subplanes)\n    mask_subplanes_curr[mask_curr_plane == False] = 0 # only keep subplanes of current plane\n    \n    loss_offset_subplanes = []\n    num_subplanes = int(mask_subplanes_curr.max().item())\n    if num_subplanes < 1:\n        return 0, 0\n    \n    num_pixels_valid_subplanes = 0\n    loss_offset_subplanes = torch.zeros(num_subplanes)\n    for i in range(num_subplanes):\n        curr_subplane = (mask_subplanes_curr == (i+1))\n        num_pixels = curr_subplane.sum()\n        if num_pixels < MIN_PIXELS_PLANE:\n            continue\n        \n        offsets = (pts*ave_normal).sum(dim=-1,keepdim=True)\n        ave_offset = ((offsets * curr_subplane).sum() / num_pixels) #.detach()  # detach?\n\n        diff_offsset = (offsets-ave_offset)*curr_subplane\n        loss_tmp = F.mse_loss(diff_offsset, torch.zeros_like(diff_offsset), reduction='sum') #/ num_pixels\n\n        loss_offset_subplanes[i] = loss_tmp\n        num_pixels_valid_subplanes += num_pixels\n    \n    return loss_offset_subplanes.sum(), num_pixels_valid_subplanes", "\ndef get_manhattan_normal_loss(normal_planes):\n    '''The major planes should be vertical to each other\n    '''\n    normal_planes = torch.stack(normal_planes, dim=0)\n    num_planes = len(normal_planes)\n    assert num_planes < 4\n    if num_planes < 2:\n        return 0\n\n    all_perms = np.array( list(combinations(np.arange(num_planes),2)) ).transpose().astype(int) # 2*N\n    normal1, normal2 = normal_planes[all_perms[0]], normal_planes[all_perms[1]]\n    inner = (normal1 * normal2).sum(-1)\n    manhattan_normal_loss = F.l1_loss(inner, torch.zeros_like(inner), reduction='mean')\n    return manhattan_normal_loss", "\nclass NeuSLoss(nn.Module):\n    def __init__(self, conf):\n        super().__init__()\n        self.color_weight = conf['color_weight']\n\n        self.igr_weight = conf['igr_weight']\n        self.smooth_weight = conf['smooth_weight']\n        self.mask_weight = conf['mask_weight']\n\n        self.depth_weight = conf['depth_weight']\n        self.normal_weight = conf['normal_weight']\n\n        self.normal_consistency_weight = conf['normal_consistency_weight']\n        self.plane_offset_weight = conf['plane_offset_weight']\n        self.manhattan_constrain_weight = conf['manhattan_constrain_weight']\n        self.plane_loss_milestone = conf['plane_loss_milestone']\n\n        self.warm_up_start = conf['warm_up_start']\n        self.warm_up_end = conf['warm_up_end']\n        \n        self.iter_step = 0\n        self.iter_end = -1\n\n    def get_warm_up_ratio(self):\n        if self.warm_up_end == 0.0:\n            return 1.0\n        elif self.iter_step < self.warm_up_start:\n            return 0.0\n        else:\n            return np.min([1.0, (self.iter_step - self.warm_up_start) / (self.warm_up_end - self.warm_up_start)])\n\n    def forward(self, input_model, render_out, sdf_network_fine, patchmatch_out = None):\n\n        true_rgb = input_model['true_rgb']\n\n        mask, rays_o, rays_d, near, far = input_model['mask'], input_model['rays_o'], input_model['rays_d'],  \\\n                                                    input_model['near'], input_model['far']\n        mask_sum = mask.sum() + 1e-5\n        batch_size = len(rays_o)\n\n        color_fine = render_out['color_fine']\n        variance = render_out['variance']\n        cdf_fine = render_out['cdf_fine']\n        gradient_error_fine = render_out['gradient_error_fine']\n        weight_max = render_out['weight_max']\n        weight_sum = render_out['weight_sum']\n        weights = render_out['weights']\n        depth = render_out['depth']\n\n        planes_gt = None\n        if 'planes_gt' in input_model:\n            planes_gt = input_model['planes_gt']\n        if 'subplanes_gt' in input_model:\n            subplanes_gt = input_model['subplanes_gt']\n        \n        logs_summary = {}\n\n        # patchmatch loss\n        normals_target, mask_use_normals_target = None, None\n        pts_target, mask_use_pts_target = None, None\n        if patchmatch_out is not None:\n            if patchmatch_out['patchmatch_mode'] == 'use_geocheck':\n                mask_use_normals_target = (patchmatch_out['idx_scores_min'] > 0).float()\n                normals_target = input_model['normals_gt']\n            else:\n                raise NotImplementedError\n        else:\n            if self.normal_weight>0:\n                normals_target = input_model['normals_gt']\n                mask_use_normals_target = torch.ones(batch_size, 1).bool()\n\n        # Color loss\n        color_fine_loss, background_loss, psnr = 0, 0, 0\n        if True:\n            color_error = (color_fine - true_rgb) * mask\n            color_fine_loss = F.l1_loss(color_error, torch.zeros_like(color_error), reduction='sum') / mask_sum\n                \n            psnr = 20.0 * torch.log10(1.0 / (((color_fine - true_rgb)**2 * mask).sum() / (mask_sum * 3.0)).sqrt())\n\n            # Mask loss, optional\n            background_loss = F.binary_cross_entropy(weight_sum.clip(1e-3, 1.0 - 1e-3), mask)\n            logs_summary.update({           \n                'Loss/loss_color':  color_fine_loss.detach().cpu(),\n                'Loss/loss_bg':     background_loss\n            })\n        \n        # Eikonal loss\n        gradient_error_loss = 0\n        if self.igr_weight > 0:\n            gradient_error_loss = gradient_error_fine\n            logs_summary.update({           \n                'Loss/loss_eik':    gradient_error_loss.detach().cpu(),\n            })            \n        \n        # Smooth loss, optional\n        surf_reg_loss = 0.0\n        if self.smooth_weight > 0:\n            depth = render_out['depth'].detach()\n            pts = rays_o + depth * rays_d\n            n_pts = pts + torch.randn_like(pts) * 1e-3  # WARN: Hard coding here\n            surf_normal = sdf_network_fine.gradient(torch.cat([pts, n_pts], dim=0)).squeeze()\n            surf_normal = surf_normal / torch.linalg.norm(surf_normal, dim=-1, ord=2, keepdim=True)\n\n            surf_reg_loss_pts = (torch.linalg.norm(surf_normal[:batch_size, :] - surf_normal[batch_size:, :], ord=2, dim=-1, keepdim=True))\n            # surf_reg_loss = (surf_reg_loss_pts*pixel_weight).mean()\n            surf_reg_loss = surf_reg_loss_pts.mean()\n\n        # normal loss\n        normals_fine_loss, mask_keep_gt_normal = 0.0, torch.ones(batch_size)\n        if self.normal_weight > 0 and normals_target is not None:\n            normals_gt = normals_target # input_model['normals_gt'] #\n            normals_fine = render_out['normal']\n            \n            normal_certain_weight = torch.ones(batch_size, 1).bool()\n            if 'normal_certain_weight' in input_model:\n                normal_certain_weight = input_model['normal_certain_weight']\n\n            thres_clip_angle = -1 #\n            normal_certain_weight = normal_certain_weight*mask_use_normals_target\n            angular_error, mask_keep_gt_normal = TrainingUtils.get_angular_error(normals_fine, normals_gt, normal_certain_weight, thres_clip_angle)\n\n            normals_fine_loss = angular_error\n            logs_summary.update({\n                'Loss/loss_normal_gt': normals_fine_loss\n            })\n\n        # depth loss, optional\n        depths_fine_loss = 0.0\n        pts_target = input_model['depth_gt']\n        mask_use_pts_target = torch.ones(batch_size, 1).bool()\n        if self.depth_weight > 0 and (pts_target is not None):\n            pts = rays_o + depth * rays_d\n            pts_error = (pts_target - pts) * mask_use_pts_target\n            pts_error = torch.linalg.norm(pts_error, dim=-1, keepdims=True)\n\n            depths_fine_loss = F.l1_loss(pts_error, torch.zeros_like(pts_error), reduction='mean') / (mask_use_pts_target.sum()+1e-6)\n            logs_summary.update({\n                'Loss/loss_depth': depths_fine_loss,\n                'Log/num_depth_target_use': mask_use_pts_target.sum().detach().cpu()\n            })\n\n        plane_loss_all = 0\n        if self.normal_consistency_weight > 0 and self.iter_step > self.plane_loss_milestone:\n            num_planes = int(planes_gt.max().item())\n\n            depth = render_out['depth']   # detach?\n            pts = rays_o + depth * rays_d\n            normals_fine = render_out['normal']\n\n            # (1) normal consistency loss\n            num_pixels_on_planes = 0\n            num_pixels_on_subplanes = 0\n\n            dominant_normal_planes = []\n            normal_consistency_loss = torch.zeros(num_planes)\n            loss_plane_offset = torch.zeros(num_planes)\n            for i in range(int(num_planes)):\n                idx_plane = i + 1\n                mask_curr_plane = planes_gt.eq(idx_plane)\n                if mask_curr_plane.float().max() < 1.0:\n                    # this plane is not existent\n                    continue\n                consistency_loss_tmp, num_pixels_tmp, normal_mean_curr = get_normal_consistency_loss(normals_fine, mask_curr_plane)\n                normal_consistency_loss[i] = consistency_loss_tmp\n                num_pixels_on_planes += num_pixels_tmp\n\n                # for Manhattan loss\n                if i < 3:\n                    # only use the 3 dominant planes\n                    dominant_normal_planes.append(normal_mean_curr)\n                \n                # (2) plane-to-origin offset loss\n                if self.plane_offset_weight > 0:\n                    # normal_mean_curr_no_grad =  normal_mean_curr.detach()\n                    plane_offset_loss_curr, num_pixels_subplanes_valid_curr = get_plane_offset_loss(pts, normal_mean_curr, mask_curr_plane, subplanes_gt)\n                    loss_plane_offset[i] = plane_offset_loss_curr\n                    num_pixels_on_subplanes += num_pixels_subplanes_valid_curr\n            \n            assert num_pixels_on_planes >= MIN_PIXELS_PLANE                   \n            normal_consistency_loss = normal_consistency_loss.sum() / (num_pixels_on_planes+1e-6)\n            loss_plane_offset = loss_plane_offset.sum() / (num_pixels_on_subplanes+1e-6)\n            \n            # (3) normal manhattan loss\n            loss_normal_manhattan = 0\n            if self.manhattan_constrain_weight > 0:\n                loss_normal_manhattan = get_manhattan_normal_loss(dominant_normal_planes)\n\n            plane_loss_all = normal_consistency_loss * self.normal_consistency_weight  + \\\n                                    loss_normal_manhattan * self.manhattan_constrain_weight + \\\n                                    loss_plane_offset * self.plane_offset_weight\n\n        loss = color_fine_loss * self.color_weight +\\\n                gradient_error_loss * self.igr_weight +\\\n                surf_reg_loss * self.smooth_weight +\\\n                plane_loss_all +\\\n                background_loss * self.mask_weight +\\\n                normals_fine_loss * self.normal_weight * self.get_warm_up_ratio()  + \\\n                depths_fine_loss * self.depth_weight  #+ \\\n\n        logs_summary.update({\n            'Loss/loss': loss.detach().cpu(),\n            'Loss/loss_smooth': surf_reg_loss,\n            'Loss/variance':    variance.mean().detach(),\n            'Log/psnr':         psnr,\n            'Log/ratio_warmup_loss':  self.get_warm_up_ratio()\n        })\n        return loss, logs_summary, mask_keep_gt_normal"]}
{"filename": "models/dataset.py", "chunked_list": ["import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\n\nimport cv2 as cv\nimport numpy as np\nimport os, copy, logging\n\nfrom glob import glob", "\nfrom glob import glob\nfrom icecream import ic\nfrom scipy.spatial.transform import Rotation as Rot\nfrom scipy.spatial.transform import Slerp\n\nfrom utils.utils_image import read_images, write_image, write_images\nfrom utils.utils_io import checkExistence, ensure_dir_existence, get_path_components, get_files_stem\nfrom utils.utils_geometry import get_pose_inv, get_world_normal, quat_to_rot, save_points\n", "from utils.utils_geometry import get_pose_inv, get_world_normal, quat_to_rot, save_points\n\nimport utils.utils_geometry as GeoUtils\nimport utils.utils_io as IOUtils\nimport models.patch_match_cuda as PatchMatch\nimport utils.utils_training as TrainingUtils\nimport utils.utils_image as ImageUtils\n\n\ndef load_K_Rt_from_P(filename, P=None):\n    if P is None:\n        lines = open(filename).read().splitlines()\n        if len(lines) == 4:\n            lines = lines[1:]\n        lines = [[x[0], x[1], x[2], x[3]] for x in (x.split(\" \") for x in lines)]\n        P = np.asarray(lines).astype(np.float32).squeeze()\n\n    out = cv.decomposeProjectionMatrix(P)\n    K = out[0]\n    R = out[1]\n    t = out[2]\n\n    K = K / K[2, 2]\n    intrinsics = np.eye(4)\n    intrinsics[:3, :3] = K\n\n    pose = np.eye(4, dtype=np.float32)\n    pose[:3, :3] = R.transpose()\n    pose[:3, 3] = (t[:3] / t[3])[:, 0]\n\n    return intrinsics, pose", "\ndef load_K_Rt_from_P(filename, P=None):\n    if P is None:\n        lines = open(filename).read().splitlines()\n        if len(lines) == 4:\n            lines = lines[1:]\n        lines = [[x[0], x[1], x[2], x[3]] for x in (x.split(\" \") for x in lines)]\n        P = np.asarray(lines).astype(np.float32).squeeze()\n\n    out = cv.decomposeProjectionMatrix(P)\n    K = out[0]\n    R = out[1]\n    t = out[2]\n\n    K = K / K[2, 2]\n    intrinsics = np.eye(4)\n    intrinsics[:3, :3] = K\n\n    pose = np.eye(4, dtype=np.float32)\n    pose[:3, :3] = R.transpose()\n    pose[:3, 3] = (t[:3] / t[3])[:, 0]\n\n    return intrinsics, pose", "\n\nclass Dataset:\n    '''Check normal and depth in folder depth_cloud\n    '''\n    def __init__(self, conf):\n        super(Dataset, self).__init__()\n        # logging.info('Load data: Begin')\n        self.device = torch.device('cuda')\n        self.conf = conf\n\n        self.data_dir = conf['data_dir']\n        self.cache_all_data = conf['cache_all_data']\n        assert self.cache_all_data == False\n        self.mask_out_image = conf['mask_out_image']\n        self.estimate_scale_mat = conf['estimate_scale_mat']\n        self.piece_size = 2**20\n        self.bbox_size_half = conf['bbox_size_half']\n        self.use_normal = conf['use_normal']\n        self.resolution_level = conf['resolution_level']\n\n        self.denoise_gray_image = self.conf['denoise_gray_image']\n        self.denoise_paras = self.conf['denoise_paras']\n\n        self.use_planes = conf['use_planes']\n        self.use_plane_offset_loss = conf['use_plane_offset_loss']\n \n        path_cam = os.path.join(self.data_dir, './cameras_sphere.npz')  # cameras_sphere, cameras_linear_init\n        camera_dict = np.load(path_cam)\n        logging.info(f'Load camera dict: {path_cam.split(\"/\")[-1]}')\n        \n        images_lis = None\n        for ext in ['.png', '.JPG']:\n            images_lis = sorted(glob(os.path.join(self.data_dir, f'image/*{ext}')))\n            self.vec_stem_files = get_files_stem(f'{self.data_dir}/image', ext_file=ext)\n            if len(images_lis) > 0:\n                break\n        assert len(images_lis) > 0\n        \n        self.n_images = len(images_lis)\n        logging.info(f\"Read {self.n_images} images.\")\n        self.images_lis = images_lis\n        self.images_np = np.stack([self.read_img(im_name, self.resolution_level) for im_name in images_lis]) / 256.0\n        masks_lis = sorted(glob(os.path.join(self.data_dir, 'mask/*.png')))\n        if len(masks_lis) ==0:\n            self.masks_np = np.ones(self.images_np.shape[:-1])\n            logging.info(f\"self.masks_np.shape: {self.masks_np.shape}\")\n        else:\n            self.masks_np = np.stack([self.read_img(im_name, self.resolution_level) for im_name in masks_lis])[:,:,:,0] / 256.0\n\n        if self.mask_out_image:\n            self.images_np[np.where(self.masks_np < 0.5)] = 0.0\n\n        # world_mat: projection matrix: world to image\n        self.world_mats_np = [camera_dict['world_mat_%d' % idx].astype(np.float32) for idx in range(self.n_images)]\n\n        self.scale_mats_np = []\n\n        # scale_mat: used for coordinate normalization, we assume the object is inside the unit sphere at origin.\n        if self.estimate_scale_mat:\n            self.scale_mats_np = self.estimated_scale_mat()\n        else:\n            self.scale_mats_np = [camera_dict['scale_mat_%d' % idx].astype(np.float32) for idx in range(self.n_images)]\n\n        self.intrinsics_all = []\n        self.pose_all = []\n\n        # i = 0\n        for scale_mat, world_mat in zip(self.scale_mats_np, self.world_mats_np):\n            P = world_mat @ scale_mat\n            P = P[:3, :4]\n            intrinsics, pose = load_K_Rt_from_P(None, P)\n            if self.resolution_level > 1.0:\n                intrinsics[:2,:3] /= self.resolution_level\n            self.intrinsics_all.append(torch.from_numpy(intrinsics).float())\n\n            self.pose_all.append(torch.from_numpy(pose).float())\n\n        self.images = torch.from_numpy(self.images_np.astype(np.float32)).cpu()  # n_images, H, W, 3   # Save GPU memory\n        self.masks  = torch.from_numpy(self.masks_np.astype(np.float32)).cpu()   # n_images, H, W, 3   # Save GPU memory\n        h_img, w_img, _ = self.images[0].shape\n        logging.info(f\"Resolution level: {self.resolution_level}. Image size: ({w_img}, {h_img})\")\n\n        if self.use_normal:\n            logging.info(f'[Use normal] Loading estimated normals...')\n            normals_np = []\n            normals_npz, stems_normal = read_images(f'{self.data_dir}/pred_normal', target_img_size=(w_img, h_img), img_ext='.npz')\n            assert len(normals_npz) == self.n_images\n            for i in tqdm(range(self.n_images)):\n                normal_img_curr = normals_npz[i]\n        \n                # transform to world coordinates\n                ex_i = torch.linalg.inv(self.pose_all[i])\n                img_normal_w = get_world_normal(normal_img_curr.reshape(-1, 3), ex_i).reshape(h_img, w_img,3)\n\n                normals_np.append(img_normal_w)\n                \n            self.normals_np = -np.stack(normals_np)   # reverse normal\n            self.normals = torch.from_numpy(self.normals_np.astype(np.float32)).cpu()\n\n            debug_ = True\n            if debug_ and IOUtils.checkExistence(f'{self.data_dir}/depth'):\n                self.depths_np, stems_depth = read_images(f'{self.data_dir}/depth', target_img_size=(w_img, h_img), img_ext='.png')\n                dir_depths_cloud = f'{self.data_dir}/depth_cloud'\n                ensure_dir_existence(dir_depths_cloud)\n                \n                for i in range(len(self.images)):\n                    ext_curr = get_pose_inv(self.pose_all[i].detach().cpu().numpy())\n                    pts = GeoUtils.get_world_points( self.depths_np[i], self.intrinsics_all[i], ext_curr)\n                    \n                    normals_curr = self.normals_np[i].reshape(-1,3)\n                    colors = self.images_np[i].reshape(-1,3)\n                    save_points(f'{dir_depths_cloud}/{stems_depth[i]}.ply', pts, colors, normals_curr)\n                    pts2 = torch.from_numpy(pts.astype(np.float32)).cpu()\n                    self.pts = pts2.unsqueeze(0).expand(self.images.shape[0], -1, -1)\n\n        if self.use_planes:\n            logging.info(f'Use planes: Loading planes...')  \n\n            planes_np = []\n            planes_lis = sorted(glob(f'{self.data_dir}/pred_normal_planes/*.png'))\n            assert len(planes_lis) == self.n_images\n            for i in range(self.n_images):\n                path = planes_lis[i]\n                img_plane = cv.imread(path, cv.IMREAD_UNCHANGED)\n\n                if img_plane.shape[0] != h_img:\n                    img_plane = cv.resize(img_plane, (w_img, h_img), interpolation=cv.INTER_NEAREST)\n\n                planes_np.append(img_plane)\n            self.planes_np = np.stack(planes_np)\n            # if self.planes_np.max() > 40:\n            #     self.planes_np = self.planes_np // 40\n            assert self.planes_np.max() <= 20 \n            self.planes = torch.from_numpy(self.planes_np.astype(np.float32)).cpu()\n\n        if self.use_plane_offset_loss:\n            logging.info(f'Use planes: Loading subplanes...')  \n\n            subplanes_np, stem_subplanes = read_images(f'{self.data_dir}/pred_normal_subplanes', \n                                                            target_img_size=(w_img, h_img), \n                                                            interpolation=cv.INTER_NEAREST, \n                                                            img_ext='.png')\n            # subplanes_np = subplanes_np // 40\n            assert subplanes_np.max() <= 20 \n            self.subplanes = torch.from_numpy(subplanes_np.astype(np.uint8)).cpu()\n\n        self.intrinsics_all = torch.stack(self.intrinsics_all).to(self.device)  # n, 4, 4\n        self.intrinsics_all_inv = torch.inverse(self.intrinsics_all) # n, 4, 4\n        self.focal = self.intrinsics_all[0][0, 0]\n        self.pose_all = torch.stack(self.pose_all).to(self.device)  # n_images, 4, 4\n        self.H, self.W = self.images.shape[1], self.images.shape[2]\n        self.image_pixels = self.H * self.W\n\n        # for patch match\n        self.min_neighbors_ncc = 3\n        if IOUtils.checkExistence(self.data_dir + '/neighbors.txt'):\n            self.min_neighbors_ncc = 1  # images are relatively sparse\n            path_neighbors = self.data_dir + '/neighbors.txt'\n            logging.info(f'Use openMVS neighbors.')\n            self.dict_neighbors = {}\n            with open(path_neighbors, 'r') as fneighbor:\n                lines = fneighbor.readlines()\n                for line in lines:\n                    line = line.split(' ')\n                    line = np.array(line).astype(np.int32)\n                    if len(line) > 1:\n                        self.dict_neighbors[line[0]] = line[1:]\n                    else:\n                        logging.info(f'[{line[0]}] No neighbors, use adjacent views')\n                        self.dict_neighbors[line[0]] = [np.min([line[0] + 1, self.n_images - 1]), np.min([line[0] - 1, 0]) ]\n                \n                for i in range(self.n_images):\n                    if i not in self.dict_neighbors:\n                        print(f'[View {i}] error: No nerighbors. Using {i-1, i+1}')\n                        self.dict_neighbors[i] = [i-1,i+1]\n                        msg = input('Check neighbor view...[y/n]')\n                        if msg == 'n':\n                            exit()\n                assert len(self.dict_neighbors) == self.n_images       \n        else:\n            logging.info(f'Use adjacent views as neighbors.')\n\n\n        self.initialize_patchmatch()\n        \n        # Gen train_data\n        self.train_data = None\n        if self.cache_all_data:\n            train_data = []\n            # Calc rays\n            rays_o, rays_d = self.gen_rays()\n            self.train_idx = []\n\n            for i in range(len(self.images)):\n                cur_data = torch.cat([rays_o[i], rays_d[i], self.images[i].to(self.device), self.masks[i, :, :, :1].to(self.device)], dim=-1).detach()\n                # cur_data: [H, W, 10]\n                cur_data = cur_data[torch.randperm(len(cur_data))]\n                train_data.append(cur_data.reshape(-1, 10).detach().cpu())\n            # train_data.append(cur_data.reshape(-1, 10))\n            self.train_data = torch.stack(train_data).reshape(-1, 10).detach().cpu()\n            self.train_piece = None\n            self.train_piece_np = None\n            self.large_counter = 0\n            self.small_counter = 0\n            del self.images\n            del self.masks\n\n        self.sphere_radius =  conf['sphere_radius']\n        if checkExistence(f'{self.data_dir}/bbox.txt'):\n            logging.info(f\"Loading bbox.txt\")\n            bbox = np.loadtxt(f'{self.data_dir}/bbox.txt')\n            self.bbox_min = bbox[:3]\n            self.bbox_max = bbox[3:6]\n        else:\n            self.bbox_min = np.array([-1.01*self.bbox_size_half, -1.01*self.bbox_size_half, -1.01*self.bbox_size_half])\n            self.bbox_max = np.array([ 1.01*self.bbox_size_half,  1.01*self.bbox_size_half,  1.01*self.bbox_size_half])\n\n        self.iter_step = 0\n        \n    def initialize_patchmatch(self):\n        self.check_occlusion = self.conf['check_occlusion']\n        \n        logging.info(f'Prepare gray images...')\n        self.extrinsics_all = torch.linalg.inv(self.pose_all)\n        self.images_gray = []\n        self.images_denoise_np = []\n\n        if self.denoise_gray_image:\n            dir_denoise = f'{self.data_dir}/image_denoised_cv{self.denoise_paras[0]:02d}{self.denoise_paras[1]:02d}{self.denoise_paras[2]:02d}{self.denoise_paras[3]:02d}'\n            if not checkExistence(dir_denoise) and len(get_files_stem(dir_denoise, '.png'))==0:\n                logging.info(f'Use opencv structural denoise...')\n                for i in tqdm(range(self.n_images)):\n                    img_idx = (self.images_np[i]*256)\n                    img_idx = cv.fastNlMeansDenoisingColored(img_idx.astype(np.uint8), None, h = self.denoise_paras[2], \n                                                                                            hColor = self.denoise_paras[3], \n                                                                                            templateWindowSize = self.denoise_paras[0], \n                                                                                            searchWindowSize = self.denoise_paras[1])\n                    self.images_denoise_np.append(img_idx)\n\n                self.images_denoise_np = np.array(self.images_denoise_np)\n\n                # save denoised images\n                stems_imgs = get_files_stem(f'{self.data_dir}/image', '.png')\n                write_images(dir_denoise, self.images_denoise_np, stems_imgs)\n            else:\n                logging.info(f'Load predenoised images by openCV structural denoise: {dir_denoise}')\n                images_denoised_lis = sorted(glob(f'{dir_denoise}/*.png'))\n                self.images_denoise_np = np.stack([self.read_img(im_name, self.resolution_level) for im_name in images_denoised_lis])\n        else:\n            logging.info(f'Use original image to generate gray image...')\n            self.images_denoise_np = self.images_np * 255\n\n        for i in tqdm(range(self.n_images)):\n            img_gray = cv.cvtColor(self.images_denoise_np[i].astype(np.uint8), cv.COLOR_BGR2GRAY)\n            self.images_gray.append(img_gray)\n\n        # range: (0,255)\n        self.images_gray_np = np.array(self.images_gray).astype(np.float32)\n        self.images_gray = torch.from_numpy(self.images_gray_np).cuda()\n\n        # For cache rendered depths and normals\n        self.confidence_accum = None\n        self.samples_accum = None\n        self.normals_accum = None\n        self.depths_accum = None\n        self.points_accum = None\n        self.render_difference_accum = None\n        self.samples_accum = torch.zeros_like(self.masks, dtype=torch.int32).cuda()\n        \n        b_accum_all_data = False\n        if b_accum_all_data:\n            self.colors_accum = torch.zeros_like(self.images, dtype=torch.float32).cuda()\n                    \n    def read_img(self, path, resolution_level):\n        img = cv.imread(path)\n        H, W = img.shape[0], img.shape[1]\n\n        if resolution_level > 1.0:\n            img = cv.resize(img, (int(W/resolution_level), int(H/resolution_level)), interpolation=cv.INTER_LINEAR)\n\n            # save resized iamge for visulization\n            ppath, stem, ext = get_path_components(path)\n            dir_resize = ppath+f'_reso{int(resolution_level)}'\n            logging.debug(f'Resize dir: {dir_resize}')\n            os.makedirs(dir_resize, exist_ok=True)\n            write_image(os.path.join(dir_resize, stem+ext), img)\n\n        return img\n\n    def estimated_scale_mat(self):\n        assert len(self.world_mats_np) > 0\n        rays_o = []\n        rays_v = []\n        for world_mat in self.world_mats_np:\n            P = world_mat[:3, :4]\n            intrinsics, c2w = load_K_Rt_from_P(None, P)\n            rays_o.append(c2w[:3, 3])\n            rays_v.append(c2w[:3, 0])\n            rays_o.append(c2w[:3, 3])\n            rays_v.append(c2w[:3, 1])\n\n        rays_o = np.stack(rays_o, axis=0)   # N * 3\n        rays_v = np.stack(rays_v, axis=0)   # N * 3\n        dot_val = np.sum(rays_o * rays_v, axis=-1, keepdims=True)  # N * 1\n        center, _, _, _ = np.linalg.lstsq(rays_v, dot_val)\n        center = center.squeeze()\n        radius = np.max(np.sqrt(np.sum((rays_o - center[None, :])**2, axis=-1)))\n        scale_mat = np.diag([radius, radius, radius, 1.0])\n        scale_mat[:3, 3] = center\n        scale_mat = scale_mat.astype(np.float32)\n        scale_mats = [scale_mat for _ in self.world_mats_np]\n\n        return scale_mats\n\n    def gen_rays(self):\n        tx = torch.linspace(0, self.W - 1, self.W)\n        ty = torch.linspace(0, self.H - 1, self.H)\n        pixels_x, pixels_y = torch.meshgrid(tx, ty)\n        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1)  # W, H, 3\n        p = torch.matmul(self.intrinsics_all_inv[:, None, None, :3, :3], p[None, :, :, :, None]).squeeze() # n, W, H, 3\n        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)  # n, W, H, 3\n        rays_v = torch.matmul(self.pose_all[:, None, None, :3, :3],  rays_v[:, :, :, :, None]).squeeze()  # n, W, H, 3\n        rays_o = self.pose_all[:, None, None, :3, 3].expand(rays_v.shape)  # n, W, H, 3\n        return rays_o.transpose(1, 2), rays_v.transpose(1, 2)\n\n    def get_pose(self, img_idx, pose):\n        pose_cur = None\n        if pose == None:\n            pose_cur = self.pose_all[img_idx]\n        elif pose is not None:\n            if pose.dim() == 1:\n                pose = pose.unsqueeze(0)\n            assert pose.dim() == 2\n            if pose.shape[1] == 7: #In case of quaternion vector representation\n                cam_loc = pose[:, 4:]\n                R = quat_to_rot(pose[:,:4])\n                p = torch.eye(4).repeat(pose.shape[0],1,1).cuda().float()\n                p[:, :3, :3] = R\n                p[:, :3, 3] = cam_loc\n            else: # In case of pose matrix representation\n                cam_loc = pose[:, :3, 3]\n                p = pose\n            pose_cur = p\n        else:\n            NotImplementedError \n\n        return pose_cur.squeeze()\n\n    def gen_rays_at(self, img_idx, pose = None, resolution_level=1):\n        pose_cur = self.get_pose(img_idx, pose)\n\n        l = resolution_level\n        tx = torch.linspace(0, self.W - 1, self.W // l)\n        ty = torch.linspace(0, self.H - 1, self.H // l)\n        pixels_x, pixels_y = torch.meshgrid(tx, ty)\n        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1) # W, H, 3\n        p = torch.matmul(self.intrinsics_all_inv[img_idx, None, None, :3, :3], p[:, :, :, None]).squeeze()  # W, H, 3\n        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)  # W, H, 3\n        rays_v = torch.matmul(pose_cur[None, None, :3, :3], rays_v[:, :, :, None]).squeeze()  # W, H, 3\n        rays_o = pose_cur[None, None, :3, 3].expand(rays_v.shape)  # W, H, 3\n        return rays_o.transpose(0, 1), rays_v.transpose(0, 1)\n\n    def gen_rays_between(self, idx_0, idx_1, ratio, resolution_level=1):\n        l = resolution_level\n        tx = torch.linspace(0, self.W - 1, self.W // l)\n        ty = torch.linspace(0, self.H - 1, self.H // l)\n        pixels_x, pixels_y = torch.meshgrid(tx, ty)\n        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1)  # W, H, 3\n        p = torch.matmul(self.intrinsics_all_inv[0, None, None, :3, :3], p[:, :, :, None]).squeeze()  # W, H, 3\n        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)  # W, H, 3\n        trans = self.pose_all[idx_0, :3, 3] * (1.0 - ratio) + self.pose_all[idx_1, :3, 3] * ratio\n        pose_0 = self.pose_all[idx_0].detach().cpu().numpy()\n        pose_1 = self.pose_all[idx_1].detach().cpu().numpy()\n        pose_0 = np.linalg.inv(pose_0)\n        pose_1 = np.linalg.inv(pose_1)\n        rot_0 = pose_0[:3, :3]\n        rot_1 = pose_1[:3, :3]\n        rots = Rot.from_matrix(np.stack([rot_0, rot_1]))\n        key_times = [0, 1]\n        key_rots = [rot_0, rot_1]\n        slerp = Slerp(key_times, rots)\n        rot = slerp(ratio)\n        pose = np.diag([1.0, 1.0, 1.0, 1.0])\n        pose = pose.astype(np.float32)\n        pose[:3, :3] = rot.as_matrix()\n        pose[:3, 3] = ((1.0 - ratio) * pose_0 + ratio * pose_1)[:3, 3]\n        pose = np.linalg.inv(pose)\n        rot = torch.from_numpy(pose[:3, :3]).cuda()\n        trans = torch.from_numpy(pose[:3, 3]).cuda()\n        rays_v = torch.matmul(rot[None, None, :3, :3], rays_v[:, :, :, None]).squeeze()  # W, H, 3\n        rays_o = trans[None, None, :3].expand(rays_v.shape)  # W, H, 3\n        return rays_o.transpose(0, 1), rays_v.transpose(0, 1)\n\n    def gen_random_rays_at(self, img_idx, batch_size):\n        \"\"\"\n        Generate random rays at world space from one camera.\n        \"\"\"\n        pixels_x = torch.randint(low=0, high=self.W, size=[batch_size])\n        pixels_y = torch.randint(low=0, high=self.H, size=[batch_size])\n        color = self.images[img_idx][(pixels_y, pixels_x)]    # batch_size, 3\n        mask = self.masks[img_idx][(pixels_y, pixels_x)]      # batch_size, 3\n        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1).float()  # batch_size, 3\n        p = torch.matmul(self.intrinsics_all_inv[img_idx, None, :3, :3], p[:, :, None]).squeeze() # batch_size, 3\n        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)    # batch_size, 3\n        rays_v = torch.matmul(self.pose_all[img_idx, None, :3, :3], rays_v[:, :, None]).squeeze()  # batch_size, 3\n        rays_o = self.pose_all[img_idx, None, :3, 3].expand(rays_v.shape) # batch_size, 3\n        return torch.cat([rays_o.cpu(), rays_v.cpu(), color, mask[:, :1]], dim=-1).cuda()    # batch_size, 10\n        \n    def random_get_rays_at(self, img_idx, batch_size, pose = None):\n        pose_cur = self.get_pose(img_idx, pose)\n        \n        pixels_x = torch.randint(low=0, high=self.W, size=[batch_size]).cpu()\n        pixels_y = torch.randint(low=0, high=self.H, size=[batch_size]).cpu()\n            \n        color = self.images[img_idx][(pixels_y, pixels_x)]    # batch_size, 3\n        mask = self.masks[img_idx][(pixels_y, pixels_x)][:,None]     # batch_size, 3\n        pts_target = self.pts[img_idx][(pixels_y*pixels_x)]\n        p = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_y)], dim=-1).float()  # batch_size, 3\n        p = p.to(self.intrinsics_all_inv.device) \n        self.intrinsics_all_inv = self.intrinsics_all_inv.to(p.device)\n        p = torch.matmul(self.intrinsics_all_inv[img_idx, None, :3, :3], p[:, :, None]).squeeze() # batch_size, 3\n        rays_v = p / torch.linalg.norm(p, ord=2, dim=-1, keepdim=True)    # batch_size, 3\n        rays_v = torch.matmul(pose_cur[None, :3, :3], rays_v[:, :, None]).squeeze()  # batch_size, 3\n        rays_o = pose_cur[None, :3, 3].expand(rays_v.shape) # batch_size, 3\n        \n        normal_sample = None\n        if self.use_normal:\n            normal_sample = self.normals[img_idx][(pixels_y, pixels_x)].cuda()\n\n        planes_sample = None\n        if self.use_planes:\n            planes_sample = self.planes[img_idx][(pixels_y, pixels_x)].unsqueeze(-1).cuda()\n        \n        subplanes_sample = None\n        if self.use_plane_offset_loss:\n            subplanes_sample = self.subplanes[img_idx][(pixels_y, pixels_x)].unsqueeze(-1).cuda()\n\n        return torch.cat([rays_o.cpu(), rays_v.cpu(), color, mask, pts_target], dim=-1).cuda(), pixels_x, pixels_y, normal_sample, planes_sample, subplanes_sample    # batch_size, 10\n\n    def near_far_from_sphere(self, rays_o, rays_d):\n        # torch\n        assert self.sphere_radius is not None\n        a = torch.sum(rays_d**2, dim=-1, keepdim=True)\n        b = 2.0 * torch.sum(rays_o * rays_d, dim=-1, keepdim=True)\n        c = torch.sum(rays_o ** 2, dim=-1, keepdim=True) - self.sphere_radius**2\n        mid = 0.5 * (-b) / a\n        near = mid - self.sphere_radius\n        far = mid + self.sphere_radius\n        return near, far\n\n    def image_at(self, idx, resolution_level):\n        img = cv.imread(self.images_lis[idx])\n        return (cv.resize(img, (self.W // resolution_level, self.H // resolution_level))).clip(0, 255)\n\n    def shuffle(self):\n        r = torch.randperm(len(self.train_data))\n        self.train_data = self.train_data[r]\n        self.large_counter = 0\n        self.small_counter = 0\n\n    def next_train_batch(self, batch_size):\n        if self.train_piece == None or self.small_counter + batch_size >= len(self.train_piece):\n            if self.train_piece == None or self.large_counter + self.piece_size >= len(self.train_data):\n                self.shuffle()\n            self.train_piece_np = self.train_data[self.large_counter: self.large_counter + self.piece_size]\n            self.train_piece = self.train_piece_np.cuda()\n            self.small_counter = 0\n            self.large_counter += self.piece_size\n\n        curr_train_data = self.train_piece[self.small_counter: self.small_counter + batch_size]\n        curr_train_data_np = self.train_piece_np[self.small_counter: self.small_counter + batch_size]\n        self.small_counter += batch_size\n\n        return curr_train_data, curr_train_data_np\n\n\n    def score_pixels_ncc(self, idx, pts_world, normals_world, pixels_coords_vu, reso_level = 1.0, _debug = False):\n        '''Use patch-match to evaluate the geometry: Smaller, better\n        Return:\n            scores_all_mean: N*1\n            diff_patch_all: N*1\n            mask_valid_all: N*1\n        '''\n        K = copy.deepcopy(self.intrinsics_all[0][:3,:3])\n        img_ref = self.images_gray[idx]\n        H, W = img_ref.shape\n        window_size, window_step= 11, 2\n        if reso_level > 1:\n            K[:2,:3] /= reso_level\n            img_ref = self.images_gray_np[idx]\n            img_ref = cv.resize(img_ref, (int(W/reso_level), int(H/reso_level)), interpolation=cv.INTER_LINEAR)\n            img_ref = torch.from_numpy(img_ref).cuda()\n            window_size, window_step= (5, 1) if reso_level== 2 else (3, 1)\n\n        if hasattr(self, 'dict_neighbors'):\n            idx_neighbors = self.dict_neighbors[int(idx)]\n            if len(idx_neighbors) < self.min_neighbors_ncc:\n                return torch.ones(pts_world.shape[0]), torch.zeros(pts_world.shape[0]), torch.zeros(pts_world.shape[0]).bool()\n        else:\n            idx_neighbors = [idx-3, idx-2, idx-1, idx+1, idx+2, idx+3]\n            if idx < 3:\n                idx_neighbors = [idx+1, idx+2, idx+3]\n            if idx > self.n_images-4:\n                idx_neighbors = [idx-3, idx-2, idx-1]\n\n        assert pixels_coords_vu.ndim == 2\n        num_patches = pixels_coords_vu.shape[0]\n\n        extrin_ref = self.extrinsics_all[idx]\n        pts_ref = (extrin_ref[None,...] @ TrainingUtils.convert_to_homo(pts_world)[..., None]).squeeze()[:,:3]\n        normals_ref = (extrin_ref[:3,:3][None,...] @ normals_world[..., None]).squeeze()\n        \n        patches_ref, idx_patch_pixels_ref, mask_idx_inside = PatchMatch.prepare_patches_src(img_ref, pixels_coords_vu, window_size, window_step)\n        scores_all_mean, diff_patch_all, count_valid_all = torch.zeros(num_patches, dtype=torch.float32), torch.zeros(num_patches, dtype=torch.float32), torch.zeros(num_patches, dtype=torch.uint8)\n        for idx_src in idx_neighbors:\n            img_src = self.images_gray[idx_src]\n            if reso_level > 1:\n                img_src = cv.resize(self.images_gray_np[idx_src], (int(W/reso_level), int(H/reso_level)), interpolation=cv.INTER_LINEAR)\n                img_src = torch.from_numpy(img_src).cuda()\n\n            extrin_src = self.extrinsics_all[idx_src]\n\n            homography = PatchMatch.compute_homography(pts_ref, normals_ref, K, extrin_ref, extrin_src)\n            idx_patch_pixels_src = PatchMatch.warp_patches(idx_patch_pixels_ref, homography)\n            patches_src = PatchMatch.sample_patches(img_src, idx_patch_pixels_src, sampling_mode = 'grid_sample')\n            scores_curr, diff_patch_mean_curr, mask_patches_valid_curr = PatchMatch.compute_NCC_score(patches_ref, patches_src)\n\n            # check occlusion\n            if self.check_occlusion:\n                mask_no_occlusion = scores_curr < 0.66\n                mask_patches_valid_curr = mask_patches_valid_curr & mask_no_occlusion\n                scores_curr[mask_no_occlusion==False] = 0.0\n                diff_patch_mean_curr[mask_no_occlusion==False] = 0.0\n\n            scores_all_mean += scores_curr\n            diff_patch_all += diff_patch_mean_curr\n            count_valid_all += mask_patches_valid_curr\n\n            if _debug:\n                corords_src = idx_patch_pixels_src[:,3,3].cpu().numpy().astype(int)\n                img_sample_ref = PatchMatch.visualize_sampled_pixels(self.images[idx].numpy()*255, pixels_coords_vu.cpu().numpy())\n                img_sample_src = PatchMatch.visualize_sampled_pixels(self.images[idx_src].numpy()*255, corords_src)\n                ImageUtils.write_image_lis(f'./test/ncc/{idx}_{idx_src}.png', [img_sample_ref, img_sample_src])\n\n                # save patches\n                ImageUtils.write_image_lis(f'./test/ncc/patches_{idx}.png',[ patches_ref[i].cpu().numpy() for i in range(len(patches_ref))], interval_img = 5 )\n                ImageUtils.write_image_lis(f'./test/ncc/patches_{idx_src}.png',[ patches_ref[i].cpu().numpy() for i in range(len(patches_src))], interval_img = 5 )\n\n        \n        # get the average scores of all neighbor views\n        mask_valid_all = count_valid_all>=self.min_neighbors_ncc\n        scores_all_mean[mask_valid_all] /= count_valid_all[mask_valid_all]\n        diff_patch_all[mask_valid_all]  /= count_valid_all[mask_valid_all]\n\n        # set unvalid scores and diffs to zero\n        scores_all_mean = scores_all_mean*mask_valid_all\n        diff_patch_all  = diff_patch_all*mask_valid_all\n\n\n        scores_all_mean[mask_valid_all==False] = 1.0 # average scores for pixels without patch.\n\n        return scores_all_mean, diff_patch_all, mask_valid_all", "            "]}
{"filename": "models/nerf_renderer.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport logging\nimport mcubes\nimport trimesh\nfrom icecream import ic\n\n\ndef extract_fields(bound_min, bound_max, resolution, query_func):\n    N = 64\n    X = torch.linspace(bound_min[0], bound_max[0], resolution).split(N)\n    Y = torch.linspace(bound_min[1], bound_max[1], resolution).split(N)\n    Z = torch.linspace(bound_min[2], bound_max[2], resolution).split(N)\n\n    u = np.zeros([resolution, resolution, resolution], dtype=np.float32)\n    with torch.no_grad():\n        for xi, xs in enumerate(X):\n            for yi, ys in enumerate(Y):\n                for zi, zs in enumerate(Z):\n                    xx, yy, zz = torch.meshgrid(xs, ys, zs)\n                    pts = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1), zz.reshape(-1, 1)], dim=-1)\n                    val = query_func(pts).reshape(len(xs), len(ys), len(zs)).detach().cpu().numpy()\n                    u[xi * N: xi * N + len(xs), yi * N: yi * N + len(ys), zi * N: zi * N + len(zs)] = val\n    return u", "\n\ndef extract_fields(bound_min, bound_max, resolution, query_func):\n    N = 64\n    X = torch.linspace(bound_min[0], bound_max[0], resolution).split(N)\n    Y = torch.linspace(bound_min[1], bound_max[1], resolution).split(N)\n    Z = torch.linspace(bound_min[2], bound_max[2], resolution).split(N)\n\n    u = np.zeros([resolution, resolution, resolution], dtype=np.float32)\n    with torch.no_grad():\n        for xi, xs in enumerate(X):\n            for yi, ys in enumerate(Y):\n                for zi, zs in enumerate(Z):\n                    xx, yy, zz = torch.meshgrid(xs, ys, zs)\n                    pts = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1), zz.reshape(-1, 1)], dim=-1)\n                    val = query_func(pts).reshape(len(xs), len(ys), len(zs)).detach().cpu().numpy()\n                    u[xi * N: xi * N + len(xs), yi * N: yi * N + len(ys), zi * N: zi * N + len(zs)] = val\n    return u", "\n\ndef extract_geometry(bound_min, bound_max, resolution, threshold, query_func):\n    logging.info('threshold: {}'.format(threshold))\n    u = extract_fields(bound_min, bound_max, resolution, query_func)\n    vertices, triangles = mcubes.marching_cubes(u, threshold)\n    b_max_np = bound_max.detach().cpu().numpy()\n    b_min_np = bound_min.detach().cpu().numpy()\n\n    vertices = vertices / (resolution - 1.0) * (b_max_np - b_min_np)[None, :] + b_min_np[None, :]\n    return vertices, triangles, u", "\n\ndef sample_pdf(bins, weights, n_samples, det=False):\n    # This implementation is from NeRF\n    # Get pdf\n    weights = weights + 1e-5  # prevent nans\n    pdf = weights / torch.sum(weights, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n    # Take uniform samples\n    if det:\n        u = torch.linspace(0. + 0.5 / n_samples, 1. - 0.5 / n_samples, steps=n_samples)\n        u = u.expand(list(cdf.shape[:-1]) + [n_samples])\n    else:\n        u = torch.rand(list(cdf.shape[:-1]) + [n_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    # inds = searchsorted(cdf, u, side='right')\n    inds = torch.searchsorted(cdf, u, right=True)\n    \n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n\n    return samples", "\n\nclass NeRFRenderer:\n    def __init__(self,\n                 nerf_coarse,\n                 nerf_fine,\n                 nerf_outside,\n                 n_samples,\n                 n_importance,\n                 n_outside,\n                 perturb):\n        self.nerf_coarse = nerf_coarse\n        self.nerf_fine = nerf_fine\n        self.nerf_outside = nerf_outside\n        self.n_samples = n_samples\n        self.n_importance = n_importance\n        self.n_outside = n_outside\n        self.perturb = perturb\n\n    def render_core_outside(self, rays_o, rays_d, z_vals, sample_dist, nerf, background_rgb=None):\n        \"\"\"\n        Render background\n        \"\"\"\n        batch_size, n_samples = z_vals.shape\n\n        # Section length\n        dists = z_vals[..., 1:] - z_vals[..., :-1]\n        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape)], -1)\n        mid_z_vals = z_vals + dists * 0.5\n\n        # Section midpoints\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # batch_size, n_samples, 3\n\n        dis_to_center = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).clip(1.0, 1e10)\n        pts = torch.cat([pts / dis_to_center, 1.0 / dis_to_center], dim=-1)       # batch_size, n_samples, 4\n\n        dirs = rays_d[:, None, :].expand(batch_size, n_samples, 3)\n\n        pts = pts.reshape(-1, 3 + int(self.n_outside > 0))\n        dirs = dirs.reshape(-1, 3)\n\n        density, sampled_color = nerf(pts, dirs)\n        alpha = 1.0 - torch.exp(-F.softplus(density.reshape(batch_size, n_samples)) * dists)\n        alpha = alpha.reshape(batch_size, n_samples)\n        weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]\n        sampled_color = sampled_color.reshape(batch_size, n_samples, 3)\n        color = (weights[:, :, None] * sampled_color).sum(dim=1)\n        if background_rgb is not None:\n            color = color + background_rgb * (1.0 - weights.sum(dim=-1, keepdim=True))\n\n        return {\n            'color': color,\n            'sampled_color': sampled_color,\n            'alpha': alpha,\n            'weights': weights,\n        }\n\n\n    def render_core(self,\n                    rays_o,\n                    rays_d,\n                    z_vals,\n                    sample_dist,\n                    nerf,\n                    background_alpha=None,\n                    background_sampled_color=None,\n                    background_rgb=None):\n        batch_size, n_samples = z_vals.shape\n        # Section length\n        dists = z_vals[..., 1:] - z_vals[..., :-1]\n        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape)], -1)\n        mid_z_vals = z_vals + dists * 0.5\n\n        # Section midpoints\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # n_rays, n_samples, 3\n        dirs = rays_d[:, None, :].expand(pts.shape)\n\n        pts = pts.reshape(-1, 3)\n        dirs = dirs.reshape(-1, 3)\n\n        density, sampled_color = nerf(pts, dirs)\n        alpha = 1.0 - torch.exp(-F.relu(density.reshape(batch_size, n_samples) + 1.0) * dists)   # plus 1.0 for better convergence\n        alpha = alpha.reshape(batch_size, n_samples)\n        sampled_color = sampled_color.reshape(batch_size, n_samples, 3)\n\n        pts_norm = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).reshape(batch_size, n_samples)\n        inside_sphere = (pts_norm < 1.0).float().detach()\n\n        # Render with background\n        if background_alpha is not None:\n            alpha = alpha * inside_sphere + background_alpha[:, :n_samples] * (1.0 - inside_sphere)\n            alpha = torch.cat([alpha, background_alpha[:, n_samples:]], dim=-1)\n            sampled_color = sampled_color * inside_sphere[:, :, None] +\\\n                            background_sampled_color[:, :n_samples] * (1.0 - inside_sphere)[:, :, None]\n            sampled_color = torch.cat([sampled_color, background_sampled_color[:, n_samples:]], dim=1)\n\n        weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]\n        weights_sum = weights.sum(dim=-1, keepdim=True)\n\n        color = (sampled_color * weights[:, :, None]).sum(dim=1)\n        if background_rgb is not None:    # Fixed background, usually black\n            color = color + background_rgb * (1.0 - weights_sum)\n\n        return {\n            'color': color,\n            'dists': dists,\n            'mid_z_vals': mid_z_vals,\n            'weights': weights,\n            'inside_sphere': inside_sphere\n        }\n\n    def render(self, rays_o, rays_d, near, far, perturb_overwrite=-1, background_rgb=None):\n        batch_size = len(rays_o)\n        sample_dist = 2.0 / self.n_samples   # Assuming the region of interest is a unit sphere\n        z_vals = torch.linspace(0.0, 1.0, self.n_samples)\n        z_vals = near + (far - near) * z_vals[None, :]\n\n        z_vals_outside = None\n        if self.n_outside > 0:\n            z_vals_outside = torch.linspace(1e-3, 1.0 - 1.0 / (self.n_outside + 1.0), self.n_outside)\n\n        n_samples = self.n_samples\n        perturb = self.perturb\n\n        if perturb_overwrite >= 0:\n            perturb = perturb_overwrite\n        if perturb > 0:\n            t_rand = (torch.rand([batch_size, 1]) - 0.5)\n            z_vals = z_vals + t_rand * 2.0 / self.n_samples\n\n            if self.n_outside > 0:\n                mids = .5 * (z_vals_outside[..., 1:] + z_vals_outside[..., :-1])\n                upper = torch.cat([mids, z_vals_outside[..., -1:]], -1)\n                lower = torch.cat([z_vals_outside[..., :1], mids], -1)\n                # stratified samples in those intervals\n                t_rand = torch.rand([batch_size, z_vals_outside.shape[-1]])\n                z_vals_outside = lower[None, :] + (upper - lower)[None, :] * t_rand\n\n        if self.n_outside > 0:\n            z_vals_outside = far / torch.flip(z_vals_outside, dims=[-1]) + 1.0 / self.n_samples\n\n        background_alpha = None\n        background_sampled_color = None\n\n        # Background model\n        if self.n_outside > 0:\n            z_vals_feed = torch.cat([z_vals, z_vals_outside], dim=-1)\n            z_vals_feed, _ = torch.sort(z_vals_feed, dim=-1)\n            ret_outside = self.render_core_outside(rays_o, rays_d, z_vals_feed, sample_dist, self.nerf_outside)\n\n            background_sampled_color = ret_outside['sampled_color']\n            background_alpha = ret_outside['alpha']\n\n        # Up sample\n        ret_coarse = self.render_core(rays_o,\n                                      rays_d,\n                                      z_vals,\n                                      sample_dist,\n                                      self.nerf_coarse,\n                                      background_rgb=background_rgb,\n                                      background_alpha=background_alpha,\n                                      background_sampled_color=background_sampled_color)\n\n        weights_coarse = ret_coarse['weights']\n        z_vals_up_sample = sample_pdf(z_vals,\n                                      weights_coarse[:, :self.n_samples - 1] * ret_coarse['inside_sphere'][:, :self.n_samples - 1],\n                                      self.n_importance, det=True)\n        z_vals_up_sample = z_vals_up_sample.detach()\n        z_vals_fine = torch.cat([z_vals, z_vals_up_sample], dim=-1)\n        z_vals_fine, _ = torch.sort(z_vals_fine, dim=-1)\n\n        # Background model\n        if self.n_outside > 0:\n            z_vals_feed_fine = torch.cat([z_vals_fine, z_vals_outside], dim=-1)\n            z_vals_feed_fine, _ = torch.sort(z_vals_feed_fine, dim=-1)\n            ret_outside = self.render_core_outside(rays_o, rays_d, z_vals_feed_fine, sample_dist, self.nerf_outside)\n\n            background_sampled_color = ret_outside['sampled_color']\n            background_alpha = ret_outside['alpha']\n\n        # Render core\n        ret_fine = self.render_core(rays_o,\n                                    rays_d,\n                                    z_vals_fine,\n                                    sample_dist,\n                                    self.nerf_fine,\n                                    background_rgb=background_rgb,\n                                    background_alpha=background_alpha,\n                                    background_sampled_color=background_sampled_color)\n\n        color_coarse = ret_coarse['color']\n        color_fine = ret_fine['color']\n        weights = ret_fine['weights']\n        weights_sum = weights.sum(dim=-1, keepdim=True)\n\n        return {\n            'color_coarse': color_coarse,\n            'color_fine': color_fine,\n            'weight_sum': weights_sum,\n            'weight_max': torch.max(weights, dim=-1, keepdim=True)[0],\n            'weights': weights,\n        }\n\n    def extract_geometry(self, bound_min, bound_max, resolution, threshold=50.0):\n        ret = extract_geometry(bound_min, bound_max, resolution, threshold, lambda pts: self.nerf_fine(pts, torch.zeros_like(pts))[0])\n        return ret", ""]}
{"filename": "models/fields.py", "chunked_list": ["import logging\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom models.embedder import get_embedder, positional_encoding_c2f\n\nclass SDFNetwork(nn.Module):\n    def __init__(self,\n                 d_in,\n                 d_out,\n                 d_hidden,\n                 n_layers,\n                 skip_in=(4,),\n                 multires=0,\n                 bias=0.5,\n                 scale=1,\n                 geometric_init=True,\n                 weight_norm=True,\n                 activation='softplus',\n                 reverse_geoinit = False,\n                 use_emb_c2f = False,\n                 emb_c2f_start = 0.1,\n                 emb_c2f_end = 0.5):\n        super(SDFNetwork, self).__init__()\n\n        dims = [d_in] + [d_hidden for _ in range(n_layers)] + [d_out]\n\n        self.embed_fn_fine = None\n\n        self.multires = multires\n        if multires > 0:\n            embed_fn, input_ch = get_embedder(multires, input_dims=d_in, normalize=False)\n            self.embed_fn_fine = embed_fn\n            dims[0] = input_ch\n        logging.info(f'SDF input dimension: {dims[0]}')\n\n        self.num_layers = len(dims)\n        self.skip_in = skip_in\n        self.scale = scale\n        self.use_emb_c2f = use_emb_c2f\n        if self.use_emb_c2f:\n            self.emb_c2f_start = emb_c2f_start\n            self.emb_c2f_end = emb_c2f_end\n            logging.info(f\"Use coarse-to-fine embedding (Level: {self.multires}): [{self.emb_c2f_start}, {self.emb_c2f_end}]\")\n\n        self.alpha_ratio = 0.0\n\n        for l in range(0, self.num_layers - 1):\n            if l + 1 in self.skip_in:\n                out_dim = dims[l + 1] - dims[0]\n            else:\n                out_dim = dims[l + 1]\n\n            lin = nn.Linear(dims[l], out_dim)\n\n            if geometric_init:\n                if l == self.num_layers - 2:\n                    if reverse_geoinit:\n                        logging.info(f\"Geometry init: Indoor scene (reverse geometric init).\")\n                        torch.nn.init.normal_(lin.weight, mean=-np.sqrt(np.pi) / np.sqrt(dims[l]), std=0.0001)\n                        torch.nn.init.constant_(lin.bias, bias)\n                    else:\n                        logging.info(f\"Geometry init: DTU scene (not reverse geometric init).\")\n                        torch.nn.init.normal_(lin.weight, mean=np.sqrt(np.pi) / np.sqrt(dims[l]), std=0.0001)\n                        torch.nn.init.constant_(lin.bias, -bias)\n                elif multires > 0 and l == 0:\n                    torch.nn.init.constant_(lin.bias, 0.0)\n                    torch.nn.init.constant_(lin.weight[:, 3:], 0.0)\n                    torch.nn.init.normal_(lin.weight[:, :3], 0.0, np.sqrt(2) / np.sqrt(out_dim))\n                elif multires > 0 and l in self.skip_in:\n                    torch.nn.init.constant_(lin.bias, 0.0)\n                    torch.nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n                    torch.nn.init.constant_(lin.weight[:, -(dims[0] - 3):], 0.0)\n                else:\n                    torch.nn.init.constant_(lin.bias, 0.0)\n                    torch.nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n\n            if weight_norm:\n                lin = nn.utils.weight_norm(lin)\n\n            setattr(self, \"lin\" + str(l), lin)\n\n        if activation == 'softplus':\n            self.activation = nn.Softplus(beta=100)\n        else:\n            assert activation == 'relu'\n            self.activation = nn.ReLU()\n\n        self.weigth_emb_c2f = None\n        self.iter_step = 0\n        self.end_iter = 3e5\n\n    def forward(self, inputs):\n        inputs = inputs * self.scale\n\n        if self.use_emb_c2f and self.multires > 0:\n            inputs, weigth_emb_c2f = positional_encoding_c2f(inputs, self.multires, emb_c2f=[self.emb_c2f_start, self.emb_c2f_end], alpha_ratio = (self.iter_step / self.end_iter))\n            self.weigth_emb_c2f = weigth_emb_c2f\n        elif self.embed_fn_fine is not None:\n            inputs = self.embed_fn_fine(inputs)\n        else:\n            NotImplementedError\n\n        x = inputs\n        for l in range(0, self.num_layers - 1):\n            lin = getattr(self, \"lin\" + str(l))\n\n            if l in self.skip_in:\n                x = torch.cat([x, inputs], 1) / np.sqrt(2)\n\n            x = lin(x)\n\n            if l < self.num_layers - 2:\n                x = self.activation(x)\n        return torch.cat([x[:, :1] / self.scale, x[:, 1:]], dim=-1)\n\n    def sdf(self, x):\n        return self.forward(x)[:, :1]\n\n    def sdf_hidden_appearance(self, x):\n        return self.forward(x)\n\n    def gradient(self, x):\n        x.requires_grad_(True)\n        y = self.sdf(x)\n        d_output = torch.ones_like(y, requires_grad=False, device=y.device)\n        gradients = torch.autograd.grad(\n            outputs=y,\n            inputs=x,\n            grad_outputs=d_output,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True)[0]\n        return gradients.unsqueeze(1)", "\nclass FixVarianceNetwork(nn.Module):\n    def __init__(self, base):\n        super(FixVarianceNetwork, self).__init__()\n        self.base = base\n        self.iter_step = 0\n\n    def set_iter_step(self, iter_step):\n        self.iter_step = iter_step\n\n    def forward(self, x):\n        return torch.ones([len(x), 1]) * np.exp(-self.iter_step / self.base)", "\nclass SingleVarianceNetwork(nn.Module):\n    def __init__(self, init_val=1.0, use_fixed_variance = False):\n        super(SingleVarianceNetwork, self).__init__()\n        if use_fixed_variance:\n            logging.info(f'Use fixed variance: {init_val}')\n            self.variance = torch.tensor([init_val])\n        else:\n            self.register_parameter('variance', nn.Parameter(torch.tensor(init_val)))\n\n    def forward(self, x):\n        return torch.ones([len(x), 1]) * torch.exp(self.variance * 10.0)", "\nclass RenderingNetwork(nn.Module):\n    def __init__(\n            self,\n            d_feature,\n            mode,\n            d_in,\n            d_out,\n            d_hidden,\n            n_layers,\n            weight_norm=True,\n            multires_view=0,\n            squeeze_out=True,\n    ):\n        super().__init__()\n\n        self.mode = mode\n        self.squeeze_out = squeeze_out\n        dims = [d_in + d_feature] + [d_hidden for _ in range(n_layers)] + [d_out]\n\n        self.embedview_fn = None\n        if multires_view > 0:\n            embedview_fn, input_ch = get_embedder(multires_view)\n            self.embedview_fn = embedview_fn\n            dims[0] += (input_ch - 3)\n\n        self.num_layers = len(dims)\n\n        for l in range(0, self.num_layers - 1):\n            out_dim = dims[l + 1]\n            lin = nn.Linear(dims[l], out_dim)\n\n            if weight_norm:\n                lin = nn.utils.weight_norm(lin)\n\n            setattr(self, \"lin\" + str(l), lin)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, points, normals, view_dirs, feature_vectors):\n        if self.embedview_fn is not None:\n            view_dirs = self.embedview_fn(view_dirs)\n\n        rendering_input = None\n\n        if self.mode == 'idr':\n            rendering_input = torch.cat([points, view_dirs, normals, feature_vectors], dim=-1)\n        elif self.mode == 'no_view_dir':\n            rendering_input = torch.cat([points, normals, feature_vectors], dim=-1)\n        elif self.mode == 'no_normal':\n            rendering_input = torch.cat([points, view_dirs, feature_vectors], dim=-1)\n\n        x = rendering_input\n\n        for l in range(0, self.num_layers - 1):\n            lin = getattr(self, \"lin\" + str(l))\n\n            x = lin(x)\n\n            if l < self.num_layers - 2:\n                x = self.relu(x)\n\n        if self.squeeze_out:\n            x = torch.sigmoid(x)\n        return x", "\n\n# Code from nerf-pytorch\nclass NeRF(nn.Module):\n    def __init__(self, D=8, W=256, d_in=3, d_in_view=3, multires=0, multires_view=0, output_ch=4, skips=[4], use_viewdirs=False):\n        \"\"\"\n        \"\"\"\n        super(NeRF, self).__init__()\n        self.D = D\n        self.W = W\n        self.d_in = d_in\n        self.d_in_view = d_in_view\n        self.input_ch = 3\n        self.input_ch_view = 3\n        self.embed_fn = None\n        self.embed_fn_view = None\n\n        if multires > 0:\n            embed_fn, input_ch = get_embedder(multires, input_dims=d_in, normalize=False)\n            self.embed_fn = embed_fn\n            self.input_ch = input_ch\n\n        if multires_view > 0:\n            embed_fn_view, input_ch_view = get_embedder(multires_view, input_dims=d_in_view, normalize=False)\n            self.embed_fn_view = embed_fn_view\n            self.input_ch_view = input_ch_view\n\n        self.skips = skips\n        self.use_viewdirs = use_viewdirs\n\n        self.pts_linears = nn.ModuleList(\n            [nn.Linear(self.input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + self.input_ch, W) for i in\n                                        range(D - 1)])\n\n        ### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)\n        self.views_linears = nn.ModuleList([nn.Linear(self.input_ch_view + W, W // 2)])\n\n        ### Implementation according to the paper\n        # self.views_linears = nn.ModuleList(\n        #     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])\n\n        if use_viewdirs:\n            self.feature_linear = nn.Linear(W, W)\n            self.alpha_linear = nn.Linear(W, 1)\n            self.rgb_linear = nn.Linear(W // 2, 3)\n        else:\n            self.output_linear = nn.Linear(W, output_ch)\n\n    def forward(self, input_pts, input_views):\n        if self.embed_fn is not None:\n            input_pts = self.embed_fn(input_pts)\n        if self.embed_fn_view is not None:\n            input_views = self.embed_fn_view(input_views)\n\n        h = input_pts\n        for i, l in enumerate(self.pts_linears):\n            h = self.pts_linears[i](h)\n            h = F.relu(h)\n            if i in self.skips:\n                h = torch.cat([input_pts, h], -1)\n\n        if self.use_viewdirs:\n            alpha = self.alpha_linear(h)\n            feature = self.feature_linear(h)\n            h = torch.cat([feature, input_views], -1)\n\n            for i, l in enumerate(self.views_linears):\n                h = self.views_linears[i](h)\n                h = F.relu(h)\n\n            rgb = self.rgb_linear(h)\n            return alpha + 1.0, rgb\n        else:\n            assert False", ""]}
{"filename": "models/patch_match_cuda.py", "chunked_list": ["import cv2, logging, copy, os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torch\n\nimport utils.utils_io as IOUtils\nimport torch.nn.functional as F\n\ndef prepare_patches_src(img_gray, indices_pixel, window_size=11, window_step=2):\n    '''\n    Args:\n        img: H*W, gray image\n        indices_pixel: N*2\n        size_window: int, patch size\n        size_step: int, interval sampling\n    Return:\n        rgb_pathes: N*M*M, gray values of images\n    '''\n\n\n    assert img_gray.dtype == torch.float32\n        \n    size_img = img_gray.shape[:2][::-1]\n\n    # get kernel indices of a patch\n    window_size+=1\n    x = torch.arange(-(window_size//2), window_size//2 + 1, window_step) # weired -1//2=-1\n    xv, yv = torch.meshgrid(x,x)\n    kernel_patch = torch.stack([xv,yv], axis=-1) # M*M*2 \n    # print(kernel_patch)\n    num_pixels = len(indices_pixel)\n    indices_pixel = indices_pixel.to(kernel_patch.device)\n    kernel_patch = kernel_patch.to(indices_pixel.device)\n    indices_patch = indices_pixel.reshape(num_pixels,1,1,2) + kernel_patch\n    \n    # sample img\n    mask_indices_inside = is_inside_border(indices_patch.reshape(-1,2), size_img).reshape(indices_patch.shape[:3]) # N*M*M\n    indices_patch_inside = (indices_patch * mask_indices_inside[...,None]).reshape(-1,2)   # N*2. let the indices of outside pixels be zero\n    \n    if img_gray.ndim ==2:\n        rgb_patches = img_gray[indices_patch_inside[:, 0], indices_patch_inside[:, 1]].reshape(indices_patch.shape[:3]) # N*M*M\n    elif img_gray.ndim ==3:\n        rgb_patches = img_gray[indices_patch_inside[:, 0], indices_patch_inside[:, 1]].reshape(indices_patch.shape[:3] + tuple([3])) # N*M*M\n    else:\n        raise NotImplementedError\n\n    rgb_patches[mask_indices_inside==False] = -1\n    return rgb_patches.cuda(), indices_patch, mask_indices_inside", "\ndef prepare_patches_src(img_gray, indices_pixel, window_size=11, window_step=2):\n    '''\n    Args:\n        img: H*W, gray image\n        indices_pixel: N*2\n        size_window: int, patch size\n        size_step: int, interval sampling\n    Return:\n        rgb_pathes: N*M*M, gray values of images\n    '''\n\n\n    assert img_gray.dtype == torch.float32\n        \n    size_img = img_gray.shape[:2][::-1]\n\n    # get kernel indices of a patch\n    window_size+=1\n    x = torch.arange(-(window_size//2), window_size//2 + 1, window_step) # weired -1//2=-1\n    xv, yv = torch.meshgrid(x,x)\n    kernel_patch = torch.stack([xv,yv], axis=-1) # M*M*2 \n    # print(kernel_patch)\n    num_pixels = len(indices_pixel)\n    indices_pixel = indices_pixel.to(kernel_patch.device)\n    kernel_patch = kernel_patch.to(indices_pixel.device)\n    indices_patch = indices_pixel.reshape(num_pixels,1,1,2) + kernel_patch\n    \n    # sample img\n    mask_indices_inside = is_inside_border(indices_patch.reshape(-1,2), size_img).reshape(indices_patch.shape[:3]) # N*M*M\n    indices_patch_inside = (indices_patch * mask_indices_inside[...,None]).reshape(-1,2)   # N*2. let the indices of outside pixels be zero\n    \n    if img_gray.ndim ==2:\n        rgb_patches = img_gray[indices_patch_inside[:, 0], indices_patch_inside[:, 1]].reshape(indices_patch.shape[:3]) # N*M*M\n    elif img_gray.ndim ==3:\n        rgb_patches = img_gray[indices_patch_inside[:, 0], indices_patch_inside[:, 1]].reshape(indices_patch.shape[:3] + tuple([3])) # N*M*M\n    else:\n        raise NotImplementedError\n\n    rgb_patches[mask_indices_inside==False] = -1\n    return rgb_patches.cuda(), indices_patch, mask_indices_inside", "\ndef normalize_coords_vu(idx_vu, shape_img):\n    '''Normalize coords to interval [-1, 1]\n    Args:\n        idx_vu: shape [..., 2]\n    '''\n    assert idx_vu.dtype == torch.float32\n    H, W = shape_img\n\n    idx_vu_norm = idx_vu.clone().detach()\n    temp = 2 * idx_vu[...,0]  / (H - 1) - 1\n    \n    # convert vu to xy\n    idx_vu_norm[..., 1] = 2 * idx_vu[...,0]  / (H - 1) - 1\n    idx_vu_norm[..., 0] = 2 * idx_vu[...,1]  / (W - 1) - 1\n    return idx_vu_norm", "\ndef sample_patches(img, idx_patches_input, sampling_mode = 'nearest'): \n    '''\n    Args:\n        img: W*H or W*H*3\n        idx_patches_warp: N*M*M*2, warped indices\n    Return:\n        rgb_patches: N*M*M, sampled pixels\n    '''\n    # sample img\n    size_img = img.shape[:2][::-1]\n    shape_patches = idx_patches_input.shape\n\n    # TODO: Add inter-linear interploation of sampling\n    if idx_patches_input.dtype not in [torch.int, torch.int16, torch.int32, torch.int64]:\n        idx_patches_input_round = torch.round(idx_patches_input).long()\n        idx_patches = idx_patches_input_round\n    else:\n        # logging.info(f'Grid sampling')\n        idx_patches = idx_patches_input\n        sampling_mode = 'nearest'\n    \n    mask_indices_inside = is_inside_border(idx_patches.reshape(-1,2), size_img).reshape(idx_patches.shape[:-1]) # N*M*M\n    indices_patch_inside = (idx_patches * mask_indices_inside[...,None]).reshape(-1,2)   # N*2. let the indices of outside pixels be zero\n    \n    # grid sampling\n    if sampling_mode == 'grid_sample':\n        assert img.ndim == 2  # 1*1*H*W\n        idx_patches_input_norm = normalize_coords_vu(idx_patches_input, img.shape).clip(-1,1)\n        rgb_patches = F.grid_sample(img[None, None,:,:], idx_patches_input_norm.reshape(1,1,-1,2), align_corners=False).squeeze()\n        rgb_patches = rgb_patches.reshape(shape_patches[:3])\n    elif sampling_mode == 'nearest':\n        if img.ndim ==2:\n            rgb_patches = img[indices_patch_inside[:, 0], indices_patch_inside[:, 1]].reshape(idx_patches.shape[:-1]) # N*M*M\n        elif img.ndim ==3:\n            rgb_patches = img[indices_patch_inside[:, 0], indices_patch_inside[:, 1]].reshape(idx_patches.shape[:-1] + tuple([3])) # N*M*M\n        else:\n            raise NotImplementedError\n\n    rgb_patches[mask_indices_inside==False] = -1\n    return rgb_patches", "\ndef is_inside_border(ind_pixels, size_img):\n    '''\n    Args:\n        ind_pixel: N*2\n        size_img: (W,H)\n    Return:\n        mask: N*1, bool\n    '''\n    assert (ind_pixels.ndim ==2 & ind_pixels.shape[1] ==2)\n    W, H = size_img\n    return (ind_pixels[:, 0] >= 0) & (ind_pixels[:,0] < H) & \\\n           (ind_pixels[:, 1] >= 0) & (ind_pixels[:,1] < W)", "\ndef compute_NCC_score(patches_ref, patches_src):\n    '''\n    Args:\n        pathces_ref: N*M*M\n        patches_src: N*M*M\n    Return:\n        score: float\n    '''\n    num_patches = patches_ref.shape[0]\n    size_patch = (patches_ref.shape[1] * patches_ref.shape[2])\n\n    # ensure pixels inside border\n    mask_inside_ref, mask_inside_src = patches_ref >= 0, patches_src >= 0\n    mask_inside = mask_inside_ref & mask_inside_src # different for diffenert neighbors\n    \n    # ensure half patch inside border\n    mask_patches_valid = (mask_inside.reshape(num_patches, -1).sum(axis=-1) == size_patch)[...,None,None]\n    mask_inside = mask_inside & mask_patches_valid\n    # logging.info(f'Pixels inside ref and src: {mask_inside.sum()}/{mask_inside.size}; Ref: {mask_inside_ref.sum()}/{mask_inside_ref.size}; Src: {mask_inside_src.sum()}/{mask_inside_src.size}')\n\n    calculate_patches_mean = lambda patches, mask: (patches*mask).reshape(patches.shape[0], -1).sum(axis=1) / (mask.reshape(patches.shape[0], -1).sum(axis=1)+1e-6)\n    mean_patches_ref = calculate_patches_mean(patches_ref, mask_inside).reshape(-1,1,1)\n    mean_patches_src = calculate_patches_mean(patches_src, mask_inside).reshape(-1,1,1)\n    \n    normalized_patches_ref = patches_ref - mean_patches_ref\n    normalized_patches_src = patches_src - mean_patches_src\n    norm = ((normalized_patches_ref * normalized_patches_src)*mask_inside).reshape(num_patches,-1).sum(axis=-1)\n    denom = torch.sqrt( \n                (torch.square(normalized_patches_ref)*mask_inside).reshape(num_patches,-1).sum(axis=-1) * \\\n                (torch.square(normalized_patches_src)*mask_inside).reshape(num_patches,-1).sum(axis=-1)\n            )\n    norm = norm.clip(10, 1e6)\n    ncc = (norm + 20) / (denom+1e-3)\n    if (denom == 0).sum() > 0:\n        # check quadratic moments to avoid degeneration.\n        # pathes with same values \n        # [Nonlinear systems], https://en.wikipedia.org/wiki/Cross-correlation\n        mask_degenerade = ( denom == 0)\n        ncc[denom == 0] = 1.0\n        # logging.info(f'Deneraded patches: {mask_degenerade.sum()}/{mask_degenerade.size}.')  # pixels with invalid patchmatch\n    score = 1-ncc.clip(-1.0,1.0) # 0->2: smaller, better\n    diff_mean = torch.abs(mean_patches_ref - mean_patches_src).squeeze()\n    return score, diff_mean, mask_patches_valid.squeeze()", "\ndef update_confmap(confmap, idx_pixels, ncc_score, idx_patches = None):\n    '''\n    Args:\n        confmap: H*W\n        idx_pixels: N*2, numpy array index\n        ncc_score: N*1,\n        idx_patches: None. (optional: N*M*M*2) \n    Return:\n        updated_confmap: H*W\n    '''\n    # ncc_score = ncc_score[...,None, None]* np.ones_like(mask_inside_ref)\n     \n    # indices_inside = indices_patch[mask_inside]\n    # score_inside = ncc_score[mask_inside]\n    confmap = copy.deepcopy(confmap)\n    prev_conf = confmap[idx_pixels[:,0], idx_pixels[:,1]]\n    confmap[idx_pixels[:,0], idx_pixels[:,1]] = ncc_score  # TODO: check duplicated values here\n    return confmap", "    \ndef prepare_neigbor_pixels(img, indices_pixel):\n    '''Get 4 neighbor indices of given pixels\n    Args:\n        img: H*W, ndarray\n        indices_pixel: N*2\n    Return:\n        indices_neighbors: N*4*2\n    '''\n    raise NotImplementedError", "\ndef sample_pixels_by_probability(prob_pixels_all, batch_size, clip_range = (0.22, 0.66), shuffle = True):\n    '''Sample pixels based on NCC confidence. Sample more pixels on low confidence areas\n    Args:\n        scores_ncc: W*H, narray\n    Return:\n        uv_sample: 2*N\n    '''\n    # Guarantee that every pixels can be sampled\n    prob = prob_pixels_all.clip(*clip_range) \n\n    H,W = prob.shape\n    sample_range = W * H\n    prob = prob / prob.sum()\n    \n    samples_indices = np.random.choice(sample_range, batch_size, p=prob.reshape(-1))\n    pixels_v = samples_indices // W\n    pixels_u = samples_indices % W\n\n    uv_sample = np.stack((pixels_u, pixels_v), axis=-1) #.transpose()\n    if shuffle:\n        # uv_sample = np.copy(uv_sample)\n        np.random.shuffle(uv_sample)\n    uv_sample = uv_sample.transpose()\n    return uv_sample", "\ndef visualize_sampled_pixels(img_input, indices_pixel, path = None):\n    '''\n    Two different format of coordinates:\n        (1) indices_pixel: in vu format\n        (2) uvs_pixel: in opencv format\n    Args:\n        indices_pixel: N*2\n    '''\n    img = copy.deepcopy(img_input)\n    for i in range(indices_pixel.shape[0]):\n        cv2.circle(img, (indices_pixel[i, 1], indices_pixel[i,0]), \n                        radius = 2, \n                        color = (255,0,0), # BGR\n                        thickness=2)  \n        cv2.putText(img, f'{i}',  (indices_pixel[i, 1] + 2 if indices_pixel[i, 1] + 2 < img.shape[1] else img.shape[1]-1, indices_pixel[i,0]), \n                        fontFace= cv2.FONT_HERSHEY_SIMPLEX, \n                        fontScale = 0.75, \n                        color = (0, 0, 255), \n                        thickness = 1) # int\n    if path is not None:\n        cv2.imwrite(path, img)\n    return img", "\ndef visualize_samples_lis(img_input, lis_samples_uv, path = None):\n    '''\n    Two different format of coordinates:\n        (1) indices_pixel: in numpy format\n        (2) uvs_pixel: in opencv format\n    Args:\n        lis_samples: M*N*2\n    '''\n    img = copy.deepcopy(img_input)\n    lis_colors = [(0,0,255), (255,0,0),(0,255,0)] # R, G, B, in cv color mode\n\n    assert len(lis_samples_uv) <= len(lis_colors)\n    for idx_samples in range(len(lis_samples_uv)):\n        samples_curr = lis_samples_uv[idx_samples]\n        for i in range(samples_curr.shape[0]):\n            cv2.circle(img, (samples_curr[i, 0], samples_curr[i,1]), \n                            radius = 2, \n                            color = lis_colors[idx_samples], # BGR\n                            thickness=2)  \n        #     cv2.putText(img, f'{i}',  (indices_pixel[i, 1] + 2 if indices_pixel[i, 1] + 2 < img.shape[1] else img.shape[1]-1, indices_pixel[i,0]), \n        #                     fontFace= cv2.FONT_HERSHEY_SIMPLEX, \n        #                     fontScale = 0.75, \n        #                     color = (0, 0, 255), \n        #                     thickness = 2) # int\n    if path is not None:\n        dir, _, _ = IOUtils.get_path_components(path)\n        IOUtils.ensure_dir_existence(dir)\n        cv2.imwrite(path, img)\n    return img", "\ndef save_patches(path, patches, interval = 2, color_mode = 'GRAY'):\n    num_patches, w_p, h_p = patches.shape[:3]\n    w_img = int( np.ceil(np.sqrt(num_patches)) * (w_p+interval) )\n    h_img = int( np.ceil(np.sqrt(num_patches)) * (h_p+ interval) )\n\n    img = np.zeros((h_img, w_img))\n    if patches.shape[-1] ==3:\n        img = np.zeros((h_img, w_img, 3))\n    patch_num_row = int(np.ceil(np.sqrt(num_patches)))\n    for i in range(patch_num_row):\n        for j in range(patch_num_row):\n            idx_patch = i*patch_num_row+j\n            if idx_patch >= num_patches:\n                continue\n            img[i*(w_p+interval):(i+1)*(w_p+interval)-interval,j*(h_p+interval):(j+1)*(h_p+interval)-interval] = patches[idx_patch]\n    cv2.imwrite(path, img)    \n    return img", "\ndef concatenate_images(img1, img2, path = None):\n    check_channel = lambda img : np.stack([img]*3, axis=-1) if img.ndim ==2 else img\n    img1 = check_channel(img1)\n    img2 = check_channel(img2)\n    img_cat = np.concatenate([img1, img2], axis=0)\n    if path is not None:\n        cv2.imwrite(path, img_cat)\n    return img_cat\n\ndef compute_homography(pt_ref, n_ref, K, extrin_ref, extrin_src):\n    '''Compute homography from reference view to source view.\n    Tranform from world to reference view coordinates.\n\n    Args:\n        n: N*3*1, reference view normal\n        pt: N*3*1, reference view coordinates of a point\n\n        K: 3*3, intrinsics\n        extrin_ref: N*4*4\n        extrin_src: N*4*4\n    Return:\n        mat_homography: N*3*3 \n    '''\n    # TODO: check this method later\n    if False:\n        def decompose_extrin(extrin):\n            rot = extrin[:3,:3]\n            trans = extrin[:3,3].reshape(3,1)\n            cam_center = - np.linalg.inv(rot) @ trans\n            return rot, cam_center\n        \n        R_ref, C_ref = decompose_extrin(extrin_ref)\n        R_src, C_src = decompose_extrin(extrin_src)\n\n        # rt = R_ref.transpose()\n        Hl = K @ R_src @ R_ref.transpose()\n        Hm = K @ R_src @ (C_ref - C_src)\n        Hr = np.linalg.inv(K)\n\n        num_pts = len(pt)\n        denom = (n * pt).sum(axis=-1, keepdims=True)[0].reshape(1, 1,1)\n        mat_homography = ( Hl[None, ...] + Hm[None, ...] @ (n.reshape(num_pts, 1,3) / denom) ) *Hr[None, ...];\n\n    ref_pose = torch.linalg.inv(extrin_ref)\n    inv_src_pose = extrin_src\n    inv_ref_pose = extrin_ref\n\n    relative_proj = inv_src_pose @ ref_pose\n    R_rel = relative_proj[:3, :3]\n    t_rel = relative_proj[:3, 3:]\n    R_ref = inv_ref_pose[:3, :3]\n    t_ref = inv_ref_pose[:3, 3:]\n\n    num_pts = len(pt_ref)\n    d = (n_ref * pt_ref).sum(axis=-1, keepdims=True).reshape(num_pts, 1,1)\n    mat_homography = (K[None, ...] @ (R_rel[None, ...] + t_rel[None,...] @ n_ref.reshape(num_pts,1,3) / d))@torch.linalg.inv(K)\n    return mat_homography", "\ndef compute_homography(pt_ref, n_ref, K, extrin_ref, extrin_src):\n    '''Compute homography from reference view to source view.\n    Tranform from world to reference view coordinates.\n\n    Args:\n        n: N*3*1, reference view normal\n        pt: N*3*1, reference view coordinates of a point\n\n        K: 3*3, intrinsics\n        extrin_ref: N*4*4\n        extrin_src: N*4*4\n    Return:\n        mat_homography: N*3*3 \n    '''\n    # TODO: check this method later\n    if False:\n        def decompose_extrin(extrin):\n            rot = extrin[:3,:3]\n            trans = extrin[:3,3].reshape(3,1)\n            cam_center = - np.linalg.inv(rot) @ trans\n            return rot, cam_center\n        \n        R_ref, C_ref = decompose_extrin(extrin_ref)\n        R_src, C_src = decompose_extrin(extrin_src)\n\n        # rt = R_ref.transpose()\n        Hl = K @ R_src @ R_ref.transpose()\n        Hm = K @ R_src @ (C_ref - C_src)\n        Hr = np.linalg.inv(K)\n\n        num_pts = len(pt)\n        denom = (n * pt).sum(axis=-1, keepdims=True)[0].reshape(1, 1,1)\n        mat_homography = ( Hl[None, ...] + Hm[None, ...] @ (n.reshape(num_pts, 1,3) / denom) ) *Hr[None, ...];\n\n    ref_pose = torch.linalg.inv(extrin_ref)\n    inv_src_pose = extrin_src\n    inv_ref_pose = extrin_ref\n\n    relative_proj = inv_src_pose @ ref_pose\n    R_rel = relative_proj[:3, :3]\n    t_rel = relative_proj[:3, 3:]\n    R_ref = inv_ref_pose[:3, :3]\n    t_ref = inv_ref_pose[:3, 3:]\n\n    num_pts = len(pt_ref)\n    d = (n_ref * pt_ref).sum(axis=-1, keepdims=True).reshape(num_pts, 1,1)\n    mat_homography = (K[None, ...] @ (R_rel[None, ...] + t_rel[None,...] @ n_ref.reshape(num_pts,1,3) / d))@torch.linalg.inv(K)\n    return mat_homography", "\ndef warp_patches(idx_patches_xy, homography):\n    '''Warp patches from reference image to source images\n    Args:\n        idx_patches: N*M*M*2, xy index coordinates (numpy array)\n        homography: N*3*3, from reference to source\n    Return:\n        idx_patches_warp: N*M*M*2, xy index coordinates (numpy array)\n    '''\n    shape = idx_patches_xy.shape\n    idx_patches = convert_xy_to_uv(idx_patches_xy)\n    idx_patches_homo = convert_to_homo(idx_patches)\n    num_patches, H_patch, W_patch = homography.shape\n    \n    if idx_patches_xy.ndim == 2:    #N*2\n        idx_patches_warp = (homography @ idx_patches_homo[...,None]).squeeze() # N*3*1\n    elif idx_patches_xy.ndim == 4:  #N*M*M*2\n        idx_patches_warp = (homography.reshape(num_patches, 1, 1, H_patch, W_patch) @ idx_patches_homo[...,None]).squeeze() # N*M*M*3*1\n    else:\n        raise NotImplementedError\n\n    idx_patches_warp = (idx_patches_warp / idx_patches_warp[..., 2][...,None])[...,:2]\n\n    # TODO: update later fpr faster calculation. Refer to NeuralWarp\n    # einsum is 30 times faster\n    # tmp = (H.view(Nsrc, N, -1, 1, 3, 3) @ hom_uv.view(1, N, 1, -1, 3, 1)).squeeze(-1).view(Nsrc, -1, 3)\n    # tmp = torch.einsum(\"vprik,pok->vproi\", H, hom_uv).reshape(Nsrc, -1, 3)\n    # patches_warp = patches_warp / patches_warp[..., 2:].clip(min=1e-8)\n\n    # TODO: use the stragety of openMVS to simplify calculatoin. O(N*h^2)->O(H+h^2)\n    # homography_step = homography*window_step\n    # kernel_homography = \n    idx_patches_warp_xy = convert_uv_to_xy(idx_patches_warp)\n    return idx_patches_warp_xy", "\ndef denoise_image(img):\n    b,g,r = cv2.split(img)           # get b,g,r\n    rgb_img = cv2.merge([r,g,b])     # switch it to rgb\n\n    # img = cv2.fastNlMeansDenoisingColored(img.astype(np.uint8),None,10,10,7,21)\n\n    # b,g,r = cv2.split(dst)           # get b,g,r\n    # rgb_dst = cv2.merge([r,g,b])     # switch it to rgb\n    return img", "\ndef convert_uv_to_xy(coord_uv):\n    '''\n    Args:\n        coord: N*2\n    '''\n    coord_xy = torch.stack([coord_uv[...,1], coord_uv[...,0]], axis=-1)\n    return coord_xy\n\ndef convert_xy_to_uv(coord_xy):\n    '''\n    Args:\n        coord: N*2\n    '''\n    coord_uv = torch.stack([coord_xy[...,1], coord_xy[...,0]], axis=-1)\n    return coord_uv", "\ndef convert_xy_to_uv(coord_xy):\n    '''\n    Args:\n        coord: N*2\n    '''\n    coord_uv = torch.stack([coord_xy[...,1], coord_xy[...,0]], axis=-1)\n    return coord_uv\n\ndef convert_to_homo(pts):\n    device = torch.device(\"cuda:0\")\n    pts = pts.to(device)\n    pts_homo = torch.cat([pts, torch.ones(pts.shape[:-1] + tuple([1])) ], dim=-1)\n    return pts_homo", "\ndef convert_to_homo(pts):\n    device = torch.device(\"cuda:0\")\n    pts = pts.to(device)\n    pts_homo = torch.cat([pts, torch.ones(pts.shape[:-1] + tuple([1])) ], dim=-1)\n    return pts_homo\n\n"]}
{"filename": "models/renderer.py", "chunked_list": ["import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport mcubes\n\ndef extract_fields(bound_min, bound_max, resolution, query_func):\n    N = 64\n    X = torch.linspace(bound_min[0], bound_max[0], resolution).split(N)\n    Y = torch.linspace(bound_min[1], bound_max[1], resolution).split(N)\n    Z = torch.linspace(bound_min[2], bound_max[2], resolution).split(N)\n\n    u = np.zeros([resolution, resolution, resolution], dtype=np.float32)\n    with torch.no_grad():\n        for xi, xs in enumerate(X):\n            for yi, ys in enumerate(Y):\n                for zi, zs in enumerate(Z):\n                    xx, yy, zz = torch.meshgrid(xs, ys, zs)\n                    pts = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1), zz.reshape(-1, 1)], dim=-1)\n                    val = query_func(pts).reshape(len(xs), len(ys), len(zs)).detach().cpu().numpy()\n                    u[xi * N: xi * N + len(xs), yi * N: yi * N + len(ys), zi * N: zi * N + len(zs)] = val\n    return u", "\n\ndef extract_geometry(bound_min, bound_max, resolution, threshold, query_func):\n    u = extract_fields(bound_min, bound_max, resolution, query_func)\n    vertices, triangles = mcubes.marching_cubes(u, threshold)\n    b_max_np = bound_max.detach().cpu().numpy()\n    b_min_np = bound_min.detach().cpu().numpy()\n\n    vertices = vertices / (resolution - 1.0) * (b_max_np - b_min_np)[None, :] + b_min_np[None, :]\n    return vertices, triangles, u", "\n\ndef sample_pdf(bins, weights, n_samples, det=False):\n    # This implementation is from NeRF\n    # Get pdf\n    weights = weights + 1e-5  # prevent nans\n    pdf = weights / torch.sum(weights, -1, keepdim=True)\n    cdf = torch.cumsum(pdf, -1)\n    cdf = torch.cat([torch.zeros_like(cdf[..., :1]), cdf], -1)\n    # Take uniform samples\n    if det:\n        u = torch.linspace(0. + 0.5 / n_samples, 1. - 0.5 / n_samples, steps=n_samples)\n        u = u.expand(list(cdf.shape[:-1]) + [n_samples])\n    else:\n        u = torch.rand(list(cdf.shape[:-1]) + [n_samples])\n\n    # Invert CDF\n    u = u.contiguous()\n    inds = torch.searchsorted(cdf, u, right=True)\n    below = torch.max(torch.zeros_like(inds - 1), inds - 1)\n    above = torch.min((cdf.shape[-1] - 1) * torch.ones_like(inds), inds)\n    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n\n    denom = (cdf_g[..., 1] - cdf_g[..., 0])\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t = (u - cdf_g[..., 0]) / denom\n    samples = bins_g[..., 0] + t * (bins_g[..., 1] - bins_g[..., 0])\n\n    return samples", "\n\nclass NeuSRenderer:\n    def __init__(self,\n                 nerf,\n                 sdf_network_fine,\n                 variance_network_fine,\n                 color_network_fine,\n                 n_samples,\n                 n_importance,\n                 n_outside,\n                 perturb,\n                 alpha_type='div'):\n        self.nerf = nerf\n        self.sdf_network_fine = sdf_network_fine\n        self.variance_network_fine = variance_network_fine\n        self.color_network_fine = color_network_fine\n        self.n_samples = n_samples\n        self.n_importance = n_importance\n        self.n_outside = n_outside\n        self.perturb = perturb\n        self.alpha_type = alpha_type\n        self.radius = 1.0\n\n    def render_core_outside(self, rays_o, rays_d, z_vals, sample_dist, nerf, background_rgb=None):\n        batch_size, n_samples = z_vals.shape\n\n        dists = z_vals[..., 1:] - z_vals[..., :-1]\n        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape)], -1)\n\n        mid_z_vals = z_vals + dists * 0.5\n        mid_dists = mid_z_vals[..., 1:] - mid_z_vals[..., :-1]\n        mid_dists = torch.cat([mid_dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape)], -1)\n\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # batch_size, n_samples, 3\n\n        if self.n_outside > 0:\n            dis_to_center = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).clip(1.0, 1e10)\n            pts = torch.cat([pts / dis_to_center, 1.0 / dis_to_center], dim=-1)       # batch_size, n_samples, 4\n\n        dirs = rays_d[:, None, :].expand(batch_size, n_samples, 3)\n\n        pts = pts.reshape(-1, 3 + int(self.n_outside > 0))\n        dirs = dirs.reshape(-1, 3)\n\n        density, sampled_color = nerf(pts, dirs)\n        alpha = 1.0 - torch.exp(-F.softplus(density.reshape(batch_size, n_samples)) * dists)\n        alpha = alpha.reshape(batch_size, n_samples)\n        weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]  # n_rays, n_samples\n        sampled_color = sampled_color.reshape(batch_size, n_samples, 3)\n        color = (weights[:, :, None] * sampled_color).sum(dim=1)\n        if background_rgb is not None:\n            color = color + background_rgb * (1.0 - weights.sum(dim=-1, keepdim=True))\n\n        return {\n            'color': color,\n            'sampled_color': sampled_color,\n            'alpha': alpha,\n            'weights': weights,\n        }\n\n\n    def up_sample(self, rays_o, rays_d, z_vals, sdf, n_importance, inv_variance):\n        batch_size, n_samples = z_vals.shape\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * z_vals[..., :, None]  # n_rays, n_samples, 3\n        radius = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=False)\n        inside_sphere = (radius[:, :-1] < 1.0) | (radius[:, 1:] < 1.0)\n        sdf = sdf.reshape(batch_size, n_samples)\n        prev_sdf, next_sdf = sdf[:, :-1], sdf[:, 1:]\n        prev_z_vals, next_z_vals = z_vals[:, :-1], z_vals[:, 1:]\n        mid_sdf = (prev_sdf + next_sdf) * 0.5\n        dot_val = None\n        if self.alpha_type == 'uniform':\n            dot_val = torch.ones([batch_size, n_samples - 1]) * -1.0\n        else:\n            dot_val = (next_sdf - prev_sdf) / (next_z_vals - prev_z_vals + 1e-5)\n            prev_dot_val = torch.cat([torch.zeros([batch_size, 1]), dot_val[:, :-1]], dim=-1)\n            dot_val = torch.stack([prev_dot_val, dot_val], dim=-1)\n            dot_val, _ = torch.min(dot_val, dim=-1, keepdim=False)\n            dot_val = dot_val.clip(-10.0, 0.0) * inside_sphere\n        dist = (next_z_vals - prev_z_vals)\n        prev_esti_sdf = mid_sdf - dot_val * dist * 0.5\n        next_esti_sdf = mid_sdf + dot_val * dist * 0.5\n        prev_cdf = torch.sigmoid(prev_esti_sdf * inv_variance)\n        next_cdf = torch.sigmoid(next_esti_sdf * inv_variance)\n        alpha = (prev_cdf - next_cdf + 1e-5) / (prev_cdf + 1e-5)\n        weights = alpha * torch.cumprod(\n            torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]\n\n        z_samples = sample_pdf(z_vals, weights, n_importance, det=True).detach()\n        return z_samples\n\n    def cat_z_vals(self, rays_o, rays_d, z_vals, new_z_vals, sdf):\n        batch_size, n_samples = z_vals.shape\n        _, n_importance = new_z_vals.shape\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * new_z_vals[..., :, None]\n        new_sdf = self.sdf_network_fine.sdf(pts.reshape(-1, 3)).reshape(batch_size, n_importance)\n        z_vals = torch.cat([z_vals, new_z_vals], dim=-1)\n        sdf = torch.cat([sdf, new_sdf], dim=-1)\n        z_vals, index = torch.sort(z_vals, dim=-1)\n        xx = torch.arange(batch_size)[:, None].expand(batch_size, n_samples + n_importance).reshape(-1)\n        index = index.reshape(-1)\n        sdf = sdf[(xx, index)].reshape(batch_size, n_samples + n_importance)\n        return z_vals, sdf\n\n    def render_core(self,\n                    rays_o,\n                    rays_d,\n                    z_vals,\n                    sample_dist,\n                    sdf_network,\n                    variance_network,\n                    color_network,\n                    background_alpha=None,\n                    background_sampled_color=None,\n                    background_rgb=None,\n                    alpha_inter_ratio=0.0):\n        logs_summary = {}\n\n        batch_size, n_samples = z_vals.shape\n        dists = z_vals[..., 1:] - z_vals[..., :-1]\n        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape)], -1)\n\n        mid_z_vals = z_vals + dists * 0.5\n        mid_dists = mid_z_vals[..., 1:] - mid_z_vals[..., :-1]\n\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # n_rays, n_samples, 3\n        dirs = rays_d[:, None, :].expand(pts.shape)\n\n        pts = pts.reshape(-1, 3)\n        dirs = dirs.reshape(-1, 3)\n        \n        sdf_nn_output = sdf_network(pts)\n        sdf = sdf_nn_output[:, :1]\n        feature_vector = sdf_nn_output[:, 1:]\n\n        gradients = sdf_network.gradient(pts).squeeze()\n        sampled_color = color_network(pts, gradients, dirs, feature_vector).reshape(batch_size, n_samples, 3)\n\n        inv_variance = variance_network(torch.zeros([1, 3]))[:, :1].clip(1e-6, 1e6)\n        inv_variance = inv_variance.expand(batch_size * n_samples, 1)\n        \n        true_dot_val = (dirs * gradients).sum(-1, keepdim=True)\n\n        iter_cos = -(F.relu(-true_dot_val * 0.5 + 0.5) * (1.0 - alpha_inter_ratio) + F.relu(-true_dot_val) * alpha_inter_ratio) # always non-positive\n\n        true_estimate_sdf_half_next = sdf + iter_cos.clip(-10.0, 10.0) * dists.reshape(-1, 1) * 0.5\n        true_estimate_sdf_half_prev = sdf - iter_cos.clip(-10.0, 10.0) * dists.reshape(-1, 1) * 0.5\n\n        prev_cdf = torch.sigmoid(true_estimate_sdf_half_prev * inv_variance)\n        next_cdf = torch.sigmoid(true_estimate_sdf_half_next * inv_variance)\n\n        p = prev_cdf - next_cdf\n        c = prev_cdf\n\n        if self.alpha_type == 'div':\n            alpha = ((p + 1e-5) / (c + 1e-5)).reshape(batch_size, n_samples).clip(0.0, 1.0)\n        elif self.alpha_type == 'uniform':\n            uniform_estimate_sdf_half_next = sdf - dists.reshape(-1, 1) * 0.5\n            uniform_estimate_sdf_half_prev = sdf + dists.reshape(-1, 1) * 0.5\n            uniform_prev_cdf = torch.sigmoid(uniform_estimate_sdf_half_prev * inv_variance)\n            uniform_next_cdf = torch.sigmoid(uniform_estimate_sdf_half_next * inv_variance)\n            uniform_alpha = F.relu(\n                (uniform_prev_cdf - uniform_next_cdf + 1e-5) / (uniform_prev_cdf + 1e-5)).reshape(\n                batch_size, n_samples).clip(0.0, 1.0)\n            alpha = uniform_alpha\n        else:\n            assert False\n\n        pts_radius = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).reshape(batch_size, n_samples)\n        inside_sphere = (pts_radius < 1.0*self.radius).float().detach()\n        relax_inside_sphere = (pts_radius < 1.2*self.radius).float().detach()\n\n        if background_alpha is not None:   # render with background\n            alpha = alpha * inside_sphere + background_alpha[:, :n_samples] * (1.0 - inside_sphere)\n            alpha = torch.cat([alpha, background_alpha[:, n_samples:]], dim=-1)\n            sampled_color = sampled_color * inside_sphere[:, :, None] + background_sampled_color[:, :n_samples] * (1.0 - inside_sphere)[:, :, None]\n            sampled_color = torch.cat([sampled_color, background_sampled_color[:, n_samples:]], dim=1)\n        \n        weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]  # n_rays, n_samples\n        weights_sum = weights.sum(dim=-1, keepdim=True)\n\n        color = (sampled_color * weights[:, :, None]).sum(dim=1)\n        if background_rgb is not None:\n            color = color + background_rgb * (1.0 - weights_sum)\n\n        gradient_error = (torch.linalg.norm(gradients.reshape(batch_size, n_samples, 3), ord=2,\n                                            dim=-1) - 1.0) ** 2\n        gradient_error = (relax_inside_sphere * gradient_error).sum() / (relax_inside_sphere.sum() + 1e-5)\n\n        variance = (1.0 /inv_variance).mean(dim=-1, keepdim=True)\n        assert (torch.isinf(variance).any() == False)\n        assert (torch.isnan(variance).any() == False)\n        \n        depth = (mid_z_vals * weights[:, :n_samples]).sum(dim=1, keepdim=True)\n        depth_varaince = ((mid_z_vals - depth) ** 2 * weights[:, :n_samples]).sum(dim=-1, keepdim=True)\n\n        normal = (gradients.reshape(batch_size, n_samples, 3) * weights[:, :n_samples].reshape(batch_size, n_samples, 1)).sum(dim=1)\n\n        # visualize embedding weights\n        if sdf_network.weigth_emb_c2f != None:\n            # print(sdf_network.weigth_emb_c2f)\n            for id_w in range(len(sdf_network.weigth_emb_c2f)):\n                logs_summary[f'weigth_emb_c2f/level{id_w+1}'] = sdf_network.weigth_emb_c2f[id_w].detach()\n       \n        with torch.no_grad():\n            if inv_variance[0, 0] > 800:\n                # logging.info(f\"Use default inv-variance to calculate peak value\")\n                depth_peak = depth.clone()\n                normal_peak = normal.clone()\n                color_peak = color.clone()\n                point_peak = rays_o + rays_d*depth\n            else:\n                # Reset a large inv-variance to get better peak value\n                inv_variance2 = torch.tensor([800])\n                inv_variance2 = inv_variance2.expand(batch_size * n_samples, 1)\n                prev_cdf2 = torch.sigmoid(true_estimate_sdf_half_prev * inv_variance2)\n                next_cdf2 = torch.sigmoid(true_estimate_sdf_half_next * inv_variance2)\n\n                p2 = prev_cdf2 - next_cdf2\n                c2 = prev_cdf2\n                alpha2 = ((p2 + 1e-5) / (c2 + 1e-5)).reshape(batch_size, n_samples).clip(0.0, 1.0)\n                weights2 = alpha2 * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha2 + 1e-7], -1), -1)[:, :-1]  # n_rays, n_samples\n        \n                depth_peak = (mid_z_vals * weights2[:, :n_samples]).sum(dim=1, keepdim=True)\n                normal_peak = (gradients.reshape(batch_size, n_samples, 3) * weights2[:, :n_samples].reshape(batch_size, n_samples, 1)).sum(dim=1)\n                color_peak = (sampled_color[:, :n_samples] * weights2[:, :n_samples, None]).sum(dim=1)\n                point_peak = rays_o + rays_d*depth_peak\n\n        return {\n            'variance': variance,\n            'variance_inv_pts': inv_variance.reshape(batch_size, n_samples),\n            'depth': depth,\n            'depth_variance': depth_varaince,\n            'normal': normal,\n            'color_fine': color,\n            'cdf_fine': c.reshape(batch_size, n_samples),\n            'sdf': sdf,\n            'dists': dists,\n            'mid_z_vals': mid_z_vals,\n            'gradients': gradients.reshape(batch_size, n_samples, 3),\n            'gradient_error_fine': gradient_error,\n            'weights': weights,\n            'weight_sum': weights.sum(dim=-1, keepdim=True),\n            'weight_max': torch.max(weights, dim=-1, keepdim=True)[0],\n            'inside_sphere': inside_sphere,\n            'depth_peak': depth_peak,\n            'normal_peak': normal_peak,\n            'color_peak': color_peak,\n            'point_peak': point_peak\n        }, logs_summary\n\n    def render(self, rays_o, rays_d, near, far, perturb_overwrite=-1, background_rgb=None, alpha_inter_ratio=0.0):\n        batch_size = len(rays_o)\n        sphere_diameter = torch.abs(far-near).mean()\n        sample_dist = sphere_diameter / self.n_samples\n        z_vals = torch.linspace(0.0, 1.0, self.n_samples)\n        z_vals = near + (far - near) * z_vals[None, :]\n\n        z_vals_outside = None\n        if self.n_outside > 0:\n            z_vals_outside = torch.linspace(1e-3, 1.0 - 1.0 / (self.n_outside + 1.0), self.n_outside)\n\n        n_samples = self.n_samples\n        perturb = self.perturb\n\n        if perturb_overwrite >= 0:\n            perturb = perturb_overwrite\n        if perturb > 0:\n            # get intervals between samples\n            t_rand = (torch.rand([batch_size, 1]) - 0.5)\n            z_vals = z_vals + t_rand * 2.0 / self.n_samples\n\n            if self.n_outside > 0:\n                mids = .5 * (z_vals_outside[..., 1:] + z_vals_outside[..., :-1])\n                upper = torch.cat([mids, z_vals_outside[..., -1:]], -1)\n                lower = torch.cat([z_vals_outside[..., :1], mids], -1)\n                # stratified samples in those intervals\n                t_rand = torch.rand([batch_size, z_vals_outside.shape[-1]])\n                z_vals_outside = lower[None, :] + (upper - lower)[None, :] * t_rand\n\n        if self.n_outside > 0:\n            z_vals_outside = far / torch.flip(z_vals_outside, dims=[-1]) + 1.0 / self.n_samples\n\n        background_alpha = None\n        background_sampled_color = None\n\n        # Up sample\n        if self.n_importance > 0:\n            with torch.no_grad():\n                pts = rays_o[:, None, :] + rays_d[:, None, :] * z_vals[..., :, None]\n                sdf = self.sdf_network_fine.sdf(pts.reshape(-1, 3)).reshape(batch_size, self.n_samples)\n\n                n_steps = 4\n                for i in range(n_steps):\n                    new_z_vals = self.up_sample(rays_o, rays_d, z_vals, sdf, self.n_importance // n_steps, 64 * 2**i)\n                    z_vals, sdf = self.cat_z_vals(rays_o, rays_d, z_vals, new_z_vals, sdf)\n\n            n_samples = self.n_samples + self.n_importance\n\n        # Background\n        if self.n_outside > 0:\n            z_vals_feed = torch.cat([z_vals, z_vals_outside], dim=-1)\n            z_vals_feed, _ = torch.sort(z_vals_feed, dim=-1)\n            ret_outside = self.render_core_outside(rays_o, rays_d, z_vals_feed, sample_dist, self.nerf)\n\n            background_sampled_color = ret_outside['sampled_color']\n            background_alpha = ret_outside['alpha']\n\n        # Render\n        ret_fine, logs_summary = self.render_core(rays_o,\n                                    rays_d,\n                                    z_vals,\n                                    sample_dist,\n                                    self.sdf_network_fine,\n                                    self.variance_network_fine,\n                                    self.color_network_fine,\n                                    background_rgb=background_rgb,\n                                    background_alpha=background_alpha,\n                                    background_sampled_color=background_sampled_color,\n                                    alpha_inter_ratio=alpha_inter_ratio)\n        return ret_fine, logs_summary\n\n    def extract_geometry(self, bound_min, bound_max, resolution, threshold=0.0):\n        ret = extract_geometry(bound_min, bound_max, resolution, threshold, lambda pts: -self.sdf_network_fine.sdf(pts))\n        return ret", "        \nclass NeRFRenderer:\n    def __init__(self,\n                 nerf_coarse,\n                 nerf_fine,\n                 nerf_outside,\n                 n_samples,\n                 n_importance,\n                 n_outside,\n                 perturb):\n        self.nerf_coarse = nerf_coarse\n        self.nerf_fine = nerf_fine\n        self.nerf_outside = nerf_outside\n        self.n_samples = n_samples\n        self.n_importance = n_importance\n        self.n_outside = n_outside\n        self.perturb = perturb\n\n    def render_core_coarse(self, rays_o, rays_d, z_vals, sample_dist, nerf, background_rgb=None):\n        batch_size, n_samples = z_vals.shape\n\n        dists = z_vals[..., 1:] - z_vals[..., :-1]\n        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape)], -1)\n\n        mid_z_vals = z_vals + dists * 0.5\n\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # batch_size, n_samples, 3\n\n        if self.n_outside > 0:\n            dis_to_center = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).clip(1.0, 1e10)\n            pts = torch.cat([pts / dis_to_center, 1.0 / dis_to_center], dim=-1)       # batch_size, n_samples, 4\n\n        dirs = rays_d[:, None, :].expand(batch_size, n_samples, 3)\n\n        pts = pts.reshape(-1, 3 + int(self.n_outside > 0))\n        dirs = dirs.reshape(-1, 3)\n\n        density, sampled_color = nerf(pts, dirs)\n        alpha = 1.0 - torch.exp(-F.relu(density.reshape(batch_size, n_samples)) * dists)\n        alpha = alpha.reshape(batch_size, n_samples)\n        weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]  # n_rays, n_samples\n        sampled_color = sampled_color.reshape(batch_size, n_samples, 3)\n        color = (weights[:, :, None] * sampled_color).sum(dim=1)\n        if background_rgb is not None:\n            color = color + background_rgb * (1.0 - weights.sum(dim=-1, keepdim=True))\n\n        return {\n            'color': color,\n            'sampled_color': sampled_color,\n            'alpha': alpha,\n            'weights': weights,\n        }\n\n    def render_core(self,\n                    rays_o,\n                    rays_d,\n                    z_vals,\n                    sample_dist,\n                    nerf,\n                    fine=False,\n                    background_rgb=None,\n                    background_alpha=None,\n                    background_sampled_color=None):\n\n        batch_size, n_samples = z_vals.shape\n        dists = z_vals[..., 1:] - z_vals[..., :-1]\n        dists = torch.cat([dists, torch.Tensor([sample_dist]).expand(dists[..., :1].shape)], -1)\n\n        mid_z_vals = z_vals + dists * 0.5\n\n        pts = rays_o[:, None, :] + rays_d[:, None, :] * mid_z_vals[..., :, None]  # n_rays, n_samples, 3\n\n        dirs = rays_d[:, None, :].expand(pts.shape)\n\n        pts = pts.reshape(-1, 3)\n        dirs = dirs.reshape(-1, 3)\n\n        density, sampled_color = nerf(pts, dirs)\n        sampled_color = sampled_color.reshape(batch_size, n_samples, 3)\n        alpha = 1.0 - torch.exp(-F.relu(density.reshape(batch_size, n_samples)) * dists)\n        alpha = alpha.reshape(batch_size, n_samples)\n\n        inside_sphere = None\n        if background_alpha is not None:  # render without mask\n            pts_radius = torch.linalg.norm(pts, ord=2, dim=-1, keepdim=True).reshape(batch_size, n_samples)\n            inside_sphere = (torch.linalg.norm(pts, ord=2, dim=-1).reshape(batch_size, n_samples) < 1.0).float()\n            # alpha = alpha * inside_sphere\n            alpha = torch.cat([alpha, background_alpha], dim=-1)\n            sampled_color = torch.cat([sampled_color, background_sampled_color], dim=1)\n\n        weights = alpha * torch.cumprod(torch.cat([torch.ones([batch_size, 1]), 1. - alpha + 1e-7], -1), -1)[:, :-1]  # n_rays, n_samples\n        color = (weights[:, :, None] * sampled_color.reshape(batch_size, n_samples + self.n_outside, 3)).sum(dim=1)\n\n        return {\n            'color': color,\n            'weights': weights,\n            'depth': (mid_z_vals * weights[:, :n_samples]).sum(dim=-1, keepdim=True)\n        }\n\n    def render(self, rays_o, rays_d, near, far, perturb_overwrite=-1, background_rgb=None):\n\n        sample_dist = ((far - near) / self.n_samples).mean().item()\n        z_vals = torch.linspace(0.0, 1.0, self.n_samples)\n        z_vals = near + (far - near) * z_vals[None, :]\n        z_vals_outside = None\n        if self.n_outside > 0:\n            z_vals_outside = torch.linspace(1e-3, 1.0 - 1.0 / (self.n_outside + 1.0), self.n_outside)\n\n        n_samples = self.n_samples\n        perturb = self.perturb\n        if perturb_overwrite >= 0:\n            perturb = perturb_overwrite\n        if perturb > 0:\n            # get intervals between samples\n            mids = .5 * (z_vals[..., 1:] + z_vals[..., :-1])\n            upper = torch.cat([mids, z_vals[..., -1:]], -1)\n            lower = torch.cat([z_vals[..., :1], mids], -1)\n            # stratified samples in those intervals\n            t_rand = torch.rand(z_vals.shape)\n            z_vals = lower + (upper - lower) * t_rand\n\n            if self.n_outside > 0:\n                mids = .5 * (z_vals_outside[..., 1:] + z_vals_outside[..., :-1])\n                upper = torch.cat([mids, z_vals_outside[..., -1:]], -1)\n                lower = torch.cat([z_vals_outside[..., :1], mids], -1)\n                # stratified samples in those intervals\n                t_rand = torch.rand(z_vals_outside.shape)\n                z_vals_outside = lower + (upper - lower) * t_rand\n\n        if self.n_outside > 0:\n            z_vals_outside = far / torch.flip(z_vals_outside, dims=[-1]) + 1.0 / self.n_samples\n\n        color_coarse = None\n\n        background_alpha = None\n        background_sampled_color = None\n        background_color = torch.zeros([1, 3])\n\n        ret_coarse = {\n            'color': None,\n            'weights': None,\n        }\n\n        # NeRF++\n        if self.n_outside > 0:\n            # z_vals_feed = torch.cat([z_vals, z_vals_outside], dim=-1)\n            # z_vals_feed, _ = torch.sort(z_vals_feed, dim=-1)\n            z_vals_feed = z_vals_outside\n            ret_outside = self.render_core_coarse(rays_o, rays_d, z_vals_feed, sample_dist, self.nerf_outside,\n                                                  background_rgb=None)\n\n            background_sampled_color = ret_outside['sampled_color']\n            background_alpha = ret_outside['alpha']\n\n        if self.n_importance > 0:\n            ret_coarse = self.render_core(rays_o,\n                                          rays_d,\n                                          z_vals,\n                                          sample_dist,\n                                          self.nerf_coarse,\n                                          fine=False,\n                                          background_rgb=background_rgb,\n                                          background_alpha=background_alpha,\n                                          background_sampled_color=background_sampled_color)\n\n            weights = ret_coarse['weights']\n            # importance sampling\n            z_samples = sample_pdf(z_vals, weights[..., :-1 + self.n_samples],\n                                   self.n_importance, det=True).detach()\n            z_vals = torch.cat([z_vals, z_samples], dim=-1)\n            z_vals, _ = torch.sort(z_vals, dim=-1)\n            z_vals = z_vals.detach()\n\n            n_samples = self.n_samples + self.n_importance\n\n        # ----------------------------------- fine --------------------------------------------\n        # render again\n        ret_fine = self.render_core(rays_o,\n                                    rays_d,\n                                    z_vals,\n                                    sample_dist,\n                                    self.nerf_fine,\n                                    fine=True,\n                                    background_rgb=background_rgb,\n                                    background_alpha=background_alpha,\n                                    background_sampled_color=background_sampled_color)\n\n        return {\n            'color_coarse': ret_coarse['color'],\n            'color_fine': ret_fine['color'],\n            'weight_sum': ret_fine['weights'].sum(dim=-1, keepdim=True),\n            'depth': ret_fine['depth']\n        }\n\n    def extract_geometry(self, bound_min, bound_max, resolution, threshold=25):\n        ret = extract_geometry(bound_min, bound_max, resolution, threshold, lambda pts: self.nerf_fine(pts, torch.zeros_like(pts))[0])\n        return ret"]}
{"filename": "confs/path.py", "chunked_list": ["import os\n\n# BIN_DIR = \"/home/depthneus\"\n# DIR_MVG_BUILD = BIN_DIR + \"/openMVG_Build\"\n# DIR_MVS_BUILD = BIN_DIR + \"/openMVS_build\"\n\n# # normal path\n# dir_snu_code = '/path/snucode' # directory of code\n# path_snu_pth = 'path/scannet.pt'\n# assert os.path.exists(path_snu_pth)", "# path_snu_pth = 'path/scannet.pt'\n# assert os.path.exists(path_snu_pth)\n\n# dir_tiltedsn_code = '/path/tiltedsn_code'\n# dir_tiltedsn_ckpt = '/path/tiltedsn_ckpt' # directory of pretrained model\n# path_tiltedsn_pth_pfpn = f\"{dir_tiltedsn_ckpt}/PFPN_SR_full/model-best.ckpt\"\n# path_tiltedsn_pth_sr = f\"{dir_tiltedsn_ckpt}/SR_only/model-latest.ckpt\"\n# assert os.path.exists(path_tiltedsn_pth_sr)\n\n# # used scenes", "\n# # used scenes\n# names_scenes_depthneus = ['scene0009_01', 'scene0085_00', 'scene0114_02',\n#                         'scene0603_00', 'scene0617_00', 'scene0625_00',\n#                         'scene0721_00', 'scene0771_00']\n# names_scenes_manhattansdf = ['scene0050_00', 'scene0084_00', \n#                                 'scene0580_00', 'scene0616_00']\n# lis_name_scenes = names_scenes_depthneus + names_scenes_manhattansdf\n\n# # update training/test split", "\n# # update training/test split\n# names_scenes_depthneus_remove = ['scene0009', 'scene0085', 'scene0114',\n#                         'scene0603', 'scene0617', 'scene0625',\n#                         'scene0721', 'scene0771']\n# names_scenes_manhattansdf_remove = ['scene0050', 'scene0084', \n#                                 'scene0580', 'scene0616']\n# lis_name_scenes_remove = names_scenes_depthneus_remove + names_scenes_manhattansdf_remove\nnames_scenes_depthneus = ['scene0625_00']\nlis_name_scenes = names_scenes_depthneus", "names_scenes_depthneus = ['scene0625_00']\nlis_name_scenes = names_scenes_depthneus"]}
