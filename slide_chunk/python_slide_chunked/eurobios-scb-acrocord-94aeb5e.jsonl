{"filename": "test/test_misc.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\nfrom acrocord.misc import execution", "# See the License for the specific language governing permissions and limitations under the License.\nfrom acrocord.misc import execution\nimport pytest\n\n\ndef test_deprecated():\n\n    @execution.deprecated\n    def foo():\n        return \"hello\"\n\n    with pytest.deprecated_call():\n        foo()", "\n\ndef test_print():\n\n    @execution.execution_time\n    def foo():\n        return \"hello\"\n\n    assert foo() == \"hello\"\n", ""]}
{"filename": "test/test_logger.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\n", "# See the License for the specific language governing permissions and limitations under the License.\n\nimport pytest\n\nfrom acrocord.utils.logger import Logger\n\n\n@pytest.fixture\ndef get_logger(get_connection) -> Logger:\n    logger = Logger(connection=get_connection, table_name='log_table', schema='test')\n    assert 'test.log_table' == logger.table_name\n    return logger", "def get_logger(get_connection) -> Logger:\n    logger = Logger(connection=get_connection, table_name='log_table', schema='test')\n    assert 'test.log_table' == logger.table_name\n    return logger\n\n\ndef test_write_log_dataframe(get_logger: Logger, get_example_log_dataframe) -> None:\n    get_logger.write_log(get_example_log_dataframe)\n\n\ndef test_write_log_series(get_logger: Logger, get_example_log_series) -> None:\n    get_logger.write_log(get_example_log_series)", "\n\ndef test_write_log_series(get_logger: Logger, get_example_log_series) -> None:\n    get_logger.write_log(get_example_log_series)\n\n\ndef test_write_log_dict(get_logger: Logger, get_example_log_dict) -> None:\n    get_logger.write_log(get_example_log_dict)\n\n\ndef test_write_log_other_type(get_logger: Logger, get_example_log_other_type) -> None:\n    with pytest.raises(ValueError) as exc_info:\n        get_logger.write_log(get_example_log_other_type)\n\n        assert str(exc_info.value) == \"Input data should be type of [pd.Series, pd.DataFrame, or dict]\"", "\n\ndef test_write_log_other_type(get_logger: Logger, get_example_log_other_type) -> None:\n    with pytest.raises(ValueError) as exc_info:\n        get_logger.write_log(get_example_log_other_type)\n\n        assert str(exc_info.value) == \"Input data should be type of [pd.Series, pd.DataFrame, or dict]\"\n"]}
{"filename": "test/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\n", "# See the License for the specific language governing permissions and limitations under the License.\n"]}
{"filename": "test/test_constraints.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\n", "# See the License for the specific language governing permissions and limitations under the License.\n\nimport pytest\n\nfrom acrocord.test import constraints\n\n\ndef test_example_span_table_constraint(\n        get_example_dataframe_building):\n    data = get_example_dataframe_building\n    example_constraints = {\n        \"first_constraint\": (constraints.unique, {\"columns\": [\"building_id\"]}),\n        \"second_constraint\": (\n            constraints.data_type,\n            {'columns': ['building_id'], 'dtype': 'int64'}),\n        \"forth_constraint\":\n            (constraints.nb_unique_index,\n             {\"columns\": [\"building_id\"], \"minimum\": 0,\n              \"maximum\": 10}),\n        \"fifth_constraint\": (constraints.quantile,\n                             {\"columns\": [\"building_id\"], \"q\": 0.99,\n                              \"threshold\": 1000}),\n        \"third_constraint\": (\n            constraints.not_nullable, {\"columns\": [\"building_id\"]})\n    }\n\n    dc = constraints.DataConstraints(data)\n    dc.add_constraint(example_constraints)\n    dc.test()", "\n\ndef test_example_span_table_constraint_raise(get_example_dataframe_building):\n    data = get_example_dataframe_building\n    example_constraints = {\n        \"first_constraint\": (constraints.eligible_data_type, {}),\n        \"second_constraint\": (\n            constraints.data_type, {'columns': 'building_id'}),\n\n    }\n\n    dc = constraints.DataConstraints(data)\n    dc.add_constraint(example_constraints)\n    with pytest.raises(AssertionError):\n        dc.test()", ""]}
{"filename": "test/test_databasepg.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\n", "# See the License for the specific language governing permissions and limitations under the License.\n\nimport warnings\n\nimport pandas as pd\n\n\ndef test_dataframe_equals_reading_writing(get_connection,\n                                          get_example_data_frame):\n\n    get_connection.write_table(get_example_data_frame, 'test.test_data')\n    dataframe_read = get_connection.read_table('test.test_data')\n    get_connection.drop_table('test.test_data')\n    assert get_example_data_frame.equals(\n        dataframe_read), \"dataframe and dataframe_read are the same\"", "\n\ndef test_warning_types(get_example_data_frame):\n    from acrocord.utils.types import warning_type\n    dataframe = get_example_data_frame.astype(int)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        warning_type(dataframe, 3)\n        assert len(w) == dataframe.shape[1]\n", "\n\ndef test_warning_types_64(get_example_data_frame):\n    from acrocord.utils.types import warning_type\n    dataframe = get_example_data_frame.astype(\"int64\")\n    warning_type(dataframe, 3)\n\n\ndef test_get_metadata(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, 'test.dataframe')\n    df_meta = get_connection.get_metadata('test.dataframe')\n    error_msg = \"The number of line in meta data does correspond to the number of columns \"\n    number_columns = len(get_connection.get_columns('test.dataframe'))\n    get_connection.drop_table('test.dataframe')\n    assert len(df_meta) == number_columns, error_msg", "def test_get_metadata(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, 'test.dataframe')\n    df_meta = get_connection.get_metadata('test.dataframe')\n    error_msg = \"The number of line in meta data does correspond to the number of columns \"\n    number_columns = len(get_connection.get_columns('test.dataframe'))\n    get_connection.drop_table('test.dataframe')\n    assert len(df_meta) == number_columns, error_msg\n\n\ndef test_foreign_keys(get_connection, get_example_data_frame,\n                      get_example_data_frame_other):\n    df1 = get_example_data_frame\n    df2 = get_example_data_frame_other\n    get_connection.write_table(df1, \"test.main\", primary_key='a')\n    get_connection.write_table(df2, \"test.other\",\n                               foreign_keys={'a': \"test.main\"})", "\ndef test_foreign_keys(get_connection, get_example_data_frame,\n                      get_example_data_frame_other):\n    df1 = get_example_data_frame\n    df2 = get_example_data_frame_other\n    get_connection.write_table(df1, \"test.main\", primary_key='a')\n    get_connection.write_table(df2, \"test.other\",\n                               foreign_keys={'a': \"test.main\"})\n\n\ndef test_get_shape(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\", primary_key='a')\n    s = get_connection.get_shape(\"test.main\")\n    assert (s == get_example_data_frame.shape)", "\n\ndef test_get_shape(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\", primary_key='a')\n    s = get_connection.get_shape(\"test.main\")\n    assert (s == get_example_data_frame.shape)\n\n\ndef test_query_where(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\",\n                               primary_key='a')\n    df = get_connection.read_table(\"test.main\", where=\"a in (155)\")\n    assert len(df) == 1", "def test_query_where(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\",\n                               primary_key='a')\n    df = get_connection.read_table(\"test.main\", where=\"a in (155)\")\n    assert len(df) == 1\n\n\ndef test_query_limit(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\",\n                               primary_key='a')\n    df = get_connection.read_table(\"test.main\", limit=2)\n    assert len(df) == 2", "\n\ndef test_create_insert(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\")\n    get_connection.create_insert(get_example_data_frame, \"test.main\")\n    df = get_connection.read_table(\"test.main\")\n    assert len(df) == len(get_example_data_frame) * 2\n\n\ndef test_create_insert_create(get_connection, get_example_data_frame):\n    get_connection.drop_table(\"test.main\")\n    get_connection.create_insert(get_example_data_frame, \"test.main\")\n    df = get_connection.read_table(\"test.main\")\n    assert len(df) == len(get_example_data_frame)", "\ndef test_create_insert_create(get_connection, get_example_data_frame):\n    get_connection.drop_table(\"test.main\")\n    get_connection.create_insert(get_example_data_frame, \"test.main\")\n    df = get_connection.read_table(\"test.main\")\n    assert len(df) == len(get_example_data_frame)\n\n\ndef test_create_drop_create_column(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\")\n    get_connection.drop_column(\"test.main\", \"b\")\n    df = get_connection.read_table(\"test.main\")\n    assert df.shape[1] == get_example_data_frame.shape[1] - 1\n\n    get_connection.add_columns(\n        \"test.main\",\n        get_example_data_frame[[\"a\", \"b\"]],\n        index=\"a\"\n    )", "def test_create_drop_create_column(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\")\n    get_connection.drop_column(\"test.main\", \"b\")\n    df = get_connection.read_table(\"test.main\")\n    assert df.shape[1] == get_example_data_frame.shape[1] - 1\n\n    get_connection.add_columns(\n        \"test.main\",\n        get_example_data_frame[[\"a\", \"b\"]],\n        index=\"a\"\n    )", "\n\ndef test_copy(get_connection, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n    get_connection.copy(\"test.main\", \"test.main_copy\")\n    df1 = get_connection.read_table(\"test.main\")\n    df2 = get_connection.read_table(\"test.main_copy\")\n    assert df1.equals(df2)\n\n\ndef test_get_table_names(get_connection, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n    assert len(get_connection.get_table_names(\"test\")) > 0", "\n\ndef test_get_table_names(get_connection, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n    assert len(get_connection.get_table_names(\"test\")) > 0\n\n\ndef test_get_view_names(get_connection, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n    get_connection.execute(\"create view public.test as select * from test.main\")\n    assert len(get_connection.get_view_names(\"public\")) > 0", "\n\ndef test_create_schema(get_connection, get_example_data_frame):\n    get_connection.create_schema(\"test_ci\")\n    get_connection.write_table(get_example_data_frame, \"test_ci.test_schema\")\n    assert \"test_ci\" in get_connection.get_schema_names()\n\n\n# def test_geopandas(get_connection, get_example_data_frame):\n#     import geopandas", "# def test_geopandas(get_connection, get_example_data_frame):\n#     import geopandas\n#     connection = get_connection\n#     dataframe = get_example_data_frame\n#     gdf = geopandas.GeoDataFrame(\n#         dataframe,\n#         geometry=geopandas.points_from_xy(dataframe.lon, dataframe.lat))\n#     connection.write_table(gdf, \"test.test_lat_lon\")\n#\n#", "#\n#\n\n\ndef test_list_table(get_connection):\n    list_table = get_connection.list_table(\"public\")\n    print(list_table)\n\n\ndef test_update(get_connection, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n    get_connection.update(\"test.other\", where=\"a=155\",\n                          value=\"150\", column=\"a\")\n    df_read = get_connection.read_table(\"test.other\")\n    assert 150 in df_read[\"a\"].values", "\ndef test_update(get_connection, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n    get_connection.update(\"test.other\", where=\"a=155\",\n                          value=\"150\", column=\"a\")\n    df_read = get_connection.read_table(\"test.other\")\n    assert 150 in df_read[\"a\"].values\n\n\ndef test_get_dtypes(get_connection, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n    res = get_connection.get_dtypes(\"test.other\")\n    assert isinstance(res, pd.DataFrame)\n    assert len(res) == len(get_connection.get_columns(\"test.other\"))", "\ndef test_get_dtypes(get_connection, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n    res = get_connection.get_dtypes(\"test.other\")\n    assert isinstance(res, pd.DataFrame)\n    assert len(res) == len(get_connection.get_columns(\"test.other\"))\n\n\ndef test_drop(get_connection, get_example_data_frame, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n    get_connection.write_table(get_example_data_frame, \"test.main\")\n    get_connection.drop(\"test.main\", \"table\", \"a\")\n    assert \"a\" not in get_connection.get_columns(\"test.main\")\n    get_connection.write_table(get_example_data_frame_other, \"test.main\")", "def test_drop(get_connection, get_example_data_frame, get_example_data_frame_other):\n    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n    get_connection.write_table(get_example_data_frame, \"test.main\")\n    get_connection.drop(\"test.main\", \"table\", \"a\")\n    assert \"a\" not in get_connection.get_columns(\"test.main\")\n    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n\n\ndef test_execute(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\",\n                               primary_key='a')\n    get_connection.execute(\"select * from test.main\")", "def test_execute(get_connection, get_example_data_frame):\n    get_connection.write_table(get_example_data_frame, \"test.main\",\n                               primary_key='a')\n    get_connection.execute(\"select * from test.main\")\n\n\ndef test_names_empty_list(get_connection):\n    get_connection.execute(\"DROP SCHEMA if exists test_names\")\n    get_connection.create_schema(\"test_names\")\n    assert len(get_connection.get_table_names(\"test_names\")) == 0\n    get_connection.execute(\"DROP SCHEMA test_names\")", ""]}
{"filename": "test/test_connect.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\n", "# See the License for the specific language governing permissions and limitations under the License.\n\nimport asyncio\nimport pytest\n\n\ndef test_password_auth(get_connection):\n    from acrocord.utils.connect import password_auth\n    try:\n        assert isinstance(password_auth(), str)\n    except (PermissionError, FileNotFoundError):\n        pass", "\n\ndef test_connect_literal():\n    from acrocord.utils.connect import connect_psql_server\n    str_ = \"Engine(postgresql+psycopg2://user:***@localhost:5432/name)\"\n\n    assert str(connect_psql_server(\n        username=\"user\",\n        connection=dict(\n            password=\"pass\",\n            sslmode=\"require\",\n            host=\"localhost\",\n            port=\"5432\",\n            ssh=False, dbname=\"name\"))) == str_", "\n\n@pytest.mark.asyncio\nasync def test_connect_async_main(postgresql):\n    from acrocord.utils.connect import connect_psql_server\n    connect_psql_server(username=postgresql.info.user,\n                        connection=dict(\n                            password=\" \",\n                            sslmode=\"require\",\n                            host=postgresql.info.host,", "                            sslmode=\"require\",\n                            host=postgresql.info.host,\n                            port=postgresql.info.port,\n                            ssh=False, dbname=postgresql.info.dbname), async_=True)\n"]}
{"filename": "test/test_table_factory.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\n", "# See the License for the specific language governing permissions and limitations under the License.\n\nimport os\nfrom typing import Dict, Tuple\n\nimport pandas as pd\nimport pytest\n\nfrom acrocord.factory.table import TableFactory\nfrom acrocord.utils.types import EligibleDataType", "from acrocord.factory.table import TableFactory\nfrom acrocord.utils.types import EligibleDataType\n\n\nclass BuildingTableFactory(TableFactory):\n    # name of the table\n    @classmethod\n    def table_name(cls) -> str:\n        return 'buildings'\n\n    # name of the schema where to store the table\n    @classmethod\n    def schema_name(cls) -> str:\n        return 'test'\n\n    # column format statement and documentation.\n    @classmethod\n    def data_definition(cls) -> Dict[str, Tuple[EligibleDataType, str]]:\n        return {\n            'building_id': (EligibleDataType.INTEGER, 'Identification number'),\n            'architect': (EligibleDataType.STRING, 'Architect name'),\n            'height': (\"float16\", ''),\n            'construction_date': (\n                EligibleDataType.DATE_TIME,\n                'Construction Date of the building'),\n            # 'is_constructed': (\n            #     EligibleDataType.BOOLEAN, 'Does the building already construct') -> test\n        }\n\n    # this is the primary key of your table it must exist in\n    # BuildingTableFactory._doc_cols\n    # and as it is a row identifier, each row of your\n    # table must have a unique non-null value\n    @classmethod\n    def id_key(cls) -> str:\n        return 'building_id'\n\n    # This is the main table where you get data from other sources\n    # (possibly another TableFactory).\n    # The only restriction is to\n    def _create_table(self) -> None:\n        df = pd.DataFrame(data={'building_id': [11, 20, 14, 34, 61],\n                                'architect': [\"Durand\", \"Blanc\", \"Blanc\",\n                                              \"Dubois\", \"Martin\"],\n                                'height': [14.4, 24.4, 35.3, 12.3, 14.4],\n                                'construction_date': ['10/03/1957',\n                                                      '30/11/2087',\n                                                      '01/02/2070',\n                                                      '04/01/1989',\n                                                      '28/10/2003']\n                                })\n        df['construction_date'] = pd.to_datetime(df.construction_date,\n                                                 format='%d/%m/%Y')\n        from datetime import datetime\n        df['is_constructed'] = df.construction_date < datetime.now()\n\n        self._set_table(df)  # Required line to store the result\n\n    @classmethod\n    def get_db_connection(cls):\n        return cls.connection", "\n\n@pytest.fixture\ndef get_table_factory(get_connection) -> BuildingTableFactory:\n    BuildingTableFactory.connection = get_connection\n    table_factory = BuildingTableFactory(verbose=True)\n    return table_factory\n\n\ndef test_write(get_table_factory: BuildingTableFactory, get_connection) -> None:\n\n    get_table_factory.write_table()\n    get_table_factory.get_table()\n    assert \"buildings\" in get_connection.get_table_names(\"test\")", "\ndef test_write(get_table_factory: BuildingTableFactory, get_connection) -> None:\n\n    get_table_factory.write_table()\n    get_table_factory.get_table()\n    assert \"buildings\" in get_connection.get_table_names(\"test\")\n\n\ndef test_read(get_table_factory: BuildingTableFactory) -> None:\n    get_table_factory.write_table()\n    get_table_factory.read_table()", "def test_read(get_table_factory: BuildingTableFactory) -> None:\n    get_table_factory.write_table()\n    get_table_factory.read_table()\n\n\ndef test_implement_foreign_keys(\n        get_table_factory: BuildingTableFactory) -> None:\n    get_table_factory.add_foreign_keys()\n\n\ndef test_get_table_name(get_table_factory: BuildingTableFactory) -> None:\n    assert \"test.buildings\" == get_table_factory.get_full_name()", "\n\ndef test_get_table_name(get_table_factory: BuildingTableFactory) -> None:\n    assert \"test.buildings\" == get_table_factory.get_full_name()\n\n\ndef test_write_to_excel(get_table_factory: BuildingTableFactory) -> None:\n    import tempfile\n    get_table_factory.write_to_excel()\n    assert \"buildings.xlsx\" in os.listdir(tempfile.gettempdir())", "\n\ndef test_columns(get_table_factory: BuildingTableFactory) -> None:\n    print(get_table_factory.columns)\n    for c in get_table_factory.columns:\n        assert c in ['building_id', 'architect', 'height',\n                     'construction_date',\n                     'is_constructed'], f\"column {c} is not in columns\"\n    assert isinstance(get_table_factory.columns,\n                      list), \"columns attribute is not a list\"", "\n\ndef test_get_instance(get_table_factory):\n    inst = get_table_factory.get_instance()\n    assert inst._table is None\n\n    inst.get_table()\n\n\ndef test_check_table_doc(get_table_factory):\n    get_table_factory.check_table_doc(get_table_factory.get_table(),\n                                      get_table_factory.data_definition())", "\ndef test_check_table_doc(get_table_factory):\n    get_table_factory.check_table_doc(get_table_factory.get_table(),\n                                      get_table_factory.data_definition())\n\n# Test to add\n# TODO:  test if deployed and _table is freed\n# TODO:  test if second _get_data is faster\n", ""]}
{"filename": "test/conftest.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\n", "# See the License for the specific language governing permissions and limitations under the License.\n\nimport pandas as pd\nimport pytest\n\nfrom acrocord import ConnectDatabase\n\n\n@pytest.fixture(autouse=True)\ndef get_connection(postgresql):\n    db = ConnectDatabase()\n    connection = db.connect(verbose=3,\n                            connection={\n                                \"dbname\": postgresql.info.dbname,\n                                \"password\": \" \",\n                                \"user\": postgresql.info.user,\n                                \"port\": postgresql.info.port,\n                                \"host\": postgresql.info.host\n                            })\n    connection.create_schema(\"test\")\n    return connection", "@pytest.fixture(autouse=True)\ndef get_connection(postgresql):\n    db = ConnectDatabase()\n    connection = db.connect(verbose=3,\n                            connection={\n                                \"dbname\": postgresql.info.dbname,\n                                \"password\": \" \",\n                                \"user\": postgresql.info.user,\n                                \"port\": postgresql.info.port,\n                                \"host\": postgresql.info.host\n                            })\n    connection.create_schema(\"test\")\n    return connection", "\n\n@pytest.fixture(scope=\"module\")\ndef get_example_data_frame():\n    dataframe = pd.DataFrame({'a': [155, 20, 3],\n                              'b': [11, 299, 45],\n                              'c': [73, 3, 39],\n                              'd': [783, 488, 739],\n                              \"lat\": [45, 45, 45],\n                              \"lon\": [0, 1, 2]\n                              })\n    return dataframe", "\n\n@pytest.fixture(scope=\"module\")\ndef get_example_data_frame_other():\n    dataframe = pd.DataFrame({'a': [155, 20, 155],\n                              'e': [\"a\", \"b\", \"b\"]\n                              })\n    return dataframe\n\n", "\n\n@pytest.fixture(scope=\"module\")\ndef db_connector(get_connection):\n    return get_connection\n\n\n@pytest.fixture(scope=\"module\")\ndef get_example_dataframe_building():\n    df = pd.DataFrame(data={'building_id': [11, 20, 14, 34, 61],\n                            'architect': [\"Durand\", \"Blanc\", \"Blanc\",\n                                          \"Dubois\", \"Martin\"],\n                            'height': [14.4, 24.4, 35.3, 12.3, 14.4],\n                            'construction_date': ['10/03/1957',\n                                                  '30/11/2087',\n                                                  '01/02/2070',\n                                                  '04/01/1989',\n                                                  '28/10/2003']\n                            })\n    return df", "def get_example_dataframe_building():\n    df = pd.DataFrame(data={'building_id': [11, 20, 14, 34, 61],\n                            'architect': [\"Durand\", \"Blanc\", \"Blanc\",\n                                          \"Dubois\", \"Martin\"],\n                            'height': [14.4, 24.4, 35.3, 12.3, 14.4],\n                            'construction_date': ['10/03/1957',\n                                                  '30/11/2087',\n                                                  '01/02/2070',\n                                                  '04/01/1989',\n                                                  '28/10/2003']\n                            })\n    return df", "\n\n@pytest.fixture(scope=\"module\")\ndef get_example_log_dataframe():\n    dataframe = pd.DataFrame({'value': [1, 2, 3],\n                              'message': 'test log with pd.DataFrame type',\n                              'other': 'tmp',\n                              })\n    return dataframe\n", "\n\n@pytest.fixture(scope=\"module\")\ndef get_example_log_series():\n    return pd.Series([1, 3, 4, 5])\n\n\n@pytest.fixture(scope=\"module\")\ndef get_example_log_dict():\n    return {'value': 1, 'message': 'test log with dict type'}", "def get_example_log_dict():\n    return {'value': 1, 'message': 'test log with dict type'}\n\n\n@pytest.fixture(scope=\"module\")\ndef get_example_log_other_type():\n    return [1, 2, 3]\n"]}
{"filename": "test/test_auxiliaries.py", "chunked_list": ["# Copyright 2023 Eurobios\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and limitations under the License.\nimport pytest", "# See the License for the specific language governing permissions and limitations under the License.\nimport pytest\nfrom sqlalchemy.exc import ProgrammingError\n\nfrom acrocord.utils import auxiliaries\n\n\ndef test_execute_sql_cmd_raise_error(get_connection):\n    sql_cmd_with_error = \"select *\"\n    with pytest.raises(ProgrammingError) as e_info:\n        auxiliaries.execute_sql_cmd(get_connection, sql_cmd_with_error)", "\n\ndef test_drop_table():\n    assert auxiliaries.drop_table(\"test\") == \"DROP TABLE IF \" \\\n                                             \"EXISTS test CASCADE;\"\n\n\ndef test_count():\n    assert auxiliaries.count(\"test\") == \"SELECT COUNT(*) FROM test;\"\n", "\n\ndef test_list_table():\n    assert auxiliaries.list_table(\n        \"test\") == f\"SELECT * FROM information_schema.tables \" \\\n                   f\"WHERE table_schema = 'test'\"\n\n\ndef test_data_types():\n    a, b, c, d = \"int32\", \"int4\", \"datetime64[s]\", \"timestamp\"\n    assert auxiliaries.db_data_type(a) == b\n    assert auxiliaries.db_data_type(b, invert=True) == a\n    assert auxiliaries.db_data_type(c) == d\n    assert auxiliaries.db_data_type(d, invert=True) == c", "def test_data_types():\n    a, b, c, d = \"int32\", \"int4\", \"datetime64[s]\", \"timestamp\"\n    assert auxiliaries.db_data_type(a) == b\n    assert auxiliaries.db_data_type(b, invert=True) == a\n    assert auxiliaries.db_data_type(c) == d\n    assert auxiliaries.db_data_type(d, invert=True) == c\n"]}
{"filename": "acrocord/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom .databasepg import ConnectDatabase\n"]}
{"filename": "acrocord/databasepg.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\n\"\"\"\nThis module contains the main class to be used\nIt solely contains the class ConnectDatabase\n\"\"\"\n", "\"\"\"\n\nimport io\nimport logging\nimport typing\nfrom typing import Union, Iterable, Dict\n\nimport pandas as pd\nfrom sqlalchemy import exc, text\n", "from sqlalchemy import exc, text\n\nfrom acrocord.misc import execution\nfrom acrocord.utils import auxiliaries\nfrom acrocord.utils import connect\nfrom acrocord.utils import insert\nfrom acrocord.utils.types import warning_type\n\nLOGGER = logging.getLogger(__name__)\nlogging.basicConfig(format='%(levelname)s:%(message)s')", "LOGGER = logging.getLogger(__name__)\nlogging.basicConfig(format='%(levelname)s:%(message)s')\n\n\nclass ConnectDatabase:\n    def __init__(self):\n        self.username = \"\"\n        self.engine = None\n        self.connection = None\n        self.replace = self.rename\n        self.rename_ = {}\n        self.active_execution_time = False\n        self.active_sql_printing = False\n        self.connections = connect.load_available_connections()\n        self.verbose = 1\n\n    def connect(self, verbose: int = 1,\n                connection: Union[dict, str] = None,\n                **kwargs) -> \"ConnectDatabase\":\n        \"\"\"\n        Instantiate the connexion with the database. This function has to be\n        called to access other methods.\n\n        Return a :obj:`ConnectDatabase` object.\n\n\n        Parameters\n        ----------\n        verbose: int,\n            level of verbosity.\n            - 0 nothing\n            - 1 metadata\n            - 2 sql command\n        connection: (str or dict),\n            connection parameters. If a string is\n            provided the file `~/.postgresql/connections.cfg` will be loaded\n            as connection parameters. If a dict is provided, it must contain\n            'host', 'port', 'dbname' and 'user' keys.\n\n        Returns\n        -------\n        :obj:`ConnectDatabase`\n        \"\"\"\n        if isinstance(connection, str):\n            connection = self.connections[connection]\n\n        self.engine = connect.connect_psql_server(\n            connection=connection, **kwargs.copy())\n        self.connection = self.engine.raw_connection()\n        self.active_execution_time = verbose >= 1\n        self.active_sql_printing = verbose >= 2\n        self.verbose = verbose\n        self.username = connection[\"user\"]\n\n        return self\n\n    def close(self) -> None:\n        \"\"\"\n        Close the connection\n\n        Returns\n        -------\n            None\n        \"\"\"\n        self.engine.dispose()\n        self.connection.detach()\n        self.connection.close()\n\n    # ===========================================================================\n    #                           READ AND WRITE\n    # ==========================================================================\n    @execution.execution_time\n    def read_table(\n            self, table_name: str,\n            columns: iter = None,\n            where: str = None,\n            **kwargs) -> pd.DataFrame:\n        \"\"\"\n        Read a table from server\n\n        Parameters\n        ---------\n        table_name: str\n            Name of the table. Naming convention \"schema.name\" if no schema\n            is specified the \"main\" schema will be used\n        columns: iter\n            Specific columns to request.\n            If None all columns are requested.\n            If errors are made or columns are missed, they will be ignored\n        where: str\n            Request specific rows in postgresql fashion\n\n        Returns\n        -------\n            Data as :obj:`pandas.DataFrame`\n        \"\"\"\n        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n        query = self._select(table_name, columns=columns, where=where, **kwargs)\n\n        copy_sql = f\"COPY ({query}) TO STDOUT WITH CSV HEADER\"\n        connection = self.engine.connect()\n        with connection.connection.cursor() as cursor:\n            storage = io.StringIO()\n            cursor.copy_expert(copy_sql, storage)\n            storage.seek(0)\n        data_frame = pd.read_csv(storage, true_values=[\"t\"], false_values=[\"f\"])\n\n        return data_frame\n\n    @execution.execution_time\n    def write_table(self, data: typing.Union[pd.DataFrame, \"GeoDataFrame\"],\n                    table_name: str = \"tmp\",\n                    primary_key=None,\n                    if_exists=\"replace_cascade\",\n                    foreign_keys=None,\n                    column_comments=None,\n                    **kwargs) -> None:\n        \"\"\"\n        Create table based on panda's DataFrame. This function upload the data\n        on server and insert all data in the table.\n\n        Parameters\n        ----------\n        data: pandas.DataFrame\n            Data (GeoDataFrame or DataFrame) to insert into the database.\n        table_name: str\n            Name of the table. Naming convention \"schema.name\" if no schema\n            is specified the \"main\" schema will be used\n        primary_key: list or str\n            List of string specifying all keys or single string\n        if_exists: str\n            In case of existing table\n\n            - \"replace_cascade\" : will delete all table with dependency\n            - \"replace\" : will delete this table. Error will raise if there are dependencies\n        foreign_keys: dict\n            specify foreign keys in the following fashion\n            {column name in the current table : foreign table name}\n            Note that if the name of the foreign key is different in the\n            foreign table, use the `add_key` method.\n        column_comments: dict\n            Columns comments of type `{column_name: column_comment}`\n\n        Returns\n        -------\n            None\n\n        >>> from acrocord import ConnectDatabase\n        >>> dataframe = pd.DataFrame({'a': [155, 20, 3],\n        >>>                           'b': [11, 299, 45],\n        >>>                           'c': [73, 3, 39],\n        >>>                           'd': [783, 488, 739],\n        >>>                           \"lat\": [45, 45, 45],\n        >>>                           \"lon\": [0, 1, 2]\n        >>>                       })\n        >>> df = ConnectDatabase().connect(\n        >>>      connection=\"connection-name\"\n        >>>      ).write_table( dataframe, \"test.test\")\n        \"\"\"\n        data = data.rename(columns=str.lower)\n        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n        warning_type(data, verbose=self.verbose)\n\n        if \"geometry\" in data.dtypes.values and self.has_postgis_extension:\n            if_exists_pg = \"replace\" if if_exists == \"replace_cascade\" else if_exists\n            # set postgis extension in database if not already set\n\n            data.to_postgis(\n                con=self.engine,\n                name=self._get_name(table_name),\n                schema=self._get_schema(table_name),\n                if_exists=if_exists_pg)\n        else:\n            if if_exists == \"replace_cascade\":\n                self.drop_table(table_name, option=\"cascade\")\n                self.create_table(table_name, data.columns,\n                                  data.dtypes.values.astype(str),\n                                  column_comments)\n            with execution.silence_stdout():\n                self.insert(data, table_name)\n\n        if primary_key is not None:\n            self.add_key(table_name, primary_key)\n        if foreign_keys is not None:\n            for key in foreign_keys.keys():\n                self.add_key(table_name, key, type_=\"foreign\",\n                             table_foreign=foreign_keys[key])\n\n    def insert(self, data: pd.DataFrame, table_name: str,\n               chunksize=1000) -> None:\n        \"\"\"\n        This function insert data in existing postgresql table\n\n        Parameters\n        ---------\n        data: pandas.DataFrame ,\n            Data to insert\n        table_name: str\n            Name of the table. Note that the column of `data` and the columns\n            of the table must be the same\n        chunksize: int\n            Size of chunk in insert function\n\n        Returns\n        -------\n            None\n\n        \"\"\"\n\n        insert.insert(self, data, table_name, chunksize)\n\n    def create_insert(self, data: pd.DataFrame, table_name: str,\n                      *args, **kwargs):\n        \"\"\"\n        This function creates table if it does not exist and insert in the\n        table otherwise.\n\n        Parameters\n        ----------\n        data: pandas.DataFrame,\n            Data to insert.\n        table_name: str\n            Name of the table. Naming convention \"schema.name\" if no schema\n            is specified the \"main\" schema will be used\n        \"\"\"\n        if self._get_name(table_name) in self.get_table_names(\n                self._get_schema(table_name)):\n            self.insert(data, table_name)\n        else:\n            self.write_table(data, table_name, *args, **kwargs)\n\n    def add_key(self, table_name: str, key: str, type_=\"primary\",\n                table_foreign=\"\",\n                key_foreign=\"\") -> None:\n        \"\"\"\n        Add key (reference between tables).\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table for which to add key\n        key: str\n            The name of the key (corresponds to a column in the table)\n        type_: str\n            The type of key. By default, the key type is a primary key. The\n            possible values are\n                - \"primary\" to define primary key. The primary key must be\n                 composed of unique values\n                - \"foreign\" to define foreign key. If type is set to foreign,\n                the argument `table_foreign` must be provided\n        table_foreign: str\n            The name of the foreign table. Argument is ignored if key type\n            is set to \"primary\".\n        key_foreign: str\n            The name of the foreign key (column) in the foreign table table\n\n        Returns\n        -------\n            None\n        \"\"\"\n\n        table_name = f\"{self._get_schema(table_name)}.\" \\\n                     f\"{self._get_name(table_name)}\"\n        try:\n            if key_foreign == \"\":\n                key_foreign = key\n            if table_foreign != \"\":\n                df_type = self.get_dtypes(table_foreign)\n                typ = df_type.loc[\n                    df_type[\"column_name\"].values == key_foreign,\n                    \"data_type\"].values[0]\n                sql_cmd = f\"ALTER TABLE {table_name} ALTER COLUMN \" \\\n                          f\"{key} TYPE {typ} USING {key}::{typ};\"\n                auxiliaries.execute_sql_cmd(self, sql_cmd, fetch=False)\n            key = _iterable_or_str(key)\n\n            sql_cmd = f\"ALTER TABLE {table_name} ADD {type_.upper()} KEY {key}\"\n            if type_ == \"foreign\":\n                sql_cmd += f\" REFERENCES {self._get_schema(table_foreign)}.\" \\\n                           f\"{self._get_name(table_foreign)} ({key_foreign});\"\n\n            auxiliaries.execute_sql_cmd(self, sql_cmd, fetch=False)\n        except exc.ProgrammingError as error:\n            print(error)\n\n    # ==========================================================================\n    #                          ALTER DATA\n    # ==========================================================================\n\n    def add_columns(self, table_name: str, data: pd.DataFrame,\n                    index: str) -> None:\n        \"\"\"\n        Add columns to `table_name` based on the data provided on the index\n        given in argument. The name of the index column given must be the same\n        in table (on postgres server) and data (the data frame object)\n\n        Parameters\n        ----------\n        table_name: str\n            The name of table\n        data: pandas.DataFrame\n            Data to add in the table.\n        index:\n            The name of the column in the table and the dataframe on which\n            to perform the merge\n\n        Returns\n        -------\n            None\n        \"\"\"\n        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n        table_tmp = self._get_schema(table_name) + \".tmp\"\n        self.write_table(data, table_tmp)\n        if isinstance(index, str):\n            index = (index,)\n        index = [id_.lower() for id_ in index]\n\n        auxiliaries.merge(self, table_name, table_tmp,\n                          out=table_name, on=index, out_type=\"table\")\n        self.drop_table(table_tmp)\n\n    def drop_column(self,\n                    table_name: str, columns: Iterable[str],\n                    option: str = \"cascade\", type_: str = \"TABLE\"):\n        \"\"\"\n        Remove some columns\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table to alter\n        columns: iter[str]\n            The columns to drop\n        option: str\n            Option can be either\n            - 'cascade' : all dependent objects will be removed\n            - '': only the columns\n\n            Default is 'cascade'\n        type_:str\n            The type of object can be either\n                - a table and type_=\"TABLE\"\n                - a view and type_=\"VIEW\"\n        \"\"\"\n\n        sql_cmd = f\"ALTER {type_} {table_name}\"\n        for col in columns:\n            sql_cmd += f\" DROP COLUMN {col} {option},\"\n        auxiliaries.execute_sql_cmd(self, sql_cmd[:-1], hide=True, fetch=False)\n\n    @auxiliaries.execute\n    def drop_table(self, table_name: str, option: str = \"cascade\",\n                   type_: str = \"TABLE\"):\n        \"\"\"\n        Delete a table\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table to drop (delete)\n        option: str\n            Option can be either\n                - 'cascade' : all depend objects will be removed\n                - '': only the columns\n            Default is 'cascade'\n\n        type_:str\n            The type of object can be either\n                - a table and type_=\"TABLE\"\n                - a view and type_=\"VIEW\"\n\n        \"\"\"\n        sql_cmd = auxiliaries.drop_table(table_name, option, type_)\n        return sql_cmd\n\n    @auxiliaries.execute\n    def rename(self, table_name: str, new_table_name: str,\n               type_: str = \"TABLE\"):\n        \"\"\"\n        Rename a table\n\n        Parameters\n        ----------\n        table_name: str\n            The current name of the table\n        new_table_name: str\n            The new name of the table\n        type_:str\n            The type of table, either\n                - a table and type_=\"TABLE\"\n                - a view and type_=\"VIEW\"\n            Default is \"TABLE\"\n\n        Returns\n        -------\n            SQL query\n        \"\"\"\n\n        sql_cmd = f\"ALTER {type_} {table_name} \" \\\n                  f\"RENAME TO {self._get_name(new_table_name)}\"\n        return sql_cmd\n\n    @auxiliaries.execute\n    def copy(self, table_name: str, new_table_name: str, type_=\"TABLE\") -> str:\n        \"\"\"\n        Copy the table and create another table (or view)\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table to copy\n        new_table_name: str\n            The name of the target table\n        type_: str\n            The target type object, it can be either\n            - \"view\" does not create new data\n            - \"table\" creates new data\n\n        Returns\n        -------\n            SQL query\n        \"\"\"\n        table_name = f\"{self._get_schema(table_name)}.\" \\\n                     f\"{self._get_name(table_name)}\"\n        new_table_name = f\"{self._get_schema(new_table_name)}.\" \\\n                         f\"{self._get_name(new_table_name)}\"\n        self.drop_table(new_table_name, type_=type_)\n        sql_cmd = f\"CREATE {type_} {new_table_name} AS {type_} {table_name} ;\"\n        return sql_cmd\n\n    # ==========================================================================\n    #                           FETCH INFORMATION\n    # ==========================================================================\n\n    def get_dtypes(\n            self,\n            table_name: str,\n            format: str = \"dataframe\") -> typing.Union[dict, pd.DataFrame]:\n        \"\"\"\n        Get the column type of the table\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table to get type from\n\n        format: str\n            Format of the output\n\n        Returns\n        -------\n           Dict or pandas.DataFrame\n        \"\"\"\n        sql_cmd = f\"select udt_name, column_name from \" \\\n                  f\"information_schema.columns where table_name=\" \\\n                  f\"'{self._get_name(table_name)}' AND table_schema=\" \\\n                  f\"'{self._get_schema(table_name)}'\"\n        ret = auxiliaries.execute_sql_cmd(self, sql_cmd, hide=True, fetch=True)\n        df_type = pd.DataFrame(\n            ret,\n            columns=[\"data_type\", \"column_name\"])\n        return df_type\n\n    def get_columns(self, table_name: str) -> list:\n        \"\"\"\n        Get the list of columns of the given table name\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table\n\n        Returns\n        -------\n            list of column names\n        \"\"\"\n        dtypes = self.get_dtypes(table_name)\n        return list(dtypes[\"column_name\"])\n\n    def get_metadata(self, table_name: str) -> pd.DataFrame:\n        \"\"\"\n        Get metadata of a given table\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table\n\n        Returns\n        -------\n            pandas.DataFrame with metadata\n        \"\"\"\n        sql_cmd = auxiliaries.get_metadata(table_name)\n        res = auxiliaries.execute_sql_cmd(self, sql_cmd, hide=True, fetch=True)\n        df_meta = pd.DataFrame(res)\n        df_type = self.get_dtypes(table_name)\n        return pd.merge(df_meta, df_type, on='column_name')\n\n    def get_shape(self, table_name: str) -> typing.Tuple[int, int]:\n        \"\"\"\n        Get shape of a table\n\n        Parameters\n        ----------\n        table_name:str\n            The name of the table\n\n        Returns\n        -------\n            tuple of row, column shape\n        \"\"\"\n        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n        row = auxiliaries.execute_sql_cmd(\n            self,\n            auxiliaries.count(table_name),\n            fetch=True,\n            hide=True\n        )[0][0]\n        col = len(self.get_columns(table_name))\n        return row, col\n\n    def get_schema_names(self) -> list:\n        \"\"\"\n        Get the names of schemas for a given connection.\n        The connection is associated with a database.\n\n        Returns\n        -------\n            List of schema names\n        \"\"\"\n\n        with self.engine.connect() as cursor:\n            res = cursor.execute(text('SELECT * FROM pg_catalog.pg_tables'))\n            data_frame = pd.DataFrame(res.fetchall())\n        list_ = list(set(data_frame.iloc[:, 0]))\n        list_ = [schema for schema in list_ if\n                 \"pg\" not in schema and \"schema\" not in schema]\n        return list_\n\n    def get_table_names(self, schema: str = \"public\") -> list:\n        \"\"\"\n        Get a list of table for a given schema\n\n        Parameters\n        ----------\n        schema: str\n            The name of the schema\n\n        Returns\n        -------\n            List of table names\n        \"\"\"\n        res = auxiliaries.execute_sql_cmd(\n            self,\n            f\"SELECT * FROM pg_catalog.pg_tables where schemaname='{schema}'\",\n            hide=True, fetch=True)\n        df_names = pd.DataFrame(res)\n        if len(df_names) == 0:\n            return []\n        return list(df_names.iloc[:, 1])\n\n    def get_view_names(self, schema=\"public\"):\n        \"\"\"\n        Get a list of views for a given schema\n\n        Parameters\n        ----------\n        schema: str\n            The name of the schema\n\n        Returns\n        -------\n            List of view names\n        \"\"\"\n        res = auxiliaries.execute_sql_cmd(\n            self, 'SELECT * FROM pg_catalog.pg_views',\n            hide=True, fetch=True)\n        df_names = pd.DataFrame(res)\n        df_names = df_names[df_names.iloc[:, 0] == schema]\n        return list(df_names.iloc[:, 1])\n\n    def _select(self, table_name: str, columns: Iterable[str] = None,\n                where: str = None,\n                limit: int = None):\n        \"\"\"\n        Select type query\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table on which to apply a select\n        columns: iterable of str\n            The name of the selected columns\n        where: str\n            Selection conditions according to the indexes\n        limit: int\n            The maximal number of row to select\n\n        Returns\n        -------\n            SQL query\n        \"\"\"\n        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n        if columns is not None:\n            columns = [col.lower() for col in columns if\n                       col in self.get_columns(table_name)]\n            columns = pd.Series(columns).drop_duplicates(keep='first')\n            query = \",\\n  \".join(columns)\n            query = \"SELECT \" + query + \" FROM \" + table_name\n        else:\n            query = \"SELECT * FROM \" + table_name\n\n        if where is not None:\n            query += \" WHERE \" + where\n\n        if limit is not None:\n            query += \" LIMIT \" + str(limit)\n        if self.active_sql_printing:\n            execution.print_(query)\n        return query\n\n    def update(self, table_name: str, where: str = \"\",\n               column: str = \"\", value: str = \"NULL\") -> None:\n        \"\"\"\n        Update values in table\n\n        Parameters\n        ----------\n        table_name: str\n            The name of the table\n        where: str\n            The location according to the indexes of the values to be modified\n        column: str\n            The column concerned\n        value\n            The value\n\n        Returns\n        -------\n            None\n        \"\"\"\n        if isinstance(value, str):\n            value = f\"'{value}'\"\n\n        sql_cmd = f\"UPDATE {table_name} SET {column}={value}\"\n        if where != \"\":\n            sql_cmd += f\" where {where}\"\n\n        auxiliaries.execute_sql_cmd(self, sql_cmd, fetch=False)\n\n    @staticmethod\n    def _get_schema(name):\n        return name.split(\".\")[0].lower() if \".\" in name else \"main\"\n\n    @staticmethod\n    def _get_name(name):\n        return name.split(\".\")[1].lower() if \".\" in name else name.lower()\n\n    @auxiliaries.execute\n    def create_table(self, name: str, columns: Iterable[str],\n                     dtypes: Iterable[str],\n                     column_comments: Dict[str, str] = None) -> str:\n        \"\"\"\n\n        Parameters\n        ----------\n        name: str\n            The name of the table\n        columns: iterable of str\n            The names of the columns\n        dtypes: iterable of str\n            The types of columns\n        column_comments : dict\n            The comment on columns `{column1: comment 1, ...}`\n\n        Returns\n        -------\n            SQL query\n        \"\"\"\n\n        return auxiliaries.create_table(name, columns, dtypes, column_comments)\n\n    @auxiliaries.execute\n    def drop(self, name: str, type_: str, column_name: str) -> str:\n        \"\"\"\n        Drop a column or object in table\n\n        Parameters\n        ----------\n        name: str\n            Object to alter\n        type_: str\n\n            Type of object to alter\n            - VIEW\n            - TABLE (default)\n\n        column_name: str\n            Column or object to drop\n\n        Returns\n        -------\n            SQL query\n        \"\"\"\n        return auxiliaries.drop(name, type_, column_name)\n\n    @auxiliaries.execute\n    def create_schema(self, name: str) -> str:\n        \"\"\"\n        Create a schema\n\n        Parameters\n        ----------\n        name: str\n            Name of the schema\n\n        Returns\n        -------\n            None\n        \"\"\"\n        return auxiliaries.create_schema(name)\n\n    def list_table(self, schema: str) -> pd.DataFrame:\n        \"\"\"\n        List tables provided a postregsql schema\n\n        Parameters\n        ----------\n        schema: str\n            Schema name\n\n        Returns\n        -------\n            :obj: `pandas.DataFrame` with table list and properties\n\n        \"\"\"\n        return pd.DataFrame(auxiliaries.execute_sql_cmd(\n            self, auxiliaries.list_table(schema),\n            fetch=True))\n\n    def execute(self, sql: str) -> None:\n        \"\"\"\n        Provided a query as string,\n        execute the query using SQLalchemy engine with psycopg2 driver\n\n        Parameters\n        ----------\n        sql: str\n            Some query to execute\n\n        Returns\n        -------\n            None\n        \"\"\"\n        auxiliaries.execute_sql_cmd(self, sql, hide=False)\n\n    # ==========================================================================\n    #                       Property of connexion\n    # ==========================================================================\n    @property\n    def has_postgis_extension(self) -> bool:\n        with self.engine.connect() as con:\n            out_ = con.execute(\n                text(\"SELECT COUNT(1) FROM information_schema.routines\"\n                     \" WHERE routine_name = 'postgis_version'\"))\n            has_postgis = next(out_)[0]\n        return has_postgis", "\n\ndef _iterable_or_str(arg) -> str:\n    if isinstance(arg, str):\n        arg = f\"({arg})\"\n    else:\n        arg = str(tuple(arg)).replace(\"'\", \"\")\n    return arg\n", ""]}
{"filename": "acrocord/test/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\n"]}
{"filename": "acrocord/test/constraints.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\nfrom typing import Type, Iterable\n\nimport pandas as pd\n\nfrom acrocord import ConnectDatabase", "\nfrom acrocord import ConnectDatabase\nfrom acrocord.utils.types import EligibleDataType\n\n\ndef not_nullable(data: pd.DataFrame, columns: Iterable[str]):\n    \"\"\" Function that verify if a column does not contains null values \"\"\"\n    for column in columns:\n        assert not data[column].isnull().values.any(), f\"Column\" \\\n            f\" {column} \" \\\n            f\"contains null values\"", "\n\ndef data_type(data: pd.DataFrame, columns: Iterable[str],\n              dtype: Type):\n    \"\"\" Function that verify if the type of column is correct \"\"\"\n    for column in columns:\n        assert data[column].dtype == dtype, \\\n            f\"Column {column} is {data[column].dtype}\"\n\n\ndef eligible_data_type(data: pd.DataFrame):\n    result = data.dtypes\n    for column, type_ in result.items():\n        msg = f'The type of the column {column} is {type_} ' \\\n              f'which is not an eligible type.'\n        assert type_ in EligibleDataType.get_list(), msg", "\n\ndef eligible_data_type(data: pd.DataFrame):\n    result = data.dtypes\n    for column, type_ in result.items():\n        msg = f'The type of the column {column} is {type_} ' \\\n              f'which is not an eligible type.'\n        assert type_ in EligibleDataType.get_list(), msg\n\n\ndef quantile(data: pd.DataFrame, columns: Iterable[str], q, threshold):\n    \"\"\" Function that verifies the quantile according to a threshold  \"\"\"\n    for column in columns:\n        assert data[column].quantile(q) < threshold", "\n\ndef quantile(data: pd.DataFrame, columns: Iterable[str], q, threshold):\n    \"\"\" Function that verifies the quantile according to a threshold  \"\"\"\n    for column in columns:\n        assert data[column].quantile(q) < threshold\n\n\ndef unique(data: pd.DataFrame, columns: Iterable[str]):\n    \"\"\" Function that checks if the values of a column is unique \"\"\"\n    for column in columns:\n        assert data[\n            column].nunique() == data.__len__(), f\"Column {column} is not unique\"", "def unique(data: pd.DataFrame, columns: Iterable[str]):\n    \"\"\" Function that checks if the values of a column is unique \"\"\"\n    for column in columns:\n        assert data[\n            column].nunique() == data.__len__(), f\"Column {column} is not unique\"\n\n\ndef nb_unique_index(data: pd.DataFrame, columns: Iterable[str], minimum,\n                    maximum):\n    \"\"\" Function that checks if the number of unique objects is included in a given interval \"\"\"\n\n    for column in columns:\n        msg = f\"Column {column} est compris dans l'intervalle \"\n        assert (data[column].nunique() < maximum) and (\n            data[column].nunique() >= minimum), msg", "\n\nclass DataConstraints:\n\n    def __init__(self, data: pd.DataFrame = None,\n                 connection: ConnectDatabase = None):\n        self.data = data.copy()\n        self.constraints = {}\n\n    def add_constraint(self, constraint: dict):\n        self.constraints = {**self.constraints, **constraint}\n\n    def test(self):\n        for name in self.constraints.keys():\n            function, args = self.constraints[name]\n            args[\"data\"] = self.data\n            function(**args)", ""]}
{"filename": "acrocord/utils/auxiliaries.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\nfrom functools import wraps\nfrom typing import Iterable, Dict\n\nfrom sqlalchemy import text\n", "from sqlalchemy import text\n\nfrom acrocord.misc import execution\n\n\ndef execute(cmd, hide=False, fetch=False):\n    @wraps(cmd)\n    def wrapper(*args, **kwargs):\n        cmd_sql = cmd(*args, **kwargs)\n        execute_sql_cmd(args[0], cmd_sql, hide=hide, fetch=fetch)\n\n    return wrapper", "\n\ndef execute_sql_cmd(connection, cmd: str, hide=False, fetch=False):\n    if not hide:\n        execution.print_(cmd)\n    with connection.engine.connect() as cursor:\n        res = cursor.execute(text(cmd))\n        cursor.commit()\n        if fetch:\n            ret = res.fetchall()\n            return ret", "\n\ndef merge(connection, table1, table2, out=\"tmp_merge\", out_type=\"VIEW\",\n          suffixe=(\"_x\", \"_y\"), on=\"\", left_on=\"\", right_on=\"\"):\n    out = f\"{connection._get_schema(out)}.{connection._get_name(out)}\"\n    out_save = out\n    if out in [table1, table2]:\n        out = f\"{connection._get_schema(out)}.tmp_merge\"\n    if left_on == \"\":\n        left_on = on\n        right_on = on\n    if isinstance(left_on, str):\n        right_on, left_on = [right_on], [left_on]\n    left_on, right_on = [l.lower() for l in left_on], [r.lower() for r in\n                                                       right_on]\n\n    all_columns = connection.get_columns(table1) + connection.get_columns(\n        table2)\n    col_intersect = set(connection.get_columns(table1)\n                        ).intersection(connection.get_columns(table2))\n    if suffixe != (\"_x\", \"_y\"):\n        col_intersect = all_columns\n\n    db_str = f\"CREATE {out_type} {out} AS (\\n  SELECT \"\n    for col in connection.get_columns(table1):\n        db_str += table1 + \".\" + col\n        if col in col_intersect and col not in left_on:\n            db_str += \" AS \" + col + suffixe[0]\n        db_str += \",\"\n    for col in connection.get_columns(table2):\n        if col not in left_on:\n            db_str += table2 + \".\" + col\n            if col in col_intersect:\n                db_str += \" AS \" + col + suffixe[1]\n            db_str += \",\"\n    db_str = db_str[:-1]\n    db_str += f\" FROM {table1},{table2}\"\n\n    left_on_ = [f\"{table1}.{l}\" for l in left_on]\n    right_on_ = [f\"{table1}.{r}\" for r in right_on]\n\n    left_on_ = \", \".join(left_on_)\n    right_on_ = \", \".join(right_on_)\n    db_str += \"\\n  WHERE (\" + left_on_ + \")=(\" + right_on_ + \"));\"\n    if connection.active_sql_printing:\n        print(db_str)\n    connection.drop_table(out, type_=out_type)\n    execute_sql_cmd(connection, db_str, hide=True)\n\n    if out_save in [table1, table2]:\n        connection.drop_table(out_save)\n        connection.replace(out, out_save, type_=out_type)", "\n\ndef count(table_name):\n    sql_cmd = f\"SELECT COUNT(*) FROM {table_name};\"\n    return sql_cmd\n\n\ndef drop_table(table_name, option=\"CASCADE\", type_=\"TABLE\"):\n    return f\"DROP {type_} IF EXISTS {table_name} {option};\"\n", "\n\ndef list_table(schema: str):\n    \"\"\"\n\n    Parameters\n    ----------\n    schema: str\n        Schema name\n\n    Returns\n    -------\n        SQL Query\n    \"\"\"\n    sql_cmd = f\"SELECT * FROM information_schema.tables \" \\\n              f\"WHERE table_schema = '{schema}'\"\n    return sql_cmd", "\n\ndef add(name, type_, column_name, dtype) -> str:\n    return f\"ALTER {type_} {name} ADD {column_name} {dtype}\"\n\n\ndef drop(name: str, type_: str, column_name: str):\n    \"\"\"\n    Drop a column or object in table\n\n    Parameters\n    ----------\n    name: str\n        Object to alter\n    type_: str\n\n        Type of object to alter\n        - VIEW\n        - TABLE (default)\n\n    column_name: str\n        Column or object to drop\n\n    Returns\n    -------\n        SQL Query\n    \"\"\"\n    return f\"ALTER {type_} {name} DROP {column_name} \"", "\n\ndef create_schema(name: str) -> str:\n    \"\"\"\n\n    Parameters\n    ----------\n    name: str\n        Name of the schema to create\n\n    Returns\n    -------\n        SQL Query\n    \"\"\"\n    return f\"CREATE SCHEMA IF NOT EXISTS {name}\"", "\n\ndef create_table(name: str, columns: Iterable[str], dtypes: Iterable[str],\n                 column_comments: Dict[str, str] = None) -> str:\n    \"\"\"\n\n    Parameters\n    ----------\n    name: str\n        The name of the table\n    columns: iterable of str\n        The names of the columns\n    dtypes: iterable of str\n        The types of columns\n    column_comments : dict\n        The comment on columns `{column1: comment 1, ...}`\n\n    Returns\n    -------\n        SQL Query\n    \"\"\"\n    col_zip = [f'{\" \" * 4}\"{c}\"{\" \" * max(1, (15 - len(c)))}' + db_data_type(\n        dtypes[i])\n        for i, c in enumerate(columns)]\n    sql_cmd = f\"CREATE TABLE {name}( \\n\"\n    sql_cmd += \",\\n\".join(col_zip) + \"\\n)\"\n    sql_cmd += '\\n;'\n    if column_comments is not None:\n        for col_name, col_comment in column_comments.items():\n            col_comment = col_comment.replace(\"'\", \"''\")\n            sql_cmd += f\"COMMENT ON COLUMN {name}\" \\\n                       f\".{col_name} IS '{col_comment}';\\n\"\n    return sql_cmd", "\n\n_conversion_table = {\"uint8\": \"int2\",\n                     \"uint16\": \"int4\",\n                     \"uint32\": \"int8\",\n                     \"int8\": \"int2\",\n                     \"Int8\": \"int2\",\n                     \"int16\": \"int2\",\n                     \"Int16\": \"int2\",\n                     \"float16\": \"float4\",", "                     \"Int16\": \"int2\",\n                     \"float16\": \"float4\",\n                     \"float32\": \"float8\",\n                     \"int32\": \"int4\",\n                     \"Int32\": \"int4\",\n                     \"int64\": \"int8\",\n                     \"Int64\": \"int8\",\n                     \"float64\": \"float8\",\n                     \"Float64\": \"float8\",\n                     \"float\": \"float8\",", "                     \"Float64\": \"float8\",\n                     \"float\": \"float8\",\n                     \"bytes_\": \"text\",\n                     \"object\": \"text\",\n                     \"nan\": \"text\",\n                     \"str\": \"text\",\n                     \"string\": \"text\",\n                     \"bool\": \"bool\",\n                     \"boolean\": \"bool\",\n                     \"datetime64\": \"timestamp\",", "                     \"boolean\": \"bool\",\n                     \"datetime64\": \"timestamp\",\n                     \"category\": 'int4'}\n\n_conversion_table_inv = {\n    'int2': 'int16', 'float4': 'float16', 'int4': 'int32', 'int8': 'int64',\n    'float8': 'float64', 'text': 'str', 'bool': 'bool',\n    'datetime64': 'timestamp'}\n\n\ndef db_data_type(data_type: str, invert: bool = False) -> str:\n    \"\"\"\n\n    Parameters\n    ----------\n    data_type: str\n        data type to translate into postgresql one\n\n    invert: bool\n        reciprocal conversion\n\n    Returns\n    -------\n\n    \"\"\"\n    if \"datetime\" in data_type:\n        return \"timestamp\"\n    if \"timestamp\" in data_type:\n        return \"datetime64[s]\"\n    data_type = str(data_type)\n\n    if invert:\n        return _conversion_table_inv[data_type]\n    return _conversion_table[data_type]", "\n\ndef db_data_type(data_type: str, invert: bool = False) -> str:\n    \"\"\"\n\n    Parameters\n    ----------\n    data_type: str\n        data type to translate into postgresql one\n\n    invert: bool\n        reciprocal conversion\n\n    Returns\n    -------\n\n    \"\"\"\n    if \"datetime\" in data_type:\n        return \"timestamp\"\n    if \"timestamp\" in data_type:\n        return \"datetime64[s]\"\n    data_type = str(data_type)\n\n    if invert:\n        return _conversion_table_inv[data_type]\n    return _conversion_table[data_type]", "\n\ndef get_metadata(table_name: str) -> str:\n    \"\"\"\n\n    Parameters\n    ----------\n    table_name: str\n        Name of the table from which the metadata should be retrieved\n\n    Returns\n    -------\n        SQL Query\n    \"\"\"\n    schema, table = table_name.split('.')\n    sql_cmd = f\"\"\"\n        select\n            table_cols.table_schema,\n            table_cols.table_name,\n            table_cols.column_name,\n            pgd.description\n        from pg_catalog.pg_statio_all_tables as st\n        full outer join pg_catalog.pg_description pgd on (\n            pgd.objoid = st.relid\n        )\n        full outer join information_schema.columns table_cols on (\n            pgd.objsubid   = table_cols .ordinal_position and\n            table_cols.table_schema = st.schemaname and\n            table_cols.table_name   = st.relname\n        )\n        where\n        table_cols.table_schema = '{schema}' and\n        table_cols.table_name = '{table}' \"\"\"\n    return sql_cmd", ""]}
{"filename": "acrocord/utils/types.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\n\"\"\"\nThis module aims  to specify data types that are consistent with\nthe usage of :object:`pypgsql`\n\"\"\"\nimport warnings", "\"\"\"\nimport warnings\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\n\n\nclass EligibleDataType:\n\n    # nullable integers\n    INTEGER_64 = pd.Int64Dtype()  # [-9_223_372_036_854_775_808   to      9_223_372_036_854_775_807]\n    INTEGER_32 = pd.Int32Dtype()  # [-2_147_483_648               to      2_147_483_647]\n    INTEGER_16 = pd.Int16Dtype()  # [-32_768                      to      32_767]\n    INTEGER_8 = pd.Int8Dtype()  # [-128                         to      127]\n    INTEGER = INTEGER_64\n\n    # float\n    FLOAT_16 = np.float16\n    FLOAT_32 = np.float32\n    FLOAT_64 = np.float64\n    FLOAT = FLOAT_64\n\n    # nullable boolean\n    BOOLEAN = pd.BooleanDtype()\n\n    STRING = pd.StringDtype()\n\n    # time\n    DATE_TIME = np.dtype('datetime64[ns]')\n\n    @classmethod\n    def get_list(cls) -> List:\n        \"\"\"\n\n        Returns\n        -------\n            list of eligible data types\n        \"\"\"\n        return [\n            cls.BOOLEAN,\n            cls.INTEGER,\n            cls.INTEGER_8,\n            cls.INTEGER_16,\n            cls.INTEGER_32,\n            cls.INTEGER_64,\n            cls.FLOAT_16,\n            cls.FLOAT_32,\n            cls.FLOAT_64,\n            cls.FLOAT,\n            cls.BOOLEAN,\n            cls.STRING,\n            cls.DATE_TIME\n        ]", "class EligibleDataType:\n\n    # nullable integers\n    INTEGER_64 = pd.Int64Dtype()  # [-9_223_372_036_854_775_808   to      9_223_372_036_854_775_807]\n    INTEGER_32 = pd.Int32Dtype()  # [-2_147_483_648               to      2_147_483_647]\n    INTEGER_16 = pd.Int16Dtype()  # [-32_768                      to      32_767]\n    INTEGER_8 = pd.Int8Dtype()  # [-128                         to      127]\n    INTEGER = INTEGER_64\n\n    # float\n    FLOAT_16 = np.float16\n    FLOAT_32 = np.float32\n    FLOAT_64 = np.float64\n    FLOAT = FLOAT_64\n\n    # nullable boolean\n    BOOLEAN = pd.BooleanDtype()\n\n    STRING = pd.StringDtype()\n\n    # time\n    DATE_TIME = np.dtype('datetime64[ns]')\n\n    @classmethod\n    def get_list(cls) -> List:\n        \"\"\"\n\n        Returns\n        -------\n            list of eligible data types\n        \"\"\"\n        return [\n            cls.BOOLEAN,\n            cls.INTEGER,\n            cls.INTEGER_8,\n            cls.INTEGER_16,\n            cls.INTEGER_32,\n            cls.INTEGER_64,\n            cls.FLOAT_16,\n            cls.FLOAT_32,\n            cls.FLOAT_64,\n            cls.FLOAT,\n            cls.BOOLEAN,\n            cls.STRING,\n            cls.DATE_TIME\n        ]", "\n\ndef warning_type(dataframe: pd.DataFrame, verbose: int):\n    for column, column_type in dataframe.dtypes.items():\n        if verbose == 2:\n            if str(column_type) not in EligibleDataType.get_list():\n                return warnings.warn(\n                    \"The types of this dataframe are not eligible types\")\n        if verbose > 2:\n            if str(column_type) not in EligibleDataType.get_list():\n                msg = f\"The type of the column {column} \" \\\n                      f\"is {column_type} which is not an eligible type.\"\n                warnings.warn(msg)", ""]}
{"filename": "acrocord/utils/logger.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\n\nfrom datetime import datetime\nfrom typing import Union\nfrom typing import Optional\nfrom acrocord.misc.execution import execution_time", "from typing import Optional\nfrom acrocord.misc.execution import execution_time\nfrom acrocord.misc.execution import print_\nfrom acrocord.databasepg import ConnectDatabase\n\nimport warnings\nimport pandas as pd\n\nwarnings.filterwarnings('ignore')\n", "warnings.filterwarnings('ignore')\n\n__DEFAULT_COL_NAME__ = ['date', 'value', 'message', 'other_info']\n__DEFAULT_COL_TYPE__ = ['datetime', 'str', 'str', 'str']\n\n\nclass Logger:\n\n    def __init__(self, connection: ConnectDatabase, table_name: str, schema: str = 'public', columns=None,\n                 column_types=None):\n        \"\"\"\n        Initialize class logger to write log messages\n        Create a new table to write logs if it doesn't already exist\n\n        Parameters\n        ----------\n        connection: ConnectDatabase's instance\n            The connection instance, allowing to connect to a specific database\n        table_name: str\n            The table name in the database\n        schema: str\n            The schema name in the database, default is 'public'\n        columns: Iterable of str\n            columns names\n        column_types: Iterable if str\n            The columns types\n        \"\"\"\n\n        if column_types is None:\n            column_types = __DEFAULT_COL_TYPE__\n        if columns is None:\n            columns = __DEFAULT_COL_NAME__\n\n        self.db_connection = connection\n        self.table_name = f\"{schema}.{table_name}\"\n        print_(f\"{self.table_name = }\", color='OKBLUE')\n\n        if table_name in self.db_connection.get_table_names(schema):\n            print_(f\"\\nTable {table_name} ALREADY EXIST, do not create a new one\", color='OKBLUE')\n            columns = self.db_connection.get_columns(self.table_name)\n            column_types = self.db_connection.get_dtypes(self.table_name)\n        else:\n            print_(f\"CREATE new table with {table_name = }\", color='OKGREEN')\n            self.db_connection.create_table(name=self.table_name, columns=columns, dtypes=column_types)\n\n        self.columns = columns\n        self.column_types = column_types\n\n    @execution_time\n    def write_log(self, data: Union[pd.Series, pd.DataFrame, dict]) -> Optional[ValueError]:\n        \"\"\"\n        Main function to write log messages\n\n        Parameters\n        ----------\n        data: pd.Series or pd.Dataframe or dictionary\n            The log to be written to the table, specifically:\n                - pd.Series: all values are written into 'value' column\n                - pd.DataFrame: only data belonging to predefined columns are written\n                - dict: only one log (one value for each key) is written\n        Returns\n        -------\n            None\n        \"\"\"\n\n        if isinstance(data, pd.Series):\n            return self._write_log(self._series2df(data))\n        elif isinstance(data, dict):\n            return self._write_log(self._dict2df(data))\n        elif isinstance(data, pd.DataFrame):\n            return self._write_log(data.copy())\n        else:\n            print_(f\"\\nError of data type = {type(data)}\", color='WARNING')\n            raise ValueError(\"Input data should be type of [pd.Series, pd.DataFrame, or dict]\")\n\n    @staticmethod\n    def _series2df(data: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Convert pd.Series into pd.Dataframe\n\n        Parameters\n        ----------\n        data: pd.Series\n            The value to be converted\n\n        Returns\n        -------\n            pd.DataFrame\n        \"\"\"\n        return data.to_frame(name='value')\n\n    @staticmethod\n    def _dict2df(data: dict) -> pd.DataFrame:\n        \"\"\"\n        Convert a dictionary into pd.Dataframe\n\n        Parameters\n        ----------\n        data: dict\n            The data to be converted, only one value for each key\n\n        Returns\n        -------\n            pd.DataFrame\n        \"\"\"\n        return pd.DataFrame(data, index=[0])\n\n    def _write_log(self, log_message: pd.DataFrame) -> None:\n        \"\"\"\n\n        Parameters\n        ----------\n        log_message: pd.DataFrame\n            The log message to be written to the predefined table\n\n        Returns\n        -------\n            None\n        \"\"\"\n        input_cols = list(log_message.columns)\n        cols_write = list(set(self.columns) & set(input_cols))\n        cols_null = list(set(self.columns) - set(cols_write))\n        print_(f\"\\n{cols_write = }\\n{cols_null = }\", color='BOLD')\n\n        df_write = pd.DataFrame()\n        df_write[cols_write] = log_message[cols_write]\n        df_write[cols_null] = ''\n\n        if 'date' in cols_write:\n            df_write['date'] = pd.to_datetime(df_write['date']).strftime(\"%Y-%m-%d %H:%M:%S\")\n        else:\n            df_write['date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        df_write.reset_index(drop=True, inplace=True)\n        self.db_connection.insert(data=df_write, table_name=self.table_name)\n\n    def print_info(self):\n        print_(f\"\\ntable_name: {self.table_name}\"\n               f\"\\ncolumns: {self.columns}\"\n               f\"\\ncolumns types: \\n{self.column_types}\", color='Magenta')", ""]}
{"filename": "acrocord/utils/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\n"]}
{"filename": "acrocord/utils/monitoring.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\ndef print_date(connection, schema: str, table: str):\n    from acrocord.misc import execution\n    with execution.silence_stdout():\n        data = connection.read_table(\"public.informations_table\").query(\n            f\"nom=='{table}'\").query(f\"schema=='{schema}'\")\n    if len(data) > 0:\n        try:\n            msg = \"[info] creation date :\"\n            msg += \"-\" * (40 - len(msg)) + \"  \"\n            msg += data[\"date_creation\"].iloc[0][:-10]\n            execution.print_(msg, \"Grey\")\n            msg = \"[info] author name :\"\n            msg += \"-\" * (40 - len(msg)) + \"  \"\n            msg += data[\"auteur_creation\"].iloc[0]\n            execution.print_(msg, \"Grey\")\n        except TypeError as error:\n            execution.print_(error, color=\"Red\")", ""]}
{"filename": "acrocord/utils/connect.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\nimport getpass\nimport glob\nimport os\nimport subprocess\nimport sys", "import subprocess\nimport sys\nfrom os.path import expanduser, abspath\n\nimport asyncpg\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nhome = expanduser(\"~\")\npath_postgresql = f\"{home}/.postgresql/\"", "home = expanduser(\"~\")\npath_postgresql = f\"{home}/.postgresql/\"\npath_acr = f\"{home}/.acrocord/\"\n\n\ndef load_available_connections():\n    import configparser\n    default = configparser.ConfigParser()\n\n    connection_dict = {}\n    for path in [path_acr, path_postgresql]:\n        abs_path = abspath(path)\n\n        dir_path = f\"{abs_path}/connections.cfg\"\n        default.read(dir_path)\n        connection_dict = {**connection_dict, **default._sections}\n\n        try:\n            _ = pd.DataFrame(connection_dict).drop(\"password\").T\n        except KeyError:\n            pass\n    for connection in connection_dict.keys():\n        try:\n            if connection_dict[connection][\"password\"] == '':\n                connection_dict[connection][\"password\"] = ' '\n        except KeyError:\n            pass\n    return connection_dict", "\n\ndef password_auth():\n    path = os.path.abspath(__file__).replace(os.path.basename(__file__), \"\")\n\n    cmd = \"python \" + path + \"getpassword.py \" + path_postgresql\n\n    if not os.path.exists(path_postgresql):\n        os.mkdir(path_postgresql)\n    if sys.platform == \"win32\":\n        with subprocess.Popen(cmd, shell=True,\n                              stderr=subprocess.PIPE,\n                              stdout=subprocess.PIPE):\n            pass\n    elif sys.platform == 'linux':\n        subprocess.call(['xterm', \"-e\", cmd])\n    list_of_files = glob.glob(path_postgresql + '*')\n    latest_file = max(list_of_files, key=os.path.getctime)\n\n    with open(latest_file) as file:\n        password = file.read()\n        file.close()\n    os.remove(latest_file)\n    return password", "\n\ndef connect_psql_server(username=getpass.getuser(), async_=False, **kwargs):\n\n    connection_param = kwargs[\"connection\"].copy()\n\n    if \"password\" not in connection_param.keys():\n        connection_param[\"password\"] = \"\"\n\n    if username != getpass.getuser():\n        connection_param[\"user\"] = username\n\n    if connection_param[\n            \"password\"] == \"\" and \"sslmode\" not in connection_param.keys():\n        connection_param[\"password\"] = password_auth()\n\n    if \"sslmode\" in connection_param.keys():\n        connect_args = {'sslmode': connection_param[\"sslmode\"]}\n\n    else:\n        connect_args = {}\n\n    sql_cmd = 'postgresql+psycopg2://'\n    sql_cmd += connection_param[\"user\"] + ':' + connection_param[\n        \"password\"] + '@' + connection_param[\"host\"] + ':'\n    sql_cmd += str(connection_param[\"port\"]) + '/' + connection_param[\"dbname\"]\n    if async_:\n        return connect_async(sql_cmd)\n    return create_engine(sql_cmd, connect_args=connect_args)", "\n\nasync def connect_async(sql_cmd):\n    conn = await asyncpg.connect(\n        sql_cmd.replace(\"postgresql+psycopg2\", \"postgresql\"))\n    return conn\n"]}
{"filename": "acrocord/utils/getpassword.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\nimport getpass\nimport sys\nimport tempfile\n\n\ndef pass_(path=\".\"):\n    \"\"\"\n\n    Parameters\n    ----------\n    path\n\n    Returns\n    -------\n\n    \"\"\"\n    password = getpass.getpass(prompt=\"Enter the Password:\")\n    tmp = tempfile.NamedTemporaryFile(dir=path, delete=False)\n    tmp.write(bytearray(password, encoding=\"utf-8\"))", "\n\ndef pass_(path=\".\"):\n    \"\"\"\n\n    Parameters\n    ----------\n    path\n\n    Returns\n    -------\n\n    \"\"\"\n    password = getpass.getpass(prompt=\"Enter the Password:\")\n    tmp = tempfile.NamedTemporaryFile(dir=path, delete=False)\n    tmp.write(bytearray(password, encoding=\"utf-8\"))", "\n\ndef pass_win():\n    return getpass.win_getpass(prompt=\"Enter the Password:\", stream=sys.stdout)\n\n\nif __name__ == \"__main__\":\n    pass_(sys.argv[1])\n", ""]}
{"filename": "acrocord/utils/insert.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\nimport io\nfrom typing import Iterator, Optional\n\nimport pandas as pd\n", "import pandas as pd\n\nfrom acrocord.misc import execution\n\n\n@execution.execution_time\ndef insert(connection: \"ConnectDatabase\", data: pd.DataFrame, table_name: str,\n           chunksize: int = 1000):\n    storage = io.StringIO()\n    data.to_csv(storage, sep='\\t', header=False,\n                index=False, encoding=\"utf8\", chunksize=chunksize)\n    storage.seek(0)\n    sii = StringIteratorIO(storage)\n    connection = connection.engine.connect()\n    columns = tuple(f'{c}' for c in data.columns)\n\n    with connection.connection.cursor() as cursor:\n        cursor.execute(f'SET search_path TO {table_name.split(\".\")[0]}')\n        cursor.copy_from(sii,\n                         table_name.split(\".\")[1], null=\"\",\n                         columns=columns, sep=\"\\t\")\n        cursor.connection.commit()\n        cursor.close()", "\n\nclass StringIteratorIO(io.TextIOBase):\n    def __init__(self, iter_: Iterator[str]):\n        self._iter = iter_\n        self._buff = ''\n\n    def readable(self) -> bool:\n        return True\n\n    def _read1(self, n_line: Optional[int] = None) -> str:\n        while not self._buff:\n            try:\n                self._buff = next(self._iter)\n            except StopIteration:\n                break\n        ret = self._buff[:n_line]\n        self._buff = self._buff[len(ret):]\n        return ret\n\n    def read(self, n: Optional[int] = None) -> str:\n        line = []\n        if n is None or n < 0:\n            while True:\n                line_str = self._read1()\n                if not line_str:\n                    break\n                line.append(line_str)\n        else:\n            while n > 0:\n                line_str = self._read1(n)\n                if not line_str:\n                    break\n                n -= len(line_str)\n                line.append(line_str)\n        return ''.join(line)", ""]}
{"filename": "acrocord/factory/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\nfrom .table import TableFactory\n\n__all__ = [\"TableFactory\"]\n", ""]}
{"filename": "acrocord/factory/table.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os.path\nfrom abc import ABC\nfrom abc import abstractmethod\nfrom typing import Dict\nfrom typing import Optional", "from typing import Dict\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import Union\n\nimport pandas as pd\nimport xlsxwriter\nfrom pandas.core.dtypes.base import ExtensionDtype", "import xlsxwriter\nfrom pandas.core.dtypes.base import ExtensionDtype\nfrom termcolor import colored\nfrom termcolor import cprint\n\nfrom acrocord import ConnectDatabase\n\n\nclass TableFactory(ABC):\n    \"\"\"\n    Description\n    -----------\n\n    Abstract class for table creation and interaction with database.\n\n    Usage\n    -----\n\n    If you want to use the static class you can call :\n\n    >>> table_factory = TableFactory.get_instance()\n\n    Otherwise you can create your own instance:\n\n    >>> table_factory= TableFactory()\n\n    Then you can access to the table by calling\n\n    >>> table_factory.get_table()\n\n    If you want to save the table to the database you can call:\n\n    >>> table_factory.write_table(...)\n\n    Then you can read the table from the same database using:\n\n    >>> table_factory.read_table(...)\n    \"\"\"\n    _instance: \"TableFactory\" = None\n\n    @classmethod\n    @abstractmethod\n    def data_definition(cls) -> Dict[str, Tuple[ExtensionDtype, str]]:\n        \"\"\"\n        Returns\n        -------\n        dict\n            Column datatypes and description. The dict follow this structure\n            {'column_name': (dtype, ' description')}\n\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def get_db_connection(cls):\n        ...\n\n    @classmethod\n    @abstractmethod\n    def table_name(cls) -> str:\n        \"\"\"\n        Returns\n        -------\n        str\n            the database table name\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def schema_name(cls) -> str:\n        \"\"\"\n        Returns\n        -------\n        str\n            the database schema name\n        \"\"\"\n\n    @classmethod\n    def read_table(cls,\n                   connection: Union[dict, str, ConnectDatabase] = None,\n                   schema_name: str = None,\n                   table_name: str = None,\n                   columns: iter = None,\n                   where: str = None\n                   ) -> pd.DataFrame:\n        \"\"\"\n        if exists, read the table from database.\n        Returns\n        -------\n        pd.DataFrame\n            the table\n        \"\"\"\n        schema_name: str = cls.schema_name() if schema_name is None else schema_name\n        table_name: str = cls.table_name() if table_name is None else table_name\n        columns = list(\n            cls.data_definition().keys()) if columns is None else columns\n\n        if isinstance(connection, ConnectDatabase):\n            connection_ = connection\n            close_db = False\n        else:\n            connection_ = cls.get_db_connection()\n            close_db = True\n        table = connection_.read_table(\n            f'{schema_name}.{table_name}', columns=columns,\n            where=where)\n\n        # assert all columns and oly columns are retrieved\n        assert table.columns.isin(columns).all() and pd.Index(columns).isin(\n            table.columns).all()\n\n        # convert to correct datatype\n        for col in columns:\n            dtype = cls.data_definition()[col][0]\n            table[col] = table[col].astype(dtype)\n\n        if close_db:\n            connection_.close()\n        return table\n\n    @classmethod\n    @abstractmethod\n    def id_key(cls) -> Optional[Union[str, Sequence[str]]]:\n        \"\"\"\n        Return Primary key column name, or None\n        \"\"\"\n        ...\n\n    @classmethod\n    def get_column_description(cls) -> dict:\n        \"\"\"\n        Returns\n        -------\n        dict\n            dictionary with name of column as key and its description as value\n        \"\"\"\n        doc = cls.data_definition()\n        return {col_name: col_data[1] for col_name, col_data in doc.items()}\n\n    @classmethod\n    def get_instance(cls) -> \"TableFactory\":\n        \"\"\"\n        Static instance getter to avoid recomputing the table for multiple usages.\n        Returns\n        -------\n        TableFactory\n            static table factory instance\n        \"\"\"\n\n        if cls._instance is None:\n            cls._instance = cls()\n        return cls._instance\n\n    @classmethod\n    def data_description(cls) -> pd.DataFrame:\n        \"\"\"\n\n        Returns\n        -------\n        pd.DataFrame\n            a dataframe with table documentation\n        \"\"\"\n        return pd.DataFrame(cls.data_definition(),\n                            index=['data type', 'column description']).T\n\n    @classmethod\n    def get_foreign_keys(cls) -> Dict[str, Tuple[Type['TableFactory'], str]]:\n        \"\"\"\n        return each foreign keys to be implemented in the following format:\n        {key in current TableFactory columns (str) :\n        (foreign table (TableFactory), foreign key (str) )}\n        \"\"\"\n        return {}\n\n    @classmethod\n    def get_full_name(cls):\n        return f\"{cls.schema_name()}.{cls.table_name()}\"\n\n    def __init__(self, verbose: bool = True):\n        self._verbose: bool = verbose\n        # does the table have been saved into the deployment database\n        self._is_deployed: bool = False\n        self._table: Optional[pd.DataFrame] = None\n        self.columns = list(self.data_definition().keys())\n\n    @abstractmethod\n    def _create_table(self) -> None:\n        \"\"\"\n        compute and set the table in the self._table attribute\n        Returns\n        -------\n        None\n        \"\"\"\n        self._set_table(pd.DataFrame())\n\n    def get_table(self, columns=None) -> pd.DataFrame:\n        \"\"\"\n        Returns\n        -------\n        pd.DataFrame\n            table g\u00e9n\u00e9r\u00e9e par la fabrique\n        \"\"\"\n        if self._is_deployed:\n            return self.read_table(self.get_db_connection(), columns=columns)\n        if self._table is None:\n            self._create_table()\n        return self._table.copy() if columns is None else self._table[\n            columns].copy()\n\n    def _set_table(self, table) -> None:\n        \"\"\"\n        save table in self._table variable after checking its consistency with documentation\n        Parameters\n        ----------\n        table\n\n        Returns\n        -------\n\n        \"\"\"\n        table_ = table.rename(columns=str.lower)\n        data_definition = self.data_definition()\n        self.check_table_doc(table_, data_definition, self.__class__)\n        self._table = table_[data_definition.keys()]\n\n    def write_table(self, db: Optional[ConnectDatabase] = None,\n                    schema: str = None, table_name: str = None,\n                    **kwargs) -> None:\n        \"\"\"\n        Write table to database\n\n        Parameters\n        ----------\n        db : ConnectDatabase\n            the database connection\n        schema : str\n            name of the schema\n        table_name : str\n            name of the tabler\n        kwargs\n            additional arguments for ConnectDatabase.write_table\n\n        Returns\n        -------\n\n        \"\"\"\n\n        if db is None:\n            db = self.get_db_connection()\n\n        schema = self.schema_name() if schema is None else schema\n        table_name = self.table_name() if table_name is None else table_name\n        full_name = f\"{schema}.{table_name}\"\n        if self._verbose:\n            cprint(f\"[write_table] {full_name}\", 'white', attrs=['reverse', ])\n        data = self.get_table()\n        db.write_table(data, table_name=full_name,\n                       column_comments=self.get_column_description(),\n                       opt_dtype=False,\n                       primary_key=self.id_key(), **kwargs)\n\n        self._is_deployed = True\n        self._table = None\n\n    @staticmethod\n    def check_table_doc(table: pd.DataFrame, doc: dict, class_warning=None):\n        \"\"\"\n        check table and documentation consistency\n        Parameters\n        ----------\n        table\n        doc\n        class_warning\n            the name of the class to print for warnings\n        Returns\n        -------\n\n        \"\"\"\n        if class_warning is None:\n            class_warning = TableFactory.__class__\n        warning_header = f\"WARNING: {class_warning} \"\n        doc_cols = doc.keys()\n        table_cols = table.columns\n\n        if table_cols.isin(doc_cols).all() and len(doc_cols) == len(table_cols):\n            for col_name, val in doc.items():\n                col_dtype, col_doc = val\n                try:\n                    table[col_name] = table[col_name].astype(col_dtype)\n                except Exception as e:\n                    cprint(f\"{warning_header} {col_name}: \"\n                           f\"cannot convert {col_name} \"\n                           f\"from {table[col_name].dtype} to {col_dtype}.\",\n                           'yellow')\n                    cprint(f\"{e}\", 'yellow')\n                if col_doc is None or col_doc == \"\":\n                    cprint(warning_header +\n                           f\"missing description of {col_name}.\",\n                           'yellow')\n        else:\n            doc_cols = pd.Series(doc_cols)\n            table_cols = pd.Series(table_cols)\n\n            missing_doc = table_cols[~(table_cols.isin(doc_cols))]\n            if len(missing_doc) > 0:\n                cprint(\n                    warning_header + f\" column statement is missing:\\n{missing_doc}\",\n                    'yellow')\n\n            missing_col = doc_cols[~(doc_cols.isin(table_cols))]\n            if len(missing_col) > 0:\n                cprint(warning_header + f\" data is missing :\\n{missing_col}\",\n                       'yellow')\n\n    def write_to_excel(self, save: bool = True, path: str = None,\n                       file_name: str = None, verbose=True) -> 'xlsxwriter':\n        if file_name is None:\n            file_name = self.table_name() + '.xlsx'\n        elif len(file_name) <= 5 or file_name[-5:] != '.xlsx':\n            file_name += '.xlsx'\n        if path is None:\n            import tempfile\n            path = tempfile.gettempdir()\n\n        data = self.get_table()\n        full_path = os.path.join(path, file_name)\n        writer = pd.ExcelWriter(full_path, engine='xlsxwriter')\n        doc = self.data_description()[\n            ['column description']].reset_index().rename(\n            columns={'index': 'column name'})\n\n        for df, sheet_name, save_index in (\n                (doc, 'Column description', False), (data, 'Data', False)):\n            if verbose:\n                print(f\"Writing '{sheet_name}' Excel sheet...\", end='')\n            df.to_excel(writer, sheet_name=sheet_name, index=save_index)\n            if verbose:\n                print(f\"\\rWriting '{sheet_name}' Excel sheet\" + colored(\" ok\",\n                                                                        'green'))\n            if verbose:\n                print(\"adjust column width \" + sheet_name + \"...\", end='')\n            worksheet = writer.sheets[sheet_name]\n            worksheet.autofilter(0, 0, 0, len(df.columns) - 1)\n            for idx, col in enumerate(df):  # loop through all columns\n                series = df[col]\n                max_len = max((\n                    series.astype(str).map(len).max(),  # len of largest item\n                    len(str(col)) + 2  # len of column name/header +2 for bold\n                )) + 1  # adding a little extra space\n                worksheet.set_column(idx, idx, max_len)\n            if verbose:\n                print(\"\\radjust column width \" + sheet_name + colored(\" ok\",\n                                                                      'green'))\n        if save:\n            if verbose:\n                print(\"save excel...\", end='')\n            if hasattr(writer, \"save\"):\n                writer.save()\n            else:\n                writer._save()\n            if verbose:\n                print(\n                    f\"ok -> {colored(full_path, 'blue', attrs=['underline'])}\")\n        return writer\n\n    def add_foreign_keys(self, db: ConnectDatabase = None):\n        if db is None:\n            db = self.get_db_connection()\n        full_table_name = f\"{self.schema_name()}.{self.table_name()}\"\n        for key, (\n                foreign_table, foreign_key) in self.get_foreign_keys().items():\n            foreign_table_full_name = f\"{foreign_table.schema_name()}\" \\\n                                      f\".{foreign_table.table_name()}\"\n            print(\n                f\"{full_table_name} ({key}) -- > ({foreign_key}) \"\n                f\"{foreign_table_full_name} \")\n            db.add_key(type_='foreign',\n                       table_name=full_table_name, key=key,\n                       table_foreign=foreign_table_full_name,\n                       key_foreign=foreign_key,\n                       )", "class TableFactory(ABC):\n    \"\"\"\n    Description\n    -----------\n\n    Abstract class for table creation and interaction with database.\n\n    Usage\n    -----\n\n    If you want to use the static class you can call :\n\n    >>> table_factory = TableFactory.get_instance()\n\n    Otherwise you can create your own instance:\n\n    >>> table_factory= TableFactory()\n\n    Then you can access to the table by calling\n\n    >>> table_factory.get_table()\n\n    If you want to save the table to the database you can call:\n\n    >>> table_factory.write_table(...)\n\n    Then you can read the table from the same database using:\n\n    >>> table_factory.read_table(...)\n    \"\"\"\n    _instance: \"TableFactory\" = None\n\n    @classmethod\n    @abstractmethod\n    def data_definition(cls) -> Dict[str, Tuple[ExtensionDtype, str]]:\n        \"\"\"\n        Returns\n        -------\n        dict\n            Column datatypes and description. The dict follow this structure\n            {'column_name': (dtype, ' description')}\n\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def get_db_connection(cls):\n        ...\n\n    @classmethod\n    @abstractmethod\n    def table_name(cls) -> str:\n        \"\"\"\n        Returns\n        -------\n        str\n            the database table name\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def schema_name(cls) -> str:\n        \"\"\"\n        Returns\n        -------\n        str\n            the database schema name\n        \"\"\"\n\n    @classmethod\n    def read_table(cls,\n                   connection: Union[dict, str, ConnectDatabase] = None,\n                   schema_name: str = None,\n                   table_name: str = None,\n                   columns: iter = None,\n                   where: str = None\n                   ) -> pd.DataFrame:\n        \"\"\"\n        if exists, read the table from database.\n        Returns\n        -------\n        pd.DataFrame\n            the table\n        \"\"\"\n        schema_name: str = cls.schema_name() if schema_name is None else schema_name\n        table_name: str = cls.table_name() if table_name is None else table_name\n        columns = list(\n            cls.data_definition().keys()) if columns is None else columns\n\n        if isinstance(connection, ConnectDatabase):\n            connection_ = connection\n            close_db = False\n        else:\n            connection_ = cls.get_db_connection()\n            close_db = True\n        table = connection_.read_table(\n            f'{schema_name}.{table_name}', columns=columns,\n            where=where)\n\n        # assert all columns and oly columns are retrieved\n        assert table.columns.isin(columns).all() and pd.Index(columns).isin(\n            table.columns).all()\n\n        # convert to correct datatype\n        for col in columns:\n            dtype = cls.data_definition()[col][0]\n            table[col] = table[col].astype(dtype)\n\n        if close_db:\n            connection_.close()\n        return table\n\n    @classmethod\n    @abstractmethod\n    def id_key(cls) -> Optional[Union[str, Sequence[str]]]:\n        \"\"\"\n        Return Primary key column name, or None\n        \"\"\"\n        ...\n\n    @classmethod\n    def get_column_description(cls) -> dict:\n        \"\"\"\n        Returns\n        -------\n        dict\n            dictionary with name of column as key and its description as value\n        \"\"\"\n        doc = cls.data_definition()\n        return {col_name: col_data[1] for col_name, col_data in doc.items()}\n\n    @classmethod\n    def get_instance(cls) -> \"TableFactory\":\n        \"\"\"\n        Static instance getter to avoid recomputing the table for multiple usages.\n        Returns\n        -------\n        TableFactory\n            static table factory instance\n        \"\"\"\n\n        if cls._instance is None:\n            cls._instance = cls()\n        return cls._instance\n\n    @classmethod\n    def data_description(cls) -> pd.DataFrame:\n        \"\"\"\n\n        Returns\n        -------\n        pd.DataFrame\n            a dataframe with table documentation\n        \"\"\"\n        return pd.DataFrame(cls.data_definition(),\n                            index=['data type', 'column description']).T\n\n    @classmethod\n    def get_foreign_keys(cls) -> Dict[str, Tuple[Type['TableFactory'], str]]:\n        \"\"\"\n        return each foreign keys to be implemented in the following format:\n        {key in current TableFactory columns (str) :\n        (foreign table (TableFactory), foreign key (str) )}\n        \"\"\"\n        return {}\n\n    @classmethod\n    def get_full_name(cls):\n        return f\"{cls.schema_name()}.{cls.table_name()}\"\n\n    def __init__(self, verbose: bool = True):\n        self._verbose: bool = verbose\n        # does the table have been saved into the deployment database\n        self._is_deployed: bool = False\n        self._table: Optional[pd.DataFrame] = None\n        self.columns = list(self.data_definition().keys())\n\n    @abstractmethod\n    def _create_table(self) -> None:\n        \"\"\"\n        compute and set the table in the self._table attribute\n        Returns\n        -------\n        None\n        \"\"\"\n        self._set_table(pd.DataFrame())\n\n    def get_table(self, columns=None) -> pd.DataFrame:\n        \"\"\"\n        Returns\n        -------\n        pd.DataFrame\n            table g\u00e9n\u00e9r\u00e9e par la fabrique\n        \"\"\"\n        if self._is_deployed:\n            return self.read_table(self.get_db_connection(), columns=columns)\n        if self._table is None:\n            self._create_table()\n        return self._table.copy() if columns is None else self._table[\n            columns].copy()\n\n    def _set_table(self, table) -> None:\n        \"\"\"\n        save table in self._table variable after checking its consistency with documentation\n        Parameters\n        ----------\n        table\n\n        Returns\n        -------\n\n        \"\"\"\n        table_ = table.rename(columns=str.lower)\n        data_definition = self.data_definition()\n        self.check_table_doc(table_, data_definition, self.__class__)\n        self._table = table_[data_definition.keys()]\n\n    def write_table(self, db: Optional[ConnectDatabase] = None,\n                    schema: str = None, table_name: str = None,\n                    **kwargs) -> None:\n        \"\"\"\n        Write table to database\n\n        Parameters\n        ----------\n        db : ConnectDatabase\n            the database connection\n        schema : str\n            name of the schema\n        table_name : str\n            name of the tabler\n        kwargs\n            additional arguments for ConnectDatabase.write_table\n\n        Returns\n        -------\n\n        \"\"\"\n\n        if db is None:\n            db = self.get_db_connection()\n\n        schema = self.schema_name() if schema is None else schema\n        table_name = self.table_name() if table_name is None else table_name\n        full_name = f\"{schema}.{table_name}\"\n        if self._verbose:\n            cprint(f\"[write_table] {full_name}\", 'white', attrs=['reverse', ])\n        data = self.get_table()\n        db.write_table(data, table_name=full_name,\n                       column_comments=self.get_column_description(),\n                       opt_dtype=False,\n                       primary_key=self.id_key(), **kwargs)\n\n        self._is_deployed = True\n        self._table = None\n\n    @staticmethod\n    def check_table_doc(table: pd.DataFrame, doc: dict, class_warning=None):\n        \"\"\"\n        check table and documentation consistency\n        Parameters\n        ----------\n        table\n        doc\n        class_warning\n            the name of the class to print for warnings\n        Returns\n        -------\n\n        \"\"\"\n        if class_warning is None:\n            class_warning = TableFactory.__class__\n        warning_header = f\"WARNING: {class_warning} \"\n        doc_cols = doc.keys()\n        table_cols = table.columns\n\n        if table_cols.isin(doc_cols).all() and len(doc_cols) == len(table_cols):\n            for col_name, val in doc.items():\n                col_dtype, col_doc = val\n                try:\n                    table[col_name] = table[col_name].astype(col_dtype)\n                except Exception as e:\n                    cprint(f\"{warning_header} {col_name}: \"\n                           f\"cannot convert {col_name} \"\n                           f\"from {table[col_name].dtype} to {col_dtype}.\",\n                           'yellow')\n                    cprint(f\"{e}\", 'yellow')\n                if col_doc is None or col_doc == \"\":\n                    cprint(warning_header +\n                           f\"missing description of {col_name}.\",\n                           'yellow')\n        else:\n            doc_cols = pd.Series(doc_cols)\n            table_cols = pd.Series(table_cols)\n\n            missing_doc = table_cols[~(table_cols.isin(doc_cols))]\n            if len(missing_doc) > 0:\n                cprint(\n                    warning_header + f\" column statement is missing:\\n{missing_doc}\",\n                    'yellow')\n\n            missing_col = doc_cols[~(doc_cols.isin(table_cols))]\n            if len(missing_col) > 0:\n                cprint(warning_header + f\" data is missing :\\n{missing_col}\",\n                       'yellow')\n\n    def write_to_excel(self, save: bool = True, path: str = None,\n                       file_name: str = None, verbose=True) -> 'xlsxwriter':\n        if file_name is None:\n            file_name = self.table_name() + '.xlsx'\n        elif len(file_name) <= 5 or file_name[-5:] != '.xlsx':\n            file_name += '.xlsx'\n        if path is None:\n            import tempfile\n            path = tempfile.gettempdir()\n\n        data = self.get_table()\n        full_path = os.path.join(path, file_name)\n        writer = pd.ExcelWriter(full_path, engine='xlsxwriter')\n        doc = self.data_description()[\n            ['column description']].reset_index().rename(\n            columns={'index': 'column name'})\n\n        for df, sheet_name, save_index in (\n                (doc, 'Column description', False), (data, 'Data', False)):\n            if verbose:\n                print(f\"Writing '{sheet_name}' Excel sheet...\", end='')\n            df.to_excel(writer, sheet_name=sheet_name, index=save_index)\n            if verbose:\n                print(f\"\\rWriting '{sheet_name}' Excel sheet\" + colored(\" ok\",\n                                                                        'green'))\n            if verbose:\n                print(\"adjust column width \" + sheet_name + \"...\", end='')\n            worksheet = writer.sheets[sheet_name]\n            worksheet.autofilter(0, 0, 0, len(df.columns) - 1)\n            for idx, col in enumerate(df):  # loop through all columns\n                series = df[col]\n                max_len = max((\n                    series.astype(str).map(len).max(),  # len of largest item\n                    len(str(col)) + 2  # len of column name/header +2 for bold\n                )) + 1  # adding a little extra space\n                worksheet.set_column(idx, idx, max_len)\n            if verbose:\n                print(\"\\radjust column width \" + sheet_name + colored(\" ok\",\n                                                                      'green'))\n        if save:\n            if verbose:\n                print(\"save excel...\", end='')\n            if hasattr(writer, \"save\"):\n                writer.save()\n            else:\n                writer._save()\n            if verbose:\n                print(\n                    f\"ok -> {colored(full_path, 'blue', attrs=['underline'])}\")\n        return writer\n\n    def add_foreign_keys(self, db: ConnectDatabase = None):\n        if db is None:\n            db = self.get_db_connection()\n        full_table_name = f\"{self.schema_name()}.{self.table_name()}\"\n        for key, (\n                foreign_table, foreign_key) in self.get_foreign_keys().items():\n            foreign_table_full_name = f\"{foreign_table.schema_name()}\" \\\n                                      f\".{foreign_table.table_name()}\"\n            print(\n                f\"{full_table_name} ({key}) -- > ({foreign_key}) \"\n                f\"{foreign_table_full_name} \")\n            db.add_key(type_='foreign',\n                       table_name=full_table_name, key=key,\n                       table_foreign=foreign_table_full_name,\n                       key_foreign=foreign_key,\n                       )", ""]}
{"filename": "acrocord/misc/execution.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\nimport datetime\nimport os\nimport sys\nimport time\nimport warnings", "import time\nimport warnings\nfrom contextlib import contextmanager\nfrom functools import wraps\n\nfrom memory_profiler import memory_usage\n\n\nclass ColorsOut:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    Red = '\\033[91m'\n    Green = '\\033[92m'\n    Blue = '\\033[94m'\n    Cyan = '\\033[96m'\n    White = '\\033[97m'\n    Yellow = '\\033[93m'\n    Magenta = '\\033[95m'\n    Grey = '\\033[90m'\n    Black = '\\033[90m'\n    Default = '\\033[99m'\n    DEFAULT = Blue\n    TIME = Grey", "class ColorsOut:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    Red = '\\033[91m'\n    Green = '\\033[92m'\n    Blue = '\\033[94m'\n    Cyan = '\\033[96m'\n    White = '\\033[97m'\n    Yellow = '\\033[93m'\n    Magenta = '\\033[95m'\n    Grey = '\\033[90m'\n    Black = '\\033[90m'\n    Default = '\\033[99m'\n    DEFAULT = Blue\n    TIME = Grey", "\n\ndef print_(output, color=\"DEFAULT\"):\n    print(\n        f\"\"\"{ColorsOut.__getattribute__(ColorsOut, color)}{output}{ColorsOut.ENDC}\"\"\")\n\n\ndef execution_time(method):\n    @wraps(method)\n    def timed(*args, **kw):\n        starting_time = time.time()\n        mem, result = memory_usage((method, args, kw), retval=True, timeout=200,\n                                   interval=1e-7)\n        stopping_time = time.time()\n\n        msg = \"[\" + method.__name__ + \"] execution time :\"\n        msg += \"-\" * (40 - len(msg)) + \"  \"\n        msg += str(datetime.timedelta(milliseconds=(stopping_time - starting_time) * 1000))\n        msg += \"  \" + f'Memory {int(max(mem) - min(mem))}' + \" MiB\"\n        if len(args) > 0 and hasattr(args[0], \"active_execution_time\"):\n            if not args[0].active_execution_time:\n                return result\n            else:\n                print_(msg, color=\"TIME\")\n        else:\n            print_(msg, color=\"TIME\")\n        return result\n\n    return timed", "\n\n@contextmanager\ndef silence_stdout():\n    new_target = open(os.devnull, \"w\")\n    old_target = sys.stdout\n    sys.stdout = new_target\n    try:\n        yield new_target\n    finally:\n        sys.stdout = old_target", "\n\ndef deprecated(func):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emitted\n    when the function is used.\"\"\"\n\n    @wraps(func)\n    def new_func(*args, **kwargs):\n        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n                      category=DeprecationWarning,\n                      stacklevel=2)\n        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n        return func(*args, **kwargs)\n\n    return new_func", ""]}
{"filename": "acrocord/misc/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.#\n"]}
