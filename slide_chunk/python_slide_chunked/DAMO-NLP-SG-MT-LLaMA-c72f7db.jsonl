{"filename": "mtllama/llama_flash_attn_monkey_patch.py", "chunked_list": ["\"\"\"\nReplace vanilla attention in Huggingface's Llama implementation with flash attention.\nAdapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py \n\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nimport transformers", "\nimport transformers\nfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\nfrom einops import rearrange\n\nfrom flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\nfrom flash_attn.bert_padding import unpad_input, pad_input\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor],\n            Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n    \n    attention_mask: [bsz, q_len]\n    \"\"\"\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    assert past_key_value is None, \"past_key_value is not supported\"\n\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n    # [bsz, nh, t, hd]\n    assert not output_attentions, \"output_attentions is not supported\"\n    assert not use_cache, \"use_cache is not supported\"\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n    key_padding_mask = attention_mask\n\n\n    if key_padding_mask is None:\n        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n        max_s = q_len\n        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n                                device=qkv.device)\n        output = flash_attn_unpadded_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n    else:\n        nheads = qkv.shape[-2]\n        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n        output_unpad = flash_attn_unpadded_qkvpacked_func(\n            x_unpad, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n                                    indices, bsz, q_len),\n                        'b s (h d) -> b s h d', h=nheads)\n    return self.o_proj(rearrange(output,\n                                    'b s h d -> b s (h d)')), None, None", "\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor],\n            Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n    \n    attention_mask: [bsz, q_len]\n    \"\"\"\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    assert past_key_value is None, \"past_key_value is not supported\"\n\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n    # [bsz, nh, t, hd]\n    assert not output_attentions, \"output_attentions is not supported\"\n    assert not use_cache, \"use_cache is not supported\"\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n    key_padding_mask = attention_mask\n\n\n    if key_padding_mask is None:\n        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n        max_s = q_len\n        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n                                device=qkv.device)\n        output = flash_attn_unpadded_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n    else:\n        nheads = qkv.shape[-2]\n        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n        output_unpad = flash_attn_unpadded_qkvpacked_func(\n            x_unpad, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n                                    indices, bsz, q_len),\n                        'b s (h d) -> b s h d', h=nheads)\n    return self.o_proj(rearrange(output,\n                                    'b s h d -> b s (h d)')), None, None", "\n\n# Disable the transformation of the attention mask in LlamaModel as the flash attention\n# requires the attention mask to be the same as the key_padding_mask\ndef _prepare_decoder_attention_mask(self, attention_mask, input_shape,\n                                    inputs_embeds, past_key_values_length):\n    # [bsz, seq_len]\n    return attention_mask\n\n\ndef replace_llama_attn_with_flash_attn():\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward", "\n\ndef replace_llama_attn_with_flash_attn():\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"]}
{"filename": "mtllama/train.py", "chunked_list": ["# Adopted from lm-sys@FastChat and tatsu-lab@stanford_alpaca. Below is the original copyright:\n#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n#\n#        http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software", "#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\nimport copy\nfrom dataclasses import dataclass, fieldjson\nimport logging", "from dataclasses import dataclass, fieldjson\nimport logging\nimport pathlib\nfrom typing import Dict, Optional, Sequence\nimport random\nimport torch\nfrom accelerate.utils import set_seed\nimport transformers\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer", "from torch.utils.data import Dataset\nfrom transformers import Trainer\nset_seed(42)\nimport conversation as conversation_lib\nfrom utils import construct_input\n# TODO: import and use code from ../data/dataset.py\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"", "DEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"</s>\"\nDEFAULT_UNK_TOKEN = \"[UNK]\"\n\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n", "\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(default=None,\n                           metadata={\"help\": \"Path to the training data.\"})\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\":\n            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )", "\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=512,\n        metadata={\n            \"help\":\n            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )", "\ndef convertBTC(data, lang2answer):\n\n    placeholder = \"IAmAPalacehodler\"\n\n    sources = []\n    while len(data) > 0:\n        data_one = data.pop(0)\n        if data_one[0] == \"P3\":\n            try:\n                source_one = [\n                    {\n                        \"from\": \"human\",\n                        \"value\": data_one[1]['inputs'].strip()\n                    },\n                    {\n                        \"from\": \"gpt\",\n                        \"value\": data_one[1]['targets'].strip()\n                    },\n                ]\n            except:\n                source_one = [\n                    {\n                        \"from\": \"human\",\n                        \"value\": data_one[1][0].strip()\n                    },\n                    {\n                        \"from\": \"gpt\",\n                        \"value\": data_one[1][1].strip()\n                    },\n                ]\n            sources.append(source_one)\n        elif data_one[0] == \"cls\":\n            lang = data_one[1].lang\n            query = data_one[1].query\n            query = query.replace(placeholder, DEFAULT_UNK_TOKEN)\n            answer = data_one[1].answer\n\n            num_neg = random.randint(1, 10)\n            num_all = len(lang2answer[lang])\n            neg_ans = random.sample(range(num_all), min(num_all, num_neg + 1))\n            neg_ans = [lang2answer[lang][x] for x in neg_ans]\n            if answer in neg_ans:\n                neg_ans.remove(answer)\n            else:\n                neg_ans = neg_ans[1:]\n            assert answer not in neg_ans\n\n            random_index = random.randint(0, len(neg_ans))\n            all_answers = neg_ans[: random_index] + [answer] + neg_ans[random_index:]\n            all_answers = sorted(set(all_answers), key=all_answers.index)\n\n            all_answers = [\"(\" + chr(ord('A') + ix) + \") \" + x + \" (/\" + chr(ord('A') + ix) + \")\" for ix, x in\n                           enumerate(all_answers)]\n\n            # input_str = \"Determine the category of the text from choices. Choices: %s. Text: %s. Category:\" % (\" \".join(all_answer), query)\n            input_str = construct_input(lang, 'cls', \" \".join(all_answers), query)\n            output_str = answer\n            source_one = [\n                {\n                    \"from\": \"human\",\n                    \"value\": input_str\n                },\n                {\n                    \"from\": \"gpt\",\n                    \"value\": output_str\n                },\n            ]\n            sources.append(source_one)\n        elif data_one[0] == \"ext\":\n            lang = data_one[1].lang\n            query = data_one[1].query\n            query = query.replace(placeholder, DEFAULT_UNK_TOKEN)\n            answer = data_one[1].answer\n            context = data_one[1].context\n            input_str = construct_input(lang, 'ext', query, context)\n            output_str = answer\n            source_one = [\n                {\n                    \"from\": \"human\",\n                    \"value\": input_str\n                },\n                {\n                    \"from\": \"gpt\",\n                    \"value\": output_str\n                },\n            ]\n            sources.append(source_one)\n    return sources", "\ndef safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n                                   output_dir: str):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    state_dict = trainer.model.state_dict()\n    if trainer.args.should_save:\n        cpu_state_dict = {\n            key: value.cpu()\n            for key, value in state_dict.items()\n        }\n        del state_dict\n        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa", "\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg", "\n\ndef _tokenize_fn(strings: Sequence[str],\n                 tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n    \"\"\"Tokenize a list of strings.\"\"\"\n    tokenized_list = [\n        tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n        ) for text in strings\n    ]\n    input_ids = labels = [\n        tokenized.input_ids[0] for tokenized in tokenized_list\n    ]\n    input_ids_lens = labels_lens = [\n        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n        for tokenized in tokenized_list\n    ]\n    return dict(\n        input_ids=input_ids,\n        labels=labels,\n        input_ids_lens=input_ids_lens,\n        labels_lens=labels_lens,\n    )", "\n\ndef _mask_targets(target, tokenized_lens, speakers, header_len, s_ids):\n    cur_idx = header_len\n    tgt_len = target.shape[0]\n    for tokenized_len, speaker, s_id in zip(tokenized_lens, speakers, s_ids):\n        if cur_idx >= tgt_len:\n            break\n        elif cur_idx + tokenized_len < tgt_len:\n            # Check whether the mask is applied to the correct position\n            if not torch.equal(target[cur_idx + 2:cur_idx + tokenized_len],\n                               s_id[2:]):\n                logging.warning(\"a sentence mismatches the corresponding piece \"\n                                \"in the conversation\")\n        if speaker == \"human\":\n            target[cur_idx:cur_idx + tokenized_len] = IGNORE_INDEX\n        cur_idx += tokenized_len", "\n\ndef _add_speaker_and_signal(header, source, get_conversation=True):\n    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n    BEGIN_SIGNAL = \"### \"\n    END_SIGNAL = \"\\n\"\n    conversation = header\n    for sentence in source:\n        from_str = sentence[\"from\"]\n        if from_str.lower() == \"human\":\n            from_str = conversation_lib.default_conversation.roles[0]\n        elif from_str.lower() == \"gpt\":\n            from_str = conversation_lib.default_conversation.roles[1]\n        else:\n            from_str = 'unknown'\n        sentence[\"value\"] = (BEGIN_SIGNAL + from_str + \": \" +\n                             sentence[\"value\"] + END_SIGNAL)\n        if get_conversation:\n            conversation += sentence[\"value\"]\n    return conversation", "\n\ndef preprocess(\n    sources: Sequence[str],\n    tokenizer: transformers.PreTrainedTokenizer,\n) -> Dict:\n    \"\"\"\n    Given a list of sources, each is a conversation list. This transform:\n    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n    2. Concatenate conversations together;\n    3. Tokenize the concatenated conversation;\n    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n    \"\"\"\n    # add end signal and concatenate together\n    conversations = []\n    header = f\"{conversation_lib.default_conversation.system}\\n\\n\"\n    for source in sources:\n        conversation = _add_speaker_and_signal(header, source)\n        conversations.append(conversation)\n    # print(conversations)\n    # tokenize conversations\n    conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n    input_ids = conversations_tokenized[\"input_ids\"]\n    targets = copy.deepcopy(input_ids)\n    header_len = _tokenize_fn([header], tokenizer)[\"input_ids_lens\"][0]\n    for target, source in zip(targets, sources):\n        tokenized_sentence = _tokenize_fn([s[\"value\"] for s in source], tokenizer)\n        tokenized_lens = tokenized_sentence[\"input_ids_lens\"]\n        # Currently, \"###\" is tokenized into 2 tokens in the whole conversation,\n        # and 1 token in a single sentence, so we do not need to use the line below.\n        # tokenized_lens = [l-1 for l in tokenized_lens]\n        speakers = [sentence[\"from\"] for sentence in source]\n        ids = tokenized_sentence[\"input_ids\"]\n        _mask_targets(target, tokenized_lens, speakers, header_len, ids)\n\n    return dict(input_ids=input_ids, labels=targets)", "\n\nclass SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, data_path: str,\n                 tokenizer: transformers.PreTrainedTokenizer):\n        super(SupervisedDataset, self).__init__()\n        logging.warning(\"Loading data...\")\n        raw_data = torch.load(data_path)\n        logging.warning(\"Formatting inputs...\")\n        sources = convertBTC(raw_data['data'], raw_data['lang2answer'])\n\n        data_dict = preprocess(sources, tokenizer)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.labels = data_dict[\"labels\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        return dict(input_ids=self.input_ids[i], labels=self.labels[i])", "\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(self, data_path: str,\n                 tokenizer: transformers.PreTrainedTokenizer):\n        super(LazySupervisedDataset, self).__init__()\n        logging.warning(\"Loading data...\")\n        raw_data = torch.load(data_path)\n\n        logging.warning(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.list_data_dict = raw_data['data']\n        self.lang2answer = raw_data['lang2answer']\n\n    def __len__(self):\n        return len(self.list_data_dict)\n\n    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n        sources = self.list_data_dict[i]\n        if isinstance(i, int):\n            sources = [sources]\n            sources = convertBTC(sources, self.lang2answer)\n            # print(sources)\n        data_dict = preprocess(\n            copy.deepcopy(sources),\n            self.tokenizer)\n        if isinstance(i, int):\n            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n                             labels=data_dict[\"labels\"][0])\n        return data_dict", "\n\n@dataclass\nclass DataCollatorForSupervisedDataset(object):\n    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n\n    tokenizer: transformers.PreTrainedTokenizer\n\n    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n        input_ids, labels = tuple([instance[key] for instance in instances]\n                                  for key in (\"input_ids\", \"labels\"))\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids,\n            batch_first=True,\n            padding_value=self.tokenizer.pad_token_id)\n        labels = torch.nn.utils.rnn.pad_sequence(labels,\n                                                 batch_first=True,\n                                                 padding_value=IGNORE_INDEX)\n        return dict(\n            input_ids=input_ids,\n            labels=labels,\n            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n        )", "\n\ndef make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n                                data_args) -> Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (LazySupervisedDataset\n                   if data_args.lazy_preprocess else SupervisedDataset)\n    train_dataset = dataset_cls(tokenizer=tokenizer,\n                                data_path=data_args.data_path)\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n    return dict(train_dataset=train_dataset,\n                eval_dataset=None,\n                data_collator=data_collator)", "\n\ndef train():\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments))\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n    model = transformers.LlamaForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        use_cache=False,\n    )\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    if tokenizer.pad_token is None:\n        smart_tokenizer_and_embedding_resize(\n            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n            tokenizer=tokenizer,\n            model=model,\n        )\n    if \"llama\" in model_args.model_name_or_path:\n        tokenizer.add_special_tokens({\n            \"eos_token\": DEFAULT_EOS_TOKEN,\n            \"bos_token\": DEFAULT_BOS_TOKEN,\n            \"unk_token\": DEFAULT_UNK_TOKEN,\n        })\n\n    data_module = make_supervised_data_module(tokenizer=tokenizer,\n                                              data_args=data_args)\n    trainer = Trainer(model=model,\n                    tokenizer=tokenizer,\n                    args=training_args,\n                    **data_module)\n\n    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    trainer.save_state()\n    safe_save_model_for_hf_trainer(trainer=trainer,\n                                   output_dir=training_args.output_dir)", "\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "mtllama/utils.py", "chunked_list": ["def construct_input(lang, task, text1, text2):\n    if lang == \"ar\":\n        if task == 'cls':\n            input_str = \"\u062d\u062f\u062f \u0641\u0626\u0629 \u0627\u0644\u0646\u0635 \u0645\u0646 \u0627\u0644\u0627\u062e\u062a\u064a\u0627\u0631\u0627\u062a. \u0627\u0644\u0627\u062e\u062a\u064a\u0627\u0631\u0627\u062a: %s. \u0627\u0644\u0646\u0635: %s. \u0627\u0644\u0641\u0626\u0629:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u062a\u062d\u062f\u064a\u062f \u0627\u0644\u0627\u0645\u062a\u062f\u0627\u062f\u0627\u062a \u0645\u0646 \u0627\u0644\u0633\u064a\u0627\u0642 \u0648\u0641\u0642\u064b\u0627 \u0644\u0644\u0627\u0633\u062a\u0639\u0644\u0627\u0645. \u0627\u0633\u062a\u0641\u0633\u0627\u0631: %s. \u0633\u064a\u0627\u0642: %s. \u064a\u0645\u062a\u062f:\" % (\n                text1, text2)\n    elif lang == \"bn\":\n        if task == 'cls':\n            input_str = \"\u09aa\u099b\u09a8\u09cd\u09a6 \u09a5\u09c7\u0995\u09c7 \u09aa\u09be\u09a0\u09cd\u09af\u09c7\u09b0 \u09ac\u09bf\u09ad\u09be\u0997 \u09a8\u09bf\u09b0\u09cd\u09a7\u09be\u09b0\u09a3 \u0995\u09b0\u09c1\u09a8\u0964 \u09aa\u099b\u09a8\u09cd\u09a6: %s \u09aa\u09be\u09a0\u09cd\u09af: %s \u09ac\u09bf\u09ad\u09be\u0997:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u0995\u09cd\u09af\u09cb\u09af\u09bc\u09be\u09b0\u09c0 \u0985\u09a8\u09c1\u09af\u09be\u09af\u09bc\u09c0 \u09aa\u09cd\u09b0\u09b8\u0999\u09cd\u0997 \u09a5\u09c7\u0995\u09c7 \u09b8\u09cd\u09aa\u09cd\u09af\u09be\u09a8 \u09b8\u09a8\u09be\u0995\u09cd\u09a4 \u0995\u09b0\u09c1\u09a8\u0964 \u09aa\u09cd\u09b0\u09b6\u09cd\u09a8: %s \u09aa\u09cd\u09b0\u09b8\u0999\u09cd\u0997: %s \u09b8\u09cd\u09aa\u09cd\u09af\u09be\u09a8:\" % (\n                text1, text2)\n    elif lang == \"de\":\n        if task == 'cls':\n            input_str = \"Bestimmen Sie die Kategorie des Textes aus Auswahlm\u00f6glichkeiten. Auswahlm\u00f6glichkeiten: %s. Text: %s. Kategorie:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identifizieren Sie Spannen aus dem Kontext gem\u00e4\u00df der Abfrage. Anfrage: %s. Kontext: %s. Spannweiten:\" % (\n                text1, text2)\n    elif lang == \"fi\":\n        if task == 'cls':\n            input_str = \"M\u00e4\u00e4rit\u00e4 tekstin luokka vaihtoehdoista. Vaihtoehdot: %s. Teksti: %s. Kategoria:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Tunnista j\u00e4nteet kontekstista kyselyn mukaan. Kysely: %s. Konteksti: %s. Kantavuus:\" % (\n                text1, text2)\n    elif lang == \"fr\":\n        if task == 'cls':\n            input_str = \"D\u00e9terminez la cat\u00e9gorie du texte parmi les choix. Les choix: %s. Texte: %s. Cat\u00e9gorie:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identifiez les \u00e9tendues du contexte en fonction de la requ\u00eate. Mettre en doute: %s. Contexte: %s. Port\u00e9es:\" % (\n                text1, text2)\n    elif lang == \"el\":\n        if task == 'cls':\n            input_str = \"\u03a0\u03c1\u03bf\u03c3\u03b4\u03b9\u03bf\u03c1\u03af\u03c3\u03c4\u03b5 \u03c4\u03b7\u03bd \u03ba\u03b1\u03c4\u03b7\u03b3\u03bf\u03c1\u03af\u03b1 \u03c4\u03bf\u03c5 \u03ba\u03b5\u03b9\u03bc\u03ad\u03bd\u03bf\u03c5 \u03b1\u03c0\u03cc \u03b5\u03c0\u03b9\u03bb\u03bf\u03b3\u03ad\u03c2. \u0395\u03c0\u03b9\u03bb\u03bf\u03b3\u03ad\u03c2: %s. \u039a\u03b5\u03af\u03bc\u03b5\u03bd\u03bf: %s. \u039a\u03b1\u03c4\u03b7\u03b3\u03bf\u03c1\u03af\u03b1:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u03a0\u03c1\u03bf\u03c3\u03b4\u03b9\u03bf\u03c1\u03af\u03c3\u03c4\u03b5 \u03c4\u03b9\u03c2 \u03b5\u03ba\u03c4\u03ac\u03c3\u03b5\u03b9\u03c2 \u03b1\u03c0\u03cc \u03c4\u03bf \u03c0\u03b5\u03c1\u03b9\u03b2\u03ac\u03bb\u03bb\u03bf\u03bd \u03c3\u03cd\u03bc\u03c6\u03c9\u03bd\u03b1 \u03bc\u03b5 \u03c4\u03bf \u03b5\u03c1\u03ce\u03c4\u03b7\u03bc\u03b1. \u0395\u03c1\u03ce\u03c4\u03b7\u03bc\u03b1: %s. \u03a3\u03c5\u03bc\u03c6\u03c1\u03b1\u03b6\u03cc\u03bc\u03b5\u03bd\u03b1: %s. \u0395\u03ba\u03c4\u03b5\u03af\u03bd\u03b5\u03c4\u03b1\u03b9:\" % (\n                text1, text2)\n    elif lang == \"en\":\n        if task == 'cls':\n            input_str = \"Determine the category of the text from choices. Choices: %s. Text: %s. Category:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identify spans from the context according to the query. Query: %s. Context: %s. Spans:\" % (\n                text1, text2)\n    elif lang == \"es\":\n        if task == 'cls':\n            input_str = \"Determinar la categor\u00eda del texto a partir de las opciones. Opciones: %s. Texto: %s. Categor\u00eda:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identifique tramos del contexto de acuerdo con la consulta. Consulta: %s. Contexto: %s. Se extiende:\" % (\n                text1, text2)\n    elif lang == \"hi\":\n        if task == 'cls':\n            input_str = \"\u0935\u093f\u0915\u0932\u094d\u092a\u094b\u0902 \u092e\u0947\u0902 \u0938\u0947 \u092a\u093e\u0920 \u0915\u0940 \u0936\u094d\u0930\u0947\u0923\u0940 \u0928\u093f\u0930\u094d\u0927\u093e\u0930\u093f\u0924 \u0915\u0930\u0947\u0902\u0964 \u0935\u093f\u0915\u0932\u094d\u092a: %s\u0964 \u092e\u0942\u0932\u092a\u093e\u0920: %s\u0964 \u0935\u0930\u094d\u0917:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u0915\u094d\u0935\u0947\u0930\u0940 \u0915\u0947 \u0905\u0928\u0941\u0938\u093e\u0930 \u0938\u0902\u0926\u0930\u094d\u092d \u0938\u0947 \u0938\u094d\u092a\u0948\u0928 \u0915\u0940 \u092a\u0939\u091a\u093e\u0928 \u0915\u0930\u0947\u0902\u0964 \u091c\u093f\u091c\u094d\u091e\u093e\u0938\u093e: %s\u0964 \u092a\u094d\u0930\u0938\u0902\u0917: %s\u0964 \u0938\u094d\u092a\u0948\u0928:\" % (\n                text1, text2)\n    elif lang == \"id\":\n        if task == 'cls':\n            input_str = \"Tentukan kategori teks dari pilihan. Pilihan: %s. Teks: %s. Kategori:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identifikasi rentang dari konteks sesuai dengan kueri. Kueri: %s. Konteks: %s. Rentang:\" % (\n                text1, text2)\n    elif lang == \"it\":\n        if task == 'cls':\n            input_str = \"Determinare la categoria del testo dalle scelte. Scelte: %s. Testo: %s. Categoria:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identifica gli intervalli dal contesto in base alla query. Domanda: %s. Contesto: %s. Campate:\" % (\n                text1, text2)\n    elif lang == \"ja\":\n        if task == 'cls':\n            input_str = \"\u9078\u629e\u80a2\u304b\u3089\u30c6\u30ad\u30b9\u30c8\u306e\u30ab\u30c6\u30b4\u30ea\u3092\u6c7a\u5b9a\u3057\u307e\u3059\u3002 \u9078\u629e\u80a2\uff1a%s\u3002 \u6587\u7ae0\uff1a%s\u3002 \u30ab\u30c6\u30b4\u30ea\u30fc\uff1a\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u30af\u30a8\u30ea\u306b\u5f93\u3063\u3066\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u30b9\u30d1\u30f3\u3092\u8b58\u5225\u3057\u307e\u3059\u3002 \u30af\u30a8\u30ea\uff1a%s\u3002 \u30b3\u30f3\u30c6\u30af\u30b9\u30c8\uff1a%s\u3002 \u30b9\u30d1\u30f3:\" % (\n                text1, text2)\n    elif lang == \"ko\":\n        if task == 'cls':\n            input_str = \"\uc120\ud0dd\uc5d0\uc11c \ud14d\uc2a4\ud2b8\uc758 \ubc94\uc8fc\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. \uc120\ud0dd: %s. \ud14d\uc2a4\ud2b8: %s. \ubc94\uc8fc:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\ucffc\ub9ac\uc5d0 \ub530\ub77c \ucee8\ud14d\uc2a4\ud2b8\uc5d0\uc11c \ubc94\uc704\ub97c \uc2dd\ubcc4\ud569\ub2c8\ub2e4. \ucffc\ub9ac: %s. \ubb38\ub9e5: %s. \uc2a4\ud32c:\" % (\n                text1, text2)\n    elif lang == \"nl\":\n        if task == 'cls':\n            input_str = \"Bepaal de categorie van de tekst uit keuzes. Keuzes: %s. Tekst: %s. Categorie:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identificeer overspanningen uit de context volgens de query. Vraag: %s. Context: %s. Overspanningen:\" % (\n                text1, text2)\n    elif lang == \"pl\":\n        if task == 'cls':\n            input_str = \"Okre\u015bl kategori\u0119 tekstu z wybor\u00f3w. Wybory: %s. Tekst: %s. Kategoria:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Zidentyfikuj rozpi\u0119to\u015bci z kontekstu zgodnie z zapytaniem. Zapytanie: %s. Kontekst: %s. Rozpi\u0119to\u015bci:\" % (\n                text1, text2)\n    elif lang == \"pt\":\n        if task == 'cls':\n            input_str = \"Determine a categoria do texto a partir das op\u00e7\u00f5es. Escolhas: %s. Texto: %s. Categoria:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identifique spans a partir do contexto de acordo com a consulta. Consulta: %s. Contexto: %s. Per\u00edodos:\" % (\n                text1, text2)\n    elif lang == \"ru\":\n        if task == 'cls':\n            input_str = \"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044e \u0442\u0435\u043a\u0441\u0442\u0430 \u0438\u0437 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432. \u0412\u044b\u0431\u043e\u0440: %s. \u0422\u0435\u043a\u0441\u0442: %s. \u041a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u0418\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u0446\u0438\u0440\u0443\u0439\u0442\u0435 \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0438 \u0438\u0437 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0430 \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c. \u0417\u0430\u043f\u0440\u043e\u0441: %s. \u041a\u043e\u043d\u0442\u0435\u043a\u0441\u0442: %s. \u041f\u0440\u043e\u043b\u0435\u0442\u044b:\" % (\n                text1, text2)\n    elif lang == \"sv\":\n        if task == 'cls':\n            input_str = \"Best\u00e4m kategorin f\u00f6r texten fr\u00e5n val. Alternativ: %s. Text: %s. Kategori:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Identifiera spann fr\u00e5n sammanhanget enligt fr\u00e5gan. Fr\u00e5ga: %s. Sammanhang: %s. Sp\u00e4nnvidder:\" % (\n                text1, text2)\n    elif lang == \"sw\":\n        if task == 'cls':\n            input_str = \"Amua aina ya maandishi kutoka kwa chaguo. Chaguo: %s. Maandishi: %s. Kategoria:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Tambua vipindi kutoka kwa muktadha kulingana na hoja. Swali: %s. Muktadha: %s. Vipindi:\" % (\n                text1, text2)\n    elif lang == \"te\":\n        if task == 'cls':\n            input_str = \"\u0c0e\u0c02\u0c2a\u0c3f\u0c15\u0c32 \u0c28\u0c41\u0c02\u0c21\u0c3f \u0c1f\u0c46\u0c15\u0c4d\u0c38\u0c4d\u0c1f\u0c4d \u0c2f\u0c4a\u0c15\u0c4d\u0c15 \u0c35\u0c30\u0c4d\u0c17\u0c3e\u0c28\u0c4d\u0c28\u0c3f \u0c28\u0c3f\u0c30\u0c4d\u0c23\u0c2f\u0c3f\u0c02\u0c1a\u0c02\u0c21\u0c3f. \u0c0e\u0c02\u0c2a\u0c3f\u0c15\u0c32\u0c41: %s. \u0c35\u0c1a\u0c28\u0c02: %s. \u0c35\u0c30\u0c4d\u0c17\u0c02:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u0c2a\u0c4d\u0c30\u0c36\u0c4d\u0c28\u0c15\u0c41 \u0c05\u0c28\u0c41\u0c17\u0c41\u0c23\u0c02\u0c17\u0c3e \u0c38\u0c02\u0c26\u0c30\u0c4d\u0c2d\u0c02 \u0c28\u0c41\u0c02\u0c21\u0c3f \u0c2a\u0c30\u0c3f\u0c27\u0c41\u0c32\u0c28\u0c41 \u0c17\u0c41\u0c30\u0c4d\u0c24\u0c3f\u0c02\u0c1a\u0c02\u0c21\u0c3f. \u0c2a\u0c4d\u0c30\u0c36\u0c4d\u0c28: %s. \u0c38\u0c02\u0c26\u0c30\u0c4d\u0c2d\u0c02: %s. \u0c2a\u0c30\u0c3f\u0c27\u0c41\u0c32\u0c41:\" % (\n                text1, text2)\n    elif lang == \"th\":\n        if task == 'cls':\n            input_str = \"\u0e01\u0e33\u0e2b\u0e19\u0e14\u0e2b\u0e21\u0e27\u0e14\u0e2b\u0e21\u0e39\u0e48\u0e02\u0e2d\u0e07\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e08\u0e32\u0e01\u0e15\u0e31\u0e27\u0e40\u0e25\u0e37\u0e2d\u0e01 \u0e15\u0e31\u0e27\u0e40\u0e25\u0e37\u0e2d\u0e01: %s \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21: %s. \u0e2b\u0e21\u0e27\u0e14\u0e2b\u0e21\u0e39\u0e48:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u0e23\u0e30\u0e1a\u0e38\u0e0a\u0e48\u0e27\u0e07\u0e08\u0e32\u0e01\u0e1a\u0e23\u0e34\u0e1a\u0e17\u0e15\u0e32\u0e21\u0e41\u0e1a\u0e1a\u0e2a\u0e2d\u0e1a\u0e16\u0e32\u0e21 \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e04\u0e49\u0e19\u0e2b\u0e32: %s \u0e1a\u0e23\u0e34\u0e1a\u0e17: %s. \u0e0a\u0e48\u0e27\u0e07:\" % (\n                text1, text2)\n    elif lang == \"tr\":\n        if task == 'cls':\n            input_str = \"Se\u00e7eneklerden metnin kategorisini belirleyin. Se\u00e7enekler: %s. Metin: %s. Kategori:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"Sorguya g\u00f6re ba\u011flamdan a\u00e7\u0131kl\u0131klar\u0131 tan\u0131mlay\u0131n. Sorgu: %s. Ba\u011flam: %s. a\u00e7\u0131kl\u0131klar:\" % (\n                text1, text2)\n    elif lang == \"vi\":\n        if task == 'cls':\n            input_str = \"X\u00e1c \u0111\u1ecbnh th\u1ec3 lo\u1ea1i c\u1ee7a v\u0103n b\u1ea3n t\u1eeb c\u00e1c l\u1ef1a ch\u1ecdn. L\u1ef1a ch\u1ecdn: %s. Ch\u1eef: %s. Lo\u1ea1i:\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"X\u00e1c \u0111\u1ecbnh c\u00e1c kho\u1ea3ng t\u1eeb ng\u1eef c\u1ea3nh theo truy v\u1ea5n. Truy v\u1ea5n: %s. B\u1ed1i c\u1ea3nh: %s. nh\u1ecbp:\" % (\n                text1, text2)\n    elif lang == \"zh\":\n        if task == 'cls':\n            input_str = \"\u6839\u636e\u9009\u9879\u786e\u5b9a\u6587\u672c\u7684\u7c7b\u522b\u3002\u9009\u9879\uff1a%s\u3002\u6587\u672c\uff1a%s\u3002\u7c7b\u522b\uff1a\" % (\n            text1, text2)\n        elif task == 'ext':\n            input_str = \"\u6839\u636e\u67e5\u8be2\u4ece\u4e0a\u4e0b\u6587\u4e2d\u8bc6\u522b\u8de8\u5ea6\u3002\u67e5\u8be2\uff1a%s\u3002\u4e0a\u4e0b\u6587\uff1a%s\u3002\u8de8\u5ea6\uff1a\" % (\n                text1, text2)\n    else:\n        if task == 'cls':\n            input_str = \"Determine the category of the text from choices. Choices: %s. Text: %s. Category:\" % (\n                text1, text2)\n        elif task == 'ext':\n            input_str = \"Identify spans from the context according to the query. Query: %s. Context: %s. Spans:\" % (\n                text1, text2)\n    return input_str", "\nclass S2SFeatures:\n    '''\n    MRC features\n    '''\n    def __init__(\n        self,\n        query,\n        context,\n        answer,\n        target_entity,\n        len_query,\n        len_context,\n        lang=None,\n    ):\n        self.query = query\n        self.context = context\n        self.answer = answer\n        self.target_entity = target_entity\n        self.len_query = len_query\n        self.len_context = len_context\n        self.lang = lang"]}
{"filename": "mtllama/conversation.py", "chunked_list": ["\"\"\"\nConversation prompt templates.\nAdopted from https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n\"\"\"\nimport dataclasses\nfrom enum import auto, Enum\nfrom typing import List, Tuple, Any\n\n\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n    SINGLE = auto()\n    TWO = auto()\n    DOLLY = auto()", "\nclass SeparatorStyle(Enum):\n    \"\"\"Different separator style.\"\"\"\n    SINGLE = auto()\n    TWO = auto()\n    DOLLY = auto()\n\n\n@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n\n    skip_next: bool = False\n    conv_id: Any = None\n\n    def get_prompt(self):\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system\n            for role, message in self.messages:\n                if message:\n                    ret += self.sep + \" \" + role + \": \" + message\n                else:\n                    ret += self.sep + \" \" + role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.DOLLY:\n            seps = [self.sep, self.sep2]\n            ret = self.system\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \":\\n\" + message + seps[i % 2]\n                    if i % 2 == 1:\n                        ret += \"\\n\\n\"\n                else:\n                    ret += role + \":\\n\"\n            return ret\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n            if i % 2 == 0:\n                ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(\n            system=self.system,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2,\n            conv_id=self.conv_id)\n\n    def dict(self):\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n            \"conv_id\": self.conv_id,\n        }", "@dataclasses.dataclass\nclass Conversation:\n    \"\"\"A class that keeps all conversation history.\"\"\"\n    system: str\n    roles: List[str]\n    messages: List[List[str]]\n    offset: int\n    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n    sep: str = \"###\"\n    sep2: str = None\n\n    skip_next: bool = False\n    conv_id: Any = None\n\n    def get_prompt(self):\n        if self.sep_style == SeparatorStyle.SINGLE:\n            ret = self.system\n            for role, message in self.messages:\n                if message:\n                    ret += self.sep + \" \" + role + \": \" + message\n                else:\n                    ret += self.sep + \" \" + role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.TWO:\n            seps = [self.sep, self.sep2]\n            ret = self.system + seps[0]\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \": \" + message + seps[i % 2]\n                else:\n                    ret += role + \":\"\n            return ret\n        elif self.sep_style == SeparatorStyle.DOLLY:\n            seps = [self.sep, self.sep2]\n            ret = self.system\n            for i, (role, message) in enumerate(self.messages):\n                if message:\n                    ret += role + \":\\n\" + message + seps[i % 2]\n                    if i % 2 == 1:\n                        ret += \"\\n\\n\"\n                else:\n                    ret += role + \":\\n\"\n            return ret\n        else:\n            raise ValueError(f\"Invalid style: {self.sep_style}\")\n\n    def append_message(self, role, message):\n        self.messages.append([role, message])\n\n    def to_gradio_chatbot(self):\n        ret = []\n        for i, (role, msg) in enumerate(self.messages[self.offset:]):\n            if i % 2 == 0:\n                ret.append([msg, None])\n            else:\n                ret[-1][-1] = msg\n        return ret\n\n    def copy(self):\n        return Conversation(\n            system=self.system,\n            roles=self.roles,\n            messages=[[x, y] for x, y in self.messages],\n            offset=self.offset,\n            sep_style=self.sep_style,\n            sep=self.sep,\n            sep2=self.sep2,\n            conv_id=self.conv_id)\n\n    def dict(self):\n        return {\n            \"system\": self.system,\n            \"roles\": self.roles,\n            \"messages\": self.messages,\n            \"offset\": self.offset,\n            \"sep\": self.sep,\n            \"sep2\": self.sep2,\n            \"conv_id\": self.conv_id,\n        }", "\n\nconv_one_shot = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(\n        (\"Human\", \"What are the key differences between renewable and non-renewable energy sources?\"),\n        (\"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"", "        (\"Assistant\",\n            \"Renewable energy sources are those that can be replenished naturally in a relatively \"\n            \"short amount of time, such as solar, wind, hydro, geothermal, and biomass. \"\n            \"Non-renewable energy sources, on the other hand, are finite and will eventually be \"\n            \"depleted, such as coal, oil, and natural gas. Here are some key differences between \"\n            \"renewable and non-renewable energy sources:\\n\"\n            \"1. Availability: Renewable energy sources are virtually inexhaustible, while non-renewable \"\n            \"energy sources are finite and will eventually run out.\\n\"\n            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"", "            \"2. Environmental impact: Renewable energy sources have a much lower environmental impact \"\n            \"than non-renewable sources, which can lead to air and water pollution, greenhouse gas emissions, \"\n            \"and other negative effects.\\n\"\n            \"3. Cost: Renewable energy sources can be more expensive to initially set up, but they typically \"\n            \"have lower operational costs than non-renewable sources.\\n\"\n            \"4. Reliability: Renewable energy sources are often more reliable and can be used in more remote \"\n            \"locations than non-renewable sources.\\n\"\n            \"5. Flexibility: Renewable energy sources are often more flexible and can be adapted to different \"\n            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"", "            \"situations and needs, while non-renewable sources are more rigid and inflexible.\\n\"\n            \"6. Sustainability: Renewable energy sources are more sustainable over the long term, while \"\n            \"non-renewable sources are not, and their depletion can lead to economic and social instability.\")\n    ),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\n\n", "\n\nconv_vicuna_v1_1 = Conversation(\n    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n    roles=(\"USER\", \"ASSISTANT\"),\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",", "    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\n\nconv_koala_v1 = Conversation(\n    system=\"BEGINNING OF CONVERSATION:\",\n    roles=(\"USER\", \"GPT\"),\n    messages=(),", "    roles=(\"USER\", \"GPT\"),\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.TWO,\n    sep=\" \",\n    sep2=\"</s>\",\n)\n\nconv_dolly = Conversation(\n    system=", "conv_dolly = Conversation(\n    system=\n    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\",\n    roles=('### Instruction', '### Response'),\n    messages=(),\n    offset=0,\n    sep_style=SeparatorStyle.DOLLY,\n    sep=\"\\n\\n\",\n    sep2=\"### End\",\n)", "    sep2=\"### End\",\n)\n\nconv_templates = {\n    \"conv_one_shot\": conv_one_shot,\n    \"vicuna_v1.1\": conv_vicuna_v1_1,\n    \"koala_v1\": conv_koala_v1,\n    \"dolly\": conv_dolly,\n}\n", "}\n\n\ndef get_default_conv_template(model_name):\n    model_name = model_name.lower()\n    if \"vicuna\" in model_name or \"output\" in model_name:\n        return conv_vicuna_v1_1\n    elif \"koala\" in model_name:\n        return conv_koala_v1\n    elif \"dolly\" in model_name:\n        return conv_dolly\n    return conv_one_shot", "\ndefault_conversation = conv_vicuna_v1_1\nif __name__ == \"__main__\":\n    print(default_conversation.get_prompt())\n"]}
{"filename": "mtllama/train_mem.py", "chunked_list": ["\"\"\"\nEnable flash attention during fine-tuning.\nAdopted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train_mem.py\n\"\"\"\nfrom llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n\nreplace_llama_attn_with_flash_attn()\n\nfrom train import train\n\nif __name__ == \"__main__\":\n    train()", "from train import train\n\nif __name__ == \"__main__\":\n    train()\n"]}
{"filename": "mtllama/cli.py", "chunked_list": ["\"\"\"\nUsage:\npython3 cli.py --model /path/to/mt-llama-7b\n\"\"\"\nimport argparse\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n\nfrom conversation import Conversation, SeparatorStyle", "\nfrom conversation import Conversation, SeparatorStyle\n\nmy_conv = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,", "    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\ndef load_model(model_name, device, num_gpus, load_8bit=False):\n    if device == \"cuda\":\n        kwargs = {\"torch_dtype\": torch.float16}\n        if load_8bit:\n            if num_gpus != \"auto\" and int(num_gpus) != 1:\n                print(\"8-bit weights are not supported on multiple GPUs. Revert to use one GPU.\")\n            kwargs.update({\"load_in_8bit\": True, \"device_map\": \"auto\"})\n        else:\n            if num_gpus == \"auto\":\n                kwargs[\"device_map\"] = \"auto\"\n            else:\n                num_gpus = int(num_gpus)\n                if num_gpus != 1:\n                    kwargs.update({\n                        \"device_map\": \"auto\",\n                        \"max_memory\": {i: \"13GiB\" for i in range(num_gpus)},\n                    })\n    elif device == \"cpu\":\n        kwargs = {}\n    else:\n        raise ValueError(f\"Invalid device: {device}\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name,\n        low_cpu_mem_usage=True, **kwargs)\n\n    # calling model.cuda() mess up weights if loading 8-bit weights\n    if device == \"cuda\" and num_gpus == 1 and not load_8bit:\n        model.cuda()\n\n    return model, tokenizer", "\n\n@torch.inference_mode()\ndef generate_stream(tokenizer, model, params, device,\n                    context_len=2048, stream_interval=2):\n    \"\"\"Adapted from fastchat/serve/model_worker.py::generate_stream\"\"\"\n\n    prompt = params[\"prompt\"]\n    l_prompt = len(prompt)\n    temperature = float(params.get(\"temperature\", 1.0))\n    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n    stop_str = params.get(\"stop\", None)\n\n    input_ids = tokenizer(prompt).input_ids\n    output_ids = list(input_ids)\n\n    max_src_len = context_len - max_new_tokens - 8\n    input_ids = input_ids[-max_src_len:]\n\n    for i in range(max_new_tokens):\n        if i == 0:\n            out = model(\n                torch.as_tensor([input_ids], device=device), use_cache=True)\n            logits = out.logits\n            past_key_values = out.past_key_values\n        else:\n            attention_mask = torch.ones(\n                1, past_key_values[0][0].shape[-2] + 1, device=device)\n            out = model(input_ids=torch.as_tensor([[token]], device=device),\n                        use_cache=True,\n                        attention_mask=attention_mask,\n                        past_key_values=past_key_values)\n            logits = out.logits\n            past_key_values = out.past_key_values\n\n        last_token_logits = logits[0][-1]\n        if temperature < 1e-4:\n            token = int(torch.argmax(last_token_logits))\n        else:\n            probs = torch.softmax(last_token_logits / temperature, dim=-1)\n            token = int(torch.multinomial(probs, num_samples=1))\n\n        output_ids.append(token)\n\n        if token == tokenizer.eos_token_id:\n            stopped = True\n        else:\n            stopped = False\n\n        if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:\n            output = tokenizer.decode(output_ids, skip_special_tokens=True)\n            pos = output.rfind(stop_str, l_prompt)\n            if pos != -1:\n                output = output[:pos]\n                stopped = True\n            yield output\n\n        if stopped:\n            break\n\n    del past_key_values", "\n\ndef main(args):\n    model_name = args.model_name\n\n    # Model\n    model, tokenizer = load_model(args.model_name, args.device,\n        args.num_gpus, args.load_8bit)\n\n    # Chat\n    conv = my_conv.copy()\n    while True:\n        try:\n            inp = input(f\"{conv.roles[0]}: \")\n        except EOFError:\n            inp = \"\"\n        if not inp:\n            print(\"exit...\")\n            break\n\n        conv.append_message(conv.roles[0], inp)\n        conv.append_message(conv.roles[1], None)\n        prompt = conv.get_prompt()\n\n        params = {\n            \"model\": model_name,\n            \"prompt\": prompt,\n            \"temperature\": args.temperature,\n            \"max_new_tokens\": args.max_new_tokens,\n            \"stop\": \"\\n\" #conv.sep if conv.sep_style == SeparatorStyle.SINGLE else conv.sep2,\n        }\n\n        print(f\"{conv.roles[1]}: \", end=\"\", flush=True)\n        pre = 0\n        for outputs in generate_stream(tokenizer, model, params, args.device):\n            outputs = outputs[len(prompt) + 1:].strip()\n            outputs = outputs.split(\" \")\n            now = len(outputs)\n            if now - 1 > pre:\n                print(\" \".join(outputs[pre:now-1]), end=\" \", flush=True)\n                pre = now - 1\n        print(\" \".join(outputs[pre:]), flush=True)\n\n        conv.messages[-1][-1] = \" \".join(outputs)\n\n        if args.debug:\n            print(\"\\n\", {\"prompt\": prompt, \"outputs\": outputs}, \"\\n\")", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/opt-350m\")\n    parser.add_argument(\"--device\", type=str, choices=[\"cuda\", \"cpu\"], default=\"cuda\")\n    parser.add_argument(\"--num-gpus\", type=str, default=\"1\")\n    parser.add_argument(\"--load-8bit\", action=\"store_true\")\n    parser.add_argument(\"--temperature\", type=float, default=0)\n    parser.add_argument(\"--max-new-tokens\", type=int, default=512)\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    args = parser.parse_args()\n    main(args)", ""]}
{"filename": "mtllama/model/make_delta.py", "chunked_list": ["\"\"\"\nhttps://github.com/lm-sys/FastChat/blob/main/fastchat/model/make_delta.py\nUsage:\npython3 -m mtllama.model.make_delta --base-model-path /path/to/llama-7b --target-model-path /path/to/mt-llama-7b --delta-path /output/path/to/mt-llama-delta-7b\n\"\"\"\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM", "from tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef make_delta(base_model_path, target_model_path, delta_path):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading target model\")\n    target = AutoModelForCausalLM.from_pretrained(target_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    DEFAULT_PAD_TOKEN = \"[PAD]\"\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n    num_new_tokens = base_tokenizer.add_special_tokens(dict(pad_token=DEFAULT_PAD_TOKEN))\n\n    base.resize_token_embeddings(len(base_tokenizer))\n    input_embeddings = base.get_input_embeddings().weight.data\n    output_embeddings = base.get_output_embeddings().weight.data\n    input_embeddings[-num_new_tokens:] = 0\n    output_embeddings[-num_new_tokens:] = 0\n\n    print(\"Calculating delta\")\n    for name, param in tqdm(target.state_dict().items(), desc=\"Calculating delta\"):\n        assert name in base.state_dict()\n        param.data -= base.state_dict()[name]\n\n    print(\"Saving delta\")\n    if args.hub_repo_id:\n        kwargs = {\"push_to_hub\": True, \"repo_id\": args.hub_repo_id, \"use_auth_token\": args.user_key}\n    else:\n        kwargs = {}\n    target.save_pretrained(delta_path, **kwargs)\n    target_tokenizer = AutoTokenizer.from_pretrained(target_model_path)\n    target_tokenizer.save_pretrained(delta_path, **kwargs)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n    parser.add_argument(\"--hub-repo-id\", type=str, required=False)\n    parser.add_argument(\"--user-key\", type=str, required=False)\n    args = parser.parse_args()\n\n    make_delta(args.base_model_path, args.target_model_path, args.delta_path)", ""]}
{"filename": "mtllama/model/apply_delta.py", "chunked_list": ["\"\"\"\nAdapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/model/apply_delta.py\nUsage:\npython3 -m mtllama.model.apply_delta --base-model-path /path/to/llama-7b --target-model-path /output/path/to/mt-llama-7b --delta-path /path/to/mt-llama-7b-delta\n\"\"\"\nimport argparse\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM", "from tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef apply_delta(base_model_path, target_model_path, delta_path):\n    print(\"Loading base model\")\n    base = AutoModelForCausalLM.from_pretrained(\n        base_model_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n\n    print(\"Loading delta\")\n    delta = AutoModelForCausalLM.from_pretrained(delta_path, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n    delta_tokenizer = AutoTokenizer.from_pretrained(delta_path)\n\n    DEFAULT_PAD_TOKEN = \"[PAD]\"\n    base_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n    num_new_tokens = base_tokenizer.add_special_tokens(dict(pad_token=DEFAULT_PAD_TOKEN))\n\n    base.resize_token_embeddings(len(base_tokenizer))\n    input_embeddings = base.get_input_embeddings().weight.data\n    output_embeddings = base.get_output_embeddings().weight.data\n    input_embeddings[-num_new_tokens:] = 0\n    output_embeddings[-num_new_tokens:] = 0\n\n    print(\"Applying delta\")\n    for name, param in tqdm(base.state_dict().items(), desc=\"Applying delta\"):\n        assert name in delta.state_dict()\n        param.data += delta.state_dict()[name]\n\n    print(\"Saving target model\")\n    base.save_pretrained(target_model_path)\n    delta_tokenizer.save_pretrained(target_model_path)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base-model-path\", type=str, required=True)\n    parser.add_argument(\"--target-model-path\", type=str, required=True)\n    parser.add_argument(\"--delta-path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    apply_delta(args.base_model_path, args.target_model_path, args.delta_path)", ""]}
{"filename": "mtllama/model/__init__.py", "chunked_list": [""]}
{"filename": "mCLS/data_clm.py", "chunked_list": ["import torch\nfrom torch.utils.data import Dataset\nfrom transformers.utils import logging\nimport random\nimport numpy as np\nfrom conversation import Conversation, SeparatorStyle\n\nmy_conv = Conversation(\n    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",", "    system=\"A chat between a curious human and an artificial intelligence assistant. \"\n           \"The assistant gives helpful, detailed, and polite answers to the human's questions.\",\n    roles=(\"Human\", \"Assistant\"),\n    messages=(),\n    offset=2,\n    sep_style=SeparatorStyle.SINGLE,\n    sep=\"###\",\n)\nlogger = logging.get_logger(__name__)\nMULTI_SEP_TOKENS_TOKENIZERS_SET = {\"roberta\", \"camembert\", \"bart\", \"mpnet\"}", "logger = logging.get_logger(__name__)\nMULTI_SEP_TOKENS_TOKENIZERS_SET = {\"roberta\", \"camembert\", \"bart\", \"mpnet\"}\nrandom.seed(42)\n\n\ndef compute_acc(preds, labels):\n    \"\"\" simple accuracy \"\"\"\n    preds = np.array(preds)\n    labels = np.array(labels)\n    acc = float((preds == labels).mean())\n    return {'acc': acc}", "\n\nMetrics = {\n    'acc':compute_acc,\n}\n\nclass GLUEDataset(Dataset):\n    \"\"\"\n    MRC NER Dataset\n    Args:\n        data_path: path to data\n        tokenizer: BertTokenizer\n    \"\"\"\n    def __init__(self, dataset, data_path,  tokenizer: None, lang='en'):\n        self.data_name = data_path\n        self.tokenizer = tokenizer\n        self.lang = lang\n        self.get_data(data_path, dataset)\n    def get_data(self, data_path, dataset):\n        if data_path == \"xnli\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            map_dict = {\"neutral\": 0, 'entailment': 1, 'contradiction': 2}\n            self.label_dict = {\"maybe\": 0, 'yes': 1, 'no': 2,\n                               \"a\": 0, \"b\": 1, 'c':2,\n                              \"inconclusive\":0, \"true\":1, \"false\":2 }\n            labels = [\"maybe.\",\n                      'yes.',\n                      'no.']\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                passage = item[0] + \" Question: Does this imply that \" + item[1] + \"? Please response with 'Yes', 'No', or 'Maybe'.\"\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": map_dict[item[2]],\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"xstorycloze\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\"1\": 0, '2': 1,\n                               \"(a)\": 0, '(b)': 1,\n                               \"a\": 0, 'b': 1,\n                                \"(/a)\": 0, '(/b)': 1}\n\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                labels = [item['sentence_quiz1'], item['sentence_quiz2']]\n                passage = item['context'] + \" As a consequence... Help me pick the more plausible option: - \" \\\n                          + item['sentence_quiz1'] + \" - \" + item['sentence_quiz2']\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": item['answer_right_ending'] - 1,\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"xcopa\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\"1\": 0, '2': 1,\n                               \"(a)\": 0, '(b)': 1,\n                               \"a\": 0, 'b': 1,\n                                \"(/a)\": 0, '(/b)': 1}\n\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                labels = [item['choice1'], item['choice2'],]\n                if item['question'] == 'cause':\n                    passage = item['premise'] +\" This happened because... Help me pick the more plausible option: - \" \\\n                              + item['choice1'] + \". - \" + item['choice2']\n                elif item['question'] == 'effect':\n                    passage = item['premise'] +\" As a consequence... Help me pick the more plausible option: - \"\\\n                              + item['choice1'] + \". - \" + item['choice2']\n                else:\n                    raise NotImplementedError\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": item['label'],\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"xwinograd\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\"1.\": 0, '2.': 1,\n                               \"a\": 0, 'b': 1,\n                               \"(a)\": 0, '(b)': 1,\n                               \"(/a)\": 0, '(/b)': 1,\n                               \"True\": 0, \"False\": 1}\n\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                labels = [f\"{item['option1']}\", f\"{item['option2']}\"]\n                passage = f\"{item['sentence']} In the previous sentence, does _ refer to {item['option1']} or {item['option2']}?\"\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": int(item['answer']) - 1,\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"mnlim\" or data_path == \"mnlimm\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\"maybe\": 0, 'yes': 1, 'no': 2,\n                               \"a\": 0, \"b\": 1, 'c': 2,\n                               \"inconclusive\": 0, \"true\": 1, \"false\": 2}\n            labels = [\"maybe.\",\n                      'yes.',\n                      'no.']\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                passage = item['premise'] + \" Question: Does this imply that \" + item['hypothesis'] + \\\n                          \"? Please response with 'Yes', 'No', or 'Maybe'.\"\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": item['label'],\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"copa\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\"1\": 0, '2': 1,\n                               \"(a)\": 0, '(b)': 1,\n                               \"a\": 0, 'b': 1,\n                                \"(/a)\": 0, '(/b)': 1}\n\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                labels = [item['choice1'], item['choice2'],]\n                if item['question'] == 'cause':\n                    passage = item['premise'] +\" This happened because... Help me pick the more plausible option: - \" \\\n                              + item['choice1'] + \". - \" + item['choice2']\n                elif item['question'] == 'effect':\n                    passage = item['premise'] +\" As a consequence... Help me pick the more plausible option: - \"\\\n                              + item['choice1'] + \". - \" + item['choice2']\n                else:\n                    raise NotImplementedError\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": item['label'],\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"wic\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {'no': 0, 'yes': 1,}\n            labels = ['no.',\n                      'yes.']\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                passage = f'Does the word \"{item[\"word\"]}\" have the same meaning in these two sentences? Yes, No? {item[\"sentence1\"]} {item[\"sentence2\"]}'\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": item['label'],\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"rte\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {'yes': 0, 'no': 1,}\n            labels = ['yes.',\n                      'no.']\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                passage = f'Given {item[\"premise\"]} Is it guaranteed true that \"{item[\"hypothesis\"]}\"? Yes or no?'\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": item['label'],\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"winogrande\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\"1.\": 0, '2.': 1,\n                               \"a\": 0, 'b': 1,\n                               \"(a)\": 0, '(b)': 1,\n                               \"(/a)\": 0, '(/b)': 1,\n                               \"True\": 0, \"False\": 1}\n\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                labels = [f\"{item['option1']}\", f\"{item['option2']}\"]\n                passage = f\"{item['sentence']} In the previous sentence, does _ refer to {item['option1']} or {item['option2']}?\"\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": int(item['answer']) - 1,\n                }\n                self.all_data.append(data_one)\n        elif data_path == \"obqa\":\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\"1.\": 0, '2.': 1, \"3.\": 2, '4.': 3,\n                               \"a\": 0, 'b': 1, \"c\":2, \"d\":3,\n                               \"(a)\": 0, '(b)': 1,\"(c)\": 2, '(d)': 3,\n                               }\n\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                labels = [f\"{item['choices']['text'][0]}\", f\"{item['choices']['text'][1]}\", f\"{item['choices']['text'][2]}\", f\"{item['choices']['text'][3]}\"]\n                passage = f\"{item['question_stem']} Which is the correct answer? - (A) {item['choices']['text'][0]} \" \\\n                          f\"- (B) {item['choices']['text'][1]} - (C) {item['choices']['text'][2]}\" \\\n                          f\" - (D) {item['choices']['text'][3]}\"\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": ord(item['answerKey']) - ord(\"A\"),\n                }\n                self.all_data.append(data_one)\n        elif data_path == 'pawsx':\n            self.all_data = []\n            self.compute_metric = compute_acc\n            self.label_dict = {'no': 0, 'yes': 1}\n            labels = ['no.', 'yes.']\n            for i, item in enumerate(dataset):\n                passage = \"Sentence 1: \" + item[0] + \" Sentence 2: \" + item[\n                    1] + \" Question: Does Sentence 1 paraphrase Sentence 2? Yes or No?\"\n\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": int(item[2]),\n                }\n                self.all_data.append(data_one)\n        elif data_path.startswith(\"SST-2\"):\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\n                \"1\": 0, \"2\": 0, \"4\": 1, \"5\": 1, \"\u2605\": 0, \"\u2605\u2605\": 0, \"\u2605\u2605\u2605\u2605\": 1, \"\u2605\u2605\u2605\u2605\u2605\": 1,\n                \"no\": 0, \"yes\": 1,\n            }\n            labels = [\"no.\", \"yes.\"]\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                passage =  f\"{item[0]} Based on this review, would the user recommend this product? No or Yes?\"\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": int(item[1]),\n                }\n                self.all_data.append(data_one)\n        elif data_path.startswith(\"marc-2\"):\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\n                               \"1\": 0, \"2\": 0,  \"4\": 1, \"5\": 1, \"\u2605\": 0, \"\u2605\u2605\": 0, \"\u2605\u2605\u2605\u2605\": 1, \"\u2605\u2605\u2605\u2605\u2605\": 1,\n                                \"no\": 0, \"yes\":1,\n                               }\n            labels = [\"no.\", \"yes.\"]\n\n            label_map = {\"1\": 0, \"2\": 0, \"4\": 1, \"5\": 1}\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                passage = \"Title: \" + item['review_title'] + \" Product Review: \" + item['review_body'] \\\n                          + \" Based on this review, would the user recommend this product? No or Yes?\"\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                star = item['stars']\n                if star not in label_map:\n                    continue\n                else:\n                    data_one = {\n                        'passage': passage,\n                        'labels': labels,\n                        \"gold_label\": label_map[star],\n                    }\n                self.all_data.append(data_one)\n        elif data_path.startswith(\"marc-5\"):\n            self.all_data = []\n            self.compute_metric = Metrics['acc']\n            self.label_dict = {\n                               \"1\":0, \"2\":1, \"3\":2, \"4\":3, \"5\":4, \"\u2605\":0, \"\u2605\u2605\":1, \"\u2605\u2605\u2605\":2, \"\u2605\u2605\u2605\u2605\":3, \"\u2605\u2605\u2605\u2605\u2605\":4,\n                               \"not at all\": 0, \"no\": 1, \"maybe\": 2, \"yes\": 3, \"definitely\": 4,\n                               }\n            labels = [\"not at all.\", \"no.\", \"maybe.\", \"yes.\", \"definitely.\"]\n            label_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n            for i, item in enumerate(dataset):\n                conv = my_conv.copy()\n                passage = \"Title: \" + item['review_title'] + \" Product Review: \" + item[\n                    'review_body'] + \" Based on this review, would the user recommend this product?\" \\\n                                     \" Not at all, No, Maybe, Yes, or Definitely?\"\n                conv.append_message(conv.roles[0], passage)\n                conv.append_message(conv.roles[1], None)\n                passage = conv.get_prompt()\n\n                star = item['stars']\n                data_one = {\n                    'passage': passage,\n                    'labels': labels,\n                    \"gold_label\": label_map[star],\n                }\n                self.all_data.append(data_one)\n        else:\n            raise NotImplementedError\n\n\n    def __len__(self):\n        return len(self.all_data)\n    def __getitem__(self, item):\n        \"\"\"\n        Args:\n            item: int, idx\n        \"\"\"\n        data = self.all_data[item]\n        passage = data['passage']\n        labels = data['labels']\n        gold_label = data['gold_label']\n        answer = labels[gold_label]\n        tokenizer = self.tokenizer\n\n        inputs = tokenizer(passage, return_tensors='pt')\n\n        outputs = tokenizer(answer, return_tensors='pt')\n        return [\n            torch.LongTensor(inputs['input_ids']),\n            torch.LongTensor(inputs['attention_mask']),\n            torch.LongTensor(outputs['input_ids']),\n            torch.LongTensor(outputs['attention_mask']),\n            gold_label,\n        ]", "\n"]}
{"filename": "mCLS/llama_flash_attn_monkey_patch.py", "chunked_list": ["\"\"\"\nReplace vanilla attention in Huggingface's Llama implementation with flash attention.\nAdapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py \n\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nimport transformers", "\nimport transformers\nfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\nfrom einops import rearrange\n\nfrom flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\nfrom flash_attn.bert_padding import unpad_input, pad_input\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor],\n            Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n    \n    attention_mask: [bsz, q_len]\n    \"\"\"\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    assert past_key_value is None, \"past_key_value is not supported\"\n\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n    # [bsz, nh, t, hd]\n    assert not output_attentions, \"output_attentions is not supported\"\n    assert not use_cache, \"use_cache is not supported\"\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n    key_padding_mask = attention_mask\n\n\n    if key_padding_mask is None:\n        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n        max_s = q_len\n        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n                                device=qkv.device)\n        output = flash_attn_unpadded_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n    else:\n        nheads = qkv.shape[-2]\n        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n        output_unpad = flash_attn_unpadded_qkvpacked_func(\n            x_unpad, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n                                    indices, bsz, q_len),\n                        'b s (h d) -> b s h d', h=nheads)\n    return self.o_proj(rearrange(output,\n                                    'b s h d -> b s (h d)')), None, None", "\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor],\n            Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n    \n    attention_mask: [bsz, q_len]\n    \"\"\"\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    assert past_key_value is None, \"past_key_value is not supported\"\n\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n    # [bsz, nh, t, hd]\n    assert not output_attentions, \"output_attentions is not supported\"\n    assert not use_cache, \"use_cache is not supported\"\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n    key_padding_mask = attention_mask\n\n\n    if key_padding_mask is None:\n        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n        max_s = q_len\n        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n                                device=qkv.device)\n        output = flash_attn_unpadded_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n    else:\n        nheads = qkv.shape[-2]\n        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n        output_unpad = flash_attn_unpadded_qkvpacked_func(\n            x_unpad, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n                                    indices, bsz, q_len),\n                        'b s (h d) -> b s h d', h=nheads)\n    return self.o_proj(rearrange(output,\n                                    'b s h d -> b s (h d)')), None, None", "\n\n# Disable the transformation of the attention mask in LlamaModel as the flash attention\n# requires the attention mask to be the same as the key_padding_mask\ndef _prepare_decoder_attention_mask(self, attention_mask, input_shape,\n                                    inputs_embeds, past_key_values_length):\n    # [bsz, seq_len]\n    return attention_mask\n\n\ndef replace_llama_attn_with_flash_attn():\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward", "\n\ndef replace_llama_attn_with_flash_attn():\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"]}
{"filename": "mCLS/train-CLS.py", "chunked_list": ["# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors, The HuggingFace Inc. team and Alibaba DAMO Academy.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Finetuning the library models for question-answering on SQuAD (DistilBERT, Bert, XLM, XLNet).\"\"\"\n\"\"\"Adapted from the HuggingFace SQuAD finetuning script for CLS.\"\"\"\n", "\"\"\"Adapted from the HuggingFace SQuAD finetuning script for CLS.\"\"\"\n\nimport argparse\nfrom decimal import Decimal\nfrom typing import Dict\nimport logging\nimport os\nimport json\nimport random\nimport timeit", "import random\nimport timeit\nimport transformers\nfrom transformers import (\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n)\nfrom utils import collate_to_max_length_llama\nimport difflib", "from utils import collate_to_max_length_llama\nimport difflib\nfrom data_clm import GLUEDataset as GLUEDataset_clm\nfrom transformers.trainer_utils import is_main_process\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom tqdm import tqdm\ntry:\n    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\nexcept:\n    print('not applicable for flash attention')", "try:\n    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\nexcept:\n    print('not applicable for flash attention')\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"</s>\"\nDEFAULT_UNK_TOKEN = \"[UNK]\"", "DEFAULT_BOS_TOKEN = \"</s>\"\nDEFAULT_UNK_TOKEN = \"[UNK]\"\n\nlogger = logging.getLogger(__name__)\n\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\ndef string_compare(s, s1, s2):\n    score1 = difflib.SequenceMatcher(None, s, s1).quick_ratio()\n    score2 = difflib.SequenceMatcher(None, s, s2).quick_ratio()\n    if score1 >= score2:\n        return 0\n    else:\n        return 1", "def string_compare(s, s1, s2):\n    score1 = difflib.SequenceMatcher(None, s, s1).quick_ratio()\n    score2 = difflib.SequenceMatcher(None, s, s2).quick_ratio()\n    if score1 >= score2:\n        return 0\n    else:\n        return 1\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)", "def set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\n\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg", "\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg", "\ndef evaluate(args, data_path, model, tokenizer, prefix=\"\", split=\"dev\", lang='en'):\n    dataset = load_and_cache_examples(args, tokenizer, evaluate=True, data_path=data_path, split=split, lang=lang)\n\n\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate_to_max_length_llama)\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.warning(\"***** Running evaluation {} *****\".format(prefix))\n    logger.warning(\"  Num examples = %d\", len(dataset))\n    logger.warning(\"  Batch size = %d\", args.eval_batch_size)\n\n    all_preds_text = []\n    start_time = timeit.default_timer()\n\n    for batch in tqdm(eval_dataloader, desc=f\"Evaluating {lang} at {data_path}\"):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n            }\n            outputs = model.generate(**inputs,\n                                     max_new_tokens=64,\n                                     eos_token_id=13,\n                                     )\n\n\n        dec = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        if args.model_type == 'llama' or args.model_type == 'bloom':\n            input_text = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[0]]\n            dec = [dec[i][len(input_text[i]):] for i in range(len(dec))]\n            dec = [dec[i].strip() for i in range(len(dec))]\n\n        all_preds_text.extend(dec)\n\n    all_preds = []\n    all_golds = []\n    fail_count = 0\n    label_dict = dataset.label_dict\n    for i_item, item in enumerate(dataset.all_data):\n        gold = item['gold_label']\n        label_text = item['labels']\n        pred_text = all_preds_text[i_item].lower()\n        if data_path in ['xcopa', \"xwinograd\", \"xstorycloze\", 'winogrande', 'copa']:\n            pred = string_compare(pred_text, label_text[0].lower(), label_text[1].lower())\n        else:\n            for i_label, label in enumerate(label_dict):\n                if label in pred_text:\n                    pred = label_dict[label]\n                    break\n            else:\n                logger.warning('unable to extract label with the following preditction: {}'.format(pred_text))\n                pred = 0\n                fail_count += 1\n\n        all_golds.append(gold)\n        all_preds.append(pred)\n\n    scores = dataset.compute_metric(all_preds, all_golds)\n    logger.warning(f\"the number of failure is {fail_count}.\")\n    logger.warning(f\"EVAL {lang} {split} at {data_path} -> acc is: {scores['acc']}. \")\n\n    return scores[\"acc\"]", "\ndef load_and_cache_examples(args, tokenizer, evaluate=False, data_path=None, split=\"train\", lang='en'):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n        torch.distributed.barrier()\n    if data_path is None:\n        data_path = args.data_path\n\n    logger.info(\"load data from {}.\".format(data_path))\n    if data_path == 'xnli' or data_path == 'pawsx':\n        tsv_path = os.path.join(\"./Data\", data_path, split + \"-\" + lang + \".tsv\")\n        dataset = []\n        with open(tsv_path, 'r') as reader:\n            for item in reader:\n                dataset.append(item.strip().split('\\t'))\n    elif data_path.startswith(\"marc\"):\n        json_path = os.path.join(\"./Data\", 'marc',  \"dataset_\" + lang + \"_\" + split + \".json\")\n        dataset = []\n        with open(json_path, 'r') as reader:\n            for line in reader:\n                dataset.append(json.loads(line))\n    elif data_path in ['xcopa', 'xwinograd', 'xstorycloze']:\n        json_path = os.path.join(\"./Data\", data_path,  lang + '-' + data_path + '-' + split + '.json')\n        with open(json_path, 'r') as reader:\n            for line in reader:\n                dataset = json.loads(line)\n    elif data_path in ['mnlim', 'mnlimm']:\n        json_path = os.path.join(\"./Data\", 'mnli',   data_path + '-' + split + '.json')\n        with open(json_path, 'r') as reader:\n            for line in reader:\n                dataset = json.loads(line)\n    elif data_path in [ 'copa', 'winogrande', 'wic', 'rte', \"obqa\"]:\n        json_path = os.path.join(\"./Data\", data_path, data_path + '-' + split + '.json')\n        with open(json_path, 'r') as reader:\n            for line in reader:\n                dataset = json.loads(line)\n    elif data_path in ['SST-2']:\n        tsv_path = os.path.join(\"./Data\", data_path,  split + '.tsv')\n        dataset = []\n        with open(tsv_path, 'r') as reader:\n            for il, line in enumerate(reader):\n                if il != 0:\n                    sen, label = line.strip().split(\"\\t\")\n                    dataset.append((sen, label))\n    else:\n        raise NotImplementedError\n\n\n    dataset = GLUEDataset_clm(\n        dataset=dataset,\n        data_path=data_path,\n        tokenizer=tokenizer,\n        lang=lang,\n    )\n    if args.local_rank == 0 and not evaluate:\n        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n        torch.distributed.barrier()\n    return dataset", "\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(\n        \"--model_type\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Model type selected in the list: \" + \", \".join(MODEL_TYPES),\n    )\n    parser.add_argument(\n        \"--model_name_or_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The output directory where the model checkpoints and predictions will be written.\",\n    )\n    parser.add_argument(\n        \"--data_path\",\n        default=None,\n        type=str,\n        help=\"The input file.\",\n    )\n    parser.add_argument(\n        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        default=\"\",\n        type=str,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--cache_dir\",\n        default=\"\",\n        type=str,\n        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n    )\n    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\n        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n    )\n    parser.add_argument(\n        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n    )\n    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n    parser.add_argument(\n        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n    )\n    parser.add_argument(\n        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n    )\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n    parser.add_argument(\n        \"--fp16\",\n        action=\"store_true\",\n        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n    )\n    parser.add_argument(\n        \"--fp16_opt_level\",\n        type=str,\n        default=\"O1\",\n        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n        \"See details at https://nvidia.github.io/apex/amp.html\",\n    )\n    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n    parser.add_argument(\"--keep_frac\", type=float, default=1.0, help=\"The fraction of the balanced dataset to keep.\")\n    parser.add_argument(\n        \"--flash_attn\", action=\"store_true\", help=\"use flash attn\"\n    )\n    args = parser.parse_args()\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n\n        print(\"Waiting for debugger attach\")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(\"cuda\", args.local_rank)\n        torch.distributed.init_process_group(backend=\"nccl\")\n        args.n_gpu = 1\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=  logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    # Set seed\n    set_seed(args)\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        # Make sure only the first process in distributed training will download model & vocab\n        torch.distributed.barrier()\n    if args.flash_attn:\n        replace_llama_attn_with_flash_attn()\n    args.model_type = args.model_type.lower()\n    kwargs = {\"torch_dtype\": torch.float16}\n    model = transformers.LlamaForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        cache_dir=args.cache_dir,\n        low_cpu_mem_usage=True, **kwargs,\n    )\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        cache_dir=args.cache_dir,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    if tokenizer.pad_token is None:\n        smart_tokenizer_and_embedding_resize(\n            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n            tokenizer=tokenizer,\n            model=model,\n        )\n    if \"llama\" in args.model_name_or_path:\n        tokenizer.add_special_tokens({\n            \"eos_token\": DEFAULT_EOS_TOKEN,\n            \"bos_token\": DEFAULT_BOS_TOKEN,\n            \"unk_token\": DEFAULT_UNK_TOKEN,\n        })\n    else:\n        raise NotImplementedError\n    # model = model_PMR[args.model_type](config)\n\n    if args.local_rank == 0:\n        # Make sure only the first process in distributed training will download model & vocab\n        torch.distributed.barrier()\n\n    model.to(args.device)\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n    # Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n    # remove the need for this code, but it is still valid.\n    if args.fp16:\n        try:\n            import apex\n\n            apex.amp.register_half_function(torch, \"einsum\")\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n\n\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_warning()\n    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n    if args.do_eval and args.local_rank in [-1, 0]:\n        if args.do_train:\n            pass\n        else:\n\n            if \"pawsx\" in args.data_path:\n                all_test = {}\n                for lang in ['en', 'de', 'es', 'fr', 'ja', 'ko', 'zh']:\n                    test_results = evaluate(args, \"pawsx\", model, tokenizer, prefix='', split=\"test\",\n                                            lang=lang)\n                    all_test[lang] = test_results\n                avg_acc = 0\n                for lang in ['en', 'de', 'es', 'fr', 'ja', 'ko', 'zh']:\n                    avg_acc += all_test[lang]\n                all_test[\"pawsx-avg\"] = avg_acc / 7\n                logger.warning(f\"EVAL INFO mix of pawsx -> valid_f1 is: {all_test['pawsx-avg']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"pawsx\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if 'xwinograd' in args.data_path:\n                all_test = {}\n                for lang in ['en', 'fr', 'jp', 'pt', 'ru', 'zh']:\n                    test_results = evaluate(args, 'xwinograd', model, tokenizer, prefix='', split=\"test\",\n                                            lang=lang)\n                    all_test[lang] = test_results\n                avg_acc = 0\n                for lang in ['en', 'fr', 'jp', 'pt', 'ru', 'zh']:\n                    avg_acc += all_test[lang]\n                all_test[\"xwinograd-avg\"] = avg_acc / 6\n                logger.warning(f\"EVAL INFO mix of xwinograd -> valid_f1 is: {all_test['xwinograd-avg']}\")\n\n                save_line = [Decimal(all_test[x]*100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"xwinograd\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if 'xnli' in args.data_path:\n                all_test = {}\n                for lang in ['en', 'zh', 'es', 'de', 'ar', 'ur', 'ru', 'bg', 'el', 'fr', 'hi', 'sw', 'th', 'tr', 'vi']:\n                    test_results = evaluate(args, 'xnli', model, tokenizer, prefix='', split=\"test\",\n                                            lang=lang)\n                    all_test[lang] = test_results\n                avg_acc = 0\n                for lang in ['en', 'zh', 'es', 'de', 'ar', 'ur', 'ru', 'bg', 'el', 'fr', 'hi', 'sw', 'th', 'tr', 'vi']:\n                    avg_acc += all_test[lang]\n                all_test[\"xnli-avg\"] = avg_acc / 15\n                logger.warning(f\"EVAL INFO mix of xnli -> valid_f1 is: {all_test['xnli-avg']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join(['xnli'] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"xcopa\" in args.data_path :\n                all_test = {}\n                for lang in ['et', 'ht', 'id', 'it', 'qu', 'sw', 'ta', 'th', 'tr', 'vi', 'zh']:\n                    test_results = evaluate(args, \"xcopa\", model, tokenizer, prefix='', split=\"test\",\n                                            lang=lang)\n                    all_test[lang] = test_results\n                avg_acc = 0\n                for lang in ['et', 'ht', 'id', 'it', 'qu', 'sw', 'ta', 'th', 'tr', 'vi', 'zh']:\n                    avg_acc += all_test[lang]\n                all_test[\"xcopa-avg\"] = avg_acc / 11\n                logger.warning(f\"EVAL INFO mix of xcopa -> valid_f1 is: {all_test['xcopa-avg']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"xcopa\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"xstorycloze\" in args.data_path :\n                all_test = {}\n                for lang in [\"en\", \"ru\", \"zh\", \"es\", \"ar\", \"hi\", \"id\", \"te\", \"sw\", \"eu\", \"my\"]:\n                    test_results = evaluate(args, \"xstorycloze\", model, tokenizer, prefix='', split=\"test\",\n                                            lang=lang)\n                    all_test[lang] = test_results\n                avg_acc = 0\n                for lang in [\"en\", \"ru\", \"zh\", \"es\", \"ar\", \"hi\", \"id\", \"te\", \"sw\", \"eu\", \"my\"]:\n                    avg_acc += all_test[lang]\n                all_test[\"xstorycloze-avg\"] = avg_acc / 11\n                logger.warning(f\"EVAL INFO mix of xstorycloze -> valid_f1 is: {all_test['xstorycloze-avg']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"xstorycloze\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n\n            if \"marc-2\" in args.data_path:\n                all_test = {}\n                for lang in ['en', 'es', 'fr', 'de', 'ja', 'zh']:\n                    test_results = evaluate(args, \"marc-2\", model, tokenizer, prefix='', split=\"test\",\n                                            lang=lang)\n                    all_test[lang] = test_results\n                avg_acc = 0\n                for lang in ['en', 'es', 'fr', 'de', 'ja', 'zh']:\n                    avg_acc += all_test[lang]\n                all_test[\"marc-avg\"] = avg_acc / 6\n                logger.warning(f\"EVAL INFO mix of marc -> valid_f1 is: {all_test['marc-avg']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"marc-2\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"marc-5\" in args.data_path:\n                all_test = {}\n                for lang in ['en', 'es', 'fr', 'de', 'ja', 'zh']:\n                    test_results = evaluate(args, \"marc-5\", model, tokenizer, prefix='', split=\"test\",\n                                            lang=lang)\n                    all_test[lang] = test_results\n                avg_acc = 0\n                for lang in ['en', 'es', 'fr', 'de', 'ja', 'zh']:\n                    avg_acc += all_test[lang]\n                all_test[\"marc-avg\"] = avg_acc / 6\n                logger.warning(f\"EVAL INFO mix of marc -> valid_f1 is: {all_test['marc-avg']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"marc-5\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"SST-2\" in args.data_path:\n                all_test = {}\n\n                test_results = evaluate(args, \"SST-2\", model, tokenizer, prefix='', split=\"test\",\n                                        lang='en')\n                all_test['en'] = test_results\n\n                logger.warning(f\"EVAL INFO mix of SST-2 -> valid_f1 is: {all_test['en']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"SST-2\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"copa\" in args.data_path:\n                all_test = {}\n\n                test_results = evaluate(args, \"copa\", model, tokenizer, prefix='', split=\"test\",\n                                        lang='en')\n                all_test['en'] = test_results\n\n                logger.warning(f\"EVAL INFO mix of copa -> valid_f1 is: {all_test['en']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"copa\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"wic\" in args.data_path:\n                all_test = {}\n\n                test_results = evaluate(args, \"wic\", model, tokenizer, prefix='', split=\"test\",\n                                        lang='en')\n                all_test['en'] = test_results\n\n                logger.warning(f\"EVAL INFO mix of wic -> valid_f1 is: {all_test['en']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"wic\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"rte\" in args.data_path:\n                all_test = {}\n\n                test_results = evaluate(args, \"rte\", model, tokenizer, prefix='', split=\"test\",\n                                        lang='en')\n                all_test['en'] = test_results\n\n                logger.warning(f\"EVAL INFO mix of rte -> valid_f1 is: {all_test['en']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"rte\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"winogrande\" in args.data_path:\n                all_test = {}\n\n                test_results = evaluate(args, \"winogrande\", model, tokenizer, prefix='', split=\"test\",\n                                        lang='en')\n                all_test['en'] = test_results\n\n                logger.warning(f\"EVAL INFO mix of winogrande -> valid_f1 is: {all_test['en']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"winogrande\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"obqa\" in args.data_path:\n                all_test = {}\n\n                test_results = evaluate(args, \"obqa\", model, tokenizer, prefix='', split=\"test\",\n                                        lang='en')\n                all_test['en'] = test_results\n\n                logger.warning(f\"EVAL INFO mix of obqa -> valid_f1 is: {all_test['en']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"obqa\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n            if \"mnli\" in args.data_path:\n                all_test = {}\n\n                test_results = evaluate(args, \"mnlim\", model, tokenizer, prefix='', split=\"test\",\n                                        lang='en')\n                all_test['en'] = test_results\n\n                logger.warning(f\"EVAL INFO mix of mnlim -> valid_f1 is: {all_test['en']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"mnlim\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n\n                all_test = {}\n\n                test_results = evaluate(args, \"mnlimm\", model, tokenizer, prefix='', split=\"test\",\n                                        lang='en')\n                all_test['en'] = test_results\n\n                logger.warning(f\"EVAL INFO mix of mnlimm -> valid_f1 is: {all_test['en']}\")\n                save_line = [Decimal(all_test[x] * 100).quantize(Decimal(\"0.1\"), rounding=\"ROUND_HALF_UP\") for x in\n                             all_test]\n\n                save_line = [str(x) for x in save_line]\n                save_line = '\\t'.join([\"mnlimm\"] + save_line)\n                with open('temp.txt', 'a') as writer2:\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')", "\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "mCLS/utils.py", "chunked_list": ["import torch\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\ndef collate_to_max_length_llama(batch):\n    \"\"\"\n    adapted form https://github.com/ShannonAI/mrc-for-flat-nested-ner\n    pad to maximum length of this batch\n    \"\"\"\n    batch_size = len(batch)\n    max_length_enc = max(x[0].shape[1] for x in batch)\n\n    output = []\n\n    for field_idx in range(3):\n        if field_idx != 1:\n            pad_output = torch.full([batch_size, max_length_enc], 32000, dtype=batch[0][field_idx].dtype) # 32000 as pad token\n        else:\n            pad_output = torch.full([batch_size, max_length_enc], 0, dtype=batch[0][field_idx].dtype)\n\n        for sample_idx in range(batch_size):\n            data = batch[sample_idx][field_idx]\n            pad_output[sample_idx][max_length_enc - data.shape[1]:] = data # padding left\n        output.append(pad_output)\n\n    return output", ""]}
{"filename": "mQA/data_clm.py", "chunked_list": ["import torch\nfrom torch.utils.data import Dataset\nfrom transformers.utils import logging\nimport random\nimport os\nfrom tqdm import tqdm\nfrom utils import SquadV1Processor\nlogger = logging.get_logger(__name__)\nMULTI_SEP_TOKENS_TOKENIZERS_SET = {\"roberta\", \"camembert\", \"bart\", \"mpnet\"}\nrandom.seed(42)", "MULTI_SEP_TOKENS_TOKENIZERS_SET = {\"roberta\", \"camembert\", \"bart\", \"mpnet\"}\nrandom.seed(42)\n\n\nclass QADataset(Dataset):\n    \"\"\"\n    MRC QA Dataset\n    \"\"\"\n    def __init__(self, features, tokenizer: None, data_name: None,):\n        self.all_data = features\n        self.data_name = data_name\n        self.tokenizer = tokenizer\n\n\n    def __len__(self):\n        return len(self.all_data)\n\n    def __getitem__(self, item):\n        \"\"\"\n        Args:\n            item: int, idx\n        \"\"\"\n        data = self.all_data[item]\n        qid, inputs, outputs = data\n        return [\n            torch.LongTensor(inputs['input_ids']),\n            torch.LongTensor(inputs['attention_mask']),\n            torch.LongTensor(outputs['input_ids']),\n            torch.LongTensor(outputs['attention_mask']),\n        ]", "\n\ndef cache_mQAexamples(args, tokenizer, dataset_name, split, lang='en'):\n\n    num_qa = 0\n    processor = SquadV1Processor()\n    if split == \"train\" and dataset_name == \"TyDiQA\":\n        input_data = processor.get_examples(os.path.join('./Data', 'tydiqa', 'tydiqa-goldp-v1.1-train'),\n                                            \"tydiqa.goldp.en.train.json\")\n    elif split == \"dev\" and dataset_name == \"TyDiQA\":\n        input_data = processor.get_examples(os.path.join('./Data', 'tydiqa', 'tydiqa-goldp-v1.1-dev'),\n                                            \"tydiqa.goldp.\" + lang + \".dev.json\")\n    elif split == \"train\" and dataset_name == \"SQuAD\":\n        input_data = processor.get_examples(os.path.join('./Data', 'squad'), \"train-v1.1.json\")\n    elif split == \"dev\" and dataset_name == \"SQuAD\":\n        input_data = processor.get_examples(os.path.join('./Data', 'squad'), \"dev-v1.1.json\")\n    elif split == \"dev\" and dataset_name == \"XQuAD\":\n        input_data = processor.get_examples(os.path.join('./Data', 'xquad'), \"xquad.\" + lang + \".json\")\n    elif split == \"dev\" and dataset_name == \"MLQA\":\n        input_data = processor.get_examples(os.path.join('./Data', 'mlqa/test'), 'test-context-' + lang + \"-question-\" + lang + \".json\")\n    features = []\n    for i_t, item in enumerate(tqdm(input_data, desc=\"creating {} features from {}\".format(split, dataset_name))):\n        context = item.doc_tokens\n        context_str = \" \".join([x[0] for x in context])\n        num_qa += 1\n        qid = item.qas_id\n        answers = item.answers\n        query_str = item.question_text\n        answer_str = answers[0]['text']\n        if split != 'train':\n            input_str = f'Answer the question according to the context. Question: {query_str}. Context: {context_str}. Answer:'\n            inputs = tokenizer(input_str, return_tensors='pt', truncation=True, max_length=1024)\n            outputs = tokenizer(answer_str, return_tensors='pt')\n\n            features.append((qid, inputs, outputs))\n        else:\n            raise NotImplementedError\n    logger.warning(\"finish creating {} features on {} questions\".format(len(features), num_qa))\n    return features"]}
{"filename": "mQA/llama_flash_attn_monkey_patch.py", "chunked_list": ["\"\"\"\nReplace vanilla attention in Huggingface's Llama implementation with flash attention.\nAdapted from: https://github.com/lm-sys/FastChat/blob/main/fastchat/train/llama_flash_attn_monkey_patch.py \n\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nimport transformers", "\nimport transformers\nfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\nfrom einops import rearrange\n\nfrom flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\nfrom flash_attn.bert_padding import unpad_input, pad_input\n\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor],\n            Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n    \n    attention_mask: [bsz, q_len]\n    \"\"\"\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    assert past_key_value is None, \"past_key_value is not supported\"\n\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n    # [bsz, nh, t, hd]\n    assert not output_attentions, \"output_attentions is not supported\"\n    assert not use_cache, \"use_cache is not supported\"\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n    key_padding_mask = attention_mask\n\n\n    if key_padding_mask is None:\n        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n        max_s = q_len\n        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n                                device=qkv.device)\n        output = flash_attn_unpadded_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n    else:\n        nheads = qkv.shape[-2]\n        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n        output_unpad = flash_attn_unpadded_qkvpacked_func(\n            x_unpad, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n                                    indices, bsz, q_len),\n                        'b s (h d) -> b s h d', h=nheads)\n    return self.o_proj(rearrange(output,\n                                    'b s h d -> b s (h d)')), None, None", "\ndef forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor],\n            Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n    \n    attention_mask: [bsz, q_len]\n    \"\"\"\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    assert past_key_value is None, \"past_key_value is not supported\"\n\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n    # [bsz, nh, t, hd]\n    assert not output_attentions, \"output_attentions is not supported\"\n    assert not use_cache, \"use_cache is not supported\"\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack([query_states, key_states, value_states], dim=2) # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3) # [bsz, q_len, 3, nh, hd]\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n    key_padding_mask = attention_mask\n\n\n    if key_padding_mask is None:\n        qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n        max_s = q_len\n        cu_q_lens = torch.arange(0, (bsz + 1) * q_len, step=q_len, dtype=torch.int32,\n                                device=qkv.device)\n        output = flash_attn_unpadded_qkvpacked_func(\n            qkv, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(output, '(b s) ... -> b s ...', b=bsz)\n    else:\n        nheads = qkv.shape[-2]\n        x = rearrange(qkv, 'b s three h d -> b s (three h d)')\n        x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n        x_unpad = rearrange(x_unpad, 'nnz (three h d) -> nnz three h d', three=3, h=nheads)\n        output_unpad = flash_attn_unpadded_qkvpacked_func(\n            x_unpad, cu_q_lens, max_s, 0.0,\n            softmax_scale=None, causal=True\n        )\n        output = rearrange(pad_input(rearrange(output_unpad, 'nnz h d -> nnz (h d)'),\n                                    indices, bsz, q_len),\n                        'b s (h d) -> b s h d', h=nheads)\n    return self.o_proj(rearrange(output,\n                                    'b s h d -> b s (h d)')), None, None", "\n\n# Disable the transformation of the attention mask in LlamaModel as the flash attention\n# requires the attention mask to be the same as the key_padding_mask\ndef _prepare_decoder_attention_mask(self, attention_mask, input_shape,\n                                    inputs_embeds, past_key_values_length):\n    # [bsz, seq_len]\n    return attention_mask\n\n\ndef replace_llama_attn_with_flash_attn():\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward", "\n\ndef replace_llama_attn_with_flash_attn():\n    transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask\n    transformers.models.llama.modeling_llama.LlamaAttention.forward = forward\n"]}
{"filename": "mQA/utils.py", "chunked_list": ["import torch\nfrom transformers.utils import logging\nfrom transformers.data.processors.squad import DataProcessor, _is_whitespace\nimport os\nfrom tqdm import tqdm\nimport json\nlogger = logging.get_logger(__name__)\n\ndef collate_to_max_length_llama(batch):\n    \"\"\"\n    adapted form https://github.com/ShannonAI/mrc-for-flat-nested-ner\n    pad to maximum length of this batch\n    Args:\n        batch: a batch of samples, each contains a list of field data(Tensor):\n            tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, match_labels, sample_idx, label_idx\n    Returns:\n        output: list of field batched data, which shape is [batch, max_length]\n    \"\"\"\n    batch_size = len(batch)\n    max_length_enc = max(x[0].shape[1] for x in batch)\n\n    output = []\n\n    for field_idx in range(3):\n        if field_idx != 1:\n            pad_output = torch.full([batch_size, max_length_enc], 32000, dtype=batch[0][field_idx].dtype) # 32000 as pad token\n        else:\n            pad_output = torch.full([batch_size, max_length_enc], 0, dtype=batch[0][field_idx].dtype)\n\n        for sample_idx in range(batch_size):\n            data = batch[sample_idx][field_idx]\n            pad_output[sample_idx][max_length_enc - data.shape[1]:] = data # padding left\n        output.append(pad_output)\n\n    return output", "def collate_to_max_length_llama(batch):\n    \"\"\"\n    adapted form https://github.com/ShannonAI/mrc-for-flat-nested-ner\n    pad to maximum length of this batch\n    Args:\n        batch: a batch of samples, each contains a list of field data(Tensor):\n            tokens, token_type_ids, start_labels, end_labels, start_label_mask, end_label_mask, match_labels, sample_idx, label_idx\n    Returns:\n        output: list of field batched data, which shape is [batch, max_length]\n    \"\"\"\n    batch_size = len(batch)\n    max_length_enc = max(x[0].shape[1] for x in batch)\n\n    output = []\n\n    for field_idx in range(3):\n        if field_idx != 1:\n            pad_output = torch.full([batch_size, max_length_enc], 32000, dtype=batch[0][field_idx].dtype) # 32000 as pad token\n        else:\n            pad_output = torch.full([batch_size, max_length_enc], 0, dtype=batch[0][field_idx].dtype)\n\n        for sample_idx in range(batch_size):\n            data = batch[sample_idx][field_idx]\n            pad_output[sample_idx][max_length_enc - data.shape[1]:] = data # padding left\n        output.append(pad_output)\n\n    return output", "\n\nclass SquadProcessor(DataProcessor):\n    \"\"\"\n    Processor for the SQuAD data set. overridden by SquadV1Processor and SquadV2Processor, used by the version 1.1 and\n    version 2.0 of SQuAD, respectively.\n    \"\"\"\n\n    train_file = None\n    dev_file = None\n\n    def _get_example_from_tensor_dict(self, tensor_dict, evaluate=False):\n        if not evaluate:\n            answer = tensor_dict[\"answers\"][\"text\"][0].numpy().decode(\"utf-8\")\n            answer_start = tensor_dict[\"answers\"][\"answer_start\"][0].numpy()\n            answers = []\n        else:\n            answers = [\n                {\"answer_start\": start.numpy(), \"text\": text.numpy().decode(\"utf-8\")}\n                for start, text in zip(tensor_dict[\"answers\"][\"answer_start\"], tensor_dict[\"answers\"][\"text\"])\n            ]\n\n            answer = None\n            answer_start = None\n\n        return SquadExample(\n            qas_id=tensor_dict[\"id\"].numpy().decode(\"utf-8\"),\n            question_text=tensor_dict[\"question\"].numpy().decode(\"utf-8\"),\n            context_text=tensor_dict[\"context\"].numpy().decode(\"utf-8\"),\n            answer_text=answer,\n            start_position_character=answer_start,\n            title=tensor_dict[\"title\"].numpy().decode(\"utf-8\"),\n            answers=answers,\n        )\n\n    def get_examples_from_dataset(self, dataset, evaluate=False):\n        \"\"\"\n        Creates a list of :class:`~transformers.data.processors.squad.SquadExample` using a TFDS dataset.\n\n        Args:\n            dataset: The tfds dataset loaded from `tensorflow_datasets.load(\"squad\")`\n            evaluate: Boolean specifying if in evaluation mode or in training mode\n\n        Returns:\n            List of SquadExample\n\n        \"\"\"\n\n        if evaluate:\n            dataset = dataset[\"validation\"]\n        else:\n            dataset = dataset[\"train\"]\n\n        examples = []\n        for tensor_dict in tqdm(dataset):\n            examples.append(self._get_example_from_tensor_dict(tensor_dict, evaluate=evaluate))\n\n        return examples\n\n    def get_examples(self, data_dir, filename=None):\n        \"\"\"\n        Returns the evaluation example from the data directory.\n\n        Args:\n            data_dir: Directory containing the data files used for training and evaluating.\n            filename: None by default, specify this if the evaluation file has a different name than the original one\n                which is `dev-v1.1.json` and `dev-v2.0.json` for squad versions 1.1 and 2.0 respectively.\n        \"\"\"\n        if data_dir is None:\n            data_dir = \"\"\n\n        if self.dev_file is None:\n            raise ValueError(\"SquadProcessor should be instantiated via SquadV1Processor or SquadV2Processor\")\n\n        with open(\n            os.path.join(data_dir, self.dev_file if filename is None else filename), \"r\", encoding=\"utf-8\"\n        ) as reader:\n            input_data = json.load(reader)[\"data\"]\n        return self._create_examples(input_data, \"dev\")\n\n    def _create_examples(self, input_data, set_type):\n        is_training = set_type == \"train\"\n        examples = []\n        for entry in tqdm(input_data):\n            title = entry[\"title\"] if entry.get('title') else None\n            for paragraph in entry[\"paragraphs\"]:\n                context_text = paragraph[\"context\"]\n                for qa in paragraph[\"qas\"]:\n                    qas_id = qa[\"id\"]\n                    question_text = qa[\"question\"]\n                    start_position_character = None\n                    answer_text = None\n                    answers = []\n\n                    is_impossible = qa.get(\"is_impossible\", False)\n                    if not is_impossible:\n                        if is_training:\n                            answer = qa[\"answers\"][0]\n                            answer_text = answer[\"text\"]\n                            start_position_character = answer[\"answer_start\"]\n                        else:\n                            answers = qa[\"answers\"]\n\n                    example = SquadExample(\n                        qas_id=qas_id,\n                        question_text=question_text,\n                        context_text=context_text,\n                        answer_text=answer_text,\n                        start_position_character=start_position_character,\n                        title=title,\n                        is_impossible=is_impossible,\n                        answers=answers,\n                    )\n                    examples.append(example)\n        return examples", "\n\nclass SquadV1Processor(SquadProcessor):\n    train_file = \"train-v1.1.json\"\n    dev_file = \"dev-v1.1.json\"\n\nclass SquadV2Processor(SquadProcessor):\n    train_file = \"train-v2.0.json\"\n    dev_file = \"dev-v2.0.json\"\n\nclass SquadExample:\n    \"\"\"\n    A single training/test example for the Squad dataset, as loaded from disk.\n\n    Args:\n        qas_id: The example's unique identifier\n        question_text: The question string\n        context_text: The context string\n        answer_text: The answer string\n        start_position_character: The character position of the start of the answer\n        title: The title of the example\n        answers: None by default, this is used during evaluation. Holds answers as well as their start positions.\n        is_impossible: False by default, set to True if the example has no possible answer.\n    \"\"\"\n\n    def __init__(\n        self,\n        qas_id,\n        question_text,\n        context_text,\n        answer_text,\n        start_position_character,\n        title,\n        answers=[],\n        is_impossible=False,\n    ):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.context_text = context_text\n        self.answer_text = answer_text\n        self.title = title\n        self.is_impossible = is_impossible\n        self.answers = answers\n\n        self.start_position, self.end_position = 0, 0\n\n        doc_tokens = []\n        doc_idx = []\n        char_to_word_offset = []\n        prev_is_whitespace = True\n\n        # Split on whitespace so that different tokens may be attributed to their original position.\n        for i_c, c in enumerate(self.context_text):\n            if _is_whitespace(c):\n                prev_is_whitespace = True\n            else:\n                if prev_is_whitespace:\n                    doc_tokens.append(c)\n                    doc_idx.append(i_c)\n                else:\n                    doc_tokens[-1] += c\n                prev_is_whitespace = False\n            char_to_word_offset.append(len(doc_tokens) - 1)\n        assert len(doc_tokens) == len(doc_idx)\n        self.doc_tokens = list(zip(doc_tokens, doc_idx))\n        self.char_to_word_offset = char_to_word_offset\n\n        # Start and end positions only has a value during evaluation.\n        if start_position_character is not None and not is_impossible:\n            self.start_position = char_to_word_offset[start_position_character]\n            self.end_position = char_to_word_offset[\n                min(start_position_character + len(answer_text) - 1, len(char_to_word_offset) - 1)\n            ]", "\nclass SquadExample:\n    \"\"\"\n    A single training/test example for the Squad dataset, as loaded from disk.\n\n    Args:\n        qas_id: The example's unique identifier\n        question_text: The question string\n        context_text: The context string\n        answer_text: The answer string\n        start_position_character: The character position of the start of the answer\n        title: The title of the example\n        answers: None by default, this is used during evaluation. Holds answers as well as their start positions.\n        is_impossible: False by default, set to True if the example has no possible answer.\n    \"\"\"\n\n    def __init__(\n        self,\n        qas_id,\n        question_text,\n        context_text,\n        answer_text,\n        start_position_character,\n        title,\n        answers=[],\n        is_impossible=False,\n    ):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.context_text = context_text\n        self.answer_text = answer_text\n        self.title = title\n        self.is_impossible = is_impossible\n        self.answers = answers\n\n        self.start_position, self.end_position = 0, 0\n\n        doc_tokens = []\n        doc_idx = []\n        char_to_word_offset = []\n        prev_is_whitespace = True\n\n        # Split on whitespace so that different tokens may be attributed to their original position.\n        for i_c, c in enumerate(self.context_text):\n            if _is_whitespace(c):\n                prev_is_whitespace = True\n            else:\n                if prev_is_whitespace:\n                    doc_tokens.append(c)\n                    doc_idx.append(i_c)\n                else:\n                    doc_tokens[-1] += c\n                prev_is_whitespace = False\n            char_to_word_offset.append(len(doc_tokens) - 1)\n        assert len(doc_tokens) == len(doc_idx)\n        self.doc_tokens = list(zip(doc_tokens, doc_idx))\n        self.char_to_word_offset = char_to_word_offset\n\n        # Start and end positions only has a value during evaluation.\n        if start_position_character is not None and not is_impossible:\n            self.start_position = char_to_word_offset[start_position_character]\n            self.end_position = char_to_word_offset[\n                min(start_position_character + len(answer_text) - 1, len(char_to_word_offset) - 1)\n            ]"]}
{"filename": "mQA/squad_evaluation_v1.py", "chunked_list": ["\"\"\"Official evaluation script for the MRQA Workshop Shared Task.\nAdapted fromt the SQuAD v1.1 official evaluation script.\nUsage:\n    python official_eval.py dataset_file.jsonl.gz prediction_file.json\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse", "\nimport argparse\nimport string\nimport re\nimport json\nimport sys\nfrom collections import Counter\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))", "\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))", "\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1", "\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)", "\n\ndef read_predictions(prediction_file):\n    with open(prediction_file) as f:\n        predictions = json.load(f)\n    return predictions\n\ndef evaluate(dataset, predictions):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        for paragraph in article['paragraphs']:\n            for qa in paragraph['qas']:\n                total += 1\n                if qa['id'] not in predictions:\n                    message = 'Unanswered question ' + qa['id'] + \\\n                              ' will receive score 0.'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n                prediction = predictions[qa['id']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {'exact_match': exact_match, 'f1': f1}", "\ndef compare(dataset, predictions_str, predictions_tok):\n    f1 = exact_match_str = exact_match_tok = total = 0\n    for article in dataset:\n        for paragraph in article['paragraphs']:\n            for qa in paragraph['qas']:\n                total += 1\n                if qa['id'] not in predictions_str and qa['id'] not in predictions_tok:\n                    message = 'Unanswered question ' + qa['id'] + \\\n                              ' will receive score 0.'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n                prediction_str = predictions_str[qa['id']]\n                prediction_tok = predictions_tok[qa['id']]\n                exact_match_str_one = metric_max_over_ground_truths(\n                    exact_match_score, prediction_str, ground_truths)\n                exact_match_str += exact_match_str_one\n                exact_match_tok_one = metric_max_over_ground_truths(\n                    exact_match_score, prediction_tok, ground_truths)\n                exact_match_tok += exact_match_tok_one\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction_str, ground_truths)\n                if exact_match_str_one == 1 and exact_match_tok_one == 0:\n                    print(qa['id'])\n                    print('str answer:', prediction_str)\n                    print('tok answer:', prediction_tok)\n                    print('ground truth:', ground_truths)\n                    print()\n\n    exact_match = 100.0 * exact_match_str / total\n    f1 = 100.0 * f1 / total\n\n    return {'exact_match': exact_match, 'f1': f1}", "\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='Evaluation for MRQA Workshop Shared Task')\n    parser.add_argument('dataset_file', type=str, help='Dataset File')\n    parser.add_argument('prediction_file', type=str, help='Prediction File')\n    parser.add_argument('--skip-no-answer', action='store_true')\n    args = parser.parse_args()\n\n    with open(args.dataset_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        dataset = dataset_json['data']\n    with open(args.prediction_file+'_tok.json') as prediction_file:\n        predictions_tok = json.load(prediction_file)\n    with open(args.prediction_file+'_str.json') as prediction_file:\n        predictions_str = json.load(prediction_file)\n    compare(dataset, predictions_str, predictions_tok)", ""]}
{"filename": "mQA/mlqa_evaluation_v1.py", "chunked_list": ["# Copyright (c) 2019-present, Facebook, Inc\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\"\"\" Official evaluation script for the MLQA dataset. \"\"\"\nfrom __future__ import print_function\nfrom collections import Counter\nimport string", "from collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\nimport unicodedata\n\n\nPUNCT = {chr(i) for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P')}.union(string.punctuation)", "\nPUNCT = {chr(i) for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P')}.union(string.punctuation)\nWHITESPACE_LANGS = ['en', 'es', 'hi', 'vi', 'de', 'ar']\nMIXED_SEGMENTATION_LANGS = ['zh']\n\n\ndef whitespace_tokenize(text):\n    return text.split()\n\n\ndef mixed_segmentation(text):\n    segs_out = []\n    temp_str = \"\"\n    for char in text:\n        if re.search(r'[\\u4e00-\\u9fa5]', char) or char in PUNCT:\n            if temp_str != \"\":\n                ss = whitespace_tokenize(temp_str)\n                segs_out.extend(ss)\n                temp_str = \"\"\n            segs_out.append(char)\n        else:\n            temp_str += char\n\n    if temp_str != \"\":\n        ss = whitespace_tokenize(temp_str)\n        segs_out.extend(ss)\n\n    return segs_out", "\n\ndef mixed_segmentation(text):\n    segs_out = []\n    temp_str = \"\"\n    for char in text:\n        if re.search(r'[\\u4e00-\\u9fa5]', char) or char in PUNCT:\n            if temp_str != \"\":\n                ss = whitespace_tokenize(temp_str)\n                segs_out.extend(ss)\n                temp_str = \"\"\n            segs_out.append(char)\n        else:\n            temp_str += char\n\n    if temp_str != \"\":\n        ss = whitespace_tokenize(temp_str)\n        segs_out.extend(ss)\n\n    return segs_out", "\n\ndef normalize_answer(s, lang):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n\n    def remove_articles(text, lang):\n        if lang == 'en':\n            return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n        elif lang == 'es':\n            return re.sub(r'\\b(un|una|unos|unas|el|la|los|las)\\b', ' ', text)\n        elif lang == 'hi':\n            return text # Hindi does not have formal articles\n        elif lang == 'vi':\n            return re.sub(r'\\b(c\u1ee7a|l\u00e0|c\u00e1i|chi\u1ebfc|nh\u1eefng)\\b', ' ', text)\n        elif lang == 'de':\n            return re.sub(r'\\b(ein|eine|einen|einem|eines|einer|der|die|das|den|dem|des)\\b', ' ', text)\n        elif lang == 'ar':\n            return re.sub('\\s\u0627\u0644^|\u0627\u0644', ' ', text)\n        elif lang == 'zh':\n            return text # Chinese does not have formal articles\n        else:\n            raise Exception('Unknown Language {}'.format(lang))\n\n    def white_space_fix(text, lang):\n        if lang in WHITESPACE_LANGS:\n            tokens = whitespace_tokenize(text)\n        elif lang in MIXED_SEGMENTATION_LANGS:\n            tokens = mixed_segmentation(text)\n        else:\n            raise Exception('Unknown Language {}'.format(lang))\n        return ' '.join([t for t in tokens if t.strip() != ''])\n\n    def remove_punc(text):\n        return ''.join(ch for ch in text if ch not in PUNCT)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s)), lang), lang)", "\n\ndef f1_score(prediction, ground_truth, lang):\n    prediction_tokens = normalize_answer(prediction, lang).split()\n    ground_truth_tokens = normalize_answer(ground_truth, lang).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1", "\n\ndef exact_match_score(prediction, ground_truth, lang):\n    return (normalize_answer(prediction, lang) == normalize_answer(ground_truth, lang))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths, lang):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth, lang)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)", "\n\ndef evaluate(dataset, predictions, lang):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        for paragraph in article['paragraphs']:\n            for qa in paragraph['qas']:\n                total += 1\n                if qa['id'] not in predictions:\n                    message = 'Unanswered question ' + qa['id'] + \\\n                              ' will receive score 0.'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n                prediction = predictions[qa['id']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths, lang)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths, lang)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {'exact_match': exact_match, 'f1': f1}", "\ndef compare(dataset, predictions_mpmr, lang):\n    count = 0\n    f1 = exact_match_mpmr = exact_match_xlmr = total = 0\n    for i_a, article in enumerate(dataset):\n        for paragraph in article['paragraphs']:\n            for qa in paragraph['qas']:\n                total += 1\n                if qa['id'] not in predictions_mpmr :\n                    message = 'Unanswered question ' + qa['id'] + \\\n                              ' will receive score 0.'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n                prediction_mpmr = predictions_mpmr[qa['id']]\n                # prediction_xlmr = predictions_xlmr[qa['id']]\n                exact_match_mpmr_one = metric_max_over_ground_truths(\n                    exact_match_score, prediction_mpmr, ground_truths, lang)\n                exact_match_mpmr += exact_match_mpmr_one\n                # exact_match_xlmr_one = metric_max_over_ground_truths(\n                #     exact_match_score, prediction_xlmr, ground_truths, lang)\n                # exact_match_xlmr += exact_match_xlmr_one\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction_mpmr, ground_truths, lang)\n\n                print('id', i_a)\n                print(qa['id'])\n                print('question', qa['question'])\n                print('mpmr answer:', prediction_mpmr, normalize_answer(prediction_mpmr, lang))\n                print('ground truth:', ground_truths, [normalize_answer(x, lang) for x in ground_truths])\n                print()\n                # if \"\uff08\" in ground_truths[0] or  \"\uff09\" in ground_truths[0]:\n\n\n    exact_match = 100.0 * exact_match_mpmr / total\n    f1 = 100.0 * f1 / total\n    print({'exact_match': exact_match, 'f1': f1})\n    return {'exact_match': exact_match, 'f1': f1}", "\nif __name__ == '__main__':\n    expected_version = '1.0'\n    parser = argparse.ArgumentParser(\n        description='Evaluation for MLQA ' + expected_version)\n    parser.add_argument('dataset_file', help='Dataset file')\n    parser.add_argument('prediction_file', help='Prediction File')\n    parser.add_argument('answer_language', help='Language code of answer language')\n\n    args = parser.parse_args()\n    with open(args.dataset_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        dataset = dataset_json['data']\n    with open(args.prediction_file) as prediction_file:\n        predictions_mpmr = json.load(prediction_file)\n    # with open(args.prediction_file+'.json') as prediction_file:\n    #     predictions_xlmr = json.load(prediction_file)\n    compare(dataset, predictions_mpmr, args.answer_language)"]}
{"filename": "mQA/mrqa_official_eval.py", "chunked_list": ["\"\"\"Official evaluation script for the MRQA Workshop Shared Task.\nAdapted fromt the SQuAD v1.1 official evaluation script.\nUsage:\n    python official_eval.py dataset_file.jsonl.gz prediction_file.json\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse", "\nimport argparse\nimport string\nimport re\nimport json\nimport gzip\nimport sys\nfrom collections import Counter\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))", "\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))", "\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1", "\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)", "\n\ndef read_predictions(prediction_file):\n    with open(prediction_file) as f:\n        predictions = json.load(f)\n    return predictions\n\n\ndef read_answers(gold_file):\n    answers = {}\n    if gold_file.endswith('.gz'):\n        with gzip.open(gold_file, 'rb') as f:\n            for i, line in enumerate(f):\n                example = json.loads(line)\n                if i == 0 and 'header' in example:\n                    continue\n                for qa in example['qas']:\n                    answers[qa['qid']] = qa['answers']\n    else:\n        with open(gold_file, 'r') as f:\n            for i, line in enumerate(f):\n                example = json.loads(line)\n                if i == 0 and ('header' in example or isinstance(example, list)):\n                    continue\n                for qa in example['qas']:\n                    answers[qa['qid']] = qa['answers']\n    return answers", "def read_answers(gold_file):\n    answers = {}\n    if gold_file.endswith('.gz'):\n        with gzip.open(gold_file, 'rb') as f:\n            for i, line in enumerate(f):\n                example = json.loads(line)\n                if i == 0 and 'header' in example:\n                    continue\n                for qa in example['qas']:\n                    answers[qa['qid']] = qa['answers']\n    else:\n        with open(gold_file, 'r') as f:\n            for i, line in enumerate(f):\n                example = json.loads(line)\n                if i == 0 and ('header' in example or isinstance(example, list)):\n                    continue\n                for qa in example['qas']:\n                    answers[qa['qid']] = qa['answers']\n    return answers", "\n\ndef evaluate(answers, predictions, skip_no_answer=False):\n    f1 = exact_match = total = 0\n    for qid, ground_truths in answers.items():\n        if qid not in predictions:\n            if not skip_no_answer:\n                message = 'Unanswered question %s will receive score 0.' % qid\n                print(message)\n                total += 1\n            continue\n        total += 1\n        prediction = predictions[qid]\n        exact_match += metric_max_over_ground_truths(\n            exact_match_score, prediction, ground_truths)\n        f1 += metric_max_over_ground_truths(\n            f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {'exact_match': exact_match, 'f1': f1}", "\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='Evaluation for MRQA Workshop Shared Task')\n    parser.add_argument('dataset_file', type=str, help='Dataset File')\n    parser.add_argument('prediction_file', type=str, help='Prediction File')\n    parser.add_argument('--skip-no-answer', action='store_true')\n    args = parser.parse_args()\n", "\n    # answers = read_answers(cached_path(args.dataset_file))\n    # predictions = read_predictions(cached_path(args.prediction_file))\n    # metrics = evaluate(answers, predictions, args.skip_no_answer)\n    #\n    # print(json.dumps(metrics))\n"]}
{"filename": "mQA/train-QA.py", "chunked_list": ["# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors, The HuggingFace Inc. team and Alibaba DAMO Academy.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#", "#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" Finetuning the library models for question-answering on SQuAD (DistilBERT, Bert, XLM, XLNet).\"\"\"\n\"\"\"Adapted from the HuggingFace SQuAD finetuning script for EQA.\"\"\"\n", "\"\"\"Adapted from the HuggingFace SQuAD finetuning script for EQA.\"\"\"\n\nimport argparse\nfrom decimal import Decimal\nimport logging\nfrom typing import Dict\nimport os\nimport random\nimport json\nimport transformers", "import json\nimport transformers\nfrom transformers import (\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n)\nfrom utils import collate_to_max_length_s2s, collate_to_max_length_bloom, collate_to_max_length_llama\nfrom data_clm import cache_mQAexamples as cache_mQAexamples_clm\nfrom transformers.trainer_utils import is_main_process", "from data_clm import cache_mQAexamples as cache_mQAexamples_clm\nfrom transformers.trainer_utils import is_main_process\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, SequentialSampler\nfrom tqdm import tqdm\nfrom squad_evaluation_v1 import evaluate as evaluateSQuAD\nfrom mlqa_evaluation_v1 import evaluate as evaluateMLQA\ntry:\n    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\nexcept:\n    print('not applicable for flash attention')", "try:\n    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\nexcept:\n    print('not applicable for flash attention')\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"</s>\"\nDEFAULT_UNK_TOKEN = \"[UNK]\"", "DEFAULT_BOS_TOKEN = \"</s>\"\nDEFAULT_UNK_TOKEN = \"[UNK]\"\n\nlogger = logging.getLogger(__name__)\n\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg", "\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg", "\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\ndef evaluate(args, data_path, model, tokenizer, prefix=\"\", split=\"dev\", lang='en'):\n    if data_path.startswith(\"SQuAD\"):\n        data_path = \"SQuAD\"\n    dataset = load_and_cache_examples(args, tokenizer, evaluate=True, data_path=data_path, split=split, lang=lang)\n\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate_to_max_length_llama)\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.warning(\"***** Running evaluation {} *****\".format(prefix))\n    logger.warning(\"  Num examples = %d\", len(dataset))\n    logger.warning(\"  Batch size = %d\", args.eval_batch_size)\n\n    all_results = []\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n            }\n\n            outputs = model.generate(**inputs,\n                                     max_new_tokens=64,\n                                     eos_token_id=13,\n                                     use_cache=False,\n                                     )\n        dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n        input_text = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[0]]\n        dec = [dec[i].replace(input_text[i], \"\").strip() for i in range(len(dec))]\n        dec = [dec[i].strip() for i in range(len(dec))]\n\n        all_results.extend(dec)\n    predictions = {}\n    for i_i, item in enumerate(all_results):\n        feature = dataset.all_data[i_i]\n        qid = feature[0]\n        item = item\n        if \"None\" in item:\n            span = \"\"\n        else:\n            span = item.strip()\n        predictions[qid] = (span,)\n    predictions = {x: predictions[x][0] for x in predictions}\n    preds_file = os.path.join(args.output_dir, args.data_path + '-predictions.json')\n    with open(preds_file, 'w') as writer:\n        json.dump(predictions, writer)\n\n    if data_path == \"SQuAD\":\n        with open(os.path.join(\"Data\", \"squad\", \"dev-v1.1.json\")) as f:\n            gold_answers = json.load(f)['data']\n        scores = evaluateSQuAD(gold_answers, predictions)\n        logger.warning(\n            f\"EVAL INFO {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n    elif data_path == \"XQuAD\":\n        with open(os.path.join(\"Data\", \"xquad\", \"xquad.\" + lang + \".json\")) as f:\n            gold_answers = json.load(f)['data']\n        scores = evaluateSQuAD(gold_answers, predictions)\n        logger.warning(\n            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n    elif data_path == \"MLQA\":\n        with open(os.path.join(\"Data\", \"mlqa/test\", \"test-context-\" + lang + \"-question-\" + lang + \".json\")) as f:\n            gold_answers = json.load(f)['data']\n        scores = evaluateMLQA(gold_answers, predictions, lang=lang)\n        logger.warning(\n            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n    elif data_path == \"TyDiQA\":\n        with open(os.path.join(\"Data\", \"tydiqa/tydiqa-goldp-v1.1-dev\", \"tydiqa.goldp.\" + lang + \".dev.json\")) as f:\n            gold_answers = json.load(f)['data']\n        scores = evaluateSQuAD(gold_answers, predictions)\n        logger.warning(\n            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n    else:\n        raise NotImplementedError\n\n    return scores, predictions", "\ndef evaluate(args, data_path, model, tokenizer, prefix=\"\", split=\"dev\", lang='en'):\n    if data_path.startswith(\"SQuAD\"):\n        data_path = \"SQuAD\"\n    dataset = load_and_cache_examples(args, tokenizer, evaluate=True, data_path=data_path, split=split, lang=lang)\n\n    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(args.output_dir)\n\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(dataset)\n    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate_to_max_length_llama)\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.warning(\"***** Running evaluation {} *****\".format(prefix))\n    logger.warning(\"  Num examples = %d\", len(dataset))\n    logger.warning(\"  Batch size = %d\", args.eval_batch_size)\n\n    all_results = []\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {\n                \"input_ids\": batch[0],\n                \"attention_mask\": batch[1],\n            }\n\n            outputs = model.generate(**inputs,\n                                     max_new_tokens=64,\n                                     eos_token_id=13,\n                                     use_cache=False,\n                                     )\n        dec = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n        input_text = [tokenizer.decode(ids, skip_special_tokens=True) for ids in batch[0]]\n        dec = [dec[i].replace(input_text[i], \"\").strip() for i in range(len(dec))]\n        dec = [dec[i].strip() for i in range(len(dec))]\n\n        all_results.extend(dec)\n    predictions = {}\n    for i_i, item in enumerate(all_results):\n        feature = dataset.all_data[i_i]\n        qid = feature[0]\n        item = item\n        if \"None\" in item:\n            span = \"\"\n        else:\n            span = item.strip()\n        predictions[qid] = (span,)\n    predictions = {x: predictions[x][0] for x in predictions}\n    preds_file = os.path.join(args.output_dir, args.data_path + '-predictions.json')\n    with open(preds_file, 'w') as writer:\n        json.dump(predictions, writer)\n\n    if data_path == \"SQuAD\":\n        with open(os.path.join(\"Data\", \"squad\", \"dev-v1.1.json\")) as f:\n            gold_answers = json.load(f)['data']\n        scores = evaluateSQuAD(gold_answers, predictions)\n        logger.warning(\n            f\"EVAL INFO {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n    elif data_path == \"XQuAD\":\n        with open(os.path.join(\"Data\", \"xquad\", \"xquad.\" + lang + \".json\")) as f:\n            gold_answers = json.load(f)['data']\n        scores = evaluateSQuAD(gold_answers, predictions)\n        logger.warning(\n            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n    elif data_path == \"MLQA\":\n        with open(os.path.join(\"Data\", \"mlqa/test\", \"test-context-\" + lang + \"-question-\" + lang + \".json\")) as f:\n            gold_answers = json.load(f)['data']\n        scores = evaluateMLQA(gold_answers, predictions, lang=lang)\n        logger.warning(\n            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n    elif data_path == \"TyDiQA\":\n        with open(os.path.join(\"Data\", \"tydiqa/tydiqa-goldp-v1.1-dev\", \"tydiqa.goldp.\" + lang + \".dev.json\")) as f:\n            gold_answers = json.load(f)['data']\n        scores = evaluateSQuAD(gold_answers, predictions)\n        logger.warning(\n            f\"EVAL INFO {lang} of {data_path} -> valid_f1 is: {scores['f1']}; exact match is: {scores['exact_match']}.\")\n    else:\n        raise NotImplementedError\n\n    return scores, predictions", "\n\ndef load_and_cache_examples(args, tokenizer, evaluate=False, data_path=None, split=\"train\", lang='en'):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n        torch.distributed.barrier()\n    if data_path is None:\n        data_path = args.data_path\n\n    features = cache_mQAexamples_clm(args, tokenizer, data_path, split, lang=lang)\n\n\n    logger.warning(\"load data from {}.\".format(data_path))\n    dataset = QADataset(features=features,\n                        data_name=data_path,\n                        tokenizer=tokenizer,\n                        )\n    if args.local_rank == 0 and not evaluate:\n        # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n        torch.distributed.barrier()\n    return dataset", "\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    # Required parameters\n    parser.add_argument(\n        \"--model_type\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Model type selected in the list: \" + \", \".join(MODEL_TYPES),\n    )\n    parser.add_argument(\n        \"--model_name_or_path\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=None,\n        type=str,\n        required=True,\n        help=\"The output directory where the model checkpoints and predictions will be written.\",\n    )\n\n    # Other parameters\n    parser.add_argument(\n        \"--data_dir\",\n        default=None,\n        type=str,\n        help=\"The input data dir. Should contain the .json files for the task.\"\n        + \"If no data dir or train/predict files are specified, will run with tensorflow_datasets.\",\n    )\n    parser.add_argument(\n        \"--data_path\",\n        # choices=[\"out_dev\", \"SQuAD\", \"NewsQA\", \"TriviaQA\", 'SearchQA', 'HotpotQA', \"NaturalQuestions\", \"SQuAD2\"],\n        default=\"SQuAD\",\n        type=str,\n        help=\"The input file.\",\n    )\n    parser.add_argument(\n        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        default=\"\",\n        type=str,\n        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n    )\n    parser.add_argument(\n        \"--cache_dir\",\n        default=\"\",\n        type=str,\n        help=\"Where do you want to store the pre-trained models downloaded from huggingface.co\",\n    )\n    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n    parser.add_argument(\n        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n    )\n\n    parser.add_argument(\n        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n    )\n    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\")\n    parser.add_argument(\n        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n    )\n    parser.add_argument(\n        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n    )\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n    parser.add_argument(\n        \"--fp16\",\n        action=\"store_true\",\n        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n    )\n    parser.add_argument(\n        \"--fp16_opt_level\",\n        type=str,\n        default=\"O1\",\n        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n        \"See details at https://nvidia.github.io/apex/amp.html\",\n    )\n    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"Can be used for distant debugging.\")\n    parser.add_argument(\"--keep_frac\", type=float, default=1.0, help=\"The fraction of the balanced dataset to keep.\")\n    parser.add_argument(\n        \"--flash_attn\", action=\"store_true\", help=\"use flash attn\"\n    )\n    args = parser.parse_args()\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n\n        print(\"Waiting for debugger attach\")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(\"cuda\", args.local_rank)\n        torch.distributed.init_process_group(backend=\"nccl\")\n        args.n_gpu = 1\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=  logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        transformers.utils.logging.enable_default_handler()\n        transformers.utils.logging.enable_explicit_format()\n    # Set seed\n    set_seed(args)\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        # Make sure only the first process in distributed training will download model & vocab\n        torch.distributed.barrier()\n    if args.flash_attn:\n        replace_llama_attn_with_flash_attn()\n    args.model_type = args.model_type.lower()\n\n    kwargs = {\"torch_dtype\": torch.float16}\n    model = transformers.LlamaForCausalLM.from_pretrained(\n        args.model_name_or_path,\n        cache_dir=args.cache_dir,\n        low_cpu_mem_usage=True, **kwargs,\n    )\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        args.model_name_or_path,\n        cache_dir=args.cache_dir,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n    if tokenizer.pad_token is None:\n        smart_tokenizer_and_embedding_resize(\n            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n            tokenizer=tokenizer,\n            model=model,\n        )\n    if \"llama\" in args.model_name_or_path:\n        tokenizer.add_special_tokens({\n            \"eos_token\": DEFAULT_EOS_TOKEN,\n            \"bos_token\": DEFAULT_BOS_TOKEN,\n            \"unk_token\": DEFAULT_UNK_TOKEN,\n        })\n    # model = model_PMR[args.model_type](config)\n\n    if args.local_rank == 0:\n        # Make sure only the first process in distributed training will download model & vocab\n        torch.distributed.barrier()\n\n    model.to(args.device)\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Before we do anything with models, we want to ensure that we get fp16 execution of torch.einsum if args.fp16 is set.\n    # Otherwise it'll default to \"promote\" mode, and we'll get fp32 operations. Note that running `--fp16_opt_level=\"O2\"` will\n    # remove the need for this code, but it is still valid.\n    if args.fp16:\n        try:\n            import apex\n\n            apex.amp.register_half_function(torch, \"einsum\")\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n\n\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(args.local_rank):\n        transformers.utils.logging.set_verbosity_warning()\n    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n    if args.do_eval and args.local_rank in [-1, 0]:\n        if args.do_train:\n            pass\n        else:\n            global_step = \"\"\n            if args.data_path == 'XQuAD':\n                results, predictions = evaluate(args, 'SQuAD', model, tokenizer, split=\"dev\", prefix='')\n\n                all_test = {}\n                for lang in ['en', 'ar', 'de', 'el', 'es', 'hi', 'ru', 'th', 'tr', 'vi', 'zh', 'ro']:\n                    test_results, predictions = evaluate(args, \"XQuAD\", model, tokenizer, prefix='', split=\"dev\", lang=lang)\n                    all_test[\"XQuAD-\" + lang] = (test_results['f1'], test_results['exact_match'])\n                avg_f1 = avg_em = 0\n                for lang in ['en', 'ar', 'de', 'el', 'es', 'hi', 'ru', 'th', 'tr', 'vi', 'zh', ]:\n                    avg_f1 += all_test[\"XQuAD-\" + lang][0]\n                    avg_em += all_test[\"XQuAD-\" + lang][1]\n                all_test[\"XQuAD-avg\"] = (avg_f1 / 11, avg_em / 11)\n                logger.warning(\n                    f\"EVAL INFO mix of XQuAD -> valid_f1 is: {all_test['XQuAD-avg'][0]}; exact match is: {all_test['XQuAD-avg'][1]}.\")\n            elif args.data_path == 'MLQA':\n                # results, predictions = evaluate(args, 'SQuAD', model, tokenizer, split=\"dev\", prefix='')\n                all_test = {}\n                for lang in ['en', 'es', 'de', 'ar', 'hi', 'vi', 'zh']: # 'en', 'es', 'de', 'ar',\n                    test_results, predictions = evaluate(args, \"MLQA\", model, tokenizer, prefix='', split=\"dev\", lang=lang)\n                    all_test[\"MLQA-\" + lang] = (test_results['f1'], test_results['exact_match'])\n                avg_f1 = avg_em = 0\n                for lang in ['en', 'es', 'de', 'ar', 'hi', 'vi', 'zh']:\n                    avg_f1 += all_test[\"MLQA-\" + lang][0]\n                    avg_em += all_test[\"MLQA-\" + lang][1]\n                all_test[\"MLQA-avg\"] = (avg_f1 / 7, avg_em / 7)\n                logger.warning(\n                    f\"EVAL INFO mix of MLQA -> valid_f1 is: {all_test['MLQA-avg'][0]}; exact match is: {all_test['MLQA-avg'][1]}.\")\n            elif args.data_path == \"TyDiQA\":\n                # results, _ = evaluate(args, args.data_path, model, tokenizer, prefix=global_step, split=\"dev\",)\n                all_test = {}\n                for lang in ['en', 'ar', 'bn', 'fi', 'id', 'ko', 'ru', 'sw', 'te']: #'en', 'ar', 'bn', 'fi', 'id', 'ko', 'ru', 'sw',\n                    test_results, predictions = evaluate(args, \"TyDiQA\", model, tokenizer, prefix=global_step, split=\"dev\",\n                                                         lang=lang)\n                    all_test[\"TyDiQA-\" + lang] = (test_results['f1'], test_results['exact_match'])\n                avg_f1 = avg_em = 0\n                for lang in ['en', 'ar', 'bn', 'fi', 'id', 'ko', 'ru', 'sw', 'te']:\n                    avg_f1 += all_test[\"TyDiQA-\" + lang][0]\n                    avg_em += all_test[\"TyDiQA-\" + lang][1]\n                all_test[\"TyDiQA-avg\"] = (avg_f1 / 9, avg_em / 9)\n                logger.warning(\n                    f\"EVAL INFO mix of TyDiQA -> valid_f1 is: {all_test['TyDiQA-avg'][0]}; exact match is: {all_test['TyDiQA-avg'][1]}.\")\n            else:\n                raise NotImplementedError\n            save_line = [(Decimal(all_test[x][0]).quantize(Decimal(\"0.01\"), rounding=\"ROUND_HALF_UP\"),\n                          Decimal(all_test[x][1]).quantize(Decimal(\"0.01\"), rounding=\"ROUND_HALF_UP\")) for x in\n                         all_test]\n\n            save_line = [str(x[0]) + '/' + str(x[1]) for x in save_line]\n            save_line = '\\t'.join([args.data_path] + save_line)\n            with open('temp.txt', 'a') as writer2:\n                if args.data_path != 'XQuAD':\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, save_line])\n                    writer2.write(save_string + '\\n')\n\n                else:\n                    save_string = \"\\t\".join(\n                        [args.model_name_or_path, str(results['f1']),\n                         str(results['exact_match']), save_line])\n                    writer2.write(save_string + '\\n')", "\nif __name__ == \"__main__\":\n    main()\n"]}
