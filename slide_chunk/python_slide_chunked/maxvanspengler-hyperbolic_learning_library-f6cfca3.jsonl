{"filename": "tests/test_manifold_tensor.py", "chunked_list": ["import pytest\nimport torch\n\nfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\nfrom hypll.tensors import ManifoldTensor\n\n\n@pytest.fixture\ndef manifold_tensor() -> ManifoldTensor:\n    return ManifoldTensor(\n        data=[\n            [1.0, 2.0],\n            [3.0, 4.0],\n        ],\n        manifold=PoincareBall(c=Curvature()),\n        man_dim=-1,\n        requires_grad=True,\n    )", "def manifold_tensor() -> ManifoldTensor:\n    return ManifoldTensor(\n        data=[\n            [1.0, 2.0],\n            [3.0, 4.0],\n        ],\n        manifold=PoincareBall(c=Curvature()),\n        man_dim=-1,\n        requires_grad=True,\n    )", "\n\ndef test_attributes(manifold_tensor: ManifoldTensor):\n    # Check if the standard attributes are set correctly\n    # TODO: fix this once __eq__ has been implemented on manifolds\n    assert isinstance(manifold_tensor.manifold, PoincareBall)\n    assert manifold_tensor.man_dim == 1\n    # Check if non-callable attributes are taken from tensor attribute\n    assert manifold_tensor.is_cpu\n", "\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"requires cuda\")\ndef test_device_methods(manifold_tensor: ManifoldTensor):\n    # Check if we can move the manifold tensor to the gpu while keeping it intact\n    manifold_tensor = manifold_tensor.cuda()\n    assert isinstance(manifold_tensor, ManifoldTensor)\n    assert manifold_tensor.is_cuda\n    assert isinstance(manifold_tensor.manifold, PoincareBall)\n    assert manifold_tensor.man_dim == 1\n\n    # And move it back to the cpu\n    manifold_tensor = manifold_tensor.cpu()\n    assert isinstance(manifold_tensor, ManifoldTensor)\n    assert manifold_tensor.is_cpu\n    assert isinstance(manifold_tensor.manifold, PoincareBall)\n    assert manifold_tensor.man_dim == 1", "\n\ndef test_slicing(manifold_tensor: ManifoldTensor):\n    # First test slicing with the usual numpy slicing\n    manifold_tensor = ManifoldTensor(\n        data=torch.ones(2, 3, 4, 5, 6, 7, 8, 9),\n        manifold=PoincareBall(Curvature()),\n        man_dim=-2,\n    )\n    sliced_tensor = manifold_tensor[1, None, [0, 2], ..., None, 2:5, :, 1]\n    # Explanation of the output size: the dimension of size 2 dissappears because of the integer\n    # index. Then, a dim of size 1 is added by None. The size 3 dimension reduces to 2 because\n    # of the list of indices. The Ellipsis skips the dimensions of sizes 4, 5 and 6. Then another\n    # None leads to an insertion of a dimension of size 1. The dimension with size 7 is reduced\n    # to 3 because of the 2:5 slice. The manifold dimension of size 8 is left alone and the last\n    # dimension is removed because of an integer index.\n    assert list(sliced_tensor.size()) == [1, 2, 4, 5, 6, 1, 3, 8]\n    assert sliced_tensor.man_dim == 7\n\n    # Now we try to slice into the manifold dimension, which should raise an error\n    with pytest.raises(ValueError):\n        manifold_tensor[1, None, [0, 2], ..., None, 2:5, 5, 1]\n\n    # Next, we try long tensor indexing, which is used in embeddings\n    embedding_manifold_tensor = ManifoldTensor(\n        data=torch.ones(10, 3),\n        manifold=PoincareBall(Curvature()),\n        man_dim=-1,\n    )\n    indices = torch.Tensor(\n        [\n            [1, 2],\n            [3, 4],\n        ]\n    ).long()\n    embedding_selection = embedding_manifold_tensor[indices]\n    assert list(embedding_selection.size()) == [2, 2, 3]\n    assert embedding_selection.man_dim == 2\n\n    # This should fail if the man_dim is 0 on the embedding tensor though\n    embedding_manifold_tensor = ManifoldTensor(\n        data=torch.ones(10, 3),\n        manifold=PoincareBall(Curvature()),\n        man_dim=0,\n    )\n    with pytest.raises(ValueError):\n        embedding_manifold_tensor[indices]\n\n    # Lastly, embeddings with more dimensions should work too\n    embedding_manifold_tensor = ManifoldTensor(\n        data=torch.ones(10, 3, 3, 3),\n        manifold=PoincareBall(Curvature()),\n        man_dim=2,\n    )\n    embedding_selection = embedding_manifold_tensor[indices]\n    assert list(embedding_selection.size()) == [2, 2, 3, 3, 3]\n    assert embedding_selection.man_dim == 3", "\n\ndef test_torch_ops(manifold_tensor: ManifoldTensor):\n    # We want torch functons to raise an error\n    with pytest.raises(TypeError):\n        torch.norm(manifold_tensor)\n\n    # Same for torch.Tensor methods (callable attributes)\n    with pytest.raises(AttributeError):\n        manifold_tensor.mean()", ""]}
{"filename": "tests/manifolds/poincare_ball/test_curvature.py", "chunked_list": ["from pytest_mock import MockerFixture\n\nfrom hypll.manifolds.poincare_ball import Curvature\n\n\ndef test_curvature(mocker: MockerFixture) -> None:\n    mocked_constraining_strategy = mocker.MagicMock()\n    mocked_constraining_strategy.return_value = 1.33  # dummy value\n    curvature = Curvature(value=1.0, constraining_strategy=mocked_constraining_strategy)\n    assert curvature() == 1.33\n    mocked_constraining_strategy.assert_called_once_with(1.0)", ""]}
{"filename": "tests/manifolds/poincare_ball/test_poincare_ball.py", "chunked_list": ["import torch\n\nfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\nfrom hypll.tensors import ManifoldTensor\n\n\ndef test_flatten__man_dim_equals_start_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = PoincareBall(c=Curvature())\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=1,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n    assert flattened.shape == (2, 8)\n    assert flattened.man_dim == 1", "\n\ndef test_flatten__man_dim_equals_start_dim__set_end_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = PoincareBall(c=Curvature())\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=1,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=2)\n    assert flattened.shape == (2, 4, 2)\n    assert flattened.man_dim == 1", "\n\ndef test_flatten__man_dim_larger_start_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = PoincareBall(c=Curvature())\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=2,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n    assert flattened.shape == (2, 8)\n    assert flattened.man_dim == 1", "\n\ndef test_flatten__man_dim_larger_start_dim__set_end_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = PoincareBall(c=Curvature())\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=2,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=2)\n    assert flattened.shape == (2, 4, 2)\n    assert flattened.man_dim == 1", "\n\ndef test_flatten__man_dim_smaller_start_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = PoincareBall(c=Curvature())\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=0,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n    assert flattened.shape == (2, 8)\n    assert flattened.man_dim == 0", "\n\ndef test_flatten__man_dim_larger_end_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = PoincareBall(c=Curvature())\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=2,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=0, end_dim=1)\n    assert flattened.shape == (4, 2, 2)\n    assert flattened.man_dim == 1", ""]}
{"filename": "tests/manifolds/euclidean/test_euclidean.py", "chunked_list": ["import torch\n\nfrom hypll.manifolds.euclidean import Euclidean\nfrom hypll.tensors import ManifoldTensor\n\n\ndef test_flatten__man_dim_equals_start_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = Euclidean()\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=1,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n    assert flattened.shape == (2, 8)\n    assert flattened.man_dim == 1", "\n\ndef test_flatten__man_dim_equals_start_dim__set_end_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = Euclidean()\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=1,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=2)\n    assert flattened.shape == (2, 4, 2)\n    assert flattened.man_dim == 1", "\n\ndef test_flatten__man_dim_larger_start_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = Euclidean()\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=2,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n    assert flattened.shape == (2, 8)\n    assert flattened.man_dim == 1", "\n\ndef test_flatten__man_dim_larger_start_dim__set_end_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = Euclidean()\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=2,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=2)\n    assert flattened.shape == (2, 4, 2)\n    assert flattened.man_dim == 1", "\n\ndef test_flatten__man_dim_smaller_start_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = Euclidean()\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=0,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=1, end_dim=-1)\n    assert flattened.shape == (2, 8)\n    assert flattened.man_dim == 0", "\n\ndef test_flatten__man_dim_larger_end_dim() -> None:\n    tensor = torch.randn(2, 2, 2, 2)\n    manifold = Euclidean()\n    manifold_tensor = ManifoldTensor(\n        data=tensor,\n        manifold=manifold,\n        man_dim=2,\n        requires_grad=True,\n    )\n    flattened = manifold.flatten(manifold_tensor, start_dim=0, end_dim=1)\n    assert flattened.shape == (4, 2, 2)\n    assert flattened.man_dim == 1", ""]}
{"filename": "tests/nn/test_change_manifold.py", "chunked_list": ["import torch\n\nfrom hypll.manifolds.euclidean import Euclidean\nfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\nfrom hypll.nn import ChangeManifold\nfrom hypll.tensors import ManifoldTensor, TangentTensor\n\n\ndef test_change_manifold__euclidean_to_euclidean() -> None:\n    # Define inputs.\n    manifold = Euclidean()\n    inputs = ManifoldTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n    # Apply change manifold.\n    change_manifold = ChangeManifold(target_manifold=Euclidean())\n    outputs = change_manifold(inputs)\n    # Assert outputs are correct.\n    assert isinstance(outputs, ManifoldTensor)\n    assert outputs.shape == inputs.shape\n    assert change_manifold.target_manifold == outputs.manifold\n    assert outputs.man_dim == 1\n    assert torch.allclose(inputs.tensor, outputs.tensor)", "def test_change_manifold__euclidean_to_euclidean() -> None:\n    # Define inputs.\n    manifold = Euclidean()\n    inputs = ManifoldTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n    # Apply change manifold.\n    change_manifold = ChangeManifold(target_manifold=Euclidean())\n    outputs = change_manifold(inputs)\n    # Assert outputs are correct.\n    assert isinstance(outputs, ManifoldTensor)\n    assert outputs.shape == inputs.shape\n    assert change_manifold.target_manifold == outputs.manifold\n    assert outputs.man_dim == 1\n    assert torch.allclose(inputs.tensor, outputs.tensor)", "\n\ndef test_change_manifold__euclidean_to_poincare_ball() -> None:\n    # Define inputs.\n    manifold = Euclidean()\n    inputs = ManifoldTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n    # Apply change manifold.\n    change_manifold = ChangeManifold(\n        target_manifold=PoincareBall(c=Curvature()),\n    )\n    outputs = change_manifold(inputs)\n    # Assert outputs are correct.\n    assert isinstance(outputs, ManifoldTensor)\n    assert outputs.shape == inputs.shape\n    assert change_manifold.target_manifold == outputs.manifold\n    assert outputs.man_dim == 1", "\n\ndef test_change_manifold__poincare_ball_to_euclidean() -> None:\n    # Define inputs.\n    manifold = PoincareBall(c=Curvature(0.1))\n    tangents = TangentTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n    inputs = manifold.expmap(tangents)\n    # Apply change manifold.\n    change_manifold = ChangeManifold(target_manifold=Euclidean())\n    outputs = change_manifold(inputs)\n    # Assert outputs are correct.\n    assert isinstance(outputs, ManifoldTensor)\n    assert outputs.shape == inputs.shape\n    assert change_manifold.target_manifold == outputs.manifold\n    assert outputs.man_dim == 1", "\n\ndef test_change_manifold__poincare_ball_to_poincare_ball() -> None:\n    # Define inputs.\n    manifold = PoincareBall(c=Curvature(0.1))\n    tangents = TangentTensor(data=torch.randn(10, 2), manifold=manifold, man_dim=1)\n    inputs = manifold.expmap(tangents)\n    # Apply change manifold.\n    change_manifold = ChangeManifold(\n        target_manifold=PoincareBall(c=Curvature(1.0)),\n    )\n    outputs = change_manifold(inputs)\n    # Assert outputs are correct.\n    assert isinstance(outputs, ManifoldTensor)\n    assert outputs.shape == inputs.shape\n    assert change_manifold.target_manifold == outputs.manifold\n    assert outputs.man_dim == 1", ""]}
{"filename": "tests/nn/test_convolution.py", "chunked_list": ["import pytest\n\nfrom hypll.nn.modules import convolution\n\n\ndef test__output_side_length() -> None:\n    assert (\n        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=1, padding=0)\n        == 30\n    )\n    assert (\n        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=2, padding=0)\n        == 15\n    )", "\n\ndef test__output_side_length__padding() -> None:\n    assert (\n        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=1, padding=1)\n        == 32\n    )\n    assert (\n        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=2, padding=1)\n        == 16\n    )\n    # Reproduce https://github.com/maxvanspengler/hyperbolic_learning_library/issues/33\n    assert (\n        convolution._output_side_length(input_side_length=224, kernel_size=11, stride=4, padding=2)\n        == 55\n    )", "\n\ndef test__output_side_length__raises() -> None:\n    with pytest.raises(RuntimeError) as e:\n        convolution._output_side_length(input_side_length=32, kernel_size=33, stride=1, padding=0)\n    with pytest.raises(RuntimeError) as e:\n        convolution._output_side_length(input_side_length=32, kernel_size=3, stride=33, padding=0)\n"]}
{"filename": "tests/nn/test_flatten.py", "chunked_list": ["from pytest_mock import MockerFixture\n\nfrom hypll.nn import HFlatten\n\n\ndef test_hflatten(mocker: MockerFixture) -> None:\n    hflatten = HFlatten(start_dim=1, end_dim=-1)\n    mocked_manifold_tensor = mocker.MagicMock()\n    flattened = hflatten(mocked_manifold_tensor)\n    mocked_manifold_tensor.flatten.assert_called_once_with(start_dim=1, end_dim=-1)", ""]}
{"filename": "hypll/__init__.py", "chunked_list": [""]}
{"filename": "hypll/utils/layer_utils.py", "chunked_list": ["from typing import Callable\n\nfrom torch.nn import Module\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor\n\n\ndef check_if_manifolds_match(layer: Module, input: ManifoldTensor) -> None:\n    if layer.manifold != input.manifold:\n        raise ValueError(\n            f\"Manifold of {layer.__class__.__name__} layer is {layer.manifold} \"\n            f\"but input has manifold {input.manifold}\"\n        )", "def check_if_manifolds_match(layer: Module, input: ManifoldTensor) -> None:\n    if layer.manifold != input.manifold:\n        raise ValueError(\n            f\"Manifold of {layer.__class__.__name__} layer is {layer.manifold} \"\n            f\"but input has manifold {input.manifold}\"\n        )\n\n\ndef check_if_man_dims_match(layer: Module, man_dim: int, input: ManifoldTensor) -> None:\n    if man_dim < 0:\n        new_man_dim = input.dim() + man_dim\n    else:\n        new_man_dim = man_dim\n\n    if input.man_dim != new_man_dim:\n        raise ValueError(\n            f\"Layer of type {layer.__class__.__name__} expects the manifold dimension to be {man_dim}, \"\n            f\"but input has manifold dimension {input.man_dim}\"\n        )", "def check_if_man_dims_match(layer: Module, man_dim: int, input: ManifoldTensor) -> None:\n    if man_dim < 0:\n        new_man_dim = input.dim() + man_dim\n    else:\n        new_man_dim = man_dim\n\n    if input.man_dim != new_man_dim:\n        raise ValueError(\n            f\"Layer of type {layer.__class__.__name__} expects the manifold dimension to be {man_dim}, \"\n            f\"but input has manifold dimension {input.man_dim}\"\n        )", "\n\ndef op_in_tangent_space(op: Callable, manifold: Manifold, input: ManifoldTensor) -> ManifoldTensor:\n    input = manifold.logmap(x=None, y=input)\n    input.tensor = op(input.tensor)\n    return manifold.expmap(input)\n"]}
{"filename": "hypll/utils/tensor_utils.py", "chunked_list": ["from torch import broadcast_shapes, equal\n\nfrom hypll.tensors import ManifoldTensor, TangentTensor\n\n\ndef check_dims_with_broadcasting(*args: ManifoldTensor) -> int:\n    # Check if shapes can be broadcasted together\n    shapes = [a.size() for a in args]\n    try:\n        broadcasted_shape = broadcast_shapes(*shapes)\n    except RuntimeError:\n        raise ValueError(f\"Shapes of inputs were {shapes} and cannot be broadcasted together\")\n\n    # Find the manifold dimensions after broadcasting\n    max_dim = len(broadcasted_shape)\n    man_dims = []\n    for a in args:\n        if isinstance(a, ManifoldTensor):\n            man_dims.append(a.man_dim + (max_dim - a.dim()))\n        elif isinstance(a, TangentTensor):\n            man_dims.append(a.broadcasted_man_dim + (max_dim - a.dim()))\n\n    for md in man_dims[1:]:\n        if man_dims[0] != md:\n            raise ValueError(\"Manifold dimensions of inputs after broadcasting do not match.\")\n\n    return man_dims[0]", "\n\ndef check_tangent_tensor_positions(*args: TangentTensor) -> None:\n    manifold_points = [a.manifold_points for a in args]\n    if any([mp is None for mp in manifold_points]):\n        if not all(mp is None for mp in manifold_points):\n            raise ValueError(f\"Some but not all tangent tensors are located at the origin\")\n\n    else:\n        broadcasted_shape = broadcast_shapes(*[a.size() for a in args])\n        broadcasted_manifold_points = [\n            mp.tensor.broadcast_to(broadcasted_shape) for mp in manifold_points\n        ]\n        for bmp in broadcasted_manifold_points[1:]:\n            if not equal(broadcasted_manifold_points[0], bmp):\n                raise ValueError(\n                    f\"Tangent tensors are positioned at the different points on the manifold\"\n                )", ""]}
{"filename": "hypll/utils/__init__.py", "chunked_list": [""]}
{"filename": "hypll/utils/math.py", "chunked_list": ["from math import exp, lgamma\n\n\ndef beta_func(a: int, b: int) -> float:\n    log_beta = lgamma(a) + lgamma(b) - lgamma(a + b)\n    return exp(log_beta)\n"]}
{"filename": "hypll/optim/sgd.py", "chunked_list": ["from collections.abc import Iterable\nfrom typing import Union\n\nfrom torch import no_grad\nfrom torch.optim import Optimizer\n\nfrom hypll.manifolds import Manifold\nfrom hypll.manifolds.euclidean import Euclidean\nfrom hypll.tensors import ManifoldParameter, ManifoldTensor\n", "from hypll.tensors import ManifoldParameter, ManifoldTensor\n\n\nclass RiemannianSGD(Optimizer):\n    def __init__(\n        self,\n        params: Iterable[Union[ManifoldParameter, ManifoldTensor]],\n        lr: float,\n        momentum: float = 0,\n        dampening: float = 0,\n        weight_decay: float = 0,\n        nesterov: bool = False,\n    ) -> None:\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if momentum < 0.0:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n        )\n\n        super(RiemannianSGD, self).__init__(params=params, defaults=defaults)\n\n    def step(self) -> None:\n        with no_grad():\n            for group in self.param_groups:\n                lr = group[\"lr\"]\n                momentum = group[\"momentum\"]\n                dampening = group[\"dampening\"]\n                weight_decay = group[\"weight_decay\"]\n                nestorov = group[\"nesterov\"]\n\n                for param in group[\"params\"]:\n                    if isinstance(param, ManifoldParameter):\n                        grad = param.grad\n                        if grad is None:\n                            continue\n                        grad = ManifoldTensor(\n                            data=grad, manifold=Euclidean(), man_dim=param.man_dim\n                        )\n                        state = self.state[param]\n\n                        if len(state) == 0:\n                            if momentum > 0:\n                                state[\"momentum_buffer\"] = ManifoldTensor(\n                                    data=grad.tensor.clone(),\n                                    manifold=Euclidean(),\n                                    man_dim=param.man_dim,\n                                )\n\n                        manifold: Manifold = param.manifold\n\n                        grad.tensor.add_(\n                            param.tensor, alpha=weight_decay\n                        )  # TODO: check if this makes sense\n                        grad = manifold.euc_to_tangent(x=param, u=grad)\n                        if momentum > 0:\n                            momentum_buffer = manifold.euc_to_tangent(\n                                x=param, u=state[\"momentum_buffer\"]\n                            )\n                            momentum_buffer.tensor.mul_(momentum).add_(\n                                grad.tensor, alpha=1 - dampening\n                            )\n                            if nestorov:\n                                grad.tensor.add_(momentum_buffer.tensor, alpha=momentum)\n                            else:\n                                grad = momentum_buffer\n\n                            grad.tensor = -lr * grad.tensor\n\n                            new_param = manifold.expmap(grad)\n\n                            momentum_buffer = manifold.transp(v=momentum_buffer, y=new_param)\n\n                            # momentum_buffer.tensor.copy_(new_momentum_buffer.tensor)\n                            param.tensor.copy_(new_param.tensor)\n                        else:\n                            grad.tensor = -lr * grad.tensor\n                            new_param = manifold.expmap(v=grad)\n                            param.tensor.copy_(new_param.tensor)\n\n                    else:\n                        grad = param.grad\n                        if grad is None:\n                            continue\n                        state = self.state[param]\n\n                        if len(state) == 0:\n                            if momentum > 0:\n                                state[\"momentum_buffer\"] = grad.clone()\n\n                        grad.add_(param, alpha=weight_decay)\n                        if momentum > 0:\n                            momentum_buffer = state[\"momentum_buffer\"]\n                            momentum_buffer.mul_(momentum).add_(grad, alpha=1 - dampening)\n                            if nestorov:\n                                grad = grad.add_(momentum_buffer, alpha=momentum)\n                            else:\n                                grad = momentum_buffer\n\n                            new_param = param - lr * grad\n                            new_momentum_buffer = momentum_buffer\n\n                            momentum_buffer.copy_(new_momentum_buffer)\n                            param.copy_(new_param)\n                        else:\n                            new_param = param - lr * grad\n                            param.copy_(new_param)", ""]}
{"filename": "hypll/optim/__init__.py", "chunked_list": ["from .adam import RiemannianAdam\nfrom .sgd import RiemannianSGD\n"]}
{"filename": "hypll/optim/adam.py", "chunked_list": ["from collections.abc import Iterable\nfrom typing import Union\n\nfrom torch import max, no_grad, zeros_like\nfrom torch.optim import Optimizer\n\nfrom hypll.manifolds import Manifold\nfrom hypll.manifolds.euclidean import Euclidean\nfrom hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n", "from hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n\n\nclass RiemannianAdam(Optimizer):\n    def __init__(\n        self,\n        params: Iterable[Union[ManifoldParameter, ManifoldTensor]],\n        lr: float,\n        betas: tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n        amsgrad: bool = False,\n    ) -> None:\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if eps < 0.0:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            amsgrad=amsgrad,\n        )\n        super(RiemannianAdam, self).__init__(params=params, defaults=defaults)\n\n    def step(self) -> None:\n        # TODO: refactor this and add some comments, because it's currently unreadable\n        with no_grad():\n            for group in self.param_groups:\n                betas = group[\"betas\"]\n                weight_decay = group[\"weight_decay\"]\n                eps = group[\"eps\"]\n                lr = group[\"lr\"]\n                amsgrad = group[\"amsgrad\"]\n                for param in group[\"params\"]:\n                    if isinstance(param, ManifoldParameter):\n                        manifold: Manifold = param.manifold\n                        grad = param.grad\n                        if grad is None:\n                            continue\n                        grad = ManifoldTensor(\n                            data=grad, manifold=Euclidean(), man_dim=param.man_dim\n                        )\n                        state = self.state[param]\n\n                        if len(state) == 0:\n                            state[\"step\"] = 0\n                            state[\"exp_avg\"] = zeros_like(param.tensor)\n                            state[\"exp_avg_sq\"] = zeros_like(param.tensor)\n                            if amsgrad:\n                                state[\"max_exp_avg_sq\"] = zeros_like(param.tensor)\n\n                        state[\"step\"] += 1\n                        exp_avg = state[\"exp_avg\"]\n                        exp_avg_sq = state[\"exp_avg_sq\"]\n\n                        # TODO: check if this next line makes sense, because I don't think so\n                        grad.tensor.add_(param.tensor, alpha=weight_decay)\n                        grad = manifold.euc_to_tangent(x=param, u=grad)\n                        exp_avg.mul_(betas[0]).add_(grad.tensor, alpha=1 - betas[0])\n                        exp_avg_sq.mul_(betas[1]).add_(\n                            manifold.inner(u=grad, v=grad, keepdim=True, safe_mode=False),\n                            alpha=1 - betas[1],\n                        )\n                        bias_correction1 = 1 - betas[0] ** state[\"step\"]\n                        bias_correction2 = 1 - betas[1] ** state[\"step\"]\n\n                        if amsgrad:\n                            max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n                            max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                            denom = max_exp_avg_sq.div(bias_correction2).sqrt_()\n                        else:\n                            denom = exp_avg_sq.div(bias_correction2).sqrt_()\n\n                        direction = -lr * exp_avg.div(bias_correction1) / denom.add_(eps)\n                        direction = TangentTensor(\n                            data=direction,\n                            manifold_points=param,\n                            manifold=manifold,\n                            man_dim=param.man_dim,\n                        )\n                        exp_avg_man = TangentTensor(\n                            data=exp_avg,\n                            manifold_points=param,\n                            manifold=manifold,\n                            man_dim=param.man_dim,\n                        )\n\n                        new_param = manifold.expmap(direction)\n                        exp_avg_new = manifold.transp(v=exp_avg_man, y=new_param)\n\n                        param.tensor.copy_(new_param.tensor)\n                        exp_avg.copy_(exp_avg_new.tensor)\n\n                    else:\n                        grad = param.grad\n                        if grad is None:\n                            continue\n\n                        state = self.state[param]\n\n                        if len(state) == 0:\n                            state[\"step\"] = 0\n                            state[\"exp_avg\"] = zeros_like(param)\n                            state[\"exp_avg_sq\"] = zeros_like(param)\n                            if amsgrad:\n                                state[\"max_exp_avg_sq\"] = zeros_like(param)\n                        state[\"step\"] += 1\n                        exp_avg = state[\"exp_avg\"]\n                        exp_avg_sq = state[\"exp_avg_sq\"]\n                        grad.add_(param, alpha=weight_decay)\n                        exp_avg.mul_(betas[0]).add_(grad, alpha=1 - betas[0])\n                        exp_avg_sq.mul_(betas[1]).add_(\n                            grad.square().sum(dim=-1, keepdim=True), alpha=1 - betas[1]\n                        )\n                        bias_correction1 = 1 - betas[0] ** state[\"step\"]\n                        bias_correction2 = 1 - betas[1] ** state[\"step\"]\n                        if amsgrad:\n                            max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n                            max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                            denom = max_exp_avg_sq.div(bias_correction2).sqrt_()\n                        else:\n                            denom = exp_avg_sq.div(bias_correction2).sqrt_()\n\n                        direction = exp_avg.div(bias_correction1) / denom.add_(eps)\n\n                        new_param = param - lr * direction\n                        exp_avg_new = exp_avg\n\n                        param.copy_(new_param)\n                        exp_avg.copy_(exp_avg_new)", ""]}
{"filename": "hypll/tensors/tangent_tensor.py", "chunked_list": ["from typing import Any, Optional\n\nfrom torch import Tensor, broadcast_shapes, tensor\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors.manifold_tensor import ManifoldTensor\n\n\nclass TangentTensor:\n    def __init__(\n        self,\n        data,\n        manifold_points: Optional[ManifoldTensor] = None,\n        manifold: Optional[Manifold] = None,\n        man_dim: int = -1,\n        requires_grad: bool = False,\n    ) -> None:\n        # Create tangent vector tensor with correct device and dtype\n        if isinstance(data, Tensor):\n            self.tensor = data\n        else:\n            self.tensor = tensor(data, requires_grad=requires_grad)\n\n        # Store manifold dimension as a nonnegative integer\n        if man_dim >= 0:\n            self.man_dim = man_dim\n        else:\n            self.man_dim = self.tensor.dim() + man_dim\n            if self.man_dim < 0:\n                raise ValueError(\n                    f\"Dimension out of range (expected to be in range of \"\n                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n                )\n\n        if manifold_points is not None:\n            # Check if the manifold points and tangent vectors are broadcastable together\n            try:\n                broadcasted_size = broadcast_shapes(self.tensor.size(), manifold_points.size())\n            except RuntimeError:\n                raise ValueError(\n                    f\"The shapes of the manifold points tensor {manifold_points.size()} and \"\n                    f\"the tangent vector tensor {self.tensor.size()} are not broadcastable \"\n                    f\"togther.\"\n                )\n\n            # Check if the manifold dimensions match after broadcasting\n            dim = len(broadcasted_size)\n            broadcast_man_dims = [\n                manifold_points.man_dim + dim - manifold_points.tensor.dim(),\n                self.man_dim + dim - self.tensor.dim(),\n            ]\n            if broadcast_man_dims[0] != broadcast_man_dims[1]:\n                raise ValueError(\n                    f\"After broadcasting the manifold points with the tangent vectors, the \"\n                    f\"manifold dimension computed from the manifold points should match the \"\n                    f\"manifold dimension computed from the supplied man_dim, but these are\"\n                    f\"{broadcast_man_dims}, respectively.\"\n                )\n\n        # Check if the supplied manifolds match\n        if manifold_points is not None and manifold is not None:\n            if manifold_points.manifold != manifold:\n                raise ValueError(\n                    f\"The manifold of the manifold_points and the provided manifold should match, \"\n                    f\"but are {manifold_points.manifold} and {manifold}, respectively.\"\n                )\n\n        self.manifold_points = manifold_points\n        self.manifold = manifold or manifold_points.manifold\n        self.man_dim = man_dim\n\n    def __getattr__(self, name: str) -> Any:\n        # TODO: go through https://pytorch.org/docs/stable/tensors.html and check which methods\n        # are relevant.\n        if hasattr(self.tensor, name):\n            torch_attribute = getattr(self.tensor, name)\n\n            if callable(torch_attribute):\n                raise AttributeError(\n                    f\"Attempting to apply the torch.Tensor method {name} on a TangentTensor.\"\n                    f\"Use TangentTensor.tensor.{name} or TangentTensor.manifold_points.tensor \"\n                    f\"instead.\"\n                )\n            else:\n                return torch_attribute\n\n        else:\n            raise AttributeError(\n                f\"Neither {self.__class__.__name__}, nor torch.Tensor has attribute {name}\"\n            )\n\n    def cuda(self, device=None):\n        new_tensor = self.tensor.cuda(device)\n        new_manifold_points = self.manifold_points.cuda(device)\n        return TangentTensor(\n            data=new_tensor,\n            manifold_points=new_manifold_points,\n            manifold=self.manifold,\n            man_dim=self.man_dim,\n        )\n\n    def cpu(self):\n        new_tensor = self.tensor.cpu()\n        new_manifold_points = self.manifold_points.cpu()\n        return TangentTensor(\n            data=new_tensor,\n            manifold_points=new_manifold_points,\n            manifold=self.manifold,\n            man_dim=self.man_dim,\n        )\n\n    def to(self, *args, **kwargs):\n        new_tensor = self.tensor.to(*args, **kwargs)\n        new_manifold_points = self.manifold_points(*args, **kwargs)\n        return TangentTensor(\n            data=new_tensor,\n            manifold_points=new_manifold_points,\n            manifold=self.manifold,\n            man_dim=self.man_dim,\n        )\n\n    def size(self, dim: Optional[int] = None):\n        if self.manifold_points is None:\n            manifold_points_size = None\n        manifold_points_size = (\n            self.manifold_points.size() if self.manifold_points is not None else ()\n        )\n        broadcasted_size = broadcast_shapes(self.tensor.size(), manifold_points_size)\n        if dim is None:\n            return broadcasted_size\n        else:\n            return broadcasted_size[dim]\n\n    @property\n    def broadcasted_man_dim(self):\n        return self.man_dim + self.dim() - self.tensor.dim()\n\n    def dim(self):\n        return len(self.size())\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # TODO: check if there are torch functions that should be allowed\n        raise TypeError(\n            f\"Attempting to apply the torch function {func} on a TangentTensor.\"\n            f\"Use TangentTensor.tensor as argument to {func} instead.\"\n        )", "class TangentTensor:\n    def __init__(\n        self,\n        data,\n        manifold_points: Optional[ManifoldTensor] = None,\n        manifold: Optional[Manifold] = None,\n        man_dim: int = -1,\n        requires_grad: bool = False,\n    ) -> None:\n        # Create tangent vector tensor with correct device and dtype\n        if isinstance(data, Tensor):\n            self.tensor = data\n        else:\n            self.tensor = tensor(data, requires_grad=requires_grad)\n\n        # Store manifold dimension as a nonnegative integer\n        if man_dim >= 0:\n            self.man_dim = man_dim\n        else:\n            self.man_dim = self.tensor.dim() + man_dim\n            if self.man_dim < 0:\n                raise ValueError(\n                    f\"Dimension out of range (expected to be in range of \"\n                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n                )\n\n        if manifold_points is not None:\n            # Check if the manifold points and tangent vectors are broadcastable together\n            try:\n                broadcasted_size = broadcast_shapes(self.tensor.size(), manifold_points.size())\n            except RuntimeError:\n                raise ValueError(\n                    f\"The shapes of the manifold points tensor {manifold_points.size()} and \"\n                    f\"the tangent vector tensor {self.tensor.size()} are not broadcastable \"\n                    f\"togther.\"\n                )\n\n            # Check if the manifold dimensions match after broadcasting\n            dim = len(broadcasted_size)\n            broadcast_man_dims = [\n                manifold_points.man_dim + dim - manifold_points.tensor.dim(),\n                self.man_dim + dim - self.tensor.dim(),\n            ]\n            if broadcast_man_dims[0] != broadcast_man_dims[1]:\n                raise ValueError(\n                    f\"After broadcasting the manifold points with the tangent vectors, the \"\n                    f\"manifold dimension computed from the manifold points should match the \"\n                    f\"manifold dimension computed from the supplied man_dim, but these are\"\n                    f\"{broadcast_man_dims}, respectively.\"\n                )\n\n        # Check if the supplied manifolds match\n        if manifold_points is not None and manifold is not None:\n            if manifold_points.manifold != manifold:\n                raise ValueError(\n                    f\"The manifold of the manifold_points and the provided manifold should match, \"\n                    f\"but are {manifold_points.manifold} and {manifold}, respectively.\"\n                )\n\n        self.manifold_points = manifold_points\n        self.manifold = manifold or manifold_points.manifold\n        self.man_dim = man_dim\n\n    def __getattr__(self, name: str) -> Any:\n        # TODO: go through https://pytorch.org/docs/stable/tensors.html and check which methods\n        # are relevant.\n        if hasattr(self.tensor, name):\n            torch_attribute = getattr(self.tensor, name)\n\n            if callable(torch_attribute):\n                raise AttributeError(\n                    f\"Attempting to apply the torch.Tensor method {name} on a TangentTensor.\"\n                    f\"Use TangentTensor.tensor.{name} or TangentTensor.manifold_points.tensor \"\n                    f\"instead.\"\n                )\n            else:\n                return torch_attribute\n\n        else:\n            raise AttributeError(\n                f\"Neither {self.__class__.__name__}, nor torch.Tensor has attribute {name}\"\n            )\n\n    def cuda(self, device=None):\n        new_tensor = self.tensor.cuda(device)\n        new_manifold_points = self.manifold_points.cuda(device)\n        return TangentTensor(\n            data=new_tensor,\n            manifold_points=new_manifold_points,\n            manifold=self.manifold,\n            man_dim=self.man_dim,\n        )\n\n    def cpu(self):\n        new_tensor = self.tensor.cpu()\n        new_manifold_points = self.manifold_points.cpu()\n        return TangentTensor(\n            data=new_tensor,\n            manifold_points=new_manifold_points,\n            manifold=self.manifold,\n            man_dim=self.man_dim,\n        )\n\n    def to(self, *args, **kwargs):\n        new_tensor = self.tensor.to(*args, **kwargs)\n        new_manifold_points = self.manifold_points(*args, **kwargs)\n        return TangentTensor(\n            data=new_tensor,\n            manifold_points=new_manifold_points,\n            manifold=self.manifold,\n            man_dim=self.man_dim,\n        )\n\n    def size(self, dim: Optional[int] = None):\n        if self.manifold_points is None:\n            manifold_points_size = None\n        manifold_points_size = (\n            self.manifold_points.size() if self.manifold_points is not None else ()\n        )\n        broadcasted_size = broadcast_shapes(self.tensor.size(), manifold_points_size)\n        if dim is None:\n            return broadcasted_size\n        else:\n            return broadcasted_size[dim]\n\n    @property\n    def broadcasted_man_dim(self):\n        return self.man_dim + self.dim() - self.tensor.dim()\n\n    def dim(self):\n        return len(self.size())\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # TODO: check if there are torch functions that should be allowed\n        raise TypeError(\n            f\"Attempting to apply the torch function {func} on a TangentTensor.\"\n            f\"Use TangentTensor.tensor as argument to {func} instead.\"\n        )", ""]}
{"filename": "hypll/tensors/__init__.py", "chunked_list": ["from .manifold_parameter import ManifoldParameter\nfrom .manifold_tensor import ManifoldTensor\nfrom .tangent_tensor import TangentTensor\n"]}
{"filename": "hypll/tensors/manifold_parameter.py", "chunked_list": ["from typing import Any\n\nimport torch\nfrom torch import Tensor, tensor\nfrom torch.nn import Parameter\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors.manifold_tensor import ManifoldTensor\n\n\nclass ManifoldParameter(ManifoldTensor, Parameter):\n    _allowed_methods = [\n        torch._has_compatible_shallow_copy_type,  # Required for torch.nn.Parameter\n        torch.Tensor.copy_,  # Required to load ManifoldParameters state dicts\n    ]\n\n    def __new__(cls, data, manifold, man_dim, requires_grad=True):\n        return super(ManifoldTensor, cls).__new__(cls)\n\n    # TODO: Create a mixin class containing the methods for this class and for ManifoldTensor\n    # to avoid all the boilerplate stuff.\n    def __init__(\n        self, data, manifold: Manifold, man_dim: int = -1, requires_grad: bool = True\n    ) -> None:\n        super(ManifoldParameter, self).__init__(data=data, manifold=manifold)\n        if isinstance(data, Parameter):\n            self.tensor = data\n        elif isinstance(data, Tensor):\n            self.tensor = Parameter(data=data, requires_grad=requires_grad)\n        else:\n            self.tensor = Parameter(data=tensor(data), requires_grad=requires_grad)\n\n        self.manifold = manifold\n\n        if man_dim >= 0:\n            self.man_dim = man_dim\n        else:\n            self.man_dim = self.tensor.dim() + man_dim\n            if self.man_dim < 0:\n                raise ValueError(\n                    f\"Dimension out of range (expected to be in range of \"\n                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n                )\n\n    def __getattr__(self, name: str) -> Any:\n        # TODO: go through https://pytorch.org/docs/stable/tensors.html and check which methods\n        # are relevant.\n        if hasattr(self.tensor, name):\n            torch_attribute = getattr(self.tensor, name)\n\n            if callable(torch_attribute):\n                raise AttributeError(\n                    f\"Attempting to apply the torch.nn.Parameter method {name} on a ManifoldParameter.\"\n                    f\"Use ManifoldTensor.tensor.{name} instead.\"\n                )\n            else:\n                return torch_attribute\n\n        else:\n            raise AttributeError(\n                f\"Neither {self.__class__.__name__}, nor torch.Tensor has attribute {name}\"\n            )\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if func.__class__.__name__ == \"method-wrapper\" or func in cls._allowed_methods:\n            args = [a.tensor if isinstance(a, ManifoldTensor) else a for a in args]\n            if kwargs is None:\n                kwargs = {}\n            kwargs = {k: (v.tensor if isinstance(v, ManifoldTensor) else v) for k, v in kwargs}\n            return func(*args, **kwargs)\n        # if func.__name__ == \"__get__\":\n        #     return func(args[0].tensor)\n        # TODO: check if there are torch functions that should be allowed\n        raise TypeError(\n            f\"Attempting to apply the torch function {func} on a ManifoldParameter. \"\n            f\"Use ManifoldParameter.tensor as argument to {func} instead.\"\n        )", "\n\nclass ManifoldParameter(ManifoldTensor, Parameter):\n    _allowed_methods = [\n        torch._has_compatible_shallow_copy_type,  # Required for torch.nn.Parameter\n        torch.Tensor.copy_,  # Required to load ManifoldParameters state dicts\n    ]\n\n    def __new__(cls, data, manifold, man_dim, requires_grad=True):\n        return super(ManifoldTensor, cls).__new__(cls)\n\n    # TODO: Create a mixin class containing the methods for this class and for ManifoldTensor\n    # to avoid all the boilerplate stuff.\n    def __init__(\n        self, data, manifold: Manifold, man_dim: int = -1, requires_grad: bool = True\n    ) -> None:\n        super(ManifoldParameter, self).__init__(data=data, manifold=manifold)\n        if isinstance(data, Parameter):\n            self.tensor = data\n        elif isinstance(data, Tensor):\n            self.tensor = Parameter(data=data, requires_grad=requires_grad)\n        else:\n            self.tensor = Parameter(data=tensor(data), requires_grad=requires_grad)\n\n        self.manifold = manifold\n\n        if man_dim >= 0:\n            self.man_dim = man_dim\n        else:\n            self.man_dim = self.tensor.dim() + man_dim\n            if self.man_dim < 0:\n                raise ValueError(\n                    f\"Dimension out of range (expected to be in range of \"\n                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n                )\n\n    def __getattr__(self, name: str) -> Any:\n        # TODO: go through https://pytorch.org/docs/stable/tensors.html and check which methods\n        # are relevant.\n        if hasattr(self.tensor, name):\n            torch_attribute = getattr(self.tensor, name)\n\n            if callable(torch_attribute):\n                raise AttributeError(\n                    f\"Attempting to apply the torch.nn.Parameter method {name} on a ManifoldParameter.\"\n                    f\"Use ManifoldTensor.tensor.{name} instead.\"\n                )\n            else:\n                return torch_attribute\n\n        else:\n            raise AttributeError(\n                f\"Neither {self.__class__.__name__}, nor torch.Tensor has attribute {name}\"\n            )\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if func.__class__.__name__ == \"method-wrapper\" or func in cls._allowed_methods:\n            args = [a.tensor if isinstance(a, ManifoldTensor) else a for a in args]\n            if kwargs is None:\n                kwargs = {}\n            kwargs = {k: (v.tensor if isinstance(v, ManifoldTensor) else v) for k, v in kwargs}\n            return func(*args, **kwargs)\n        # if func.__name__ == \"__get__\":\n        #     return func(args[0].tensor)\n        # TODO: check if there are torch functions that should be allowed\n        raise TypeError(\n            f\"Attempting to apply the torch function {func} on a ManifoldParameter. \"\n            f\"Use ManifoldParameter.tensor as argument to {func} instead.\"\n        )", ""]}
{"filename": "hypll/tensors/manifold_tensor.py", "chunked_list": ["from __future__ import annotations\n\nfrom typing import Optional\n\nfrom torch import Tensor, long, tensor\n\nfrom hypll.manifolds import Manifold\n\n\nclass ManifoldTensor:\n    \"\"\"Represents a tensor on a manifold.\n\n    Attributes:\n        tensor:\n            Torch tensor of points on the manifold.\n        manifold:\n            Manifold instance.\n        man_dim:\n            Dimension along which points are on the manifold.\n\n    \"\"\"\n\n    def __init__(\n        self, data: Tensor, manifold: Manifold, man_dim: int = -1, requires_grad: bool = False\n    ) -> None:\n        \"\"\"Creates an instance of ManifoldTensor.\n\n        Args:\n            data:\n                Torch tensor of points on the manifold.\n            manifold:\n                Manifold instance.\n            man_dim:\n                Dimension along which points are on the manifold. -1 by default.\n\n        TODO(Philipp, 05/23): Let's get rid of requires_grad if possible.\n\n        \"\"\"\n        self.tensor = data if isinstance(data, Tensor) else tensor(data, requires_grad=True)\n        self.manifold = manifold\n\n        if man_dim >= 0:\n            self.man_dim = man_dim\n        else:\n            self.man_dim = self.tensor.dim() + man_dim\n            if self.man_dim < 0:\n                raise ValueError(\n                    f\"Dimension out of range (expected to be in range of \"\n                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n                )\n\n    def __getitem__(self, *args):\n        # Catch some undefined behaviour by checking if args is a single element\n        if len(args) != 1:\n            raise ValueError(\n                f\"No support for slicing with these arguments. If you think there should be \"\n                f\"support, please consider opening a issue on GitHub describing your case.\"\n            )\n\n        # Deal with the case where the argument is a long tensor\n        if isinstance(args[0], Tensor) and args[0].dtype == long:\n            if self.man_dim == 0:\n                raise ValueError(\n                    f\"Long tensor indexing is only possible when the manifold dimension \"\n                    f\"is not 0, but the manifold dimension is {self.man_dim}\"\n                )\n            new_tensor = self.tensor.__getitem__(*args)\n            new_man_dim = self.man_dim + args[0].dim() - 1\n            return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n\n        # Convert the args to a list and replace Ellipsis by the correct number of full slices\n        arg_list = list(args[0])\n        if Ellipsis in arg_list:\n            ell_id = arg_list.index(Ellipsis)\n            colon_repeats = self.dim() - sum(1 for a in arg_list if a is not None) + 1\n            arg_list[ell_id : ell_id + 1] = colon_repeats * [slice(None, None, None)]\n\n        new_tensor = self.tensor.__getitem__(*args)\n        output_man_dim = self.man_dim\n        counter = self.man_dim + 1\n\n        # Compute output manifold dimension\n        for arg in arg_list:\n            # None values add a dimension\n            if arg is None:\n                output_man_dim += 1\n                continue\n            # Integers remove a dimension\n            elif isinstance(arg, int):\n                output_man_dim -= 1\n                counter -= 1\n            # Other values leave the dimension intact\n            else:\n                counter -= 1\n\n            # When the counter hits 0 and the next term isn't None, we hit the man_dim term\n            if counter == 0:\n                if isinstance(arg, int) or isinstance(arg, list):\n                    raise ValueError(\n                        f\"Attempting to slice into the manifold dimension, but this is not a \"\n                        \"valid operation\"\n                    )\n                # If we get past the man_dim term, the output man_dim doesn't change anymore\n                break\n\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=output_man_dim)\n\n    def cpu(self) -> ManifoldTensor:\n        \"\"\"Returns a copy of this object with self.tensor in CPU memory.\"\"\"\n        new_tensor = self.tensor.cpu()\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\n    def cuda(self, device=None) -> ManifoldTensor:\n        \"\"\"Returns a copy of this object with self.tensor in CUDA memory.\"\"\"\n        new_tensor = self.tensor.cuda(device)\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\n    def dim(self) -> int:\n        \"\"\"Returns the number of dimensions of self.tensor.\"\"\"\n        return self.tensor.dim()\n\n    def detach(self) -> ManifoldTensor:\n        \"\"\"Returns a new Tensor, detached from the current graph.\"\"\"\n        detached = self.tensor.detach()\n        return ManifoldTensor(data=detached, manifold=self.manifold, man_dim=self.man_dim)\n\n    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> ManifoldTensor:\n        \"\"\"Flattens tensor by reshaping it. If start_dim or end_dim are passed,\n        only dimensions starting with start_dim and ending with end_dim are flattend.\n\n        \"\"\"\n        return self.manifold.flatten(self, start_dim=start_dim, end_dim=end_dim)\n\n    @property\n    def is_cpu(self):\n        return self.tensor.is_cpu\n\n    @property\n    def is_cuda(self):\n        return self.tensor.is_cuda\n\n    def is_floating_point(self) -> bool:\n        \"\"\"Returns true if the tensor is of dtype float.\"\"\"\n        return self.tensor.is_floating_point()\n\n    def project(self) -> ManifoldTensor:\n        \"\"\"Projects the tensor to the manifold.\"\"\"\n        return self.manifold.project(x=self)\n\n    @property\n    def shape(self):\n        \"\"\"Alias for size().\"\"\"\n        return self.size()\n\n    def size(self, dim: Optional[int] = None):\n        \"\"\"Returns the size of self.tensor.\"\"\"\n        if dim is None:\n            return self.tensor.size()\n        else:\n            return self.tensor.size(dim)\n\n    def squeeze(self, dim=None):\n        \"\"\"Returns a squeezed version of the manifold tensor.\"\"\"\n        if dim == self.man_dim or (dim is None and self.size(self.man_dim) == 1):\n            raise ValueError(\"Attempting to squeeze the manifold dimension\")\n\n        if dim is None:\n            new_tensor = self.tensor.squeeze()\n            new_man_dim = self.man_dim - sum(self.size(d) == 1 for d in range(self.man_dim))\n        else:\n            new_tensor = self.tensor.squeeze(dim=dim)\n            new_man_dim = self.man_dim - (1 if dim < self.man_dim else 0)\n\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n\n    def to(self, *args, **kwargs) -> ManifoldTensor:\n        \"\"\"Returns a new tensor with the specified device and (optional) dtype.\"\"\"\n        new_tensor = self.tensor.to(*args, **kwargs)\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\n    def transpose(self, dim0: int, dim1: int) -> ManifoldTensor:\n        \"\"\"Returns a transposed version of the manifold tensor. The given dimensions\n        dim0 and dim1 are swapped.\n        \"\"\"\n        if self.man_dim == dim0:\n            new_man_dim = dim1\n        elif self.man_dim == dim1:\n            new_man_dim = dim0\n        new_tensor = self.tensor.transpose(dim0, dim1)\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # TODO: check if there are torch functions that should be allowed\n        raise TypeError(\n            f\"Attempting to apply the torch function {func} on a ManifoldTensor. \"\n            f\"Use ManifoldTensor.tensor as argument to {func} instead.\"\n        )", "\nclass ManifoldTensor:\n    \"\"\"Represents a tensor on a manifold.\n\n    Attributes:\n        tensor:\n            Torch tensor of points on the manifold.\n        manifold:\n            Manifold instance.\n        man_dim:\n            Dimension along which points are on the manifold.\n\n    \"\"\"\n\n    def __init__(\n        self, data: Tensor, manifold: Manifold, man_dim: int = -1, requires_grad: bool = False\n    ) -> None:\n        \"\"\"Creates an instance of ManifoldTensor.\n\n        Args:\n            data:\n                Torch tensor of points on the manifold.\n            manifold:\n                Manifold instance.\n            man_dim:\n                Dimension along which points are on the manifold. -1 by default.\n\n        TODO(Philipp, 05/23): Let's get rid of requires_grad if possible.\n\n        \"\"\"\n        self.tensor = data if isinstance(data, Tensor) else tensor(data, requires_grad=True)\n        self.manifold = manifold\n\n        if man_dim >= 0:\n            self.man_dim = man_dim\n        else:\n            self.man_dim = self.tensor.dim() + man_dim\n            if self.man_dim < 0:\n                raise ValueError(\n                    f\"Dimension out of range (expected to be in range of \"\n                    f\"{[-self.tensor.dim() - 1, self.tensor.dim()]}, but got {man_dim})\"\n                )\n\n    def __getitem__(self, *args):\n        # Catch some undefined behaviour by checking if args is a single element\n        if len(args) != 1:\n            raise ValueError(\n                f\"No support for slicing with these arguments. If you think there should be \"\n                f\"support, please consider opening a issue on GitHub describing your case.\"\n            )\n\n        # Deal with the case where the argument is a long tensor\n        if isinstance(args[0], Tensor) and args[0].dtype == long:\n            if self.man_dim == 0:\n                raise ValueError(\n                    f\"Long tensor indexing is only possible when the manifold dimension \"\n                    f\"is not 0, but the manifold dimension is {self.man_dim}\"\n                )\n            new_tensor = self.tensor.__getitem__(*args)\n            new_man_dim = self.man_dim + args[0].dim() - 1\n            return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n\n        # Convert the args to a list and replace Ellipsis by the correct number of full slices\n        arg_list = list(args[0])\n        if Ellipsis in arg_list:\n            ell_id = arg_list.index(Ellipsis)\n            colon_repeats = self.dim() - sum(1 for a in arg_list if a is not None) + 1\n            arg_list[ell_id : ell_id + 1] = colon_repeats * [slice(None, None, None)]\n\n        new_tensor = self.tensor.__getitem__(*args)\n        output_man_dim = self.man_dim\n        counter = self.man_dim + 1\n\n        # Compute output manifold dimension\n        for arg in arg_list:\n            # None values add a dimension\n            if arg is None:\n                output_man_dim += 1\n                continue\n            # Integers remove a dimension\n            elif isinstance(arg, int):\n                output_man_dim -= 1\n                counter -= 1\n            # Other values leave the dimension intact\n            else:\n                counter -= 1\n\n            # When the counter hits 0 and the next term isn't None, we hit the man_dim term\n            if counter == 0:\n                if isinstance(arg, int) or isinstance(arg, list):\n                    raise ValueError(\n                        f\"Attempting to slice into the manifold dimension, but this is not a \"\n                        \"valid operation\"\n                    )\n                # If we get past the man_dim term, the output man_dim doesn't change anymore\n                break\n\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=output_man_dim)\n\n    def cpu(self) -> ManifoldTensor:\n        \"\"\"Returns a copy of this object with self.tensor in CPU memory.\"\"\"\n        new_tensor = self.tensor.cpu()\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\n    def cuda(self, device=None) -> ManifoldTensor:\n        \"\"\"Returns a copy of this object with self.tensor in CUDA memory.\"\"\"\n        new_tensor = self.tensor.cuda(device)\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\n    def dim(self) -> int:\n        \"\"\"Returns the number of dimensions of self.tensor.\"\"\"\n        return self.tensor.dim()\n\n    def detach(self) -> ManifoldTensor:\n        \"\"\"Returns a new Tensor, detached from the current graph.\"\"\"\n        detached = self.tensor.detach()\n        return ManifoldTensor(data=detached, manifold=self.manifold, man_dim=self.man_dim)\n\n    def flatten(self, start_dim: int = 0, end_dim: int = -1) -> ManifoldTensor:\n        \"\"\"Flattens tensor by reshaping it. If start_dim or end_dim are passed,\n        only dimensions starting with start_dim and ending with end_dim are flattend.\n\n        \"\"\"\n        return self.manifold.flatten(self, start_dim=start_dim, end_dim=end_dim)\n\n    @property\n    def is_cpu(self):\n        return self.tensor.is_cpu\n\n    @property\n    def is_cuda(self):\n        return self.tensor.is_cuda\n\n    def is_floating_point(self) -> bool:\n        \"\"\"Returns true if the tensor is of dtype float.\"\"\"\n        return self.tensor.is_floating_point()\n\n    def project(self) -> ManifoldTensor:\n        \"\"\"Projects the tensor to the manifold.\"\"\"\n        return self.manifold.project(x=self)\n\n    @property\n    def shape(self):\n        \"\"\"Alias for size().\"\"\"\n        return self.size()\n\n    def size(self, dim: Optional[int] = None):\n        \"\"\"Returns the size of self.tensor.\"\"\"\n        if dim is None:\n            return self.tensor.size()\n        else:\n            return self.tensor.size(dim)\n\n    def squeeze(self, dim=None):\n        \"\"\"Returns a squeezed version of the manifold tensor.\"\"\"\n        if dim == self.man_dim or (dim is None and self.size(self.man_dim) == 1):\n            raise ValueError(\"Attempting to squeeze the manifold dimension\")\n\n        if dim is None:\n            new_tensor = self.tensor.squeeze()\n            new_man_dim = self.man_dim - sum(self.size(d) == 1 for d in range(self.man_dim))\n        else:\n            new_tensor = self.tensor.squeeze(dim=dim)\n            new_man_dim = self.man_dim - (1 if dim < self.man_dim else 0)\n\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n\n    def to(self, *args, **kwargs) -> ManifoldTensor:\n        \"\"\"Returns a new tensor with the specified device and (optional) dtype.\"\"\"\n        new_tensor = self.tensor.to(*args, **kwargs)\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=self.man_dim)\n\n    def transpose(self, dim0: int, dim1: int) -> ManifoldTensor:\n        \"\"\"Returns a transposed version of the manifold tensor. The given dimensions\n        dim0 and dim1 are swapped.\n        \"\"\"\n        if self.man_dim == dim0:\n            new_man_dim = dim1\n        elif self.man_dim == dim1:\n            new_man_dim = dim0\n        new_tensor = self.tensor.transpose(dim0, dim1)\n        return ManifoldTensor(data=new_tensor, manifold=self.manifold, man_dim=new_man_dim)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # TODO: check if there are torch functions that should be allowed\n        raise TypeError(\n            f\"Attempting to apply the torch function {func} on a ManifoldTensor. \"\n            f\"Use ManifoldTensor.tensor as argument to {func} instead.\"\n        )", ""]}
{"filename": "hypll/manifolds/__init__.py", "chunked_list": ["from .base import Manifold\n"]}
{"filename": "hypll/manifolds/poincare_ball/curvature.py", "chunked_list": ["from typing import Callable\n\nimport torch\nfrom torch import Tensor, as_tensor\nfrom torch.nn import Module\nfrom torch.nn.functional import softplus\nfrom torch.nn.parameter import Parameter\n\n\nclass Curvature(Module):\n    \"\"\"Class representing curvature of a manifold.\n\n    Attributes:\n        value:\n            Learnable parameter indicating curvature of the manifold. The actual\n            curvature is calculated as constraining_strategy(value).\n        constraining_strategy:\n            Function applied to the curvature value in order to constrain the\n            curvature of the manifold. By default uses softplus to guarantee\n            positive curvature.\n        requires_grad:\n            If the curvature requires gradient. False by default.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value: float = 1.0,\n        constraining_strategy: Callable[[Tensor], Tensor] = softplus,\n        requires_grad: bool = False,\n    ):\n        super(Curvature, self).__init__()\n        self.value = Parameter(\n            data=as_tensor(value, dtype=torch.float32),\n            requires_grad=requires_grad,\n        )\n        self.constraining_strategy = constraining_strategy\n\n    def forward(self) -> Tensor:\n        \"\"\"Returns curvature calculated as constraining_strategy(value).\"\"\"\n        return self.constraining_strategy(self.value)", "\nclass Curvature(Module):\n    \"\"\"Class representing curvature of a manifold.\n\n    Attributes:\n        value:\n            Learnable parameter indicating curvature of the manifold. The actual\n            curvature is calculated as constraining_strategy(value).\n        constraining_strategy:\n            Function applied to the curvature value in order to constrain the\n            curvature of the manifold. By default uses softplus to guarantee\n            positive curvature.\n        requires_grad:\n            If the curvature requires gradient. False by default.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        value: float = 1.0,\n        constraining_strategy: Callable[[Tensor], Tensor] = softplus,\n        requires_grad: bool = False,\n    ):\n        super(Curvature, self).__init__()\n        self.value = Parameter(\n            data=as_tensor(value, dtype=torch.float32),\n            requires_grad=requires_grad,\n        )\n        self.constraining_strategy = constraining_strategy\n\n    def forward(self) -> Tensor:\n        \"\"\"Returns curvature calculated as constraining_strategy(value).\"\"\"\n        return self.constraining_strategy(self.value)", ""]}
{"filename": "hypll/manifolds/poincare_ball/manifold.py", "chunked_list": ["import functools\nfrom typing import Optional, Union\n\nimport torch\nfrom torch import Tensor, empty, eye, no_grad\nfrom torch.nn import Parameter\nfrom torch.nn.common_types import _size_2_t\nfrom torch.nn.functional import softplus, unfold\nfrom torch.nn.init import normal_, zeros_\n", "from torch.nn.init import normal_, zeros_\n\nfrom hypll.manifolds.base import Manifold\nfrom hypll.manifolds.euclidean import Euclidean\nfrom hypll.manifolds.poincare_ball.curvature import Curvature\nfrom hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\nfrom hypll.utils.math import beta_func\nfrom hypll.utils.tensor_utils import (\n    check_dims_with_broadcasting,\n    check_tangent_tensor_positions,", "    check_dims_with_broadcasting,\n    check_tangent_tensor_positions,\n)\n\nfrom .math.diffgeom import (\n    dist,\n    euc_to_tangent,\n    expmap,\n    expmap0,\n    gyration,", "    expmap0,\n    gyration,\n    inner,\n    logmap,\n    logmap0,\n    mobius_add,\n    project,\n    transp,\n)\nfrom .math.linalg import poincare_fully_connected, poincare_hyperplane_dists", ")\nfrom .math.linalg import poincare_fully_connected, poincare_hyperplane_dists\nfrom .math.stats import frechet_mean, frechet_variance, midpoint\n\n\nclass PoincareBall(Manifold):\n    \"\"\"Class representing the Poincare ball model of hyperbolic space.\n\n    Implementation based on the geoopt implementation, but changed to use\n    hyperbolic torch functions.\n\n    Attributes:\n        c:\n            Curvature of the manifold.\n\n    \"\"\"\n\n    def __init__(self, c: Curvature):\n        \"\"\"Initializes an instance of PoincareBall manifold.\n\n        Examples:\n            >>> from hypll.manifolds.poincare_ball import PoincareBall, Curvature\n            >>> curvature = Curvature(value=1.0)\n            >>> manifold = Manifold(c=curvature)\n\n        \"\"\"\n        super(PoincareBall, self).__init__()\n        self.c = c\n\n    def mobius_add(self, x: ManifoldTensor, y: ManifoldTensor) -> ManifoldTensor:\n        dim = check_dims_with_broadcasting(x, y)\n        new_tensor = mobius_add(x=x.tensor, y=y.tensor, c=self.c(), dim=dim)\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=dim)\n\n    def project(self, x: ManifoldTensor, eps: float = -1.0) -> ManifoldTensor:\n        new_tensor = project(x=x.tensor, c=self.c(), dim=x.man_dim, eps=eps)\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=x.man_dim)\n\n    def expmap(self, v: TangentTensor) -> ManifoldTensor:\n        dim = v.broadcasted_man_dim\n        if v.manifold_points is None:\n            new_tensor = expmap0(v=v.tensor, c=self.c(), dim=dim)\n        else:\n            new_tensor = expmap(x=v.manifold_points.tensor, v=v.tensor, c=self.c(), dim=dim)\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=dim)\n\n    def logmap(self, x: Optional[ManifoldTensor], y: ManifoldTensor):\n        if x is None:\n            dim = y.man_dim\n            new_tensor = logmap0(y=y.tensor, c=self.c(), dim=y.man_dim)\n        else:\n            dim = check_dims_with_broadcasting(x, y)\n            new_tensor = logmap(x=x.tensor, y=y.tensor, c=self.c(), dim=dim)\n        return TangentTensor(data=new_tensor, manifold_points=x, manifold=self, man_dim=dim)\n\n    def gyration(self, u: ManifoldTensor, v: ManifoldTensor, w: ManifoldTensor) -> ManifoldTensor:\n        dim = check_dims_with_broadcasting(u, v, w)\n        new_tensor = gyration(u=u.tensor, v=v.tensor, w=w.tensor, c=self.c(), dim=dim)\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=dim)\n\n    def transp(self, v: TangentTensor, y: ManifoldTensor) -> TangentTensor:\n        dim = check_dims_with_broadcasting(v, y)\n        tangent_vectors = transp(\n            x=v.manifold_points.tensor, y=y.tensor, v=v.tensor, c=self.c(), dim=dim\n        )\n        return TangentTensor(\n            data=tangent_vectors,\n            manifold_points=y,\n            manifold=self,\n            man_dim=dim,\n        )\n\n    def dist(self, x: ManifoldTensor, y: ManifoldTensor) -> Tensor:\n        dim = check_dims_with_broadcasting(x, y)\n        return dist(x=x.tensor, y=y.tensor, c=self.c(), dim=dim)\n\n    def inner(\n        self, u: TangentTensor, v: TangentTensor, keepdim: bool = False, safe_mode: bool = True\n    ) -> Tensor:\n        dim = check_dims_with_broadcasting(u, v)\n        if safe_mode:\n            check_tangent_tensor_positions(u, v)\n\n        return inner(\n            x=u.manifold_points.tensor, u=u.tensor, v=v.tensor, c=self.c(), dim=dim, keepdim=keepdim\n        )\n\n    def euc_to_tangent(self, x: ManifoldTensor, u: ManifoldTensor) -> TangentTensor:\n        dim = check_dims_with_broadcasting(x, u)\n        tangent_vectors = euc_to_tangent(x=x.tensor, u=u.tensor, c=self.c(), dim=x.man_dim)\n        return TangentTensor(\n            data=tangent_vectors,\n            manifold_points=x,\n            manifold=self,\n            man_dim=dim,\n        )\n\n    def hyperplane_dists(self, x: ManifoldTensor, z: ManifoldTensor, r: Optional[Tensor]) -> Tensor:\n        if x.man_dim != 1 or z.man_dim != 0:\n            raise ValueError(\n                f\"Expected the manifold dimension of the inputs to be 1 and the manifold \"\n                f\"dimension of the hyperplane orientations to be 0, but got {x.man_dim} and \"\n                f\"{z.man_dim}, respectively\"\n            )\n        return poincare_hyperplane_dists(x=x.tensor, z=z.tensor, r=r, c=self.c())\n\n    def fully_connected(\n        self, x: ManifoldTensor, z: ManifoldTensor, bias: Optional[Tensor]\n    ) -> ManifoldTensor:\n        if z.man_dim != 0:\n            raise ValueError(\n                f\"Expected the manifold dimension of the hyperplane orientations to be 0, but got \"\n                f\"{z.man_dim} instead\"\n            )\n        new_tensor = poincare_fully_connected(\n            x=x.tensor, z=z.tensor, bias=bias, c=self.c(), dim=x.man_dim\n        )\n        new_tensor = ManifoldTensor(data=new_tensor, manifold=self, man_dim=x.man_dim)\n        return self.project(new_tensor)\n\n    def frechet_mean(\n        self,\n        x: ManifoldTensor,\n        batch_dim: Union[int, list[int]] = 0,\n        keepdim: bool = False,\n    ) -> ManifoldTensor:\n        if isinstance(batch_dim, int):\n            batch_dim = [batch_dim]\n        output_man_dim = x.man_dim - sum(bd < x.man_dim for bd in batch_dim)\n        new_tensor = frechet_mean(\n            x=x.tensor, c=self.c(), vec_dim=x.man_dim, batch_dim=batch_dim, keepdim=keepdim\n        )\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=output_man_dim)\n\n    def midpoint(\n        self,\n        x: ManifoldTensor,\n        batch_dim: int = 0,\n        w: Optional[Tensor] = None,\n        keepdim: bool = False,\n    ) -> ManifoldTensor:\n        if isinstance(batch_dim, int):\n            batch_dim = [batch_dim]\n\n        if x.man_dim in batch_dim:\n            raise ValueError(\n                f\"Tried to aggregate over dimensions {batch_dim}, but input has manifold \"\n                f\"dimension {x.man_dim} and cannot aggregate over this dimension\"\n            )\n\n        # Output manifold dimension is shifted left for each batch dim that disappears\n        man_dim_shift = sum(bd < x.man_dim for bd in batch_dim)\n        new_man_dim = x.man_dim - man_dim_shift if not keepdim else x.man_dim\n\n        new_tensor = midpoint(\n            x=x.tensor, c=self.c(), man_dim=x.man_dim, batch_dim=batch_dim, w=w, keepdim=keepdim\n        )\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=new_man_dim)\n\n    def frechet_variance(\n        self,\n        x: ManifoldTensor,\n        mu: Optional[ManifoldTensor] = None,\n        batch_dim: Union[int, list[int]] = -1,\n        keepdim: bool = False,\n    ) -> Tensor:\n        if mu is not None:\n            mu = mu.tensor\n\n        # TODO: Check if x and mu have compatible man_dims\n        return frechet_variance(\n            x=x.tensor,\n            c=self.c(),\n            mu=mu,\n            vec_dim=x.man_dim,\n            batch_dim=batch_dim,\n            keepdim=keepdim,\n        )\n\n    def construct_dl_parameters(\n        self, in_features: int, out_features: int, bias: bool = True\n    ) -> tuple[ManifoldParameter, Optional[Parameter]]:\n        weight = ManifoldParameter(\n            data=empty(in_features, out_features),\n            manifold=Euclidean(),\n            man_dim=0,\n        )\n\n        if bias:\n            b = Parameter(data=empty(out_features))\n        else:\n            b = None\n\n        return weight, b\n\n    def reset_parameters(self, weight: ManifoldParameter, bias: Optional[Parameter]) -> None:\n        in_features, out_features = weight.size()\n        if in_features <= out_features:\n            with no_grad():\n                weight.tensor.copy_(1 / 2 * eye(in_features, out_features))\n        else:\n            normal_(\n                weight.tensor,\n                mean=0,\n                std=(2 * in_features * out_features) ** -0.5,\n            )\n        if bias is not None:\n            zeros_(bias)\n\n    def unfold(\n        self,\n        input: ManifoldTensor,\n        kernel_size: _size_2_t,\n        dilation: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        stride: _size_2_t = 1,\n    ) -> ManifoldTensor:\n        # TODO: may have to cache some of this stuff for efficiency.\n        in_channels = input.size(1)\n        if len(kernel_size) == 2:\n            kernel_vol = kernel_size[0] * kernel_size[1]\n        else:\n            kernel_vol = kernel_size**2\n            kernel_size = (kernel_size, kernel_size)\n\n        beta_ni = beta_func(in_channels / 2, 1 / 2)\n        beta_n = beta_func(in_channels * kernel_vol / 2, 1 / 2)\n\n        input = self.logmap(x=None, y=input)\n        input.tensor = input.tensor * beta_n / beta_ni\n        new_tensor = unfold(\n            input=input.tensor,\n            kernel_size=kernel_size,\n            dilation=dilation,\n            padding=padding,\n            stride=stride,\n        )\n\n        new_tensor = TangentTensor(data=new_tensor, manifold_points=None, manifold=self, man_dim=1)\n        return self.expmap(new_tensor)\n\n    def flatten(self, x: ManifoldTensor, start_dim: int = 1, end_dim: int = -1) -> ManifoldTensor:\n        \"\"\"Flattens a manifold tensor by reshaping it. If start_dim or end_dim are passed,\n        only dimensions starting with start_dim and ending with end_dim are flattend.\n\n        If the manifold dimension of the input tensor is among the dimensions which\n        are flattened, applies beta-concatenation to the points on the manifold.\n        Otherwise simply flattens the tensor using torch.flatten.\n\n        Updates the manifold dimension if necessary.\n\n        \"\"\"\n        start_dim = x.dim() + start_dim if start_dim < 0 else start_dim\n        end_dim = x.dim() + end_dim if end_dim < 0 else end_dim\n\n        # Get the range of dimensions to flatten.\n        dimensions_to_flatten = x.shape[start_dim + 1 : end_dim + 1]\n\n        if start_dim <= x.man_dim and end_dim >= x.man_dim:\n            # Use beta concatenation to flatten the manifold dimension of the tensor.\n            #\n            # Start by applying logmap at the origin and computing the betas.\n            tangents = self.logmap(None, x)\n            n_i = x.shape[x.man_dim]\n            n = n_i * functools.reduce(lambda a, b: a * b, dimensions_to_flatten)\n            beta_n = beta_func(n / 2, 0.5)\n            beta_n_i = beta_func(n_i / 2, 0.5)\n            # Flatten the tensor and rescale.\n            tangents.tensor = torch.flatten(\n                input=tangents.tensor,\n                start_dim=start_dim,\n                end_dim=end_dim,\n            )\n            tangents.tensor = tangents.tensor * beta_n / beta_n_i\n            # Set the new manifold dimension\n            tangents.man_dim = start_dim\n            # Apply exponential map at the origin.\n            return self.expmap(tangents)\n        else:\n            flattened = torch.flatten(\n                input=x.tensor,\n                start_dim=start_dim,\n                end_dim=end_dim,\n            )\n            man_dim = x.man_dim if end_dim > x.man_dim else x.man_dim - len(dimensions_to_flatten)\n            return ManifoldTensor(data=flattened, manifold=x.manifold, man_dim=man_dim)", ""]}
{"filename": "hypll/manifolds/poincare_ball/__init__.py", "chunked_list": ["from .curvature import Curvature\nfrom .manifold import PoincareBall\n"]}
{"filename": "hypll/manifolds/poincare_ball/math/diffgeom.py", "chunked_list": ["import torch\n\n\ndef mobius_add(x: torch.Tensor, y: torch.Tensor, c: torch.Tensor, dim: int = -1):\n    broadcasted_dim = max(x.dim(), y.dim())\n    dim = dim if dim >= 0 else broadcasted_dim + dim\n    x2 = x.pow(2).sum(\n        dim=dim - broadcasted_dim + x.dim(),\n        keepdim=True,\n    )\n    y2 = y.pow(2).sum(\n        dim=dim - broadcasted_dim + y.dim(),\n        keepdim=True,\n    )\n    xy = (x * y).sum(dim=dim, keepdim=True)\n    num = (1 + 2 * c * xy + c * y2) * x + (1 - c * x2) * y\n    denom = 1 + 2 * c * xy + c**2 * x2 * y2\n    return num / denom.clamp_min(1e-15)", "\n\ndef project(x: torch.Tensor, c: torch.Tensor, dim: int = -1, eps: float = -1.0):\n    if eps < 0:\n        if x.dtype == torch.float32:\n            eps = 4e-3\n        else:\n            eps = 1e-5\n    maxnorm = (1 - eps) / ((c + 1e-15) ** 0.5)\n    maxnorm = torch.where(c.gt(0), maxnorm, c.new_full((), 1e15))\n    norm = x.norm(dim=dim, keepdim=True, p=2).clamp_min(1e-15)\n    cond = norm > maxnorm\n    projected = x / norm * maxnorm\n    return torch.where(cond, projected, x)", "\n\ndef expmap0(v: torch.Tensor, c: torch.Tensor, dim: int = -1):\n    v_norm_c_sqrt = v.norm(dim=dim, keepdim=True).clamp_min(1e-15) * c.sqrt()\n    return project(torch.tanh(v_norm_c_sqrt) * v / v_norm_c_sqrt, c, dim=dim)\n\n\ndef logmap0(y: torch.Tensor, c: torch.Tensor, dim: int = -1):\n    y_norm_c_sqrt = y.norm(dim=dim, keepdim=True).clamp_min(1e-15) * c.sqrt()\n    return torch.atanh(y_norm_c_sqrt) * y / y_norm_c_sqrt", "\n\ndef expmap(x: torch.Tensor, v: torch.Tensor, c: torch.Tensor, dim: int = -1):\n    broadcasted_dim = max(x.dim(), v.dim())\n    dim = dim if dim >= 0 else broadcasted_dim + dim\n    v_norm = v.norm(dim=dim - broadcasted_dim + v.dim(), keepdim=True).clamp_min(1e-15)\n    lambda_x = 2 / (\n        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n    ).clamp_min(1e-15)\n    c_sqrt = c.sqrt()\n    second_term = torch.tanh(c_sqrt * lambda_x * v_norm / 2) * v / (c_sqrt * v_norm)\n    return project(mobius_add(x, second_term, c, dim=dim), c, dim=dim)", "\n\ndef logmap(x: torch.Tensor, y: torch.Tensor, c: torch.Tensor, dim: int = -1):\n    broadcasted_dim = max(x.dim(), y.dim())\n    dim = dim if dim >= 0 else broadcasted_dim + dim\n    min_x_y = mobius_add(-x, y, c, dim=dim)\n    min_x_y_norm = min_x_y.norm(dim=dim, keepdim=True).clamp_min(1e-15)\n    lambda_x = 2 / (\n        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n    ).clamp_min(1e-15)\n    c_sqrt = c.sqrt()\n    return 2 / (c_sqrt * lambda_x) * torch.atanh(c_sqrt * min_x_y_norm) * min_x_y / min_x_y_norm", "\n\ndef gyration(\n    u: torch.Tensor,\n    v: torch.Tensor,\n    w: torch.Tensor,\n    c: torch.Tensor,\n    dim: int = -1,\n):\n    broadcasted_dim = max(u.dim(), v.dim(), w.dim())\n    dim = dim if dim >= 0 else broadcasted_dim + dim\n    u2 = u.pow(2).sum(dim=dim - broadcasted_dim + u.dim(), keepdim=True)\n    v2 = v.pow(2).sum(dim=dim - broadcasted_dim + v.dim(), keepdim=True)\n    uv = (u * v).sum(dim=dim - broadcasted_dim + max(u.dim(), v.dim()), keepdim=True)\n    uw = (u * w).sum(dim=dim - broadcasted_dim + max(u.dim(), w.dim()), keepdim=True)\n    vw = (v * w).sum(dim=dim - broadcasted_dim + max(v.dim(), w.dim()), keepdim=True)\n    K2 = c**2\n    a = -K2 * uw * v2 + c * vw + 2 * K2 * uv * vw\n    b = -K2 * vw * u2 - c * uw\n    d = 1 + 2 * c * uv + K2 * u2 * v2\n    return w + 2 * (a * u + b * v) / d.clamp_min(1e-15)", "\n\ndef transp(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    v: torch.Tensor,\n    c: torch.Tensor,\n    dim: int = -1,\n):\n    broadcasted_dim = max(x.dim(), y.dim(), v.dim())\n    dim = dim if dim >= 0 else broadcasted_dim + dim\n    lambda_x = 2 / (\n        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n    ).clamp_min(1e-15)\n    lambda_y = 2 / (\n        1 - c * y.pow(2).sum(dim=dim - broadcasted_dim + y.dim(), keepdim=True)\n    ).clamp_min(1e-15)\n    return gyration(y, -x, v, c, dim=dim) * lambda_x / lambda_y", "\n\ndef dist(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    c: torch.Tensor,\n    dim: int = -1,\n    keepdim: bool = False,\n) -> torch.Tensor:\n    return (\n        2\n        / c.sqrt()\n        * (c.sqrt() * mobius_add(-x, y, c, dim=dim).norm(dim=dim, keepdim=keepdim)).atanh()\n    )", "\n\ndef inner(\n    x: torch.Tensor,\n    u: torch.Tensor,\n    v: torch.Tensor,\n    c: torch.Tensor,\n    dim: int = -1,\n    keepdim: bool = False,\n) -> torch.Tensor:\n    broadcasted_dim = max(x.dim(), u.dim(), v.dim())\n    dim = dim if dim >= 0 else broadcasted_dim + dim\n    lambda_x = 2 / (\n        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n    ).clamp_min(1e-15)\n    dot_prod = (u * v).sum(dim=dim, keepdim=keepdim)\n    return lambda_x.square() * dot_prod", "\n\ndef euc_to_tangent(\n    x: torch.Tensor,\n    u: torch.Tensor,\n    c: torch.Tensor,\n    dim: int = -1,\n) -> torch.Tensor:\n    broadcasted_dim = max(x.dim(), u.dim())\n    dim = dim if dim >= 0 else broadcasted_dim + dim\n    lambda_x = 2 / (\n        1 - c * x.pow(2).sum(dim=dim - broadcasted_dim + x.dim(), keepdim=True)\n    ).clamp_min(1e-15)\n    return u / lambda_x**2", ""]}
{"filename": "hypll/manifolds/poincare_ball/math/__init__.py", "chunked_list": [""]}
{"filename": "hypll/manifolds/poincare_ball/math/linalg.py", "chunked_list": ["from typing import Optional\n\nimport torch\n\n\ndef poincare_hyperplane_dists(\n    x: torch.Tensor,\n    z: torch.Tensor,\n    r: Optional[torch.Tensor],\n    c: torch.Tensor,\n    dim: int = -1,\n) -> torch.Tensor:\n    \"\"\"\n    The Poincare signed distance to hyperplanes operation.\n\n    Parameters\n    ----------\n    x : tensor\n        contains input values\n    z : tensor\n        contains the hyperbolic vectors describing the hyperplane orientations\n    r : tensor\n        contains the hyperplane offsets\n    c : tensor\n        curvature of the Poincare disk\n\n    Returns\n    -------\n    tensor\n        signed distances of input w.r.t. the hyperplanes, denoted by v_k(x) in\n        the HNN++ paper\n    \"\"\"\n    dim_shifted_x = x.movedim(source=dim, destination=-1)\n\n    c_sqrt = c.sqrt()\n    lam = 2 * (1 - c * dim_shifted_x.pow(2).sum(dim=-1, keepdim=True))\n    z_norm = z.norm(dim=0).clamp_min(1e-15)\n\n    # Computation can be simplified if there is no offset\n    if r is None:\n        dim_shifted_output = (\n            2\n            * z_norm\n            / c_sqrt\n            * torch.asinh(c_sqrt * lam / z_norm * torch.matmul(dim_shifted_x, z))\n        )\n    else:\n        two_csqrt_r = 2.0 * c_sqrt * r\n        dim_shifted_output = (\n            2\n            * z_norm\n            / c_sqrt\n            * torch.asinh(\n                c_sqrt * lam / z_norm * torch.matmul(dim_shifted_x, z) * two_csqrt_r.cosh()\n                - (lam - 1) * two_csqrt_r.sinh()\n            )\n        )\n\n    return dim_shifted_output.movedim(source=-1, destination=dim)", "\n\ndef poincare_fully_connected(\n    x: torch.Tensor,\n    z: torch.Tensor,\n    bias: Optional[torch.Tensor],\n    c: torch.Tensor,\n    dim: int = -1,\n) -> torch.Tensor:\n    \"\"\"\n    The Poincare fully connected layer operation.\n\n    Parameters\n    ----------\n    x : tensor\n        contains the layer inputs\n    z : tensor\n        contains the hyperbolic vectors describing the hyperplane orientations\n    bias : tensor\n        contains the biases (hyperplane offsets)\n    c : tensor\n        curvature of the Poincare disk\n\n    Returns\n    -------\n    tensor\n        Poincare FC transformed hyperbolic tensor, commonly denoted by y\n    \"\"\"\n    c_sqrt = c.sqrt()\n    x = poincare_hyperplane_dists(x=x, z=z, r=bias, c=c, dim=dim)\n    x = (c_sqrt * x).sinh() / c_sqrt\n    return x / (1 + (1 + c * x.pow(2).sum(dim=dim, keepdim=True)).sqrt())", ""]}
{"filename": "hypll/manifolds/poincare_ball/math/stats.py", "chunked_list": ["from typing import Optional, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom .diffgeom import dist\n\n_TOLEPS = {torch.float32: 1e-6, torch.float64: 1e-12}\n\n\nclass FrechetMean(torch.autograd.Function):\n    \"\"\"\n    This implementation is copied mostly from:\n        https://github.com/CUVL/Differentiable-Frechet-Mean.git\n\n    which is itself based on the paper:\n        https://arxiv.org/abs/2003.00335\n\n    Both by Aaron Lou (et al.)\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x, c, vec_dim, batch_dim, keepdim):\n        # Convert input dimensions to positive values\n        vec_dim = vec_dim if vec_dim > 0 else x.dim() + vec_dim\n        batch_dim = [bd if bd >= 0 else x.dim() + bd for bd in batch_dim]\n\n        # Compute some dim ids for later\n        output_vec_dim = vec_dim - sum(bd < vec_dim for bd in batch_dim)\n        batch_start_id = -len(batch_dim) - 1\n        batch_stop_id = -2\n        original_dims = [x.size(bd) for bd in batch_dim]\n\n        # Move dims around and flatten the batch dimensions\n        x = x.movedim(\n            source=batch_dim + [vec_dim],\n            destination=list(range(batch_start_id, 0)),\n        )\n        x = x.flatten(\n            start_dim=batch_start_id, end_dim=batch_stop_id\n        )  # ..., prod(batch_dim), vec_dim\n\n        # Compute frechet mean and store variables\n        mean = frechet_ball_forward(X=x, K=c, rtol=_TOLEPS[x.dtype], atol=_TOLEPS[x.dtype])\n        ctx.save_for_backward(x, mean, c)\n        ctx.vec_dim = vec_dim\n        ctx.batch_dim = batch_dim\n        ctx.output_vec_dim = output_vec_dim\n        ctx.batch_start_id = batch_start_id\n        ctx.original_dims = original_dims\n\n        # Reorder dimensions to match dimensions of input\n        mean = mean.movedim(\n            source=list(range(output_vec_dim - mean.dim(), 0)),\n            destination=list(range(output_vec_dim + 1, mean.dim())) + [output_vec_dim],\n        )\n\n        # Add dimensions back to mean if keepdim\n        if keepdim:\n            for bd in sorted(batch_dim):\n                mean = mean.unsqueeze(bd)\n\n        return mean\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, mean, c = ctx.saved_tensors\n        # Reshift dims in grad to match the dims of the mean that was stored in ctx\n        grad_output = grad_output.movedim(\n            source=list(range(ctx.output_vec_dim - mean.dim(), 0)),\n            destination=[-1] + list(range(ctx.output_vec_dim - mean.dim(), -1)),\n        )\n        dx, dc = frechet_ball_backward(X=x, y=mean, grad=grad_output, K=c)\n        vec_dim = ctx.vec_dim\n        batch_dim = ctx.batch_dim\n        batch_start_id = ctx.batch_start_id\n        original_dims = ctx.original_dims\n\n        # Unflatten the batch dimensions\n        dx = dx.unflatten(dim=-2, sizes=original_dims)\n\n        # Move the vector dimension back\n        dx = dx.movedim(\n            source=list(range(batch_start_id, 0)),\n            destination=batch_dim + [vec_dim],\n        )\n\n        return dx, dc, None, None, None", "\n\nclass FrechetMean(torch.autograd.Function):\n    \"\"\"\n    This implementation is copied mostly from:\n        https://github.com/CUVL/Differentiable-Frechet-Mean.git\n\n    which is itself based on the paper:\n        https://arxiv.org/abs/2003.00335\n\n    Both by Aaron Lou (et al.)\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x, c, vec_dim, batch_dim, keepdim):\n        # Convert input dimensions to positive values\n        vec_dim = vec_dim if vec_dim > 0 else x.dim() + vec_dim\n        batch_dim = [bd if bd >= 0 else x.dim() + bd for bd in batch_dim]\n\n        # Compute some dim ids for later\n        output_vec_dim = vec_dim - sum(bd < vec_dim for bd in batch_dim)\n        batch_start_id = -len(batch_dim) - 1\n        batch_stop_id = -2\n        original_dims = [x.size(bd) for bd in batch_dim]\n\n        # Move dims around and flatten the batch dimensions\n        x = x.movedim(\n            source=batch_dim + [vec_dim],\n            destination=list(range(batch_start_id, 0)),\n        )\n        x = x.flatten(\n            start_dim=batch_start_id, end_dim=batch_stop_id\n        )  # ..., prod(batch_dim), vec_dim\n\n        # Compute frechet mean and store variables\n        mean = frechet_ball_forward(X=x, K=c, rtol=_TOLEPS[x.dtype], atol=_TOLEPS[x.dtype])\n        ctx.save_for_backward(x, mean, c)\n        ctx.vec_dim = vec_dim\n        ctx.batch_dim = batch_dim\n        ctx.output_vec_dim = output_vec_dim\n        ctx.batch_start_id = batch_start_id\n        ctx.original_dims = original_dims\n\n        # Reorder dimensions to match dimensions of input\n        mean = mean.movedim(\n            source=list(range(output_vec_dim - mean.dim(), 0)),\n            destination=list(range(output_vec_dim + 1, mean.dim())) + [output_vec_dim],\n        )\n\n        # Add dimensions back to mean if keepdim\n        if keepdim:\n            for bd in sorted(batch_dim):\n                mean = mean.unsqueeze(bd)\n\n        return mean\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, mean, c = ctx.saved_tensors\n        # Reshift dims in grad to match the dims of the mean that was stored in ctx\n        grad_output = grad_output.movedim(\n            source=list(range(ctx.output_vec_dim - mean.dim(), 0)),\n            destination=[-1] + list(range(ctx.output_vec_dim - mean.dim(), -1)),\n        )\n        dx, dc = frechet_ball_backward(X=x, y=mean, grad=grad_output, K=c)\n        vec_dim = ctx.vec_dim\n        batch_dim = ctx.batch_dim\n        batch_start_id = ctx.batch_start_id\n        original_dims = ctx.original_dims\n\n        # Unflatten the batch dimensions\n        dx = dx.unflatten(dim=-2, sizes=original_dims)\n\n        # Move the vector dimension back\n        dx = dx.movedim(\n            source=list(range(batch_start_id, 0)),\n            destination=batch_dim + [vec_dim],\n        )\n\n        return dx, dc, None, None, None", "\n\ndef frechet_mean(\n    x: torch.Tensor,\n    c: torch.Tensor,\n    vec_dim: int = -1,\n    batch_dim: Union[int, list[int]] = 0,\n    keepdim: bool = False,\n) -> torch.Tensor:\n    if isinstance(batch_dim, int):\n        batch_dim = [batch_dim]\n\n    return FrechetMean.apply(x, c, vec_dim, batch_dim, keepdim)", "\n\ndef midpoint(\n    x: torch.Tensor,\n    c: torch.Tensor,\n    man_dim: int = -1,\n    batch_dim: Union[int, list[int]] = 0,\n    w: Optional[torch.Tensor] = None,\n    keepdim: bool = False,\n) -> torch.Tensor:\n    lambda_x = 2 / (1 - c * x.pow(2).sum(dim=man_dim, keepdim=True)).clamp_min(1e-15)\n    if w is None:\n        numerator = (lambda_x * x).sum(dim=batch_dim, keepdim=True)\n        denominator = (lambda_x - 1).sum(dim=batch_dim, keepdim=True)\n    else:\n        # TODO: test weights\n        numerator = (lambda_x * w * x).sum(dim=batch_dim, keepdim=True)\n        denominator = ((lambda_x - 1) * w).sum(dim=batch_dim, keepdim=True)\n\n    frac = numerator / denominator.clamp_min(1e-15)\n    midpoint = 1 / (1 + (1 - c * frac.pow(2).sum(dim=man_dim, keepdim=True)).sqrt()) * frac\n    if not keepdim:\n        midpoint = midpoint.squeeze(dim=batch_dim)\n\n    return midpoint", "\n\ndef frechet_variance(\n    x: torch.Tensor,\n    c: torch.Tensor,\n    mu: Optional[torch.Tensor] = None,\n    vec_dim: int = -1,\n    batch_dim: Union[int, list[int]] = 0,\n    keepdim: bool = False,\n) -> torch.Tensor:  # TODO\n    \"\"\"\n    Args\n    ----\n        x (tensor): points of shape [..., points, dim]\n        mu (tensor): mean of shape [..., dim]\n\n        where the ... of the three variables match\n\n    Returns\n    -------\n        tensor of shape [...]\n    \"\"\"\n    if isinstance(batch_dim, int):\n        batch_dim = [batch_dim]\n\n    if mu is None:\n        mu = frechet_mean(x=x, c=c, vec_dim=vec_dim, batch_dim=batch_dim, keepdim=True)\n\n    if x.dim() != mu.dim():\n        for bd in sorted(batch_dim):\n            mu = mu.unsqueeze(bd)\n\n    distance = dist(x=x, y=mu, c=c, dim=vec_dim, keepdim=keepdim)\n    distance = distance.pow(2)\n    return distance.mean(dim=batch_dim, keepdim=keepdim)", "\n\ndef l_prime(y: torch.Tensor) -> torch.Tensor:\n    cond = y < 1e-12\n    val = 4 * torch.ones_like(y)\n    ret = torch.where(cond, val, 2 * (1 + 2 * y).acosh() / (y.pow(2) + y).sqrt())\n    return ret\n\n\ndef frechet_ball_forward(\n    X: torch.Tensor,\n    K: torch.Tensor = torch.Tensor([1]),\n    max_iter: int = 1000,\n    rtol: float = 1e-6,\n    atol: float = 1e-6,\n) -> torch.Tensor:\n    \"\"\"\n    Args\n    ----\n        X (tensor): point of shape [..., points, dim]\n        K (float): curvature (must be negative)\n\n    Returns\n    -------\n        frechet mean (tensor): shape [..., dim]\n    \"\"\"\n    mu = X[..., 0, :].clone()\n\n    x_ss = X.pow(2).sum(dim=-1)\n\n    mu_prev = mu\n    iters = 0\n    for _ in range(max_iter):\n        mu_ss = mu.pow(2).sum(dim=-1)\n        xmu_ss = (X - mu.unsqueeze(-2)).pow(2).sum(dim=-1)\n\n        alphas = l_prime(K * xmu_ss / ((1 - K * x_ss) * (1 - K * mu_ss.unsqueeze(-1)))) / (\n            1 - K * x_ss\n        )\n\n        alphas = alphas\n\n        c = (alphas * x_ss).sum(dim=-1)\n        b = (alphas.unsqueeze(-1) * X).sum(dim=-2)\n        a = alphas.sum(dim=-1)\n\n        b_ss = b.pow(2).sum(dim=-1)\n\n        eta = (a + K * c - ((a + K * c).pow(2) - 4 * K * b_ss).sqrt()) / (2 * K * b_ss).clamp_min(\n            1e-15\n        )\n\n        mu = eta.unsqueeze(-1) * b\n\n        dist = (mu - mu_prev).norm(dim=-1)\n        prev_dist = mu_prev.norm(dim=-1)\n        if (dist < atol).all() or (dist / prev_dist < rtol).all():\n            break\n\n        mu_prev = mu\n        iters += 1\n\n    return mu", "\ndef frechet_ball_forward(\n    X: torch.Tensor,\n    K: torch.Tensor = torch.Tensor([1]),\n    max_iter: int = 1000,\n    rtol: float = 1e-6,\n    atol: float = 1e-6,\n) -> torch.Tensor:\n    \"\"\"\n    Args\n    ----\n        X (tensor): point of shape [..., points, dim]\n        K (float): curvature (must be negative)\n\n    Returns\n    -------\n        frechet mean (tensor): shape [..., dim]\n    \"\"\"\n    mu = X[..., 0, :].clone()\n\n    x_ss = X.pow(2).sum(dim=-1)\n\n    mu_prev = mu\n    iters = 0\n    for _ in range(max_iter):\n        mu_ss = mu.pow(2).sum(dim=-1)\n        xmu_ss = (X - mu.unsqueeze(-2)).pow(2).sum(dim=-1)\n\n        alphas = l_prime(K * xmu_ss / ((1 - K * x_ss) * (1 - K * mu_ss.unsqueeze(-1)))) / (\n            1 - K * x_ss\n        )\n\n        alphas = alphas\n\n        c = (alphas * x_ss).sum(dim=-1)\n        b = (alphas.unsqueeze(-1) * X).sum(dim=-2)\n        a = alphas.sum(dim=-1)\n\n        b_ss = b.pow(2).sum(dim=-1)\n\n        eta = (a + K * c - ((a + K * c).pow(2) - 4 * K * b_ss).sqrt()) / (2 * K * b_ss).clamp_min(\n            1e-15\n        )\n\n        mu = eta.unsqueeze(-1) * b\n\n        dist = (mu - mu_prev).norm(dim=-1)\n        prev_dist = mu_prev.norm(dim=-1)\n        if (dist < atol).all() or (dist / prev_dist < rtol).all():\n            break\n\n        mu_prev = mu\n        iters += 1\n\n    return mu", "\n\ndef darcosh(x):\n    cond = x < 1 + 1e-7\n    x = torch.where(cond, 2 * torch.ones_like(x), x)\n    x = torch.where(~cond, 2 * (x).acosh() / torch.sqrt(x**2 - 1), x)\n    return x\n\n\ndef d2arcosh(x):\n    cond = x < 1 + 1e-7\n    x = torch.where(cond, -2 / 3 * torch.ones_like(x), x)\n    x = torch.where(\n        ~cond,\n        2 / (x**2 - 1) - 2 * x * x.acosh() / ((x**2 - 1) ** (3 / 2)),\n        x,\n    )\n    return x", "\ndef d2arcosh(x):\n    cond = x < 1 + 1e-7\n    x = torch.where(cond, -2 / 3 * torch.ones_like(x), x)\n    x = torch.where(\n        ~cond,\n        2 / (x**2 - 1) - 2 * x * x.acosh() / ((x**2 - 1) ** (3 / 2)),\n        x,\n    )\n    return x", "\n\ndef grad_var(\n    X: torch.Tensor,\n    y: torch.Tensor,\n    K: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Args\n    ----\n        X (tensor): point of shape [..., points, dim]\n        y (tensor): mean point of shape [..., dim]\n        K (float): curvature (must be negative)\n\n    Returns\n    -------\n        grad (tensor): gradient of variance [..., dim]\n    \"\"\"\n    yl = y.unsqueeze(-2)\n    xnorm = 1 - K * X.pow(2).sum(dim=-1)\n    ynorm = 1 - K * yl.pow(2).sum(dim=-1)\n    xynorm = (X - yl).pow(2).sum(dim=-1)\n\n    D = xnorm * ynorm\n    v = 1 + 2 * K * xynorm / D\n\n    Dl = D.unsqueeze(-1)\n    vl = v.unsqueeze(-1)\n\n    first_term = (X - yl) / Dl\n    sec_term = -K / Dl.pow(2) * yl * xynorm.unsqueeze(-1) * xnorm.unsqueeze(-1)\n    return -(4 * darcosh(vl) * (first_term + sec_term)).sum(dim=-2)", "\n\ndef inverse_hessian(\n    X: torch.Tensor,\n    y: torch.Tensor,\n    K: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Args\n    ----\n        X (tensor): point of shape [..., points, dim]\n        y (tensor): mean point of shape [..., dim]\n        K (float): curvature (must be negative)\n\n    Returns\n    -------\n        inv_hess (tensor): inverse hessian of [..., points, dim, dim]\n    \"\"\"\n    yl = y.unsqueeze(-2)\n    xnorm = 1 - K * X.pow(2).sum(dim=-1)\n    ynorm = 1 - K * yl.pow(2).sum(dim=-1)\n    xynorm = (X - yl).pow(2).sum(dim=-1)\n\n    D = xnorm * ynorm\n    v = 1 + 2 * K * xynorm / D\n\n    Dl = D.unsqueeze(-1)\n    vl = v.unsqueeze(-1)\n    vll = vl.unsqueeze(-1)\n\n    \"\"\"\n    \\partial T/ \\partial y\n    \"\"\"\n    first_const = -8 * (K**2) * xnorm / D.pow(2)\n    matrix_val = (first_const.unsqueeze(-1) * yl).unsqueeze(-1) * (X - yl).unsqueeze(-2)\n    first_term = matrix_val + matrix_val.transpose(-1, -2)\n\n    sec_const = 16 * (K**3) * xnorm.pow(2) / D.pow(3) * xynorm\n    sec_term = (sec_const.unsqueeze(-1) * yl).unsqueeze(-1) * yl.unsqueeze(-2)\n\n    third_const = 4 * K / D + 4 * (K**2) * xnorm / D.pow(2) * xynorm\n    third_term = third_const.reshape(*third_const.shape, 1, 1) * torch.eye(y.shape[-1]).to(\n        X\n    ).reshape((1,) * len(third_const.shape) + (y.shape[-1], y.shape[-1]))\n\n    Ty = first_term + sec_term + third_term\n\n    \"\"\"\n    T\n    \"\"\"\n\n    first_term = -K / Dl * (X - yl)\n    sec_term = K.pow(2) / Dl.pow(2) * yl * xynorm.unsqueeze(-1) * xnorm.unsqueeze(-1)\n    T = 4 * (first_term + sec_term)\n\n    \"\"\"\n    inverse of shape [..., points, dim, dim]\n    \"\"\"\n    first_term = d2arcosh(vll) * T.unsqueeze(-1) * T.unsqueeze(-2)\n    sec_term = darcosh(vll) * Ty\n    hessian = (first_term + sec_term).sum(dim=-3) / K\n    inv_hess = torch.inverse(hessian)\n    return inv_hess", "\n\ndef frechet_ball_backward(\n    X: torch.Tensor,\n    y: torch.Tensor,\n    grad: torch.Tensor,\n    K: torch.Tensor,\n) -> tuple[torch.Tensor]:\n    \"\"\"\n    Args\n    ----\n        X (tensor): point of shape [..., points, dim]\n        y (tensor): mean point of shape [..., dim]\n        grad (tensor): gradient\n        K (float): curvature (must be negative)\n\n    Returns\n    -------\n        gradients (tensor, tensor, tensor):\n            gradient of X [..., points, dim], curvature []\n    \"\"\"\n    if not torch.is_tensor(K):\n        K = torch.tensor(K).to(X)\n\n    with torch.no_grad():\n        inv_hess = inverse_hessian(X, y, K=K)\n\n    with torch.enable_grad():\n        # clone variables\n        X = nn.Parameter(X.detach())\n        y = y.detach()\n        K = nn.Parameter(K)\n\n        grad = (inv_hess @ grad.unsqueeze(-1)).squeeze()\n        gradf = grad_var(X, y, K)\n        dx, dK = torch.autograd.grad(-gradf.squeeze(), (X, K), grad)\n\n    return dx, dK", ""]}
{"filename": "hypll/manifolds/base/manifold.py", "chunked_list": ["from __future__ import annotations\n\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Optional, Union\n\nfrom torch import Tensor\nfrom torch.nn import Module, Parameter\nfrom torch.nn.common_types import _size_2_t\n\n# TODO: find a less hacky solution for this\nif TYPE_CHECKING:\n    from hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor", "\n# TODO: find a less hacky solution for this\nif TYPE_CHECKING:\n    from hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n\n\nclass Manifold(Module, ABC):\n    def __init__(self) -> None:\n        super(Manifold, self).__init__()\n\n    @abstractmethod\n    def project(self, x: ManifoldTensor, eps: float = -1.0) -> ManifoldTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def expmap(self, v: TangentTensor) -> ManifoldTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def logmap(self, x: Optional[ManifoldTensor], y: ManifoldTensor) -> TangentTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def transp(self, v: TangentTensor, y: ManifoldTensor) -> TangentTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def dist(self, x: ManifoldTensor, y: ManifoldTensor) -> Tensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def euc_to_tangent(self, x: ManifoldTensor, u: ManifoldTensor) -> TangentTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def hyperplane_dists(self, x: ManifoldTensor, z: ManifoldTensor, r: Optional[Tensor]) -> Tensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def fully_connected(\n        self, x: ManifoldTensor, z: ManifoldTensor, bias: Optional[Tensor]\n    ) -> ManifoldTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def frechet_mean(\n        self,\n        x: ManifoldTensor,\n        batch_dim: Union[int, list[int]] = 0,\n        keepdim: bool = False,\n    ) -> ManifoldTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def midpoint(\n        self,\n        x: ManifoldTensor,\n        batch_dim: Union[int, list[int]] = 0,\n        w: Optional[Tensor] = None,\n        keepdim: bool = False,\n    ) -> ManifoldTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def frechet_variance(\n        self,\n        x: ManifoldTensor,\n        mu: Optional[ManifoldTensor] = None,\n        batch_dim: Union[int, list[int]] = -1,\n        keepdim: bool = False,\n    ) -> Tensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def construct_dl_parameters(\n        self, in_features: int, out_features: int, bias: bool = True\n    ) -> Union[ManifoldParameter, tuple[ManifoldParameter, Parameter]]:\n        # TODO: make an annotation object for the return type of this method\n        raise NotImplementedError\n\n    @abstractmethod\n    def reset_parameters(self, weight: ManifoldParameter, bias: Parameter) -> None:\n        raise NotImplementedError\n\n    @abstractmethod\n    def flatten(self, x: ManifoldTensor, start_dim: int = 1, end_dim: int = -1) -> ManifoldTensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def unfold(\n        self,\n        input: ManifoldTensor,\n        kernel_size: _size_2_t,\n        dilation: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        stride: _size_2_t = 1,\n    ) -> ManifoldTensor:\n        raise NotImplementedError", ""]}
{"filename": "hypll/manifolds/base/__init__.py", "chunked_list": ["from .manifold import Manifold\n"]}
{"filename": "hypll/manifolds/euclidean/manifold.py", "chunked_list": ["from math import sqrt\nfrom typing import Optional, Union\n\nimport torch\nfrom torch import Tensor, broadcast_shapes, empty, matmul, var\nfrom torch.nn import Parameter\nfrom torch.nn.common_types import _size_2_t\nfrom torch.nn.functional import unfold\nfrom torch.nn.init import _calculate_fan_in_and_fan_out, kaiming_uniform_, uniform_\n", "from torch.nn.init import _calculate_fan_in_and_fan_out, kaiming_uniform_, uniform_\n\nfrom hypll.manifolds.base import Manifold\nfrom hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\nfrom hypll.utils.tensor_utils import (\n    check_dims_with_broadcasting,\n    check_tangent_tensor_positions,\n)\n\n\nclass Euclidean(Manifold):\n    def __init__(self):\n        super(Euclidean, self).__init__()\n\n    def project(self, x: ManifoldTensor, eps: float = -1.0) -> ManifoldTensor:\n        return x\n\n    def expmap(self, v: TangentTensor) -> ManifoldTensor:\n        if v.manifold_points is None:\n            new_tensor = v.tensor\n        else:\n            new_tensor = v.manifold_points.tensor + v.tensor\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=v.broadcasted_man_dim)\n\n    def logmap(self, x: Optional[ManifoldTensor], y: ManifoldTensor) -> TangentTensor:\n        if x is None:\n            dim = y.man_dim\n            new_tensor = y.tensor\n        else:\n            dim = check_dims_with_broadcasting(x, y)\n            new_tensor = y.tensor - x.tensor\n        return TangentTensor(data=new_tensor, manifold_points=x, manifold=self, man_dim=dim)\n\n    def transp(self, v: TangentTensor, y: ManifoldTensor) -> TangentTensor:\n        dim = check_dims_with_broadcasting(v, y)\n        output_shape = broadcast_shapes(v.size(), y.size())\n        new_tensor = v.tensor.broadcast_to(output_shape)\n        return TangentTensor(data=new_tensor, manifold_points=y, manifold=self, man_dim=dim)\n\n    def dist(self, x: ManifoldTensor, y: ManifoldTensor) -> Tensor:\n        dim = check_dims_with_broadcasting(x, y)\n        return (y.tensor - x.tensor).norm(dim=dim)\n\n    def inner(\n        self, u: TangentTensor, v: TangentTensor, keepdim: bool = False, safe_mode: bool = True\n    ) -> Tensor:\n        dim = check_dims_with_broadcasting(u, v)\n        if safe_mode:\n            check_tangent_tensor_positions(u, v)\n\n        return (u.tensor * v.tensor).sum(dim=dim, keepdim=keepdim)\n\n    def euc_to_tangent(self, x: ManifoldTensor, u: ManifoldTensor) -> TangentTensor:\n        dim = check_dims_with_broadcasting(x, u)\n        return TangentTensor(\n            data=u.tensor,\n            manifold_points=x,\n            manifold=self,\n            man_dim=dim,\n        )\n\n    def hyperplane_dists(self, x: ManifoldTensor, z: ManifoldTensor, r: Optional[Tensor]) -> Tensor:\n        if x.man_dim != 1 or z.man_dim != 0:\n            raise ValueError(\n                f\"Expected the manifold dimension of the inputs to be 1 and the manifold \"\n                f\"dimension of the hyperplane orientations to be 0, but got {x.man_dim} and \"\n                f\"{z.man_dim}, respectively\"\n            )\n        if r is None:\n            return matmul(x.tensor, z.tensor)\n        else:\n            return matmul(x.tensor, z.tensor) + r\n\n    def fully_connected(\n        self, x: ManifoldTensor, z: ManifoldTensor, bias: Optional[Tensor]\n    ) -> ManifoldTensor:\n        if z.man_dim != 0:\n            raise ValueError(\n                f\"Expected the manifold dimension of the hyperplane orientations to be 0, but got \"\n                f\"{z.man_dim} instead\"\n            )\n\n        dim_shifted_x_tensor = x.tensor.movedim(source=x.man_dim, destination=-1)\n        dim_shifted_new_tensor = matmul(dim_shifted_x_tensor, z.tensor)\n        if bias is not None:\n            dim_shifted_new_tensor = dim_shifted_new_tensor + bias\n        new_tensor = dim_shifted_new_tensor.movedim(source=-1, destination=x.man_dim)\n\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=x.man_dim)\n\n    def frechet_mean(\n        self,\n        x: ManifoldTensor,\n        batch_dim: Union[int, list[int]] = 0,\n        keepdim: bool = False,\n    ) -> ManifoldTensor:\n        if isinstance(batch_dim, int):\n            batch_dim = [batch_dim]\n\n        if x.man_dim in batch_dim:\n            raise ValueError(\n                f\"Tried to aggregate over dimensions {batch_dim}, but input has manifold \"\n                f\"dimension {x.man_dim} and cannot aggregate over this dimension\"\n            )\n\n        # Output manifold dimension is shifted left for each batch dim that disappears\n        man_dim_shift = sum(bd < x.man_dim for bd in batch_dim)\n        new_man_dim = x.man_dim - man_dim_shift if not keepdim else x.man_dim\n\n        mean_tensor = x.tensor.mean(dim=batch_dim, keepdim=keepdim)\n\n        return ManifoldTensor(data=mean_tensor, manifold=self, man_dim=new_man_dim)\n\n    def midpoint(\n        self,\n        x: ManifoldTensor,\n        batch_dim: Union[int, list[int]] = 0,\n        w: Optional[Tensor] = None,\n        keepdim: bool = False,\n    ) -> ManifoldTensor:\n        return self.frechet_mean(x=x, batch_dim=batch_dim, keepdim=keepdim)\n\n    def frechet_variance(\n        self,\n        x: ManifoldTensor,\n        mu: Optional[ManifoldTensor] = None,\n        batch_dim: Union[int, list[int]] = -1,\n        keepdim: bool = False,\n    ) -> Tensor:\n        if isinstance(batch_dim, int):\n            batch_dim = [batch_dim]\n\n        if mu is None:\n            return var(x.tensor, dim=batch_dim, keepdim=keepdim)\n        else:\n            if x.dim() != mu.dim():\n                for bd in sorted(batch_dim):\n                    mu.man_dim += 1 if bd <= mu.man_dim else 0\n                    mu.tensor = mu.tensor.unsqueeze(bd)\n            if mu.man_dim != x.man_dim:\n                raise ValueError(\"Input tensor and mean do not have matching manifold dimensions\")\n            n = 1\n            for bd in batch_dim:\n                n *= x.size(dim=bd)\n            return (x.tensor - mu.tensor).pow(2).sum(dim=batch_dim, keepdim=keepdim) / (n - 1)\n\n    def construct_dl_parameters(\n        self, in_features: int, out_features: int, bias: bool = True\n    ) -> tuple[ManifoldParameter, Optional[Parameter]]:\n        weight = ManifoldParameter(\n            data=empty(in_features, out_features),\n            manifold=self,\n            man_dim=0,\n        )\n\n        if bias:\n            b = Parameter(data=empty(out_features))\n        else:\n            b = None\n\n        return weight, b\n\n    def reset_parameters(self, weight: ManifoldParameter, bias: Parameter) -> None:\n        kaiming_uniform_(weight.tensor, a=sqrt(5))\n        if bias is not None:\n            fan_in, _ = _calculate_fan_in_and_fan_out(weight.tensor)\n            bound = 1 / sqrt(fan_in) if fan_in > 0 else 0\n            uniform_(bias, -bound, bound)\n\n    def unfold(\n        self,\n        input: ManifoldTensor,\n        kernel_size: _size_2_t,\n        dilation: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        stride: _size_2_t = 1,\n    ) -> ManifoldTensor:\n        new_tensor = unfold(\n            input=input.tensor,\n            kernel_size=kernel_size,\n            dilation=dilation,\n            padding=padding,\n            stride=stride,\n        )\n        return ManifoldTensor(data=new_tensor, manifold=input.manifold, man_dim=1)\n\n    def flatten(self, x: ManifoldTensor, start_dim: int = 1, end_dim: int = -1) -> ManifoldTensor:\n        \"\"\"Flattens a manifold tensor by reshaping it. If start_dim or end_dim are passed,\n        only dimensions starting with start_dim and ending with end_dim are flattend.\n\n        Updates the manifold dimension if necessary.\n\n        \"\"\"\n        start_dim = x.dim() + start_dim if start_dim < 0 else start_dim\n        end_dim = x.dim() + end_dim if end_dim < 0 else end_dim\n\n        # Get the range of dimensions to flatten.\n        dimensions_to_flatten = x.shape[start_dim + 1 : end_dim + 1]\n\n        # Get the new manifold dimension.\n        if start_dim <= x.man_dim and end_dim >= x.man_dim:\n            man_dim = start_dim\n        elif end_dim <= x.man_dim:\n            man_dim = x.man_dim - len(dimensions_to_flatten)\n        else:\n            man_dim = x.man_dim\n\n        # Flatten the tensor and return the new instance.\n        flattened = torch.flatten(\n            input=x.tensor,\n            start_dim=start_dim,\n            end_dim=end_dim,\n        )\n        return ManifoldTensor(data=flattened, manifold=x.manifold, man_dim=man_dim)", "\n\nclass Euclidean(Manifold):\n    def __init__(self):\n        super(Euclidean, self).__init__()\n\n    def project(self, x: ManifoldTensor, eps: float = -1.0) -> ManifoldTensor:\n        return x\n\n    def expmap(self, v: TangentTensor) -> ManifoldTensor:\n        if v.manifold_points is None:\n            new_tensor = v.tensor\n        else:\n            new_tensor = v.manifold_points.tensor + v.tensor\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=v.broadcasted_man_dim)\n\n    def logmap(self, x: Optional[ManifoldTensor], y: ManifoldTensor) -> TangentTensor:\n        if x is None:\n            dim = y.man_dim\n            new_tensor = y.tensor\n        else:\n            dim = check_dims_with_broadcasting(x, y)\n            new_tensor = y.tensor - x.tensor\n        return TangentTensor(data=new_tensor, manifold_points=x, manifold=self, man_dim=dim)\n\n    def transp(self, v: TangentTensor, y: ManifoldTensor) -> TangentTensor:\n        dim = check_dims_with_broadcasting(v, y)\n        output_shape = broadcast_shapes(v.size(), y.size())\n        new_tensor = v.tensor.broadcast_to(output_shape)\n        return TangentTensor(data=new_tensor, manifold_points=y, manifold=self, man_dim=dim)\n\n    def dist(self, x: ManifoldTensor, y: ManifoldTensor) -> Tensor:\n        dim = check_dims_with_broadcasting(x, y)\n        return (y.tensor - x.tensor).norm(dim=dim)\n\n    def inner(\n        self, u: TangentTensor, v: TangentTensor, keepdim: bool = False, safe_mode: bool = True\n    ) -> Tensor:\n        dim = check_dims_with_broadcasting(u, v)\n        if safe_mode:\n            check_tangent_tensor_positions(u, v)\n\n        return (u.tensor * v.tensor).sum(dim=dim, keepdim=keepdim)\n\n    def euc_to_tangent(self, x: ManifoldTensor, u: ManifoldTensor) -> TangentTensor:\n        dim = check_dims_with_broadcasting(x, u)\n        return TangentTensor(\n            data=u.tensor,\n            manifold_points=x,\n            manifold=self,\n            man_dim=dim,\n        )\n\n    def hyperplane_dists(self, x: ManifoldTensor, z: ManifoldTensor, r: Optional[Tensor]) -> Tensor:\n        if x.man_dim != 1 or z.man_dim != 0:\n            raise ValueError(\n                f\"Expected the manifold dimension of the inputs to be 1 and the manifold \"\n                f\"dimension of the hyperplane orientations to be 0, but got {x.man_dim} and \"\n                f\"{z.man_dim}, respectively\"\n            )\n        if r is None:\n            return matmul(x.tensor, z.tensor)\n        else:\n            return matmul(x.tensor, z.tensor) + r\n\n    def fully_connected(\n        self, x: ManifoldTensor, z: ManifoldTensor, bias: Optional[Tensor]\n    ) -> ManifoldTensor:\n        if z.man_dim != 0:\n            raise ValueError(\n                f\"Expected the manifold dimension of the hyperplane orientations to be 0, but got \"\n                f\"{z.man_dim} instead\"\n            )\n\n        dim_shifted_x_tensor = x.tensor.movedim(source=x.man_dim, destination=-1)\n        dim_shifted_new_tensor = matmul(dim_shifted_x_tensor, z.tensor)\n        if bias is not None:\n            dim_shifted_new_tensor = dim_shifted_new_tensor + bias\n        new_tensor = dim_shifted_new_tensor.movedim(source=-1, destination=x.man_dim)\n\n        return ManifoldTensor(data=new_tensor, manifold=self, man_dim=x.man_dim)\n\n    def frechet_mean(\n        self,\n        x: ManifoldTensor,\n        batch_dim: Union[int, list[int]] = 0,\n        keepdim: bool = False,\n    ) -> ManifoldTensor:\n        if isinstance(batch_dim, int):\n            batch_dim = [batch_dim]\n\n        if x.man_dim in batch_dim:\n            raise ValueError(\n                f\"Tried to aggregate over dimensions {batch_dim}, but input has manifold \"\n                f\"dimension {x.man_dim} and cannot aggregate over this dimension\"\n            )\n\n        # Output manifold dimension is shifted left for each batch dim that disappears\n        man_dim_shift = sum(bd < x.man_dim for bd in batch_dim)\n        new_man_dim = x.man_dim - man_dim_shift if not keepdim else x.man_dim\n\n        mean_tensor = x.tensor.mean(dim=batch_dim, keepdim=keepdim)\n\n        return ManifoldTensor(data=mean_tensor, manifold=self, man_dim=new_man_dim)\n\n    def midpoint(\n        self,\n        x: ManifoldTensor,\n        batch_dim: Union[int, list[int]] = 0,\n        w: Optional[Tensor] = None,\n        keepdim: bool = False,\n    ) -> ManifoldTensor:\n        return self.frechet_mean(x=x, batch_dim=batch_dim, keepdim=keepdim)\n\n    def frechet_variance(\n        self,\n        x: ManifoldTensor,\n        mu: Optional[ManifoldTensor] = None,\n        batch_dim: Union[int, list[int]] = -1,\n        keepdim: bool = False,\n    ) -> Tensor:\n        if isinstance(batch_dim, int):\n            batch_dim = [batch_dim]\n\n        if mu is None:\n            return var(x.tensor, dim=batch_dim, keepdim=keepdim)\n        else:\n            if x.dim() != mu.dim():\n                for bd in sorted(batch_dim):\n                    mu.man_dim += 1 if bd <= mu.man_dim else 0\n                    mu.tensor = mu.tensor.unsqueeze(bd)\n            if mu.man_dim != x.man_dim:\n                raise ValueError(\"Input tensor and mean do not have matching manifold dimensions\")\n            n = 1\n            for bd in batch_dim:\n                n *= x.size(dim=bd)\n            return (x.tensor - mu.tensor).pow(2).sum(dim=batch_dim, keepdim=keepdim) / (n - 1)\n\n    def construct_dl_parameters(\n        self, in_features: int, out_features: int, bias: bool = True\n    ) -> tuple[ManifoldParameter, Optional[Parameter]]:\n        weight = ManifoldParameter(\n            data=empty(in_features, out_features),\n            manifold=self,\n            man_dim=0,\n        )\n\n        if bias:\n            b = Parameter(data=empty(out_features))\n        else:\n            b = None\n\n        return weight, b\n\n    def reset_parameters(self, weight: ManifoldParameter, bias: Parameter) -> None:\n        kaiming_uniform_(weight.tensor, a=sqrt(5))\n        if bias is not None:\n            fan_in, _ = _calculate_fan_in_and_fan_out(weight.tensor)\n            bound = 1 / sqrt(fan_in) if fan_in > 0 else 0\n            uniform_(bias, -bound, bound)\n\n    def unfold(\n        self,\n        input: ManifoldTensor,\n        kernel_size: _size_2_t,\n        dilation: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        stride: _size_2_t = 1,\n    ) -> ManifoldTensor:\n        new_tensor = unfold(\n            input=input.tensor,\n            kernel_size=kernel_size,\n            dilation=dilation,\n            padding=padding,\n            stride=stride,\n        )\n        return ManifoldTensor(data=new_tensor, manifold=input.manifold, man_dim=1)\n\n    def flatten(self, x: ManifoldTensor, start_dim: int = 1, end_dim: int = -1) -> ManifoldTensor:\n        \"\"\"Flattens a manifold tensor by reshaping it. If start_dim or end_dim are passed,\n        only dimensions starting with start_dim and ending with end_dim are flattend.\n\n        Updates the manifold dimension if necessary.\n\n        \"\"\"\n        start_dim = x.dim() + start_dim if start_dim < 0 else start_dim\n        end_dim = x.dim() + end_dim if end_dim < 0 else end_dim\n\n        # Get the range of dimensions to flatten.\n        dimensions_to_flatten = x.shape[start_dim + 1 : end_dim + 1]\n\n        # Get the new manifold dimension.\n        if start_dim <= x.man_dim and end_dim >= x.man_dim:\n            man_dim = start_dim\n        elif end_dim <= x.man_dim:\n            man_dim = x.man_dim - len(dimensions_to_flatten)\n        else:\n            man_dim = x.man_dim\n\n        # Flatten the tensor and return the new instance.\n        flattened = torch.flatten(\n            input=x.tensor,\n            start_dim=start_dim,\n            end_dim=end_dim,\n        )\n        return ManifoldTensor(data=flattened, manifold=x.manifold, man_dim=man_dim)", ""]}
{"filename": "hypll/manifolds/euclidean/__init__.py", "chunked_list": ["from .manifold import Euclidean\n"]}
{"filename": "hypll/nn/__init__.py", "chunked_list": ["from .modules.activation import HReLU\nfrom .modules.batchnorm import HBatchNorm, HBatchNorm2d\nfrom .modules.change_manifold import ChangeManifold\nfrom .modules.container import TangentSequential\nfrom .modules.convolution import HConvolution2d\nfrom .modules.embedding import HEmbedding\nfrom .modules.flatten import HFlatten\nfrom .modules.linear import HLinear\nfrom .modules.pooling import HAvgPool2d, HMaxPool2d\n", "from .modules.pooling import HAvgPool2d, HMaxPool2d\n"]}
{"filename": "hypll/nn/modules/pooling.py", "chunked_list": ["from functools import partial\nfrom typing import Optional\n\nfrom torch.nn import Module\nfrom torch.nn.common_types import _size_2_t, _size_any_t\nfrom torch.nn.functional import max_pool2d\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor\nfrom hypll.utils.layer_utils import (", "from hypll.tensors import ManifoldTensor\nfrom hypll.utils.layer_utils import (\n    check_if_man_dims_match,\n    check_if_manifolds_match,\n    op_in_tangent_space,\n)\n\n\nclass HAvgPool2d(Module):\n    def __init__(\n        self,\n        kernel_size: _size_2_t,\n        manifold: Manifold,\n        stride: Optional[_size_2_t] = None,\n        padding: _size_2_t = 0,\n        use_midpoint: bool = False,\n    ):\n        super().__init__()\n        self.kernel_size = (\n            kernel_size\n            if isinstance(kernel_size, tuple) and len(kernel_size) == 2\n            else (kernel_size, kernel_size)\n        )\n        self.manifold = manifold\n        self.stride = stride if (stride is not None) else self.kernel_size\n        self.padding = (\n            padding if isinstance(padding, tuple) and len(padding) == 2 else (padding, padding)\n        )\n        self.use_midpoint = use_midpoint\n\n    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=input)\n        check_if_man_dims_match(layer=self, man_dim=1, input=input)\n\n        batch_size, channels, height, width = input.size()\n        out_height = int((height + 2 * self.padding[0] - self.kernel_size[0]) / self.stride[0] + 1)\n        out_width = int((width + 2 * self.padding[1] - self.kernel_size[1]) / self.stride[1] + 1)\n\n        unfolded_input = self.manifold.unfold(\n            input=input,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            stride=self.stride,\n        )\n        per_kernel_view = unfolded_input.tensor.view(\n            batch_size,\n            channels,\n            self.kernel_size[0] * self.kernel_size[1],\n            unfolded_input.size(-1),\n        )\n\n        x = ManifoldTensor(data=per_kernel_view, manifold=self.manifold, man_dim=1)\n\n        if self.use_midpoint:\n            aggregates = self.manifold.midpoint(x=x, batch_dim=2)\n\n        else:\n            aggregates = self.manifold.frechet_mean(x=x, batch_dim=2)\n\n        return ManifoldTensor(\n            data=aggregates.tensor.reshape(batch_size, channels, out_height, out_width),\n            manifold=self.manifold,\n            man_dim=1,\n        )", "class HAvgPool2d(Module):\n    def __init__(\n        self,\n        kernel_size: _size_2_t,\n        manifold: Manifold,\n        stride: Optional[_size_2_t] = None,\n        padding: _size_2_t = 0,\n        use_midpoint: bool = False,\n    ):\n        super().__init__()\n        self.kernel_size = (\n            kernel_size\n            if isinstance(kernel_size, tuple) and len(kernel_size) == 2\n            else (kernel_size, kernel_size)\n        )\n        self.manifold = manifold\n        self.stride = stride if (stride is not None) else self.kernel_size\n        self.padding = (\n            padding if isinstance(padding, tuple) and len(padding) == 2 else (padding, padding)\n        )\n        self.use_midpoint = use_midpoint\n\n    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=input)\n        check_if_man_dims_match(layer=self, man_dim=1, input=input)\n\n        batch_size, channels, height, width = input.size()\n        out_height = int((height + 2 * self.padding[0] - self.kernel_size[0]) / self.stride[0] + 1)\n        out_width = int((width + 2 * self.padding[1] - self.kernel_size[1]) / self.stride[1] + 1)\n\n        unfolded_input = self.manifold.unfold(\n            input=input,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            stride=self.stride,\n        )\n        per_kernel_view = unfolded_input.tensor.view(\n            batch_size,\n            channels,\n            self.kernel_size[0] * self.kernel_size[1],\n            unfolded_input.size(-1),\n        )\n\n        x = ManifoldTensor(data=per_kernel_view, manifold=self.manifold, man_dim=1)\n\n        if self.use_midpoint:\n            aggregates = self.manifold.midpoint(x=x, batch_dim=2)\n\n        else:\n            aggregates = self.manifold.frechet_mean(x=x, batch_dim=2)\n\n        return ManifoldTensor(\n            data=aggregates.tensor.reshape(batch_size, channels, out_height, out_width),\n            manifold=self.manifold,\n            man_dim=1,\n        )", "\n\nclass _HMaxPoolNd(Module):\n    def __init__(\n        self,\n        kernel_size: _size_any_t,\n        manifold: Manifold,\n        stride: Optional[_size_any_t] = None,\n        padding: _size_any_t = 0,\n        dilation: _size_any_t = 1,\n        return_indices: bool = False,\n        ceil_mode: bool = False,\n    ) -> None:\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.manifold = manifold\n        self.stride = stride if (stride is not None) else kernel_size\n        self.padding = padding\n        self.dilation = dilation\n        self.return_indices = return_indices\n        self.ceil_mode = ceil_mode", "\n\nclass HMaxPool2d(_HMaxPoolNd):\n    kernel_size: _size_2_t\n    stride: _size_2_t\n    padding: _size_2_t\n    dilation: _size_2_t\n\n    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=input)\n\n        # TODO: check if defining this partial func each forward pass is slow.\n        # If it is, put this stuff inside the init or add kwargs to op_in_tangent_space.\n        max_pool2d_partial = partial(\n            max_pool2d,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            ceil_mode=self.ceil_mode,\n            return_indices=self.return_indices,\n        )\n        return op_in_tangent_space(op=max_pool2d_partial, manifold=self.manifold, input=input)", ""]}
{"filename": "hypll/nn/modules/change_manifold.py", "chunked_list": ["from torch import Tensor\nfrom torch.nn import Module\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor, TangentTensor\n\n\nclass ChangeManifold(Module):\n    \"\"\"Changes the manifold of the input manifold tensor to the target manifold.\n", "    \"\"\"Changes the manifold of the input manifold tensor to the target manifold.\n\n    Attributes:\n        target_manifold:\n            Manifold the output tensor should be on.\n\n    \"\"\"\n\n    def __init__(self, target_manifold: Manifold):\n        super(ChangeManifold, self).__init__()", "    def __init__(self, target_manifold: Manifold):\n        super(ChangeManifold, self).__init__()\n        self.target_manifold = target_manifold\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        \"\"\"Changes the manifold of the input tensor to self.target_manifold.\n\n        By default, applies logmap and expmap at the origin to switch between\n        manifold. Applies shortcuts if possible.\n", "        manifold. Applies shortcuts if possible.\n\n        Args:\n            x:\n                Input manifold tensor.\n\n        Returns:\n            Tensor on the target manifold.\n\n        \"\"\"", "\n        \"\"\"\n\n        match (x.manifold, self.target_manifold):\n            # TODO(Philipp, 05/23): Apply shortcuts where possible: For example,\n            # case (PoincareBall(), PoincareBall()): ...\n            #\n            # By default resort to logmap + expmap at the origin.\n            case _:\n                tangent_tensor = x.manifold.logmap(None, x)", "            case _:\n                tangent_tensor = x.manifold.logmap(None, x)\n                return self.target_manifold.expmap(tangent_tensor)\n"]}
{"filename": "hypll/nn/modules/embedding.py", "chunked_list": ["from torch import Tensor, empty, no_grad, normal, ones_like, zeros_like\nfrom torch.nn import Module\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldParameter, ManifoldTensor, TangentTensor\n\n\nclass HEmbedding(Module):\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n        manifold: Manifold,\n    ) -> None:\n        super(HEmbedding, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.manifold = manifold\n\n        self.weight = ManifoldParameter(\n            data=empty((num_embeddings, embedding_dim)), manifold=manifold, man_dim=-1\n        )\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        with no_grad():\n            new_weight = normal(\n                mean=zeros_like(self.weight.tensor),\n                std=ones_like(self.weight.tensor),\n            )\n            new_weight = TangentTensor(\n                data=new_weight,\n                manifold_points=None,\n                manifold=self.manifold,\n                man_dim=-1,\n            )\n            self.weight.copy_(self.manifold.expmap(new_weight).tensor)\n\n    def forward(self, input: Tensor) -> ManifoldTensor:\n        return self.weight[input]", ""]}
{"filename": "hypll/nn/modules/convolution.py", "chunked_list": ["from torch.nn import Module\nfrom torch.nn.common_types import _size_1_t, _size_2_t\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor\nfrom hypll.utils.layer_utils import check_if_man_dims_match, check_if_manifolds_match\n\n\nclass HConvolution2d(Module):\n    \"\"\"Applies a 2D convolution over a hyperbolic input signal.\n\n    Attributes:\n        in_channels:\n            Number of channels in the input image.\n        out_channels:\n            Number of channels produced by the convolution.\n        kernel_size:\n            Size of the convolving kernel.\n        manifold:\n            Hyperbolic manifold of the tensors.\n        bias:\n            If True, adds a learnable bias to the output. Default: True\n        stride:\n            Stride of the convolution. Default: 1\n        padding:\n            Padding added to all four sides of the input. Default: 0\n        id_init:\n            Use identity initialization (True) if appropriate or use HNN++ initialization (False).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        manifold: Manifold,\n        bias: bool = True,\n        stride: int = 1,\n        padding: int = 0,\n        id_init: bool = True,\n    ) -> None:\n        super(HConvolution2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = (\n            kernel_size\n            if isinstance(kernel_size, tuple) and len(kernel_size) == 2\n            else (kernel_size, kernel_size)\n        )\n        self.kernel_vol = self.kernel_size[0] * self.kernel_size[1]\n        self.manifold = manifold\n        self.stride = stride\n        self.padding = padding\n        self.id_init = id_init\n        self.has_bias = bias\n\n        self.weights, self.bias = self.manifold.construct_dl_parameters(\n            in_features=self.kernel_vol * in_channels,\n            out_features=out_channels,\n            bias=self.has_bias,\n        )\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        \"\"\"Resets parameter weights based on the manifold.\"\"\"\n        self.manifold.reset_parameters(weight=self.weights, bias=self.bias)\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        \"\"\"Does a forward pass of the 2D convolutional layer.\n\n        Args:\n            x:\n                Manifold tensor of shape (B, C_in, H, W) with manifold dimension 1.\n\n        Returns:\n            Manifold tensor of shape (B, C_in, H_out, W_out) with manifold dimension 1.\n\n        Raises:\n            ValueError: If the manifolds or manifold dimensions don't match.\n\n        \"\"\"\n        check_if_manifolds_match(layer=self, input=x)\n        check_if_man_dims_match(layer=self, man_dim=1, input=x)\n\n        batch_size, height, width = x.size(0), x.size(2), x.size(3)\n        out_height = _output_side_length(\n            input_side_length=height,\n            kernel_size=self.kernel_size[0],\n            padding=self.padding,\n            stride=self.stride,\n        )\n        out_width = _output_side_length(\n            input_side_length=width,\n            kernel_size=self.kernel_size[1],\n            padding=self.padding,\n            stride=self.stride,\n        )\n\n        x = self.manifold.unfold(\n            input=x,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            stride=self.stride,\n        )\n        x = self.manifold.fully_connected(x=x, z=self.weights, bias=self.bias)\n        x = ManifoldTensor(\n            data=x.tensor.reshape(batch_size, self.out_channels, out_height, out_width),\n            manifold=x.manifold,\n            man_dim=1,\n        )\n        return x", "class HConvolution2d(Module):\n    \"\"\"Applies a 2D convolution over a hyperbolic input signal.\n\n    Attributes:\n        in_channels:\n            Number of channels in the input image.\n        out_channels:\n            Number of channels produced by the convolution.\n        kernel_size:\n            Size of the convolving kernel.\n        manifold:\n            Hyperbolic manifold of the tensors.\n        bias:\n            If True, adds a learnable bias to the output. Default: True\n        stride:\n            Stride of the convolution. Default: 1\n        padding:\n            Padding added to all four sides of the input. Default: 0\n        id_init:\n            Use identity initialization (True) if appropriate or use HNN++ initialization (False).\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        manifold: Manifold,\n        bias: bool = True,\n        stride: int = 1,\n        padding: int = 0,\n        id_init: bool = True,\n    ) -> None:\n        super(HConvolution2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = (\n            kernel_size\n            if isinstance(kernel_size, tuple) and len(kernel_size) == 2\n            else (kernel_size, kernel_size)\n        )\n        self.kernel_vol = self.kernel_size[0] * self.kernel_size[1]\n        self.manifold = manifold\n        self.stride = stride\n        self.padding = padding\n        self.id_init = id_init\n        self.has_bias = bias\n\n        self.weights, self.bias = self.manifold.construct_dl_parameters(\n            in_features=self.kernel_vol * in_channels,\n            out_features=out_channels,\n            bias=self.has_bias,\n        )\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        \"\"\"Resets parameter weights based on the manifold.\"\"\"\n        self.manifold.reset_parameters(weight=self.weights, bias=self.bias)\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        \"\"\"Does a forward pass of the 2D convolutional layer.\n\n        Args:\n            x:\n                Manifold tensor of shape (B, C_in, H, W) with manifold dimension 1.\n\n        Returns:\n            Manifold tensor of shape (B, C_in, H_out, W_out) with manifold dimension 1.\n\n        Raises:\n            ValueError: If the manifolds or manifold dimensions don't match.\n\n        \"\"\"\n        check_if_manifolds_match(layer=self, input=x)\n        check_if_man_dims_match(layer=self, man_dim=1, input=x)\n\n        batch_size, height, width = x.size(0), x.size(2), x.size(3)\n        out_height = _output_side_length(\n            input_side_length=height,\n            kernel_size=self.kernel_size[0],\n            padding=self.padding,\n            stride=self.stride,\n        )\n        out_width = _output_side_length(\n            input_side_length=width,\n            kernel_size=self.kernel_size[1],\n            padding=self.padding,\n            stride=self.stride,\n        )\n\n        x = self.manifold.unfold(\n            input=x,\n            kernel_size=self.kernel_size,\n            padding=self.padding,\n            stride=self.stride,\n        )\n        x = self.manifold.fully_connected(x=x, z=self.weights, bias=self.bias)\n        x = ManifoldTensor(\n            data=x.tensor.reshape(batch_size, self.out_channels, out_height, out_width),\n            manifold=x.manifold,\n            man_dim=1,\n        )\n        return x", "\n\ndef _output_side_length(\n    input_side_length: int, kernel_size: _size_1_t, padding: int, stride: int\n) -> int:\n    \"\"\"Calculates the output side length of the kernel.\n\n    Based on https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html.\n\n    \"\"\"\n    if kernel_size > input_side_length:\n        raise RuntimeError(\n            f\"Encountered invalid kernel size {kernel_size} \"\n            f\"larger than input side length {input_side_length}\"\n        )\n    if stride > input_side_length:\n        raise RuntimeError(\n            f\"Encountered invalid stride {stride} \"\n            f\"larger than input side length {input_side_length}\"\n        )\n    return 1 + (input_side_length + 2 * padding - (kernel_size - 1) - 1) // stride", ""]}
{"filename": "hypll/nn/modules/fold.py", "chunked_list": ["from torch.nn import Module\nfrom torch.nn.common_types import _size_2_t\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor\nfrom hypll.utils.layer_utils import check_if_man_dims_match, check_if_manifolds_match\n\n\nclass HUnfold(Module):\n    def __init__(\n        self,\n        kernel_size: _size_2_t,\n        manifold: Manifold,\n        dilation: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        stride: _size_2_t = 1,\n    ) -> None:\n        self.kernel_size = kernel_size\n        self.manifold = manifold\n        self.dilation = dilation\n        self.padding = padding\n        self.stride = stride\n\n    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=input)\n        check_if_man_dims_match(layer=self, man_dim=1, input=input)\n        return self.manifold.unfold(\n            input=input,\n            kernel_size=self.kernel_size,\n            dilation=self.dilation,\n            padding=self.padding,\n            stride=self.stride,\n        )", "class HUnfold(Module):\n    def __init__(\n        self,\n        kernel_size: _size_2_t,\n        manifold: Manifold,\n        dilation: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        stride: _size_2_t = 1,\n    ) -> None:\n        self.kernel_size = kernel_size\n        self.manifold = manifold\n        self.dilation = dilation\n        self.padding = padding\n        self.stride = stride\n\n    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=input)\n        check_if_man_dims_match(layer=self, man_dim=1, input=input)\n        return self.manifold.unfold(\n            input=input,\n            kernel_size=self.kernel_size,\n            dilation=self.dilation,\n            padding=self.padding,\n            stride=self.stride,\n        )", ""]}
{"filename": "hypll/nn/modules/container.py", "chunked_list": ["from torch.nn import Module, Sequential\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor\nfrom hypll.utils.layer_utils import check_if_manifolds_match\n\n\nclass TangentSequential(Module):\n    def __init__(self, seq: Sequential, manifold: Manifold) -> None:\n        super(TangentSequential, self).__init__()\n        self.seq = seq\n        self.manifold = manifold\n\n    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=input)\n        man_dim = input.man_dim\n\n        input = self.manifold.logmap(x=None, y=input)\n        for module in self.seq:\n            input.tensor = module(input.tensor)\n        return self.manifold.expmap(input)", ""]}
{"filename": "hypll/nn/modules/__init__.py", "chunked_list": [""]}
{"filename": "hypll/nn/modules/linear.py", "chunked_list": ["from torch.nn import Module\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor\nfrom hypll.utils.layer_utils import check_if_man_dims_match, check_if_manifolds_match\n\n\nclass HLinear(Module):\n    \"\"\"Poincare fully connected linear layer\"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        manifold: Manifold,\n        bias: bool = True,\n    ) -> None:\n        super(HLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.manifold = manifold\n        self.has_bias = bias\n\n        # TODO: torch stores weights transposed supposedly due to efficiency\n        # https://discuss.pytorch.org/t/why-does-the-linear-module-seems-to-do-unnecessary-transposing/6277/7\n        # We may want to do the same\n        self.z, self.bias = self.manifold.construct_dl_parameters(\n            in_features=in_features, out_features=out_features, bias=self.has_bias\n        )\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        self.manifold.reset_parameters(self.z, self.bias)\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=x)\n        check_if_man_dims_match(layer=self, man_dim=-1, input=x)\n        return self.manifold.fully_connected(x=x, z=self.z, bias=self.bias)", ""]}
{"filename": "hypll/nn/modules/batchnorm.py", "chunked_list": ["from torch import tensor, zeros\nfrom torch.nn import Module, Parameter\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor, TangentTensor\nfrom hypll.utils.layer_utils import check_if_manifolds_match\n\n\nclass HBatchNorm(Module):\n    \"\"\"\n    Basic implementation of hyperbolic batch normalization.\n\n    Based on:\n        https://arxiv.org/abs/2003.00335\n    \"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        manifold: Manifold,\n        use_midpoint: bool = False,\n    ) -> None:\n        super(HBatchNorm, self).__init__()\n        self.features = features\n        self.manifold = manifold\n        self.use_midpoint = use_midpoint\n\n        # TODO: Store bias on manifold\n        self.bias = Parameter(zeros(features))\n        self.weight = Parameter(tensor(1.0))\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=x)\n        bias_on_manifold = self.manifold.expmap(\n            v=TangentTensor(data=self.bias, manifold_points=None, manifold=self.manifold)\n        )\n\n        if self.use_midpoint:\n            input_mean = self.manifold.midpoint(x=x, batch_dim=0)\n        else:\n            input_mean = self.manifold.frechet_mean(x=x, batch_dim=0)\n\n        input_var = self.manifold.frechet_variance(x=x, mu=input_mean, batch_dim=0)\n\n        input_logm = self.manifold.transp(\n            v=self.manifold.logmap(input_mean, x),\n            y=bias_on_manifold,\n        )\n\n        input_logm.tensor = (self.weight / (input_var + 1e-6)).sqrt() * input_logm.tensor\n\n        output = self.manifold.expmap(input_logm)\n\n        return output", "class HBatchNorm(Module):\n    \"\"\"\n    Basic implementation of hyperbolic batch normalization.\n\n    Based on:\n        https://arxiv.org/abs/2003.00335\n    \"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        manifold: Manifold,\n        use_midpoint: bool = False,\n    ) -> None:\n        super(HBatchNorm, self).__init__()\n        self.features = features\n        self.manifold = manifold\n        self.use_midpoint = use_midpoint\n\n        # TODO: Store bias on manifold\n        self.bias = Parameter(zeros(features))\n        self.weight = Parameter(tensor(1.0))\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=x)\n        bias_on_manifold = self.manifold.expmap(\n            v=TangentTensor(data=self.bias, manifold_points=None, manifold=self.manifold)\n        )\n\n        if self.use_midpoint:\n            input_mean = self.manifold.midpoint(x=x, batch_dim=0)\n        else:\n            input_mean = self.manifold.frechet_mean(x=x, batch_dim=0)\n\n        input_var = self.manifold.frechet_variance(x=x, mu=input_mean, batch_dim=0)\n\n        input_logm = self.manifold.transp(\n            v=self.manifold.logmap(input_mean, x),\n            y=bias_on_manifold,\n        )\n\n        input_logm.tensor = (self.weight / (input_var + 1e-6)).sqrt() * input_logm.tensor\n\n        output = self.manifold.expmap(input_logm)\n\n        return output", "\n\nclass HBatchNorm2d(Module):\n    \"\"\"\n    2D implementation of hyperbolic batch normalization.\n\n    Based on:\n        https://arxiv.org/abs/2003.00335\n    \"\"\"\n\n    def __init__(\n        self,\n        features: int,\n        manifold: Manifold,\n        use_midpoint: bool = False,\n    ) -> None:\n        super(HBatchNorm2d, self).__init__()\n        self.features = features\n        self.manifold = manifold\n        self.use_midpoint = use_midpoint\n\n        self.norm = HBatchNorm(\n            features=features,\n            manifold=manifold,\n            use_midpoint=use_midpoint,\n        )\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=x)\n        batch_size, height, width = x.size(0), x.size(2), x.size(3)\n        flat_x = ManifoldTensor(\n            data=x.tensor.permute(0, 2, 3, 1).flatten(start_dim=0, end_dim=2),\n            manifold=x.manifold,\n            man_dim=-1,\n        )\n        flat_x = self.norm(flat_x)\n        new_tensor = flat_x.tensor.reshape(batch_size, height, width, self.features).permute(\n            0, 3, 1, 2\n        )\n        return ManifoldTensor(data=new_tensor, manifold=x.manifold, man_dim=1)", ""]}
{"filename": "hypll/nn/modules/activation.py", "chunked_list": ["from torch.nn import Module\nfrom torch.nn.functional import relu\n\nfrom hypll.manifolds import Manifold\nfrom hypll.tensors import ManifoldTensor\nfrom hypll.utils.layer_utils import check_if_manifolds_match, op_in_tangent_space\n\n\nclass HReLU(Module):\n    def __init__(self, manifold: Manifold) -> None:\n        super(HReLU, self).__init__()\n        self.manifold = manifold\n\n    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=input)\n        return op_in_tangent_space(\n            op=relu,\n            manifold=self.manifold,\n            input=input,\n        )", "class HReLU(Module):\n    def __init__(self, manifold: Manifold) -> None:\n        super(HReLU, self).__init__()\n        self.manifold = manifold\n\n    def forward(self, input: ManifoldTensor) -> ManifoldTensor:\n        check_if_manifolds_match(layer=self, input=input)\n        return op_in_tangent_space(\n            op=relu,\n            manifold=self.manifold,\n            input=input,\n        )", ""]}
{"filename": "hypll/nn/modules/flatten.py", "chunked_list": ["from torch.nn import Module, functional\n\nfrom hypll.tensors import ManifoldTensor\n\n\nclass HFlatten(Module):\n    \"\"\"Flattens a contiguous range of dims into a tensor.\n\n    Attributes:\n        start_dim:\n            First dimension to flatten (default = 1).\n        end_dim:\n            Last dimension to flatten (default = -1).\n\n    \"\"\"\n\n    def __init__(self, start_dim: int = 1, end_dim: int = -1):\n        super(HFlatten, self).__init__()\n        self.start_dim = start_dim\n        self.end_dim = end_dim\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        \"\"\"Flattens the manifold input tensor.\"\"\"\n        return x.flatten(start_dim=self.start_dim, end_dim=self.end_dim)", ""]}
{"filename": "tutorials/cifar10_resnet_tutorial.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\"\"\"\nTraining a Poincare ResNet\n==========================\n\nThis is an implementation based on the Poincare Resnet paper, which can be found at:\n\n- https://arxiv.org/abs/2303.14027\n\nDue to the complexity of hyperbolic operations we strongly advise to only run this tutorial with a ", "\nDue to the complexity of hyperbolic operations we strongly advise to only run this tutorial with a \nGPU.\n\nWe will perform the following steps in order:\n\n1. Define a hyperbolic manifold\n2. Load and normalize the CIFAR10 training and test datasets using ``torchvision``\n3. Define a Poincare ResNet\n4. Define a loss function and optimizer", "3. Define a Poincare ResNet\n4. Define a loss function and optimizer\n5. Train the network on the training data\n6. Test the network on the test data\n\n\"\"\"\n\n##############################\n# 0. Grab the available device\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^", "# 0. Grab the available device\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n#############################\n# 1. Define the Poincare ball\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^", "# 1. Define the Poincare ball\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\n# Making the curvature a learnable parameter is usually suboptimal but can\n# make training smoother. An initial curvature of 0.1 has also been shown\n# to help during training.\nmanifold = PoincareBall(c=Curvature(value=0.1, requires_grad=True))\n", "manifold = PoincareBall(c=Curvature(value=0.1, requires_grad=True))\n\n\n###############################\n# 2. Load and normalize CIFAR10\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nimport torchvision\nimport torchvision.transforms as transforms\n", "import torchvision.transforms as transforms\n\n########################################################################\n# .. note::\n#     If running on Windows and you get a BrokenPipeError, try setting\n#     the num_worker of torch.utils.data.DataLoader() to 0.\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),", "    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\n\nbatch_size = 128\n\ntrainset = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=transform", "trainset = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=transform\n)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n)\n\ntestset = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=False, download=True, transform=transform\n)", "    root=\"./data\", train=False, download=True, transform=transform\n)\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size=batch_size, shuffle=False, num_workers=2\n)\n\n\n###############################\n# 3. Define a Poincare ResNet\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^", "# 3. Define a Poincare ResNet\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# This implementation is based on the Poincare ResNet paper, which can\n# be found at https://arxiv.org/abs/2303.14027 and which, in turn, is\n# based on the original Euclidean implementation described in the paper\n# Deep Residual Learning for Image Recognition by He et al. from 2015:\n# https://arxiv.org/abs/1512.03385.\n\n\nfrom typing import Optional", "\nfrom typing import Optional\n\nfrom torch import nn\n\nfrom hypll import nn as hnn\nfrom hypll.tensors import ManifoldTensor\n\n\nclass PoincareResidualBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        manifold: PoincareBall,\n        stride: int = 1,\n        downsample: Optional[nn.Sequential] = None,\n    ):\n        # We can replace each operation in the usual ResidualBlock by a manifold-agnostic\n        # operation and supply the PoincareBall object to these operations.\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.manifold = manifold\n        self.stride = stride\n        self.downsample = downsample\n\n        self.conv1 = hnn.HConvolution2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            manifold=manifold,\n            stride=stride,\n            padding=1,\n        )\n        self.bn1 = hnn.HBatchNorm2d(features=out_channels, manifold=manifold)\n        self.relu = hnn.HReLU(manifold=self.manifold)\n        self.conv2 = hnn.HConvolution2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            manifold=manifold,\n            padding=1,\n        )\n        self.bn2 = hnn.HBatchNorm2d(features=out_channels, manifold=manifold)\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        residual = x\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        # We replace the addition operation inside the skip connection by a Mobius addition.\n        x = self.manifold.mobius_add(x, residual)\n        x = self.relu(x)\n\n        return x", "\nclass PoincareResidualBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        manifold: PoincareBall,\n        stride: int = 1,\n        downsample: Optional[nn.Sequential] = None,\n    ):\n        # We can replace each operation in the usual ResidualBlock by a manifold-agnostic\n        # operation and supply the PoincareBall object to these operations.\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.manifold = manifold\n        self.stride = stride\n        self.downsample = downsample\n\n        self.conv1 = hnn.HConvolution2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            manifold=manifold,\n            stride=stride,\n            padding=1,\n        )\n        self.bn1 = hnn.HBatchNorm2d(features=out_channels, manifold=manifold)\n        self.relu = hnn.HReLU(manifold=self.manifold)\n        self.conv2 = hnn.HConvolution2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            manifold=manifold,\n            padding=1,\n        )\n        self.bn2 = hnn.HBatchNorm2d(features=out_channels, manifold=manifold)\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        residual = x\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        # We replace the addition operation inside the skip connection by a Mobius addition.\n        x = self.manifold.mobius_add(x, residual)\n        x = self.relu(x)\n\n        return x", "\n\nclass PoincareResNet(nn.Module):\n    def __init__(\n        self,\n        channel_sizes: list[int],\n        group_depths: list[int],\n        manifold: PoincareBall,\n    ):\n        # For the Poincare ResNet itself we again replace each layer by a manifold-agnostic one\n        # and supply the PoincareBall to each of these. We also replace the ResidualBlocks by\n        # the manifold-agnostic one defined above.\n        super().__init__()\n        self.channel_sizes = channel_sizes\n        self.group_depths = group_depths\n        self.manifold = manifold\n\n        self.conv = hnn.HConvolution2d(\n            in_channels=3,\n            out_channels=channel_sizes[0],\n            kernel_size=3,\n            manifold=manifold,\n            padding=1,\n        )\n        self.bn = hnn.HBatchNorm2d(features=channel_sizes[0], manifold=manifold)\n        self.relu = hnn.HReLU(manifold=manifold)\n        self.group1 = self._make_group(\n            in_channels=channel_sizes[0],\n            out_channels=channel_sizes[0],\n            depth=group_depths[0],\n        )\n        self.group2 = self._make_group(\n            in_channels=channel_sizes[0],\n            out_channels=channel_sizes[1],\n            depth=group_depths[1],\n            stride=2,\n        )\n        self.group3 = self._make_group(\n            in_channels=channel_sizes[1],\n            out_channels=channel_sizes[2],\n            depth=group_depths[2],\n            stride=2,\n        )\n\n        self.avg_pool = hnn.HAvgPool2d(kernel_size=8, manifold=manifold)\n        self.fc = hnn.HLinear(in_features=channel_sizes[2], out_features=10, manifold=manifold)\n\n    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.group1(x)\n        x = self.group2(x)\n        x = self.group3(x)\n        x = self.avg_pool(x)\n        x = self.fc(x.squeeze())\n        return x\n\n    def _make_group(\n        self,\n        in_channels: int,\n        out_channels: int,\n        depth: int,\n        stride: int = 1,\n    ) -> nn.Sequential:\n        if stride == 1:\n            downsample = None\n        else:\n            downsample = hnn.HConvolution2d(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=1,\n                manifold=self.manifold,\n                stride=stride,\n            )\n\n        layers = [\n            PoincareResidualBlock(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                manifold=self.manifold,\n                stride=stride,\n                downsample=downsample,\n            )\n        ]\n\n        for _ in range(1, depth):\n            layers.append(\n                PoincareResidualBlock(\n                    in_channels=out_channels,\n                    out_channels=out_channels,\n                    manifold=self.manifold,\n                )\n            )\n\n        return nn.Sequential(*layers)", "\n\n# Now, let's create a thin Poincare ResNet with channel sizes [4, 8, 16] and with a depth of 20\n# layers.\nnet = PoincareResNet(\n    channel_sizes=[4, 8, 16],\n    group_depths=[3, 3, 3],\n    manifold=manifold,\n).to(device)\n", ").to(device)\n\n\n#########################################\n# 4. Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and RiemannianAdam optimizer.\n\ncriterion = nn.CrossEntropyLoss()\n# net.parameters() includes the learnable curvature \"c\" of the manifold.", "criterion = nn.CrossEntropyLoss()\n# net.parameters() includes the learnable curvature \"c\" of the manifold.\nfrom hypll.optim import RiemannianAdam\n\noptimizer = RiemannianAdam(net.parameters(), lr=0.001)\n\n\n######################\n# 5. Train the network\n# ^^^^^^^^^^^^^^^^^^^^", "# 5. Train the network\n# ^^^^^^^^^^^^^^^^^^^^\n# We simply have to loop over our data iterator, project the inputs onto the\n# manifold, and feed them to the network and optimize. We will train for a limited\n# number of epochs here due to the long training time of this model.\n\nfrom hypll.tensors import TangentTensor\n\nfor epoch in range(2):  # Increase this number to at least 100 for good results\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # move the inputs to the manifold\n        tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n        manifold_inputs = manifold.expmap(tangents)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(manifold_inputs)\n        loss = criterion(outputs.tensor, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        print(f\"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}\")\n        running_loss = 0.0", "for epoch in range(2):  # Increase this number to at least 100 for good results\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # move the inputs to the manifold\n        tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n        manifold_inputs = manifold.expmap(tangents)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(manifold_inputs)\n        loss = criterion(outputs.tensor, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        print(f\"[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}\")\n        running_loss = 0.0", "\nprint(\"Finished Training\")\n\n\n######################################\n# 6. Test the network on the test data\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#\n# Let us look at how the network performs on the whole dataset.\n", "# Let us look at how the network performs on the whole dataset.\n\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data[0].to(device), data[1].to(device)\n\n        # move the images to the manifold\n        tangents = TangentTensor(data=images, man_dim=1, manifold=manifold)\n        manifold_images = manifold.expmap(tangents)\n\n        # calculate outputs by running images through the network\n        outputs = net(manifold_images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.tensor, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()", "\nprint(f\"Accuracy of the network on the 10000 test images: {100 * correct // total} %\")\n"]}
{"filename": "tutorials/poincare_embeddings_tutorial.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\"\"\"\nTraining Poincare embeddings for the mammals subset from WordNet\n================================================================\n\nThis implementation is based on the \"Poincare Embeddings for Learning Hierarchical Representations\"\npaper by Maximilian Nickel and Douwe Kiela, which can be found at: \n\n- https://arxiv.org/pdf/1705.08039.pdf\n", "- https://arxiv.org/pdf/1705.08039.pdf\n\nTheir implementation can be found at:\n\n- https://github.com/facebookresearch/poincare-embeddings\n\nWe will perform the following steps in order:\n\n1. Load the mammals subset from the WordNet hierarchy using NetworkX\n2. Create a dataset containing the graph from which we can sample", "1. Load the mammals subset from the WordNet hierarchy using NetworkX\n2. Create a dataset containing the graph from which we can sample\n3. Initialize the Poincare ball on which the embeddings will be trained\n4. Define the Poincare embedding model\n5. Define the Poincare embedding loss function\n6. Perform a few \"burn-in\" training epochs with reduced learning rate\n\n\"\"\"\n\n######################################################################", "\n######################################################################\n# 1. Load the mammals subset from the WordNet hierarchy using NetworkX\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nimport json\nimport os\n\nimport networkx as nx\n", "import networkx as nx\n\n# If you have stored the wordnet_mammals.json file differently that the definition here,\n# adjust the mammals_json_file string to correctly point to this json file. The file itself can\n# be found in the repository under tutorials/data/wordnet_mammals.json.\nroot = os.path.dirname(os.path.abspath(__file__))\nmammals_json_file = os.path.join(root, \"data\", \"wordnet_mammals.json\")\nwith open(mammals_json_file, \"r\") as json_file:\n    graph_dict = json.load(json_file)\n", "\nmammals_graph = nx.node_link_graph(graph_dict)\n\n\n###################################################################\n# 2. Create a dataset containing the graph from which we can sample\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nimport random\n", "import random\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass MammalsEmbeddingDataset(Dataset):\n    def __init__(\n        self,\n        mammals: nx.DiGraph,\n    ):\n        super().__init__()\n        self.mammals = mammals\n        self.edges_list = list(mammals.edges())\n\n    # This Dataset object has a sample for each edge in the graph.\n    def __len__(self) -> int:\n        return len(self.edges_list)\n\n    def __getitem__(self, idx: int):\n        # For each existing edge in the graph we choose 10 fake or negative edges, which we build\n        # from the idx-th existing edge. So, first we grab this edge from the graph.\n        rel = self.edges_list[idx]\n\n        # Next, we take our source node rel[0] and see which nodes in the graph are not a child of\n        # this node.\n        negative_target_nodes = list(\n            self.mammals.nodes() - nx.descendants(self.mammals, rel[0]) - {rel[0]}\n        )\n\n        # Then, we sample at most 5 of these negative target nodes...\n        negative_target_sample_size = min(5, len(negative_target_nodes))\n        negative_target_nodes_sample = random.sample(\n            negative_target_nodes, negative_target_sample_size\n        )\n\n        # and add these to a tensor which will be used as input for our embedding model.\n        edges = torch.tensor([rel] + [[rel[0], neg] for neg in negative_target_nodes_sample])\n\n        # Next, we do the same with our target node rel[1], but now where we sample from nodes\n        # which aren't a parent of it.\n        negative_source_nodes = list(\n            self.mammals.nodes() - nx.ancestors(self.mammals, rel[1]) - {rel[1]}\n        )\n\n        # We sample from these negative source nodes until we have a total of 10 negative edges...\n        negative_source_sample_size = 10 - negative_target_sample_size\n        negative_source_nodes_sample = random.sample(\n            negative_source_nodes, negative_source_sample_size\n        )\n\n        # and add these to the tensor that we created above.\n        edges = torch.cat(\n            tensors=(edges, torch.tensor([[neg, rel[1]] for neg in negative_source_nodes_sample])),\n            dim=0,\n        )\n\n        # Lastly, we create a tensor containing the labels of the edges, indicating whether it's a\n        # True or a False edge.\n        edge_label_targets = torch.cat(tensors=[torch.ones(1).bool(), torch.zeros(10).bool()])\n\n        return edges, edge_label_targets", "\n\n# Now, we construct the dataset.\ndataset = MammalsEmbeddingDataset(\n    mammals=mammals_graph,\n)\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n\n\n#########################################################################", "\n#########################################################################\n# 3. Initialize the Poincare ball on which the embeddings will be trained\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\npoincare_ball = PoincareBall(Curvature(1.0))\n\n", "\n\n########################################\n# 4. Define the Poincare embedding model\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nimport hypll.nn as hnn\n\n\nclass PoincareEmbedding(hnn.HEmbedding):\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n        manifold: PoincareBall,\n    ):\n        super().__init__(num_embeddings, embedding_dim, manifold)\n\n    # The model outputs the distances between the nodes involved in the input edges as these are\n    # used to compute the loss.\n    def forward(self, edges: torch.Tensor) -> torch.Tensor:\n        embeddings = super().forward(edges)\n        edge_distances = self.manifold.dist(x=embeddings[:, :, 0, :], y=embeddings[:, :, 1, :])\n        return edge_distances", "\nclass PoincareEmbedding(hnn.HEmbedding):\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n        manifold: PoincareBall,\n    ):\n        super().__init__(num_embeddings, embedding_dim, manifold)\n\n    # The model outputs the distances between the nodes involved in the input edges as these are\n    # used to compute the loss.\n    def forward(self, edges: torch.Tensor) -> torch.Tensor:\n        embeddings = super().forward(edges)\n        edge_distances = self.manifold.dist(x=embeddings[:, :, 0, :], y=embeddings[:, :, 1, :])\n        return edge_distances", "\n\n# We want to embed every node into a 2-dimensional Poincare ball.\nmodel = PoincareEmbedding(\n    num_embeddings=len(mammals_graph.nodes()),\n    embedding_dim=2,\n    manifold=poincare_ball,\n)\n\n", "\n\n################################################\n# 5. Define the Poincare embedding loss function\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n# This function is given in equation (5) of the Poincare Embeddings paper.\ndef poincare_embeddings_loss(dists: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n    logits = dists.neg().exp()\n    numerator = torch.where(condition=targets, input=logits, other=0).sum(dim=-1)\n    denominator = logits.sum(dim=-1)\n    loss = (numerator / denominator).log().mean().neg()\n    return loss", "def poincare_embeddings_loss(dists: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n    logits = dists.neg().exp()\n    numerator = torch.where(condition=targets, input=logits, other=0).sum(dim=-1)\n    denominator = logits.sum(dim=-1)\n    loss = (numerator / denominator).log().mean().neg()\n    return loss\n\n\n#######################################################################\n# 6. Perform a few \"burn-in\" training epochs with reduced learning rate", "#######################################################################\n# 6. Perform a few \"burn-in\" training epochs with reduced learning rate\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nfrom hypll.optim import RiemannianSGD\n\n# The learning rate of 0.3 is dived by 10 during burn-in.\noptimizer = RiemannianSGD(\n    params=model.parameters(),\n    lr=0.3 / 10,", "    params=model.parameters(),\n    lr=0.3 / 10,\n)\n\n# Perform training as we would usually.\nfor epoch in range(10):\n    average_loss = 0\n    for idx, (edges, edge_label_targets) in enumerate(dataloader):\n        optimizer.zero_grad()\n\n        dists = model(edges)\n        loss = poincare_embeddings_loss(dists=dists, targets=edge_label_targets)\n        loss.backward()\n        optimizer.step()\n\n        average_loss += loss\n\n    average_loss /= len(dataloader)\n    print(f\"Burn-in epoch {epoch} loss: {average_loss}\")", "\n\n#########################\n# 6. Train the embeddings\n# ^^^^^^^^^^^^^^^^^^^^^^^\n\n# Now we use the actual learning rate 0.3.\noptimizer = RiemannianSGD(\n    params=model.parameters(),\n    lr=0.3,", "    params=model.parameters(),\n    lr=0.3,\n)\n\nfor epoch in range(300):\n    average_loss = 0\n    for idx, (edges, edge_label_targets) in enumerate(dataloader):\n        optimizer.zero_grad()\n\n        dists = model(edges)\n        loss = poincare_embeddings_loss(dists=dists, targets=edge_label_targets)\n        loss.backward()\n        optimizer.step()\n\n        average_loss += loss\n\n    average_loss /= len(dataloader)\n    print(f\"Epoch {epoch} loss: {average_loss}\")", "\n# You have now trained your own Poincare Embeddings!\n"]}
{"filename": "tutorials/cifar10_tutorial.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\"\"\"\nTraining a Hyperbolic Classifier\n================================\n\nThis is an adaptation of torchvision's tutorial \"Training a Classifier\" to \nhyperbolic space. The original tutorial can be found here:\n\n- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n", "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n\nTraining a Hyperbolic Image Classifier\n--------------------------------------\n\nWe will do the following steps in order:\n\n1. Define a hyperbolic manifold\n2. Load and normalize the CIFAR10 training and test datasets using ``torchvision``\n3. Define a hyperbolic Convolutional Neural Network", "2. Load and normalize the CIFAR10 training and test datasets using ``torchvision``\n3. Define a hyperbolic Convolutional Neural Network\n4. Define a loss function and optimizer\n5. Train the network on the training data\n6. Test the network on the test data\n\n\"\"\"\n\n########################################################################\n# 1. Define a hyperbolic manifold", "########################################################################\n# 1. Define a hyperbolic manifold\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# We use the Poincar\u00e9 ball model for the purposes of this tutorial.\n\n\nfrom hypll.manifolds.poincare_ball import Curvature, PoincareBall\n\n# Making the curvature a learnable parameter is usually suboptimal but can\n# make training smoother.", "# Making the curvature a learnable parameter is usually suboptimal but can\n# make training smoother.\nmanifold = PoincareBall(c=Curvature(requires_grad=True))\n\n\n########################################################################\n# 2. Load and normalize CIFAR10\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nimport torch", "\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n########################################################################\n# .. note::\n#     If running on Windows and you get a BrokenPipeError, try setting\n#     the num_worker of torch.utils.data.DataLoader() to 0.\n", "#     the num_worker of torch.utils.data.DataLoader() to 0.\n\ntransform = transforms.Compose(\n    [\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\n\n", "\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=True, download=True, transform=transform\n)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n)", "    trainset, batch_size=batch_size, shuffle=True, num_workers=2\n)\n\ntestset = torchvision.datasets.CIFAR10(\n    root=\"./data\", train=False, download=True, transform=transform\n)\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size=batch_size, shuffle=False, num_workers=2\n)\n", ")\n\nclasses = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n\n\n########################################################################\n# 3. Define a hyperbolic Convolutional Neural Network\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's rebuild the convolutional neural network from torchvision's tutorial\n# using hyperbolic modules.", "# Let's rebuild the convolutional neural network from torchvision's tutorial\n# using hyperbolic modules.\n\nfrom torch import nn\n\nfrom hypll import nn as hnn\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = hnn.HConvolution2d(\n            in_channels=3, out_channels=6, kernel_size=5, manifold=manifold\n        )\n        self.pool = hnn.HMaxPool2d(kernel_size=2, manifold=manifold, stride=2)\n        self.conv2 = hnn.HConvolution2d(\n            in_channels=6, out_channels=16, kernel_size=5, manifold=manifold\n        )\n        self.fc1 = hnn.HLinear(in_features=16 * 5 * 5, out_features=120, manifold=manifold)\n        self.fc2 = hnn.HLinear(in_features=120, out_features=84, manifold=manifold)\n        self.fc3 = hnn.HLinear(in_features=84, out_features=10, manifold=manifold)\n        self.relu = hnn.HReLU(manifold=manifold)\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = x.flatten(start_dim=1)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x", "class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = hnn.HConvolution2d(\n            in_channels=3, out_channels=6, kernel_size=5, manifold=manifold\n        )\n        self.pool = hnn.HMaxPool2d(kernel_size=2, manifold=manifold, stride=2)\n        self.conv2 = hnn.HConvolution2d(\n            in_channels=6, out_channels=16, kernel_size=5, manifold=manifold\n        )\n        self.fc1 = hnn.HLinear(in_features=16 * 5 * 5, out_features=120, manifold=manifold)\n        self.fc2 = hnn.HLinear(in_features=120, out_features=84, manifold=manifold)\n        self.fc3 = hnn.HLinear(in_features=84, out_features=10, manifold=manifold)\n        self.relu = hnn.HReLU(manifold=manifold)\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = x.flatten(start_dim=1)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x", "\n\nnet = Net()\n\n########################################################################\n# 4. Define a Loss function and optimizer\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# Let's use a Classification Cross-Entropy loss and RiemannianAdam optimizer.\n# Adam is preferred because hyperbolic linear layers can sometimes have training\n# difficulties early on due to poor initialization.", "# Adam is preferred because hyperbolic linear layers can sometimes have training\n# difficulties early on due to poor initialization.\n\ncriterion = nn.CrossEntropyLoss()\n# net.parameters() includes the learnable curvature \"c\" of the manifold.\nfrom hypll.optim import RiemannianAdam\n\noptimizer = RiemannianAdam(net.parameters(), lr=0.001)\n\n", "\n\n########################################################################\n# 5. Train the network\n# ^^^^^^^^^^^^^^^^^^^^\n# This is when things start to get interesting.\n# We simply have to loop over our data iterator, project the inputs onto the\n# manifold, and feed them to the network and optimize.\n\nfrom hypll.tensors import TangentTensor", "\nfrom hypll.tensors import TangentTensor\n\nfor epoch in range(2):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # move the inputs to the manifold\n        tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n        manifold_inputs = manifold.expmap(tangents)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(manifold_inputs)\n        loss = criterion(outputs.tensor, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:  # print every 2000 mini-batches\n            print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}\")\n            running_loss = 0.0", "\nprint(\"Finished Training\")\n\n\n########################################################################\n# Let's quickly save our trained model:\n\nPATH = \"./cifar_net.pth\"\ntorch.save(net.state_dict(), PATH)\n", "torch.save(net.state_dict(), PATH)\n\n\n########################################################################\n# Next, let's load back in our saved model (note: saving and re-loading the model\n# wasn't necessary here, we only did it to illustrate how to do so):\n\nnet = Net()\nnet.load_state_dict(torch.load(PATH))\n", "net.load_state_dict(torch.load(PATH))\n\n########################################################################\n# 6. Test the network on the test data\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#\n# Let us look at how the network performs on the whole dataset.\n\ncorrect = 0\ntotal = 0", "correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n\n        # move the images to the manifold\n        tangents = TangentTensor(data=images, man_dim=1, manifold=manifold)\n        manifold_images = manifold.expmap(tangents)\n\n        # calculate outputs by running images through the network\n        outputs = net(manifold_images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.tensor, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()", "\nprint(f\"Accuracy of the network on the 10000 test images: {100 * correct // total} %\")\n\n\n########################################################################\n# That looks way better than chance, which is 10% accuracy (randomly picking\n# a class out of 10 classes).\n# Seems like the network learnt something.\n#\n# Hmmm, what are the classes that performed well, and the classes that did", "#\n# Hmmm, what are the classes that performed well, and the classes that did\n# not perform well:\n\n# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n\n        # move the images to the manifold\n        tangents = TangentTensor(data=images, man_dim=1, manifold=manifold)\n        manifold_images = manifold.expmap(tangents)\n\n        outputs = net(manifold_images)\n        _, predictions = torch.max(outputs.tensor, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1", "# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n\n        # move the images to the manifold\n        tangents = TangentTensor(data=images, man_dim=1, manifold=manifold)\n        manifold_images = manifold.expmap(tangents)\n\n        outputs = net(manifold_images)\n        _, predictions = torch.max(outputs.tensor, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1", "\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f\"Accuracy for class: {classname:5s} is {accuracy:.1f} %\")\n\n\n########################################################################\n#\n# Training on GPU", "#\n# Training on GPU\n# ----------------\n# Just like how you transfer a Tensor onto the GPU, you transfer the neural\n# net onto the GPU.\n#\n# Let's first define our device as the first visible cuda device if we have\n# CUDA available:\n#\n# .. code:: python", "#\n# .. code:: python\n#\n#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#\n#\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\n#\n# .. code:: python\n#", "# .. code:: python\n#\n#     print(device)\n#\n#\n# The rest of this section assumes that ``device`` is a CUDA device.\n#\n# Then these methods will recursively go over all modules and convert their\n# parameters and buffers to CUDA tensors:\n#", "# parameters and buffers to CUDA tensors:\n#\n# .. code:: python\n#\n#     net.to(device)\n#\n#\n# Remember that you will have to send the inputs and targets at every step\n# to the GPU too:\n#", "# to the GPU too:\n#\n# .. code:: python\n#\n#         inputs, labels = data[0].to(device), data[1].to(device)\n#\n#\n# **Goals achieved**:\n#\n# - Train a small hyperbolic neural network to classify images.", "#\n# - Train a small hyperbolic neural network to classify images.\n#\n"]}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n#\n# For the full list of built-in configuration values, see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Project information -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nimport os\nimport sys", "import os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\"../..\"))  # Source code dir relative to this file\n\nproject = \"hypll\"\ncopyright = '2023, \"\"'\nauthor = '\"\"'\n\n# -- General configuration ---------------------------------------------------", "\n# -- General configuration ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nextensions = [\n    \"sphinx_copybutton\",\n    \"sphinx.ext.autodoc\",  # Core library for html generation from docstrings\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx_gallery.gen_gallery\",", "    \"sphinx.ext.napoleon\",\n    \"sphinx_gallery.gen_gallery\",\n    \"sphinx_tabs.tabs\",\n]\n\ntemplates_path = [\"_templates\"]\nexclude_patterns = []\n\n# Autodoc options:\nautodoc_class_signature = \"separated\"", "# Autodoc options:\nautodoc_class_signature = \"separated\"\nautodoc_default_options = {\n    \"members\": True,\n    \"member-order\": \"bysource\",\n    \"special-members\": \"__init__\",\n    \"undoc-members\": True,\n    \"exclude-members\": \"__weakref__\",\n}\nautodoc_default_flags = [", "}\nautodoc_default_flags = [\n    \"members\",\n    \"undoc-members\",\n    \"special-members\",\n    \"show-inheritance\",\n]\n\n# Napoleon options:\nnapoleon_google_docstring = True", "# Napoleon options:\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = False\nnapoleon_include_init_with_doc = False\nnapoleon_include_private_with_doc = False\nnapoleon_include_special_with_doc = False\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False", "napoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = False\nnapoleon_use_rtype = False\nnapoleon_type_aliases = None\n\n# Sphinx gallery config:\nsphinx_gallery_conf = {\n    \"examples_dirs\": \"../../tutorials\",\n    \"gallery_dirs\": \"tutorials/\",", "    \"examples_dirs\": \"../../tutorials\",\n    \"gallery_dirs\": \"tutorials/\",\n    \"filename_pattern\": \"\",\n    # TODO(Philipp, 06/23): Figure out how we can build and host tutorials on RTD.\n    \"plot_gallery\": \"False\",\n}\n\n# -- Options for HTML output -------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = \"alabaster\"\nhtml_static_path = [\"_static\"]\n"]}
