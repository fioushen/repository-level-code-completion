{"filename": "stats_mngr.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport logging\nimport re\nfrom dataclasses import dataclass\nfrom datetime import timedelta", "from dataclasses import dataclass\nfrom datetime import timedelta\nfrom enum import Enum, auto\n\nimport regexes\nimport utils\n\nformat_err_msg = utils.format_err_msg\nParsingAssertion = utils.ParsingAssertion\nErrContext = utils.ErrorContext", "ParsingAssertion = utils.ParsingAssertion\nErrContext = utils.ErrorContext\nformat_line_num_from_entry = utils.format_line_num_from_entry\nformat_line_num_from_line_idx = utils.format_line_num_from_line_idx\nget_line_num_from_entry = utils.get_line_num_from_entry\n\n\ndef is_empty_line(line):\n    return re.fullmatch(regexes.EMPTY_LINE, line) is not None\n", "\n\ndef parse_uptime_line(line, allow_mismatch=False):\n    # Uptime(secs): 603.0 total, 600.0 interval\n    match = re.search(regexes.UPTIME_STATS_LINE, line)\n\n    if not match:\n        if allow_mismatch:\n            return None\n        if not match:\n            raise ParsingAssertion(\"Failed parsing uptime line\",\n                                   ErrContext(**{\"line\": line}))\n\n    total_sec = float(match.group('total'))\n    interval_sec = float(match.group('interval'))\n    return total_sec, interval_sec", "\n\ndef parse_line_with_cf(line, regex_str, allow_mismatch=False):\n    line_parts = re.findall(regex_str, line)\n\n    if not line_parts:\n        if allow_mismatch:\n            return None\n        if not line_parts:\n            raise ParsingAssertion(\"Failed parsing line with column-family\",\n                                   ErrContext(**{\"line\": line}))\n\n    cf_name = line_parts[0]\n    return cf_name", "\n\nclass DbWideStatsMngr:\n    \"\"\" Parses and stores dumps of the db-wide stats at the top of the\n     DUMPING STATS dump\n    \"\"\"\n    @dataclass\n    class CumulativeWritesInfo:\n        num_writes: int = 0\n        num_keys: int = 0\n        ingest: int = 0\n        ingest_rate_mbps: float = 0.0\n\n    @staticmethod\n    def is_start_line(line):\n        return re.fullmatch(regexes.DB_STATS, line) is not None\n\n    def __init__(self):\n        self.stalls = {}\n        self.cumulative_writes = {}\n\n    def add_lines(self, time, db_stats_lines):\n        assert len(db_stats_lines) > 0\n\n        self.stalls[time] = {}\n\n        for line in db_stats_lines[1:]:\n            if self.try_parse_as_interval_stall_line(time, line):\n                continue\n            elif self.try_parse_as_cumulative_stall_line(time, line):\n                continue\n            elif self.try_parse_as_cumulative_writes_line(time, line):\n                continue\n\n        if DbWideStatsMngr.is_all_zeroes_entry(self.stalls[time]):\n            del self.stalls[time]\n\n    @staticmethod\n    def try_parse_as_stalls_line(regex, line):\n        line_parts = re.findall(regex, line)\n        if not line_parts:\n            return None\n\n        assert len(line_parts) == 1 and len(line_parts[0]) == 5\n\n        hours, minutes, seconds, ms, stall_percent = line_parts[0]\n        stall_duration = timedelta(hours=int(hours),\n                                   minutes=int(minutes),\n                                   seconds=int(seconds),\n                                   milliseconds=int(ms))\n        return stall_duration, stall_percent\n\n    def try_parse_as_interval_stall_line(self, time, line):\n        stall_info = DbWideStatsMngr.try_parse_as_stalls_line(\n            regexes.DB_WIDE_INTERVAL_STALL, line)\n        if stall_info is None:\n            return None\n\n        stall_duration, stall_percent = stall_info\n        self.stalls[time].update({\"interval_duration\": stall_duration,\n                                  \"interval_percent\": float(stall_percent)})\n\n    def try_parse_as_cumulative_stall_line(self, time, line):\n        stall_info = DbWideStatsMngr.try_parse_as_stalls_line(\n            regexes.DB_WIDE_CUMULATIVE_STALL, line)\n        if stall_info is None:\n            return None\n\n        stall_duration, stall_percent = stall_info\n        self.stalls[time].update({\"cumulative_duration\": stall_duration,\n                                  \"cumulative_percent\": float(stall_percent)})\n\n    def try_parse_as_cumulative_writes_line(self, time, line):\n        writes_info = re.findall(regexes.DB_WIDE_CUMULATIVE_WRITES, line)\n        if not writes_info:\n            return None\n        assert len(writes_info) == 1 and len(writes_info[0]) == 6\n\n        writes_info = writes_info[0]\n        info = DbWideStatsMngr.CumulativeWritesInfo()\n        info.num_writes = utils.get_number_from_human_readable_components(\n            writes_info[0], writes_info[1])\n        info.num_keys = utils.get_number_from_human_readable_components(\n            writes_info[2], writes_info[3])\n        # Although ingest is a counter it's printed in units of GB (bytes)\n        info.ingest = \\\n            utils.get_num_bytes_from_human_readable_str(f\"{writes_info[4]} GB\")\n        # Keep the ingest rate in MBPS as it is printed\n        info.ingest_rate_mbps = float(writes_info[5])\n\n        self.cumulative_writes[time] = info\n\n    @staticmethod\n    def is_all_zeroes_entry(entry):\n        interval_duration_total_secs = 0.0\n        interval_percent = 0.0\n        cumulative_duration_total_secs = 0\n        cumulative_percent = 0.0\n\n        if \"interval_duration\" in entry:\n            interval_duration_total_secs = \\\n                entry[\"interval_duration\"].total_seconds()\n        if \"interval_percent\" in entry:\n            interval_percent = entry[\"interval_percent\"]\n        if \"cumulative_duration\" in entry:\n            cumulative_duration_total_secs = \\\n                entry[\"cumulative_duration\"].total_seconds()\n        if \"cumulative_percent\" in entry:\n            interval_percent = entry[\"cumulative_percent\"]\n\n        return interval_duration_total_secs == 0.0 and \\\n            interval_percent == 0.0 and \\\n            cumulative_duration_total_secs == 0.0 and \\\n            cumulative_percent == 0.0\n\n    def get_stalls_entries(self):\n        return self.stalls\n\n    def get_cumulative_writes_entries(self):\n        return self.cumulative_writes\n\n    def get_last_cumulative_writes_entry(self):\n        if not self.cumulative_writes:\n            return None\n        return utils.get_last_dict_entry(self.cumulative_writes)", "\n\nclass CompactionStatsMngr:\n    class LineType(Enum):\n        LEVEL = auto()\n        SUM = auto()\n        INTERVAL = auto()\n        USER = auto()\n\n    class LevelFields(str, Enum):\n        SIZE_BYTES = 'size_bytes'\n        WRITE_AMP = 'W-Amp'\n        COMP_SEC = 'Comp(sec)'\n        COMP_MERGE_CPU = 'CompMergeCPU(sec)'\n\n        @staticmethod\n        def get_fields_list():\n            return list(CompactionStatsMngr.LevelFields.__members__)\n\n        @staticmethod\n        def has_field(field):\n            assert isinstance(field, CompactionStatsMngr.LevelFields)\n            return field.name in \\\n                CompactionStatsMngr.LevelFields.get_fields_list()\n\n    class CfLevelEntry:\n        def __init__(self, entry):\n            self.time = list(entry.keys())[0]\n            self.lines = entry[self.time]\n\n        @staticmethod\n        def get_level_key(level):\n            return f\"LEVEL-{level}\"\n\n        def get_time(self):\n            return self.time\n\n        def get_levels(self):\n            level_regex = fr\"LEVEL-{regexes.INT_C}\"\n            levels = []\n            for key in self.lines.keys():\n                match = re.findall(level_regex, key)\n                if match:\n                    levels.append(int(match[0]))\n            return levels\n\n        def get_level_line(self, level):\n            level_key = __class__.get_level_key(level)\n            if level_key not in self.lines:\n                return None\n            return self.lines[level_key]\n\n        def get_sum_line(self):\n            return self.lines['SUM']\n\n        def get_total_size_bytes(self):\n            return\n\n    @staticmethod\n    def parse_start_line(line, allow_mismatch=False):\n        return parse_line_with_cf(line, regexes.COMPACTION_STATS,\n                                  allow_mismatch)\n\n    @staticmethod\n    def is_start_line(line):\n        return CompactionStatsMngr.parse_start_line(line,\n                                                    allow_mismatch=True) \\\n               is not None\n\n    def __init__(self):\n        self.level_entries = dict()\n        self.priority_entries = dict()\n\n    def add_lines(self, time, cf_name, stats_lines, line_num):\n        stats_lines = [line.strip() for line in stats_lines]\n        assert cf_name == \\\n               CompactionStatsMngr.parse_start_line(stats_lines[0])\n\n        try:\n            if stats_lines[1].startswith('Level'):\n                self.parse_level_lines(time, cf_name, stats_lines[1:])\n            elif stats_lines[1].startswith('Priority'):\n                self.parse_priority_lines(time, cf_name, stats_lines[1:])\n            else:\n                assert 0\n        except utils.ParsingError as e:\n            logging.warning(f\"Failed parsing compaction stats lines ({e}) - \"\n                            f\"Ignoring these lines.\"\n                            f\".\\n\"\n                            f\"time:{time}, cf:{cf_name}, \"\n                            f\"lines:\\n{stats_lines[1:]}\")\n\n    @staticmethod\n    def parse_header_line(header_line, separator_line):\n        # separator line is expected to be all \"-\"-s\n        if set(separator_line.strip()) != {\"-\"}:\n            # TODO - Issue an error / warning\n            return None\n\n        header_fields = header_line.split()\n\n        if header_fields[0] != 'Level' or header_fields[1] != \"Files\" or \\\n                header_fields[2] != \"Size\":\n            # TODO - Issue an error / warning\n            return None\n\n        return header_fields\n\n    @staticmethod\n    def determine_line_type(type_field_str):\n        type_field_str = type_field_str.strip()\n        level_num = None\n        line_type = None\n        if type_field_str == \"Sum\":\n            line_type = CompactionStatsMngr.LineType.SUM\n        elif type_field_str == \"Int\":\n            line_type = CompactionStatsMngr.LineType.INTERVAL\n        elif type_field_str == \"User\":\n            line_type = CompactionStatsMngr.LineType.USER\n        else:\n            level_match = re.findall(r\"L(\\d+)\", type_field_str)\n            if level_match:\n                line_type = CompactionStatsMngr.LineType.LEVEL\n                level_num = int(level_match[0])\n            else:\n                # TODO - Error\n                pass\n\n        return line_type, level_num\n\n    @staticmethod\n    def parse_files_field(files_field):\n        files_parts = re.findall(r\"(\\d+)/(\\d+)\", files_field)\n        if not files_parts:\n            # TODO - Error\n            return None\n\n        return files_parts[0][0], files_parts[0][1]\n\n    @staticmethod\n    def parse_size_field(size_value, size_units):\n        return utils.get_num_bytes_from_human_readable_components(\n            size_value,\n            size_units)\n\n    def parse_level_lines(self, time, cf_name, stats_lines):\n        if len(stats_lines) < 2:\n            raise utils.ParsingError(\"Missing lines\")\n\n        header_fields = CompactionStatsMngr.parse_header_line(stats_lines[0],\n                                                              stats_lines[1])\n        if header_fields is None:\n            raise utils.ParsingError(\"Failed parsing compaction stats header\")\n\n        new_entry = {}\n        for line in stats_lines[2:]:\n            line_fields = line.strip().split()\n            if not line_fields:\n                continue\n            line_type, level_num = \\\n                CompactionStatsMngr.determine_line_type(line_fields[0])\n            if line_type is None:\n                raise utils.ParsingError(\n                    f\"Failed determining line type ({line_fields[0]})\")\n\n            num_files, files_in_comp = \\\n                CompactionStatsMngr.parse_files_field(line_fields[1])\n            if files_in_comp is None:\n                raise utils.ParsingError(\n                    f\"Failed parsing files field ({line_fields[1]})\")\n\n            size_in_units = line_fields[2]\n            size_units = line_fields[3]\n\n            key = line_type.name\n            if line_type is CompactionStatsMngr.LineType.LEVEL:\n                key += f\"-{level_num}\"\n\n            new_entry[key] = {\n                \"Num-Files\": num_files,\n                \"Files-In-Comp\": files_in_comp,\n                \"size_bytes\":\n                    CompactionStatsMngr.parse_size_field(size_in_units,\n                                                         size_units)\n            }\n            # A valid line must have one more field than in the header line\n            if len(line_fields) != len(header_fields) + 1:\n                logging.error(\n                    f\"Expected #{len(header_fields)+1} fields in line, \"\n                    f\"when there are {len(line_fields)} in compaction level \"\n                    f\"stats. time:{time}, cf:{cf_name}.\\n\"\n                    f\"line:{line}\")\n                return\n\n            new_entry[key].update({\n                header_fields[i]: line_fields[i+1]\n                for i in range(3, len(header_fields))\n            })\n\n        if CompactionStatsMngr.LineType.SUM.name not in new_entry:\n            logging.error(\n                f\"Error parsing compaction stats level lines. \"\n                f\"time:{time}, cf:{cf_name}.\\n\"\n                f\"stats lines:{stats_lines}\")\n            return\n\n        if time not in self.level_entries:\n            self.level_entries[time] = {}\n\n        self.level_entries[time][cf_name] = new_entry\n\n    def parse_priority_lines(self, time, cf_name, stats_lines):\n        # TODO - Consider issuing an info message as Redis (e.g.) don not\n        #  have any content here\n        if len(stats_lines) < 4:\n            return\n\n        # TODO: Parse when doing something with the data\n        pass\n\n    def get_level_entries(self):\n        return self.level_entries\n\n    def get_cf_level_entries(self, cf_name):\n        cf_entries = []\n        for time, time_entries in self.level_entries.items():\n            if cf_name in time_entries:\n                cf_entries.append({time: time_entries[cf_name]})\n\n        return cf_entries\n\n    def get_first_cf_level_entry(self, cf_name):\n        all_entries = self.get_cf_level_entries(cf_name)\n        if not all_entries:\n            return None\n        return all_entries[0]\n\n    def get_last_cf_level_entry(self, cf_name):\n        all_entries = self.get_cf_level_entries(cf_name)\n        if not all_entries:\n            return None\n        return all_entries[-1]\n\n    def get_first_level_entry_all_cfs(self):\n        all_entries = self.get_level_entries()\n        if not all_entries:\n            return None\n\n        time, first_all_cfs_dumps = \\\n            utils.get_first_dict_entry_components(all_entries)\n        return {cf_name: {time: cf_entry} for cf_name, cf_entry in\n                first_all_cfs_dumps.items()}\n\n    def get_last_level_entry_all_cfs(self):\n        all_entries = self.get_level_entries()\n        if not all_entries:\n            return None\n\n        time, last_all_cfs_dumps = \\\n            utils.get_last_dict_entry_components(all_entries)\n        return {cf_name: {time: cf_entry} for cf_name, cf_entry in\n                last_all_cfs_dumps.items()}\n\n    @staticmethod\n    def get_time_of_entry(stats_table):\n        keys = list(stats_table.keys())\n        assert len(keys) == 1\n        return keys[0]\n\n    @staticmethod\n    def get_level_entry_uptime_seconds(entry, log_start_time):\n        entry_time = CompactionStatsMngr.get_time_of_entry(entry)\n        return utils.get_times_strs_diff_seconds(log_start_time, entry_time)\n\n    @staticmethod\n    def get_field_value_for_line_in_entry(stats_table, line_key, field):\n        assert CompactionStatsMngr.LevelFields.has_field(field)\n\n        time = CompactionStatsMngr.get_time_of_entry(stats_table)\n        if line_key not in list(stats_table[time].keys()):\n            logging.info(f\"{line_key} not in entry. time:{time}\")\n            return None\n\n        level_line = stats_table[time][line_key]\n        if field.value not in level_line:\n            logging.info(f\"{field.value} not in entry's line. time:{time}\\n\"\n                         f\"{level_line}\")\n            return None\n\n        return level_line[field.value]\n\n    @staticmethod\n    def get_field_value_for_all_levels(stats_table, field):\n        assert CompactionStatsMngr.LevelFields.has_field(field)\n\n        per_level_value = {}\n        time = CompactionStatsMngr.get_time_of_entry(stats_table)\n\n        line_key_regex = fr\"LEVEL-{regexes.INT_C}\"\n        for key, value in stats_table[time].items():\n            match = re.findall(line_key_regex, key)\n            if not match:\n                continue\n\n            level = int(match[0])\n            per_level_value[level] = \\\n                CompactionStatsMngr.get_field_value_for_line_in_entry(\n                    stats_table, key, field)\n\n        if not per_level_value:\n            return None\n        return per_level_value\n\n    @staticmethod\n    def get_level_field_value(stats_table, level, field):\n        per_level_value = \\\n            CompactionStatsMngr.get_field_value_for_all_levels(\n                stats_table, field)\n\n        if per_level_value is None:\n            return None\n\n        if level not in per_level_value:\n            logging.info(f\"No info for level {level} in table.\")\n            return None\n\n        return per_level_value[level]\n\n    @staticmethod\n    def get_sum_value(stats_table, field):\n        line_key = CompactionStatsMngr.LineType.SUM.name\n        value = \\\n            CompactionStatsMngr.get_field_value_for_line_in_entry(\n                stats_table, line_key, field)\n\n        if value is None:\n            # SUM must be present always\n            raise utils.ParsingError(\n                f\"{line_key} is missing from compaction stats.\\n{stats_table}\")\n\n        return value\n\n    def get_cf_size_bytes_at_end(self, cf_name):\n        last_cf_entry = self.get_last_cf_level_entry(cf_name)\n        if not last_cf_entry:\n            return None\n        return CompactionStatsMngr.get_field_value_for_line_in_entry(\n            last_cf_entry, CompactionStatsMngr.LineType.SUM.name,\n            CompactionStatsMngr.LevelFields.SIZE_BYTES)\n\n    def get_cf_level_size_bytes(self, cf_name, level):\n        last_cf_entry = self.get_last_cf_level_entry(cf_name)\n        if not last_cf_entry:\n            return None\n        # TODO - Turn this into a utility\n        level_key = f\"LEVEL-{level}\"\n        return CompactionStatsMngr.get_field_value_for_line_in_entry(\n            last_cf_entry, level_key,\n            CompactionStatsMngr.LevelFields.SIZE_BYTES)", "\n\nclass BlobStatsMngr:\n    @staticmethod\n    def parse_blob_stats_line(line, allow_mismatch=False):\n        line_parts = re.findall(regexes.BLOB_STATS_LINE, line)\n        if not line_parts:\n            if allow_mismatch:\n                return None\n            assert line_parts\n\n        file_count, total_size_gb, garbage_size_gb, space_amp = line_parts[0]\n        return \\\n            int(file_count), float(total_size_gb), float(garbage_size_gb), \\\n            float(space_amp)\n\n    @staticmethod\n    def is_start_line(line):\n        return \\\n            BlobStatsMngr.parse_blob_stats_line(line, allow_mismatch=True) \\\n            is not None\n\n    def __init__(self):\n        self.entries = dict()\n\n    def add_lines(self, time, cf_name, db_stats_lines):\n        assert len(db_stats_lines) > 0\n        line = db_stats_lines[0]\n\n        line_parts = re.findall(regexes.BLOB_STATS_LINE, line)\n        assert line_parts and len(line_parts) == 1 and len(line_parts[0]) == 4\n\n        components = line_parts[0]\n        file_count = int(components[0])\n        total_size_bytes = \\\n            utils.get_num_bytes_from_human_readable_components(\n                components[1],\n                \"GB\")\n        garbage_size_bytes = \\\n            utils.get_num_bytes_from_human_readable_components(\n                components[2],\n                \"GB\")\n        space_amp = float(components[3])\n\n        if cf_name not in self.entries:\n            self.entries[cf_name] = dict()\n        self.entries[cf_name][time] = {\n            \"File Count\": file_count,\n            \"Total Size\": total_size_bytes,\n            \"Garbage Size\": garbage_size_bytes,\n            \"Space Amp\": space_amp\n        }\n\n    def get_cf_stats(self, cf_name):\n        if cf_name not in self.entries:\n            return []\n        return self.entries[cf_name]", "\n\nclass CfNoFileStatsMngr:\n    @staticmethod\n    def is_start_line(line):\n        return parse_uptime_line(line, allow_mismatch=True)\n\n    def __init__(self):\n        self.stall_counts = {}\n\n    def try_parse_as_stalls_count_line(self, time, cf_name, line):\n        if not line.startswith(regexes.CF_STALLS_LINE_START):\n            return None\n\n        if cf_name not in self.stall_counts:\n            self.stall_counts[cf_name] = {}\n        # TODO - I have seen compaction stats for the same cf twice - WHY?\n        #######assert time not in self.stall_counts[cf_name] # noqa\n        self.stall_counts[cf_name][time] = {}\n\n        stall_count_and_reason_matches = \\\n            re.compile(regexes.CF_STALLS_COUNT_AND_REASON)\n        sum_fields_count = 0\n        for match in stall_count_and_reason_matches.finditer(line):\n            count = int(match[1])\n            self.stall_counts[cf_name][time][match[2]] = count\n            sum_fields_count += count\n        assert self.stall_counts[cf_name][time]\n\n        total_count_match = re.findall(\n            regexes.CF_STALLS_INTERVAL_COUNT, line)\n\n        # TODO - Last line of Redis's log was cropped in the middle\n        ###### assert total_count_match and len(total_count_match) == 1 # noqa\n        if not total_count_match or len(total_count_match) != 1:\n            del self.stall_counts[cf_name][time]\n            return None\n\n        total_count = int(total_count_match[0])\n        self.stall_counts[cf_name][time][\"interval_total_count\"] = total_count\n        sum_fields_count += total_count\n\n        if sum_fields_count == 0:\n            del self.stall_counts[cf_name][time]\n\n    def add_lines(self, time, cf_name, stats_lines):\n        for line in stats_lines:\n            line = line.strip()\n            if self.try_parse_as_stalls_count_line(time, cf_name, line):\n                continue\n\n    def get_stall_counts(self):\n        return self.stall_counts", "\n\nclass CfFileHistogramStatsMngr:\n    @dataclass\n    class CfLevelStats:\n        count: int = 0\n        average: float = 0.0\n        std_dev: float = 0.0\n        min: int = 0\n        median: float = 0.0\n        max: int = 0\n\n    class CfEntry:\n        def __init__(self, entry):\n            self.time = __class__.get_time(entry)\n            self.levels_stats = entry[self.time]\n\n        @staticmethod\n        def get_time(entry):\n            return list(entry.keys())[0]\n\n        def get_levels(self):\n            return list(self.levels_stats.keys())\n\n        def get_all_levels_stats(self):\n            return self.levels_stats\n\n        def get_level_stats(self, level):\n            if level not in self.levels_stats:\n                return None\n            return self.levels_stats[level]\n\n    @staticmethod\n    def parse_start_line(line, allow_mismatch=False):\n        return parse_line_with_cf(line,\n                                  regexes.FILE_READ_LATENCY_STATS,\n                                  allow_mismatch)\n\n    @staticmethod\n    def is_start_line(line):\n        return CfFileHistogramStatsMngr.parse_start_line(line,\n                                                         allow_mismatch=True) \\\n               is not None\n\n    def __init__(self):\n        # Format: {<cf name>: {time: {<level>: CfLevelStats}}}\n        self.stats = dict()\n\n    def add_lines(self, time, cf_name, db_stats_lines):\n        # The lines are organized as follows:\n        # <cf-1> Header\n        # Level-<X> stats\n        # Level-<Y> stats\n        # ...\n        # A cf header is:\n        # ** File Read Latency Histogram By Level [<cf-name>] **\n        #\n        # The per level dumps are:\n        #  ** Level 0 read latency histogram (<cf-name>):\n        # Count: 25 Average: 1571.7200  StdDev: 5194.93\n        # Min: 1  Median: 2.8333  Max: 26097\n        # Percentiles: <Percentiles Line> (Not parsed currently)\n        # Per-Bucket Histogram Table (Not parsed currently)\n        #\n        assert isinstance(db_stats_lines, list)\n\n        num_lines = len(db_stats_lines)\n        assert num_lines > 0\n\n        # First line must be the start of a cf section\n        parsed_cf_name = CfFileHistogramStatsMngr.parse_start_line(\n            db_stats_lines[0])\n        assert cf_name == parsed_cf_name, \\\n            f\"cf_name:{cf_name}, parsed_cf_name:{parsed_cf_name}\"\n\n        line_idx = 1\n        while line_idx < num_lines:\n            next_line_idx = \\\n                CfFileHistogramStatsMngr.find_next_level_stats_start(\n                    db_stats_lines, line_idx + 1)\n            self.parse_next_level_stats(\n                time, cf_name, db_stats_lines[line_idx:next_line_idx])\n            line_idx = next_line_idx\n\n    @staticmethod\n    def find_next_level_stats_start(db_stats_lines, line_idx):\n        while line_idx < len(db_stats_lines):\n            if CfFileHistogramStatsMngr.\\\n                    is_level_stats_start_line(db_stats_lines[line_idx]):\n                break\n            line_idx += 1\n\n        return line_idx\n\n    def parse_next_level_stats(self, time, cf_name, level_stats_lines):\n        if len(level_stats_lines) < 3:\n            raise utils.ParsingError(\n                f\"Expecting at least 3 lines. time:{time}\\n\"\n                f\"{level_stats_lines}\")\n\n        new_stats = CfFileHistogramStatsMngr.CfLevelStats()\n        level = CfFileHistogramStatsMngr.parse_level_line(\n            time, cf_name, level_stats_lines[0])\n\n        CfFileHistogramStatsMngr.parse_stats_line_1(\n            time, cf_name, level_stats_lines[1], new_stats)\n        CfFileHistogramStatsMngr.parse_stats_line_2(\n            time, cf_name, level_stats_lines[2], new_stats)\n\n        self.add_cf_if_necessary(cf_name)\n        self.add_cf_time_if_necessary(cf_name, time)\n        if level in self.stats[cf_name][time]:\n            logging.warning(\n                f\"Duplicate file read latency for level, Ignoring. \"\n                f\"tims:{time}, cf:{cf_name}. level:{level}\\n\"\n                f\"{level_stats_lines}\")\n            return\n\n        self.stats[cf_name][time][level] = new_stats\n\n    def add_cf_if_necessary(self, cf_name):\n        if cf_name not in self.stats:\n            self.stats[cf_name] = {}\n\n    def add_cf_time_if_necessary(self, cf_name, time):\n        if time not in self.stats[cf_name]:\n            self.stats[cf_name][time] = {}\n\n    @staticmethod\n    def parse_level_line(time, cf_name, line):\n        match = re.findall(regexes.LEVEL_READ_LATENCY_LEVEL_LINE, line)\n        if not match:\n            raise utils.ParsingError(\n                f\"Failed parsing read latency level line. \"\n                f\"time:{time}, cf:{cf_name}\\n\"\n                f\"{line}\")\n        assert len(match) == 1\n\n        # Return the level\n        return int(match[0])\n\n    @staticmethod\n    def is_level_stats_start_line(line):\n        match = re.findall(regexes.LEVEL_READ_LATENCY_LEVEL_LINE, line)\n        return True if match else False\n\n    @staticmethod\n    def parse_stats_line_1(time, cf_name, line, new_stats):\n        match = re.findall(regexes.LEVEL_READ_LATENCY_STATS_LINE1, line)\n        if not match:\n            raise utils.ParsingError(\n                f\"Failed parsing read latency level line. \"\n                f\"time:{time}, cf:{cf_name}\\n\"\n                f\"{line}\")\n        assert len(match) == 1 or len(match[0]) == 3\n\n        new_stats.count = int(match[0][0])\n        new_stats.average = float(match[0][1])\n        new_stats.std_dev = float(match[0][2])\n\n    @staticmethod\n    def parse_stats_line_2(time, cf_name, line, new_stats):\n        match = re.findall(regexes.LEVEL_READ_LATENCY_STATS_LINE2, line)\n        if not match:\n            raise utils.ParsingError(\n                f\"Failed parsing read latency level line. \"\n                f\"time:{time}, cf:{cf_name}\\n\"\n                f\"{line}\")\n        assert len(match) == 1 or len(match[0]) == 3\n\n        new_stats.min = int(match[0][0])\n        new_stats.median = float(match[0][1])\n        new_stats.max = int(match[0][2])\n\n    def get_all_entries(self):\n        if not self.stats:\n            return None\n\n        return self.stats\n\n    def get_cf_entries(self, cf_name):\n        if cf_name not in self.stats:\n            return None\n\n        return self.stats[cf_name]\n\n    def get_last_cf_entry(self, cf_name):\n        all_entries = self.get_cf_entries(cf_name)\n        if not all_entries:\n            return None\n        return utils.get_last_dict_entry(all_entries)", "\n\nclass BlockCacheStatsMngr:\n    @staticmethod\n    def is_start_line(line):\n        return re.findall(regexes.BLOCK_CACHE_STATS_START, line)\n\n    def __init__(self):\n        self.caches = dict()\n\n    def add_lines(self, time, cf_name, db_stats_lines):\n        if len(db_stats_lines) < 2:\n            return\n\n        cache_id = self.parse_cache_id_line(db_stats_lines[0])\n        self.parse_global_entry_stats_line(time, cache_id, db_stats_lines[1])\n        if len(db_stats_lines) > 2:\n            self.parse_cf_entry_stats_line(time, cache_id, db_stats_lines[2])\n        return cache_id\n\n    def parse_cache_id_line(self, line):\n        line_parts = re.findall(regexes.BLOCK_CACHE_STATS_START, line)\n        assert line_parts and len(line_parts) == 1 and len(line_parts[0]) == 3\n        cache_id, cache_capacity, capacity_units = line_parts[0]\n        capacity_bytes = utils.\\\n            get_num_bytes_from_human_readable_components(cache_capacity,\n                                                         capacity_units)\n\n        if cache_id not in self.caches:\n            self.caches[cache_id] = {\"Capacity\": capacity_bytes,\n                                     \"Usage\": 0}\n\n        return cache_id\n\n    def parse_global_entry_stats_line(self, time, cache_id, line):\n        line_parts = re.findall(regexes.BLOCK_CACHE_ENTRY_STATS, line)\n        assert line_parts and len(line_parts) == 1\n\n        roles, roles_stats = BlockCacheStatsMngr.parse_entry_stats_line(\n            line_parts[0])\n\n        self.add_time_if_necessary(cache_id, time)\n        self.caches[cache_id][time][\"Usage\"] = 0\n\n        usage = 0\n        for i, role in enumerate(roles):\n            count, size_with_unit, portion = roles_stats[i].split(',')\n            size_bytes = \\\n                utils.get_num_bytes_from_human_readable_str(\n                    size_with_unit)\n            portion = f\"{float(portion.split('%')[0]):.2f}%\"\n            self.caches[cache_id][time][role] = \\\n                {\"Count\": int(count), \"Size\": size_bytes, \"Portion\": portion}\n            usage += size_bytes\n\n        self.caches[cache_id][time][\"Usage\"] = usage\n        self.caches[cache_id][\"Usage\"] = usage\n\n    def parse_cf_entry_stats_line(self, time, cache_id, line):\n        line_parts = re.findall(regexes.BLOCK_CACHE_CF_ENTRY_STATS, line)\n        if not line_parts:\n            return\n        assert len(line_parts) == 1 and len(line_parts[0]) == 2\n\n        cf_name, roles_info_part = line_parts[0]\n\n        roles, roles_stats = BlockCacheStatsMngr.parse_entry_stats_line(\n            roles_info_part)\n\n        cf_entry = {}\n        for i, role in enumerate(roles):\n            size_bytes = \\\n                utils.get_num_bytes_from_human_readable_str(\n                    roles_stats[i])\n            if size_bytes > 0:\n                cf_entry[role] = size_bytes\n\n        if cf_entry:\n            if \"CF-s\" not in self.caches[cache_id][time]:\n                self.add_time_if_necessary(cache_id, time)\n                self.caches[cache_id][time][\"CF-s\"] = {}\n            self.caches[cache_id][time][\"CF-s\"][cf_name] = cf_entry\n\n    @staticmethod\n    def parse_entry_stats_line(line):\n        roles = re.findall(regexes.BLOCK_CACHE_ENTRY_ROLES_NAMES,\n                           line)\n        roles_stats = re.findall(regexes.BLOCK_CACHE_ENTRY_ROLES_STATS,\n                                 line)\n        if len(roles) != len(roles_stats):\n            assert False, str(ParsingAssertion(\n                f\"Error Parsing block cache stats line. \"\n                f\"roles:{roles}, roles_stats:{roles_stats}\",\n                ErrContext(**{'log_line': line})))\n\n        return roles, roles_stats\n\n    def add_time_if_necessary(self, cache_id, time):\n        if time not in self.caches[cache_id]:\n            self.caches[cache_id][time] = {}\n\n    def get_cache_entries(self, cache_id):\n        if cache_id not in self.caches:\n            return {}\n        return self.caches[cache_id]\n\n    def get_cf_cache_entries(self, cache_id, cf_name):\n        cf_entries = {}\n\n        all_cache_entries = self.get_cache_entries(cache_id)\n        if not all_cache_entries:\n            return cf_entries\n\n        cf_entries = {}\n        for key in all_cache_entries.keys():\n            time = utils.parse_time_str(key, expect_valid_str=False)\n            if time:\n                time = key\n                if \"CF-s\" in all_cache_entries[time]:\n                    if cf_name in all_cache_entries[time][\"CF-s\"]:\n                        cf_entries[time] = \\\n                            all_cache_entries[time][\"CF-s\"][cf_name]\n\n        return cf_entries\n\n    def get_all_cache_entries(self):\n        return self.caches\n\n    def get_last_usage(self, cache_id):\n        usage = 0\n        if self.caches:\n            usage = self.caches[cache_id][\"Usage\"]\n        return usage", "\n\nclass StatsMngr:\n    class StatsType(Enum):\n        DB_WIDE = auto()\n        COMPACTION = auto()\n        BLOB = auto()\n        BLOCK_CACHE = auto()\n        CF_NO_FILE = auto()\n        CF_FILE_HISTOGRAM = auto()\n        COUNTERS = auto()\n\n    def __init__(self):\n        self.dump_stats_entry_found = False\n        self.db_wide_stats_mngr = DbWideStatsMngr()\n        self.compaction_stats_mngr = CompactionStatsMngr()\n        self.blob_stats_mngr = BlobStatsMngr()\n        self.block_cache_stats_mngr = BlockCacheStatsMngr()\n        self.cf_no_file_stats_mngr = CfNoFileStatsMngr()\n        self.cf_file_histogram_stats_mngr = CfFileHistogramStatsMngr()\n\n    @staticmethod\n    def is_dump_stats_start(entry):\n        return entry.get_msg().startswith(regexes.DUMP_STATS_STR)\n\n    @staticmethod\n    def find_next_start_line_in_db_stats(db_stats_lines,\n                                         start_line_idx,\n                                         curr_stats_type):\n        line_idx = start_line_idx + 1\n        next_stats_type = None\n        cf_name = None\n        # DB Wide Stats must be the first and were verified above\n        while line_idx < len(db_stats_lines) and next_stats_type is None:\n            line = db_stats_lines[line_idx]\n\n            if CompactionStatsMngr.is_start_line(line):\n                next_stats_type = StatsMngr.StatsType.COMPACTION\n                cf_name = CompactionStatsMngr.parse_start_line(line)\n            elif BlobStatsMngr.is_start_line(line):\n                next_stats_type = StatsMngr.StatsType.BLOB\n            elif BlockCacheStatsMngr.is_start_line(line):\n                next_stats_type = StatsMngr.StatsType.BLOCK_CACHE\n            elif CfFileHistogramStatsMngr.is_start_line(line):\n                next_stats_type = StatsMngr.StatsType.CF_FILE_HISTOGRAM\n                cf_name = CfFileHistogramStatsMngr.parse_start_line(line)\n            elif CfNoFileStatsMngr.is_start_line(line) and \\\n                    curr_stats_type != StatsMngr.StatsType.DB_WIDE:\n                next_stats_type = StatsMngr.StatsType.CF_NO_FILE\n            else:\n                line_idx += 1\n\n        return line_idx, next_stats_type, cf_name\n\n    def parse_next_db_stats_entry_lines(self, time, cf_name, stats_type,\n                                        entry_start_line_num,\n                                        db_stats_lines, start_line_idx,\n                                        end_line_idx):\n        assert end_line_idx <= len(db_stats_lines)\n        stats_lines_to_parse = db_stats_lines[start_line_idx:end_line_idx]\n        stats_lines_to_parse = [line.strip() for line in stats_lines_to_parse]\n\n        line_num = entry_start_line_num + start_line_idx + 1\n        try:\n            logging.debug(f\"Parsing Stats Component ({stats_type.name}) \"\n                          f\"[line# {line_num}]\")\n\n            valid_stats_type = True\n            if stats_type == StatsMngr.StatsType.DB_WIDE:\n                self.db_wide_stats_mngr.add_lines(time, stats_lines_to_parse)\n            elif stats_type == StatsMngr.StatsType.COMPACTION:\n                self.compaction_stats_mngr.add_lines(\n                    time, cf_name, stats_lines_to_parse, line_num)\n            elif stats_type == StatsMngr.StatsType.BLOB:\n                self.blob_stats_mngr.add_lines(time, cf_name,\n                                               stats_lines_to_parse)\n            elif stats_type == StatsMngr.StatsType.BLOCK_CACHE:\n                self.block_cache_stats_mngr.add_lines(time, cf_name,\n                                                      stats_lines_to_parse)\n            elif stats_type == StatsMngr.StatsType.CF_NO_FILE:\n                self.cf_no_file_stats_mngr.add_lines(time, cf_name,\n                                                     stats_lines_to_parse)\n            elif stats_type == StatsMngr.StatsType.CF_FILE_HISTOGRAM:\n                self.cf_file_histogram_stats_mngr.add_lines(\n                    time, cf_name, stats_lines_to_parse)\n            else:\n                valid_stats_type = False\n        except utils.ParsingError as e:  # noqa\n            logging.exception(format_err_msg(\n                f\"Error parsing a Stats Entry. time:{time}, cf:{cf_name}\" +\n                str(ErrContext(**{\n                    \"log_line_idx\": line_num - 1,\n                    \"log_line\": db_stats_lines[start_line_idx]\n                }))))\n\n            valid_stats_type = True\n\n        if not valid_stats_type:\n            assert False, f\"Unexpected stats type ({stats_type})\"\n\n    def try_adding_entries(self, log_entries, start_entry_idx):\n        cf_names_found = set()\n        entry_idx = start_entry_idx\n\n        # A stats entry starts with the\n        # \"------- DUMPING STATS -------\" entry, however, there may be\n        # unrelated entries between this entry and the entry with\n        # the actual stats (the one starting with \"DB Stats\").\n        dump_stats_entry_found_now = False\n        if StatsMngr.is_dump_stats_start(log_entries[entry_idx]):\n            logging.debug(\n                f\"Found Stats Dump Entry Start (\"\n                f\"{format_line_num_from_entry(log_entries[entry_idx])}\")\n            self.dump_stats_entry_found = True\n            dump_stats_entry_found_now = True\n            entry_idx += 1\n        elif not self.dump_stats_entry_found:\n            return False, entry_idx, cf_names_found\n\n        db_stats_entry = log_entries[entry_idx]\n        db_stats_lines = \\\n            utils.remove_empty_lines_at_start(\n                db_stats_entry.get_msg_lines())\n        db_stats_time = db_stats_entry.get_time()\n\n        # Now check if the actual stats follow immediately, or, if not, wait\n        # for a later entry with the actual stats\n        if len(db_stats_lines) == 0 or \\\n                not DbWideStatsMngr.is_start_line(db_stats_lines[0]):\n            # Found the start => return True\n            return dump_stats_entry_found_now, entry_idx, cf_names_found\n\n        # \"** DB Stats **\" is immediately following \"DUMP STATS\"\n        logging.debug(f\"Parsing Stats Dump Entry (\"\n                      f\"{format_line_num_from_entry(log_entries[entry_idx])}\")\n        self.dump_stats_entry_found = False\n\n        def log_parsing_error(msg_prefix):\n            logging.error(format_err_msg(\n                f\"{msg_prefix} While parsing Stats Entry. time:\"\n                f\"{db_stats_time}, cf:{curr_cf_name}\",\n                ErrContext(**{\n                    \"log_line_idx\":\n                        db_stats_entry.get_start_line_num() + line_idx})))\n\n        line_idx = 0\n        stats_type = StatsMngr.StatsType.DB_WIDE\n        curr_cf_name = utils.NO_CF\n        try:\n            while line_idx < len(db_stats_lines):\n                next_line_num, next_stats_type, next_cf_name = \\\n                    StatsMngr.find_next_start_line_in_db_stats(db_stats_lines,\n                                                               line_idx,\n                                                               stats_type)\n                # parsing must progress\n                assert next_line_num > line_idx\n\n                self.parse_next_db_stats_entry_lines(\n                    db_stats_time,\n                    curr_cf_name,\n                    stats_type,\n                    db_stats_entry.get_start_line_num(),\n                    db_stats_lines,\n                    line_idx,\n                    next_line_num)\n\n                line_idx = next_line_num\n                stats_type = next_stats_type\n\n                if next_cf_name is not None:\n                    curr_cf_name = next_cf_name\n                    if next_cf_name != utils.NO_CF:\n                        cf_names_found.add(curr_cf_name)\n        except AssertionError:\n            log_parsing_error(\"Assertion\")\n            raise\n        except Exception:  # noqa\n            log_parsing_error(\"Exception\")\n            raise\n\n        # Done parsing the stats entry\n        entry_idx += 1\n\n        line_num = format_line_num_from_entry(log_entries[entry_idx]) \\\n            if entry_idx < len(log_entries) else \\\n            format_line_num_from_line_idx(log_entries[-1].get_end_line_idx())\n        logging.debug(f\"Completed Parsing Stats Dump Entry ({line_num})\")\n\n        return True, entry_idx, cf_names_found\n\n    def get_db_wide_stats_mngr(self):\n        return self.db_wide_stats_mngr\n\n    def get_compactions_stats_mngr(self):\n        return self.compaction_stats_mngr\n\n    def get_blob_stats_mngr(self):\n        return self.blob_stats_mngr\n\n    def get_block_cache_stats_mngr(self):\n        return self.block_cache_stats_mngr\n\n    def get_cf_no_file_stats_mngr(self):\n        return self.cf_no_file_stats_mngr\n\n    def get_cf_file_histogram_stats_mngr(self):\n        return self.cf_file_histogram_stats_mngr", ""]}
{"filename": "log_file_options_parser.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport logging\nimport re\n\nimport regexes", "\nimport regexes\nimport utils\nfrom log_entry import LogEntry\n\nformat_lines_range_from_entries_idxs = \\\n    utils.format_lines_range_from_entries_idxs\nformat_line_num_from_entry = utils.format_line_num_from_entry\n\n", "\n\nTABLE_OPTIONS_TOPIC_TITLES = [(\"metadata_cache_options\", \"metadata_cache_\"),\n                              (\"block_cache_options\", \"block_cache_\")]\n\n\ndef get_table_options_topic_info(topic_name):\n    for topic_info in TABLE_OPTIONS_TOPIC_TITLES:\n        if topic_info[0] == topic_name:\n            return topic_info\n    return None", "\n\nclass LogFileOptionsParser:\n    @staticmethod\n    def try_parsing_as_options_entry(log_entry):\n        assert isinstance(log_entry, LogEntry)\n\n        option_parts_match = re.findall(regexes.OPTION_LINE,\n                                        log_entry.get_msg())\n        if len(option_parts_match) != 1 or len(option_parts_match[0]) != 2:\n            return None\n        assert len(option_parts_match) == 1 or len(option_parts_match[0]) == 2\n\n        option_name = option_parts_match[0][0].strip()\n        option_value = option_parts_match[0][1].strip()\n\n        return option_name, option_value\n\n    @staticmethod\n    def is_options_entry(line):\n        if LogFileOptionsParser.try_parsing_as_options_entry(line):\n            return True\n        else:\n            return False\n\n    @staticmethod\n    def try_parsing_as_table_options_entry(log_entry):\n        assert isinstance(log_entry, LogEntry)\n\n        options_lines = log_entry.get_non_stripped_msg_lines()\n        if len(options_lines) < 1:\n            # TODO - Maybe a bug - consider asserting\n            return None\n\n        options_dict = dict()\n        # first line has the \"table_factory options:\" prefix\n        # example:\n        # options:   flush_block_policy_factory: FlushBlockBySizePolicyFactory\n        option_parts_match = re.findall(regexes.TABLE_OPTIONS_START_LINE,\n                                        options_lines[0])\n        if len(option_parts_match) != 1 or len(option_parts_match[0]) != 2:\n            return None\n        options_dict[option_parts_match[0][0].strip()] = \\\n            option_parts_match[0][1].strip()\n\n        options_lines_to_parse = options_lines[1:]\n        line_idx = 0\n        while line_idx < len(options_lines_to_parse):\n            line = options_lines_to_parse[line_idx]\n            option_name, option_value = \\\n                LogFileOptionsParser.parse_table_options_line(line)\n            if option_name is None:\n                line_idx += 1\n                continue\n            topic_info = \\\n                get_table_options_topic_info(option_name)\n            if topic_info is None:\n                options_dict[option_name] = option_value\n                line_idx += 1\n            else:\n                line_idx = \\\n                    LogFileOptionsParser.parse_table_options_topic_options(\n                        topic_info[1], options_lines_to_parse, line_idx,\n                        options_dict)\n\n        return options_dict\n\n    @staticmethod\n    def parse_table_options_line(line):\n        option_parts_match = re.findall(\n            regexes.TABLE_OPTIONS_CONTINUATION_LINE, line)\n        if not option_parts_match:\n            return None, None\n\n        assert len(option_parts_match) == 1 and \\\n               len(option_parts_match[0]) == 2\n        option_name = option_parts_match[0][0].strip()\n        option_value = option_parts_match[0][1].strip()\n        return option_name, option_value\n\n    @staticmethod\n    def parse_table_options_topic_options(topic_prefix,\n                                          options_lines_to_parse, line_idx,\n                                          options_dict):\n        topic_line = options_lines_to_parse[line_idx]\n        topic_line_indentation_size =\\\n            utils.get_num_leading_spaces(topic_line)\n        line_idx += 1\n\n        while line_idx < len(options_lines_to_parse):\n            checked_line = options_lines_to_parse[line_idx]\n            checked_line_indentation_size = \\\n                utils.get_num_leading_spaces(checked_line)\n            if checked_line_indentation_size <=   \\\n                    topic_line_indentation_size:\n                break\n            option_name, option_value = \\\n                LogFileOptionsParser.parse_table_options_line(checked_line)\n            if option_name is None:\n                break\n            options_dict[f\"{topic_prefix}{option_name}\"] = option_value\n            line_idx += 1\n\n        return line_idx\n\n    @staticmethod\n    def try_parsing_as_cf_options_start_entry(log_entry):\n        parts = re.findall(regexes.CF_OPTIONS_START, log_entry.get_msg())\n        if not parts or len(parts) != 1:\n            return None\n        # In case of match, we return the column-family name\n        return parts[0]\n\n    @staticmethod\n    def is_cf_options_start_entry(log_entry):\n        result = LogFileOptionsParser.try_parsing_as_cf_options_start_entry(\n            log_entry)\n        return result is not None\n\n    @staticmethod\n    def parse_db_wide_wbm_sub_pseudo_options(entry, options_dict):\n        wbm_pseudo_options = \\\n            re.findall(regexes.DB_WIDE_WBM_PSEUDO_OPTION_LINE,\n                       entry.get_msg(), re.MULTILINE)\n        for pseudo_option_name, pseudo_option_value in wbm_pseudo_options:\n            options_dict[f\"write_buffer_manager_{pseudo_option_name}\"] =\\\n                pseudo_option_value\n\n    @staticmethod\n    def parse_db_wide_options(log_entries, start_entry_idx, end_entry_idx):\n        \"\"\"\n        Parses all of the entries in the specified range of\n        [start_entry_idx, end_entry_idx)\n\n        Returns:\n            options_dict: The parsed options:\n                dict(<option name>: <option value>)\n            entry_idx: the index of the entry\n        \"\"\"\n        logging.debug(f\"Parsing DB-Wide Options (\"\n                      f\"{format_lines_range_from_entries_idxs(log_entries,start_entry_idx, end_entry_idx)})\") # noqa\n\n        options_dict = {}\n        entry_idx = start_entry_idx\n        while entry_idx < end_entry_idx:\n            entry = log_entries[entry_idx]\n            options_kv = \\\n                LogFileOptionsParser.try_parsing_as_options_entry(entry)\n            if options_kv:\n                option_name, option_value = options_kv\n                options_dict[option_name] = option_value\n                if option_name == \\\n                    utils.\\\n                        DB_WIDE_WRITE_BUFFER_MANAGER_OPTIONS_NAME:\n                    # Special case write buffer manager \"Options\"\n                    LogFileOptionsParser.parse_db_wide_wbm_sub_pseudo_options(\n                        entry, options_dict)\n            else:\n                # TODO - Add info to Error\n                logging.error(f\"TODO - ERROR In DB Wide Entry \"\n                              f\"(idx:{entry_idx}), {entry}\")\n\n            entry_idx += 1\n\n        return options_dict\n\n    @staticmethod\n    def parse_cf_options(log_entries, start_entry_idx, cf_name=None):\n        logging.debug(\n            f\"Parsing CF Options (\"\n            f\"{format_line_num_from_entry(log_entries[start_entry_idx])}\")\n\n        entry_idx = start_entry_idx\n\n        # If cf_name was specified, it means we have received cf options\n        # without the CF options header entry\n        if cf_name is None:\n            cf_name = LogFileOptionsParser. \\\n                try_parsing_as_cf_options_start_entry(log_entries[entry_idx])\n            entry_idx += 1\n\n        # cf_name may be the emtpy string, but not None\n        assert cf_name is not None\n\n        options_dict = {}\n        table_options_dict = None\n        duplicate_option = False\n        while entry_idx < len(log_entries):\n            entry = log_entries[entry_idx]\n            options_kv = \\\n                LogFileOptionsParser.try_parsing_as_options_entry(entry)\n            if options_kv:\n                option_name, option_value = options_kv\n                if option_name in options_dict:\n                    # finding the same option twice implies that the options\n                    # for this cf are over.\n                    duplicate_option = True\n                    break\n                options_dict[option_name] = option_value\n            else:\n                temp_table_options_dict = \\\n                    LogFileOptionsParser.try_parsing_as_table_options_entry(\n                           entry)\n                if temp_table_options_dict:\n                    assert table_options_dict is None\n                    table_options_dict = temp_table_options_dict\n                else:\n                    # The entry is a different type of entry => done\n                    break\n\n            entry_idx += 1\n\n        assert options_dict, \"No Options for Column Family\"\n        assert table_options_dict, \"Missing table options in CF options\"\n\n        line_num = \"\"\n        if entry_idx < len(log_entries):\n            line_num = format_line_num_from_entry(log_entries[entry_idx])\n        logging.debug(f\"Completed Parsing CF Options ([{cf_name}] {line_num}\"\n                      f\"duplicate:{duplicate_option})\")\n\n        return cf_name, options_dict, table_options_dict, entry_idx, \\\n            duplicate_option", ""]}
{"filename": "log_entry.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport re\n\nimport regexes\nimport utils", "import regexes\nimport utils\n\n\nclass LogEntry:\n    @staticmethod\n    def is_entry_start(log_line, regex=None):\n        token_list = log_line.strip().split()\n        if not token_list:\n            return False\n\n        # The assumption is that a new log will start with a date\n        if not re.findall(regexes.TIMESTAMP, token_list[0]):\n            return False\n\n        if regex:\n            # token_list[1] should be the context\n            if not re.findall(regex, \" \".join(token_list[2:])):\n                return False\n\n        return True\n\n    @staticmethod\n    def validate_entry_start(line_idx, log_line):\n        if not LogEntry.is_entry_start(log_line):\n            raise utils.ParsingAssertion(\n                \"Line isn't entry's start.\",\n                utils.ErrorContext(**{'log_line': log_line,\n                                      'log_line_idx': line_idx}))\n\n    def validate_finalized(self):\n        if not self.is_finalized:\n            raise utils.ParsingAssertion(\n                f\"Entry already finalized. {self}\")\n\n    def __init__(self, line_idx, log_line, last_line=False):\n        LogEntry.validate_entry_start(line_idx, log_line)\n\n        self.is_finalized = False\n        self.start_line_idx = line_idx\n\n        # Try to parse as a warning line\n        parts = re.findall(regexes.START_LINE_WITH_WARN_PARTS, log_line)\n        if parts:\n            num_parts_expected = 6\n        else:\n            # Not a warning line => Parse as \"normal\" line\n            parts = re.findall(regexes.START_LINE_PARTS, log_line)\n            if not parts:\n                raise utils.ParsingError(\n                    \"Failed parsing Log Entry start line.\",\n                    utils.ErrorContext(**{'log_line': log_line,\n                                          'log_line_idx': line_idx}))\n            num_parts_expected = 5\n\n        assert len(parts) == 1 and len(parts[0]) == num_parts_expected, \\\n            f\"Unexpected # of parts (expected {num_parts_expected}) ({parts})\"\n\n        parts = parts[0]\n\n        self.time = parts[0]\n        self.context = parts[1]\n        self.orig_time = parts[2]\n\n        # warn msg\n        if num_parts_expected == 6:\n            self.warning_type = utils.WarningType(parts[3])\n            part_increment = 1\n        else:\n            self.warning_type = None\n            part_increment = 0\n\n        # File + Line in a source file\n        # example: '... [/column_family.cc: 932] ...'\n        self.code_pos = parts[3 + part_increment]\n        if self.code_pos:\n            code_pos_value_match = re.findall(r\"\\[(.*)\\]\", self.code_pos)\n            if code_pos_value_match:\n                self.code_pos = code_pos_value_match[0]\n\n        # Rest of 1st line's text starts the msg_lines part\n        self.msg_lines = list()\n        if parts[4 + part_increment]:\n            # self.msg_lines.append(parts[4 + part_increment].strip())\n            self.msg_lines.append(parts[4 + part_increment])\n\n        self.cf_name = None\n        self.job_id = None\n        self.try_parsing_cf_name_and_job_id(log_line)\n\n        if last_line:\n            self.all_lines_added()\n\n    def __str__(self):\n        return f\"LogEntry (lines:{self.get_lines_idxs_range()}), Start:\\n\" \\\n               f\"{self.msg_lines[0]}\"\n\n    def try_parsing_cf_name_and_job_id(self, log_line):\n        match = re.findall(regexes.CF_WITH_JOB_ID, log_line)\n        if not match:\n            return\n        assert len(match) == 1 and len(match[0]) == 2\n\n        self.cf_name, self.job_id = match[0]\n        self.job_id = int(self.job_id)\n\n    def validate_not_finalized(self, log_line=None):\n        if self.is_finalized:\n            msg = \"Entry already finalized.\"\n            if log_line:\n                msg += f\". Added line:\\n{log_line}\\n\"\n            msg += f\"\\n{self}\"\n            raise utils.ParsingAssertion(msg, self.start_line_idx)\n\n    def validate_not_adding_entry_start_line(self, log_line):\n        if LogEntry.is_entry_start(log_line):\n            raise utils.ParsingAssertion(\n                f\"Illegal attempt to add an entry start as a line to \"\n                f\"an existing entry. Line:\\n{log_line}\\n{self}\")\n\n    def add_line(self, log_line, last_line=False):\n        self.validate_not_finalized(log_line)\n        self.validate_not_adding_entry_start_line(log_line)\n\n        # self.msg_lines.append(log_line.strip())\n        self.msg_lines.append(log_line)\n        if last_line:\n            self.all_lines_added()\n\n    def all_lines_added(self):\n        self.validate_not_finalized()\n        assert not self.is_finalized\n\n        self.is_finalized = True\n        return self\n\n    def have_all_lines_been_added(self):\n        return self.is_finalized\n\n    def get_start_line_idx(self):\n        return self.start_line_idx\n\n    def get_start_line_num(self):\n        return self.get_start_line_idx() + 1\n\n    def get_end_line_idx(self):\n        return self.start_line_idx + len(self.msg_lines) - 1\n\n    def get_end_line_num(self):\n        return self.get_end_line_idx() + 1\n\n    def get_lines_idxs_range(self):\n        return self.start_line_idx, self.start_line_idx+len(self.msg_lines)\n\n    def get_time(self):\n        return self.time\n\n    def get_gmt_timestamp(self):\n        return utils.get_gmt_timestamp_us(self.time)\n\n    def get_code_pos(self):\n        return self.code_pos\n\n    def get_msg_lines(self):\n        return [line.strip() for line in self.msg_lines]\n\n    def get_non_stripped_msg_lines(self):\n        return self.msg_lines\n\n    def get_msg(self):\n        return \"\\n\".join(self.get_msg_lines()).strip()\n\n    def get_non_stripped_msg(self):\n        return \"\\n\".join(self.msg_lines)\n\n    def is_warn_msg(self):\n        return self.warning_type\n\n    def get_warning_type(self):\n        return self.warning_type\n\n    def get_cf_name(self):\n        return self.cf_name\n\n    def get_job_id(self):\n        return self.job_id", ""]}
{"filename": "console_outputter.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport io\nimport logging\n\nimport display_utils", "\nimport display_utils\nimport log_file\nimport utils\nfrom log_file import ParsedLog\n\n\ndef get_title(log_file_path, parsed_log):\n    return f\"Parsing of: {log_file_path}\"\n", "\n\ndef print_title(f, log_file_path, parsed_log):\n    title = get_title(log_file_path, parsed_log)\n    print(f\"{title}\", file=f)\n    print(len(title) * \"=\", file=f)\n\n\ndef print_cf_console_printout(f, parsed_log, db_size_msg_suffix):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    cfs_info_for_display = \\\n        display_utils.prepare_general_cf_info_for_display(parsed_log)\n\n    cfs_display_values = []\n    for cf_name, cf_info in cfs_info_for_display.items():\n        row = [\n            cf_name,\n            cf_info[\"CF Size\"],\n            cf_info[\"Avg. Key Size\"],\n            cf_info[\"Avg. Value Size\"],\n            cf_info[\"Compaction Style\"],\n            cf_info[\"Compression\"],\n            cf_info[\"Filter-Policy\"]\n        ]\n        cfs_display_values.append(row)\n\n    size_suffix = \"\"\n    if db_size_msg_suffix is not None:\n        size_suffix = f\"({db_size_msg_suffix})\"\n\n    table_header = [\"Column Family\",\n                    f\"Size {size_suffix}\",\n                    \"Avg. Key Size\",\n                    \"Avg. Value Size\",\n                    \"Compaction Style\",\n                    \"Compression\",\n                    \"Filter-Policy\"]\n    ascii_table = display_utils.generate_ascii_table(table_header,\n                                                     cfs_display_values)\n    print(ascii_table, file=f)", "def print_cf_console_printout(f, parsed_log, db_size_msg_suffix):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    cfs_info_for_display = \\\n        display_utils.prepare_general_cf_info_for_display(parsed_log)\n\n    cfs_display_values = []\n    for cf_name, cf_info in cfs_info_for_display.items():\n        row = [\n            cf_name,\n            cf_info[\"CF Size\"],\n            cf_info[\"Avg. Key Size\"],\n            cf_info[\"Avg. Value Size\"],\n            cf_info[\"Compaction Style\"],\n            cf_info[\"Compression\"],\n            cf_info[\"Filter-Policy\"]\n        ]\n        cfs_display_values.append(row)\n\n    size_suffix = \"\"\n    if db_size_msg_suffix is not None:\n        size_suffix = f\"({db_size_msg_suffix})\"\n\n    table_header = [\"Column Family\",\n                    f\"Size {size_suffix}\",\n                    \"Avg. Key Size\",\n                    \"Avg. Value Size\",\n                    \"Compaction Style\",\n                    \"Compression\",\n                    \"Filter-Policy\"]\n    ascii_table = display_utils.generate_ascii_table(table_header,\n                                                     cfs_display_values)\n    print(ascii_table, file=f)", "\n\ndef print_general_info(f, parsed_log: ParsedLog):\n    disp_dict = \\\n        display_utils.prepare_db_wide_info_for_display(parsed_log)\n\n    if isinstance(disp_dict[\"Error Messages\"], dict):\n        error_lines = \"\"\n        for error_time, error_msg in \\\n                disp_dict[\"Error Messages\"].items():\n            error_lines += f\"\\n{error_time} {error_msg}\"\n        disp_dict[\"Error Messages\"] = error_lines\n\n    if isinstance(disp_dict[\"Fatal Messages\"], dict):\n        error_lines = \"\"\n        for error_time, error_msg in \\\n                disp_dict[\"Fatal Messages\"].items():\n            error_lines += f\"\\n{error_time} {error_msg}\"\n        disp_dict[\"Fatal Messages\"] = error_lines\n\n    suffix = \"\"\n\n    msg1 = None\n    db_size_msg_suffix = None\n    db_size_time = disp_dict[\"DB Size Time\"]\n    if disp_dict[\"DB Size Time\"] is not None:\n        suffix += \"*\"\n        db_size_msg_suffix = suffix\n        disp_dict = \\\n            utils.replace_key_keep_order(disp_dict,\n                                         \"DB Size\",\n                                         f\"DB Size ({suffix})\")\n        msg1 = f\"({suffix}) Data is calculated at: {db_size_time}\"\n    del disp_dict[\"DB Size Time\"]\n\n    msg2 = None\n    ingest_time = disp_dict[\"Ingest Time\"]\n    if ingest_time is not None:\n        if db_size_time != ingest_time:\n            suffix += \"*\"\n            msg2 = f\"({suffix}) Ingest Data are calculated at: {ingest_time}\"\n        disp_dict = \\\n            utils.replace_key_keep_order(\n                disp_dict, \"Ingest\", f\"Ingest ({suffix})\")\n    del disp_dict[\"Ingest Time\"]\n\n    msg3 = None\n    num_cfs_info_key = \"Num CF-s Info\"\n    if \"Num CF-s Info\" in disp_dict:\n        suffix += \"*\"\n        disp_dict = \\\n            utils.replace_key_keep_order(\n                disp_dict, \"Num CF-s\", f\"Num CF-s ({suffix})\")\n        num_cfs_info = disp_dict[num_cfs_info_key]\n        msg3 = f\"({suffix}) {num_cfs_info}\"\n        del disp_dict[num_cfs_info_key]\n\n    width = 25\n    for field_name, value in disp_dict.items():\n        print(f\"{field_name.ljust(width)}: {value}\", file=f)\n    print_cf_console_printout(f, parsed_log, db_size_msg_suffix)\n\n    if msg1 is not None:\n        print(msg1, file=f)\n    if msg2 is not None:\n        print(msg2, file=f)\n    if msg3 is not None:\n        print(msg3, file=f)", "\n\ndef get_console_output(log_file_path, parsed_log, output_type):\n    logging.debug(f\"Preparing {output_type} Console Output\")\n\n    f = io.StringIO()\n\n    if output_type == utils.ConsoleOutputType.SHORT:\n        print_title(f, log_file_path, parsed_log)\n        print_general_info(f, parsed_log)\n\n    return f.getvalue()", ""]}
{"filename": "log_parser.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport argparse\nimport io\nimport logging\nimport logging.config", "import logging\nimport logging.config\nimport os\nimport pathlib\nimport shutil\nimport sys\nimport textwrap\nimport threading\nimport time\nfrom pathlib import Path", "import time\nfrom pathlib import Path\n\nimport console_outputter\nimport csv_outputter\nimport json_outputter\nimport utils\nfrom log_file import ParsedLog\n\nDEBUG_MODE = True", "\nDEBUG_MODE = True\n\n# event to signal the process bar thread to exit\nexit_event = threading.Event()\n\n\ndef exit_program(exit_code):\n    exit_event.set()\n    exit(exit_code)", "\n\ndef display_process_bar():\n    # Simple textual process bar that starts \"playing\" once parsing\n    # exceeds 1 second\n    time_from_last_print = 0\n\n    line = \"Parsing \"\n    while True:\n        time.sleep(0.05)\n        if exit_event.is_set():\n            break\n        time_from_last_print += 50\n        if time_from_last_print >= 1000:\n            line += \".\"\n            print(line, end='\\r')\n            sys.stdout.flush()\n            time_from_last_print = 0", "\n\ndef parse_log(log_file_path):\n    if not os.path.isfile(log_file_path):\n        raise utils.LogFileNotFoundError(log_file_path)\n\n    logging.debug(f\"Parsing {log_file_path}\")\n    with open(log_file_path) as log_file:\n        logging.debug(f\"Starting to read the contents of {log_file_path}\")\n        log_lines = log_file.readlines()\n        logging.debug(f\"Completed reading the contents of {log_file_path}\")\n\n        return ParsedLog(log_file_path, log_lines,\n                         should_init_baseline_info=True)", "\n\ndef setup_cmd_line_parser():\n    epilog = textwrap.dedent('''\\\n    Notes:\n    - The default it to print to the console in a short format.\n    - It is possible to specify both json and console outputs. Both will be generated.\n    ''')  # noqa\n\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=epilog)\n    parser.add_argument(\"log_file_path\",\n                        metavar=\"log-file-path\",\n                        help=\"A path to a log file to parse \"\n                             \"(default: %(default)s)\")\n    parser.add_argument(\"-c\", \"--console\",\n                        choices=[\"short\", \"long\"],\n                        help=\"Print to console a summary (short) or a \"\n                             \"detailed (long) output (default: %(default)s)\")\n    parser.add_argument(\"-j\", \"--generate-json\",\n                        action=\"store_true\",\n                        default=False,\n                        help=f\"Optionally generate a JSON file in the \"\n                             f\"output folder with detailed information. \"\n                             f\"If generated, it will be called \"\n                             f\"{utils.DEFAULT_JSON_FILE_NAME}.\"\n                             f\"(default: %(default)s)\")\n    parser.add_argument(\"-o\", \"--output-folder\",\n                        default=utils.DEFAULT_OUTPUT_FOLDER,\n                        help=\"The name of the folder where output \"\n                             \"files will be stored in SUB-FOLDERS \"\n                             f\"named \"\n                             f\"{utils.OUTPUT_SUB_FOLDER_PREFIX}dddd.\"\n                             \"'dddd' is the run number (default: %(default)s)\")\n    parser.add_argument(\"-l\", \"--generate-log\",\n                        action=\"store_true\",\n                        default=False,\n                        help=\"Generate a log file for the parser's log \"\n                             \"messages (default: %(default)s)\")\n    return parser", "\n\ndef validate_and_sanitize_cmd_line_args(cmdline_args):\n    # The default is short console output\n    if not cmdline_args.console and not cmdline_args.generate_json:\n        cmdline_args.console = utils.ConsoleOutputType.SHORT\n\n\ndef handle_exception(exception, console, should_exit):\n    logging.exception(f\"\\n{exception}\")\n    if console:\n        if hasattr(exception, 'msg'):\n            print(exception.msg, file=sys.stderr)\n        else:\n            print(exception, file=sys.stderr)\n    if should_exit:\n        exit_program(1)", "def handle_exception(exception, console, should_exit):\n    logging.exception(f\"\\n{exception}\")\n    if console:\n        if hasattr(exception, 'msg'):\n            print(exception.msg, file=sys.stderr)\n        else:\n            print(exception, file=sys.stderr)\n    if should_exit:\n        exit_program(1)\n", "\n\ndef report_exception(exception, console):\n    handle_exception(exception, console, should_exit=True)\n\n\ndef fatal_exception(exception, console=True):\n    handle_exception(exception, console, should_exit=True)\n\n\ndef verify_min_python_version():\n    if sys.version_info.major < utils.MIN_PYTHON_VERSION_MAJOR or \\\n            sys.version_info.minor < utils.MIN_PYTHON_VERSION_MINOR:\n        msg = f\"The log parser tool requires Python Version >= \" \\\n              f\"{utils.MIN_PYTHON_VERSION_MAJOR}.\" \\\n              f\"{utils.MIN_PYTHON_VERSION_MINOR} \" \\\n              f\"(Yours is {sys.version_info.major}.{sys.version_info.minor})\"\n        print(msg, file=sys.stderr)\n        exit_program(1)", "\n\ndef verify_min_python_version():\n    if sys.version_info.major < utils.MIN_PYTHON_VERSION_MAJOR or \\\n            sys.version_info.minor < utils.MIN_PYTHON_VERSION_MINOR:\n        msg = f\"The log parser tool requires Python Version >= \" \\\n              f\"{utils.MIN_PYTHON_VERSION_MAJOR}.\" \\\n              f\"{utils.MIN_PYTHON_VERSION_MINOR} \" \\\n              f\"(Yours is {sys.version_info.major}.{sys.version_info.minor})\"\n        print(msg, file=sys.stderr)\n        exit_program(1)", "\n\ndef setup_logger(generate_log, output_folder):\n    if not generate_log:\n        logging.root.setLevel(logging.FATAL)\n        return\n\n    my_log_file_path = utils.get_log_file_path(output_folder)\n    logging.basicConfig(filename=my_log_file_path,\n                        format=\"%(asctime)s - %(levelname)s [%(filename)s \"\n                               \"(%(funcName)s:%(lineno)d)] %(message)s)\",\n                        level=logging.DEBUG)\n    return my_log_file_path", "\n\ndef prepare_output_folder(output_folder_parent):\n    output_path_parent = pathlib.Path(output_folder_parent)\n    largest_num = 0\n    if output_path_parent.exists():\n        for file in output_path_parent.iterdir():\n            name = file.name\n            if name.startswith(utils.OUTPUT_SUB_FOLDER_PREFIX):\n                name = name[len(utils.OUTPUT_SUB_FOLDER_PREFIX):]\n                if name.isnumeric() and len(name) == 4:\n                    num = int(name)\n                    largest_num = max(largest_num, num)\n\n        if largest_num == 9999:\n            largest_num = 1\n\n    output_folder = f\"{output_folder_parent}/\" \\\n                    f\"{utils.OUTPUT_SUB_FOLDER_PREFIX}\" \\\n                    f\"{largest_num + 1:04}\"\n    if not output_path_parent.exists():\n        os.makedirs(output_folder_parent)\n    shutil.rmtree(output_folder, ignore_errors=True)\n    os.makedirs(output_folder)\n    return output_folder", "\n\ndef print_to_console_if_applicable(cmdline_args, log_file_path,\n                                   parsed_log, json_content):\n    f = io.StringIO()\n    console_content = None\n    if cmdline_args.console == utils.ConsoleOutputType.SHORT:\n        f.write(\n            console_outputter.get_console_output(\n                log_file_path,\n                parsed_log,\n                cmdline_args.console))\n        console_content = f.getvalue()\n    elif cmdline_args.console == utils.ConsoleOutputType.LONG:\n        console_content = json_outputter.get_json_dump_str(json_content)\n\n    if console_content is not None:\n        print()\n        print(console_content)", "\n\ndef generate_json_if_applicable(\n        cmdline_args, parsed_log, csvs_paths, output_folder,\n        report_to_console):\n    json_content = None\n    if cmdline_args.generate_json or \\\n            cmdline_args.console == utils.ConsoleOutputType.LONG:\n        json_content = json_outputter.get_json(parsed_log)\n        json_content[\"CSV-s\"] = csvs_paths\n\n        if cmdline_args.generate_json:\n            json_file_name = utils.DEFAULT_JSON_FILE_NAME\n            json_outputter.write_json(json_file_name,\n                                      json_content, output_folder,\n                                      report_to_console)\n\n    return json_content", "\n\ndef generate_csvs_if_applicable(parsed_log, output_folder, report_to_console):\n    assert isinstance(parsed_log, ParsedLog)\n\n    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n    events_mngr = parsed_log.get_events_mngr()\n    stats_mngr = parsed_log.get_stats_mngr()\n    compactions_monitor = parsed_log.get_compactions_monitor()\n\n    counters_mngr = \\\n        parsed_log.get_counters_mngr()\n    counters_csv_path = csv_outputter.generate_counters_csv(\n        counters_mngr, output_folder, report_to_console)\n    human_readable_histograms_csv_file_path, tools_histograms_csv_file_path = \\\n        csv_outputter.generate_histograms_csv(\n            counters_mngr, output_folder, report_to_console)\n\n    compaction_stats_mngr = stats_mngr.get_compactions_stats_mngr()\n    compactions_stats_csv_path = csv_outputter.generate_compactions_stats_csv(\n        compaction_stats_mngr, output_folder, report_to_console)\n\n    compactions_csv_path = csv_outputter.generate_compactions_csv(\n        compactions_monitor, output_folder, report_to_console)\n\n    flushes_csv_path = csv_outputter.generate_flushes_csv(\n        cfs_names, events_mngr, output_folder, report_to_console)\n\n    def generate_disp_path(path):\n        if path is None:\n            return utils.FILE_NOT_GENERATED_TEXT\n\n        assert isinstance(path, Path)\n        return str(path)\n\n    return {\n        \"Counters\": generate_disp_path(counters_csv_path),\n        \"Histograms (Human-Readable)\":\n            generate_disp_path(human_readable_histograms_csv_file_path),\n        \"Histograms (Tools)\":\n            generate_disp_path(tools_histograms_csv_file_path),\n        \"Compactions-Stats\": generate_disp_path(compactions_stats_csv_path),\n        \"Compactions\": generate_disp_path(compactions_csv_path),\n        \"Flushes\": generate_disp_path(flushes_csv_path)\n    }", "\n\ndef main():\n    verify_min_python_version()\n    parser = setup_cmd_line_parser()\n    cmdline_args = parser.parse_args()\n\n    output_folder = cmdline_args.output_folder\n\n    output_folder = prepare_output_folder(output_folder)\n    my_log_file_path = setup_logger(cmdline_args.generate_log,\n                                    output_folder)\n    validate_and_sanitize_cmd_line_args(cmdline_args)\n\n    t = threading.Thread(target=display_process_bar)\n    t.start()\n\n    try:\n        log_file_path = cmdline_args.log_file_path\n        log_file_path = os.path.abspath(log_file_path)\n        parsed_log = parse_log(log_file_path)\n\n        exit_event.set()\n\n        if cmdline_args.console == utils.ConsoleOutputType.LONG:\n            report_to_console = False\n        else:\n            report_to_console = True\n\n        if report_to_console:\n            log_file_path_str = parsed_log.get_log_file_path()\n            print(f\"Log file: {str(Path(log_file_path_str).as_uri())}\")\n\n            baseline_info = parsed_log.get_baseline_info()\n            if baseline_info is not None:\n                print(f\"Baseline Log: \"\n                      f\"{str(baseline_info.baseline_log_path.as_uri())}\")\n            else:\n                print(\"No Available Baseline Log\")\n\n        csvs_paths = generate_csvs_if_applicable(parsed_log, output_folder,\n                                                 report_to_console)\n        json_content = generate_json_if_applicable(\n            cmdline_args, parsed_log, csvs_paths, output_folder,\n            report_to_console)\n        print_to_console_if_applicable(cmdline_args, log_file_path, parsed_log,\n                                       json_content)\n    except utils.ParsingError as exception:\n        report_exception(exception, console=True)\n    except utils.LogFileNotFoundError as exception:\n        report_exception(exception, console=True)\n    except utils.EmptyLogFile as exception:\n        report_exception(exception, console=True)\n    except utils.InvalidLogFile as exception:\n        report_exception(exception, console=True)\n    except ValueError as exception:\n        fatal_exception(exception)\n    except AssertionError as exception:\n        if not DEBUG_MODE:\n            fatal_exception(exception)\n        else:\n            exit_event.set()\n            raise\n    except Exception as exception:  # noqa\n        if not DEBUG_MODE:\n            print(f\"An unrecoverable error occurred while parsing \"\n                  f\"{log_file_path}.\", file=sys.stderr)\n            print(\"Please open an issue in the tool's GitHub repository \"\n                  \"(https://github.com/speedb-io/log-parser)\", file=sys.stderr)\n            if my_log_file_path:\n                print(f\"\\nMore details may be found in {my_log_file_path}\",\n                      file=sys.stderr)\n\n            fatal_exception(exception, console=False)\n        else:\n            exit_event.set()\n            raise", "\n\nif __name__ == '__main__':\n    main()\n"]}
{"filename": "display_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport io\nimport logging\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path", "from dataclasses import dataclass, asdict\nfrom pathlib import Path\n\nimport baseline_log_files_utils\nimport cache_utils\nimport calc_utils\nimport db_files\nimport db_options\nimport log_file\nimport utils", "import log_file\nimport utils\nfrom counters import CountersMngr\nfrom db_options import DatabaseOptions, CfsOptionsDiff, SectionType\nfrom db_options import SanitizedValueType\nfrom stats_mngr import CompactionStatsMngr, StatsMngr\nfrom warnings_mngr import WarningType, WarningElementInfo, WarningsMngr\n\nnum_for_display = utils.get_human_readable_number\nnum_bytes_for_display = utils.get_human_readable_num_bytes", "num_for_display = utils.get_human_readable_number\nnum_bytes_for_display = utils.get_human_readable_num_bytes\n\nCFS_COMMON_KEY = \"CF-s (Common)\"\nCFS_SPECIFIC_KEY = \"CF-s (Specific)\"\nTABLE_KEY = \"Block-Based Table\"\n\n\ndef format_value(value, suffix=None, conv_func=None):\n    if value is not None:\n        if conv_func is not None:\n            value = conv_func(value)\n        if suffix is not None:\n            suffix = \" \" + suffix\n        else:\n            suffix = \"\"\n        return f\"{value}{suffix}\"\n    else:\n        return \"No Information\"", "def format_value(value, suffix=None, conv_func=None):\n    if value is not None:\n        if conv_func is not None:\n            value = conv_func(value)\n        if suffix is not None:\n            suffix = \" \" + suffix\n        else:\n            suffix = \"\"\n        return f\"{value}{suffix}\"\n    else:\n        return \"No Information\"", "\n\ndef prepare_db_wide_user_opers_stats_for_display(db_wide_info):\n    display_info = dict()\n\n    def get_disp_value(percent, num, total_num, oper_name,\n                       unavailability_reason):\n        if unavailability_reason is None:\n            assert total_num is not None\n            if total_num > 0 and num > 0:\n                return f\"{percent:.1f}% ({num}/{total_num})\"\n            else:\n                return f\"0 (No {oper_name} Operations)\"\n        else:\n            return f\"{utils.DATA_UNAVAILABLE_TEXT} ({unavailability_reason})\"\n\n    user_opers_stats = db_wide_info[\"user_opers_stats\"]\n    assert isinstance(user_opers_stats, calc_utils.UserOpersStats)\n\n    total_num_user_opers = user_opers_stats.total_num_user_opers\n    reason = user_opers_stats.unavailability_reason\n    display_info['Writes'] = \\\n        get_disp_value(user_opers_stats.percent_written,\n                       user_opers_stats.num_written,\n                       total_num_user_opers,\n                       \"Write\", reason)\n    display_info['Reads'] = \\\n        get_disp_value(user_opers_stats.percent_read,\n                       user_opers_stats.num_read,\n                       total_num_user_opers,\n                       \"Read\", reason)\n    display_info['Seeks'] = \\\n        get_disp_value(user_opers_stats.percent_seek,\n                       user_opers_stats.num_seek,\n                       total_num_user_opers,\n                       \"Seek\", reason)\n\n    delete_opers_stats = db_wide_info[\"delete_opers_stats\"]\n    assert isinstance(delete_opers_stats, calc_utils.DeleteOpersStats)\n\n    display_info['Deleted (Flushed) Entries'] = \\\n        get_disp_value(delete_opers_stats.total_percent_deletes,\n                       delete_opers_stats.total_num_deletes,\n                       delete_opers_stats.total_num_flushed_entries,\n                       \"Delete\",\n                       delete_opers_stats.unavailability_reason)\n\n    return display_info", "\n\n@dataclass\nclass NotableEntityInfo:\n    display_title: str\n    display_text: str\n    special_value_type: SanitizedValueType\n    special_value_text: str\n    display_value: bool\n", "\n\nnotable_entities = {\n    \"statistics\": NotableEntityInfo(\"Statistics\",\n                                    \"Available\",\n                                    SanitizedValueType.NULL_PTR,\n                                    utils.NO_STATS_TEXT,\n                                    display_value=False)\n}\n", "}\n\n\ndef get_db_wide_notable_entities_display_info(parsed_log):\n    display_info = {}\n    db_opts = parsed_log.get_database_options()\n    for option_name, info in notable_entities.items():\n        option_value = db_opts.get_db_wide_option(option_name)\n        if option_value is None:\n            logging.warning(f\"Option {option_name} not found in \"\n                            f\"{parsed_log.get_log_file_path()}\")\n            continue\n\n        option_value_type = SanitizedValueType.get_type_from_str(option_value)\n        if option_value_type == info.special_value_type:\n            display_value = info.special_value_text\n        else:\n            if info.display_text is None:\n                display_value = option_value\n            else:\n                if info.display_value:\n                    display_value = f\"{info.display_text} ({option_value})\"\n                else:\n                    display_value = info.display_text\n        display_info[info.display_title] = display_value\n\n    return display_info", "\n\ndef prepare_error_or_fatal_warnings_for_display(warnings_mngr, prepare_error):\n    assert isinstance(warnings_mngr, WarningsMngr)\n\n    if prepare_error:\n        all_type_warnings = \\\n            warnings_mngr.get_warnings_of_type(WarningType.ERROR)\n        if not all_type_warnings:\n            return \"No Errors\"\n    else:\n        all_type_warnings = \\\n            warnings_mngr.get_warnings_of_type(WarningType.FATAL)\n        if not all_type_warnings:\n            return \"No Fatals\"\n\n    warnings_tuples = list()\n    for cf_name, cf_info in all_type_warnings.items():\n        for category, err_infos in cf_info.items():\n            for info in err_infos:\n                assert isinstance(info, WarningElementInfo)\n                warnings_tuples.append((info.time, info.warning_msg))\n\n    # First item in every tuple is time => will be sorted on time\n    warnings_tuples.sort()\n\n    return {time: msg for time, msg in warnings_tuples}", "\n\ndef prepare_ingest_info_for_db_wide_info_display(db_wide_info):\n    ingest_info = db_wide_info[\"ingest_info\"]\n\n    if ingest_info is not None:\n        assert isinstance(ingest_info, calc_utils.DbIngestInfo)\n        return prepare_db_ingest_info_for_display(ingest_info)\n    else:\n        unavailability_reason = utils.NO_INGEST_TEXT\n        return {\n            \"Ingest\": unavailability_reason,\n            \"Ingest Rate\": unavailability_reason,\n            \"Ingest Time\": None\n        }", "\n\ndef prepare_db_wide_info_for_display(parsed_log):\n    log_file_time_info = calc_utils.get_log_file_time_info(parsed_log)\n    assert isinstance(log_file_time_info, calc_utils.LogFileTimeInfo)\n\n    display_info = {}\n\n    db_wide_info = calc_utils.get_db_wide_info(parsed_log)\n    db_wide_notable_entities = \\\n        get_db_wide_notable_entities_display_info(parsed_log)\n    display_info[\"Name\"] = str(Path(parsed_log.get_log_file_path()))\n    display_info[\"Start Time\"] = log_file_time_info.start_time\n    display_info[\"End Time\"] = log_file_time_info.end_time\n    display_info[\"Log Time Span\"] = \\\n        utils.convert_seconds_to_dd_hh_mm_ss(log_file_time_info.span_seconds)\n    display_info[\"Creator\"] = db_wide_info['creator']\n    display_info[\"Version\"] = f\"{db_wide_info['version']} \" \\\n                              f\"[{db_wide_info['git_hash']}]\"\n\n    db_size_bytes_time = db_wide_info['db_size_bytes_time']\n\n    if db_wide_info['db_size_bytes'] is not None:\n        display_info[\"DB Size\"] = \\\n            num_bytes_for_display(db_wide_info['db_size_bytes'])\n    else:\n        display_info[\"DB Size\"] = utils.DATA_UNAVAILABLE_TEXT\n\n    display_info[\"DB Size Time\"] = db_size_bytes_time\n\n    if db_wide_info[\"num_keys_written\"] is not None:\n        display_info[\"Num Keys Written\"] = \\\n            num_for_display(db_wide_info['num_keys_written'])\n    else:\n        display_info[\"Num Keys Written\"] = utils.DATA_UNAVAILABLE_TEXT\n\n    if db_wide_info['avg_key_size_bytes'] is not None:\n        display_info[\"Avg. Written Key Size\"] = \\\n            num_bytes_for_display(db_wide_info['avg_key_size_bytes'])\n    else:\n        display_info[\"Avg. Written Key Size\"] = utils.DATA_UNAVAILABLE_TEXT\n\n    if db_wide_info['avg_value_size_bytes'] is not None:\n        display_info[\"Avg. Written Value Size\"] = \\\n            num_bytes_for_display(db_wide_info['avg_value_size_bytes'])\n    else:\n        display_info[\"Avg. Written Value Size\"] = utils.DATA_UNAVAILABLE_TEXT\n\n    display_info[\"Num Warnings\"] = db_wide_info['num_warnings']\n\n    if db_wide_info['errors'] is not None:\n        display_info[\"Error Messages\"] = db_wide_info['errors']\n    else:\n        display_info[\"Error Messages\"] = \"No Error Messages\"\n\n    if db_wide_info['fatals'] is not None:\n        display_info[\"Fatal Messages\"] = db_wide_info['fatals']\n    else:\n        display_info[\"Fatal Messages\"] = \"No Fatal Messages\"\n\n    display_info.update(\n        prepare_ingest_info_for_db_wide_info_display(db_wide_info))\n\n    display_info.update(db_wide_notable_entities)\n    display_info.update(\n        prepare_db_wide_user_opers_stats_for_display(db_wide_info))\n\n    num_cfs_info_msg = \\\n        \"Please see the 'Ability to determine the number of cf-s' section in the log parser's documentation for more information\" # noqa\n    if db_wide_info['num_cfs'] is not None:\n        total_num_cfs = db_wide_info['num_cfs']\n        num_non_auto_gen_cfs_with_options = \\\n            parsed_log.get_cfs_names_that_have_options(\n                include_auto_generated=False)\n        display_info['Num CF-s'] = total_num_cfs\n        if total_num_cfs != len(num_non_auto_gen_cfs_with_options):\n            display_info[\"Num CF-s Info\"] = num_cfs_info_msg\n    else:\n        display_info['Num CF-s'] = \"Can't be accurately determined\"\n        display_info[\"Num CF-s Info\"] = num_cfs_info_msg\n\n    return display_info", "\n\ndef prepare_general_cf_info_for_display(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n\n    filter_stats = \\\n        calc_utils.calc_filter_stats(cfs_names,\n                                     parsed_log.get_database_options(),\n                                     parsed_log.get_files_monitor(),\n                                     parsed_log.get_counters_mngr())\n    display_info = {}\n\n    events_mngr = parsed_log.get_events_mngr()\n    compaction_stats_mngr = \\\n        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\n    for cf_name in cfs_names:\n        table_creation_stats = \\\n            calc_utils.calc_cf_table_creation_stats(cf_name, events_mngr)\n        cf_options = calc_utils.get_applicable_cf_options(\n            parsed_log.get_database_options())\n        cf_size_bytes = compaction_stats_mngr.get_cf_size_bytes_at_end(cf_name)\n\n        display_info[cf_name] = {}\n        cf_display_info = display_info[cf_name]\n\n        if cf_size_bytes is not None:\n            cf_display_info[\"CF Size\"] = num_bytes_for_display(cf_size_bytes)\n        else:\n            cf_display_info[\"CF Size\"] = utils.DATA_UNAVAILABLE_TEXT\n\n        cf_display_info[\"Avg. Key Size\"] = \\\n            num_bytes_for_display(table_creation_stats['avg_key_size'])\n        cf_display_info[\"Avg. Value Size\"] = \\\n            num_bytes_for_display(table_creation_stats['avg_value_size'])\n\n        if cf_name in cf_options['compaction_style']:\n            if cf_options['compaction_style'][cf_name] is not None:\n                cf_display_info[\"Compaction Style\"] = \\\n                    cf_options['compaction_style'][cf_name]\n            else:\n                cf_display_info[\"Compaction Style\"] = utils.UNKNOWN_VALUE_TEXT\n        else:\n            cf_display_info[\"Compaction Style\"] = utils.UNKNOWN_VALUE_TEXT\n\n        if cf_name in cf_options['compression']:\n            if cf_options['compression'][cf_name] is not None:\n                cf_display_info[\"Compression\"] = \\\n                    cf_options['compression'][cf_name]\n            else:\n                cf_display_info[\"Compression\"] = utils.UNKNOWN_VALUE_TEXT\n        elif calc_utils.is_cf_compression_by_level(parsed_log, cf_name):\n            cf_display_info[\"Compression\"] = \"Per-Level\"\n        else:\n            cf_display_info[\"Compression\"] = utils.UNKNOWN_VALUE_TEXT\n\n        if cf_name in filter_stats.files_filter_stats:\n            cf_files_filter_stats = filter_stats.files_filter_stats[cf_name]\n        else:\n            cf_files_filter_stats = None\n        cf_display_info[\"Filter-Policy\"] = \\\n            prepare_cf_filter_stats_for_display(cf_files_filter_stats,\n                                                format_as_dict=False)\n\n    return display_info", "\n\ndef prepare_warn_warnings_for_display(warn_warnings_info):\n    # The input is in the following format:\n    # {<warning-type>: {<cf-name>: {<category>: <number of messages>}}\n\n    disp = dict()\n    disp_db = dict()\n    disp_cfs = dict()\n    for cf_name, cf_info in warn_warnings_info.items():\n        if cf_name == utils.NO_CF:\n            disp_dict = disp_db\n        else:\n            disp_cfs[cf_name] = dict()\n            disp_dict = disp_cfs[cf_name]\n\n        for category, num_in_category in cf_info.items():\n            disp_category = category.value\n            disp_dict[disp_category] = num_in_category\n\n    if not disp_db and not disp_cfs:\n        return None\n\n    if disp_db:\n        disp[\"DB\"] = disp_db\n    else:\n        disp[\"DB\"] = \"No DB Warnings\"\n\n    if disp_cfs:\n        disp[\"CF-s\"] = disp_cfs\n    else:\n        disp[\"CF-s\"] = \"No CF-s Warnings\"\n\n    return disp", "\n\ndef prepare_cfs_common_options_for_display(cfs_common_options):\n    if cfs_common_options:\n        options, table_options = \\\n            DatabaseOptions.prepare_flat_full_names_cf_options_for_display(\n                cfs_common_options, None)\n        return {\n            \"CF\": options,\n            TABLE_KEY: table_options\n        }\n    else:\n        return \"No Common Options to All CF-s\"", "\n\ndef prepare_cfs_specific_options_for_display(cfs_specific_options):\n    disp = {}\n\n    if cfs_specific_options:\n        for cf_name, cf_options in cfs_specific_options.items():\n            if cf_options:\n                disp_cf_options, disp_cf_table_options =\\\n                    DatabaseOptions.\\\n                    prepare_flat_full_names_cf_options_for_display(\n                        cf_options, None)\n                disp[cf_name] = {}\n                if disp_cf_options:\n                    disp[cf_name][\"CF\"] = \\\n                        disp_cf_options\n                else:\n                    disp[cf_name][\"CF\"] = \\\n                        \"No Specific Options\"\n                if disp_cf_table_options:\n                    disp[cf_name][TABLE_KEY] = \\\n                        disp_cf_table_options\n                else:\n                    disp[cf_name][TABLE_KEY] = \\\n                        \"No Specific Table Options\"\n                if not disp[cf_name]:\n                    del(disp[cf_name])\n\n    if not disp:\n        disp = \"No Specific CF-s Options\"\n\n    return disp", "\n\ndef get_all_options_for_display(parsed_log):\n    all_options = {}\n\n    db_opts = parsed_log.get_database_options()\n\n    cfs_common_options, cfs_specific_options =  \\\n        calc_utils.get_cfs_common_and_specific_options(db_opts)\n\n    db_disp_opts = db_opts.get_db_wide_options_for_display()\n\n    cfs_disp_opts = dict()\n    cfs_disp_opts[CFS_COMMON_KEY] = \\\n        prepare_cfs_common_options_for_display(cfs_common_options)\n    cfs_disp_opts[CFS_SPECIFIC_KEY] = \\\n        prepare_cfs_specific_options_for_display(cfs_specific_options)\n\n    all_options[\"DB\"] = db_disp_opts\n    all_options[\"CF-s\"] = cfs_disp_opts\n\n    return all_options", "\n\ndef get_diff_tuple_for_display(raw_diff_tuple):\n    return {utils.DIFF_BASELINE_NAME: raw_diff_tuple[0],\n            utils.DIFF_LOG_NAME: raw_diff_tuple[1]}\n\n\ndef prepare_db_wide_diff_dict_for_display(\n        product_name, baseline_log_path, baseline_version, db_wide_diff):\n    display_db_wide_diff = {\n        \"Baseline\": f\"{str(baseline_version)} ({product_name})\",\n        \"Baseline Log\": str(baseline_log_path)\n    }\n\n    if db_wide_diff is None:\n        display_db_wide_diff[\"DB\"] = \"No Diff\"\n        return display_db_wide_diff\n\n    del (db_wide_diff[CfsOptionsDiff.CF_NAMES_KEY])\n    display_db_wide_diff[\"DB\"] = {}\n\n    for full_option_name in db_wide_diff:\n        section_type = SectionType.extract_section_type(full_option_name)\n        option_name = \\\n            db_options.extract_option_name(full_option_name)\n\n        if section_type == SectionType.DB_WIDE:\n            display_db_wide_diff[\"DB\"][option_name] = \\\n                get_diff_tuple_for_display(db_wide_diff[full_option_name])\n        elif section_type == SectionType.VERSION:\n            pass\n        else:\n            assert False, \"Unexpected section type\"\n\n    if not display_db_wide_diff[\"DB\"]:\n        del (display_db_wide_diff[\"DB\"])\n    if list(display_db_wide_diff.keys()) == [\"Baseline\"]:\n        display_db_wide_diff = {}\n\n    return display_db_wide_diff", "\n\ndef prepare_cfs_diff_dict_for_display(common_diff, cfs_specific_diffs):\n    display_cfs_diff = dict()\n\n    if common_diff:\n        assert isinstance(common_diff, db_options.CfsOptionsDiff)\n        common_diff_dict = common_diff.get_diff_dict()\n        del(common_diff_dict[db_options.CfsOptionsDiff.CF_NAMES_KEY])\n        options, table_options = \\\n            DatabaseOptions.prepare_flat_full_names_cf_options_for_display(\n                common_diff_dict, get_diff_tuple_for_display)\n        display_cfs_diff[CFS_COMMON_KEY] = {\n            \"CF\": options,\n            TABLE_KEY: table_options\n        }\n    else:\n        display_cfs_diff[CFS_COMMON_KEY] = \"No Common Diff\"\n\n    display_cfs_diff[CFS_SPECIFIC_KEY] = dict()\n    if cfs_specific_diffs:\n        for cf_name, cf_specific_diff in cfs_specific_diffs.items():\n            if cf_specific_diff is not None:\n                assert isinstance(cf_specific_diff, db_options.CfsOptionsDiff)\n\n                cf_specific_diff_dict = cf_specific_diff.get_diff_dict()\n                del (cf_specific_diff_dict[\n                    db_options.CfsOptionsDiff.CF_NAMES_KEY])\n                options, table_options = \\\n                    DatabaseOptions.\\\n                    prepare_flat_full_names_cf_options_for_display(\n                        cf_specific_diff_dict, get_diff_tuple_for_display)\n                display_cfs_diff[CFS_SPECIFIC_KEY][cf_name] = {\n                    \"CF\": options,\n                    TABLE_KEY: table_options\n                }\n    if not display_cfs_diff[CFS_SPECIFIC_KEY]:\n        display_cfs_diff[CFS_SPECIFIC_KEY] = \"No CF-s Specific Diff\"\n\n    return display_cfs_diff", "\n\ndef get_options_baseline_diff_for_display(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    log_metadata = parsed_log.get_metadata()\n    log_database_options = parsed_log.get_database_options()\n    baseline_info = parsed_log.get_baseline_info()\n\n    if baseline_info is None:\n        return \"NO BASELINE FOUND\"\n\n    assert isinstance(baseline_info,\n                      baseline_log_files_utils.BaselineDBOptionsInfo)\n\n    baseline_opts = baseline_info.baseline_options.get_all_options()\n    log_opts = log_database_options.get_all_options()\n\n    db_wide_diff = \\\n        DatabaseOptions.get_db_wide_options_diff(baseline_opts, log_opts)\n    if db_wide_diff is not None:\n        db_wide_diff = db_wide_diff.get_diff_dict()\n    display_diff = prepare_db_wide_diff_dict_for_display(\n        log_metadata.get_product_name(), baseline_info.baseline_log_path,\n        baseline_info.closest_version, db_wide_diff)\n\n    common_diff, cfs_specific_diffs = \\\n        calc_utils.get_cfs_common_and_specific_diff_dicts(\n            baseline_info.baseline_options, log_database_options)\n\n    display_diff[\"CF-s\"] = \\\n        prepare_cfs_diff_dict_for_display(common_diff, cfs_specific_diffs)\n\n    return display_diff", "\n\ndef prepare_cf_flushes_stats_for_display(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    disp = {}\n\n    def calc_sizes_histogram():\n        sizes_histogram = {}\n        bucket_min_size_mb = 0\n        for i, num_in_bucket in \\\n                enumerate(reason_stats.sizes_histogram):\n            if i < len(calc_utils.FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB):\n                bucket_max_size_mb = \\\n                    calc_utils.FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB[i]\n                bucket_title = f\"{bucket_min_size_mb} - \" \\\n                               f\"{bucket_max_size_mb} [MB]\"\n            else:\n                bucket_title = f\"> {bucket_min_size_mb} [MB]\"\n            bucket_min_size_mb = bucket_max_size_mb\n\n            sizes_histogram[bucket_title] = num_in_bucket\n        return sizes_histogram\n\n    def get_write_amp_level1():\n        cf_compaction_stats =\\\n            compactions_stats_mngr.get_cf_level_entries(cf_name)\n        if not cf_compaction_stats:\n            return None\n        last_dump_stats = cf_compaction_stats[-1]\n\n        return CompactionStatsMngr.get_level_field_value(\n            last_dump_stats, level=1,\n            field=CompactionStatsMngr.LevelFields.WRITE_AMP)\n\n    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n    events_mngr = parsed_log.get_events_mngr()\n    stats_mngr = parsed_log.get_stats_mngr()\n    compactions_stats_mngr = stats_mngr.get_compactions_stats_mngr()\n\n    for cf_name in cfs_names:\n        cf_disp = dict()\n\n        cf_flushes_stats = \\\n            calc_utils.calc_cf_flushes_stats(cf_name, events_mngr)\n        if not cf_flushes_stats:\n            continue\n\n        write_amp_level1 = get_write_amp_level1()\n        if not write_amp_level1:\n            write_amp_level1 = utils.DATA_UNAVAILABLE_TEXT\n        cf_disp[\"L0->L1 Write-Amp\"] = write_amp_level1\n\n        for reason, reason_stats in cf_flushes_stats.items():\n            assert isinstance(reason_stats, calc_utils.PerFlushReasonStats)\n\n            cf_reason_disp = dict()\n            cf_reason_disp[\"Sizes Histogram\"] = calc_sizes_histogram()\n            cf_reason_disp[\"Num Flushes\"] = \\\n                num_for_display(reason_stats.num_flushes)\n\n            cf_reason_disp[\"Min Duration\"] = \\\n                format_value(reason_stats.min_duration_ms,\n                             suffix=\"ms\",\n                             conv_func=None)\n\n            cf_reason_disp[\"Max Duration\"] = \\\n                format_value(reason_stats.max_duration_ms,\n                             suffix=\"ms\",\n                             conv_func=None)\n\n            cf_reason_disp[\"Min Num Memtables\"] = \\\n                format_value(reason_stats.min_num_memtables,\n                             suffix=None,\n                             conv_func=None)\n\n            cf_reason_disp[\"Max Num Memtables\"] = \\\n                format_value(reason_stats.max_num_memtables,\n                             suffix=None,\n                             conv_func=None)\n\n            cf_reason_disp[\"Min Total Data Size\"] = \\\n                format_value(reason_stats.min_total_data_size_bytes,\n                             suffix=None,\n                             conv_func=num_bytes_for_display)\n\n            cf_reason_disp[\"Max Total Data Size\"] = \\\n                format_value(reason_stats.max_total_data_size_bytes,\n                             suffix=None,\n                             conv_func=num_bytes_for_display)\n            cf_disp[reason] = cf_reason_disp\n        disp[cf_name] = cf_disp\n\n    return disp", "\n\ndef prepare_global_compactions_stats_for_display(parsed_log):\n    disp = {}\n    compactions_monitor = parsed_log.get_compactions_monitor()\n    largest_compaction_size_bytes = \\\n        calc_utils.get_largest_compaction_size_bytes(compactions_monitor)\n    disp[\"Largest compaction size\"] = \\\n        num_bytes_for_display(largest_compaction_size_bytes)\n    return disp", "\n\ndef prepare_cf_compactions_stats_for_display(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    disp = {}\n\n    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n    log_start_time = parsed_log.get_metadata().get_start_time()\n    compactions_monitor = parsed_log.get_compactions_monitor()\n    compactions_stats_mngr = \\\n        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\n    for cf_name in cfs_names:\n        cf_compactions_stats = \\\n            calc_utils.calc_cf_compactions_stats(\n                cf_name, log_start_time, compactions_monitor,\n                compactions_stats_mngr)\n\n        if cf_compactions_stats:\n            assert isinstance(cf_compactions_stats,\n                              calc_utils.CfCompactionStats)\n            s = cf_compactions_stats\n            if s.per_level_write_amp is not None:\n                per_level_write_amp = s.per_level_write_amp\n            else:\n                per_level_write_amp = \"No Write-Amp Info Found\"\n\n            disp[cf_name] = {\n                \"Num Compactions\": s.num_compactions,\n                \"Min Compactions BW\":\n                    format_value(s.min_compaction_bw_mbps, \"MBPS\"),\n                \"Max Compactions BW\":\n                    format_value(s.max_compaction_bw_mbps, \"MBPS\"),\n                \"Comp\": format_value(s.comp_sec, \"seconds\"),\n                \"Comp Merge CPU\": format_value(s.comp_merge_cpu_sec,\n                                               \"seconds\"),\n                \"Per-Level Write-Amp\": per_level_write_amp\n            }\n        else:\n            disp[cf_name] = \"No Compaction Stats\"\n\n    return disp", "\n\ndef prepare_cf_stalls_entries_for_display(parsed_log):\n    mngr = parsed_log.get_stats_mngr().get_cf_no_file_stats_mngr()\n    stall_counts = mngr.get_stall_counts()\n\n    display_stall_counts = {}\n    for cf_name in stall_counts.keys():\n        if stall_counts[cf_name]:\n            display_stall_counts[cf_name] = stall_counts[cf_name]\n\n    return display_stall_counts if display_stall_counts \\\n        else \"No Stalls\"", "\n\ndef generate_ascii_table(columns_names, table):\n    f = io.StringIO()\n\n    if len(table) < 1:\n        return\n\n    max_columns_widths = []\n    num_columns = len(columns_names)\n    for i in range(num_columns):\n        max_value_len = max([len(str(row[i])) for row in table])\n        column_name_len = len(columns_names[i])\n        max_columns_widths.append(2 + max([max_value_len, column_name_len]))\n\n    header_line = \"\"\n    for i, name in enumerate(columns_names):\n        width = max_columns_widths[i]\n        header_line += f'|{name.center(width)}'\n    header_line += '|'\n\n    print('-' * len(header_line), file=f)\n    print(header_line, file=f)\n    print('-' * len(header_line), file=f)\n\n    for row in table:\n        row_line = \"\"\n        for i, value in enumerate(row):\n            width = max_columns_widths[i]\n            row_line += f'|{str(value).center(width)}'\n        row_line += '|'\n        print(row_line, file=f)\n\n    print('-' * len(header_line), file=f)\n\n    return f.getvalue()", "\n\ndef prepare_cfs_size_bytes_growth_for_display(growth):\n    disp = {}\n    if not growth:\n        return utils.NO_GROWTH_INFO_TEXT\n\n    def get_delta_str(delta_value):\n        abs_delta_str = num_bytes_for_display(abs(delta_value))\n        if delta_value >= 0:\n            return f\"(+{abs_delta_str})\"\n        else:\n            return f\"(-{abs_delta_str})\"\n\n    def get_growth_str(start_value, end_value):\n        start_size_str = num_bytes_for_display(start_value)\n\n        if end_value is not None:\n            if start_value == end_value:\n                if start_value > 0:\n                    value_str = f\"{start_size_str} (No Change)\"\n                else:\n                    value_str = \"Empty Level\"\n            else:\n                end_size_str = num_bytes_for_display(end_value)\n                delta = end_value - start_value\n                delta_str = get_delta_str(delta)\n                value_str = \\\n                    f\"{start_size_str} -> {end_size_str}  {delta_str}\"\n        else:\n            # End size is unknown\n            value_str = f\"{start_size_str} -> (UNKNOWN SIZE)\"\n\n        return value_str\n\n    for cf_name in growth:\n        if growth[cf_name] is None:\n            disp[cf_name] = utils.NO_GROWTH_INFO_TEXT\n            continue\n\n        disp[cf_name] = {}\n\n        if not growth[cf_name]:\n            disp[cf_name] = utils.NO_GROWTH_INFO_TEXT\n            continue\n\n        total_bytes_start = 0\n        total_bytes_end = None\n        # The levelt are not ordered within growth[cf_name]\n        levels_and_sizes = list(growth[cf_name].items())\n        levels_and_sizes.sort()\n\n        for level, sizes_bytes in levels_and_sizes:\n            start_size_bytes = sizes_bytes[0]\n            end_size_bytes = sizes_bytes[1]\n\n            if start_size_bytes is None:\n                start_size_bytes = 0\n            # if end_size_bytes is None:\n            #     end_size_bytes = 0\n\n            disp[cf_name][f\"Level {level}\"] = get_growth_str(\n                start_size_bytes, end_size_bytes)\n\n            total_bytes_start += start_size_bytes\n            if end_size_bytes is not None:\n                if total_bytes_end is None:\n                    total_bytes_end = end_size_bytes\n                else:\n                    total_bytes_end += end_size_bytes\n\n        disp[cf_name][\"Sum\"] =\\\n            get_growth_str(total_bytes_start, total_bytes_end)\n\n    return disp", "\n\ndef prepare_db_ingest_info_for_display(ingest_info):\n    assert isinstance(ingest_info, calc_utils.DbIngestInfo)\n\n    disp = {}\n    if not ingest_info:\n        return \"No Ingest Info\"\n\n    disp[\"Ingest\"] = utils.get_human_readable_num_bytes(ingest_info.ingest)\n    disp[\"Ingest Rate\"] = f\"{ingest_info.ingest_rate_mbps} MBps\"\n    disp[\"Ingest Time\"] = ingest_info.time\n\n    return disp", "\n\ndef prepare_seek_stats_for_display(seek_stats):\n    assert isinstance(seek_stats, calc_utils.SeekStats)\n\n    disp = dict()\n    disp[\"Num Seeks\"] = num_for_display(seek_stats.num_seeks)\n    disp[\"Num Found Seeks\"] = num_for_display(seek_stats.num_found_seeks)\n    disp[\"Num Nexts\"] = num_for_display(seek_stats.num_nexts)\n    disp[\"Num Prevs\"] = num_for_display(seek_stats.num_prevs)\n    disp[\"Avg. Seek Range Size\"] = f\"{seek_stats.avg_seek_range_size:.1f}\"\n    disp[\"Avg. Seeks Rate Per Second\"] = \\\n        num_for_display(seek_stats.avg_seek_rate_per_second)\n    disp[\"Avg. Seek Latency\"] = f\"{seek_stats.avg_seek_latency_us:.1f} us\"\n\n    return disp", "\n\ndef prepare_cache_id_options_for_display(options):\n    assert isinstance(options, cache_utils.CacheOptions)\n\n    disp = dict()\n\n    disp[\"Capacity\"] = num_bytes_for_display(options.cache_capacity_bytes)\n    disp[\"Num Shards\"] = 2 ** options.num_shard_bits\n    disp[\"Shard Size\"] = num_bytes_for_display(options.shard_size_bytes)\n    disp[\"CF-s\"] = \\\n        {cf_name: asdict(cf_options) for cf_name, cf_options in\n         options.cfs_specific_options.items()}\n\n    return disp", "\n\ndef prepare_block_stats_of_cache_for_display(block_stats):\n    assert isinstance(block_stats, db_files.BlockLiveFileStats)\n\n    disp = dict()\n\n    disp[\"Avg. Size\"] = \\\n        num_bytes_for_display(int(block_stats.get_avg_block_size()))\n    disp[\"Max Size\"] = num_bytes_for_display(block_stats.max_size_bytes)\n    disp[\"Max Size At\"] = block_stats.max_size_time\n\n    return disp", "\n\ndef prepare_block_cache_info_for_display(cache_info):\n    assert isinstance(cache_info, cache_utils.CacheIdInfo)\n\n    disp = dict()\n    disp.update(prepare_cache_id_options_for_display(cache_info.options))\n\n    blocks_stats = cache_info.files_stats.blocks_stats\n    disp[\"Index Block\"] = \\\n        prepare_block_stats_of_cache_for_display(\n            blocks_stats[db_files.BlockType.INDEX])\n    if blocks_stats[db_files.BlockType.FILTER].num_created > 0:\n        disp[\"Filter Block\"] = \\\n            prepare_block_stats_of_cache_for_display(\n                blocks_stats[db_files.BlockType.FILTER])\n    else:\n        disp[\"Filter Block\"] = \"No Stats (Filters not in use)\"\n\n    return disp", "\n\ndef prepare_block_counters_for_display(cache_counters):\n    assert isinstance(cache_counters, cache_utils.CacheCounters)\n\n    disp = asdict(cache_counters)\n    disp = {key: num_for_display(value) for key, value in disp.items()}\n    return disp\n\n", "\n\n# TODO: The detailed display should be moved to its own csv\n# TODO: The cache id in the detailed should not include the process id so it\n#  matches the non-detailed display\ndef prepare_detailed_block_cache_stats_for_display(detailed_block_cache_stats):\n    disp_stats = detailed_block_cache_stats.copy()\n    for cache_stats in disp_stats.values():\n        cache_stats['Capacity'] = \\\n            num_bytes_for_display(cache_stats['Capacity'])\n        cache_stats['Usage'] = num_bytes_for_display(cache_stats['Usage'])\n        for cache_stats_key, entry in cache_stats.items():\n            if utils.parse_time_str(cache_stats_key, expect_valid_str=False):\n                entry['Usage'] = num_bytes_for_display(entry['Usage'])\n                for entry_key, role_values in entry.items():\n                    if entry_key == 'CF-s':\n                        for cf_name in entry['CF-s']:\n                            for role, cf_role_value in \\\n                                    entry['CF-s'][cf_name].items():\n                                entry['CF-s'][cf_name][role] =\\\n                                    num_bytes_for_display(cf_role_value)\n                    elif entry_key != 'Usage':\n                        role_values['Size'] =\\\n                            num_bytes_for_display(role_values['Size'])\n    return disp_stats", "\n\ndef prepare_block_cache_stats_for_display(cache_stats,\n                                          detailed_block_cache_stats):\n    assert isinstance(cache_stats, cache_utils.CacheStats)\n\n    disp = dict()\n    if cache_stats.per_cache_id_info:\n        disp[\"Caches\"] = {}\n        for cache_id, cache_info in cache_stats.per_cache_id_info.items():\n            disp[\"Caches\"][cache_id] = \\\n                prepare_block_cache_info_for_display(cache_info)\n\n    if cache_stats.global_cache_counters:\n        disp[\"DB Counters\"] = \\\n            prepare_block_counters_for_display(\n                cache_stats.global_cache_counters)\n    else:\n        disp[\"DB Counters\"] = utils.NO_COUNTERS_DUMPS_TEXT\n\n    detailed_disp_block_cache_stats = None\n    if detailed_block_cache_stats:\n        detailed_disp_block_cache_stats = \\\n            prepare_detailed_block_cache_stats_for_display(\n                detailed_block_cache_stats)\n    if detailed_disp_block_cache_stats:\n        disp[\"Detailed\"] = detailed_disp_block_cache_stats\n    else:\n        disp[\"Detailed\"] = \"No Detailed Block Cache Stats Available\"\n\n    return disp", "\n\ndef prepare_cf_filter_stats_for_display(cf_filter_stats, format_as_dict):\n    if cf_filter_stats.filter_policy:\n        assert isinstance(cf_filter_stats, calc_utils.CfFilterFilesStats)\n        if cf_filter_stats.filter_policy != utils.INVALID_FILTER_POLICY:\n            sanitized_filter_policy = \\\n                SanitizedValueType.get_type_from_str(\n                    cf_filter_stats.filter_policy)\n            if sanitized_filter_policy == SanitizedValueType.NULL_PTR:\n                cf_disp_stats = \"No Filter\"\n            else:\n                if cf_filter_stats.avg_bpk is not None:\n                    bpk_str = f\"{cf_filter_stats.avg_bpk:.1f}\"\n                else:\n                    bpk_str = \"unknown bpk\"\n                if format_as_dict:\n                    cf_disp_stats = {\n                        \"Filter-Policy\": cf_filter_stats.filter_policy,\n                        \"Avg. BPK\": bpk_str\n                    }\n                else:\n                    cf_disp_stats = \\\n                        f\"{cf_filter_stats.filter_policy} ({bpk_str})\"\n        else:\n            cf_disp_stats = \"Filter Data Not Available\"\n    else:\n        cf_disp_stats = \"Filter Data Not Available\"\n\n    return cf_disp_stats", "\n\ndef prepare_filter_stats_for_display(filter_stats):\n    assert isinstance(filter_stats, calc_utils.FilterStats)\n\n    disp = {}\n\n    if filter_stats.files_filter_stats:\n        disp[\"CF-s\"] = {}\n        for cf_name, cf_stats in filter_stats.files_filter_stats.items():\n            assert isinstance(cf_stats, calc_utils.CfFilterFilesStats)\n            disp[\"CF-s\"][cf_name] = \\\n                prepare_cf_filter_stats_for_display(cf_stats,\n                                                    format_as_dict=True)\n    else:\n        disp[\"CF-s\"] = \"No Filters used In SST-s\"\n\n    if filter_stats.filter_counters and not \\\n            filter_stats.filter_counters.are_all_zeroes():\n        disp_counters = {\n            \"False-Positive-Rate\":\n                f\"1 in {filter_stats.filter_counters.one_in_n_fpr}\",\n            \"False-Positives\":\n                num_for_display(filter_stats.filter_counters.false_positives),\n            \"Negatives\":\n                num_for_display(filter_stats.filter_counters.negatives),\n            \"True-Positives\":\n                num_for_display(filter_stats.filter_counters.true_positives)\n        }\n        disp[\"Counters\"] = disp_counters\n    else:\n        disp[\"Counters\"] = \"No Filter Counters Available\"\n\n    return disp", "\n\ndef prepare_filter_effectiveness_stats_for_display(\n        cfs_names, db_opts, counters_mngr, files_monitor):\n    assert isinstance(db_opts, db_options.DatabaseOptions)\n    assert isinstance(counters_mngr, CountersMngr)\n    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n\n    filter_stats = calc_utils.calc_filter_stats(\n        cfs_names, db_opts, files_monitor, counters_mngr)\n\n    if filter_stats:\n        return prepare_filter_stats_for_display(filter_stats)\n    else:\n        return \"No Filter Stats Available\"", "\n\ndef prepare_get_histogram_for_display(counters_mngr):\n    assert isinstance(counters_mngr, CountersMngr)\n\n    get_counter_name = \"rocksdb.db.get.micros\"\n    get_histogram = \\\n        counters_mngr.get_last_histogram_entry(\n            get_counter_name, non_zero=True)\n\n    if get_histogram:\n        return CountersMngr.\\\n            get_histogram_entry_display_values(get_histogram)\n    else:\n        logging.info(\"No Get latency histogram (maybe no stats)\")\n        return \"No Get Info\"", "\n\ndef prepare_multi_get_histogram_for_display(counters_mngr):\n    assert isinstance(counters_mngr, CountersMngr)\n\n    multi_get_counter_name = \"rocksdb.db.multiget.micros\"\n\n    multi_get_histogram = \\\n        counters_mngr.get_last_histogram_entry(\n            multi_get_counter_name, non_zero=True)\n\n    if multi_get_histogram:\n        return counters_mngr.\\\n            get_histogram_entry_display_values(multi_get_histogram)\n    else:\n        logging.info(\"No Multi-Get latency histogram (maybe no stats)\")\n        return \"No Multi-Get Info\"", "\n\ndef prepare_per_cf_read_latency_for_display(cf_file_histogram_stats_mngr):\n    per_cf_stats = \\\n        calc_utils.calc_read_latency_per_cf_stats(cf_file_histogram_stats_mngr)\n\n    stats = dict()\n\n    if per_cf_stats:\n        for cf_name, cf_stats in per_cf_stats.items():\n            stats[cf_name] = {\n                \"Num Reads\": num_for_display(cf_stats.num_reads),\n                \"Avg. Read Latency\": f\"{cf_stats.avg_read_latency_us:.1f} us\",\n                \"Max Read Latency\": f\"{cf_stats.max_read_latency_us:.1f} us\",\n                \"Read % of All CF-s\":\n                    f\"{cf_stats.read_percent_of_all_cfs:.1f}%\"\n            }\n\n    return stats", "\n\ndef prepare_applicable_read_stats(\n        cfs_names, db_opts, counters_mngr, stats_mngr, files_monitor):\n    assert isinstance(db_opts, db_options.DatabaseOptions)\n    assert isinstance(counters_mngr, CountersMngr)\n    assert isinstance(stats_mngr, StatsMngr)\n    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n\n    stats = dict()\n    stats[\"Get Histogram\"] = prepare_get_histogram_for_display(counters_mngr)\n    stats[\"Multi-Get Histogram\"] = \\\n        prepare_multi_get_histogram_for_display(counters_mngr)\n\n    cf_file_histogram_stats_mngr =\\\n        stats_mngr.get_cf_file_histogram_stats_mngr()\n    stats[\"Per CF Read Latency\"] = \\\n        prepare_per_cf_read_latency_for_display(cf_file_histogram_stats_mngr)\n\n    stats[\"Filter Effectiveness\"] = \\\n        prepare_filter_effectiveness_stats_for_display(\n            cfs_names, db_opts, counters_mngr, files_monitor)\n\n    return stats if stats else None", ""]}
{"filename": "calc_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport copy\nimport logging\nfrom bisect import bisect\nfrom dataclasses import dataclass, field", "from bisect import bisect\nfrom dataclasses import dataclass, field\n\nimport db_files\nimport db_options\nimport utils\nfrom counters import CountersMngr\nfrom events import EventType\nfrom events import FlowType, MatchingEventInfo, EventsMngr\nfrom log_file import ParsedLog", "from events import FlowType, MatchingEventInfo, EventsMngr\nfrom log_file import ParsedLog\nfrom stats_mngr import CompactionStatsMngr, CfFileHistogramStatsMngr, \\\n    DbWideStatsMngr\nfrom warnings_mngr import WarningType, WarningsMngr, WarningElementInfo\n\n\ndef get_db_size_bytes_at_start(compaction_stats_mngr):\n    assert isinstance(compaction_stats_mngr, CompactionStatsMngr)\n\n    first_entry_all_cfs = compaction_stats_mngr.get_first_level_entry_all_cfs()\n    if not first_entry_all_cfs:\n        return 0\n\n    size_bytes = 0\n    for cf_name, cf_entry in first_entry_all_cfs.items():\n        size_bytes += \\\n            int(CompactionStatsMngr.get_sum_value(\n                cf_entry, CompactionStatsMngr.LevelFields.SIZE_BYTES))", "\n\n@dataclass\nclass DbSizeBytesInfo:\n    size_bytes: int = None\n    size_time: str = None\n\n\ndef get_db_size_bytes_info_at_end(cfs_names, compaction_stats_mngr):\n    assert isinstance(compaction_stats_mngr, CompactionStatsMngr)\n\n    last_entry_all_cfs = compaction_stats_mngr.get_last_level_entry_all_cfs()\n    if not last_entry_all_cfs:\n        return DbSizeBytesInfo(size_bytes=None, size_time=None)\n\n    size_bytes = 0\n    size_time = None\n    for cf_name in cfs_names:\n        if cf_name not in last_entry_all_cfs:\n            continue\n        cf_entry = last_entry_all_cfs[cf_name]\n        if size_time is None:\n            size_time, _ = utils.get_last_dict_entry_components(cf_entry)\n\n        size_bytes += \\\n            int(CompactionStatsMngr.get_sum_value(\n                cf_entry, CompactionStatsMngr.LevelFields.SIZE_BYTES))\n\n    return DbSizeBytesInfo(size_time=size_time, size_bytes=size_bytes)", "def get_db_size_bytes_info_at_end(cfs_names, compaction_stats_mngr):\n    assert isinstance(compaction_stats_mngr, CompactionStatsMngr)\n\n    last_entry_all_cfs = compaction_stats_mngr.get_last_level_entry_all_cfs()\n    if not last_entry_all_cfs:\n        return DbSizeBytesInfo(size_bytes=None, size_time=None)\n\n    size_bytes = 0\n    size_time = None\n    for cf_name in cfs_names:\n        if cf_name not in last_entry_all_cfs:\n            continue\n        cf_entry = last_entry_all_cfs[cf_name]\n        if size_time is None:\n            size_time, _ = utils.get_last_dict_entry_components(cf_entry)\n\n        size_bytes += \\\n            int(CompactionStatsMngr.get_sum_value(\n                cf_entry, CompactionStatsMngr.LevelFields.SIZE_BYTES))\n\n    return DbSizeBytesInfo(size_time=size_time, size_bytes=size_bytes)", "\n\ndef get_per_cf_per_level_size_bytes(entry_all_cfs):\n    if not entry_all_cfs:\n        return {}\n\n    growth = dict()\n    for cf_name, cf_entry in entry_all_cfs.items():\n        size_bytes_per_level = \\\n            CompactionStatsMngr.get_field_value_for_all_levels(\n                cf_entry, CompactionStatsMngr.LevelFields.SIZE_BYTES)\n\n        growth[cf_name] = size_bytes_per_level\n\n    return growth", "\n\ndef calc_cfs_size_bytes_growth(cfs_names, compaction_stats_mngr):\n    assert isinstance(compaction_stats_mngr, CompactionStatsMngr)\n\n    growth = {cf_name: None for cf_name in cfs_names}\n\n    first_entry_all_cfs = compaction_stats_mngr.get_first_level_entry_all_cfs()\n    if not first_entry_all_cfs:\n        return {}\n\n    start_per_cf_and_level_sizes_bytes = \\\n        get_per_cf_per_level_size_bytes(first_entry_all_cfs)\n\n    last_entry_all_cfs = compaction_stats_mngr.get_last_level_entry_all_cfs()\n    assert last_entry_all_cfs\n\n    start_cf_names = list(start_per_cf_and_level_sizes_bytes.keys())\n    for cf_name in start_cf_names:\n        growth[cf_name] = {}\n        start_cf_level_sizes = start_per_cf_and_level_sizes_bytes[cf_name]\n        if start_cf_level_sizes:\n            for level in start_cf_level_sizes:\n                growth[cf_name][level] = (start_cf_level_sizes[level], None)\n\n    end_per_cf_and_level_sizes_bytes = \\\n        get_per_cf_per_level_size_bytes(last_entry_all_cfs)\n    end_cf_names = list(end_per_cf_and_level_sizes_bytes.keys())\n    for cf_name in end_cf_names:\n        if cf_name not in growth:\n            growth[cf_name] = {}\n        end_cf_level_sizes = end_per_cf_and_level_sizes_bytes[cf_name]\n        if end_cf_level_sizes:\n            for level in end_cf_level_sizes:\n                if level in growth[cf_name]:\n                    start_value = growth[cf_name][level][0]\n                else:\n                    start_value = None\n                growth[cf_name][level] = (start_value,\n                                          end_cf_level_sizes[level])\n    return growth", "\n\ndef calc_cf_table_creation_stats(cf_name, events_mngr):\n    assert isinstance(events_mngr, EventsMngr)\n\n    creation_events = \\\n        events_mngr.get_cf_events_by_type(cf_name,\n                                          EventType.TABLE_FILE_CREATION)\n\n    total_num_entries = 0\n    total_keys_sizes = 0\n    total_values_sizes = 0\n    for event in creation_events:\n        table_properties = event.event_details_dict[\"table_properties\"]\n        total_num_entries += table_properties[\"num_entries\"]\n        total_keys_sizes += table_properties[\"raw_key_size\"]\n        total_values_sizes += table_properties[\"raw_value_size\"]\n\n    num_tables_created = len(creation_events)\n    avg_num_table_entries = 0\n    avg_key_size = 0\n    avg_value_size = 0\n\n    if num_tables_created > 0:\n        avg_num_table_entries = int(total_num_entries / num_tables_created)\n        avg_key_size = int(total_keys_sizes / total_num_entries)\n        avg_value_size = int(total_values_sizes / total_num_entries)\n\n    return {\"num_tables_created\": num_tables_created,\n            \"total_num_entries\": total_num_entries,\n            \"total_keys_sizes\": total_keys_sizes,\n            \"total_values_sizes\": total_values_sizes,\n            \"avg_num_table_entries\": avg_num_table_entries,\n            \"avg_key_size\": avg_key_size,\n            \"avg_value_size\": avg_value_size}", "\n\n@dataclass\nclass DeleteOpersStats:\n    total_num_flushed_entries: int = None\n    total_num_deletes: int = None\n    total_percent_deletes: float = None\n    unavailability_reason: str = None\n\n\ndef calc_cf_delete_opers_stats(cf_name, events_mngr):\n    flush_started_events = \\\n        events_mngr.get_cf_events_by_type(cf_name, EventType.FLUSH_STARTED)\n\n    if not flush_started_events:\n        return DeleteOpersStats(\n            unavailability_reason=utils.NO_FLUSHES_TEXT)\n\n    stats = DeleteOpersStats(total_num_flushed_entries=0, total_num_deletes=0)\n\n    for flush_event in flush_started_events:\n        stats.total_num_flushed_entries += flush_event.get_num_entries()\n        stats.total_num_deletes += flush_event.get_num_deletes()\n\n    return stats", "\n\ndef calc_cf_delete_opers_stats(cf_name, events_mngr):\n    flush_started_events = \\\n        events_mngr.get_cf_events_by_type(cf_name, EventType.FLUSH_STARTED)\n\n    if not flush_started_events:\n        return DeleteOpersStats(\n            unavailability_reason=utils.NO_FLUSHES_TEXT)\n\n    stats = DeleteOpersStats(total_num_flushed_entries=0, total_num_deletes=0)\n\n    for flush_event in flush_started_events:\n        stats.total_num_flushed_entries += flush_event.get_num_entries()\n        stats.total_num_deletes += flush_event.get_num_deletes()\n\n    return stats", "\n\ndef calc_delete_opers_stats(cfs_names, events_mngr):\n    assert cfs_names\n    assert isinstance(events_mngr, EventsMngr)\n\n    stats = DeleteOpersStats(total_num_flushed_entries=0, total_num_deletes=0)\n\n    unavailability_reason = None\n    has_any_data = False\n    for cf_name in cfs_names:\n        cf_stats = calc_cf_delete_opers_stats(cf_name, events_mngr)\n        assert isinstance(cf_stats, DeleteOpersStats)\n\n        if cf_stats.total_num_flushed_entries:\n            has_any_data = True\n            stats.total_num_flushed_entries += \\\n                cf_stats.total_num_flushed_entries\n\n            assert cf_stats.total_num_deletes is not None\n            stats.total_num_deletes += cf_stats.total_num_deletes\n        else:\n            # arbitrarily use the first reason for all\n            assert cf_stats.unavailability_reason is not None\n            unavailability_reason = cf_stats.unavailability_reason\n\n    if not has_any_data:\n        return DeleteOpersStats(unavailability_reason=unavailability_reason)\n\n    if stats.total_num_flushed_entries > 0:\n        stats.total_percent_deletes = \\\n            float(100 * stats.total_num_deletes /\n                  stats.total_num_flushed_entries)\n\n    return stats", "\n\n@dataclass\nclass UserOpersStats:\n    num_written: int = None\n    num_read: int = None\n    num_seek: int = None\n    total_num_user_opers: int = None\n    percent_written: float = None\n    percent_read: float = None\n    percent_seek: float = None\n    unavailability_reason: str = None", "\n\ndef get_user_operations_stats(counters_mngr):\n    if not counters_mngr.does_have_counters_values():\n        return UserOpersStats(unavailability_reason=utils.NO_STATS_TEXT)\n\n    stats = UserOpersStats()\n    mngr = counters_mngr\n    stats.num_written = \\\n        mngr.get_last_counter_value(\"rocksdb.number.keys.written\")\n    stats.num_read = mngr.get_last_counter_value(\"rocksdb.number.keys.read\")\n    stats.num_seek = mngr.get_last_counter_value(\"rocksdb.number.db.seek\")\n    stats.total_num_user_opers = \\\n        stats.num_written + stats.num_read + stats.num_seek\n\n    if stats.total_num_user_opers > 0:\n        stats.percent_written = float(100 * stats.num_written /\n                                      stats.total_num_user_opers)\n        stats.percent_read = float(100 * stats.num_read /\n                                   stats.total_num_user_opers)\n        stats.percent_seek = float(100 * stats.num_seek /\n                                   stats.total_num_user_opers)\n\n    return stats", "\n\n@dataclass\nclass LogFileTimeInfo:\n    start_time: str = None\n    end_time: str = None\n    span_seconds: float = 0.0\n\n\ndef get_log_file_time_info(parsed_log):\n    metadata = parsed_log.get_metadata()\n\n    return LogFileTimeInfo(start_time=metadata.get_start_time(),\n                           end_time=metadata.get_end_time(),\n                           span_seconds=metadata.get_log_time_span_seconds())", "\ndef get_log_file_time_info(parsed_log):\n    metadata = parsed_log.get_metadata()\n\n    return LogFileTimeInfo(start_time=metadata.get_start_time(),\n                           end_time=metadata.get_end_time(),\n                           span_seconds=metadata.get_log_time_span_seconds())\n\n\ndef get_warn_messages(raw_elements):\n    if not raw_elements:\n        return None\n\n    returned_errors = dict()\n    for cf_errors in raw_elements.values():\n        for category_errors in cf_errors.values():\n            for error_info in category_errors:\n                assert isinstance(error_info, WarningElementInfo)\n                returned_errors[error_info.time] = error_info.warning_msg\n\n    return returned_errors", "\ndef get_warn_messages(raw_elements):\n    if not raw_elements:\n        return None\n\n    returned_errors = dict()\n    for cf_errors in raw_elements.values():\n        for category_errors in cf_errors.values():\n            for error_info in category_errors:\n                assert isinstance(error_info, WarningElementInfo)\n                returned_errors[error_info.time] = error_info.warning_msg\n\n    return returned_errors", "\n\ndef get_error_warnings(warnings_mngr):\n    assert isinstance(warnings_mngr, WarningsMngr)\n    return get_warn_messages(warnings_mngr.get_error_warnings())\n\n\ndef get_fatal_warnings(warnings_mngr):\n    assert isinstance(warnings_mngr, WarningsMngr)\n    return get_warn_messages(warnings_mngr.get_fatal_warnings())", "\n\ndef get_db_wide_info(parsed_log: ParsedLog):\n    metadata = parsed_log.get_metadata()\n    warns_mngr = parsed_log.get_warnings_mngr()\n    stats_mngr = parsed_log.get_stats_mngr()\n    db_wide_stats_mngr = stats_mngr.get_db_wide_stats_mngr()\n    counters_mngr = parsed_log.get_counters_mngr()\n\n    user_opers_stats = get_user_operations_stats(counters_mngr)\n    assert isinstance(user_opers_stats, UserOpersStats)\n\n    cumulative_writes_stats_dict = \\\n        stats_mngr.get_db_wide_stats_mngr(). \\\n        get_last_cumulative_writes_entry()\n\n    num_keys_written = None\n    if cumulative_writes_stats_dict:\n        _, cumulative_writes_stats = \\\n            utils.get_first_dict_entry_components(cumulative_writes_stats_dict)\n        if user_opers_stats.num_written:\n            num_keys_written = max(user_opers_stats.num_written,\n                                   cumulative_writes_stats.num_keys)\n        else:\n            num_keys_written = cumulative_writes_stats.num_keys\n\n    total_num_table_created_entries = 0\n    total_keys_sizes = 0\n    total_values_size = 0\n\n    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n    events_mngr = parsed_log.get_events_mngr()\n\n    delete_opers_stats = calc_delete_opers_stats(cfs_names, events_mngr)\n    assert isinstance(delete_opers_stats, DeleteOpersStats)\n\n    for cf_name in cfs_names:\n        table_creation_stats = calc_cf_table_creation_stats(cf_name,\n                                                            events_mngr)\n        total_num_table_created_entries += \\\n            table_creation_stats[\"total_num_entries\"]\n        total_keys_sizes += table_creation_stats[\"total_keys_sizes\"]\n        total_values_size += table_creation_stats[\"total_values_sizes\"]\n\n    # TODO - Add unit test when total_num_table_created_entries == 0\n    # TODO - Consider whether this means data is not available\n    avg_key_size_bytes = None\n    avg_value_size_bytes = None\n    if total_num_table_created_entries > 0:\n        avg_key_size_bytes = \\\n            int(total_keys_sizes / total_num_table_created_entries)\n        avg_value_size_bytes = \\\n            int(total_values_size / total_num_table_created_entries)\n\n    compactions_stats_mngr = \\\n        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\n    db_size_bytes_info = get_db_size_bytes_info_at_end(cfs_names,\n                                                       compactions_stats_mngr)\n    assert isinstance(db_size_bytes_info, DbSizeBytesInfo)\n\n    ingest_info = get_db_ingest_info(db_wide_stats_mngr)\n\n    info = {\n        \"creator\": metadata.get_product_name(),\n        \"version\": metadata.get_version(),\n        \"git_hash\": metadata.get_git_hash(),\n        \"db_size_bytes\": db_size_bytes_info.size_bytes,\n        \"db_size_bytes_time\": db_size_bytes_info.size_time,\n        \"num_cfs\": parsed_log.get_num_cfs_when_certain(),\n        \"avg_key_size_bytes\": avg_key_size_bytes,\n        \"avg_value_size_bytes\": avg_value_size_bytes,\n        \"num_warnings\": warns_mngr.get_total_num_warns(),\n        \"errors\": get_error_warnings(warns_mngr),\n        \"fatals\": get_fatal_warnings(warns_mngr),\n        \"total_num_table_created_entries\": total_num_table_created_entries,\n        \"num_keys_written\": num_keys_written,\n        \"user_opers_stats\": user_opers_stats,\n        \"delete_opers_stats\": delete_opers_stats,\n        \"ingest_info\": ingest_info\n    }\n\n    return info", "\n\n@dataclass\nclass DbIngestInfo:\n    time: str\n    ingest: int = 0\n    ingest_rate_mbps: float = 0.0\n\n\ndef get_db_ingest_info(db_wide_stats_mngr):\n    assert isinstance(db_wide_stats_mngr, DbWideStatsMngr)\n\n    cumulative_writes_stats_dict = \\\n        db_wide_stats_mngr.get_last_cumulative_writes_entry()\n    if not cumulative_writes_stats_dict:\n        return None\n\n    time, cumulative_writes_stats = \\\n        utils.get_first_dict_entry_components(cumulative_writes_stats_dict)\n\n    return DbIngestInfo(\n        time=time,\n        ingest=cumulative_writes_stats.ingest,\n        ingest_rate_mbps=cumulative_writes_stats.ingest_rate_mbps)", "\ndef get_db_ingest_info(db_wide_stats_mngr):\n    assert isinstance(db_wide_stats_mngr, DbWideStatsMngr)\n\n    cumulative_writes_stats_dict = \\\n        db_wide_stats_mngr.get_last_cumulative_writes_entry()\n    if not cumulative_writes_stats_dict:\n        return None\n\n    time, cumulative_writes_stats = \\\n        utils.get_first_dict_entry_components(cumulative_writes_stats_dict)\n\n    return DbIngestInfo(\n        time=time,\n        ingest=cumulative_writes_stats.ingest,\n        ingest_rate_mbps=cumulative_writes_stats.ingest_rate_mbps)", "\n\ndef calc_event_histogram(cf_name, events_mngr, event_type, group_by_field):\n    events = events_mngr.get_cf_events_by_type(cf_name, event_type)\n\n    histogram = dict()\n    for event in events:\n        event_grouping = event.event_details_dict[group_by_field]\n        if event_grouping not in histogram:\n            histogram[event_grouping] = 0\n        histogram[event_grouping] += 1\n\n    return histogram", "\n\n# A list of flushed data sizes (in MB) to use for the generation of the\n# flush sizes histogram\n# The sizes are integers and must be ordered and always increasing\nFLUSHED_SIZES_HISTOGRAM_BUCKETS_MB = [2, 10, 32, 64]\n\n\n@dataclass\nclass PerFlushReasonStats:\n    num_flushes: int = 0\n\n    # Prepare a count per bucket (+1 for sizes > last)\n    sizes_histogram: list = \\\n        field(default_factory=lambda: ([0] * (len(\n            FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB)+1)))\n\n    min_duration_ms: int = None\n    max_duration_ms: int = None\n    min_num_memtables: int = None\n    max_num_memtables: int = None\n    min_total_data_size_bytes: int = None\n    max_total_data_size_bytes: int = None", "@dataclass\nclass PerFlushReasonStats:\n    num_flushes: int = 0\n\n    # Prepare a count per bucket (+1 for sizes > last)\n    sizes_histogram: list = \\\n        field(default_factory=lambda: ([0] * (len(\n            FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB)+1)))\n\n    min_duration_ms: int = None\n    max_duration_ms: int = None\n    min_num_memtables: int = None\n    max_num_memtables: int = None\n    min_total_data_size_bytes: int = None\n    max_total_data_size_bytes: int = None", "\n\ndef calc_cf_flushes_stats(cf_name, events_mngr):\n    cf_flush_events = events_mngr.get_cf_flow_events(FlowType.FLUSH,\n                                                     cf_name)\n    if not cf_flush_events:\n        return {}\n\n    def get_min(curr, new):\n        return min(curr, new) if curr is not None else new\n\n    def get_max(curr, new):\n        return max(curr, new) if curr is not None else new\n\n    stats = {}\n    for events_pair in cf_flush_events:\n        start_flush_event = events_pair[0]\n        flush_reason = start_flush_event.get_flush_reason()\n\n        num_memtables = start_flush_event.get_num_memtables()\n        total_data_size_bytes = start_flush_event.get_total_data_size_bytes()\n\n        flush_duration_ms = 0\n        # It's possible that there is no matching end event\n        end_event = events_pair[1]\n        if end_event:\n            start_event_info = MatchingEventInfo(start_flush_event,\n                                                 FlowType.FLUSH, True)\n            end_event_info = MatchingEventInfo(end_event,\n                                               FlowType.FLUSH, False)\n            flush_duration_ms = \\\n                start_event_info.get_duration_ms(end_event_info)\n\n        # Find the bucket for the flushed size\n        bucket_idx = bisect(FLUSHED_SIZES_HISTOGRAM_BUCKETS_MB,\n                            total_data_size_bytes / (2**20))\n\n        if flush_reason not in stats:\n            stats[flush_reason] = PerFlushReasonStats()\n\n        reason_stats = stats[flush_reason]\n        reason_stats.num_flushes += 1\n        reason_stats.sizes_histogram[bucket_idx] += 1\n        reason_stats.min_duration_ms = \\\n            get_min(reason_stats.min_duration_ms, flush_duration_ms)\n        reason_stats.max_duration_ms = \\\n            get_max(reason_stats.max_duration_ms, flush_duration_ms)\n        reason_stats.min_num_memtables = \\\n            get_min(reason_stats.min_num_memtables, num_memtables)\n        reason_stats.max_num_memtables = \\\n            get_max(reason_stats.max_num_memtables, num_memtables)\n        reason_stats.min_total_data_size_bytes = \\\n            get_min(reason_stats.min_total_data_size_bytes,\n                    total_data_size_bytes)\n        reason_stats.max_total_data_size_bytes = \\\n            get_max(reason_stats.max_total_data_size_bytes,\n                    total_data_size_bytes)\n\n    if stats:\n        for reason in stats:\n            stats[reason] = stats[reason]\n\n    return stats", "\n\ndef get_largest_compaction_size_bytes(compactions_monitor):\n    jobs = compactions_monitor.get_finished_jobs()\n\n    largest_size_bytes = 0\n    for job in jobs.values():\n        largest_size_bytes = max(largest_size_bytes,\n                                 job.start_event.get_input_data_size_bytes())\n\n    return largest_size_bytes", "\n\ndef get_cf_per_level_write_amp(compactions_stats_mngr, cf_name):\n    cf_compaction_stats = \\\n        compactions_stats_mngr.get_cf_level_entries(cf_name)\n    if not cf_compaction_stats:\n        return None\n\n\n@dataclass\nclass CfCompactionStats:\n    num_compactions: int = 0\n    min_compaction_bw_mbps: float = None\n    max_compaction_bw_mbps: float = None\n    per_level_write_amp: dict = None\n    comp_sec: float = None\n    comp_merge_cpu_sec: float = None", "\n@dataclass\nclass CfCompactionStats:\n    num_compactions: int = 0\n    min_compaction_bw_mbps: float = None\n    max_compaction_bw_mbps: float = None\n    per_level_write_amp: dict = None\n    comp_sec: float = None\n    comp_merge_cpu_sec: float = None\n", "\n\ndef calc_cf_compactions_stats(cf_name, log_start_time, compactions_monitor,\n                              compactions_stats_mngr):\n    cf_jobs = compactions_monitor.get_cf_finished_jobs(cf_name)\n    if not cf_jobs:\n        return None\n\n    stats = CfCompactionStats()\n    for job in cf_jobs.values():\n        stats.num_compactions += 1\n\n        if job.pre_finish_info:\n            if stats.min_compaction_bw_mbps is None:\n                stats.min_compaction_bw_mbps = \\\n                    job.pre_finish_info.write_rate_mbps\n            else:\n                stats.min_compaction_bw_mbps = \\\n                    min(stats.min_compaction_bw_mbps,\n                        job.pre_finish_info.write_rate_mbps)\n            if stats.max_compaction_bw_mbps is None:\n                stats.max_compaction_bw_mbps = \\\n                    job.pre_finish_info.write_rate_mbps\n            else:\n                stats.max_compaction_bw_mbps = \\\n                    max(stats.max_compaction_bw_mbps,\n                        job.pre_finish_info.write_rate_mbps)\n\n    last_entry = compactions_stats_mngr.get_last_cf_level_entry(cf_name)\n    if last_entry:\n        per_level_write_amp = \\\n            CompactionStatsMngr.get_field_value_for_all_levels(\n                last_entry, CompactionStatsMngr.LevelFields.WRITE_AMP)\n        if per_level_write_amp:\n            sum_write_amp = \\\n                CompactionStatsMngr.get_sum_value(\n                    last_entry, CompactionStatsMngr.LevelFields.WRITE_AMP)\n            per_level_write_amp[\"SUM\"] = sum_write_amp\n            stats.per_level_write_amp = per_level_write_amp\n\n        uptime = \\\n            CompactionStatsMngr.get_level_entry_uptime_seconds(last_entry,\n                                                               log_start_time)\n        if uptime > 0.0:\n            stats.comp_sec = \\\n                float(CompactionStatsMngr.get_sum_value(\n                    last_entry, CompactionStatsMngr.LevelFields.COMP_SEC))\n            stats.comp_merge_cpu_sec =\\\n                float(CompactionStatsMngr.get_sum_value(\n                    last_entry,\n                    CompactionStatsMngr.LevelFields.COMP_MERGE_CPU))\n\n    return stats", "\n\ndef calc_all_events_histogram(cf_names, events_mngr):\n    # Returns a dictionary of:\n    # {<cf_name>: {<event_type>: [events]}}   # noqa\n    histogram = {}\n\n    for cf_name in cf_names:\n        for event_type in EventType:\n            cf_events_of_type = events_mngr.get_cf_events_by_type(cf_name,\n                                                                  event_type)\n            if cf_name not in histogram:\n                histogram[cf_name] = {}\n\n            if cf_events_of_type:\n                histogram[cf_name][event_type] = len(cf_events_of_type)\n    return histogram", "\n\ndef is_cf_compression_by_level(parsed_log, cf_name):\n    db_opts = parsed_log.get_database_options()\n    return db_opts.get_cf_option(cf_name, \"compression[0]\") is not None\n\n\ndef get_applicable_cf_options(db_opts):\n    assert isinstance(db_opts, db_options.DatabaseOptions)\n\n    cf_names = db_opts.get_cfs_names()\n    cfs_options = {\"compaction_style\": {},\n                   \"compression\": {},\n                   \"filter_policy\": {}}\n\n    for cf_name in cf_names:\n        cfs_options[\"compaction_style\"][cf_name] = \\\n            db_opts.get_cf_option(cf_name, \"compaction_style\")\n        cfs_options[\"compression\"][cf_name] = \\\n            db_opts.get_cf_option(cf_name, \"compression\")\n        cfs_options[\"filter_policy\"][cf_name] = \\\n            db_opts.get_cf_table_option(cf_name, \"filter_policy\")\n\n    compaction_styles = list(set(cfs_options[\"compaction_style\"].values()))\n    if len(compaction_styles) == 1 and compaction_styles[0] is not None:\n        common_compaction_style = compaction_styles[0]\n    else:\n        common_compaction_style = \"Per Column Family\"\n    cfs_options[\"compaction_style\"][\"common\"] = common_compaction_style\n\n    compressions = list(set(cfs_options[\"compression\"].values()))\n    if len(compressions) == 1 and compressions[0] is not None:\n        common_compression = compressions[0]\n    else:\n        common_compression = \"Per Column Family\"\n    cfs_options[\"compression\"][\"common\"] = common_compression\n\n    filter_policies = list(set(cfs_options[\"filter_policy\"].values()))\n    if len(filter_policies) == 1 and filter_policies[0] is not None:\n        common_filter_policy = filter_policies[0]\n    else:\n        common_filter_policy = \"Per Column Family\"\n    cfs_options[\"filter_policy\"][\"common\"] = common_filter_policy\n\n    return cfs_options", "\n\n@dataclass\nclass CfReadLatencyStats:\n    num_reads: int = 0\n    avg_read_latency_us: float = 0.0\n    max_read_latency_us: float = 0.0\n    read_percent_of_all_cfs: float = 0.0\n\n\ndef calc_read_latency_per_cf_stats(cf_file_histogram_stats_mngr):\n    stats = {}\n\n    all_entries = cf_file_histogram_stats_mngr.get_all_entries()\n    if not all_entries:\n        return {}\n\n    total_num_reads = 0\n    for cf_name in all_entries:\n        last_cf_entry = cf_file_histogram_stats_mngr.get_last_cf_entry(cf_name)\n        if not last_cf_entry:\n            continue\n        total_cf_num_reads = 0\n        total_cf_read_latency_us = 0.0\n        max_cf_read_latency_us = 0.0\n\n        levels_stats = CfFileHistogramStatsMngr.CfEntry(last_cf_entry)\n        for level_stats in levels_stats.get_all_levels_stats().values():\n            total_cf_num_reads += level_stats.count\n            total_cf_read_latency_us += \\\n                level_stats.count * level_stats.average\n            max_cf_read_latency_us = \\\n                max(max_cf_read_latency_us, level_stats.max)\n\n        avg_cf_read_latency_us = total_cf_read_latency_us / total_cf_num_reads\n        stats[cf_name] = \\\n            CfReadLatencyStats(\n                num_reads=total_cf_num_reads,\n                avg_read_latency_us=avg_cf_read_latency_us,\n                max_read_latency_us=max_cf_read_latency_us)\n        total_num_reads += total_cf_num_reads\n\n    for cf_stats in stats.values():\n        cf_stats.read_percent_of_all_cfs = \\\n            (cf_stats.num_reads/total_num_reads) * 100\n    return stats if stats else None", "\n\ndef calc_read_latency_per_cf_stats(cf_file_histogram_stats_mngr):\n    stats = {}\n\n    all_entries = cf_file_histogram_stats_mngr.get_all_entries()\n    if not all_entries:\n        return {}\n\n    total_num_reads = 0\n    for cf_name in all_entries:\n        last_cf_entry = cf_file_histogram_stats_mngr.get_last_cf_entry(cf_name)\n        if not last_cf_entry:\n            continue\n        total_cf_num_reads = 0\n        total_cf_read_latency_us = 0.0\n        max_cf_read_latency_us = 0.0\n\n        levels_stats = CfFileHistogramStatsMngr.CfEntry(last_cf_entry)\n        for level_stats in levels_stats.get_all_levels_stats().values():\n            total_cf_num_reads += level_stats.count\n            total_cf_read_latency_us += \\\n                level_stats.count * level_stats.average\n            max_cf_read_latency_us = \\\n                max(max_cf_read_latency_us, level_stats.max)\n\n        avg_cf_read_latency_us = total_cf_read_latency_us / total_cf_num_reads\n        stats[cf_name] = \\\n            CfReadLatencyStats(\n                num_reads=total_cf_num_reads,\n                avg_read_latency_us=avg_cf_read_latency_us,\n                max_read_latency_us=max_cf_read_latency_us)\n        total_num_reads += total_cf_num_reads\n\n    for cf_stats in stats.values():\n        cf_stats.read_percent_of_all_cfs = \\\n            (cf_stats.num_reads/total_num_reads) * 100\n    return stats if stats else None", "\n\ndef calc_cf_read_density(compactions_stats_mngr, cf_file_histogram_stats_mngr,\n                         cf_read_latecny_stats, cf_name):\n    assert isinstance(cf_read_latecny_stats, CfReadLatencyStats)\n\n    last_cf_compaction_stats_entry = \\\n        compactions_stats_mngr.get_last_cf_level_entry(cf_name)\n    if not last_cf_compaction_stats_entry:\n        return None\n\n    last_cf_read_latency_raw_entry = \\\n        cf_file_histogram_stats_mngr.get_last_cf_entry(cf_name)\n    last_cf_read_latency_entry = \\\n        CfFileHistogramStatsMngr.CfEntry(last_cf_read_latency_raw_entry)\n\n    total_num_cf_reads = cf_read_latecny_stats.num_reads\n    if total_num_cf_reads == 0:\n        logging.info(f\"No read density as no reads for cf ({cf_name}). \"\n                     f\"time:{last_cf_read_latency_entry.get_time()} \")\n        return {}\n\n    # Calculate per level READ-NORM\n    per_level_read_norm = dict()\n    for level, level_stats in \\\n            last_cf_read_latency_entry.get_all_levels_stats().items():\n        per_level_read_norm[level] = level_stats.count / total_num_cf_reads\n\n    compaction_entry = CompactionStatsMngr.CfLevelEntry(\n        last_cf_compaction_stats_entry)\n\n    total_cf_size_bytes = \\\n        compactions_stats_mngr.get_cf_size_bytes_at_end(cf_name)\n    if total_cf_size_bytes == 0:\n        logging.info(f\"No read density as cf ({cf_name}) size if 0 \"\n                     f\"time:{compaction_entry.get_time()} \")\n        return None\n\n    # Calculate per level SIZE-NORM\n    per_level_size_norm = dict()\n    compaction_levels = compaction_entry.get_levels()\n    for level in compaction_levels:\n        # TODO - If this ever gets used again - level_size_bytes could be None\n        level_size_bytes = \\\n            compactions_stats_mngr.get_cf_level_size_bytes(cf_name, level)\n        per_level_size_norm[level] = level_size_bytes / total_cf_size_bytes\n\n    per_level_read_density = dict()\n    for level, level_size_bytes in per_level_size_norm.items():\n        if level_size_bytes == 0:\n            logging.info(f\"0 Size level {level} skipped.\"\n                         f\"cf:{cf_name}. time:{compaction_entry.get_time()}\")\n            continue\n        if level not in per_level_read_norm:\n            logging.info(f\"Level {level} skipped since it's missing in \"\n                         f\"compaction levels dump. cf:{cf_name}. time:\"\n                         f\"{compaction_entry.get_time()}\")\n            continue\n\n        per_level_read_density[level] =\\\n            per_level_read_norm[level] / per_level_size_norm[level]\n\n    # Calculate the weighted-average of the norms\n    sum_densities = sum(per_level_read_density.values())\n    per_level_weighted_avg_density = \\\n        {level: density / sum_densities for level, density in\n         per_level_read_density.items()}\n\n    return per_level_weighted_avg_density", "\n\n@dataclass\nclass SeekStats:\n    num_seeks: int = 0\n    num_found_seeks: int = 0\n    num_nexts: int = 0\n    num_prevs: int = 0\n    avg_seek_range_size: float = 0.0\n    avg_seek_rate_per_second: float = 0.0\n    avg_seek_latency_us: float = 0.0", "\n\ndef get_applicable_seek_stats(counters_mngr):\n    # The names of the counters in the stats dump in the log\n    prefix = 'rocksdb.number.db'\n    seek_name = f\"{prefix}.seek\"\n    seek_found_name = f\"{prefix}.seek.found\"\n    seek_next_name = f\"{prefix}.next\"\n    seek_prev_name = f\"{prefix}.prev\"\n    seek_latency_hist_us = \"rocksdb.db.seek.micros\"\n\n    mngr = counters_mngr\n\n    # First see if there are any seeks recorded\n    last_seek_entry = mngr.get_last_counter_entry(seek_name)\n    if not last_seek_entry:\n        logging.info(\"No seeks (maybe no stats) or no actual seeks in log.\")\n        return None\n\n    last_seek_time = last_seek_entry[\"time\"]\n    last_seek_count = last_seek_entry[\"value\"]\n\n    first_seek_entry = mngr.get_first_counter_entry(seek_name)\n    first_seek_time = first_seek_entry[\"time\"]\n    first_seek_count = first_seek_entry[\"value\"]\n\n    num_seeks = last_seek_count - first_seek_count\n    if num_seeks == 0:\n        logging.info(\"No seeks log (seek counter is 0).\")\n        return None\n\n    seek_time_span_seconds = \\\n        utils.get_times_strs_diff_seconds(\n            first_seek_time, last_seek_time)\n\n    stats = SeekStats()\n\n    last_seek_found_count = mngr.get_last_counter_value(seek_found_name)\n    last_num_nexts = mngr.get_last_counter_value(seek_next_name)\n    last_num_prevs = mngr.get_last_counter_value(seek_prev_name)\n\n    first_seek_found_count = mngr.get_first_counter_value(seek_found_name)\n    first_num_nexts = mngr.get_first_counter_value(seek_next_name)\n    first_num_prevs = mngr.get_first_counter_value(seek_prev_name)\n\n    stats.num_seeks = num_seeks\n    stats.num_found_seeks = last_seek_found_count - first_seek_found_count\n    stats.num_nexts = last_num_nexts - first_num_nexts\n    stats.num_prevs = last_num_prevs - first_num_prevs\n    if stats.num_seeks > 0:\n        stats.avg_seek_range_size = \\\n            (stats.num_prevs + stats.num_nexts) / stats.num_seeks\n\n    avg_seek_latency_us = \\\n        mngr.get_last_histogram_entry(seek_latency_hist_us, non_zero=True)\n\n    if avg_seek_latency_us:\n        if seek_time_span_seconds > 0.0:\n            stats.avg_seek_rate_per_second = \\\n                num_seeks / seek_time_span_seconds\n        seek_latency_hist = avg_seek_latency_us[\"values\"]\n        stats.avg_seek_latency_us = seek_latency_hist[\"Average\"]\n\n    return stats", "\n\ndef get_warn_warnings_info(cfs_names, warnings_mngr):\n    assert isinstance(warnings_mngr, WarningsMngr)\n\n    all_warn_warnings = warnings_mngr.get_warnings_of_type(WarningType.WARN)\n    if not all_warn_warnings:\n        return None\n\n    returned_warn_warnings = {}\n    cfs_names_including_db = [utils.NO_CF] + cfs_names\n    for cf_name in cfs_names_including_db:\n        if cf_name in all_warn_warnings:\n            cf_info = copy.deepcopy(all_warn_warnings[cf_name])\n            for category in list(cf_info.keys()):\n                cf_info[category] = len(cf_info[category])\n            returned_warn_warnings[cf_name] = cf_info\n        else:\n            returned_warn_warnings[cf_name] = {}\n    return returned_warn_warnings", "\n\n@dataclass\nclass CfFilterFilesStats:\n    filter_policy: str = None\n    avg_bpk: float = None\n\n\ndef calc_files_filter_stats(cfs_names, db_opts, files_monitor):\n    assert isinstance(db_opts, db_options.DatabaseOptions)\n    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n\n    stats = dict()\n\n    cfs_options = get_applicable_cf_options(db_opts)\n    cfs_filters_from_options = dict()\n    for cf_name in cfs_names:\n        if cf_name in cfs_options['filter_policy']:\n            cfs_filters_from_options[cf_name] = \\\n                cfs_options['filter_policy'][cf_name]\n\n    for cf_name in cfs_names:\n        cf_filter_files_stats = \\\n            db_files.calc_cf_files_stats([cf_name], files_monitor)\n        if cf_filter_files_stats:\n            assert isinstance(cf_filter_files_stats, db_files.CfsFilesStats)\n\n            filter_policy = \\\n                cf_filter_files_stats.cfs_filter_specific[\n                    cf_name].filter_policy\n            avg_bpk = \\\n                cf_filter_files_stats.cfs_filter_specific[cf_name].avg_bpk\n\n            stats[cf_name] = CfFilterFilesStats(filter_policy=filter_policy,\n                                                avg_bpk=avg_bpk)\n        elif cf_name in cfs_filters_from_options:\n            filter_policy = cfs_filters_from_options[cf_name]\n            stats[cf_name] = CfFilterFilesStats(filter_policy=filter_policy,\n                                                avg_bpk=None)\n        else:\n            # INVALID_FILTER_POLICY to indicate this cf's filter policy\n            # can't be deduced (instead of None which means - we know it has\n            # no filter policy)\n            stats[cf_name] = CfFilterFilesStats(\n                filter_policy=utils.INVALID_FILTER_POLICY, avg_bpk=None)\n\n    return stats", "def calc_files_filter_stats(cfs_names, db_opts, files_monitor):\n    assert isinstance(db_opts, db_options.DatabaseOptions)\n    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n\n    stats = dict()\n\n    cfs_options = get_applicable_cf_options(db_opts)\n    cfs_filters_from_options = dict()\n    for cf_name in cfs_names:\n        if cf_name in cfs_options['filter_policy']:\n            cfs_filters_from_options[cf_name] = \\\n                cfs_options['filter_policy'][cf_name]\n\n    for cf_name in cfs_names:\n        cf_filter_files_stats = \\\n            db_files.calc_cf_files_stats([cf_name], files_monitor)\n        if cf_filter_files_stats:\n            assert isinstance(cf_filter_files_stats, db_files.CfsFilesStats)\n\n            filter_policy = \\\n                cf_filter_files_stats.cfs_filter_specific[\n                    cf_name].filter_policy\n            avg_bpk = \\\n                cf_filter_files_stats.cfs_filter_specific[cf_name].avg_bpk\n\n            stats[cf_name] = CfFilterFilesStats(filter_policy=filter_policy,\n                                                avg_bpk=avg_bpk)\n        elif cf_name in cfs_filters_from_options:\n            filter_policy = cfs_filters_from_options[cf_name]\n            stats[cf_name] = CfFilterFilesStats(filter_policy=filter_policy,\n                                                avg_bpk=None)\n        else:\n            # INVALID_FILTER_POLICY to indicate this cf's filter policy\n            # can't be deduced (instead of None which means - we know it has\n            # no filter policy)\n            stats[cf_name] = CfFilterFilesStats(\n                filter_policy=utils.INVALID_FILTER_POLICY, avg_bpk=None)\n\n    return stats", "\n\n@dataclass\nclass FilterCounters:\n    negatives: int = 0\n    positives: int = 0\n    true_positives: int = 0\n    false_positives: int = 0\n    one_in_n_fpr: int = 0\n\n    def are_all_zeroes(self):\n        return self.negatives + self.positives + self.true_positives + \\\n               self.false_positives + self.one_in_n_fpr == 0", "\n\ndef collect_filter_counters(counters_mngr):\n    assert isinstance(counters_mngr, CountersMngr)\n\n    if not counters_mngr.does_have_counters_values():\n        logging.info(\"Can't collect Filter counters. No counters available\")\n        return None\n\n    filter_counters_names = {\n        \"negatives\": \"rocksdb.bloom.filter.useful\",\n        \"positives\":  \"rocksdb.bloom.filter.full.positive\",\n        \"true_positives\": \"rocksdb.bloom.filter.full.true.positive\"}\n\n    counters = FilterCounters()\n\n    for field_name, counter_name in filter_counters_names.items():\n        counter_value = counters_mngr.get_last_counter_value(counter_name)\n        setattr(counters, field_name, counter_value)\n\n    assert counters.positives >= counters.true_positives\n    counters.false_positives = counters.positives - counters.true_positives\n\n    return counters", "\n\ndef calc_bloom_1_in_fpr(counters):\n    assert isinstance(counters, FilterCounters)\n    total = counters.negatives + counters.positives\n    false_positives = counters.false_positives\n    if false_positives == 0:\n        return 0\n    return int(total / false_positives)\n", "\n\n@dataclass\nclass FilterStats:\n    files_filter_stats: dict = None\n    filter_counters: FilterCounters = None\n\n\ndef calc_filter_stats(cfs_names, db_opts, files_monitor, counters_mngr):\n    assert isinstance(db_opts, db_options.DatabaseOptions)\n    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n    assert isinstance(counters_mngr, CountersMngr)\n\n    stats = FilterStats()\n    files_filter_stats = calc_files_filter_stats(cfs_names,\n                                                 db_opts,\n                                                 files_monitor)\n    if files_filter_stats:\n        stats.files_filter_stats = files_filter_stats\n\n    filter_counters = collect_filter_counters(counters_mngr)\n    if filter_counters:\n        assert isinstance(filter_counters, FilterCounters)\n\n        one_in_n_fpr = calc_bloom_1_in_fpr(filter_counters)\n        filter_counters.one_in_n_fpr = one_in_n_fpr\n        stats.filter_counters = filter_counters\n\n    return stats", "def calc_filter_stats(cfs_names, db_opts, files_monitor, counters_mngr):\n    assert isinstance(db_opts, db_options.DatabaseOptions)\n    assert isinstance(files_monitor, db_files.DbFilesMonitor)\n    assert isinstance(counters_mngr, CountersMngr)\n\n    stats = FilterStats()\n    files_filter_stats = calc_files_filter_stats(cfs_names,\n                                                 db_opts,\n                                                 files_monitor)\n    if files_filter_stats:\n        stats.files_filter_stats = files_filter_stats\n\n    filter_counters = collect_filter_counters(counters_mngr)\n    if filter_counters:\n        assert isinstance(filter_counters, FilterCounters)\n\n        one_in_n_fpr = calc_bloom_1_in_fpr(filter_counters)\n        filter_counters.one_in_n_fpr = one_in_n_fpr\n        stats.filter_counters = filter_counters\n\n    return stats", "\n\ndef get_cfs_common_and_specific_options(db_opts):\n    assert isinstance(db_opts, db_options.DatabaseOptions)\n\n    cfs_names = db_opts.get_cfs_names()\n    cfs_options = {cf_name: db_opts.get_cf_options(cf_name)\n                   for cf_name in cfs_names}\n\n    cfs_common_options, cfs_specific_options = \\\n        db_opts.get_unified_cfs_options(cfs_options)\n\n    return cfs_common_options, cfs_specific_options", "\n\ndef get_cfs_common_and_specific_diff_dicts(\n        baseline_options, log_database_options):\n    assert isinstance(baseline_options, db_options.DatabaseOptions)\n    assert isinstance(log_database_options, db_options.DatabaseOptions)\n\n    baseline_opts = baseline_options.get_all_options()\n\n    cfs_common_options, cfs_specific_options =  \\\n        get_cfs_common_and_specific_options(log_database_options)\n\n    common_dummy_cf_name = \"COMMON-DUMMY-CF-NAME\"\n    common_log_file_full_name_options = db_options.FullNamesOptionsDict()\n    common_log_file_full_name_options.\\\n        init_from_full_names_options_no_cf_dict(common_dummy_cf_name,\n                                                cfs_common_options)\n\n    # We need to compare the common to the baseline,\n    # but a baseline that only contains the options common\n    # to all the cf-s\n    baseline_opts_for_diff_dict = dict()\n    baseline_opts_dict = baseline_opts.get_options_dict()\n    for full_common_option_name in cfs_common_options.keys():\n        if full_common_option_name in baseline_opts_dict:\n            baseline_opts_for_diff_dict[full_common_option_name] = \\\n                baseline_opts_dict[full_common_option_name]\n    baseline_opts_for_diff =\\\n        db_options.FullNamesOptionsDict(baseline_opts_for_diff_dict)\n\n    common_diff = db_options.DatabaseOptions.get_cfs_options_diff(\n        baseline_opts_for_diff,\n        utils.DEFAULT_CF_NAME,\n        common_log_file_full_name_options,\n        common_dummy_cf_name)\n    if common_diff is None or common_diff.is_empty_diff():\n        common_diff = {}\n\n    # We need to compare every cf to the baseline, but a baseline that doesn't\n    # have the options common to all the cf-s (they are missing in the\n    # cfs_specific_options)\n    baseline_opts_for_diff_dict = \\\n        copy.deepcopy(baseline_opts.get_options_dict())\n    utils.delete_dict_keys(baseline_opts_for_diff_dict,\n                           cfs_common_options.keys())\n    baseline_opts_for_diff =\\\n        db_options.FullNamesOptionsDict(baseline_opts_for_diff_dict)\n\n    cfs_specific_diffs = dict()\n    for cf_name, cf_options in cfs_specific_options.items():\n        cf_full_name_options = db_options.FullNamesOptionsDict()\n        cf_full_name_options. \\\n            init_from_full_names_options_no_cf_dict(cf_name, cf_options)\n\n        cf_diff = db_options.DatabaseOptions.get_cfs_options_diff(\n            baseline_opts_for_diff,\n            utils.DEFAULT_CF_NAME,\n            cf_full_name_options,\n            cf_name)\n        cfs_specific_diffs[cf_name] = cf_diff\n\n    return common_diff, cfs_specific_diffs", ""]}
{"filename": "events.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport json\nimport logging\nimport re\nimport typing", "import re\nimport typing\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nimport regexes\nimport utils\nfrom log_entry import LogEntry\n\n\nclass EventType(str, Enum):\n    FLUSH_STARTED = \"flush_started\"\n    FLUSH_FINISHED = \"flush_finished\"\n    COMPACTION_STARTED = \"compaction_started\"\n    COMPACTION_FINISHED = \"compaction_finished\"\n    TABLE_FILE_CREATION = 'table_file_creation'\n    TABLE_FILE_DELETION = \"table_file_deletion\"\n    TRIVIAL_MOVE = \"trivial_move\"\n    RECOVERY_STARTED = \"recovery_started\"\n    RECOVERY_FINISHED = \"recovery_finished\"\n    INGEST_FINISHED = \"ingest_finished\"\n    BLOB_FILE_CREATION = \"blob_file_creation\"\n    BLOB_FILE_DELETION = \"blob_file_deletion\"\n    UNKNOWN = \"UNKNOWN\"\n\n    def __str__(self):\n        return str(self.value)\n\n    @staticmethod\n    def type_from_str(event_type_str):\n        try:\n            return EventType(event_type_str)\n        except ValueError:\n            return EventType.UNKNOWN", "\n\nclass EventType(str, Enum):\n    FLUSH_STARTED = \"flush_started\"\n    FLUSH_FINISHED = \"flush_finished\"\n    COMPACTION_STARTED = \"compaction_started\"\n    COMPACTION_FINISHED = \"compaction_finished\"\n    TABLE_FILE_CREATION = 'table_file_creation'\n    TABLE_FILE_DELETION = \"table_file_deletion\"\n    TRIVIAL_MOVE = \"trivial_move\"\n    RECOVERY_STARTED = \"recovery_started\"\n    RECOVERY_FINISHED = \"recovery_finished\"\n    INGEST_FINISHED = \"ingest_finished\"\n    BLOB_FILE_CREATION = \"blob_file_creation\"\n    BLOB_FILE_DELETION = \"blob_file_deletion\"\n    UNKNOWN = \"UNKNOWN\"\n\n    def __str__(self):\n        return str(self.value)\n\n    @staticmethod\n    def type_from_str(event_type_str):\n        try:\n            return EventType(event_type_str)\n        except ValueError:\n            return EventType.UNKNOWN", "\n\nclass EventField(str, Enum):\n    EVENT_TYPE = \"event\"\n    JOB_ID = \"job\"\n    TIME_MICROS = \"time_micros\"\n    CF_NAME = \"cf_name\"\n    FLUSH_REASON = \"flush_reason\"\n    COMPACTION_REASON = \"compaction_reason\"\n    FILE_NUMBER = \"file_number\"\n    FILE_SIZE = \"file_size\"\n    TABLE_PROPERTIES = \"table_properties\"\n    WAL_ID = \"wal_id\",\n    NUM_ENTRIES = \"num_entries\"\n    NUM_DELETES = \"num_deletes\"\n    NUM_MEMTABLES = \"num_memtables\"\n    TOTAL_DATA_SIZE = \"total_data_size\",\n    INPUT_DATA_SIZE = \"input_data_size\"\n    COMPACTION_TIME_MICROS = \"compaction_time_micros\"\n    TOTAL_OUTPUT_SIZE = \"total_output_size\"\n    # Compaction Finished\n    OUTPUT_LEVEL = \"output_level\"\n    NUM_OUTPUT_FILES = \"num_output_files\"\n    NUM_INPUT_RECORDS = \"num_input_records\"\n    RECORDS_IN = \"records_in\"\n    RECORDS_DROPPED = \"records_dropped\"", "\n\nclass TablePropertiesField(str, Enum):\n    CF_ID = \"column_family_id\"\n    DATA_SIZE = \"data_size\"\n    INDEX_SIZE = \"index_size\"\n    FILTER_SIZE = \"filter_size\"\n    FILTER_POLICY = \"filter_policy\"\n    NUM_FILTER_ENTRIES = \"num_filter_entries\"\n    NUM_DATA_BLOCKS = \"num_data_blocks\"\n    TOTAL_KEY_SIZE = \"raw_key_size\"\n    TOTAL_VALUE_SIZE = \"raw_value_size\"\n    COMPRESSION_TYPE = \"compression\"", "\n\nclass FlowType(str, Enum):\n    FLUSH = \"Flush\"\n    COMPACTION = \"Compaction\"\n    RECOVERY = \"Recovery\"\n\n    def __str__(self):\n        return str(self.value)\n", "\n\n# Events that have a start and finish events that should be matched\n# based on their job-id\n# Every tuple consists of:\n# - The event in the log marking the start\n# - The event in the log marking the end\n# - The type of the associated operation / flow (e.g., Flush)\n# The duration of the associated operation is the time difference\n# between the 2", "# The duration of the associated operation is the time difference\n# between the 2\nMATCHING_EVENTS = [\n    (EventType.FLUSH_STARTED, EventType.FLUSH_FINISHED, FlowType.FLUSH),\n    (EventType.COMPACTION_STARTED, EventType.COMPACTION_FINISHED,\n     FlowType.COMPACTION),\n    (EventType.RECOVERY_STARTED, EventType.RECOVERY_FINISHED,\n     FlowType.RECOVERY)\n]\n", "]\n\n\nEvent = typing.NewType(\"Event\", None)\n\n\n@dataclass\nclass MatchingEventTypeInfo:\n    event_type: EventType\n    associated_flow_type: FlowType\n    is_start: bool", "\n\n@dataclass\nclass MatchingEventInfo:\n    event: Event\n    associated_flow_type: FlowType\n    is_start: bool\n\n    def get_duration_ms(self, matching_event_info):\n        if self.is_start:\n            start_time_epoch_micro = \\\n                self.event.get_time_since_epoch_microseconds()\n            end_time_epoch_micro = \\\n                matching_event_info.event.get_time_since_epoch_microseconds()\n        else:\n            start_time_epoch_micro = \\\n                matching_event_info.event.get_time_since_epoch_microseconds()\n            end_time_epoch_micro = \\\n                self.event.get_time_since_epoch_microseconds()\n\n        assert end_time_epoch_micro >= start_time_epoch_micro\n        return int((end_time_epoch_micro - start_time_epoch_micro) / 1000)", "\n\ndef get_flow_start_event_type(flow_type):\n    for match in MATCHING_EVENTS:\n        if flow_type == match[2]:\n            return match[0]\n    return None\n\n\nclass Event:\n    @dataclass\n    class EventPreambleInfo:\n        cf_name: str\n        type: str\n        job_id: str\n        wal_id: str = None\n\n        def are_equal_ignoring_wal_id(self, other):\n            return self.cf_name == other.cf_name and \\\n                   self.type == other.type and \\\n                   self.job_id == other.job_id\n\n    @staticmethod\n    def is_an_event_entry(log_entry):\n        assert isinstance(log_entry, LogEntry)\n        return re.findall(regexes.EVENT, log_entry.get_msg()) != []\n\n    @staticmethod\n    def try_parse_as_preamble(log_entry):\n        event_msg = log_entry.get_msg()\n\n        is_preamble, cf_name, job_id, wal_id = \\\n            FlushStartedEvent.try_parse_as_preamble(event_msg)\n        if is_preamble:\n            event_type = EventType.FLUSH_STARTED\n        else:\n            wal_id = None\n\n        if not is_preamble:\n            is_preamble, cf_name, job_id = \\\n                CompactionStartedEvent.try_parse_as_preamble(event_msg)\n            if is_preamble:\n                event_type = EventType.COMPACTION_STARTED\n\n        if not is_preamble:\n            return None\n\n        return Event.EventPreambleInfo(cf_name, event_type, job_id, wal_id)\n\n    @staticmethod\n    def create_event(log_entry):\n        assert Event.is_an_event_entry(log_entry)\n\n        entry_msg = log_entry.get_msg()\n        event_json_str = entry_msg[entry_msg.find(\"{\"):]\n\n        try:\n            event_details_dict = json.loads(event_json_str)\n        except json.JSONDecodeError:\n            raise utils.ParsingError(\n                f\"Error decoding event's json fields.n{log_entry}\")\n\n        event_type = Event.get_event_data_field(\n            event_details_dict, EventField.EVENT_TYPE)\n\n        if event_type == EventType.FLUSH_STARTED:\n            event = FlushStartedEvent(log_entry)\n        elif event_type == EventType.FLUSH_FINISHED:\n            event = FlushFinishedEvent(log_entry)\n        elif event_type == EventType.COMPACTION_STARTED:\n            event = CompactionStartedEvent(log_entry)\n        elif event_type == EventType.COMPACTION_FINISHED:\n            event = CompactionFinishedEvent(log_entry)\n        elif event_type == EventType.TABLE_FILE_CREATION:\n            event = TableFileCreationEvent(log_entry)\n        elif event_type == EventType.TABLE_FILE_DELETION:\n            event = TableFileDeletionEvent(log_entry)\n        else:\n            raise utils.ParsingError(\n                f\"Unsupported event type ({event_type}).\\n{log_entry}\")\n\n        if not event.is_valid():\n            raise utils.ParsingError(\n                f\"Invalid event (Probably missing an Event Field) (\"\n                f\"Mandatory: {event.get_all_mandatory_fields()}.\\\n                n{log_entry}\")\n\n        return event\n\n    def __init__(self, log_entry):\n        assert Event.is_an_event_entry(log_entry)\n\n        entry_msg = log_entry.get_msg()\n        event_json_str = entry_msg[entry_msg.find(\"{\"):]\n\n        self.log_time_str = log_entry.get_time()\n        self.matching_event_info = None\n        self.event_details_dict = None\n        self.cf_name = None\n\n        try:\n            self.event_details_dict = json.loads(event_json_str)\n        except json.JSONDecodeError:\n            raise utils.ParsingError(\n                f\"Error decoding event's json fields.n{log_entry}\")\n\n        self.cf_name = self.get_event_data_field1(\n            EventField.CF_NAME, default=utils.NO_CF, field_expected=False)\n\n    def __str__(self):\n        if not self.does_have_details():\n            return \"Event: No Details\"\n\n        # Accessing all fields via their methods and not logging errors\n        # to avoid endless recursion\n        return f\"Event: type: {self.get_type(default=EventType.UNKNOWN)},\" \\\n               f\"job-id: {self.get_job_id(default=utils.INVALID_JOB_ID)}, \" \\\n               f\"cf: {self.get_cf_name(utils.INVALID_CF)}\"\n\n    # By default, sort events based on their time\n    def __lt__(self, other):\n        return self.get_log_time() < other.get_log_time()\n\n    def __eq__(self, other):\n        return self.get_log_time() == other.get_log_time() and \\\n               self.get_type() == other.get_type() and \\\n               self.get_cf_name() == other.get_cf_name()\n\n    def does_have_details(self):\n        return self.event_details_dict is not None\n\n    def does_have_field(self, field):\n        return self.get_event_data_field1(\n            field, default=None, field_expected=False) is not None\n\n    @staticmethod\n    def get_common_mandatory_fields():\n        return [EventField.EVENT_TYPE,\n                EventField.JOB_ID]\n\n    def get_all_mandatory_fields(self):\n        all_mandatory_fields = self.get_mandatory_fields()\n        all_mandatory_fields.extend(Event.get_common_mandatory_fields())\n        return all_mandatory_fields\n\n    def is_valid(self):\n        mandatory_fields = self.get_all_mandatory_fields()\n        for field in mandatory_fields:\n            if not self.does_have_field(field):\n                return False\n\n        return True\n\n    @staticmethod\n    def get_dict_field(field, fields_dict, default=None, field_expected=True):\n        field_str = field.value\n        if field_str not in fields_dict:\n            if default is None and field_expected:\n                raise utils.ParsingError(\n                    f\"Can't find field ({field.value}) in dict.\\\n                    n{fields_dict}\")\n            return default\n\n        return fields_dict[field_str]\n\n    @staticmethod\n    def get_event_data_field(\n            event_details_dict, field, default=None, field_expected=True):\n        assert isinstance(field, EventField)\n\n        try:\n            return Event.get_dict_field(\n                field, event_details_dict, default, field_expected)\n        except utils.ParsingError:\n            raise utils.ParsingError(\n                f\"Can't find field ({field.value}) in event details.\"\n                f\"\\n{event_details_dict}\")\n\n    def get_event_data_field1(self, field, default=None, field_expected=True):\n        return Event.get_event_data_field(self.event_details_dict, field,\n                                          default, field_expected)\n\n    def get_log_time(self):\n        return self.log_time_str\n\n    def get_type(self, default=None):\n        return self.get_event_data_field1(EventField.EVENT_TYPE, default)\n\n    def get_job_id(self, default=None):\n        job_id = self.get_event_data_field1(EventField.JOB_ID, default)\n        return job_id\n\n    def get_time_since_epoch_microseconds(self, default=None):\n        return self.get_event_data_field1(EventField.TIME_MICROS, default)\n\n    def get_event_data_dict(self):\n        return self.event_details_dict\n\n    def is_db_wide_event(self):\n        return self.get_cf_name() == utils.NO_CF\n\n    def is_cf_event(self):\n        return not self.is_db_wide_event()\n\n    def get_cf_name(self, default=utils.INVALID_CF):\n        return self.cf_name\n\n    def set_cf_name(self, cf_name):\n        curr_cf_name = self.get_cf_name()\n        if curr_cf_name == cf_name:\n            return\n\n        if curr_cf_name != utils.NO_CF:\n            raise utils.ParsingError(\n                f\"Trying to set cf name {cf_name} to an event that already \"\n                f\"has a cf name ({curr_cf_name}.\"\n                f\"\\n{self}\")\n\n        self.cf_name = cf_name\n\n    def set_wal_id(self, wal_id):\n        curr_wal_id = self.get_wal_id_if_available()\n\n        if curr_wal_id and curr_wal_id != wal_id:\n            raise utils.ParsingError(\n                f\"Trying to set wal id {wal_id} to an event that already \"\n                f\"has a wal id ({curr_wal_id}.\"\n                f\"\\n{self}\")\n\n        self.event_details_dict[EventField.WAL_ID.value] = wal_id\n\n    def get_wal_id_if_available(self, default=None):\n        return self.get_event_data_field1(EventField.WAL_ID, default,\n                                          field_expected=False)\n\n    def get_matching_event_info_if_exists(self):\n        return self.matching_event_info\n\n    def try_adding_preamble_event(self, preamble_info):\n        if self.get_type() != preamble_info.type:\n            return False\n\n        # Add the cf_name as if it was part of the event\n        self.set_cf_name(preamble_info.cf_name)\n\n        if preamble_info.wal_id is not None:\n            self.set_wal_id(preamble_info.wal_id)\n\n        return True\n\n    def set_matching_event_info(self, matching_event_info):\n        assert isinstance(matching_event_info, MatchingEventInfo)\n\n        matching_event = matching_event_info.event\n        assert self.get_job_id() == matching_event.get_job_id()\n        assert self.get_cf_name(default=utils.NO_CF) == \\\n               matching_event.get_cf_name(default=utils.NO_CF)\n\n        if self.matching_event_info:\n            logging.error(f\"Already have a matching event.\"\n                          f\"\\nMe:{self}\"\n                          f\"\\nCandidate:{matching_event_info.event}\")\n\n        self.matching_event_info = matching_event_info\n\n    def get_my_matching_type_info_if_exists(self):\n        return Event.get_matching_type_info_if_exists(self.get_type())\n\n    @staticmethod\n    def get_matching_type_info_if_exists(event_type):\n        for match in MATCHING_EVENTS:\n            if event_type == match[0]:\n                return MatchingEventTypeInfo(event_type=match[1],\n                                             associated_flow_type=match[2],\n                                             is_start=False)\n            elif event_type == match[1]:\n                return MatchingEventTypeInfo(event_type=match[0],\n                                             associated_flow_type=match[2],\n                                             is_start=True)\n\n        return None\n\n    def try_adding_matching_event(self, candidate_event):\n        assert isinstance(candidate_event, Event)\n\n        my_matching_type_info = self.get_my_matching_type_info_if_exists()\n        if my_matching_type_info is None:\n            return False\n\n        if my_matching_type_info.event_type != candidate_event.get_type():\n            return False\n\n        if self.get_job_id() != candidate_event.get_job_id():\n            return False\n\n        if self.get_cf_name() != candidate_event.get_cf_name():\n            return False\n\n        # The candidate is matching. Record it\n        associated_flow_type = my_matching_type_info.associated_flow_type\n        my_matching_event_info = \\\n            MatchingEventInfo(event=candidate_event,\n                              associated_flow_type=associated_flow_type,\n                              is_start=my_matching_type_info.is_start)\n        self.set_matching_event_info(my_matching_event_info)\n\n        candidate_matching_event_info =\\\n            MatchingEventInfo(event=self,\n                              associated_flow_type=associated_flow_type,\n                              is_start=not my_matching_event_info.is_start)\n        candidate_event.set_matching_event_info(candidate_matching_event_info)\n\n        return True", "\nclass Event:\n    @dataclass\n    class EventPreambleInfo:\n        cf_name: str\n        type: str\n        job_id: str\n        wal_id: str = None\n\n        def are_equal_ignoring_wal_id(self, other):\n            return self.cf_name == other.cf_name and \\\n                   self.type == other.type and \\\n                   self.job_id == other.job_id\n\n    @staticmethod\n    def is_an_event_entry(log_entry):\n        assert isinstance(log_entry, LogEntry)\n        return re.findall(regexes.EVENT, log_entry.get_msg()) != []\n\n    @staticmethod\n    def try_parse_as_preamble(log_entry):\n        event_msg = log_entry.get_msg()\n\n        is_preamble, cf_name, job_id, wal_id = \\\n            FlushStartedEvent.try_parse_as_preamble(event_msg)\n        if is_preamble:\n            event_type = EventType.FLUSH_STARTED\n        else:\n            wal_id = None\n\n        if not is_preamble:\n            is_preamble, cf_name, job_id = \\\n                CompactionStartedEvent.try_parse_as_preamble(event_msg)\n            if is_preamble:\n                event_type = EventType.COMPACTION_STARTED\n\n        if not is_preamble:\n            return None\n\n        return Event.EventPreambleInfo(cf_name, event_type, job_id, wal_id)\n\n    @staticmethod\n    def create_event(log_entry):\n        assert Event.is_an_event_entry(log_entry)\n\n        entry_msg = log_entry.get_msg()\n        event_json_str = entry_msg[entry_msg.find(\"{\"):]\n\n        try:\n            event_details_dict = json.loads(event_json_str)\n        except json.JSONDecodeError:\n            raise utils.ParsingError(\n                f\"Error decoding event's json fields.n{log_entry}\")\n\n        event_type = Event.get_event_data_field(\n            event_details_dict, EventField.EVENT_TYPE)\n\n        if event_type == EventType.FLUSH_STARTED:\n            event = FlushStartedEvent(log_entry)\n        elif event_type == EventType.FLUSH_FINISHED:\n            event = FlushFinishedEvent(log_entry)\n        elif event_type == EventType.COMPACTION_STARTED:\n            event = CompactionStartedEvent(log_entry)\n        elif event_type == EventType.COMPACTION_FINISHED:\n            event = CompactionFinishedEvent(log_entry)\n        elif event_type == EventType.TABLE_FILE_CREATION:\n            event = TableFileCreationEvent(log_entry)\n        elif event_type == EventType.TABLE_FILE_DELETION:\n            event = TableFileDeletionEvent(log_entry)\n        else:\n            raise utils.ParsingError(\n                f\"Unsupported event type ({event_type}).\\n{log_entry}\")\n\n        if not event.is_valid():\n            raise utils.ParsingError(\n                f\"Invalid event (Probably missing an Event Field) (\"\n                f\"Mandatory: {event.get_all_mandatory_fields()}.\\\n                n{log_entry}\")\n\n        return event\n\n    def __init__(self, log_entry):\n        assert Event.is_an_event_entry(log_entry)\n\n        entry_msg = log_entry.get_msg()\n        event_json_str = entry_msg[entry_msg.find(\"{\"):]\n\n        self.log_time_str = log_entry.get_time()\n        self.matching_event_info = None\n        self.event_details_dict = None\n        self.cf_name = None\n\n        try:\n            self.event_details_dict = json.loads(event_json_str)\n        except json.JSONDecodeError:\n            raise utils.ParsingError(\n                f\"Error decoding event's json fields.n{log_entry}\")\n\n        self.cf_name = self.get_event_data_field1(\n            EventField.CF_NAME, default=utils.NO_CF, field_expected=False)\n\n    def __str__(self):\n        if not self.does_have_details():\n            return \"Event: No Details\"\n\n        # Accessing all fields via their methods and not logging errors\n        # to avoid endless recursion\n        return f\"Event: type: {self.get_type(default=EventType.UNKNOWN)},\" \\\n               f\"job-id: {self.get_job_id(default=utils.INVALID_JOB_ID)}, \" \\\n               f\"cf: {self.get_cf_name(utils.INVALID_CF)}\"\n\n    # By default, sort events based on their time\n    def __lt__(self, other):\n        return self.get_log_time() < other.get_log_time()\n\n    def __eq__(self, other):\n        return self.get_log_time() == other.get_log_time() and \\\n               self.get_type() == other.get_type() and \\\n               self.get_cf_name() == other.get_cf_name()\n\n    def does_have_details(self):\n        return self.event_details_dict is not None\n\n    def does_have_field(self, field):\n        return self.get_event_data_field1(\n            field, default=None, field_expected=False) is not None\n\n    @staticmethod\n    def get_common_mandatory_fields():\n        return [EventField.EVENT_TYPE,\n                EventField.JOB_ID]\n\n    def get_all_mandatory_fields(self):\n        all_mandatory_fields = self.get_mandatory_fields()\n        all_mandatory_fields.extend(Event.get_common_mandatory_fields())\n        return all_mandatory_fields\n\n    def is_valid(self):\n        mandatory_fields = self.get_all_mandatory_fields()\n        for field in mandatory_fields:\n            if not self.does_have_field(field):\n                return False\n\n        return True\n\n    @staticmethod\n    def get_dict_field(field, fields_dict, default=None, field_expected=True):\n        field_str = field.value\n        if field_str not in fields_dict:\n            if default is None and field_expected:\n                raise utils.ParsingError(\n                    f\"Can't find field ({field.value}) in dict.\\\n                    n{fields_dict}\")\n            return default\n\n        return fields_dict[field_str]\n\n    @staticmethod\n    def get_event_data_field(\n            event_details_dict, field, default=None, field_expected=True):\n        assert isinstance(field, EventField)\n\n        try:\n            return Event.get_dict_field(\n                field, event_details_dict, default, field_expected)\n        except utils.ParsingError:\n            raise utils.ParsingError(\n                f\"Can't find field ({field.value}) in event details.\"\n                f\"\\n{event_details_dict}\")\n\n    def get_event_data_field1(self, field, default=None, field_expected=True):\n        return Event.get_event_data_field(self.event_details_dict, field,\n                                          default, field_expected)\n\n    def get_log_time(self):\n        return self.log_time_str\n\n    def get_type(self, default=None):\n        return self.get_event_data_field1(EventField.EVENT_TYPE, default)\n\n    def get_job_id(self, default=None):\n        job_id = self.get_event_data_field1(EventField.JOB_ID, default)\n        return job_id\n\n    def get_time_since_epoch_microseconds(self, default=None):\n        return self.get_event_data_field1(EventField.TIME_MICROS, default)\n\n    def get_event_data_dict(self):\n        return self.event_details_dict\n\n    def is_db_wide_event(self):\n        return self.get_cf_name() == utils.NO_CF\n\n    def is_cf_event(self):\n        return not self.is_db_wide_event()\n\n    def get_cf_name(self, default=utils.INVALID_CF):\n        return self.cf_name\n\n    def set_cf_name(self, cf_name):\n        curr_cf_name = self.get_cf_name()\n        if curr_cf_name == cf_name:\n            return\n\n        if curr_cf_name != utils.NO_CF:\n            raise utils.ParsingError(\n                f\"Trying to set cf name {cf_name} to an event that already \"\n                f\"has a cf name ({curr_cf_name}.\"\n                f\"\\n{self}\")\n\n        self.cf_name = cf_name\n\n    def set_wal_id(self, wal_id):\n        curr_wal_id = self.get_wal_id_if_available()\n\n        if curr_wal_id and curr_wal_id != wal_id:\n            raise utils.ParsingError(\n                f\"Trying to set wal id {wal_id} to an event that already \"\n                f\"has a wal id ({curr_wal_id}.\"\n                f\"\\n{self}\")\n\n        self.event_details_dict[EventField.WAL_ID.value] = wal_id\n\n    def get_wal_id_if_available(self, default=None):\n        return self.get_event_data_field1(EventField.WAL_ID, default,\n                                          field_expected=False)\n\n    def get_matching_event_info_if_exists(self):\n        return self.matching_event_info\n\n    def try_adding_preamble_event(self, preamble_info):\n        if self.get_type() != preamble_info.type:\n            return False\n\n        # Add the cf_name as if it was part of the event\n        self.set_cf_name(preamble_info.cf_name)\n\n        if preamble_info.wal_id is not None:\n            self.set_wal_id(preamble_info.wal_id)\n\n        return True\n\n    def set_matching_event_info(self, matching_event_info):\n        assert isinstance(matching_event_info, MatchingEventInfo)\n\n        matching_event = matching_event_info.event\n        assert self.get_job_id() == matching_event.get_job_id()\n        assert self.get_cf_name(default=utils.NO_CF) == \\\n               matching_event.get_cf_name(default=utils.NO_CF)\n\n        if self.matching_event_info:\n            logging.error(f\"Already have a matching event.\"\n                          f\"\\nMe:{self}\"\n                          f\"\\nCandidate:{matching_event_info.event}\")\n\n        self.matching_event_info = matching_event_info\n\n    def get_my_matching_type_info_if_exists(self):\n        return Event.get_matching_type_info_if_exists(self.get_type())\n\n    @staticmethod\n    def get_matching_type_info_if_exists(event_type):\n        for match in MATCHING_EVENTS:\n            if event_type == match[0]:\n                return MatchingEventTypeInfo(event_type=match[1],\n                                             associated_flow_type=match[2],\n                                             is_start=False)\n            elif event_type == match[1]:\n                return MatchingEventTypeInfo(event_type=match[0],\n                                             associated_flow_type=match[2],\n                                             is_start=True)\n\n        return None\n\n    def try_adding_matching_event(self, candidate_event):\n        assert isinstance(candidate_event, Event)\n\n        my_matching_type_info = self.get_my_matching_type_info_if_exists()\n        if my_matching_type_info is None:\n            return False\n\n        if my_matching_type_info.event_type != candidate_event.get_type():\n            return False\n\n        if self.get_job_id() != candidate_event.get_job_id():\n            return False\n\n        if self.get_cf_name() != candidate_event.get_cf_name():\n            return False\n\n        # The candidate is matching. Record it\n        associated_flow_type = my_matching_type_info.associated_flow_type\n        my_matching_event_info = \\\n            MatchingEventInfo(event=candidate_event,\n                              associated_flow_type=associated_flow_type,\n                              is_start=my_matching_type_info.is_start)\n        self.set_matching_event_info(my_matching_event_info)\n\n        candidate_matching_event_info =\\\n            MatchingEventInfo(event=self,\n                              associated_flow_type=associated_flow_type,\n                              is_start=not my_matching_event_info.is_start)\n        candidate_event.set_matching_event_info(candidate_matching_event_info)\n\n        return True", "\n\nclass FlushStartedEvent(Event):\n    @staticmethod\n    def try_parse_as_preamble(event_msg):\n        # [column_family_name_000018] [JOB 38] Flushing memtable with next log file: 5 # noqa\n        # Returns is_preamble, cf_name, job_id, wal_id\n        match = re.search(regexes.FLUSH_EVENT_PREAMBLE, event_msg)\n        if not match:\n            return False, None, None, None\n\n        assert len(match.groups()) == 3\n\n        return True, \\\n            match.group('cf'), int(match.group('job_id')), \\\n            int(match.group('wal_id'))\n\n    # 2023/01/04-08:55:00.625647 27424 EVENT_LOG_v1\n    # {\"time_micros\": 1672822500625643, \"job\": 8,\n    # \"event\": \"flush_started\",\n    # \"num_memtables\": 1, \"num_entries\": 59913, \"num_deletes\": 0,\n    # \"total_data_size\": 61530651, \"memory_usage\": 66349552,\n    # \"flush_reason\": \"Write Buffer Full\"}\n    #\n    def __init__(self, log_entry):\n        super().__init__(log_entry)\n\n    @staticmethod\n    def get_mandatory_fields():\n        return [EventField.TIME_MICROS,\n                EventField.FLUSH_REASON]\n\n    def get_flush_reason(self, default=None):\n        return self.get_event_data_field1(EventField.FLUSH_REASON, default)\n\n    def get_num_entries(self, default=0):\n        return self.get_event_data_field1(EventField.NUM_ENTRIES, default)\n\n    def get_num_deletes(self, default=0):\n        return self.get_event_data_field1(EventField.NUM_DELETES, default)\n\n    def get_num_memtables(self, default=0):\n        return self.get_event_data_field1(EventField.NUM_MEMTABLES, default)\n\n    def get_total_data_size_bytes(self, default=0):\n        return self.get_event_data_field1(EventField.TOTAL_DATA_SIZE, default)", "\n\nclass FlushFinishedEvent(Event):\n    # 2023/01/04-08:55:00.743632 27424\n    # (Original Log Time 2023/01/04-08:55:00.743481)\n    # EVENT_LOG_v1 {\"time_micros\": 1672822500743473, \"job\": 8,\n    # \"event\": \"flush_finished\", \"output_compression\": \"NoCompression\",\n    # \"lsm_state\": [8, 3, 45, 427, 822, 0, 0], \"immutable_memtables\": 0}\n    #\n    def __init__(self, log_entry):\n        super().__init__(log_entry)\n\n    @staticmethod\n    def get_mandatory_fields():\n        return [EventField.TIME_MICROS]", "\n\nclass CompactionStartedEvent(Event):\n    @staticmethod\n    def try_parse_as_preamble(event_msg):\n        # [default] [JOB 13] Compacting 1@1 + 5@2 files to L2, score 1.63\n        # Returns is_preamble, cf_name, job_id\n        match = re.search(regexes.COMPACTION_EVENT_PREAMBLE, event_msg)\n        if not match:\n            return False, None, None\n\n        assert len(match.groups()) == 2\n\n        return True, match.group('cf'), int(match.group('job_id'))\n\n    # 2023/01/04-08:55:00.743718 27420 EVENT_LOG_v1\n    # {\"time_micros\": 1672822500743711, \"job\": 9,\n    # \"event\": \"compaction_started\", \"compaction_reason\": \"LevelL0FilesNum\",\n    # \"files_L0\": [17250, 17247, 17243, 17239], \"score\": 1,\n    # \"input_data_size\": 251316602}\n    #\n    def __init__(self, log_entry):\n        super().__init__(log_entry)\n\n    @staticmethod\n    def get_mandatory_fields():\n        return [EventField.TIME_MICROS,\n                EventField.COMPACTION_REASON]\n\n    def get_compaction_reason(self, default=None):\n        return self.get_event_data_field1(EventField.COMPACTION_REASON,\n                                          default)\n\n    def get_input_data_size_bytes(self, default=0):\n        return self.get_event_data_field1(EventField.INPUT_DATA_SIZE, default)\n\n    def get_input_files(self):\n        # returns {<level>: list(files)}\n        input_files = dict()\n\n        for level_str, files_list in self.get_event_data_dict().items():\n            if level_str.startswith('files_L'):\n                level = int(level_str.lstrip('files_L'))\n                input_files[level] = files_list\n\n        return input_files", "\n\nclass CompactionFinishedEvent(Event):\n    # 2023/01/04-08:55:00.746783 27413\n    # (Original Log Time 2023/01/04-08:55:00.746653) EVENT_LOG_v1\n    # {\"time_micros\": 1672822500746645, \"job\": 4,\n    # \"event\": \"compaction_finished\",\n    # \"compaction_time_micros\": 971568, \"compaction_time_cpu_micros\": 935180,\n    # \"output_level\": 1, \"num_output_files\": 7, \"total_output_size\": 437263613,\n    # \"num_input_records\": 424286, \"num_output_records\": 423497,\n    # \"num_subcompactions\": 1, \"output_compression\": \"NoCompression\",\n    # \"num_single_delete_mismatches\": 0, \"num_single_delete_fallthrough\": 0,\n    # \"lsm_state\": [4, 7, 45, 427, 822, 0, 0]}\n    #\n    def __init__(self, log_entry):\n        super().__init__(log_entry)\n\n    @staticmethod\n    def get_mandatory_fields():\n        return [EventField.TIME_MICROS]\n\n    def get_num_input_records(self, default=0):\n        return self.get_event_data_field1(EventField.NUM_INPUT_RECORDS,\n                                          default)\n\n    def get_compaction_duration_micros(self, default=0):\n        return self.get_event_data_field1(\n            EventField.COMPACTION_TIME_MICROS, default)\n\n    def get_compaction_duration_seconds(self, default=0):\n        return int(self.get_compaction_duration_micros() / 1000)\n\n    def get_total_output_size_bytes(self, default=0):\n        return self.get_event_data_field1(EventField.TOTAL_OUTPUT_SIZE,\n                                          default)\n\n    def get_output_level(self, default=utils.INVALID_LEVEL):\n        return self.get_event_data_field1(EventField.OUTPUT_LEVEL, default)\n\n    def get_num_output_files(self, default=0):\n        return self.get_event_data_field1(EventField.NUM_OUTPUT_FILES, default)", "\n\nclass TableFileCreationEvent(Event):\n    # Sample Event:\n    # 2023/01/04-09:04:59.399021 27424 EVENT_LOG_v1\n    # {\"time_micros\": 1672823099398998, \"cf_name\": \"default\", \"job\": 8564,\n    # \"event\": \"table_file_creation\", \"file_number\": 37155, \"file_size\":\n    # 62762756, \"file_checksum\": \"\", \"file_checksum_func_name\": \"Unknown\",\n    #\n    # \"table_properties\":\n    # {\"data_size\": 62396458, \"index_size\": 289284,\n    # \"index_partitions\": 0, \"top_level_index_size\": 0,\n    # \"index_key_is_user_key\": 1, \"index_value_is_delta_encoded\": 1,\n    # \"filter_size\": 75973, \"raw_key_size\": 1458576,\"raw_average_key_size\": 24,\n    # \"raw_value_size\": 60774000, \"raw_average_value_size\": 1000,\n    # \"num_data_blocks\": 15194, \"num_entries\": 60774, \"num_filter_entries\":\n    # 60774, \"num_deletions\": 0, \"num_merge_operands\": 0,\n    # \"num_range_deletions\": 0, \"format_version\": 0, \"fixed_key_len\": 0,\n    # \"filter_policy\": \"bloomfilter\", \"column_family_name\": \"default\",\n    # \"column_family_id\": 0, \"comparator\": \"leveldb.BytewiseComparator\",\n    # \"merge_operator\": \"nullptr\", \"prefix_extractor_name\": \"nullptr\",\n    # \"property_collectors\": \"[]\", \"compression\": \"NoCompression\",\n    # \"compression_options\": \"window_bits=-14; level=32767; strategy=0;\n    # max_dict_bytes=0; zstd_max_train_bytes=0; enabled=0;\n    # max_dict_buffer_bytes=0; \", \"creation_time\": 1672823099,\n    # \"oldest_key_time\": 1672823099, \"file_creation_time\": 1672823099,\n    # \"slow_compression_estimated_data_size\": 0,\n    # \"fast_compression_estimated_data_size\": 0,\n    # \"db_id\": \"c100448c-dc04-4c74-8ab2-65d72f3aa3a8\",\n    # \"db_session_id\": \"4GAWIG5RIF8PQWM3NOQG\", \"orig_file_number\": 37155}}\n    #\n\n    NO_FILTER = \"\"\n\n    def __init__(self, log_entry):\n        super().__init__(log_entry)\n\n    @staticmethod\n    def get_mandatory_fields():\n        return [EventField.TIME_MICROS,\n                EventField.CF_NAME,\n                EventField.FILE_NUMBER,\n                EventField.TABLE_PROPERTIES]\n\n    def get_cf_id(self, default=utils.INVALID_CF_ID):\n        cf_id =\\\n            self.get_table_properties_field(\n                TablePropertiesField.CF_ID, default)\n        if cf_id == utils.INVALID_CF_ID:\n            return None\n\n        return cf_id\n\n    def get_created_file_number(self, default=None):\n        return self.get_event_data_field1(EventField.FILE_NUMBER, default)\n\n    def get_compressed_file_size_bytes(self, default=0):\n        return self.get_event_data_field1(EventField.FILE_SIZE, default)\n\n    def get_compressed_data_size_bytes(self, default=0):\n        return self.get_table_properties_field(\n            TablePropertiesField.DATA_SIZE, default)\n\n    def get_num_data_blocks(self, default=0):\n        return self.get_table_properties_field(\n            TablePropertiesField.NUM_DATA_BLOCKS, default)\n\n    def get_total_keys_sizes_bytes(self, default=0):\n        return self.get_table_properties_field(\n            TablePropertiesField.TOTAL_KEY_SIZE, default)\n\n    def get_total_values_sizes_bytes(self, default=0):\n        return self.get_table_properties_field(\n            TablePropertiesField.TOTAL_VALUE_SIZE, default)\n\n    def get_data_size_bytes(self, default=0):\n        return self.get_total_keys_sizes_bytes() + \\\n            self.get_total_values_sizes_bytes()\n\n    def get_index_size_bytes(self, default=0):\n        return self.get_table_properties_field(\n            TablePropertiesField.INDEX_SIZE, default)\n\n    def get_filter_size_bytes(self, default=0):\n        return self.get_table_properties_field(\n            TablePropertiesField.FILTER_SIZE, default)\n\n    def does_use_filter(self):\n        filter_policy = \\\n            self.get_table_properties_field(\n                TablePropertiesField.FILTER_POLICY,\n                default=None, field_expected=False)\n        return filter_policy is not None and filter_policy != \\\n            TableFileCreationEvent.NO_FILTER\n\n    def get_filter_policy(self):\n        if not self.does_use_filter():\n            return None\n        return \\\n            self.get_table_properties_field(TablePropertiesField.FILTER_POLICY)\n\n    def get_num_filter_entries(self, default=0):\n        return self.get_table_properties_field(\n            TablePropertiesField.NUM_FILTER_ENTRIES, default)\n\n    def get_compression_type(self, default=None):\n        compression_type = \\\n            self.get_table_properties_field(\n                TablePropertiesField.COMPRESSION_TYPE, default=\"\")\n        if compression_type == \"\":\n            return None\n        return compression_type\n\n    def get_table_properties_field(self, table_field, default=None,\n                                   field_expected=True):\n        assert isinstance(table_field, TablePropertiesField)\n\n        table_properties_dict = \\\n            self.get_event_data_field1(EventField.TABLE_PROPERTIES)\n\n        try:\n            return self.get_dict_field(\n                table_field, table_properties_dict, default, field_expected)\n        except utils.ParsingError:\n            raise utils.ParsingError(\n                f\"{table_field} not in table properties.\\n{self}\")", "\n\nclass TableFileDeletionEvent(Event):\n    # Sample Event:\n    # 2023/01/04-09:05:00.808463 27416 EVENT_LOG_v1\n    # {\"time_micros\": 1672823100808460, \"job\": 8423,\n    # \"event\": \"table_file_deletion\", \"file_number\": 37162}\n    #\n    def __init__(self, log_entry):\n        super().__init__(log_entry)\n\n    @staticmethod\n    def get_mandatory_fields():\n        return [EventField.TIME_MICROS,\n                EventField.FILE_NUMBER]\n\n    def get_deleted_file_number(self, default=None):\n        return self.get_event_data_field1(EventField.FILE_NUMBER, default)", "\n\nclass EventsMngr:\n    \"\"\"\n    The events manager contains all of the events.\n\n    It stores them in a dictionary of the following format:\n    <cf-name>: Dictionary of cf events\n    (The db-wide events are stored under the \"cf name\" No_COL_NAME)\n\n    Dictionary of cf events is itself a dictionary of the following format:\n    <event-type>: List of Event-s, ordered by their time\n    \"\"\"\n    def __init__(self, job_id_to_cf_name_map):\n        assert isinstance(job_id_to_cf_name_map, dict)\n\n        self.job_id_to_cf_name_map = job_id_to_cf_name_map\n        self.preambles = dict()\n        self.events = dict()\n\n    def try_parsing_as_preamble(self, entry):\n        preamble_info = Event.try_parse_as_preamble(entry)\n        if not preamble_info:\n            return None\n        assert isinstance(preamble_info, Event.EventPreambleInfo)\n\n        # If a preamble was already encountered, it must be for the same\n        # parameters\n        job_id = preamble_info.job_id\n        if preamble_info.job_id not in self.preambles:\n            self.preambles[job_id] = preamble_info\n        else:\n            if not self.preambles[job_id].are_equal_ignoring_wal_id(\n                    preamble_info):\n                logging.error(\n                    f\"A preamble with same job id exists, but other preamble\"\n                    f\" info mismatching (IGNORING NEW). \"\n                    f\"Existing:{self.preambles[job_id]}. New:{preamble_info}\")\n                # Indicating as a preamble, but returning the existing one\n                return self.preambles[job_id]\n\n        return preamble_info\n\n    def try_adding_entry(self, entry):\n        assert isinstance(entry, LogEntry)\n\n        # A preamble event is an entry that will be pending for its\n        # associated event entry to provide the event with its cf name\n        preamble_info = self.try_parsing_as_preamble(entry)\n        if preamble_info:\n            return True, None, preamble_info.cf_name\n\n        if not Event.is_an_event_entry(entry):\n            return False, None, None\n\n        try:\n            event = Event.create_event(entry)\n        except utils.ParsingError as e:\n            # telling caller I have added it as an event since it's\n            # supposedly an event, but badly formatted somehow\n            logging.error(f\"Discarding badly constructed event.\\n{entry} \"\n                          f\"(exception: {e})\")\n            return True, None, None\n\n        # Combine associated event preamble, if any exists\n        event_job_id = event.get_job_id()\n\n        # The preamble may provide the cf_name and possible other info\n        if event_job_id in self.preambles:\n            preamble_info = self.preambles[event_job_id]\n            if event.try_adding_preamble_event(preamble_info):\n                del(self.preambles[event_job_id])\n\n        if event.is_db_wide_event():\n            self.__try_finding_cf_for_newly_added_event(event_job_id, event)\n\n        cf_name = event.get_cf_name()\n        try:\n            self.__add_event(event_job_id, cf_name, event.get_type(), event)\n        except utils.ParsingError:\n            logging.error(f\"Error adding an event.Discarding it. \\n\"\n                          f\"event:{event}\")\n            return True, None, None\n\n        self.__try_to_match_newly_added_event(event)\n\n        if cf_name == utils.NO_CF:\n            cf_name = None\n        return True, event, cf_name\n\n    def __try_finding_cf_for_newly_added_event(self, event_job_id, event):\n        assert event.is_db_wide_event()\n\n        if event_job_id not in self.events:\n            return None\n\n        # Existing (earlier) events with the same job id should have\n        # their cf name set. Try to find one and use it\n        # Assuming a job id is unique to a cf\n        job_cfs_names = [cf_name for cf_name in\n                         self.events[event_job_id].keys() if cf_name !=\n                         utils.NO_CF]\n        if not job_cfs_names:\n            return None\n\n        assert len(job_cfs_names) == 1\n        cf_name = job_cfs_names[0]\n        event.set_cf_name(cf_name)\n\n        return cf_name\n\n    def __add_event(self, job_id, cf_name, event_type, event):\n        if job_id not in self.events:\n            self.events[job_id] = dict()\n        job_events = self.events[job_id]\n        EventsMngr.__validate_job_has_cf_or_no_other(job_events, cf_name,\n                                                     event)\n\n        if cf_name not in job_events:\n            job_events[cf_name] = dict()\n        if event_type not in job_events[cf_name]:\n            job_events[cf_name][event_type] = []\n        job_events[cf_name][event_type].append(event)\n\n    @staticmethod\n    def __validate_job_has_cf_or_no_other(job_events, cf_name, event):\n        # It is illegal to have a job id with events in multiple cf-s.\n        # The only other allowed \"cf\" is the no-col-family cf\n        if not job_events or \\\n                cf_name == utils.NO_CF or \\\n                cf_name in job_events:\n            return\n\n        # The only valid option is that job_events will only have\n        # the NO_COL_FAMILY or be empty (checked above)\n        job_cf_names = list(job_events.keys())\n        if job_cf_names != [utils.NO_CF]:\n            raise utils.ParsingError(\n                f\"Job has events for more than one cf. \"\n                f\"CF-s: ({job_cf_names}). cf_name:{cf_name}, \"\n                f\"\\nEvent:{event}\")\n\n    def __try_to_match_newly_added_event(self, new_event):\n        cf_name = new_event.get_cf_name()\n        new_event_type = new_event.get_type()\n\n        matching_event_type_info = \\\n            Event.get_matching_type_info_if_exists(new_event_type)\n        if not matching_event_type_info:\n            return\n        if not matching_event_type_info.is_start:\n            # Attempt to match only the end event (the matching will be the\n            # start)\n            return\n\n        potentially_matching_events =\\\n            self.get_cf_events_by_type(cf_name,\n                                       matching_event_type_info.event_type)\n        # Try to find a match from the most recent to the first\n        for potential_event in reversed(potentially_matching_events):\n            if new_event.try_adding_matching_event(potential_event):\n                break\n\n    def get_cf_events(self, cf_name):\n        all_cf_events = []\n\n        for job_events in self.events.values():\n            if cf_name not in job_events:\n                continue\n            for cf_events in job_events[cf_name].values():\n                all_cf_events.extend(list(cf_events))\n\n        # Return the events sorted by their time\n        all_cf_events.sort()\n        return all_cf_events\n\n    def get_cf_events_by_type(self, cf_name, event_type):\n        assert isinstance(event_type, EventType)\n\n        all_cf_events = []\n\n        for job_events in self.events.values():\n            if cf_name not in job_events:\n                continue\n            if event_type not in job_events[cf_name]:\n                continue\n            all_cf_events.extend(job_events[cf_name][event_type])\n\n        # The list may not be ordered due to the original time issue\n        # or having event preambles matched to their events somehow\n        # out of order. Sorting will insure correctness even if the list\n        # is already sorted\n        all_cf_events.sort()\n        return all_cf_events\n\n    def get_cf_flow_events(self, flow_type, cf_name):\n        flow_events = []\n\n        event_start_type = get_flow_start_event_type(flow_type)\n\n        starting_events = self.get_cf_events_by_type(cf_name,\n                                                     event_start_type)\n        for start_event in starting_events:\n            end_event = None\n            matching_end_event_info = \\\n                start_event.get_matching_event_info_if_exists()\n            if matching_end_event_info:\n                end_event = matching_end_event_info.event\n            flow_events.append((start_event, end_event))\n\n        return flow_events\n\n    def get_all_flow_events(self, flow_type, cfs_names):\n        # The cf_name is in the event data => no need to have a per-cf\n        # dictionary and we will sort the events based on their time\n        flow_events = []\n\n        for cf_name in cfs_names:\n            cf_flow_events = self.get_cf_flow_events(flow_type, cf_name)\n            if cf_flow_events:\n                flow_events.extend(cf_flow_events)\n\n        # Sort the events based on the start event (\n        flow_events.sort(key=lambda a: a[0])\n\n        return flow_events\n\n    def debug_get_all_events(self):\n        return self.events", ""]}
{"filename": "json_outputter.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport io\nimport json\nimport logging\n", "import logging\n\nimport cache_utils\nimport calc_utils\nimport display_utils\nimport log_file\nimport utils\n\n\ndef get_general_json(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    general_json = display_utils.prepare_db_wide_info_for_display(parsed_log)\n    if general_json[\"DB Size Time\"] is None:\n        del(general_json[\"DB Size Time\"])\n    if general_json[\"Ingest Time\"] is None:\n        del(general_json[\"Ingest Time\"])\n\n    return general_json", "\ndef get_general_json(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    general_json = display_utils.prepare_db_wide_info_for_display(parsed_log)\n    if general_json[\"DB Size Time\"] is None:\n        del(general_json[\"DB Size Time\"])\n    if general_json[\"Ingest Time\"] is None:\n        del(general_json[\"Ingest Time\"])\n\n    return general_json", "\n\ndef get_db_size_json(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n    stats_mngr = parsed_log.get_stats_mngr()\n    db_wide_stats_mngr = stats_mngr.get_db_wide_stats_mngr()\n    compactions_stats_mngr = stats_mngr.get_compactions_stats_mngr()\n\n    db_size_json = {}\n\n    ingest_info = calc_utils.get_db_ingest_info(db_wide_stats_mngr)\n    if ingest_info:\n        ingest_json = \\\n            display_utils.prepare_db_ingest_info_for_display(ingest_info)\n        db_size_json[\"Ingest\"] = ingest_json\n    else:\n        db_size_json[\"Ingest\"] = utils.DATA_UNAVAILABLE_TEXT\n\n    growth_info = \\\n        calc_utils.calc_cfs_size_bytes_growth(cfs_names,\n                                              compactions_stats_mngr)\n    growth_json =\\\n        display_utils.prepare_cfs_size_bytes_growth_for_display(growth_info)\n    db_size_json[\"CF-s Growth\"] = growth_json\n\n    return db_size_json", "\n\ndef get_flushes_json(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    flushes_json = {}\n\n    flushes_stats =\\\n        display_utils.prepare_cf_flushes_stats_for_display(parsed_log)\n\n    if flushes_stats:\n        for cf_name in parsed_log.get_cfs_names(include_auto_generated=False):\n            if cf_name in flushes_stats:\n                flushes_json[cf_name] = flushes_stats[cf_name]\n    else:\n        flushes_json = utils.NO_FLUSHES_TEXT\n\n    return flushes_json", "\n\ndef get_compactions_json(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    compactions_json = {}\n\n    compactions_stats = \\\n        display_utils.prepare_cf_compactions_stats_for_display(parsed_log)\n    if compactions_stats:\n        compactions_json.update(\n            display_utils.prepare_global_compactions_stats_for_display(\n                parsed_log))\n        compactions_json[\"CF-s\"] = {}\n        for cf_name in parsed_log.get_cfs_names(include_auto_generated=False):\n            if cf_name in compactions_stats:\n                compactions_json[\"CF-s\"][cf_name] = compactions_stats[cf_name]\n    else:\n        compactions_json = utils.NO_COMPACTIONS_TEXT\n\n    return compactions_json", "\n\ndef get_reads_json(parsed_log):\n    assert isinstance(parsed_log, log_file.ParsedLog)\n\n    db_options = parsed_log.get_database_options()\n    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n    stats_mngr = parsed_log.get_stats_mngr()\n    counters_mngr = parsed_log.get_counters_mngr()\n    files_monitor = parsed_log.get_files_monitor()\n\n    read_stats = \\\n        display_utils.prepare_applicable_read_stats(cfs_names,\n                                                    db_options,\n                                                    counters_mngr,\n                                                    stats_mngr,\n                                                    files_monitor)\n    if read_stats:\n        return read_stats\n    else:\n        return utils.NO_READS_TEXT", "\n\ndef get_seeks_json(parsed_log):\n    counters_mngr = \\\n        parsed_log.get_counters_mngr()\n    seek_stats = calc_utils.get_applicable_seek_stats(\n        counters_mngr)\n    if seek_stats:\n        disp_dict = display_utils.prepare_seek_stats_for_display(seek_stats)\n        return disp_dict\n    else:\n        return utils.NO_SEEKS_TEXT", "\n\ndef get_warn_warnings_json(cfs_names, warnings_mngr):\n    warnings_info = calc_utils.get_warn_warnings_info(cfs_names, warnings_mngr)\n    if warnings_info:\n        disp_dict = \\\n            display_utils.prepare_warn_warnings_for_display(warnings_info)\n        return disp_dict\n    else:\n        return utils.NO_WARNS_TEXT", "\n\ndef get_error_warnings_json(warnings_mngr):\n    return display_utils.prepare_error_or_fatal_warnings_for_display(\n        warnings_mngr, prepare_error=True)\n\n\ndef get_fatal_warnings_json(warnings_mngr):\n    return display_utils.prepare_error_or_fatal_warnings_for_display(\n        warnings_mngr, prepare_error=False)", "\n\ndef get_warnings_json(parsed_log):\n    cfs_names = parsed_log.get_cfs_names(include_auto_generated=False)\n    warnings_mngr = parsed_log.get_warnings_mngr()\n    warnings_json = get_warn_warnings_json(cfs_names, warnings_mngr)\n    return warnings_json\n\n\ndef get_block_cache_json(parsed_log):\n    cache_stats = \\\n        cache_utils.calc_block_cache_stats(\n            parsed_log.get_database_options(),\n            parsed_log.get_counters_mngr(),\n            parsed_log.get_files_monitor())\n    if cache_stats:\n        stats_mngr = parsed_log.get_stats_mngr()\n        detailed_block_cache_stats = \\\n            stats_mngr.get_block_cache_stats_mngr().get_all_cache_entries()\n\n        display_stats = \\\n            display_utils.prepare_block_cache_stats_for_display(\n                cache_stats, detailed_block_cache_stats)\n        return display_stats\n    else:\n        return utils.NO_BLOCK_CACHE_STATS", "\ndef get_block_cache_json(parsed_log):\n    cache_stats = \\\n        cache_utils.calc_block_cache_stats(\n            parsed_log.get_database_options(),\n            parsed_log.get_counters_mngr(),\n            parsed_log.get_files_monitor())\n    if cache_stats:\n        stats_mngr = parsed_log.get_stats_mngr()\n        detailed_block_cache_stats = \\\n            stats_mngr.get_block_cache_stats_mngr().get_all_cache_entries()\n\n        display_stats = \\\n            display_utils.prepare_block_cache_stats_for_display(\n                cache_stats, detailed_block_cache_stats)\n        return display_stats\n    else:\n        return utils.NO_BLOCK_CACHE_STATS", "\n\ndef get_json(parsed_log):\n    j = dict()\n\n    j[\"General\"] = get_general_json(parsed_log)\n    j[\"General\"][\"CF-s\"] = \\\n        display_utils.prepare_general_cf_info_for_display(parsed_log)\n\n    j[\"Options\"] = {\n        \"Diff\":\n            display_utils.get_options_baseline_diff_for_display(parsed_log),\n        \"All Options\": display_utils.get_all_options_for_display(parsed_log)\n    }\n\n    j[\"DB-Size\"] = get_db_size_json(parsed_log)\n\n    j[\"Flushes\"] = get_flushes_json(parsed_log)\n    j[\"Compactions\"] = get_compactions_json(parsed_log)\n    j[\"Reads\"] = get_reads_json(parsed_log)\n    j[\"Seeks\"] = get_seeks_json(parsed_log)\n    j[\"Warnings\"] = get_warnings_json(parsed_log)\n    j[\"Block-Cache-Stats\"] = get_block_cache_json(parsed_log)\n\n    return j", "\n\ndef get_json_dump_str(json_content):\n    f = io.StringIO()\n    json.dump(json_content, f, indent=1)\n    return f.getvalue()\n\n\ndef write_json(json_file_name, json_content, output_folder, report_to_console):\n    json_path = utils.get_json_file_path(output_folder, json_file_name)\n    with json_path.open(mode='w') as json_file:\n        json.dump(json_content, json_file)\n    logging.info(f\"JSON Output is in {json_path}\")\n    if report_to_console:\n        print(f\"JSON Output is in {json_path.as_uri()}\")", "def write_json(json_file_name, json_content, output_folder, report_to_console):\n    json_path = utils.get_json_file_path(output_folder, json_file_name)\n    with json_path.open(mode='w') as json_file:\n        json.dump(json_content, json_file)\n    logging.info(f\"JSON Output is in {json_path}\")\n    if report_to_console:\n        print(f\"JSON Output is in {json_path.as_uri()}\")\n"]}
{"filename": "baseline_log_files_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport bisect\nimport pathlib\nimport re\nfrom dataclasses import dataclass", "import re\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nimport log_file\nimport regexes\nimport utils\nfrom db_options import OptionsDiff, DatabaseOptions\n\n", "\n\n@dataclass\nclass Version:\n    major: int\n    minor: int\n    patch: int\n\n    def __init__(self, version_str):\n        version_parts = re.findall(regexes.VERSION, version_str)\n        assert len(version_parts) == 1 and len(version_parts[0]) == 3\n        self.major = int(version_parts[0][0])\n        self.minor = int(version_parts[0][1])\n        self.patch = int(version_parts[0][2]) if version_parts[0][2] else None\n\n    def get_patch_for_comparison(self):\n        if self.patch is None:\n            return -1\n        return self.patch\n\n    def __eq__(self, other):\n        return self.major == other.major and \\\n               self.minor == other.minor and \\\n               self.get_patch_for_comparison() == \\\n               other.get_patch_for_comparison()\n\n    def __lt__(self, other):\n        if self.major != other.major:\n            return self.major < other.major\n        elif self.minor != other.minor:\n            return self.minor < other.minor\n        else:\n            return self.get_patch_for_comparison() < \\\n                   other.get_patch_for_comparison()\n\n    def __repr__(self):\n        if self.patch is not None:\n            patch = f\".{self.patch}\"\n        else:\n            patch = \"\"\n\n        return f\"{self.major}.{self.minor}{patch}\"", "\n\n@dataclass\nclass BaselineLogFileInfo:\n    file_path: Path\n    version: Version\n\n    def __lt__(self, other):\n        return self.version < other.version\n", "\n\ndef find_all_baseline_log_files(baselines_logs_folder, product_name):\n    if product_name == utils.ProductName.ROCKSDB:\n        logs_regex = regexes.ROCKSDB_BASELINE_LOG_FILE\n    elif product_name == utils.ProductName.SPEEDB:\n        logs_regex = regexes.SPEEDB_BASELINE_LOG_FILE\n    else:\n        assert False\n\n    logs_folder_path = pathlib.Path(baselines_logs_folder).resolve()\n\n    files = []\n    for file_path in logs_folder_path.iterdir():\n        file_match = re.findall(logs_regex, file_path.name)\n        if file_match:\n            assert len(file_match) == 1\n            files.append(\n                BaselineLogFileInfo(file_path, Version(file_match[0])))\n\n    files.sort()\n    return files", "\n\ndef find_closest_version_idx(baseline_versions, version):\n    if isinstance(version, str):\n        version = Version(version)\n\n    if baseline_versions[0] == version:\n        return 0\n\n    baseline_versions.sort()\n    closest_version_idx = bisect.bisect_right(baseline_versions, version)\n\n    if closest_version_idx:\n        return closest_version_idx-1\n    else:\n        return None", "\n\ndef find_closest_baseline_info(baselines_logs_folder, product_name,\n                               version_str):\n    baseline_files = find_all_baseline_log_files(baselines_logs_folder,\n                                                 product_name)\n    baseline_versions = [file_info.version for file_info in baseline_files]\n    if not baseline_versions:\n        return None\n\n    closest_version_idx = find_closest_version_idx(baseline_versions,\n                                                   version_str)\n    if closest_version_idx is None:\n        return None\n\n    return baseline_files[closest_version_idx]", "\n\n@dataclass\nclass BaselineDBOptionsInfo:\n    baseline_log_path: Path = None\n    baseline_options: DatabaseOptions = None\n    closest_version: Version = None\n\n\ndef get_baseline_database_options(baselines_logs_folder,\n                                  product_name,\n                                  version_str):\n    closest_baseline_info = \\\n        find_closest_baseline_info(baselines_logs_folder,\n                                   product_name,\n                                   version_str)\n    if closest_baseline_info is None:\n        return None\n    assert isinstance(closest_baseline_info, BaselineLogFileInfo)\n\n    baseline_log_path = closest_baseline_info.file_path\n    with baseline_log_path.open() as baseline_log_file:\n        log_lines = baseline_log_file.readlines()\n        log_lines = [line for line in log_lines]\n\n        main_parsing_context = utils.parsing_context\n        parsed_log = log_file.ParsedLog(str(baseline_log_path), log_lines,\n                                        should_init_baseline_info=False)\n        utils.parsing_context = main_parsing_context\n\n        baseline_options = parsed_log.get_database_options()\n        return BaselineDBOptionsInfo(baseline_log_path,\n                                     baseline_options,\n                                     closest_baseline_info.version)", "\ndef get_baseline_database_options(baselines_logs_folder,\n                                  product_name,\n                                  version_str):\n    closest_baseline_info = \\\n        find_closest_baseline_info(baselines_logs_folder,\n                                   product_name,\n                                   version_str)\n    if closest_baseline_info is None:\n        return None\n    assert isinstance(closest_baseline_info, BaselineLogFileInfo)\n\n    baseline_log_path = closest_baseline_info.file_path\n    with baseline_log_path.open() as baseline_log_file:\n        log_lines = baseline_log_file.readlines()\n        log_lines = [line for line in log_lines]\n\n        main_parsing_context = utils.parsing_context\n        parsed_log = log_file.ParsedLog(str(baseline_log_path), log_lines,\n                                        should_init_baseline_info=False)\n        utils.parsing_context = main_parsing_context\n\n        baseline_options = parsed_log.get_database_options()\n        return BaselineDBOptionsInfo(baseline_log_path,\n                                     baseline_options,\n                                     closest_baseline_info.version)", "\n\n@dataclass\nclass OptionsDiffRelToBaselineInfo:\n    baseline_log_path: Path = None\n    diff: OptionsDiff = None\n    closest_version: Version = None\n\n\ndef find_options_diff_relative_to_baseline(baselines_logs_folder,\n                                           product_name,\n                                           version,\n                                           db_options):\n    baseline_info = get_baseline_database_options(baselines_logs_folder,\n                                                  product_name,\n                                                  version)\n    assert isinstance(baseline_info, BaselineDBOptionsInfo)\n\n    if baseline_info is None:\n        return None\n\n    diff = DatabaseOptions.get_options_diff(\n        baseline_info.baseline_options.get_all_options(),\n        db_options.get_all_options())\n\n    return OptionsDiffRelToBaselineInfo(baseline_info.baseline_log_path,\n                                        diff,\n                                        baseline_info.closest_version)", "\ndef find_options_diff_relative_to_baseline(baselines_logs_folder,\n                                           product_name,\n                                           version,\n                                           db_options):\n    baseline_info = get_baseline_database_options(baselines_logs_folder,\n                                                  product_name,\n                                                  version)\n    assert isinstance(baseline_info, BaselineDBOptionsInfo)\n\n    if baseline_info is None:\n        return None\n\n    diff = DatabaseOptions.get_options_diff(\n        baseline_info.baseline_options.get_all_options(),\n        db_options.get_all_options())\n\n    return OptionsDiffRelToBaselineInfo(baseline_info.baseline_log_path,\n                                        diff,\n                                        baseline_info.closest_version)", ""]}
{"filename": "db_files.py", "chunked_list": ["import copy\nimport logging\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\n\nimport events\nimport utils\n\n\n@dataclass\nclass DbFileInfo:\n    file_number: int\n    cf_name: str\n    creation_time: str\n    deletion_time: str = None\n    size_bytes: int = 0\n    compressed_size_bytes: int = 0\n    compressed_data_size_bytes: int = 0\n    data_size_bytes: int = 0\n    index_size_bytes: int = 0\n    filter_size_bytes: int = 0\n    filter_policy: str = None\n    num_filter_entries: int = 0\n    compression_type: str = None\n    level: int = None\n\n    def is_alive(self):\n        return self.deletion_time is None\n\n    def file_deleted(self, deletion_time):\n        assert self.is_alive()\n        self.deletion_time = deletion_time", "\n@dataclass\nclass DbFileInfo:\n    file_number: int\n    cf_name: str\n    creation_time: str\n    deletion_time: str = None\n    size_bytes: int = 0\n    compressed_size_bytes: int = 0\n    compressed_data_size_bytes: int = 0\n    data_size_bytes: int = 0\n    index_size_bytes: int = 0\n    filter_size_bytes: int = 0\n    filter_policy: str = None\n    num_filter_entries: int = 0\n    compression_type: str = None\n    level: int = None\n\n    def is_alive(self):\n        return self.deletion_time is None\n\n    def file_deleted(self, deletion_time):\n        assert self.is_alive()\n        self.deletion_time = deletion_time", "\n\nclass BlockType(Enum):\n    INDEX = auto()\n    FILTER = auto()\n\n\nclass BlockLiveFileStats:\n    def __init__(self):\n        self.num_created = 0\n        self.num_live = 0\n        # Total size of all the blocks in created files (never decreasing)\n        self.total_created_size_bytes = 0\n\n        # Current total size of live indexes / filters\n        self.curr_total_live_size_bytes = 0\n\n        # The size of the largest block created and its creation time\n        self.max_size_bytes = 0\n        self.max_size_time = None\n\n        # The max total size at some point in time\n        self.max_total_live_size_bytes = 0\n        self.max_total_live_size_time = None\n\n    def __eq__(self, other):\n        assert isinstance(other, BlockLiveFileStats)\n\n        return self.num_created == other.num_created and \\\n            self.num_live == other.num_live and \\\n            self. total_created_size_bytes == \\\n            other.total_created_size_bytes and \\\n            self.curr_total_live_size_bytes == \\\n            other.curr_total_live_size_bytes and \\\n            self.max_size_bytes == other.max_size_bytes and \\\n            self.max_size_time == other.max_size_time\n        # These are incorrect when there is more than a single cf\n        # and \\\n        # self.max_total_live_size_bytes == \\\n        # other.max_total_live_size_bytes and \\\n        # self.max_total_live_size_time == other.max_total_live_size_time\n\n    def block_created(self, size_bytes, time):\n        if size_bytes == 0:\n            return\n\n        self.num_created += 1\n        self.num_live += 1\n        self.total_created_size_bytes += size_bytes\n        self.curr_total_live_size_bytes += size_bytes\n\n        if self.max_size_bytes < size_bytes:\n            self.max_size_bytes = size_bytes\n            self.max_size_time = time\n\n        if self.max_total_live_size_bytes <\\\n                self.curr_total_live_size_bytes:\n            self.max_total_live_size_bytes = \\\n                self.curr_total_live_size_bytes\n            self.max_total_live_size_time = time\n\n    def block_deleted(self, size_bytes):\n        if size_bytes == 0:\n            return\n\n        assert self.num_live > 0\n        assert self.num_live <= self.num_created\n        self.num_live -= 1\n\n        assert self.curr_total_live_size_bytes >= \\\n               size_bytes\n        self.curr_total_live_size_bytes -= size_bytes\n\n    def get_avg_block_size(self):\n        if self.num_created == 0:\n            return 0\n        return self.total_created_size_bytes / self.num_created", "\n\nclass DbLiveFilesStats:\n    def __init__(self):\n        self.num_created = 0\n        self.num_live = 0\n\n        self.blocks_stats = {\n            BlockType.INDEX: BlockLiveFileStats(),\n            BlockType.FILTER: BlockLiveFileStats()\n        }\n\n    def file_created(self, file_info):\n        assert isinstance(file_info, DbFileInfo)\n\n        self.num_created += 1\n        self.num_live += 1\n        self.blocks_stats[BlockType.INDEX].block_created(\n            file_info.index_size_bytes, file_info.creation_time)\n        self.blocks_stats[BlockType.FILTER].block_created(\n            file_info.filter_size_bytes, file_info.creation_time)\n\n    def file_deleted(self, file_info):\n        assert isinstance(file_info, DbFileInfo)\n\n        assert self.num_live > 0\n        assert self.num_live <= self.num_created\n        self.num_live -= 1\n\n        self.blocks_stats[BlockType.INDEX].block_deleted(\n            file_info.index_size_bytes)\n        self.blocks_stats[BlockType.FILTER].block_deleted(\n            file_info.filter_size_bytes)", "\n\nclass DbFilesMonitor:\n    def __init__(self):\n        # Format: {<file_number>\n        self.files = dict()\n        self.live_files_stats = dict()\n        self.cfs_names = list()\n\n    def new_event(self, event):\n        event_type = event.get_type()\n        if event_type == events.EventType.TABLE_FILE_CREATION:\n            return self.file_created(event)\n        elif event_type == events.EventType.TABLE_FILE_DELETION:\n            return self.file_deleted(event)\n\n        return True\n\n    def file_created(self, event):\n        assert isinstance(event, events.TableFileCreationEvent)\n\n        file_number = event.get_created_file_number()\n        if file_number in self.files:\n            logging.error(\n                f\"Created File (number {file_number}) already exists, \"\n                f\"Ignoring Event.\\n{event}\")\n            return False\n\n        cf_name = event.get_cf_name()\n        file_info = DbFileInfo(file_number,\n                               cf_name=cf_name,\n                               creation_time=event.get_log_time())\n        file_info.compressed_size_bytes = \\\n            event.get_compressed_file_size_bytes()\n        file_info.compressed_data_size_bytes = \\\n            event.get_compressed_data_size_bytes()\n        file_info.data_size_bytes = event.get_data_size_bytes()\n        file_info.index_size_bytes = event.get_index_size_bytes()\n        file_info.filter_size_bytes = event.get_filter_size_bytes()\n        file_info.compression_type = event.get_compression_type()\n\n        if event.does_use_filter():\n            file_info.filter_policy = event.get_filter_policy()\n            file_info.num_filter_entries = event.get_num_filter_entries()\n\n        self.files[file_number] = file_info\n\n        if cf_name not in self.cfs_names:\n            self.cfs_names.append(cf_name)\n            assert cf_name not in self.live_files_stats\n            self.live_files_stats[cf_name] = DbLiveFilesStats()\n\n        self.live_files_stats[cf_name].file_created(file_info)\n\n        return True\n\n    def file_deleted(self, event):\n        assert isinstance(event, events.TableFileDeletionEvent)\n\n        file_number = event.get_deleted_file_number()\n        if file_number not in self.files:\n            logging.info(f\"File deleted event ((number {file_number}) \"\n                         f\"without a previous file created event, Ignoring \"\n                         f\"Event.\\n{event}\")\n            return False\n\n        file_info = self.files[file_number]\n        if not file_info.is_alive():\n            logging.error(\n                f\"Deleted File (number {file_number}) already \"\n                f\"deleted, Ignoring Event.\\n{event}\")\n            return False\n\n        file_info.file_deleted(event.get_log_time())\n        assert file_info.cf_name in self.live_files_stats\n        self.live_files_stats[file_info.cf_name].file_deleted(file_info)\n\n        return True\n\n    def get_all_files(self, file_filter=lambda info: True):\n        # Returns {<cf>: [<live files for this cf>\n        files = {}\n        for info in self.files.values():\n            if file_filter(info):\n                if info.cf_name not in files:\n                    files[info.cf_name] = list()\n                files[info.cf_name].append(info)\n        return files\n\n    def get_all_cf_files(self, cf_name):\n        all_cf_files = \\\n            self.get_all_files(lambda info: info.cf_name == cf_name)\n        if not all_cf_files:\n            return []\n        return all_cf_files[cf_name]\n\n    def get_all_live_files(self):\n        return self.get_all_files(lambda info: info.is_alive())\n\n    def get_cf_live_files(self, cf_name):\n        cf_live_files = \\\n            self.get_all_files(\n                lambda info: info.is_alive() and info.cf_name == cf_name)\n        if not cf_live_files:\n            return []\n        return cf_live_files[cf_name]\n\n    def get_blocks_stats(self):\n        stats = {}\n        for cf_name, cf_blocks_stats in self.live_files_stats.items():\n            stats[cf_name] = {\n                block_type: block_stats for block_type, block_stats in\n                cf_blocks_stats.blocks_stats.items()}\n        return stats\n\n    def get_cfs_names(self):\n        return self.cfs_names", "\n\n#\n# Files Stats\n#\n\n@dataclass\nclass CfFilterSpecificStats:\n    filter_policy: str = None\n    avg_bpk: float = 0.0", "\n\nclass CfsFilesStats:\n    def __init__(self):\n        self.blocks_stats = \\\n            {block_type: BlockLiveFileStats() for block_type in BlockType}\n\n        # {<cf_name>: FilterSpecificStats}\n        self.cfs_filter_specific = dict()\n", "\n\ndef get_block_stats_for_cfs_group(cfs_names, files_monitor, block_type):\n    assert isinstance(files_monitor, DbFilesMonitor)\n    assert isinstance(block_type, BlockType)\n\n    blocks_stats_all_cfs = files_monitor.get_blocks_stats()\n\n    stats = None\n    for cf_name in cfs_names:\n        if cf_name not in blocks_stats_all_cfs:\n            continue\n\n        assert block_type in blocks_stats_all_cfs[cf_name]\n        if stats is None:\n            stats = copy.deepcopy(blocks_stats_all_cfs[cf_name][block_type])\n        else:\n            block_stats = blocks_stats_all_cfs[cf_name][block_type]\n            assert isinstance(block_stats, BlockLiveFileStats)\n            stats.num_created += block_stats.num_created\n            stats.num_live += block_stats.num_live\n            stats.total_created_size_bytes += \\\n                block_stats.total_created_size_bytes\n            stats.curr_total_live_size_bytes += \\\n                block_stats.curr_total_live_size_bytes\n            if stats.max_size_bytes < block_stats.max_size_bytes:\n                stats.max_size_bytes = block_stats.max_size_bytes\n                stats.max_size_time = block_stats.max_size_time\n            stats.max_total_live_size_bytes += \\\n                block_stats.max_total_live_size_bytes\n\n    return stats", "\n\ndef calc_cf_files_stats(cfs_names, files_monitor):\n    assert isinstance(files_monitor, DbFilesMonitor)\n\n    stats = CfsFilesStats()\n\n    for block_type in BlockType:\n        stats.blocks_stats[block_type] = get_block_stats_for_cfs_group(\n            cfs_names, files_monitor, block_type)\n\n    num_cf_files = 0\n    for cf_name in cfs_names:\n        cf_files = files_monitor.get_all_cf_files(cf_name)\n        num_cf_files += len(cf_files) if cf_files else 0\n\n        if num_cf_files == 0:\n            logging.info(f\"No files for cf {cf_name}\")\n            continue\n\n        total_filters_sizes_bytes = 0\n        total_num_filters_entries = 0\n        filter_policy = None\n        for i, file_info in enumerate(cf_files):\n            if i == 0:\n                # Set the filter policy for the cf on the first. Changing\n                # it later is either a bug, or an unsupported (by the\n                # parser) dynamic change of the filter policy\n                filter_policy = file_info.filter_policy\n            else:\n                if filter_policy != utils.INVALID_FILTER_POLICY \\\n                        and filter_policy != file_info.filter_policy:\n                    logging.warning(\n                        f\"Filter policy changed for CF. Not supported\"\n                        f\"currently. cf:{cf_name}\\nfile_info:{file_info}\")\n                    filter_policy = utils.INVALID_FILTER_POLICY\n                    continue\n            total_filters_sizes_bytes += file_info.filter_size_bytes\n            total_num_filters_entries += file_info.num_filter_entries\n\n        avg_bpk = 0\n        if filter_policy is not None and filter_policy != \\\n                utils.INVALID_FILTER_POLICY:\n            if total_num_filters_entries > 0:\n                avg_bpk =\\\n                    (8 * total_filters_sizes_bytes) / total_num_filters_entries\n            else:\n                logging.warning(f\"No filter entries. cf:{cf_name}\\n\"\n                                f\"file info:{file_info}\")\n        stats.cfs_filter_specific[cf_name] =\\\n            CfFilterSpecificStats(filter_policy=filter_policy, avg_bpk=avg_bpk)\n\n    if num_cf_files == 0:\n        logging.info(f\"No files for all cf-s ({[cfs_names]}\")\n        return None\n\n    return stats", ""]}
{"filename": "log_file.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport logging\nimport re\n\nimport baseline_log_files_utils", "\nimport baseline_log_files_utils\nimport regexes\nimport utils\nfrom cfs_infos import CfsMetadata\nfrom compactions import CompactionsMonitor\nfrom counters import CountersMngr\nfrom db_files import DbFilesMonitor\nfrom db_options import DatabaseOptions\nfrom events import EventsMngr", "from db_options import DatabaseOptions\nfrom events import EventsMngr\nfrom log_entry import LogEntry\nfrom log_file_options_parser import LogFileOptionsParser\nfrom stats_mngr import StatsMngr\nfrom warnings_mngr import WarningsMngr\n\nget_error_context = utils.get_error_context_from_entry\n\n\nclass LogFileMetadata:\n    \"\"\"\n    Contains the metadata information about a log file:\n    - Product Name (RocksDB / Speedb)\n    - S/W Version\n    - Git Hash\n    - DB Session Id\n    - Times of first and last log entries in the file\n    \"\"\"\n    def __init__(self, log_entries, start_entry_idx):\n        self.product_name = None\n        self.version = None\n        self.db_session_id = None\n        self.git_hash = None\n        self.start_time = None\n        # Will be set later (when file is fully parsed)\n        self.end_time = None\n\n        if len(log_entries) == 0:\n            logging.warning(\"Empty Metadata pars (no entries)\")\n            return\n\n        self.start_time = log_entries[0].get_time()\n\n        # Parsing all entries and searching for predefined metadata\n        # entities. Some may or may not exist (e.g., the DB Session Id is\n        # not present in rolled logs). Also, no fixed order is assumed.\n        # However, it is a parsing error if the same entitiy is found more\n        # than once\n        for i, entry in enumerate(log_entries):\n            if self.try_parse_as_product_and_version_entry(entry):\n                continue\n            elif self.try_parse_as_git_hash_entry(entry):\n                continue\n            elif self.try_parse_as_db_session_id_entry(entry):\n                continue\n\n    def __str__(self):\n        start_time = self.start_time if self.start_time else \"UNKNOWN\"\n        end_time = self.end_time if self.end_time else \"UNKNOWN\"\n        return f\"LogFileMetadata: Start:{start_time}, End:{end_time}\"\n\n    def try_parse_as_product_and_version_entry(self, log_entry):\n        lines = str(log_entry.msg_lines[0]).strip()\n        match_parts = re.findall(regexes.PRODUCT_AND_VERSION, lines)\n\n        if not match_parts or len(match_parts) != 1:\n            return False\n\n        if self.product_name or self.version:\n            raise utils.ParsingError(\n                    f\"Product / Version already parsed. Product:\"\n                    f\"{self.product_name}, Version:{self.version}).\"\n                    f\"\\n{log_entry}\")\n\n        self.product_name, self.version = match_parts[0]\n        return True\n\n    def try_parse_as_git_hash_entry(self, log_entry):\n        lines = str(log_entry.msg_lines[0]).strip()\n        match_parts = re.findall(regexes.GIT_HASH_LINE, lines)\n\n        if not match_parts or len(match_parts) != 1:\n            return False\n\n        if self.git_hash:\n            raise utils.ParsingError(\n                f\"Git Hash Already Parsed ({self.git_hash})\\n{log_entry}\")\n\n        self.git_hash = match_parts[0]\n        return True\n\n    def try_parse_as_db_session_id_entry(self, log_entry):\n        lines = str(log_entry.msg_lines[0]).strip()\n        match_parts = re.findall(regexes.DB_SESSION_ID, lines)\n\n        if not match_parts or len(match_parts) != 1:\n            return False\n\n        if self.db_session_id:\n            raise utils.ParsingError(\n                f\"DB Session Id Already Parsed ({self.db_session_id})\"\n                f\"n{log_entry}\")\n\n        self.db_session_id = match_parts[0]\n        return True\n\n    def set_end_time(self, end_time):\n        assert not self.end_time,\\\n            f\"End time already set ({self.end_time})\"\n        assert utils.compare_times_strs(end_time, self.start_time) > 0\n\n        self.end_time = end_time\n\n    def is_valid(self):\n        return self.product_name and self.version\n\n    def get_product_name(self):\n        return self.product_name\n\n    def get_version(self):\n        return self.version\n\n    def get_git_hash(self):\n        return self.git_hash\n\n    def get_db_session_id(self):\n        return self.db_session_id\n\n    def get_start_time(self):\n        return self.start_time\n\n    def get_end_time(self):\n        return self.end_time\n\n    def get_log_time_span_seconds(self):\n        if not self.end_time:\n            raise utils.ParsingAssertion(f\"Unknown end time.\\n{self}\")\n        return int(utils.get_times_strs_diff_seconds(self.start_time,\n                                                     self.end_time))", "\n\nclass LogFileMetadata:\n    \"\"\"\n    Contains the metadata information about a log file:\n    - Product Name (RocksDB / Speedb)\n    - S/W Version\n    - Git Hash\n    - DB Session Id\n    - Times of first and last log entries in the file\n    \"\"\"\n    def __init__(self, log_entries, start_entry_idx):\n        self.product_name = None\n        self.version = None\n        self.db_session_id = None\n        self.git_hash = None\n        self.start_time = None\n        # Will be set later (when file is fully parsed)\n        self.end_time = None\n\n        if len(log_entries) == 0:\n            logging.warning(\"Empty Metadata pars (no entries)\")\n            return\n\n        self.start_time = log_entries[0].get_time()\n\n        # Parsing all entries and searching for predefined metadata\n        # entities. Some may or may not exist (e.g., the DB Session Id is\n        # not present in rolled logs). Also, no fixed order is assumed.\n        # However, it is a parsing error if the same entitiy is found more\n        # than once\n        for i, entry in enumerate(log_entries):\n            if self.try_parse_as_product_and_version_entry(entry):\n                continue\n            elif self.try_parse_as_git_hash_entry(entry):\n                continue\n            elif self.try_parse_as_db_session_id_entry(entry):\n                continue\n\n    def __str__(self):\n        start_time = self.start_time if self.start_time else \"UNKNOWN\"\n        end_time = self.end_time if self.end_time else \"UNKNOWN\"\n        return f\"LogFileMetadata: Start:{start_time}, End:{end_time}\"\n\n    def try_parse_as_product_and_version_entry(self, log_entry):\n        lines = str(log_entry.msg_lines[0]).strip()\n        match_parts = re.findall(regexes.PRODUCT_AND_VERSION, lines)\n\n        if not match_parts or len(match_parts) != 1:\n            return False\n\n        if self.product_name or self.version:\n            raise utils.ParsingError(\n                    f\"Product / Version already parsed. Product:\"\n                    f\"{self.product_name}, Version:{self.version}).\"\n                    f\"\\n{log_entry}\")\n\n        self.product_name, self.version = match_parts[0]\n        return True\n\n    def try_parse_as_git_hash_entry(self, log_entry):\n        lines = str(log_entry.msg_lines[0]).strip()\n        match_parts = re.findall(regexes.GIT_HASH_LINE, lines)\n\n        if not match_parts or len(match_parts) != 1:\n            return False\n\n        if self.git_hash:\n            raise utils.ParsingError(\n                f\"Git Hash Already Parsed ({self.git_hash})\\n{log_entry}\")\n\n        self.git_hash = match_parts[0]\n        return True\n\n    def try_parse_as_db_session_id_entry(self, log_entry):\n        lines = str(log_entry.msg_lines[0]).strip()\n        match_parts = re.findall(regexes.DB_SESSION_ID, lines)\n\n        if not match_parts or len(match_parts) != 1:\n            return False\n\n        if self.db_session_id:\n            raise utils.ParsingError(\n                f\"DB Session Id Already Parsed ({self.db_session_id})\"\n                f\"n{log_entry}\")\n\n        self.db_session_id = match_parts[0]\n        return True\n\n    def set_end_time(self, end_time):\n        assert not self.end_time,\\\n            f\"End time already set ({self.end_time})\"\n        assert utils.compare_times_strs(end_time, self.start_time) > 0\n\n        self.end_time = end_time\n\n    def is_valid(self):\n        return self.product_name and self.version\n\n    def get_product_name(self):\n        return self.product_name\n\n    def get_version(self):\n        return self.version\n\n    def get_git_hash(self):\n        return self.git_hash\n\n    def get_db_session_id(self):\n        return self.db_session_id\n\n    def get_start_time(self):\n        return self.start_time\n\n    def get_end_time(self):\n        return self.end_time\n\n    def get_log_time_span_seconds(self):\n        if not self.end_time:\n            raise utils.ParsingAssertion(f\"Unknown end time.\\n{self}\")\n        return int(utils.get_times_strs_diff_seconds(self.start_time,\n                                                     self.end_time))", "\n\nclass ParsedLog:\n    def __init__(self, log_file_path, log_lines, should_init_baseline_info):\n        logging.debug(f\"Starting to parse {log_file_path}\")\n        utils.parsing_context = utils.ParsingContext()\n        utils.parsing_context.parsing_starts(log_file_path)\n\n        self.log_file_path = log_file_path\n        self.metadata = None\n        self.db_options = DatabaseOptions()\n        self.cfs_metadata = CfsMetadata(self.log_file_path)\n        self.cfs_names = {}\n        self.next_unknown_cf_name_suffix = None\n\n        self.entry_idx = 0\n        self.log_entries, self.job_id_to_cf_name_map = \\\n            self.parse_log_to_entries(log_file_path, log_lines)\n\n        self.events_mngr = EventsMngr(self.job_id_to_cf_name_map)\n        self.compactions_monitor = CompactionsMonitor()\n        self.files_monitor = DbFilesMonitor()\n        self.warnings_mngr = WarningsMngr()\n        self.stats_mngr = StatsMngr()\n        self.counters_mngr = CountersMngr()\n        self.not_parsed_entries = []\n\n        self.parse_metadata()\n        self.set_end_time()\n        self.parse_rest_of_log()\n\n        # no need to take up the memory for that\n        self.log_entries = None\n\n        utils.parsing_context.parsing_ends()\n        self.cfs_metadata.parsing_complete()\n        self.warnings_mngr.set_cfs_names_on_parsing_complete(\n            self.get_cfs_names(include_auto_generated=False))\n\n        self.baseline_info = None\n        if should_init_baseline_info:\n            self.init_baseline_info()\n\n        logging.debug(f\"Parsing of {self.log_file_path} Complete\")\n\n    def __str__(self):\n        return f\"ParsedLog ({self.log_file_path})\"\n\n    @staticmethod\n    def parse_log_to_entries(log_file_path, log_lines):\n        if len(log_lines) < 1:\n            raise utils.EmptyLogFile(log_file_path)\n\n        # first line must be the beginning of a log entry\n        if not LogEntry.is_entry_start(log_lines[0]):\n            raise utils.InvalidLogFile(log_file_path)\n\n        logging.debug(\"Parsing log to entries\")\n\n        # Failure to parse an entry should just skip that entry\n        # (best effort)\n        log_entries = list()\n        job_id_to_cf_name_map = dict()\n\n        new_entry = None\n        skip_until_next_entry_start = False\n        for line_idx, line in enumerate(log_lines):\n            try:\n                if LogEntry.is_entry_start(line):\n                    skip_until_next_entry_start = False\n                    if new_entry:\n                        log_entries.append(new_entry.all_lines_added())\n                    new_entry = LogEntry(line_idx, line)\n                    ParsedLog.add_job_id_to_cf_mapping_if_available(\n                        new_entry, job_id_to_cf_name_map)\n                else:\n                    # To account for logs split into multiple lines\n                    if new_entry:\n                        new_entry.add_line(line)\n                    else:\n                        if not skip_until_next_entry_start:\n                            raise utils.ParsingAssertion(\n                                \"Bug while parsing log to entries.\",\n                                log_file_path, line_idx)\n            except utils.ParsingError as e:\n                logging.error(str(e.value))\n                # Discarding the \"bad\" entry and skipping all lines until\n                # finding the start of the next one.\n                new_entry = None\n                skip_until_next_entry_start = True\n\n        # Handle the last entry in the file.\n        if new_entry:\n            log_entries.append(new_entry.all_lines_added())\n\n        logging.debug(\"Completed Parsing log to entries\")\n\n        return log_entries, job_id_to_cf_name_map\n\n    @staticmethod\n    def add_job_id_to_cf_mapping_if_available(entry, job_id_to_cf_name_map):\n        job_id = entry.get_job_id()\n        if not job_id:\n            return\n\n        cf_name = entry.get_cf_name()\n        if not cf_name:\n            return\n\n        if job_id in job_id_to_cf_name_map:\n            assert job_id_to_cf_name_map[job_id] == cf_name\n        else:\n            job_id_to_cf_name_map[job_id] = cf_name\n\n    @staticmethod\n    def find_next_options_entry(log_entries, start_entry_idx):\n        entry_idx = start_entry_idx\n        while entry_idx < len(log_entries) and \\\n                not LogFileOptionsParser.is_options_entry(\n                    log_entries[entry_idx]):\n            entry_idx += 1\n\n        return (entry_idx < len(log_entries)), entry_idx\n\n    def parse_metadata(self):\n        # Metadata must be at the top of the log and surely doesn't extend\n        # beyond the first options line\n        has_found, options_entry_idx = \\\n            ParsedLog.find_next_options_entry(self.log_entries, self.entry_idx)\n\n        self.metadata = \\\n            LogFileMetadata(self.log_entries[self.entry_idx:options_entry_idx],\n                            self.entry_idx)\n        if not self.metadata.is_valid():\n            raise utils.InvalidLogFile(self.log_file_path)\n\n        self.entry_idx = options_entry_idx\n\n    def generate_next_unknown_cf_name(self):\n        # The first one is always \"default\" - NOT considered auto-generated\n        if self.next_unknown_cf_name_suffix is None:\n            self.next_unknown_cf_name_suffix = 1\n            return False, utils.DEFAULT_CF_NAME\n        else:\n            next_cf_name = f\"Unknown-CF-#{self.next_unknown_cf_name_suffix}\"\n            self.next_unknown_cf_name_suffix += 1\n            return True, next_cf_name\n\n    def parse_cf_options(self, cf_options_header_available):\n        if cf_options_header_available:\n            is_auto_generated = False\n            auto_generated_cf_name = None\n        else:\n            is_auto_generated, auto_generated_cf_name = \\\n                self.generate_next_unknown_cf_name()\n\n        cf_entry_idx = self.entry_idx\n        cf_name, options_dict, table_options_dict, self.entry_idx, \\\n            duplicate_option = \\\n            LogFileOptionsParser.parse_cf_options(self.log_entries,\n                                                  self.entry_idx,\n                                                  auto_generated_cf_name)\n        self.db_options.set_cf_options(cf_name,\n                                       options_dict,\n                                       table_options_dict)\n\n        # TODO - Handle failure in add_cf_found_during_cf_options_parsing\n        cf_id = None\n        self.cfs_metadata.add_cf_found_during_cf_options_parsing(\n            cf_name, cf_id, is_auto_generated, self.log_entries[cf_entry_idx])\n\n    @staticmethod\n    def find_support_info_start_index(log_entries, start_entry_idx):\n        entry_idx = start_entry_idx\n        while entry_idx < len(log_entries):\n            if re.findall(regexes.SUPPORT_INFO_START_LINE,\n                          log_entries[entry_idx].get_msg_lines()[0]):\n                return entry_idx\n            entry_idx += 1\n\n        raise utils.ParsingError(\n            f\"Failed finding Support Info. start-idx:{start_entry_idx}\")\n\n    def try_parse_as_cf_lifetime_entry(self):\n        parse_result, self.entry_idx = \\\n            self.cfs_metadata.try_parse_as_cf_lifetime_entries(\n                self.log_entries, self.entry_idx)\n        return parse_result\n\n    def are_dw_wide_options_set(self):\n        return self.db_options.are_db_wide_options_set()\n\n    def try_parse_as_db_wide_options(self):\n        if self.are_dw_wide_options_set() or \\\n                not LogFileOptionsParser.is_options_entry(\n                    self.get_curr_entry()):\n            return False\n\n        support_info_entry_idx = \\\n            ParsedLog.find_support_info_start_index(self.log_entries,\n                                                    self.entry_idx)\n\n        options_dict =\\\n            LogFileOptionsParser.parse_db_wide_options(self.log_entries,\n                                                       self.entry_idx,\n                                                       support_info_entry_idx)\n        if not options_dict:\n            raise utils.ParsingError(\n                f\"Empy db-wide options dictionary ({self}).\",\n                self.get_curr_error_context())\n\n        self.db_options.set_db_wide_options(options_dict)\n        self.entry_idx = support_info_entry_idx\n\n        return True\n\n    def try_parse_as_cf_options(self):\n        entry = self.get_curr_entry()\n        result = False\n        if LogFileOptionsParser.is_cf_options_start_entry(entry):\n            self.parse_cf_options(cf_options_header_available=True)\n            result = True\n        elif LogFileOptionsParser.is_options_entry(entry):\n            assert self.are_dw_wide_options_set()\n            self.parse_cf_options(cf_options_header_available=False)\n            result = True\n\n        return result\n\n    def try_parse_as_warning_entries(self):\n        entry = self.log_entries[self.entry_idx]\n\n        result = self.warnings_mngr.try_adding_entry(entry)\n        if result:\n            self.entry_idx += 1\n\n        return result\n\n    def try_parse_as_event_entries(self):\n        entry = self.get_curr_entry()\n\n        result, event, cf_name = self.events_mngr.try_adding_entry(entry)\n        if not result:\n            return False\n\n        self.add_cf_name_found_during_parsing(cf_name, entry)\n        if event:\n            self.compactions_monitor.new_event(event)\n            self.files_monitor.new_event(event)\n\n        self.entry_idx += 1\n\n        return True\n\n    def try_parse_as_stats_entries(self):\n        entry_idx_on_entry = self.entry_idx\n        result, self.entry_idx, cfs_names = \\\n            self.stats_mngr.try_adding_entries(self.log_entries,\n                                               self.entry_idx)\n\n        if result:\n            self.add_cfs_names_found_during_parsing(\n                cfs_names, self.get_entry(entry_idx_on_entry))\n\n        return result\n\n    def try_parse_as_counters_stats_entries(self):\n        result, self.entry_idx = \\\n            self.counters_mngr.try_adding_entries(\n                self.log_entries, self.entry_idx)\n\n        return result\n\n    def try_processing_in_monitors(self):\n        curr_entry = self.get_curr_entry()\n        processed, cf_name = \\\n            self.compactions_monitor.consider_entry(curr_entry)\n        if cf_name:\n            self.add_cf_name_found_during_parsing(cf_name, curr_entry)\n\n        return processed\n\n    def add_cf_name_found_during_parsing(self, cf_name, entry):\n        if cf_name is None:\n            return\n        self.add_cfs_names_found_during_parsing([cf_name], entry)\n\n    def add_cfs_names_found_during_parsing(self, cfs_names, entry):\n        if not cfs_names:\n            return\n        for cf_name in cfs_names:\n            self.cfs_metadata.handle_cf_name_found_during_parsing(\n                cf_name, entry)\n\n    def parse_rest_of_log(self):\n        # Parse all the entries and process those that are required\n        try:\n            while self.entry_idx < len(self.log_entries):\n                curr_entry_idx = self.entry_idx\n                try:\n                    if self.try_parse_as_cf_lifetime_entry():\n                        continue\n\n                    if self.try_parse_as_db_wide_options():\n                        continue\n\n                    if self.try_parse_as_cf_options():\n                        continue\n\n                    if self.try_parse_as_warning_entries():\n                        continue\n\n                    if self.try_parse_as_event_entries():\n                        continue\n\n                    if self.try_parse_as_stats_entries():\n                        continue\n\n                    if self.try_parse_as_counters_stats_entries():\n                        continue\n\n                    if not self.try_processing_in_monitors():\n                        self.not_parsed_entries.append(self.get_curr_entry())\n\n                    self.entry_idx += 1\n\n                except utils.ParsingError:\n                    logging.error(\"Error while parsing, skipping.\")\n\n                    # Make sure we are not stuck forever\n                    if curr_entry_idx == self.entry_idx:\n                        self.entry_idx += 1\n\n        except AssertionError:\n            logging.error(f\"Assertion While Parsing {self.log_file_path}\")\n            raise\n\n    def handle_cf_name_found_during_parsing(self, cf_name):\n        if not self.cfs_metadata.handle_cf_name_found_during_parsing(\n                cf_name, self.get_curr_entry()):\n            return\n\n    def init_baseline_info(self):\n        self.baseline_info = \\\n            baseline_log_files_utils.get_baseline_database_options(\n                utils.BASELINE_LOGS_FOLDER,\n                self.metadata.get_product_name(),\n                self.metadata.get_version())\n\n    def get_start_time(self):\n        return self.metadata.get_start_time()\n\n    def set_end_time(self):\n        last_entry = self.log_entries[-1]\n        end_time = last_entry.get_time()\n        self.metadata.set_end_time(end_time)\n\n    def get_num_seconds_from_start(self, time_str):\n        num_seconds =\\\n            utils.get_times_strs_diff_seconds(\n                self.get_start_time(), time_str)\n        if num_seconds < 0:\n            logging.warning(\n                f\"time ({time_str}) is before log start\\n{self.metadata}\")\n            return 0\n\n        return num_seconds\n\n    def get_log_file_path(self):\n        return self.log_file_path\n\n    def get_metadata(self):\n        return self.metadata\n\n    def get_cfs_names(self, include_auto_generated):\n        if include_auto_generated:\n            return self.cfs_metadata.get_all_cfs_names()\n        else:\n            return self.cfs_metadata.get_non_auto_generated_cfs_names()\n\n    def get_cfs_names_that_have_options(self, include_auto_generated):\n        # Return only the names of cf-s for which options exist\n        return self.cfs_metadata.get_cfs_names_that_have_options(\n            include_auto_generated)\n\n    def get_auto_generated_cfs_names(self):\n        return self.cfs_metadata.get_auto_generated_cf_names()\n\n    def does_have_auto_generated_cfs_names(self):\n        return len(self.get_auto_generated_cfs_names()) > 0\n\n    def get_num_cfs_when_certain(self):\n        return self.cfs_metadata.get_num_cfs_when_certain()\n\n    def get_database_options(self):\n        return self.db_options\n\n    def get_events_mngr(self):\n        return self.events_mngr\n\n    def get_compactions_monitor(self):\n        return self.compactions_monitor\n\n    def get_stats_mngr(self):\n        return self.stats_mngr\n\n    def get_counters_mngr(self):\n        return self.counters_mngr\n\n    def get_files_monitor(self):\n        return self.files_monitor\n\n    def get_warnings_mngr(self):\n        return self.warnings_mngr\n\n    def get_entry(self, entry_idx):\n        return self.log_entries[entry_idx]\n\n    def get_curr_entry(self):\n        if self.entry_idx < len(self.log_entries):\n            return self.log_entries[self.entry_idx]\n        else:\n            return None\n\n    def get_curr_error_context(self):\n        curr_entry = self.get_curr_entry()\n        if curr_entry is not None:\n            return get_error_context(curr_entry, self.log_file_path)\n        else:\n            return None\n\n    def get_baseline_info(self):\n        return self.baseline_info", ""]}
{"filename": "warnings_mngr.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport logging\nimport re\nfrom dataclasses import dataclass\nfrom enum import Enum", "from dataclasses import dataclass\nfrom enum import Enum\n\nimport regexes\nimport utils\nfrom log_entry import LogEntry\n\n\nclass WarningType(str, Enum):\n    WARN = \"WARN\"\n    ERROR = \"ERROR\"\n    FATAL = \"FATAL\"", "class WarningType(str, Enum):\n    WARN = \"WARN\"\n    ERROR = \"ERROR\"\n    FATAL = \"FATAL\"\n\n\n@dataclass\nclass WarningElementInfo:\n    time: str\n    code_pos: str\n    warning_msg: str\n\n    def __str__(self):\n        return f\"{self.time} [{self.code_pos}] {self.warning_msg}\"", "\n\nclass WarningCategory(str, Enum):\n    WRITE_DELAY = \"Write-Delay\"\n    WRITE_STOP = \"Write-Stop\"\n    OTHER = \"Other\"\n\n\nclass WarningsMngr:\n    def __init__(self):\n        self.cfs_names = None\n        # Stores all warnings during parsing (and before all cfs-names are\n        # known)\n        self.unprocessed_warnings = \\\n            {warning_type: list() for warning_type in WarningType}\n        # Replaces unprocessed_warnings once parsing completes\n        # Format:\n        # {<warning-type>: {<cf-name>: {<category>: [warn-info]}}}\n\n        self.processed_warnings = None\n\n    def try_adding_entry(self, entry):\n        assert isinstance(entry, LogEntry)\n\n        if not entry.is_warn_msg():\n            return False\n\n        warning_type = entry.get_warning_type()\n        warning_time = entry.get_time()\n        code_pos = entry.get_code_pos()\n        warning_msg = entry.get_msg()\n\n        warning_info = WarningElementInfo(warning_time, code_pos, warning_msg)\n        self.unprocessed_warnings[warning_type].append(warning_info)\n\n        return True\n\n    @staticmethod\n    def is_write_delay_msg(warn_msg):\n        return re.search(regexes.WRITE_DELAY_WARN_MSG, warn_msg.strip()) is \\\n               not None\n\n    @staticmethod\n    def is_write_stop_msg(warn_msg):\n        return re.search(regexes.WRITE_STOP_WARN_MSG, warn_msg.strip()) is \\\n               not None\n\n    @staticmethod\n    def classify_warning_msg(msg):\n        if WarningsMngr.is_write_delay_msg(msg):\n            return WarningCategory.WRITE_DELAY\n        elif WarningsMngr.is_write_stop_msg(msg):\n            return WarningCategory.WRITE_STOP\n        else:\n            return WarningCategory.OTHER\n\n    @staticmethod\n    def determine_warning_msg_cf(cfs_names, msg):\n        cfs_in_warn = utils.try_find_cfs_in_lines(cfs_names, msg.splitlines())\n\n        if cfs_in_warn is None:\n            return utils.NO_CF\n        elif isinstance(cfs_in_warn, list):\n            logging.info(f\"Warning msg with multiple cfs. Can't determine. \"\n                         f\"Placing in DB's bucket. cfs:{cfs_in_warn}\\nmsg\")\n            return utils.NO_CF\n        else:\n            return cfs_in_warn\n\n    def set_cfs_names_on_parsing_complete(self, cfs_names):\n        assert self.processed_warnings is None\n\n        self.cfs_names = cfs_names\n\n        # Now process all the warnings\n        # Processed warnings are organized as follows:\n        # {<warning-type>: {<cf-name>: {<category>: [warn-info]}}}\n        #\n        self.processed_warnings = \\\n            {warning_type: dict() for warning_type in WarningType}\n\n        for warning_type in self.unprocessed_warnings:\n            for info in self.unprocessed_warnings[warning_type]:\n                assert isinstance(info, WarningElementInfo)\n\n                category = WarningsMngr.classify_warning_msg(info.warning_msg)\n                assert isinstance(category, WarningCategory)\n\n                cf_name = \\\n                    WarningsMngr.determine_warning_msg_cf(\n                        self.cfs_names, info.warning_msg)\n                assert cf_name is not None\n\n                if cf_name not in self.processed_warnings[warning_type]:\n                    self.processed_warnings[warning_type][cf_name] = dict()\n                if category not in \\\n                        self.processed_warnings[warning_type][cf_name]:\n                    self.processed_warnings[warning_type][cf_name][\n                        category] = list()\n\n                self.processed_warnings[warning_type][cf_name][\n                    category].append(info)\n\n        for warning_type in list(self.processed_warnings.keys()):\n            if not self.processed_warnings[warning_type]:\n                del(self.processed_warnings[warning_type])\n\n        self.unprocessed_warnings = None\n\n    def is_parsing_complete(self):\n        return self.unprocessed_warnings is None\n\n    def verify_parsing_complete(self):\n        assert self.is_parsing_complete()\n\n    def get_all_warnings(self):\n        self.verify_parsing_complete()\n        return self.processed_warnings\n\n    def get_warnings_of_type(self, warning_type):\n        assert isinstance(warning_type, WarningType)\n\n        all_warnings = self.get_all_warnings()\n        if not all_warnings:\n            return None\n\n        if warning_type not in all_warnings:\n            return None\n\n        return all_warnings[warning_type]\n\n    def get_warn_warnings(self):\n        return self.get_warnings_of_type(WarningType.WARN)\n\n    def get_error_warnings(self):\n        return self.get_warnings_of_type(WarningType.ERROR)\n\n    def get_fatal_warnings(self):\n        return self.get_warnings_of_type(WarningType.FATAL)\n\n    def get_cf_warnings_of_type(self, cf_name, warning_type):\n        all_warnings_of_type = self.get_warnings_of_type(warning_type)\n        if not all_warnings_of_type:\n            return None\n\n        if cf_name not in all_warnings_of_type:\n            return None\n\n        return all_warnings_of_type[cf_name]\n\n    def get_cf_warn_warnings(self, cf_name):\n        return self.get_cf_warnings_of_type(cf_name, WarningType.WARN)\n\n    def get_cf_error_warnings(self, cf_name):\n        return self.get_cf_warnings_of_type(cf_name, WarningType.ERROR)\n\n    def get_cf_fatal_warnings(self, cf_name):\n        return self.get_cf_warnings_of_type(cf_name, WarningType.FATAL)\n\n    def get_cf_warnings_of_type_and_category(\n            self, cf_name, warning_type, category):\n        assert isinstance(category, WarningCategory)\n\n        all_cf_warnings_of_type = \\\n            self.get_cf_warnings_of_type(cf_name, warning_type)\n        if not all_cf_warnings_of_type:\n            return None\n\n        if category not in all_cf_warnings_of_type:\n            return None\n\n        return all_cf_warnings_of_type[category]\n\n    def get_total_num_of_type(self, warn_type):\n        all_of_type = self.get_warnings_of_type(warn_type)\n        if not all_of_type:\n            return 0\n\n        total_num = 0\n        for cf_name, cf_data in all_of_type.items():\n            for category in cf_data.keys():\n                total_num += len(cf_data[category])\n\n        return total_num\n\n    def get_total_num_warns(self):\n        return self.get_total_num_of_type(WarningType.WARN)\n\n    def get_total_num_errors(self):\n        return self.get_total_num_of_type(WarningType.ERROR)\n\n    def get_total_num_fatals(self):\n        return self.get_total_num_of_type(WarningType.FATAL)", "class WarningsMngr:\n    def __init__(self):\n        self.cfs_names = None\n        # Stores all warnings during parsing (and before all cfs-names are\n        # known)\n        self.unprocessed_warnings = \\\n            {warning_type: list() for warning_type in WarningType}\n        # Replaces unprocessed_warnings once parsing completes\n        # Format:\n        # {<warning-type>: {<cf-name>: {<category>: [warn-info]}}}\n\n        self.processed_warnings = None\n\n    def try_adding_entry(self, entry):\n        assert isinstance(entry, LogEntry)\n\n        if not entry.is_warn_msg():\n            return False\n\n        warning_type = entry.get_warning_type()\n        warning_time = entry.get_time()\n        code_pos = entry.get_code_pos()\n        warning_msg = entry.get_msg()\n\n        warning_info = WarningElementInfo(warning_time, code_pos, warning_msg)\n        self.unprocessed_warnings[warning_type].append(warning_info)\n\n        return True\n\n    @staticmethod\n    def is_write_delay_msg(warn_msg):\n        return re.search(regexes.WRITE_DELAY_WARN_MSG, warn_msg.strip()) is \\\n               not None\n\n    @staticmethod\n    def is_write_stop_msg(warn_msg):\n        return re.search(regexes.WRITE_STOP_WARN_MSG, warn_msg.strip()) is \\\n               not None\n\n    @staticmethod\n    def classify_warning_msg(msg):\n        if WarningsMngr.is_write_delay_msg(msg):\n            return WarningCategory.WRITE_DELAY\n        elif WarningsMngr.is_write_stop_msg(msg):\n            return WarningCategory.WRITE_STOP\n        else:\n            return WarningCategory.OTHER\n\n    @staticmethod\n    def determine_warning_msg_cf(cfs_names, msg):\n        cfs_in_warn = utils.try_find_cfs_in_lines(cfs_names, msg.splitlines())\n\n        if cfs_in_warn is None:\n            return utils.NO_CF\n        elif isinstance(cfs_in_warn, list):\n            logging.info(f\"Warning msg with multiple cfs. Can't determine. \"\n                         f\"Placing in DB's bucket. cfs:{cfs_in_warn}\\nmsg\")\n            return utils.NO_CF\n        else:\n            return cfs_in_warn\n\n    def set_cfs_names_on_parsing_complete(self, cfs_names):\n        assert self.processed_warnings is None\n\n        self.cfs_names = cfs_names\n\n        # Now process all the warnings\n        # Processed warnings are organized as follows:\n        # {<warning-type>: {<cf-name>: {<category>: [warn-info]}}}\n        #\n        self.processed_warnings = \\\n            {warning_type: dict() for warning_type in WarningType}\n\n        for warning_type in self.unprocessed_warnings:\n            for info in self.unprocessed_warnings[warning_type]:\n                assert isinstance(info, WarningElementInfo)\n\n                category = WarningsMngr.classify_warning_msg(info.warning_msg)\n                assert isinstance(category, WarningCategory)\n\n                cf_name = \\\n                    WarningsMngr.determine_warning_msg_cf(\n                        self.cfs_names, info.warning_msg)\n                assert cf_name is not None\n\n                if cf_name not in self.processed_warnings[warning_type]:\n                    self.processed_warnings[warning_type][cf_name] = dict()\n                if category not in \\\n                        self.processed_warnings[warning_type][cf_name]:\n                    self.processed_warnings[warning_type][cf_name][\n                        category] = list()\n\n                self.processed_warnings[warning_type][cf_name][\n                    category].append(info)\n\n        for warning_type in list(self.processed_warnings.keys()):\n            if not self.processed_warnings[warning_type]:\n                del(self.processed_warnings[warning_type])\n\n        self.unprocessed_warnings = None\n\n    def is_parsing_complete(self):\n        return self.unprocessed_warnings is None\n\n    def verify_parsing_complete(self):\n        assert self.is_parsing_complete()\n\n    def get_all_warnings(self):\n        self.verify_parsing_complete()\n        return self.processed_warnings\n\n    def get_warnings_of_type(self, warning_type):\n        assert isinstance(warning_type, WarningType)\n\n        all_warnings = self.get_all_warnings()\n        if not all_warnings:\n            return None\n\n        if warning_type not in all_warnings:\n            return None\n\n        return all_warnings[warning_type]\n\n    def get_warn_warnings(self):\n        return self.get_warnings_of_type(WarningType.WARN)\n\n    def get_error_warnings(self):\n        return self.get_warnings_of_type(WarningType.ERROR)\n\n    def get_fatal_warnings(self):\n        return self.get_warnings_of_type(WarningType.FATAL)\n\n    def get_cf_warnings_of_type(self, cf_name, warning_type):\n        all_warnings_of_type = self.get_warnings_of_type(warning_type)\n        if not all_warnings_of_type:\n            return None\n\n        if cf_name not in all_warnings_of_type:\n            return None\n\n        return all_warnings_of_type[cf_name]\n\n    def get_cf_warn_warnings(self, cf_name):\n        return self.get_cf_warnings_of_type(cf_name, WarningType.WARN)\n\n    def get_cf_error_warnings(self, cf_name):\n        return self.get_cf_warnings_of_type(cf_name, WarningType.ERROR)\n\n    def get_cf_fatal_warnings(self, cf_name):\n        return self.get_cf_warnings_of_type(cf_name, WarningType.FATAL)\n\n    def get_cf_warnings_of_type_and_category(\n            self, cf_name, warning_type, category):\n        assert isinstance(category, WarningCategory)\n\n        all_cf_warnings_of_type = \\\n            self.get_cf_warnings_of_type(cf_name, warning_type)\n        if not all_cf_warnings_of_type:\n            return None\n\n        if category not in all_cf_warnings_of_type:\n            return None\n\n        return all_cf_warnings_of_type[category]\n\n    def get_total_num_of_type(self, warn_type):\n        all_of_type = self.get_warnings_of_type(warn_type)\n        if not all_of_type:\n            return 0\n\n        total_num = 0\n        for cf_name, cf_data in all_of_type.items():\n            for category in cf_data.keys():\n                total_num += len(cf_data[category])\n\n        return total_num\n\n    def get_total_num_warns(self):\n        return self.get_total_num_of_type(WarningType.WARN)\n\n    def get_total_num_errors(self):\n        return self.get_total_num_of_type(WarningType.ERROR)\n\n    def get_total_num_fatals(self):\n        return self.get_total_num_of_type(WarningType.FATAL)", ""]}
{"filename": "cfs_infos.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport logging\nimport re\nfrom dataclasses import dataclass\nfrom enum import Enum, auto", "from dataclasses import dataclass\nfrom enum import Enum, auto\n\nimport regexes\nimport utils\n\nget_error_context = utils.get_error_context_from_entry\n\n\nclass CfDiscoveryType(Enum):\n    OPTIONS = 1\n    CREATE_NO_OPTIONS = 2\n    RECOVERED_NO_OPTIONS = 3\n    DURING_PARSING = 4", "\nclass CfDiscoveryType(Enum):\n    OPTIONS = 1\n    CREATE_NO_OPTIONS = 2\n    RECOVERED_NO_OPTIONS = 3\n    DURING_PARSING = 4\n\n\n@dataclass\nclass CfMetadata:\n    discovery_type: CfDiscoveryType\n    name: str\n    discovery_time: str\n    has_options: bool\n    auto_generated: bool\n    id: int = None\n    drop_time: str = None\n\n    def __lt__(self, other):\n        assert isinstance(other, CfMetadata)\n\n        if self.id is not None and other.id is not None:\n            return self.id < other.id\n        if self.discovery_type.value != other.discovery_type.value:\n            return self.discovery_type.value < other.discovery_type.value\n        if self.discovery_type == CfDiscoveryType.OPTIONS or \\\n                self.discovery_type == CfDiscoveryType.CREATE_NO_OPTIONS or \\\n                self.discovery_type == CfDiscoveryType.RECOVERED_NO_OPTIONS:\n            return self.discovery_time < other.discovery_time\n        return self.name < other.name", "@dataclass\nclass CfMetadata:\n    discovery_type: CfDiscoveryType\n    name: str\n    discovery_time: str\n    has_options: bool\n    auto_generated: bool\n    id: int = None\n    drop_time: str = None\n\n    def __lt__(self, other):\n        assert isinstance(other, CfMetadata)\n\n        if self.id is not None and other.id is not None:\n            return self.id < other.id\n        if self.discovery_type.value != other.discovery_type.value:\n            return self.discovery_type.value < other.discovery_type.value\n        if self.discovery_type == CfDiscoveryType.OPTIONS or \\\n                self.discovery_type == CfDiscoveryType.CREATE_NO_OPTIONS or \\\n                self.discovery_type == CfDiscoveryType.RECOVERED_NO_OPTIONS:\n            return self.discovery_time < other.discovery_time\n        return self.name < other.name", "\n\nclass CfOper(Enum):\n    CREATE = auto()\n    RECOVER = auto()\n    DROP = auto()\n\n\nclass CfsMetadata:\n    def __init__(self, log_file_path):\n        self.cfs_info = {}\n        self.log_file_path = log_file_path\n\n    def add_cf_found_during_cf_options_parsing(self, cf_name, cf_id,\n                                               is_auto_generated, entry):\n        if not self._validate_cf_doesnt_exist(cf_name, entry):\n            return False\n\n        self.cfs_info[cf_name] = \\\n            CfMetadata(discovery_type=CfDiscoveryType.OPTIONS,\n                       name=cf_name,\n                       discovery_time=entry.get_time(),\n                       has_options=True,\n                       auto_generated=is_auto_generated,\n                       id=cf_id)\n        return True\n\n    def handle_cf_name_found_during_parsing(self, cf_name, entry, cf_id=None):\n        if cf_name in self.cfs_info:\n            return False\n\n        logging.info(f\"Found new cf name during parsing [{cf_name}]\")\n        self.cfs_info[cf_name] = \\\n            CfMetadata(discovery_type=CfDiscoveryType.DURING_PARSING,\n                       name=cf_name,\n                       discovery_time=entry.get_time(),\n                       has_options=False,\n                       auto_generated=False,\n                       id=cf_id)\n        return True\n\n    def try_parse_as_cf_lifetime_entries(self, log_entries, entry_idx):\n        # CF lifetime messages in log are currently always single liners\n        entry = log_entries[entry_idx]\n        next_entry_idx = entry_idx + 1\n\n        try:\n            # The cf lifetime entries are printed when the CF will have been\n            # discovered\n            if self.try_parse_as_drop_cf(entry):\n                return True, next_entry_idx\n            if self.try_parse_as_recover_cf(entry):\n                return True, next_entry_idx\n            if self.try_parse_as_create_cf(entry):\n                return True, next_entry_idx\n\n            return None, entry_idx\n\n        except utils.ParsingError as e:\n            logging.error(\n                f\"Failed parsing entry as cf lifetime entry ({e}).\\n \"\n                f\"entry:{entry}\")\n            return True, next_entry_idx\n\n    def try_parse_as_drop_cf(self, entry):\n        cf_id_match = re.findall(regexes.DROP_CF, entry.get_msg())\n        if not cf_id_match:\n            return False\n\n        assert len(cf_id_match) == 1\n\n        dropped_cf_id = cf_id_match[0]\n        # A dropped cf may have been discovered without knowing its id (\n        # e.g., auto-generated one)\n        cf_info = self.get_cf_info_by_id(dropped_cf_id)\n        if not cf_info:\n            logging.info(utils.format_err_msg(\n                f\"CF with Id {dropped_cf_id} \"\n                f\"dropped but no cf with matching id.\", error_context=None,\n                entry=entry, file_path=self.log_file_path))\n            return True\n\n        self._validate_cf_wasnt_dropped(cf_info, entry, raise_exception=True)\n        cf_info.drop_time = entry.get_time()\n\n        return True\n\n    def try_parse_as_recover_cf(self, entry):\n        cf_match = re.search(regexes.RECOVERED_CF, entry.get_msg())\n        if not cf_match:\n            return False\n        assert len(cf_match.groups()) == 3\n\n        cf_name = cf_match.group('cf')\n        cf_id = int(cf_match.group('cf_id'))\n\n        if self._does_cf_exist(cf_name):\n            self._update_cf_id(cf_name, cf_id, entry, raise_exception=True)\n        else:\n            logging.info(f\"Created cf without options [{cf_name}]\")\n            self.cfs_info[cf_name] = \\\n                CfMetadata(discovery_type=CfDiscoveryType.RECOVERED_NO_OPTIONS,\n                           name=cf_name,\n                           discovery_time=entry.get_time(),\n                           has_options=False,\n                           auto_generated=False,\n                           id=cf_id)\n        return True\n\n    def try_parse_as_create_cf(self, entry):\n        cf_match = re.search(regexes.CREATE_CF, entry.get_msg())\n        if not cf_match:\n            return False\n\n        assert len(cf_match.groups()) == 2\n\n        cf_name = cf_match.group('cf')\n        cf_id = int(cf_match.group('cf_id'))\n\n        if self._does_cf_exist(cf_name):\n            self._update_cf_id(cf_name, cf_id, entry, raise_exception=True)\n        else:\n            logging.info(f\"Created cf without options [{cf_name}]\")\n            self.cfs_info[cf_name] = \\\n                CfMetadata(discovery_type=CfDiscoveryType.CREATE_NO_OPTIONS,\n                           name=cf_name,\n                           discovery_time=entry.get_time(),\n                           has_options=False,\n                           auto_generated=False,\n                           id=cf_id)\n\n        return True\n\n    def parsing_complete(self):\n        self.cfs_info = utils.sort_dict_on_values(self.cfs_info)\n\n    def set_cf_id(self, cf_name, cf_id, line, line_idx):\n        self._validate_cf_exists(cf_name, line, line_idx)\n\n        cf_id = int(cf_id)\n        self._update_cf_id(cf_name, cf_id, line, line_idx)\n\n        self.cfs_info[cf_name].id = int(cf_id)\n\n    def get_cf_id(self, cf_name):\n        cf_id = None\n        if cf_name in self.cfs_info:\n            cf_id = self.cfs_info[cf_name].id\n        return cf_id\n\n    def get_cf_info_by_name(self, cf_name):\n        cf_info = None\n        if cf_name in self.cfs_info:\n            cf_info = self.cfs_info[cf_name]\n        return cf_info\n\n    def get_cf_info_by_id(self, cf_id):\n        cf_id = int(cf_id)\n        for cf_info in self.cfs_info.values():\n            if cf_info.id == cf_id:\n                return cf_info\n\n        return None\n\n    def get_non_auto_generated_cfs_names(self):\n        return [cf_name for cf_name in self.cfs_info.keys() if\n                not self.cfs_info[cf_name].auto_generated]\n\n    def get_auto_generated_cf_names(self):\n        return [cf_name for cf_name in self.cfs_info.keys() if\n                self.cfs_info[cf_name].auto_generated]\n\n    def get_cfs_names_that_have_options(self, include_auto_generated):\n        returned_cfs_names = []\n        for cf_name in self.cfs_info.keys():\n            if self.cfs_info[cf_name].has_options:\n                if include_auto_generated or \\\n                        not self.cfs_info[cf_name].auto_generated:\n                    returned_cfs_names.append(cf_name)\n        return returned_cfs_names\n\n    def get_all_cfs_names(self):\n        return list(self.cfs_info.keys())\n\n    def get_num_cfs(self):\n        return len(self.cfs_info) if self.cfs_info else 0\n\n    def are_there_auto_generated_cf_names(self):\n        return len(self.get_auto_generated_cf_names()) > 0\n\n    def get_num_cfs_when_certain(self):\n        # Once a log has auto-generated cf names we have no certain way of\n        # knowing how many cf-s there are\n        if self.are_there_auto_generated_cf_names():\n            return None\n        else:\n            return self.get_num_cfs()\n\n    def was_cf_dropped(self, cf_name):\n        cf_info = self.get_cf_info_by_name(cf_name)\n        if not cf_info:\n            return False\n        return cf_info.drop_time is not None\n\n    def get_cf_drop_time(self, cf_name):\n        drop_time = None\n        cf_info = self.get_cf_info_by_name(cf_name)\n        if cf_info:\n            drop_time = cf_info.drop_time\n        return drop_time\n\n    def _validate_cf_exists(self, cf_name, entry):\n        if cf_name not in self.cfs_info:\n            raise utils.ParsingError(\n                f\"cf ({cf_name}) doesn't exist.\",\n                self.get_error_context(entry))\n\n    def _does_cf_exist(self, cf_name):\n        return cf_name in self.cfs_info\n\n    def _validate_cf_doesnt_exist(self, cf_name, entry):\n        if self._does_cf_exist(cf_name):\n            context = self.get_error_context(entry)\n            logging.error(f\"cf ({cf_name}) already exists.{context}\")\n            return False\n        return True\n\n    def _update_cf_id(self, cf_name, cf_id, entry, raise_exception):\n        curr_cf_id = self.cfs_info[cf_name].id\n        if curr_cf_id and curr_cf_id != int(cf_id):\n            context = self.get_error_context(entry)\n            if raise_exception:\n                raise utils.ParsingError(\n                    f\"id ({cf_id}) of cf ({cf_name}) already exists.\", context)\n            else:\n                logging.error(\n                    f\"id ({cf_id}) of cf ({cf_name}) already exists.{context}\")\n                return False\n\n        self.cfs_info[cf_name].id = cf_id\n        return True\n\n    def _validate_cf_wasnt_dropped(self, cf_info, entry, raise_exception):\n        if cf_info.drop_time:\n            context = self.get_error_context(entry)\n            if raise_exception:\n                raise utils.ParsingError(\n                    f\"CF ({cf_info.name}) already dropped at \"\n                    f\"({cf_info.drop_time}).\", context)\n            else:\n                logging.error(f\"CF ({cf_info.name}) already dropped at \"\n                              f\"({cf_info.drop_time}).{context}\")\n                return False\n        return True\n\n    def get_error_context(self, entry):\n        if entry is None:\n            return None\n        return utils.get_error_context_from_entry(entry, self.log_file_path)", "class CfsMetadata:\n    def __init__(self, log_file_path):\n        self.cfs_info = {}\n        self.log_file_path = log_file_path\n\n    def add_cf_found_during_cf_options_parsing(self, cf_name, cf_id,\n                                               is_auto_generated, entry):\n        if not self._validate_cf_doesnt_exist(cf_name, entry):\n            return False\n\n        self.cfs_info[cf_name] = \\\n            CfMetadata(discovery_type=CfDiscoveryType.OPTIONS,\n                       name=cf_name,\n                       discovery_time=entry.get_time(),\n                       has_options=True,\n                       auto_generated=is_auto_generated,\n                       id=cf_id)\n        return True\n\n    def handle_cf_name_found_during_parsing(self, cf_name, entry, cf_id=None):\n        if cf_name in self.cfs_info:\n            return False\n\n        logging.info(f\"Found new cf name during parsing [{cf_name}]\")\n        self.cfs_info[cf_name] = \\\n            CfMetadata(discovery_type=CfDiscoveryType.DURING_PARSING,\n                       name=cf_name,\n                       discovery_time=entry.get_time(),\n                       has_options=False,\n                       auto_generated=False,\n                       id=cf_id)\n        return True\n\n    def try_parse_as_cf_lifetime_entries(self, log_entries, entry_idx):\n        # CF lifetime messages in log are currently always single liners\n        entry = log_entries[entry_idx]\n        next_entry_idx = entry_idx + 1\n\n        try:\n            # The cf lifetime entries are printed when the CF will have been\n            # discovered\n            if self.try_parse_as_drop_cf(entry):\n                return True, next_entry_idx\n            if self.try_parse_as_recover_cf(entry):\n                return True, next_entry_idx\n            if self.try_parse_as_create_cf(entry):\n                return True, next_entry_idx\n\n            return None, entry_idx\n\n        except utils.ParsingError as e:\n            logging.error(\n                f\"Failed parsing entry as cf lifetime entry ({e}).\\n \"\n                f\"entry:{entry}\")\n            return True, next_entry_idx\n\n    def try_parse_as_drop_cf(self, entry):\n        cf_id_match = re.findall(regexes.DROP_CF, entry.get_msg())\n        if not cf_id_match:\n            return False\n\n        assert len(cf_id_match) == 1\n\n        dropped_cf_id = cf_id_match[0]\n        # A dropped cf may have been discovered without knowing its id (\n        # e.g., auto-generated one)\n        cf_info = self.get_cf_info_by_id(dropped_cf_id)\n        if not cf_info:\n            logging.info(utils.format_err_msg(\n                f\"CF with Id {dropped_cf_id} \"\n                f\"dropped but no cf with matching id.\", error_context=None,\n                entry=entry, file_path=self.log_file_path))\n            return True\n\n        self._validate_cf_wasnt_dropped(cf_info, entry, raise_exception=True)\n        cf_info.drop_time = entry.get_time()\n\n        return True\n\n    def try_parse_as_recover_cf(self, entry):\n        cf_match = re.search(regexes.RECOVERED_CF, entry.get_msg())\n        if not cf_match:\n            return False\n        assert len(cf_match.groups()) == 3\n\n        cf_name = cf_match.group('cf')\n        cf_id = int(cf_match.group('cf_id'))\n\n        if self._does_cf_exist(cf_name):\n            self._update_cf_id(cf_name, cf_id, entry, raise_exception=True)\n        else:\n            logging.info(f\"Created cf without options [{cf_name}]\")\n            self.cfs_info[cf_name] = \\\n                CfMetadata(discovery_type=CfDiscoveryType.RECOVERED_NO_OPTIONS,\n                           name=cf_name,\n                           discovery_time=entry.get_time(),\n                           has_options=False,\n                           auto_generated=False,\n                           id=cf_id)\n        return True\n\n    def try_parse_as_create_cf(self, entry):\n        cf_match = re.search(regexes.CREATE_CF, entry.get_msg())\n        if not cf_match:\n            return False\n\n        assert len(cf_match.groups()) == 2\n\n        cf_name = cf_match.group('cf')\n        cf_id = int(cf_match.group('cf_id'))\n\n        if self._does_cf_exist(cf_name):\n            self._update_cf_id(cf_name, cf_id, entry, raise_exception=True)\n        else:\n            logging.info(f\"Created cf without options [{cf_name}]\")\n            self.cfs_info[cf_name] = \\\n                CfMetadata(discovery_type=CfDiscoveryType.CREATE_NO_OPTIONS,\n                           name=cf_name,\n                           discovery_time=entry.get_time(),\n                           has_options=False,\n                           auto_generated=False,\n                           id=cf_id)\n\n        return True\n\n    def parsing_complete(self):\n        self.cfs_info = utils.sort_dict_on_values(self.cfs_info)\n\n    def set_cf_id(self, cf_name, cf_id, line, line_idx):\n        self._validate_cf_exists(cf_name, line, line_idx)\n\n        cf_id = int(cf_id)\n        self._update_cf_id(cf_name, cf_id, line, line_idx)\n\n        self.cfs_info[cf_name].id = int(cf_id)\n\n    def get_cf_id(self, cf_name):\n        cf_id = None\n        if cf_name in self.cfs_info:\n            cf_id = self.cfs_info[cf_name].id\n        return cf_id\n\n    def get_cf_info_by_name(self, cf_name):\n        cf_info = None\n        if cf_name in self.cfs_info:\n            cf_info = self.cfs_info[cf_name]\n        return cf_info\n\n    def get_cf_info_by_id(self, cf_id):\n        cf_id = int(cf_id)\n        for cf_info in self.cfs_info.values():\n            if cf_info.id == cf_id:\n                return cf_info\n\n        return None\n\n    def get_non_auto_generated_cfs_names(self):\n        return [cf_name for cf_name in self.cfs_info.keys() if\n                not self.cfs_info[cf_name].auto_generated]\n\n    def get_auto_generated_cf_names(self):\n        return [cf_name for cf_name in self.cfs_info.keys() if\n                self.cfs_info[cf_name].auto_generated]\n\n    def get_cfs_names_that_have_options(self, include_auto_generated):\n        returned_cfs_names = []\n        for cf_name in self.cfs_info.keys():\n            if self.cfs_info[cf_name].has_options:\n                if include_auto_generated or \\\n                        not self.cfs_info[cf_name].auto_generated:\n                    returned_cfs_names.append(cf_name)\n        return returned_cfs_names\n\n    def get_all_cfs_names(self):\n        return list(self.cfs_info.keys())\n\n    def get_num_cfs(self):\n        return len(self.cfs_info) if self.cfs_info else 0\n\n    def are_there_auto_generated_cf_names(self):\n        return len(self.get_auto_generated_cf_names()) > 0\n\n    def get_num_cfs_when_certain(self):\n        # Once a log has auto-generated cf names we have no certain way of\n        # knowing how many cf-s there are\n        if self.are_there_auto_generated_cf_names():\n            return None\n        else:\n            return self.get_num_cfs()\n\n    def was_cf_dropped(self, cf_name):\n        cf_info = self.get_cf_info_by_name(cf_name)\n        if not cf_info:\n            return False\n        return cf_info.drop_time is not None\n\n    def get_cf_drop_time(self, cf_name):\n        drop_time = None\n        cf_info = self.get_cf_info_by_name(cf_name)\n        if cf_info:\n            drop_time = cf_info.drop_time\n        return drop_time\n\n    def _validate_cf_exists(self, cf_name, entry):\n        if cf_name not in self.cfs_info:\n            raise utils.ParsingError(\n                f\"cf ({cf_name}) doesn't exist.\",\n                self.get_error_context(entry))\n\n    def _does_cf_exist(self, cf_name):\n        return cf_name in self.cfs_info\n\n    def _validate_cf_doesnt_exist(self, cf_name, entry):\n        if self._does_cf_exist(cf_name):\n            context = self.get_error_context(entry)\n            logging.error(f\"cf ({cf_name}) already exists.{context}\")\n            return False\n        return True\n\n    def _update_cf_id(self, cf_name, cf_id, entry, raise_exception):\n        curr_cf_id = self.cfs_info[cf_name].id\n        if curr_cf_id and curr_cf_id != int(cf_id):\n            context = self.get_error_context(entry)\n            if raise_exception:\n                raise utils.ParsingError(\n                    f\"id ({cf_id}) of cf ({cf_name}) already exists.\", context)\n            else:\n                logging.error(\n                    f\"id ({cf_id}) of cf ({cf_name}) already exists.{context}\")\n                return False\n\n        self.cfs_info[cf_name].id = cf_id\n        return True\n\n    def _validate_cf_wasnt_dropped(self, cf_info, entry, raise_exception):\n        if cf_info.drop_time:\n            context = self.get_error_context(entry)\n            if raise_exception:\n                raise utils.ParsingError(\n                    f\"CF ({cf_info.name}) already dropped at \"\n                    f\"({cf_info.drop_time}).\", context)\n            else:\n                logging.error(f\"CF ({cf_info.name}) already dropped at \"\n                              f\"({cf_info.drop_time}).{context}\")\n                return False\n        return True\n\n    def get_error_context(self, entry):\n        if entry is None:\n            return None\n        return utils.get_error_context_from_entry(entry, self.log_file_path)", ""]}
{"filename": "db_options.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport copy\nimport re\nfrom enum import Enum, auto\n", "from enum import Enum, auto\n\nimport regexes\nimport utils\n\nDB_WIDE_CF_NAME = utils.NO_CF\n\nSANITIZED_NO_VALUE = \"Missing\"\nRAW_NULL_PTR = \"Uninitialised\"\nSANITIZED_NULL_PTR = f\"Pointer ({RAW_NULL_PTR})\"", "RAW_NULL_PTR = \"Uninitialised\"\nSANITIZED_NULL_PTR = f\"Pointer ({RAW_NULL_PTR})\"\nSANITIZED_FALSE = str(False)\nSANITIZED_TRUE = str(False)\n\n\nclass SectionType(str, Enum):\n    VERSION = \"Version\"\n    DB_WIDE = \"DBOptions\"\n    CF = \"CFOptions\"\n    TABLE_OPTIONS = \"TableOptions.BlockBasedTable\"\n\n    def __str__(self):\n        return str(self.value)\n\n    @staticmethod\n    def extract_section_type(full_option_name):\n        # Try to match a section type by combining dot-delimited words\n        # in the full name. Longer combinations are tested before shorter\n        # ones (\"TableOptions.BlockBasedTable\" will be tested before\n        # \"CFOptions\")\n        name_parts = full_option_name.split('.')\n        for i in range(len(name_parts)):\n            try:\n                potential_section_type = '.'.join(name_parts[:-i])\n                return SectionType(potential_section_type)\n            except Exception: # noqa\n                continue\n\n        raise utils.ParsingError(\n            f\"Invalid full option name ({full_option_name}\")", "\n\ndef validate_section(section_type, expected_type=None):\n    SectionType.extract_section_type(f\"{section_type}.dummy\")\n    if expected_type is not None:\n        if section_type != expected_type:\n            raise utils.ParsingError(\n                f\"Not DB Wide section name ({section_type}\")\n\n\ndef parse_full_option_name(full_option_name):\n    section_type = SectionType.extract_section_type(full_option_name)\n    option_name = full_option_name.split('.')[-1]\n    return section_type, option_name", "\n\ndef parse_full_option_name(full_option_name):\n    section_type = SectionType.extract_section_type(full_option_name)\n    option_name = full_option_name.split('.')[-1]\n    return section_type, option_name\n\n\ndef extract_option_name(full_option_name):\n    section_type, option_name = parse_full_option_name(full_option_name)\n    return option_name", "def extract_option_name(full_option_name):\n    section_type, option_name = parse_full_option_name(full_option_name)\n    return option_name\n\n\ndef get_full_option_name(section_type, option_name):\n    validate_section(section_type)\n    return f\"{section_type}.{option_name}\"\n\n\ndef get_db_wide_full_option_name(option_name):\n    return get_full_option_name(SectionType.DB_WIDE, option_name)", "\n\ndef get_db_wide_full_option_name(option_name):\n    return get_full_option_name(SectionType.DB_WIDE, option_name)\n\n\ndef extract_db_wide_option_name(full_option_name):\n    section_type, option_name = parse_full_option_name(full_option_name)\n    validate_section(section_type, SectionType.DB_WIDE)\n    return option_name", "\n\ndef get_cf_full_option_name(option_name):\n    return get_full_option_name(SectionType.CF, option_name)\n\n\ndef extract_cf_option_name(full_option_name):\n    section_type, option_name = parse_full_option_name(full_option_name)\n    validate_section(section_type, SectionType.CF)\n    return option_name", "\n\ndef get_cf_table_full_option_name(option_name):\n    return get_full_option_name(SectionType.TABLE_OPTIONS, option_name)\n\n\ndef extract_cf_table_option_name(full_option_name):\n    section_type, option_name = parse_full_option_name(full_option_name)\n    validate_section(section_type, SectionType.TABLE_OPTIONS)\n    return option_name", "\n\ndef is_sanitized_pointer_value(value):\n    return re.findall(regexes.SANITIZED_POINTER, value.strip())\n\n\ndef sanitized_to_raw_ptr_value(sanitized_value):\n    if sanitized_value.strip() == SANITIZED_NULL_PTR:\n        return RAW_NULL_PTR\n    assert is_sanitized_pointer_value(sanitized_value)\n\n    match = re.fullmatch(regexes.SANITIZED_POINTER, sanitized_value.strip())\n    assert match\n    return match.group('ptr')", "\n\nclass SanitizedValueType(Enum):\n    NO_VALUE = auto()\n    BOOL = auto()\n    NULL_PTR = auto()\n    POINTER = auto()\n    OTHER = auto()\n\n    @staticmethod\n    def get_type_from_str(value):\n        if value == SANITIZED_NO_VALUE:\n            return SanitizedValueType.NO_VALUE\n        elif value == SANITIZED_NULL_PTR:\n            return SanitizedValueType.NULL_PTR\n        elif value == SANITIZED_FALSE or value == SANITIZED_TRUE:\n            return SanitizedValueType.BOOL\n        elif is_sanitized_pointer_value(value):\n            return SanitizedValueType.POINTER\n        else:\n            return SanitizedValueType.OTHER", "\n\ndef check_and_sanitize_if_null_ptr(value):\n    if isinstance(value, str):\n        temp_value = value.lower()\n        if temp_value == \"none\" or \\\n                temp_value == \"(nil)\" or \\\n                temp_value == \"nil\" or \\\n                temp_value == \"nullptr\" or \\\n                temp_value == \"null\" or \\\n                temp_value == \"0x0\":\n            return True, SANITIZED_NULL_PTR\n\n    return False, value", "\n\ndef check_and_sanitize_if_bool_value(value, include_int):\n    if isinstance(value, bool):\n        return True, str(value)\n    elif isinstance(value, str):\n        temp_value = value.lower()\n        if temp_value == \"false\":\n            return True, str(False)\n        elif temp_value == \"true\":\n            return True, str(True)\n        elif include_int and temp_value == '0':\n            return True, str(False)\n        elif include_int and temp_value == '1':\n            return True, str(True)\n        else:\n            return False, value\n    elif include_int and isinstance(value, int):\n        is_bool = value == 0 or value == 1\n        if is_bool:\n            return True, str(bool(value))\n\n    return False, value", "\n\ndef check_and_sanitize_if_pointer_value(value):\n    is_null, sanitized_value = check_and_sanitize_if_null_ptr(value)\n    if is_null:\n        return False, value\n    if not isinstance(value, str):\n        return False, value\n\n    pointer_match = re.findall(regexes.POINTER, value.strip())\n    if not pointer_match:\n        return False, value\n    assert len(pointer_match) == 1\n\n    return True, f\"Pointer ({pointer_match[0]})\"", "\n\ndef get_sanitized_value_with_type(value):\n    if value is None:\n        return SANITIZED_NO_VALUE, SanitizedValueType.NO_VALUE\n\n    is_bool_value, sanitized_value =\\\n        check_and_sanitize_if_bool_value(value, include_int=False)\n    if is_bool_value:\n        return sanitized_value, SanitizedValueType.BOOL\n\n    is_null_ptr, sanitized_value = check_and_sanitize_if_null_ptr(value)\n    if is_null_ptr:\n        return sanitized_value, SanitizedValueType.NULL_PTR\n\n    is_pointer_value, sanitized_value =\\\n        check_and_sanitize_if_pointer_value(value)\n    if is_pointer_value:\n        return sanitized_value, SanitizedValueType.POINTER\n\n    return value, SanitizedValueType.OTHER", "\n\ndef get_sanitized_value(value):\n    sanitized_value, _ = get_sanitized_value_with_type(value)\n    return sanitized_value\n\n\ndef get_sanitized_options_diff(base_value, new_value, expect_diff):\n    sanitized_base_value, base_type = \\\n        get_sanitized_value_with_type(base_value)\n    sanitized_new_value, new_type = \\\n        get_sanitized_value_with_type(new_value)\n\n    # We expect a diff => It's a bug if both are no-value (=> equal)\n    if expect_diff:\n        assert base_type != SanitizedValueType.NO_VALUE or \\\n           new_type != SanitizedValueType.NO_VALUE\n        # Same - 2 pointers are considered equal => bug\n        assert base_type != SanitizedValueType.POINTER or \\\n               new_type != SanitizedValueType.POINTER\n\n    if base_type == SanitizedValueType.BOOL or \\\n            new_type == SanitizedValueType.BOOL:\n        _, sanitized_base_value = check_and_sanitize_if_bool_value(\n            sanitized_base_value, include_int=True)\n        _, sanitized_new_value = check_and_sanitize_if_bool_value(\n            sanitized_new_value, include_int=True)\n\n    if base_type == new_type and base_type == SanitizedValueType.POINTER:\n        are_diff = False\n    else:\n        are_diff = sanitized_base_value != sanitized_new_value\n\n    if expect_diff:\n        assert are_diff\n        return sanitized_base_value, sanitized_new_value\n    else:\n        return are_diff, sanitized_base_value, sanitized_new_value", "\n\ndef are_non_sanitized_values_different(base_value, new_value):\n    are_diff, _, _ = get_sanitized_options_diff(base_value, new_value,\n                                                expect_diff=False)\n\n    return are_diff\n\n\nclass FullNamesOptionsDict:\n    # {Full-option-name: {<cf>: <option-value>}}\n    def __init__(self, options_dict=None):\n        self.options_dict = {}\n        if options_dict is not None:\n            self.options_dict = copy.deepcopy(options_dict)\n\n    def __eq__(self, other):\n        if isinstance(other, FullNamesOptionsDict):\n            return self.options_dict == other.options_dict\n        elif isinstance(other, dict):\n            return self.options_dict == other\n        else:\n            assert False, \"Comparing to an invalid type \" \\\n                          f\"({type(other)})\"\n\n    def init_from_full_names_options_no_cf_dict(\n            self, cf_name, options_dict_no_cf):\n        # Input: {Full-option-name: <option-value>}\n        # Converts to the format of this class using the specified cf_name\n        assert self.options_dict == {}\n\n        for full_option_name, option_value in options_dict_no_cf.items():\n            self.options_dict[full_option_name] = {cf_name: option_value}\n\n    def set_option(self, section_type, cf_name, option_name, option_value):\n        if section_type == SectionType.DB_WIDE:\n            assert cf_name == DB_WIDE_CF_NAME\n\n        full_option_name = get_full_option_name(section_type, option_name)\n        if full_option_name not in self.options_dict:\n            self.options_dict[full_option_name] = {}\n\n        self.options_dict[full_option_name].update({\n            cf_name: get_sanitized_value(option_value)})\n\n    def get_option_by_full_name(self, full_option_name):\n        if full_option_name not in self.options_dict:\n            return None\n        return self.options_dict[full_option_name]\n\n    def get_option_by_parts(self, section_type, option_name, cf_name=None):\n        value = self.get_option_by_full_name(get_full_option_name(\n            section_type, option_name))\n        if value is None or cf_name is None:\n            return value\n        if cf_name not in value:\n            return None\n        return value[cf_name]\n\n    def set_db_wide_option(self, option_name, option_value):\n        self.set_option(SectionType.DB_WIDE, DB_WIDE_CF_NAME, option_name,\n                        option_value)\n\n    def get_db_wide_option(self, option_name):\n        return self.get_option_by_parts(SectionType.DB_WIDE, option_name,\n                                        DB_WIDE_CF_NAME)\n\n    def set_cf_option(self, cf_name, option_name, option_value):\n        self.set_option(SectionType.CF, cf_name, option_name,\n                        option_value)\n\n    def get_cf_option(self, option_name, cf_name=None):\n        return self.get_option_by_parts(SectionType.CF, option_name, cf_name)\n\n    def set_cf_table_option(self, cf_name, option_name, option_value):\n        self.set_option(SectionType.TABLE_OPTIONS, cf_name, option_name,\n                        option_value)\n\n    def get_cf_table_option(self, option_name, cf_name=None):\n        return self.get_option_by_parts(SectionType.TABLE_OPTIONS,\n                                        option_name, cf_name)\n\n    def get_options_dict(self):\n        return self.options_dict", "\nclass FullNamesOptionsDict:\n    # {Full-option-name: {<cf>: <option-value>}}\n    def __init__(self, options_dict=None):\n        self.options_dict = {}\n        if options_dict is not None:\n            self.options_dict = copy.deepcopy(options_dict)\n\n    def __eq__(self, other):\n        if isinstance(other, FullNamesOptionsDict):\n            return self.options_dict == other.options_dict\n        elif isinstance(other, dict):\n            return self.options_dict == other\n        else:\n            assert False, \"Comparing to an invalid type \" \\\n                          f\"({type(other)})\"\n\n    def init_from_full_names_options_no_cf_dict(\n            self, cf_name, options_dict_no_cf):\n        # Input: {Full-option-name: <option-value>}\n        # Converts to the format of this class using the specified cf_name\n        assert self.options_dict == {}\n\n        for full_option_name, option_value in options_dict_no_cf.items():\n            self.options_dict[full_option_name] = {cf_name: option_value}\n\n    def set_option(self, section_type, cf_name, option_name, option_value):\n        if section_type == SectionType.DB_WIDE:\n            assert cf_name == DB_WIDE_CF_NAME\n\n        full_option_name = get_full_option_name(section_type, option_name)\n        if full_option_name not in self.options_dict:\n            self.options_dict[full_option_name] = {}\n\n        self.options_dict[full_option_name].update({\n            cf_name: get_sanitized_value(option_value)})\n\n    def get_option_by_full_name(self, full_option_name):\n        if full_option_name not in self.options_dict:\n            return None\n        return self.options_dict[full_option_name]\n\n    def get_option_by_parts(self, section_type, option_name, cf_name=None):\n        value = self.get_option_by_full_name(get_full_option_name(\n            section_type, option_name))\n        if value is None or cf_name is None:\n            return value\n        if cf_name not in value:\n            return None\n        return value[cf_name]\n\n    def set_db_wide_option(self, option_name, option_value):\n        self.set_option(SectionType.DB_WIDE, DB_WIDE_CF_NAME, option_name,\n                        option_value)\n\n    def get_db_wide_option(self, option_name):\n        return self.get_option_by_parts(SectionType.DB_WIDE, option_name,\n                                        DB_WIDE_CF_NAME)\n\n    def set_cf_option(self, cf_name, option_name, option_value):\n        self.set_option(SectionType.CF, cf_name, option_name,\n                        option_value)\n\n    def get_cf_option(self, option_name, cf_name=None):\n        return self.get_option_by_parts(SectionType.CF, option_name, cf_name)\n\n    def set_cf_table_option(self, cf_name, option_name, option_value):\n        self.set_option(SectionType.TABLE_OPTIONS, cf_name, option_name,\n                        option_value)\n\n    def get_cf_table_option(self, option_name, cf_name=None):\n        return self.get_option_by_parts(SectionType.TABLE_OPTIONS,\n                                        option_name, cf_name)\n\n    def get_options_dict(self):\n        return self.options_dict", "\n\nclass OptionsDiff:\n    def __init__(self, baseline_options, new_options):\n        self.baseline_options = baseline_options\n        self.new_options = new_options\n        self.diff_dict = {}\n\n    def __eq__(self, other):\n        if isinstance(other, OptionsDiff):\n            return self.diff_dict == other.diff_dict\n        elif isinstance(other, dict):\n            return self.diff_dict == other\n        else:\n            assert False, \"Comparing to an invalid type \" \\\n                          f\"({type(other)})\"\n\n    def diff_in_base(self, cf_name, full_option_name):\n        self.add_option_if_necessary(full_option_name)\n        self.diff_dict[full_option_name][cf_name] =\\\n            get_sanitized_options_diff(\n                self.baseline_options[full_option_name][cf_name], None,\n                expect_diff=True)\n\n    def diff_in_new(self, cf_name, full_option_name):\n        self.add_option_if_necessary(full_option_name)\n        self.diff_dict[full_option_name][cf_name] =\\\n            get_sanitized_options_diff(\n                None, self.new_options[full_option_name][cf_name],\n                expect_diff=True)\n\n    def diff_between(self, cf_name, full_option_name):\n        self.add_option_if_necessary(full_option_name)\n        self.diff_dict[full_option_name][cf_name] =\\\n            get_sanitized_options_diff(\n                self.baseline_options[full_option_name][cf_name],\n                self.new_options[full_option_name][cf_name],\n                expect_diff=True)\n\n    def add_option_if_necessary(self, full_option_name):\n        if full_option_name not in self.diff_dict:\n            self.diff_dict[full_option_name] = {}\n\n    def is_empty_diff(self):\n        return self.diff_dict == {}\n\n    def get_diff_dict(self):\n        return self.diff_dict", "\n\nclass CfsOptionsDiff:\n    CF_NAMES_KEY = \"cf names\"\n\n    def __init__(self, baseline_options, baseline_cf_name,\n                 new_options, new_cf_name, diff_dict=None):\n        self.baseline_options = baseline_options\n        self.baseline_cf_name = baseline_cf_name\n        self.new_options = new_options\n        self.new_cf_name = new_cf_name\n        self.diff_dict = {}\n        if diff_dict is not None:\n            self.diff_dict = diff_dict\n\n    def __eq__(self, other):\n        new_dict = None\n        if isinstance(other, CfsOptionsDiff):\n            new_dict = other.get_diff_dict()\n        elif isinstance(other, dict):\n            new_dict = other\n        else:\n            assert False, \"Comparing to an invalid type \" \\\n                          f\"({type(other)})\"\n        assert CfsOptionsDiff.CF_NAMES_KEY in new_dict, \\\n            f\"{CfsOptionsDiff.CF_NAMES_KEY} key missing in other\"\n\n        baseline_dict = self.get_diff_dict()\n        assert baseline_dict[CfsOptionsDiff.CF_NAMES_KEY] == \\\n               new_dict[CfsOptionsDiff.CF_NAMES_KEY], (\n            \"Comparing diff entities for mismatching cf-s: \"\n            f\"baseline:{baseline_dict[CfsOptionsDiff.CF_NAMES_KEY]}, \"\n            f\"new:{new_dict[CfsOptionsDiff.CF_NAMES_KEY]}\")\n\n        return self.diff_dict == new_dict\n\n    def diff_in_base(self, full_option_name):\n        self.add_option_if_necessary(full_option_name)\n        self.diff_dict[full_option_name] = \\\n            get_sanitized_options_diff(\n                self.baseline_options[full_option_name][self.baseline_cf_name],\n                None, expect_diff=True)\n\n    def diff_in_new(self, full_option_name):\n        self.add_option_if_necessary(full_option_name)\n        self.diff_dict[full_option_name] = \\\n            get_sanitized_options_diff(\n                None,\n                self.new_options[full_option_name][self.new_cf_name],\n                expect_diff=True)\n\n    def diff_between(self, full_option_name):\n        self.add_option_if_necessary(full_option_name)\n        self.diff_dict[full_option_name] = \\\n            get_sanitized_options_diff(\n                self.baseline_options[full_option_name][self.baseline_cf_name],\n                self.new_options[full_option_name][self.new_cf_name],\n                expect_diff=True)\n\n    def add_option_if_necessary(self, full_option_name):\n        if full_option_name not in self.diff_dict:\n            self.diff_dict[full_option_name] = {}\n\n    def is_empty_diff(self):\n        return self.diff_dict == {}\n\n    def get_diff_dict(self, exclude_compared_cfs_names=False):\n        if self.is_empty_diff():\n            return {}\n\n        result_dict = self.diff_dict\n        if not exclude_compared_cfs_names:\n            result_dict.update({CfsOptionsDiff.CF_NAMES_KEY:\n                                {\"Base\": self.baseline_cf_name,\n                                 \"New\": self.new_cf_name}})\n        return result_dict", "\n\nclass DatabaseOptions:\n    def __init__(self, section_based_options_dict=None):\n        # The options are stored in the following data structure:\n        # {DBOptions: {DB_WIDE: {<option-name>: <option-value>, ...}},\n        # {CFOptions: {<cf-name>: {<option-name>: <option-value>, ...}},\n        # {TableOptions.BlockBasedTable:\n        #   {<cf-name>: {<option-name>: <option-value>, ...}}}\n        # If section_based_options_dict is specified, it must be in the\n        # internal format (e.g., used when loading options from an options\n        # file)\n        self.options_dict = {}\n        if section_based_options_dict:\n            # TODO - Verify the format of the options_dict\n            self.options_dict = section_based_options_dict\n\n        self.cfs_names = None\n        self.setup_column_families()\n\n    def __str__(self):\n        return \"DatabaseOptions\"\n\n    def set_db_wide_options(self, options_dict):\n        # The input dictionary is expected to be like this:\n        # {<option name>: <option_value>}\n        if self.are_db_wide_options_set():\n            raise utils.ParsingAssertion(\n                \"DB Wide Options Already Set\")\n\n        self.options_dict[SectionType.DB_WIDE] = {\n            DB_WIDE_CF_NAME: options_dict}\n        self.setup_column_families()\n\n    def create_section_if_necessary(self, section_type):\n        if section_type not in self.options_dict:\n            self.options_dict[section_type] = dict()\n\n    def create_cf_in_section_if_necessary(self, section_type, cf_name):\n        if cf_name not in self.options_dict[section_type]:\n            self.cfs_names.append(cf_name)\n            self.options_dict[section_type][cf_name] = dict()\n\n    def create_section_and_cf_in_section_if_necessary(self, section_type,\n                                                      cf_name):\n        self.create_section_if_necessary(section_type)\n        self.create_cf_in_section_if_necessary(section_type, cf_name)\n\n    def validate_no_section(self, section_type):\n        if section_type in self.options_dict:\n            raise utils.ParsingAssertion(\n                f\"{section_type} Already Set\")\n\n    def validate_no_cf_name_in_section(self, section_type, cf_name):\n        if cf_name in self.options_dict[section_type]:\n            raise utils.ParsingAssertion(\n                f\"{section_type} Already Set for this CF ({cf_name})\")\n\n    def set_cf_options(self, cf_name, cf_non_table_options, table_options):\n        # Both input dictionaries are expected to be like this:\n        # {<option name>: <option_value>}\n        self.create_section_if_necessary(SectionType.CF)\n        self.validate_no_cf_name_in_section(SectionType.CF, cf_name)\n\n        self.create_section_if_necessary(SectionType.TABLE_OPTIONS)\n        self.validate_no_cf_name_in_section(SectionType.TABLE_OPTIONS,\n                                            cf_name)\n\n        self.options_dict[SectionType.CF][cf_name] = cf_non_table_options\n        self.options_dict[SectionType.TABLE_OPTIONS][cf_name] = table_options\n\n        self.setup_column_families()\n\n    def setup_column_families(self):\n        self.cfs_names = []\n\n        if SectionType.CF in self.options_dict:\n            self.cfs_names =\\\n                list(self.options_dict[SectionType.CF].keys())\n\n        if SectionType.DB_WIDE in self.options_dict:\n            self.cfs_names.extend(\n                list(self.options_dict[SectionType.DB_WIDE].keys()))\n\n    def are_db_wide_options_set(self):\n        return SectionType.DB_WIDE in self.options_dict\n\n    def get_cfs_names(self):\n        cf_names_no_db_wide = copy.deepcopy(self.cfs_names)\n        if DB_WIDE_CF_NAME in cf_names_no_db_wide:\n            cf_names_no_db_wide.remove(DB_WIDE_CF_NAME)\n        return cf_names_no_db_wide\n\n    def set_db_wide_option(self, option_name, option_value,\n                           allow_new_option=False):\n        if not allow_new_option:\n            assert self.get_db_wide_option(option_name) is not None,\\\n                \"Trying to update a non-existent DB Wide Option.\" \\\n                f\"{option_name} = {option_value}\"\n\n        self.create_section_and_cf_in_section_if_necessary(\n            SectionType.DB_WIDE, DB_WIDE_CF_NAME)\n        self.options_dict[SectionType.DB_WIDE][\n            DB_WIDE_CF_NAME][option_name] = option_value\n\n    def get_all_options(self):\n        # Returns all the options that as a FullNamesOptionsDict\n        full_options_names = []\n        for section_type in self.options_dict:\n            for cf_name in self.options_dict[section_type]:\n                for option_name in self.options_dict[section_type][cf_name]:\n                    full_option_name = get_full_option_name(section_type,\n                                                            option_name)\n                    full_options_names.append(full_option_name)\n\n        return self.get_options(full_options_names)\n\n    def get_db_wide_options(self):\n        # Returns the DB-Wide options as a FullNamesOptionsDict\n        full_options_names = []\n        section_type = SectionType.DB_WIDE\n        if section_type in self.options_dict:\n            sec_dict = self.options_dict[section_type]\n            for option_name in sec_dict[DB_WIDE_CF_NAME]:\n                full_option_name = get_full_option_name(section_type,\n                                                        option_name)\n                full_options_names.append(full_option_name)\n\n        return self.get_options(full_options_names, DB_WIDE_CF_NAME)\n\n    def get_db_wide_options_for_display(self):\n        options_for_display = {}\n        db_wide_options = self.get_db_wide_options().get_options_dict()\n        for full_option_name, option_value_with_cf in db_wide_options.items():\n            option_name = extract_db_wide_option_name(full_option_name)\n            option_value = option_value_with_cf[DB_WIDE_CF_NAME]\n            options_for_display[option_name] = option_value\n        return options_for_display\n\n    def get_db_wide_option(self, option_name):\n        # Returns the raw value of a single DB-Wide option\n        full_option_name = get_full_option_name(SectionType.DB_WIDE,\n                                                option_name)\n        option_value_dict = \\\n            self.get_options([full_option_name]).get_options_dict()\n        if not option_value_dict:\n            return None\n        else:\n            return option_value_dict[full_option_name][DB_WIDE_CF_NAME]\n\n    def get_cf_options(self, cf_name):\n        # Returns the options for cf-name as a FullNamesOptionsDict\n        full_options_names = []\n        for section_type in self.options_dict:\n            if cf_name in self.options_dict[section_type]:\n                for option_name in self.options_dict[section_type][cf_name]:\n                    full_option_name = \\\n                        get_full_option_name(section_type, option_name)\n                    full_options_names.append(full_option_name)\n\n        return self.get_options(full_options_names, cf_name)\n\n    @staticmethod\n    def get_unified_cfs_options(cfs_options):\n        # dictionaries that will contain the results\n        common_cfs_options = {}\n        unique_cfs_options = {}\n\n        if not cfs_options:\n            return common_cfs_options, unique_cfs_options\n\n        cfs_names = [cf_name for cf_name in cfs_options.keys()]\n\n        # First assume no common options, later remove common\n        # Remove cf-name from values to allow comparison\n        for cf_name in cfs_names:\n            assert isinstance(cfs_options[cf_name], FullNamesOptionsDict)\n            unique_cfs_options[cf_name] = {}\n            for option_name, option_value_with_cf_name in \\\n                    cfs_options[cf_name].options_dict.items():\n                unique_cfs_options[cf_name][option_name] = \\\n                    option_value_with_cf_name[cf_name]\n\n        # Check all options\n        first_cf_name = cfs_names[0]\n        for option_name in list(unique_cfs_options[first_cf_name].keys()):\n            try:\n                options_values =\\\n                    [unique_cfs_options[cf_name][option_name] for\n                     cf_name in unique_cfs_options.keys()]\n                different_options_values = set(options_values)\n            except KeyError:\n                # At least one cf doesn't have this option => not common\n                continue\n\n            if len(different_options_values) != 1:\n                # At least one cf has a different value for this options =>\n                # not common\n                continue\n\n            # The option is common to all cf-s => place in common\n            # dict, and remove from all unique dicts\n            common_cfs_options[option_name] = options_values[0]\n            for cf_name in cfs_names:\n                del(unique_cfs_options[cf_name][option_name])\n\n        return common_cfs_options, unique_cfs_options\n\n    @staticmethod\n    def prepare_flat_full_names_cf_options_for_display(\n            cf_options, option_value_prepare_func):\n        if option_value_prepare_func is None:\n            def option_value_prepare_func(value):\n                return value\n\n        options_for_display = {}\n        table_options_for_display = {}\n\n        for full_option_name, option_value in cf_options.items():\n            section_type, option_name = \\\n                parse_full_option_name(full_option_name)\n\n            if section_type == SectionType.CF:\n                options_for_display[option_name] = \\\n                    option_value_prepare_func(option_value)\n\n            else:\n                assert section_type == SectionType.TABLE_OPTIONS\n                table_options_for_display[option_name] = \\\n                    option_value_prepare_func(option_value)\n\n        return options_for_display, table_options_for_display\n\n    def get_cf_options_for_display(self, cf_name):\n        options_for_display = {}\n        table_options_for_display = {}\n        cf_options = self.get_cf_options(cf_name)\n        for full_option_name, option_value_with_cf in \\\n                cf_options.get_options_dict().items():\n            option_value = option_value_with_cf[cf_name]\n            section_type, option_name =\\\n                parse_full_option_name(full_option_name)\n\n            if section_type == SectionType.CF:\n                options_for_display[option_name] = option_value\n            else:\n                assert section_type == SectionType.TABLE_OPTIONS\n                table_options_for_display[option_name] = option_value\n\n        return options_for_display, table_options_for_display\n\n    def get_cf_option(self, cf_name, option_name):\n        # Returns the raw value of a single CF option\n        full_option_name = get_full_option_name(SectionType.CF, option_name)\n        option_value_dict = self.get_options([full_option_name], cf_name). \\\n            get_options_dict()\n        if not option_value_dict:\n            return None\n        else:\n            return option_value_dict[full_option_name][cf_name]\n\n    def set_cf_option(self, cf_name, option_name, option_value,\n                      allow_new_option=False):\n        if not allow_new_option:\n            assert self.get_cf_option(cf_name, option_name) is not None,\\\n                \"Trying to update a non-existent CF Option.\" \\\n                f\"cf:{cf_name} - {option_name} = {option_value}\"\n\n        self.create_section_and_cf_in_section_if_necessary(SectionType.CF,\n                                                           cf_name)\n        self.options_dict[SectionType.CF][cf_name][option_name] = \\\n            option_value\n\n    def get_cf_table_option(self, cf_name, option_name):\n        # Returns the raw value of a single CF Table option\n        full_option_name = get_full_option_name(SectionType.TABLE_OPTIONS,\n                                                option_name)\n        option_value_dict = self.get_options([full_option_name], cf_name). \\\n            get_options_dict()\n        if not option_value_dict:\n            return None\n        else:\n            return option_value_dict[full_option_name][cf_name]\n\n    def get_cf_table_raw_ptr_str(self, cf_name, options_name):\n        sanitized_ptr = self.get_cf_table_option(cf_name, options_name)\n        if sanitized_ptr is None:\n            return None\n\n        return sanitized_to_raw_ptr_value(sanitized_ptr)\n\n    def set_cf_table_option(self, cf_name, option_name, option_value,\n                            allow_new_option=False):\n        if not allow_new_option:\n            assert self.get_cf_table_option(cf_name, option_name) is not None,\\\n                \"Trying to update a non-existent CF Table Option.\" \\\n                f\"cf:{cf_name} - {option_name} = {option_value}\"\n\n        self.create_section_and_cf_in_section_if_necessary(\n            SectionType.TABLE_OPTIONS, cf_name)\n        self.options_dict[SectionType.TABLE_OPTIONS][cf_name][option_name] = \\\n            option_value\n\n    def get_options(self, requested_full_options_names,\n                    requested_cf_name=None):\n        # The input is expected to be a set or a list of the format:\n        # [<full option name>, ...]\n        # Returns a FullNamesOptionsDict for the requested options\n        assert isinstance(requested_full_options_names, list) or isinstance(\n            requested_full_options_names, set),\\\n            f\"Illegal requested_full_options_names type \" \\\n            f\"({type(requested_full_options_names)}\"\n\n        options = FullNamesOptionsDict()\n\n        for full_option_name in requested_full_options_names:\n            section_type, option_name = \\\n                parse_full_option_name(full_option_name)\n            if section_type not in self.options_dict:\n                continue\n            if requested_cf_name is None:\n                cf_names = self.options_dict[section_type].keys()\n            else:\n                cf_names = [requested_cf_name]\n\n            for cf_name in cf_names:\n                if cf_name not in self.options_dict[section_type]:\n                    continue\n                if option_name in self.options_dict[section_type][cf_name]:\n                    option_value = \\\n                        self.options_dict[section_type][cf_name][\n                            option_name]\n\n                    options.set_option(section_type, cf_name, option_name,\n                                       option_value)\n        return options\n\n    @staticmethod\n    def get_options_diff(baseline, new):\n        # Receives 2 sets of options and returns the difference between them.\n        # Three cases exist:\n        # 1. An option exists in the old but not in the new\n        # 2. An option exists in the new but not in the old\n        # 3. The option exists in both but the values differ\n        # The inputs must be FullNamesOptionsDict instances. These may be\n        # obtained\n        # The resulting diff is an OptionsDiff with only actual\n        # differences\n        assert isinstance(baseline, FullNamesOptionsDict)\n        assert isinstance(new, FullNamesOptionsDict)\n\n        baseline_options = baseline.get_options_dict()\n        new_options = new.get_options_dict()\n\n        diff = OptionsDiff(baseline_options, new_options)\n\n        full_options_names_union =\\\n            set(baseline_options.keys()).union(set(new_options.keys()))\n        for full_option_name in full_options_names_union:\n            # if option in options_union, then it must be in one of the configs\n            if full_option_name not in baseline_options:\n                for cf_name in new_options[full_option_name]:\n                    diff.diff_in_new(cf_name, full_option_name)\n            elif full_option_name not in new_options:\n                for cf_name in baseline_options[full_option_name]:\n                    diff.diff_in_base(cf_name, full_option_name)\n            else:\n                for cf_name in baseline_options[full_option_name]:\n                    if cf_name in new_options[full_option_name]:\n                        are_different = \\\n                            are_non_sanitized_values_different(\n                                baseline_options[full_option_name][cf_name],\n                                new_options[full_option_name][cf_name])\n                        if are_different:\n                            diff.diff_between(cf_name, full_option_name)\n                    else:\n                        diff.diff_in_base(cf_name, full_option_name)\n\n                for cf_name in new_options[full_option_name]:\n                    if cf_name in baseline_options[full_option_name]:\n                        are_different = are_non_sanitized_values_different(\n                            baseline_options[full_option_name][cf_name],\n                            new_options[full_option_name][cf_name])\n                        if are_different:\n                            diff.diff_between(cf_name, full_option_name)\n                    else:\n                        diff.diff_in_new(cf_name, full_option_name)\n\n        return diff if not diff.is_empty_diff() else None\n\n    def get_options_diff_relative_to_me(self, new):\n        baseline = self.get_all_options()\n        return DatabaseOptions.get_options_diff(baseline, new)\n\n    @staticmethod\n    def get_cfs_options_diff(baseline, baseline_cf_name, new, new_cf_name):\n        assert isinstance(baseline, FullNamesOptionsDict)\n        assert isinstance(new, FullNamesOptionsDict)\n\n        # Same as get_options_diff, but for specific column families.\n        # This is needed to compare a parsed log file with a baseline version.\n        # The baseline version contains options only for the default column\n        # family. So, there is a need to compare the options for all column\n        # families in the parsed log with the same default column family in\n        # the base version\n        baseline_opts_dict = baseline.get_options_dict()\n        new_opts_dict = new.get_options_dict()\n\n        diff = CfsOptionsDiff(baseline_opts_dict, baseline_cf_name,\n                              new_opts_dict, new_cf_name)\n\n        # Unify the names of the options in both, remove the duplicates, but\n        # preserve the order of the options\n        baseline_keys_list = list(baseline_opts_dict.keys())\n        new_keys_list = list(new_opts_dict.keys())\n        options_union = \\\n            utils.unify_lists_preserve_order(baseline_keys_list, new_keys_list)\n\n        for full_option_name in options_union:\n            # if option in options_union, then it must be in one of the configs\n            if full_option_name not in baseline_opts_dict:\n                new_option_values = new_opts_dict[full_option_name]\n                if new_cf_name in new_option_values:\n                    diff.diff_in_new(full_option_name)\n            elif full_option_name not in new_opts_dict:\n                baseline_option_values = baseline_opts_dict[full_option_name]\n                if baseline_cf_name in baseline_option_values:\n                    diff.diff_in_base(full_option_name)\n            else:\n                baseline_option_values = baseline_opts_dict[full_option_name]\n                new_option_values = new_opts_dict[full_option_name]\n\n                if baseline_cf_name in baseline_option_values:\n                    if new_cf_name in new_option_values:\n                        are_different = are_non_sanitized_values_different(\n                            baseline_option_values[baseline_cf_name],\n                            new_option_values[new_cf_name])\n                        if are_different:\n                            diff.diff_between(full_option_name)\n                    else:\n                        diff.diff_in_base(full_option_name)\n                elif new_cf_name in new_option_values:\n                    diff.diff_in_new(full_option_name)\n\n        return diff if not diff.is_empty_diff() else None\n\n    @staticmethod\n    def get_db_wide_options_diff(opt_old, opt_new):\n        return DatabaseOptions.get_cfs_options_diff(\n            opt_old,\n            DB_WIDE_CF_NAME,\n            opt_new,\n            DB_WIDE_CF_NAME)\n\n    @staticmethod\n    def get_unified_cfs_diffs(cfs_diffs):\n        # Input [CfsOptionsDiff-1, CfsOptionsDiff-2...]\n\n        # dictionaries that will contain the results\n        common_cfs_diffs = {}\n        unique_cfs_diffs = {}\n\n        if not cfs_diffs:\n            return common_cfs_diffs, unique_cfs_diffs\n\n        # First assume no common options, later remove common\n        # unique_cfs_diffs = copy.deepcopy(cfs_diffs)\n        unique_cfs_diffs = copy.deepcopy(cfs_diffs)\n\n        # Check all options diffs\n        for option_name in list(unique_cfs_diffs[0].keys()):\n            try:\n                if option_name == CfsOptionsDiff.CF_NAMES_KEY:\n                    continue\n                option_diffs =\\\n                    [unique_cfs_diffs[cf_idx][option_name] for\n                     cf_idx in range(len(unique_cfs_diffs))]\n                different_options_diffs = set(option_diffs)\n            except KeyError:\n                # At least one cf doesn't have this option => not common\n                continue\n\n            if len(different_options_diffs) != 1:\n                # At least one cf has a different diff for this option =>\n                # not common\n                continue\n\n            # The option is common to all cf-s => place in common\n            # dict, and remove from all unique dicts\n            common_cfs_diffs[option_name] = option_diffs[0]\n            for cf_diff in unique_cfs_diffs:\n                del(cf_diff[option_name])\n\n        return common_cfs_diffs, unique_cfs_diffs", ""]}
{"filename": "__init__.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n"]}
{"filename": "counters.py", "chunked_list": ["import logging\nimport re\n\nimport regexes\nimport utils\n\nformat_err_msg = utils.format_err_msg\nParsingAssertion = utils.ParsingAssertion\nErrContext = utils.ErrorContext\nformat_line_num_from_entry = utils.format_line_num_from_entry", "ErrContext = utils.ErrorContext\nformat_line_num_from_entry = utils.format_line_num_from_entry\nformat_line_num_from_line_idx = utils.format_line_num_from_line_idx\nget_line_num_from_entry = utils.get_line_num_from_entry\n\n\nclass CountersMngr:\n    @staticmethod\n    def is_start_line(line):\n        return re.findall(regexes.STATS_COUNTERS_AND_HISTOGRAMS, line)\n\n    @staticmethod\n    def is_your_entry(entry):\n        entry_lines = entry.get_msg_lines()\n        return CountersMngr.is_start_line(entry_lines[0])\n\n    def try_adding_entries(self, log_entries, start_entry_idx):\n        entry_idx = start_entry_idx\n        entry = log_entries[entry_idx]\n\n        if not CountersMngr.is_your_entry(entry):\n            return False, entry_idx\n\n        try:\n            self.add_entry(entry)\n        except utils.ParsingError:\n            logging.error(f\"Error while parsing Counters entry, Skipping.\\n\"\n                          f\"entry:{entry}\")\n        entry_idx += 1\n\n        return True, entry_idx\n\n    def __init__(self):\n        # list of counters names in the order of their appearance\n        # in the log file (retaining this order assuming it is\n        # convenient for the user)\n        self.counters_names = []\n        self.counters = dict()\n        self.histogram_counters_names = []\n        self.histograms = dict()\n\n    def add_entry(self, entry):\n        time = entry.get_time()\n        lines = entry.get_msg_lines()\n        assert CountersMngr.is_start_line(lines[0])\n\n        logging.debug(f\"Parsing Counter and Histograms Entry (\"\n                      f\"{format_line_num_from_entry(entry)}\")\n\n        for i, line in enumerate(lines[1:]):\n            if self.try_parse_counter_line(time, line):\n                continue\n            if self.try_parse_histogram_line(time, line):\n                continue\n\n            # Skip badly formed lines\n            logging.error(format_err_msg(\n                \"Failed parsing Counters / Histogram line\"\n                f\"Entry. time:{time}\",\n                ErrContext(**{\n                    \"log_line_idx\": get_line_num_from_entry(entry, i + 1),\n                    \"log_line\": line})))\n\n    def try_parse_counter_line(self, time, line):\n        line_parts = re.findall(regexes.STATS_COUNTER, line)\n        if not line_parts:\n            return False\n        assert len(line_parts) == 1 and len(line_parts[0]) == 2\n\n        value = int(line_parts[0][1])\n        counter_name = line_parts[0][0]\n        if counter_name not in self.counters:\n            self.counters_names.append(counter_name)\n            self.counters[counter_name] = list()\n\n        entries = self.counters[counter_name]\n        if entries:\n            prev_entry = entries[-1]\n            prev_value = prev_entry[\"value\"]\n\n            if value < prev_value:\n                logging.error(format_err_msg(\n                    f\"count or sum DECREASED during interval - Ignoring Entry.\"\n                    f\"prev_value:{prev_value}, count:{value}\"\n                    f\" (counter:{counter_name}), \"\n                    f\"prev_time:{prev_entry['time']}, time:{time}\",\n                    ErrContext(**{\"log_line\": line})))\n                return True\n\n        self.counters[counter_name].append({\n            \"time\": time,\n            \"value\": value})\n\n        return True\n\n    def try_parse_histogram_line(self, time, line):\n        match = re.fullmatch(regexes.STATS_HISTOGRAM, line)\n        if not match:\n            return False\n        assert len(match.groups()) == 7\n\n        counter_name = match.group('name')\n        count = int(match.group('count'))\n        total = int(match.group('sum'))\n        if total > 0 and count == 0:\n            logging.error(format_err_msg(\n                f\"0 Count but total > 0 in a histogram (counter:\"\n                f\"{counter_name}), time:{time}\",\n                ErrContext(**{\"log_line\": line})))\n\n        if counter_name not in self.histograms:\n            self.histograms[counter_name] = list()\n            self.histogram_counters_names.append(counter_name)\n\n        # There are cases where the count is > 0 but the\n        # total is 0 (e.g., 'rocksdb.prefetched.bytes.discarded')\n        if total > 0:\n            average = float(f\"{(total / count):.2f}\")\n        else:\n            average = float(f\"{0.0:.2f}\")\n\n        entries = self.histograms[counter_name]\n\n        prev_count = 0\n        prev_total = 0\n        if entries:\n            prev_entry = entries[-1]\n            prev_count = prev_entry[\"values\"][\"Count\"]\n            prev_total = prev_entry[\"values\"][\"Sum\"]\n\n            if count < prev_count or total < prev_total:\n                logging.error(format_err_msg(\n                    f\"count or sum DECREASED during interval - Ignoring Entry.\"\n                    f\"prev_count:{prev_count}, count:{count}\"\n                    f\"prev_sum:{prev_total}, sum:{total},\"\n                    f\" (counter:{counter_name}), \"\n                    f\"prev_time:{prev_entry['time']}, time:{time}\",\n                    ErrContext(**{\"log_line\": line})))\n                return True\n\n        entries.append(\n            {\"time\": time,\n             \"values\": {\"P50\": float(match.group('P50')),\n                        \"P95\": float(match.group('P95')),\n                        \"P99\": float(match.group('P99')),\n                        \"P100\": float(match.group('P100')),\n                        \"Count\": count,\n                        \"Sum\": total,\n                        \"Average\": average,\n                        \"Interval Count\": count - prev_count,\n                        \"Interval Sum\": total - prev_total}})\n\n        return True\n\n    def does_have_counters_values(self):\n        return self.counters != {}\n\n    def does_have_histograms_values(self):\n        return self.histograms != {}\n\n    def get_counters_names(self):\n        return self.counters_names\n\n    def get_counters_times(self):\n        all_entries = self.get_all_counters_entries()\n        times = list(\n            {counter_entry[\"time\"]\n             for counter_entries in all_entries.values()\n             for counter_entry in counter_entries})\n        times.sort()\n        return times\n\n    def get_counter_entries(self, counter_name):\n        if counter_name not in self.counters:\n            return {}\n        return self.counters[counter_name]\n\n    def get_non_zeroes_counter_entries(self, counter_name):\n        counter_entries = self.get_counter_entries(counter_name)\n        return list(filter(lambda entry: entry['value'] > 0,\n                           counter_entries))\n\n    def are_all_counter_entries_zero(self, counter_name):\n        return len(self.get_non_zeroes_counter_entries(counter_name)) == 0\n\n    def get_all_counters_entries(self):\n        return self.counters\n\n    def get_counters_entries_not_all_zeroes(self):\n        result = {}\n\n        for counter_name, counter_entries in self.counters.items():\n            if not self.are_all_counter_entries_zero(counter_name):\n                result.update({counter_name: counter_entries})\n\n        return result\n\n    def get_first_counter_entry(self, counter_name):\n        entries = self.get_counter_entries(counter_name)\n        if not entries:\n            return {}\n        return entries[0]\n\n    def get_first_counter_value(self, counter_name, default=0):\n        last_entry = self.get_first_counter_entry(counter_name)\n\n        if not last_entry:\n            return default\n\n        return last_entry[\"value\"]\n\n    def get_last_counter_entry(self, counter_name):\n        entries = self.get_counter_entries(counter_name)\n        if not entries:\n            return {}\n        return entries[-1]\n\n    def get_last_counter_value(self, counter_name, default=0):\n        last_entry = self.get_last_counter_entry(counter_name)\n\n        if not last_entry:\n            return default\n\n        return last_entry[\"value\"]\n\n    def get_histogram_counters_names(self):\n        return self.histogram_counters_names\n\n    def get_histogram_counters_times(self):\n        all_entries = self.get_all_histogram_entries()\n        times = list(\n            {counter_entry[\"time\"]\n             for counter_entries in all_entries.values()\n             for counter_entry in counter_entries})\n        times.sort()\n        return times\n\n    def get_histogram_entries(self, counter_name):\n        if counter_name not in self.histograms:\n            return {}\n        return self.histograms[counter_name]\n\n    def get_all_histogram_entries(self):\n        return self.histograms\n\n    def get_last_histogram_entry(self, counter_name, non_zero):\n        entries = self.get_histogram_entries(counter_name)\n        if not entries:\n            return {}\n        last_entry = entries[-1]\n        is_zero_entry_func = \\\n            CountersMngr.is_histogram_entry_count_zero\n        if non_zero and is_zero_entry_func(last_entry):\n            return {}\n\n        return entries[-1]\n\n    @staticmethod\n    def is_histogram_entry_count_zero(entry):\n        return entry['values']['Count'] == 0\n\n    @staticmethod\n    def is_histogram_entry_count_not_zero(entry):\n        return entry['values']['Count'] > 0\n\n    def get_non_zeroes_histogram_entries(self, counter_name):\n        histogram_entries = self.get_histogram_entries(counter_name)\n        return list(filter(lambda entry: entry['values']['Count'] > 0,\n                           histogram_entries))\n\n    def are_all_histogram_entries_zero(self, counter_name):\n        return len(self.get_non_zeroes_histogram_entries(counter_name)) == 0\n\n    def get_histogram_entries_not_all_zeroes(self):\n        result = {}\n\n        for counter_name, histogram_entries in self.histograms.items():\n            if not self.are_all_histogram_entries_zero(counter_name):\n                result.update({counter_name: histogram_entries})\n\n        return result\n\n    @staticmethod\n    def get_histogram_entry_display_values(entry):\n        disp_values = {}\n        values = entry[\"values\"]\n\n        disp_values[\"Count\"] = \\\n            utils.get_human_readable_number(values[\"Count\"])\n        disp_values[\"Sum\"] = \\\n            utils.get_human_readable_number(values[\"Sum\"])\n        disp_values[\"Avg. Read Latency\"] = f'{values[\"Average\"]:.1f} us'\n        disp_values[\"P50\"] = f'{values[\"P50\"]:.1f} us'\n        disp_values[\"P95\"] = f'{values[\"P95\"]:.1f} us'\n        disp_values[\"P99\"] = f'{values[\"P99\"]:.1f} us'\n        disp_values[\"P100\"] = f'{values[\"P100\"]:.1f} us'\n\n        return disp_values", ""]}
{"filename": "utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\n\"\"\" Common constants and utilities used in the log parser's modules \"\"\"\n\nimport copy\nimport logging", "import copy\nimport logging\nimport pathlib\nimport re\nimport time\nfrom calendar import timegm\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom typing import Optional", "from enum import Enum\nfrom typing import Optional\n\nimport regexes\n\nMIN_PYTHON_VERSION_MAJOR = 3\nMIN_PYTHON_VERSION_MINOR = 8\n\nNO_CF = 'DB_WIDE'\nINVALID_CF = \"UNKNOWN-CF\"", "NO_CF = 'DB_WIDE'\nINVALID_CF = \"UNKNOWN-CF\"\nDEFAULT_CF_NAME = \"default\"\n\nINVALID_CF_ID = -1\nINVALID_JOB_ID = -1\nINVALID_FILE_NUMBER = -1\nINVALID_LEVEL = -1\nINVALID_FILTER_POLICY = \"INVALID-FILTER-POLICY\"\n", "INVALID_FILTER_POLICY = \"INVALID-FILTER-POLICY\"\n\nDB_WIDE_WRITE_BUFFER_MANAGER_OPTIONS_NAME = \"write_buffer_manager\"\n\nBASELINE_LOGS_FOLDER = \"baseline_logs\"\nDIFF_BASELINE_NAME = \"Baseline\"\nDIFF_LOG_NAME = \"Parsed Log\"\n\nDEFAULT_OUTPUT_FOLDER = \"output_files\"\nOUTPUT_SUB_FOLDER_PREFIX = \"run_\"", "DEFAULT_OUTPUT_FOLDER = \"output_files\"\nOUTPUT_SUB_FOLDER_PREFIX = \"run_\"\nDEFAULT_LOG_FILE_NAME = \"log_parser.log\"\nDEFAULT_JSON_FILE_NAME = \"log.json\"\nDEFAULT_COUNTERS_FILE_NAME = \"counters.csv\"\nDEFAULT_HUMAN_READABLE_HISTOGRAMS_FILE_NAME = \"histograms_human_readable.csv\"\nDEFAULT_TOOLS_HISTOGRAMS_FILE_NAME = \"histograms_tools.csv\"\nDEFAULT_COMPACTIONS_STATS_FILE_NAME = \"compactions_stats.csv\"\nDEFAULT_COMPACTIONS_FILE_NAME = \"compactions.csv\"\nDEFAULT_FLUSHES_FILE_NAME = \"flushes.csv\"", "DEFAULT_COMPACTIONS_FILE_NAME = \"compactions.csv\"\nDEFAULT_FLUSHES_FILE_NAME = \"flushes.csv\"\n\nFILE_NOT_GENERATED_TEXT = \"File Not Generated\"\nDATA_UNAVAILABLE_TEXT = \"Data Unavailable\"\nUNKNOWN_VALUE_TEXT = \"UNKNOWN\"\nNO_INGEST_TEXT = \"No Ingest Info Available\"\nNO_STATS_TEXT = \"No Statistics\"\nNO_COUNTERS_DUMPS_TEXT = \"No Counters Dumps Available\"\nNO_FLUSHES_TEXT = \"No Flush Started Events\"", "NO_COUNTERS_DUMPS_TEXT = \"No Counters Dumps Available\"\nNO_FLUSHES_TEXT = \"No Flush Started Events\"\nNO_SEEKS_TEXT = \"No Seeks\"\nNO_WARNS_TEXT = \"No Warnings\"\nNO_READS_TEXT = \"No Reads\"\nNO_COMPACTIONS_TEXT = \"No Compactions\"\nNO_BLOCK_CACHE_STATS = \"No Block Cache Statistics\"\nNO_GROWTH_INFO_TEXT = \"No Growth Information Available\"\n\n", "\n\n# =====================================\n#           MISC UTILS\n# =====================================\n\n\ndef get_last_dict_entry(d):\n    assert d is None or isinstance(d, dict)\n\n    if not d:\n        return None\n    key = list(d.keys())[-1]\n    return {key: d[key]}", "\n\ndef get_first_dict_entry_components(d):\n    assert d is None or isinstance(d, dict)\n\n    if not d:\n        return None\n    key = list(d.keys())[0]\n    return key, d[key]\n", "\n\ndef get_last_dict_entry_components(d):\n    assert d is None or isinstance(d, dict)\n\n    if not d:\n        return None\n    key = list(d.keys())[-1]\n    return key, d[key]\n", "\n\ndef delete_dict_keys(in_dict, keys_to_delete):\n    \"\"\" Delete specific keys from an dictionary\"\"\"\n    for key in keys_to_delete:\n        if key in in_dict:\n            del in_dict[key]\n\n\ndef unify_dicts(dict1, dict2, favor_first):\n    # Avoid mutating the input dictionary\n    unified_dict = copy.deepcopy(dict1)\n\n    for key, value in dict2.items():\n        if key in unified_dict:\n            if not favor_first:\n                unified_dict[key] = dict2[key]\n        else:\n            unified_dict[key] = value\n\n    return unified_dict", "\ndef unify_dicts(dict1, dict2, favor_first):\n    # Avoid mutating the input dictionary\n    unified_dict = copy.deepcopy(dict1)\n\n    for key, value in dict2.items():\n        if key in unified_dict:\n            if not favor_first:\n                unified_dict[key] = dict2[key]\n        else:\n            unified_dict[key] = value\n\n    return unified_dict", "\n\ndef replace_key_keep_order(d, existing_key, new_key):\n    if existing_key not in d:\n        return\n\n    pos = list(d.keys()).index(existing_key)\n    value = d[existing_key]\n\n    items = list(d.items())\n    items.insert(pos, (new_key, value))\n    updated_d = dict(items)\n    del(updated_d[existing_key])\n\n    return updated_d", "\n\ndef insert_dict_entry_before_key(d, existing_key, new_key, new_value):\n    if existing_key not in d:\n        return\n\n    pos = list(d.keys()).index(existing_key)\n    value = d[existing_key]\n\n    items = list(d.items())\n    items.insert(pos, (new_key, value))\n    updated_d = dict(items)\n\n    return updated_d", "\n\ndef insert_dict_entry_after_key(d, existing_key, new_key, new_value):\n    if existing_key not in d:\n        return\n\n    pos = list(d.keys()).index(existing_key)\n    value = d[existing_key]\n\n    items = list(d.items())\n    items.insert(pos, (new_key, value))\n    updated_d = dict(items)\n\n    return updated_d", "\n\ndef find_dict_keys_matching_prefix(d, key_prefix):\n    assert isinstance(d, dict)\n    matching = list()\n    for key in d.keys():\n        if isinstance(key, str) and key.startswith(key_prefix):\n            matching.append(key)\n    return matching\n", "\n\ndef find_list_items_matching_prefix(lst, prefix):\n    assert isinstance(lst, list)\n    return [e for e in lst if isinstance(e, str) and e.startswith(prefix)]\n\n\ndef are_dicts_equal_and_in_same_keys_order(d1, d2):\n    if d1 != d2:\n        return False\n    return list(d1.keys()) == list(d2.keys())", "\n\ndef unify_lists_preserve_order(l1, l2):\n    # Unifies the lists, removes duplicates, but maintains the order\n    unified_with_duplicates = l1 + l2\n    seen = set()\n    return [x for x in unified_with_duplicates\n            if not (x in seen or seen.add(x))]\n\n\ndef sort_dict_on_values(d):\n    return dict(sorted(d.items(), key=lambda x: x[1]))", "\n\ndef sort_dict_on_values(d):\n    return dict(sorted(d.items(), key=lambda x: x[1]))\n\n\ndef has_method(obj, method_name):\n    \"\"\" Checks if a method exists in an obj \"\"\"\n    return method_name in dir(obj)\n", "\n\ndef get_gmt_timestamp_us(time_str):\n    \"\"\"\n    Converts a time string in the log's format to a GMT Unix Timestamp.\n    The resolution is in seconds\n\n    Example: '2018/07/25-11:25:45.782710' will be converted into 1532517945\n    \"\"\"\n    hr_time = time_str + 'GMT'\n    dt = datetime.strptime(hr_time,  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n    us = dt.microsecond\n    epoch_seconds = timegm(time.strptime(hr_time, \"%Y/%m/%d-%H:%M:%S.%f%Z\"))\n    return epoch_seconds * 10**6 + us", "\n\ndef get_time_relative_to(base_time, num_seconds, num_us=0):\n    base_dt = datetime.strptime(base_time + 'GMT',  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n    num_seconds = num_seconds + num_us / 10**6\n    dt = timedelta(seconds=num_seconds)\n    rel_time_dt = base_dt + dt\n    return rel_time_dt.strftime(\"%Y/%m/%d-%H:%M:%S.%f%Z\")\n\n\ndef get_times_strs_diff(time_str1, time_str2):\n    \"\"\" Calculates the difference between 2 log time string \"\"\"\n    dt1 = datetime.strptime(time_str1 + 'GMT',  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n    dt2 = datetime.strptime(time_str2 + 'GMT',  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n    return dt2 - dt1", "\n\ndef get_times_strs_diff(time_str1, time_str2):\n    \"\"\" Calculates the difference between 2 log time string \"\"\"\n    dt1 = datetime.strptime(time_str1 + 'GMT',  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n    dt2 = datetime.strptime(time_str2 + 'GMT',  \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n    return dt2 - dt1\n\n\ndef get_times_strs_diff_seconds(time_str1, time_str2):\n    return get_times_strs_diff(time_str1, time_str2).total_seconds()", "\ndef get_times_strs_diff_seconds(time_str1, time_str2):\n    return get_times_strs_diff(time_str1, time_str2).total_seconds()\n\n\ndef compare_times_strs(time1_str, time2_str):\n    \"\"\" Compares 2 log time strings\n\n    Returns:\n        -1: time1_str < time2_str\n        0: time1_str == time2_str\n        1: time1_str > time2_str\n    \"\"\"\n    diff_total_seconds = get_times_strs_diff_seconds(time2_str, time1_str)\n    if diff_total_seconds < 0:\n        return -1\n    elif diff_total_seconds > 0:\n        return 1\n    else:\n        return 0", "\n\ndef convert_seconds_to_dd_hh_mm_ss(seconds):\n    seconds = int(seconds)\n    days = int(seconds / 86400)\n    return time.strftime(f'{days}d %Hh %Mm %Ss', time.gmtime(seconds))\n\n\n# =====================================\n#       LOGGING TYPES & UTILS", "# =====================================\n#       LOGGING TYPES & UTILS\n# =====================================\n\n@dataclass\nclass ParsingContext:\n    file_path: str = None\n    parsing_done: bool = False\n    line_idx: int = 0\n    line: str = None\n\n    def parsing_starts(self, file_path):\n        self.file_path = file_path\n\n    def parsing_ends(self):\n        assert not self.parsing_done\n        self.parsing_done = True\n\n    def is_parsing_done(self):\n        return self.parsing_done\n\n    def update_line(self, line_idx, new_line=None):\n        self.line_idx = line_idx\n        if new_line:\n            self.line = new_line\n        else:\n            self.line = None\n\n    def increment_line(self, increment=1, new_line=None):\n        new_line_idx = self.line_idx + increment\n        self.update_line(new_line_idx, new_line)", "\n\nparsing_context = None\n\n\n@dataclass\nclass ErrorContext:\n    file_path: Optional[str] = None\n    log_line_idx: Optional[int] = None\n    log_line: Optional[str] = None\n\n    def __str__(self):\n        file_path = self.file_path if self.file_path is not None else \"?\"\n        line_num = self.log_line_idx + 1 \\\n            if self.log_line_idx is not None else \"?\"\n\n        result_str = f\"[File:{file_path} (line#:{line_num})]\"\n        if self.log_line is not None:\n            result_str += f\"\\n{self.log_line}\"\n        return result_str", "\n\ndef get_error_context_from_entry(entry, file_path=None):\n    error_context = ErrorContext()\n    if not error_context:\n        error_context = ErrorContext()\n    if file_path:\n        error_context.file_path = file_path\n    error_context.log_line = entry.get_msg()\n    error_context.log_line_idx = entry.get_start_line_idx()\n    return error_context", "\n\ndef format_err_msg(msg, error_context=None, entry=None, file_path=None):\n    if entry:\n        error_context = get_error_context_from_entry(entry, file_path)\n\n    result_str = msg\n    if error_context is not None:\n        result_str += \" - \" + str(error_context)\n    return result_str", "\n\ndef get_line_num_from_entry(entry, rel_line_idx=None):\n    line_idx = entry.get_start_line_idx()\n    if rel_line_idx is not None:\n        line_idx += rel_line_idx\n    return line_idx + 1\n\n\ndef format_line_num_from_line_idx(line_idx):\n    return f\"[line# {line_idx + 1}]\"", "\ndef format_line_num_from_line_idx(line_idx):\n    return f\"[line# {line_idx + 1}]\"\n\n\ndef format_line_num_from_entry(entry, rel_line_idx=None):\n    line_idx = entry.get_start_line_idx()\n    if rel_line_idx is not None:\n        line_idx += rel_line_idx\n    return format_line_num_from_line_idx(line_idx)", "\n\ndef format_lines_range_from_entries(start_entry, end_entry):\n    result = \"[lines#\"\n    result += f\"{start_entry.get_start_line_idx() + 1}\"\n    result += \"-\"\n    result += f\"{end_entry.get_end_line_idx() + 1}\"\n    result += \"]\"\n    return result\n", "\n\ndef format_lines_range_from_entries_idxs(log_entries, start_idx, end_idx):\n    return format_lines_range_from_entries(log_entries[start_idx],\n                                           log_entries[end_idx])\n\n\ndef print_msg(msg, to_console=True, console_msg=None):\n    logging.info(msg)\n    if to_console:\n        if console_msg:\n            print(console_msg)\n        else:\n            print(msg)", "\n\n# =====================================\n#       EXCEPTIONS\n# =====================================\n\n\nclass ParsingError(Exception):\n    def __init__(self, msg, error_context=None):\n        self.msg = msg\n        self.context = error_context\n\n    def __str__(self):\n        result_str = self.msg\n        if self.context is not None:\n            result_str += str(self.context)\n        return result_str\n\n    def set_context(self, error_context):\n        self.context = error_context", "\n\nclass ParsingAssertion(ParsingError):\n    def __init__(self, msg, error_context=None):\n        super().__init__(msg, error_context)\n\n\nclass LogFileNotFoundError(Exception):\n    def __init__(self, file_path):\n        self.msg = f\"Log file to parse ({file_path}) Not Found\"", "\n\nclass EmptyLogFile(Exception):\n    def __init__(self, file_path):\n        self.msg = f\"{file_path} Is Empty\"\n\n\nclass InvalidLogFile(Exception):\n    def __init__(self, file_path):\n        self.msg = f\"{file_path} is not a valid log File\"", "\n\nclass WarningType(str, Enum):\n    WARN = \"WARN\"\n    ERROR = \"ERROR\"\n    FATAL = \"FATAL\"\n\n\nclass ConsoleOutputType(str, Enum):\n    SHORT = \"short\"\n    LONG = \"long\"", "class ConsoleOutputType(str, Enum):\n    SHORT = \"short\"\n    LONG = \"long\"\n\n\nclass ProductName(str, Enum):\n    ROCKSDB = \"RocksDB\"\n    SPEEDB = \"Speedb\"\n\n    def __eq__(self, other):\n        return self.lower() == other.lower()", "\n\n# =====================================\n#       PARSING UTILITIES\n# =====================================\n\n\ndef parse_time_str(time_str, expect_valid_str=True):\n    try:\n        return datetime.strptime(time_str, '%Y/%m/%d-%H:%M:%S.%f')\n    except ValueError:\n        if expect_valid_str:\n            raise ParsingError(f\"Invalid time str ({time_str}\")\n        return None", "\n\ndef is_valid_time_str(time_str):\n    return parse_time_str(time_str, expect_valid_str=False) is not None\n\n\nNUM_BYTES_UNITS_STRS = [\"KB\", \"MB\", \"GB\", \"TB\"]\nNUM_UNITS_STRS = [\"K\", \"M\", \"G\"]\n\n\ndef __convert_human_readable_components(\n        num_bytes_without_unit_str, size_units_str, units_list, factor):\n    try:\n        num_bytes_without_unit_str = float(num_bytes_without_unit_str)\n    except ValueError:\n        raise ParsingError(\n            f\"Num bytes is not a nummer: {num_bytes_without_unit_str}\")\n\n    size_units_str = size_units_str.strip()\n\n    try:\n        unit_idx = units_list.index(size_units_str)\n        multiplier = factor ** (unit_idx + 1)\n    except ValueError:\n        if size_units_str != '':\n            raise ParsingAssertion(\n                f\"Unexpected size units ({size_units_str}\")\n        multiplier = 1\n\n    result = float(num_bytes_without_unit_str) * multiplier\n    return int(result)", "\n\ndef __convert_human_readable_components(\n        num_bytes_without_unit_str, size_units_str, units_list, factor):\n    try:\n        num_bytes_without_unit_str = float(num_bytes_without_unit_str)\n    except ValueError:\n        raise ParsingError(\n            f\"Num bytes is not a nummer: {num_bytes_without_unit_str}\")\n\n    size_units_str = size_units_str.strip()\n\n    try:\n        unit_idx = units_list.index(size_units_str)\n        multiplier = factor ** (unit_idx + 1)\n    except ValueError:\n        if size_units_str != '':\n            raise ParsingAssertion(\n                f\"Unexpected size units ({size_units_str}\")\n        multiplier = 1\n\n    result = float(num_bytes_without_unit_str) * multiplier\n    return int(result)", "\n\ndef get_num_bytes_from_human_readable_components(num_bytes_without_unit_str,\n                                                 size_units_str):\n    return __convert_human_readable_components(\n        num_bytes_without_unit_str,\n        size_units_str,\n        NUM_BYTES_UNITS_STRS,\n        1024)\n", "\n\ndef get_num_bytes_from_human_readable_str(size_with_unit_str):\n    match = re.findall(f\"{regexes.NUM_BYTES_WITH_UNIT_ONLY}\",\n                       size_with_unit_str)\n    if not match:\n        raise ParsingError(f\"Invalid size with unit str ({size_with_unit_str}\")\n\n    size, size_unit = match[0]\n\n    return get_num_bytes_from_human_readable_components(size, size_unit)", "\n\ndef get_human_readable_num_bytes(size_in_bytes):\n    if size_in_bytes < 2 ** 10:\n        return str(size_in_bytes) + \" B\"\n    elif size_in_bytes < 2 ** 20:\n        size_units_str = \"KB\"\n        divider = 2 ** 10\n    elif size_in_bytes < 2 ** 30:\n        size_units_str = \"MB\"\n        divider = 2 ** 20\n    elif size_in_bytes < 2 ** 40:\n        size_units_str = \"GB\"\n        divider = 2 ** 30\n    else:\n        size_units_str = \"TB\"\n        divider = 2 ** 40\n\n    return f\"{float(size_in_bytes) / divider:.1f} {size_units_str}\"", "\n\ndef get_number_from_human_readable_components(num_bytes_without_unit_str,\n                                              size_units_str):\n    return __convert_human_readable_components(\n        num_bytes_without_unit_str,\n        size_units_str,\n        NUM_UNITS_STRS,\n        1000)\n", "\n\ndef get_number_from_human_readable_str(number_with_units_str):\n    match = re.findall(f\"{regexes.NUM_WITH_UNIT_ONLY}\", number_with_units_str)\n    if not match:\n        raise ParsingError(\n            f\"Invalid size with unit str ({number_with_units_str}\")\n\n    size, size_unit = match[0]\n    return get_number_from_human_readable_components(size, size_unit)", "\n\ndef get_human_readable_number(number):\n    assert number >= 0\n\n    if number < 10 ** 4:\n        return str(number)\n    elif number < 10 ** 7:\n        size_units_str = \"K\"\n        divider = 10**3\n    elif number < 10 ** 10:\n        size_units_str = \"M\"\n        divider = 10**6\n    else:\n        size_units_str = \"G\"\n        divider = 10**9\n\n    return f\"{float(number) / divider:.1f} {size_units_str}\"", "\n\ndef get_num_leading_spaces(line):\n    return len(line) - len(line.lstrip())\n\n\ndef remove_empty_lines_at_start(lines):\n    return [line for line in lines if line.strip()]\n\n\ndef try_find_cfs_in_lines(cfs_names, lines):\n    \"\"\" Try to find the first cf name in cfs_names that appears in lines\n    cf names are searched as \"[<cf-name>]\"\n    Returns either the cf-name(s) (if found) or None (if not)\n    \"\"\"\n    if isinstance(lines, list):\n        lines = \"\\n\".join(lines)\n    match = re.findall(regexes.CF_NAME_OLD, lines, re.MULTILINE)\n    if not match:\n        return None\n\n    potential_cfs_names_set = set(match)\n    cfs_names_set = set(cfs_names)\n    found_cfs_names = list(potential_cfs_names_set.intersection(cfs_names_set))\n\n    if not found_cfs_names:\n        return None\n    if len(found_cfs_names) == 1:\n        return found_cfs_names[0]\n    else:\n        return found_cfs_names", "\n\ndef try_find_cfs_in_lines(cfs_names, lines):\n    \"\"\" Try to find the first cf name in cfs_names that appears in lines\n    cf names are searched as \"[<cf-name>]\"\n    Returns either the cf-name(s) (if found) or None (if not)\n    \"\"\"\n    if isinstance(lines, list):\n        lines = \"\\n\".join(lines)\n    match = re.findall(regexes.CF_NAME_OLD, lines, re.MULTILINE)\n    if not match:\n        return None\n\n    potential_cfs_names_set = set(match)\n    cfs_names_set = set(cfs_names)\n    found_cfs_names = list(potential_cfs_names_set.intersection(cfs_names_set))\n\n    if not found_cfs_names:\n        return None\n    if len(found_cfs_names) == 1:\n        return found_cfs_names[0]\n    else:\n        return found_cfs_names", "\n\n# =====================================\n#           FILE PATHS UTILS\n# =====================================\n\n\ndef get_file_path(file_folder_name, file_basename):\n    return pathlib.Path(f\"{file_folder_name}/{file_basename}\").resolve()\n", "\n\ndef get_json_file_path(output_folder, json_file_name):\n    return get_file_path(output_folder, json_file_name)\n\n\ndef get_log_file_path(output_folder):\n    return get_file_path(output_folder, DEFAULT_LOG_FILE_NAME)\n\n\ndef get_counters_csv_file_path(output_folder):\n    return get_file_path(output_folder, DEFAULT_COUNTERS_FILE_NAME)", "\n\ndef get_counters_csv_file_path(output_folder):\n    return get_file_path(output_folder, DEFAULT_COUNTERS_FILE_NAME)\n\n\ndef get_human_readable_histograms_csv_file_path(output_folder):\n    return get_file_path(output_folder,\n                         DEFAULT_HUMAN_READABLE_HISTOGRAMS_FILE_NAME)\n", "\n\ndef get_tools_histograms_csv_file_path(output_folder):\n    return get_file_path(output_folder,\n                         DEFAULT_TOOLS_HISTOGRAMS_FILE_NAME)\n\n\ndef get_compactions_stats_csv_file_path(output_folder):\n    return get_file_path(output_folder,\n                         DEFAULT_COMPACTIONS_STATS_FILE_NAME)", "\n\ndef get_compactions_csv_file_path(output_folder):\n    return get_file_path(output_folder,\n                         DEFAULT_COMPACTIONS_FILE_NAME)\n\n\ndef get_flushes_csv_file_path(output_folder):\n    return get_file_path(output_folder, DEFAULT_FLUSHES_FILE_NAME)\n", ""]}
{"filename": "compactions.py", "chunked_list": ["from __future__ import annotations\n\nimport logging\nimport re\nfrom dataclasses import asdict, dataclass\nfrom enum import auto\n\nimport events\nimport regexes\nimport utils", "import regexes\nimport utils\n\n\nclass CompactionState:\n    WAITING_START = auto()\n    STARTED = auto()\n    FINISHED = auto()\n\n", "\n\n@dataclass\nclass PreFinishStatsInfo:\n    cf_name: str\n    read_rate_mbps: float = 0.0\n    write_rate_mbps: float = 0.0\n    read_write_amplify: float = 0.0\n    write_amplify: float = 0.0\n    records_in: int = 0\n    records_dropped: int = 0\n\n    def as_dict(self):\n        return asdict(self)", "\n\n@dataclass\nclass CompactionJobInfo:\n    job_id: int\n    cf_name: str = None\n    start_event: events.CompactionStartedEvent = None\n    finish_event: events.CompactionFinishedEvent = None\n    pre_finish_info: PreFinishStatsInfo = None\n\n    def __str__(self):\n        return f\"Compaction Info: \" \\\n               f\"job_id:{self.job_id}, \" \\\n               f\"cf_name:{self.cf_name}\" \\\n               f\"start_time:{self.get_start_time()}\" \\\n               f\"finish_time:{self.get_finish_time()}\"\n\n    def get_state(self):\n        if self.finish_event is not None:\n            assert self.start_event is not None\n            return CompactionState.FINISHED\n        elif self.start_event is not None:\n            return CompactionState.STARTED\n        else:\n            return CompactionState.WAITING_START\n\n    def has_finished(self):\n        return self.get_state() == CompactionState.FINISHED\n\n    def get_start_time(self):\n        return self.start_event.get_log_time() if self.start_event else \"\"\n\n    def get_finish_time(self):\n        return self.finish_event.get_log_time() if self.finish_event else \"\"\n\n    def set_start_event(self, start_event):\n        if self.get_state() != CompactionState.WAITING_START:\n            raise utils.ParsingError(f\"Unexpected Compaction's state to \"\n                                     f\"set start event ({start_event}. {self}\")\n        self.start_event = start_event\n\n    def set_finish_event(self, finish_event):\n        if self.get_state() != CompactionState.STARTED:\n            raise utils.ParsingError(f\"Unexpected Compaction's state to set \"\n                                     f\"finish event ({finish_event}. {self}\")\n        if utils.compare_times_strs(self.get_start_time(),\n                                    finish_event.get_log_time()) > 0:\n            raise utils.ParsingError(\n                f\"finish event's time ({finish_event} before \"\n                f\"start event time.\\n{finish_event}\")\n        self.finish_event = finish_event", "\n\nclass CompactionsMonitor:\n    def __init__(self):\n        self.jobs = dict()\n        self.pre_finish_stats_infos = list()\n\n    def consider_entry(self, entry):\n        try:\n            is_finish_stats_line, cf_name = \\\n                self.try_parse_as_pre_finish_stats_line(entry)\n            if is_finish_stats_line:\n                return True, cf_name\n        except utils.ParsingError as e:\n            logging.WARN(f\"Error adding entry to compaction jobs.\\n\"\n                         f\"error: {e}\\n\"\n                         f\"entry:{entry}\")\n\n        return False, None\n\n    def add_job_if_applicable(self, job_id, cf_name=None, entry=None):\n        entry_cf_name = entry.get_cf_name() if entry else cf_name\n        entry_job_id = entry.get_job_id() if entry else job_id\n\n        if cf_name is not None and entry_cf_name != cf_name or \\\n                entry_job_id != job_id:\n            raise utils.ParsingError(\n                f\"entry's cf name ({entry_cf_name}) or entry_job_id \"\n                f\"({entry_job_id}) != what was parsed here.\\nentry: {entry}\")\n\n        if job_id not in self.jobs:\n            self.jobs[job_id] = CompactionJobInfo(job_id, cf_name)\n        else:\n            if self.jobs[job_id].cf_name != cf_name:\n                raise utils.ParsingError(\n                    f\"new cf_name ({cf_name}) != existing (\"\n                    f\"{self.jobs[job_id].cf_name}) for same job.\")\n\n    def try_parse_as_score_entry(self, entry):\n        match = \\\n            re.findall(regexes.COMPACTION_BEFORE_SCORE_LINE, entry.get_msg())\n        if not match:\n            return False\n        assert len(match) == 1 and len(match[0]) == 4\n\n        cf_name, job_id, _, before_score = match[0]\n        job_id = int(job_id)\n        before_score = float(before_score)\n\n        self.add_job_if_applicable(job_id, cf_name, entry)\n        self.jobs[job_id].before_score = before_score\n\n        return True\n\n    def try_parse_as_pre_finish_stats_line(self, entry):\n        match = re.findall(regexes.COMPACTION_JOB_FINISH_STATS_LINE,\n                           entry.get_msg())\n        if not match:\n            return False, None\n        assert len(match) == 1 and len(match[0]) == 7\n\n        info = PreFinishStatsInfo(*match[0])\n        info.read_rate_mbps = float(info.read_rate_mbps)\n        info.write_rate_mbps = float(info.write_rate_mbps)\n        info.read_write_amplify = float(info.read_write_amplify)\n        info.write_amplify = float(info.write_amplify)\n        info.records_in = int(info.records_in)\n        info.records_dropped = int(info.records_dropped)\n\n        self.pre_finish_stats_infos.append(info)\n\n        return True, info.cf_name\n\n    def new_event(self, event):\n        event_type = event.get_type()\n        if event_type == events.EventType.COMPACTION_STARTED:\n            self.compaction_started(event)\n        elif event_type == events.EventType.COMPACTION_FINISHED:\n            self.compaction_finished(event)\n\n    def compaction_started(self, event):\n        assert isinstance(event, events.CompactionStartedEvent)\n\n        # 2023/01/04-08:55:00.743718 27420 EVENT_LOG_v1\n        # {\"time_micros\": 1672822500743711, \"job\": 9,\n        # \"event\": \"compaction_started\",\n        # \"compaction_reason\": \"LevelL0FilesNum\",\n        # \"files_L0\": [17250, 17247, 17243, 17239], \"score\": 1,\n        # \"input_data_size\": 251316602}\n        #\n        job_id = event.get_job_id()\n        self.add_job_if_applicable(job_id, event.get_cf_name())\n        self.jobs[job_id].set_start_event(event)\n\n    def compaction_finished(self, event):\n        # 2023/01/04-08:55:00.746783 27413\n        # (Original Log Time 2023/01/04-08:55:00.746653) EVENT_LOG_v1\n        # {\"time_micros\": 1672822500746645, \"job\": 4,\n        # \"event\": \"compaction_finished\",\n        # \"compaction_time_micros\": 971568,\n        # \"compaction_time_cpu_micros\": 935180,\n        # \"output_level\": 1, \"num_output_files\": 7,\n        # \"total_output_size\": 437263613,\n        # \"num_input_records\": 424286, \"num_output_records\": 423497,\n        # \"num_subcompactions\": 1, \"output_compression\": \"NoCompression\",\n        # \"num_single_delete_mismatches\": 0,\n        # \"num_single_delete_fallthrough\": 0,\n        # \"lsm_state\": [4, 7, 45, 427, 822, 0, 0]}\n        #\n        assert isinstance(event, events.CompactionFinishedEvent)\n\n        job_id = event.get_job_id()\n        if job_id not in self.jobs:\n            logging.info(\n                f\"Compaction finished event for job for which there is no \"\n                f\"recorded compaction started. job-id:{job_id}\\n\"\n                f\"event:{event}\")\n            return\n\n        job = self.jobs[job_id]\n        job.set_finish_event(event)\n\n        # Try to match the pre-finish info based on the number of\n        # input records and cf name\n        num_input_records = event.get_num_input_records()\n        for idx, info in enumerate(self.pre_finish_stats_infos):\n            if num_input_records == info.records_in:\n                if job.cf_name == info.cf_name:\n                    job.pre_finish_info = info\n                    self.pre_finish_stats_infos.pop(idx)\n                    break\n                else:\n                    logging.info(\n                        f\"# input records match, but different cf-s.\\n\"\n                        f\"pre-finish info:{info}\\n\"\n                        f\"job:{asdict(job)}\\n\"\n                        f\"event:{event}\")\n\n    def get_finished_jobs(self):\n        finished_jobs = dict()\n        for job_id, job_info in self.jobs.items():\n            if job_info.has_finished():\n                finished_jobs[job_id] = job_info\n        return finished_jobs\n\n    def get_cf_finished_jobs(self, cf_name):\n        finished_cf_jobs = dict()\n        for job_id, job_info in self.jobs.items():\n            if job_info.has_finished() and job_info.cf_name == cf_name:\n                finished_cf_jobs[job_id] = job_info\n        return finished_cf_jobs", ""]}
{"filename": "csv_outputter.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\nimport copy\nimport csv\nimport io\nimport logging\nfrom dataclasses import dataclass", "import logging\nfrom dataclasses import dataclass\n\nimport utils\nfrom events import FlowType, EventField\n\n\ndef get_counters_csv(counter_and_histograms_mngr):\n    f = io.StringIO()\n    writer = csv.writer(f)\n\n    mngr = counter_and_histograms_mngr\n    # Get all counters for which at least one entry is not 0 (=> there is at\n    # least one value that should be included for them in the CSV)\n    all_applicable_entries = mngr.get_counters_entries_not_all_zeroes()\n\n    if not all_applicable_entries:\n        logging.info(\"No counters with non-zero values => NO CSV\")\n        return None\n\n    counters_names = list(all_applicable_entries.keys())\n    times = mngr.get_counters_times()\n\n    # Support counter entries with missing entries for some time point\n    # Maintain an index per counter that advances only when the counter has\n    # a value per csv row (one row per time point)\n    counters_idx = {name: 0 for name in counters_names}\n\n    # csv header line (counter names)\n    writer.writerow([\"Time\"] + counters_names)\n\n    # Write one line per time:\n    for time_idx, time in enumerate(times):\n        csv_line = list()\n        csv_line.append(time)\n        for counter_name in counters_names:\n            counter_entries = all_applicable_entries[counter_name]\n\n            value = 0\n            if counters_idx[counter_name] < len(counter_entries):\n                counter_entry_time =\\\n                    counter_entries[counters_idx[counter_name]][\"time\"]\n                time_diff = \\\n                    utils.compare_times_strs(counter_entry_time, time)\n                assert time_diff >= 0\n                if time_diff == 0:\n                    value =\\\n                        counter_entries[counters_idx[counter_name]][\"value\"]\n                    counters_idx[counter_name] += 1\n\n            csv_line.append(value)\n\n        writer.writerow(csv_line)\n\n    return f.getvalue()", "\n\ndef get_human_readable_histogram_csv(counter_and_histograms_mngr):\n    f = io.StringIO()\n    writer = csv.writer(f)\n\n    mngr = counter_and_histograms_mngr\n    # Get all histograms for which at least one entry is not 0 (=> there is at\n    # least one value that should be included for them in the CSV)\n    all_applicable_entries = mngr.get_histogram_entries_not_all_zeroes()\n\n    if not all_applicable_entries:\n        logging.info(\"No Histograms with non-zero values => NO CSV\")\n        return None\n\n    counters_names = list(all_applicable_entries.keys())\n    times = mngr.get_histogram_counters_times()\n\n    # Support histogram entries with missing entries for some time point\n    # Maintain an index per histogram that advances only when the histogram has\n    # a value per csv row (one row per time point)\n    histograms_idx = {name: 0 for name in counters_names}\n\n    # csv header lines (counter names)\n    header_line1 = [\"\"]\n    header_line2 = [\"\"]\n\n    counter_histogram_columns =\\\n        list(all_applicable_entries[counters_names[0]][0][\"values\"].keys())\n    counter_histogram_columns.remove(\"Average\")\n    counter_histogram_columns.remove(\"Interval Count\")\n    counter_histogram_columns.remove(\"Interval Sum\")\n\n    num_counter_columns = len(counter_histogram_columns)\n    for counter_name in counters_names:\n        name_columns = [\".\" for i in range(num_counter_columns-1)]\n        name_columns.insert(0, counter_name)\n        # name_columns[int(num_counter_columns/2)] = counter_name\n        header_line1.extend(name_columns)\n\n        header_line2.extend(counter_histogram_columns)\n\n    writer.writerow(header_line1)\n    writer.writerow(header_line2)\n\n    # Write one line per time:\n    zero_values = [0 for i in range(num_counter_columns)]\n    for time_idx, time in enumerate(times):\n        csv_line = list()\n        csv_line.append(time)\n        for counter_name in counters_names:\n            histogram_entries = all_applicable_entries[counter_name]\n\n            values = zero_values\n            idx = histograms_idx[counter_name]\n            if idx < len(histogram_entries):\n                counter_entry_time = histogram_entries[idx][\"time\"]\n                time_diff = \\\n                    utils.compare_times_strs(counter_entry_time, time)\n                assert time_diff >= 0\n                if time_diff == 0:\n                    values = list(histogram_entries[idx][\"values\"].values())\n                    histograms_idx[counter_name] += 1\n\n            csv_line.extend(values)\n\n        writer.writerow(csv_line)\n\n    return f.getvalue()", "\n\ndef get_tools_histogram_csv(counter_and_histograms_mngr):\n    f = io.StringIO()\n    writer = csv.writer(f)\n\n    mngr = counter_and_histograms_mngr\n    # Get all histograms for which at least one entry is not 0 (=> there is at\n    # least one value that should be included for them in the CSV)\n    all_applicable_entries = mngr.get_histogram_entries_not_all_zeroes()\n\n    if not all_applicable_entries:\n        logging.info(\"No Histograms with non-zero values => NO CSV\")\n        return None\n\n    counters_names = list(all_applicable_entries.keys())\n    times = mngr.get_histogram_counters_times()\n\n    # Support histogram entries with missing entries for some time point\n    # Maintain an index per histogram that advances only when the histogram has\n    # a value per csv row (one row per time point)\n    histograms_idx = {name: 0 for name in counters_names}\n\n    # csv header lines (counter names)\n    header_line = [\"Name\", \"Time\"]\n    counter_histogram_columns =\\\n        list(all_applicable_entries[counters_names[0]][0][\"values\"].keys())\n    header_line.extend(counter_histogram_columns)\n    num_counter_columns = len(counter_histogram_columns)\n    writer.writerow(header_line)\n\n    # Write one line per time:\n    zero_values = [0 for i in range(num_counter_columns)]\n    for counter_name in counters_names:\n        for time_idx, time in enumerate(times):\n            csv_line = [counter_name, time]\n            histogram_entries = all_applicable_entries[counter_name]\n\n            values = zero_values\n            idx = histograms_idx[counter_name]\n            if idx < len(histogram_entries):\n                counter_entry_time = histogram_entries[idx][\"time\"]\n                time_diff = \\\n                    utils.compare_times_strs(counter_entry_time, time)\n                assert time_diff >= 0\n                if time_diff == 0:\n                    values = list(histogram_entries[idx][\"values\"].values())\n                    histograms_idx[counter_name] += 1\n\n                csv_line.extend(values)\n\n            writer.writerow(csv_line)\n\n    return f.getvalue()", "\n\ndef get_compaction_stats_csv(compaction_stats_mngr):\n    f = io.StringIO()\n    writer = csv.writer(f)\n\n    entries = compaction_stats_mngr.get_level_entries()\n\n    if not entries:\n        logging.info(\"No Compaction Stats => NO CSV\")\n        return None\n\n    temp = list(list(entries.values())[0].values())[0]\n    columns_names = list(list(temp.values())[0].keys())\n    header_line = [\"Time\", \"Column Family\", \"Level\"] + columns_names\n    writer.writerow(header_line)\n\n    for time, time_entry in entries.items():\n        for cf_name, cf_entry in time_entry.items():\n            for level, level_values in cf_entry.items():\n                row = [time, cf_name, level]\n                row += list(level_values.values())\n                writer.writerow(row)\n\n    return f.getvalue()", "\n\ndef get_flow_events_csv(cfs_names, events_mngr, flow_type):\n    f = io.StringIO()\n    writer = csv.writer(f)\n\n    immutable_events = events_mngr.get_all_flow_events(flow_type, cfs_names)\n    if not immutable_events:\n        return None\n\n    # Going to modify the events so make a modifiable copy first\n    events = copy.deepcopy(immutable_events)\n    first_event = True\n    for event_pair in events:\n        start_event = event_pair[0]\n        finish_event = event_pair[1]\n        start_event_data = start_event.get_event_data_dict()\n        cf_name = start_event.get_cf_name()\n        event_start_time = start_event.get_log_time()\n\n        if not finish_event:\n            event_finish_time = \"UNKNOWN\"\n            event_data = start_event_data\n        else:\n            event_finish_time = finish_event.get_log_time()\n            finish_event_data = finish_event.get_event_data_dict()\n            event_data = utils.unify_dicts(\n                start_event_data, finish_event_data, favor_first=True)\n\n        fields_to_del = [EventField.CF_NAME,\n                         EventField.TIME_MICROS,\n                         EventField.EVENT_TYPE]\n        utils.delete_dict_keys(event_data, fields_to_del)\n\n        if first_event:\n            first_event = False\n            event_columns_names = list(list(event_data.keys()))\n            header_line = [\"Start Time\", \"Finish Time\", \"Column Family\"] + \\\n                event_columns_names\n            writer.writerow(header_line)\n\n        row = [event_start_time, event_finish_time, cf_name]\n        row += list(event_data.values())\n        writer.writerow(row)\n\n    return f.getvalue()", "\n\n@dataclass\nclass CompactionsCsvInputFilesInfo:\n    updated_columns_names: list = None\n    first_column_idx: int = None\n    first_level: int = None\n    second_level: int = None\n\n\ndef process_compactions_csv_header(columns_names):\n    # Assume that, in general, compactions potentially have 2 \"files_\" columns\n    # (They may have one, and, maybe more than 2)\n    # Name them:\n    # 1. The first: \"Input Level Files\"\n    # 2. The second: \"Input Files from Output Level\"\n    prefix = \"files_L\"\n    prefix_len = len(prefix)\n\n    updated_columns_names = copy.deepcopy(columns_names)\n    input_files_columns = \\\n        utils.find_list_items_matching_prefix(updated_columns_names, prefix)\n\n    if not input_files_columns:\n        return None\n\n    if len(input_files_columns) > 2:\n        logging.warning(\n            f\"Compactions have more than 2 'files_' columns. Including only \"\n            f\"the first 2. columns_names:{columns_names}\")\n        for to_remove in input_files_columns[2:]:\n            updated_columns_names.remove(to_remove)\n        input_files_columns = input_files_columns[:2]\n\n    def extract_level(column_idx):\n        level_str = columns_names[column_idx][prefix_len:]\n        try:\n            return int(level_str)\n        except ValueError:\n            logging.warning(f\"Unexpected column name (\"\n                            f\"{columns_names[column_idx]}\")\n            return None\n\n    first_column_idx = updated_columns_names.index(input_files_columns[0])\n    first_level = extract_level(first_column_idx)\n    if first_level is None:\n        return None\n    updated_columns_names[first_column_idx] = \"Input Level Files\"\n\n    second_level = None\n    if len(input_files_columns) > 1:\n        second_column_idx = updated_columns_names.index(input_files_columns[1])\n        if second_column_idx != first_column_idx+1:\n            # Currently, support only consecutive columns\n            logging.warning(\n                f\"non-consecutive file_<Level> columns ({columns_names})\")\n            return None\n\n        second_level = extract_level(second_column_idx)\n        if not second_level:\n            return None\n        updated_columns_names[second_column_idx] = \\\n            \"Input Files from Output Level\"\n    else:\n        updated_columns_names.insert(first_column_idx + 1,\n                                     \"Input Files from Output Level\")\n\n    return CompactionsCsvInputFilesInfo(\n        updated_columns_names=updated_columns_names,\n        first_column_idx=first_column_idx,\n        first_level=first_level,\n        second_level=second_level\n    )", "\n\ndef process_compactions_csv_header(columns_names):\n    # Assume that, in general, compactions potentially have 2 \"files_\" columns\n    # (They may have one, and, maybe more than 2)\n    # Name them:\n    # 1. The first: \"Input Level Files\"\n    # 2. The second: \"Input Files from Output Level\"\n    prefix = \"files_L\"\n    prefix_len = len(prefix)\n\n    updated_columns_names = copy.deepcopy(columns_names)\n    input_files_columns = \\\n        utils.find_list_items_matching_prefix(updated_columns_names, prefix)\n\n    if not input_files_columns:\n        return None\n\n    if len(input_files_columns) > 2:\n        logging.warning(\n            f\"Compactions have more than 2 'files_' columns. Including only \"\n            f\"the first 2. columns_names:{columns_names}\")\n        for to_remove in input_files_columns[2:]:\n            updated_columns_names.remove(to_remove)\n        input_files_columns = input_files_columns[:2]\n\n    def extract_level(column_idx):\n        level_str = columns_names[column_idx][prefix_len:]\n        try:\n            return int(level_str)\n        except ValueError:\n            logging.warning(f\"Unexpected column name (\"\n                            f\"{columns_names[column_idx]}\")\n            return None\n\n    first_column_idx = updated_columns_names.index(input_files_columns[0])\n    first_level = extract_level(first_column_idx)\n    if first_level is None:\n        return None\n    updated_columns_names[first_column_idx] = \"Input Level Files\"\n\n    second_level = None\n    if len(input_files_columns) > 1:\n        second_column_idx = updated_columns_names.index(input_files_columns[1])\n        if second_column_idx != first_column_idx+1:\n            # Currently, support only consecutive columns\n            logging.warning(\n                f\"non-consecutive file_<Level> columns ({columns_names})\")\n            return None\n\n        second_level = extract_level(second_column_idx)\n        if not second_level:\n            return None\n        updated_columns_names[second_column_idx] = \\\n            \"Input Files from Output Level\"\n    else:\n        updated_columns_names.insert(first_column_idx + 1,\n                                     \"Input Files from Output Level\")\n\n    return CompactionsCsvInputFilesInfo(\n        updated_columns_names=updated_columns_names,\n        first_column_idx=first_column_idx,\n        first_level=first_level,\n        second_level=second_level\n    )", "\n\ndef get_compactions_csv(compactions_monitor):\n    # return get_flow_events_csv(events_mngr, FlowType.COMPACTION)\n    f = io.StringIO()\n    writer = csv.writer(f)\n\n    jobs = compactions_monitor.get_finished_jobs()\n    if not jobs:\n        return None\n\n    updated_header_columns_info = None\n    for job_id, job_info in jobs.items():\n        # Skipping incomplete jobs\n        if not job_info.has_finished():\n            logging.info(\"Compaction job hasn't finished, Not including in \"\n                         \"csv (skipping).\\n{job_info}\")\n            continue\n\n        start_event = job_info.start_event\n        finish_event = job_info.finish_event\n\n        job_info_dict = {}\n        if job_info.pre_finish_info:\n            job_info_dict = job_info.pre_finish_info.as_dict()\n\n        job_info_dict = utils.unify_dicts(job_info_dict,\n                                          start_event.get_event_data_dict(),\n                                          favor_first=True)\n        job_info_dict = \\\n            utils.unify_dicts(job_info_dict,\n                              finish_event.get_event_data_dict(),\n                              favor_first=True)\n\n        fields_to_del = [EventField.CF_NAME,\n                         EventField.TIME_MICROS,\n                         EventField.EVENT_TYPE,\n                         EventField.RECORDS_IN,\n                         EventField.RECORDS_DROPPED]\n        utils.delete_dict_keys(job_info_dict, fields_to_del)\n\n        columns_names = list(list(job_info_dict.keys()))\n        if updated_header_columns_info is None:\n            updated_header_columns_info = \\\n                process_compactions_csv_header(columns_names)\n            if updated_header_columns_info is None:\n                logging.warning(\"Failed processing CSV's header. Aborting\")\n                return None\n            curr_updated_columns_info = updated_header_columns_info\n\n            header_line = [\"Start Time\", \"Finish Time\", \"Column Family\"] + \\\n                updated_header_columns_info.updated_columns_names\n            writer.writerow(header_line)\n        else:\n            curr_updated_columns_info = \\\n                process_compactions_csv_header(columns_names)\n            if updated_header_columns_info.first_column_idx != \\\n                    curr_updated_columns_info.first_column_idx:\n                logging.warning(\n                    f\"Mismatching compaction job fields. \"\n                    f\"Skipping entry:{job_info}\")\n                continue\n\n        job_values = list(job_info_dict.values())\n        first_idx = curr_updated_columns_info.first_column_idx\n        first_level_str = f\"Level{curr_updated_columns_info.first_level}: \"\n        job_values[first_idx] = first_level_str + str(job_values[first_idx])\n\n        if curr_updated_columns_info.second_level is not None:\n            second_level_str = \\\n                f\"Level{curr_updated_columns_info.second_level}: \"\n            job_values[first_idx+1] = \\\n                second_level_str + str(job_values[first_idx+1])\n        else:\n            job_values.insert(curr_updated_columns_info.first_column_idx+1, \"\")\n        row = [job_info.get_start_time(),\n               job_info.get_finish_time(),\n               job_info.cf_name]\n        row += job_values\n        writer.writerow(row)\n\n    if updated_header_columns_info is None:\n        return None\n\n    return f.getvalue()", "\n\ndef get_flushes_csv(cfs_names, events_mngr):\n    return get_flow_events_csv(cfs_names, events_mngr, FlowType.FLUSH)\n\n\ndef generate_counters_csv(mngr, output_folder, report_to_console):\n    counters_csv = get_counters_csv(mngr)\n\n    if counters_csv:\n        counters_csv_path = \\\n            utils.get_counters_csv_file_path(output_folder)\n        with open(counters_csv_path, \"w\") as f:\n            f.write(counters_csv)\n        msg_start = \"Counters CSV Is in \"\n        utils.print_msg(\n            f\"{msg_start}{counters_csv_path}\", report_to_console,\n            f\"{msg_start}{counters_csv_path.as_uri()}\")\n        return counters_csv_path\n    else:\n        utils.print_msg(\"No Counters to report\", report_to_console)\n        return None", "\n\ndef generate_human_readable_histograms_csv(mngr, output_folder,\n                                           report_to_console):\n    histograms_csv = \\\n        get_human_readable_histogram_csv(mngr)\n    if not histograms_csv:\n        utils.print_msg(\"No Counters Histograms to report\", report_to_console)\n        return None\n\n    histograms_csv_file_name = \\\n        utils. \\\n        get_human_readable_histograms_csv_file_path(output_folder)\n    with open(histograms_csv_file_name, \"w\") as f:\n        f.write(histograms_csv)\n    msg_start = \"Human Readable Counters Histograms CSV Is in \"\n    utils.print_msg(\n        f\"{msg_start}{histograms_csv_file_name}\", report_to_console,\n        f\"{msg_start}{histograms_csv_file_name.as_uri()}\")\n    return histograms_csv_file_name", "\n\ndef generate_tools_histograms_csv(mngr, output_folder, report_to_console):\n    histograms_csv = get_tools_histogram_csv(mngr)\n    if not histograms_csv:\n        logging.info(\"No Counters Histograms to report\")\n        return None\n\n    histograms_csv_file_name = \\\n        utils.get_tools_histograms_csv_file_path(output_folder)\n    with open(histograms_csv_file_name, \"w\") as f:\n        f.write(histograms_csv)\n    logging.info(f\"Tools Counters Histograms CSV Is in\"\n                 f\" {histograms_csv_file_name}\")\n    return histograms_csv_file_name", "\n\ndef generate_histograms_csv(mngr, output_folder, report_to_console):\n    human_readable_csv_file_path = \\\n        generate_human_readable_histograms_csv(\n            mngr, output_folder, report_to_console)\n    if human_readable_csv_file_path is None:\n        return None, None\n\n    tools_csv_file_path = generate_tools_histograms_csv(\n        mngr, output_folder, report_to_console)\n\n    return human_readable_csv_file_path, tools_csv_file_path", "\n\ndef generate_compactions_stats_csv(compaction_stats_mngr, output_folder,\n                                   report_to_console):\n    compaction_stats_csv = get_compaction_stats_csv(compaction_stats_mngr)\n    if compaction_stats_csv is None:\n        utils.print_msg(\"No Compaction Stats to report\", report_to_console)\n        return None\n\n    compactions_stats_csv_path = \\\n        utils.get_compactions_stats_csv_file_path(output_folder)\n    with open(compactions_stats_csv_path, \"w\") as f:\n        f.write(compaction_stats_csv)\n    msg_start = \"Compactions Stats CSV Is in \"\n    utils.print_msg(\n        f\"{msg_start}{compactions_stats_csv_path}\", report_to_console,\n        f\"{msg_start}{compactions_stats_csv_path.as_uri()}\")\n    return compactions_stats_csv_path", "\n\ndef generate_compactions_csv(\n        compactions_monitor, output_folder, report_to_console):\n    compaction_csv = get_compactions_csv(compactions_monitor)\n    if compaction_csv is None:\n        utils.print_msg(\"No Compactions to report\", report_to_console)\n        return None\n\n    compactions_csv_path = \\\n        utils.get_compactions_csv_file_path(output_folder)\n    with open(compactions_csv_path, \"w\") as f:\n        f.write(compaction_csv)\n    msg_start = \"Compactions CSV Is in \"\n    utils.print_msg(\n        f\"{msg_start}{compactions_csv_path}\", report_to_console,\n        f\"{msg_start}{compactions_csv_path.as_uri()}\")\n    return compactions_csv_path", "\n\ndef generate_flushes_csv(\n        cfs_names, events_mngr, output_folder, report_to_console):\n    flushes_csv = get_flushes_csv(cfs_names, events_mngr)\n    if flushes_csv is None:\n        utils.print_msg(\"No Flushes to report\", report_to_console)\n        return None\n\n    flushes_csv_path = utils.get_flushes_csv_file_path(output_folder)\n    with open(flushes_csv_path, \"w\") as f:\n        f.write(flushes_csv)\n    msg_start = \"Flushes CSV Is in \"\n    utils.print_msg(\n        f\"{msg_start}{flushes_csv_path}\", report_to_console,\n        f\"{msg_start}{flushes_csv_path.as_uri()}\")\n    return flushes_csv_path", ""]}
{"filename": "regexes.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\n#\n# BASIC TYPES AND CONSTRUCTS\n#\nWS = r'\\s*'", "#\nWS = r'\\s*'\nINT = r'[\\d]+'\nINT_C = r\"([\\d]+)\"\nFLOAT = r'[-+]?(?:\\d+(?:[.,]\\d*)?|[.,]\\d+)(?:[eE][-+]?\\d+)?'\nFLOAT_C = fr'({FLOAT})'\nNUM_UNIT = r'(K|M|G)'\nBYTES_UNIT = r'(KB|MB|GB|TB)'\nNUM_WITH_UNIT = fr'{FLOAT_C}\\s*{NUM_UNIT}?\\s*'\nNUM_WITH_UNIT_ONLY = fr\"{NUM_WITH_UNIT}\\Z\"", "NUM_WITH_UNIT = fr'{FLOAT_C}\\s*{NUM_UNIT}?\\s*'\nNUM_WITH_UNIT_ONLY = fr\"{NUM_WITH_UNIT}\\Z\"\nNUM_BYTES_WITH_UNIT = fr'{FLOAT_C}\\s*{BYTES_UNIT}?\\s*'\nNUM_BYTES_WITH_UNIT_ONLY = fr'{NUM_BYTES_WITH_UNIT}\\Z'\nCF_NAME_OLD = r'\\[(?P<cf>[\\w\\]]*)\\]'\nCF_NAME = r'\\[(?P<cf>.*)\\]'\nCF_ID = fr'\\(ID\\s+(?P<cf_id>{INT})\\)'\nJOB_ID = r\"\\[JOB (?P<job_id>[\\d+]+)\\]\"\nPOINTER_NC = r'0x[\\dA-Fa-f]+'\nPOINTER = fr'({POINTER_NC})'", "POINTER_NC = r'0x[\\dA-Fa-f]+'\nPOINTER = fr'({POINTER_NC})'\nSANITIZED_POINTER = fr\"Pointer \\((?P<ptr>{POINTER_NC})\\)\"\n\n#\n# LOG ENTRY PARTS REGEXES\n#\nEMPTY_LINE = r'^\\s*$'\nTIMESTAMP = r'\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}'\nORIG_TIME = fr\"\\(Original Log Time ({TIMESTAMP})\\)\"", "TIMESTAMP = r'\\d{4}/\\d{2}/\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d{6}'\nORIG_TIME = fr\"\\(Original Log Time ({TIMESTAMP})\\)\"\n# Example: [/flush_job.cc:858]\nCODE_POS = r\"\\[\\/?.*?\\.[\\w:]+:\\d+\\]\"\n\nSTART_LINE_WITH_WARN_PARTS = \\\n    fr\"({TIMESTAMP}) (\\w+)\\s*(?:{ORIG_TIME})?\\s*\" \\\n    fr\"\\[(WARN|ERROR|FATAL)\\]\\s*({CODE_POS})?(.*)\"\nSTART_LINE_PARTS = \\\n    fr\"({TIMESTAMP}) (\\w+)\\s*\" \\", "START_LINE_PARTS = \\\n    fr\"({TIMESTAMP}) (\\w+)\\s*\" \\\n    fr\"(?:{ORIG_TIME})?\\s*({CODE_POS})?(.*)\"\n\n# A log entry that has the cf-name followed by the job id\n# Example [column_family_name_000001] [JOB 31]\nCF_WITH_JOB_ID = fr\"{CF_NAME_OLD}\\s*{JOB_ID}\"\n\n#\n# METADATA REGEXES", "#\n# METADATA REGEXES\nDB_SESSION_ID = r\"DB Session ID:\\s*([0-9A-Z]+)\"\n\n# <product name> version: <version number>\nPRODUCT_AND_VERSION = r\"(\\S+) version: ([0-9.]+)\"\n\nGIT_HASH_LINE = r\"Git sha \\s*(\\S+)\"\n\n#", "\n#\n# OPTIONS REGEXES\nOPTION_LINE = r\"\\s*Options\\.(\\S+)\\s*:\\s*(.+)?\"\nDB_WIDE_WBM_PSEUDO_OPTION_LINE = r\"\\s*wbm\\.(\\S+)\\s*:\\s*(.+)\"\n\n# example:\n# --------------- Options for column family [default]:\n# In case of a match the result will be [<column-family name>] (List)\nCF_OPTIONS_START = \\", "# In case of a match the result will be [<column-family name>] (List)\nCF_OPTIONS_START = \\\n    r\"--------------- Options for column family \\[(.*)\\]:.*\"\n\nTABLE_OPTIONS_START_LINE = \\\n    r\"^\\s*table_factory options:\\s*(\\S+)\\s*:(.*)\"\nTABLE_OPTIONS_CONTINUATION_LINE = r\"^\\s*(\\S+)\\s*:(.*)\"\n\n#\n# EVENTS", "#\n# EVENTS\n#\nPREAMBLE_EVENT = r\"\\[(.*?)\\] \\[JOB ([0-9]+)\\]\\s*(.*)\"\n\n# [column_family_name_000018] [JOB 38] Flushing memtable with next log file: 5\n# Capturing: cf_name, job_id, wal-id\nFLUSH_EVENT_PREAMBLE = \\\n    fr\"^{WS}{CF_NAME}{WS}{JOB_ID}{WS}Flushing memtable \" \\\n    fr\"with next log file:{WS}(?P<wal_id>{INT})\"", "    fr\"^{WS}{CF_NAME}{WS}{JOB_ID}{WS}Flushing memtable \" \\\n    fr\"with next log file:{WS}(?P<wal_id>{INT})\"\n\n# [default] [JOB 13] Compacting 1@1 + 5@2 files to L2, score 1.63\n# Capturing: cf_name, job_id\nCOMPACTION_EVENT_PREAMBLE = \\\n    fr\"^{WS}{CF_NAME}{WS}{JOB_ID}{WS}Compacting.*score\"\n\nEVENT = r\"\\s*EVENT_LOG_v1\"\n", "EVENT = r\"\\s*EVENT_LOG_v1\"\n\n\nWRITE_DELAY_WARN_MSG = fr\"{CF_NAME_OLD}{WS}Stalling writes\"\nWRITE_STOP_WARN_MSG = fr\"{CF_NAME_OLD}{WS}Stopping writes\"\n\n#\n# STATISTICS RELATED\n#\nDUMP_STATS_STR = r'------- DUMPING STATS -------'", "#\nDUMP_STATS_STR = r'------- DUMPING STATS -------'\nDB_STATS = fr'^{WS}\\*\\* DB Stats \\*\\*{WS}$'\nCOMPACTION_STATS = fr'^{WS}\\*\\* Compaction Stats{WS}{CF_NAME}{WS}\\*\\*{WS}$'\n\nFILE_READ_LATENCY_STATS = \\\n    fr'^{WS}\\*\\* File Read Latency Histogram By Level{WS}{CF_NAME}' \\\n    fr'{WS}\\*\\*{WS}$'\n\nLEVEL_READ_LATENCY_LEVEL_LINE = \\", "\nLEVEL_READ_LATENCY_LEVEL_LINE = \\\n    fr'\\*\\* Level {INT_C} read latency histogram \\(micros\\):'\n\n# Count: 26687513 Average: 3.1169  StdDev: 34.39\nLEVEL_READ_LATENCY_STATS_LINE1 = \\\n    fr\"Count:{WS}{INT_C}{WS}Average:{WS}{FLOAT_C}{WS}StdDev:{WS}{FLOAT_C}\"\n\n# Min: 0  Median: 2.4427  Max: 56365\nLEVEL_READ_LATENCY_STATS_LINE2 = \\", "# Min: 0  Median: 2.4427  Max: 56365\nLEVEL_READ_LATENCY_STATS_LINE2 = \\\n    fr\"Min:{WS}{INT_C}{WS}Median:{WS}{FLOAT_C}{WS}Max:{WS}{INT_C}\"\n\nSTATS_COUNTERS_AND_HISTOGRAMS = r'^\\s*STATISTICS:\\s*$'\n\n# Uptime(secs): 603.0 total, 600.0 interval\nUPTIME_STATS_LINE = \\\n    fr'^{WS}Uptime\\(secs\\):{WS}(?P<total>{FLOAT}){WS}total,' \\\n    fr'{WS}(?P<interval>{FLOAT}){WS}interval'", "    fr'^{WS}Uptime\\(secs\\):{WS}(?P<total>{FLOAT}){WS}total,' \\\n    fr'{WS}(?P<interval>{FLOAT}){WS}interval'\n\n#\n# BLOCK CACHE STATS REGEXES\n#\nCACHE_ID = r\"(\\S+)\"\n\n# LRUCache@0x556ae9ce9770#27411\n# <name>@<ptr>#<process-id>", "# LRUCache@0x556ae9ce9770#27411\n# <name>@<ptr>#<process-id>\nCACHE_ID_PARTS = fr\"(?P<name>.*?)@(?P<ptr>{POINTER_NC})#(?P<pid>{INT})\"\n\nBLOCK_CACHE_STATS_START = \\\n    fr'Block cache {CACHE_ID} capacity: {FLOAT_C} {BYTES_UNIT} '\nBLOCK_CACHE_ENTRY_STATS = \\\n    r\"Block cache entry stats\\(count,size,portion\\): (.*)\"\nBLOCK_CACHE_CF_ENTRY_STATS = fr\"Block cache {CF_NAME} (.*)\"\nBLOCK_CACHE_ENTRY_ROLES_NAMES = r\"([A-Za-z]+)\\(\"", "BLOCK_CACHE_CF_ENTRY_STATS = fr\"Block cache {CF_NAME} (.*)\"\nBLOCK_CACHE_ENTRY_ROLES_NAMES = r\"([A-Za-z]+)\\(\"\nBLOCK_CACHE_ENTRY_ROLES_STATS = r\"[a-zA-Z]+\\(([^\\)]+?)\\)\"\nSTATS_COUNTER = fr\"^{WS}([\\w\\.]+){WS}COUNT{WS}:{WS} {INT_C}{WS}$\"\n\nSTATS_HISTOGRAM = \\\n    fr'^\\s*([\\w\\.]+) P50 : {FLOAT_C} P95 : {FLOAT_C} P99 : {FLOAT_C} ' \\\n    fr'P100 : {FLOAT_C} COUNT : {INT_C} SUM : {INT_C}'\nSTATS_HISTOGRAM = \\\n    fr'^{WS}(?P<name>[\\w\\.]+){WS}P50{WS}:{WS}(?P<P50>{FLOAT})' \\", "STATS_HISTOGRAM = \\\n    fr'^{WS}(?P<name>[\\w\\.]+){WS}P50{WS}:{WS}(?P<P50>{FLOAT})' \\\n    fr'{WS}P95{WS}:{WS}(?P<P95>{FLOAT}){WS}P99{WS}:{WS}(?P<P99>{FLOAT})' \\\n    fr'{WS}P100{WS}:{WS}(?P<P100>{FLOAT})' \\\n    fr'{WS}COUNT{WS}:{WS}(?P<count>{INT}){WS}SUM{WS}:{WS}(?P<sum>{INT})'\n\nBLOB_STATS_LINE = \\\n    fr'Blob file count: ([\\d]+), total size: {FLOAT_C} GB, ' \\\n    fr'garbage size: {FLOAT_C} GB, space amp: {FLOAT_C}'\n", "    fr'garbage size: {FLOAT_C} GB, space amp: {FLOAT_C}'\n\nSUPPORT_INFO_START_LINE = r'\\s*Compression algorithms supported:\\s*$'\n\nVERSION = r'(\\d+)\\.(\\d+)\\.?(\\d+)?'\n\n# Interval stall: 00:00:0.000 H:M:S, 0.0 percent\nDB_WIDE_INTERVAL_STALL = \\\n    fr\"Interval stall: (\\d+):(\\d+):(\\d+)\\.(\\d+) H:M:S, {FLOAT_C} percent\"\n", "    fr\"Interval stall: (\\d+):(\\d+):(\\d+)\\.(\\d+) H:M:S, {FLOAT_C} percent\"\n\n#  Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent\nDB_WIDE_CUMULATIVE_STALL = \\\n    fr\"Cumulative stall: (\\d+):(\\d+):(\\d+)\\.(\\d+) H:M:S, {FLOAT_C} percent\"\n\n# Cumulative writes: 819K writes, 1821M keys, 788K commit groups, 1.0 writes per commit group, ingest: 80.67 GB, 68.66 MB/s # noqa\nDB_WIDE_CUMULATIVE_WRITES = \\\n    fr\"Cumulative writes:\\s*{NUM_WITH_UNIT} writes,\\s*{NUM_WITH_UNIT} keys.*\"\\\n    fr\"ingest: {FLOAT_C}\\s*GB,\\s*{FLOAT_C}\\s*MB/s\"", "    fr\"Cumulative writes:\\s*{NUM_WITH_UNIT} writes,\\s*{NUM_WITH_UNIT} keys.*\"\\\n    fr\"ingest: {FLOAT_C}\\s*GB,\\s*{FLOAT_C}\\s*MB/s\"\n\nCF_STALLS_LINE_START = \"Stalls(count):\"\nCF_STALLS_COUNT_AND_REASON = r\"\\b(\\d+) (.*?),\"\nCF_STALLS_INTERVAL_COUNT = r\".*interval (\\d+) total count$\"\n\n# 2022/12/17-11:12:17.399948 38990 [/version_set.cc:4980] Column family [column_family_name_000009] (ID 9), log number is 41576 # noqa\nRECOVERED_CF = \\\n    fr\"Column family {CF_NAME}\\s*{CF_ID},{WS}log number is (?P<log_num>{INT})\"", "RECOVERED_CF = \\\n    fr\"Column family {CF_NAME}\\s*{CF_ID},{WS}log number is (?P<log_num>{INT})\"\n\n# 2022/12/17-06:02:37.695926 24554 [/db_impl/db_impl.cc:2799] Created column family [column_family_name_000001] (ID 1) # noqa\nCREATE_CF = fr\"Created column family {CF_NAME}\\s*{CF_ID}\"\n\nDROP_CF = fr\"Dropped column family with id {INT_C}\\s*\"\n\n\nROCKSDB_BASELINE_LOG_FILE = r\"LOG-rocksdb-(\\d+\\.\\d+\\.?\\d*)\"", "\nROCKSDB_BASELINE_LOG_FILE = r\"LOG-rocksdb-(\\d+\\.\\d+\\.?\\d*)\"\nSPEEDB_BASELINE_LOG_FILE = r\"LOG-speedb-(\\d+\\.\\d+\\.?\\d*)\"\n\n# 2021/05/04-20:54:25.385487 7fa4245ff700 [/compaction/compaction_job.cc:1762] [default] [JOB 38680] Compacting 1@5 + 1@6 files to L6, score 0.9 # noqa\nCOMPACTION_BEFORE_SCORE_LINE = \\\n    fr\"{CF_NAME}\\s*{JOB_ID}\\s*Compacting .*\" \\\n    fr\"files to L{INT_C},\\s*score\\s*{FLOAT_C}\"\n\n# A line in a compaction job preceding the compaction_finished event", "\n# A line in a compaction job preceding the compaction_finished event\n# [default] compacted to: files[4 7 45 427 822 0 0] max score 1.63,\n# MB/sec: 450.9 rd, 450.1 wr, level 1, files in(4, 3) out(7 +0 blob)\n# MB in(224.7, 193.2 +0.0 blob) out(417.0 +0.0 blob), read-write-amplify(3.7)\n# write-amplify(1.9) OK, records in: 424286, records dropped: 789\n# output_compression: NoCompression\n# The following regex matches this line and extracts the following fields:\n# 0: cf name\n# 1: MB/sec - <Rate> rd", "# 0: cf name\n# 1: MB/sec - <Rate> rd\n# 2: MB/sec - <Rate> wr\n# 3: read-write-amplify\n# 4: write-amplify\n# 5: records in\n# 6: records dropped\n#\nCOMPACTION_JOB_FINISH_STATS_LINE = \\\n    fr\"{CF_NAME_OLD}.*,\\s*MB\\/sec:\\s*{FLOAT_C}\\s*rd,\" \\", "COMPACTION_JOB_FINISH_STATS_LINE = \\\n    fr\"{CF_NAME_OLD}.*,\\s*MB\\/sec:\\s*{FLOAT_C}\\s*rd,\" \\\n    fr\"\\s*{FLOAT_C}\\s*wr,.*read-write-amplify\\({FLOAT_C}\\)\\s*write-amplify\\(\" \\\n    fr\"{FLOAT_C}\\).*records in:\\s*{INT_C},\\s*records dropped:\\s*{INT_C}\"\n"]}
{"filename": "cache_utils.py", "chunked_list": ["import logging\nfrom dataclasses import dataclass\n\nimport db_files\nfrom counters import CountersMngr\nfrom db_files import DbFilesMonitor\nfrom db_options import RAW_NULL_PTR, DatabaseOptions, \\\n    sanitized_to_raw_ptr_value\n\n\ndef get_cf_raw_cache_ptr_str(cf_name, db_options):\n    assert isinstance(db_options, DatabaseOptions)\n\n    sanitized_cache_ptr = \\\n        db_options.get_cf_table_option(cf_name, \"block_cache\")\n    return sanitized_to_raw_ptr_value(sanitized_cache_ptr)", "\n\ndef get_cf_raw_cache_ptr_str(cf_name, db_options):\n    assert isinstance(db_options, DatabaseOptions)\n\n    sanitized_cache_ptr = \\\n        db_options.get_cf_table_option(cf_name, \"block_cache\")\n    return sanitized_to_raw_ptr_value(sanitized_cache_ptr)\n\n\ndef does_cf_has_cache(cf_name, db_options):\n    raw_ptr_str = db_options.get_cf_table_raw_ptr_str(cf_name, \"block_cache\")\n    return raw_ptr_str is not None and raw_ptr_str != RAW_NULL_PTR", "\n\ndef does_cf_has_cache(cf_name, db_options):\n    raw_ptr_str = db_options.get_cf_table_raw_ptr_str(cf_name, \"block_cache\")\n    return raw_ptr_str is not None and raw_ptr_str != RAW_NULL_PTR\n\n\ndef get_cache_id(cf_name, db_options):\n    assert isinstance(db_options, DatabaseOptions)\n\n    cache_ptr = db_options.get_cf_table_raw_ptr_str(cf_name, \"block_cache\")\n    cache_name = db_options.get_cf_table_option(cf_name, \"block_cache_name\")\n\n    if cache_ptr is None or cache_name is None:\n        return None\n\n    return f\"{cache_name}@{cache_ptr}\"", "\n\n@dataclass\nclass CfCacheOptions:\n    cf_name: str\n    cache_id: str = None\n    cache_index_and_filter_blocks: bool = None\n    cache_capacity_bytes: int = 0\n    num_shard_bits: int = 0\n", "\n\n@dataclass\nclass CfSpecificCacheOptions:\n    cache_index_and_filter_blocks: bool = None\n\n\n@dataclass\nclass CacheOptions:\n    cache_capacity_bytes: int = 0\n    num_shard_bits: int = 0\n    shard_size_bytes: int = 0\n    # {<cf-name>: CfSpecificCacheOptions}\n    cfs_specific_options: dict = None", "class CacheOptions:\n    cache_capacity_bytes: int = 0\n    num_shard_bits: int = 0\n    shard_size_bytes: int = 0\n    # {<cf-name>: CfSpecificCacheOptions}\n    cfs_specific_options: dict = None\n\n\ndef collect_cf_cache_options(cf_name, db_options):\n    assert isinstance(db_options, DatabaseOptions)\n\n    if not does_cf_has_cache(cf_name, db_options):\n        return None\n\n    cache_id = get_cache_id(cf_name, db_options)\n    cache_index_and_filter_blocks = \\\n        db_options.get_cf_table_option(\n            cf_name, \"cache_index_and_filter_blocks\")\n    cache_capacity_bytes_str = \\\n        db_options.get_cf_table_option(cf_name, \"block_cache_capacity\")\n    num_shard_bits_str = \\\n        db_options.get_cf_table_option(cf_name, \"block_cache_num_shard_bits\")\n\n    if cache_id is None or cache_index_and_filter_blocks is None or \\\n            cache_capacity_bytes_str is None or num_shard_bits_str is None:\n        logging.warning(\n            f\"{cf_name} has cache but its cache options are \"\n            f\"corrupted. cache_id:{cache_id}, \"\n            f\"cache_index_and_filter_blocks:{cache_index_and_filter_blocks}\"\n            f\"cache_capacity_bytes_str:{cache_capacity_bytes_str}\"\n            f\"num_shard_bits_str:{num_shard_bits_str}\")\n        return None\n\n    options = CfCacheOptions(cf_name)\n    options.cache_id = cache_id\n    options.cache_index_and_filter_blocks = cache_index_and_filter_blocks\n    options.cache_capacity_bytes = int(cache_capacity_bytes_str)\n    options.num_shard_bits = int(num_shard_bits_str)\n\n    return options", "def collect_cf_cache_options(cf_name, db_options):\n    assert isinstance(db_options, DatabaseOptions)\n\n    if not does_cf_has_cache(cf_name, db_options):\n        return None\n\n    cache_id = get_cache_id(cf_name, db_options)\n    cache_index_and_filter_blocks = \\\n        db_options.get_cf_table_option(\n            cf_name, \"cache_index_and_filter_blocks\")\n    cache_capacity_bytes_str = \\\n        db_options.get_cf_table_option(cf_name, \"block_cache_capacity\")\n    num_shard_bits_str = \\\n        db_options.get_cf_table_option(cf_name, \"block_cache_num_shard_bits\")\n\n    if cache_id is None or cache_index_and_filter_blocks is None or \\\n            cache_capacity_bytes_str is None or num_shard_bits_str is None:\n        logging.warning(\n            f\"{cf_name} has cache but its cache options are \"\n            f\"corrupted. cache_id:{cache_id}, \"\n            f\"cache_index_and_filter_blocks:{cache_index_and_filter_blocks}\"\n            f\"cache_capacity_bytes_str:{cache_capacity_bytes_str}\"\n            f\"num_shard_bits_str:{num_shard_bits_str}\")\n        return None\n\n    options = CfCacheOptions(cf_name)\n    options.cache_id = cache_id\n    options.cache_index_and_filter_blocks = cache_index_and_filter_blocks\n    options.cache_capacity_bytes = int(cache_capacity_bytes_str)\n    options.num_shard_bits = int(num_shard_bits_str)\n\n    return options", "\n\ndef calc_shard_size_bytes(cache_capacity_bytes, num_shard_bits):\n    num_shards = 2 ** num_shard_bits\n    return int((cache_capacity_bytes + (num_shards - 1)) / num_shards)\n\n\ndef collect_cache_options_info(db_options):\n    assert isinstance(db_options, DatabaseOptions)\n\n    # returns {<cache-id>: [<cf-cache-options>]\n    cache_options = dict()\n\n    cfs_names = db_options.get_cfs_names()\n    for cf_name in cfs_names:\n        cf_cache_options = collect_cf_cache_options(cf_name, db_options)\n        if cf_cache_options is None:\n            continue\n\n        cache_id = cf_cache_options.cache_id\n        cfs_specific_options =\\\n            CfSpecificCacheOptions(\n                cf_cache_options.cache_index_and_filter_blocks)\n\n        if cache_id not in cache_options:\n            shard_size_bytes = \\\n                calc_shard_size_bytes(cf_cache_options.cache_capacity_bytes,\n                                      cf_cache_options.num_shard_bits)\n\n            cfs_specific_options =\\\n                {cf_cache_options.cf_name: cfs_specific_options}\n            cache_options[cache_id] = \\\n                CacheOptions(\n                    cache_capacity_bytes=cf_cache_options.cache_capacity_bytes,\n                    num_shard_bits=cf_cache_options.num_shard_bits,\n                    shard_size_bytes=shard_size_bytes,\n                    cfs_specific_options=cfs_specific_options)\n        else:\n            assert cache_options[cache_id].cache_capacity_bytes == \\\n                   cf_cache_options.cache_capacity_bytes\n            assert cache_options[cache_id].num_shard_bits == \\\n                   cf_cache_options.num_shard_bits\n            cache_options[cache_id].cfs_specific_options[\n                cf_cache_options.cf_name] = cfs_specific_options\n\n    if not cache_options:\n        return None\n\n    return cache_options", "\n\n@dataclass\nclass CacheCounters:\n    cache_add: int = 0\n    cache_miss: int = 0\n    cache_hit: int = 0\n    index_add: int = 0\n    index_miss: int = 0\n    index_hit: int = 0\n    filter_add: int = 0\n    filter_miss: int = 0\n    filter_hit: int = 0\n    data_add: int = 0\n    data_miss: int = 0\n    data_hit: int = 0", "\n\ndef collect_cache_counters(counters_mngr):\n    assert isinstance(counters_mngr, CountersMngr)\n\n    if not counters_mngr.does_have_counters_values():\n        logging.info(\"Can't collect cache counters. No counters available\")\n        return None\n\n    cache_counters_names = {\n        \"cache_miss\": \"rocksdb.block.cache.miss\",\n        \"cache_hit\": \"rocksdb.block.cache.hit\",\n        \"cache_add\": \"rocksdb.block.cache.add\",\n        \"index_miss\": \"rocksdb.block.cache.index.miss\",\n        \"index_hit\": \"rocksdb.block.cache.index.hit\",\n        \"index_add\": \"rocksdb.block.cache.index.add\",\n        \"filter_miss\": \"rocksdb.block.cache.filter.miss\",\n        \"filter_hit\": \"rocksdb.block.cache.filter.hit\",\n        \"filter_add\": \"rocksdb.block.cache.filter.add\",\n        \"data_miss\": \"rocksdb.block.cache.data.miss\",\n        \"data_hit\": \"rocksdb.block.cache.data.hit\",\n        \"data_add\": \"rocksdb.block.cache.data.add\"}\n\n    counters = CacheCounters()\n\n    for field_name, counter_name in cache_counters_names.items():\n        counter_value = counters_mngr.get_last_counter_value(counter_name)\n        setattr(counters, field_name, counter_value)\n\n    return counters", "\n\n@dataclass\nclass CacheIdInfo:\n    options: CacheOptions = None\n    files_stats: db_files.CfsFilesStats = None\n\n\n@dataclass\nclass CacheStats:\n    # {<cache-id>: CacheIdInfo}\n    per_cache_id_info: dict = None\n    global_cache_counters: CacheCounters = None", "@dataclass\nclass CacheStats:\n    # {<cache-id>: CacheIdInfo}\n    per_cache_id_info: dict = None\n    global_cache_counters: CacheCounters = None\n\n\ndef calc_block_cache_stats(db_options: object, counters_mngr,\n                           files_monitor) -> object:\n    assert isinstance(db_options, DatabaseOptions)\n    assert isinstance(counters_mngr, CountersMngr)\n    assert isinstance(files_monitor, DbFilesMonitor)\n\n    stats = CacheStats()\n\n    cache_options = collect_cache_options_info(db_options)\n    if cache_options is None:\n        return None\n\n    stats.per_cache_id_info = dict()\n    for cache_id, options in cache_options.items():\n        assert isinstance(options, CacheOptions)\n\n        cache_cfs_names = list(options.cfs_specific_options.keys())\n        cache_files_stats =\\\n            db_files.calc_cf_files_stats(cache_cfs_names, files_monitor)\n        if not cache_files_stats:\n            return None\n\n        assert isinstance(cache_files_stats, db_files.CfsFilesStats)\n\n        stats.per_cache_id_info[cache_id] = \\\n            CacheIdInfo(options=options, files_stats=cache_files_stats)\n\n    cache_counters = collect_cache_counters(counters_mngr)\n    if cache_counters:\n        stats.global_cache_counters = cache_counters\n\n    return stats", ""]}
{"filename": "test/test_db_options.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport pytest\n\nimport db_options as db_opts\nimport utils", "import db_options as db_opts\nimport utils\n\ndefault = 'default'\ncf1 = 'cf1'\ncf2 = 'cf2'\ncf3 = 'cf3'\n\nEMPTY_FULL_NAMES_OPTIONS_DICT = db_opts.FullNamesOptionsDict()\nDB_WIDE_CF_NAME = utils.NO_CF", "EMPTY_FULL_NAMES_OPTIONS_DICT = db_opts.FullNamesOptionsDict()\nDB_WIDE_CF_NAME = utils.NO_CF\nCF_SECTION_TYPE = db_opts.SectionType.CF\nTABLE_SECTION_TYPE = db_opts.SectionType.TABLE_OPTIONS\n\n\nbaseline_options = {\n    'DBOptions.stats_dump_freq_sec': {utils.NO_CF: '20'},\n    'DBOptions.enable_pipelined_write': {utils.NO_CF: '0'},\n    'CFOptions.write_buffer_size': {", "    'DBOptions.enable_pipelined_write': {utils.NO_CF: '0'},\n    'CFOptions.write_buffer_size': {\n        default: '1024000',\n        cf1: '128000',\n        cf2: '128000000'\n    },\n    'TableOptions.BlockBasedTable.index_type': {cf2: '1'},\n    'TableOptions.BlockBasedTable.format_version': {cf1: '4', cf2: '5'},\n    'TableOptions.BlockBasedTable.block_size': {cf1: '4096', cf2: '4096'},\n    'DBOptions.use_fsync': {utils.NO_CF: 'true'},", "    'TableOptions.BlockBasedTable.block_size': {cf1: '4096', cf2: '4096'},\n    'DBOptions.use_fsync': {utils.NO_CF: 'true'},\n    'DBOptions.max_log_file_size':\n        {utils.NO_CF: '128000000'},\n    'DBOptions.ptr_option1': {utils.NO_CF: '0x12345678'}\n}\nnew_options = {\n    'bloom_bits': {utils.NO_CF: '4'},\n    'DBOptions.enable_pipelined_write': {utils.NO_CF: 'True'},\n    'CFOptions.write_buffer_size': {", "    'DBOptions.enable_pipelined_write': {utils.NO_CF: 'True'},\n    'CFOptions.write_buffer_size': {\n        default: '128000000',\n        cf1: '128000',\n        cf3: '128000000'\n    },\n    'TableOptions.BlockBasedTable.checksum': {cf1: 'true'},\n    'TableOptions.BlockBasedTable.format_version': {cf1: '5', cf2: '5'},\n    'TableOptions.BlockBasedTable.block_size': {cf1: '4096', cf2: '4096'},\n    'DBOptions.use_fsync': {utils.NO_CF: 'true'},", "    'TableOptions.BlockBasedTable.block_size': {cf1: '4096', cf2: '4096'},\n    'DBOptions.use_fsync': {utils.NO_CF: 'true'},\n    'DBOptions.max_log_file_size': {utils.NO_CF: '0'},\n    'DBOptions.ptr_option1': {utils.NO_CF: '0x55555555'}\n}\n\nbaseline = db_opts.FullNamesOptionsDict(baseline_options)\nnew = db_opts.FullNamesOptionsDict(new_options)\n\n\ndef test_sanitized_to_raw_ptr_value():\n    assert db_opts.sanitized_to_raw_ptr_value(db_opts.SANITIZED_NULL_PTR) == \\\n           db_opts.RAW_NULL_PTR\n    assert db_opts.sanitized_to_raw_ptr_value(\"Pointer (0x1234)\") == '0x1234'\n    assert db_opts.sanitized_to_raw_ptr_value(\"Pointer (0xa0FAA55)\") == \\\n           '0xa0FAA55'", "\n\ndef test_sanitized_to_raw_ptr_value():\n    assert db_opts.sanitized_to_raw_ptr_value(db_opts.SANITIZED_NULL_PTR) == \\\n           db_opts.RAW_NULL_PTR\n    assert db_opts.sanitized_to_raw_ptr_value(\"Pointer (0x1234)\") == '0x1234'\n    assert db_opts.sanitized_to_raw_ptr_value(\"Pointer (0xa0FAA55)\") == \\\n           '0xa0FAA55'\n\n\ndef test_extract_section_type():\n    assert db_opts.SectionType.\\\n           extract_section_type(\"Version.option-name1\") == \"Version\"\n    assert db_opts.SectionType.\\\n           extract_section_type(\"DBOptions.option-name1\") == \"DBOptions\"\n    assert db_opts.SectionType.\\\n           extract_section_type(\"CFOptions.option-name1\") == \"CFOptions\"\n    assert db_opts.SectionType.extract_section_type(\n        \"TableOptions.BlockBasedTable.option-name1\") ==\\\n        \"TableOptions.BlockBasedTable\"\n\n    with pytest.raises(utils.ParsingError):\n        db_opts.SectionType.extract_section_type(\"Dummy.option-name1\")", "\n\ndef test_extract_section_type():\n    assert db_opts.SectionType.\\\n           extract_section_type(\"Version.option-name1\") == \"Version\"\n    assert db_opts.SectionType.\\\n           extract_section_type(\"DBOptions.option-name1\") == \"DBOptions\"\n    assert db_opts.SectionType.\\\n           extract_section_type(\"CFOptions.option-name1\") == \"CFOptions\"\n    assert db_opts.SectionType.extract_section_type(\n        \"TableOptions.BlockBasedTable.option-name1\") ==\\\n        \"TableOptions.BlockBasedTable\"\n\n    with pytest.raises(utils.ParsingError):\n        db_opts.SectionType.extract_section_type(\"Dummy.option-name1\")", "\n\ndef test_misc_option_name_utils():\n    # parse_full_option_name\n    assert db_opts.parse_full_option_name(\"Version.option-name1\") \\\n           == (\"Version\", \"option-name1\")\n    with pytest.raises(utils.ParsingError):\n        db_opts.SectionType.extract_section_type(\"Dummy.option-name1\")\n\n    # get_full_option_name\n    assert db_opts.get_full_option_name(\"DBOptions\", \"option1\") == \\\n           \"DBOptions.option1\"\n    with pytest.raises(utils.ParsingError):\n        db_opts.get_full_option_name(\"Dummy\", \"option-name1\")\n\n    # get_db_wide_full_option_name\n    assert db_opts.get_db_wide_full_option_name(\"option1\") == \\\n           \"DBOptions.option1\"\n\n    # extract_option_name\n    assert db_opts.extract_option_name(\"DBOptions.option1\") == \\\n           \"option1\"\n    with pytest.raises(utils.ParsingError):\n        db_opts.extract_option_name(\"Dummy.option-name1\")\n\n    # extract_db_wide_option_name\n    assert db_opts.extract_db_wide_option_name(\"DBOptions.option1\")\\\n           == \"option1\"\n    with pytest.raises(utils.ParsingError):\n        db_opts.extract_db_wide_option_name(\"CFOptions.option1\")\n\n    # get_cf_full_option_name\n    assert db_opts.get_cf_full_option_name(\"option1\") == \\\n           \"CFOptions.option1\"\n\n    # extract_db_wide_option_name\n    assert db_opts.extract_cf_option_name(\"CFOptions.option1\")\\\n           == \"option1\"\n    with pytest.raises(utils.ParsingError):\n        db_opts.extract_cf_option_name(\"DBOptions.option1\")\n\n    # get_cf_table_full_option_name\n    assert db_opts.get_cf_table_full_option_name(\"option1\") == \\\n           \"TableOptions.BlockBasedTable.option1\"\n\n    # extract_db_wide_option_name\n    assert db_opts.extract_cf_table_option_name(\n        \"TableOptions.BlockBasedTable.option1\") == \"option1\"\n    with pytest.raises(utils.ParsingError):\n        db_opts.extract_cf_table_option_name(\"DBOptions.option1\")", "\n\ndef test_misc_options_sanitization_utils():\n    # check_and_sanitize_if_no_value\n    assert db_opts.check_and_sanitize_if_null_ptr(None) == \\\n           (False, None)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"None\") == \\\n           (True, db_opts.SANITIZED_NULL_PTR)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"NONE\") == \\\n           (True, db_opts.SANITIZED_NULL_PTR)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"(nil)\") == \\\n           (True, db_opts.SANITIZED_NULL_PTR)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"nil\") == \\\n           (True, db_opts.SANITIZED_NULL_PTR)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"nullptr\") == \\\n           (True, db_opts.SANITIZED_NULL_PTR)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"null\") == \\\n           (True, db_opts.SANITIZED_NULL_PTR)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"NullPtr\") == \\\n           (True, db_opts.SANITIZED_NULL_PTR)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"0x0\") == \\\n           (True, db_opts.SANITIZED_NULL_PTR)\n\n    assert db_opts.check_and_sanitize_if_null_ptr(\"\") == \\\n           (False, \"\")\n    assert db_opts.check_and_sanitize_if_null_ptr(\"XXX\") == \\\n           (False, \"XXX\")\n    assert db_opts.check_and_sanitize_if_null_ptr(0) == \\\n           (False, 0)\n    assert db_opts.check_and_sanitize_if_null_ptr(False) == \\\n           (False, False)\n    assert db_opts.check_and_sanitize_if_null_ptr(\"0x1234\") == \\\n           (False, \"0x1234\")\n\n    # is_bool_value\n    assert db_opts.check_and_sanitize_if_bool_value(\n        True, include_int=False) == (True, \"True\")\n    assert db_opts.check_and_sanitize_if_bool_value(\n        False, include_int=False) == (True, \"False\")\n    assert db_opts.check_and_sanitize_if_bool_value(\n        \"true\", include_int=False) == (True, \"True\")\n    assert db_opts.check_and_sanitize_if_bool_value(\n        \"TRUE\", include_int=False) == (True, \"True\")\n    assert db_opts.check_and_sanitize_if_bool_value(\n        \"false\", include_int=False) == (True, \"False\")\n    assert db_opts.check_and_sanitize_if_bool_value(\n        \"FALSE\", include_int=False) == (True, \"False\")\n    assert db_opts.check_and_sanitize_if_bool_value(\n        0, include_int=False) == (False, 0)\n    assert db_opts.check_and_sanitize_if_bool_value(\n        1, include_int=False) == (False, 1)\n    assert db_opts.check_and_sanitize_if_bool_value(\n        0, include_int=True) == (True, \"False\")\n    assert db_opts.check_and_sanitize_if_bool_value(\n        1, include_int=True) == (True, \"True\")\n\n    assert db_opts.check_and_sanitize_if_pointer_value(None) == \\\n           (False, None)\n    assert db_opts.check_and_sanitize_if_pointer_value(\"(nil)\") ==\\\n           (False, \"(nil)\")\n    assert db_opts.check_and_sanitize_if_pointer_value(0) ==\\\n           (False, 0)\n    assert db_opts.check_and_sanitize_if_pointer_value(\"0x0\") ==\\\n           (False, \"0x0\")\n    assert db_opts.check_and_sanitize_if_pointer_value(\"0x123\") ==\\\n           (True, \"Pointer (0x123)\")\n\n    # get_sanitized_value\n    assert db_opts.get_sanitized_value(None) == db_opts.SANITIZED_NO_VALUE\n    assert db_opts.get_sanitized_value(\"None\") == db_opts.SANITIZED_NULL_PTR\n    assert db_opts.get_sanitized_value(\"None\") == db_opts.SANITIZED_NULL_PTR\n    assert db_opts.get_sanitized_value(\"0x0\") == db_opts.SANITIZED_NULL_PTR\n    assert db_opts.get_sanitized_value(True) == \"True\"\n    assert db_opts.get_sanitized_value(False) == \"False\"\n    assert db_opts.get_sanitized_value(False) == \"False\"\n    assert db_opts.get_sanitized_value(\"False\") == \"False\"\n    assert db_opts.get_sanitized_value(\"100\") == \"100\"\n    assert db_opts.get_sanitized_value(\"0x123\") == \"Pointer (0x123)\"\n\n    # get_sanitized_options_diff\n    assert db_opts.are_non_sanitized_values_different(0, 1)\n    assert db_opts.are_non_sanitized_values_different(False, 1)\n    assert db_opts.are_non_sanitized_values_different(True, 0)\n    assert db_opts.are_non_sanitized_values_different(\"True\", 0)\n    assert db_opts.are_non_sanitized_values_different(\"True\", \"0\")\n    assert db_opts.are_non_sanitized_values_different(\"False\", 1)\n    assert db_opts.are_non_sanitized_values_different(\"False\", \"1\")\n    assert not db_opts.are_non_sanitized_values_different(False, 0)\n    assert not db_opts.are_non_sanitized_values_different(True, 1)\n    assert not db_opts.are_non_sanitized_values_different(\"False\", 0)\n    assert not db_opts.are_non_sanitized_values_different(\"False\", \"0\")\n    assert not db_opts.are_non_sanitized_values_different(\"True\", 1)\n    assert not db_opts.are_non_sanitized_values_different(\"True\", \"1\")\n    assert not db_opts.are_non_sanitized_values_different(0, \"False\")\n    assert not db_opts.are_non_sanitized_values_different(1, \"TRUE\")\n\n    assert not db_opts.are_non_sanitized_values_different(None, None)\n    assert db_opts.are_non_sanitized_values_different(None, \"nil\")\n    assert db_opts.are_non_sanitized_values_different(None, \"False\")\n    assert db_opts.are_non_sanitized_values_different(False, None)\n\n    assert not db_opts.are_non_sanitized_values_different(\"0x0\", \"(nil)\")\n    assert not db_opts.are_non_sanitized_values_different(\"0x1234\", \"0x5678\")\n    assert db_opts.are_non_sanitized_values_different(\"0x0\", \"0x5678\")\n\n    # get_sanitized_options_diff\n    assert db_opts.get_sanitized_options_diff(\n        \"None\", None, expect_diff=False) == \\\n        (True, db_opts.SANITIZED_NULL_PTR, db_opts.SANITIZED_NO_VALUE)\n\n    assert \\\n        db_opts.get_sanitized_options_diff(\n            \"None\", \"false\", expect_diff=True) == \\\n        (db_opts.SANITIZED_NULL_PTR, \"False\")\n\n    assert db_opts.get_sanitized_options_diff(\n        \"false\", True, expect_diff=True) == (\"False\", \"True\")\n\n    assert db_opts.get_sanitized_options_diff(\n        0, True, expect_diff=True) == (\"False\", \"True\")\n\n    assert db_opts.get_sanitized_options_diff(\n        0, 1, expect_diff=True) == (0, 1)\n\n    assert db_opts.get_sanitized_options_diff(\n        0, \"1\", expect_diff=True) == (0, \"1\")\n\n    assert db_opts.get_sanitized_options_diff(\n        \"0x1234\", \"0x5678\", expect_diff=False) == \\\n        (False, \"Pointer (0x1234)\", \"Pointer (0x5678)\")", "\n\ndef test_full_name_options_dict():\n    db_option1 = \"DB-OPTION-1\"\n    db_value1 = \"DB-VALUE-1\"\n    db_value1_1 = \"DB-VALUE-1-1\"\n\n    cf1 = \"CF-1\"\n    option1 = \"OPTION-1\"\n    value1 = \"VALUE-1\"\n    value1_1 = \"VALUE-1-1\"\n\n    cf2 = \"CF-2\"\n    option2 = \"OPTION-2\"\n    value2 = \"VALUE-2\"\n    value2_1 = \"VALUE-2-1\"\n\n    fnd1 = db_opts.FullNamesOptionsDict()\n    fnd2 = db_opts.FullNamesOptionsDict()\n    assert fnd1 == fnd2\n    assert fnd1 == fnd2.get_options_dict()\n\n    assert fnd1.get_option_by_full_name(f\"DBOptions.{db_option1}\") is None\n    fnd1.set_option(db_opts.SectionType.DB_WIDE, DB_WIDE_CF_NAME, db_option1,\n                    db_value1)\n    assert fnd1 != fnd2\n\n    assert fnd1.get_option_by_full_name(f\"DBOptions.{db_option1}\") == {\n        DB_WIDE_CF_NAME: db_value1}\n    assert fnd1.\\\n           get_option_by_parts(db_opts.SectionType.DB_WIDE, db_option1) == \\\n           {DB_WIDE_CF_NAME: db_value1}\n    assert fnd1.get_option_by_parts(db_opts.SectionType.DB_WIDE, db_option1,\n                                    DB_WIDE_CF_NAME) == db_value1\n    assert fnd1.get_option_by_parts(db_opts.SectionType.CF, db_option1,\n                                    DB_WIDE_CF_NAME) is None\n    assert fnd1.get_option_by_parts(db_opts.SectionType.DB_WIDE, db_option1,\n                                    cf1) is None\n    assert fnd1.get_db_wide_option(db_option1) == db_value1\n    assert fnd1.get_cf_option(db_option1) is None\n    assert fnd1.get_cf_option(db_option1, cf1) is None\n    assert fnd1.get_cf_table_option(db_option1) is None\n    assert fnd1.get_cf_table_option(db_option1, cf1) is None\n    fnd1.set_db_wide_option(db_option1, db_value1_1)\n    assert fnd1.get_db_wide_option(db_option1) == db_value1_1\n\n    fnd1.set_cf_option(cf1, option1, value1)\n    assert fnd1.\\\n           get_option_by_parts(db_opts.SectionType.CF, option1) == \\\n           {cf1: value1}\n    assert fnd1.\\\n           get_option_by_parts(db_opts.SectionType.CF, option1, cf1) == value1\n    assert fnd1.\\\n           get_option_by_parts(db_opts.SectionType.DB_WIDE, option1) is None\n    assert fnd1.get_option_by_parts(db_opts.SectionType.DB_WIDE, option1,\n                                    DB_WIDE_CF_NAME) is None\n    assert fnd1.get_cf_option(option1) == {cf1: value1}\n    assert fnd1.get_cf_option(option1, cf1) == value1\n    assert fnd1.get_cf_table_option(option1) is None\n    assert fnd1.get_cf_table_option(option1, cf1) is None\n    fnd1.set_cf_option(cf1, option1, value1_1)\n    assert fnd1.get_cf_option(option1, cf1) == value1_1\n\n    fnd1.set_cf_table_option(cf2, option2, value2)\n    assert fnd1.\\\n           get_option_by_parts(db_opts.SectionType.TABLE_OPTIONS, option2) == \\\n           {cf2: value2}\n    assert fnd1.get_option_by_parts(db_opts.SectionType.TABLE_OPTIONS, option2,\n                                    cf2) == value2\n    assert fnd1.\\\n           get_option_by_parts(db_opts.SectionType.DB_WIDE, option2) is None\n    assert fnd1.get_option_by_parts(db_opts.SectionType.DB_WIDE, option2,\n                                    DB_WIDE_CF_NAME) is None\n    assert fnd1.get_cf_table_option(option2) == {cf2: value2}\n    assert fnd1.get_cf_table_option(option2, cf2) == value2\n    assert fnd1.get_cf_option(option2) is None\n    assert fnd1.get_cf_option(option2, cf2) is None\n    fnd1.set_cf_table_option(cf2, option2, value2_1)\n    assert fnd1.get_cf_table_option(option2, cf2) == value2_1\n\n    expected_dict = {'DBOptions.DB-OPTION-1': {'DB_WIDE': 'DB-VALUE-1-1'},\n                     'CFOptions.OPTION-1': {'CF-1': 'VALUE-1-1'},\n                     'TableOptions.BlockBasedTable.OPTION-2':\n                         {'CF-2': 'VALUE-2-1'}}\n    assert fnd1.get_options_dict() == expected_dict\n\n    fnd1.set_db_wide_option(option2, value1_1)\n    fnd1.set_cf_option(cf2, option1, db_value1)\n    fnd1.set_cf_table_option(cf1, option2, value2)\n    expected_dict = {'DBOptions.DB-OPTION-1': {'DB_WIDE': 'DB-VALUE-1-1'},\n                     'DBOptions.OPTION-2': {'DB_WIDE': 'VALUE-1-1'},\n                     'CFOptions.OPTION-1': {'CF-1': 'VALUE-1-1',\n                                            'CF-2': 'DB-VALUE-1'},\n                     'TableOptions.BlockBasedTable.OPTION-2':\n                         {'CF-2': 'VALUE-2-1', 'CF-1': 'VALUE-2'}}\n    assert fnd1.get_options_dict() == expected_dict", "\n\ndef test_empty_db_options():\n    dbo = db_opts.DatabaseOptions()\n    assert dbo.get_all_options() == EMPTY_FULL_NAMES_OPTIONS_DICT\n    assert dbo.get_db_wide_options() == EMPTY_FULL_NAMES_OPTIONS_DICT\n    assert not dbo.are_db_wide_options_set()\n    assert dbo.get_cfs_names() == []\n    assert dbo.get_db_wide_options_for_display() == {}\n    assert dbo.get_db_wide_option(\"manual_wal_flush\") is None\n\n    with pytest.raises(AssertionError):\n        dbo.set_db_wide_option(\"manual_wal_flush\", \"1\",\n                               allow_new_option=False)\n    dbo.set_db_wide_option(\"manual_wal_flush\", \"1\",\n                           allow_new_option=True)\n    assert dbo.get_db_wide_option(\"manual_wal_flush\") == \"1\"\n    assert dbo.get_db_wide_option(\"Dummy-Options\") is None\n    assert dbo.get_options({\"DBOptions.manual_wal_flush\"}).\\\n        get_options_dict() == {\n        'DBOptions.manual_wal_flush': {\"DB_WIDE\": \"1\"}}\n    assert dbo.get_options({\"DBOptions.Dummy-Option\"}) == \\\n           EMPTY_FULL_NAMES_OPTIONS_DICT\n\n    assert dbo.get_cf_options(default) == EMPTY_FULL_NAMES_OPTIONS_DICT\n    assert dbo.get_cf_options_for_display(default) == ({}, {})\n    assert dbo.get_cf_option(default, \"write_buffer_size\") is None\n    with pytest.raises(AssertionError):\n        dbo.set_cf_option(default, \"write_buffer_size\", \"100\",\n                          allow_new_option=False)\n    dbo.set_cf_option(default, \"write_buffer_size\", \"100\",\n                      allow_new_option=True)\n    assert dbo.get_cf_option(default, \"write_buffer_size\") == \"100\"\n    assert dbo.get_cf_option(default, \"Dummmy-Options\") is None\n    assert dbo.get_cf_option(\"Dummy-CF\", \"write_buffer_size\") is None\n    assert dbo.get_options({\"CFOptions.write_buffer_size\"}) == \\\n           {'CFOptions.write_buffer_size': {default: '100'}}\n    assert dbo.get_options({\"CFOptions.write_buffer_size\"}, default) \\\n           == {'CFOptions.write_buffer_size': {default: '100'}}\n    assert dbo.get_options({\"CFOptions.write_buffer_size\"}, \"Dummy-CF\")\\\n           == {}\n    assert dbo.get_options({\"CFOptions.Dummy-Options\"}, default) == {}\n\n    assert dbo.get_cf_table_option(default, \"write_buffer_size\") is \\\n           None\n\n    assert dbo.\\\n           get_options([\"TableOptions.BlockBasedTable.block_align\"], cf1) == {}\n\n    with pytest.raises(AssertionError):\n        dbo.set_cf_table_option(cf1, \"index_type\", \"3\", allow_new_option=False)\n    dbo.set_cf_table_option(cf1, \"index_type\", \"3\", allow_new_option=True)\n    assert dbo.get_cf_table_option(cf1, \"index_type\") == \"3\"\n    assert dbo.get_cf_table_option(cf1, \"Dummy-Options\") is None\n    assert dbo.get_cf_table_option(default, \"index_type\") is None\n\n    assert dbo.get_options({\"TableOptions.BlockBasedTable.index_type\"})\\\n           == {'TableOptions.BlockBasedTable.index_type': {cf1: '3'}}\n    assert dbo.get_options({\"TableOptions.BlockBasedTable.index_type\"},\n                           cf1) == \\\n           {'TableOptions.BlockBasedTable.index_type': {cf1: '3'}}\n    assert dbo.get_options({\n        \"TableOptions.BlockBasedTable.index_type\"}, \"Dummy-CF\") == {}\n    assert dbo.get_options({\n        \"TableOptions.BlockBasedTable.Dummy-Option\"}) == {}", "\n\ndef test_set_db_wide_options():\n    input_dict = {\"manual_wal_flush\": \"false\",\n                  \"db_write_buffer_size\": \"0\"}\n    expected_options = {\n        'DBOptions.manual_wal_flush': {utils.NO_CF: 'False'},\n        'DBOptions.db_write_buffer_size': {utils.NO_CF: '0'},\n    }\n\n    dbo = db_opts.DatabaseOptions()\n    dbo.set_db_wide_options(input_dict)\n\n    assert dbo.get_all_options() == expected_options\n    assert dbo.get_db_wide_options() == expected_options\n    assert dbo.get_db_wide_option(\"manual_wal_flush\") == 'False'\n    assert dbo.get_db_wide_option(\"manual_wal_flush-X\") is None\n    assert dbo.get_cfs_names() == []\n\n    dbo.set_db_wide_option('manual_wal_flush', 'true', allow_new_option=False)\n    assert dbo.get_db_wide_option(\"manual_wal_flush\") == 'True'\n\n    dbo.set_db_wide_option('manual_wal_flush', 'false', allow_new_option=False)\n    assert dbo.get_db_wide_option(\"manual_wal_flush\") == 'False'\n\n    assert dbo.get_db_wide_option(\"DUMMY\") is None\n    with pytest.raises(AssertionError):\n        dbo.set_db_wide_option('DUMMY', 'XXX', allow_new_option=False)\n\n    dbo.set_db_wide_option('DUMMY', 'XXX', True)\n    assert dbo.get_db_wide_option(\"DUMMY\") == \"XXX\"\n\n    expected_db_wide_display_options = {'manual_wal_flush': 'False',\n                                        'db_write_buffer_size': '0',\n                                        'DUMMY': 'XXX'}\n    assert dbo.get_db_wide_options_for_display() == \\\n           expected_db_wide_display_options", "\n\ndef test_set_cf_options():\n    default_input_dict = {\"write_buffer_size\": \"1000\"}\n    default_table_input_dict = {\"index_type\": \"1\"}\n\n    cf1_input_dict = {\"write_buffer_size\": \"2000\"}\n    cf1_table_input_dict = {\"index_type\": \"2\"}\n\n    cf2_input_dict = {\"write_buffer_size\": \"3000\"}\n    cf2_table_input_dict = {\"index_type\": \"3\"}\n\n    dbo = db_opts.DatabaseOptions()\n\n    expected_options = dict()\n    expected_options['CFOptions.write_buffer_size'] = {default: '1000'}\n    expected_options['TableOptions.BlockBasedTable.index_type'] = \\\n        {default: '1'}\n    dbo.set_cf_options(default, default_input_dict, default_table_input_dict)\n    assert dbo.get_all_options() == expected_options\n    assert dbo.get_cfs_names() == [default]\n\n    expected_options['CFOptions.write_buffer_size'][cf1] = \"2000\"\n    expected_options['TableOptions.BlockBasedTable.index_type'][cf1] = \"2\"\n    dbo.set_cf_options(cf1, cf1_input_dict, cf1_table_input_dict)\n    assert dbo.get_all_options().get_options_dict() == expected_options\n    assert dbo.get_cfs_names() == [default, cf1]\n\n    expected_options['CFOptions.write_buffer_size'][cf2] = \"3000\"\n    expected_options['TableOptions.BlockBasedTable.index_type'][cf2] = \"3\"\n    dbo.set_cf_options(cf2, cf2_input_dict, cf2_table_input_dict)\n    assert dbo.get_all_options() == expected_options\n    assert dbo.get_cfs_names() == [default, cf1, cf2]\n\n    expected_dict_default = \\\n        {'CFOptions.write_buffer_size': {default: '1000'},\n         'TableOptions.BlockBasedTable.index_type': {default: '1'}}\n\n    expected_dict_cf1 = \\\n        {'CFOptions.write_buffer_size': {cf1: '2000'},\n         'TableOptions.BlockBasedTable.index_type': {cf1: '2'}}\n\n    expected_dict_cf2 = \\\n        {'CFOptions.write_buffer_size': {cf2: '3000'},\n         'TableOptions.BlockBasedTable.index_type': {cf2: '3'}}\n\n    assert dbo.get_db_wide_options().get_options_dict() == {}\n    assert expected_dict_default == dbo.get_cf_options(default)\n    assert expected_dict_cf1 == dbo.get_cf_options(cf1)\n    assert expected_dict_cf2 == dbo.get_cf_options(cf2)\n    assert dbo.get_cf_option(default, \"write_buffer_size\") == '1000'\n    assert dbo.get_cf_table_option(default, \"index_type\", ) == '1'\n    assert dbo.get_cf_option(cf1, \"write_buffer_size\") == '2000'\n    assert dbo.get_cf_table_option(cf1, \"index_type\", ) == '2'\n    dbo.set_cf_table_option(cf1, \"index_type\", '100')\n    assert dbo.get_cf_table_option(cf1, \"index_type\", ) == '100'\n    assert dbo.get_cf_option(cf2, \"write_buffer_size\") == '3000'\n    assert dbo.get_cf_table_option(cf2, \"index_type\") == '3'\n    dbo.set_cf_table_option(cf2, \"index_type\", 'XXXX')\n    assert dbo.get_cf_table_option(cf2, \"index_type\") == 'XXXX'\n\n    assert dbo.get_cf_option(default, \"index_type\") is None\n    dbo.set_cf_option(default, \"index_type\", \"200\", True)\n    assert dbo.get_cf_option(default, \"index_type\") == \"200\"\n    assert dbo.get_cf_table_option(cf1, \"write_buffer_size\") is None\n\n    expected_default_display_options = {'write_buffer_size': '1000',\n                                        'index_type': '200'}\n    expected_default_display_table_options = {'index_type': '1'}\n\n    expected_cf1_display_options = {'write_buffer_size': '2000'}\n    expected_cf1_display_table_options = {'index_type': '100'}\n\n    expected_cf2_display_options = {'write_buffer_size': '3000'}\n    expected_cf2_display_table_options = {'index_type': 'XXXX'}\n\n    assert dbo.get_cf_options_for_display(default) == \\\n           (expected_default_display_options,\n            expected_default_display_table_options)\n\n    assert dbo.get_cf_options_for_display(cf1) == \\\n           (expected_cf1_display_options,\n            expected_cf1_display_table_options)\n\n    assert dbo.get_cf_options_for_display(cf2) == \\\n           (expected_cf2_display_options,\n            expected_cf2_display_table_options)", "\n\ndef test_get_unified_options():\n    get_unified = db_opts.DatabaseOptions.get_unified_cfs_options\n\n    assert get_unified({}) == ({}, {})\n\n    cf1_options =\\\n        db_opts.FullNamesOptionsDict({\n            'CFOptions.write_buffer_size': {cf1: 1000}})\n    cfs_options = {cf1: cf1_options}\n    assert get_unified(cfs_options) == \\\n           ({\"CFOptions.write_buffer_size\": 1000},\n            {cf1: {}})\n\n    cf2_options =\\\n        db_opts.FullNamesOptionsDict({\n            'CFOptions.write_buffer_size': {cf2: 1000}})\n    cfs_options = {cf1: cf1_options, cf2: cf2_options}\n    assert get_unified(cfs_options) == \\\n           ({\"CFOptions.write_buffer_size\": 1000},\n            {cf1: {}, cf2: {}})\n\n    cf2_options.set_cf_option(cf2, \"write_buffer_size\", 2000)\n    assert get_unified(cfs_options) == \\\n           ({},\n            {cf1: {\"CFOptions.write_buffer_size\": 1000},\n             cf2: {\"CFOptions.write_buffer_size\": 2000}})\n\n    cf1_options.set_cf_option(cf1, \"cf1_only_option\", \"1234\")\n    assert get_unified(cfs_options) == \\\n           ({},\n            {cf1: {\"CFOptions.write_buffer_size\": 1000,\n                   \"CFOptions.cf1_only_option\": \"1234\"},\n             cf2: {\"CFOptions.write_buffer_size\": 2000}})\n\n    cf2_options.set_cf_option(cf2, \"cf2_only_option\", \"5678\")\n    assert get_unified(cfs_options) == \\\n           ({},\n            {cf1: {\"CFOptions.write_buffer_size\": 1000,\n                   \"CFOptions.cf1_only_option\": \"1234\"},\n             cf2: {\"CFOptions.write_buffer_size\": 2000,\n                   \"CFOptions.cf2_only_option\": \"5678\"}})\n\n    cf2_options.set_cf_option(cf2, \"write_buffer_size\", 1000)\n    assert get_unified(cfs_options) == \\\n           ({\"CFOptions.write_buffer_size\": 1000},\n            {cf1: {\"CFOptions.cf1_only_option\": \"1234\"},\n             cf2: {\"CFOptions.cf2_only_option\": \"5678\"}})", "\n\ndef test_options_diff_class():\n    diff = db_opts.OptionsDiff(baseline_options, new_options)\n    assert diff.get_diff_dict() == {}\n\n    diff.diff_in_base(cf2, 'CFOptions.write_buffer_size')\n    diff.diff_in_new(cf3, 'CFOptions.write_buffer_size')\n    diff.diff_between(default, 'CFOptions.write_buffer_size')\n\n    assert diff.get_diff_dict() == {\n        'CFOptions.write_buffer_size': {\n            cf2: ('128000000', db_opts.SANITIZED_NO_VALUE),\n            cf3: (db_opts.SANITIZED_NO_VALUE, '128000000'),\n            default: ('1024000', '128000000')\n        }\n    }", "\n\ndef test_get_options_diff():\n    expected_diff = {\n        'DBOptions.stats_dump_freq_sec':\n            {utils.NO_CF: ('20', db_opts.SANITIZED_NO_VALUE)},\n        'DBOptions.enable_pipelined_write':\n            {utils.NO_CF: ('False', 'True')},\n        'bloom_bits': {utils.NO_CF: (db_opts.SANITIZED_NO_VALUE, '4')},\n        'CFOptions.write_buffer_size': {\n            default: ('1024000', '128000000'),\n            cf2: ('128000000', db_opts.SANITIZED_NO_VALUE),\n            cf3: (db_opts.SANITIZED_NO_VALUE, '128000000')},\n        'TableOptions.BlockBasedTable.index_type':\n            {'cf2': ('1', db_opts.SANITIZED_NO_VALUE)},\n        'TableOptions.BlockBasedTable.checksum':\n            {cf1: (db_opts.SANITIZED_NO_VALUE, 'True')},\n        'TableOptions.BlockBasedTable.format_version': {'cf1': ('4', '5')},\n        'DBOptions.max_log_file_size':\n            {utils.NO_CF: ('128000000', '0')}\n    }\n\n    full_name_baseline_dict = db_opts.FullNamesOptionsDict(baseline_options)\n    full_name_new_dict = db_opts.FullNamesOptionsDict(new_options)\n    diff = db_opts.DatabaseOptions.\\\n        get_options_diff(full_name_baseline_dict, full_name_new_dict)\n    assert diff.get_diff_dict() == expected_diff", "\n\ndef test_cfs_options_diff_class():\n    diff = db_opts.CfsOptionsDiff(baseline_options, default, new_options, cf1)\n    assert diff.get_diff_dict() == {}\n\n    diff.diff_between('CFOptions.write_buffer_size')\n\n    assert diff.get_diff_dict() == {\n        db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default,\n                                              \"New\": cf1},\n        'CFOptions.write_buffer_size': ('1024000', '128000')\n    }", "\n\ndef test_get_db_wide_options_diff():\n    assert db_opts.DatabaseOptions.\\\n               get_db_wide_options_diff(baseline, baseline) is None\n\n    expected_diff = {'bloom_bits': (db_opts.SANITIZED_NO_VALUE, '4'),\n                     'DBOptions.enable_pipelined_write': ('False', 'True'),\n                     'DBOptions.max_log_file_size': ('128000000', '0'),\n                     'DBOptions.stats_dump_freq_sec':\n                         ('20', db_opts.SANITIZED_NO_VALUE),\n                     db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n                         {\"Base\": 'DB_WIDE',\n                          \"New\": 'DB_WIDE'}}\n\n    assert db_opts.DatabaseOptions.get_db_wide_options_diff(baseline, new).\\\n        get_diff_dict() == expected_diff", "\n\ndef test_get_cfs_options_diff():\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(\n            baseline, utils.NO_CF, baseline, utils.NO_CF) is None\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(baseline, default, baseline, default) is None\n\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(baseline, cf1, baseline, cf1) is None\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(baseline, cf2, baseline, cf2) is None\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(baseline, cf3, baseline, cf3) is None\n\n    assert db_opts.DatabaseOptions.get_cfs_options_diff(\n        new, utils.NO_CF,\n        new, utils.NO_CF) is None\n\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(new, default, new, default) is None\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(new, cf1, new, cf1) is None\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(new, cf2, new, cf2) is None\n    assert db_opts.DatabaseOptions.\\\n           get_cfs_options_diff(new, cf3, new, cf3) is None\n\n    def compare_cf_actual_and_expected_diffs(actual_diff, expected_diff_dict):\n        assert isinstance(actual_diff, db_opts.CfsOptionsDiff)\n        assert isinstance(expected_diff_dict, dict)\n\n        assert actual_diff == expected_diff_dict\n        assert utils.are_dicts_equal_and_in_same_keys_order(\n            actual_diff.diff_dict, expected_diff)\n\n    expected_diff = {\n        'CFOptions.write_buffer_size': ('1024000', '128000000'),\n        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n            {\"Base\": default,\n             \"New\": default}\n    }\n\n    actual_diff = db_opts.DatabaseOptions.\\\n        get_cfs_options_diff(baseline, default, new, default)\n    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\n    expected_diff = {\n        'CFOptions.write_buffer_size': ('1024000', '128000'),\n        'TableOptions.BlockBasedTable.format_version':\n            (db_opts.SANITIZED_NO_VALUE, '5'),\n        'TableOptions.BlockBasedTable.block_size':\n            (db_opts.SANITIZED_NO_VALUE, '4096'),\n        'TableOptions.BlockBasedTable.checksum':\n            (db_opts.SANITIZED_NO_VALUE, 'True'),\n        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n            {\"Base\": default,\n             \"New\": cf1}\n    }\n\n    actual_diff = db_opts.DatabaseOptions.\\\n        get_cfs_options_diff(baseline, default, new, cf1)\n    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\n    expected_diff = {\n        'CFOptions.write_buffer_size': ('128000', '1024000'),\n        'TableOptions.BlockBasedTable.checksum':\n            ('True', db_opts.SANITIZED_NO_VALUE),\n        'TableOptions.BlockBasedTable.format_version':\n            ('5', db_opts.SANITIZED_NO_VALUE),\n        'TableOptions.BlockBasedTable.block_size':\n            ('4096', db_opts.SANITIZED_NO_VALUE),\n        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n            {\"Base\": cf1,\n             \"New\": default}\n    }\n\n    actual_diff = db_opts.DatabaseOptions. \\\n        get_cfs_options_diff(new, cf1, baseline, default)\n    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\n    expected_diff = {\n        'CFOptions.write_buffer_size':\n            ('128000000', db_opts.SANITIZED_NO_VALUE),\n        'TableOptions.BlockBasedTable.index_type':\n            ('1', db_opts.SANITIZED_NO_VALUE),\n        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n            {\"Base\": cf2,\n             \"New\": cf2},\n    }\n    actual_diff = db_opts.DatabaseOptions.\\\n        get_cfs_options_diff(baseline, cf2, new, cf2)\n    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)\n\n    expected_diff = {\n        'CFOptions.write_buffer_size':\n            (db_opts.SANITIZED_NO_VALUE, '128000000'),\n        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n            {\"Base\": cf3,\n             \"New\": cf3}\n    }\n    actual_diff = db_opts.DatabaseOptions.\\\n        get_cfs_options_diff(baseline, cf3, new, cf3)\n    compare_cf_actual_and_expected_diffs(actual_diff, expected_diff)", "\n\ndef test_get_unified_cfs_diffs():\n    cfs_diffs = {}\n    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == ({}, {})\n\n    cf1_vs_default_diff = {\n        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n            {\"Base\": default,\n             \"New\": cf1},\n        'CFOptions.write_buffer_size': ('1024000', '128000'),\n    }\n    cfs_diffs = [cf1_vs_default_diff]\n    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n        {'CFOptions.write_buffer_size': ('1024000', '128000')},\n        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1}}])\n\n    cf2_vs_default_diff = {\n        db_opts.CfsOptionsDiff.CF_NAMES_KEY:\n            {\"Base\": default,\n             \"New\": cf2},\n        'CFOptions.write_buffer_size': ('1024000', '128000'),\n    }\n    cfs_diffs = [cf1_vs_default_diff, cf2_vs_default_diff]\n    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n        {'CFOptions.write_buffer_size': ('1024000', '128000')},\n        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1}},\n         {db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf2}}])\n\n    cf1_vs_default_diff['CFOptions.write_buffer_size'] = (1000, 2000)\n    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n        {},\n        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1},\n          'CFOptions.write_buffer_size': (1000, 2000)},\n         {db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf2},\n          'CFOptions.write_buffer_size': ('1024000', '128000')}])\n\n    cf1_vs_default_diff['CFOptions.write_buffer_size'] = \\\n        cf2_vs_default_diff['CFOptions.write_buffer_size']\n    cf2_vs_default_diff['TableOptions.BlockBasedTable.cf2_option'] = (200, 300)\n    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n        {'CFOptions.write_buffer_size': ('1024000', '128000')},\n        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1}},\n         {db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf2},\n          'TableOptions.BlockBasedTable.cf2_option': (200, 300)}])\n\n    cf1_vs_default_diff['TableOptions.BlockBasedTable.cf1_option'] = (True,\n                                                                      False)\n    assert db_opts.DatabaseOptions.get_unified_cfs_diffs(cfs_diffs) == (\n        {'CFOptions.write_buffer_size': ('1024000', '128000')},\n        [{db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf1},\n          'TableOptions.BlockBasedTable.cf1_option': (True, False)},\n         {db_opts.CfsOptionsDiff.CF_NAMES_KEY: {\"Base\": default, \"New\": cf2},\n          'TableOptions.BlockBasedTable.cf2_option': (200, 300)}])", ""]}
{"filename": "test/test_csv_outputter.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport counters\nimport csv_outputter\nfrom test.testing_utils import \\\n    add_stats_entry_lines_to_counters_mngr", "from test.testing_utils import \\\n    add_stats_entry_lines_to_counters_mngr\n\nadd_stats_entry_lines_to_mngr = \\\n    add_stats_entry_lines_to_counters_mngr\n\n\ndef test_get_counters_csv():\n    counter1_entry_lines = [\n        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 COUNT : 0'''.splitlines(),\n        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 COUNT : 10'''.splitlines(),\n        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 COUNT : 0'''.splitlines()\n    ]\n\n    # All 0-s => Should be in the CSV at all\n    counter2_entry_lines = [\n        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter2 COUNT : 0'''.splitlines()\n    ]\n\n    # Has an entry only once. Later doesn't even have an entry\n    counter3_entry_lines = [\n        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter3 COUNT : 100'''.splitlines()\n    ]\n\n    mngr = counters.CountersMngr()\n    assert csv_outputter.get_counters_csv(mngr) is None\n\n    for entry in counter1_entry_lines:\n        add_stats_entry_lines_to_mngr(entry, mngr)\n    for entry in counter2_entry_lines:\n        add_stats_entry_lines_to_mngr(entry, mngr)\n    for entry in counter3_entry_lines:\n        add_stats_entry_lines_to_mngr(entry, mngr)\n\n    expected_csv = 'Time,counter1,counter3\\r\\n'\\\n                   '2022/11/24-15:50:09.512106,0,0\\r\\n'\\\n                   '2022/11/24-15:50:10.512106,10,0\\r\\n'\\\n                   '2022/11/24-15:50:12.512106,0,100\\r\\n'\n\n    assert csv_outputter.get_counters_csv(mngr) == expected_csv", "\n\ndef test_get_human_readable_histograms_csv():\n    counter1_entry_lines = [\n        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines(), # noqa\n        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 1 SUM : 10'''.splitlines(), # noqa\n        '''2022/11/24-15:50:11.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 7 SUM : 24'''.splitlines(), # noqa\n    ]\n\n    # All 0-s => Should be in the CSV at all\n    counter2_entry_lines = [\n        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter2 P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines(), # noqa\n    ]\n\n    # Has an entry only once. Later doesn't even have an entry\n    counter3_entry_lines = [\n        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter3 P50 : 0.500000 P95 : 0.500000 P99 : 0.000000 P100 : 0.000000 COUNT : 12 SUM : 36'''.splitlines(), # noqa\n    ]\n\n    mngr = counters.CountersMngr()\n    assert csv_outputter.get_counters_csv(mngr) is None\n\n    for entry in counter1_entry_lines:\n        add_stats_entry_lines_to_mngr(entry, mngr)\n    for entry in counter2_entry_lines:\n        add_stats_entry_lines_to_mngr(entry, mngr)\n    for entry in counter3_entry_lines:\n        add_stats_entry_lines_to_mngr(entry, mngr)\n\n    csv = csv_outputter.get_human_readable_histogram_csv(mngr)\n    assert csv == \\\n        ',counter1,.,.,.,.,.,counter3,.,.,.,.,.\\r\\n' \\\n        ',P50,P95,P99,P100,Count,Sum,P50,P95,P99,P100,Count,Sum\\r\\n' \\\n        '2022/11/24-15:50:09.512106,0.0,0.0,0.0,0.0,0,0,0.0,0,0,0,0,0,0,0,0\\r\\n' \\\n        '2022/11/24-15:50:10.512106,1.0,0.0,0.0,0.0,1,10,10.0,1,10,0,0,0,0,0,0\\r\\n' \\\n        '2022/11/24-15:50:11.512106,1.0,0.0,0.0,0.0,7,24,3.43,6,14,0,0,0,0,0,0\\r\\n' \\\n        '2022/11/24-15:50:12.512106,0,0,0,0,0,0,0.5,0.5,0.0,0.0,12,36,3.0,12,36\\r\\n' # noqa\n\n    csv = csv_outputter.get_tools_histogram_csv(mngr)\n    assert csv == \\\n        'Name,Time,P50,P95,P99,P100,Count,Sum,Average,Interval Count,Interval Sum\\r\\n' \\\n        'counter1,2022/11/24-15:50:09.512106,0.0,0.0,0.0,0.0,0,0,0.0,0,0\\r\\n' \\\n        'counter1,2022/11/24-15:50:10.512106,1.0,0.0,0.0,0.0,1,10,10.0,1,10\\r\\n' \\\n        'counter1,2022/11/24-15:50:11.512106,1.0,0.0,0.0,0.0,7,24,3.43,6,14\\r\\n' \\\n        'counter1,2022/11/24-15:50:12.512106\\r\\n' \\\n        'counter3,2022/11/24-15:50:09.512106,0,0,0,0,0,0,0,0,0\\r\\n' \\\n        'counter3,2022/11/24-15:50:10.512106,0,0,0,0,0,0,0,0,0\\r\\n' \\\n        'counter3,2022/11/24-15:50:11.512106,0,0,0,0,0,0,0,0,0\\r\\n' \\\n        'counter3,2022/11/24-15:50:12.512106,0.5,0.5,0.0,0.0,12,36,3.0,12,36\\r\\n' # noqa", "\n\ndef test_process_compactions_csv_header():\n    test_func = csv_outputter.process_compactions_csv_header\n    result_type = csv_outputter.CompactionsCsvInputFilesInfo\n\n    assert test_func([]) is None\n    assert test_func([\"files_L\", \"files1\"]) is None\n    assert test_func([\"files_\"]) is None\n\n    assert test_func([\"files_L0\"]) == \\\n           result_type([\"Input Level Files\", \"Input Files from Output Level\"],\n                       first_column_idx=0,\n                       first_level=0,\n                       second_level=None)\n\n    assert test_func([\"files_L1\", \"files_L2\"]) == \\\n           result_type([\"Input Level Files\", \"Input Files from Output Level\"],\n                       first_column_idx=0,\n                       first_level=1,\n                       second_level=2)\n\n    assert test_func([\"COL1\", \"files_L1\", \"files_L2\"]) == \\\n           result_type(\n               [\"COL1\", \"Input Level Files\", \"Input Files from Output Level\"],\n               first_column_idx=1,\n               first_level=1,\n               second_level=2)\n\n    assert test_func([\"COL1\", \"files_L1\", \"COL2\"]) == \\\n           result_type(\n               [\"COL1\", \"Input Level Files\", \"Input Files from Output \"\n                                             \"Level\", \"COL2\"],\n               first_column_idx=1,\n               first_level=1,\n               second_level=None)\n\n    assert test_func([\"COL1\", \"files_L10\", \"files_L20\", \"files_L30\"]) == \\\n           result_type(\n               [\"COL1\", \"Input Level Files\", \"Input Files from Output Level\"],\n               first_column_idx=1,\n               first_level=10,\n               second_level=20)\n\n    # Non Consecutive \"files_<Level>\" columns\n    assert test_func([\"COL1\", \"files_L10\", \"COL2\", \"files_L20\", \"files_L30\",\n                      \"COL4\", \"COL5\", \"files_40\"]) is None", ""]}
{"filename": "test/sample_log_info.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nfrom datetime import timedelta\n\nimport utils\n", "import utils\n\n\nclass SampleLogInfo:\n    FILE_PATH = \"input_files/LOG_sample\"\n    START_TIME = \"2022/04/17-14:13:10.723796\"\n    END_TIME = \"2022/04/17-14:14:32.645120\"\n    PRODUCT_NAME = \"SpeeDB\"\n    GIT_HASH = \"UNKNOWN:0a396684d6c08f6fe4a37572c0429d91176c51d1\"\n    VERSION = \"6.22.1\"\n    NUM_ENTRIES = 73\n    CF_DEFAULT = 'default'\n    CF_NAMES = [CF_DEFAULT, '_sample/CF_1', '_sample/CF-2', '']\n    DB_WIDE_OPTIONS_START_ENTRY_IDX = 7\n    SUPPORT_INFO_START_ENTRY_IDX = 15\n\n    CF_DEFAULT_LEVEL_SIZES = {\n        0: utils.get_num_bytes_from_human_readable_components(\"149.99\", \"MB\"),\n        1: utils.get_num_bytes_from_human_readable_components(\"271.79\", \"MB\"),\n        2: utils.get_num_bytes_from_human_readable_components(\"2.73\", \"GB\"),\n        3: utils.get_num_bytes_from_human_readable_components(\"24.96\", \"GB\"),\n        4: utils.get_num_bytes_from_human_readable_components(\"54.33\", \"GB\"),\n    }\n\n    CF_SIZE_BYTES = \\\n        {\n            'default':\n                utils.get_num_bytes_from_human_readable_components(\n                    \"82.43\", \"GB\"),\n            '_sample/CF_1': None,\n            '_sample/CF-2': None,\n            '': None,\n        }\n\n    DB_SIZE_BYTES = sum([size if size is not None else 0 for size in\n                         CF_SIZE_BYTES.values()])\n\n    NUM_WARNS = 1\n\n    OPTIONS_ENTRIES_INDICES = [21, 33, 42, 50]\n    TABLE_OPTIONS_ENTRIES_INDICES = [24, 38, 45, 55]\n\n    DB_WIDE_OPTIONS_DICT = {\n        'error_if_exists': '0',\n        'create_if_missing': '1',\n        'db_log_dir': '',\n        'wal_dir': '',\n        'track_and_verify_wals_in_manifest': '0',\n        'env': '0x7f4a9117d5c0',\n        'fs': 'Posix File System',\n        'info_log': '0x7f4af4020bf0'\n    }\n\n    DEFAULT_OPTIONS_DICT = {\n        'comparator': 'leveldb.BytewiseComparator',\n        'merge_operator': 'StringAppendTESTOperator',\n        'write_buffer_size': '67108864',\n        'max_write_buffer_number': '2',\n        'ttl': '2592000'\n    }\n\n    SAMPLE_CF1_OPTIONS_DICT = {\n        'comparator': 'leveldb.BytewiseComparator-XXX',\n        'merge_operator': 'StringAppendTESTOperator-XXX',\n        'compaction_filter': 'None',\n        'table_factory': 'BlockBasedTable',\n        'write_buffer_size': '67108864',\n        'max_write_buffer_number': '2',\n    }\n\n    SAMPLE_CF2_OPTIONS_DICT = {\n        'comparator': 'leveldb.BytewiseComparator-YYY',\n        'table_factory': 'BlockBasedTable-YYY',\n        'write_buffer_size': '123467108864',\n        'max_write_buffer_number': '10',\n        'compression': 'Snappy'\n    }\n\n    EMPTY_CF_OPTIONS_DICT = {\n        'comparator': 'leveldb.BytewiseComparator-ZZZ',\n        'merge_operator': 'StringAppendTESTOperator-ZZZ',\n        'compaction_filter': 'None',\n        'table_factory': 'BlockBasedTable'\n    }\n\n    DEFAULT_TABLE_OPTIONS_DICT = {\n        'flush_block_policy_factory':\n            'FlushBlockBySizePolicyFactory (0x7f4af401f570)',\n        'cache_index_and_filter_blocks': '1',\n        'cache_index_and_filter_blocks_with_high_priority': '1',\n        'pin_l0_filter_and_index_blocks_in_cache': '1',\n        'pin_top_level_index_and_filter': '1',\n        'metadata_cache_top_level_index_pinning': '0',\n        'block_cache_capacity': '209715200',\n        'block_cache_compressed': '(nil)',\n        'prepopulate_block_cache': '0'}\n\n    SAMPLE_CF1_TABLE_OPTIONS_DICT = {\n        'flush_block_policy_factory':\n            'FlushBlockBySizePolicyFactory (0x7f4af4031090)',\n        'pin_top_level_index_and_filter': '1',\n        'metadata_cache_unpartitioned_pinning': '3',\n        'no_block_cache': '0',\n        'block_cache': '0x7f4bc07214d0',\n        'block_cache_memory_allocator': 'None',\n        'block_cache_high_pri_pool_ratio': '0.100',\n        'block_cache_compressed': '(nil)'}\n\n    SAMPLE_CF2_TABLE_OPTIONS_DICT = {\n        'flush_block_policy_factory':\n            'FlushBlockBySizePolicyFactory (0x7f4af4091b90)',\n        'cache_index_and_filter_blocks': '1',\n        'cache_index_and_filter_blocks_with_high_priority': '1'\n    }\n\n    EMPTY_CF_TABLE_OPTIONS_DICT = {\n        'flush_block_policy_factory':\n            'FlushBlockBySizePolicyFactory (0x7f4af4030f30)',\n        'pin_top_level_index_and_filter': '1'}\n\n    OPTIONS_DICTS = [\n        DEFAULT_OPTIONS_DICT,\n        SAMPLE_CF1_OPTIONS_DICT,\n        SAMPLE_CF2_OPTIONS_DICT,\n        EMPTY_CF_OPTIONS_DICT\n    ]\n\n    TABLE_OPTIONS_DICTS = [\n        DEFAULT_TABLE_OPTIONS_DICT,\n        SAMPLE_CF1_TABLE_OPTIONS_DICT,\n        SAMPLE_CF2_TABLE_OPTIONS_DICT,\n        EMPTY_CF_TABLE_OPTIONS_DICT\n    ]\n\n    DB_STATS_ENTRY_TIME = \"2022/04/17-14:14:28.645150\"\n    CUMULATIVE_DURATION = \\\n        timedelta(hours=12, minutes=10, seconds=56, milliseconds=123)\n    INTERVAL_DURATION = \\\n        timedelta(hours=45, minutes=34, seconds=12, milliseconds=789)\n    DB_WIDE_STALLS_ENTRIES = \\\n        {DB_STATS_ENTRY_TIME: {\"cumulative_duration\": CUMULATIVE_DURATION,\n                               \"cumulative_percent\": 98.7,\n                               \"interval_duration\": INTERVAL_DURATION,\n                               \"interval_percent\": 12.3}}\n\n    EVENTS_HISTOGRAM = {'default': {\"table_file_creation\": 2},\n                        '_sample/CF_1': {},\n                        '_sample/CF-2': {},\n                        '': {'flush_started': 1,\n                             \"table_file_creation\": 1}}", "\n\nclass SampleRolledLogInfo:\n    FILE_PATH = \"input_files/Rolled_LOG_sample.txt\"\n    START_TIME = \"2022/04/17-14:13:10.723796\"\n    END_TIME = \"2022/04/17-14:14:32.645120\"\n    PRODUCT_NAME = \"SpeeDB\"\n    GIT_HASH = \"UNKNOWN:0a396684d6c08f6fe4a37572c0429d91176c51d1\"\n    VERSION = \"6.22.1\"\n    NUM_ENTRIES = 59\n    CF_NAMES = [\"default\", \"\", \"CF1\"]\n    AUTO_GENERATED_CF_NAMES = [\"Unknown-CF-#1\",\n                               \"Unknown-CF-#2\",\n                               \"Unknown-CF-#3\"]\n    DB_WIDE_OPTIONS_START_ENTRY_IDX = 7\n    SUPPORT_INFO_START_ENTRY_IDX = 15\n\n    NUM_WARNS = 1\n\n    OPTIONS_ENTRIES_INDICES = [19, 25, 32, 38]\n    TABLE_OPTIONS_ENTRIES_INDICES = [21, 29, 34, 42]\n\n    DB_WIDE_OPTIONS_DICT = SampleLogInfo.DB_WIDE_OPTIONS_DICT\n    DEFAULT_OPTIONS_DICT = SampleLogInfo.DEFAULT_OPTIONS_DICT\n    SAMPLE_CF1_OPTIONS_DICT = SampleLogInfo.SAMPLE_CF1_OPTIONS_DICT\n    SAMPLE_CF2_OPTIONS_DICT = SampleLogInfo.SAMPLE_CF2_OPTIONS_DICT\n    EMPTY_CF_OPTIONS_DICT = SampleLogInfo.EMPTY_CF_OPTIONS_DICT\n    DEFAULT_TABLE_OPTIONS_DICT = SampleLogInfo.DEFAULT_TABLE_OPTIONS_DICT\n    SAMPLE_CF1_TABLE_OPTIONS_DICT = SampleLogInfo.SAMPLE_CF1_TABLE_OPTIONS_DICT\n    SAMPLE_CF2_TABLE_OPTIONS_DICT = SampleLogInfo.SAMPLE_CF2_TABLE_OPTIONS_DICT\n    EMPTY_CF_TABLE_OPTIONS_DICT = SampleLogInfo.EMPTY_CF_TABLE_OPTIONS_DICT\n\n    OPTIONS_DICTS = [\n        DEFAULT_OPTIONS_DICT,\n        SAMPLE_CF1_OPTIONS_DICT,\n        SAMPLE_CF2_OPTIONS_DICT,\n        EMPTY_CF_OPTIONS_DICT\n    ]\n\n    TABLE_OPTIONS_DICTS = [\n        DEFAULT_TABLE_OPTIONS_DICT,\n        SAMPLE_CF1_TABLE_OPTIONS_DICT,\n        SAMPLE_CF2_TABLE_OPTIONS_DICT,\n        EMPTY_CF_TABLE_OPTIONS_DICT\n    ]\n\n    DB_STATS_ENTRY_TIME = \"2022/04/17-14:14:28.645150\"\n    CUMULATIVE_DURATION = \\\n        timedelta(hours=12, minutes=10, seconds=56, milliseconds=123)\n    INTERVAL_DURATION = \\\n        timedelta(hours=45, minutes=34, seconds=12, milliseconds=789)\n    DB_WIDE_STALLS_ENTRIES = \\\n        {DB_STATS_ENTRY_TIME: {\"cumulative_duration\": CUMULATIVE_DURATION,\n                               \"cumulative_percent\": 98.7,\n                               \"interval_duration\": INTERVAL_DURATION,\n                               \"interval_percent\": 12.3}}", ""]}
{"filename": "test/test_counters.py", "chunked_list": ["import counters\nfrom log_entry import LogEntry\nfrom test.testing_utils import \\\n    add_stats_entry_lines_to_counters_mngr\n\nadd_stats_entry_lines_to_mngr = \\\n    add_stats_entry_lines_to_counters_mngr\n\n\ndef test_stats_counter_and_histograms_is_your_entry():\n    lines = \\\n        '''2022/11/24-15:58:09.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n        rocksdb.block.cache.miss COUNT : 61\n        '''.splitlines()  # noqa\n\n    entry = LogEntry(0, lines[0])\n    entry.add_line(lines[1], True)\n\n    assert counters.CountersMngr.is_your_entry(entry)", "\ndef test_stats_counter_and_histograms_is_your_entry():\n    lines = \\\n        '''2022/11/24-15:58:09.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n        rocksdb.block.cache.miss COUNT : 61\n        '''.splitlines()  # noqa\n\n    entry = LogEntry(0, lines[0])\n    entry.add_line(lines[1], True)\n\n    assert counters.CountersMngr.is_your_entry(entry)", "\n\ndef test_counters_mngr():\n    entry_lines = \\\n        '''2022/11/24-15:58:09.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n         rocksdb.block.cache.miss COUNT : 61\n        rocksdb.block.cache.hit COUNT : 0\n        rocksdb.block.cache.add COUNT : 0\n        rocksdb.block.cache.data.miss COUNT : 71\n        rocksdb.db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0\n        rocksdb.read.block.compaction.micros P50 : 1.321429 P95 : 3.650000 P99 : 17.000000 P100 : 17.000000 COUNT : 67 SUM : 140\n        rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines()  # noqa\n\n    mngr = counters.CountersMngr()\n    assert mngr.get_counters_names() == []\n    assert mngr.get_counters_times() == []\n    assert mngr.get_histogram_counters_names() == []\n    assert mngr.get_histogram_counters_times() == []\n\n    add_stats_entry_lines_to_mngr(entry_lines, mngr)\n\n    assert mngr.get_counter_entries(\"rocksdb.block.cache.hit\") == \\\n           [{'time': '2022/11/24-15:58:09.512106',\n             'value': 0}]\n\n    assert mngr.get_counter_entries(\"rocksdb.block.cache.data.miss\") == \\\n           [{'time': '2022/11/24-15:58:09.512106',\n             'value': 71}]\n\n    assert mngr.get_counter_entries('XXXXX') == {}\n\n    assert mngr.get_counters_names() == ['rocksdb.block.cache.miss',\n                                         'rocksdb.block.cache.hit',\n                                         'rocksdb.block.cache.add',\n                                         'rocksdb.block.cache.data.miss']\n\n    assert mngr.get_counters_times() == ['2022/11/24-15:58:09.512106']\n\n    assert mngr.get_histogram_entries('rocksdb.db.get.micros') == \\\n           [{'time': '2022/11/24-15:58:09.512106',\n             'values': {'P100': 0.0,\n                        'P50': 0.0,\n                        'P95': 0.0,\n                        'P99': 0.0,\n                        'Count': 0,\n                        'Sum': 0,\n                        'Average': 0.0,\n                        'Interval Count': 0,\n                        'Interval Sum': 0}}]\n\n    assert \\\n        mngr.get_histogram_entries('rocksdb.read.block.compaction.micros') ==\\\n        [{'time': '2022/11/24-15:58:09.512106',\n          'values': {'P100': 17.0,\n                     'P50': 1.321429,\n                     'P95': 3.65,\n                     'P99': 17.0,\n                     'Count': 67,\n                     'Sum': 140,\n                     'Average': 2.09,\n                     'Interval Count': 67,\n                     'Interval Sum': 140}}]\n\n    assert mngr.get_histogram_counters_names() == [\n        'rocksdb.db.get.micros',\n        'rocksdb.read.block.compaction.micros',\n        'rocksdb.blobdb.next.micros'\n    ]\n    assert mngr.get_histogram_counters_times() == \\\n           ['2022/11/24-15:58:09.512106']\n\n    assert mngr.get_histogram_entries('YYYY') == {}\n\n    entry_lines1 = \\\n        '''2022/11/24-15:58:10.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n         rocksdb.block.cache.miss COUNT : 61\n        rocksdb.block.cache.hit COUNT : 10\n        rocksdb.block.cache.add COUNT : 20\n        rocksdb.block.cache.data.miss COUNT : 81\n        rocksdb.db.get.micros P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0\n        rocksdb.read.block.compaction.micros P50 : 1.321429 P95 : 3.650000 P99 : 17.000000 P100 : 17.000000 COUNT : 75 SUM : 180\n        rocksdb.blobdb.next.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines()  # noqa\n    entry1 = LogEntry(0, entry_lines1[0])\n    for line in entry_lines1[1:]:\n        entry1.add_line(line)\n    mngr.add_entry(entry1.all_lines_added())\n\n    assert mngr.get_all_counters_entries() == {\n        \"rocksdb.block.cache.miss\": [{'time': '2022/11/24-15:58:09.512106',\n                                      'value': 61},\n                                     {'time': '2022/11/24-15:58:10.512106',\n                                      'value': 61}],\n        \"rocksdb.block.cache.hit\": [{'time': '2022/11/24-15:58:09.512106',\n                                     'value': 0},\n                                    {'time': '2022/11/24-15:58:10.512106',\n                                     'value': 10}],\n        \"rocksdb.block.cache.add\": [{'time': '2022/11/24-15:58:09.512106',\n                                     'value': 0},\n                                    {'time': '2022/11/24-15:58:10.512106',\n                                     'value': 20}],\n        \"rocksdb.block.cache.data.miss\":\n            [{'time': '2022/11/24-15:58:09.512106',\n              'value': 71},\n             {'time': '2022/11/24-15:58:10.512106',\n              'value': 81}]\n    }\n\n    assert mngr.get_all_histogram_entries() == {\n        \"rocksdb.db.get.micros\":\n            [{'time': '2022/11/24-15:58:09.512106',\n              'values': {'P100': 0.0,\n                         'P50': 0.0,\n                         'P95': 0.0,\n                         'P99': 0.0,\n                         'Average': 0.0,\n                         'Count': 0,\n                         'Sum': 0,\n                         'Interval Count': 0,\n                         'Interval Sum': 0}},\n             {'time': '2022/11/24-15:58:10.512106',\n              'values': {'P100': 0.0,\n                         'P50': 0.0,\n                         'P95': 0.0,\n                         'P99': 0.0,\n                         'Average': 0.0,\n                         'Count': 0,\n                         'Sum': 0,\n                         'Interval Count': 0,\n                         'Interval Sum': 0}}],\n        \"rocksdb.read.block.compaction.micros\":\n            [{'time': '2022/11/24-15:58:09.512106',\n              'values': {'P100': 17.0,\n                         'P50': 1.321429,\n                         'P95': 3.65,\n                         'P99': 17.0,\n                         'Average': 2.09,\n                         'Count': 67,\n                         'Sum': 140,\n                         'Interval Count': 67,\n                         'Interval Sum': 140}},\n             {'time': '2022/11/24-15:58:10.512106',\n              'values': {'P100': 17.0,\n                         'P50': 1.321429,\n                         'P95': 3.65,\n                         'P99': 17.0,\n                         'Average': 2.4,\n                         'Count': 75,\n                         'Sum': 180,\n                         'Interval Count': 8,\n                         'Interval Sum': 40}}],\n        \"rocksdb.blobdb.next.micros\":\n            [{'time': '2022/11/24-15:58:09.512106',\n              'values': {'P100': 0.0,\n                         'P50': 0.0,\n                         'P95': 0.0,\n                         'P99': 0.0,\n                         'Average': 0.0,\n                         'Count': 0,\n                         'Sum': 0,\n                         'Interval Count': 0,\n                         'Interval Sum': 0}},\n             {'time': '2022/11/24-15:58:10.512106',\n              'values': {'P100': 0.0,\n                         'P50': 0.0,\n                         'P95': 0.0,\n                         'P99': 0.0,\n                         'Average': 0.0,\n                         'Count': 0,\n                         'Sum': 0,\n                         'Interval Count': 0,\n                         'Interval Sum': 0}}]\n    }\n    assert mngr.get_histogram_counters_names() == [\n        'rocksdb.db.get.micros',\n        'rocksdb.read.block.compaction.micros',\n        'rocksdb.blobdb.next.micros'\n    ]\n    assert mngr.get_histogram_counters_times() == [\n        '2022/11/24-15:58:09.512106',\n        '2022/11/24-15:58:10.512106'\n    ]", "\n\ndef test_counters_zero_handling():\n    # Just for testing, allowing the counter to return to 0 after being > 0\n    entry_lines = [\n        '''2022/11/24-15:50:08.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 COUNT : 0'''.splitlines(),\n        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 COUNT : 0'''.splitlines(),\n        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 COUNT : 10'''.splitlines(),\n        '''2022/11/24-15:50:11.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 COUNT : 11'''.splitlines(),\n        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 COUNT : 0'''.splitlines()\n    ]\n\n    mngr = counters.CountersMngr()\n\n    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == []\n    assert mngr.are_all_counter_entries_zero(\"counter1\")\n    assert mngr.get_counters_entries_not_all_zeroes() == {}\n\n    add_stats_entry_lines_to_mngr(entry_lines[0], mngr)\n    assert mngr.are_all_counter_entries_zero(\"counter1\")\n    assert mngr.get_counters_entries_not_all_zeroes() == {}\n\n    add_stats_entry_lines_to_mngr(entry_lines[1], mngr)\n    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == []\n    assert mngr.are_all_counter_entries_zero(\"counter1\")\n    assert mngr.get_counters_entries_not_all_zeroes() == {}\n\n    add_stats_entry_lines_to_mngr(entry_lines[2], mngr)\n    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == \\\n           [{'time': '2022/11/24-15:50:10.512106', 'value': 10}]\n    assert not mngr.are_all_counter_entries_zero(\"counter1\")\n    assert mngr.get_counters_entries_not_all_zeroes() == \\\n           {'counter1': [{'time': '2022/11/24-15:50:08.512106', 'value': 0},\n                         {'time': '2022/11/24-15:50:09.512106', 'value': 0},\n                         {'time': '2022/11/24-15:50:10.512106', 'value': 10}]}\n\n    add_stats_entry_lines_to_mngr(entry_lines[3], mngr)\n    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == \\\n           [{'time': '2022/11/24-15:50:10.512106', 'value': 10},\n            {'time': '2022/11/24-15:50:11.512106', 'value': 11}]\n    assert not mngr.are_all_counter_entries_zero(\"counter1\")\n    assert mngr.get_counters_entries_not_all_zeroes() == \\\n           {'counter1': [{'time': '2022/11/24-15:50:08.512106', 'value': 0},\n                         {'time': '2022/11/24-15:50:09.512106', 'value': 0},\n                         {'time': '2022/11/24-15:50:10.512106', 'value': 10},\n                         {'time': '2022/11/24-15:50:11.512106', 'value': 11}]}\n\n    add_stats_entry_lines_to_mngr(entry_lines[4], mngr)\n    assert mngr.get_non_zeroes_counter_entries(\"counter1\") == \\\n           [{'time': '2022/11/24-15:50:10.512106', 'value': 10},\n            {'time': '2022/11/24-15:50:11.512106', 'value': 11}]\n    assert not mngr.are_all_counter_entries_zero(\"counter1\")\n    assert mngr.get_counters_entries_not_all_zeroes() == \\\n           {'counter1': [{'time': '2022/11/24-15:50:08.512106', 'value': 0},\n                         {'time': '2022/11/24-15:50:09.512106', 'value': 0},\n                         {'time': '2022/11/24-15:50:10.512106', 'value': 10},\n                         {'time': '2022/11/24-15:50:11.512106', 'value': 11}]}", "\n\ndef test_histograms_zero_handling():\n    # Just for testing, allowing the counter to return to 0 after being > 0\n    entry_lines = [\n        '''2022/11/24-15:50:08.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines(), # noqa\n        '''2022/11/24-15:50:09.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 P50 : .000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines(), # noqa\n        '''2022/11/24-15:50:10.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 1 SUM : 10'''.splitlines(), # noqa\n        '''2022/11/24-15:50:11.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 7 SUM : 24'''.splitlines(), # noqa\n        '''2022/11/24-15:50:12.512106 32851 [db_impl.cc:761] STATISTICS:\n        counter1 P50 : 1.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0'''.splitlines() # noqa\n    ]\n\n    mngr = counters.CountersMngr()\n\n    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == []\n    assert mngr.are_all_histogram_entries_zero(\"counter1\")\n    assert mngr.get_histogram_entries_not_all_zeroes() == {}\n\n    add_stats_entry_lines_to_mngr(entry_lines[0], mngr)\n    assert mngr.are_all_histogram_entries_zero(\"counter1\")\n    assert mngr.get_histogram_entries_not_all_zeroes() == {}\n\n    add_stats_entry_lines_to_mngr(entry_lines[1], mngr)\n    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == []\n    assert mngr.are_all_histogram_entries_zero(\"counter1\")\n    assert mngr.get_histogram_entries_not_all_zeroes() == {}\n\n    add_stats_entry_lines_to_mngr(entry_lines[2], mngr)\n    expected_non_zero_entries = \\\n        [{'time': '2022/11/24-15:50:10.512106',\n          'values': {'P100': 0.0,\n                     'P50': 1.0,\n                     'P95': 0.0,\n                     'P99': 0.0,\n                     'Average': 10.0,\n                     'Count': 1,\n                     'Sum': 10,\n                     'Interval Count': 1,\n                     'Interval Sum': 10}}]\n    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == \\\n           expected_non_zero_entries\n\n    assert not mngr.are_all_histogram_entries_zero(\"counter1\")\n    expected_not_all_zeroes = {\n        'counter1':\n            [{'time': '2022/11/24-15:50:08.512106',\n              'values': {'P100': 0.0,\n                         'P50': 0.0,\n                         'P95': 0.0,\n                         'P99': 0.0,\n                         'Average': 0.0,\n                         'Count': 0,\n                         'Sum': 0,\n                         'Interval Count': 0,\n                         'Interval Sum': 0}},\n             {'time': '2022/11/24-15:50:09.512106',\n              'values': {'P100': 0.0,\n                         'P50': 0.0,\n                         'P95': 0.0,\n                         'P99': 0.0,\n                         'Average': 0.0,\n                         'Count': 0,\n                         'Sum': 0,\n                         'Interval Count': 0,\n                         'Interval Sum': 0}},\n             {'time': '2022/11/24-15:50:10.512106',\n              'values': {'P100': 0.0,\n                         'P50': 1.0,\n                         'P95': 0.0,\n                         'P99': 0.0,\n                         'Average': 10.0,\n                         'Count': 1,\n                         'Sum': 10,\n                         'Interval Count': 1,\n                         'Interval Sum': 10}}\n             ]\n    }\n    assert mngr.get_histogram_entries_not_all_zeroes() == \\\n           expected_not_all_zeroes\n\n    add_stats_entry_lines_to_mngr(entry_lines[3], mngr)\n    expected_non_zero_entries.append(\n        {'time': '2022/11/24-15:50:11.512106',\n         'values': {'P100': 0.0,\n                    'P50': 1.0,\n                    'P95': 0.0,\n                    'P99': 0.0,\n                    'Average': 3.43,\n                    'Count': 7,\n                    'Sum': 24,\n                    'Interval Count': 6,\n                    'Interval Sum': 14}})\n    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == \\\n           expected_non_zero_entries\n\n    assert not mngr.are_all_histogram_entries_zero(\"counter1\")\n    expected_not_all_zeroes[\"counter1\"].append(\n        {'time': '2022/11/24-15:50:11.512106',\n         'values': {'P100': 0.0,\n                    'P50': 1.0,\n                    'P95': 0.0,\n                    'P99': 0.0,\n                    'Average': 3.43,\n                    'Count': 7,\n                    'Sum': 24,\n                    'Interval Count': 6,\n                    'Interval Sum': 14}})\n    assert mngr.get_histogram_entries_not_all_zeroes() == \\\n           expected_not_all_zeroes\n\n    # Line 4's count / sum are < line 3 => Line is ignored\n    add_stats_entry_lines_to_mngr(entry_lines[4], mngr)\n    assert mngr.get_non_zeroes_histogram_entries(\"counter1\") == \\\n           expected_non_zero_entries\n    assert not mngr.are_all_histogram_entries_zero(\"counter1\")\n    assert mngr.get_histogram_entries_not_all_zeroes() == \\\n           expected_not_all_zeroes", "\n\ndef test_badly_formed_counters_and_histograms_entries():\n    # There is only a single valid counter line and a single histogram line\n    # Issues:\n    # - spaces within a counter / histogram name\n    # - missing values\n    # - Unexpected text at the end of the line\n    entry_lines = \\\n        '''2022/11/24-15:58:09.512106 32851 [/db_impl/db_impl.cc:761] STATISTICS:\n        rocksdb.block.cache.miss COUNT : 61\n        rocksdb. block.cache.hit COUNT : 100\n        rocksdb.block.cache.XXX COUNT : \n        rocksdb.block.cache.YYY COUNT : 61  UNEXPECTED TEXT\n        rocksdb. db.get.micros P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM : 0\n        rocksdb.db.get.micros.XXX P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM :\n        rocksdb.db.get.micros.XXX P50 : 0.000000 P95 : 0.000000 P99 : 0.000000 P100 : 0.000000 COUNT : 0 SUM :            UNEXPECTED TEXT\n        rocksdb.read.block.compaction.micros P50 : 1.321429 P95 : 3.650000 P99 : 17.000000 P100 : 17.000000 COUNT : 75 SUM : 180'''.splitlines()  # noqa\n\n    entry = LogEntry(0, entry_lines[0])\n    for line in entry_lines[1:]:\n        entry.add_line(line)\n\n    mngr = counters.CountersMngr()\n    mngr.add_entry(entry.all_lines_added())\n\n    assert mngr.get_all_counters_entries() == \\\n           {\"rocksdb.block.cache.miss\": [{'time': '2022/11/24-15:58:09.512106',\n                                          'value': 61}]}\n\n    assert mngr.get_all_histogram_entries() ==\\\n           {\"rocksdb.read.block.compaction.micros\":\n            [{'time': '2022/11/24-15:58:09.512106',\n              'values': {'P100': 17.0,\n                         'P50': 1.321429,\n                         'P95': 3.65,\n                         'P99': 17.0,\n                         'Average': 2.4,\n                         'Count': 75,\n                         'Sum': 180,\n                         'Interval Count': 75,\n                         'Interval Sum': 180}}]\n            }", ""]}
{"filename": "test/test_events.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nfrom datetime import datetime, timedelta\n\nimport pytest\n", "import pytest\n\nimport events\nimport utils\nfrom log_entry import LogEntry\nfrom test.testing_utils import create_event_entry, create_event, \\\n    entry_to_event, entry_msg_to_entry\n\n\ndef test_try_parse_as_flush_preamble():\n    cf = \"cf1\"\n    job_id = 38\n    wal_id = 55\n\n    valid_preamble = \\\n        f\"[{cf}] [JOB {job_id}] Flushing memtable with next log file: {wal_id}\"\n    partial1 = f\"[{cf}] [JOB {job_id}] Flushing\"\n    partial2 = f\"[{cf}] [JOB {job_id}] Flushing memtable with next log file:\"\n\n    test_func = events.FlushStartedEvent.try_parse_as_preamble\n    assert test_func(\"\") == (False, None, None, None)\n    assert test_func(valid_preamble) == (True, cf, job_id, wal_id)\n    assert test_func(partial1) == (False, None, None, None)\n    assert test_func(partial2) == (False, None, None, None)", "\ndef test_try_parse_as_flush_preamble():\n    cf = \"cf1\"\n    job_id = 38\n    wal_id = 55\n\n    valid_preamble = \\\n        f\"[{cf}] [JOB {job_id}] Flushing memtable with next log file: {wal_id}\"\n    partial1 = f\"[{cf}] [JOB {job_id}] Flushing\"\n    partial2 = f\"[{cf}] [JOB {job_id}] Flushing memtable with next log file:\"\n\n    test_func = events.FlushStartedEvent.try_parse_as_preamble\n    assert test_func(\"\") == (False, None, None, None)\n    assert test_func(valid_preamble) == (True, cf, job_id, wal_id)\n    assert test_func(partial1) == (False, None, None, None)\n    assert test_func(partial2) == (False, None, None, None)", "\n\ndef test_try_parse_as_compaction_preamble():\n    cf = \"cf2\"\n    job_id = 157\n\n    valid_preamble = \\\n        f\"[{cf}] [JOB {job_id}] Compacting 1@1 + 5@2 files to L2, score 1.63\"\n    partial1 = \\\n        f\"[{cf}] [JOB {job_id}] Compacting\"\n    partial2 = \\\n        f\"[{cf}] [JOB {job_id}] Compacting 1@1 + 5@2 files\"\n\n    test_func = events.CompactionStartedEvent.try_parse_as_preamble\n    assert test_func(\"\") == (False, None, None)\n    assert test_func(valid_preamble) == (True, cf, job_id)\n    assert test_func(partial1) == (False, None, None)\n    assert test_func(partial2) == (False, None, None)", "\n\ndef test_try_parse_event_preamble():\n    cf = \"cf1\"\n    job_id = 38\n    wal_id = 55\n    time_str = \"2022/04/17-14:42:19.220573\"\n\n    valid_flush_preamble = \\\n        f\"[{cf}] [JOB {job_id}] Flushing memtable with next log file: {wal_id}\"\n    flush_preamble_entry = entry_msg_to_entry(time_str, valid_flush_preamble)\n\n    valid_compaction_preamble = \\\n        f\"[{cf}] [JOB {job_id}] Compacting 1@1 + 5@2 files to L2, score 1.63\"\n    compaction_preamble_entry =\\\n        entry_msg_to_entry(time_str, valid_compaction_preamble)\n\n    partial_compaction_preamble = f\"[{cf}] [JOB {job_id}] Compacting\"\n    invalid_compaction_preamble_entry =\\\n        entry_msg_to_entry(time_str, partial_compaction_preamble)\n\n    test_func = events.Event.try_parse_as_preamble\n    info = events.Event.EventPreambleInfo\n\n    assert test_func(flush_preamble_entry) == \\\n           info(cf, events.EventType.FLUSH_STARTED, job_id, wal_id=wal_id)\n    assert test_func(compaction_preamble_entry) == \\\n           info(cf, events.EventType.COMPACTION_STARTED, job_id, wal_id=None)\n\n    assert test_func(invalid_compaction_preamble_entry) is None", "\n\ndef test_compaction_started_event():\n    time = \"2023/01/24-08:54:40.130553\"\n    job_id = 1\n    cf = \"cf1\"\n    cf_names = [cf]\n    reason = \"Reason1\"\n    files_l1 = [17248]\n    files_l2 = [16778, 16779, 16780, 16781, 17022]\n\n    event = create_event(job_id, cf_names, time,\n                         events.EventType.COMPACTION_STARTED, cf,\n                         compaction_reason=reason,\n                         files_L1=files_l1, files_L2=files_l2)\n\n    assert isinstance(event, events.CompactionStartedEvent)\n    assert event.get_compaction_reason() == reason\n    assert event.get_input_files() == {\n        1: files_l1,\n        2: files_l2\n    }", "\n\ndef test_table_file_creation_event():\n    time = \"2023/01/24-08:54:40.130553\"\n    file_number = 1234\n    job_id = 1\n    cf = \"cf1\"\n    cf_names = [cf]\n    table_properties = {}\n    event = create_event(job_id, cf_names, time,\n                         events.EventType.TABLE_FILE_CREATION, cf,\n                         file_number=file_number,\n                         table_properties=table_properties)\n\n    assert event.get_type() == events.EventType.TABLE_FILE_CREATION.value\n    assert event.get_created_file_number() == file_number\n    assert event.get_cf_id() is None\n    assert event.get_compressed_data_size_bytes() == 0\n    assert event.get_num_data_blocks() == 0\n    assert event.get_total_keys_sizes_bytes() == 0\n    assert event.get_total_values_sizes_bytes() == 0\n    assert event.get_data_size_bytes() == 0\n    assert event.get_index_size_bytes() == 0\n    assert event.get_filter_size_bytes() == 0\n    assert not event.does_use_filter()\n    assert event.get_num_filter_entries() == 0\n    assert event.get_compression_type() is None\n\n    table_properties2 = {\n        \"column_family_id\": 1,\n        \"data_size\": 100,\n        \"index_size\": 200,\n        \"filter_size\": 300,\n        \"raw_key_size\": 400,\n        \"raw_value_size\": 500,\n        \"num_data_blocks\": 600,\n        \"num_entries\": 700,\n        \"compression\": \"NoCompression\",\n        \"filter_policy\": \"BloomFilter\",\n        \"num_filter_entries\": 800}\n\n    event2 = create_event(job_id, cf_names, time,\n                          events.EventType.TABLE_FILE_CREATION, cf,\n                          file_number=file_number,\n                          table_properties=table_properties2)\n\n    assert event2.get_type() == events.EventType.TABLE_FILE_CREATION.value\n    assert event2.get_created_file_number() == file_number\n    assert event2.get_cf_id() == 1\n    assert event2.get_compressed_data_size_bytes() == 100\n    assert event2.get_num_data_blocks() == 600\n    assert event2.get_total_keys_sizes_bytes() == 400\n    assert event2.get_total_values_sizes_bytes() == 500\n    assert event2.get_data_size_bytes() == 900\n    assert event2.get_index_size_bytes() == 200\n    assert event2.get_filter_size_bytes() == 300\n    assert event2.does_use_filter()\n    assert event2.get_filter_policy() == \"BloomFilter\"\n    assert event2.get_num_filter_entries() == 800\n    assert event2.get_compression_type() == \"NoCompression\"", "\n\ndef test_table_file_deletion_event():\n    time = \"2023/01/24-08:54:40.130553\"\n    file_number = 1234\n    job_id = 1\n    cf = \"cf1\"\n    cf_names = [cf]\n    event = create_event(job_id, cf_names, time,\n                         events.EventType.TABLE_FILE_DELETION, cf,\n                         file_number=file_number)\n    assert event.get_type() == events.EventType.TABLE_FILE_DELETION.value\n    assert event.get_deleted_file_number() == file_number", "\n\ndef test_table_file_creation_event1():\n    time = \"2023/01/24-08:54:40.130553\"\n    file_number = 1234\n    job_id = 1\n    cf = \"cf1\"\n    cf_names = [cf]\n    cf_id = 100\n    compressed_data_size_bytes = 62396458\n    num_data_blocks = 1000\n    total_keys_sizes_bytes = 2000\n    total_values_sizes_bytes = 3333\n    index_size = 3000\n    filter_size = 4000\n    num_entries = 5555\n    compression_type = \"NoCompression\"\n    table_properties = {\n        \"data_size\": compressed_data_size_bytes,\n        \"index_size\": index_size,\n        \"index_partitions\": 0,\n        \"top_level_index_size\": 0,\n        \"index_key_is_user_key\": 1,\n        \"index_value_is_delta_encoded\": 1,\n        \"filter_size\": filter_size,\n        \"raw_key_size\": total_keys_sizes_bytes,\n        \"raw_average_key_size\": 24,\n        \"raw_value_size\": total_values_sizes_bytes,\n        \"raw_average_value_size\": 1000,\n        \"num_data_blocks\": num_data_blocks,\n        \"num_entries\": num_entries,\n        \"num_filter_entries\": 60774,\n        \"num_deletions\": 0,\n        \"num_merge_operands\": 0,\n        \"num_range_deletions\": 0,\n        \"format_version\": 0,\n        \"fixed_key_len\": 0,\n        \"filter_policy\": \"bloomfilter\",\n        \"column_family_name\": \"default\",\n        \"column_family_id\": cf_id,\n        \"comparator\": \"leveldb.BytewiseComparator\",\n        \"merge_operator\": \"nullptr\",\n        \"prefix_extractor_name\": \"nullptr\",\n        \"property_collectors\": \"[]\",\n        \"compression\": compression_type,\n        \"oldest_key_time\": 1672823099,\n        \"file_creation_time\": 1672823099,\n        \"slow_compression_estimated_data_size\": 0,\n        \"fast_compression_estimated_data_size\": 0,\n        \"db_id\": \"c100448c-dc04-4c74-8ab2-65d72f3aa3a8\",\n        \"db_session_id\": \"4GAWIG5RIF8PQWM3NOQG\",\n        \"orig_file_number\": 37155}\n\n    event = create_event(job_id, cf_names, time,\n                         events.EventType.TABLE_FILE_CREATION, cf,\n                         file_number=file_number,\n                         table_properties=table_properties)\n\n    assert event.get_type() == events.EventType.TABLE_FILE_CREATION.value\n    assert event.get_created_file_number() == file_number\n    assert event.get_cf_id() == cf_id\n    assert event.get_compressed_data_size_bytes() == compressed_data_size_bytes\n    assert event.get_num_data_blocks() == num_data_blocks\n    assert event.get_total_keys_sizes_bytes() == total_keys_sizes_bytes\n    assert event.get_total_values_sizes_bytes() == total_values_sizes_bytes\n    assert event.get_data_size_bytes() ==\\\n           total_keys_sizes_bytes + total_values_sizes_bytes\n    assert event.get_index_size_bytes() == index_size\n    assert event.get_filter_size_bytes() == filter_size\n    assert event.get_compression_type() == compression_type", "\n\ndef verify_expected_events(events_mngr, expected_events_dict):\n    # Expecting a dictionary of:\n    # {<cf_name>: [<events entries for this cf>]}\n    cf_names = list(expected_events_dict.keys())\n\n    # prepare the expected events per (cf, event type)\n    expected_cf_events_per_type = dict()\n    for name in cf_names:\n        expected_cf_events_per_type[name] = {event_type: [] for event_type\n                                             in events.EventType}\n\n    for cf_name, cf_events_entries in expected_events_dict.items():\n        expected_cf_events = \\\n            [events.Event(event_entry) for event_entry in\n             cf_events_entries]\n        assert events_mngr.get_cf_events(cf_name) == expected_cf_events\n\n        for event in expected_cf_events:\n            expected_cf_events_per_type[cf_name][event.get_type()].append(\n                event)\n\n        for event_type in events.EventType:\n            assert events_mngr.get_cf_events_by_type(cf_name, event_type) ==\\\n                   expected_cf_events_per_type[cf_name][event_type]", "\n\ndef test_event_type():\n    assert events.EventType.type_from_str(\"flush_finished\") == \\\n           events.EventType.FLUSH_FINISHED\n    assert str(events.EventType.type_from_str(\"flush_finished\")) == \\\n           \"flush_finished\"\n\n    assert events.EventType.type_from_str(\"Dummy\") == events.EventType.UNKNOWN\n    assert str(events.EventType.type_from_str(\"Dummy\")) == \"UNKNOWN\"", "\n\ndef test_get_matching_type_info_if_exists():\n    assert events.Event.\\\n           get_matching_type_info_if_exists(\n            events.EventType.FLUSH_STARTED) == \\\n           events.MatchingEventTypeInfo(\n               events.EventType.FLUSH_FINISHED, events.FlowType.FLUSH, False)\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.FLUSH_FINISHED) == events.MatchingEventTypeInfo(\n        events.EventType.FLUSH_STARTED, events.FlowType.FLUSH, True)\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.COMPACTION_STARTED) == \\\n        events.MatchingEventTypeInfo(\n            events.EventType.COMPACTION_FINISHED, events.FlowType.COMPACTION,\n            False)\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.COMPACTION_FINISHED) == \\\n        events.MatchingEventTypeInfo(events.EventType.COMPACTION_STARTED,\n                                     events.FlowType.COMPACTION, True)\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.RECOVERY_STARTED) == \\\n        events.MatchingEventTypeInfo(events.EventType.RECOVERY_FINISHED,\n                                     events.FlowType.RECOVERY, False)\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.RECOVERY_FINISHED) == \\\n        events.MatchingEventTypeInfo(events.EventType.RECOVERY_STARTED,\n                                     events.FlowType.RECOVERY, True)\n\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.TABLE_FILE_CREATION) is None\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.TABLE_FILE_DELETION) is None\n    assert events.Event.\\\n           get_matching_type_info_if_exists(events.EventType.TRIVIAL_MOVE) \\\n           is None\n    assert events.Event.\\\n           get_matching_type_info_if_exists(events.EventType.INGEST_FINISHED) \\\n           is None\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.BLOB_FILE_CREATION) is None\n    assert events.Event.get_matching_type_info_if_exists(\n        events.EventType.BLOB_FILE_DELETION) is None\n    assert events.Event.\\\n           get_matching_type_info_if_exists(events.EventType.UNKNOWN) is None", "\n\ndef test_matching_event_info_get_duration_ms():\n    cf_names = [\"default\"]\n    job_id = 1\n    start_event_time = \"2023/01/04-08:54:59.130996\"\n    start_event = create_event(\n        job_id, cf_names, start_event_time,\n        event_type=events.EventType.FLUSH_STARTED, flush_reason=\"Reason1\")\n    start_event_info = \\\n        events.MatchingEventInfo(start_event, events.FlowType.FLUSH,\n                                 is_start=True)\n\n    start_datetime = datetime.strptime(f\"{start_event_time}GMT\",\n                                       \"%Y/%m/%d-%H:%M:%S.%f%Z\")\n    delta_ms = 1500\n    time_delta_ms = timedelta(milliseconds=delta_ms)\n    end_datetime = start_datetime + time_delta_ms\n    end_event_time = end_datetime.strftime(\"%Y/%m/%d-%H:%M:%S.%f\")\n    end_event = create_event(\n        job_id, cf_names, end_event_time,\n        event_type=events.EventType.FLUSH_FINISHED)\n    end_event_info = events.MatchingEventInfo(\n        end_event, events.FlowType.FLUSH, is_start=False)\n\n    assert start_event_info.get_duration_ms(end_event_info) == delta_ms\n    assert end_event_info.get_duration_ms(start_event_info) == delta_ms", "\n\n@pytest.mark.parametrize(\"cf_name\", [\"default\", \"\"])\ndef test_event(cf_name):\n    job_id = 35\n    event_entry = create_event_entry(job_id, \"2022/04/17-14:42:19.220573\",\n                                     events.EventType.FLUSH_FINISHED, cf_name)\n\n    assert events.Event.is_an_event_entry(event_entry)\n    assert not events.Event.try_parse_as_preamble(event_entry)\n\n    event1 = events.Event(event_entry)\n    assert event1.get_type() == events.EventType.FLUSH_FINISHED\n    assert event1.get_job_id() == 35\n    assert event1.get_cf_name() == cf_name\n    assert not event1.is_db_wide_event()\n    assert event1.is_cf_event()\n    assert event1.get_my_matching_type_info_if_exists() == \\\n           events.MatchingEventTypeInfo(\n               events.EventType.FLUSH_STARTED, events.FlowType.FLUSH, True)\n    event1 = None\n\n    # Unknown event (legal)\n    event_entry = create_event_entry(job_id, \"2022/04/17-14:42:19.220573\",\n                                     events.EventType.UNKNOWN, cf_name)\n    event2 = events.Event(event_entry)\n    assert event2.get_type() == events.EventType.UNKNOWN\n    assert event2.get_job_id() == 35\n    assert event2.get_cf_name() == cf_name\n    assert not event2.is_db_wide_event()\n    assert event2.is_cf_event()\n    assert event2.get_my_matching_type_info_if_exists() is None", "\n\n@pytest.mark.parametrize(\"cf_name\", [\"default\", \"\"])\ndef test_event_preamble(cf_name):\n    preamble_line = f\"\"\"2022/04/17-14:42:11.398681 7f4a8b5bb700 \n    [/flush_job.cc:333] [{cf_name}] [JOB 8] \n    Flushing memtable with next log file: 5\n    \"\"\" # noqa\n\n    preamble_line = \" \".join(preamble_line.splitlines())\n\n    cf_names = [cf_name, \"dummy_cf\"]\n    job_id = 8\n\n    preamble_entry = LogEntry(0, preamble_line, True)\n    event_entry = create_event_entry(job_id, \"2022/11/24-15:58:17.683316\",\n                                     events.EventType.FLUSH_STARTED,\n                                     utils.NO_CF, flush_reason=\"REASON1\")\n\n    assert not events.Event.try_parse_as_preamble(event_entry)\n\n    preamble_info = \\\n        events.Event.try_parse_as_preamble(preamble_entry)\n    assert preamble_info\n    assert preamble_info.job_id == 8\n    assert preamble_info.type == events.EventType.FLUSH_STARTED\n    assert preamble_info.cf_name == cf_name\n\n    assert events.Event.is_an_event_entry(event_entry)\n    assert not events.Event.try_parse_as_preamble(event_entry)\n\n    event = events.Event.create_event(event_entry)\n    assert event.get_type() == events.EventType.FLUSH_STARTED\n    assert event.get_job_id() == 8\n    assert event.get_cf_name() == utils.NO_CF\n    assert event.is_db_wide_event()\n    assert not event.is_cf_event()\n    assert event.get_wal_id_if_available() is None\n    assert event.get_flush_reason() == \"REASON1\"\n\n    assert event.try_adding_preamble_event(preamble_info)\n    assert event.get_cf_name() == cf_name\n    assert not event.is_db_wide_event()\n    assert event.is_cf_event()\n    assert event.get_wal_id_if_available() == 5\n    assert event.get_my_matching_type_info_if_exists() == \\\n           events.MatchingEventTypeInfo(\n               events.EventType.FLUSH_FINISHED, events.FlowType.FLUSH, False)\n    assert event.get_matching_event_info_if_exists() is None\n\n    compaction_finished_event = \\\n        create_event(job_id, cf_names, \"2022/11/24-15:59:17.683316\",\n                     events.EventType.COMPACTION_FINISHED, cf_name)\n    event.try_adding_matching_event(compaction_finished_event)\n    assert event.get_matching_event_info_if_exists() is None\n\n    flush_end_event = create_event(job_id, cf_names,\n                                   \"2022/11/24-15:59:17.683316\",\n                                   events.EventType.FLUSH_FINISHED,\n                                   cf_name)\n    event.try_adding_matching_event(flush_end_event)\n    assert event.get_matching_event_info_if_exists() ==\\\n           events.MatchingEventInfo(flush_end_event, events.FlowType.FLUSH,\n                                    is_start=False)", "\n\ndef test_illegal_events():\n    cf1 = \"CF1\"\n\n    # Illegal event json\n    event_entry = create_event_entry(35, \"2022/04/17-14:42:19.220573\",\n                                     events.EventType.FLUSH_FINISHED, cf1,\n                                     make_illegal_json=True)\n    assert events.Event.try_parse_as_preamble(event_entry) is None\n    with pytest.raises(utils.ParsingError):\n        events.Event.create_event(event_entry)\n\n    # Missing Job id (illegal)\n    event_entry = create_event_entry(None, \"2022/04/17-14:42:19.220573\",\n                                     events.EventType.FLUSH_FINISHED,\n                                     cf_name=cf1)\n    assert events.Event.try_parse_as_preamble(event_entry) is None\n    assert events.Event.is_an_event_entry(event_entry)\n    with pytest.raises(utils.ParsingError):\n        events.Event.create_event(event_entry)\n\n    # Missing events.Event Type (definitely illegal)\n    event_entry = create_event_entry(200, \"2022/04/17-14:42:19.220573\",\n                                     event_type=None, cf_name=cf1)\n    assert events.Event.try_parse_as_preamble(event_entry) is None\n    assert events.Event.is_an_event_entry(event_entry)\n    with pytest.raises(utils.ParsingError):\n        events.Event.create_event(event_entry)", "\n\ndef test_handling_same_job_id_multiple_cfs():\n    cf1 = \"cf1\"\n    cf2 = \"cf2\"\n    event1_time = \"2023/01/04-08:54:59.130996\"\n    event2_time = \"2023/01/04-08:55:59.130996\"\n    event3_time = \"2023/01/04-08:56:59.130996\"\n    job_id = 1\n    job_id_to_cf_name_map = {job_id: cf1}\n\n    events_mngr = events.EventsMngr(job_id_to_cf_name_map)\n\n    event_entry_cf1 = create_event_entry(job_id, event1_time,\n                                         events.EventType.FLUSH_STARTED, cf1,\n                                         flush_reason=\"FLUSH_REASON1\")\n    event_cf1 = entry_to_event(event_entry_cf1)\n    assert events_mngr.try_adding_entry(event_entry_cf1) ==\\\n           (True, event_cf1, cf1)\n\n    event_entry_cf2 = create_event_entry(job_id, event2_time,\n                                         events.EventType.FLUSH_FINISHED, cf2)\n    assert events_mngr.try_adding_entry(event_entry_cf2) == (True, None, None)\n\n    # Now adding without a cf => should match the event to the 1st one\n    event_entry_cf3 = create_event_entry(job_id, event3_time,\n                                         events.EventType.FLUSH_FINISHED,\n                                         utils.NO_CF)\n    event_no_cf = entry_to_event(event_entry_cf3)\n    event_no_cf.set_cf_name(cf1)\n    assert events_mngr.try_adding_entry(event_entry_cf3) == \\\n           (True, event_no_cf, cf1)", "\n\ndef test_adding_events_to_events_mngr():\n    cf1 = \"cf1\"\n    job_id1 = 1\n    cf2 = \"cf2\"\n    job_id2 = 2\n    job_id_to_cf_name_map = {job_id1: cf1, job_id2: cf2}\n    events_mngr = events.EventsMngr(job_id_to_cf_name_map)\n\n    assert not events_mngr.get_cf_events(cf1)\n    assert not events_mngr.\\\n        get_cf_events_by_type(cf2, events.EventType.FLUSH_FINISHED)\n\n    expected_events_entries = {cf1: [], cf2: []}\n\n    event1_entry = create_event_entry(job_id1, \"2022/04/17-14:42:19.220573\",\n                                      events.EventType.FLUSH_FINISHED, cf1)\n    event1 = entry_to_event(event1_entry)\n    assert events_mngr.try_adding_entry(event1_entry) == (True, event1, cf1)\n    expected_events_entries[cf1] = [event1_entry]\n    verify_expected_events(events_mngr, expected_events_entries)\n\n    event2_entry = create_event_entry(job_id2, \"2022/04/18-14:42:19.220573\",\n                                      events.EventType.FLUSH_STARTED, cf2,\n                                      flush_reason=\"FLUSH_REASON1\")\n    event2 = entry_to_event(event2_entry)\n    assert events_mngr.try_adding_entry(event2_entry) == (True, event2, cf2)\n    expected_events_entries[cf2] = [event2_entry]\n    verify_expected_events(events_mngr, expected_events_entries)\n\n    # Create another cf1 event, but set its time to EARLIER than event1\n    event3_entry = create_event_entry(job_id1, \"2022/03/17-14:42:19.220573\",\n                                      events.EventType.FLUSH_FINISHED, cf1)\n    event3 = entry_to_event(event3_entry)\n    assert events_mngr.try_adding_entry(event3_entry) == (True, event3, cf1)\n    # Expecting event3 to be before event1\n    expected_events_entries[cf1] = [event3_entry, event1_entry]\n    verify_expected_events(events_mngr, expected_events_entries)\n\n    # Create some more cf21 event, later in time\n    event4_entry = \\\n        create_event_entry(job_id2, \"2022/05/17-14:42:19.220573\",\n                           events.EventType.COMPACTION_STARTED, cf2,\n                           compaction_reason=\"Reason1\")\n    event4 = entry_to_event(event4_entry)\n    event5_entry = \\\n        create_event_entry(job_id2, \"2022/05/17-15:42:19.220573\",\n                           events.EventType.COMPACTION_STARTED, cf2,\n                           compaction_reason=\"Reason2\")\n    event5 = entry_to_event(event5_entry)\n    event6_entry =\\\n        create_event_entry(job_id2, \"2022/05/17-16:42:19.220573\",\n                           events.EventType.COMPACTION_FINISHED, cf2)\n    event6 = entry_to_event(event6_entry)\n    assert events_mngr.try_adding_entry(event4_entry) == (True, event4, cf2)\n    assert events_mngr.try_adding_entry(event5_entry) == (True, event5, cf2)\n    assert events_mngr.try_adding_entry(event6_entry) == (True, event6, cf2)\n    expected_events_entries[cf2] = [event2_entry, event4_entry,\n                                    event5_entry, event6_entry]\n    verify_expected_events(events_mngr, expected_events_entries)", "\n\ndef test_get_flow_events():\n    cf1 = \"cf1\"\n    cf2 = \"cf2\"\n    cfs_names = [cf1, cf2]\n    job_id = 1\n    job_id_to_cf_name_map = {job_id: cf1}\n    events_mngr = events.EventsMngr(job_id_to_cf_name_map)\n\n    assert events_mngr.get_cf_flow_events(events.FlowType.FLUSH, cf1) == []\n    assert events_mngr.get_cf_flow_events(events.FlowType.FLUSH, cf2) == []\n    assert events_mngr.get_all_flow_events(events.FlowType.FLUSH, cfs_names)\\\n           == []\n\n    flush_started_cf1_1 = create_event_entry(job_id,\n                                             \"2022/04/18-14:42:19.220573\",\n                                             events.EventType.FLUSH_STARTED,\n                                             cf1,\n                                             flush_reason=\"FLUSH_REASON1\")\n    flush_started_event = entry_to_event(flush_started_cf1_1)\n    assert events_mngr.try_adding_entry(flush_started_cf1_1) == \\\n           (True, flush_started_event, cf1)\n    cf1_flush_started_events =\\\n        events_mngr.get_cf_events_by_type(cf1, events.EventType.FLUSH_STARTED)\n\n    expected_flush_events = [(cf1_flush_started_events[0], None)]\n    assert events_mngr.get_cf_flow_events(events.FlowType.FLUSH, cf1) == \\\n           expected_flush_events\n    assert events_mngr.get_all_flow_events(events.FlowType.FLUSH, cfs_names)\\\n           == expected_flush_events\n\n    flush_finished_cf1_1 = \\\n        create_event_entry(job_id,\n                           \"2022/04/18-14:43:19.220573\",\n                           events.EventType.FLUSH_FINISHED,\n                           cf1)\n    flush_finished_event = entry_to_event(flush_finished_cf1_1)\n    assert events_mngr.try_adding_entry(flush_finished_cf1_1) == \\\n           (True, flush_finished_event, cf1)\n    cf1_flush_started_events =\\\n        events_mngr.get_cf_events_by_type(cf1, events.EventType.FLUSH_STARTED)\n    cf1_flush_finished_events =\\\n        events_mngr.get_cf_events_by_type(cf1, events.EventType.FLUSH_FINISHED)\n\n    expected_flush_events = [(cf1_flush_started_events[0],\n                              cf1_flush_finished_events[0])]\n    assert events_mngr.get_all_flow_events(events.FlowType.FLUSH, cfs_names)\\\n           == expected_flush_events", "\n\ndef test_try_adding_invalid_event_to_events_mngr():\n    cf1 = \"cf1\"\n    job_id = 35\n    job_id_to_cf_name_map = {job_id: cf1}\n    events_mngr = events.EventsMngr(job_id_to_cf_name_map)\n\n    # Illegal event json\n    invalid_event_entry =\\\n        create_event_entry(job_id,\n                           \"2022/04/17-14:42:19.220573\",\n                           events.EventType.FLUSH_FINISHED,\n                           cf1,\n                           make_illegal_json=True)\n\n    assert events_mngr.try_adding_entry(invalid_event_entry) == \\\n           (True, None, None)\n    assert events_mngr.debug_get_all_events() == {}\n\n    event1_entry = create_event_entry(job_id, \"2022/04/17-14:42:19.220573\",\n                                      events.EventType.FLUSH_FINISHED, cf1)\n    event1 = entry_to_event(event1_entry)\n    assert events_mngr.try_adding_entry(event1_entry) == (True, event1, cf1)\n    assert len(events_mngr.debug_get_all_events()) == 1\n\n    assert events_mngr.try_adding_entry(invalid_event_entry) == \\\n           (True, None, None)\n    assert len(events_mngr.debug_get_all_events()) == 1", ""]}
{"filename": "test/test_cache_utils.py", "chunked_list": ["from dataclasses import dataclass\n\nimport db_files\nfrom db_files import DbFilesMonitor\nfrom events import EventType\nfrom test.testing_utils import create_event\n\n\n@dataclass\nclass FileCreationHelperInfo:\n    job_id: int\n    file_number: int\n    cf_name: str\n    creation_time: str\n    compressed_data_size_bytes: int = 10000\n    num_data_blocks: int = 500\n    total_keys_sizes_bytes: int = 800\n    total_values_sizes_bytes: int = 1600\n    index_size: int = 1000\n    filter_size: int = 2000\n    num_entries: int = 100\n    compression_type: str = \"NoCompression\"\n    filter_policy: str = \"\"\n    num_filter_entries: int = 50", "@dataclass\nclass FileCreationHelperInfo:\n    job_id: int\n    file_number: int\n    cf_name: str\n    creation_time: str\n    compressed_data_size_bytes: int = 10000\n    num_data_blocks: int = 500\n    total_keys_sizes_bytes: int = 800\n    total_values_sizes_bytes: int = 1600\n    index_size: int = 1000\n    filter_size: int = 2000\n    num_entries: int = 100\n    compression_type: str = \"NoCompression\"\n    filter_policy: str = \"\"\n    num_filter_entries: int = 50", "\n\ndef create_file_event_helper(cf_names, creation_info, monitor):\n    assert isinstance(creation_info, FileCreationHelperInfo)\n\n    table_properties = {\n        \"data_size\": creation_info.compressed_data_size_bytes,\n        \"index_size\": creation_info.index_size,\n        \"filter_size\": creation_info.filter_size,\n        \"raw_key_size\": creation_info.total_keys_sizes_bytes,\n        \"raw_value_size\": creation_info.total_values_sizes_bytes,\n        \"num_data_blocks\": creation_info.num_data_blocks,\n        \"num_entries\": creation_info.num_entries,\n        \"compression\": creation_info.compression_type,\n        \"filter_policy\": creation_info.filter_policy,\n        \"num_filter_entries\": creation_info.num_filter_entries}\n\n    creation_event = create_event(creation_info.job_id, cf_names,\n                                  creation_info.creation_time,\n                                  EventType.TABLE_FILE_CREATION,\n                                  creation_info.cf_name,\n                                  file_number=creation_info.file_number,\n                                  table_properties=table_properties)\n\n    assert monitor.new_event(creation_event)", "\n\ndef test_calc_cf_files_stats():\n    cf1 = \"cf1\"\n    cf2 = \"cf2\"\n    cf_names = [cf1, cf2]\n    time1 = \"2023/01/24-08:54:40.130553\"\n    time1_plus_10_sec = \"2023/01/24-08:54:50.130553\"\n    time1_plus_11_sec = \"2023/01/24-08:55:50.130553\"\n\n    monitor = DbFilesMonitor()\n\n    helper_info1 = FileCreationHelperInfo(job_id=1,\n                                          file_number=1234,\n                                          cf_name=cf1,\n                                          creation_time=time1,\n                                          index_size=1000,\n                                          filter_size=2000,\n                                          filter_policy=\"Filter1\",\n                                          num_filter_entries=10000)\n\n    # the block statistics are tested as part of the test_db_files suite\n    expected_cf1_filter_specific_stats = \\\n        {cf1: db_files.CfFilterSpecificStats(filter_policy=\"Filter1\",\n                                             avg_bpk=8*2000/10000)}\n    create_file_event_helper(cf_names, helper_info1, monitor)\n    actual_cf1_stats = db_files.calc_cf_files_stats([cf1], monitor)\n    assert actual_cf1_stats.cfs_filter_specific == \\\n           expected_cf1_filter_specific_stats\n    assert db_files.calc_cf_files_stats([cf2], monitor) is None\n\n    helper_info2 = FileCreationHelperInfo(job_id=2,\n                                          file_number=5678,\n                                          cf_name=cf1,\n                                          creation_time=time1_plus_10_sec,\n                                          index_size=500,\n                                          filter_size=3000,\n                                          filter_policy=\"Filter1\",\n                                          num_filter_entries=5000)\n    expected_cf1_filter_specific_stats = \\\n        {cf1: db_files.CfFilterSpecificStats(filter_policy=\"Filter1\",\n                                             avg_bpk=8*5000/15000)}\n    create_file_event_helper(cf_names, helper_info2, monitor)\n    actual_cf1_stats = db_files.calc_cf_files_stats([cf1], monitor)\n    assert actual_cf1_stats.cfs_filter_specific == \\\n           expected_cf1_filter_specific_stats\n    assert db_files.calc_cf_files_stats([cf2], monitor) is None\n\n    helper_info3 = FileCreationHelperInfo(job_id=3,\n                                          file_number=9999,\n                                          cf_name=cf2,\n                                          creation_time=time1_plus_11_sec,\n                                          index_size=1500,\n                                          filter_size=1000,\n                                          filter_policy=\"\",\n                                          num_filter_entries=0)\n    expected_cf2_filter_specific_stats = \\\n        {cf2: db_files.CfFilterSpecificStats(filter_policy=None, avg_bpk=0)}\n    expected_all_cfs_filter_specific_stats = {\n        cf1: expected_cf1_filter_specific_stats[cf1],\n        cf2: expected_cf2_filter_specific_stats[cf2]\n    }\n\n    create_file_event_helper(cf_names, helper_info3, monitor)\n    actual_cf1_stats = db_files.calc_cf_files_stats([cf1], monitor)\n    assert actual_cf1_stats.cfs_filter_specific == \\\n           expected_cf1_filter_specific_stats\n    actual_cf2_stats = db_files.calc_cf_files_stats([cf2], monitor)\n    assert actual_cf2_stats.cfs_filter_specific == \\\n           expected_cf2_filter_specific_stats\n    actual_all_cfs_stats = db_files.calc_cf_files_stats([cf1, cf2], monitor)\n    assert actual_all_cfs_stats.cfs_filter_specific == \\\n           expected_all_cfs_filter_specific_stats", ""]}
{"filename": "test/test_calc_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nfrom dataclasses import dataclass\n\nimport calc_utils\nimport db_options as db_opts", "import calc_utils\nimport db_options as db_opts\nimport test.testing_utils as test_utils\nimport utils\nfrom counters import CountersMngr\nfrom stats_mngr import CfFileHistogramStatsMngr\nfrom test.sample_log_info import SampleLogInfo\n\n\n# TODO: Move to the stats mngr test file", "\n# TODO: Move to the stats mngr test file\n\ndef test_get_cf_size_bytes():\n    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n    compactions_stats_mngr = \\\n        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n    for cf_name in SampleLogInfo.CF_NAMES:\n        actual_size_bytes = \\\n            compactions_stats_mngr.get_cf_size_bytes_at_end(cf_name)\n        assert actual_size_bytes == SampleLogInfo.CF_SIZE_BYTES[cf_name]", "\n\ndef test_get_db_size_bytes():\n    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n    compactions_stats_mngr = \\\n        parsed_log.get_stats_mngr().get_compactions_stats_mngr()\n\n    for level, level_size_bytes in \\\n            SampleLogInfo.CF_DEFAULT_LEVEL_SIZES.items():\n        actual_level_size_bytes = \\\n            compactions_stats_mngr.get_cf_level_size_bytes(\n                SampleLogInfo.CF_DEFAULT, level)\n        assert actual_level_size_bytes == level_size_bytes", "\n\ndef test_calc_all_events_histogram():\n    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\n    events_histogram = calc_utils.calc_all_events_histogram(\n        SampleLogInfo.CF_NAMES, parsed_log.get_events_mngr())\n    assert events_histogram == SampleLogInfo.EVENTS_HISTOGRAM\n\n\ndef test_calc_read_latency_per_cf_stats():\n    time = \"2023/01/04-09:04:59.378877 27442\"\n    l0 = 0\n    l1 = 1\n    l4 = 4\n    cf1 = \"cf1\"\n    cf2 = \"cf2\"\n\n    stats_lines_cf1 = \\\n    f'''** File Read Latency Histogram By Level [{cf1}] **\n    ** Level {l0} read latency histogram (micros):\n    Count: 100 Average: 1.5  StdDev: 34.39\n    Min: 0  Median: 2.4427  Max: 1000\n    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02\n    \n    ** Level {l4} read latency histogram (micros):\n    Count: 200 Average: 2.5  StdDev: 2.2\n    Min: 1000  Median: 3.3  Max: 2000\n    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02'''  # noqa\n    stats_lines_cf1 = stats_lines_cf1.splitlines()\n    stats_lines_cf1 = [line.strip() for line in stats_lines_cf1]\n\n    mngr = CfFileHistogramStatsMngr()\n\n    mngr.add_lines(time, cf1, stats_lines_cf1)\n    total_reads_cf1 = 100 + 200\n    expected_cf1_avg_read_latency_us = (100*1.5 + 200*2.5) / total_reads_cf1\n    expected_cf1_stats = \\\n        calc_utils.CfReadLatencyStats(\n            num_reads=total_reads_cf1,\n            avg_read_latency_us=expected_cf1_avg_read_latency_us,\n            max_read_latency_us=2000,\n            read_percent_of_all_cfs=100.0)\n\n    actual_stats = calc_utils.calc_read_latency_per_cf_stats(mngr)\n    assert actual_stats == {cf1: expected_cf1_stats}\n\n    stats_lines_cf2 = \\\n        f'''** File Read Latency Histogram By Level [{cf2}] **\n    ** Level {l1} read latency histogram (micros):\n    Count: 1000 Average: 2.5  StdDev: 34.39\n    Min: 0  Median: 2.4427  Max: 20000\n    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02\n\n    ** Level {l4} read latency histogram (micros):\n    Count: 500 Average: 1.1  StdDev: 2.2\n    Min: 1000  Median: 3.3  Max: 10000\n    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\n    stats_lines_cf2 = stats_lines_cf2.splitlines()\n    stats_lines_cf2 = [line.strip() for line in stats_lines_cf2]\n\n    mngr.add_lines(time, cf2, stats_lines_cf2)\n    total_reads = 300 + 1500\n    total_reads_cf2 = 1000 + 500\n    expected_cf1_stats.read_percent_of_all_cfs = \\\n        (total_reads_cf1 / total_reads) * 100\n    expected_cf2_read_percent_of_all_cfs = \\\n        (total_reads_cf2 / total_reads) * 100\n    expected_cf2_avg_read_latency_us = (1000*2.5 + 500*1.1) / (1000+500)\n    expected_cf2_stats = \\\n        calc_utils.CfReadLatencyStats(\n            num_reads=total_reads_cf2,\n            avg_read_latency_us=expected_cf2_avg_read_latency_us,\n            max_read_latency_us=20000,\n            read_percent_of_all_cfs=expected_cf2_read_percent_of_all_cfs)\n\n    actual_stats = calc_utils.calc_read_latency_per_cf_stats(mngr)\n    assert actual_stats == {cf1: expected_cf1_stats,\n                            cf2: expected_cf2_stats}", "\n\ndef test_calc_read_latency_per_cf_stats():\n    time = \"2023/01/04-09:04:59.378877 27442\"\n    l0 = 0\n    l1 = 1\n    l4 = 4\n    cf1 = \"cf1\"\n    cf2 = \"cf2\"\n\n    stats_lines_cf1 = \\\n    f'''** File Read Latency Histogram By Level [{cf1}] **\n    ** Level {l0} read latency histogram (micros):\n    Count: 100 Average: 1.5  StdDev: 34.39\n    Min: 0  Median: 2.4427  Max: 1000\n    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02\n    \n    ** Level {l4} read latency histogram (micros):\n    Count: 200 Average: 2.5  StdDev: 2.2\n    Min: 1000  Median: 3.3  Max: 2000\n    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02'''  # noqa\n    stats_lines_cf1 = stats_lines_cf1.splitlines()\n    stats_lines_cf1 = [line.strip() for line in stats_lines_cf1]\n\n    mngr = CfFileHistogramStatsMngr()\n\n    mngr.add_lines(time, cf1, stats_lines_cf1)\n    total_reads_cf1 = 100 + 200\n    expected_cf1_avg_read_latency_us = (100*1.5 + 200*2.5) / total_reads_cf1\n    expected_cf1_stats = \\\n        calc_utils.CfReadLatencyStats(\n            num_reads=total_reads_cf1,\n            avg_read_latency_us=expected_cf1_avg_read_latency_us,\n            max_read_latency_us=2000,\n            read_percent_of_all_cfs=100.0)\n\n    actual_stats = calc_utils.calc_read_latency_per_cf_stats(mngr)\n    assert actual_stats == {cf1: expected_cf1_stats}\n\n    stats_lines_cf2 = \\\n        f'''** File Read Latency Histogram By Level [{cf2}] **\n    ** Level {l1} read latency histogram (micros):\n    Count: 1000 Average: 2.5  StdDev: 34.39\n    Min: 0  Median: 2.4427  Max: 20000\n    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02\n\n    ** Level {l4} read latency histogram (micros):\n    Count: 500 Average: 1.1  StdDev: 2.2\n    Min: 1000  Median: 3.3  Max: 10000\n    Percentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\n    stats_lines_cf2 = stats_lines_cf2.splitlines()\n    stats_lines_cf2 = [line.strip() for line in stats_lines_cf2]\n\n    mngr.add_lines(time, cf2, stats_lines_cf2)\n    total_reads = 300 + 1500\n    total_reads_cf2 = 1000 + 500\n    expected_cf1_stats.read_percent_of_all_cfs = \\\n        (total_reads_cf1 / total_reads) * 100\n    expected_cf2_read_percent_of_all_cfs = \\\n        (total_reads_cf2 / total_reads) * 100\n    expected_cf2_avg_read_latency_us = (1000*2.5 + 500*1.1) / (1000+500)\n    expected_cf2_stats = \\\n        calc_utils.CfReadLatencyStats(\n            num_reads=total_reads_cf2,\n            avg_read_latency_us=expected_cf2_avg_read_latency_us,\n            max_read_latency_us=20000,\n            read_percent_of_all_cfs=expected_cf2_read_percent_of_all_cfs)\n\n    actual_stats = calc_utils.calc_read_latency_per_cf_stats(mngr)\n    assert actual_stats == {cf1: expected_cf1_stats,\n                            cf2: expected_cf2_stats}", "\n\n@dataclass\nclass HistogramInfo:\n    count: int = 0\n    sum: int = 0\n\n    def get_average(self):\n        assert self.sum > 0\n        return self.sum / self.count", "\n\n@dataclass\nclass SeekInfo:\n    time: str\n    num_seeks: int = 0\n    num_found: int = 0\n    num_next: int = 0\n    num_prev: int = 0\n    seek_micros: HistogramInfo = HistogramInfo()", "\n\ndef get_seek_counter_and_histogram_entry(test_seek_info):\n    assert isinstance(test_seek_info, SeekInfo)\n\n    prefix = 'rocksdb.number.db'\n    lines = list()\n    lines.append(\n        f'{test_seek_info.time} 100 [db/db_impl/db_impl.cc:753] STATISTICS:')\n    lines.append(f'{prefix}.seek COUNT : {test_seek_info.num_seeks}')\n    lines.append(f'{prefix}.seek.found COUNT : {test_seek_info.num_found}')\n    lines.append(f'{prefix}.next COUNT : {test_seek_info.num_next}')\n    lines.append(f'{prefix}.prev COUNT : {test_seek_info.num_prev}')\n    lines.append(\n        f'rocksdb.db.seek.micros P50 : 0.0 P95 : 0.0 P99 : 0.0 P100 : 0.0 '\n        f'COUNT : {test_seek_info.seek_micros.count} '\n        f'SUM : {test_seek_info.seek_micros.sum}')\n    return test_utils.lines_to_entries(lines)", "\n\ndef test_get_applicable_seek_stats():\n    get_seek_stats = calc_utils.get_applicable_seek_stats\n\n    start_time = \"2022/06/16-15:36:02.993900\"\n    start_plus_1_second = utils.get_time_relative_to(start_time, num_seconds=1)\n    start_plus_10_seconds = \\\n        utils.get_time_relative_to(start_time, num_seconds=10)\n    start_plus_20_seconds = \\\n        utils.get_time_relative_to(start_time, num_seconds=20)\n\n    start_seek_stats = SeekInfo(start_time)\n    start_entry = get_seek_counter_and_histogram_entry(start_seek_stats)\n\n    start_plus_1_seek_stats = SeekInfo(start_plus_1_second)\n    start_plus_1_seek_entry = \\\n        get_seek_counter_and_histogram_entry(start_plus_1_seek_stats)\n\n    start_plus_10_seek_stats = \\\n        SeekInfo(start_plus_10_seconds, num_seeks=1000, num_found=500)\n    start_plus_10_seek_entry = \\\n        get_seek_counter_and_histogram_entry(start_plus_10_seek_stats)\n\n    start_plus_20_seek_stats = \\\n        SeekInfo(start_plus_20_seconds,\n                 num_seeks=20000,\n                 num_found=10000,\n                 num_next=1500,\n                 num_prev=500,\n                 seek_micros=HistogramInfo(count=20000, sum=2000))\n    start_plus_20_seek_entry = \\\n        get_seek_counter_and_histogram_entry(start_plus_20_seek_stats)\n\n    mngr = CountersMngr()\n    assert get_seek_stats(mngr) is None\n\n    assert mngr.try_adding_entries(start_entry, 0) == (True, 1)\n    assert mngr.get_last_counter_entry('rocksdb.number.db.seek') == \\\n           {'time': start_time, 'value': 0}\n    assert get_seek_stats(mngr) is None\n\n    assert mngr.try_adding_entries(start_plus_1_seek_entry, 0) == (True, 1)\n    assert mngr.get_last_counter_entry('rocksdb.number.db.seek') == \\\n           {'time': start_plus_1_second, 'value': 0}\n    assert get_seek_stats(mngr) is None\n\n    assert mngr.try_adding_entries(start_plus_10_seek_entry, 0) == (True, 1)\n    assert mngr.get_last_counter_entry('rocksdb.number.db.seek') == \\\n           {'time': start_plus_10_seconds, 'value': 1000}\n    get_seek_stats(mngr) == calc_utils.SeekStats(num_seeks=1000,\n                                                 num_found_seeks=500)\n\n    assert mngr.try_adding_entries(start_plus_20_seek_entry, 0) == (True, 1)\n    assert mngr.get_last_counter_entry('rocksdb.number.db.seek') == \\\n           {'time': start_plus_20_seconds, 'value': 20000}\n    assert mngr.get_last_histogram_entry('rocksdb.db.seek.micros',\n                                         non_zero=False) == \\\n           {'time': start_plus_20_seconds,\n            'values': {'Average': 0.1,\n                       'Count': 20000,\n                       'Interval Count': 20000,\n                       'Interval Sum': 2000,\n                       'P100': 0.0,\n                       'P50': 0.0,\n                       'P95': 0.0,\n                       'P99': 0.0,\n                       'Sum': 2000}}\n    assert get_seek_stats(mngr) == \\\n           calc_utils.SeekStats(num_seeks=20000,\n                                num_found_seeks=10000,\n                                num_nexts=1500,\n                                num_prevs=500,\n                                avg_seek_range_size=0.1,\n                                avg_seek_rate_per_second=(20000/20),\n                                avg_seek_latency_us=0.1)", "\n\nCF_SECTION_TYPE = db_opts.SectionType.CF\nTABLE_SECTION_TYPE = db_opts.SectionType.TABLE_OPTIONS\n\n\ndef test_get_cfs_common_and_specific_options():\n    get_opts = calc_utils.get_cfs_common_and_specific_options\n\n    cf1 = 'cf1'\n    cf2 = 'cf2'\n\n    cf_option1 = \"CF-Option1\"\n    cf_option2 = \"CF-Option2\"\n    cf_option3 = \"CF-Option3\"\n\n    cf_table_option1 = \"CF-Table-Option1\"\n    cf_table_option2 = \"CF-Table-Option2\"\n\n    full_cf_option1_name = db_opts.get_full_option_name(CF_SECTION_TYPE,\n                                                        cf_option1)\n    full_cf_option2_name = db_opts.get_full_option_name(CF_SECTION_TYPE,\n                                                        cf_option2)\n    full_cf_option3_name = db_opts.get_full_option_name(CF_SECTION_TYPE,\n                                                        cf_option3)\n    full_cf_table_option1_name =\\\n        db_opts.get_full_option_name(TABLE_SECTION_TYPE, cf_table_option1)\n    full_cf_table_option2_name = \\\n        db_opts.get_full_option_name(TABLE_SECTION_TYPE, cf_table_option2)\n\n    dbo = db_opts.DatabaseOptions()\n    assert get_opts(dbo) == ({}, {})\n\n    dbo.set_cf_option(cf1, cf_option1, 1, allow_new_option=True)\n    expected_common = {full_cf_option1_name: 1}\n    expected_specific = {cf1: {}}\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_option(cf1, cf_option2, 2, allow_new_option=True)\n    expected_common = {\n        full_cf_option1_name: 1,\n        full_cf_option2_name: 2\n    }\n    expected_specific = {cf1: {}}\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_option(cf2, cf_option3, 3, allow_new_option=True)\n    expected_common = {}\n    expected_specific = {\n        cf1: {full_cf_option1_name: 1,\n              full_cf_option2_name: 2},\n        cf2: {full_cf_option3_name: 3}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_option(cf2, cf_option1, 1, allow_new_option=True)\n    expected_common = {full_cf_option1_name: 1}\n    expected_specific = {\n        cf1: {full_cf_option2_name: 2},\n        cf2: {full_cf_option3_name: 3}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_option(cf2, cf_option1, 5)\n    expected_common = {}\n    expected_specific = {\n        cf1: {full_cf_option1_name: 1,\n              full_cf_option2_name: 2},\n        cf2: {full_cf_option1_name: 5,\n              full_cf_option3_name: 3}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_table_option(cf2, cf_table_option1, 100, allow_new_option=True)\n    expected_common = {}\n    expected_specific = {\n        cf1: {full_cf_option1_name: 1,\n              full_cf_option2_name: 2},\n        cf2: {full_cf_option1_name: 5,\n              full_cf_option3_name: 3,\n              full_cf_table_option1_name: 100}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_table_option(cf1, cf_table_option1, 100, allow_new_option=True)\n    expected_common = {full_cf_table_option1_name: 100}\n    expected_specific = {\n        cf1: {full_cf_option1_name: 1,\n              full_cf_option2_name: 2},\n        cf2: {full_cf_option1_name: 5,\n              full_cf_option3_name: 3}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_table_option(cf1, cf_table_option2, 200, allow_new_option=True)\n    expected_common = {full_cf_table_option1_name: 100}\n    expected_specific = {\n        cf1: {full_cf_option1_name: 1,\n              full_cf_option2_name: 2,\n              full_cf_table_option2_name: 200},\n        cf2: {full_cf_option1_name: 5,\n              full_cf_option3_name: 3}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_table_option(cf2, cf_table_option2, 200, allow_new_option=True)\n    expected_common = {\n        full_cf_table_option1_name: 100,\n        full_cf_table_option2_name: 200\n    }\n    expected_specific = {\n        cf1: {full_cf_option1_name: 1,\n              full_cf_option2_name: 2},\n        cf2: {full_cf_option1_name: 5,\n              full_cf_option3_name: 3}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_option(cf1, cf_option3, 3, allow_new_option=True)\n    expected_common = {\n        full_cf_option3_name: 3,\n        full_cf_table_option1_name: 100,\n        full_cf_table_option2_name: 200\n    }\n    expected_specific = {\n        cf1: {full_cf_option1_name: 1,\n              full_cf_option2_name: 2},\n        cf2: {full_cf_option1_name: 5}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_option(cf1, cf_option1, 5)\n    expected_common = {\n        full_cf_option1_name: 5,\n        full_cf_option3_name: 3,\n        full_cf_table_option1_name: 100,\n        full_cf_table_option2_name: 200\n    }\n    expected_specific = {\n        cf1: {full_cf_option2_name: 2},\n        cf2: {}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)\n\n    dbo.set_cf_option(cf2, cf_option2, 2, allow_new_option=True)\n    expected_common = {\n        full_cf_option1_name: 5,\n        full_cf_option2_name: 2,\n        full_cf_option3_name: 3,\n        full_cf_table_option1_name: 100,\n        full_cf_table_option2_name: 200\n    }\n    expected_specific = {\n        cf1: {},\n        cf2: {}\n    }\n    assert get_opts(dbo) == (expected_common, expected_specific)", "\n\ndef test_get_cfs_common_and_specific_diff_dicts():\n    get_cfs_diff = calc_utils.get_cfs_common_and_specific_diff_dicts\n\n    # db_cf = utils.NO_CF\n    dflt_cf = utils.DEFAULT_CF_NAME\n    cf1 = 'cf1'\n    # cf2 = 'cf2'\n\n    db_option1 = \"DB-Options1\"\n    db_option2 = \"DB-Options2\"\n\n    cf_option1 = \"CF-Option1\"\n    cf_option2 = \"CF-Option2\"\n    # cf_option3 = \"CF-Option3\"\n    # cf_table_option1 = \"CF-Table-Option1\"\n    # cf_table_option2 = \"CF-Table-Option2\"\n\n    full_cf_option1_name = db_opts.get_full_option_name(CF_SECTION_TYPE,\n                                                        cf_option1)\n\n    def extract_dict(cfs_options_diff):\n        assert isinstance(cfs_options_diff, db_opts.CfsOptionsDiff)\n        return cfs_options_diff.get_diff_dict(exclude_compared_cfs_names=True)\n\n    base_dbo = db_opts.DatabaseOptions()\n    log_dbo = db_opts.DatabaseOptions()\n    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {})\n\n    base_dbo.set_db_wide_option(db_option1, 1, allow_new_option=True)\n    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {})\n\n    log_dbo.set_db_wide_option(db_option2, 1, allow_new_option=True)\n    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {})\n\n    # base has a cf option but log has no cf-s => empty diff\n    base_dbo.set_cf_option(dflt_cf, cf_option1, 10, allow_new_option=True)\n    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {})\n\n    base_dbo.set_cf_option(dflt_cf, cf_option1, 10, allow_new_option=True)\n    log_dbo.set_cf_option(dflt_cf, cf_option1, 10, allow_new_option=True)\n    assert get_cfs_diff(base_dbo, log_dbo) == ({}, {dflt_cf: None})\n\n    base_dbo.set_cf_option(dflt_cf, cf_option1, 11, allow_new_option=True)\n    log_dbo.set_cf_option(dflt_cf, cf_option1, 10, allow_new_option=True)\n    actual_common, actual_specific = get_cfs_diff(base_dbo, log_dbo)\n    assert actual_common.get_diff_dict(exclude_compared_cfs_names=True) == {\n        full_cf_option1_name: (11, 10)}\n    assert actual_specific == {dflt_cf: None}\n\n    base_dbo.set_cf_option(dflt_cf, cf_option2, 20, allow_new_option=True)\n    log_dbo.set_cf_option(dflt_cf, cf_option2, 20, allow_new_option=True)\n    actual_common, actual_specific = get_cfs_diff(base_dbo, log_dbo)\n    assert actual_common.get_diff_dict(exclude_compared_cfs_names=True) == {\n        full_cf_option1_name: (11, 10)}\n    assert actual_specific == {dflt_cf: None}\n\n    log_dbo.set_cf_option(cf1, cf_option2, 20, allow_new_option=True)\n    # base (dflt): opt1=11, opt2=20\n    # log: dflt: opt1=10,  opt2=20\n    #      cf1:  Missing   opt2=20\n    # Expected diff: Common: None (opt2 identical)\n    #                Specific: dflt: (11, 10), cf1: (11, Missing)\n    actual_common, actual_specific = get_cfs_diff(base_dbo, log_dbo)\n    actual_dflt = extract_dict(actual_specific[dflt_cf])\n    actual_cf1 = extract_dict(actual_specific[cf1])\n    assert actual_common == {}\n    assert actual_dflt == {full_cf_option1_name: (11, 10)}\n    assert actual_cf1 == {full_cf_option1_name: (11, \"Missing\")}\n\n    log_dbo.set_cf_option(cf1, cf_option1, 11, allow_new_option=True)\n    # base (dflt): opt1=11, opt2=20\n    # log: dflt: opt1=10,  opt2=20\n    #      cf1:  opt1=11   opt2=20\n    # Expected diff: Common: None (opt2 identical)\n    #                Specific: dflt: (11, 10), cf1: None\n    actual_common, actual_specific = get_cfs_diff(base_dbo, log_dbo)\n    actual_dflt = extract_dict(actual_specific[dflt_cf])\n    assert actual_common == {}\n    assert actual_dflt == {full_cf_option1_name: (11, 10)}\n    assert actual_specific[cf1] is None", ""]}
{"filename": "test/testing_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport utils\nfrom events import Event\nfrom log_entry import LogEntry\nfrom log_file import ParsedLog", "from log_entry import LogEntry\nfrom log_file import ParsedLog\nfrom test.sample_log_info import SampleLogInfo\n\n\ndef read_file(file_path):\n    with open(file_path, \"r\") as f:\n        return f.readlines()\n\n\ndef create_parsed_log(file_path):\n    log_lines = read_file(file_path)\n    return ParsedLog(SampleLogInfo.FILE_PATH, log_lines,\n                     should_init_baseline_info=False)", "\n\ndef create_parsed_log(file_path):\n    log_lines = read_file(file_path)\n    return ParsedLog(SampleLogInfo.FILE_PATH, log_lines,\n                     should_init_baseline_info=False)\n\n\ndef entry_msg_to_entry(time_str, msg, process_id=\"0x123456\", code_pos=None):\n    if code_pos is not None:\n        line = f\"{time_str} {process_id} {code_pos} {msg}\"\n    else:\n        line = f\"{time_str} {process_id} {msg}\"\n\n    assert LogEntry.is_entry_start(line)\n    return LogEntry(0, line, last_line=True)", "def entry_msg_to_entry(time_str, msg, process_id=\"0x123456\", code_pos=None):\n    if code_pos is not None:\n        line = f\"{time_str} {process_id} {code_pos} {msg}\"\n    else:\n        line = f\"{time_str} {process_id} {msg}\"\n\n    assert LogEntry.is_entry_start(line)\n    return LogEntry(0, line, last_line=True)\n\n\ndef line_to_entry(line, last_line=True):\n    assert LogEntry.is_entry_start(line)\n    return LogEntry(0, line, last_line)", "\n\ndef line_to_entry(line, last_line=True):\n    assert LogEntry.is_entry_start(line)\n    return LogEntry(0, line, last_line)\n\n\ndef lines_to_entries(lines):\n    entries = []\n    entry = None\n    for i, line in enumerate(lines):\n        if LogEntry.is_entry_start(line):\n            if entry:\n                entries.append(entry.all_lines_added())\n            entry = LogEntry(i, line)\n        else:\n            assert entry\n            entry.add_line(line)\n\n    if entry:\n        entries.append(entry.all_lines_added())\n\n    return entries", "\n\ndef add_stats_entry_lines_to_counters_mngr(entry_lines, mngr):\n    entry = LogEntry(0, entry_lines[0])\n    for line in entry_lines[1:]:\n        entry.add_line(line)\n    mngr.add_entry(entry.all_lines_added())\n\n\ndef create_event_entry(job_id, time_str, event_type=None, cf_name=None,\n                       make_illegal_json=False, **kwargs):\n    event_line = time_str + \" \"\n    event_line += '7f4a8b5bb700 EVENT_LOG_v1 {\"time_micros\": '\n\n    event_line += str(utils.get_gmt_timestamp_us(time_str))\n\n    if event_type is not None:\n        event_line += f', \"event\": \"{str(event_type.value)}\"'\n    if cf_name is not None:\n        event_line += f', \"cf_name\": \"{cf_name}\"'\n    if job_id is not None:\n        event_line += f', \"job\": {job_id}'\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            event_line += f', \"{k}\": \"{v}\"'\n        elif isinstance(v, dict):\n            event_line += f', \"{k}\":' + ' {'\n            first_key = True\n            for k1, v1 in v.items():\n                if not first_key:\n                    event_line += \", \"\n                if isinstance(v1, str):\n                    event_line += f'\"{k1}\": \"{v1}\"'\n                else:\n                    event_line += f'\"{k1}\": {v1}'\n                first_key = False\n\n            event_line += \"}\"\n            pass\n        else:\n            event_line += f', \"{k}\": {v}'\n\n    if make_illegal_json:\n        event_line += \", \"\n\n    event_line += '}'\n\n    event_entry = LogEntry(0, event_line, True)\n    assert Event.is_an_event_entry(event_entry)\n    return event_entry", "\ndef create_event_entry(job_id, time_str, event_type=None, cf_name=None,\n                       make_illegal_json=False, **kwargs):\n    event_line = time_str + \" \"\n    event_line += '7f4a8b5bb700 EVENT_LOG_v1 {\"time_micros\": '\n\n    event_line += str(utils.get_gmt_timestamp_us(time_str))\n\n    if event_type is not None:\n        event_line += f', \"event\": \"{str(event_type.value)}\"'\n    if cf_name is not None:\n        event_line += f', \"cf_name\": \"{cf_name}\"'\n    if job_id is not None:\n        event_line += f', \"job\": {job_id}'\n\n    for k, v in kwargs.items():\n        if isinstance(v, str):\n            event_line += f', \"{k}\": \"{v}\"'\n        elif isinstance(v, dict):\n            event_line += f', \"{k}\":' + ' {'\n            first_key = True\n            for k1, v1 in v.items():\n                if not first_key:\n                    event_line += \", \"\n                if isinstance(v1, str):\n                    event_line += f'\"{k1}\": \"{v1}\"'\n                else:\n                    event_line += f'\"{k1}\": {v1}'\n                first_key = False\n\n            event_line += \"}\"\n            pass\n        else:\n            event_line += f', \"{k}\": {v}'\n\n    if make_illegal_json:\n        event_line += \", \"\n\n    event_line += '}'\n\n    event_entry = LogEntry(0, event_line, True)\n    assert Event.is_an_event_entry(event_entry)\n    return event_entry", "\n\ndef entry_to_event(event_entry):\n    assert Event.is_an_event_entry(event_entry)\n    return Event(event_entry)\n\n\ndef create_event(job_id, cf_names, time_str, event_type=None, cf_name=None,\n                 make_illegal_json=False, **kwargs):\n    event_entry = create_event_entry(job_id, time_str, event_type, cf_name,\n                                     make_illegal_json, **kwargs)\n\n    assert Event.is_an_event_entry(event_entry)\n    return Event.create_event(event_entry)", ""]}
{"filename": "test/test_baseline_log_files_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport baseline_log_files_utils as baseline_utils\nimport db_options\nimport utils\n", "import utils\n\nVer = baseline_utils.Version\nbaseline_logs_folder_path = \"../\" + utils.BASELINE_LOGS_FOLDER\n\n\ndef test_version():\n    assert str(Ver(\"1.2.3\")) == \"1.2.3\"\n    assert str(Ver(\"1.2\")) == \"1.2\"\n    assert str(Ver(\"10.20.30\")) == \"10.20.30\"\n    assert str(Ver(\"10.20\")) == \"10.20\"\n\n    v1 = Ver(\"1.2.3\")\n    assert v1.major == 1\n    assert v1.minor == 2\n    assert v1.patch == 3\n\n    v2 = Ver(\"4.5\")\n    assert v2.major == 4\n    assert v2.minor == 5\n    assert v2.patch is None\n\n    assert Ver(\"1.2.3\") < Ver(\"2.1.1\")\n    assert Ver(\"2.1.1\") > Ver(\"1.2.3\")\n    assert Ver(\"2.2.3\") < Ver(\"2.3.1\")\n    assert Ver(\"2.2.1\") < Ver(\"2.2.2\")\n    assert not Ver(\"2.2\") < Ver(\"2.2\")\n    assert Ver(\"2.2\") < Ver(\"2.2.0\")\n    assert Ver(\"2.2\") < Ver(\"2.3\")\n    assert Ver(\"2.2\") < Ver(\"2.2.2\")\n    assert Ver(\"2.2.1\") > Ver(\"2.2\")\n\n    assert Ver(\"2.2.3\") == Ver(\"2.2.3\")\n    assert Ver(\"2.2\") == Ver(\"2.2\")\n    assert Ver(\"2.2.3\") != Ver(\"2.2\")", "\n\ndef test_find_closest_version():\n    # prepare an unsorted list on purpose\n    baseline_versions = [Ver(\"1.2\"),\n                         Ver(\"1.2.0\"),\n                         Ver(\"1.2.1\"),\n                         Ver(\"2.1\"),\n                         Ver(\"2.1.0\"),\n                         Ver(\"3.0\"),\n                         Ver(\"3.0.0\")]\n\n    find = baseline_utils.find_closest_version_idx\n\n    assert find(baseline_versions, Ver(\"1.1\")) is None\n    assert find(baseline_versions, Ver(\"1.1.9\")) is None\n    assert find(baseline_versions, Ver(\"1.2\")) == 0  # Ver(\"1.2\")\n    assert find(baseline_versions, Ver(\"1.2.0\")) == 1  # Ver(\"1.2.0\")\n    assert find(baseline_versions, Ver(\"1.2.2\")) == 2  # Ver(\"1.2.1\")\n    assert find(baseline_versions, Ver(\"2.0.0\")) == 2  # Ver(\"1.2.1\")\n    assert find(baseline_versions, Ver(\"2.1\")) == 3  # Ver(\"2.1\")\n    assert find(baseline_versions, Ver(\"2.1.0\")) == 4  # Ver(\"2.1.0\")\n    assert find(baseline_versions, Ver(\"2.1.1\")) == 4  # Ver(\"2.1.0\")\n    assert find(baseline_versions, Ver(\"3.0\")) == 5  # Ver(\"3.0\")\n    assert find(baseline_versions, Ver(\"3.0.0\")) == 6  # Ver(\"3.0.0\")\n    assert find(baseline_versions, Ver(\"3.0.1\")) == 6  # Ver(\"3.0.0\")\n    assert find(baseline_versions, Ver(\"3.1\")) == 6  # Ver(\"3.0.0\")\n    assert find(baseline_versions, Ver(\"99.99\")) == 6  # Ver(\"3.0.0\")\n    assert find(baseline_versions, Ver(\"99.99.99\")) == 6  # Ver(\"3.0.0\")", "\n\ndef test_find_closest_baseline_log_file():\n    find = baseline_utils.find_closest_baseline_info\n\n    versions_to_test_info = [\n        (\"1.0.0\", utils.ProductName.SPEEDB, None, None),\n        (\"2.2.1\", utils.ProductName.SPEEDB, \"LOG-speedb-2.2.1\",\n         \"2.2.1\"),\n        (\"2.2.2\", utils.ProductName.SPEEDB, \"LOG-speedb-2.2.1\",\n         \"2.2.1\"),\n        (\"32.0.1\", utils.ProductName.SPEEDB, \"LOG-speedb-2.5.0\",\n         \"2.5.0\"),\n        (\"6.0.0\", utils.ProductName.ROCKSDB, None, None),\n        (\"6.26.0\", utils.ProductName.ROCKSDB,\n         \"LOG-rocksdb-6.26.0\", \"6.26.0\"),\n        (\"6.27.0\", utils.ProductName.ROCKSDB,\n         \"LOG-rocksdb-6.26.0\", \"6.26.0\"),\n        (\"7.10.11\", utils.ProductName.ROCKSDB,\n         \"LOG-rocksdb-7.9.2\", \"7.9.2\"),\n        (\"8.0.0\", utils.ProductName.ROCKSDB,\n         \"LOG-rocksdb-8.0.0\", \"8.0.0\"),\n        (\"9.0.0\", utils.ProductName.ROCKSDB,\n         \"LOG-rocksdb-8.0.0\", \"8.0.0\")\n    ]\n\n    for version_info in versions_to_test_info:\n        closest_baseline_info = \\\n            find(baseline_logs_folder_path,\n                 version_info[1],\n                 Ver(version_info[0]))\n        if version_info[2] is None:\n            assert closest_baseline_info is None\n        else:\n            assert closest_baseline_info is not None\n            assert closest_baseline_info.file_path.name == version_info[2]\n            assert closest_baseline_info.version == Ver(version_info[3])", "\n\ndef test_find_options_diff():\n    baseline_info_rocksdb_7_7_3 = \\\n        baseline_utils.get_baseline_database_options(\n            baseline_logs_folder_path,\n            utils.ProductName.ROCKSDB,\n            version_str=\"7.7.3\")\n    assert isinstance(baseline_info_rocksdb_7_7_3,\n                      baseline_utils.BaselineDBOptionsInfo)\n\n    database_options_rocksdb_7_7_3 = \\\n        baseline_info_rocksdb_7_7_3.baseline_options\n\n    diff_rocksdb_7_7_3_vs_itself = \\\n        baseline_utils.find_options_diff_relative_to_baseline(\n            baseline_logs_folder_path,\n            utils.ProductName.ROCKSDB,\n            Ver(\"7.7.3\"),\n            database_options_rocksdb_7_7_3)\n    assert isinstance(diff_rocksdb_7_7_3_vs_itself,\n                      baseline_utils.OptionsDiffRelToBaselineInfo)\n    assert diff_rocksdb_7_7_3_vs_itself.diff is None\n    assert diff_rocksdb_7_7_3_vs_itself.closest_version == Ver(\"7.7.3\")\n\n    baseline_info_rocksdb_2_1_0 = \\\n        baseline_utils.get_baseline_database_options(\n            baseline_logs_folder_path,\n            utils.ProductName.SPEEDB,\n            version_str=\"2.1.0\")\n\n    database_options_speedb_2_1_0 = \\\n        baseline_info_rocksdb_2_1_0.baseline_options\n    diff_speedb_2_1_0_vs_itself = \\\n        baseline_utils.find_options_diff_relative_to_baseline(\n            baseline_logs_folder_path,\n            utils.ProductName.SPEEDB,\n            Ver(\"2.1.0\"),\n            database_options_speedb_2_1_0)\n    assert diff_speedb_2_1_0_vs_itself.diff is None\n    assert diff_speedb_2_1_0_vs_itself.closest_version == Ver(\"2.1.0\")\n\n    expected_diff = {}\n    updated_database_options_2_1_0 = database_options_speedb_2_1_0\n\n    curr_max_open_files_value = \\\n        updated_database_options_2_1_0.get_db_wide_option('max_open_files')\n    new_max_open_files_value = str(int(curr_max_open_files_value) + 100)\n    updated_database_options_2_1_0.set_db_wide_option('max_open_files',\n                                                      new_max_open_files_value)\n    expected_diff['DBOptions.max_open_files'] = \\\n        {utils.NO_CF: (curr_max_open_files_value,\n                       new_max_open_files_value)}\n\n    updated_database_options_2_1_0.set_db_wide_option(\"NEW_DB_WIDE_OPTION1\",\n                                                      \"NEW_DB_WIDE_VALUE1\",\n                                                      True)\n    expected_diff['DBOptions.NEW_DB_WIDE_OPTION1'] = \\\n        {utils.NO_CF: (db_options.SANITIZED_NO_VALUE, \"NEW_DB_WIDE_VALUE1\")}\n\n    cf_name = \"default\"\n\n    curr_ttl_value = updated_database_options_2_1_0.get_cf_option(cf_name,\n                                                                  \"ttl\")\n    new_ttl_value = str(int(curr_ttl_value) + 1000)\n    updated_database_options_2_1_0.set_cf_option(cf_name, \"ttl\",\n                                                 new_ttl_value)\n    updated_database_options_2_1_0.set_cf_option('default', 'ttl',\n                                                 new_ttl_value)\n    expected_diff['CFOptions.ttl'] = {'default': (curr_ttl_value,\n                                                  new_ttl_value)}\n\n    updated_database_options_2_1_0.set_cf_option(cf_name, \"NEW_CF_OPTION1\",\n                                                 \"NEW_CF_VALUE1\", \"True\")\n    expected_diff['CFOptions.NEW_CF_OPTION1'] =\\\n        {'default': (db_options.SANITIZED_NO_VALUE, \"NEW_CF_VALUE1\")}\n\n    curr_block_align_value = \\\n        updated_database_options_2_1_0.get_cf_table_option(cf_name,\n                                                           \"block_align\")\n    new_block_align_value = \"dummy_block_align_value\"\n    updated_database_options_2_1_0.set_cf_table_option(cf_name,\n                                                       \"block_align\",\n                                                       new_block_align_value)\n    expected_diff['TableOptions.BlockBasedTable.block_align'] = \\\n        {'default': (curr_block_align_value, new_block_align_value)}\n    updated_database_options_2_1_0.set_cf_table_option(cf_name,\n                                                       \"NEW_CF_TABLE_OPTION1\",\n                                                       \"NEW_CF_TABLE_VALUE1\",\n                                                       \"True\")\n    expected_diff['TableOptions.BlockBasedTable.NEW_CF_TABLE_OPTION1'] = \\\n        {'default': (db_options.SANITIZED_NO_VALUE, \"NEW_CF_TABLE_VALUE1\")}\n\n    diff_speedb_2_1_0_vs_itself = \\\n        baseline_utils.find_options_diff_relative_to_baseline(\n            baseline_logs_folder_path,\n            utils.ProductName.SPEEDB,\n            Ver(\"2.1.0\"),\n            updated_database_options_2_1_0)\n    assert diff_speedb_2_1_0_vs_itself.diff.get_diff_dict() == expected_diff\n    assert diff_speedb_2_1_0_vs_itself.closest_version == Ver(\"2.1.0\")", ""]}
{"filename": "test/log_analyzer_test_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport log_entry\n\n\ndef read_sample_file(file_name, expected_num_entries):\n    file_path = \"input_files/\" + file_name\n    f = open(file_path)\n    lines = f.readlines()\n    entries = []\n    entry = None\n    for i, line in enumerate(lines):\n        if log_entry.LogEntry.is_entry_start(line):\n            if entry:\n                entries.append(entry.all_lines_added())\n            entry = log_entry.LogEntry(i, line)\n        else:\n            assert entry\n            entry.add_line(line)\n\n    if entry:\n        entries.append(entry.all_lines_added())\n\n    assert len(entries) == expected_num_entries\n\n    return entries", "\ndef read_sample_file(file_name, expected_num_entries):\n    file_path = \"input_files/\" + file_name\n    f = open(file_path)\n    lines = f.readlines()\n    entries = []\n    entry = None\n    for i, line in enumerate(lines):\n        if log_entry.LogEntry.is_entry_start(line):\n            if entry:\n                entries.append(entry.all_lines_added())\n            entry = log_entry.LogEntry(i, line)\n        else:\n            assert entry\n            entry.add_line(line)\n\n    if entry:\n        entries.append(entry.all_lines_added())\n\n    assert len(entries) == expected_num_entries\n\n    return entries", ""]}
{"filename": "test/__init__.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n"]}
{"filename": "test/test_cfs_infos.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport test.testing_utils as test_utils\nfrom cfs_infos import CfMetadata, CfsMetadata, CfDiscoveryType\n\ndefault = \"default\"", "\ndefault = \"default\"\ncf1 = \"cf1\"\ncf2 = \"cf2\"\ncf3 = \"cf3\"\ncf1_id = 10\ncf2_id = 20\n\ncf1_options_line = \\\n    '2023/03/07-16:05:55.538873 8379 [/column_family.cc:631] ' \\", "cf1_options_line = \\\n    '2023/03/07-16:05:55.538873 8379 [/column_family.cc:631] ' \\\n    f'--------------- Options for column family [{cf1}]:'\n\n\ncf1_recovered_line = \\\n    '2023/03/07-16:05:55.532774 8379 [/version_set.cc:5585] ' \\\n    f'Column family [{cf1}] (ID {cf1_id}), log number is 0'.strip()\n\n", "\n\ncf1_cf2_id_recovered_line = \\\n    '2023/03/07-16:05:55.532774 8379 [/version_set.cc:5585] ' \\\n    f'Column family [{cf1}] (ID {cf2_id}), log number is 0'.strip()\n\ncf1_created_line = \\\n    '2023/03/07-16:06:09.051479 8432 [/db_impl/db_impl.cc:3200] ' \\\n    f'Created column family [{cf1}] (ID {cf1_id})'\n", "    f'Created column family [{cf1}] (ID {cf1_id})'\n\ncf1_cf2_id_created_line = \\\n    '2023/03/07-16:06:09.051479 8432 [/db_impl/db_impl.cc:3200] ' \\\n    f'Created column family [{cf1}] (ID {cf2_id})'\n\ncf2_created_line = \\\n    '2023/03/07-16:06:09.051479 8432 [/db_impl/db_impl.cc:3200] ' \\\n    f'Created column family [{cf2}] (ID {cf2_id})'\n", "    f'Created column family [{cf2}] (ID {cf2_id})'\n\ncf2_cf1_id_created_line = \\\n    '2023/03/07-16:06:09.051479 8432 [/db_impl/db_impl.cc:3200] ' \\\n    f'Created column family [{cf2}] (ID {cf1_id})'\n\ncf1_drop_time = '2023/03/07-16:06:09.037627'\ncf1_dropped_line = \\\n    f'{cf1_drop_time} 8432 [/db_impl/db_impl.cc:3311] ' \\\n    f'Dropped column family with id {cf1_id}'", "    f'{cf1_drop_time} 8432 [/db_impl/db_impl.cc:3311] ' \\\n    f'Dropped column family with id {cf1_id}'\n\ncf2_dropped_line = \\\n    '2023/03/07-16:06:09.037627 8432 [/db_impl/db_impl.cc:3311] ' \\\n    f'Dropped column family with id {cf2_id}'\n\ncf1_options_entry = test_utils.line_to_entry(cf1_options_line)\ncf1_recovered_entry = test_utils.line_to_entry(cf1_recovered_line)\ncf1_cf2_id_recovered_entry = \\", "cf1_recovered_entry = test_utils.line_to_entry(cf1_recovered_line)\ncf1_cf2_id_recovered_entry = \\\n    test_utils.line_to_entry(cf1_cf2_id_recovered_line)\n\ncf1_created_entry = test_utils.line_to_entry(cf1_created_line)\ncf1_cf2_id_created_entry = test_utils.line_to_entry(cf1_cf2_id_created_line)\n\ncf2_created_entry = test_utils.line_to_entry(cf2_created_line)\ncf2_cf1_id_created_entry = test_utils.line_to_entry(cf2_cf1_id_created_line)\n", "cf2_cf1_id_created_entry = test_utils.line_to_entry(cf2_cf1_id_created_line)\n\ncf1_dropped_entry = test_utils.line_to_entry(cf1_dropped_line)\ncf2_dropped_entry = test_utils.line_to_entry(cf2_dropped_line)\n\n\ndef test_empty():\n    cfs = CfsMetadata(\"dummy-path\")\n    assert cfs.get_cf_id(cf1) is None\n    assert cfs.get_cf_info_by_name(cf1) is None\n    assert cfs.get_cf_info_by_id(cf1_id) is None\n    assert cfs.get_non_auto_generated_cfs_names() == []\n    assert cfs.get_auto_generated_cf_names() == []\n    assert cfs.get_num_cfs() == 0", "\n\ndef test_add_cf_found_during_options_parsing_cf_name_known():\n    cfs = CfsMetadata(\"dummy-path\")\n\n    assert cfs.add_cf_found_during_cf_options_parsing(cf1,\n                                                      cf1_id,\n                                                      is_auto_generated=False,\n                                                      entry=cf1_options_entry)\n    expected_cf_metadata = \\\n        CfMetadata(discovery_type=CfDiscoveryType.OPTIONS,\n                   name=cf1,\n                   discovery_time='2023/03/07-16:05:55.538873',\n                   has_options=True,\n                   auto_generated=False,\n                   id=cf1_id,\n                   drop_time=None)\n    assert cfs.get_cf_id(cf1) == cf1_id\n    assert cfs.get_cf_info_by_name(cf1) == expected_cf_metadata\n    assert cfs.get_cf_info_by_id(cf1_id) == expected_cf_metadata\n    assert cfs.get_non_auto_generated_cfs_names() == [cf1]\n    assert cfs.get_auto_generated_cf_names() == []\n    assert cfs.get_num_cfs() == 1\n    assert not cfs.was_cf_dropped(cf1)\n    assert cfs.get_cf_drop_time(cf1) is None", "\n\ndef test_add_cf_found_during_options_parsing_auto_generated_cf_name():\n    cfs = CfsMetadata(\"dummy-path\")\n\n    unknown_cf_name = \"Unknonw-1\"\n    options_entry = \\\n        test_utils.line_to_entry(\n            '2023/01/23-22:09:21.013513 7f6bdfd54700 '\n            'Options.comparator: leveldb.BytewiseComparator')\n\n    assert cfs.add_cf_found_during_cf_options_parsing(cf_name=unknown_cf_name,\n                                                      cf_id=None,\n                                                      is_auto_generated=True,\n                                                      entry=options_entry)\n    expected_cf_metadata = \\\n        CfMetadata(discovery_type=CfDiscoveryType.OPTIONS,\n                   name=unknown_cf_name,\n                   discovery_time='2023/01/23-22:09:21.013513',\n                   has_options=True,\n                   auto_generated=True,\n                   id=None,\n                   drop_time=None)\n    assert cfs.get_cf_id(cf1) is None\n    assert cfs.get_cf_info_by_name(unknown_cf_name) == expected_cf_metadata\n    assert cfs.get_non_auto_generated_cfs_names() == []\n    assert cfs.get_auto_generated_cf_names() == [unknown_cf_name]\n    assert cfs.get_num_cfs() == 1\n    assert not cfs.was_cf_dropped(cf1)\n    assert cfs.get_cf_drop_time(cf1) is None", "\n\ndef test_handle_cf_name_found_during_parsing():\n    cfs = CfsMetadata(\"dummy-path\")\n\n    assert cfs.add_cf_found_during_cf_options_parsing(cf1,\n                                                      cf1_id,\n                                                      is_auto_generated=False,\n                                                      entry=cf1_options_entry)\n\n    stats_start_entry =\\\n        test_utils.line_to_entry('2023/01/23-22:19:21.014278 7f6bdfd54700 ['\n                                 '/db_impl/db_impl.cc:947]')\n    assert not cfs.handle_cf_name_found_during_parsing(cf1, stats_start_entry)\n    assert cfs.get_num_cfs() == 1\n\n    stats_start_entry =\\\n        test_utils.line_to_entry('2023/01/23-22:19:21.014278 7f6bdfd54700 ['\n                                 '/db_impl/db_impl.cc:947]')\n    assert cfs.handle_cf_name_found_during_parsing(cf2,\n                                                   stats_start_entry)\n    expected_cf_metadata = \\\n        CfMetadata(discovery_type=CfDiscoveryType.DURING_PARSING,\n                   name=cf2,\n                   discovery_time='2023/01/23-22:19:21.014278',\n                   has_options=False,\n                   auto_generated=False,\n                   id=None,\n                   drop_time=None)\n    assert cfs.get_cf_id(cf2) is None\n    assert cfs.get_cf_info_by_name(cf2) == expected_cf_metadata\n    assert cfs.get_cf_info_by_id(cf2_id) is None\n    assert cfs.get_non_auto_generated_cfs_names() == [cf1, cf2]\n    assert cfs.get_auto_generated_cf_names() == []\n    assert cfs.get_num_cfs() == 2", "\n\ndef test_parse_cf_recovered_entry():\n    cfs = CfsMetadata(\"dummy-path\")\n\n    assert cfs.add_cf_found_during_cf_options_parsing(cf1,\n                                                      cf_id=None,\n                                                      is_auto_generated=False,\n                                                      entry=cf1_options_entry)\n    assert cfs.get_cf_id(cf1) is None\n\n    assert cfs.try_parse_as_cf_lifetime_entries(\n        [cf1_recovered_entry], entry_idx=0) == (True, 1)\n    assert cfs.get_cf_id(cf1) == 10\n\n    # Illegal (different cf_id), but should be silently ignored\n    assert cfs.try_parse_as_cf_lifetime_entries(\n        [cf1_cf2_id_recovered_entry], entry_idx=0) == (True, 1)\n    assert cfs.get_cf_id(cf1) == 10\n\n    assert cfs.try_parse_as_cf_lifetime_entries([cf1_recovered_entry],\n                                                entry_idx=0) == (True, 1)\n    assert cfs.get_cf_id(cf1) == 10", "\n\ndef test_parse_cf_recovered_entry_2():\n    cfs = CfsMetadata(\"dummy-path\")\n\n    # Recovered without options\n    assert cfs.try_parse_as_cf_lifetime_entries(\n        [cf1_recovered_entry], entry_idx=0) == (True, 1)\n    assert cfs.get_cf_id(cf1) == 10\n\n    # Already discovered without options => Rejected\n    assert not cfs.add_cf_found_during_cf_options_parsing(\n        cf1, cf_id=None, is_auto_generated=False, entry=cf1_recovered_entry)\n    assert cfs.get_cf_id(cf1) == 10", "\n\ndef test_parse_cf_created_entry():\n    cfs = CfsMetadata(\"dummy-path\")\n\n    assert cfs.add_cf_found_during_cf_options_parsing(\n        cf1, cf_id=None, is_auto_generated=False, entry=cf1_recovered_entry)\n    assert cfs.get_cf_id(cf1) is None\n\n    assert cfs.try_parse_as_cf_lifetime_entries([cf1_created_entry],\n                                                entry_idx=0) == (True, 1)\n    assert cfs.get_cf_id(cf1) == cf1_id\n\n    # Already discovered without options => Rejected\n    assert cfs.try_parse_as_cf_lifetime_entries([cf1_cf2_id_recovered_entry],\n                                                entry_idx=0) == (True, 1)\n    assert cfs.get_cf_id(cf1) == cf1_id", "\n\ndef test_parse_cf_created_entry_2():\n    cfs = CfsMetadata(\"dummy-path\")\n\n    # Created without options\n    cfs.try_parse_as_cf_lifetime_entries(\n        [cf1_created_entry], entry_idx=0) == (True, 1)\n    assert cfs.get_cf_id(cf1) == cf1_id\n\n    assert not cfs.add_cf_found_during_cf_options_parsing(\n        cf1, cf_id=None, is_auto_generated=False, entry=cf1_recovered_entry)\n    assert cfs.get_cf_id(cf1) == cf1_id", "\n\ndef test_parse_cf_dropped_entry():\n    cfs = CfsMetadata(\"dummy-path\")\n\n    # Dropping a cf doesn't specify the dropped cf's name, only its id =>\n    # We may have an auto-generated cf (=> no id) that is later dropped\n    cfs.try_parse_as_cf_lifetime_entries([cf1_dropped_entry],\n                                         entry_idx=0)\n    assert not cfs.was_cf_dropped(cf1)\n    assert cfs.get_cf_drop_time(cf1) is None\n\n    assert cfs.add_cf_found_during_cf_options_parsing(\n        cf1, cf_id=cf1_id, is_auto_generated=False, entry=cf1_recovered_entry)\n    assert cfs.get_cf_id(cf1) == cf1_id\n\n    assert cfs.try_parse_as_cf_lifetime_entries([cf1_dropped_entry],\n                                                entry_idx=0) == (True, 1)\n    assert cfs.get_cf_id(cf1) == cf1_id\n    assert cfs.was_cf_dropped(cf1)\n    assert cfs.get_cf_drop_time(cf1) == cf1_drop_time\n\n    assert cfs.try_parse_as_cf_lifetime_entries(\n        [cf1_dropped_entry], entry_idx=0) == (True, 1)\n    assert cfs.was_cf_dropped(cf1)\n    assert cfs.get_cf_drop_time(cf1) == cf1_drop_time", ""]}
{"filename": "test/test_log_file_options_parser.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nfrom log_entry import LogEntry\nfrom log_file_options_parser import get_table_options_topic_info, \\\n    LogFileOptionsParser\nfrom test.sample_log_info import SampleLogInfo, SampleRolledLogInfo", "    LogFileOptionsParser\nfrom test.sample_log_info import SampleLogInfo, SampleRolledLogInfo\n\n\ndef test_get_table_options_topic_info():\n    assert get_table_options_topic_info(\n        \"metadata_cache_options\") == (\"metadata_cache_options\",\n                                      \"metadata_cache_\")\n    assert get_table_options_topic_info(\n        \"block_cache_options\") == (\"block_cache_options\", \"block_cache_\")\n\n    assert get_table_options_topic_info(\"block_cache\") is None", "\n\ndef read_sample_file(InfoClass):\n    f = open(InfoClass.FILE_PATH)\n    lines = f.readlines()\n    entries = []\n    entry = None\n    for i, line in enumerate(lines):\n        if LogEntry.is_entry_start(line):\n            if entry:\n                entries.append(entry.all_lines_added())\n            entry = LogEntry(i, line)\n        else:\n            assert entry\n            entry.add_line(line)\n\n    if entry:\n        entries.append(entry.all_lines_added())\n\n    assert len(entries) == InfoClass.NUM_ENTRIES\n\n    return entries", "\n\ndef test_try_parsing_as_options_entry():\n    date = \"2022/04/17-14:13:10.725596 7f4a9fdff700\"\n    context = \"7f4a9fdff700\"\n    line_start = date + \" \" + context + \" \"\n    option1 = \"Options.track_and_verify_wals_in_manifest: 0\"\n    option2 = \"Options.wal_dir: /data/rocksdb2/\"\n    option3 = \"Options.statistics: (nil)\"\n    option4 = \"Options.comparator: leveldb.BytewiseComparator\"\n    table_file_option = \"data_block_index_type: 0\"\n\n    assert (\"track_and_verify_wals_in_manifest\", \"0\") == \\\n           LogFileOptionsParser.try_parsing_as_options_entry(\n               LogEntry(0, line_start + option1, True))\n    assert (\"track_and_verify_wals_in_manifest\", \"0\") == \\\n           LogFileOptionsParser.try_parsing_as_options_entry(\n               LogEntry(0, line_start + option1 + \"   \", True))\n\n    assert (\"wal_dir\", \"/data/rocksdb2/\") == \\\n           LogFileOptionsParser.try_parsing_as_options_entry(\n               LogEntry(0, line_start + option2, True))\n    assert (\"statistics\", \"(nil)\") == \\\n           LogFileOptionsParser.try_parsing_as_options_entry(\n               LogEntry(0, line_start + option3, True))\n    assert (\"comparator\", \"leveldb.BytewiseComparator\") == \\\n           LogFileOptionsParser.try_parsing_as_options_entry(\n               LogEntry(0, line_start + option4, True))\n\n    assert not LogFileOptionsParser.try_parsing_as_options_entry(\n        LogEntry(0, line_start, True))\n    assert not LogFileOptionsParser.try_parsing_as_options_entry(\n        LogEntry(0, line_start + \"   \", True))\n    assert not LogFileOptionsParser.try_parsing_as_options_entry(\n        LogEntry(0, line_start + \"Options.xxx\", True))\n    assert not LogFileOptionsParser.try_parsing_as_options_entry(\n        LogEntry(0, line_start + table_file_option, True))", "\n\ndef test_try_parsing_as_table_options_entry():\n    date = \"2022/04/17-14:13:10.725596\"\n    context = \"7f4a9fdff700\"\n    line_start = date + \" \" + context + \" \"\n    option1 = \"Options.wal_dir: /data/rocksdb2/\"\n    table_options_start = \"table_factory options:\"\n    table_options_line_start = line_start + table_options_start\n    table_option1 = \"flush_block_policy_factory: \" \\\n                    \"FlushBlockBySizePolicyFactory (0x7f4af4091b90)\"\n    table_option2 = \"cache_index_and_filter_blocks: 1\"\n    table_option_special_value = \" metadata_cache_options: \"\n    table_option_special_value_cont1 = \"  partition_pinning: 0 \"\n\n    expected_result = dict()\n\n    table_options_entry = LogEntry(0, table_options_line_start + \" \" +\n                                   table_option1)\n    expected_result[\"flush_block_policy_factory\"] = \\\n        \"FlushBlockBySizePolicyFactory (0x7f4af4091b90)\"\n    actual_result = LogFileOptionsParser.try_parsing_as_table_options_entry(\n        table_options_entry)\n    assert expected_result == actual_result\n\n    table_options_entry.add_line(table_option2)\n    expected_result[\"cache_index_and_filter_blocks\"] = '1'\n    actual_result = LogFileOptionsParser.try_parsing_as_table_options_entry(\n        table_options_entry)\n    assert expected_result == actual_result\n\n    table_options_entry.add_line(table_option_special_value)\n    actual_result = LogFileOptionsParser.try_parsing_as_table_options_entry(\n        table_options_entry)\n    assert expected_result == actual_result\n\n    table_options_entry.add_line(table_option_special_value_cont1)\n    expected_result[\"metadata_cache_partition_pinning\"] = \"0\"\n    actual_result = LogFileOptionsParser.try_parsing_as_table_options_entry(\n        table_options_entry)\n    assert actual_result == expected_result\n\n    options_entry = LogEntry(0, line_start + option1, True)\n    assert LogFileOptionsParser.try_parsing_as_options_entry(options_entry)\n    assert not LogFileOptionsParser.try_parsing_as_table_options_entry(\n        options_entry)", "\n\ndef test_parse_db_wide_options():\n    log_entries = read_sample_file(SampleLogInfo)\n\n    start_entry_idx = SampleLogInfo.DB_WIDE_OPTIONS_START_ENTRY_IDX\n    actual_options_dict =\\\n        LogFileOptionsParser.parse_db_wide_options(\n            log_entries,\n            start_entry_idx,\n            SampleLogInfo.SUPPORT_INFO_START_ENTRY_IDX)\n\n    assert actual_options_dict == SampleLogInfo.DB_WIDE_OPTIONS_DICT", "\n\ndef test_parsing_as_table_options_entry():\n    log_entries = read_sample_file(SampleLogInfo)\n\n    for i, idx in enumerate(SampleLogInfo.TABLE_OPTIONS_ENTRIES_INDICES):\n        actual_options_dict = \\\n            LogFileOptionsParser.try_parsing_as_table_options_entry(\n                log_entries[idx])\n        assert SampleLogInfo.TABLE_OPTIONS_DICTS[i] == actual_options_dict", "\n\ndef test_parse_cf_options_with_cf_options_header():\n    log_entries = read_sample_file(SampleLogInfo)\n\n    for i, idx in enumerate(SampleLogInfo.OPTIONS_ENTRIES_INDICES):\n        cf_name, options_dict, table_options_dict, entry_idx, \\\n            duplicate_option =\\\n            LogFileOptionsParser.parse_cf_options(log_entries, idx)\n\n        assert cf_name == SampleLogInfo.CF_NAMES[i]\n        assert options_dict == SampleLogInfo.OPTIONS_DICTS[i]\n        assert table_options_dict == SampleLogInfo.TABLE_OPTIONS_DICTS[i]\n        assert not duplicate_option\n\n        # +1 entry for the cf options start line (not a cf-options entry)\n        # +1 for the table options entry (single entry)\n        num_parsed_entries = 1 + len(options_dict) + 1\n        assert entry_idx == idx + num_parsed_entries", "\n\ndef test_parse_cf_options_without_cf_options_header():\n    log_entries = read_sample_file(SampleRolledLogInfo)\n\n    for i, idx in enumerate(SampleRolledLogInfo.OPTIONS_ENTRIES_INDICES):\n        provided_cf_name = SampleLogInfo.CF_NAMES[i]\n        cf_name, options_dict, table_options_dict, entry_idx, \\\n            duplicate_option =\\\n            LogFileOptionsParser.parse_cf_options(log_entries, idx,\n                                                  provided_cf_name)\n\n        assert cf_name == provided_cf_name\n        assert options_dict == SampleLogInfo.OPTIONS_DICTS[i]\n        assert table_options_dict == SampleLogInfo.TABLE_OPTIONS_DICTS[i]\n        # the last cf should NOT have a duplicate, the ones befire it should\n        assert duplicate_option == (i+1 < len(\n            SampleRolledLogInfo.OPTIONS_ENTRIES_INDICES))\n\n        # +1 for the table options entry (single entry)\n        num_parsed_entries = len(options_dict) + 1\n        assert entry_idx == idx + num_parsed_entries", ""]}
{"filename": "test/test_warnings_mngr.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport utils\nfrom log_entry import LogEntry\nfrom warnings_mngr import WarningCategory, WarningElementInfo, WarningsMngr\n", "from warnings_mngr import WarningCategory, WarningElementInfo, WarningsMngr\n\n\ndef create_warning_entry(warning_type, cf_name, warning_element_info):\n    assert isinstance(warning_type, utils.WarningType)\n    assert isinstance(warning_element_info, WarningElementInfo)\n\n    warning_line = warning_element_info.time + \" \"\n    warning_line += '7f4a8b5bb700 '\n\n    warning_line += f'[{warning_type.name}] '\n    warning_line += f'[{warning_element_info.code_pos}] '\n\n    if cf_name != utils.NO_CF:\n        warning_line += f'[{cf_name}] '\n    warning_line += warning_element_info.warning_msg\n\n    warning_entry = LogEntry(0, warning_line, True)\n    assert warning_entry.is_warn_msg()\n    assert warning_entry.get_warning_type() == warning_type\n    assert warning_entry.get_code_pos() == warning_element_info.code_pos\n\n    return warning_entry", "\n\ndef add_warning(mngr, warning_type, time, code_pos, cf_name, warn_msg):\n    assert isinstance(warning_type, utils.WarningType)\n\n    warning_element_info = WarningElementInfo(time, code_pos, warn_msg)\n    warn_entry = create_warning_entry(\n        warning_type, cf_name, warning_element_info)\n    assert mngr.try_adding_entry(warn_entry)\n", "\n\ndef test_non_warnings_entries():\n    line1 = '''2022/04/17-14:21:50.026058 7f4a8b5bb700 [/flush_job.cc:333]\n    [cf1] [JOB 9] Flushing memtable with next log file: 5'''\n\n    line2 = '''2022/04/17-14:21:50.026087 7f4a8b5bb700 EVENT_LOG_v1\n    {\"time_micros\": 1650205310026076, \"job\": 9, \"event\": \"flush_started\"'''\n\n    cfs_names = [\"cf1\", \"cf2\"]\n    warnings_mngr = WarningsMngr()\n\n    assert LogEntry.is_entry_start(line1)\n    entry1 = LogEntry(0, line1, True)\n    assert LogEntry.is_entry_start(line2)\n    entry2 = LogEntry(0, line2, True)\n\n    assert not warnings_mngr.try_adding_entry(entry1)\n    assert not warnings_mngr.try_adding_entry(entry2)\n    warnings_mngr.set_cfs_names_on_parsing_complete(cfs_names)", "\n\ndef test_warn_entries_empty():\n    mngr = WarningsMngr()\n    mngr.set_cfs_names_on_parsing_complete([\"cf1\", \"cf2\"])\n    assert mngr.get_all_warnings() == {}\n\n\ndef test_warn_entries_basic():\n    cf1 = \"cf1\"\n    cf2 = \"cf2\"\n    cf3 = \"cf3\"\n    cfs_names = [cf1, cf2, cf3]\n\n    time1 = \"2022/04/17-14:21:50.026058\"\n    time2 = \"2022/04/17-14:21:51.026058\"\n    time3 = \"2022/04/17-14:21:52.026058\"\n    time4 = \"2022/04/17-14:21:53.026058\"\n    time5 = \"2022/04/17-14:21:54.026058\"\n\n    warn_msg1 = \"Warning Message 1\"\n    cf_warn_msg1 = f\"[{cf1}] {warn_msg1}\"\n    warn_msg2 = \"Warning Message 2\"\n    cf_warn_msg2 = f\"[{cf2}] {warn_msg2}\"\n    warn_msg3 = \"Warning Message 3\"\n    cf_warn_msg3 = f\"[{cf2}] {warn_msg3}\"\n\n    delay_msg = \"Stalling writes, L0 files 2, memtables 2\"\n    cf_delay_msg = f\"[{cf2}] {delay_msg}\"\n    stop_msg = \"Stopping writes Dummy Text 1\"\n    cf_stop_msg = f\"[{cf1}] {stop_msg}\"\n\n    code_pos1 = \"/flush_job.cc:333\"\n    code_pos2 = \"/column_family.cc:932\"\n    code_pos3 = \"/column_family1.cc:999\"\n    code_pos4 = \"/column_family2.cc:1111\"\n\n    mngr = WarningsMngr()\n    add_warning(mngr, utils.WarningType.WARN, time1, code_pos1, cf1, warn_msg1)\n    add_warning(mngr, utils.WarningType.ERROR, time2,\n                code_pos1, cf2, warn_msg2)\n    add_warning(mngr, utils.WarningType.WARN, time3, code_pos2, cf2, warn_msg3)\n    add_warning(mngr, utils.WarningType.WARN, time4, code_pos3, cf2, delay_msg)\n    add_warning(mngr, utils.WarningType.WARN, time5, code_pos4, cf1, stop_msg)\n    mngr.set_cfs_names_on_parsing_complete(cfs_names)\n\n    expected_cf1_warn_warnings = {\n        WarningCategory.WRITE_STOP:\n            [WarningElementInfo(time5, code_pos4, cf_stop_msg)],\n        WarningCategory.OTHER:\n            [WarningElementInfo(time1, code_pos1, cf_warn_msg1)]\n    }\n    expected_cf2_warn_warnings = {\n        WarningCategory.WRITE_DELAY:\n            [WarningElementInfo(time4, code_pos3, cf_delay_msg)],\n        WarningCategory.OTHER:\n            [WarningElementInfo(time3, code_pos2, cf_warn_msg3)]\n    }\n    expected_cf2_error_warnings = {\n        WarningCategory.OTHER:\n            [WarningElementInfo(time2, code_pos1, cf_warn_msg2)]}\n\n    expected_warn_warnings = {\n            cf1: expected_cf1_warn_warnings,\n            cf2: expected_cf2_warn_warnings\n    }\n    expected_error_warnings = {cf2: expected_cf2_error_warnings}\n\n    all_expected_warnings = {\n        utils.WarningType.WARN: expected_warn_warnings,\n        utils.WarningType.ERROR: expected_error_warnings\n    }\n\n    actual_cf1_warn_warnings = mngr.get_cf_warn_warnings(cf1)\n    assert actual_cf1_warn_warnings == expected_cf1_warn_warnings\n\n    actual_cf2_warn_warnings = mngr.get_cf_warn_warnings(cf2)\n    assert actual_cf2_warn_warnings == expected_cf2_warn_warnings\n\n    actual_cf2_error_warnings = mngr.get_cf_error_warnings(cf2)\n    assert actual_cf2_error_warnings == expected_cf2_error_warnings\n\n    assert mngr.get_cf_warn_warnings(cf3) is None\n    assert mngr.get_cf_error_warnings(cf3) is None\n\n    actual_warn_warnings = mngr.get_warn_warnings()\n    assert expected_warn_warnings == actual_warn_warnings\n\n    all_actual_warnings = mngr.get_all_warnings()\n    assert all_actual_warnings == all_expected_warnings", "def test_warn_entries_basic():\n    cf1 = \"cf1\"\n    cf2 = \"cf2\"\n    cf3 = \"cf3\"\n    cfs_names = [cf1, cf2, cf3]\n\n    time1 = \"2022/04/17-14:21:50.026058\"\n    time2 = \"2022/04/17-14:21:51.026058\"\n    time3 = \"2022/04/17-14:21:52.026058\"\n    time4 = \"2022/04/17-14:21:53.026058\"\n    time5 = \"2022/04/17-14:21:54.026058\"\n\n    warn_msg1 = \"Warning Message 1\"\n    cf_warn_msg1 = f\"[{cf1}] {warn_msg1}\"\n    warn_msg2 = \"Warning Message 2\"\n    cf_warn_msg2 = f\"[{cf2}] {warn_msg2}\"\n    warn_msg3 = \"Warning Message 3\"\n    cf_warn_msg3 = f\"[{cf2}] {warn_msg3}\"\n\n    delay_msg = \"Stalling writes, L0 files 2, memtables 2\"\n    cf_delay_msg = f\"[{cf2}] {delay_msg}\"\n    stop_msg = \"Stopping writes Dummy Text 1\"\n    cf_stop_msg = f\"[{cf1}] {stop_msg}\"\n\n    code_pos1 = \"/flush_job.cc:333\"\n    code_pos2 = \"/column_family.cc:932\"\n    code_pos3 = \"/column_family1.cc:999\"\n    code_pos4 = \"/column_family2.cc:1111\"\n\n    mngr = WarningsMngr()\n    add_warning(mngr, utils.WarningType.WARN, time1, code_pos1, cf1, warn_msg1)\n    add_warning(mngr, utils.WarningType.ERROR, time2,\n                code_pos1, cf2, warn_msg2)\n    add_warning(mngr, utils.WarningType.WARN, time3, code_pos2, cf2, warn_msg3)\n    add_warning(mngr, utils.WarningType.WARN, time4, code_pos3, cf2, delay_msg)\n    add_warning(mngr, utils.WarningType.WARN, time5, code_pos4, cf1, stop_msg)\n    mngr.set_cfs_names_on_parsing_complete(cfs_names)\n\n    expected_cf1_warn_warnings = {\n        WarningCategory.WRITE_STOP:\n            [WarningElementInfo(time5, code_pos4, cf_stop_msg)],\n        WarningCategory.OTHER:\n            [WarningElementInfo(time1, code_pos1, cf_warn_msg1)]\n    }\n    expected_cf2_warn_warnings = {\n        WarningCategory.WRITE_DELAY:\n            [WarningElementInfo(time4, code_pos3, cf_delay_msg)],\n        WarningCategory.OTHER:\n            [WarningElementInfo(time3, code_pos2, cf_warn_msg3)]\n    }\n    expected_cf2_error_warnings = {\n        WarningCategory.OTHER:\n            [WarningElementInfo(time2, code_pos1, cf_warn_msg2)]}\n\n    expected_warn_warnings = {\n            cf1: expected_cf1_warn_warnings,\n            cf2: expected_cf2_warn_warnings\n    }\n    expected_error_warnings = {cf2: expected_cf2_error_warnings}\n\n    all_expected_warnings = {\n        utils.WarningType.WARN: expected_warn_warnings,\n        utils.WarningType.ERROR: expected_error_warnings\n    }\n\n    actual_cf1_warn_warnings = mngr.get_cf_warn_warnings(cf1)\n    assert actual_cf1_warn_warnings == expected_cf1_warn_warnings\n\n    actual_cf2_warn_warnings = mngr.get_cf_warn_warnings(cf2)\n    assert actual_cf2_warn_warnings == expected_cf2_warn_warnings\n\n    actual_cf2_error_warnings = mngr.get_cf_error_warnings(cf2)\n    assert actual_cf2_error_warnings == expected_cf2_error_warnings\n\n    assert mngr.get_cf_warn_warnings(cf3) is None\n    assert mngr.get_cf_error_warnings(cf3) is None\n\n    actual_warn_warnings = mngr.get_warn_warnings()\n    assert expected_warn_warnings == actual_warn_warnings\n\n    all_actual_warnings = mngr.get_all_warnings()\n    assert all_actual_warnings == all_expected_warnings", ""]}
{"filename": "test/test_db_files.py", "chunked_list": ["import copy\nfrom dataclasses import dataclass\n\nimport db_files\nimport events\nimport utils\nfrom test.testing_utils import create_event\n\njob_id1 = 1\njob_id2 = 2", "job_id1 = 1\njob_id2 = 2\n\ncf1 = \"cf1\"\ncf2 = \"cf2\"\ncf_names = [cf1, cf2]\n\ntime1_minus_10_sec = \"2023/01/24-08:54:30.130553\"\ntime1 = \"2023/01/24-08:54:40.130553\"\ntime1_plus_10_sec = \"2023/01/24-08:54:50.130553\"", "time1 = \"2023/01/24-08:54:40.130553\"\ntime1_plus_10_sec = \"2023/01/24-08:54:50.130553\"\ntime1_plus_11_sec = \"2023/01/24-08:55:50.130553\"\n\nfile_number1 = 1234\nfile_number2 = 5678\nfile_number3 = 9999\n\n\n@dataclass\nclass GlobalTestVars:\n    event_time_micros: int = 0\n\n    cmprsd_data_size_bytes: int = 62396458\n    num_data_blocks: int = 1000\n    total_keys_sizes_bytes: int = 2000\n    total_values_sizes_bytes: int = 3333\n    index_size: int = 3000\n    filter_size: int = 4000\n    num_entries: int = 5555\n    filter_policy: str = \"bloomfilter\"\n    num_filter_entries: int = 6666\n    compression_type: str = \"NoCompression\"", "\n@dataclass\nclass GlobalTestVars:\n    event_time_micros: int = 0\n\n    cmprsd_data_size_bytes: int = 62396458\n    num_data_blocks: int = 1000\n    total_keys_sizes_bytes: int = 2000\n    total_values_sizes_bytes: int = 3333\n    index_size: int = 3000\n    filter_size: int = 4000\n    num_entries: int = 5555\n    filter_policy: str = \"bloomfilter\"\n    num_filter_entries: int = 6666\n    compression_type: str = \"NoCompression\"", "\n\ndef get_table_properties(global_vars):\n    assert isinstance(global_vars, GlobalTestVars)\n\n    return {\n        \"data_size\": global_vars.cmprsd_data_size_bytes,\n        \"index_size\": global_vars.index_size,\n        \"index_partitions\": 0,\n        \"top_level_index_size\": 0,\n        \"index_key_is_user_key\": 1,\n        \"index_value_is_delta_encoded\": 1,\n        \"filter_size\": global_vars.filter_size,\n        \"raw_key_size\": global_vars.total_keys_sizes_bytes,\n        \"raw_average_key_size\": 24,\n        \"raw_value_size\": global_vars.total_values_sizes_bytes,\n        \"raw_average_value_size\": 1000,\n        \"num_data_blocks\": global_vars.num_data_blocks,\n        \"num_entries\": global_vars.num_entries,\n        \"num_filter_entries\": global_vars.num_filter_entries,\n        \"num_deletions\": 0,\n        \"num_merge_operands\": 0,\n        \"num_range_deletions\": 0,\n        \"format_version\": 0,\n        \"fixed_key_len\": 0,\n        \"filter_policy\": global_vars.filter_policy,\n        \"column_family_name\": \"default\",\n        \"column_family_id\": 0,\n        \"comparator\": \"leveldb.BytewiseComparator\",\n        \"merge_operator\": \"nullptr\",\n        \"prefix_extractor_name\": \"nullptr\",\n        \"property_collectors\": \"[]\",\n        \"compression\": global_vars.compression_type,\n        \"oldest_key_time\": 1672823099,\n        \"file_creation_time\": 1672823099,\n        \"slow_compression_estimated_data_size\": 0,\n        \"fast_compression_estimated_data_size\": 0,\n        \"db_id\": \"c100448c-dc04-4c74-8ab2-65d72f3aa3a8\",\n        \"db_session_id\": \"4GAWIG5RIF8PQWM3NOQG\",\n        \"orig_file_number\": 37155}", "\n\ndef test_create_delete_file():\n    vars = GlobalTestVars()\n\n    creation_event1 = create_event(job_id1, cf_names, time1,\n                                   events.EventType.TABLE_FILE_CREATION, cf1,\n                                   file_number=file_number1,\n                                   table_properties=get_table_properties(vars))\n\n    monitor = db_files.DbFilesMonitor()\n    assert monitor.get_all_live_files() == {}\n    assert monitor.get_cf_live_files(cf1) == []\n\n    expected_data_size_bytes = \\\n        vars.total_keys_sizes_bytes + vars.total_values_sizes_bytes\n\n    info1 = \\\n        db_files.DbFileInfo(\n            file_number=file_number1,\n            cf_name=cf1,\n            creation_time=time1,\n            deletion_time=None,\n            size_bytes=0,\n            compressed_size_bytes=0,\n            compressed_data_size_bytes=vars.cmprsd_data_size_bytes,\n            data_size_bytes=expected_data_size_bytes,\n            index_size_bytes=vars.index_size,\n            filter_size_bytes=vars.filter_size,\n            filter_policy=vars.filter_policy,\n            num_filter_entries=vars.num_filter_entries,\n            compression_type=vars.compression_type,\n            level=None)\n\n    assert monitor.new_event(creation_event1)\n    assert monitor.get_all_files() == {cf1: [info1]}\n    assert monitor.get_all_cf_files(cf1) == [info1]\n    assert monitor.get_all_cf_files(cf2) == []\n    assert monitor.get_all_live_files() == {cf1: [info1]}\n    assert monitor.get_cf_live_files(cf1) == [info1]\n    assert monitor.get_cf_live_files(cf2) == []\n\n    deletion_event = create_event(job_id1, cf_names, time1_plus_10_sec,\n                                  events.EventType.TABLE_FILE_DELETION, cf1,\n                                  file_number=file_number1)\n    assert monitor.new_event(deletion_event)\n    info1.deletion_time = time1_plus_10_sec\n    assert monitor.get_all_files() == {cf1: [info1]}\n    assert monitor.get_all_cf_files(cf1) == [info1]\n    assert monitor.get_all_cf_files(cf2) == []\n    assert monitor.get_all_live_files() == {}\n    assert monitor.get_cf_live_files(cf1) == []\n    assert monitor.get_cf_live_files(cf2) == []\n\n    creation_event2 = create_event(job_id1, cf_names, time1_plus_10_sec,\n                                   events.EventType.TABLE_FILE_CREATION, cf1,\n                                   file_number=file_number2,\n                                   table_properties=get_table_properties(vars))\n    info2 = copy.deepcopy(info1)\n    info2.file_number = file_number2\n    info2.creation_time = time1_plus_10_sec\n    info2.deletion_time = None\n\n    assert monitor.new_event(creation_event2)\n    assert monitor.get_all_files() == {cf1: [info1, info2]}\n    assert monitor.get_all_cf_files(cf1) == [info1, info2]\n    assert monitor.get_all_cf_files(cf2) == []\n    assert monitor.get_all_live_files() == {cf1: [info2]}\n    assert monitor.get_cf_live_files(cf2) == []\n    assert monitor.get_cf_live_files(cf1) == [info2]\n\n    creation_event3 = create_event(job_id1, cf_names, time1_plus_10_sec,\n                                   events.EventType.TABLE_FILE_CREATION, cf2,\n                                   file_number=file_number3,\n                                   table_properties=get_table_properties(vars))\n    info3 = copy.deepcopy(info1)\n    info3.file_number = file_number3\n    info3.cf_name = cf2\n    info3.creation_time = time1_plus_10_sec\n    info3.deletion_time = None\n    assert monitor.new_event(creation_event3)\n    assert monitor.get_all_files() == {cf1: [info1, info2], cf2: [info3]}\n    assert monitor.get_all_cf_files(cf1) == [info1, info2]\n    assert monitor.get_all_cf_files(cf2) == [info3]\n    assert monitor.get_all_live_files() == {cf1: [info2], cf2: [info3]}\n    assert monitor.get_cf_live_files(cf1) == [info2]\n    assert monitor.get_cf_live_files(cf2) == [info3]", "\n\ncreate_job_id = 100\ndelete_job_id = 200\nelapsed_seconds = 0\ncreated_file_number = 1\n\n\ndef test_live_file_stats():\n    @dataclass\n    class CreateTestVars:\n        job_id: int = None\n        file_number: int = None\n        time: str = None\n        cf: str = None\n        index_size: int = None\n        filter_size: int = None\n\n        def __init__(self, cf, index_size, filter_size):\n            global create_job_id, elapsed_seconds, created_file_number\n\n            create_job_id += 1\n            self.job_id = create_job_id\n\n            created_file_number += 1\n            self.file_number = created_file_number\n\n            self.cf = cf\n            self.index_size = index_size\n            self.filter_size = filter_size\n\n            global elapsed_seconds\n            elapsed_seconds += 1\n            self.time = utils.get_time_relative_to(time1, elapsed_seconds)\n\n    @dataclass\n    class DeleteTestVars:\n        job_id: int = None\n        file_number: int = None\n        time: str = None\n        cf: str = None\n\n        def __init__(self, create_vars):\n            assert isinstance(create_vars, CreateTestVars)\n\n            global delete_job_id, elapsed_seconds\n\n            delete_job_id += 1\n            self.job_id = delete_job_id\n\n            self.file_number = create_vars.file_number\n            self.cf = create_vars.cf\n\n            elapsed_seconds += 1\n            self.time = utils.get_time_relative_to(time1, elapsed_seconds)\n\n    def create_test_file(monitor, vars, global_vars):\n        assert isinstance(vars, CreateTestVars)\n\n        global_vars.index_size = vars.index_size\n        global_vars.filter_size = vars.filter_size\n\n        creation_event = \\\n            create_event(vars.job_id, cf_names, vars.time,\n                         events.EventType.TABLE_FILE_CREATION, vars.cf,\n                         file_number=vars.file_number,\n                         table_properties=get_table_properties(global_vars))\n        assert monitor.new_event(creation_event)\n        return creation_event\n\n    def delete_test_file(monitor, vars):\n        assert isinstance(vars, DeleteTestVars)\n\n        deletion_event = create_event(vars.job_id, cf_names, vars.time,\n                                      events.EventType.TABLE_FILE_DELETION,\n                                      vars.cf,\n                                      file_number=vars.file_number)\n        assert monitor.new_event(deletion_event)\n\n    global_vars = GlobalTestVars()\n\n    cf1_create_1_vars = \\\n        CreateTestVars(cf=cf1, index_size=100, filter_size=200)\n    cf1_create_2_vars = \\\n        CreateTestVars(cf=cf1, index_size=50, filter_size=500)\n    cf1_create_3_vars = \\\n        CreateTestVars(cf=cf1, index_size=170, filter_size=30)\n\n    cf1_delete_1_vars = DeleteTestVars(cf1_create_1_vars)\n    cf1_delete_2_vars = DeleteTestVars(cf1_create_2_vars)\n\n    cf2_create_1_vars = \\\n        CreateTestVars(cf=cf2, index_size=100, filter_size=200)\n    cf2_create_2_vars = \\\n        CreateTestVars(cf=cf2, index_size=400, filter_size=200)\n\n    cf2_delete_1_vars = \\\n        DeleteTestVars(cf2_create_1_vars)\n\n    monitor = db_files.DbFilesMonitor()\n\n    create_test_file(monitor, cf1_create_1_vars, global_vars)\n    # Live - cf1: index=100, filter=200\n\n    delete_test_file(monitor, cf1_delete_1_vars)\n    # Live: - cf1: index=0, filter=0\n\n    cf1_create_2_event =\\\n        create_test_file(monitor, cf1_create_2_vars, global_vars)\n    cf1_create_2_time = cf1_create_2_event.get_log_time()\n    # Live: - cf1: index=50, filter=500\n\n    cf2_create_1_event =\\\n        create_test_file(monitor, cf2_create_1_vars, global_vars)\n    cf2_create_1_time = cf2_create_1_event.get_log_time()\n    # Live: - cf1: index=50, filter=500, cf2: index=100, filter=200\n\n    cf1_create_3_event = \\\n        create_test_file(monitor, cf1_create_3_vars, global_vars)\n    cf1_create_3_time = cf1_create_3_event.get_log_time()\n    # Live: - cf1: index=220, filter=530, cf2: index=100, filter=200\n\n    delete_test_file(monitor, cf2_delete_1_vars)\n    # Live: - cf1: index=220, filter=530, cf2: index=0, filter=0\n\n    delete_test_file(monitor, cf1_delete_2_vars)\n    # Live: - cf1: index=170, filter=30, cf2: index=0, filter=0\n\n    cf2_create_2_event =\\\n        create_test_file(monitor, cf2_create_2_vars, global_vars)\n    cf2_create_2_time = cf2_create_2_event.get_log_time()\n    # Live: - cf1: index=170, filter=30, cf2: index=400, filter=200\n\n    expected_cf1_index_stats = db_files.BlockLiveFileStats()\n    expected_cf1_index_stats.num_created = 3\n    expected_cf1_index_stats.num_live = 1\n    expected_cf1_index_stats.total_created_size_bytes = 320\n    expected_cf1_index_stats.curr_total_live_size_bytes = 170\n    expected_cf1_index_stats.max_size_bytes = 170\n    expected_cf1_index_stats.max_size_time = cf1_create_3_time\n    expected_cf1_index_stats.max_total_live_size_bytes = 220\n    expected_cf1_index_stats.max_total_live_size_time = cf1_create_3_time\n\n    expected_cf1_filter_stats = db_files.BlockLiveFileStats()\n    expected_cf1_filter_stats.num_created = 3\n    expected_cf1_filter_stats.num_live = 1\n    expected_cf1_filter_stats.total_created_size_bytes = 730\n    expected_cf1_filter_stats.curr_total_live_size_bytes = 30\n    expected_cf1_filter_stats.max_size_bytes = 500\n    expected_cf1_filter_stats.max_size_time = cf1_create_2_time\n    expected_cf1_filter_stats.max_total_live_size_bytes = 530\n    expected_cf1_filter_stats.max_total_live_size_time = cf1_create_3_time\n\n    expected_cf2_index_stats = db_files.BlockLiveFileStats()\n    expected_cf2_index_stats.num_created = 2\n    expected_cf2_index_stats.num_live = 1\n    expected_cf2_index_stats.total_created_size_bytes = 500\n    expected_cf2_index_stats.curr_total_live_size_bytes = 400\n    expected_cf2_index_stats.max_size_bytes = 400\n    expected_cf2_index_stats.max_size_time = cf2_create_2_time\n    expected_cf2_index_stats.max_total_live_size_bytes = 400\n    expected_cf2_index_stats.max_total_live_size_time = cf2_create_2_time\n\n    expected_cf2_filter_stats = db_files.BlockLiveFileStats()\n    expected_cf2_filter_stats.num_created = 2\n    expected_cf2_filter_stats.num_live = 1\n    expected_cf2_filter_stats.total_created_size_bytes = 400\n    expected_cf2_filter_stats.curr_total_live_size_bytes = 200\n    expected_cf2_filter_stats.max_size_bytes = 200\n    expected_cf2_filter_stats.max_size_time = cf2_create_1_time\n    expected_cf2_filter_stats.max_total_live_size_bytes = 200\n    expected_cf2_filter_stats.max_total_live_size_time = cf2_create_1_time\n\n    actual_stats = monitor.get_blocks_stats()\n    assert actual_stats[cf1][db_files.BlockType.INDEX] == \\\n           expected_cf1_index_stats\n    assert actual_stats[cf1][db_files.BlockType.FILTER] == \\\n           expected_cf1_filter_stats\n\n    assert actual_stats[cf2][db_files.BlockType.INDEX] == \\\n           expected_cf2_index_stats\n    assert actual_stats[cf2][db_files.BlockType.FILTER] == \\\n           expected_cf2_filter_stats\n\n    expected_index_stats = db_files.BlockLiveFileStats()\n    expected_index_stats.num_created = 5\n    expected_index_stats.num_live = 2\n    expected_index_stats.total_created_size_bytes = 820\n    expected_index_stats.curr_total_live_size_bytes = 570\n    expected_index_stats.max_size_bytes = 400\n    expected_index_stats.max_size_time = cf2_create_2_time\n    # These are incorrect for more than 1 cf\n    expected_index_stats.max_total_live_size_bytes = 0\n    expected_index_stats.max_total_live_size_time = None\n\n    actual_index_stats = \\\n        db_files.get_block_stats_for_cfs_group(\n            [cf1, cf2], monitor, db_files.BlockType.INDEX)\n    assert actual_index_stats == expected_index_stats\n\n    expected_filter_stats = db_files.BlockLiveFileStats()\n    expected_filter_stats.num_created = 5\n    expected_filter_stats.num_live = 2\n    expected_filter_stats.total_created_size_bytes = 1130\n    expected_filter_stats.curr_total_live_size_bytes = 230\n    expected_filter_stats.max_size_bytes = 500\n    expected_filter_stats.max_size_time = cf1_create_2_time\n    # These are incorrect for more than 1 cf\n    # expected_filter_stats.max_total_live_size_bytes = 530\n    # expected_filter_stats.max_total_live_size_time = cf1_create_3_time\n\n    actual_filter_stats = \\\n        db_files.get_block_stats_for_cfs_group(\n            [cf1, cf2], monitor, db_files.BlockType.FILTER)\n    assert actual_filter_stats == expected_filter_stats", "def test_live_file_stats():\n    @dataclass\n    class CreateTestVars:\n        job_id: int = None\n        file_number: int = None\n        time: str = None\n        cf: str = None\n        index_size: int = None\n        filter_size: int = None\n\n        def __init__(self, cf, index_size, filter_size):\n            global create_job_id, elapsed_seconds, created_file_number\n\n            create_job_id += 1\n            self.job_id = create_job_id\n\n            created_file_number += 1\n            self.file_number = created_file_number\n\n            self.cf = cf\n            self.index_size = index_size\n            self.filter_size = filter_size\n\n            global elapsed_seconds\n            elapsed_seconds += 1\n            self.time = utils.get_time_relative_to(time1, elapsed_seconds)\n\n    @dataclass\n    class DeleteTestVars:\n        job_id: int = None\n        file_number: int = None\n        time: str = None\n        cf: str = None\n\n        def __init__(self, create_vars):\n            assert isinstance(create_vars, CreateTestVars)\n\n            global delete_job_id, elapsed_seconds\n\n            delete_job_id += 1\n            self.job_id = delete_job_id\n\n            self.file_number = create_vars.file_number\n            self.cf = create_vars.cf\n\n            elapsed_seconds += 1\n            self.time = utils.get_time_relative_to(time1, elapsed_seconds)\n\n    def create_test_file(monitor, vars, global_vars):\n        assert isinstance(vars, CreateTestVars)\n\n        global_vars.index_size = vars.index_size\n        global_vars.filter_size = vars.filter_size\n\n        creation_event = \\\n            create_event(vars.job_id, cf_names, vars.time,\n                         events.EventType.TABLE_FILE_CREATION, vars.cf,\n                         file_number=vars.file_number,\n                         table_properties=get_table_properties(global_vars))\n        assert monitor.new_event(creation_event)\n        return creation_event\n\n    def delete_test_file(monitor, vars):\n        assert isinstance(vars, DeleteTestVars)\n\n        deletion_event = create_event(vars.job_id, cf_names, vars.time,\n                                      events.EventType.TABLE_FILE_DELETION,\n                                      vars.cf,\n                                      file_number=vars.file_number)\n        assert monitor.new_event(deletion_event)\n\n    global_vars = GlobalTestVars()\n\n    cf1_create_1_vars = \\\n        CreateTestVars(cf=cf1, index_size=100, filter_size=200)\n    cf1_create_2_vars = \\\n        CreateTestVars(cf=cf1, index_size=50, filter_size=500)\n    cf1_create_3_vars = \\\n        CreateTestVars(cf=cf1, index_size=170, filter_size=30)\n\n    cf1_delete_1_vars = DeleteTestVars(cf1_create_1_vars)\n    cf1_delete_2_vars = DeleteTestVars(cf1_create_2_vars)\n\n    cf2_create_1_vars = \\\n        CreateTestVars(cf=cf2, index_size=100, filter_size=200)\n    cf2_create_2_vars = \\\n        CreateTestVars(cf=cf2, index_size=400, filter_size=200)\n\n    cf2_delete_1_vars = \\\n        DeleteTestVars(cf2_create_1_vars)\n\n    monitor = db_files.DbFilesMonitor()\n\n    create_test_file(monitor, cf1_create_1_vars, global_vars)\n    # Live - cf1: index=100, filter=200\n\n    delete_test_file(monitor, cf1_delete_1_vars)\n    # Live: - cf1: index=0, filter=0\n\n    cf1_create_2_event =\\\n        create_test_file(monitor, cf1_create_2_vars, global_vars)\n    cf1_create_2_time = cf1_create_2_event.get_log_time()\n    # Live: - cf1: index=50, filter=500\n\n    cf2_create_1_event =\\\n        create_test_file(monitor, cf2_create_1_vars, global_vars)\n    cf2_create_1_time = cf2_create_1_event.get_log_time()\n    # Live: - cf1: index=50, filter=500, cf2: index=100, filter=200\n\n    cf1_create_3_event = \\\n        create_test_file(monitor, cf1_create_3_vars, global_vars)\n    cf1_create_3_time = cf1_create_3_event.get_log_time()\n    # Live: - cf1: index=220, filter=530, cf2: index=100, filter=200\n\n    delete_test_file(monitor, cf2_delete_1_vars)\n    # Live: - cf1: index=220, filter=530, cf2: index=0, filter=0\n\n    delete_test_file(monitor, cf1_delete_2_vars)\n    # Live: - cf1: index=170, filter=30, cf2: index=0, filter=0\n\n    cf2_create_2_event =\\\n        create_test_file(monitor, cf2_create_2_vars, global_vars)\n    cf2_create_2_time = cf2_create_2_event.get_log_time()\n    # Live: - cf1: index=170, filter=30, cf2: index=400, filter=200\n\n    expected_cf1_index_stats = db_files.BlockLiveFileStats()\n    expected_cf1_index_stats.num_created = 3\n    expected_cf1_index_stats.num_live = 1\n    expected_cf1_index_stats.total_created_size_bytes = 320\n    expected_cf1_index_stats.curr_total_live_size_bytes = 170\n    expected_cf1_index_stats.max_size_bytes = 170\n    expected_cf1_index_stats.max_size_time = cf1_create_3_time\n    expected_cf1_index_stats.max_total_live_size_bytes = 220\n    expected_cf1_index_stats.max_total_live_size_time = cf1_create_3_time\n\n    expected_cf1_filter_stats = db_files.BlockLiveFileStats()\n    expected_cf1_filter_stats.num_created = 3\n    expected_cf1_filter_stats.num_live = 1\n    expected_cf1_filter_stats.total_created_size_bytes = 730\n    expected_cf1_filter_stats.curr_total_live_size_bytes = 30\n    expected_cf1_filter_stats.max_size_bytes = 500\n    expected_cf1_filter_stats.max_size_time = cf1_create_2_time\n    expected_cf1_filter_stats.max_total_live_size_bytes = 530\n    expected_cf1_filter_stats.max_total_live_size_time = cf1_create_3_time\n\n    expected_cf2_index_stats = db_files.BlockLiveFileStats()\n    expected_cf2_index_stats.num_created = 2\n    expected_cf2_index_stats.num_live = 1\n    expected_cf2_index_stats.total_created_size_bytes = 500\n    expected_cf2_index_stats.curr_total_live_size_bytes = 400\n    expected_cf2_index_stats.max_size_bytes = 400\n    expected_cf2_index_stats.max_size_time = cf2_create_2_time\n    expected_cf2_index_stats.max_total_live_size_bytes = 400\n    expected_cf2_index_stats.max_total_live_size_time = cf2_create_2_time\n\n    expected_cf2_filter_stats = db_files.BlockLiveFileStats()\n    expected_cf2_filter_stats.num_created = 2\n    expected_cf2_filter_stats.num_live = 1\n    expected_cf2_filter_stats.total_created_size_bytes = 400\n    expected_cf2_filter_stats.curr_total_live_size_bytes = 200\n    expected_cf2_filter_stats.max_size_bytes = 200\n    expected_cf2_filter_stats.max_size_time = cf2_create_1_time\n    expected_cf2_filter_stats.max_total_live_size_bytes = 200\n    expected_cf2_filter_stats.max_total_live_size_time = cf2_create_1_time\n\n    actual_stats = monitor.get_blocks_stats()\n    assert actual_stats[cf1][db_files.BlockType.INDEX] == \\\n           expected_cf1_index_stats\n    assert actual_stats[cf1][db_files.BlockType.FILTER] == \\\n           expected_cf1_filter_stats\n\n    assert actual_stats[cf2][db_files.BlockType.INDEX] == \\\n           expected_cf2_index_stats\n    assert actual_stats[cf2][db_files.BlockType.FILTER] == \\\n           expected_cf2_filter_stats\n\n    expected_index_stats = db_files.BlockLiveFileStats()\n    expected_index_stats.num_created = 5\n    expected_index_stats.num_live = 2\n    expected_index_stats.total_created_size_bytes = 820\n    expected_index_stats.curr_total_live_size_bytes = 570\n    expected_index_stats.max_size_bytes = 400\n    expected_index_stats.max_size_time = cf2_create_2_time\n    # These are incorrect for more than 1 cf\n    expected_index_stats.max_total_live_size_bytes = 0\n    expected_index_stats.max_total_live_size_time = None\n\n    actual_index_stats = \\\n        db_files.get_block_stats_for_cfs_group(\n            [cf1, cf2], monitor, db_files.BlockType.INDEX)\n    assert actual_index_stats == expected_index_stats\n\n    expected_filter_stats = db_files.BlockLiveFileStats()\n    expected_filter_stats.num_created = 5\n    expected_filter_stats.num_live = 2\n    expected_filter_stats.total_created_size_bytes = 1130\n    expected_filter_stats.curr_total_live_size_bytes = 230\n    expected_filter_stats.max_size_bytes = 500\n    expected_filter_stats.max_size_time = cf1_create_2_time\n    # These are incorrect for more than 1 cf\n    # expected_filter_stats.max_total_live_size_bytes = 530\n    # expected_filter_stats.max_total_live_size_time = cf1_create_3_time\n\n    actual_filter_stats = \\\n        db_files.get_block_stats_for_cfs_group(\n            [cf1, cf2], monitor, db_files.BlockType.FILTER)\n    assert actual_filter_stats == expected_filter_stats", ""]}
{"filename": "test/test_log_file.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport itertools\n\nimport pytest\n", "import pytest\n\nimport test.testing_utils as test_utils\nimport utils\nfrom db_options import DatabaseOptions\nfrom log_file import LogFileMetadata, ParsedLog\nfrom test.sample_log_info import SampleLogInfo, SampleRolledLogInfo\n\n\ndef test_empty_md():\n    md = LogFileMetadata([], 0)\n    assert md.get_product_name() is None\n    assert md.get_version() is None\n    assert md.get_git_hash() is None\n    assert md.get_db_session_id() is None\n    assert md.get_start_time() is None\n    assert md.get_end_time() is None\n\n    with pytest.raises(utils.ParsingAssertion):\n        md.get_log_time_span_seconds()", "\ndef test_empty_md():\n    md = LogFileMetadata([], 0)\n    assert md.get_product_name() is None\n    assert md.get_version() is None\n    assert md.get_git_hash() is None\n    assert md.get_db_session_id() is None\n    assert md.get_start_time() is None\n    assert md.get_end_time() is None\n\n    with pytest.raises(utils.ParsingAssertion):\n        md.get_log_time_span_seconds()", "\n\ndef test_md_parse_product_and_version():\n    lines =\\\n        [\"2022/02/17-12:38:38.054710 7f574ef65f80 SpeeDB version: 6.11.4\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 RocksDB version: 6.11.5\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 SpeeDB version:\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 XXXXXX version: 6.11.6\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 version:6.11.4\",\n         ]\n\n    expected_values = [\n        (\"SpeeDB\", \"6.11.4\"),\n        (\"RocksDB\", \"6.11.5\"),\n        (None, None),\n        (\"XXXXXX\", \"6.11.6\"),\n        (None, None),\n    ]\n\n    for i, line in enumerate(lines):\n        entry = test_utils.line_to_entry(line)\n        md = LogFileMetadata([entry], 0)\n        assert md.get_product_name() == expected_values[i][0]\n        assert md.get_version() == expected_values[i][1]\n\n    with pytest.raises(utils.ParsingError):\n        lines_with_duplicates = lines[0:2]\n        entries_with_duplicates =\\\n            test_utils.lines_to_entries(lines_with_duplicates)\n        md = LogFileMetadata(entries_with_duplicates, 0)", "\n\ndef test_md_parse_git_hash():\n    lines =\\\n        [\"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha 123456\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha    123456\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git SHA 123456\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha XXXXX:123456\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha \",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 Git 123456\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 sha 123456\",\n         ]\n\n    expected_values = [\n        \"123456\",\n        \"123456\",\n        None,\n        \"XXXXX:123456\",\n        None,\n        None,\n        None,\n    ]\n\n    for i, line in enumerate(lines):\n        entry = test_utils.line_to_entry(line)\n        md = LogFileMetadata([entry], 0)\n        assert md.get_git_hash() == expected_values[i]\n\n    # Test duplicates\n    with pytest.raises(utils.ParsingError):\n        lines_with_duplicates = lines[0:2]\n        entries_with_duplicates =\\\n            test_utils.lines_to_entries(lines_with_duplicates)\n        md = LogFileMetadata(entries_with_duplicates, 0)", "\n\ndef test_md_parse_db_session_id():\n    lines =\\\n        [\"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID:  21GZXO5TD\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID: 21GZXO5TD\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID:21GZXO5TD\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID:\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB ID: 21GZXO5TD\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 Session ID: 21GZXO5TD\",\n         \"2022/02/17-12:38:38.054710 7f574ef65f80 DB Session ID:\",\n         ]\n\n    expected_values = [\n        \"21GZXO5TD\",\n        \"21GZXO5TD\",\n        \"21GZXO5TD\",\n        None,\n        None,\n        None,\n        None,\n    ]\n\n    for i, line in enumerate(lines):\n        entry = test_utils.line_to_entry(line)\n        md = LogFileMetadata([entry], 0)\n        assert md.get_db_session_id() == expected_values[i]\n\n    # Test duplicates\n    with pytest.raises(utils.ParsingError):\n        lines_with_duplicates = lines[0:2]\n        entries_with_duplicates =\\\n            test_utils.lines_to_entries(lines_with_duplicates)\n        md = LogFileMetadata(entries_with_duplicates, 0)", "\n\ndef test_parse_md_valid_combinations():\n    lines = [\n        \"2022/02/17-12:38:38.054710 f65f80 SpeeDB version: 6.11.4\",\n        \"2022/02/17-12:38:38.054710 f65f80 DB Session ID: 21GZXO5TD9VAIRA\",\n        \"2022/02/17-12:38:38.054710 7f574ef65f80 Git sha 123456\",\n    ]\n\n    # Test all combinations of line orderings. Parsing should not be affected\n    for i, lines_permutation in enumerate(list(itertools.permutations(lines))):\n        entries = test_utils.lines_to_entries(lines_permutation)\n        md = LogFileMetadata(entries, 0)\n        assert md.get_product_name() == \"SpeeDB\"\n        assert md.get_version() == \"6.11.4\"\n        assert md.get_git_hash() == \"123456\"\n        assert md.get_db_session_id() == \"21GZXO5TD9VAIRA\"", "\n\ndef test_parse_log_to_entries():\n    lines = '''2022/04/17-14:13:10.724683 7f4a9fdff700 Entry 1\n    2022/04/17-14:14:10.724683 7f4a9fdff700 Entry 2\n    Entry 2 Continuation 1\n\n    \n    2022/04/17-14:14:20.724683 7f4a9fdff700 Entry 3\n    \n    Entry 3 Continuation 1\n    '''.splitlines() # noqa\n\n    entries, job_id_to_cf_name_map =\\\n        ParsedLog.parse_log_to_entries(\"DummyPath\", lines[:1])\n    assert len(entries) == 1\n    assert job_id_to_cf_name_map == {}\n    assert entries[0].get_msg() == \"Entry 1\"\n\n    entries, job_id_to_cf_name_map = \\\n        ParsedLog.parse_log_to_entries(\"DummyPath\", lines[:2])\n    assert len(entries) == 2\n    assert entries[0].get_msg() == \"Entry 1\"\n    assert entries[1].get_msg() == \"Entry 2\"\n\n    entries, job_id_to_cf_name_map = \\\n        ParsedLog.parse_log_to_entries(\"DummyPath\", lines)\n    assert len(entries) == 3\n\n    assert entries[1].get_msg_lines() == [\"Entry 2\",\n                                          \"Entry 2 Continuation 1\",\n                                          \"\",\n                                          \"\"]\n    assert entries[1].get_msg() == \"Entry 2\\nEntry 2 Continuation 1\"\n\n    assert entries[2].get_msg_lines() == [\"Entry 3\",\n                                          \"\",\n                                          \"Entry 3 Continuation 1\",\n                                          \"\"]\n    assert entries[2].get_msg() == \"Entry 3\\n\\nEntry 3 Continuation 1\"", "\n\ndef test_parse_metadata():\n    lines = '''2022/11/24-15:58:04.758352 32819 RocksDB version: 7.2.2\n    2022/11/24-15:58:04.758397 32819 Git sha 35f6432643807267f210eda9e9388a80ea2ecf0e\n    2022/11/24-15:58:04.758398 32819 Compile date\n    2022/11/24-15:58:04.758402 32819 DB SUMMARY\n    2022/11/24-15:58:04.758403 32819 DB Session ID:  V90YQ8JY6T5E5H2ES6LK\n    2022/11/24-15:58:04.759056 32819 CURRENT file:  CURRENT\n    2022/11/24-15:58:04.759058 32819 IDENTITY file:  IDENTITY\n    2022/11/24-15:58:04.759060 32819 MANIFEST file:  MANIFEST-025591 size: 439474 Bytes\n    2022/11/24-15:58:04.759061 32819 SST files in /data/ dir, Total Num: 1498,\"\n    2022/11/24-15:58:04.759062 32819 Write Ahead Log file in /data/: 028673.log\n    '''.splitlines() # noqa\n\n    entries = test_utils.lines_to_entries(lines)\n\n    metadata = LogFileMetadata(entries, start_entry_idx=0)\n\n    assert metadata.get_product_name() == \"RocksDB\"\n    assert metadata.get_version() == \"7.2.2\"\n    assert metadata.get_git_hash() == \\\n           \"35f6432643807267f210eda9e9388a80ea2ecf0e\"\n\n    assert metadata.get_start_time() == \"2022/11/24-15:58:04.758352\"\n\n    with pytest.raises(utils.ParsingAssertion):\n        metadata.get_log_time_span_seconds()\n\n    assert str(metadata) == \\\n           \"LogFileMetadata: Start:2022/11/24-15:58:04.758352, End:UNKNOWN\"\n\n    metadata.set_end_time(\"2022/11/24-16:08:04.758352\")\n    assert metadata.get_end_time() == \"2022/11/24-16:08:04.758352\"\n    assert metadata.get_log_time_span_seconds() == 600\n\n    assert str(metadata) == \\\n           \"LogFileMetadata: Start:2022/11/24-15:58:04.758352, \" \\\n           \"End:2022/11/24-16:08:04.758352\"", "\n\ndef test_parse_metadata1():\n    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n    metadata = parsed_log.get_metadata()\n\n    assert metadata.get_product_name() == SampleLogInfo.PRODUCT_NAME\n    assert metadata.get_version() == SampleLogInfo.VERSION\n    assert metadata.get_git_hash() == SampleLogInfo.GIT_HASH\n\n    assert metadata.get_start_time() == SampleLogInfo.START_TIME\n    assert metadata.get_end_time() == SampleLogInfo.END_TIME\n\n    expected_time_span = \\\n        (utils.parse_time_str(SampleLogInfo.END_TIME) -\n         utils.parse_time_str(SampleLogInfo.START_TIME)).seconds\n    assert metadata.get_log_time_span_seconds() == expected_time_span", "\n\ndef test_parse_options():\n    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n    assert parsed_log.get_cfs_names(include_auto_generated=False) == \\\n           SampleLogInfo.CF_NAMES\n\n    actual_db_options = parsed_log.get_database_options()\n    assert actual_db_options.are_db_wide_options_set()\n    assert actual_db_options.get_cfs_names() == \\\n           parsed_log.get_cfs_names(include_auto_generated=False)\n\n    # Assuming LogFileOptionsParser is fully tested and may be used\n    expected_db_options = DatabaseOptions()\n    expected_db_options.set_db_wide_options(SampleLogInfo.DB_WIDE_OPTIONS_DICT)\n    for i in range(len(SampleLogInfo.CF_NAMES)):\n        expected_db_options.set_cf_options(\n            SampleLogInfo.CF_NAMES[i],\n            SampleLogInfo.OPTIONS_DICTS[i],\n            SampleLogInfo.TABLE_OPTIONS_DICTS[i])\n\n    actual_db_wide_options = actual_db_options.get_db_wide_options()\n    expected_db_wide_options = expected_db_options.get_db_wide_options()\n    assert expected_db_wide_options == actual_db_wide_options\n\n    for i in range(len(SampleLogInfo.CF_NAMES)):\n        cf_name = SampleLogInfo.CF_NAMES[i]\n        actual_options = actual_db_options.get_cf_options(cf_name)\n        expected_options = expected_db_options.get_cf_options(cf_name)\n        assert expected_options == actual_options", "\n\ndef test_parse_options_in_rolled_log():\n    parsed_log = test_utils.create_parsed_log(SampleRolledLogInfo.FILE_PATH)\n    assert parsed_log.get_cfs_names(include_auto_generated=False) == \\\n           SampleRolledLogInfo.CF_NAMES\n    assert parsed_log.get_auto_generated_cfs_names() == \\\n           SampleRolledLogInfo.AUTO_GENERATED_CF_NAMES\n\n    actual_db_options = parsed_log.get_database_options()\n    assert actual_db_options.are_db_wide_options_set()\n\n    expected_cfs_names_with_options = [\"default\"] + \\\n        SampleRolledLogInfo.AUTO_GENERATED_CF_NAMES\n    assert actual_db_options.get_cfs_names() == \\\n           expected_cfs_names_with_options\n\n    # Assuming LogFileOptionsParser is fully tested and may be used\n    expected_db_options = DatabaseOptions()\n    expected_db_options.set_db_wide_options(SampleLogInfo.DB_WIDE_OPTIONS_DICT)\n    for i in range(len(expected_cfs_names_with_options)):\n        expected_db_options.set_cf_options(\n            expected_cfs_names_with_options[i],\n            SampleLogInfo.OPTIONS_DICTS[i],\n            SampleLogInfo.TABLE_OPTIONS_DICTS[i])\n\n    actual_db_wide_options = actual_db_options.get_db_wide_options()\n    expected_db_wide_options = expected_db_options.get_db_wide_options()\n    assert expected_db_wide_options == actual_db_wide_options\n\n    for i in range(len(expected_cfs_names_with_options)):\n        cf_name = expected_cfs_names_with_options[i]\n        actual_options = actual_db_options.get_cf_options(cf_name)\n        expected_options = expected_db_options.get_cf_options(cf_name)\n        assert expected_options == actual_options", "\n\ndef test_parse_warns():\n    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\n    warns_mngr = parsed_log.get_warnings_mngr()\n    assert warns_mngr.get_total_num_warns() == 1\n\n\ndef test_parse_db_wide_stats():\n    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\n    mngr = parsed_log.get_stats_mngr()\n    db_wide_stats_mngr = mngr.get_db_wide_stats_mngr()\n    assert db_wide_stats_mngr.get_stalls_entries() == \\\n           SampleLogInfo.DB_WIDE_STALLS_ENTRIES", "\ndef test_parse_db_wide_stats():\n    parsed_log = test_utils.create_parsed_log(SampleLogInfo.FILE_PATH)\n\n    mngr = parsed_log.get_stats_mngr()\n    db_wide_stats_mngr = mngr.get_db_wide_stats_mngr()\n    assert db_wide_stats_mngr.get_stalls_entries() == \\\n           SampleLogInfo.DB_WIDE_STALLS_ENTRIES\n\n\ndef test_empty_log_file():\n    with pytest.raises(utils.EmptyLogFile):\n        ParsedLog(\"DummyPath\", [], should_init_baseline_info=False)", "\n\ndef test_empty_log_file():\n    with pytest.raises(utils.EmptyLogFile):\n        ParsedLog(\"DummyPath\", [], should_init_baseline_info=False)\n\n\ndef test_unexpected_1st_log_line():\n    with pytest.raises(utils.InvalidLogFile):\n        ParsedLog(\"DummyPath\", [\"Dummy Line\", \"Another Dummy Line\"],\n                  should_init_baseline_info=False)", ""]}
{"filename": "test/test_stats_mngr.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nfrom datetime import timedelta\n\nimport utils\nfrom stats_mngr import DbWideStatsMngr, CompactionStatsMngr, BlobStatsMngr, \\", "import utils\nfrom stats_mngr import DbWideStatsMngr, CompactionStatsMngr, BlobStatsMngr, \\\n    CfFileHistogramStatsMngr, BlockCacheStatsMngr, \\\n    StatsMngr, parse_uptime_line, CfNoFileStatsMngr\nfrom test.testing_utils import lines_to_entries\n\nNUM_LINES = 381\nSTATS_DUMP_IDX = 0\nDB_STATS_IDX = 2\nCOMPACTION_STATS_DEFAULT_PER_LEVEL_IDX = 11", "DB_STATS_IDX = 2\nCOMPACTION_STATS_DEFAULT_PER_LEVEL_IDX = 11\nCOMPACTION_STATS_DEFAULT_BY_PRIORITY_IDX = 22\nBLOB_STATS_IDX = 27\nCF_NO_FILE_HISTOGRAM_DEFAULT_IDX = 29\nBLOCK_CACHE_DEFAULT_IDX = 38\nCF_FILE_READ_LATENCY_IDX = 41\nSTATISTICS_COUNTERS_IDX = 138\nONE_PAST_STATS_IDX = 377\n", "ONE_PAST_STATS_IDX = 377\n\nEMPTY_LINE1 = ''\nEMPTY_LINE2 = '                      '\n\nget_value_by_size_with_unit = \\\n    utils.get_num_bytes_from_human_readable_str\n\n\n# TODO  - Test that errors during parsing of a stats entry are handled", "\n# TODO  - Test that errors during parsing of a stats entry are handled\n#  gracefully (parsing continues, partial/corrupt data is not saved, and only\n#  the faulty lines are skipped)\n\n'''\nTODO:\nGeneral:\n- Test parse_line_with_cf\n", "- Test parse_line_with_cf\n\nDB-Wide:\nAdd tests for stalls lines or remove the code\n'''\n\n\ndef read_sample_stats_file():\n    f = open(\"input_files/LOG_sample_stats.txt\")\n    lines = f.readlines()\n    assert len(lines) == NUM_LINES\n    return lines", "\n\ndef test_parse_uptime_line():\n    line1 = \"Uptime(secs): 4.8 total, 8.4 interval\"\n    assert (4.8, 8.4) == parse_uptime_line(line1)\n\n    line2 = \"Uptime(secs): 4.8 total, XXX 8.4 interval\"\n    assert parse_uptime_line(line2, allow_mismatch=True) is None\n\n", "\n\n#\n#   DB Wide Stats Mngr\n#\n\ndef test_db_wide_is_stats_start_line():\n    assert DbWideStatsMngr.is_start_line(\"** DB Stats **\")\n    assert DbWideStatsMngr.is_start_line(\"** DB Stats **      \")\n    assert not DbWideStatsMngr.is_start_line(\"** DB Stats **   DUMMY TEXT\")\n    assert not DbWideStatsMngr.is_start_line(\"** DB XXX Stats **\")", "\n\ndef test_db_wide_try_parse_as_stalls_line():\n    pass\n\n\ndef test_db_wide_stats_mngr():\n    time1 = \"2022/11/24-15:58:09.511260\"\n    db_wide_stats_lines1 = \\\n        '''Uptime(secs): 4.8 total, 4.8 interval\n        Cumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\n        Cumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n        Cumulative stall: 12:10:56.123 H:M:S, 98.7 percent\n        Interval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\n        Interval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\n        Interval stall: 45:34:12.789 H:M:S, 12.3 percent\n        '''.splitlines()  # noqa\n\n    time2 = \"2022/11/24-15:59:09.511260\"\n    db_wide_stats_lines2 = \\\n        '''Uptime(secs): 4.8 total, 4.8 interval\n        Cumulative writes: 10M writes, 2M keys, 0 commit groups, 0.0 writes per commit group, ingest: 1.23 GB, 5.67 MB/s\n        '''.splitlines()  # noqa\n\n    mngr = DbWideStatsMngr()\n    assert mngr.get_stalls_entries() == {}\n\n    mngr.add_lines(time1, db_wide_stats_lines1)\n    expected_cumulative_duration = \\\n        timedelta(hours=12, minutes=10, seconds=56, milliseconds=123)\n    expected_interval_duration = \\\n        timedelta(hours=45, minutes=34, seconds=12, milliseconds=789)\n    expected_stalls_entries = \\\n        {time1: {\"cumulative_duration\": expected_cumulative_duration,\n                 \"cumulative_percent\": 98.7,\n                 \"interval_duration\": expected_interval_duration,\n                 \"interval_percent\": 12.3}}\n    expected_cumulative_writes_entries = {\n        time1: DbWideStatsMngr.CumulativeWritesInfo()\n    }\n\n    actual_stalls_entries = mngr.get_stalls_entries()\n    assert actual_stalls_entries == expected_stalls_entries\n\n    actual_cumulative_writes_entries = mngr.get_cumulative_writes_entries()\n    assert actual_cumulative_writes_entries == \\\n           expected_cumulative_writes_entries\n\n    mngr.add_lines(time2, db_wide_stats_lines2)\n    actual_stalls_entries = mngr.get_stalls_entries()\n    assert actual_stalls_entries == expected_stalls_entries\n\n    expected_cumulative_writes_entries.update({\n        time2: DbWideStatsMngr.CumulativeWritesInfo(\n            num_writes=utils.get_number_from_human_readable_str(\"10 M\"),\n            num_keys=utils.get_number_from_human_readable_str(\"2 M\"),\n            ingest=utils.get_num_bytes_from_human_readable_str(\"1.23 GB\"),\n            ingest_rate_mbps=5.67)\n    })\n\n    actual_cumulative_writes_entries = mngr.get_cumulative_writes_entries()\n    assert actual_cumulative_writes_entries == \\\n           expected_cumulative_writes_entries", "\n\ndef test_is_compaction_stats_start_line():\n    line1 = \"** Compaction Stats [default] **\"\n    assert CompactionStatsMngr.is_start_line(line1)\n    assert CompactionStatsMngr.parse_start_line(line1) == \"default\"\n\n    line2 = \"** Compaction Stats [col-family] **       \"\n    assert CompactionStatsMngr.is_start_line(line2)\n    assert CompactionStatsMngr.parse_start_line(line2) == \"col-family\"\n\n    line3 = \"       ** Compaction Stats [col-family] **\"\n    assert CompactionStatsMngr.is_start_line(line3)\n    assert CompactionStatsMngr.parse_start_line(line3) == \"col-family\"\n\n    line4 = \"** Compaction Stats    [col-family]     **     \"\n    assert CompactionStatsMngr.is_start_line(line4)\n    assert CompactionStatsMngr.parse_start_line(line4) == \"col-family\"\n\n    line5 = \"** Compaction XXX Stats  [col-family] **\"\n    assert not CompactionStatsMngr.is_start_line(line5)", "\n\ndef test_is_blob_stats_start_line():\n    line1 = \\\n        'Blob file count: 0, total size: 0.0 GB, garbage size: 0.0 GB,' \\\n        ' space amp: 0.0'\n\n    assert BlobStatsMngr.is_start_line(line1)\n    assert (0, 0.0, 0.0, 0.0) == BlobStatsMngr.parse_blob_stats_line(line1)\n\n    line2 = \\\n        'Blob file count: 100, total size: 1.5 GB, garbage size: 3.5 GB,' \\\n        ' space amp: 0.2'\n    assert BlobStatsMngr.is_start_line(line2)\n    assert (100, 1.5, 3.5, 0.2) == BlobStatsMngr.parse_blob_stats_line(line2)", "\n\ndef test_is_cf_file_histogram_stats_start_line():\n    cf1 = \"default\"\n    cf2 = \"col_family\"\n\n    line1 = f\"** File Read Latency Histogram By Level [{cf1}] **\"\n    assert CfFileHistogramStatsMngr.is_start_line(line1)\n    assert CfFileHistogramStatsMngr.parse_start_line(line1) == cf1\n\n    line2 = f\"** File Read Latency Histogram By Level [{cf2}] **       \"\n    assert CfFileHistogramStatsMngr.is_start_line(line2)\n    assert CfFileHistogramStatsMngr.parse_start_line(line2) == cf2\n\n    line3 = f\"       ** File Read Latency Histogram By Level [{cf2}] **\"\n    assert CfFileHistogramStatsMngr.is_start_line(line3)\n    assert CfFileHistogramStatsMngr.parse_start_line(line3) == cf2\n\n    line4 = \\\n        f\"** File Read Latency Histogram By Level    [{cf2}]     **     \"\n    assert CfFileHistogramStatsMngr.is_start_line(line4)\n    assert CfFileHistogramStatsMngr.parse_start_line(line4) == cf2\n\n    line5 = \\\n        f\"** File Read Latency Histogram XXX By Level Stats  [{cf2}] **\"\n    assert not CfFileHistogramStatsMngr.is_start_line(line5)", "\n\ndef test_is_block_cache_stats_start_line():\n    line1 = 'Block cache LRUCache@0x5600bb634770#32819 capacity: 8.00 MB ' \\\n            'collections: 1 last_copies: 0 last_secs: 4.9e-05 secs_since: 0'\n\n    line2 = \\\n        'Block cache entry stats(count,size,portion): ' \\\n        'Misc(3,8.12 KB, 0.0991821%)'\n\n    assert BlockCacheStatsMngr.is_start_line(line1)\n    assert not BlockCacheStatsMngr.is_start_line(line2)", "\n\ndef test_find_next_start_line_in_db_stats():\n    lines = read_sample_stats_file()\n    assert DbWideStatsMngr.is_start_line(lines[DB_STATS_IDX])\n\n    expected_next_line_idxs = [COMPACTION_STATS_DEFAULT_PER_LEVEL_IDX,\n                               COMPACTION_STATS_DEFAULT_BY_PRIORITY_IDX,\n                               BLOB_STATS_IDX,\n                               CF_NO_FILE_HISTOGRAM_DEFAULT_IDX,\n                               BLOCK_CACHE_DEFAULT_IDX,\n                               CF_FILE_READ_LATENCY_IDX]\n    expected_next_types = [StatsMngr.StatsType.COMPACTION,\n                           StatsMngr.StatsType.COMPACTION,\n                           StatsMngr.StatsType.BLOB,\n                           StatsMngr.StatsType.CF_NO_FILE,\n                           StatsMngr.StatsType.BLOCK_CACHE,\n                           StatsMngr.StatsType.CF_FILE_HISTOGRAM]\n\n    expected_next_cf_names = [\"default\", \"default\", None, None,\n                              None, \"CF1\"]\n\n    line_idx = DB_STATS_IDX\n    stats_type = StatsMngr.StatsType.DB_WIDE\n    for i, expected_next_line_idx in enumerate(expected_next_line_idxs):\n        next_line_idx, next_stats_type, next_cf_name = \\\n            StatsMngr.find_next_start_line_in_db_stats(lines,\n                                                       line_idx,\n                                                       stats_type)\n        assert (next_line_idx > line_idx) or (next_stats_type is None)\n        assert next_line_idx == expected_next_line_idx\n        assert next_stats_type == expected_next_types[i]\n        assert next_cf_name == expected_next_cf_names[i]\n\n        line_idx = next_line_idx\n        stats_type = next_stats_type", "\n\ndef test_blob_stats_mngr():\n    blob_line = \\\n        'Blob file count: 10, total size: 1.5 GB, garbage size: 2.0 GB, ' \\\n        'space amp: 4.0'\n    blob_lines = [blob_line, EMPTY_LINE2]\n    time = '2022/11/24-15:58:09.511260 32851'\n    cf = \"cf1\"\n    mngr = BlobStatsMngr()\n    mngr.add_lines(time, cf, blob_lines)\n\n    expected_blob_entries = \\\n        {time: {\"File Count\": 10,\n                \"Total Size\": float(1.5 * 2 ** 30),\n                \"Garbage Size\": float(2 * 2 ** 30),\n                \"Space Amp\": 4.0}}\n\n    assert mngr.get_cf_stats(cf) == expected_blob_entries", "\n\ndef test_block_cache_stats_mngr_no_cf():\n    lines = \\\n        '''Block cache LRUCache@0x5600bb634770#32819 capacity: 8.00 GB collections: 1 last_copies: 0 last_secs: 4.9e-05 secs_since: 0\n        Block cache entry stats(count,size,portion): DataBlock(1548,6.97 MB,0.136142%) IndexBlock(843,3.91 GB,78.2314%) Misc(6,16.37 KB,1.86265e-08%)\n        '''.splitlines()  # noqa\n\n    time = '2022/11/24-15:58:09.511260'\n    cf_name = \"cf1\"\n    mngr = BlockCacheStatsMngr()\n    cache_id = mngr.add_lines(time, cf_name, lines)\n    assert cache_id == \"LRUCache@0x5600bb634770#32819\"\n\n    entries = mngr.get_cache_entries(cache_id)\n    assert entries['Capacity'] == get_value_by_size_with_unit('8.00 GB')\n\n    expected_data_block_stats = \\\n        {'Count': 1548,\n         'Size': get_value_by_size_with_unit('6.97 MB'),\n         'Portion': '0.14%'\n         }\n    expected_index_block_stats = \\\n        {'Count': 843,\n         'Size': get_value_by_size_with_unit('3.91 GB'),\n         'Portion': '78.23%'\n         }\n\n    assert time in entries\n    time_entries = entries[time]\n    assert 'DataBlock' in time_entries\n    assert time_entries['DataBlock'] == expected_data_block_stats\n\n    assert 'IndexBlock' in time_entries\n    assert time_entries['IndexBlock'] == expected_index_block_stats\n\n    total_expected_usage = \\\n        get_value_by_size_with_unit('6.97 MB') + \\\n        get_value_by_size_with_unit('3.91 GB') + \\\n        get_value_by_size_with_unit('16.37 KB')\n    assert time_entries[\"Usage\"] == total_expected_usage\n    assert entries[\"Usage\"] == total_expected_usage\n\n    assert mngr.get_cf_cache_entries(cache_id, cf_name) == {}\n    assert mngr.get_last_usage(cache_id) == total_expected_usage", "\n\ndef test_block_cache_stats_mngr_with_cf():\n    lines = \\\n        '''Block cache LRUCache@0x5600bb634770#32819 capacity: 8.00 GB collections: 1 last_copies: 0 last_secs: 4.9e-05 secs_since: 0\n        Block cache entry stats(count,size,portion): DataBlock(1548,6.97 MB,0.136142%) IndexBlock(843,3.91 GB,78.2314%) Misc(6,16.37 KB,1.86265e-08%)\n        Block cache [CF1]  DataBlock(4.50 KB) FilterBlock(0.00 KB) IndexBlock(0.91 GB)\n        '''.splitlines()  # noqa\n\n    time = '2022/11/24-15:58:09.511260'\n    cf_name = \"CF1\"\n    mngr = BlockCacheStatsMngr()\n    cache_id = mngr.add_lines(time, cf_name, lines)\n    assert cache_id == \"LRUCache@0x5600bb634770#32819\"\n\n    cf_entries = mngr.get_cf_cache_entries(cache_id, cf_name)\n    expected_cf_entries = \\\n        {time: {'DataBlock': get_value_by_size_with_unit('4.50 KB'),\n                'IndexBlock': get_value_by_size_with_unit('0.91 GB')}\n         }\n    assert cf_entries == expected_cf_entries\n\n    entries = mngr.get_cache_entries(cache_id)\n    total_expected_usage = \\\n        get_value_by_size_with_unit('6.97 MB') + \\\n        get_value_by_size_with_unit('3.91 GB') + \\\n        get_value_by_size_with_unit('16.37 KB')\n    assert entries[time][\"Usage\"] == total_expected_usage\n    assert entries[\"Usage\"] == total_expected_usage", "\n\ndef test_stats_mngr():\n    lines = read_sample_stats_file()\n    entries = lines_to_entries(lines)\n\n    mngr = StatsMngr()\n\n    expected_entry_idx = 1\n    expected_cfs_names_found = set()\n    assert mngr.try_adding_entries(entries, start_entry_idx=1) == \\\n           (False, expected_entry_idx, expected_cfs_names_found)\n    expected_entry_idx = 2\n    expected_cfs_names_found = {\"default\", \"CF1\"}\n    assert mngr.try_adding_entries(entries, start_entry_idx=0) == \\\n           (True, expected_entry_idx, expected_cfs_names_found)", "\n\ndef test_stats_mngr_non_contig_entries_1():\n    lines = \\\n'''2023/07/18-19:27:01.889729 27127 [/db_impl/db_impl.cc:1084] ------- DUMPING STATS -------\n2023/07/18-19:27:01.889745 26641 [/column_family.cc:1044] [default] Increasing compaction threads because of estimated pending compaction bytes 18555651178\n2023/07/18-19:27:01.890259 27127 [/db_impl/db_impl.cc:1086] \n** DB Stats **\nUptime(secs): 0.7 total, 0.7 interval\nCumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\nCumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\nInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\nInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nInterval stall: 00:00:0.000 H:M:S, 0.0 percent\nWrite Stall (count): write-buffer-manager-limit-stops: 0,\n ** Compaction Stats [default] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0   322.40 MB   1.3      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n  L1      6/1   350.91 MB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n  L2     59/11   3.16 GB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n  L3    487/28  27.78 GB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n  L4    166/0   10.17 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.2   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n Sum    720/40  41.77 GB   0.0      0.0     0.0      0.0       0.1      0.1       0.2   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.1      0.1       0.2   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n'''.splitlines() # noqa\n\n    entries = lines_to_entries(lines)\n\n    mngr = StatsMngr()\n\n    expected_entry_idx = 1\n    expected_cfs_names_found = set()\n    assert mngr.try_adding_entries(entries, start_entry_idx=0) == \\\n           (True, expected_entry_idx, expected_cfs_names_found)\n\n    assert mngr.try_adding_entries(entries, start_entry_idx=1) == \\\n           (False, expected_entry_idx, expected_cfs_names_found)\n\n    expected_entry_idx = 3\n    expected_cfs_names_found = {\"default\"}\n    assert mngr.try_adding_entries(entries, start_entry_idx=2) == \\\n           (True, expected_entry_idx, expected_cfs_names_found)", "\n\ndef test_stats_mngr_non_contig_entries_2():\n    lines = \\\n'''2023/07/18-19:27:01.889729 27127 [/db_impl/db_impl.cc:1084] ------- DUMPING STATS -------\n2023/07/18-19:27:01.889745 26641 [/column_family.cc:1044] [default] Increasing compaction threads because of estimated pending compaction bytes 18555651178\n2023/07/18-19:27:01.889806 26641 (Original Log Time 2023/07/18-19:27:01.887253) [/db_impl/db_impl_compaction_flush.cc:3428] [default] Moving #13947 to level-4 67519682 bytes\n2023/07/18-19:27:01.889746 27127 [/db_impl/db_impl.cc:1084] ------- DUMPING STATS -------\n2023/07/18-19:27:01.890259 27127 [/db_impl/db_impl.cc:1086] \n** DB Stats **\nUptime(secs): 0.7 total, 0.7 interval\nCumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\nCumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\nInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\nInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nInterval stall: 00:00:0.000 H:M:S, 0.0 percent\nWrite Stall (count): write-buffer-manager-limit-stops: 0,\n ** Compaction Stats [default] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0   322.40 MB   1.3      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n  L1      6/1   350.91 MB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n  L2     59/11   3.16 GB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n  L3    487/28  27.78 GB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n  L4    166/0   10.17 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.2   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n Sum    720/40  41.77 GB   0.0      0.0     0.0      0.0       0.1      0.1       0.2   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.1      0.1       0.2   1.0      0.0    594.4      0.12              0.00         1    0.120       0      0       0.0       0.0\n'''.splitlines() # noqa\n\n    entries = lines_to_entries(lines)\n\n    mngr = StatsMngr()\n\n    expected_entry_idx = 1\n    expected_cfs_names_found = set()\n    assert mngr.try_adding_entries(entries, start_entry_idx=0) == \\\n           (True, expected_entry_idx, expected_cfs_names_found)\n\n    expected_entry_idx = 1\n    expected_cfs_names_found = set()\n    assert mngr.try_adding_entries(entries, start_entry_idx=1) == \\\n           (False, expected_entry_idx, expected_cfs_names_found)\n\n    expected_entry_idx = 2\n    expected_cfs_names_found = set()\n    assert mngr.try_adding_entries(entries, start_entry_idx=2) == \\\n           (False, expected_entry_idx, expected_cfs_names_found)\n\n    expected_entry_idx = 5\n    expected_cfs_names_found = {\"default\"}\n    assert mngr.try_adding_entries(entries, start_entry_idx=3) == \\\n           (True, expected_entry_idx, expected_cfs_names_found)", "\n\ndef test_compaction_stats_mngr():\n    lines_level = \\\n        '''** Compaction Stats [default] **\n        Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n        ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n          L0      1/0   149.99 MB   0.6      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    218.9      0.69              0.00         1    0.685       0      0       0.0       0.0\n          L1      5/0   271.79 MB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n          L2     50/1    2.73 GB   1.1      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n          L3    421/6   24.96 GB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n          L4   1022/0   54.33 GB   0.2      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0       0.0       0.0\n         Sum   1499/7   82.43 GB   0.0      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    218.9      0.69              0.00         1    0.685       0      0       0.0       0.0\n         Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.1      0.1       0.0   1.0      0.0    218.9      0.69              0.00         1    0.685       0      0       0.0       0.0\n        '''.splitlines()  # noqa\n\n    lines_priority = \\\n        '''** Compaction Stats [CF1] **\n        Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n        ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n        User      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.1      0.1       0.0   0.0      0.0    218.9      0.69              0.00         1    0.685       0      0       0.0       0.0\n        '''.splitlines()  # noqa\n\n    lines_level = [line.strip() for line in lines_level]\n    lines_priority = [line.strip() for line in lines_priority]\n\n    time = \"2022/11/24-15:58:09.511260\"\n    mngr = CompactionStatsMngr()\n\n    assert mngr.get_cf_size_bytes_at_end(\"default\") is None\n\n    mngr.add_lines(time, \"default\", lines_level, 0)\n    mngr.add_lines(time, \"CF1\", lines_priority, 0)\n\n    assert mngr.get_cf_size_bytes_at_end(\"default\") == \\\n           utils.get_num_bytes_from_human_readable_components(\"82.43\", \"GB\")\n    assert mngr.get_cf_size_bytes_at_end(\"CF1\") is None", "\n\ndef test_cf_no_file_stats_mngr():\n    time = \"2022/11/24-15:58:09.511260\"\n    cf_name = \"cf1\"\n    lines = '''\n    Uptime(secs): 22939219.8 total, 0.0 interval\n    Flush(GB): cumulative 158.813, interval 0.000\n    AddFile(GB): cumulative 0.000, interval 0.000\n    AddFile(Total Files): cumulative 0, interval 0\n    AddFile(L0 Files): cumulative 0, interval 0\n    AddFile(Keys): cumulative 0, interval 0\n    Cumulative compaction: 364.52 GB write, 0.02 MB/s write, 363.16 GB read, 0.02 MB/s read, 1942.7 seconds\n    Interval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\n    Stalls(count): 0 level0_slowdown, 1 level0_slowdown_with_compaction, 2 level0_numfiles, 3 level0_numfiles_with_compaction, 4 stop for pending_compaction_bytes, 5 slowdown for pending_compaction_bytes, 6 memtable_compaction, 7 memtable_slowdown, interval 100 total count\n    '''.splitlines()  # noqa\n\n    expected_stall_counts = \\\n        {cf_name: {time: {'level0_slowdown': 0,\n                          'level0_slowdown_with_compaction': 1,\n                          'level0_numfiles': 2,\n                          'level0_numfiles_with_compaction': 3,\n                          'stop for pending_compaction_bytes': 4,\n                          'slowdown for pending_compaction_bytes': 5,\n                          'memtable_compaction': 6,\n                          'memtable_slowdown': 7,\n                          'interval_total_count': 100}}}\n    mngr = CfNoFileStatsMngr()\n    mngr.add_lines(time, cf_name, lines)\n    stall_counts = mngr.get_stall_counts()\n\n    assert stall_counts == expected_stall_counts", "\n\ndef test_compaction_stats_get_field_value():\n    stats = \\\n        {'2023/04/09-12:37:27.398344':\n         {'LEVEL-0': {'Moved(GB)': '1.2',\n                      'W-Amp': '1.0',\n                      'Comp(sec)': '74.4',\n                      'CompMergeCPU(sec)': '10.1',\n                      'Comp(cnt)': '15.03'},\n          'LEVEL-1': {'Moved(GB)': '1.2', 'W-Amp': '2.0',\n                      'Comp(sec)': '84.4',\n                      'CompMergeCPU(sec)': '20.2',\n                      'Comp(cnt)': '15.03'},\n          'SUM': {'Moved(GB)': '1.2',\n                  'W-Amp': '3.0',\n                  'Comp(sec)': '158.8',\n                  'CompMergeCPU(sec)': '30.3',\n                  'Comp(cnt)': '15.03'},\n          'INTERVAL': {'Moved(GB)': '1.2',\n                       'W-Amp': '1.5',\n                       'Comp(sec)': '74.4',\n                       'CompMergeCPU(sec)': '15.97',\n                       'Comp(cnt)': '15.03'}}}\n\n    field = CompactionStatsMngr.LevelFields.WRITE_AMP\n    assert CompactionStatsMngr.get_level_field_value(stats, 0, field) == '1.0'\n    assert CompactionStatsMngr.get_level_field_value(stats, 1, field) == '2.0'\n    assert CompactionStatsMngr.get_sum_value(stats, field) == '3.0'\n\n    assert CompactionStatsMngr.get_field_value_for_all_levels(stats, field) ==\\\n           {0: '1.0', 1: '2.0'}\n\n    field = CompactionStatsMngr.LevelFields.COMP_SEC\n    assert CompactionStatsMngr.get_level_field_value(stats, 0, field) == '74.4'\n    assert CompactionStatsMngr.get_level_field_value(stats, 1, field) == '84.4'\n    assert CompactionStatsMngr.get_sum_value(stats, field) == '158.8'\n\n    assert CompactionStatsMngr.get_field_value_for_all_levels(stats, field) ==\\\n           {0: '74.4', 1: '84.4'}\n\n    field = CompactionStatsMngr.LevelFields.COMP_MERGE_CPU\n    assert CompactionStatsMngr.get_level_field_value(stats, 0, field) == '10.1'\n    assert CompactionStatsMngr.get_level_field_value(stats, 1, field) == '20.2'\n    assert CompactionStatsMngr.get_sum_value(stats, field) == '30.3'\n\n    assert CompactionStatsMngr.get_field_value_for_all_levels(stats, field) ==\\\n           {0: '10.1', 1: '20.2'}", "\n\ndef test_compactions_stats_mngr():\n    stats_lines = \\\n        f'''** Compaction Stats [default] **\n    Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop Rblob(GB) Wblob(GB)\n    ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n      L0     16/11   3.90 GB   5.9    131.5     0.0    131.5     231.0     99.5       0.0   2.3    224.8    394.9    599.04            488.24      2319    0.258    136M   322K       0.0       0.0\n      L1     30/30   1.81 GB   0.0    156.4    95.8     60.6     153.8     93.2       0.0   1.6    354.8    349.0    451.24            372.78        52    8.678    162M  2648K       0.0       0.0\n      L2     43/0    2.47 GB   1.0    162.1    77.6     84.5     159.7     75.2      14.0   2.1    319.5    314.8    519.66            404.59      1075    0.483    168M  2502K       0.0       0.0\n      L3    436/0   24.96 GB   1.0    473.6    89.2    384.4     445.7     61.3       0.0   5.0    327.9    308.5   1479.17           1127.43      1470    1.006    492M    29M       0.0       0.0\n      L4   1369/0   66.58 GB   0.3    169.1    61.3    107.9     129.6     21.8       0.0   2.1    362.4    277.7    477.96            344.39       996    0.480    175M    41M       0.0       0.0\n     Sum   1894/41  99.72 GB   0.0   1092.7   323.9    768.8    1119.9    351.0      14.0  11.2    317.3    325.1   3527.07           2737.42      5912    0.597   1136M    75M       0.0       0.0\n     Int      0/0    0.00 KB   0.0   1092.7   323.9    768.8    1119.9    351.0      14.0  11.2    317.3    325.1   3527.07           2737.42      5912    0.597   1136M    75M       0.0       0.0'''.splitlines() # noqa\n    stats_lines = [line.strip() for line in stats_lines]\n\n    expected_level0_dict = \\\n        {'Num-Files': '16',\n         'Files-In-Comp': '11',\n         'size_bytes': 4187593113,\n         'Score': '5.9',\n         'Read(GB)': '131.5',\n         'Rn(GB)': '0.0',\n         'Rnp1(GB)': '131.5',\n         'Write(GB)': '231.0',\n         'Wnew(GB)': '99.5',\n         'Moved(GB)': '0.0',\n         'W-Amp': '2.3',\n         'Rd(MB/s)': '224.8',\n         'Wr(MB/s)': '394.9',\n         'Comp(sec)': '599.04',\n         'CompMergeCPU(sec)': '488.24',\n         'Comp(cnt)': '2319',\n         'Avg(sec)': '0.258',\n         'KeyIn': '136M',\n         'KeyDrop': '322K',\n         'Rblob(GB)': '0.0',\n         'Wblob(GB)': '0.0'}\n\n    expected_sum_dict = \\\n        {'Num-Files': '1894',\n         'Files-In-Comp': '41',\n         'size_bytes': 107073534689,\n         'Score': '0.0',\n         'Read(GB)': '1092.7',\n         'Rn(GB)': '323.9',\n         'Rnp1(GB)': '768.8',\n         'Write(GB)': '1119.9',\n         'Wnew(GB)': '351.0',\n         'Moved(GB)': '14.0',\n         'W-Amp': '11.2',\n         'Rd(MB/s)': '317.3',\n         'Wr(MB/s)': '325.1',\n         'Comp(sec)': '3527.07',\n         'CompMergeCPU(sec)': '2737.42',\n         'Comp(cnt)': '5912',\n         'Avg(sec)': '0.597',\n         'KeyIn': '1136M',\n         'KeyDrop': '75M',\n         'Rblob(GB)': '0.0',\n         'Wblob(GB)': '0.0'}\n\n    time = \"2023/01/04-09:04:59.378877 27442\"\n    cf_name = \"default\"\n    level0_key = 'LEVEL-0'\n    sum_key = \"SUM\"\n    mngr = CompactionStatsMngr()\n    mngr.add_lines(time, cf_name, stats_lines, 0)\n    entries = mngr.get_cf_level_entries(cf_name)\n    assert isinstance(entries, list)\n    assert len(entries) == 1\n    assert isinstance(entries[0], dict)\n    assert list(entries[0].keys()) == [time]\n    values = entries[0][time]\n    assert isinstance(values, dict)\n    assert len(values.keys()) == 7\n    assert level0_key in values\n    level0_values_dict = values[level0_key]\n    assert level0_values_dict == expected_level0_dict\n    assert sum_key in values\n    sum_values_dict = values[sum_key]\n    assert sum_values_dict == expected_sum_dict\n\n    field = CompactionStatsMngr.LevelFields.WRITE_AMP\n    assert CompactionStatsMngr.get_level_field_value(entries[0], 1, field) ==\\\n           '1.6'\n    assert CompactionStatsMngr.get_sum_value(entries[0], field) == '11.2'\n    assert CompactionStatsMngr.get_field_value_for_all_levels(\n        entries[0], field) == {0: '2.3',\n                               1: '1.6',\n                               2: '2.1',\n                               3: '5.0',\n                               4: '2.1'}\n\n    field = CompactionStatsMngr.LevelFields.COMP_SEC\n    assert CompactionStatsMngr.get_level_field_value(entries[0], 1, field) == \\\n           '451.24'\n    assert CompactionStatsMngr.get_sum_value(entries[0], field) == '3527.07'\n\n    field = CompactionStatsMngr.LevelFields.COMP_MERGE_CPU\n    assert CompactionStatsMngr.get_level_field_value(entries[0], 1, field) == \\\n           '372.78'\n    assert CompactionStatsMngr.get_sum_value(entries[0], field) == '2737.42'", "\n\n#\n# FILE READ LATENCY TESTS INFO\n#\ntime1 = \"2023/01/04-09:04:59.378877 27442\"\ntime2 = \"2023/01/04-09:14:59.378877 27442\"\nl0 = 0\nl4 = 4\ncf1 = \"cf1\"", "l4 = 4\ncf1 = \"cf1\"\ncf2 = \"cf2\"\n\nstats_lines_cf1_t1_l0 = \\\nf'''** File Read Latency Histogram By Level [{cf1}] **\n** Level {l0} read latency histogram (micros):\nCount: 26687513 Average: 3.1169  StdDev: 34.39\nMin: 0  Median: 2.4427  Max: 56365\nPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa", "Min: 0  Median: 2.4427  Max: 56365\nPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\nstats_lines_cf1_t1_l0 = stats_lines_cf1_t1_l0.splitlines()\nstats_lines_cf1_t1_l0 = [line.strip() for line in stats_lines_cf1_t1_l0]\n\nstats_lines_cf1_t1_l4 = \\\nf'''** File Read Latency Histogram By Level [{cf1}] **\n** Level {l4} read latency histogram (micros):\nCount: 100 Average: 1.1  StdDev: 2.2\nMin: 1000  Median: 3.3  Max: 2000", "Count: 100 Average: 1.1  StdDev: 2.2\nMin: 1000  Median: 3.3  Max: 2000\nPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\nstats_lines_cf1_t1_l4 = stats_lines_cf1_t1_l4.splitlines()\nstats_lines_cf1_t1_l4 = [line.strip() for line in stats_lines_cf1_t1_l4]\n\nstats_lines_cf1_t2_l4 = \\\nf'''** File Read Latency Histogram By Level [{cf1}] **\n** Level {l4} read latency histogram (micros):\nCount: 500 Average: 10.10  StdDev: 20.20", "** Level {l4} read latency histogram (micros):\nCount: 500 Average: 10.10  StdDev: 20.20\nMin: 10000  Median: 30.30  Max: 20000\nPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\nstats_lines_cf1_t2_l4 = stats_lines_cf1_t2_l4.splitlines()\nstats_lines_cf1_t2_l4 = [line.strip() for line in stats_lines_cf1_t2_l4]\n\nstats_lines_cf2_t1_l0 = \\\nf'''** File Read Latency Histogram By Level [{cf2}] **\n** Level {l0} read latency histogram (micros):", "f'''** File Read Latency Histogram By Level [{cf2}] **\n** Level {l0} read latency histogram (micros):\nCount: 200 Average: 6.6  StdDev: 7.7\nMin: 5000  Median: 8.8  Max: 6000\nPercentiles: P50: 2.44 P75: 3.52 P99: 5.93 P99.9: 7.64 P99.99: 8.02''' # noqa\nstats_lines_cf2_t1_l0 = stats_lines_cf2_t1_l0.splitlines()\nstats_lines_cf2_t1_l0 = [line.strip() for line in stats_lines_cf2_t1_l0]\n\nexpected_cf1_t1_l0_stats = \\\n    CfFileHistogramStatsMngr.CfLevelStats(", "expected_cf1_t1_l0_stats = \\\n    CfFileHistogramStatsMngr.CfLevelStats(\n        count=26687513,\n        average=3.1169,\n        std_dev=34.39,\n        min=0,\n        median=2.4427,\n        max=56365)\n\nexpected_cf1_t1_l4_stats = \\", "\nexpected_cf1_t1_l4_stats = \\\n    CfFileHistogramStatsMngr.CfLevelStats(\n        count=100,\n        average=1.1,\n        std_dev=2.2,\n        min=1000,\n        median=3.3,\n        max=2000)\n", "        max=2000)\n\nexpected_cf1_t2_l4_stats = \\\n    CfFileHistogramStatsMngr.CfLevelStats(\n        count=500,\n        average=10.10,\n        std_dev=20.20,\n        min=10000,\n        median=30.30,\n        max=20000)", "        median=30.30,\n        max=20000)\n\nexpected_cf2_t1_l0_stats = \\\n    CfFileHistogramStatsMngr.CfLevelStats(\n        count=200,\n        average=6.6,\n        std_dev=7.7,\n        min=5000,\n        median=8.8,", "        min=5000,\n        median=8.8,\n        max=6000)\n\n\ndef test_cf_file_histogram_mngr1():\n    mngr = CfFileHistogramStatsMngr()\n\n    expected_cf1_entries = dict()\n\n    mngr.add_lines(time1, cf1, stats_lines_cf1_t1_l0)\n    expected_cf1_entries[time1] = {l0:  expected_cf1_t1_l0_stats}\n    assert mngr.get_cf_entries(cf1) == expected_cf1_entries\n    assert mngr.get_last_cf_entry(cf1) == expected_cf1_entries\n    assert mngr.get_cf_entries(cf2) is None\n\n    mngr.add_lines(time1, cf1, stats_lines_cf1_t1_l4)\n    expected_cf1_entries[time1][l4] = expected_cf1_t1_l4_stats\n    assert mngr.get_cf_entries(cf1) == expected_cf1_entries\n    assert mngr.get_last_cf_entry(cf1) == expected_cf1_entries\n    assert mngr.get_cf_entries(cf2) is None\n\n    expected_cf2_entries = dict()\n    expected_cf2_entries[time1] = {}\n    expected_cf2_entries[time1][l0] = expected_cf2_t1_l0_stats\n\n    mngr.add_lines(time1, cf2, stats_lines_cf2_t1_l0)\n    assert mngr.get_cf_entries(cf2) == expected_cf2_entries\n    assert mngr.get_last_cf_entry(cf2) == expected_cf2_entries\n\n    expected_cf1_entries[time2] = {}\n    expected_cf1_entries[time2][l4] = expected_cf1_t2_l4_stats\n\n    mngr.add_lines(time2, cf1, stats_lines_cf1_t2_l4)\n    assert mngr.get_cf_entries(cf1) == expected_cf1_entries\n    assert mngr.get_last_cf_entry(cf1) == \\\n           {time2: {l4: expected_cf1_t2_l4_stats}}", "\n\ndef test_cf_file_histogram_mngr2():\n    empty_line = \"                      \"\n    all_cf1_stats_lines = list()\n    all_cf1_stats_lines.extend(stats_lines_cf1_t1_l0)\n    all_cf1_stats_lines.extend([empty_line] * 2)\n    all_cf1_stats_lines.extend(stats_lines_cf1_t1_l4[1:])\n    all_cf1_stats_lines.extend([empty_line])\n\n    mngr = CfFileHistogramStatsMngr()\n\n    expected_cf1_entries = {\n        time1: {l0: expected_cf1_t1_l0_stats,\n                l4: expected_cf1_t1_l4_stats}}\n\n    mngr.add_lines(time1, cf1, all_cf1_stats_lines)\n    assert mngr.get_cf_entries(cf1) == expected_cf1_entries", "\n\nstats_dump = \"\"\"2022/10/07-16:50:52.365328 7f68d1999700 [db/db_impl/db_impl.cc:901] ------- DUMPING STATS -------\n2022/10/07-16:50:52.365535 7f68d1999700 [db/db_impl/db_impl.cc:903] \n** DB Stats **\nUptime(secs): 4.1 total, 4.1 interval\nCumulative writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 GB, 0.00 MB/s\nCumulative WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 GB, 0.00 MB/s\nCumulative stall: 00:00:0.000 H:M:S, 0.0 percent\nInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s", "Cumulative stall: 00:00:0.000 H:M:S, 0.0 percent\nInterval writes: 0 writes, 0 keys, 0 commit groups, 0.0 writes per commit group, ingest: 0.00 MB, 0.00 MB/s\nInterval WAL: 0 writes, 0 syncs, 0.00 writes per sync, written: 0.00 MB, 0.00 MB/s\nInterval stall: 00:00:0.000 H:M:S, 0.0 percent\n\n** Compaction Stats [default] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      5/0   154.26 MB   1.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n Sum   1127/0   70.96 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0", "  L0      5/0   154.26 MB   1.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n Sum   1127/0   70.96 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n\n** Compaction Stats [default] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     11.7      0.32              0.00         1    0.315       0      0\nUptime(secs): 4.1 total, 4.1 interval\nFlush(GB): cumulative 0.004, interval 0.004", "Uptime(secs): 4.1 total, 4.1 interval\nFlush(GB): cumulative 0.004, interval 0.004\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.90 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.3 seconds\nInterval compaction: 0.00 GB write, 0.90 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.3 seconds\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n", "Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** File Read Latency Histogram By Level [default] **\n** Level 0 read latency histogram (micros):\nCount: 25 Average: 1571.7200  StdDev: 5194.93\nMin: 1  Median: 2.8333  Max: 26097\nPercentiles: P50: 2.83 P75: 32.50 P99: 26097.00 P99.9: 26097.00 P99.99: 26097.00\n------------------------------------------------------\n(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####", "(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####\n\n\n** Compaction Stats [meta-Kdm3W] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0    1.95 KB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n  L1      1/0    1.04 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n Sum      3/0    2.99 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0", "  L1      1/0    1.04 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n Sum      3/0    2.99 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n\n** Compaction Stats [meta-Kdm3W] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.1      0.01              0.00         1    0.007       0      0\nUptime(secs): 4.1 total, 4.1 interval\nFlush(GB): cumulative 0.000, interval 0.000", "Uptime(secs): 4.1 total, 4.1 interval\nFlush(GB): cumulative 0.000, interval 0.000\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n", "Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** File Read Latency Histogram By Level [meta-Kdm3W] **\n** Level 0 read latency histogram (micros):\nCount: 8 Average: 3.0000  StdDev: 3.20\nMin: 1  Median: 1.0000  Max: 11\nPercentiles: P50: 1.00 P75: 3.00 P99: 11.00 P99.9: 11.00 P99.99: 11.00\n------------------------------------------------------\n(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####", "(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####\n\n** Level 1 read latency histogram (micros):\nCount: 4 Average: 2.5000  StdDev: 0.87\nMin: 2  Median: 2.0000  Max: 4\nPercentiles: P50: 2.00 P75: 2.00 P99: 3.96 P99.9: 4.00 P99.99: 4.00\n------------------------------------------------------\n(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####", "(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####\n\n\n** Compaction Stats [txlog-Kdm3W] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n", " Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\n** Compaction Stats [txlog-Kdm3W] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUptime(secs): 4.1 total, 4.1 interval\nFlush(GB): cumulative 0.000, interval 0.000\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0", "AddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** File Read Latency Histogram By Level [txlog-Kdm3W] **\n\n** Compaction Stats [!qs-stats-nvbl9e] **", "\n** Compaction Stats [!qs-stats-nvbl9e] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0    1.88 KB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.003       0      0\n  L1      1/0    1.10 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n Sum      3/0    2.97 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.003       0      0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.4      0.00              0.00         1    0.003       0      0\n\n** Compaction Stats [!qs-stats-nvbl9e] **", "\n** Compaction Stats [!qs-stats-nvbl9e] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.4      0.00              0.00         1    0.003       0      0\nUptime(secs): 4.1 total, 4.1 interval\nFlush(GB): cumulative 0.000, interval 0.000\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0", "AddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** File Read Latency Histogram By Level [!qs-stats-nvbl9e] **\n** Level 0 read latency histogram (micros):\nCount: 8 Average: 2.1250  StdDev: 1.27", "** Level 0 read latency histogram (micros):\nCount: 8 Average: 2.1250  StdDev: 1.27\nMin: 1  Median: 1.3333  Max: 5\nPercentiles: P50: 1.33 P75: 2.00 P99: 5.00 P99.9: 5.00 P99.99: 5.00\n------------------------------------------------------\n(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####\n\n\n** Compaction Stats [default] **", "\n** Compaction Stats [default] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      5/0   154.26 MB   1.2      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n Sum   1127/0   70.96 GB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0     11.7      0.32              0.00         1    0.315       0      0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\n** Compaction Stats [default] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop", "** Compaction Stats [default] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0     11.7      0.32              0.00         1    0.315       0      0\nUptime(secs): 4.1 total, 0.0 interval\nFlush(GB): cumulative 0.004, interval 0.000\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0", "AddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.90 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.3 seconds\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** File Read Latency Histogram By Level [default] **\n** Level 0 read latency histogram (micros):\nCount: 25 Average: 1571.7200  StdDev: 5194.93\nMin: 1  Median: 2.8333  Max: 26097", "Count: 25 Average: 1571.7200  StdDev: 5194.93\nMin: 1  Median: 2.8333  Max: 26097\nPercentiles: P50: 2.83 P75: 32.50 P99: 26097.00 P99.9: 26097.00 P99.99: 26097.00\n------------------------------------------------------\n(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####\n\n** Compaction Stats [meta-Kdm3W] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------", "Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  L0      2/0    1.95 KB   1.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n  L1      1/0    1.04 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n Sum      3/0    2.99 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   1.0      0.0      0.1      0.01              0.00         1    0.007       0      0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\n** Compaction Stats [meta-Kdm3W] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------", "Priority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUser      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.1      0.01              0.00         1    0.007       0      0\nUptime(secs): 4.1 total, 0.0 interval\nFlush(GB): cumulative 0.000, interval 0.000\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds", "AddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** File Read Latency Histogram By Level [meta-Kdm3W] **\n** Level 0 read latency histogram (micros):\nCount: 8 Average: 3.0000  StdDev: 3.20\nMin: 1  Median: 1.0000  Max: 11\nPercentiles: P50: 1.00 P75: 3.00 P99: 11.00 P99.9: 11.00 P99.99: 11.00", "Min: 1  Median: 1.0000  Max: 11\nPercentiles: P50: 1.00 P75: 3.00 P99: 11.00 P99.9: 11.00 P99.99: 11.00\n------------------------------------------------------\n(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####\n\n** Level 1 read latency histogram (micros):\nCount: 4 Average: 2.5000  StdDev: 0.87\nMin: 2  Median: 2.0000  Max: 4\nPercentiles: P50: 2.00 P75: 2.00 P99: 3.96 P99.9: 4.00 P99.99: 4.00", "Min: 2  Median: 2.0000  Max: 4\nPercentiles: P50: 2.00 P75: 2.00 P99: 3.96 P99.9: 4.00 P99.99: 4.00\n------------------------------------------------------\n(       1,       2 ]        3  75.000%  75.000% ###############\n(       3,       4 ]        1  25.000% 100.000% #####\n\n\n** Compaction Stats [txlog-Kdm3W] **\nLevel    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------", "Level    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n Sum      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n Int      0/0    0.00 KB   0.0      0.0     0.0      0.0       0.0      0.0       0.0   0.0      0.0      0.0      0.00              0.00         0    0.000       0      0\n\n** Compaction Stats [txlog-Kdm3W] **\nPriority    Files   Size     Score Read(GB)  Rn(GB) Rnp1(GB) Write(GB) Wnew(GB) Moved(GB) W-Amp Rd(MB/s) Wr(MB/s) Comp(sec) CompMergeCPU(sec) Comp(cnt) Avg(sec) KeyIn KeyDrop\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nUptime(secs): 4.1 total, 0.0 interval\nFlush(GB): cumulative 0.000, interval 0.000", "Uptime(secs): 4.1 total, 0.0 interval\nFlush(GB): cumulative 0.000, interval 0.000\nAddFile(GB): cumulative 0.000, interval 0.000\nAddFile(Total Files): cumulative 0, interval 0\nAddFile(L0 Files): cumulative 0, interval 0\nAddFile(Keys): cumulative 0, interval 0\nCumulative compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nInterval compaction: 0.00 GB write, 0.00 MB/s write, 0.00 GB read, 0.00 MB/s read, 0.0 seconds\nStalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n", "Stalls(count): 0 level0_slowdown, 0 level0_slowdown_with_compaction, 0 level0_numfiles, 0 level0_numfiles_with_compaction, 0 stop for pending_compaction_bytes, 0 slowdown for pending_compaction_bytes, 0 memtable_compaction, 0 memtable_slowdown, interval 0 total count\n\n** File Read Latency Histogram By Level [txlog-Kdm3W] **\n\n \"\"\" # noqa\nstats_dump = stats_dump.splitlines()\nstats_dump = [line.strip() for line in stats_dump]\nstats_dump_entries = lines_to_entries(stats_dump)\n\n\ndef test_stats_mngr_parsing_ignoring_repeating_cfs():\n    mngr = StatsMngr()\n    assert mngr.is_dump_stats_start(stats_dump_entries[0])\n    mngr.try_adding_entries(stats_dump_entries, 0)", "\n\ndef test_stats_mngr_parsing_ignoring_repeating_cfs():\n    mngr = StatsMngr()\n    assert mngr.is_dump_stats_start(stats_dump_entries[0])\n    mngr.try_adding_entries(stats_dump_entries, 0)\n"]}
{"filename": "test/test_compactions.py", "chunked_list": ["import pytest\n\nimport compactions\nimport utils\nfrom events import EventType\nfrom test.testing_utils import create_event, line_to_entry\n\njob_id1 = 1\njob_id2 = 2\n", "job_id2 = 2\n\ncf1 = \"cf1\"\ncf2 = \"cf2\"\ncf_names = [cf1, cf2]\n\ntime1_minus_10_sec = \"2023/01/24-08:54:30.130553\"\ntime1 = \"2023/01/24-08:54:40.130553\"\ntime1_plus_10_sec = \"2023/01/24-08:54:50.130553\"\ntime1_plus_11_sec = \"2023/01/24-08:55:50.130553\"", "time1_plus_10_sec = \"2023/01/24-08:54:50.130553\"\ntime1_plus_11_sec = \"2023/01/24-08:55:50.130553\"\n\ntime2 = \"2023/01/24-08:55:40.130553\"\ntime2_plus_5_sec = \"2023/01/24-08:55:45.130553\"\n\nreason1 = \"Reason1\"\nreason2 = \"Reason2\"\n\nfiles_l1 = [17248]", "\nfiles_l1 = [17248]\nfiles_l2 = [16778, 16779, 16780, 16781, 17022]\ninput_files = {\n    1: files_l1,\n    2: files_l2\n}\n\n\ndef test_compaction_info():\n    info = compactions.CompactionJobInfo(job_id1)\n    assert info.get_state() == compactions.CompactionState.WAITING_START\n\n    start_event1 = create_event(job_id1, cf_names, time1_minus_10_sec,\n                                EventType.COMPACTION_STARTED, cf1,\n                                compaction_reason=reason1)\n    start_event2 = create_event(job_id1, cf_names, time1,\n                                EventType.COMPACTION_STARTED, cf1,\n                                compaction_reason=reason1)\n\n    finish_event1 = create_event(job_id1, cf_names, time1_minus_10_sec,\n                                 EventType.COMPACTION_FINISHED, cf1)\n    finish_event2 = create_event(job_id1, cf_names, time1_plus_10_sec,\n                                 EventType.COMPACTION_FINISHED, cf1)\n    finish_event3 = create_event(job_id1, cf_names, time1_plus_11_sec,\n                                 EventType.COMPACTION_FINISHED, cf1)\n\n    with pytest.raises(utils.ParsingError):\n        info.set_finish_event(finish_event1)\n\n    info.set_start_event(start_event2)\n    assert info.get_state() == compactions.CompactionState.STARTED\n    with pytest.raises(utils.ParsingError):\n        info.set_start_event(start_event1)\n\n    with pytest.raises(utils.ParsingError):\n        info.set_finish_event(finish_event1)\n\n    info.set_finish_event(finish_event2)\n    assert info.get_state() == compactions.CompactionState.FINISHED\n\n    with pytest.raises(utils.ParsingError):\n        info.set_finish_event(finish_event3)", "\ndef test_compaction_info():\n    info = compactions.CompactionJobInfo(job_id1)\n    assert info.get_state() == compactions.CompactionState.WAITING_START\n\n    start_event1 = create_event(job_id1, cf_names, time1_minus_10_sec,\n                                EventType.COMPACTION_STARTED, cf1,\n                                compaction_reason=reason1)\n    start_event2 = create_event(job_id1, cf_names, time1,\n                                EventType.COMPACTION_STARTED, cf1,\n                                compaction_reason=reason1)\n\n    finish_event1 = create_event(job_id1, cf_names, time1_minus_10_sec,\n                                 EventType.COMPACTION_FINISHED, cf1)\n    finish_event2 = create_event(job_id1, cf_names, time1_plus_10_sec,\n                                 EventType.COMPACTION_FINISHED, cf1)\n    finish_event3 = create_event(job_id1, cf_names, time1_plus_11_sec,\n                                 EventType.COMPACTION_FINISHED, cf1)\n\n    with pytest.raises(utils.ParsingError):\n        info.set_finish_event(finish_event1)\n\n    info.set_start_event(start_event2)\n    assert info.get_state() == compactions.CompactionState.STARTED\n    with pytest.raises(utils.ParsingError):\n        info.set_start_event(start_event1)\n\n    with pytest.raises(utils.ParsingError):\n        info.set_finish_event(finish_event1)\n\n    info.set_finish_event(finish_event2)\n    assert info.get_state() == compactions.CompactionState.FINISHED\n\n    with pytest.raises(utils.ParsingError):\n        info.set_finish_event(finish_event3)", "\n\ndef test_mon_basic():\n    monitor = compactions.CompactionsMonitor()\n    assert monitor.get_finished_jobs() == {}\n\n    # Compaction Job 1 - Start + Finish Events\n    start_event1 = create_event(job_id1, cf_names, time1,\n                                EventType.COMPACTION_STARTED, cf1,\n                                compaction_reason=reason1,\n                                files_L1=files_l1, files_L2=files_l2)\n    monitor.new_event(start_event1)\n    assert monitor.get_finished_jobs() == {}\n\n    finish_event1 = create_event(job_id1, cf_names, time1_plus_10_sec,\n                                 EventType.COMPACTION_FINISHED, cf1)\n    monitor.new_event(finish_event1)\n\n    expected_jobs = {\n        job_id1: compactions.CompactionJobInfo(job_id1,\n                                               cf_name=cf1,\n                                               start_event=start_event1,\n                                               finish_event=finish_event1,\n                                               pre_finish_info=None)\n    }\n    assert monitor.get_finished_jobs() == expected_jobs\n\n    # Compaction Job 2 - Start + Finish Events\n    start_event2 = create_event(job_id2, cf_names, time2,\n                                EventType.COMPACTION_STARTED, cf2,\n                                compaction_reason=reason2)\n    monitor.new_event(start_event2)\n    assert monitor.get_finished_jobs() == expected_jobs\n\n    finish_event2 = create_event(job_id2, cf_names, time2_plus_5_sec,\n                                 EventType.COMPACTION_FINISHED, cf2)\n    monitor.new_event(finish_event2)\n\n    expected_jobs[job_id2] = \\\n        compactions.CompactionJobInfo(job_id2,\n                                      cf_name=cf2,\n                                      start_event=start_event2,\n                                      finish_event=finish_event2)\n    assert monitor.get_finished_jobs() == expected_jobs\n\n    expected_cf1_jobs = {\n        job_id1: compactions.CompactionJobInfo(job_id1,\n                                               cf_name=cf1,\n                                               start_event=start_event1,\n                                               finish_event=finish_event1,\n                                               pre_finish_info=None)\n    }\n    assert monitor.get_cf_finished_jobs(cf1) == expected_cf1_jobs\n\n    expected_cf2_jobs = {\n        job_id2: compactions.CompactionJobInfo(job_id2,\n                                               cf_name=cf2,\n                                               start_event=start_event2,\n                                               finish_event=finish_event2,\n                                               pre_finish_info=None)\n    }\n    assert monitor.get_cf_finished_jobs(cf2) == expected_cf2_jobs", "\n\ndef test_try_parse_as_pre_finish_stats_line():\n    max_score = 4.56\n    read_rate_mbps = 475.6\n    write_rate_mbps = 475.0\n    level = 10\n    read_write_amplify = 1.2\n    write_amplify = 3.4\n    records_in = 5678\n    records_dropped = 9876\n\n    pre_fihish_entry = \\\n        line_to_entry(\n            f\"{time1} 1234 (Original Log Time {time1_minus_10_sec}) \"\n            f\"[/compaction/compaction_job.cc:952] \"\n            f\"[{cf2}] compacted to: files[3 7 45 427 822 0 0] \"\n            f\"max score {max_score}, \"\n            f\"MB/sec: {read_rate_mbps} rd, {write_rate_mbps} wr, \"\n            f\"level {level}, files in(0, 4) out(1 +0 blob) \"\n            f\"MB in(0.0, 239.7 +0.0 blob) out(239.4 +0.0 blob), \"\n            f\"read-write-amplify({read_write_amplify}) \"\n            f\"write-amplify({write_amplify}) OK, \"\n            f\"records in: {records_in}, records dropped: {records_dropped} \"\n            f\"output_compression: NoCompression\")\n\n    start_event = create_event(job_id1, cf_names, time1,\n                               EventType.COMPACTION_STARTED, cf2,\n                               compaction_reason=reason1,\n                               files_L1=files_l1, files_L2=files_l2)\n\n    finish_event = create_event(job_id1, cf_names, time1_plus_11_sec,\n                                EventType.COMPACTION_FINISHED, cf2,\n                                num_input_records=records_in)\n\n    monitor = compactions.CompactionsMonitor()\n    monitor.new_event(start_event)\n    assert monitor.consider_entry(pre_fihish_entry) == (True, cf2)\n    monitor.new_event(finish_event)\n\n    expected_pre_finish_info = \\\n        compactions.PreFinishStatsInfo(cf_name=cf2,\n                                       read_rate_mbps=read_rate_mbps,\n                                       write_rate_mbps=write_rate_mbps,\n                                       read_write_amplify=read_write_amplify,\n                                       write_amplify=write_amplify,\n                                       records_in=records_in,\n                                       records_dropped=records_dropped)\n\n    expected_jobs = {\n        job_id1: compactions.CompactionJobInfo(\n            job_id=job_id1,\n            cf_name=cf2,\n            start_event=start_event,\n            finish_event=finish_event,\n            pre_finish_info=expected_pre_finish_info)\n    }\n    assert monitor.get_finished_jobs() == expected_jobs", ""]}
{"filename": "test/test_utils.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport copy\nfrom datetime import datetime\n\nimport pytest", "\nimport pytest\n\nimport utils\n\n\ndef test_unify_dicts():\n    unify = utils.unify_dicts\n\n    d1 = dict(a=100, b=200)\n    d1_copy = copy.deepcopy(d1)\n    d2 = dict(a=300, c=500)\n    d2_copy = copy.deepcopy(d2)\n\n    assert unify({}, {}, True) == {}\n    assert unify(d1, {}, True) == d1\n    assert unify({}, d2, True) == d2\n    assert unify(d1, d2, True) == dict(a=100, b=200, c=500)\n    assert unify(d1, d2, False) == dict(a=300, b=200, c=500)\n\n    # Verify no mutation of inputes\n    assert d1_copy == d1\n    assert d2_copy == d2", "\n\ndef test_delete_dict_keys():\n    d1 = dict(a=100, b=200, c=300)\n    keys = [\"a\"]\n    utils.delete_dict_keys(d1, keys)\n    assert d1 == dict(b=200, c=300)\n\n\ndef test_are_dicts_equal_and_in_same_keys_order():\n    d1 = dict(x=10, y=20)\n    d2 = dict(y=20, x=10)\n\n    are_equal = utils.are_dicts_equal_and_in_same_keys_order\n    assert are_equal({}, {})\n    assert are_equal(d1, d1)\n    assert d1 == d2\n    assert not are_equal(d1, d2)\n    assert not are_equal(d2, d1)", "\ndef test_are_dicts_equal_and_in_same_keys_order():\n    d1 = dict(x=10, y=20)\n    d2 = dict(y=20, x=10)\n\n    are_equal = utils.are_dicts_equal_and_in_same_keys_order\n    assert are_equal({}, {})\n    assert are_equal(d1, d1)\n    assert d1 == d2\n    assert not are_equal(d1, d2)\n    assert not are_equal(d2, d1)", "\n\ndef test_unify_lists_preserve_order():\n    unify = utils.unify_lists_preserve_order\n\n    assert unify([], []) == []\n    assert unify([1, 2], [1, 2]) == [1, 2]\n    assert unify([2, 1], [4, 3]) == [2, 1, 4, 3]\n    assert unify([2, 1], [2]) == [2, 1]\n    assert unify([2, 1], [1]) == [2, 1]\n    assert unify([2, 1], [1, 2]) == [2, 1]\n    assert unify([2, 1], [2, 3]) == [2, 1, 3]\n    assert unify([2, 1, 4], [4, 3, 1, 5, 2]) == [2, 1, 4, 3, 5]", "\n\ndef test_get_get_times_strs_diff_seconds():\n    diff = utils.get_times_strs_diff_seconds\n    M = 10 ** 6\n\n    time1 = \"2022/11/24-15:50:09.512106\"\n    time1_minus_10_micros = \"2022/11/24-15:50:09.512096\"\n    time1_plus_1_second = \"2022/11/24-15:50:10.512106\"\n\n    assert diff(time1, time1_minus_10_micros) == -10 / M\n    assert diff(time1_minus_10_micros, time1) == 10 / M\n    assert diff(time1, time1_plus_1_second) == 1", "\n\ndef test_get_time_relative_to():\n    diff = utils.get_times_strs_diff_seconds\n    rel_time = utils.get_time_relative_to\n\n    time1 = \"2022/11/24-15:50:09.512106\"\n    assert rel_time(time1, 0) == time1\n    assert diff(time1, rel_time(time1, 1)) == 1\n    assert diff(time1, rel_time(time1, -1)) == -1\n    assert diff(time1, rel_time(time1, 10**6)) == 10**6\n    assert diff(time1, rel_time(time1, 0, num_us=1)) == 1/10**6\n    assert diff(time1, rel_time(time1, 0, num_us=10000)) == 1/100\n    assert diff(time1, rel_time(time1, 10, num_us=10000)) == 10.01", "\n\ndef test_compare_times_strs():\n    time1 = \"2022/11/24-15:50:09.512106\"\n    time2 = \"2022/11/24-15:51:09.512107\"\n\n    assert utils.compare_times_strs(time1, time1) == 0\n    assert utils.compare_times_strs(time1, time2) < 0\n    assert utils.compare_times_strs(time2, time1) > 0\n", "\n\ndef test_parse_time_str():\n    parse_time = utils.parse_time_str\n\n    now = datetime.now()\n    now_str = now.strftime(\"%Y/%m/%d-%H:%M:%S.%f\")\n\n    assert now == utils.parse_time_str(now_str)\n\n    assert parse_time(\"\", False) is None\n    assert parse_time(\"XXXX\", False) is None\n    assert parse_time(\"2022/11/24-15:50:09\", False) is None\n    with pytest.raises(utils.ParsingError):\n        assert parse_time(\"\", True)", "\n\ndef test_is_valid_time_str():\n    is_valid = utils.is_valid_time_str\n\n    assert is_valid(\"2022/11/24-15:50:09.123456\")\n    assert not is_valid(\"\")\n    assert not is_valid(\"XXX\")\n    assert not is_valid(\"2022/11/24-15:50:09\")\n", "\n\ndef test_convert_seconds_to_dd_hh_mm_ss():\n    one_minute = 60\n    one_hour = 3600\n    one_day = 24 * 3600\n\n    assert utils.convert_seconds_to_dd_hh_mm_ss(0) == \"0d 00h 00m 00s\"\n    assert utils.convert_seconds_to_dd_hh_mm_ss(1) == \"0d 00h 00m 01s\"\n    assert utils.convert_seconds_to_dd_hh_mm_ss(one_minute) == \"0d 00h 01m 00s\"\n    assert utils.convert_seconds_to_dd_hh_mm_ss(one_hour) == \"0d 01h 00m 00s\"\n    assert utils.convert_seconds_to_dd_hh_mm_ss(one_day) == \"1d 00h 00m 00s\"\n    assert utils.convert_seconds_to_dd_hh_mm_ss(\n        one_day + one_hour + one_minute + 30) == \"1d 01h 01m 30s\"", "\n\ndef test_get_num_bytes_from_human_readable_components():\n    num_bytes = utils.get_num_bytes_from_human_readable_components\n\n    assert num_bytes(\"1\", \"\") == 1\n    assert num_bytes(\"  1    \", \"\") == 1\n    assert num_bytes(\"1\", \"KB\") == 2**10\n    assert num_bytes(\"1\", \"  KB    \") == 2**10\n    assert num_bytes(\"1\", \"MB\") == 2**20\n    assert num_bytes(\"1\", \"GB\") == 2**30\n    assert num_bytes(\"1\", \"TB\") == 2**40\n\n    with pytest.raises(utils.ParsingError):\n        num_bytes(\"\", \"\")\n\n    with pytest.raises(utils.ParsingError):\n        num_bytes(\"1\", \"X\")", "\n\ndef test_get_num_bytes_from_human_readable_str():\n    num_bytes = utils.get_num_bytes_from_human_readable_str\n\n    assert num_bytes(\"1\") == 1\n    assert num_bytes(\"  1    \") == 1\n    assert num_bytes(\"1KB\") == 2**10\n    assert num_bytes(\"   1  KB    \") == 2**10\n    assert num_bytes(\"1 MB\") == 2**20\n    assert num_bytes(\"1 GB\") == 2**30\n    assert num_bytes(\"1 TB\") == 2**40\n\n    with pytest.raises(utils.ParsingError):\n        num_bytes(\"\")\n\n    with pytest.raises(utils.ParsingError):\n        num_bytes(\"1 X\")\n\n    with pytest.raises(utils.ParsingError):\n        num_bytes(\"KB\")", "\n\ndef test_get_human_readable_num_bytes():\n    human = utils.get_human_readable_num_bytes\n\n    assert human(1) == \"1 B\"\n    assert human(1000) == \"1000 B\"\n    assert human(1024) == \"1.0 KB\"\n    assert human(2048) == \"2.0 KB\"\n    assert human(2**20) == \"1.0 MB\"\n    assert human(2**30) == \"1.0 GB\"\n    assert human(2**40) == \"1.0 TB\"\n    assert human(2 * 2**40) == \"2.0 TB\"\n    assert human(2**50) == \"1024.0 TB\"", "\n\ndef test_get_number_from_human_readable_components():\n    num_bytes = utils.get_number_from_human_readable_components\n\n    assert num_bytes(\"1\", \"\") == 1\n    assert num_bytes(\"  1    \", \"\") == 1\n    assert num_bytes(\"1\", \"K\") == 1000\n    assert num_bytes(\"1\", \"  K    \") == 1000\n    assert num_bytes(\"1\", \"M\") == 10**6\n    assert num_bytes(\"1\", \"G\") == 10**9\n\n    with pytest.raises(utils.ParsingError):\n        num_bytes(\"\", \"\")\n\n    with pytest.raises(utils.ParsingError):\n        num_bytes(\"1\", \"X\")", "\n\ndef test_get_human_readable_number():\n    human = utils.get_human_readable_number\n\n    assert human(1) == \"1\"\n    assert human(1000) == \"1000\"\n    assert human(9999) == \"9999\"\n    assert human(10000) == \"10.0 K\"\n    assert human(20000) == \"20.0 K\"\n    assert human(100000) == \"100.0 K\"\n    assert human(1000000) == \"1000.0 K\"\n    assert human(10000000) == \"10.0 M\"\n    assert human(10**10) == \"10.0 G\"\n    assert human(10**11) == \"100.0 G\"\n    assert human(7 * 10**11) == \"700.0 G\"", "\n\ndef test_get_number_from_human_readable_str():\n    get_num = utils.get_number_from_human_readable_str\n\n    assert get_num(\"0\") == 0\n    assert get_num(\"    0\") == 0\n    assert get_num(\"0   \") == 0\n    assert get_num(\" 1000 \") == 1000\n    assert get_num(\"1K\") == 1000\n    assert get_num(\"1 K\") == 1000\n    assert get_num(\"   1  K   \") == 1000\n    assert get_num(\"1M\") == 10**6\n    assert get_num(\"1 M \") == 10**6\n    assert get_num(\"   1M  \") == 10**6\n    assert get_num(\"1G\") == 10**9\n    assert get_num(\"1 G\") == 10**9\n    assert get_num(\"   1G  \") == 10**9\n\n    with pytest.raises(utils.ParsingError):\n        assert get_num(\"0X\")\n\n    with pytest.raises(utils.ParsingError):\n        assert get_num(\"1KR\")\n\n    with pytest.raises(utils.ParsingError):\n        assert get_num(\"1K R\")", "\n\ndef test_try_find_cf_in_lines():\n    cf1 = \"cf1\"\n    cf2 = \"cf2\"\n    assert utils.try_find_cfs_in_lines([], \"\") is None\n    assert utils.try_find_cfs_in_lines([cf1], \"\") is None\n    assert utils.try_find_cfs_in_lines([cf1], \"cf1\") is None\n    assert utils.try_find_cfs_in_lines([cf1], \"[cf1]\") is cf1\n    assert utils.try_find_cfs_in_lines([cf2], \"cf1\") is None\n    assert utils.try_find_cfs_in_lines([cf1, cf2], \"[cf2]\") == cf2\n    assert set(utils.try_find_cfs_in_lines([cf1, cf2], \"[cf2] [cf1]\")) == \\\n           set([cf2, cf1])\n    assert set(utils.try_find_cfs_in_lines([cf2, cf1], \"[cf2] [cf1]\")) == \\\n           set([cf2, cf1])\n\n    lines1 = f\"Line 1 No cf\\nLine 2 with [{cf1}]\".splitlines()\n    assert len(lines1) == 2\n    assert utils.try_find_cfs_in_lines([cf1], lines1) == cf1\n\n    lines2 = f\"Line 1 No cf\\nLine 2 with [{cf2}]\\nLine3 [{cf2}]\".splitlines()\n    assert len(lines2) == 3\n    assert utils.try_find_cfs_in_lines([cf2], lines2) == cf2\n    assert utils.try_find_cfs_in_lines([cf1], lines2) is None\n    assert utils.try_find_cfs_in_lines([cf1, cf2], lines2) == cf2\n\n    lines3 = f\"Line 1 No cf\\nLine 2 with [{cf2}]\\nLine3 [{cf1}]\".splitlines()\n    assert len(lines3) == 3\n    assert utils.try_find_cfs_in_lines([cf2], lines3) == cf2\n    assert utils.try_find_cfs_in_lines([cf1], lines3) == cf1\n    assert set(utils.try_find_cfs_in_lines([cf1, cf2], lines3)) == \\\n           set([cf1, cf2])", ""]}
{"filename": "test/test_log_entry.py", "chunked_list": ["# Copyright (C) 2023 Speedb Ltd. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.'''\n\nimport pytest\nfrom log_entry import LogEntry\nimport utils\n", "import utils\n\n\ndef test_is_entry_start():\n    # Dummy text\n    assert not LogEntry.is_entry_start((\"XXXX\"))\n\n    # Invalid line - timestamp missing microseconds\n    assert not LogEntry.is_entry_start(\"2022/11/24-15:58:04\")\n\n    # Invalid line - timestamp microseconds is cropped\n    assert not LogEntry.is_entry_start(\"2022/11/24-15:58:04.758\")\n\n    # Valid line\n    assert LogEntry.is_entry_start(\"2022/11/24-15:58:04.758352 32819 \")", "\n\ndef test_basic_single_line():\n    log_line1 = \"2022/11/24-15:58:04.758402 32819 DB SUMMARY\"\n    log_line2 = \"2022/11/24-15:58:05.068464 32819 [/version_set.cc:4965] \" \\\n                \"Recovered from manifest\"\n\n    entry = LogEntry(100, log_line1, True)\n    assert \"2022/11/24-15:58:04.758402\" == entry.get_time()\n    assert entry.get_start_line_idx() == 100\n    assert entry.get_lines_idxs_range() == (100, 101)\n    assert not entry.get_code_pos()\n    assert not entry.is_warn_msg()\n    assert entry.get_warning_type() is None\n    assert entry.have_all_lines_been_added()\n\n    with pytest.raises(utils.ParsingAssertion):\n        entry.add_line(log_line2, last_line=True)\n    with pytest.raises(utils.ParsingAssertion):\n        entry.add_line(log_line2, last_line=False)\n    with pytest.raises(utils.ParsingAssertion):\n        entry.all_lines_added()", "\n\ndef test_warn_single_line():\n    warn_msg = \"2022/04/17-15:24:51.089890 7f4a9fdff700 [WARN] \" \\\n               \"[/column_family.cc:932] Stalling writes, \" \\\n               \"L0 files 2, memtables 2\"\n\n    entry = LogEntry(100, warn_msg, True)\n    assert \"2022/04/17-15:24:51.089890\" == entry.get_time()\n    assert entry.get_code_pos() == \"/column_family.cc:932\"\n    assert entry.is_warn_msg()\n    assert entry.get_warning_type() == utils.WarningType.WARN", "\n\ndef test_multi_line_entry():\n    log_line1 = \"2022/11/24-15:58:04.758402 32819 DB SUMMARY\"\n    log_line2 = \"Continuation Line 1\"\n    log_line3 = \"Continuation Line 2\"\n    log_line4 = \"Continuation Line 2\"\n\n    log_line5 = \"2022/11/24-15:58:05.068464 32819 [/version_set.cc:4965] \" \\\n                \"Recovered from manifest\"\n\n    entry = LogEntry(100, log_line1, False)\n    assert \"2022/11/24-15:58:04.758402\" == entry.get_time()\n    assert not entry.have_all_lines_been_added()\n\n    entry.add_line(log_line2, last_line=False)\n    assert not entry.have_all_lines_been_added()\n    assert entry.get_lines_idxs_range() == (100, 102)\n\n    # Attempting to add the start of a new entry\n    with pytest.raises(utils.ParsingAssertion):\n        entry.add_line(log_line5, last_line=True)\n\n    assert not entry.have_all_lines_been_added()\n    assert entry.get_lines_idxs_range() == (100, 102)\n\n    entry.add_line(log_line3, last_line=False)\n    assert not entry.have_all_lines_been_added()\n    assert entry.get_lines_idxs_range() == (100, 103)\n\n    entry.all_lines_added()\n    assert entry.have_all_lines_been_added()\n\n    with pytest.raises(utils.ParsingAssertion):\n        entry.all_lines_added()\n\n    with pytest.raises(utils.ParsingAssertion):\n        entry.add_line(log_line4, last_line=True)\n    with pytest.raises(utils.ParsingAssertion):\n        entry.add_line(log_line4, last_line=False)", "\n\ndef test_invalid_entry_start():\n    with pytest.raises(utils.ParsingAssertion):\n        LogEntry(10, \"Not an entry start line\")\n\n    log_line = \"2022/11/24-15:58:04.758402\"\n    with pytest.raises(utils.ParsingError):\n        LogEntry(10, log_line)\n", ""]}
{"filename": "test/test_regexes.py", "chunked_list": ["import re\n\nimport regexes as r\n\n\ndef test_cf_name_regex():\n    assert re.search(r.CF_NAME, \"[CF1]\").group('cf') == 'CF1'\n    assert re.search(r.CF_NAME, \"[ C F 1 ]\").group('cf') == ' C F 1 '\n    assert re.search(r.CF_NAME, \"[]\").group('cf') == ''\n    assert re.search(r.CF_NAME, \"[...]\").group('cf') == '...'\n    assert re.search(r.CF_NAME, \"[\\0\\1]\").group('cf') == '\\0\\1'\n    assert re.search(r.CF_NAME, \"[!@#$%_-=]\").group('cf') == '!@#$%_-='\n    assert re.search(r.CF_NAME, \"CF1\") is None\n    assert re.search(r.CF_NAME, \"CF1]\") is None\n    assert re.search(r.CF_NAME, \"[CF1\") is None", "\n\ndef test_uptime_stats_line_regex():\n    line1 = \"Uptime(secs): 654.3 total, 789.1 interval\"\n    line2 = \" Uptime(secs):123.4 total, 56.7 interval  \"\n    line3 = \"Uptime(secs):123.4 total,\"\n    line4 = \"Uptime(secs):123.4 total, 78.9\"\n\n    m1 = re.search(r.UPTIME_STATS_LINE, line1)\n    assert m1\n    assert m1.group('total') == '654.3'\n    assert m1.group('interval') == '789.1'\n\n    m2 = re.search(r.UPTIME_STATS_LINE, line2)\n    assert m2\n    assert m2.group('total') == '123.4'\n    assert m2.group('interval') == '56.7'\n\n    assert not re.search(r.UPTIME_STATS_LINE, line3)\n    assert not re.search(r.UPTIME_STATS_LINE, line4)", "\n\ndef test_db_wide_interval_stall_regex():\n    line = 'Interval stall: 01:02:3.456 H:M:S, 7.8 percent'\n    m = re.search(r.DB_WIDE_INTERVAL_STALL, line)\n    assert m\n    assert m.groups() == ('01', '02', '3', '456', '7.8')\n\n\ndef test_db_wide_cumulative_stall_regex():\n    line = 'Cumulative stall: 01:02:3.456 H:M:S, 7.8 percent'\n    m = re.search(r.DB_WIDE_CUMULATIVE_STALL, line)\n    assert m\n    assert m.groups() == ('01', '02', '3', '456', '7.8')", "\ndef test_db_wide_cumulative_stall_regex():\n    line = 'Cumulative stall: 01:02:3.456 H:M:S, 7.8 percent'\n    m = re.search(r.DB_WIDE_CUMULATIVE_STALL, line)\n    assert m\n    assert m.groups() == ('01', '02', '3', '456', '7.8')\n\n\ndef test_db_wide_cumulative_writes():\n    line = 'Cumulative writes: 819K writes, 1821M keys, 788K commit groups, ' \\\n           '1.0 writes per commit group, ingest: 80.67 GB, 68.66 MB/s '\n    m = re.search(r.DB_WIDE_CUMULATIVE_WRITES, line)\n    assert m\n    assert m.groups() == ('819', 'K', '1821', 'M', '80.67', '68.66')", "def test_db_wide_cumulative_writes():\n    line = 'Cumulative writes: 819K writes, 1821M keys, 788K commit groups, ' \\\n           '1.0 writes per commit group, ingest: 80.67 GB, 68.66 MB/s '\n    m = re.search(r.DB_WIDE_CUMULATIVE_WRITES, line)\n    assert m\n    assert m.groups() == ('819', 'K', '1821', 'M', '80.67', '68.66')\n"]}
