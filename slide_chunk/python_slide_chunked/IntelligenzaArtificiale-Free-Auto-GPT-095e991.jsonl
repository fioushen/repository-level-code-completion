{"filename": "BABYAGI.py", "chunked_list": ["import os\nimport json\nfrom dotenv import load_dotenv\nfrom pathlib import Path\nfrom json import JSONDecodeError\nfrom collections import deque\nfrom typing import Dict, List, Optional, Any\nfrom langchain.vectorstores import FAISS\nfrom langchain import HuggingFaceHub\nfrom langchain.docstore import InMemoryDocstore", "from langchain import HuggingFaceHub\nfrom langchain.docstore import InMemoryDocstore\nfrom langchain import LLMChain, PromptTemplate\nfrom langchain.llms import BaseLLM\nfrom FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\nfrom FreeLLM import ChatGPTAPI  # FREE CHATGPT API\nfrom FreeLLM import BingChatAPI  # FREE BINGCHAT API\nfrom FreeLLM import BardChatAPI  # FREE GOOGLE BARD API\nfrom langchain.vectorstores.base import VectorStore\nfrom pydantic import BaseModel, Field", "from langchain.vectorstores.base import VectorStore\nfrom pydantic import BaseModel, Field\nfrom langchain.chains.base import Chain\nfrom langchain.experimental import BabyAGI\nfrom BabyAgi import BabyAGIMod\n\nimport faiss\n\nload_dotenv()\n", "load_dotenv()\n\nselect_model = input(\n    \"Select the model you want to use (1, 2, 3 or 4) \\n \\\n1) ChatGPT \\n \\\n2) HuggingChat \\n \\\n3) BingChat (NOT GOOD RESULT)\\n \\\n4) BardChat \\n \\\n>>> \"\n)", ">>> \"\n)\n\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt-4\"\n    else:\n        model = \"default\"\n\n    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\n            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n        )\n    cookie_path = Path() / \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", \"r\") as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\n                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n            )\n    llm = BingChatAPI.BingChat(\n        cookiepath=str(cookie_path), conversation_style=\"creative\"\n    )\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\n            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n        )\n    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n    llm = BardChatAPI.BardChat(cookie=cookie_path)", "\n\n    \n\nHF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n\nif HF_TOKEN != \"your-huggingface-token\":\n    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\nelse:\n    raise ValueError(\n        \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n    )", "\n\nfrom Embedding import HuggingFaceEmbedding  # EMBEDDING FUNCTION\n\n# Define your embedding model\nembeddings_model = HuggingFaceEmbedding.newEmbeddingFunction\n\nembedding_size = 1536\nindex = faiss.IndexFlatL2(embedding_size)\nvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})", "index = faiss.IndexFlatL2(embedding_size)\nvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n\nprint(vectorstore)\n\n# DEFINE TOOL\nfrom langchain.agents import ZeroShotAgent, Tool, AgentExecutor\nfrom langchain import OpenAI, LLMChain\nfrom langchain.tools import BaseTool, DuckDuckGoSearchRun\n", "from langchain.tools import BaseTool, DuckDuckGoSearchRun\n\n\ntodo_prompt = PromptTemplate.from_template(\n    \"I need to create a plan for complete me GOAl. Can you help me to create a TODO list? Create only the todo list for this objective: '{objective}'.\"\n)\ntodo_chain = LLMChain(llm=llm, prompt=todo_prompt)\nsearch = DuckDuckGoSearchRun()\ntools = [\n    Tool(", "tools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events\",\n    ),\n    Tool(\n        name=\"TODO\",\n        func=todo_chain.run,\n        description=\"useful for when you need to create a task list to complete a objective. You have to give an Input: a objective for which to create a to-do list. Output: just a list of tasks to do for that objective. It is important to give the target input 'objective' correctly!\",", "        func=todo_chain.run,\n        description=\"useful for when you need to create a task list to complete a objective. You have to give an Input: a objective for which to create a to-do list. Output: just a list of tasks to do for that objective. It is important to give the target input 'objective' correctly!\",\n    ),\n]\n\n\nprefix = \"\"\"Can you help me to performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.\"\"\"\nsuffix = \"\"\"Question: {task}. \n{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(", "{agent_scratchpad}\"\"\"\nprompt = ZeroShotAgent.create_prompt(\n    tools,\n    prefix=prefix,\n    suffix=suffix,\n    input_variables=[\"objective\", \"task\", \"context\", \"agent_scratchpad\"],\n)\n\nllm_chain = LLMChain(llm=llm, prompt=prompt)\ntool_names = [tool.name for tool in tools]", "llm_chain = LLMChain(llm=llm, prompt=prompt)\ntool_names = [tool.name for tool in tools]\nagent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent, tools=tools, verbose=True\n)\n\n# START\n\n# Logging of LLMChains", "\n# Logging of LLMChains\nverbose = False\n\nint_max_iterations = input(\n    \"Enter the maximum number of iterations: (Suggest from 3 and 5) \"\n)\nmax_iterations = int(int_max_iterations)\n\nif input(\"Do you want to store the results? (y/n) \") == \"y\":\n    store_results = True\nelse:\n    store_results = False", "\nif input(\"Do you want to store the results? (y/n) \") == \"y\":\n    store_results = True\nelse:\n    store_results = False\n\n\n# If None, will keep on going forever\nmax_iterations: Optional[int] = max_iterations\nbaby_agi = BabyAGIMod.BabyAGI.from_llm(", "max_iterations: Optional[int] = max_iterations\nbaby_agi = BabyAGIMod.BabyAGI.from_llm(\n    llm=llm,\n    vectorstore=vectorstore,\n    task_execution_chain=agent_executor,\n    verbose=verbose,\n    max_iterations=max_iterations,\n    store=store_results,\n)\n", ")\n\n\n# DEFINE THE OBJECTIVE - MODIFY THIS\nOBJECTIVE = input(\"Enter the objective of the AI system: (Be realistic!) \")\n\n\nbaby_agi({\"objective\": OBJECTIVE})\n", ""]}
{"filename": "MetaPrompt.py", "chunked_list": ["import json\nfrom dotenv import load_dotenv\nfrom pathlib import Path\nfrom json import JSONDecodeError\nfrom langchain import LLMChain, PromptTemplate\nfrom FreeLLM import ChatGPTAPI  # FREE CHATGPT API\nfrom FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\nfrom FreeLLM import BingChatAPI  # FREE BINGCHAT API\nfrom FreeLLM import BardChatAPI  # FREE GOOGLE BARD API\n", "from FreeLLM import BardChatAPI  # FREE GOOGLE BARD API\n\nfrom langchain.memory import ConversationBufferWindowMemory\nimport os\n\nload_dotenv()\n\n#### LOG IN FOR CHATGPT FREE LLM\nselect_model = input(\n    \"Select the model you want to use (1, 2, 3 or 4) \\n \\", "select_model = input(\n    \"Select the model you want to use (1, 2, 3 or 4) \\n \\\n1) ChatGPT \\n \\\n2) HuggingChat \\n \\\n3) BingChat \\n \\\n4) Google Bard \\n \\\n>>> \"\n)\n\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt4\"\n    else:\n        model = \"default\"\n\n    if start_chat:\n        chat_id = os.getenv(\"CHAT_ID\")\n        if chat_id == None:\n            raise ValueError(\"You have to set up your chat-id in the .env file\")\n        llm = ChatGPTAPI.ChatGPT(\n            token=os.environ[\"CHATGPT_TOKEN\"], conversation=chat_id, model=model\n        )\n    else:\n        llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\n            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n        )\n    cookie_path = Path() / \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", \"r\") as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\n                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n            )\n    llm = BingChatAPI.BingChat(\n        cookiepath=str(cookie_path), conversation_style=\"creative\"\n    )\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\n            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n        )\n    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n    llm = BardChatAPI.BardChat(cookie=cookie_path)", "\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt4\"\n    else:\n        model = \"default\"\n\n    if start_chat:\n        chat_id = os.getenv(\"CHAT_ID\")\n        if chat_id == None:\n            raise ValueError(\"You have to set up your chat-id in the .env file\")\n        llm = ChatGPTAPI.ChatGPT(\n            token=os.environ[\"CHATGPT_TOKEN\"], conversation=chat_id, model=model\n        )\n    else:\n        llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\n            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n        )\n    cookie_path = Path() / \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", \"r\") as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\n                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n            )\n    llm = BingChatAPI.BingChat(\n        cookiepath=str(cookie_path), conversation_style=\"creative\"\n    )\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\n            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n        )\n    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n    llm = BardChatAPI.BardChat(cookie=cookie_path)", "\n\n####\n\n\ndef initialize_chain(instructions, memory=None):\n    if memory is None:\n        memory = ConversationBufferWindowMemory()\n        memory.ai_prefix = \"Assistant\"\n\n    template = f\"\"\"\n    Instructions: {instructions}\n    {{{memory.memory_key}}}\n    Human: {{human_input}}\n    Assistant:\"\"\"\n\n    prompt = PromptTemplate(\n        input_variables=[\"history\", \"human_input\"], template=template\n    )\n\n    chain = LLMChain(\n        llm=llm,\n        prompt=prompt,\n        verbose=True,\n        memory=ConversationBufferWindowMemory(),\n    )\n    return chain", "\n\ndef initialize_meta_chain():\n    meta_template = \"\"\"\n    Assistant has just had the below interactions with a User. Assistant followed their \"Instructions\" closely. Your job is to critique the Assistant's performance and then revise the Instructions so that Assistant would quickly and correctly respond in the future.\n\n    ####\n\n    {chat_history}\n\n    ####\n\n    Please reflect on these interactions.\n\n    You should first critique Assistant's performance. What could Assistant have done better? What should the Assistant remember about this user? Are there things this user always wants? Indicate this with \"Critique: ...\".\n\n    You should next revise the Instructions so that Assistant would quickly and correctly respond in the future. Assistant's goal is to satisfy the user in as few interactions as possible. Assistant will only see the new Instructions, not the interaction history, so anything important must be summarized in the Instructions. Don't forget any important details in the current Instructions! Indicate the new Instructions by \"Instructions: ...\".\n    \"\"\"\n\n    meta_prompt = PromptTemplate(\n        input_variables=[\"chat_history\"], template=meta_template\n    )\n\n    meta_chain = LLMChain(\n        llm=llm,\n        prompt=meta_prompt,\n        verbose=True,\n    )\n    return meta_chain", "\n\ndef get_chat_history(chain_memory):\n    memory_key = chain_memory.memory_key\n    chat_history = chain_memory.load_memory_variables(memory_key)[memory_key]\n    return chat_history\n\n\ndef get_new_instructions(meta_output):\n    delimiter = \"Instructions: \"\n    new_instructions = meta_output[meta_output.find(delimiter) + len(delimiter) :]\n    return new_instructions", "def get_new_instructions(meta_output):\n    delimiter = \"Instructions: \"\n    new_instructions = meta_output[meta_output.find(delimiter) + len(delimiter) :]\n    return new_instructions\n\n\ndef main(task, max_iters=3, max_meta_iters=5):\n    failed_phrase = \"task failed\"\n    success_phrase = \"task succeeded\"\n    key_phrases = [success_phrase, failed_phrase]\n\n    instructions = \"None\"\n    for i in range(max_meta_iters):\n        print(f\"[Episode {i+1}/{max_meta_iters}]\")\n        chain = initialize_chain(instructions, memory=None)\n        output = chain.predict(human_input=task)\n        for j in range(max_iters):\n            print(f\"(Step {j+1}/{max_iters})\")\n            print(f\"Assistant: {output}\")\n            print(f\"Human: \")\n            human_input = input()\n            if any(phrase in human_input.lower() for phrase in key_phrases):\n                break\n            output = chain.predict(human_input=human_input)\n        if success_phrase in human_input.lower():\n            print(f\"You succeeded! Thanks for playing!\")\n            return\n        meta_chain = initialize_meta_chain()\n        meta_output = meta_chain.predict(chat_history=get_chat_history(chain.memory))\n        print(f\"Feedback: {meta_output}\")\n        instructions = get_new_instructions(meta_output)\n        print(f\"New Instructions: {instructions}\")\n        print(\"\\n\" + \"#\" * 80 + \"\\n\")\n    print(f\"You failed! Thanks for playing!\")", "\n\ntask = input(\"Enter the objective of the AI system: (Be realistic!) \")\nmax_iters = int(input(\"Enter the maximum number of interactions per episode: \"))\nmax_meta_iters = int(input(\"Enter the maximum number of episodes: \"))\nmain(task, max_iters, max_meta_iters)\n"]}
{"filename": "AUTOGPT.py", "chunked_list": ["# !pip install bs4\n# !pip install nest_asyncio\n\n# General\nimport os\nimport json\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom pathlib import Path\nfrom json import JSONDecodeError", "from pathlib import Path\nfrom json import JSONDecodeError\nfrom langchain.experimental.autonomous_agents.autogpt.agent import AutoGPT\nfrom FreeLLM import ChatGPTAPI  # FREE CHATGPT API\nfrom FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\nfrom FreeLLM import BingChatAPI  # FREE BINGCHAT API\nfrom FreeLLM import BardChatAPI  # FREE GOOGLE BARD API\nfrom langchain.agents.agent_toolkits.pandas.base import create_pandas_dataframe_agent\nfrom langchain.docstore.document import Document\nimport asyncio", "from langchain.docstore.document import Document\nimport asyncio\nimport nest_asyncio\n\n\n# Needed synce jupyter runs an async eventloop\nnest_asyncio.apply()\n# [Optional] Set the environment variable Tokenizers_PARALLELISM to false to get rid of the warning\n# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n", "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nload_dotenv()\nselect_model = input(\n    \"Select the model you want to use (1, 2, 3 or 4) \\n \\\n1) ChatGPT \\n \\\n2) HuggingChat \\n \\\n3) BingChat \\n \\\n4) Google Bard \\n \\\n>>> \"", "4) Google Bard \\n \\\n>>> \"\n)\n\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt-4\"\n    else:\n        model = \"default\"\n\n    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\n            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n        )\n    cookie_path = Path() / \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", \"r\") as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\n                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n            )\n    llm = BingChatAPI.BingChat(\n        cookiepath=str(cookie_path), conversation_style=\"creative\"\n    )\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\n            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n        )\n    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n    llm = BardChatAPI.BardChat(cookie=cookie_path)", "\n\nHF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n\nif HF_TOKEN != \"your-huggingface-token\":\n    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\nelse:\n    raise ValueError(\n        \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n    )", "\n# Tools\nimport os\nfrom contextlib import contextmanager\nfrom typing import Optional\nfrom langchain.agents import tool\nfrom langchain.tools.file_management.read import ReadFileTool\nfrom langchain.tools.file_management.write import WriteFileTool\nfrom tempfile import TemporaryDirectory\n", "from tempfile import TemporaryDirectory\n\nROOT_DIR = TemporaryDirectory()\n\n\n@contextmanager\ndef pushd(new_dir):\n    \"\"\"Context manager for changing the current working directory.\"\"\"\n    prev_dir = os.getcwd()\n    os.chdir(new_dir)\n    try:\n        yield\n    finally:\n        os.chdir(prev_dir)", "\n\n@tool\ndef process_csv(\n    csv_file_path: str, instructions: str, output_path: Optional[str] = None\n) -> str:\n    \"\"\"Process a CSV by with pandas in a limited REPL.\\\n Only use this after writing data to disk as a csv file.\\\n Any figures must be saved to disk to be viewed by the human.\\\n Instructions should be written in natural language, not code. Assume the dataframe is already loaded.\"\"\"\n    with pushd(ROOT_DIR):\n        try:\n            df = pd.read_csv(csv_file_path)\n        except Exception as e:\n            return f\"Error: {e}\"\n        agent = create_pandas_dataframe_agent(llm, df, max_iterations=30, verbose=True)\n        if output_path is not None:\n            instructions += f\" Save output to disk at {output_path}\"\n        try:\n            result = agent.run(instructions)\n            return result\n        except Exception as e:\n            return f\"Error: {e}\"", "\n\n# !pip install playwright\n# !playwright install\nasync def async_load_playwright(url: str) -> str:\n    \"\"\"Load the specified URLs using Playwright and parse using BeautifulSoup.\"\"\"\n    from bs4 import BeautifulSoup\n    from playwright.async_api import async_playwright\n\n    try:\n        print(\">>> WARNING <<<\")\n        print(\n            \"If you are running this for the first time, you nedd to install playwright\"\n        )\n        print(\">>> AUTO INSTALLING PLAYWRIGHT <<<\")\n        os.system(\"playwright install\")\n        print(\">>> PLAYWRIGHT INSTALLED <<<\")\n    except:\n        print(\">>> PLAYWRIGHT ALREADY INSTALLED <<<\")\n        pass", "\n    try:\n        print(\">>> WARNING <<<\")\n        print(\n            \"If you are running this for the first time, you nedd to install playwright\"\n        )\n        print(\">>> AUTO INSTALLING PLAYWRIGHT <<<\")\n        os.system(\"playwright install\")\n        print(\">>> PLAYWRIGHT INSTALLED <<<\")\n    except:\n        print(\">>> PLAYWRIGHT ALREADY INSTALLED <<<\")\n        pass", "    results = \"\"\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        try:\n            page = await browser.new_page()\n            await page.goto(url)\n\n            page_source = await page.content()\n            soup = BeautifulSoup(page_source, \"html.parser\")\n\n            for script in soup([\"script\", \"style\"]):\n                script.extract()\n\n            text = soup.get_text()\n            lines = (line.strip() for line in text.splitlines())\n            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n            results = \"\\n\".join(chunk for chunk in chunks if chunk)\n        except Exception as e:\n            results = f\"Error: {e}\"", "        await browser.close()\n    return results\n\n\ndef run_async(coro):\n    event_loop = asyncio.get_event_loop()\n    return event_loop.run_until_complete(coro)\n\n\n@tool\ndef browse_web_page(url: str) -> str:\n    \"\"\"Verbose way to scrape a whole webpage. Likely to cause issues parsing.\"\"\"\n    return run_async(async_load_playwright(url))", "\n@tool\ndef browse_web_page(url: str) -> str:\n    \"\"\"Verbose way to scrape a whole webpage. Likely to cause issues parsing.\"\"\"\n    return run_async(async_load_playwright(url))\n\n\nfrom langchain.tools import BaseTool, DuckDuckGoSearchRun\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n\nfrom pydantic import Field\nfrom langchain.chains.qa_with_sources.loading import (\n    load_qa_with_sources_chain,\n    BaseCombineDocumentsChain,\n)\n\n\ndef _get_text_splitter():\n    return RecursiveCharacterTextSplitter(\n        # Set a really small chunk size, just to show.\n        chunk_size=500,\n        chunk_overlap=20,\n        length_function=len,\n    )", "\ndef _get_text_splitter():\n    return RecursiveCharacterTextSplitter(\n        # Set a really small chunk size, just to show.\n        chunk_size=500,\n        chunk_overlap=20,\n        length_function=len,\n    )\n\n\nclass WebpageQATool(BaseTool):\n    name = \"query_webpage\"\n    description = (\n        \"Browse a webpage and retrieve the information relevant to the question.\"\n    )\n    text_splitter: RecursiveCharacterTextSplitter = Field(\n        default_factory=_get_text_splitter\n    )\n    qa_chain: BaseCombineDocumentsChain\n\n    def _run(self, url: str, question: str) -> str:\n        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n        result = browse_web_page.run(url)\n        docs = [Document(page_content=result, metadata={\"source\": url})]\n        web_docs = self.text_splitter.split_documents(docs)\n        results = []\n        # TODO: Handle this with a MapReduceChain\n        for i in range(0, len(web_docs), 4):\n            input_docs = web_docs[i : i + 4]\n            window_result = self.qa_chain(\n                {\"input_documents\": input_docs, \"question\": question},\n                return_only_outputs=True,\n            )\n            results.append(f\"Response from window {i} - {window_result}\")\n        results_docs = [\n            Document(page_content=\"\\n\".join(results), metadata={\"source\": url})\n        ]\n        return self.qa_chain(\n            {\"input_documents\": results_docs, \"question\": question},\n            return_only_outputs=True,\n        )\n\n    async def _arun(self, url: str, question: str) -> str:\n        raise NotImplementedError", "\n\nclass WebpageQATool(BaseTool):\n    name = \"query_webpage\"\n    description = (\n        \"Browse a webpage and retrieve the information relevant to the question.\"\n    )\n    text_splitter: RecursiveCharacterTextSplitter = Field(\n        default_factory=_get_text_splitter\n    )\n    qa_chain: BaseCombineDocumentsChain\n\n    def _run(self, url: str, question: str) -> str:\n        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n        result = browse_web_page.run(url)\n        docs = [Document(page_content=result, metadata={\"source\": url})]\n        web_docs = self.text_splitter.split_documents(docs)\n        results = []\n        # TODO: Handle this with a MapReduceChain\n        for i in range(0, len(web_docs), 4):\n            input_docs = web_docs[i : i + 4]\n            window_result = self.qa_chain(\n                {\"input_documents\": input_docs, \"question\": question},\n                return_only_outputs=True,\n            )\n            results.append(f\"Response from window {i} - {window_result}\")\n        results_docs = [\n            Document(page_content=\"\\n\".join(results), metadata={\"source\": url})\n        ]\n        return self.qa_chain(\n            {\"input_documents\": results_docs, \"question\": question},\n            return_only_outputs=True,\n        )\n\n    async def _arun(self, url: str, question: str) -> str:\n        raise NotImplementedError", "\n\nquery_website_tool = WebpageQATool(qa_chain=load_qa_with_sources_chain(llm))\n\n\n# Memory\nimport faiss\nfrom langchain.vectorstores import FAISS\nfrom langchain.docstore import InMemoryDocstore\nfrom Embedding import HuggingFaceEmbedding  # EMBEDDING FUNCTION", "from langchain.docstore import InMemoryDocstore\nfrom Embedding import HuggingFaceEmbedding  # EMBEDDING FUNCTION\n\nfrom langchain.tools.human.tool import HumanInputRun\n\n# Define your embedding model\nembeddings_model = HuggingFaceEmbedding.newEmbeddingFunction\nembedding_size = 1536  # if you change this you need to change also in Embedding/HuggingFaceEmbedding.py\nindex = faiss.IndexFlatL2(embedding_size)\nvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})", "index = faiss.IndexFlatL2(embedding_size)\nvectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n\n\n# !pip install duckduckgo_search\nweb_search = DuckDuckGoSearchRun()\n\ntools = [\n    web_search,\n    WriteFileTool(),", "    web_search,\n    WriteFileTool(),\n    ReadFileTool(),\n    process_csv,\n    query_website_tool,\n    # HumanInputRun(), # Activate if you want the permit asking for help from the human\n]\n\n\nagent = AutoGPT.from_llm_and_tools(", "\nagent = AutoGPT.from_llm_and_tools(\n    ai_name=\"BingChat\",\n    ai_role=\"Assistant\",\n    tools=tools,\n    llm=llm,\n    memory=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n    # human_in_the_loop=True, # Set to True if you want to add feedback at each step.\n)\n# agent.chain.verbose = True", ")\n# agent.chain.verbose = True\n\nagent.run([input(\"Enter the objective of the AI system: (Be realistic!) \")])\n"]}
{"filename": "TransformersAgent.py", "chunked_list": ["from hfAgent import agents\nfrom dotenv import load_dotenv\n\nimport os\nimport json\n\nfrom json import JSONDecodeError\nfrom pathlib import Path\n\nimport huggingface_hub", "\nimport huggingface_hub\n\nload_dotenv()\n\nselect_model = input(\n    \"Select the model you want to use (1, 2, 3, 4, 5, 6) \\n \\\n1) ChatGPT \\n \\\n2) HuggingChat (NOT GOOD RESULT)\\n \\\n3) BingChat (NOT GOOD RESULT)\\n \\", "2) HuggingChat (NOT GOOD RESULT)\\n \\\n3) BingChat (NOT GOOD RESULT)\\n \\\n4) BardChat \\n \\\n5) StarCoder\\n \\\n6) OpenAssistant\\n \\\n>>> \"\n)\n\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt-4\"\n    else:\n        model = \"default\"\n\n    agent = agents.ChatGPTAgent(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\n\nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    agent = agents.HuggingChatAgent()\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\n            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n        )\n    cookie_path = \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", \"r\") as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\n                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n            )\n    agent = agents.BingChatAgent(cookiepath=cookie_path, conversation=\"balanced\")\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\n            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n        )\n    cookie = os.environ[\"BARDCHAT_TOKEN\"]\n    agent = agents.BardChatAgent(token=cookie)\nelif select_model == \"5\":\n    HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n    if HF_TOKEN != \"your-huggingface-token\":\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n        huggingface_hub.login(token=HF_TOKEN)\n    else:\n        raise ValueError(\n            \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n        )\n\n    from transformers.tools import HfAgent\n\n    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\nelif select_model == \"6\":\n    HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n    if HF_TOKEN != \"your-huggingface-token\":\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n        huggingface_hub.login(token=HF_TOKEN)\n    else:\n        raise ValueError(\n            \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n        )\n\n    from transformers.tools import HfAgent\n\n    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\n    from transformers.tools import HfAgent\n\n    agent = HfAgent(\n        url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n    )", "if select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt-4\"\n    else:\n        model = \"default\"\n\n    agent = agents.ChatGPTAgent(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\n\nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    agent = agents.HuggingChatAgent()\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\n            \"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\"\n        )\n    cookie_path = \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", \"r\") as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\n                \"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\"\n            )\n    agent = agents.BingChatAgent(cookiepath=cookie_path, conversation=\"balanced\")\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n\n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\n            \"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\"\n        )\n    cookie = os.environ[\"BARDCHAT_TOKEN\"]\n    agent = agents.BardChatAgent(token=cookie)\nelif select_model == \"5\":\n    HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n    if HF_TOKEN != \"your-huggingface-token\":\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n        huggingface_hub.login(token=HF_TOKEN)\n    else:\n        raise ValueError(\n            \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n        )\n\n    from transformers.tools import HfAgent\n\n    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\nelif select_model == \"6\":\n    HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"your-huggingface-token\")\n    if HF_TOKEN != \"your-huggingface-token\":\n        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n        huggingface_hub.login(token=HF_TOKEN)\n    else:\n        raise ValueError(\n            \"HuggingFace Token EMPTY. Edit the .env file and put your HuggingFace token\"\n        )\n\n    from transformers.tools import HfAgent\n\n    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n\n    from transformers.tools import HfAgent\n\n    agent = HfAgent(\n        url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\n    )", "\n\nprompt = input(\">>> Input prompt:\\n>\")\nwhile prompt != \"exit\":\n    agent.run(prompt, remote=True)\n    prompt = input(\">>> Input prompt:\\n>\")\n"]}
{"filename": "Camel.py", "chunked_list": ["from langchain.prompts.chat import (\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    BaseMessage,\n)", "    BaseMessage,\n)\nimport os\nimport json\nfrom pathlib import Path\nfrom json import JSONDecodeError\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nfrom FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\nfrom FreeLLM import ChatGPTAPI  # FREE CHATGPT API", "from FreeLLM import HuggingChatAPI  # FREE HUGGINGCHAT API\nfrom FreeLLM import ChatGPTAPI  # FREE CHATGPT API\nfrom FreeLLM import BingChatAPI  # FREE BINGCHAT API\nimport streamlit as st\nfrom streamlit_chat_media import message\nimport os\n\nst.set_page_config(\n    page_title=\"FREE AUTOGPT \ud83d\ude80 by Intelligenza Artificiale Italia\",\n    page_icon=\"\ud83d\ude80\",", "    page_title=\"FREE AUTOGPT \ud83d\ude80 by Intelligenza Artificiale Italia\",\n    page_icon=\"\ud83d\ude80\",\n    layout=\"wide\",\n    menu_items={\n        \"Get help\": \"https://www.intelligenzaartificialeitalia.net/\",\n        \"Report a bug\": \"mailto:servizi@intelligenzaartificialeitalia.net\",\n        \"About\": \"# *\ud83d\ude80  FREE AUTOGPT  \ud83d\ude80* \",\n    },\n)\n", ")\n\n\nst.markdown(\n    \"<style> iframe > div {    text-align: left;} </style>\", unsafe_allow_html=True\n)\n\n\nclass CAMELAgent:\n    def __init__(\n        self,\n        system_message: SystemMessage,\n        model: None,\n    ) -> None:\n        self.system_message = system_message.content\n        self.model = model\n        self.init_messages()\n\n    def reset(self) -> None:\n        self.init_messages()\n        return self.stored_messages\n\n    def init_messages(self) -> None:\n        self.stored_messages = [self.system_message]\n\n    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\n        self.stored_messages.append(message)\n        return self.stored_messages\n\n    def step(\n        self,\n        input_message: HumanMessage,\n    ) -> AIMessage:\n        messages = self.update_messages(input_message)\n        output_message = self.model(str(input_message.content))\n        self.update_messages(output_message)\n        print(f\"AI Assistant:\\n\\n{output_message}\\n\\n\")\n        return output_message", "class CAMELAgent:\n    def __init__(\n        self,\n        system_message: SystemMessage,\n        model: None,\n    ) -> None:\n        self.system_message = system_message.content\n        self.model = model\n        self.init_messages()\n\n    def reset(self) -> None:\n        self.init_messages()\n        return self.stored_messages\n\n    def init_messages(self) -> None:\n        self.stored_messages = [self.system_message]\n\n    def update_messages(self, message: BaseMessage) -> List[BaseMessage]:\n        self.stored_messages.append(message)\n        return self.stored_messages\n\n    def step(\n        self,\n        input_message: HumanMessage,\n    ) -> AIMessage:\n        messages = self.update_messages(input_message)\n        output_message = self.model(str(input_message.content))\n        self.update_messages(output_message)\n        print(f\"AI Assistant:\\n\\n{output_message}\\n\\n\")\n        return output_message", "\n\ncol1, col2 = st.columns(2)\nassistant_role_name = col1.text_input(\"Assistant Role Name\", \"Python Programmer\")\nuser_role_name = col2.text_input(\"User Role Name\", \"Stock Trader\")\ntask = st.text_area(\"Task\", \"Develop a trading bot for the stock market\")\nword_limit = st.number_input(\"Word Limit\", 10, 1500, 50)\n\nif not os.path.exists(\"cookiesHuggingChat.json\"):\n    raise ValueError(\n        \"File 'cookiesHuggingChat.json' not found! Create it and put your cookies in there in the JSON format.\"\n    )", "if not os.path.exists(\"cookiesHuggingChat.json\"):\n    raise ValueError(\n        \"File 'cookiesHuggingChat.json' not found! Create it and put your cookies in there in the JSON format.\"\n    )\ncookie_path = Path() / \"cookiesHuggingChat.json\"\nwith open(\"cookiesHuggingChat.json\", \"r\") as file:\n    try:\n        file_json = json.loads(file.read())\n    except JSONDecodeError:\n        raise ValueError(\n            \"You did not put your cookies inside 'cookiesHuggingChat.json'! You can find the simple guide to get the cookie file here: https://github.com/IntelligenzaArtificiale/Free-Auto-GPT\"\n        )  ", "llm = HuggingChatAPI.HuggingChat(cookiepath = str(cookie_path))\n\n\nif st.button(\"Start Autonomus AI AGENT\"):\n    task_specifier_sys_msg = SystemMessage(content=\"You can make a task more specific.\")\n    task_specifier_prompt = \"\"\"Here is a task that {assistant_role_name} will help {user_role_name} to complete: {task}.\n    Please make it more specific. Be creative and imaginative.\n    Please reply with the specified task in {word_limit} words or less. Do not add anything else.\"\"\"\n    task_specifier_template = HumanMessagePromptTemplate.from_template(\n        template=task_specifier_prompt\n    )\n\n    task_specify_agent = CAMELAgent(\n        task_specifier_sys_msg, llm\n    )\n    task_specifier_msg = task_specifier_template.format_messages(\n        assistant_role_name=assistant_role_name,\n        user_role_name=user_role_name,\n        task=task,\n        word_limit=word_limit,\n    )[0]\n\n    specified_task_msg = task_specify_agent.step(task_specifier_msg)\n\n    print(f\"Specified task: {specified_task_msg}\")\n    message(\n        f\"Specified task: {specified_task_msg}\",\n        allow_html=True,\n        key=\"specified_task\",\n        avatar_style=\"adventurer\",\n    )\n\n    specified_task = specified_task_msg\n\n    # messages.py\n    from langchain.prompts.chat import (\n        SystemMessagePromptTemplate,\n        HumanMessagePromptTemplate,\n    )\n\n    assistant_inception_prompt = \"\"\"Never forget you are a {assistant_role_name} and I am a {user_role_name}. Never flip roles! Never instruct me!\n    We share a common interest in collaborating to successfully complete a task.\n    You must help me to complete the task.\n    Here is the task: {task}. Never forget our task and to focus only to complete the task do not add anything else!\n    I must instruct you based on your expertise and my needs to complete the task.\n\n    I must give you one instruction at a time.\n    It is important that when the . \"{task}\" is completed, you need to tell {user_role_name} that the task has completed and to stop!\n    You must write a specific solution that appropriately completes the requested instruction.\n    Do not add anything else other than your solution to my instruction.\n    You are never supposed to ask me any questions you only answer questions.\n    REMEMBER NEVER INSTRUCT ME! \n    Your solution must be declarative sentences and simple present tense.\n    Unless I say the task is completed, you should always start with:\n\n    Solution: <YOUR_SOLUTION>\n\n    <YOUR_SOLUTION> should be specific and provide preferable implementations and examples for task-solving.\n    Always end <YOUR_SOLUTION> with: Next request.\"\"\"\n\n    user_inception_prompt = \"\"\"Never forget you are a {user_role_name} and I am a {assistant_role_name}. Never flip roles! You will always instruct me.\n    We share a common interest in collaborating to successfully complete a task.\n    I must help you to complete the task.\n    Here is the task: {task}. Never forget our task!\n    You must instruct me based on my expertise and your needs to complete the task ONLY in the following two ways:\n\n    1. Instruct with a necessary input:\n    Instruction: <YOUR_INSTRUCTION>\n    Input: <YOUR_INPUT>\n\n    2. Instruct without any input:\n    Instruction: <YOUR_INSTRUCTION>\n    Input: None\n\n    The \"Instruction\" describes a task or question. The paired \"Input\" provides further context or information for the requested \"Instruction\".\n\n    You must give me one instruction at a time.\n    I must write a response that appropriately completes the requested instruction.\n    I must decline your instruction honestly if I cannot perform the instruction due to physical, moral, legal reasons or my capability and explain the reasons.\n    You should instruct me not ask me questions.\n    Now you must start to instruct me using the two ways described above.\n    Do not add anything else other than your instruction and the optional corresponding input!\n    Keep giving me instructions and necessary inputs until you think the task is completed.\n    It's Important wich when the task . \"{task}\" is completed, you must only reply with a single word <CAMEL_TASK_DONE>.\n    Never say <CAMEL_TASK_DONE> unless my responses have solved your task!\n    It's Important wich when the task . \"{task}\" is completed, you must only reply with a single word <CAMEL_TASK_DONE>\"\"\"\n\n    def get_sys_msgs(assistant_role_name: str, user_role_name: str, task: str):\n        assistant_sys_template = SystemMessagePromptTemplate.from_template(\n            template=assistant_inception_prompt\n        )\n        assistant_sys_msg = assistant_sys_template.format_messages(\n            assistant_role_name=assistant_role_name,\n            user_role_name=user_role_name,\n            task=task,\n        )[0]\n\n        user_sys_template = SystemMessagePromptTemplate.from_template(\n            template=user_inception_prompt\n        )\n        user_sys_msg = user_sys_template.format_messages(\n            assistant_role_name=assistant_role_name,\n            user_role_name=user_role_name,\n            task=task,\n        )[0]\n\n        return assistant_sys_msg, user_sys_msg\n\n    # define the role system messages\n    assistant_sys_msg, user_sys_msg = get_sys_msgs(\n        assistant_role_name, user_role_name, specified_task\n    )\n\n    # AI ASSISTANT setup                           |-> add the agent LLM MODEL HERE <-|\n    assistant_agent = CAMELAgent(assistant_sys_msg, llm)\n\n    # AI USER setup                      |-> add the agent LLM MODEL HERE <-|\n    user_agent = CAMELAgent(user_sys_msg, llm)\n\n    # Reset agents\n    assistant_agent.reset()\n    user_agent.reset()\n\n    # Initialize chats\n    assistant_msg = HumanMessage(\n        content=(\n            f\"{user_sys_msg}. \"\n            \"Now start to give me introductions one by one. \"\n            \"Only reply with Instruction and Input.\"\n        )\n    )\n\n    user_msg = HumanMessage(content=f\"{assistant_sys_msg.content}\")\n    user_msg = assistant_agent.step(user_msg)\n    message(\n        f\"AI Assistant ({assistant_role_name}):\\n\\n{user_msg}\\n\\n\",\n        is_user=False,\n        allow_html=True,\n        key=\"0_assistant\",\n        avatar_style=\"pixel-art\",\n    )\n    print(f\"Original task prompt:\\n{task}\\n\")\n    print(f\"Specified task prompt:\\n{specified_task}\\n\")\n\n    chat_turn_limit, n = 30, 0\n    while n < chat_turn_limit:\n        n += 1\n        user_ai_msg = user_agent.step(assistant_msg)\n        user_msg = HumanMessage(content=user_ai_msg)\n        # print(f\"AI User ({user_role_name}):\\n\\n{user_msg}\\n\\n\")\n        message(\n            f\"AI User ({user_role_name}):\\n\\n{user_msg.content}\\n\\n\",\n            is_user=True,\n            allow_html=True,\n            key=str(n) + \"_user\",\n        )\n\n        assistant_ai_msg = assistant_agent.step(user_msg)\n        assistant_msg = HumanMessage(content=assistant_ai_msg)\n        # print(f\"AI Assistant ({assistant_role_name}):\\n\\n{assistant_msg}\\n\\n\")\n        message(\n            f\"AI Assistant ({assistant_role_name}):\\n\\n{assistant_msg.content}\\n\\n\",\n            is_user=False,\n            allow_html=True,\n            key=str(n) + \"_assistant\",\n            avatar_style=\"pixel-art\",\n        )\n        if (\n            \"<CAMEL_TASK_DONE>\" in user_msg.content\n            or \"task  completed\" in user_msg.content\n        ):\n            message(\"Task completed!\", allow_html=True, key=\"task_done\")\n            break\n        if \"Error\" in user_msg.content:\n            message(\"Task failed!\", allow_html=True, key=\"task_failed\")\n            break", ""]}
{"filename": "BabyAgi/task_creation.py", "chunked_list": ["from langchain import LLMChain, PromptTemplate\nfrom langchain.base_language import BaseLanguageModel\n\nclass TaskCreationChain(LLMChain):\n    \"\"\"Chain to generates tasks.\"\"\"\n\n    @classmethod\n    def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:\n        \"\"\"Get the response parser.\"\"\"\n        task_creation_template = (\n            \"Can you hel me to\"\n            \" to create new tasks with the following objective: {objective},\"\n            \" The last completed task has the result: {result}.\"\n            \" This result was based on this task description: {task_description}.\"\n            \" These are incomplete tasks: {incomplete_tasks}.\"\n            \" Based on the result, create new tasks to be completed\"\n            \" Return the task as an List without anything else.\"\n        )\n        prompt = PromptTemplate(\n            template=task_creation_template,\n            input_variables=[\n                \"result\",\n                \"task_description\",\n                \"incomplete_tasks\",\n                \"objective\",\n            ],\n        )\n        return cls(prompt=prompt, llm=llm, verbose=verbose)", ""]}
{"filename": "BabyAgi/BabyAGIMod.py", "chunked_list": ["\"\"\"BabyAGI agent.\"\"\"\nfrom collections import deque\nfrom typing import Any, Dict, List, Optional\n\nfrom pydantic import BaseModel, Field\nfrom langchain import LLMChain, PromptTemplate\n\nfrom langchain.base_language import BaseLanguageModel\nfrom langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain", "from langchain.callbacks.manager import CallbackManagerForChainRun\nfrom langchain.chains.base import Chain\nfrom langchain.vectorstores.base import VectorStore\nimport os\nfrom .task_creation import TaskCreationChain\nfrom .task_execution import TaskExecutionChain\nfrom .task_prioritization import TaskPrioritizationChain\n\nclass BabyAGI(Chain, BaseModel):\n    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n\n    task_list: deque = Field(default_factory=deque)\n    task_creation_chain: Chain = Field(...)\n    task_prioritization_chain: Chain = Field(...)\n    execution_chain: Chain = Field(...)\n    task_id_counter: int = Field(1)\n    vectorstore: VectorStore = Field(init=False)\n    max_iterations: Optional[int] = None\n    store: Optional[bool] = False\n    write_step: Optional[int] = 0\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        arbitrary_types_allowed = True\n\n    def add_task(self, task: Dict) -> None:\n        self.task_list.append(task)\n\n    def print_task_list(self) -> None:\n        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n        for t in self.task_list:\n            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n\n    def print_next_task(self, task: Dict) -> None:\n        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n\n    def print_task_result(self, result: str) -> None:\n        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n        print(result)\n\n    @property\n    def input_keys(self) -> List[str]:\n        return [\"objective\"]\n\n    @property\n    def output_keys(self) -> List[str]:\n        return []\n\n    def get_next_task(\n        self, result: str, task_description: str, objective: str\n    ) -> List[Dict]:\n        \"\"\"Get the next task.\"\"\"\n        task_names = [t[\"task_name\"] for t in self.task_list]\n\n        incomplete_tasks = \", \".join(task_names)\n        response = self.task_creation_chain.run(\n            result=result,\n            task_description=task_description,\n            incomplete_tasks=incomplete_tasks,\n            objective=objective,\n        )\n        new_tasks = response.split(\"\\n\")\n        return [\n            {\"task_name\": task_name} for task_name in new_tasks if task_name.strip()\n        ]\n\n    def prioritize_tasks(self, this_task_id: int, objective: str) -> List[Dict]:\n        \"\"\"Prioritize tasks.\"\"\"\n        task_names = [t[\"task_name\"] for t in list(self.task_list)]\n        next_task_id = int(this_task_id) + 1\n        response = self.task_prioritization_chain.run(\n            task_names=\", \".join(task_names),\n            next_task_id=str(next_task_id),\n            objective=objective,\n        )\n        new_tasks = response.split(\"\\n\")\n        prioritized_task_list = []\n        for task_string in new_tasks:\n            if not task_string.strip():\n                continue\n            task_parts = task_string.strip().split(\".\", 1)\n            if len(task_parts) == 2:\n                task_id = task_parts[0].strip()\n                task_name = task_parts[1].strip()\n                prioritized_task_list.append(\n                    {\"task_id\": task_id, \"task_name\": task_name}\n                )\n        return prioritized_task_list\n\n    def _get_top_tasks(self, query: str, k: int) -> List[str]:\n        \"\"\"Get the top k tasks based on the query.\"\"\"\n        results = self.vectorstore.similarity_search(query, k=k)\n        if not results:\n            return []\n        return [str(item.metadata[\"task\"]) for item in results]\n\n    def execute_task(self, objective: str, task: str, k: int = 5) -> str:\n        \"\"\"Execute a task.\"\"\"\n        context = self._get_top_tasks(query=objective, k=k)\n        return self.execution_chain.run(\n            objective=objective, context=\"\\n\".join(context), task=task\n        )\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Run the agent.\"\"\"\n        objective = inputs[\"objective\"]\n        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n        num_iters = 0\n        \n        \n        dir_name=\"\"\n        if self.store:\n            try:    \n                # create a directory to store the results of evry task\n                os.mkdir(\"BABYAGI_RESULTS_FOR_\" + objective.replace(\" \", \"_\"))\n                dir_name = \"BABYAGI_RESULTS_FOR_\" + objective.replace(\" \", \"_\")\n                self.write_step = 0\n            except:\n                print(\"ATTENTION: directory already exists, Delete the directory to store the results of evry task\")\n                self.store = False\n                \n        while True:\n            if self.task_list:\n                self.print_task_list()\n\n                # Step 1: Pull the first task\n                task = self.task_list.popleft()\n                self.print_next_task(task)\n\n                # Step 2: Execute the task\n                result = self.execute_task(objective, task[\"task_name\"])\n                this_task_id = int(task[\"task_id\"])  # THIS LINE GIVE ERROR  WOLUD BE FIXED\n                self.print_task_result(result)\n                \n                if self.store:\n                    # save the result in a file\n                    self.write_step += 1\n                    with open(dir_name + \"/\" + str(self.write_step) + \".txt\", \"w\") as f:\n                        f.write(result)\n                    print(\"<<BABY AGI>> : result saved in \" + dir_name + \"/\" + str(self.write_step) +  \".txt\")\n\n                # Step 3: Store the result in Pinecone\n                result_id = f\"result_{task['task_id']}\"\n                self.vectorstore.add_texts(\n                    texts=[result],\n                    metadatas=[{\"task\": task[\"task_name\"]}],\n                    ids=[result_id],\n                )\n\n                # Step 4: Create new tasks and reprioritize task list\n                new_tasks = self.get_next_task(result, task[\"task_name\"], objective)\n                for new_task in new_tasks:\n                    self.task_id_counter += 1\n                    new_task.update({\"task_id\": self.task_id_counter})\n                    self.add_task(new_task)\n                self.task_list = deque(self.prioritize_tasks(this_task_id, objective))\n            num_iters += 1\n            if self.max_iterations is not None and num_iters == self.max_iterations:\n                print(\n                    \"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\"\n                )\n                \n                if self.store:\n                    #create final file to append in order by write_step \n                    final_file = open(dir_name + \"/\" + \"final.txt\", \"w\")    \n                    all_step = os.listdir(dir_name)\n                    all_step.sort()\n                    for step in all_step:\n                        #append the result of each step in the final file\n                        with open(dir_name + \"/\" + step, \"r\") as f:\n                            final_file.write(f.read())\n                    final_file.close()\n                    \n                    print(\n                        \"\\033[91m\\033[1m\" + \"\\n*****RESULT STORED*****\\n\" + \"\\033[0m\\033[0m\"\n                    )\n                    \n                break\n        return {}\n\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLanguageModel,\n        vectorstore: VectorStore,\n        verbose: bool = False,\n        task_execution_chain: Optional[Chain] = None,\n        **kwargs: Dict[str, Any],\n    ) -> \"BabyAGI\":\n        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n        task_creation_chain = TaskCreationChain.from_llm(llm, verbose=verbose)\n        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n            llm, verbose=verbose\n        )\n        if task_execution_chain is None:\n            execution_chain: Chain = TaskExecutionChain.from_llm(llm, verbose=verbose)\n        else:\n            execution_chain = task_execution_chain\n        return cls(\n            task_creation_chain=task_creation_chain,\n            task_prioritization_chain=task_prioritization_chain,\n            execution_chain=execution_chain,\n            vectorstore=vectorstore,\n            **kwargs,\n        )", "class BabyAGI(Chain, BaseModel):\n    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n\n    task_list: deque = Field(default_factory=deque)\n    task_creation_chain: Chain = Field(...)\n    task_prioritization_chain: Chain = Field(...)\n    execution_chain: Chain = Field(...)\n    task_id_counter: int = Field(1)\n    vectorstore: VectorStore = Field(init=False)\n    max_iterations: Optional[int] = None\n    store: Optional[bool] = False\n    write_step: Optional[int] = 0\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        arbitrary_types_allowed = True\n\n    def add_task(self, task: Dict) -> None:\n        self.task_list.append(task)\n\n    def print_task_list(self) -> None:\n        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n        for t in self.task_list:\n            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n\n    def print_next_task(self, task: Dict) -> None:\n        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n\n    def print_task_result(self, result: str) -> None:\n        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n        print(result)\n\n    @property\n    def input_keys(self) -> List[str]:\n        return [\"objective\"]\n\n    @property\n    def output_keys(self) -> List[str]:\n        return []\n\n    def get_next_task(\n        self, result: str, task_description: str, objective: str\n    ) -> List[Dict]:\n        \"\"\"Get the next task.\"\"\"\n        task_names = [t[\"task_name\"] for t in self.task_list]\n\n        incomplete_tasks = \", \".join(task_names)\n        response = self.task_creation_chain.run(\n            result=result,\n            task_description=task_description,\n            incomplete_tasks=incomplete_tasks,\n            objective=objective,\n        )\n        new_tasks = response.split(\"\\n\")\n        return [\n            {\"task_name\": task_name} for task_name in new_tasks if task_name.strip()\n        ]\n\n    def prioritize_tasks(self, this_task_id: int, objective: str) -> List[Dict]:\n        \"\"\"Prioritize tasks.\"\"\"\n        task_names = [t[\"task_name\"] for t in list(self.task_list)]\n        next_task_id = int(this_task_id) + 1\n        response = self.task_prioritization_chain.run(\n            task_names=\", \".join(task_names),\n            next_task_id=str(next_task_id),\n            objective=objective,\n        )\n        new_tasks = response.split(\"\\n\")\n        prioritized_task_list = []\n        for task_string in new_tasks:\n            if not task_string.strip():\n                continue\n            task_parts = task_string.strip().split(\".\", 1)\n            if len(task_parts) == 2:\n                task_id = task_parts[0].strip()\n                task_name = task_parts[1].strip()\n                prioritized_task_list.append(\n                    {\"task_id\": task_id, \"task_name\": task_name}\n                )\n        return prioritized_task_list\n\n    def _get_top_tasks(self, query: str, k: int) -> List[str]:\n        \"\"\"Get the top k tasks based on the query.\"\"\"\n        results = self.vectorstore.similarity_search(query, k=k)\n        if not results:\n            return []\n        return [str(item.metadata[\"task\"]) for item in results]\n\n    def execute_task(self, objective: str, task: str, k: int = 5) -> str:\n        \"\"\"Execute a task.\"\"\"\n        context = self._get_top_tasks(query=objective, k=k)\n        return self.execution_chain.run(\n            objective=objective, context=\"\\n\".join(context), task=task\n        )\n\n    def _call(\n        self,\n        inputs: Dict[str, Any],\n        run_manager: Optional[CallbackManagerForChainRun] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Run the agent.\"\"\"\n        objective = inputs[\"objective\"]\n        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n        num_iters = 0\n        \n        \n        dir_name=\"\"\n        if self.store:\n            try:    \n                # create a directory to store the results of evry task\n                os.mkdir(\"BABYAGI_RESULTS_FOR_\" + objective.replace(\" \", \"_\"))\n                dir_name = \"BABYAGI_RESULTS_FOR_\" + objective.replace(\" \", \"_\")\n                self.write_step = 0\n            except:\n                print(\"ATTENTION: directory already exists, Delete the directory to store the results of evry task\")\n                self.store = False\n                \n        while True:\n            if self.task_list:\n                self.print_task_list()\n\n                # Step 1: Pull the first task\n                task = self.task_list.popleft()\n                self.print_next_task(task)\n\n                # Step 2: Execute the task\n                result = self.execute_task(objective, task[\"task_name\"])\n                this_task_id = int(task[\"task_id\"])  # THIS LINE GIVE ERROR  WOLUD BE FIXED\n                self.print_task_result(result)\n                \n                if self.store:\n                    # save the result in a file\n                    self.write_step += 1\n                    with open(dir_name + \"/\" + str(self.write_step) + \".txt\", \"w\") as f:\n                        f.write(result)\n                    print(\"<<BABY AGI>> : result saved in \" + dir_name + \"/\" + str(self.write_step) +  \".txt\")\n\n                # Step 3: Store the result in Pinecone\n                result_id = f\"result_{task['task_id']}\"\n                self.vectorstore.add_texts(\n                    texts=[result],\n                    metadatas=[{\"task\": task[\"task_name\"]}],\n                    ids=[result_id],\n                )\n\n                # Step 4: Create new tasks and reprioritize task list\n                new_tasks = self.get_next_task(result, task[\"task_name\"], objective)\n                for new_task in new_tasks:\n                    self.task_id_counter += 1\n                    new_task.update({\"task_id\": self.task_id_counter})\n                    self.add_task(new_task)\n                self.task_list = deque(self.prioritize_tasks(this_task_id, objective))\n            num_iters += 1\n            if self.max_iterations is not None and num_iters == self.max_iterations:\n                print(\n                    \"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\"\n                )\n                \n                if self.store:\n                    #create final file to append in order by write_step \n                    final_file = open(dir_name + \"/\" + \"final.txt\", \"w\")    \n                    all_step = os.listdir(dir_name)\n                    all_step.sort()\n                    for step in all_step:\n                        #append the result of each step in the final file\n                        with open(dir_name + \"/\" + step, \"r\") as f:\n                            final_file.write(f.read())\n                    final_file.close()\n                    \n                    print(\n                        \"\\033[91m\\033[1m\" + \"\\n*****RESULT STORED*****\\n\" + \"\\033[0m\\033[0m\"\n                    )\n                    \n                break\n        return {}\n\n    @classmethod\n    def from_llm(\n        cls,\n        llm: BaseLanguageModel,\n        vectorstore: VectorStore,\n        verbose: bool = False,\n        task_execution_chain: Optional[Chain] = None,\n        **kwargs: Dict[str, Any],\n    ) -> \"BabyAGI\":\n        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n        task_creation_chain = TaskCreationChain.from_llm(llm, verbose=verbose)\n        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n            llm, verbose=verbose\n        )\n        if task_execution_chain is None:\n            execution_chain: Chain = TaskExecutionChain.from_llm(llm, verbose=verbose)\n        else:\n            execution_chain = task_execution_chain\n        return cls(\n            task_creation_chain=task_creation_chain,\n            task_prioritization_chain=task_prioritization_chain,\n            execution_chain=execution_chain,\n            vectorstore=vectorstore,\n            **kwargs,\n        )", ""]}
{"filename": "BabyAgi/task_prioritization.py", "chunked_list": ["from langchain import LLMChain, PromptTemplate\nfrom langchain.base_language import BaseLanguageModel\n\n\nclass TaskPrioritizationChain(LLMChain):\n    \"\"\"Chain to prioritize tasks.\"\"\"\n\n    @classmethod\n    def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:\n        \"\"\"Get the response parser.\"\"\"\n        task_prioritization_template = (\n            \"Please help me to cleaning the formatting of \"\n            \"and reprioritizing the following tasks: {task_names}.\"\n            \"Consider the ultimate objective of your team: {objective}.\"\n            \"Do not remove any tasks. Return ONLY the result as a numbered list without anything else, like:\\n\"\n            \"1. First task\\n\"\n            \"2. Second task\\n\"\n            \"Start the task list with number {next_task_id}.\"\n        )\n        prompt = PromptTemplate(\n            template=task_prioritization_template,\n            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n        )\n        return cls(prompt=prompt, llm=llm, verbose=verbose)", ""]}
{"filename": "BabyAgi/task_execution.py", "chunked_list": ["from langchain import LLMChain, PromptTemplate\nfrom langchain.base_language import BaseLanguageModel\n\n\nclass TaskExecutionChain(LLMChain):\n    \"\"\"Chain to execute tasks.\"\"\"\n\n    @classmethod\n    def from_llm(cls, llm: BaseLanguageModel, verbose: bool = True) -> LLMChain:\n        \"\"\"Get the response parser.\"\"\"\n        execution_template = (\n            \"Can you help me to performs one task based on the following objective: \"\n            \"{objective}.\"\n            \"Take into account these previously completed tasks: {context}.\"\n            \"Can you perform this task? Your task: {task}. Response:\"\n        )\n        prompt = PromptTemplate(\n            template=execution_template,\n            input_variables=[\"objective\", \"context\", \"task\"],\n        )\n        return cls(prompt=prompt, llm=llm, verbose=verbose)"]}
{"filename": "FreeLLM/HuggingChatAPI.py", "chunked_list": ["\nfrom hugchat import hugchat\nfrom hugchat.login import Login\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nfrom time import sleep\n\n\n\nclass HuggingChat(LLM):\n    \n    history_data: Optional[List] = []\n    chatbot : Optional[hugchat.ChatBot] = None\n    conversation : Optional[str] = \"\"\n    email : Optional[str]\n    psw : Optional[str]\n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.email is None and self.psw is None:\n                ValueError(\"Email and Password is required, pls check the documentation on github\")\n            else: \n                if self.conversation == \"\":\n                    sign = Login(self.email, self.psw)\n                    cookies = sign.login()\n\n                    # Save cookies to usercookies/<email>.json\n                    sign.saveCookies()\n\n                    # Create a ChatBot\n                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n                else:\n                    raise ValueError(\"Something went wrong\")\n            \n        \n        sleep(2)\n        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n    \n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n        \n        return data\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"HuggingCHAT\"}", "\nclass HuggingChat(LLM):\n    \n    history_data: Optional[List] = []\n    chatbot : Optional[hugchat.ChatBot] = None\n    conversation : Optional[str] = \"\"\n    email : Optional[str]\n    psw : Optional[str]\n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.email is None and self.psw is None:\n                ValueError(\"Email and Password is required, pls check the documentation on github\")\n            else: \n                if self.conversation == \"\":\n                    sign = Login(self.email, self.psw)\n                    cookies = sign.login()\n\n                    # Save cookies to usercookies/<email>.json\n                    sign.saveCookies()\n\n                    # Create a ChatBot\n                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n                else:\n                    raise ValueError(\"Something went wrong\")\n            \n        \n        sleep(2)\n        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n    \n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n        \n        return data\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"HuggingCHAT\"}", "\n\n\n#llm = HuggingChat(email = \"YOUR-COOKIES-PATH\" , psw ) #for start new chat\n\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n", "#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n\n"]}
{"filename": "FreeLLM/ChatGPTAPI.py", "chunked_list": ["from gpt4_openai import GPT4OpenAI\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nfrom time import sleep\n\n\n\nclass ChatGPT(LLM):\n    \n    history_data: Optional[List] = []\n    token : Optional[str]\n    chatbot : Optional[GPT4OpenAI] = None\n    call : int = 0\n    model : str = \"gpt-3\" # or gpt-4\n    plugin_id : Optional[List] = []\n    \n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.token is None:\n                raise ValueError(\"Need a token , check https://chat.openai.com/api/auth/session for get your token\")\n            else:\n                try:\n                    if self.plugin_id == []:\n                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model)\n                    else:\n                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model, plugin_ids=self.plugin_id)\n                except:\n                    raise ValueError(\"Error on create chatbot, check your token, or your model\")\n                \n        response = \"\"\n        # OpenAI: 50 requests / hour for each account\n        if (self.call >= 45 and self.model == \"default\") or (self.call >= 23 and self.model == \"gpt4\"):\n            raise ValueError(\"You have reached the maximum number of requests per hour ! Help me to Improve. Abusing this tool is at your own risk\")\n        else:\n            sleep(2)\n            response = self.chatbot(prompt)\n            \n            self.call += 1\n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response})    \n        \n        return response\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"ChatGPT\", \"token\": self.token, \"model\": self.model}", "\n\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\") #for start new chat\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\" , model=\"gpt4\") # REQUIRED CHATGPT PLUS subscription\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\", conversation = \"Add-XXXX-XXXX-Convesation-ID\") #for use a chat already started\n\n#print(llm(\"Hello, how are you?\"))", "\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "FreeLLM/BardChatAPI.py", "chunked_list": ["from Bard import Chatbot\nimport asyncio\n\nimport requests\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nimport pydantic\nimport os\nfrom langchain import PromptTemplate, LLMChain\nfrom time import sleep", "from langchain import PromptTemplate, LLMChain\nfrom time import sleep\n\n\n\nclass BardChat(LLM):\n    \n    history_data: Optional[List] = []\n    cookie : Optional[str]\n    chatbot : Optional[Chatbot] = None\n\n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #cookie is a must check\n        if self.chatbot is None:\n            if self.cookie is None:\n                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n            else:\n                #if self.chatbot == None:\n                self.chatbot = Chatbot(self.cookie)\n               \n        response = self.chatbot.ask(prompt)\n        #print(response)\n        response_text = response['content']\n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n        \n        return response_text\n    \n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        return asyncio.run(self.call(prompt=prompt))\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"BardCHAT\", \"cookie\": self.cookie}", "\n\n\n#llm = BardChat(cookie = \"YOURCOOKIE\") #for start new chat\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n", ""]}
{"filename": "FreeLLM/BingChatAPI.py", "chunked_list": ["from EdgeGPT import Chatbot, ConversationStyle\nimport asyncio\n\nimport requests\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nimport pydantic\nimport os\nfrom langchain import PromptTemplate, LLMChain\nfrom time import sleep", "from langchain import PromptTemplate, LLMChain\nfrom time import sleep\n\n\n\nclass BingChat(LLM):\n    \n    history_data: Optional[List] = []\n    cookiepath : Optional[str]\n    chatbot : Optional[Chatbot] = None\n    conversation_style : Optional[str] \n    conversation_style_on : Optional[ConversationStyle] = ConversationStyle.precise\n    search_result : Optional[bool] = False\n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n    \n    def select_conversation(self, conversation_style: str):\n        if conversation_style == \"precise\":\n            self.conversation_style_on = ConversationStyle.precise\n        elif conversation_style == \"creative\":\n            self.conversation_style_on = ConversationStyle.creative\n        elif conversation_style == \"balanced\":\n            self.conversation_style_on = ConversationStyle.balanced\n        else:\n            raise ValueError(\"conversation_style must be precise, creative or balaced\")\n        self.conversation_style = conversation_style\n\n    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            raise ValueError(\"stop kwargs are not permitted.\")\n        #cookiepath is a must check\n        if self.chatbot is None:\n            if self.cookiepath is None:\n                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n            else:\n                #if self.chatbot == None:\n                self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n               \n        if self.conversation_style is not None:\n            self.conversation_style_on = self.select_conversation(self.conversation_style)\n            \n        response = await self.chatbot.ask(prompt=prompt, conversation_style=self.conversation_style, search_result=self.search_result)\n        \"\"\"\n        this is a sample response. \n        {'type': 2, 'invocationId': '0', \n        'item': {'messages': [{'text': 'Hello, how are you?', 'author': 'user', 'from': {'id': '985157152860707', 'name': None}, 'createdAt': '2023-05-03T19:51:39.5491558+00:00', 'timestamp': '2023-05-03T19:51:39.5455787+00:00', 'locale': 'en-us', 'market': 'en-us', 'region': 'us', 'messageId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'nlu': {'scoredClassification': {'classification': 'CHAT_GPT', 'score': None}, 'classificationRanking': [{'classification': 'CHAT_GPT', 'score': None}], 'qualifyingClassifications': None, 'ood': None, 'metaData': None, 'entities': None}, 'offense': 'None', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'cib', 'privacy': None, 'inputMethod': 'Keyboard'}, {'text': \"Hello! I'm doing well, thank you. How can I assist you today?\", 'author': 'bot', 'createdAt': '2023-05-03T19:51:41.5176164+00:00', 'timestamp': '2023-05-03T19:51:41.5176164+00:00', 'messageId': '1d013e71-408b-4031-a131-2f5c009fe938', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'offense': 'None', 'adaptiveCards': [{'type': 'AdaptiveCard', 'version': '1.0', 'body': [{'type': 'TextBlock', 'text': \"Hello! I'm doing well, thank you. How can I assist you today?\\n\", 'wrap': True}]}], \n        'sourceAttributions': [], \n        'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, \n        'contentOrigin': 'DeepLeo', \n        'privacy': None, \n        'suggestedResponses': [{'text': 'What is the weather like today?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502696+00:00', 'timestamp': '2023-05-03T19:51:42.7502696+00:00', 'messageId': 'cd7a84d3-f9bc-47ff-9897-077b2de12e21', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'What is the latest news?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502739+00:00', 'timestamp': '2023-05-03T19:51:42.7502739+00:00', 'messageId': 'b611632a-9a8e-42de-86eb-8eb3b7b8ddbb', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'Tell me a joke.', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502743+00:00', 'timestamp': '2023-05-03T19:51:42.7502743+00:00', 'messageId': '70232e45-d7e8-4d77-83fc-752b3cd3355c', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}], 'spokenText': 'How can I assist you today?'}], 'firstNewMessageIndex': 1, 'defaultChatName': None, 'conversationId': '51D|BingProd|3E1274E188350D7BE273FFE95E02DD2984DAB52F95260300D0A2937162F98FDA', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'conversationExpiryTime': '2023-05-04T01:51:42.8260286Z', 'shouldInitiateConversation': True, 'telemetry': {'metrics': None, 'startTime': '2023-05-03T19:51:39.5456555Z'}, 'throttling': {'maxNumUserMessagesInConversation': 20, 'numUserMessagesInConversation': 1}, 'result': {'value': 'Success', 'serviceVersion': '20230501.30'}}}\n        \"\"\"\n        response_messages = response.get(\"item\", {}).get(\"messages\", [])\n        response_text = response_messages[1].get(\"text\", \"\")\n        \n        if response_text == \"\":\n            hidden_text = response_messages[1].get(\"hiddenText\", \"\")\n            print(\">>>> [DEBBUGGER] hidden_text = \" + str(hidden_text) + \" [DEBBUGGER] <<<<\")\n            print(\">>>> [DEBBUGGER] BING CHAT dont is open Like CHATGPT , BingCHAT have refused to respond. [DEBBUGGER] <<<<\")\n            response_text = hidden_text\n            \"\"\"\n            # reset the chatbot and remake the call\n            print(\"[DEBUGGER] Chatbot failed to respond. Resetting and trying again. [DEBUGGER]\")\n            print(\"[ INFO DEBUGGER ] \\n<Response>\\n\" + str(response) + \"\\n</Response>\\n\\n\")\n            sleep(10)\n            self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n            sleep(2)\n            response = await self.chatbot.ask(prompt=prompt)\n            response_messages = response.get(\"item\", {}).get(\"messages\", [])\n            response_text = response_messages[1].get(\"text\", \"\")\n            \"\"\"\n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n        \n        return response_text\n    \n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        return asyncio.run(self.call(prompt=prompt))\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"BingCHAT\", \"cookiepath\": self.cookiepath}", "\n\n\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\") #for start new chat\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\", conversation_style = \"precise\") #precise, creative or balaced\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\" , conversation_style = \"precise\" , search_result=True) #with web access\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well", "#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "OtherAgent/pythonAgent.py", "chunked_list": ["import json\nfrom pathlib import Path\nfrom json import JSONDecodeError\nfrom langchain.agents.agent_toolkits import create_python_agent\nfrom langchain.tools.python.tool import PythonREPLTool\nfrom FreeLLM import ChatGPTAPI # FREE CHATGPT API\nfrom FreeLLM import HuggingChatAPI # FREE HUGGINGCHAT API\nfrom FreeLLM import BingChatAPI # FREE BINGCHAT API\nfrom FreeLLM import BardChatAPI # FREE GOOGLE BARD API\n", "from FreeLLM import BardChatAPI # FREE GOOGLE BARD API\n\nimport os\n\n#### LOG IN FOR CHATGPT FREE LLM\nfrom dotenv import load_dotenv\nload_dotenv()\n\nselect_model = input(\"Select the model you want to use (1, 2, 3 or 4) \\n \\\n1) ChatGPT \\n \\", "select_model = input(\"Select the model you want to use (1, 2, 3 or 4) \\n \\\n1) ChatGPT \\n \\\n2) HuggingChat \\n \\\n3) BingChat \\n \\\n4) Google Bard \\n \\\n>>> \")\n\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt-4\"\n    else:\n        model = \"default\"\n\n    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n              \nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\")\n    cookie_path = Path() / \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", 'r') as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\")\n    llm=BingChatAPI.BingChat(cookiepath=str(cookie_path), conversation_style=\"creative\")\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n    \n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\")\n    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n    llm=BardChatAPI.BardChat(cookie=cookie_path)", "    \n    \n\n####\n\n\n\nagent_executor = create_python_agent(\n    llm=llm,\n    tool=PythonREPLTool(),", "    llm=llm,\n    tool=PythonREPLTool(),\n    verbose=True\n)\n\n#todo : ADD MEMORY\n\nprint(\">> START Python AGENT\")\nprint(\"> Digit 'exit' for exit or 'your task or question' for start\\n\\n\")\nprompt = input(\"(Enter your task or question) >> \")", "print(\"> Digit 'exit' for exit or 'your task or question' for start\\n\\n\")\nprompt = input(\"(Enter your task or question) >> \")\nwhile prompt != \"exit\":\n    agent_executor.run(prompt)\n    prompt = input(\"(Enter your task or question) >> \")\n"]}
{"filename": "OtherAgent/customAgent.py", "chunked_list": ["import json\nfrom pathlib import Path\nfrom json import JSONDecodeError\nfrom langchain.agents import initialize_agent\nfrom langchain.utilities import PythonREPL\nfrom langchain.utilities import WikipediaAPIWrapper\nfrom langchain.tools import BaseTool, DuckDuckGoSearchRun\nfrom langchain.tools.human.tool import HumanInputRun\nfrom FreeLLM import ChatGPTAPI # FREE CHATGPT API\nfrom FreeLLM import HuggingChatAPI # FREE HUGGINGCHAT API", "from FreeLLM import ChatGPTAPI # FREE CHATGPT API\nfrom FreeLLM import HuggingChatAPI # FREE HUGGINGCHAT API\nfrom FreeLLM import BingChatAPI # FREE BINGCHAT API\nfrom FreeLLM import BardChatAPI # FREE Google BArd API\nfrom langchain.agents import initialize_agent, Tool\n\nimport os\n\n#### LOG IN FOR CHATGPT FREE LLM\nfrom dotenv import load_dotenv", "#### LOG IN FOR CHATGPT FREE LLM\nfrom dotenv import load_dotenv\nload_dotenv()\nselect_model = input(\"Select the model you want to use (1, 2, 3 or 4) \\n \\\n1) ChatGPT \\n \\\n2) HuggingChat \\n \\\n3) BingChat \\n \\\n4) Google Bard \\n \\\n>>> \")\n\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt-4\"\n    else:\n        model = \"default\"\n\n    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\n    \nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\")\n    cookie_path = Path() / \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", 'r') as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\")\n    llm=BingChatAPI.BingChat(cookiepath=str(cookie_path), conversation_style=\"creative\")\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n    \n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\")\n    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n    llm=BardChatAPI.BardChat(cookie=cookie_path)", ">>> \")\n\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt-4\"\n    else:\n        model = \"default\"\n\n    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n\n    \nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\")\n    cookie_path = Path() / \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", 'r') as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\")\n    llm=BingChatAPI.BingChat(cookiepath=str(cookie_path), conversation_style=\"creative\")\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n    \n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\")\n    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n    llm=BardChatAPI.BardChat(cookie=cookie_path)", "    \n####\n\nwikipedia = WikipediaAPIWrapper()\npython_repl = PythonREPL()\nsearch = DuckDuckGoSearchRun()\n\n\n#from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain, BaseCombineDocumentsChain\n#from Tool import browserQA", "#from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain, BaseCombineDocumentsChain\n#from Tool import browserQA\n#query_website_tool = browserQA.WebpageQATool(qa_chain=load_qa_with_sources_chain(llm))\n\n\n\n#define TOOLs\n\ntools = [\n    Tool(", "tools = [\n    Tool(\n        name = \"python repl\",\n        func=python_repl.run,\n        description=\"useful for when you need to use python to answer a question. You should input python code\"\n    )\n]\n\nwikipedia_tool = Tool(\n    name='wikipedia',", "wikipedia_tool = Tool(\n    name='wikipedia',\n    func= wikipedia.run,\n    description=\"Useful for when you need to look up a topic, country or person on wikipedia\"\n)\n\n\nduckduckgo_tool = Tool(\n    name='DuckDuckGo Search',\n    func= search.run,", "    name='DuckDuckGo Search',\n    func= search.run,\n    description=\"Useful for when you need to do a search on the internet to find information that another tool can't find. be specific with your input.\"\n)\n\"\"\"\n\nqueryWebsite_tool = Tool(\n    name= query_website_tool.name,\n    func= query_website_tool.run,\n    description= query_website_tool.description", "    func= query_website_tool.run,\n    description= query_website_tool.description\n)\n\n\"\"\"\n#human_input_tool = Tool(\n    #name='human input',\n    #func= HumanInputRun.run,\n    #description=\"Useful for when you need to ask a human a question. be specific with your input.\"\n#)", "    #description=\"Useful for when you need to ask a human a question. be specific with your input.\"\n#)\n\n#Add here your tools\n#custom_tool = Tool(\n    #name='custom tool',\n    #func= custom_tool.run,\n    #description=\"My fantasitc tool\"\n#)\n", "#)\n\n\ntools.append(duckduckgo_tool)\n#tools.append(queryWebsite_tool)\ntools.append(wikipedia_tool)\n#tools.append(human_input_tool)\n#tools.append(custom_tool)\n\n", "\n\n#Create the Agent\niteration = (int(input(\"Enter the number of iterations: \")) if input(\"Do you want to set the number of iterations? (y/n): \") == \"y\" else 3)\n\nzero_shot_agent = initialize_agent(\n    agent=\"zero-shot-react-description\", \n    tools=tools, \n    llm=llm,\n    verbose=True,", "    llm=llm,\n    verbose=True,\n    max_iterations=iteration,\n    \n)\n\n\n\nprint(\">> START CUSTOM AGENT\")\nprint(\"> Digit 'exit' for exit or 'your task or question' for start\\n\\n\")", "print(\">> START CUSTOM AGENT\")\nprint(\"> Digit 'exit' for exit or 'your task or question' for start\\n\\n\")\nprompt = input(\"(Enter your task or question) >> \")\nwhile prompt != \"exit\":\n    zero_shot_agent.run(prompt)\n    prompt = input(\"(Enter your task or question) >> \")\n"]}
{"filename": "OtherAgent/csvAgent.py", "chunked_list": ["import json\nfrom pathlib import Path\nfrom json import JSONDecodeError\nfrom langchain.agents import create_csv_agent\nfrom FreeLLM import ChatGPTAPI # FREE CHATGPT API\nfrom FreeLLM import HuggingChatAPI # FREE HUGGINGCHAT API\nfrom FreeLLM import BingChatAPI # FREE BINGCHAT API\nfrom FreeLLM import BardChatAPI # FREE GOOGLE BARD API\n\nfrom langchain.utilities import PythonREPL", "\nfrom langchain.utilities import PythonREPL\nimport os\n\n#### LOG IN FOR CHATGPT FREE LLM\nfrom dotenv import load_dotenv\nload_dotenv()\n\nselect_model = input(\"Select the model you want to use (1, 2, 3 or 4) \\n \\\n1) ChatGPT \\n \\", "select_model = input(\"Select the model you want to use (1, 2, 3 or 4) \\n \\\n1) ChatGPT \\n \\\n2) HuggingChat \\n \\\n3) BingChat \\n \\\n4) Google Bard \\n \\\n>>> \")\n\nif select_model == \"1\":\n    CG_TOKEN = os.getenv(\"CHATGPT_TOKEN\", \"your-chatgpt-token\")\n\n    if CG_TOKEN != \"your-chatgpt-token\":\n        os.environ[\"CHATGPT_TOKEN\"] = CG_TOKEN\n    else:\n        raise ValueError(\n            \"ChatGPT Token EMPTY. Edit the .env file and put your ChatGPT token\"\n        )\n\n    start_chat = os.getenv(\"USE_EXISTING_CHAT\", False)\n    if os.getenv(\"USE_GPT4\") == \"True\":\n        model = \"gpt-4\"\n    else:\n        model = \"default\"\n\n    llm = ChatGPTAPI.ChatGPT(token=os.environ[\"CHATGPT_TOKEN\"], model=model)\n              \nelif select_model == \"2\":\n    emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n    pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n    if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n        os.environ[\"emailHF\"] = emailHF\n        os.environ[\"pswHF\"] = pswHF\n    else:\n        raise ValueError(\n            \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n        )\n    \n    llm = HuggingChatAPI.HuggingChat(email=os.environ[\"emailHF\"], psw=os.environ[\"pswHF\"])\n\nelif select_model == \"3\":\n    if not os.path.exists(\"cookiesBing.json\"):\n        raise ValueError(\"File 'cookiesBing.json' not found! Create it and put your cookies in there in the JSON format.\")\n    cookie_path = Path() / \"cookiesBing.json\"\n    with open(\"cookiesBing.json\", 'r') as file:\n        try:\n            file_json = json.loads(file.read())\n        except JSONDecodeError:\n            raise ValueError(\"You did not put your cookies inside 'cookiesBing.json'! You can find the simple guide to get the cookie file here: https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required.\")\n    llm=BingChatAPI.BingChat(cookiepath=str(cookie_path), conversation_style=\"creative\")\n\nelif select_model == \"4\":\n    GB_TOKEN = os.getenv(\"BARDCHAT_TOKEN\", \"your-googlebard-token\")\n    \n    if GB_TOKEN != \"your-googlebard-token\":\n        os.environ[\"BARDCHAT_TOKEN\"] = GB_TOKEN\n    else:\n        raise ValueError(\"GoogleBard Token EMPTY. Edit the .env file and put your GoogleBard token\")\n    cookie_path = os.environ[\"BARDCHAT_TOKEN\"]\n    llm=BardChatAPI.BardChat(cookie=cookie_path)", "\n####\n\npath_csv = input(\"Enter the path of the csv file: \") or \"OtherAgent/startup.csv\"\n\nagent = create_csv_agent(llm=llm, tool=PythonREPL(), path=path_csv, verbose=True)\n\n#todo : ADD MEMORY\n\n", "\n\nprint(\">> START CSV AGENT\")\nprint(\"> Digit 'exit' for exit or 'your task or question' for start\\n\\n\")\nprompt = input(\"(Enter your task or question) >> \")\nwhile prompt != \"exit\":\n    agent.run(prompt)\n    prompt = input(\"(Enter your task or question) >> \")\n\n", "\n"]}
{"filename": "OtherAgent/FreeLLM/HuggingChatAPI.py", "chunked_list": ["\nfrom hugchat import hugchat\nfrom hugchat.login import Login\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nfrom time import sleep\n\n\n\n\nclass HuggingChat(LLM):\n    \n    history_data: Optional[List] = []\n    chatbot : Optional[hugchat.ChatBot] = None\n    conversation : Optional[str] = \"\"\n    email : Optional[str]\n    psw : Optional[str]\n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.email is None and self.psw is None:\n                ValueError(\"Email and Password is required, pls check the documentation on github\")\n            else: \n                if self.conversation == \"\":\n                    sign = Login(self.email, self.psw)\n                    cookies = sign.login()\n\n                    # Save cookies to usercookies/<email>.json\n                    sign.saveCookies()\n\n                    # Create a ChatBot\n                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n                else:\n                    raise ValueError(\"Something went wrong\")\n            \n        \n        sleep(2)\n        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n    \n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n        \n        return data\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"HuggingCHAT\"}", "\n\nclass HuggingChat(LLM):\n    \n    history_data: Optional[List] = []\n    chatbot : Optional[hugchat.ChatBot] = None\n    conversation : Optional[str] = \"\"\n    email : Optional[str]\n    psw : Optional[str]\n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.email is None and self.psw is None:\n                ValueError(\"Email and Password is required, pls check the documentation on github\")\n            else: \n                if self.conversation == \"\":\n                    sign = Login(self.email, self.psw)\n                    cookies = sign.login()\n\n                    # Save cookies to usercookies/<email>.json\n                    sign.saveCookies()\n\n                    # Create a ChatBot\n                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n                else:\n                    raise ValueError(\"Something went wrong\")\n            \n        \n        sleep(2)\n        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n    \n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n        \n        return data\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"HuggingCHAT\"}", "\n\n\n#llm = HuggingChat(cookiepath = \"YOUR-COOKIES-PATH\") #for start new chat\n\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n", "#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n\n"]}
{"filename": "OtherAgent/FreeLLM/ChatGPTAPI.py", "chunked_list": ["from gpt4_openai import GPT4OpenAI\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nfrom time import sleep\n\n\n\nclass ChatGPT(LLM):\n    \n    history_data: Optional[List] = []\n    token : Optional[str]\n    chatbot : Optional[GPT4OpenAI] = None\n    call : int = 0\n    model : str = \"gpt-3\" # or gpt-4\n    plugin_id : Optional[List] = []\n    \n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.token is None:\n                raise ValueError(\"Need a token , check https://chat.openai.com/api/auth/session for get your token\")\n            else:\n                try:\n                    if self.plugin_id == []:\n                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model)\n                    else:\n                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model, plugin_ids=self.plugin_id)\n                except:\n                    raise ValueError(\"Error on create chatbot, check your token, or your model\")\n                \n        response = \"\"\n        # OpenAI: 50 requests / hour for each account\n        if (self.call >= 45 and self.model == \"default\") or (self.call >= 23 and self.model == \"gpt4\"):\n            raise ValueError(\"You have reached the maximum number of requests per hour ! Help me to Improve. Abusing this tool is at your own risk\")\n        else:\n            sleep(2)\n            response = self.chatbot(prompt)\n            \n            self.call += 1\n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response})    \n        \n        return response\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"ChatGPT\", \"token\": self.token, \"model\": self.model}", "\n\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\") #for start new chat\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\" , model=\"gpt4\") # REQUIRED CHATGPT PLUS subscription\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\", conversation = \"Add-XXXX-XXXX-Convesation-ID\") #for use a chat already started\n\n#print(llm(\"Hello, how are you?\"))", "\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "OtherAgent/FreeLLM/BardChatAPI.py", "chunked_list": ["from Bard import Chatbot\nimport asyncio\n\nimport requests\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nimport pydantic\nimport os\nfrom langchain import PromptTemplate, LLMChain\nfrom time import sleep", "from langchain import PromptTemplate, LLMChain\nfrom time import sleep\n\n\n\nclass BardChat(LLM):\n    \n    history_data: Optional[List] = []\n    cookie : Optional[str]\n    chatbot : Optional[Chatbot] = None\n\n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #cookie is a must check\n        if self.chatbot is None:\n            if self.cookie is None:\n                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n            else:\n                #if self.chatbot == None:\n                self.chatbot = Chatbot(self.cookie)\n               \n        response = self.chatbot.ask(prompt)\n        #print(response)\n        response_text = response['content']\n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n        \n        return response_text\n    \n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        return asyncio.run(self.call(prompt=prompt))\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"BardCHAT\", \"cookie\": self.cookie}", "\n\n\n#llm = BardChat(cookie = \"YOURCOOKIE\") #for start new chat\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n", ""]}
{"filename": "OtherAgent/FreeLLM/BingChatAPI.py", "chunked_list": ["from EdgeGPT import Chatbot, ConversationStyle\nimport asyncio\n\nimport requests\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nimport pydantic\nimport os\nfrom langchain import PromptTemplate, LLMChain\nfrom time import sleep", "from langchain import PromptTemplate, LLMChain\nfrom time import sleep\n\n\n\nclass BingChat(LLM):\n    \n    history_data: Optional[List] = []\n    cookiepath : Optional[str]\n    chatbot : Optional[Chatbot] = None\n    conversation_style : Optional[str] \n    conversation_style_on : Optional[ConversationStyle] = ConversationStyle.precise\n    search_result : Optional[bool] = False\n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n    \n    def select_conversation(self, conversation_style: str):\n        if conversation_style == \"precise\":\n            self.conversation_style_on = ConversationStyle.precise\n        elif conversation_style == \"creative\":\n            self.conversation_style_on = ConversationStyle.creative\n        elif conversation_style == \"balanced\":\n            self.conversation_style_on = ConversationStyle.balanced\n        else:\n            raise ValueError(\"conversation_style must be precise, creative or balaced\")\n        self.conversation_style = conversation_style\n\n    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #cookiepath is a must check\n        if self.chatbot is None:\n            if self.cookiepath is None:\n                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n            else:\n                #if self.chatbot == None:\n                self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n               \n        if self.conversation_style is not None:\n            self.conversation_style_on = self.select_conversation(self.conversation_style)\n            \n        response = await self.chatbot.ask(prompt=prompt, conversation_style=self.conversation_style, search_result=self.search_result)\n        response_messages = response.get(\"item\", {}).get(\"messages\", [])\n        response_text = response_messages[1].get(\"text\", \"\")\n        \n        if response_text == \"\":\n            hidden_text = response_messages[1].get(\"hiddenText\", \"\")\n            print(\">>>> [DEBBUGGER] hidden_text = \" + str(hidden_text) + \" [DEBBUGGER] <<<<\")\n            print(\">>>> [DEBBUGGER] BING CHAT dont is open Like CHATGPT , BingCHAT have refused to respond. [DEBBUGGER] <<<<\")\n            response_text = hidden_text\n            \"\"\"\n            # reset the chatbot and remake the call\n            print(\"[DEBUGGER] Chatbot failed to respond. Resetting and trying again. [DEBUGGER]\")\n            print(\"[ INFO DEBUGGER ] \\n<Response>\\n\" + str(response) + \"\\n</Response>\\n\\n\")\n            sleep(10)\n            self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n            sleep(2)\n            response = await self.chatbot.ask(prompt=prompt)\n            response_messages = response.get(\"item\", {}).get(\"messages\", [])\n            response_text = response_messages[1].get(\"text\", \"\")\n            \"\"\"\n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n        \n        return response_text\n    \n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        return asyncio.run(self.call(prompt=prompt))\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"BingCHAT\", \"cookiepath\": self.cookiepath}", "\n\n\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\") #for start new chat\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\", conversation_style = \"precise\") #precise, creative or balaced\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\" , conversation_style = \"precise\" , search_result=True) #with web access\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well", "#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "OtherAgent/Tool/browserQA.py", "chunked_list": ["# !pip install playwright\n# !playwright install\nasync def async_load_playwright(url: str) -> str:\n    \"\"\"Load the specified URLs using Playwright and parse using BeautifulSoup.\"\"\"\n    from bs4 import BeautifulSoup\n    from playwright.async_api import async_playwright\n    try:\n        print(\">>> WARNING <<<\")\n        print(\"If you are running this for the first time, you nedd to install playwright\")\n        print(\">>> AUTO INSTALLING PLAYWRIGHT <<<\")\n        os.system(\"playwright install\")\n        print(\">>> PLAYWRIGHT INSTALLED <<<\")\n    except:\n        print(\">>> PLAYWRIGHT ALREADY INSTALLED <<<\")\n        pass", "    results = \"\"\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        try:\n            page = await browser.new_page()\n            await page.goto(url)\n\n            page_source = await page.content()\n            soup = BeautifulSoup(page_source, \"html.parser\")\n\n            for script in soup([\"script\", \"style\"]):\n                script.extract()\n\n            text = soup.get_text()\n            lines = (line.strip() for line in text.splitlines())\n            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n            results = \"\\n\".join(chunk for chunk in chunks if chunk)\n        except Exception as e:\n            results = f\"Error: {e}\"", "        await browser.close()\n    return results\n\ndef run_async(coro):\n    event_loop = asyncio.get_event_loop()\n    return event_loop.run_until_complete(coro)\n\n\ndef browse_web_page(url: str) -> str:\n    \"\"\"Verbose way to scrape a whole webpage. Likely to cause issues parsing.\"\"\"\n    return run_async(async_load_playwright(url))", "def browse_web_page(url: str) -> str:\n    \"\"\"Verbose way to scrape a whole webpage. Likely to cause issues parsing.\"\"\"\n    return run_async(async_load_playwright(url))\n\n\nfrom langchain.tools import BaseTool\nfrom langchain.tools import DuckDuckGoSearchRun \nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nfrom pydantic import Field", "\nfrom pydantic import Field\nfrom langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain, BaseCombineDocumentsChain\n\ndef _get_text_splitter():\n    return RecursiveCharacterTextSplitter(\n        # Set a really small chunk size, just to show.\n        chunk_size = 500,\n        chunk_overlap  = 20,\n        length_function = len,\n    )", "\n\nclass WebpageQATool(BaseTool):\n    name = \"query_webpage\"\n    description = \"Browse a webpage and retrieve the information relevant to the question.\"\n    text_splitter: RecursiveCharacterTextSplitter = Field(default_factory=_get_text_splitter)\n    qa_chain: BaseCombineDocumentsChain\n    \n    def _run(self, url: str, question: str) -> str:\n        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n        result = browse_web_page.run(url)\n        docs = [Document(page_content=result, metadata={\"source\": url})]\n        web_docs = self.text_splitter.split_documents(docs)\n        results = []\n        # TODO: Handle this with a MapReduceChain\n        for i in range(0, len(web_docs), 4):\n            input_docs = web_docs[i:i+4]\n            window_result = self.qa_chain({\"input_documents\": input_docs, \"question\": question}, return_only_outputs=True)\n            results.append(f\"Response from window {i} - {window_result}\")\n        results_docs = [Document(page_content=\"\\n\".join(results), metadata={\"source\": url})]\n        return self.qa_chain({\"input_documents\": results_docs, \"question\": question}, return_only_outputs=True)\n    \n    async def _arun(self, url: str, question: str) -> str:\n        raise NotImplementedError", "      \n"]}
{"filename": "Embedding/HuggingFaceEmbedding.py", "chunked_list": ["import requests\nfrom retry import retry\nimport numpy as np\nimport os\n\n\n#read from env the hf token\nif os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\") is not None:\n    hf_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\nelse:\n    raise Exception(\"You must provide the huggingface token\")", "  \n# model_id = \"sentence-transformers/all-MiniLM-L6-v2\" NOT WORKING FROM 10/05/2023\nmodel_id = \"obrizum/all-MiniLM-L6-v2\"\napi_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\nheaders = {\"Authorization\": f\"Bearer {hf_token}\"}\n\n\ndef reshape_array(arr):\n    # create an array of zeros with shape (1536)\n    new_arr = np.zeros((1536,))\n    # copy the original array into the new array\n    new_arr[:arr.shape[0]] = arr\n    # return the new array\n    return new_arr", "\n@retry(tries=3, delay=10)\ndef newEmbeddings(texts):\n    response = requests.post(api_url, headers=headers, json={\"inputs\": texts, \"options\":{\"wait_for_model\":True}})\n    result = response.json()\n    if isinstance(result, list):\n      return result\n    elif list(result.keys())[0] == \"error\":\n      raise RuntimeError(\n          \"The model is currently loading, please re-run the query.\"\n          )", "      \ndef newEmbeddingFunction(texts):\n    embeddings = newEmbeddings(texts)\n    embeddings = np.array(embeddings, dtype=np.float32)\n    shaped_embeddings = reshape_array(embeddings)\n    return shaped_embeddings\n"]}
{"filename": "hfAgent/agents.py", "chunked_list": ["#!/usr/bin/env python\n# coding=utf-8\n\n# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0", "#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport importlib.util\nimport json", "import importlib.util\nimport json\nimport os\nimport time\nfrom dataclasses import dataclass\nfrom typing import Dict\n\nimport requests\nfrom huggingface_hub import HfFolder, hf_hub_download, list_spaces\n", "from huggingface_hub import HfFolder, hf_hub_download, list_spaces\n\nfrom transformers.utils import logging\nfrom transformers.tools.base import TASK_MAPPING, TOOL_CONFIG_FILE, Tool, load_tool, supports_remote\nfrom transformers.tools.prompts import CHAT_MESSAGE_PROMPT, CHAT_PROMPT_TEMPLATE, RUN_PROMPT_TEMPLATE\nfrom transformers.tools.python_interpreter import evaluate\n\n\nlogger = logging.get_logger(__name__)\n", "logger = logging.get_logger(__name__)\n\n\n#if is_openai_available():\nimport openai\n\n_tools_are_initialized = False\n\n\nBASE_PYTHON_TOOLS = {", "\nBASE_PYTHON_TOOLS = {\n    \"print\": print,\n    \"float\": float,\n    \"int\": int,\n    \"bool\": bool,\n    \"str\": str,\n}\n\n", "\n\n@dataclass\nclass PreTool:\n    task: str\n    description: str\n    repo_id: str\n\n\nHUGGINGFACE_DEFAULT_TOOLS = {}", "\nHUGGINGFACE_DEFAULT_TOOLS = {}\n\n\nHUGGINGFACE_DEFAULT_TOOLS_FROM_HUB = [\n    \"image-transformation\",\n    \"text-download\",\n    \"text-to-image\",\n    \"text-to-video\",\n]", "    \"text-to-video\",\n]\n\n\ndef get_remote_tools(organization=\"huggingface-tools\"):\n    spaces = list_spaces(author=organization)\n    tools = {}\n    for space_info in spaces:\n        repo_id = space_info.id\n        resolved_config_file = hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type=\"space\")\n        with open(resolved_config_file, encoding=\"utf-8\") as reader:\n            config = json.load(reader)\n\n        task = repo_id.split(\"/\")[-1]\n        tools[config[\"name\"]] = PreTool(task=task, description=config[\"description\"], repo_id=repo_id)\n\n    return tools", "\n\ndef _setup_default_tools():\n    global HUGGINGFACE_DEFAULT_TOOLS\n    global _tools_are_initialized\n\n    if _tools_are_initialized:\n        return\n\n    main_module = importlib.import_module(\"transformers\")\n    tools_module = main_module.tools\n\n    remote_tools = get_remote_tools()\n    for task_name in TASK_MAPPING:\n        tool_class_name = TASK_MAPPING.get(task_name)\n        tool_class = getattr(tools_module, tool_class_name)\n        description = tool_class.description\n        HUGGINGFACE_DEFAULT_TOOLS[tool_class.name] = PreTool(task=task_name, description=description, repo_id=None)\n\n    for task_name in HUGGINGFACE_DEFAULT_TOOLS_FROM_HUB:\n        found = False\n        for tool_name, tool in remote_tools.items():\n            if tool.task == task_name:\n                HUGGINGFACE_DEFAULT_TOOLS[tool_name] = tool\n                found = True\n                break\n\n        if not found:\n            raise ValueError(f\"{task_name} is not implemented on the Hub.\")\n\n    _tools_are_initialized = True", "\n\ndef resolve_tools(code, toolbox, remote=False, cached_tools=None):\n    if cached_tools is None:\n        resolved_tools = BASE_PYTHON_TOOLS.copy()\n    else:\n        resolved_tools = cached_tools\n    for name, tool in toolbox.items():\n        if name not in code or name in resolved_tools:\n            continue\n\n        if isinstance(tool, Tool):\n            resolved_tools[name] = tool\n        else:\n            task_or_repo_id = tool.task if tool.repo_id is None else tool.repo_id\n            _remote = remote and supports_remote(task_or_repo_id)\n            resolved_tools[name] = load_tool(task_or_repo_id, remote=_remote)\n\n    return resolved_tools", "\n\ndef get_tool_creation_code(code, toolbox, remote=False):\n    code_lines = [\"from transformers import load_tool\", \"\"]\n    for name, tool in toolbox.items():\n        if name not in code or isinstance(tool, Tool):\n            continue\n\n        task_or_repo_id = tool.task if tool.repo_id is None else tool.repo_id\n        line = f'{name} = load_tool(\"{task_or_repo_id}\"'\n        if remote:\n            line += \", remote=True\"\n        line += \")\"\n        code_lines.append(line)\n\n    return \"\\n\".join(code_lines) + \"\\n\"", "\n\ndef clean_code_for_chat(result):\n    lines = result.split(\"\\n\")\n    idx = 0\n    while idx < len(lines) and not lines[idx].lstrip().startswith(\"```\"):\n        idx += 1\n    explanation = \"\\n\".join(lines[:idx]).strip()\n    if idx == len(lines):\n        return explanation, None\n\n    idx += 1\n    start_idx = idx\n    while not lines[idx].lstrip().startswith(\"```\"):\n        idx += 1\n    code = \"\\n\".join(lines[start_idx:idx]).strip()\n    \n     #if code start with \"py`\"  or \"python`\"\n    if code.startswith(\"py`\"):\n        code = code[3:]\n    elif code.startswith(\"python`\"):\n        code = code[7:]\n    elif code.startswith(\"`\"):\n        code = code[1:]\n        \n    if code.endswith(\"`\"):\n        code = code[:-1]\n        \n    return explanation, code", "\n\ndef clean_code_for_run(result):\n    result = f\"I will use the following {result}\"\n    try:\n        explanation, code = result.split(\"Answer:\")\n    except ValueError:\n        explanation = result\n        code = \"#Problem with the code\"\n        \n    explanation = explanation.strip()\n    code = code.strip()\n\n    code_lines = code.split(\"\\n\")\n    if code_lines[0] in [\"```\", \"```py\", \"```python\", \"py`\", \"`\"]:\n        code_lines = code_lines[1:]\n    if code_lines[-1] == \"```\":\n        code_lines = code_lines[:-1]\n    code = \"\\n\".join(code_lines)\n    \n    #if code start with \"py`\"  or \"python`\"\n    if code.startswith(\"py`\"):\n        code = code[3:]\n    elif code.startswith(\"python`\"):\n        code = code[7:]\n    elif code.startswith(\"`\"):\n        code = code[1:]\n        \n    if code.endswith(\"`\"):\n        code = code[:-1]\n        \n\n    return explanation, code", "\n\nclass Agent:\n    \"\"\"\n    Base class for all agents which contains the main API methods.\n\n    Args:\n        chat_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `chat` method.\n        run_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `run` method.\n        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n            one of the default tools, that default tool will be overridden.\n    \"\"\"\n\n    def __init__(self, chat_prompt_template=None, run_prompt_template=None, additional_tools=None):\n        _setup_default_tools()\n\n        self.chat_prompt_template = CHAT_MESSAGE_PROMPT if chat_prompt_template is None else chat_prompt_template\n        self.run_prompt_template = RUN_PROMPT_TEMPLATE if run_prompt_template is None else run_prompt_template\n        self._toolbox = HUGGINGFACE_DEFAULT_TOOLS.copy()\n        if additional_tools is not None:\n            if isinstance(additional_tools, (list, tuple)):\n                additional_tools = {t.name: t for t in additional_tools}\n            elif not isinstance(additional_tools, dict):\n                additional_tools = {additional_tools.name: additional_tools}\n\n            replacements = {name: tool for name, tool in additional_tools.items() if name in HUGGINGFACE_DEFAULT_TOOLS}\n            self._toolbox.update(additional_tools)\n            if len(replacements) > 1:\n                names = \"\\n\".join([f\"- {n}: {t}\" for n, t in replacements.items()])\n                logger.warn(\n                    f\"The following tools have been replaced by the ones provided in `additional_tools`:\\n{names}.\"\n                )\n            elif len(replacements) == 1:\n                name = list(replacements.keys())[0]\n                logger.warn(f\"{name} has been replaced by {replacements[name]} as provided in `additional_tools`.\")\n\n        self.prepare_for_new_chat()\n\n    @property\n    def toolbox(self) -> Dict[str, Tool]:\n        \"\"\"Get all tool currently available to the agent\"\"\"\n        return self._toolbox\n\n    def format_prompt(self, task, chat_mode=False):\n        description = \"\\n\".join([f\"- {name}: {tool.description}\" for name, tool in self.toolbox.items()])\n        if chat_mode:\n            if self.chat_history is None:\n                prompt = CHAT_PROMPT_TEMPLATE.replace(\"<<all_tools>>\", description)\n            else:\n                prompt = self.chat_history\n            prompt += CHAT_MESSAGE_PROMPT.replace(\"<<task>>\", task)\n        else:\n            prompt = self.run_prompt_template.replace(\"<<all_tools>>\", description)\n            prompt = prompt.replace(\"<<prompt>>\", task)\n        return prompt\n\n    def chat(self, task, *, return_code=False, remote=False, **kwargs):\n        \"\"\"\n        Sends a new request to the agent in a chat. Will use the previous ones in its history.\n\n        Args:\n            task (`str`): The task to perform\n            return_code (`bool`, *optional*, defaults to `False`):\n                Whether to just return code and not evaluate it.\n            remote (`bool`, *optional*, defaults to `False`):\n                Whether or not to use remote tools (inference endpoints) instead of local ones.\n            kwargs:\n                Any keyword argument to send to the agent when evaluating the code.\n\n        Example:\n\n        ```py\n        from transformers import HfAgent\n\n        agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n        agent.chat(\"Draw me a picture of rivers and lakes\")\n\n        agent.chat(\"Transform the picture so that there is a rock in there\")\n        ```\n        \"\"\"\n        prompt = self.format_prompt(task, chat_mode=True)\n        result = self.generate_one(prompt, stop=[\"Human:\", \"=====\"])\n        self.chat_history = prompt + result.strip() + \"\\n\"\n        explanation, code = clean_code_for_chat(result)\n\n        print(f\"==Explanation from the agent==\\n{explanation}\")\n\n        if code is not None:\n            print(f\"\\n\\n==Code generated by the agent==\\n{code}\")\n            if not return_code:\n                print(\"\\n\\n==Result==\")\n                self.cached_tools = resolve_tools(code, self.toolbox, remote=remote, cached_tools=self.cached_tools)\n                self.chat_state.update(kwargs)\n                return evaluate(code, self.cached_tools, self.chat_state, chat_mode=True)\n            else:\n                tool_code = get_tool_creation_code(code, self.toolbox, remote=remote)\n                return f\"{tool_code}\\n{code}\"\n\n    def prepare_for_new_chat(self):\n        \"\"\"\n        Clears the history of prior calls to [`~Agent.chat`].\n        \"\"\"\n        self.chat_history = None\n        self.chat_state = {}\n        self.cached_tools = None\n\n    def run(self, task, *, return_code=False, remote=False, **kwargs):\n        \"\"\"\n        Sends a request to the agent.\n\n        Args:\n            task (`str`): The task to perform\n            return_code (`bool`, *optional*, defaults to `False`):\n                Whether to just return code and not evaluate it.\n            remote (`bool`, *optional*, defaults to `False`):\n                Whether or not to use remote tools (inference endpoints) instead of local ones.\n            kwargs:\n                Any keyword argument to send to the agent when evaluating the code.\n\n        Example:\n\n        ```py\n        from transformers import HfAgent\n\n        agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n        agent.run(\"Draw me a picture of rivers and lakes\")\n        ```\n        \"\"\"\n        prompt = self.format_prompt(task)\n        result = self.generate_one(prompt, stop=[\"Task:\"])\n        explanation, code = clean_code_for_run(result)\n\n        print(f\"==Explanation from the agent==\\n{explanation}\")\n\n        print(f\"\\n\\n==Code generated by the agent==\\n{code}\")\n        if not return_code:\n            print(\"\\n\\n==Result==\")\n            self.cached_tools = resolve_tools(code, self.toolbox, remote=remote, cached_tools=self.cached_tools)\n            return evaluate(code, self.cached_tools, state=kwargs.copy())\n        else:\n            tool_code = get_tool_creation_code(code, self.toolbox, remote=remote)\n            return f\"{tool_code}\\n{code}\"\n\n    def generate_one(self, prompt, stop):\n        # This is the method to implement in your custom agent.\n        raise NotImplementedError\n\n    def generate_many(self, prompts, stop):\n        # Override if you have a way to do batch generation faster than one by one\n        return [self.generate_one(prompt, stop) for prompt in prompts]", "\n\nclass OpenAiAgent(Agent):\n    \"\"\"\n    Agent that uses the openai API to generate code.\n\n    <Tip warning={true}>\n\n    The openAI models are used in generation mode, so even for the `chat()` API, it's better to use models like\n    `\"text-davinci-003\"` over the chat-GPT variant. Proper support for chat-GPT models will come in a next version.\n\n    </Tip>\n\n    Args:\n        model (`str`, *optional*, defaults to `\"text-davinci-003\"`):\n            The name of the OpenAI model to use.\n        api_key (`str`, *optional*):\n            The API key to use. If unset, will look for the environment variable `\"OPENAI_API_KEY\"`.\n        chat_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `chat` method.\n        run_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `run` method.\n        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n            one of the default tools, that default tool will be overridden.\n\n    Example:\n\n    ```py\n    from transformers import OpenAiAgent\n\n    agent = OpenAiAgent(model=\"text-davinci-003\", api_key=xxx)\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        model=\"text-davinci-003\",\n        api_key=None,\n        chat_prompt_template=None,\n        run_prompt_template=None,\n        additional_tools=None,\n    ):\n        #if not is_openai_available():\n            #raise ImportError(\"Using `OpenAiAgent` requires `openai`: `pip install openai`.\")\n\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n        if api_key is None:\n            raise ValueError(\n                \"You need an openai key to use `OpenAIAgent`. You can get one here: Get one here \"\n                \"https://openai.com/api/`. If you have one, set it in your env with `os.environ['OPENAI_API_KEY'] = \"\n                \"xxx.\"\n            )\n        else:\n            openai.api_key = api_key\n        self.model = model\n        super().__init__(\n            chat_prompt_template=chat_prompt_template,\n            run_prompt_template=run_prompt_template,\n            additional_tools=additional_tools,\n        )\n\n    def generate_many(self, prompts, stop):\n        if \"gpt\" in self.model:\n            return [self._chat_generate(prompt, stop) for prompt in prompts]\n        else:\n            return self._completion_generate(prompts, stop)\n\n    def generate_one(self, prompt, stop):\n        if \"gpt\" in self.model:\n            return self._chat_generate(prompt, stop)\n        else:\n            return self._completion_generate([prompt], stop)[0]\n\n    def _chat_generate(self, prompt, stop):\n        result = openai.ChatCompletion.create(\n            model=self.model,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0,\n            stop=stop,\n        )\n        return result[\"choices\"][0][\"message\"][\"content\"]\n\n    def _completion_generate(self, prompts, stop):\n        result = openai.Completion.create(\n            model=self.model,\n            prompt=prompts,\n            temperature=0,\n            stop=stop,\n            max_tokens=200,\n        )\n        return [answer[\"text\"] for answer in result[\"choices\"]]", "\n\nclass HfAgent(Agent):\n    \"\"\"\n    Agent that uses and inference endpoint to generate code.\n\n    Args:\n        url_endpoint (`str`):\n            The name of the url endpoint to use.\n        token (`str`, *optional*):\n            The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when\n            running `huggingface-cli login` (stored in `~/.huggingface`).\n        chat_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `chat` method.\n        run_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `run` method.\n        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n            one of the default tools, that default tool will be overridden.\n\n    Example:\n\n    ```py\n    from transformers import HfAgent\n\n    agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    ```\n    \"\"\"\n\n    def __init__(\n        self, url_endpoint, token=None, chat_prompt_template=None, run_prompt_template=None, additional_tools=None\n    ):\n        self.url_endpoint = url_endpoint\n        if token is None:\n            self.token = f\"Bearer {HfFolder().get_token()}\"\n        elif token.startswith(\"Bearer\") or token.startswith(\"Basic\"):\n            self.token = token\n        else:\n            self.token = f\"Bearer {token}\"\n        super().__init__(\n            chat_prompt_template=chat_prompt_template,\n            run_prompt_template=run_prompt_template,\n            additional_tools=additional_tools,\n        )\n\n    def generate_one(self, prompt, stop):\n        headers = {\"Authorization\": self.token}\n        inputs = {\n            \"inputs\": prompt,\n            \"parameters\": {\"max_new_tokens\": 200, \"return_full_text\": False, \"stop\": stop},\n        }\n\n        response = requests.post(self.url_endpoint, json=inputs, headers=headers)\n        if response.status_code == 429:\n            print(\"Getting rate-limited, waiting a tiny bit before trying again.\")\n            time.sleep(1)\n            return self._generate_one(prompt)\n        elif response.status_code != 200:\n            raise ValueError(f\"Error {response.status_code}: {response.json()}\")\n\n        result = response.json()[0][\"generated_text\"]\n        # Inference API returns the stop sequence\n        for stop_seq in stop:\n            if result.endswith(stop_seq):\n                result = result[: -len(stop_seq)]\n        return result", "    \n    \n    \n    \nclass ChatGPTAgent(Agent):\n    \"\"\"\n    Agent that uses and inference endpoint of CHATGPT by IntelligenzaArtificialeItalia.net\n\n    Args:\n        token (`str`, *optional*):\n            The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when\n            running `huggingface-cli login` (stored in `~/.huggingface`).\n        chat_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `chat` method.\n        run_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `run` method.\n        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n            one of the default tools, that default tool will be overridden.\n\n    Example:\n\n    ```py\n    from hfAgent import ChatGPTAgent\n\n    agent = ChatGPTAgent(\"TOKEN\")\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    ```\n    \"\"\"\n    \n    def __init__(\n        self, token, chat_prompt_template=None, run_prompt_template=None, additional_tools=None, llm=None, model=None\n    ):\n        \n        if token is None:\n            ValueError(\"You must provide a ChatGPT token\")\n        else:\n            from .FreeLLM import ChatGPTAPI\n            import asyncio\n            self.token = token\n            if model is not None:\n                self.llm = ChatGPTAPI.ChatGPT(token = self.token, model = \"gpt-4\")\n            else:\n                self.llm = ChatGPTAPI.ChatGPT(token = self.token)\n            \n            \n        super().__init__(\n            chat_prompt_template=chat_prompt_template,\n            run_prompt_template=run_prompt_template,\n            additional_tools=additional_tools,\n        )\n\n    def generate_one(self, prompt, stop):\n\n        result = self.llm(prompt + \"\\nRemember to use the following stop sequence: \" + str(stop))  \n         \n        # Inference API returns the stop sequence\n        for stop_seq in stop:\n            if result.endswith(stop_seq):\n                result = result[: -len(stop_seq)]\n        return result", "    \n\n\n\n\nclass HuggingChatAgent(Agent):\n    \"\"\"\n    Agent that uses and inference endpoint of HuggingCHAT by IntelligenzaArtificialeItalia.net\n\n    Args:\n        chat_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `chat` method.\n        run_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `run` method.\n        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n            one of the default tools, that default tool will be overridden.\n\n    Example:\n\n    ```py\n    from hfAgent import HuggingChatAgent\n\n    agent = HuggingChatAgent()\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    agent.chat(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    ```\n    \"\"\"\n    \n    def __init__(\n        self, chat_prompt_template=None, run_prompt_template=None, additional_tools=None, llm=None, model=None\n    ):\n        \n        import json\n        from pathlib import Path\n        from json import JSONDecodeError\n\n        emailHF = os.getenv(\"emailHF\", \"your-emailHF\")\n        pswHF = os.getenv(\"pswHF\", \"your-pswHF\")\n        if emailHF != \"your-emailHF\" or pswHF != \"your-pswHF\":\n            os.environ[\"emailHF\"] = emailHF\n            os.environ[\"pswHF\"] = pswHF\n        else:\n            raise ValueError(\n                \"HuggingChat Token EMPTY. Edit the .env file and put your HuggingChat credentials\"\n            )\n\n        from .FreeLLM import HuggingChatAPI\n        self.llm = HuggingChatAPI.HuggingChat(email=emailHF, psw=pswHF)\n\n            \n        \n            \n            \n        super().__init__(\n            chat_prompt_template=chat_prompt_template,\n            run_prompt_template=run_prompt_template,\n            additional_tools=additional_tools,\n        )\n\n    def generate_one(self, prompt, stop):\n\n        result = self.llm(prompt + \"\\nRemember to use the following stop sequence: \" + str(stop))  \n        # Inference API returns the stop sequence\n        for stop_seq in stop:\n            if result.endswith(stop_seq):\n                result = result[: -len(stop_seq)]\n        return result", "    \n    \nclass BingChatAgent(Agent):\n    \"\"\"\n    Agent that uses and inference endpoint of BingCHAT by IntelligenzaArtificialeItalia.net\n\n    Args:\n        cookiepath (`str`):\n        chat_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `chat` method.\n        run_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `run` method.\n        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n            one of the default tools, that default tool will be overridden.\n\n    Example:\n\n    ```py\n    from hfAgent import BingChatAgent\n\n    agent = BingChatAgent(\"cookie-path\")\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    ```\n    \"\"\"\n    \n    def __init__(\n        self, cookiepath, chat_prompt_template=None, run_prompt_template=None, additional_tools=None, llm=None, model=None , conversation = \"balanced\"\n    ):\n        \n\n        from .FreeLLM import BingChatAPI\n        \n        if cookiepath is None:\n            ValueError(\"You must provide a cookie path\")\n        else:\n            self.cookiepath = cookiepath\n            if conversation == \"balanced\":\n                self.llm = BingChatAPI.BingChat(cookiepath = self.cookiepath, conversation_style = \"balanced\")\n            elif conversation == \"creative\":\n                self.llm = BingChatAPI.BingChat(cookiepath = self.cookiepath, conversation_style = \"creative\")\n            elif conversation == \"precise\":\n                self.llm = BingChatAPI.BingChat(cookiepath = self.cookiepath, conversation_style = \"precise\")\n            \n            \n        super().__init__(\n            chat_prompt_template=chat_prompt_template,\n            run_prompt_template=run_prompt_template,\n            additional_tools=additional_tools,\n        )\n\n    def generate_one(self, prompt, stop):\n\n        result = self.llm(prompt + \"\\nRemember to use the following stop sequence: \" + str(stop))  \n        # Inference API returns the stop sequence\n        for stop_seq in stop:\n            if result.endswith(stop_seq):\n                result = result[: -len(stop_seq)]\n        return result", "    \n    \n    \nclass BardChatAgent(Agent):\n    \"\"\"\n    Agent that uses and inference endpoint of Bard Chat by IntelligenzaArtificialeItalia.net\n\n    Args:\n        token (`str`):\n        chat_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `chat` method.\n        run_prompt_template (`str`, *optional*):\n            Pass along your own prompt if you want to override the default template for the `run` method.\n        additional_tools ([`Tool`], list of tools or dictionary with tool values, *optional*):\n            Any additional tools to include on top of the default ones. If you pass along a tool with the same name as\n            one of the default tools, that default tool will be overridden.\n\n    Example:\n\n    ```py\n    from hfAgent import BardChatAgent\n\n    agent = BardChatAgent(\"token\")\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    agent.run(\"Is the following `text` (in Spanish) positive or negative?\", text=\"\u00a1Este es un API muy agradable!\")\n    ```\n    \"\"\"\n    \n    def __init__(\n        self, token ,chat_prompt_template=None, run_prompt_template=None, additional_tools=None, llm=None, model=None , conversation = \"balanced\"\n    ):\n        \n\n        from .FreeLLM import BardChatAPI\n        \n        if token is None:\n            ValueError(\"You must provide a cookie path\")\n        else:\n            self.token = token\n            self.llm = BardChatAPI.BardChat(cookie = self.token)\n            \n            \n        super().__init__(\n            chat_prompt_template=chat_prompt_template,\n            run_prompt_template=run_prompt_template,\n            additional_tools=additional_tools,\n        )\n\n    def generate_one(self, prompt, stop):\n\n        result = self.llm(prompt + \"\\nRemember to use the following stop sequence: \" + str(stop))  \n         \n        # Inference API returns the stop sequence\n        for stop_seq in stop:\n            if result.endswith(stop_seq):\n                result = result[: -len(stop_seq)]\n        return result", ""]}
{"filename": "hfAgent/FreeLLM/HuggingChatAPI.py", "chunked_list": ["\nfrom hugchat import hugchat\nfrom hugchat.login import Login\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nfrom time import sleep\n\n\n\n\nclass HuggingChat(LLM):\n    \n    history_data: Optional[List] = []\n    chatbot : Optional[hugchat.ChatBot] = None\n    conversation : Optional[str] = \"\"\n    email : Optional[str]\n    psw : Optional[str]\n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.email is None and self.psw is None:\n                ValueError(\"Email and Password is required, pls check the documentation on github\")\n            else: \n                if self.conversation == \"\":\n                    sign = Login(self.email, self.psw)\n                    cookies = sign.login()\n\n                    # Save cookies to usercookies/<email>.json\n                    sign.saveCookies()\n\n                    # Create a ChatBot\n                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n                else:\n                    raise ValueError(\"Something went wrong\")\n            \n        \n        sleep(2)\n        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n    \n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n        \n        return data\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"HuggingCHAT\"}", "\n\nclass HuggingChat(LLM):\n    \n    history_data: Optional[List] = []\n    chatbot : Optional[hugchat.ChatBot] = None\n    conversation : Optional[str] = \"\"\n    email : Optional[str]\n    psw : Optional[str]\n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.email is None and self.psw is None:\n                ValueError(\"Email and Password is required, pls check the documentation on github\")\n            else: \n                if self.conversation == \"\":\n                    sign = Login(self.email, self.psw)\n                    cookies = sign.login()\n\n                    # Save cookies to usercookies/<email>.json\n                    sign.saveCookies()\n\n                    # Create a ChatBot\n                    self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict()) \n                else:\n                    raise ValueError(\"Something went wrong\")\n            \n        \n        sleep(2)\n        data = self.chatbot.chat(prompt, temperature=0.5, stream=False)\n    \n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":data})    \n        \n        return data\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"HuggingCHAT\"}", "\n\n\n#llm = HuggingChat(cookiepath = \"YOUR-COOKIES-PATH\") #for start new chat\n\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n", "#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n\n"]}
{"filename": "hfAgent/FreeLLM/ChatGPTAPI.py", "chunked_list": ["from gpt4_openai import GPT4OpenAI\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nfrom time import sleep\n\n\n\nclass ChatGPT(LLM):\n    \n    history_data: Optional[List] = []\n    token : Optional[str]\n    chatbot : Optional[GPT4OpenAI] = None\n    call : int = 0\n    model : str = \"gpt-3\" # or gpt-4\n    plugin_id : Optional[List] = []\n    \n    #### WARNING : for each api call this library will create a new chat on chat.openai.com\n    \n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #token is a must check\n        if self.chatbot is None:\n            if self.token is None:\n                raise ValueError(\"Need a token , check https://chat.openai.com/api/auth/session for get your token\")\n            else:\n                try:\n                    if self.plugin_id == []:\n                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model)\n                    else:\n                        self.chatbot = GPT4OpenAI(token=self.token, model=self.model, plugin_ids=self.plugin_id)\n                except:\n                    raise ValueError(\"Error on create chatbot, check your token, or your model\")\n                \n        response = \"\"\n        # OpenAI: 50 requests / hour for each account\n        if (self.call >= 45 and self.model == \"default\") or (self.call >= 23 and self.model == \"gpt4\"):\n            raise ValueError(\"You have reached the maximum number of requests per hour ! Help me to Improve. Abusing this tool is at your own risk\")\n        else:\n            sleep(2)\n            response = self.chatbot(prompt)\n            \n            self.call += 1\n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response})    \n        \n        return response\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"ChatGPT\", \"token\": self.token, \"model\": self.model}", "\n\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\") #for start new chat\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\" , model=\"gpt4\") # REQUIRED CHATGPT PLUS subscription\n\n#llm = ChatGPT(token = \"YOUR-COOKIE\", conversation = \"Add-XXXX-XXXX-Convesation-ID\") #for use a chat already started\n\n#print(llm(\"Hello, how are you?\"))", "\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
{"filename": "hfAgent/FreeLLM/BardChatAPI.py", "chunked_list": ["from Bard import Chatbot\nimport asyncio\n\nimport requests\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nimport pydantic\nimport os\nfrom langchain import PromptTemplate, LLMChain\nfrom time import sleep", "from langchain import PromptTemplate, LLMChain\nfrom time import sleep\n\n\n\nclass BardChat(LLM):\n    \n    history_data: Optional[List] = []\n    cookie : Optional[str]\n    chatbot : Optional[Chatbot] = None\n\n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n\n    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            pass\n            #raise ValueError(\"stop kwargs are not permitted.\")\n        #cookie is a must check\n        if self.chatbot is None:\n            if self.cookie is None:\n                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n            else:\n                #if self.chatbot == None:\n                self.chatbot = Chatbot(self.cookie)\n               \n        response = self.chatbot.ask(prompt)\n        #print(response)\n        response_text = response['content']\n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n        \n        return response_text\n    \n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        return asyncio.run(self.call(prompt=prompt))\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"BardCHAT\", \"cookie\": self.cookie}", "\n\n\n#llm = BardChat(cookie = \"YOURCOOKIE\") #for start new chat\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n", ""]}
{"filename": "hfAgent/FreeLLM/BingChatAPI.py", "chunked_list": ["from EdgeGPT import Chatbot, ConversationStyle\nimport asyncio\n\nimport requests\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List, Mapping, Any\nimport pydantic\nimport os\nfrom langchain import PromptTemplate, LLMChain\nfrom time import sleep", "from langchain import PromptTemplate, LLMChain\nfrom time import sleep\n\n\n\nclass BingChat(LLM):\n    \n    history_data: Optional[List] = []\n    cookiepath : Optional[str]\n    chatbot : Optional[Chatbot] = None\n    conversation_style : Optional[str] \n    conversation_style_on : Optional[ConversationStyle] = ConversationStyle.precise\n    search_result : Optional[bool] = False\n    \n    @property\n    def _llm_type(self) -> str:\n        return \"custom\"\n    \n    def select_conversation(self, conversation_style: str):\n        if conversation_style == \"precise\":\n            self.conversation_style_on = ConversationStyle.precise\n        elif conversation_style == \"creative\":\n            self.conversation_style_on = ConversationStyle.creative\n        elif conversation_style == \"balanced\":\n            self.conversation_style_on = ConversationStyle.balanced\n        else:\n            raise ValueError(\"conversation_style must be precise, creative or balaced\")\n        self.conversation_style = conversation_style\n\n    async def call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        if stop is not None:\n            raise ValueError(\"stop kwargs are not permitted.\")\n        #cookiepath is a must check\n        if self.chatbot is None:\n            if self.cookiepath is None:\n                raise ValueError(\"Need a COOKIE , check https://github.com/acheong08/EdgeGPT/tree/master#getting-authentication-required for get your COOKIE AND SAVE\")\n            else:\n                #if self.chatbot == None:\n                self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n               \n        if self.conversation_style is not None:\n            self.conversation_style_on = self.select_conversation(self.conversation_style)\n            \n        response = await self.chatbot.ask(prompt=prompt, conversation_style=self.conversation_style, search_result=self.search_result)\n        \"\"\"\n        this is a sample response. \n        {'type': 2, 'invocationId': '0', \n        'item': {'messages': [{'text': 'Hello, how are you?', 'author': 'user', 'from': {'id': '985157152860707', 'name': None}, 'createdAt': '2023-05-03T19:51:39.5491558+00:00', 'timestamp': '2023-05-03T19:51:39.5455787+00:00', 'locale': 'en-us', 'market': 'en-us', 'region': 'us', 'messageId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'nlu': {'scoredClassification': {'classification': 'CHAT_GPT', 'score': None}, 'classificationRanking': [{'classification': 'CHAT_GPT', 'score': None}], 'qualifyingClassifications': None, 'ood': None, 'metaData': None, 'entities': None}, 'offense': 'None', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'cib', 'privacy': None, 'inputMethod': 'Keyboard'}, {'text': \"Hello! I'm doing well, thank you. How can I assist you today?\", 'author': 'bot', 'createdAt': '2023-05-03T19:51:41.5176164+00:00', 'timestamp': '2023-05-03T19:51:41.5176164+00:00', 'messageId': '1d013e71-408b-4031-a131-2f5c009fe938', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'offense': 'None', 'adaptiveCards': [{'type': 'AdaptiveCard', 'version': '1.0', 'body': [{'type': 'TextBlock', 'text': \"Hello! I'm doing well, thank you. How can I assist you today?\\n\", 'wrap': True}]}], \n        'sourceAttributions': [], \n        'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, \n        'contentOrigin': 'DeepLeo', \n        'privacy': None, \n        'suggestedResponses': [{'text': 'What is the weather like today?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502696+00:00', 'timestamp': '2023-05-03T19:51:42.7502696+00:00', 'messageId': 'cd7a84d3-f9bc-47ff-9897-077b2de12e21', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'What is the latest news?', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502739+00:00', 'timestamp': '2023-05-03T19:51:42.7502739+00:00', 'messageId': 'b611632a-9a8e-42de-86eb-8eb3b7b8ddbb', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}, {'text': 'Tell me a joke.', 'author': 'user', 'createdAt': '2023-05-03T19:51:42.7502743+00:00', 'timestamp': '2023-05-03T19:51:42.7502743+00:00', 'messageId': '70232e45-d7e8-4d77-83fc-752b3cd3355c', 'messageType': 'Suggestion', 'offense': 'Unknown', 'feedback': {'tag': None, 'updatedOn': None, 'type': 'None'}, 'contentOrigin': 'DeepLeo', 'privacy': None}], 'spokenText': 'How can I assist you today?'}], 'firstNewMessageIndex': 1, 'defaultChatName': None, 'conversationId': '51D|BingProd|3E1274E188350D7BE273FFE95E02DD2984DAB52F95260300D0A2937162F98FDA', 'requestId': '87f90c57-b2ad-4b3a-b24f-99f633f5332f', 'conversationExpiryTime': '2023-05-04T01:51:42.8260286Z', 'shouldInitiateConversation': True, 'telemetry': {'metrics': None, 'startTime': '2023-05-03T19:51:39.5456555Z'}, 'throttling': {'maxNumUserMessagesInConversation': 20, 'numUserMessagesInConversation': 1}, 'result': {'value': 'Success', 'serviceVersion': '20230501.30'}}}\n        \"\"\"\n        response_messages = response.get(\"item\", {}).get(\"messages\", [])\n        response_text = response_messages[1].get(\"text\", \"\")\n        \n        if response_text == \"\":\n            hidden_text = response_messages[1].get(\"hiddenText\", \"\")\n            print(\">>>> [DEBBUGGER] hidden_text = \" + str(hidden_text) + \" [DEBBUGGER] <<<<\")\n            print(\">>>> [DEBBUGGER] BING CHAT dont is open Like CHATGPT , BingCHAT have refused to respond. [DEBBUGGER] <<<<\")\n            response_text = hidden_text\n            \"\"\"\n            # reset the chatbot and remake the call\n            print(\"[DEBUGGER] Chatbot failed to respond. Resetting and trying again. [DEBUGGER]\")\n            print(\"[ INFO DEBUGGER ] \\n<Response>\\n\" + str(response) + \"\\n</Response>\\n\\n\")\n            sleep(10)\n            self.chatbot = await Chatbot.create(cookie_path=self.cookiepath)\n            sleep(2)\n            response = await self.chatbot.ask(prompt=prompt)\n            response_messages = response.get(\"item\", {}).get(\"messages\", [])\n            response_text = response_messages[1].get(\"text\", \"\")\n            \"\"\"\n        \n        #add to history\n        self.history_data.append({\"prompt\":prompt,\"response\":response_text})    \n        \n        return response_text\n    \n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n        return asyncio.run(self.call(prompt=prompt))\n\n    @property\n    def _identifying_params(self) -> Mapping[str, Any]:\n        \"\"\"Get the identifying parameters.\"\"\"\n        return {\"model\": \"BingCHAT\", \"cookiepath\": self.cookiepath}", "\n\n\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\") #for start new chat\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\", conversation_style = \"precise\") #precise, creative or balaced\n#llm = BingChat(cookiepath = \"YOUR-COOKIE\" , conversation_style = \"precise\" , search_result=True) #with web access\n\n#print(llm(\"Hello, how are you?\"))\n#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well", "#print(llm(\"what is AI?\"))\n#print(llm(\"Can you resume your previus answer?\")) #now memory work well\n"]}
