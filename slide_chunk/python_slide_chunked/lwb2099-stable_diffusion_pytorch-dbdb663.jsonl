{"filename": "train_unet.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   train.py\n@Time    :   2023/05/12 19:24:02\n@Author  :   Wenbo Li\n@Desc    :   Main Class for training stable diffusion model\n\"\"\"\n\n", "\n\nimport math\nimport os\nimport shutil\nimport time\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import (", "from accelerate.logging import get_logger\nfrom accelerate.utils import (\n    set_seed,\n    DummyOptim,\n    ProjectConfiguration,\n    DummyScheduler,\n    DeepSpeedPlugin,\n)\nfrom transformers import get_scheduler\nimport logging", "from transformers import get_scheduler\nimport logging\nfrom stable_diffusion.models.latent_diffusion import LatentDiffusion\nfrom utils.model_utils import build_models\nfrom utils.parse_args import load_config\nfrom utils.prepare_dataset import collate_fn, detransform, get_dataset, to_img\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm", "import torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, Dataset\nfrom diffusers import AutoencoderKL\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\n# build environment\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\nos.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"", "os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\nos.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"\nlogger = get_logger(__name__)\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    level=logging.INFO,\n)\n\n\nclass StableDiffusionTrainer:\n    def __init__(\n        self,\n        model: LatentDiffusion,\n        args,\n        cfg,\n        train_dataset,\n        eval_dataset,\n        collate_fn,\n    ):\n        # check params\n        assert train_dataset is not None, \"must specify an training dataset\"\n        assert (\n            eval_dataset is not None and cfg.train.log_interval > 0\n        ), \"if passed log_interval > 0, you must specify an evaluation dataset\"\n        self.model: LatentDiffusion = model\n        # test autoencoder\n        self.model.autoencoder = AutoencoderKL.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\",\n            subfolder=\"vae\",\n            cache_dir=\"data/pretrained\",\n        )\n        self.model.autoencoder.requires_grad_(False)\n        self.cfg = cfg\n        self.train_dataset: Dataset = train_dataset\n        self.eval_dataset: Dataset = eval_dataset\n        self.last_ckpt = None\n        # * 1. init accelerator\n        accelerator_log_kwcfg = {}\n        if cfg.log.with_tracking:\n            try:\n                accelerator_log_kwcfg[\"log_with\"] = cfg.log.report_to\n            except AttributeError:\n                print(\"need to specify report_to when passing in with_tracking=True\")\n            accelerator_log_kwcfg[\"logging_dir\"] = cfg.log.logging_dir\n\n        accelerator_project_config = ProjectConfiguration()\n        # check deepspeed\n        if cfg.train.use_deepspeed is True:\n            try:\n                import deepspeed\n            except ImportError as e:\n                raise ImportError(\n                    'You passed use_deepspeed=True, please install deepspeed by running `pip install deepspeed`, also deepspeed requies a matched cuda version, so you may need to run `conda install -c \"nvidia/label/cuda-11.5.0\" cuda-toolkit`, see https://anaconda.org/nvidia/cuda-toolkit for more options'\n                ) from e\n\n        self.accelerator = Accelerator(\n            gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n            **accelerator_log_kwcfg,\n            project_config=accelerator_project_config,\n            deepspeed_plugin=DeepSpeedPlugin(\n                zero_stage=2,\n                gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n                gradient_clipping=cfg.optim.max_grad_norm,\n                offload_optimizer_device=\"cpu\",\n                offload_param_device=\"cpu\",\n            )\n            if cfg.train.use_deepspeed is True\n            else None,\n        )\n        if cfg.log.with_tracking:\n            if cfg.log.report_to != \"wandb\":\n                raise NotImplementedError(\n                    \"Currently only support wandb, init trakcer for your platforms\"\n                )\n            if self.accelerator.is_main_process:\n                try:\n                    import wandb\n                except ImportError as e:\n                    raise ImportError(\n                        \"You passed with_tracking and report_to `wandb`, please install wandb by running `pip install wandb`\"\n                    ) from e\n\n                wandb_kwargs = {\n                    \"name\": f\"run_{time.strftime('%Y-%m-%d_%H:%M:%S', time.localtime())}\",\n                    \"notes\": \"train unet only\",\n                    \"group\": \"train unet\",\n                    \"tags\": [\"stable diffusion\", \"pytorch\"],\n                    \"entity\": \"liwenbo2099\",\n                    \"resume\": cfg.log.resume,\n                    \"save_code\": True,\n                    \"allow_val_change\": True,\n                }\n\n                self.accelerator.init_trackers(\n                    \"stable_diffusion_pytorch\",\n                    args,\n                    init_kwargs={\"wandb\": wandb_kwargs},\n                )\n                wandb.config.update(\n                    args, allow_val_change=True  # wandb_kwargs[\"allow_val_change\"]\n                )\n\n        logger.info(self.accelerator.state, main_process_only=False)\n        # * 2. set seed\n        if cfg.train.seed is not None:\n            set_seed(cfg.train.seed)\n        # * 4. build optimizer and lr_scheduler\n        self.optimizer = self.__build_optimizer(cfg.optim)\n        self.lr_scheduler = self.__build_lr_scheduler(cfg.optim)\n        # * 5. get dataset\n        self.train_dataloader = self.get_dataloader(\n            train=True,\n            dataset=self.train_dataset,\n            collate_fn=collate_fn,\n            batch_size=cfg.train.train_batch_size,\n            num_workers=cfg.dataset.dataloader_num_workers\n            or self.accelerator.num_processes,\n        )\n        self.eval_dataloader = self.get_dataloader(\n            train=False,\n            dataset=self.eval_dataset,\n            collate_fn=collate_fn,\n            batch_size=cfg.train.eval_batch_size,\n            num_workers=cfg.dataset.dataloader_num_workers\n            or self.accelerator.num_processes,\n        )\n\n        # * 5. Prepare everything with our `accelerator`.\n        (\n            self.model,\n            self.optimizer,\n            self.train_dataloader,\n            self.eval_dataloader,\n            self.lr_scheduler,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.optimizer,\n            self.train_dataloader,\n            self.eval_dataloader,\n            self.lr_scheduler,\n        )\n        # * enable variable annotations so that we can easily debug\n        self.model: LatentDiffusion = self.model\n        self.train_dataloader: DataLoader = self.train_dataloader\n        self.eval_dataloader: DataLoader = self.eval_dataloader\n        # * 6. Move text_encoder and autoencoder to gpu and cast to weight_dtype\n        self.weight_dtype = torch.float32\n        if self.accelerator.mixed_precision == \"bf16\":\n            self.weight_dtype = torch.bfloat16\n        elif self.accelerator.mixed_precision == \"fp16\":\n            self.weight_dtype = torch.float16\n        self.model.text_encoder.to(self.accelerator.device, dtype=self.weight_dtype)\n        self.model.autoencoder.to(self.accelerator.device, dtype=self.weight_dtype)\n        self.model.unet.to(self.accelerator.device, dtype=self.weight_dtype)\n\n    def get_dataloader(\n        self, train, dataset, collate_fn, batch_size, num_workers\n    ) -> DataLoader:\n        return DataLoader(\n            dataset,\n            shuffle=train,\n            collate_fn=collate_fn,\n            batch_size=batch_size,\n            num_workers=num_workers,\n        )\n\n    def __build_optimizer(self, optim_cfg):\n        # Initialize the optimizer\n        if optim_cfg.use_8bit_adam:\n            try:\n                import bitsandbytes as bnb\n            except ImportError as e:\n                raise ImportError(\n                    \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n                ) from e\n\n            optimizer_cls = bnb.optim.AdamW8bit\n        else:\n            optimizer_cls = torch.optim.AdamW\n\n        # * code change to fit deepspeed\n        optimizer_cls = (\n            optimizer_cls\n            if self.accelerator.state.deepspeed_plugin is None\n            or \"optimizer\"\n            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n            else DummyOptim\n        )\n\n        return optimizer_cls(\n            # only train unet\n            self.model.unet.parameters(),\n            lr=optim_cfg.learning_rate,\n            weight_decay=optim_cfg.adam_weight_decay,\n        )\n\n    def __build_lr_scheduler(self, optim_cfg):\n        lr_scheduler = None\n        if (\n            self.accelerator.state.deepspeed_plugin is None\n            or \"scheduler\"\n            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n        ):\n            lr_scheduler = get_scheduler(\n                optim_cfg.scheduler_type,\n                optimizer=self.optimizer,\n                num_warmup_steps=optim_cfg.lr_warmup_steps\n                * self.cfg.train.gradient_accumulation_steps,\n                num_training_steps=self.cfg.train.max_train_steps\n                * self.cfg.train.gradient_accumulation_steps,\n            )\n        else:\n            lr_scheduler = DummyScheduler(\n                self.optimizer,\n                total_num_steps=self.cfg.train.max_train_steps,\n                warmup_num_steps=optim_cfg.lr_warmup_steps,\n            )\n        return lr_scheduler\n\n    def __resume_from_ckpt(self, ckpt_cfg):\n        \"resume from checkpoint or start a new train\"\n        ckpt_path = None\n        if ckpt_cfg.resume_from_checkpoint:\n            ckpt_path = os.path.basename(ckpt_cfg.resume_from_checkpoint)\n            if ckpt_cfg.resume_from_checkpoint == \"latest\":\n                # None, Get the most recent checkpoint or start from scratch\n                dirs = os.listdir(ckpt_cfg.ckpt_dir)\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n                dirs = sorted(\n                    dirs, key=lambda x: int(x.split(\"-\")[1])\n                )  # checkpoint-100 => 100\n                ckpt_path = dirs[-1] if len(dirs) > 0 else None\n        if ckpt_path is None:\n            self.accelerator.print(\n                f\"Checkpoint '{ckpt_cfg.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n            )\n            ckpt_cfg.resume_from_checkpoint = None\n        else:\n            self.accelerator.print(f\"Resuming from checkpoint {ckpt_path}\")\n            self.accelerator.load_state(os.path.join(ckpt_cfg.ckpt_dir, ckpt_path))\n        self.ckpt_path = ckpt_path\n\n    def __resume_train_state(self, train_cfg, ckpt_path):\n        \"\"\"\n        resume train steps and epochs\n        \"\"\"\n        # * Calculate train steps\n        # total batch num / accumulate steps => actual update steps per epoch\n        num_update_steps_per_epoch = math.ceil(\n            len(self.train_dataloader) / train_cfg.gradient_accumulation_steps\n        )\n        if train_cfg.max_train_steps is None:\n            train_cfg.max_train_steps = (\n                train_cfg.max_train_epochs * num_update_steps_per_epoch\n            )\n        else:\n            # override max_train_epochs\n            train_cfg.max_train_epochs = math.ceil(\n                train_cfg.max_train_steps / num_update_steps_per_epoch\n            )\n        # actual updated steps\n        self.global_step = int(ckpt_path.split(\"-\")[1]) if ckpt_path else 0\n        self.start_epoch = (\n            self.global_step // num_update_steps_per_epoch if ckpt_path else 0\n        )\n        # * change diffusers implementation: 20 % 6 = 2 = 2*(10 % 3)\n        self.resume_step = (\n            self.global_step\n            % (num_update_steps_per_epoch)\n            * train_cfg.gradient_accumulation_steps\n        )\n\n    def train(self):\n        cfg = self.cfg\n        # * 7. Resume training state and ckpt\n        self.__resume_from_ckpt(cfg.checkpoint)\n        self.__resume_train_state(cfg.train, self.ckpt_path)\n\n        total_batch_size = (\n            cfg.train.train_batch_size\n            * self.accelerator.num_processes\n            * cfg.train.gradient_accumulation_steps\n        )\n        self.checkpointing_steps = cfg.checkpoint.checkpointing_steps\n        if self.checkpointing_steps is not None and self.checkpointing_steps.isdigit():\n            self.checkpointing_steps = int(self.checkpointing_steps)\n        logger.info(\"****************Start Training******************\")\n        logger.info(f\"Total training data: {len(self.train_dataloader.dataset)}\")\n        if hasattr(self, \"eval_dataloader\") and self.eval_dataloader is not None:\n            logger.info(f\"Total eval data: {len(self.eval_dataloader.dataset)}\")\n        logger.info(f\"Total update steps: {cfg.train.max_train_steps}\")\n        logger.info(f\"Total Epochs: {cfg.train.max_train_epochs}\")\n        logger.info(f\"Total Batch size: {total_batch_size}\")\n        logger.info(f\"Resume from epoch={self.start_epoch}, step={self.resume_step}\")\n        logger.info(\"**********************************************\")\n        self.progress_bar = tqdm(\n            range(self.global_step, cfg.train.max_train_steps),\n            total=cfg.train.max_train_steps,\n            disable=not self.accelerator.is_main_process,\n            initial=self.global_step,  # @note: huggingface seemed to missed this, should first update to global step\n            desc=\"Step\",\n        )\n        self.model.train()\n        for epoch in range(self.start_epoch, cfg.train.max_train_epochs):\n            train_loss = 0\n            for step, batch in enumerate(self.train_dataloader):\n                # * Skip steps until we reach the resumed step\n                if (\n                    self.ckpt_path is not None\n                    and epoch == self.start_epoch\n                    and step < self.resume_step\n                ):\n                    if step % cfg.train.gradient_accumulation_steps == 0:\n                        # @note: huggingface seemed to missed this, global step should also be updated\n                        self.global_step += 1\n                        self.progress_bar.update(1)\n                    continue\n                with self.accelerator.accumulate(self.model.unet):\n                    loss = self.__one_step(batch)\n                    # gather loss across processes for logging\n                    avg_loss = self.accelerator.gather(\n                        loss.repeat(cfg.train.train_batch_size)\n                    ).mean()\n                    train_loss += avg_loss.item()\n                    # * 7. backward\n                    self.accelerator.backward(loss)\n                    if self.accelerator.sync_gradients:\n                        self.accelerator.clip_grad_norm_(\n                            self.model.unet.parameters(), cfg.optim.max_grad_norm\n                        )\n                    # * 8. update\n                    self.optimizer.step()\n                    self.lr_scheduler.step()\n                    self.optimizer.zero_grad()\n\n                # Checks if the accelerator has performed an optimization step behind the scenes\n                if self.accelerator.sync_gradients:\n                    self.progress_bar.update(1)\n                    self.global_step += 1\n                    if self.cfg.log.with_tracking:\n                        self.accelerator.log(\n                            {\n                                \"train_loss\": train_loss,\n                                \"lr\": self.lr_scheduler.get_last_lr()[0],\n                            },\n                            step=self.global_step,\n                        )\n                    train_loss = 0.0\n                    if (\n                        isinstance(self.checkpointing_steps, int)\n                        and self.global_step % self.checkpointing_steps == 0\n                    ):\n                        ckpt_path = os.path.join(\n                            cfg.checkpoint.ckpt_dir,\n                            f\"checkpoint-{self.global_step}\",\n                        )\n                        if (\n                            self.cfg.checkpoint.keep_last_only\n                            and self.accelerator.is_main_process\n                        ):\n                            # del last save path\n                            if self.last_ckpt is not None:\n                                shutil.rmtree(self.last_ckpt)\n                            self.last_ckpt = ckpt_path\n                        self.accelerator.save_state(ckpt_path)\n                        logger.info(f\"Saved state to {ckpt_path}\")\n\n                logs = {\n                    \"loss\": loss.detach().item(),\n                    \"lr\": self.lr_scheduler.get_last_lr()[0],\n                }\n                self.progress_bar.set_postfix(**logs)\n                if self.global_step >= cfg.train.max_train_steps:\n                    break\n                # =======================Evaluation==========================\n                if (\n                    self.global_step > 0\n                    and cfg.train.log_interval > 0\n                    and self.global_step % cfg.train.log_interval == 0\n                ):\n                    logger.info(\n                        f\"Evaluate on eval dataset [len: {len(self.eval_dataset)}]\"\n                    )\n                    self.model.eval()\n                    losses = []\n                    eval_bar = tqdm(\n                        self.eval_dataloader,\n                        disable=not self.accelerator.is_main_process,\n                    )\n                    for step, batch in enumerate(eval_bar):\n                        with torch.no_grad():\n                            loss = self.__one_step(batch)\n                        losses.append(\n                            self.accelerator.gather_for_metrics(\n                                loss.repeat(cfg.train.eval_batch_size)\n                            )\n                        )\n                    losses = torch.cat(losses)\n                    eval_loss = torch.mean(losses)\n                    logger.info(\n                        f\"global step {self.global_step}: eval_loss: {eval_loss}\"\n                    )\n                    if cfg.log.with_tracking:\n                        self.accelerator.log(\n                            {\n                                \"eval_loss\": eval_loss,\n                            },\n                            step=self.global_step,\n                        )\n                    # log image\n                    if self.cfg.log.log_image:\n                        prompt = \"a white cat wearing a hat\"\n                        sample = self.sample(prompt=prompt)\n                        if self.cfg.log.with_tracking:\n                            import wandb\n\n                            self.accelerator.log(\n                                {\n                                    \"sampled image\": wandb.Image(\n                                        sample, caption=prompt\n                                    ),\n                                },\n                                step=self.global_step,\n                            )\n\n                    self.model.train()  # back to train mode\n            # save ckpt for each epoch\n            if self.checkpointing_steps == \"epoch\":\n                ckpt_dir = f\"epoch_{epoch}\"\n                if cfg.checkpoint.ckpt_dir is not None:\n                    ckpt_dir = os.path.join(cfg.checkpoint.ckpt_dir, ckpt_dir)\n                if (\n                    self.cfg.checkpoint.keep_last_only\n                    and self.accelerator.is_main_process\n                ):\n                    # del last save path\n                    if self.last_ckpt is not None:\n                        shutil.rmtree(self.last_ckpt)\n                    self.last_ckpt = ckpt_path\n                self.accelerator.save_state(ckpt_path)\n                logger.info(f\"Saved state to {ckpt_path}\")\n\n        # end training\n        self.accelerator.wait_for_everyone()\n        if self.cfg.log.with_tracking:\n            self.accelerator.end_training()\n\n    def __one_step(self, batch: dict):\n        \"\"\"\n        __train_one_step: one diffusion backward step\n\n        Args:\n            - batch (dict):\n                  a batch of data, contains: pixel_values and input_ids\n\n        Returns:\n            - torch.Tensor:\n                  mse loss between sampled real noise and pred noise\n        \"\"\"\n        # * 1. encode image\n        latent_vector = self.model.autoencoder.encode(\n            batch[\"pixel_values\"].to(self.weight_dtype)\n        ).latent_dist.sample()\n        noise = torch.randn(latent_vector.shape).to(self.accelerator.device)\n        # * 2. Sample a random timestep for each image\n        timesteps = torch.randint(\n            self.model.noise_scheduler.noise_steps, (batch[\"pixel_values\"].shape[0],)\n        ).to(self.accelerator.device, dtype=torch.long)\n        # timesteps = timesteps.long()\n        # * 3. add noise to latent vector\n        x_t = self.model.noise_scheduler.add_noise(\n            original_samples=latent_vector, timesteps=timesteps, noise=noise\n        ).to(dtype=self.weight_dtype)\n        # * 4. get text encoding latent\n        tokenized_text = batch[\"input_ids\"]\n        # 90 % of the time we use the true text encoding, 10 % of the time we use an empty string\n        if np.random.random() < 0.1:\n            tokenized_text = self.model.text_encoder.tokenize(\n                [\"\"] * len(tokenized_text)\n            ).input_ids.to(self.accelerator.device)\n        text_condition = self.model.text_encoder.encode_text(\n            tokenized_text  # adding `.to(self.weight_dtype)` causes error...\n        )[0].to(self.weight_dtype)\n        # * 5. predict noise\n        pred_noise = self.model.pred_noise(\n            x_t, timesteps, text_condition, guidance_scale=self.cfg.train.guidance_scale\n        )\n        return F.mse_loss(pred_noise.float(), noise.float(), reduction=\"mean\")\n\n    def sample(\n        self,\n        image_size=64,\n        prompt: str = \"\",\n        guidance_scale: float = 7.5,\n        scale_factor=1.0,\n        save_dir: str = \"output\",\n    ):\n        \"Sample an image given prompt\"\n        # random noise\n        # to get the right latent size\n        img_util = torch.randn(size=(1, 3, image_size, image_size)).to(\n            self.accelerator.device, dtype=self.weight_dtype\n        )\n        noise = self.model.autoencoder.encode(img_util).latent_dist.sample()\n        noise = torch.rand_like(noise).to(\n            self.accelerator.device, dtype=self.weight_dtype\n        )\n        # tokenize prompt\n        tokenized_prompt = self.model.text_encoder.tokenize([prompt]).input_ids.to(\n            self.accelerator.device\n        )\n        context_emb = self.model.text_encoder.encode_text(tokenized_prompt)[0].to(\n            self.weight_dtype\n        )\n        x_0 = self.model.sample(\n            noised_sample=noise,\n            context_emb=context_emb,\n            guidance_scale=guidance_scale,\n            scale_factor=scale_factor,\n        )\n        sample = self.model.autoencoder.decode(x_0)\n        sample = detransform(sample.sample)\n        return to_img(sample, output_path=save_dir, name=\"unet_sample\")", "\n\nclass StableDiffusionTrainer:\n    def __init__(\n        self,\n        model: LatentDiffusion,\n        args,\n        cfg,\n        train_dataset,\n        eval_dataset,\n        collate_fn,\n    ):\n        # check params\n        assert train_dataset is not None, \"must specify an training dataset\"\n        assert (\n            eval_dataset is not None and cfg.train.log_interval > 0\n        ), \"if passed log_interval > 0, you must specify an evaluation dataset\"\n        self.model: LatentDiffusion = model\n        # test autoencoder\n        self.model.autoencoder = AutoencoderKL.from_pretrained(\n            \"runwayml/stable-diffusion-v1-5\",\n            subfolder=\"vae\",\n            cache_dir=\"data/pretrained\",\n        )\n        self.model.autoencoder.requires_grad_(False)\n        self.cfg = cfg\n        self.train_dataset: Dataset = train_dataset\n        self.eval_dataset: Dataset = eval_dataset\n        self.last_ckpt = None\n        # * 1. init accelerator\n        accelerator_log_kwcfg = {}\n        if cfg.log.with_tracking:\n            try:\n                accelerator_log_kwcfg[\"log_with\"] = cfg.log.report_to\n            except AttributeError:\n                print(\"need to specify report_to when passing in with_tracking=True\")\n            accelerator_log_kwcfg[\"logging_dir\"] = cfg.log.logging_dir\n\n        accelerator_project_config = ProjectConfiguration()\n        # check deepspeed\n        if cfg.train.use_deepspeed is True:\n            try:\n                import deepspeed\n            except ImportError as e:\n                raise ImportError(\n                    'You passed use_deepspeed=True, please install deepspeed by running `pip install deepspeed`, also deepspeed requies a matched cuda version, so you may need to run `conda install -c \"nvidia/label/cuda-11.5.0\" cuda-toolkit`, see https://anaconda.org/nvidia/cuda-toolkit for more options'\n                ) from e\n\n        self.accelerator = Accelerator(\n            gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n            **accelerator_log_kwcfg,\n            project_config=accelerator_project_config,\n            deepspeed_plugin=DeepSpeedPlugin(\n                zero_stage=2,\n                gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n                gradient_clipping=cfg.optim.max_grad_norm,\n                offload_optimizer_device=\"cpu\",\n                offload_param_device=\"cpu\",\n            )\n            if cfg.train.use_deepspeed is True\n            else None,\n        )\n        if cfg.log.with_tracking:\n            if cfg.log.report_to != \"wandb\":\n                raise NotImplementedError(\n                    \"Currently only support wandb, init trakcer for your platforms\"\n                )\n            if self.accelerator.is_main_process:\n                try:\n                    import wandb\n                except ImportError as e:\n                    raise ImportError(\n                        \"You passed with_tracking and report_to `wandb`, please install wandb by running `pip install wandb`\"\n                    ) from e\n\n                wandb_kwargs = {\n                    \"name\": f\"run_{time.strftime('%Y-%m-%d_%H:%M:%S', time.localtime())}\",\n                    \"notes\": \"train unet only\",\n                    \"group\": \"train unet\",\n                    \"tags\": [\"stable diffusion\", \"pytorch\"],\n                    \"entity\": \"liwenbo2099\",\n                    \"resume\": cfg.log.resume,\n                    \"save_code\": True,\n                    \"allow_val_change\": True,\n                }\n\n                self.accelerator.init_trackers(\n                    \"stable_diffusion_pytorch\",\n                    args,\n                    init_kwargs={\"wandb\": wandb_kwargs},\n                )\n                wandb.config.update(\n                    args, allow_val_change=True  # wandb_kwargs[\"allow_val_change\"]\n                )\n\n        logger.info(self.accelerator.state, main_process_only=False)\n        # * 2. set seed\n        if cfg.train.seed is not None:\n            set_seed(cfg.train.seed)\n        # * 4. build optimizer and lr_scheduler\n        self.optimizer = self.__build_optimizer(cfg.optim)\n        self.lr_scheduler = self.__build_lr_scheduler(cfg.optim)\n        # * 5. get dataset\n        self.train_dataloader = self.get_dataloader(\n            train=True,\n            dataset=self.train_dataset,\n            collate_fn=collate_fn,\n            batch_size=cfg.train.train_batch_size,\n            num_workers=cfg.dataset.dataloader_num_workers\n            or self.accelerator.num_processes,\n        )\n        self.eval_dataloader = self.get_dataloader(\n            train=False,\n            dataset=self.eval_dataset,\n            collate_fn=collate_fn,\n            batch_size=cfg.train.eval_batch_size,\n            num_workers=cfg.dataset.dataloader_num_workers\n            or self.accelerator.num_processes,\n        )\n\n        # * 5. Prepare everything with our `accelerator`.\n        (\n            self.model,\n            self.optimizer,\n            self.train_dataloader,\n            self.eval_dataloader,\n            self.lr_scheduler,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.optimizer,\n            self.train_dataloader,\n            self.eval_dataloader,\n            self.lr_scheduler,\n        )\n        # * enable variable annotations so that we can easily debug\n        self.model: LatentDiffusion = self.model\n        self.train_dataloader: DataLoader = self.train_dataloader\n        self.eval_dataloader: DataLoader = self.eval_dataloader\n        # * 6. Move text_encoder and autoencoder to gpu and cast to weight_dtype\n        self.weight_dtype = torch.float32\n        if self.accelerator.mixed_precision == \"bf16\":\n            self.weight_dtype = torch.bfloat16\n        elif self.accelerator.mixed_precision == \"fp16\":\n            self.weight_dtype = torch.float16\n        self.model.text_encoder.to(self.accelerator.device, dtype=self.weight_dtype)\n        self.model.autoencoder.to(self.accelerator.device, dtype=self.weight_dtype)\n        self.model.unet.to(self.accelerator.device, dtype=self.weight_dtype)\n\n    def get_dataloader(\n        self, train, dataset, collate_fn, batch_size, num_workers\n    ) -> DataLoader:\n        return DataLoader(\n            dataset,\n            shuffle=train,\n            collate_fn=collate_fn,\n            batch_size=batch_size,\n            num_workers=num_workers,\n        )\n\n    def __build_optimizer(self, optim_cfg):\n        # Initialize the optimizer\n        if optim_cfg.use_8bit_adam:\n            try:\n                import bitsandbytes as bnb\n            except ImportError as e:\n                raise ImportError(\n                    \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n                ) from e\n\n            optimizer_cls = bnb.optim.AdamW8bit\n        else:\n            optimizer_cls = torch.optim.AdamW\n\n        # * code change to fit deepspeed\n        optimizer_cls = (\n            optimizer_cls\n            if self.accelerator.state.deepspeed_plugin is None\n            or \"optimizer\"\n            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n            else DummyOptim\n        )\n\n        return optimizer_cls(\n            # only train unet\n            self.model.unet.parameters(),\n            lr=optim_cfg.learning_rate,\n            weight_decay=optim_cfg.adam_weight_decay,\n        )\n\n    def __build_lr_scheduler(self, optim_cfg):\n        lr_scheduler = None\n        if (\n            self.accelerator.state.deepspeed_plugin is None\n            or \"scheduler\"\n            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n        ):\n            lr_scheduler = get_scheduler(\n                optim_cfg.scheduler_type,\n                optimizer=self.optimizer,\n                num_warmup_steps=optim_cfg.lr_warmup_steps\n                * self.cfg.train.gradient_accumulation_steps,\n                num_training_steps=self.cfg.train.max_train_steps\n                * self.cfg.train.gradient_accumulation_steps,\n            )\n        else:\n            lr_scheduler = DummyScheduler(\n                self.optimizer,\n                total_num_steps=self.cfg.train.max_train_steps,\n                warmup_num_steps=optim_cfg.lr_warmup_steps,\n            )\n        return lr_scheduler\n\n    def __resume_from_ckpt(self, ckpt_cfg):\n        \"resume from checkpoint or start a new train\"\n        ckpt_path = None\n        if ckpt_cfg.resume_from_checkpoint:\n            ckpt_path = os.path.basename(ckpt_cfg.resume_from_checkpoint)\n            if ckpt_cfg.resume_from_checkpoint == \"latest\":\n                # None, Get the most recent checkpoint or start from scratch\n                dirs = os.listdir(ckpt_cfg.ckpt_dir)\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n                dirs = sorted(\n                    dirs, key=lambda x: int(x.split(\"-\")[1])\n                )  # checkpoint-100 => 100\n                ckpt_path = dirs[-1] if len(dirs) > 0 else None\n        if ckpt_path is None:\n            self.accelerator.print(\n                f\"Checkpoint '{ckpt_cfg.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n            )\n            ckpt_cfg.resume_from_checkpoint = None\n        else:\n            self.accelerator.print(f\"Resuming from checkpoint {ckpt_path}\")\n            self.accelerator.load_state(os.path.join(ckpt_cfg.ckpt_dir, ckpt_path))\n        self.ckpt_path = ckpt_path\n\n    def __resume_train_state(self, train_cfg, ckpt_path):\n        \"\"\"\n        resume train steps and epochs\n        \"\"\"\n        # * Calculate train steps\n        # total batch num / accumulate steps => actual update steps per epoch\n        num_update_steps_per_epoch = math.ceil(\n            len(self.train_dataloader) / train_cfg.gradient_accumulation_steps\n        )\n        if train_cfg.max_train_steps is None:\n            train_cfg.max_train_steps = (\n                train_cfg.max_train_epochs * num_update_steps_per_epoch\n            )\n        else:\n            # override max_train_epochs\n            train_cfg.max_train_epochs = math.ceil(\n                train_cfg.max_train_steps / num_update_steps_per_epoch\n            )\n        # actual updated steps\n        self.global_step = int(ckpt_path.split(\"-\")[1]) if ckpt_path else 0\n        self.start_epoch = (\n            self.global_step // num_update_steps_per_epoch if ckpt_path else 0\n        )\n        # * change diffusers implementation: 20 % 6 = 2 = 2*(10 % 3)\n        self.resume_step = (\n            self.global_step\n            % (num_update_steps_per_epoch)\n            * train_cfg.gradient_accumulation_steps\n        )\n\n    def train(self):\n        cfg = self.cfg\n        # * 7. Resume training state and ckpt\n        self.__resume_from_ckpt(cfg.checkpoint)\n        self.__resume_train_state(cfg.train, self.ckpt_path)\n\n        total_batch_size = (\n            cfg.train.train_batch_size\n            * self.accelerator.num_processes\n            * cfg.train.gradient_accumulation_steps\n        )\n        self.checkpointing_steps = cfg.checkpoint.checkpointing_steps\n        if self.checkpointing_steps is not None and self.checkpointing_steps.isdigit():\n            self.checkpointing_steps = int(self.checkpointing_steps)\n        logger.info(\"****************Start Training******************\")\n        logger.info(f\"Total training data: {len(self.train_dataloader.dataset)}\")\n        if hasattr(self, \"eval_dataloader\") and self.eval_dataloader is not None:\n            logger.info(f\"Total eval data: {len(self.eval_dataloader.dataset)}\")\n        logger.info(f\"Total update steps: {cfg.train.max_train_steps}\")\n        logger.info(f\"Total Epochs: {cfg.train.max_train_epochs}\")\n        logger.info(f\"Total Batch size: {total_batch_size}\")\n        logger.info(f\"Resume from epoch={self.start_epoch}, step={self.resume_step}\")\n        logger.info(\"**********************************************\")\n        self.progress_bar = tqdm(\n            range(self.global_step, cfg.train.max_train_steps),\n            total=cfg.train.max_train_steps,\n            disable=not self.accelerator.is_main_process,\n            initial=self.global_step,  # @note: huggingface seemed to missed this, should first update to global step\n            desc=\"Step\",\n        )\n        self.model.train()\n        for epoch in range(self.start_epoch, cfg.train.max_train_epochs):\n            train_loss = 0\n            for step, batch in enumerate(self.train_dataloader):\n                # * Skip steps until we reach the resumed step\n                if (\n                    self.ckpt_path is not None\n                    and epoch == self.start_epoch\n                    and step < self.resume_step\n                ):\n                    if step % cfg.train.gradient_accumulation_steps == 0:\n                        # @note: huggingface seemed to missed this, global step should also be updated\n                        self.global_step += 1\n                        self.progress_bar.update(1)\n                    continue\n                with self.accelerator.accumulate(self.model.unet):\n                    loss = self.__one_step(batch)\n                    # gather loss across processes for logging\n                    avg_loss = self.accelerator.gather(\n                        loss.repeat(cfg.train.train_batch_size)\n                    ).mean()\n                    train_loss += avg_loss.item()\n                    # * 7. backward\n                    self.accelerator.backward(loss)\n                    if self.accelerator.sync_gradients:\n                        self.accelerator.clip_grad_norm_(\n                            self.model.unet.parameters(), cfg.optim.max_grad_norm\n                        )\n                    # * 8. update\n                    self.optimizer.step()\n                    self.lr_scheduler.step()\n                    self.optimizer.zero_grad()\n\n                # Checks if the accelerator has performed an optimization step behind the scenes\n                if self.accelerator.sync_gradients:\n                    self.progress_bar.update(1)\n                    self.global_step += 1\n                    if self.cfg.log.with_tracking:\n                        self.accelerator.log(\n                            {\n                                \"train_loss\": train_loss,\n                                \"lr\": self.lr_scheduler.get_last_lr()[0],\n                            },\n                            step=self.global_step,\n                        )\n                    train_loss = 0.0\n                    if (\n                        isinstance(self.checkpointing_steps, int)\n                        and self.global_step % self.checkpointing_steps == 0\n                    ):\n                        ckpt_path = os.path.join(\n                            cfg.checkpoint.ckpt_dir,\n                            f\"checkpoint-{self.global_step}\",\n                        )\n                        if (\n                            self.cfg.checkpoint.keep_last_only\n                            and self.accelerator.is_main_process\n                        ):\n                            # del last save path\n                            if self.last_ckpt is not None:\n                                shutil.rmtree(self.last_ckpt)\n                            self.last_ckpt = ckpt_path\n                        self.accelerator.save_state(ckpt_path)\n                        logger.info(f\"Saved state to {ckpt_path}\")\n\n                logs = {\n                    \"loss\": loss.detach().item(),\n                    \"lr\": self.lr_scheduler.get_last_lr()[0],\n                }\n                self.progress_bar.set_postfix(**logs)\n                if self.global_step >= cfg.train.max_train_steps:\n                    break\n                # =======================Evaluation==========================\n                if (\n                    self.global_step > 0\n                    and cfg.train.log_interval > 0\n                    and self.global_step % cfg.train.log_interval == 0\n                ):\n                    logger.info(\n                        f\"Evaluate on eval dataset [len: {len(self.eval_dataset)}]\"\n                    )\n                    self.model.eval()\n                    losses = []\n                    eval_bar = tqdm(\n                        self.eval_dataloader,\n                        disable=not self.accelerator.is_main_process,\n                    )\n                    for step, batch in enumerate(eval_bar):\n                        with torch.no_grad():\n                            loss = self.__one_step(batch)\n                        losses.append(\n                            self.accelerator.gather_for_metrics(\n                                loss.repeat(cfg.train.eval_batch_size)\n                            )\n                        )\n                    losses = torch.cat(losses)\n                    eval_loss = torch.mean(losses)\n                    logger.info(\n                        f\"global step {self.global_step}: eval_loss: {eval_loss}\"\n                    )\n                    if cfg.log.with_tracking:\n                        self.accelerator.log(\n                            {\n                                \"eval_loss\": eval_loss,\n                            },\n                            step=self.global_step,\n                        )\n                    # log image\n                    if self.cfg.log.log_image:\n                        prompt = \"a white cat wearing a hat\"\n                        sample = self.sample(prompt=prompt)\n                        if self.cfg.log.with_tracking:\n                            import wandb\n\n                            self.accelerator.log(\n                                {\n                                    \"sampled image\": wandb.Image(\n                                        sample, caption=prompt\n                                    ),\n                                },\n                                step=self.global_step,\n                            )\n\n                    self.model.train()  # back to train mode\n            # save ckpt for each epoch\n            if self.checkpointing_steps == \"epoch\":\n                ckpt_dir = f\"epoch_{epoch}\"\n                if cfg.checkpoint.ckpt_dir is not None:\n                    ckpt_dir = os.path.join(cfg.checkpoint.ckpt_dir, ckpt_dir)\n                if (\n                    self.cfg.checkpoint.keep_last_only\n                    and self.accelerator.is_main_process\n                ):\n                    # del last save path\n                    if self.last_ckpt is not None:\n                        shutil.rmtree(self.last_ckpt)\n                    self.last_ckpt = ckpt_path\n                self.accelerator.save_state(ckpt_path)\n                logger.info(f\"Saved state to {ckpt_path}\")\n\n        # end training\n        self.accelerator.wait_for_everyone()\n        if self.cfg.log.with_tracking:\n            self.accelerator.end_training()\n\n    def __one_step(self, batch: dict):\n        \"\"\"\n        __train_one_step: one diffusion backward step\n\n        Args:\n            - batch (dict):\n                  a batch of data, contains: pixel_values and input_ids\n\n        Returns:\n            - torch.Tensor:\n                  mse loss between sampled real noise and pred noise\n        \"\"\"\n        # * 1. encode image\n        latent_vector = self.model.autoencoder.encode(\n            batch[\"pixel_values\"].to(self.weight_dtype)\n        ).latent_dist.sample()\n        noise = torch.randn(latent_vector.shape).to(self.accelerator.device)\n        # * 2. Sample a random timestep for each image\n        timesteps = torch.randint(\n            self.model.noise_scheduler.noise_steps, (batch[\"pixel_values\"].shape[0],)\n        ).to(self.accelerator.device, dtype=torch.long)\n        # timesteps = timesteps.long()\n        # * 3. add noise to latent vector\n        x_t = self.model.noise_scheduler.add_noise(\n            original_samples=latent_vector, timesteps=timesteps, noise=noise\n        ).to(dtype=self.weight_dtype)\n        # * 4. get text encoding latent\n        tokenized_text = batch[\"input_ids\"]\n        # 90 % of the time we use the true text encoding, 10 % of the time we use an empty string\n        if np.random.random() < 0.1:\n            tokenized_text = self.model.text_encoder.tokenize(\n                [\"\"] * len(tokenized_text)\n            ).input_ids.to(self.accelerator.device)\n        text_condition = self.model.text_encoder.encode_text(\n            tokenized_text  # adding `.to(self.weight_dtype)` causes error...\n        )[0].to(self.weight_dtype)\n        # * 5. predict noise\n        pred_noise = self.model.pred_noise(\n            x_t, timesteps, text_condition, guidance_scale=self.cfg.train.guidance_scale\n        )\n        return F.mse_loss(pred_noise.float(), noise.float(), reduction=\"mean\")\n\n    def sample(\n        self,\n        image_size=64,\n        prompt: str = \"\",\n        guidance_scale: float = 7.5,\n        scale_factor=1.0,\n        save_dir: str = \"output\",\n    ):\n        \"Sample an image given prompt\"\n        # random noise\n        # to get the right latent size\n        img_util = torch.randn(size=(1, 3, image_size, image_size)).to(\n            self.accelerator.device, dtype=self.weight_dtype\n        )\n        noise = self.model.autoencoder.encode(img_util).latent_dist.sample()\n        noise = torch.rand_like(noise).to(\n            self.accelerator.device, dtype=self.weight_dtype\n        )\n        # tokenize prompt\n        tokenized_prompt = self.model.text_encoder.tokenize([prompt]).input_ids.to(\n            self.accelerator.device\n        )\n        context_emb = self.model.text_encoder.encode_text(tokenized_prompt)[0].to(\n            self.weight_dtype\n        )\n        x_0 = self.model.sample(\n            noised_sample=noise,\n            context_emb=context_emb,\n            guidance_scale=guidance_scale,\n            scale_factor=scale_factor,\n        )\n        sample = self.model.autoencoder.decode(x_0)\n        sample = detransform(sample.sample)\n        return to_img(sample, output_path=save_dir, name=\"unet_sample\")", "\n\n@record\ndef main():\n    args, cfg = load_config()\n    model = build_models(cfg.model, logger)\n    train_dataset = get_dataset(\n        cfg.dataset,\n        split=\"train\",\n        tokenizer=model.text_encoder.tokenizer,\n        logger=logger,\n    )\n    eval_dataset = get_dataset(\n        cfg.dataset,\n        split=\"validation\",\n        tokenizer=model.text_encoder.tokenizer,\n        logger=logger,\n    )\n    trainer = StableDiffusionTrainer(\n        model,\n        args,\n        cfg,\n        train_dataset,\n        eval_dataset,\n        collate_fn=collate_fn,\n    )\n    trainer.train()", "\n\nif __name__ == \"__main__\":\n    main()\n\n\n# to run without debug:\n# accelerate launch --config_file stable_diffusion/config/accelerate_config/deepspeed.yaml --main_process_port 29511 train_unet.py --use-deepspeed --with-tracking --log-image --max-train-steps 10000 --max-train-samples 700 --max-val-samples 50 --max-test-samples 50 --resume-from-checkpoint latest --ckpt-dir model/unet --learning-rate 5e-7 --keep-last-only\n", ""]}
{"filename": "train_autoencoder.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   train.py\n@Time    :   2023/05/12 19:24:02\n@Author  :   Wenbo Li\n@Desc    :   Main Class for training stable diffusion model\n\"\"\"\n\n", "\n\nimport math\nimport os\nimport shutil\nimport time\n\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import (", "from accelerate.logging import get_logger\nfrom accelerate.utils import (\n    set_seed,\n    DummyOptim,\n    ProjectConfiguration,\n    DummyScheduler,\n    DeepSpeedPlugin,\n)\nfrom transformers import get_scheduler\nimport logging", "from transformers import get_scheduler\nimport logging\nfrom stable_diffusion.models.autoencoder import AutoEncoderKL\nfrom stable_diffusion.models.latent_diffusion import LatentDiffusion\nfrom stable_diffusion.modules.distributions import GaussianDistribution\nfrom utils.model_utils import build_models\nfrom utils.parse_args import load_config\nfrom utils.prepare_dataset import (\n    collate_fn,\n    detransform,", "    collate_fn,\n    detransform,\n    get_dataset,\n    get_transform,\n    sample_test_image,\n    to_img,\n)\nimport numpy as np\nimport torch\nimport torch.nn.functional as F", "import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.distributed.elastic.multiprocessing.errors import record\nfrom PIL import Image\nfrom transformers import CLIPTokenizer\n\n# build environment\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"", "# build environment\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\nos.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\"\nlogger = get_logger(__name__)\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    level=logging.INFO,", "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    level=logging.INFO,\n)\n\n\nclass AutoencoderKLTrainer:\n    def __init__(\n        self,\n        model: AutoEncoderKL,\n        args,\n        cfg,\n        train_dataset,\n        eval_dataset,\n        test_images,\n        collate_fn,\n    ):\n        # check params\n        assert train_dataset is not None, \"must specify an training dataset\"\n        assert (\n            eval_dataset is not None and cfg.train.log_interval > 0\n        ), \"if passed log_interval > 0, you must specify an evaluation dataset\"\n        self.model: AutoEncoderKL = model\n        # make sure model is in train mode\n        self.model.requires_grad_(True)\n        self.cfg = cfg\n        self.train_dataset: Dataset = train_dataset\n        self.eval_dataset: Dataset = eval_dataset\n        self.test_images = test_images\n        self.last_ckpt = None\n        # * 1. init accelerator\n        accelerator_log_kwcfg = {}\n        if cfg.log.with_tracking:\n            try:\n                accelerator_log_kwcfg[\"log_with\"] = cfg.log.report_to\n            except AttributeError:\n                print(\"need to specify report_to when passing in with_tracking=True\")\n            accelerator_log_kwcfg[\"logging_dir\"] = cfg.log.logging_dir\n\n        accelerator_project_config = ProjectConfiguration()\n        # check deepspeed\n        if cfg.train.use_deepspeed is True:\n            try:\n                import deepspeed\n            except ImportError as e:\n                raise ImportError(\n                    'You passed use_deepspeed=True, please install deepspeed by running `pip install deepspeed`, also deepspeed requies a matched cuda version, so you may need to run `conda install -c \"nvidia/label/cuda-11.5.0\" cuda-toolkit`, see https://anaconda.org/nvidia/cuda-toolkit for more options'\n                ) from e\n\n        self.accelerator = Accelerator(\n            gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n            **accelerator_log_kwcfg,\n            project_config=accelerator_project_config,\n            deepspeed_plugin=DeepSpeedPlugin(\n                zero_stage=2,\n                gradient_accumulation_steps=cfg.train.gradient_accumulation_steps,\n                gradient_clipping=cfg.optim.max_grad_norm,\n                offload_optimizer_device=\"cpu\",\n                offload_param_device=\"cpu\",\n            )\n            if cfg.train.use_deepspeed is True\n            else None,\n        )\n        if cfg.log.with_tracking:\n            if cfg.log.report_to != \"wandb\":\n                raise NotImplementedError(\n                    \"Currently only support wandb, init trakcer for your platforms\"\n                )\n            if self.accelerator.is_main_process:\n                try:\n                    import wandb\n                except ImportError as e:\n                    raise ImportError(\n                        \"You passed with_tracking and report_to `wandb`, please install wandb by running `pip install wandb`\"\n                    ) from e\n\n                wandb_kwargs = {\n                    \"name\": f\"run_{time.strftime('%Y-%m-%d_%H:%M:%S', time.localtime())}\",\n                    \"notes\": \"train autoencoder\",\n                    \"group\": \"train autoencoder\",\n                    \"tags\": [\"stable diffusion\", \"pytorch\"],\n                    \"entity\": \"liwenbo2099\",\n                    \"resume\": cfg.log.resume,\n                    # \"id\": ,\n                    \"save_code\": True,\n                    \"allow_val_change\": True,\n                }\n\n                self.accelerator.init_trackers(\n                    \"stable_diffusion_pytorch\",\n                    args,\n                    init_kwargs={\"wandb\": wandb_kwargs},\n                )\n                wandb.config.update(\n                    args, allow_val_change=True  # wandb_kwargs[\"allow_val_change\"]\n                )\n\n        logger.info(self.accelerator.state, main_process_only=False)\n        # * 2. set seed\n        if cfg.train.seed is not None:\n            set_seed(cfg.train.seed)\n        # * 4. build optimizer and lr_scheduler\n        self.optimizer = self.__build_optimizer(cfg.optim)\n        self.lr_scheduler = self.__build_lr_scheduler(cfg.optim)\n        # * 5. get dataset\n        self.train_dataloader = self.get_dataloader(\n            train=True,\n            dataset=self.train_dataset,\n            collate_fn=collate_fn,\n            batch_size=cfg.train.train_batch_size,\n            num_workers=cfg.dataset.dataloader_num_workers\n            or self.accelerator.num_processes,\n        )\n        self.eval_dataloader = self.get_dataloader(\n            train=False,\n            dataset=self.eval_dataset,\n            collate_fn=collate_fn,\n            batch_size=cfg.train.eval_batch_size,\n            num_workers=cfg.dataset.dataloader_num_workers\n            or self.accelerator.num_processes,\n        )\n\n        # * 5. Prepare everything with our `accelerator`.\n        (\n            self.model,\n            self.optimizer,\n            self.train_dataloader,\n            self.eval_dataloader,\n            self.lr_scheduler,\n        ) = self.accelerator.prepare(\n            self.model,\n            self.optimizer,\n            self.train_dataloader,\n            self.eval_dataloader,\n            self.lr_scheduler,\n        )\n        # * enable variable annotations so that we can easily debug\n        self.model: AutoEncoderKL = self.model\n        self.train_dataloader: DataLoader = self.train_dataloader\n        self.eval_dataloader: DataLoader = self.eval_dataloader\n        # * 6. Move text_encoder and autoencoder to gpu and cast to weight_dtype\n        self.weight_dtype = torch.float32\n        if self.accelerator.mixed_precision == \"bf16\":\n            self.weight_dtype = torch.bfloat16\n        elif self.accelerator.mixed_precision == \"fp16\":\n            self.weight_dtype = torch.float16\n        self.model.to(self.accelerator.device, dtype=self.weight_dtype)\n\n    def get_dataloader(\n        self, train, dataset, collate_fn, batch_size, num_workers\n    ) -> DataLoader:\n        return DataLoader(\n            dataset,\n            shuffle=train,\n            collate_fn=collate_fn,\n            batch_size=batch_size,\n            num_workers=num_workers,\n        )\n\n    def __build_optimizer(self, optim_cfg):\n        # Initialize the optimizer\n        if optim_cfg.use_8bit_adam:\n            try:\n                import bitsandbytes as bnb\n            except ImportError as e:\n                raise ImportError(\n                    \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n                ) from e\n\n            optimizer_cls = bnb.optim.AdamW8bit\n        else:\n            optimizer_cls = torch.optim.AdamW\n\n        # * code change to fit deepspeed\n        optimizer_cls = (\n            optimizer_cls\n            if self.accelerator.state.deepspeed_plugin is None\n            or \"optimizer\"\n            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n            else DummyOptim\n        )\n\n        return optimizer_cls(\n            # only train unet\n            self.model.parameters(),\n            lr=optim_cfg.learning_rate,\n            weight_decay=optim_cfg.adam_weight_decay,\n        )\n\n    def __build_lr_scheduler(self, optim_cfg):\n        lr_scheduler = None\n        if (\n            self.accelerator.state.deepspeed_plugin is None\n            or \"scheduler\"\n            not in self.accelerator.state.deepspeed_plugin.deepspeed_config\n        ):\n            lr_scheduler = get_scheduler(\n                optim_cfg.scheduler_type,\n                optimizer=self.optimizer,\n                num_warmup_steps=optim_cfg.lr_warmup_steps\n                * self.cfg.train.gradient_accumulation_steps,\n                num_training_steps=self.cfg.train.max_train_steps\n                * self.cfg.train.gradient_accumulation_steps,\n            )\n        else:\n            lr_scheduler = DummyScheduler(\n                self.optimizer,\n                total_num_steps=self.cfg.train.max_train_steps,\n                warmup_num_steps=optim_cfg.lr_warmup_steps,\n            )\n        return lr_scheduler\n\n    def __resume_from_ckpt(self, ckpt_cfg):\n        \"resume from checkpoint or start a new train\"\n        ckpt_path = None\n        if ckpt_cfg.resume_from_checkpoint:\n            ckpt_path = os.path.basename(ckpt_cfg.resume_from_checkpoint)\n            if ckpt_cfg.resume_from_checkpoint == \"latest\":\n                # None, Get the most recent checkpoint or start from scratch\n                dirs = os.listdir(ckpt_cfg.ckpt_dir)\n                dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n                dirs = sorted(\n                    dirs, key=lambda x: int(x.split(\"-\")[1])\n                )  # checkpoint-100 => 100\n                ckpt_path = dirs[-1] if len(dirs) > 0 else None\n        if ckpt_path is None:\n            self.accelerator.print(\n                f\"Checkpoint '{ckpt_cfg.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n            )\n            ckpt_cfg.resume_from_checkpoint = None\n        else:\n            self.accelerator.print(f\"Resuming from checkpoint {ckpt_path}\")\n            self.accelerator.load_state(os.path.join(ckpt_cfg.ckpt_dir, ckpt_path))\n        self.ckpt_path = ckpt_path\n\n    def __resume_train_state(self, train_cfg, ckpt_path):\n        \"\"\"\n        resume train steps and epochs\n        \"\"\"\n        # * Calculate train steps\n        # total batch num / accumulate steps => actual update steps per epoch\n        num_update_steps_per_epoch = math.ceil(\n            len(self.train_dataloader) / train_cfg.gradient_accumulation_steps\n        )\n        if train_cfg.max_train_steps is None:\n            train_cfg.max_train_steps = (\n                train_cfg.max_train_epochs * num_update_steps_per_epoch\n            )\n        else:\n            # override max_train_epochs\n            train_cfg.max_train_epochs = math.ceil(\n                train_cfg.max_train_steps / num_update_steps_per_epoch\n            )\n        # actual updated steps\n        self.global_step = int(ckpt_path.split(\"-\")[1]) if ckpt_path else 0\n        self.start_epoch = (\n            self.global_step // num_update_steps_per_epoch if ckpt_path else 0\n        )\n        # * change diffusers implementation: 20 % 6 = 2 = 2*(10 % 3)\n        self.resume_step = (\n            self.global_step\n            % (num_update_steps_per_epoch)\n            * train_cfg.gradient_accumulation_steps\n        )\n\n    def train(self):\n        cfg = self.cfg\n        # * 7. Resume training state and ckpt\n        self.__resume_from_ckpt(cfg.checkpoint)\n        self.__resume_train_state(cfg.train, self.ckpt_path)\n\n        total_batch_size = (\n            cfg.train.train_batch_size\n            * self.accelerator.num_processes\n            * cfg.train.gradient_accumulation_steps\n        )\n        self.checkpointing_steps = cfg.checkpoint.checkpointing_steps\n        if self.checkpointing_steps is not None and self.checkpointing_steps.isdigit():\n            self.checkpointing_steps = int(self.checkpointing_steps)\n        logger.info(\"****************Start Training******************\")\n        logger.info(f\"Total training data: {len(self.train_dataloader.dataset)}\")\n        if hasattr(self, \"eval_dataloader\") and self.eval_dataloader is not None:\n            logger.info(f\"Total eval data: {len(self.eval_dataloader.dataset)}\")\n        logger.info(f\"Total update steps: {cfg.train.max_train_steps}\")\n        logger.info(f\"Total Epochs: {cfg.train.max_train_epochs}\")\n        logger.info(f\"Total Batch size: {total_batch_size}\")\n        logger.info(f\"Resume from epoch={self.start_epoch}, step={self.resume_step}\")\n        logger.info(\"**********************************************\")\n        self.progress_bar = tqdm(\n            range(self.global_step, cfg.train.max_train_steps),\n            total=cfg.train.max_train_steps,\n            disable=not self.accelerator.is_main_process,\n            initial=self.global_step,  # @note: huggingface seemed to missed this, should first update to global step\n            desc=\"Step\",\n        )\n        self.model.train()\n        for epoch in range(self.start_epoch, cfg.train.max_train_epochs):\n            train_loss = 0\n            for step, batch in enumerate(self.train_dataloader):\n                # * Skip steps until we reach the resumed step\n                if (\n                    self.ckpt_path is not None\n                    and epoch == self.start_epoch\n                    and step < self.resume_step\n                ):\n                    if step % cfg.train.gradient_accumulation_steps == 0:\n                        # @note: huggingface seemed to missed this, global step should also be updated\n                        self.global_step += 1\n                        self.progress_bar.update(1)\n                    continue\n                with self.accelerator.accumulate(self.model):\n                    loss = self.__one_step(batch)\n                    # gather loss across processes for logging\n                    avg_loss = self.accelerator.gather(\n                        loss.repeat(cfg.train.train_batch_size)\n                    ).mean()\n                    train_loss += avg_loss.item()\n                    # * 7. backward\n                    self.accelerator.backward(loss)\n                    if self.accelerator.sync_gradients:\n                        self.accelerator.clip_grad_norm_(\n                            self.model.parameters(), cfg.optim.max_grad_norm\n                        )\n                    # * 8. update\n                    self.optimizer.step()\n                    self.lr_scheduler.step()\n                    self.optimizer.zero_grad()\n\n                # Checks if the accelerator has performed an optimization step behind the scenes\n                if self.accelerator.sync_gradients:\n                    self.progress_bar.update(1)\n                    self.global_step += 1\n                    if self.cfg.log.with_tracking:\n                        self.accelerator.log(\n                            {\n                                \"train_loss\": train_loss,\n                                \"lr\": self.lr_scheduler.get_last_lr()[0],\n                            },\n                            step=self.global_step,\n                        )\n                    train_loss = 0.0\n                    if (\n                        isinstance(self.checkpointing_steps, int)\n                        and self.global_step % self.checkpointing_steps == 0\n                    ):\n                        ckpt_path = os.path.join(\n                            cfg.checkpoint.ckpt_dir, f\"checkpoint-{self.global_step}\"\n                        )\n                        if (\n                            self.cfg.checkpoint.keep_last_only\n                            and self.accelerator.is_main_process\n                        ):\n                            # del last save path\n                            if self.last_ckpt is not None:\n                                shutil.rmtree(self.last_ckpt)\n                            self.last_ckpt = ckpt_path\n                            logger.info(f\"self.savepath={ckpt_path}\")\n                        # wait main process handle dir del and create\n                        self.accelerator.wait_for_everyone()\n                        # @note: when using deepspeed, we can't use is_main_process, or it will get stucked\n                        self.accelerator.save_state(ckpt_path)\n                        logger.info(f\"Saved state to {ckpt_path}\")\n\n                logs = {\n                    \"loss\": loss.detach().item(),\n                    \"lr\": self.lr_scheduler.get_last_lr()[0],\n                }\n                self.progress_bar.set_postfix(**logs)\n                if self.global_step >= cfg.train.max_train_steps:\n                    break\n                # =======================Evaluation==========================\n                if (\n                    self.global_step > 0\n                    and cfg.train.log_interval > 0\n                    and (self.global_step + 1) % cfg.train.log_interval == 0\n                ):\n                    logger.info(\n                        f\"Evaluate on eval dataset [len: {len(self.eval_dataset)}]\"\n                    )\n                    self.model.eval()\n                    losses = []\n                    eval_bar = tqdm(\n                        self.eval_dataloader,\n                        disable=not self.accelerator.is_main_process,\n                    )\n                    for step, batch in enumerate(eval_bar):\n                        with torch.no_grad():\n                            loss = self.__one_step(batch)\n                        losses.append(\n                            self.accelerator.gather_for_metrics(\n                                loss.repeat(cfg.train.eval_batch_size)\n                            )\n                        )\n                    losses = torch.cat(losses)\n                    eval_loss = torch.mean(losses)\n                    logger.info(\n                        f\"global step {self.global_step}: eval_loss: {eval_loss}\"\n                    )\n                    if cfg.log.with_tracking:\n                        self.accelerator.log(\n                            {\n                                \"eval_loss\": eval_loss,\n                            },\n                            step=self.global_step,\n                        )\n                    # log image\n                    if cfg.log.log_image:\n                        self.log_image()\n                    self.model.train()  # back to train mode\n            # save ckpt for each epoch\n            if self.checkpointing_steps == \"epoch\":\n                ckpt_path = f\"epoch_{epoch}\"\n                if cfg.checkpoint.ckpt_dir is not None:\n                    ckpt_path = os.path.join(cfg.ckpt_dir, ckpt_path, \"autoencoder\")\n                if (\n                    self.cfg.checkpoint.keep_last_only\n                    and self.accelerator.is_main_process\n                ):\n                    # del last save path\n                    if self.last_ckpt is not None:\n                        shutil.rmtree(self.last_ckpt)\n                    self.last_ckpt = ckpt_path\n                # @note: when using deepspeed, we can't use is_main_process, or it will get stucked\n                self.accelerator.save_state(ckpt_path)\n                logger.info(f\"Saved state to {ckpt_path}\")\n\n        # end training\n        self.accelerator.wait_for_everyone()\n        if self.cfg.log.with_tracking:\n            self.accelerator.end_training()\n\n    def __one_step(self, batch: dict):\n        \"\"\"\n        __train_one_step: one diffusion backward step\n\n        Args:\n            - batch (dict):\n                  a batch of data, contains: pixel_values and input_ids\n\n        Returns:\n            - torch.Tensor:\n                  recon loss + kl loss\n        \"\"\"\n        # * 1. encode image\n        img = batch[\"pixel_values\"].to(self.weight_dtype)\n        dist: GaussianDistribution = self.model.encode(img).latent_dist\n        latent_vector = dist.sample()\n        recon_image = self.model.decode(latent_vector)\n        recon_loss = F.mse_loss(img.float(), recon_image.float(), reduction=\"mean\")\n        kl_loss = dist.kl()[0].to(dtype=torch.float32)  # recon loss is float32\n        # @ recon loss is float32, pass float16 loss will raise bias correction error in deepspeed cpu adam\n        return recon_loss + self.cfg.model.autoencoder.kl_weight * kl_loss\n\n    def recon(self, image):\n        image = image.unsqueeze(0).to(\n            self.accelerator.device, dtype=self.weight_dtype\n        )  # [1, ...]\n        latent_vector = self.model.encode(image).latent_dist.sample()\n        recon_latent = self.model.decode(latent_vector)\n        recon_digit = detransform(recon_latent)\n        return to_img(recon_digit, output_path=\"output\", name=\"autoencoder\")\n\n    def log_image(self):\n        recons = [self.recon(img) for img in self.test_images]\n        if self.cfg.log.with_tracking:\n            import wandb\n\n            self.accelerator.log(\n                {\n                    \"original_imgs\": [wandb.Image(img) for img in self.test_images],\n                    \"recon_imgs\": [wandb.Image(recon) for recon in recons],\n                },\n                step=self.global_step,\n            )", "\n\n@record\ndef main():\n    args, cfg = load_config()\n    model = AutoEncoderKL(cfg.model.autoencoder)\n    tokenizer = CLIPTokenizer.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        subfolder=\"tokenizer\",\n        use_fast=False,\n        cache_dir=\"data/pretrained\",\n    )\n    train_dataset = get_dataset(\n        cfg.dataset,\n        split=\"train\",\n        tokenizer=tokenizer,\n        logger=logger,\n    )\n    eval_dataset = get_dataset(\n        cfg.dataset,\n        split=\"validation\",\n        tokenizer=tokenizer,\n        logger=logger,\n    )\n    test_images = sample_test_image(\n        cfg.dataset,\n        split=\"test\",\n        tokenizer=tokenizer,\n        logger=logger,\n        num=10,\n    )\n    trainer = AutoencoderKLTrainer(\n        model,\n        args,\n        cfg,\n        train_dataset,\n        eval_dataset,\n        test_images=test_images,\n        collate_fn=collate_fn,\n    )\n    trainer.train()", "\n\nif __name__ == \"__main__\":\n    main()\n\n\n# to run without debug:\n# accelerate launch --config_file stable_diffusion/config/accelerate_config/deepspeed.yaml --main_process_port 29511 train_autoencoder.py --use-deepspeed --with-tracking --log-image --max-train-steps 10000 --max-train-samples 700 --max-val-samples 50 --max-test-samples 50 --resume-from-checkpoint latest --ckpt-dir model/autoencoder --learning-rate 1e-4\n", ""]}
{"filename": "trainer_args.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   trainer_args.py\n@Time    :   2023/05/26 20:11:49\n@Author  :   Wenbo Li\n@Desc    :   Args for logging, training, optimizer, scheduler, etc.\n\"\"\"\n\nfrom dataclasses import dataclass, field", "\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom stable_diffusion.dataclass import BaseDataclass\n\n\n@dataclass\nclass LogConfig(BaseDataclass):\n    logging_dir: str = field(default=\"logs\", metadata={\"help\": \"log directory\"})\n    with_tracking: bool = field(\n        default=False, metadata={\"help\": \"whether enable tracker\"}\n    )\n    report_to: str = field(\n        default=\"wandb\",\n        metadata={\"help\": \"tracker to use, only enabled when passed in --with_tracker\"},\n    )\n    resume: bool = field(\n        default=False, metadata={\"help\": \"whether resume from latest run\"}\n    )\n    log_image: bool = field(\n        default=False, metadata={\"help\": \"whether test image gen at each evaluation\"}\n    )\n    test_image: str = field(\n        default=\"data/test_images/test01.png\",\n        metadata={\"help\": \"test image path for log_image\"},\n    )", "class LogConfig(BaseDataclass):\n    logging_dir: str = field(default=\"logs\", metadata={\"help\": \"log directory\"})\n    with_tracking: bool = field(\n        default=False, metadata={\"help\": \"whether enable tracker\"}\n    )\n    report_to: str = field(\n        default=\"wandb\",\n        metadata={\"help\": \"tracker to use, only enabled when passed in --with_tracker\"},\n    )\n    resume: bool = field(\n        default=False, metadata={\"help\": \"whether resume from latest run\"}\n    )\n    log_image: bool = field(\n        default=False, metadata={\"help\": \"whether test image gen at each evaluation\"}\n    )\n    test_image: str = field(\n        default=\"data/test_images/test01.png\",\n        metadata={\"help\": \"test image path for log_image\"},\n    )", "\n\n@dataclass\nclass TrainConfig(BaseDataclass):\n    seed: int = field(default=42, metadata={\"help\": \"seed argument\"})\n    max_train_steps: int = field(\n        default=20000,\n        metadata={\"help\": \"total train steps, if provided, overrides max_train_epochs\"},\n    )\n    max_train_epochs: int = field(default=100, metadata={\"help\": \"max train epochs\"})\n    train_batch_size: int = field(\n        default=8, metadata={\"help\": \"train batch size per processor\"}\n    )\n    eval_batch_size: int = field(\n        default=8, metadata={\"help\": \"eval batch size per processor\"}\n    )\n    log_interval: int = field(\n        default=100,\n        metadata={\n            \"help\": \"do evaluation every n steps, default 0 means no evaluation during training\"\n        },\n    )\n    gradient_accumulation_steps: int = field(\n        default=4, metadata={\"help\": \"gradient accumulation steps\"}\n    )\n    use_deepspeed: bool = field(\n        default=False, metadata={\"help\": \"whether use deepspeed\"}\n    )\n    guidance_scale: float = field(\n        default=7.5, metadata={\"help\": \"guidance scale for classifier free guidance\"}\n    )", "\n\n@dataclass\nclass OptimConfig(BaseDataclass):\n    learning_rate: float = field(\n        default=4e-5, metadata={\"help\": \"learning rate argument\"}\n    )\n    adam_weight_decay: float = field(\n        default=0.1, metadata={\"help\": \"Adam weight decay argument\"}\n    )\n    use_8bit_adam: bool = field(\n        default=False, metadata={\"help\": \"Use 8-bit Adam argument\"}\n    )\n    max_grad_norm: float = field(\n        default=0.1, metadata={\"help\": \"max grad norm argument\"}\n    )\n    scheduler_type: str = field(\n        default=\"linear\", metadata={\"help\": \"scheduler type argument\"}\n    )\n    lr_warmup_steps: int = field(\n        default=500, metadata={\"help\": \"learning rate warm-up steps argument\"}\n    )", "\n\n# below are deprecated, now we use dataclass\n\n\ndef add_distributed_training_args(parser):\n    train_group = parser.add_argument_group(\"train\")\n    train_group.add_argument(\n        \"--logging_dir\", type=str, default=\"logs\", help=\"log directory\"\n    )\n    train_group.add_argument(\n        \"--with_tracker\",\n        type=str,\n        default=None,\n    )\n    train_group.add_argument(\"--report_to\", type=int, default=0, help=\"seed argument\")\n    train_group.add_argument(\"--seed\", type=int, default=0)\n    train_group.add_argument(\n        \"--train_batch_size\",\n        type=int,\n        default=8,\n    )\n    train_group.add_argument(\n        \"--max_train_steps\",\n        type=int,\n        default=20000,\n        help=\"total train steps, if provided, overrides max_train_epochs\",\n    )\n    train_group.add_argument(\n        \"--max_train_epochs\",\n        type=int,\n        default=100,\n        help=\"max train epochs, orverides by max_training_steps\",\n    )\n    train_group.add_argument(\n        \"--eval_batch_size\",\n        type=int,\n        default=1,\n    )\n    train_group.add_argument(\n        \"--gradient_accumulation_steps\",\n        type=int,\n        default=1,\n    )\n    return train_group", "\n\ndef add_optimization_args(parser):\n    optim_group = parser.add_argument_group(\"optim\")\n    optim_group.add_argument(\n        \"--learning_rate\",\n        type=float,\n        default=1e-4,\n    )\n    optim_group.add_argument(\"--adam_weight_decay\", type=float, default=0.1)\n    optim_group.add_argument(\n        \"--use_8bit_adam\",\n        action=\"store_true\",\n        default=False,\n    )\n    return optim_group", "\n\ndef add_lr_scheduler_args(parser):\n    lr_scheduler_group = parser.add_argument_group(\"lr_scheduler\")\n    lr_scheduler_group.add_argument(\n        \"--type\",\n        type=str,\n        default=\"linear\",\n    )\n    lr_scheduler_group.add_argument(\n        \"--lr_warmup_steps\",\n        type=int,\n        default=0,\n    )\n    return lr_scheduler_group", ""]}
{"filename": "test/test_args.py", "chunked_list": ["import argparse\nfrom dataclasses import dataclass, field, fields\nfrom omegaconf import DictConfig, OmegaConf\n\n\n# Define data classes\n@dataclass\nclass Train:\n    a1: int = field(default=1)\n", "\n\n@dataclass\nclass Val:\n    a2: int = field(default=2)\n\n\n# Create an argument parser\nparser = argparse.ArgumentParser()\n", "parser = argparse.ArgumentParser()\n\n# Get a list of data classes\ndata_classes = [Train, Val]\n\n# Create argument groups and arguments for each data class\nfor data_class in data_classes:\n    group = parser.add_argument_group(data_class.__name__.lower())\n    for field_info in fields(data_class):\n        arg_name = \"--\" + field_info.name\n        arg_type = field_info.type\n        default_value = field_info.default\n        group.add_argument(arg_name, type=arg_type, default=default_value)", "\n# Parse the command-line arguments\nargs = parser.parse_args()\n\n# Convert data classes to nested DictConfigs\ncfg = DictConfig({})\n\nfor data_class in data_classes:\n    data_instance = data_class()\n    for field_info in fields(data_class):\n        field_name = field_info.name\n        field_value = getattr(args, field_name)\n        setattr(data_instance, field_name, field_value)\n    group_name = data_class.__name__.lower()\n    group_config = OmegaConf.structured(data_instance)\n    cfg[group_name] = group_config", "\n# Access the arguments using the desired structure\nprint(cfg.train.a1)  # Output: 1\nprint(cfg.val.a2)  # Output: 2\n"]}
{"filename": "test/test_load_data.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   load_data.py\n@Time    :   2023/05/27 12:20:44\n@Author  :   Wenbo Li\n@Desc    :   test connection and path, If load dataset failed, you can also try this script\n\"\"\"\n\nfrom datasets import load_dataset", "\nfrom datasets import load_dataset\nimport os\n\n# from huggingface_hub import snapshot_download\n\nload_dataset(\n    \"poloclub/diffusiondb\",\n    \"2m_first_100k\",\n    cache_dir=os.path.join(\"data/dataset\", \"poloclub/diffusiondb\"),", "    \"2m_first_100k\",\n    cache_dir=os.path.join(\"data/dataset\", \"poloclub/diffusiondb\"),\n)\n"]}
{"filename": "utils/checkpointing_args.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   checkpointing_args.py\n@Time    :   2023/05/26 20:09:15\n@Author  :   Wenbo Li\n@Desc    :   dataclass to store arguments for ckpt management\n\"\"\"\n\nfrom dataclasses import dataclass, field", "\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom stable_diffusion.dataclass import BaseDataclass\n\n\n@dataclass\nclass CheckpointConfig(BaseDataclass):\n    keep_last_only: bool = field(\n        default=False,\n        metadata={\"help\": \"whether only keep the last ckpt\"},\n    )\n    ckpt_dir: str = field(\n        default=\"model\",\n        metadata={\"help\": \"dir to save and load checkpoints\"},\n    )\n    resume_from_checkpoint: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"dir to load checkpoints from\uff0c None refers to a new run, pass ltest for a latest resume\"\n        },\n    )\n    checkpointing_steps: Optional[str] = field(\n        default=100,\n        metadata={\n            \"help\": \"Whether the various states should be saved at the end of every n steps\",\n        },\n    )", "class CheckpointConfig(BaseDataclass):\n    keep_last_only: bool = field(\n        default=False,\n        metadata={\"help\": \"whether only keep the last ckpt\"},\n    )\n    ckpt_dir: str = field(\n        default=\"model\",\n        metadata={\"help\": \"dir to save and load checkpoints\"},\n    )\n    resume_from_checkpoint: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"dir to load checkpoints from\uff0c None refers to a new run, pass ltest for a latest resume\"\n        },\n    )\n    checkpointing_steps: Optional[str] = field(\n        default=100,\n        metadata={\n            \"help\": \"Whether the various states should be saved at the end of every n steps\",\n        },\n    )", "\n\n# deprecated\n\n\ndef add_checkpoint_args(parser):\n    checkpoint_group = parser.add_argument_group(\"checkpoint\")\n    checkpoint_group.add_argument(\n        \"--resume_from_checkpoint\",\n        type=str,\n        default=None,\n        help=\"dir to load checkpoints from\",\n    )\n    checkpoint_group.add_argument(\n        \"--output_dir\",\n        type=str,\n        default=\"dir to save and load checkpoints\",\n    )", ""]}
{"filename": "utils/model_utils.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   model_utils.py\n@Time    :   2023/05/26 20:08:47\n@Author  :   Wenbo Li\n@Desc    :   util script to load and build models\n\"\"\"\n\nfrom stable_diffusion.models.autoencoder import AutoEncoderKL", "\nfrom stable_diffusion.models.autoencoder import AutoEncoderKL\nfrom stable_diffusion.models.clip_model import CLIPModel\nfrom stable_diffusion.models.latent_diffusion import LatentDiffusion\n\nfrom stable_diffusion.models.scheduler import DDPMScheduler\nfrom stable_diffusion.models.unet import UNetModel\n\n\n# deprecated\ndef add_model_args(parser):\n    model_group = parser.add_argument_group(\"model\")\n    UNetModel.add_unet_args(model_group)\n    DDPMScheduler.add_ddpm_args(model_group)\n    CLIPModel.add_clip_args(model_group)\n    AutoEncoderKL.add_autoencoder_args(model_group)\n\n    return model_group", "\n# deprecated\ndef add_model_args(parser):\n    model_group = parser.add_argument_group(\"model\")\n    UNetModel.add_unet_args(model_group)\n    DDPMScheduler.add_ddpm_args(model_group)\n    CLIPModel.add_clip_args(model_group)\n    AutoEncoderKL.add_autoencoder_args(model_group)\n\n    return model_group", "\n\ndef build_models(model_cfg, logger=None):\n    noise_scheduler = DDPMScheduler(model_cfg.ddpm)\n    unet = UNetModel(\n        model_cfg.autoencoder.latent_channels,\n        model_cfg.autoencoder.groups,\n        model_cfg.unet,\n    )\n    text_encoder = CLIPModel(model_cfg.clip)\n    text_encoder.requires_grad_(False)\n    autoencoder = AutoEncoderKL(model_cfg.autoencoder)\n\n    count_params(unet, logger=logger)\n    count_params(text_encoder, logger=logger)\n    count_params(autoencoder, logger=logger)\n\n    return LatentDiffusion(\n        unet,\n        autoencoder,\n        text_encoder,\n        noise_scheduler,\n    )", "\n\ndef count_params(model, trainable_only=True, logger=None):\n    \"\"\"\n    Copied from original stable diffusion code [ldm/util.py](https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/util.py#L71)\n    Calculate model's total param count. If trainable_only is True then count only those requiring grads\n    \"\"\"\n\n    def numel(p):\n        return p.numel()\n\n    total_params = sum(\n        numel(p) for p in model.parameters() if not trainable_only or p.requires_grad\n    )\n    if logger is not None:\n        logger.info(\n            f\"{model.__class__.__name__} has {total_params * 1.e-6:.2f} M {'trainable' if trainable_only else ''} params.\"\n        )\n    return total_params", ""]}
{"filename": "utils/prepare_dataset.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   prepare_dataset.py\n@Time    :   2023/05/26 20:07:10\n@Author  :   Wenbo Li\n@Desc    :   code for loading and transform txt2img dataset\n\"\"\"\n\nimport os", "\nimport os\nimport random\nfrom typing import Optional\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom huggingface_hub import snapshot_download\nfrom datasets import load_dataset\nfrom transformers import CLIPTokenizer", "from datasets import load_dataset\nfrom transformers import CLIPTokenizer\n\nfrom dataclasses import dataclass, field\n\nfrom stable_diffusion.dataclass import BaseDataclass\nfrom PIL import Image\n\n\n@dataclass\nclass DatasetConfig(BaseDataclass):\n    dataset: str = field(\n        default=\"poloclub/diffusiondb\",\n        metadata={\"help\": \"name of the dataset to use.\"},\n    )\n    subset: Optional[str] = field(\n        default=None,  # \"2m_first_10k\",\n        metadata={\"help\": \"subset of the dataset to use.\"},\n    )\n    # dataset: str = field(\n    #     default=\"lambdalabs/pokemon-blip-captions\",\n    # )\n    data_dir: str = field(\n        default=\"data/dataset\",\n        metadata={\"help\": \"Cache directory to store loaded dataset.\"},\n    )\n    dataloader_num_workers: int = field(\n        default=4, metadata={\"help\": \"number of workers for the dataloaders.\"}\n    )\n    resolution: int = field(default=64, metadata={\"help\": \"resolution of the images.\"})\n    center_crop: bool = field(\n        default=True, metadata={\"help\": \"whether to apply center cropping.\"}\n    )\n    random_flip: bool = field(\n        default=False, metadata={\"help\": \"whether to apply random flipping.\"}\n    )\n    max_train_samples: Optional[int] = field(\n        default=9000, metadata={\"help\": \"max number of training samples to load.\"}\n    )\n    max_val_samples: Optional[int] = field(\n        default=500, metadata={\"help\": \"max number of validation samples to load.\"}\n    )\n    max_test_samples: Optional[int] = field(\n        default=500, metadata={\"help\": \"max number of test samples to load.\"}\n    )", "\n@dataclass\nclass DatasetConfig(BaseDataclass):\n    dataset: str = field(\n        default=\"poloclub/diffusiondb\",\n        metadata={\"help\": \"name of the dataset to use.\"},\n    )\n    subset: Optional[str] = field(\n        default=None,  # \"2m_first_10k\",\n        metadata={\"help\": \"subset of the dataset to use.\"},\n    )\n    # dataset: str = field(\n    #     default=\"lambdalabs/pokemon-blip-captions\",\n    # )\n    data_dir: str = field(\n        default=\"data/dataset\",\n        metadata={\"help\": \"Cache directory to store loaded dataset.\"},\n    )\n    dataloader_num_workers: int = field(\n        default=4, metadata={\"help\": \"number of workers for the dataloaders.\"}\n    )\n    resolution: int = field(default=64, metadata={\"help\": \"resolution of the images.\"})\n    center_crop: bool = field(\n        default=True, metadata={\"help\": \"whether to apply center cropping.\"}\n    )\n    random_flip: bool = field(\n        default=False, metadata={\"help\": \"whether to apply random flipping.\"}\n    )\n    max_train_samples: Optional[int] = field(\n        default=9000, metadata={\"help\": \"max number of training samples to load.\"}\n    )\n    max_val_samples: Optional[int] = field(\n        default=500, metadata={\"help\": \"max number of validation samples to load.\"}\n    )\n    max_test_samples: Optional[int] = field(\n        default=500, metadata={\"help\": \"max number of test samples to load.\"}\n    )", "\n\ndef add_dataset_args(parser):\n    dataset_group = parser.add_argument_group(\"dataset\")\n    dataset_group.add_argument(\n        \"--dataset\",\n        type=str,\n        default=\"poloclub/diffusiondb\",\n    )\n    dataset_group.add_argument(\n        \"--subset\",\n        type=str,\n        default=\"2m_first_100k\",\n    )\n    dataset_group.add_argument(\n        \"--data_dir\",\n        type=str,\n        default=\"data\",\n    )\n    dataset_group.add_argument(\n        \"--resolution\",\n        type=int,\n        default=64,\n    )\n    dataset_group.add_argument(\n        \"--center_crop\",\n        type=bool,\n        default=True,\n    )\n    dataset_group.add_argument(\n        \"--random_flip\",\n        type=bool,\n        default=True,\n    )", "\n\ndef collate_fn(examples):\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n\n\ndef tokenize_captions(examples, tokenizer, is_train=True):\n    captions = []\n    for caption in examples:\n        if isinstance(caption, str):\n            captions.append(caption)\n        elif isinstance(caption, (list, np.ndarray)):\n            # take a random caption if there are multiple\n            captions.append(random.choice(caption) if is_train else caption[0])\n        else:\n            raise ValueError(\n                \"Caption column should contain either strings or lists of strings.\"\n            )\n    inputs = tokenizer(\n        captions,\n        max_length=tokenizer.model_max_length,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    return inputs.input_ids", "\ndef tokenize_captions(examples, tokenizer, is_train=True):\n    captions = []\n    for caption in examples:\n        if isinstance(caption, str):\n            captions.append(caption)\n        elif isinstance(caption, (list, np.ndarray)):\n            # take a random caption if there are multiple\n            captions.append(random.choice(caption) if is_train else caption[0])\n        else:\n            raise ValueError(\n                \"Caption column should contain either strings or lists of strings.\"\n            )\n    inputs = tokenizer(\n        captions,\n        max_length=tokenizer.model_max_length,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    return inputs.input_ids", "\n\ndef get_transform(resolution, random_flip, center_crop):\n    return transforms.Compose(\n        [\n            transforms.Resize(\n                resolution, interpolation=transforms.InterpolationMode.BILINEAR\n            ),\n            transforms.CenterCrop(resolution)\n            if center_crop\n            else transforms.RandomCrop(resolution),\n            transforms.RandomHorizontalFlip()\n            if random_flip\n            else transforms.Lambda(lambda x: x),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5], [0.5]),\n        ]\n    )", "\n\ndef detransform(latent: torch.Tensor):\n    latent = latent.squeeze().cpu().detach().numpy()\n    latent = np.transpose(latent, (1, 2, 0))  # [c,h,w] -> [h,w,c]\n    latent = (latent + 1) / 2\n    latent = np.clip(latent, 0, 1)\n    return (latent * 255).astype(np.uint8)\n\n\ndef to_img(digit_img, output_path: str = \"\", name=\"sample\"):\n    img = Image.fromarray(digit_img.astype(np.uint8))\n    img.save(os.path.join(output_path, f\"{name}.png\"))\n    return img", "\n\ndef to_img(digit_img, output_path: str = \"\", name=\"sample\"):\n    img = Image.fromarray(digit_img.astype(np.uint8))\n    img.save(os.path.join(output_path, f\"{name}.png\"))\n    return img\n\n\ndef get_dataset(\n    args,\n    split: str = \"train\",\n    tokenizer: CLIPTokenizer = None,\n    logger=None,\n):\n    # check params\n    assert tokenizer is not None, \"you need to specify a tokenizer\"\n\n    assert split in {\n        \"train\",\n        \"validation\",\n        \"test\",\n    }, \"split should be one of train, validation, test\"\n\n    # most of the txt2img datasets are not splited into train, validation and test, manually split it\n    dataset = load_dataset(\n        args.dataset,\n        args.subset,\n        cache_dir=os.path.join(args.data_dir, args.dataset),\n    )[\"train\"]\n\n    if args.max_train_samples is not None and split == \"train\":\n        if args.max_train_samples < len(dataset):\n            dataset = dataset.select(range(args.max_train_samples))\n        elif logger is not None:\n            logger.info(\n                f\"max_train_samples({args.max_train_samples}) is larger than the number of train samples({len(dataset)})\"\n            )\n    if args.max_val_samples is not None and split == \"validation\":\n        if args.max_train_samples + args.max_val_samples < len(dataset):\n            dataset = dataset.select(\n                range(\n                    args.max_train_samples,\n                    args.max_train_samples + args.max_val_samples,\n                )\n            )\n        elif logger is not None:\n            logger.info(\n                f\"max_val_samples({args.max_val_samples}) is larger than the number of val samples({len(dataset)})\"\n            )\n    if args.max_test_samples is not None and split == \"test\":\n        if args.max_train_samples + args.max_val_samples + args.max_test_samples < len(\n            dataset\n        ):\n            dataset = dataset.select(\n                range(\n                    args.max_train_samples + args.max_val_samples,\n                    args.max_train_samples\n                    + args.max_val_samples\n                    + args.max_test_samples,\n                )\n            )\n        elif logger is not None:\n            logger.info(\n                f\"max_test_samples({args.max_test_samples}) is larger than the number of test samples({len(dataset)})\"\n            )\n\n    image_column = [col for col in [\"image\", \"img\"] if col in dataset.column_names][0]\n    caption_colum = [\n        col for col in [\"text\", \"caption\", \"prompt\"] if col in dataset.column_names\n    ][0]\n\n    transform = get_transform(args.resolution, args.random_flip, args.center_crop)\n\n    def preprocess_train(examples):\n        \"\"\"tokenize captions and convert images to pixel values\"\"\"\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"] = [transform(image) for image in images]\n        examples[\"input_ids\"] = tokenize_captions(examples[caption_colum], tokenizer)\n        return examples\n\n    if logger is not None:\n        logger.info(\n            f\"Loaded {len(dataset)} {split} samples from dataset:{args.dataset}\"\n        )\n\n    return dataset.with_transform(preprocess_train)", "def get_dataset(\n    args,\n    split: str = \"train\",\n    tokenizer: CLIPTokenizer = None,\n    logger=None,\n):\n    # check params\n    assert tokenizer is not None, \"you need to specify a tokenizer\"\n\n    assert split in {\n        \"train\",\n        \"validation\",\n        \"test\",\n    }, \"split should be one of train, validation, test\"\n\n    # most of the txt2img datasets are not splited into train, validation and test, manually split it\n    dataset = load_dataset(\n        args.dataset,\n        args.subset,\n        cache_dir=os.path.join(args.data_dir, args.dataset),\n    )[\"train\"]\n\n    if args.max_train_samples is not None and split == \"train\":\n        if args.max_train_samples < len(dataset):\n            dataset = dataset.select(range(args.max_train_samples))\n        elif logger is not None:\n            logger.info(\n                f\"max_train_samples({args.max_train_samples}) is larger than the number of train samples({len(dataset)})\"\n            )\n    if args.max_val_samples is not None and split == \"validation\":\n        if args.max_train_samples + args.max_val_samples < len(dataset):\n            dataset = dataset.select(\n                range(\n                    args.max_train_samples,\n                    args.max_train_samples + args.max_val_samples,\n                )\n            )\n        elif logger is not None:\n            logger.info(\n                f\"max_val_samples({args.max_val_samples}) is larger than the number of val samples({len(dataset)})\"\n            )\n    if args.max_test_samples is not None and split == \"test\":\n        if args.max_train_samples + args.max_val_samples + args.max_test_samples < len(\n            dataset\n        ):\n            dataset = dataset.select(\n                range(\n                    args.max_train_samples + args.max_val_samples,\n                    args.max_train_samples\n                    + args.max_val_samples\n                    + args.max_test_samples,\n                )\n            )\n        elif logger is not None:\n            logger.info(\n                f\"max_test_samples({args.max_test_samples}) is larger than the number of test samples({len(dataset)})\"\n            )\n\n    image_column = [col for col in [\"image\", \"img\"] if col in dataset.column_names][0]\n    caption_colum = [\n        col for col in [\"text\", \"caption\", \"prompt\"] if col in dataset.column_names\n    ][0]\n\n    transform = get_transform(args.resolution, args.random_flip, args.center_crop)\n\n    def preprocess_train(examples):\n        \"\"\"tokenize captions and convert images to pixel values\"\"\"\n        images = [image.convert(\"RGB\") for image in examples[image_column]]\n        examples[\"pixel_values\"] = [transform(image) for image in images]\n        examples[\"input_ids\"] = tokenize_captions(examples[caption_colum], tokenizer)\n        return examples\n\n    if logger is not None:\n        logger.info(\n            f\"Loaded {len(dataset)} {split} samples from dataset:{args.dataset}\"\n        )\n\n    return dataset.with_transform(preprocess_train)", "\n\ndef sample_test_image(args, split, tokenizer, logger, num: int = 10):\n    test_data = get_dataset(args, split=split, tokenizer=tokenizer, logger=logger)\n    images = []\n    for _ in range(num):\n        idx = np.random.randint(0, len(test_data))\n        images.append(test_data[idx][\"pixel_values\"])\n    return images\n", ""]}
{"filename": "utils/__init__.py", "chunked_list": [""]}
{"filename": "utils/parse_args.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   parse_args.py\n@Time    :   2023/05/25 10:46:53\n@Author  :   Wenbo Li\n@Desc    :   convert dataclass into argparse and group, copied a lot from fairseq\n\"\"\"\n\nimport argparse", "\nimport argparse\nfrom dataclasses import MISSING, fields\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom omegaconf import DictConfig, OmegaConf\nfrom stable_diffusion.dataclass import BaseDataclass\nfrom stable_diffusion.models.autoencoder import AutoencoderConfig\nfrom stable_diffusion.models.clip_model import ClipConfig\nfrom stable_diffusion.models.scheduler import DDPMConfig", "from stable_diffusion.models.clip_model import ClipConfig\nfrom stable_diffusion.models.scheduler import DDPMConfig\nfrom stable_diffusion.models.unet import UnetConfig\n\nfrom trainer_args import (\n    LogConfig,\n    TrainConfig,\n    OptimConfig,\n)\nfrom utils.checkpointing_args import CheckpointConfig", ")\nfrom utils.checkpointing_args import CheckpointConfig\nfrom utils.prepare_dataset import DatasetConfig\nimport re, ast, inspect\n\n\n# TODO: fix bug: type\ndef convert_dataclass_to_argparse(\n    data_classes: List[BaseDataclass], parser: argparse.ArgumentParser\n):\n    \"\"\"Create argument groups and arguments for each data class\"\"\"\n    for data_class in data_classes:\n        group = parser.add_argument_group(\n            data_class.__name__.lower().replace(\"config\", \"\")\n        )\n        # iterate over each arg\n        for field_info in fields(data_class):\n            # get args name\n            arg_name = f\"--{field_info.name.replace('_', '-')}\"\n            # attributes of this argument\n            kwargs = {\"type\": type(field_info.type)}\n            default_value = field_info.default\n            if default_value is MISSING:\n                kwargs[\"required\"] = True\n            # if arg_name == \"--use-8bit-adam\":\n            #     kwargs[\"action\"] = \"store_true\"\n            if field_info.type == bool:\n                kwargs[\"action\"] = (\n                    \"store_false\" if field_info.default is True else \"store_true\"\n                )\n\n            print(\n                f\"parse dataclass={data_class.__name__.lower()}, arg_name={arg_name}, \"\n            )\n            group.add_argument(arg_name, **kwargs)", "\n\ndef get_parser_from_dataclass(\n    dataclasses: List[BaseDataclass],\n    parser: argparse.ArgumentParser,\n):\n    \"\"\"\n    implementation using code from [fairseq](https://github.com/facebookresearch/fairseq/blob/25c20e6a5e781e4ef05e23642f21c091ba64872e/fairseq/dataclass/utils.py#L53)\n    \"\"\"\n    # comments are from https://zhuanlan.zhihu.com/p/558760615?utm_id=0\n\n    def eval_str_list(x, x_type=float):\n        if x is None:\n            return None\n        if isinstance(x, str):\n            if len(x) == 0:\n                return []\n            x = ast.literal_eval(x)\n        try:\n            return list(map(x_type, x))\n        except TypeError:\n            return [x_type(x)]\n\n    def interpret_dc_type(field_type):\n        if isinstance(field_type, str):\n            raise RuntimeError(\"field should be a type\")\n\n        if field_type == Any:\n            return str\n\n        typestring = str(field_type)\n        if re.match(\n            r\"(typing.|^)Union\\[(.*), NoneType\\]$\", typestring\n        ) or typestring.startswith(\"typing.Optional\"):\n            return field_type.__args__[0]\n        return field_type\n\n    def gen_parser_from_dataclass(\n        parser: argparse.ArgumentParser,\n        dataclass_instance: BaseDataclass,  # FairseqDataclass,\n        delete_default: bool = False,\n        with_prefix: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        convert a dataclass instance to tailing parser arguments.\n\n        If `with_prefix` is provided, prefix all the keys in the resulting parser with it. It means that we are\n        building a flat namespace from a structured dataclass (see transformer_config.py for example).\n        \"\"\"\n\n        def argparse_name(name: str):\n            \"\"\"\n            \u68c0\u67e5\u4e24\u4e2aCorner case\uff0cdata\u4e00\u822c\u662f\u4f4d\u7f6e\u53c2\u6570\uff0c\u4e0d\u9700\u8981\u4fee\u9970\u3002_name\u662fdataclass\u9690\u542b\u7684\u79c1\u6709\u53c2\u6570\uff0c\u4e0d\u9700\u8981\u4fee\u9970\u3002\n            \u5176\u4ed6\u7684\u90fd\u52a0\u4e0a--\uff0c\u540c\u65f6\u628a\u53d8\u91cf\u540d\u91cc\u7684\u4e0b\u5212\u7ebf\u6539\u6210\u77ed\u6a2a\u7ebf\uff0c\u8fd9\u662f\u56e0\u4e3alinux\u5728\u78b0\u5230 A_B \u7684\u65f6\u5019\uff0c\u4f1a\u5148\u89e3\u6790 A_ \uff0c\u5982\u679c\u6070\u597d\n            \u5b58\u5728\uff0c\u5c31\u8fd4\u56de {A_\u6307\u5411\u7684\u5185\u5bb9}B\uff0c\u5426\u5219\u5c31\u8fd4\u56de {\u7a7a}B\uff0c\u4f1a\u5f15\u8d77\u89e3\u6790\u9519\u8bef\u3002\n            \"\"\"\n            if name == \"data\" and (with_prefix is None or with_prefix == \"\"):\n                # normally data is positional args, so we don't add the -- nor the prefix\n                return name\n            if name == \"_name\":\n                # private member, skip\n                return None\n            full_name = \"--\" + name.replace(\"_\", \"-\")\n            if with_prefix is not None and with_prefix != \"\":\n                # if a prefix is specified, construct the prefixed arg name\n                full_name = with_prefix + \"-\" + full_name[2:]  # strip -- when composing\n            return full_name\n\n        def get_kwargs_from_dc(\n            dataclass_instance: BaseDataclass, k: str  # : FairseqDataclass,\n        ) -> Dict[str, Any]:\n            \"\"\"k: dataclass attributes\"\"\"\n\n            kwargs = {}\n\n            field_type = dataclass_instance._get_type(k)\n            # \u5bf9filed_type\u8fdb\u884c\u5408\u6cd5\u6027\u68c0\u67e5\uff0c\u5bf9any\u548coptional\u7c7b\u578b\u8fdb\u884c\u989d\u5916\u7684\u5904\u7406\uff0c\n            # \u5982\u679c\u662foptional\u6216\u8005union\uff0c\u5c31\u8fd4\u56de\u4e0d\u662fNONE\u7684\u7c7b\u578b\n            inter_type = interpret_dc_type(field_type)\n\n            field_default = dataclass_instance._get_default(k)\n\n            if isinstance(inter_type, type) and issubclass(inter_type, Enum):\n                field_choices = [t.value for t in list(inter_type)]\n            else:\n                field_choices = None\n\n            field_help = dataclass_instance._get_help(k)\n            # \u6709\u4e00\u4e9b\u5e38\u91cf\u4f1a\u88ab\u653e\u5728metadata\u91cc\uff0c\u8fd9\u4e9b\u5e38\u91cf\u66f4\u50cf\u662f\u9ed8\u8ba4\u503c\uff0c\u4f46\u8fd8\u4e0d\u6e05\u695a\u4e3a\u4ec0\u4e48\u4e0d\u653e\u5728default\u91cc\u5934\n            field_const = dataclass_instance._get_argparse_const(k)\n\n            if isinstance(field_default, str) and field_default.startswith(\"${\"):\n                kwargs[\"default\"] = field_default\n            else:\n                if field_default is MISSING:\n                    kwargs[\"required\"] = True\n                if field_choices is not None:\n                    kwargs[\"choices\"] = field_choices\n                # \u8fd9\u4e2a\u5730\u65b9\u5f88\u6709\u610f\u601d\uff0ckwargs\u91cc\u7684type\u5c45\u7136\u4e0d\u662f\u4e00\u4e2a\u56fa\u5b9a\u503c\uff0c\u800c\u662f\u4e00\u4e2alambda\u8868\u8fbe\u5f0f\uff0c\u4f3c\u4e4e\u662f\u5b9e\u9645\u7ed3\u5408\u518d\u8f6c\u6362\uff1f\n                if (\n                    isinstance(inter_type, type)\n                    and (issubclass(inter_type, List) or issubclass(inter_type, Tuple))\n                ) or (\"List\" in str(inter_type) or \"Tuple\" in str(inter_type)):\n                    if \"int\" in str(inter_type):\n                        kwargs[\"type\"] = lambda x: eval_str_list(x, int)\n                    elif \"float\" in str(inter_type):\n                        kwargs[\"type\"] = lambda x: eval_str_list(x, float)\n                    elif \"str\" in str(inter_type):\n                        kwargs[\"type\"] = lambda x: eval_str_list(x, str)\n                    else:\n                        raise NotImplementedError(\n                            \"parsing of type \" + str(inter_type) + \" is not implemented\"\n                        )\n                    if field_default is not MISSING:\n                        kwargs[\"default\"] = (\n                            \",\".join(map(str, field_default))\n                            if field_default is not None\n                            else None\n                        )\n                elif (\n                    isinstance(inter_type, type) and issubclass(inter_type, Enum)\n                ) or \"Enum\" in str(inter_type):\n                    kwargs[\"type\"] = str\n                    if field_default is not MISSING:\n                        if isinstance(field_default, Enum):\n                            kwargs[\"default\"] = field_default.value\n                        else:\n                            kwargs[\"default\"] = field_default\n                elif inter_type is bool:\n                    kwargs[\"action\"] = (\n                        \"store_false\" if field_default is True else \"store_true\"\n                    )\n                    kwargs[\"default\"] = field_default\n                else:\n                    kwargs[\"type\"] = inter_type\n                    if field_default is not MISSING:\n                        kwargs[\"default\"] = field_default\n\n            # build the help with the hierarchical prefix\n            if with_prefix is not None and with_prefix != \"\" and field_help is not None:\n                field_help = with_prefix[2:] + \": \" + field_help\n\n            kwargs[\"help\"] = field_help\n            if field_const is not None:\n                kwargs[\"const\"] = field_const\n                kwargs[\"nargs\"] = \"?\"\n\n            return kwargs\n\n        # _get_all_attributes\u53ef\u4ee5\u83b7\u53d6\u4e00\u4e2a\u6570\u636e\u7c7b\u91cc\u6240\u6709\u7684\u6570\u636e\u5b57\u6bb5\u7684\u53d8\u91cf\u540d->[]\n        for k in dataclass_instance._get_all_attributes():\n            # _get_name\u662f\u4ece__dataclass_fields__\u7684\u5b57\u5178\u91cc\u6839\u636e\u53d8\u91cf\u540d\u53bb\u62ffFields\u91cc\u7684name\uff0c\u4f46\u8fd9\u4fe9\u5e94\u8be5\u662f\u4e00\u6837\u7684\uff1f\n            # argparse_name\u4f1a\u4e3a\u5927\u90e8\u5206\u7684\u53d8\u91cf\u540d\u52a0\u4e0a--\uff0c\u4ece\u800c\u5f62\u6210\u53c2\u6570\u540d\u3002\u5e76\u628a\u4e0b\u5212\u7ebf\u8f6c\u51fa\u77ed\u6a2a\u7ebf\uff0c\u9632\u6b62\u89e3\u6790\u9519\u8bef\n            field_name = argparse_name(dataclass_instance._get_name(k))\n            # \u8fd4\u56de\u9ed8\u8ba4\u7c7b\u578b\n            field_type = dataclass_instance._get_type(k)\n            if field_name is None:\n                continue\n            elif inspect.isclass(field_type) and issubclass(field_type, BaseDataclass):\n                # \u5982\u679c\u8be5Fields\u662f\u6570\u636e\u7c7b\uff0c\u800c\u4e14\u8fd8\u662fFairseqDtaclass\u7684\u5b50\u7c7b\uff0c\u5c31\u53ef\u4ee5\u9012\u5f52\u52a0\u5165\u8be5\u5b50\u7c7b\u7684fields\n                # for fields that are of type FairseqDataclass, we can recursively\n                # add their fields to the namespace (so we add the args from model, task, etc. to the root namespace)\n                prefix = None\n                if with_prefix is not None:\n                    # if a prefix is specified, then we don't want to copy the subfields directly to the root namespace\n                    # but we prefix them with the name of the current field.\n                    prefix = field_name\n                gen_parser_from_dataclass(parser, field_type(), delete_default, prefix)\n                continue\n\n            kwargs = get_kwargs_from_dc(dataclass_instance, k)\n\n            field_args = [field_name]\n            # \u83b7\u53d6\u8fd9\u4e2a\u53c2\u6570\u7684\u522b\u540d\uff0c\u6bd4\u5982batch_size\u548c--max-sentence\uff0c\u8f93\u5165\u53c2\u6570\u7684\u65f6\u5019\u4e0d\u7ba1\u7ed9\u54ea\u4e2a\u4f20\u9012\uff0c\u90fd\u662f\u4e00\u6837\u7684\u3002\n            alias = dataclass_instance._get_argparse_alias(k)\n            if alias is not None:\n                field_args.append(alias)\n\n            if \"default\" in kwargs:\n                if isinstance(kwargs[\"default\"], str) and kwargs[\"default\"].startswith(\n                    \"${\"\n                ):\n                    if kwargs[\"help\"] is None:\n                        # this is a field with a name that will be added elsewhere\n                        continue\n                    else:\n                        del kwargs[\"default\"]\n                if delete_default and \"default\" in kwargs:\n                    del kwargs[\"default\"]\n            try:\n                # \u8fd9\u4e2a\u5730\u65b9\u86ee\u6709\u610f\u601d\uff0c\u5173\u4e8e\u5728\u5b9e\u53c2\u548c\u5f62\u53c2\u524d\u52a0\u661f\u53f7\u3002\u5f62\u53c2\u52a0\u661f\u53f7\u662f\u544a\u8bc9\u5927\u5bb6\u8fd9\u4e2a\u51fd\u6570\u91cc\u4f1a\u628a\u4ed6\u4f5c\u4e3a\u5143\u7ec4\u64cd\u4f5c\n                # \u5982\u679c\u5bf9\u4e8e\u5f62\u53c2\u662f\u5355\u661f\u53f7\uff0c\u5b9e\u53c2\u662f\u5143\u7ec4\u4f46\u662f\u6ca1\u5e26\u5355\u661f\u53f7\u7684\uff0c\u4f1a\u628a\u5b9e\u53c2\u4f5c\u4e3a\u5143\u7ec4\u5185\u7684\u4e00\u4e2a\u5143\u7d20\u5904\u7406\n                # \u4ece\u800c\u5728\u51fd\u6570\u4f53\u91cc\u9020\u6210\u5143\u7ec4\u7684\u5143\u7ec4\u7684\u95ee\u9898\uff0c\u5982\u679c\u5b9e\u53c2\u524d\u52a0\u4e86\u5355\u661f\u53f7\uff0c\u5c31\u4f1a\u628a\u5b9e\u53c2\u4ece\u5143\u7ec4\u89e3\u6210\u5f88\u591a\u4e2a\u53d8\u91cf\u4f20\u8fdb\u53bb\u3002\n                # https://www.cnblogs.com/sddai/p/14303453.html\n                parser.add_argument(*field_args, **kwargs)\n            except argparse.ArgumentError:\n                pass\n\n    for dataclass in dataclasses:\n        gen_parser_from_dataclass(parser, dataclass())", "\n\ndef load_config(gen_args_fn=get_parser_from_dataclass):\n    # train data classes\n    train_data_classes = [\n        LogConfig,\n        TrainConfig,\n        OptimConfig,\n        DatasetConfig,\n        CheckpointConfig,\n    ]\n\n    # model data classes\n    model_data_classes = [\n        UnetConfig,\n        AutoencoderConfig,\n        ClipConfig,\n        DDPMConfig,\n    ]\n\n    # Create an argument parser\n    parser = argparse.ArgumentParser()\n\n    # add train data classes into argparse\n    gen_args_fn(train_data_classes, parser)\n    gen_args_fn(model_data_classes, parser)\n\n    # Parse the command-line arguments\n    args = parser.parse_args()\n\n    cfg = DictConfig({})\n\n    def convert_dataclass_to_dictconfig(dataclasses, cfg: DictConfig):\n        \"\"\"Convert data classes to nested DictConfigs\"\"\"\n        for data_class in dataclasses:\n            data_instance = data_class()\n            for field_info in fields(data_class):\n                field_name = field_info.name\n                field_value = getattr(args, field_name)\n                setattr(data_instance, field_name, field_value)\n            group_name: str = data_class.__name__.lower().replace(\"config\", \"\")\n            group_config: DictConfig = OmegaConf.structured(data_instance)\n            cfg[group_name] = group_config\n\n    # add train data classes into DictConfig\n    convert_dataclass_to_dictconfig(train_data_classes, cfg=cfg)\n    # nest model data classes into cfg.model\n    cfg[\"model\"] = DictConfig({})\n    convert_dataclass_to_dictconfig(model_data_classes, cfg=cfg[\"model\"])\n    return args, cfg", ""]}
{"filename": "scripts/__init__.py", "chunked_list": ["from .txt2img import sample\n"]}
{"filename": "scripts/txt2img.py", "chunked_list": ["import torch\nimport os\nimport sys\n\n# TODO: fix this\nsys.path.append(os.path.join(os.path.dirname(__file__), \"..\"))\n\nfrom stable_diffusion.models.latent_diffusion import LatentDiffusion\nfrom utils.model_utils import build_models\nfrom utils.parse_args import load_config", "from utils.model_utils import build_models\nfrom utils.parse_args import load_config\nfrom utils.prepare_dataset import detransform, to_img\n\n\ndef sample(\n    model: LatentDiffusion,\n    image_size=64,\n    prompt: str = \"\",\n    time_steps: int = 50,\n    guidance_scale: float = 7.5,\n    scale_factor=1.0,\n    save_dir: str = \"output\",\n    device: str = \"cuda\",\n    weight_dtype=torch.float16,\n):\n    \"Sample an image given prompt\"\n    model.to(device=device, dtype=weight_dtype)\n    # random noise\n    # to get the right latent size\n    img_util = torch.randn(size=(1, 3, image_size, image_size)).to(\n        device, dtype=weight_dtype\n    )\n    noise = model.autoencoder.encode(img_util).latent_dist.sample()\n    noise = torch.rand_like(noise).to(device, dtype=weight_dtype)\n    # tokenize prompt\n    tokenized_prompt = model.text_encoder.tokenize([prompt]).input_ids.to(device)\n    context_emb = model.text_encoder.encode_text(tokenized_prompt)[0].to(\n        device, weight_dtype\n    )\n    x_0 = model.sample(\n        noised_sample=noise,\n        context_emb=context_emb,\n        guidance_scale=guidance_scale,\n        scale_factor=scale_factor,\n        time_steps=time_steps,\n    )\n    sample = model.autoencoder.decode(x_0)\n    sample = detransform(sample)\n    to_img(sample, output_path=save_dir, name=\"txt2img\")", "\n\nif __name__ == \"__main__\":\n    args, cfg = load_config()\n    model: LatentDiffusion = build_models(cfg.model)\n    sample(model, prompt=\"a cat\", image_size=64, time_steps=50, save_dir=\"output\")\n"]}
{"filename": "stable_diffusion/dataclass.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   dataclass.py\n@Time    :   2023/05/26 20:09:51\n@Author  :   Wenbo Li\n@Desc    :   Base dataclass copied from fairseq, Configs inherit from this class so that fairseq script can better convert dataclass into argparse\n\"\"\"\n\n", "\n\nfrom dataclasses import _MISSING_TYPE, dataclass\nfrom typing import List, Any, Optional\n\n\n@dataclass\nclass BaseDataclass:\n    \"\"\"Copied wonderful code from [fairseq/fairseq/dataclass/configs.py](https://github.com/facebookresearch/fairseq/blob/25c20e6a5e781e4ef05e23642f21c091ba64872e/fairseq/dataclass/configs.py#L28)\"\"\"\n\n    # _name: Optional[str] = None\n\n    # @staticmethod\n    # def name():\n    #     return None\n\n    def _get_all_attributes(self) -> List[str]:\n        return list(self.__dataclass_fields__.keys())\n\n    def _get_meta(\n        self, attribute_name: str, meta: str, default: Optional[Any] = None\n    ) -> Any:\n        return self.__dataclass_fields__[attribute_name].metadata.get(meta, default)\n\n    def _get_name(self, attribute_name: str) -> str:\n        return self.__dataclass_fields__[attribute_name].name\n\n    def _get_default(self, attribute_name: str) -> Any:\n        if hasattr(self, attribute_name):\n            if str(getattr(self, attribute_name)).startswith(\"${\"):\n                return str(getattr(self, attribute_name))\n            elif str(self.__dataclass_fields__[attribute_name].default).startswith(\n                \"${\"\n            ):\n                return str(self.__dataclass_fields__[attribute_name].default)\n            elif (\n                getattr(self, attribute_name)\n                != self.__dataclass_fields__[attribute_name].default\n            ):\n                return getattr(self, attribute_name)\n\n        f = self.__dataclass_fields__[attribute_name]\n        if not isinstance(f.default_factory, _MISSING_TYPE):\n            return f.default_factory()\n        return f.default\n\n    def _get_type(self, attribute_name: str) -> Any:\n        return self.__dataclass_fields__[attribute_name].type\n\n    def _get_help(self, attribute_name: str) -> Any:\n        return self._get_meta(attribute_name, \"help\")\n\n    def _get_argparse_const(self, attribute_name: str) -> Any:\n        return self._get_meta(attribute_name, \"argparse_const\")\n\n    def _get_argparse_alias(self, attribute_name: str) -> Any:\n        return self._get_meta(attribute_name, \"argparse_alias\")\n\n    def _get_choices(self, attribute_name: str) -> Any:\n        return self._get_meta(attribute_name, \"choices\")", ""]}
{"filename": "stable_diffusion/__init__.py", "chunked_list": ["\n"]}
{"filename": "stable_diffusion/models/clip_model.py", "chunked_list": ["from transformers import CLIPTextModel, CLIPTokenizer\nfrom torch import nn\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom stable_diffusion.dataclass import BaseDataclass\n\n\n@dataclass\nclass ClipConfig(BaseDataclass):\n    tokenizer: str = field(\n        default=\"runwayml/stable-diffusion-v1-5\",\n        metadata={\"help\": \"Tokenizer to use for text encoding.\"},\n    )\n    text_encoder: str = field(\n        default=\"runwayml/stable-diffusion-v1-5\",\n        metadata={\"help\": \"Text encoder model to use.\"},\n    )\n    max_seq_len: int = field(\n        default=77, metadata={\"help\": \"Maximum sequence length for tokenized text.\"}\n    )\n    model_dir: Optional[str] = field(\n        default=\"data/pretrained\",\n        metadata={\"help\": \"Path to a directory to store the pretrained CLIP model.\"},\n    )", "\n@dataclass\nclass ClipConfig(BaseDataclass):\n    tokenizer: str = field(\n        default=\"runwayml/stable-diffusion-v1-5\",\n        metadata={\"help\": \"Tokenizer to use for text encoding.\"},\n    )\n    text_encoder: str = field(\n        default=\"runwayml/stable-diffusion-v1-5\",\n        metadata={\"help\": \"Text encoder model to use.\"},\n    )\n    max_seq_len: int = field(\n        default=77, metadata={\"help\": \"Maximum sequence length for tokenized text.\"}\n    )\n    model_dir: Optional[str] = field(\n        default=\"data/pretrained\",\n        metadata={\"help\": \"Path to a directory to store the pretrained CLIP model.\"},\n    )", "\n\nclass CLIPModel(nn.Module):\n    @staticmethod\n    def add_clip_args(model_parser):\n        clip_group = model_parser.add_argument_group(\"clip\")\n        clip_group.add_argument(\n            \"--tokenizer\",\n            type=str,\n            default=\"runwayml/stable-diffusion-v1-5\",\n        )\n        clip_group.add_argument(\n            \"--text_encoder\",\n            type=str,\n            default=\"runwayml/stable-diffusion-v1-5\",\n        )\n        clip_group.add_argument(\n            \"--max_seq_len\",\n            type=int,\n            default=77,\n        )\n        clip_group.add_argument(\n            \"--cache_dir\",\n            type=str,\n            default=None,\n            help=\"Path to a directory to store the pretrained clip model\",\n        )\n        return clip_group\n\n    def __init__(\n        self,\n        cfg,\n    ):\n        super().__init__()\n        self.max_seq_len = cfg.max_seq_len\n        self.text_encoder: CLIPTextModel = CLIPTextModel.from_pretrained(\n            cfg.text_encoder, cache_dir=cfg.model_dir, subfolder=\"text_encoder\"\n        )\n        self.tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(\n            cfg.tokenizer,\n            use_fast=False,\n            cache_dir=cfg.model_dir,\n            subfolder=\"tokenizer\",\n        )\n\n    def tokenize(\n        self,\n        prompt: str = \"\",\n        max_length: int = None,\n        padding: str = \"max_length\",\n        truncation: bool = True,\n    ):\n        return self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            padding=padding,\n            truncation=truncation,\n            max_length=(self.max_seq_len if max_length is None else max_length),\n        )\n\n    def encode_text(self, text):\n        \"\"\"Encode text to text embedding\n        Args:\n            - text (str):\n                  text to encode, shape = [batch, seq_len]\n        Returns:\n            - text_embedding (torch.Tensor):\n                  text embedding, shape = [batch, seq_len, d_model]\n        \"\"\"\n        return self.text_encoder(text)", ""]}
{"filename": "stable_diffusion/models/unet.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   unet_2d_conditional.py\n@Time    :   2023/05/14 15:37:56\n@Author  :   Wenbo Li\n@Desc    :   implementation of Unet2d model and sub modules\n\"\"\"\n\nimport numpy as np", "\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom stable_diffusion.dataclass import BaseDataclass\nfrom .utils import (\n    build_conv_in,\n    build_input_blocks,\n    build_bottleneck,", "    build_input_blocks,\n    build_bottleneck,\n    build_output_blocks,\n    build_final_output,\n)\n\nfrom ..modules.timestep_embedding import sinusoidal_time_proj\n\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional", "from dataclasses import dataclass, field\nfrom typing import List, Optional\n\n\n@dataclass\nclass UnetConfig(BaseDataclass):\n    num_res_blocks: int = field(\n        default=2, metadata={\"help\": \"Number of residual blocks at each level.\"}\n    )\n    n_heads: int = field(\n        default=8, metadata={\"help\": \"Number of attention heads in transformers.\"}\n    )\n    attention_resolutions: List[int] = field(\n        default_factory=lambda: [0, 1],\n        metadata={\n            \"help\": \"At which level attention should be performed. e.g., [1, 2] means attention is performed at level 1 and 2.\"\n        },\n    )\n    channels_list: List[int] = field(\n        default_factory=lambda: [160, 320],\n        metadata={\"help\": \"Channels at each level.\"},\n    )\n    time_emb_dim: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"Time embedding dimension. If not specified, use 4 * channels_list[0] instead.\"\n        },\n    )\n    dropout: float = field(default=0.1, metadata={\"help\": \"Dropout rate.\"})\n    n_layers: int = field(default=2, metadata={\"help\": \"Number of transformer layers.\"})\n    context_dim: int = field(\n        default=768, metadata={\"help\": \"Embedding dim of context condition.\"}\n    )", "\n\nclass UNetModel(nn.Module):\n    r\"\"\"\n    The full `U-Net` model=`\u03b5_\u03b8(x_t, t, condition)` that takes noised latent x, time step and context condition, predicts the noise\n\n    `U-Net` as several levels(total `len(channel_mult)`), each level is a `TimestepEmbSequential`,\n    at each level there are sevreal(total `num_res_blocks`)\n    `ResBlocks`/`SpatialTransformer`/`AttentionBlock`/`UpSample`/`DownSample` blocks\n\n    Archetecture:\n      - input_blocks: [\n                    TimestepEmbSeq[`Conv2d]`,\n                    (num_levels-1) * (\n                        (num_res_blocks)* (TimestepEmbSeq[`Resblock`, Optioanl[`SpatialTransformer`]]),\n                        TimestepEmbSeq[`DownSample`]\n                    ),\n                    (num_res_blocks)* (TimestepEmbSeq[`Resblock`, Optioanl[`SpatialTransformer`]])\n            ]\n      - bottleneck: TimestepEmbSeq[`ResBlock`, `SpatialTransformer`, `ResBlock`]\n\n      - output_blocks: [\n                    (num_levels-1) * (\n                        (num_res_blocks)* (TimestepEmbSeq[`Resblock`, Optioanl[`SpatialTransformer`]]),\n                        TimestepEmbSeq[`UpSample`]\n                    ),\n                    (num_res_blocks)* (TimestepEmbSeq[`Resblock`, Optioanl[`SpatialTransformer`]])\n               ]\n      - out: [GroupNorm, SiLU, Conv2d]\n\n    Args:\n        - in_channels (int):\n                number of channels in the input feature map\n        - out_channels (int):\n                number of channels in the output feature map\n        - num_res_blocks (int):\n                number of residual blocks at each level\n        - n_heads (int):\n                num of attention heads in transformers\n        - attention_resolutions (List[int]):\n                at which level should attention be performed.\n                e.g. [1, 2] means attention is performed at level 1 and 2.\n        - channels_list (List[int]):\n                channels for each level of the UNet.\n        - dropout (float, optional):\n                dropout rate. Default: `0`.\n        - n_layers (int, optional):\n                num of transformer layers. Default: `1`.\n        - context_dim (int, optional):\n                embedding dim of context condition. Default: `768`.\n    \"\"\"\n\n    @staticmethod\n    def add_unet_args(parser):\n        unet_group = parser.add_argument_group(\"unet\")\n        unet_group.add_argument(\n            \"--num_res_blocks\",\n            type=int,\n            default=2,\n            help=\"number of residual blocks at each level\",\n        )\n        unet_group.add_argument(\n            \"--n_heads\",\n            type=int,\n            default=1,\n            help=\"num of attention heads in transformers\",\n        )\n        unet_group.add_argument(\n            \"--attention_resolutions\",\n            type=int,\n            nargs=\"+\",\n            default=[1],\n            help=\"at which level should attention be performed. e.g. [1, 2] means attention is performed at level 1 and 2.\",\n        )\n        unet_group.add_argument(\n            \"--channels_list\",\n            type=int,\n            nargs=\"+\",\n            default=[64, 128],\n            help=\"channels at each level\",\n        )\n        unet_group.add_argument(\n            \"--time_emb_dim\",\n            type=int,\n            default=None,\n            help=\"time embedding dimension, if not specified, use 4 * channels_list[0] instead\",\n        )\n        unet_group.add_argument(\n            \"--dropout\",\n            type=float,\n            default=0.0,\n            help=\"dropout rate\",\n        )\n        unet_group.add_argument(\n            \"--n_layers\",\n            type=int,\n            default=1,\n            help=\"num of transformer layers\",\n        )\n        unet_group.add_argument(\n            \"--context_dim\",\n            type=int,\n            default=768,\n            help=\"embedding dim of context condition\",\n        )\n\n    def __init__(\n        self,\n        latent_channels,\n        groups,\n        cfg,  # unet config\n    ):\n        super().__init__()\n        num_res_blocks = cfg.num_res_blocks\n        n_heads = cfg.n_heads\n        attention_resolutions = cfg.attention_resolutions\n        channels_list = cfg.channels_list\n        dropout = cfg.dropout\n        n_layers = cfg.n_layers\n        context_dim = cfg.context_dim\n        self.context_dim = cfg.context_dim\n\n        self.channels = channels_list[0]\n        # * 1. time emb\n        time_emb_dim = cfg.time_emb_dim or channels_list[0] * 4\n        timestep_input_dim = channels_list[0]\n        self.time_embedding = nn.Sequential(\n            nn.Linear(timestep_input_dim, time_emb_dim),\n            nn.SiLU(),\n            nn.Linear(time_emb_dim, time_emb_dim),\n        )\n        # * 2. conv in\n        self.conv_in = build_conv_in(latent_channels, self.channels)\n        # num of levels\n        levels = len(channels_list)\n        # * 3. input blocks\n        (\n            self.input_blocks,\n            input_block_channels,\n            mid_ch,\n            d_head,\n            attn_mult,\n        ) = build_input_blocks(\n            in_channels=self.channels,\n            num_res_blocks=num_res_blocks,\n            attention_resolutions=attention_resolutions,\n            n_heads=n_heads,\n            n_layers=n_layers,\n            dropout=dropout,\n            context_dim=context_dim,\n            levels=levels,\n            time_emb_dim=time_emb_dim,\n            channels_list=channels_list,\n            groups=groups,\n        )\n        # @ note: openai recalculated d_heads for attention in the bottoleneck, but that seems redundant(so as out_ch=ch and then ch=out_ch)\n        # see origin code: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/modules/diffusionmodules/openaimodel.py#L443\n        # * 4. bottleneck\n        # bottleneck has one resblock, then one attention block/spatial transformer, and then one resblock\n        self.middle_block = build_bottleneck(\n            in_ch=mid_ch,\n            time_emb_dim=time_emb_dim,\n            n_heads=n_heads,\n            d_head=d_head,\n            n_layers=n_layers,\n            dropout=dropout,\n            context_dim=context_dim,\n            groups=groups,\n        )\n        # * 5. output blocks\n        self.output_blocks, out_ch = build_output_blocks(\n            num_res_blocks=num_res_blocks,\n            attention_resolutions=attention_resolutions,\n            n_heads=n_heads,\n            n_layers=n_layers,\n            dropout=dropout,\n            context_dim=context_dim,\n            input_block_channels=input_block_channels,\n            levels=levels,\n            time_emb_dim=time_emb_dim,\n            channels_list=channels_list,\n            in_ch=mid_ch,\n            attn_mult=attn_mult,\n            groups=groups,\n        )\n        # * 6. final output\n        self.out = build_final_output(\n            out_ch=out_ch, out_channels=latent_channels, groups=groups\n        )\n\n    def time_proj(self, time_steps: torch.Tensor, max_len: int = 10000) -> torch.Tensor:\n        \"\"\"\n        time_step_embedding use sinusoidal time step embedding as default, feel free to try out other embeddings\n\n        Args:\n            - time_steps (torch.Tensor):\n                  time step of shape `[batch_size,]`\n            - max_len (int, optional):\n                  max len of embedding. Default: `10000`.\n\n        Returns:\n            - torch.Tensor:\n                  time embedding of shape `[batch, emb_dim]`\n        \"\"\"\n        return sinusoidal_time_proj(time_steps, self.channels, max_len)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        timesteps: torch.Tensor,\n        context_emb: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        forward pass, predict noise given noised image x, time step and context\n\n        Args:\n            - x (torch.Tensor):\n                  input latent of shape `[batch_size, channels, width, height]`\n            - timesteps (torch.Tensor):\n                  time steps of shape `[batch_size]`\n            - context_emb (torch.Tensor, optional):\n                  cross attention context embedding of shape `[batch_size, seq_len, context_dim]`. Default: None.\n\n        Returns:\n            - torch.Tensor:\n                  output latent of shape `[batch_size, channels, width, height]`\n        \"\"\"\n        # check parameters\n        if context_emb is not None:\n            assert (\n                context_emb.shape[-1] == self.context_dim\n            ), f\"context dim from passed in context({context_emb.shape}) should be equal to self.context_dim({self.context_dim})\"\n\n        # store input blocks for skip connection\n        x_input_blocks = []\n        # * Get time embedding\n        time_emb = self.time_proj(timesteps).to(x.dtype)\n        time_emb = self.time_embedding(time_emb)\n        # * conv in layer\n        x = self.conv_in(x)\n        x_input_blocks.append(x)\n        # * input blocks\n        for module in self.input_blocks:\n            x = module(x, time_emb, context_emb)\n            x_input_blocks.append(x)\n        # * bottleneck\n        x = self.middle_block(x, time_emb, context_emb)\n        # * output blocks\n        for module in self.output_blocks:\n            # skip connection from input blocks\n            x = torch.cat([x, x_input_blocks.pop()], dim=1)\n            x = module(x, time_emb, context_emb)\n        return self.out(x)", ""]}
{"filename": "stable_diffusion/models/scheduler.py", "chunked_list": ["from torch import nn\nimport torch\nfrom typing import List\nfrom dataclasses import dataclass, field\n\nfrom stable_diffusion.dataclass import BaseDataclass\n\n\n@dataclass\nclass DDPMConfig(BaseDataclass):\n    noise_schedule: str = field(\n        default=\"linear\",\n        metadata={\n            \"help\": \"Noise schedule type.\",\n            \"choices\": [\"linear\", \"cosine\", \"cubic\"],\n        },\n    )\n    noise_steps: int = field(default=1000, metadata={\"help\": \"Number of noise steps.\"})\n    beta_start: float = field(\n        default=1e-4, metadata={\"help\": \"Starting value of beta.\"}\n    )\n    beta_end: float = field(default=0.02, metadata={\"help\": \"Ending value of beta.\"})", "@dataclass\nclass DDPMConfig(BaseDataclass):\n    noise_schedule: str = field(\n        default=\"linear\",\n        metadata={\n            \"help\": \"Noise schedule type.\",\n            \"choices\": [\"linear\", \"cosine\", \"cubic\"],\n        },\n    )\n    noise_steps: int = field(default=1000, metadata={\"help\": \"Number of noise steps.\"})\n    beta_start: float = field(\n        default=1e-4, metadata={\"help\": \"Starting value of beta.\"}\n    )\n    beta_end: float = field(default=0.02, metadata={\"help\": \"Ending value of beta.\"})", "\n\nclass DDPMScheduler:\n    @staticmethod\n    def add_ddpm_args(parser):\n        noise_group = parser.add_argument_group(\"ddpm\")\n        noise_group.add_argument(\n            \"--noise_schedule\",\n            type=str,\n            default=\"linear\",\n            choices=[\"linear\", \"cosine\", \"cubic\"],\n        )\n        noise_group.add_argument(\n            \"--noise_steps\",\n            type=int,\n            default=1000,\n        )\n        noise_group.add_argument(\n            \"--beta_start\",\n            type=float,\n            default=1e-4,\n        )\n        noise_group.add_argument(\n            \"--beta_end\",\n            type=float,\n            default=0.02,\n        )\n        return noise_group\n\n    def __init__(self, cfg):\n        \"\"\"modified from [labmlai - DDPMSampler](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/stable_diffusion/sampler/ddpm.py)\"\"\"\n        super().__init__()\n        self.noise_steps: int = cfg.noise_steps\n        self.noise_time_steps: torch.Tensor[List[int]] = torch.arange(\n            self.noise_steps - 1, -1, -1\n        )\n        self.betas = self.prepare_linear_noise_schedule(cfg.beta_start, cfg.beta_end)\n        self.alpha = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alpha, dim=0)\n        #  $\\bar\\alpha_{t-1}$\n        alpha_bar_prev = torch.cat(\n            [self.alphas_cumprod.new_tensor([1.0]), self.alphas_cumprod[:-1]]\n        )\n        # $\\sqrt{\\bar\\alpha}$\n        self.sqrt_alpha_bar = self.alphas_cumprod**0.5\n        # $\\sqrt{1 - \\bar\\alpha}$\n        self.sqrt_1m_alpha_bar = (1.0 - self.alphas_cumprod) ** 0.5\n        # $\\frac{1}{\\sqrt{\\bar\\alpha_t}}$\n        self.sqrt_recip_alpha_bar = self.alphas_cumprod**-0.5\n        # $\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}$\n        self.sqrt_recip_m1_alpha_bar = (1 / self.alphas_cumprod - 1) ** 0.5\n        # $\\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t$\n        variance = self.betas * (1.0 - alpha_bar_prev) / (1.0 - self.alphas_cumprod)\n        # Clamped log of $\\tilde\\beta_t$\n        self.log_var = torch.log(torch.clamp(variance, min=1e-20))\n        # $\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$\n        self.mean_x0_coef = (\n            self.betas * (alpha_bar_prev**0.5) / (1.0 - self.alphas_cumprod)\n        )\n        # $\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$\n        self.mean_xt_coef = (\n            (1.0 - alpha_bar_prev)\n            * ((1 - self.betas) ** 0.5)\n            / (1.0 - self.alphas_cumprod)\n        )\n\n    def prepare_linear_noise_schedule(self, beta_start, beta_end):\n        \"\"\"\n        Returns the linear noise schedule\n        \"\"\"\n        return torch.linspace(beta_start, beta_end, self.noise_steps)\n\n    def add_noise(\n        self,\n        original_samples: torch.Tensor,\n        noise: torch.Tensor,\n        timesteps: torch.Tensor,\n    ):\n        r\"\"\"\n        sample x_t from q(x_t|x_0), where\n        `q(x_t|x_0) = N(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t)I)`\n        Modified from [Huggingface/Diffusers - scheduling_ddpm.py](https://github.com/huggingface/diffusers/blob/67cf0445ef48b1f913b90ce0025ac0c75673e32e/src/diffusers/schedulers/scheduling_ddpm.py#L419)\n\n        Args:\n            - original_samples (torch.Tensor):\n                  x_0, origin latent vector, shape=`[batch, channels, height, width]`\n            - noise (torch.Tensor):\n                  random noise, shape=`[batch, channels, height, width]`\n            - timesteps (torch.Tensor):\n                  time step to add noise, shape=`[batch,]`\n        Returns:\n            - noised latent vector (torch.Tensor):\n                    x_t, shape=`[batch, channels, height, width]`\n        \"\"\"\n        assert (\n            timesteps.max().item() < self.noise_steps\n        ), f\"timesteps({timesteps}) should be less than {self.noise_steps}\"\n        # Make sure alphas_cumprod and timestep have same device and dtype as original_samples\n        alphas_cumprod = self.alphas_cumprod.to(\n            device=original_samples.device, dtype=original_samples.dtype\n        )\n        timesteps = timesteps.to(original_samples.device)\n\n        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n\n        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n\n        noisy_samples = (\n            sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n        )\n        return noisy_samples\n\n    @torch.no_grad()\n    def step(\n        self,\n        pred_noise: torch.Tensor,\n        x_t: torch.Tensor,\n        time_step: int,\n        repeat_noise: bool = False,\n        scale_factor: float = 1.0,\n    ):\n        \"\"\"\n        predict x_t from\n        Sample \ud835\udc99_{\ud835\udc95-1} from \ud835\udc91_\u03b8(\ud835\udc99_{\ud835\udc95-1} | \ud835\udc99_\ud835\udc95) i.e. decode one step\n        Modified from [labmlai - DDPMSampler](https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/diffusion/stable_diffusion/sampler/ddpm.py)\n\n        Args:\n            - pred_noise (torch.Tensor):\n                  predicted noise from U-Net model = \u03b5_\u03b8(x_t, t, condition)\n            - x_t (torch.Tensor):\n                  noised latent at timestep=t, shape=`[batch_size, channels, height, width]`\n            - time_step (int):\n                  integer of timestep=t\n            - repeat_noise (bool, optional):\n                  whether use the same noise for all items in batch. Default: `False`.\n            - scale_factor (float, optional):\n                  scale_factor of noise. Default: `1.`.\n        \"\"\"\n        assert (\n            time_step < self.noise_steps\n        ), f\"timesteps({time_step}) should be less than {self.noise_steps}\"\n        bsz = x_t.shape[0]\n\n        # 1 / (\\sqrt{\\bar\\alpha_t})\n        sqrt_recip_alpha_bar = x_t.new_full(\n            (bsz, 1, 1, 1), self.sqrt_recip_alpha_bar[time_step]\n        )\n        # $\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}$\n        sqrt_recip_m1_alpha_bar = x_t.new_full(\n            (bsz, 1, 1, 1), self.sqrt_recip_m1_alpha_bar[time_step]\n        )\n\n        # Calculate $x_0$ with current $\\epsilon_\\theta$\n        # Eq (15) from DDPM paper: https://arxiv.org/pdf/2006.11239.pdf\n        # $$x_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}} x_t -  \\Big(\\sqrt{\\frac{1}{\\bar\\alpha_t} - 1}\\Big)\\epsilon_\\theta$$\n        x0 = sqrt_recip_alpha_bar * x_t - sqrt_recip_m1_alpha_bar * pred_noise\n\n        # $\\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}$\n        mean_x0_coef = x_t.new_full((bsz, 1, 1, 1), self.mean_x0_coef[time_step])\n        # $\\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}$\n        mean_xt_coef = x_t.new_full((bsz, 1, 1, 1), self.mean_xt_coef[time_step])\n\n        # Calculate $\\mu_t(x_t, t)$\n        # $$\\mu_t(x_t, t) = \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n        #    + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t$$\n        mean = mean_x0_coef * x0 + mean_xt_coef * x_t\n        # $\\log \\tilde\\beta_t$\n        log_var = x_t.new_full((bsz, 1, 1, 1), self.log_var[time_step])\n\n        # Do not add noise when $t = 1$ (final step sampling process).\n        # Note that `step` is `0` when $t = 1$)\n        if time_step == 0:\n            noise = torch.zeros(x_t.shape)\n        # If same noise is used for all samples in the batch\n        elif repeat_noise:\n            noise = torch.randn((1, *x_t.shape[1:]))\n        # Different noise for each sample\n        else:\n            noise = torch.randn(x_t.shape)\n\n        # Multiply noise by the temperature\n        noise = noise.to(mean.device, dtype=mean.dtype) * scale_factor\n        # Sample from,\n        # $$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\mu_\\theta(x_t, t), \\tilde\\beta_t \\mathbf{I} \\big)$$\n        x_prev = mean + (0.5 * log_var).exp() * noise\n        # return x_0 if we try to get x_0 directly from x_t, but it makes distortion more difficult to evaluate,\n        # see Eq (5) from DDPM paper: https://arxiv.org/pdf/2006.11239.pdf\n        return (\n            x_prev,\n            x0,\n        )", ""]}
{"filename": "stable_diffusion/models/__init__.py", "chunked_list": ["\n"]}
{"filename": "stable_diffusion/models/autoencoder.py", "chunked_list": ["from typing import List\nfrom torch import nn\n\nfrom stable_diffusion.dataclass import BaseDataclass\n\nfrom ..modules.distributions import GaussianDistribution\nimport torch\n\nfrom .utils import (\n    build_conv_in,", "from .utils import (\n    build_conv_in,\n    build_input_blocks,\n    build_bottleneck,\n    build_output_blocks,\n    build_final_output,\n)\n\n\nfrom dataclasses import dataclass, field", "\nfrom dataclasses import dataclass, field\nfrom typing import List, Optional\n\n\n@dataclass\nclass AutoencoderConfig(BaseDataclass):\n    in_channels: int = field(\n        default=3, metadata={\"help\": \"Number of input channels of the input image.\"}\n    )\n    latent_channels: int = field(\n        default=4, metadata={\"help\": \"Embedding channels of the latent vector.\"}\n    )\n    out_channels: Optional[int] = field(\n        default=3,\n        metadata={\n            \"help\": \"Number of output channels of the decoded image. Should be the same as in_channels.\"\n        },\n    )\n    autoencoder_channels_list: List[int] = field(\n        default_factory=lambda: [64, 128],\n        metadata={\n            \"help\": \"Comma-separated list of channel multipliers for each level.\"\n        },\n    )\n    autoencoder_num_res_blocks: int = field(\n        default=2, metadata={\"help\": \"Number of residual blocks per level.\"}\n    )\n    groups: int = field(\n        default=32, metadata={\"help\": \"Number of groups for GroupNorm.\"}\n    )\n    kl_weight: float = field(default=1.0, metadata={\"help\": \"Weight of the KL loss.\"})", "\n\nclass AutoEncoderKL(nn.Module):\n    @staticmethod\n    def add_autoencoder_args(parser):\n        autoencoder = parser.add_argument_group(\"autoencoder\")\n        autoencoder.add_argument(\n            \"--in_channels\",\n            type=int,\n            default=3,\n            help=\"number of input channels of input image\",\n        )\n        autoencoder.add_argument(\n            \"--latent_channels\",\n            type=int,\n            default=4,\n            help=\"embedding channels of latent vector\",\n        )\n        autoencoder.add_argument(\n            \"--out_channels\",\n            type=int,\n            default=3,\n            help=\"number of output channels of decoded image, should be the same with in_channels\",\n        )\n        autoencoder.add_argument(\n            \"--autoencoder_channels_list\",\n            type=int,\n            nargs=\"+\",\n            default=[64, 128],\n            help=\"comma separated list of channels multipliers for each level\",\n        )\n        autoencoder.add_argument(\n            \"--autoencoder_num_res_blocks\",\n            type=int,\n            default=2,\n            help=\"number of residual blocks per level\",\n        )\n        autoencoder.add_argument(\n            \"--groups\",\n            type=int,\n            default=32,\n            help=\"number of groups for GroupNorm\",\n        )\n\n    def __init__(\n        self,\n        cfg,\n    ):\n        # check params\n        assert (\n            cfg.out_channels is None or cfg.out_channels == cfg.in_channels\n        ), f\"input channels({cfg.input_channels}) of image should be equal to output channels({cfg.out_channels})\"\n        super(AutoEncoderKL, self).__init__()\n        self.latent_channels = latent_channels = cfg.latent_channels\n        self.encoder = self.build_encoder(cfg)\n        self.decoder = self.build_decoder(cfg)\n        # Convolution to map from embedding space to\n        # quantized embedding space moments (mean and log variance)\n        self.quant_conv = nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1)\n        # Convolution to map from quantized embedding space back to\n        # embedding space\n        self.post_quant_conv = nn.Conv2d(latent_channels, latent_channels, 1)\n\n    @classmethod\n    def build_encoder(cls, cfg):\n        return Encoder(\n            in_channels=cfg.in_channels,\n            out_channels=cfg.latent_channels,\n            channels_list=cfg.autoencoder_channels_list,\n            num_res_blocks=cfg.autoencoder_num_res_blocks,\n            groups=cfg.groups,\n        )\n\n    @staticmethod\n    def build_decoder(cfg):\n        return Decoder(\n            in_channels=cfg.latent_channels,\n            out_channels=cfg.out_channels or cfg.in_channels,\n            channels_list=cfg.autoencoder_channels_list,\n            num_res_blocks=cfg.autoencoder_num_res_blocks,\n            groups=cfg.groups,\n        )\n\n    def encode(self, img: torch.Tensor) -> GaussianDistribution:\n        \"\"\"\n        Encode image into latent vector\n        Args:\n            - x (torch.Tensor):\n                  image, shape = `[batch, channel, height, width]`\n        Returns:\n            - gaussian distribution (torch.Tensor):\n\n        \"\"\"\n        z = self.encoder(img)\n        # Get the moments in the quantized embedding space\n        moments = self.quant_conv(z)\n        # Return the distribution(posterior)\n        return AutoEncoderKLOutput(GaussianDistribution(moments))\n\n    def decode(self, latent: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Decode images from latent representation\n\n        Args:\n            - z(torch.Tensor):\n                  latent representation with shape `[batch_size, emb_channels, z_height, z_height]`\n        \"\"\"\n        # check params\n        assert (\n            latent.shape[1] == self.latent_channels\n        ), f\"Expected latent representation to have {self.latent_channels} channels, got {z.shape[1]}\"\n        z = self.post_quant_conv(latent)\n        return self.decoder(z)", "\n\nclass Encoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        channels_list: List[int],\n        num_res_blocks: int,\n        groups: int = 4,\n    ):\n        super(Encoder, self).__init__()\n        self.conv_in = build_conv_in(\n            in_channels=in_channels, out_channels=channels_list[0]\n        )\n        levels = len(channels_list)\n        self.down, _, mid_ch, _, _ = build_input_blocks(\n            in_channels=channels_list[0],\n            num_res_blocks=num_res_blocks,\n            levels=levels,\n            channels_list=channels_list,\n            groups=groups,\n        )\n        self.bottleneck = build_bottleneck(\n            mid_ch, d_head=mid_ch, use_attn_only=True, groups=groups\n        )\n        self.out = build_final_output(\n            out_ch=mid_ch, out_channels=2 * out_channels, groups=groups\n        )\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        for module in self.down:\n            x = module(x)\n        x = self.bottleneck(x)\n        for module in self.out:\n            x = module(x)\n        return x", "\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        channels_list: List[int],\n        num_res_blocks: int,\n        groups: int = 4,\n    ):\n        super(Decoder, self).__init__()\n        self.conv_in = build_conv_in(\n            in_channels=in_channels, out_channels=channels_list[0]\n        )\n        levels = len(channels_list)\n        self.bottleneck = build_bottleneck(\n            in_ch=channels_list[0],\n            d_head=channels_list[0],\n            use_attn_only=True,\n            groups=groups,\n        )\n        self.up, mid_ch = build_output_blocks(\n            num_res_blocks=num_res_blocks,\n            in_ch=channels_list[0],\n            levels=levels,\n            channels_list=channels_list,\n            groups=groups,\n        )\n\n        self.out = build_final_output(\n            out_ch=mid_ch, out_channels=out_channels, groups=groups\n        )\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.bottleneck(x)\n        for module in self.up:\n            x = module(x)\n        for module in self.out:\n            x = module(x)\n        return x", "\n\n@dataclass\nclass AutoEncoderKLOutput:\n    latent_dist: \"GaussianDistribution\"\n"]}
{"filename": "stable_diffusion/models/latent_diffusion.py", "chunked_list": ["from typing import Optional\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\n\nfrom stable_diffusion.models.autoencoder import AutoEncoderKL\nfrom stable_diffusion.models.clip_model import CLIPModel\nfrom stable_diffusion.models.scheduler import DDPMScheduler\nfrom stable_diffusion.models.unet import UNetModel\n", "from stable_diffusion.models.unet import UNetModel\n\n\nclass LatentDiffusion(nn.Module):\n    def __init__(\n        self,\n        unet: UNetModel,\n        autoencoder: AutoEncoderKL,\n        text_encoder: CLIPModel,\n        noise_scheduler: DDPMScheduler,\n    ):\n        \"\"\"main class\"\"\"\n        super().__init__()\n        self.unet = unet\n        self.autoencoder = autoencoder\n        self.text_encoder = text_encoder\n        self.noise_scheduler = noise_scheduler\n\n    def pred_noise(\n        self,\n        noised_sample: torch.Tensor,\n        time_step: torch.Tensor,\n        context_emb: torch.Tensor,\n        guidance_scale: float = 1.0,\n    ):\n        \"\"\"\n        predicts the noise added on latent vector at time step=t\n\n        Args:\n            - x (torch.Tensor):\n                  noised latent vector, shape = `[batch, channels, height, width]`\n            - time_steps (torch.Tensor):\n                  time steps, shape = `[batch]`\n            - context_emb (torch.Tensor):\n                  conditional embedding, shape = `[batch, seq_len, d_model]`\n\n        Returns:\n            - pred noise (torch.Tensor):\n                  predicted noise, shape = `[batch, channels, height, width]`\n        \"\"\"\n        do_classifier_free_guidance = guidance_scale > 1\n        if not do_classifier_free_guidance:\n            return self.unet(noised_sample, time_step, context_emb)\n        t_in = torch.cat([time_step] * 2)\n        x_in = torch.cat([noised_sample] * 2)\n        bsz = noised_sample.shape[0]\n        tokenized_text = self.text_encoder.tokenize([\"\"] * bsz).input_ids.to(\n            noised_sample.device\n        )\n        uncond_emb = self.text_encoder.encode_text(\n            tokenized_text  # adding `.to(self.weight_dtype)` causes error...\n        )[0]\n        c_in = torch.cat([uncond_emb, context_emb])\n        pred_noise_cond, pred_noise_uncond = torch.chunk(\n            self.unet(x_in, t_in, c_in), 2, dim=0\n        )\n        return pred_noise_cond + guidance_scale * (pred_noise_cond - pred_noise_uncond)\n\n    def sample(\n        self,\n        noised_sample: torch.Tensor,\n        context_emb: torch.Tensor,\n        guidance_scale: float = 7.5,\n        repeat_noise: bool = False,\n        scale_factor: float = 1.0,\n        time_steps: Optional[int] = None,\n    ):\n        \"\"\"\n        Sample loop to get x_0 given x_t and conditional embedding\n\n        Args:\n            - noised_sample (torch.Tensor):\n                  original x_t of shape=`[batch, channels, height, width]`\n            - context_emb (torch.Tensor):\n                  conditional embedding of shape=`[batch, seq_len, context_dim]`\n            - guidence_scale (float, optional):\n                  scale used for classifer free guidance. Default: `7.5`.\n            - repeat_noise (bool, optional):\n                  whether use the same noise in a batch duuring each p_sample. Default: `False`.\n            - scale_factor (float, optional):\n                  scaling factor of noise. Default: `1.0`.\n\n        Returns:\n            - x_0 (torch.Tensor):\n                  denoised latent x_0 of shape=`[batch, channels, height, width]`\n        \"\"\"\n        bsz = noised_sample.shape[0]\n\n        # Get x_T\n        x = noised_sample\n\n        # Time steps to sample at $T - t', T - t' - 1, \\dots, 1$\n\n        # Sampling loop\n        if time_steps is not None:\n            noise_time_steps = range(time_steps - 1, -1, -1)\n        else:\n            noise_time_steps = self.noise_scheduler.noise_time_steps\n        progress_bar = tqdm(reversed(noise_time_steps), desc=\"Sampling\")\n        for step in progress_bar:\n            # fill time step t from int to tensor of shape=`[batch]`\n            time_step = x.new_full((bsz,), step, dtype=torch.long)\n            pred_noise = self.pred_noise(\n                noised_sample=x,\n                time_step=time_step,\n                context_emb=context_emb,\n                guidance_scale=guidance_scale,\n            )\n            # Sample x_{t-1}\n            x, pred_x0 = self.noise_scheduler.step(\n                pred_noise=pred_noise,\n                x_t=x,\n                time_step=step,\n                repeat_noise=repeat_noise,\n                scale_factor=scale_factor,\n            )\n        # Return $x_0$\n        return x", ""]}
{"filename": "stable_diffusion/models/utils.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   utils.py\n@Time    :   2023/05/15 12:55:11\n@Author  :   Wenbo Li\n@Desc    :   Util class for build model blocks\n\"\"\"\n\nfrom typing import Optional", "\nfrom typing import Optional\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom torch.nn import ModuleList\nfrom ..modules.timestep_embedding import TimestepEmbedSequential\nfrom ..modules.resnet2d import DownSample, ResBlock, UpSample\n", "from ..modules.resnet2d import DownSample, ResBlock, UpSample\n\nfrom ..modules.transformer import SpatialTransformer, CrossAttention\n\n\ndef zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module", "\n\ndef build_conv_in(in_channels: int, out_channels: int):\n    return nn.Conv2d(in_channels, out_channels, 3, padding=1)\n\n\ndef build_input_blocks(\n    in_channels: int,\n    num_res_blocks: int,\n    attention_resolutions: Optional[list] = None,\n    n_heads: int = 1,\n    n_layers: int = 1,\n    dropout: float = 0.0,\n    context_dim: Optional[int] = None,\n    levels: int = 1,\n    time_emb_dim: Optional[int] = None,\n    channels_list: list = None,\n    groups: int = 4,\n):\n    input_blocks = nn.ModuleList([])\n    # num of channels at each block\n    # will use reversly in output blocks\n    input_block_channels = [in_channels]\n    # * add levels input blocks(down sample layers)\n    in_ch = in_channels\n    # @ note: in labmlai, they use level in attn_resolutions but in origin stable diffusion paper, they use ds = [1,2,4,...],total num levels\n    attn_mult = 1\n    d_head = None\n    for level in range(levels):\n        for _ in range(num_res_blocks):\n            # Residual block maps from previous number of channels to the number of\n            # channels in the current level\n            out_ch = channels_list[level]\n            layers = [\n                ResBlock(\n                    in_channels=in_ch,\n                    out_channels=out_ch,\n                    time_emb_dim=time_emb_dim,\n                    groups=groups,\n                )\n            ]\n            in_ch = (\n                out_ch  # this level's output channels is next level's input channels\n            )\n            # * add attention layers\n            if attention_resolutions is not None and attn_mult in attention_resolutions:\n                d_head = in_ch // n_heads\n                layers.append(\n                    SpatialTransformer(\n                        in_ch,\n                        n_heads=n_heads,\n                        d_head=d_head,\n                        n_layers=n_layers,\n                        dropout=dropout,\n                        context_dim=context_dim,\n                        groups=groups,\n                    )\n                )\n            input_blocks.append(TimestepEmbedSequential(*layers))\n            input_block_channels.append(in_ch)\n        # * add DownSample block except last level\n        if level != levels - 1:\n            input_blocks.append(TimestepEmbedSequential(DownSample(in_ch)))\n            # add additional in_channels for downsample\n            input_block_channels.append(in_ch)\n            attn_mult *= (\n                2  # double it for next itr, this won't be multiplied in the last layer\n            )\n    return input_blocks, input_block_channels, in_ch, d_head, attn_mult", "\n\ndef build_bottleneck(\n    in_ch: int,\n    time_emb_dim: Optional[int] = None,\n    n_heads: int = 1,\n    d_head: int = 1,\n    n_layers: int = 1,\n    dropout: float = 0.0,\n    context_dim: Optional[int] = None,\n    use_attn_only: bool = False,\n    groups: int = 4,\n):\n    return TimestepEmbedSequential(\n        ResBlock(in_channels=in_ch, time_emb_dim=time_emb_dim, dropout=dropout),\n        (\n            CrossAttention(\n                query_dim=in_ch, n_heads=n_heads, d_head=d_head, dropout=dropout\n            )\n            if use_attn_only\n            else SpatialTransformer(\n                in_ch,\n                n_heads=n_heads,\n                d_head=d_head,\n                n_layers=n_layers,\n                dropout=dropout,\n                context_dim=context_dim,\n                groups=groups,\n            )\n        ),\n        ResBlock(\n            in_channels=in_ch, time_emb_dim=time_emb_dim, dropout=dropout, groups=groups\n        ),\n    )", "\n\ndef build_output_blocks(\n    num_res_blocks: int,\n    attention_resolutions: Optional[list] = None,\n    n_heads: int = 1,\n    n_layers: int = 1,\n    dropout: float = 0.0,\n    context_dim: Optional[int] = None,\n    input_block_channels: Optional[ModuleList] = None,\n    levels: int = 1,\n    time_emb_dim: Optional[int] = None,\n    channels_list: list = None,\n    in_ch: int = 1,\n    attn_mult: Optional[int] = None,\n    groups: int = 4,\n):\n    output_blocks = nn.ModuleList([])\n    for level in reversed(range(levels)):\n        # * add resblocks\n        for res_block in range(num_res_blocks + 1):\n            # Residual block maps from previous number of channels to the number of\n            # channels in the current level\n            # * note: here should add input_block_channels.pop() because input blocks are used as skip connection\n            out_ch = channels_list[level]\n            layers = [\n                ResBlock(\n                    in_channels=in_ch + input_block_channels.pop()\n                    if input_block_channels\n                    else in_ch,\n                    out_channels=out_ch,\n                    time_emb_dim=time_emb_dim,\n                    dropout=dropout,\n                    groups=groups,\n                )\n            ]\n            in_ch = out_ch\n            # * add attention layers\n            if attention_resolutions is not None and attn_mult in attention_resolutions:\n                d_head = in_ch // n_heads\n                layers.append(\n                    SpatialTransformer(\n                        in_ch,\n                        n_heads=n_heads,\n                        d_head=d_head,\n                        n_layers=n_layers,\n                        dropout=dropout,\n                        context_dim=context_dim,\n                        groups=groups,\n                    )\n                )\n\n            # *add UpSample except the last one, note that in reversed order, level==0 is the last\n            if level != 0 and res_block == num_res_blocks:\n                layers.append(TimestepEmbedSequential(UpSample(in_ch)))\n                if attn_mult:\n                    attn_mult //= 2\n            output_blocks.append(TimestepEmbedSequential(*layers))\n    return output_blocks, in_ch", "\n\ndef build_final_output(out_ch: int, out_channels: int, groups: int = 4):\n    return nn.Sequential(\n        nn.GroupNorm(groups, out_ch),\n        nn.SiLU(),\n        nn.Conv2d(\n            in_channels=out_ch, out_channels=out_channels, kernel_size=3, padding=1\n        ),\n    )", "\n\nif __name__ == \"__main__\":\n    batch = 10\n    image_size = 64\n    in_channels = 3\n    channels = 128\n    out_channels = 3\n    tim_emb_dim = 512\n    x = np.random.randn(batch, in_channels, image_size, image_size)\n    x = torch.from_numpy(x).float()\n    # test upsample\n    upsample = UpSample(\n        in_channels=channels,\n    )\n    up = upsample(x)\n    print(up.shape)\n    # test downsample\n    downsample = DownSample(\n        in_channels=channels,\n    )\n    down = downsample(x)\n    print(down.shape)\n    # test resblock\n    resblock = ResBlock(\n        in_channels=channels, out_channels=out_channels, time_emb_dim=tim_emb_dim\n    )\n    res = resblock(x, time_emb=torch.ones(size=(batch, tim_emb_dim)))\n    print(res.shape)", ""]}
{"filename": "stable_diffusion/modules/timestep_embedding.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   timestep_embedding.py\n@Time    :   2023/05/22 22:54:29\n@Author  :   Wenbo Li\n@Desc    :   Time step embedding implement\n\"\"\"\n\nfrom abc import abstractmethod", "\nfrom abc import abstractmethod\nfrom typing import Optional\nimport torch\nimport math\nfrom torch import nn\nfrom .resnet2d import ResBlock\nfrom .transformer import SpatialTransformer\n\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, time_emb):\n        pass", "\n\nclass TimestepBlock(nn.Module):\n    \"\"\"\n    Any module where forward() takes timestep embeddings as a second argument.\n    \"\"\"\n\n    @abstractmethod\n    def forward(self, x, time_emb):\n        pass", "\n\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    def forward(\n        self,\n        x: torch.Tensor,\n        time_emb: Optional[torch.Tensor] = None,\n        context: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        forward pass of TimestepEmbeddingSequential\n\n        passed in layers are basically:\n            `ResBlock`, `Conv2d`, `UpSample`,`DownSample`, `SpatialTransformer`\n        apply different forward pass depending on instance type\n\n        Args:\n            - x (Tensor):\n                  input shape=[batch, in_channels, height, width]\n            - time_emb (Tensor):\n                  input shape=[batch, time_emb_dim]\n            - context (Tensor, optional):\n                  input shape=[batch, seq_len, context_dim]. Default: None.\n\n        Returns:\n            - Tensor:\n                - output shape:\n                    - `ResBlock`:           [batch, out_channels, height, width]\n                    - `SpatialTransformer`: [batch, out_channels, height, width]\n                    - `Conv2d`:             [batch, out_channels, height, width]\n                    - `UpSample`:           [batch, out_channels, height*scale_factor, width*scale_factor]\n                    - `DownSample`:         [batch, out_channels, height/scale_factor, width/scale_factor]\n        \"\"\"\n        for module in self:\n            # pass ResBlock\n            if isinstance(module, ResBlock):\n                x = module(x, time_emb)\n            # pass spatial transformer\n            elif isinstance(module, SpatialTransformer):\n                x = module(x, context)\n            # pass Conv2d, UpSample, DownSample\n            else:\n                x = module(x)\n        return x", "\n\ndef sinusoidal_time_proj(\n    time_steps: torch.Tensor, emb_dim: int, max_len: int = 10000\n) -> torch.Tensor:\n    \"\"\"\n    sinusoidal time step embedding implementation\n\n    Args:\n        - time_steps (torch.Tensor):\n                time step of shape [batch_size,]\n        - emb_dim (int):\n                embed dimension\n        - max_len (int, optional):\n                max len of embedding. Default: 10000.\n\n    Returns:\n        - torch.Tensor:\n                time embedding of shape [batch, emb_dim]\n    \"\"\"\n    # half is sin, the other half is cos\n    half = emb_dim // 2\n    freq = torch.exp(\n        math.log(max_len)\n        / (half)\n        * torch.arange(\n            0, end=half, dtype=torch.float32\n        )  # get the position of each time step\n    ).to(time_steps.device)\n    # shape=[batch, 1]*[1, half] = [batch, half]\n    # freq[None]: shape=[half] -> [1, half]\n    # time_steps[:, None]: shape=[batch] -> [batch, 1]\n    args = time_steps[:, None].float() * freq[None]\n    # shape=[batch, emb_dim]\n    return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)", ""]}
{"filename": "stable_diffusion/modules/resnet2d.py", "chunked_list": ["import torch.nn as nn\nimport numpy as np\nimport torch\nfrom typing import Optional\nimport torch.nn.functional as F\nfrom ..models import utils\n\n\nclass UpSample(nn.Module):\n    \"\"\"\n    constructor for UpSample layer\n\n    Architecture:\n        - Interpolate\n        - Conv2d\n\n    Args:\n        - in_channels (int):\n                input  num of channels\n        - out_channels (int, optional):\n                output  num of channels\n        - scale_factor (int, optional):\n                Up Sample by a factor of `scale factor`. Default: `2`.\n        - padding (int, optional):\n                padding for `Conv2d`. Default: `1`.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: Optional[int] = None,\n        scale_factor: float = 2,\n        padding: int = 1,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        # if has out_channels passed in, use it or use default = in_channels\n        self.out_channels = out_channels or in_channels\n        self.scale_factor = scale_factor\n        # * default we use Conv2d, use_conv and conv_nd are not implemented for simpilicity\n        self.conv = nn.Conv2d(\n            self.in_channels, self.out_channels, kernel_size=3, padding=padding\n        )\n\n    def forward(self, x) -> torch.Tensor:\n        \"\"\"\n        forward Up Sample layer which\n\n        Args:\n            - x (Tensor):\n                  input shape=`[batch, in_channels, height, width]`\n\n        Returns:\n            - Tensor:\n                  output shape=`[batch, out_channels, height*scale_factor, width*scale_factor]`\n        \"\"\"\n        assert (\n            x.shape[1] == self.in_channels\n        ), f\"input channel does not match: x.shape[1]({x.shape[1]}) != self.in_channels({self.in_channels}))\"\n        # * e.g. [1,3,256,256] -> [1,3,512,512]\n        x = F.interpolate(x, scale_factor=self.scale_factor, mode=\"nearest\")\n        x = self.conv(x)\n        return x", "class UpSample(nn.Module):\n    \"\"\"\n    constructor for UpSample layer\n\n    Architecture:\n        - Interpolate\n        - Conv2d\n\n    Args:\n        - in_channels (int):\n                input  num of channels\n        - out_channels (int, optional):\n                output  num of channels\n        - scale_factor (int, optional):\n                Up Sample by a factor of `scale factor`. Default: `2`.\n        - padding (int, optional):\n                padding for `Conv2d`. Default: `1`.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: Optional[int] = None,\n        scale_factor: float = 2,\n        padding: int = 1,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        # if has out_channels passed in, use it or use default = in_channels\n        self.out_channels = out_channels or in_channels\n        self.scale_factor = scale_factor\n        # * default we use Conv2d, use_conv and conv_nd are not implemented for simpilicity\n        self.conv = nn.Conv2d(\n            self.in_channels, self.out_channels, kernel_size=3, padding=padding\n        )\n\n    def forward(self, x) -> torch.Tensor:\n        \"\"\"\n        forward Up Sample layer which\n\n        Args:\n            - x (Tensor):\n                  input shape=`[batch, in_channels, height, width]`\n\n        Returns:\n            - Tensor:\n                  output shape=`[batch, out_channels, height*scale_factor, width*scale_factor]`\n        \"\"\"\n        assert (\n            x.shape[1] == self.in_channels\n        ), f\"input channel does not match: x.shape[1]({x.shape[1]}) != self.in_channels({self.in_channels}))\"\n        # * e.g. [1,3,256,256] -> [1,3,512,512]\n        x = F.interpolate(x, scale_factor=self.scale_factor, mode=\"nearest\")\n        x = self.conv(x)\n        return x", "\n\nclass DownSample(nn.Module):\n    \"\"\"\n    constructor for DownSample layer\n\n    Architecture:\n        - Conv2d\n        - Interpolate\n\n    Args:\n        - in_channels (int):\n                input  num of channels\n        - out_channels (int, optional):\n                output  num of channels\n        - scale_factor (int, optional):\n                Down Sample by a factor of `scale factor`. Default: 1/2.\n        - padding (int, optional):\n                padding for Conv2d. Default: 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: Optional[int] = None,\n        scale_factor: float = 1 / 2,\n        padding: int = 1,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.scale_factor = scale_factor\n        # if has out_channels passed in, use it or use default = in_channels\n        self.out_channels = out_channels or in_channels\n        # It turn out that origin openai unet model did not use UpSample and DownSample,\n        # instead, they use ResNetBlock with parameters up, down\n        # Here we use UpSample and DownSample to make it clean\n        # Change: => add stride=2, to downsample by a factor of 2\n        # or use scale factor = 1/2\n        # TODO: compare difference between stride=2 and scale_factor=1/2\n        self.conv = nn.Conv2d(\n            self.in_channels, self.out_channels, kernel_size=3, padding=padding\n        )\n\n    def forward(self, x) -> torch.Tensor:\n        \"\"\"\n        forward Down Sample layer which reduce height and width of x by a factor of 2\n\n        Args:\n            - x (Tensor):\n                  input shape=[batch, in_channels, height, width]\n\n        Returns:\n            - Tensor:\n                  output shape=[batch, out_channels, height/scale_factor, width/scale_factor]\n        \"\"\"\n        assert (\n            x.shape[1] == self.in_channels\n        ), f\"input channel does not match: x.shape[1]({x.shape[1]}) != self.in_channels({self.in_channels}))\"\n        x = self.conv(x)\n        x = F.interpolate(x, scale_factor=self.scale_factor, mode=\"nearest\")\n        return x", "\n\nclass ResBlock(nn.Module):\n    \"\"\"\n    ResBlock used in U-Net and AutoEncoder\n\n    Archetecture::\n        - in_layers = [`GroupNorm`, `SiLU`, `Conv2d`]\n        - time_emb = [`SiLU`, `Linear`]\n        - out_layers = [`GroupNorm`, `SiLU`, `Dropout`]\n\n    Args:\n        - in_channels (int):\n              input num of channels\n        - out_channels (Optional[int], optional):\n              output num of channels. Default: equal to `in_channels`.\n        - time_emb_dim (int, optional):\n              time embedding dim. Default: `512`.\n        - dropout (int, optional):\n              dropout rate. Default: `0.`\n        - padding (int, optional):\n              padding idx. Default: `1`.\n        - groups (int, optional):\n              num of groups for `GroupNorm`. Default: `2`.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: Optional[int] = None,\n        time_emb_dim: Optional[int] = None,\n        dropout: Optional[int] = 0,\n        padding: Optional[int] = 1,\n        groups: int = 2,\n    ) -> None:\n        super().__init__()\n        # check parameters\n        assert (\n            in_channels % groups == 0\n        ), f\"in_channels({in_channels}) must be divisible by num_groups({groups})\"\n        self.in_channels = in_channels\n        # if has out_channels passed in, use it or use default = in_channels\n        self.out_channels = out_channels or in_channels\n        self.time_emb_dim = time_emb_dim\n        self.dropout = dropout\n\n        self.in_layers = nn.Sequential(\n            nn.GroupNorm(num_groups=groups, num_channels=in_channels),\n            nn.SiLU(),\n            nn.Conv2d(\n                self.in_channels, self.out_channels, kernel_size=3, padding=padding\n            ),\n        )\n        # Time embedding\n        if self.time_emb_dim:\n            self.time_embedding = nn.Sequential(\n                nn.SiLU(),\n                nn.Linear(self.time_emb_dim, self.out_channels),\n            )\n        else:\n            self.time_embedding = nn.Identity()\n\n        self.out_layers = nn.Sequential(\n            nn.GroupNorm(num_groups=groups, num_channels=self.out_channels),\n            nn.SiLU(),\n            nn.Dropout(self.dropout),\n            # ? openai model used zero_module(conv_nd)\n            # TODO: figure out why zero_module is used\n            # [batch, in_channel, height, width] => [batch, out_channel, height, height]\n            utils.zero_module(\n                nn.Conv2d(\n                    self.out_channels, self.out_channels, kernel_size=3, padding=padding\n                )\n            ),\n        )\n        # Map input to output channel\n        if self.in_channels != self.out_channels:\n            self.skip_connection = nn.Conv2d(\n                self.in_channels, self.out_channels, kernel_size=1\n            )\n        else:\n            self.skip_connection = nn.Identity()\n\n    def forward(\n        self, x: torch.Tensor, time_emb: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        \"\"\"\n        forward pass of ResBlock\n\n        Args:\n            - x (torch.Tensor):\n                  input shape = `[batch, in_channels, height, width]`\n            - time_emb (torch.Tensor):\n                  input shape = `[batch, time_emb_dim]`\n\n        Returns:\n            - torch.Tensor:\n                  output shape = `[batch, out_channels, height, width]`\n        \"\"\"\n        if time_emb is not None:\n            assert (\n                x.shape[0] == time_emb.shape[0]\n            ), f\"batch size does not match: x.shape[0]({x.shape[0]}) != time_emb.shape[0]({time_emb.shape[0]})\"\n            assert (\n                time_emb.shape[1] == self.time_emb_dim\n            ), f\"time_emb_dim does not match: time_emb.shape[1]({time_emb.shape[1]}) != self.time_emb_dim({self.time_emb_dim})\"\n        # h: [batch, out_channels, height, width]\n        h = self.in_layers(x)\n        # [batch, time_emb_dim] => [batch, out_channels] => [batch, out_channels, 1, 1]\n        if time_emb is not None:\n            time_emb = self.time_embedding(time_emb)\n            h += time_emb[:, :, None, None]\n        h = self.out_layers(h)\n        return h + self.skip_connection(x)", ""]}
{"filename": "stable_diffusion/modules/transformer.py", "chunked_list": ["#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\n@File    :   attention.py\n@Time    :   2023/05/14 16:03:43\n@Author  :   Wenbo Li\n@Desc    :   Transformer module for Stable Diffusion U-Net\n\"\"\"\n\nfrom typing import Optional", "\nfrom typing import Optional\nimport torch\nfrom torch import nn\nfrom einops import rearrange\n\nfrom ..models import utils\n\n\nclass CrossAttention(nn.Module):\n    \"\"\"\n    Cross attention module for transformer\n\n    Architecture:\n        - Q, K, V = Linear(x), Linear(context), Linear(context)\n        - sim = Q * K^T / sqrt(dk)\n        - attn = softmax(sim) * V\n        - out = Linear(attn)\n\n    Args:\n        - query_dim (int):\n                query dimension\n        - context_dim (Optional[int]):\n                context dimension, if not previded, equal to query_dim\n        - n_heads (int):\n                num of heads\n        - d_head (int):\n                dim of each head\n        - dropout (float, optional):\n                dropout rate. Default: `0.`.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim: int,\n        context_dim: Optional[int] = None,\n        n_heads: int = 1,\n        d_head: int = 1,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        if not context_dim:\n            context_dim = query_dim\n        d_model = n_heads * d_head\n        self.n_heads = n_heads\n        self.scale = 1 / (d_head**0.5)  # 1 / sqrt(d_k) from paper\n        self.to_q = nn.Linear(query_dim, d_model, bias=False)\n        self.to_k = nn.Linear(context_dim, d_model, bias=False)\n        self.to_v = nn.Linear(context_dim, d_model, bias=False)\n        self.out = nn.Sequential(nn.Linear(d_model, query_dim), nn.Dropout(dropout))\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        context_emb: torch.Tensor = None,\n        mask: torch.Tensor = None,\n    ):\n        \"\"\"\n        Cross attention forward pass\n        - first calculate similarity between query and context embedding: sim = Q * K^T / sqrt(dk)\n        - then calculate attention value: attn = softmax(sim) * V\n        - finally, linear projection and dropout\n\n        Args:\n            - query (torch.Tensor):\n                  feature map of shape `[batch, height*width, query_dim]`\n                  height*width is equivalent to tgt_len in origin transformer\n            - context_emb (torch.Tensor, optional):\n                  conditional embeddings of shape `[batch, seq_len, context_dim]`. Default: `None`.\n                  seq_len is equivalent to src_len in origin transformer\n            - mask (torch.Tensor, optional):\n                  mask of shape = `[batch, height*width, seq_len]`. Default: `None`.\n                  actually never used...\n        \"\"\"\n        # if no context_emb, equal to self-attention\n        # when use cross attn without wrapped spatial transformer, convert to [batch, height*width, d_model] first\n        convert = len(query.shape) == 4\n        if convert:\n            h = query.shape[2]\n            query = rearrange(query, \"b c h w -> b (h w) c\")\n        if context_emb is None:\n            context_emb = query\n        Q, K, V = self.to_q(query), self.to_k(context_emb), self.to_v(context_emb)\n        # q: [batch, h*w, d_model] -> [batch * n_head, h*w, d_head]\n        # k,v: [batch, seq_len, d_model] -> [batch * n_head, seq_len, d_head]\n        Q, K, V = map(\n            lambda t: rearrange(\n                t, \"b n (n_heads d_head) -> (b n_heads) n d_head\", n_heads=self.n_heads\n            ),\n            (Q, K, V),\n        )\n        # similarity = Q*K^T / sqrt(dk): [batch * n_head, h*w, d_head] * [batch * n_head, d_head, seq_len] -> [batch * n_head, h*w, seq_len]\n        sim = torch.einsum(\"b n d,b m d->b n m\", Q, K) * self.scale\n        if mask is not None:\n            # repeat for each head\n            # mask: [batch, height*width, seq_len] -> [batch * n_head, height*width, seq_len]\n            mask = mask.repeat(\"b n m, b h n m\", h=self.n_heads)\n            max_neg_value = -torch.finfo(sim.dtype).max\n            sim.masked_fill_(mask, max_neg_value)\n        # softmax\n        attn = sim.softmax(dim=-1)\n        # attn value = attn*V: [batch * n_head, h*w, seq_len] * [batch * n_head, seq_len, d_head] -> [batch * n_head, h*w, d_head]\n        attn_v = torch.einsum(\"b n m,b m d->b n d\", attn, V)\n        attn_v = rearrange(\n            attn_v, \"(b n_heads) n d_head -> b n (n_heads d_head)\", n_heads=self.n_heads\n        )\n        out = self.out(attn_v)\n        # convert it back to [batch, channels, height, width]\n        if convert:\n            out = rearrange(out, \"b (h w) c -> b c h w\", h=h)\n        return out", "\nclass CrossAttention(nn.Module):\n    \"\"\"\n    Cross attention module for transformer\n\n    Architecture:\n        - Q, K, V = Linear(x), Linear(context), Linear(context)\n        - sim = Q * K^T / sqrt(dk)\n        - attn = softmax(sim) * V\n        - out = Linear(attn)\n\n    Args:\n        - query_dim (int):\n                query dimension\n        - context_dim (Optional[int]):\n                context dimension, if not previded, equal to query_dim\n        - n_heads (int):\n                num of heads\n        - d_head (int):\n                dim of each head\n        - dropout (float, optional):\n                dropout rate. Default: `0.`.\n    \"\"\"\n\n    def __init__(\n        self,\n        query_dim: int,\n        context_dim: Optional[int] = None,\n        n_heads: int = 1,\n        d_head: int = 1,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        if not context_dim:\n            context_dim = query_dim\n        d_model = n_heads * d_head\n        self.n_heads = n_heads\n        self.scale = 1 / (d_head**0.5)  # 1 / sqrt(d_k) from paper\n        self.to_q = nn.Linear(query_dim, d_model, bias=False)\n        self.to_k = nn.Linear(context_dim, d_model, bias=False)\n        self.to_v = nn.Linear(context_dim, d_model, bias=False)\n        self.out = nn.Sequential(nn.Linear(d_model, query_dim), nn.Dropout(dropout))\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        context_emb: torch.Tensor = None,\n        mask: torch.Tensor = None,\n    ):\n        \"\"\"\n        Cross attention forward pass\n        - first calculate similarity between query and context embedding: sim = Q * K^T / sqrt(dk)\n        - then calculate attention value: attn = softmax(sim) * V\n        - finally, linear projection and dropout\n\n        Args:\n            - query (torch.Tensor):\n                  feature map of shape `[batch, height*width, query_dim]`\n                  height*width is equivalent to tgt_len in origin transformer\n            - context_emb (torch.Tensor, optional):\n                  conditional embeddings of shape `[batch, seq_len, context_dim]`. Default: `None`.\n                  seq_len is equivalent to src_len in origin transformer\n            - mask (torch.Tensor, optional):\n                  mask of shape = `[batch, height*width, seq_len]`. Default: `None`.\n                  actually never used...\n        \"\"\"\n        # if no context_emb, equal to self-attention\n        # when use cross attn without wrapped spatial transformer, convert to [batch, height*width, d_model] first\n        convert = len(query.shape) == 4\n        if convert:\n            h = query.shape[2]\n            query = rearrange(query, \"b c h w -> b (h w) c\")\n        if context_emb is None:\n            context_emb = query\n        Q, K, V = self.to_q(query), self.to_k(context_emb), self.to_v(context_emb)\n        # q: [batch, h*w, d_model] -> [batch * n_head, h*w, d_head]\n        # k,v: [batch, seq_len, d_model] -> [batch * n_head, seq_len, d_head]\n        Q, K, V = map(\n            lambda t: rearrange(\n                t, \"b n (n_heads d_head) -> (b n_heads) n d_head\", n_heads=self.n_heads\n            ),\n            (Q, K, V),\n        )\n        # similarity = Q*K^T / sqrt(dk): [batch * n_head, h*w, d_head] * [batch * n_head, d_head, seq_len] -> [batch * n_head, h*w, seq_len]\n        sim = torch.einsum(\"b n d,b m d->b n m\", Q, K) * self.scale\n        if mask is not None:\n            # repeat for each head\n            # mask: [batch, height*width, seq_len] -> [batch * n_head, height*width, seq_len]\n            mask = mask.repeat(\"b n m, b h n m\", h=self.n_heads)\n            max_neg_value = -torch.finfo(sim.dtype).max\n            sim.masked_fill_(mask, max_neg_value)\n        # softmax\n        attn = sim.softmax(dim=-1)\n        # attn value = attn*V: [batch * n_head, h*w, seq_len] * [batch * n_head, seq_len, d_head] -> [batch * n_head, h*w, d_head]\n        attn_v = torch.einsum(\"b n m,b m d->b n d\", attn, V)\n        attn_v = rearrange(\n            attn_v, \"(b n_heads) n d_head -> b n (n_heads d_head)\", n_heads=self.n_heads\n        )\n        out = self.out(attn_v)\n        # convert it back to [batch, channels, height, width]\n        if convert:\n            out = rearrange(out, \"b (h w) c -> b c h w\", h=h)\n        return out", "\n\nclass FeedForward(nn.Module):\n    \"\"\"\n    origin paper use linear-relu-dropout-linear: `FFN(x) = max(0, xW1 + b1)W2 + b2`, equation(2) from Attention is all you need(https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)\n\n    here we use FFN(x) = `Dropout(GEGLU(x))*W + b`, where `GEGLU(x) = (xW + b) * GELU(xV + c)`\n\n    Architecture:\n        - GEGLU(x) = (xW + b) * GELU(xV + c)\n        - Dropout(GEGLU(x))\n        - Linear(Dropout(GEGLU(x)))\n\n    Args:\n        - d_model (int):\n                d_model from transformer paper\n        - dim_mult (int, optional):\n                multiplicative factor for the hidden layer size. Default: `4`.\n        - dropout (float, optional):\n                dropout rate. Default: `0.`.\n    Returns:\n    TODO: x shape and output shape\n        - Tensor:\n                _description_\n    \"\"\"\n\n    def __init__(self, d_model: int, dim_mult: int = 4, dropout: float = 0.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            GEGLU(d_model, dim_mult * d_model),\n            nn.Dropout(p=dropout),\n            nn.Linear(d_model * dim_mult, d_model),\n        )\n\n    def forward(self, x: torch.Tensor):\n        return self.net(x)", "\n\nclass GEGLU(nn.Module):\n    \"\"\"\n    GeGLU(x) = (xW + b) * GELU(xV + c) from paper: https://arxiv.org/abs/2002.05202\n\n    Architecture:\n        - xW + b, xV + c = Linear(x)\n        - out = (xW + b) * GELU(xV + c)\n\n    Args:\n        - in_features (int):\n                input feature dimension\n        - out_features (int):\n                output feature dimension\n    \"\"\"\n\n    def __init__(self, in_features: int, out_features: int):\n        super().__init__()\n        # xW + b and xV + c all together\n        self.proj = nn.Linear(in_features, out_features * 2)\n        self.gelu = nn.GELU()\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        forward pass of GEGLU\n\n        Args:\n            - x (torch.Tensor):\n                  shape=`[]`\n\n        Returns:\n            - torch.Tensor:\n                  shape=`[]`\n        \"\"\"\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * self.gelu(gate)", "\n\nclass BasicTransformerBlock(nn.Module):\n    \"\"\"\n    Basic Transformer Block\n\n    Architecture:\n        - self attention = `CrossAttention(context=None)`\n        - add norm\n        - cross attention = `CrossAttention(context)`\n        -add norm\n        - feed forward = `FeedForward`\n        - add norm\n\n    Args:\n        - d_model (int):\n                dim of embedding\n        - n_heads (int):\n                num of heads\n        - d_head (int):\n                dim of each head\n        - dropout (float, optional):\n                dropout rate. Default: `0.`.\n        - context_dim (int, optional):\n                dim of conditional context. Default: `768`.\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model: int,\n        n_heads: int,\n        d_head: int,\n        dropout: float = 0.0,\n        context_dim: int = 768,\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.context_dim = context_dim\n        self.self_attn = CrossAttention(\n            d_model,\n            context_dim=d_model,\n            n_heads=n_heads,\n            d_head=d_head,\n            dropout=dropout,\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.cross_attn = CrossAttention(\n            d_model,\n            context_dim=context_dim,\n            n_heads=n_heads,\n            d_head=d_head,\n            dropout=dropout,\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n        self.ffn = FeedForward(d_model, dropout=dropout)\n        self.norm3 = nn.LayerNorm(d_model)\n\n    def forward(self, x: torch.Tensor, context_emb: Optional[torch.Tensor] = None):\n        \"\"\"\n        forward pass of BasicTransformerBlock\n\n        Args:\n            - x (torch.Tensor):\n                   input embeddings of shape `[batch_size, height * width, d_model]`\n\n            - context (torch.Tensor, optional):\n                  conditional embeddings of shape `[batch_size,  seq_len, context_dim]`\n\n        Returns:\n            - torch.Tensor:\n                  x with attention and skip connection and normalization, shape=`[batch_size, height * width, d_model]`\n        \"\"\"\n        # check params\n        assert (\n            x.shape[-1] == self.d_model\n        ), f\"input dim {x.shape[-1]} should be equal to d_model {self.d_model}\"\n        if context_emb is not None:\n            assert (\n                context_emb.shape[-1] == self.context_dim\n            ), f\"context dim {context_emb.shape[-1]} should be equal to context_dim {self.context_dim}\"\n        # self attention\n        x = self.norm1(x + self.self_attn(x, context_emb=None))\n        # cross attention\n        x = self.norm2(x + self.cross_attn(x, context_emb=context_emb))\n        # feed forward\n        x = self.norm3(x + self.ffn(x))\n        return x", "\n\nclass SpatialTransformer(nn.Module):\n    \"\"\"\n    Transformer block for image-like data.\n\n    Architecture:\n        - norm = GroupNorm\n        - proj_in = Conv2d\n        - transformer_blocks = [BasicTransformerBlock] * n_layers\n        - proj_out = Conv2d\n\n    Args:\n        - in_channels (int):\n            input num of channels in the feature map\n        - n_heads (int):\n            num of attention heads\n        - d_head (int):\n            dim of each head\n        - n_layer (int, optional):\n            num of transformer block. Default: `1`.\n        - dropout (float, optional):\n            dropout rate. Default: `0.`.\n        - context_dim (int, optional):\n            dim of context condition. Default: `None`.\n        - groups (int, optional):\n            num of groups for GroupNorm. Default: `2`.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        n_heads: int,\n        d_head: int,\n        n_layers: int = 1,\n        dropout: float = 0.0,\n        context_dim: int = None,\n        groups: int = 2,\n    ):\n        super().__init__()\n        # check params\n        assert (\n            n_heads > 0\n        ), f\"n_heads({n_heads}) should be greater than 0 for SpatialTransformer\"\n        assert (\n            in_channels % groups == 0\n        ), f\"in_channels({in_channels}) should be divisible by num_groups({groups}) for GroupNorm\"\n        self.in_channels = in_channels\n        self.context_dim = context_dim\n        self.norm = nn.GroupNorm(groups, in_channels)\n        self.proj_in = nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)\n        # Transformer layers\n        # @ note: origin openai code use inner_dim = n_heads * d_head, but if legacy, d_head = in_channels // n_heads\n        # => here we use in_channels for simiplicity\n        self.transformer_blocks = nn.ModuleList(\n            [\n                BasicTransformerBlock(\n                    in_channels,\n                    n_heads,\n                    d_head,\n                    dropout=dropout,\n                    context_dim=context_dim,\n                )\n                for _ in range(n_layers)\n            ]\n        )\n        self.proj_out = utils.zero_module(\n            nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0)\n        )\n\n    def forward(\n        self, x: torch.Tensor, context_emb: torch.Tensor = None\n    ) -> torch.Tensor:\n        \"\"\"\n        forward pass\n\n        Args:\n            - x (torch.Tensor):\n                  feature map of shape `[batch_size, channels, height, width]`\n            - context_emb (torch.Tensor, optional):\n                  conditional embeddings of shape `[batch_size,  seq_len, context_dim]`. Default: `None`.\n\n        Returns:\n            - torch.Tensor:\n                  shape=`[batch_size, channels, height, width]`\n        \"\"\"\n        # check params\n        assert (\n            x.shape[1] == self.in_channels\n        ), f\"input channels {x.shape[1]} should be equal to in_channels {self.in_channels}\"\n        if context_emb is not None:\n            assert (\n                context_emb.shape[-1] == self.context_dim\n            ), f\"context dim {context_emb.shape[-1]} should be equal to context_dim {self.context_dim}\"\n        # use for skip connection\n        x_in = x\n        x = self.norm(x)\n        x = self.proj_in(x)\n        x = rearrange(x, \"b c h w -> b (h w) c\")\n        for module in self.transformer_blocks:\n            x = module(x, context_emb=context_emb)\n        x = rearrange(x, \"b (h w) c -> b c h w\", h=x_in.shape[2])\n        x = self.proj_out(x)\n        return x + x_in", "\n\nif __name__ == \"__main__\":\n    x = torch.randn(2, 128, 32, 32)\n    context = torch.randn(2, 10, 768)\n    model = SpatialTransformer(128, 4, 32, n_layers=2, context_dim=768)\n    y = model(x, context)\n    print(y.shape)\n", ""]}
{"filename": "stable_diffusion/modules/__init__.py", "chunked_list": [""]}
{"filename": "stable_diffusion/modules/distributions.py", "chunked_list": ["import torch\nfrom torch import nn\n\n\nclass GaussianDistribution(nn.Module):\n    def __init__(self, moments: torch.Tensor):\n        super(GaussianDistribution, self).__init__\n        self.mean, self.log_var = torch.chunk(moments, 2, dim=1)\n\n    def sample(self):\n        std = torch.exp(0.5 * self.log_var)\n        eps = torch.randn_like(std)\n        return self.mean + eps * std\n\n    def kl(self):\n        self.var = torch.exp(self.log_var)\n        return 0.5 * torch.sum(\n            torch.pow(self.mean, 2) + self.var - 1.0 - self.log_var, dim=[1, 2, 3]\n        )", ""]}
