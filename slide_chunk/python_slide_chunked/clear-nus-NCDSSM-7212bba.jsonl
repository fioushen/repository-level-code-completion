{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\ncore_requires = [\n    \"torch~=1.11\",\n    \"tensorboardX\",\n    \"torchdiffeq @ git+https://github.com/rtqichen/torchdiffeq\",\n    \"tqdm\",\n    \"pyyaml\",\n]\nexp_requires = [", "]\nexp_requires = [\n    \"matplotlib\",\n    \"seaborn\",\n    \"torchvision~=0.14\",\n    \"scikit-learn\",\n    \"pygame\",\n    \"pymunk~=5.6.0\",\n    \"POT~=0.9.0\"\n]", "    \"POT~=0.9.0\"\n]\n\nsetup(\n    name=\"ncdssm\",\n    description=\"Neural Continuous Discrete State Space Models\",\n    long_description='Neural Continuous Discrete State Space Models presented in the IMCL 2023 paper titled \"Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series\"',  # noqa\n    version=\"0.0.1\",\n    install_requires=core_requires,\n    extras_require={", "    install_requires=core_requires,\n    extras_require={\n        \"exp\": exp_requires,\n    },\n    packages=find_packages(where=\"src\"),\n    package_dir={\"\": \"src\"},\n    python_requires=\">=3.8\",\n)\n", ""]}
{"filename": "train_pymunk.py", "chunked_list": ["import os\nimport copy\nimport yaml\nimport torch\nimport argparse\nimport matplotlib\nimport numpy as np\n\nfrom tensorboardX import SummaryWriter\n", "from tensorboardX import SummaryWriter\n\nfrom ncdssm.torch_utils import grad_norm, torch2numpy\nfrom ncdssm.plotting import show_pymunk_forecast, show_wasserstein_distance\nfrom ncdssm.evaluation import evaluate_pymunk_dataset\nimport experiments.utils\nfrom experiments.setups import get_model, get_dataset\n\n\ndef train_step(train_batch, model, optimizer, reg_scheduler, step, device, config):\n    batch_target = train_batch[\"past_target\"]\n    batch_times = train_batch[\"past_times\"]\n    batch_mask = train_batch[\"past_mask\"]\n    batch_target = batch_target.to(device)\n    batch_times = batch_times.to(device)\n    batch_mask = batch_mask.to(device)\n    optimizer.zero_grad()\n    out = model(\n        batch_target,\n        batch_mask,\n        batch_times,\n        num_samples=config.get(\"num_samples\", 1),\n    )\n    cond_ll = out[\"likelihood\"]\n    reg = out[\"regularizer\"]\n    loss = -(cond_ll + reg_scheduler.val * reg).mean(0)\n    loss.backward()\n\n    if step <= config.get(\"ssm_params_warmup_steps\", 0):\n        ctkf_lr = optimizer.param_groups[0][\"lr\"]\n        optimizer.param_groups[0][\"lr\"] = 0\n    total_grad_norm = grad_norm(model.parameters())\n    if float(config[\"max_grad_norm\"]) != float(\"inf\"):\n        torch.nn.utils.clip_grad_norm_(\n            model.parameters(), max_norm=config[\"max_grad_norm\"]\n        )\n    optimizer.step()\n    if step <= config.get(\"ssm_params_warmup_steps\", 0):\n        optimizer.param_groups[0][\"lr\"] = ctkf_lr\n    print(\n        f\"Step {step}: Loss={loss.item():.4f}, Grad Norm: {total_grad_norm.item():.2f},\"\n        f\" Reg-Coeff: {reg_scheduler.val:.2f}\"\n    )\n    return dict(\n        loss=loss.item(), cond_ll=cond_ll.mean(0).item(), reg=reg.mean(0).item()\n    )", "\ndef train_step(train_batch, model, optimizer, reg_scheduler, step, device, config):\n    batch_target = train_batch[\"past_target\"]\n    batch_times = train_batch[\"past_times\"]\n    batch_mask = train_batch[\"past_mask\"]\n    batch_target = batch_target.to(device)\n    batch_times = batch_times.to(device)\n    batch_mask = batch_mask.to(device)\n    optimizer.zero_grad()\n    out = model(\n        batch_target,\n        batch_mask,\n        batch_times,\n        num_samples=config.get(\"num_samples\", 1),\n    )\n    cond_ll = out[\"likelihood\"]\n    reg = out[\"regularizer\"]\n    loss = -(cond_ll + reg_scheduler.val * reg).mean(0)\n    loss.backward()\n\n    if step <= config.get(\"ssm_params_warmup_steps\", 0):\n        ctkf_lr = optimizer.param_groups[0][\"lr\"]\n        optimizer.param_groups[0][\"lr\"] = 0\n    total_grad_norm = grad_norm(model.parameters())\n    if float(config[\"max_grad_norm\"]) != float(\"inf\"):\n        torch.nn.utils.clip_grad_norm_(\n            model.parameters(), max_norm=config[\"max_grad_norm\"]\n        )\n    optimizer.step()\n    if step <= config.get(\"ssm_params_warmup_steps\", 0):\n        optimizer.param_groups[0][\"lr\"] = ctkf_lr\n    print(\n        f\"Step {step}: Loss={loss.item():.4f}, Grad Norm: {total_grad_norm.item():.2f},\"\n        f\" Reg-Coeff: {reg_scheduler.val:.2f}\"\n    )\n    return dict(\n        loss=loss.item(), cond_ll=cond_ll.mean(0).item(), reg=reg.mean(0).item()\n    )", "\n\ndef main():\n    matplotlib.use(\"Agg\")\n    # SET SEED\n    # seed = 111\n    # print(seed)\n    # np.random.seed(seed)\n    # torch.manual_seed(seed)\n    # random.seed(seed)\n\n    # COMMAND-LINE ARGS\n    parser = argparse.ArgumentParser()\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--config\", type=str, help=\"Path to config file.\")\n    group.add_argument(\"--ckpt\", type=str, help=\"Path to checkpoint file.\")\n\n    args, _ = parser.parse_known_args()\n    # CONFIG\n    if args.ckpt:\n        ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n        config = ckpt[\"config\"]\n    else:\n        config = experiments.utils.get_config_and_setup_dirs(args.config)\n        parser = experiments.utils.add_config_to_argparser(config=config, parser=parser)\n        args = parser.parse_args()\n        # Update config from command line args, if any.\n        updated_config_dict = vars(args)\n        for k in config.keys() & updated_config_dict.keys():\n            o_v = config[k]\n            u_v = updated_config_dict[k]\n            if u_v != o_v:\n                print(f\"{k}: {o_v} -> {u_v}\")\n        config.update(updated_config_dict)\n    # DATA\n    train_dataset, val_dataset, _ = get_dataset(config)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config[\"train_batch_size\"],\n        num_workers=4,\n        shuffle=True,\n        collate_fn=train_dataset.collate_fn,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=config[\"test_batch_size\"],\n        collate_fn=train_dataset.collate_fn,\n    )\n    train_gen = iter(train_loader)\n    # test_gen = iter(test_loader)\n\n    # MODEL\n    device = torch.device(config[\"device\"])\n    model = get_model(config=config)\n\n    kf_param_names = {\n        name for name, _ in model.named_parameters() if \"base_ssm\" in name\n    }\n    kf_params = [\n        param for name, param in model.named_parameters() if name in kf_param_names\n    ]\n    non_kf_params = [\n        param for name, param in model.named_parameters() if name not in kf_param_names\n    ]\n    print(kf_param_names)\n    optim = torch.optim.Adam(\n        params=[\n            {\"params\": kf_params},\n            {\"params\": non_kf_params},\n        ],\n        lr=config[\"learning_rate\"],\n    )\n    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n        optim, gamma=config[\"lr_decay_rate\"]\n    )\n    reg_scheduler = experiments.utils.LinearScheduler(\n        iters=config.get(\"reg_anneal_iters\", 0),\n        maxval=config.get(\"reg_coeff_maxval\", 1.0),\n    )\n    start_step = 1\n    if args.ckpt:\n        model.load_state_dict(ckpt[\"model\"])\n        optim.load_state_dict(ckpt[\"optimizer\"])\n        # Hack to move optim states from CPU to GPU.\n        for state in optim.state.values():\n            for k, v in state.items():\n                if torch.is_tensor(v):\n                    state[k] = v.to(device)\n        lr_scheduler.load_state_dict(ckpt[\"scheduler\"])\n        start_step = ckpt[\"step\"] + 1\n    model = model.to(device)\n    num_params = 0\n    for name, param in model.named_parameters():\n        num_params += np.prod(param.size())\n        print(name, param.size())\n    print(f\"Total Paramaters: {num_params.item()}\")\n\n    # TRAIN & EVALUATE\n    num_steps = config[\"num_steps\"]\n    log_steps = config[\"log_steps\"]\n    save_steps = config[\"save_steps\"]\n    log_dir = config[\"log_dir\"]\n    writer = SummaryWriter(logdir=log_dir)\n    with open(os.path.join(log_dir, \"config.yaml\"), \"w\") as fp:\n        yaml.dump(config, fp, default_flow_style=False, sort_keys=False)\n    for step in range(start_step, num_steps + 1):\n        try:\n            train_batch = next(train_gen)\n        except StopIteration:\n            train_gen = iter(train_loader)\n            train_batch = next(train_gen)\n        train_result = train_step(\n            train_batch, model, optim, reg_scheduler, step, device, config\n        )\n        summary_items = copy.deepcopy(train_result)\n        if step % config[\"lr_decay_steps\"] == 0:\n            lr_scheduler.step()\n        if step % config.get(\"reg_anneal_every\", 1) == 0:\n            reg_scheduler.step()\n        if step % save_steps == 0 or step == num_steps:\n            model_path = os.path.join(config[\"ckpt_dir\"], f\"model_{step}.pt\")\n            torch.save(\n                {\n                    \"step\": step,\n                    \"model\": model.state_dict(),\n                    \"optimizer\": optim.state_dict(),\n                    \"scheduler\": lr_scheduler.state_dict(),\n                    \"config\": config,\n                },\n                model_path,\n            )\n\n        if step % log_steps == 0 or step == num_steps:\n            folder = os.path.join(log_dir, \"plots\", f\"step{step}\")\n            os.makedirs(folder, exist_ok=True)\n            (\n                wt_mean,\n                wt_conf_interval,\n                future_w_mean,\n                future_w_conf_interval,\n            ) = evaluate_pymunk_dataset(\n                val_loader,\n                model,\n                device=device,\n                num_samples=config[\"num_forecast\"],\n                max_size=100,\n            ).values()\n            writer.add_scalar(\"future_w_mean\", future_w_mean.item(), global_step=step)\n            writer.add_scalar(\n                \"future_w_conf_interval\",\n                future_w_conf_interval.item(),\n                global_step=step,\n            )\n            fig = show_wasserstein_distance(\n                (15, 2),\n                wt_mean,\n                conf_intervals=wt_conf_interval,\n                fig_title=\"Wasserstein Distance\",\n            )\n            writer.add_figure(\"w_dist\", fig, global_step=step)\n            plot_count = 0\n            for test_batch in val_loader:\n                past_target = test_batch[\"past_target\"].to(device)\n                B, T, _ = past_target.shape\n                mask = test_batch[\"past_mask\"].to(device)\n                future_target = test_batch[\"future_target\"].to(device)\n                past_times = test_batch[\"past_times\"].to(device)\n                future_times = test_batch[\"future_times\"].to(device)\n                predict_result = model.forecast(\n                    past_target,\n                    mask,\n                    past_times.view(-1),\n                    future_times.view(-1),\n                    num_samples=config[\"num_forecast\"],\n                )\n                reconstruction = predict_result[\"reconstruction\"]\n                forecast = predict_result[\"forecast\"]\n                full_prediction = torch.cat([reconstruction, forecast], dim=-2)\n                full_target = torch.cat([past_target, future_target], dim=-2)\n                for j in range(B):\n                    full_prediction_j = full_prediction[:, j].view(\n                        full_prediction.shape[0],\n                        full_prediction.shape[-2],\n                        1,\n                        config[\"img_size\"],\n                        config[\"img_size\"],\n                    )\n                    full_target_j = full_target[j].view(\n                        full_target.shape[1], 1, config[\"img_size\"], config[\"img_size\"]\n                    )\n                    full_target_j[:T][mask[j] == 0.0] = 0.0\n                    # Plot first five samples\n                    show_pymunk_forecast(\n                        torch2numpy(full_target_j),\n                        torch2numpy(full_prediction_j[:5]),\n                        os.path.join(folder, f\"series_{plot_count}.png\"),\n                    )\n                    plot_count += 1\n\n                    if plot_count == config[\"num_plots\"]:\n                        break\n                if plot_count == config[\"num_plots\"]:\n                    break\n        for k, v in summary_items.items():\n            writer.add_scalar(k, v, global_step=step)\n        writer.flush()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "train_ts.py", "chunked_list": ["import os\nimport copy\nimport yaml\nimport torch\nimport argparse\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom tensorboardX import SummaryWriter", "\nfrom tensorboardX import SummaryWriter\n\nfrom ncdssm.torch_utils import grad_norm, prepend_time_zero, torch2numpy\nfrom ncdssm.plotting import show_time_series_forecast\nfrom ncdssm.evaluation import evaluate_simple_ts, evaluate_sporadic\nimport experiments.utils\nfrom experiments.setups import get_model, get_dataset\n\n\ndef train_step(train_batch, model, optimizer, reg_scheduler, step, device, config):\n    batch_target = train_batch[\"past_target\"].to(device)\n    batch_times = train_batch[\"past_times\"].to(device)\n    batch_mask = train_batch[\"past_mask\"].to(device)\n    optimizer.zero_grad()\n    out = model(\n        batch_target,\n        batch_mask,\n        batch_times,\n        num_samples=config.get(\"num_samples\", 1),\n    )\n    cond_ll = out[\"likelihood\"]\n    reg = out[\"regularizer\"]\n    loss = -(cond_ll + reg_scheduler.val * reg).mean(0)\n    loss.backward()\n    if step <= config.get(\"ssm_params_warmup_steps\", 0):\n        ctkf_lr = optimizer.param_groups[0][\"lr\"]\n        optimizer.param_groups[0][\"lr\"] = 0\n    total_grad_norm = grad_norm(model.parameters())\n    if total_grad_norm < float(\"inf\"):\n        if config[\"max_grad_norm\"] != float(\"inf\"):\n            torch.nn.utils.clip_grad_norm_(\n                model.parameters(), max_norm=config[\"max_grad_norm\"]\n            )\n        optimizer.step()\n    else:\n        print(\"Skipped gradient update!\")\n        optimizer.zero_grad()\n    if step <= config.get(\"ssm_params_warmup_steps\", 0):\n        optimizer.param_groups[0][\"lr\"] = ctkf_lr\n    print(\n        f\"Step {step}: Loss={loss.item():.4f},\"\n        f\" Grad Norm: {total_grad_norm.item():.2f},\"\n        f\" Reg-Coeff: {reg_scheduler.val:.2f}\"\n    )\n    return dict(\n        loss=loss.item(), cond_ll=cond_ll.mean(0).item(), reg=reg.mean(0).item()\n    )", "\n\ndef train_step(train_batch, model, optimizer, reg_scheduler, step, device, config):\n    batch_target = train_batch[\"past_target\"].to(device)\n    batch_times = train_batch[\"past_times\"].to(device)\n    batch_mask = train_batch[\"past_mask\"].to(device)\n    optimizer.zero_grad()\n    out = model(\n        batch_target,\n        batch_mask,\n        batch_times,\n        num_samples=config.get(\"num_samples\", 1),\n    )\n    cond_ll = out[\"likelihood\"]\n    reg = out[\"regularizer\"]\n    loss = -(cond_ll + reg_scheduler.val * reg).mean(0)\n    loss.backward()\n    if step <= config.get(\"ssm_params_warmup_steps\", 0):\n        ctkf_lr = optimizer.param_groups[0][\"lr\"]\n        optimizer.param_groups[0][\"lr\"] = 0\n    total_grad_norm = grad_norm(model.parameters())\n    if total_grad_norm < float(\"inf\"):\n        if config[\"max_grad_norm\"] != float(\"inf\"):\n            torch.nn.utils.clip_grad_norm_(\n                model.parameters(), max_norm=config[\"max_grad_norm\"]\n            )\n        optimizer.step()\n    else:\n        print(\"Skipped gradient update!\")\n        optimizer.zero_grad()\n    if step <= config.get(\"ssm_params_warmup_steps\", 0):\n        optimizer.param_groups[0][\"lr\"] = ctkf_lr\n    print(\n        f\"Step {step}: Loss={loss.item():.4f},\"\n        f\" Grad Norm: {total_grad_norm.item():.2f},\"\n        f\" Reg-Coeff: {reg_scheduler.val:.2f}\"\n    )\n    return dict(\n        loss=loss.item(), cond_ll=cond_ll.mean(0).item(), reg=reg.mean(0).item()\n    )", "\n\ndef main():\n    matplotlib.use(\"Agg\")\n\n    # COMMAND-LINE ARGS\n    parser = argparse.ArgumentParser()\n    group = parser.add_mutually_exclusive_group(required=True)\n    group.add_argument(\"--config\", type=str, help=\"Path to config file.\")\n    group.add_argument(\"--ckpt\", type=str, help=\"Path to checkpoint file.\")\n    parser.add_argument(\n        \"--sporadic\",\n        action=\"store_true\",\n        help=\"Whether sporadic dataset (e.g., climate) is used.\",\n    )\n\n    args, _ = parser.parse_known_args()\n    # CONFIG\n    if args.ckpt:\n        ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n        config = ckpt[\"config\"]\n    else:\n        config = experiments.utils.get_config_and_setup_dirs(args.config)\n        parser = experiments.utils.add_config_to_argparser(config=config, parser=parser)\n        args = parser.parse_args()\n        # Update config from command line args, if any.\n        updated_config_dict = vars(args)\n        for k in config.keys() & updated_config_dict.keys():\n            o_v = config[k]\n            u_v = updated_config_dict[k]\n            if u_v != o_v:\n                print(f\"{k}: {o_v} -> {u_v}\")\n        config.update(updated_config_dict)\n\n    if args.sporadic:\n        evaluate_fn = evaluate_sporadic\n    else:\n        evaluate_fn = evaluate_simple_ts\n\n    # DATA\n    train_dataset, val_dataset, _ = get_dataset(config)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config[\"train_batch_size\"],\n        num_workers=4,  # NOTE: 0 may be faster for climate dataset\n        shuffle=True,\n        collate_fn=train_dataset.collate_fn,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=config[\"test_batch_size\"],\n        collate_fn=train_dataset.collate_fn,\n    )\n    train_gen = iter(train_loader)\n    # test_gen = iter(test_loader)\n\n    # MODEL\n    device = torch.device(config[\"device\"])\n    model = get_model(config=config)\n    kf_param_names = {\n        name for name, _ in model.named_parameters() if \"base_ssm\" in name\n    }\n    kf_params = [\n        param for name, param in model.named_parameters() if name in kf_param_names\n    ]\n    non_kf_params = [\n        param for name, param in model.named_parameters() if name not in kf_param_names\n    ]\n    print(kf_param_names)\n    optim = torch.optim.Adam(\n        params=[\n            {\"params\": kf_params},\n            {\"params\": non_kf_params},\n        ],\n        lr=config[\"learning_rate\"],\n        weight_decay=config.get(\"weight_decay\", 0.0),\n    )\n    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n        optim, gamma=config[\"lr_decay_rate\"]\n    )\n    reg_scheduler = experiments.utils.LinearScheduler(\n        iters=config.get(\"reg_anneal_iters\", 0),\n        maxval=config.get(\"reg_coeff_maxval\", 1.0),\n    )\n    start_step = 1\n    if args.ckpt:\n        model.load_state_dict(ckpt[\"model\"])\n        optim.load_state_dict(ckpt[\"optimizer\"])\n        # Hack to move optim states from CPU to GPU.\n        for state in optim.state.values():\n            for k, v in state.items():\n                if torch.is_tensor(v):\n                    state[k] = v.to(device)\n        lr_scheduler.load_state_dict(ckpt[\"scheduler\"])\n        start_step = ckpt[\"step\"] + 1\n    model = model.to(device)\n    num_params = 0\n    for name, param in model.named_parameters():\n        num_params += np.prod(param.size())\n        print(name, param.size())\n    print(f\"Total Paramaters: {num_params.item()}\")\n    # TRAIN & EVALUATE\n    num_steps = config[\"num_steps\"]\n    log_steps = config[\"log_steps\"]\n    save_steps = config[\"save_steps\"]\n    log_dir = config[\"log_dir\"]\n    writer = SummaryWriter(logdir=log_dir)\n    with open(os.path.join(log_dir, \"config.yaml\"), \"w\") as fp:\n        yaml.dump(config, fp, default_flow_style=False, sort_keys=False)\n    for step in range(start_step, num_steps + 1):\n        try:\n            train_batch = next(train_gen)\n        except StopIteration:\n            train_gen = iter(train_loader)\n            train_batch = next(train_gen)\n        train_result = train_step(\n            train_batch, model, optim, reg_scheduler, step, device, config\n        )\n        summary_items = copy.deepcopy(train_result)\n        if step % config[\"lr_decay_steps\"] == 0:\n            lr_scheduler.step()\n        if step % config.get(\"reg_anneal_every\", 1) == 0:\n            reg_scheduler.step()\n        if step % save_steps == 0 or step == num_steps:\n            model_path = os.path.join(config[\"ckpt_dir\"], f\"model_{step}.pt\")\n            torch.save(\n                {\n                    \"step\": step,\n                    \"model\": model.state_dict(),\n                    \"optimizer\": optim.state_dict(),\n                    \"scheduler\": lr_scheduler.state_dict(),\n                    \"config\": config,\n                },\n                model_path,\n            )\n\n        if step % log_steps == 0 or step == num_steps:\n            metrics = evaluate_fn(\n                val_loader, model, device, num_samples=config[\"num_forecast\"]\n            )\n            for m, v in metrics.items():\n                writer.add_scalar(m, v, global_step=step)\n            folder = os.path.join(log_dir, \"plots\", f\"step{step}\")\n            os.makedirs(folder, exist_ok=True)\n            plot_count = 0\n            while plot_count < config[\"num_plots\"]:\n                for test_batch in val_loader:\n                    past_target = test_batch[\"past_target\"].to(device)\n                    B, T, D = past_target.shape\n                    mask = test_batch[\"past_mask\"].to(device)\n                    future_target = test_batch[\"future_target\"].to(device)\n                    past_times = test_batch[\"past_times\"].to(device)\n                    future_times = test_batch[\"future_times\"].to(device)\n                    if past_times[0] > 0:\n                        past_times, past_target, mask = prepend_time_zero(\n                            past_times, past_target, mask\n                        )\n                    predict_result = model.forecast(\n                        past_target,\n                        mask,\n                        past_times.view(-1),\n                        future_times.view(-1),\n                        num_samples=config[\"num_forecast\"],\n                    )\n                    reconstruction = predict_result[\"reconstruction\"]\n                    forecast = predict_result[\"forecast\"]\n                    for j in range(B):\n                        masked_past_target = past_target.clone()\n                        masked_past_target[mask == 0.0] = float(\"nan\")\n                        fig = show_time_series_forecast(\n                            (12, 5),\n                            torch2numpy(past_times),\n                            torch2numpy(future_times),\n                            torch2numpy(torch.cat([past_target, future_target], 1))[j],\n                            torch2numpy(\n                                torch.cat([masked_past_target, future_target], 1)\n                            )[j],\n                            torch2numpy(reconstruction)[:, j],\n                            torch2numpy(forecast)[:, j],\n                            file_path=os.path.join(folder, f\"series_{plot_count}.png\"),\n                        )\n                        plt.close(fig)\n                        plot_count += 1\n\n                        if plot_count >= config[\"num_plots\"]:\n                            break\n                    if plot_count >= config[\"num_plots\"]:\n                        break\n        for k, v in summary_items.items():\n            writer.add_scalar(k, v, global_step=step)\n        writer.flush()", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "eval_pymunk.py", "chunked_list": ["import os\nimport yaml\nimport torch\nimport argparse\nimport matplotlib\nimport numpy as np\n\n\nfrom ncdssm.torch_utils import torch2numpy\nfrom ncdssm.plotting import (", "from ncdssm.torch_utils import torch2numpy\nfrom ncdssm.plotting import (\n    show_latents,\n    show_pymunk_forecast,\n    show_wasserstein_distance,\n)\nfrom ncdssm.evaluation import evaluate_pymunk_dataset\nfrom experiments.setups import get_model, get_dataset\n\n\ndef main():\n    matplotlib.use(\"Agg\")\n\n    # COMMAND-LINE ARGS\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--ckpt\", required=True, type=str, help=\"Path to checkpoint file.\"\n    )\n\n    parser.add_argument(\"--seed\", type=int, help=\"Random seed.\")\n\n    parser.add_argument(\n        \"--wass\",\n        action=\"store_true\",\n        help=\"Whether to compute the Wasserstein distance.\",\n    )\n    parser.add_argument(\"--device\", type=str, help=\"Device to eval on\")\n    parser.add_argument(\n        \"--max_size\",\n        type=int,\n        default=np.inf,\n        help=\"Maximum number of time series to evaluate on. Only for debugging.\",\n    )\n    parser.add_argument(\n        \"--no_state_sampling\",\n        action=\"store_true\",\n        help=\"Use only the means of the predicted state distributions without sampling\",\n    )\n    parser.add_argument(\n        \"--smooth\",\n        action=\"store_true\",\n        help=\"Use smoothing for imputation\",\n    )\n    parser.add_argument(\n        \"--num_plots\", type=int, default=0, help=\"The number of plots to save\"\n    )\n\n    args, _ = parser.parse_known_args()\n    if args.seed is not None:\n        torch.manual_seed(args.seed)\n        np.random.seed(args.seed)\n    # CONFIG\n    ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n    config = ckpt[\"config\"]\n    config[\"device\"] = args.device or config[\"device\"]\n    # DATA\n    train_dataset, _, test_dataset = get_dataset(config)\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=config[\"test_batch_size\"],\n        collate_fn=train_dataset.collate_fn,\n    )\n\n    # MODEL\n    device = torch.device(config[\"device\"])\n    model = get_model(config=config)\n    model.load_state_dict(ckpt[\"model\"])\n    step = ckpt[\"step\"]\n    model = model.to(device)\n    num_params = 0\n    for name, param in model.named_parameters():\n        num_params += np.prod(param.size())\n        print(name, param.size())\n    print(f\"Total Paramaters: {num_params.item()}\")\n\n    # REEVALUATE\n    log_dir = config[\"log_dir\"]\n    folder = os.path.join(log_dir, \"test_plots\", f\"step{step}\")\n    os.makedirs(folder, exist_ok=True)\n\n    results = {\"config\": config}\n    save_dict = dict(predictions=[])\n\n    if args.wass:\n        (\n            wt_mean,\n            wt_conf_interval,\n            future_w_mean,\n            future_w_conf_interval,\n        ) = evaluate_pymunk_dataset(\n            test_loader,\n            model,\n            device=device,\n            num_samples=config[\"num_forecast\"],\n            max_size=args.max_size,\n            no_state_sampling=args.no_state_sampling,\n            use_smooth=args.smooth,\n        ).values()\n        save_dict[\"wass_dist\"] = wt_mean\n\n        show_wasserstein_distance(\n            (15, 2),\n            wt_mean,\n            conf_intervals=wt_conf_interval,\n            fig_title=\"Wasserstein Distance\",\n            file_path=os.path.join(folder, \"wass.png\"),\n        )\n        print(\n            f\"Forecast W: {future_w_mean.item():.3f} \"\n            f\"+/- {future_w_conf_interval.item():.3f}\"\n        )\n        results[\"future_w_mean\"] = future_w_mean.item()\n        results[\"future_w_conf_interval\"] = future_w_conf_interval.item()\n\n    plot_count = 0\n    while plot_count < args.num_plots:\n        for test_batch in test_loader:\n            past_target = test_batch[\"past_target\"].to(device)\n            B, T, _ = past_target.shape\n            mask = test_batch[\"past_mask\"].to(device)\n            future_target = test_batch[\"future_target\"].to(device)\n            past_times = test_batch[\"past_times\"].to(device)\n            future_times = test_batch[\"future_times\"].to(device)\n            predict_result = model.forecast(\n                past_target,\n                mask,\n                past_times.view(-1),\n                future_times.view(-1),\n                num_samples=config[\"num_forecast\"],\n                no_state_sampling=args.no_state_sampling,\n                use_smooth=args.smooth,\n            )\n            reconstruction = predict_result[\"reconstruction\"]\n            forecast = predict_result[\"forecast\"]\n            full_times = torch.cat([past_times, future_times], 0)\n            full_prediction = torch.cat([reconstruction, forecast], dim=-2)\n            full_target = torch.cat([past_target, future_target], dim=-2)\n            latent_variables = dict()\n            if \"z_reconstruction\" in predict_result:\n                full_z = torch.cat(\n                    [predict_result[\"z_reconstruction\"], predict_result[\"z_forecast\"]],\n                    dim=-2,\n                )\n                latent_variables[\"z\"] = full_z\n            if \"alpha_reconstruction\" in predict_result:\n                full_alpha = torch.cat(\n                    [\n                        predict_result[\"alpha_reconstruction\"],\n                        predict_result[\"alpha_forecast\"],\n                    ],\n                    dim=-2,\n                )\n                latent_variables[\"alpha\"] = full_alpha\n            if \"aux_reconstruction\" in predict_result:\n                full_aux = torch.cat(\n                    [\n                        predict_result[\"aux_reconstruction\"],\n                        predict_result[\"aux_forecast\"],\n                    ],\n                    dim=-2,\n                )\n                latent_variables[\"aux\"] = full_aux\n            for j in range(B):\n                full_prediction_j = full_prediction[:, j].view(\n                    full_prediction.shape[0],\n                    full_prediction.shape[-2],\n                    1,\n                    config[\"img_size\"],\n                    config[\"img_size\"],\n                )\n                full_target_j = full_target[j].view(\n                    full_target.shape[1], 1, config[\"img_size\"], config[\"img_size\"]\n                )\n                # Plot first five samples\n                samples_dir = os.path.join(folder, f\"series_{j}\")\n                os.makedirs(samples_dir, exist_ok=True)\n                full_target_j[:T][mask[j] == 0.0] = 0.0\n                save_dict[\"predictions\"].append(\n                    dict(\n                        target=torch2numpy(full_target_j),\n                        pred=torch2numpy(full_prediction_j[0]),\n                    )\n                )\n                show_pymunk_forecast(\n                    torch2numpy(full_target_j),\n                    torch2numpy(full_prediction_j[:5]),\n                    os.path.join(samples_dir, \"prediction.png\"),\n                )\n                if len(latent_variables) > 0:\n                    latent_variables_j = {\n                        k: torch2numpy(v[:, j]) for k, v in latent_variables.items()\n                    }\n                    for m in range(5):\n                        latent_variables_jm = {\n                            k: v[m] for k, v in latent_variables_j.items()\n                        }\n                        plot_path = os.path.join(samples_dir, f\"lat_{m}.png\")\n                        show_latents(\n                            (15, 8),\n                            time=torch2numpy(full_times),\n                            latents=latent_variables_jm,\n                            fig_title=\"Latents\",\n                            file_path=plot_path,\n                        )\n                plot_count += 1\n\n                if plot_count == args.num_plots:\n                    break\n            if plot_count == args.num_plots:\n                break\n\n    with open(os.path.join(log_dir, \"metrics.yaml\"), \"w\") as fp:\n        yaml.dump(results, fp, default_flow_style=False)\n\n    np.savez(os.path.join(log_dir, \"plot_data.npz\"), **save_dict)", "\n\ndef main():\n    matplotlib.use(\"Agg\")\n\n    # COMMAND-LINE ARGS\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--ckpt\", required=True, type=str, help=\"Path to checkpoint file.\"\n    )\n\n    parser.add_argument(\"--seed\", type=int, help=\"Random seed.\")\n\n    parser.add_argument(\n        \"--wass\",\n        action=\"store_true\",\n        help=\"Whether to compute the Wasserstein distance.\",\n    )\n    parser.add_argument(\"--device\", type=str, help=\"Device to eval on\")\n    parser.add_argument(\n        \"--max_size\",\n        type=int,\n        default=np.inf,\n        help=\"Maximum number of time series to evaluate on. Only for debugging.\",\n    )\n    parser.add_argument(\n        \"--no_state_sampling\",\n        action=\"store_true\",\n        help=\"Use only the means of the predicted state distributions without sampling\",\n    )\n    parser.add_argument(\n        \"--smooth\",\n        action=\"store_true\",\n        help=\"Use smoothing for imputation\",\n    )\n    parser.add_argument(\n        \"--num_plots\", type=int, default=0, help=\"The number of plots to save\"\n    )\n\n    args, _ = parser.parse_known_args()\n    if args.seed is not None:\n        torch.manual_seed(args.seed)\n        np.random.seed(args.seed)\n    # CONFIG\n    ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n    config = ckpt[\"config\"]\n    config[\"device\"] = args.device or config[\"device\"]\n    # DATA\n    train_dataset, _, test_dataset = get_dataset(config)\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=config[\"test_batch_size\"],\n        collate_fn=train_dataset.collate_fn,\n    )\n\n    # MODEL\n    device = torch.device(config[\"device\"])\n    model = get_model(config=config)\n    model.load_state_dict(ckpt[\"model\"])\n    step = ckpt[\"step\"]\n    model = model.to(device)\n    num_params = 0\n    for name, param in model.named_parameters():\n        num_params += np.prod(param.size())\n        print(name, param.size())\n    print(f\"Total Paramaters: {num_params.item()}\")\n\n    # REEVALUATE\n    log_dir = config[\"log_dir\"]\n    folder = os.path.join(log_dir, \"test_plots\", f\"step{step}\")\n    os.makedirs(folder, exist_ok=True)\n\n    results = {\"config\": config}\n    save_dict = dict(predictions=[])\n\n    if args.wass:\n        (\n            wt_mean,\n            wt_conf_interval,\n            future_w_mean,\n            future_w_conf_interval,\n        ) = evaluate_pymunk_dataset(\n            test_loader,\n            model,\n            device=device,\n            num_samples=config[\"num_forecast\"],\n            max_size=args.max_size,\n            no_state_sampling=args.no_state_sampling,\n            use_smooth=args.smooth,\n        ).values()\n        save_dict[\"wass_dist\"] = wt_mean\n\n        show_wasserstein_distance(\n            (15, 2),\n            wt_mean,\n            conf_intervals=wt_conf_interval,\n            fig_title=\"Wasserstein Distance\",\n            file_path=os.path.join(folder, \"wass.png\"),\n        )\n        print(\n            f\"Forecast W: {future_w_mean.item():.3f} \"\n            f\"+/- {future_w_conf_interval.item():.3f}\"\n        )\n        results[\"future_w_mean\"] = future_w_mean.item()\n        results[\"future_w_conf_interval\"] = future_w_conf_interval.item()\n\n    plot_count = 0\n    while plot_count < args.num_plots:\n        for test_batch in test_loader:\n            past_target = test_batch[\"past_target\"].to(device)\n            B, T, _ = past_target.shape\n            mask = test_batch[\"past_mask\"].to(device)\n            future_target = test_batch[\"future_target\"].to(device)\n            past_times = test_batch[\"past_times\"].to(device)\n            future_times = test_batch[\"future_times\"].to(device)\n            predict_result = model.forecast(\n                past_target,\n                mask,\n                past_times.view(-1),\n                future_times.view(-1),\n                num_samples=config[\"num_forecast\"],\n                no_state_sampling=args.no_state_sampling,\n                use_smooth=args.smooth,\n            )\n            reconstruction = predict_result[\"reconstruction\"]\n            forecast = predict_result[\"forecast\"]\n            full_times = torch.cat([past_times, future_times], 0)\n            full_prediction = torch.cat([reconstruction, forecast], dim=-2)\n            full_target = torch.cat([past_target, future_target], dim=-2)\n            latent_variables = dict()\n            if \"z_reconstruction\" in predict_result:\n                full_z = torch.cat(\n                    [predict_result[\"z_reconstruction\"], predict_result[\"z_forecast\"]],\n                    dim=-2,\n                )\n                latent_variables[\"z\"] = full_z\n            if \"alpha_reconstruction\" in predict_result:\n                full_alpha = torch.cat(\n                    [\n                        predict_result[\"alpha_reconstruction\"],\n                        predict_result[\"alpha_forecast\"],\n                    ],\n                    dim=-2,\n                )\n                latent_variables[\"alpha\"] = full_alpha\n            if \"aux_reconstruction\" in predict_result:\n                full_aux = torch.cat(\n                    [\n                        predict_result[\"aux_reconstruction\"],\n                        predict_result[\"aux_forecast\"],\n                    ],\n                    dim=-2,\n                )\n                latent_variables[\"aux\"] = full_aux\n            for j in range(B):\n                full_prediction_j = full_prediction[:, j].view(\n                    full_prediction.shape[0],\n                    full_prediction.shape[-2],\n                    1,\n                    config[\"img_size\"],\n                    config[\"img_size\"],\n                )\n                full_target_j = full_target[j].view(\n                    full_target.shape[1], 1, config[\"img_size\"], config[\"img_size\"]\n                )\n                # Plot first five samples\n                samples_dir = os.path.join(folder, f\"series_{j}\")\n                os.makedirs(samples_dir, exist_ok=True)\n                full_target_j[:T][mask[j] == 0.0] = 0.0\n                save_dict[\"predictions\"].append(\n                    dict(\n                        target=torch2numpy(full_target_j),\n                        pred=torch2numpy(full_prediction_j[0]),\n                    )\n                )\n                show_pymunk_forecast(\n                    torch2numpy(full_target_j),\n                    torch2numpy(full_prediction_j[:5]),\n                    os.path.join(samples_dir, \"prediction.png\"),\n                )\n                if len(latent_variables) > 0:\n                    latent_variables_j = {\n                        k: torch2numpy(v[:, j]) for k, v in latent_variables.items()\n                    }\n                    for m in range(5):\n                        latent_variables_jm = {\n                            k: v[m] for k, v in latent_variables_j.items()\n                        }\n                        plot_path = os.path.join(samples_dir, f\"lat_{m}.png\")\n                        show_latents(\n                            (15, 8),\n                            time=torch2numpy(full_times),\n                            latents=latent_variables_jm,\n                            fig_title=\"Latents\",\n                            file_path=plot_path,\n                        )\n                plot_count += 1\n\n                if plot_count == args.num_plots:\n                    break\n            if plot_count == args.num_plots:\n                break\n\n    with open(os.path.join(log_dir, \"metrics.yaml\"), \"w\") as fp:\n        yaml.dump(results, fp, default_flow_style=False)\n\n    np.savez(os.path.join(log_dir, \"plot_data.npz\"), **save_dict)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "eval_ts.py", "chunked_list": ["import os\nimport yaml\nimport torch\nimport argparse\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom ncdssm.evaluation import evaluate_simple_ts, evaluate_sporadic\nfrom ncdssm.plotting import show_time_series_forecast, show_latents", "from ncdssm.evaluation import evaluate_simple_ts, evaluate_sporadic\nfrom ncdssm.plotting import show_time_series_forecast, show_latents\nfrom ncdssm.torch_utils import torch2numpy, prepend_time_zero\nfrom experiments.setups import get_model, get_dataset\n\n\ndef main():\n    matplotlib.use(\"Agg\")\n\n    # COMMAND-LINE ARGS\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--ckpt\", required=True, type=str, help=\"Path to checkpoint file.\"\n    )\n    parser.add_argument(\n        \"--sporadic\",\n        action=\"store_true\",\n        help=\"Whether sporadic dataset (e.g., climate) is used.\",\n    )\n    parser.add_argument(\"--seed\", type=int, help=\"Random seed.\")\n    parser.add_argument(\n        \"--max_size\",\n        type=int,\n        default=np.inf,\n        help=\"Max number of time series to test.\",\n    )\n    parser.add_argument(\"--device\", type=str, help=\"Device to eval on\")\n    parser.add_argument(\n        \"--no_state_sampling\",\n        action=\"store_true\",\n        help=\"Use only the means of the predicted state distributions without sampling\",\n    )\n    parser.add_argument(\n        \"--smooth\",\n        action=\"store_true\",\n        help=\"Use smoothing for imputation\",\n    )\n    parser.add_argument(\n        \"--num_plots\", type=int, default=0, help=\"The number of plots to save\"\n    )\n\n    args, _ = parser.parse_known_args()\n    if args.seed is not None:\n        torch.manual_seed(args.seed)\n        np.random.seed(args.seed)\n\n    if args.sporadic:\n        evaluate_fn = evaluate_sporadic\n    else:\n        evaluate_fn = evaluate_simple_ts\n    # CONFIG\n    ckpt = torch.load(args.ckpt, map_location=\"cpu\")\n    config = ckpt[\"config\"]\n    config[\"device\"] = args.device or config[\"device\"]\n    # DATA\n    _, _, test_dataset = get_dataset(config)\n\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=config[\"test_batch_size\"],\n        collate_fn=test_dataset.collate_fn,\n    )\n\n    # MODEL\n    device = torch.device(config[\"device\"])\n    model = get_model(config=config)\n    model.load_state_dict(ckpt[\"model\"], strict=True)\n    step = ckpt[\"step\"]\n    model = model.to(device)\n    num_params = 0\n    for name, param in model.named_parameters():\n        num_params += np.prod(param.size())\n        print(name, param.size())\n    print(f\"Total Paramaters: {num_params.item()}\")\n    # print(model.A, model.C)\n\n    # REEVALUATE\n    log_dir = config[\"log_dir\"]\n    folder = os.path.join(log_dir, \"test_plots\", f\"step{step}\")\n    os.makedirs(folder, exist_ok=True)\n\n    results = {\"config\": config}\n\n    if args.max_size > 0:\n        metrics = evaluate_fn(\n            test_loader,\n            model,\n            device,\n            num_samples=config[\"num_forecast\"],\n            no_state_sampling=args.no_state_sampling,\n            use_smooth=args.smooth,\n        )\n\n        results[\"test\"] = metrics\n\n    plot_count = 0\n    plot_data = []\n    while plot_count < args.num_plots:\n        for test_batch in test_loader:\n            past_target = test_batch[\"past_target\"].to(device)\n            B, T, D = past_target.shape\n            mask = test_batch[\"past_mask\"].to(device)\n            future_target = test_batch[\"future_target\"].to(device)\n            past_times = test_batch[\"past_times\"].to(device)\n            future_times = test_batch[\"future_times\"].to(device)\n\n            if past_times[0] > 0:\n                past_times, past_target, mask = prepend_time_zero(\n                    past_times, past_target, mask\n                )\n            predict_result = model.forecast(\n                past_target,\n                mask,\n                past_times.view(-1),\n                future_times.view(-1),\n                num_samples=config[\"num_forecast\"],\n                no_state_sampling=args.no_state_sampling,\n                use_smooth=args.smooth,\n            )\n            reconstruction = predict_result[\"reconstruction\"]\n            forecast = predict_result[\"forecast\"]\n            full_times = torch.cat([past_times, future_times], 0)\n            latent_variables = dict()\n            if \"z_reconstruction\" in predict_result:\n                full_z = torch.cat(\n                    [predict_result[\"z_reconstruction\"], predict_result[\"z_forecast\"]],\n                    dim=-2,\n                )\n                latent_variables[\"z\"] = full_z\n            if \"alpha_reconstruction\" in predict_result:\n                full_alpha = torch.cat(\n                    [\n                        predict_result[\"alpha_reconstruction\"],\n                        predict_result[\"alpha_forecast\"],\n                    ],\n                    dim=-2,\n                )\n                latent_variables[\"alpha\"] = full_alpha\n\n            for j in range(B):\n                print(f\"Plotting {plot_count + 1}/{config['num_plots']}\")\n                samples_dir = os.path.join(folder, f\"series_{j}\")\n                os.makedirs(samples_dir, exist_ok=True)\n                masked_past_target = past_target.clone()\n                masked_past_target[mask == 0.0] = float(\"nan\")\n                plot_data_j = dict(\n                    fig_size=(12, 5),\n                    past_times=torch2numpy(past_times),\n                    future_times=torch2numpy(future_times),\n                    inputs=torch2numpy(torch.cat([past_target, future_target], 1))[j],\n                    masked_inputs=torch2numpy(\n                        torch.cat([masked_past_target, future_target], 1)\n                    )[j],\n                    reconstruction=torch2numpy(reconstruction)[:, j],\n                    forecast=torch2numpy(forecast)[:, j],\n                )\n                plot_data.append(plot_data_j)\n                fig = show_time_series_forecast(\n                    **plot_data_j,\n                    file_path=os.path.join(samples_dir, f\"series_{plot_count}.png\"),\n                )\n                plt.close(fig)\n                if len(latent_variables) > 0:\n                    latent_variables_j = {\n                        k: torch2numpy(v[:, j]) for k, v in latent_variables.items()\n                    }\n                    for m in range(5):\n                        latent_variables_jm = {\n                            k: v[m] for k, v in latent_variables_j.items()\n                        }\n                        plot_path = os.path.join(samples_dir, f\"lat_{m}.png\")\n                        show_latents(\n                            (15, 8),\n                            time=torch2numpy(full_times),\n                            latents=latent_variables_jm,\n                            fig_title=\"Latents\",\n                            file_path=plot_path,\n                        )\n                plot_count += 1\n\n                if plot_count == args.num_plots:\n                    break\n            if plot_count == args.num_plots:\n                break\n    if args.max_size > 0:\n        with open(os.path.join(log_dir, \"metrics.yaml\"), \"w\") as fp:\n            yaml.dump(results, fp, default_flow_style=False)", "\n\nif __name__ == \"__main__\":\n    main()\n"]}
{"filename": "data/pong.py", "chunked_list": ["import pygame\nimport pymunk.pygame_util\nimport numpy as np\nimport os\nfrom pygame.color import THECOLORS as color\nfrom pathlib import Path\n\n# Note: This code is taken from\n# https://github.com/simonkamronn/kvae/blob/master/kvae/datasets/box.py\n# Made only minor adjustments, e.g. the saving paths,", "# https://github.com/simonkamronn/kvae/blob/master/kvae/datasets/box.py\n# Made only minor adjustments, e.g. the saving paths,\n# number of time-steps for test data.\n\nscale = 1\n\n\nclass Pong:\n    def __init__(self, dt=0.2, res=(32, 32), init_pos=(3, 3), init_std=0, wall=None):\n        pygame.init()\n\n        self.dt = dt\n        self.res = res\n        if os.environ.get(\"SDL_VIDEODRIVER\", \"\") == \"dummy\":\n            pygame.display.set_mode(res, 0, 24)\n            self.screen = pygame.Surface(res, pygame.SRCCOLORKEY, 24)\n            pygame.draw.rect(self.screen, (0, 0, 0), (0, 0, res[0], res[1]), 0)\n        else:\n            self.screen = pygame.display.set_mode(res, 0, 24)\n        self.gravity = (0.0, 0.0)\n        self.initial_position = init_pos\n        self.initial_std = init_std\n        self.space = pymunk.Space()\n        self.space.gravity = self.gravity\n        self.draw_options = pymunk.pygame_util.DrawOptions(self.screen)\n        self.clock = pygame.time.Clock()\n        self.wall = wall\n        self.static_lines = None\n\n        self.dd = 2\n\n    def _clear(self):\n        self.screen.fill(color[\"black\"])\n\n    class Paddle:\n        def __init__(self, pong, position):\n            self.pong = pong\n            self.area = pong.res\n            if position == \"left\":\n                self.rect = pymunk.Segment(\n                    pong.space.static_body,\n                    (0, self.area[1] / 2 + 3 * scale),\n                    (0, self.area[1] / 2 - 3 * scale),\n                    1.0,\n                )\n            else:\n                self.rect = pymunk.Segment(\n                    pong.space.static_body,\n                    (self.area[0] - 2, self.area[1] / 2 + 3 * scale),\n                    (self.area[0] - 2, self.area[1] / 2 - 3 * scale),\n                    1.0,\n                )\n            self.speed = 2 * scale\n            self.rect.elasticity = 0.99\n            self.rect.color = color[\"white\"]\n            self.rect.collision_type = 1\n\n        def update(self, ball):\n            a, b = self.rect.a, self.rect.b\n            center = (a.y + b.y) / 2\n            if ball.body.position.y < center - self.speed / 2:\n                delta_y = min(b.y, self.speed)\n                a.y -= delta_y\n                b.y -= delta_y\n                self.rect.unsafe_set_endpoints(a, b)\n            if ball.body.position.y > center + self.speed / 2:\n                delta_y = min(self.area[1] - a.y, self.speed)\n                a.y += delta_y\n                b.y += delta_y\n                self.rect.unsafe_set_endpoints(a, b)\n            return self.rect\n\n    def add_walls(self):\n        self.static_lines = []\n\n        # Add floor\n        self.static_lines.append(\n            pymunk.Segment(self.space.static_body, (0, 1), (self.res[1], 1), 0.0)\n        )\n\n        # Add roof\n        self.static_lines.append(\n            pymunk.Segment(\n                self.space.static_body,\n                (0, self.res[1]),\n                (self.res[1], self.res[1]),\n                0.0,\n            )\n        )\n\n        # Set properties\n        for line in self.static_lines:\n            line.elasticity = 0.99\n            line.color = color[\"white\"]\n        self.space.add(self.static_lines)\n        return True\n\n    def create_ball(self, radius=3):\n        inertia = pymunk.moment_for_circle(1, 0, radius, (0, 0))\n        body = pymunk.Body(1, inertia)\n        position = np.array(\n            self.initial_position\n        ) + self.initial_std * np.random.normal(size=(2,))\n        position = np.clip(\n            position, self.dd + radius + 1, self.res[0] - self.dd - radius - 1\n        )\n        body.position = position\n\n        shape = pymunk.Circle(body, radius, (0, 0))\n        shape.elasticity = 0.9\n        shape.color = color[\"white\"]\n        return shape\n\n    def fire(self, angle=50, velocity=20, radius=3):\n        speedX = velocity * np.cos(angle * np.pi / 180)\n        speedY = velocity * np.sin(angle * np.pi / 180)\n\n        ball = self.create_ball(radius)\n        ball.body.velocity = (speedX, speedY)\n\n        self.space.add(ball, ball.body)\n        return ball\n\n    def run(\n        self,\n        iterations=20,\n        sequences=500,\n        angle_limits=(0, 360),\n        radius=3,\n        save=None,\n        filepath=\"/tmp/data/pong.npz\",\n        delay=None,\n    ):\n        if save:\n            data = np.empty(\n                (sequences, iterations, self.res[0], self.res[1]),\n                dtype=np.float32,\n            )\n        controls = None, None\n\n        # Add roof and floor\n        self.add_walls()\n\n        for s in range(sequences):\n            if s % 100 == 0:\n                print(f\"sequence {s}/{sequences}\")\n\n            angle = np.random.uniform(*angle_limits)\n            velocity = 10 * scale\n            ball = self.fire(angle, velocity, radius)\n\n            # Create pong paddles\n            paddle1 = self.Paddle(self, \"left\")\n            paddle2 = self.Paddle(self, \"right\")\n\n            for i in range(iterations):\n                self._clear()\n\n                # Add paddles\n                self.space.add(paddle1.update(ball))\n                self.space.add(paddle2.update(ball))\n\n                # Step\n                self.space.step(self.dt)\n\n                # Draw objects\n                self.space.debug_draw(self.draw_options)\n                pygame.display.flip()\n\n                if delay:\n                    self.clock.tick(delay)\n\n                if save == \"png\":\n                    pygame.image.save(\n                        self.screen,\n                        os.path.join(filepath, \"pong_%02d_%02d.png\" % (s, i)),\n                    )\n                elif save == \"npz\":\n                    data[s, i] = (\n                        pygame.surfarray.array2d(self.screen)\n                        .swapaxes(1, 0)\n                        .astype(np.float32)\n                        / 255\n                    )\n\n                # Remove the paddles\n                self.space.remove(paddle1.rect)\n                self.space.remove(paddle2.rect)\n\n            # Remove the ball and the wall from the space\n            self.space.remove(ball, ball.body)\n\n        if save == \"npz\":\n            np.savez(os.path.abspath(filepath), images=data, controls=controls)", "\n\ndef generate_dataset(data_path, seed=1234, n_timesteps_train=20, n_timesteps_test=100):\n    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n    np.random.seed(seed=seed)\n\n    # Create data dir\n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n\n    cannon = Pong(\n        dt=0.2,\n        res=(32 * scale, 32 * scale),\n        init_pos=(16 * scale, 16 * scale),\n        init_std=3,\n        wall=None,\n    )\n    cannon.run(\n        delay=None,\n        iterations=n_timesteps_train,\n        sequences=5000,\n        radius=3 * scale,\n        angle_limits=(0, 360),\n        filepath=os.path.join(data_path, \"train.npz\"),\n        save=\"npz\",\n    )\n    cannon.run(\n        delay=None,\n        iterations=n_timesteps_test,\n        sequences=100,\n        radius=3 * scale,\n        angle_limits=(0, 360),\n        filepath=os.path.join(data_path, \"val.npz\"),\n        save=\"npz\",\n    )\n    np.random.seed(5678)\n    cannon.run(\n        delay=None,\n        iterations=n_timesteps_test,\n        sequences=1000,\n        radius=3 * scale,\n        angle_limits=(0, 360),\n        filepath=os.path.join(data_path, \"test.npz\"),\n        save=\"npz\",\n    )", "\n\nif __name__ == \"__main__\":\n    data_path = str(Path(__file__).resolve().parent / \"pymunk\" / \"pong\")\n    print(f\"Saving dataset to: {data_path}.\")\n    generate_dataset(data_path)\n"]}
{"filename": "data/climate.py", "chunked_list": ["import urllib.request\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nCLIMATE_DATA_URL = \"https://raw.githubusercontent.com/edebrouwer/gru_ode_bayes/master/gru_ode_bayes/datasets/Climate/small_chunked_sporadic.csv\"  # noqa\nDATA_ROOT = Path(__file__).resolve().parent / \"climate\"\nLOCAL_PATH = DATA_ROOT / \"climate-data-preproc.csv\"", "DATA_ROOT = Path(__file__).resolve().parent / \"climate\"\nLOCAL_PATH = DATA_ROOT / \"climate-data-preproc.csv\"\n\n\ndef download_preprocessed():\n    print(\"Downloading dataset.\")\n    DATA_ROOT.mkdir(exist_ok=True)\n    urllib.request.urlretrieve(CLIMATE_DATA_URL, LOCAL_PATH)\n\n\ndef generate_folds(num_folds=5, seed=432):\n    print(\"Generating folds.\")\n    # Modified from https://github.com/edebrouwer/gru_ode_bayes/blob/master/data_preproc/Climate/generate_folds.py # noqa\n    num_series = pd.read_csv(LOCAL_PATH)[\"ID\"].nunique()\n    np.random.seed(seed)\n\n    for fold in range(num_folds):\n        train_idx, test_idx = train_test_split(np.arange(num_series), test_size=0.1)\n        train_idx, val_idx = train_test_split(train_idx, test_size=0.2)\n        fold_dir = DATA_ROOT / f\"fold_idx_{fold}/\"\n        fold_dir.mkdir(exist_ok=True)\n\n        np.save(fold_dir / \"train_idx.npy\", train_idx)\n        np.save(fold_dir / \"val_idx.npy\", val_idx)\n        np.save(fold_dir / \"test_idx.npy\", test_idx)", "\n\ndef generate_folds(num_folds=5, seed=432):\n    print(\"Generating folds.\")\n    # Modified from https://github.com/edebrouwer/gru_ode_bayes/blob/master/data_preproc/Climate/generate_folds.py # noqa\n    num_series = pd.read_csv(LOCAL_PATH)[\"ID\"].nunique()\n    np.random.seed(seed)\n\n    for fold in range(num_folds):\n        train_idx, test_idx = train_test_split(np.arange(num_series), test_size=0.1)\n        train_idx, val_idx = train_test_split(train_idx, test_size=0.2)\n        fold_dir = DATA_ROOT / f\"fold_idx_{fold}/\"\n        fold_dir.mkdir(exist_ok=True)\n\n        np.save(fold_dir / \"train_idx.npy\", train_idx)\n        np.save(fold_dir / \"val_idx.npy\", val_idx)\n        np.save(fold_dir / \"test_idx.npy\", test_idx)", "\n\nif __name__ == \"__main__\":\n    download_preprocessed()\n    generate_folds()\n"]}
{"filename": "data/box.py", "chunked_list": ["import pygame\nimport pymunk.pygame_util\nimport numpy as np\nimport os\nfrom pathlib import Path\n\n# Note: This code is taken from\n# https://github.com/simonkamronn/kvae/blob/master/kvae/datasets/box.py\n# Made only minor adjustments, e.g. the saving paths,\n# number of time-steps for test data.", "# Made only minor adjustments, e.g. the saving paths,\n# number of time-steps for test data.\n\n\nclass BallBox:\n    def __init__(\n        self,\n        dt=0.2,\n        res=(32, 32),\n        init_pos=(3, 3),\n        init_std=0,\n        wall=None,\n        gravity=(0.0, 0.0),\n    ):\n        pygame.init()\n\n        self.dt = dt\n        self.res = res\n        if os.environ.get(\"SDL_VIDEODRIVER\", \"\") == \"dummy\":\n            pygame.display.set_mode(res, 0, 24)\n            self.screen = pygame.Surface(res, pygame.SRCCOLORKEY, 24)\n            pygame.draw.rect(self.screen, (0, 0, 0), (0, 0, res[0], res[1]), 0)\n        else:\n            self.screen = pygame.display.set_mode(res, 0, 24)\n        self.gravity = gravity\n        self.initial_position = init_pos\n        self.initial_std = init_std\n        self.space = pymunk.Space()\n        self.space.gravity = self.gravity\n        self.draw_options = pymunk.pygame_util.DrawOptions(self.screen)\n        self.clock = pygame.time.Clock()\n        self.wall = wall\n        self.static_lines = None\n\n        self.dd = 2\n\n    def _clear(self):\n        self.screen.fill(pygame.color.THECOLORS[\"black\"])\n\n    def create_ball(self, radius=3):\n        inertia = pymunk.moment_for_circle(1, 0, radius, (0, 0))\n        body = pymunk.Body(1, inertia)\n        position = np.array(\n            self.initial_position\n        ) + self.initial_std * np.random.normal(size=(2,))\n        position = np.clip(\n            position, self.dd + radius + 1, self.res[0] - self.dd - radius - 1\n        )\n        body.position = position\n\n        shape = pymunk.Circle(body, radius, (0, 0))\n        shape.elasticity = 1.0\n        shape.color = pygame.color.THECOLORS[\"white\"]\n        return shape\n\n    def fire(self, angle=50, velocity=20, radius=3):\n        speedX = velocity * np.cos(angle * np.pi / 180)\n        speedY = velocity * np.sin(angle * np.pi / 180)\n\n        ball = self.create_ball(radius)\n        ball.body.velocity = (speedX, speedY)\n\n        self.space.add(ball, ball.body)\n        return ball\n\n    def run(\n        self,\n        iterations=20,\n        sequences=500,\n        angle_limits=(0, 360),\n        velocity_limits=(10, 25),\n        radius=3,\n        flip_gravity=None,\n        save=None,\n        filepath=\"/tmp/data/box.npz\",\n        delay=None,\n    ):\n        if save:\n            images = np.empty(\n                (sequences, iterations, self.res[0], self.res[1]),\n                dtype=np.float32,\n            )\n            state = np.empty((sequences, iterations, 4), dtype=np.float32)\n\n        dd = self.dd\n        self.static_lines = [\n            pymunk.Segment(\n                self.space.static_body, (dd, dd), (dd, self.res[1] - dd), 0.0\n            ),\n            pymunk.Segment(\n                self.space.static_body, (dd, dd), (self.res[0] - dd, dd), 0.0\n            ),\n            pymunk.Segment(\n                self.space.static_body,\n                (self.res[0] - dd, self.res[1] - dd),\n                (dd, self.res[1] - dd),\n                0.0,\n            ),\n            pymunk.Segment(\n                self.space.static_body,\n                (self.res[0] - dd, self.res[1] - dd),\n                (self.res[0] - dd, dd),\n                0.0,\n            ),\n        ]\n        for line in self.static_lines:\n            line.elasticity = 1.0\n            line.color = pygame.color.THECOLORS[\"white\"]\n        self.space.add(self.static_lines)\n\n        for s in range(sequences):\n            if s % 100 == 0:\n                print(f\"sequence: {s}/{sequences}\")\n\n            angle = np.random.uniform(*angle_limits)\n            velocity = np.random.uniform(*velocity_limits)\n            # controls[:, s] = np.array([angle, velocity])\n            ball = self.fire(angle, velocity, radius)\n            for i in range(iterations):\n                self._clear()\n                self.space.debug_draw(self.draw_options)\n                self.space.step(self.dt)\n                pygame.display.flip()\n\n                if delay:\n                    self.clock.tick(delay)\n\n                if save == \"png\":\n                    pygame.image.save(\n                        self.screen,\n                        os.path.join(filepath, \"bouncing_balls_%02d_%02d.png\" % (s, i)),\n                    )\n                elif save == \"npz\":\n                    images[s, i] = pygame.surfarray.array2d(self.screen).swapaxes(\n                        1, 0\n                    ).astype(np.float32) / (2**24 - 1)\n                    state[s, i] = list(ball.body.position) + list(ball.body.velocity)\n\n            # Remove the ball and the wall from the space\n            self.space.remove(ball, ball.body)\n\n        if save == \"npz\":\n            np.savez(os.path.abspath(filepath), images=images, state=state)", "\n\ndef generate_dataset(data_path, seed=1234, n_timesteps_train=20, n_timesteps_test=100):\n    os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # Remove and add delay to see the videos\n    scale = 1\n    np.random.seed(seed=seed)\n\n    # Create data dir\n    if not os.path.exists(data_path):\n        os.makedirs(data_path)\n\n    cannon = BallBox(\n        dt=0.2,\n        res=(32 * scale, 32 * scale),\n        init_pos=(16 * scale, 16 * scale),\n        init_std=10 * scale,\n        wall=None,\n    )\n\n    print(\"Generating training sequences\")\n    cannon.run(\n        delay=None,\n        iterations=n_timesteps_train,\n        sequences=5000,\n        radius=3 * scale,\n        angle_limits=(0, 360),\n        velocity_limits=(10.0 * scale, 15.0 * scale),\n        filepath=os.path.join(data_path, \"train.npz\"),\n        save=\"npz\",\n    )\n\n    print(\"Generating val sequences\")\n    cannon.run(\n        delay=None,\n        iterations=n_timesteps_test,\n        sequences=100,\n        radius=3 * scale,\n        angle_limits=(0, 360),\n        velocity_limits=(10.0 * scale, 15.0 * scale),\n        filepath=os.path.join(data_path, \"val.npz\"),\n        save=\"npz\",\n    )\n\n    print(\"Generating test sequences\")\n    np.random.seed(5678)\n    cannon.run(\n        delay=None,\n        iterations=n_timesteps_test,\n        sequences=1000,\n        radius=3 * scale,\n        angle_limits=(0, 360),\n        velocity_limits=(10.0 * scale, 15.0 * scale),\n        filepath=os.path.join(data_path, \"test.npz\"),\n        save=\"npz\",\n    )", "\n\nif __name__ == \"__main__\":\n    data_path = str(Path(__file__).resolve().parent / \"pymunk\" / \"box\")\n    print(f\"Saving dataset to: {data_path}.\")\n    generate_dataset(data_path)\n"]}
{"filename": "data/bouncing_ball.py", "chunked_list": ["import os\nimport numpy as np\nimport argparse\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\n\n\ndef generate_sequence(low=-1, high=1.0, vel=None, num_steps=300):\n    y = np.random.uniform(low=low, high=high)\n    if vel is None:\n        vel = np.random.uniform(low=0.05, high=0.5) * np.random.choice([-1, 1])\n    noise_scale = 0.05\n    points = [y + noise_scale * np.random.randn(1)]\n    step_size = 0.1\n    for i in range(num_steps - 1):\n        y = y + vel * step_size\n        points.append(y + noise_scale * np.random.randn(1))\n        if y <= low or y >= high:\n            vel = -vel\n    return np.stack(points)", "\n\ndef generate_sequences(num_samples, vels=None, num_steps=300):\n    all_target = []\n    for _ in tqdm(range(num_samples)):\n        chosen_v = np.random.choice(vels) if vels is not None else None\n        y = generate_sequence(vel=chosen_v, num_steps=num_steps)\n        all_target.append(y)\n    all_target = np.stack(all_target)\n    return all_target", "\n\ndef generate_dataset(\n    seed=42,\n    vels=None,\n    dataset_path=None,\n    n_train=5000,\n    n_val=500,\n    n_test=500,\n    n_timesteps=300,\n    file_prefix=\"\",\n):\n    if dataset_path is None:\n        dataset_path = \"./bouncing_ball/\"\n    os.makedirs(dataset_path, exist_ok=True)\n\n    np.random.seed(seed=seed)\n\n    obs_train = generate_sequences(n_train, vels=vels, num_steps=n_timesteps)\n    obs_val = generate_sequences(n_val, vels=vels, num_steps=n_timesteps)\n    obs_test = generate_sequences(n_test, vels=vels, num_steps=n_timesteps)\n\n    np.savez(os.path.join(dataset_path, f\"{file_prefix}train.npz\"), target=obs_train)\n    np.savez(os.path.join(dataset_path, f\"{file_prefix}val.npz\"), target=obs_val)\n    np.savez(os.path.join(dataset_path, f\"{file_prefix}test.npz\"), target=obs_test)", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--num_vels\",\n        type=int,\n        default=0,\n        help=\"Number of fixed velocities, 0 indicates ranodm velocity for every sample\",\n        choices=[0, 1, 2, 5],\n    )\n    args = parser.parse_args()\n\n    data_path = str(Path(__file__).resolve().parent / \"bouncing_ball\")\n    print(f\"Saving dataset to: {data_path}.\")\n    vels = None\n    if args.num_vels == 0:\n        vels = None\n        print(\"Generating dataset with random velocties...\")\n        generate_dataset(file_prefix=\"rv_\", vels=vels, dataset_path=data_path)\n    elif args.num_vels == 1:\n        vels = [0.2]\n        print(\"Generating dataset with 1 veloctity...\")\n        generate_dataset(file_prefix=\"1fv_\", vels=vels, dataset_path=data_path)\n    elif args.num_vels == 2:\n        vels = [0.2, 0.4]\n        print(\"Generating dataset with 2 velocties...\")\n        generate_dataset(file_prefix=\"2fv_\", vels=vels, dataset_path=data_path)\n    elif args.num_vels == 5:\n        vels = [0.1, 0.2, 0.3, 0.4, 0.5]\n        print(\"Generating dataset with 5 velocties...\")\n        generate_dataset(file_prefix=\"5fv_\", vels=vels, dataset_path=data_path)", ""]}
{"filename": "data/damped_pendulum.py", "chunked_list": ["import os\nimport numpy as np\nfrom pathlib import Path\n\nimport contextlib\nimport torch\n\n\n@contextlib.contextmanager\ndef local_seed(seed):\n    state_np = np.random.get_state()\n    state_torch = torch.random.get_rng_state()\n    np.random.seed(seed)\n    torch.random.manual_seed(seed)\n    try:\n        yield\n    finally:\n        np.random.set_state(state_np)\n        torch.random.set_rng_state(state_torch)", "@contextlib.contextmanager\ndef local_seed(seed):\n    state_np = np.random.get_state()\n    state_torch = torch.random.get_rng_state()\n    np.random.seed(seed)\n    torch.random.manual_seed(seed)\n    try:\n        yield\n    finally:\n        np.random.set_state(state_np)\n        torch.random.set_rng_state(state_torch)", "\n\ndef rungekutta4(state: np.ndarray, t: float, dt: float, f: callable):\n    k1 = dt * f(state=state, t=t)\n    k2 = dt * f(state=state + 0.5 * k1, t=t + 0.5 * dt)\n    k3 = dt * f(state=state + 0.5 * k2, t=t + 0.5 * dt)\n    k4 = dt * f(state=state + k3, t=t + dt)\n\n    next_state = state + (k1 + 2.0 * (k2 + k3) + k4) / 6.0\n    next_t = t + dt\n    return next_state, next_t", "\n\nclass Environment(object):\n    def __init__(self, n_obs, n_state, n_ctrl, dt):\n        self.n_obs = n_obs\n        self.n_state = n_state\n        self.n_ctrl = n_ctrl\n        self.dt = dt\n\n    def enter(self, *args, **kwargs):\n        raise NotImplementedError()\n\n    def observe(self, *args, **kwargs):\n        raise NotImplementedError()\n\n    def transit(self, ctrl):\n        raise NotImplementedError()", "\n\nclass Pendulum3DCoordEnvironment(Environment):\n    def __init__(\n        self,\n        dt=0.1,\n        center_pendulum=np.array((0, 0, 2.0)),\n        cov_pendulum=np.array((0.1, 0.2, 0.1)),\n        noise_std_obs=0.05,\n        radius=1.0,\n        g=9.81,\n        mass=1.0,\n        dampening=0.25,\n        noise_std_dyn=0.0,\n        f=1.0,  # focal length\n    ):\n        super().__init__(\n            n_obs=2,\n            n_state=2,\n            # We use first order ODE representation --> state is (angle, d/dt angle)\n            n_ctrl=None,  # skip ctrl for now\n            dt=dt,\n        )\n        self.g = g\n        self.mass = mass\n        self.dampening = dampening\n        self.cov_pendulum = cov_pendulum\n        self.noise_std_obs = noise_std_obs\n        self.noise_std_dyn = noise_std_dyn\n        self.center_pendulum = center_pendulum\n        # initial position of pendulum. last entry is the distance\n        # from the camera, assumed to\n        # have the same initial coordinate system,\n        # with just a translation in direction z.\n        self.r_rotation_pendulum = radius\n        self.f = f\n\n    def _stochastic_differential_equation(self, state, t):\n        w = np.random.normal(loc=0.0, scale=self.noise_std_dyn)\n        return np.array(\n            [\n                state[1],\n                -(\n                    (self.g / 2.0) * np.sin(state[0])\n                    + (self.dampening / self.mass) * state[1]\n                    + w\n                ),\n            ]\n        )\n\n    def enter(self):\n        self.state = np.zeros(self.n_state)\n        self.state[0] = np.pi + np.clip(np.random.randn(1), -2, 2) * 1.0\n        self.state[1] = np.clip(np.random.randn(1), -2, 2) * 4.0\n        self.t = 0.0\n\n    def transit(self, ctrl):\n        self.state, self.t = rungekutta4(\n            state=self.state,\n            t=self.t,\n            dt=self.dt,\n            f=self._stochastic_differential_equation,\n        )\n        return self.state, self.t\n\n    def observe(self, perspective):\n        pendulum_loc, _ = self.project_to_pixel_space(perspective=perspective)\n        pendulum_loc_noisy = pendulum_loc + np.random.normal(\n            loc=0, scale=self.noise_std_obs, size=pendulum_loc.shape\n        )\n        return pendulum_loc_noisy, pendulum_loc\n\n    def project_to_pixel_space(self, perspective):\n        \"\"\"\n        1) Rotate pendulum, represented as 3D normal distribution,\n        2) rotate Camera to some perspective with fixed distance to\n            center of pendulum rotation point,\n        3) project the scene into image space\n        4) and further into pixel space\n        This function works with homogenous coordinates (hom)\n        and assumes the initial camera location at (0, 0, 0).\n        The transformed covariance matrix is calculated by\n            transforming the vectors of variance directions.\n        :param angle: float, angle of pendulum, around z-axis\n        :param perspective: list or tuple of length 2,\n            rotation of camera arond x and y axis.\n        :return: 1D-array: pixel values of projected image\n        -------------------------------------------\n\n        Example:\n        dims_img = (16, 16, 1)\n        env = PendulumWithPerspectiveEnvironment(dims_img=dims_img)\n        env.enter()\n        for angle in np.linspace(0, 2 * np.pi, 20):\n            plt.cla()\n            obs = env.observe(\n                state=angle, perspective=[0, np.pi / 4]\n            ).reshape(dims_img[:2])\n            plt.imshow(obs, cmap='gray')\n            plt.pause(0.02)\n        \"\"\"\n        # 1) Rotate Pendulum around z-axis\n        angle = self.state[0]\n        M_pendulum = np.array(\n            [\n                [np.cos(angle), -np.sin(angle), 0, 0],\n                [np.sin(angle), np.cos(angle), 0, 0],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1],\n            ]\n        )\n\n        mu_pendulum_hom = np.append(\n            self.center_pendulum - np.array([0, self.r_rotation_pendulum, 0]),\n            1,\n        )\n        cov_pendulum_hom = np.diag(np.append(self.cov_pendulum, 1))\n\n        mu_pendulum_rotated = M_pendulum.dot(mu_pendulum_hom)\n        cov_pendulum_rotated = M_pendulum.dot(cov_pendulum_hom).dot(M_pendulum.T)\n\n        # 2) Rotate Camera around pendulum center on x-axis and/or y-axis\n        x, y, z = self.center_pendulum\n        T_camera = np.array([[1, 0, 0, -x], [0, 1, 0, -y], [0, 0, 1, -z], [0, 0, 0, 1]])\n        inv_T_camera = np.array(\n            [[1, 0, 0, x], [0, 1, 0, y], [0, 0, 1, z], [0, 0, 0, 1]]\n        )\n\n        c = np.cos(perspective[0])\n        s = np.sin(perspective[0])\n        rx = np.array([[1, 0, 0, 0], [0, c, -s, 0], [0, s, c, 0], [0, 0, 0, 1]])\n\n        c = np.cos(perspective[1])\n        s = np.sin(perspective[1])\n        ry = np.array([[c, 0, s, 0], [0, 1, 0, 0], [-s, 0, c, 0], [0, 0, 0, 1]])\n\n        R_camera = rx.dot(\n            ry\n        )  # rotation around center of camera, first rotate around y, then x.\n        M_camera = inv_T_camera.dot(R_camera).dot(\n            T_camera\n        )  # rotation around center of pendulum\n\n        # To rotate the mean, need to project it first to origin,\n        # then rotate, then project back\n        mu_pendulum_camera = M_camera.dot(mu_pendulum_rotated)\n        # The vectors for cov-matrix are already vectors\n        # w.r.t. origin, no need to translate\n        cov_pendulum_camera = R_camera.dot(cov_pendulum_rotated).dot(R_camera.T)\n\n        # 3) Project 3D coordinates of mu and cov vector\n        # into image-space, using focal length of camera\n        f = self.f\n        F = np.array([[f, 0, 0, 0], [0, f, 0, 0], [0, 0, 1, 0]])\n        mu_pendulum_projected = np.dot(F, mu_pendulum_camera)\n        cov_pendulum_projected = np.dot(F, cov_pendulum_camera).dot(F.T)\n\n        # 4) Transform to Pixel coordinates\n        M_pixel = np.array(\n            [\n                [1 / mu_pendulum_projected[-1], 0, 0],\n                [0, 1 / mu_pendulum_projected[-1], 0],\n            ]\n        )\n        mu_pendulum_pixel = M_pixel.dot(mu_pendulum_projected)\n        cov_pendulum_pixel = M_pixel.dot(cov_pendulum_projected).dot(M_pixel.T)\n        return mu_pendulum_pixel, cov_pendulum_pixel", "\n\ndef generate_dataset(\n    seed=42,\n    dataset_path=None,\n    n_train=5000,\n    n_val=1000,\n    n_test=1000,\n    n_timesteps=150,\n    perspective=(0.0, 0.0),\n):\n    if dataset_path is None:\n        dataset_path = \"./damped_pendulum/\"\n    os.makedirs(dataset_path, exist_ok=True)\n\n    def generate_sequence(n_timesteps_obs, n_steps_sim_per_obs=10, dt_sim=0.01):\n        assert isinstance(n_steps_sim_per_obs, int) and n_steps_sim_per_obs > 0\n        env = Pendulum3DCoordEnvironment(dt=dt_sim)\n        env.enter()\n        observations = []\n        observations_gt = []\n        states = []\n        for t_obs in range(n_timesteps_obs):\n            obs, obs_gt = env.observe(perspective=perspective)\n            observations.append(obs)\n            observations_gt.append(obs_gt)\n            states.append(env.state)\n            for t_sim in range(n_steps_sim_per_obs):\n                env.transit(ctrl=None)\n        observations = np.stack(observations, axis=0)\n        observations_gt = np.stack(observations_gt, axis=0)\n        states = np.stack(states, axis=0)\n        return observations, observations_gt, states\n\n    def generate_dataset(n_data, n_timesteps):\n        observations_dataset = []\n        observations_gt_dataset = []\n        states_dataset = []\n        for idx_sample in range(n_data):\n            if idx_sample % 100 == 0:\n                print(f\"sequence {idx_sample}/{n_data}\")\n            observations, observations_gt, states = generate_sequence(\n                n_timesteps_obs=n_timesteps\n            )\n            observations_dataset.append(observations)\n            observations_gt_dataset.append(observations_gt)\n            states_dataset.append(states)\n        observations_dataset = np.stack(observations_dataset, axis=0)\n        observations_gt_dataset = np.stack(observations_gt_dataset, axis=0)\n        states_dataset = np.stack(states_dataset, axis=0)\n        return observations_dataset, observations_gt_dataset, states_dataset\n\n    with local_seed(seed=seed):\n        (\n            observations_train,\n            observations_gt_train,\n            states_train,\n        ) = generate_dataset(n_data=n_train, n_timesteps=n_timesteps)\n        (\n            observations_val,\n            observations_gt_val,\n            states_val,\n        ) = generate_dataset(n_data=n_val, n_timesteps=n_timesteps)\n        (\n            observations_test,\n            observations_gt_test,\n            states_test,\n        ) = generate_dataset(n_data=n_test, n_timesteps=n_timesteps)\n\n    np.savez(\n        os.path.join(dataset_path, \"train.npz\"),\n        obs=observations_train,\n        obs_gt=observations_gt_train,\n        state=states_train,\n    )\n    np.savez(\n        os.path.join(dataset_path, \"val.npz\"),\n        obs=observations_val,\n        obs_gt=observations_gt_val,\n        state=states_val,\n    )\n    np.savez(\n        os.path.join(dataset_path, \"test.npz\"),\n        obs=observations_test,\n        obs_gt=observations_gt_test,\n        state=states_test,\n    )", "\n\nif __name__ == \"__main__\":\n    data_path = str(Path(__file__).resolve().parent / \"damped_pendulum\")\n    print(f\"Saving dataset to: {data_path}.\")\n    generate_dataset(dataset_path=data_path)\n"]}
{"filename": "experiments/utils.py", "chunked_list": ["import re\nfrom argparse import ArgumentParser, ArgumentTypeError\nfrom torch import nn\nimport yaml\n\n\nimport os\nfrom datetime import datetime\n\nfrom ncdssm.type import Dict", "\nfrom ncdssm.type import Dict\n\n\ndef get_config_and_setup_dirs(filename: str = \"config.yaml\"):\n    with open(filename, \"r\") as fp:\n        config = yaml.load(fp, Loader=yaml.Loader)\n\n    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n    config[\"exp_root_dir\"] = config[\"exp_root_dir\"].format(\n        model=config[\"model\"].lower(),\n        dataset=config[\"dataset\"].lower(),\n        timestamp=timestamp,\n    )\n    config[\"log_dir\"] = os.path.join(config[\"exp_root_dir\"], \"logs\")\n    config[\"ckpt_dir\"] = os.path.join(config[\"exp_root_dir\"], \"ckpts\")\n    os.makedirs(config[\"log_dir\"])\n    os.makedirs(config[\"ckpt_dir\"])\n\n    return config", "\n\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    else:\n        raise ArgumentTypeError(\"Boolean value expected.\")", "\n\ndef add_config_to_argparser(config: Dict, parser: ArgumentParser):\n    for k, v in config.items():\n        sanitized_key = re.sub(r\"[^\\w\\-]\", \"\", k).replace(\"-\", \"_\")\n        val_type = type(v)\n        if val_type not in {int, float, str, bool}:\n            print(f\"WARNING: Skipping key {k}!\")\n            continue\n        if val_type == bool:\n            parser.add_argument(f\"--{sanitized_key}\", type=str2bool, default=v)\n        else:\n            parser.add_argument(f\"--{sanitized_key}\", type=val_type, default=v)\n    return parser", "\n\ndef get_activation(name: str) -> nn.Module:\n    name = name.lower()\n    if name == \"tanh\":\n        return nn.Tanh\n    elif name == \"softplus\":\n        return nn.Softplus\n    elif name == \"relu\":\n        return nn.ReLU\n    elif name == \"softmax\":\n        return nn.Softmax\n    else:\n        raise ValueError(f\"Unknown non-linearity {name}\")", "\n\nclass LinearScheduler(object):\n    def __init__(self, iters, maxval=1.0):\n        self._iters = max(1, iters)\n        self._val = maxval / self._iters\n        self._maxval = maxval\n\n    def step(self):\n        self._val = min(self._maxval, self._val + self._maxval / self._iters)\n\n    @property\n    def val(self):\n        return self._val", ""]}
{"filename": "experiments/setups/pendulum.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom ncdssm.models import NCDSSMLTI, NCDSSMLL, NCDSSMNL\nfrom ncdssm.modules import MLP\nfrom ncdssm.models.components import AuxInferenceModel, GaussianOutput\nfrom ..utils import get_activation\n\n\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        non_linear_drift_func = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=non_linear_drift_func,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", "\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        non_linear_drift_func = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=non_linear_drift_func,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", ""]}
{"filename": "experiments/setups/mocap.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom ncdssm.models import NCDSSMLTI, NCDSSMLL, NCDSSMNL\nfrom ncdssm.modules import MLP\nfrom ncdssm.models.components import AuxInferenceModel, GaussianOutput\nfrom ..utils import get_activation\n\n\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_in_dim = config[\"y_dim\"]\n        aux_inf_base_net = MLP(\n            in_dim=aux_inf_in_dim,\n            h_dim=config[\"inference_mlp_units\"],\n            out_dim=config[\"inference_mlp_units\"],\n            nonlinearity=get_activation(config[\"inference_nonlinearity\"]),\n            last_nonlinearity=True,\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        decoder_out_dim = (\n            config[\"y_dim\"] if config[\"emission_tied_cov\"] else 2 * config[\"y_dim\"]\n        )\n\n        y_emission_net = GaussianOutput(\n            MLP(\n                in_dim=config[\"aux_dim\"],\n                h_dim=config[\"emission_mlp_units\"],\n                out_dim=decoder_out_dim,\n                nonlinearity=get_activation(config[\"emission_nonlinearity\"]),\n                n_hidden_layers=config[\"emission_hidden_layers\"],\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=config[\"emission_tied_cov\"],\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_in_dim = config[\"y_dim\"]\n        aux_inf_base_net = MLP(\n            in_dim=aux_inf_in_dim,\n            h_dim=config[\"inference_mlp_units\"],\n            out_dim=config[\"inference_mlp_units\"],\n            nonlinearity=get_activation(config[\"inference_nonlinearity\"]),\n            last_nonlinearity=True,\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        decoder_out_dim = (\n            config[\"y_dim\"] if config[\"emission_tied_cov\"] else 2 * config[\"y_dim\"]\n        )\n\n        y_emission_net = GaussianOutput(\n            MLP(\n                in_dim=config[\"aux_dim\"],\n                h_dim=config[\"emission_mlp_units\"],\n                out_dim=decoder_out_dim,\n                nonlinearity=get_activation(config[\"emission_nonlinearity\"]),\n                n_hidden_layers=config[\"emission_hidden_layers\"],\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=config[\"emission_tied_cov\"],\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_in_dim = config[\"y_dim\"]\n        aux_inf_base_net = MLP(\n            in_dim=aux_inf_in_dim,\n            h_dim=config[\"inference_mlp_units\"],\n            out_dim=config[\"inference_mlp_units\"],\n            nonlinearity=get_activation(config[\"inference_nonlinearity\"]),\n            last_nonlinearity=True,\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        decoder_out_dim = (\n            config[\"y_dim\"] if config[\"emission_tied_cov\"] else 2 * config[\"y_dim\"]\n        )\n\n        y_emission_net = GaussianOutput(\n            MLP(\n                in_dim=config[\"aux_dim\"],\n                h_dim=config[\"emission_mlp_units\"],\n                out_dim=decoder_out_dim,\n                nonlinearity=get_activation(config[\"emission_nonlinearity\"]),\n                n_hidden_layers=config[\"emission_hidden_layers\"],\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=config[\"emission_tied_cov\"],\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n        )\n\n        drift_net = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        diffusion_nets = None\n        if not config[\"fixed_diffusion\"]:\n            diffusion_nets = [\n                nn.Sequential(\n                    MLP(\n                        in_dim=1,\n                        h_dim=config[\"diffusion_mlp_units\"],\n                        out_dim=1,\n                        nonlinearity=get_activation(config[\"diffusion_nonlinearity\"]),\n                        n_hidden_layers=config[\"diffusion_hidden_layers\"],\n                        apply_spectral_norm=config[\"diffusion_spectral_norm\"],\n                    ),\n                    nn.Sigmoid(),\n                )\n                for _ in range(config[\"z_dim\"])\n            ]\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=drift_net,\n            gs=diffusion_nets,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", "\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_in_dim = config[\"y_dim\"]\n        aux_inf_base_net = MLP(\n            in_dim=aux_inf_in_dim,\n            h_dim=config[\"inference_mlp_units\"],\n            out_dim=config[\"inference_mlp_units\"],\n            nonlinearity=get_activation(config[\"inference_nonlinearity\"]),\n            last_nonlinearity=True,\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        decoder_out_dim = (\n            config[\"y_dim\"] if config[\"emission_tied_cov\"] else 2 * config[\"y_dim\"]\n        )\n\n        y_emission_net = GaussianOutput(\n            MLP(\n                in_dim=config[\"aux_dim\"],\n                h_dim=config[\"emission_mlp_units\"],\n                out_dim=decoder_out_dim,\n                nonlinearity=get_activation(config[\"emission_nonlinearity\"]),\n                n_hidden_layers=config[\"emission_hidden_layers\"],\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=config[\"emission_tied_cov\"],\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_in_dim = config[\"y_dim\"]\n        aux_inf_base_net = MLP(\n            in_dim=aux_inf_in_dim,\n            h_dim=config[\"inference_mlp_units\"],\n            out_dim=config[\"inference_mlp_units\"],\n            nonlinearity=get_activation(config[\"inference_nonlinearity\"]),\n            last_nonlinearity=True,\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        decoder_out_dim = (\n            config[\"y_dim\"] if config[\"emission_tied_cov\"] else 2 * config[\"y_dim\"]\n        )\n\n        y_emission_net = GaussianOutput(\n            MLP(\n                in_dim=config[\"aux_dim\"],\n                h_dim=config[\"emission_mlp_units\"],\n                out_dim=decoder_out_dim,\n                nonlinearity=get_activation(config[\"emission_nonlinearity\"]),\n                n_hidden_layers=config[\"emission_hidden_layers\"],\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=config[\"emission_tied_cov\"],\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_in_dim = config[\"y_dim\"]\n        aux_inf_base_net = MLP(\n            in_dim=aux_inf_in_dim,\n            h_dim=config[\"inference_mlp_units\"],\n            out_dim=config[\"inference_mlp_units\"],\n            nonlinearity=get_activation(config[\"inference_nonlinearity\"]),\n            last_nonlinearity=True,\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        decoder_out_dim = (\n            config[\"y_dim\"] if config[\"emission_tied_cov\"] else 2 * config[\"y_dim\"]\n        )\n\n        y_emission_net = GaussianOutput(\n            MLP(\n                in_dim=config[\"aux_dim\"],\n                h_dim=config[\"emission_mlp_units\"],\n                out_dim=decoder_out_dim,\n                nonlinearity=get_activation(config[\"emission_nonlinearity\"]),\n                n_hidden_layers=config[\"emission_hidden_layers\"],\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=config[\"emission_tied_cov\"],\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n        )\n\n        drift_net = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        diffusion_nets = None\n        if not config[\"fixed_diffusion\"]:\n            diffusion_nets = [\n                nn.Sequential(\n                    MLP(\n                        in_dim=1,\n                        h_dim=config[\"diffusion_mlp_units\"],\n                        out_dim=1,\n                        nonlinearity=get_activation(config[\"diffusion_nonlinearity\"]),\n                        n_hidden_layers=config[\"diffusion_hidden_layers\"],\n                        apply_spectral_norm=config[\"diffusion_spectral_norm\"],\n                    ),\n                    nn.Sigmoid(),\n                )\n                for _ in range(config[\"z_dim\"])\n            ]\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=drift_net,\n            gs=diffusion_nets,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", ""]}
{"filename": "experiments/setups/__init__.py", "chunked_list": ["from pathlib import Path\nimport numpy as np\n\nfrom ncdssm.type import Dict\n\nDATA_ROOT = Path(__file__).resolve().parent.parent.parent / \"data\"\n\n\ndef get_model(config):\n    if config[\"dataset\"] == \"mocap\" or config[\"dataset\"] == \"mocap2\":\n        from .mocap import build_model\n    elif config[\"dataset\"] == \"bouncing_ball\":\n        from .bouncing_ball import build_model\n    elif config[\"dataset\"] == \"damped_pendulum\":\n        from .pendulum import build_model\n    elif config[\"dataset\"] in {\"box\", \"pong\"}:\n        from .pymunk import build_model\n    elif config[\"dataset\"] == \"climate\":\n        from .climate import build_model\n    else:\n        raise ValueError(f\"Unknown dataset {config['dataset']}\")\n    return build_model(config=config)", "def get_model(config):\n    if config[\"dataset\"] == \"mocap\" or config[\"dataset\"] == \"mocap2\":\n        from .mocap import build_model\n    elif config[\"dataset\"] == \"bouncing_ball\":\n        from .bouncing_ball import build_model\n    elif config[\"dataset\"] == \"damped_pendulum\":\n        from .pendulum import build_model\n    elif config[\"dataset\"] in {\"box\", \"pong\"}:\n        from .pymunk import build_model\n    elif config[\"dataset\"] == \"climate\":\n        from .climate import build_model\n    else:\n        raise ValueError(f\"Unknown dataset {config['dataset']}\")\n    return build_model(config=config)", "\n\ndef get_dataset(config: Dict):\n    from ncdssm.datasets import (\n        PymunkDataset,\n        MocapDataset,\n        BouncingBallDataset,\n        DampedPendulumDataset,\n        ClimateDataset,\n    )\n\n    dataset_name = config[\"dataset\"]\n    if dataset_name == \"mocap\":\n        train_dataset = MocapDataset(\n            file_path=DATA_ROOT / \"mocap/mocap35.mat\",\n            mode=\"train\",\n            ctx_len=300,\n            pred_len=0,\n            missing_p=config[\"train_missing_p\"],\n        )\n        val_dataset = MocapDataset(\n            file_path=DATA_ROOT / \"mocap/mocap35.mat\",\n            mode=\"val\",\n            ctx_len=3,\n            pred_len=297,\n        )\n        test_dataset = MocapDataset(\n            file_path=DATA_ROOT / \"mocap/mocap35.mat\",\n            mode=\"test\",\n            ctx_len=3,\n            pred_len=297,\n        )\n    elif dataset_name == \"mocap2\":\n        train_dataset = MocapDataset(\n            file_path=DATA_ROOT / \"mocap/mocap35.mat\",\n            mode=\"train\",\n            ctx_len=200,\n            pred_len=100,\n            missing_p=config[\"train_missing_p\"],\n        )\n        val_dataset = MocapDataset(\n            file_path=DATA_ROOT / \"mocap/mocap35.mat\",\n            mode=\"val\",\n            ctx_len=100,\n            pred_len=200,\n        )\n        test_dataset = MocapDataset(\n            file_path=DATA_ROOT / \"mocap/mocap35.mat\",\n            mode=\"test\",\n            ctx_len=100,\n            pred_len=200,\n        )\n    elif dataset_name == \"bouncing_ball\":\n        train_dataset = BouncingBallDataset(\n            path=DATA_ROOT / \"bouncing_ball/rv_train.npz\",\n            ctx_len=100,\n            pred_len=200,\n            missing_p=config[\"train_missing_p\"],\n        )\n        val_dataset = BouncingBallDataset(\n            path=DATA_ROOT / \"bouncing_ball/rv_val.npz\",\n            ctx_len=100,\n            pred_len=200,\n            missing_p=config[\"train_missing_p\"],\n        )\n        test_dataset = BouncingBallDataset(\n            path=DATA_ROOT / \"bouncing_ball/rv_test.npz\",\n            ctx_len=100,\n            pred_len=200,\n            missing_p=config[\"train_missing_p\"],\n        )\n    elif dataset_name == \"damped_pendulum\":\n        train_dataset = DampedPendulumDataset(\n            path=DATA_ROOT / \"damped_pendulum/train.npz\",\n            ctx_len=50,\n            pred_len=100,\n            missing_p=config[\"train_missing_p\"],\n        )\n        val_dataset = DampedPendulumDataset(\n            path=DATA_ROOT / \"damped_pendulum/val.npz\",\n            ctx_len=50,\n            pred_len=100,\n            missing_p=config[\"train_missing_p\"],\n        )\n        test_dataset = DampedPendulumDataset(\n            path=DATA_ROOT / \"damped_pendulum/test.npz\",\n            ctx_len=50,\n            pred_len=100,\n            missing_p=config[\"train_missing_p\"],\n        )\n    elif dataset_name == \"climate\":\n        csv_path = DATA_ROOT / \"climate/climate-data-preproc.csv\"\n        fold_idx = config[\"data_fold\"]\n        train_idx = np.load(DATA_ROOT / f\"climate/fold_idx_{fold_idx}/train_idx.npy\")\n        val_idx = np.load(DATA_ROOT / f\"climate/fold_idx_{fold_idx}/val_idx.npy\")\n        test_idx = np.load(DATA_ROOT / f\"climate/fold_idx_{fold_idx}/test_idx.npy\")\n        train_dataset = ClimateDataset(\n            csv_path=csv_path,\n            train=True,\n            ids=train_idx,\n        )\n        val_dataset = ClimateDataset(\n            csv_path=csv_path,\n            train=False,\n            ids=val_idx,\n            val_options=dict(T_val=150, forecast_steps=3),\n        )\n        test_dataset = ClimateDataset(\n            csv_path=csv_path,\n            train=False,\n            ids=test_idx,\n            val_options=dict(T_val=150, forecast_steps=3),\n        )\n    elif dataset_name in {\"pong\", \"box\"}:\n        data_root = DATA_ROOT / f\"pymunk/{dataset_name}\"\n        train_dataset = PymunkDataset(\n            file_path=data_root / \"train.npz\",\n            missing_p=config[\"train_missing_p\"],\n            train=True,\n        )\n        val_dataset = PymunkDataset(\n            file_path=data_root / \"val.npz\",\n            missing_p=config[\"train_missing_p\"],\n            train=False,\n        )\n        test_dataset = PymunkDataset(\n            file_path=data_root / \"test.npz\",\n            missing_p=config[\"train_missing_p\"],\n            train=False,\n        )\n    else:\n        raise ValueError(f\"Unknown dataset {dataset_name}!\")\n    return train_dataset, val_dataset, test_dataset", ""]}
{"filename": "experiments/setups/pymunk.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom ncdssm.models import NCDSSMLTI, NCDSSMLL, NCDSSMNL\nfrom ncdssm.modules import MLP, ImageEncoder, ImageDecoder, MergeLastDims\nfrom ncdssm.models.components import AuxInferenceModel, GaussianOutput, BernoulliOutput\nfrom ..utils import get_activation\n\n\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_base_net = ImageEncoder(\n            img_size=config[\"img_size\"],\n            channels=1,\n            out_dim=config[\"inference_img_enc_dim\"],\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = BernoulliOutput(\n            nn.Sequential(\n                ImageDecoder(\n                    in_dim=config[\"aux_dim\"],\n                    img_size=config[\"img_size\"],\n                    channels=1,\n                ),\n                MergeLastDims(ndims=3),\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_indepedent=False,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_base_net = ImageEncoder(\n            img_size=config[\"img_size\"],\n            channels=1,\n            out_dim=config[\"inference_img_enc_dim\"],\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = BernoulliOutput(\n            nn.Sequential(\n                ImageDecoder(\n                    in_dim=config[\"aux_dim\"],\n                    img_size=config[\"img_size\"],\n                    channels=1,\n                ),\n                MergeLastDims(ndims=3),\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_indepedent=False,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_base_net = ImageEncoder(\n            img_size=config[\"img_size\"],\n            channels=1,\n            out_dim=config[\"inference_img_enc_dim\"],\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = BernoulliOutput(\n            nn.Sequential(\n                ImageDecoder(\n                    in_dim=config[\"aux_dim\"],\n                    img_size=config[\"img_size\"],\n                    channels=1,\n                ),\n                MergeLastDims(ndims=3),\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_indepedent=False,\n        )\n\n        drift_net = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        diffusion_nets = None\n        if not config[\"fixed_diffusion\"]:\n            diffusion_nets = [\n                nn.Sequential(\n                    MLP(\n                        in_dim=1,\n                        h_dim=config[\"diffusion_mlp_units\"],\n                        out_dim=1,\n                        nonlinearity=get_activation(config[\"diffusion_nonlinearity\"]),\n                        n_hidden_layers=config[\"diffusion_hidden_layers\"],\n                        apply_spectral_norm=config[\"diffusion_spectral_norm\"],\n                    ),\n                    nn.Sigmoid(),\n                )\n                for _ in range(config[\"z_dim\"])\n            ]\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=drift_net,\n            gs=diffusion_nets,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", "\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_base_net = ImageEncoder(\n            img_size=config[\"img_size\"],\n            channels=1,\n            out_dim=config[\"inference_img_enc_dim\"],\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = BernoulliOutput(\n            nn.Sequential(\n                ImageDecoder(\n                    in_dim=config[\"aux_dim\"],\n                    img_size=config[\"img_size\"],\n                    channels=1,\n                ),\n                MergeLastDims(ndims=3),\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_indepedent=False,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_base_net = ImageEncoder(\n            img_size=config[\"img_size\"],\n            channels=1,\n            out_dim=config[\"inference_img_enc_dim\"],\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = BernoulliOutput(\n            nn.Sequential(\n                ImageDecoder(\n                    in_dim=config[\"aux_dim\"],\n                    img_size=config[\"img_size\"],\n                    channels=1,\n                ),\n                MergeLastDims(ndims=3),\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_indepedent=False,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_base_net = ImageEncoder(\n            img_size=config[\"img_size\"],\n            channels=1,\n            out_dim=config[\"inference_img_enc_dim\"],\n        )\n        inf_out_dim = (\n            config[\"aux_dim\"] if config[\"inference_tied_cov\"] else 2 * config[\"aux_dim\"]\n        )\n        aux_inf_dist_net = GaussianOutput(\n            nn.Linear(aux_inf_base_net.out_dim, inf_out_dim),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=config[\"inference_tied_cov\"],\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = BernoulliOutput(\n            nn.Sequential(\n                ImageDecoder(\n                    in_dim=config[\"aux_dim\"],\n                    img_size=config[\"img_size\"],\n                    channels=1,\n                ),\n                MergeLastDims(ndims=3),\n            ),\n            dist_dim=config[\"y_dim\"],\n            use_indepedent=False,\n        )\n\n        drift_net = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        diffusion_nets = None\n        if not config[\"fixed_diffusion\"]:\n            diffusion_nets = [\n                nn.Sequential(\n                    MLP(\n                        in_dim=1,\n                        h_dim=config[\"diffusion_mlp_units\"],\n                        out_dim=1,\n                        nonlinearity=get_activation(config[\"diffusion_nonlinearity\"]),\n                        n_hidden_layers=config[\"diffusion_hidden_layers\"],\n                        apply_spectral_norm=config[\"diffusion_spectral_norm\"],\n                    ),\n                    nn.Sigmoid(),\n                )\n                for _ in range(config[\"z_dim\"])\n            ]\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=drift_net,\n            gs=diffusion_nets,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", ""]}
{"filename": "experiments/setups/climate.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom ncdssm.models import NCDSSMLTI, NCDSSMLL, NCDSSMNL\nfrom ncdssm.modules import MLP\nfrom ncdssm.models.components import AuxInferenceModel, GaussianOutput\nfrom ..utils import get_activation\n\n\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n            sigma=1e-4,\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n            sigma=1e-4,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n            sporadic=True,\n        )\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n            sigma=1e-4,\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n            sigma=1e-4,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n            sporadic=True,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n            sigma=1e-4,\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n            sigma=1e-4,\n        )\n\n        non_linear_drift_func = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=non_linear_drift_func,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n            sporadic=True,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", "\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n            sigma=1e-4,\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n            sigma=1e-4,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n            sporadic=True,\n        )\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n            sigma=1e-4,\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n            sigma=1e-4,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n            sporadic=True,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n            sigma=1e-4,\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"emission_trainable_cov\"],\n            use_independent=False,\n            sigma=1e-4,\n        )\n\n        non_linear_drift_func = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=non_linear_drift_func,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n            sporadic=True,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", ""]}
{"filename": "experiments/setups/bouncing_ball.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom ncdssm.models import NCDSSMLTI, NCDSSMLL, NCDSSMNL\nfrom ncdssm.modules import MLP\nfrom ncdssm.models.components import AuxInferenceModel, GaussianOutput\nfrom ..utils import get_activation\n\n\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        non_linear_drift_func = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=non_linear_drift_func,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", "\ndef build_model(config):\n    if config[\"model\"] == \"NCDSSMLTI\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLTI(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMLL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        alpha_net = nn.Sequential(\n            MLP(\n                in_dim=config[\"z_dim\"],\n                h_dim=config[\"alpha_mlp_units\"],\n                out_dim=config[\"K\"],\n                nonlinearity=get_activation(config[\"alpha_nonlinearity\"]),\n                n_hidden_layers=config[\"alpha_hidden_layers\"],\n            ),\n            nn.Softmax(dim=-1),\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n\n        model = NCDSSMLL(\n            aux_inference_net,\n            y_emission_net,\n            K=config[\"K\"],\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            alpha_net=alpha_net,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    elif config[\"model\"] == \"NCDSSMNL\":\n        aux_inf_base_net = nn.Identity()\n\n        aux_inf_dist_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"aux_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=config[\"inference_trainable_cov\"],\n        )\n        aux_inference_net = AuxInferenceModel(\n            aux_inf_base_net,\n            aux_inf_dist_net,\n            aux_dim=config[\"aux_dim\"],\n            concat_mask=False,\n        )\n\n        y_emission_net = GaussianOutput(\n            nn.Identity(),\n            dist_dim=config[\"y_dim\"],\n            use_tied_cov=True,\n            use_trainable_cov=True,\n            use_independent=False,\n        )\n\n        non_linear_drift_func = MLP(\n            in_dim=config[\"z_dim\"],\n            h_dim=config[\"drift_mlp_units\"],\n            out_dim=config[\"z_dim\"],\n            nonlinearity=get_activation(config[\"drift_nonlinearity\"]),\n            last_nonlinearity=config[\"drift_last_nonlinearity\"],\n            n_hidden_layers=config[\"drift_hidden_layers\"],\n            zero_init_last=config[\"drift_zero_init_last\"],\n            apply_spectral_norm=config[\"drift_spectral_norm\"],\n        )\n\n        H = torch.eye(config[\"aux_dim\"], config[\"z_dim\"]) if config[\"fixed_H\"] else None\n        model = NCDSSMNL(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim=config[\"aux_dim\"],\n            z_dim=config[\"z_dim\"],\n            y_dim=config[\"y_dim\"],\n            u_dim=config[\"u_dim\"],\n            f=non_linear_drift_func,\n            integration_step_size=config[\"integration_step_size\"],\n            integration_method=config[\"integration_method\"],\n            H=H,\n        )\n    else:\n        raise ValueError(f'Unknown model {config[\"model\"]}')\n    return model", ""]}
{"filename": "src/ncdssm/torch_utils.py", "chunked_list": ["import torch\nimport numpy as np\n\nfrom .functions import bm_t\nfrom .type import Tensor\n\n\ndef merge_leading_dims(tensor, ndims=1):\n    assert ndims <= tensor.ndim\n    shape = tensor.size()\n    lead_dim_size = np.prod(shape[:ndims])\n    tensor = tensor.view(lead_dim_size, *shape[ndims:])\n    return tensor", "\n\ndef torch2numpy(tensor):\n    return tensor.data.cpu().numpy()\n\n\ndef grad_norm(parameters):\n    parameters = [p for p in parameters if p.grad is not None]\n    device = parameters[0].grad.device\n    total_norm = torch.norm(\n        torch.stack([torch.norm(p.grad.detach(), 2).to(device) for p in parameters]), 2\n    )\n    return total_norm", "\n\ndef prepend_time_zero(times: Tensor, target: Tensor, mask: Tensor):\n    \"\"\"Prepends t = 0 to the batch\n\n    Args:\n        times (Tensor): 1D Tensor of times without t = 0 as the first time\n        target (Tensor): B x T x D Tensor\n        mask (Tensor): Mask\n    \"\"\"\n    B, _, D = target.size()\n    times = torch.cat(\n        [torch.zeros([1], device=times.device), times],\n        dim=0,\n    )\n    target = torch.cat(\n        [\n            torch.zeros((B, 1, D), device=target.device),\n            target,\n        ],\n        dim=1,\n    )\n    if mask.ndim == 3:\n        mask = torch.cat([torch.zeros((B, 1, D), device=mask.device), mask], dim=1)\n    else:\n        mask = torch.cat([torch.zeros((B, 1), device=mask.device), mask], dim=1)\n    return times, target, mask", "\n\ndef skew_symmetric_init_(tensor, gain=1):\n    if tensor.ndim not in {2, 3}:\n        raise ValueError(\"Only tensors with 2 or 3 dimensions are supported\")\n\n    rand_matrix = torch.rand_like(tensor)\n    if tensor.ndim == 2:\n        rand_matrix.unsqueeze_(0)\n\n    skew_symm_matrix = (rand_matrix - bm_t(rand_matrix)) / 2\n\n    with torch.no_grad():\n        tensor.view_as(skew_symm_matrix).copy_(skew_symm_matrix)\n        tensor.mul_(gain)\n    return tensor", ""]}
{"filename": "src/ncdssm/evaluation.py", "chunked_list": ["import ot\nimport torch\nimport scipy\nimport numpy as np\nimport multiprocessing\n\nfrom tqdm.auto import tqdm\n\nfrom .torch_utils import torch2numpy\n", "from .torch_utils import torch2numpy\n\n\ndef student_t_conf_interval(samples, confidence_level=0.95, axis=0):\n    deg_free = samples.shape[axis] - 1\n    mean = np.mean(samples, axis=axis)\n    str_err = scipy.stats.sem(samples, axis=axis)\n    a, b = scipy.stats.t.interval(confidence_level, deg_free, mean, str_err)\n    return mean, a, b\n", "\n\ndef compute_wasserstein_distance(img_gt, img_model, metric=\"euclidean\"):\n    assert img_gt.ndim == img_model.ndim == 2\n\n    # get positions in x-y-plane for pixels that\n    # take value \"1\" (interpreted as our samples).\n    pos_gt = np.stack(np.where(img_gt == 1), axis=-1)\n    pos_model = np.stack(np.where(img_model == 1), axis=-1)\n\n    # assume that the binary distribution over\n    # pixel value taking value 1 at x-y position\n    # is the uniform empirical distribution.\n    prob_gt = ot.unif(len(pos_gt))\n    prob_model = ot.unif(len(pos_model))\n\n    # euclidean distance times number of pixels\n    #  --> *total* (not avg) number of pixel movements.\n    M = ot.dist(pos_gt, pos_model, metric=metric)\n    dist_avg = ot.emd2(prob_gt, prob_model, M)\n    # dist_total = dist_avg * len(pos_gt)\n    return dist_avg", "\n\ndef _wasserstein_worker_function(data):\n    orig_data, pred_data = data\n    assert orig_data.ndim == 2\n    assert pred_data.ndim == 3\n    assert orig_data.shape[0] == pred_data.shape[1]\n    N, T, _ = pred_data.shape\n    w_dist = np.zeros((N, T))\n    for t in range(T):\n        for n in range(N):\n            gt_img = orig_data[t]\n            pred_img = pred_data[n, t]\n            wass_dist = compute_wasserstein_distance(\n                gt_img.reshape(32, 32), pred_img.reshape(32, 32)\n            )\n            w_dist[n, t] = wass_dist\n    return w_dist", "\n\ndef evaluate_pymunk_dataset(\n    dataloader,\n    model,\n    device=torch.device(\"cpu\"),\n    num_samples: int = 80,\n    max_size: int = np.inf,  # type: ignore\n    no_state_sampling: bool = False,\n    use_smooth: bool = False,\n):\n    def _binarize_image(img):\n        img[img < 0.5] = 0.0\n        img[img >= 0.5] = 1.0\n        return img\n\n    wass_dists = []\n    size = 0\n    for test_batch in tqdm(dataloader, desc=\"Evaluating\"):\n        past_target = test_batch[\"past_target\"].to(device)\n        mask = test_batch[\"past_mask\"].to(device)\n        future_target = test_batch[\"future_target\"].to(device)\n        past_times = test_batch[\"past_times\"].to(device)\n        future_times = test_batch[\"future_times\"].to(device)\n        predict_result = model.forecast(\n            past_target,\n            mask,\n            past_times.view(-1),\n            future_times.view(-1),\n            num_samples=num_samples,\n            no_state_sampling=no_state_sampling,\n            use_smooth=use_smooth,\n        )\n        reconstruction = predict_result[\"reconstruction\"]\n        forecast = predict_result[\"forecast\"]\n        full_prediction = torch.cat([reconstruction, forecast], dim=-2)\n        full_target = torch.cat([past_target, future_target], dim=1)\n\n        full_prediction = torch2numpy(full_prediction)\n        full_target = torch2numpy(full_target)\n\n        full_prediction = np.swapaxes(full_prediction, 0, 1)  # Batch first\n\n        batch_wass_dist = []\n        mp_pool = multiprocessing.Pool(\n            initializer=None, processes=multiprocessing.cpu_count()\n        )\n        batch_wass_dist = mp_pool.map(\n            func=_wasserstein_worker_function,\n            iterable=zip(full_target, full_prediction),\n        )\n        mp_pool.close()\n        mp_pool.join()\n        wass_dists.append(np.array(batch_wass_dist))\n        size += past_target.shape[0]\n        if size >= max_size:\n            break\n    # Concate on batch dim\n    wass_dists: np.ndarray = np.concatenate(wass_dists, axis=0)  # shape = B, N, T\n    # Average over the batch\n    batch_wass_dists = wass_dists.mean(0)\n    wt_mean, _, wt_conf_interval = student_t_conf_interval(\n        batch_wass_dists, confidence_level=0.95, axis=0\n    )  # Used for plotting\n    future_w_mean, _, future_w_conf_interval = student_t_conf_interval(\n        batch_wass_dists[:, past_target.size(1) :].mean(axis=-1),\n        confidence_level=0.95,\n        axis=0,\n    )\n    return dict(\n        wt_mean=wt_mean,\n        wt_conf_interval=wt_conf_interval - wt_mean,\n        future_w_mean=future_w_mean,\n        future_w_conf_interval=future_w_conf_interval - future_w_mean,\n    )", "\n\ndef evaluate_simple_ts(\n    dataloader,\n    model,\n    device=torch.device(\"cpu\"),\n    num_samples=50,\n    no_state_sampling=False,\n    use_smooth=False,\n    return_states=False,\n):\n    imputation_sq_errs = []\n    forecast_sq_errs = []\n    mses_mean_forecast = []\n    mses_mean_imputation = []\n    mask_sum = 0.0\n    states = []\n    for test_batch in tqdm(dataloader, desc=\"Evaluating\"):\n        past_target = test_batch[\"past_target\"].to(device)\n        B, T, F = past_target.shape\n        mask = test_batch[\"past_mask\"].to(device)\n        future_target = test_batch[\"future_target\"].to(device)\n        past_times = test_batch[\"past_times\"].to(device)\n        future_times = test_batch[\"future_times\"].to(device)\n\n        predict_result = model.forecast(\n            past_target,\n            mask,\n            past_times.view(-1),\n            future_times.view(-1),\n            num_samples=num_samples,\n            no_state_sampling=no_state_sampling,\n            use_smooth=use_smooth,\n        )\n        reconstruction = predict_result[\"reconstruction\"]\n        forecast = predict_result[\"forecast\"]\n        if return_states:\n            states.append(\n                torch.cat(\n                    [predict_result[\"z_reconstruction\"], predict_result[\"z_forecast\"]],\n                    dim=-2,\n                )\n            )\n        # Compute MSE using samples\n        rec_sq_err = (past_target[None] - reconstruction) ** 2\n        rec_sq_err = torch.einsum(\"sbtf, bt -> sbtf\", rec_sq_err, 1 - mask)\n        mask_sum += torch.sum(1 - mask, dim=(-1, -2))\n        imputation_sq_errs.append(rec_sq_err)\n        forecast_sq_err = (future_target[None] - forecast) ** 2\n        forecast_sq_errs.append(forecast_sq_err)\n        # Compute MSE using mean forecast\n        mean_rec = reconstruction.mean(0)\n        batch_mse_mean_rec = torch.einsum(\n            \"btf, bt -> b\", (past_target - mean_rec) ** 2, 1 - mask\n        ) / (torch.sum(1 - mask, dim=-1) * F)\n        mses_mean_imputation.append(batch_mse_mean_rec)\n        mean_forecast = forecast.mean(0)\n        batch_mse_mean_forecast = torch.mean(\n            (future_target - mean_forecast) ** 2, (1, 2)\n        )\n        mses_mean_forecast.append(batch_mse_mean_forecast)\n    imputation_sq_errs = torch.cat(imputation_sq_errs, 1)\n    imputation_msq_errs = (\n        (torch.sum(imputation_sq_errs, (1, 2, 3)) / (mask_sum * F))\n        .detach()\n        .cpu()\n        .numpy()\n    )\n    imputation_mean_mse, _, imputation_conf_interval = student_t_conf_interval(\n        imputation_msq_errs, confidence_level=0.95, axis=0\n    )\n\n    forecast_sq_errs = torch.cat(forecast_sq_errs, 1)\n    forecast_msq_errs = torch.mean(forecast_sq_errs, (1, 2, 3)).detach().cpu().numpy()\n    forecast_mean_mse, _, forecast_conf_interval = student_t_conf_interval(\n        forecast_msq_errs, confidence_level=0.95, axis=0\n    )\n\n    mse_mean_imputation = torch.cat(mses_mean_imputation, 0).mean(0)\n    mse_mean_forecast = torch.cat(mses_mean_forecast, 0).mean(0)\n    imputation_delta_ci = imputation_conf_interval - imputation_mean_mse\n    forecast_delta_ci = forecast_conf_interval - forecast_mean_mse\n    print(\n        f\"Imputation MSE: {imputation_mean_mse.item():.5f} +/- {imputation_delta_ci:.5f}\"  # noqa\n    )\n    print(f\"Imputation MSE (of mean rec): {mse_mean_imputation.item():.5f}\")\n    print(f\"MSE: {forecast_mean_mse.item():.5f} +/- {forecast_delta_ci:.5f}\")\n    print(f\"MSE (of mean forecast): {mse_mean_forecast.item():.5f}\")\n\n    extra_return = {}\n    if return_states:\n        states = torch.cat(states, dim=1)\n        extra_return = {\"states\": states}\n    return dict(\n        imputation_mse=imputation_mean_mse.item(),\n        mse_imputation_rec=mse_mean_imputation.item(),\n        imputation_conf_interval=imputation_delta_ci.item(),\n        forecast_mse=forecast_mean_mse.item(),\n        mse_mean_forecast=mse_mean_forecast.item(),\n        forecast_conf_interval=forecast_delta_ci.item(),\n        **extra_return,\n    )", "\n\ndef evaluate_sporadic(\n    dataloader,\n    model,\n    device=torch.device(\"cpu\"),\n    num_samples=50,\n    no_state_sampling=False,\n    use_smooth=False,\n):\n    sq_err_sum = 0\n    mask_sum = 0\n    forecast_sq_errs = []\n    mses_mean_forecast = []\n    for test_batch in tqdm(dataloader, desc=\"Evaluating\"):\n        past_target = test_batch[\"past_target\"].to(device)\n        B, T, F = past_target.shape\n        mask = test_batch[\"past_mask\"].to(device)\n        future_target = test_batch[\"future_target\"].to(device)\n        past_times = test_batch[\"past_times\"].to(device)\n        future_times = test_batch[\"future_times\"].to(device)\n        future_mask = test_batch[\"future_mask\"].to(device)\n\n        predict_result = model.forecast(\n            past_target,\n            mask,\n            past_times.view(-1),\n            future_times.view(-1),\n            num_samples=num_samples,\n            no_state_sampling=no_state_sampling,\n            use_smooth=use_smooth,\n        )\n        forecast = predict_result[\"forecast\"]\n        # Compute MSE using samples\n\n        forecast_sq_err = (future_target[None] - forecast) ** 2\n        forecast_sq_err = torch.einsum(\n            \"sbtf, btf -> sb\", forecast_sq_err, future_mask\n        ) / (torch.sum(future_mask, dim=(-1, -2)))\n        forecast_sq_errs.append(forecast_sq_err)\n        # Compute MSE using mean forecast\n\n        mean_forecast = forecast.mean(0)\n        batch_mse_mean_forecast = torch.einsum(\n            \"btf, btf -> b\", (future_target - mean_forecast) ** 2, future_mask\n        ) / (torch.sum(future_mask, dim=(-1, -2)))\n        mses_mean_forecast.append(batch_mse_mean_forecast)\n        sq_err_sum += torch.einsum(\n            \"btf, btf -> \", (future_target - mean_forecast) ** 2, future_mask\n        )\n        mask_sum += torch.sum(future_mask)\n\n    forecast_sq_errs = torch.cat(forecast_sq_errs, 1)\n    forecast_msq_errs = torch.mean(forecast_sq_errs, 1).detach().cpu().numpy()\n    forecast_mean_mse, _, forecast_conf_interval = student_t_conf_interval(\n        forecast_msq_errs, confidence_level=0.95, axis=0\n    )\n\n    mse_mean_forecast = torch.cat(mses_mean_forecast, 0).mean(0)\n    delta_ci = forecast_conf_interval - forecast_mean_mse\n    mse_gru_ode_style = sq_err_sum / mask_sum\n    print(f\"MSE (as computed in GRUODE-B): {mse_gru_ode_style:5f}\")\n\n    return dict(\n        forecast_mse=forecast_mean_mse.item(),\n        mse_mean_forecast=mse_mean_forecast.item(),\n        forecast_conf_interval=delta_ci.item(),\n        mse_gru_ode_style=mse_gru_ode_style.item(),\n    )", ""]}
{"filename": "src/ncdssm/functions.py", "chunked_list": ["import torch\nimport warnings\nfrom .type import Tensor, Tuple\n\n\ndef inverse_softplus(tensor: Tensor):\n    return tensor + torch.log(1 - torch.exp(-tensor))\n\n\ndef bm_t(tensor: Tensor):\n    \"\"\"Batched matrix transpose. Swaps the last two axes.\n\n    Args:\n        tensor (torch.Tensor): N-D tensor.\n\n    Returns:\n        torch.Tensor: tensor with last two dims swapped.\n    \"\"\"\n    return torch.transpose(tensor, -2, -1)", "\ndef bm_t(tensor: Tensor):\n    \"\"\"Batched matrix transpose. Swaps the last two axes.\n\n    Args:\n        tensor (torch.Tensor): N-D tensor.\n\n    Returns:\n        torch.Tensor: tensor with last two dims swapped.\n    \"\"\"\n    return torch.transpose(tensor, -2, -1)", "\n\ndef mbvp(matrix: Tensor, batched_vectors: Tensor):\n    \"\"\"Compute matrix-batched vector product.\n\n    Args:\n        matrix (Tensor): 2D tensor.\n        batched_vectors (Tensor): 2D tensor.\n\n    Returns:\n        Tensor: The matrix-batched vector product.\n    \"\"\"\n    assert (\n        matrix.ndim == 2 and batched_vectors.ndim == 2\n    ), \"matrix and batched_vectors should be 2D tensors!\"\n    m, n = matrix.size()\n    b, d = batched_vectors.size()\n    assert n == d, f\"dim 2 of matrix ({n}) should match dim 2 of batched_vectors ({d})!\"\n    batched_matrix = matrix.unsqueeze(0)\n    return bmbvp(batched_matrix, batched_vectors)", "\n\ndef bmbvp(batched_matrix: Tensor, batched_vectors: Tensor):\n    \"\"\"Compute batched matrix-batched vector product.\n\n    Args:\n        batched_matrix (Tensor): 3D tensor.\n        batched_vectors (Tensor): 2D tensor.\n\n    Returns:\n        Tensor: The batched matrix-batched vector product.\n    \"\"\"\n    assert batched_matrix.ndim == 3 and batched_vectors.ndim == 2, (\n        \"batched_matrix and batched_vectors should \"\n        \"be 3D and 2D tensors, respectively!\"\n    )\n    bm, m, n = batched_matrix.size()\n    bv, d = batched_vectors.size()\n    assert bm == bv or bm == 1 or bv == 1, (\n        f\"Matrix ({bm}) and vector ({bv}) batch dim should match \" \"or be singletons!\"\n    )\n    assert n == d, (\n        f\"dim 3 of matrix ({n}) should match dim 2 \" f\"of batched_vectors ({d})!\"\n    )\n    batched_vectors = batched_vectors.unsqueeze(-1)\n    output = torch.matmul(batched_matrix, batched_vectors)\n    output = output.squeeze(-1)\n    return output", "\n\ndef linear_interpolation(t0, x0, t1, x1, t):\n    assert t0 <= t <= t1, f\"Incorrect time order: t0={t0}, t={t}, t1={t1}!\"\n    x = (t1 - t) / (t1 - t0) * x0 + (t - t0) / (t1 - t0) * x1\n    return x\n\n\ndef symmetrize(matrix: Tensor) -> Tensor:\n    \"\"\"Symmetrize a matrix A by (A + A^T) / 2.\n\n    Args:\n        matrix (Tensor): An N-D tensor with N > 2.\n\n    Returns:\n        Tensor: The symmetrized matrices.\n    \"\"\"\n    return 0.5 * (matrix + matrix.transpose(-1, -2))", "def symmetrize(matrix: Tensor) -> Tensor:\n    \"\"\"Symmetrize a matrix A by (A + A^T) / 2.\n\n    Args:\n        matrix (Tensor): An N-D tensor with N > 2.\n\n    Returns:\n        Tensor: The symmetrized matrices.\n    \"\"\"\n    return 0.5 * (matrix + matrix.transpose(-1, -2))", "\n\ndef regularize_matrix(\n    batched_matrix, regularization=1e-6, relative_regularization=True\n):\n    batch_size = batched_matrix.size(0)\n    batched_identity = (\n        torch.eye(\n            batched_matrix.size(-1),\n            dtype=batched_matrix.dtype,\n            device=batched_matrix.device,\n        )\n        .unsqueeze(0)\n        .repeat(batch_size, 1, 1)\n    )\n\n    if relative_regularization:\n        diag_mean = (\n            torch.einsum(\"jii->ji\", batched_matrix)\n            .mean(dim=-1)\n            .view(batch_size, 1, 1)\n            .detach()\n        )\n        regularized_matrix = (\n            batched_matrix + regularization * diag_mean * batched_identity\n        )\n    else:\n        regularized_matrix = batched_matrix + regularization * batched_identity\n    return regularized_matrix", "\n\ndef make_positive_definite(\n    batched_matrix,\n    regularization=1e-4,\n    max_regularization=1,\n    step_factor=10,\n    warn=True,\n    relative_regularization=True,\n):\n    assert batched_matrix.ndim == 3\n    if regularization > max_regularization:\n        raise ValueError(\n            \"Attempted to regularize beyond max_regularization:\" f\"{max_regularization}\"\n        )\n    if warn:\n        warnings.warn(f\"Regularizing matrix with factor: {regularization}!\")\n    regularized_matrix = regularize_matrix(\n        batched_matrix,\n        regularization=regularization,\n        relative_regularization=relative_regularization,\n    )\n    is_pd = torch.all(torch.linalg.cholesky_ex(regularized_matrix).info.eq(0))\n    if is_pd:\n        return regularized_matrix\n    else:\n        return make_positive_definite(\n            batched_matrix,\n            regularization=regularization * step_factor,\n            max_regularization=max_regularization,\n            step_factor=step_factor,\n            warn=warn,\n            relative_regularization=relative_regularization,\n        )", "\n\ndef cholesky(batched_matrix: Tensor):\n    \"\"\"Cholesky decomposition which attempts to regularize the\n    matrix diagonal with some salt if the decomposition fails.\n\n    Args:\n        batched_matrix (Tensor): A 3-D tensor.\n\n    Returns:\n        Tensor: The cholesky factor.\n    \"\"\"\n    try:\n        cholesky_factor = torch.linalg.cholesky(batched_matrix)\n    except RuntimeError:\n        regularized_matrix = make_positive_definite(batched_matrix)\n        cholesky_factor = torch.linalg.cholesky(regularized_matrix)\n    return cholesky_factor", "\n\ndef qr(batched_matrix, positive_diag=True) -> Tuple[Tensor, Tensor]:\n    \"\"\"QR decomposition with the option of making the diagonal elements\n    of R positive.\n\n    Args:\n        batched_matrix (Tensor): An N-D tensor with N > 2.\n        positive_diag (bool, optional): Whether to make the diagonal elements\n            of R positive. Defaults to True.\n\n    Returns:\n        Tuple[Tensor, Tensor]: The Q and R.\n    \"\"\"\n    Q, R = torch.linalg.qr(batched_matrix)\n    if positive_diag:\n        # The QR decomposition returned by pytorch does not guarantee\n        # that the matrix R has positive diagonal elements.\n        # To ensure that, we first construct a signature matrix S\n        # which is a diagonal matrix with diagonal elements +1 or -1\n        # depending on the sign of the corresponding diagonal element in R.\n        # If we premultiply R with S, we ensure that the diagonal is positive;\n        # however, the corresponding Q needs to be changed now to ensure that\n        # that we get back the original matrix when we multiply the new Q and\n        # the new R. This can be done by postmultiplying Q with S because\n        # then we get (Q @ S) @ (S @ R) = Q @ R as S @ S = I, where (Q @ S)\n        # and (S @ R) are the new Q and new R respectively.\n        R_diag_sign = torch.sgn(torch.diagonal(R, dim1=-2, dim2=-1))\n        S = torch.diag_embed(R_diag_sign, dim1=-2, dim2=-1)\n        Q = Q @ S\n        R = S @ R\n    return Q, R", "\n\ndef bm_diag_positive(A):\n    dim = A.size(-1)\n    diag = torch.abs(torch.diagonal(A, dim1=-2, dim2=-1))\n    mask = torch.eye(dim, device=A.device)[None]\n    A = mask * torch.diag_embed(diag) + (1 - mask) * A\n    return A\n\n\ndef sum_mat_sqrts(sqrt_A, sqrt_B):\n    tmp = bm_t(torch.cat([sqrt_A, sqrt_B], dim=-1))\n    _, Rtmp = qr(tmp)\n    return bm_t(Rtmp)", "\n\ndef sum_mat_sqrts(sqrt_A, sqrt_B):\n    tmp = bm_t(torch.cat([sqrt_A, sqrt_B], dim=-1))\n    _, Rtmp = qr(tmp)\n    return bm_t(Rtmp)\n\n\ndef batch_jacobian(func, x, create_graph=False, vectorize=False):\n    def _func_sum(x):\n        return func(x).sum(dim=0)\n\n    return torch.autograd.functional.jacobian(\n        _func_sum, x, create_graph=create_graph, vectorize=vectorize\n    ).permute(1, 0, 2)", "def batch_jacobian(func, x, create_graph=False, vectorize=False):\n    def _func_sum(x):\n        return func(x).sum(dim=0)\n\n    return torch.autograd.functional.jacobian(\n        _func_sum, x, create_graph=create_graph, vectorize=vectorize\n    ).permute(1, 0, 2)\n"]}
{"filename": "src/ncdssm/inference.py", "chunked_list": ["import math\nimport torch\n\nfrom torchdiffeq._impl.rk_common import rk4_alt_step_func as rk4_step_func\n\nfrom .type import Tensor, Union\nfrom .functions import cholesky, mbvp, bm_t, bmbvp, qr, symmetrize, sum_mat_sqrts\nfrom .models.dynamics import (\n    ContinuousLTI,\n    ContinuousNL,", "    ContinuousLTI,\n    ContinuousNL,\n    ContinuousLL,\n)\n\n\ndef analytic_linear_step(mu: Tensor, LSigma: Tensor, t0, t1, dynamics: ContinuousLTI):\n    # Analytic step for homogenous linear SDE using matrix exponentials\n    # Based on Sarkka and Solin, Section 6.2 & 6.3\n    batch_size = mu.size(0)\n    z_dim = dynamics.z_dim\n    F = dynamics.F\n    Q = dynamics.Q\n\n    mexp_F_t1mt0 = torch.matrix_exp(F * (t1 - t0))\n    mu_pred = mbvp(mexp_F_t1mt0, mu)\n    C_t0 = LSigma @ bm_t(LSigma)\n    D_t0 = torch.eye(z_dim, device=C_t0.device).repeat(batch_size, 1, 1)\n    CD_t0 = torch.cat([C_t0, D_t0], dim=1)\n    tmp = torch.zeros(2 * z_dim, 2 * z_dim, device=C_t0.device)\n    tmp[:z_dim, :z_dim] = F\n    tmp[:z_dim, z_dim:] = Q\n    tmp[z_dim:, z_dim:] = -F.T\n    mexp_tmp_t1mt0 = torch.matrix_exp(tmp * (t1 - t0))\n\n    CD_t1 = mexp_tmp_t1mt0[None] @ CD_t0\n\n    C_t1 = CD_t1[:, :z_dim]\n    D_t1 = CD_t1[:, z_dim:]\n\n    Sigma_pred = C_t1 @ torch.inverse(D_t1)\n    LSigma_pred = cholesky(symmetrize(Sigma_pred))\n\n    return mu_pred, LSigma_pred", "\n\ndef linear_step(\n    mu: Tensor,\n    Phi: Tensor,\n    sqrt_Phi_sum: Tensor,\n    tn,\n    h,\n    dynamics: ContinuousLTI,\n    method: str = \"rk4\",\n):\n    assert method in {\"euler\", \"rk4\"}, f\"Unknown solver: {method}!\"\n    F = dynamics.F\n    LQ = cholesky(dynamics.Q[None])\n    batched_F = F[None]\n\n    def _mu_rate_func(t, z, **unused_kwargs):\n        dz_by_dt = mbvp(F, z)\n        return dz_by_dt\n\n    def _Phi_rate_func(t, z, **unused_kwargs):\n        return batched_F @ z\n\n    if method == \"euler\":\n        mu_next = mu + h * _mu_rate_func(tn, mu)\n        Phi_next = Phi + h * _Phi_rate_func(tn, Phi)\n    elif method == \"rk4\":\n        mu_next = mu + rk4_step_func(_mu_rate_func, tn, h, tn + h, mu)\n        Phi_next = Phi + rk4_step_func(_Phi_rate_func, tn, h, tn + h, Phi)\n\n    Rtmp = sum_mat_sqrts(Phi @ LQ, Phi_next @ LQ)\n    Rtmp = math.sqrt(h / 2) * Rtmp\n    Rtmp = sum_mat_sqrts(Rtmp, sqrt_Phi_sum)\n    sqrt_Phi_sum = Rtmp\n\n    mu = mu_next\n    Phi = Phi_next\n    return mu, Phi, sqrt_Phi_sum", "\n\ndef cont_disc_linear_predict(\n    mu: Tensor,\n    LSigma: Tensor,\n    dynamics: ContinuousLTI,\n    t0: float,\n    t1: float,\n    step_size: float,\n    method: str = \"rk4\",\n    cache_params: bool = False,\n    min_step_size: float = 1e-5,\n):\n    batch_size = mu.size(0)\n    z_dim = dynamics.z_dim\n    t = t0\n    mu_pred = mu\n\n    if method == \"matrix_exp\":\n        return *analytic_linear_step(mu, LSigma, t0, t1, dynamics), ([], [], [])\n\n    Phi = torch.eye(z_dim, device=mu.device).repeat(batch_size, 1, 1)\n    sqrt_Phi_sum = torch.zeros(batch_size, z_dim, z_dim, device=mu.device)\n    cached_mus = []\n    cached_LSigmas = []\n    cached_timestamps = []\n\n    while t < t1:\n        h = min(step_size, t1 - t)\n        if h < min_step_size:\n            break\n        mu_pred, Phi, sqrt_Phi_sum = linear_step(\n            mu_pred, Phi, sqrt_Phi_sum, t, h, dynamics, method\n        )\n        if cache_params:\n            LSigma_pred = sum_mat_sqrts(Phi @ LSigma, sqrt_Phi_sum)\n            cached_mus.append(mu_pred.detach().clone())\n            cached_LSigmas.append(LSigma_pred.detach().clone())\n        t += h\n        if cache_params:\n            cached_timestamps.append(t)\n    if cache_params:\n        # Remove the predicted distribution for t1\n        # because this will be replaced by filter distribution\n        cached_mus.pop()\n        cached_LSigmas.pop()\n        cached_timestamps.pop()\n    cache = (cached_mus, cached_LSigmas, cached_timestamps)\n    LSigma_pred = sum_mat_sqrts(Phi @ LSigma, sqrt_Phi_sum)\n    return mu_pred, LSigma_pred, cache", "\n\ndef cont_disc_linear_update(y, mask, mu_pred, LSigma_pred, H, R, sporadic=False):\n    batch_size = y.size(0)\n    y_dim = y.size(-1)\n    z_dim = mu_pred.size(-1)\n\n    if R.size(0) != batch_size:\n        R = R.repeat(batch_size, 1, 1)\n    LR = cholesky(R)\n    if H.size(0) != batch_size:\n        H = H.repeat(batch_size, 1, 1)\n\n    if sporadic:\n        # mask.shape = B x D\n        mask_mat = torch.diag_embed(mask)\n        inv_mask_mat = torch.diag_embed(1 - mask)\n        # mask_mat.shape = B x D x D\n\n        # y.shape = B x D\n        y = y * mask\n\n        # Sqrt factor for:\n        # mask_mat @ R @ mask_mat.T + inv_mask_mat @ inv_mask_mat.T\n        LR = sum_mat_sqrts(mask_mat @ LR, inv_mask_mat)\n        # H.shape = B x D x M\n        H = mask_mat @ H\n\n    tmp = torch.zeros(batch_size, y_dim + z_dim, y_dim + z_dim, device=mu_pred.device)\n    tmp[:, :y_dim, :y_dim] = LR\n    tmp[:, :y_dim, y_dim:] = H @ LSigma_pred\n    tmp[:, y_dim:, y_dim:] = LSigma_pred\n\n    _, U = qr(bm_t(tmp))\n    U = bm_t(U)\n    X = U[:, :y_dim, :y_dim]\n    Y = U[:, y_dim:, :y_dim]\n    Z = U[:, y_dim:, y_dim:]\n\n    K = Y @ torch.inverse(X)  # Kalman Gain\n    y_pred = bmbvp(H, mu_pred)\n    r = y - y_pred  # Residual\n    # Update mu\n    mu = mu_pred + bmbvp(K, r)\n    # Update LSigma\n    LSigma = Z\n    # Compute LS for loss\n    LS = sum_mat_sqrts(H @ LSigma_pred, LR)\n    return mu, LSigma, y_pred, LS", "\n\ndef locallylinear_step(\n    mu: Tensor,\n    Phi: Tensor,\n    sqrt_Phi_sum: Tensor,\n    tn: float,\n    h: float,\n    dynamics: ContinuousLL,\n    method: str = \"rk4\",\n):\n    assert method in {\"euler\", \"rk4\"}, f\"Unknown solver: {method}!\"\n    F = dynamics.F\n    alpha0 = dynamics.alpha_net(mu)\n    F0 = torch.einsum(\n        \"bk, knm -> bnm\", alpha0, F\n    )  # Assuming fixed F(t) in the interval for cov\n    LQ = cholesky(dynamics.Q[None])\n\n    def _mu_rate_func(t, z, **unused_kwargs):\n        alpha = dynamics.alpha_net(z)\n        Ft = torch.einsum(\"bk, knm -> bnm\", alpha, F)\n        dz_by_dt = bmbvp(Ft, z)\n        return dz_by_dt\n\n    def _Phi_rate_func(t, z, **unused_kwargs):\n        # NOTE: Can matrix exp be used here?\n        return F0 @ z\n\n    if method == \"euler\":\n        mu_next = mu + h * _mu_rate_func(tn, mu)\n        Phi_next = Phi + h * _Phi_rate_func(tn, Phi)\n    elif method == \"rk4\":\n        mu_next = mu + rk4_step_func(_mu_rate_func, tn, h, tn + h, mu)\n        Phi_next = Phi + rk4_step_func(_Phi_rate_func, tn, h, tn + h, Phi)\n\n    Rtmp = sum_mat_sqrts(Phi @ LQ, Phi_next @ LQ)\n    Rtmp = math.sqrt(h / 2) * Rtmp\n    Rtmp = sum_mat_sqrts(Rtmp, sqrt_Phi_sum)\n    sqrt_Phi_sum = Rtmp\n\n    mu = mu_next\n    Phi = Phi_next\n    return mu, Phi, sqrt_Phi_sum", "\n\ndef cont_disc_locallylinear_predict(\n    mu,\n    LSigma,\n    dynamics,\n    t0,\n    t1,\n    step_size,\n    method=\"rk4\",\n    cache_params: bool = False,\n    min_step_size: float = 1e-5,\n):\n    batch_size = mu.size(0)\n    z_dim = dynamics.z_dim\n    t = t0\n    mu_pred = mu\n    Phi = torch.eye(z_dim, device=mu.device).repeat(batch_size, 1, 1)\n    sqrt_Phi_sum = torch.zeros(batch_size, z_dim, z_dim, device=mu.device)\n    cached_mus = []\n    cached_LSigmas = []\n    cached_timestamps = []\n\n    while t < t1:\n        h = min(step_size, t1 - t)\n        if h < min_step_size:\n            break\n        mu_pred, Phi, sqrt_Phi_sum = locallylinear_step(\n            mu_pred, Phi, sqrt_Phi_sum, t, h, dynamics, method\n        )\n        if cache_params:\n            LSigma_pred = sum_mat_sqrts(Phi @ LSigma, sqrt_Phi_sum)\n            cached_mus.append(mu_pred.detach().clone())\n            cached_LSigmas.append(LSigma_pred.detach().clone())\n        t += h\n        if cache_params:\n            cached_timestamps.append(t)\n    if cache_params:\n        # Remove the predicted distribution for t1\n        # because this will be replaced by filter distribution\n        cached_mus.pop()\n        cached_LSigmas.pop()\n        cached_timestamps.pop()\n    cache = (cached_mus, cached_LSigmas, cached_timestamps)\n    LSigma_pred = sum_mat_sqrts(Phi @ LSigma, sqrt_Phi_sum)\n    return mu_pred, LSigma_pred, cache", "\n\ndef cont_disc_locallylinear_update(\n    y: Tensor,\n    mask: Tensor,\n    mu_pred: Tensor,\n    LSigma_pred: Tensor,\n    H: Tensor,\n    R: Tensor,\n    sporadic: bool = False,\n):\n    return cont_disc_linear_update(\n        y, mask, mu_pred, LSigma_pred, H, R, sporadic=sporadic\n    )", "\n\ndef nonlinear_step(\n    mu: Tensor,\n    Phi: Tensor,\n    sqrt_Phi_sum: Tensor,\n    tn,\n    h,\n    dynamics: ContinuousNL,\n    method: str = \"rk4\",\n):\n    assert method in {\"euler\", \"rk4\"}, f\"Unknown solver: {method}!\"\n    f = dynamics.f\n    J_f0 = dynamics.jac_f(mu)  # Assuming fixed Jac_f(t) in the interval for cov\n    LGQGt = cholesky(dynamics.GQGt(mu))\n\n    def _mu_rate_func(t, z, **unused_kwargs):\n        dz_by_dt = f(z)\n        return dz_by_dt\n\n    def _Phi_rate_func(t, z, **unused_kwargs):\n        # NOTE: Can matrix exp be used here?\n        return J_f0 @ z\n\n    if method == \"euler\":\n        mu_next = mu + h * _mu_rate_func(tn, mu)\n        Phi_next = Phi + h * _Phi_rate_func(tn, Phi)\n    elif method == \"rk4\":\n        mu_next = mu + rk4_step_func(_mu_rate_func, tn, h, tn + h, mu)\n        Phi_next = Phi + rk4_step_func(_Phi_rate_func, tn, h, tn + h, Phi)\n\n    Rtmp = sum_mat_sqrts(Phi @ LGQGt, Phi_next @ LGQGt)\n    Rtmp = math.sqrt(h / 2) * Rtmp\n    Rtmp = sum_mat_sqrts(Rtmp, sqrt_Phi_sum)\n    sqrt_Phi_sum = Rtmp\n\n    mu = mu_next\n    Phi = Phi_next\n    return mu, Phi, sqrt_Phi_sum", "\n\ndef cont_disc_nonlinear_predict(\n    mu: Tensor,\n    LSigma: Tensor,\n    dynamics: ContinuousNL,\n    t0: float,\n    t1: float,\n    step_size: float,\n    method: str = \"rk4\",\n    cache_params: bool = False,\n    min_step_size: float = 1e-5,\n):\n    batch_size = mu.size(0)\n    z_dim = dynamics.z_dim\n    t = t0\n    mu_pred = mu\n    Phi = torch.eye(z_dim, device=mu.device).repeat(batch_size, 1, 1)\n    sqrt_Phi_sum = torch.zeros(batch_size, z_dim, z_dim, device=mu.device)\n    cached_mus = []\n    cached_LSigmas = []\n    cached_timestamps = []\n\n    while t < t1:\n        h = min(step_size, t1 - t)\n        if h < min_step_size:\n            break\n        mu_pred, Phi, sqrt_Phi_sum = nonlinear_step(\n            mu_pred, Phi, sqrt_Phi_sum, t, h, dynamics, method\n        )\n        if cache_params:\n            LSigma_pred = sum_mat_sqrts(Phi @ LSigma, sqrt_Phi_sum)\n            cached_mus.append(mu_pred.detach().clone())\n            cached_LSigmas.append(LSigma_pred.detach().clone())\n        t += h\n        if cache_params:\n            cached_timestamps.append(t)\n    if cache_params:\n        # Remove the predicted distribution for t1\n        # because this will be replaced by filter distribution\n        cached_mus.pop()\n        cached_LSigmas.pop()\n        cached_timestamps.pop()\n    cache = (cached_mus, cached_LSigmas, cached_timestamps)\n    LSigma_pred = sum_mat_sqrts(Phi @ LSigma, sqrt_Phi_sum)\n    return mu_pred, LSigma_pred, cache", "\n\ndef cont_disc_nonlinear_update(y, mask, mu_pred, LSigma_pred, H, R, sporadic=False):\n    return cont_disc_linear_update(\n        y, mask, mu_pred, LSigma_pred, H, R, sporadic=sporadic\n    )\n\n\ndef linear_smooth_step(\n    mu_s: Tensor,\n    Phi_s: Tensor,\n    sqrt_Phi_sum: Tensor,\n    mu_f: Tensor,\n    LSigma_f: Tensor,\n    tn,\n    h,\n    dynamics: ContinuousLTI,\n    method: str = \"rk4\",\n):\n    assert h <= 0\n    assert method in {\"euler\", \"rk4\"}, f\"Unknown solver: {method}!\"\n    F = dynamics.F\n    LQ = cholesky(dynamics.Q[None])\n    Sigma_f_inv = torch.cholesky_inverse(LSigma_f)\n    QSigma_f_inv = dynamics.Q[None] @ Sigma_f_inv\n    A = F[None] + QSigma_f_inv\n\n    def _mu_rate_func(t, z, **unused_kwargs):\n        dz_by_dt = mbvp(F, z) + bmbvp(QSigma_f_inv, z - mu_f)\n        return dz_by_dt\n\n    def _Phi_rate_func(t, z, **unused_kwargs):\n        # NOTE: Can matrix exp be used here?\n        return A @ z\n\n    if method == \"euler\":\n        mu_s_next = mu_s + h * _mu_rate_func(tn, mu_s)\n        Phi_s_next = Phi_s + h * _Phi_rate_func(tn, Phi_s)\n    elif method == \"rk4\":\n        mu_s_next = mu_s + rk4_step_func(_mu_rate_func, tn, h, tn + h, mu_s)\n        Phi_s_next = Phi_s + rk4_step_func(_Phi_rate_func, tn, h, tn + h, Phi_s)\n\n    Rtmp = sum_mat_sqrts(Phi_s @ LQ, Phi_s_next @ LQ)\n    Rtmp = math.sqrt(-h / 2) * Rtmp\n    Rtmp = sum_mat_sqrts(Rtmp, sqrt_Phi_sum)\n    sqrt_Phi_sum = Rtmp\n\n    mu_s = mu_s_next\n    Phi_s = Phi_s_next\n    return mu_s, Phi_s, sqrt_Phi_sum", "def linear_smooth_step(\n    mu_s: Tensor,\n    Phi_s: Tensor,\n    sqrt_Phi_sum: Tensor,\n    mu_f: Tensor,\n    LSigma_f: Tensor,\n    tn,\n    h,\n    dynamics: ContinuousLTI,\n    method: str = \"rk4\",\n):\n    assert h <= 0\n    assert method in {\"euler\", \"rk4\"}, f\"Unknown solver: {method}!\"\n    F = dynamics.F\n    LQ = cholesky(dynamics.Q[None])\n    Sigma_f_inv = torch.cholesky_inverse(LSigma_f)\n    QSigma_f_inv = dynamics.Q[None] @ Sigma_f_inv\n    A = F[None] + QSigma_f_inv\n\n    def _mu_rate_func(t, z, **unused_kwargs):\n        dz_by_dt = mbvp(F, z) + bmbvp(QSigma_f_inv, z - mu_f)\n        return dz_by_dt\n\n    def _Phi_rate_func(t, z, **unused_kwargs):\n        # NOTE: Can matrix exp be used here?\n        return A @ z\n\n    if method == \"euler\":\n        mu_s_next = mu_s + h * _mu_rate_func(tn, mu_s)\n        Phi_s_next = Phi_s + h * _Phi_rate_func(tn, Phi_s)\n    elif method == \"rk4\":\n        mu_s_next = mu_s + rk4_step_func(_mu_rate_func, tn, h, tn + h, mu_s)\n        Phi_s_next = Phi_s + rk4_step_func(_Phi_rate_func, tn, h, tn + h, Phi_s)\n\n    Rtmp = sum_mat_sqrts(Phi_s @ LQ, Phi_s_next @ LQ)\n    Rtmp = math.sqrt(-h / 2) * Rtmp\n    Rtmp = sum_mat_sqrts(Rtmp, sqrt_Phi_sum)\n    sqrt_Phi_sum = Rtmp\n\n    mu_s = mu_s_next\n    Phi_s = Phi_s_next\n    return mu_s, Phi_s, sqrt_Phi_sum", "\n\ndef type2_locallylinear_smooth_step(\n    mu_s: Tensor,\n    Phi_s: Tensor,\n    sqrt_Phi_sum: Tensor,\n    mu_f: Tensor,\n    LSigma_f: Tensor,\n    tn,\n    h,\n    dynamics: ContinuousLL,\n    method: str = \"rk4\",\n):\n    assert h <= 0\n    assert method in {\"euler\", \"rk4\"}, f\"Unknown solver: {method}!\"\n    F = dynamics.F\n    alpha0 = dynamics.alpha_net(mu_f)\n    F0 = torch.einsum(\"bk, knm -> bnm\", alpha0, F)\n    LQ = cholesky(dynamics.Q[None])\n    Sigma_f_inv = torch.cholesky_inverse(LSigma_f)\n    QSigma_f_inv = dynamics.Q[None] @ Sigma_f_inv\n    A = F0 + QSigma_f_inv\n\n    def _mu_rate_func(t, z, **unused_kwargs):\n        dz_by_dt = bmbvp(F0, mu_f) + bmbvp(F0 + QSigma_f_inv, z - mu_f)\n        return dz_by_dt\n\n    def _Phi_rate_func(t, z, **unused_kwargs):\n        # NOTE: Can matrix exp be used here?\n        return A @ z\n\n    if method == \"euler\":\n        mu_s_next = mu_s + h * _mu_rate_func(tn, mu_s)\n        Phi_s_next = Phi_s + h * _Phi_rate_func(tn, Phi_s)\n    elif method == \"rk4\":\n        mu_s_next = mu_s + rk4_step_func(_mu_rate_func, tn, h, tn + h, mu_s)\n        Phi_s_next = Phi_s + rk4_step_func(_Phi_rate_func, tn, h, tn + h, Phi_s)\n\n    Rtmp = sum_mat_sqrts(Phi_s @ LQ, Phi_s_next @ LQ)\n    Rtmp = math.sqrt(-h / 2) * Rtmp\n    Rtmp = sum_mat_sqrts(Rtmp, sqrt_Phi_sum)\n    sqrt_Phi_sum = Rtmp\n\n    mu_s = mu_s_next\n    Phi_s = Phi_s_next\n    return mu_s, Phi_s, sqrt_Phi_sum", "\n\ndef type2_nonlinear_smooth_step(\n    mu_s: Tensor,\n    Phi_s: Tensor,\n    sqrt_Phi_sum: Tensor,\n    mu_f: Tensor,\n    LSigma_f: Tensor,\n    tn,\n    h,\n    dynamics: ContinuousNL,\n    method: str = \"rk4\",\n):\n    assert h <= 0\n    assert method in {\"euler\", \"rk4\"}, f\"Unknown solver: {method}!\"\n    f = dynamics.f\n    GQGt = dynamics.GQGt(mu_f)\n    LGQGt = cholesky(GQGt)\n    Sigma_f_inv = torch.cholesky_inverse(LSigma_f)\n    QSigma_f_inv = GQGt @ Sigma_f_inv\n    J_f0 = dynamics.jac_f(mu_f)\n    A = J_f0 + QSigma_f_inv\n\n    def _mu_rate_func(t, z, **unused_kwargs):\n        dz_by_dt = f(mu_f) + bmbvp(J_f0 + QSigma_f_inv, z - mu_f)\n        return dz_by_dt\n\n    def _Phi_rate_func(t, z, **unused_kwargs):\n        # NOTE: Can matrix exp be used here?\n        return A @ z\n\n    if method == \"euler\":\n        mu_s_next = mu_s + h * _mu_rate_func(tn, mu_s)\n        Phi_s_next = Phi_s + h * _Phi_rate_func(tn, Phi_s)\n    elif method == \"rk4\":\n        mu_s_next = mu_s + rk4_step_func(_mu_rate_func, tn, h, tn + h, mu_s)\n        Phi_s_next = Phi_s + rk4_step_func(_Phi_rate_func, tn, h, tn + h, Phi_s)\n\n    Rtmp = sum_mat_sqrts(Phi_s @ LGQGt, Phi_s_next @ LGQGt)\n    Rtmp = math.sqrt(-h / 2) * Rtmp\n    Rtmp = sum_mat_sqrts(Rtmp, sqrt_Phi_sum)\n    sqrt_Phi_sum = Rtmp\n\n    mu_s = mu_s_next\n    Phi_s = Phi_s_next\n    return mu_s, Phi_s, sqrt_Phi_sum", "\n\n@torch.no_grad()\ndef cont_disc_smooth(\n    filter_mus: Tensor,\n    filter_LSigmas: Tensor,\n    filter_timestamps,\n    dynamics: Union[\n        ContinuousLTI,\n        ContinuousLL,\n        ContinuousNL,\n    ],\n    method: str = \"rk4\",\n):\n    batch_size = filter_mus.size(1)\n    z_dim = dynamics.z_dim\n\n    filter_mus = torch.flip(filter_mus, dims=(0,))\n    filter_LSigmas = torch.flip(filter_LSigmas, dims=(0,))\n    filter_timestamps = torch.flip(filter_timestamps, dims=(0,))\n\n    mu_s = filter_mus[0]\n    LSigma_s = filter_LSigmas[0]\n\n    smoothed_mus = [mu_s]\n    smoothed_LSigmas = [LSigma_s]\n\n    if isinstance(dynamics, ContinuousLTI):\n        _smooth_step = linear_smooth_step\n    elif isinstance(dynamics, ContinuousLL):\n        _smooth_step = type2_locallylinear_smooth_step\n    elif isinstance(dynamics, ContinuousNL):\n        _smooth_step = type2_nonlinear_smooth_step\n    else:\n        raise ValueError(f\"Unknown dynamics type {type(dynamics)}!\")\n\n    for idx, t0 in enumerate(filter_timestamps[:-1]):\n        t1 = filter_timestamps[idx + 1]\n        h = t1.item() - t0.item()\n        mu_f = filter_mus[idx]\n        LSigma_f = filter_LSigmas[idx]\n        Phi_s = torch.eye(z_dim, device=filter_mus.device).repeat(batch_size, 1, 1)\n        sqrt_Phi_sum = torch.zeros(batch_size, z_dim, z_dim, device=Phi_s.device)\n        mu_s, Phi_s, sqrt_Phi_sum = _smooth_step(\n            mu_s, Phi_s, sqrt_Phi_sum, mu_f, LSigma_f, t0.item(), h, dynamics, method\n        )\n        LSigma_s = sum_mat_sqrts(Phi_s @ LSigma_s, sqrt_Phi_sum)\n        smoothed_mus.append(mu_s.clone())\n        smoothed_LSigmas.append(LSigma_s.clone())\n\n    smoothed_mus: Tensor = torch.stack(smoothed_mus)\n    smoothed_LSigmas: Tensor = torch.stack(smoothed_LSigmas)\n    smoothed_mus, smoothed_LSigmas = torch.flip(smoothed_mus, dims=(0,)), torch.flip(\n        smoothed_LSigmas, dims=(0,)\n    )\n    return smoothed_mus, smoothed_LSigmas", ""]}
{"filename": "src/ncdssm/plotting.py", "chunked_list": ["import torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom torchvision.utils import save_image\n\n\nsns.set(style=\"white\")\ncolor_names = [\n    \"purple\",", "color_names = [\n    \"purple\",\n    \"orange\",\n    \"windows blue\",\n    \"red\",\n    \"amber\",\n    \"faded green\",\n    \"dusty purple\",\n    \"clay\",\n    \"pink\",", "    \"clay\",\n    \"pink\",\n    \"green\",\n    \"greyish\",\n    \"light cyan\",\n    \"steel blue\",\n    \"pastel purple\",\n    \"mint\",\n    \"salmon\",\n]", "    \"salmon\",\n]\nxkcd_colors = colors = sns.xkcd_palette(color_names)\n# colors = sns.color_palette(\"muted\")\n\n\ndef plot_on_axis(\n    ax,\n    past_times,\n    future_times,\n    inputs,\n    masked_inputs,\n    sorted_rec_and_forecast,\n    prediction_intervals=[90.0],\n    yticks_off=True,\n    ylim=None,\n    colors=colors,\n    ylabel=\"\",\n    shade_context=False,\n):\n    all_times = np.hstack([past_times, future_times])\n    num_samples, _, num_feats = sorted_rec_and_forecast.shape\n\n    for c in prediction_intervals:\n        assert 0.0 <= c <= 100.0\n\n    ps = [50.0] + [\n        50.0 + f * c / 2.0 for c in prediction_intervals for f in [-1.0, +1.0]\n    ]\n    percentiles_sorted = sorted(set(ps))\n\n    def alpha_for_percentile(p):\n        return (p / 100.0) ** 0.6\n\n    def quantile(q):\n        sample_idx = int(np.round((num_samples - 1) * q))\n        return sorted_rec_and_forecast[sample_idx, :]\n\n    ps_data = [quantile(p / 100.0) for p in percentiles_sorted]\n    i_p50 = len(percentiles_sorted) // 2\n\n    p50_data = ps_data[i_p50]\n    if shade_context:\n        ax.axvspan(0, future_times[0], facecolor=xkcd_colors[-6], alpha=0.2)\n    for o in range(num_feats):\n        ax.plot(\n            all_times,\n            inputs[:, o],\n            ls=\"--\",\n            lw=1.4,\n            color=colors[o],\n            alpha=0.6,\n        )\n        # Median forecast\n        ax.plot(all_times, p50_data[:, o], ls=\"-\", lw=1.5, color=colors[o], alpha=1.0)\n        ax.scatter(\n            past_times,\n            masked_inputs[: len(past_times), o],\n            s=20,\n            marker=\"o\",\n            color=colors[o],\n            alpha=0.6,\n        )\n        ax.axvline(future_times[0], color=xkcd_colors[-6], ls=\":\")\n\n        for i in range(len(percentiles_sorted) // 2):\n            ptile = percentiles_sorted[i]\n            alpha = alpha_for_percentile(ptile)\n\n            ax.fill_between(\n                all_times,\n                ps_data[i][:, o],\n                ps_data[-i - 1][:, o],\n                facecolor=colors[o],\n                # edgecolor=colors[o],\n                alpha=alpha,\n                interpolate=True,\n                # label=f\"{prediction_intervals[i]}% PI\",\n            )\n            ax.set_ylabel(ylabel)\n            if ylim:\n                ax.set_ylim(ylim)\n            if yticks_off:\n                ax.set_yticks([])\n    return ax", "\n\ndef show_time_series_forecast(\n    fig_size,\n    past_times,\n    future_times,\n    inputs,\n    masked_inputs,\n    reconstruction,\n    forecast,\n    prediction_intervals=[90.0],\n    fig_title=None,\n    file_path=None,\n    max_feats=6,\n    single_plot=False,\n    yticks_off=True,\n    ylim=None,\n):\n    obs_dim = inputs.shape[-1]\n    max_feats = min(max_feats, obs_dim)\n\n    rec_and_forecast = np.concatenate([reconstruction, forecast], axis=1)\n    rec_and_forecast = rec_and_forecast[..., :, :max_feats]\n    inputs = inputs[..., :, :max_feats]\n    all_times = np.hstack([past_times, future_times])  # [:150]\n    # num_samples = rec_and_forecast.shape[0]\n\n    if single_plot:\n        fig_size = (12, 1.0)\n        fig, axn = plt.subplots(figsize=fig_size, nrows=1, sharex=True)\n        axn = [axn] * max_feats\n    else:\n        fig_size = (12, max_feats * 1.0)\n        fig, axn = plt.subplots(figsize=fig_size, nrows=max_feats, sharex=True)\n        if max_feats == 1:\n            axn = [axn]\n    if fig_title:\n        plt.title(fig_title)\n\n    sorted_rec_and_forecast = np.sort(rec_and_forecast, axis=0)\n    axn[0].scatter(\n        [],\n        [],\n        s=20,\n        marker=\"o\",\n        color=\"k\",\n        alpha=0.6,\n        label=\"Observations\",\n    )\n    axn[0].plot(\n        [],\n        [],\n        ls=\"--\",\n        lw=1.4,\n        color=\"k\",\n        alpha=0.6,\n        label=\"Ground Truth\",\n    )\n    axn[0].plot(\n        [],\n        [],\n        ls=\"-\",\n        lw=1.5,\n        color=\"k\",\n        label=\"Median Prediction\",\n    )\n    for o in range(max_feats):\n        plot_on_axis(\n            axn[o],\n            past_times,\n            future_times,\n            inputs[:, o : o + 1],\n            masked_inputs[:, o : o + 1],\n            sorted_rec_and_forecast[:, :, o : o + 1],\n            prediction_intervals,\n            yticks_off,\n            ylim,\n            colors=colors[o : o + 1],\n            ylabel=f\"$y_{o}$\",\n        )\n\n    axn[-1].set_xlabel(\"Time\")\n    axn[0].legend(bbox_to_anchor=(0.5, 1.6), ncol=3, loc=\"upper center\")\n    axn[0].set_xlim((all_times[0], all_times[-1]))\n    # plt.tight_layout()\n    if file_path:\n        plt.savefig(file_path, dpi=200, bbox_inches=\"tight\")\n    return fig", "\n\ndef show_pymunk_forecast(orig, pred, file_path):\n    assert orig.shape == pred.shape[1:]\n    N, T, C, H, W = pred.shape\n    orig = torch.as_tensor(orig)\n    pred = torch.as_tensor(pred)\n    orig = orig.unsqueeze(0)\n    img = torch.cat([orig, pred], 0)\n    img = img.view((N + 1) * T, C, H, W)\n    save_image(img, file_path, nrow=T, pad_value=0.5)\n    return img", "\n\ndef show_wasserstein_distance(\n    fig_size, w_dist, conf_intervals=None, fig_title=None, ylim=None, file_path=None\n):\n    fig = plt.figure(figsize=fig_size)\n    if fig_title:\n        plt.title(fig_title)\n    ax = fig.gca()\n    if conf_intervals is not None:\n        ax.errorbar(\n            np.arange(w_dist.shape[0]),\n            w_dist,\n            yerr=conf_intervals,\n            capsize=5,\n            color=colors[-1],\n        )\n    else:\n        ax.plot(w_dist, color=colors[-1])\n    ax.set_ylabel(\"W\")\n    ax.set_xlabel(\"T\")\n    if ylim is not None:\n        ax.set_ylim(ylim)\n    if file_path:\n        plt.savefig(file_path, dpi=200, bbox_inches=\"tight\")\n    return fig", "\n\ndef show_latents(fig_size, time, latents, fig_title, file_path=None):\n    fig, axn = plt.subplots(figsize=fig_size, nrows=len(latents), sharex=True)\n    if len(latents) == 1:\n        axn = [axn]\n    if fig_title:\n        plt.suptitle(fig_title)\n    for i, key in enumerate(latents):\n        ts = latents[key]\n        dims = ts.shape[-1]\n        for d in range(dims):\n            axn[i].plot(time, ts[:, d], color=colors[d])\n        axn[i].set_ylabel(key)\n    axn[-1].set_xlabel(\"time\")\n    if file_path:\n        plt.savefig(file_path, dpi=200, bbox_inches=\"tight\")\n        plt.close(fig)\n    else:\n        return fig", ""]}
{"filename": "src/ncdssm/type.py", "chunked_list": ["from typing import Optional, Callable, Tuple, Dict, List, Union\nimport torch\nimport numpy as np\n\nTensor = torch.Tensor\nNumpyArray = np.ndarray\n\n__all__ = [\"Optional\", \"Callable\", \"Tuple\", \"Dict\", \"List\", \"Tensor\", \"Union\"]\n", ""]}
{"filename": "src/ncdssm/utils.py", "chunked_list": ["from ncdssm.type import Dict\n\n\ndef listofdict2dictoflist(lod: Dict):\n    keys = lod[0].keys()\n    return {k: [elem[k] for elem in lod] for k in keys}\n"]}
{"filename": "src/ncdssm/modules.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torchdiffeq import odeint\nfrom torch.nn.utils import spectral_norm\n\nfrom .type import Callable\n\n\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        in_dim: int,\n        h_dim: int,\n        out_dim: int,\n        n_hidden_layers: int = 1,\n        nonlinearity: Callable = nn.ReLU,\n        last_nonlinearity: bool = False,\n        zero_init_last: bool = False,\n        apply_spectral_norm: bool = False,\n    ):\n        super().__init__()\n        assert n_hidden_layers >= 1\n        self.in_dim = in_dim\n        self.out_dim = h_dim\n\n        module_list = []\n\n        sn: Callable = (\n            spectral_norm if apply_spectral_norm else lambda x: x  # type: ignore\n        )\n\n        module_list.append(sn(nn.Linear(in_dim, h_dim)))\n        module_list.append(nonlinearity())\n\n        for _ in range(n_hidden_layers - 1):\n            module_list.append(sn(nn.Linear(h_dim, h_dim)))\n            module_list.append(nonlinearity())\n\n        module_list.append(sn(nn.Linear(h_dim, out_dim)))\n\n        if zero_init_last:\n            module_list[-1].weight.data.zero_()\n            module_list[-1].bias.data.zero_()\n        if last_nonlinearity:\n            module_list.append(nonlinearity())\n        self.mlp = nn.Sequential(*module_list)\n\n    def forward(self, x):\n        return self.mlp(x)", "\nclass MLP(nn.Module):\n    def __init__(\n        self,\n        in_dim: int,\n        h_dim: int,\n        out_dim: int,\n        n_hidden_layers: int = 1,\n        nonlinearity: Callable = nn.ReLU,\n        last_nonlinearity: bool = False,\n        zero_init_last: bool = False,\n        apply_spectral_norm: bool = False,\n    ):\n        super().__init__()\n        assert n_hidden_layers >= 1\n        self.in_dim = in_dim\n        self.out_dim = h_dim\n\n        module_list = []\n\n        sn: Callable = (\n            spectral_norm if apply_spectral_norm else lambda x: x  # type: ignore\n        )\n\n        module_list.append(sn(nn.Linear(in_dim, h_dim)))\n        module_list.append(nonlinearity())\n\n        for _ in range(n_hidden_layers - 1):\n            module_list.append(sn(nn.Linear(h_dim, h_dim)))\n            module_list.append(nonlinearity())\n\n        module_list.append(sn(nn.Linear(h_dim, out_dim)))\n\n        if zero_init_last:\n            module_list[-1].weight.data.zero_()\n            module_list[-1].bias.data.zero_()\n        if last_nonlinearity:\n            module_list.append(nonlinearity())\n        self.mlp = nn.Sequential(*module_list)\n\n    def forward(self, x):\n        return self.mlp(x)", "\n\nclass ODEFunc(nn.Module):\n    def __init__(self, ode_net: nn.Module):\n        super().__init__()\n        self.ode_net = ode_net\n\n    def forward(self, t, x):\n        return self.ode_net(x)\n", "\n\nclass ODEGRU(nn.Module):\n    def __init__(\n        self,\n        data_dim: int,\n        state_dim: int,\n        ode_h_dim: int,\n        ode_n_layers: int,\n        ode_nonlinearity: nn.Module,\n        integration_step_size: float = 0.1,\n        integration_method: str = \"rk4\",\n    ):\n        super().__init__()\n        self.data_dim = data_dim\n        self.state_dim = state_dim\n        self.ode_func = ODEFunc(\n            MLP(\n                in_dim=state_dim,\n                out_dim=state_dim,\n                h_dim=ode_h_dim,\n                n_hidden_layers=ode_n_layers,\n                nonlinearity=ode_nonlinearity,\n            )\n        )\n        self.integration_step_size = integration_step_size\n        self.integration_method = integration_method\n        self.gru_cell = nn.GRUCell(input_size=data_dim, hidden_size=state_dim)\n\n    def gru_update(self, x, h, mask):\n        h_next = self.gru_cell(x, h)\n        mask = mask.unsqueeze(-1)\n        h_next = mask * h_next + (1 - mask) * h\n        return h_next\n\n    def ode_update(self, h, t1, t2):\n        out = odeint(\n            self.ode_func,\n            h,\n            torch.tensor([t1, t2], device=h.device),\n            method=self.integration_method,\n            options={\"step_size\": self.integration_step_size},\n        )\n        return out[-1]\n\n    def forward(self, x, times, mask):\n        B, T, F = x.size()\n        out = []\n        if T == 1:\n            h = torch.zeros(B, self.state_dim, device=x.device)\n            h = self.gru_update(x[:, 0], h, mask[:, 0])\n            out.append(h)\n        else:\n            out = []\n            h = torch.zeros(B, self.state_dim, device=x.device)\n\n            h = self.gru_update(x[:, 0], h, mask[:, 0])\n            out.append(h)\n            for i, (t1, t2) in enumerate(zip(times[:-1], times[1:])):\n                h_ode = self.ode_update(h, t1.item(), t2.item())\n                h = self.gru_update(x[:, i + 1], h_ode, mask[:, i + 1])\n                out.append(h)\n        out = torch.stack(out)\n        out = out.permute(1, 0, 2)\n        return out", "\n\nclass MergeLastDims(nn.Module):\n    def __init__(self, ndims=1):\n        super().__init__()\n        self.ndims = ndims\n\n    def forward(self, x):\n        shape = x.shape\n        last_dim = np.prod(shape[-self.ndims :])\n        x = x.view(shape[: -self.ndims] + (last_dim,))\n        return x", "\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, img_size, channels, out_dim):\n        super().__init__()\n        self.img_size = img_size\n        self.ch = channels\n        self.out_dim = out_dim\n        self.network = nn.Sequential(\n            nn.ZeroPad2d(padding=[0, 1, 0, 1]),\n            nn.Conv2d(\n                in_channels=self.ch,\n                out_channels=32,\n                kernel_size=3,\n                stride=2,\n                padding=0,\n            ),\n            nn.ReLU(),\n            nn.ZeroPad2d(padding=[0, 1, 0, 1]),\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=32,\n                kernel_size=3,\n                stride=2,\n                padding=0,\n            ),\n            nn.ReLU(),\n            nn.ZeroPad2d(padding=[0, 1, 0, 1]),\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=32,\n                kernel_size=3,\n                stride=2,\n                padding=0,\n            ),\n            nn.ReLU(),\n            MergeLastDims(ndims=3),\n            nn.Linear(32 * 4 * 4, self.out_dim),\n        )\n\n    def forward(self, x):\n        assert x.dim() == 3, \"Expected 3 dims B, T, C * H * W\"\n        B, T, _ = x.shape\n        x = x.view(B, T, self.ch, self.img_size, self.img_size)\n        B, T, C, H, W = x.shape\n        assert H == W\n        x = x.reshape(B * T, C, H, W)\n        h = self.network(x)\n        h = h.view(B, T, self.out_dim)\n        return h", "\n\nclass ImageDecoder(nn.Module):\n    def __init__(self, in_dim, img_size, channels):\n        super().__init__()\n        self.img_size = img_size\n        self.ch = channels\n        self.linear = nn.Linear(\n            in_features=in_dim,\n            out_features=32 * 4 * 4,\n        )\n        self.convnet = nn.Sequential(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=32 * 2**2,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            nn.ReLU(),\n            nn.PixelShuffle(upscale_factor=2),\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=32 * 2**2,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            nn.ReLU(),\n            nn.PixelShuffle(upscale_factor=2),\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=32 * 2**2,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            nn.ReLU(),\n            nn.PixelShuffle(upscale_factor=2),\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=self.ch,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n            ),\n        )\n\n    def forward(self, x):\n        assert x.dim() == 3, \"Expected 3 dims B, T, D\"\n        B, T, _ = x.shape\n        h = self.linear(x)\n        h = h.view(B * T, 32, 4, 4)\n        h = self.convnet(h)\n        h = h.view(B, T, self.ch, self.img_size, self.img_size)\n        return h", "\n\nclass Lambda(nn.Module):\n    def __init__(self, func: Callable):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n", ""]}
{"filename": "src/ncdssm/models/dynamics.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nfrom ..modules import Lambda\n\nfrom ..functions import batch_jacobian, bm_t, mbvp, bmbvp\nfrom ..type import Tuple, Tensor, List\n\n\nclass ContinuousLTI(nn.Module):\n    def __init__(\n        self,\n        z_dim: int,\n        u_dim: int,\n        F: Tensor,\n        B: Tensor,\n        uQ: Tensor,\n    ):\n        super().__init__()\n        self.z_dim = z_dim\n        if u_dim > 0:\n            if B is not None:\n                self.register_buffer(\"B\", B)\n            else:\n                self.B = nn.Parameter(\n                    nn.init.xavier_uniform_(torch.empty(z_dim, u_dim))\n                )\n\n        self.F = nn.Parameter(F)\n        self.uQ = nn.Parameter(uQ)\n\n    @property\n    def Q(self):\n        return torch.diag(torch.clamp(torch.exp(self.uQ), min=1e-4))\n\n    def forward(\n        self,\n        t: Tensor,\n        m_and_P: Tuple[Tensor, Tensor],\n    ):\n        \"\"\"Dynamics for mean and covariance.\n\n        Args:\n            t (Tensor): A 1-D tensor of times.\n            m_and_P (Tuple[Tensor, Tensor]): A tuple of\n                mean and covariance.\n\n        Returns:\n            Tuple: Tuple of \"velocity\" of mean and covariance.\n        \"\"\"\n\n        m, P = m_and_P\n        # Dynamics for mean\n        velocity_m = mbvp(self.F, m)\n\n        # Dynamics for Covariance\n        F = self.F[None]\n        Q = self.Q[None]\n        velocity_P = F @ P + P @ bm_t(F) + Q\n        return velocity_m, velocity_P", "\nclass ContinuousLTI(nn.Module):\n    def __init__(\n        self,\n        z_dim: int,\n        u_dim: int,\n        F: Tensor,\n        B: Tensor,\n        uQ: Tensor,\n    ):\n        super().__init__()\n        self.z_dim = z_dim\n        if u_dim > 0:\n            if B is not None:\n                self.register_buffer(\"B\", B)\n            else:\n                self.B = nn.Parameter(\n                    nn.init.xavier_uniform_(torch.empty(z_dim, u_dim))\n                )\n\n        self.F = nn.Parameter(F)\n        self.uQ = nn.Parameter(uQ)\n\n    @property\n    def Q(self):\n        return torch.diag(torch.clamp(torch.exp(self.uQ), min=1e-4))\n\n    def forward(\n        self,\n        t: Tensor,\n        m_and_P: Tuple[Tensor, Tensor],\n    ):\n        \"\"\"Dynamics for mean and covariance.\n\n        Args:\n            t (Tensor): A 1-D tensor of times.\n            m_and_P (Tuple[Tensor, Tensor]): A tuple of\n                mean and covariance.\n\n        Returns:\n            Tuple: Tuple of \"velocity\" of mean and covariance.\n        \"\"\"\n\n        m, P = m_and_P\n        # Dynamics for mean\n        velocity_m = mbvp(self.F, m)\n\n        # Dynamics for Covariance\n        F = self.F[None]\n        Q = self.Q[None]\n        velocity_P = F @ P + P @ bm_t(F) + Q\n        return velocity_m, velocity_P", "\n\nclass ContinuousLL(nn.Module):\n    def __init__(\n        self,\n        z_dim: int,\n        u_dim: int,\n        K: int,\n        F: Tensor,\n        B: Tensor,\n        uQ: Tensor,\n        alpha_net: nn.Module,\n    ):\n        super().__init__()\n        self.z_dim = z_dim\n        self.u_dim = u_dim\n        self.K = K\n        if u_dim > 0:\n            if B is not None:\n                self.register_buffer(\"B\", B)\n            else:\n                self.B = nn.Parameter(\n                    nn.init.xavier_uniform_(torch.empty(K, z_dim, u_dim))\n                )\n        self.F = nn.Parameter(F)\n        self.uQ = nn.Parameter(uQ)\n        self.alpha_net = alpha_net\n\n    @property\n    def Q(self):\n        return torch.diag(torch.clamp(torch.exp(self.uQ), min=1e-4))\n\n    def forward(self, t: Tensor, m_and_P: Tuple[Tensor, Tensor]):\n        \"\"\"Dynamics for mean and covariance.\n\n        Args:\n            t (Tensor): A 1-D tensor of times.\n            m_and_P (Tuple[Tensor, Tensor]):\n                A tuple of mean and covariance.\n\n        Returns:\n            Tuple: Tuple of \"velocity\" of mean and covariance.\n        \"\"\"\n\n        m, P = m_and_P\n        # Linear combination of base matrices\n        alpha = self.alpha_net(m)\n        F = torch.einsum(\"bk, knm -> bnm\", alpha, self.F)\n        # F.shape = B x z_dim x z_dim\n        # m.shape = B x z_dim\n        # Dynamics for mean\n        velocity_m = bmbvp(F, m)\n\n        # Dynamics for Covariance\n        Q = self.Q[None]\n        velocity_P = F @ P + P @ bm_t(F) + Q\n        return velocity_m, velocity_P", "\n\nclass ContinuousNL(nn.Module):\n    def __init__(\n        self,\n        z_dim: int,\n        u_dim: int,\n        f: nn.Module,\n        gs: List[nn.Module],\n        B: Tensor,\n    ):\n        super().__init__()\n        self.z_dim = z_dim\n        self.f = f\n\n        if u_dim > 0:\n            if B is not None:\n                self.register_buffer(\"B\", B)\n            else:\n                self.B = nn.Parameter(\n                    nn.init.xavier_uniform_(torch.empty(z_dim, u_dim))\n                )\n\n        self.fixed_diffusion = gs is None\n        if gs is None:\n            gs = [\n                nn.Sequential(\n                    nn.Linear(1, 1, bias=False), Lambda(lambda x: torch.exp(x))\n                )\n                for _ in range(z_dim)\n            ]\n            with torch.no_grad():\n                for g in gs:\n                    g[0].weight.data.zero_()\n        self.gs = nn.ModuleList(gs)\n\n    def jac_f(self, z: Tensor) -> Tensor:\n        J = batch_jacobian(self.f, z, create_graph=True, vectorize=True)\n        return J\n\n    def GQGt(self, z):\n        # Assuming Q = I, so GQGt = GGt\n        # For fixed diffusion the normalizer is Exp(), see __init__().\n        # For non linear diffusion, it is assumed that\n        # the diffusion networks normalize the scale to R+.\n\n        if self.fixed_diffusion:\n            z = torch.ones_like(z)\n        zs = torch.split(z, split_size_or_sections=1, dim=-1)\n        diag_G = torch.cat([g_i(z_i) for (g_i, z_i) in zip(self.gs, zs)], dim=-1)\n        diag_Gsq = diag_G * diag_G\n        return torch.diag_embed(torch.clamp(diag_Gsq, min=1e-4))\n\n    def forward(self, t: Tensor, m_P: Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]:\n        \"\"\"Dynamics for mean and covariance.\n\n        Args:\n            t (Tensor): A 1-D tensor of times.\n            m_P (Tuple[Tensor, Tensor, Tensor]):\n                A tuple of mean, and covariance.\n\n        Returns:\n            Tuple: Tuple of \"velocity\" of mean and covariance.\n        \"\"\"\n\n        m, P = m_P\n        # Dynamics for mean\n        velocity_m = self.f(m)\n\n        # Dynamics for Covariance\n        GQGt = self.GQGt(m)\n        J_f = self.jac_f(m)\n        velocity_P = J_f @ P + P @ bm_t(J_f) + GQGt\n        return velocity_m, velocity_P", ""]}
{"filename": "src/ncdssm/models/base.py", "chunked_list": ["from abc import ABC, abstractmethod\nimport torch\nimport torch.nn as nn\nfrom torch.distributions.multivariate_normal import MultivariateNormal\n\n\nfrom .dynamics import (\n    ContinuousLTI,\n    ContinuousNL,\n    ContinuousLL,", "    ContinuousNL,\n    ContinuousLL,\n)\nfrom ..inference import (\n    cont_disc_linear_predict,\n    cont_disc_linear_update,\n    cont_disc_locallylinear_predict,\n    cont_disc_locallylinear_update,\n    cont_disc_nonlinear_predict,\n    cont_disc_nonlinear_update,", "    cont_disc_nonlinear_predict,\n    cont_disc_nonlinear_update,\n    cont_disc_smooth,\n)\nfrom ..torch_utils import skew_symmetric_init_\nfrom ..functions import cholesky, bmbvp\nfrom ..type import Optional, Tensor, List, Dict, Union\n\n\nclass Base(ABC):\n    @abstractmethod\n    def predict_step(\n        self,\n        mu: Tensor,\n        LSigma: Tensor,\n        dynamics: Union[ContinuousLTI, ContinuousNL, ContinuousLL],\n        t0: float,\n        t1: float,\n        step_size: float,\n        method: str,\n        cache_params: bool = False,\n        min_step_size: float = 1e-5,\n    ):\n        pass\n\n    @abstractmethod\n    def update_step(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        mu_pred: Tensor,\n        LSigma_pred: Tensor,\n        H: Tensor,\n        R: Tensor,\n        sporadic: bool = False,\n    ):\n        pass\n\n    def filter(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        times: Tensor,\n        cache_params: bool = False,\n    ) -> Dict[str, Tensor]:\n        \"\"\"The filter step of the continuous-discrete model.\n\n        Parameters\n        ----------\n        y\n            The tensor of observations, of shape (batch_size, num_timesteps, y_dim)\n        mask\n            The mask of missing values (1: observed, 0: missing),\n            of shape (batch_size, num_timesteps, y_dim), if sporadic,\n            else (batch_size, num_timesteps)\n        times\n            The tensor observations times, of shape (num_timesteps,)\n        cache_params, optional\n            Whether to cache the intermediate distributions computed during\n            predict step, by default False and all distrbutions at observation ``times``\n            are cached in any case,\n            should be set to True if you're planning to use smoothing\n\n        Returns\n        -------\n            The dictionary of filter outputs, including the log-likelihood\n            and the parameters of filtered distributions\n        \"\"\"\n        if self.sporadic:\n            assert (\n                y.size() == mask.size()\n            ), f\"Shapes of y ({y.size()}) and mask ({mask.size()}) should match!\"\n        else:\n            assert y.size()[:-1] == mask.size(), (\n                f\"Shapes of y ({y.size()}) and mask ({mask.size()})\"\n                \" should match except in last dim!\"\n            )\n        batch_size = y.size(0)\n\n        # Assumes that t = 0 is in times\n        assert times[0] == 0.0, \"First timestep should be 0!\"\n        mu_pred = self.mu0[None].repeat(batch_size, 1)\n        LSigma_pred = cholesky(self.Sigma0[None]).repeat(batch_size, 1, 1)\n        log_prob = []\n\n        filtered_mus = []\n        filtered_LSigmas = []\n        cached_mus = []\n        cached_LSigmas = []\n        cached_timestamps = []\n\n        for idx, t in enumerate(times):\n            H = self.H[None]\n            R = self.R[None]\n            # UPDATE step\n            y_i = y[:, idx]\n            mask_i = mask[:, idx]\n            mu, LSigma, y_pred, LS = self.update_step(\n                y=y_i,\n                mask=mask_i,\n                mu_pred=mu_pred,\n                LSigma_pred=LSigma_pred,\n                H=H,\n                R=R,\n                sporadic=self.sporadic,\n            )\n            if not self.sporadic:\n                # Mask out updates\n                # For the sporadic case, masks are directly incorporated\n                # during the update step\n                # TODO: Modify update step such that masking is not required here\n                mu = mask_i[:, None] * mu + (1 - mask_i[:, None]) * mu_pred\n                LSigma = (\n                    mask_i[:, None, None] * LSigma\n                    + (1 - mask_i[:, None, None]) * LSigma_pred\n                )\n\n            # Calculate log_prob\n            if not self.sporadic:\n                dist = MultivariateNormal(loc=y_pred, scale_tril=LS)\n                log_prob.append(dist.log_prob(y_i) * mask_i)\n            else:\n                dist = MultivariateNormal(loc=y_pred * mask_i, scale_tril=LS)\n                log_prob.append(dist.log_prob(y_i * mask_i))\n\n            # Cache filtered distribution\n            filtered_mus.append(mu.clone())\n            filtered_LSigmas.append(LSigma.clone())\n            cached_mus.append(mu.detach().clone())\n            cached_LSigmas.append(LSigma.detach().clone())\n            cached_timestamps.append(t.item())\n            if idx == len(times) - 1:\n                break\n            # PREDICT step\n            (\n                mu_pred,\n                LSigma_pred,\n                (cached_mus_t, cached_LSigmas_t, cached_timestamps_t),\n            ) = self.predict_step(\n                mu=mu,\n                LSigma=LSigma,\n                dynamics=self.dynamics,\n                t0=t.item(),\n                t1=times[idx + 1].item(),\n                step_size=self.integration_step_size,\n                method=self.integration_method,\n                cache_params=cache_params,\n            )\n            cached_mus.extend(cached_mus_t)\n            cached_LSigmas.extend(cached_LSigmas_t)\n            cached_timestamps.extend(cached_timestamps_t)\n\n        filtered_mus: Tensor = torch.stack(filtered_mus)\n        filtered_LSigmas: Tensor = torch.stack(filtered_LSigmas)\n        cached_mus: Tensor = torch.stack(cached_mus)\n        cached_LSigmas: Tensor = torch.stack(cached_LSigmas)\n        cached_timestamps: Tensor = torch.tensor(\n            cached_timestamps, dtype=times.dtype, device=times.device\n        )\n        log_prob = torch.stack(log_prob).sum(0)\n        return dict(\n            filtered_mus=filtered_mus,\n            filtered_LSigmas=filtered_LSigmas,\n            last_filtered_dist=(mu, LSigma),\n            log_prob=log_prob,\n            cached_mus=cached_mus,\n            cached_LSigmas=cached_LSigmas,\n            cached_timestamps=cached_timestamps,\n        )\n\n    def emit(self, z_t: Tensor) -> Tensor:\n        \"\"\"Emit an observation from the latent state.\n\n        Parameters\n        ----------\n        z_t\n            The latent state at a specific time, of shape (batch_size, z_dim)\n\n        Returns\n        -------\n            The observation at a specific time, of shape (batch_size, y_dim)\n        \"\"\"\n        R = self.R\n        H = self.H[None]\n        p_v = MultivariateNormal(\n            torch.zeros(1, self.y_dim, device=z_t.device), covariance_matrix=R\n        )\n        v = p_v.sample((z_t.shape[0],)).view(z_t.shape[0], self.y_dim)\n        y = bmbvp(H, z_t) + v\n        return y\n\n    @torch.no_grad()\n    def forecast(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        past_times: Tensor,\n        future_times: Tensor,\n        num_samples: int = 80,\n        no_state_sampling: bool = False,\n        use_smooth: bool = False,\n        **unused_kwargs,\n    ) -> Dict[str, Tensor]:\n        \"\"\"Make predictions (imputation and forecast) using the observed data.\n\n        Parameters\n        ----------\n        y\n            The tensor of observations, of shape (batch_size, num_timesteps, y_dim)\n        mask\n            The mask of missing values (1: observed, 0: missing),\n            of shape (batch_size, num_timesteps, y_dim), if sporadic,\n            else (batch_size, num_timesteps)\n        past_times\n            The times of the past observations, of shape (num_past_steps,)\n        future_times\n            The times of the forecast, of shape (num_forecast_steps,)\n        num_samples, optional\n            The number of sample paths to draw, by default 80\n        no_state_sampling, optional\n            Whether to sample from the predicted state distributions,\n            by default False and only uses the means of the distributions\n        use_smooth, optional\n            Whether to perform smoothing after filtering (useful for imputation),\n            by default False\n\n        Returns\n        -------\n            The reconstructed context (imputing values, if required) and the forecast\n        \"\"\"\n        B, _, _ = y.shape\n        filter_result = self.filter(y, mask, past_times, cache_params=use_smooth)\n        filtered_mus = filter_result[\"filtered_mus\"]\n        filtered_LSigmas = filter_result[\"filtered_LSigmas\"]\n\n        if use_smooth:\n            cached_mus = filter_result[\"cached_mus\"]\n            cached_LSigmas = filter_result[\"cached_LSigmas\"]\n            cached_timestamps = filter_result[\"cached_timestamps\"]\n            smoothed_mus, smoothed_LSigmas = cont_disc_smooth(\n                filter_mus=cached_mus,\n                filter_LSigmas=cached_LSigmas,\n                filter_timestamps=cached_timestamps,\n                dynamics=self.dynamics,\n                method=self.integration_method,\n            )\n            relevant_indices = torch.isin(cached_timestamps, past_times)\n            assert torch.allclose(cached_timestamps[relevant_indices], past_times)\n            filtered_mus, filtered_LSigmas = (\n                smoothed_mus[relevant_indices],\n                smoothed_LSigmas[relevant_indices],\n            )\n\n        filter_dists = MultivariateNormal(filtered_mus, scale_tril=filtered_LSigmas)\n        z_filtered = filter_dists.sample([num_samples])\n        # z_filtered.shape = num_samples x T x B x z_dim\n\n        z_reconstruction = []\n        y_reconstruction = []\n        for i, t in enumerate(past_times):\n            z_t = z_filtered[:, i].reshape(-1, self.z_dim)\n            y_t = self.emit(z_t)\n            z_reconstruction.append(z_t)\n            y_reconstruction.append(y_t)\n\n        z_reconstruction: Tensor = torch.stack(z_reconstruction)\n        y_reconstruction: Tensor = torch.stack(y_reconstruction)\n\n        z_reconstruction = z_reconstruction.view(\n            past_times.shape[0], num_samples, B, self.z_dim\n        )\n        y_reconstruction = y_reconstruction.view(\n            past_times.shape[0], num_samples, B, self.y_dim\n        )\n        z_reconstruction = z_reconstruction.permute(1, 2, 0, 3)\n        y_reconstruction = y_reconstruction.permute(1, 2, 0, 3)\n\n        (mu, LSigma) = filter_result[\"last_filtered_dist\"]\n        future_times = torch.cat([past_times[-1:], future_times], 0)\n        mu_t = mu.repeat(num_samples, 1)\n        LSigma_t = LSigma.repeat(num_samples, 1, 1)\n        y_forecast = []\n        z_forecast = []\n        for t1, t2 in zip(future_times[:-1], future_times[1:]):\n            mu_t2, LSigma_t2, _ = self.predict_step(\n                mu=mu_t,\n                LSigma=LSigma_t,\n                dynamics=self.dynamics,\n                t0=t1.item(),\n                t1=t2.item(),\n                step_size=self.integration_step_size,\n                method=self.integration_method,\n            )\n            pred_dist = MultivariateNormal(mu_t2, scale_tril=LSigma_t2)\n            z_t2 = pred_dist.mean if no_state_sampling else pred_dist.sample()\n            y_t2 = self.emit(z_t2)\n            z_forecast.append(z_t2)\n            y_forecast.append(y_t2)\n            mu_t = mu_t2\n            LSigma_t = LSigma_t2\n\n        z_forecast = torch.stack(z_forecast)\n        y_forecast = torch.stack(y_forecast)\n        z_forecast = z_forecast.view(\n            future_times.shape[0] - 1, num_samples, B, self.z_dim\n        )\n        y_forecast = y_forecast.view(\n            future_times.shape[0] - 1, num_samples, B, self.y_dim\n        )\n        z_forecast = z_forecast.permute(1, 2, 0, 3)\n        y_forecast = y_forecast.permute(1, 2, 0, 3)\n        return dict(\n            reconstruction=y_reconstruction,\n            forecast=y_forecast,\n            z_reconstruction=z_reconstruction,\n            z_forecast=z_forecast,\n        )", "\nclass Base(ABC):\n    @abstractmethod\n    def predict_step(\n        self,\n        mu: Tensor,\n        LSigma: Tensor,\n        dynamics: Union[ContinuousLTI, ContinuousNL, ContinuousLL],\n        t0: float,\n        t1: float,\n        step_size: float,\n        method: str,\n        cache_params: bool = False,\n        min_step_size: float = 1e-5,\n    ):\n        pass\n\n    @abstractmethod\n    def update_step(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        mu_pred: Tensor,\n        LSigma_pred: Tensor,\n        H: Tensor,\n        R: Tensor,\n        sporadic: bool = False,\n    ):\n        pass\n\n    def filter(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        times: Tensor,\n        cache_params: bool = False,\n    ) -> Dict[str, Tensor]:\n        \"\"\"The filter step of the continuous-discrete model.\n\n        Parameters\n        ----------\n        y\n            The tensor of observations, of shape (batch_size, num_timesteps, y_dim)\n        mask\n            The mask of missing values (1: observed, 0: missing),\n            of shape (batch_size, num_timesteps, y_dim), if sporadic,\n            else (batch_size, num_timesteps)\n        times\n            The tensor observations times, of shape (num_timesteps,)\n        cache_params, optional\n            Whether to cache the intermediate distributions computed during\n            predict step, by default False and all distrbutions at observation ``times``\n            are cached in any case,\n            should be set to True if you're planning to use smoothing\n\n        Returns\n        -------\n            The dictionary of filter outputs, including the log-likelihood\n            and the parameters of filtered distributions\n        \"\"\"\n        if self.sporadic:\n            assert (\n                y.size() == mask.size()\n            ), f\"Shapes of y ({y.size()}) and mask ({mask.size()}) should match!\"\n        else:\n            assert y.size()[:-1] == mask.size(), (\n                f\"Shapes of y ({y.size()}) and mask ({mask.size()})\"\n                \" should match except in last dim!\"\n            )\n        batch_size = y.size(0)\n\n        # Assumes that t = 0 is in times\n        assert times[0] == 0.0, \"First timestep should be 0!\"\n        mu_pred = self.mu0[None].repeat(batch_size, 1)\n        LSigma_pred = cholesky(self.Sigma0[None]).repeat(batch_size, 1, 1)\n        log_prob = []\n\n        filtered_mus = []\n        filtered_LSigmas = []\n        cached_mus = []\n        cached_LSigmas = []\n        cached_timestamps = []\n\n        for idx, t in enumerate(times):\n            H = self.H[None]\n            R = self.R[None]\n            # UPDATE step\n            y_i = y[:, idx]\n            mask_i = mask[:, idx]\n            mu, LSigma, y_pred, LS = self.update_step(\n                y=y_i,\n                mask=mask_i,\n                mu_pred=mu_pred,\n                LSigma_pred=LSigma_pred,\n                H=H,\n                R=R,\n                sporadic=self.sporadic,\n            )\n            if not self.sporadic:\n                # Mask out updates\n                # For the sporadic case, masks are directly incorporated\n                # during the update step\n                # TODO: Modify update step such that masking is not required here\n                mu = mask_i[:, None] * mu + (1 - mask_i[:, None]) * mu_pred\n                LSigma = (\n                    mask_i[:, None, None] * LSigma\n                    + (1 - mask_i[:, None, None]) * LSigma_pred\n                )\n\n            # Calculate log_prob\n            if not self.sporadic:\n                dist = MultivariateNormal(loc=y_pred, scale_tril=LS)\n                log_prob.append(dist.log_prob(y_i) * mask_i)\n            else:\n                dist = MultivariateNormal(loc=y_pred * mask_i, scale_tril=LS)\n                log_prob.append(dist.log_prob(y_i * mask_i))\n\n            # Cache filtered distribution\n            filtered_mus.append(mu.clone())\n            filtered_LSigmas.append(LSigma.clone())\n            cached_mus.append(mu.detach().clone())\n            cached_LSigmas.append(LSigma.detach().clone())\n            cached_timestamps.append(t.item())\n            if idx == len(times) - 1:\n                break\n            # PREDICT step\n            (\n                mu_pred,\n                LSigma_pred,\n                (cached_mus_t, cached_LSigmas_t, cached_timestamps_t),\n            ) = self.predict_step(\n                mu=mu,\n                LSigma=LSigma,\n                dynamics=self.dynamics,\n                t0=t.item(),\n                t1=times[idx + 1].item(),\n                step_size=self.integration_step_size,\n                method=self.integration_method,\n                cache_params=cache_params,\n            )\n            cached_mus.extend(cached_mus_t)\n            cached_LSigmas.extend(cached_LSigmas_t)\n            cached_timestamps.extend(cached_timestamps_t)\n\n        filtered_mus: Tensor = torch.stack(filtered_mus)\n        filtered_LSigmas: Tensor = torch.stack(filtered_LSigmas)\n        cached_mus: Tensor = torch.stack(cached_mus)\n        cached_LSigmas: Tensor = torch.stack(cached_LSigmas)\n        cached_timestamps: Tensor = torch.tensor(\n            cached_timestamps, dtype=times.dtype, device=times.device\n        )\n        log_prob = torch.stack(log_prob).sum(0)\n        return dict(\n            filtered_mus=filtered_mus,\n            filtered_LSigmas=filtered_LSigmas,\n            last_filtered_dist=(mu, LSigma),\n            log_prob=log_prob,\n            cached_mus=cached_mus,\n            cached_LSigmas=cached_LSigmas,\n            cached_timestamps=cached_timestamps,\n        )\n\n    def emit(self, z_t: Tensor) -> Tensor:\n        \"\"\"Emit an observation from the latent state.\n\n        Parameters\n        ----------\n        z_t\n            The latent state at a specific time, of shape (batch_size, z_dim)\n\n        Returns\n        -------\n            The observation at a specific time, of shape (batch_size, y_dim)\n        \"\"\"\n        R = self.R\n        H = self.H[None]\n        p_v = MultivariateNormal(\n            torch.zeros(1, self.y_dim, device=z_t.device), covariance_matrix=R\n        )\n        v = p_v.sample((z_t.shape[0],)).view(z_t.shape[0], self.y_dim)\n        y = bmbvp(H, z_t) + v\n        return y\n\n    @torch.no_grad()\n    def forecast(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        past_times: Tensor,\n        future_times: Tensor,\n        num_samples: int = 80,\n        no_state_sampling: bool = False,\n        use_smooth: bool = False,\n        **unused_kwargs,\n    ) -> Dict[str, Tensor]:\n        \"\"\"Make predictions (imputation and forecast) using the observed data.\n\n        Parameters\n        ----------\n        y\n            The tensor of observations, of shape (batch_size, num_timesteps, y_dim)\n        mask\n            The mask of missing values (1: observed, 0: missing),\n            of shape (batch_size, num_timesteps, y_dim), if sporadic,\n            else (batch_size, num_timesteps)\n        past_times\n            The times of the past observations, of shape (num_past_steps,)\n        future_times\n            The times of the forecast, of shape (num_forecast_steps,)\n        num_samples, optional\n            The number of sample paths to draw, by default 80\n        no_state_sampling, optional\n            Whether to sample from the predicted state distributions,\n            by default False and only uses the means of the distributions\n        use_smooth, optional\n            Whether to perform smoothing after filtering (useful for imputation),\n            by default False\n\n        Returns\n        -------\n            The reconstructed context (imputing values, if required) and the forecast\n        \"\"\"\n        B, _, _ = y.shape\n        filter_result = self.filter(y, mask, past_times, cache_params=use_smooth)\n        filtered_mus = filter_result[\"filtered_mus\"]\n        filtered_LSigmas = filter_result[\"filtered_LSigmas\"]\n\n        if use_smooth:\n            cached_mus = filter_result[\"cached_mus\"]\n            cached_LSigmas = filter_result[\"cached_LSigmas\"]\n            cached_timestamps = filter_result[\"cached_timestamps\"]\n            smoothed_mus, smoothed_LSigmas = cont_disc_smooth(\n                filter_mus=cached_mus,\n                filter_LSigmas=cached_LSigmas,\n                filter_timestamps=cached_timestamps,\n                dynamics=self.dynamics,\n                method=self.integration_method,\n            )\n            relevant_indices = torch.isin(cached_timestamps, past_times)\n            assert torch.allclose(cached_timestamps[relevant_indices], past_times)\n            filtered_mus, filtered_LSigmas = (\n                smoothed_mus[relevant_indices],\n                smoothed_LSigmas[relevant_indices],\n            )\n\n        filter_dists = MultivariateNormal(filtered_mus, scale_tril=filtered_LSigmas)\n        z_filtered = filter_dists.sample([num_samples])\n        # z_filtered.shape = num_samples x T x B x z_dim\n\n        z_reconstruction = []\n        y_reconstruction = []\n        for i, t in enumerate(past_times):\n            z_t = z_filtered[:, i].reshape(-1, self.z_dim)\n            y_t = self.emit(z_t)\n            z_reconstruction.append(z_t)\n            y_reconstruction.append(y_t)\n\n        z_reconstruction: Tensor = torch.stack(z_reconstruction)\n        y_reconstruction: Tensor = torch.stack(y_reconstruction)\n\n        z_reconstruction = z_reconstruction.view(\n            past_times.shape[0], num_samples, B, self.z_dim\n        )\n        y_reconstruction = y_reconstruction.view(\n            past_times.shape[0], num_samples, B, self.y_dim\n        )\n        z_reconstruction = z_reconstruction.permute(1, 2, 0, 3)\n        y_reconstruction = y_reconstruction.permute(1, 2, 0, 3)\n\n        (mu, LSigma) = filter_result[\"last_filtered_dist\"]\n        future_times = torch.cat([past_times[-1:], future_times], 0)\n        mu_t = mu.repeat(num_samples, 1)\n        LSigma_t = LSigma.repeat(num_samples, 1, 1)\n        y_forecast = []\n        z_forecast = []\n        for t1, t2 in zip(future_times[:-1], future_times[1:]):\n            mu_t2, LSigma_t2, _ = self.predict_step(\n                mu=mu_t,\n                LSigma=LSigma_t,\n                dynamics=self.dynamics,\n                t0=t1.item(),\n                t1=t2.item(),\n                step_size=self.integration_step_size,\n                method=self.integration_method,\n            )\n            pred_dist = MultivariateNormal(mu_t2, scale_tril=LSigma_t2)\n            z_t2 = pred_dist.mean if no_state_sampling else pred_dist.sample()\n            y_t2 = self.emit(z_t2)\n            z_forecast.append(z_t2)\n            y_forecast.append(y_t2)\n            mu_t = mu_t2\n            LSigma_t = LSigma_t2\n\n        z_forecast = torch.stack(z_forecast)\n        y_forecast = torch.stack(y_forecast)\n        z_forecast = z_forecast.view(\n            future_times.shape[0] - 1, num_samples, B, self.z_dim\n        )\n        y_forecast = y_forecast.view(\n            future_times.shape[0] - 1, num_samples, B, self.y_dim\n        )\n        z_forecast = z_forecast.permute(1, 2, 0, 3)\n        y_forecast = y_forecast.permute(1, 2, 0, 3)\n        return dict(\n            reconstruction=y_reconstruction,\n            forecast=y_forecast,\n            z_reconstruction=z_reconstruction,\n            z_forecast=z_forecast,\n        )", "\n\nclass BaseLTI(nn.Module, Base):\n    \"\"\"Base continuous-discrete linear time-invariant state space model.\n\n    Parameters\n    ----------\n    z_dim\n        The dimension of latent state z\n    y_dim\n        The dimension of observation y\n    u_dim\n        The dimension of control input u\n    F, optional\n        The linear dynamics matrix F, by default None\n    B, optional\n        The linear control matrix B, by default None\n    Q, optional\n        The state covariance matrix Q, by default None\n    H, optional\n        The observation matrix H, by default None\n    R, optional\n        The observation covariance matrix R, by default None\n    mu0, optional\n        The mean of the initial state distribution, by default None\n    Sigma0, optional\n        The covariance of the initial state distribution, by default None\n    integration_method, optional\n        The ODE integration method, should be one of \"euler\" or \"rk4\", by default \"rk4\"\n    integration_step_size, optional\n        The ODE integration step size, by default 0.1\n    sporadic, optional\n        A flag to indicate whether the dataset is sporadic,\n        i.e., with values missing in both time and feature dimensions, by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        z_dim: int,\n        y_dim: int,\n        u_dim: int,\n        F: Optional[Tensor] = None,\n        B: Optional[Tensor] = None,\n        Q: Optional[Tensor] = None,\n        H: Optional[Tensor] = None,\n        R: Optional[Tensor] = None,\n        mu0: Optional[Tensor] = None,\n        Sigma0: Optional[Tensor] = None,\n        integration_method: str = \"rk4\",\n        integration_step_size: float = 0.1,\n        sporadic: bool = False,\n    ):\n        super().__init__()\n        self.z_dim = z_dim\n        self.y_dim = y_dim\n        self.u_dim = u_dim\n        self.integration_method = integration_method\n        self.integration_step_size = integration_step_size\n        self.sporadic = sporadic\n\n        self.mu0 = nn.Parameter(\n            mu0\n            if mu0 is not None\n            else torch.zeros(\n                z_dim,\n            ),\n        )\n        self.uSigma0 = nn.Parameter(\n            torch.log(Sigma0)\n            if Sigma0 is not None\n            else torch.zeros(\n                z_dim,\n            )\n        )\n\n        F = (\n            F\n            if F is not None\n            else (\n                skew_symmetric_init_(torch.empty(z_dim, z_dim))\n                if sporadic\n                else nn.init.xavier_uniform_(torch.empty(z_dim, z_dim))\n            )\n        )\n\n        uQ = (\n            torch.log(Q)\n            if Q is not None\n            else torch.zeros(\n                z_dim,\n            )\n        )\n\n        self.dynamics = ContinuousLTI(z_dim=z_dim, u_dim=u_dim, F=F, B=B, uQ=uQ)\n\n        if H is not None:\n            self.register_buffer(\"H\", H)\n        else:\n            self.H = nn.Parameter(nn.init.xavier_uniform_(torch.empty(y_dim, z_dim)))\n        self.uR = nn.Parameter(\n            torch.log(R)\n            if R is not None\n            else torch.zeros(\n                y_dim,\n            )\n        )\n        self.register_buffer(\"I\", torch.eye(z_dim))\n\n    @property\n    def Sigma0(self):\n        return torch.diag(torch.clamp(torch.exp(self.uSigma0), min=1e-4))\n\n    @property\n    def R(self):\n        return torch.diag(torch.clamp(torch.exp(self.uR), min=1e-4))\n\n    def forward(\n        self, y: Tensor, mask: Tensor, times: Tensor, **unused_kwargs\n    ) -> Dict[str, Tensor]:\n        likelihood = self.filter(y, mask, times)[\"log_prob\"]\n        regularizer = torch.tensor(0.0)\n        return dict(likelihood=likelihood, regularizer=regularizer)\n\n    def predict_step(\n        self,\n        mu: Tensor,\n        LSigma: Tensor,\n        dynamics: Union[ContinuousLTI, ContinuousNL, ContinuousLL],\n        t0: float,\n        t1: float,\n        step_size: float,\n        method: str,\n        cache_params: bool = False,\n        min_step_size: float = 1e-5,\n    ):\n        return cont_disc_linear_predict(\n            mu,\n            LSigma,\n            dynamics,\n            t0,\n            t1,\n            step_size,\n            method,\n            cache_params=cache_params,\n            min_step_size=min_step_size,\n        )\n\n    def update_step(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        mu_pred: Tensor,\n        LSigma_pred: Tensor,\n        H: Tensor,\n        R: Tensor,\n        sporadic: bool = False,\n    ):\n        return cont_disc_linear_update(y, mask, mu_pred, LSigma_pred, H, R, sporadic)", "\n\nclass BaseLL(nn.Module, Base):\n    \"\"\"Base continuous-discrete locally-linear state space model.\n\n    Parameters\n    ----------\n    K\n        The number of base matrices (i.e., dynamics)\n    z_dim\n        The dimension of latent state z\n    y_dim\n        The dimension of observation y\n    u_dim\n        The dimension of control input u\n    alpha_net\n        A mixing network that takes the state `z` as input\n        and outputs the mixing coefficients for the base dynamics\n    F, optional\n        The linear dynamics matrix F, by default None\n    B, optional\n        The linear control matrix B, by default None\n    Q, optional\n        The state covariance matrix Q, by default None\n    H, optional\n        The observation matrix H, by default None\n    R, optional\n        The observation covariance matrix R, by default None\n    mu0, optional\n        The mean of the initial state distribution, by default None\n    Sigma0, optional\n        The covariance of the initial state distribution, by default None\n    integration_method, optional\n        The ODE integration method, should be one of \"euler\" or \"rk4\", by default \"rk4\"\n    integration_step_size, optional\n        The ODE integration step size, by default 0.1\n    sporadic, optional\n        A flag to indicate whether the dataset is sporadic,\n        i.e., with values missing in both time and feature dimensions, by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        K: int,\n        z_dim: int,\n        y_dim: int,\n        u_dim: int,\n        alpha_net: nn.Module,\n        F: Optional[Tensor] = None,\n        B: Optional[Tensor] = None,\n        Q: Optional[Tensor] = None,\n        H: Optional[Tensor] = None,\n        R: Optional[Tensor] = None,\n        mu0: Optional[Tensor] = None,\n        Sigma0: Optional[Tensor] = None,\n        integration_method: str = \"rk4\",\n        integration_step_size: float = 0.1,\n        sporadic: bool = False,\n    ):\n        super().__init__()\n\n        self.K = K\n        self.z_dim = z_dim\n        self.y_dim = y_dim\n        self.u_dim = u_dim\n        self.integration_method = integration_method\n        self.integration_step_size = integration_step_size\n        self.sporadic = sporadic\n\n        self.mu0 = nn.Parameter(\n            mu0\n            if mu0 is not None\n            else torch.zeros(\n                z_dim,\n            ),\n        )  # shared across the K base dynamics\n        self.uSigma0 = nn.Parameter(\n            torch.log(Sigma0)\n            if Sigma0 is not None\n            else torch.zeros(\n                z_dim,\n            )\n        )  # shared across the K base dynamics\n\n        F = F if F is not None else nn.init.orthogonal_(torch.empty(K, z_dim, z_dim))\n\n        uQ = (\n            torch.log(Q)\n            if Q is not None\n            else torch.zeros(\n                z_dim,\n            )\n        )  # shared across the K base dynamics\n\n        self.dynamics = ContinuousLL(\n            z_dim=z_dim,\n            u_dim=u_dim,\n            K=K,\n            F=F,\n            B=B,\n            uQ=uQ,\n            alpha_net=alpha_net,\n        )\n        # H is shared across the K base dynamics\n        if H is not None:\n            self.register_buffer(\"H\", H)\n        else:\n            self.H = nn.Parameter(nn.init.xavier_uniform_(torch.empty(y_dim, z_dim)))\n        self.uR = nn.Parameter(\n            torch.log(R)\n            if R is not None\n            else torch.zeros(\n                y_dim,\n            )\n        )  # shared across the K base dynamics\n        self.register_buffer(\"I\", torch.eye(z_dim))\n\n    @property\n    def Sigma0(self):\n        return torch.diag(torch.clamp(torch.exp(self.uSigma0), min=1e-4))\n\n    @property\n    def R(self):\n        return torch.diag(torch.clamp(torch.exp(self.uR), min=1e-4))\n\n    def forward(self, y: Tensor, mask: Tensor, times: Tensor, **unused_kwargs):\n        likelihood = self.filter(y, mask, times)[\"log_prob\"]\n        regularizer = torch.tensor(0.0)\n        return dict(likelihood=likelihood, regularizer=regularizer)\n\n    def predict_step(\n        self,\n        mu: Tensor,\n        LSigma: Tensor,\n        dynamics: Union[ContinuousLTI, ContinuousNL, ContinuousLL],\n        t0: float,\n        t1: float,\n        step_size: float,\n        method: str,\n        cache_params: bool = False,\n        min_step_size: float = 1e-5,\n    ):\n        return cont_disc_locallylinear_predict(\n            mu,\n            LSigma,\n            dynamics,\n            t0,\n            t1,\n            step_size,\n            method,\n            cache_params=cache_params,\n            min_step_size=min_step_size,\n        )\n\n    def update_step(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        mu_pred: Tensor,\n        LSigma_pred: Tensor,\n        H: Tensor,\n        R: Tensor,\n        sporadic: bool = False,\n    ):\n        return cont_disc_locallylinear_update(\n            y, mask, mu_pred, LSigma_pred, H, R, sporadic\n        )\n\n    @torch.no_grad()\n    def forecast(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        past_times: Tensor,\n        future_times: Tensor,\n        num_samples: int = 80,\n        no_state_sampling: bool = False,\n        use_smooth: bool = False,\n        **unused_kwargs,\n    ):\n        \"\"\"Same as :func:`Base.forecast` but additionally returns\n        the mixing coefficients (alpha).\n        \"\"\"\n        B, T, _ = y.shape\n        filter_result = self.filter(y, mask, past_times, cache_params=use_smooth)\n        filtered_mus = filter_result[\"filtered_mus\"]\n        filtered_LSigmas = filter_result[\"filtered_LSigmas\"]\n\n        if use_smooth:\n            cached_mus = filter_result[\"cached_mus\"]\n            cached_LSigmas = filter_result[\"cached_LSigmas\"]\n            cached_timestamps = filter_result[\"cached_timestamps\"]\n            smoothed_mus, smoothed_LSigmas = cont_disc_smooth(\n                filter_mus=cached_mus,\n                filter_LSigmas=cached_LSigmas,\n                filter_timestamps=cached_timestamps,\n                dynamics=self.dynamics,\n                method=self.integration_method,\n            )\n            relevant_indices = torch.isin(cached_timestamps, past_times)\n            assert torch.allclose(cached_timestamps[relevant_indices], past_times)\n            filtered_mus, filtered_LSigmas = (\n                smoothed_mus[relevant_indices],\n                smoothed_LSigmas[relevant_indices],\n            )\n\n        filter_dists = MultivariateNormal(filtered_mus, scale_tril=filtered_LSigmas)\n        z_filtered = filter_dists.sample([num_samples])\n        # z_filtered.shape = num_samples x T x B x z_dim\n        z_reconstruction = []\n        y_reconstruction = []\n        alpha_reconstruction = []\n        for i, t in enumerate(past_times):\n            z_t = z_filtered[:, i].reshape(-1, self.z_dim)\n            alpha_t = self.dynamics.alpha_net(z_t)\n            y_t = self.emit(z_t)\n            z_reconstruction.append(z_t)\n            alpha_reconstruction.append(alpha_t)\n            y_reconstruction.append(y_t)\n        z_reconstruction = torch.stack(z_reconstruction)\n        alpha_reconstruction = torch.stack(alpha_reconstruction)\n        y_reconstruction = torch.stack(y_reconstruction)\n\n        z_reconstruction = z_reconstruction.view(\n            past_times.shape[0], num_samples, B, self.z_dim\n        )\n        alpha_reconstruction = alpha_reconstruction.view(\n            past_times.shape[0], num_samples, B, self.K\n        )\n        y_reconstruction = y_reconstruction.view(\n            past_times.shape[0], num_samples, B, self.y_dim\n        )\n        z_reconstruction = z_reconstruction.permute(1, 2, 0, 3)\n        alpha_reconstruction = alpha_reconstruction.permute(1, 2, 0, 3)\n        y_reconstruction = y_reconstruction.permute(1, 2, 0, 3)\n\n        (mu, LSigma) = filter_result[\"last_filtered_dist\"]\n        future_times = torch.cat([past_times[-1:], future_times], 0)\n        mu_t = mu.repeat(num_samples, 1)\n        LSigma_t = LSigma.repeat(num_samples, 1, 1)\n\n        y_forecast = []\n        alpha_forecast = []\n        z_forecast = []\n        for t1, t2 in zip(future_times[:-1], future_times[1:]):\n            mu_t2, LSigma_t2, _ = self.predict_step(\n                mu=mu_t,\n                LSigma=LSigma_t,\n                dynamics=self.dynamics,\n                t0=t1.item(),\n                t1=t2.item(),\n                step_size=self.integration_step_size,\n                method=self.integration_method,\n            )\n\n            pred_dist = MultivariateNormal(mu_t2, scale_tril=LSigma_t2)\n            z_t2 = pred_dist.mean if no_state_sampling else pred_dist.sample()\n            alpha_t2 = self.dynamics.alpha_net(z_t2)\n            y_t2 = self.emit(z_t2)\n            z_forecast.append(z_t2)\n            alpha_forecast.append(alpha_t2)\n            y_forecast.append(y_t2)\n            mu_t = mu_t2\n            LSigma_t = LSigma_t2\n\n        z_forecast = torch.stack(z_forecast)\n        alpha_forecast = torch.stack(alpha_forecast)\n        y_forecast = torch.stack(y_forecast)\n        z_forecast = z_forecast.view(\n            future_times.shape[0] - 1, num_samples, B, self.z_dim\n        )\n        alpha_forecast = alpha_forecast.view(\n            future_times.shape[0] - 1, num_samples, B, self.K\n        )\n        y_forecast = y_forecast.view(\n            future_times.shape[0] - 1, num_samples, B, self.y_dim\n        )\n        z_forecast = z_forecast.permute(1, 2, 0, 3)\n        alpha_forecast = alpha_forecast.permute(1, 2, 0, 3)\n        y_forecast = y_forecast.permute(1, 2, 0, 3)\n\n        return dict(\n            reconstruction=y_reconstruction,\n            forecast=y_forecast,\n            z_reconstruction=z_reconstruction,\n            z_forecast=z_forecast,\n            alpha_reconstruction=alpha_reconstruction,\n            alpha_forecast=alpha_forecast,\n        )", "\n\nclass BaseNL(nn.Module, Base):\n    \"\"\"Base continuous-discrete non-linear state space model.\n\n    Parameters\n    ----------\n    z_dim\n        The dimension of latent state z\n    y_dim\n        The dimension of observation y\n    u_dim\n        The dimension of control input u\n    f, optional\n        The dynamics/drift function f(z), by default None\n    gs, optional\n        The list of diffusion functions g(z), one for each z_dim, by default None\n    B, optional\n        The linear control matrix B, by default None\n    H, optional\n        The observation matrix H, by default None\n    R, optional\n        The observation covariance matrix R, by default None\n    mu0, optional\n        The mean of the initial state distribution, by default None\n    Sigma0, optional\n        The covariance of the initial state distribution, by default None\n    integration_method, optional\n        The ODE integration method, should be one of \"euler\" or \"rk4\", by default \"rk4\"\n    integration_step_size, optional\n        The ODE integration step size, by default 0.1\n    sporadic, optional\n        A flag to indicate whether the dataset is sporadic,\n        i.e., with values missing in both time and feature dimensions, by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        z_dim: int,\n        y_dim: int,\n        u_dim: int,\n        f: nn.Module,\n        gs: Optional[List[nn.Module]] = None,\n        B: Optional[Tensor] = None,\n        H: Optional[Tensor] = None,\n        R: Optional[Tensor] = None,\n        mu0: Optional[Tensor] = None,\n        Sigma0: Optional[Tensor] = None,\n        integration_method: str = \"rk4\",\n        integration_step_size: float = 0.1,\n        sporadic: bool = False,\n    ):\n        super().__init__()\n\n        self.z_dim = z_dim\n        self.y_dim = y_dim\n        self.u_dim = u_dim\n        self.integration_method = integration_method\n        self.integration_step_size = integration_step_size\n        self.sporadic = sporadic\n\n        self.mu0 = nn.Parameter(\n            mu0\n            if mu0 is not None\n            else nn.init.normal_(\n                torch.empty(\n                    z_dim,\n                ),\n                std=0.01,\n            )\n        )  # shared\n        self.uSigma0 = nn.Parameter(\n            torch.log(Sigma0)\n            if Sigma0 is not None\n            else torch.zeros(\n                z_dim,\n            )\n        )  # shared\n\n        self.dynamics = ContinuousNL(z_dim=z_dim, u_dim=u_dim, f=f, gs=gs, B=B)\n\n        # TODO: Fix initialization of params\n        # If passed as args, they should be fixed\n        if H is not None:\n            self.register_buffer(\"H\", H)\n        else:\n            self.H = nn.Parameter(nn.init.xavier_uniform_(torch.empty(y_dim, z_dim)))\n        self.uR = nn.Parameter(\n            torch.log(R)\n            if R is not None\n            else torch.zeros(\n                y_dim,\n            )\n        )  # shared\n        self.register_buffer(\"I\", torch.eye(z_dim))\n\n    @property\n    def Sigma0(self):\n        return torch.diag(torch.clamp(torch.exp(self.uSigma0), min=1e-4))\n\n    @property\n    def R(self):\n        return torch.diag(torch.clamp(torch.exp(self.uR), min=1e-4))\n\n    def forward(self, y: Tensor, mask: Tensor, times: Tensor, **unused_kwargs):\n        likelihood = self.filter(y, mask, times)[\"log_prob\"]\n        regularizer = torch.tensor(0.0)\n        return dict(likelihood=likelihood, regularizer=regularizer)\n\n    def predict_step(\n        self,\n        mu: Tensor,\n        LSigma: Tensor,\n        dynamics: Union[ContinuousLTI, ContinuousNL, ContinuousLL],\n        t0: float,\n        t1: float,\n        step_size: float,\n        method: str,\n        cache_params: bool = False,\n        min_step_size: float = 1e-5,\n    ):\n        return cont_disc_nonlinear_predict(\n            mu,\n            LSigma,\n            dynamics,\n            t0,\n            t1,\n            step_size,\n            method,\n            cache_params=cache_params,\n            min_step_size=min_step_size,\n        )\n\n    def update_step(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        mu_pred: Tensor,\n        LSigma_pred: Tensor,\n        H: Tensor,\n        R: Tensor,\n        sporadic: bool = False,\n    ):\n        return cont_disc_nonlinear_update(y, mask, mu_pred, LSigma_pred, H, R, sporadic)", ""]}
{"filename": "src/ncdssm/models/components.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport torch.distributions as td\n\nfrom torch.nn.functional import softplus\n\nfrom ..functions import inverse_softplus\n\n\nclass GaussianOutput(nn.Module):\n    def __init__(\n        self,\n        network,\n        dist_dim,\n        use_tied_cov=False,\n        use_trainable_cov=True,\n        sigma=0.1,\n        use_independent=True,\n        scale_function=\"exp\",\n        min_scale=0.01,\n    ):\n        super().__init__()\n        assert scale_function in {\"exp\", \"softplus\"}\n        self.scale_function = scale_function\n        self.dist_dim = dist_dim\n        self.network = network\n        self.use_tied_cov = use_tied_cov\n        self.use_trainable_cov = use_trainable_cov\n        self.use_independent = use_independent\n        self.min_scale = min_scale\n        if not self.use_trainable_cov:\n            self.sigma = sigma\n        if self.use_trainable_cov and self.use_tied_cov:\n            if scale_function == \"softplus\":\n                init_sigma = inverse_softplus(torch.full([1, dist_dim], sigma))\n            else:\n                init_sigma = torch.zeros([1, dist_dim])\n            self.usigma = nn.Parameter(init_sigma)\n\n    def forward(self, tensor):\n        args_tensor = self.network(tensor)\n        mean_tensor = args_tensor[..., : self.dist_dim]\n        if self.use_trainable_cov:\n            if self.use_tied_cov:\n                if self.scale_function == \"softplus\":\n                    scale_tensor = softplus(self.usigma)\n                else:\n                    scale_tensor = torch.exp(self.usigma)\n                scale_tensor = torch.clamp(scale_tensor, min=self.min_scale)\n                out_dist = td.normal.Normal(mean_tensor, scale_tensor)\n                if self.use_independent:\n                    out_dist = td.independent.Independent(out_dist, 1)\n            else:\n                if self.scale_function == \"softplus\":\n                    scale_tensor = softplus(args_tensor[..., self.dist_dim :])\n                else:\n                    scale_tensor = torch.exp(args_tensor[..., self.dist_dim :])\n                scale_tensor = torch.clamp(scale_tensor, min=self.min_scale)\n                out_dist = td.normal.Normal(mean_tensor, scale_tensor)\n                if self.use_independent:\n                    out_dist = td.independent.Independent(out_dist, 1)\n        else:\n            out_dist = td.normal.Normal(mean_tensor, self.sigma)\n            if self.use_independent:\n                out_dist = td.independent.Independent(out_dist, 1)\n        return out_dist", "\nclass GaussianOutput(nn.Module):\n    def __init__(\n        self,\n        network,\n        dist_dim,\n        use_tied_cov=False,\n        use_trainable_cov=True,\n        sigma=0.1,\n        use_independent=True,\n        scale_function=\"exp\",\n        min_scale=0.01,\n    ):\n        super().__init__()\n        assert scale_function in {\"exp\", \"softplus\"}\n        self.scale_function = scale_function\n        self.dist_dim = dist_dim\n        self.network = network\n        self.use_tied_cov = use_tied_cov\n        self.use_trainable_cov = use_trainable_cov\n        self.use_independent = use_independent\n        self.min_scale = min_scale\n        if not self.use_trainable_cov:\n            self.sigma = sigma\n        if self.use_trainable_cov and self.use_tied_cov:\n            if scale_function == \"softplus\":\n                init_sigma = inverse_softplus(torch.full([1, dist_dim], sigma))\n            else:\n                init_sigma = torch.zeros([1, dist_dim])\n            self.usigma = nn.Parameter(init_sigma)\n\n    def forward(self, tensor):\n        args_tensor = self.network(tensor)\n        mean_tensor = args_tensor[..., : self.dist_dim]\n        if self.use_trainable_cov:\n            if self.use_tied_cov:\n                if self.scale_function == \"softplus\":\n                    scale_tensor = softplus(self.usigma)\n                else:\n                    scale_tensor = torch.exp(self.usigma)\n                scale_tensor = torch.clamp(scale_tensor, min=self.min_scale)\n                out_dist = td.normal.Normal(mean_tensor, scale_tensor)\n                if self.use_independent:\n                    out_dist = td.independent.Independent(out_dist, 1)\n            else:\n                if self.scale_function == \"softplus\":\n                    scale_tensor = softplus(args_tensor[..., self.dist_dim :])\n                else:\n                    scale_tensor = torch.exp(args_tensor[..., self.dist_dim :])\n                scale_tensor = torch.clamp(scale_tensor, min=self.min_scale)\n                out_dist = td.normal.Normal(mean_tensor, scale_tensor)\n                if self.use_independent:\n                    out_dist = td.independent.Independent(out_dist, 1)\n        else:\n            out_dist = td.normal.Normal(mean_tensor, self.sigma)\n            if self.use_independent:\n                out_dist = td.independent.Independent(out_dist, 1)\n        return out_dist", "\n\nclass BernoulliOutput(nn.Module):\n    def __init__(self, network, dist_dim, use_indepedent=True):\n        super().__init__()\n        self.network = network\n        self.dist_dim = dist_dim\n        self.use_indepedent = use_indepedent\n\n    def forward(self, x):\n        h = self.network(x)\n        assert h.shape[-1] == self.dist_dim\n        dist = td.Bernoulli(logits=h)\n        if self.use_indepedent:\n            dist = td.Independent(dist, 1)\n        return dist", "\n\nclass AuxInferenceModel(nn.Module):\n    def __init__(\n        self,\n        base_network: nn.Module,\n        dist_network: nn.Module,\n        aux_dim: int,\n        concat_mask: bool = False,\n    ):\n        super().__init__()\n        self.aux_dim = aux_dim\n        self.base_network = base_network\n        self.dist_network = dist_network\n        self.concat_mask = concat_mask\n\n    def forward(\n        self,\n        y: torch.Tensor,\n        mask: torch.Tensor,\n        num_samples: int = 1,\n        deterministic: bool = False,\n    ):\n        assert y.ndim >= 3\n        if self.concat_mask:\n            assert y.size() == mask.size()\n            # Concatenate mask to feature dim\n            y = torch.cat([y, mask], dim=-1)\n            # Convert feature mask to at least one feature presence mask\n            # i.e. mask = 1 if at least one feature is present at that time\n            # step.\n            mask = (mask.sum(dim=-1) > 0).float()\n\n        sporadic = False\n        if y.ndim > mask.ndim:\n            mask = mask.unsqueeze(-1)\n        else:\n            sporadic = True\n\n        h = self.base_network(y)\n        h = h * mask\n        dist = self.dist_network(h)\n        if deterministic:\n            aux_samples = dist.mean.unsqueeze(0)\n            one_args = (1,) * dist.mean.ndim\n            aux_samples = aux_samples.repeat(num_samples, *one_args)\n        else:\n            aux_samples = dist.rsample((num_samples,))\n\n        aux_samples = aux_samples * mask\n        if sporadic:\n            aux_entropy = (dist.base_dist.entropy() * mask).sum(dim=-1)\n            aux_log_prob = (dist.base_dist.log_prob(aux_samples) * mask).sum(dim=-1)\n        else:\n            aux_entropy = dist.entropy()\n            aux_log_prob = dist.log_prob(aux_samples)\n            aux_entropy = aux_entropy * mask.squeeze(-1)\n            aux_log_prob = aux_log_prob * mask.squeeze(-1)\n\n        aux_entropy = aux_entropy[None].repeat(num_samples, 1, 1)\n        return (\n            aux_samples,  # .shape = N, B, T, aux_dim\n            aux_entropy,  # .shape = N, B, T\n            aux_log_prob,  # .shape = N, B, T\n        )", ""]}
{"filename": "src/ncdssm/models/__init__.py", "chunked_list": ["from .base import BaseLTI, BaseLL, BaseNL\nfrom .ncdssm import NCDSSMLTI, NCDSSMLL, NCDSSMNL\n\n__all__ = [\n    \"BaseLTI\",\n    \"NCDSSMLTI\",\n    \"BaseLL\",\n    \"NCDSSMLL\",\n    \"BaseNL\",\n    \"NCDSSMNL\",", "    \"BaseNL\",\n    \"NCDSSMNL\",\n]\n"]}
{"filename": "src/ncdssm/models/ncdssm.py", "chunked_list": ["import torch\nimport torch.nn as nn\nimport numpy as np\n\nfrom .components import AuxInferenceModel\nfrom .base import BaseNL, BaseLTI, BaseLL\nfrom ..torch_utils import merge_leading_dims\nfrom ..type import Tensor, Optional, List\n\n\nclass NCDSSM(nn.Module):\n    \"\"\"Base class for NCDSSM models.\n\n    Parameters\n    ----------\n    aux_inference_net\n        The auxiliary inference model parameterized by a neural network\n    y_emission_net\n        The emission model parameterized by a neural network\n    aux_dim\n        The dimension of the auxiliary variables\n    z_dim\n        The dimension of the latent states\n    y_dim\n        The dimension of the observations\n    u_dim\n        The dimension of the control inputs\n    integration_method, optional\n        The integration method, can be one of \"euler\" or \"rk4\",\n        by default \"rk4\"\n    integration_step_size, optional\n        The integration step size, by default 0.1\n    sporadic, optional\n        A flag to indicate whether the dataset is sporadic,\n        i.e., with values missing in both time and feature dimensions,\n        by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        aux_inference_net: nn.Module,\n        y_emission_net: nn.Module,\n        aux_dim: int,\n        z_dim: int,\n        y_dim: int,\n        u_dim: int,\n        integration_method: str = \"rk4\",\n        integration_step_size: float = 0.1,\n        sporadic: bool = False,\n        **kwargs,\n    ):\n        super().__init__()\n        assert u_dim == 0, \"Support for control inputs is not implemented yet\"\n        self.aux_inference_net = aux_inference_net\n        self.y_emission_net = y_emission_net\n        self.aux_dim = aux_dim\n        self.z_dim = z_dim\n        self.y_dim = y_dim\n        self.u_dim = u_dim\n        self.integration_method = integration_method\n        self.integration_step_size = integration_step_size\n        self.sporadic = sporadic\n\n    def forward(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        times: Tensor,\n        num_samples: int = 1,\n        deterministic: bool = False,\n    ):\n        assert (\n            y.size() == mask.size() or y.size()[:-1] == mask.size()\n        ), \"Shapes of y and mask should match!\"\n        # Currently assumes that t = 0 is in times\n        assert times[0] == 0.0, \"First timestep should be 0!\"\n        batch_size = y.size(0)\n\n        aux_samples, aux_entropy, aux_post_log_prob = self.aux_inference_net(\n            y,\n            mask,\n            num_samples=num_samples,\n            deterministic=deterministic,\n        )\n        aux_samples = merge_leading_dims(aux_samples, ndims=2)\n        aux_entropy = merge_leading_dims(aux_entropy, ndims=2)\n        aux_post_log_prob = merge_leading_dims(aux_post_log_prob, ndims=2)\n        # Compute likelihoods\n        if mask.ndim < y.ndim:\n            mask = mask.unsqueeze(-1)\n        repeated_mask = mask.repeat(num_samples, 1, 1)\n        emission_dist = self.y_emission_net(aux_samples)\n        y_log_likelihood = emission_dist.log_prob(y.repeat(num_samples, 1, 1))\n        y_log_likelihood = y_log_likelihood * repeated_mask\n        y_log_likelihood = y_log_likelihood.sum(-1).sum(-1)\n        filter_result = self.base_ssm.filter(\n            aux_samples,\n            repeated_mask if self.sporadic else (repeated_mask.sum(-1) > 0).float(),\n            times,\n        )\n        aux_log_likelihood = filter_result[\"log_prob\"]\n        aux_entropy = aux_entropy.sum(dim=-1)\n        # Compute ELBO\n        regularizer = aux_log_likelihood + aux_entropy\n        elbo = y_log_likelihood + regularizer\n        # Compute IWELBO\n        iwelbo = torch.logsumexp(\n            y_log_likelihood.view(num_samples, batch_size)\n            + aux_log_likelihood.view(num_samples, batch_size)\n            - aux_post_log_prob.sum(dim=-1).view(num_samples, batch_size),\n            dim=0,\n        ) - np.log(num_samples)\n        return dict(\n            elbo=elbo,\n            iwelbo=iwelbo,\n            likelihood=y_log_likelihood,\n            regularizer=regularizer,\n        )\n\n    @torch.no_grad()\n    def forecast(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        past_times: Tensor,\n        future_times: Tensor,\n        num_samples: int = 80,\n        deterministic: bool = False,\n        no_state_sampling: bool = False,\n        use_smooth: bool = False,\n    ):\n        \"\"\"Make predictions (imputation and forecast) using the observed data.\n\n        Parameters\n        ----------\n        y\n            The tensor of observations, of shape (batch_size, num_timesteps, y_dim)\n        mask\n            The mask of missing values (1: observed, 0: missing),\n            of shape (batch_size, num_timesteps, y_dim), if sporadic,\n            else (batch_size, num_timesteps)\n        past_times\n            The times of the past observations, of shape (num_past_steps,)\n        future_times\n            The times of the forecast, of shape (num_forecast_steps,)\n        num_samples, optional\n            The number of sample paths to draw, by default 80\n        deterministic, optional\n            Whether to peform deterministic sampling from auxiliary model,\n            by default False (not really used)\n        no_state_sampling, optional\n            Whether to sample from the predicted state distributions,\n            by default False and only uses the means of the distributions\n        use_smooth, optional\n            Whether to perform smoothing after filtering (useful for imputation),\n            by default False\n\n        Returns\n        -------\n            The reconstructed context (imputing values, if required) and the forecast\n        \"\"\"\n        B, T, _ = y.shape\n\n        aux_samples, _, _ = self.aux_inference_net(\n            y,\n            mask,\n            num_samples=1,\n            deterministic=deterministic,\n        )\n        # aux_samples.shape = num_samples x B x time x aux_dim\n        aux_samples = merge_leading_dims(aux_samples, ndims=2)\n        if mask.ndim < y.ndim:\n            mask = mask.unsqueeze(-1)\n        # Generate predictions from the base CDKF\n        base_predictions = self.base_ssm.forecast(\n            aux_samples,\n            mask if self.sporadic else (mask.sum(-1) > 0).float(),\n            past_times,\n            future_times,\n            num_samples=num_samples,\n            no_state_sampling=no_state_sampling,\n            use_smooth=use_smooth,\n        )\n        aux_reconstruction = base_predictions[\"reconstruction\"]\n        aux_forecast = base_predictions[\"forecast\"]\n        z_reconstruction = base_predictions[\"z_reconstruction\"]\n        z_forecast = base_predictions[\"z_forecast\"]\n\n        # Decode aux --> y\n        reconstruction_emit_dist = self.y_emission_net(\n            merge_leading_dims(aux_reconstruction, ndims=2)\n        )\n        y_reconstruction = reconstruction_emit_dist.sample()\n        y_reconstruction = y_reconstruction.view(\n            num_samples, B, aux_reconstruction.shape[-2], self.y_dim\n        )\n        forecast_emit_dist = self.y_emission_net(\n            merge_leading_dims(aux_forecast, ndims=2)\n        )\n        y_forecast = forecast_emit_dist.sample()\n        y_forecast = y_forecast.view(num_samples, B, aux_forecast.shape[-2], self.y_dim)\n\n        return dict(\n            reconstruction=y_reconstruction,\n            forecast=y_forecast,\n            z_reconstruction=z_reconstruction,\n            z_forecast=z_forecast,\n            aux_reconstruction=aux_reconstruction,\n            aux_forecast=aux_forecast,\n        )", "\n\nclass NCDSSM(nn.Module):\n    \"\"\"Base class for NCDSSM models.\n\n    Parameters\n    ----------\n    aux_inference_net\n        The auxiliary inference model parameterized by a neural network\n    y_emission_net\n        The emission model parameterized by a neural network\n    aux_dim\n        The dimension of the auxiliary variables\n    z_dim\n        The dimension of the latent states\n    y_dim\n        The dimension of the observations\n    u_dim\n        The dimension of the control inputs\n    integration_method, optional\n        The integration method, can be one of \"euler\" or \"rk4\",\n        by default \"rk4\"\n    integration_step_size, optional\n        The integration step size, by default 0.1\n    sporadic, optional\n        A flag to indicate whether the dataset is sporadic,\n        i.e., with values missing in both time and feature dimensions,\n        by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        aux_inference_net: nn.Module,\n        y_emission_net: nn.Module,\n        aux_dim: int,\n        z_dim: int,\n        y_dim: int,\n        u_dim: int,\n        integration_method: str = \"rk4\",\n        integration_step_size: float = 0.1,\n        sporadic: bool = False,\n        **kwargs,\n    ):\n        super().__init__()\n        assert u_dim == 0, \"Support for control inputs is not implemented yet\"\n        self.aux_inference_net = aux_inference_net\n        self.y_emission_net = y_emission_net\n        self.aux_dim = aux_dim\n        self.z_dim = z_dim\n        self.y_dim = y_dim\n        self.u_dim = u_dim\n        self.integration_method = integration_method\n        self.integration_step_size = integration_step_size\n        self.sporadic = sporadic\n\n    def forward(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        times: Tensor,\n        num_samples: int = 1,\n        deterministic: bool = False,\n    ):\n        assert (\n            y.size() == mask.size() or y.size()[:-1] == mask.size()\n        ), \"Shapes of y and mask should match!\"\n        # Currently assumes that t = 0 is in times\n        assert times[0] == 0.0, \"First timestep should be 0!\"\n        batch_size = y.size(0)\n\n        aux_samples, aux_entropy, aux_post_log_prob = self.aux_inference_net(\n            y,\n            mask,\n            num_samples=num_samples,\n            deterministic=deterministic,\n        )\n        aux_samples = merge_leading_dims(aux_samples, ndims=2)\n        aux_entropy = merge_leading_dims(aux_entropy, ndims=2)\n        aux_post_log_prob = merge_leading_dims(aux_post_log_prob, ndims=2)\n        # Compute likelihoods\n        if mask.ndim < y.ndim:\n            mask = mask.unsqueeze(-1)\n        repeated_mask = mask.repeat(num_samples, 1, 1)\n        emission_dist = self.y_emission_net(aux_samples)\n        y_log_likelihood = emission_dist.log_prob(y.repeat(num_samples, 1, 1))\n        y_log_likelihood = y_log_likelihood * repeated_mask\n        y_log_likelihood = y_log_likelihood.sum(-1).sum(-1)\n        filter_result = self.base_ssm.filter(\n            aux_samples,\n            repeated_mask if self.sporadic else (repeated_mask.sum(-1) > 0).float(),\n            times,\n        )\n        aux_log_likelihood = filter_result[\"log_prob\"]\n        aux_entropy = aux_entropy.sum(dim=-1)\n        # Compute ELBO\n        regularizer = aux_log_likelihood + aux_entropy\n        elbo = y_log_likelihood + regularizer\n        # Compute IWELBO\n        iwelbo = torch.logsumexp(\n            y_log_likelihood.view(num_samples, batch_size)\n            + aux_log_likelihood.view(num_samples, batch_size)\n            - aux_post_log_prob.sum(dim=-1).view(num_samples, batch_size),\n            dim=0,\n        ) - np.log(num_samples)\n        return dict(\n            elbo=elbo,\n            iwelbo=iwelbo,\n            likelihood=y_log_likelihood,\n            regularizer=regularizer,\n        )\n\n    @torch.no_grad()\n    def forecast(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        past_times: Tensor,\n        future_times: Tensor,\n        num_samples: int = 80,\n        deterministic: bool = False,\n        no_state_sampling: bool = False,\n        use_smooth: bool = False,\n    ):\n        \"\"\"Make predictions (imputation and forecast) using the observed data.\n\n        Parameters\n        ----------\n        y\n            The tensor of observations, of shape (batch_size, num_timesteps, y_dim)\n        mask\n            The mask of missing values (1: observed, 0: missing),\n            of shape (batch_size, num_timesteps, y_dim), if sporadic,\n            else (batch_size, num_timesteps)\n        past_times\n            The times of the past observations, of shape (num_past_steps,)\n        future_times\n            The times of the forecast, of shape (num_forecast_steps,)\n        num_samples, optional\n            The number of sample paths to draw, by default 80\n        deterministic, optional\n            Whether to peform deterministic sampling from auxiliary model,\n            by default False (not really used)\n        no_state_sampling, optional\n            Whether to sample from the predicted state distributions,\n            by default False and only uses the means of the distributions\n        use_smooth, optional\n            Whether to perform smoothing after filtering (useful for imputation),\n            by default False\n\n        Returns\n        -------\n            The reconstructed context (imputing values, if required) and the forecast\n        \"\"\"\n        B, T, _ = y.shape\n\n        aux_samples, _, _ = self.aux_inference_net(\n            y,\n            mask,\n            num_samples=1,\n            deterministic=deterministic,\n        )\n        # aux_samples.shape = num_samples x B x time x aux_dim\n        aux_samples = merge_leading_dims(aux_samples, ndims=2)\n        if mask.ndim < y.ndim:\n            mask = mask.unsqueeze(-1)\n        # Generate predictions from the base CDKF\n        base_predictions = self.base_ssm.forecast(\n            aux_samples,\n            mask if self.sporadic else (mask.sum(-1) > 0).float(),\n            past_times,\n            future_times,\n            num_samples=num_samples,\n            no_state_sampling=no_state_sampling,\n            use_smooth=use_smooth,\n        )\n        aux_reconstruction = base_predictions[\"reconstruction\"]\n        aux_forecast = base_predictions[\"forecast\"]\n        z_reconstruction = base_predictions[\"z_reconstruction\"]\n        z_forecast = base_predictions[\"z_forecast\"]\n\n        # Decode aux --> y\n        reconstruction_emit_dist = self.y_emission_net(\n            merge_leading_dims(aux_reconstruction, ndims=2)\n        )\n        y_reconstruction = reconstruction_emit_dist.sample()\n        y_reconstruction = y_reconstruction.view(\n            num_samples, B, aux_reconstruction.shape[-2], self.y_dim\n        )\n        forecast_emit_dist = self.y_emission_net(\n            merge_leading_dims(aux_forecast, ndims=2)\n        )\n        y_forecast = forecast_emit_dist.sample()\n        y_forecast = y_forecast.view(num_samples, B, aux_forecast.shape[-2], self.y_dim)\n\n        return dict(\n            reconstruction=y_reconstruction,\n            forecast=y_forecast,\n            z_reconstruction=z_reconstruction,\n            z_forecast=z_forecast,\n            aux_reconstruction=aux_reconstruction,\n            aux_forecast=aux_forecast,\n        )", "\n\nclass NCDSSMLTI(NCDSSM):\n    \"\"\"The NCDSSM model with linear time-invariant dynamics.\n\n    Parameters\n    ----------\n    aux_inference_net\n        The auxiliary inference model parameterized by a neural network\n    y_emission_net\n        The emission model parameterized by a neural network\n    aux_dim\n        The dimension of the auxiliary variables\n    z_dim\n        The dimension of the latent states\n    y_dim\n        The dimension of the observations\n    u_dim\n        The dimension of the control inputs\n    integration_method, optional\n        The integration method, can be one of \"euler\" or \"rk4\",\n        by default \"rk4\"\n    integration_step_size, optional\n        The integration step size, by default 0.1\n    sporadic, optional\n        A flag to indicate whether the dataset is sporadic,\n        i.e., with values missing in both time and feature dimensions,\n        by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        aux_inference_net: nn.Module,\n        y_emission_net: nn.Module,\n        aux_dim: int,\n        z_dim: int,\n        y_dim: int,\n        u_dim: int,\n        integration_method: str = \"rk4\",\n        integration_step_size: float = 0.1,\n        sporadic: bool = False,\n        **kwargs,\n    ):\n        super().__init__(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim,\n            z_dim,\n            y_dim,\n            u_dim,\n            integration_method,\n            integration_step_size,\n            sporadic,\n            **kwargs,\n        )\n        self.base_ssm = BaseLTI(\n            z_dim=z_dim,\n            y_dim=aux_dim,\n            u_dim=u_dim,\n            integration_method=integration_method,\n            integration_step_size=integration_step_size,\n            sporadic=sporadic,\n            **kwargs,\n        )", "\n\nclass NCDSSMLL(NCDSSM):\n    \"\"\"The NCDSSM model with locally-linear dynamics.\n\n    Parameters\n    ----------\n    aux_inference_net\n        The auxiliary inference model parameterized by a neural network\n    y_emission_net\n        The emission model parameterized by a neural network\n    aux_dim\n        The dimension of the auxiliary variables\n    K\n        The number of base matrices (i.e., dynamics)\n    z_dim\n        The dimension of the latent states\n    y_dim\n        The dimension of the observations\n    u_dim\n        The dimension of the control inputs\n    alpha_net\n        A mixing network that takes the state `z` as input\n        and outputs the mixing coefficients for the base dynamics\n    integration_method, optional\n        The integration method, can be one of \"euler\" or \"rk4\",\n        by default \"rk4\"\n    integration_step_size, optional\n        The integration step size, by default 0.1\n    sporadic, optional\n        A flag to indicate whether the dataset is sporadic,\n        i.e., with values missing in both time and feature dimensions,\n        by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        aux_inference_net: AuxInferenceModel,\n        y_emission_net: nn.Module,\n        aux_dim: int,\n        K: int,\n        z_dim: int,\n        y_dim: int,\n        u_dim: int,\n        alpha_net: nn.Module,\n        integration_method: str = \"rk4\",\n        integration_step_size: float = 0.1,\n        sporadic: bool = False,\n        **kwargs,\n    ):\n        super().__init__(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim,\n            z_dim,\n            y_dim,\n            u_dim,\n            integration_method,\n            integration_step_size,\n            sporadic,\n            **kwargs,\n        )\n        self.base_ssm = BaseLL(\n            K=K,\n            z_dim=z_dim,\n            y_dim=aux_dim,\n            u_dim=u_dim,\n            alpha_net=alpha_net,\n            integration_method=integration_method,\n            integration_step_size=integration_step_size,\n            sporadic=sporadic,\n            **kwargs,\n        )\n\n    @torch.no_grad()\n    def forecast(\n        self,\n        y: Tensor,\n        mask: Tensor,\n        past_times: Tensor,\n        future_times: Tensor,\n        num_samples: int = 80,\n        deterministic: bool = False,\n        no_state_sampling: bool = False,\n        use_smooth: bool = False,\n    ):\n        B, T, _ = y.shape\n\n        aux_samples, _, _ = self.aux_inference_net(\n            y,\n            mask,\n            num_samples=1,\n            deterministic=deterministic,\n        )\n        # aux_samples.shape = num_samples x B x time x aux_dim\n        aux_samples = merge_leading_dims(aux_samples, ndims=2)\n        if mask.ndim < y.ndim:\n            mask = mask.unsqueeze(-1)\n        # Generate predictions from the base CDKF\n        base_predictions = self.base_ssm.forecast(\n            aux_samples,\n            mask if self.sporadic else (mask.sum(-1) > 0).float(),\n            past_times,\n            future_times,\n            num_samples=num_samples,\n            no_state_sampling=no_state_sampling,\n            use_smooth=use_smooth,\n        )\n        aux_reconstruction = base_predictions[\"reconstruction\"]\n        aux_forecast = base_predictions[\"forecast\"]\n        z_reconstruction = base_predictions[\"z_reconstruction\"]\n        z_forecast = base_predictions[\"z_forecast\"]\n        alpha_reconstruction = base_predictions[\"alpha_reconstruction\"]\n        alpha_forecast = base_predictions[\"alpha_forecast\"]\n\n        # Decode aux --> y\n        reconstruction_emit_dist = self.y_emission_net(\n            merge_leading_dims(aux_reconstruction, ndims=2)\n        )\n        y_reconstruction = reconstruction_emit_dist.sample()\n        y_reconstruction = y_reconstruction.view(\n            num_samples, B, aux_reconstruction.shape[-2], self.y_dim\n        )\n        forecast_emit_dist = self.y_emission_net(\n            merge_leading_dims(aux_forecast, ndims=2)\n        )\n        y_forecast = forecast_emit_dist.sample()\n        y_forecast = y_forecast.view(num_samples, B, aux_forecast.shape[-2], self.y_dim)\n\n        return dict(\n            reconstruction=y_reconstruction,\n            forecast=y_forecast,\n            z_reconstruction=z_reconstruction,\n            z_forecast=z_forecast,\n            alpha_reconstruction=alpha_reconstruction,\n            alpha_forecast=alpha_forecast,\n            aux_reconstruction=aux_reconstruction,\n            aux_forecast=aux_forecast,\n        )", "\n\nclass NCDSSMNL(NCDSSM):\n    \"\"\"The NCDSSM model with non-linear dynamics.\n\n    Parameters\n    ----------\n    aux_inference_net\n        The auxiliary inference model parameterized by a neural network\n    y_emission_net\n        The emission model parameterized by a neural network\n    aux_dim\n        The dimension of the auxiliary variables\n    z_dim\n        The dimension of the latent states\n    y_dim\n        The dimension of the observations\n    u_dim\n        The dimension of the control inputs\n    f, optional\n        The dynamics/drift function f(z), by default None\n    gs, optional\n        The list of diffusion functions g(z), one for each z_dim, by default None\n    integration_method, optional\n        The integration method, can be one of \"euler\" or \"rk4\",\n        by default \"rk4\"\n    integration_step_size, optional\n        The integration step size, by default 0.1\n    sporadic, optional\n        A flag to indicate whether the dataset is sporadic,\n        i.e., with values missing in both time and feature dimensions,\n        by default False\n    \"\"\"\n\n    def __init__(\n        self,\n        aux_inference_net: AuxInferenceModel,\n        y_emission_net: nn.Module,\n        aux_dim: int,\n        z_dim: int,\n        y_dim: int,\n        u_dim: int,\n        f: nn.Module,\n        gs: Optional[List[nn.Module]] = None,\n        integration_method: str = \"rk4\",\n        integration_step_size: float = 0.1,\n        sporadic: bool = False,\n        **kwargs,\n    ):\n        super().__init__(\n            aux_inference_net,\n            y_emission_net,\n            aux_dim,\n            z_dim,\n            y_dim,\n            u_dim,\n            integration_method,\n            integration_step_size,\n            sporadic,\n            **kwargs,\n        )\n        self.base_ssm = BaseNL(\n            z_dim=z_dim,\n            y_dim=aux_dim,\n            u_dim=u_dim,\n            f=f,\n            gs=gs,\n            integration_method=integration_method,\n            integration_step_size=integration_step_size,\n            sporadic=sporadic,\n            **kwargs,\n        )", ""]}
{"filename": "src/ncdssm/datasets/mocap.py", "chunked_list": ["import torch\nimport numpy as np\nfrom scipy.io import loadmat\n\nfrom ..utils import listofdict2dictoflist\n\n\nclass MocapDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        file_path: str,\n        mode: str = \"train\",\n        dt: float = 0.1,\n        ctx_len: int = 200,\n        pred_len: int = 100,\n        missing_p: float = 0.0,\n    ):\n        data = loadmat(file_path)\n\n        if mode == \"train\":\n            self._data = data[\"Xtr\"]\n        elif mode == \"val\":\n            self._data = data[\"Xval\"]\n        else:\n            self._data = data[\"Xtest\"]\n        self.dt = dt\n        self.ctx_len = ctx_len\n        self.pred_len = pred_len\n        self.missing_p = missing_p\n        self._set_mask()\n\n    def _set_mask(self):\n        self.observed_mask = np.random.choice(\n            [True, False],\n            p=[1 - self.missing_p, self.missing_p],\n            size=(self._data.shape[0], self.ctx_len),\n        )\n        self.observed_mask[:, :3] = True\n\n    def __len__(self):\n        return self._data.shape[0]\n\n    def __getitem__(self, idx):\n        target = self._data[idx]\n        past_target = target[: self.ctx_len]\n        past_times = np.arange(self.ctx_len) * self.dt\n        past_mask = self.observed_mask[idx]\n        # past_target = past_target * past_mask[:, None]\n        future_target = target[self.ctx_len :]\n        future_times = np.arange(target.shape[0]) * self.dt\n        future_times = future_times[self.ctx_len :]\n        return dict(\n            past_target=torch.as_tensor(past_target.astype(np.float32)),\n            future_target=torch.as_tensor(future_target.astype(np.float32)),\n            past_times=torch.as_tensor(past_times),\n            future_times=torch.as_tensor(future_times),\n            past_mask=torch.as_tensor(past_mask.astype(np.float32)),\n        )\n\n    def collate_fn(self, list_of_samples):\n        dict_of_samples = listofdict2dictoflist(list_of_samples)\n        comb_past_target = torch.stack(dict_of_samples[\"past_target\"])\n        comb_past_times = dict_of_samples[\"past_times\"][0]\n        comb_past_mask = torch.stack(dict_of_samples[\"past_mask\"])\n        comb_future_target = None\n        comb_future_times = None\n        if dict_of_samples[\"future_target\"][0] is not None:\n            comb_future_target = torch.stack(dict_of_samples[\"future_target\"])\n            comb_future_times = dict_of_samples[\"future_times\"][0]\n        return dict(\n            past_target=comb_past_target,\n            past_times=comb_past_times,\n            past_mask=comb_past_mask,\n            future_target=comb_future_target,\n            future_times=comb_future_times,\n        )", ""]}
{"filename": "src/ncdssm/datasets/__init__.py", "chunked_list": ["from .pymunk import PymunkDataset\nfrom .synthetic import BouncingBallDataset, DampedPendulumDataset\n\nfrom .mocap import MocapDataset\nfrom .climate import ClimateDataset\n\n\n__all__ = [\n    \"PymunkDataset\",\n    \"BouncingBallDataset\",", "    \"PymunkDataset\",\n    \"BouncingBallDataset\",\n    \"MocapDataset\",\n    \"DampedPendulumDataset\",\n    \"ClimateDataset\",\n]\n"]}
{"filename": "src/ncdssm/datasets/pymunk.py", "chunked_list": ["import torch\nimport numpy as np\n\nfrom ..utils import listofdict2dictoflist\n\n\nclass PymunkDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        file_path: str,\n        missing_p=0.0,\n        train=True,\n        dt=0.1,\n        ctx_len=20,\n        pred_len=40,\n    ):\n        self._data, self._ground_truth = self._load_dataset(\n            file_path, num_timesteps=ctx_len + pred_len\n        )\n        self.missing_p = missing_p\n        self.train = train\n        self.dt = dt\n        self.ctx_len = ctx_len\n        self.observed_mask = np.random.choice(\n            [True, False],\n            p=[1 - missing_p, missing_p],\n            size=(self._data[\"y\"].shape[0], ctx_len),\n        )\n        self.observed_mask[:, 0] = True\n\n    def __len__(self):\n        idx_batch = 0\n        sizes = [val.shape[idx_batch] for val in self._data.values()]\n        assert all(size == sizes[0] for size in sizes)\n        size = sizes[0]\n        return size\n\n    def __getitem__(self, idx):\n        target = self._data[\"y\"][idx]\n        past_target = target[: self.ctx_len]\n        past_times = np.arange(self.ctx_len) * self.dt\n        past_mask = self.observed_mask[idx].astype(np.float32)\n        # past_target = past_target * past_mask[:, None]\n        if not self.train:\n            future_target = target[self.ctx_len :]\n            future_times = np.arange(target.shape[0]) * self.dt\n            future_times = future_times[self.ctx_len :]\n        return dict(\n            past_target=torch.as_tensor(past_target),\n            future_target=None if self.train else torch.as_tensor(future_target),\n            past_times=torch.as_tensor(past_times),\n            future_times=None if self.train else torch.as_tensor(future_times),\n            past_mask=torch.as_tensor(past_mask),\n        )\n\n    def collate_fn(self, list_of_samples):\n        dict_of_samples = listofdict2dictoflist(list_of_samples)\n        comb_past_target = torch.stack(dict_of_samples[\"past_target\"])\n        comb_past_times = dict_of_samples[\"past_times\"][0]\n        comb_past_mask = torch.stack(dict_of_samples[\"past_mask\"])\n        comb_future_target = None\n        comb_future_times = None\n        if dict_of_samples[\"future_target\"][0] is not None:\n            comb_future_target = torch.stack(dict_of_samples[\"future_target\"])\n            comb_future_times = dict_of_samples[\"future_times\"][0]\n        return dict(\n            past_target=comb_past_target,\n            past_times=comb_past_times,\n            past_mask=comb_past_mask,\n            future_target=comb_future_target,\n            future_times=comb_future_times,\n        )\n\n    def _load_dataset(self, file_path, num_timesteps=100):\n        npzfile = np.load(file_path)\n        images = npzfile[\"images\"].astype(np.float32)\n        # The datasets in KVAE are binarized images\n        images = (images > 0).astype(np.float32)\n        assert images.ndim == 4\n        images = images.reshape(\n            images.shape[0], images.shape[1], images.shape[2] * images.shape[3]\n        )\n        data = {\"y\": images[:, :num_timesteps]}\n\n        if \"state\" in npzfile:  # all except Pong have state.\n            position = npzfile[\"state\"].astype(np.float32)[:, :, :2]\n            velocity = npzfile[\"state\"].astype(np.float32)[:, :, 2:]\n            ground_truth_state = {\"position\": position, \"velocity\": velocity}\n        else:\n            ground_truth_state = None\n        return data, ground_truth_state", ""]}
{"filename": "src/ncdssm/datasets/synthetic.py", "chunked_list": ["import torch\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\nfrom ..utils import listofdict2dictoflist\n\n\nclass BouncingBallDataset(Dataset):\n    def __init__(\n        self,\n        path: str = \"./data/bouncing_ball.npz\",\n        dt: float = 0.1,\n        ctx_len: int = 200,\n        pred_len: int = 100,\n        missing_p: float = 0.0,\n        num_always_obs: float = 0,\n    ):\n        self._data = np.load(path)[\"target\"]\n        self.dt = dt\n        self.ctx_len = ctx_len\n        self.pred_len = pred_len\n        self.missing_p = missing_p\n        self.num_always_obs = num_always_obs\n        self._set_mask()\n\n    def _set_mask(self):\n        self.observed_mask = np.random.choice(\n            [True, False],\n            p=[1 - self.missing_p, self.missing_p],\n            size=(self._data.shape[0], self.ctx_len),\n        )\n        self.observed_mask[:, : self.num_always_obs] = True\n\n    def __len__(self):\n        return self._data.shape[0]\n\n    def __getitem__(self, idx):\n        target = self._data[idx]\n        past_target = target[: self.ctx_len]\n        past_times = np.arange(self.ctx_len) * self.dt\n        past_mask = self.observed_mask[idx]\n        # past_target = past_target * past_mask[:, None]\n        future_target = target[self.ctx_len :]\n        future_times = np.arange(target.shape[0]) * self.dt\n        future_times = future_times[self.ctx_len :]\n        return dict(\n            past_target=torch.as_tensor(past_target.astype(np.float32)),\n            future_target=torch.as_tensor(future_target.astype(np.float32)),\n            past_times=torch.as_tensor(past_times),\n            future_times=torch.as_tensor(future_times),\n            past_mask=torch.as_tensor(past_mask.astype(np.float32)),\n        )\n\n    def collate_fn(self, list_of_samples):\n        dict_of_samples = listofdict2dictoflist(list_of_samples)\n        comb_past_target = torch.stack(dict_of_samples[\"past_target\"])\n        comb_past_times = dict_of_samples[\"past_times\"][0]\n        comb_past_mask = torch.stack(dict_of_samples[\"past_mask\"])\n        comb_future_target = None\n        comb_future_times = None\n        if dict_of_samples[\"future_target\"][0] is not None:\n            comb_future_target = torch.stack(dict_of_samples[\"future_target\"])\n            comb_future_times = dict_of_samples[\"future_times\"][0]\n        return dict(\n            past_target=comb_past_target,\n            past_times=comb_past_times,\n            past_mask=comb_past_mask,\n            future_target=comb_future_target,\n            future_times=comb_future_times,\n        )", "class BouncingBallDataset(Dataset):\n    def __init__(\n        self,\n        path: str = \"./data/bouncing_ball.npz\",\n        dt: float = 0.1,\n        ctx_len: int = 200,\n        pred_len: int = 100,\n        missing_p: float = 0.0,\n        num_always_obs: float = 0,\n    ):\n        self._data = np.load(path)[\"target\"]\n        self.dt = dt\n        self.ctx_len = ctx_len\n        self.pred_len = pred_len\n        self.missing_p = missing_p\n        self.num_always_obs = num_always_obs\n        self._set_mask()\n\n    def _set_mask(self):\n        self.observed_mask = np.random.choice(\n            [True, False],\n            p=[1 - self.missing_p, self.missing_p],\n            size=(self._data.shape[0], self.ctx_len),\n        )\n        self.observed_mask[:, : self.num_always_obs] = True\n\n    def __len__(self):\n        return self._data.shape[0]\n\n    def __getitem__(self, idx):\n        target = self._data[idx]\n        past_target = target[: self.ctx_len]\n        past_times = np.arange(self.ctx_len) * self.dt\n        past_mask = self.observed_mask[idx]\n        # past_target = past_target * past_mask[:, None]\n        future_target = target[self.ctx_len :]\n        future_times = np.arange(target.shape[0]) * self.dt\n        future_times = future_times[self.ctx_len :]\n        return dict(\n            past_target=torch.as_tensor(past_target.astype(np.float32)),\n            future_target=torch.as_tensor(future_target.astype(np.float32)),\n            past_times=torch.as_tensor(past_times),\n            future_times=torch.as_tensor(future_times),\n            past_mask=torch.as_tensor(past_mask.astype(np.float32)),\n        )\n\n    def collate_fn(self, list_of_samples):\n        dict_of_samples = listofdict2dictoflist(list_of_samples)\n        comb_past_target = torch.stack(dict_of_samples[\"past_target\"])\n        comb_past_times = dict_of_samples[\"past_times\"][0]\n        comb_past_mask = torch.stack(dict_of_samples[\"past_mask\"])\n        comb_future_target = None\n        comb_future_times = None\n        if dict_of_samples[\"future_target\"][0] is not None:\n            comb_future_target = torch.stack(dict_of_samples[\"future_target\"])\n            comb_future_times = dict_of_samples[\"future_times\"][0]\n        return dict(\n            past_target=comb_past_target,\n            past_times=comb_past_times,\n            past_mask=comb_past_mask,\n            future_target=comb_future_target,\n            future_times=comb_future_times,\n        )", "\n\nclass DampedPendulumDataset(BouncingBallDataset):\n    def __init__(\n        self,\n        path: str = \"./data/damped_pendulum/train.npz\",\n        dt: float = 0.1,\n        ctx_len: int = 50,\n        pred_len: int = 100,\n        missing_p: float = 0,\n        num_always_obs: float = 0,\n    ):\n        self._data = np.load(path)[\"obs\"]\n        self.dt = dt\n        self.ctx_len = ctx_len\n        self.pred_len = pred_len\n        self.missing_p = missing_p\n        self.num_always_obs = num_always_obs\n        self._set_mask()", ""]}
{"filename": "src/ncdssm/datasets/climate.py", "chunked_list": ["import torch\nimport numpy as np\nimport pandas as pd\n\nfrom ..type import Dict, NumpyArray\nfrom ..utils import listofdict2dictoflist\n\n\nclass ClimateDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        csv_path: str,\n        time_scale: float = 1.0,\n        train: bool = True,\n        ids: NumpyArray = None,\n        val_options: Dict = None,\n    ) -> None:\n        super().__init__()\n        self.train = train\n        self.df = pd.read_csv(csv_path)\n        assert self.df.columns[0] == \"ID\"\n\n        if not train:\n            assert val_options is not None\n            ids_before_tval = self.df.loc[\n                self.df[\"Time\"] <= val_options[\"T_val\"], \"ID\"\n            ].unique()\n            ids_after_tval = self.df.loc[\n                self.df[\"Time\"] > val_options[\"T_val\"], \"ID\"\n            ].unique()\n            filtered_ids = np.intersect1d(ids_before_tval, ids_after_tval)\n            self.df = self.df.loc[self.df[\"ID\"].isin(filtered_ids)]\n\n        if ids is not None:\n            self.df = self.df.loc[self.df[\"ID\"].isin(ids)].copy()\n            map_dict = dict(\n                zip(self.df[\"ID\"].unique(), np.arange(self.df[\"ID\"].nunique()))\n            )\n            self.df[\"ID\"] = self.df[\"ID\"].map(map_dict)\n\n        self.df.Time = self.df.Time * time_scale\n\n        if not train:\n            self.df_before_tval = self.df.loc[\n                self.df[\"Time\"] <= val_options[\"T_val\"]\n            ].copy()\n            self.df_after_tval = (\n                self.df.loc[self.df[\"Time\"] > val_options[\"T_val\"]]\n                .sort_values(\"Time\")\n                .copy()\n            )\n            self.df_after_tval = (\n                self.df_after_tval.groupby(\"ID\")\n                .head(val_options[\"forecast_steps\"])\n                .copy()\n            )\n\n            self.df = self.df_before_tval\n\n            self.df_after_tval = self.df_after_tval.astype(np.float32)\n            self.df_after_tval.ID = self.df_after_tval.ID.astype(int)\n            self.df_after_tval.sort_values(\"Time\", inplace=True)\n\n        self.df = self.df.astype(np.float32)\n        self.df.ID = self.df.ID.astype(int)\n        self.size = self.df[\"ID\"].nunique()\n        # self.df.set_index(\"ID\", inplace=True)\n        self.df.sort_values(\"Time\", inplace=True)\n        self.data_columns = list(\n            filter(lambda c: c.startswith(\"Value\"), self.df.columns)\n        )\n        self.mask_columns = list(\n            filter(lambda c: c.startswith(\"Mask\"), self.df.columns)\n        )\n\n        self.data = []\n\n        for id, sub_df in self.df.groupby(\"ID\"):\n            sub_df = sub_df.reset_index(drop=True).sort_values(\"Time\")\n            item = {\n                \"Time\": sub_df[\"Time\"].to_numpy(),\n                \"Value\": sub_df[self.data_columns].to_numpy(),\n                \"Mask\": sub_df[self.mask_columns].to_numpy(),\n            }\n            if not self.train:\n                val_sub_df = self.df_after_tval.loc[self.df_after_tval[\"ID\"] == id]\n                item[\"Future_Time\"] = val_sub_df[\"Time\"].to_numpy()\n                item[\"Future_Value\"] = val_sub_df[self.data_columns].to_numpy()\n                item[\"Future_Mask\"] = val_sub_df[self.mask_columns].to_numpy()\n            self.data.append(item)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        subset = self.data[idx]\n        past_target = subset[\"Value\"]\n        past_mask = subset[\"Mask\"]\n        past_times = subset[\"Time\"]\n        if not self.train:\n            future_target = subset[\"Future_Value\"]\n            future_mask = subset[\"Future_Mask\"]\n            future_times = subset[\"Future_Time\"]\n        return dict(\n            past_target=torch.as_tensor(past_target),\n            past_mask=torch.as_tensor(past_mask),\n            past_times=torch.as_tensor(past_times),\n            future_target=None if self.train else torch.as_tensor(future_target),\n            future_mask=None if self.train else torch.as_tensor(future_mask),\n            future_times=None if self.train else torch.as_tensor(future_times),\n        )\n\n    def collate_fn(self, list_of_samples):\n        dict_of_samples = listofdict2dictoflist(list_of_samples)\n        batch_size = len(list_of_samples)\n        target_dim = dict_of_samples[\"past_target\"][0].shape[-1]\n        # Collate past\n        comb_past_times, past_inverse_indices = torch.unique(\n            torch.cat(dict_of_samples[\"past_times\"]), sorted=True, return_inverse=True\n        )\n        comb_past_target = torch.zeros(\n            [batch_size, comb_past_times.shape[0], target_dim]\n        )\n        comb_past_mask = torch.zeros_like(comb_past_target)\n        past_offset = 0\n        for i, (tgt, mask, time) in enumerate(\n            zip(\n                dict_of_samples[\"past_target\"],\n                dict_of_samples[\"past_mask\"],\n                dict_of_samples[\"past_times\"],\n            )\n        ):\n            past_indices = past_inverse_indices[\n                past_offset : past_offset + time.shape[0]\n            ]\n            past_offset += time.shape[0]\n            comb_past_target[i, past_indices] = tgt\n            comb_past_mask[i, past_indices] = mask\n        # Collate future\n        comb_future_target = None\n        comb_future_times = None\n        comb_future_mask = None\n        if dict_of_samples[\"future_target\"][0] is not None:\n            comb_future_times, future_inverse_indices = torch.unique(\n                torch.cat(dict_of_samples[\"future_times\"]),\n                sorted=True,\n                return_inverse=True,\n            )\n            comb_future_target = torch.zeros(\n                [batch_size, comb_future_times.shape[0], target_dim]\n            )\n            comb_future_mask = torch.zeros_like(comb_future_target)\n            future_offset = 0\n            for i, (tgt, mask, time) in enumerate(\n                zip(\n                    dict_of_samples[\"future_target\"],\n                    dict_of_samples[\"future_mask\"],\n                    dict_of_samples[\"future_times\"],\n                )\n            ):\n                future_indices = future_inverse_indices[\n                    future_offset : future_offset + time.shape[0]\n                ]\n                future_offset += time.shape[0]\n                comb_future_target[i, future_indices] = tgt\n                comb_future_mask[i, future_indices] = mask\n        return dict(\n            past_target=comb_past_target,\n            past_times=comb_past_times,\n            past_mask=comb_past_mask,\n            future_target=comb_future_target,\n            future_times=comb_future_times,\n            future_mask=comb_future_mask,\n        )", "class ClimateDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        csv_path: str,\n        time_scale: float = 1.0,\n        train: bool = True,\n        ids: NumpyArray = None,\n        val_options: Dict = None,\n    ) -> None:\n        super().__init__()\n        self.train = train\n        self.df = pd.read_csv(csv_path)\n        assert self.df.columns[0] == \"ID\"\n\n        if not train:\n            assert val_options is not None\n            ids_before_tval = self.df.loc[\n                self.df[\"Time\"] <= val_options[\"T_val\"], \"ID\"\n            ].unique()\n            ids_after_tval = self.df.loc[\n                self.df[\"Time\"] > val_options[\"T_val\"], \"ID\"\n            ].unique()\n            filtered_ids = np.intersect1d(ids_before_tval, ids_after_tval)\n            self.df = self.df.loc[self.df[\"ID\"].isin(filtered_ids)]\n\n        if ids is not None:\n            self.df = self.df.loc[self.df[\"ID\"].isin(ids)].copy()\n            map_dict = dict(\n                zip(self.df[\"ID\"].unique(), np.arange(self.df[\"ID\"].nunique()))\n            )\n            self.df[\"ID\"] = self.df[\"ID\"].map(map_dict)\n\n        self.df.Time = self.df.Time * time_scale\n\n        if not train:\n            self.df_before_tval = self.df.loc[\n                self.df[\"Time\"] <= val_options[\"T_val\"]\n            ].copy()\n            self.df_after_tval = (\n                self.df.loc[self.df[\"Time\"] > val_options[\"T_val\"]]\n                .sort_values(\"Time\")\n                .copy()\n            )\n            self.df_after_tval = (\n                self.df_after_tval.groupby(\"ID\")\n                .head(val_options[\"forecast_steps\"])\n                .copy()\n            )\n\n            self.df = self.df_before_tval\n\n            self.df_after_tval = self.df_after_tval.astype(np.float32)\n            self.df_after_tval.ID = self.df_after_tval.ID.astype(int)\n            self.df_after_tval.sort_values(\"Time\", inplace=True)\n\n        self.df = self.df.astype(np.float32)\n        self.df.ID = self.df.ID.astype(int)\n        self.size = self.df[\"ID\"].nunique()\n        # self.df.set_index(\"ID\", inplace=True)\n        self.df.sort_values(\"Time\", inplace=True)\n        self.data_columns = list(\n            filter(lambda c: c.startswith(\"Value\"), self.df.columns)\n        )\n        self.mask_columns = list(\n            filter(lambda c: c.startswith(\"Mask\"), self.df.columns)\n        )\n\n        self.data = []\n\n        for id, sub_df in self.df.groupby(\"ID\"):\n            sub_df = sub_df.reset_index(drop=True).sort_values(\"Time\")\n            item = {\n                \"Time\": sub_df[\"Time\"].to_numpy(),\n                \"Value\": sub_df[self.data_columns].to_numpy(),\n                \"Mask\": sub_df[self.mask_columns].to_numpy(),\n            }\n            if not self.train:\n                val_sub_df = self.df_after_tval.loc[self.df_after_tval[\"ID\"] == id]\n                item[\"Future_Time\"] = val_sub_df[\"Time\"].to_numpy()\n                item[\"Future_Value\"] = val_sub_df[self.data_columns].to_numpy()\n                item[\"Future_Mask\"] = val_sub_df[self.mask_columns].to_numpy()\n            self.data.append(item)\n\n    def __len__(self):\n        return self.size\n\n    def __getitem__(self, idx):\n        subset = self.data[idx]\n        past_target = subset[\"Value\"]\n        past_mask = subset[\"Mask\"]\n        past_times = subset[\"Time\"]\n        if not self.train:\n            future_target = subset[\"Future_Value\"]\n            future_mask = subset[\"Future_Mask\"]\n            future_times = subset[\"Future_Time\"]\n        return dict(\n            past_target=torch.as_tensor(past_target),\n            past_mask=torch.as_tensor(past_mask),\n            past_times=torch.as_tensor(past_times),\n            future_target=None if self.train else torch.as_tensor(future_target),\n            future_mask=None if self.train else torch.as_tensor(future_mask),\n            future_times=None if self.train else torch.as_tensor(future_times),\n        )\n\n    def collate_fn(self, list_of_samples):\n        dict_of_samples = listofdict2dictoflist(list_of_samples)\n        batch_size = len(list_of_samples)\n        target_dim = dict_of_samples[\"past_target\"][0].shape[-1]\n        # Collate past\n        comb_past_times, past_inverse_indices = torch.unique(\n            torch.cat(dict_of_samples[\"past_times\"]), sorted=True, return_inverse=True\n        )\n        comb_past_target = torch.zeros(\n            [batch_size, comb_past_times.shape[0], target_dim]\n        )\n        comb_past_mask = torch.zeros_like(comb_past_target)\n        past_offset = 0\n        for i, (tgt, mask, time) in enumerate(\n            zip(\n                dict_of_samples[\"past_target\"],\n                dict_of_samples[\"past_mask\"],\n                dict_of_samples[\"past_times\"],\n            )\n        ):\n            past_indices = past_inverse_indices[\n                past_offset : past_offset + time.shape[0]\n            ]\n            past_offset += time.shape[0]\n            comb_past_target[i, past_indices] = tgt\n            comb_past_mask[i, past_indices] = mask\n        # Collate future\n        comb_future_target = None\n        comb_future_times = None\n        comb_future_mask = None\n        if dict_of_samples[\"future_target\"][0] is not None:\n            comb_future_times, future_inverse_indices = torch.unique(\n                torch.cat(dict_of_samples[\"future_times\"]),\n                sorted=True,\n                return_inverse=True,\n            )\n            comb_future_target = torch.zeros(\n                [batch_size, comb_future_times.shape[0], target_dim]\n            )\n            comb_future_mask = torch.zeros_like(comb_future_target)\n            future_offset = 0\n            for i, (tgt, mask, time) in enumerate(\n                zip(\n                    dict_of_samples[\"future_target\"],\n                    dict_of_samples[\"future_mask\"],\n                    dict_of_samples[\"future_times\"],\n                )\n            ):\n                future_indices = future_inverse_indices[\n                    future_offset : future_offset + time.shape[0]\n                ]\n                future_offset += time.shape[0]\n                comb_future_target[i, future_indices] = tgt\n                comb_future_mask[i, future_indices] = mask\n        return dict(\n            past_target=comb_past_target,\n            past_times=comb_past_times,\n            past_mask=comb_past_mask,\n            future_target=comb_future_target,\n            future_times=comb_future_times,\n            future_mask=comb_future_mask,\n        )", ""]}
