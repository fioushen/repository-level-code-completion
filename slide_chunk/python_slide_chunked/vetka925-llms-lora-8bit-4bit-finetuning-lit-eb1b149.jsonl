{"filename": "modelutils.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\n\nDEV = torch.device('cuda:0')\n\n\ndef find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n    if type(module) in layers:\n        return {name: module}\n    res = {}\n    for name1, child in module.named_children():\n        res.update(find_layers(\n            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n        ))\n    return res", ""]}
{"filename": "finetuners.py", "chunked_list": ["import torch\nfrom dataclasses import dataclass\nimport numpy as np\nfrom torch.nn.functional import cross_entropy\nimport pytorch_lightning as pl\nfrom transformers import GPTJForCausalLM, LlamaForCausalLM, AdamW\nfrom bitsandbytes.optim import AdamW8bit\n\nfrom autograd_4bit import load_gptj_model_4bit_low_ram, load_llama_model_4bit_low_ram\nfrom peft import (", "from autograd_4bit import load_gptj_model_4bit_low_ram, load_llama_model_4bit_low_ram\nfrom peft import (\n    prepare_model_for_int8_training,\n    LoraConfig,\n    get_peft_model,\n    get_peft_model_state_dict,\n)\n\n@dataclass\nclass FinetunerConfig():\n    target_lora_modules: list\n    lr: float = 1e-3\n    batch_size: int = 1\n    warmup_steps: int = 10\n    num_epochs: int = 1\n    adapter_dim: int = 2\n    classification: bool = False\n    lora_r: int = 2\n    lora_alpha: int = 16\n    lora_dropout: float = 0", "@dataclass\nclass FinetunerConfig():\n    target_lora_modules: list\n    lr: float = 1e-3\n    batch_size: int = 1\n    warmup_steps: int = 10\n    num_epochs: int = 1\n    adapter_dim: int = 2\n    classification: bool = False\n    lora_r: int = 2\n    lora_alpha: int = 16\n    lora_dropout: float = 0", "\n\nclass CommonFinetuner(pl.LightningModule):\n    def __init__(self, model_name, fine_tuning_config, train_dataset, val_dataset=None):\n        super().__init__()\n\n        self.model_name = model_name\n        self.config = fine_tuning_config\n        self.train_dataset = train_dataset\n        self.val_dataset = val_dataset\n\n        self.validation_step_outputs = []\n        self.model = None\n        self.squeeze_input = True\n\n    def load(self):\n        pass\n\n    def forward(self, input_ids, attention_masks):\n        assert self.model != None, 'Load model with load() method.'\n        return self.model.forward(\n                            input_ids=input_ids,\n                            attention_mask=attention_masks\n                            )\n\n    def common_step(self, batch, batch_idx):\n        input_ids, attention_masks, loss_mask = batch\n        roll_dim = 2\n        if self.squeeze_input:\n            input_ids = input_ids.squeeze(-2)\n            attention_masks = attention_masks.squeeze(-2)\n            loss_mask = loss_mask.squeeze(-2)\n            roll_dim = 1\n        out = self(\n                    input_ids=input_ids,\n                    attention_masks=attention_masks\n                    )\n        logits = out.logits[loss_mask.roll(shifts=-1, dims=roll_dim)]\n        completion_tok_ids = input_ids[loss_mask]\n        loss = cross_entropy(logits, completion_tok_ids)\n        preds = None\n        if self.config.classification:\n            preds = torch.argmax(logits, dim=1)\n        return loss, preds, completion_tok_ids\n\n\n    def training_step(self, batch, batch_idx):\n        loss, preds, trues = self.common_step(batch, batch_idx)\n        self.log('train_loss', loss)\n        if self.config.classification:\n            acc = round((preds == trues).sum().cpu().item() / preds.shape[0], 2)\n            self.log('train_acc', acc)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, preds, labels = self.common_step(batch, batch_idx)\n        self.log('val_loss', loss)\n        if self.config.classification:\n            trues = torch.sum(preds == labels).cpu()\n            total = len(labels)\n            self.validation_step_outputs.append((loss, trues, total))\n            return loss, trues, total\n        self.validation_step_outputs.append((loss, None, None))\n        return loss, None, None\n    \n    def on_validation_epoch_end(self):\n        self.log('val_epoch_loss', np.mean([e[0].cpu() for e in self.validation_step_outputs]))\n        if self.config.classification:\n            self.log('val_total_epoch_samples',  np.sum([e[2] for e in self.validation_step_outputs]))\n            self.log('val_epoch_accuracy', np.sum([e[1] for e in self.validation_step_outputs]) /  np.sum([e[2] for e in self.validation_step_outputs]))\n        self.validation_step_outputs.clear()\n\n\n    def train_dataloader(self):\n        train_loader = torch.utils.data.DataLoader(self.train_dataset, \n                                                   batch_size=self.config.batch_size, \n                                                  shuffle=True)\n        return train_loader\n    \n    def val_dataloader(self):\n        if self.val_dataset:\n            val_dataloader = torch.utils.data.DataLoader(self.val_dataset, \n                                                    batch_size=self.config.batch_size, \n                                                    shuffle=True)\n            return val_dataloader\n        \n    def configure_optimizers(self):\n        optimizer = AdamW(self.model.parameters(), lr=self.config.lr)\n        return optimizer", "\n\nclass LLaMA8bitFineTuner(CommonFinetuner):\n    def __init__(self, model_name, fine_tuning_config, train_dataset, val_dataset=None):\n        super().__init__(model_name, fine_tuning_config, train_dataset, val_dataset)\n\n    def load(self):\n        self.model = prepare_model_for_int8_training(LlamaForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map={'': 0}))\n        \n        lora_config = LoraConfig(\n                r=self.config.lora_r,\n                lora_alpha=self.config.lora_alpha,\n                target_modules=self.config.target_lora_modules,\n                lora_dropout=self.config.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n        self.model = get_peft_model(self.model, lora_config)\n\n        self.model.config.use_cache = False\n\n        old_state_dict = self.model.state_dict\n        self.model.state_dict = (\n            lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n        ).__get__(self.model, type(self.model))", "\n\nclass LLaMA4bitFineTuner(CommonFinetuner):\n    def __init__(self, model_name, checkpoint_path, fine_tuning_config, train_dataset, val_dataset=None):\n        super().__init__(model_name, fine_tuning_config, train_dataset, val_dataset)\n        self.checkpoint_path = checkpoint_path\n\n    def load(self):\n        self.model = load_llama_model_4bit_low_ram(self.model_name, self.checkpoint_path, half=True)\n        lora_config = LoraConfig(\n                r=self.config.lora_r,\n                lora_alpha=self.config.lora_alpha,\n                target_modules=self.config.target_lora_modules,\n                lora_dropout=self.config.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n        self.model = get_peft_model(self.model, lora_config)\n        self.model.config.use_cache = False\n        old_state_dict = self.model.state_dict\n        self.model.state_dict = (\n            lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n        ).__get__(self.model, type(self.model))", "\n\n\nclass GPTJ8bitFineTuner(CommonFinetuner):\n        def __init__(self, model_name, fine_tuning_config, train_dataset, val_dataset=None):\n            super().__init__(model_name, fine_tuning_config, train_dataset, val_dataset)\n            self.squeeze_input = False\n\n        def load(self):    \n            self.model =  prepare_model_for_int8_training(GPTJForCausalLM.from_pretrained(self.model_name, load_in_8bit=True, device_map={'': 0}))\n            lora_config = LoraConfig(\n                    r=self.config.lora_r,\n                    lora_alpha=self.config.lora_alpha,\n                    target_modules=self.config.target_lora_modules,\n                    lora_dropout=self.config.lora_dropout,\n                    bias=\"none\",\n                    task_type=\"CAUSAL_LM\",\n                )\n            \n            self.model = get_peft_model(self.model, lora_config)\n\n            self.model.config.use_cache = False\n\n            old_state_dict = self.model.state_dict\n            self.model.state_dict = (\n                lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n            ).__get__(self.model, type(self.model))", "\n\n\nclass GPTJ4bitFineTuner(CommonFinetuner):\n    def __init__(self, model_name, checkpoint_path, fine_tuning_config, train_dataset, val_dataset=None):\n        super().__init__(model_name, fine_tuning_config, train_dataset, val_dataset)\n        self.checkpoint_path = checkpoint_path\n        self.squeeze_input = False\n        \n    def load(self):\n        self.model = load_gptj_model_4bit_low_ram(self.model_name, self.checkpoint_path, half=True)\n        lora_config = LoraConfig(\n                r=self.config.lora_r,\n                lora_alpha=self.config.lora_alpha,\n                target_modules=self.config.target_lora_modules,\n                lora_dropout=self.config.lora_dropout,\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n        self.model = get_peft_model(self.model, lora_config)\n        self.model.config.use_cache = False\n        old_state_dict = self.model.state_dict\n        self.model.state_dict = (\n            lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n        ).__get__(self.model, type(self.model))"]}
{"filename": "setup_cuda.py", "chunked_list": ["from setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(\n    name='quant_cuda',\n    ext_modules=[cpp_extension.CUDAExtension(\n        'quant_cuda', ['quant_cuda.cpp', 'quant_cuda_kernel.cu']\n    )],\n    cmdclass={'build_ext': cpp_extension.BuildExtension}\n)", "    cmdclass={'build_ext': cpp_extension.BuildExtension}\n)\n"]}
{"filename": "custom_datasets.py", "chunked_list": ["from torch.utils.data import Dataset\nimport pandas as pd\nimport torch\nfrom transformers import LlamaTokenizer\n\n\ndef balance_dataframe(df, column_label):\n    count_classes = df[column_label].value_counts()\n    min_class = min(count_classes)\n    balanced_df = pd.DataFrame()\n    \n    for class_index, _ in count_classes.items():\n        class_df = df[df[column_label] == class_index]\n        balanced_class_df = class_df.sample(min_class)\n        balanced_df = balanced_df.append(balanced_class_df)\n    return balanced_df", "\ndef prompt_tokenize(prompt, completion, tokenizer, max_len, llama=False, truncation=True, padding=True):\n    prompt_toks =  tokenizer.encode(prompt)\n    completion_toks = tokenizer.encode(completion)\n    if llama:\n        completion_toks = completion_toks[1:]\n    if truncation:\n        prompt_toks = prompt_toks[:max_len - len(completion_toks)]\n    sample = torch.tensor(prompt_toks + completion_toks + [tokenizer.eos_token_id], dtype=int).unsqueeze(0)\n    loss_mask = torch.zeros((1, sample.shape[1]), dtype=bool)\n    loss_mask[:, list(range(len(prompt_toks), len(prompt_toks) + len(completion_toks)))] = True\n    attention_mask = torch.ones(sample.shape, dtype=int)\n    if padding:\n        pad_zeros = torch.nn.ConstantPad1d((0, max_len - sample.shape[1]), 0)\n        pad_eos = torch.nn.ConstantPad1d((0, max_len - sample.shape[1]), tokenizer.pad_token_id)\n        \n        sample = pad_eos(sample)\n        loss_mask = pad_zeros(loss_mask)\n        attention_mask = pad_zeros(attention_mask)\n    return sample, attention_mask, loss_mask", "\n\nclass PromptDataset(Dataset):\n\n    @staticmethod\n    def create_prompt(text):\n        prompt =  f''' Classify the following messages into one of the following categories: [Hate Speech], [Offensive language], [Neutral]\n\nMessage: {text}\n\nCategory: '''\n        return prompt\n\n\n    def __init__(self, data_df, tokenizer, max_prompt_len=100, truncation=True, padding=True):\n        self.df = data_df\n        self.tokenizer = tokenizer\n        self.max_prompt_len = max_prompt_len\n        self.truncation = truncation\n        self.padding = padding\n        \n        self.llama = isinstance(tokenizer, LlamaTokenizer)\n\n    def __getitem__(self, idx):\n        \n        data = self.df.iloc[idx]\n        prompt = data['prompt']\n        completion = data['completion']\n        input_ids, attention_mask, loss_mask = prompt_tokenize(prompt, completion, self.tokenizer,  self.max_prompt_len, self.llama, self.truncation, self.padding)\n        return  input_ids, attention_mask, loss_mask\n    \n    def __len__(self):\n        return len(self.df)", "    "]}
{"filename": "autograd_4bit.py", "chunked_list": ["import quant\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport time\nimport transformers\nimport accelerate\nfrom transformers import GPTJConfig, GPTJForCausalLM, LlamaConfig, LlamaForCausalLM, AutoConfig, AutoModelForCausalLM\nfrom modelutils import find_layers\n", "from modelutils import find_layers\n\n# Global Buffer\nbuffer_mat_dic = {}\nuse_new = True\nauto_switch = True\nauto_switch_thd = 16\n\n\ndef get_buffer(shape_of_qweight, dtype=torch.float16, device='cuda'):\n    if shape_of_qweight not in buffer_mat_dic.keys():\n        buffer_mat_dic[shape_of_qweight] = torch.zeros((shape_of_qweight[0] * 8, shape_of_qweight[1]), dtype=dtype, device=device)\n    return buffer_mat_dic[shape_of_qweight]", "\ndef get_buffer(shape_of_qweight, dtype=torch.float16, device='cuda'):\n    if shape_of_qweight not in buffer_mat_dic.keys():\n        buffer_mat_dic[shape_of_qweight] = torch.zeros((shape_of_qweight[0] * 8, shape_of_qweight[1]), dtype=dtype, device=device)\n    return buffer_mat_dic[shape_of_qweight]\n    \n\ndef matmul4bit(x, qweight, scales, zeros):\n    \"\"\"\n    input x: (n, m)\n    qweight: (j, k)\n    where m == j*8\n    \n    perform x @ qweight\n    \n    return y: \n    \"\"\"\n    assert qweight.shape[0] * 8 == x.shape[-1]\n    outshape = tuple(list(x.shape[:-1]) + [qweight.shape[1]])\n    x = x.reshape(-1, x.shape[-1])\n    y = torch.zeros((x.shape[0], qweight.shape[-1]), dtype=torch.float32, device=x.device)\n    dtype = x.dtype\n    x = x.float()\n    quant.quant_cuda.vecquant4matmul(x, qweight, y, scales, zeros)\n    y = y.to(dtype)\n    return y.reshape(outshape)", "\n\ndef matmul4bit_transpose(x, qweight, scales, zeros):\n    \"\"\"\n    input x: (n, m)\n    qweight: (j, k)\n    where m == k\n    \n    perform qweight @ x.T\n    \n    return y: \n    \"\"\"\n    assert qweight.shape[1] == x.shape[-1]\n    outshape = tuple(list(x.shape[:-1]) + [qweight.shape[0] * 8])\n    x = x.reshape(-1, x.shape[-1])\n    y = torch.zeros((qweight.shape[0] * 8, x.shape[0]), dtype=torch.float32, device=x.device)\n    dtype = x.dtype\n    x = x.float()\n    quant.quant_cuda.vecquant4transposematmul(x, qweight, y, scales, zeros)\n    y = y.to(dtype)\n    return y.reshape(outshape)", "\n\ndef matmul4bit_half(x, qweight, scales, zeros):\n    \"\"\"\n    input x: (n, m)\n    qweight: (j, k)\n    where m == j*8\n    \n    perform x @ qweight\n    \n    return y: \n    \"\"\"\n    assert qweight.shape[0] * 8 == x.shape[-1]\n    outshape = tuple(list(x.shape[:-1]) + [qweight.shape[1]])\n    x = x.reshape(-1, x.shape[-1])\n    y = torch.zeros((x.shape[0], qweight.shape[-1]), dtype=x.dtype, device=x.device)\n    dtype = x.dtype\n    quant.quant_cuda.vecquant4matmul_half(x, qweight, y, scales, zeros)\n    y = y.to(dtype)\n    return y.reshape(outshape)", "    \n    \ndef matmul4bit_transpose_half(x, qweight, scales, zeros):\n    \"\"\"\n    input x: (n, m)\n    qweight: (j, k)\n    where m == k\n    \n    perform qweight @ x.T\n    \n    return y: \n    \"\"\"\n    assert qweight.shape[1] == x.shape[-1]\n    outshape = tuple(list(x.shape[:-1]) + [qweight.shape[0] * 8])\n    x = x.reshape(-1, x.shape[-1])\n    y = torch.zeros((qweight.shape[0] * 8, x.shape[0]), dtype=x.dtype, device=x.device)\n    dtype = x.dtype\n    quant.quant_cuda.vecquant4transposematmul_half(x, qweight, y, scales, zeros)\n    y = y.to(dtype)\n    return y.reshape(outshape)", "    \n\ndef fast_4bit_forward(x, qweight, scales, zeros, bias):\n    use_new_flag = use_new\n    if auto_switch:\n        if x.shape[1] > auto_switch_thd:\n            use_new_flag = True\n        else:\n            use_new_flag = False\n    if use_new_flag:\n        buffer = get_buffer(qweight.shape, dtype=scales.dtype, device=qweight.device)\n        quant.quant_cuda.vecquant4recons(qweight, buffer, scales, zeros)\n        output = torch.matmul(x, buffer)\n    else:\n        output = matmul4bit(x, qweight, scales.float(), zeros.float())\n    output += bias\n    return output", "    \n\nclass AutogradMatmul4bit(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, x, qweight, scales, zeros):\n        ctx.save_for_backward(qweight, scales, zeros)\n        buffer = get_buffer(qweight.shape, dtype=scales.dtype, device=qweight.device)\n        quant.quant_cuda.vecquant4recons(qweight, buffer, scales, zeros)\n        output = torch.matmul(x, buffer).clone()\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        qweight, scales, zeros = ctx.saved_tensors\n        buffer = get_buffer(qweight.shape, dtype=scales.dtype, device=qweight.device)\n        quant.quant_cuda.vecquant4recons(qweight, buffer, scales, zeros)\n        grad = torch.matmul(grad_output, buffer.T)\n        return grad, None, None, None", "\n\n# Assumes layer is perfectly divisible into 256 * 256 blocks\nclass Autograd4bitQuantLinear(nn.Module): \n\n    def __init__(self, infeatures, outfeatures):\n        super().__init__()\n        bits = 4\n        self.in_features = infeatures\n        self.out_features = outfeatures\n        self.bits = bits\n        self.register_buffer('zeros', torch.empty((outfeatures, 1)))\n        self.register_buffer('scales', torch.empty((outfeatures, 1)))\n        self.register_buffer('bias', torch.empty(outfeatures))\n        self.register_buffer(\n            'qweight', torch.empty((infeatures // 256 * (bits * 8), outfeatures), dtype=torch.int)\n        )\n\n    def forward(self, x):\n        if torch.is_grad_enabled():\n            out = AutogradMatmul4bit.apply(x, self.qweight, self.scales, self.zeros)\n            out += self.bias\n        else:\n            out = fast_4bit_forward(x, self.qweight, self.scales, self.zeros, self.bias)\n        return out", "\n\ndef make_quant_for_4bit_autograd(module, names, name=''):\n    if isinstance(module, Autograd4bitQuantLinear):\n        return\n    for attr in dir(module):\n        tmp = getattr(module, attr)\n        name1 = name + '.' + attr if name != '' else attr\n        if name1 in names:\n            setattr(\n                module, attr, Autograd4bitQuantLinear(tmp.in_features, tmp.out_features)\n            )\n    for name1, child in module.named_children():\n        make_quant_for_4bit_autograd(child, names, name + '.' + name1 if name != '' else name1)", "\n\ndef model_to_half(model):\n    model.half()\n    for n, m in model.named_modules():\n        if isinstance(m, Autograd4bitQuantLinear):\n            m.zeros = m.zeros.half()\n            m.scales = m.scales.half()\n            m.bias = m.bias.half()\n    print('Converted as Half.')", "\n\ndef model_to_float(model):\n    model.float()\n    for n, m in model.named_modules():\n        if isinstance(m, Autograd4bitQuantLinear):\n            m.zeros = m.zeros.float()\n            m.scales = m.scales.float()\n            m.bias = m.bias.float()\n    print('Converted as Float.')", "\n\ndef load_auto_model_4bit_low_ram(config_path, model_path, half=False):\n\n    print(\"Loading Auto Model ...\")\n    t0 = time.time()\n\n    with accelerate.init_empty_weights():\n        config = AutoConfig.from_pretrained(config_path)\n        torch.set_default_dtype(torch.half)\n        transformers.modeling_utils._init_weights = False\n        torch.set_default_dtype(torch.half)\n        model = AutoModelForCausalLM.from_config(config)\n        torch.set_default_dtype(torch.float)\n        model = model.eval()\n        layers = find_layers(model)\n        for name in ['embed_out', 'lm_head']:\n           if name in layers:\n              del layers[name]\n#        for name in ['lm_head']:\n#           if name in layers:\n#               del layers[name]\n        make_quant_for_4bit_autograd(model, layers)\n    model = accelerate.load_checkpoint_and_dispatch(model=model, checkpoint=model_path, device_map='auto')\n    model.cuda()\n    model.seqlen = 2048\n    \n    if half:\n        model_to_half(model)\n\n    print(f\"Loaded the model in {(time.time()-t0):.2f} seconds.\")\n    \n    return model    ", "\ndef load_llama_model_4bit_low_ram(config_path, model_path, half=False):\n    import transformers\n    import accelerate\n    from transformers import LlamaConfig, LlamaForCausalLM, LlamaTokenizer\n    from modelutils import find_layers\n    \n    print(\"Loading Llama Model ...\")\n    t0 = time.time()\n\n    with accelerate.init_empty_weights():\n        config = LlamaConfig.from_pretrained(config_path)\n        torch.set_default_dtype(torch.half)\n        transformers.modeling_utils._init_weights = False\n        torch.set_default_dtype(torch.half)\n        model = LlamaForCausalLM(config)\n        torch.set_default_dtype(torch.float)\n        model = model.eval()\n        layers = find_layers(model)\n        for name in ['lm_head']:\n            if name in layers:\n                del layers[name]\n        make_quant_for_4bit_autograd(model, layers)\n    model = accelerate.load_checkpoint_and_dispatch(model=model, checkpoint=model_path, device_map='auto')\n    model.cuda()\n    model.seqlen = 2048\n    \n    if half:\n        model_to_half(model)\n\n    print(f\"Loaded the model in {(time.time()-t0):.2f} seconds.\")\n    \n    return model", "    \ndef load_gptj_model_4bit_low_ram(config_path, model_path, half=False):\n    \n    print(\"Loading Llama Model ...\")\n    t0 = time.time()\n\n    with accelerate.init_empty_weights():\n        config = GPTJConfig.from_pretrained(config_path)\n        torch.set_default_dtype(torch.half)\n        transformers.modeling_utils._init_weights = False\n        torch.set_default_dtype(torch.half)\n        model = GPTJForCausalLM(config)\n        torch.set_default_dtype(torch.float)\n        model = model.eval()\n        layers = find_layers(model)\n        for name in ['lm_head']:\n            if name in layers:\n                del layers[name]\n        make_quant_for_4bit_autograd(model, layers)\n    model = accelerate.load_checkpoint_and_dispatch(model=model, checkpoint=model_path, device_map='auto')\n    model.cuda()\n    model.seqlen = 2048\n    \n    if half:\n        model_to_half(model)\n\n\n    print(f\"Loaded the model in {(time.time()-t0):.2f} seconds.\")\n    \n    return model", "    "]}
{"filename": "quant.py", "chunked_list": ["import numpy as np\nimport torch\nimport torch.nn as nn\n\n\ndef quantize(x, scale, zero, maxq):\n    q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n    return scale * (q - zero)\n\nclass Quantizer(nn.Module):\n\n    def __init__(self, shape=1):\n        super(Quantizer, self).__init__()\n        self.register_buffer('maxq', torch.tensor(0))\n        self.register_buffer('scale', torch.zeros(shape))\n        self.register_buffer('zero', torch.zeros(shape))\n\n    def configure(\n            self,\n            bits, perchannel=False, sym=True, \n            mse=False, norm=2.4, grid=100, maxshrink=.8\n        ):\n        self.maxq = torch.tensor(2 ** bits - 1)\n        self.perchannel = perchannel\n        self.sym = sym\n        self.mse = mse\n        self.norm = norm\n        self.grid = grid\n        self.maxshrink = maxshrink \n\n    def find_params(self, x, weight=False):\n        dev = x.device\n        self.maxq = self.maxq.to(dev)\n\n        shape = x.shape\n        if self.perchannel:\n            if weight:\n                x = x.flatten(1)\n            else:\n                if len(shape) == 4:\n                    x = x.permute([1, 0, 2, 3])\n                    x = x.flatten(1)\n                if len(shape) == 3:\n                    x = x.reshape((-1, shape[-1])).t()\n                if len(shape) == 2:\n                    x = x.t()\n        else:\n            x = x.flatten().unsqueeze(0)\n\n        tmp = torch.zeros(x.shape[0], device=dev)\n        xmin = torch.minimum(x.min(1)[0], tmp)\n        xmax = torch.maximum(x.max(1)[0], tmp)\n\n        if self.sym:\n            xmax = torch.maximum(torch.abs(xmin), xmax)\n            tmp = xmin < 0\n            if torch.any(tmp):\n                xmin[tmp] = -xmax[tmp]\n        tmp = (xmin == 0) & (xmax == 0)\n        xmin[tmp] = -1\n        xmax[tmp] = +1\n\n        self.scale = (xmax - xmin) / self.maxq\n        if self.sym:\n            self.zero = torch.full_like(self.scale, (self.maxq + 1) / 2)\n        else:\n            self.zero = torch.round(-xmin / self.scale)\n\n        if self.mse:\n            best = torch.full([x.shape[0]], float('inf'), device=dev)\n            for i in range(int(self.maxshrink * self.grid)):\n                p = 1 - i / self.grid \n                xmin1 = p * xmin\n                xmax1 = p * xmax\n                scale1 = (xmax1 - xmin1) / self.maxq\n                zero1 = torch.round(-xmin1 / scale1) if not self.sym else self.zero\n                q = quantize(x, scale1.unsqueeze(1), zero1.unsqueeze(1), self.maxq)\n                q -= x\n                q.abs_()\n                q.pow_(self.norm)\n                err = torch.sum(q, 1)\n                tmp = err < best\n                if torch.any(tmp):\n                    best[tmp] = err[tmp]\n                    self.scale[tmp] = scale1[tmp]\n                    self.zero[tmp] = zero1[tmp]\n        if not self.perchannel:\n            if weight:\n                tmp = shape[0]\n            else:\n                tmp = shape[1] if len(shape) != 3 else shape[2]\n            self.scale = self.scale.repeat(tmp)\n            self.zero = self.zero.repeat(tmp)\n\n        if weight:\n            shape = [-1] + [1] * (len(shape) - 1)\n            self.scale = self.scale.reshape(shape)\n            self.zero = self.zero.reshape(shape)\n            return\n        if len(shape) == 4:\n            self.scale = self.scale.reshape((1, -1, 1, 1))\n            self.zero = self.zero.reshape((1, -1, 1, 1))\n        if len(shape) == 3:\n            self.scale = self.scale.reshape((1, 1, -1))\n            self.zero = self.zero.reshape((1, 1, -1)) \n        if len(shape) == 2:\n            self.scale = self.scale.unsqueeze(0)\n            self.zero = self.zero.unsqueeze(0)\n\n    def quantize(self, x):\n        if self.ready():\n            return quantize(x, self.scale, self.zero, self.maxq)\n        return x\n\n    def enabled(self):\n        return self.maxq > 0\n\n    def ready(self):\n        return torch.all(self.scale != 0)", "\nclass Quantizer(nn.Module):\n\n    def __init__(self, shape=1):\n        super(Quantizer, self).__init__()\n        self.register_buffer('maxq', torch.tensor(0))\n        self.register_buffer('scale', torch.zeros(shape))\n        self.register_buffer('zero', torch.zeros(shape))\n\n    def configure(\n            self,\n            bits, perchannel=False, sym=True, \n            mse=False, norm=2.4, grid=100, maxshrink=.8\n        ):\n        self.maxq = torch.tensor(2 ** bits - 1)\n        self.perchannel = perchannel\n        self.sym = sym\n        self.mse = mse\n        self.norm = norm\n        self.grid = grid\n        self.maxshrink = maxshrink \n\n    def find_params(self, x, weight=False):\n        dev = x.device\n        self.maxq = self.maxq.to(dev)\n\n        shape = x.shape\n        if self.perchannel:\n            if weight:\n                x = x.flatten(1)\n            else:\n                if len(shape) == 4:\n                    x = x.permute([1, 0, 2, 3])\n                    x = x.flatten(1)\n                if len(shape) == 3:\n                    x = x.reshape((-1, shape[-1])).t()\n                if len(shape) == 2:\n                    x = x.t()\n        else:\n            x = x.flatten().unsqueeze(0)\n\n        tmp = torch.zeros(x.shape[0], device=dev)\n        xmin = torch.minimum(x.min(1)[0], tmp)\n        xmax = torch.maximum(x.max(1)[0], tmp)\n\n        if self.sym:\n            xmax = torch.maximum(torch.abs(xmin), xmax)\n            tmp = xmin < 0\n            if torch.any(tmp):\n                xmin[tmp] = -xmax[tmp]\n        tmp = (xmin == 0) & (xmax == 0)\n        xmin[tmp] = -1\n        xmax[tmp] = +1\n\n        self.scale = (xmax - xmin) / self.maxq\n        if self.sym:\n            self.zero = torch.full_like(self.scale, (self.maxq + 1) / 2)\n        else:\n            self.zero = torch.round(-xmin / self.scale)\n\n        if self.mse:\n            best = torch.full([x.shape[0]], float('inf'), device=dev)\n            for i in range(int(self.maxshrink * self.grid)):\n                p = 1 - i / self.grid \n                xmin1 = p * xmin\n                xmax1 = p * xmax\n                scale1 = (xmax1 - xmin1) / self.maxq\n                zero1 = torch.round(-xmin1 / scale1) if not self.sym else self.zero\n                q = quantize(x, scale1.unsqueeze(1), zero1.unsqueeze(1), self.maxq)\n                q -= x\n                q.abs_()\n                q.pow_(self.norm)\n                err = torch.sum(q, 1)\n                tmp = err < best\n                if torch.any(tmp):\n                    best[tmp] = err[tmp]\n                    self.scale[tmp] = scale1[tmp]\n                    self.zero[tmp] = zero1[tmp]\n        if not self.perchannel:\n            if weight:\n                tmp = shape[0]\n            else:\n                tmp = shape[1] if len(shape) != 3 else shape[2]\n            self.scale = self.scale.repeat(tmp)\n            self.zero = self.zero.repeat(tmp)\n\n        if weight:\n            shape = [-1] + [1] * (len(shape) - 1)\n            self.scale = self.scale.reshape(shape)\n            self.zero = self.zero.reshape(shape)\n            return\n        if len(shape) == 4:\n            self.scale = self.scale.reshape((1, -1, 1, 1))\n            self.zero = self.zero.reshape((1, -1, 1, 1))\n        if len(shape) == 3:\n            self.scale = self.scale.reshape((1, 1, -1))\n            self.zero = self.zero.reshape((1, 1, -1)) \n        if len(shape) == 2:\n            self.scale = self.scale.unsqueeze(0)\n            self.zero = self.zero.unsqueeze(0)\n\n    def quantize(self, x):\n        if self.ready():\n            return quantize(x, self.scale, self.zero, self.maxq)\n        return x\n\n    def enabled(self):\n        return self.maxq > 0\n\n    def ready(self):\n        return torch.all(self.scale != 0)", "\n\ntry:\n    import quant_cuda\nexcept:\n    print('CUDA extension not installed.')\n\n# Assumes layer is perfectly divisible into 256 * 256 blocks\nclass QuantLinear(nn.Module): \n    def __init__(self, bits, infeatures, outfeatures):\n        super().__init__()\n        if bits not in [2,3,4,8]:\n            raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n        self.bits = bits\n        self.register_buffer('zeros', torch.zeros((outfeatures, 1)))\n        self.register_buffer('scales', torch.zeros((outfeatures, 1)))\n        self.register_buffer('bias', torch.zeros(outfeatures))\n        self.register_buffer(\n            'qweight', torch.zeros((infeatures // 256 * (bits * 8), outfeatures), dtype=torch.int)\n        )\n\n    def pack(self, linear, scales, zeros):\n        self.zeros = zeros * scales\n        self.scales = scales.clone()\n        if linear.bias is not None:\n            self.bias = linear.bias.clone()  \n\n        intweight = torch.round((linear.weight.data + self.zeros) / self.scales).to(torch.int)\n        intweight = intweight.t().contiguous()\n        intweight = intweight.numpy().astype(np.uint32)\n        qweight = np.zeros(\n            (intweight.shape[0] // 256 * (self.bits * 8), intweight.shape[1]), dtype=np.uint32\n        )\n        i = 0\n        row = 0\n        while row < qweight.shape[0]:\n            if self.bits in [2,4,8]:\n                for j in range(i, i + (32//self.bits)):\n                    qweight[row] |= intweight[j] << (self.bits * (j - i))\n                i += 32//self.bits\n                row += 1\n            elif self.bits == 3:\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i))\n                i += 10\n                qweight[row] |= intweight[i] << 30\n                row += 1\n                qweight[row] |= (intweight[i] >> 2) & 1\n                i += 1\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i) + 1)\n                i += 10\n                qweight[row] |= intweight[i] << 31\n                row += 1\n                qweight[row] |= (intweight[i] >> 1) & 0x3\n                i += 1\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i) + 2)\n                i += 10\n                row += 1\n            else:\n                raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n                \n        qweight = qweight.astype(np.int32)\n        self.qweight = torch.from_numpy(qweight) \n\n    def forward(self, x):\n        outshape = list(x.shape)\n        x = x.reshape(-1, x.shape[-1])\n        y = self.bias.clone().repeat(x.shape[0],1)\n        outshape[-1] = self.bias.numel()\n        dtype = x.dtype\n        x = x.float()\n        if self.bits == 2:\n            quant_cuda.vecquant2matmul(x, self.qweight, y, self.scales, self.zeros)\n        elif self.bits == 3:\n            quant_cuda.vecquant3matmul(x, self.qweight, y, self.scales, self.zeros)\n        elif self.bits == 4:\n            quant_cuda.vecquant4matmul(x, self.qweight, y, self.scales, self.zeros)\n        elif self.bits == 8:\n            quant_cuda.vecquant8matmul(x, self.qweight, y, self.scales, self.zeros)\n        else:\n            raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n        y = y.to(dtype)\n        return y.reshape(outshape)", "class QuantLinear(nn.Module): \n    def __init__(self, bits, infeatures, outfeatures):\n        super().__init__()\n        if bits not in [2,3,4,8]:\n            raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n        self.bits = bits\n        self.register_buffer('zeros', torch.zeros((outfeatures, 1)))\n        self.register_buffer('scales', torch.zeros((outfeatures, 1)))\n        self.register_buffer('bias', torch.zeros(outfeatures))\n        self.register_buffer(\n            'qweight', torch.zeros((infeatures // 256 * (bits * 8), outfeatures), dtype=torch.int)\n        )\n\n    def pack(self, linear, scales, zeros):\n        self.zeros = zeros * scales\n        self.scales = scales.clone()\n        if linear.bias is not None:\n            self.bias = linear.bias.clone()  \n\n        intweight = torch.round((linear.weight.data + self.zeros) / self.scales).to(torch.int)\n        intweight = intweight.t().contiguous()\n        intweight = intweight.numpy().astype(np.uint32)\n        qweight = np.zeros(\n            (intweight.shape[0] // 256 * (self.bits * 8), intweight.shape[1]), dtype=np.uint32\n        )\n        i = 0\n        row = 0\n        while row < qweight.shape[0]:\n            if self.bits in [2,4,8]:\n                for j in range(i, i + (32//self.bits)):\n                    qweight[row] |= intweight[j] << (self.bits * (j - i))\n                i += 32//self.bits\n                row += 1\n            elif self.bits == 3:\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i))\n                i += 10\n                qweight[row] |= intweight[i] << 30\n                row += 1\n                qweight[row] |= (intweight[i] >> 2) & 1\n                i += 1\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i) + 1)\n                i += 10\n                qweight[row] |= intweight[i] << 31\n                row += 1\n                qweight[row] |= (intweight[i] >> 1) & 0x3\n                i += 1\n                for j in range(i, i + 10):\n                    qweight[row] |= intweight[j] << (3 * (j - i) + 2)\n                i += 10\n                row += 1\n            else:\n                raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n                \n        qweight = qweight.astype(np.int32)\n        self.qweight = torch.from_numpy(qweight) \n\n    def forward(self, x):\n        outshape = list(x.shape)\n        x = x.reshape(-1, x.shape[-1])\n        y = self.bias.clone().repeat(x.shape[0],1)\n        outshape[-1] = self.bias.numel()\n        dtype = x.dtype\n        x = x.float()\n        if self.bits == 2:\n            quant_cuda.vecquant2matmul(x, self.qweight, y, self.scales, self.zeros)\n        elif self.bits == 3:\n            quant_cuda.vecquant3matmul(x, self.qweight, y, self.scales, self.zeros)\n        elif self.bits == 4:\n            quant_cuda.vecquant4matmul(x, self.qweight, y, self.scales, self.zeros)\n        elif self.bits == 8:\n            quant_cuda.vecquant8matmul(x, self.qweight, y, self.scales, self.zeros)\n        else:\n            raise NotImplementedError(\"Only 2,3,4,8 bits are supported.\")\n        y = y.to(dtype)\n        return y.reshape(outshape)", "\ndef make_quant(module, names, bits, name=''):\n    if isinstance(module, QuantLinear):\n        return\n    for attr in dir(module):\n        tmp = getattr(module, attr)\n        name1 = name + '.' + attr if name != '' else attr\n        if name1 in names:\n            setattr(\n                module, attr, QuantLinear(bits, tmp.in_features, tmp.out_features)\n            )\n    for name1, child in module.named_children():\n        make_quant(child, names, bits, name + '.' + name1 if name != '' else name1)", ""]}
{"filename": "gradient_checkpointing.py", "chunked_list": ["from transformers.models.llama.modeling_llama import LlamaDecoderLayer\nfrom torch.utils.checkpoint import checkpoint\nfrom torch.autograd import Variable\nimport torch\nfrom torch import nn\nimport numpy as np\n\n\nclass NewForward:\n\n    def __init__(self, layer):\n        self.layer = layer\n        self.apply_patch()\n\n    def apply_patch(self):\n        self.layer.old_forward_for_cp = self.layer.forward\n        self.layer.forward = self.new_forward\n\n    def new_forward(self, *args, **kwargs):\n        def func(*args):\n            return self.layer.old_forward_for_cp(*args, **kwargs)\n        output = checkpoint(func, *args)\n        return output", "class NewForward:\n\n    def __init__(self, layer):\n        self.layer = layer\n        self.apply_patch()\n\n    def apply_patch(self):\n        self.layer.old_forward_for_cp = self.layer.forward\n        self.layer.forward = self.new_forward\n\n    def new_forward(self, *args, **kwargs):\n        def func(*args):\n            return self.layer.old_forward_for_cp(*args, **kwargs)\n        output = checkpoint(func, *args)\n        return output", "\n\nclass VarWrapper:\n\n    def __init__(self, model):\n        self.model = model\n        self.apply_patch()\n        print('Var Wrapper Patch Applied')\n\n    def apply_patch(self):\n        self.model.old_forward_for_cp = self.model.forward\n        self.model.forward = self.new_forward\n\n    def new_forward(self, *args, **kwargs):\n        out = self.model.old_forward_for_cp(*args, **kwargs)\n        out = Variable(out.data, requires_grad=True)\n        return out", "\n\ndef apply_gradient_checkpointing(model, checkpoint_ratio=1):\n    new_forwards = []\n    modules = []\n    for n, m in model.named_modules():\n        if isinstance(m, LlamaDecoderLayer):\n            modules.append(m)\n    if checkpoint_ratio < 1 and checkpoint_ratio > 0:\n        checkpoint_locs = np.array((np.linspace(0, 1, int(len(modules) * checkpoint_ratio)) * (len(modules)-1)).round(), dtype=int)\n    else:\n        checkpoint_locs = np.arange(len(modules))\n    for i in checkpoint_locs:\n        m = modules[i]\n        new_forwards.append(NewForward(m))\n        print('Forward Patch Applied For Block {}'.format(i))\n    for n, m in model.named_modules():\n        if isinstance(m, torch.nn.Embedding):\n            wrapper = VarWrapper(m)\n            break\n    return new_forwards, wrapper", ""]}
{"filename": "peft/peft_model.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nimport os\nimport warnings", "import os\nimport warnings\nfrom contextlib import contextmanager\n\nimport torch\nfrom accelerate import dispatch_model, infer_auto_device_map\nfrom accelerate.hooks import AlignDevicesHook, add_hook_to_module, remove_hook_from_submodules\nfrom accelerate.utils import get_balanced_memory\nfrom huggingface_hub import hf_hub_download\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss", "from huggingface_hub import hf_hub_download\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom transformers import PreTrainedModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput, TokenClassifierOutput\nfrom transformers.utils import PushToHubMixin\n\nfrom .tuners import LoraModel, PrefixEncoder, PromptEmbedding, PromptEncoder\nfrom .utils import (\n    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n    WEIGHTS_NAME,", "    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n    WEIGHTS_NAME,\n    PeftConfig,\n    PeftType,\n    PromptLearningConfig,\n    TaskType,\n    _set_trainable,\n    get_peft_model_state_dict,\n    set_peft_model_state_dict,\n    shift_tokens_right,", "    set_peft_model_state_dict,\n    shift_tokens_right,\n)\n\n\nclass PeftModel(PushToHubMixin, torch.nn.Module):\n    \"\"\"\n    Parameter-Efficient Fine-Tuning Model. Base model encompassing various Peft methods.\n\n    Args:\n        model ([`PreTrainedModel`]): The base transformer model used for Peft.\n        peft_config ([`PeftConfig`]): The configuration of the Peft model.\n\n\n    **Attributes**:\n        - **base_model** ([`PreTrainedModel`]) -- The base transformer model used for Peft.\n        - **peft_config** ([`PeftConfig`]) -- The configuration of the Peft model.\n        - **modules_to_save** (`list` of `str`) -- The list of sub-module names to save when\n        saving the model.\n        - **prompt_encoder** ([`PromptEncoder`]) -- The prompt encoder used for Peft if\n        `isinstance(self.peft_config, PromptLearningConfig)`.\n        - **prompt_tokens** (`torch.Tensor`) -- The virtual prompt tokens used for Peft if\n        `isinstance(self.peft_config, PromptLearningConfig)`.\n        - **transformer_backbone_name** (`str`) -- The name of the transformer\n        backbone in the base model if `isinstance(self.peft_config, PromptLearningConfig)`.\n        - **word_embeddings** (`torch.nn.Embedding`) -- The word embeddings of the transformer backbone\n        in the base model if `isinstance(self.peft_config, PromptLearningConfig)`.\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig):\n        super().__init__()\n        self.peft_config = peft_config\n        self.base_model = model\n        self.config = self.base_model.config\n        self.modules_to_save = None\n        if isinstance(self.peft_config, PromptLearningConfig):\n            self._setup_prompt_encoder()\n        else:\n            self.base_model = LoraModel(peft_config, model)\n        if getattr(self.peft_config, \"modules_to_save\", None) is not None:\n            self.modules_to_save = self.peft_config.modules_to_save\n            _set_trainable(self)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def save_pretrained(self, save_directory, **kwargs):\n        r\"\"\"\n        Args:\n        This function saves the adapter model and the adapter configuration files to a directory, so that it can be\n        re-loaded using the `LoraModel.from_pretrained` class method, and also used by the `LoraModel.push_to_hub`\n        method.\n            save_directory (`str`):\n                Directory where the adapter model and configuration files will be saved (will be created if it does not\n                exist).\n            **kwargs:\n                Additional keyword arguments passed along to the `push_to_hub` method.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise ValueError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n        os.makedirs(save_directory, exist_ok=True)\n\n        # save only the trainable weights\n        output_state_dict = get_peft_model_state_dict(self, kwargs.get(\"state_dict\", None))\n        torch.save(output_state_dict, os.path.join(save_directory, WEIGHTS_NAME))\n\n        # save the config and change the inference mode to `True`\n        if self.peft_config.base_model_name_or_path is None:\n            self.peft_config.base_model_name_or_path = (\n                self.base_model.__dict__.get(\"name_or_path\", None)\n                if isinstance(self.peft_config, PromptLearningConfig)\n                else self.base_model.model.__dict__.get(\"name_or_path\", None)\n            )\n        inference_mode = self.peft_config.inference_mode\n        self.peft_config.inference_mode = True\n        self.peft_config.save_pretrained(save_directory)\n        self.peft_config.inference_mode = inference_mode\n\n    @classmethod\n    def from_pretrained(cls, model, model_id, **kwargs):\n        r\"\"\"\n        Args:\n        Instantiate a `LoraModel` from a pretrained Lora configuration and weights.\n            model (`transformers.PreTrainedModel`):\n                The model to be adapted. The model should be initialized with the `from_pretrained` method. from\n                `transformers` library.\n            model_id (`str`):\n                The name of the Lora configuration to use. Can be either:\n                    - A string, the `model id` of a Lora configuration hosted inside a model repo on\n                        huggingface Hub\n                    - A path to a directory containing a Lora configuration file saved using the\n                        `save_pretrained` method, e.g., ``./my_lora_config_directory/``.\n        \"\"\"\n        from .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING, PEFT_TYPE_TO_CONFIG_MAPPING\n\n        # load the config\n        config = PEFT_TYPE_TO_CONFIG_MAPPING[PeftConfig.from_pretrained(model_id).peft_type].from_pretrained(model_id)\n\n        if getattr(model, \"hf_device_map\", None) is not None:\n            remove_hook_from_submodules(model)\n\n        if config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys():\n            model = cls(model, config)\n        else:\n            model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config)\n\n        # load weights if any\n        if os.path.exists(os.path.join(model_id, WEIGHTS_NAME)):\n            filename = os.path.join(model_id, WEIGHTS_NAME)\n        else:\n            try:\n                filename = hf_hub_download(model_id, WEIGHTS_NAME)\n            except:  # noqa\n                raise ValueError(\n                    f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. \"\n                    f\"Please check that the file {WEIGHTS_NAME} is present at {model_id}.\"\n                )\n\n        adapters_weights = torch.load(\n            filename, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        )\n        # load the weights into the model\n        model = set_peft_model_state_dict(model, adapters_weights)\n        if getattr(model, \"hf_device_map\", None) is not None:\n            device_map = kwargs.get(\"device_map\", \"auto\")\n            max_memory = kwargs.get(\"max_memory\", None)\n            no_split_module_classes = model._no_split_modules\n            if device_map != \"sequential\":\n                max_memory = get_balanced_memory(\n                    model,\n                    max_memory=max_memory,\n                    no_split_module_classes=no_split_module_classes,\n                    low_zero=(device_map == \"balanced_low_0\"),\n                )\n            if isinstance(device_map, str):\n                device_map = infer_auto_device_map(\n                    model, max_memory=max_memory, no_split_module_classes=no_split_module_classes\n                )\n            model = dispatch_model(model, device_map=device_map)\n            hook = AlignDevicesHook(io_same_device=True)\n            if model.peft_config.peft_type == PeftType.LORA:\n                add_hook_to_module(model.base_model.model, hook)\n            else:\n                remove_hook_from_submodules(model.prompt_encoder)\n                add_hook_to_module(model.base_model, hook)\n        return model\n\n    def _setup_prompt_encoder(self):\n        transformer_backbone = None\n        for name, module in self.base_model.named_children():\n            for param in module.parameters():\n                param.requires_grad = False\n            if isinstance(module, PreTrainedModel):\n                # Make sure to freeze Tranformers model\n                if transformer_backbone is None:\n                    transformer_backbone = module\n                    self.transformer_backbone_name = name\n\n        if self.peft_config.num_transformer_submodules is None:\n            self.peft_config.num_transformer_submodules = (\n                2 if self.peft_config.task_type == TaskType.SEQ_2_SEQ_LM else 1\n            )\n\n        for named_param, value in list(transformer_backbone.named_parameters()):\n            if value.shape[0] == self.base_model.config.vocab_size:\n                self.word_embeddings = transformer_backbone.get_submodule(named_param.replace(\".weight\", \"\"))\n                break\n\n        if self.peft_config.peft_type == PeftType.PROMPT_TUNING:\n            prompt_encoder = PromptEmbedding(self.peft_config, self.word_embeddings)\n        elif self.peft_config.peft_type == PeftType.P_TUNING:\n            prompt_encoder = PromptEncoder(self.peft_config)\n        elif self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n            prompt_encoder = PrefixEncoder(self.peft_config)\n        else:\n            raise ValueError(\"Not supported\")\n        self.prompt_encoder = prompt_encoder\n        self.prompt_tokens = torch.arange(\n            self.peft_config.num_virtual_tokens * self.peft_config.num_transformer_submodules\n        ).long()\n\n    def get_prompt_embedding_to_save(self):\n        \"\"\"\n        Returns the prompt embedding to save when saving the model. Only applicable when `peft_config.peft_type !=\n        PeftType.LORA`.\n        \"\"\"\n        prompt_tokens = self.prompt_tokens.unsqueeze(0).expand(1, -1).to(self.device)\n        if self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n            prompt_tokens = prompt_tokens[:, : self.peft_config.num_virtual_tokens]\n        prompt_embeddings = self.prompt_encoder(prompt_tokens)\n        return prompt_embeddings[0].detach().cpu()\n\n    def get_prompt(self, batch_size):\n        \"\"\"\n        Returns the virtual prompts to use for Peft. Only applicable when `peft_config.peft_type != PeftType.LORA`.\n        \"\"\"\n        prompt_tokens = self.prompt_tokens.unsqueeze(0).expand(batch_size, -1).to(self.device)\n        if self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n            prompt_tokens = prompt_tokens[:, : self.peft_config.num_virtual_tokens]\n            if self.peft_config.inference_mode:\n                past_key_values = self.prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n            else:\n                past_key_values = self.prompt_encoder(prompt_tokens)\n            past_key_values = past_key_values.view(\n                batch_size,\n                self.peft_config.num_virtual_tokens,\n                self.peft_config.num_layers * 2,\n                self.peft_config.num_attention_heads,\n                self.peft_config.token_dim // self.peft_config.num_attention_heads,\n            )\n            if self.peft_config.num_transformer_submodules == 2:\n                past_key_values = torch.cat([past_key_values, past_key_values], dim=2)\n            past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(\n                self.peft_config.num_transformer_submodules * 2\n            )\n            if TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING.get(self.config.model_type, None) is not None:\n                post_process_fn = TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING[self.config.model_type]\n                past_key_values = post_process_fn(past_key_values)\n            return past_key_values\n        else:\n            if self.peft_config.inference_mode:\n                prompts = self.prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n            else:\n                prompts = self.prompt_encoder(prompt_tokens)\n            return prompts\n\n    def print_trainable_parameters(self):\n        \"\"\"\n        Prints the number of trainable parameters in the model.\n        \"\"\"\n        trainable_params = 0\n        all_param = 0\n        for _, param in self.named_parameters():\n            num_params = param.numel()\n            # if using DS Zero 3 and the weights are initialized empty\n            if num_params == 0 and hasattr(param, \"ds_numel\"):\n                num_params = param.ds_numel\n\n            all_param += num_params\n            if param.requires_grad:\n                trainable_params += num_params\n        print(\n            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n        )\n\n    def __getattr__(self, name: str):\n        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n        try:\n            return super().__getattr__(name)  # defer to nn.Module's logic\n        except AttributeError:\n            return getattr(self.base_model, name)\n\n    def forward(self, *args, **kwargs):\n        \"\"\"\n        Forward pass of the model.\n        \"\"\"\n        return self.get_base_model()(*args, **kwargs)\n\n    @contextmanager\n    def disable_adapter(self):\n        \"\"\"\n        Disables the adapter module.\n        \"\"\"\n        if isinstance(self.peft_config, PromptLearningConfig):\n            old_forward = self.forward\n            self.forward = self.base_model.forward\n        else:\n            self.base_model.disable_adapter_layers()\n        yield\n        if isinstance(self.peft_config, PromptLearningConfig):\n            self.forward = old_forward\n        else:\n            self.base_model.enable_adapter_layers()\n\n    def get_base_model(self):\n        \"\"\"\n        Returns the base model.\n        \"\"\"\n        return self.base_model if isinstance(self.peft_config, PromptLearningConfig) else self.base_model.model", "\n\nclass PeftModelForSequenceClassification(PeftModel):\n    \"\"\"\n    Peft model for sequence classification tasks.\n\n    Args:\n        model ([`PreTrainedModel`]): Base transformer model\n        peft_config ([`PeftConfig`]): Peft config.\n\n    **Attributes**:\n        - **config** ([`PretrainedConfig`]) -- The configuration object of the base model.\n        - **cls_layer_name** (`str`) -- The name of the classification layer.\n\n    Example::\n\n        >>> from transformers import AutoModelForSequenceClassification >>> from peft import\n        PeftModelForSequenceClassification, get_peft_config >>> config = {\n                'peft_type': 'PREFIX_TUNING', 'task_type': 'SEQ_CLS', 'inference_mode': False, 'num_virtual_tokens':\n                20, 'token_dim': 768, 'num_transformer_submodules': 1, 'num_attention_heads': 12, 'num_layers': 12,\n                'encoder_hidden_size': 768, 'prefix_projection': False, 'postprocess_past_key_value_function': None\n            }\n        >>> peft_config = get_peft_config(config) >>> model =\n        AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\") >>> peft_model =\n        PeftModelForSequenceClassification(model, peft_config) >>> peft_model.print_trainable_parameters() trainable\n        params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig):\n        super().__init__(model, peft_config)\n        self.modules_to_save = [\"classifier\", \"score\"]\n\n        for name, _ in self.base_model.named_children():\n            if any(module_name in name for module_name in self.modules_to_save):\n                self.cls_layer_name = name\n                break\n\n        # to make sure classifier layer is trainable\n        _set_trainable(self)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if not isinstance(self.peft_config, PromptLearningConfig):\n            return self.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                **kwargs,\n            )\n\n        batch_size = input_ids.shape[0]\n        if attention_mask is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(batch_size, self.peft_config.num_virtual_tokens).to(self.device)\n            attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n            kwargs[\"position_ids\"] = None\n        kwargs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n            }\n        )\n\n        if self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n            return self._prefix_tuning_forward(input_ids=input_ids, **kwargs)\n        else:\n            if kwargs.get(\"token_type_ids\", None) is not None:\n                kwargs[\"token_type_ids\"] = torch.cat(\n                    (\n                        torch.zeros(batch_size, self.peft_config.num_virtual_tokens).to(self.device),\n                        kwargs[\"token_type_ids\"],\n                    ),\n                    dim=1,\n                ).long()\n            if inputs_embeds is None:\n                inputs_embeds = self.word_embeddings(input_ids)\n            prompts = self.get_prompt(batch_size=batch_size)\n            prompts = prompts.to(inputs_embeds.dtype)\n            inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)\n            return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n\n    def _prefix_tuning_forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        batch_size = input_ids.shape[0]\n        past_key_values = self.get_prompt(batch_size)\n        fwd_params = list(inspect.signature(self.base_model.forward).parameters.keys())\n        kwargs.update(\n            {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"inputs_embeds\": inputs_embeds,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n                \"past_key_values\": past_key_values,\n            }\n        )\n        if \"past_key_values\" in fwd_params:\n            return self.base_model(labels=labels, **kwargs)\n        else:\n            transformer_backbone_name = self.base_model.get_submodule(self.transformer_backbone_name)\n            fwd_params = list(inspect.signature(transformer_backbone_name.forward).parameters.keys())\n            if \"past_key_values\" not in fwd_params:\n                raise ValueError(\"Model does not support past key values which are required for prefix tuning.\")\n            outputs = transformer_backbone_name(**kwargs)\n            pooled_output = outputs[1] if len(outputs) > 1 else outputs[0]\n            if \"dropout\" in [name for name, _ in list(self.base_model.named_children())]:\n                pooled_output = self.base_model.dropout(pooled_output)\n            logits = self.base_model.get_submodule(self.cls_layer_name)(pooled_output)\n\n            loss = None\n            if labels is not None:\n                if self.config.problem_type is None:\n                    if self.base_model.num_labels == 1:\n                        self.config.problem_type = \"regression\"\n                    elif self.base_model.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                        self.config.problem_type = \"single_label_classification\"\n                    else:\n                        self.config.problem_type = \"multi_label_classification\"\n\n                if self.config.problem_type == \"regression\":\n                    loss_fct = MSELoss()\n                    if self.base_model.num_labels == 1:\n                        loss = loss_fct(logits.squeeze(), labels.squeeze())\n                    else:\n                        loss = loss_fct(logits, labels)\n                elif self.config.problem_type == \"single_label_classification\":\n                    loss_fct = CrossEntropyLoss()\n                    loss = loss_fct(logits.view(-1, self.base_model.num_labels), labels.view(-1))\n                elif self.config.problem_type == \"multi_label_classification\":\n                    loss_fct = BCEWithLogitsLoss()\n                    loss = loss_fct(logits, labels)\n            if not return_dict:\n                output = (logits,) + outputs[2:]\n                return ((loss,) + output) if loss is not None else output\n\n            return SequenceClassifierOutput(\n                loss=loss,\n                logits=logits,\n                hidden_states=outputs.hidden_states,\n                attentions=outputs.attentions,\n            )", "\n\nclass PeftModelForCausalLM(PeftModel):\n    \"\"\"\n    Peft model for Causal LM\n\n    Args:\n        model ([`PreTrainedModel`]): Base transformer model\n        peft_config ([`PeftConfig`]): Peft config.\n\n\n    Example::\n\n        >>> from transformers import AutoModelForCausalLM >>> from peft import PeftModelForCausalLM, get_peft_config\n        >>> config = {\n                'peft_type': 'PREFIX_TUNING', 'task_type': 'CAUSAL_LM', 'inference_mode': False, 'num_virtual_tokens':\n                20, 'token_dim': 1280, 'num_transformer_submodules': 1, 'num_attention_heads': 20, 'num_layers': 36,\n                'encoder_hidden_size': 1280, 'prefix_projection': False, 'postprocess_past_key_value_function': None\n            }\n        >>> peft_config = get_peft_config(config) >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\") >>>\n        peft_model = PeftModelForCausalLM(model, peft_config) >>> peft_model.print_trainable_parameters() trainable\n        params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig):\n        super().__init__(model, peft_config)\n        self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        if not isinstance(self.peft_config, PromptLearningConfig):\n            return self.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                **kwargs,\n            )\n\n        batch_size = input_ids.shape[0]\n        if attention_mask is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(batch_size, self.peft_config.num_virtual_tokens).to(self.device)\n            attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n            kwargs[\"position_ids\"] = None\n        if kwargs.get(\"token_type_ids\", None) is not None:\n            warnings.warn(\"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\")\n            kwargs[\"token_type_ids\"] = None\n        kwargs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n            }\n        )\n\n        if self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n            past_key_values = self.get_prompt(batch_size)\n            return self.base_model(input_ids=input_ids, past_key_values=past_key_values, **kwargs)\n        else:\n            if inputs_embeds is None:\n                inputs_embeds = self.word_embeddings(input_ids)\n            # concat prompt labels\n            if labels is not None:\n                prefix_labels = torch.full((batch_size, self.peft_config.num_virtual_tokens), -100).to(self.device)\n                kwargs[\"labels\"] = torch.cat((prefix_labels, labels), dim=1)\n            prompts = self.get_prompt(batch_size=batch_size)\n            prompts = prompts.to(inputs_embeds.dtype)\n            inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)\n            return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n\n    def generate(self, **kwargs):\n        self.base_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation\n        try:\n            if not isinstance(self.peft_config, PromptLearningConfig):\n                outputs = self.base_model.generate(**kwargs)\n            else:\n                if \"input_ids\" not in kwargs:\n                    raise ValueError(\"input_ids must be provided for Peft model generation\")\n                if kwargs.get(\"attention_mask\", None) is not None:\n                    # concat prompt attention mask\n                    prefix_attention_mask = torch.ones(\n                        kwargs[\"input_ids\"].shape[0], self.peft_config.num_virtual_tokens\n                    ).to(kwargs[\"input_ids\"].device)\n                    kwargs[\"attention_mask\"] = torch.cat((prefix_attention_mask, kwargs[\"attention_mask\"]), dim=1)\n\n                if kwargs.get(\"position_ids\", None) is not None:\n                    warnings.warn(\n                        \"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\"\n                    )\n                    kwargs[\"position_ids\"] = None\n                if kwargs.get(\"token_type_ids\", None) is not None:\n                    warnings.warn(\n                        \"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\"\n                    )\n                    kwargs[\"token_type_ids\"] = None\n\n                outputs = self.base_model.generate(**kwargs)\n        except:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            raise\n        else:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            return outputs\n\n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        model_kwargs = self.base_model_prepare_inputs_for_generation(*args, **kwargs)\n        if isinstance(self.peft_config, PromptLearningConfig):\n            if model_kwargs[\"past_key_values\"] is None and self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n                past_key_values = self.get_prompt(batch_size=model_kwargs[\"input_ids\"].shape[0])\n                model_kwargs[\"past_key_values\"] = past_key_values\n            else:\n                if model_kwargs[\"past_key_values\"] is None:\n                    inputs_embeds = self.word_embeddings(model_kwargs[\"input_ids\"])\n                    prompts = self.get_prompt(batch_size=model_kwargs[\"input_ids\"].shape[0])\n                    prompts = prompts.to(inputs_embeds.dtype)\n                    model_kwargs[\"inputs_embeds\"] = torch.cat((prompts, inputs_embeds), dim=1)\n                    model_kwargs[\"input_ids\"] = None\n\n        return model_kwargs", "\n\nclass PeftModelForSeq2SeqLM(PeftModel):\n    \"\"\"\n    Peft model for Seq2Seq LM\n\n    Args:\n        model ([`PreTrainedModel`]): Base transformer model\n        peft_config ([`PeftConfig`]): Peft config.\n\n\n    Example::\n\n        >>> from transformers import AutoModelForSeq2SeqLM >>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n        >>> config = {\n                'peft_type': 'LORA', 'task_type': 'SEQ_2_SEQ_LM', 'inference_mode': False, 'r': 8, 'target_modules':\n                ['q', 'v'], 'lora_alpha': 32, 'lora_dropout': 0.1, 'merge_weights': False, 'fan_in_fan_out': False,\n                'enable_lora': None, 'bias': 'none'\n            }\n        >>> peft_config = get_peft_config(config) >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\") >>>\n        peft_model = PeftModelForSeq2SeqLM(model, peft_config) >>> peft_model.print_trainable_parameters() trainable\n        params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig):\n        super().__init__(model, peft_config)\n        self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\n        self.base_model_prepare_encoder_decoder_kwargs_for_generation = (\n            self.base_model._prepare_encoder_decoder_kwargs_for_generation\n        )\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        decoder_input_ids=None,\n        decoder_attention_mask=None,\n        decoder_inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        if not isinstance(self.peft_config, PromptLearningConfig):\n            return self.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                decoder_input_ids=decoder_input_ids,\n                decoder_attention_mask=decoder_attention_mask,\n                decoder_inputs_embeds=decoder_inputs_embeds,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                **kwargs,\n            )\n\n        batch_size = input_ids.shape[0]\n        if decoder_attention_mask is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(batch_size, self.peft_config.num_virtual_tokens).to(self.device)\n            decoder_attention_mask = torch.cat((prefix_attention_mask, decoder_attention_mask), dim=1)\n\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n            kwargs[\"position_ids\"] = None\n        if kwargs.get(\"token_type_ids\", None) is not None:\n            warnings.warn(\"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\")\n            kwargs[\"token_type_ids\"] = None\n        kwargs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"decoder_attention_mask\": decoder_attention_mask,\n                \"labels\": labels,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n            }\n        )\n\n        if self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n            past_key_values = self.get_prompt(batch_size)\n            return self.base_model(\n                input_ids=input_ids, decoder_input_ids=decoder_input_ids, past_key_values=past_key_values, **kwargs\n            )\n        else:\n            if inputs_embeds is None:\n                inputs_embeds = self.word_embeddings(input_ids)\n            if decoder_inputs_embeds is None and decoder_input_ids is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n                decoder_inputs_embeds = self.word_embeddings(decoder_input_ids)\n\n            if attention_mask is not None:\n                # concat prompt attention mask\n                prefix_attention_mask = torch.ones(batch_size, self.peft_config.num_virtual_tokens).to(self.device)\n                kwargs[\"attention_mask\"] = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n            # concat prompt labels\n            if labels is not None:\n                if self.peft_config.num_transformer_submodules == 1:\n                    kwargs[\"labels\"] = labels\n                elif self.peft_config.num_transformer_submodules == 2:\n                    prefix_labels = torch.full((batch_size, self.peft_config.num_virtual_tokens), -100).to(self.device)\n                    kwargs[\"labels\"] = torch.cat((prefix_labels, labels), dim=1)\n            prompts = self.get_prompt(batch_size=batch_size)\n            prompts = prompts.to(inputs_embeds.dtype)\n            inputs_embeds = torch.cat((prompts[:, : self.peft_config.num_virtual_tokens], inputs_embeds), dim=1)\n            if self.peft_config.num_transformer_submodules == 1:\n                return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n            elif self.peft_config.num_transformer_submodules == 2:\n                decoder_inputs_embeds = torch.cat(\n                    (prompts[:, self.peft_config.num_virtual_tokens :], decoder_inputs_embeds), dim=1\n                )\n                return self.base_model(\n                    inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, **kwargs\n                )\n\n    def generate(self, **kwargs):\n        self.base_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation\n        self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n            self._prepare_encoder_decoder_kwargs_for_generation\n        )\n        try:\n            if not isinstance(self.peft_config, PromptLearningConfig):\n                outputs = self.base_model.generate(**kwargs)\n            else:\n                if \"input_ids\" not in kwargs:\n                    raise ValueError(\"input_ids must be provided for Peft model generation\")\n                if kwargs.get(\"position_ids\", None) is not None:\n                    warnings.warn(\n                        \"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\"\n                    )\n                    kwargs[\"position_ids\"] = None\n                if kwargs.get(\"token_type_ids\", None) is not None:\n                    warnings.warn(\n                        \"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\"\n                    )\n                    kwargs[\"token_type_ids\"] = None\n\n                if self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n                    outputs = self.base_model.generate(**kwargs)\n                else:\n                    raise NotImplementedError\n        except:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n                self.base_model_prepare_encoder_decoder_kwargs_for_generation\n            )\n            raise\n        else:\n            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n            self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n                self.base_model_prepare_encoder_decoder_kwargs_for_generation\n            )\n            return outputs\n\n    def prepare_inputs_for_generation(self, *args, **kwargs):\n        model_kwargs = self.base_model_prepare_inputs_for_generation(*args, **kwargs)\n        if model_kwargs[\"past_key_values\"] is None and self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n            batch_size = model_kwargs[\"decoder_input_ids\"].shape[0]\n            past_key_values = self.get_prompt(batch_size)\n            model_kwargs[\"past_key_values\"] = past_key_values\n        return model_kwargs", "\n\nclass PeftModelForTokenClassification(PeftModel):\n    \"\"\"\n    Peft model for sequence classification tasks.\n\n    Args:\n        model ([`PreTrainedModel`]): Base transformer model\n        peft_config ([`PeftConfig`]): Peft config.\n\n    **Attributes**:\n        - **config** ([`PretrainedConfig`]) -- The configuration object of the base model.\n        - **cls_layer_name** (`str`) -- The name of the classification layer.\n\n    Example::\n\n        >>> from transformers import AutoModelForSequenceClassification >>> from peft import\n        PeftModelForTokenClassification, get_peft_config >>> config = {\n                'peft_type': 'PREFIX_TUNING', 'task_type': 'TOKEN_CLS', 'inference_mode': False, 'num_virtual_tokens':\n                20, 'token_dim': 768, 'num_transformer_submodules': 1, 'num_attention_heads': 12, 'num_layers': 12,\n                'encoder_hidden_size': 768, 'prefix_projection': False, 'postprocess_past_key_value_function': None\n            }\n        >>> peft_config = get_peft_config(config) >>> model =\n        AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\") >>> peft_model =\n        PeftModelForTokenClassification(model, peft_config) >>> peft_model.print_trainable_parameters() trainable\n        params: 370178 || all params: 108680450 || trainable%: 0.3406113979101117\n    \"\"\"\n\n    def __init__(self, model, peft_config: PeftConfig):\n        super().__init__(model, peft_config)\n        self.modules_to_save = [\"classifier\", \"score\"]\n\n        for name, _ in self.base_model.named_children():\n            if any(module_name in name for module_name in self.modules_to_save):\n                self.cls_layer_name = name\n                break\n\n        # to make sure classifier layer is trainable\n        _set_trainable(self)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if not isinstance(self.peft_config, PromptLearningConfig):\n            return self.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                labels=labels,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n                **kwargs,\n            )\n\n        batch_size = input_ids.shape[0]\n        if attention_mask is not None:\n            # concat prompt attention mask\n            prefix_attention_mask = torch.ones(batch_size, self.peft_config.num_virtual_tokens).to(self.device)\n            attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n        if kwargs.get(\"position_ids\", None) is not None:\n            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n            kwargs[\"position_ids\"] = None\n        kwargs.update(\n            {\n                \"attention_mask\": attention_mask,\n                \"labels\": labels,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n            }\n        )\n\n        if self.peft_config.peft_type == PeftType.PREFIX_TUNING:\n            return self._prefix_tuning_forward(input_ids=input_ids, **kwargs)\n        else:\n            if kwargs.get(\"token_type_ids\", None) is not None:\n                kwargs[\"token_type_ids\"] = torch.cat(\n                    (\n                        torch.zeros(batch_size, self.peft_config.num_virtual_tokens).to(self.device),\n                        kwargs[\"token_type_ids\"],\n                    ),\n                    dim=1,\n                ).long()\n            if inputs_embeds is None:\n                inputs_embeds = self.word_embeddings(input_ids)\n            prompts = self.get_prompt(batch_size=batch_size)\n            prompts = prompts.to(inputs_embeds.dtype)\n            inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)\n            return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n\n    def _prefix_tuning_forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        **kwargs,\n    ):\n        batch_size = input_ids.shape[0]\n        past_key_values = self.get_prompt(batch_size)\n        fwd_params = list(inspect.signature(self.base_model.forward).parameters.keys())\n        kwargs.update(\n            {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"inputs_embeds\": inputs_embeds,\n                \"output_attentions\": output_attentions,\n                \"output_hidden_states\": output_hidden_states,\n                \"return_dict\": return_dict,\n                \"past_key_values\": past_key_values,\n            }\n        )\n        if \"past_key_values\" in fwd_params:\n            return self.base_model(labels=labels, **kwargs)\n        else:\n            transformer_backbone_name = self.base_model.get_submodule(self.transformer_backbone_name)\n            fwd_params = list(inspect.signature(transformer_backbone_name.forward).parameters.keys())\n            if \"past_key_values\" not in fwd_params:\n                raise ValueError(\"Model does not support past key values which are required for prefix tuning.\")\n            outputs = transformer_backbone_name(**kwargs)\n            sequence_output = outputs[0]\n            if \"dropout\" in [name for name, _ in list(self.base_model.named_children())]:\n                sequence_output = self.base_model.dropout(sequence_output)\n            logits = self.base_model.get_submodule(self.cls_layer_name)(sequence_output)\n\n            loss = None\n            loss = None\n            if labels is not None:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n            if not return_dict:\n                output = (logits,) + outputs[2:]\n                return ((loss,) + output) if loss is not None else output\n\n            return TokenClassifierOutput(\n                loss=loss,\n                logits=logits,\n                hidden_states=outputs.hidden_states,\n                attentions=outputs.attentions,\n            )", ""]}
{"filename": "peft/mapping.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .peft_model import (\n    PeftModel,\n    PeftModelForCausalLM,", "    PeftModel,\n    PeftModelForCausalLM,\n    PeftModelForSeq2SeqLM,\n    PeftModelForSequenceClassification,\n    PeftModelForTokenClassification,\n)\nfrom .tuners import LoraConfig, PrefixTuningConfig, PromptEncoderConfig, PromptTuningConfig\nfrom .utils import PromptLearningConfig\n\n", "\n\nMODEL_TYPE_TO_PEFT_MODEL_MAPPING = {\n    \"SEQ_CLS\": PeftModelForSequenceClassification,\n    \"SEQ_2_SEQ_LM\": PeftModelForSeq2SeqLM,\n    \"CAUSAL_LM\": PeftModelForCausalLM,\n    \"TOKEN_CLS\": PeftModelForTokenClassification,\n}\n\nPEFT_TYPE_TO_CONFIG_MAPPING = {", "\nPEFT_TYPE_TO_CONFIG_MAPPING = {\n    \"PROMPT_TUNING\": PromptTuningConfig,\n    \"PREFIX_TUNING\": PrefixTuningConfig,\n    \"P_TUNING\": PromptEncoderConfig,\n    \"LORA\": LoraConfig,\n}\n\nTRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n    \"t5\": [\"q\", \"v\"],", "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\n    \"t5\": [\"q\", \"v\"],\n    \"mt5\": [\"q\", \"v\"],\n    \"bart\": [\"q_proj\", \"v_proj\"],\n    \"gpt2\": [\"c_attn\"],\n    \"bloom\": [\"query_key_value\"],\n    \"opt\": [\"q_proj\", \"v_proj\"],\n    \"gptj\": [\"q_proj\", \"v_proj\"],\n    \"gpt_neox\": [\"query_key_value\"],\n    \"gpt_neo\": [\"q_proj\", \"v_proj\"],", "    \"gpt_neox\": [\"query_key_value\"],\n    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n    \"bert\": [\"query\", \"value\"],\n    \"roberta\": [\"query\", \"value\"],\n    \"xlm-roberta\": [\"query\", \"value\"],\n    \"electra\": [\"query\", \"value\"],\n    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n    \"deberta\": [\"in_proj\"],\n    \"layoutlm\": [\"query\", \"value\"],\n    \"llama\": [\"q_proj\", \"v_proj\"],", "    \"layoutlm\": [\"query\", \"value\"],\n    \"llama\": [\"q_proj\", \"v_proj\"],\n    \"chatglm\": [\"query_key_value\"],\n}\n\n\ndef get_peft_config(config_dict):\n    \"\"\"\n    Returns a Peft config object from a dictionary.\n\n    Args:\n        config_dict (`Dict[str, Any]`): Dictionary containing the configuration parameters.\n    \"\"\"\n\n    return PEFT_TYPE_TO_CONFIG_MAPPING[config_dict[\"peft_type\"]](**config_dict)", "\n\ndef _prepare_prompt_learning_config(peft_config, model_config):\n    if peft_config.num_layers is None:\n        if \"num_hidden_layers\" in model_config:\n            num_layers = model_config[\"num_hidden_layers\"]\n        elif \"num_layers\" in model_config:\n            num_layers = model_config[\"num_layers\"]\n        elif \"n_layer\" in model_config:\n            num_layers = model_config[\"n_layer\"]\n        else:\n            raise ValueError(\"Please specify `num_layers` in `peft_config`\")\n        peft_config.num_layers = num_layers\n\n    if peft_config.token_dim is None:\n        if \"hidden_size\" in model_config:\n            token_dim = model_config[\"hidden_size\"]\n        elif \"n_embd\" in model_config:\n            token_dim = model_config[\"n_embd\"]\n        elif \"d_model\" in model_config:\n            token_dim = model_config[\"d_model\"]\n        else:\n            raise ValueError(\"Please specify `token_dim` in `peft_config`\")\n        peft_config.token_dim = token_dim\n\n    if peft_config.num_attention_heads is None:\n        if \"num_attention_heads\" in model_config:\n            num_attention_heads = model_config[\"num_attention_heads\"]\n        elif \"n_head\" in model_config:\n            num_attention_heads = model_config[\"n_head\"]\n        elif \"num_heads\" in model_config:\n            num_attention_heads = model_config[\"num_heads\"]\n        elif \"encoder_attention_heads\" in model_config:\n            num_attention_heads = model_config[\"encoder_attention_heads\"]\n        else:\n            raise ValueError(\"Please specify `num_attention_heads` in `peft_config`\")\n        peft_config.num_attention_heads = num_attention_heads\n\n    if getattr(peft_config, \"encoder_hidden_size\", None) is None:\n        setattr(peft_config, \"encoder_hidden_size\", token_dim)\n\n    return peft_config", "\n\ndef _prepare_lora_config(peft_config, model_config):\n    if peft_config.target_modules is None:\n        if model_config[\"model_type\"] not in TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:\n            raise ValueError(\"Please specify `target_modules` in `peft_config`\")\n        peft_config.target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config[\"model_type\"]]\n    if len(peft_config.target_modules) == 1:\n        peft_config.fan_in_fan_out = True\n        peft_config.enable_lora = [True, False, True]\n    if peft_config.inference_mode:\n        peft_config.merge_weights = True\n    return peft_config", "\n\ndef get_peft_model(model, peft_config):\n    \"\"\"\n    Returns a Peft model object from a model and a config.\n\n    Args:\n        model ([`transformers.PreTrainedModel`]): Model to be wrapped.\n        peft_config ([`PeftConfig`]): Configuration object containing the parameters of the Peft model.\n    \"\"\"\n\n    model_config = model.config.to_dict()\n    peft_config.base_model_name_or_path = model.__dict__.get(\"name_or_path\", None)\n    if peft_config.task_type not in MODEL_TYPE_TO_PEFT_MODEL_MAPPING.keys():\n        peft_config = _prepare_lora_config(peft_config, model_config)\n        return PeftModel(model, peft_config)\n    if not isinstance(peft_config, PromptLearningConfig):\n        peft_config = _prepare_lora_config(peft_config, model_config)\n    else:\n        peft_config = _prepare_prompt_learning_config(peft_config, model_config)\n    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config)", ""]}
{"filename": "peft/__init__.py", "chunked_list": ["# flake8: noqa\n# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n# module, but to preserve other warnings. So, don't check this module at all.\n\n# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at", "# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.", "# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__version__ = \"0.3.0.dev0\"\n\nfrom .mapping import MODEL_TYPE_TO_PEFT_MODEL_MAPPING, PEFT_TYPE_TO_CONFIG_MAPPING, get_peft_config, get_peft_model\nfrom .peft_model import (\n    PeftModel,\n    PeftModelForCausalLM,\n    PeftModelForSeq2SeqLM,", "    PeftModelForCausalLM,\n    PeftModelForSeq2SeqLM,\n    PeftModelForSequenceClassification,\n    PeftModelForTokenClassification,\n)\nfrom .tuners import (\n    LoraConfig,\n    LoraModel,\n    PrefixEncoder,\n    PrefixTuningConfig,", "    PrefixEncoder,\n    PrefixTuningConfig,\n    PromptEmbedding,\n    PromptEncoder,\n    PromptEncoderConfig,\n    PromptEncoderReparameterizationType,\n    PromptTuningConfig,\n    PromptTuningInit,\n)\nfrom .utils import (", ")\nfrom .utils import (\n    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n    PeftConfig,\n    PeftType,\n    PromptLearningConfig,\n    TaskType,\n    bloom_model_postprocess_past_key_value,\n    get_peft_model_state_dict,\n    prepare_model_for_int8_training,", "    get_peft_model_state_dict,\n    prepare_model_for_int8_training,\n    set_peft_model_state_dict,\n    shift_tokens_right,\n)\n"]}
{"filename": "peft/utils/save_and_load.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .config import PeftType\n\n\ndef get_peft_model_state_dict(model, state_dict=None):\n    \"\"\"\n    Get the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\n        the model should be the underlying model/unwrapped model (i.e. model.module).\n        state_dict (`dict`, *optional*, defaults to `None`):\n            The state dict of the model. If not provided, the state dict of the model\n        will be used.\n    \"\"\"\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if model.peft_config.peft_type == PeftType.LORA:\n        # to_return = lora_state_dict(model, bias=model.peft_config.bias)\n        # adapted from `https://github.com/microsoft/LoRA/blob/main/loralib/utils.py`\n        # to directly with the state dict which is necessary when using DeepSpeed or FSDP\n        bias = model.peft_config.bias\n        if bias == \"none\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k}\n        elif bias == \"all\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k or \"bias\" in k}\n        elif bias == \"lora_only\":\n            to_return = {}\n            for k in state_dict:\n                if \"lora_\" in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split(\"lora_\")[0] + \"bias\"\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n    else:\n        to_return = {}\n        if model.peft_config.inference_mode:\n            prompt_embeddings = model.prompt_encoder.embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save()\n        to_return[\"prompt_embeddings\"] = prompt_embeddings\n    if model.modules_to_save is not None:\n        for key, value in state_dict.items():\n            if any(module_name in key for module_name in model.modules_to_save):\n                to_return[key] = value\n    return to_return", "\n\ndef get_peft_model_state_dict(model, state_dict=None):\n    \"\"\"\n    Get the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model. When using torch.nn.DistributedDataParallel, DeepSpeed or FSDP,\n        the model should be the underlying model/unwrapped model (i.e. model.module).\n        state_dict (`dict`, *optional*, defaults to `None`):\n            The state dict of the model. If not provided, the state dict of the model\n        will be used.\n    \"\"\"\n    if state_dict is None:\n        state_dict = model.state_dict()\n    if model.peft_config.peft_type == PeftType.LORA:\n        # to_return = lora_state_dict(model, bias=model.peft_config.bias)\n        # adapted from `https://github.com/microsoft/LoRA/blob/main/loralib/utils.py`\n        # to directly with the state dict which is necessary when using DeepSpeed or FSDP\n        bias = model.peft_config.bias\n        if bias == \"none\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k}\n        elif bias == \"all\":\n            to_return = {k: state_dict[k] for k in state_dict if \"lora_\" in k or \"bias\" in k}\n        elif bias == \"lora_only\":\n            to_return = {}\n            for k in state_dict:\n                if \"lora_\" in k:\n                    to_return[k] = state_dict[k]\n                    bias_name = k.split(\"lora_\")[0] + \"bias\"\n                    if bias_name in state_dict:\n                        to_return[bias_name] = state_dict[bias_name]\n        else:\n            raise NotImplementedError\n    else:\n        to_return = {}\n        if model.peft_config.inference_mode:\n            prompt_embeddings = model.prompt_encoder.embedding.weight\n        else:\n            prompt_embeddings = model.get_prompt_embedding_to_save()\n        to_return[\"prompt_embeddings\"] = prompt_embeddings\n    if model.modules_to_save is not None:\n        for key, value in state_dict.items():\n            if any(module_name in key for module_name in model.modules_to_save):\n                to_return[key] = value\n    return to_return", "\n\ndef set_peft_model_state_dict(model, peft_model_state_dict):\n    \"\"\"\n    Set the state dict of the Peft model.\n\n    Args:\n        model ([`PeftModel`]): The Peft model.\n        peft_model_state_dict (`dict`): The state dict of the Peft model.\n    \"\"\"\n\n    model.load_state_dict(peft_model_state_dict, strict=False)\n    if model.peft_config.peft_type != PeftType.LORA:\n        model.prompt_encoder.embedding.load_state_dict(\n            {\"weight\": peft_model_state_dict[\"prompt_embeddings\"]}, strict=True\n        )\n    return model", ""]}
{"filename": "peft/utils/other.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\n\n", "\n\n# needed for prefix-tuning of bloom model\ndef bloom_model_postprocess_past_key_value(past_key_values):\n    past_key_values = torch.cat(past_key_values)\n    total_layers, batch_size, num_attention_heads, num_virtual_tokens, head_dim = past_key_values.shape\n    keys = past_key_values[: total_layers // 2]\n    keys = keys.transpose(2, 3).reshape(\n        total_layers // 2, batch_size * num_attention_heads, head_dim, num_virtual_tokens\n    )\n    values = past_key_values[total_layers // 2 :]\n    values = values.reshape(total_layers // 2, batch_size * num_attention_heads, num_virtual_tokens, head_dim)\n\n    return tuple(zip(keys, values))", "\n\ndef prepare_model_for_int8_training(\n    model, output_embedding_layer_name=\"lm_head\", use_gradient_checkpointing=True, layer_norm_names=[\"layer_norm\"]\n):\n    r\"\"\"\n    This method wrapps the entire protocol for preparing a model before running a training. This includes:\n        1- Cast the layernorm in fp32 2- making output embedding layer require grads 3- Add the upcasting of the lm\n        head to fp32\n\n    Args:\n        model, (`transformers.PreTrainedModel`):\n            The loaded model from `transformers`\n    \"\"\"\n    loaded_in_8bit = getattr(model, \"is_loaded_in_8bit\", False)\n\n    for name, param in model.named_parameters():\n        # freeze base model's layers\n        param.requires_grad = False\n\n        if loaded_in_8bit:\n            # cast layer norm in fp32 for stability for 8bit models\n            if param.ndim == 1 and any(layer_norm_name in name for layer_norm_name in layer_norm_names):\n                param.data = param.data.to(torch.float32)\n\n    if loaded_in_8bit and use_gradient_checkpointing:\n        # For backward compatibility\n        if hasattr(model, \"enable_input_require_grads\"):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n\n        # enable gradient checkpointing for memory efficiency\n        model.gradient_checkpointing_enable()\n\n    if hasattr(model, output_embedding_layer_name):\n        output_embedding_layer = getattr(model, output_embedding_layer_name)\n        input_dtype = output_embedding_layer.weight.dtype\n\n        class CastOutputToFloat(torch.nn.Sequential):\n            r\"\"\"\n            Manually cast to the expected dtype of the lm_head as sometimes there is a final layer norm that is casted\n            in fp32\n\n            \"\"\"\n\n            def forward(self, x):\n                return super().forward(x.to(input_dtype)).to(torch.float32)\n\n        setattr(model, output_embedding_layer_name, CastOutputToFloat(output_embedding_layer))\n\n    return model", "\n\nTRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING = {\n    \"bloom\": bloom_model_postprocess_past_key_value,\n}\n\n\n# copied from transformers.models.bart.modeling_bart\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`): input ids\n        pad_token_id (`int`): The id of the `padding` token.\n        decoder_start_token_id (`int`): The id of the `start` token.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids", "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`): input ids\n        pad_token_id (`int`): The id of the `padding` token.\n        decoder_start_token_id (`int`): The id of the `start` token.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids", "\n\ndef _set_trainable(model):\n    if model.modules_to_save is not None:\n        for name, param in model.named_parameters():\n            if any(module_name in name for module_name in model.modules_to_save):\n                param.requires_grad = True\n\n\ndef fsdp_auto_wrap_policy(model):\n    import functools\n    import os\n\n    from accelerate import FullyShardedDataParallelPlugin\n    from torch.distributed.fsdp.wrap import _or_policy, lambda_auto_wrap_policy, transformer_auto_wrap_policy\n\n    from ..tuners import PrefixEncoder, PromptEmbedding, PromptEncoder\n\n    def lambda_policy_fn(module):\n        if (\n            len(list(module.named_children())) == 0\n            and getattr(module, \"weight\", None) is not None\n            and module.weight.requires_grad\n        ):\n            return True\n        return False\n\n    lambda_policy = functools.partial(lambda_auto_wrap_policy, lambda_fn=lambda_policy_fn)\n    transformer_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls=(\n            PrefixEncoder,\n            PromptEncoder,\n            PromptEmbedding,\n            FullyShardedDataParallelPlugin.get_module_class_from_name(\n                model, os.environ.get(\"FSDP_TRANSFORMER_CLS_TO_WRAP\", \"\")\n            ),\n        ),\n    )\n\n    auto_wrap_policy = functools.partial(_or_policy, policies=[lambda_policy, transformer_wrap_policy])\n    return auto_wrap_policy", "\ndef fsdp_auto_wrap_policy(model):\n    import functools\n    import os\n\n    from accelerate import FullyShardedDataParallelPlugin\n    from torch.distributed.fsdp.wrap import _or_policy, lambda_auto_wrap_policy, transformer_auto_wrap_policy\n\n    from ..tuners import PrefixEncoder, PromptEmbedding, PromptEncoder\n\n    def lambda_policy_fn(module):\n        if (\n            len(list(module.named_children())) == 0\n            and getattr(module, \"weight\", None) is not None\n            and module.weight.requires_grad\n        ):\n            return True\n        return False\n\n    lambda_policy = functools.partial(lambda_auto_wrap_policy, lambda_fn=lambda_policy_fn)\n    transformer_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls=(\n            PrefixEncoder,\n            PromptEncoder,\n            PromptEmbedding,\n            FullyShardedDataParallelPlugin.get_module_class_from_name(\n                model, os.environ.get(\"FSDP_TRANSFORMER_CLS_TO_WRAP\", \"\")\n            ),\n        ),\n    )\n\n    auto_wrap_policy = functools.partial(_or_policy, policies=[lambda_policy, transformer_wrap_policy])\n    return auto_wrap_policy", "\n\ndef transpose(weight, fan_in_fan_out):\n    return weight.T if fan_in_fan_out else weight\n"]}
{"filename": "peft/utils/config.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport enum\nimport json\nimport os\nfrom dataclasses import asdict, dataclass, field", "import os\nfrom dataclasses import asdict, dataclass, field\nfrom typing import Optional, Union\n\nfrom huggingface_hub import hf_hub_download\nfrom transformers.utils import PushToHubMixin\n\nfrom .adapters_utils import CONFIG_NAME\n\n\nclass PeftType(str, enum.Enum):\n    PROMPT_TUNING = \"PROMPT_TUNING\"\n    P_TUNING = \"P_TUNING\"\n    PREFIX_TUNING = \"PREFIX_TUNING\"\n    LORA = \"LORA\"", "\n\nclass PeftType(str, enum.Enum):\n    PROMPT_TUNING = \"PROMPT_TUNING\"\n    P_TUNING = \"P_TUNING\"\n    PREFIX_TUNING = \"PREFIX_TUNING\"\n    LORA = \"LORA\"\n\n\nclass TaskType(str, enum.Enum):\n    SEQ_CLS = \"SEQ_CLS\"\n    SEQ_2_SEQ_LM = \"SEQ_2_SEQ_LM\"\n    CAUSAL_LM = \"CAUSAL_LM\"\n    TOKEN_CLS = \"TOKEN_CLS\"", "\nclass TaskType(str, enum.Enum):\n    SEQ_CLS = \"SEQ_CLS\"\n    SEQ_2_SEQ_LM = \"SEQ_2_SEQ_LM\"\n    CAUSAL_LM = \"CAUSAL_LM\"\n    TOKEN_CLS = \"TOKEN_CLS\"\n\n\n@dataclass\nclass PeftConfigMixin(PushToHubMixin):\n    r\"\"\"\n    This is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\n    PEFT adapter models. This class inherits from `transformers.utils.PushToHubMixin` which contains the methods to\n    push your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a\n    directory. The method `from_pretrained` will load the configuration of your adapter model from a directory.\n\n    Args:\n        peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n    \"\"\"\n    peft_type: Optional[PeftType] = field(default=None, metadata={\"help\": \"The type of PEFT model.\"})\n\n    @property\n    def __dict__(self):\n        return asdict(self)\n\n    def to_dict(self):\n        return self.__dict__\n\n    def save_pretrained(self, save_directory, **kwargs):\n        r\"\"\"\n        This method saves the configuration of your adapter model in a directory.\n\n        Args:\n            save_directory (`str`):\n                The directory where the configuration will be saved.\n            **kwargs:\n                Additional keyword arguments passed along to the `transformers.utils.PushToHubMixin.push_to_hub`\n                method.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        output_dict = self.__dict__\n        output_path = os.path.join(save_directory, CONFIG_NAME)\n\n        # save it\n        with open(output_path, \"w\") as writer:\n            writer.write(json.dumps(output_dict, indent=2, sort_keys=True))\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        r\"\"\"\n        This method loads the configuration of your adapter model from a directory.\n\n        Args:\n            pretrained_model_name_or_path (`str`):\n                The directory or the hub-id where the configuration is saved.\n            **kwargs:\n                Additional keyword arguments passed along to the child class initialization.\n        \"\"\"\n        if os.path.isfile(os.path.join(pretrained_model_name_or_path, CONFIG_NAME)):\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        else:\n            try:\n                config_file = hf_hub_download(pretrained_model_name_or_path, CONFIG_NAME)\n            except Exception:\n                raise ValueError(f\"Can't find config.json at '{pretrained_model_name_or_path}'\")\n\n        loaded_attributes = cls.from_json_file(config_file)\n\n        config = cls(**kwargs)\n\n        for key, value in loaded_attributes.items():\n            if hasattr(config, key):\n                setattr(config, key, value)\n\n        return config\n\n    @classmethod\n    def from_json_file(cls, path_json_file, **kwargs):\n        r\"\"\"\n        Loads a configuration file from a json file.\n\n        Args:\n            path_json_file (`str`):\n                The path to the json file.\n        \"\"\"\n        with open(path_json_file, \"r\") as file:\n            json_object = json.load(file)\n\n        return json_object", "@dataclass\nclass PeftConfigMixin(PushToHubMixin):\n    r\"\"\"\n    This is the base configuration class for PEFT adapter models. It contains all the methods that are common to all\n    PEFT adapter models. This class inherits from `transformers.utils.PushToHubMixin` which contains the methods to\n    push your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a\n    directory. The method `from_pretrained` will load the configuration of your adapter model from a directory.\n\n    Args:\n        peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n    \"\"\"\n    peft_type: Optional[PeftType] = field(default=None, metadata={\"help\": \"The type of PEFT model.\"})\n\n    @property\n    def __dict__(self):\n        return asdict(self)\n\n    def to_dict(self):\n        return self.__dict__\n\n    def save_pretrained(self, save_directory, **kwargs):\n        r\"\"\"\n        This method saves the configuration of your adapter model in a directory.\n\n        Args:\n            save_directory (`str`):\n                The directory where the configuration will be saved.\n            **kwargs:\n                Additional keyword arguments passed along to the `transformers.utils.PushToHubMixin.push_to_hub`\n                method.\n        \"\"\"\n        if os.path.isfile(save_directory):\n            raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n\n        os.makedirs(save_directory, exist_ok=True)\n\n        output_dict = self.__dict__\n        output_path = os.path.join(save_directory, CONFIG_NAME)\n\n        # save it\n        with open(output_path, \"w\") as writer:\n            writer.write(json.dumps(output_dict, indent=2, sort_keys=True))\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        r\"\"\"\n        This method loads the configuration of your adapter model from a directory.\n\n        Args:\n            pretrained_model_name_or_path (`str`):\n                The directory or the hub-id where the configuration is saved.\n            **kwargs:\n                Additional keyword arguments passed along to the child class initialization.\n        \"\"\"\n        if os.path.isfile(os.path.join(pretrained_model_name_or_path, CONFIG_NAME)):\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        else:\n            try:\n                config_file = hf_hub_download(pretrained_model_name_or_path, CONFIG_NAME)\n            except Exception:\n                raise ValueError(f\"Can't find config.json at '{pretrained_model_name_or_path}'\")\n\n        loaded_attributes = cls.from_json_file(config_file)\n\n        config = cls(**kwargs)\n\n        for key, value in loaded_attributes.items():\n            if hasattr(config, key):\n                setattr(config, key, value)\n\n        return config\n\n    @classmethod\n    def from_json_file(cls, path_json_file, **kwargs):\n        r\"\"\"\n        Loads a configuration file from a json file.\n\n        Args:\n            path_json_file (`str`):\n                The path to the json file.\n        \"\"\"\n        with open(path_json_file, \"r\") as file:\n            json_object = json.load(file)\n\n        return json_object", "\n\n@dataclass\nclass PeftConfig(PeftConfigMixin):\n    \"\"\"\n    This is the base configuration class to store the configuration of a :class:`~peft.PeftModel`.\n\n    Args:\n        peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n        task_type (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.\n        inference_mode (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.\n    \"\"\"\n\n    base_model_name_or_path: str = field(default=None, metadata={\"help\": \"The name of the base model to use.\"})\n    peft_type: Union[str, PeftType] = field(default=None, metadata={\"help\": \"Peft type\"})\n    task_type: Union[str, TaskType] = field(default=None, metadata={\"help\": \"Task type\"})\n    inference_mode: bool = field(default=False, metadata={\"help\": \"Whether to use inference mode\"})", "\n\n@dataclass\nclass PromptLearningConfig(PeftConfig):\n    \"\"\"\n    This is the base configuration class to store the configuration of a Union[[`~peft.PrefixTuning`],\n    [`~peft.PromptEncoder`], [`~peft.PromptTuning`]].\n\n    Args:\n        num_virtual_tokens (`int`): The number of virtual tokens to use.\n        token_dim (`int`): The hidden embedding dimension of the base transformer model.\n        num_transformer_submodules (`int`): The number of transformer submodules in the base transformer model.\n        num_attention_heads (`int`): The number of attention heads in the base transformer model.\n        num_layers (`int`): The number of layers in the base transformer model.\n    \"\"\"\n\n    num_virtual_tokens: int = field(default=None, metadata={\"help\": \"Number of virtual tokens\"})\n    token_dim: int = field(\n        default=None, metadata={\"help\": \"The hidden embedding dimension of the base transformer model\"}\n    )\n    num_transformer_submodules: Optional[int] = field(\n        default=None, metadata={\"help\": \"Number of transformer submodules\"}\n    )\n    num_attention_heads: Optional[int] = field(default=None, metadata={\"help\": \"Number of attention heads\"})\n    num_layers: Optional[int] = field(default=None, metadata={\"help\": \"Number of transformer layers\"})", ""]}
{"filename": "peft/utils/adapters_utils.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nWEIGHTS_NAME = \"adapter_model.bin\"\nCONFIG_NAME = \"adapter_config.json\"\n\n# TODO: add automapping and superclass here?", "\n# TODO: add automapping and superclass here?\n"]}
{"filename": "peft/utils/__init__.py", "chunked_list": ["# flake8: noqa\n# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n# module, but to preserve other warnings. So, don't check this module at all\n\n# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at", "# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.", "# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .adapters_utils import CONFIG_NAME, WEIGHTS_NAME\nfrom .config import PeftConfig, PeftType, PromptLearningConfig, TaskType\nfrom .other import (\n    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n    _set_trainable,\n    bloom_model_postprocess_past_key_value,\n    prepare_model_for_int8_training,", "    bloom_model_postprocess_past_key_value,\n    prepare_model_for_int8_training,\n    shift_tokens_right,\n    transpose,\n)\nfrom .save_and_load import get_peft_model_state_dict, set_peft_model_state_dict\n"]}
{"filename": "peft/tuners/prompt_tuning.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport enum\nimport math\nfrom dataclasses import dataclass, field", "import math\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Union\n\nimport torch\n\nfrom ..utils import PeftType, PromptLearningConfig\n\n\nclass PromptTuningInit(str, enum.Enum):\n    TEXT = \"TEXT\"\n    RANDOM = \"RANDOM\"", "\nclass PromptTuningInit(str, enum.Enum):\n    TEXT = \"TEXT\"\n    RANDOM = \"RANDOM\"\n\n\n@dataclass\nclass PromptTuningConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`~peft.PromptEmbedding`].\n\n    Args:\n        prompt_tuning_init (Union[[`PromptTuningInit`], `str`]): The initialization of the prompt embedding.\n        prompt_tuning_init_text ( Optional[`str`]): The text to initialize the prompt embedding.\n            Only used if `prompt_tuning_init` is `TEXT`\n        tokenizer_name_or_path ( Optional[`str`]): The name or path of the tokenizer.\n            Only used if `prompt_tuning_init` is `TEXT`\n    \"\"\"\n\n    prompt_tuning_init: Union[PromptTuningInit, str] = field(\n        default=PromptTuningInit.RANDOM,\n        metadata={\"help\": \"How to initialize the prompt tuning parameters\"},\n    )\n    prompt_tuning_init_text: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The text to use for prompt tuning initialization. Only used if prompt_tuning_init is `TEXT`\"\n        },\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"The tokenizer to use for prompt tuning initialization. Only used if prompt_tuning_init is `TEXT`\"\n        },\n    )\n\n    def __post_init__(self):\n        self.peft_type = PeftType.PROMPT_TUNING", "\n\nclass PromptEmbedding(torch.nn.Module):\n    \"\"\"\n    The model to encode virtual tokens into prompt embeddings.\n\n    Args:\n        config ([`PromptTuningConfig`]): The configuration of the prompt embedding.\n        word_embeddings (`torch.nn.Module`): The word embeddings of the base transformer model.\n\n    **Attributes**:\n        **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prompt embedding.\n\n    Example::\n\n        >>> from peft import PromptEmbedding, PromptTuningConfig >>> config = PromptTuningConfig(\n                peft_type=\"PROMPT_TUNING\", task_type=\"SEQ_2_SEQ_LM\", num_virtual_tokens=20, token_dim=768,\n                num_transformer_submodules=1, num_attention_heads=12, num_layers=12, prompt_tuning_init=\"TEXT\",\n                prompt_tuning_init_text=\"Predict if sentiment of this review is positive, negative or neutral\",\n                tokenizer_name_or_path=\"t5-base\",\n            )\n        >>> # t5_model.shared is the word embeddings of the base model >>> prompt_embedding = PromptEmbedding(config,\n        t5_model.shared)\n\n\n    Input Shape: (batch_size, total_virtual_tokens)\n\n    Output Shape: (batch_size, total_virtual_tokens, token_dim)\n    \"\"\"\n\n    def __init__(self, config, word_embeddings):\n        super().__init__()\n\n        total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\n        self.embedding = torch.nn.Embedding(total_virtual_tokens, config.token_dim)\n        if config.prompt_tuning_init == PromptTuningInit.TEXT:\n            from transformers import AutoTokenizer\n\n            tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name_or_path)\n            init_text = config.prompt_tuning_init_text\n            init_token_ids = tokenizer(init_text)[\"input_ids\"]\n            # Trim or iterate until num_text_tokens matches total_virtual_tokens\n            num_text_tokens = len(init_token_ids)\n            if num_text_tokens > total_virtual_tokens:\n                init_token_ids = init_token_ids[:total_virtual_tokens]\n            elif num_text_tokens < total_virtual_tokens:\n                num_reps = math.ceil(total_virtual_tokens / num_text_tokens)\n                init_token_ids = init_token_ids * num_reps\n            init_token_ids = init_token_ids[:total_virtual_tokens]\n\n            word_embedding_weights = word_embeddings(torch.LongTensor(init_token_ids)).detach().clone()\n            word_embedding_weights = word_embedding_weights.to(torch.float32)\n            self.embedding.weight = torch.nn.Parameter(word_embedding_weights)\n\n    def forward(self, indices):\n        # Just get embeddings\n        prompt_embeddings = self.embedding(indices)\n        return prompt_embeddings", ""]}
{"filename": "peft/tuners/p_tuning.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport enum\nimport warnings\nfrom dataclasses import dataclass, field", "import warnings\nfrom dataclasses import dataclass, field\nfrom typing import Union\n\nimport torch\n\nfrom ..utils import PeftType, PromptLearningConfig\n\n\nclass PromptEncoderReparameterizationType(str, enum.Enum):\n    MLP = \"MLP\"\n    LSTM = \"LSTM\"", "\nclass PromptEncoderReparameterizationType(str, enum.Enum):\n    MLP = \"MLP\"\n    LSTM = \"LSTM\"\n\n\n@dataclass\nclass PromptEncoderConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`~peft.PromptEncoder`].\n\n    Args:\n        encoder_reparameterization_type\n            (Union[[`PromptEncoderReparameterizationType`], `str`]): The type of reparameterization to use.\n        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n        encoder_num_layers (`int`): The number of layers of the prompt encoder.\n        encoder_dropout (`float`): The dropout probability of the prompt encoder.\n    \"\"\"\n\n    encoder_reparameterization_type: Union[str, PromptEncoderReparameterizationType] = field(\n        default=PromptEncoderReparameterizationType.MLP,\n        metadata={\"help\": \"How to reparameterize the prompt encoder\"},\n    )\n    encoder_hidden_size: int = field(\n        default=None,\n        metadata={\"help\": \"The hidden size of the prompt encoder\"},\n    )\n    encoder_num_layers: int = field(\n        default=2,\n        metadata={\"help\": \"The number of layers of the prompt encoder\"},\n    )\n    encoder_dropout: float = field(\n        default=0.0,\n        metadata={\"help\": \"The dropout of the prompt encoder\"},\n    )\n\n    def __post_init__(self):\n        self.peft_type = PeftType.P_TUNING", "\n\n# Based on https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/modules/common/prompt_encoder.py\n# with some refactor\nclass PromptEncoder(torch.nn.Module):\n    \"\"\"\n    The prompt encoder network that is used to generate the virtual token embeddings for p-tuning.\n\n    Args:\n        config ([`PromptEncoderConfig`]): The configuration of the prompt encoder.\n\n    Example::\n\n        >>> from peft import PromptEncoder, PromptEncoderConfig >>> config = PromptEncoderConfig(\n                peft_type=\"P_TUNING\", task_type=\"SEQ_2_SEQ_LM\", num_virtual_tokens=20, token_dim=768,\n                num_transformer_submodules=1, num_attention_heads=12, num_layers=12,\n                encoder_reparameterization_type=\"MLP\", encoder_hidden_size=768\n            )\n        >>> prompt_encoder = PromptEncoder(config)\n\n    **Attributes**:\n        - **embedding** ([`~torch.nn.Embedding`]) -- The embedding layer of the prompt encoder.\n        - **mlp_head** ([`~torch.nn.Sequential`]) -- The MLP head of the prompt encoder if `inference_mode=False`.\n        - **lstm_head** ([`~torch.nn.LSTM`]) -- The LSTM head of the prompt encoder if `inference_mode=False` and\n        `encoder_reparameterization_type=\"LSTM\"`.\n        - **token_dim** (`int`) -- The hidden embedding dimension of the base transformer model.\n        - **input_size** (`int`) -- The input size of the prompt encoder.\n        - **output_size** (`int`) -- The output size of the prompt encoder.\n        - **hidden_size** (`int`) -- The hidden size of the prompt encoder.\n        - **total_virtual_tokens** (`int`): The total number of virtual tokens of the\n        prompt encoder.\n        - **encoder_type** (Union[[`PromptEncoderReparameterizationType`], `str`]):\n            The encoder type of the prompt encoder.\n\n\n    Input shape: (batch_size, total_virtual_tokens)\n\n    Output shape: (batch_size, total_virtual_tokens, token_dim)\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.token_dim = config.token_dim\n        self.input_size = self.token_dim\n        self.output_size = self.token_dim\n        self.hidden_size = config.encoder_hidden_size\n        self.total_virtual_tokens = config.num_virtual_tokens * config.num_transformer_submodules\n        self.encoder_type = config.encoder_reparameterization_type\n\n        # embedding\n        self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim)\n        if not config.inference_mode:\n            if self.encoder_type == PromptEncoderReparameterizationType.LSTM:\n                lstm_dropout = config.encoder_dropout\n                num_layers = config.encoder_num_layers\n                # LSTM\n                self.lstm_head = torch.nn.LSTM(\n                    input_size=self.input_size,\n                    hidden_size=self.hidden_size,\n                    num_layers=num_layers,\n                    dropout=lstm_dropout,\n                    bidirectional=True,\n                    batch_first=True,\n                )\n\n                self.mlp_head = torch.nn.Sequential(\n                    torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(self.hidden_size * 2, self.output_size),\n                )\n\n            elif self.encoder_type == PromptEncoderReparameterizationType.MLP:\n                warnings.warn(\n                    f\"for {self.encoder_type}, the `encoder_num_layers` is ignored. Exactly 2 MLP layers are used.\"\n                )\n                layers = [\n                    torch.nn.Linear(self.input_size, self.hidden_size),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(self.hidden_size, self.hidden_size),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(self.hidden_size, self.output_size),\n                ]\n                self.mlp_head = torch.nn.Sequential(*layers)\n\n            else:\n                raise ValueError(\"Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.\")\n\n    def forward(self, indices):\n        input_embeds = self.embedding(indices)\n        if self.encoder_type == PromptEncoderReparameterizationType.LSTM:\n            output_embeds = self.mlp_head(self.lstm_head(input_embeds)[0])\n        elif self.encoder_type == PromptEncoderReparameterizationType.MLP:\n            output_embeds = self.mlp_head(input_embeds)\n        else:\n            raise ValueError(\"Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.\")\n\n        return output_embeds", ""]}
{"filename": "peft/tuners/prefix_tuning.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom dataclasses import dataclass, field\n", "from dataclasses import dataclass, field\n\nimport torch\n\nfrom ..utils import PeftType, PromptLearningConfig\n\n\n@dataclass\nclass PrefixTuningConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`~peft.PrefixEncoder`].\n\n    Args:\n        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n        prefix_projection (`bool`): Whether to project the prefix embeddings.\n    \"\"\"\n\n    encoder_hidden_size: int = field(\n        default=None,\n        metadata={\"help\": \"The hidden size of the encoder\"},\n    )\n    prefix_projection: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to project the prefix tokens\"},\n    )\n\n    def __post_init__(self):\n        self.peft_type = PeftType.PREFIX_TUNING", "class PrefixTuningConfig(PromptLearningConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`~peft.PrefixEncoder`].\n\n    Args:\n        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n        prefix_projection (`bool`): Whether to project the prefix embeddings.\n    \"\"\"\n\n    encoder_hidden_size: int = field(\n        default=None,\n        metadata={\"help\": \"The hidden size of the encoder\"},\n    )\n    prefix_projection: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether to project the prefix tokens\"},\n    )\n\n    def __post_init__(self):\n        self.peft_type = PeftType.PREFIX_TUNING", "\n\n# Based on https://github.com/THUDM/P-tuning-v2/blob/main/model/prefix_encoder.py\n# with some refactor\nclass PrefixEncoder(torch.nn.Module):\n    r\"\"\"\n    The torch.nn model to encode the prefix\n\n    Args:\n        config ([`PrefixTuningConfig`]): The configuration of the prefix encoder.\n\n    Example::\n\n        >>> from peft import PrefixEncoder, PrefixTuningConfig >>> config = PrefixTuningConfig(\n                peft_type=\"PREFIX_TUNING\", task_type=\"SEQ_2_SEQ_LM\", num_virtual_tokens=20, token_dim=768,\n                num_transformer_submodules=1, num_attention_heads=12, num_layers=12, encoder_hidden_size=768\n            )\n        >>> prefix_encoder = PrefixEncoder(config)\n\n\n    **Attributes**:\n        - **embedding** (`torch.nn.Embedding`) --\n            The embedding layer of the prefix encoder.\n        - **transform** (`torch.nn.Sequential`) -- The\n        two-layer MLP to transform the prefix embeddings if `prefix_projection` is `True`.\n        - **prefix_projection** (`bool`) -- Whether to project the prefix embeddings.\n\n    Input shape: (batch_size, num_virtual_tokens)\n\n    Output shape: (batch_size, num_virtual_tokens, 2*layers*hidden)\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.prefix_projection = config.prefix_projection\n        token_dim = config.token_dim\n        num_layers = config.num_layers\n        encoder_hidden_size = config.encoder_hidden_size\n        num_virtual_tokens = config.num_virtual_tokens\n        if self.prefix_projection and not config.inference_mode:\n            # Use a two-layer MLP to encode the prefix\n            self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)\n            self.transform = torch.nn.Sequential(\n                torch.nn.Linear(token_dim, encoder_hidden_size),\n                torch.nn.Tanh(),\n                torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),\n            )\n        else:\n            self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)\n\n    def forward(self, prefix: torch.Tensor):\n        if self.prefix_projection:\n            prefix_tokens = self.embedding(prefix)\n            past_key_values = self.transform(prefix_tokens)\n        else:\n            past_key_values = self.embedding(prefix)\n        return past_key_values", ""]}
{"filename": "peft/tuners/__init__.py", "chunked_list": ["# flake8: noqa\n# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n# module, but to preserve other warnings. So, don't check this module at all\n\n# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at", "# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.", "# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .lora import LoraConfig, LoraModel\nfrom .p_tuning import PromptEncoder, PromptEncoderConfig, PromptEncoderReparameterizationType\nfrom .prefix_tuning import PrefixEncoder, PrefixTuningConfig\nfrom .prompt_tuning import PromptEmbedding, PromptTuningConfig, PromptTuningInit\n"]}
{"filename": "peft/tuners/lora.py", "chunked_list": ["# coding=utf-8\n# Copyright 2023-present the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software", "#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport importlib\nimport math\nimport re\nimport warnings", "import re\nimport warnings\nfrom dataclasses import asdict, dataclass, field\nfrom enum import Enum\nfrom typing import List, Optional, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.pytorch_utils import Conv1D", "import torch.nn.functional as F\nfrom transformers.pytorch_utils import Conv1D\n\nfrom ..utils import PeftConfig, PeftType, transpose\n\n\ndef is_bnb_available():\n    return importlib.util.find_spec(\"bitsandbytes\") is not None\n\n\ndef is_gptq_available():\n    return importlib.util.find_spec(\"quant\") is not None", "\n\ndef is_gptq_available():\n    return importlib.util.find_spec(\"quant\") is not None\n\n\nif is_bnb_available():\n    import bitsandbytes as bnb\n\n\nif is_gptq_available():\n    import quant", "\n\nif is_gptq_available():\n    import quant\n\n\n@dataclass\nclass LoraConfig(PeftConfig):\n    \"\"\"\n    This is the configuration class to store the configuration of a [`~peft.Lora`].\n\n    Args:\n        r (`int`): Lora attention dimension\n        target_modules (`Union[List[str],str]`): The names of the modules to apply Lora to.\n        lora_alpha (`float`): The alpha parameter for Lora scaling.\n        lora_dropout (`float`): The dropout probability for Lora layers.\n        merge_weights (`bool`):\n            Whether to merge the weights of the Lora layers with the base transformer model in `eval` mode.\n        fan_in_fan_out (`bool`): Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n        enable_lora ( `List[bool]`): Used with `lora.MergedLinear`.\n        bias (`str`): Bias type for Lora. Can be 'none', 'all' or 'lora_only'\n        modules_to_save (`List[str]`):List of modules apart from LoRA layers to be set as trainable\n            and saved in the final checkpoint.\n    \"\"\"\n\n    r: int = field(default=8, metadata={\"help\": \"Lora attention dimension\"})\n    target_modules: Optional[Union[List[str], str]] = field(\n        default=None,\n        metadata={\n            \"help\": \"List of module names or regex expression of the module names to replace with Lora.\"\n            \"For example, ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$' \"\n        },\n    )\n    lora_alpha: int = field(default=None, metadata={\"help\": \"Lora alpha\"})\n    lora_dropout: float = field(default=None, metadata={\"help\": \"Lora dropout\"})\n    merge_weights: bool = field(\n        default=False, metadata={\"help\": \"Merge weights of the original model and the Lora model\"}\n    )\n    fan_in_fan_out: bool = field(\n        default=False,\n        metadata={\"help\": \"Set this to True if the layer to replace stores weight like (fan_in, fan_out)\"},\n    )\n    enable_lora: Optional[List[bool]] = field(default=None, metadata={\"help\": \"Used with `lora.MergedLinear`.\"})\n    bias: str = field(default=\"none\", metadata={\"help\": \"Bias type for Lora. Can be 'none', 'all' or 'lora_only'\"})\n    modules_to_save: Optional[List[str]] = field(\n        default=None,\n        metadata={\n            \"help\": \"List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. \"\n            \"For example, in Sequence Classification or Token Classification tasks, \"\n            \"the final layer `classifier/score` are randomly initialized and as such need to be trainable and saved.\"\n        },\n    )\n\n    def __post_init__(self):\n        self.peft_type = PeftType.LORA", "\n\nclass LoraModel(torch.nn.Module):\n    \"\"\"\n    Creates Low Rank Adapter (Lora) model from a pretrained transformers model.\n\n    Args:\n        model ([`transformers.PreTrainedModel`]): The model to be adapted.\n        config ([`LoraConfig`]): The configuration of the Lora model.\n\n    Returns:\n        `torch.nn.Module`: The Lora model.\n\n    Example::\n\n        >>> from transformers import AutoModelForSeq2SeqLM, LoraConfig >>> from peft import LoraModel, LoraConfig >>>\n        config = LoraConfig(\n            peft_type=\"LORA\", task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=32, target_modules=[\"q\", \"v\"],\n            lora_dropout=0.01, )\n        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\") >>> lora_model = LoraModel(config, model)\n\n    **Attributes**:\n        - **model** ([`transformers.PreTrainedModel`]) -- The model to be adapted.\n        - **peft_config** ([`LoraConfig`]): The configuration of the Lora model.\n    \"\"\"\n\n    def __init__(self, config, model):\n        super().__init__()\n        self.peft_config = config\n        self.model = model\n        self._find_and_replace()\n        mark_only_lora_as_trainable(self.model, self.peft_config.bias)\n        self.forward = self.model.forward\n\n    def _find_and_replace(self):\n        loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)\n        if loaded_in_8bit and not is_bnb_available():\n            raise ImportError(\n                \"To use Lora with 8-bit quantization, please install the `bitsandbytes` package. \"\n                \"You can install it with `pip install bitsandbytes`.\"\n            )\n        is_target_modules_in_base_model = False\n        is_hf_device_map_available = hasattr(self.model, \"hf_device_map\")\n        kwargs = {\n            \"r\": self.peft_config.r,\n            \"lora_alpha\": self.peft_config.lora_alpha,\n            \"lora_dropout\": self.peft_config.lora_dropout,\n            \"fan_in_fan_out\": self.peft_config.fan_in_fan_out,\n            \"merge_weights\": (self.peft_config.merge_weights or self.peft_config.inference_mode)\n            and not is_hf_device_map_available,\n        }\n        key_list = [key for key, _ in self.model.named_modules()]\n        for key in key_list:\n            if isinstance(self.peft_config.target_modules, str):\n                target_module_found = re.fullmatch(self.peft_config.target_modules, key)\n            else:\n                target_module_found = any(key.endswith(target_key) for target_key in self.peft_config.target_modules)\n            if target_module_found:\n                if not is_target_modules_in_base_model:\n                    is_target_modules_in_base_model = True\n                parent, target, target_name = self._get_submodules(key)\n                bias = target.bias is not None\n                if loaded_in_8bit and isinstance(target, bnb.nn.Linear8bitLt):\n                    kwargs.update(\n                        {\n                            \"has_fp16_weights\": target.state.has_fp16_weights,\n                            \"memory_efficient_backward\": target.state.memory_efficient_backward,\n                            \"threshold\": target.state.threshold,\n                            \"index\": target.index,\n                        }\n                    )\n                    if self.peft_config.enable_lora is None:\n                        new_module = Linear8bitLt(target.in_features, target.out_features, bias=bias, **kwargs)\n                    else:\n                        kwargs.update({\"enable_lora\": self.peft_config.enable_lora})\n                        new_module = MergedLinear8bitLt(target.in_features, target.out_features, bias=bias, **kwargs)\n                elif isinstance(target, torch.nn.Linear) and self.peft_config.enable_lora is None:\n                    new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs)\n                elif isinstance(target, Autograd4bitQuantLinear) and self.peft_config.enable_lora is None:\n                    new_module = Linear4bitLt(target.in_features, target.out_features, bias=bias, **kwargs)\n                elif self.peft_config.enable_lora is not None:\n                    kwargs.update({\"enable_lora\": self.peft_config.enable_lora})\n                    if isinstance(target, Conv1D):\n                        in_features, out_features = (\n                            target.weight.ds_shape if hasattr(target.weight, \"ds_shape\") else target.weight.shape\n                        )\n                    else:\n                        in_features, out_features = target.in_features, target.out_features\n                        if kwargs[\"fan_in_fan_out\"]:\n                            warnings.warn(\n                                \"fan_in_fan_out is set to True but the target module is not a Conv1D. \"\n                                \"Setting fan_in_fan_out to False.\"\n                            )\n                            kwargs[\"fan_in_fan_out\"] = self.peft_config.fan_in_fan_out = False\n                    new_module = MergedLinear(in_features, out_features, bias=bias, **kwargs)\n                self._replace_module(parent, target_name, new_module, target)\n        if not is_target_modules_in_base_model:\n            raise ValueError(\n                f\"Target modules {self.peft_config.target_modules} not found in the base model. \"\n                f\"Please check the target modules and try again.\"\n            )\n\n    def _get_submodules(self, key):\n        parent = self.model.get_submodule(\".\".join(key.split(\".\")[:-1]))\n        target_name = key.split(\".\")[-1]\n        target = self.model.get_submodule(key)\n        return parent, target, target_name\n\n    def _replace_module(self, parent_module, child_name, new_module, old_module):\n        setattr(parent_module, child_name, new_module)\n        if isinstance(old_module, Autograd4bitQuantLinear) and isinstance(new_module, Linear4bitLt):\n            new_module.qweight = old_module.qweight\n            new_module.scales = old_module.scales\n            new_module.zeros = old_module.zeros\n            new_module.bias = old_module.bias\n            if getattr(old_module, \"state\", None) is not None:\n                new_module.state = old_module.state\n                new_module.to(old_module.qweight.device)\n\n            # dispatch to correct device\n            for name, module in new_module.named_modules():\n                if \"lora_\" in name:\n                    module.to(old_module.qweight.device)\n        else:\n            new_module.weight = old_module.weight\n            if old_module.bias is not None:\n                new_module.bias = old_module.bias\n            if getattr(old_module, \"state\", None) is not None:\n                new_module.state = old_module.state\n                new_module.to(old_module.weight.device)\n\n            # dispatch to correct device\n            for name, module in new_module.named_modules():\n                if \"lora_\" in name:\n                    module.to(old_module.weight.device)\n\n    def __getattr__(self, name: str):\n        \"\"\"Forward missing attributes to the wrapped module.\"\"\"\n        try:\n            return super().__getattr__(name)  # defer to nn.Module's logic\n        except AttributeError:\n            return getattr(self.model, name)\n\n    @property\n    def modules_to_save(self):\n        return None\n\n    def get_peft_config_as_dict(self, inference: bool = False):\n        config = {k: v.value if isinstance(v, Enum) else v for k, v in asdict(self.peft_config).items()}\n        if inference:\n            config[\"inference_mode\"] = True\n        return config\n\n    def _set_adapter_layers(self, enabled=True):\n        for module in self.model.modules():\n            if isinstance(module, LoraLayer):\n                module.disable_adapters = False if enabled else True\n\n    def enable_adapter_layers(self):\n        self._set_adapter_layers(enabled=True)\n\n    def disable_adapter_layers(self):\n        self._set_adapter_layers(enabled=False)", "\n\n# Below code is based on https://github.com/microsoft/LoRA/blob/main/loralib/layers.py\n# and modified to work with PyTorch FSDP\n\n\n#  ------------------------------------------------------------------------------------------\n#  Copyright (c) Microsoft Corporation. All rights reserved.\n#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n#  ------------------------------------------------------------------------------------------", "#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n#  ------------------------------------------------------------------------------------------\n\n\n# had to adapt it for `lora_only` to work\ndef mark_only_lora_as_trainable(model: nn.Module, bias: str = \"none\") -> None:\n    for n, p in model.named_parameters():\n        if \"lora_\" not in n:\n            p.requires_grad = False\n    if bias == \"none\":\n        return\n    elif bias == \"all\":\n        for n, p in model.named_parameters():\n            if \"bias\" in n:\n                p.requires_grad = True\n    elif bias == \"lora_only\":\n        for m in model.modules():\n            if isinstance(m, LoraLayer) and hasattr(m, \"bias\") and m.bias is not None:\n                m.bias.requires_grad = True\n    else:\n        raise NotImplementedError", "\n\nclass LoraLayer:\n    def __init__(\n        self,\n        r: int,\n        lora_alpha: int,\n        lora_dropout: float,\n        merge_weights: bool,\n    ):\n        self.r = r\n        self.lora_alpha = lora_alpha\n        # Optional dropout\n        if lora_dropout > 0.0:\n            self.lora_dropout = nn.Dropout(p=lora_dropout)\n        else:\n            self.lora_dropout = lambda x: x\n        # Mark the weight as unmerged\n        self.merged = False\n        self.merge_weights = merge_weights\n        self.disable_adapters = False", "\n\nclass Linear(nn.Linear, LoraLayer):\n    # Lora implemented in a dense layer\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,  # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n        merge_weights: bool = True,\n        **kwargs,\n    ):\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        LoraLayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)\n\n        self.fan_in_fan_out = fan_in_fan_out\n        # Actual trainable parameters\n        if r > 0:\n            self.lora_A = nn.Linear(in_features, r, bias=False)\n            self.lora_B = nn.Linear(r, out_features, bias=False)\n            self.scaling = self.lora_alpha / self.r\n            # Freezing the pre-trained weight matrix\n            self.weight.requires_grad = False\n        self.reset_parameters()\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.T\n\n    def reset_parameters(self):\n        nn.Linear.reset_parameters(self)\n        if hasattr(self, \"lora_A\"):\n            # initialize A the same way as the default for nn.Linear and B to zero\n            nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B.weight)\n\n    def train(self, mode: bool = True):\n        nn.Linear.train(self, mode)\n        self.lora_A.train(mode)\n        self.lora_B.train(mode)\n        if not mode and self.merge_weights and not self.merged:\n            # Merge the weights and mark it\n            if self.r > 0:\n                self.weight.data += (\n                    transpose(self.lora_B.weight @ self.lora_A.weight, self.fan_in_fan_out) * self.scaling\n                )\n            self.merged = True\n        elif self.merge_weights and self.merged:\n            # Make sure that the weights are not merged\n            if self.r > 0:\n                self.weight.data -= (\n                    transpose(self.lora_B.weight @ self.lora_A.weight, self.fan_in_fan_out) * self.scaling\n                )\n            self.merged = False\n\n    def eval(self):\n        nn.Linear.eval(self)\n        self.lora_A.eval()\n        self.lora_B.eval()\n\n    def forward(self, x: torch.Tensor):\n        if self.disable_adapters:\n            if self.r > 0 and self.merged:\n                self.weight.data -= (\n                    transpose(self.lora_B.weight @ self.lora_A.weight, self.fan_in_fan_out) * self.scaling\n                )\n                self.merged = False\n\n            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        elif self.r > 0 and not self.merged:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n            if self.r > 0:\n                result += self.lora_B(self.lora_A(self.lora_dropout(x))) * self.scaling\n            return result\n        else:\n            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)", "\n\nclass MergedLinear(nn.Linear, LoraLayer):\n    # Lora implemented in a dense layer\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 1,\n        lora_dropout: float = 0.0,\n        enable_lora: List[bool] = [False],\n        fan_in_fan_out: bool = False,\n        merge_weights: bool = True,\n        **kwargs,\n    ):\n        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n        LoraLayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)\n        if out_features % len(enable_lora) != 0:\n            raise ValueError(\"The length of enable_lora must divide out_features\")\n        self.enable_lora = enable_lora\n        self.fan_in_fan_out = fan_in_fan_out\n        # Actual trainable parameters\n        if r > 0 and any(enable_lora):\n            self.lora_A = nn.Linear(in_features, r * sum(enable_lora), bias=False)\n            self.lora_B = nn.Conv1d(\n                r * sum(enable_lora),\n                out_features // len(enable_lora) * sum(enable_lora),\n                kernel_size=1,\n                groups=2,\n                bias=False,\n            )\n            self.scaling = self.lora_alpha / self.r\n            # Freezing the pre-trained weight matrix\n            self.weight.requires_grad = False\n            # Compute the indices\n            self.lora_ind = self.weight.new_zeros((out_features,), dtype=torch.bool).view(len(enable_lora), -1)\n            self.lora_ind[enable_lora, :] = True\n            self.lora_ind = self.lora_ind.view(-1)\n        self.reset_parameters()\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.T\n\n    def reset_parameters(self):\n        nn.Linear.reset_parameters(self)\n        if hasattr(self, \"lora_A\"):\n            # initialize A the same way as the default for nn.Linear and B to zero\n            nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B.weight)\n\n    def zero_pad(self, x):\n        result = x.new_zeros((*x.shape[:-1], self.out_features))\n        result = result.view(-1, self.out_features)\n        result[:, self.lora_ind] = x.reshape(-1, self.out_features // len(self.enable_lora) * sum(self.enable_lora))\n        return result.view((*x.shape[:-1], self.out_features))\n\n    def train(self, mode: bool = True):\n        nn.Linear.train(self, mode)\n        self.lora_A.train(mode)\n        self.lora_B.train(mode)\n        if not mode and self.merge_weights and not self.merged:\n            # Merge the weights and mark it\n            if self.r > 0 and any(self.enable_lora):\n                delta_w = (\n                    F.conv1d(\n                        self.lora_A.weight.data.unsqueeze(0),\n                        self.lora_B.weight.data,\n                        groups=sum(self.enable_lora),\n                    )\n                    .squeeze(0)\n                    .transpose(-2, -1)\n                )\n                self.weight.data += transpose(self.zero_pad(delta_w * self.scaling), not self.fan_in_fan_out)\n            self.merged = True\n        elif self.merge_weights and self.merged:\n            # Make sure that the weights are not merged\n            if self.r > 0 and any(self.enable_lora):\n                delta_w = (\n                    F.conv1d(\n                        self.lora_A.weight.data.unsqueeze(0),\n                        self.lora_B.weight.data,\n                        groups=sum(self.enable_lora),\n                    )\n                    .squeeze(0)\n                    .transpose(-2, -1)\n                )\n                self.weight.data -= transpose(self.zero_pad(delta_w * self.scaling), not self.fan_in_fan_out)\n            self.merged = False\n\n    def eval(self):\n        nn.Linear.eval(self)\n        self.lora_A.eval()\n        self.lora_B.eval()\n\n    def forward(self, x: torch.Tensor):\n        if self.disable_adapters:\n            if self.r > 0 and self.merged and any(self.enable_lora):\n                delta_w = (\n                    F.conv1d(\n                        self.lora_A.weight.data.unsqueeze(0),\n                        self.lora_B.weight.data,\n                        groups=sum(self.enable_lora),\n                    )\n                    .squeeze(0)\n                    .transpose(-2, -1)\n                )\n                self.weight.data -= transpose(self.zero_pad(delta_w * self.scaling), not self.fan_in_fan_out)\n                self.merged = False\n            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        elif self.merged:\n            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n        else:\n            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)\n            if self.r > 0:\n                after_A = self.lora_A(self.lora_dropout(x))\n                after_B = self.lora_B(after_A.transpose(-2, -1)).transpose(-2, -1)\n                result += self.zero_pad(after_B) * self.scaling\n            return result", "\n\nif is_bnb_available():\n\n    class Linear8bitLt(bnb.nn.Linear8bitLt, LoraLayer):\n        # Lora implemented in a dense layer\n        def __init__(\n            self,\n            in_features,\n            out_features,\n            r: int = 0,\n            lora_alpha: int = 1,\n            lora_dropout: float = 0.0,\n            **kwargs,\n        ):\n            bnb.nn.Linear8bitLt.__init__(\n                self,\n                in_features,\n                out_features,\n                bias=kwargs.get(\"bias\", True),\n                has_fp16_weights=kwargs.get(\"has_fp16_weights\", True),\n                memory_efficient_backward=kwargs.get(\"memory_efficient_backward\", False),\n                threshold=kwargs.get(\"threshold\", 0.0),\n                index=kwargs.get(\"index\", None),\n            )\n            LoraLayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=False)\n            # Actual trainable parameters\n            if r > 0:\n                self.lora_A = nn.Linear(in_features, r, bias=False)\n                self.lora_B = nn.Linear(r, out_features, bias=False)\n                self.scaling = self.lora_alpha / self.r\n                # Freezing the pre-trained weight matrix\n                self.weight.requires_grad = False\n            self.reset_parameters()\n\n        def reset_parameters(self):\n            if hasattr(self, \"lora_A\"):\n                # initialize A the same way as the default for nn.Linear and B to zero\n                nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n                nn.init.zeros_(self.lora_B.weight)\n\n        def forward(self, x: torch.Tensor):\n            result = super().forward(x)\n\n            if self.disable_adapters:\n                return result\n            elif self.r > 0:\n                if not torch.is_autocast_enabled():\n                    expected_dtype = result.dtype\n\n                    if x.dtype != torch.float32:\n                        x = x.float()\n                    output = self.lora_B(self.lora_A(self.lora_dropout(x))).to(expected_dtype) * self.scaling\n                    result += output\n                else:\n                    output = self.lora_B(self.lora_A(self.lora_dropout(x))) * self.scaling\n                    result += output\n            return result\n\n    class MergedLinear8bitLt(bnb.nn.Linear8bitLt, LoraLayer):\n        # Lora implemented in a dense layer\n        def __init__(\n            self,\n            in_features: int,\n            out_features: int,\n            r: int = 0,\n            lora_alpha: int = 1,\n            lora_dropout: float = 0.0,\n            enable_lora: List[bool] = [False],\n            **kwargs,\n        ):\n            bnb.nn.Linear8bitLt.__init__(\n                self,\n                in_features,\n                out_features,\n                bias=kwargs.get(\"bias\", True),\n                has_fp16_weights=kwargs.get(\"has_fp16_weights\", True),\n                memory_efficient_backward=kwargs.get(\"memory_efficient_backward\", False),\n                threshold=kwargs.get(\"threshold\", 0.0),\n                index=kwargs.get(\"index\", None),\n            )\n            LoraLayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=False)\n            if out_features % len(enable_lora) != 0:\n                raise ValueError(\"The length of enable_lora must divide out_features\")\n            self.enable_lora = enable_lora\n            # Actual trainable parameters\n            if r > 0 and any(enable_lora):\n                self.lora_A = nn.Linear(in_features, r * sum(enable_lora), bias=False)\n                self.lora_B = nn.Conv1d(\n                    r * sum(enable_lora),\n                    out_features // len(enable_lora) * sum(enable_lora),\n                    kernel_size=1,\n                    groups=2,\n                    bias=False,\n                )\n                self.scaling = self.lora_alpha / self.r\n                # Freezing the pre-trained weight matrix\n                self.weight.requires_grad = False\n                # Compute the indices\n                self.lora_ind = self.weight.new_zeros((out_features,), dtype=torch.bool).view(len(enable_lora), -1)\n                self.lora_ind[enable_lora, :] = True\n                self.lora_ind = self.lora_ind.view(-1)\n            self.reset_parameters()\n\n        def reset_parameters(self):\n            if hasattr(self, \"lora_A\"):\n                # initialize A the same way as the default for nn.Linear and B to zero\n                nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n                nn.init.zeros_(self.lora_B.weight)\n\n        def zero_pad(self, x):\n            result = x.new_zeros((*x.shape[:-1], self.out_features))\n            result = result.view(-1, self.out_features)\n            result[:, self.lora_ind] = x.reshape(\n                -1, self.out_features // len(self.enable_lora) * sum(self.enable_lora)\n            )\n            return result.view((*x.shape[:-1], self.out_features))\n\n        def forward(self, x: torch.Tensor):\n            result = super().forward(x)\n            if self.disable_adapters:\n                return result\n            elif self.r > 0:\n                if not torch.is_autocast_enabled():\n                    expected_dtype = result.dtype\n                    if x.dtype != torch.float32:\n                        x = x.float()\n                    after_A = self.lora_A(self.lora_dropout(x))\n                    after_B = self.lora_B(after_A.transpose(-2, -1)).transpose(-2, -1)\n                    output = self.zero_pad(after_B).to(expected_dtype) * self.scaling\n                    result += output\n                else:\n                    after_A = self.lora_A(self.lora_dropout(x))\n                    after_B = self.lora_B(after_A.transpose(-2, -1)).transpose(-2, -1)\n                    output = self.zero_pad(after_B) * self.scaling\n                    result += output\n            return result", "\nif is_gptq_available():\n\n    from autograd_4bit import Autograd4bitQuantLinear\n\n    class Linear4bitLt(Autograd4bitQuantLinear, LoraLayer):\n        # Lora implemented in a dense layer\n        def __init__(\n                self,\n                in_features,\n                out_features,\n                r: int = 0,\n                lora_alpha: int = 1,\n                lora_dropout: float = 0.0,\n                **kwargs,\n        ):\n            Autograd4bitQuantLinear.__init__(\n                self,\n                in_features,\n                out_features\n            )\n            LoraLayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=False)\n            # Actual trainable parameters\n            if r > 0:\n                self.lora_A = nn.Linear(in_features, r, bias=False)\n                self.lora_B = nn.Linear(r, out_features, bias=False)\n                self.scaling = self.lora_alpha / self.r\n                # Freezing the pre-trained weight matrix\n                self.qweight.requires_grad = False\n                self.scales.requires_grad = False\n                self.zeros.requires_grad = False\n                self.bias.requires_grad = False\n            self.reset_parameters()\n\n        def reset_parameters(self):\n            if hasattr(self, \"lora_A\"):\n                # initialize A the same way as the default for nn.Linear and B to zero\n                nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n                nn.init.zeros_(self.lora_B.weight)\n\n        def forward(self, x: torch.Tensor):\n            result = super().forward(x)\n\n            if self.disable_adapters:\n                return result\n            elif self.r > 0:\n                if not torch.is_autocast_enabled():\n                    expected_dtype = result.dtype\n\n                    if x.dtype != torch.float32:\n                        x = x.float()\n                    output = self.lora_B(self.lora_A(self.lora_dropout(x))).to(expected_dtype) * self.scaling\n                    result += output\n                else:\n                    output = self.lora_B(self.lora_A(self.lora_dropout(x))) * self.scaling\n                    result += output\n            return result", ""]}
