{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nsetup(\n  name = 'blockwise-parallel-transformer',\n  packages = find_packages(exclude=[]),\n  version = '0.1.2',\n  license='MIT',\n  description = '32x Faster Attentionn',\n  author = 'Kye Gomez',\n  author_email = 'kye@apac.ai',", "  author = 'Kye Gomez',\n  author_email = 'kye@apac.ai',\n  long_description_content_type = 'text/markdown',\n  url = 'https://github.com/kyegomez/Blockwise-Parallel-Transformer',\n  keywords = [\n    'artificial intelligence',\n    'deep learning',\n    'optimizers',\n    \"Prompt Engineering\"\n  ],", "    \"Prompt Engineering\"\n  ],\n  install_requires=[\n    'jax',\n    'torch'\n  ],\n  classifiers=[\n    'Development Status :: 4 - Beta',\n    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',", "    'Intended Audience :: Developers',\n    'Topic :: Scientific/Engineering :: Artificial Intelligence',\n    'License :: OSI Approved :: MIT License',\n    'Programming Language :: Python :: 3.6',\n  ],\n)"]}
{"filename": "example.py", "chunked_list": ["from jax import random\nfrom blockwise_parallel import BlockwiseParallelTransformerAttention\nfrom torch.nn import Embedding\n\n\n#hyperparams\ninput_size = 512\nnum_heads = 8\nhidden_size = 512\nnum_layers = 6", "hidden_size = 512\nnum_layers = 6\nmax_seq_len = 1024\nblock_size = 64\n\n#create random input sequence\nkey = random.PRNGKey(0)\nx = random.normal(key, (1, max_seq_len, input_size))\n\n#create instance", "\n#create instance\nattention = BlockwiseParallelTransformerAttention(input_size,\n                                                  num_heads,\n                                                  hidden_size,\n                                                  num_layers,\n                                                  max_seq_len,\n                                                  block_size)\n\n##compute the output of the attention", "\n##compute the output of the attention\noutput = attention(x)\n\n#print the shape of the output\nprint(output.shape)"]}
{"filename": "blockwise_parallel/test1.py", "chunked_list": ["import torch\nimport torch.nn as nn\n\nclass BlockwiseParallelTransformer(nn.Module):\n    def __init__(self, input_dim, output_dim, head_dim, num_heads, num_query_blocks, num_kv_blocks):\n        super(BlockwiseParallelTransformer, self).__init__()\n        self.query_blocks = num_query_blocks\n        self.kv_blocks = num_kv_blocks\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.head_dim = head_dim\n        self.num_heads = num_heads\n\n        self.query_layer = nn.Linear(input_dim, num_heads * head_dim)\n        self.key_layer = nn.Linear(input_dim, num_heads * head_dim)\n        self.value_layer = nn.Linear(input_dim, num_heads * head_dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(input_dim, output_dim),\n            nn.ReLU(),\n            nn.Linear(output_dim, input_dim),\n        )\n\n    def forward(self, x):\n        b, n, _ = x.shape\n        q_chunk_size = n // self.query_blocks\n        kv_chunk_size = n // self.kv_blocks\n\n        outputs = torch.zeros_like(x)\n        for q_idx in range(self.query_blocks):\n            q_chunk_start = q_idx * q_chunk_size\n            q_chunk_end = (q_idx + 1) * q_chunk_size\n\n            q_ = self.query_layer(x[:, q_chunk_start:q_chunk_end])\n            q = q_.view(b, q_chunk_size, self.num_heads, self.head_dim)\n            q = q / torch.sqrt(torch.tensor(self.head_dim).float())\n\n            attn_numerator = torch.zeros_like(q)\n            attn_denominator = torch.zeros_like(q)\n\n            for kv_idx in range(self.kv_blocks):\n                kv_chunk_start = kv_idx * kv_chunk_size\n                kv_chunk_end = (kv_idx + 1) * kv_chunk_size\n\n                k_ = self.key_layer(x[:, kv_chunk_start:kv_chunk_end])\n                v_ = self.value_layer(x[:, kv_chunk_start:kv_chunk_end])\n\n                k = k_.view(b, kv_chunk_size, self.num_heads, self.head_dim)\n                v = v_.view(b, kv_chunk_size, self.num_heads, self.head_dim)\n\n                attn_weight = torch.einsum('bhqd,bkhd->bhqk', q, k)\n\n                max_score, _ = torch.max(attn_weight, dim=-1, keepdim=True)\n                exp_weight = torch.exp(attn_weight - max_score)\n                attn_numerator += torch.einsum('bhqv,bvhf->bhqf', exp_weight, v)\n                attn_denominator += exp_weight.sum(dim=-1, keepdim=True)\n\n            attn_out = (attn_numerator / attn_denominator)\n            attn_out = attn_out.contiguous().view(-1, self.num_heads * self.head_dim)\n            ffn_out = self.ffn(attn_out + x[:, q_chunk_start:q_chunk_end])\n            outputs[:, q_chunk_start:q_chunk_end] = ffn_out + attn_out + x[:, q_chunk_start:q_chunk_end]\n\n        return outputs", "\n    \n#inpout sequence\nbatch_size = 2\nseq_len = 1024\ninput_size = 512\nx = torch.randn(batch_size, seq_len, input_size)\n\n\n#define params", "\n#define params\nnum_heads = 8\nhidden_size = 512\nnum_layers = 6\nmax_seq_len = 1024\nblock_size = 64\n\n#crete an instance of blockwise paralel\nmodel = BlockwiseParallelTransformer(input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size)", "#crete an instance of blockwise paralel\nmodel = BlockwiseParallelTransformer(input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size)\n\n\n#pass the input sequence to the module to get the output\noutput = model(x)\n\nprint(output.shape)"]}
{"filename": "blockwise_parallel/blockwise_parallel_jax.py", "chunked_list": ["# import jax\n# import jax.numpy as jnp\n# from jax import nn, lax\n# from jax.experimental.stax import Dense\n\n# class BlockwiseParallelTransformerAttention:\n#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n#         self.input_size = input_size\n#         self.num_heads = num_heads\n#         self.hidden_size = hidden_size", "#         self.num_heads = num_heads\n#         self.hidden_size = hidden_size\n#         self.num_layers = num_layers\n#         self.max_seq_len = max_seq_len\n#         self.block_size = block_size\n#         self.dim_per_head = hidden_size // num_heads\n\n#         self.query_chunk_size = max_seq_len // block_size\n#         self.key_value_chunk_size = max_seq_len // block_size\n#         self.num_query_chunks = (max_seq_len + self.query_chunk_size - 1) // self.query_chunk_size", "#         self.key_value_chunk_size = max_seq_len // block_size\n#         self.num_query_chunks = (max_seq_len + self.query_chunk_size - 1) // self.query_chunk_size\n#         self.num_key_value_chunks = (max_seq_len + self.key_value_chunk_size - 1) // self.key_value_chunk_size\n\n#         self.query_position_ids = jnp.arange(max_seq_len)\n#         self.key_value_position_ids = jnp.arange(max_seq_len)\n\n#         self.query_blocks = Dense(hidden_size, name='query')\n#         self.key_blocks = Dense(hidden_size, name='key')\n#         self.value_blocks = Dense(hidden_size, name='value')", "#         self.key_blocks = Dense(hidden_size, name='key')\n#         self.value_blocks = Dense(hidden_size, name='value')\n#         self.feedforward = Dense(hidden_size, name='feedforward')\n\n#     def _chunk_bias_fn(self, query_chunk_idx, key_chunk_idx):\n#         start = key_chunk_idx * self.key_value_chunk_size\n#         end = (key_chunk_idx + 1) * self.key_value_chunk_size\n#         bias_chunk = jnp.zeros((self.num_heads, self.query_chunk_size, self.key_value_chunk_size))\n#         bias_chunk = lax.dynamic_update_slice(bias_chunk, jnp.ones((self.num_heads, self.query_chunk_size, end - start)), (slice(None), slice(None), slice(start, end)))\n#         bias_chunk = jnp.expand_dims(bias_chunk, axis=0)", "#         bias_chunk = lax.dynamic_update_slice(bias_chunk, jnp.ones((self.num_heads, self.query_chunk_size, end - start)), (slice(None), slice(None), slice(start, end)))\n#         bias_chunk = jnp.expand_dims(bias_chunk, axis=0)\n#         bias_chunk = jnp.tile(bias_chunk, (query_chunk_idx.shape[0], 1, 1, 1))\n#         return bias_chunk\n\n#     def _query_block(self, input_chunk, query_chunk_idx):\n#         query_chunk = self.query_blocks(input_chunk)\n#         query_chunk = query_chunk / jnp.sqrt(query_chunk.shape[-1])\n#         return query_chunk\n", "#         return query_chunk\n\n#     def _key_value_blocks(self, carry, args):\n#         kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n#         query_chunk, query_chunk_idx = carry\n#         key_chunk = self.key_blocks(kv_chunk)\n#         value_chunk = self.value_blocks(kv_chunk)\n#         attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk)\n#         bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n#         bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)", "#         bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n#         bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n#         attn_weights = attn_weights + bias_chunk\n#         max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n#         exp_weights = jnp.exp(attn_weights - max_score)\n#         exp_values = jnp.einsum('bqhv,bvhf->bqhf', exp_weights, value_chunk)\n#         numerator = jax.lax.dynamic_update_slice(query_chunk, exp_values, (slice(None), key_chunk_idx, slice(None), slice(None)))\n#         denominator = jax.lax.dynamic_update_slice(query_chunk, exp_weights.sum(axis=-1, keepdims=True), (slice(None), key_chunk_idx, slice(None), slice(None)))\n#         return (numerator, denominator), None\n    ", "#         return (numerator, denominator), None\n    \n#     def __call__(self, x, deterministic=True):\n#         batch_size, seq_len, input_size = x.shape\n#         assert input_size == self.input_size, f\"Input size must be {self.input_size} but got {input_size}\"\n\n#         query_chunks = x.reshape(batch_size, self.num_query_chunks, self.query_chunk_size, input_size)\n#         query_chunks = self.query_blocks(query_chunks)\n\n#         query_chunks = query_chunks / jnp.sqrt(query_chunks.shape[-1])", "\n#         query_chunks = query_chunks / jnp.sqrt(query_chunks.shape[-1])\n#         query_position_ids = jnp.tile(self.query_position_ids, (batch_size, 1))\n#         query_position_ids = query_position_ids.reshape(batch_size, self.num_query_chunks, self.query_chunk_size)\n#         query_position_ids = jax.lax.dynamic_slide(query_position_ids, (0, 0, 0), (batch_size, self.num_query_chunks, self.query_chunk_size - 1))\n#         query_position_ids = jnp.concatenate([query_position_ids, jnp.ones((batch_size, self.num_query_chunks, 1)) * (self.max_seq_len - 1)], axis=-1)\n#         query_position_ids = query_position_ids.astype(jnp.int32)\n\n#         key_value_chunks = x.reshape(batch_size, self.num_key_value_chinks, self.key_value_chunk_size, input_size)\n#         key_value_chunks = jax.lax.stop_gradient(key_value_chunks) if deterministic else key_value_chunks", "#         key_value_chunks = x.reshape(batch_size, self.num_key_value_chinks, self.key_value_chunk_size, input_size)\n#         key_value_chunks = jax.lax.stop_gradient(key_value_chunks) if deterministic else key_value_chunks\n#         key_value_position_ids = jnp.tile(self.key_value_position_ids, (batch_size, 1))\n#         key_value_position_ids = key_value_position_ids.reshape(batch_size, self.num_value_chunks, self.key_value_chunk_size)\n#         key_value_position_ids = jax.lax.dynamic_slice(key_value_position_ids, (0, 0, 0), (batch_size, self.num_key_value_chunks, self.key_value_chunk_size - 1))\n#         key_value_position_ids = jnp.concatenate([key_value_position_ids, jnp.ones((batch_size, self.num_key_value_chunks, 1)) * (self.max_seq_len - 1)], axis=-1)\n#         key_value_position_ids = key_value_position_ids.astype(jnp.int32)\n\n#         query_blocks = jax.lax.map(self._query_block, query_chunks, jnp.arange(self.num_query_chunks))\n#         query_blocks = query_blocks.reshape(batch_size, self.num_query_chunks, self.num_heads, self.query_chunk_size, self.dim_per_head)", "#         query_blocks = jax.lax.map(self._query_block, query_chunks, jnp.arange(self.num_query_chunks))\n#         query_blocks = query_blocks.reshape(batch_size, self.num_query_chunks, self.num_heads, self.query_chunk_size, self.dim_per_head)\n#         query_blocks = jnp.moveaxis(query_blocks, 2, 3)\n\n\n#         key_value_blocks = key_value_chunks.reshape(batch_size, self.num_key_value_chunks, self.num_heads, self.key_value_chunk_size, self.dim_per_head)\n#         key_value_blocks = jnp.moveaxis(key_value_blocks, 2, 3)\n\n#         carry = (query_blocks, None)\n#         key_value_blocks = jax.lax.scan(self._key_value_blocks, carry, (key_value_blocks, jnp.arange(self.num_key_value_chunks), key_value_position_ids))[0][0]", "#         carry = (query_blocks, None)\n#         key_value_blocks = jax.lax.scan(self._key_value_blocks, carry, (key_value_blocks, jnp.arange(self.num_key_value_chunks), key_value_position_ids))[0][0]\n\n#         key_value_blocks = jnp.moveaxis(key_value_blocks, 2, 3)\n#         key_value_blocks = key_value_blocks.reshape(batch_size, self.num_key_value_chunks, self.key_value_chunk_size, self.hidden_size)\n\n#         output = jax.lax.map(lambda x: self.feedforward(x.reshape(-1, self.hidden_size)), key_value_blocks)\n#         output = output.reshape(batch_size, seq_len, self.hidden_size)\n\n#         return output", "\n#         return output\n\n    \n    \n    \n    \n    \n\n#==================================== v2", "\n#==================================== v2\n\n\n# import jax\n# import jax.numpy as jnp\n# from jax.experimental import stax\n\n# class BlockwiseParallelTransformerAttention(nn.Module):\n#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):", "# class BlockwiseParallelTransformerAttention(nn.Module):\n#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n#         super(BlockwiseParallelTransformerAttention, self).__init__()\n#         self.input_size = input_size\n#         self.num_heads = num_heads\n#         self.hidden_size = hidden_size\n#         self.num_layers = num_layers\n#         self.max_seq_len = max_seq_len\n#         self.block_size = block_size\n        ", "#         self.block_size = block_size\n        \n#         self.query_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         self.key_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         self.value_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         self.feedforward = nn.Sequential(\n#             stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal()),\n#             nn.ReLU(),\n#             stax.Dense(num_heads * hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         )", "#             stax.Dense(num_heads * hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         )\n#         self.layer_norm1 = nn.LayerNorm(input_size)\n#         self.layer_norm2 = nn.LayerNorm(num_heads * hidden_size)\n        \n#     def forward(self, x):\n#         batch_size, seq_len, input_size = x.shape\n#         num_blocks = seq_len // self.block_size\n#         query_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n#         key_value_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)", "#         query_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n#         key_value_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n        \n#         for i in range(self.num_layers):\n#             query = self.query_blocks(query_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n#             key = self.key_blocks(key_value_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n#             value = self.value_blocks(key_value_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n            \n#             query = query.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n#             key = key.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))", "#             query = query.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n#             key = key.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n#             value = value.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n            \n#             attention_scores = jnp.matmul(query, key.transpose((0, 1, 2, 4, 3))) / jnp.sqrt(jnp.array(self.hidden_size, dtype=jnp.float32))\n#             attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n#             attention_output = jnp.matmul(attention_weights, value)\n#             attention_output = attention_output.transpose((0, 2, 3, 1, 4)).reshape(batch_size*num_blocks, self.block_size, self.num_heads*self.hidden_size)\n#             attention_output = self.feedforward(attention_output)\n#             attention_output = attention_output.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 2, 1, 3, 4)).reshape(batch_size, seq_len, self.num_heads*self.hidden_size)", "#             attention_output = self.feedforward(attention_output)\n#             attention_output = attention_output.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 2, 1, 3, 4)).reshape(batch_size, seq_len, self.num_heads*self.hidden_size)\n#             attention_output = self.layer_norm1(query_blocks + attention_output)\n#             attention_output = self.layer_norm2(attention_output)\n            \n#         return attention_output\n\n\n\n", "\n\n\n\n\n\n\n    \n    \n    ", "    \n    \n    \n    \n    \n    \n    \n    \n    \n    ", "    \n    \n    # def __call__(self, x, deterministic=True):\n    #     batch_size, seq_len, input_size = x.shape\n    #     assert input_size == self.input_size, f'Input size must be {self.input_size}, but got {input_size}'\n\n    #     query_chunks = x.reshape(batch_size, self.num_query_chunks, self.query_chunk_size, input_size)\n    #     query_chunks = self.query_blocks(query_chunks)\n    #     query_chunks = query_chunks / jnp.sqrt(query_chunks.shape[-1])\n", "    #     query_chunks = query_chunks / jnp.sqrt(query_chunks.shape[-1])\n\n    #     kv_chunks = x.reshape(batch_size, self.num_key_value_chunks, self.key_value_chunk_size, input_size)\n    #     kv_chunks = self.key_blocks(kv_chunks), self.value_blocks(kv_chunks)\n\n    #     init_carry = (jnp.zeros((batch_size, self.query_chunk_size, self.num_heads, self.dim_per_head)),\n    #                   jnp.zeros((batch_size, self.query_chunk_size, self.num_heads, self.dim_per_head)),\n    #                   (-jnp.inf) * jnp.ones((batch_size, self.query_chunk_size, self.num_heads, 1)))\n            \n    #     def attention_block(carry, args):", "            \n    #     def attention_block(carry, args):\n    #         query_chunk, query_chunk_idx = carry\n    #         kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n\n    #         key_chunk, value_chunk = kv_chunk\n    #         attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk)\n    #         bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n    #         bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n    #         attn_weights = attn_weights + bias_chunk", "    #         bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n    #         attn_weights = attn_weights + bias_chunk\n    #         max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n    #         exp_weights = jnp.exp(attn_weights - max_score)\n    #         exp_values = jnp.einsum('bqhv,bvhf->bqhf', exp_weights, value_chunk)\n    #         numerator = jax.lax.dynamic_update_slice(query_chunk, exp_values, (slice(None), query_chunk_idx, slice(None), slice(None)))\n    #         denominator = jax.lax.dynamic_update_slice(query_chunk, exp_weights.sum(axis=-1, keepdims=True), (slice(None), query_chunk_idx, slice(None), slice(None)))\n    #         return (numerator, denominator), None\n\n    #     def combine_blocks(carry, args):", "\n    #     def combine_blocks(carry, args):\n    #         query_chunk, query_chunk_idx = carry\n    #         numerator, denominator = args\n    #         numerator = jnp.concatenate([query_chunk, numerator], axis=2)\n    #         denominator = jnp.concatenate([jnp.ones_like(query_chunk), denominator], axis=2)\n    #         attn_output = jnp.sum(numerator / denominator, axis=2)\n    #         attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)\n    #         attn_output = attn_output + x\n    #         return (attn_output, query_chunk_idx + 1), None", "    #         attn_output = attn_output + x\n    #         return (attn_output, query_chunk_idx + 1), None\n\n    #     def feedforward_block(x):\n    #         hidden = self.feedforward(x)\n    #         hidden = nn.gelu(hidden)\n    #         return hidden + x\n\n    #     for layer_idx in range(self.num_layers):\n    #         query_chunk_idx = 0", "    #     for layer_idx in range(self.num_layers):\n    #         query_chunk_idx = 0\n    #         carry = (query_chunks[:, query_chunk_idx], query_chunk_idx)\n    #         for key_chunk_idx in range(self.num_key_value_chunks):\n    #             kv_chunk = kv_chunks[:, key_chunk_idx]\n    #             kv_position_ids_chunk = self.key_value_position_ids[key_chunk_idx * self.key_value_chunk_size:(key_chunk_idx + 1) * self.key_value_chunk_size]\n    #             carry, _ = BlockParallel(self.num_heads)(attention_block, carry, (kv_chunk, key_chunk_idx, kv_position_ids_chunk))\n    #         attn_output, _ = BlockParallel()(combine_blocks, carry, None)\n    #         x = attn_output\n    #         x = BlockParallel()(feedforward_block, x)", "    #         x = attn_output\n    #         x = BlockParallel()(feedforward_block, x)\n    #     return x\n            \n        \n        \n        \n        \n        \n        ", "        \n        \n        \n        \n        \n    #     # for key_chunk_idx in range(self.num_key_value_chunks):\n    #     #     for key_chunk_idx in range(self.num_key_value_chunks):\n    #     #         key_value_chunk = kv_chunks[:, key_chunk_idx]\n    #     #         key_value_position_ids_chunk = self.key_value_position_ids[key_chunk_idx * self.key_value_chunk_size:(key_chunk_idx + 1) * self.key_value_chunk_size]\n    #     #         carry, _ = lax.scan(self._key_value_blocks, carry, (key_value_chunk, key_chunk_idx, key_value_position_ids_chunk))", "    #     #         key_value_position_ids_chunk = self.key_value_position_ids[key_chunk_idx * self.key_value_chunk_size:(key_chunk_idx + 1) * self.key_value_chunk_size]\n    #     #         carry, _ = lax.scan(self._key_value_blocks, carry, (key_value_chunk, key_chunk_idx, key_value_position_ids_chunk))\n\n    #     #     numerator, denominator, bias = carry\n    #     #     attn_weights = numerator / denominator\n    #     #     attn_weights = jax.lax.dynamic_update_slice(attn_weights, bias, (slice(None), slice(None), slice(None), 0))\n    #     #     attn_weights = nn.softmax(attn_weights, axis=-2)\n    #     #     attn_weights = jax.lax.dynamic_update_slice(attn_weights, jnp.zeros_like(bias), (slice(None), slice(None), slice(None), 0))\n\n    #     #     value_chunk = jnp.einsum('bqhv,bvhf->bqhf', attn_weights, kv_chunks)", "\n    #     #     value_chunk = jnp.einsum('bqhv,bvhf->bqhf', attn_weights, kv_chunks)\n    #     #     value_chunk = value_chunk.reshape(batch_size, self.num_heads * self.query_chunk_size, self.dim_per_head)\n    #     #     value_chunk = self.feedforward(value_chunk)\n    #     #     value_chunk = value_chunk.reshape(batch_size, self.num_heads, self.query_chunk_size, self.dim_per_head)\n    #     #     value_chunk = jnp.moveaxis(value_chunk, 1, 2)\n    #     #     if query_chunk_idx == 0:\n    #     #         output = value_chunk\n    #     #     else:\n    #     #         output = jnp.concatenate([output, value_chunk], axis=2)", "    #     #     else:\n    #     #         output = jnp.concatenate([output, value_chunk], axis=2)\n\n    #     # output = output.reshape(batch_size, seq_len, self.hidden_size)\n    #     # return output\n\n    #     # # def _key_value_blocks(cell, carry, args):\n    #     # #     kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n    #     # #     query_chunk, query_chunk_idx = carry\n    #     # #     key_chunk = self.key_blocks(kv_chunk)", "    #     # #     query_chunk, query_chunk_idx = carry\n    #     # #     key_chunk = self.key_blocks(kv_chunk)\n    #     # #     value_chunk = self.value_blocks(kv_chunk)\n    #     # #     attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk)\n    #     # #     bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n    #     # #     bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n    #     # #     attn_weights = attn_weights + bias_chunk\n    #     # #     max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n    #     # #     exp_weights = jnp.exp(attn_weights - max_score)\n    #     # #     exp_values = jnp.einsum('bqhv,bvhf->bqhf', exp_weights, value_chunk)", "    #     # #     exp_weights = jnp.exp(attn_weights - max_score)\n    #     # #     exp_values = jnp.einsum('bqhv,bvhf->bqhf', exp_weights, value_chunk)\n    #     # #     numerator = jax.lax.dynamic_update_slice(query_chunk, exp_values, (slice(None), key_chunk_idx, slice(None), slice(None)))\n    #     # #     denominator = jax.lax.dynamic_update_slice(query_chunk, exp_weights.sum(axis=-1, keepdims=True), (slice(None), key_chunk_idx, slice(None), slice(None)))\n    #     # #     return (numerator, denominator), None\n        \n    #     # # for query_chunk_idx in range(self.num_query_chunks):\n    #     # #     query_chunk = self._query_block(query_chunks[:, query_chunk_idx], query_chunk_idx)\n    #     # #     for key_value_chunk_idx in range(self.num_key_value_chunks):\n    #     # #         kv_chunk = kv_chunks[:, key_value_chunk_idx, :, :]", "    #     # #     for key_value_chunk_idx in range(self.num_key_value_chunks):\n    #     # #         kv_chunk = kv_chunks[:, key_value_chunk_idx, :, :]\n    #     # #         init_carry = (query_chunk, query_chunk_idx)\n    #     # #         (numerator, denominator), _ = lax.scan(_key_value_blocks, init_carry, (kv_chunk, key_value_chunk_idx))\n    #     # #     attention_output_chunk = numerator / denominator \n    #     # #     attention_output_chunk = self.feedforward(attention_output_chunk)\n    #     # #     query_chunk = query_chunks[:, query_chunk_idx]\n    #     # #     attention_output_chunk = attention_output_chunk + query_chunk\n    #     # #     attention_output_chunk = nn.LayerNorm(attention_output_chunk)\n    #     # #     query_chunks = jax.lax.dynamic_update_slice(query_chunks, attention_output_chunk, (slice(None), query_chunk_idx, slice(None), slice(None)))", "    #     # #     attention_output_chunk = nn.LayerNorm(attention_output_chunk)\n    #     # #     query_chunks = jax.lax.dynamic_update_slice(query_chunks, attention_output_chunk, (slice(None), query_chunk_idx, slice(None), slice(None)))\n        \n    #     # # attention_output = query_chunks.reshape(batch_size, seq_len, self.hidden_size)\n    #     # # return attention_output\n        \n\n        \n    # def BlockParallel(num_blocks=None):\n    #     def decorator(f):", "    # def BlockParallel(num_blocks=None):\n    #     def decorator(f):\n    #         def wrapper(*args, **kwargs):\n    #             if num_blocks is None:\n    #                 num_blocks = jax.local_device_count()\n    #             block_size = args[0].shape[0] // num_blocks\n    #             blocks = [jax.lax.dynamic_slice_in_dim(args[0], i * block_size, block_size, axis=0) for i in range(num_blocks)]\n    #             args = [(block,) + args[1:] for block in blocks]\n    #             outputs = jax.pmap(f)(*args, **kwargs)\n    #             return jnp.concatenate(outputs, axis=0)", "    #             outputs = jax.pmap(f)(*args, **kwargs)\n    #             return jnp.concatenate(outputs, axis=0)\n    #         return wrapper\n#     #     return decorator\n#     import jax\n# import jax.numpy as jnp\n# from jax.experimental import stax\n\n# class BlockwiseParallelTransformerAttention(nn.Module):\n#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):", "# class BlockwiseParallelTransformerAttention(nn.Module):\n#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n#         super(BlockwiseParallelTransformerAttention, self).__init__()\n#         self.input_size = input_size\n#         self.num_heads = num_heads\n#         self.hidden_size = hidden_size\n#         self.num_layers = num_layers\n#         self.max_seq_len = max_seq_len\n#         self.block_size = block_size\n        ", "#         self.block_size = block_size\n        \n#         self.query_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         self.key_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         self.value_blocks = stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         self.feedforward = nn.Sequential(\n#             stax.Dense(hidden_size, W_init=jax.nn.initializers.glorot_normal()),\n#             nn.ReLU(),\n#             stax.Dense(num_heads * hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         )", "#             stax.Dense(num_heads * hidden_size, W_init=jax.nn.initializers.glorot_normal())\n#         )\n#         self.layer_norm1 = nn.LayerNorm(input_size)\n#         self.layer_norm2 = nn.LayerNorm(num_heads * hidden_size)\n        \n#     def forward(self, x):\n#         batch_size, seq_len, input_size = x.shape\n#         num_blocks = seq_len // self.block_size\n#         query_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n#         key_value_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)", "#         query_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n#         key_value_blocks = x[:, :num_blocks*self.block_size, :].reshape(batch_size, num_blocks, self.block_size, input_size)\n        \n#         for i in range(self.num_layers):\n#             query = self.query_blocks(query_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n#             key = self.key_blocks(key_value_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n#             value = self.value_blocks(key_value_blocks.reshape(batch_size*num_blocks, self.block_size, input_size))\n            \n#             query = query.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n#             key = key.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))", "#             query = query.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n#             key = key.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n#             value = value.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 3, 1, 2, 4))\n            \n#             attention_scores = jnp.matmul(query, key.transpose((0, 1, 2, 4, 3))) / jnp.sqrt(jnp.array(self.hidden_size, dtype=jnp.float32))\n#             attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n#             attention_output = jnp.matmul(attention_weights, value)\n#             attention_output = attention_output.transpose((0, 2, 3, 1, 4)).reshape(batch_size*num_blocks, self.block_size, self.num_heads*self.hidden_size)\n#             attention_output = self.feedforward(attention_output)\n#             attention_output = attention_output.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 2, 1, 3, 4)).reshape(batch_size, seq_len, self.num_heads*self.hidden_size)", "#             attention_output = self.feedforward(attention_output)\n#             attention_output = attention_output.reshape(batch_size, num_blocks, self.block_size, self.num_heads, self.hidden_size).transpose((0, 2, 1, 3, 4)).reshape(batch_size, seq_len, self.num_heads*self.hidden_size)\n#             attention_output = self.layer_norm1(query_blocks + attention_output)\n#             attention_output = self.layer_norm2(attention_output)\n            \n#         return attention_output\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n#==================================== v3\nimport functools", "#==================================== v3\nimport functools\nimport json\nimport math\nfrom functools import partial\nfrom typing import Callable, NamedTuple, Optional\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp", "import jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom einops import rearrange\nfrom flax.linen import combine_masks, make_causal_mask\nfrom jax import lax\nfrom jax import numpy as jnp\n\n\ndef quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)", "\ndef quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)\n\nACT2FN = {\n    \"gelu\": partial(nn.gelu, approximate=False),\n    \"relu\": nn.relu,\n    \"silu\": nn.swish,\n    \"swish\": nn.swish,\n    \"gelu_new\": partial(nn.gelu, approximate=True),", "    \"swish\": nn.swish,\n    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,\n}\n\ndef get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n    }[name]", "\nMASK_VALUE = -1e10\n\nQ_CHUNK_SIZE = 1024\nK_CHUNK_SIZE = 1024\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)", "\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)", "def apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass _AttentionBlock(nn.Module):\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    float32_logits: bool = False\n\n    def setup(self):\n        self.embed_dim = self.hidden_size\n        self.head_dim = self.embed_dim // self.num_heads\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.variance_scaling(\n                scale=1.0, mode='fan_in',\n                distribution='normal',\n            )\n        )\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\n        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n        self.fc_in = nn.Dense(self.intermediate_size,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.fc_out = nn.Dense(self.embed_dim,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.act = ACT2FN[self.activation_function]\n        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            pos_embd_dim = self.rotary_dim\n        else:\n            pos_embd_dim = self.embed_dim // self.num_heads\n        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    def attn_out_proj(self, attn_output, deterministic):\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.out_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        return attn_output\n\n    def forward_qkv(\n        self,\n        hidden_states,\n        position_ids,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_1(hidden_states)\n        query = self.q_proj(hidden_states)\n        key = self.k_proj(hidden_states)\n        value = self.v_proj(hidden_states)\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            k_rot = key[:, :, :, : self.rotary_dim]\n            k_pass = key[:, :, :, self.rotary_dim :]\n\n            q_rot = query[:, :, :, : self.rotary_dim]\n            q_pass = query[:, :, :, self.rotary_dim :]\n\n            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\n            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n        else:\n            key = apply_rotary_pos_emb(key, sincos)\n            query = apply_rotary_pos_emb(query, sincos)\n\n        if self.float32_logits:\n            query = query.astype(jnp.float32)\n            key = key.astype(jnp.float32)\n\n        return query, key, value\n\n    def forward_ffn(\n        self,\n        hidden_states,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_2(hidden_states)\n        hidden_states = self.fc_in(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc_out(hidden_states)\n        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\n        return hidden_states", "\n\nclass AttentionBlock(nn.Module):\n    q_chunk_size: int\n    k_chunk_size: int\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    attn_pdrop: float = 0.0\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    policy: str = 'nothing_saveable'\n    prevent_cse: bool = False\n    float32_logits: bool = False\n\n    def setup(self):\n        self.attn = _AttentionBlock(\n            self.hidden_size,\n            self.num_heads,\n            self.rotary_dim,\n            self.intermediate_size,\n            self.layer_norm_epsilon,\n            self.activation_function,\n            self.resid_pdrop,\n            self.max_position_embeddings,\n            self.dtype,\n            self.causal,\n            self.float32_logits,\n        )\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        \"\"\"\n        This function takes projected key, value states from a single input token and concatenates the states to cached\n        states from previous steps. This function is slighly adapted from the official Flax repository:\n        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n        \"\"\"\n        # detect if we're initializing by absence of existing cache data.\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask,\n        position_ids,\n        deterministic: bool = True,\n        init_cache: bool = False,\n    ):\n        query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n        query = query / jnp.sqrt(query.shape[-1])\n\n        dropout_rng = None\n        if not deterministic and self.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        attention_bias = lax.select(\n            attention_mask > 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n        )\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n            # use standard dot product attention since query length is 1\n            attn_weights = nn.dot_product_attention_weights(\n                query,\n                key,\n                bias=attention_bias,\n                dropout_rng=dropout_rng,\n                dropout_rate=self.config.attn_pdrop,\n                deterministic=deterministic,\n                dtype=self.dtype,\n                precision=None,\n            )\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n            outputs = attn_output + ffn_output + hidden_states\n        else:\n            attn_output = blockwise_compute_attn(\n                query,\n                key,\n                value,\n                bias=attention_bias,\n                deterministic=not deterministic,\n                dropout_rng=dropout_rng,\n                attn_pdrop=self.attn_pdrop,\n                causal_mask=self.causal,\n                query_chunk_size=self.q_chunk_size,\n                key_chunk_size=self.k_chunk_size,\n                dtype=self.dtype,\n                policy=self.policy,\n                precision=None,\n                prevent_cse=self.prevent_cse,\n            )\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = blockwise_compute_ffn(\n                self.attn,\n                hidden_states + attn_output,\n                chunk_size=self.q_chunk_size,\n                deterministic=deterministic,\n                policy=self.policy,\n                prevent_cse=self.prevent_cse,\n            )\n            outputs = ffn_output + hidden_states + attn_output\n        return outputs", "\n\ndef _chunk_attention_bias(query_chunk_size, key_chunk_size,\n            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n            query_chunk_idx, key_chunk_idx):\n    query_offset = query_chunk_idx * query_chunk_size\n    key_offset = key_chunk_idx * key_chunk_size\n    chunk_bias = jnp.zeros((1, 1, 1, 1))\n    if bias is not None:\n        chunk_bias = lax.dynamic_slice(\n            bias,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n        )\n\n    if causal_mask:\n        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n        offset = query_offset - key_offset\n        query_idx += offset\n        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_slice = lax.dynamic_slice(\n            attn_dropout,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(\n                *attn_dropout.shape[:2],\n                min(attn_dropout.shape[-2], query_chunk_size),\n                min(attn_dropout.shape[-1], key_chunk_size),\n            ),\n        )\n        chunk_bias -= attn_dropout_slice * 1e6\n    return chunk_bias", "\nclass Carry(NamedTuple):\n    numerator: jax.Array\n    denominator: jax.Array\n    max_so_far: jax.Array\n\ndef blockwise_compute_attn(query, key, value,\n        bias=None,\n        deterministic=False,\n        dropout_rng=None,\n        attn_pdrop=0.0,\n        causal_mask=True,\n        query_chunk_size=None,\n        key_chunk_size=None,\n        dtype=jnp.float32,\n        policy='nothing_saveable',\n        precision=lax.Precision.HIGHEST,\n        prevent_cse=False,):\n    q_len = query.shape[1]\n    kv_len = key.shape[1]\n    query = rearrange(query, 'b (n c) h q -> b n c h q', c=query_chunk_size)\n    key, value = map(lambda t: rearrange(t, 'b (n c) h v -> b n c h v', c=key_chunk_size), (key, value))\n    query, key, value = map(lambda t: rearrange(t, 'b n c h d -> n b c h d'), (query, key, value))\n    num_q, batch, _, num_heads, dim_per_head = query.shape\n    num_kv, _, _, _, _ = key.shape\n\n    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n        assert bias_dim == 1 or bias_dim == broadcast_dim\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n    else:\n        attn_dropout = None\n\n    _chunk_bias_fn = functools.partial(\n        _chunk_attention_bias,\n        query_chunk_size, key_chunk_size,\n        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\n    def _query_chunk_attention(args):\n        query_chunk, query_chunk_idx = args\n\n        @functools.partial(jax.checkpoint, prevent_cse=prevent_cse,\n                           policy=get_gradient_checkpoint_policy(policy))\n        def summarize_chunk(carry, args):\n            key_chunk, value_chunk, key_chunk_idx = args\n            (numerator, denominator, prev_max_score) = carry\n            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n            attn_weights = attn_weights + bias_chunk\n\n            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n            max_score = jnp.maximum(prev_max_score, max_score)\n            max_score = jax.lax.stop_gradient(max_score)\n            exp_weights = jnp.exp(attn_weights - max_score)\n            exp_values = jnp.einsum(\n                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n            )\n            correction = jnp.exp(prev_max_score - max_score)\n            numerator = numerator * correction + exp_values\n            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n            return Carry(numerator, denominator, max_score), None\n\n        init_carry = Carry(\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n        )\n        (numerator, denominator, max_score), _ = lax.scan(\n            summarize_chunk, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n        )\n        outputs = (numerator / denominator).astype(dtype)\n        return outputs\n\n    _, res = lax.scan(\n        lambda _, x: ((), _query_chunk_attention(x)),\n        (), xs=(query, jnp.arange(0, num_q))\n    )\n    res = rearrange(res, 'n b c h d -> b (n c) h d')\n    return res", "\ndef blockwise_compute_ffn(cell, inputs, chunk_size, deterministic, policy, prevent_cse):\n    inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=chunk_size)\n    inputs = rearrange(inputs, 'b n c d -> n b c d')\n    num_q, _, _, _ = inputs.shape\n    def ffn(cell, _, hidden_states):\n        outputs = cell.forward_ffn(hidden_states, deterministic=deterministic)\n        return _, outputs\n    ffn_remat = nn.remat(\n        ffn,\n        variables=\"params\",\n        rngs={\"params\" : False},\n        prevent_cse=prevent_cse,\n        policy=get_gradient_checkpoint_policy(policy),\n    )\n    _, res = nn.scan(\n        ffn_remat,\n        variable_broadcast=\"params\",\n        split_rngs={\"params\": False},\n        in_axes=0,\n        out_axes=0,\n        length=num_q,\n    )(cell, None, inputs)\n    res = rearrange(res, 'n b c d -> b (n c) d')\n    return res", "\nclass Blockwise_LM_Head(nn.Module):\n    vocab_size: int\n    chunk_size: int\n    policy: str = 'nothing_saveable'\n    dtype: jnp.dtype = jnp.float32\n    prevent_cse: bool = False\n\n    def setup(self):\n        self.lm_head = nn.Dense(\n            self.vocab_size,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.variance_scaling(\n                scale=1.0, mode='fan_in',\n                distribution='normal',\n            )\n        )\n\n    def __call__(self, inputs):\n        inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=self.chunk_size)\n        inputs = rearrange(inputs, 'b n c d -> n b c d')\n        num_q, _, _, _ = inputs.shape\n        def lm_head(cell, _, hidden_states):\n            outputs = cell(hidden_states)\n            return _, outputs\n        lm_head_remat = nn.remat(\n            lm_head,\n            variables=\"params\",\n            rngs={\"params\" : False},\n            prevent_cse=self.prevent_cse,\n            policy=get_gradient_checkpoint_policy(self.policy),\n        )\n        _, res = nn.scan(\n            lm_head_remat,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False},\n            in_axes=0,\n            out_axes=0,\n            length=num_q,\n        )(self.lm_head, None, inputs)\n        res = rearrange(res, 'n b c d -> b (n c) d')\n        return res", "\ndef blockwise_cross_entropy(logits, tokens, valid=None,\n                            chunk_size=None, policy=None, prevent_cse=None):\n    if valid is None:\n        valid = jnp.ones(tokens.shape[:2])\n    valid = valid.astype(jnp.float32)\n    logits = jnp.reshape(logits, (-1, logits.shape[-1]))\n    tokens = jnp.reshape(tokens, (-1,))\n    valid = jnp.reshape(valid, (-1,))\n\n    def _cross_entropy_loss_and_accuracy(logits, tokens, valid):\n        valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n\n        token_log_prob = jnp.squeeze(\n            jnp.take_along_axis(\n                jax.nn.log_softmax(logits, axis=-1),\n                jnp.expand_dims(tokens, -1),\n                axis=-1,\n            ),\n            -1,\n        )\n        token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n        correct = jnp.where(\n            valid > 0.0,\n            jnp.argmax(logits, axis=-1) == tokens,\n            jnp.array(False)\n        )\n        return token_log_prob, correct, valid_text_length\n    @partial(jax.checkpoint, prevent_cse=prevent_cse,\n             policy=get_gradient_checkpoint_policy(policy))\n    def _loss_and_accuracy(carry, args):\n        loss, accuracy, num = carry\n        logits, tokens, valid = args\n        token_log_prob, correct, valid_text_length = \\\n            _cross_entropy_loss_and_accuracy(logits, tokens, valid)\n        loss = loss + jnp.sum(token_log_prob, axis=-1) / valid_text_length\n        accuracy = accuracy + jnp.sum(correct, axis=-1) / valid_text_length\n        num = num + 1\n        return (loss, accuracy, num), None\n    num_chunk = logits.shape[0] // chunk_size\n    logits = rearrange(logits, '(n c) d -> n c d', c=chunk_size)\n    tokens = rearrange(tokens, '(n c) -> n c', c=chunk_size)\n    valid = rearrange(valid, '(n c) -> n c', c=chunk_size)\n    (loss, accuracy, num), _ = jax.lax.scan(\n        _loss_and_accuracy, (0.0, 0.0, 0), xs=(logits, tokens, valid),\n        length=num_chunk,\n    )\n    loss = - loss / num\n    accuracy = accuracy / num\n    return loss, accuracy", "\nif __name__ == '__main__':\n    with jax.profiler.trace('/tmp/prof/blockwise_parallel_simplified'):\n        class Model(nn.Module):\n            def setup(self):\n                self.blocks = [\n                    AttentionBlock(\n                        q_chunk_size=256,\n                        k_chunk_size=256,\n                        hidden_size=2048,\n                        num_heads=16,\n                        rotary_dim=128,\n                        intermediate_size=8192,\n                        layer_norm_epsilon=1e-5,\n                        activation_function=\"gelu\",\n                        resid_pdrop=0.0,\n                        max_position_embeddings=2048,\n                        dtype=jnp.float32,\n                        causal=True,\n                )\n                for _ in range(2)\n                ]\n            def __call__(self, hidden_states, attention_mask, position_ids):\n                for block in self.blocks:\n                    hidden_states = block(hidden_states, attention_mask, position_ids)\n                return hidden_states\n\n        hidden_states = jnp.zeros((2, 1024, 2048))\n        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n        model = Model()\n        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n        output = output.block_until_ready()", ""]}
{"filename": "blockwise_parallel/__init__.py", "chunked_list": ["# from blockwise_parallel.blockwise_paralle import BlockwiseParallelTransformerAttention\n# from blockwise_parallel.test1 import BlockwiseParallelTransformer/\nfrom blockwise_parallel.blockwise_parallel_jax import BlockwiseParallelTransformerAttention"]}
{"filename": "blockwise_parallel/blockwise_torch.py", "chunked_list": ["import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom einops import rearrange\n\nfrom types import partial\n\ndef quick_gelu(x):\n    return x * torch.sigmoid(1.702 * x)", "def quick_gelu(x):\n    return x * torch.sigmoid(1.702 * x)\n\n\nACT2FN = {\n    \"gelu\": F.gelu,\n    \"relu\": F.relu,\n    \"silu\": F.silu,\n    \"swish\": F.swish,\n    \"gelu_new\": quick_gelu,", "    \"swish\": F.swish,\n    \"gelu_new\": quick_gelu,\n    \"quick_gelu\": quick_gelu,\n}\n\n\nMASK_VALUE = -1e10\nQ_CHUNK_SIZE = 1024\nK_CHUNK_SIZE = 1024\n", "K_CHUNK_SIZE = 1024\n\n\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 * (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i, j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, 0:sentinel] = cos\n\n    return torch.tensor(out)", "\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = torch.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), dim=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(1, 1, 2, 1)\n    cos_pos = cos_pos[:, :, None, :].repeat(1, 1, 2, 1)\n    return (torch * cos_pos) + (rotate_every_two(tensor) * sin_pos)", "def apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(1, 1, 2, 1)\n    cos_pos = cos_pos[:, :, None, :].repeat(1, 1, 2, 1)\n    return (torch * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass BlockwiseParallel(nn.Module):\n    def __init__(self, hidden_size, num_heads, rotary_dim, intermediate_size, layer_norm_epsilon=1e-5,\n                 activation_function=\"gelu\", resid_pdrop=0.0, max_position_embeddings=1024, dtype=torch.float32,\n                 casual=True, float32_logits=False):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.rotary_dim = rotary_dim\n        self.intermediate_size = intermediate_size\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.activation_function = activation_function\n        self.resid_pdrop = resid_pdrop\n        self.max_position_embeddings = max_position_embeddings\n        self.dtype = dtype\n        self.casual = casual\n        self.float32_logits = float32_logits\n\n        self.embed_dim = self.hidden_size\n        self.head_dim = self.embed_dim // self.num_heads\n        dense = partial(\n            nn.Linear,\n            self.embed_dim,\n            bias=False,\n            dtype=self.dtype\n        )\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n        self.ln_1 = nn.LayerNorm(self.hideen_size, eps=self.layer_norm_epsilon, elementwise_affine=True)\n\n        self.ln_2 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_epsilon, elementwise_affine=True)\n        self.fc_in = nn.Linear(self.hidden_size, self.intermediate_size, dtype=self.dtype)\n        self.fc_out = nn.Linear(self.intermediate_size, self.hidden_size, dtype=self.dtype)\n        self.act = ACT2FN(self.activation_function)\n        self.resid_pdrop = nn.Dropout(p=self.resid_pdrop)\n\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            pos_embd_dim = self.rotary_dim\n        else:\n            pos_embd_dim = self.embed_dim // self.num_heads\n        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.view(hidden_states.shape[-2] + (self.num_heads, self.head_dim))\n    \n    def _merge_heads(self, hidden_states):\n        return hidden_states.view(hidden_states.shape[:-2] + (self.embed_dim,))\n    \n\n    def attn_out_proj(self, attn_output, deterministic):\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.out_proj(attn_output)\n        attn_output = self.resid_pdrop(attn_output)\n        return attn_output\n    \n    def forward_qkv(self, hidden_states, position_ids, deterministic=True):\n        hidden_states = self.ln_1(hidden_states)\n        q = self.q_proj(hidden_states)\n        k = self.k_proj(hidden_states)\n        v = self.v_proj(hidden_states)\n\n\n\n        q = self._split_heads(q)\n        k = self._split_heads(k)\n        v = self._split_heads(v)\n\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            sincos = self.embed_positions[position_ids].unsqueeze(1)\n            q, k = apply_rotary_pos_emb(q, sincos), apply_rotary_pos_emb(k, sincos)\n\n        return q, k, v\n\n    def forward(self, hidden_states, position_ids, attention_mask=None, deterministic=True):\n        q, k, v = self.forward_qkv(hidden_states, position_ids, deterministic)\n\n        attn_output, attn_weights = self._attn(q, k, v, attention_mask, deterministic)\n        attn_output = self.attn_out_proj(attn_output, deterministic)\n\n        hidden_states = hidden_states + attn_output\n        hidden_states = self.ln_2(hidden_states)\n\n        ffn_output = self.fc_in(hidden_states)\n        ffn_output = self.act(ffn_output)\n        ffn_output = self.fc_out(ffn_output)\n        ffn_output = self.resid_pdrop(ffn_output)\n\n        hidden_states = hidden_states + ffn_output\n\n        return hidden_states, attn_weights\n    \n    def _attn(self, q, k, v, attention_mask=None, deterministic=True):\n        attn_weights = torch.matmul(q, k.transpose(-1, -2))\n\n        if attention_mask is None:\n            attn_weights = attn_weights + attention_mask\n\n        attn_weights = F.softmax(attn_weights, dim=-1)\n\n        if not deterministic:\n            attn_weights = self.resid_pdrop(attn_weights)\n\n        attn_output = torch.matmul(attn_weights, v)\n\n        return attn_output, attn_weights", "    \n\n\n"]}
{"filename": "blockwise_parallel/blockwise_parallel_torch.py", "chunked_list": ["import torch \nimport torch.nn as nn\n\nclass BlockwiseParallelTransformerAttention(nn.Module):\n    def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n        super(BlockwiseParallelTransformerAttention, self).__init__()\n        self.input_size = input_size\n        self.num_heads = num_heads\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.max_seq_len = max_seq_len\n        self.block_size = block_size\n        self.dim_per_head = hidden_size // num_heads\n\n        self.query_chunk_size = max_seq_len // block_size\n        self.key_value_chunk_size = max_seq_len // block_size\n        self.num_query_chunks = (max_seq_len + self.query_chunk_size - 1) // self.query_chunk_size\n        self.num_key_value_chunks = (max_seq_len + self.key_value_chunk_size - 1) // self.key_value_chunk_size\n\n        self.query_position_ids = torch.arange(max_seq_len)\n        self.key_value_position_ids = torch.arange(max_seq_len)\n\n        self.query_blocks = nn.Linear(input_size, hidden_size, bias=False)\n        self.key_blocks = nn.Linear(input_size, hidden_size, bias=False)\n        self.value_blocks = nn.Linear(input_size, hidden_size, bias=False)\n        self.feedforward = nn.Linear(hidden_size, hidden_size)\n\n    def _chunk_bias_fn(self, query_chunk_idx, key_chunk_idx):\n        start = key_chunk_idx * self.key_value_chunk_size\n        end = (key_chunk_idx + 1) * self.key_value_chunk_size\n        bias_chunk = torch.zeros((self.num_heads, self.query_chunk_size, self.key_value_chunk_size))\n        bias_chunk[:, :, start:end] = 1\n        bias_chunk = bias_chunk.unsqueeze(0)\n        bias_chunk = bias_chunk.repeat(query_chunk_idx.shape[0], 1, 1, 1)\n        return bias_chunk\n    \n    def _query_block(self, input_chunk, query_chunk_idx):\n        query_chunk = self.query_blocks(input_chunk)\n        query_chunk = query_chunk / torch.sqrt(query_chunk.shape[-1])\n        return query_chunk\n    \n    def _key_value_blocks(self, carry, args):\n        kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n        query_chunk, query_chunk_idx = carry\n        key_chunk = self.key_blocks(kv_chunk)\n        value_chunk = self.value_blocks(kv_chunk)\n        attn_weights = torch.einsum('bqhd, bkhd->bqhk', query_chunk. key_chunk)\n        bias_chunk = self._chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n        bias_chunk = bias_chunk.permute(0, 1, 3, 2)\n        attn_weights = attn_weights + bias_chunk\n        max_score = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n        exp_weights = torch.exp(attn_weights - max_score)\n        exp_values = torch.einsum('bqhv, bvhf->bqhf', exp_weights, value_chunk)\n        numerator = query_chunk.clone()\n        numerator[:, key_chunk_idx, :, :] = exp_values\n        denominator = query_chunk.clone()\n        denominator[:, key_chunk_idx, :, :] = exp_weights.sum(dim=-1, keepdim=True)\n        return (numerator, denominator), None\n    \n    def forward(self, x, deterministic=None):\n        batch_size, seq_len, input_size = x.shape\n        assert input_size == self.input_size, f\"Input size must be {self.input_size} but got {input_size}\"\n\n        query_chunks = x.reshape(batch_size, self.num_query_chunks, self.query_chunk_size, input_size)\n        query_chunks = self.query_blocks(query_chunks)\n\n        query_chunks = query_chunks / torch.sqrt(query_chunks.shape[-1])\n        query_position_ids = self.query_position_ids.repeat(batch_size, 1)\n        query_position_ids = query_position_ids.reshape(batch_size, self.num_query_chunks, self.query_chunk_size)\n        query_position_ids = query_position_ids.roll(shift=-1, dims=-1)\n        query_position_ids[:, :, -1] = self.max_seq_len - 1\n\n        key_value_chunks = x.reshape(batch_size, self.num_key_value_chunks, self.key_value_chunk_size, input_size)\n        key_value_chunks = key_value_chunks.detach() if deterministic else key_value_chunks\n        key_value_position_ids = self.key_value_chunk_position_ids.repeat(batch_size, 1)\n        key_value_position_ids = key_value_position_ids[:, :-1, :]\n        key_value_position_ids = torch.cat([key_value_position_ids, torch.ones((batch_size, 1, self.key_value_chunk_size)) * (self.max_seq_len -1)], dim=1)\n\n        carry = (query_chunks, None)\n        for key_chunk_idx in range(self.num_key_value_chunks):\n            kv_chunk = key_value_chunks[:, key_chunk_idx, :, :]\n            kv_position_ids_chunk = key_value_position_ids[:, key_chunk_idx, :]\n            carry, _ = self._key_value_blocks(carry, (kv_chunk, key_chunk_idx, kv_position_ids_chunk))\n\n        attn_output = carry[0]\n        attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)\n        attn_output = self.feedforward(attn_output)\n\n        return attn_output", " \n\n    \n\n\n\n\n#inpout sequence\nbatch_size = 2\nseq_len = 1024", "batch_size = 2\nseq_len = 1024\ninput_size = 512\nx = torch.randn(batch_size, seq_len, input_size)\n\n\n#define params\nnum_heads = 8\nhidden_size = 512\nnum_layers = 6", "hidden_size = 512\nnum_layers = 6\nmax_seq_len = 1024\nblock_size = 64\n\n#crete an instance of blockwise paralel\nmodel = BlockwiseParallelTransformerAttention(input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size)\n\n\n#pass the input sequence to the module to get the output", "\n#pass the input sequence to the module to get the output\noutput = model(x)\n\nprint(output.shape)\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n\n\n\n\n\n\n\n", "\n\n# import torch \n# from torch import nn\n\n# class BlockwiseParallelTransformerAttention(nn.Module):\n#     def __init__(self, input_size, num_heads, hidden_size, num_layers, max_seq_len, block_size):\n#         super(BlockwiseParallelTransformerAttention, self).__init__()\n#         self.input_size = input_size\n#         self.num_heads = num_heads", "#         self.input_size = input_size\n#         self.num_heads = num_heads\n#         self.hidden_size = hidden_size\n#         self.num_layers = num_layers\n#         self.max_seq_len = max_seq_len\n#         self.block_size = block_size\n        \n#         self.query_projection = nn.Linear(input_size, num_heads * hidden_size)\n#         self.key_projection = nn.Linear(input_size, num_heads * hidden_size)\n#         self.value_projection = nn.Linear(input_size, num_heads * hidden_size)", "#         self.key_projection = nn.Linear(input_size, num_heads * hidden_size)\n#         self.value_projection = nn.Linear(input_size, num_heads * hidden_size)\n#         self.feedforward = nn.Sequential(\n#             nn.Linear(num_heads * hidden_size, hidden_size),\n#             nn.ReLU(),\n#             nn.Linear(hidden_size, num_heads * hidden_size)\n#         )\n#         self.layer_norm1 = nn.LayerNorm(input_size)\n#         self.layer_norm2 = nn.LayerNorm(num_heads * hidden_size)\n        ", "#         self.layer_norm2 = nn.LayerNorm(num_heads * hidden_size)\n        \n#     def forward(self, x):\n#         batch_size, seq_len, input_size = x.size()\n#         num_blocks = seq_len // self.block_size\n#         query_blocks = x[:, :num_blocks*self.block_size, :].view(batch_size, num_blocks, self.block_size, input_size)\n#         key_value_blocks = x[:, :num_blocks*self.block_size, :].view(batch_size, num_blocks, self.block_size, input_size)\n        \n#         for i in range(self.num_layers):\n#             for outer in range(num_blocks):", "#         for i in range(self.num_layers):\n#             for outer in range(num_blocks):\n#                 query = self.query_projection(query_blocks[:, outer, :, :])\n#                 for inner in range(num_blocks):\n#                     key = self.key_projection(key_value_blocks[:, inner, :, :])\n#                     value = self.value_projection(key_value_blocks[:, inner, :, :])\n                    \n#                     attention_scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.hidden_size, dtype=torch.float32))\n#                     attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n#                     attention_output = torch.matmul(attention_weights, value)", "#                     attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n#                     attention_output = torch.matmul(attention_weights, value)\n                    \n#                     if inner == 0:\n#                         blockwise_attention_output = attention_output\n#                     else:\n#                         blockwise_attention_output = torch.cat((blockwise_attention_output, attention_output), dim=2)\n                \n#                 blockwise_attention_output = blockwise_attention_output / torch.sqrt(torch.tensor(blockwise_attention_output.size(-1), dtype=torch.float32))\n#                 feedforward_output = self.feedforward(blockwise_attention_output)", "#                 blockwise_attention_output = blockwise_attention_output / torch.sqrt(torch.tensor(blockwise_attention_output.size(-1), dtype=torch.float32))\n#                 feedforward_output = self.feedforward(blockwise_attention_output)\n#                 residual_output = query_blocks[:, outer, :, :] + feedforward_output\n#                 query_blocks[:, outer, :, :] = self.layer_norm1(residual_output)\n                \n#             query_blocks = self.layer_norm2(query_blocks.view(batch_size, num_blocks*self.block_size, self.num_heads*self.hidden_size)).view(batch_size, num_blocks, self.block_size, self.num_heads*self.hidden_size)\n            \n#         return query_blocks.view(batch_size, seq_len, self.num_heads*self.hidden_size)\n    ", "    "]}
{"filename": "blockwise-parallel-transformer/bpt/model.py", "chunked_list": ["# coding=utf-8\n# Copyright 2021 The EleutherAI and The HuggingFace Inc. team.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,", "# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom functools import partial\nfrom typing import Optional, Tuple\nimport json", "from typing import Optional, Tuple\nimport json\n\nimport numpy as np\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nfrom flax.core.frozen_dict import FrozenDict, freeze, unfreeze\nfrom flax.linen import combine_masks, make_causal_mask", "from flax.core.frozen_dict import FrozenDict, freeze, unfreeze\nfrom flax.linen import combine_masks, make_causal_mask\nfrom flax.linen.attention import dot_product_attention_weights\nfrom flax.traverse_util import flatten_dict, unflatten_dict\nfrom jax import lax\nfrom flax.linen import partitioning as nn_partitioning\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput\nfrom transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel, append_call_sample_docstring", "from transformers.modeling_flax_outputs import FlaxBaseModelOutput, FlaxCausalLMOutput\nfrom transformers.modeling_flax_utils import ACT2FN, FlaxPreTrainedModel, append_call_sample_docstring\nfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\nfrom transformers.generation.flax_logits_process import FlaxLogitsProcessorList\nfrom transformers import AutoTokenizer\nfrom jax.sharding import PartitionSpec as PS\n\nfrom ml_collections import ConfigDict\nfrom ml_collections.config_dict import config_dict\nfrom bpt.tools.utils import function_args_to_config, load_pickle, open_file", "from ml_collections.config_dict import config_dict\nfrom bpt.tools.utils import function_args_to_config, load_pickle, open_file\n\nfrom bpt.tools.jax_utils import (\n    with_sharding_constraint, get_jax_mesh, get_gradient_checkpoint_policy\n)\nfrom bpt.blocks.memeff import AttentionBlock as MemEffAttentionBlock\nfrom bpt.blocks.blockwise_parallel_v1 import AttentionBlock as BPAttentionBlock_v1\nfrom bpt.blocks.blockwise_parallel import AttentionBlock as BPAttentionBlock, Blockwise_LM_Head\nfrom bpt.blocks.vanilla import AttentionBlock as VanillaAttentionBlock", "from bpt.blocks.blockwise_parallel import AttentionBlock as BPAttentionBlock, Blockwise_LM_Head\nfrom bpt.blocks.vanilla import AttentionBlock as VanillaAttentionBlock\n\n\nGPT_STANDARD_CONFIGS = {\n    # 1.3B\n    '1b': {\n        'vocab_size': 50432,\n        'n_embd': 2048,\n        'n_inner': 8192,", "        'n_embd': 2048,\n        'n_inner': 8192,\n        'n_layer': 24,\n        'n_head': 16,\n        'n_positions': 16384,\n        'initializer_range': 0.02,\n        'layer_norm_epsilon': 1e-5,\n        'use_cache': True,\n        'tie_word_embeddings': False,\n        'rotary_dim': 128,", "        'tie_word_embeddings': False,\n        'rotary_dim': 128,\n        'bos_token_id': 50256,\n        'eos_token_id': 50256,\n        'n_real_tokens': 50257,\n    },\n    # 2.7B\n    '3b': {\n        'vocab_size': 50432,\n        'n_embd': 2560,", "        'vocab_size': 50432,\n        'n_embd': 2560,\n        'n_inner': 10240,\n        'n_layer': 32,\n        'n_head': 32,\n        'n_positions': 16384,\n        'initializer_range': 0.02,\n        'layer_norm_epsilon': 1e-5,\n        'use_cache': True,\n        'tie_word_embeddings': False,", "        'use_cache': True,\n        'tie_word_embeddings': False,\n        'rotary_dim': 80,\n        'bos_token_id': 50256,\n        'eos_token_id': 50256,\n        'n_real_tokens': 50257,\n    },\n    # 6.7B\n    '7b': {\n        'vocab_size': 50432,", "    '7b': {\n        'vocab_size': 50432,\n        'n_embd': 4096,\n        'n_inner': 16384,\n        'n_layer': 32,\n        'n_head': 32,\n        'n_positions': 16384,\n        'initializer_range': 0.02,\n        'layer_norm_epsilon': 1e-5,\n        'use_cache': True,", "        'layer_norm_epsilon': 1e-5,\n        'use_cache': True,\n        'tie_word_embeddings': False,\n        'rotary_dim': 128,\n        'bos_token_id': 50256,\n        'eos_token_id': 50256,\n        'n_real_tokens': 50257,\n    },\n    # 13B\n    '13b': {", "    # 13B\n    '13b': {\n        'vocab_size': 50432,\n        'n_embd': 5120,\n        'n_inner': 20480,\n        'n_layer': 40,\n        'n_head': 40,\n        'n_positions': 16384,\n        'initializer_range': 0.02,\n        'layer_norm_epsilon': 1e-5,", "        'initializer_range': 0.02,\n        'layer_norm_epsilon': 1e-5,\n        'use_cache': True,\n        'tie_word_embeddings': False,\n        'rotary_dim': 128,\n        'bos_token_id': 50256,\n        'eos_token_id': 50256,\n        'n_real_tokens': 50257,\n    },\n    # 30B", "    },\n    # 30B\n    '30b': {\n        'vocab_size': 50432,\n        'n_embd': 7168,\n        'n_inner': 28672,\n        'n_layer': 48,\n        'n_head': 56,\n        'n_positions': 16384,\n        'initializer_range': 0.02,", "        'n_positions': 16384,\n        'initializer_range': 0.02,\n        'layer_norm_epsilon': 1e-5,\n        'use_cache': True,\n        'tie_word_embeddings': False,\n        'rotary_dim': 128,\n        'bos_token_id': 50256,\n        'eos_token_id': 50256,\n        'n_real_tokens': 50257,\n    },", "        'n_real_tokens': 50257,\n    },\n    # 70B\n    '70b': {\n        'vocab_size': 50432,\n        'n_embd': 8192,\n        'n_inner': 32768,\n        'n_layer': 80,\n        'n_head': 64,\n        'n_positions': 16384,", "        'n_head': 64,\n        'n_positions': 16384,\n        'initializer_range': 0.02,\n        'layer_norm_epsilon': 1e-5,\n        'use_cache': True,\n        'tie_word_embeddings': False,\n        'rotary_dim': 128,\n        'bos_token_id': 50256,\n        'eos_token_id': 50256,\n        'n_real_tokens': 50257,", "        'eos_token_id': 50256,\n        'n_real_tokens': 50257,\n    },\n    'debug': { # A small model for debugging\n        'vocab_size': 50432,\n        'n_embd': 128,\n        'n_inner': 256,\n        'n_layer': 2,\n        'n_head': 4,\n        'n_positions': 16384,", "        'n_head': 4,\n        'n_positions': 16384,\n        'initializer_range': 0.02,\n        'layer_norm_epsilon': 1e-5,\n        'use_cache': True,\n        'tie_word_embeddings': False,\n        'rotary_dim': 32,\n        'bos_token_id': 50256,\n        'eos_token_id': 50256,\n        'n_real_tokens': 50257,", "        'eos_token_id': 50256,\n        'n_real_tokens': 50257,\n    },\n}\n\nclass GPTConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`GPTModel`]. It is used to instantiate a GPT-J\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the GPT-J\n    [EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B) architecture. Configuration objects inherit from\n    [`PretrainedConfig`] and can be used to control the model outputs. Read the documentation from [`PretrainedConfig`]\n    for more information.\n    Args:\n        vocab_size (`int`, *optional*, defaults to 50432):\n            Vocabulary size of the GPT-J model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`GPTModel`].\n        n_positions (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        n_embd (`int`, *optional*, defaults to 4096):\n            Dimensionality of the embeddings and hidden states.\n        n_layer (`int`, *optional*, defaults to 28):\n            Number of hidden layers in the Transformer encoder.\n        n_head (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        rotary_dim (`int`, *optional*, defaults to 64):\n            Number of dimensions in the embedding that Rotary Position Embedding is applied to.\n        n_inner (`int`, *optional*, defaults to 0):\n            Dimensionality of the inner feed-forward layers. 0 will set it to 4 times n_embd\n        activation_function (`str`, *optional*, defaults to `\"gelu_new\"`):\n            Activation function, to be selected in the list `[\"relu\", \"silu\", \"gelu\", \"tanh\", \"gelu_new\"]`.\n        resid_pdrop (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        embd_pdrop (`int`, *optional*, defaults to 0.1):\n            The dropout ratio for the embeddings.\n        attn_pdrop (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention.\n        layer_norm_epsilon (`float`, *optional*, defaults to 1e-5):\n            The epsilon to use in the layer normalization layers.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        scale_attn_weights (`bool`, *optional*, defaults to `True`):\n            Scale attention weights by dividing by sqrt(hidden_size).\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models).\n    Example:\n    ```python\n    >>> from transformers import GPTModel, GPTConfig\n    >>> # Initializing a GPT-J 6B configuration\n    >>> configuration = GPTConfig()\n    >>> # Initializing a model from the configuration\n    >>> model = GPTModel(configuration)\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"gpt\"\n    attribute_map = {\n        \"max_position_embeddings\": \"n_positions\",\n        \"hidden_size\": \"n_embd\",\n        \"num_attention_heads\": \"n_head\",\n        \"num_hidden_layers\": \"n_layer\",\n    }\n\n    def __init__(\n        self,\n        vocab_size=50432,\n        n_positions=2048,\n        n_embd=4096,\n        n_layer=28,\n        n_head=16,\n        rotary_dim=64,\n        n_inner=None,\n        activation_function=\"gelu_new\",\n        resid_pdrop=0.0,\n        embd_pdrop=0.0,\n        attn_pdrop=0.0,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n        scale_attn_weights=True,\n        use_cache=True,\n        bos_token_id=50256,\n        eos_token_id=50256,\n        tie_word_embeddings=False,\n        gradient_checkpointing='nothing_saveable',\n        n_real_tokens=50257,\n        fcm_min_ratio=0.0,\n        fcm_max_ratio=0.0,\n        causal=True,\n        attn_type='dot',\n        q_chunk_size=1024,\n        k_chunk_size=2048,\n        scan_layers=True,\n        param_scan_axis=0,\n        float32_logits=False,\n        **kwargs\n    ):\n        self.vocab_size = vocab_size\n        self.n_positions = n_positions\n        self.n_embd = n_embd\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.n_inner = n_inner\n        self.rotary_dim = rotary_dim\n        self.activation_function = activation_function\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n        self.scale_attn_weights = scale_attn_weights\n        self.use_cache = use_cache\n        self.gradient_checkpointing = gradient_checkpointing\n        self.n_real_tokens = n_real_tokens\n        self.fcm_min_ratio = fcm_min_ratio\n        self.fcm_max_ratio = fcm_max_ratio\n        self.causal = causal\n        self.attn_type = attn_type\n        self.q_chunk_size = q_chunk_size\n        self.k_chunk_size = k_chunk_size\n        self.scan_layers = scan_layers\n        self.param_scan_axis = param_scan_axis\n        self.float32_logits = float32_logits\n        if self.n_real_tokens is None:\n            self.n_real_tokens = self.vocab_size\n\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n\n        super().__init__(\n            bos_token_id=bos_token_id, eos_token_id=eos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n        )\n\n    @classmethod\n    def get_default_config(cls, updates=None):\n        none_arg_types = dict(\n            n_inner=int,\n            rotary_dim=int,\n        )\n        config = function_args_to_config(cls.__init__, none_arg_types=none_arg_types)\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n\n        return config\n\n    @staticmethod\n    def get_jax_mesh(axis_dims):\n        return get_jax_mesh(axis_dims, ('dp', 'fsdp', 'mp'))\n\n    @staticmethod\n    def get_partition_rules(scan_layers=False):\n        \"\"\" Parition rules for GPT. Note that these rules are orderd, so that\n            the beginning rules match first. It is important to use\n            PartitionSpec() instead of None here because JAX does not treat\n            None as a pytree leaf.\n        \"\"\"\n        if scan_layers:\n            return (\n                ('transformer/wte/embedding', PS('mp', 'fsdp')),\n                ('attn/(k_proj|q_proj|v_proj)/kernel', PS(None, 'fsdp', 'mp')),\n                ('attn/out_proj/kernel', PS(None, 'mp', 'fsdp')),\n                ('attn/fc_in/kernel', PS(None, 'fsdp', 'mp')),\n                ('attn/fc_in/bias', PS(None, 'mp')),\n                ('attn/fc_out/kernel', PS(None, 'mp', 'fsdp')),\n                ('attn/fc_out/bias', PS(None, None)),\n                ('ln_[0-9]+/bias', PS(None, None)),\n                ('[0-9]+/ln_[0-9]+/scale', PS(None, None)),\n                ('ln_f/bias', PS(None)),\n                ('ln_f/scale', PS(None)),\n                ('lm_head/kernel', PS('fsdp', 'mp')),\n                ('lm_head/bias', PS('mp')),\n                ('.*', PS(None)),\n            )\n        else:\n            return (\n                ('transformer/wte/embedding', PS('mp', 'fsdp')),\n                ('attn/(k_proj|q_proj|v_proj)/kernel', PS('fsdp', 'mp')),\n                ('attn/out_proj/kernel', PS('mp', 'fsdp')),\n                ('attn/fc_in/kernel', PS('fsdp', 'mp')),\n                ('attn/fc_in/bias', PS('mp')),\n                ('attn/fc_out/kernel', PS('mp', 'fsdp')),\n                ('attn/fc_out/bias', PS(None)),\n                ('ln_[0-9]+/bias', PS(None)),\n                ('[0-9]+/ln_[0-9]+/scale', PS(None)),\n                ('ln_f/bias', PS(None)),\n                ('ln_f/scale', PS(None)),\n                ('lm_head/kernel', PS('fsdp', 'mp')),\n                ('lm_head/bias', PS('mp')),\n                ('.*', PS(None)),\n            )\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return (\n            'ln_[0-9]+/bias', 'ln_[0-9]+/scale', 'ln_f/bias', 'ln_f/scale',\n            'bias'\n        )\n\n    @staticmethod\n    def rng_keys():\n        return ('params', 'dropout', 'fcm')\n\n    @staticmethod\n    def get_tokenizer_config(updates=None):\n        config = ConfigDict()\n        config.name = 'EleutherAI/gpt-j-6B'\n        config.bos_token = '<|endoftext|>'\n        config.eos_token = '<|endoftext|>'\n        config.pad_token = '<|extratoken_40|>'\n        config.cls_token = '<|extratoken_41|>'\n        config.mask_token = '<|extratoken_42|>'\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n\n        return config\n\n    @classmethod\n    def get_tokenizer(cls, config, padding_side='left', truncation_side='right'):\n        config = cls.get_tokenizer_config(config)\n        return AutoTokenizer.from_pretrained(\n            config.name,\n            bos_token=config.bos_token,\n            eos_token=config.eos_token,\n            pad_token=config.pad_token,\n            cls_token=config.cls_token,\n            mask_token=config.mask_token,\n            padding_side=padding_side,\n            truncation_side=truncation_side,\n        )\n\n    @staticmethod\n    def load_pretrained(name, dtype=jnp.float32):\n        with jax.default_device(jax.devices(\"cpu\")[0]):\n            params = FlaxGPTForCausalLM.from_pretrained(\n                name, _do_init=False, dtype=dtype\n            )[1]\n            params = freeze({'params': params})\n        return jax.device_get(params)\n\n    @classmethod\n    def load_config(cls, path):\n        if path in GPT_STANDARD_CONFIGS:\n            return cls.from_dict(GPT_STANDARD_CONFIGS[path])\n        load_type, load_path = path.split('::', 1)\n        if load_type == 'pickle':\n            return cls.from_dict(load_pickle(load_path)['gpt_config'])\n        elif load_type == 'json':\n            with open_file(load_path, 'r') as fin:\n                raw_config = fin.read()\n            return cls.from_dict(json.loads(raw_config))\n        elif load_type == 'huggingface':\n            return cls.from_pretrained(load_path)\n        else:\n            raise ValueError(f'Unsupported load config type: {load_type}')", "\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"gpt\"\n_CONFIG_FOR_DOC = \"GPTConfig\"\n\nGPT_START_DOCSTRING = r\"\"\"\n    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads", "    This model inherits from [`FlaxPreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n    This model is also a Flax Linen\n    [flax.nn.Module](https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html) subclass. Use it as a\n    regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.\n    Finally, this model supports inherent JAX features such as:\n    - [Just-In-Time (JIT) compilation](https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit)\n    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)", "    - [Automatic Differentiation](https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation)\n    - [Vectorization](https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap)\n    - [Parallelization](https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap)\n    Parameters:\n        config ([`GPTConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~FlaxPreTrainedModel.from_pretrained`] method to load the model weights.\n        dtype (`jax.numpy.dtype`, *optional*, defaults to `jax.numpy.float32`):\n            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n            `jax.numpy.bfloat16` (on TPUs).", "            The data type of the computation. Can be one of `jax.numpy.float32`, `jax.numpy.float16` (on GPUs) and\n            `jax.numpy.bfloat16` (on TPUs).\n            This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If\n            specified all the computation will be performed with the given `dtype`.\n            **Note that this only specifies the dtype of the computation and does not influence the dtype of model\n            parameters.**\n            If you wish to change the dtype of the model parameters, see [`~FlaxPreTrainedModel.to_fp16`] and\n            [`~FlaxPreTrainedModel.to_bf16`].\n\"\"\"\n", "\"\"\"\n\nGPT_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`numpy.ndarray` of shape `(batch_size, input_ids_length)`):\n            `input_ids_length` = `sequence_length`. Indices of input sequence tokens in the vocabulary.\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):", "            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n            [What are attention masks?](../glossary#attention-mask)\n        position_ids (`numpy.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.max_position_embeddings - 1]`.\n        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):", "            config.max_position_embeddings - 1]`.\n        past_key_values (`Dict[str, np.ndarray]`, *optional*, returned by `init_cache` or when passing previous `past_key_values`):\n            Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast\n            auto-regressive decoding. Pre-computed key and value hidden-states are of shape *[batch_size, max_length]*.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.", "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)", "def create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)", "\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)", "def apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass FlaxGPTBlock(nn.Module):\n    config: GPTConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        hidden_size = self.config.hidden_size\n        inner_dim = self.config.n_inner if self.config.n_inner is not None else 4 * hidden_size\n\n        attention_blocks = {\n            # default vanilla transformer (Vaswani et al).\n            'vanilla': VanillaAttentionBlock,\n            # default memory efficient transformer (Rabe et al and Dao et al).\n            'memeff': MemEffAttentionBlock,\n            # default blockwise parallel transformer (Liu et al).\n            'blockwise_parallel': BPAttentionBlock,\n            # less cleaner blockwise parallel transformer used in the paper.\n            'blockwise_parallel_v1': BPAttentionBlock_v1,\n        }\n\n        if self.config.attn_type in attention_blocks:\n            Block = attention_blocks[self.config.attn_type]\n        else:\n            raise ValueError(f\"Unknown attention type {self.config.attn_type}\")\n\n        self.attn = Block(\n            self.config.q_chunk_size,\n            self.config.k_chunk_size,\n            self.config.hidden_size,\n            self.config.num_attention_heads,\n            self.config.rotary_dim,\n            inner_dim,\n            self.config.layer_norm_epsilon,\n            self.config.activation_function,\n            self.config.attn_pdrop,\n            self.config.resid_pdrop,\n            self.config.max_position_embeddings,\n            self.dtype,\n            self.config.causal,\n            policy=self.config.gradient_checkpointing,\n            prevent_cse=not self.config.scan_layers,\n            float32_logits=self.config.float32_logits,\n        )\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask=None,\n        position_ids=None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n    ):\n        attn_outputs = self.attn(\n            hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n        )\n        attn_weights = None\n        if self.config.scan_layers: # NOTE: this is a hack to work with scan_layers\n            outputs = attn_outputs, None\n        else:\n            outputs = (attn_outputs, attn_weights) if output_attentions else (attn_outputs,)\n        return outputs", "\n\nclass FlaxGPTPreTrainedModel(FlaxPreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = GPTConfig\n    base_model_prefix = \"transformer\"\n    module_class: nn.Module = None\n\n    def __init__(\n        self,\n        config: GPTConfig,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n    ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -> FrozenDict:\n        # init input tensors\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(input_shape + (self.config.n_embd,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n        r\"\"\"\n        Args:\n            batch_size (`int`):\n                batch_size used for fast auto-regressive decoding. Defines the batch size of the initialized cache.\n            max_length (`int`):\n                maximum possible length for auto-regressive decoding. Defines the sequence length of the initialized\n                cache.\n        \"\"\"\n        # init input variables to retrieve cache\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def _get_logits_processor(self,*args, **kwargs) -> FlaxLogitsProcessorList:\n        processors = super()._get_logits_processor(*args, **kwargs)\n        def squash_extra_tokens(input_ids, scores, cur_len):\n            return scores.at[:, self.config.n_real_tokens:].set(-float('inf'))\n\n        processors.append(squash_extra_tokens)\n        return processors\n\n    @add_start_docstrings_to_model_forward(GPT_INPUTS_DOCSTRING)\n    def __call__(\n        self,\n        input_ids,\n        attention_mask=None,\n        position_ids=None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ):\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        # Handle any PRNG if needed\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params}\n\n        # if past_key_values are passed then cache is already initialized a private flag init_cache has to be passed down to ensure cache is used. It has to be made sure that cache is marked as mutable so that it can be changed by FlaxGPTAttention module\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        # add updated cache to model output\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs", "\n\nclass FlaxGPTBlockCollection(nn.Module):\n    config: GPTConfig\n    dtype: jnp.dtype = jnp.float32\n\n    @nn.compact\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask=None,\n        position_ids=None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n\n        if not deterministic and self.config.fcm_max_ratio > 0:\n            # Apply forgetful causal mask\n            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n            fcm_ratio = jax.random.uniform(\n                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n                minval=self.config.fcm_min_ratio,\n                maxval=self.config.fcm_max_ratio\n            )\n            fcm_mask = jax.random.uniform(\n                self.make_rng('fcm'),\n                shape=(batch_size, 1, seq_length, seq_length)\n            ) > fcm_ratio\n            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n            fcm_mask = fcm_mask.astype('bool')\n        else:\n            fcm_mask = None\n\n        block = FlaxGPTBlock\n        if self.config.gradient_checkpointing != '':\n            FlaxGPT2CheckpointBlock = nn.remat(\n                block, static_argnums=(3, 4, 5, 6),\n                prevent_cse=not self.config.scan_layers,\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing)\n            )\n            block = FlaxGPT2CheckpointBlock\n        if self.config.scan_layers:\n            initializing = self.is_mutable_collection('params')\n            params_spec = (\n                self.config.param_scan_axis if initializing else\n                nn_partitioning.ScanIn(self.config.param_scan_axis))\n            cache_spec = 0\n            hidden_states, _ = nn.scan(\n                block,\n                variable_axes={\n                    'params': params_spec,\n                    'cache': cache_spec,\n                    'intermediates': 0\n                },\n                split_rngs={\n                    'params': True,\n                    'dropout': True\n                },\n                in_axes=(nn.broadcast, nn.broadcast, nn.broadcast, nn.broadcast, nn.broadcast, nn.broadcast),\n                length=self.config.num_hidden_layers,\n                metadata_params={nn.PARTITION_NAME: 'scan_decoder_layer'},\n                )(config=self.config, name='scan_decoder', dtype=self.dtype)(\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    deterministic,\n                    init_cache,\n                    output_attentions,\n                    fcm_mask,\n                )\n        else:\n            blocks = [\n                block(self.config, name=str(i), dtype=self.dtype) for i in range(self.config.num_hidden_layers)\n            ]\n            for block in blocks:\n                if output_hidden_states:\n                    all_hidden_states += (hidden_states,)\n\n                layer_outputs = block(\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    deterministic,\n                    init_cache,\n                    output_attentions,\n                    fcm_mask,\n                )\n                hidden_states = layer_outputs[0]\n\n                if output_attentions:\n                    all_attentions += (layer_outputs[1],)\n\n        # this contains possible `None` values - `FlaxGPTModule` will filter them out\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n\n        return outputs", "\n\nclass FlaxGPTModule(nn.Module):\n    config: GPTConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.embed_dim = self.config.hidden_size\n\n        self.wte = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n        )\n        self.dropout = nn.Dropout(rate=self.config.embd_pdrop)\n        self.h = FlaxGPTBlockCollection(self.config, dtype=self.dtype)\n        self.ln_f = nn.LayerNorm(epsilon=self.config.layer_norm_epsilon, dtype=self.dtype)\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        input_embeds = self.wte(input_ids.astype(\"i4\"))\n\n        hidden_states = self.dropout(input_embeds, deterministic=deterministic)\n\n        outputs = self.h(\n            hidden_states,\n            attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.ln_f(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )", "\n\n@add_start_docstrings(\n    \"The bare GPT Model transformer outputting raw hidden-states without any specific head on top.\",\n    GPT_START_DOCSTRING,\n)\nclass FlaxGPTModel(FlaxGPTPreTrainedModel):\n    module_class = FlaxGPTModule\n\n", "\n\nappend_call_sample_docstring(\n    FlaxGPTModel,\n    _CHECKPOINT_FOR_DOC,\n    FlaxCausalLMOutput,\n    _CONFIG_FOR_DOC,\n)\n\n\nclass FlaxGPTForCausalLMModule(nn.Module):\n    config: GPTConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.transformer = FlaxGPTModule(self.config, dtype=self.dtype)\n        if self.config.attn_type == 'blockwise_parallel' or self.config.attn_type == 'blockwise_parallel_v1':\n            self.lm_head = Blockwise_LM_Head(self.config.vocab_size,\n                                      self.config.q_chunk_size, dtype=self.dtype,\n                                      prevent_cse=not self.config.scan_layers)\n        else:\n            self.lm_head = nn.Dense(\n                self.config.vocab_size,\n                dtype=self.dtype,\n                kernel_init=jax.nn.initializers.variance_scaling(\n                    scale=1.0, mode='fan_in',\n                    distribution='normal',\n                )\n            )\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask=None,\n        position_ids=None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)", "\n\nclass FlaxGPTForCausalLMModule(nn.Module):\n    config: GPTConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.transformer = FlaxGPTModule(self.config, dtype=self.dtype)\n        if self.config.attn_type == 'blockwise_parallel' or self.config.attn_type == 'blockwise_parallel_v1':\n            self.lm_head = Blockwise_LM_Head(self.config.vocab_size,\n                                      self.config.q_chunk_size, dtype=self.dtype,\n                                      prevent_cse=not self.config.scan_layers)\n        else:\n            self.lm_head = nn.Dense(\n                self.config.vocab_size,\n                dtype=self.dtype,\n                kernel_init=jax.nn.initializers.variance_scaling(\n                    scale=1.0, mode='fan_in',\n                    distribution='normal',\n                )\n            )\n\n    def __call__(\n        self,\n        input_ids,\n        attention_mask=None,\n        position_ids=None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ):\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)", "\n\n@add_start_docstrings(\n    \"\"\"\n    The GPT Model transformer with a language modeling head on top.\n    \"\"\",\n    GPT_START_DOCSTRING,\n)\nclass FlaxGPTForCausalLM(FlaxGPTPreTrainedModel):\n    module_class = FlaxGPTForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jnp.DeviceArray] = None):\n        # initializing the cache\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        # Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.\n        # But since GPT uses a causal mask, those positions are masked anyways.\n        # Thus we can create a single static attention_mask here, which is more efficient for compilation\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs", "class FlaxGPTForCausalLM(FlaxGPTPreTrainedModel):\n    module_class = FlaxGPTForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[jnp.DeviceArray] = None):\n        # initializing the cache\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        # Note that usually one would have to put 0's in the attention_mask for x > input_ids.shape[-1] and x < cache_length.\n        # But since GPT uses a causal mask, those positions are masked anyways.\n        # Thus we can create a single static attention_mask here, which is more efficient for compilation\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs", "\n\nappend_call_sample_docstring(\n    FlaxGPTForCausalLM,\n    _CHECKPOINT_FOR_DOC,\n    FlaxCausalLMOutput,\n    _CONFIG_FOR_DOC,\n)\n", ""]}
{"filename": "blockwise-parallel-transformer/bpt/train.py", "chunked_list": ["import dataclasses\nimport pprint\nfrom functools import partial\nimport re\n\nfrom tqdm import tqdm, trange\nimport numpy as np\nimport bpt.tools.utils as utils\n\nimport jax", "\nimport jax\nimport jax.numpy as jnp\nfrom jax.experimental.pjit import pjit\nfrom jax.sharding import PartitionSpec as PS\nimport flax\nfrom flax import linen as nn\nfrom flax.jax_utils import prefetch_to_device\nfrom flax.training.train_state import TrainState\nimport optax", "from flax.training.train_state import TrainState\nimport optax\n\nfrom bpt.data import Dataset, TextProcessor\nfrom bpt.tools.checkpoint import StreamingCheckpointer\nfrom bpt.tools.optimizers import OptimizerFactory\nfrom bpt.tools.jax_utils import (\n    JaxRNG, next_rng, match_partition_rules,\n    cross_entropy_loss_and_accuracy, named_tree_map, global_norm,\n    set_random_seed, average_metrics, get_weight_decay_mask,", "    cross_entropy_loss_and_accuracy, named_tree_map, global_norm,\n    set_random_seed, average_metrics, get_weight_decay_mask,\n    make_shard_and_gather_fns, with_sharding_constraint, tree_apply, get_metrics,\n)\nfrom bpt.model import GPTConfig, FlaxGPTForCausalLMModule\nfrom bpt.blocks.blockwise_parallel import blockwise_cross_entropy\n\n\nFLAGS, FLAGS_DEF = utils.define_flags_with_default(\n    seed=42,", "FLAGS, FLAGS_DEF = utils.define_flags_with_default(\n    seed=42,\n    initialize_jax_distributed=False,\n    mesh_dim='1,-1,1',\n    total_steps=10000,\n    load_gpt_config='',\n    update_gpt_config='',\n    load_checkpoint='',\n    load_dataset_state='',\n    log_freq=50,", "    load_dataset_state='',\n    log_freq=50,\n    save_model_freq=0,\n    save_milestone_freq=0,\n    eval_steps=0,\n    tokenizer=GPTConfig.get_tokenizer_config(),\n    text_processor=TextProcessor.get_default_config(),\n    train_dataset=Dataset.get_default_config(),\n    eval_dataset=Dataset.get_default_config(),\n    optimizer=OptimizerFactory.get_default_config(),", "    eval_dataset=Dataset.get_default_config(),\n    optimizer=OptimizerFactory.get_default_config(),\n    checkpointer=StreamingCheckpointer.get_default_config(),\n    gpt=GPTConfig.get_default_config(),\n    logger=utils.WandBLogger.get_default_config(),\n    log_all_worker=False,\n    profile_steps=0,\n    stop_after_profile=True,\n)\n", ")\n\n\ndef main(argv):\n    if FLAGS.initialize_jax_distributed:\n        jax.distributed.initialize()\n\n    variant = utils.get_user_flags(FLAGS, FLAGS_DEF)\n    flags_config_dict = utils.user_flags_to_config_dict(FLAGS, FLAGS_DEF)\n    logger = utils.WandBLogger(\n        config=FLAGS.logger,\n        variant=variant,\n        enable=FLAGS.log_all_worker or (jax.process_index() == 0),\n    )\n    set_random_seed(FLAGS.seed)\n\n    if FLAGS.load_dataset_state != '':\n        dataset = utils.load_pickle(FLAGS.load_dataset_state)\n    else:\n        tokenizer = GPTConfig.get_tokenizer(FLAGS.tokenizer)\n        text_processor = TextProcessor(FLAGS.text_processor, tokenizer)\n        dataset = Dataset(FLAGS.train_dataset, tokenizer, text_processor)\n\n    if FLAGS.eval_steps > 0:\n        eval_dataset = Dataset(\n            FLAGS.eval_dataset, dataset.tokenizer, dataset.text_processor,\n        )\n        eval_iterator = iter(eval_dataset.val_iter())\n\n    seq_length = dataset.seq_length\n\n    if FLAGS.load_gpt_config != '':\n        gpt_config = GPTConfig.load_config(FLAGS.load_gpt_config)\n        update_gpt_config = GPTConfig(**FLAGS.gpt)\n        gpt_config.update(dict(\n            q_chunk_size=update_gpt_config.q_chunk_size,\n            k_chunk_size=update_gpt_config.k_chunk_size,\n            attn_type=update_gpt_config.attn_type,\n            n_positions=update_gpt_config.n_positions,\n            gradient_checkpointing=update_gpt_config.gradient_checkpointing,\n            scan_layers=update_gpt_config.scan_layers,\n            param_scan_axis=update_gpt_config.param_scan_axis,\n        ))\n    else:\n        gpt_config = GPTConfig(**FLAGS.gpt)\n\n    if FLAGS.update_gpt_config != '':\n        gpt_config.update(dict(eval(FLAGS.update_gpt_config)))\n\n    gpt_config.update(dict(\n        bos_token_id=dataset.tokenizer.bos_token_id,\n        eos_token_id=dataset.tokenizer.eos_token_id,\n    ))\n    if gpt_config.vocab_size < dataset.vocab_size:\n        gpt_config.update(dict(vocab_size=dataset.vocab_size))\n    model = FlaxGPTForCausalLMModule(gpt_config)\n\n    optimizer, optimizer_info = OptimizerFactory.get_optimizer(\n        FLAGS.optimizer,\n        get_weight_decay_mask(GPTConfig.get_weight_decay_exclusions()),\n    )\n\n    def create_trainstate_from_params(params):\n        return TrainState.create(params=params, tx=optimizer, apply_fn=None)\n\n    def init_fn(rng):\n        rng_generator = JaxRNG(rng)\n        params = model.init(\n            input_ids=jnp.zeros((4, seq_length), dtype=jnp.int32),\n            position_ids=jnp.zeros((4, seq_length), dtype=jnp.int32),\n            attention_mask=jnp.ones((4, seq_length), dtype=jnp.int32),\n            rngs=rng_generator(gpt_config.rng_keys()),\n        )\n        return TrainState.create(params=params, tx=optimizer, apply_fn=None)\n\n    if FLAGS.gpt.attn_type == 'blockwise_parallel' or FLAGS.gpt.attn_type == 'blockwise_parallel_v1':\n        cross_entropy_loss_and_accuracy_fn = partial(blockwise_cross_entropy,\n                                                     policy=FLAGS.gpt.gradient_checkpointing,\n                                                     chunk_size=FLAGS.gpt.q_chunk_size,\n                                                     prevent_cse=not FLAGS.gpt.scan_layers,)\n    else:\n        cross_entropy_loss_and_accuracy_fn = cross_entropy_loss_and_accuracy\n\n    def train_step(train_state, rng, batch):\n        rng_generator = JaxRNG(rng)\n        input_tokens = with_sharding_constraint(batch['input_tokens'], PS(('dp', 'fsdp')))\n        output_tokens = with_sharding_constraint(batch['output_tokens'], PS(('dp', 'fsdp')))\n        loss_masks = with_sharding_constraint(batch['loss_masks'], PS(('dp', 'fsdp')))\n        def loss_and_accuracy(params):\n            logits = model.apply(\n                params,\n                input_tokens,\n                deterministic=False,\n                rngs=rng_generator(gpt_config.rng_keys()),\n            ).logits\n            return cross_entropy_loss_and_accuracy_fn(logits, output_tokens, loss_masks)\n\n        grad_fn = jax.value_and_grad(loss_and_accuracy, has_aux=True)\n        (loss, accuracy), grads = grad_fn(train_state.params)\n        train_state = train_state.apply_gradients(grads=grads)\n        metrics = dict(\n            loss=loss,\n            accuracy=accuracy,\n            learning_rate=optimizer_info['learning_rate_schedule'](train_state.step),\n            gradient_norm=global_norm(grads),\n            param_norm=global_norm(train_state.params),\n        )\n        return train_state, rng_generator(), metrics\n\n    def eval_step(train_state, rng, batch):\n        rng_generator = JaxRNG(rng)\n        input_tokens = with_sharding_constraint(batch['input_tokens'], PS(('dp', 'fsdp')))\n        output_tokens = with_sharding_constraint(batch['output_tokens'], PS(('dp', 'fsdp')))\n        loss_masks = with_sharding_constraint(batch['loss_masks'], PS(('dp', 'fsdp')))\n        logits = model.apply(\n            train_state.params,\n            input_tokens,\n            deterministic=True,\n            rngs=rng_generator(gpt_config.rng_keys()),\n        ).logits\n        loss, accuracy = cross_entropy_loss_and_accuracy_fn(logits, output_tokens, loss_masks)\n        metrics = dict(\n            loss=loss,\n            accuracy=accuracy,\n        )\n        return rng_generator(), metrics\n\n    train_state_shapes = jax.eval_shape(init_fn, next_rng())\n    train_state_partition = match_partition_rules(\n        GPTConfig.get_partition_rules(FLAGS.gpt.scan_layers), train_state_shapes\n    )\n\n    num_params = sum(x.size for x in jax.tree_leaves(train_state_shapes.params))\n    num_nonembed_params = num_params - gpt_config.vocab_size * gpt_config.n_embd\n    param_stats = {\"num_params\": num_params,\"num_nonembed_params\": num_nonembed_params}\n    logger.log(param_stats)\n    tqdm.write(\"\\n\" + pprint.pformat(param_stats) + \"\\n\")\n\n    shard_fns, gather_fns = make_shard_and_gather_fns(\n        train_state_partition, train_state_shapes\n    )\n    checkpointer = StreamingCheckpointer(\n        FLAGS.checkpointer, logger.output_dir,\n        enable=jax.process_index() == 0,\n    )\n\n    sharded_init_fn = pjit(\n        init_fn,\n        in_shardings=PS(),\n        out_shardings=train_state_partition\n    )\n\n    sharded_create_trainstate_from_params = pjit(\n        create_trainstate_from_params,\n        in_shardings=(train_state_partition.params, ),\n        out_shardings=train_state_partition,\n        donate_argnums=(0, ),\n    )\n\n    sharded_train_step = pjit(\n        train_step,\n        in_shardings=(train_state_partition, PS(), PS()),\n        out_shardings=(train_state_partition, PS(), PS()),\n        donate_argnums=(0, 1),\n    )\n\n    sharded_eval_step = pjit(\n        eval_step,\n        in_shardings=(train_state_partition, PS(), PS()),\n        out_shardings=(PS(), PS()),\n        donate_argnums=(1,),\n    )\n\n    def save_checkpoint(train_state, milestone=False):\n        step = int(jax.device_get(train_state.step))\n        metadata = dict(\n            step=step,\n            variant=variant,\n            flags=flags_config_dict,\n            gpt_config=gpt_config.to_dict(),\n        )\n        checkpointer.save_all(\n            train_state=train_state,\n            gather_fns=gather_fns,\n            metadata=metadata,\n            dataset=dataset.get_state_dict(),\n            milestone=milestone,\n        )\n\n    if FLAGS.profile_steps > 0:\n        import os\n        os.makedirs(logger.profile_dir, exist_ok=True)\n        mesh = GPTConfig.get_jax_mesh(FLAGS.mesh_dim)\n        with mesh:\n            train_state, restored_params = None, None\n            if train_state is None and restored_params is None:\n                # Initialize from scratch\n                train_state = sharded_init_fn(next_rng())\n            elif train_state is None and restored_params is not None:\n                # Restore from params but initialize train_state\n                train_state = sharded_create_trainstate_from_params(restored_params)\n                del restored_params\n            sharded_rng = next_rng()\n            # warmup\n            for batch, dataset_metrics in dataset:\n                train_state, sharded_rng, metrics = sharded_train_step(\n                    train_state, sharded_rng, batch\n                )\n                break\n            # profile\n            jax.profiler.start_trace(logger.profile_dir)\n            for step, (batch, dataset_metrics) in zip(trange(FLAGS.profile_steps), dataset):\n                train_state, sharded_rng, metrics = sharded_train_step(\n                    train_state, sharded_rng, batch\n                )\n                jax.block_until_ready(train_state)\n                jax.profiler.save_device_memory_profile(f'{logger.profile_dir}/memory{step}.prof')\n            jax.profiler.stop_trace()\n        if FLAGS.stop_after_profile:\n            exit()\n\n    mesh = GPTConfig.get_jax_mesh(FLAGS.mesh_dim)\n    with mesh:\n        train_state, restored_params = None, None\n        if FLAGS.load_checkpoint != '':\n            load_type, load_path = FLAGS.load_checkpoint.split('::', 1)\n            if load_type == 'huggingface':\n                restored_params = tree_apply(\n                    shard_fns.params, gpt_config.load_pretrained(load_path)\n                )\n                train_state = None\n            else:\n                train_state, restored_params = checkpointer.load_trainstate_checkpoint(\n                    FLAGS.load_checkpoint, train_state_shapes, shard_fns\n                )\n\n        if train_state is None and restored_params is None:\n            # Initialize from scratch\n            train_state = sharded_init_fn(next_rng())\n        elif train_state is None and restored_params is not None:\n            # Restore from params but initialize train_state\n            train_state = sharded_create_trainstate_from_params(restored_params)\n            del restored_params\n\n        start_step = int(jax.device_get(train_state.step))\n\n        if FLAGS.save_model_freq > 0:\n            save_checkpoint(train_state)\n\n        sharded_rng = next_rng()\n\n        step_counter = trange(start_step, FLAGS.total_steps, ncols=0)\n\n        def run_eval(sharded_rng, eval_fn, batch, eval_steps, eval_name):\n            eval_metric_list = []\n            for _ in range(eval_steps):\n                sharded_rng, eval_metrics = eval_fn(\n                    train_state, sharded_rng, batch\n                )\n                eval_metric_list.append(eval_metrics)\n            log_metrics = get_metrics(eval_metric_list, stack=True)\n            mean_metrics = {\n                f\"{eval_name}/{k}\": np.mean(v)\n                for k, v in log_metrics.items()\n            }\n            mean_metrics[\"step\"] = step\n            logger.log(mean_metrics)\n            tqdm.write(\"\\n\" + pprint.pformat(mean_metrics) + \"\\n\")\n            return sharded_rng\n\n        for step, (batch, dataset_metrics) in zip(step_counter, dataset):\n            train_state, sharded_rng, metrics = sharded_train_step(\n                train_state, sharded_rng, batch\n            )\n\n            if step % FLAGS.log_freq == 0:\n                if FLAGS.eval_steps > 0:\n                    batch, _ = next(eval_iterator)\n                    sharded_rng = run_eval(sharded_rng, sharded_eval_step,\n                                           batch, FLAGS.eval_steps, \"val\")\n                log_metrics = {\"step\": step}\n                log_metrics.update(metrics)\n                log_metrics.update(dataset_metrics)\n                log_metrics = jax.device_get(log_metrics)\n                logger.log(log_metrics)\n                tqdm.write(\"\\n\" + pprint.pformat(log_metrics) + \"\\n\")\n\n            if FLAGS.save_milestone_freq > 0 and (step + 1) % FLAGS.save_milestone_freq == 0:\n                save_checkpoint(train_state, milestone=True)\n            elif FLAGS.save_model_freq > 0 and (step + 1) % FLAGS.save_model_freq == 0:\n                save_checkpoint(train_state)\n\n        if FLAGS.save_model_freq > 0:\n            save_checkpoint(train_state)", "\n\nif __name__ == \"__main__\":\n    utils.run(main)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/__init__.py", "chunked_list": [""]}
{"filename": "blockwise-parallel-transformer/bpt/data.py", "chunked_list": ["import dataclasses\nimport pprint\nimport time\nfrom functools import partial\nimport json\nfrom multiprocessing import Pool\n\nimport h5py\nimport bpt.tools.utils as utils\nfrom ml_collections.config_dict import config_dict", "import bpt.tools.utils as utils\nfrom ml_collections.config_dict import config_dict\nfrom ml_collections import ConfigDict\nfrom tqdm import tqdm, trange\nimport numpy as np\n\nfrom datasets import load_dataset\n\n\nclass TextProcessor(object):\n    \"\"\" Example processor that converts a dictionary of texts into tokens. \"\"\"\n\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.fields_from_example = ''\n        config.fields = ''\n        config.subfield_separator = ' '\n        config.add_eos_token = True\n        config.prepend_text = ''\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    def __init__(self, config, tokenizer):\n        self.config = self.get_default_config(config)\n        assert self.config.fields != '' or self.config.fields_from_example != '', (\n            'Either fields or fields_from_example must be specified.'\n        )\n        self.tokenizer = tokenizer\n\n    def __call__(self, example, has_aux=False):\n        if has_aux:\n            example, *aux = example\n        else:\n            aux = tuple()\n        token_buffer = []\n        loss_mask_buffer = []\n        if self.config.fields_from_example != '':\n            fields = example[self.config.fields_from_example].split(',')\n        else:\n            fields = self.config.fields.split(',')\n\n        for i, field in enumerate(fields):\n            if field.startswith('[') and field.endswith(']'):\n                # No loss for this field.\n                field = field[1:-1]\n                mask = 0.0\n            else:\n                mask = 1.0\n\n            if field == '<|bos|>':\n                token_buffer.append(self.tokenizer.bos_token_id)\n                loss_mask_buffer.append(mask)\n            elif field == '<|eos|>':\n                token_buffer.append(self.tokenizer.eos_token_id)\n                loss_mask_buffer.append(mask)\n            else:\n                subfields = field.split('+')\n                text = self.config.subfield_separator.join(\n                    [example[subfield] for subfield in subfields]\n                )\n                if i == 0:\n                    text = self.config.prepend_text + text\n                tokens = self.tokenizer.encode(text)\n                token_buffer.extend(tokens)\n                loss_mask_buffer.extend([mask for _ in range(len(tokens))])\n\n        if self.config.add_eos_token:\n            token_buffer.append(self.tokenizer.eos_token_id)\n            loss_mask_buffer.append(1.0)\n\n        return token_buffer, loss_mask_buffer, *aux", "\nclass TextProcessor(object):\n    \"\"\" Example processor that converts a dictionary of texts into tokens. \"\"\"\n\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.fields_from_example = ''\n        config.fields = ''\n        config.subfield_separator = ' '\n        config.add_eos_token = True\n        config.prepend_text = ''\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    def __init__(self, config, tokenizer):\n        self.config = self.get_default_config(config)\n        assert self.config.fields != '' or self.config.fields_from_example != '', (\n            'Either fields or fields_from_example must be specified.'\n        )\n        self.tokenizer = tokenizer\n\n    def __call__(self, example, has_aux=False):\n        if has_aux:\n            example, *aux = example\n        else:\n            aux = tuple()\n        token_buffer = []\n        loss_mask_buffer = []\n        if self.config.fields_from_example != '':\n            fields = example[self.config.fields_from_example].split(',')\n        else:\n            fields = self.config.fields.split(',')\n\n        for i, field in enumerate(fields):\n            if field.startswith('[') and field.endswith(']'):\n                # No loss for this field.\n                field = field[1:-1]\n                mask = 0.0\n            else:\n                mask = 1.0\n\n            if field == '<|bos|>':\n                token_buffer.append(self.tokenizer.bos_token_id)\n                loss_mask_buffer.append(mask)\n            elif field == '<|eos|>':\n                token_buffer.append(self.tokenizer.eos_token_id)\n                loss_mask_buffer.append(mask)\n            else:\n                subfields = field.split('+')\n                text = self.config.subfield_separator.join(\n                    [example[subfield] for subfield in subfields]\n                )\n                if i == 0:\n                    text = self.config.prepend_text + text\n                tokens = self.tokenizer.encode(text)\n                token_buffer.extend(tokens)\n                loss_mask_buffer.extend([mask for _ in range(len(tokens))])\n\n        if self.config.add_eos_token:\n            token_buffer.append(self.tokenizer.eos_token_id)\n            loss_mask_buffer.append(1.0)\n\n        return token_buffer, loss_mask_buffer, *aux", "\n\nclass Dataset(object):\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.path = ''\n        config.seq_length = 1024\n        config.batch_size = 8\n        config.start_seek_loc = 0\n        config.index_at_start = 0\n        config.tokenizer_processes = 1\n        config.tokenizer_parallel_chunk_size = 32\n        config.tokenizer_parallel_batch_size = 1024\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    def __init__(self, config, tokenizer, text_processor):\n        self.config = self.get_default_config(config)\n        assert self.config.path != ''\n        self._tokenizer = tokenizer\n        self._text_processor = text_processor\n        self._index = self.config.index_at_start\n        self._file_loc = self.config.start_seek_loc\n        self._n_batch = 0\n\n    def parse_json(self, line):\n        if not line or line == '\\n':\n            return None\n        try:\n            data = json.loads(line)\n        except json.decoder.JSONDecodeError:\n            print(f'Error parsing json line:\\n{line}')\n            return None\n        return data\n\n    def json_iterator(self):\n        with utils.open_file(self.config.path, 'r') as fin:\n            fin.seek(self._file_loc)\n            while True:\n                line = fin.readline()\n                self._file_loc = fin.tell()\n                if not line:   # Reached EOF\n                    self._index = 0\n                    fin.seek(0)\n                    continue\n\n                data = self.parse_json(line)\n                if data is not None:\n                    # JSON parsing succeeded\n                    yield data, self._file_loc, self._index\n                self._index += 1\n\n    def batched(self, iterator, batch_size):\n        batch = []\n        for example in iterator:\n            batch.append(example)\n            if len(batch) == batch_size:\n                yield batch\n                batch = []\n        if len(batch) > 0:\n            yield batch\n\n    def parallel_example_iterator(self):\n        if self.config.tokenizer_processes == 1:\n            for example, loc, index in self.json_iterator():\n                yield self.text_processor((example, loc, index), has_aux=True)\n        else:\n            process_pool = Pool(self.config.tokenizer_processes)\n            batched_iterator = self.batched(\n                self.json_iterator(), self.config.tokenizer_parallel_batch_size\n            )\n            with process_pool as pool:\n                map_fn = partial(self.text_processor, has_aux=True)\n                next_batch = pool.map_async(\n                    map_fn, next(batched_iterator),\n                    chunksize=self.config.tokenizer_parallel_chunk_size\n                )\n                while True:\n                    current_batch = next_batch\n                    next_batch = pool.map_async(\n                        map_fn, next(batched_iterator),\n                        chunksize=self.config.tokenizer_parallel_chunk_size\n                    )\n                    for example in current_batch.get():\n                        yield example\n\n    def __iter__(self):\n        chunk_size = self.config.batch_size * self.config.seq_length\n        token_buffer = []\n        loss_mask_buffer = []\n        total_tokens = 0\n        last_time = 0.0\n        for tokens, loss_masks, loc, index in self.parallel_example_iterator():\n            token_buffer.extend(tokens)\n            loss_mask_buffer.extend(loss_masks)\n            while len(token_buffer) > chunk_size + 1:\n                total_tokens += chunk_size\n                metrics = {\n                    'dataset_file_loc': loc,\n                    'dataset_example_index': index,\n                    'dataset_total_tokens': total_tokens,\n                    'dataset_throughput_tps': chunk_size / (time.time() - last_time),\n                }\n                last_time = time.time()\n                input_tokens = np.array(token_buffer[:chunk_size], dtype=np.int32)\n                output_tokens = np.array(token_buffer[1:chunk_size+1], dtype=np.int32)\n                # reshape to batch_size x seq_length\n                input_tokens = input_tokens.reshape(self.config.batch_size, -1)\n                output_tokens = output_tokens.reshape(self.config.batch_size, -1)\n                loss_masks = np.array(loss_mask_buffer[:chunk_size], dtype=np.float32).reshape(self.config.batch_size, -1)\n                yield {\n                    \"input_tokens\": input_tokens,\n                    \"output_tokens\": output_tokens,\n                    \"loss_masks\": loss_masks,\n                }, metrics\n                token_buffer = token_buffer[chunk_size:]\n                loss_mask_buffer = loss_mask_buffer[chunk_size:]\n\n    def val_iter(self):\n        chunk_size = self.config.batch_size * self.config.seq_length\n        token_buffer = []\n        loss_mask_buffer = []\n        total_tokens = 0\n        last_time = 0.0\n        for tokens, loss_masks, loc, index in self.parallel_example_iterator():\n            token_buffer.extend(tokens)\n            loss_mask_buffer.extend(loss_masks)\n            while len(token_buffer) > chunk_size + 1:\n                total_tokens += chunk_size\n                metrics = {\n                    'dataset_file_loc': loc,\n                    'dataset_example_index': index,\n                    'dataset_total_tokens': total_tokens,\n                    'dataset_throughput_tps': chunk_size / (time.time() - last_time),\n                }\n                last_time = time.time()\n                input_tokens = np.array(token_buffer[:chunk_size], dtype=np.int32)\n                output_tokens = np.array(token_buffer[1:chunk_size+1], dtype=np.int32)\n                # reshape to batch_size x seq_length\n                input_tokens = input_tokens.reshape(self.config.batch_size, -1)\n                output_tokens = output_tokens.reshape(self.config.batch_size, -1)\n                loss_masks = np.array(loss_mask_buffer[:chunk_size], dtype=np.float32).reshape(self.config.batch_size, -1)\n                yield {\n                    \"input_tokens\": input_tokens,\n                    \"output_tokens\": output_tokens,\n                    \"loss_masks\": loss_masks,\n                }, metrics\n                token_buffer = token_buffer[chunk_size:]\n                loss_mask_buffer = loss_mask_buffer[chunk_size:]\n\n    def get_state_dict(self):\n        return dict(\n            config=self.config,\n            index=self._index,\n            file_loc=self._file_loc,\n        )\n\n    def load_state_dict(self, state_dict):\n        self.config = state_dict.get('config', self.config)\n        self._index = state_dict.get('index', self.config.index_at_start)\n        self._file_loc = state_dict.get('file_loc', self.config.start_seek_loc)\n\n    @property\n    def seq_length(self):\n        return self.config.seq_length\n\n    @property\n    def tokenizer(self):\n        return self._tokenizer\n\n    @property\n    def text_processor(self):\n        return self._text_processor\n\n    @property\n    def vocab_size(self):\n        return len(self.tokenizer)", ""]}
{"filename": "blockwise-parallel-transformer/bpt/tools/checkpoint.py", "chunked_list": ["import os\nimport numpy as np\nfrom ml_collections import ConfigDict\nimport bpt.tools.utils as utils\nimport jax\nimport jax.numpy as jnp\nimport flax\nfrom flax.serialization import (\n    from_bytes, to_bytes, to_state_dict, from_state_dict\n)", "    from_bytes, to_bytes, to_state_dict, from_state_dict\n)\nfrom flax.traverse_util import flatten_dict, unflatten_dict, empty_node\nimport msgpack\n\nfrom bpt.tools.jax_utils import tree_apply, float_tensor_to_dtype\n\n\nclass StreamingCheckpointer(object):\n    \"\"\" Custom msgpack checkpointer that saves large train states by serializing\n        and saving tensors one by one in a streaming fashion. Avoids running\n        out of memory or local TPU disk with default flax checkpointer.\n    \"\"\"\n\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.float_dtype = 'bf16'\n        config.save_optimizer_state = False\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    def __init__(self, config, checkpoint_dir, enable=True):\n        self.config = self.get_default_config(config)\n        self.checkpoint_dir = checkpoint_dir\n        self.enable = enable\n\n    def save_checkpoint(self, train_state, filename, gather_fns=None):\n        if self.enable:\n            path = os.path.join(self.checkpoint_dir, filename)\n        else:\n            path = '/dev/null'\n        self.save_train_state_to_file(\n            train_state, path, gather_fns, self.config.float_dtype\n        )\n\n    @staticmethod\n    def save_train_state_to_file(train_state, path, gather_fns=None, float_dtype=None):\n        train_state = to_state_dict(train_state)\n        packer = msgpack.Packer()\n        flattend_train_state = flatten_dict(train_state)\n        if gather_fns is not None:\n            gather_fns = flatten_dict(to_state_dict(gather_fns))\n\n        with utils.open_file(path, \"wb\") as fout:\n            for key, value in flattend_train_state.items():\n                if gather_fns is not None:\n                    value = gather_fns[key](value)\n                value = float_tensor_to_dtype(value, float_dtype)\n                fout.write(packer.pack((key, to_bytes(value))))\n\n    def save_pickle(self, obj, filename):\n        if self.enable:\n            path = os.path.join(self.checkpoint_dir, filename)\n        else:\n            path = '/dev/null'\n        utils.save_pickle(obj, path)\n\n    def save_all(self, train_state, gather_fns, metadata=None, dataset=None, milestone=False):\n        step = int(jax.device_get(train_state.step))\n        if self.config.save_optimizer_state:\n            checkpoint_state = train_state\n            checkpoint_name = 'streaming_train_state'\n            checkpoint_gather_fns = gather_fns\n        else:\n            checkpoint_state = train_state.params['params']\n            checkpoint_name = 'streaming_params'\n            checkpoint_gather_fns = gather_fns.params['params']\n\n        if milestone:\n            # Save a milestone checkpoint that will not be overwritten\n            self.save_pickle(metadata, f'metadata_{step}.pkl')\n            self.save_pickle(dataset, f'dataset_{step}.pkl')\n            self.save_checkpoint(\n                checkpoint_state, f'{checkpoint_name}_{step}', checkpoint_gather_fns\n            )\n        else:\n            # Save a normal checkpoint that can be overwritten\n            self.save_pickle(metadata, 'metadata.pkl')\n            self.save_pickle(dataset, 'dataset.pkl')\n            self.save_checkpoint(\n                checkpoint_state, f'{checkpoint_name}', checkpoint_gather_fns\n            )\n\n    @staticmethod\n    def load_checkpoint(path, target=None, shard_fns=None, remove_dict_prefix=None):\n        if shard_fns is not None:\n            shard_fns = flatten_dict(\n                to_state_dict(shard_fns)\n            )\n        if remove_dict_prefix is not None:\n            remove_dict_prefix = tuple(remove_dict_prefix)\n        flattend_train_state = {}\n        with utils.open_file(path) as fin:\n            # 83886080 bytes = 80 MB, which is 16 blocks on GCS\n            unpacker = msgpack.Unpacker(fin, read_size=83886080, max_buffer_size=0)\n            for key, value in unpacker:\n                key = tuple(key)\n                if remove_dict_prefix is not None:\n                    if key[:len(remove_dict_prefix)] == remove_dict_prefix:\n                        key = key[len(remove_dict_prefix):]\n                    else:\n                        continue\n\n                tensor = from_bytes(None, value)\n                if shard_fns is not None:\n                    tensor = shard_fns[key](tensor)\n                flattend_train_state[key] = tensor\n\n        if target is not None:\n            flattened_target = flatten_dict(\n                to_state_dict(target), keep_empty_nodes=True\n            )\n            for key, value in flattened_target.items():\n                if key not in flattend_train_state and value == empty_node:\n                    flattend_train_state[key] = value\n\n        train_state = unflatten_dict(flattend_train_state)\n        if target is None:\n            return train_state\n\n        return from_state_dict(target, train_state)\n\n    @staticmethod\n    def load_flax_checkpoint(path, target=None, shard_fns=None):\n        \"\"\" Load a standard flax checkpoint that's not saved with the\n            msgpack streaming format.\n        \"\"\"\n        with utils.open_file(path, \"rb\") as fin:\n            encoded_bytes = fin.read()\n\n        state_dict = flax.serialization.msgpack_restore(encoded_bytes)\n        if shard_fns is not None:\n            shard_fns = to_state_dict(shard_fns)\n            state_dict = tree_apply(shard_fns, state_dict)\n\n        if target is None:\n            return state_dict\n        return from_state_dict(target, state_dict)\n\n    @classmethod\n    def load_trainstate_checkpoint(cls, load_from, trainstate_target=None,\n                                   trainstate_shard_fns=None,\n                                   disallow_trainstate=False):\n        if trainstate_target is not None:\n            params_target = trainstate_target.params['params']\n        else:\n            params_target = None\n\n        if trainstate_shard_fns is not None:\n            params_shard_fns = trainstate_shard_fns.params['params']\n        else:\n            params_shard_fns = None\n\n        load_type, load_path = load_from.split('::', 1)\n        if disallow_trainstate:\n            assert load_type != 'trainstate', 'Loading full trainstate is not allowed!'\n        train_state = None\n        restored_params = None\n        if load_type == 'trainstate':\n            # Load the entire train state in the streaming format\n            train_state = cls.load_checkpoint(\n                path=load_path,\n                target=trainstate_target,\n                shard_fns=trainstate_shard_fns,\n            )\n        elif load_type == 'trainstate_params':\n            # Load the params part of the train state in the streaming format\n            restored_params = cls.load_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns,\n                remove_dict_prefix=('params', 'params'),\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {'params': restored_params}\n            )\n        elif load_type == 'params':\n            # Load the params in the streaming format\n            restored_params = cls.load_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns,\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {'params': restored_params}\n            )\n        elif load_type == 'flax_params':\n            # Load the params in the standard flax format (non-streaming)\n            # This requires the entire params to fit in memory\n            restored_params = cls.load_flax_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {'params': restored_params}\n            )\n        else:\n            raise ValueError(f'Invalid load_from type: {load_type}')\n\n        return train_state, restored_params", "class StreamingCheckpointer(object):\n    \"\"\" Custom msgpack checkpointer that saves large train states by serializing\n        and saving tensors one by one in a streaming fashion. Avoids running\n        out of memory or local TPU disk with default flax checkpointer.\n    \"\"\"\n\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.float_dtype = 'bf16'\n        config.save_optimizer_state = False\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    def __init__(self, config, checkpoint_dir, enable=True):\n        self.config = self.get_default_config(config)\n        self.checkpoint_dir = checkpoint_dir\n        self.enable = enable\n\n    def save_checkpoint(self, train_state, filename, gather_fns=None):\n        if self.enable:\n            path = os.path.join(self.checkpoint_dir, filename)\n        else:\n            path = '/dev/null'\n        self.save_train_state_to_file(\n            train_state, path, gather_fns, self.config.float_dtype\n        )\n\n    @staticmethod\n    def save_train_state_to_file(train_state, path, gather_fns=None, float_dtype=None):\n        train_state = to_state_dict(train_state)\n        packer = msgpack.Packer()\n        flattend_train_state = flatten_dict(train_state)\n        if gather_fns is not None:\n            gather_fns = flatten_dict(to_state_dict(gather_fns))\n\n        with utils.open_file(path, \"wb\") as fout:\n            for key, value in flattend_train_state.items():\n                if gather_fns is not None:\n                    value = gather_fns[key](value)\n                value = float_tensor_to_dtype(value, float_dtype)\n                fout.write(packer.pack((key, to_bytes(value))))\n\n    def save_pickle(self, obj, filename):\n        if self.enable:\n            path = os.path.join(self.checkpoint_dir, filename)\n        else:\n            path = '/dev/null'\n        utils.save_pickle(obj, path)\n\n    def save_all(self, train_state, gather_fns, metadata=None, dataset=None, milestone=False):\n        step = int(jax.device_get(train_state.step))\n        if self.config.save_optimizer_state:\n            checkpoint_state = train_state\n            checkpoint_name = 'streaming_train_state'\n            checkpoint_gather_fns = gather_fns\n        else:\n            checkpoint_state = train_state.params['params']\n            checkpoint_name = 'streaming_params'\n            checkpoint_gather_fns = gather_fns.params['params']\n\n        if milestone:\n            # Save a milestone checkpoint that will not be overwritten\n            self.save_pickle(metadata, f'metadata_{step}.pkl')\n            self.save_pickle(dataset, f'dataset_{step}.pkl')\n            self.save_checkpoint(\n                checkpoint_state, f'{checkpoint_name}_{step}', checkpoint_gather_fns\n            )\n        else:\n            # Save a normal checkpoint that can be overwritten\n            self.save_pickle(metadata, 'metadata.pkl')\n            self.save_pickle(dataset, 'dataset.pkl')\n            self.save_checkpoint(\n                checkpoint_state, f'{checkpoint_name}', checkpoint_gather_fns\n            )\n\n    @staticmethod\n    def load_checkpoint(path, target=None, shard_fns=None, remove_dict_prefix=None):\n        if shard_fns is not None:\n            shard_fns = flatten_dict(\n                to_state_dict(shard_fns)\n            )\n        if remove_dict_prefix is not None:\n            remove_dict_prefix = tuple(remove_dict_prefix)\n        flattend_train_state = {}\n        with utils.open_file(path) as fin:\n            # 83886080 bytes = 80 MB, which is 16 blocks on GCS\n            unpacker = msgpack.Unpacker(fin, read_size=83886080, max_buffer_size=0)\n            for key, value in unpacker:\n                key = tuple(key)\n                if remove_dict_prefix is not None:\n                    if key[:len(remove_dict_prefix)] == remove_dict_prefix:\n                        key = key[len(remove_dict_prefix):]\n                    else:\n                        continue\n\n                tensor = from_bytes(None, value)\n                if shard_fns is not None:\n                    tensor = shard_fns[key](tensor)\n                flattend_train_state[key] = tensor\n\n        if target is not None:\n            flattened_target = flatten_dict(\n                to_state_dict(target), keep_empty_nodes=True\n            )\n            for key, value in flattened_target.items():\n                if key not in flattend_train_state and value == empty_node:\n                    flattend_train_state[key] = value\n\n        train_state = unflatten_dict(flattend_train_state)\n        if target is None:\n            return train_state\n\n        return from_state_dict(target, train_state)\n\n    @staticmethod\n    def load_flax_checkpoint(path, target=None, shard_fns=None):\n        \"\"\" Load a standard flax checkpoint that's not saved with the\n            msgpack streaming format.\n        \"\"\"\n        with utils.open_file(path, \"rb\") as fin:\n            encoded_bytes = fin.read()\n\n        state_dict = flax.serialization.msgpack_restore(encoded_bytes)\n        if shard_fns is not None:\n            shard_fns = to_state_dict(shard_fns)\n            state_dict = tree_apply(shard_fns, state_dict)\n\n        if target is None:\n            return state_dict\n        return from_state_dict(target, state_dict)\n\n    @classmethod\n    def load_trainstate_checkpoint(cls, load_from, trainstate_target=None,\n                                   trainstate_shard_fns=None,\n                                   disallow_trainstate=False):\n        if trainstate_target is not None:\n            params_target = trainstate_target.params['params']\n        else:\n            params_target = None\n\n        if trainstate_shard_fns is not None:\n            params_shard_fns = trainstate_shard_fns.params['params']\n        else:\n            params_shard_fns = None\n\n        load_type, load_path = load_from.split('::', 1)\n        if disallow_trainstate:\n            assert load_type != 'trainstate', 'Loading full trainstate is not allowed!'\n        train_state = None\n        restored_params = None\n        if load_type == 'trainstate':\n            # Load the entire train state in the streaming format\n            train_state = cls.load_checkpoint(\n                path=load_path,\n                target=trainstate_target,\n                shard_fns=trainstate_shard_fns,\n            )\n        elif load_type == 'trainstate_params':\n            # Load the params part of the train state in the streaming format\n            restored_params = cls.load_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns,\n                remove_dict_prefix=('params', 'params'),\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {'params': restored_params}\n            )\n        elif load_type == 'params':\n            # Load the params in the streaming format\n            restored_params = cls.load_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns,\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {'params': restored_params}\n            )\n        elif load_type == 'flax_params':\n            # Load the params in the standard flax format (non-streaming)\n            # This requires the entire params to fit in memory\n            restored_params = cls.load_flax_checkpoint(\n                path=load_path,\n                target=params_target,\n                shard_fns=params_shard_fns\n            )\n            restored_params = flax.core.frozen_dict.freeze(\n                {'params': restored_params}\n            )\n        else:\n            raise ValueError(f'Invalid load_from type: {load_type}')\n\n        return train_state, restored_params", ""]}
{"filename": "blockwise-parallel-transformer/bpt/tools/__init__.py", "chunked_list": [""]}
{"filename": "blockwise-parallel-transformer/bpt/tools/utils.py", "chunked_list": ["import inspect\nimport logging\nimport os\nimport pprint\nimport random\nimport tempfile\nimport time\nimport uuid\nfrom concurrent.futures import ThreadPoolExecutor\nfrom copy import copy", "from concurrent.futures import ThreadPoolExecutor\nfrom copy import copy\nfrom io import BytesIO\nfrom socket import gethostname\nimport dataclasses\n\nimport absl.flags\nimport absl.logging\nimport cloudpickle as pickle\nimport flax", "import cloudpickle as pickle\nimport flax\nimport gcsfs\nimport jax\nimport jax.numpy as jnp\nimport msgpack\nimport numpy as np\nimport wandb\nfrom flax.serialization import from_bytes, to_bytes\nfrom ml_collections import ConfigDict", "from flax.serialization import from_bytes, to_bytes\nfrom ml_collections import ConfigDict\nfrom ml_collections.config_dict.config_dict import placeholder\nfrom ml_collections.config_flags import config_flags\nfrom flax.training.train_state import TrainState\nfrom flax.core import FrozenDict\nfrom absl.app import run\n\n\nclass WandBLogger(object):\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.project_id = \"\"\n        config.project_entity = placeholder(str)\n        config.experiment_id = placeholder(str)\n        config.append_uuid = True\n        config.experiment_note = placeholder(str)\n\n        config.output_dir = \"/tmp/\"\n        config.wandb_dir = \"\"\n        config.profile_dir = \"\"\n\n        config.online = False\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    def __init__(self, config, variant, enable=True):\n        self.enable = enable\n        self.config = self.get_default_config(config)\n\n        if self.config.experiment_id is None or self.config.experiment_id == \"\":\n            self.config.experiment_id = uuid.uuid4().hex\n        else:\n            if self.config.append_uuid:\n                self.config.experiment_id = (\n                    str(self.config.experiment_id) + \"_\" + uuid.uuid4().hex\n                )\n            else:\n                self.config.experiment_id = str(self.config.experiment_id)\n\n        if self.enable:\n            if self.config.output_dir == \"\":\n                self.config.output_dir = tempfile.mkdtemp()\n            else:\n                self.config.output_dir = os.path.join(\n                    self.config.output_dir, self.config.experiment_id\n                )\n                if not self.config.output_dir.startswith(\"gs://\"):\n                    os.makedirs(self.config.output_dir, exist_ok=True)\n\n            if self.config.wandb_dir == \"\":\n                if not self.config.output_dir.startswith(\"gs://\"):\n                    # Use the same directory as output_dir if it is not a GCS path.\n                    self.config.wandb_dir = self.config.output_dir\n                else:\n                    # Otherwise, use a temporary directory.\n                    self.config.wandb_dir = tempfile.mkdtemp()\n            else:\n                # Join the wandb_dir with the experiment_id.\n                self.config.wandb_dir = os.path.join(\n                    self.config.wandb_dir, self.config.experiment_id\n                )\n                os.makedirs(self.config.wandb_dir, exist_ok=True)\n\n            if self.config.profile_dir == \"\":\n                if not self.config.output_dir.startswith(\"gs://\"):\n                    # Use the same directory as output_dir if it is not a GCS path.\n                    self.config.profile_dir = self.config.output_dir\n                else:\n                    # Otherwise, use a temporary directory.\n                    self.config.profile_dir = tempfile.mkdtemp()\n            else:\n                # Join the profile_dir with the experiment_id.\n                self.config.profile_dir = os.path.join(\n                    self.config.profile_dir, self.config.experiment_id\n                )\n                os.makedirs(self.config.profile_dir, exist_ok=True)\n\n        self._variant = flatten_config_dict(variant)\n\n        if \"hostname\" not in self._variant:\n            self._variant[\"hostname\"] = gethostname()\n\n        if self.enable:\n            self.run = wandb.init(\n                reinit=True,\n                config=self._variant,\n                project=self.config.project_id,\n                dir=self.config.wandb_dir,\n                id=self.config.experiment_id,\n                resume=\"allow\",\n                notes=self.config.experiment_note,\n                entity=self.config.project_entity,\n                settings=wandb.Settings(\n                    start_method=\"thread\",\n                    _disable_stats=True,\n                ),\n                mode=\"online\" if self.config.online else \"offline\",\n            )\n        else:\n            self.run = None\n\n    def log(self, *args, **kwargs):\n        if self.enable:\n            self.run.log(*args, **kwargs)\n\n    def save_pickle(self, obj, filename):\n        if self.enable:\n            save_pickle(obj, os.path.join(self.config.output_dir, filename))\n\n    @property\n    def experiment_id(self):\n        return self.config.experiment_id\n\n    @property\n    def variant(self):\n        return self.config.variant\n\n    @property\n    def output_dir(self):\n        return self.config.output_dir\n\n    @property\n    def wandb_dir(self):\n        return self.config.wandb_dir\n\n    @property\n    def profile_dir(self):\n        return self.config.profile_dir", "\nclass WandBLogger(object):\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.project_id = \"\"\n        config.project_entity = placeholder(str)\n        config.experiment_id = placeholder(str)\n        config.append_uuid = True\n        config.experiment_note = placeholder(str)\n\n        config.output_dir = \"/tmp/\"\n        config.wandb_dir = \"\"\n        config.profile_dir = \"\"\n\n        config.online = False\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    def __init__(self, config, variant, enable=True):\n        self.enable = enable\n        self.config = self.get_default_config(config)\n\n        if self.config.experiment_id is None or self.config.experiment_id == \"\":\n            self.config.experiment_id = uuid.uuid4().hex\n        else:\n            if self.config.append_uuid:\n                self.config.experiment_id = (\n                    str(self.config.experiment_id) + \"_\" + uuid.uuid4().hex\n                )\n            else:\n                self.config.experiment_id = str(self.config.experiment_id)\n\n        if self.enable:\n            if self.config.output_dir == \"\":\n                self.config.output_dir = tempfile.mkdtemp()\n            else:\n                self.config.output_dir = os.path.join(\n                    self.config.output_dir, self.config.experiment_id\n                )\n                if not self.config.output_dir.startswith(\"gs://\"):\n                    os.makedirs(self.config.output_dir, exist_ok=True)\n\n            if self.config.wandb_dir == \"\":\n                if not self.config.output_dir.startswith(\"gs://\"):\n                    # Use the same directory as output_dir if it is not a GCS path.\n                    self.config.wandb_dir = self.config.output_dir\n                else:\n                    # Otherwise, use a temporary directory.\n                    self.config.wandb_dir = tempfile.mkdtemp()\n            else:\n                # Join the wandb_dir with the experiment_id.\n                self.config.wandb_dir = os.path.join(\n                    self.config.wandb_dir, self.config.experiment_id\n                )\n                os.makedirs(self.config.wandb_dir, exist_ok=True)\n\n            if self.config.profile_dir == \"\":\n                if not self.config.output_dir.startswith(\"gs://\"):\n                    # Use the same directory as output_dir if it is not a GCS path.\n                    self.config.profile_dir = self.config.output_dir\n                else:\n                    # Otherwise, use a temporary directory.\n                    self.config.profile_dir = tempfile.mkdtemp()\n            else:\n                # Join the profile_dir with the experiment_id.\n                self.config.profile_dir = os.path.join(\n                    self.config.profile_dir, self.config.experiment_id\n                )\n                os.makedirs(self.config.profile_dir, exist_ok=True)\n\n        self._variant = flatten_config_dict(variant)\n\n        if \"hostname\" not in self._variant:\n            self._variant[\"hostname\"] = gethostname()\n\n        if self.enable:\n            self.run = wandb.init(\n                reinit=True,\n                config=self._variant,\n                project=self.config.project_id,\n                dir=self.config.wandb_dir,\n                id=self.config.experiment_id,\n                resume=\"allow\",\n                notes=self.config.experiment_note,\n                entity=self.config.project_entity,\n                settings=wandb.Settings(\n                    start_method=\"thread\",\n                    _disable_stats=True,\n                ),\n                mode=\"online\" if self.config.online else \"offline\",\n            )\n        else:\n            self.run = None\n\n    def log(self, *args, **kwargs):\n        if self.enable:\n            self.run.log(*args, **kwargs)\n\n    def save_pickle(self, obj, filename):\n        if self.enable:\n            save_pickle(obj, os.path.join(self.config.output_dir, filename))\n\n    @property\n    def experiment_id(self):\n        return self.config.experiment_id\n\n    @property\n    def variant(self):\n        return self.config.variant\n\n    @property\n    def output_dir(self):\n        return self.config.output_dir\n\n    @property\n    def wandb_dir(self):\n        return self.config.wandb_dir\n\n    @property\n    def profile_dir(self):\n        return self.config.profile_dir", "\ndef config_dict(*args, **kwargs):\n    return ConfigDict(dict(*args, **kwargs))\n\n\ndef define_flags_with_default(**kwargs):\n    for key, val in kwargs.items():\n        if isinstance(val, tuple):\n            val, help_str = val\n        else:\n            help_str = \"\"\n\n        if isinstance(val, ConfigDict):\n            config_flags.DEFINE_config_dict(key, val)\n        elif isinstance(val, bool):\n            # Note that True and False are instances of int.\n            absl.flags.DEFINE_bool(key, val, help_str)\n        elif isinstance(val, int):\n            absl.flags.DEFINE_integer(key, val, help_str)\n        elif isinstance(val, float):\n            absl.flags.DEFINE_float(key, val, help_str)\n        elif isinstance(val, str):\n            absl.flags.DEFINE_string(key, val, help_str)\n        else:\n            raise ValueError(\"Incorrect value type\")\n    return absl.flags.FLAGS, kwargs", "\n\ndef print_flags(flags, flags_def):\n    flag_srings = [\n        \"{}: {}\".format(key, val)\n        for key, val in get_user_flags(flags, flags_def).items()\n    ]\n    logging.info(\n        \"Hyperparameter configs: \\n{}\".format(\n            pprint.pformat(flag_srings)\n        )\n    )", "\n\ndef get_user_flags(flags, flags_def):\n    output = {}\n    for key in flags_def:\n        val = getattr(flags, key)\n        if isinstance(val, ConfigDict):\n            output.update(flatten_config_dict(val, prefix=key))\n        else:\n            output[key] = val\n\n    return output", "\n\ndef user_flags_to_config_dict(flags, flags_def):\n    output = ConfigDict()\n    for key in flags_def:\n        output[key] = getattr(flags, key)\n\n    return output\n\n\ndef flatten_config_dict(config, prefix=None):\n    output = {}\n    for key, val in config.items():\n        if isinstance(val, ConfigDict) or isinstance(val, dict):\n            output.update(flatten_config_dict(val, prefix=key))\n        else:\n            if prefix is not None:\n                output[\"{}.{}\".format(prefix, key)] = val\n            else:\n                output[key] = val\n    return output", "\n\ndef flatten_config_dict(config, prefix=None):\n    output = {}\n    for key, val in config.items():\n        if isinstance(val, ConfigDict) or isinstance(val, dict):\n            output.update(flatten_config_dict(val, prefix=key))\n        else:\n            if prefix is not None:\n                output[\"{}.{}\".format(prefix, key)] = val\n            else:\n                output[key] = val\n    return output", "\n\ndef function_args_to_config(fn, none_arg_types=None, exclude_args=None, override_args=None):\n    config = ConfigDict()\n    arg_spec = inspect.getargspec(fn)\n    n_args = len(arg_spec.defaults)\n    arg_names = arg_spec.args[-n_args:]\n    default_values = arg_spec.defaults\n    for name, value in zip(arg_names, default_values):\n        if exclude_args is not None and name in exclude_args:\n            continue\n        elif override_args is not None and name in override_args:\n            config[name] = override_args[name]\n        elif none_arg_types is not None and value is None and name in none_arg_types:\n            config[name] = placeholder(none_arg_types[name])\n        else:\n            config[name] = value\n\n    return config", "\n\ndef prefix_metrics(metrics, prefix):\n    return {\"{}/{}\".format(prefix, key): value for key, value in metrics.items()}\n\n\ndef open_file(path, mode='rb', cache_type='readahead'):\n    if path.startswith(\"gs://\"):\n        logging.getLogger(\"fsspec\").setLevel(logging.WARNING)\n        return gcsfs.GCSFileSystem().open(path, mode, cache_type=cache_type)\n    else:\n        return open(path, mode)", "\n\ndef save_pickle(obj, path):\n    with open_file(path, \"wb\") as fout:\n        pickle.dump(obj, fout)\n\n\ndef load_pickle(path):\n    with open_file(path, \"rb\") as fin:\n        data = pickle.load(fin)\n    return data", "\n\ndef text_to_array(text, encoding=\"utf-8\"):\n    return np.frombuffer(text.encode(encoding), dtype=\"uint8\")\n\n\ndef array_to_text(array, encoding=\"utf-8\"):\n    with BytesIO(array) as fin:\n        text = fin.read().decode(encoding)\n    return text", "\n\nclass JaxRNG(object):\n    \"\"\" A convenient stateful Jax RNG wrapper. Can be used to wrap RNG inside\n        pure function.\n    \"\"\"\n\n    @classmethod\n    def from_seed(cls, seed):\n        return cls(jax.random.PRNGKey(seed))\n\n    def __init__(self, rng):\n        self.rng = rng\n\n    def __call__(self, keys=None):\n        if keys is None:\n            self.rng, split_rng = jax.random.split(self.rng)\n            return split_rng\n        elif isinstance(keys, int):\n            split_rngs = jax.random.split(self.rng, num=keys + 1)\n            self.rng = split_rngs[0]\n            return tuple(split_rngs[1:])\n        else:\n            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n            self.rng = split_rngs[0]\n            return {key: val for key, val in zip(keys, split_rngs[1:])}", "\n\ndef wrap_function_with_rng(rng):\n    \"\"\" To be used as decorator, automatically bookkeep a RNG for the wrapped function. \"\"\"\n    def wrap_function(function):\n        def wrapped(*args, **kwargs):\n            nonlocal rng\n            rng, split_rng = jax.random.split(rng)\n            return function(split_rng, *args, **kwargs)\n        return wrapped\n    return wrap_function", "\n\ndef init_rng(seed):\n    global jax_utils_rng\n    jax_utils_rng = JaxRNG.from_seed(seed)\n\n\ndef next_rng(*args, **kwargs):\n    global jax_utils_rng\n    return jax_utils_rng(*args, **kwargs)", "\n\ndef flatten_tree(xs, is_leaf=None, sep=None):\n    \"\"\" A stronger version of flax.traverse_util.flatten_dict, supports\n        dict, tuple, list and TrainState. Tuple and list indices will be\n        converted to strings.\n    \"\"\"\n    tree_node_classes = (FrozenDict, dict, tuple, list, TrainState)\n    if not isinstance(xs, tree_node_classes):\n        ValueError('fUnsupported node type: {type(xs)}')\n\n    def _is_leaf(prefix, fx):\n        if is_leaf is not None:\n            return is_leaf(prefix, xs)\n        return False\n\n    def _key(path):\n        if sep is None:\n            return path\n        return sep.join(path)\n\n    def _convert_to_dict(xs):\n        if isinstance(xs, (FrozenDict, dict)):\n            return xs\n        elif isinstance(xs, (tuple, list)):\n            return {f'{i}': v for i, v in enumerate(xs)}\n        elif isinstance(xs, TrainState):\n            output = {}\n            for field in dataclasses.fields(xs):\n                if 'pytree_node' not in field.metadata or field.metadata['pytree_node']:\n                    output[field.name] = getattr(xs, field.name)\n            return output\n        else:\n            raise ValueError('fUnsupported node type: {type(xs)}')\n\n    def _flatten(xs, prefix):\n        if not isinstance(xs, tree_node_classes) or _is_leaf(prefix, xs):\n            return {_key(prefix): xs}\n\n        result = {}\n        is_empty = True\n        for (key, value) in _convert_to_dict(xs).items():\n            is_empty = False\n            path = prefix + (key, )\n            result.update(_flatten(value, path))\n        return result\n\n    return _flatten(xs, ())", "\n\ndef named_tree_map(f, tree, is_leaf=None, sep=None):\n    \"\"\" An extended version of jax.tree_util.tree_map, where the mapped function\n        f takes both the name (path) and the tree leaf as input.\n    \"\"\"\n    flattened_tree = flatten_tree(tree, is_leaf=is_leaf, sep=sep)\n    id_to_name = {id(val): key for key, val in flattened_tree.items()}\n    def map_fn(leaf):\n        name = id_to_name[id(leaf)]\n        return f(name, leaf)\n    return jax.tree_util.tree_map(map_fn, tree)", "\n\ndef get_pytree_shape_info(tree):\n    flattend_tree = flatten_tree(tree, sep='/')\n    shapes = []\n    for key in sorted(list(flattend_tree.keys())):\n        val = flattend_tree[key]\n        shapes.append(f'{key}: {val.dtype}, {val.shape}')\n    return '\\n'.join(shapes)\n", "\n\ndef collect_metrics(metrics, names, prefix=None):\n    collected = {}\n    for name in names:\n        if name in metrics:\n            collected[name] = jnp.mean(metrics[name])\n    if prefix is not None:\n        collected = {\n            '{}/{}'.format(prefix, key): value for key, value in collected.items()\n        }\n    return collected", "\n\ndef set_random_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    init_rng(seed)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/tools/jax_utils.py", "chunked_list": ["import os\nimport math\nfrom typing import Any, Mapping, Text, Tuple, Union, NamedTuple\nfrom functools import partial\nimport re\nimport dataclasses\nimport random\n\nimport dill\nimport flax", "import dill\nimport flax\nimport jax\nimport jax.numpy as jnp\nfrom jax.sharding import PartitionSpec as PS\nfrom jax.sharding import Mesh\nfrom jax.experimental.pjit import with_sharding_constraint as _with_sharding_constraint\nfrom jax.experimental.pjit import pjit\nfrom jax.interpreters import pxla\nimport numpy as np", "from jax.interpreters import pxla\nimport numpy as np\nfrom absl import logging\nfrom flax import jax_utils\nfrom flax.training.train_state import TrainState\nfrom flax.core import FrozenDict\nimport optax\nfrom transformers import FlaxLogitsWarper\n\n\nclass JaxRNG(object):\n    \"\"\" A convenient stateful Jax RNG wrapper. Can be used to wrap RNG inside\n        pure function.\n    \"\"\"\n\n    @classmethod\n    def from_seed(cls, seed):\n        return cls(jax.random.PRNGKey(seed))\n\n    def __init__(self, rng):\n        self.rng = rng\n\n    def __call__(self, keys=None):\n        if keys is None:\n            self.rng, split_rng = jax.random.split(self.rng)\n            return split_rng\n        elif isinstance(keys, int):\n            split_rngs = jax.random.split(self.rng, num=keys + 1)\n            self.rng = split_rngs[0]\n            return tuple(split_rngs[1:])\n        else:\n            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n            self.rng = split_rngs[0]\n            return {key: val for key, val in zip(keys, split_rngs[1:])}", "\n\nclass JaxRNG(object):\n    \"\"\" A convenient stateful Jax RNG wrapper. Can be used to wrap RNG inside\n        pure function.\n    \"\"\"\n\n    @classmethod\n    def from_seed(cls, seed):\n        return cls(jax.random.PRNGKey(seed))\n\n    def __init__(self, rng):\n        self.rng = rng\n\n    def __call__(self, keys=None):\n        if keys is None:\n            self.rng, split_rng = jax.random.split(self.rng)\n            return split_rng\n        elif isinstance(keys, int):\n            split_rngs = jax.random.split(self.rng, num=keys + 1)\n            self.rng = split_rngs[0]\n            return tuple(split_rngs[1:])\n        else:\n            split_rngs = jax.random.split(self.rng, num=len(keys) + 1)\n            self.rng = split_rngs[0]\n            return {key: val for key, val in zip(keys, split_rngs[1:])}", "\n\nclass FlaxTemperatureLogitsWarper(FlaxLogitsWarper):\n    \"\"\" JIT traceable version of FlaxLogitsWarper that performs temperature scaling.\"\"\"\n    def __init__(self, temperature):\n        self.temperature = temperature\n\n    def __call__(self, input_ids, scores, cur_len):\n        return scores / jnp.clip(self.temperature, a_min=1e-8)\n", "\n\ndef make_shard_and_gather_fns(partition_specs, dtype_specs=None):\n    \"\"\" Create pytree of sharding and gathering functions from pytree of\n        partition specs.\n    \"\"\"\n    float_dtypes = (jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64)\n\n    def make_to_dtype_fn(dtype_spec):\n        def to_dtype(tensor):\n            if dtype_specs in float_dtypes and getattr(tensor, 'dtype', None) in float_dtypes:\n                # Convert all float tensors to the same dtype\n                return tensor.astype(dtype_specs)\n            elif hasattr(dtype_spec, 'dtype') and hasattr(tensor, 'dtype'):\n                return tensor.astype(dtype_spec.dtype)\n            return tensor\n        return to_dtype\n\n    def make_shard_fn(partition_spec, dtype_spec=None):\n        jax_shard_function = pjit(\n            make_to_dtype_fn(dtype_spec),\n            in_shardings=None,\n            out_shardings=partition_spec\n        )\n        def shard_fn(tensor):\n            return jax_shard_function(tensor).block_until_ready()\n        return shard_fn\n\n    def make_gather_fn(partition_spec, dtype_spec=None):\n        jax_gather_fn = pjit(\n            make_to_dtype_fn(dtype_spec),\n            in_shardings=partition_spec,\n            out_shardings=None\n        )\n        def gather_fn(tensor):\n            return jax.device_get(jax_gather_fn(tensor))\n        return gather_fn\n\n    if dtype_specs is None or dtype_specs in float_dtypes:\n        shard_fns = jax.tree_util.tree_map(make_shard_fn, partition_specs)\n        gather_fns = jax.tree_util.tree_map(make_gather_fn, partition_specs)\n    else:\n        shard_fns = jax.tree_util.tree_map(\n            make_shard_fn, partition_specs, dtype_specs\n        )\n        gather_fns = jax.tree_util.tree_map(\n            make_gather_fn, partition_specs, dtype_specs\n        )\n    return shard_fns, gather_fns", "\n\ndef set_random_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    init_rng(seed)\n\n\ndef get_jax_mesh(axis_dims, names):\n    if ':' in axis_dims:\n        dims = []\n        dim_names = []\n        for axis in axis_dims.split(','):\n            name, dim = axis.split(':')\n            assert name in names\n            dims.append(int(dim))\n            dim_names.append(name)\n        assert(set(dim_names) == set(names))\n    else:\n        dims = [int(x) for x in axis_dims.split(',')]\n        dim_names = names\n    assert len(dims) == len(names)\n    return Mesh(np.array(jax.devices()).reshape(dims), dim_names)", "def get_jax_mesh(axis_dims, names):\n    if ':' in axis_dims:\n        dims = []\n        dim_names = []\n        for axis in axis_dims.split(','):\n            name, dim = axis.split(':')\n            assert name in names\n            dims.append(int(dim))\n            dim_names.append(name)\n        assert(set(dim_names) == set(names))\n    else:\n        dims = [int(x) for x in axis_dims.split(',')]\n        dim_names = names\n    assert len(dims) == len(names)\n    return Mesh(np.array(jax.devices()).reshape(dims), dim_names)", "\n\ndef names_in_current_mesh(*names):\n    \"\"\" Check if current mesh axes contain these names. \"\"\"\n    mesh_axis_names = pxla.thread_resources.env.physical_mesh.axis_names\n    return set(names) <= set(mesh_axis_names)\n\n\ndef get_names_from_parition_spec(partition_specs):\n    \"\"\" Return axis names from partition specs. \"\"\"\n    names = set()\n    if isinstance(partition_specs, dict):\n        partition_specs = partition_specs.values()\n    for item in partition_specs:\n        if item is None:\n            continue\n        elif isinstance(item, str):\n            names.add(item)\n        else:\n            names.update(get_names_from_parition_spec(item))\n\n    return list(names)", "def get_names_from_parition_spec(partition_specs):\n    \"\"\" Return axis names from partition specs. \"\"\"\n    names = set()\n    if isinstance(partition_specs, dict):\n        partition_specs = partition_specs.values()\n    for item in partition_specs:\n        if item is None:\n            continue\n        elif isinstance(item, str):\n            names.add(item)\n        else:\n            names.update(get_names_from_parition_spec(item))\n\n    return list(names)", "\n\ndef with_sharding_constraint(x, partition_specs):\n    \"\"\" A smarter version of with_sharding_constraint that only applies the\n        constraint if the current mesh contains the axes in the partition specs.\n    \"\"\"\n    axis_names = get_names_from_parition_spec(partition_specs)\n    if names_in_current_mesh(*axis_names):\n        x = _with_sharding_constraint(x, partition_specs)\n    return x", "\n\ndef wrap_function_with_rng(rng):\n    \"\"\" To be used as decorator, automatically bookkeep a RNG for the wrapped function. \"\"\"\n    def wrap_function(function):\n        def wrapped(*args, **kwargs):\n            nonlocal rng\n            rng, split_rng = jax.random.split(rng)\n            return function(split_rng, *args, **kwargs)\n        return wrapped\n    return wrap_function", "\n\ndef init_rng(seed):\n    global jax_utils_rng\n    jax_utils_rng = JaxRNG.from_seed(seed)\n\n\ndef next_rng(*args, **kwargs):\n    global jax_utils_rng\n    return jax_utils_rng(*args, **kwargs)", "\n\ndef get_metrics(metrics, unreplicate=False, stack=False):\n    if unreplicate:\n        metrics = flax.jax_utils.unreplicate(metrics)\n    metrics = jax.device_get(metrics)\n    if stack:\n        return jax.tree_map(lambda *args: np.stack(args), *metrics)\n    else:\n        return {key: float(val) for key, val in metrics.items()}", "\n\ndef mse_loss(val, target, valid=None):\n    if valid is None:\n        valid = jnp.ones((*target.shape[:2], 1))\n    valid = valid.astype(jnp.float32)\n    loss = jnp.mean(\n        jnp.where(\n            valid > 0.0,\n            jnp.square(val - target),\n            0.0\n        )\n    )\n    return loss", "\n\ndef cross_entropy_loss(logits, labels, smoothing_factor=0.):\n    num_classes = logits.shape[-1]\n    if labels.dtype == jnp.int32 or labels.dtype == jnp.int64:\n        labels = jax.nn.one_hot(labels, num_classes)\n    if smoothing_factor > 0.:\n        labels = labels * (1. - smoothing_factor) + smoothing_factor / num_classes\n    logp = jax.nn.log_softmax(logits, axis=-1)\n    return -jnp.mean(jnp.sum(logp * labels, axis=-1))", "\n\ndef cross_entropy_loss_and_accuracy(logits, tokens, valid=None):\n    if valid is None:\n        valid = jnp.ones(tokens.shape[:2])\n    valid = valid.astype(jnp.float32)\n    valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n\n    token_log_prob = jnp.squeeze(\n        jnp.take_along_axis(\n            jax.nn.log_softmax(logits, axis=-1),\n            jnp.expand_dims(tokens, -1),\n            axis=-1,\n        ),\n        -1,\n    )\n    token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n    loss = -jnp.mean(jnp.sum(token_log_prob, axis=-1) / valid_text_length)\n    correct = jnp.where(\n        valid > 0.0,\n        jnp.argmax(logits, axis=-1) == tokens,\n        jnp.array(False)\n    )\n    accuracy = jnp.mean(jnp.sum(correct, axis=-1) / valid_text_length)\n    return loss, accuracy", "\n\ndef global_norm(tree):\n    \"\"\" Return the global L2 norm of a pytree. \"\"\"\n    squared = jax.tree_util.tree_map(lambda x: jnp.sum(jnp.square(x)), tree)\n    flattened, _ = jax.flatten_util.ravel_pytree(squared)\n    return jnp.sqrt(jnp.sum(flattened))\n\n\ndef average_metrics(metrics):\n    return jax.tree_map(\n        lambda *args: jnp.mean(jnp.stack(args)),\n        *metrics\n    )", "\ndef average_metrics(metrics):\n    return jax.tree_map(\n        lambda *args: jnp.mean(jnp.stack(args)),\n        *metrics\n    )\n\n\ndef get_float_dtype_by_name(dtype):\n    return {\n        'bf16': jnp.bfloat16,\n        'fp16': jnp.float16,\n        'fp32': jnp.float32,\n        'fp64': jnp.float64,\n    }[dtype]", "def get_float_dtype_by_name(dtype):\n    return {\n        'bf16': jnp.bfloat16,\n        'fp16': jnp.float16,\n        'fp32': jnp.float32,\n        'fp64': jnp.float64,\n    }[dtype]\n\n\ndef float_tensor_to_dtype(tensor, dtype):\n    if dtype is None or dtype == '':\n        return tensor\n    if isinstance(dtype, str):\n        dtype = get_float_dtype_by_name(dtype)\n    float_dtypes = (jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64)\n    if getattr(tensor, 'dtype', None) in float_dtypes:\n        tensor = tensor.astype(dtype)\n    return tensor", "\ndef float_tensor_to_dtype(tensor, dtype):\n    if dtype is None or dtype == '':\n        return tensor\n    if isinstance(dtype, str):\n        dtype = get_float_dtype_by_name(dtype)\n    float_dtypes = (jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64)\n    if getattr(tensor, 'dtype', None) in float_dtypes:\n        tensor = tensor.astype(dtype)\n    return tensor", "\n\ndef float_to_dtype(tree, dtype):\n    return jax.tree_util.tree_map(\n        partial(float_tensor_to_dtype, dtype=dtype), tree\n    )\n\n\ndef get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n    }[name]", "def get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n    }[name]\n\n\ndef tree_path_to_string(path, sep=None):\n    keys = []\n    for key in path:\n        if isinstance(key, jax.tree_util.SequenceKey):\n            keys.append(str(key.idx))\n        elif isinstance(key, jax.tree_util.DictKey):\n            keys.append(str(key.key))\n        elif isinstance(key, jax.tree_util.GetAttrKey):\n            keys.append(str(key.name))\n        elif isinstance(key, jax.tree_util.FlattenedIndexKey):\n            keys.append(str(key.key))\n        else:\n            keys.append(str(key))\n    if sep is None:\n        return tuple(keys)\n    return sep.join(keys)", "\ndef tree_path_to_string(path, sep=None):\n    keys = []\n    for key in path:\n        if isinstance(key, jax.tree_util.SequenceKey):\n            keys.append(str(key.idx))\n        elif isinstance(key, jax.tree_util.DictKey):\n            keys.append(str(key.key))\n        elif isinstance(key, jax.tree_util.GetAttrKey):\n            keys.append(str(key.name))\n        elif isinstance(key, jax.tree_util.FlattenedIndexKey):\n            keys.append(str(key.key))\n        else:\n            keys.append(str(key))\n    if sep is None:\n        return tuple(keys)\n    return sep.join(keys)", "\n\ndef flatten_tree(xs, is_leaf=None, sep=None):\n    flattened, _ = jax.tree_util.tree_flatten_with_path(xs, is_leaf=is_leaf)\n    output = {}\n    for key, val in flattened:\n        output[tree_path_to_string(key, sep=sep)] = val\n    return output\n\n\ndef named_tree_map(f, tree, *rest, is_leaf=None, sep=None):\n    \"\"\" An extended version of jax.tree_util.tree_map, where the mapped function\n        f takes both the name (path) and the tree leaf as input.\n    \"\"\"\n    return jax.tree_util.tree_map_with_path(\n        lambda path, x, *r: f(tree_path_to_string(path, sep=sep), x, *r),\n        tree, *rest,\n        is_leaf=is_leaf\n    )", "\n\ndef named_tree_map(f, tree, *rest, is_leaf=None, sep=None):\n    \"\"\" An extended version of jax.tree_util.tree_map, where the mapped function\n        f takes both the name (path) and the tree leaf as input.\n    \"\"\"\n    return jax.tree_util.tree_map_with_path(\n        lambda path, x, *r: f(tree_path_to_string(path, sep=sep), x, *r),\n        tree, *rest,\n        is_leaf=is_leaf\n    )", "\n\ndef match_partition_rules(rules, params):\n    \"\"\" Returns a pytree of PartitionSpec according to rules. Supports handling\n        Flax TrainState and Optax optimizer state.\n    \"\"\"\n    def get_partition_spec(name, leaf):\n        if len(leaf.shape) == 0 or np.prod(leaf.shape) == 1:\n            \"\"\" Don't partition scalar values. \"\"\"\n            return PS()\n        for rule, ps in rules:\n            if re.search(rule, name) is not None:\n                return ps\n        raise ValueError(f'Partition rule not found for param: {name}')\n    return named_tree_map(get_partition_spec, params, sep='/')", "\n\ndef get_weight_decay_mask(exclusions):\n    \"\"\" Return a weight decay mask function that computes the pytree masks\n        according to the given exclusion rules.\n    \"\"\"\n    def decay(name, _):\n        for rule in exclusions:\n            if re.search(rule, name) is not None:\n                return False\n        return True\n\n    def weight_decay_mask(params):\n        return named_tree_map(decay, params, sep='/')\n\n    return weight_decay_mask", "\n\ndef tree_apply(fns, tree):\n    \"\"\" Apply a pytree of functions to the pytree. \"\"\"\n    return jax.tree_util.tree_map(lambda fn, x: fn(x), fns, tree)\n"]}
{"filename": "blockwise-parallel-transformer/bpt/tools/optimizers.py", "chunked_list": ["import os\nimport time\nfrom typing import Any, Mapping, Text, Tuple, Union, NamedTuple\nfrom functools import partial\nimport re\nimport dataclasses\nimport random\n\nfrom ml_collections.config_dict import config_dict\nfrom ml_collections import ConfigDict", "from ml_collections.config_dict import config_dict\nfrom ml_collections import ConfigDict\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom absl import logging\nimport optax\n\nfrom bpt.tools.jax_utils import float_to_dtype\n", "from bpt.tools.jax_utils import float_to_dtype\n\n\nclass OptimizerFactory(object):\n    \"\"\" Configurable optax optimizer factory. \"\"\"\n\n    def __init__(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.accumulate_gradient_steps = 1\n        config.type = 'adamw'\n        config.palm_optimizer = PalmOptimizerFactory.get_default_config()\n        config.adamw_optimizer = AdamWOptimizerFactory.get_default_config()\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    @classmethod\n    def get_optimizer(cls, config, weight_decay_mask=None):\n        config = cls.get_default_config(config)\n        if config.type == 'palm':\n            optimizer, optimizer_info = PalmOptimizerFactory.get_optimizer(\n                config.palm_optimizer, weight_decay_mask\n            )\n        elif config.type == 'adamw':\n            optimizer, optimizer_info = AdamWOptimizerFactory.get_optimizer(\n                config.adamw_optimizer, weight_decay_mask\n            )\n        else:\n            raise ValueError(f'Unknown optimizer type: {config.type}')\n\n        if config.accumulate_gradient_steps > 1:\n            optimizer = optax.MultiSteps(\n                optimizer, config.accumulate_gradient_steps\n            )\n\n        return optimizer, optimizer_info", "\n\nclass PalmOptimizerFactory(object):\n    \"\"\" PaLM optimizer factory. This optimizer implements the optimizer\n        described in the PaLM paper: https://arxiv.org/abs/2204.02311\n    \"\"\"\n\n    def __init__(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.lr = 0.01\n        config.lr_warmup_steps = 10000\n        config.b1 = 0.9\n        config.b2 = 0.99\n        config.clip_gradient = 1.0\n        config.weight_decay = 1e-4\n        config.bf16_momentum = True\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    @classmethod\n    def get_optimizer(cls, config, weight_decay_mask=None):\n        config = cls.get_default_config(config)\n\n        def learning_rate_schedule(step):\n            multiplier = config.lr / 0.01\n            return multiplier / jnp.sqrt(jnp.maximum(step, config.lr_warmup_steps))\n\n        def weight_decay_schedule(step):\n            multiplier = config.weight_decay / 1e-4\n            return -multiplier * jnp.square(learning_rate_schedule(step))\n\n        optimizer_info = dict(\n            learning_rate_schedule=learning_rate_schedule,\n            weight_decay_schedule=weight_decay_schedule,\n        )\n\n        optimizer = optax.chain(\n            optax.clip_by_global_norm(config.clip_gradient),\n            optax.adafactor(\n                learning_rate=learning_rate_schedule,\n                multiply_by_parameter_scale=True,\n                momentum=config.b1,\n                decay_rate=config.b2,\n                factored=False,\n                clipping_threshold=None,\n                dtype_momentum=jnp.bfloat16 if config.bf16_momentum else jnp.float32,\n            ),\n            optax_add_scheduled_weight_decay(\n                weight_decay_schedule, weight_decay_mask\n            )\n        )\n        return optimizer, optimizer_info", "\n\nclass AdamWOptimizerFactory(object):\n    \"\"\" AdamW optimizer with cosine schedule. \"\"\"\n\n    def __init__(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_default_config(updates=None):\n        config = ConfigDict()\n        config.init_lr = 0.0\n        config.end_lr = 0.001\n        config.lr = 0.01\n        config.lr_warmup_steps = 2000\n        config.lr_decay_steps = 500000\n        config.b1 = 0.9\n        config.b2 = 0.95\n        config.clip_gradient = 1.0\n        config.weight_decay = 1e-4\n        config.bf16_momentum = True\n        config.multiply_by_parameter_scale = True\n\n        if updates is not None:\n            config.update(ConfigDict(updates).copy_and_resolve_references())\n        return config\n\n    @classmethod\n    def get_optimizer(cls, config, weight_decay_mask=None):\n        config = cls.get_default_config(config)\n\n        learning_rate_schedule = optax.warmup_cosine_decay_schedule(\n            init_value=config.init_lr,\n            peak_value=config.lr,\n            warmup_steps=config.lr_warmup_steps,\n            decay_steps=config.lr_decay_steps,\n            end_value=config.end_lr,\n        )\n\n        optimizer_info = dict(\n            learning_rate_schedule=learning_rate_schedule,\n        )\n\n        if config.multiply_by_parameter_scale:\n            optimizer = optax.chain(\n                optax.clip_by_global_norm(config.clip_gradient),\n                optax.adafactor(\n                    learning_rate=learning_rate_schedule,\n                    multiply_by_parameter_scale=True,\n                    momentum=config.b1,\n                    decay_rate=config.b2,\n                    factored=False,\n                    clipping_threshold=None,\n                    dtype_momentum=jnp.bfloat16 if config.bf16_momentum else jnp.float32,\n                ),\n                optax_add_scheduled_weight_decay(\n                    lambda step: -learning_rate_schedule(step) * config.weight_decay,\n                    weight_decay_mask\n                )\n            )\n        else:\n            optimizer = optax.chain(\n                optax.clip_by_global_norm(config.clip_gradient),\n                optax.adamw(\n                    learning_rate=learning_rate_schedule,\n                    weight_decay=config.weight_decay,\n                    b1=0.9,\n                    b2=0.95,\n                    mask=weight_decay_mask,\n                    mu_dtype=jnp.bfloat16 if config.bf16_momentum else jnp.float32,\n                ),\n            )\n\n        return optimizer, optimizer_info", "\n\nclass OptaxScheduledWeightDecayState(NamedTuple):\n    count: jnp.DeviceArray\n\n\ndef optax_add_scheduled_weight_decay(schedule_fn, mask=None):\n    \"\"\" Apply weight decay with schedule. \"\"\"\n\n    def init_fn(params):\n        del params\n        return OptaxScheduledWeightDecayState(count=jnp.zeros([], jnp.int32))\n\n    def update_fn(updates, state, params):\n        if params is None:\n            raise ValueError('Params cannot be None for weight decay!')\n\n        weight_decay = schedule_fn(state.count)\n        updates = jax.tree_util.tree_map(\n            lambda g, p: g + weight_decay * p, updates, params\n        )\n        return updates, OptaxScheduledWeightDecayState(\n            count=optax.safe_int32_increment(state.count)\n        )\n\n    if mask is not None:\n        return optax.masked(optax.GradientTransformation(init_fn, update_fn), mask)\n    return optax.GradientTransformation(init_fn, update_fn)", ""]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/vanilla.py", "chunked_list": ["import functools\nimport json\nimport math\nfrom functools import partial\nfrom typing import Optional, Tuple\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport numpy as np", "import jax.numpy as jnp\nimport numpy as np\nfrom einops import rearrange\nfrom flax.linen import combine_masks, make_causal_mask\nfrom jax import lax\nfrom jax import numpy as jnp\n\n\ndef quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)", "def quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)\n\nACT2FN = {\n    \"gelu\": partial(nn.gelu, approximate=False),\n    \"relu\": nn.relu,\n    \"silu\": nn.swish,\n    \"swish\": nn.swish,\n    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,", "    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,\n}\n\n\nMASK_VALUE = -1e10\n\nQ_CHUNK_SIZE = 1024\nK_CHUNK_SIZE = 1024\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)", "K_CHUNK_SIZE = 1024\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)", "\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)", "def apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass _AttentionBlock(nn.Module):\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    float32_logits: bool = False\n\n    def setup(self):\n        self.embed_dim = self.hidden_size\n        self.head_dim = self.embed_dim // self.num_heads\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.variance_scaling(\n                scale=1.0, mode='fan_in',\n                distribution='normal',\n            )\n        )\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\n        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n        self.fc_in = nn.Dense(self.intermediate_size,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.fc_out = nn.Dense(self.embed_dim,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.act = ACT2FN[self.activation_function]\n        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            pos_embd_dim = self.rotary_dim\n        else:\n            pos_embd_dim = self.embed_dim // self.num_heads\n        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    def attn_out_proj(self, attn_output, deterministic):\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.out_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        return attn_output\n\n    def forward_qkv(\n        self,\n        hidden_states,\n        position_ids,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_1(hidden_states)\n        query = self.q_proj(hidden_states)\n        key = self.k_proj(hidden_states)\n        value = self.v_proj(hidden_states)\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            k_rot = key[:, :, :, : self.rotary_dim]\n            k_pass = key[:, :, :, self.rotary_dim :]\n\n            q_rot = query[:, :, :, : self.rotary_dim]\n            q_pass = query[:, :, :, self.rotary_dim :]\n\n            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\n            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n        else:\n            key = apply_rotary_pos_emb(key, sincos)\n            query = apply_rotary_pos_emb(query, sincos)\n\n        if self.float32_logits:\n            query = query.astype(jnp.float32)\n            key = key.astype(jnp.float32)\n\n        return query, key, value\n\n    def forward_ffn(\n        self,\n        hidden_states,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_2(hidden_states)\n        hidden_states = self.fc_in(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc_out(hidden_states)\n        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\n        return hidden_states", "\n\n\nclass AttentionBlock(nn.Module):\n    q_chunk_size: int # not used\n    k_chunk_size: int # not used\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    attn_pdrop: float = 0.0\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    policy: str = None # not used\n    prevent_cse: bool = False # not used\n    float32_logits: bool = False\n\n    def setup(self):\n        self.attn = _AttentionBlock(\n            self.hidden_size,\n            self.num_heads,\n            self.rotary_dim,\n            self.intermediate_size,\n            self.layer_norm_epsilon,\n            self.activation_function,\n            self.resid_pdrop,\n            self.max_position_embeddings,\n            self.dtype,\n            self.causal,\n            self.float32_logits,\n        )\n        self.causal_mask = make_causal_mask(jnp.ones((1, self.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\")\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        \"\"\"\n        This function takes projected key, value states from a single input token and concatenates the states to cached\n        states from previous steps. This function is slighly adapted from the official Flax repository:\n        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n        \"\"\"\n        # detect if we're initializing by absence of existing cache data.\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask,\n        position_ids,\n        deterministic: bool = True,\n        init_cache: bool = False,\n    ):\n\n        query, key, value = self.attn.forward_qkv(\n            hidden_states,\n            position_ids,\n            deterministic=deterministic,\n        )\n\n        query_length, key_length = query.shape[1], key.shape[1]\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                self.causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = self.causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n        if self.causal:\n            attention_mask = combine_masks(attention_mask, causal_mask)\n        else:\n            attention_mask = attention_mask\n\n        dropout_rng = None\n        if not deterministic and self.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n\n        attention_bias = lax.select(\n            attention_mask > 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n        )\n        attn_weights = nn.dot_product_attention_weights(\n            query,\n            key,\n            bias=attention_bias,\n            dropout_rng=dropout_rng,\n            dropout_rate=self.attn_pdrop,\n            deterministic=deterministic,\n            dtype=self.dtype,\n            precision=None,\n        )\n        attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n        attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n        ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n        outputs = attn_output + ffn_output + hidden_states\n        return outputs", "\n\nif __name__ == '__main__':\n    with jax.profiler.trace('/tmp/prof/vanilla'):\n        class Model(nn.Module):\n            def setup(self):\n                self.blocks = [\n                    AttentionBlock(\n                        q_chunk_size=256,\n                        k_chunk_size=256,\n                        hidden_size=2048,\n                        num_heads=16,\n                        rotary_dim=128,\n                        intermediate_size=8192,\n                        layer_norm_epsilon=1e-5,\n                        activation_function=\"gelu\",\n                        resid_pdrop=0.0,\n                        max_position_embeddings=2048,\n                        dtype=jnp.float32,\n                        causal=True,\n                )\n                for _ in range(2)\n                ]\n            def __call__(self, hidden_states, attention_mask, position_ids):\n                for block in self.blocks:\n                    hidden_states = block(hidden_states, attention_mask, position_ids)\n                return hidden_states\n\n        hidden_states = jnp.zeros((2, 1024, 2048))\n        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n        model = Model()\n        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n        output = output.block_until_ready()", "\n"]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/memeff.py", "chunked_list": ["import functools\nimport json\nimport math\nfrom functools import partial\nfrom typing import Callable, NamedTuple, Optional\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport numpy as np", "import jax.numpy as jnp\nimport numpy as np\nfrom einops import rearrange\nfrom flax.linen import combine_masks, make_causal_mask\nfrom jax import lax\nfrom jax import numpy as jnp\n\n\ndef quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)", "def quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)\n\nACT2FN = {\n    \"gelu\": partial(nn.gelu, approximate=False),\n    \"relu\": nn.relu,\n    \"silu\": nn.swish,\n    \"swish\": nn.swish,\n    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,", "    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,\n}\n\ndef get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n    }[name]", "\nMASK_VALUE = -1e10\n\nQ_CHUNK_SIZE = 1024\nK_CHUNK_SIZE = 1024\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)", "\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)", "def apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass _AttentionBlock(nn.Module):\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    float32_logits: bool = False\n\n    def setup(self):\n        self.embed_dim = self.hidden_size\n        self.head_dim = self.embed_dim // self.num_heads\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.variance_scaling(\n                scale=1.0, mode='fan_in',\n                distribution='normal',\n            )\n        )\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\n        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n        self.fc_in = nn.Dense(self.intermediate_size,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.fc_out = nn.Dense(self.embed_dim,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.act = ACT2FN[self.activation_function]\n        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            pos_embd_dim = self.rotary_dim\n        else:\n            pos_embd_dim = self.embed_dim // self.num_heads\n        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    def attn_out_proj(self, attn_output, deterministic):\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.out_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        return attn_output\n\n    def forward_qkv(\n        self,\n        hidden_states,\n        position_ids,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_1(hidden_states)\n        query = self.q_proj(hidden_states)\n        key = self.k_proj(hidden_states)\n        value = self.v_proj(hidden_states)\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            k_rot = key[:, :, :, : self.rotary_dim]\n            k_pass = key[:, :, :, self.rotary_dim :]\n\n            q_rot = query[:, :, :, : self.rotary_dim]\n            q_pass = query[:, :, :, self.rotary_dim :]\n\n            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\n            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n        else:\n            key = apply_rotary_pos_emb(key, sincos)\n            query = apply_rotary_pos_emb(query, sincos)\n\n        if self.float32_logits:\n            query = query.astype(jnp.float32)\n            key = key.astype(jnp.float32)\n\n        return query, key, value\n\n    def forward_ffn(\n        self,\n        hidden_states,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_2(hidden_states)\n        hidden_states = self.fc_in(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc_out(hidden_states)\n        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\n        return hidden_states", "\n\n\nclass AttentionBlock(nn.Module):\n    q_chunk_size: int\n    k_chunk_size: int\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    attn_pdrop: float = 0.0\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    policy: str = 'nothing_saveable'\n    prevent_cse: bool = False\n    float32_logits: bool = False\n\n    def setup(self):\n        self.attn = _AttentionBlock(\n            self.hidden_size,\n            self.num_heads,\n            self.rotary_dim,\n            self.intermediate_size,\n            self.layer_norm_epsilon,\n            self.activation_function,\n            self.resid_pdrop,\n            self.max_position_embeddings,\n            self.dtype,\n            self.causal,\n            self.float32_logits,\n        )\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        \"\"\"\n        This function takes projected key, value states from a single input token and concatenates the states to cached\n        states from previous steps. This function is slighly adapted from the official Flax repository:\n        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n        \"\"\"\n        # detect if we're initializing by absence of existing cache data.\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask,\n        position_ids,\n        deterministic: bool = True,\n        init_cache: bool = False,\n    ):\n\n        query, key, value = self.attn.forward_qkv(\n            hidden_states,\n            position_ids,\n            deterministic=deterministic,\n        )\n        query = query / jnp.sqrt(query.shape[-1])\n\n        dropout_rng = None\n        if not deterministic and self.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        attention_bias = lax.select(\n            attention_mask > 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n        )\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n            # use standard dot product attention since query length is 1\n            attn_weights = nn.dot_product_attention_weights(\n                query,\n                key,\n                bias=attention_bias,\n                dropout_rng=dropout_rng,\n                dropout_rate=self.config.attn_pdrop,\n                deterministic=deterministic,\n                dtype=self.dtype,\n                precision=None,\n            )\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n            outputs = attn_output + ffn_output + hidden_states\n        else:\n            attn_output = blockwise_compute_attn(\n                query,\n                key,\n                value,\n                bias=attention_bias,\n                deterministic=not deterministic,\n                dropout_rng=dropout_rng,\n                attn_pdrop=self.attn_pdrop,\n                causal_mask=self.causal,\n                query_chunk_size=self.q_chunk_size,\n                key_chunk_size=self.k_chunk_size,\n                dtype=self.dtype,\n                policy=self.policy,\n                precision=None,\n                prevent_cse=self.prevent_cse,\n            )\n\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n            outputs = ffn_output + hidden_states + attn_output\n        return outputs", "\n\ndef _chunk_attention_bias(query_chunk_size, key_chunk_size,\n            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n            query_chunk_idx, key_chunk_idx):\n    query_offset = query_chunk_idx * query_chunk_size\n    key_offset = key_chunk_idx * key_chunk_size\n    chunk_bias = jnp.zeros((1, 1, 1, 1))\n    if bias is not None:\n        chunk_bias = lax.dynamic_slice(\n            bias,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n        )\n\n    if causal_mask:\n        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n        offset = query_offset - key_offset\n        query_idx += offset\n        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_slice = lax.dynamic_slice(\n            attn_dropout,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(\n                *attn_dropout.shape[:2],\n                min(attn_dropout.shape[-2], query_chunk_size),\n                min(attn_dropout.shape[-1], key_chunk_size),\n            ),\n        )\n        chunk_bias -= attn_dropout_slice * 1e6\n    return chunk_bias", "\nclass Carry(NamedTuple):\n    numerator: jax.Array\n    denominator: jax.Array\n    max_so_far: jax.Array\n\ndef blockwise_compute_attn(query, key, value,\n        bias=None,\n        deterministic=False,\n        dropout_rng=None,\n        attn_pdrop=0.0,\n        causal_mask=True,\n        query_chunk_size=None,\n        key_chunk_size=None,\n        dtype=jnp.float32,\n        policy='nothing_saveable',\n        precision=lax.Precision.HIGHEST,\n        prevent_cse=False,):\n    q_len = query.shape[1]\n    kv_len = key.shape[1]\n    query = rearrange(query, 'b (n c) h q -> b n c h q', c=query_chunk_size)\n    key, value = map(lambda t: rearrange(t, 'b (n c) h v -> b n c h v', c=key_chunk_size), (key, value))\n    query, key, value = map(lambda t: rearrange(t, 'b n c h d -> n b c h d'), (query, key, value))\n    num_q, batch, _, num_heads, dim_per_head = query.shape\n    num_kv, _, _, _, _ = key.shape\n\n    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n        assert bias_dim == 1 or bias_dim == broadcast_dim\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n    else:\n        attn_dropout = None\n\n    _chunk_bias_fn = functools.partial(\n        _chunk_attention_bias,\n        query_chunk_size, key_chunk_size,\n        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\n    def _query_chunk_attention(args):\n        query_chunk, query_chunk_idx = args\n\n        @functools.partial(jax.checkpoint, prevent_cse=prevent_cse,\n                           policy=get_gradient_checkpoint_policy(policy))\n        def summarize_chunk(carry, args):\n            key_chunk, value_chunk, key_chunk_idx = args\n            (numerator, denominator, prev_max_score) = carry\n            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n            attn_weights = attn_weights + bias_chunk\n\n            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n            max_score = jnp.maximum(prev_max_score, max_score)\n            max_score = jax.lax.stop_gradient(max_score)\n            exp_weights = jnp.exp(attn_weights - max_score)\n            exp_values = jnp.einsum(\n                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n            )\n            correction = jnp.exp(prev_max_score - max_score)\n            numerator = numerator * correction + exp_values\n            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n            return Carry(numerator, denominator, max_score), None\n\n        init_carry = Carry(\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n        )\n        (numerator, denominator, max_score), _ = lax.scan(\n            summarize_chunk, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n        )\n        outputs = (numerator / denominator).astype(dtype)\n        return outputs\n\n    _, res = lax.scan(\n        lambda _, x: ((), _query_chunk_attention(x)),\n        (), xs=(query, jnp.arange(0, num_q))\n    )\n    res = rearrange(res, 'n b c h d -> b (n c) h d')\n    return res", "\n\nif __name__ == '__main__':\n    with jax.profiler.trace('/tmp/prof/memeff'):\n        class Model(nn.Module):\n            def setup(self):\n                self.blocks = [\n                    AttentionBlock(\n                        q_chunk_size=256,\n                        k_chunk_size=256,\n                        hidden_size=2048,\n                        num_heads=16,\n                        rotary_dim=128,\n                        intermediate_size=8192,\n                        layer_norm_epsilon=1e-5,\n                        activation_function=\"gelu\",\n                        resid_pdrop=0.0,\n                        max_position_embeddings=2048,\n                        dtype=jnp.float32,\n                        causal=True,\n                )\n                for _ in range(2)\n                ]\n            def __call__(self, hidden_states, attention_mask, position_ids):\n                for block in self.blocks:\n                    hidden_states = block(hidden_states, attention_mask, position_ids)\n                return hidden_states\n\n        hidden_states = jnp.zeros((2, 1024, 2048))\n        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n        model = Model()\n        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n        output = output.block_until_ready()", "\n"]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/__init__.py", "chunked_list": [""]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/blockwise_parallel_v1.py", "chunked_list": ["import functools\nimport json\nimport math\nfrom functools import partial\nfrom typing import Callable, NamedTuple, Optional\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport numpy as np", "import jax.numpy as jnp\nimport numpy as np\nfrom einops import rearrange\nfrom flax.linen import combine_masks, make_causal_mask\nfrom jax import lax\nfrom jax import numpy as jnp\n\n\ndef quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)", "def quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)\n\nACT2FN = {\n    \"gelu\": partial(nn.gelu, approximate=False),\n    \"relu\": nn.relu,\n    \"silu\": nn.swish,\n    \"swish\": nn.swish,\n    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,", "    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,\n}\n\ndef get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n    }[name]", "\nMASK_VALUE = -1e10\n\nQ_CHUNK_SIZE = 1024\nK_CHUNK_SIZE = 1024\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)", "\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)", "def apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass _AttentionBlock(nn.Module):\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    float32_logits: bool = False\n\n    def setup(self):\n        self.embed_dim = self.hidden_size\n        self.head_dim = self.embed_dim // self.num_heads\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.variance_scaling(\n                scale=1.0, mode='fan_in',\n                distribution='normal',\n            )\n        )\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\n        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n        self.fc_in = nn.Dense(self.intermediate_size,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.fc_out = nn.Dense(self.embed_dim,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.act = ACT2FN[self.activation_function]\n        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            pos_embd_dim = self.rotary_dim\n        else:\n            pos_embd_dim = self.embed_dim // self.num_heads\n        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    def attn_out_proj(self, attn_output, deterministic):\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.out_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        return attn_output\n\n    def forward_qkv(\n        self,\n        hidden_states,\n        position_ids,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_1(hidden_states)\n        query = self.q_proj(hidden_states)\n        key = self.k_proj(hidden_states)\n        value = self.v_proj(hidden_states)\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            k_rot = key[:, :, :, : self.rotary_dim]\n            k_pass = key[:, :, :, self.rotary_dim :]\n\n            q_rot = query[:, :, :, : self.rotary_dim]\n            q_pass = query[:, :, :, self.rotary_dim :]\n\n            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\n            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n        else:\n            key = apply_rotary_pos_emb(key, sincos)\n            query = apply_rotary_pos_emb(query, sincos)\n\n        if self.float32_logits:\n            query = query.astype(jnp.float32)\n            key = key.astype(jnp.float32)\n\n        return query, key, value\n\n    def forward_ffn(\n        self,\n        hidden_states,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_2(hidden_states)\n        hidden_states = self.fc_in(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc_out(hidden_states)\n        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\n        return hidden_states\n\n    def forward_query(\n        self,\n        hidden_states,\n        position_ids,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_1(hidden_states)\n        query = self.q_proj(hidden_states)\n        query = self._split_heads(query)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            q_rot = query[:, :, :, : self.rotary_dim]\n            q_pass = query[:, :, :, self.rotary_dim :]\n            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n        else:\n            query = apply_rotary_pos_emb(query, sincos)\n\n        if self.float32_logits:\n            query = query.astype(jnp.float32)\n\n        return query\n\n    def forward_key_value(\n        self,\n        hidden_states,\n        position_ids,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_1(hidden_states)\n        key = self.k_proj(hidden_states)\n        value = self.v_proj(hidden_states)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            k_rot = key[:, :, :, : self.rotary_dim]\n            k_pass = key[:, :, :, self.rotary_dim :]\n\n            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n\n            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n        else:\n            key = apply_rotary_pos_emb(key, sincos)\n\n        if self.float32_logits:\n            key = key.astype(jnp.float32)\n\n        return key, value", "\n\n\nclass AttentionBlock(nn.Module):\n    q_chunk_size: int\n    k_chunk_size: int\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    attn_pdrop: float = 0.0\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    policy: str = 'nothing_saveable'\n    prevent_cse: bool = False\n    float32_logits: bool = False\n\n    def setup(self):\n        self.attn = _AttentionBlock(\n            self.hidden_size,\n            self.num_heads,\n            self.rotary_dim,\n            self.intermediate_size,\n            self.layer_norm_epsilon,\n            self.activation_function,\n            self.resid_pdrop,\n            self.max_position_embeddings,\n            self.dtype,\n            self.causal,\n            self.float32_logits,\n        )\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        \"\"\"\n        This function takes projected key, value states from a single input token and concatenates the states to cached\n        states from previous steps. This function is slighly adapted from the official Flax repository:\n        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n        \"\"\"\n        # detect if we're initializing by absence of existing cache data.\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask,\n        position_ids,\n        deterministic: bool = True,\n        init_cache: bool = False,\n    ):\n        dropout_rng = None\n        if not deterministic and self.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        attention_bias = lax.select(\n            attention_mask > 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n        )\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n            # use standard dot product attention since query length is 1\n            attn_weights = nn.dot_product_attention_weights(\n                query,\n                key,\n                bias=attention_bias,\n                dropout_rng=dropout_rng,\n                dropout_rate=self.config.attn_pdrop,\n                deterministic=deterministic,\n                dtype=self.dtype,\n                precision=None,\n            )\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n            outputs = attn_output + ffn_output + hidden_states\n        else:\n            outputs = blockwise_compute(\n                self.attn,\n                hidden_states,\n                hidden_states,\n                position_ids,\n                num_heads=self.num_heads,\n                bias=attention_bias,\n                deterministic=deterministic,\n                dropout_rng=dropout_rng,\n                attn_pdrop=self.attn_pdrop,\n                causal_mask=self.causal,\n                query_chunk_size=self.q_chunk_size,\n                key_chunk_size=self.k_chunk_size,\n                dtype=self.dtype,\n                policy=self.policy,\n                precision=None,\n                prevent_cse=self.prevent_cse,\n            )\n\n        return outputs", "\n\ndef _chunk_attention_bias(query_chunk_size, key_chunk_size,\n            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n            query_chunk_idx, key_chunk_idx):\n    query_offset = query_chunk_idx * query_chunk_size\n    key_offset = key_chunk_idx * key_chunk_size\n    chunk_bias = jnp.zeros((1, 1, 1, 1))\n    if bias is not None:\n        chunk_bias = lax.dynamic_slice(\n            bias,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n        )\n\n    if causal_mask:\n        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n        offset = query_offset - key_offset\n        query_idx += offset\n        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_slice = lax.dynamic_slice(\n            attn_dropout,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(\n                *attn_dropout.shape[:2],\n                min(attn_dropout.shape[-2], query_chunk_size),\n                min(attn_dropout.shape[-1], key_chunk_size),\n            ),\n        )\n        chunk_bias -= attn_dropout_slice * 1e6\n    return chunk_bias", "\nclass Carry(NamedTuple):\n    numerator: jax.Array\n    denominator: jax.Array\n    max_so_far: jax.Array\n\ndef blockwise_compute(cell,\n        q_inputs,\n        kv_inputs,\n        position_ids,\n        num_heads,\n        bias=None,\n        deterministic=False,\n        dropout_rng=None,\n        attn_pdrop=0.0,\n        causal_mask=True,\n        query_chunk_size=None,\n        key_chunk_size=None,\n        dtype=jnp.float32,\n        policy='nothing_saveable',\n        precision=lax.Precision.HIGHEST,\n        prevent_cse=False,):\n    q_len = q_inputs.shape[1]\n    kv_len = kv_inputs.shape[1]\n    q_inputs = rearrange(q_inputs, 'b (n c) d -> b n c d', c=query_chunk_size)\n    kv_inputs = rearrange(kv_inputs, 'b (n c) d -> b n c d', c=key_chunk_size)\n    q_inputs, kv_inputs = map(lambda t: rearrange(t, 'b n c d -> n b c d'), (q_inputs, kv_inputs))\n    num_q, batch, _, _ = q_inputs.shape\n    num_kv, _, _, _ = kv_inputs.shape\n    q_position_ids = rearrange(position_ids, 'b (n c) -> n b c', c=query_chunk_size)\n    kv_position_ids = rearrange(position_ids, 'b (n c) -> n b c', c=key_chunk_size)\n    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n        assert bias_dim == 1 or bias_dim == broadcast_dim\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n    else:\n        attn_dropout = None\n\n    _chunk_bias_fn = functools.partial(\n        _chunk_attention_bias,\n        query_chunk_size, key_chunk_size,\n        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\n    def _query_chunk_attention(cell, _, args):\n        input_chunk, query_chunk_idx, query_position_ids_chunk = args\n        query_chunk = cell.forward_query(input_chunk, query_position_ids_chunk)\n        query_chunk = query_chunk / jnp.sqrt(query_chunk.shape[-1])\n        dim_per_head = query_chunk.shape[-1]\n\n        def summarize_chunk(cell, carry, args):\n            kv_chunk, key_chunk_idx, kv_position_ids_chunk = args\n            (numerator, denominator, prev_max_score) = carry\n            key_chunk, value_chunk = cell.forward_key_value(kv_chunk, kv_position_ids_chunk)\n            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n            attn_weights = attn_weights + bias_chunk\n            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n            max_score = jnp.maximum(prev_max_score, max_score)\n            max_score = jax.lax.stop_gradient(max_score)\n            exp_weights = jnp.exp(attn_weights - max_score)\n            exp_values = jnp.einsum(\n                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n            )\n            correction = jnp.exp(prev_max_score - max_score)\n            numerator = numerator * correction + exp_values\n            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n            return Carry(numerator, denominator, max_score), None\n\n        init_carry = Carry(\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n        )\n        summarize_chunk = nn.remat(\n            summarize_chunk,\n            variables=\"params\",\n            rngs={\"params\" : False, \"dropout\": False},\n            prevent_cse=prevent_cse,\n            policy=get_gradient_checkpoint_policy(policy),\n        )\n        (numerator, denominator, max_score), _ = nn.scan(\n            summarize_chunk,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\" : False, \"dropout\": False},\n            in_axes=0,\n            out_axes=0,\n            length=num_kv,\n        )(cell, init_carry, (kv_inputs, jnp.arange(0, num_kv), kv_position_ids))\n        attn_chunk = (numerator / denominator).astype(dtype)\n        attn_chunk = cell.attn_out_proj(attn_chunk, deterministic)\n        ffn_chunk = cell.forward_ffn(attn_chunk + input_chunk, deterministic)\n        outputs = ffn_chunk + attn_chunk + input_chunk\n        return _, outputs\n\n    _query_chunk_attention = nn.remat(\n        _query_chunk_attention,\n        variables=\"params\",\n        rngs={\"params\" : False, \"dropout\": False},\n        prevent_cse=prevent_cse,\n        policy=get_gradient_checkpoint_policy(policy),\n    )\n    _, res = nn.scan(\n        _query_chunk_attention,\n        variable_broadcast=\"params\",\n        split_rngs={\"params\" : False, \"dropout\": False},\n        in_axes=0,\n        out_axes=0,\n        length=num_q,\n    )(cell, None, (q_inputs, jnp.arange(0, num_q), q_position_ids))\n    res = rearrange(res, 'n b c d -> b (n c) d')\n    return res", "\nif __name__ == '__main__':\n    with jax.profiler.trace('/tmp/prof/blockwise_parallel_v1'):\n        class Model(nn.Module):\n            def setup(self):\n                self.blocks = [\n                    AttentionBlock(\n                        q_chunk_size=256,\n                        k_chunk_size=256,\n                        hidden_size=2048,\n                        num_heads=16,\n                        rotary_dim=128,\n                        intermediate_size=8192,\n                        layer_norm_epsilon=1e-5,\n                        activation_function=\"gelu\",\n                        resid_pdrop=0.0,\n                        max_position_embeddings=2048,\n                        dtype=jnp.float32,\n                        causal=True,\n                )\n                for _ in range(2)\n                ]\n            def __call__(self, hidden_states, attention_mask, position_ids):\n                for block in self.blocks:\n                    hidden_states = block(hidden_states, attention_mask, position_ids)\n                return hidden_states\n\n        hidden_states = jnp.zeros((2, 1024, 2048))\n        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n        model = Model()\n        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n        output = output.block_until_ready()", ""]}
{"filename": "blockwise-parallel-transformer/bpt/blocks/blockwise_parallel.py", "chunked_list": ["import functools\nimport json\nimport math\nfrom functools import partial\nfrom typing import Callable, NamedTuple, Optional\n\nimport flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport numpy as np", "import jax.numpy as jnp\nimport numpy as np\nfrom einops import rearrange\nfrom flax.linen import combine_masks, make_causal_mask\nfrom jax import lax\nfrom jax import numpy as jnp\n\n\ndef quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)", "def quick_gelu(x):\n    return x * jax.nn.sigmoid(1.702 * x)\n\nACT2FN = {\n    \"gelu\": partial(nn.gelu, approximate=False),\n    \"relu\": nn.relu,\n    \"silu\": nn.swish,\n    \"swish\": nn.swish,\n    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,", "    \"gelu_new\": partial(nn.gelu, approximate=True),\n    \"quick_gelu\": quick_gelu,\n}\n\ndef get_gradient_checkpoint_policy(name):\n    return {\n        'everything_saveable': jax.checkpoint_policies.everything_saveable,\n        'nothing_saveable': jax.checkpoint_policies.nothing_saveable,\n        'dots_saveable': jax.checkpoint_policies.dots_saveable,\n        'dots_with_no_batch_dims_saveable': jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n    }[name]", "\nMASK_VALUE = -1e10\n\nQ_CHUNK_SIZE = 1024\nK_CHUNK_SIZE = 1024\n\ndef create_sinusoidal_positions(num_pos, dim):\n    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n    sinusoid_inp = np.einsum(\"i , j -> i j\", np.arange(num_pos), inv_freq).astype(\"float32\")\n    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n\n    sentinel = dim // 2 + dim % 2\n    out = np.zeros((num_pos, dim))\n    out[:, 0:sentinel] = sin\n    out[:, sentinel:] = cos\n\n    return jnp.array(out)", "\n\ndef rotate_every_two(tensor):\n    rotate_half_tensor = jnp.stack((-tensor[:, :, :, 1::2], tensor[:, :, :, ::2]), axis=-1)\n    rotate_half_tensor = rotate_half_tensor.reshape(rotate_half_tensor.shape[:-2] + (-1,))\n    return rotate_half_tensor\n\n\ndef apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)", "def apply_rotary_pos_emb(tensor, sincos):\n    sin_pos, cos_pos = sincos\n    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)\n    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)\n    return (tensor * cos_pos) + (rotate_every_two(tensor) * sin_pos)\n\n\nclass _AttentionBlock(nn.Module):\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    float32_logits: bool = False\n\n    def setup(self):\n        self.embed_dim = self.hidden_size\n        self.head_dim = self.embed_dim // self.num_heads\n        dense = partial(\n            nn.Dense,\n            self.embed_dim,\n            use_bias=False,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.variance_scaling(\n                scale=1.0, mode='fan_in',\n                distribution='normal',\n            )\n        )\n        self.q_proj, self.k_proj, self.v_proj = dense(), dense(), dense()\n        self.out_proj = dense()\n        self.ln_1 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n\n        self.ln_2 = nn.LayerNorm(epsilon=self.layer_norm_epsilon, dtype=self.dtype)\n        self.fc_in = nn.Dense(self.intermediate_size,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.fc_out = nn.Dense(self.embed_dim,\n                            dtype=self.dtype,\n                            kernel_init=jax.nn.initializers.variance_scaling(\n                            scale=1.0, mode='fan_in',\n                            distribution='normal',\n            )\n        )\n        self.act = ACT2FN[self.activation_function]\n        self.resid_dropout = nn.Dropout(rate=self.resid_pdrop)\n\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            pos_embd_dim = self.rotary_dim\n        else:\n            pos_embd_dim = self.embed_dim // self.num_heads\n        self.embed_positions = create_sinusoidal_positions(self.max_position_embeddings, pos_embd_dim)\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.num_heads, self.head_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.embed_dim,))\n\n    def attn_out_proj(self, attn_output, deterministic):\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.out_proj(attn_output)\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        return attn_output\n\n    def forward_qkv(\n        self,\n        hidden_states,\n        position_ids,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_1(hidden_states)\n        query = self.q_proj(hidden_states)\n        key = self.k_proj(hidden_states)\n        value = self.v_proj(hidden_states)\n        query = self._split_heads(query)\n        key = self._split_heads(key)\n        value = self._split_heads(value)\n\n        sincos = jnp.take(self.embed_positions, position_ids, axis=0)\n        sincos = jnp.split(sincos, 2, axis=-1)\n        if self.rotary_dim is not None and self.rotary_dim > 0:\n            k_rot = key[:, :, :, : self.rotary_dim]\n            k_pass = key[:, :, :, self.rotary_dim :]\n\n            q_rot = query[:, :, :, : self.rotary_dim]\n            q_pass = query[:, :, :, self.rotary_dim :]\n\n            k_rot = apply_rotary_pos_emb(k_rot, sincos)\n            q_rot = apply_rotary_pos_emb(q_rot, sincos)\n\n            key = jnp.concatenate([k_rot, k_pass], axis=-1)\n            query = jnp.concatenate([q_rot, q_pass], axis=-1)\n        else:\n            key = apply_rotary_pos_emb(key, sincos)\n            query = apply_rotary_pos_emb(query, sincos)\n\n        if self.float32_logits:\n            query = query.astype(jnp.float32)\n            key = key.astype(jnp.float32)\n\n        return query, key, value\n\n    def forward_ffn(\n        self,\n        hidden_states,\n        deterministic: bool = True,\n    ):\n        hidden_states = self.ln_2(hidden_states)\n        hidden_states = self.fc_in(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc_out(hidden_states)\n        hidden_states = self.resid_dropout(hidden_states, deterministic=deterministic)\n\n        return hidden_states", "\n\nclass AttentionBlock(nn.Module):\n    q_chunk_size: int\n    k_chunk_size: int\n    hidden_size: int\n    num_heads: int\n    rotary_dim: Optional[int]\n    intermediate_size: int\n    layer_norm_epsilon: float = 1e-5\n    activation_function: str = \"gelu\"\n    attn_pdrop: float = 0.0\n    resid_pdrop: float = 0.0\n    max_position_embeddings: int = 1024\n    dtype: jnp.dtype = jnp.float32\n    causal: bool = True\n    policy: str = 'nothing_saveable'\n    prevent_cse: bool = False\n    float32_logits: bool = False\n\n    def setup(self):\n        self.attn = _AttentionBlock(\n            self.hidden_size,\n            self.num_heads,\n            self.rotary_dim,\n            self.intermediate_size,\n            self.layer_norm_epsilon,\n            self.activation_function,\n            self.resid_pdrop,\n            self.max_position_embeddings,\n            self.dtype,\n            self.causal,\n            self.float32_logits,\n        )\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        \"\"\"\n        This function takes projected key, value states from a single input token and concatenates the states to cached\n        states from previous steps. This function is slighly adapted from the official Flax repository:\n        https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n        \"\"\"\n        # detect if we're initializing by absence of existing cache data.\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) < cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(\n        self,\n        hidden_states,\n        attention_mask,\n        position_ids,\n        deterministic: bool = True,\n        init_cache: bool = False,\n    ):\n        query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n        query = query / jnp.sqrt(query.shape[-1])\n\n        dropout_rng = None\n        if not deterministic and self.attn_pdrop > 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        attention_bias = lax.select(\n            attention_mask > 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, -1e9).astype(self.dtype),\n        )\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            query, key, value = self.attn.forward_qkv(hidden_states, position_ids)\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query, attention_mask)\n            # use standard dot product attention since query length is 1\n            attn_weights = nn.dot_product_attention_weights(\n                query,\n                key,\n                bias=attention_bias,\n                dropout_rng=dropout_rng,\n                dropout_rate=self.config.attn_pdrop,\n                deterministic=deterministic,\n                dtype=self.dtype,\n                precision=None,\n            )\n            attn_output = jnp.einsum(\"...hqk,...khd->...qhd\", attn_weights, value)\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = self.attn.forward_ffn(hidden_states + attn_output, deterministic=deterministic)\n            outputs = attn_output + ffn_output + hidden_states\n        else:\n            attn_output = blockwise_compute_attn(\n                query,\n                key,\n                value,\n                bias=attention_bias,\n                deterministic=not deterministic,\n                dropout_rng=dropout_rng,\n                attn_pdrop=self.attn_pdrop,\n                causal_mask=self.causal,\n                query_chunk_size=self.q_chunk_size,\n                key_chunk_size=self.k_chunk_size,\n                dtype=self.dtype,\n                policy=self.policy,\n                precision=None,\n                prevent_cse=self.prevent_cse,\n            )\n            attn_output = self.attn.attn_out_proj(attn_output, deterministic=deterministic)\n            ffn_output = blockwise_compute_ffn(\n                self.attn,\n                hidden_states + attn_output,\n                chunk_size=self.q_chunk_size,\n                deterministic=deterministic,\n                policy=self.policy,\n                prevent_cse=self.prevent_cse,\n            )\n            outputs = ffn_output + hidden_states + attn_output\n        return outputs", "\n\ndef _chunk_attention_bias(query_chunk_size, key_chunk_size,\n            bias, deterministic, attn_dropout, attn_pdrop, causal_mask,\n            query_chunk_idx, key_chunk_idx):\n    query_offset = query_chunk_idx * query_chunk_size\n    key_offset = key_chunk_idx * key_chunk_size\n    chunk_bias = jnp.zeros((1, 1, 1, 1))\n    if bias is not None:\n        chunk_bias = lax.dynamic_slice(\n            bias,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(*bias.shape[:2], min(bias.shape[-2], query_chunk_size), min(bias.shape[-1], key_chunk_size)),\n        )\n\n    if causal_mask:\n        query_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(query_chunk_size, 1), dimension=0)\n        key_idx = lax.broadcasted_iota(dtype=jnp.int32, shape=(1, key_chunk_size), dimension=1)\n        offset = query_offset - key_offset\n        query_idx += offset\n        causal_mask_value = (query_idx < key_idx) * MASK_VALUE\n        chunk_bias += causal_mask_value.reshape(1, 1, *causal_mask_value.shape)\n\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_slice = lax.dynamic_slice(\n            attn_dropout,\n            start_indices=(0, 0, query_offset, key_offset),\n            slice_sizes=(\n                *attn_dropout.shape[:2],\n                min(attn_dropout.shape[-2], query_chunk_size),\n                min(attn_dropout.shape[-1], key_chunk_size),\n            ),\n        )\n        chunk_bias -= attn_dropout_slice * 1e6\n    return chunk_bias", "\nclass Carry(NamedTuple):\n    numerator: jax.Array\n    denominator: jax.Array\n    max_so_far: jax.Array\n\ndef blockwise_compute_attn(query, key, value,\n        bias=None,\n        deterministic=False,\n        dropout_rng=None,\n        attn_pdrop=0.0,\n        causal_mask=True,\n        query_chunk_size=None,\n        key_chunk_size=None,\n        dtype=jnp.float32,\n        policy='nothing_saveable',\n        precision=lax.Precision.HIGHEST,\n        prevent_cse=False,):\n    q_len = query.shape[1]\n    kv_len = key.shape[1]\n    query = rearrange(query, 'b (n c) h q -> b n c h q', c=query_chunk_size)\n    key, value = map(lambda t: rearrange(t, 'b (n c) h v -> b n c h v', c=key_chunk_size), (key, value))\n    query, key, value = map(lambda t: rearrange(t, 'b n c h d -> n b c h d'), (query, key, value))\n    num_q, batch, _, num_heads, dim_per_head = query.shape\n    num_kv, _, _, _, _ = key.shape\n\n    for bias_dim, broadcast_dim in zip(bias.shape, (batch, num_heads, q_len, kv_len)):\n        assert bias_dim == 1 or bias_dim == broadcast_dim\n    if not deterministic and attn_pdrop > 0.0:\n        attn_dropout_rng, dropout_rng = jax.random.split(dropout_rng)\n        attn_dropout = jax.random.bernoulli(attn_dropout_rng, attn_pdrop, (batch, num_heads, q_len, kv_len))\n    else:\n        attn_dropout = None\n\n    _chunk_bias_fn = functools.partial(\n        _chunk_attention_bias,\n        query_chunk_size, key_chunk_size,\n        bias, deterministic, attn_dropout, attn_pdrop, causal_mask)\n\n    def _query_chunk_attention(args):\n        query_chunk, query_chunk_idx = args\n\n        @functools.partial(jax.checkpoint, prevent_cse=prevent_cse,\n                           policy=get_gradient_checkpoint_policy(policy))\n        def summarize_chunk(carry, args):\n            key_chunk, value_chunk, key_chunk_idx = args\n            (numerator, denominator, prev_max_score) = carry\n            attn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n            bias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n            bias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n            attn_weights = attn_weights + bias_chunk\n\n            max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n            max_score = jnp.maximum(prev_max_score, max_score)\n            max_score = jax.lax.stop_gradient(max_score)\n            exp_weights = jnp.exp(attn_weights - max_score)\n            exp_values = jnp.einsum(\n                'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n            )\n            correction = jnp.exp(prev_max_score - max_score)\n            numerator = numerator * correction + exp_values\n            denominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n            return Carry(numerator, denominator, max_score), None\n\n        init_carry = Carry(\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            jnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=dtype),\n            (-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=dtype),\n        )\n        (numerator, denominator, max_score), _ = lax.scan(\n            summarize_chunk, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n        )\n        outputs = (numerator / denominator).astype(dtype)\n        return outputs\n\n    _, res = lax.scan(\n        lambda _, x: ((), _query_chunk_attention(x)),\n        (), xs=(query, jnp.arange(0, num_q))\n    )\n    res = rearrange(res, 'n b c h d -> b (n c) h d')\n    return res", "\ndef blockwise_compute_ffn(cell, inputs, chunk_size, deterministic, policy, prevent_cse):\n    inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=chunk_size)\n    inputs = rearrange(inputs, 'b n c d -> n b c d')\n    num_q, _, _, _ = inputs.shape\n    def ffn(cell, _, hidden_states):\n        outputs = cell.forward_ffn(hidden_states, deterministic=deterministic)\n        return _, outputs\n    ffn_remat = nn.remat(\n        ffn,\n        variables=\"params\",\n        rngs={\"params\" : False},\n        prevent_cse=prevent_cse,\n        policy=get_gradient_checkpoint_policy(policy),\n    )\n    _, res = nn.scan(\n        ffn_remat,\n        variable_broadcast=\"params\",\n        split_rngs={\"params\": False},\n        in_axes=0,\n        out_axes=0,\n        length=num_q,\n    )(cell, None, inputs)\n    res = rearrange(res, 'n b c d -> b (n c) d')\n    return res", "\nclass Blockwise_LM_Head(nn.Module):\n    vocab_size: int\n    chunk_size: int\n    policy: str = 'nothing_saveable'\n    dtype: jnp.dtype = jnp.float32\n    prevent_cse: bool = False\n\n    def setup(self):\n        self.lm_head = nn.Dense(\n            self.vocab_size,\n            dtype=self.dtype,\n            kernel_init=jax.nn.initializers.variance_scaling(\n                scale=1.0, mode='fan_in',\n                distribution='normal',\n            )\n        )\n\n    def __call__(self, inputs):\n        inputs = rearrange(inputs, 'b (n c) d -> b n c d', c=self.chunk_size)\n        inputs = rearrange(inputs, 'b n c d -> n b c d')\n        num_q, _, _, _ = inputs.shape\n        def lm_head(cell, _, hidden_states):\n            outputs = cell(hidden_states)\n            return _, outputs\n        lm_head_remat = nn.remat(\n            lm_head,\n            variables=\"params\",\n            rngs={\"params\" : False},\n            prevent_cse=self.prevent_cse,\n            policy=get_gradient_checkpoint_policy(self.policy),\n        )\n        _, res = nn.scan(\n            lm_head_remat,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False},\n            in_axes=0,\n            out_axes=0,\n            length=num_q,\n        )(self.lm_head, None, inputs)\n        res = rearrange(res, 'n b c d -> b (n c) d')\n        return res", "\ndef blockwise_cross_entropy(logits, tokens, valid=None,\n                            chunk_size=None, policy=None, prevent_cse=None):\n    if valid is None:\n        valid = jnp.ones(tokens.shape[:2])\n    valid = valid.astype(jnp.float32)\n    logits = jnp.reshape(logits, (-1, logits.shape[-1]))\n    tokens = jnp.reshape(tokens, (-1,))\n    valid = jnp.reshape(valid, (-1,))\n\n    def _cross_entropy_loss_and_accuracy(logits, tokens, valid):\n        valid_text_length = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n\n        token_log_prob = jnp.squeeze(\n            jnp.take_along_axis(\n                jax.nn.log_softmax(logits, axis=-1),\n                jnp.expand_dims(tokens, -1),\n                axis=-1,\n            ),\n            -1,\n        )\n        token_log_prob = jnp.where(valid > 0.0, token_log_prob, jnp.array(0.0))\n        correct = jnp.where(\n            valid > 0.0,\n            jnp.argmax(logits, axis=-1) == tokens,\n            jnp.array(False)\n        )\n        return token_log_prob, correct, valid_text_length\n    @partial(jax.checkpoint, prevent_cse=prevent_cse,\n             policy=get_gradient_checkpoint_policy(policy))\n    def _loss_and_accuracy(carry, args):\n        loss, accuracy, num = carry\n        logits, tokens, valid = args\n        token_log_prob, correct, valid_text_length = \\\n            _cross_entropy_loss_and_accuracy(logits, tokens, valid)\n        loss = loss + jnp.sum(token_log_prob, axis=-1) / valid_text_length\n        accuracy = accuracy + jnp.sum(correct, axis=-1) / valid_text_length\n        num = num + 1\n        return (loss, accuracy, num), None\n    num_chunk = logits.shape[0] // chunk_size\n    logits = rearrange(logits, '(n c) d -> n c d', c=chunk_size)\n    tokens = rearrange(tokens, '(n c) -> n c', c=chunk_size)\n    valid = rearrange(valid, '(n c) -> n c', c=chunk_size)\n    (loss, accuracy, num), _ = jax.lax.scan(\n        _loss_and_accuracy, (0.0, 0.0, 0), xs=(logits, tokens, valid),\n        length=num_chunk,\n    )\n    loss = - loss / num\n    accuracy = accuracy / num\n    return loss, accuracy", "\nif __name__ == '__main__':\n    with jax.profiler.trace('/tmp/prof/blockwise_parallel_simplified'):\n        class Model(nn.Module):\n            def setup(self):\n                self.blocks = [\n                    AttentionBlock(\n                        q_chunk_size=256,\n                        k_chunk_size=256,\n                        hidden_size=2048,\n                        num_heads=16,\n                        rotary_dim=128,\n                        intermediate_size=8192,\n                        layer_norm_epsilon=1e-5,\n                        activation_function=\"gelu\",\n                        resid_pdrop=0.0,\n                        max_position_embeddings=2048,\n                        dtype=jnp.float32,\n                        causal=True,\n                )\n                for _ in range(2)\n                ]\n            def __call__(self, hidden_states, attention_mask, position_ids):\n                for block in self.blocks:\n                    hidden_states = block(hidden_states, attention_mask, position_ids)\n                return hidden_states\n\n        hidden_states = jnp.zeros((2, 1024, 2048))\n        attention_mask = jnp.zeros((2, 1024), dtype=jnp.int32)\n        position_ids = jnp.zeros((2, 1024), dtype=jnp.int32)\n        model = Model()\n        variables = model.init(jax.random.PRNGKey(0), hidden_states, attention_mask, position_ids)\n        output = model.apply(variables, hidden_states, attention_mask, position_ids)\n        output = output.block_until_ready()", ""]}
